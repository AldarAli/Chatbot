ADVCOMP 2023
The Seventeenth International Conference on Advanced Engineering Computing
and Applications in Sciences
ISBN: 978-1-68558-107-7
September 25 - 29, 2023
Porto, Portugal
ADVCOMP 2023 Editors
Alice Koniges, University of Hawaii, USA
José Miguel Jimenez, Universitat Politecnica de Valencia, Spain

ADVCOMP 2023
Forward
The Seventeenth International Conference on Advanced Engineering Computing and Applications in
Sciences (ADVCOMP 2023), held between September 25th and September 29th, 2023, continued a series
of international events meant to bring together researchers from the academia and practitioners from
the industry in order to address fundamentals of advanced scientific computing and specific
mechanisms and algorithms for particular sciences.
With the advent of high-performance computing environments, virtualization, distributed and
parallel computing, as well as the increasing memory, storage and computational power, processing
particularly complex scientific applications and voluminous data is more affordable. With the current
computing software, hardware and distributed platforms, effective use of advanced computing
techniques is more achievable.
The conference provided a forum where researchers were able to present recent research results
and new research problems and directions related to them. The conference sought contributions
presenting novel research in all aspects of new scientific methods for computing and hybrid methods for
computing optimization, as well as advanced algorithms and computational procedures, software and
hardware solutions dealing with specific domains of science.
We take here the opportunity to warmly thank all the members of the ADVCOMP 2023 technical
program committee, as well as all the reviewers. The creation of such a high-quality conference program
would not have been possible without their involvement. We also kindly thank all the authors who
dedicated much of their time and effort to contribute to ADVCOMP 2023. We truly believe that, thanks
to all these efforts, the final conference program consisted of top-quality contributions. We also thank
the members of the ADVCOMP 2023 organizing committee for their help in handling the logistics of this
event.
We hope that ADVCOMP 2023 was a successful international forum for the exchange of ideas and
results between academia and industry and for the promotion of progress related to advanced
engineering computing and applications in sciences.
ADVCOMP 2023 Chairs
ADVCOMP 2023 Steering Committee
Dean Vucinic, Vrije Universiteit Brussel (VUB), Belgium, FERIT, Croatia
Juha Röning, University of Oulu, Finland
Hans-Joachim Bungartz, TUM, Germany
Marcin Hojny, AGH University of Science and Technology, Poland
Andreas Rausch, TU Clausthal, Clausthal-Zellerfeld, Germany
Alice E. Koniges, University of Hawai‘i at Mānoa, USA
ADVCOMP 2023 Publicity Chairs
Laura Garcia, Universitat Politecnica de Valencia, Spain
Lorena Parra Boronat, Universitat Politecnica de Valencia, Spain

ADVCOMP 2023
Committee
ADVCOMP 2023 Steering Committee
Dean Vucinic, Vrije Universiteit Brussel (VUB), Belgium, FERIT, Croatia
Juha Röning, University of Oulu, Finland
Hans-Joachim Bungartz, TUM, Germany
Marcin Hojny, AGH University of Science and Technology, Poland
Andreas Rausch, TU Clausthal, Clausthal-Zellerfeld, Germany
Alice E. Koniges, University of Hawai‘i at Mānoa, USA
ADVCOMP 2023 Publicity Chairs
Laura Garcia, Universitat Politecnica de Valencia, Spain
Lorena Parra Boronat, Universitat Politecnica de Valencia, Spain
ADVCOMP 2023 Technical Program Committee
Waleed H. Abdulla, University of Auckland, New Zealand
José Abellán, Catholic University of Murcia, Spain
Mohamed Riduan Abid, Alakhawayn University, Morocco
Rashmi Agrawal, Manav Rachna International Institute of Research and Studies, India
Francisco Airton Silva, Federal University of Piauí, Brazil
M. Azeem Akbar, Nanjing University of Aeronautics and Astronautics, China
Haifa Alharthi, Saudi Electronic University, Saudi Arabia
Sónia Maria Almeida da Luz, Polytechnic Institute of Leiria - School of Technology and Management,
Portugal
Madyan Alsenwi, Kyung Hee University, Global Campus, South Korea
Mohamed E. Aly, California State Polytechnic University, Pomona, USA
Daniel Andresen, Kansas State University, USA
Anindya Das Antar, University of Michigan, USA
Alberto Antonietti, Politecnico di Milano / University of Pavia, Italy
Mansur Arief, Carnegie Mellon University, Pittsburgh, USA
Abhinav Arora, Meta Platforms, USA
Ehsan Atoofian, Lakehead University, Canada
Vadim Azhmyakov, Universidad Central, Bogota, Republic of Colombia
Carlos Becker Westphall, University of Santa Catarina, Brazil
Raoudha Ben Djemaa, ISITCOM | University of Sousse, Tunisia
Peter Bentley, University College London, UK
Sergiy Bogomolov, Newcastle University, UK
Alessandro Borri, CNR-IASI Biomathematics Laboratory, Rome, Italy
David Bouck-Standen, Kingsbridge Research Center, UK
Sofiane Bououden, University Abbes Laghrour Khenchela, Algeria
Hans-Joachim Bungartz, TUM, Germany
Xiao-Chuan Cai, University of Colorado Boulder, USA
Jadson Castro Gertrudes, Federal University of Ouro Preto, Brazil

Graziana Cavone, Polytechnic of Bari, Italy
Mete Celik, Erciyes University, Turkey
Jieyang Chen, Oak Ridge National Laboratory, USA
Jinyuan Chen, Louisiana Tech University, USA
Vassilios V. Dimakopoulos, University of Ioannina, Greece
Inês Domingues, IPO Porto Research Centre (CI-IPOP), Portugal
Maha Elarbi, University of Tunis, Tunisia
Javier Fabra, Universidad de Zaragoza, Spain
Akemi Galvez, University of Cantabria, Spain / Toho University, Japan
Tong Geng, Boston University, USA
Jing Gong, KTH Royal Institute of Technology, Sweden
Teofilo Gonzalez, UC Santa Barbara, USA
Maki Habib, American University in Cairo, Egypt
Yang He, University of Technology Sydney, Australia
Mohd Helmy Abd Wahab, Universiti Tun Hussein Onn Malaysia, Malaysia
Marcin Hojny, AGH University of Science and Technology, Poland
Wladyslaw Homenda, Warsaw University of Technology, Poland
Tzung-Pei Hong, National University of Kaohsiung, Taiwan
Mehdi Hosseinzadeh, Washington University in St. Louis, USA
Paul Humphreys, Ulster University | Ulster University Business School, UK
Andres Iglesias, University of Cantabria, Spain / Toho University, Japan
Joanna Isabelle Olszewska, University of West Scotland, UK
Hiroshi Ishikawa, Tokyo Metropolitan University, Japan
Félix J. García Clemente, University of Murcia, Spain
Rishabh Joshi, Google Research - Brain Team, USA
Attila Kertesz, University of Szeged, Hungary
Zaheer Khan, University of the West of England, UK
Alice E. Koniges, University of Hawai‘i at Mānoa, USA
Seyong Lee, Oak Ridge National Laboratory, USA
Maurizio Leotta, University of Genova, Italy
Clement Leung, Chinese University of Hong Kong, Shenzhen, China
Yiu-Wing Leung, Hong Kong Baptist University, Hong Kong
Yiheng Liang, Bridgewater State University, USA
Stephane Maag, Telecom SudParis, France
Elbert E. N. Macau, Federal University of Sao Paulo - UNIFESP at Sao Jose dos Campos, Brazil
Rafael Magdalena Benedicto, University of Valencia, Spain
Marcin Markowski, Wroclaw University of Science and Technology, Poland
Mirko Marras, University of Cagliari, Italy
Yoan Martinez Lopez, Camagüey University, Cuba
René Meier, Hochschule Luzern, Switzerland
Mohamed Wiem Mkaouer, Rochester Institute of Technology, USA
Sébastien Monnet, Savoie Mont Blanc University (USMB), France
Shana Moothedath, University of Washington, Seattle, USA
Laurent Nana, University of Brest, France
Ehsan Nekouei, City University of Hong Kong, Hong Kong
Kaiming Ouyang, Nvidia, USA
Marcin Paprzycki, Systems Research Institute | Polish Academy of Sciences, Poland
Prantosh Kumar Paul, Raiganj University, India

Damien Pellier, Université Grenoble Alpes, France
Sonia Pérez-Díaz, University of Alcalá, Spain
Antonio Petitti, Institute of Intelligent Industrial Systems and Technologies for Advanced Manufacturing
(STIIMA) - National Research Council of Italy (CNR) , Italy
Tamas Pflanzner, University of Szeged, Hungary
Agostino Poggi, Università degli Studi di Parma, Italy
Evgeny Pyshkin, University of Aizu, Japan
Andreas Rausch, Technische Universität Clausthal, Germany
Michele Roccotelli, Politecnico di Bari, Italy
Ivan Rodero, Rutgers University, USA
Juha Röning, University of Oulu, Finland
Diego P. Ruiz, University of Granada, Spain
Bibhudatta Sahoo, National Institute of Technology, Rourkela, India
Julio Sahuquillo, Universitat Politècnica de València, Spain
Subhash Saini, NASA, USA
Aadesh Salecha, University of Minnesota, USA
Shailaja Sampat, Arizona State University, USA
Hamed Sarvari, George Mason University, USA
Alireza Shahrabi, Glasgow Caledonian University, Scotland, UK
Justin Shi, Temple University, USA
Piotr Sowiński, Systems Research Institute, Polish Academy of Sciences, Poland
Sudarshan Srinivasan, Oak Ridge National Laboratory, USA
Mohammed Tanash, Kansas State University, USA
Yeming Tang, Pennsylvania State University, USA
Costas Vassilakis, University of the Peloponnese, Greece
Bhavan Vasu, Oregon State University, USA
Flavien Vernier, LISTIC – Savoie University, France
Juan Vicente Capella Hernández, Universitat Politècnica de València, Spain
Dean Vucinic, Vrije Universiteit Brussel (VUB), Belgium / FERIT, Croatia
Guangjing Wang, Michigan State University, USA
Hanrui Wang, Massachusetts Institute of Technology, USA
Lei Wang, University of Connecticut, USA
Shi Wenxuan, Nankai University, China
Adriano V. Werhli, Universidade Federal do Rio Grande - FURG, Brazil
Gabriel Wittum, Goethe University Frankfurt, Germany
Zongshen Wu, University of Wisconsin, Madison, USA
Mudasser F. Wyne, National University, USA
Cong-Cong Xing, Nicholls State University, USA
Feng Yan, University of Nevada, Reno, USA
Limin Yang, University of Illinois at Urbana-Champaign, USA
Carolina Yukari Veludo Watanabe, Federal Unversity of Rondônia, Brazil
Michael Zapf, Technische Hochschule Nürnberg Georg Simon Ohm (University of Applied Sciences
Nuremberg), Germany
Vesna Zeljkovic, Lincoln University, USA
Ruochen Zeng, NXP Semiconductors, USA
Penghui Zhang, Arizona State University, USA
Qian Zhang, Liverpool John Moores University, UK

Copyright Information
For your reference, this is the text governing the copyright release for material published by IARIA.
The copyright release is a transfer of publication rights, which allows IARIA and its partners to drive the
dissemination of the published material. This allows IARIA to give articles increased visibility via
distribution, inclusion in libraries, and arrangements for submission to indexes.
I, the undersigned, declare that the article is original, and that I represent the authors of this article in
the copyright release matters. If this work has been done as work-for-hire, I have obtained all necessary
clearances to execute a copyright release. I hereby irrevocably transfer exclusive copyright for this
material to IARIA. I give IARIA permission or reproduce the work in any media format such as, but not
limited to, print, digital, or electronic. I give IARIA permission to distribute the materials without
restriction to any institutions or individuals. I give IARIA permission to submit the work for inclusion in
article repositories as IARIA sees fit.
I, the undersigned, declare that to the best of my knowledge, the article is does not contain libelous or
otherwise unlawful contents or invading the right of privacy or infringing on a proprietary right.
Following the copyright release, any circulated version of the article must bear the copyright notice and
any header and footer information that IARIA applies to the published article.
IARIA grants royalty-free permission to the authors to disseminate the work, under the above
provisions, for any academic, commercial, or industrial use. IARIA grants royalty-free permission to any
individuals or institutions to make the article available electronically, online, or in print.
IARIA acknowledges that rights to any algorithm, process, procedure, apparatus, or articles of
manufacture remain with the authors and their employers.
I, the undersigned, understand that IARIA will not be liable, in contract, tort (including, without
limitation, negligence), pre-contract or other representations (other than fraudulent
misrepresentations) or otherwise in connection with the publication of my work.
Exception to the above is made for work-for-hire performed while employed by the government. In that
case, copyright to the material remains with the said government. The rightful owners (authors and
government entity) grant unlimited and unrestricted permission to IARIA, IARIA's contractors, and
IARIA's partners to further distribute the work.

Table of Contents
Optimized Hardware Configuration for High Performance Computing Systems
Scott Hutchison, Daniel Andresen, William Hsu, Mitchell Neilsen, and Benjamin Parsons
1
Multifractal Analysis of Thermal Images of Electronic Devices in Different Colour Profiles
Marina Diaz-Jimenez, Juan Carlos de la Torre, Javier Jareno-Dorado, Patricia Ruiz, Bernabe Dorronsoro, and
Pablo Pavon-Dominguez
7
A Graphical Analysis of the Multimodal Public Transport Network – The Bay of Cadiz
Patricia Camacho Magrinan, Pablo Pavon Dominguez, and Patricia Ruiz
14
A Survey of Recent Applications of the PISALE Code and PDE Framework
Alice Koniges, David Eder, Jonghyun Lee, Aaron Fisher, Yuriy Mileyko, Monique Chyba, Jack McKee, Young-Ho
Seo, Peter Yip, Thomas Schwartzentruber, Claudia Parisuana, and Siegfried Glenzer
20
Powered by TCPDF (www.tcpdf.org)

Optimized Hardware Configuration for High
Performance Computing Systems
Scott Hutchison
Department of Computer Science
Kansas State University
Manhattan, KS 66505, USA
email: scotthutch@ksu.edu
Daniel Andresen
Department of Computer Science
Kansas State University
Manhattan, KS 66505, USA
email: dan@ksu.edu
William Hsu
Department of Computer Science
Kansas State University
Manhattan, KS 66505, USA
email: bhsu@ksu.edu
Mitchell Neilsen
Department of Computer Science
Kansas State University
Manhattan, KS 66505, USA
email: neilsen@ksu.edu
Benjamin Parsons
High Performance Computing Modernization Program
Engineering Research and Development Center
Vicksburg, MS 39180, USA
email: ben.s.parsons@erdc.dren.mil
Abstract—When faced with upgrading or replacing High Per-
formance Computing or High Throughput Computing systems,
system administrators can be overwhelmed by hardware options.
Servers come with various configurations of memory, processors,
and hardware accelerators, like graphics cards. Differing server
capabilities greatly affect their performance and their resulting
cost. For a fixed budget, it is often difficult to determine
what server package composition will maximize the performance
of these systems once they are purchased and installed. This
research uses simulation to evaluate the performance of different
server packages on a set of jobs, and then trains a machine
learning model to predict the performance of un-simulated server
package compositions. In addition to being orders of magnitude
faster than conducting simulations, this model is used to power
a recommender system that provides a precision@50 of 92%.
Index
Terms—HPC;
Procurement
Optimization;
Recom-
mender system; XGBoost.
I. INTRODUCTION
When faced with upgrading or expanding a High Per-
formance Computing (HPC) or High Throughput Comput-
ing (HTC) system, administrators of these systems can be
overwhelmed by options. It is a challenging task to get
the best performance for a fixed budget. Server capabilities
(i.e., number and types of processors, amount of memory,
and number and types of Graphics Processing Units (GPU)
or other hardware accelerators) greatly affect their costs,
and for a fixed spending ceiling, it is desirable to get the
“best bang for your buck.” For an HPC system, an optimal
server package composition is dictated by its typical use.
For instance, if many users rely upon a GPU-accelerated
application or library, a higher GPU count may be desirable,
even if this means fewer servers can be purchased. With many
factors to consider, HPC administrators often rely upon their
preferences, intuition, and experience to inform procurement
decisions. This research uses historical job data from an HPC
system, a discrete event simulator, and a machine learning
model to power a recommender system, which can help inform
a hardware procurement decision. These techniques provide
additional information to HPC system administrators about
which set of budget-constrained hardware minimizes wait
time for users’ jobs, and provides quantifiable support for
procurement decisions when upgrading or expanding existing
HPC infrastructure. The contributions of this work can be
summarized as follows:
1) A data set consisting of roughly 12,700 HPC scheduling
simulations, each with a different HPC server set
2) An optimized XGBoost regression model for predicting
average wait time when given a composition of servers
3) A recommender system with precision@50=92%, which
can inform hardware procurement decisions
This paper is laid out as follows: Section II provides
additional background on the problem and describes similar
work done by others, Section III provides the methodology
and some implementation details, Section IV provides details
of formulas for metric calculations, Section V provides the
results of the experiments, and Sections VI provides our final
conclusions.
II. BACKGROUND AND RELATED WORKS
The Open Science Grid (OSG) [1] [2] is a worldwide
collaboration that offers distributed computing for scientific
research. In the central United States, one of the organiza-
tions contributing resources to the OSG is the Great Plains
Augmented Regional Gateway to the Open Science Grid (GP-
ARGO) [3]. In part, GP-ARGO receives funding through
governmental grants. These grants are often used to procure
new equipment to expand or improve the capabilities of GP-
ARGO’s participating organizations. Consequentially, there
is a fixed budget ceiling for HPC equipment procurement,
and the administrator’s goal is to purchase new equipment
that will maximize computational performance for our typical
applications while ensuring costs remain under the fixed grant
budget. The research question for this work is as follows: for a
1
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

planned HPC expansion, can experimental simulation provide
an optimal set of hardware under a given budget that will
minimize job wait time?
The challenge of optimal hardware procurement is not
exclusive to our organization. Similar work was done by Evans
et al. [4]. They collected benchmarks for various software
applications on different hardware to optimize the ratio of
Central Processing Unit (CPU) and GPU architectures for HPC
jobs. Their work is similar to ours, but we took a different
approach by using a scheduling simulator to evaluate the
performance of a set of jobs that were actually submitted to an
HPC system. We are solving a very similar problem as Evans
et al., but using a different approach to arrive at an optimal
hardware configuration.
Other researchers have attempted to optimize for a partic-
ular application, such as the work Kutzner et al. [5] did to
improve the utilization of GPU nodes when using GROMACs.
Although these techniques are not without their merits for HPC
systems that run a large number of homogeneous applications,
users of the GP-ARGO HPC systems run a wide variety of jobs
and applications. A more broad scheduler-based optimization
was more appropriate for our application.
Various public HPC workloads exist [6], and have been used
by HPC researchers in the past. However, as we are attempting
to identify and evaluate new hardware for a specific HPC
system, log data from that HPC system was utilized as the
workload for this research.
Different scheduling applications like SLURM, HTCondor,
or PBS, operate on HPC systems and perform the function
of assigning HPC resources to jobs. This job to machine
assignment task is as an extension of the online bin packing
problem [7]. For the bin packing problem, the goal is to pack a
sequence of items with sizes between 0 and 1 into as few bins
of size 1 as possible. Each job specifies the resources requested
(the object sizes), and each HPC machine has a certain amount
of available resources (the bins with their respective sizes). The
scheduler is given the task to meet job requirements by assign-
ing them to HPC nodes (pack the objects into the available
bins) as efficiently as possible. This is an online problem as
new jobs are submitted over time to the scheduler. The best
fit bin packing (BFBP) algorithm has been shown by Dosa
and Sgall [8] to use at most ⌊1.7OPT⌋ bins, ensuring this
algorithm will provide a reasonably close to optimal average
wait time when it is used as an HPC job scheduling algorithm.
Since scheduling algorithms vary between applications, most
being highly customizable, and others being proprietary, a
discrete event simulator utilizing the BFBP algorithm served
as a stand-in for our scheduling application in an attempt to
make it more universally applicable. The BFBP scheduling
algorithm is described in Figure 1.
Although various HPC simulators have been used for similar
research, such as SimGrid [9], GridSim [10], or Alea [11], this
experiment needed a simple discrete event simulator using the
BFBP scheduler. The simulators mentioned above were either
deemed overly complex for our purposes, or they failed to
allow for the three limiting resources (memory, CPUs, and
Algorithm 1 Best Fit Bin Packing Scheduling
1: while The simulation is incomplete do
2:
if Some job in the queue can be executed on some
machine then
3:
Find the (job, machine) pairing which results in
the fewest remaining resources for some machine. Begin
executing that job on that machine.
4:
else
5:
Advance simulation time until a new job is sub-
mitted or a running job ends, whichever is sooner.
6:
Queue submitted jobs and stop ending jobs.
7:
end if
8: end while
Fig. 1. Pseudocode for the best fit bin packing algorithm
GPUs) we were interested in investigating. An HPC scheduler
simulator was also considered, such as the SLURM simulator
developed at SUNY University in Buffalo [12]. Although this
option was investigated further, scaling a job’s actual duration
from the log data to the new machine once it is assigned
to a machine was challenging. As such, a custom discrete
event simulator was developed and utilized for this research.
The simulator allows for three resource constraints in each
machine: memory, CPUs, and GPUs. It is fairly lightweight,
fast, and easy to understand.
A significant consideration when evaluating new server
hardware is the performance increase newer technology or
architectures can provide. Using log data, we know how long
a job took on a machine with known hardware. Since the
specifications for the new hardware under consideration are
also known, the actual duration of the jobs from the historic
log data was scaled using base performance of the processor
as reported by SPEC CPU2017 benchmark, second quarter,
2023 [13].
Knowing how a particular job performed on one set of
hardware and estimating how it will perform on some other
hypothetical set of hardware is challenging. Sharkawi et al.
[14] successfully used a similar SPEC benchmark to estimate
the performance projections of HPC applications. Other re-
searchers, like Wang et al. [15] have pointed out that these
benchmarks fail to account for all the variables affecting job
resource utilization and should be avoided. Although CPU
performance is not the only factor by which we could have
scaled job duration, and perhaps it is not the best factor by
which to scale, it worked well for our purposes. The discrete
event simulator was implemented such that the scaling factor
could be easily changed if other researchers should find a
different factor more relevant to their situation.
Various metrics are typically used when evaluating the
performance of HPC scheduling algorithms. Some of these are
average wait time, HPC utilization, average turnaround time,
makespan, throughput, etc. Which metric is used depends on
2
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

the application and function of the HPC system, and different
organizations may value one metric over another. The metric
used for this research was average wait time, or the average
number of seconds each job spent waiting in the job queue
for execution on HPC resources. We presume that the same
techniques could be applied by other researchers using a
different metric, should they prefer a different one.
Recommender systems power a variety of applications like
search engines and music recommendation systems. First, the
hits for the system must be defined. Hits are the elements from
the data set that are relevant to the user’s search. Next, the user
specifies the number of recommendations, k, that they would
like to receive. If the recommender system is precise, a large
portion of the k items returned will be hits.
III. METHODOLOGY
The general plan for optimizing a hardware package for our
fixed budget can be summarized as follows:
1) Receive vendor quotes with potential server options.
2) Generate potential server combinations to purchase un-
der the specified budget which meet our procurement
requirements.
3) Identify a typical set of jobs representing the workloads
typically submitted to our HPC system.
4) Conduct simulations using a subset of the server pack-
ages to schedule the representative job set and compute
metrics to determine their performances.
5) Use machine learning to train and refine a model that
can predict the performance of un-simulated server
combinations.
6) Develop a recommender system using the machine
learning model.
7) Subjectively evaluate the recommended server packages
and make a more informed procurement decision.
A. Generate Server Options
To begin, we received several vendor quotes specifying the
costs and capabilities of 21 potential servers to purchase. When
considering upgrade options, we typically separate servers
into one of three categories: compute nodes, big memory
nodes, or GPU nodes. A compute node typically has a large
number of processor cores, a moderate amount of memory,
and no GPU. A big memory node will have a large amount of
memory with a moderate amount of CPU cores and no GPU.
A GPU node is any node which has a GPU. Table I lays out
the options we received from several different vendors. The
procurement budget was fixed at $1 million, and all possible
server combinations were generated in the following way:
• Separate servers into three categories: compute nodes, big
memory nodes, and GPU nodes.
• Choose all combinations of one node from each category.
• Determine all quantities of the three node types under a
given budget such that there is at least one GPU node
and there is not enough funding remaining to purchase
another node.
TABLE I. SERVER CAPABILITIES AND COSTS UNDER INVESTIGATION
Node type
Distinct
nodes
consid-
ered
Memory
range
per
node
CPUs
range
per
node
GPUs
per
node
Cost
range
per
node
Compute
4
256-512
Gb
24-64
cores
0 GPUs
$6k-
$10k
Big memory
2
1024
Gb
24-64
cores
0 GPUs
$11k-
$13k
GPU
15
256-
1024
Gb
24-64
cores
1-8
GPUs
$14k-
$100k
FIG. 2. THE NUMBER OF JOBS SUBMITTED OVER TIME FOR THE SELECTED
DAY
In our selected job set, many jobs requested GPUs as a
resource. These jobs would automatically fail if at least one
GPU node were not included in a potential server package.
Roughly 127,000 different server combinations met these
requirements. Table II provides an illustrative example of how
the server combinations were generated. Many server options
and packages were omitted from the table for the sake of
brevity.
B. Identify a Representative Set of Jobs
One typical day’s worth of submitted jobs (roughly 16,000
jobs) was subjectively pulled from the log data of the local
HPC system. As with most HPC systems, jobs were submitted
in a bursty manner, and variety of resources were requested.
Figure 2 and Table III display some descriptive statistics and
information about the jobs used by this research.
C. Job Duration Scaling
The submitted jobs were scaled using the base performance
of the processor on the SPEC CPU2017 benchmark suite. The
requested duration was not modified, but the actual duration
of each job was calculated using the following formula:
New duration = logged duration∗logged processor performance
new processor performance
D. Discrete Event Simulator
A discrete event simulator was implemented in Python that
provides the following functionality:
3
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

TABLE II. GENERATED SERVER COMBINATIONS
ComputeNode1, $6,960 ea.
BigMemNode1, $11,112 ea.
GPUNode1, $14,730 ea.
. . .
Package Cost
Funds Remaining
141
0
1
. . .
$996,090
$3,910
139
1
1
. . .
$993,282
$6,718
138
2
1
. . .
$997,434
$2,566
...
...
...
...
...
...
0
1
67
. . .
$998,022
$1,978
TABLE III. DESCRIPTIVE STATISTICS FOR THE POOL OF SELECTED JOBS
Requested
Mem (in
Gb)
Requested
CPUs
Requested
GPUs
Requested
Duration
(in
hours)
Actual
Duration
(in
hours)
Mean
5.12
4.75
0.002
2.82
2.27
Std Dev.
16.73
3.33
0.055
1.02
13.67
Min
1
1
0
0
0
Max
800
64
4
11.20
11.20
• A global clock to keep track of simulation time.
• Several queues, priority queues, or lists to track jobs
as they progress through the execution process: future
jobs, queued jobs, running jobs, completed jobs, and
unrunnable jobs.
• Jobs and machines are specified using comma separated
value (csv) files, which is loaded prior to the simulation.
• Machines have three limiting resources: available mem-
ory, CPUs, and GPUs.
• Jobs are specified with the following attributes: submit
time, actual duration, and requested duration, memory,
CPUs, and GPUs. Jobs track their start time and end
time as the simulation progresses to allow for metric
calculation.
• Job end time is set when the job starts running as the job
start time plus the job actual duration.
• When a job starts running on a machine, that machine’s
available resources are decremented by the resources re-
quested by the job. Conversely, when a job completes, the
machine executing it has its available resources increased
by the amount requested by the ending job.
• Jobs with a submit time greater than the current global
clock reside in the future jobs priority queue.
• Jobs with a submit time less than or equal to the current
global clock, but not yet assigned to a machine, reside in
the job queue.
• Jobs that have begun their execution and have an ending
time less than the current global clock, reside in the
running jobs priority queue.
• Jobs with an ending time less than or equal to the current
global clock reside in the completed jobs list.
• If no node in the cluster has adequate resources to run a
particular job, that job is moved to the unrunnable jobs
list.
• In the event that no queued jobs can run on available
resources, the simulation time “fast forwards” to the next
event: either job submission or job ending.
• Jobs in the job queue are run as soon as there are available
resources and are chosen using the best fit bin packing
scheduling algorithm described in Algorithm 1.
• Actual job duration from logged job data can be scaled to
allow for hardware improvement with newer hardware.
E. Machine Learning
Although each simulation completed fairly quickly, requir-
ing no more than 30 minutes each, this particular combi-
nation of server quotes yielded roughly 127,000 combina-
tions that need to be evaluated. To reduce the computational
requirement, every tenth line from the file with the server
combinations was sampled, and roughly 12,700 simulations
for these server packages were completed in parallel us-
ing HPC resources. By sampling from the generated server
packages uniformly, various quantities of each server under
consideration were included in the simulated data. Each server
package was summarized into the package total memory, total
CPUs, and total GPUs, by summing the resources of every
machine comprising the package. The average wait time for
the simulation served as the label for each package. The data
was split into 90% training and 10% test data, and an XGBoost
[16] regression model was trained using training data. The
regression model was evaluated using Root Mean Squared
Error (RMSE) on the test data. An accurate regression model
enabled the prediction of the average wait time for unsimulated
server combinations and saved countless hours of additional
simulation.
F. Recommender System
In our case, a hit was defined as a server combination
with an average wait time in the lowest 5% of simulated
combinations (or 632 hits out of the ∼12,700 simulated server
combinations). The value of k was varied to evaluate the per-
formance of the recommender system. Then, once confidence
was gained that our recommender system was functioning
properly, it was used to recommend systems from the entire
server combination pool of 127,000 server combinations. The
recommendations were summarized and evaluated subjectively
before arriving at a final procurement decision.
G. Simplifying Assumptions
The current nodes comprising the HPC system were not
added to the set of nodes simulating the selected jobs. The
benefit current nodes would provide to the new servers under
investigation would be common to all.
Any additional equipment required to install and operate
the new servers (e.g., networking hardware, additional cooling
equipment, server racks, power infrastructure, etc.) were not
4
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

deducted from the total procurement budget. It was thought
that these costs would be a relatively fixed regardless of the
server package chosen. The same analysis described in this
research could be done by reducing the total budget by the
cost of additional hardware and then completing the analysis
with a reduced budget.
IV. EVALUATION
Pearson’s Correlation Coefficient [17] determined the extent
of the correlation between the total memory, CPUs, and GPUs
of a package and the average wait time. This coefficient
provides a value between -1 and 1, where values closer to -1
or 1 indicate that the feature and the label are more strongly
correlated. A coefficient of 0 indicates no correlation.
Wait time was calculated by analyzing the completed jobs
output from each simulation. The wait time for each job was
the number of seconds from the time the job was submitted
until it began. For N jobs, the average wait time was calculated
as follows:
AvgWaitTime = ΣN
i=0(Start Timei − Submit Timei)
N
Root Mean Squared Error was utilized for regression model
evaluation calculated according to the following formula:
RMSE =
r
ΣN
i=0(actual wait timei − predicted wait timei)2
N
The performance of the final recommender system was
evaluated using precision@k, recall@k, and F1@k. In general,
precision@k is the proportion of recommended items in the
top-k set that are relevant, and recall@k is the proportion of
relevant items found in the top-k recommendations. F1@k
is the harmonic mean of precision@k and recall@k, which
simplifies them into a single metric. They were calculated
according to the following formulas:
Precision@k = (# of recommended items @k that are relevant)
(# of recommended items @k)
Recall@k = (# of recommended items @k that are relevant)
(total # of relevant items)
F1@k = (2 ∗ precision@k ∗ recall@k)
(precision@k + recall@k)
V. RESULTS
The correlation of features, the performance of the regres-
sion model and the recommender system, and some analysis
about the recommended server compositions are described
below.
A. Feature Correlation
The correlation between the features and the labels is shown
in Table IV. For this set of jobs, the total CPUs in a server
package were most strongly correlated to the average wait
time. For the chosen jobs, the more CPUs a package had, the
lower its average wait time.
Since we are constrained by our available budget of $1
million, choosing to buy one type of node over another is
a zero-sum game. The more GPU nodes we purchase, and the
more GPUs there are per node, the fewer compute nodes or
big memory nodes we are able to afford. This is indicated by
the positive correlation between GPUs and the average wait
time.
TABLE IV. PEARSON CORRELATION COEFFICIENTS
TotalMem
TotalCPUs
TotalGPUs
AvgWaitTime
TotalMem
1.00
0.14
-0.54
-0.23
TotalCPUs
0.14
1.00
-0.42
-0.70
TotalGPUs
-0.54
-0.42
1.00
0.44
AvgWaitTime
-0.23
-0.70
0.44
1.00
B. Regression Model
The XGBoost regression model had a RMSE = 150.13
seconds, indicating that the total memory, CPUs, and GPU
features made excellent predictors for the average wait time for
these jobs when simulated with the discrete event simulator.
The predicted vs. actual wait time is shown in Figure 3. If the
regression model were perfect, all these points would lie upon
the y = x line, and it is clear that this model does a good job
at predicting the average wait time for a given composition of
servers.
FIG. 3. THE PREDICTED VS. ACTUAL WAIT TIMES SHOWING THE
ACCURACY OF OUR REGRESSION MODEL.
C. Recommender System
The regression model was used to predict the 12,700 labeled
simulations, and their precision@k, recall@k, and F1@k for
various values of k are displayed in Table V. The goal was
to reduce the number of possibilities from roughly 127,000
different possible combinations of servers down to a reason-
able number which could be evaluated by an HPC system
administrator and have a large percentage of the recommended
server combinations be hits (among the best 5% of server
combinations with the lowest average wait times). Although
precision@10 was 100%, it is thought that seeing more server
package options would allow system administrators a wider
variety from which to choose. A system administrator could
easily and quickly review up to 50 recommendations (k = 50),
and more than 46 out of 50 of these recommendations returned
5
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

by this system (92%) would be top performing server combi-
nations, which is excellent. Recall@k when k is less than the
number of total hits (632 hits total) is unfairly penalized, but
the recall@k above 632 is also excellent. When k = 1, 000,
the recall@1000 = 91%, meaning the recommender system
successfully retrieved 91% of the top 5% performing server
packages when returning less than 1% of the 127,000 different
options.
TABLE V. PRECISION@K AND RECALL@K FOR TEST DATA
k value
Precision@k
Recall@k
F1@k
10
1.00
0.02
0.03
50
0.92
0.07
0.13
100
0.81
0.13
0.22
500
0.74
0.59
0.66
632
0.72
0.72
0.72
1000
0.58
0.91
0.71
D. Recommended Compositions
Beyond looking at the individual server compositions rec-
ommended, we wanted to draw some conclusion about the
types and quantity of nodes that the recommender system
returned. The sum of the server quantities for the top 50
recommendations can be found in Table VI. Compute nodes
with the larger number of cores were vastly preferred, and the
recommender system did not recommend spending additional
funds on more memory for the compute nodes. Additionally,
the recommender system preferred the cheaper big memory
node with fewer cores. Finally, for our typical workload,
the recommender system did not recommended purchasing a
large number of GPUs per GPU node, instead recommending
servers with 2 GPUs per server most often. In terms of
our budget breakdown, the recommender system suggests
spending on average 58% of our total budget on compute
nodes, 8% on big memory nodes, and 34% on GPU nodes.
TABLE VI. RECOMMENDATIONS DRAWN FROM MODEL PREDICTED
RESULTS
Node Type
Node Description
Sum of Servers Across
Top 50
Compute Nodes
Cheapest w/ 256Gb
232
Cheapest w/ 512Gb
0
Expensive w/ 256Gb
3,467
Expensive w/ 512Gb
0
Big Memory Nodes
Cheapest w/ 1024Gb
232
Expensive w/ 1024Gb
111
GPU Nodes
2 GPUs in one server
732
4 GPUs in one server
267
VI. CONCLUSIONS
This recommender system is not intended to replace the
expertise of HPC administrators when it comes to decisions
for hardware procurement. It is our hope that this tool can
provide a data-driven technique which will help narrow the
search space with which administrators are confronted when
they make procurement decisions. Returning to the research
question: experimental simulation coupled with a regression
model enabled a recommender system to return server com-
positions under a given budget with low average wait times
with a precision@50 of 92%. Additionally, the discrete event
simulator, job data set, machine learning code, and recom-
mender system code are released under the GPLv3 license
should other researchers find it useful [18].
REFERENCES
[1] R. Pordes et al., “The open science grid,” in J. Phys. Conf. Ser., vol. 78
of 78, p. 012057, 2007.
[2] I. Sfiligoi et al., “The pilot way to grid resources using glideinwms,”
in 2009 WRI World Congress on Computer Science and Information
Engineering, vol. 2 of 2, pp. 428–432, 2009.
[3] “The great plains augmented regional gateway to the open science grid.”
https://gp-argo.greatplains.net/. Accessed 2023-01-18.
[4] R. T. Evans et al., “Optimizing gpu-enhanced hpc system and cloud
procurements for scientific workloads,” in International Conference on
High Performance Computing, pp. 313–331, Springer, 2021.
[5] C. Kutzner et al., “More bang for your buck: Improved use of gpu nodes
for gromacs 2018,” Journal of computational chemistry, vol. 40, no. 27,
pp. 2418–2431, 2019.
[6] D. G. Feitelson, D. Tsafrir, and D. Krakov, “Experience with using
the parallel workloads archive,” Journal of Parallel and Distributed
Computing, vol. 74, no. 10, pp. 2967–2982, 2014.
[7] S. Martello and P. Toth, Knapsack problems: algorithms and computer
implementations. John Wiley & Sons, Inc., 1990.
[8] G. D´osa and J. Sgall, “Optimal analysis of best fit bin packing,” in Au-
tomata, Languages, and Programming: 41st International Colloquium,
ICALP 2014, Copenhagen, Denmark, July 8-11, 2014, Proceedings, Part
I 41, pp. 429–441, Springer, 2014.
[9] H. Casanova, A. Giersch, A. Legrand, M. Quinson, and F. Suter,
“Versatile, scalable, and accurate simulation of distributed applications
and platforms,” Journal of Parallel and Distributed Computing, vol. 74,
pp. 2899–2917, June 2014.
[10] R. Buyya and M. Murshed, “Gridsim: A toolkit for the modeling and
simulation of distributed resource management and scheduling for grid
computing,” Concurrency and computation: practice and experience,
vol. 14, no. 13-15, pp. 1175–1220, 2002.
[11] D. Klus´aˇcek, M. Soysal, and F. Suter, “Alea–complex job scheduling
simulator,” in Parallel Processing and Applied Mathematics: 13th Inter-
national Conference, PPAM 2019, Bialystok, Poland, September 8–11,
2019, Revised Selected Papers, Part II 13, pp. 217–229, Springer, 2020.
[12] N. A. Simakov et al., “Slurm simulator: Improving slurm scheduler
performance on large hpc systems by utilization of multiple controllers
and node sharing,” in Proceedings of the Practice and Experience on
Advanced Research Computing, pp. 1–8, 2018.
[13] “Second
quarter
2023
spec
cpu2017
results,”
2023.
https://www.spec.org/cpu2017/results/res2023q2,
Accessed
on
June
14, 2023.
[14] S. Sharkawi et al., “Performance projection of hpc applications using
spec cfp2006 benchmarks,” in 2009 IEEE International Symposium on
Parallel & Distributed Processing, pp. 1–12, IEEE, 2009.
[15] Y. Wang, V. Lee, G.-Y. Wei, and D. Brooks, “Predicting new workload
or cpu performance by analyzing public datasets,” ACM Transactions on
Architecture and Code Optimization (TACO), vol. 15, no. 4, pp. 1–21,
2019.
[16] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”
in Proceedings of the 22nd ACM sigkdd international conference on
knowledge discovery and data mining, pp. 785–794, 2016.
[17] K. Pearson, “Note on regression and inheritance in the case of two
parents,” Proceedings of the royal society of London, vol. 58, no. 347-
352, pp. 240–242, 1895.
[18] S.
Hutchison,
“Optimal-hardware-procurement-for-a-hpc-expansion.”
https://github.com/shutchison/Optimal-Hardware-Procurement-for-a-
HPC-Expansion, Accessed on September 19, 2023.
6
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

Multifractal Analysis of Thermal Images of Electronic Devices in Different Colour
Profiles
Marina D´ıaz-Jim´enez
Dept. Computer Science Eng.
University of C´adiz, Spain
email: marina.diaz@uca.es
Juan Carlos de la Torre
Dept. Computer Science Eng.
University of C´adiz, Spain
email: juan.detorre@uca.es
Javier Jare˜no-Dorado
Dept. Computer Science Eng.
University of C´adiz, Spain
email: javier.jareno@uca.es
Patricia Ruiz
Dept. Mechanical Eng. and Ind. Design
University of C´adiz, Spain
University of Sydney, Australia
email: patricia.ruiz@uca.es
Bernab´e Dorronsoro
Dept. Computer Science Eng.
University of C´adiz, Spain
University of Sydney, Australia
email: bernabe.dorronsoro@uca.es
Pablo Pav´on-Dom´ınguez
Dept. Mechanical Eng. and Ind. Design
University of C´adiz, Spain
email: pablo.pavon@uca.es
Abstract—Nowadays, there are many methods for analysing
images. Among them, a multifractal approach is able to char-
acterise images in terms of the complexity of patterns they
contain. However, multifractal methods are very sensitive to small
changes in colour images. This study presents a preliminary
study for the accurate characterisation of electronic devices in
terms of heat dissipation using multifractal analysis. Specifically,
a novel methodology using multifractal approach is proposed to
characterise images in different colour formats: Grey-Scale, Red-
Green-Blue (RGB), Cyan-Magenta-Yellow-Key (CMYK), Hue-
Saturation-Value (HSV) and Hue-Saturation-Intensity (HSI), and
the results are compared. For this purpose, thermal images of
a Pi3 Raspbian Desktop board are analysed with the aim of
finding out the results of the multifractal analysis on different
areas of the board. Multifractal analysis is carried out through
the box-counting method and the method of moments, since
they allow to extract the main multifractal parameters, such
as, the generalised dimensions function, D(q) and the degree
of multifractality, ∆D(q). The results obtained show that both
scaling properties and multifractal parameters vary according
to the colour format, i.e., the same image exhibits different
multifractal properties depending on the colour format.
Keywords - Multifractal analysis; thermal images; colour
formats; box-counting; colour images.
I. INTRODUCTION
Fractal shapes are mainly characterised by self-similar prop-
erties and a fractal dimension (Df), which is dependent of
the scaling measure [1]. However, in some structures, due
to geometrical complexity, several fractal structures overlap
simultaneously and are considered as multifractal objects [2].
These multifractal objects are completely characterised by a
function, known as a generalised dimensions function (D(q))
[3]. The box-counting method and the method of moments [4]
allow to carry out a simple and robust multifractal analysis on
images. These methods are able to estimate the generalised
fractal dimensions, D(q), and describe complex structures
with sets of regions exhibiting different fractal properties,
providing a relatively more concrete characterisation.
From a fractal approach, image analysis is typically accom-
plished on binary [5] or RGB images [6] [7]; however, they
are hardly found from a multifractal approach nor in different
colour formats [8] [9]. For this reason, a novel methodology
for analysing images in different colour formats by means of
multifractal analysis is proposed in this work. Specifically, four
well-known formats namely RGB, CMYK, HSV and HSI, are
analysed and compared using the multifractal box-counting
method. The analysis has been carried out on thermal images
of a Pi3 Raspbian Desktop board, in order to make a first
approach towards the characterisation of electronic devices
based on heat dissipation. Additionally, three different areas
of the board are analysed, in order to provide our study with
different cases of study. Results show that the multifractal
analysis varies depending on the colour profile, i.e., the same
image, depending on the colour format, presents different
multifractal properties. This result points out that the image
colour profile may hinder the multifractal properties of the heat
dissipation of the electronic device. Therefore, it is concluded
that in order to carry out a multifractal analysis of thermal
images, the raw file should ideally be considered to avoid
alterations in the results.
The paper is organised as follows. Section II briefly presents
the most relevant works in the state of the art. Section III
introduces the methodology proposed in this work. Thermal
image acquisition and multifractal analysis are described in
sections III-A and III-B, respectively. Results are explained in
Section IV. Finally, Section V concludes the work.
II. RELATED WORKS
Fractal analysis has extensively been used for the analysis
of digital images [5], however, there are few studies of digital
colour images using a more comprehensive approach, the
multifractal analysis. Most of the existing fractal methods for
analysing images are defined for 1D signal or binary images,
extending to Grey-Scale images [5]. In [8], a multifractal
analysis using the box-counting method is applied on RGB
images. Specifically, the authors analyse a three-dimensional
histogram of the image, which contains the R, G and B colour
coordinates. In the same way, in [10] the three-dimensional
7
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

histogram of RGB images is also analysed. However, here,
the authors use the multifractal analysis as an image noise
detection tool. In works such as [9], RGB images are analysed
by decomposing the channels that make up each image. They
propose a methodology similar to the one proposed in this
work. However, they only select one channel from the RGB
image and then binarise it, so the multifractal analysis is
actually conducted on binary images, unlike the proposed
work, where the pixels of the analysed images take values
from 0 to 255. Furthermore, in this work, not only are
RGB images analysed, but also images in different colour
formats (including all the channels that compose each format).
Moreover, [6] highlights the difficulty in estimating the fractal
dimension of RGB colour images. Finally, [7] again analyses
RGB and Grey-Scale images from a fractal approach using
the box-counting method. The authors adopt a methodology
similar to the one followed in this work, since they analyse a
Grey-Scale image as well as each of the channels that make
up the RGB image.
As it can be seen, the number of existing studies on
images considering different colour formats from a multifractal
approach is low. For this reason, the purpose of this work
is to present a new methodology, using multifractal analysis,
to characterise images in different colour formats, in order to
evaluate the influence of the colour format in the results of the
analysis. Specifically, this work is focused on analysing ther-
mal images of an electronic device with the aim of establishing
robust relationships between the multifractal characteristics of
the device and heat dissipation in the future.
III. METHODOLOGY
This section is structured in two subsections. Section III-A
summarises the thermal images under study, as well as the
image capture process and image processing. Then, the mul-
tifractal analysis approach and its application to the study of
thermal images is explained in Section III-B.
A. Thermal images
The images under study correspond to a Pi3 Raspbian
Desktop board, on which the basal consumption was evaluated.
That is, the images are captured during the base consumption
of the Operating System, from an initial state, when the board
is turned off, to a second state, after one hour, when the board
is on. It should be noted that the images were taken by the
authors. The images in the different colour profiles have also
been created by them.
The image capture is performed by a FLIR ETS320 infrared
camera, which allows the visualisation of hot spots and poten-
tial failure points, capturing the emissivity of the object. This
camera model has a wide temperature range [−20◦C, 250◦]
with a measurement accuracy of ±3◦C, allowing to quantify
the heat generation and heat dissipation of small components
(up to 170 µm). It consist of an InfraRed (IR) Sensor, which
offers 76,800 points of non-contact temperature measurement.
Finally, standard radiometric jpeg files are obtained, with 14
bit measurement data.
Image capture is carried out for one hour (3,600 seconds),
from an initial time (t0 = 0s) when the board is off, to a
second time (t1 = 3, 600s), so the board is on for an hour.
During this time period, a total of 31,970 images of the board
were captured, at an average of 8-9 images per second. This
camera model produces rectangular images of 320x240 pixels.
In Figure 1, an example of the captured images is shown.
Figure 1. Example of a 320x240 pixel image of the board.
The subsequent method used for multifractal analysis re-
quires that the analysed images must be square with a side
length power of 2. For this, each of the rectangular images
on the board is cropped into different square sub-images
of 128x128 pixels. Specifically, this study focuses on three
different areas of the board at three different time instants. In
this way, three 128x128 pixels square cutouts were obtained,
each one corresponding to a different time instant, which are
independent of each other (see Figure 2). It should be noted
that each of the images in Figure 2 will be studied, from a
multifractal approach, in different colour profiles: Grey-Scale
(GS), RGB, CMYK, HSV and HSI (see Figure 3).
(a)
(b)
(c)
Figure 2. Images of different areas of the board at different time instants, (a)
Image area 1, (b) Image area 2, (c) Image area 3.
As already mentioned, the images obtained by the infrared
camera represent the emissivity values captured by the IR
sensor (values from 0.0 to 1.0). Subsequently, a normalisation
is carried out, where these values between 0 and 1 become
the values of a Grey-Scale image (values between 0 and 255).
These Grey-Scale images are used as a reference, because
they are considered to represent the most faithful and realistic
situation to the raw files captured by the camera.
B. Multifractal Analysis
Fractal shapes are mainly characterised by the fact that
the measurement of the object is dependent on the scale of
measurement. When this dependence follows a power law, it
8
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

(a)
(b)
(c)
RGB
CMYK
HSV
HSI
GS
Image area 1
Image area 2
Image area 3
Figure 3.
Images corresponding to the three defined areas of the board in
each of the four different colour profiles: RGB, CMYK, HSK, HSI, and the
Grey-Scale image (GS).
is said that the object manifests fractal properties. The main
parameter is known as the fractal dimension (Df), which is
estimated by means of the slope of the linear fit on the log-
log relationship between the scale and the measure [11] [12].
However, due to geometrical complexity, several fractal struc-
tures overlap simultaneously and are considered as multifractal
objects. Multifractal objects are completely characterised by a
function, known as a generalised dimensions function, D(q)
[13] [14]. The main advantage of multifractal analysis is the
description of complex structures exhibiting sets of regions
with different fractal properties, providing a relatively more
specific characterisation [2].
To carry out the multifractal analysis, it is proposed to use
the box-counting method along with the method of moments
[4]. These methods allow a robust estimation of the gener-
alised fractal dimensions, D(q) [13] [14]. In the box-counting
method, an image of side L is completely covered by a set of
square boxes, of side δ, which do not overlap. Values of δ are
obtained as follows: δ = {L/1, L/2, L/4, L/8, ...}. Regarding
the method of moments, parameter q is a remarkable one,
which is a real value varying between −∞ and +∞ [2].
Positive q values magnify regions with high colour intensity
values, while negative q values intensify regions with low
colour intensity values. With these methods, the partition func-
tion, χ(q, δ) = Pn
i=1(ci(δ))q, is obtained, being (ci(δ))q the
mass probability function. It is well known that multifractality
appears when the existence of a power law between χ(q, δ)
vs. δ is trusted. This function allows to differentiate the scale
ranges (δ) where the linear fits will be performed. In this work,
it is taken as a criterion to select the scale range where all the
moments q can be considered, both positive and negative, and
where, in addition, there is linearity. Once the range of scales
is established, we turn to the generalised dimensions function,
D(q) = τ(q)
q−1, where τ(q) is the mass exponent function. τ(q)
is obtained from the slope of the linear fittings performed in
the selected regions of the partition function.
Notice that D(q) exhibits multifractal behaviour when it is
a monotonically decreasing function dependent on q. By con-
trast, it is considered monofractal when D(q) is independent of
q, i.e., it is represented by a horizontal straight line of constant
slope. Another multifractal parameter used in this work is the
degree of multifractality, ∆D(q) = D(qmin)−D(qmax). This
parameter indicates the multifractal strength of the image, i.e.,
the higher the ∆D(q) value, the greater the multifractality of
the image. By contrast, the smaller the ∆D(q) value, the more
monofractal it is.
In this work, we first propose to study the partition func-
tions of each image establishing the scale ranges where the
linear fits will be made to complete the analysis. Note that
there will be a partition function for each of the channels
that make up each colour profile. Subsequently, once the
scale ranges have been established, the values of D(q) are
computed and depicted against q to obtain the generalised
dimensions function. Finally, since Grey-Scale images are
considered as reference images, comparisons are made with
the different colour formats. For this purpose, the difference
between the values of the Grey-Scale dimension functions and
the dimension functions of each channel is evaluated. This
difference allows to determine the channels and scale ranges
that capture the multifractal behaviour of the original image
and, therefore, determine which colour format and/or channel
is more representative of each original image.
IV. RESULTS AND DISCUSSION
In this section, the main findings after applying the box-
counting and the method of moments to the image set are
shown. First, the results of the analysis of the image of
reference, i.e., the Grey-Scale, are presented followed by the
RGB, CMYK, HSV and HSI formats. Finally, a comparison
between the colour formats and the Grey-Scale images is
accomplished.
A. Multifractal Analysis of Grey-Scale images
First of all, it should be noted that the box-couting method
and the method of moments are used to carry out the mul-
tifractal analysis. Both are performed in Matlab software. In
this case, for an image size of 128x128 pixels, the values of
the scale (δ) range from δ = 20 = 1 to δ = 27 = 128. The
values of the moments q are set between -4 and 4.
The partition functions of the three images selected and rep-
resented in Grey-Scale (see Figure 2) present similar shapes.
Figure 4 (a) shows the partition function corresponding to the
image of the area 1 of the board. However, it is representative
of the rest of the Grey-Scale images. As observed, none of
them show crossovers that distinguish scale ranges, i.e., a
single linear region is observed from δ = 1 to δ = 128.
In this way, the generalised dimensions function, D(q), can
be completely reconstructed for all moments q. Linear fits are
performed with a coefficient R2 greater than 0.99 for all cases.
Figure 4 (b) shows the generalised dimensions function, D(q),
9
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

corresponding to the image of the area 1 of the board. Again,
it is representative of the rest of the Grey-Scale images. As
seen, the generalised dimension function is horizontal, thus it
can be considered as a monofractal behaviour.
(a)
(b)
Figure 4.
Graphs corresponding to the image of the area 1, (a) Partition
function, (b) Generalised dimensions function.
Table I shows the results for the D(q) of the three areas of
the board. The values of D(q) exhibit a very small degree of
multifractality, so that, monofractal behaviour is considered in
all three cases. It should be noted that the ∆D(q) of image
area 1 is higher than in the rest of the images. This is due
to the fact that the estimation for negative q values is worse.
However, the rest of the function D(q) is monofractal. Images
of areas 2 and 3 are completely monofractal.
TABLE I. MULTIFRACTAL PARAMETERS OF THE 3 GREY-SCALE IMAGES
Degree of
multifractality, ∆D(q)
Image area 1
0.279
Image area 2
0.059
Image area 3
0.074
B. Multifractal Analysis of RGB images
In Figure 5, the partition functions obtained from the image
of area 1 of the board for the 3 channels of the RGB colour
profile are shown, as an example. In contrast to Grey-Scale
images, the RGB images have different crossovers depending
on the colour channel, so that there are different linear regions.
Therefore, multifractal analysis is performed in different scale
ranges depending on the colour channel. The same behaviour
is observed for the RGB images of areas 2 and 3, i.e.,
depending on the image and the colour channel, partition
functions with linear regions differing from each other are
obtained.
Table II summarises the results obtained from the 3 RGB
images analysed. On the one hand, the values of the degree of
multifractality, ∆D(q), are presented. On the other hand, in
order to detect which channel is capable of exhibiting the same
monofractal behaviour as the reference Grey-Scale image (see
Table I), the quadratic error (P x2 = P(D(q)gs − D(q)c)2)
is estimated, which summarises the differences between the
Grey-Scale and the colour channel. A value of the difference
less than or equal to 0.2 is considered acceptable.
It should be noted that Table II indicates which colour chan-
nels do or do not exhibit scaling behaviour for a given range
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel R
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel G
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel B
(a)
(b)
(c)
Region 1
Region 2
Region 3
Region 1
Region 2
Region 1
Region 2
Figure 5. Partition functions of the RGB image of area 1, (a) Partition function
of channel R, (b) Partition function of channel G, (c) Partition function of
channel B. The crossovers for the linear fits have been made by vertical broken
lines.
of scales. That is, scaling behaviour exists when the values of
D(q) are constant (monofractal) or decrease (multifractal) as
the q moments grow. Increasing D(q) functions implies a non-
scaling behaviour. Therefore, regions which exhibit scaling
behaviour are denoted by the symbol ✓, otherwise they are
marked with −. In Table II, Scaling Behaviour is denoted as
SB.
As it can be seen in Table II, only channel G presents a
scaling behaviour in all the δ regions for the three images.
For channel R, only one δ region of images 1 and 2 show
scaling properties, as opposed to channel B, where only one
δ region of images 1 and 3 do not show it. In terms of the
P x2, regions exhibiting the least difference compared to the
Grey-Scale images are specified next: In G channel, region 2
(δ = 16−128) for the image of area 1; region 1 (δ = 1−128)
for the image of area 2; and region 2 (δ = 16 − 128) for the
image of area 3. In channel B, only in the image of area 2,
specifically in the region 2 (δ = 16 − 128). In channel R, an
adequate approximation to the multifractal behaviour of the
Grey-Scale reference image is not achieved.
C. Multifractal Analysis of CMYK images
The partition functions of the CMYK image 1 area 1 are
shown in Figure 6 as a representative example. As seen,
the CMYK images also show different partition functions
depending on the chosen colour channel. Each of the channels
that make up the CMYK colour profile have different linear
10
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

TABLE II. MULTIFRACTAL PARAMETERS OF THE 3 RGB IMAGES.
COMPARISON WITH THE GREY-SCALE REFERENCE IMAGES
Channel R
SBa
∆D(q)
P x2
∆D(q) Image 1
Reg. 1 δ = 1 − 4
-
-
-
Reg. 2 δ = 8 − 32
-
-
-
Reg. 3 δ = 32 − 128
✓
2.051
6.363
∆D(q) Image 2
Reg. 1 δ = 1 − 8
-
-
-
Reg. 2 δ = 16 − 128
✓
2.400
8.150
∆D(q) Image 3
Reg. 1 δ = 1 − 4
-
-
-
Reg. 2 δ = 8 − 32
-
-
-
Channel G
SBa
∆D(q)
P x2
∆D(q) Image 1
Reg. 1 δ = 4 − 16
✓
2.224
9.179
Reg. 2 δ = 16 − 128
✓
0.588
0.165
∆D(q) Image 2
Reg. 1 δ = 1 − 128
✓
0.072
0.001
∆D(q) Image 3
Reg. 1 δ = 4 − 16
✓
3.112
21.062
Reg. 2 δ = 16 − 128
✓
0.436
0.186
Channel B
SBa
∆D(q)
P x2
∆D(q) Image 1
Reg. 1 δ = 1 − 4
-
-
-
Reg. 2 δ = 8 − 128
✓
1.862
4.937
∆D(q) Image 2
Reg. 1 δ = 4 − 16
✓
1.980
8.251
Reg. 2 δ = 16 − 128
✓
0.315
0.116
∆D(q) Image 3
Reg. 1 δ = 1 − 4
-
-
-
Reg. 2 δ = 8 − 32
✓
0.622
0.774
a SB: Scaling Behaviour
regions between them, so the multifractal analysis is performed
in different scale ranges and, therefore, different results are
obtained for each channel.
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel C
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel M
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel Y
(a)
(b)
(c)
Region 1
Region 2
Region 1
Region 1
Region 2
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel K
(c)
Region 1
Figure 6. Partition functions of the CMYK image area 1, (a) Partition function
of channel C, (b) Partition function of channel M, (c) Partition function of
channel Y, (d) Partition function of channel K.
The same procedure used in the analysis of RGB images is
explained next. First, it is determined which channel exhibits a
scaling behaviour. In this case, with respect to channel C, the
image of area 1 in region 2 (δ = 32 − 128) shows a scaling
behaviour, as well as the image of area 2 for the regions 1
(δ = 1 − 8) and 2 (δ = 16 − 128). On the other hand, no
scaling features are found in the image area 3. In terms of
channel M, only region 2 (δ = 32−128) of the image of area
2 shows a scaling behaviour. In channel Y, both images 1 and
2, both in region 2 (δ = 32 − 128), present scaling properties.
Finally, in channel K, only the region 2 of image of area 2
(δ = 32 − 128) exhibits a scaling behaviour.
Once the scale ranges where a scaling behaviour occurs have
been established, a comparison with the Grey-Scale images is
made. Only channel C can capture the scaling behaviour of
the reference image, specifically in image area 2 for both scale
ranges. In this case, it could be considered that the CMYK
colour profile does not provide proper results, since it fails to
capture the monofractality of the original image.
D. Multifractal Analysis of HSV images
The HSV images present the same behaviour as the pre-
vious ones, i.e., each of the images analysed shows different
partition functions depending on the channel. In Figure 7, the
partition functions of the HSV image area 1 are shown as
a representative example. Again, as it can be seen, different
linear regions and different scale ranges are obtained from the
multifractal analysis.
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel H
0
1
2
3
4
5
ln [d]
-20
0
20
40
60
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel S
0
1
2
3
4
5
ln [d]
-20
0
20
40
60
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel V
(a)
(b)
(c)
Region 1
Figure 7. Partition functions of the HSV image area 1, (a) Partition function
of channel H, (b) Partition function of channel S, (c) Partition function of
channel V.
Next, the scale ranges that exhibit a scaling behaviour are
explained. Regarding channel H, the 3 analysed images present
scaling features in region 1, specifically from δ = 2 to δ = 128
for the image of areas 1 and 3, and from δ = 1 to δ = 128
11
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

for image area 2. The same result is found for channels S and
V, where the images exhibit scaling properties for region 1
(δ = 1 to δ = 128).
In terms of the comparison with the Grey-Scale reference
images, it is noteworthy to mention that all 3 channels are
capable of capturing the scaling behaviour of the reference
images, except in the case of image area 3 and channel H.
E. Multifractal Analysis of HSI images
Finally, the results obtained in the analysis of the HSI
images are discussed. Again, for the same image, each colour
channel has different linear regions (see Figure 8). This
happens in the rest of images, as already discussed in the
previous cases.
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel H
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel S
0
1
2
3
4
5
ln [d]
0
100
200
ln [c(q,d)]
q = -4
q = -3
q = -2
q = -1
q = 0
q = 1
q = 2
q = 3
q = 4
Channel I
(a)
(b)
(c)
Region 1
Region 2
Region 1
Region 1
Region 2
Figure 8. Partition functions of the HSI image area 1, (a) Partition function
of channel H, (b) Partition function of channel S, (c) Partition function of
channel I.
The scale ranges that exhibit a scaling behaviour are ex-
plained next: In channel H, the image of area 1 for a scale
range from δ = 8 to δ = 32 in region 2, as well as image area
2 for the region 2 (δ = 16 − 128), and image area 3 again in
region 2 (δ = 8 − 32). In channel S, a scaling behaviour is
found in region 1 (δ = 4 − 128) of the image of area 1, in the
image of area 2 in region 1 (δ = 1 − 128) and in the image
of area 3 in regions 1 (δ = 4 − 16) and 2 (δ = 16 − 128).
In I channel, image area 1 shows scaling features in region 2
(δ = 8 − 128), the image area 2 in regions 1 (δ = 4 − 16)
and 2 (δ = 16 − 128), and image of area 3 only in region 2
(δ = 8 − 32).
Compared to the Grey-Scale reference images, it should
be highlighted that channel H fails to capture the reference
scaling behaviour in any of the images. Regarding channel
S, the results are acceptable for the image of area 2, in
region 1 (δ = 1 − 128), and in image of area 3 in region
2 (δ = 16 − 128). Finally, channel I only provides acceptable
results for image area 2, specifically in region 2 (δ = 16−128).
F. Comparison between Grey-Scale images and colour for-
mats
The aim of this subsection is to summarise the colour
channels that have been detected as the best approximation to
the monofractal behaviour of the Grey-Scale reference images.
In order to do so, the focus is on the difference between the
D(q) values of the Grey-Scale images and those of the colour
models assessed with the quadratic error (P x2). Table III
shows how the colour channels resemble to the original image
measured as percentage. This percentage is calculated by
considering the colour channels that exhibit scaling behaviour
and taking into account those whose quadratic error (P x2) is
less than or equal to 0.2. This way, those colour channels that
have presented a lower error will have a higher percentage.
That is, if a channel, in all cases, has managed to capture the
monofractal behaviour of the reference image, it will have a
percentage of 100% and, therefore, if it has not been able to
capture such behaviour in any of the cases, it will be scored
with 0%. Therefore, the higher the percentage value, the more
representative and better approximation the channel presents.
It should be noted that Table III shows the results of the three
images analysed from an overall global approach.
TABLE III. APPROXIMATION BETWEEN GREY-SCALE REFERENCE IMAGES
AND DIFFERENT COLOUR PROFILES IN PERCENTAGE
Colour profile
Approximation rate (%)
RGB
Channel R
0%
Channel G
60%
Channel B
25%
CMYK
Channel C
67%
Channel M
0%
Channel Y
0%
Channel K
0%
HSV
Channel H
67%
Channel S
100%
Channel V
100%
HSI
Channel H
0%
Channel S
50%
Channel I
25%
As seen, three out of the four colour formats studied fail to
capture the reference monofractal behaviour. For RGB, CMYK
and HSI formats, there is at least one channel that can not be
accepted as a proper approximation of the reference images,
exhibiting a percentage of 0%. By contrast, there are other
channels with an acceptable approximation with a percentage
over 60%, e.g., channels G, C and S for RGB, CMYK and HSI,
respectively. Finally, the HSV colour model provides proper
results, since a high approximation (between 67% and 100%)
is achieved for three channels and their three images.
V. CONCLUSIONS
This work proposes a novel methodology to analyse images
in different colour formats: Grey-Scale, RGB, CMYK, HSV
12
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

and HSI by means of a multifractal approach based on the
box-counting method and the method of moments. The study
is carried out on thermal images of a Raspberry Pi3 electronic
device.
Results show that the images in all the studied colour
profiles have different partition functions as well as different
scale ranges. This implies that the multifractal analysis varies
depending on both the image format and the colour profile. In
fact, the same image presents different multifractal behaviour
depending on the colour format. On the contrary, the partition
functions of the Grey-Scale images show linearity for all
values of δ (1-128) in all images analysed.
The capability of capturing the behaviour of the original
image of each format is evaluated by estimating the approxi-
mation of each colour channel with respect to the Grey-Scale
image.
Results show that the CMYK colour profile is not consid-
ered suitable, as it fails to capture the multifractal behaviour
of the original image in most of its channels. Among them
some channels of RGB and HSI are able to approximate to
the behaviour of the original image, the HSV arises as the
most suitable format for analysing thermal images from a
multifractal approach. Finally, it should also be noted that the
capability to reproduce the monofractal behaviour of Grey-
Scale images also depends on the range of scales considered
(δ). For RGB and HSI formats, the scale range is usually
from 16 to 128, so no monofractality can be found for scales
(δ < 16). Conversely, in the HSV format, the scale range
covers the entire image with δ ranging from 1 to 128, so in
this sense, this format is the one that exhibits monofractality
over a larger range of scales.
The disparity of scaling behaviours found for different
image channels and formats suggests an in-depth study on the
multifractal features of images represented in different formats
using the usual image analysis benchmarks from the literature
is needed. As future work, it would be interesting to build a
meta-model, in which different formats would consider their
“understanding” of the image, while that meta-model would
combine their “opinions”.
ACKNOWLEDGMENT
Work
supported
by
eFracWare
project
(TED2021-
131880B-I00),
funded
by
Spanish
MCIN
and
the
European
Union
“NextGenerationEU”/PRTR
on
MCIN/AEI/10.13039/501100011033, and eMob (PID2022-
137858OB-I00), funded by Spanish MCIN/AEI/10.13039/
501100011033/FEDER, UE.
REFERENCES
[1] B. Mandelbrot, “How long is the coast of britain? statistical self-
similarity and fractional dimension,” Science, vol. 156, pp. 636–638,
1967.
[2] A. N. Kravchenko, C. W. Boast, and D. G. Bullock, “Multifractal
analysis of soil spatial variability,” Agronomy Journal, vol. 91, no. 6,
pp. 1033–1041, 1999.
[3] M. S. Jouini, S. Vega, and E. A. Mokhtar, “Multiscale characteriza-
tion of pore spaces using multifractals analysis of scanning electronic
microscopy images of carbonates,” Nonlinear Processes in Geophysics,
vol. 18, pp. 941–953, 2011.
[4] C. Evertsz and B. Mandelbrot, “Appendix b. multifractal measures. chaos
fractals.” Springer, pp. 922–953, 1992.
[5] M. Ivanovici and N. Richard, “Fractal dimension of color fractal
images,” IEEE Trans. Image Processing, vol. 20, pp. 227–235, 1 2011.
[6] S. R. Nayak, J. Mishra, A. Khandual, and G. Palai, “Fractal dimension
of rgb color images,” Optik, vol. 162, pp. 196–205, 6 2018.
[7] S. Verd´u, J. Barat, and R. Grau, “Fresh-sliced tissue inspection: Charac-
terization of pork and salmon composition based on fractal analytics,”
Food and Bioproducts Processing, vol. 116, pp. 20–29, 7 2019.
[8] J. Chauveau, D. Rousseau, P. Richard, and F. Chapeau-Blondeau, “Mul-
tifractal analysis of three-dimensional histogram from color images,”
Chaos, Solitons and Fractals, vol. 43, pp. 57–67, 2010.
[9] N. Reljin, M. Slavkovic-Ilic, C. Tapia, N. Cihoric, and S. Stankovic,
“Multifractal-based nuclei segmentation in fish images,” Biomedical
Microdevices, vol. 19, no. 17, 9 2017.
[10] R. Uthayakumar and D. Easwaramoorthy, “Multifractal analysis in
denoising of color images.”
IEEE, 2012, pp. 228–234.
[11] B. Mandelbrot, The fractal geometry of nature, 3rd ed.
New York: W.
H. Freeman and Comp., 1983.
[12] J. Feder, Fractals, ser. Physics of Solids and Liquids.
Springer, 2013.
[13] P. Grassberger, “Generalized dimensions of strange attractors,” Phys.
Lett. A, vol. 97, no. 6, pp. 227–230, 1983.
[14] H. Hentschel and I. Procaccia, “The infinite number of generalized
dimensions of fractals and strange attractors,” Physica 8D, vol. 8, pp.
435–444, 1983.
13
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

A Graphical Analysis of the Multimodal Public Transport Network – The Bay of Cadiz
Patricia Camacho Magri˜n´an
University of Cadiz, Puerto Real, Spain
email: patricia.camachomagri@uca.es
Pablo Pav´on Dom´ınguez
University of Cadiz, Puerto Real, Spain
email: pablo.pavon@uca.es
Patricia Ruiz
University of Cadiz, Puerto Real, Spain
The University of Sydney, Sydney, Australia
email: patricia.ruiz@uca.es
Abstract—The structure of the current public transport net-
work is the result of many different political, economical and soci-
etal decisions over a long period of time. Further developments of
these networks require exhaustive analysis of the actual topology
and its properties in order to achieve significant improvements. In
this work, a statistical analysis and graph theory approach of the
multimodal transport network of the Bay of Cadiz is proposed.
First, the statistical characteristics of centrality and robustness
of the global network are calculated. Later, the influence of
specific means of transport on the complete public transport
network is analyzed by comparing these parameters against
different subnetworks where one or more transport networks
have been eliminated. The results obtained evince the importance
and influence of the different transport networks in the Bay
of Cadiz, highlighting those presenting high robustness on the
overall network, as well as those whose influence is limited.
Index Terms—Multimodal transport network; public transport
efficiency; graph theory; centrality and robustness metrics.
I. INTRODUCTION
Society is currently experiencing the migration of popula-
tion to big cities world wide. It is forecasted that by 2050
over 66 % of the population will live in urban areas [1].
This fact brings many opportunities but also many challenges
that governments need to face to overcome traffic congestion,
spatial inequalities and promote sustainability and economic
development [2]. Among many other issues, an in-depth study
of the Public Transport System (PTS) is essential for the devel-
opment of cities to make them more efficient, sustainable and
attractive to users, thus reducing traffic congestion, pollution
or noise, and making cities more livable.
However, their design, planning and expansion of PTSs are
not straightforward due to the demands of all actors involved.
For example, quality of service and passenger comfort are
essential to increase ridership. Low operational cost is required
to make it profitable for the operator. Efficiency is required
not only to reduce operational cost but also to address sus-
tainability, as well as connectivity between different transport
networks [3].
Public transport networks can be studied or assessed from
different points of view, such as trip and route planning,
user ease of use, or graph theory, among others [4]. In the
latter, transport networks are represented as a graph, whose
nodes and edges determine the shape and composition of the
network. The nodes represent the transport stops; while the
edges represent the route between stops.
Multimodal transport is defined as the process of trans-
porting a person/goods between two distant points using
two or more modes of transport. It is experiencing a great
development around the world because of its high flexibility
and efficiency and low cost and energy consumption [5].
In this work, we propose an analysis of the multimodal
public transport network of the Bay of Cadiz, including
train, tram, boat, urban and interurban bus networks. Different
characteristic parameters of the network topology are obtained
and compared against different subnetworks where not all
the means of transport are presented. The results obtained
demonstrate the level of network connectivity, as well as the
effect of extracting one or more transport networks and the
influence of such extraction. Thus, the relevance of each of
them and the connections between the different stops of the
targeted network are checked.
The remainder of the paper is structured as follows: the
most relevant related works are briefly presented in Section II;
the methodology used is introduced in Section III; Section IV
shows the results obtained; and finally, Section V reflects the
main conclusions and outlines some future research lines.
II. RELATED WORKS
The study of multimodal transport networks has been widely
addressed in the literature. He et al. [6] study freight transport
in the Netherlands, focusing on road, river and rail transport,
obtaining a series of critical nodes essential for the stability
of the network structure, thus obtaining the key areas of
the network. Wang et al. [7] study the multimodal transport
network of the China-Europe Railway Express, obtaining that
the transfer between road and rail manages to alleviate the
cascade failures that can occur. Tympakianaki et al. [8] study
the effect of tunnel closures on the Stockholm transport
network, resulting in the need for network redistribution to
avoid traffic management problems for the city. Guo et al. [5]
analyse the multimodal transport network of the Sichuan-Tibet
(China) region of rail, road and air to place emergency rescue
facilities, concluding that the whole network should be taken
into account, especially at the most important nodes and links.
Graph theory has been also applied to analyze transport
networks. In [9], the Madrid metro network is analysed
through centrality and robustness parameters, focusing on
closeness centrality and cluster coefficient. As a result, the
14
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

vulnerability of the entire network is obtained, as well as the
identification of the most important stops to achieve robustness
in the network. Similarly, Mari˜nas Collado et al. [10] carried
out a study of the Barcelona metro network using indicators
of robustness, cluster coefficient and average efficiency. Thus,
they obtained the characteristics of the different stops with
the aim of obtaining a specific analysis to plan and restructure
the network in search of better viability. Derrible makes use
of graph theory in [11], to study the centrality of the metro
network of various cities, focusing on centrality measures in
search of future efficiency improvement for network planning
and sustainable cities. Frutos Bernal et al. [12] analyze the
Madrid metro network in terms of centrality measures to
verify the location of the different stops, showing that the
stops in the center of the city are those with greater centrality
and, therefore, greater connection. Cats [13] analyzes the
historical evolution of the multimodal rail transport network
(tram, light rail, metro, high-speed express connections and
local, suburban and regional trains), focusing on the efficiency
and centrality of the nodes. In this way, it shows how it
is affected by the technological factor in search of better
connectivity, taking into account the increase in urban centres
and population growth.
III. METHODOLOGY AND CASE OF STUDY
The metropolitan area of the Bay of Cadiz comprises 12
municipalities, with a total of 823,806 inhabitants [14] [15].
These towns are: Cadiz (CA), El Puerto de Santa Mar´ıa (PSM),
San Fernando (SF), Puerto Real (PR), Jerez de la Frontera (JF),
Chiclana de la Frontera (CF), Rota (RO), Conil de la Frontera
(CoF), Arcos de la Frontera (AF), Chipiona (CH), Sanl´ucar
de Barrameda (SB), and Medina Sidonia (MS). As it can be
seen in Figure 1, they are all located in the province of Cadiz,
south-west of Andalucia (Spain).
Fig. 1. Location of the municipalities that form the Bay of Cadiz in Andalusia,
Spain.
These municipalities are scattered across the territory and
require urban and interurban transport systems to interconnect
them and meet passenger demands. The Transport Consortium
of the Bay of Cadiz operates a transport network comprising
train, boat, tram and bus services, both at urban and interurban
level. The stops of the train, tram and boat networks are
outward and return, thus providing two-way journeys. For the
rest of the networks, any stop only refers to one way trip,
because they are generally circular lines (see [15] [16] for
more detailed information). Table I shows the different types
of transport offered in each of the cities [16].
TABLE I
THE DIFFERENT MEANS OF TRANSPORT INCLUDED IN EACH OF THE
TRANSPORT NETWORKS FOR THE TWELVE CITIES INCLUDED IN THE
METROPOLITAN AREA OF BAY OF CADIZ
City
Pop.
Interurban
Urban
Train
Boat
Tram
CA
113066
✓
✓
✓
✓
✓
PSM
89435
✓
✓
✓
✓
SF
94120
✓
✓
✓
✓
PR
41963
✓
✓
✓
JF
212730
✓
✓
✓
CF
87493
✓
✓
✓
RO
29491
✓
✓
✓
CoF
23497
✓
✓
AF
30953
✓
✓
CH
19592
✓
SB
69727
✓
MS
11739
✓
The representation of the public transport network of the
Bay of Cadiz has been determined using the L-Space method
initially proposed in [17] [18]. In this method, the nodes
represent stops, while the edges represent the given connection
between two nodes of a route. The multimodal transport
network is represented as a directed graph G = (V, E), where
the set of nodes (V = {v1, v2, ..., vN}) corresponds to the
different transport stops, and the trips between any two stops
correspond to the set of directed edges (E). The adjacency
matrix of G, AG is an asymmetric matrix of size N x N,
where each entry aij equals 1 if there is a mean of transport
that offers trip that goes from stop vi to stop vj, 0 otherwise.
Figure
2 shows the resulting graph of the complete public
transport network in the Bay of Cadiz using Matlab software.
Fig. 2. Resulting graph of the complete public transport network in the Bay
of Cadiz.
In order to study the relevance of the different modes of
transport that constitute the multimodal network, a set of sub-
networks has been created based on the combination of the
different existing transports. Specifically, three categories or
groups have been considered. They are explained next, and
15
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

shown in Figure 3. Group A is composed of the different
possible combinations that can be obtained from the different
interurban networks of the set (train, tram, boat). Group B
is related to the interurban bus network and two subgroups
are possible. First, the global network is analyzed without the
interurban network. Second, only the interurban is considered.
Regarding group C, two possible subgroups are considered. On
the one hand, all the means of transport are included except the
urban bus networks for all cities. On the other hand, there are
twelve subgroups C, one per each city whose urban network
is not included.
Fig. 3. Classification of Subgraph Groups. In brackets, abbreviation of each,
where ”W/” stands for ”Without” and ”O/” stands for ”Only”.
A. Statistical and robustness parameters
The analysis of a transport network involves examining
specific data about its structure, including relevant statistical
and robustness parameters [19] [20]. Next, the most relevant
features analyzed in this work are presented:
• The density (d) of the network is a measure of the number
of existing connections in relation to the total number of
possible connections in the network.
d =
E
V (V − 1)
(1)
• The frequency distribution of degree (P(k)) in a network
assigns to each degree k the number of nodes that have
that value, describing the proportion of nodes in the
network with that value.
P(k) = Vk
V
(2)
Where Vk is the number of nodes with degree k in the
graph, and V is the total number of nodes given. Graphs
with a more uniform degree distribution tend to have a
higher normalised robustness indicator compared to those
with a more heterogeneous distribution. The value of this
is higher when more alternative routes are available, and
lower when the system is larger.
• The diameter (D) of a graph is defined as the longest
distance between any pair of nodes in the network,
representing the longest path.
• The closeness centrality Cc(i) is defined as the proximity
to all other nodes in the graph. It is calculated as stated
in ( 3), where d(i,j) is the shortest distance from node i
to node j; while d(j,i) is the shortest distance from node
j to node i.
Cc(i) =
1
P
j̸=i(d(i, j) + d(j, i))
(3)
Nodes with a higher closeness are considered more influ-
ential in the network, occupying a more central position.
In this way, it is possible to identify those that are most
relevant to the connection in the graph.
• The betweenness centrality (Bci), measures the degree to
which a node is on the shortest path between other pairs
of nodes in the network, i.e., the number of times a node
acts as an intermediary in communication. It is calculated
as stated in ( 4), where σjk is the total number of shortest
paths between nodes j and k, and σjk(i) is the number
of such paths passing through node i.
Bci =
X
j̸=i̸=k
σjk(i)
σjk
(4)
Nodes with high betweenness centrality occupy strategic
positions in the network, so their removal can have a
significant impact on the connectivity and efficiency of
the network.
• The average efficiency (E[ 1
H ]), represents the overall
communication capacity of a network by calculating the
average of the efficiencies of all the pairs of nodes that
make it up as shown in ( 5), where d(vi, vj) is the shortest
path distance between nodes vi and vj of the network,
using
1
V (V −1) as the normalisation factor.
E[ 1
H ] =
1
V (V − 1)
V
X
i,j=1,i̸=j
1
d(vi, vj)
(5)
The efficiency of a pair of nodes is defined as the inverse
of the shortest distance between them. It ranges between
0 and 1. The higher the value of this parameter, the higher
the robustness of the analysed network because it refers
to the survivability against random failures or deliberate
attacks involving the removal of nodes and links [20]
[21].
• The normalised robustness indicator (¯rT ), which ranges
from 0 to 1, evaluates the resilience of a network to
failures and its ability to maintain connectivity. ¯rT is
given by the ratio between the number of alternative
routes in the network and the total number of nodes [20],
from the total number of edges E and the number of
nodes V.
¯rT =
ln(E − V + 2)
ln( V (V −1)
2
) − V + 2
(6)
16
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

• The global cluster coefficient parameter (Gc) indicates
how neighbours of a given node are connected to other
nodes, ranging between values of 0 and 1.
Gc = 1
V
V
X
i=1
Gc(vi)
(7)
Where Gc(vi) represents the individual cluster coefficient
of each node, while V means the total number of nodes.
The clustering coefficient reflects the fault tolerance ca-
pability, so as the value of Gc increases, the local fault
tolerance increases.
The use of the aforementioned statistical and robustness
parameters, in addition to the basic statistical parameters of
standard deviation σ applied to degree distribution, have been
used to compare the subgraphs versus the global network.
Thus, by extracting one or several transport networks, the
relevance of each transport system on the global network is
evaluated.
IV. RESULTS AND DISCUSSION
This section first presents an analysis of the global mul-
timodal network and then evaluates the influence of each
subgroup on the complete network once the following are
extracted from it.
A. Analysis of the complete Multimodal Public Transport
Network of the Bay of Cadiz
First of all, the density d of the network studied, indicates a
poorly connected network according to (1), with this value be-
ing approximately 0.00183. In comparison with other national
transport networks, such as the metro networks of Madrid
and Barcelona, which have values of 0.009421 and 0.0157
respectively [9] [10], the connectivity of the Bay’s multimodal
transport network is lower. This result is reasonable because
the targeted network is a multimodal transport network where
more than one type of transport is studied.
In terms of the node degree, the studied multimodal network
presents a highly variable value varying from 80 to 2. The
average degree of the network is ⟨k⟩ ≈ 4.867, meaning that
most of the nodes has a low degree value. In Figure 4, the
distribution degree P(k) is shown.
As it can be seen in Figure 4, most of the nodes are
concentrated in the first grades, being connected to at least two
nodes. This is the most frequent grade (about 60%) although
the average is 4.867. It is followed by grade 4 with a little
more than 20%, i.e. the stops with the possibility to change
route. After that, the decrease in percentage for the following
grades occurs from grade 10 onwards, being less than around
2%.
Table II shows the five nodes with the highest degree values.
As it can be seen, the station of Jerez presents the highest
value, 80. Indeed, this node is included in three different
means of transport (urban, interurban and train) and in many
different lines in each of them. Table II also shows that there
are two more locations in the overall network with a high
Fig. 4.
Distribution Degree of the Public Transport network in the Bay of
Cadiz.
degree value: Asdrubal Square and Puerta del Mar in Cadiz
(in both directions), the latter being where the Cadiz hospital
is located.
TABLE II
NODES WITH HIGHEST VALUES OF DEGREE IN THE GLOBAL MULTIMODAL
TRANSPORT GRAPH OF BAY OF CADIZ
Node
Degree
Transport
Jerez Station
80
Interurban, Urban, Train
Asdrubal Square (to Cadiz)
70
Interurban, Urban
Puerta del Mar (to Cadiz)
64
Interurban, Urban
Puerta del Mar (from Cadiz)
60
Interurban, Urban
Asdrubal Square (from Cadiz)
58
Interurban, Urban
The diameter of the network is 73, representing the longest
distance between any pair of nodes. However, the average
shortest path is 19,622. In terms of the the centrality param-
eters, the closeness and betweenness centrality of each node
forming the global network have been studied. The former
reaches a maximum value of 0.090, shown in nodes such as
Telegrafia (to Cadiz), Puerta del Mar (the stop of the hospital
to Cadiz) and El Puerto Station (to Cadiz). The latter, i.e. the
betweenness centrality, reaches a maximum value of 0.50 for
Jerez Station, followed by values of 0.28 approximately, as in
the nodes found in El Puerto Station. This makes the areas of
Cadiz, PSM and Jerez, the most relevant in terms of centrality
and, therefore, connectivity.
Regarding to the robustness parameters of the global net-
work, the average cluster coefficient of the network is 0.031,
a value similar to relevant networks such as the London
Metro Network (Gc = 0.0409) or Tokyo Metro Network
(Gc = 0.0285) [21] and, on Spanish territory, exceeding the
Barcelona Metro Network (Gc = 0.0044) [10].
B. Transport system relevance on the multimodal network
The statistical parameters of groups A, B and C presented
in Section III are summarised in Tables III, IV and V,
respectively. These tables show the values of the different
statistical and robustness parameters for each subgroup. These
parameters are: number of nodes, number of edges, diameter,
average degree (k), standard deviation of the nodes’ degree
17
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

(σ), cluster coefficient (Gc), average efficiency (E[ 1
H ]) and
normalised robustness indicator (rT ).
TABLE III
STATISTICAL PARAMETERS OF THE SEVEN DIFFERENT SUBGRAPHS
INCLUDED IN GROUP A.
Global
W/T
W/B
W/Tr
Nodes
1325
1323
1324
1310
Edges
3226
3202
3222
3186
Diameter
73
73
73
73
¯k
4.867
4.838
4.867
4.861
σ Degree
7.141
7.070
7.148
7.168
Gc
0.031
0.031
0.031
0.031
E[ 1
H ]
0.032
0.032
0.032
0.032
rT
0.986
0.986
0.986
0.986
W/T+B
W/T+Tr
W/B+Tr
W/T+B+Tr
Nodes
1322
1304
1309
1303
Edges
3198
3162
3182
3158
Diameter
73
73
73
73
¯k
4.835
4.847
4.859
4.844
σ Degree
7.072
7.110
7.171
7.113
Gc
0.031
0.031
0.031
0.031
E[ 1
H ]
0.032
0.032
0.032
0.032
rT
0.986
0.986
0.986
0.986
In Table III, the statistical parameters of each of the
seven subgraphs of group A are compared against the global
network. This group considers maintaining the urban and
interurban buses in all subgroups, but not considering the
other means of transport in turns. As it is shown, there are
no remarkable variations in any of the parameters studied.
Indeed, the diameter, the normalised robustness indicator,
the clustering coefficient as well as the average efficiency
do not change in any of the studied subnetworks. In terms
of the number of nodes and edges, the maximum variation
is obviously found when not considering the combination of
the three means of transport (W/T+B+Tr). However, when
considering the deletion of a single mean of transport, the
one presenting a higher impact is the tram. This is mainly
because the stops and therefore, the links included in it, are
exclusively for this kind of transport.
TABLE IV
STATISTICAL PARAMETERS OF THE TWO DIFFERENT SUBGRAPHS
INCLUDED IN GROUP B.
Global
W/I
O/I
Nodes
1325
1190
251
Edges
3226
1904
1322
Diameter
73
92
40
¯k
4.867
3.202
10.534
σ Degree
7.141
2.201
12.372
Gc
0.031
0.015
0.077
E[ 1
H ]
0.032
0.015
0.082
rT
0.986
0.990
0.879
The impact of the interurban bus network is analyzed in
Table IV, where the parameters of the two different subgraphs
that conform Group B are presented. It is observed that
removing the interurban, subgraph (W/I), means losing a total
of 135 nodes, a considerable loss, which highly affects all
the statistical parameters of the subgraph, as it it is shown in
Table IV. This means that some localities are disconnected
from the rest of the graph, such as Chipiona, Sanlucar de
Barrameda and Medina-Sidonia, and it makes more difficult
the access to the rest of the network. Indeed, the value of
the diameter has highly increased from 73 to 92. Thus, as
this network has a smaller number of nodes than the global
network, the standard deviation σ of the degree distribution
is remarkably lower, showing that the distribution is more
evenly distributed across the network nodes. In relation to the
robustness parameters, it is observed that both the average
efficiency E[ 1
H ] and the clustering coefficient are lower than
the global network. In contrast, it has a similar value for the
normalized robustness indicator rT , indicating that it has a
similar number of alternative routes as the global network.
Table IV also analyzes the interurban network by itself. It
has a total of 251 nodes, differing greatly from the global
network. On the one hand, its diameter is 40, so the distance
between any two nodes is shorter compared to the global
network. In terms of robustness parameters, the normalized
robustness indicator rT is close to the values of the other
subgraphs in the group, so it has a similar number of al-
ternative routes as the other graphs in this group. Regarding
both cluster coefficient parameter GC and average efficiency
E[ 1
H ] present a remarkable increase in their values, indicating
a higher robustness in this network.
TABLE V
STATISTICAL PARAMETERS OF THE TEN DIFFERENT SUBGRAPHS
INCLUDED IN GROUP C.
Global
W/U
W/AF
W/CA
W/CF
W/CoF
Nodes
1325
276
1270
1256
1214
1294
Edges
3226
1390
3151
3092
3116
3179
Diameter
73
41
73
73
73
73
¯k
4.867
10.072
4.962
4.924
5.008
4.913
σ
Degree
7.141
12.098
7.261
7.085
7.405
7.207
Gc
0.031
0.073
0.030
0.032
0.033
0.031
E[ 1
H ]
0.032
0.078
0.031
0.033
0.034
0.032
rT
0.986
0.887
0.986
0.985
0.985
0.986
Global
W/PSM
W/JF
W/PR
W/RO
W/SF
Nodes
1325
1193
877
1278
1227
1267
Edges
3226
2975
2470
3079
3079
3133
Diameter
73
73
66
73
64
73
¯k
4.867
4.987
5.633
4.818
5.019
4.946
σ
Degree
7.141
7.334
8.279
7.203
7.378
7.285
Cc
0.031
0.033
0.042
0.029
0.033
0.029
E[ 1
H ]
0.032
0.034
0.044
0.030
0.034
0.030
rT
0.986
0.985
0.976
0.986
0.985
0.986
Finally, the parameters of subgraphs included in group C
are shown in Table V. In this case the impact of the urban bus
network is studied. As can be seen, the values obtained by
removing a single urban network are similar to those of the
overall network, having similar characteristics of nodes and
edges, as well as in terms of robustness parameters. However,
in the case of Jerez de la Frontera it is different. In this case,
as it is the largest urban network of those studied, the effect of
eliminating it is more significant in comparison with the rest.
18
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

It is interesting to note the results obtained when all city buses
are eliminated. It can be seen that the impact on the number
of nodes and edges is high, as well as the diameter becomes
smaller and, therefore, there are smaller maximum distances
between nodes. Indeed, this behaviour is very similar to the
subgroup of group B where only interurban bus network is
considered. Both subgraphs present lower number of nodes,
edges and diameter than the global network, but higher average
degree and standard deviation, as well as similar values for the
robustness parameters, meaning that both the interurban and
urban networks are the most representative in the global graph.
This analysis shows that changes in the urban and in-
terurban networks will directly influence the global network
significantly, i.e. in terms of the robustness and connectivity.
However, the tram, train and boat have a low impact. More-
over, independently of the studied subgraph, the normalized
robustness indicator keeps a high value, highlighting the
resilience of the network (in terms of connectivity) to failures.
V. CONCLUSIONS
In this work, a graphical analysis of graph theory has been
carried out on the public transport network of the Bay of
Cadiz, since it is a transport network that is in continuous
growth and can give rise to studies on its efficiency. In order
to achieve the objectives of this study, a statistical and graph
theory analysis of the network structure is carried out. The
global network has been partitioned in different graphs and
subgraphs in order to analyze the influence of the different
means of transport composing it.
The results of this work show the importance or influence
of the different types of transport in the Bay of Cadiz. In
this way, the networks that function as the core of the overall
network are both the urban and the interurban bus networks.
The interurban network presents the highest normalised ro-
bustness indicator as it is the one that connects the different
localities included in the Bay of Cadiz. Some of them become
isolated when this subnetwork is not considered. However, it
is worth noting that in any of the studied subnetworks, the
normalised robustness indicator presents a high value (always
over 0.87) even when not considering many of the transport
modes, meaning that the system is resilient and it maintains
connectivity in case of failure of any transport mode.
As future work, the travel time or fuel consumption will
be considered, including these features as weights in the
network. In addition, a multifractal study of the Bay of Cadiz
transport network based on the statistical study can provide
more information in this respect, as well as studying how
to adapt fractal algorithms to directed graphs of the different
types of transport.
ACKNOWLEDGMENT
This work was supported by the project eFracWare
(TED2021-131880B-I00)
funded
by
Spanish
MCIN
and the European Union “NextGenerationEU”/PRTR on
MCIN/AEI/10.13039/501100011033, and the project eMob
(PID2022-137858OB-I00) funded by Spanish MCIN, and
the Agency and the European Regional Development on
MCIN/AEI/10.13039/501100011033/FEDER, UE.
REFERENCES
[1] U. Nations, “Un-habitat,” Retrieved: 06, 2023. [Online]. Available:
https://unhabitat.org/topic/urban-planning
[2] European Strategy and Policy Analysis System (ESPAS), “Global Trends
to 2030: The Future of Urbanization and Megacities,” ESPAS Ideas,
2019.
[3] J. Malasek, “A set of tools for making urban transport more sustainable,”
Transportation Research Procedia, vol. 14, pp. 876–885, 2016.
[4] J. Zhang, F. Liao, T. Arentze, and H. Timmermans, “A multimodal
transport network model for advanced traveler information systems,”
Procedia - Social and Behavioral Sciences, vol. 20, pp. 313–322, 2011.
[5] J. Guo, Q. Du, and Z. He, “A method to improve the resilience of
multimodal transport network: Location selection strategy of emergency
rescue facilities,” Computers & Industrial Engineering, vol. 161, pp.
1–15, 2021.
[6] Z. He, K. Navneet, W. van Dam, and P. Van Mieghem, “Robustness
assessment of multimodal freight transport networks,” Reliability Engi-
neering & System Safety, vol. 207, p. 107315, 2021.
[7] B. Wang, Q. Su, and K. S. Chin, “Vulnerability assessment of
china–europe railway express multimodal transport network under cas-
cading failures,” Physica A: Statistical Mechanics and its Applications,
vol. 584, no. C, pp. 11–36, 2021.
[8] A. Tympakianaki, H. Koutsopoulos, E. Jenelius, and M. Cebecauer,
“Impact analysis of transport network disruptions using multimodal
data: A case study for tunnel closures in stockholm,” Case Studies on
Transport Policy, vol. 6, 2018.
[9] E. Frutos Bernal and A. Mart´ın del Rey, “Study of the structural
and robustness characteristics of madrid metro network,” Sustainability,
vol. 11, no. 12, 2019.
[10] I. Mari˜nas-Collado, E. Frutos Bernal, M. T. Santos Martin, A. Mart´ın del
Rey, R. Casado Vara, and A. B. Gil-Gonz´alez, “A mathematical study
of barcelona metro network,” Electronics, vol. 10, no. 5, 2021.
[11] S. Derrible, “Network centrality of metro systems,” PLOS ONE, vol. 7,
pp. 1–10, 2012.
[12] E. Frutos Bernal, A. Mart´ın del Rey, and P. Galindo Villard´on, “Analysis
of madrid metro network: From structural to hj-biplot perspective,”
Applied Sciences, vol. 10, no. 16, 2020.
[13] O. Cats, “Topological evolution of a metropolitan rail transport network:
The case of stockholm,” Journal of Transport Geography, vol. 62, pp.
172–183, 2017.
[14] INE, “National Statistical Institute,” Retrieved: 06,2023. [Online].
Available: https://www.ine.es/nomen2/index.do
[15] CTBC,
“Bay
of
Cadiz
Transport
Consortium,”
Retrieved:
06,
2023.
[Online].
Available:
https://cmtbc.es/generica.php?pagina=
creacion-y-objetivos
[16] Junta de Andaluc´ıa, “Junta de Andaluc´ıa. (Bay of Cadiz Metropolitan
Transport Plan. Mobility Plan,” Retrieved: 06, 2023. [Online]. Available:
http://www.juntadeandalucia.es/medioambiente/portal web/web/
temas ambientales/evaluacion integracion planificacion/evaluacion
ambiental/evaluacion planes programas/otros planes programas/PTM
Bahia Cadiz/documentos/18132 06 15 3 Documento Sintesis.pdf
[17] S. Porta, P. Crucitti, and V. Latora, “The network analysis of urban
streets: A primal approach,” Environment and Planning B: Planning
and Design, vol. 33, no. 5, pp. 705–725, 2006.
[18] ——, “The network analysis of urban streets: A dual approach,” Physica
A: Statistical Mechanics and its Applications, vol. 369, no. 2, pp. 853–
866, 2006.
[19] A. L. Barab´asi, “Network Science,” Retrieved: 06, 2023. [Online].
Available: http://networksciencebook.com/
[20] Z. Ali and S. Bhaskar, “Basic statistical tools in research and data
analysis,” Indian Journal of Anaesthesia, vol. 60, p. 662, 2016.
[21] X. Wu, C. K. Tse, H. Dong, I. W. H. Ho, and F. C. Lau, “A
Network Analysis of World’s Metro Systems,” in Proceedings of the
2016 International Symposium on Nonlinear Theory and its Applications
(NOLTA2016).
The Institute of Electronics, Information and Commu-
nication Engineers, 2016, pp. 606–609.
19
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

A Survey of Recent Applications of the PISALE
Code and PDE Framework
Alice Koniges
Information and Computer Sciences
University of Hawai’i at M¯anoa
Honolulu, HI, USA
email: koniges@hawaii.edu
David Eder
Physics and Astronomy
University of Hawai’i at M¯anoa
Honolulu, HI, USA
email: dceder@hawaii.edu
Jonghyun Lee
Civil & Env Eng and Water Res Center
University of Hawai’i at M¯anoa
Honolulu, HI, USA
email: jonghyun.harry.lee@hawaii.edu
Aaron Fisher
Information Technology Services
University of Hawai’i at M¯anoa
Honolulu, HI, USA
email: fallen@andcheese.org
Yuriy Mileyko
Mathematics
University of Hawai’i at M¯anoa
Honolulu, HI, USA
email: ymileyko@hawaii.edu
Monique Chyba
Mathematics
University of Hawai’i at M¯anoa
Honolulu, HI, USA
email: chyba@hawaii.edu
Jack McKee
Mathematics
University of Hawai’i at M¯anoa
Honolulu, HI, USA
email: jmckee@math.hawaii.edu
Young-Ho Seo
Civil & Env Eng and Water Res Center
University of Hawai’i at M¯anoa
Honolulu, HI, USA
email: yhseo@hawaii.edu
Peter Yip
Aerospace Engineering and Mechanics
University of Minnesota
Minneapolis, MN, USA
email: yipxx043@umn.edu
Thomas Schwartzentruber
Aerospace Engineering and Mechanics
University of Minnesota
Minneapolis, MN, USA
email: schwart@umn.edu
Claudia Parisua˜na
Department of Mechanical Engineering
Stanford University
Stanford, CA USA
email: cparisua@stanford.edu
Siegfried Glenzer
SLAC National Accelerator Laboratory
Stanford University
Menlo Park, CA USA
email: glenzer@slac.stanford.edu
Abstract—We review the basic equations, numerical solution
techniques, and new application areas of a novel multi-purpose
computer code framework, PISALE, for the solution of complex
Partial Differential Equation (PDE) systems on modern com-
puting platforms. We describe how the code solves equations in
the fluid approximation using a novel combination of Arbitrary
Lagrangian Eulerian (ALE) and Adaptive Mesh Refinement
(AMR) methods. Sample problems from areas of ground water
flow, high-speed impacts, and X-ray Free Electron Laser (XFEL)
experiments are given.
Keywords—Adaptive Mesh Refinement; Computational Fluid Dynam-
ics; Arbitrary Lagrangian Eulerian Methods; Volume of Fluid; High
Performance Computing; Surface Tension; Ground Water Flow.
I. INTRODUCTION
The PISALE codebase contains a Partial Differential Equa-
tion (PDE) solver framework based on the combined methods
of Arbitrary Lagrangian Eulerian (ALE) dynamics and struc-
tured Adaptive Mesh Refinement (AMR). The PISALE code
uses an explicit time-marching Lagrange step to advance the
flow-field through a physical time step. The optional second
phase involves a modification of the grid and a remapping
(interpolation) of the solution to the new grid. The solution
of PDEs on modern High Performance Computing (HPC)
platforms is essential to the continued success of research
and modeling for a wide variety of areas. The PISALE code
name comes from the acronym Pacific Island Structured-AMR
with ALE. In some earlier papers (e.g., [10]) the code is
called ALE-AMR as it was one of the first codes to combine
those two methods. There are several branches of PISALE
to deal with disparate applications. These applications range
from high energy density physics problems to geothermal
flows. In this paper, we detail some of the wide variety of
applications suitable for modeling with PISALE and discuss
recent improvements to the code base.
In Section II, we give an overview of the equations used
in the PISALE code. In Subsection II-A, we describe how
the different physics modules are coupled using operator
splitting. We also provide some additional information on the
surface tension modules used in PISALE. In Section III, we
give a summary of past PISALE applications followed by
discussion on modeling groundwater flow in Subsection III-A.
We then discuss recent hypervelocity impacts simulations with
a comparison of PISALE results with experimental data and
with results from other codes in Subsection III-B. The third
new application is discussed in Subsection III-C, where we
describe the modeling of droplet dynamics. We provide some
conclusions and comments on future work in Section IV.
20
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

II. PISALE EQUATIONS
The numerical methodology of PISALE was initially devel-
oped for pure gas dynamics and its novelty at the time was
based on the coupling of AMR with a Lagrangian formulation
that retains accuracy in standard two- and three-dimension
problems, such as Sedov blast wave simulations [1]. By adding
a complex Right Hand Side (RHS) to the basic gas dynamics
with elastic/plastic flow terms, we are able to use operator
splitting methods to model an extremely diverse variety of
physical processes, each affecting the dynamically evolving
fields at successive time intervals according to timescales.
An example of a complex RHS is the use of anisotropic
material failure models with material history in multi-material
problems. Unlike the first implementations of these numerical
techniques such as Wilkins [26], PISALE was written from
the ground-up to be modular and take advantage of parallelism
opportunities on HPC platforms. PISALE operates on top of
the Structured AMR Application Interface (SAMRAI) library
[8] and contains a general purpose PDE solver that uses a
staggered-grid, Lagrangian formulation, written for coupled
plasma/fluids with position and velocity being nodal variables
and density, internal energy, temperature, pressure, strain, and
stress being zonal (cell centered) variables. This basic solver
is primarily for equations that can be written in conservation
form and uses a volume of fluid approach. Thermal conduction
and radiation transport coupled to the basic conservation law
equations are solved by implementing the diffusion approx-
imation, which uses a nodal radiation energy and a zone-
averaged nodal temperature. These plasma/fluid equations in
a Lagrangian formulation (in vector and indicial notation
i, j, k = 1, 2, 3) are:
Dρ
Dt
=
−ρ∇ · ⃗U = ρUi,i
(1)
D⃗U
Dt
=
1
ρ∇ · σ = 1
ρσij,j
(2)
De
Dt
=
1
ρV s : ˙ε − P ˙V = 1
ρV (sij ˙εij) − P ˙V
(3)
where
D
Dt =
∂
∂t + ⃗U · ∇ is the substantial derivative, ρ is the
density, ⃗U = (u, v, w) is the material velocity, t is time, σ is
the total stress tensor, P is the pressure, e is the internal energy,
V is the relative volume (ρV = ρ0 where ρ0 is the reference
density), s is the deviatoric stress defined as sij = σij + Pδij
where δ is the Kronecker delta and ˙ε is the strain rate tensor.
PISALE has a range of different strength and failure models
that can be used for impact and other applications.
A. PISALE Physics Modules
PISALE has significant physical modeling capabilities in
addition to the core ALE hydrodynamics and structural me-
chanics. These capabilities are provided through a flexible
physics module system which allows their PDEs to be updated
separately from the hydrodynamic models with a classical
operator splitting scheme. Field variables in these physics
modules are advanced in time after the hydrodynamic time
step and the modules have hooks that allow them to update
the field quantities when the ALE hydrodynamics alters the
underlying mesh. Physical models provided in this manner
include a laser ray tracing package [14], various surface
tension models [11] [12], heat conduction, and diffusion-based
radiation transport [7].
PISALE includes infrastructure to lower the barriers for
the introduction of additional physics modules. Currently, this
includes tools for transforming data and meshes from a block
structured format to an unstructured indexing format, tools
for tracing rays through meshes, and a basic Finite Element
Method (FEM) framework for discretizing PDEs, see Figure
1. Field data in PISALE physics modules is updated after
the ALE hydrodynamics step is complete. First field data is
collected from the SAMRAI patch-AMR hierarchy into a par-
allel composite mesh, which can be utilized with unstructured
solvers. Then FEM-based PDE discretizations are applied on
the composite mesh to update the relevant field data. The
updated field data on the composite mesh is then transformed
back into the patch-AMR hierarchy. This approach enables the
development of external solvers for PISALE without requiring
the data to be represented as a SAMRAI patch hierarchy.
Fig. 1. Field data on PISALE patch mesh is transferred to composite mesh
prior to solution by external PDE solver and then updated on patch mesh.
The current PISALE FEM framework includes only first
order H1 quadrilateral and hexahedron elements in 2D and
3D. This works well for the diffusion equation solvers utilized
in the heat conduction and radiation diffusion modules. We are
exploring the addition of elements from the H(curl), H(div),
and L2 spaces that would open up a path to discretizing a
wider array of PDEs in topic as diverse as: plasma physics,
mesoscale material modeling, seismicity in geophysics, and
electromagnetics. The current FEM framework in PISALE
only supports AMR in 2D. We are exploring FEM frameworks
with 3D AMR capability that would improve the existing heat
conduction, radiation diffusion, and surface tension models in
PISALE. Some examples are Modular Finite Element Methods
(MFEM) library [16] and the Multiphysics Object-Oriented
Simulation Environment (MOOSE) [17].
1) Surface Tension: A major use case of the operator
splitting system is the surface tension package, which models
2D and 3D surface tension using a multi-fluid Volume-of-Fluid
(VOF) paradigm[4]. The VOF paradigm avoids tracking an ex-
plicit representation of fluid boundaries, instead reconstructing
them at each time step based on the relative volume fractions
of each fluid within every cell. This was chosen because
volume fractions are already a core aspect of PISALE’s multi-
material formulation, meaning the surface tension package
21
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

only needs to generate and apply a body force. A height
function for each fluid is generated by summing up relative
volume fractions along columns of a 3x5 or 3x3x5 stencil.
Orientation and position of the stencil is chosen so that the
interface is always in the central cell of the stencil, the fluid
body is always on the ”bottom” of the stencil, and the first
derivatives of the height function are minimized. See Figure
2.
In 2D, three points are chosen based on the height function
and mapped back to real coordinates, where they are used
to generate a three-point circular fit of the boundary, which
approximates an osculating circle. This scheme is robust to
nonlinear meshes with regular column widths, but a correc-
tion term must be applied in the case of irregular column
widths[15]. A circular fit of this kind can be shown second-
order accurate when the mesh is regular enough[3], although
is only first-order accurate when the mesh is non-orthogonal or
nonlinear. Because the calculation of surface tension does not
rely on mesh regularity, surface tension can be incorporated as
a body force during Lagrangian time steps, allowing full ALE
simulations with properly coupled surface tension effects.
Fig. 2. Visual depiction of surface tension calculation using height functions
in 2D.
In 3D, six points are chosen for a linear least-squares
polynomial fit of the boundary with a quadratic polyno-
mial P(i, j) of the logical coordinates[12]. The polynomial’s
derivatives are then mapped through the mesh Jacobian to find
first and second derivatives of the corresponding approximate
parameterization p(x, y) in real coordinates, which are used to
calculate the mean curvature of the surface. Since the current
3D curvature calculation relies on a linear approximation of
the mesh, the surface tension force can only be applied after
the remap phase, and with mesh adaptation disabled.
III. RECENT APPLICATIONS AREAS
PISALE has been applied to a wide range of applications.
One early application was the protection of optics and diagnos-
tics at the National Ignition Facility (NIF) and other large laser
facilities [6]. It was also used to model ion heating experiments
at Neutralized Drift Compression Experiment-II (NDCX-II)
to study Warm Dense Matter (WDM) properties [2]. One of
the early applications that used the surface tension capability
of the code was modeling the dynamics of tin droplets for
extreme ultraviolet (EUV) lithography [22]. Our recent ap-
plications include groundwater flow simulations for Hawaiian
aquifers[23], impacts of particulates and rain droplets on hy-
personic vehicles[27], and a range of experiments at Stanford
Linear Accelerator Center (SLAC), a Department of Energy
(DOE) Office of Science User Facility. Experiments include
X-ray Free Electron Laser (XFEL) beam heating of hydrogen
and water droplets[18] and interaction of laser produced proton
beams with different materials. We summarize recent work in
these three new application areas.
A. Groundwater Flow and Transport in Pacific Islands
Density-dependent miscible seawater-freshwater systems in
island aquifers have been previously modeled with finite
element or finite volume methods in an Eulerian framework.
These simulations however, are deemed inadequate due to
inappropriate grids and problems with scalability. Due to the
density dependency, the governing Darcy-type flow equation
is nonlinearly coupled with the mass and/or heat transport
equations with a constitutive relationship, ρ = ρ(p, T, c) where
ρ is the density, p is the pressure, T is the temperature, and c
is the salinity.
For seawater-freshwater simulation in the subsurface, the
isothermal assumption, e.g., ρ = ρ(c) under Boussinesq
approximation is typically used for coupled flow and transport
with appropriate boundary conditions [5]:
∇ · q = 0
(4)
q = −ρ0gk
µ
 ∇p
ρ0g + ρ − ρ0
ρ0
∇z

(5)
ϕ∂c
∂t + ∇ · (qc − ϕDeff∇c) = 0
(6)
where q is the fluid flux, ρ0 is the density at the reference c
and T, g is the gravity constant, k is the permeability tensor
of the porous medium, µ is the dynamic viscosity, ϕ is the
porosity, and Deff is the dispersion coefficient tensor.
Fig. 3. Conservative tracer transport with heterogeneous hydraulic conduc-
tivity field at t = 0 yr (left) and 37.8 yrs (right).
A critical previously unaddressed challenge is the simulation
of salinity transport in a large-scale aquifer with a spatially
heterogeneous permeability field. Typically, and unsatisfac-
torily, coarse meshes with large diffusion coefficients (i.e.,
dispersivities) are implemented as numerical compromises
at the cost of overestimated mixing physics. The simulated
groundwater flow velocities are then used for reactive transport
22
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

modeling, which often leads to the underestimated residence
time of contaminant plumes and associated overly promising
groundwater remediation decisions with increasing risks of
cleanup operation failure. The flexibility of PISALE, with its
ability to define interfaces and the use of AMR to dynamically
define the grid, addresses these issues as shown in Figure
3. We show convective tracer transport with a heterogeneous
hydraulic conductivity field and its corresponding groundwater
velocity. Additionally, PISALE generally reduces simulation
time to a tractable problem via the parallel HPC performance
of the underlying scalable SAMRAI library.
Fig. 4.
Recent comparison of PISALE results with two openly published
results from large code efforts (neither is open source) and experimental
impact data.
B. Hypervelocity impacts
Impacts by particulates and rain droplets on hypersonic
(Mach 5 and above) vehicles pose a major concern for safety
of flight. It is understood that particulates of even micron-
size are capable of impacting the surface quality of the
vehicle [9]. Therefore, larger particles are likely to induce
more surface damage and additional material erosion. To make
the result more complex, the particles must traverse a shock
discontinuity formed from hypersonic flight. As a particle
traverses the shock, the post-shock conditions may result in
a pressure, density, and temperature differential that leads to
a spatio-temporal dependent particle shape change prior to
making contact with the surface. PISALE offers the capability
to capture such high-rate loading in a computationally cost
effective manner with the implementation of AMR. PISALE
has been verified and validated in its ability to capture hy-
pervelocity impact phenomena of well-characterized ductile
metals. Figure 4 shows impact crater depth in Al 6061-T6
as a function of time caused by an aluminum projectile with
PISALE results in excellent agreement with experimental data
[20]. The projectile has a diameter of 6.35mm and a velocity
of 7 km/s. Figure 5 shows the PISALE simulation of the
impact at three times.
Fig. 5. PISALE simulation showing time of impact and crater at 5 and 15
µs.
t = 4.5 ns
PISALE Simulation
SLAC Optical Image
XFEL heating
water droplet
Fig. 6. Comparison of PISALE simulation of a water droplet heated by an
XFEL with experimental image.
C. Modeling of SLAC experiments using XFEL beams
XFEL beams can be used to produce and study matter under
extreme conditions. Liquids provide a means of bringing sam-
ples into the path of the X-ray beam for analysis. Ultrabright
x-rays can blow up samples within a tiny fraction of a second,
and modeling the dynamics of this interaction is critical to
understanding which droplets can be used to collect data.
These ultrashort (∼ 50 fs), high energy (∼ 8 keV), coherent
x-ray sources can be used to both heat and probe targets. We
discuss some recent results in modeling XFEL heating of water
and hydrogen droplets. Figure 6 shows early time dynamics
from a 3D PISALE simulation of a water droplet heated by
an XFEL beam with a comparison to an experimental optical
image [24] at the Coherent X-ray Imaging (CXI) instrument
at SLAC. Previous work [19] modeling early stages of liquid-
water-drop explosion showed 1D and 2D results assuming
axisymmetry. This assumption is broken for later times due to
distortions of the cavity boundary [19]. Therefore, the need of
a 3D simulation not only as a diagnostic tool for early times,
23
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

but also for modeling reflection and spallation mechanisms
that lead to fragmentation and expansion observed in water
under extreme conditions at later times [25].
The need for High Repetition Rate (HRR) targets in High
Energy Density Science has been stated many times as the
areas moves towards HRR facilities [21] [13]. When using
droplets as HRR targets, fratricide can occur. This is caused
by the explosion of a heated droplet impacting the following
droplet, which can make it ineffective for the following shot.
The need to skip this perturbed droplet causes a reduction in
the effective repetition rate and data acquisition quality. Figure
7 shows a 3D PISALE simulation of three hydrogen droplets
with the first droplet (lowest in the image) already exploded
after being heated by an XFEL beam. The image is 9 ns after
the XFEL beam passes through the first droplet. The resulting
explosion creates debris seen impacting the following droplets.
The middle droplet is clearly perturbed by the debris causing
a reduction of the effective repetition rate factor of this target
system by at least by factor of 2 because this perturbed droplet
cannot be used as a target. The XFEL beam travels along the
x-axis, which passes through the center of the lowest droplet.
The orientation (not the location) of the three axes is shown
for the inline and side views.
Fig. 7.
Three dimensional PISALE simulation of three hydrogen droplets.
The droplet following the XFEL heated droplet shields the second following
droplet. The inline view is along the XFEL beam.
Surface tension can impact droplet dynamics in the lower
temperature regions of this experiment. Early simulations show
that enabling the surface tension package impacts the large-
scale dynamics of the system. Figure 8 shows the side views of
3D PISALE simulations of hydrogen droplets with and without
including surface tension effects. Even with surface tension
forces included, the following droplet is still significantly
perturbed by the exploding droplet. There is experimental
evidence of larger water droplets heated by XFEL beams
perturbing following droplets [24] but to our knowledge no
published results for hydrogen droplets. Our modeling results
for hydrogen show that fratricide is also a concern for these
droplets and reduces the effective repetition rate.
Fig. 8. Effects of surface tension are important for modeling. Comparison of
PISALE simulation of hydrogen droplet heated by an XFEL without surface
tension model (left) and including surface tension (right) at t = 8 ns.
IV. CONCLUSIONS AND FUTURE WORK
The PISALE code is being used in a wide variety of new
areas showing excellent agreement with experimental data and
contributing to a basic understanding of the physical processes.
Three dimensional simulations are critical to most of these
physical processes, and this is enabled through modern HPC
platforms and appropriate numerical techniques and program-
ming models. The combination of AMR with ALE is a pow-
erful numerical approach with unique capabilities. Significant
physics effects can be included in the numerical methods with
careful implementation of the operator splitting approach. We
are exploring paths discretize a wider array of PDEs relevant
to plasma physics, mesoscale material modeling, seismicity
in geophysics, and electromagnetics. We are also exploring a
FEM framework with 3D AMR capability that would improve
the existing heat conduction, radiation diffusion, and surface
tension models in PISALE.
ACKNOWLEDGMENT
We would like to thank additional members of the PISALE
team for their expertise and contribution to the code. The
PISALE code is supported by the National Science Founda-
tion, under Office of Advanced Cyber Infrastructure Award
Number 2005259, the U.S. Department of Energy, Office of
Science, under Fusion Energy Sciences Research Division
Award Numbers DE-SC0021374 and DE-SC0023475, and the
Office of Naval Research (ONR), under ONR MURI Award
Number N00014- 20-1-2682. This research used resources of
the National Energy Research Scientific Computing Center
(NERSC), a U.S. Department of Energy Office of Science
User Facility located at Lawrence Berkeley National Lab-
oratory, operated under Contract No. DE-AC02-05CH11231
using NERSC awards ERCAP0024761 and EERCAP0024762.
The technical support and advanced computing resources
from University of Hawaii Information Technology Services
– Cyberinfrastructure, funded in part by the National Science
24
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences

Foundation MRI award Number 1920304, are gratefully ac-
knowledged.
REFERENCES
[1]
R. W. Anderson, N. S. Elliott, and R. B. Pember. “An ar-
bitrary Lagrangian-Eulerian method with adaptive mesh
refinement for the solution of the Euler equations”. In:
Journal of Computational Physics 199.2 (Sept. 2004),
pp. 598–617.
[2]
J. Barnard et al. “NDCX-II target experiments and
simulations”. In: Nuclear Instruments and Methods in
Physics Research Section A: Accelerators, Spectrome-
ters, Detectors and Associated Equipment 733 (2014),
pp. 45–50.
[3]
G. Bornia, A. Cervone, S. Manservisi, R. Scardovelli,
and S. Zaleski. “On the properties and limitations of the
height function method in two-dimensional Cartesian
geometry”. In: Journal of Computational Physics 230.4
(2011), pp. 851–862.
[4]
J. U. Brackbill, D. B. Kothe, and C. Zemach. “A
continuum method for modeling surface tension”. In:
J. Comput. Phys. 100 (1992), pp. 335–354.
[5]
H.-J. G. Diersch. FEFLOW: finite element modeling of
flow, mass and heat transport in porous and fractured
media. Springer Science & Business Media, 2013.
[6]
D. C. Eder, A. C. Fisher, A. E. Koniges, and N. D.
Masters. “Modelling debris and shrapnel generation
in inertial confinement fusion experiments”. In: Nucl.
Fusion 53 (2013), p. 113037.
[7]
A. C. Fisher et al. “Modeling heat conduction and
radiation transport with the diffusion Equation in NIF
ALE-AMR”. In: Journal of Physics: Conference Series
244 (2010), p. 022075.
[8]
B. T. N. Gunney and R. W. Anderson. “Advances
in patch-based adaptive mesh refinement scalability”.
In: Journal of Parallel and Distributed Computing 89
(2016), pp. 65–84.
[9]
J. B. Habeck, M. D. Kroells, T. E. Schwartzentruber,
and G. V. Candler. “Characterization of particle-surface
impacts on a sphere-cone at hypersonic flight condi-
tions”. In: AIAA SCITECH 2023 Forum AIAA 2023-
0205 (2023).
[10]
A. Koniges et al. “Multi-material ALE with AMR for
modeling hot plasmas and cold fragmenting materials”.
In: Plasma Sci. and Technol. 17 (2015), pp. 117–128.
[11]
W. Liu et al. “Using a Korteweg-type model for model-
ing surface tension and its application”. In: 54th Annual
Meeting of the APS Division of Plasma Physics abstract
id. BP8.058 (2012).
[12]
W. Liu et al. “Surface tension models for a multi-
material ALE code with AMR”. In: Computers and
Fluids 151 (2017), pp. 91–101.
[13]
T. Ma et al. “Accelerating the rate of discovery: Toward
high-repetition-rate HED science”. In: Plasma Physics
and Controlled Fusion 63.10 (2021), p. 104003.
[14]
N. D. Masters, T. B. Kaiser, R. W. Anderson, D. C.
Eder, A. C. Fisher, and A. E. Koniges. “Laser ray trac-
ing in a parallel arbitrary Lagrangian-Eulerian adaptive
mesh refinement hydrocode”. In: Journal of Physics:
Conference Series 244 (2010), p. 032022.
[15]
J. McKee, Y. Mileyko, A. Fisher, and A. Koniges.
“Developing a modern CFD framework with parallel
algorithms and mesh adaption”. In: Eleventh Interna-
tional Conference on Computational Fluid Dynamics
(ICCFD11) (2022). Paper 1301.
[16]
MFEM. https://mfem.org/. Accessed: 2023-09-18.
[17]
MOOSE. https://mooseframework.inl.gov/. Accessed:
2023-09-18.
[18]
C. Parisuana, D. C. Eder, M. Gauthier, C. Schoen-
waelder, C. A. Stan, and S. H. Glenzer. “CFD modeling
of droplets heated by an x-ray free electron laser”.
In: Eleventh International Conference on Computational
Fluid Dynamics (ICCFD11) (2022). Paper 2003.
[19]
T. Paula, S. Adami, and N. A. Adams. “Analysis of the
early stages of liquid-water-drop explosion by numeri-
cal simulation”. In: Physical Review Fluids 4.4 (2019),
p. 044003.
[20]
E. Pierazzo et al. “Validation of numerical codes for
impact and explosion cratering: Impacts on strengthless
and metal targets”. In: Meteoritics & Planetary Science
43.12 (2008), pp. 1917–1938.
[21]
I. Prencipe et al. “Targets for high repetition rate laser
facilities: needs, challenges and perspectives”. In: High
Power Laser Science and Engineering 5 (2017), e17.
[22]
M. A. Purvis et al. “Advancements in predictive plasma
formation modeling”. In: Proc. SPIE 9776. Extreme
Ultraviolet (EUV) Lithography VII. 2016, 97760K–
97760K.
[23]
Y. Seo, J. Lee, A. Koniges, and A. Fisher. “Develop-
ment of the PISALE codebase for simulating flow and
transport in large-scale coastal aquifer”. In: Eleventh
International Conference on Computational Fluid Dy-
namics (ICCFD11) (2022). Paper 1502.
[24]
C. A. Stan et al. “Liquid explosions induced by X-ray
laser pulses”. In: Nature Physics 12.10 (2016), pp. 966–
971.
[25]
C. A. Stan et al. “Negative pressures and spallation in
water drops subjected to nanosecond shock waves”. In:
The journal of physical chemistry letters 7.11 (2016),
pp. 2055–2062.
[26]
M. L. Wilkins. Computer simulation of dynamic phe-
nomena. Springer Science & Business Media, 1999.
[27]
P. T. Yip et al. “Arbitrary Lagrangian Eulerian simula-
tions of high speed particle impacts encountered during
hypersonic flight”. In: Eleventh International Confer-
ence on Computational Fluid Dynamics (ICCFD11)
(2022). Paper 2202.
25
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-107-7
ADVCOMP 2023 : The Seventeenth International Conference on Advanced Engineering Computing and Applications in Sciences
Powered by TCPDF (www.tcpdf.org)

