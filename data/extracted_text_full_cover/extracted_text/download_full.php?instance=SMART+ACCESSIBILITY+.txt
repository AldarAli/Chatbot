SMART ACCESSIBILITY 2017
The Second International Conference on Universal Accessibility in the Internet of
Things and Smart Environments
ISBN: 978-1-61208-589-0
March 19 – 23, 2017
Nice, France
SMART ACCESSIBILITY 2017 Editors
Lukas Smirek, Stuttgart-Media University, Germany
Daniela Marghitu, Auburn University - Auburn, USA
Kristin Skeide Fuglerud, NR–Norsk Regnesentral, Norway

SMART ACCESSIBILITY 2017
Forward
The Second International Conference on Universal Accessibility in the Internet of Things and
Smart Environments (SMART ACCESIBILITY 2017) was held in Nice, France, March 19 - 23, 2017.
There are several similar definitions for universal accessibility, such as design for all, universal
design, inclusive design, accessible design, and barrier free design.
These and similar
approaches are relevant to this conference. The focus will be on methods, tools, techniques
and applications for human diversity, social inclusion and equality, enabling all people to have
equal opportunities and to participate in the information society.
The accepted papers covered topics such as accessibility by design, digital inclusion,
accessibility devices and applications.We believe that the SMART ACCESIBILITY 2017
contributions offered a large panel of solutions to key problems in areas of accessibility.
We take here the opportunity to warmly thank all the members of the SMART ACCESIBILITY
2017 technical program committee as well as the numerous reviewers. The creation of such a
broad and high quality conference program would not have been possible without their
involvement. We also kindly thank all the authors that dedicated much of their time and efforts
to contribute to the SMART ACCESIBILITY 2017. We truly believe that thanks to all these efforts,
the final conference program consists of top quality contributions.
This event could also not have been a reality without the support of many individuals,
organizations and sponsors. In addition, we also gratefully thank the members of the SMART
ACCESIBILITY 2017 organizing committee for their help in handling the logistics and for their
work that is making this professional meeting a success.
We hope the SMART ACCESIBILITY 2017 was a successful international forum for the exchange
of ideas and results between academia and industry and to promote further progress in the
universal acccesibility field.
We also hope that Nice provided a pleasant environment during the conference and everyone
saved some time for exploring this beautiful city.
SMART ACCESIBILITY 2017 Chairs
Lukas Smirek, Stuttgart-Media University, Germany
Kristin Skeide Fuglerud, NR–Norsk Regnesentral, Norway
Christian Bühler, University of Dortmund, Germany

Daniela Marghitu, Auburn University - Auburn, USA
Simeon Keates, University of Greenwich, UK

SMART ACCESSIBILITY 2017
COMMITTEE
SMART ACCESSIBILITY Advisory Committee
Lukas Smirek, Stuttgart-Media University, Germany
Kristin Skeide Fuglerud, NR–Norsk Regnesentral, Norway
Christian Bühler, University of Dortmund, Germany
Daniela Marghitu, Auburn University - Auburn, USA
Simeon Keates, University of Greenwich, UK
SMART ACCESSIBILITY 2017 Technical Program Committee
Margherita Antona, ICS-FORTH, Greece
Lars Ballieu Christensen, Sensus ApS, Denmark
Anthony Lewis Brooks, Aalborg University Esbjerg, Denmark
Christian Bühler, University of Dortmund, Germany
Iyad Abu Doush, American University of Kuwait, Kuwait
Deborah Fels, Ryerson University, Canada
Simeon Keates, University of Greenwich, UK
Georgios Kouroupetroglou, National and Kapodistrian University of Athens, Greece
Daniela Marghitu, Auburn University - Auburn, USA
Paulo Martins, University of Trás-os-Montes e Alto Douro (UTAD), Portugal
Elisabeth Metais, Le Cnam, France
Christos Mettouris, University of Cyprus, Cyprus
Yehya Mohamad, Fraunhofer FIT, Sankt Augustin, Germany
Gerhard Nussbaum, Competence Network Information Technology to Support the Integration of People
with Disabilities (KI-I), Linz, Austria
Hugo Paredes, INESC TEC / University of Trás-os-Montes e Alto Douro, Portugal
Mansoor Pervaiz, Northeastern University, USA
Franz Pühretmair, Competence network information technology to support the integration of people
with disabilities (KI-I), Austria
Sashko Ristov, University of Innsbruck, Austria
Kristin Skeide Fuglerud, NR–Norsk Regnesentral, Norway
Lukas Smirek, Stuttgart-Media University, Germany
Cristian Stanciu, University Politehnica of Bucharest, Romania
Nadine Vigouroux, IRIT | Paul Sabatier University, Toulouse, France
Konstantinos Votis, Information Technologies Institute | Centre for Research and Technology Hellas,
Greece

Copyright Information
For your reference, this is the text governing the copyright release for material published by IARIA.
The copyright release is a transfer of publication rights, which allows IARIA and its partners to drive the
dissemination of the published material. This allows IARIA to give articles increased visibility via
distribution, inclusion in libraries, and arrangements for submission to indexes.
I, the undersigned, declare that the article is original, and that I represent the authors of this article in
the copyright release matters. If this work has been done as work-for-hire, I have obtained all necessary
clearances to execute a copyright release. I hereby irrevocably transfer exclusive copyright for this
material to IARIA. I give IARIA permission or reproduce the work in any media format such as, but not
limited to, print, digital, or electronic. I give IARIA permission to distribute the materials without
restriction to any institutions or individuals. I give IARIA permission to submit the work for inclusion in
article repositories as IARIA sees fit.
I, the undersigned, declare that to the best of my knowledge, the article is does not contain libelous or
otherwise unlawful contents or invading the right of privacy or infringing on a proprietary right.
Following the copyright release, any circulated version of the article must bear the copyright notice and
any header and footer information that IARIA applies to the published article.
IARIA grants royalty-free permission to the authors to disseminate the work, under the above
provisions, for any academic, commercial, or industrial use. IARIA grants royalty-free permission to any
individuals or institutions to make the article available electronically, online, or in print.
IARIA acknowledges that rights to any algorithm, process, procedure, apparatus, or articles of
manufacture remain with the authors and their employers.
I, the undersigned, understand that IARIA will not be liable, in contract, tort (including, without
limitation, negligence), pre-contract or other representations (other than fraudulent
misrepresentations) or otherwise in connection with the publication of my work.
Exception to the above is made for work-for-hire performed while employed by the government. In that
case, copyright to the material remains with the said government. The rightful owners (authors and
government entity) grant unlimited and unrestricted permission to IARIA, IARIA's contractors, and
IARIA's partners to further distribute the work.

Table of Contents
OpenAPE - A Framework for Personalised Interaction in Smart Environments
Lukas Smirek, Patrick Munster, and Gottfried Zimmermann
1
Enhancing Accessibility Information in Google Maps
Paloma Caceres, Almudena Sierra-Alonso, Carlos E. Cuesta, Jose Maria Cavero, and Belen Vela
6
The Clinical Potential of a Cognitive Training Program Embedded in an Adaptive Video Game
Martina Ratto, John Harrison, Keiron Sparrowhawk, and Paul Cliveden
10
Experimental Study on User Acceptance and Affordability of Intelligent Wheelchair -Questionnaires on Human
Machine Interface-
Naohisa Hashimoto, Ali Boyali, Yusuke Takinami, and Osamu Matsumoto
14
Inclusion of Down Syndrome in Architectural Design: Towards a Methodology
Clementine Schelings and Catherine Elsen
20
The Development of a Sharing System for Virtual Graffiti of Tourism Information among Tourists using Image
Recognition
Rei Miyagawa, Keima Kumano, Takayuki Kunieda, Tetsuya Ikeda, Naka Gotoda, Masanobu Kii, and Rihito
Yaegashi
26
Powered by TCPDF (www.tcpdf.org)

 
 
 OpenAPE  
A framework for personalised interaction in smart environments 
 
Lukas Smirek, Patrick Münster, Gottfried Zimmermann 
Stuttgart Media University 
Stuttgart, Germany 
e-mail: {smirek, muenster, gzimmermann}@hdm-stuttgart.de 
 
 
Abstract—In this contribution, we describe our preliminary 
work on openAPE - the open Accessibility and Personalization 
Extension framework. The main goal of the framework is to 
transfer platform independent context information from one 
device to another and infer personalized settings for user 
interface and device adaptation according to the user’s needs. 
This shall contribute to improved usability and accessibility in 
smart environments. Two exemplary use cases are described, 
to illustrate in which contexts the framework can be used. 
Keywords-openAPE; 
Adaptive 
User 
Interfaces;Smart 
Environments; Eclipse Smart Home; Create@school 
I. 
 INTRODUCTION 
During the last decades, information and communication 
technology (ICT) has found brought entrance in our 
everyday lives. This trend is expected to go on for the next 
years. Thereby, it is not only the amount of electronic 
devices and services that will increase but also their 
interdependencies and network capabilities. Overall, we can 
find more and more setups of smart environments in our 
surroundings. The field of Smart Environments and its 
subdomain of Ambient Assisted Living (AAL) have the 
high potential to support regular users as well as the elderly 
and people with disabilities in their everyday lives [1].  
It is also expected that the way we interact with smart 
environments will differ from the way we interact with 
current ICT in the sense that interaction patterns will change 
from explicit to implicit and more natural ones [2][3]. 
Considering the huge amount of interconnected devices 
and new interaction patterns it must be assured that 
everyone, independent of age, computer skills, cultural 
background or disability is still able to profit from these 
developments. One major obstacle might however be 
inaccessible user interfaces (UI) [1]. Since almost everyone 
in the society will be affected from that, the users’ 
requirements 
for 
accessible 
UIs 
might 
be 
very 
heterogeneous and sometimes even contradictory [4]. 
Therefore, it will be difficult to follow a one-size-fits-all 
approach and there will appear the need for personalized 
UIs in smart environments that take the individual user 
needs and preferences into account. Due to the huge amount 
of devices that users will face in future smart environments, 
it will be exhausting to adapt every device by hand to the 
user’s needs. To transfer settings from one device to another 
is therefore a major step for enabling wide spread usable 
and accessible UIs. This is in line with the development 
goals of openAPE – the open Accessibility Personalization 
Extension [5].     
The paper is structured as follows. In Section 2, we 
describe an illustrative use case and infer requirements for 
personalization in smart environment scenarios.  The 
consecutive Section 3 will give a brief overview of related, 
existing systems. In Section 4, we will explain our 
approach. In Section 5, we will describe two research 
projects in which our framework is used.  
II. 
USE CASE AND REQUIREMENTS FOR 
PERSONALISATION IN SMART ENVIRONMENTS 
Let us think about a visually impaired businessperson 
from Germany. In Germany, he is living in his own 
apartment equipped with different devices like a smart TV, 
a lighting system and a smart heating system. He has 
configured his home in a way, that when he switches to TV 
mode the TV set is switched on and the light is dimmed 
down. He has also set a preferred room temperature. He can 
control the status of his home via his smart phone. The 
smart phone is configured with large font size and strong 
contrast. 
Now, the businessperson has to travel to China. When he 
arrives in his hotel room, he immediately notices that the 
air-conditioning system has cooled the room too much. He 
approaches the control panel at the wall that is connected to 
the 
openAPE 
infrastructure. 
The 
businessperson 
authenticates himself via a RFID tag and the panel connects 
to the openAPE infrastructure to look up the users preferred 
settings. Among other information there is stored that the 
user has configured its smart phone with a larger font size 
and stronger contrast and that his preferred language is 
German. Therefore, the control panel reads aloud a short 
welcome message with some basic explanations. It also 
increases the font size and contrast and downloads all text 
labels for the UI in German language. Furthermore, it 
proposes the user’s preferred room temperature. 
Some weeks later, a deaf person stays in the same hotel. 
For him there are no adjustments made with regard to font 
size and contrast. However, since the person has problems 
1
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

 
 
with written language, for him the welcome message and all 
help texts are displayed as sign language videos. With 
regard to this use case, the following requirements for 
personalization in smart environments can be deduced: 
When looking at different systems from the fields of 
smart homes, AAL, context  aware computing and mobile 
computing,  the following development goals can be 
identified: 
 
Interoperability: users will have to cope with different 
back-end technologies that must be integrated (e.g., in 
Smart Homes the integration of existing and new 
devices/systems).  
 
Device and service overarching use cases: in most 
cases, users do not want to make use of a single device 
or service only. Instead, several ones should be 
integrated to help the user perform a task. 
 
Adaptive UIs: to provide the best user experience, a UI 
should adjust to the context of use (user preferences, 
environmental conditions, technical conditions and the 
current task). It is also important, that in smart 
environments the possibilities of traditional GUIs are 
restricted and adaptivity must be considered from a 
generic interaction perspective <>. 
 
Adaptivity independent of controller device and place: 
in smart environments, users will move around and will 
interact with different devices. Hence, it is important 
that user preferences required for adaptation can be 
shared between devices. 
 
Openness for third party contributions: for the average 
UI developer it is difficult to develop user interfaces for 
people with disabilities, hence the system must enable 
the injection of expert knowledge [4]. 
 
III. 
RELATED WORK 
The Eclipse Smart Home project [6] deals with 
interoperability problems and device abstraction in Smart 
Homes. The existing UIs and rule engine enable device 
overarching use cases and automatization. However, UI 
personalization can be achieved only to a very low degree 
by hand[7]. Other frameworks like AllJoyn [8] or OCF [9]  
provide abstract descriptions of device functions and states 
that can be accessed by a UI. The device models could be 
used to auto-generate UIs. However, currently there seem to 
be no adaptation engines available. Furthermore, the models 
mainly contain information about data types that shall be 
displayed in UI elements. Anyway, Mayer at al. [10] claims 
that  this kind of information enables the generation of very 
simplistic UIs only. The authors argue that not only data 
types, but also the semantic of the interaction should be 
modeled. 
The 
authors 
present 
their 
own 
solution 
accordingly. 
Projects like Supple [11] or MyUI [12] have tried to 
provide adaptive UIs, but relay on application models that 
do not abstract from devices and backend technologies and 
are consequently difficult to use in smart environments. The 
Universal Remote Console (URC) [13] and its runtime 
implementation the Universal Remote Hub [14] provide 
abstract device descriptions and a mechanism for 
exchanging personalized UIs to one or several devices. 
Furthermore, the URC framework leverages the concept of a 
resource server to provide specialized UI resources. This 
enables third party UI contributions like labels in different 
languages, icon sets, sign language videos, etc., (e.g., from 
accessibility experts) [15], even at runtime. However, an 
adaptation engine is missing. 
MyUI 
provides 
a 
mechanism 
for 
third 
party 
contributions. Nevertheless, in the URC framework, 
specialized UI content can be provided, while in MyUI only 
a generic interaction pattern for a certain interaction 
situation can be contributed, but no content. 
The Global Public Inclusive Infrastructure [16] provides 
a mechanism to transfer platform independent user 
preferences from one device to another, but lacks an 
adaptation engine.  
IV. OPENAPE 
When developing the openAPE framework [17][5] the focus 
was to address the following requirements: 
 
enable a platform independent mechanism to transfer 
context of use data from one device to another 
 
provide adaptation and UI settings information 
independent of place 
 
Enable third parties to contribute specialized content 
 
Provide specialized content independent of place. 
 
Considering these developments, it is clear that there are 
overlapping with some other technologies, mainly GPII and 
URC. Nevertheless, there are some important differences. 
Similar to URC, openAPE implements the concept of a re-
source server. However, OpenAPE ships with a context 
management infrastructure, something that is missing in 
URC. 
GPII also provides a way to exchange context data (mainly 
user preferences regarding UI settings). OpenAPE differs in 
this case in the sense that it is not implemented as a 
monolithic system like GPII; instead, it is a very lightweight 
RESTFUL web service. It is also further in its API 
specification. OpenAPE is the reference implementation of 
ISO/IEC 24752-8 [18]that has already reached the status of a 
Committee Draft.  
A. Main components 
The openAPE infrastructure shown in Figure 1 is based 
on the specifications defined in ISO/IEC 24752-8 [18]. The 
main services are the following: 
 
Context services that can be used by any device to 
upload 
user 
preferences/settings, 
equipment, 
environment and task contexts (context of use) in order 
to make them globally available. 
2
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

 
 
 
 Listing service that can be requested to get  
recommendations for UI settings and adaptations 
 
Resource service to make additional UI components 
available 
 
A feedback service to rate the proposed solution.  
B. The related workflow is as follows: 
 
 
Figure 1: OpenAPE Architecture 
 
1. A user personalizes a device according to his needs 
(e.g., font size, language etc.). 
2. The device creates different context objects that contain 
the relevant user settings and for which context they 
were made. These contexts are uploaded to the 
corresponding context web services. 
3. In a next step, the user can approach any other device 
connected to the openAPE infrastructure and can 
authenticate himself. 
4. After the authentication, this second device uploads the 
current context conditions (equipment, environment and 
task context).  
5. In a next step, it sends a request message to the listing 
service in order to obtain information about optimized 
UI settings and additional UI resources. Thereby it 
refers to the uploaded context information.  
6. The listing service starts a matchmaking mechanism to 
infer the recommended UI settings and adaptations. 
7. The listing service exposes the recommended settings 
to the client. 
8. The client downloads the recommendations and adjusts 
its UI. 
9. If mentioned in the recommendations, the device can 
download additional UI resources. 
10. Optionally, the client gives feedback on the quality of 
the recommended settings to openAPE.  
V. 
APPLICATIONS 
A. Eclipse Smart Home 
Smart Homes and AAL yield the high potential to enable 
a longer independent life for the elderly and people with 
disabilities. 
However, 
as 
mentioned 
before, 
such 
environments must be adjusted to the users in order to let 
them exploit the full advantages of these technologies. The 
Eclipse Smart Home (ESH) is an open source framework 
addressing not only the field of smart homes but also of 
AAL. As pointed out in [7] personalization features are 
currently not very far developed yet. Nevertheless, the ESH 
framework provides enough connecting factors that enable 
the establishment of personalization features. Therefore, 
concepts from the URC framework and a connection 
between ESH and openAPE will be utilized. 
Our goal is to develop a module that is deployed inside 
the ESH runtime and that connects to the openAPE 
infrastructure. The module can upload different context data 
such as devices being connected to the ESH server 
(equipment context) or environmental data (environment 
context). These data are than used to either download 
additional UIs or configuration data and automation rules. 
Downloading 
automatization 
rules 
goes 
far 
beyond 
personalization of UIs. They enable to personalize the 
behavior of the whole environment.  
B. Creat@School 
Playing games is a popular leisure activity for young 
people, it makes them focusing onto problem, accept 
challenges and push them further. Also creating games is a 
very motivating challenge, but creating a game seems to be a 
difficult task. Therefore, the Pocket Code app was created. It 
allows students to program small applications easily directly 
on their smartphone without the requirement for any 
additional hardware or learning a programming language. 
Within the context of the “No One Left Behind” (NOLB) 
project, mobile game-based learning should be integrated 
into school curricula. 
Create@School is an enhanced version of Pocket Code 
which integrates the results of the pilot studies with teachers 
and students. One of these extensions is our integration of 
personalization features in Create@School to use the 
openAPE framework and to make the app more useable for 
various user groups. Software programs, mobile apps and 
websites have a default UI that tries to cater for many people, 
but that is often unsuitable for people with special needs. 
Many such programs are adaptable, but onside observations 
3
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

 
 
has shown that most people never adapt the settings of the 
software they use. This may be because they think the 
default settings are all there is, because they are afraid of 
breaking something or because it is too difficult. Another 
reason is that devices at schools are not used every lesson by 
the same student. For this reason, the students can choose a 
predefined profile in Create@School whenever they want. 
The chosen profile only influences the Create@School app 
and not the general device settings.   
Therefore, we have provided the profile changing option 
direct within the Create@School settings menu and have 
made them selectable by the name of mythological 
characters to make them more distinctive and attractive for 
the user.  
At the moment Create@School has five profiles 
(includign the standard profile) to personalize the UI. 
Individual settings are stored on the device and not in the 
openAPE framework. Therefore, if a pupil uses another 
device, his settings are not available on this one and 
everything must be customized manual again. 
For this reason, the next step is to develop GPII enabled 
featured to provide individual profiles which are stored in the 
cloud and which are available on every device. All these 
further individual personalization features will be developed 
in the openAPE project, which will provide GPII enabled 
services to auto-adapt the Create@School UI to the user's 
needs and desires. 
VI. 
CONCLUSION 
At this stage, the matchmaking algorithm to infer the 
settings recommendations is a very simplistic one. In the 
future, we will work towards a more advanced one. Rule 
based solutions as well as such that utilize concepts from 
machine learning can be thought of. Furthermore, we will 
work on use cases in the field of smart homes and e-
learning. Furthermore, the system must be evaluated with 
regard to different dimensions. First of all, there is the 
technical dimension. Is the REST infrastructure robust 
under high load, especially if there are frequent changes of 
context conditions? Next, it must be evaluated, wheater 
developers of adaptive applications see a benefit in using a 
REST API? 
ACKNOWLEDGMENT 
The research leading to these results has received funding 
from the European Union Seventh Framework Program 
(FP7/2007-2011) under grant agreement no. 610510, 
Prosperity4All (”Ecosystem infrastructure for smart and 
personalized 
inclusion 
and PROSPERITY 
for 
ALL 
stakeholders”) as well as under the EC H2020 Innovation 
Action No One Left Behind, Grant Agreement No. 645215. 
This publication reflects only the authors’ views and the 
European Union is not liable for any use that may be made 
of the information contained herein. 
 
 
 
REFERENCES 
[1] P. L. Emiliani and C. Stephanidis, “Universal access to 
ambient intelligence environments: opportunities and 
challenges for people with disabilities,” IBM Systems 
Journal, vol. 44, no. 3, pp. 605–619, 2005. 
[2] J. Nehmer, M. Becker, A. Karshmer, and R. Lamm, “Living 
assistance systems: an ambient intelligence approach,” in 
Proceedings of the 28th international conference on Software 
engineering, 2006, pp. 43–50. 
[3] M.-R. Tazari, “An open distributed framework for adaptive 
user interaction in ambient intelligence,” in International 
Joint Conference on Ambient Intelligence, 2010, pp. 227–
238. 
[4] M. Peissner, A. Schuller, D. Ziegler, C. Knecht, and G. 
Zimmermann, “Requirements for the Successful Market 
Adoption of Adaptive User Interfaces for Accessibility,” in 
Universal Access in Human-Computer Interaction. Design 
for All and Accessibility Practice, C. Stephanidis and M. 
Antona, Eds. Springer International Publishing, 2014, pp. 
431–442. 
[5] “openAPE,” Home, 2017. [Online]. Available: http://gpii.eu. 
[Accessed: 26-Feb-2017]. 
[6] “Eclipse SmartHome - A Flexible Framework for the Smart 
Home,” 2016. [Online]. Available: 
http://www.eclipse.org/smarthome/. [Accessed: 26-Feb-
2017]. 
[7] L. Smirek, G. Zimmermann, and M. Beigl, “Just a Smart 
Home or Your Smart Home – A Framework for Personalized 
User Interfaces Based on Eclipse Smart Home and Universal 
Remote Console,” Procedia Computer Science, vol. 98, pp. 
107–116, 2016. 
[8] AllSeen Alliance, “AllJoyn Framework,” 2016. [Online]. 
Available: https://allseenalliance.org/framework. [Accessed: 
26-Feb-2017]. 
[9] Open connectivity foundation (OCF), 2016. [Online]. 
Available: https://openconnectivity.org/. [Accessed: 26-Feb-
2017]. 
[10] S. Mayer, A. Tschofen, A. K. Dey, and F. Mattern, “User 
Interfaces for Smart Things,” Institute for Pervasive 
Computing, Zurich, 2014. 
[11] K. Gajos and D. S. Weld, “SUPPLE: automatically 
generating user interfaces,” in Proceedings of the 9th 
international conference on Intelligent user interfaces, 2004, 
pp. 93–100. 
[12] Matthias Peißner, “Entwurfsmusterbasierter Ansatz zur 
Überwindung von Nutzungsbarrieren,” Fraunhofer IAO, 
Universität Stuttgart, 2014. 
[13] G. Zimmermann et al., “Universal Remote Console Standard 
- Toward Natural User Interaction in Ambient Intelligence,” 
in Extended abstracts of the 2004 conference on Human 
factors and computing systems - CHI ’04, New York, New 
York, USA, 2004, p. 1608. 
[14] G. Zimmermann and G. Vanderheiden, “The Universal 
Control Hub: An Open Platform for Remote User Interfaces 
in the Digital Home,” Springer LNCS. Human-Computer 
Interaction. Interaction Platforms and Techniques., vol. 
4551/2007, pp. 1040–1049, 2007. 
[15] G. Zimmermann and B. Wassermann, “Why We Need a User 
Interface Resource Server for Intelligent Environments,” in 
Workshops Proceedings of the 5th International Conference 
on Intelligent Environments, 2009, vol. 4, pp. 209–216. 
4
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

 
 
[16] “GPII,” Global Public Inclusive Infrastructure, 2016. 
[Online]. Available: www.gpii.net. [Accessed: 26-Feb-2017]. 
[17] “GitHub - REMEXLabs/OpenAPE: Settings-based 
personalization framework,” 2016. [Online]. Available: 
https://github.com/REMEXLabs/OpenAPE. [Accessed: 26-
Feb-2017]. 
[18] ISO/IEC 24752-8, “Information technology — User 
interfaces — Universal remote console — Part 8: User 
interface resource framework (Committee Draft).” 2017. 
 
 
5
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

Enhancing Accessibility Information in Google Maps
Adding new pieces of information to GTFS to improve accessibility
Paloma Cáceres, Almudena Sierra-Alonso, Carlos E. Cuesta, José María Cavero, Belén Vela
Escuela Técnica Superior de Ingeniería Informática
Universidad Rey Juan Carlos, URJC
Móstoles (Madrid), Spain
e-mail: {paloma.caceres, almudena.sierra, carlos.cuesta, josemaria.cavero, belen.vela}@urjc.es
Abstract—Google is the most important information provider
on Internet. Within the Google ecosystem, Maps is a relevant
tool, which is used to calculate routes and to find points of
interest. As part of this effort, it has defined Google Transit
Feed Specification (GTFS), a format to specify data of public
transport. Now public transport agents can provide a “feed”
complying with this specification and Google can use them and
represent them on Maps. In spite of their relevance for
accessibility in mobility, Google Maps does not offer detailed
information about accessibility facilities to transit, and GTFS
does not specify the necessary structure to provide that
information about public transport. In this work, we propose
an extension to GTFS to provide relevant data to represent
accessibility elements on Google Maps and similar systems to
offer different social accessibility services.
Keywords-Public transport;
accessibility;
Google Maps;
GTFS.
I. INTRODUCTION
Google says “Discover the world with Google Maps.
Experience 
Street 
View, 
3D 
Mapping, 
turn-by-turn
directions, indoor maps and more across your devices”. But,
what about accessibility information? On December 2016,
the Verge announced that “Google Maps now shows if a
location is wheelchair accessible” [1]. The novelty is that
Google Maps, an app favored by nearly 70 percent of iPhone
users and installed by default in Android, will now list
wheelchair accessibility alongside other information, such as
traffic and store hours. The new addition makes easier for
people with disabilities to use the app; this also applies to
other groups, such as parents with strollers and the elderly.
With regard to the accessibility information provided by
Maps, even with this addition, it is only including wheelchair
accessibility, that is, it just takes into account mobility
disabilities or mobility special needs. And what about other
disabilities? Is it possible to provide accessibility information
to ease the mobility for blind or deaf people? It would be
interesting to offer new social accessibility services to help
people with different needs. In this way, we feel that it is
necessary to express the accessibility information of public
transport in a more detailed way, taking into account that this
information is even more important to impaired people. In
this work, we propose a method to publish such accessibility
data for the public transport. This method has been already
validated against real data for the subway in the city of
Madrid, Spain (METRO Madrid [2]).
Our proposal to specify the accessibility data of public
transport is based on the Identification of Fixed Objects in
Public Transport (IFOPT [3]) standard, which is an extension
of the European Reference Data Model for Public Transport
Information (Transmodel [4]) standard. IFOPT defines a
model for the main fixed objects related to access to Public
Transport, which also includes constructions to describe
accessibility data.
The article is structured as follows: Section 2 describes
the GTFS. Section 3 describes our proposal, specifying the
context of this work, the modification proposal of GTFS and
a case study. In Section 4, the conclusions and future work
are presented.
II.
ACCESSIBILITY IN GOOGLE TRANSIT FEED
SPECIFICATION
GTFS [5] is a format for public transportation schedules
and associated geographic information. A feed of GTFS is a
collection of a maximum of six CSV files, with a .txt
extension. Currently, only two of these files include some
information about accessibility and special needs: stops.txt
and trips.txt.
With regard to the stops.txt file, GTFS defines that “A
stop is a location where vehicles stop to pick up or drop off
passengers. Stops can be grouped together, such as when
there are multiple stops within a single station. This is done
by defining one stop for the station, and defining it as a
parent for all the stops it contains. Stops may also have zone
identifiers, to group them together into zones”. The stops.txt
file includes an optional column named wheelchair_boarding
to indicate accessibility this kind of information about a stop.
GTFS states that “It identifies whether wheelchair boardings
are possible from the specified stop or station. The field can
have the following values:
0 (or empty): Indicates that there is no accessibility
information for the stop.
1: Indicates that at least some vehicles at this stop can
be boarded by a rider in a wheelchair.
2: Wheelchair boarding is not possible at this stop”.
GTFS also specifies that “When a stop is part of a larger
station complex, as indicated by a stop with a
parent_station value, the stop's wheelchair_boarding field
has the following additional semantics:
6
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

0 
(or 
empty): 
The 
stop 
will 
inherit 
its
wheelchair_boarding value from the parent station, if
specified in the parent.
1: There exists some accessible path from outside the
station to the specific stop / platform.
2: There exists no accessible path from outside the
station to the specific stop/platform”.
With regard to the trips.txt file, GTFS defines that “A
trip represents a journey taken by a vehicle through stops.
So, a single trip represents one journey along a transit line
or route”. This file includes two columns, both optional,
related to accessibility limitations or special needs:
wheelchair_accessible and bikes_allowed. In this work, we
only address accessibility aspects and therefore we will not
discuss the bikes_allowed field. The wheelchair_accessible
field has the following additional semantics:
0 (or empty): Indicates that there is no accessibility
information for the trip.
1: Indicates that the vehicle being used on this particular
trip can accommodate at least one rider in a wheelchair.
2: Indicates that no riders in wheelchairs can be
accommodated on this trip.
In summary, Google Transit Feed Specification only
specifies the accessibility information related to mobility
needs, not taking into account other disabilities. For
instance, we provide a subset of METRO Madrid real data
following GTFS in Figure 1 and Figure 2. Figure 1 is a
portion of stops.txt file, which contains data of Sol stop
place. The first line of the file is the header, the second one
shows data about the stop place in Sol (stop_id = est_4_12)
and its wheelchair accessibility (wheelchair_boarding = 1),
that is, “at least some vehicles at this stop can be boarded by
a rider in a wheelchair”. Moreover, as Sol stop place is
located in a larger station complex, the file also specifies a
parent_station value and additional information. For this
reason, the following seven lines describe other stops
associated to Sol (parent_station = est_4_12), and they
inherit its wheelchair_boarding value from the parent station
(wheelchair_boarding = 0).
Figure 2 is a portion of trips.txt file, which contains the
services associated to trips of line 3 (Sol is a stop place in
lines 1, 2 and 3). The first row of the file is the header, next
rows
represent, for each line, the associated service
performing the trip. Each line has an associated value for
wheelchair_accessible (in this case value number 1, that is,
there exists some accessible path from outside to the
specific stop / platform).
These data have been provided by the Regional
Consortium for Public Transports of Madrid (CRTM) [6].
As you can see in previous figures, GTFS only makes
possible to express that Sol stop place is not fully
accessible. It just states that in Sol stop place “at least some
vehicles at this stop can be boarded by a rider in a
wheelchair” (motor disability). That is, it is impossible to
describe the needs for people with audible or visual
disabilities, which intend to travel starting or finishing the
trip on Sol stop place.
III. OUR PROPOSAL
A. The Context: CoMobility and Access@City Projects
This work is being developed in the context of two
related research projects: CoMobility and Access@City.
CoMobility [7] defines a multimodal architecture based
on linked open data for a sustainable mobility. Its main
goals are improving the citizen mobility, optimizing their
trips combining public transport and the sharing of private
transport (e.g., car sharing), and also providing a means for
accessible trips.
Access@City is a coordinated project, which defines a
technological framework to process, manage and use open
data about public transport with the goal to promote its
accessibility. Multiply@City is a subproject within it, which
is focused on processing and armonizing accessibility data
of public transport in a semantic way, taking into account
that data is provided by different sources and will have
different formats. Therefore, it is necessary to integrate
accessibility data from open data, together with web
scraping, and accessible routes, obtained by crowsourcing
from the users who use mobile technologies. Figure 3
provides a general depiction of this project.
stop_id,stop_code,stop_name,stop_desc,stop_lat,stop_lon,zone_id,stop_url,location_type,parent_station,stop_timezone,wheelchair_boarding
est_4_12,12,SOL,Plaza de la Puerta del Sol 6,40.4168864401114,-3.70316633485051,A,http://www.crtm.es,1,,Europe/Madrid,1
acc_4_12_1034,12,Alcalá,Plaza de la Puerta del Sol 13,40.4170832770092,-3.70286697000651,,http://www.crtm.es,2,est_4_12,,0
acc_4_12_1048,12,RENFE,Plaza de la Puerta del Sol 5,40.4168574567112,-3.70286733093632,,http://www.crtm.es,2,est_4_12,,0
acc_4_12_41,12,Carretas,Plaza de la Puerta del Sol 7,40.4166563378432,-3.70344111780431,,http://www.crtm.es,2,est_4_12,,0
acc_4_12_42,12,Carmen,Plaza de la Puerta del Sol 12,40.417145603815,-3.70320130556473,,http://www.crtm.es,2,est_4_12,,0
acc_4_12_43,12,Mayor,Plaza de la Puerta del Sol 9,40.4167162968701,-3.7044923366942,,http://www.crtm.es,2,est_4_12,,0
acc_4_12_776,12,Ascensor,Plaza de la Puerta del Sol 8,40.4166797644568,-3.70432248486764,,http://www.crtm.es,2,est_4_12,,0
acc_4_12_777,12,Preciados,Calle de Preciados 1,40.4172525245465,-3.70405646302912,,http://www.crtm.es,2,est_4_12,,0
Figure 1.GTFS stops.txt file about Sol stop place of METRO Madrid.
route_id,service_id,trip_id,trip_headsign,trip_short_name,direction_id,block_id,shape_id,wheelchair_accessible
4__3___,4_I12,4_I12-003_2015I12_1_1_4__3___,MONCLOA,VILLAVERDE ALTO-MONCLOA,0,,4__3____1__IT_1,1
4__3___,4_I12,4_I12-003_2015I12_2_1_4__3___,VILLAVERDE ALTO,MONCLOA-VILLAVERDE ALTO,1,,4__3____2__IT_1,1
4__3___,4_I13,4_I13-003_2015I13_1_1_4__3___,MONCLOA,VILLAVERDE ALTO-MONCLOA,0,,4__3____1__IT_1,1
4__3___,4_I13,4_I13-003_2015I13_2_1_4__3___,VILLAVERDE ALTO,MONCLOA-VILLAVERDE ALTO,1,,4__3____2__IT_1,1
4__3___,4_I14,4_I14-003_2015I14_1_1_4__3___,MONCLOA,VILLAVERDE ALTO-MONCLOA,0,,4__3____1__IT_1,1
4__3___,4_I14,4_I14-003_2015I14_2_1_4__3___,VILLAVERDE ALTO,MONCLOA-VILLAVERDE ALTO,1,,4__3____2__IT_1,1
4__3___,4_I15,4_I15-003_2015I15_1_1_4__3___,MONCLOA,VILLAVERDE ALTO-MONCLOA,0,,4__3____1__IT_1,1
4__3___,4_I15,4_I15-003_2015I15_2_1_4__3___,VILLAVERDE ALTO,MONCLOA-VILLAVERDE ALTO,1,,4__3____2__IT_1,1
Figure 2. GTFS trips.txt file: only line 3 (Sol stop place of METRO Madrid included).
7
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

Figure 3. Multiply@City Project architecture.
The Regional Consortium for Public Transports of
Madrid (CRTM) [6], the Madrid public bus company (EMT
Madrid) [8], the Spanish National Society of Blind People
(ONCE) [9], and the Chair of EcoTransport, Technology and
Mobility of Rey Juan Carlos University [10] have expressed
an interest in our CoMobility and Access@City projects’
results.
B. Proposal of the GTFS Modification
The intent of our proposal is to improve accessibility
information to support new social accessibility services. We
have then integrated information relevant to blindness,
hearing and mobility impairments and we have included the
following pieces of information:
1) Stops.txt file: We have added two new colums, which
are blindness_accessing and deaf_accessing. The values of
these columns follow the same pattern as the GTFS
specification for the mobility disability:
0 (or empty): There is no accessibility information for
the stop.
1: Indicates that at least some vehicles at this stop can
be boarded by blind (resp. deaf) people.
2: The access for blind or deaf people is not possible at
this stop.
Moreover, GTFS specifies: “When a stop is part of a
larger station complex, as indicated by a stop with a
parent_station value, the values of the blind_accessing and
deaf_accessing have the following additional semantics:
0 (or empty): The stop will inherit its value from the
parent station blind_accessing and deaf_accessing fields, if
specified in the parent.
1: There exists some accessible path for blind or deaf
people from outside the station to the specific stop or
platform.
2: There exists no accessible path for blind or deaf
people from outside the station to the specific stop/platform.
2) Trips.txt file: We have also added two new colums,
which are blind_accessible and deaf_accessible. The values
of these columns follow the same pattern as the GTFS
specificaction. The meanings of these values are:
0 (or empty): Indicates that there is no accessibility
information for the trip
1: Indicates that the vehicle being used on this particular
trip has elements to assist blind or deaf people.
2: Indicates that there are no elements to help blind or
deaf people on this trip.
C. Case Study: Proposed GTFS extension with data of
METRO Madrid
In this subsection we describe the process to represent
the accessibility data of METRO Madrid on Google Maps,
using our proposed extension.
To realize the case study, it was necessary to use real
data of METRO Madrid, namely lines, stations in each line
and their accessibility. METRO Madrid specifies the
accessibility in two different ways. The first one is on the
stations web page. There, the user can determine if a station
is accessible or not by means of icons (e.g., a wheelchair
icon is shown to identify if a station is absolutely accessible,
i.e., it has universal accessibility). The second way is on the
specific accessible stations web page. In that case, METRO
Madrid only indicates the accessible stations for each line.
There are three different types of accessibility:

Universal accessibility (UA): A stop place with UA
indicates that any person should be able to access to
the stop place, regardless of potential disabilities.

Complementary accessibility measures without lifts
and/or ramps (CAM): A stop place with CAM
indicates that there are facilities to simplify the
access of blind or deaf people, but the access of
mobility impaired people could present issues.

Lifts 
and/or 
ramps
without 
complementary
accessibility measures (LAR): A stop place with
LAR indicates that there are facilities to simplify the
access of mobility impaired people, but there are not
special resources for blind or deaf people.
As METRO Madrid does not provide the means for
downloading these data, we used a scraper to determine
which stations are accessible, and their kind of accessibility
in every case. This information was stored as a CSV file.
To integrate this information with previous GTFS data
in the new structure proposed for GTFS, we have defined
the following correspondences (see 0):
TABLE I. CORRESPONDENCES BETWEEN NEW GTFS AND METRO MADRID
GTFS Extension
METRO Madrid
Accessibility
Files
Added fields
UA
CAM
LAR
Stops.txt
wheelchair_boarding
1
2
1
blind_accessing
1
1
2
deaf_accessing
1
1
2
Trips.txt
wheelchair_accessible
1
2
1
blind_accessible
1
1
2
deaf_accessible
1
1
2
8
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

stop_code,stop_name,stop_desc,…,parent_station,stop_timezone,wheelchair_boarding, blindness_accessing, deaf_accessing
est_4_12,12,SOL,Plaza de la Puerta del Sol 6,…,,Europe/Madrid,1,1,1
acc_4_12_1034,12,Alcalá,Plaza de la Puerta del Sol 13,…,est_4_12,,0,0,0
acc_4_12_1048,12,RENFE,Plaza de la Puerta del Sol 5,…,est_4_12,,0,0,0
acc_4_12_41,12,Carretas,Plaza de la Puerta del Sol 7,…,est_4_12,,0,0,0
acc_4_12_42,12,Carmen,Plaza de la Puerta del Sol 12,…,est_4_12,,0,0,0
acc_4_12_43,12,Mayor,Plaza de la Puerta del Sol 9,…,est_4_12,,0,0,0
acc_4_12_776,12,Ascensor,Plaza de la Puerta del Sol 8,…,est_4_12,,0,0,0
acc_4_12_777,12,Preciados,Calle de Preciados 1,…,est_4_12,,0,0,0
Figure 4. New GTFS stops.txt file, including improved accessibility information in the Sol stop place
Following these specifications, we added these new
attribute values to GTFS stops.txt file and trips.txt files.
Figure 4 shows the stops.txt file with the values concerning
the accessibility in the Sol stop place. We have omitted
some fields in the central part of every row with the purpose
of highlighting the additional accessibility fields added,
which are also emphasized. This new accessibility
information could be represented in Google Maps. In Figure
5 we mark the accessibility by means of colours in the Sol
area:
Figure 5. Adding detailed accessibility in Google Maps.
The key box points to the Sol stop place. It is annotated
with three colours: this is done to highlight that universal
accessibility also includes lifts or/and ramps (LAR) and
complementary measures of accessibility (CAM). However,
when a stop place is only marked with dark blue colour, it
has only ramps or/and lifts to access. When a stop place is
only marked with green colour, it has only complementary
measures of accessibility.
IV. CONCLUSION AND FUTURE WORK
Google Maps is probably the most widely used service for
mobility. But currently it does not show the accessibility
elements available in public transport. To represent the
public transport information on the map and to calculate
routes, Google uses the “feeds” (specified in GTFS)
provided by transport companies. Due to the relevance of
accessibility in mobility, we think it is necessary to calculate
accessible routes, and to represent accessibility on Maps. In
this work, we have extended the GTFS specification to
support new social accessibility services.
This extension consists of adding relevant fields in the
stops.txt and trips.txt files. To prove the validity of this
extension, we have used real data of METRO Madrid. After
obtaining
them, we have
identified the accessibility
correspondences between GTFS and METRO Madrid data,
and we added these new values to the stops.txt and trips.txt
files. Next, we have represented them on the map: if a
station has universal accessibility, complementary measures
(for blind and deaf people) or only lifts or/and ramps (for
mobility impairments).
For future work, we intend to provide fully accessible
routes (from a stop to other) using these accessibility data.
ACKNOWLEDGMENTS
This work is supported by the Multiply@City project
(TIN2016-78103-C2-1-R), funded by the Spanish Ministry
of Economy and Competitiveness. We would like to thank
Eva María Jiménez Criado for her support.
REFERENCES
[1]
The Verge, http://www.theverge.com/2016/12/15/13968054/
google-maps-twenty-percent-wheelchair-accessible
[retrieved: february, 2017].
[2]
METRO Madrid, https://www.metromadrid.es/en/index.html
[retrieved: february, 2017].
[3]
CEN/TC 278. Intelligent transport systems - Public transport -
Identification of Fixed Objects In Public Transport (IFOPT),
EN 28701:2012.
[4]
Transmodel, standardization, E. C. (2016). Transmodel, Road
Transport and Traffic Telematics. Public Transport. Ref. Data
Model, EN 12896, http://www.transmodel.org/index.html
[5]
Google 
Transit 
Feed 
Specification,
https://developers.google.com/transit/gtfs/
[retrieved:
february, 2017].
[6]
Regional Consortium for Public Transports of Madrid
(CRTM), http://datos.crtm.es/ [retrieved: february, 2017].
[7]
C.E. Cuesta, P. Cáceres, B. Vela, J.M. Cavero, CoMobility: A
Mobile Platform for Transport Sharing,
ERCIM News
2013(93), pp. 22-23.
[8]
EMT 
Madrid 
home 
page,
http://www.emtmadrid.es
[retrieved: february, 2017].
[9]
Spanish National Society 
of Blind People (ONCE,
Organización 
Nacional 
de 
Ciegos 
de 
España)
http://www.once.es/new [retrieved: february, 2017].
[10] Chair of EcoTransport, Technology and Mobility (ChETM) of
the Rey Juan Carlos University, http://www.catedraetm.es/
[retrieved: february, 2017].
9
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

 
 
The Clinical Potential of a Cognitive Training Program Embedded in an Adaptive 
Video Game  
 
Martina Ratto, John E. Harrison, Keiron T. Sparrowhawk, Paul B. Cliveden 
MyCognition Ltd. 
London, United Kingdom 
e-mail: martina@mycognition.com 
 
 
Abstract—Cognitive training programs commonly involve 
single game features, but they are rarely embedded in a 
complete, adaptive video game where the player can 
experience game-flow. The aim of this paper is to present a 
video game designed to train users’ cognition across five key 
domains and to describe its potential. The training program is 
calibrated based on the individual scores obtained in a self-
administered, online assessment targeting attention, working 
memory, episodic memory, executive function, and processing 
speed, which is automatically linked to the video game 
software, adapting itself to the player’s progress. Structural 
features in the game contribute to creating an engaging 
experience 
for 
the 
users. 
Successful 
examples 
of 
implementation of the program have been tested in diverse 
settings, including educational programs and clinical trials. 
Improvements in cognitive function and transfer effects in 
academic and everyday life skills and behavior have been 
demonstrated and show promise for further analyses. The 
adaptive mechanisms and the game-like structure of the 
presented training program make it a potentially valuable 
starting point for further research on innovative cognitive 
programs. 
Keywords-Cognitive assessment & training; gamification; 
adaptive video games; performance improvements. 
I. 
 INTRODUCTION 
Computer-based solutions for cognitive training are 
becoming progressively more popular in the commercial, 
clinical, educational, and business sectors. Most of these 
solutions have introduced elements of gamification [1], 
including 
feedback, 
an 
achievement 
level 
structure, 
competitions, and time pressure. However, only a few of 
these show a true involvement of the player, typically 
observed in the most sophisticated of video games, which 
can produce a game-flow experience. Flow may be defined 
as the complete absorption in an activity in which a person is 
involved [2]. In the case of gaming, the flow primarily 
depends on how much the game itself is adaptive, i.e., how 
much it can modify itself according to the user’s progress to 
be sufficiently challenging. Other factors producing the flow 
experience may be a representation of the self inside within 
the game, a 3D environment, a narrative context, or a music 
background [3].  
Traditional brain-training programs may be considered as 
a separate category compared to the video games, as they do 
not completely incorporate such game features, stopping at a 
puzzle level [4]. Most traditional cognitive games only 
introduce single game features, such as progress bars, level 
structure, and feedback, without producing an actual video 
game, even if some exceptions have been identified [5]. 
However, the cognitive programs following this type of 
approach do not embed an assessment tool to monitor the 
progress in cognition and to set in the game a level of play 
adapting to the user’s cognitive level.  
The aim of this paper is to present a novel approach 
which attempts to overcome these limitations by proposing 
an online, self-administered software tool, integrating both 
an assessment and a training program, where the training is 
embedded in an engaging video game, MyCognition 
AquaSnap. 
In Section 2, the MyCognition software, integrating a 
cognitive assessment and a training video game is introduced 
and game elements are described. The structure of the 
training game will then be described more in more detail in 
Section 3. Some examples of successful adoption of the 
MyCognition programs are presented in Section 4, 
underlining the training conditions necessary to gain 
performance 
improvements. 
In 
Section 
5 
some 
considerations about future perspectives will be appraised 
and future directions will be outlined in Section 6. 
II. 
THE MYCOGNITION PROGRAM 
AquaSnap is a scientifically designed, cognitive training 
video game developed to improve cognitive function, 
targeting five key cognitive domains — attention, working 
memory, episodic memory, executive function, and 
processing speed [10].  
To produce personalized training for every user, the 
amount of training for each cognitive domain is calibrated on 
the individual scores obtained in a cognitive assessment 
integrated into the system, the MyCognitive Quotient 
(MyCQ) assessment. MyCQ comprises a digital version of 
the most validated psychometric tests widely used over 
almost 200 years of neuropsychological research in their 
original, traditional, paper-and-pencil versions. In contrast, 
MyCQ is an online, self-administered assessment, whose 
final scores for each cognitive domain are automatically 
generated by the software system and feed the game engine. 
In this system, the lower a player’s score in a particular 
cognitive domain, the more intense the training will be for 
that domain.  
10
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

 
 
The game has an aquatic theme in which players must 
venture into the ocean to undertake various activities. The 
activities include exploring rich underwater worlds populated 
with a range of fish and sea creatures, seeking out and 
photographing different types of fish, each with their own 
characteristics. The photos are placed for sale on the open 
market, to build wealth and reputation for the player. 
Finances must be mastered, as currency is spent to push 
further into the ocean to see the rarest and most elusive of 
fish. 
The game is built from the player’s self-perspective, 
which is represented in the ship moving across the ocean 
chart and in the camera’s lens in the underwater 
environment. The ocean chart itself embeds a narrative 
aspect in the game, as it tracks the players’ progress history 
and can be explored backward and forward. The evocative 
names of each oceanic area contribute to make the 
environment the scenario of a narration. 
Moving from the 2D environment of the map to the 
underwater zone, the player can experience a 3D 
environment with the illusion of moving through the water 
and encountering different objects and animals as a part of 
each training task. The music in the background also 
contributes to creating a flow experience, facilitating the 
player’s concentration during the training and ultimately 
advancing to produce a longer-term effect on behavior, as 
suggested in studies involving children with behavioral 
difficulties [7]. 
 
III. 
THE STRUCTURE OF AQUASNAP 
The training works by encouraging the player to 
undertake repetitive, and increasingly more challenging, 
tasks that are embedded in the video game. The tasks are 
designed to train a specific cognitive domain.  
Each cognitive domain is mainly trained by a specific 
loop, with some domains trained using several tasks. The 
game develops on different structural levels. At the basic 
structural level, there are the loops which correspond to the 
five first tasks in Table 1. The loops are organized in dives, 
so that in each different dive the user can experience a set of 
loops. 
The ocean map represents a meta-level of cognition. At 
the map level, users must organize their dive to both achieve 
the proposed mission and to discover new areas. 
The progress of the players on the map, and consequently 
the growth of difficulty in the training game, depend on the 
coins the players can collect during their dives. In this way, 
the game adapts its difficulty to the level of improvement 
reached by the player. 
The intensity of the training depends on the individual 
MyCQ scores, too, as mentioned above, as the number of 
loops for each type of task that the user experiences depends 
on the score obtained in each cognitive domain. In this way, 
more impaired domains will receive more intensive training. 
In the following section, further details about the amount 
of time the users play the training game and in-game 
adaptive mechanisms will be introduced, as these relate to 
real-world implementations of the program. 
  
IV. 
MYCOGNITION PROGRAM IMPLEMENTATION STUDIES 
Considerable evidence has been produced in randomized 
controlled trials (RCT) in clinical and school settings and in 
individual and group case-studies in school and home 
settings, as described in the following paragraphs. These 
have shown the effectiveness of the described training video 
game 
in 
improving 
players’ 
cognition 
and 
related 
performance for different categories of users. MyCognition 
generally recommends following the training program by 
playing the cognitive game at least 90 minutes per week, at 
least three times each week for eight/twelve weeks, and 
taking the MyCQ assessment at the baseline, in the middle, 
and after the conclusion of the program. The game itself 
keeps track of the diving time and displays it to the player. 
Also, the game proposes among the daily missions the task 
of playing at least 15 minutes in order to get 3 coins. As the 
coins are an essential tool to progress in exploring the 
oceanic map, players not complying with the recommended 
time adaptively have a slowed progress through training 
levels, which corresponds to their slowed cognitive 
improvement.  
TABLE I.  
INDIVIDUAL TASKS IN AQUASNAP AND TRAINED DOMAINS 
Task 
Task Description 
Name 
Activity 
Cognitive 
Domain 
1 
Memory Shot 
Remember the 
position of the 
glowing fish in the 
loop. 
Working 
Memory 
2 
Quick Shot 
Snap the fish as 
soon as you see it. 
Processing 
Speed & 
Attention 
3 
Careful Quick Shot 
Snap the fish as 
soon as you see it, 
but be careful not to 
snap the shocking 
fish. 
Attention & 
Executive 
Function 
4 
Group Shot 
Snap the group of 
fish when all 
together. 
Attention & 
Processing 
Speed 
5 
Fish Tracker 
Remember which 
fish are glowing 
after they change 
position. 
Working 
Memory 
6 
Oceanic Survey 
Remember which 
fish you have seen at 
the end of the dive. 
Episodic 
Memory 
7 
Missions & Map 
Exploration 
Achieve the goals 
proposed by the 
daily missions and 
try to discover new 
areas on the ocean 
map. 
Executive 
Function 
 
11
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

 
 
Users following the recommended program are usually 
able to get at least a 10 points increase in their MyCQ score 
on a scale going from 1 to 100, corresponding to a 20% 
increase for the average population, having a baseline score 
of 50 points. 
The first clinical evidence came from two studies 
involving 
a 
psychiatric 
population 
affected 
by 
schizophrenia, 
schizoaffective 
disorder, 
obsessive-
compulsive disorder, and major depressive disorder. 
Significant improvements in episodic memory were shown 
in the group of patients playing the cognitive game in 
addition to the usual treatment [15]. 
Outcomes are currently being measured across all the key 
cognitive domains in a Parkinson’s disease population with 
mild cognitive impairment [18]. 
Several other clinical trials are ongoing, investigating the 
usefulness and usability of the cognitive video games in 
populations with neurodegenerative and psychiatric disorders 
and in other conditions, including cancer. 
Evidence of the effectiveness of the training program and 
the attractiveness of the video game have been studied also 
in various education scenarios, involving mainstream and 
special 
educational 
needs 
students. 
In 
addition 
to 
improvement in cognition, students of different ages also 
showed advances in their learning skills, in academic 
achievements, and in class behavior [11][12][17]. 
Current studies are still investigating the usability of the 
programs in other contexts, such as social and health services 
and corporate wellbeing, showing preliminary promising 
outcomes [16]. 
V. 
FUTURE PERSPECTIVES 
Once the potential and the effectiveness of the training 
program presented has been shown in different settings, a 
further step in the development work would be to export the 
basic structure of the game and its different cognitive tasks t 
other analogous video games with different environments 
which can embed the training program itself.  
Also, further research can be done in order to develop 
more sophisticated features to enhance players’ engagement 
and the game’s user-adaption, together with more sensitive 
assessment tasks and training loops. 
Evaluations of similar and comparable studies can also 
be deepened, as well as analogous counterparts in the fields 
of serious games and exergames.  
VI. 
CONCLUSION 
Even if the work presented has limitations due to its “in 
progress” state, some innovative points have been identified 
and early evidence of the potential of the program has been 
shown.  Future research can lead to the development of 
more sophisticated game features that are able to produce a 
more totally engaging game-flow experience in different 
categories of players, from the youngest children to the 
elderly population.  
ACKNOWLEDGMENT 
We thank Anna Domen, Sjors C. van de Weijer, and Dr. 
Anne Bellens for their assistance with recruiting and 
managing participants in trials. We are grateful to all study 
participants for their contributions. 
 
REFERENCES 
[1] S. Deterding, D. Dixon, R. Khaled, and L. Nacke, “From 
game design elements to gamefulness: defining gamification,” 
Proceedings of the 15th international academic MindTrek 
conference: Envisioning future media environments, pp. 9-15, 
2011. 
[2] M. Csiksentmihalyi, Flow. The psychology of optimal 
experience, New York: Harper & Row, 1990. 
[3] B. Reeves and J. L. Read, Total engagement: How games and 
virtual worlds are changing the way people work and 
businesses compete, Harvard Business Press, 2015. 
[4] M. Ninaus et al., “Game elements improve performance in a 
working memory training task,” International Journal of 
Serious Games, Vol. 2, pp. 3-16, 2015. 
[5] J. Lumsden, E. A. Edwards, N. S. Lawrence, D. Coyle, and 
M. R. Munafò, “Gamification of cognitive assessment and 
cognitive training: a systematic review of applications and 
efficacy,” 
JMIR 
Serious 
Games, 
vol 
4(2), 
doi: 
10.2196/games.5888, 2016. 
[6] J. A. Anguera and A. Gazzaley, "Video games, cognitive 
exercises, and the enhancement of cognitive abilities,” 
Current Opinion in Behavioral Sciences, vol. 4, pp. 160-165, 
2015. 
[7] S. Hallam and J. Price, “Can the use of background music 
improve the behaviour and academic performance of children 
with emotional and behavioural difficulties?,” British journal 
of special education, vol. 25(2), pp. 88-91, 1998. 
[8] A. C. Domen et al., “The validation of a new, online cognitive 
assessment tool,” European Neuropsychopharmacology, vol. 
25, p. S344, 2015.  
[9] A. Domen et al., “The validation of a new online cognitive 
assessment tool,” European Neuropsychopharmacology, vol. 
26, pp. S342-S343, 2016. 
[10] J. E. Harrison,  J. H. Van Rijswijk, K. T. Sparrowhawk, and 
D. A. Knight, U.S. Patent No. 20,150,279,226. Washington, 
DC: U.S. Patent and Trademark Office, 2015.  
[11] P. Shah, R. Kumar, A. McCone, K. Sparrowhawk, and J. 
Thomas, “The impact of cognitive function assessment and 
adaptive training on academic performance in students with 
learning 
difficulties,” 
Poster 
session 
presented 
at: 
Copenhagen, DK: 10th Federation of European Neuroscience 
Societies (FENS) Forum of Neuroscience, Jul. 2016. 
[12] K. Sparrowhawk, R. Kumar, and A. McCone, “Evaluation of 
cognitive function and training in school children,” Poster 
session presented at: Copenhagen, DK: 10th Federation of 
European 
Neuroscience 
Societies 
(FENS) 
Forum 
of 
Neuroscience, Jul. 2016. 
[13] A. McCone, R. Kumar, K. Sparrowhawk, and L. Franchi, 
“Evaluating the impact of cognitive training for SEN children 
when used at home,” Poster session presented at: 
Copenhagen, DK: 10th Federation of European Neuroscience 
Societies (FENS) Forum of Neuroscience, Jul. 2015. 
[14] K. Sparrowhawk, R. Kumar, and J. Harrison, “Adaptive 
Video Games can Assess and Enhance Cognitive Health,”  
Value in Health, vol. 17(7), p. A454, 2014. 
[15] D. Nieman et al., “Cognitive remediation in psychiatric 
patients with an online cognitive game and assessment tool,”  
12
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

 
 
European Neuropsychopharmacology, vol. 25, pp. S344-
S345, 2015. 
[16] K. Sparrowhawk, P. Cliveden, M. Ratto, W. Ogle-Welbourne, 
A. Sunley, “Evaluating the cost-benefit of a cognitive 
assessment and training program across a smart city 
population in the UK,” Value in Health, vol. 19(7), pp. A693-
A694, 2016. 
[17] K. Sparrowhawk, J. Harrison, R. Kumar, D. Knight, 
“Working memory and executive functioning are improved in 
school students using an applied action video game,” Poster 
session presented at: Milan, IT: 9th Federation of European 
Neuroscience Societies (FENS) Forum of Neuroscience, Jul. 
2014. 
[18] S. C. van de Weijer et al., “The Parkin’Play study: protocol of 
a phase II randomized controlled trial to assess the effects of a 
health game on cognition in Parkinson’s disease,” BMC 
Neurology, vol. 16(1), 209, doi:10.1186/s12883-016-0731-z, 
2016. 
 
 
 
 
 
13
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

Experimental Study on User Acceptance and Affordability                                   
of Intelligent Wheelchair 
- Questionnaires on Human Machine Interface-  
Naohisa Hashimoto, Yusuke Takinami, Osamu Mastumoto 
National Institute of Advanced Industrial Science and Technology (AIST) 
Tsukuba, Japan  
e-mail:{naohisa-hashimoto}@aist.go.jp 
 
Ali Boyali 
Toyota Technical Institute (TTI),  
Nagoya, Japan 
 
Abstract—We proposed the use of an intelligent wheelchair for 
new mobility for not only elderly people but also everyone. Our 
intelligent wheelchair has autonomous function and new gesture 
interface. For the introduction of the proposed intelligent 
wheelchair, there are several challenges associated with the use 
of the wheelchair. One of the most important points is the user 
acceptance, and it must be investigated with a plenty of subjects 
by experiments. In addition, the affordability is also important 
for the introduction. The two points were investigated by 
performing the several experiments with subjects. In the 
experiments, we did a questionnaire about the intelligent 
wheelchair. In this paper, we introduce experiments and the 
questionnaire results, and the results are compared and 
discussed in this paper. The questionnaire results proved that 
most subjects had the favorable opinions about autonomous 
function and new gesture interfaces. On the other hand, the 
challenging issues for improving the user acceptance of 
intelligent wheelchair were also found especially for the gesture 
interface. These results must be valuable for developers and 
researchers of new wheelchairs. 
 
Keywords- Intelligent Wheelchair; Elderly People; User 
Acceptance; Human Machine Interface; Pilito Study. 
 
I. 
 INTRODUCTION 
The rapid increase in the proportion of elderly people in the 
population has caused several issues in Japan [1]. Thanks to 
the advancing science and inherent adaptability of humankind 
to though life conditions, there has been an increase not only 
in the average life expectancy, but also the population of aged 
and disabled people that in the need of mobility aid. The 
current figures report that nearly 15% of the population, 
corresponding to one billion in the world, are with some form 
of disability or impairment [2]. Besides that, according to the 
studies [3][4] the household rate of the people using 
wheelchair only in the USA, doubled from 1.5% to 3% from 
1990 to 2010 and the majority of the wheelchair users are 
elderly. Useful, affordable and safe wheelchair is expected for 
elderly and disabled people [5-7]. 
We proposed an intelligent wheelchair to solve mobility 
issues [8-12]. The proposed wheelchair has two new features. 
One is an autonomous function, and the other is a new 
interface. With the autonomous function the rider in the 
wheelchair is not required to control the chair by a joystick, 
which is a conventional controller for a wheelchair. However, 
there are several challenges associated with the use of an 
autonomous system, the most difficult being cost. Several 
expensive sensors are necessary for a wheelchair to achieve 
complete autonomous functionality. These sensors increase 
the cost of an intelligent wheelchair, making it difficult for 
elderly people to purchase an intelligent wheelchair. With 
respect to new interface, as wheelchairs are smarter and more 
intelligent, a conventional joystick controller cannot be 
suitable. Also, people usually would like to use a new 
interface, and elderly people would like to use cool interface, 
because they want to look younger than they are. Seeing that 
the gestures as a one-way process from mind to body; how can 
gestures be used in creating user interfaces remains an open 
research question. A few numbers of studies discuss the 
usability and acceptance of gestural interfaces by the different 
age groups while touching the fact that design of intuitive 
gestures must be separately handled. 
Preliminary experiments were performed with the proposed 
wheelchair, and we proved that there was a possibility to use 
the proposed wheelchair by performing experiments. It is 
important to evaluate user acceptance, and affordability with 
real subjects, and to get feedback from the real users 
In this paper, the performed experiments with the proposed 
intelligent wheelchair and questionnaire results will be 
introduced, and user acceptance will be evaluated with 
questionnaire result. 
Herein, in Section 2, the Tsukuba Designated Zone, where 
the real-world experiment was conducted, is described. In 
Section 3, we explain the proposed intelligent wheelchair. In 
Section 4, the experiments and the questionnaire result of real-
world experiments with the proposed intelligent wheelchair 
used by several subjects are explained. 
 
II. 
TSUKUBA DESIGNATED ZONE FOR EXPERIMENTS 
This section describes the Tsukuba Designated Zone, 
where the experiment was performed. This institution was 
formed to improve robotics technology. It was officially 
approved as the Tsukuba Designated Zone by the Cabinet 
Office in Japan on January 29th, 2010 [13]. 
14
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

The Designated Zone has two areas for conducting 
experiments. One is the Tsukuba Center Station area, and the 
other is the Kenkyugakuen Station area, shown in Fig.1. The 
Tsukuba Center Station area consists mainly of a pedestrian 
road from the University of Tsukuba to Akatsuka Park, with a 
major focus on Tsukuba Central Station. The width of this 
road is greater than three meter and is sufficient to allow use 
by bicycles. For these reasons, this public area is appropriate 
for experimental research. Even within the Tsukuba 
Designated Zone, there are some regulations that apply to 
conducting experiments. The committee of Tsukuba 
Designated Zone is applying relaxations of regulations for the 
experiments. We performed experiments on automated 
function in Tsukuba Center Area and Kenkyugakuen Area, 
and experiments on gesture interface. 
 
 
Figure 1. Tsukuba Designated Zone (Red: Tsukuba center area, Blue: 
Kenkyugakuen area) [13] 
 
III. 
DEVELOPPED INTELLIGENT WHEELCHAIR 
The intelligent wheelchair used in the experiment have 
been developed at the National Institute of Advanced 
Industrial Science and Technology (AIST). This wheelchair 
was modified from the wheelchair produced by AISIN SEIKI 
Corporation, shown in Fig.2. Figure 3 shows the system 
configuration of this wheelchair. It can be controlled by an on-
board PC through an electrical signal. This wheelchair can 
move at 6 [km/h]; hence, the maximum velocity was set to 4 
[km/h] during the experiments. This wheelchair can be used 
for traveling for about 2 hours without charging. The 
wheelchair has one Real Time Kinematic (RTK)-GPS sensor, 
one laser scanner sensor (LSS), one gyro sensor, two encoder 
sensors for counting left and right wheel speeds, a laptop PC, 
and an onboard PC. This wheelchair has two modes, one is 
autonomous mode and the other is a gesture interface mode.  
In the autonomous mode, the system enables the intelligent 
wheelchair to travel autonomously with accurate positions 
estimated by the Kalman Filter and desired path[8][11][14]. 
In this mode, the rider doesn’t need to do anything on 
controlling. This autonomous function was already developed 
and has enough level to do experiments outside and indoor 
environments. 
 
 
Figure 2 Intelligent wheelchair 
 
In the gesture interface mode, functions of obstacle 
avoidance and autonomous navigation were not used during 
the experiments. A gesture based interface is implemented by 
adding a Leap Motion camera to the wheelchair under the arm 
support, shown in Fig.4. Gesture interface needs the gesture 
recognition algorithm, which can estimate which rider’s 
gesture is. This algorithm was already proposed [9][10] by 
referring to the presented theory[16-20]. There are four 
patterns of gestures, which were defined for the wheel chair 
control. These gestures are “Go Straight”, “Turn Left”, “Turn 
Right” and “Stop” hand gestures, shown in Fig.5. The gesture 
recognition algorithm recognizes both the hand gestures and 
postures with an overwhelming accuracy. Along with the 
gesture recognition system, a function of the Leap Motion 
development kit is used to recognize fingers touching and 
hand fist actions to add extra caution to the stop gesture. The 
system halts using three stop conditions as the experiments 
15
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

were conducted in a public area in Tsukuba. The wheelchair 
comes to stop in three conditions for either of finger touches, 
hand fist or hand is not seen in the sight volume of the camera. 
It was confirmed that the algorithm can correspond to 
everyone including elderly people [12]. Thus, it has enough 
robust and high accuracy to perform experiments in indoor 
and outdoor environments. 
 
 
Figure 3 System Configuration 
 
 
Figure 4 Gesture Sensor 
 
 
Figure 5 Gesture Pattern 
IV. 
EXPERIMENTS WITH INTELLIGENT WHEELCHAIR 
We conducted an operational evaluation through real-world 
experiments and introduced some of the experimental results 
the previous papers [10][11][14]. In this section, two 
conducted scenarios are described with the proposed 
intelligent wheelchair. The experimental conditions, the 
questionnaires provided, and the experimental results, as well 
as an overall discussion are explained. In addition to previous 
results, discussion based on experimental and questionnaire 
results, which were not presented in the previous papers, will 
be done in the next section. Experimental places used in this 
study were located in the Tsukuba Designated Zone in Japan, 
which is described in Section 2. Before conducting the 
experiments, we applied a risk assessment of riding the 
intelligent wheelchair for every route. Each of the two 
scenarios is described in the following sub-sections. 
 
A. Experiments for autonomous wheelchair 
59 subjects who are not disable people were employed for 
these experiments. We asked the subjects to ride the 
intelligent wheelchair with automated function. The 
experimental duration was set to 20 minutes for each subject. 
After the experiments, several questionnaires about the 
intelligent wheelchair were done.  
 
1) Provided Questionnaire 
Before participating in the experiment, the subjects 
answered same questionnaire about gender and age. 
After experiments, the questionnaires, which all subjects 
answered are as follows: 
Q1. Did you feel any near miss events during the riding? 
(If yes, please explain in detail. If no, please imagine 
near miss event with the intelligent wheelchair) 
Q2. What distance do you think is appropriate between the 
wheelchair and surroundings?  
Q3. How much do you want to pay for this autonomous 
function? 
Q4. How do you feel about the stability on a scale of one 
to ten? (10 is best) 
Q5. How do you feel about the fun on a scale of one to 
ten? (10 is best) 
Q6. How do you feel about the comfortability on a scale 
of one to ten? (10 is best) 
Q7. When these intelligent wheelchairs are available in a 
supermarket or shopping center, do you think do they 
encourage you to go there? 
Q8. If you have any comments regarding these 
experiments, please let us know. 
 
2) Questionnaire Result 
In question1 (Q1), about 20 % subjects answered “yes”, 
and comments about the event are as follows: 
- 
The wheelchair traveled very close to pedestrian 
- 
The wheelchair suddenly stopped for the obstacle 
- 
He unintentionally touched joystick controller 
16
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

- 
The route, which the wheelchair chose, was different 
from the route he expected 
- 
Suddenly, a pedestrian crossed in front of the 
wheelchair  
Those who answered “no”, and comments they imagined 
are as follows: 
- 
The wheelchair travels at high speed 
- 
The rider forgets to turn off the switch 
- 
The wheelchair travels in rainy, crowded, non-flat or 
slope conditions 
- 
Software bugs exist 
- 
High speed obstacles including bicycle cross in front 
of the wheelchair 
With respect to Q2, average distance is about 1.29[m] and 
standard deviation is about 0.64[m]. This distance is about 
double person’s width, and the personal differences are large 
by considering the standard deviation. Thus, it is supposed 
that autonomous wheelchair needs to keep enough distance 
(more than 1.2[m]) between obstacles and the wheelchair, 
and the distance should be able to be changed by users. 
With respect to Q3, average cost is about 137000 Japanese 
yen (1370 US dollar) and standard deviation is about 209000 
Japanese yen (2090 US dollar). In Japan, an electric 
wheelchair costs from 200000 to 500000 Japanese Yen [21]. 
This autonomous function must be under this wheelchair 
costs, and the number of this standard deviation means that 
the value of autonomous function depends on users, thus, 
some users strongly want to use this function, and there is a 
strong possibility that they will pay for this function. 
With respect to Q4, Q5 and Q6, Table 1 shows the 
questionnaire results. This result shows that comfortability 
and fun are enough high, but stability needs to be improved. 
This means that subjects didn’t trust the autonomous function 
yet, despite no accident in the experiments. 
In Q7, the result shows that about 90 % subjects answered 
“yes”.  This means that the intelligent wheelchair is very 
attractive for a supermarket or a shopping center. 
Their comments in Q8 are given below. 
- 
The design of the wheelchair should be changed 
- 
Interface for a rider is important even when the 
wheelchair travels autonomously 
- 
The wheelchair should move more smoothly 
- 
A rider wants to confirm surroundings, as the 
wheelchair recognizes 
We found that the subjects expressed favorable opinions 
regarding the intelligent wheelchairs, and many expressed a 
desire to use it again in the future. 
 
TABLE. 1 QUESTIONNAIRE RESULT OF Q4, Q5 AND Q6 (Average 
and Standard Deviation of each score. 10 is best and 1 is worst.) 
 
B. Experiments for new interface 
The experiments for evaluating the intelligent wheelchair 
with the new gesture interface were performed. The subjects 
who are not disable people were instructed how to use these 
gestures to command the wheelchair before using the 
intelligent wheelchair. The experimental place was Tsukuba 
designated zone, which was explained in Section 2. The 
duration of the experiment for each subject was about 20 
minutes. Figure 6 shows the experimental scene.  
 
 
Figure 6 Experimental Scene 
1) Provided Questionnaire  
Before participating in the experiment, the subjects 
answered their gender, age. We wanted to know the 
impressions of two kinds of controlling wheelchair (one is 
new gesture method, the other is conventional joystick 
method), thus we asked the following questions after we 
showed the movie about joystick and gesture interfaces. 
Q1. How do you feel about the gesture interface on a scale 
of one to five? (5 is best) 
Q2. How do you feel about the conventional joystick 
interface on a scale of one to five? (5 is best) 
   After the experiment, each subject filled out a questionnaire 
providing answers to the following questions. 
Q3. How did you feel about the gesture interface on a scale 
of one to five after you used it? (5 is best) 
Q4. How did you feel about the conventional joystick 
interface on a scale of one to five after you used it? (5 
is best) 
Q5. Can you tell me pros and cons of using the joystick for 
controlling the intelligent wheelchair? 
Q6. Could you control the intelligent wheelchair by the 
gesture interface? If not, please let know about when 
and what condition you thought that you couldn’t 
control it. 
Q7. Do you have any comments about the gesture for “go 
straight”?  
Q8. Do you have any comments about the gesture for “turn 
right”?  
Q9. Do you have any comments about the gesture for “turn 
left”?  
Q10. Do you have any comments about the gesture for 
“stop”?  
Q11. Where or which condition would you like to use the 
gesture interface? 
Q12. If you have any comments regarding these 
experiments, please let us know. 
Q4
Q5
Q6
Average
6.99
8.20
8.95
Standard
Deviation
2.03
1.96
1.80
17
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

2) Questionnaire Result and Discussion 
    Table 2 shows the results for Question 1, 2, 3 and 4 (Q1, 
Q2, Q3 and Q4). From this results, before using gesture, 
subjects thought the gesture interface was very interesting 
and useful more conventional joystick. On the other hand, 
after the experiments, unfortunately, the subjects thought the 
gesture interface wasn’t satisfied yet. Thus, there are several 
challenging issue remaining in the gesture interface, and 
comments, which will be introduced in the result of Q6-Q12, 
are important.  
Their comments in Q5 are as follows: 
- 
Good reaction of turning and forward 
- 
Easy to operate than gestures 
- 
To get accustomed to the joystick interface more easily 
than the gesture interface 
- 
Easy to use, without learning 
With respect to Q6, 14.2% subjects answered “no”. Those 
who answered “no” regarding Q6, their reasons are as 
follows: 
- 
It was difficult to understand proper spacing of the 
hands and the sensor 
- 
It was possible to smoothly steer in the beginning of the 
experiment. But, after practicing the gesture interface, it 
became easy. 
- 
I wasn’t able to successfully steer because the 
sensitivity of the sensor wasn’t enough. 
- 
Operation of the left hand, which was not dominant 
hand,  was difficult 
- 
I was able to operate “go straight” and “turn left”, but to 
operate “turn right” was difficult. 
- 
It was possible to operate “turn left”, but to operate “turn 
right” was difficult. 
From this result, it is confirmed that we need to improve the 
gesture interface, but this comments are valuable for 
improving the gesture interface. For example, the 
specification of the sensor should be improved, and 
explanation and trial are important to get accustomed to this 
new interface. The operation of “turn right” seems to be 
difficult for several users, and this problem can be fixed if 
this gesture pattern of “turn right” is changed. 
With respect to Q7, Q8, Q9 and Q10, Table 3 shows the 
questionnaire results. As shown in the result of Q6, the 
operation of “turn right” should be changed and appropriate 
places of the arm and the hand must be easily fixed. 
With respect to Q11, several interesting comments about 
condition are given and they are as follows: 
- 
Operations using the center of gravity of the body 
- 
Operation by using the leg or the neck.  
- 
I want to operate in my dominant hand for the gesture 
interface. 
- 
Not for me, but I think some people are interested in this 
gesture interface by considering the level of disability 
In these experiments, we chose hand gesture but it will be 
interesting that other body gestures are employed for the new 
interface. 
Their comments about the gesture interface in Q12 are as 
follows: 
- 
The hand and arm should be fixed for keeping the 
appropriate place of the hand. 
- 
We expect that the system should be more reliable 
and accurate 
- 
It takes some time for this operation to get 
accustomed to. 
- 
Gesture interface was funny 
- 
I was impressed that the wheelchair moves with the 
gesture interface, and I want to use this sometime. 
- 
It was fun to use, but it may be difficult for elderly to 
understand the operation for proper use 
We found that the subjects expressed favorable opinions 
regarding the gesture interface, and many expressed a desire 
to use it again in the future. These comments indicate that we 
need to strongly improve the intelligent wheelchair. 
 
TABLE.2 QUESTIONNAIRE RESULT OF Q1, Q2, Q3 AND Q4 
(Average and Standard Deviation of each score. 10 is best and 1 is worst.) 
 
 
Q1
Q2
Q3
Q4
Average
4.14
3.71
2.71
4.00
Standard
Deviation
0.35
0.88
1.16
0.76
Go Straight
Turn Right
Turn Left 
Stop
Good
To keep appropriate distance between
the hand and the sensor was difficult
Good
Good
Very easy
Difficult to operate
It was easier than "turn right"
Reaction of the wheelchair was good
It was easy to operate straight
comparing to left and right
"Turn right" is more difficult than "turn left" Turn left was simpler than "turn right"
Easy
It was difficult to fix the place
of the hand
At first, it was not able to turn right.
It was possible to operate
after changing position of the arm
It was easy to operate.
Distance between the sensor and
the hand was important
It was easy
because the system recognizes "stop"
when the hand was released from the sensor
Hard to operate
On operationg "turn right",
the left hand was too close to the sensor,
and sensor couldn't recognize the hand
It was able to operate as intended
I didn't feel the anxiety
because the wheelchair stopped in safety
I could not proceed straight
without shifting a little to the left
Very easy
Very easy
Table.3 Qustionnaire results of Q7, Q8, Q9 and Q10
TABLE.3 QUESTIONNAIRE RESULT OF Q7, Q8, Q9 AND Q10 
18
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

V. 
CONCLUSION 
We proposed the use of an intelligent wheelchair for new 
mobility not only for elderly people but also for everyone. 
Our intelligent wheelchair has autonomous function and new 
gesture interface. For the introduction of the proposed 
intelligent wheelchair, there are several challenges associated 
with the use of the wheelchair. One of the most important 
points is the user acceptance, and it must be investigated with 
a plenty of subjects by experiments. In addition, the 
affordability is also important for the introduction. The two 
points were investigated by performing the several 
experiments with subjects. In the experiments, we did a 
questionnaire about the intelligent wheelchair. In this paper, 
we introduce experiments and the questionnaire results, and 
the results are compared and discussed in this paper. The 
questionnaire results proved that most subjects had the 
favorable opinions about autonomous function and new 
gesture interfaces. On the other hand, the challenging issues 
for improving the user acceptance of intelligent wheelchair 
were also found especially for the gesture interface. One of 
the most important points is to choose easy operation 
especially for elderly people. These results must be valuable 
for developers and researchers of new wheelchairs. 
For future work, we will perform the experiments under 
more situations and with more subjects including disable 
subjects. In addition, we will develop new intelligent 
wheelchairs by using feedback from this research. 
 
ACKNOWLEDGMENT 
 This work was supported by Japan Society for the 
Promotion of Science (JSPS) fellowship program, the 
KAKENHI Grant (Grant Number 15F13739) and Tsukuba 
City Government Office. 
 
REFERENCES 
[1] 
National Police Agency in Japan, “Statictics Repot 2012” , (in 
Japanese). 
[2] 
Disability and health fact sheet 352,World Health Organization 
“http://www.who.int/mediacentre/factsheets/fs352/en/index.html”, 
(Accessed 22 Feb 2017) 
[3] 
M. LaPlante and H. Kaye, “Demographics and trends in wheeled 
mobility equipment use and accessibility in the community,” Assistive 
Tech., vol. 22, no. 1, pp. 3–17, 2010. 
[4] 
H. Kaye, T. Kang and M. LaPlante, “Mobility device use in the United 
States. National Institute on Disability and Rehabilitation Research,” 
US Dept. Educ., vol.14, 2000. 
[5] 
B. Balcik, B. Beamon and K. Smilowitz, “Last Mile Distribution in 
Humanitarian Relief,” Journal of Intelligent Transportation Systems, 
Vol. 12, No. 2, 2008, p.51–63. 
[6] 
N. Hashimoto, S. Kato and S. Tsugawa, “A Cooperative Assistance 
System Between Vehicles for Elderly Drivers”, IATSS research, vol.33, 
No.1, 2009, p.35-41. 
[7] 
S. Tsugawa, S. Kato, N. Hashimoto, N. Minobe, M. Kawai, “Elderly 
driver assistance systems with cooperation between vehicles: the 
concept and experiments”, Proceddings of Intelligent Vehicles 
Symposium, 2007, pp.668-673. 
[8] 
N. Hashimoto, Y. Takinami and O. Matsumoto, “An Experimental 
Study on Vehicle Behavior to Wheel Chairs and Standing-type 
Vehicles at Intersection”, Proceedings of 13th International 
Conference on ITS Telecommunications, 2013, p.350–355. 
[9] 
A. Boyali , N. Hashimoto and O. Matsumato, "Hand posture control of 
a robotic wheelchair using a leap motion sensor and block sparse 
representation based classification.", The Third International 
Conference on Smart Systems, Devices and Technologies. 2014, 
pp.20-25. 
[10] A. Boyali and N. Hashimoto, “Block-Sparse Representation 
Classification based Gesture Recognition Approach for a Robotic 
Wheelchair”, Proceedings of Intelligent Vehicles Symposium, 2014, 
p.1133-1138. 
[11] N. Hashimoto, K. Tomita, A. Boyali, Y. Takinami and O. Matsumoto, 
“Surveillance of Capability, Acceptability, and Usability of Riding an 
Autonomous Wheelchair System in a Public Area: An Experimental 
Study”, Proceedings of Transportation Research Board 95th Annual 
Meeting, 2016. 
[12] A. Boyali and N. Hashimoto, Y.Takinami and S.Mita, “An 
Experimental Study Wheelchair Navigation Control by Gesture for 
elderly”, IEICE Technical Reort, 2015. 
[13] Tsukuba robot community for real worl experiments, http://mobility.rt-
tsukuba.jp/ , (Accessd 22 Feb 2017) 
[14] M. Omae, N. Hashimoto, T. Sugamoto and H. Shimizu, “Measurement 
of driver's reaction time to failure of steering controller during 
automatic driving”, Review of automotive engineering, Vol.26, No.2, 
pp.213-215. 
[15] N. Hashimoto, U. Ozguner and N. Sawant, “Evaluation of control in a 
convoy scenario”, Proceedings of Intelligent Vehicles Symposium, 
2011, pp.350-355 
[16] M. David, “Gesture and thought”, University of Chicago Press, 2008. 
[17] C. Hans, “Acceptance of 3d-gestures based on age, gender and 
experience”. MSc Thesis, Gjøvik University College, 2013. 
[18] G. Sukeshini, G. Joue and I. Mittelberg, "Understanding naturalness 
and intuitiveness in gesture production: insights for touchless gestural 
interfaces." Proceedings of the SIGCHI Conference on Human Factors 
in Computing Systems. ACM, 2011. 
[19] D. Paul. “Where the action is: the foundations of embodied interaction”, 
MIT press, 2004. 
[20] E. Ehsan and R. Vidal. "Sparse subspace clustering: Algorithm, theory, 
and applications." Pattern Analysis and Machine Intelligence, IEEE 
Transactions on 35.11 (2013): 2765-2781. 
[21] Organization of safety and spread for electric wheelchair 
http://www.den-ankyo.org, (in Japanese), (Accessd 22 Feb 2017)  
 
 
19
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

Inclusion of Down Syndrome in Architectural Design: Towards a Methodology  
Clémentine Schelings, Catherine Elsen 
LUCID-ULg 
University of Liège 
Liège, Belgium 
e-mail: {clementine.schelings; catherine.elsen}@ulg.ac.be 
 
 
Abstract—This paper develops an in-situ methodology to help 
architects insure better inclusion of people with Down 
syndrome all along preliminary phases of the architectural 
design process, and eventually to the designed space. This 
methodology first offers architects some design keys in regard 
of how people with Down syndrome interact with two types of 
spaces: their personal dwellings and some completely unknown 
spaces. The methodology then unfolds towards more pro-active 
inclusion of the participants thanks to playful expression of 
their feelings and perceptions. This paper discusses how this 
methodology relates to inclusive and universal principles, 
useful to design smart environments be they ICT-enabled or 
not. This paper closes on prevalent models of disability in 
architecture and how they articulate with the model of 
“architectural handicap”. 
Keywords-disability; Down syndrome; inclusive design; 
universal design; methodological framework. 
I. 
 INTRODUCTION 
This paper tackles the challenge of disability inclusion to 
architecture, disability considered here as a temporary or 
permanent condition likely to show up at any time of 
everyone’s life. Statistically speaking, disability concerns 
15% of the European population, i.e., more than 80 millions 
individuals [1]. Among them, only 20% are disabled from 
birth, while 80% will experience impairment later in life, as a 
result of an accident, an illness, ageing or a more temporary 
condition such as pregnancy [2]. We are therefore all 
concerned with disability, whatever our current situation. 
Designers are yet struggling with the inclusion of 
disabled people, given the variety of disabilities and the 
variety of adaptations those disabilities require on both 
spatial and functional levels. In architectural design, and in 
Belgium more specifically, norms about persons with 
reduced mobility (PRM) constitute one of the few 
frameworks available to help designers integrate the needs of 
people who use a wheelchair or blind people. This 
regulation, yet, does not take into account cognitive 
impairments (nor hearing loss) that are thus generally 
neglected during the architectural design process. Likewise, 
in ICT related fields, cognitive impairments seem to be less 
often considered than visual or hearing loss impairments. 
Consequently, this paper aims at offering concrete design 
tools to architects confronted to the needs of people with 
cognitive disabilities, and more specifically people with 
Down syndrome. The paper will first aim at studying the 
impact of architecture on the spatial perception of people 
with 
cognitive 
disabilities. 
In-situ 
observations 
of 
participants evolving through various spaces will provide 
some useful design keys in that regard. The methodology 
will then be expanded in order to include those users into a 
more active encounter with architecture, providing architects 
with fruitful information about how people with Down 
syndrome experience space on a more multisensory level.  
In Section 2, literature review and the resulting research 
questions are presented. Section 3 details the methodology 
developed in order to conduct the observations. Section 4 
describes the obtained results, presented in two subsections: 
design keys (Subsection A) and methodological keys 
(Subsection B) for inclusion of Down syndrome in 
architectural design. Section 5 closes on a theoretical 
discussion considering prevalent models of inclusion and 
disability in architecture and how these models should be 
revised in order to consider people with Down syndrome’s 
sensitiveness as opportunity rather than threat to the 
architectural design process. Some insights built in this paper 
might be relevant for universal/inclusive design for ICT 
related fields. 
II. 
STATE OF THE ART 
We highlight here two main observations from 
architectural state of the art. First, as observed by several 
phenomenologists, architecture suffers some kind of uni-
sensoriality hegemony. Architecture, according to these 
authors, has been reduced through the Modernist era to the 
sole consequence of visual expression and experience, 
neglecting the other perceptual senses and consequently 
deviating from the users’ multisensorial realities [3], [4], [5]. 
This hegemony, authors argue, has impoverished the 
architectural experience and, as a result, the whole design 
process [6]. Second, theories of environmental psychology 
and healing environments suggest that the architectural 
environment 
influences 
the 
wellbeing, 
considering 
architecture either as a factor having a positive (curative 
architecture) or a negative (disabling architecture) impact on 
the emotional and physical experience [7], [8]. 
Building on these two main observations, some authors 
propose to interact with disabled people and to integrate their 
perceptions as soon as early stages of the design process [9], 
[10]. This early integration helps architects consider other 
users than the “average, six-foot-tall, 20-years-old male, with 
perfect vision and a good grip [11 (p. 60.7)],” encouraging 
them to question and reinstate users’ multi-sensoriality and 
sensitivity into their work. In this case, disabled people are 
20
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

considered as experts and become a real source of creativity 
for designers [10]. The disability is then considered as an 
opportunity, both for architects who develop new ideas and 
for disabled people who take part in a process from which 
they are usually excluded. 
This design approach fits the inclusive design theory and 
its two main principles, i.e., (i) considering the users’ and 
designers’ complementarity given their respective specific 
knowledge and expertise [12] and (ii) re-integrating the 
users’ emotions and reactions in order to design sensitive 
architecture ensuring their wellbeing [13]. 
As opposed to this inclusive vision, more traditional 
approaches characterize disability as a constraint for both 
designers and users. Architects indeed sometimes apprehend 
the norms regarding disabled people rather as obstacles to 
their creativity [14]. Those traditional approaches, along with 
their regulations, moreover only consider limited variety of 
disabilities, not taking into account variations within the 
same disability. The main studied disabilities are motor 
impairments and blindness, while cognitive impairments are 
more rarely addressed, except for autism that has been 
widely explored. Yet, just like people with autism spectrum 
disorders, Tufvesson and Tufvesson argue people with Down 
syndrome present a remarkable hypersensitivity and a 
particular spatial perception [15]. Even studies aiming at 
“turning disability experience into expertise in assessing 
building accessibility [16 (p. 144)]” or at designing multi-
sensorial spaces [6] until now remained essentially focused 
on motor and visual impairments, neglecting the assessment 
of other peculiar ways to experience space. 
The resulting recommendations and designs are thus 
never perfectly adapted to the users with cognitive disability 
who can then feel excluded and misunderstood [17]. We 
therefore formulate the following two research questions: 
• 
How do people with Down syndrome perceive 
space at a multi-sensory level? 
• 
How to set up a specific methodology to approach 
and leverage Down syndrome’s specificities in 
architectural design? 
III. 
METHODOLOGY 
To answer those research questions, we build on a 
methodology of in-situ observation and interaction with 
disabled participants as suggested by Nijs and Heylighen 
[16]. Their methodology consists in considering disabled 
people as experts of their own peculiar way of experiencing 
spatiality and architecture. Through several cases studies, 
these researchers invited groups of disabled people (mainly 
persons with reduced mobility and visually impaired people) 
to experience a building and to discuss their own experience 
verbally, thanks to different keywords suggested by the 
researchers. While this section will develop how we 
implemented this methodology, Section 4 will come back on 
how and why this methodology had to be adapted given the 
communication difficulties of people with Down syndrome. 
Firstly, we proceeded to the selection of the participants 
affected by Down syndrome among the residents of a 
Belgian non-profit association welcoming adults with 
cognitive disabilities and specifically intended to develop 
residents’ artistic skills. Six participants were eventually 
chosen on the basis of several criteria such as the sex (to 
ensure gender parity), the housing type (in order to compare 
the participants’ experience in terms of living with family or 
living permanently in the residence) or the severity of their 
disability and the impact it could have on their capability to 
express their experiences and feelings (Tab. 1). 
Secondly, 
we 
conducted 
two 
phases 
of 
in-situ 
observations: first the visit of the residents’ own dwellings 
and later the discovery of a public building, a local town hall 
unknown by the participants. The goal here was to compare 
the spatial perceptions of people with Down syndrome when 
confronted to familiar vs. unknown spaces. Those two 
observation sequences were video-recorded for practical 
reasons. 
At the beginning of the visit of each dwelling, we set up a 
discussion table in order to collect some basic information 
such as, for instance, the resident’s age or favorite room(s). 
This stage also helped us create a climate of confidence with 
the participant and his or her referee (family member or close 
relative). We then organized a playful activity that consisted 
in visiting the resident’s three preferred rooms and 
interviewing him or her about his or her felt experience 
thanks to illustrated cards. 
This combination of observation and interview methods, 
close to the “shadowing” technique, enables the researcher to 
follow a person in his or her daily activities while asking him 
or her some questions to complete the observed information 
[18]. Within this framework, the researcher takes over the 
role of observer-as-participant, i.e., he or she spends more 
time observing than participating. This role has several 
benefits: it is especially adapted for short interviews, it 
enables real-time filling of observation grids and it ensures 
transparency of the research goals towards the observed 
subjects [19]. However, given the brevity of each session (40 
minutes in average), a mutual misunderstanding can occur 
between the observer and the observed person. Hence there 
is the need to quickly build confidence [19]. This could be 
achieved with the help of the participant’s relatives that were 
present.  
The methodology implemented during the visit of the 
town hall was rather similar: a few days later, we invited the 
same  six  participants  to  visit  three  rooms of the town hall, 
TABLE I.  
DEMOGRAPHIC PROFILE OF THE PARTICIPANTS 
HIGHLIGHTING SOME ADDITIONAL SPECIFICITIES 
# 
Table Column Head 
Gender 
Age 
Housing type 
Cognitive 
specificity 
Mobility 
specificity 
1 
female 
25 
family house 
/ 
artificial 
hip 
2 
female 
48 
residence 
/ 
slower 
motion 
3 
male 
27 
family house 
verbalizes 
through 
onomatopoeias 
/ 
4 
male 
36 
family house 
/ 
/ 
5 
male 
27 
residence 
/ 
/ 
6 
male 
49 
residence 
/ 
/ 
21
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

hall, this time chosen by the researcher in order to compare 
each participant’s reactions. The visit of those three selected 
rooms was made individually. In the meantime the five other 
participants were guided by a social worker for a photo 
recreational activity. The pictures taken by the residents, as 
well as drawings produced later, are an additional means of 
expression completing or confirming the information 
collected during the individual visits. 
IV. 
RESULTS 
The two next sections will present the results of the in-
situ observations, starting with design tools in regard of 
space perception and following with some methodological 
recommendations. 
A. Design Keys in Regard of Space Perception 
During 
the 
two 
observation 
phases, 
four 
main 
phenomena have been observed. 
Firstly, the people with Down syndrome who took part to 
this study all experienced some difficulties in identifying the 
limits between spaces that were not clearly delineated by a 
physical boundary. In the town hall, the reception and 
entrance halls were separated by a simple inner bay frame 
(Fig. 1), but the participants designated those two spaces as 
one single room. When asked to walk around the reception 
hall, they indeed systematically travelled both halls, 
obviously confused by the proximity of two sub-spaces 
whose functions were insufficiently distinct. Similarly in the 
case of private dwelling, one participant walked around the 
living room when asked to delineate the kitchen. 
Secondly, and in contrast with the previous point, people 
with Down syndrome who took part to this study paid 
particular attention to the privacy of a space and how this 
sense of privacy could make distinct one space from another. 
During the visits of their dwellings, the participants have 
always chosen their own bedroom as their favorite room, 
which underlines their need to have a personal space 
available. This characteristic could also be observed while 
experiencing  the  public  building,  especially  when  some 
 
 
Figure 1.  Reception and entrance halls separated by an inner bay frame. 
residents felt the need to be alone and left in search of some 
smaller, more comfortable and/or less traveled space to 
retreat to for some time. In the case of their private spaces 
(their rooms), privacy did, in spite of its intangible nature, 
build some boundary between two subspaces. This 
phenomenon was specifically observed in a bedroom shared 
by two residents who never crossed the invisible line 
dividing the room into two individual and appropriated 
zones. 
Thirdly, the participants demonstrated a particular 
attraction for light, bay windows, illuminated objects and 
surfaces. This characteristic was observed several times, 
particularly when participants were asked to point to their 
favorite object within a room. One of them, for instance, 
showed us his stereo, occupying a special spot on the 
windowsill of his bedroom, which was particularly well lit. 
Fourthly, our observations revealed the great importance 
of material landmarks in the everyday-life of the participants, 
especially in regard of their day-to-day rituals and habits. 
Those well-known elements, which could be objects, pieces 
of furniture or even a specific material (e.g., local brown 
stone), were reassuring to them especially because they 
reminded them of aspects of their daily life and 
environments. In one of the residences, we visited a living 
room that had just been rearranged and refurnished. Inside 
this living room, social workers had left a small wooden 
table (Fig. 2) greatly appreciated by the participants because 
it had been crafted by one of the residents. This small table, 
placed there as a landmark of the previous space 
configuration, greatly facilitated the occupants’ appropriation 
of this new way of organizing the room. The presence of this 
recognizable piece of furniture helped the acceptance of a 
new situation otherwise potentially disturbing. 
Besides those four design keys of perceiving space, we 
have observed two additional mechanisms engaged in 
different settings: the visuo-spatial memory participants 
developed in regard of everyday spaces, and the multi-
sensoriality participants deployed especially in unknown 
spaces. 
- 
 
Figure 2.  Wooden table in the living room of one residence: the 
reassuring landmark easing the space re-organisation and appropriation. 
22
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

When interviewed inside their dwellings, the residents 
generally looked beyond the current situation and appealed 
to their memory to describe the space as they generally 
experience it, rather than describing it in regard of its 
specificities at the time of observation. For instance, one 
participant stated that the living room was a place where “it 
was dark” while it was a bright middle of the afternoon at the 
time. The participant described the room as he usually 
perceives it in situation of most frequent use, i.e., when he 
watches TV in the evening, appealing to his visuo-spatial 
memory instead of his instant capacities of observation. 
In the town hall, moreover, participants largely mobilized 
their five senses to experience space. For example, they 
relied on their hearing to determine the level of activity of 
the rooms: one participant said that the entrance hall was 
“here, quiet, everything is quiet” because we were alone in 
the room, while another one later found the space 
“animated” because several employees were present at the 
time. We observed that multi-sensoriality was generally only 
engaged during the discovery phases of a new space or a 
potentially disturbing environment. 
B. Methodological Recommendations 
In this section, we summarize adaptations made to Nijs 
and Heylighen’s methodology [16] in order to make it more 
suitable to the specificities of people with cognitive disability 
(for which oral expression, for instance, can be difficult). 
The importance of the referee (family member, close 
relative or educator) was made clear during the first phases 
of “discussion tables” we added to the methodology: this 
person, acting as mediator between the observer and the 
observed person, played a crucial role in decoding both 
stakeholders’ words, intentions and behaviors and in 
ensuring their mutual understanding. In one particular case, 
the presence of the participants’ parents turned out to be 
essential to “translate” his personal vocabulary mainly 
composed of onomatopoeias. 
Expression of feelings and perceptual spatial experiences 
were moreover greatly facilitated by the use of four cards 
illustrated with cartoony human faces, each featuring one of 
the most widespread human primary emotions (happiness, 
sadness, nervousness and fear). These cards, chosen with the 
help of a psychologist specialized in assisting people with 
Down syndrome, were voluntary simple (free of superfluous 
details) and limited in their number in order to help 
participants express their feelings as accurately as possible. 
Participants were nevertheless free to combine several 
pictures to enrich their answers if necessary. Those cards, as 
suggested by Chase, adequately complement the content 
usually collected through narrative inquiry [20]. One 
important preliminary step, when presenting these cards for 
the first time, was to proceed to the emotions’ recognition, 
i.e., to align our understanding to what the cards meant in the 
eyes of the participants. For instance, one resident had 
identified the card of the scared figure as a person “who 
winced”, and this definition was therefore used for the rest of 
those observations. Those cards proved really useful to 
interact with the participants once on the field, and could 
efficiently replace the keywords used by Nijs and Heylighen 
[16] when interacting with people experiencing difficulties 
with verbal expression. 
From an organizational perspective, we visited each 
room in two phases: first, we started interviewing the 
participant, and then we let him or her walk around the room. 
During the visit of one dwelling, one of the residents at first 
refused to sit and to answer our questions. We had to wait 
until he stopped moving before obtaining a single answer. 
Organizing the intervention in several, distinct and 
repeatable phases thus allowed us to progressively channel 
the resident’s attention on our questions. We moreover 
observed that interviewing each participant separately proved 
particularly important to avoid participants influencing each 
other: at one point of the town hall visit, all six participants 
started to interact about the space and the influence of one of 
them was clearly at the disadvantage of self-expression. 
Eventually, considering additional means of expression, 
such as photography or drawing for instance, proved very 
useful to complete some participants’ comments. 
V. 
DISCUSSION 
Our in-situ observations contribute to an adapted 
methodology and to design keys useful for architects willing 
to include people with Down syndrome (their specific needs, 
their specific ways of experiencing spaces) into preliminary 
phases of their design processes. Since the results presented 
here are issued from six participants only, the findings 
should not be generalized to a larger group. As Kinnaer, 
Baumers and Heylighen underline in their research about 
autism, individual preferences play an important role for the 
perception and appreciation of certain spaces and should not 
be dismissed [21]. This has proven also true for people with 
Down syndrome, as one of the participants distinguished 
from the five others by his particular appeal for dark spaces. 
In this case, the participant considered his own bedroom, 
indeed rather dark, as his personal shelter of privacy, a space 
where he could freely unleash his emotions. He therefore 
associated dark spaces to this personal space, a protective 
cocoon where he could express himself untroubled. 
Designers willing to replicate the suggested adapted 
methodology might apply the saturation criterion [22] as a 
way to capture both specific and shared spatial perceptions. 
 Down syndrome, as any other cognitive disability, 
consequently ought to be considered as a complex condition, 
characterized by a variety of realities confined to a global 
medical model [17]. Yet, current theoretical and practical 
disability frameworks hardly take into account this 
variability. On the one hand, norms and regulations have the 
tendency to reduce the user to a single, « representative » 
profile: even the architectural norms applied to the inclusion 
of persons with reduced mobility (PRM) tend to dismiss 
personal specificities one wheelchair user can develop in 
regard of another. Theories such as Universal design, on the 
other hand, intend to transform architecture into some 
universal product including the diversity of needs of all 
potential users [23]. Such Universal architecture, by doing 
so, might even reduce the model of the user and his/her uses, 
as each Universal user potentially accumulates the 
incapacities of a larger diversity of users, the design object 
23
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

being consequently reduced to its lowest common possible 
use [24]. 
This research is therefore rather in favor of the inclusive 
model, taking into account the specificities of users and 
considering them, as much as possible, as creative input. We 
argue the methodology developed in this paper, favoring 
playfulness rather than simple consultation of the end-users, 
might potentially help architects in conducting in-situ 
research and in gaining knowledge about how specific 
groups of people with Down syndrome interact with 
architecture. Participants, considered as experts of their own 
disability and their own specific ways of experiencing space, 
might in this way contribute to architectural projects more 
prone to benefit the greatest number of users. As much as 
hypersensitivity [15], people with Down syndrome’s specific 
ways to apprehend an architectural space, for instance 
through higher multisensoriality, could equip designers in 
their perception of end-users’ needs. Whereas universal 
design aims at the lowest common denominator, inclusive 
design, we argue, provides more diversified avenues for 
design exploration. 
Including participants with Down syndrome as soon as 
preliminary phases of the architectural design process, and 
specifically empowering them with a certain expertise, 
moreover suggests a possible evolution of current models of 
handicap in architecture. Disability has originally been 
considered the result of a medical condition, therefore 
building the “medical model” of disability in architecture. 
This model, focusing exclusively on disability as an illness 
together with its symptoms, nurtured a hygienist design of 
specialized institutions. Later, a social model of disability in 
architecture rather focused on the human being rather than 
on the mere “patient” and integrated notions such as “origin, 
milieu, education, profession, economical position and social 
status [25 (p. 11)], quoted by [26 (p. 19)]” to the design of 
adapted spaces. This social model, as a consequence, 
informed the design of healing environments outside the 
institutionalized boundaries of the hospitals and proposed 
living environments “accommodating people with a social 
framework and, thus, supporting residents in developing 
their identity [26 (p. 24)].” 
Following our observations, we would advocate a third 
model of disability, i.e., architecture considered as a 
potentially disabling factor. This model, as an extension of 
the social model, would “focus on individuality, difference 
(instead of commonality), experience and giving voice to 
people [26 (p. 25)],” while redefining the role of architecture 
and the architects. 
This concept, introduced by Goldsmith in the context of a 
research focusing on motor and visual impairments [27], 
states that architecture can constitute a proper physical 
barrier as much for disabled users than for people with 
temporary limited mobility (injured or pregnant person for 
instance). This “architectural handicap” therefore translates 
into an uncomfortable and constraining situation for the user, 
caused by the lack of consideration or anticipation from the 
designer that would not, or could not take into account the 
specificities of a larger group of potential users [8]. 
We argue this notion of architectural handicap extends to 
any type of disability, including cognitive ones, as well as 
any type of design field, including ICT-related ones. In the 
case of people with Down syndrome, our results suggest that 
architecture sometimes not only constitutes some physical 
barrier to one’s mobility, but also a psychological barrier. 
Unclearly delineated spaces, for instance, can generate loss 
of reference points, misunderstanding of sub-functions and 
consequently loss of autonomy and social exclusion. 
Architecture and architects therefore have a crucial role 
to play in terms of avoiding such handicapping situations: 
the design keys and methodology proposed in this paper 
offer support to architects who wish to deal with this new 
responsibility. 
CONCLUSION AND FUTURE WORK 
This paper develops a methodology to approach Down 
syndrome in architectural design, in line with inclusive 
design theories. The originality of this methodology lies in 
its early integration of participants and its playfulness, 
enabling to go beyond simple consultation with users and to 
value the disability experience as an expertise. 
The methodology and design keys suggested in this paper 
may be suitable to other user profiles, such as people bearers 
of another cognitive impairment, seniors or children who 
share some characteristics with people with Down syndrome. 
Our research also highlights the limits of the current 
normative frameworks. Nonetheless, the actual lack of 
consideration 
for 
people 
with 
cognitive 
impairment 
compared with other disabilities, like motor impairment, 
demonstrates the benefits of such a norm. Since a strict 
regulatory framework would not be an adequate solution, 
this paper rather paves the way for a toolbox for designers, 
encouraging them to take into account people with cognitive 
disability and suggesting them some interaction techniques 
to reach this goal. 
No longer considering disability as a threat or obstacle 
for architectural design, this work rather suggests that people 
with Down syndrome experience space with some specific 
sensitiveness. This sensitiveness could be leveraged as a 
source of creativity for the designer (“disability as 
opportunity”), while architecture could be considered as a 
potentially handicapping factor for the user (“architectural 
handicap”). 
ACKNOWLEDGMENTS 
The authors would like to thank all who contributed to 
this research, in particular the team and residents of “Les 
Hautes Ardennes” center (Vielsalm, Belgium). The authors 
also thank Ms. Henry, psychologist at “Cité de l’Espoir” 
(Andrimont, Belgium), for her advices in carefully choosing 
the content of the illustrated cards. 
REFERENCES 
[1] AViQ Agence pour une Vie de Qualité. How many people are 
disabled in Belgium, in Wallonia? [Online]. Available from: 
https://www.aviq.be/handicap/questions/infos_conseils/statisti
ques.html 2017.02.20 
24
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

[2] AViQ Agence pour une Vie de Qualité. The disabled 
workers? 
Workers! 
[Online]. 
Available 
from: 
https://www.aviq.be/handicap/pdf/documentation/publications
/emploi/Brochure-Travailleurs-handicapes.pdf 2017.02.20 
[3] A. Dalhin, On architecture, aesthetic experience and the 
embodied mind. Seven Essays. Stockholm: Royal Institute of 
Technology, School of Architecture, KTH, 2002. 
[4] M. Merleau-Ponty, Phenomenology of perception. London 
and New York: Routledge, 2002. 
[5] J. Pallasmaa, The eyes of the skin. Chichester: John Wiley, 
2005. 
[6] J. Herssens and A. Heylighen, “Haptics and vision in 
architecture: designing for more senses,” Conference Sensory 
Urbanism, Flâneur Press, Jan. 2008, pp. 102-112, ISBN: 978-
0-95599-060-1. 
[7] V. Van der Linden, M. Annemans, and A. Heylighen, “You’d 
want an energy from a building: user experience of healing 
environment in a Maggie’s cancer caring centre,” Proceedings 
of the 3rd European Conference on Design4Health, Design 
Society, Jul. 2015, pp. 1-9, ISBN: 978-1-84387-385-3. 
[8] H. Froyen, E. Verdonck, D. De Meester, and A. Heylighen,  
“Documenting handicap situations and eliminations through 
universal design patterns,” Australasian Medical Journal, vol. 
1 (12), pp. 199-203, 2009, doi:10.4066/AMJ.2009.158. 
[9] A. Heylighen, P. Devlieger, and M. Strickfaden, “Design 
expertise as disability and vice versa,” Communicating (by) 
Design, Chalmers University of Technology, Hogeschool 
voor Wetenschap & Kunst, School of Architecture Sint-
Lucas, Apr. 2009, pp. 227-235, ISBN: 978-9-08132-380-2. 
[10] J. Herssens and A. Heylighen, “Haptic architecture becomes 
architectural hap,” Ergonomics for a future, Proceedings of 
the 39th Annual Congres of the Nordic Ergonomic Society 
(NES), NES, Oct. 2007, ISBN: 978-9-19715-212-9. 
[11] V. Fletcher, “A neighborhood fit for people,” in Universal 
design handbook, W. Preiser and E. Ostroff, Eds. New York: 
McGraw-Hill, pp. 60.1–60.15, 2001. 
[12] A. Heylighen and M. Bianchin, “How does inclusive design 
relate to good design? Designing as a deliberative enterprise,” 
Design Studies, vol. 34 (1), pp. 93-110, Jan. 2013, 
doi:10.1016/j.destud.2012.05.002. 
[13] M. Ferreira, D. Cabral de Mello, and J. Duarte, “Embodied 
emotions: a phenomenological approach to computation to 
explore empathy through architecture,” Digital Physicality,  
Proceedings of the 30th eCAADe Conference, vol. 2, Czech 
Technical University in Prague, Faculty of Architecture, Sep. 
2012, pp. 599-604. 
[14] G. Nijs, P. Vermeersch, P. Devlieger, and A. Heylighen,  
“Extending the dialogue between design(ers) and disabled 
use(rs): from conversation to embodied skill,” in DS 60: 
Proceedings of Design 2010, the 11th International Design 
Conference, D. Marjanovic, M. Storga, N. Pavkovic, and N. 
Bojcetic, Eds. Dubrovnik: Design society, pp. 1817-1826, 
2010. 
[15] C. Tufvesson and J. Tufvesson, “The building process as a 
tool towards an all-inclusive school. A Swedish example 
focusing on children with defined concentration difficulties 
such as ADHD, autism and Down's syndrome,” Journal of 
Housing and the Built Environment, vol. 24 (1), pp. 47-66, 
Apr. 2009, doi:10.1007/s10901-008-9129-6. 
[16] G. Nijs and A. Heylighen,  “Turning disability experience into 
expertise in assessing building accessibility: a contribution to 
articulating disability epistemology,” Alter, vol. 9 (2), pp. 
144-156, Apr. 2015, doi:10.1016/j.alter.2014.12.001. 
[17] K. McAllister and B. Maguire, “Design considerations for the 
autism spectrum disorder-friendly key stage 1 classroom,” 
Support for Learning, vol.  27 (3), pp. 103-112, Aug. 2012, 
doi:10.1111/j.1467-9604.2012.01525.x. 
[18] S. McDonald, “Studying actions in context: a qualitative 
shadowing method for organizational research,” Qualitative 
Research, 
vol. 
5 
(4), 
pp. 
455-473, 
Nov. 
2005, 
doi:10.1177/1468794105056923. 
[19] R. L. Gold, “Roles in sociological field observations,” Social 
Forces, 
vol. 
36 
(3), 
pp. 
217-223, 
Mar. 
1958, 
doi:10.2307/2573808. 
[20] S. E. Chase, “Narrative inquiry Still a field in the making,” in 
The sage handbook of qualitative research 4, N. K. Denzin & 
Y. S. Lincoln, Eds. Oaks: Sage, pp. 421-434, 2011. 
[21] M. Kinnaer, S. Baumers, and A. Heylighen, “How do people 
with autism (like to) live?,” in Inclusive designing Joining 
usability, accessibility, and inclusion, P. Langdon, J. Lazar, A. 
Heylighen, and H. Dong, Eds. Cambridge: Springer, pp. 175-
185, 2014. 
[22] G. Guest, A. Bunce, and L. Johnson, “How many interviews 
are enough? An experiment with data saturation and 
variability,” Field Methods, vol. 18 (1), pp. 59-82, Feb. 2006, 
doi: 10.1177/1525822X05279903. 
[23] R. L. Mace, “Universal design in housing,” Assistive 
Technology, vol. 10 (1), pp. 21-28, Oct. 2010, doi: 
10.1080/10400435.1998.10131957. 
[24] M. Winance, “Universal design and the challenge of diversity. 
Reflections on the principles of universal design, based on 
empirical research of people’s mobility,” Disability and 
Rehabilitation, vol. 36 (16), pp. 1334-1343, Jul. 2014, doi: 
10.3109/09638288.2014.936564. 
[25] N. Mens and C. Wagenaar, The architecture of the elderly. 
Building for housing and care. Rotterdam: NAi Uitgevers, 
2009. 
[26] I. Van Steenwinckel, Offering architects insights into living 
with dementia Three case studies on orientation in space-
time-identity (doctoral dissertation). 
Kampenhout: 
KU 
Leuven, 2015. 
[27] S. Goldsmith, Designing for the disabled The new paradigm. 
New-York: Routledge, pp. 147-158, 2012. 
 
25
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

Development of a Sharing System for Virtual Grafﬁti of Tourism Information among
Tourists using Image Recognition
Rei Miyagawa ∗, Keima Kumano ∗, Takayuki Kunieda †, Tetsuya Ikeda †, Naka Gotoda‡,
Masanobu Kii§ and Rihito Yaegashi§,
∗Graduate School of Engineering, Kagawa University, Japan
Hayashi-cho 2217-20, Takamatsu, Kagawa 761-0396, Japan
Email: s16g471@stu.kagawa-u.ac.jp
†Ricoh Company, Ltd., Japan
Ginza 8-13-1, Chuou-ku, Tokyo 104-8222, Japan
Email: takayuki.kunieda@nts.ricoh.co.jp
‡ Information Technology Center, Kagawa University
Saiwai-cho 1-1, Takamatsu, Kagawa 760-8521, Japan
Email: gotoda@eng.kagawa-u.ac.jp
§Faculty of Engineering, Kagawa University
Hayashi-cho 2217-20, Takamatsu, Kagawa 761-0396, Japan
Email: rihito@eng.kagawa-u.ac.jp
Abstract—We developed a sharing system for virtual grafﬁti of
tourism information among tourists using image recognition.
A tourist writes grafﬁti on a photo taken at a tourist spot
using virtual grafﬁti interface and shares the grafﬁti among
tourists who take similar photos on the system. Administrator
of tourist destination need not to prepare for any information.
Using our system, tourists can share tourism information with
other tourists who visited the same place just by taking a photo.
This paper describes the sharing system for virtual grafﬁti of
tourism information among tourists using image recognition.
Keywords–Virtual-grafﬁti;
Tourism-information;
Image-
recognition
I.
INTRODUCTION
The most important source of information for tourists is the
reviews from other tourists [1]. Tourists can look at personal
blogs and SNS to get reviews. However, when we focus on
the information that tourists get during sightseeing, most of
information is prepared by the tourist operator.
As the media to share reviews at tourists spot, there are
communication notebooks (grafﬁti notebooks) which are put
at shops and facilities, and grafﬁti on tourism resources. We
can write whatever in our mind by handwriting, and notes by
handwriting are more correct than typing [2]. It means that
handwritten information is effective for sharing. However, it
is insufﬁcient that the tourists voluntarily share information
without damaging the tourism resources in anywhere.
We developed the sharing system for tourist information
that shares information at the tourist spots and encourages
a casual input of information. Using the image recognition
technology, this system realizes to attach the scenes at tourist
spots to tourism information left by tourists by taking a
photo at a touring spot which is the general behaviour during
sightseeing. Besides, it does not limit the writing space for an
individual, and many and unspeciﬁed tourist write something
on an objective. The system provides virtual grafﬁti interface
like real grafﬁti that people write on one object. This paper
describes the development of a sharing system for virtual
grafﬁti of tourism information among tourists using image
recognition.
II.
DEVELOPMENT
This section describes the development of our prototype
system, which can share tourism information. The system
shares comments (Grafﬁti), which were written on a photo
by a tourist with other tourists using image-recognition. 2.1
explains the outline of the system. 2.2 describes the ﬂow of
system utilization during sightseeing.
A. System Overview
Figure 1 shows the outline of the sharing system for
virtual grafﬁti of tourist information. The system consists of
management server and virtual grafﬁti application.
We developed the Virtual grafﬁti application as Web appli-
cation, and thus it can run on tourists’mobile devices access to
the application without install special software. The application
has four functions, upload function, display function, grafﬁti
function and set up function. Upload function uploads a photo
that was taken by tourist (taken photo) to the management
server. Display function displays content which was created
by creation function in management server. The content is
the image which combines photos and some grafﬁti. Grafﬁti
function adds grafﬁti to the content and tourists handwrite
grafﬁti on the content. Set up function sets up the users ’
attribute which is given to grafﬁti.
The management server manages Grafﬁti Library. The
management server has four functions, Registration function,
Search function, Creation function and Save function, and has
registered images and a library. Registration function registers
a taken photo as a registered image. Search function searches
registered images which are similar to a taken photo in Library.
Search function uses Ricoh Visual Search (RVS) Technology
[3] developed by Ricoh Innovations Corporation at Silicon Val-
ley in U.S. It analyzes and quantiﬁes the features of image, and
can register the data and rapidly searches the database.Create
function creates content which combines registered images
with grafﬁti. Save function saves grafﬁti in library. In the
grafﬁti library, there are taken photos and registered images
in JPEG format and Grafﬁti in GIF format. Metadata include
information of creating content and user type.
26
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments

Figure 1. The Outline of the Sharing System for Virtual Grafﬁti of Tourist
Information
Figure 2. The Use Image of the Share System for
Virtual Grafﬁti of Tourist Information.
B. Workﬂow of the System
Figure 2 shows the use image of the share system for
virtual grafﬁti of tourist information. Users of the system are
tourists who visit tourist spots. A tourist takes a photo in tourist
spot. The taken photo is uploaded to management server and
registered image is searched. If there is not registered image
which is similar to the taken photo in the library, it will be
registered in the library as a new registered image. If there is
a similar image, content which is added to past grafﬁti will be
created and sent to virtual grafﬁti application. Tourists look at
the content and add grafﬁti on it.Figure 3 shows the screen of
grafﬁti interface which a user wrote grafﬁti.
Figure 3. Screen of Grafﬁti Interfacen
III.
CONCLUSION
This paper described the development of a sharing system
for virtual grafﬁti of tourism information among tourists using
image recognition. Using the image recognition technology,
the system provides virtual grafﬁti interface, which is attached
to a scene in tourist spot and add new grafﬁti. The system
shares comments as grafﬁti which were described on a photo
by tourist with other tourists using image-recognition. We have
realized the system to provide the virtual grafﬁti interface
to a scene by taking a photo at a touring spot which is the
general behaviour during sightseeing. The sharing system of
tourism information gives metadata to the registered image,
grafﬁti, and taken photos. We can analyze grafﬁti and tourism
behaviours using metadata. Now, we are planning to conduct
a demonstration experiment from stored images and grafﬁti to
conﬁrm the effect of this system on the sharing of tourism
information at tourist spots.
REFERENCES
[1]
S. W. Litvin, R. E. Goldsmith, and B. Pan, “Electronic word-of-mouth
in hospitality and tourism management,” Tourism management, vol. 29,
no. 3, 2008, pp. 458–468.
[2]
M. D. Hamzah, S. Tano, M. Iwata, and T. Hashiyama, “Effectiveness of
annotating by hand for non-alphabetical languages,” in Proceedings of
the SIGCHI conference on Human Factors in computing systems. ACM,
2006, pp. 841–850.
[3]
“Ricoh
Visual
Search
(RVS)
Technology,”
2017,
URL:
http://www.ricoh.com/technology/tech/044search.html[accessed
:
2017 − 02 − 19].
27
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-589-0
SMART ACCESSIBILITY 2017 : The Second International Conference on Universal Accessibility in the Internet of Things and Smart Environments
Powered by TCPDF (www.tcpdf.org)

