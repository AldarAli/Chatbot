COMPUTATION TOOLS 2015
The Sixth International Conference on Computational Logics, Algebras,
Programming, Tools, and Benchmarking
ISBN: 978-1-61208-394-0
March 22 - 27, 2015
Nice, France
COMPUTATION TOOLS 2015 Editors
Claus-Peter R√ºckemann, Leibniz Universit√§t Hannover / Westf√§lische
Wilhelms-Universit√§t M√ºnster / North-German Supercomputing Alliance
(HLRN), Germany
Pascal Lorenz, University of Haute Alsace, France

COMPUTATION TOOLS 2015
Forward
The Sixth International Conference on Computational Logics, Algebras, Programming, Tools,
and Benchmarking (COMPUTATION TOOLS 2015), held between March 22-27, 2015 in Nice,
France, continued a series of events dealing with logics, algebras, advanced computation
techniques, specialized programming languages, and tools for distributed computation. Mainly,
the event targeted those aspects supporting context-oriented systems, adaptive systems,
service computing, patterns and content-oriented features, temporal and ubiquitous aspects,
and many facets of computational benchmarking.
The conference had the following tracks:
ÔÇ∑
Advanced computation techniques
ÔÇ∑
Tools for distributed computation
ÔÇ∑
Logics
Similar to the previous edition, this event attracted excellent contributions and active
participation from all over the world. We were very pleased to receive top quality
contributions.
We take here the opportunity to warmly thank all the members of the COMPUTATION TOOLS
2015 technical program committee, as well as the numerous reviewers. The creation of such a
high quality conference program would not have been possible without their involvement. We
also kindly thank all the authors that dedicated much of their time and effort to contribute to
COMPUTATION TOOLS 2015. We truly believe that, thanks to all these efforts, the final
conference program consisted of top quality contributions.
Also, this event could not have been a reality without the support of many individuals,
organizations and sponsors. We also gratefully thank the members of the COMPUTATION
TOOLS 2015 organizing committee for their help in handling the logistics and for their work that
made this professional meeting a success.
We hope COMPUTATION TOOLS 2015 was a successful international forum for the exchange of
ideas and results between academia and industry and to promote further progress in the area
of computational logics, algebras, programming, tools, and benchmarking. We also hope that
Nice, France provided a pleasant environment during the conference and everyone saved some
time to enjoy the charm of the city.

COMPUTATION TOOLS 2015 Chairs
COMPUTATION TOOLS Advisory Chairs
Kenneth Scerri, University of Malta, Malta
Alexander Gegov, University of Portsmouth, UK
Ahmed Khedr, University of Sharjah, UAE
COMPUTATIONAL TOOLS Industry/Research Chairs
Torsten Ullrich, Fraunhofer Austria Research GmbH - Graz, Austria
Zhiming Liu, UNU-IIST, Macao
COMPUTATION TOOLS Publicity Chair
Lev Naiman, University of Toronto, Canada
Ingram Bondin, University of Malta, Malta
Tom√°≈° Bubl√≠k, Czech Technical University in Prague, Czech Republic

COMPUTATION TOOLS 2015
Committee
COMPUTATION TOOLS Advisory Chairs
Kenneth Scerri, University of Malta, Malta
Alexander Gegov, University of Portsmouth, UK
Ahmed Khedr, University of Sharjah, UAE
COMPUTATIONAL TOOLS Industry/Research Chairs
Torsten Ullrich, Fraunhofer Austria Research GmbH - Graz, Austria
Zhiming Liu, UNU-IIST, Macao
COMPUTATION TOOLS Publicity Chair
Lev Naiman, University of Toronto, Canada
Ingram Bondin, University of Malta, Malta
Tom√°≈° Bubl√≠k, Czech Technical University in Prague, Czech Republic
COMPUTATION TOOLS 2015 Technical Program Committee
Yas Aksultanny, Arabian Gulf University, Bahrain
Youssif B. Al-Nashif, Old Dominion University, USA
Adel Alimi, University of Sfax, Tunisia
Fran√ßois Anton, Technical University of Denmark, Denmark
Henri Basson, University of Lille North of France (Littoral), France
Steffen Bernhard, TU-Dortmund, Germany
Ateet Bhalla, Oriental Institute of Science & Technology - Bhopal, India
Paul-Antoine Bisgambiglia, Universit√© de Corse, France
Narhimene Boustia, Saad Dahlab University - Blida, Algeria
Azahara Camacho-Magri√±√°n, Universidad de C√°diz, Spain
Luca Cassano, University of Pisa, Italy
Emanuele Covino, Universit√† di Bari, Italy
Hepu Deng, RMIT University - Melbourne, Australia
Rene de Souza Pinto, University of Sao Paulo, Brazil
Craig C. Douglas, University of Wyoming / Yale University, USA
Ant√≥nio Dourado, University of Coimbra, Portugal
Eugene Feinberg, Stony Brook University, USA
Tommaso Flaminio, University of Insubria, Italy

Janos Fodor, Obuda University, Hungary
Alexander Gegov, University of Portsmouth, UK
Victor Gergel, University of Nizhni Novgorod (UNN), Russia
Veronica Gil-Costa, University of San Luis, Argentina
Luis Gomes, Universidade Nova de Lisboa, Portugal
Yuriy Gorbachev, Geolink Technologies LLC, Russia
George A. Gravvanis, Democritus University of Thrace, Greece
Rajiv Gupta, University of California - Riverside, USA
Fikret Gurgen, Bogazici University - Istanbul, Turkey
Hani Hamdan, √âcole Sup√©rieure d‚Äô√âlectricit√© (SUP√âLEC), France
Nawaz Khan, Middlesex University London, UK
Cornel Klein, Siemens AG - Munich, Germany
Stano Krajci, Safarik University - Kosice, Slovakia
Isaac Lera, University of the Balearic Islands, Spain
Tsung-Chih Lin, Feng-Chia University, Taichung, Taiwan
Giuseppe Longo, Ecole Normale Sup√©rieure Paris, France
Glenn R. Luecke, Iowa State University, USA
Reza Madankan, University of Texas MD Anderson Cancer Center, USA
Elisa Marengo, Free University of Bozen-Bolzano, Italy
Roderick Melnik, Wilfrid Laurier University, Canada
Julian Molina, University of Malaga, Spain
Francisco Jos√© Monaco, University of S√£o Paulo, Brazil
Susana Mu√±oz Hern√°ndez, Universidad Polit√©cnica de Madrid, Spain
Gianina Alina Negoita, Iowa State University, USA
Cecilia E. Nugraheni, Parahyangan Catholic University - Bandung, Indonesia
Flavio Oquendo, European University of Brittany/IRISA-UBS, France
Javier Panadero, Universitat Aut√≤noma de Barcelona, Spain
Juan Jose Pardo, University of Castilla-la Mancha, Spain
Vangelis Paschos, LAMSADE - University Paris-Dauphine, France
Mario Pavone, University of Catania, Italy
Mikhail Peretyat'kin, Institute of mathematics and mathematical modeling, Kazakhstan
Alexandre Pinto, ISG - Royal Holloway University of London, UK / Instituto Superior da Maia,
Portugal
Enrico Pontelli, New Mexico State University, USA
Corrado Priami, CoSBi & University of Trento, Italy
Marcus Randall, Bond University, Australia
Dolores Rexachs, University Autonoma of Barcelona (UAB), Spain
Morris Riedel, University of Iceland, Iceland / Juelich Supercomputing Centre, Germany
Guido Sciavicco, University of Murcia, Spain
Evgenia Smirni, College of William and Mary - Williamsburg, USA
Patrick Siarry, Universit√© de Paris 12, France
Hooman Tahayori, Ryerson University, Canada
James Tan, SIM University, Singapore
Torsten Ullrich, Fraunhofer Austria Research GmbH, Austria

Miroslav Velev, Aries Design Automation, USA
Zhonglei Wang, Karlsruhe Institute of Technology, Germany
Chao-Tung Yang, Tunghai University, Taiwan
Marek Zaremba, Universite du Quebec en Outaouais - Gatineau, Canada
Naijun Zhan, Institute of Software/Chinese Academy of Sciences - Beijing, China

Copyright Information
For your reference, this is the text governing the copyright release for material published by IARIA.
The copyright release is a transfer of publication rights, which allows IARIA and its partners to drive the
dissemination of the published material. This allows IARIA to give articles increased visibility via
distribution, inclusion in libraries, and arrangements for submission to indexes.
I, the undersigned, declare that the article is original, and that I represent the authors of this article in
the copyright release matters. If this work has been done as work-for-hire, I have obtained all necessary
clearances to execute a copyright release. I hereby irrevocably transfer exclusive copyright for this
material to IARIA. I give IARIA permission or reproduce the work in any media format such as, but not
limited to, print, digital, or electronic. I give IARIA permission to distribute the materials without
restriction to any institutions or individuals. I give IARIA permission to submit the work for inclusion in
article repositories as IARIA sees fit.
I, the undersigned, declare that to the best of my knowledge, the article is does not contain libelous or
otherwise unlawful contents or invading the right of privacy or infringing on a proprietary right.
Following the copyright release, any circulated version of the article must bear the copyright notice and
any header and footer information that IARIA applies to the published article.
IARIA grants royalty-free permission to the authors to disseminate the work, under the above
provisions, for any academic, commercial, or industrial use. IARIA grants royalty-free permission to any
individuals or institutions to make the article available electronically, online, or in print.
IARIA acknowledges that rights to any algorithm, process, procedure, apparatus, or articles of
manufacture remain with the authors and their employers.
I, the undersigned, understand that IARIA will not be liable, in contract, tort (including, without
limitation, negligence), pre-contract or other representations (other than fraudulent
misrepresentations) or otherwise in connection with the publication of my work.
Exception to the above is made for work-for-hire performed while employed by the government. In that
case, copyright to the material remains with the said government. The rightful owners (authors and
government entity) grant unlimited and unrestricted permission to IARIA, IARIA's contractors, and
IARIA's partners to further distribute the work.

Table of Contents
Advanced Computation of a Sparse Precision Matrix
Mohammed Elanbari, Reda Rawi, Michele Ceccarelli, Othamane Bouhali, and Halima Bensmail
1
A Specialized Recursive Language for Capturing Time-Space Complexity Classes
Emanuele Covino and Giovanni Pani
8
Hardware Realization of Robust Controller Designed Using Reflection Vectors
Jan Ciganek, Michal Kocur, and Stefan Kozak
14
Combining Code Refactoring and Auto-Tuning to Improve Performance Portability of High-Performance
Computing Applications
Chunyan Wang, Shoichi Hirasawa, Hiroyuki Takizawa, and Hiroaki Kobayashi
20
The Study of Statistical Simualtion for Multicore Processor Architectures
Jongbok Lee
27
A New Refutation Calculus With Logical Optimizations for PLTL
Mauro Ferrari, Camillo Fiorentini, and Guido Fiorino
31
Powered by TCPDF (www.tcpdf.org)

Advanced Computation of a Sparse Precision Matrix
HADAP: A Hadamard-Dantzig Estimation of a Sparse Precision Matrix
Mohammed Elanbari‚àó, Reda Rawi‚Ä†, Michele Ceccarelli‚Ä†, Othmane Bouhali‚Ä°, Halima Bensmail‚Ä†
‚àóSidra Medical and Research Center, Qatar Foundation
‚Ä†Qatar Computing Research Institute, Qatar Foundation
‚Ä°Texas A&M University-Qatar,
Doha, Qatar
Email: ‚àómelanbari@sidra.org; ‚Ä†{rrawi, mceccarelli, hbensmail}@qf.org.qa; ‚Ä°othmane.bouhali@qatar.tamu.edu
Abstract‚ÄîEstimating large sparse precision matrices is an in-
teresting and challenging problem in many Ô¨Åelds of sciences,
engineering, and humanities, thanks to advances in computing
technologies. Recent applications often encounter high dimension-
ality with a limited number of data points leading to a number
of covariance parameters that greatly exceeds the number of
observations. Several methods have been proposed to deal with
this problem, but there is no guarantee that the obtained
estimator is positive deÔ¨Ånite. Furthermore, in many cases, one
needs to capture some additional information on the setting of
the problem. In this work, we propose an innovative approach
named HADAP for estimating the precision matrix by minimizing
a criterion combining a relaxation of the gradient-log likelihood
and a penalization of lasso type. We derive an efÔ¨Åcient Alternating
Direction Method of multipliers algorithm to obtain the optimal
solution.
Keywords‚ÄìCovariance
matrix;
Frobenius
norm;
Gaussian
graphical model; Precision matrix; Alternating method of multi-
pliers; Positive-deÔ¨Ånite estimation; Sparsity..
I.
INTRODUCTION
Recent applications often encounter high dimensionality
with a limited number of data points leading to a number
of covariance parameters that greatly exceeds the number of
observations. Examples include marketing, e-commerce, and
warehouse data in business; microarray, and proteomics data
in genomics and heath sciences; and biomedical imaging,
functional magnetic resonance imaging, tomography, signal
processing, high-resolution imaging, and functional and longi-
tudinal data. In biological sciences, one may want to classify
diseases and predict clinical outcomes using microarray gene
expression or proteomics data, in which hundreds of thousands
of expression levels are potential covariates, but there are typi-
cally only tens or hundreds of subjects. Hundreds of thousands
of single-nucleotide polymorphisms are potential predictors
in genome-wide association studies. The dimensionality of
the variables spaces grows rapidly when interactions of such
predictors are considered. Large-scale data analysis is also
a common feature of many problems in machine learning,
such as text and document classiÔ¨Åcation and computer vision.
For a p √ó p covariance matrix Œ£, there are p(p + 1)/2
parameters to estimate, yet the sample size n is often small.
In addition, the positive-deÔ¨Åniteness of Œ£ makes the problem
even more complicated. When n > p, the sample covariance
matrix is positive-deÔ¨Ånite and unbiased, but as the dimension
p increases, the sample covariance matrix tends to become
unstable and can fail to be consistent.
II.
BACKGROUND
In this paper, we use the notation in Table I.
TABLE I. NOTATION USED IN THIS PAPER
Notation
Description
A ‚™∞ 0
A ‚àà Rn√óp is symmetric and positive
semideÔ¨Ånite
A ‚âª 0
A ‚àà Rn√óp is symmetric and positive
deÔ¨Ånite
‚à•A‚à•1
‚Ñì1 norm of A ‚àà Rn√óp, i.e P
ij aij
‚à•A‚à•‚àû
‚Ñì‚àû norm of A ‚àà Rn√óp, i.e maxijaij
‚à•A‚à•2
spectral norm of A ‚àà Ri√ój i.e. the max-
mum eigenvalues of A ‚âª 0
‚à•A‚à•F
Frobenius norm of A ‚àà Rn√óp i.e
qP
ij a2
ij
Tr(A)
trace of A ‚àà Rp√óp, i.e Tr(A) = P
i aii
vec(A)
stacked form of A ‚àà Rn√óp, i.e
vec(A) = (a1,1, . . . , an,1, a1,2, . . . , a1,p, . . . , an,p)t,
vec(ABC)
(Ct ‚äó A) vec(B)
vec(A ‚ó¶ B)
diag(vec(A))vec(B)
diag(A)
diag(u) =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
u1
0
¬∑ ¬∑ ¬∑
0
0
u2
¬∑ ¬∑ ¬∑
0
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
uk
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
‚àà RN√óN,
A ‚ó¶ B
Hadamard product of A and B, ‚àà RN√óM, i.e
element-wise multiplication (A ‚ó¶ B)ij = (A)ij √ó (B)ij
A ‚äó B
Kronecker product of A and B
A ‚äó B =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
a1,1B
a1,2B
¬∑ ¬∑ ¬∑
a1,mB
a2,1B
a2,2B
¬∑ ¬∑ ¬∑
a2,mB
...
...
...
...
an,1B
an,2B
¬∑ ¬∑ ¬∑
an,mB
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
A ‚äó B ‚àà Rnp√ómq
a) Existing Methods: Due in part to its importance, there
has been an active line of work on efÔ¨Åcient optimization
methods for solving the ‚Ñì1 regularized Gaussian MLE problem:
PSM that uses projected subgradients [1], ALM using alter-
nating linearization [2], IPM an inexact interior point method
[11], SINCO a greedy coordinate descent method [3] and
Glass a block coordinate descent method [4][5] etc. For typical
high-dimensional statistical problems, optimization methods
typically suffer sub-linear rates of convergence [6]. This would
be too expensive for the Gaussian MLE problem, since the
number of matrix entries scales quadratically with the number
of nodes.
b) Sparse Modeling: Sparse modeling has been widely
used to deal with high dimensionality. The main assumption
is that the p-dimensional parameter vector is sparse, with
1
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

many components being exactly zero or negligibly small.
Such an assumption is crucial in identiÔ¨Åability, especially
for the relatively small sample size. Although the notion of
sparsity gives rise to biased estimation in general, it has proved
to be effective in many applications. In particular, variable
selection can increase the estimation accuracy by effectively
identifying important predictors and can improve the model in-
terpretability. To solve this, constraints are frequently imposed
on the covariance to reduce the number of parameters in the
model including the spectral decomposition, Bayesian meth-
ods, modeling the matrix-logarithm, nonparametric smoothing,
banding/thresholding techniques (see [5], [7][8]).
SpeciÔ¨Åcally,
thresholding
is
proposed
for
estimating
permutation-invariant consistent covariance matrices when the
true covariance matrix is bandable [4]. In this sense, threshold-
ing is more robust than banding/tapering for real applications.
In this paper we focus on the soft-thresholding technique as
in [4] and [9] because it can be formulated as the solution
of a convex optimization problem. In fact, Graphical Lasso
approach (Glasso) is introduced as the following. Let ‚à•.‚à•F be
the Frobenius norm and |.|1 be the element-wise ‚Ñì1-norm of all
non-diagonal elements. Then the soft-thresholding covariance
estimator is equal to
ÀÜ‚Ñ¶+ = arg min
‚Ñ¶
1
2‚à•‚Ñ¶ ‚àí ÀÜ‚Ñ¶n‚à•2
F + Œª|‚Ñ¶|1,
(1)
where ‚Ñ¶ = Œ£‚àí1, where ‚Ñ¶ = œâij1‚â§i,j‚â§p is the precision
matrix, ÀÜ‚Ñ¶ is the solution of (1) and Œª is a tuning parameter.
This equation emphasizes the fact that the solution ‚Ñ¶ may not
be unique [such nonuniqueness can occur if rank(‚Ñ¶) < p].
It has been shown that the existence of a robust optimization
formulation is related to kernel density estimation [10] where
property of the solution and a proof that Lasso is consistent
was given using robustness directly. Moreover, [11] proved
that there exists a linear subspace that is almost surely unique,
meaning that it will be the same under different boundary sets
corresponding to different solutions of equations of type (1).
However, there is no guarantee that the thresholding estimator
is always positive deÔ¨Ånite (see [9], [12] and [13]). Although
the positive deÔ¨Ånite property is guaranteed in the asymptotic
setting with high probability, the actual estimator can be an
indeÔ¨Ånite matrix, especially in real data analysis.
Structure of the inverse covariance matrix has attracted
special interest. Example, when dealing with asset allocation
in Ô¨Ånance. The Ô¨Ånancial problems are often written in terms
of the inverse covariance matrix, in such case the covariance
structure of return series is often estimated using only the
most recent data, resulting in a small sample size compared
to the number of parameters to be estimated. In biological
applications (graphical models), zero correlations represent
conditional independence between the variables. For exam-
ple to account for network information in the analysis of
metabolites data, the reconstruction of metabolic networks
from a collection of metabolite patterns is a key question in
the computational research Ô¨Åeld. Previous attempts focused on
linear metabolite associations measured by Pearson correlation
coefÔ¨Åcients [14]. A major drawback of correlation networks,
however, is their inability to distinguish between direct and
indirect associations. Correlation coefÔ¨Åcients are generally
high in large-scale omics data sets, suggesting a plethora of
indirect and systemic associations. Gaussian Graphical models
(GGMs) circumvent indirect association effects by evaluating
conditional dependencies in multivariate Gaussian distributions
or equivalently the inverse covariance matrix (see [15]).
On the other hand, additional structure on the precision ma-
trix coefÔ¨Åcients is also often required (Comparative genomic
hybridizaton) where the difference between two successive
coefÔ¨Åcients is required to be small or to vary slowly.
Our Contributions: In this paper, we emphasize on intro-
ducing a new criteria that insures the positive-deÔ¨Åniteness of
the covariance matrix adding a tuning parameter œµ > 0 in
the constraints. This additional constraint will guard against
positive semi-deÔ¨Ånite. We add structure on the coefÔ¨Åcient
of the precision matrix and we derive an efÔ¨Åcient ADMM
algorithm form to obtain an optimal solution. We perform
Alternating Direction Method of Multipliers steps, a variant of
the standard Augmented Lagrangian method, that uses partial
updates, but with three innovations that enable Ô¨Ånessing the
caveats detailed above.
In Section III, we link the gaussian graphical model to the
precision matrix estimation, in which we show that recovering
the structure of a graph G is equivalent to the estimation
of the support of the precision matrix and describe different
penalized optimization algorithm that have been used to solve
this problem and their limitations.
We describe, in Section IV, the ADMM and its application
to solve the estimation of the precision matrix under the
Dantzig-selector setting and we show its ability to perform
distributed optimization where we break up the big optimiza-
tion problem into smaller problems that are more manageable.
In fact, as in the recent methods [9][12], we build on the
observation that the Newton direction computation is a Lasso
problem, and perform iterative coordinate descent to solve this
Lasso problem. Then, we use a Dantzig-selector rule to obtain
a step-size that ensures positive-deÔ¨Åniteness of the next iterate.
In Section V, we validate our algorithm on artiÔ¨Åcial and real
data.
III.
LINK WITH GAUSSIAN GRAPHICAL MODEL (GGM)
Given a data set consisting of samples from a zero mean
Gaussian distribution in Rp,
X(i) ‚àº N(0, Œ£), i = 1, ..., n,
(2)
with positive deÔ¨Ånite p√óp covariance matrix Œ£. Note that the
zero mean assumption in equation 2 is mainly for simplifying
notation. The task here is how to estimate the precision matrix
‚Ñ¶ = Œ£‚àí1 when it is sparse. We are particularly interested by
the case of sparse ‚Ñ¶ because it is closely linked to the selection
of graphical models.
To be more speciÔ¨Åc, let G = (V, E) be a graph representing
conditional independence relations between components of
X = (X1, ..., Xp).
‚Ä¢
The vertex set V has p components X1, ..., Xp;
‚Ä¢
The edge set E consists of ordered pairs (i, j) of V √ó
V , where (i, j) ‚àà E if there is an edge between Xi
and Xj.
We exclude the edge between two vertexes Xi and Xj if and
only if Xi and Xj are independent given {Xk, k Ã∏= i, j}.
If in addition X ‚àº N(0, Œ£), the conditional independence
between Xi and Xj given the remaining variables is equivalent
2
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

to œâij = 0, where ‚Ñ¶ = Œ£‚àí1 = {œâij}1‚â§i,j‚â§p. Hence, in the
case of Gaussian Graphical Model (GGM), the edges are given
by the inverse of covariance matrix. More precisely, recovering
the structure of a graph G is equivalent to the estimation of
the support of the precision matrix ‚Ñ¶. When n > p, one
can estimate Œ£‚àí1 by maximum likelihood, but when p > n
this is not possible because the empirical covariance matrix is
singular and often performs poorly [16]. Since the ‚Ñì1-norm
is the tighest convex upper bound of the cardinality of a
matrix, several ‚Ñì1-regularization methods have been proposed.
Consequently a number of authors have considered minimizing
an ‚Ñì1 penalized log-likelihood (see [4] and [17]). It is also
sometimes called Glasso [4] after the algorithm that efÔ¨Åciently
computes the solution. It consists of solving the penalized
problem
ÀÜ‚Ñ¶Glasso = arg min
‚Ñ¶‚™∞0{‚ü®ÀÜŒ£n, ‚Ñ¶‚ü© ‚àí log det ‚Ñ¶ + Œª‚à•‚Ñ¶‚à•1}.
(3)
where ÀÜŒ£n is the sample covariance matrix and Œª is a tuning
parameter. The term ‚à•‚Ñ¶‚à•1 encourages sparseness of the pre-
cision matrix. The asymptotic properties of the estimator has
been studied in [17]. Lauritzen [13] proposed a constrained l1
minimization proposed a constrained l1 minimization proce-
dure for estimating sparse precision matrices by solving the
optimization problem
minimize
‚à•‚Ñ¶‚à•1
subject to
‚à•ÀÜŒ£n‚Ñ¶ ‚àí I‚à•‚àû ‚â§ Œª,
(4)
where Œª is a tuning parameter. The authors established the
rates of convergence under both the entry-wise l‚àû and the
Frobenius norm. In computationally viewpoint, equation 4 can
be solved by remarking that it can be decomposed into a series
of Dantzig selector problems [18]. This observation is useful
in both implementation and technical analysis. Theoretically,
the authors prove that the estimator is positive deÔ¨Ånite with
high probability. However, in practice there is no guarantee
that the estimator is always positive deÔ¨Ånite, especially in real
data analysis.
IV.
ALTERNATING DIRECTION METHOD OF MULTIPLIERS
FOR HADAP
We deÔ¨Åne our algorithm HADAP as a solution to the
following problem
ÀÜ‚Ñ¶+ = arg min
‚Ñ¶‚™∞œµI
1
2‚à•ÀÜŒ£n‚Ñ¶ ‚àí I‚à•2
F + Œª|D ‚ó¶ ‚Ñ¶|1,
(5)
where
ÀÜŒ£n
is
the
empirical
covariance
matrix,
D
=
(dij)1‚â§i,j‚â§p is an arbitrary matrix with non-negative elements
where D can take different forms: it can be the matrix of all
ones or it can be a matrix with zeros on the diagonal to avoid
shrinking diagonal elements of ‚Ñ¶. Furthermore, we can take
D with elements dij = 1iÃ∏=j|ÀÜ‚Ñ¶init
ij |, where ÀÜ‚Ñ¶init is an initial
estimator of ‚Ñ¶. The later choice of D corresponds to precision
matrix analogue of the Adaptive Lasso penalty.
We propose an ADMM algorithm to solve the problem (5).
To derive ADMM, we will Ô¨Årst introduce a new variable Œò
and an equality constraint as follows:
(ÀÜŒò+, ÀÜ‚Ñ¶+)
=
arg min
Œò,‚Ñ¶
1
2‚à•ÀÜŒ£n‚Ñ¶ ‚àí I‚à•2
F + Œª|D ‚ó¶ ‚Ñ¶|1
:
‚Ñ¶ = Œò, Œò ‚™∞ œµI} .
(6)
The solution to (6) gives the solution to (5). To deal with the
problem (6), we have to minimize its augmented Lagrangian
function for some given penalty parameter œÅ, i.e.
LœÅ(Œò, ‚Ñ¶, Œõ)
=
1
2‚à•ÀÜŒ£n‚Ñ¶ ‚àí I‚à•2
F + Œª|D ‚ó¶ ‚Ñ¶|1 ‚àí ‚ü®Œõ, Œò ‚àí ‚Ñ¶‚ü©
+
1
2œÅ‚à•Œò ‚àí ‚Ñ¶‚à•2
F ,
(7)
where Œõ is the Lagrange multiplier.
At iteration k, the ADMM algorithm consists of the three
steps, namely Œò-Step, ‚Ñ¶-Step and the dual-update step :
Œòk+1 := arg min
Œò‚™∞œµI LœÅ(Œò, ‚Ñ¶k, Œõk);
Œò-minimization
(8)
‚Ñ¶k+1 := arg min
‚Ñ¶ LœÅ(Œòk+1, ‚Ñ¶, Œõk);
‚Ñ¶-minimization
(9)
Œõk+1 := Œõk ‚àí 1
œÅ(Œòk+1 ‚àí ‚Ñ¶k+1).
dual-update
(10)
‚Ä¢
In the Ô¨Årst step of the ADMM algorithm, we Ô¨Åx ‚Ñ¶
and Œõ and minimize the augmented Lagrangian over
Œò.
‚Ä¢
In the second step, we Ô¨Åx Œò and Œõ and minimize the
augmented Lagrangian over ‚Ñ¶.
‚Ä¢
Finally, we update the dual variable Œõ.
To further simplify the ADMM algorithm, we will derive the
closed-form solutions for (8)-(10).
A. The Œò-Step.
Let A+ be the projection of a matrix A onto the
convex cone {Œò ‚™∞ œµI}. Assume that A has the eigen-
decomposition Pp
j=1 Œªjvjvt
j. Then, it is well known that
A+ = Pp
j=1 max(œµ, Œªj)vjvt
j. Using this property, the Œò-Step
can be analytically solved as follows
Œòk+1
=
arg min
Œò‚™∞œµI LœÅ(Œò, ‚Ñ¶k, Œõk)
=
arg min
Œò‚™∞œµI
1
2‚à•ÀÜŒ£n‚Ñ¶k ‚àí I‚à•2
F + Œª|D ‚ó¶ ‚Ñ¶k|1
‚àí
‚ü®Œõk, Œò ‚àí ‚Ñ¶k‚ü© + 1
2œÅ‚à•Œò ‚àí ‚Ñ¶k‚à•2
F
=
arg min
Œò‚™∞œµI ‚àí‚ü®Œõk, Œò‚ü© + 1
2œÅ‚à•Œò ‚àí ‚Ñ¶k‚à•2
F
=
arg min
Œò‚™∞œµI ‚à•Œò ‚àí (‚Ñ¶k + œÅŒõk)‚à•2
F
=
(‚Ñ¶k + œÅŒõk)+
=
p
X
j=1
max(œµ, Œªj)vjvt
j,
where Pp
j=1 Œªjvjvt
j is eigen-decomposition of ‚Ñ¶k + œÅŒõk.
3
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

B. The ‚Ñ¶-Step.
‚Ñ¶k+1
=
arg min
‚Ñ¶ LœÅ(Œòk+1, ‚Ñ¶, Œõk)
=
arg min
‚Ñ¶
1
2‚à•ÀÜŒ£n‚Ñ¶ ‚àí I‚à•2
F + Œª|D ‚ó¶ ‚Ñ¶|1
‚àí
‚ü®Œõk, Œòk+1 ‚àí ‚Ñ¶‚ü© + 1
2œÅ‚à•Œòk+1 ‚àí ‚Ñ¶‚à•2
F
=
arg min
‚Ñ¶
1
2‚à•ÀÜŒ£n‚Ñ¶ ‚àí I‚à•2
F + Œª|D ‚ó¶ ‚Ñ¶|1
+
‚ü®Œõk, ‚Ñ¶‚ü© + 1
2œÅ‚à•Œòk+1 ‚àí ‚Ñ¶‚à•2
F
=
arg min
‚Ñ¶
1
2‚à•ÀÜŒ£n‚Ñ¶ ‚àí I‚à•2
F + 1
2œÅ‚à•‚Ñ¶ ‚àí Œòk+1‚à•2
F
+
‚ü®Œõk, ‚Ñ¶‚ü© + Œª|D ‚ó¶ ‚Ñ¶|1.
We have
1
2‚à•ÀÜŒ£n‚Ñ¶ ‚àí I‚à•2
F
=
1
2‚ü®ÀÜŒ£n‚Ñ¶ ‚àí I, ÀÜŒ£n‚Ñ¶ ‚àí I‚ü©
=
1
2
n
‚à•ÀÜŒ£n‚Ñ¶‚à•2
F ‚àí 2‚ü®ÀÜŒ£n‚Ñ¶, I‚ü© + ‚à•I‚à•2
F
o
=
1
2
n
‚à•ÀÜŒ£n‚Ñ¶‚à•2
F ‚àí 2 Tr(‚Ñ¶t ÀÜŒ£t
nI) + Tr(ItI)
o
=
1
2
n
‚à•ÀÜŒ£n‚Ñ¶‚à•2
F ‚àí 2 Tr(‚Ñ¶t ÀÜŒ£n) + p
o
=
1
2
n
‚à•ÀÜŒ£n‚Ñ¶‚à•2
F ‚àí 2‚ü®‚Ñ¶, ÀÜŒ£n‚ü© + p
o
.
and
1
2œÅ‚à•‚Ñ¶ ‚àí Œòk+1‚à•2
F
=
1
2œÅ‚ü®‚Ñ¶ ‚àí Œòk+1, ‚Ñ¶ ‚àí Œòk+1‚ü©
=
1
2œÅ

‚à•‚Ñ¶‚à•2
F ‚àí 2‚ü®‚Ñ¶, Œòk+1‚ü© + ‚à•Œòk+1‚à•2
F
	
.
Then, the ‚Ñ¶-Step is equivalent to
‚Ñ¶k+1
=
arg min
‚Ñ¶
1
2‚à•ÀÜŒ£n‚Ñ¶ ‚àí I‚à•2
F + 1
2œÅ‚à•‚Ñ¶ ‚àí Œòk+1‚à•2
F
+
‚ü®Œõk, ‚Ñ¶‚ü© + Œª|D ‚ó¶ ‚Ñ¶|1
=
arg min
‚Ñ¶
1
2
n
‚à•ÀÜŒ£n‚Ñ¶‚à•2
F ‚àí 2‚ü®‚Ñ¶, ÀÜŒ£n‚ü© + p
o
+
1
2œÅ

‚à•‚Ñ¶‚à•2
F ‚àí 2‚ü®‚Ñ¶, Œòk+1‚ü© + ‚à•Œòk+1‚à•2
F
	
+ ‚ü®Œõk, ‚Ñ¶‚ü©
+
Œª|D ‚ó¶ ‚Ñ¶|1
=
arg min
‚Ñ¶
1
2
n
‚à•ÀÜŒ£n‚Ñ¶‚à•2
F ‚àí 2‚ü®‚Ñ¶, ÀÜŒ£n‚ü©
o
+
1
2œÅ

‚à•‚Ñ¶‚à•2
F ‚àí 2‚ü®‚Ñ¶, Œòk+1‚ü©
	
+
‚ü®Œõk, ‚Ñ¶‚ü© + Œª|D ‚ó¶ ‚Ñ¶|1
=
arg min
‚Ñ¶
1
2

‚à•ÀÜŒ£n‚Ñ¶‚à•2
F + 1
œÅ‚à•‚Ñ¶‚à•2
F
‚àí
2‚ü®‚Ñ¶, ÀÜŒ£n + 1
œÅŒòk+1 ‚àí Œõk‚ü©

+ Œª|D ‚ó¶ ‚Ñ¶|1
=
arg min
‚Ñ¶
1
2

‚à•ÀÜŒ£n‚Ñ¶‚à•2
F + 1
œÅ‚à•‚Ñ¶‚à•2
F
‚àí
2
œÅ‚ü®‚Ñ¶, œÅ(ÀÜŒ£n ‚àí Œõk) + Œòk+1‚ü©

+ Œª|D ‚ó¶ ‚Ñ¶|1.
At this level, we are not able to derive a closed form for ‚Ñ¶. To
overcome this problem, we propose to derive a new ADMM to
update ‚Ñ¶. To do this, we reparametrize the D ‚ó¶ ‚Ñ¶ with Œì and
we add an equality constraint D ‚ó¶ ‚Ñ¶ = Œì, then we minimize
1
2

‚à•ÀÜŒ£n‚Ñ¶‚à•2
F + 1
œÅ‚à•‚Ñ¶‚à•2
F ‚àí 2
œÅ‚ü®‚Ñ¶, œÅ(ÀÜŒ£n ‚àí Œõk) + Œòk+1‚ü©

+ Œª|Œì|1
subject to
D ‚ó¶ ‚Ñ¶ = Œì.
(11)
The
augmented
Lagrangian
associated
to
this
problem
Lk
œÅ(‚Ñ¶, Œì, ‚àÜ) is
1
2

‚à•ÀÜŒ£n‚Ñ¶‚à•2
F + 1
œÅ‚à•‚Ñ¶‚à•2
F ‚àí 2
œÅ‚ü®‚Ñ¶, œÅ(ÀÜŒ£n ‚àí Œõk) + Œòk+1‚ü©

+
Œª|Œì|1 ‚àí ‚ü®‚àÜ, Œì ‚àí D ‚ó¶ ‚Ñ¶‚ü© + 1
2œÅ‚à•Œì ‚àí D ‚ó¶ ‚Ñ¶‚à•2
F ,
(12)
where ‚àÜ is the Lagrange multiplier and œÅ is the same param-
eter as in (7).
As before, the ADMM for this problem consists of the follow-
ing three intermediate steps:
‚Ñ¶j+1
k
:= arg min
‚Ñ¶ Lk
œÅ(‚Ñ¶, Œìj, ‚àÜj);
‚Ñ¶-minimization
(13)
Œìj+1 := arg min
Œì Lk
œÅ(‚Ñ¶j+1
k
, Œì, ‚àÜj);
Œì-minimization
(14)
‚àÜj+1 := ‚àÜj ‚àí 1
œÅ(‚Ñ¶j+1
k
‚àí Œìj+1).
dual-update
(15)
C. The intermediate ‚Ñ¶-Step.
‚Ñ¶j+1
k
=
arg min
‚Ñ¶ Lk
œÅ(‚Ñ¶, Œìj, ‚àÜj)
=
arg min
‚Ñ¶
1
2

‚à•ÀÜŒ£n‚Ñ¶‚à•2
F + 1
œÅ‚à•‚Ñ¶‚à•2
F
‚àí
2
œÅ‚ü®‚Ñ¶, œÅ(ÀÜŒ£n ‚àí Œõk) + Œòk+1‚ü©

+ Œª|Œìj|1
‚àí
‚ü®‚àÜj, Œìj ‚àí D ‚ó¶ ‚Ñ¶‚ü© + 1
2œÅ‚à•Œìj ‚àí D ‚ó¶ ‚Ñ¶‚à•2
F
=
1
2

‚à•ÀÜŒ£n‚Ñ¶‚à•2
F + 1
œÅ‚à•‚Ñ¶‚à•2
F ‚àí 2
œÅ‚ü®‚Ñ¶, œÅ(ÀÜŒ£n ‚àí Œõk)
+
Œòk+1‚ü©
	
+ ‚ü®‚àÜj, D ‚ó¶ ‚Ñ¶‚ü© + 1
2œÅ‚à•Œìj ‚àí D ‚ó¶ ‚Ñ¶‚à•2
F
= 1
2 Tr

‚Ñ¶t ÀÜŒ£t
n ÀÜŒ£n‚Ñ¶ + 1
œÅ‚Ñ¶t‚Ñ¶ ‚àí 2
œÅ‚Ñ¶t(œÅ(ÀÜŒ£n ‚àí Œõk) + Œòk+1)

+ Tr((‚àÜj)tD ‚ó¶ ‚Ñ¶) + 1
2œÅ Tr((Œìj ‚àí D ‚ó¶ ‚Ñ¶)t(Œìj ‚àí D ‚ó¶ ‚Ñ¶)).
Then, the partial differential of Lk
œÅ with respect to ‚Ñ¶,
‚àÇLk
œÅ(‚Ñ¶,Œìj,‚àÜj)
‚àÇ‚Ñ¶
is
= 1
2

2ÀÜŒ£t
n ÀÜŒ£n‚Ñ¶ + 2
œÅ‚Ñ¶ ‚àí 2
œÅ(œÅ(ÀÜŒ£n ‚àí Œõk) + Œòk+1)

+‚àÜj ‚ó¶ D + 1
2œÅ

‚àí2Œìj ‚ó¶ D + 2D ‚ó¶ D ‚ó¶ ‚Ñ¶
	
=

ÀÜŒ£t
n ÀÜŒ£n + 1
œÅI

‚Ñ¶ + 1
œÅD ‚ó¶ D ‚ó¶ ‚Ñ¶
4
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

+

‚àÜj ‚àí 1
œÅŒìj

‚ó¶ D ‚àí 1
œÅ

œÅ(ÀÜŒ£n ‚àí Œõk) + Œòk+1
.
Since ‚Ñ¶j+1
k
is the minimizer of Lk
œÅ(., Œìj, ‚àÜj), we must have
‚àÇLk
œÅ(‚Ñ¶j+1
k
, Œìj, ‚àÜj)
‚àÇ‚Ñ¶
=
0,
which is equivalent to

ÀÜŒ£t
n ÀÜŒ£n + 1
œÅI

‚Ñ¶j+1
k
+ 1
œÅD ‚ó¶ D ‚ó¶ ‚Ñ¶j+1
k
+

‚àÜj ‚àí 1
œÅŒìj

‚ó¶ D ‚àí 1
œÅ

œÅ(ÀÜŒ£n ‚àí Œõk) + Œòk+1
= 0.
Finally, ‚Ñ¶j+1
k
has a closed form given by the previous expres-
sion despite the additional but straightforward computational
effort at this level. This additional step, can be considered as a
warming start for the original ADMM algorithm. This is very
important when dealing with complex problem and large data.
D. The intermediate Œì-Step.
To
deal
with
this
Step,
deÔ¨Åne
an
entry-wise
soft-
thresholding rule for all the off-diagonal elements of a matrix
A as S(A, Œ∫) = {s(ajl, Œ∫)}1‚â§j,l‚â§p with
s(ajl, Œ∫) = sign(ajl) max(|ajl| ‚àí Œ∫, 0)I{jÃ∏=l}.
Then the Œì-Step has a closed form given by
Œìj+1
=
arg min
Œì Lk
œÅ(‚Ñ¶j+1
k
, Œì, ‚àÜj)
=
arg min
Œì
1
2

‚à•ÀÜŒ£n‚Ñ¶j+1
k
‚à•2
F + 1
œÅ‚à•‚Ñ¶j+1
k
‚à•2
F
‚àí
2
œÅ‚ü®‚Ñ¶j+1
k
, œÅ(ÀÜŒ£n ‚àí Œõk) + Œòk+1‚ü©

+
Œª|Œì|1 ‚àí ‚ü®‚àÜj, Œì ‚àí D ‚ó¶ ‚Ñ¶j+1
k
‚ü© + 1
2œÅ‚à•Œì ‚àí D ‚ó¶ ‚Ñ¶j+1
k
‚à•2
F
=
arg min
Œì
1
2‚à•Œì ‚àí D ‚ó¶ ‚Ñ¶j+1
k
‚à•2
F ‚àí œÅ‚ü®‚àÜj, Œì‚ü© + œÅŒª|Œì|1
=
arg min
Œì
1
2
n
‚à•Œì‚à•2
F ‚àí 2‚ü®œÅ‚àÜj + D ‚ó¶ ‚Ñ¶j+1
k
, Œì‚ü©
o
+
œÅŒª|Œì|1
=
arg min
Œì
1
2‚à•Œì ‚àí (œÅ‚àÜj + D ‚ó¶ ‚Ñ¶j+1
k
)‚à•2
F + œÅŒª|Œì|1
=
S(œÅ‚àÜj + D ‚ó¶ ‚Ñ¶j+1
k
, œÅŒª).
Algorithm 1 shows complete details of HADAP using ADMM
(see Figure 1).
V.
EXPERIMENTS
Among existing methods, the lasso penalized Gaussian
likelihood estimator is the only popular matrix precision es-
timator that can simultaneously retain sparsity and positive
deÔ¨Åniteness. To show the goodness of our approach, we use
simulations and real example to compare the performance of
our estimator with Glasso.
Algorithm 1: HADAP algorithm
initialize the variables: Œò0 = 0, ‚Ñ¶0 = 0, Œõ0 = 0,
Œì0 = 0, ‚àÜ0 = 0 ;
Select a scalar œÅ > 0;
for k = 0, 1, 2, ... until convergence do
Œòk+1 := (‚Ñ¶k + œÅŒõk)+;
for j = 0, 1, 2, ... until convergence do
Aj+1
Œ£
‚Üê

œÅÀÜŒ£t
n ÀÜŒ£n + DtD + I
‚àí1
;
Bj+1
Œ£
‚Üê

(1 ‚àí œÅ)Dt‚àÜj + œÅ(ÀÜŒ£n ‚àí Œõk) + Œòk+1
;
‚Ñ¶j+1
k
:= AŒ£ √ó BŒ£ ;
Œìj+1 := S(œÅ‚àÜj + D‚Ñ¶j+1
k
, œÅŒª);
‚àÜj+1 := ‚àÜj ‚àí 1
œÅ(‚Ñ¶j+1
k
‚àí Œìj+1);
end
‚Ñ¶k+1 := limj‚Üí+‚àû ‚Ñ¶j
k;
Œõk+1 := Œõk ‚àí 1
œÅ(Œòk+1 ‚àí ‚Ñ¶k+1);
end
Figure 1. Complete details of HADAP using the Alternating Direction
Method of Multipliers.
A. Validation on synthetic data
In order to validate our approach, we used the same
simulation structure as in [13]. We generated n = 1000
samples from a p = 600-dimensional normal distribution with
correlation structure of the form œÉ(xi, xj) = 0.6|i ‚àí j|. This
model has a banded structure, and the values of the entries
decay exponentially as they move away from the diagonal.
We generated an independent sample of size 1000 from the
same distribution for validating the tuning parameter Œª. Using
the training data, we compute a series of estimators with
50 different values of Œª and use the one with the smallest
likelihood loss on the validation sample, where the likelihood
loss [19], is deÔ¨Åned by
L(Œ£, ‚Ñ¶) = ‚ü®Œ£, ‚Ñ¶‚ü© ‚àí log det(‚Ñ¶)
(16)
We mention that all the experiments are conducted on a PC
with 4 Gb RAM, 3Ghz CPU using Matlab 2009a.
B. Measurable quantities and results.
Output displays the primal residual |rk|, the primal feasi-
bility tolerance epsilon œµpri, the dual residual sk, and the dual
feasibility tolerance œµdual quantities. Also included is a plot of
the objective values by iterations. Note that the objective value
at any particular iteration can go below the true solution value
p‚àó because the iterates does not need to be feasible (e.g., if
the constraint is x‚àíz = 0, we can have xk ‚àízk Ã∏= 0 for some
k).
Results for Œª = 0.01 are summarized in Figure 2 and Table
II. The convergence is achieved in 25 steps and need just 0.54
seconds. After a few steps of Ô¨Çuctuations (‚âà 12 iterations),
the objective function stabilizes and converges to its optimal
value where the eigenvalues of the precision matrix estimated
by the HADAP are real and positive, which prove the positive
deÔ¨Åniteness of the obtained precision matrix as shown in
Figure 3.
5
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

Figure 2. Plot of the objective function for Œª = 0.01
TABLE II. VALUES OF THE PRIMAL RESIDUAL |rk|, THE PRIMAL
FEASIBILITY TOLERANCE œµpri, THE DUAL RESIDUAL sk, AND
THE DUAL FEASIBILITY TOLERANCE œµdual for Œª = 0.01. ALSO
INCLUDED THE OBJECTIVE VALUES BY ITERATION.
iter
r.norm
eps.pri
s.norm
eps.dual
objective
1
2.76
0.04
2.86
0.04
8.49
2
1.08
0.07
1.86
0.03
8.11
3
0.88
0.08
1.99
0.02
8.89
4
1.34
0.09
3.37
0.03
7.95
5
1.88
0.10
3.77
0.03
8.64
6
1.42
0.11
2.52
0.02
8.94
7
0.94
0.11
1.28
0.02
7.79
8
0.62
0.11
0.81
0.01
9.70
9
0.41
0.11
0.58
0.01
7.70
10
0.28
0.11
0.42
0.01
6.70
11
0.16
0.11
0.25
0.01
6.71
12
0.09
0.11
0.16
0.01
6.73
13
0.05
0.11
0.11
0.01
6.75
14
0.03
0.11
0.08
0.01
6.77
15
0.02
0.11
0.07
0.01
6.79
16
0.01
0.12
0.06
0.01
6.80
17
0.01
0.12
0.06
0.01
6.82
18
0.01
0.12
0.04
0.01
6.83
19
0.01
0.12
0.04
0.01
6.84
20
0.01
0.12
0.03
0.01
6.85
21
0.01
0.12
0.03
0.01
6.86
22
0.01
0.12
0.02
0.01
6.86
23
0.01
0.12
0.02
0.01
6.87
24
0.01
0.12
0.02
0.01
6.88
25
0.01
0.12
0.02
0.01
6.88
C. Validation on real data
For experimental validation, we used 4 cancer datasets
publicly available at the Gene Expression Omnibus [20]. For
a fair comparison with the other method of estimating the
inverse covariance matrix, we follow the same analysis scheme
used by [19]. Datasets are: Liver cancer (GSE1898), Colon
cancer (GSE29638), Breast cancer (GSE20194) and Prostate
cancer (GSE17951) with sample size n = 182; 50; 278 and 154
respectively and number of genes p = 21794; 22011; 22283
and 54675. We preprocessed the data so that each variable is
zero mean and unit variance across the dataset. We performed
100 repetitions on a 50%‚àí50% validation and testing samples.
Since regular sparseness promoting methods do not scale to
large number of variables, we used the same regime proposed
by [19] and validated our method in two regimes. In the Ô¨Årst
Figure 3. Plot of the objective function
regime, for each of the 50 repetitions, we selected n = 200
variables uniformly at random and use the glasso. In the
second regime, we use all the variables in the dataset, and
use the method dpglasso from [21]. Since the whole sample
covariance matrix could not Ô¨Åt in memory, we computed it
in batches of rows [21]. In order to make a fair comparison,
the runtime includes the time needed to produce the optimal
precision matrix from a given input dataset. Average runtimes
was summarized in table III. This includes not only the time to
solve each optimization problem but also the time to compute
the covariance matrix (if needed). Our HADAP method is
considerably faster than the Glasso method as shown in table
III .
TABLE III. RUNTIMES FOR GENE EXPRESSION DATASETS. OUR
HADAP METHOD IS CONSIDERABLY FASTER THAN SPARSE
METHOD.
Dataset
Graphical lasso
Our estimator
GSE1898
3.8 min
1.0 min
GSE29638
3.8 min
2.6 min
GSE20194
3.8 min
2.5 min
GSE17951
14.9min
4.8 min
VI.
CONCLUSION AND FUTURE WORK
The sparse precision matrix estimator has been shown
to be useful in many applications. Penalizing the matrix is
a tool with good asymptotic properties for estimating large
sparse covariance and precision matrices. However, its positive
deÔ¨Åniteness property and unconstrained structure can be easily
violated in practice, which prevents its use in many important
applications such as graphical models, Ô¨Ånancial assets and
comparative genomic hybridization. In this paper, we have
expressed the precision matrix estimation equation in a convex
optimization framework and considered a natural modiÔ¨Åcation
by imposing the positive deÔ¨Åniteness and problem-solving
constraints. We have developed a fast alternating direction
method to solve the constrained optimization problem and the
resulting estimator retains the sparsity and positive deÔ¨Åniteness
properties simultaneously. We are at the phase of demonstrat-
ing the general validity of the method and its advantages over
6
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

correlation networks based on competitive precision matrix
estimators with computer-simulated reaction systems, to be
able to demonstrate strong signatures of intracellular pathways
and provide a valuable tool for the unbiased reconstruction of
metabolic reactions from large-scale metabolomics data sets.
REFERENCES
[1]
J. Duchi, S. Gould, and D. Koller, ‚ÄúProjected Subgradient Methods
for Learning Sparse Gaussians,‚Äù in Proceedings of the Twenty-fourth
Conference on Uncertainty in AI (UAI), 2008.
[2]
K. Scheinberg, S. Ma, and D. Goldfarb, ‚ÄúSparse inverse covariance
selection via alternating linearization methods,‚Äù in Advances in Neural
Information Processing, NIPS10, 2010.
[3]
L. Li and K. Toh, ‚ÄúAn inexact interior point method for l1-reguarlized
sparse covariance selection,‚Äù Mathematical Programming Computation,
vol. 2, 2010, pp. 291‚Äì315.
[4]
J. Friedman, T. Hastie, and R. Tibshirani, ‚ÄúSparse Inverse Covariance
Estimation With the Graphical Lasso,‚Äù Biostatistics, vol. 9, 2008, pp.
432‚Äì441.
[5]
O. Banerjee, L. El Ghaoui, and A. d‚ÄôAspremont, ‚ÄúModel selection
through sparse maximum likelihood estimation for multivariate Gaus-
sian or binary data,‚Äù Journal of Machine Learning Research, vol. 9,
2008, pp. 485‚Äì516.
[6]
A. Agarwal, S. Negahban, and M. Wainwright, ‚ÄúConvergence rates of
gradient methods for high-dimensional statistical recovery,‚Äù in Advances
in Neural Information Processing, NIPS10, 2010.
[7]
P. Bickel and E. Levina, ‚ÄúRegularized estimation of large covariance
matrices,‚Äù Ann. Statist., vol. 36, 2008a, pp. 199‚Äì227.
[8]
R. Yang and J. Berger, ‚ÄúEstimation of a covariance matrix using the
reference prior,‚Äù Ann. Statist., vol. 3, 1994, pp. 1195‚Äì1211.
[9]
X. Lingzhou, S. Ma, and H. Zhou, ‚ÄúPositive DeÔ¨Ånite L1 Penalized
Estimation of Large Covariance Matrices,‚Äù Journal of the American
Statistical Association, vol. 500, 2012, pp. 1480‚Äì1491.
[10]
H. Xu, C. Caramanis, and S. Mannor, ‚ÄúRobust Regression and Lasso,‚Äù
IEEE Transaction on Information Theory, vol. 56, 2010, pp. 3561‚Äì357.
[11]
R. Tibshirani and J. Taylor, ‚ÄúThe solution path of the generalized lasso,‚Äù
Ann. Statist, vol. 39 (3), 2011, pp. 1335‚Äì1371.
[12]
N. El Karoui, ‚ÄúOperator norm consistent estimation of large dimensional
sparse covariance matrices,‚Äù Annals of Statistics, vol. 36, 2008, pp.
2717‚Äì2756.
[13]
T. Cai and H. Zhou, ‚ÄúA constrained l1 minimization approach to sparse
precision matrix estimation,‚Äù J. American Statistical Association, vol.
106, 2011, pp. 594‚Äì607.
[14]
J. Krumsiek, K. Suhre, T. Illig, J. Adamski, and F. Theis, ‚ÄúGaus-
sian graphical modeling reconstructs pathway reactions from high-
throughput metabolomics data,‚Äù BMC Systems Biology, vol. 5 (21),
2011, pp. 2‚Äì16.
[15]
S. Lauritzen, Graphical Models.
Clarendon Press, Oxford, 1996.
[16]
I. Johnstone, ‚ÄúOn the distribution of the largest eigenvalue in principal
components analysis,‚Äù Ann. Statist., vol. 29 (2), 2001, pp. 295‚Äì327.
[17]
P. Ravikumar, W. M.J., G. Raskutti, and B. Yu, ‚ÄúHigh-dimensional
covariance estimation by minimizing l1-penalized log-determinant di-
vergence,‚Äù Electron. J. Statist., vol. 5, 2011, pp. 935‚Äì980.
[18]
E. Candes and T. Tao, ‚ÄúThe Dantzig selector: statistical estimation when
p is much larger than n,‚Äù Annals of Statistics, vol. 35, 2007, pp. 2313‚Äì
2351.
[19]
J. Honorio and T. Jaakkola, ‚ÄúInverse covariance estimation for high-
dimensional data in linear time and space: Spectral methods for riccati
and sparse models,‚Äù in Proceedings of the 29th Conference on Uncer-
tainty in ArtiÔ¨Åcial Intelligence, 2013.
[20]
R. Edgar, M. Domrachev, and A. Lash, ‚ÄúGene Expression Omnibus:
NCBI gene expression and hybridization array data repository,‚Äù Nucleic
Acids Res, vol. 30 (1), 2002, pp. 207‚Äì210.
[21]
R. Mazumder and T. Hastie, ‚ÄúExact covariance thresholding into
connected components for largescale graphical lasso,‚Äù The Journal of
Machine Learning Research, vol. 13, 2012, pp. 781‚Äì794.
7
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

A Specialized Recursive Language
for Capturing Time-Space Complexity Classes
Emanuele Covino and Giovanni Pani
Dipartimento di Informatica
Universit√† di Bari, Italy
Email: emanuele.covino@uniba.it, giovanni.pani@uniba.it
Abstract‚ÄîWe provide a resource-free characterization of reg-
ister machines that computes their output within polynomial
time O(nk), by deÔ¨Åning our version of predicative recursion
and a related recursive programming language. Then, by
means of some restriction on composition of programs, we
deÔ¨Åne a programming language that characterize the register
machines with a polynomial bound imposed over time and
space complexity, simultaneously. A simple syntactical analysis
allows us to evaluate the complexity of a program written in
these languages.
Keywords‚ÄìSpecialized
computation
languages,
time-space
classes, implicit computational complexity, predicative recursion.
I.
INTRODUCTION
The deÔ¨Ånition of a complexity class is usually made by
imposing an explicit bound on time and/or space resources
used by a Turing Machine (or another equivalent model)
during its computation. On the other hand, different ap-
proaches use logic and formal methods to provide languages
for complexity-bounded computation; they aim at studying
computational complexity without referring to external mea-
suring conditions or a particular machine model, but only by
considering language restrictions or logical/computational
principles implying complexity properties. In particular, this
is achieved by characterizing complexity classes by means
of recursive operators with explicit syntactical restrictions
on the role of variables.
The Ô¨Årst characterization of this type was given by
Cobham [4], in which the polynomial-time computable
functions are exactly those functions generated by bounded
recursion on notation; Leivan [8] and Bellantoni and Cook
[1] gave other characterizations of PTIMEF. Several other
complexity classes have been characterized by means of
unlimited operators: see, for instance, Leivant and Marion
[9] and Oitavem [10] for PSPACEF and the class of the
elementary functions; Clote [3] for the deÔ¨Ånition of a
time/space hierarchy between PTIMEF and PSPACEF; Leivant
[6], [7] and [8] for a theoretical insight. All these approaches
have been dubbed Implicit Computational Complexity: they
share the idea that no explicitly bounded schemes are needed
to characterize a great number of classes of functions and
that, in order to do this, it sufÔ¨Åces to distinguish between safe
and unsafe variables (or, following Simmons [11], between
dormant and normal ones) in the recursion schemes. This
distinction yields many forms of predicative recursion, in
which the being-deÔ¨Åned function cannot be used as counter
into the deÔ¨Åning one. The two main objectives of this
area are to Ô¨Ånd natural implicit characterizations of various
complexity classes of functions, thereby illuminating their
nature and importance, and to design methods suitable for
static veriÔ¨Åcation of program complexity. This approach
represent a bridge between the complexity theory and the
programming language theory; a mere syntactical inspection
allows us to evaluate the complexity of a given program writ-
ten in one of the previous mentioned specialized languages.
Our version of the safe recursion scheme on a binary
word algebra is such that f(x, y, za) = h(f(x, y, z), y, za);
throughout this paper we will call x, y and z the auxiliary
variable, the parameter, and the principal variable of a
program deÔ¨Åned by recursion, respectively. We don‚Äôt allow
the renaming of variable z as x, and this implies that the step
program h cannot assign the previous value of the being-
deÔ¨Åned program f to the principal variable z: in other words,
we always know in advance the number of recursive calls
of the step program in a recursive deÔ¨Ånition. We obtain that
z is a dormant variable, according to Simmons‚Äô approach,
or a safe one, following Bellantoni and Cook.
In Section II, starting from a natural deÔ¨Ånition of con-
structors and destructors over an algebra of lists, we give
our deÔ¨Ånition of recursion-free programs and of the safe
recursion scheme. In section III, we recall the deÔ¨Ånition
of computation by register machines as provided by [8]. In
section IV, we deÔ¨Åne the hierarchy of classes of programs
Tk, with k ‚àà N, where programs in T1 can be computed by
register machines within linear time, and Tk+1 are programs
obtained by one application of safe recursion to elements in
Tk; we prove that they are computable within time O(nk).
We then restrict Tk to the hierarchy Sk, whose elements
are the programs computable by a register machine in
linear space. By means of a restricted form of composition
between programs, we deÔ¨Åne, in Section V, a polytime-
space hierarchy T Sqp, such that each program in T Sqp
can be computed by a register machine within time O(np)
and space O(nq), simultaneously. We have that S
k<œâ Tk
captures PTIME. Even though this is a well-known result
(see [8]), we use it to prove the second one (see also [5]
and [9] for other approaches to the characterization of joint
time-space classes). Both results are a preliminary step for an
implicit classiÔ¨Åcation of the hierarchy of time-space classes
between PTIME and PSPACE, as deÔ¨Åned in [3]. In Section VI
we summarize the results and give some hints about future
work.
8
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

II.
BASIC INSTRUCTIONS AND DEFINITION SCHEMES
In this section we introduce the basic operators of our
programming language (for a different approach see [2]).
The language is deÔ¨Åned over a binary word algebra, with
the only restriction that words are packed into lists, with the
symbol ¬© acting as a separator between words. In this way,
we are able to handle a sequence of words as a single object.
A. Recursion-free programs and classes T0 and S0
B is the binary alphabet {0, 1}. a, b, a1, . . . denotes
elements
of
B,
and
U, V, . . . , Y
denotes
words
over
B.
p, q, . . . , s, . . .
denotes
lists
in
the
form
Y1¬©Y2¬© . . . ¬©Yn‚àí1¬©Yn. œµ is the empty word. The i-
th component (s)i of s = Y10Y20 . . . 0Yn‚àí10Yn is Yi. |s|
is the length of the list s, that is the number of letters
occurring in s. We write x, y, z for the variables used in a
program, and we write u for one among x, y, z. Programs
will be denoted with f, g, h, and they will have the form
f(x, y, z), where some among the variables may be absent.
DeÔ¨Ånition 2.1: The basic instructions are:
1)
the identity I(u), that returns the value s assigned
to u;
2)
the constructors Ca
i (s), that adds the digit a at the
right of the last digit of (s)i, with a = 0, 1 and
i ‚â• 1;
3)
the destructors Di(s), that erases the rightmost digit
of (s)i, with a = 0, 1 and i ‚â• 1.
Constructors Ca
i (s) and destructors Di(s) leave s unchanged
if it has less than i components.
Example 2.1: Given the word s = 01¬©11¬©¬©00, we
have that |s| = 9 and (s)2 = 11. We also have C1
1(01¬©11) =
011¬©11, D2(0¬©0¬©) = 0¬©¬©, D2(0¬©¬©) = 0¬©¬©.
DeÔ¨Ånition 2.2: Given the programs g and h, f is deÔ¨Åned
by simple schemes if it is obtained by:
1)
renaming of x as y in g, that is, f is the result of
the substitution of the value of y to all occurrences
of x into g. Notation: f =RNMx/y(g);
2)
renaming of z as y in g, that is, f is the result of
the substitution of the value of y to all occurrences
of z into g. Notation: f =RNMz/y(g);
3)
selection in g and h, when for all s, t, r we have
f(s, t, r) =
( g(s, t, r)
if the rightmost digit
of (s)i is b
h(s, t, r)
otherwise,
with i ‚â• 1 and b = 0, 1. Notation: f =SELb
i(g, h).
Example 2.2: if f is deÔ¨Åned by RNMx/y(g) we have
that f(t, r) = g(t, t, r). Similarly, f deÔ¨Åned by RNMz/y(g)
implies that f(s, t) = g(s, t, t). Let s be the word 00¬©1010,
and f =SEL0
2(g, h); we have that f(s, t, r) = g(s, t, r), since
the rightmost digit of (s)2 is 0.
DeÔ¨Ånition 2.3: f is obtained by safe composition of h
and g in the variable u if it is obtained by substitution of h
to u in g; if u = z, then x must be absent in h. Notation:
f =SCMPu(h, g).
DeÔ¨Ånition 2.4: A modiÔ¨Åer is obtained by the safe com-
position of a sequence of constructors and a sequence of
destructors.
DeÔ¨Ånition 2.5: T0 is the class of programs deÔ¨Åned by
closure of modiÔ¨Åers under SEL and SCMP.
DeÔ¨Ånition 2.6: Given f ‚àà T0, the rate of growth rog(f)
is such that
1)
if f is a modiÔ¨Åer, rog(f) is the difference between
the number of constructors and the number of
destructors occurring in its deÔ¨Ånition;
2)
if
f=SELb
i(g, h),
then
rog(f)
is
max(rog(g), rog(h));
3)
if
f=SCMPu(h, g),
then
rog(f)
is
max(rog(g), rog(h)).
DeÔ¨Ånition 2.7: S0 is the class of programs in T0 with
non-positive rate of growth, that is S0 = {f ‚àà T0|rog(f) ‚â§
0}.
Note that all elements in T0 and in S0 modify their inputs
according to the result of some test performed over a Ô¨Åxed
number of digits. Moreover, elements in S0 cannot return
values longer than their input.
B. Safe recursion and classes T1 and S1
DeÔ¨Ånition 2.8: Given
the
programs
g(x, y)
and
h(x, y, z), f(x, y, z) is deÔ¨Åned by safe recursion in the
basis g and in the step h if for all s, t, r we have

f(s, t, a)
=
g(s, t)
f(s, t, ra)
=
h(f(s, t, r), t, ra).
Notation: f =SREC(g, h).
In particular, f(x, z) is deÔ¨Åned by iteration of h(x) if for
all s, r we have

f(s, a)
=
s
f(s, ra)
=
h(f(s, r)).
Notation: f =ITER(h). We write h|r|(s) for ITER(h)(s, r)
(i.e., the |r|-th iteration of h on s).
DeÔ¨Ånition 2.9: T1 (respectively, S1) is the class deÔ¨Åned
by closure under simple schemes and SCMP of programs in
T0 (resp., S0) and programs obtained by one application of
ITER to T0 (resp., S0).
Notation: T1=(T0, ITER(T0); SCMP, SIMPLE)
(resp., S1=(S0, ITER(S0); SCMP, SIMPLE)).
As we have already stated in the Introduction, we call
x, y and z the auxiliary variable, the parameter, and the
principal variable of a program obtained by means of the
previous recursion scheme. The renaming of z as x is not
allowed (see deÔ¨Ånitions 2.2 and 2.3), implying that the step
program of a recursive deÔ¨Ånition cannot assign the recursive
9
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

call to the principal variable. This is the key of the time-
complexity bound intrinsic into our programs, together with
the limitations imposed to the renaming of variables.
DeÔ¨Ånition 2.10:
1)
Given f ‚àà T1, the number of
components of f is max{i|Di or Ca
i or SELb
i occurs
in f}. Notation: #(f).
2)
Given a program f, its length is the number of
constructors, destructors and deÔ¨Åning schemes oc-
curring in its deÔ¨Ånition. Notation: lh(f).
III.
COMPUTATION BY REGISTER MACHINES
In this section, we recall the deÔ¨Ånition of register machine
(see [8]), and we give the deÔ¨Ånition of computation within
a given time (or space) bound.
DeÔ¨Ånition 3.1: Given a free algebra A generated from
constructors c1, . . . , cn (with arity(ci) = ri), a register
machine over A is a computational device M having the
following components:
1)
a Ô¨Ånite set of states S = {s0, . . . , sn};
2)
a Ô¨Ånite set of registers Œ¶ = {œÄ0, . . . , œÄm};
3)
a collection of commands, where a command may
be:
a branching siœÄjsi1 . . . sik, such that when M is in
the state si, switches to state si1, . . . , sik according
to whether the main constructor (i.e., the leftmost)
of the term stored in register œÄj is c1, . . . , ck;
a constructor siœÄj1 . . . œÄjri ciœÄlsr, such that when
M is in the state si, store in œÄl the result of the
application of the constructor ci to the values stored
in œÄj1 . . . œÄjri , and switches to sr;
a p-destructor siœÄjœÄlsr (p ‚â§ max(ri)i=1...k),
such that when M is in the state si, store in œÄl
the p-th subterm of the term in œÄj, if it exists;
otherwise, store the term in œÄj. Then it switched
to sr.
A conÔ¨Åguration of M is a pair (s, F), where s ‚àà S and
F
: Œ¶ ‚Üí A. M induces a transition relation ‚ä¢M on
conÔ¨Ågurations, where Œ∫ ‚ä¢M Œ∫‚Ä≤ holds if there is a command
of M whose execution converts the conÔ¨Åguration Œ∫ in Œ∫‚Ä≤. A
computation of M on input ‚ÉóX = X1, . . . , Xp with output
‚ÉóY = Y1, . . . , Yq is a sequence of conÔ¨Ågurations, starting
with (s0, F0), and ending with (s1, F1) such that:
1)
F0(œÄj‚Ä≤(i)) = Xi, for 1 ‚â§ i ‚â§ p and j‚Ä≤ a
permutation of the p registers;
2)
F1(œÄj‚Ä≤‚Ä≤(i)) = Yi, for 1 ‚â§ i ‚â§ q and j‚Ä≤‚Ä≤ a
permutation of the q registers;
3)
each conÔ¨Åguration is related to its successor by
‚ä¢M;
4)
the last conÔ¨Åguration has no successor by ‚ä¢M.
DeÔ¨Ånition 3.2: A register machine M computes the pro-
gram f if, for all s, t, r, we have that f(s, t, r) = q implies
that M computes (q)1, . . . , (q)#(f) on input (s)1, . . . ,
(s)#(f), (t)1, . . . , (t)#(f), (r)1, . . . , (r)#(f).
DeÔ¨Ånition 3.3:
1)
For each input ‚ÉóX (with | ‚ÉóX| =
n), M computes its output within time O(p(n)) if
its computation runs through O(p(n)) conÔ¨Ågura-
tions; M computes its output in space O(q(n)) if,
during the whole computation, the global length of
the contents of its registers is O(q(n)).
2)
For each input ‚ÉóX (with | ‚ÉóX| = n), M needs time
O(p(n)) and space O(q(n)) if the two bounds oc-
cur simultaneously, during the same computation.
Note that the number of registers needed by M to compute
a given f has to be Ô¨Åxed a priori (otherwise, we should have
to deÔ¨Åne a family of register machines for each program to
be computed, with each element of the family associated to
an input of a given length). According to deÔ¨Ånition 2.10 and
3.2, M uses a number of registers which linearly depends
on the highest component‚Äôs index that f can manipulate or
access with one of its constructors, destructors or selections;
and which depends on the number of times a variable is used
by f, that is, on the total number of different copies of the
registers that M needs during the computation. Both these
numbers are constant values.
Unlike the usual operators cons, head and tail over Lisp-
like lists, our constructors and destructors can have direct
access to any component of a list, according to deÔ¨Ånition 2.1.
Hence, their computation by means of a register machine
requires constant time, but it requires an amount of time
which is linear in the length of the input, when performed
by a Turing machine.
Codes. We write si¬©Fj(œÄ0)¬© . . . ¬©Fj(œÄk) for the word
that encodes a conÔ¨Åguration (si, Fj) of M, where each
component is a binary word over {0, 1}.
Lemma 3.1: f belongs to T1 if and only if f is com-
putable by a register machine within time O(n).
Proof: To prove the Ô¨Årst implication we show (by
induction on the structure of f) that each f ‚àà T1 can be
computed by a register machine Mf in time cn, where c is
a constant which depends on the construction of f, and n
is the length of the input.
Base. f ‚àà T0. This means that f is obtained by closure of
a number of modiÔ¨Åers under selection and simple schemes;
each modiÔ¨Åer g can be computed within time bounded by
lh(g), the overall number of basic instructions and deÔ¨Ånition
schemes of g, i.e. by a machine running over a constant
number of conÔ¨Ågurations; the result follows, since the safe
composition and the selection can be simulated by our model
of computation.
Step. Case 1. f =ITER(g), with g ‚àà T0. We have that
f(s, r) = g|r|(s). A register machine Mf can be deÔ¨Åned as
follows: (s)i is stored in the register œÄi (i = 1 . . . #(f)) and
(r)j is stored in the register œÄj (j = #(f) + 1 . . . 2#(f));
Mf runs Mg (within time lh(g)) for |r| times. Each time
Mg is called, Mf deletes one digit from one of the registers
œÄ#(f)+1 . . . œÄ2#(f), starting from the Ô¨Årst; the computation
stops, returning the Ô¨Ånal result, when they are all empty.
Thus, Mf computes f(s, r) within time |r|lh(g).
Case 2. Let f be deÔ¨Åned by simple schemes or SCMP. The
result follows by direct simulation of the schemes.
10
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

In order to prove the second implication, we show that
the behaviour of a k-register machine M which operates
in time cn can be simulated by a program in T1. Let
nxtM be a program in T0, such that nxtM operates on
input s = si¬©Fj(œÄ0)¬© . . . ¬©Fj(œÄk) and it has the form
if state[i](s) then Ei, where state[i](s) is a test which
is true if the state of M is si, and Ei is a modiÔ¨Åer which
updates the code of the state and the code of one among
the registers, according to the deÔ¨Ånition of M. By means of
c ‚àí 1 SCMP‚Äôs we deÔ¨Åne nxtc
M in T0, which applies c times
nxtM to the word that encodes a conÔ¨Åguration of M. We
deÔ¨Åne in T1

linsimM(x, a) =
x
linsimM(x, za) =
nxtc
M(linsimM(x, z))
linsimM(s, t) iterates nxtM(s) for c|t| times, returning the
code of the conÔ¨Åguration which contains the Ô¨Ånal result of
M.
IV.
THE TIME HIERARCHY
In this section, we recall the deÔ¨Ånition of classes of
programs T1 and S1; we deÔ¨Åne our hierarchy of classes
of programs, and we state the relation with the classes
of register machines, which compute their output within a
polynomially-bounded amount of time.
DeÔ¨Ånition 4.1: ITER(T0) denotes the class of programs
obtained by one application of iteration to programs in T0.
T1 is the class of programs obtained by closure under safe
composition and simple schemes of programs in T0 and
programs in ITER(T0).
Tk+1 is the class of programs obtained by closure under
safe composition and simple schemes of programs in Tk
and programs in SREC(Tk).
Notation: T1=(T0, ITER(T0); SCMP, SIMPLE).
Tk+1=(Tk, SREC(Tk); SCMP, SIMPLE).
DeÔ¨Ånition 4.2: ITER(S0) denotes the class of programs
obtained by one application of iteration to programs in S0.
S1 is the class of programs obtained by closure under safe
composition and simple schemes of programs in S0 and
programs in ITER(S0).
Sk+1 is the class of programs obtained by closure under sim-
ple schemes of programs in Sk and programs in SREC(Sk).
Notation: S1=(S0, ITER(S0); SCMP, SIMPLE).
Sk+1=(Sk, SREC(Sk); SIMPLE).
Hence, hierarchy Sk, with k ‚àà N, is a version of Tk in
which each program returns a result whose length is exactly
bounded by the length of the input; this doesn‚Äôt happen if
we allow the closure of Sk under SCMP. We will use this
result to evaluate the space complexity of our programs.
Lemma 4.1: Each f(s, t, r) in Tk (k ‚â• 1) can be com-
puted by a register machine within time |s|+lh(f)(|t|+|r|)k.
Proof: Base. f ‚àà T1. The relevant case is when f is
in the form ITER(h), with h a modiÔ¨Åer in T0. In lemma
3.1 (case 1 of the step) we have proved that f(s, r) can be
computed within time |r|lh(h); hence, we have the thesis.
Step. f
‚àà
Tp+1. The most signiÔ¨Åcant case is when
f =SREC(g, h). The inductive hypothesis gives two register
machines Mg and Mh which compute g and h within
the required time. Let r be the word a1 . . . a|r|; recalling
that f(s, t, ra) = h(f(s, t, r), t, ra), we deÔ¨Åne a register
machine Mf such that it calls Mg on input s, t, and calls
Mh for |r| times on input stored into the appropriate set of
registers (in particular, the result of the previous recursive
step has to be stored always in the same register). By
inductive hypothesis, Mg needs time |s|+lh(g)(|t|)p in order
to compute g; for the Ô¨Årst computation of the step program
h, Mh needs time |g(s, t)| + lh(h)(|t| + |a|r|‚àí1a|r||)p.
After |r| calls of Mh, the Ô¨Ånal conÔ¨Åguration is obtained
within overall time |s| + max(lh(g), lh(h))(|t| + |r|)p+1 ‚â§
|s| + lh(f)(|t| + |r|)p+1.
Lemma 4.2: The behaviour of a register machine which
computes its output within time O(nk) can be simulated by
an f in Tk.
Proof: Let M be a register machine respecting the
hypothesis. As we have already seen, there exists nxtM ‚àà T0
such that, for input the code of a conÔ¨Åguration of M, it
returns the code of the conÔ¨Åguration induced by the relation
‚ä¢M. Given a Ô¨Åxed i, we write the program œÉi by means of i
safe recursions nested over nxtM, such that it iterates nxtM
on input s for ni times, with n the length of the input:
œÉ0 :=ITER(nxtM) and
œÉn+1 :=IDTz/y(Œ≥n+1), where Œ≥n+1 :=SREC(œÉn, œÉn).
We have that
œÉ0(s, t) = nxt|t|
M(s), œÉn+1(s, t) = Œ≥n+1(s, t, t), and
( Œ≥n+1(s, t, a)
= œÉn(s, t)
Œ≥n+1(s, t, ra)
= œÉn(Œ≥n+1(s, t, r), t)
= Œ≥n(Œ≥n+1(s, t, r), t, t)
In particular we have
œÉ1(s, t) =
Œ≥1(s, t, t) =
œÉ0(œÉ0(. . . œÉ0(s, t) . . .))
|
{z
}
|t| times
= nxt|t|2
M
œÉ2(s, t) =
Œ≥2(s, t, t) =
œÉ1(œÉ1(. . . œÉ1(s, t) . . .))
|
{z
}
|t| times
= nxt|t|3
M
By induction we see that œÉk‚àí1 iterates nxtM on input s
for |t|k times, and that it belongs to Tk. The result follows
deÔ¨Åning f(t) = œÉk‚àí1(t, t), with t the code of an initial
conÔ¨Åguration of M.
Theorem 4.1: f belongs to Tk if and only if f is com-
putable by a register machine within time O(nk).
Proof: By lemma 4.1 and lemma 4.2.
We recall that register machines are polytime reducible to
Turing machines; this implies that S
k<œâ Tk captures PTIME
(see [8]).
11
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

V.
THE TIME-SPACE HIERARCHY
In this section, we deÔ¨Åne a time-space hierarchy of
recursive programs (see [4]), and we state the equivalence
with the classes of register machines which compute their
output with a simultaneous bound on time and space.
DeÔ¨Ånition 5.1: Given the programs g and h, f is ob-
tained by weak composition of h in g if f(x, y, z) =
g(h(x, y, z), y, z). Notation: f =WCMP(h, g).
In the weak form of composition the program h can be sub-
stituted only in the variable x, while in the safe composition
the substitution is possible in all variables.
DeÔ¨Ånition 5.2: For all p, q ‚â• 1, T Sqp is the class of
programs obtained by weak composition of h in g, with
h ‚àà Tq, g ‚àà Sp and q ‚â§ p.
Lemma 5.1: For all f in Sp, we have |f(s, t, r)| ‚â§
max(|s|, |t|, |r|).
Proof: By induction on p. Base. The relevant case is
when f ‚àà S1 and f is deÔ¨Åned by iteration of g in S0 (that is,
rog(g) ‚â§ 0). By induction on r, we have that |f(s, a)| = |s|,
and |f(s, ra)| = |g(f(s, r))| ‚â§ |f(s, r)| ‚â§ max(|s|, |r|).
Step. Given f ‚àà Sp+1, deÔ¨Åned by SREC in g and h in Sp,
we have
|f(s, t, a)|
= |g(s, t)|
by deÔ¨Ånition of f
‚â§ | max(|s|, |t|)|
by inductive hypothesis.
and
|f(s, t, ra)|
= |h(f(s, t, r), t, ra)|
‚â§ | max(|f(s, t, r)|, |t|, |ra|)|
‚â§ | max(max(|s|, |t|, |r|), |t|, |ra|)|
‚â§ | max(|s|, |t|, |ra|)|.
by deÔ¨Ånition of f, inductive hypothesis on h and induction
on r.
Lemma 5.2: Each f in T Sqp (with p, q ‚â• 1) can be
computed by a register machine within time O(np) and
space O(nq).
Proof: Let f be in T Sqp. By deÔ¨Ånition 5.2, f is deÔ¨Åned
by weak composition of h ‚àà Tq into g ‚àà Sp, that is,
f(s, t, r) = g(h(s, t, r), t, r). The theorem 4.1 states that
there exists a register machine Mh which computes h within
time nq, and there exists another register machine Mg which
computes g within time np. Since g belongs to Sp, lemma
5.1 holds for g; hence, the space needed by Mg is at most
n.
DeÔ¨Åne now a machine Mf that, by input s, t, r, performs
the following steps:
(1) it calls Mh on input s, t, r;
(2) it calls Mg on input h(s, t, r), t, r, stored in the appro-
priate registers.
According to lemma 4.1, Mh needs time equal to |s| +
lh(h)(|t| + |r|)q to compute h, and Mg needs |h(s, t, r)| +
lh(g)(|t| + |r|)p to compute g.
This happens because lemma 4.1 shows, in general, that the
time used by a register machine to compute a program is
bounded by a polynomial in the length of its inputs, but,
more precisely, it shows that the time complexity is linear
in |s|. Moreover, since in our language there is no kind
of identiÔ¨Åcation of x as z, Mf never moves the content
of a register associated to h(s, t, r) into another register
and, in particular, into a register whose value plays the
role of recursive counter. Thus, the overall time-bound is
|s|+lh(h)(|t|+|r|)q+lh(g)(|t|+|r|)p which can be reduced
to np, being q ‚â§ p.
Mh requires space nq to compute the value of h on input
s, t, r; as we noted above, the space needed by Mg for the
computation of g is linear in the length of the input, and
thus the overall space needed by Mf is still nq.
Lemma 5.3: A register machine which computes its out-
put within time O(np) and space O(nq) can be simulated
by an f ‚àà T Sqp.
Proof: Let M be a register machine, whose computation
is time-bounded by np and, simultaneously, it is space-
bounded by nq. M can be simulated by the composition
of two machines, Mh (time-bounded by nq), and Mg (time-
bounded by np and, simultaneously, space-bounded by n):
the former delimits (within nq steps) the space that the latter
will successively use in order to simulate M.
By theorem 4.1 there exists h ‚àà Tq which simulates the
behaviour of Mh, and there exists g ‚àà Tp which simulates
the behaviour of Mg; this is done by means of nxtg, which
belongs to S0, since it never adds a digit to the description
of Mg without erasing another one.
According to the proof of lemma 4.2, we are able to deÔ¨Åne
œÉn‚àí1 ‚àà Sn, such that œÉn‚àí1(s, t) = nxt|t|n
g
. The result
follows deÔ¨Åning sim(s) = œÉp‚àí1(h(s), s) ‚àà T Sqp.
Theorem 5.1: f belongs to T Sqp if and only if f is
computable by a register machine within time O(np) and
space O(nq).
Proof: By lemma 5.2 and lemma 5.3.
VI.
CONCLUSIONS
We have provided a resource-free characterization of reg-
ister machines that computes their output within polynomial
time, and we have extended it to register machines that
computes their output with a polynomial bound imposed on
time and space, simultaneously; this is made by a version of
predicative recursion and a related recursive programming
language. A program written in this languages can be
analyzed, and the complexity of the program is evaluated
by means of this simple analysis. This result represents
a preliminary step for a resource-free classiÔ¨Åcation of the
hierarchy of time-space classes between PTIME and PSPACE,
as deÔ¨Åned in [3].
12
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

REFERENCES
[1]
S. Bellantoni and S. Cook, "A new recursion-theoretic characterization
of the poly-time functions," Computational Complexity, vol. 2, 1992,
pp. 97-110.
[2]
S. Caporaso, G. Pani, E. Covino, "A Predicative Approach to the Clas-
siÔ¨Åcation Problem," Journal of Functional Programming, vol. 11(1),
Jan. 2001, pp. 95-116.
[3]
P. Clote, "A time-space hierarchy between polynomial time and
polynomial space," Math. Sys. The., vol. 25, 1992, pp. 77-92.
[4]
A. Cobham, "The intrinsic computational difÔ¨Åculty of functions," in
Proceedings of the International Conference on Logic, Methodology,
and Philosophy of Science. North-Holland, Amsterdam, 1962, pp. 24-
30, Y. Bar-Hillel Ed.
[5]
M. Hofmann, "Linear types and non-size-increasing polynomial time
computation," Information and Computation, vol. 183(1), May 2003,
pp. 57-85.
[6]
D. Leivant, "A foundational delineation of computational feasibility,"
in Proceedings of the Sixth Annual IEEE symposium on Logic in
Computer Science (LICS‚Äô91). IEEE Computer Society Press, 1991, pp.
2-11.
[7]
D. Leivant, "StratiÔ¨Åed functional programs and computational com-
plexity," in Proceedings of the 20th Annual ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages (POPL‚Äô93).
ACM, New York, 1993, pp. 325-333.
[8]
D. Leivant, RamiÔ¨Åed recurrence and computational complexity I: word
recurrence and polytime. Birkauser, 1994, pp. 320-343, in Feasible
Mathematics II, P. Clote and J. Remmel Eds.
[9]
D. Leivant and J.-Y. Marion, "RamiÔ¨Åed recurrence and computational
complexity II: substitution and polyspace," Computer Science Logic,
LNCS vol. 933, 1995, pp. 486-500, J. Tiuryn and L. Pocholsky Eds.
[10]
I. Oitavem, "New recursive characterization of the elementary func-
tions and the functions computable in polynomial space," Revista
Matematica de la Univaersidad Complutense de Madrid, vol 10.1, 1997,
pp. 109-125.
[11]
H. Simmons, "The realm of primitive recursion," Arch.Math. Logic,
vol. 27, 1988, pp. 177-188.
13
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

Hardware Realization of Robust Controller Designed Using Reflection Vectors
J√°n Cig√°nek, Michal Koc√∫r, ≈†tefan Koz√°k 
Faculty of Electrical Engineering and Information Technology  
Slovak University of Technology in Bratislava  
Bratislava, Slovakia 
E-mail: jan.ciganek@stuba.sk, michal.kocur@stuba.sk, stefan.kozak@stuba.sk 
 
 
Abstract‚Äî The presented paper deals with the new approach 
of robust controller design using the reflection vectors 
techniques. The control structure consists of feed-forward and 
feedback part. Proposed algorithms were tested using Field-
Programmable Gate Array (FPGA) technology for DC motor. 
Simulations and co-summations were realized in MATLAB-
Simulink. The co-simulation allows as to validate working 
hardware and to accelerate simulations in Simulink and 
MATLAB. The obtained results demonstrate very effective 
applicability of the theoretical principles for control of 
processes subject to parametrical model uncertainty. 
Keywords - robust control; robust stability; parametrical 
uncertainty; quadratic programming; reflection vectors; FPGA; 
co-simulation. 
I. 
 INTRODUCTION 
During the last ten years, development of robust control 
elementary principles and evolution of new robust control 
methods for different model uncertainty types are visible. 
Based on theoretical assumptions, modeling and simulation 
methods, an effective approach to the control of processes 
with strong and undefined uncertainties is designed. Such 
uncertainties are typical for biotechnology processes, 
chemical plants, automobile industry, aviation, etc. For such 
processes, it is necessary to design robust and practical 
algorithms which ensure the high performance and robust 
stability using proposed mathematical techniques with 
respect the parametric and unmodelled uncertainties [1][2].  
Solution to such problems is possible using robust predictive 
methods and ‚Äûsoft-techniques‚Äú which include fuzzy sets [3], 
neuron networks and genetic algorithms. 
Robust control is used to guarantee stability of plants 
with parameter changes. The robust controller design 
consists of two steps:  
‚Ä¢ 
analysis of parameter changes and their influence 
for closed-loop stability, 
‚Ä¢ 
robust control synthesis. 
There are two approaches for implementing control 
systems using digital technology. The Ô¨Årst approach is based 
on software which implies a memory-processor interaction. 
The memory holds the application program while the 
processor fetches, decodes, and executes the program 
instructions. Programmable Logic Controllers (PLCs), 
microcontrollers, microprocessors, Digital Signal Processors 
(DSPs) and general purpose computers are tools for software 
implementation. On the other hand, the second approach is 
based on hardware. Early hardware implementation is 
achieved by magnetic relays extensively used in old industry 
automation systems. Then, it became achievable by means of 
digital logic gates and Medium Scale Integration (MSI) 
components. When the system size and complexity 
increases, Application SpeciÔ¨Åc Integrated Circuits (ASICs) 
are utilized. The ASIC must be fabricated on a 
manufacturing line, a process that takes several months, 
before it can be used or even tested [4][5]. FPGAs are 
conÔ¨Ågurable ICs and used to implement logic functions. 
Today‚Äôs high-end FPGAs can hold several millions gates 
and have some signiÔ¨Åcant advantages over ASICs. They 
ensure ease of design, lower development costs, more 
product revenue and the opportunity to speed products to 
market [6]. At the same time, they are superior to software-
based controllers as they are more compact, power-efÔ¨Åcient, 
while adding high speed capabilities. 
The presented paper is organized as follows. In Section 
II, the complete procedure of robust controller design using 
reflection vectors is presented. Section III offers a short 
overview of hardware implementation of the proposed 
control algorithm using FPGA. The applicability of the 
control algorithm is shown on the case study for a real DC 
system with parametrical model uncertainty in Section IV. In 
Section V, the summary of the paper is discussed. 
II. 
PROBLEM STATEMENT 
Let us consider the robust control synthesis of a scalar 
discrete-time control loop. The transfer function of the 
original continuous-time system is described by the transfer 
function 
ÔÄ†
ÔÄ® ÔÄ©
ÔÄ® ÔÄ©
ÔÄ® ÔÄ©
Ds
n
n
n
n
m
m
m
m
Ds
P
e
a
s
a
s
a
b
s
b
b s
e
s
A
B s
s
G
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄΩ
ÔÄΩ
0
_
1
1
_
_
0
_
1
1
_
_
_
_
ÔÅã
ÔÅã
ÔÄ† ÔÄ®ÔÄ±ÔÄ©ÔÄ†
 
The transfer function of (1) can be converted to its 
discrete-time counterpart  
ÔÄ†
ÔÄ®
ÔÄ©
d
n
n
n
n
P
z
a z
z
a
b z
b z
b
z
G
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄΩ
ÔÅã
ÔÅã
1
1
1
1
0
1
1
ÔÄ†
ÔÄ®ÔÄ≤ÔÄ©ÔÄ†
14
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

For (2), a discrete-time controller is to be designed in 
form 
ÔÄ†ÔÄ†
ÔÄ® ÔÄ©
)
(
)
(
)
(
)
(
1
1
1
1
1
0
z
E
z
U
z
P
z
Q
p z
z
p
q z
q z
q
GR z
ÔÄΩ
ÔÄΩ
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄΩ
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÅ≠
ÔÅ≠
ÔÅµ
ÔÅµ
ÔÅã
ÔÅã
ÔÄ†
ÔÄ®ÔÄ≥ÔÄ©ÔÄ†
The corresponding closed-loop characteristic equation is 
ÔÄ†
0
)
)G ( z
G ( z
1
1
R
1
P
ÔÄΩ
ÔÄ´
ÔÄ≠
ÔÄ≠
ÔÄ†
ÔÄ®ÔÄ¥ÔÄ©ÔÄ†
Substituting (3) and (2) in (4), after a simple 
manipulation yield the characteristic equation 
ÔÄ†
0
)
)(
(
)
)(1
1(
1
1
1
1
1
0
1
1
1
1
ÔÄΩ
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄΩ
ÔÄ´
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
d
n
n
n
n
R
P
z
b z
b z
q z
q z
q
a z
a z
p z
p z
G
G
ÔÅã
ÔÅã
ÔÅã
ÔÅã
ÔÅµ
ÔÅµ
ÔÅ≠
ÔÅ≠
ÔÄ†ÔÄ®ÔÄµÔÄ©ÔÄ†
Unknown coefficients of the discrete controller can be 
designed using various methods. In this paper, a robust 
controller design method based on reflection vectors is used. 
The pole assignment problem is as follows: find a 
controller GR(z) such that C(z)=e(z) where e(z) is a given 
(target) polynomial of degree k. It is known [7] that, when 
Œº=n‚Äì1, the above problem has a solution for arbitrary e(z) 
whenever the plant has no common pole-zero pairs. In 
general, for Œº < n ‚Äì 1 exact attainment of a desired target 
polynomial e(z) is impossible. 
Let us relax the requirement of attaining the target 
polynomial e(z) exactly and enlarge the target region to a 
polytope V  in the polynomial space containing the point e 
representing 
the 
desired 
closed-loop 
characteristic 
polynomial. Without any restriction, we can assume that 
an ÔÄΩ p0 ÔÄΩ1
 and deal with monic polynomials C(z), i.e., 
ÔÅ°0 ÔÄΩ 1
. 
Let us introduce the stability measure as œÅ = cT c, where 
ÔÄ†
C
S
c
ÔÄ≠1
ÔÄΩ
ÔÄ†
ÔÄ®ÔÄ∂ÔÄ©ÔÄ†
and S is a matrix of dimensions (n + Œº + 1) x (n + Œº + 1) 
representing vertices of the target polytope V. For monic 
polynomials holds 
ÔÄ†
1
c
1
k
1
i
i ÔÄΩ
ÔÉ•
ÔÄ´
ÔÄΩ
ÔÄ†
ÔÄ®ÔÄ∑ÔÄ©ÔÄ†
where k = n + Œº. If all coefficients are positive, i.e., ci > 0, 
i = 1,..., k + 1, then the point C is placed inside the polytope 
V.  
The minimum œÅ is attained if 
ÔÄ†
1
k
1
c
c
c
k 1
2
1
ÔÄ´
ÔÄΩ
ÔÄΩ
ÔÄΩ
ÔÄΩ
ÔÄ´
ÔÅã
ÔÄ†
ÔÄ®ÔÄ∏ÔÄ©ÔÄ†
then the point C is placed in centre of the polytope V. 
In the matrix form, we have 
ÔÄ†
C ÔÄΩ Gx
ÔÄ†
ÔÄ®ÔÄπÔÄ©ÔÄ†
where G is the Sylvester matrix of the plant with dimensions 
ÔÄ®
ÔÄ© ÔÄ®
2ÔÄ©
1
ÔÄ´
ÔÄ´
ÔÇ¥
ÔÄ´
ÔÄ´
ÔÄ´
ÔÅµ
ÔÅ≠
ÔÅ≠
d
n
 and  x is the ÔÄ®
ÔÅ≠ ÔÄ´ÔÅµ ÔÄ´ 2ÔÄ©
-vector of 
controller parameters: 
ÔÅõ
ÔÅù
q T
q
p
p
x
0
1
,
,
, ,1
,
,
ÔÅã
ÔÅã
ÔÅµ
ÔÅ≠
ÔÄΩ
. 
Now, we can formulate the following control design 
problem: find a discrete controller, where the closed-loop 
characteristic polynomial C(z) is placed: 
a) In a stable target polytope V, 
V
C( z )
ÔÉé
 (to 
guarantee stability), 
b) As close as possible to a target polynomial e(z), 
V
(e z )
ÔÉé
(to guarantee performance). 
Let the polytope V denote the (k+1)√óN matrix composed 
of coefficient vectors vj, j=1,‚Ä¶,N  corresponding to vertices 
of the polytope V. 
Then, we can formulate the above controller design 
problem as an optimization task: Find x that minimizes the 
cost function 
ÔÄ†
2
1
min
2
min
e
Gx
e Gx
x G Gx
J
x
T
T
T
x
ÔÄ≠
ÔÄΩ
ÔÄ≠
ÔÄΩ
ÔÄ† ÔÄ®ÔÄ±ÔÄ∞ÔÄ©ÔÄ†
subject to the linear constraints 
ÔÄ†
V w(x),
Gx
ÔÄΩ
ÔÄ†
ÔÄ®ÔÄ±ÔÄ±ÔÄ©ÔÄ†
ÔÄ†
,
1,...,
,0
( )
N
j
wj x
ÔÄΩ
ÔÄæ
ÔÄ†
ÔÄ®ÔÄ±ÔÄ≤ÔÄ©ÔÄ†
ÔÄ†
ÔÉ•
ÔÄΩ
j
wj x
.1
( )
ÔÄ†
ÔÄ®ÔÄ±ÔÄ≥ÔÄ©ÔÄ†
Here, w(x) is the vector of weights of the polytope V 
vertices to obtain the point C = G x. Fulfillment of the latter 
two constraints (12), (13) guarantees that the point C is 
indeed located inside the polytope V. Then, finding the 
robust pole-placement controller coefficients represents an 
optimization problem that can be solved using the Matlab 
Toolbox OPTIM (quadprog) with constraints. 
Generally J1 is a kind of distance to the centre of the 
target polytope V. It is better to use another criterion J2, 
which measures the distance to the Schur polynomial E(z) 
ÔÄ†
).
) (
(
)
) (
(
2
E
Gx
E
Gx
E
C
E
C
J
T
T
ÔÄ≠
ÔÄ≠
ÔÄΩ
ÔÄ≠
ÔÄ≠
ÔÄΩ
ÔÄ† ÔÄ®ÔÄ±ÔÄ¥ÔÄ©ÔÄ†
It is possible to use the weighted combination of J1 and J2 
ÔÄ†
1
0
,
)
1(
2
1
ÔÇ£
ÔÇ£
ÔÄ´
ÔÄ≠
ÔÄΩ
ÔÅ°
ÔÅ°
ÔÅ°
J
J
J
ÔÄ†
ÔÄ®ÔÄ±ÔÄµÔÄ©ÔÄ†
and to solve the following quadratic programming task 
ÔÄ†
ÔÅõ
ÔÅù
ÔÅª
ÔÅΩ
.0
,
2
)
)(
1(
min
1
1
1
ÔÄº
ÔÄ≠
ÔÄ´
ÔÄ≠
ÔÄΩ
ÔÄ≠
ÔÄ´
ÔÄ≠
Gx
S
E Gx
Gx
I
SS
x G
J
T
k
T
T
T
x
ÔÅ°
ÔÅ°
ÔÅ°
ÔÄ†ÔÄ®ÔÄ±ÔÄ∂ÔÄ©ÔÄ†
Assume the discrete robust controller design task with 
parametrical uncertainties in system description. Let us also 
15
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

assume that coefficients of the discrete-time system transfer 
functions an , ..., a1 and bn , ..., b1 are placed in polytope W 
with the vertices 
ÔÅõ
j ÔÅù
j
n
j
j
n
j
b
a b
a
d
1
1
,
,
,
,
ÔÅã
ÔÅã
ÔÄΩ
: 
ÔÄ†
}
1,...,
,
{
M
j
conv d
W
j
ÔÄΩ
ÔÄΩ
ÔÄ†
ÔÄ®ÔÄ±ÔÄ∑ÔÄ©ÔÄ†
As (9) is linear in system parameters, it is possible to 
claim that for arbitrary vector of the controller coefficients x 
is the vector of the characteristic polynomial coefficients 
C(z) placed in the polytope A with vertices 
M
1
a,
,
a ÔÅã
: 
ÔÄ†
}
,1 ...,
,
{
M
j
conv a
A
j
ÔÄΩ
ÔÄΩ
ÔÄ†
ÔÄ®ÔÄ±ÔÄ∏ÔÄ©ÔÄ†
where a j = D j x and D j is the Sylvester matrix of 
dimensions (n + Œº + d + 1) x (Œº + œÖ + 2), composed of 
vertices set  d j , as in case of the exact model (9). 
A. Stable Region Computation via Reflection Coefficients 
Polynomials are usually specified by their coefficients or 
roots. They can be characterized also by their reflection 
coefficients using Schur-Cohn recursion. 
Let Ck(z-1) be a monic polynomial of degree k with real 
coefficients ciÔÉéR, i = 0, ..., k 
ÔÄ†
k
ckz
c z
C z
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ´
ÔÄ´
ÔÄΩ ÔÄ´
...
1
)
(
1
1
1
ÔÄ†
ÔÄ®ÔÄ±ÔÄπÔÄ©ÔÄ†
Reciprocal polynomial
)
(
ÔÄ™ zÔÄ≠1
Ck
 of the polynomial 
)
C ( z
1
k
ÔÄ≠  
is defined in [8] as follows 
ÔÄ†
k
k
k
k
k
z
c z
z
c
c
z
C
ÔÄ≠
ÔÄ≠ ÔÄ´
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ™
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄΩ
1
1
1
1
1)
(
ÔÅã
ÔÄ†ÔÄ®ÔÄ≤ÔÄ∞ÔÄ©ÔÄ†
Reflection coefficients ri, i = 1, ..., k, can be obtained 
from the polynomial 
)
(
ÔÄ≠z 1
Ck
 using backward Levinson 
recursion [9] 
ÔÄ†
ÔÅõ
ÔÅù)
(
)
(
1
1
)
(
1
1
2
1
1
1
ÔÄ≠
ÔÄ™
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄΩ
z
rC
z
C
r
z
C
z
i
i
i
i
i
ÔÄ†
ÔÄ®ÔÄ≤ÔÄ±ÔÄ©ÔÄ†
where 
i
i
c
r
ÔÄΩ ÔÄ≠
 and ic  is the last coefficient of 
)
(
Ci zÔÄ≠1
of 
degree i. From (21) we obtain in a straightforward way: 
ÔÄ†
).
( z
r C
)
( z
C
z
)
C ( z
1
i 1
i
1
1 i 1
1
i
ÔÄ≠
ÔÄ™ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ´
ÔÄΩ
ÔÄ† ÔÄ®ÔÄ≤ÔÄ≤ÔÄ©ÔÄ†
Expressions for polynomial coefficients 
)
(
1
1
ÔÄ≠
CiÔÄ≠ z
 and 
)
(
Ci zÔÄ≠1
 result from equations (22,23): 
ÔÄ†
ÔÄ®
ÔÄ©
ÔÉ∫
ÔÉª
ÔÉπ
ÔÉ™
ÔÉ´
ÔÉ©
ÔÄ≠
ÔÄ≠
ÔÄΩ
ÔÄ≠
ÔÄ≠
ÔÄΩ
ÔÄ≠ ÔÄ≠
ÔÄ´
ÔÄ≠
ÔÄ≠
ÔÉ•
)
1
1
)
(
1
0
1
,
1
,
2
1
1
j
i
j
j
i i
i
j
i
i
i
z
r c
c
r
z
C
ÔÄ†
ÔÄ®ÔÄ≤ÔÄ≥ÔÄ©ÔÄ†
 
ÔÄ†
ÔÄ®
ÔÄ©
.
)
(
0
1
,1
1
,1
1
j
i
j
j
i
i i
j
i
i
z
rc
c
z
C
ÔÄ≠
ÔÄΩ
ÔÄ≠ ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÉ•
ÔÄ´
ÔÄΩ
ÔÄ†
ÔÄ®ÔÄ≤ÔÄ¥ÔÄ©ÔÄ†
The reflection coefficients ri are also known as Schur-
Szeg√∂ parameters [8], partial correlation coefficients [11] or 
k-parameters [10]. Presented forms and structures were 
effectively used in many applications of signal processing 
[10] 
and 
system 
identification 
[11]. 
A 
complete 
characterization and classification of polynomials using their 
reflection coefficients instead of roots (zeros) of polynomials 
is given in [8]. 
The main advantage of using reflection coefficients is 
that the transformation from reflection to polynomial 
coefficients is very simple. Indeed, according to (22) and 
(24), polynomial coefficients ci depend multilinearly on the 
reflection coefficients ri. If the coefficients 
ci ÔÉé R
 are real, 
then also the reflection coefficients 
ri ÔÉé R
 are real. 
Transformation from reflection coefficients ri,i=1,...,k, to 
polynomial coefficients ci,i=1,...,k, is as follows 
( k )
i
i
c ÔÄΩ c
ÔÄ¨ÔÄ†
i
i i
r
c
) ÔÄΩ ÔÄ≠
(
ÔÄ¨ÔÄ†
1)
i(
j
i
i
1)
i(
j
)i(
j
r c
c
c
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄΩ
ÔÄ†
1
,
,1
, ;
,1
ÔÄ≠
ÔÄΩ
ÔÄΩ
i
k j
i
ÔÅã
ÔÅã
ÔÄ†
(25) 
Lemma 1.  A linear discrete-time dynamic system is 
stable if its characteristic polynomial is Schur stable, i.e., if 
all its poles lie inside the unit circle.  
The stability criterion in terms of reflection coefficients is 
as follows [8]. 
Lemma 2. A polynomial C(z-1) has all its roots inside the 
unit disk if and only if 
ri ÔÄº ,1
 i = 1, ..., k.  
A polynomial C(z-1) lies on the stability boundary if some 
,1
ri
ÔÄΩ ÔÇ±
 i = 1, ..., k. For monic Schur polynomials, there is 
a one-to-one correspondence between 
ÔÅõ
ÔÅùT
1
k
c,
c ,
C
ÔÅã
ÔÄΩ
 and 
ÔÅõ
ÔÅù .
r,
r ,
r
T
ÔÄΩ 1 ÔÅã k
 
Stability region in the reflection coefficient space is 
simply 
the 
k-dimensional 
unit 
hypercube 
ÔÅª
k, ÔÅΩ.
,1
1,1 ),i
(
r
R
i
ÔÅã
ÔÄΩ
ÔÉé ÔÄ≠
ÔÄΩ
The stability region in the 
polynomial coefficient space can be found starting from the 
hypercube R. 
B. Stable Polytope of Reflection Vectors 
It will be shown that, for a family of polynomials, the 
linear cover of the so-called reflection vectors is Schur 
stable. 
Definition 1. The reflection vectors of a Schur stable 
monic polynomial C(z-1) are defined as the points on stability 
boundary in polynomial coefficient space generated by 
changing a single reflection coefficient ri of the polynomial 
C(z-1). 
16
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

Let us denote the positive reflection vectors of C(z-1) as 
ÔÄ®
ÔÄ©
,k,
,1
1 i,
C r
v (C )
i
i
ÔÅã
ÔÄΩ
ÔÄΩ
ÔÄΩ
ÔÄ´
 and the negative reflection vectors of 
C(z-1) as 
ÔÄ®
ÔÄ©
.k,
,1
1 i,
C r
v (C )
i
i
ÔÅã
ÔÄΩ
ÔÄΩ ÔÄ≠
ÔÄΩ
ÔÄ≠
 
The following assertions hold: 
1) every Schur polynomial has 2k reflection vectors 
(C)
vi
ÔÄ´
and 
, ;
,1
( ),
k
vi C i
ÔÅã
ÔÄΩ
ÔÄ≠
 
2) all reflection vectors lie on the stability boundary 
1);
(
irv ÔÄΩ ÔÇ±
 
3) the line segments between reflection vectors 
(C)
vi
ÔÄ´
and 
(C)
vi
ÔÄ≠
are Schur stable. 
In the following theorem a family of stable polynomials 
is defined such that the polytope generated by reflection 
vectors of these polynomials is stable. 
Theorem 
1. 
Consider 
ÔÄ®
ÔÄ©,
1,1
1
ÔÄ≠
rC ÔÉé
ÔÄ®
ÔÄ©1,1
krC ÔÉé ÔÄ≠
 and 
0
r
r
C
1
k
C
2
ÔÄΩ
ÔÄΩ
ÔÄΩ
ÔÄ≠
ÔÅã
. Then, the inner points of the polytope V(C) 
generated by the reflection vectors of the point C 
ÔÄ†
ÔÅª
kÔÅΩ
C i
conv v
V C
i
,
,1
( ),
( )
ÔÄΩ ÔÅã
ÔÄΩ
ÔÇ±
ÔÄ†
ÔÄ®ÔÄ≤ÔÄ∂ÔÄ©ÔÄ†
are Schur stable. 
C. Robust Controller Design 
A robust controller is to be designed such that the closed-
loop characteristic polynomial is placed in the stable 
polytope (linear cover) of reflection vectors. It means that the 
following problems have to be solved: 
1. choice of initial polynomial C(z-1) for generating the 
polytope V(C), 
2. choice of  k + 1 most suitable vertices of V(C) to 
build a target simplex S, 
3. choice of a target polynomial E(z-1). 
In the following section some ‚Äúthumb rules‚Äù are given for 
choosing a stable target simplex S. 
To choose k + 1 vertices of the target simplex S we use 
the well known fact that poles with positive real parts are 
preferred to those with negative ones [1]. The positive 
reflection vectors 
(C )
vi
ÔÄ´
 with i odd and negative reflection 
vectors 
(C )
vi
ÔÄ≠
 with i even are chosen yielding k vertices. The 
(k+1)th vertex of the target simplex S is chosen as the mean 
of the remaining reflection vectors. 
The target polynomial E(z-1) of order k is reasonable to 
be chosen inside the stable polytope of reflection vectors 
V(C). A common choice is E(z-1)=C(z-1). 
For higher-order polynomials, the size of the target 
simplex S is considerably less than the volume of the 
polytope of reflection vectors V. That is why the above 
quadratic programming method with a preselected target 
simplex S works only if uncertainties are sufficiently small. 
Otherwise, it is reasonable to use some search procedure to 
find a robust controller such that the polytope of closed-loop 
characteristic polynomial is placed inside the stable polytope 
of reflection vectors V(C). 
In terms of the performance, the comparison of the 
proposed solution with other solutions would not be quite 
transparent due to different structure of the control loops or 
due to the different polynomial degrees of the controller. 
III. 
IMPLEMENTATION OF CONTROL ALGORITHM 
The digital form of the controller can be obtained from 
(3). Recursive form of control algorithm is expressed by the 
following equation: 
ÔÄ†
ÔÄ® ÔÄ©
ÔÄ®
ÔÄ©
ÔÄ®
ÔÄ©
ÔÄ®
ÔÄ©
ÔÄ®
2ÔÄ©
1
2
1
2
1
2
1
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ´
ÔÄ≠
ÔÄΩ
p u k
p u k
q e k
q e k
u k
ÔÄ®ÔÄ≤ÔÄ∑ÔÄ©ÔÄ†
For implementation of control algorithm (27) for FPGA 
it is necessary to decompose equation into simple arithmetic 
operations: 
ÔÄ® ÔÄ©
ÔÄ® ÔÄ©
ÔÄ® ÔÄ©
y k
w k
e k
ÔÄ≠
ÔÄΩ
ÔÄ†
ÔÄ®
ÔÄ©1
1 *
1 1
ÔÄ≠
ÔÄ≠ ÔÄΩ
e k
q
q e
ÔÄ†
ÔÄ®
2ÔÄ©
2 *
2
2
ÔÄ≠
ÔÄ≠ ÔÄΩ
e k
q
q e
ÔÄ†
ÔÄ®
ÔÄ©1
1 *
1 1
ÔÄ≠
ÔÄ≠ ÔÄΩ
u k
p
p u
ÔÄ†
ÔÄ®
2ÔÄ©
2 *
2
2
ÔÄ≠
ÔÄ≠ ÔÄΩ
u k
p
p u
ÔÄ†
2
2
1
1
1
ÔÄ≠
ÔÄ≠ ÔÄ´
ÔÄΩ
q e
q e
s
ÔÄ†
2
2
1
1
2
ÔÄ≠
ÔÄ≠ ÔÄ´
ÔÄΩ
p u
p u
s
ÔÄ†
2
1
3
s
s
s
ÔÄ≠
ÔÄΩ
 
ÔÄ®ÔÄ≤ÔÄ∏ÔÄ©ÔÄ†ÔÄ†
Control output u must be bounded in the range from umin 
to umax. 
ÔÄ†
ÔÄ® ÔÄ©
ÔÄ®
ÔÄ©
ÔÄ®
ÔÄ©
ÔÄ®
min ÔÄ©
min
max
min
max
max
3
3
3
3
u
if s
u
u
s
if u
s
u
if s
u
k
u
ÔÄº
ÔÇÆ
ÔÇ£
ÔÇ£
ÔÇÆ
ÔÄæ
ÔÇÆ
ÔÄΩ
ÔÄ†
ÔÄ®ÔÄ≤ÔÄπÔÄ©ÔÄ†
In this case, the parallel design of control algorithm is 
used, which means that each of the operation has its own 
arithmetic unit, either accumulator or multiplier. Parallel 
design is shown in Figure 1. 
 
w
y
Reg2
Reg1
Reg4
Reg3
-
*
*
+
q2e-2
q1e-1
*
*
+
-
p2e-2
p1e-1
<
>
mux
mux
U_max
U_min
U_max
e(k)
e(k-1)
u(k-1)
u(k-2)
u(k)
s1
s2
s3
q1
q2
p1
p2
 
Figure 1.  Parallel design of control algorithm 
Each sampling period is loaded the motor system output 
y(k) from the input in. Control error e(k) is computed in sub 
block where the signal y(k) is subtracted from w(k). Signal 
e(k) is held in the registry REG1 for one sampling period. 
Register REG1 output signal is thus e(k-1). In the same 
17
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

manner, e(k‚àí2), u(k‚àí1) and u(k‚àí2) are recorded at REG2, 
REG3 and REG4 by latching e(k‚àí1), u(k) and u(k-1) 
respectively. For multiplication, they are using digital signal 
processor (DSP) cores in FPGA chip. Results of 
multiplications are counted in to control output. This control 
output is then bounded in the range from ‚àí12V to 12V. 
Inputs w and y are represented with rpm (Revolutions per 
minute). Input range is -2048 to 2047 rpm, because of the 
12bit signed data type. Output of the controller is represented 
with volts. In signed binary representations the maximum 
control output is 12(10)V = 01100(2) and the minimum is 
‚àí12(10) V = 10100(2). We can write this range into 5 bits. Real 
numbers are useful for better quantization of the control 
output. For implementation of the real numbers, it has been 
used fixed point arithmetic [12]. As we can see in the Figure 
2, the first (MSB) bit of output vectors is reserved for a sign. 
Next four bits are reserved for the integer part and last seven 
bits are used for the fractional part. 
 
Figure 2.  Fixed-point control output 
Fixed point arithmetic is applied throughout the control 
algorithm. In designing this algorithm, the fixed-point 
arithmetic range rules must be respected. The data widths in 
the fixed-point arithmetic were designed that there is no 
possibility of an overflow. For example, the result of 
summation or subtraction of two 12-bit vectors has range 13-
bit. Table 1 represents used range rules for fixed point 
arithmetic. 
TABLE I.  
FIXED-POINT RANGE RULES 
Operation 
Result Range 
A + B 
Max(A'left, B'left)+1 
Min(A'right, B'right) 
A ‚Äì B 
Max(A'left, B'left)+1 
Min(A'right, B'right) 
A * B 
A'left + B'left+1 
A'right + B'right 
 
In the case of parallel the design of control algorithm, the 
control output after last summation (resp. subtraction) has 
range 40-bit (16-bit for fractional part). It must be used a 
bounder block to ensure of range (-12 V to 12 V) for 12bit. 
Bounder is the value limitation logic that keeps the output in 
the defined range. Bounded signal is latched at register 
REG3, thus becomes u(k‚àí1) of next cycle. In this way, the 
anti-windup protection is also ensured. 
System Generator toolbox for Simulink ensures that 
between the blocks gateway in and gateway out algorithm 
performs as it was implemented on FPGA (Figure 3). The 
parallel design of control algorithm designed in VHDL is 
contained in the control algorithm block.  
 
 
Figure 3.  Schematic of control circuit using Xilinx blocks 
In the co-simulation process, we used Xilinx Spartan-6 
FPGA which is included in SP-601 demoboard. Spartan-6 
FPGAs offer advanced power management technology, up to 
150K logic cells, advanced memory support, 250MHz DSP 
slices, and 3.2Gbps low-power transceivers [13]. 
 
 
Figure 4.  Schematic of control circuit using Co-sim block 
Based on the successful verification of the Xilinx blocks 
algorithm, we generated the co-simulation block by Xilinx 
System Generator (Figure 4). If is co-sim block included in 
simulation scheme it must be FPGA connected during the 
simulation process into the computer. At the start of 
simulation System Generator records functionality of co-sim 
block into FPGA. The co-sim block behaves as an in-out 
black box. Control output is computed in FPGA. Other 
blocks like the transfer function or step generator are still 
simulated in Simulink. Communication between FPGA 
board and the computer can be provided by Point to Point 
Ethernet or USB JTAG. 
IV. 
CASE STUDY  
Let us consider a DC system described by the first order 
nominal transfer function  
ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†
ÔÄ® ÔÄ©
ÔÄ® ÔÄ©
ÔÄ® ÔÄ©
s
Ds
Ds
P
e
s
e
s
T
K
s e
A
B s
s
G
001
.0
1
1
004856
.0
.28
25
1
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ´
ÔÄΩ
ÔÄ´
ÔÄΩ
ÔÄΩ
ÔÄ† ÔÄ®ÔÄ≥ÔÄ∞ÔÄ©ÔÄ†
where the coefficients K, T1 are varying in uncertainty 
intervals
K ÔÉé 25  ; 25.5
, 
 
 0.0052
T1 ÔÉé 0.0045;
 
Let us consider the nominal continuous-time transfer 
function which is converted to the discrete-time form with 
the sampling period T=0.001s: 
ÔÄ†
1
2
1
1
.0 8139
1
.4 282
.0 4228
)
(
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ´
ÔÄΩ
z
z
z
Gp z
ÔÄ†
ÔÄ®ÔÄ≥ÔÄ±ÔÄ©ÔÄ†
The main task is to design a robust iscrete-time controller 
(3), with polynomial degrees œÖ=1, Œº=2. 
From the transfer function (31) and matrix form of (9), 
we can obtain:  
00010.1110000(2) = 2.875(10) 
 
Fractional part 
 
Integer part 
 
Sign bit 
 
18
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉª
ÔÉπ
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ´
ÔÉ©
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉª
ÔÉπ
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ´
ÔÉ©
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄΩ
0
1
1
2
1
0
0
1
0
0
0
0
.0 8139
1
0
0
0
0
.0 8139
1
.0 4228
0
0
0
8139
.0
0
.0 4228
0
0
0
q
q
p
p
C
ÔÄ†
Let us choose the initial polynomial C(z-1) for generating 
the polytope V(C) as follows 
ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†
]
3.0
][1
2.0
][1
1.0
][1
2.0
1[
)
(
1
1
1
1
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ´
ÔÄ´
ÔÄ≠
ÔÄ≠
ÔÄΩ
z
z
z
z
C z
ÔÄ†
ÔÄ®ÔÄ≥ÔÄ≤ÔÄ©ÔÄ†
with reflection coefficients 
,2.0
r1 ÔÄΩ
 
.0 07,
2
r ÔÄΩ ÔÄ≠
 
.0 008,
3
r ÔÄΩ ÔÄ≠
 
r4 ÔÄΩ .0 0012.
  
Now, we can find the reflection vectors 
vi (C )
of the 
initial polynomial C(z-1) leading to the matrix form of the 
target simplex S (vertex polynomial coefficients) 
ÔÄ†
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉ∫
ÔÉª
ÔÉπ
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ™
ÔÉ´
ÔÉ©
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄΩ
1
1
1
1
1
0
0
0
5.0
3.0
0
0
5.0
3.0
0
0
5.0
3.0
0
0
5.0
3.0
0
0
0
S
ÔÄ†
ÔÄ®ÔÄ≥ÔÄ≥ÔÄ©ÔÄ†
The discrete-time controller design task for the nominal 
transfer function (30) has been solved via quadratic 
programming taking Œ±=0.3 in the cost function J (16). 
For the selected target simplex S, we have obtained the 
following discrete-time feedback controller 
ÔÄ†
ÔÄ®
ÔÄ©
2
1
1
1
1
1
.0 013
.0 0159
1
.0 0231
025
.0
)
(
)
(
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄΩ
ÔÄΩ
z
z
z
z
P
Q z
GFB z
ÔÄ†
ÔÄ®ÔÄ≥ÔÄ¥ÔÄ©ÔÄ†
and the control law is expressed in recursive form 
ÔÄ†
ÔÄ® ÔÄ©
ÔÄ®
ÔÄ©
ÔÄ®
ÔÄ©
)1
.0 0231 (
025 ( )
.0
2
.0 013
1
.0 016
2
2
2
ÔÄ≠
ÔÄ´
ÔÄ´
ÔÄ´
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄ≠
ÔÄΩ
y k
k
y
u k
u k
u k
ÔÄ†
ÔÄ®ÔÄ≥ÔÄµÔÄ©ÔÄ†
For verification of the FGPA hardware co-simulation of 
the digital controller, we realized a practical experiment. In 
co-simulation, we made step of reference signal at 0.01s.  
 
 
Figure 5.  Time response of output and reference variable under robust 
controller   
The corresponding closed-loop step response under the 
feedback 
controller 
(34) 
and feed-forward 
controller 
ÔÄ®
ÔÄ©
2
)
(
/)
(
1
1
1
ÔÄΩ
ÔÄΩ
ÔÄ≠
ÔÄ≠
ÔÄ≠
P z
S z
GFF z
 is in Figure 5. 
V. CONCLUSION 
The paper deals with the development of robust control 
algorithm based on reflection vectors methodology. The 
proposed algorithm can be effective realized using FPGA 
structure end guaranteeing stability, robustness and high 
performance. Theoretical results were verified on the 
example for feedback and feedforward control structures. 
The methods were also successfully tested for stable and 
unstable processes. 
The illustrative example was solved using quadratic 
programming for suitably defined performance function. 
Simulation results prove applicability of the proposed robust 
controller design theory for systems with parametric 
uncertainty. 
Digital controller was successfully implemented and 
hardware co-simulated on Spartan-6 FPGA board. From the 
obtained results, it is evident that the application of FPGA 
structure is very suitable for high speed processes. In this 
paper, we presented the basic necessary principles how to 
realize and modified existing digital robust control 
algorithms. The co-simulation option can be useful to 
accelerate simulation of advanced control algorithms. 
ACKNOWLEDGMENT 
This paper is supported by APVV project No. APVV-
0772-12. 
REFERENCES 
[1] J. Ackermann, ‚ÄúRobust Control‚Äù Systems with Uncertain 
Physical Parameters. Springer-Verlag. London, UK, 1993. 
[2] J. Ciganek and S. Kozak, ,,Robust Controller Design Based 
on Reflection Vectors‚Äù. Control Applications, (CCA) & 
Intelligent Control, (ISIC), 2009, pp. 938‚Äì943. 
[3] J. Ciganek and F. Noge,‚Äù Fuzzy Logic Control of 
Mechatronic Systems‚Äù, IEEE Int. conf. on Process Control, 
Strbske Pleso, Slovak Republic, 2013, pp. 309‚Äì313. 
[4] ≈†. Kozak ‚ÄúDevelopment of control engineering methods and 
their applications in industry‚Äù In 5th Int. Scientific-Technical 
Conference Process Control 2002. Kouty nad Desnou, Czech 
Rep., 2002, pp. 218-222. 
[5] M. Kocur, ‚ÄúHW Realization of PID algorithm based on 
FPGA,‚Äù Slovak University of Technology in Bratislava, 
Bratislava.  
[6] V. Viswanathan ‚ÄúEmbedded Control Using FPGA‚Äù. Indian 
Institute of Technology, Bombay, 2005 
[7] L. H. Keel and S. P. Bhattachayya, ‚ÄúA linear programming 
approach to controller design.‚Äù Automatica, vol. 35, 1999, pp. 
1717‚Äì1724. 
[8] J. L. Diaz-Barrero and J. J. Egozcue, ‚ÄúCharacterization of 
polynomials using reflection coefficients.‚Äù Appl. Math. E-
Notes, vol. 4, 2004, pp. 114‚Äì121. 
[9] B. Picinbono and M. Bendir, ‚ÄúSome properties of lattice 
autoregressive filters‚Äù, IEEE Trans. Acoust. Speech Signal 
Process, 34, 1986, pp. 342‚Äì349. 
[10] A. M. Oppenheim and R. W. Schaffer, ‚ÄúDiscrete-Time Signal 
Processing.‚Äù Prentice-Hall, Englewood Cliffs,1989. 
[11] S. M. Kay, ‚ÄúModern Spectral Estimation,‚Äù Prentice Hall, New 
Jersey, 1988. 
[12] D. 
Bishop. 
‚ÄúFixed 
point 
package 
user‚Äôs 
guide‚Äù. 
http://www.vhdl.org, [retrieved: 11, 2014] 
[13] Xilinx, ‚ÄúSP601 Hardware User Guide‚Äù 
19
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

Combining Code Refactoring and Auto-Tuning to Improve Performance Portability
of High-Performance Computing Applications
Chunyan Wang‚àó‚Ä†, Shoichi Hirasawa‚àó‚Ä†, Hiroyuki Takizawa‚àó‚Ä†, and Hiroaki Kobayashi‚Ä°
‚àó Graduate School of Information Sciences, Tohoku University
Sendai, Japan
Email: {wchunyan, hirasawa}@sc.isc.tohoku.ac.jp, takizawa@cc.tohoku.ac.jp
‚Ä†CREST, Japan Science and Technology Agency
‚Ä°Cyberscience Center, Tohoku University
Email: koba@cc.tohoku.ac.jp
Abstract‚ÄîAn existing High-Performance Computing (HPC) ap-
plication is usually optimized for a particular platform to achieve
high performance. Hence, such an application is often unable to
run efÔ¨Åciently on other platforms, i. e., its performance is not
portable. The purpose of this work is to establish a systematic
way to improve performance portability of HPC applications,
to which various kinds of platform-speciÔ¨Åc optimizations have
already been applied. To this end, we combine code refactoring
and auto-tuning technologies, and develop a programming tool
for HPC refactoring. Auto-tuning is a promising technology to
enable an HPC application to adapt to different platforms. In
general, however, an auto-tuning tool is not applicable to an
HPC application if the application has already been optimized
for a particular platform. In this work, a code refactoring tool
that interactively asks a user for necessary information to undo
some platform-speciÔ¨Åc optimizations in an existing application is
developed based on Eclipse, and hence auto-tuning techniques can
be applied to the application. The evaluation results demonstrate
that combining code refactoring and auto-tuning is a promising
way to replace platform-speciÔ¨Åc optimizations with auto-tuning
annotations, and thereby to improve the performance portability
of an HPC application.
Keywords‚Äìauto-tuning;
code
refactoring;
high-performance
computing; performance portability.
I.
INTRODUCTION
Legacy applications are successful and therefore mature,
and likely have been in existence for a long period of time.
Thus, legacy applications are often required to migrate to new
platforms, to use new algorithms, and to be components in
larger systems. Typical platforms are determined by a hard-
ware architecture, an operating system, and runtime libraries.
Moore‚Äôs law [1] has predicted the dramatic improvement of
computing hardware. As legacy applications are ported to meet
ever-changing requirements, even well-designed applications
are subject to structural erosion. The quality of any code
base with a long lifespan tends to degrade over time [2].
Maintenance of a legacy application can become an error-
prone, time- and resource-consuming work.
In the speciÔ¨Åc Ô¨Åeld of HPC, the main concern is to fully
utilize the potential of a speciÔ¨Åc target platform to achieve
high performance [3]. Today, an existing HPC application
is usually optimized for a particular platform. While HPC
systems (hardware and software) reach unprecedented levels
of complexity, such an application is often unable to run
efÔ¨Åciently on other platforms because different platforms re-
quire different optimizations, and hence its performance is not
portable. In order to maximize performance, the developer
must carefully consider optimizations relevant to each target
platform. As a result, tuning for an individual platform is a
highly-specialized and time-intensive process. The increasing
complexity of HPC systems, their long lifespans, and the
plethora of desirable source code optimizations make HPC
applications hard to maintain.
Automatic performance tuning, also known as auto-tuning,
provides performance portability, as the auto-tuning process
can easily be re-run on new platforms which require different
sets of optimizations [4]. Auto-tuning is increasingly being
used in the domain of HPC to optimize programs. However,
auto-tuning is usually designed for programs not optimized for
any speciÔ¨Åc platform. It is difÔ¨Åcult to auto-tune an HPC ap-
plication, to which platform-speciÔ¨Åc optimizations are already
applied.
Refactoring is a promising solution to gradual software
decay [2][5]. An application code can be refactored so that
it is easier to apply auto-tuning technologies. Refactoring
is a disciplined technique for restructuring an existing body
of code, altering its internal structure without changing its
external behavior [5]. Refactoring tools that can automate the
refactoring process have shown their advantages in improving
readability, maintainability and extensibility of object-oriented
programs. Behavior-preserving code transformations, which
are simply called refactorings in the research Ô¨Åeld of code
refactoring, can help to improve the code structure, thereby
potentially setting the stage for improvements. Despite the
potential, refactorings that are speciÔ¨Åc to HPC applications
are rarely provided.
However, some refactorings for HPC applications can-
not be fully automated because the information required for
those refactorings has been lost when the platform-speciÔ¨Åc
optimizations were applied to the application code. Users
have to specify where and how the application code was
optimized. Usually, a signiÔ¨Åcant part of the knowledge required
to perform the refactoring remains implicitly in the user‚Äôs
head. It is promising to use user knowledge to compensate the
lost information that is necessary for the refactoring. Semi-
automated refactoring can be a promising approach to refac-
toring application codes to be tunable with user knowledge.
Photran [6] provides some refactorings, called performance
refactorings, to facilitate some loop optimizations (interchange
loop, fuse loop, reverse loop, tile loop and unroll loop) speciÔ¨Åc
to Fortran, which is the backbone of the HPC community [7].
On one hand, these refactorings help to improve the perfor-
20
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

mance of an application. On the other hand, loop refactorings
such as loop tiling and loop unrolling make the code difÔ¨Åcult
to understand, and consequently make it hard for further
optimization. Due to the differences in system conÔ¨Ågurations,
an appropriate loop optimization on an HPC application usu-
ally changes depending on its target platform [8]. When the
application code is ported to a newly available platform, the
performance is usually not portable to the new platform. To
make matters worse, it is difÔ¨Åcult to optimize the refactored
code for a new platform. Therefore, those performance refac-
torings always lead to low performance portability.
The purpose of this work is to establish a systematic way
to improve performance portability of HPC applications, to
which various kinds of platform-speciÔ¨Åc optimizations have
already been applied. To achieve this goal, we combine code
refactoring and auto-tuning technologies. A code refactoring
tool is designed to support the process of undoing platform-
speciÔ¨Åc optimizations of an existing HPC application. Note
that some information such as the original loop length may be
lost by platform-speciÔ¨Åc loop optimizations such as Photran‚Äôs
performance refactorings. Hence, we develop a refactoring
tool that is assumed to be a part of an integrated develop-
ment environment (IDE) so that the tool can interactively
ask the user to specify the missing information for ‚Äúreverse
transformations‚Äù of performance refactorings. In this work,
the reverse transformations are called HPC refactorings. As
a result of HPC refactorings, platform-speciÔ¨Åc optimizations
are replaced with annotations for auto-tuning to make the
application adaptable to different platforms. Moreover, the
resulting application code becomes easy to read and maintain.
The evaluation results indicate that the HPC refactoring tool is
helpful to support replacement of platform-speciÔ¨Åc optimiza-
tions with auto-tuning annotations, and thereby to improve the
performance portability of an HPC application.
The remainder of this paper is organized as follows. Section
II introduces the related work. Section III describes the pro-
posed method and illustrates it with two examples. Section IV
shows the evaluation of the proposed method. Finally, Section
V gives the conclusion of this work, and states future work.
II.
RELATED WORK
This section reviews the related researches, and classiÔ¨Åes
them into four categories: (1) automation of refactorings; (2)
code refactorings for HPC applications; (3) platform-speciÔ¨Åc
optimizations that degrade performance portability; and (4)
refactoring and performance tuning.
A. Fully-automated versus Semi-automated Refactoring Tools
Mens and Tourw¬¥e [9] identiÔ¨Åed three activities associated
with the process of refactoring:
1)
IdentiÔ¨Åcation of where an application code should be
refactored.
2)
Determination of which refactoring(s) should be ap-
plied.
3)
Application of the selected refactorings.
The degree of the automation of a refactoring tool depends on
which of the refactoring activities are supported by the tool.
Contemporary IDEs such as Eclipse [10] often support a
semi-automated approach to refactoring. Tokuda and Batory‚Äôs
research [11] indicated that a semi-automated approach can
drastically increase the productivity in comparison with manual
refactoring.
Some researchers demonstrated the feasibility of fully-
automated refactoring [12][13]. Guru is a fully automated
tool for refactoring inheritance hierarchies and refactoring
methods in SELF programs [13]. Optimization techniques that
are performed by compilers can also be considered as fully
automated refactoring techniques. Although fully automated
tools provide the ability to quickly and safely improve the
structure of the code without altering its functionality, the
lack of user input leads to the introduction of meaningless
identiÔ¨Åers and could make the current application become
more difÔ¨Åcult to understand than before.
Semi-automated refactoring tools involve human interac-
tion to address the problems caused by fully-automated refac-
toring. The semi-automated refactoring tool may become time-
consuming when the application is in large scale. Despite this
problem, semi-automated refactoring remains the most useful
approach in practice, since a signiÔ¨Åcant part of the knowledge
required to perform the refactoring cannot be extracted from
the software, but remains implicit in the developer‚Äôs head [9].
B. Refactorings for HPC Applications
What HPC programmers concern most is to maximize the
performance on their target platforms. While refactorings are
typically used to improve the readability and maintainability
of a code, refactorings speciÔ¨Åc to Fortran and HPC to improve
performance are rarely provided. HPC programs are more
difÔ¨Åcult to restructure because data Ô¨Çow and control Ô¨Çow
are tightly interwoven [9]. Because of this, restructurings are
typically limited to the level of a function or a block of
code [14].
Nevertheless, many refactorings that improve performance
still have to be done manually. The parser and general language
infrastructure of a refactoring tool and performance preserving
requirements make it still a great challenge to develop a new
refactoring tool for the speciÔ¨Åc domain of HPC. Great efforts
have been made to develop tools to restructure the Fortran and
HPC codes [3][15].
Bodin et al. built an object-oriented toolkit and class
library for building Fortran and C++ restructuring tools [16]. It
requires complete understanding of the internal parser struc-
tures for users to add language extensions to Fortran or C.
CamFort [17] is a tool that provides automatic refactoring for
improving the code quality of existing models. SPAG [18] is
a restructuring tool, which unscrambles spaghetti Fortran66
code, and convert it to structured Fortran 77. New projects
such as Eclipse Parallel Tools Platform (PTP) have recently
made strides to address the needs of HPC developers [15].
It provides some refactorings such as Rename and Extract
Method. As a component of the Eclipse PTP, Photran provides
an IDE and refactoring tool for Fortran. However, since
Photran was originally designed to improve maintainability or
performance [19], it does not go far enough towards solving
the performance portability problem.
C. Platform-SpeciÔ¨Åc Optimizations
Some researchers have indicated that platform-speciÔ¨Åc op-
timizations degrade the performance portability of application
codes. Ratzinger et al. [20] exploited historical data extracted
from repositories, and pointed out that certain design fragments
in software architectures can have a negative impact on system
maintainability. Then, they proposed an approach to detecting
21
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

such design problems to improve the evolvability of an appli-
cation. Bailey et al. [21] pointed out that, an application may
not be able to run efÔ¨Åciently with available computer resource
unless the application has been optimized for the particular
system. Our previous research [22] has also reported that
platform-speciÔ¨Åc optimizations have a strong negative impact
on performance portability of an existing HPC application.
D. Refactoring and Performance Tuning
Fowler [5] stated that refactoring certainly will make
software go more slowly, but it also makes the software more
amenable to performance tuning. Du et al. [23] claimed that if
the architectural details are known, auto-tuning is an effective
way to generate tuned kernels that deliver acceptable levels of
performance. Moore‚Äôs research [24] applied Extract Procedure
refactoring to the performance critical regions to facilitate
performance tuning. The research also shows the possibility to
enhance refactoring tools with auto-tuning techniques. In [25],
it is veriÔ¨Åed that OpenACC directives for accelerators provide
a mechanism to stay in a current high-level language. Ope-
nACC directives are expected to enable programmers to easily
develop portable applications that maximize the performance
with the hybrid CPU/GPU architecture, and accelerate existing
applications quickly by adding a few lines of code. However,
platform-speciÔ¨Åc optimizations are still required even if Ope-
nACC directives are adopted for accelerator computing [26].
III.
THE PROPOSED HPC REFACTORING TOOL
In this work, we develop an HPC refactoring tool to
improve performance portability of HPC applications. Section
III describes the methodology of the proposed HPC refac-
toring tool. This method provides an undo mechanism with
code refactoring to undo platform-speciÔ¨Åc optimizations to
make an application code tunable, and a redo mechanism of
the optimizations with an auto-tuning technique. Undoing of
optimizations is necessary because auto-tuning tools usually
assume un-optimized codes as their inputs. By using auto-
tuning to make an existing HPC application adaptive to other
platforms, the performance portability of the application can
be improved in a systematic way.
A. Overview of the Proposed Method
Given an application that is optimized for a speciÔ¨Åc plat-
form for high performance, the performance portability of this
application is the ability to retain the performance when the
application is ported to other platforms.
Figure 1 illustrates the proposed method to improve perfor-
mance portability. Original Program represents the source pro-
gram that has already been optimized for a speciÔ¨Åc platform.
In this work, we develop a code refactoring tool as a plug-in
of Eclipse IDE to automate the process of undoing platform-
speciÔ¨Åc optimizations such as loop unrolling with a certain
unroll factor. With our refactoring tool, Original Program is
refactored to Refactored Program, in which platform-speciÔ¨Åc
optimizations are undone. Hence, an auto-tuning technique
can be applied to redo the optimizations to achieve high
performance on various platforms.
While it is difÔ¨Åcult to auto-tune Original Program, Refac-
tored Program can use an auto-tuning technique because such
a technique is usually designed for programs not optimized
for a speciÔ¨Åc platform. As a result, Autotuned Program that
is Refactored Program with an auto-tuning technique can
Autotuned Program
Refactored Program
Original Program
Opt2
Opt1
Opt2
Refactoring
Auto-tuning
UndoOpt1
UndoOpt2
Opt1
Figure 1. Overview of the proposed method for improving performance
portability.
achieve high performance portability across multiple platforms
while maintaining the performance on the original platform,
for which Original Program was optimized. In this way,
we systematically improve the performance portability of an
existing HPC application.
The code refactoring tool can undo a platform-speciÔ¨Åc
optimization only if the transformation rule of the optimization
is already known. Performance refactorings, such as loop
tiling and loop unrolling, provided by Photran degrade the
performance portability of an application code. Since the
transformation rule of these refactorings are already known,
we will use these refactorings to explain the undo mechanism
and show how the proposed method can contribute to the
performance portability.
B. Undo Mechanism of the Proposed Method
The undo mechanism provides a semi-automated refactor-
ing tool to allow users to specify the necessary information
for refactoring. Users select the code region and determine
which refactoring should be applied. The refactoring tool
then checks the preconditions for behavior preserving. Once
the preconditions are guaranteed, users are asked to input
necessary information for applying corresponding refactoring.
1) Undo Loop Unrolling: Loop unrolling is a loop trans-
formation technique that attempts to optimize a program‚Äôs per-
formance by reducing instructions that control the loop [27].
The general form of the do-loop is as follows.
do var = expr1, expr2, expr3
statements
end do
Here, var is the loop index, expr1 speciÔ¨Åes the initial value
of var, expr1 is the upper bound, and expr3 is the increment
(step). The variable deÔ¨Åned in the do-statements is incremented
by 1 by default.
Loop unrolling replicates the code inside a loop body mul-
tiple times. The step that determines the number of replications
is called an unroll factor.
The unroll loop refactoring provided by Photran applies
loop unrolling to the outermost loop of the selected nested do-
loop. Figure 2 shows a typical do-loop unrolled by unroll loop
refactoring when the selected do-loop is the innermost loop
and the unroll factor is set to be ‚ÄúB.‚Äù
To perform an undoing operation, users have to specify the
do-loop nest that loop unrolling was applied to and the unroll
factor prior to the unrolled loop nest. Then the loop header
is rewritten according to the user input, and the duplicated
statements are removed. The user should input the necessary
information in the following format.
22
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

do j=1,N
do k=1,N
do i=1,N,B
c(i,j)=c(i,j)+a(i,k)*b(k,j)
if(i+1>N) exit
c((i+1),j)=c((i+1),j)+a((i+1),k)*b(k,j)
if(i+2>N) exit
c((i+2),j)=c((i+2),j)+a((i+2),k)*b(k,j)
...
if(i+B-1>N) exit
c((i+B-1),j)=c((i+B-1),j)+a((i+B-1),k)*b(k,j)
end do
end do
end do
Figure 2. A do-loop unrolled B times by unroll loop refactoring in Photran.
!$Undo unroll(factor)
!$Undo unroll speciÔ¨Åes the refactoring that should be per-
formed, and argument factor indicates the unroll factor that
was applied to the loop nest. The new step value will be the
current step value (B for example in Figure 2) divided by
factor. As for the duplicated statements removal, it is assumed
that the number of statements is a multiple of the unroll
factor. The number of statements remaining after undoing loop
unrolling, called NoofStat, is the current number of statements
divided by factor. The Ô¨Årst NoofStat statements in the loop
body will be kept, and the rest statements will be removed.
With the help of our refactoring tool, the platform-speciÔ¨Åc
optimization, e. g., loop unrolling applied to the code in Fig-
ure 2, is undone. Figure 3 shows the resulting code by applying
the undo mechanism to the example shown in Figure 2. In
this case, the loop variable is set to increment by 1 and the
duplicated loop body is removed.
do j=1,N
do k=1,N
do i=1,N
c(i,j)=c(i,j)+a(i,k)*b(k,j)
end do
end do
end do
Figure 3. The resulting do-loop after applying undo mechanism.
2) Undo Loop Tiling: Loop tiling partitions a loop‚Äôs iter-
ation into smaller chunks or blocks, thus helps eliminate as
many cache misses as possible, and maximize data reuse [28].
The tile loop refactoring in Photran takes a double-nested
do-loop of unit-step loops, and creates a nested do-loop with
four levels of depth. This refactoring requires a user to provide
inputs of the tile size and the tile offset, so that the user can
set those parameters according to her/his will. The tile size
determines the size of the accessing block. For instance, if the
tile size is 3, an array will be accessed in 3 √ó 3 blocks. The
tile offset adjusts where the blocks start.
Let loopBound and newBound represent the loop bounds
of the double-nested do-loop before and after loop tiling,
respectively. The new bounds are computed using Equation (1).
newBound = Ô¨Çoor((loopBound ‚àí tileOÔ¨Äset)
/tileSize) ‚àó tileSize + tileOÔ¨Äset
(1)
Figure 4 shows a typical do-loop format before loop tiling.
Given a tile size ‚ÄúIS‚Äù and tile offset ‚Äúoffset,‚Äù the tile loop
refactoring in Photran always generates a tiled loop with the
following format shown in Figure 5. The outer loop goes over
the ‚Äúblocks‚Äù and the inner loop traverses each block in its turn.
Block size IS should be chose to Ô¨Åt in the cache or a memory
page (whichever is smaller).
!before loop tiling
do j=1,N
do i=1,N
a(i,j)=a(i,j)+b(i,j)*a(i,j)
end do
end do
Figure 4. A typical do-loop before loop tiling.
!after loop tiling
do j1=j_newLb, j_newUb, IS
do i1=i_newLb, i_newUb, IS
do j=max(j1,1), min(N, j1+IS-1)
do i=max(i1,1), min(N, i1+IS-1)
a(i,j)=a(i,j)+b(i,j)*a(i,j)
end do
end do
end do
end do
Figure 5. A do-loop tiled by tile loop refactoring in Photran with tile size IS.
To perform the undo mechanism of loop tiling to a do-loop
nest whose loop header has the format as shown in Figure 5,
users are required to specify the do-loop nest that loop tiling
was applied to and give the loop bounds for the new do-loop
nest. Users should provide the necessary information in the
following format prior to the do-loop nest.
!$Undo tile(loopName1,start1, end1,
loopName2, start2, end2)
!$Undo tile speciÔ¨Åes the refactoring that is going to be per-
formed, and arguments (loopName1,start1, end1, loopName2,
start2, end2) specify the new loop index variables and cor-
responding loop bounds. When the loop index matches the
information from users and its step value does not equal to
1, its loop header is replaced as the users speciÔ¨Åed. The loop
header whose index does not match the user information is
removed.
With the help of our refactoring tool, the nested do-loop
‚Äúafter loop tiling‚Äù can be restored to that ‚Äúbefore loop tiling.‚Äù
In this way, the proposed method undoes the loop tiling that
has been applied to the program. Hence, the modiÔ¨Åed program
can be tunable for further optimization by using auto-tuning
techniques developed for un-optimized codes.
23
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

C. Redo Mechanism of Proposed Method
The redo mechanism is supported by an auto-tuning tech-
nique. This paper assumes auto-tuning based on full parameter
search that tests all code variants of an annotated code frag-
ment and selects the best one with the highest performance,
even though any auto-tuning tools, such as the ROSE auto-
tuning mechanism [29] can be employed in the proposed
method. An annotation-based code generator, such as HMP-
PCG [30], is assumed to generate the code variants.
Generally, an auto-tuning tool is designed for un-optimized
codes. Our refactoring tool, which undoes platform-speciÔ¨Åc
optimizations, can refactor the Original Program to be tunable,
thus the Refactored Program can easily incorporate the auto-
tuning tool to obtain the best parameters for each platform.
As a result, Autotuned Program becomes adaptable to each
platform. By combining code refactoring and auto-tuning, an
existing HPC application can become adaptable to different
platforms. Accordingly, its performance portability is improved
in a systematic way.
IV.
PERFORMANCE EVALUATION
This section shows the evaluation of our refactoring tool
and illustrates the beneÔ¨Åt of the proposed method. The undo
mechanism is supported by a code refactoring tool, which
is developed by considering the Eclipse IDE [10] in mind
so that the IDE can provide an interactive user interface to
ask the user about necessary information for HPC refactoring.
The redo mechanism is supported by an empirical auto-tuning
technique that can search for the optimal tuning parameter for
each different platform.
The performance of Original Program on the speciÔ¨Åc CPU
platform, for which the program has originally been optimized,
is taken as the baseline performance on each platform. We
apply the proposed undo mechanism to Original Program
and obtain Refactored Program whose kernel code is tunable.
An auto-tuning technique based on full parameter search is
applied to Refactored Program, and thus we can get Autotuned
Program that has been adapted to another platform by using
the auto-tuning technique. Performance portability is discussed
according to the evaluation results.
A. Experimental Setup
To validate the effectiveness of the proposed method, we
measure the execution performance of a program on different
platforms to evaluate the performance portability. Platforms
with different cache sizes used in the following evaluation are
listed in Table I.
To show the effects of optimizations more clearly, we used
the ‚Äú-O0‚Äù option for the GNU compiler to disable compiler‚Äôs
optimizations. For the programs running on the SX-9 system,
we used the ‚Äú-O nounroll‚Äù option to disable the automatic
unrolling optimization of the FORTRAN90/SX compiler. To
evaluate the effects of HPC refactoring of unrolled loops, For-
tran matrix multiplication programs of a simple triple-nested
loop are used for multiplying two matrices of 512√ó512. With
the consideration of Ô¨Åtting the accessed array elements into
last level cache, we change the matrix size to be 3500 √ó 3500
to evaluate HPC refactoring of tiled loops.
B. Results and Discussions
Auto-tuning based on full parameter search is used to
Ô¨Ånd the optimal unroll factor and tile size for each target
platform. The execution time of the original program whose
unroll factor and tile size are optimized for Intel Core i7
930 are evaluated on the four platforms listed in Table I.
As shown in Figure 6, the execution time changes with the
unroll factor and tile size on each platform. The experimental
results indicate that the unroll factor and tile size have a
considerable impact on performance of different platforms.
Thus, the optimal parameters can be different for individual
platforms with different conÔ¨Ågurations. It is necessary to tune
those parameters to achieve high performance on each target
platform. Therefore, auto-tuning is needed to achieve high
performance portability. The experiment based on empirical
auto-tuning found that the optimal unroll factors for platforms
1 to 4 are 64, 64, 16 and 1, respectively. The optimal tile sizes
for platforms 1 to 4 are 1201, 1280, 720 and 256, respectively.
These optimal parameters are used for further experiments to
evaluate the performance portability.
It is shown that, the performance of the SX-9 system is
sensitive to the parameter conÔ¨Åguration. This is because the
Original Program is optimized for a totally different system
with Intel Core i7. As a simple matrix multiplication program
of a triple-nested loop is used in the evaluation, the sustained
performance of each platform is thus limited by the cache size
and memory bandwidth. The SX-9 system should achieve a
much higher performance than the others. The results indicate
that inappropriate optimizations mismatching the system ar-
chitecture could critically degrade the sustained performance,
even if the theoretical performance is high. These results hence
show the importance of making an HPC application adaptive
to other platforms to cope with system diversity.
To validate the usability of the proposed method, we mea-
sure the speedup ratios of target programs on each platform to
evaluate the performance portability. The evaluation results are
shown in Figure 7. In this Ô¨Ågure, the horizontal axis shows the
four target platforms, and the vertical axis shows the speedup
on each target platform. ‚ÄúOriginal Program‚Äù represents the
program whose unroll factor and tile size are optimized for
Intel Core i7 930. ‚ÄúRefactored Program‚Äù indicates the program
after undoing the optimizations with our refactoring tool.
‚ÄúAutotuned Program ‚Äù shows the program that is optimized
for each platform with the optimal unroll factor and tile size
obtained from the previous evaluation.
In Figure 7, Autotuned Program can achieve the same
performance as Original Program on the original target plat-
form, i .e., Intel Core i7 930. Even though Original Program
can get fair or higher performance on platforms with high
peak computational performance than Intel Core i7 930, the
computational ability of each platform is not well utilized,
and the performance portability of Original Program is low.
Refactored Program can incorporate auto-tuning more eas-
ily than Original Program with the help of our refactoring
tool. Thus, Autotuned Program can achieve comparable to
or higher performance than Original Program on other plat-
forms by auto-tuning optimization parameters. Accordingly,
by replacing platform-speciÔ¨Åc optimizations with auto-tuning
annotations, an HPC application can be performance-tunable
and its performance becomes, at a certain level, portable to
other platforms. These results clearly indicate the effectiveness
of our method to improve the performance portability. The
refactoring tools will be helpful to easily and safely do the
refactoring.
24
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

TABLE I. EXPERIMENTAL ENVIRONMENT.
Platform
CPU
Last Level Cache
Peak Computational Performance (GÔ¨Çops)
Maximum Memory Bandwidth (GB/s)
1
Intel Core i7 930
8MB
51.2
25.6
2
Intel Core i7 920
8MB
42.56
25.6
3
Intel Xeon CPU E5-2695 v2
30MB
230.4
59.7
4
NEC SX-9
256KB
102.4
256
0
0.2
0.4
0.6
0.8
1
1.2
1
2
4
8
16
32
64
128
256
512
Execution¬†Time¬†(s)
Unroll¬†Factor
Intel¬†Core¬†i7
930
Intel¬†Core¬†i7
920
Intel¬†Xeon¬†CPU
E5‚Äê2695¬†v2
NEC¬†SX‚Äê9
(a) Execution time changes with different unroll factors.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1
2
4
8
16
32
64
128
256
512
1024
2048
Execution¬†Time¬†(s)
Tile¬†Size
Intel¬†Core¬†i7¬†930
Intel¬†Core¬†i7¬†920
Intel¬†Xeon¬†CPU
E5‚Äê2695¬†v2
NEC¬†SX‚Äê9
(b) Execution time changes with different tile sizes.
Figure 6. Execution performance evaluation with different parameter conÔ¨Ågurations.
1
10
100
1000
Intel¬†Core¬†i7¬†930
Intel¬†Core¬†i7¬†920
Intel¬†Xeon¬†CPU¬†E5‚Äê
2695¬†v2
NEC¬†SX‚Äê9
Speedup
Original¬†Program
Refactored¬†Program
Autotuned¬†Program
0
1
10
100
62
62
(a) Speedup for Loop Unrolling.
0
2
4
6
8
10
12
14
16
Intel¬†Core¬†i7¬†930
Intel¬†Core¬†i7¬†920
Intel¬†Xeon¬†CPU¬†E5‚Äê
2695¬†v2
NEC¬†SX‚Äê9
Speedup
Original¬†Program
Refactored¬†Program
Autotuned¬†Program
(b) Speedup for Loop Tiling.
Figure 7. Performance evaluation results by applying the proposed method. Original Program is optimized for Intel Core i7 930.
Since platform-speciÔ¨Åc optimizations are removed from
Original Program, Refactored Program runs slower than Orig-
inal Program. The refactoring of undoing loop unrolling brings
performance optimization on the NEC SX-9 platform. The
vectorization processing on SX-9 is considered to be the reason
of performance improvement.
The refactoring of undoing loop tiling optimization de-
grades the performance signiÔ¨Åcantly on the NEC SX-9 plat-
form. NEC SX-9 is equipped with an on-chip memory of
256KB called ADB (Assignable Data Buffer) [31] to realize
a higher memory bandwidth as well as a shorter latency on
a chip for efÔ¨Åcient vector data accesses. The performance on
SX-9 is signiÔ¨Åcantly affected by the ADB size. This indicates
that the appropriate optimization is crucial to the performance
on different platform. The platform-speciÔ¨Åc optimizations in
an application should be removed to improve the performance
portability.
V.
CONCLUSION AND FUTURE WORK
This paper has proposed a systematic way for improv-
ing performance portability of HPC applications by combin-
ing code refactoring and auto-tuning technologies. A semi-
automated code refactoring tool that uses user knowledge is de-
veloped as an Eclipse plug-in to support undo platform-speciÔ¨Åc
optimizations so that the HPC applications can be refactored to
25
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

be tunable. Then, an auto-tuning technique is applied to Refac-
tored Program to redo optimizations in different ways so that
the application can adapt to multiple platforms. Therefore, the
performance portability of an existing HPC application can be
improved. The evaluation results demonstrate that combining
code refactoring and auto-tuning is a promising way to replace
platform-speciÔ¨Åc optimizations with auto-tuning annotations,
and thereby to improve the performance portability of an
existing HPC application.
This paper considered that only one kind of optimizations
is applied to the selected code region. However, in practice,
multiple optimizations can be applied to the same code region.
In the future, we will explore more complicated codes in which
multiple optimizations are applied.
ACKNOWLEDGMENT
This work was partially supported by JST CREST ‚ÄúAn
Evolutionary Approach to Construction of a Software Develop-
ment Environment for Massively-Parallel Heterogeneous Sys-
tems‚Äù and Grant-in-Aid for ScientiÔ¨Åc Research (B) #25280041.
REFERENCES
[1]
C. A. Mack, ‚ÄúFifty years of Moore‚Äôs law,‚Äù IEEE Transactions on
Semiconductor Manufacturing, vol. 24, no. 2, May 2011, pp. 202‚Äì207.
[2]
J. Overbey, S. Xanthos, R. Johnson, and B. Foote, ‚ÄúRefactorings for
Fortran and high-performance computing,‚Äù in Proceedings of the Second
International Workshop on Software Engineering for High Performance
Computing System Applications, ser. SE-HPCS‚Äô05.
New York, USA:
ACM, 2005, pp. 37‚Äì39.
[3]
R. Johnson, B. Foote, J. Overbey, and S. Xanthos, ‚ÄúChanging the face
of high-performance Fortran code,‚Äù White Paper, pp. 1‚Äì9, January 2006.
[4]
J. Demmel, S. Williams, and K. Yelick, ‚ÄúAutomatic performance
tuning (autotuning),‚Äù in The Berkeley Par Lab: Progress in the Parallel
Computing Landscape, D. Patterson, D. Gannon, and M. Wrinn, Eds.
Microsoft Research, August 2013, pp. 337‚Äì339.
[5]
M. Fowler, K. Beck, J. Brant, W. Opdyke, and D. Roberts, Refactoring:
Improving the Design of Existing Code. Addison-Wesley, 1999, iSBN-
10:0-201-48567-2.
[6]
‚ÄúPhotran
7.0
advanced
features,‚Äù
The
Elipse
Foundation,
2011.
[Online].
Available:
https://wiki.eclipse.org/PTP/photran/
documentation/photran7advanced [retrieved: 2015.02.09]
[7]
F. G. Tinetti and M. M¬¥endez, ‚ÄúFortran legacy software: Source code
update and possible parallelisation issues,‚Äù SIGPLAN Fortran Forum,
vol. 31, no. 1, March 2012, pp. 5‚Äì22.
[8]
S. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick, and J. Demmel,
‚ÄúOptimization of sparse matrix-vector multiplication on emerging mul-
ticore platforms,‚Äù in Proceedings of the 2007 ACM/IEEE Conference
on Supercomputing (SC‚Äô07), November 2007, pp. 1‚Äì12.
[9]
T. Mens and T. Tourw¬¥e, ‚ÄúA survey of software refactoring,‚Äù IEEE
Transactions on Software Engineering, vol. 30, no. 2, 2004, pp. 126‚Äì
139.
[10]
G. Watson, J. Alameda, B. Tibbitts, and J. Overbey, ‚ÄúDeveloping
scientiÔ¨Åc applications using Eclipse and the parallel tools platform,‚Äù
2010. [Online]. Available: http://download.eclipse.org/tools/ptp/docs/
ptp-sc10-tutorial.pdf [retrieved: 2015.02.09]
[11]
L. Tokuda and D. Batory, ‚ÄúEvolving object-oriented designs with refac-
torings,‚Äù in Automated Software Engineering, ser. 1, vol. 8.
Kluwer
Academic Publishers, 2001, pp. 89‚Äì120.
[12]
T. M. Peter Ebraert, Theo D‚ÄôHondt, ‚ÄúEnabling dynamic software
evolution through automatic refactoring,‚Äù in Proceedings of the 1st Int‚Äôl
Workshop on Software Evolution Transformations, November 2004, pp.
3‚Äì7.
[13]
I. Moore, ‚ÄúGuru - a tool for automatic restructuring of self inheri-
tance hierarchies,‚Äù in Technology of Object-Oriented Language Systems
(TOOLS), vol. 17, 1995, pp. 267‚Äì275.
[14]
J. L. Overbey, S. Negara, and R. E. Johnson, ‚ÄúRefactoring and the
evolution of Fortran,‚Äù in Proceedings of the 2009 ICSE Workshop on
Software Engineering for Computational Science and Engineering, ser.
SECSE‚Äô09.
Washington, DC, USA: IEEE Computer Society, 2009,
pp. 28‚Äì34.
[15]
F. B. Kjolstad, D. Dig, and M. Snir, ‚ÄúBringing the HPC programmer‚Äôs
ide into the 21st century through refactoring,‚Äù in SPLASH 2010
Workshop on Concurrency for the Application Programmer (CAP‚Äô10).
Association for Computing Machinery (ACM), October 2010, pp. 1‚Äì4.
[16]
F. Bodin et al., ‚ÄúSage++: An object-oriented toolkit and class library
for building Fortran and C++ restructuring tools,‚Äù in The second annual
object-oriented numerics conference (OON-SKI, 1994, pp. 122‚Äì136.
[17]
D. Orchard and A. Rice, ‚ÄúUpgrading Fortran source code using au-
tomatic refactoring,‚Äù in Proceedings of the 2013 ACM Workshop on
Workshop on Refactoring Tools, ser. WRT‚Äô13. New York, USA: ACM,
2013, pp. 29‚Äì32.
[18]
‚ÄúSPAG - Fortran code restructuring,‚Äù Polyhedron Software Products,
2014.
[Online].
Available:
http://www.polyhedron.com/products/
fortran-tools/plusfort-with-spag/spag-fortran-code-restructuring.html
[retrieved: 2015.02.09]
[19]
M. M¬¥endez, J. Overbey, A. Garrido, F. G. Tinetti, and R. Johnson, ‚ÄúA
catalog and classiÔ¨Åcation of Fortran refactorings,‚Äù in In 11th Argentine
Symposium on Software Engineering (ASSE 2010), 2010, pp. 500‚Äì505.
[20]
J. Ratzinger, M. Fischer, and H. Gall, ‚ÄúImproving evolvability through
refactoring,‚Äù SIGSOFT Softw. Eng. Notes, vol. 30, no. 4, May 2005,
pp. 1‚Äì5.
[21]
D. Bailey, R. Lucas, and S. Williams, Performance Tuning of ScientiÔ¨Åc
Applications, ser. Chapman & Hall/CRC Computational Science. CRC
Press, 2010.
[22]
C. Wang, S. Hirasawa, H. Takizawa, and H. Kobayashi, ‚ÄúCode refactor-
ing for high performance computing applications,‚Äù in Tohoku-Section
Joint Convention Record of Institutes of Electrical and Information
Engineers, 2013, p. 1A02.
[23]
P. Du, R. Weber, P. Luszczek, S. Tomov, G. Peterson, and J. Dongarra,
‚ÄúFrom CUDA to OpenCL: Towards a performance-portable solution for
multi-platform GPU programming,‚Äù Parallel Computing, vol. 38, no. 8,
August 2012, pp. 391‚Äì407.
[24]
S. Moore, ‚ÄúRefactoring and automated performance tuning of computa-
tional chemistry application codes,‚Äù in Simulation Conference (WSC),
Proceedings of the 2012 Winter, December 2012, pp. 1‚Äì9.
[25]
J. M. Levesque, R. Sankaran, and R. Grout, ‚ÄúHybridizing S3D into
an exascale application using OpenACC: An approach for moving
to multi-petaÔ¨Çops and beyond,‚Äù in 2012 International Conference on
High Performance Computing, Networking, Storage and Analysis (SC),
November 2012, pp. 1‚Äì11.
[26]
M.
Sugawara,
K.
Komatsu,
S.
Hirasawa,
H.
Takizawa,
and
H. Kobayashi, ‚ÄúImplementation and evaluation of the nanopowder
growth simulation with OpenACC,‚Äù The Special Interest Group Tech-
nical Reports of IPSJ, Tech. Rep. 10, 2012.
[27]
G. S. Murthy, M. Ravishankar, M. M. Baskaran, and P. Sadayappan.,
‚ÄúOptimal loop unrolling for GPGPU programs,‚Äù in 2010 IEEE Inter-
national Symposium on Parallel Distributed Processing (IPDPS), April
2010, pp. 1‚Äì11.
[28]
B. Bao and C. Ding, ‚ÄúDefensive loop tiling for shared cache,‚Äù in
2013 IEEE/ACM International Symposium on Code Generation and
Optimization (CGO), February 2013, pp. 1‚Äì11.
[29]
C. Liao and D. Quinlan, A ROSE-Based End-to-End Empirical
Tuning System for Whole Applications, Lawrence Livermore National
Laboratory, Livermore, CA 94550, July 2013. [Online]. Available:
http://rosecompiler.org/autoTuning.pdf [retrieved: 2015.02.09]
[30]
CAPS,
‚ÄúHMPP
codelet
generator
directives.‚Äù
2008.
[Online].
Available:
https://www.olcf.ornl.gov/wp-content/uploads/2012/02/
HMPPWorkbench-3.0 HMPPCG Directives ReferenceManual.pdf
[retrieved: 2015.02.09]
[31]
T. Soga et al., ‚ÄúPerformance evaluation of NEC SX-9 using real science
and engineering applications,‚Äù in Proceedings of the Conference on
High Performance Computing Networking, Storage and Analysis, ser.
SC‚Äô09.
New York, USA: ACM, 2009, pp. 28:1‚Äì28:12.
26
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

The Study of Statistical Simulation for
Multicore Processor Architectures
Jongbok Lee
Dept. of Information and Communications Engineering
Hansung University
Seoul, Republic of Korea
Email: jblee@hansung.ac.kr
Abstract‚ÄîThe execution-driven or trace-driven simulation is
often used for the performance analysis of widely used multicore
processors in the initial design stage. However much time and
disk space is necessary. In this paper, statistical simulations are
performed for high performance multicore processors with vari-
ous hardware conÔ¨Ågurations. For the experiment, the SPEC2000
benchmarks programs are used for the statistical proÔ¨Åling and
synthesis. As a result, the performance obtained by the statistical
simulation is comparable to that of the trace-driven simulation,
with a tremendous reduction in the simulation time.
Keywords‚Äìmulticore processor, statistical simulation
I.
INTRODUCTION
Currently, multicore processors are widely used for enhanc-
ing the performance of the computer system, such as smart
phones, tablet PCs, notebook computers, desk top computers,
etc. [1][2][3]. Since extensive simulations are necessary in
the initial design stage of these multicore processors, the
execution-driven simulation or trace-driven simulation is gen-
erally used. The execution-driven simulation is accurate but
requires excessive time, whereas the trace-driven simulation
is less accurate with the beneÔ¨Åt of relatively reduced time. In
addition, the trace-driven simulation has the disadvantage of
requiring excessive disk space.
In order to address these obstacles, various alternative
techniques have been studied. In the statistical simulation, the
statistical characteristics of the processor architecture and the
benchmark programs are collected. The statistical proÔ¨Åling is
the collection of the characteristic distribution of the programs
and the processor architectures. And then, new instruction
traces are synthesized upon these statistical proÔ¨Åles. Since
the new instruction traces are randomly generated by the
statistical proÔ¨Åles, they represent the characteristics of each
benchmark implicitly. Finally, the statistical simulation is per-
formed with the new instruction traces on a simple statistical
trace-driven simulator, which drastically reduces the simulation
time [4][5][6][7][8]. Therefore, the statistical simulation can be
useful for measuring the performance of multicore processors
in the initial design stage, with the reduced time and space.
In this paper, the SPEC2000 integer benchmark programs
are used for estimating the performance of multicore proces-
sors using statistical simulation. The result is compared with
the performance of the trace-driven simulation by calculating
the relative errors. This paper is organized as follows. In
the Section 2, the statistical proÔ¨Åling will be discussed. The
simulation environment will be described in Section 3. In
Section 4, the simulation results will be analyzed. Finally,
Section 5 concludes our paper.
II.
THE STATISTICAL PROFILING
The statistical simulation consists of four stages, as shown
in the Figure 1.
Generate Instruction
Trace
Statistical Profiling
Synthesize Instruction
Trace
Statistical Simulation
Figure 1. The statistical simulation process
The Ô¨Årst stage is the generation of the instruction traces,
which is the same as the conventional method. Each bench-
mark program is executed and its instruction traces are ob-
tained by the instruction trace generator.
Secondly, the statistical proÔ¨Åling and the statistical data
collection are performed. In this process, the instruction is
analyzed and the intrinsic characteristic and the local property
of each benchmark program is collected. The intrinsic charac-
teristic of each benchmark program consists of instruction class
distribution, the number of operand register distribution, and
the data dependency among instructions. The instruction class
distribution is obtained by reducing the original instruction
set to that of only nine simple instructions. The register
dependency among instructions is deÔ¨Åned as the distance
between the preceding instruction that includes the destination
register and the subsequent instruction that has the source
register on which it depends. These characteristics of bench-
mark are independent of the given microprocessor architecture,
but dependent only on the instruction set and the compiler.
The local characteristics include task misprediction rates, and
27
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

various cache miss rates, etc. [9]. These are affected by the
microprocessor hardware. This procedure is performed only
once during the whole process.
Thirdly, using the collected statistical proÔ¨Åles of the second
stage, a new benchmark is synthesized by the random number
generation. A random number between 0 and 1 is generated
and mapped onto the cumulative distribution function obtained
from statistical proÔ¨Åle in order to synthesize a new instruction
trace.
Finally, the synthesized instruction trace is input to the
statistical multicore simulator with the task prediction hit rates
and the cache hit rates. Once all the data from the statistical
proÔ¨Åle and the synthetic instruction traces are secured,various
experiments can be conducted by modifying the hardware ar-
chitecture such as the number of cores, the task size, instruction
fetch rates, the number of pipeline stages. Thus, a number of
various simulations can be performed in a dramatically reduced
time and space.
III.
THE SIMULATION ENVIRONMENT
A. The multicore processor architecture
Figure 2 shows the multicore processor with N cores. Each
core is an in-order or out-of-order superscalar processor which
can execute instructions in a task [2][9].
Figure 2. The multicore processor
In addition, it has a L1 instruction cache and a L1 data
cache. For the cache coherency of the L1 data cache, MESI
(ModiÔ¨Åed Exclusive Shared Invalidate) protocol is utilized
[10]. If the data in the L1 data cache associated with a core is
over-written by another core, it is invalidated. The L2 cache
is shared among the cores, which is connected with the main
memory.
The superscalar processor core is allocated with a task
which consists of a number of instructions. The fetched
instructions in the task are decoded, renamed, executed, and
written back. When all the instructions in the task are retired
and becomes empty, new instructions of task are fetched. If the
task is mispredicted, the fetch is aborted, and all the remained
instructions in the task are squashed. Since the instructions
are renamed, the instructions can be issued and executed out-
of-order as long as there is no true-dependency. Although the
instructions can be retired out-of-order, the instructions are
inserted into the reorder buffer and committed in-order as to
preserve the original program order.
The detailed architecture conÔ¨Ågurations and cache param-
eters for each core are listed in Table I. The number of
simulated cores are 2, 4, and 8. Each core is assigned one
task respectively. Since the small task size cannot take the
beneÔ¨Åt of the instruction level parallelism, the task sizes are
set to 4, 8, and 16. The functional unit of each core consists of
a number of ALUs (Arithmetic Logic Units), load/store units
according to each conÔ¨Åguration.
TABLE I. ARCHITECTURE CONFIGURATION FOR EACH CORE.
Item
Value
number of cores
2,4,8
number of task
1
task size
4,8,16
fetch rate
2,4,8
issue rate
2,4,8
retire rate
2,4,8
functional
integer ALU
2,4,8
unit
load/store
1,2,4
L1-instruction
64 KB, 2-way set assoc.,
cache
16 B block,
10 cycles miss penalty
L1-data
64 KB, 2-way set assoc.,
32 B block
cache
10 cycles miss penalty
task address cache
2K entry
task predictor
14-bit global history based
6 cycle mispred. penalty
For the memory disambiguation, load-store and store-store
pairs are inhibited from the speculative execution when the
effective addresses are matched, within or among the cores.
The L1 instruction cache and L1 data cache for each core
is 64 KB, and it is designed as 2-way set associative. This
is because the data cache hit ratio can be degraded by using
MESI protocol among multicores. Tasks are predicted using
the Two-level Adaptive Task Prediction scheme, and the task
address cache has the size of 2048 entries. Since we do not
model the main memory, the hit ratio of L2 cache is assumed
to be 100 %.
B. The multicore processor simulator
Figure 3 depicts how the developed simulator works.
Initialize function initializes all the associated variables,
and Grouping, Create Window, and Fetch One Instr function
fetches new instructions every cycle. The instruction fetched
by Get Node function is renamed at Rename function by
receiving timestamps. After the instruction is renamed, it
is inserted into the instruction window by Insert function.
At the Issue function, the instruction in the window can
be retired as long as the corresponding functional unit is
available and its time stamp is less than or equal to the
current cycle. For the multicore simulation, Grouping function
Ô¨Ålls an instruction into n-cores, and Issue function deletes
instructions according to their timestamps. This process is
repeated until all the fetched instructions are deleted so that all
the instruction windows are empty. Then, the cores are Ô¨Ålled
again with instructions with Grouping function. Since the cycle
is incremented for each process, the core which spends the
longest cycles determines the global cycle. If the total number
of executed instructions is divided by the number of cycles
spent, then IPC (Instruction per Cycle) can be obtained.
The seven SPEC 2000 integer benchmark programs that
are used for the input are bzip2,crafty, gap, gcc, gzip, parser,
28
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

Analysis
Initialize
Grouping(N)
Create_Window(N)
Fetch_One_Instr(N)
Get_Node(N)
Rename(N)
Insert(N)
Grouping(1)
Create_Window(1)
Fetch_One_Instr(1)
Get_Node(1)
Rename(1)
Insert(1)
Issue(N)
Mem_Process(N)
Mark_Node(N)
Delete_Node(N)
Issue(1)
Mem_Process(1)
Mark_Node(1)
Delete_Node(1)
Figure 3. The Ô¨Çow chart of the multicore processor simulator
and twolf, as shown in Table II. The programs are compiled
by SimpleScalar cross C compiler to obtain executables under
Linux 3.3.4 [11]. The execution Ô¨Åles are run with SimpleScalar
to obtain 100 million MIPS IV instruction traces. While these
are used as input for the multicore processors, the task-level
parallelism is mapped onto each core.
TABLE II. SPEC 2000 BENCHMARK PROGRAMS
benchmark
description
bzip2
compression
crafty
chess game
gap
group theory interpreter
gcc
C programming language compiler
gzip
compression
parser
word processor
twolf
placement and global routing
IV.
THE SIMULATION RESULTS
Figure 4 presents the simulation results of running SPEC
2000 integer programs on the three different task sizes of dual
core, quad core, and octa core processors. The performance
results obtained by the general trace-driven simulation and the
statistical simulation are compared in parallel. Figure 4a and
4b are the result of the multicore processors with the maximum
task size of four. For the dual core processors, the trace-driven
simulation brings the geometrical mean of 1.1 IPC, whereas
the statistical simulation results in 1.3 IPC. For the quad
core processors, the trace-driven simulation and the statistical
simulation results in 2.0 IPC and 2.4 IPC, respectively. Finally,
the respective performances for the octa core processors are 3.2
IPC and 4.3 IPC. Unlike the respective relative error of 18 %
and 20 % of dual core and quad core processors, the octa core
processors scores the relative error of 25 % in the average.
Figure 4c and 4d show the results with the maximum task
size of eight. The performance of the dual core, the quad core,
and the octa core processors measured by the trace-driven
simulations are 1.3 IPC, 2.4 IPC, and 4.0 IPC, respectively.
The corresponding values by the statistical simulations are 1.8
IPC, 3.1 IPC, and 5.4 IPC. The average relative error results
in 35 %.
Finally, Figure 4e and 4f presents the comparison result
when the maximum task size is sixteen. For the dual core
processors, the trace-driven simulation brings 1.5 IPC, whereas
the statistical simulation results in 2.1 IPC. For the quad cores,
the respective values are 2.8 IPC and 3.7 IPC. And for the octa
cores, the relative error is increased by the results of 4.7 IPC
and 6.4 IPC, respectively. However, the average relative error
does not exceed 35 %.
As the result shows, the performance of multicore proces-
sors evaluated by statistical simulation has the similar tendency
of the trace-driven simulation with the average relative error
of 18 % to 35 %. Encouragingly, the average statistical
simulation time is 9 seconds per benchmark program, which
is 30 times faster than the trace-driven simulation. Therefore,
this compensates for not being highly accurate.
V.
CONCLUSIONS
In this paper, the performance of multicore processors
has been evaluated and analyzed by the statistical simulation.
Since the statistical simulation takes advantage of the newly
synthesized instruction traces by statistical proÔ¨Åle, the disk
space is saved and the average simulation time is drastically
reduced. Although the experiment shows the average relative
error of 18 % to 35 %, the simulation time has been drastically
reduced to 1/30.
For future research, we will study the method to further
improve the accuracy of the statistical simulation, as well as
expanding our scope to the multicore embedded and multicore
digital signal processor architectures.
ACKNOWLEDGEMENT
The author would like to thank Hansung University for the
Ô¨Ånancial support of this research.
REFERENCES
[1]
T. Ungerer, B. Robic, and J. Silk, ‚ÄúMultithreaded Processors,‚Äù The
Computer Journal, vol. 45, no. 3, 2002.
[2]
S. W. Keckler, K. Olukotun, and H. P. Hofsee, Multicore Processors
and Systems.
Springer, 2009.
[3]
M. Monchiero, ‚ÄúHow to simulate 1000 cores,‚Äù ACM SIGARCH Com-
puter Architecture News archive, vol. 37, no. 2, May 2009, pp. 10‚Äì19.
[4]
D. B. Noonburg and J. P. Shen, ‚ÄúA Framework for Statistical Mod-
eling of Superscalar Processor Performance,‚Äù in Proceedings on Third
International Symposium on High Performance Computer Architecture,
1997.
[5]
R. Carl and J. E. Smith, ‚ÄúModeling Superscalar Processors via Statistical
Simulation,‚Äù in Workshop on Performance Analysis and Its Impact on
Design, Jun. 1998.
[6]
L. Eeckout, K. D. Bosschere, and H. Neefs, ‚ÄúPerformance Analysis
through Synthetic Trace Generation,‚Äù in International Symposium on
Performance Analysis of Systems and Software, Apr. 2000.
[7]
D. Genbrugge and L. Eckhout, ‚ÄúChip multiprocessor design space
exploration through statistical simulation,‚Äù IEEE Transactions on Com-
puters, vol. 58, no. 12, Dec. 2009, pp. 1668‚Äì1681.
[8]
A.Rico, A. Duran, F. Cabarcas, A. Ramirex, and M. Valero, ‚ÄúTrace-
driven simulation of multithreaded applications,‚Äù in ISPASS, 2011.
[9]
T. N. Vijaykumar and G. S. Sohi, ‚ÄúTask selection for a multiscalar
processor,‚Äù in 31st International Symposium on Microarchitecture, Dec
1998.
29
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

 0
 1
 2
 3
 4
 5
 6
 7
 8
bzip2
crafty
gap
gcc
gzip
parser
twolf
IPC
benchmarks
2-cores
4-cores
8-cores
(a) simulated, task size =4
 0
 1
 2
 3
 4
 5
 6
 7
 8
bzip2
crafty
gap
gcc
gzip
parser
twolf
IPC
benchmarks
2-cores
4-cores
8-cores
(b) statistical, task size =4
 0
 2
 4
 6
 8
 10
 12
bzip2
crafty
gap
gcc
gzip
parser
twolf
IPC
benchmarks
2-cores
4-cores
8-cores
(c) simulated, task size =8
 0
 2
 4
 6
 8
 10
 12
bzip2
crafty
gap
gcc
gzip
parser
twolf
IPC
benchmarks
2-cores
4-cores
8-cores
(d) statistical, task size =8
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
bzip2
crafty
gap
gcc
gzip
parser
twolf
IPC
benchmarks
2-cores
4-cores
8-cores
(e) simulated, task size =16
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
bzip2
crafty
gap
gcc
gzip
parser
twolf
IPC
benchmarks
2-cores
4-cores
8-cores
(f) statistical, task size =16
Figure 4. Performance results of the trace-driven and the statistical simulation
[10]
M. S. Papamarcos and J. H. Patel, ‚ÄúA low-overhead coherence solution
for multiprocessors with private cache memories,‚Äù in Proceedings of
the 11th Annual International Symposium on Computer Architecture,
Jun 1984, pp. 348‚Äì354.
[11]
T. Austin, E. Larson, and D. Ernest, ‚ÄúSimpleScalar : An Infrastructure
for Computer System Modeling,‚Äù Computer, vol. 35, no. 2, Feb. 2002,
pp. 59‚Äì67.
30
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

A New Refutation Calculus With Logical
Optimizations for PLTL
Mauro Ferrari
DiSTA
Universit`a degli Studi dell‚ÄôInsubria
Email: mauro.ferrari@uninsubria.it
Camillo Fiorentini
DI
Universit`a degli Studi di Milano
Email: Ô¨Åorentini@di.unimi.it
Guido Fiorino
DISCo
Universit`a degli Studi di Milano-Bicocca
Email: guido.Ô¨Åorino@unimib.it
Abstract‚ÄîPropositional Linear Temporal Logic (PLTL) is a tool
for reasoning about systems whose states change in time. We
present an ongoing work on a new proof-search procedure for
Propositional Linear Temporal Logic and its implementation. The
proof-search procedure is based on a one-pass tableau calculus
with a multiple-conclusion rule treating temporal-operators and
on some logical optimization rules. These rules have been devised
by applying techniques developed by the authors for logics with
Kripke semantics and here applied to the Kripke-based semantics
for Propositional Linear Temporal Logic.
Keywords‚ÄìPropositional Linear Temporal Logic; Tableaux; Sat-
isÔ¨Åability checking.
I.
INTRODUCTION
In recent years, while studying proof-search procedures
for non classical logics, we have introduced new tableau
calculi and logical optimization rules for propositional Intu-
itionistic Logic (INT) [1] and propositional G¬®odel-Dummett
Logic (DUM) [2]. As an application of these results, we
have implemented theorem provers for these logics [2][3]
that outperform their competitors. The above quoted calculi
and optimizations are the result of a deep analysis of the
Kripke semantics of the logic at hand. In this paper, we show
how such semantical analysis can also be fruitfully applied to
other non-classical logics with a Kripke-based semantics, by
analyzing the case of PLTL. In particular, we present a new
refutation tableau calculus and logical optimizations for PLTL
and we brieÔ¨Çy discuss a prototype Prolog implementation of
the resulting proof-search procedure.
As for related works, our tableau calculus for PLTL lies
in the line of the one-pass calculi based on sequents and
tableaux of [4][5][6], whose features are suitable for automated
deduction. We also cite as related the approaches based on
sequent calculi discussed in [7][8] and the natural deduction
based proof-search technique discussed in [9]. The results
in [10][11] are based on resolution, thus they are related less
to our approach.
The paper is organized as follows. Section II provides
the core of our logical characterization for PLTL, Section III
describes some optimization rules based on replacement of
formulas, Section IV gives an account of the performances
of the Prolog prototype under development, Ô¨Ånally Section V
summarizes the work with a short discussion.
II.
A NEW LOGICAL CHARACTERIZATION OF PLTL
We restrict ourselves to the temporal connectives Until U
and Next ‚ó¶. Entering more in details, our Ô¨Årst contribution is
a new logical characterization of PLTL by means of a tableau
calculus whose distinguished feature is the multiple-conclusion
rule Lin, that is a rule whose number of conclusions depends
on the number of U-formulas in the premise. The rule Lin is
inspired by the multiple-conclusion rules we have developed
in [2][12] to logically characterize the logic DUM. As a matter
of fact, PLTL and DUM are semantically characterized by
Kripke models based on linearly ordered states (we recall
that PLTL and DUM have different languages and a different
interpretation of the connective ‚Üí).
We give an account of rule Lin by means of an example
which also introduces the argument to prove its correctness.
Figure 1 contains the version for the language restricted
to the temporal connectives U and ‚ó¶. Let us suppose that
at time t a PLTL model K satisÔ¨Åes the set of formulas
S = {‚ó¶A, ‚ó¶(BUC), ‚ó¶(DUE)} (in formulas t ‚ä© S), where
the main connective of A is not U. This implies that: (i)
t + 1 ‚ä© A holds; (ii) there exists t1 > t such that t1 ‚ä© C
and, for every t < t‚Ä≤ < t1 , t‚Ä≤ ‚ä© B hold; (iii) there exists
t2 > t such that t2 ‚ä© E and, for every t < t‚Ä≤ < t2,
t‚Ä≤ ‚ä© D hold. The possible relationships among t + 1, t1
and t2 are the following: (1) t + 1 < t1, t2. In this case,
t+1 ‚ä© {A, B, D, ‚ó¶(BUC), ‚ó¶(DUE)} holds; (2) t+1 = t1 and
t1 ‚â§ t2. Hence t + 1 ‚ä© {A, C, (DUE)} holds; (3) t + 1 = t2
and t1 > t2. So we get t + 1 ‚ä© {A, B, ‚ó¶(BUC), E} holds.
Summarizing rule Lin handles ‚ó¶-formulas introducing in the
conclusion one branch for every formula of the kind ‚ó¶(AUB)
and one branch for all the other kind of ‚ó¶-formulas. In Figure 2
we show the tableau tree for the example.
At Ô¨Årst sight, rule Lin does not seem helpful to perform
automated deduction, since it can generate an huge number
of branches. However, as discussed in [2] for DUM, theorem
provers using the multiple-conclusion rules as Lin can be
effective. Moreover, a theorem prover can beneÔ¨Åt from some
further formulas (we call them side formulas) that we can
insert in the conclusions of Lin. The side formulas represent
correct information which is not necessary to handle to get
the completeness. Let us suppose that neither (1) nor (2) hold.
Then we can also prove that t + 1 ‚ä© ¬¨(D ‚àß ‚ó¶E) holds. When
D = ‚ä§ (that is DUE coincides with ‚ãÑE), then we can prove
that for every t‚Ä≤ ‚â• t+2, t‚Ä≤ ‚ä© ¬¨E holds. This information is not
necessary to the deduction but it can be exploited to perform
automated deduction. In particular, when the eventuality DUE
31
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

‚ó¶A1, . . . , ‚ó¶An, ‚ó¶(B1UC1), . . . , ‚ó¶(BmUCm)
S, B1, . . . , Bm, ‚ó¶(B1UC1), . . . , ‚ó¶(BmUCm)|S, C1, B2UC2, . . . , BmUCm|H2| . . . |Hm
Lin
where, S = {A1, . . . , An} and for i=2,...,m
Hi = ((S ‚à™{‚ó¶(B1UC1), . . . , ‚ó¶(BmUCm)})\{‚ó¶(BiUCi), . . . , ‚ó¶(BmUCm)})‚à™{B1, . . . , Bi‚àí1, Ci, ¬¨(Bi‚àß‚ó¶Ci), Bi+1UCi+1, . . . , BmUCm}
Figure 1. The multiple-conclusion rule Lin
‚ó¶A, ‚ó¶(BUC), ‚ó¶(DUE)
A, B, D, ‚ó¶(BUC), ‚ó¶(DUE)
C, DUE
C, E
C, ‚ó¶(DUE)
E
Lin
U
‚ó¶(BUC), E
C
Lin
Lin
A, C, DUE
A, C, E
A, C, ‚ó¶(DUE)
E
Lin
U
A, ‚ó¶(BUC), E
C
Lin
Lin
where U denotes the application of rule
AUB
B|‚ó¶(AUB)
U
Figure 2. Example of application of rule Lin
coincides with ‚ãÑE we get that from the time t+2 the formula
E coincides with ‚ä•. As a consequence, we have that if there is
a loop, then t and t+1 are not part of the loop and a theorem
prover can reset the history information. Moreover, since E is
equivalent to ‚ä•, it is correct to replace all the occurrences of
the formula E with ‚ä•. Rules based on replacement of formulas
with logical constants have been proved effective both for
INT [1] and for DUM [2].
III.
REPLACEMENTS RULES
In this section, we consider ‚ñ° in the language. We write
S[B/A] to denote the set of formulas obtained by replacing
in S every occurrence of A with B. The rules Replace-‚ñ° and
Replace-‚ñ°¬¨ given below are the analogous of rules Replace-T
and Replace-T¬¨ given in [1][13]:
S, ‚ñ°A
S[‚ä§/A], ‚ñ°A
Replace-‚ñ°,
S, ‚ñ°¬¨A
S[‚ä•/A], ‚ñ°¬¨A
Replace-‚ñ°¬¨.
A special case of Replace-‚ñ° and Replace-‚ñ°¬¨ are the rules
S, A
S{‚ä§/A}, A
Replace-cl,
S, ¬¨A
S{‚ä•/A}, ¬¨A
Replace-cl-¬¨.
where S{B/A} (note the curly braces) denotes the set of
formulas obtained by replacing with B the occurrences of A
in S that are not under the scope of any temporal connective.
In [1], we have introduced some variants of Replace-‚ñ° and
Replace-‚ñ°¬¨ based on the Kleene sign property. We exploit
some conditions under which we can replace a propositional
variable p applying the rules Replace-‚ñ° and Replace-‚ñ°¬¨ also
when neither ‚ñ°p nor ‚ñ°¬¨p explicitly occur in the premise of
the rule. The condition for the applicability of these rules is
based on the notion of polarity of p: p can be eliminated
from a set of formulas S (replaced with ‚ä§ or ‚ä•) if all the
occurrences of p in S have the same polarity. The notion of
polarity can be easily explained in the framework of PLTL
where formulas of the kind A ‚Üí B are written as ¬¨A ‚à® B
and Negation Normal Form is applied: a propositional variable
p occurs with positive (resp. negative) polarity in a set S,
denoted with p‚™Ø+ S (resp. p‚™Ø‚àí S) iff no occurrence (resp.
every occurrence) of p in S is of the kind ¬¨p. The following
are replacement rules of propositional variables fulÔ¨Ålling the
notion of positive or negative occurrence in a set that can be
computed in linear time on S:
S
S[‚ä§/p]
‚™Ø+ , provided p‚™Ø+ S;
S
S[‚ä•/p]
‚™Ø‚àí , provided p‚™Ø‚àí S.
In Figure 3 we show a piece of deduction for the formula
acacia-demo-v3_1, where also we apply some obvious
boolean simpliÔ¨Åcations.
IV.
PRELIMINARY RESULTS
We have developed a Prolog prototype to perform some
experiments on the benchmark formulas for PLTL. The devel-
opment of the prover is at the very early stage. We have only
focused on the implementation of the logical calculus with
rule Lin in its Ô¨Årst simpliÔ¨Åed version (without side formulas)
and the optimization rules provided in Section III. The part of
the prover related to the history construction, loop-checking
and loop-satisfaction is very naive. Thus, in general, the
known provers outperform our implementation. However, there
are some remarks related to the proposed optimizations that
deserve a comment. First, all the optimizations rules we have
described are effective in speeding-up the deduction. Second,
the rules ‚™Ø+ and ‚™Ø‚àí
apply to the benchmark formulas
acacia-demo-v3, alaska-szymanski, rozier (some
subfamilies), O1-schuppan and trp (some subfamilies)
without requiring any deduction step. On our Mac OS X
(2.7 GHz, Core i7, 8GB), in less than 10 (often in less
that 1) seconds, the prototype decides the families acacia,
alaska-lift
(except
for
the
non-negated
l
vari-
ant), alaska-szymanski, anzu-amba, anzu-amba_c
and anzu-amba_cl in negated version, forobotsr1f0
(many formulas in the negated version and in a few
cases also the non-negated versions), rozier-formulas,
rozier-patterns,
schuppan-O1
and
trp
(some
cases). Without the described optimizations timings would be
greater by some order of magnitudes.
32
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

H ‚â° ‚ä§ U (canc ‚àß ‚ó¶¬¨go) ‚à® (‚ñ°(¬¨req ‚à® ‚ó¶grt ‚à® ‚ó¶‚ó¶grt ‚à® ‚ó¶‚ó¶‚ó¶grt) ‚àß ‚ñ°(¬¨grt ‚à® ‚ó¶¬¨grt) ‚àß ‚ñ°(¬¨canc ‚à® ‚ó¶(¬¨grt U go)))
‚ä§ U (canc ‚àß ‚ó¶¬¨go) ‚à® (‚ñ°(¬¨‚ä• ‚à® ‚ó¶grt ‚à® ‚ó¶‚ó¶grt ‚à® ‚ó¶‚ó¶‚ó¶grt) ‚àß ‚ñ°(¬¨grt ‚à® ‚ó¶¬¨grt) ‚àß ‚ñ°(¬¨canc ‚à® ‚ó¶(¬¨grt U go)))
‚ä§ U (canc ‚àß ‚ó¶¬¨go) ‚à® (‚ñ°(‚ä§ ‚à® ‚ó¶grt ‚à® ‚ó¶‚ó¶grt ‚à® ‚ó¶‚ó¶‚ó¶grt) ‚àß ‚ñ°(¬¨grt ‚à® ‚ó¶¬¨grt) ‚àß ‚ñ°(¬¨canc ‚à® ‚ó¶(¬¨grt U go)))
H‚Ä≤ ‚â° ‚ä§ U (canc ‚àß ‚ó¶¬¨go) ‚à® (‚ñ°(‚ä§) ‚àß ‚ñ°(¬¨grt ‚à® ‚ó¶¬¨grt) ‚àß ‚ñ°(¬¨canc ‚à® ‚ó¶(¬¨grt U go)))
‚ä§ U (canc ‚àß ‚ó¶¬¨go) ‚à® (‚ñ°(‚ä§) ‚àß ‚ñ°(¬¨‚ä• ‚à® ‚ó¶¬¨‚ä•) ‚àß ‚ñ°(¬¨canc ‚à® ‚ó¶(¬¨‚ä• U go)))
‚ä§ U (canc ‚àß ‚ó¶¬¨go) ‚à® (‚ñ°(‚ä§) ‚àß ‚ñ°(‚ä§ ‚à® ‚ó¶‚ä§) ‚àß ‚ñ°(¬¨canc ‚à® ‚ó¶(‚ä§ U go)))
‚ä§ U (canc ‚àß ‚ó¶¬¨go) ‚à® (‚ñ°(‚ä§) ‚àß ‚ñ°(‚ä§) ‚àß ‚ñ°(¬¨canc ‚à® ‚ó¶(‚ä§ U go)))
‚ä§ U (canc ‚àß ‚ó¶¬¨go) ‚à® ‚ñ°(¬¨canc ‚à® ‚ó¶(‚ä§ U go))
bool
bool
bool
‚™Ø‚àí , grt‚™Ø‚àí H‚Ä≤
bool
bool
‚™Ø‚àí , req‚™Ø‚àí H
Figure 3. Example of application of rule ‚™Ø‚àí to the formula acacia-demo-v3_1
V.
CONCLUSION AND FUTURE WORK
We have presented our ongoing research on automated
deduction for PLTL. We face the problem along different
lines. In this note we have discussed two of them: Section II
provides some ideas for a new proof-theoretical characteriza-
tion of PLTL based on a multiple-conclusion rule; Section III
describes logical rules to cut the size of the proofs. In addition
to the given results, an important part of the future work is to
exploit the notions of local formula [3] and evaluation [14] to
develop an advanced strategy to avoid some rule application.
ACKNOWLEDGMENT
The third author acknowledges the support of the MIUR
PRIN 2010-2011 grant ‚ÄúAutomi e Linguaggi Formali: Aspetti
Matematici e Applicativi‚Äù, code 2010LYA9RH.
REFERENCES
[1]
M. Ferrari, C. Fiorentini, and G. Fiorino, ‚ÄúSimpliÔ¨Åcation rules for
intuitionistic propositional tableaux,‚Äù ACM Trans. Comput. Logic,
vol. 13, no. 2, Apr. 2012, pp. 14:1‚Äì14:23. [Online]. Available:
http://doi.acm.org/10.1145/2159531.2159536
[2]
G. Fiorino, ‚ÄúRefutation in Dummett logic using a sign to express the
truth at the next possible world,‚Äù in IJCAI, T. Walsh, Ed. IJCAI/AAAI,
2011, pp. 869‚Äì874.
[3]
M. Ferrari, C. Fiorentini, and G. Fiorino, ‚ÄúfCube: An efÔ¨Åcient prover for
intuitionistic propositional logic,‚Äù in LPAR (Yogyakarta), ser. Lecture
Notes in Computer Science, C. G. Ferm¬®uller and A. Voronkov, Eds.,
vol. 6397.
Springer, 2010, pp. 294‚Äì301.
[4]
K. Br¬®unnler and M. Lange, ‚ÄúCut-free sequent systems for temporal
logic,‚Äù J. Log. Algebr. Program., vol. 76, no. 2, 2008, pp. 216‚Äì225.
[5]
J. Gaintzarain, M. Hermo, P. Lucio, M. Navarro, and F. Orejas, ‚ÄúDual
systems of tableaux and sequents for PLTL,‚Äù J. Log. Algebr. Program.,
vol. 78, no. 8, 2009, pp. 701‚Äì722.
[6]
S. Schwendimann, ‚ÄúA new one-pass tableau calculus for PLTL,‚Äù in
Tableaux‚Äô98, 1998, pp. 277‚Äì291.
[7]
R. Pliuskevicius, ‚ÄúInvestigation of Ô¨Ånitary calculus for a discrete linear
time logic by means of inÔ¨Ånitary calculus,‚Äù in Baltic Computer Science,
ser. Lecture Notes in Computer Science, J. Barzdins and D. Bj√∏rner,
Eds., vol. 502.
Springer, 1991, pp. 504‚Äì528.
[8]
B. Paech, ‚ÄúGentzen-systems for propositional temporal logics,‚Äù in CSL,
ser. Lecture Notes in Computer Science, E. B¬®orger, H. K. B¬®uning, and
M. M. Richter, Eds., vol. 385.
Springer, 1988, pp. 240‚Äì253.
[9]
A. Bolotov, O. Grigoriev, and V. Shangin, ‚ÄúAutomated natural deduction
for propositional linear-time temporal logic,‚Äù in TIME. IEEE Computer
Society, 2007, pp. 47‚Äì58.
[10]
M. Fisher, C. Dixon, and M. Peim, ‚ÄúClausal temporal resolution,‚Äù ACM
Trans. Comput. Log., vol. 2, no. 1, 2001, pp. 12‚Äì56.
[11]
M. Suda and C. Weidenbach, ‚ÄúLabelled superposition for PLTL,‚Äù
in LPAR, ser. Lecture Notes in Computer Science, N. Bj√∏rner and
A. Voronkov, Eds., vol. 7180.
Springer, 2012, pp. 391‚Äì405.
[12]
G. Fiorino, ‚ÄúTableau calculus based on a multiple premise rule,‚Äù
Information Sciences, vol. 180, no. 19, 2010, pp. 371‚Äì399.
[13]
F. Massacci, ‚ÄúSimpliÔ¨Åcation: A general constraint propagation tech-
nique for propositional and modal tableaux,‚Äù in Proc. International
Conference on Automated Reasoning with Analytic Tableaux and
Related Methods, Oosterwijk, The Netherlands, ser. LNCS, H. de Swart,
Ed., vol. 1397.
Springer-Verlag, 1998, pp. 217‚Äì232.
[14]
M. Ferrari, C. Fiorentini, and G. Fiorino, ‚ÄúAn evaluation-driven decision
procedure for G3i,‚Äù TOCL, in press.
33
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking
Powered by TCPDF (www.tcpdf.org)

