BUSTECH 2018
The Eighth International Conference on Business Intelligence and Technology
ISBN: 978-1-61208-614-9
February 18 - 22, 2018
Barcelona, Spain
BUSTECH 2018 Editors
Philippe Marchildon, ESG-UQAM, Canada
Rawad Hammad, King's College London, UK

BUSTECH 2018
Forward
The Eighth International Conference on Business Intelligence and Technology (BUSTECH 2018),
held between February 18 - 22, 2018 - Barcelona, Spain, continued a series of events covering
topics
related
to
business
process
management
and
intelligence,
integration
and
interoperability of different approaches, technology-oriented business solutions and specific
features to be considered in business/technology development.
The conference had the following tracks:

Modeling and simulation

BPM and Intelligence

Features of business/technology development
Similar to the previous edition, this event attracted excellent contributions and active
participation from all over the world. We were very pleased to receive top quality
contributions.
We take here the opportunity to warmly thank all the members of the BUSTECH 2018 technical
program committee, as well as the numerous reviewers. The creation of such a high quality
conference program would not have been possible without their involvement. We also kindly
thank all the authors that dedicated much of their time and effort to contribute to BUSTECH
2018. We truly believe that, thanks to all these efforts, the final conference program consisted
of top quality contributions.
Also, this event could not have been a reality without the support of many individuals,
organizations and sponsors. We also gratefully thank the members of the BUSTECH 2018
organizing committee for their help in handling the logistics and for their work that made this
professional meeting a success.
We hope BUSTECH 2018 was a successful international forum for the exchange of ideas and
results between academia and industry and to promote further progress in the area of business
intelligence and technology. We also hope that Barcelona provided a pleasant environment
during the conference and everyone saved some time for exploring this beautiful city.
BUSTECH 2018 Chairs
BUSTECH 2018 Steering Committee
Malgorzata Pankowska, University of Economics in Katowice, Poland

Hermann Kaindl, Vienna University of Technology, Austria
Jürgen Sauer, Universität Oldenburg, Germany
Pierre Hadaya, ESG-UQAM, Canada
BUSTECH 2018 Industry/Research Advisory Committee
Anbang Xu, IBM Research - Almaden, USA
Hans-Friedrich Witschel, University of Applied Sciences and Arts, Northwestern Switzerland
Oscar Ferrandez-Escamez, Nuance Communications Inc., USA
Silvia Biasotti, CNR - IMATI, Italy

BUSTECH 2018
Committee
BUSTECH 2018 Steering Committee
Malgorzata Pankowska, University of Economics in Katowice, Poland
Hermann Kaindl, Vienna University of Technology, Austria
Jürgen Sauer, Universität Oldenburg, Germany
Pierre Hadaya, ESG-UQAM, Canada
BUSTECH 2018 Industry/Research Advisory Committee
Anbang Xu, IBM Research - Almaden, USA
Hans-Friedrich Witschel, University of Applied Sciences and Arts, Northwestern Switzerland
Oscar Ferrandez-Escamez, Nuance Communications Inc., USA
Silvia Biasotti, CNR - IMATI, Italy
BUSTECH 2018 Technical Program Committee
Abdullah Saad AL-Malaise AL-Ghamdi, King Abdulaziz University, Saudi Arabia
Sascha Alda, Bonn-Rhein-Sieg University of Applied Sciences, Germany
Bernardo Almada-Lobo, INESC-TEC | Porto University, Portugal
Seyed-Mehdi-Reza Beheshti, University of New South Wales, Sydney, Australia
Stefanie Betz, Karlsruher Institut für Technologie (KIT), Germany
Silvia Biasotti, CNR – IMATI, Italy
Peter Bollen, Maastricht University, Netherlands
Albertas Caplinskas, Vilnius University, Lithuania
Adela del Río Ortega, Universidad de Sevilla, Spain
Giuseppe A. Di Lucca, University of Sannio - RCOST (Research Center on Software Technology), Italy
Johannes Edler, University of Applied Sciences Upper Austria Campus Hagenberg, Austria
Oscar Ferrandez-Escamez, Nuance Communications Inc., USA
M. Teresa Gómez López, Universidad de Sevilla, Spain
Fernanda Gonzalez-Lopez, Pontificia Universidad Católica de Valparaíso, Chile
Foteini Grivokostopoulou, University of Patras, Greece
Pierre Hadaya, ESG-UQAM, Canada
Rawad Hammad, King's College London, UK
Ioannis Hatzilygeroudis, University of Patras, Greece
Hércules José, Federal University of Rio de Janeiro (UFRJ), Brazil
Hermann Kaindl, Vienna University of Technology, Austria
Thomas Kessel, Baden-Wuerttemberg Cooperative State University Stuttgart, Germany
Petros Kostagiolas, Ionian University, Greece
Franck Le Gall, Easy Global Market, France
Haim Levkowitz, UMass Lowell, USA
Wenbin Li, Easy Global Market, France

Daniel Lübke, innoQ, Switzerland / Leibniz Universität Hannover/FG, Germany
Goreti Marreiros, Engineering Institute - Polytechnic of Porto, Portugal
Malgorzata Pankowska, University of Economics in Katowice, Poland
Andreas Pashalidis, BSI, Belgium
Isidoros Perikos, University of Patras, Greece
Erwin Pesch, University in Siegen, Germany
Elke Pulvermueller, University of Osnabrueck, Germany
Manjeet Rege, University of Saint Thomas, USA
Felix Reher, University of the West of Scotland - School of Engineering & Computing, Paisley, UK
Nina Rizun, Gdansk University of Technology, Poland
Farrukh Saleem, King Abdulaziz University, Saudi Arabia
Jürgen Sauer, Universität Oldenburg, Germany
Adriana Schiopoiu Burlea, University of Craiova, Romania
Patrick Siarry, Université Paris-Est Créteil, France
Mu-Chun Su, National Central University, Taiwan
Henrique Vicente, University of Évora, Portugal
Rüdiger Weißbach, Hamburg University of Applied Sciences (HAW Hamburg), Germany
Hans-Friedrich Witschel, University of Applied Sciences and Arts, Northwestern Switzerland
Anbang Xu, IBM Research - Almaden, USA
Sira Yongchareon, Auckland University of Technology, New Zealand

Copyright Information
For your reference, this is the text governing the copyright release for material published by IARIA.
The copyright release is a transfer of publication rights, which allows IARIA and its partners to drive the
dissemination of the published material. This allows IARIA to give articles increased visibility via
distribution, inclusion in libraries, and arrangements for submission to indexes.
I, the undersigned, declare that the article is original, and that I represent the authors of this article in
the copyright release matters. If this work has been done as work-for-hire, I have obtained all necessary
clearances to execute a copyright release. I hereby irrevocably transfer exclusive copyright for this
material to IARIA. I give IARIA permission or reproduce the work in any media format such as, but not
limited to, print, digital, or electronic. I give IARIA permission to distribute the materials without
restriction to any institutions or individuals. I give IARIA permission to submit the work for inclusion in
article repositories as IARIA sees fit.
I, the undersigned, declare that to the best of my knowledge, the article is does not contain libelous or
otherwise unlawful contents or invading the right of privacy or infringing on a proprietary right.
Following the copyright release, any circulated version of the article must bear the copyright notice and
any header and footer information that IARIA applies to the published article.
IARIA grants royalty-free permission to the authors to disseminate the work, under the above
provisions, for any academic, commercial, or industrial use. IARIA grants royalty-free permission to any
individuals or institutions to make the article available electronically, online, or in print.
IARIA acknowledges that rights to any algorithm, process, procedure, apparatus, or articles of
manufacture remain with the authors and their employers.
I, the undersigned, understand that IARIA will not be liable, in contract, tort (including, without
limitation, negligence), pre-contract or other representations (other than fraudulent
misrepresentations) or otherwise in connection with the publication of my work.
Exception to the above is made for work-for-hire performed while employed by the government. In that
case, copyright to the material remains with the said government. The rightful owners (authors and
government entity) grant unlimited and unrestricted permission to IARIA, IARIA's contractors, and
IARIA's partners to further distribute the work.

Table of Contents
Calculating Test Coverage for BPEL Processes With Process Log Analysis
Daniel Luebke
1
Maximizing Operational Performance in Dyadic Business Relationships: The Moderating Impact of
Interorganizational Information Systems
Pierre Hadaya and Philippe Marchildon
8
Enablers of Business Process Transformation Success in Japan: How Super-ordinate Groups Achieve
Effectiveness?
Kayo Iizuka and Chihiro Suematsu
14
Automated Analysis of Patient Experience Text Mining using a Design Science Research (DSR) Approach
Mohammed Bahja and Manzoor Razaak
21
Requirement-Driven Architecture for Service-Oriented e-Learning Systems
Rawad Hammad
25
Information Systems: From Innovations to Innovation Generators
Philippe Marchildon and Pierre Hadaya
31
Two-level Architecture for Rule-based Business Process Management
Kanana Ezekiel, Vassil Vassilev, and Karim Ouazzane
37
Detecting Adverse Events in an Active Theater of War Using Data Mining Techniques
Jozef Zurada, Donghui Shi, Waldemar Karwowski, Jian Guan, and Erman Cakit
43
Revenue Optimization of Telecom Marketing Campaigns for Prepaid Customers
Maurus Riedweg, Pavol Svaba, and Gwendolin Wilke
45
Powered by TCPDF (www.tcpdf.org)

Calculating Test Coverage for BPEL Processes With Process Log Analysis
Daniel L¨ubke
Leibniz Universit¨at Hannover
FG Software Engineering
Welfengarten 1, D-30167 Hannover, Germany
Email: daniel.luebke@inf.uni-hannover.de
Abstract—Today more and more business processes are digitized
by implementing them in specialized workﬂow languages like
the Business Process Execution Language (BPEL) or Business
Process Model and Notation (BPMN 2.0), which orchestrate
services along the process ﬂow. Because these process models are
software artefacts of critical importance to the functioning of the
organization, high quality and reliability of these processes are
mandatory. Testing therefore becomes an important activity in
the development process. Test Coverage Metrics have long been
used in software development projects to assess test quality and
test progress. Current approaches to test coverage calculation for
BPEL either relies on instrumentation, which is slow, or is limited
to vendor-provided unit test frameworks, in which all dependent
services are mocked (unit tests), which limits the applicability of
such approaches. Our approach relies on analyzing process event
logs that are written during process execution. This approach
does not require additional infrastructure and can be used in
unit tests, as well as in system and integration tests. We found
that our approach for measuring test coverage is not only more
ﬂexible but also faster than an instrumentation-based approach.
Keywords–Test Coverage; Process Mining; BPEL; Event Log.
I.
INTRODUCTION
Executable Business Processes, implemented with WS-
BPEL or BPMN2, are used to automate business processes in
large companies. They are software artefacts and can contain
complex orchestration logic. With the increasing demand for
fully digitized solutions, it is likely that more and more
business processes are being implemented in these or similar
orchestration languages.
Because business processes and as such their software
implementations are very critical to the functioning and per-
formance of organizations, it is mandatory to do good quality
assurance in order to avoid costly problems in production [1]. It
has been shown by Piwowarski et. al [2] that a) test coverage
measurements are deemed beneﬁcial by testers, although b)
they are rarely applied because of being difﬁcult to use, and c)
that higher coverage values lead to more defects being found.
These ﬁndings are supported by Horgan et al. [3], who linked
data-ﬂow testing metrics to reliability, and Braind et al. [4],
who simulated the impact of higher test coverage. Furthermore,
Malaiya et al. [5] and Cai & Lyu [6] have developed prediction
models that can link test coverage with test effort and software
reliability.
Quality Assurance, and thus test coverage calculation, are
an ongoing activity because executable processes will evolve
over time [7]. One way for continously measuring test quality
is to measure test coverage as part of all ongoing testing
activities. Test Coverage then serves as measurement of test
data adequacy [8].
While approaches applicable for unit testing executable
processes have been proposed by academia (e.g., [9], [10])
and developed by vendors for their respective process engines,
there is no practical way to efﬁciently calculate test coverage
for tests that are not controlled by a process testing framework.
Also, approaches relying on instrumentation create signiﬁcant
additional overhead by a factor larger than 2.0 compared with
the “plain” test case execution times [11] – which is far more
than instrumentation approaches for “normal” programming
languages, e.g., Java, impose.
For improving the guidance of quality assurance in soft-
ware projects developing executable processes, an approach
is required that can be used in non-unit test scenarios and is
ideally faster and easy to set up. Within this paper, we propose
a new approach based on analyzing process event logs, which
are written by process engines regardless of whether testing
frameworks are used or not. Our research goal is to calculate
test coverage metrics more ﬂexible and faster by leveraging
processes’ events logs.
The paper is structured as follows: First, the process
modeling language BPEL is shortly explained in Section II
before related work is presented in Section III. Our approach
for mining test coverage metrics is described in detail in
Section IV. For validating our approach, we conducted an
experiment, which is presented in Section V. Finally, we
conclude the paper and give an outlook on future work.
II.
BACKGROUND ON BPEL
BPEL (short for WS-BPEL; Web Services Business Pro-
cess Execution Language) is an OASIS standard that deﬁnes
a modeling language for developing executable business pro-
cesses by orchestrating Web services.
BPEL Models consist of Activities, which are divided
into Basic Activities and Structured Activities. Basic Activities
carry out actual work, e.g., performing data transformations
or calling a service, while Structured Activities are controlling
the process-ﬂow, e.g., conditional branching, loops, etc.
Important Basic Activities include the invoke activity
(which calls Web services), the assign activity (which performs
data transformations), and the receive and reply activities
(which offer others to call a process via service interfaces). Im-
portant Structured Activities are the if, while, repeatUntil, and
1
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Figure 1. Sample BPEL Process for processing an Order.
forEach activities, which offer the same control-ﬂow structures
like their pendants in general purpose programming languages,
and the ﬂow activity, which allows process designers to build a
graph-based model including parallel execution. For building
the graph, BPEL offers links that can also carry conditions for
modelling conditional branches.
For handling error conditions and scoped messages, BPEL
provides different kinds of Handlers: Fault Handlers are
comparable to try/catch constructs: Whenever a SOAP Fault
is returned by an invoked service or is thrown by the pro-
cess itself, the Process Engine searches for deﬁned Fault
Handlers. These may trigger Compensation Handlers, which
can undo already executed operations. For receiving events
asynchronously outside the main process ﬂow, Event Handlers
can be deﬁned. These come in two ﬂavors: onEvent Handlers
for receiving SOAP messages, and onAlarm Handlers for
reacting on (possibly reoccurring) times and time intervals.
BPEL does not deﬁne a graphical representation, like, e.g.,
BPMN2 does, but standardizes the XML format in which it is
saved. Vendors have developed their own graphical represen-
tation. Within this paper we use the notation used in Eclipse’s
BPEL Designer. A process that will be used for examples
in this paper is shown in Figure 1: A customer places an
order (“receive input”). A check is made, whether the customer
has VIP status or not. In case of a VIP customer, points are
credited to the customer’s account (“SavePointsEarned”). In
both cases appropriate response message to the customer are
prepared (“PrepareReplyFor...”), which is then sent back to
the customer (“reply”).
BPEL processes are deployed to a Process Engine, which
has the responsibility for executing process instances and
managing all aspects around process versioning, persistence,
etc. The amount of data, which is persisted during process
execution, is vendor-dependent and can be conﬁgured during
the deployment of a process model in most engines.
BPEL has been designed to be extensible. Many extensions
by both standard committees and vendors have been made.
For example, BPEL4People allows to interact not only with
services but also with humans during process execution.
III.
RELATED WORK
With the rise of BPEL, testing of these critical software
artefacts became subject of many research projects. For ex-
ample, Li et al. (BPEL4WS Unit Testing Framework [9]),
Mayer & L¨ubke (BPELUnit [10]), and Dong et al. (Petri Net
Approach to BPEL Testing [12]) published their ideas.
The BPELUnit framework was later extended by L¨ubke
et al. [11] with test coverage measurement support. First, the
metrics needed to be deﬁned, which is not as straightforward as
for other languages due to BPEL’s different mechanisms for
deﬁning the process-ﬂow. Consequently, three metrics were
deﬁned: Activity Coverage, Handler Coverage, and Link
Coverage. Coverage Measurement was done by instrumenting
the BPEL process: For tracing the execution, the process
is changed prior to deployment. Additional service calls are
inserted for every activity. The service calls send the current
position (“Markers”) to the test framework. Because of this the
test framework knows which activities have been executed in
the test run. However, the test framework needs to run while
the instrumented processes are executed, which makes its use
limited to automatic tests only. Also, the overhead introduced
by many new service calls is considerable: The reported
overhead is more than 100%, i.e., the test execution times are
more than doubled. This is because every execution trace point
needs to be send out of the process via a service call, which
requires XML serialization and involves the network stack.
This also makes the BPEL process much larger: The number
of basic activities tripples for instrumenting all measurement
points for calculating activity coverage alone. One advantage
of the approach is that is only slightly dependent on the
Process Engine being used: The changes to the BPEL process
are completely standards-compliant. Only the new service for
collecting the Markers needs to be added to the engine-speciﬁc
deployment descriptor.
Process Engine vendors have also developed their own pro-
prietary solutions: Test Cases are developed in the development
environment of the process engine and can be executed from
there or on a server. All services are mocked and the test
frameworks simply inject predeﬁned SOAP messages. Such
test frameworks use a striped-down version of the process
engine. This results in a mixture between simulation and test:
The process engine uses the same logic but not all parts of
its code are triggered because some features are disabled.
Also, there is no possiblity of calling “real” services instead
of mocks. While test coverage calculation is very fast, because
the algorithms have access to internal engine data, its use is
limited to unit test scenarios only. Examples of such vendor-
provided test frameworks are Informatica’s BUnit [13] and
Oracle’s BPELTest [14].
All in all, there is currently no approach available for BPEL
processes that can be used to measure test coverage on code
level with acceptable performance and the ability to be used in
conjunction with manual tests and integration & system tests.
IV.
TEST COVERAGE MINING
This section will present the different steps that are per-
formed for analyzing the process log in order to calculate test
2
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

coverage.
A. Metric Calculation Process
For calculating test coverage, we use process mining tech-
niques. Process Mining is concerned to build “a strong relation
between a process model and ‘reality’captured in the form of
an event log” [15, p. 41]. By having the BPEL process model
and all test cases available as event logs from the process
engine, we are able to “replay” the event logs generated from
the tests on top of the BPEL process model. Out of the many
possible motivations to do a replay, our goal is to extend our
model with frequency information [15, p. 43].
Accordingly, our approach is divided into three sub-steps,
which are described in the following sections:
1)
Build the BPEL Process Model Syntax Tree from its
XML representation (BPEL Analysis),
2)
Fetch the event log from the Process Engine (Data
Gathering),
3)
Replay the event logs on top of the BPEL Process
and calculate coverage metrics (Mining).
B. BPEL Analysis
Within this step, the BPEL XML representation is read
and the control-ﬂow graph is being constructed as described
by the block-based structured activities. For example, activities
contained in sequence activity are chained together by control-
ﬂow links. The construction of the control-ﬂow graph is the
same as for the instrumentation approach to measuring BPEL
test coverage [11] and thus takes the same time to build.
The BPEL Models are accessible via the process engine’s
repository and can be extracted as part of the coverage mining.
C. Data Gathering
The case study project, which we could analyze, uses Infor-
matica ActiveVOS [13]. ActiveVOS is a process engine fully
compliant with the BPEL 2.0 and BPEL4People standards
and stores all data (process models, process instances, process
logs, . . . ) in a relation database. This allowed us to access
and analyze the available data that can be mined for test
coverage metrics. For different persistence settings ActiveVOS
stores different lifecycle events for every BPEL activity, which
include ready to execute, executing, completed, faulting and
will not execute. In addition, there are two more event types
for links (edges for graph-based modeling): link evaluated
to true and link evaluated to false. Besides the event time,
the event timestamp, the corresponding process instance, an
internal activity or link ID is logged.
This means that all necessary data is available in order
to reconstruct the process-ﬂow and thereby calculating the
test coverage metrics: For calculating activity coverage, all
completed, faulting and link evaluated events need to be
fetched for a given test run. All other event types can be
ignored, which allows to use all engine settings except for
“no logging.” The underlying conceptual data model, as it is
implemented in the ActiveVOS engine, is shown in Figure 2.
Figure 2. Conceptual Data Model of the Process Engine being used.
D. Replay & Metric Calculation
Test Coverage can be calculated with the data extracted in
the previous steps. At ﬁrst, all activities and conditional links
in the syntax tree are marked as not executed. In the second
step, all events are being applied to the syntax tree and all
activities that have a corresponding completed or faulting event
are marked as being executed. Also, conditional BPEL links
for graph-based modeling are marked. However, every link
can carry two different markers: one if the condition was true
and another if the condition was false. Because links without
a condition are excluded from the coverage metric, they are
ignored from further analysis.
During this phase, loop activities can be marked as exe-
cuted twice for calculating the branch coverage in later stages.
This happens for most loops by setting this marker, if the loop
is executed twice after each other. The only exception is the
parallel forEach loop, in which the activity ID contains the
number of the currently executed parallel branch. If a counter
larger than one is encountered, the forEach activity is marked
as executed twice.
The main problem in this step was to link events and
activity nodes in the BPEL model. Because the activity IDs
in the events are generated by the process engine and are not
part of the BPEL model, it is necessary to ﬁrst resolve the
proprietary IDs to the activities. The generated activity IDs are
in an XPath-like structure, which closely resembles the XML
structure of the BPEL model. However, some cleaning and
re-writing of these IDs is necessary, because they sometimes
reference internal data structures and do not directly map to the
BPEL activities. After re-writing the IDs, they can be converted
to XPath expressions that directly point to the BPEL activity
being executed. This step is highly speciﬁc to the process
engine being used and needed reverse-engineering the format
and construction rules for the proprietary IDs.
After all events have been applied to the syntax tree, the
coverage metrics can be calculated. The easiest test coverage
metric to compute is the Activity Coverage CA metric: The
syntax tree is traversed and all activities are counted which
are marked (Am) and which are not marked (Au) as shown in
equation 1.
CA :=
|Am|
|Am| + |Au|
(1)
3
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Similarily, Handler Coverage CH can be calculated by
searching the syntax tree for handlers that have been success-
fully executed.
Calculating Link Coverage CL by process mining is easier
than with instrumentation: In order to detect the different
conditions on links, instrumentation needs to insert many new
links and activities. However, with process mining, dedicated
events are triggered whenever a link condition has been
evaluated.
E. Example
To illustrate the replay of the event log on top of the process
model we assume two test cases for the example BPEL process
as shown in Figure 3. The ﬁrst test case tests the VIP Customer.
As can be seen by the trace, the completion events are
differently ordered than the deﬁnition in the BPEL process
model: structured activities like a sequence or an if are
completed after all their child activities have been completed.
The replay algorithm needs to take this into account when
replaying the event log against the process model.
Taking the event log for the ﬁrst test case and replaying
it on top of the BPEL process model yields the markings as
illustrated in the center of Figure 3. Replaying the second test
case yields the markings as shown on the right hand side of
the same ﬁgure. With these two test cases, all basic activities
are covered.
F. Comparison to Instrumentation
When we compare our approach to instrumentation (see
Figure 4), there are many parts of the calculation that are
similar or even the same. Instrumentation would initially load
the BPEL process model and construct a syntax tree. However,
it would then change the process model by introducing service
calls that signal the internal process state to the test framework.
During run-time these service calls are equivalent to log events.
These events are replayed on the process model in both
approaches. Thus, the main differences are that
•
instrumentation needs to build the syntax tree prior to
the test run and a service receiving all markers must be
active during the whole test while process mining can
perform all activities after the test run is completed,
•
instrumentation needs to change the BPEL process
model while process mining does not, and
•
the events are collected in the instrumentation ap-
proach by signaling service calls instead of extracting
all event logs with one database query like in our
approach. For a test run, the instrumentation approach
requires at least as many service calls for signaling
the process state as the number of executed basic
activities depending on the coverage metrics that shall
be calculated.
Due to these structural differences, we expect our approach
to be overall faster than the instrumentation approach: Mak-
ing and answering many ﬁne-grained service calls is time-
consuming as outlined above. Being able to fetch all events
from the Process Engine’s event log at once should yield better
performance. In addition, our approach does not slow down
execution times of the executable processes itself because they
behave as they are implemented and are not changed by an
instrumentation process and their run-time behavior is not
altered by introducing probes. This means that no additional
error sources (e.g., by defects in the instrumentation) or
different behavior (e.g., in parallel activities by instrumentation
code) can occur.
G. Sample Implementation
We implemented a tool that performs the outlined test
coverage calculation. The tool connects to the database of
the process engine and extracts all relevant information. After
the tests have been completed, the tool extracts the events
for all newly created process instances. It expects that the
tested processes have been conﬁgured appropriately to at least
store the events for successfully and unsuccessfully completed
activities.
The implementation is highly dependent on the process
engine being used. The available process log data and its
format is deﬁned by vendors because it is not speciﬁed in any
standard. As outlined in the previous section, a post-processing
of the event log data is needed in order to properly resolve the
referenced activities.
V.
EXPERIMENT
In order to evaluate our approach, we conducted a small
experiment that is described in this section.
A. Experiment Description & Design
For evaluating the practical applicability and the perfor-
mance implications of our approach, we want to research the
following two research questions:
RQ1:
What is the associated overhead for mining pro-
cess coverage?
RQ2:
Is the associated overhead for mining process cov-
erage less than for instrumentation-based coverage
calculation?
For this we deﬁne a two factor/two treatments with-in
group experiment design: The ﬁrst independent variable is the
coverage method (Instrumentation vs. Mining) and the second
is the test suite size. Our dependent variable is the execution
time of the measured test suites.
As subjects we used 4 BPEL processes, for which we could
automatically – and thus unbiased – generated test suites of
different sizes by using facet classiﬁcation trees [16]. Two
processes are taken from Schnelle [17] and two processes
are taken from Terravis, which is an industrial project, which
develops and runs a process-integration platform between
land registers, notaries, banks and other parties across whole
Switzerland [18].
B. Data Collection
For running the experiment we set up a process engine on
a dedicated virtual server together with the required infrastruc-
ture, e.g., the tools for measuring test coverage.
4
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Figure 3. Event Logs for two Test Cases (Left), ﬁrst Test Case replayed on BPEL Model (center) and both Test Cases replayed on BPEL Model (right).
Figure 4. Comparison of Instrumentation and Mining.
Because the original BPELUnit tool for measuring test
coverage [11] did not support vendor extensions and the
deployment artefacts of the used process engine, we needed
to re-implement the instrumentation tool with full support for
all features, which are used by the industry project.
We measured the execution times by following the de-
scribed process:
1)
For every BPEL process, generate the test suites of
different sizes,
2)
For every test suite and for every calculation method,
run 10x:
a)
Instrument the deployment unit (if necessary)
b)
Deploy the process,
c)
Run test suite,
d)
Wait for process log and calculate coverage
(if necessary)
For every process, we generated random test suites with the
sizes n ∈ {1, 5, 10, 25, 50, 75, 100} if possible. The processes
by Schnelle had only a smaller number of possible test cases,
thereby the experiment could only use test cases with max. 25
and respectively 50 test cases.
We executed all test suites ten times in order to build mean
values for all time measurements. All in all, 460 test suites runs
were made for each coverage measurement method.
For all our test executions we used a virtual machine with
2 virtual CPUs and 4 GiByte of RAM running on Kubuntu
with Informatica ActiveVOS 9.2 and MySQL.
C. Results
The mean execution times of our measurements (calculated
in milliseconds) are shown in Table ??. T or S indicate the
process set (Terravis or Schnelle), 1 or 2 indicate which
process, and I or L indicate the coverage measurement method
(instrumentation or log analysis).
The mean value for the smallest test suites with only one
test case are smaller for instrumentation than for log analysis.
For all other chosen test suite sizes, log analysis performs
faster.
TABLE I. TOTAL MEAN EXECUTION TIME (ms)
#TC
T1-I
T1-L
T2-I
T2-L
S1-I
S1-L
S2-I
S2-L
1
8532
7718
5138
4915
4544
3825
4029
4140
5
16240
10958
11736
6533
6646
4492
6553
4249
10
19523
12446
14356
7090
10567
5942
9737
5586
25
38262
19309
34086
11760
15856
6688
18290
7675
50
62288
27264
62589
17736
29584
9799
-
-
100
120720
48628
118413
28318
-
-
-
-
By subtracting the normal execution time of a test suite
we derive the absolute overhead (calculated in ms) as shown
in Table II. In general, the numbers for log analysis are much
lower than for instrumentation and do not increase that much.
The highest overhead for log analysis is 6519ms in contrast for
up to 94287ms for instrumentation. The overhead is the largest
for the ﬁrst Terravis process (T1) for process log analysis while
it is the largest for instrumentation with the second Terravis
process (T2).
TABLE II. ABSOLUTE OVERHEAD OF COVERAGE CALCULATION
#TC
T1-I
T1-L
T2-I
T2-L
S1-I
S1-L
S2-I
S2-L
1
2565
1751
2054
1830
1872
1153
1549
1660
5
7288
2006
7124
1920
3562
1408
3562
1258
10
9447
2370
9100
1834
6722
2098
5792
1641
25
22580
3628
24808
2482
10741
1574
12672
2057
50
39707
4683
48003
3150
21637
1852
-
-
100
78611
6519
94287
4193
-
-
-
-
We calculated the relative overhead for the processes by
dividing the absolute overhead by the normal test suite execu-
tion time as shown in Table III. While for larger test suites the
relative overhead increases with instrumentation, it decreases
for log analysis. Relative overhead of instrumentation ranges
between 43% and 391%, while it ranges bwetween 16% and
68% for log analysis.
The
measurements
grouped
by
coverage
calculation
method and process for all test suite runs are shown in Fig-
5
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

TABLE III. RELATIVE OVERHEAD OF COVERAGE CALCULATION
#TC
T1-I
T1-L
T2-I
T2-L
S1-I
S1-L
S2-I
S2-L
1
0.43
0.30
0.67
0.59
0.71
0.43
0.64
0.68
5
0.78
0.23
1.55
0.42
1.16
0.46
1.19
0.42
10
0.94
0.24
1.73
0.35
1.75
0.55
1.48
0.41
25
1.44
0.23
2.68
0.27
2.10
0.31
2.26
0.37
50
1.76
0.21
3.29
0.22
2.73
0.23
-
-
100
1.93
0.16
3.91
0.17
-
-
-
-
ure 5. Test suites with more test cases expectedly take longer
to execute. Log analysis is usually faster than instrumentation.
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
GG
G
GG
G
GG
Instrumentation
Log Analysis
Terravis 1
Terravis 2
Schnelle 1
Schnelle 2
1
5
10
25
50
75
100
1
5
10
25
50
75
100
0
50
100
0
50
100
0
50
100
0
50
100
Execution Time (s)
Figure 5. Overall Execution Times
The absolute and relative overhead of both coverage cal-
culation methods are shown in Figure 6 and can be compared
directly. Different colors indicate different processes. The
absolute overhead shows clusters of overhead times that are
associated with a test suite. As can be seen the values for both
the absolute – and following from that – the relative overhead
are higher most of the time for the instrumentation approach.
Only in 13 of 460 measurements instrumentation was faster
than log analysis. All of those measurements are concerned
with test suites with only one test case.
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
2000
4000
6000
0
25000
50000
75000
100000
Instrumentation
Log Analysis
Absolute Overhead
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.25
0.50
0.75
1.00
1
2
3
4
Instrumentation
Log Analysis
Relative Overhead
Figure 6. Coverage Measurement Overhead
In order to answer RQ2 we performed a paired, two-sided
Wilcoxon hypothesis test with the null hypothesis H0 being
TABLE IV. P-VALUES (TWO-SIDED, PAIRED WILCOXON TEST) FOR
TEST SUITES WITH n TEST CASES AND FOR ALL TEST SUITES
Test Suite Size
p-Value
1
0.087 69
5
1.25 × 10−6 (***)
10
5.154 × 10−10 (***)
25
2.918 × 10−10 (***)
50
1.691 × 10−17 (***)
75
1.451 × 10−11 (***)
100
1.451 × 10−11 (***)
All
5.034 × 10−12 (***)
that no difference exists in the test suite execution times when
using instrumentation or log analysis: Over all executed test
suites, p = 5.034 × 10−12. However, we have seen that at
least test suites with only one test case behaves differently
from other test suites. Therefore, we blocked for the test
suite size and derived the p-values as shown in Table IV.
Values marked with (***) are less than 0.001 and thus highly
statistical signiﬁcant.
D. Interpretation
1) RQ1: Overhead of Log Analysis: Our measurements for
the overhead of log analysisshow demonstrate that the absolute
overhead increases and the relative overhead decreases with
more test cases. The maximum absolute overhead of 6.5s for
100 test cases the performance penalty is little. This means
that measuring approx. 92 test suites of such size would only
impose a ten minute overhead (e.g., during nightly builds).
2) RQ2: Overhead of Log Analysis compared to Instru-
mentation: Our measurements clearly show that log analysis
is signiﬁcantly faster than instrumentation for non-trivial test
suites, i.e. test suites with more than one test case. While the
relative overhead of instrumentation increases with more test
cases and reaches 391% (i.e., nearly quintuples the test suite
execution time), log analysis imposes 68% overhead in the
worst case of a small test suite but decreases to 16% for large
test suites. For a further interpretation typical test case sizes
in industry are needed in order to evaluate typical overhead
ranges. If we suppose that a typical test suite consists of
25 test cases the relative overhead is between 24% and 55%
for log analysis while it already is between 144% and 268%
for instrumentation. This means that for any non-trivial test
suite, log analysis brings a huge performance beneﬁt when
measuring test coverage.
E. Threats to Validity
As with every empirical research there are associated
threats to validity. Because we could only use four BPEL
processes for our experiment, the question of generalizability
arises.
Since we research technical effects only, the ﬁndings
should be generalizable to all BPEL processes that execute
a minimum threshold number of activities or test cases. The
p-value for rejecting the null hypothesis and accepting that log
analysis is faster than instrumentation for a test suite size t ≥ 5
is so low that we are conﬁdent that replications will ﬁnd the
same results.
6
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

As long as the process engine stores all relevant events that
are required for calculating the test coverage metrics, the log
analysis can proceed. To our knowledge, all BPEL engines are
able to write event logs that contain the required event types.
For every newly supported BPEL engine, a new interpreter of
these events needs to be developed. The analysis and replay
components can be reused. However, as part of our study
we also found that this is also true for instrumentation tools:
While BPEL is standardized, its extensions and the deployment
artefacts are not.
The presented numbers are only applicable to automated
unit tests. While we think it is safe to generalize the absolute
overhead to other test scenarios, we expect that the relative
numbers to be different: Manual tests take longer for executing
the same number of processes, because user interactions take
time, which makes the process duration longer. Thus, we do
not think that the relative overhead can be generalized to other
test types.
VI.
CONCLUSION & FUTURE WORK
Within this paper we presented a new approach to mine
process event logs – which are usually already written when
using a process execution engine – to calculate test coverage
metrics of BPEL processes. Our new approach shows clear
performance advantages over the instrumentation approach.
Furthermore, the process mining approach can be used in
other scenarios than the instrumentation approach: Because all
activities for mining the test coverage are performed after the
tests are run, it does not matter how the tests are run and when
they were run. In contrast, coverage calculation needs a marker
collection service running the whole time, which in practice is
mostly only feasible during unit tests. Mining the process logs
is completely independent of any test automation and can be
used for automatic unit tests, automatic integration tests but
also manual integration and system tests. The only drawback
is, however, that the Process Engine needs to be conﬁgured to
write the event log for all measured processes.
Although we have implemented test coverage mining for
BPEL processes, the approach can be applied to other exe-
cutable process languages as well: Process engine architectures
are the same, e.g., BPMN 2.0 as the successor to BPEL deﬁnes
other activities and is completely graph based. However, pro-
cess engines executing BPMN 2.0 are also logging events for
executed activities which can be replayed on top of BPMN 2.0
process models. Writing the process mining algorithm should
be even simpler, because BPMN 2.0 deﬁnes process-wide
unique identiﬁers for activities that are hopefully contained
in the event log making reverse-engineering of vendor-speciﬁc
identiﬁers obsolete.
While we have ﬁnished our research implementation, we
want to optimize the implementation further and contribute it
into the BPELUnit test framework. We hope to ﬁnd further
industry BPEL processes to apply our approach to and have
a larger data set for evaluating performance – especially the
use of other process engines is interesting and see whether all
necessary event data is generally available.
Being able to calculate test coverage for non-unit tests also
allows further research into executable process test methods:
For example, experiments on the inﬂuence of different system
testing approaches on test coverage.
REFERENCES
[1]
D. L¨ubke, Test and Analysis of Service-Oriented Systems.
Springer,
2007, ch. Unit Testing BPEL Compositions, pp. 149–171.
[2]
P. Piwowarski, M. Ohba, and J. Caruso, “Coverage measurement
experience during function test,” in Proceedings of the 15th International
Conference on Software Engineering, ser. ICSE ’93.
Los Alamitos,
CA, USA: IEEE Computer Society Press, 1993, pp. 287–301.
[3]
J. R. Horgan, S. London, and M. R. Lyu, “Achieving software quality
with testing coverage measures,” Computer, vol. 27, no. 9, Sept 1994,
pp. 60–69.
[4]
L. C. Briand, Y. Labiche, and Y. Wang, “Using simulation to empirically
investigate test coverage criteria based on statechart,” in Proceedings.
26th International Conference on Software Engineering, May 2004, pp.
86–95.
[5]
Y. K. Malaiya, M. N. Li, J. M. Bieman, and R. Karcich, “Software
reliability growth with test coverage,” IEEE Transactions on Reliability,
vol. 51, no. 4, Dec 2002, pp. 420–426.
[6]
X. Cai and M. R. Lyu, “Software reliability modeling with test cover-
age: Experimentation and measurement with a fault-tolerant software
project,” in The 18th IEEE International Symposium on Software
Reliability (ISSRE ’07), Nov 2007, pp. 17–26.
[7]
D. L¨ubke, “Using Metric Time Lines for Identifying Architecture Short-
comings in Process Execution Architectures,” in Software Architecture
and Metrics (SAM), 2015 IEEE/ACM 2nd International Workshop on.
IEEE, 2015, pp. 55–58.
[8]
H. Zhu, P. A. V. Hall, and J. H. R. May, “Software unit test coverage and
adequacy,” ACM Comput. Surv., vol. 29, no. 4, Dec. 1997, pp. 366–427.
[Online]. Available: http://doi.acm.org/10.1145/267580.267590
[9]
Z. Li, W. Sun, Z. B. Jiang, and X. Zhang, “BPEL4WS Unit Testing:
Framework and Implementation,” in ICWS ’05: Proceedings of the
IEEE International Conference on Web Services (ICWS’05).
Wash-
ington, DC, USA: IEEE Computer Society, 2005, pp. 103–110.
[10]
P. Mayer and D. L¨ubke, “Towards a BPEL unit testing framework,” in
TAV-WEB ’06: Proceedings of the 2006 workshop on Testing, analysis,
and veriﬁcation of web services and applications. New York, NY, USA:
ACM Press, 2006, pp. 33–42.
[11]
D. L¨ubke, L. Singer, and A. Salnikow, “Calculating BPEL Test Cov-
erage through Instrumentation,” in Workshop on Automated Software
Testing (AST 2009), ICSE 2009, 2009, pp. 115–122.
[12]
W. l. Dong, H. Yu, and Y. b. Zhang, “Testing bpel-based web ser-
vice composition using high-level petri nets,” in 2006 10th IEEE
International Enterprise Distributed Object Computing Conference
(EDOC’06), Oct 2006, pp. 441–444.
[13]
Informatica.
Bpel
unit
testing.
[Online].
Avail-
able:
http://infocenter.activevos.com/infocenter/ActiveVOS/v92/index.
jsp?topic=/com.activee.bpep.doc/html/UG21.html (2016)
[14]
Oracle. Oracle bpel process manager developer’s guide: Testing bpel
processes. [Online]. Available: https://docs.oracle.com/cd/E11036 01/
integrate.1013/b28981/testsuite.htm (2007)
[15]
W. van der Aalst, Process Mining – Data Science in Action.
Springer,
2016.
[16]
T. Schnelle and D. L¨ubke, “Towards the generation of test cases for ex-
ecutable business processes from classiﬁcation trees,” in Proceedings of
the 9th Central European Workshop on Services and their Composition
(ZEUS) 2017, 2017, pp. 15–22.
[17]
T.
Schnelle,
“Generierung
von
bpelunit-testsuites
aus
klassiﬁka-
tionsb¨aumen,” Master’s thesis, Leibniz Universit¨at Hannover, Fachge-
biet Software Engineering, 2016.
[18]
W. Berli, D. L¨ubke, and W. M¨ockli, “Terravis – large scale business pro-
cess integration between public and private partners,” in Lecture Notes
in Informatics (LNI), Proceedings INFORMATIK 2014, E. Pl¨odereder,
L. Grunske, E. Schneider, and D. Ull, Eds., vol. P-232, Gesellschaft f¨ur
Informatik e.V. Gesellschaft f¨ur Informatik e.V., 2014, pp. 1075–1090.
7
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

The Moderating Role of Interorganizational Information Systems in Maximizing 
the Operational Performance of Dyadic Business Relationships: a Conceptual 
Model 
 
Pierre Hadaya and Philippe Marchildon 
Department of Management and Technology 
ESG UQAM 
Montréal, Canada 
e-mail: marchildon.philippe@courrier.uqam.ca 
hadaya.pierre@uqam.ca 
 
 
Abstract—The objective of this study is twofold: first, to 
examine how the structure of a buyer-supplier relationship 
impacts the operational performance of the supplier involved 
in the dyadic relationship and second, to test the moderating 
impact of interorganizational information systems (IOS) usage 
(assessed through the dimensions of volume, diversity and 
depth) on the relationship between the structure of a buyer-
supplier relationship and the operational performance of the 
supplier involved in the dyadic relationship. To do so, the 
marketing and IOS literatures are reviewed to propose a 
model and seven research hypotheses. Then, the methodology 
we intend to use to test our conceptual framework is explained. 
Lastly, the anticipated theoretical contributions and practical 
implications of the study are discussed. 
Keywords-dyadic business relationship strucutre; inter-
organizational information systems; operational performance. 
I. 
INTRODUCTION 
Within the field of marketing, the structure of dyadic 
business relationships and its impact on the performance of 
each partnering firms is a key concern for practitioners and 
academics alike [1][2]. The structure of a dyadic business 
relationship can be defined as the patterned or regularized 
aspects of exchange between two business counterparts [3]. 
To date, most studies on this specific concern have focused 
on the political impact (i.e., cooperation, satisfaction, trust 
and commitment) while neglecting the economic impact 
(i.e., cost, speed, quality and reliability) of dyadic business 
relationship structures [4]. Among the few researchers that 
have empirically investigated the economic impact of the 
structure of buyer-supplier relationships, Bonner and 
Calantone [5] have shown that certain structures provide 
economic value to buyers. To date, however, no study has 
yet to investigate whether certain dyadic business 
relationships structures also provide economic value to 
suppliers. To partially address this gap in the marketing 
literature, the first objective of this study is to complement 
Bonner and Calantone [5] pioneering work by examining 
how the structure of a buyer-supplier relationship impacts the 
operational performance of the supplier involved in the 
dyadic relationship. 
Within the field of information systems (IS), numerous 
authors 
have 
demonstrated 
that 
interorganizational 
information systems (IOS) – defined as computer networks 
that support information exchanges across organizational 
boundaries [6] – can add economic value to business 
relationships. For example, Saeed et al. [7] demonstrated that 
IOS can provide either sourcing leverage or process 
efficiency, depending on IOS functionalities used. In 
addition, other IS researchers have discussed the possible 
symbiosis between IOS usage and the structure of business 
relationships [2][6][8][9][10]. For example, Malone et al. [9] 
relied on the transaction cost paradigm in their prediction 
that the evolution of computer-aided buying and selling 
would disrupt conventional marketing and distribution 
patterns. 
According 
to 
these 
authors, 
by 
reducing 
coordination costs, IOS would increase organizations’ 
reliance on markets rather than hierarchies. A couple of years 
later, Clemons et al. [8] refuted Malone et al.’s [9] theory by 
demonstrating that the transaction economies of scale, 
learning curve effects, and other factors related to IOS use 
favor a move toward long-term relationships with a smaller 
set of suppliers (i.e., the “move to the middle” hypothesis). 
Yet, while conceptually very convincing, no study has 
empirically validated these propositions. To partially address 
this gap in the IS literature, the second objective of this study 
is to complement previous IOS studies by testing the 
moderating impact IOS usage on the relationship between 
the structure of a buyer-supplier relationship and the 
operational performance of the supplier involved in the 
dyadic relationship. 
The rest of this article is organized as follows. First, in 
Section 2, the literatures on the structure of business 
relationships and IOS usage are reviewed. Based on these 
theoretical underpinnings we then present our research 
model and related hypotheses in Section 3. This is followed, 
in Section 4, by a discussion of the methodology that will be 
used to validate our research model. Lastly, Section 5 
concludes the article by presenting the anticipated theoretical 
8
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

contributions and practical implications of the study, as well 
as its limits and future research avenues. 
II. 
THEORETICAL BACKGROUND 
Taking into account the objective of this study, this 
section reviews the literatures on the structure of business 
relationships and IOS usage to propose a sound theoretical 
background. 
A. Structure of a Business Relationship 
Traditionally, conceptualizations of the structure of a 
business relationship have been anchored on two different 
approaches: an economic approach and a behavioral 
approach [10]. By applying microeconomic theory and 
industrial organization analysis, the economic approach is 
essentially 
“efficiency” 
oriented, 
focusing 
on 
costs, 
functional 
differentiation 
and 
channel 
design 
[10]. 
Williamson [11] is a key contributor to this approach. He 
asserted that firms organize their exchanges in order to 
minimize transaction costs and that the ensuing transaction 
forms may vary according to the degree to which exchange 
partners maintain decision-making autonomy. Discrete 
transactions are located at one extreme while highly 
centralized hierarchical transactions are at the other. Hybrid 
transactions, such as joint ventures and alliances, are located 
in between. His pioneering work was among many studies 
that rely on microeconomic factors to explain the structure of 
business relationships. On the other hand, the behavioral 
approach, which is anchored in social psychology and 
organization theory, is essentially “socially” oriented, and 
focuses on power and conflict phenomena [10]. Macneil [12] 
is a key contributor to this approach. Based on a set of 
contracting norms, he defined the concept of relational 
exchange on a continuum ranging from discrete to relational. 
Subsequently, Kaufmann and Stern [13] made an initial 
attempt to comprehensively operationalize contracting 
norms, which led the way to more empirical investigations to 
assess the structure of business relationships according to 
such norms.  
More recently, several authors have combined the 
economic and behavioral approaches to examine the 
structure of business relationships, its antecedents and its 
outcomes [1][10][14][15]. These authors argue that both 
approaches should be used simultaneously, as interaction 
effects between economic and behavioral elements may 
influence the outcome of the exchange [14]. Stern and 
Reve [10] were the first to adopt this combined approach to 
study the structure of business relationships. Later on, 
Boyle et al. [1] proposed a framework anchored on both 
economic and behavioral elements to demonstrate that 
communication strategies, captured through communication 
frequency and content, differ significantly from one channel 
configuration to the next. 
Out of the numerous studies that used the economic 
approach, the behavioral approach or a combination of the 
two, three dominant paradigms have emerged to characterize 
the structure of business relationships: transaction cost, 
relational marketing and political economy [16]. Of these 
three paradigms only the latter (political economy) integrates 
aspects from both the economic and behavioral approaches, 
making it the most suitable to study the complex business 
relationship structures present in today’s economy [1][14]. 
Accordingly, numerous authors have adopted the Political 
Economy 
Paradigm 
(PEP) 
to 
study 
business 
relationships [1][10][14]. Amongst these authors, it is 
certainly Robicheaux and Coleman [16] who have proposed 
the most comprehensive framework to characterize and asses 
the structure of business relationships. Within their 
framework, the structure of a business relationship is 
assessed along two continuous axes: decision-making 
structure and operational integration. The former represents 
the degree to which the decision-making structure is clannish 
or bureaucratic (i.e., the polity structure), while the latter 
represents the degree to which the exchange relationship 
within a channel dyad is discrete or integrated (i.e., the 
economy structure). Four measures, namely centralization, 
formalization, participation and shared paradigm, are 
proposed to assess the decision-making structure (polity) 
while the level of operational integration (economy) can be 
captured through four other variables: joint actions, 
assistances, monitoring and information exchange. To 
complement 
their 
major 
contribution 
to 
the 
field, 
Robicheaux and Coleman [16] also proposed sets of 
antecedents and outcomes related to the structure of business 
relationships. 
B. Conceptualization of IOS usage 
Early studies on IOS usage, such as Venkatraman and 
Zaheer’s [17] empirical work, relied on a single dichotomous 
variable to assess IOS usage. However, the appropriateness 
of such a measure was rapidly challenged since the issue 
from a management’s perspective is typically not one of use 
or non-use, but rather one of how and how much use [18]. 
To address this issue, researchers rapidly arrived to the 
conclusion that they needed continuous (rather than 
dichotomous) variables to assess IOS usage. 
Later, two distinct approaches were adopted to develop 
continuous IOS use variables. The first relied on 
unidimensional measures to assess IOS usage. Among the 
numerous authors promoting this approach, we note 
Wang et al. [19], who conceptualized IOS usage along the 
dimension of virtual integration, defined as the extent to 
which trading partners use IT to support the processes related 
to collaborative operation execution and collaborative 
process planning and control. The second approach relied on 
multidimensional measures to assess IOS usage. Authors 
who favored this approach include Keen [20], who proposed 
a framework describing the usage of IOS along two 
dimensions: reach and range. According to this author, reach 
determines the locations that the system can access and to 
which it can link, while range is defined as the kind of 
information that can be seamlessly and automatically shared 
across the system and services.  
More recently, some researchers have combined 
IT implementation models [21] and the diffusion of 
innovation theory [22] to propose unidimensional measures 
to assess IOS usage at different phases of the technology 
assimilation process. For example, Zhu et al. [23] used a 
9
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

three-stage approach to assess firms’ e-business assimilation. 
The first stage, e-business initiation, was measured by an 
aggregated index capturing whether the firm had used the 
Internet for each of the seven value chain activities proposed. 
The second stage, e-business adoption, was assessed by 
aggregating the seven adoption items. Finally, e-business 
routinization was measured by the extent of organizational 
use of e-business to support value chain activities. 
To conclude this section, it is important to note that most 
of the conceptualizations of IOS use proposed to date, 
whether unidimensional or multidimensional, were inspired 
by Massetti and Zmud’s [24] approach to EDI measurement, 
which comprised four facets: (1) volume, which assesses the 
extent to which a firm’s document exchanges are handled 
through EDI connections; (2) diversity, which captures the 
extent to which different types of documents are handled 
though EDI connections; (3) breadth, which assesses the 
extent to which a firm has developed EDI connections with 
each of its trading partners; and (4) depth, which captures the 
extent to which a firm’s business processes are intertwined 
with those of its trading partners through EDI connections. 
Depending on the context of the research, one or more 
dimensions of this scheme have been used to assess firms’ 
usage of different types of IOS, including EDI [25], SCM 
systems [26] and IOS in general [27]. 
III. 
CONCEPTUAL FRAMEWORK 
Taking into account that this research focuses on the 
economic aspects of dyadic business relationships, we build 
on the theoretical background presented above to propose a 
research model that first considers the direct impact of the 
structure of a buyer-supplier relationship on the operational 
performance of the supplier involved in the dyadic 
relationship (Figure 1). Operational integration, which is one 
of the two axes proposed by Robicheaux and Coleman to 
assess the structure of a business relationship, represents the 
degree to which an exchange relationship within a business 
dyad is discrete or integrated [16]. In a discrete relationship, 
the exchange pursues self-interest vigorously without any 
consideration of future exchange [28], whereas in an 
integrated relationship, the distinct and interdependent 
organizational components of the partners involved in the 
channel dyad constitute a unified whole [29]. The four 
variables proposed by Robicheaux and Coleman [16], 
namely 
joint 
actions, 
assistances, 
monitoring 
and 
information exchange, are used to measure the level of 
operational integration between the partners (i.e., buyer and 
supplier) involved in a dyadic relationship. Joint actions 
capture the extent to which the partners are involved in each 
other’s operations [30]. The assistance assesses the position 
taken by the supplier toward assisting the buyer [31]. 
Monitoring captures the ex-ante and ex post control or 
supervisory actions taken by the buyer over the supplier [32]. 
Finally, 
information 
exchange 
assesses 
the 
bilateral 
expectation that the partners will proactively provide 
information to each other [33].  
A multidimensional view is also adopted to test the 
moderating effect of three IOS usage variables, namely 
volume, diversity and depth, on the relationship between the 
predictor and criterion variables. The fourth facet from 
Massetti and Zmud’s [24] conceptualization of IOS usage, 
breadth, is not considered, as this research focuses on dyadic 
relationships. Volume of IOS usage assesses the extent to 
which the supplier’s activities with the buyer are supported 
by IOS use. Diversity of IOS usage assesses the number of 
e-business functionalities adopted by the supplier to support 
its activities with the buyer [34]. Depth of IOS usage 
assesses the extent to which the supplier’s processes are 
intertwined with those of its buyer through IOS use [30]. 
A. The Impact of the Strucuture of a Buyer-Supplier 
Relationship on Operational Performance of the 
Supplier Involved in the Dyadic Relationship 
Several studies have highlighted or demonstrated the 
positive impact that an integrated exchange relationship can 
have on the operational performance of the partners 
involved. Indeed, integration between supply chain partners 
can improve manufacturing productivity [35], generate 
economies of scale and scope [11], reduce shipment 
discrepancy levels [36] and speed up product development, 
delivery and payment [29]. Furthermore, operational 
integration between partners, as assessed through assistances, 
monitoring, information exchange, continuity expectations 
and flexibility, decreases purchasing costs [30].  
In regards to the particular facets of operational 
integration proposed by Robicheaux and Coleman [16], 
numerous researchers have demonstrated that joint actions 
between a buyer and a seller can improve the performance of 
the parties involved [36]. For example, Cannon et al. [36] 
confirmed the importance of joint actions to improve 
relationship performance under both low and high levels of 
environmental uncertainty. Based on this premise, we posit 
our first hypothesis (H1): Joint actions between the partners 
involved in a dyadic relationship will positively impact the 
operational performance of the supplier involved in the 
relationship. 
Several authors have demonstrated that when a 
seller/buyer offers assistance to a buyer/seller, the 
buyer/seller is likely to interpret such actions as a 
manifestation of commitment by its business counterpart, 
which may be the basis of trust [5][37]. And, given that trust 
and commitment have been found to directly and indirectly 
influence 
exchange 
performance 
or 
organizational 
performance [38], it is likely that supplier’s assistance to the 
buyer will positively impact the supplier’s operational 
performance. Based on this premise, we posit our second 
hypothesis (H2): Supplier’s assistance to the buyer will 
positively impact the supplier’s operational performance. 
The buyer’s monitoring of the performance of the 
supplier is also an important means of assessing the health of 
a relationship. Indeed, in addition to showing that buyer’s 
monitoring of the supplier positively impacts the buyer’s 
performance [39], the literature also shows that buyer’s 
monitoring of the supplier also positively impact the 
supplier’s performance by enticing him to improve its 
activities and processes [40]. Based on this premise, we posit 
our third hypothesis (H3): Buyer’s monitoring of the supplier 
10
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

will 
positively 
influence 
the 
supplier’s 
economic 
performance. 
Finally, past marketing and supply chain management 
studies have shown that effective interorganizational 
information sharing can enhance the performance of a supply 
relationship [41][42]. For example, as stated by Paulraj et al. 
[41, p. 49], when “buyers and suppliers share important 
information relating to materials procurement and product 
design issues, they are more likely to (1) improve the quality 
of their products, (2) reduce customer response time, (3) 
reduce the costs of protecting against opportunistic behavior, 
and (4) increase cost savings through greater product design 
and operational efficiencies”. Based on this premise, we 
posit our fourth hypothesis (H4): Information exchange 
between the partners involved in a dyadic relationship will 
positively impact the operational performance of the supplier 
involved in the relationship. 
B. The Moderating Impact of IOS Usage 
Various 
mechanisms 
facilitating 
the 
operational 
integration among supply chain partners have been identified 
in the management literature [42]. As noted by Barki and 
Pinsonneault [29], these mechanisms include standardizing 
work, standardizing output, standardizing skills and 
knowledge, 
standardizing 
norms, 
direct 
supervision, 
planning, and mutual adjustment. It has also been 
demonstrated that the introduction and usage of an IOS eases 
the implementation of these mechanisms, which, in turn, 
facilitates the operational integration between supply chain 
partners [26]. 
In regards to the particular facets of IOS usage proposed 
by Massetti and Zmud’s [24], various authors have 
demonstrated that unless IOS volume reaches a sufficient 
level, it is unlikely that the reengineering of associated 
business processes will provide significant benefits [24]. 
Indeed, a high volume of IOS usage will allow a firm to fully 
exploit 
the 
key 
mechanisms 
facilitating 
operational 
integration that were initially implemented through the 
adoption of the IOS. Based on this premise, we posit our 
fifth hypothesis (H5): The impact of (a) joint actions between 
the partners, (b) supplier’s assistance to the buyer, (c) 
buyer’s monitoring of the supplier and (d) information 
exchange between the supplier on the supplier’s operational 
performance will be greater when the volume of IOS usage is 
high than when the volume of IOS usage is low. 
 
Volume of 
IOS usage
Diversity of 
IOS usage
Information 
exchange
Assistance
Monitoring
Economic 
performance
H3
H1
H2
H4
Joint actions
Depth of 
IOS usage
H5a
H5b
H5c
H5d
H6a
H6b
H6c
H6d
H7d
H7a
H7b
H7c
 
Figure 1.  Research Model. 
Also, the greater the variety or diversity of documents 
exchanged 
through 
IOS, 
the 
more 
automated 
and 
standardized the document’s generation, transmission and 
reception processes become [24]. By automating and 
standardizing document exchange, IOS diversity also 
optimizes other mechanisms that facilitate operational 
integration such as mutual adjustments and planning [27]. 
Thus, high diversity of IOS usage will allow a firm to 
implement a wide range of mechanisms facilitating 
operational integration. Based on this premise, we posit our 
sixth hypothesis (H6): The impact of (a) joint actions 
between the partners, (b) supplier’s assistance to the buyer, 
(c) buyer’s monitoring of the supplier and (d) information 
exchange between the supplier on the supplier’s operational 
performance will be greater when the diversity of IOS usage 
is high than when the diversity of IOS usage is low. 
Lastly, establishing integrated IOS links (or greater 
depth) through the redesign of business processes and the 
establishment of unique information exchange routines 
increases procedural specificity between the partners 
involved [7]. Furthermore, IOS depth automates and shortens 
the time required to exchange information [25], which in turn 
may allow for the implementation of other mechanisms that 
facilitate operational integration such as direct supervision 
planning and mutual adjustment. Thus, greater depth of IOS 
usage will allow a firm to implement a wide range of 
mechanisms facilitating operational integration. Based on 
this premise, we posit our seventh hypothesis (H7): The 
impact of (a) joint actions between the partners, (b) 
supplier’s assistance to the buyer, (c) buyer’s monitoring of 
the supplier and (d) information exchange between the 
supplier on the supplier’s operational performance will be 
greater when the depth of IOS usage is high than when the 
depth of IOS usage is low. 
IV. 
METHODOLOGY 
As our research is still in progress, this Section explains 
the methodological framework we have devised, but not yet 
used, to test our research model. More precisely, we present 
our unit of analysis as well as our intended research setting, 
data collection procedures, survey instrument and data 
analyses procedures. 
A. Research Setting 
This study explores the structure of business relationships 
from the perspective of the supplier. Hence, the unit of 
analysis of this research is the supplier’s relationship with a 
particular buyer. The up-to-date list of manufacturing firms 
from the greater Montréal area maintained by a local 
government agency (CRIQ) will constitute this study’s 
sample frame. In addition, the sample frame will be limited 
to manufacturing firms active in four industrial sectors: (1) 
machinery manufacturing (NAICS 333); (2) computer and 
electronic product manufacturing (NAICS 334); (3) 
electrical 
equipment, 
appliance 
and 
component 
manufacturing (NAICS 335); and (4) transportation 
equipment manufacturing (NAICS 336). Two reasons 
justified this choice. First, the adoption level of IOS in these 
four sectors is among the highest (Forester Research 2011). 
11
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Second, previous studies have demonstrated the validity of 
these sectors in the study of business relationships [15][30]. 
B. Data Collection 
Data will be collected by the mean of an online survey. 
We will follow the key informant approach and collect data 
from one sales professional at each supplier because 
specialists in this boundary role are most likely to be 
knowledgeable about study constructs [35]. Prior to 
answering the online survey, respondents will be asked to 
focus on an important buyer relationship for a major product 
manufactured by their company. To maximize this study’s 
response rate, the diffusion of the survey will be based on 
four key elements: (1) a respondent-friendly questionnaire; 
(2) a five-contact strategy (in the form of five different 
e-mails to be sent to sales professionals); (3) a personalized 
correspondence; and (4) two incentives (i.e., a tailored 
benchmark report and a chance to win an electronic gift card 
of a 500$ value on Amazon). To ensure the anonymity of our 
respondents all collected data will be anonymized. 
C. Survey Instrument 
The survey instrument will comprise measures adapted 
from the literature. Measures tied to the structure of the 
dyadic business relationship (Joint actions [43], Assistance 
[38], Monitoring [32], and Information exchange [33]) will 
be adapted from marketing studies while those related to IOS 
usage (volume of IOS usage [34], diversity of IOS usage 
[44] and depth of IOS usage [24] will be adapted from IS 
studies. In addition, the measure to assess the operational (or 
economic) performance of the supplier will be adapted from 
the work of [26]. All measures are available upon request to 
the authors. 
D. Data Analyses 
The analysis of our data comprises four steps: 
(1) assessing the unidimensionality and convergent validity 
of the constructs; (2) assessing the internal consistency of the 
constructs; (3) assessing the discriminant validity of the 
constructs; and (4) testing our research hypotheses via a 
hierarchical regression model. For the last step, consistent 
with standard practice for analyzing models with interaction 
effects [45], variables will be entered in blocks in the 
hierarchical regression model. First, we will include the 
variables of joint actions, assistances, monitoring and 
information exchange into the model and assess their impact 
of operational performance. Then, we will enter the IOS 
usage variables into the model and asses their impact on 
operational performance. Finally, we will enter the 
interaction variables into the model and asses their impact on 
economic performance. 
V. 
CONCLUSION 
The objective of this study was twofold. First, to examine 
how the structure of a buyer-supplier relationship impacts the 
operational performance of the supplier involved in the 
dyadic relationship. Second, to test the moderating impact of 
IOS usage on the relationship between the structure of a 
buyer-supplier relationship and the operational performance 
of the supplier involved in the dyadic relationship. 
Accordingly, this research is likely to yield important 
theoretical contributions and practical implications despites 
certain limitations.  
A. Theoretical Contributions 
This study should make two important theoretical 
contributions to the marketing and IS literatures. First, this 
research will be the first to empirically test a key sub-set of 
Robicheaux and Coleman’s [16] framework to characterize 
and assess the structure of business relationships. Second, 
this study will link past findings from marketing and IOS 
studies and show that IOS can help maximize the economic 
value of supplier involved in buyer-supplier business 
relationships. 
B. Practical Implications 
From a practical standpoint, the present research 
anticipated results should allow managers to identify the key 
marketing activities (i.e., joint actions, assistance, monitoring 
and information exchange) that foster economic value in 
dyadic business relationships. Also, this study should help 
managers maximize the economic value they derive from 
their dyadic business relationships by revealing the key role 
that IOS play in this context. 
C. Limits and Future Research Avenues 
There are two main limitations to this study. First, the 
research model will be tested with data collected from a 
small sample, which evidently limits the scope and 
generalizability of our results. To alleviate this issue, future 
researches could test our research model with manufacturing 
firms pertaining to other industries and/or localized in other 
regions of the world. Second, we did not investigate the 
impact of IOS ownership in the context of dyadic business 
relationships. Future research should focus on this important 
variable since several studies have shown that benefits tied to 
IOS usage are often skewed in favor of the IOS owner [7]. 
REFERENCES 
[1] A. Boyle, F. R. Dwyer, R. A. Robicheaux, and J.T. Simpson, 
“Influence strategy in marketing channels: Measures and use 
in different relationship structures,” J. Marketing Res., vol. 
29, pp. 462–473, November 1992. 
[2] B. Chae, H. R. Yen, and C. Sheu, “Information Technology 
and Supply Chain Collaboration: Moderating Effects of 
Existing Relationships Between Partners,” IEEE Trans. Eng. 
Manag., vol. 52, no. 4, pp. 440-448, 2005. 
[3] I. Geyskens, J.-B. E. M. Steenkamp, and N. Kumar, “A meta-
analysis of satisfaction in marketing channel relationships,” J. 
Marketing Res., vol. 36, no. 2, pp. 223-238, May 1999. 
[4] J. P. Cannon and C. Homburg, “Buyer-supplier relationships 
and customer firm costs,” Journal of Marketing, vol. 65, no. 1, 
pp. 29-43, 2001. 
[5] J. M. Bonner and R. J. Calantone, “Buyer attentiveness in 
buyer-supplier relationships,” Ind. Market. Manage., vol. 34, 
no. 1, pp. 53-61, 2005. 
[6] V. Choudhury, “Strategic choices in the development of 
interorganizational information systems,” Inf. Syst. Res., vol. 
8, no. 1, pp. 1-24, 1997. 
[7] K. A. Saeed, M. K. Malhotra, and V. Grover, “Examining the 
impact of interorganizational systems on process efficiency 
12
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

and sourcing leverage in buyer-supplier dyads,” Decis. Sci., 
vol. 36, no. 3, pp. 365-396, 2005. 
[8] E. K. Clemons, S. Reddi, and M. C. Row, “The impact of 
information technology on the organization of economic 
activity: The ‘move to the middle’ hypothesis,” J. Manage. 
Inf. Syst., vol. 10, no. 2, pp. 9-35, 1993. 
[9] T. W. Malone, J. Yates, and R. I Benjamin, “Electronic 
markets and electronic hierarchies,” Commun. ACM, vol. 30, 
no. 6, pp. 484-497, 1987. 
[10] W. L. Stern and T. Reve, “Distribution channels as political 
economies: A framework for comparative analysis,” J. 
Marketing, vol. 44, no. 3, pp. 52-64, 1980. 
[11] O. Williamson, The Economic Institutions of Capitalism, 
New York: Free Press, 1985. 
[12] I. R. Macneil, The New Social Contract: An Inquiry into 
Modern Contractual Relations, New Haven, CT: Yale 
University Press, 1980. 
[13] P. J. Kaufmann and L. W. Stern, “Relational exchange norms, 
perceptions of unfairness, and retained hostility in commercial 
litigation,” Journal of Conflict Resolution, vol. 32, no. 3, pp. 
534-552, 1988. 
[14] J. J. Mohr and J. R. Nevin, “Communication strategies in 
marketing channels: A theoretical perspective,” J. Marketing, 
vol. 54, no. 4, pp. 36-51, 1990. 
[15] J. B. Heide and G. John, “Alliances in industrial purchasing: 
The 
determinants 
of 
joint 
action 
in 
buyer-supplier 
relationships,” J. Marketing Res., vol. 27, no. 1, pp. 24-36, 
1990. 
[16] R. A. Robicheaux and J. E. Coleman, “The structure of 
marketing channel relationships,” J. Acad. Market. Sci., vol. 
22, no. 1, pp. 38-51, 1994. 
[17] N. Venkatraman and A. Zaheer, “Electronic integration and 
strategic advantage: A quasi experimental study in the 
insurance industry,” Inf. Syst. Res., vol. 1, no. 4, pp. 377-393, 
December 1990. 
[18] G. E. Truman, “A discrepancy-based measurement approach 
for data integration,” working paper IS-95-24, Stern School of 
Business, New York University, 1995. 
[19] E. T. G. Wang, J. C. F. Tai, and H. L. Wei, “A virtual 
integration theory of improved supply chain performance,” J. 
Manage. Inf. Syst., vol. 23, no. 2, pp. 41-64, 2006. 
[20] P. G. W. Keen, Shaping the Future: Business Design through 
Information Technology, Cambridge, MA: Harvard Business 
Press, 1991.  
[21] T. H. Kwon and R. W. Zmud, “Unifying the fragmented 
models of information systems implementation,” in Critical 
Issues in Information Systems Research, R. Boland and R. 
Hirscheim, Eds., Chichester, UK: Wiley, 1987, pp. 88-97. 
[22] E. M. Rogers, Diffusion of Innovations, New York: Free 
Press, 1985. 
[23] K. Zhu, K. L. Kraemer, and S. Xu, “The process of innovation 
assimilation by firms in different countries: A technology 
diffusion perspective,” Manage. Sci., vol. 52, no. 10, pp. 1557 
1576, 2006. 
[24] B. Massetti and W. R. Zmud, “Measuring the extent of EDI 
usage in complex organizations: Strategies and illustrative 
examples,” MIS Quart., vol. 30, no. 3, pp. 331-345, 1996. 
[25] P. Hart and C. Saunders, “Emerging electronic partnerships: 
Antecedents and dimensions of EDI use from the supplier’s 
perspective,” J. Manage. Inf. Syst., vol. 14, no. 4, pp. 87-112, 
1998. 
[26] M. Subramani, “How do suppliers benefit from information 
technology use in supply chain relationships?,” MIS Quart., 
vol. 28, no. 1, pp. 45-73, 2004. 
[27] L. Chi, C. Holsapple, and C. Srinivasan, “Competitive 
dynamics in electronic networks: A model and the case of 
interorganizational 
systems,” 
International 
Journal 
of 
Electronic Commerce, vol. 11, no. 3, pp. 7-49, Spring 2007. 
[28] J. B. Heide, “Interorganizational governance in marketing 
channels,” J. Marketing, vol. 58, pp. 71-85, April 1994. 
[29] H. Barki and A. Pinsonneault, “A model of organizational 
integration, implementation effort, and performance,” Org. 
Sci., vol. 16, no. 2, pp. 165–179, March–April 2005. 
[30] A. W. Joshi and R. L. Stump, “The contingent effect of 
specific asset investments on joint action in manufacturer-
supplier relationships: An empirical test of the moderating 
role of reciprocal asset investments, uncertainty, and trust,” J. 
Acad. Market. Sci., vol. 27, no. 3, pp. 291 305, 1999. 
[31] T. G. Noordewier, G. John, and J. R. Nevin, “Performance 
outcomes of purchasing arrangements in industrial buyer-
vendor relationships,” J. Marketing, vol. 54, no. 4, pp. 80-93, 
1990. 
[32] R. L. Stump and J. B. Heide, “Controlling supplier 
opportunism in industrial relationships,” J. Marketing Res., 
vol. 33, no. 4, pp. 431-441, 1996. 
[33] J. B. Heide and A. S. Miner, “The shadow of the future: 
Effects of anticipated interaction and frequency of contact on 
buyer-seller cooperation,” Acad. Manage. J., vol. 35, no. 2, 
pp. 265-291, 1992. 
[34] P. Hadaya, “Benchmarking firms’ operational performance 
according to their use of Internet based interorganizational 
systems,” Benchmarking, vol. 16, no. 5, pp. 621-639, 2009. 
[35] J. E. Ettlie and E. M. Reza, “Organizational integration and 
process innovation,” Acad. Manage. J., vol. 35, no. 4, pp. 
795-827, 2001. 
[36] J. P. Cannon, R. S. Achrol, and G. T. Gundlach, “Contracts, 
norms, and plural form governance,” J. Acad. Market. Sci., 
vol. 28, no. 2, pp. 180-194, 2000. 
[37] J. H. Dyer and W. Chu, “The Determinants of Trust in 
Supplier-Automaker Relationships in the U.S., Japan, and 
Korea,” Journal of international business studies, vol. 31, no. 
2, pp. 259-285, 2000. 
[38] J. A. Siguaw, P. M. Simpson, and T. L. Baker, “Effects of 
supplier market orientation on distributor market orientation 
and the channel relationship: The distributor perspective,” J. 
Marketing, vol. 62, no. 3, pp. 99-111, 1998. 
[39] P. D. Cousin, B. Lawson, and B. Squire, “Performance 
measurement in strategic buyer-supplier relationships: The 
mediating role of socialization mechanisms,” Int.  J. Oper. 
Prod. Manag., vol. 28, no. 3, pp. 238-258, 2008. 
[40] P. K. Dey, A. Bhattacharya, and W. Ho, “Strategic supplier 
performance evaluation: A case-based action research of a 
UK manufacturing organization,” Int. J. Prod. Econ., vol. 166, 
pp. 192-214, 2015. 
[41] A. Paulraj, A. A. Lado, and I. J. Chen, “Inter-organizational 
communication as a relational competency: Antecedents and 
performance outcomes in collaborative buyer–supplier 
relationships,” J. Oper. Manage., vol. 26, pp. 45-64, 2008. 
[42] S. Glouberman and H. Mintzberg, “Managing the care of 
health and the cure of disease – Part II: Integration,” Health 
Care Management Review, vol. 26, no. 1, pp. 70-84, 2001. 
[43] M. Bensaou and N. Venkatraman, “Configurations of 
interorganizational relationships: A comparison between U.S. 
and Japanese automakers,” Manage. Sci., vol. 41, pp. 1471 
1492, 1995. 
[44] M. T. Frohlich and R. Westbrook, “Demand chain 
management in manufacturing and services: Web-based 
integration, drivers and performance,” J. Oper. Manage., vol. 
20, no. 6, pp. 729 745, 2002. 
[45] L. Aiken and S. West, Multiple Regression: Testing and 
Interpreting Interactions, Newbury Park, CA: Sage, 1991. 
13
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Enablers of Business Process Transformation Success in Japan: 
How Super-ordinate Groups Achieve Effectiveness? 
Kayo Iizuka 
School of Network and Information, Senshu University 
Kawasaki, Japan 
 e-mail: iizuka@isc.senshu-u.ac.jp 
Chihiro Suematsu 
Graduate School of Management, Kyoto University 
Kyoto, Japan 
e-mail: suematsu@econ.kyoto-u.ac.jp 
Abstract— The aim of this paper is to present the analysis 
result of research that focuses on the enablers of business 
process transformation in Japan. It has been said for some 
decades that the overall quality and efficiency of production 
lines in Japan are superior to many other countries. However, 
the overall business processes, including work of back offices 
throughout all industries, are not always efficient.  According 
to 
the 
Organisation 
for 
Economic 
Co-operation 
and 
Development (OECD), labor productivity in Japan ranks 22nd 
out of 34 countries. Ministry of Economy, Trade and Industry 
(METI) mentions that the scope of information system 
integration is rather narrow in Japan, compare to many other 
countries. In addition, about 70% of firms have aimed at 
drastic process change but only 30% have attained it. Dealing 
with these issues, the authors conducted a survey and 
identified the enablers of Business Process Transformation 
(BPT) focusing on “super-ordinate” firms. 
Keywords-business process transformation; success factor; 
IT utilization stage. 
I.
INTRODUCTION
It has been said for some decades that the overall quality 
and efficiency of production lines in Japan are superior to 
other 
countries. 
The 
word 
“KAIZEN 
(continuous 
improvement)” has become a word that is used not only in 
Japan. However, the overall business processes including the 
work of back offices throughout all industries are not always 
efficient [1][2], and labor productivity in Japan ranks 22nd 
out of 34 countries according to the Organization for 
Economic Co-operation and Development (OECD) [3]. 
Long working hours are a serious issue in Japan.  Firms in 
Japan are trying to make their business processes more 
efficient by using Information Technology (IT). The 
objective of IT investment may differ in each firm; however, 
there are various tendencies according to region, industry 
and so on. As for the regions, Higano mentioned that 
Japanese firms tend to spend much more, in percentage 
terms, on improving the operational efficiency of their 
business compared to firms in Western countries, but the 
results do not seem to be satisfactory because the 
contribution of IT capital services to value added growth in 
Japan is lower than in Western countries [4]. There are many 
methodologies for business process orientation including 
business process re-engineering (BPR) or business process 
integration (BPI) [5]. However, it is difficult to achieve 
effectiveness by conforming to an ideal or to picture-perfect 
models. Data from the survey conducted by the authors show 
that 72.9% of the respondents (managers of information 
systems, business planning, or internal audit divisions) stated 
that the policy of their BPR was “drastic BPR,” but only 
28.4% had attained it [6].  
In this paper, we present the analysis result of research 
that focused on the enablers of business process 
transformation (BPT), focusing on what the “super-ordinate” 
firms do to achieve effectiveness considering the unique 
situation in Japan. In the subsequent section, we review 
related studies with the category of business process 
orientation and the unique situation of firms in Japan 
considered for BPT success. Section 3 describes our research 
model of BPT success and the analysis results of the survey. 
The differences seen between “super-ordinate” firms and 
other firms are discussed in Section 4. Finally, Section 5 
concludes this work and mentions about our future work. 
II. RELATED STUDIES 
In this section, we will look through the related studies 
about business process orientation, and the unique situation 
of firms in Japan should be considered for BPT success. 
A.
Studies about business process orientation  
Studies related to business process orientation can be 
roughly classified into several groups: methodologies [7][8], 
tools [9][10], the effectiveness of business process 
integration including critical success factors (CSF) [11] 
[12][13], etc. As for CSF, some researchers mention that 
culture is also one of them [14][15]. Takei et al. mentioned 
that some CSF of Enterprise Resource Planning (ERP) vary 
in different regions or countries, while some CSF are 
common to many countries [16].  For instance, BPR is not so 
important in developing countries because they are making 
new business processes and do not need re-engineering. The 
CSF of business process orientation including BPT can be 
considered to vary in different countries. 
B.
The unique situation of firms in Japan should be 
considered for BPT success  
・Scope of the information system. One of the reasons why 
business process transformation is not efficient in Japan may 
be the scope of information system integration [17]. Figure 1 
shows the result of a survey about the IT implementation 
14
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

stage conducted by the Ministry of Economy, Trade and 
Industry (METI) in Japan. The “IT utilization stage” is 
defined as follows [17]: 
•Stage 1: The information system is implemented. 
•Stage 2: The information system is optimally utilized 
within a department or section within a firm. 
•Stage 3: The information system is optimally utilized 
within an enterprise (expanded beyond departments or 
sections).  
•Stage 4: The information system is optimally utilized 
among enterprises (including suppliers or customers). 
The fact that about 70% of the firms in Japan (that is 
much lower compare to many of Western countries) are in 
stage 1 or stage 2 can be considered one of the reasons for 
the difficulty in implementing ERP systems in Japan, and for 
the low effectiveness of IT investment in Japan. Behind this, 
there is the culture of “TATEWARI”, which is a kind of 
vertically-segmented administrative system organization 
which takes a “silo approach” in Japan. In the TATEWARI 
type of organization, each section does not interfere with 
other departments, and everyone works hard only within the 
organization that they belong to. 
Figure 1. IT utilization four stages (Source: METI 2010) 
・Improvement but not drastic. Data from the survey 
conducted by we show that 72.9% of the respondents 
(managers of information systems, business planning, or 
internal audit divisions) stated that the policy of their 
business 
process 
transformation 
was 
“drastic 
transformation,” but only 28.4% had attained it [6] 
(Figure2). 

Planned 
Total
Others
Service
Finance
Retail
Manufacturing 
72.9
66.7
76.5
78.9
67.7
74.6
26.7
33.3
23.5
21.1
32.3
24.6
0.4
0.8
Drastic re-engineering
As-Is based improvement
Others

Actual 
Total
Others
Service
Finance
Retail
Manufacturing 
28.4
16.3
33.3
27.8
27.3
31.9
70
81.4
66.7
72.2
72.7
65.5
1.6
2.3
2.6
Drastic re-engineering
As-Is based improvement
Others
Figure 2. Business process transformation policy (Planed and Actual) 
・Outsource. Figure 3 shows the ratio of IT investment by 
software type [18]. Japanese firms rely far more on system 
engineers of outsourcing services, compared to the United 
States. Some of the reasons for this situation are related to 
employment conditions in Japan (Figure 3). 
Figure 3. Difference in IT Investment Ratio by Software Type       
(Source: Motohashi 2010, in Japanese) 
As we mentioned above, it seems to be important to seek 
ways to achieve effectiveness though BPT considering the 
unique situation in Japan. In the following sections, we 
describe the analysis result of the survey that we conducted. 
III. ANALYSIS OF BPT SUCCESS 
To address the issues mentioned in the previous section, 
we conducted a new survey. In this section, we describe the 
analysis result concerning enablers of BPT success. 
A.
Conceptual research framework and hypotheses 
Enabler items are considered to consist of BPT 
management, executing drastic transformation, and wide-
scope transformation. Figure 4 is a conceptual model for 
analysis. 
BPT Effectiveness
Business – IT 
Management
Wide – Scope
Transformation
Execute Drastic
Transformation
Figure 4. Conceptual model 
The major hypotheses for the analysis are as follows: 
(n=240)
(n=243)
(%)
(%)
15
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Hypothesis 1 (H1): BPT management level has a positive 
impact on BPT effectiveness.  
It is hardly surprising that BPT management is important 
for achieving BPT effectiveness. We defined research items 
for BPT management considering the unique situation of 
firms in Japan: “business executives’ involvement in IT 
strategy”, “communication between business and IT 
section”, “motivation for improvement”, “business - IT 
alignment function”, “clear rules for cross-department 
decisions”, “evaluation criteria for external resources” 
Hypothesis 2 (H2): Planning and executing drastic 
transformation has a positive impact on the BPT 
effectiveness. 
As mentioned in the previous section, many firms in 
Japan failed to realize drastic BPT (realized only As-is 
based improvement finally), and that is considered one of 
the reasons why such firms have Japan is hardly achieved 
effectiveness by BPT which focuses on business efficiency. 
Hypothesis 3 (H3): Wide- scope transformation has a 
positive impact on BPT effectiveness. 
As mentioned in the previous section, many firms in 
Japan are at a lower stage of IT utilization, and that is 
considered as one of the reason why firms in Japan is hardly 
achieve their effectiveness by BPT that focuses on business 
efficiency. 
B.
Overall Research Results 
For the survey, 413 samples were gathered from the 
internet in March 2015. Table 1 shows the profile of the 
survey data. In next section, we describe the result of their 
analysis of BPT success. 
TABLE I. 
PROFILE OF SURVEY DATA
Industry
Frequency
Percentage
Manufacturing
153
37.05%
Distribution
38
9.20%
Finance
18
4.36%
Service
150
36.32%
Others
54
13.08%
Total
413
Posision (※count duplicate for conccurent post)
Frequency
Percentage
Manager of Business PlanningDivision
95
13.34%
Sttaff of Business Planning Division
53
7.44%
Manager of IT Division
91
12.78%
Staff of IT Division
103
14.47%
Manager of IT User Division
187
26.26%
Sttaff of IT User Division
135
18.96%
Chief Executive Officer
25
3.51%
n.a.
23
3.23%
Total
712
Annual Sales (Yen)
Frequency
Percentage
Above 1000T
29
2.66%
Between 700B - 1T
12
6.54%
Between 400B - 700B
23
14.04%
Between 100B - 400B
11
7.02%
Between 70B - 100B
14
5.57%
Between 40B - 70B
27
6.54%
Between 10B - 40B
58
3.39%
Under 10B
188
45.52%
n.a.
51
12.35%
Total
413
Number of Employees
Frequency
Percentage
Above 10,000
54
9.93%
Between 7,000 - 10,000
15
10.65%
Between 4,000 - 7,000
19
19.61%
Between 1,000-4,000B
41
13.08%
Between 700 - 1,000
42
4.60%
Between 400 - 700
44
10.65%
Between 100 - 400
81
10.17%
Under 100
115
27.85%
n.a.
2
0.48%
Total
413
Business – IT management level and BPT effectiveness. 
The multi regression result of IT management level of 
super-ordinate firms (target variable: BPT effect) are shown 
in Table 2. Predictor variables “business executive’s 
involvement in IT strategy”, “communication between 
business and IT section”, “motivation for improvement”, 
“evaluation criteria for external resources” had positive and 
significant impacts on BPT effect. The item “business - IT 
alignment function” was not significant, and “clear rules for 
cross-department decisions” had a negative impact. Clear 
rules for cross-department decisions may reinforce the 
“TATEWARI” culture which is considered one of the 
obstructive factors to wide-ranging BPT. 
TABLE II. 
MULTI REGRESSION RESULT                                             
(TARGET VARIABLE: BPT EFFECT) 
 Predictor Variable 
Coefficients 
F-value 
p-value
Involvement of business 
management in the IT 
strategy decision process 
0.2017 
20.7115 
** 
Communication 
between 
Business and IT section 
0.1147 
5.4192 
* 
Motivation for Innovation 
/Improvement 
0.1849 
12.8373 
** 
Business - IT alignment 
function 
0.0900 
3.0136 
- 
Clear 
rules 
for 
cross-
department decisions 
-0.0978 
3.8994 
* 
Evaluation 
criteria 
for 
external resources 
0.1930 
25.9699 
** 
Constant 
1.0737 
62.7238 
** 
Overall model 
53.3488 
** 
*：P<0.05, **：P<0.01 
Drastic BPT level and effectiveness. The multi regression 
result of drastic BPT level and BPT effectiveness is shown 
in Table 3. The drastic BPT level is as follows: 

Level 4: Planned and able to execute drastic BPT. 

Level 3: Planned drastic BPT but could not execute 
it (could execute As-is based improvement). 

Level 2:  Planned as-is based improvement and 
executed drastic transformation. 

Level 1: Planned and executed as-is based 
improvement. 
TABLE III. 
REGRESSION RESULT  (TARGET VARIABLE: BPT EFFECT) 
 Predictor Variable 
Coefficients 
F-value 
p-value 
BPT Drastic Level 
0.2050 
16.3841 
** 
Constant 
2.7773 
382.5463 
** 
Overall model 
16.3841 
** 
*：P<0.05, **：P<0.01 
As we had expected, the BPT drastic level has a positive 
significant relationship with BPT effect. Figure 5 shows the 
cross-tabulation result displayed as a bar chart. High level 
firms tend to achieve effectiveness. 
16
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

1
6
2
1
1
20
17
5
18
68
54
25
7
44
50
27
0
7
5
16
0%
20%
40%
60%
80%
100%
Level 1
Level 2
Level 3
Level 4
BPT Drastic Level
Satisfaction about BPT Effectiveness
1 (Dissatisfied)
2 (Rather Dissatisfied)
3 (Neutral)
4 (Rather satisfied)
5 (Satisfied)
Figure 5.  Drastic BPT level and BPT effectiveness 
Wide-scope and BPT effectiveness.
The regression result 
of IT utilization stage (target variable: BPT effect) are 
shown in Table 4, and Figure 6 shows the cross-tabulation 
result displayed as a bar chart. 
TABLE IV. 
REGRESSION RESULT
(TARGET VARIABLE: BPT EFFECT) 
 Predictor Variable 
Coefficients 
F-value 
p-value 
IT Utilization stage 
- 0.0797 
2.5379 
0.1120 
-  
Constant 
3.5359 
670.3786 
** 
Overall model 
2.5379 
** 
*：P<0.05, **：P<0.01 
2
3
5
1
5
6
24
3
24
32
91
19
21
27
62
16
11
6
7
4
0%
20%
40%
60%
80%
100%
Stage 1
Stage 2
Stage 3
Stage 4
IT Utilization Stage
Satisfaction about BPT Effectiveness
1 (Dissatisfied)
2 (Rather Dissatisfied)
3 (Neutral)
4 (Rather satisfied)
5 (Satisfied)
Figure 6. IT Utilization stage and BPT effectiveness 
The p-value was 0.112 and this was not smaller than 
0.05. In that sense, we can say we cannot find a significant 
relationship between “IT utilization stage” and BPT effect. 
However the p-value is 0.112, it is nearly at the 10% level of 
significance. (It is sometimes considered that <10% is 
significant, rather than <5% is significant). The coefficient 
value of this item is negative. That means wide-ranging BPT 
projects seldom achieve their goal. Figure 7 shows the IT 
utilization stage ratio. There are about 50% of stage 4 and 3 
firms, but this is still far less than in Western countries. 
The reason why the item “IT utilization stage” shows a 
negative impact on BPT effect may be that many firms in 
Japan achieve only a small effect by targeting small scope 
of the organization. 
7%
39%
34%
20%
Stage 1: Just implemented
Stage 2: Department optimized
Stage 3: Company-wide optimized
Stage 4: Inter-company optimized
Figure 7. IT Utilization stage ratio 
Summary of overall analysis. Figure 8 is the summary of 
overall analysis. Most of the BPT items have a positive 
impact on BPT effectiveness. Executing drastic BPT has a 
positive impact on BPT effectiveness, but only 18 % of the 
firms have been able to execute the drastic BPT that they 
had planned. Wide – scope BPT has a positive impact on 
BPT effectiveness, but many of the firms that have wide–
scope transformation have not achieved effectiveness. 
Figure 8. Summary of overall analysis 
In addition to the result mentioned in 3.2.3, there some 
issues are founded from the survey as follows: 

The firms where the IT utilization stage is high do not 
tend to achieve BPT effectiveness. 
 METI mentioned that most firms in Japan are at a 
lower stage, and that is one of the reasons EISs in 
Japan are not efficient. However, even firms at 
higher stages do not always achieve BPT 
effectiveness. 

The firms whose IT utilization stage is at a high level 
do not always achieve a BPT effect which is IT 
contribution on business efficiency. 

Although about 50% of the firms have planned drastic 
transformation, only 37% of the firms above have been 
able to implement such transformation (2015). 
 It has progressed from the survey conducted in 
2008 (30% of the firms could implement drastic 
transformation), but the percentage (37%) is still 
behind other countries. 
C.
How do “super-ordinate firms” achieve their 
effectiveness? 
Addressing the issues that relationship between drastic 
BPT and wide-scope BPT is negative but this negative 
relationship can be thought the one of the reason of low 
efficiency of business processes in Japan, we tried to 
BPT Effectiveness
Business – IT 
Management
Wide – Scope
Transformation
Execute Drastic
Transformation
+
+
-
-
17
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

analyze that how do “super-ordinate firms” achieve their 
BPT effectiveness. 
Definition of “super-ordinate firms”. We defined “super-
ordinate firms” as follows: 

Drastic BPT Level: Level 4 (Planned drastic BPT and 
could execute it) 

IT utilization Stage: Stage 4 (IS is optimally utilized 
among firms)
 or 3 (IS is optimally utilized within a 
firm.)  
The reason why we defined “super-ordinate firms” as 
above is that “super-ordinate firms” are overcoming their 
drawbacks and achieving the BPT effectiveness. 
IT management level of “super-ordinate firms”. The 
multi regression result of IT management level of super-
ordinate firms (target variable: BPT effect) are shown in 
Table 5.  
TABLE V. 
REGRESSION RESUL (TARGET VARIABLE: BPT EFFECT) 
Predictor 
 Variable 
Super-ordinate firms (n=30) 
Other firms (n=378) 
Co-
efficients 
F-value
p-value
Co-
efficients 
F-value
p-value
Involvement of 
business 
management in 
the IT strategy 
decision 
process 
0.2075 
19.0905 
** 
Managements' 
will  to change 
business 
process 
according to 
business 
environmental 
change  
0.5796 
16.8674 
** 
Communicatio
n between 
Business and 
IT section 
0.1162 
5.1579 
* 
Motivation for 
Improvement 
0.1737 
10.6313 
** 
Business - IT 
alignment 
function 
0.2185 
2.3347 
0.1391 
0.0888 
2.6973 
0.1015 
Clear rules for 
cross-
department 
decisions 
-0.1027 
3.8186 
0.0515 
Evaluation 
criteria for 
external 
resources 
0.2528 
5.1090 
* 
Provision for 
IT Risk 
-0.1996 
2.8360 
0.1046 
Constant 
0.4745 
1.0711 
- 
1.1368 
65.4518 
** 
Overall model 
** 
** 
*：P<0.05, **：P<0.01 
There is a significant, positive relationship between the 
following items and BPT effect; “Managements' will to 
change 
business 
process 
according 
to 
business 
environmental change” and “Evaluation criteria for external 
resources”.  There is a positive relationship between 
“Business - IT alignment function” and BPT effect, its p-
value is about 0.13, which can be said to be rather 
significant (because sometimes up to around 0.1 is 
considered as significant).  This multi regression result of 
“super-ordinate firms” is quite different from that of other 
countries. For the group of other firms, the items which 
have a positive significant relationship with BPT effect are 
“Involvement of business management in the IT strategy 
decision process”, “Communication between Business and 
IT section” and “Motivation for Improvement”. 
Meeting style of “super-ordinate firms”. The multi 
regression result of IT management level of super-ordinate 
firms (target variable: BPT effect) are shown in Table 6. 
The following items have a positive impact on BPT effect: 
“System users’ division leads the meeting” and “Meeting is 
held at IT division’s office”. For the meeting style, we could 
not find any difference between super-ordinate firms and 
other firms by item, but the coefficient values were 
different. 
TABLE VI. 
REGRESSION RESULT (TARGET VARIABLE: BPT EFFECT) 
Predictor 
Variable 
Super-ordinate firms (n=26) 
Other firms (n=274) 
Coefficients 
F-value 
p-value 
Coefficients 
F-value 
p-value
System 
users’ 
division 
has the 
key role 
at the 
meeting. 
0.2731 
3.6525 
0.0685 
- 
0.1868 
12.7846 
0.0000 
** 
Meeting 
is held at 
IT 
division’s 
office. 
0.4882 
13.2862 
0.0014 
** 
0.0981 
4.0598 
0.0449 
* 
Informal 
meeting 
about 
BPT is 
held in 
coffee 
lounge 
etc. 
0.0970 
2.2612 
0.0245 
* 
Constant 
0.5756 
0.5997 
0.4466 
2.0858 
10.8761 
0.0000 
** 
Overall 
model 
0.0019 
** 
0.0000 
** 
*：P<0.05, **：P<0.01 
What is difficult about BPT?  The multi regression results 
as regards what is difficult about BPT of super-ordinate 
firms and other firms (target variable: BPT effect) are 
shown in Table 7. As for the super-ordinate firms, the 
answer of the question “It is difficult to estimate the BPT 
effectiveness of each division” has a positive impact on BPT 
effectiveness. That means the “super-ordinate” firms are 
good at estimating the BPT effectiveness of each division 
(5: No (=Do not feel difficulty), 4: Rather no, 3: Neutral, 2: 
Rather no, 1: No) . It seems this makes sense, because the 
adjustment of each division’s requirements can be 
considered as one of the critical issues of wide-scope 
transformation.  
18
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

TABLE VII. 
REGRESSION RESULT (TARGET VARIABLE: BPT EFFECT) 
Predictor 
Variable 
Super-ordinate firms (n=26) 
Other firms (n=274) 
Coefficie
nts 
F-value 
p-value 
Coefficie
nts 
F-value 
p-value 
It is 
difficult to 
estimate 
the BPT 
effectivene
ss of each 
division. 
0.2909 
3.2105 
0.0844 
It is 
difficult to 
determine 
the 
business 
process 
standardiza
tion scope 
of the BPT 
project. 
0.1061 
2.6425 
0.1050 
- 
It is 
difficult to 
adjust 
system 
users’
requiremen
ts and IT 
feasibility. 
-0.0955 
2.2245 
0.1368 
- 
Constant 
2.6670 
38.1676 
0.0000 
** 
3.2901 
572.541 
0.0000 
** 
Overall 
model 
3.2105 
0.0844 
- 
0.2343 
5: No (=Do not feel difficulty), 4: Rather no, 3:  Neutral, 2: Rather no, 1:No 
*：P<0.05, **：P<0.01 
On the other hand, as for the other firms, the answer of 
the question “It is difficult to adjust system users’ 
requirements and IT feasibility” has negative impact on 
BPT effectiveness (5:No (=Do not feel difficulty), 4:Rather 
no, 3: Neutral, 2:Rather no, 1:No) . That means, for the 
other 
firms, 
difficulty 
of 
adjusting 
system 
users’ 
requirement 
and 
IT 
feasibility 
is 
lowering 
BPT 
effectiveness. 
IV. DISCUSSION 
Some differences were seen between “super-ordinate” 
firms and other firms in some categories.  
The items that show positive or negative impact on the 
BPT effect are listed below. 

Super-ordinate firms 
 (+)Managements' will to change business process 
according to business environmental change. 
 (+)Business - IT alignment function. 
 (+)Evaluation criteria for external resources. 
 (-)Provision for IT risk. 
 (+)System users’ division has a key role at the 
meeting. 
 (+)Meeting is held at IT division’s office. 
 (+)It is not difficult to estimate the BPT 
effectiveness of each division. 

Other firms 
 (+)Involvement of business management in the IT 
strategy decision process 
 (+)System users’ division has a key role at the 
meeting. 
     “Managements’ will to change business process 
according to business environmental change” and “Business 
- IT alignment function” are the items discussed at CSF of 
business process orientation (BPR, BPI, BPT). They can be 
considered the common enablers of BPT success.  On the 
other hand, “Evaluation criteria for external resources”, “It 
is not difficult to estimate BPT effectiveness of each 
division” and negative impact of “Provision for IT Risk” 
can be considered as items fitting the unique situation of 
firms in Japan. 
  Having “evaluation criteria for external resources” as an 
enabler, firms can manage outsourcing resources, which 
have more weight in Japan. Having “It is not difficult to 
estimate BPT effectiveness of each division” as an enabler, 
a firm can adjust the requirement of the divisions and avoid 
conflict. Not having “provision for IT risk” means taking IT 
risks (preparing for IT risk) must be an enabler to achieve 
the effectiveness of new technology. 
Figure 9 shows operating income on sales by “super-
ordinate” firms and other firms. “Super-ordinate” firms look 
a little superior to other firms. They will be considered to 
achieve much greater effectiveness by continuing their 
transformation using their enablers. 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Others (n=258)
Super-ordinate (n=26)
Operating imcome on sales
under 0%
0 - 3 %
3 - 6 %
6 - 10 %
10 - 13%
13 - 16 %
above 16 %
Figure 9. Operating income on sales 
V. CONCLUSION AND FUTURE WORK
The aim of this paper was to present the analysis result 
of research that focuses on the enablers of business process 
transformation in Japan.  At the time we constructed the 
conceptual research framework and hypotheses, enabler 
items were considered to consist of BPT management, 
executing 
drastic 
transformation, 
and 
wide-scope 
transformation which helps to achieve BPT effectiveness. 
However, from the research results, executing drastic 
transformation and wide-scope transformation seemed to be 
19
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

conflicting items. Then, we focused on the group that we 
had defined as “super-ordinate firms”, and a difference was 
seen between “super-ordinate” firms and other firms in 
some categories.(e.g., managements' will to change business 
process according to business environmental change, 
business - IT alignment function, evaluation criteria for 
external resources, system users’ role.) These items can be 
considered as the enablers of BPT for Japanese firms. The 
item “evaluation criteria for the external resources” must be 
important for many firms, not limited to Japanese firms. 
However, Japanese firms rely far more on system engineers 
of outsourcing services, compared to the United States. That 
means, evaluation criteria for the external resources must be 
one of the crucially important items for the Japanese firms. 
The enablers we had mentioned in this paper would help the 
firms in Japan who are struggling to realize drastic 
transformations. For the future research, we plan to delve 
more deeply into these enabler items, by interviews. Trust 
can be considered one of the key factors of business 
transformation effect [19]. So, we are planning to focus on 
the process of developing trust in business process 
transformation, and will analyze the differences between the 
super-ordinate firms and other firms. 
ACKNOWLEDGMENT
This work was supported in part by a JSPS Grant-in-Aid 
for Scientific Research in Japan (16K03819).  
REFERENCES
[1]
M. Hojo, “Recent Issues in the Japanese Labor Market  (in 
Japanese)”, Journal of international studies, School of 
Internationa Studies, Kwanse University, 2017.  
[2]
Organizsation for Economic Co-operation and Development 
(OECD), OECD Compendium of Productivity Indicators 
2016. 
[3]
Japan 
Productivity 
Center 
Cabinet 
Office 
Japan, 
Rodoseisansei 
no 
Koksai 
Hikaku 
2016 
Nenndoban 
(International Comparison of labor productivity 2016, in 
Japanese) .                                                     
http://www.jpc-
net.jp/intl_comparison/intl_comparison_2016.pdf. 
[4]
T. Higano, “IT ni Yoru Work Style Henkaku (Work Style 
Innovation, in Japanese)”, IT Solution Frontier 2009.11, 
Nomura Research Institute, 2009, pp. 16-19. 
[5]
A.Khosravi, “Business process rearrangement and renaming: 
A new approach to process orientation and improvement”, 
Business Process Management Journal, vol. 22 Issue: 1, 
Emerald Group Publishing Limited, 2005, pp. 116-139. 
[6]
K. Iizuka, T. Okawada, M. Tsubone, Y. Iizuka and C. 
Suematsu, C., “Issues about Inter-organizational Process Flow 
Adjustment in Business Process Modeling”, Enterprise and 
Organizational Modeling and Simulation, Selected Papers, 
vol.53, LNBIP, Springer, 2013.pp. 24-41. 
[7]
M. Zairi, “Business Process Re‐engineering and Process 
Management: a Survey of Current Practice and Future Trends 
in Integrated Management”, Management Decision, vol. 33 
Issue: 3, MCB UP Ltd., 1995, pp. 3-16. 
[8]
S. Mohapatra and A. Choudhury, “Readiness Framework for 
Business Process Re-engineering”, Strategic Change, John 
Wiley & Sons Ltd, Volume 25, Issue 5, September 2016, pp. 
509–524. 
[9]
P. Bradley, J. Browne, S. Jackson and H. Jagdev, “Business 
Process Re-engineering (BPR) — A study of the Software 
Tools Currently Available”, Computers in Industry, Volume 
25, Issue 3, March 1995, pp. Elsevier B.V.,pp. 309-330. 
[10] H. S. B. Jayatilake, A. S. Withanaarachchi and S. Peter, 
“Possibility of Applying Industry 4.0 as a Business Process 
Re-engineering Tool: Case Study from an Apparel Production 
Plant”, Proceedings of the International Research Symposium 
on Pure and Applied Sciences (IRSPAS 2016), Faculty of 
Science, University of Kelaniya, Sri Lanka. 2016, p 84. 
[11] P. Trkman, “The Critical Success Factors of Business Process 
Management”, 
International 
Journal 
of 
Information 
Management, Volume 30, Issue 2, April 2010, Elsevier 
B.V.,pp. 125–134. 
[12] K. Iizuka, Y. Iizuka and C. Suematsu, “Consideration of the 
Business Process Re-Engineering Effect: Business Flow 
Notation Structure and the Management Perspective”, 
Reichert M., Reijers H. (eds) Business Process Management 
Workshops. BPM 2015. Lecture Notes in Business 
Information Processing, vol 256. Springer, 2016, pp. 323-333. 
[13] M. N. Haviv,  “Understanding Critical Success and Failure 
Factors of Business Process Reengineering”, International 
Review of Management and Business Research, Volume 2, 
Issue.1,  March 2013, pp. 1-10. 
[14] G. Hall, J. Rosenthal and J. Wade, “How to make 
reengineering really work”, Harvard Business Review, 
November-December, 1994, pp. 119-131. 
[15] J. Peppard and D. Fitzgerald, “The Transfer of Culturally-
Grounded Management Techniques: the Case of Business 
Process, reengineering in Germany.” European Management 
Journal Volume 15, Issue 4, 1997,pp.446-460. 
[16] Y. Takei, R. Nagase and K. Iizuka,  “Consideration on 
Achieving Effectiveness by Using ERP Systems: From the 
Analysis of Satisfaction Structure, International Symposium 
on Business Management (ISBM),2014, pp.1035-1053. 
[17] Ministry of Economy, Trade and Industry (METI), IT 
Keieiryoku Sihyo wo Mochiita IT no Rikatuyou ni Kansuru 
Genjou Chousa (Current Status Survey on IT Utilization 
Stage Evaluated by IT managerial Capability Index, in 
Japanese), 2010. 
[18] K. Motohashi, IT to Seisansei ni kansuru jisshou bunseki 
(Anallysis on relationship between IT and Productivity, in 
Japanse), Institute of Economy, Trade and Industry,2010.  
[19] K. Iizuka and C. Suematsu, “Structure of the Trust: Seeking 
the Real Effectiveness of Business Process Transformation 
Projects”, International Social Sciences & Technology 
Conference (ICTECH) 2017, pp. 1-5. 
20
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Automated Analysis of Patient Experience Text Mining using a Design Science 
Research (DSR) Approach 
Mohammed Bahja 
IRAC Group 
School of Computer Science and Technology 
University of Bedfordshire 
Luton, United Kingdom 
Email: mohammed.bahja@beds.ac.uk   
Manzoor Razaak 
WMN Group 
Kingston University 
London, United Kingdom 
Email: manzoor.razaak@kingston.ac.uk 
 
 
Abstract— Online forums of hospitals are a common method of 
collecting patient feedback on the healthcare received. The 
feedback data obtained are often free text and large which may 
make a manual analysis of the data difficult and time-
consuming. An approach to automatically analyse patient 
experience data would be beneficial for the hospital staff in 
several ways. In this paper, a Design Science Research (DSR) 
paradigm based framework is proposed that is used for our 
ongoing research in developing solutions with an aim for an 
automated approach to analyse patient experience data using 
natural languages processing techniques such as Sentiment 
Analysis, Topic Modelling, and Dependency Parsing. The 
framework design proposed provides a three-stage iterative 
process wherein at each iteration the patient feedback is deeply 
analysed based on the outcomes obtained from the preceding 
ones. This iterative approach facilitates the development of a 
strong, effective patient feedback analysis system. 
Keywords-patient 
experience; 
sentiment 
analysis; 
text 
mining; topic modelling; DSR 
I. 
 INTRODUCTION 
Understanding patient experience enables hospitals to 
identify their weaknesses in providing healthcare. It provides 
opportunities for them to reflect on their functioning and 
thus, make efforts towards addressing the limitations in 
relation to the service provided. Further, understanding the 
patient experience can also contribute towards making the 
hospital processes more efficient, which in turn will lead 
towards better utilisation of resources and the addressing of 
patient concerns. 
The National Health Service (NHS) provides health care 
within England, Scotland and Wales and is a public health 
service established shortly after the Second World War and 
caters for all residents of the UK. The NHS describes patient 
experience as a core dimension of good quality care [1]. It is 
an imperative task for the NHS to measure patient 
experience in order to monitor and improve health care 
performance, enhance strategic decision making and record 
progress for health care organisations [2]. 
An effective method of collecting a patients’ feedback is 
to provide an online forum, where they can anonymously 
provide their views on the health care service they have 
received on a website feedback form at their own 
convenience. In 2008, the NHS created a website called NHS 
Choices that invites patients to leave their feedback about 
their experience with the healthcare providers of the hospital, 
which they can do using two approaches. One method is to 
provide ratings to a given hospital on different metrics on a 
scale of 1 to 5 stars. Moreover, in addition to rating their visit 
on different metrics, they can also leave feedback in the form 
of comments in the comments section of the website. Thus, 
the NHS has a large database of patient feedback covering 
most hospitals across the United Kingdom (UK). The 
number of comments and feedback across the database runs 
into the hundreds of thousands.  
The process of collecting patient feedback via website 
source provides the healthcare professionals easy and 
convenient access to patient feedback data that can be used 
for analysis purposes. The hospital administration and 
researchers can use the feedback database to identify the 
strengths and weaknesses of their healthcare service and 
further employ them to address the identified issues. 
Online reviews are generally written as free text and do 
not adhere to any structure or format. This makes analysing 
and understanding the patient experience more challenging 
than when dealing with closed questions, as the possibilities 
regarding the feedback content are endless. Techniques, such 
as keyword searching can help in searching for topics in 
patient feedback; however, this does not recognise positive 
and negative feedback. To address the challenge of analysing 
a large patient feedback database, automated methods would 
enable the analysis process to be more sustainable and time 
effective. The recent advancements in information analysis 
technologies, such as text mining and natural language 
processing, has resulted in them being widely applied to 
analyse the user experience of products by various 
companies.  
In the last few years, studies have applied sentiment 
analysis approaches for analysis of patient experience data. 
Studies such as [6] and [7] applied SVM, decision trees, and 
Naive Bayes approaches of Sentiment Analysis and were 
able to classify patient comments into complaints/praise 
attributed to specific staff as well as feedback about other 
aspects, such as access, wait time, privacy, facilities, etc. In 
[8], the authors analysed the patient experience data provided 
by US hospitals on Twitter. They assessed tweeted reviews 
for over 2,000 US hospitals and identified those related to 
patient experience. Their preliminary results showed that the 
patient experience present on the tweets did not match that 
21
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

regarding the Hospital Consumer Assessment of Healthcare 
Providers and Systems (HCAHPS) ratings. However, there 
are limited studies that have explored how such language 
processing methods could be utilised to analyse patient 
experience. 
In this paper, the ongoing research on applying natural 
language processing to provide a framework with an aim for 
automated analysis of a patient experience is introduced. The 
methodology that is being used in the ongoing research is 
explained in detail in Section II. Section III describes our 
developed framework for the research followed by 
conclusion in Section IV. 
II. 
THE DESIGN SCIENCE RESEARCH (DSR) PARADIGM 
 The Design Science Research (DSR) methodology is 
currently being used for our research. The DSR paradigm is 
a widely popular research approach in Information Systems 
(IS) research. It is referred to as a problem-solving paradigm 
because it aims at building “artefacts” that are aimed at 
addressing a problem. The artefacts address the problems or 
enhance existing solutions and are important tools for 
arriving at research outcomes and reviewing them to decide 
how the artefact adopted can be further utilised [3]. The DSR 
process follows a systematic procedure in which the artefacts 
are developed with systematic creation, capturing, and 
communication of knowledge from the design process. DSR 
uses an iterative process, whereby the artefacts are 
reconstructed at each iteration and thus, can be described as a 
continuous learning process that enhances the artefacts 
quality incrementally [4]. 
Vaishnavi & Kuechler developed a methodology for 
DSR as shown in Figure 1 [5]. The model puts more 
emphasis on the process of contributing to knowledge. The 
DSR process here is based on the knowledge built and 
comprises five main stages:  Awareness of Problem, 
Suggestion, Development, Evaluation, and Conclusion. 
The awareness of problem aims at understanding the 
problem in the context of the application using various 
resources available. The outcome of this process leads to the 
development of the suggestion of the research project. In the 
suggestion phase, various insights into the application 
domain are obtained during this phase and the specifications 
for the solutions are acquired which leads to the development 
of a tentative design. In the development stage, the first 
artefact is developed. The evaluation phase focuses on 
evaluating the performance of the developed artefact, the 
outcomes of which may be used to further improve the 
artefact design and performance. 
In our research, the aim is to develop an automated 
analysis of patient feedback to identify their sentiment and 
opinions 
about 
the 
healthcare 
service. 
The 
DSR 
methodology is well suited for us to achieve this, as DSR is 
an iterative process and our artefact developed will be a three 
stage iterative process wherein at each iteration the patient 
feedback is deeply analysed based on the outcomes obtained 
from the preceding ones. This iterative approach facilitates 
the development of a strong, effective automated patient 
feedback analysis system. In the following section, the DSR 
methodology based framework of our research is presented. 
III. 
A DSR BASED FRAMEWORK FOR AUTOMATED 
PATIENT EXPERIENCE ANALYSIS  
To develop a patient feedback analysis system that can 
potentially automate the process, the five-phase design 
process steps mentioned in Figure 1 is adapted and used as 
the DSR approach for the study. The study will be carried 
out in three iterations and the adapted DSR approach 
enables the identification of the problem in each iteration, 
finds a solution, develops and evaluates the performance of 
the solution methodically and hence, is suitable for the 
research goals.  The DSR methodology of our study is 
illustrated in Figure 2. 
 
 
Figure 1. Design Science Research Phases (Kuechler, Park and Vaishnavi, 2007)
22
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

 
Figure 2: The proposed DSR based framework for automated analysis of patient experience data 
 
A. 
Iteration One: Sentiment Analysis of Patient 
Feedback Data 
The first iteration of our ongoing research is to apply 
Sentiment Analysis (SA) approaches to the NHS Choices 
patient feedback database. The aim of this iteration is to 
find the sentiment of the patient feedback in their 
comments provided on NHS hospitals on the NHS 
website. The sentiment identification is performed by the 
application of Sentiment Analysis approaches that analyse 
and classify a comment into positive or negative sentiment 
based on its contents. 
It can be seen that the problem is the need to identify 
the patient sentiment from the comments obtained for each 
NHS hospital from the NHS Choices website. The 
suggestion is to develop an artefact that can automatically 
or semi-automatically identify the patient feedback or 
sentiment in the comment as positive or negative. To 
develop this artefact, there are several sub-steps. The first 
is to identify suitable Sentiment Analysis (SA) models 
from the literature research that can identify the sentiment 
in the comment. The next sub-step is to design the 
experiment settings where the SA models are implemented 
on the feedback database. This involves identifying the 
processes for cleaning and preparing the database for the 
analysis, tools and technology required to implement the 
SA models. 
The next step is to evaluate the outcomes of the SA 
model implementation. The evaluation is an analytical 
approach in this iteration, whereby it is focused on the 
accuracy of the SA model in identifying the correct 
sentiment in the patient feedback. If the sentiment 
identified by the SA model matches the ground truth 
sentiment, then the SA model is said to be accurate. The 
outcomes of this iteration will be used for the second 
iteration of the study. 
B. 
Iteration Two: Topic Identification from 
Patient Feedback 
The next iteration of the study is to identify topics in 
the given patient feedback. The aim is to understand what 
specific area of the healthcare service the patient has 
discussed in their feedback, such as the maternity 
department, parking facilities or the waiting period in the 
hospital. In Figure 2, it can be observed that the DSR 
framework for iteration two is more or less similar to the 
iteration one approach. The problem in this iteration is to 
identify the topic being discussed by the patient in their 
feedback about the hospital on the NHS website. 
Therefore, in line with the problem, the artefact aimed at 
providing a model that can potentially automatically 
identify the topics or themes being discussed by the patient 
in a given comment. 
To develop the artefact, the Latent Dirichlet Allocation 
(LDA) Topic Modelling (TM) approach will be used. 
After the TM approach is implemented, and then the topics 
and themes for each patient comment are automatically 
identified. This reveals which topics are being discussed in 
the patient feedback. The next step is to identify the 
sentiment for each topic identified in the patient comment. 
To achieve this, the sentiment identified for each comment 
from the SA models from iteration one is used i.e. for each 
comment, the identified sentiment is mapped to the 
relevant topics to obtain a sentiment score for that topic 
across the patient feedback database. A difference in 
iteration two when compared to iteration one during the 
development stage is that the iteration two study requires 
the outcomes of the SA model obtained from the first 
iteration, i.e. second iteration of the study is dependent on 
the previous iteration. 
The evaluation of the topics identified in iteration two 
involves both analytical and observational evaluation. The 
former refers to when the sentiment score for each topic is 
computed based on the sentiment scores found from the 
23
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

iteration one. The observational part of the evaluation 
pertains to when the topics are identified by the TM 
method. The topics are bags of words that are likely to 
belong to a particular theme and they need to be manually 
analysed in order to assign a label to each of them. This is 
achieved by involving NHS experts to observe each 
topic’s elements and provide a label for each category.  
The outcomes of iteration two will be disseminated by 
representing the topics identified using data visualisation 
approaches in the R programming environment to make 
them accessible to hospital staff. Finally, the topics 
identified by the TM approach in iteration two will be used 
for further study and analysis in iteration three for the 
Dependency Parsing investigation aimed at identifying the 
rationale behind the patient sentiment. 
C. 
Iteration Three: Rationale Identification of 
Patient Sentiment 
In the third iteration, the aim is to identify the rationale 
behind a particular sentiment of the patient in a given 
comment or review. In other words, the purpose is to find 
out why the patient is happy or unhappy about a particular 
topic in a given comment. The problem being addressed in 
this iteration of the study is to find the possible reason 
behind a patients’ sentiment for a particular topic in a 
given comment. To achieve this, the artefact developed 
will implement a Natural Language Processing method 
called Dependency Parsing. This identifies the keywords 
in a given comment that could potentially summarise the 
patient feedback for a given topic in the comment. 
Specifically, this is achieved by extracting a “noun-
adjective” pair for each topic in a given comment and this 
pair are expected to provide the keywords in the comment 
that indicate the reason behind the patients’ happiness or 
unhappiness about the healthcare service provided. 
Similar to the previous two iterations, once the suitable 
Dependency Parsing methods are identified and the 
implementation environments are finalised, the experiment 
settings are created and implemented.  For this iteration, 
the R programming environment will be utilised and the 
openNLP and coreNLP methods available in the literature 
will be adopted. Further, similar to how iteration two was 
dependent on the outcomes of the iteration one, iteration 
three is also dependent on those from the two preceding 
iterations. This is because the “noun-adjective” pair that is 
extracted to summarise the patient comment is performed 
on a per topic basis. In other words, for each topic, the 
reason behind the patient sentiment is identified. In 
particular, the unigram topics identified from the TM 
method are used for Dependency Parsing in this study.  
The evaluation of the topics identified in iteration three 
is both analytical and descriptive. The analytical 
evaluation refers to when the Dependency Parsing 
methods implemented automatically analyse the feedback 
data and identify a noun-adjective pair from the comments. 
The descriptive part of the evaluation pertains to the noun-
adjective pair identified providing a summary of the 
patient comment on a given topic. Thus, it potentially 
describes the patient comment by a pair of words. 
Similar to iteration two, the outcomes of iteration three 
will be disseminated via visualisation techniques. In this 
visualisation, the users will be able to list all the comments 
for a chosen topic and then for each comment, they will be 
able to visualise the most salient noun-adjective pair that 
potentially summarises the comment for the given topic, 
either negatively or positively. 
Thus, it can be noted that the DSR approach is deemed 
suitable to design a framework for our study as it provides 
a systematic approach to formulating the problem, 
identifying potential solutions, implementing and testing 
the solutions and finally, evaluating and disseminating the 
outcomes. The iterative approach is a strong aspect of the 
DSR method and is particularly suitable for the current 
study as each part of the study is dependent on the 
outcomes of its predecessor study. 
IV. 
CONCLUSION 
The DSR method is an iterative process that enables 
the development of an artefact for solving a problem. It is 
suitable for our research, as the main goal is developing an 
artefact for analysing patient experience automatically. 
Our artefact will be developed in three iterations and each 
successive iteration study is dependent on the outcomes of 
the previous iteration. This approach would enable us to 
develop an effective framework to analyse the patient 
experience data and can be beneficial to hospitals to assess 
their performance in achieving high patient satisfaction. 
REFERENCES 
[1] S. LaVela and A. Gallan, “Evaluation and measurement 
of patient experience,” Patient Experience Journal, vol.1, no. 1, 
pp. 28-36, 2014.  
[2] A. Coulter, L. Locock, Z. S. and J. Calabrese, “Collecting 
data on patient experience is not enough: they must be used to 
improve care,” BMJ: British Medical Journal, p. 348, 2014.  
[3] K. Peffers, T. Tuunanen, C. Gengler, M. Rossi, W. Hui, 
V. Virtanen and J. Bragge, “ The design science research process: 
a model for producing and presenting information systems 
research,” In Proceedings of the first international conference on 
design science research in information systems and technology , 
2006.  
[4] S. Gregor and A. Hevner, “Positioning and presenting 
design science research for maximum impact,” MIS quarterly, 
vol. 37, no. 2, pp. 337-355, 2013. 
[5] V. Vaishnavi and W. Kuechler, Design Science Research 
methods 
and 
Patterns: 
Innovating 
Information 
and 
Communication Technology, New York: 1st edn. Taylor & 
Francis Group., 2007. 
[6] F. Alemi, M. Torii, L. Clementz, L. and D.C. Aron, 
'Feasibility of real-time satisfaction surveys through automated 
analysis of patients' unstructured comments and sentiments', 
Quality Management in Healthcare, 21(1), pp. 9-19, 2012. 
[7] F. Greaves, D. Ramirez-Cano, C. Millett, A. Darzi, and L. 
Donaldson, 'Machine learning and sentiment analysis of 
unstructured free-text information about patient experience 
online', The Lancet, 380, pp. S10, 2013. 
[8] C. Lees, 'Measuring the patient experience', Nurse 
researcher, 19(1), pp. 25-28, 2011. 
24
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Requirement-Driven Architecture for Service-Oriented e-Learning Systems  
Rawad Hammad 
King’s College London 
London, UK 
Email: Rawad.Hammad@kcl.ac.uk  
 
Abstract—The 
continuous 
evolving 
of 
Technology 
Enhanced Learning (TEL) requirements, more specifically 
Functional Requirements, increases the complexity of TEL 
software system since such requirements cannot be met by one 
TEL/e-learning solution. In addition to the traditional Virtual 
Learning 
Environments/Learning 
Management 
Systems 
capabilities, such Functional Requirements include: video 
streaming, plagiarism checker for students’ submissions, e-
portfolio, etc. Therefore, combining various e-learning 
software systems, solutions, or tools seems more realistic. 
However, a limited effort has been done to investigate and 
control the impact of combining different solutions on the 
quality, i.e., Non-Functional Requirements (NFRs), of the 
overall e-learning software system. This paper proposes a new 
approach to elicit, precisely specify, and manage NFRs for 
TEL software systems. To meet these capabilities (i.e., 
Functional Requirements and Non-Functional Requirements), 
this paper also proposes a flexible service-oriented architecture 
for e-learning systems. The proposed list of NFRs is 
comprehensive 
and 
can 
be 
customized 
to 
various  
e-learning systems to meet stakeholders’ requirements. 
Moreover, the proposed architecture needs to be further 
developed to test its impact on TEL software systems in real 
scenarios.  
Keywords-Technology 
Enhanced 
Learning; 
e-learning; 
architecture; 
Non-Functional 
Requirements; 
Software 
architecture; SOA; Web Services. 
I. 
 INTRODUCTION 
The continuous rise of using eLearning in higher 
education 
increases 
the 
complexity 
of  
e-Learning/Technology Enhanced Learning (TEL) Software 
Systems [1]. On one hand, there is a continuous demand for 
various 
supplementary 
capabilities, 
more 
specifically 
Functional Requirements, that cannot be met by one TEL/ 
e-learning software system only. For instance, in addition to 
the traditional Virtual Learning Environments (VLE) 
capabilities, various supportive capabilities are required (e.g., 
video 
streaming, 
plagiarism 
checker 
for 
students’ 
assignments, e-portfolio, etc. Therefore, combining various 
e-learning software systems, solutions, or tools seems more 
realistic. On the other hand, a limited effort has been done to 
investigate the impact of combining different solutions on 
Non-Functional Requirements (NFRs) of the overall e-
learning service or software system. Such Non-Functional 
Requirements include performance, reliability, availability, 
recoverability, etc.  
Literature 
evidence 
shows 
that 
Non-Functional 
Requirements are not properly managed over the Software 
Development Life Cycle (SDLC) [2]. This applies to NFRs 
elicitation, specification, documentation, and evaluation. One 
of the potential reasons behind this is related to the nature of 
applying TEL solutions in higher education institutions as 
they focus on Functional Requirements at the expense of 
Non-Functional Requirements. Also, there is a lack of 
literature evidence on how NFRs are elicited and specified. 
Most of the e-learning systems evaluation is performed 
against the Functional Requirements only (e.g., [3] and [4]). 
Moreover, NFRs subtle nature makes them challenging to 
elicit in advance, and most likely to be approached iteratively 
along software development life cycle [5]. NFRs are very 
important to software architecture; they are also known as 
Architecturally Significant Requirements because they have 
a measurable impact on the architecture of software system 
[6] [7].  
Therefore, this paper investigates the current approaches 
to manage, more specifically elicit and specify, NFRs over 
TEL software development life cycle. NFRs management 
refers to the process of eliciting, specifying, communicating, 
and controlling Non-Functional Requirements over software 
development life cycle [8]. Since, NFRs are persistent, rarely 
changed, this paper focuses on the early stages of NFRs 
management process. These stages include NFRs elicitation, 
specification, and communication. Then, it proposes a 
flexible architecture for e-learning solutions to meet the 
early-identified NFRs. The rest of this paper is organized as 
follows. Section II summarizes related work; Section III 
proposes a new approach to elicit and specify NFRs for TEL 
systems; Section IV proposes a service-oriented architecture 
for e-learning software systems; Section V concludes the 
paper with future research directions. 
II. 
RELATED WORK 
There exist different e-Learning/TEL Software Systems, 
where some of them are: (i) open source, such as: Moodle 
[3], Atutor [9], Sakai [10], and Ilias [11] or (ii) propriety, 
such as: Blackboard [12] and Desire2Learn [13]. Such 
systems are known as Learning Management Systems 
(LMSs) or Virtual Learning Environments (VLEs). The 
current LMSs/VLEs cannot support architectural flexibility, 
to different extents, due to their monolithic design [14]. 
LMSs evolved from black box systems, known as first 
generation LMSs, towards more modular architectural 
approach, known as second generation LMSs [14]. Much of 
this evolution was due to the standardization initiatives, such 
as: Sharable Content Object Reference Model (SCORM) and 
IMS Global Learning Consortium Learning Design (IMS 
LD), which allow good level of interoperability between 
different 
LMSs, 
their 
components, 
and 
third-party 
plugins/tools. 
For 
instance, 
IMS 
Learning 
Tools 
25
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Interoperability 
(LTI) 
is 
used 
by 
many  
e-learning tools to align or map their configurations with 
LMS configurations. Meanwhile, various architecture-
oriented improvements on Atutor LMS have been introduced 
[9]. Similarly, more modular structure has been considered in 
the case of Sakai LMS, especially in relation to service 
orientation [15]. This has increased the scalability and 
extendibility of the current LMSs via plugins deployment. 
However, such structure is not sufficiently agile. New 
requested plugin needs planning and deployment procedures 
and might have impact on systems performance or other 
related NFRs. In addition, the recent move towards cloud-
based e-learning solutions, especially Software as a Service 
(SaaS), made it more challenging for the current  
e-learning systems to effectively exchange assets and 
efficiently co-exist with each other (e.g., sharing hardware 
resources). Hence, the next section will present a 
comprehensive and consistent approach to elicit, specify, and 
communicate NFRs in relation to TEL solutions to consider 
them later to design a flexible architecture for TEL systems.   
III. 
NON FUNCTIONAL REQUIREMENTS IN TEL 
In the light of the above discussion, a good starting point 
for architecting e-learning solutions is to thoroughly consider 
its NFRs in a consistent way. Our approach is inspired by 
one of the most reliable approaches to elicit NFRs, which is 
the Quality Attribute Workshop (QAW) approach [16], 
developed by Carnegie Mellon University Software 
Engineering Institute. Simply, this approach refers to 
engaging system stakeholders, or their representatives, early 
in the life cycle to discover the driving Non-Functional 
Requirements of software system through a series of 
workshops. Unlike QAW structure that has a rigid structure 
and lacks the base definitions for NFRs, we opt for a flexible 
structure for our proposed approach. The structure of the 
proposed approach must address the following phases: (i) 
induction phase, to introduce the approach to stakeholders, 
or their representatives, and explain the rationale behind it 
and who is doing what, (ii) business view phase, to introduce 
high-level Functional Requirements for the proposed 
solution, (iii) architecture view phase, to present the 
proposed solution architecture at a high-level including 
useful details (e.g., hardware, certain technologies, etc.), (iv) 
architectural drivers phase, to summarise the key drivers of 
the proposed solution, which could be high-level capabilities, 
organizational concerns, etc., (v) scenario phase, to divide 
the audience into groups to brainstorm real scenarios for 
using the systems, and to validate them, and link them with 
NFRs, and finally (vi) precisely specify TEL software system 
NFRs based on the NFRS template introduced later in this 
section (i.e., Tables I and II). Precise specifications of NFRs 
means to pick up the definition listed in Table I or II, and to 
add certain parameters to the definition as explained later. 
The proposed phases could be conducted as separate 
workshops, meetings, interviews or other potential formats, 
which can be better decided by organisational business 
analysts. Also, phases can be merged together or divided into 
two or more depending on the context factors that include: 
the scale of the TEL software system, nature of stakeholders, 
their technical background and interest, etc. The key role of 
the business analyst team is to maximise the benefits of 
stakeholders’ engagement to get accurate enough NFRs 
specifications. One of the central steps here is to avoid 
natural language-based specification as this may lead to 
subtle 
requirement 
specifications, 
which 
cannot 
be 
measured. To do so, we opt for a standard-based approach 
based on ISO 25010 Systems and Software Quality 
Requirements and Evaluation: (i) Product Quality (PQ) 
Model that measures the static qualities of a certain software 
system, depicted in Figure 1, and (ii) Quality in Use (QiU) 
Model that measures the dynamic qualities of a certain 
software system when it is applied in a particular context 
[17], depicted in Figure 2. Both models have a set of 
precisely defined list of qualities that can be easily 
customised to be smart enough for architecting e-learning 
solutions. In this context, smart means: specific, measurable, 
achievable, resource and time bound. Also, using a standard 
coherent set of precisely defined quality characteristics is 
appropriate for negotiation with industries, especially in 
Service Level Agreements for SaaS solutions. 
 
 
Figure 1.   System-related Non-Functional Requirements 
The above-depicted NFRs are organised as characteristic 
(e.g., compatibility) and sub-characteristics (e.g., co-
existence and interoperability). The former provides a 
general definition that does not need to be smart, while the 
latter (i.e., sub-characteristics) needs further customisation to 
be smart NFRs. All the above-mentioned product quality-
related NFRs are defined [18] in Table 1 below. For 
readability purpose, different background colour has been 
given to characteristics (e.g., performance efficiency), while 
sub-characteristics (e.g., time-behaviour) background colour 
is white. 
TABLE I.  
SYSTEM-RELATED NON-FUNCTIONAL REQUIREMENTS 
Characteristic/ 
Sub-characteristic Definition 
Performance 
efficiency  
performance relative to the amount of resources used 
under stated conditions. 
Time-behaviour  
degree to which the response and processing times and 
throughput rates of a system, when performing its 
functions, meet requirements. 
Resource 
utilisation 
degree to which the amounts and types of resources 
used by a system, when performing its functions, meet 
requirements. 
Capacity 
degree to which the maximum limits of a system 
parameter meet requirements 
Compatibility  
degree to which a system or component can exchange 
26
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

information with other systems or components, and/or 
perform its required functions, while sharing the same 
hardware or software environment. 
Co-existence 
degree to which a system can perform its required 
functions 
efficiently 
while 
sharing 
a 
common 
environment and resources with other products, without 
detrimental impact on any other product. 
Interoperability 
degree to which two or more systems or components 
can exchange information and use the information that 
has been exchanged. 
Usability  
degree to which a system can be used by specified users 
to achieve specified goals with effectiveness, efficiency 
and satisfaction in a specified context of use. 
Learnability  
degree to which a system can be used by specified users 
to achieve specified goals of learning to use the product 
or system with effectiveness, efficiency, freedom from 
risk and satisfaction in a specified context of use. 
User error 
protection  
degree to which a system protects users against making 
errors. 
User interface 
aesthetics 
degree to which a user interface enables pleasing and 
satisfying interaction for the user. 
Accessibility  
degree to which a system can be used by people with 
the widest range of characteristics and capabilities to 
achieve a specified goal in a specified context of use. 
Reliability  
degree to which a system or component performs 
specified functions funder specified conditions for a 
specified period of time. 
Availability  
degree to which a system or component is operational 
and accessible when required for use. 
Fault tolerance  
degree to which a system or component operates as 
intended despite the presence of hardware or software 
faults. 
Recoverability  
degree to which, in the event of an interruption or a 
failure, a system can recover the data directly affected 
and re-establish the desired state of the system. 
Security 
degree to which a system protects information and data 
so that persons or other products or systems have the 
degree of data access appropriate to their types and 
levels of authorization. 
Confidentiality  
degree to which a system ensures that data are 
accessible only to those authorized to have access. 
Integrity  
degree to which a system or component prevents 
unauthorized access to, or modification of, computer 
programs or data. 
Non-repudiation  
degree to which actions or events can be proven to have 
taken place, so that the events or actions cannot be 
repudiated later. 
Accountability  
degree to which the actions of an entity can be traced 
uniquely to the entity. 
Authenticity 
degree to which the identity of a subject or resource can 
be proved to be the one claimed. 
Maintainability  
degree of effectiveness and efficiency in which a 
system can be modified by the intended maintainers. 
Modularity 
degree to which a system is composed of discrete 
components such that a change to one component has 
minimal impact on other components. 
Reusability 
degree to which an asset can be used in more than one 
system, or in building other assets. 
Analysability  
degree of effectiveness and efficiency in which it is 
possible to assess the impact on a system of an intended 
change to one or more of its parts, or to diagnose a 
product for deficiencies or causes of failures, or to 
identify parts to be modified. 
Modifiability 
degree to which a system can be effectively and 
efficiently modified without introducing defects or 
degrading existing product quality. 
Testability  
degree of effectiveness and efficiency in which test 
criteria can be established for a system or component 
and tests can be performed to determine whether those 
criteria have been met. 
Portability  
degree of effectiveness and efficiency in which a 
system or component can be transferred from one 
hardware, software or other operational or usage 
environment to another. 
Adaptability  
degree to which a product or system can effectively and 
efficiently be adapted for different or evolving 
hardware, software or other operational or usage 
environments. 
Installability  
degree of effectiveness and efficiency in which a 
system can be successfully installed and/or uninstalled 
in a specified environment. 
Replaceability  
degree to which a system can replace another specified 
software product for the same purpose in the same 
environment. 
  
Following 
the 
above-listed 
system-oriented 
Non-
Functional Requirements, another complementary set of 
NFRs is needed to specify the system behaviour in a certain 
context. Such NFRs describe the quality to which the 
anticipated system can be used by specific users to meet their 
demands to achieve specific goals with effectiveness, 
efficiency, freedom from risk, and satisfaction in specific 
contexts of use [18]. This complementary list is depicted in 
Figure 2, and fully described in Table II, as well. Like 
system-related NFRs, this list is organised as characteristics 
and sub-characteristics, where the former provides a generic 
description that does not need to smart, while as the latter 
needs to be refined to be smart NFRs. 
 
Figure 2. Quality in Use-related Non-Functional Requirements 
As depicted in Figure 2, this list is limited to the qualities 
that can be affected by the context of use. Context of use 
includes users, tasks, equipment (hardware, software, and 
material), and the physical and social environments in which 
a system is used [18]. Some of the system-related NFRs can 
be affected by the context of use, but generally they are not 
affected by the context of use. 
TABLE II.      QUALITY-IN-USE RELATED NON FUNCTIONAL REQUIREMENTS 
Characteristic/ 
Sub-characteristic Definition 
Effectiveness 
accuracy and completeness in which users achieve 
specified goals. 
Efficiency 
resources expended in relation to the accuracy and 
completeness in which users achieve goals. 
Satisfaction  
degree to which user needs are satisfied when a system 
is used in a specified context of use. 
Trust 
degree to which a user or other stakeholder has 
confidence that a system will behave as intended. 
Pleasure 
degree to which a user obtains pleasure from fulfilling 
their personal needs. 
Comfort 
degree to which the user is satisfied with physical 
comfort. 
Freedom of risk 
degree to which a system mitigates the potential risk to 
27
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

economic status, human life, health, or the environment. 
Economic risk 
mitigation   
degree to which a system mitigates the potential risk to 
financial 
status, 
efficient 
operation, 
commercial 
property, reputation or other resources in the intended 
contexts of use. 
Health and safety 
risk mitigation  
degree to which a system mitigates the potential risk to 
people in the intended contexts of use. 
Environmental risk 
mitigation 
degree to which a system mitigates the potential risk to 
property or the environment in the intended contexts of 
use. 
Context coverage  degree to which a system can be used with 
effectiveness, efficiency, freedom from risk and 
satisfaction in both specified contexts of use and in 
contexts beyond those initially explicitly identified. 
Context 
completeness  
degree to which a system can be used with 
effectiveness, efficiency, freedom from risk and 
satisfaction in all the specified contexts of use. 
Flexibility  
degree to which a system can be used with 
effectiveness, efficiency, freedom from risk and 
satisfaction in contexts beyond those initially specified 
in the requirements. 
 
Finally, the definitions of the above-mentioned NFRs 
(i.e., System-oriented and Quality-in-Use-oriented) are 
generic enough to accommodate NFRs specifications for a 
wide range of software systems. For effective TEL system 
architecture, these NFRs need to be refined to be smart. This 
means that various thresholds need to be added to these 
generic definitions based on the NFRs elicitation workshop, 
explained in Section III. For instance, Recoverability will 
have more specific numbers to describe the conditions in 
which the system can recover the data affected and re-
establish the desired state of the system. This means 
recoverability requirement specification will look like: “In 
the event of interruption or failure, the system must recover 
the data affected and re-establish the desired state of the 
system according to the following parameters: (i) Recovery 
Time Objective (RTO): 30 minutes and (ii) Recovery Point 
Objective (RPO): three hours. For clarification, RTO refers 
to time duration in which users/organisations want to be able 
to recover the replicated data, while RPO refers to the 
maximum amount of data that can be lost before causing 
serious damage to the organisational services. Similarly, 
Capacity NFR needs to be customised with a precise list of 
parameters, so the refined specification will look like: the 
system must be capable of effectively and efficiently 
performing its functions as expected in the case of having 
2500 concurrent users and hosting the contents/activities of 
80000 online courses. In this case, 2500 concurrent users and 
80000 courses represent the maximum parameters required 
by a certain institution. To respond to the early-identified 
NFRs, a high-level architecture for TEL software system will 
be proposed in the next section.  
IV. 
THE PROPOSED E-LEARNING SYSTEM ARCHITECTURE 
As introduced earlier, NFRs are known as architecturally 
significant 
requirements. 
Ideal 
software 
architecture 
describes the concerned software system through a set of 
artefacts and relationships between these artefacts. Such 
artefacts 
include 
models, 
processes, 
principles, 
and 
guidelines 
that 
guide 
the 
selection, 
creation, 
and 
implementation of software solutions aligned with business 
requirements. Furthermore, this includes decisions taken 
during building the high-level architecture of the software 
system, where these decisions have significant impact on the 
system quality, performance, availability, etc. [19]. This 
explains why software architecture is influenced by NFRs, 
and 
consequently, 
justifies 
investigating 
NFRs 
and 
architecture together. Literature evidence, especially [2], [5]-
[7], reveals that the key Non-Functional Requirements that 
influence software architecture decisions are: performance, 
compatibility including: co-existence and interoperability, 
maintainability 
especially 
reusability 
and 
modularity, 
adaptation, and flexibility. Moreover, lessons learned from 
current TEL practices in academic institutions, such as the 
heavy move towards cloud-based e-learning solutions, puts 
further emphasis on interoperability and co-existence 
requirements, because different cloud-based e-learning 
systems usually share the same hardware environments. 
Such requirements can be better met by flexible 
architecture, such as Service-Oriented Architecture (SOA) 
that is designed to support flexibility, interoperability, 
reusability, etc [6]. Therefore, we opt for a service-oriented 
enabled architecture for e-learning system, where the overall 
e-learning service is composed of more than one software 
system, such as LMS (e.g., Atutor, Moodle, etc.), plagiarism 
checker (e.g., Turnitin), video streaming (e.g., Kaltura), 
student record system, etc. Some of these systems might be 
developed in-house, hosted on premises, or provided as a 
SaaS. Figure 3 depicts the architecture of SOA-enabled e-
Learning System. As explained in Figure 3, the proposed 
architecture is composed of the following three layers: (i) 
presentation-service layer, that has the necessary set of 
interfaces to communicate with underneath layers, (ii) 
business layer, that includes all sub-systems or components 
(e.g., LMS and video streaming), and (iii) data layer that 
hosts every possible source of data. 
 
 
Figure 3. Service-enabled e-Learning System Architecture 
The first layer, presentation-service layer, includes the 
following four components. The first component, learning 
process interface, manages and monitors all learning and 
teaching processes carried out by students. Such processes 
mainly include LMS capabilities, such as finding learning 
28
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

contents and managing e-learning activities. To better 
facilitate this component’s job, there is a need to improve the 
architecture/design of the current LMSs via adding what we 
call here “Service-oriented component”. This component 
allows flexible access to the internal capabilities of the LMS 
(e.g., looking for certain contents of a particular course or 
analysing various activities done by a group of users for 
learning analytics purposes). This component needs 
significant changes to be introduced to the LMS stretching 
from architectural design of the intended LMS through 
specific algorithms that can identify and discover web 
services that meet users’ demands. 
The second component, pedagogy process interface, 
handles learning content through all of its stages (i.e., design, 
development, publishing, etc.) This can be done through 
interaction with Learning Content Management System 
(LCMS) component in the business layer. This component 
retains complex processes since designing learning contents 
includes various pedagogical approaches that considerably 
vary. For instance, designing behavioural-based content, 
which is instructor-centred contents that contain: a) learning 
objectives, b) learning contents, and c) assessment exercise is 
different from designing a social constructive-based content 
which is driven by students’ interactions. Both types of 
contents are based on underpinning pedagogical models that 
can be represented via a set of processes that explain the 
workflow needed to design, develop, and publish learning 
contents. Similarly, service-oriented component, for LCMS, 
is needed here to make this process achievable via web 
services.  
The third component, institutional process interface, 
handles all institutional processes that are related to  
e-learning, such as assigning roles to e-learning actors (e.g., 
module leader, instructor, and examiners). This also includes 
students’ enrolments, other administration tasks, tracking 
other related processes (e.g., financial processes). The 
automatic execution of these processes is challenging 
because most institutions have their own business rules that 
could be complicated due to the wide range of programmes 
offered by universities and the adopted service models. This 
component will communicate, via web services, with Human 
Resources (HR) systems and students’ record systems that 
can provide the necessary information to achieve this task.  
The fourth component, integration interface, handles all 
technical aspects needed for successful e-learning services. 
One of the most important aspects here is the security 
because e-learning service, as introduced earlier, is a hybrid 
service that may combine on-premises software, public 
cloud, and private cloud. Also, considering the evolving 
requirements for academic institutions is highly important. 
This requires continuous monitoring for the current  
e-learning services, such as doing performance testing and 
penetration testing. This allows benchmarking for the current 
level of service, so the institution can investigate the impact 
of adding additional components to the e-learning service.   
As described earlier, each of the above-mentioned 
interface component, in the presentation layer, liaises with 
one or more service-oriented component in the concerned 
sub-system in the business layer. For instance, institutional 
interface might liaise with one or more than one service 
component to setup the proper plagiarism check processes 
that might be dynamic as they differ from undergraduate to 
postgraduate or lifelong learning programme. This applies to 
models/tools that use specific learning approaches (e.g., 
Game-based Learning model [20]). In addition, certain 
arrangements need to be done at the data level to make sure 
data are accessible by permitted stakeholders whenever is 
needed. Despite the fact that Non-Functional Requirements 
are more persistent, with little changes are expected, there is 
a need to manage the changes that could happen over TEL 
software 
development 
life 
cycle. 
Therefore, 
it 
is 
recommended to use suitable requirement management tool 
or model to keep the e-learning service reliable and efficient. 
This includes reviewing the current set of requirements either 
based on agreed timeframe or whenever we have new 
requirements from stakeholders. Finally, it is worth 
mentioning that the early-identified business layer might 
have 
extra 
sub-systems 
based 
on 
the 
Functional 
Requirements coming from different departments in the 
academic institution. 
V. 
CONCLUSION AND FUTURE WORK  
In this paper, we handled the challenging problem of 
managing Non-Functional Requirements, more specifically 
eliciting and specifying NFRs, in the context of TEL. 
Lessons learned from TEL practices revealed that NFRs are 
ignored due to many reasons, which could seriously impact 
the overall e-learning system/service. Therefore, we opt for a 
comprehensive approach based on ISO 25010 to elicit and 
specify Non-Functional Requirements. Furthermore, this 
paper presented flexible service-oriented architecture for e-
learning software systems that can better meet the required 
capabilities 
(i.e., 
Functional 
and 
Non-Functional 
Requirements). This work revealed the need to adopt open 
and flexible architecture for TEL systems. This means that 
these systems should be designed in a way that is accessible 
via web service mechanism to allow further agility. 
Moreover, it highlighted the need to develop service 
identification and service discovery algorithms that consider 
e-learning particularities. 
REFERENCES 
[1] 
R. Hammad, M. Odeh and Z. Khan, "Towards a generic 
requirements model for hybrid and cloud-based e-learning 
systems," The IEEE 5th International Conference on Cloud 
Computing Technology and Science (CloudCom), 2013, pp. 
106-111, Bristol, UK. 
[2] 
J. Eckhardt, A. Vogelsang and D. M. Fernández, "Are non-
functional 
requirements 
really 
non-functional? 
an 
investigation of non-functional requirements in practice," The 
38th IEEE/ACM International Conference on Software 
Engineering (ICSE), 2016, pp. 832-842. 
[3] 
S. Kumar, A. K. Gankotiya and K. Dutta, "A comparative 
study of Moodle with other e-learning systems," The 3rd 
International 
Conference 
in 
Electronics 
Computer 
Technology (ICECT), 2011, 414-418. 
[4] 
S. Graf and B. List, "An evaluation of open source E-learning 
platforms stressing adaptation issues," The 5th IEEE 
29
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

International 
Conference 
on 
Advanced 
Learning 
Technologies (ICALT), 2005, 163-165. 
[5] 
D. Ameller et al., "Non-functional requirements in 
architectural decision making," IEEE Software, vol. 30, (2), 
pp. 61-67, 2013, doi:10.1109/MS.2012.176.  
[6] 
L. Chen, M. A. Babar and B. Nuseibeh, "Characterizing 
architecturally significant requirements," IEEE Software, vol. 
30, (2), pp. 38-45, 2013, doi: 10.1109/MS.2012.174. 
[7] 
C. 
Miksovic 
and 
O. 
Zimmermann, 
"Architecturally 
significant 
requirements, 
reference 
architecture, 
and 
metamodel for knowledge management in information 
technology services," The 9th IEEE/IFIP Conference on 
Software Architecture (WICSA), 2011, pp. 270-279, doi: 
10.1109/WICSA.2011.43. 
[8] 
D. Pandey and V. Pandey, "Importance of requirement 
management: 
a 
requirement 
engineering 
concern," 
International Journal of Research and Development A 
Management Review, vol. 1, (1), pp. 66-70, 2012. ISSN 
(Print): 2319–5479. 
[9] 
H. Men, J. Liu and J. Han, "Applied research on Atutor," The 
International 
Conference 
on 
E-Learning, 
E-Business, 
Enterprise 
Information 
Systems, 
and 
E-Government 
(IEEEE'09), 2009, pp.107-110. ISBN: 978-0-7695-3907-2. 
[10] T. Acosta and S. Luján-Mora, "Comparison from the levels of 
accessibility on LMS platforms that supports the online 
learning system," The 8th Annual International Conference on 
Education and New Learning Technologies, 2016, pp. 2704-
2711, doi: 10.21125/edulearn.2016.1579. 
[11] I. Vlasin and C. Chirila, "Online contest based on integration 
of activities, adaptability and students cooperation using Ilias 
LMS," The International Scientific Conference eLearning and 
Software for Education, 2016, pp. 67-74. 
[12] K. Logan and T. Neumann, "Comparison of Blackboard 9.1 
and Moodle 2.0," Learning Technologies Unit. University of 
London, London, UK, 2010.  
[13] R. D. Rucker and L. R. Frass, "Migrating Learning 
Management 
Systems 
in 
Higher 
Education: 
Faculty 
Members’ Perceptions of System Usage and Training When 
Transitioning from Blackboard Vista to Desire2Learn," 
Journal of Educational Technology Systems, vol. 46, (2), pp. 
259-277, 2017, doi: 10.1177/0047239517711954.  
[14] D. Dagger et al., "Service-oriented e-learning platforms: from 
monolithic systems to flexible services," IEEE Internet 
Computing, 
vol. 
11, 
(3), 
pp. 
28-35, 
2007, 
10.1109/MIC.2007.70.  
[15] A. G. Booth and B. P. Clark, "A service-oriented virtual 
learning environment," On the Horizon, vol. 17, (3), pp. 232-
244, 2009, doi:10.1108/10748120910993268.  
[16] M. R. Barbacci et al., "Quality attribute workshops (QAWs)," 
International society for Bioelectricity, Shreveport L.A., 
2003. 
[17] R. Hammad, M. Odeh and Z. Khan, "Towards a model-based 
approach to evaluate the effectiveness of e-learning," The 9th 
European Conference on IS Management and Evaluation – 
ECIME, UK, Bristol, 2015, pp. 111-119, ISBN: 978-1-
910810-55-2. 
[18] ISO/IEC 25010 "Systems and software engineering–Systems 
and software quality requirements and evaluation (SQuaRE)–
System and software quality models," The International 
Standard Organisation (ISO), 2011. 
[19] N. Medvidovic and R. N. Taylor, "Software Architecture: 
Foundations, Theory, and Practice," John Wiley & Sons, 1st 
edition, 2010, ISBN: 9780470167748. 
[20] R. Hammad "Game-Enhanced and Process-Based e-Learning 
Framework," In: Tian F., Gatzidis C., El Rhalibi A., Tang W., 
Charles F. (eds) E-Learning and Games, July 2017, Lecture 
Notes in Computer Science, vol 10345, Springer, Cham. pp. 
279-284, doi:10.1007/978-3-319-65849-0_30. 
 
 
 
 
30
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Information Systems: From Innovations to Innovation Generators 
 
 
Philippe Marchildon and Pierre Hadaya 
Department of Management and Technology 
ESG UQAM 
Montréal, Canada 
e-mail: marchildon.philippe@uqam.ca 
hadaya.pierre@uqam.ca 
 
 
Abstract—The objective of this study is thus to investigate how 
IS can enable organizational capabilities that may lead to 
organizational innovativeness. (i.e., an organization’s potency to 
generate innovations). To do so, we review the innovation and 
IS literatures and use structuration theory to propose a research 
model, its related hypotheses and methodological aspects tied to 
its validation. Finally, the proposed model’s anticipated 
contributions are discussed. 
Keywords-innovation; 
organizational 
innovativeness; 
structuration theory; product lifecylce management systems; radio 
frequency identification devices. 
I. 
 INTRODUCTION 
In today’s complex technological era, organizational 
innovativeness – defined as an organization’s potency to 
develop and introduce innovations in various forms (e.g., 
product, service, process, system, business structure, business 
model) [1][2] – is increasingly viewed as a multidisciplinary 
activity [3][4] based on a few key organizational capabilities: 
functional and integrative capabilities [5]. As such, and 
because evidence from the IS literature suggests that 
functional and integrative capabilities could potentially 
benefit from the support of IS [6][7][8] [9], it seems 
primordial that the IS literature acknowledges the double 
nature of IS by studying them not only as innovation per se 
but also as innovation generators.  
Based on this premise, the present study aims to answer 
the following research question: “What is the role of IS in the 
creation of organizational innovations?” Accordingly, the 
objective of this study is to investigate the impact of IS on 
functional and integrative organizational capabilities and, 
indirectly, on organizational innovativeness. 
The rest of the paper is organized as follows. First, in 
Section 2, the literature on innovations is reviewed to identify 
the potential antecedents of organizational innovativeness 
while structuration theory is examined to resolve the apparent 
conflict between the “push” and “pull” perspectives within the 
innovation literature. Third, in Section 3, anchored on this 
resolution and thus the now established duality of innovations, 
a set of hypotheses linking key organizational capabilities to 
organizational innovativeness is proposed. Fourth, in Section 
4, we investigate the IS literature and more specifically studies 
that discuss Product Life-cycle Management (PLM) systems 
and Closed-loop PLM systems to propose a second set of 
hypotheses that establish the enabling role of IS in the creation 
of certain organizational capabilities which may lead to 
organizational innovativeness. Fifth, in Section 5, the key 
methodological aspects tied to the empirical validation of this 
study’s hypotheses are discussed. Finally, this research’s 
anticipated 
theoretical 
contributions 
and 
practical 
implications are discussed in Section 6. 
II. 
THEORETICAL DEVELOPMENT 
A. Novelty: the Key Characteristic of Innovations 
Novelty – defined as the extent to which a product, service, 
process, system, organizational structure, or business model 
departs from what already exist – is the key criterion that an 
organizational component must have to meet to be considered 
an innovation. Aware of the importance of novelty in the 
making of innovations, researchers generally identify 
innovations as either radical or incremental [10]. Radical 
innovations are those that produce fundamental changes in the 
activities of an organization or an industry and represent clear 
departures from existing practices [11]. On the other hand, 
incremental innovations merely call for marginal departure 
from existing practices [11].  
Another important aspect of innovations is that they are 
externally determined. That is, the concept of innovation can 
only make sense when an innovation is compared to a specific 
external referent. For example, cell phones can only be 
considered innovations when they are compared to regular 
house phones and not when they are compared to more 
evolved smart phones. Thus, to innovate an organization has 
to create a product, service, process, system, organizational 
structure, or business model that departs from what already 
exists in its specific frame of reference. As such, an 
organization in the manufacturing industry, relying on supply 
chain logistics and transportation systems might be considered 
innovative whereas a firm relying on these same systems in 
the transportation industry will only be considered attuned to 
what already exists. Therefore, organizational innovativeness 
not only depends on what an organization does but also on 
what is done by other external stakeholders within the same 
environment. 
This dual nature of organizational innovativeness is 
epitomized in the innovation literature by the presence of two 
31
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

overarching rival perspectives which see innovation 
generation as either a “push” or a “pull” phenomenon [12]. 
More precisely, there seems to be a polarization on innovation 
antecedents where the characteristics of the organization (i.e., 
“push” perspective) and the characteristics of its environment 
(i.e., “pull” perspective) are viewed as rival predictors of a 
firm innovativeness. Partisans of the “push” perspective have 
shown, for example, that a firm’s structure, culture, 
management practices and strategies significantly influence 
organizational innovativeness while, on the other hand, adepts 
of the “pull” perspective have identified that an organization’s 
industry, region, government policies, and technological 
environment significantly impact its innovativeness [12].  
However, doubtful of the apparent conflict between 
“push” and “pull” adherents, some authors have argued, based 
on evidence from practice, that the process of innovation 
generation might be better conceptualized as an evolutionary, 
non-linear, iterative process between the firm and its 
environment [3][4]. In other words, this third and new 
perspective suggests reconciling previous research findings 
by defining organizational innovativeness as a duality where 
an organization is at the same time influencing and influenced 
by its environment rather than as a dualism where both types 
of innovation antecedents are considered orthogonal. 
Nevertheless, 
in 
spite 
of 
these 
authors’ 
insightful 
observations, little theoretical let alone empirical work has 
been undertaken in the innovation research field to account for 
this duality. As such, the best rationale on the duality of 
innovations doesn’t come from innovation studies but rather 
from the field of sociology and more precisely from Giddens’ 
[13] answer to the debate between “functionalist/determinist” 
and “voluntarist” sociologists. 
B. Duality of Innovations: an Explanation Anchored on 
Giddens’ Structuration Theory 
Giddens [13], analogous to authors of innovation research 
doubtful of the “push” and “pull” orthodoxy, acknowledged 
that the debate between Functionalist/determinist” – defined 
as sociologists entrenched in the orthodox consensus who saw 
agents’ actions as a result of environmental forces and 
demands [14] – and “voluntarist” – defined as sociologists 
who argue that agents reflectively act without restraint and 
have the power to tailor their surrounding environment to their 
needs [15] – could in fact be resolved by acknowledging the 
duality of agents’ behaviors. More precisely, noticing 
common grounds between these protagonists and the mutual 
influence between an agent and its environment (i.e., the 
duality of structure), Giddens [13] redefined the agency 
concept and then developed the structuration theory to provide 
a sound explanation to this duality. Indeed, recognizing that 
previous conceptualizations, which defined agency in terms of 
intention, failed to account for the duality of structure, 
Giddens [13] proposed to redefine agency in terms of 
transformative capacity. Thus, according to Giddens [13], 
agents are characterized by their ability to take action, to 
deploy a range of causal powers, including that of influencing 
those deployed by others [13]. Furthermore, similar to 
Bachrach and Baratz [16], Giddens [13] also recognized that 
agents’ actions and thus their transformative capacity are to 
some extent limited, due to the rules and resources agents use 
when taking action. In other words, the rules and resources 
drawn upon by agents when taking action are simultaneously 
constraining and enabling agents’ actions, creating the duality 
of structure.  Thus, the concept of agency, which defines 
agents in terms of transformative capacity and entails the 
enabling and constraining role of rules and resources, is at the 
essence of the duality identified in the sociology and 
innovation literatures. As such, by instantiating the concept of 
agency to the particular context of society construction, 
Giddens [13] was able to demonstrate that social structures are 
not fixed but rather exhibit structural properties since social 
structures emerge from agents’ constant reproduction of 
already existing rules and resources. 
By instantiating the concept of agency in the context of 
innovation generation, one can realize that its basic 
assumptions still hold: agents, by taking action, still draw on 
rules and resources that simultaneously constrain and enable. 
However, instead of simply reproducing them, agents, 
because of the novelty criteria, also need to significantly 
depart from them (i.e., they enact a novel behavior). 
Therefore, similar to Schumpeter [17] who depicted 
innovation activities as a process of recombining and/or 
reconfiguring existing pieces of knowledge in some novel 
way [17][18], this view of innovations establishes that 
organizational innovativeness rests on the transformative 
capacity of organizations. In other words, an organization’s 
innovativeness rests on its ability to draw from existing rules 
and resources while significantly departing from them by 
creating new knowledge. 
III. 
CONCEPTUAL FRAMEWORK 
Based on the theoretical background presented above, the 
premise of this article is that IS usage improves an 
organization’s capacity to innovate. This section exposes the 
nine hypotheses tied to our research model shown in Figure 1. 
The first four hypotheses of our model explain how 
organizational capabilities (functional and integrative) lead to 
organizational innovativeness while the last five hypotheses 
explain how two types of IS (PLM and Closed-Loop PLM 
systems) may provide a platform to support and improve 
these key organizational capabilities. 
A. The Positive Impact of Functional and Integrative 
Capabilities on Organizational Innovativeness 
Having established, through structuration theory, that 
organizational innovativeness rests on an organization’s 
ability to draw from existing rules and resources and to create 
knowledge, it is now essential to go back to the innovation 
literature and more precisely to the work of Verona [5] to 
define these key capabilities. Verona [5], who also anchored 
his research efforts on the premise that an organization’s 
capacity for action resides in its capabilities [19], identified a 
set of capabilities that, similar to the ones suggested in 
structuration theory, are linked to the duality of organizational 
innovativeness. More precisely, the author identified two 
types of capabilities, functional and integrative [20], that 
enable the creation of knowledge. Functional capabilities refer 
32
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

to capabilities that allow an organization to develop new 
knowledge while integrative capabilities are defined as 
capabilities that allow an organization to integrate knowledge 
from different sources [20]. Recognizing that sources of 
knowledge can be located within and outside organizational 
boundaries, Verona [5] sub-divided integrative capabilities 
into internal integrative capabilities and external integrative 
capabilities (i.e., those that integrate knowledge from sources 
within the organization’s boundaries and those that integrate 
knowledge 
from 
sources 
outside 
the 
organization’s 
boundaries). As such, Verona [5] and Giddens [13], although 
guided by different research objectives, both posited 
capabilities as a core property of agents and defined 
capabilities tied to organizational innovativeness as an ability 
to draw from existing sources of knowledge to create new 
knowledge. Accordingly, organizations aiming to maximize 
their innovativeness must develop functional capabilities as 
well as internal and external integrative capabilities. Based on 
this premise, the following three hypotheses are formulated: 
 
Hypothesis #1: An organization’s functional capabilities 
positively influence its innovativeness. 
 
Hypothesis #2: An 
organization’s 
internal 
integrative 
capabilities 
positively 
influence 
its 
innovativeness. 
 
Hypothesis #3: An 
organization’s 
external 
integrative 
capabilities 
positively 
influence 
its 
innovativeness. 
 
Furthermore, because the duality of organizational 
innovativeness implies that sources of knowledge both enable 
and constrain the creation of innovations, an additive 
interaction between functional and integrative organizational 
capabilities is also expected. Based on this premise, the 
following hypothesis is formulated: 
 
Hypothesis #4: The 
interaction 
between 
the 
three 
organizational capabilities will have a 
positive 
influence 
on 
organizational 
innovativeness.  
 
B. The Positivie Impact of IS Usage on Functional and 
Integrative Capabilities 
Recognizing 
the 
importance 
of 
organizational 
innovativeness to develop and maintain a competitive 
advantage, organizations have adopted various practices and 
IS to support their initiatives [9]. Amongst these, PLM 
practices and PLM systems seem the more promising since 
there is an increased awareness that they may represent the 
foundation upon which the key organizational capabilities that 
lead to organizational innovativeness may be developed [9]. 
For instance, Pratt & Whitney used the PLM system 
developed by Siemens (i.e., the Tecnomatix solution) to limit 
the development of its engines to within 3 years, which was 
considered competitive at the time [21]. Furthermore, by 
using this system, Pratt & Whitney was able to design 
innovative engines that minimize downtimes by making 
maintenance operations as simple as possible [21]. 
PLM can be defined as a strategic approach that aims to 
provide more product-related information to the organization 
during the whole product lifecycle. Building on and extending 
the ideas of product data management (PDM) [22], this 
approach emerged from the necessity to move beyond simple 
engineering concerns of products and to provide a shared 
platform for creation, organization, and dissemination of 
product related knowledge [23][24]. PLM systems consisting 
of information processing systems or a set of information 
technology (IT) systems were conceived in order to support 
these considerations by forming an organization’s product 
information backbone [25]. These systems, rooted in 
computer-aided design (CAD) and PDM systems, establish a 
set of tools and technologies that provide a shared platform for 
collaboration among product stakeholders and streamline the 
flow of information along all the stages of the product 
lifecycle [22][26]. However, although these systems were 
originally intended to support every phase of a product 
lifecycle (i.e., beginning of life (BOL), middle of life (MOL) 
and end of life (EOL)), most PLM systems currently fail to 
support products once they are sold, that is during their MOL 
and EOL phases [27]. More precisely, PLM systems, because 
of business and technological constraints, cease to collect 
product information once the product leaves the control of the 
organizations or its boundaries [23][24][27]. As such, by (1) 
integrating 
people, 
processes, 
business 
systems 
and 
information throughout the whole product lifecycle, (2) 
fostering horizontal connection between an organization’s 
silos, (3) enhancing information sharing, (3) facilitating 
change management, and (4) enticing use of past knowledge 
[23][24], current PLM systems only support functional and 
internal integrative organizational capabilities and leave 
external integrative capabilities unsupported. Based on this 
premise, the following two hypotheses are formulated: 
 
Hypothesis #5: The use of PLM systems positively influences 
an organization’s functional capabilities. 
 
Hypothesis #6: The use of PLM systems positively influences 
an 
organization’s 
internal 
integrative 
capabilities. 
 
Even though traditional PLM systems fail to address 
external integrative capabilities, recent advances in product 
identification technologies (PIED), such as radio frequency 
identification (RFID) and Auto-ID now enable organizations 
to collect product information beyond their organizational 
boundaries [23][24][27]. These new systems that comprise 
previous functionality of PLM systems while also including 
the new functionalities provided by PEID are referred to as 
Closed-loop PLM systems [28]. These new systems, by 
automatically capturing data outside the boundary of the firm, 
alleviate both the business and technological constraints of 
traditional PLM systems and give organizations the potential 
to also support external integrative capabilities. Based on this 
premise, the following hypotheses are formulated: 
33
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

 
 
Figure 1.  Research Model. 
 
Hypothesis #7: The use of Closed-loop PLM systems 
positively 
influences 
an 
organization’s 
functional capabilities. 
 
Hypothesis #8: The use of Closed-loop PLM systems 
positively 
influences 
an 
organization’s 
internal integrative capabilities. 
 
Hypothesis #9: The use of Closed-loop PLM systems 
positively 
influences 
an 
organization’s 
external integrative capabilities. 
 
IV. 
METHODOLOGY 
As our research is still in progress, this section explains the 
methodological framework we have devised, but not yet used, 
to test our research model. More precisely, we present our 
intended research setting, data collection procedures, survey 
instrument and data analyses procedures. 
A. Research Setting 
The up-to-date list of all Canadian manufacturing firms 
maintained by Statistics Canada (a Canadian government 
agency) will constitute this study’s sample frame. More 
precisely, the sample frame will be limited to Canadian 
manufacturing firms active in the transportation equipment 
manufacturing industry (NAICS 336) (e.g., motor vehicle, 
aerospace, shipbuilding and other transportation equipment 
manufacturing). Three reasons justify the selection of this 
specific frame: First, manufacturing firms are often the middle 
men in the supply chain to which they pertain enabling them 
to influence and be influenced by both upstream and 
downstream partners and thus to the full range of the duality 
of innovativeness. Second, organizational innovativeness is 
already recognized as a key driver of success in this specific 
industry and thus is a salient topic to potential respondents. 
Third, manufacturing firms, especially in this particular 
industry, are amongst the very few organizations that have 
adopted to varying degrees PLM and Closed-Loop PLM 
systems, making this a rare setting where their influence on 
organizational capabilities is likely to be observed [29][30]. 
B. Data Collection 
Data will be collected by the means of a field survey. More 
precisely, business managers will be asked via e-mail to fill 
the first-half of an online survey (available through a first 
hyperlink) and to ask their engineering counterparts to fill the 
second-half of the online survey (available through a second 
hyperlink). Business managers will answer questions 
regarding their organization’s innovativeness (i.e., criterion 
variable). Considering the fact that innovation may be of 
various forms and taking into account this study’s research 
setting, business managers will be asked to evaluate the 
innovativeness of their organization in regards to two forms 
of innovation: product and process innovations. On the other 
hand, engineering managers will answer questions regarding 
the organization’s functional and integrative capabilities as 
well as the use of PLM and Closed-Loop PLM systems within 
their organizations (i.e., predictor variables). Seeking 
responses from respondents from two different organizational 
sub-units should alleviate some potential sources of common 
method bias. In addition, to maximize this study’s response 
rate, a Tailored Design Method (TDM) will be used. 
Specifically, this procedure comprises four essential elements: 
(1) a respondent-friendly questionnaire; (2) a five-contact 
strategy (in the form of five different e-mails to be sent to 
business managers); (3) a personalized correspondence; and 
(4) incentives in the form of a privileged access to the research 
findings (e.g., a tailored benchmark report) and a chance to 
win an electronic gift card of a 500$ value on Amazon (i.e. 
one gift card for each type of respondent). 
C. Survey Instrument 
The electronic survey will comprise measures adapted 
from the literature. The measures tied to the organizational 
capabilities will be adapted from [5][31][32]. Measures tied to 
Interaction between functional, 
internal integrative and external 
integrative capabilities
PLM system 
usage
Closed-loop 
PLM system 
usage 
External 
integrative 
capabilities
Functional 
capabilities
Internal 
integrative 
capabilities
Organizational 
innovativeness
H3
H1
H2
H5
H6
H7
x
x
H4
H9
H8
34
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

PLM and Closed-PLM systems use will be adapted from 
[17][18][27][33][34][35]. Finally, measures of organizational 
innovativeness will be adapted from [1][2][36]. A pre-test of 
the questionnaire will also be conducted with several business 
and engineering managers in order to test the reliability and 
validity of the questionnaire as well as to identify potential 
upgrades prior to the full-scale inquiry. 
D. Data Analyses 
Structural equation modeling (SEM) based on LISREL 
covariance software package will be used to test this study’s 
research model. LISREL is chosen over PLS and traditional 
regression methods as it better supports the study of 
interactions amongst research variables [37], a key 
particularity of this study. Furthermore, because most of this 
study’s measures were previously used and tested, this 
research is more confirmatory rather than exploratory in 
regards to its measures. As such, it is more appropriate to use 
LISREL, which is more stringent then PLS [37]. Also, since 
using the SEM approach allows for the evaluation of both the 
quality 
of 
the 
measurement 
and 
the 
construct 
interrelationships, this approach will be used to first test the 
measurement model and then to test the structural model. To 
test the measurement model, a confirmatory factor model (i.e., 
the measurement model) will be used to measure the fit 
between the theorized model and observed variables. Then, to 
test the structural model, results of the measurement model 
will be used to create a path-analytic model to investigate the 
relationships hypothesized in this study [38]. 
V. 
CONCLUSION 
Recognizing the potential of IS as innovation generators, 
the present study aimed to answer the following research 
question: “What is the role of IS in the creation of 
organizational innovations?” Based on the theoretical and 
conceptual background put forth in this study, we assess that 
IS play an enabling role in the creation of organizational 
innovations. Specifically, IS provide a platform to support and 
improve the key organizational capabilities (functional and 
integrative) that lead to organizational innovativeness. If 
validated, our understanding of the role of IS in the creation 
of organizational innovations is likely to yield important 
theoretical contributions and practical implications. 
A. Theoretical Contibutions 
From a theoretical perspective, by identifying novelty as a 
key criterion of innovations and by instantiating the concept 
of agency in the context of innovations, this research 
significantly improves our understanding of the creation of 
organizational innovations. Also, the sound conceptual 
framework developed in this study allows for the 
reconciliation of conflicting views on innovation generation 
and sets the groundwork for further research on this important 
topic. 
Furthermore, 
by 
highlighting 
the 
duality 
of 
organizational innovativeness, this study theoretically 
justifies the necessity of functional and integrative capabilities 
as well as the additive interaction stemming from their mutual 
influence. In doing so, this research also establishes the 
importance of IS in the innovation generation process, by 
revealing their positive impact on functional and integrative 
organizational capabilities. Accordingly, the present research 
also explains how IS can support the creation of 
organizational innovations. 
B. Practical Implications 
From a practical standpoint, the present research 
anticipated results should allow managers to improve the 
innovation capabilities of their organizations by identifying 
the key role of functional and integrative capabilities as well 
as their additive mutual influence. Furthermore, by being 
amongst the first study to highlight the role of IS in the 
generation of innovations, this study should enable 
organizations to not only improve their innovativeness but 
also to reap greater benefits from their current IS. 
Furthermore, by highlighting the key characteristics of PLM 
and Closed-Loop PLM systems, this research should also 
provide sound arguments for why firms that aim to innovate 
should use Closed-loop PLM systems that incorporate PEID 
and not limit their use to PLM systems. 
C. Limits and Future Research Avenues 
The theoretical and methodological contents presented 
above suggest a few limits and related future research 
avenues. First, our study sample only comprises Canadian 
organizations involved in specific industries. To address this 
limit, future research could aim to replicate our research 
efforts with a different sample frame. For example, it could be 
interesting to replicate our research efforts with organizations 
from service industries and/or from different countries. 
Second, although we use the concept of duality at the heart of 
structuration 
theory 
to 
show 
that 
organizational 
innovativeness rests on an organization’s ability to draw from 
existing rules and resources and to create knowledge, our 
research model does incorporate all the tenets of structuration 
theory. In the future, we hope to extend our research model in 
order to take into account more tenets of the theory. 
REFERENCES 
[1] A. Azedegan and K. J. Dooley, “Supplier Innovativeness, 
Organizational 
Learning 
Styles 
and 
Manufacturer 
Performance: an Empirical Assessment,” Journal of Operation 
Management, vol. 28, no. 6, pp. 488-505, 2010. 
[2] T. M. Hult, R. F. Hurley and G. A. Knight, “Innovativeness: its 
Antecedents and Impact on Business Performance,” Industrial 
Marketing Management, vol. 33, no. 5, pp. 429–438, 2004. 
[3] D. Leonard-Barton, “Develper-User Interaction and User 
Satisfactionin Internal Technology Tranfer,” Academy of 
Management Journal, vol. 36, no. 5, pp. 1125-1139, 1993. 
[4] R. Subroto, K. Sivakumar and I. F. Wilkinson, “Innovation 
Generation in Supply Chain Relationships : a Conceptual 
Model and Research Propositions,” Journal of the Academy of 
Marketing Science, vol. 32, no. 1, pp. 61-79, 2004. 
[5] G. 
Verona, 
“A 
Resources-Based 
View 
of 
Product 
Development,” Academy of Management Review, vol. 24, no. 
1, pp. 132-141, 1999. 
[6] E. K. Clemons and M. C. Row, “Sustaining IT Advantage: the 
Role of Structural Differences,” MIS Quarterly, vol. 15, no. 3, 
pp. 275-292, 1991. 
[7] E. K. Clemons and M. C. Row, “Information Technology and 
Industrial 
Cooperation: 
the 
Changing 
Economics 
of 
35
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Coordination and Ownership,” Journal of Management 
Information Systems vol. 9, no. 2, pp. 9-28, 1992. 
[8] K. D. Joshi, L. Chi, A. Datta, and H. Shu, “Changing the 
Competitive Landscape: Continuous Innovation through IT-
Enabled Knowledge Capabilities,” Information System 
Research, vol. 21, no. 3, pp. 472-495, 2010. 
[9] Y.-M. Tai, “Effects of Product Lifecycle Management Systems 
on New Product Development Performance,” Journal of 
Engineering and Technology Management, vol. 46, no. 1, pp. 
67-83, 2017. 
[10] J. E. Ettlie, W. P. Bridges, and R. D. O' Keefe “Organization 
Strategies and Structural Differences for Radical versus 
Incremental Innovation,” Management Science, vol. 30, no. 6, 
pp. 682-695, 1984. 
[11] S. Gopalakrishnan, and F. Damanpour, “A Review of 
Innovation Research in Economics, Sociology and Technology 
Management,” Omega, International Journal of Management 
Science, vol., 25, no. 1, pp. 15-28, 1997. 
[12] N. Becheikh, R. Landry and N. Aamara, “Lessons from 
Innovation Empirical Studies in the Manufacturing Sector: a 
Systematic Review of the Literature from 1993-2000,” 
Technovation, vol. 26, no. 5-6 , pp. 644-664, 2006  
[13] A. Giddens, “Elements of the Theory of Structuration”, The 
Constitution of Society, University of California Press, 1984.  
[14] L. Winner, “Engines of Changes,” Autonomous Technology: 
Technics out of Control as a Theme in Political Thought, MIT 
Press, Boston, MA, 1977. 
[15] J. R. Galbraith,  “Designing Complex Organization,” Addison-
Wesley, Reading, MA, 1973. 
[16] P. Bachrach and M.S. Baratz, “The Two Faces of Power,” 
American Political Science Review, vol. 56, no. 4, pp. 947-952, 
1962. 
[17] J. Schumpeter, “The Theory of Economic Development,” 
Cambridge, Mass: Harvard University Press, 1934. 
[18] S. 
Paruchuri, 
“Intraorganizational 
Networks, 
Interorganizational Networks, and the Impact of Central 
Inventors : a Longitudinal Study of Pharmaceutical Firms,” 
Organization Science, vol. 21, no. 1, pp. 63-80, 2010. 
[19] M. Iansiti and  K. B. Clark,  “Integration and Dynamic 
Capability: 
Evidence 
from 
Product 
Development 
in 
Automobiles and Mainframe Computers,” Industrial and 
Corporate Change, vol. 3, no. 3, pp. 557-605, 1994. 
[20] B. Lawson and D. Samson, “Developing Innovation Capability 
in Organizations: a Dynamic Capabilities Approach,” 
International Journal of Innovation Management, vol. 5, no. 3, 
pp. 377-400, 2001. 
[21] S. G. Lee, Y.-S. Ma, G. L. Thimm, and J. Verstraeten “Product 
Lifecycle Management in Aviation Maintenance, Repair and 
Overhaul,” Computers in Industry, vol. 59, no. 2-3, pp. 296–
303, 2008. 
[22] F. Ameri, and D. Dutta, “Product Lifecycle Management: 
Closing the Knowledge Loops,” Computer-Aided Design and 
Applications, vol. 2, no. 5, pp. 577-590, 2005. 
[23] H.-B, Jun, J.-H. Shin, D. Kiritsis and P. Xirouchakis, “System 
Architecture for Closed-Loop PLM,” International Journal of 
Computer Integrated Manaufacturing, vol. 20, no. 7, pp. 684-
698, 2007. 
[24] H.-B, Jun, D. Kiritsis and P. Xirouchakis, “Research Issues on 
Closed-Loop PLM,” Computer in industry, vol. 58, no. 8-9, pp. 
855-868, 2007 
[25] A. Sääksvuori and A. Immonen, “Product Lifecycle 
Management,” 2nd edition. Springer, London, 2002. 
[26] J. Stark “Product Lifecycle Management: Volume 1 - 21st 
Century Paradigm for Product Realisation,” 3rd edition, 
Springer, London, 2015. 
[27] S. Talbot, E. Lefebvre and L.-A. Lefebvre, “Closed-Loop 
Supply 
Chain 
Activities 
and 
Derived 
Benefits 
in 
Manufacturing SMEs,” Journal of Manufacturing Technology 
Management, vol. 18, no. 6, pp. 627-658, 2007. 
[28] P. Hadaya and P. Marchildon, “Understanding Product 
Lifecycle Management and Supporting Systems,” Industrial 
Management and Data Systems, vol. 112, no. 4, pp. 559-583, 
2012. 
[29] M. Abramovici and O. C. Sieg, “Status and development trends 
of product lifecycle management systems,” The conference on 
Integrated Products and Product Development (IPPD-2002) 
Feb., 2002. 
[30] L. Zhekun, R. Gadh and B. S.Prabhu, “Applications of RFID 
Technology and Smart Parts in Manufacturing," The confrence 
on Design Engineering Technical Confernces (DETC-2004) 
ASME, Sept. 2004, 
[31] A. Malhotra, S. Gosain and O. El Sawy, “Absortive Capacity 
Configurations in Supply Chains : Gearing for Partner-Enabled 
Market Knowledge Creation,” MIS Quarterly, vol. 29, no. 1, 
pp. 145-187, 2004. 
[32] S. A. Zahra and G. Georges, “Absorptive Capacity, a Review, 
Reconceptualization, 
and 
Extension,” 
Academy 
of 
Management Review, vol. 27, no. 2, pp. 185-203, 2002. 
[33] T. Chiang and A. J. C. Trappey, “Development of Value Chain 
Collaborative Model for Product Lifecycle Management and 
its LCD Industry Adoption,” International Journal of 
Production Economics, vol. 109, no. 1-2, pp. 90-104, 2007. 
[34] D. Kiritsis, “Closed-Loop PLM for Intelligent Products in the 
Era of the Internet of Things,” Computer-Aided Design, vol. 
43, no. 5, pp. 479-501, 2011. 
[35] S. Lee, “Business Use of Internet-Based Information Systems: 
the Case of Korea,” European Journal of Information Systems 
vol. 12, no. 3, pp. 168-181, 2003. 
[36] J. P. Jansen, F. A. J. Van Den Bosch, and H. W. Volberda, 
“Exploratory 
Innovation, 
Exploitative 
Innovation, 
and 
Performance: Effects of Organizational Antecedents and 
Environmental Moderators,” Management Science, vol. 52, 
no.11, pp. 1661-1674, 2006 
[37] D. Gefen, D. W. Straub and M. C. Boudreau, “Structural 
Equation Modeling and Regression: Guidelines for Research 
and Practice,” Communications of the Association for 
Information Systems, vol. 4, no. 1, pp. 1-77, 2000. 
[38] G. S. Kearns and A. L. Lederer, “A Resource-Based View of 
Strategic IT Alignment: How Knowledge Sharing Creates 
Competitive Advantage,” Decision Sciences, vol. 34, no. 1, pp. 
1-29, 2003. 
 
36
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Two-level Architecture for Rule-based Business Process Management
Kanana Ezekiel 
Vertiv Co  
Accurist House, Baker Street  
London, UK 
Email: kanana.ezekiel@vertivco.com 
 
 
Vassil Vassilev, Karim Ouazzane 
School of Computing and Digital Media 
London Metropolitan University 
London, UK 
Email: v.vassilev@londonmet.ac.uk,        
k.ouazzane@londonmet.ac.uk 
Abstract— One of the main challenges in Business Process 
Management (BPM) systems is the need to adapt business rules 
in real time. A serious obstacle is the lack of adaptable formal 
models for managing dynamic business rules. This is, due to 
the inadequacy of the models ability to describe the rule 
components, 
meta-rules, 
relationships 
and 
logical 
dependencies. To overcome this drawback, this paper presents 
a two-level rule-based approach to control BPM systems. The 
model accounts for logical representation of rules components 
and their relationships in Process-based Systems, as well as a 
method for incremental indexing of the business rules. The 
incremental indexing mechanism is described as an approach 
to control process execution and adaptation of business rules in 
real time based on rules propagation. Therefore this model 
provides a basis for an efficient and adaptable solution for 
managing business rules changes. 
Keywords-Business Process Management; Rule-based Systems; 
Meta-Rules; Rule Dependencies;  Object-orientation. 
I. 
 INTRODUCTION  
There are several definitions for business rules proposed 
in the literature. The most commonly used definitions of 
business rules are described [4]. In general, a business rule is 
defined as a rule that constrains, controls or structures some 
aspect of information, applications and processes in business. 
Business rules have been considered from many different 
perspectives. For example, business rules can be used by 
credit card companies to approve credit card applications. E-
commerce businesses use business rules to understand 
customers shopping habits. Banks may use business rules to 
analyse data to establish suspicious or fraudulent online 
activities. Other applications that use business rules exist in 
areas such as insurance, airline, telecom, and manufacturing 
industries, etc.  
In Business Process Management (BPM) systems, the 
behaviour of executing business process workflows is 
controlled by various business rules. Transforming and 
configuring dynamic and scattered business rules through 
process flow routines is very demanding. Typically, the 
organizations will have many business rules to enforce in 
their business processes. However, the business rules tend to 
change frequently. The most challenging task is to propagate 
these changes when there are multiple rule dependencies. In 
BPM systems, a change to business rules means 
reconfiguration of every process and other related rules. 
Inefficiency and inconsistency of the business rules are often 
unavoidable. The manageability and maintainability of the 
business rules is therefore becoming time consuming and a 
costly exercise. To address these problems, an adaptive 
Business Rules Framework for Workflow Management [1] 
has been developed. It is based on modelling of both 
business rule components and meta-rules, as well as business 
processes, flows and events in a unified manner, accounting 
for the structural patterns of description for various objects. 
This unified approach allows for the defining of the explicit 
and implicit relationships between business rules and 
indexing them incrementally, which eliminates the need for 
keeping a log of the changes. 
This article has six sections to follow. Section II gives an 
overview of related work. Section III introduces the two-
level approach for building the architecture of rule-based 
systems for BPM. Section IV describes the basic concepts 
used to construct the two-level architecture. Section V 
describes the current status of implementation of the whole 
framework. Section VI presents formal definitions and 
illustrates the use of dependency trees to define business 
rules relationships. Section VII concludes the article with a 
brief description of the next stage of implementation of the 
framework. 
II. 
RELATED WORK 
In recent years, substantial efforts have been made 
towards developing solutions to tackle the ever-growing 
problem of business rules adaptation. This section presents 
some methodologies and approaches adopted by existing 
rule-based systems. The existing commercial Business Rule 
Management Systems (BRMSs) integrate rule technology 
(rule engine) specifically for rule management. The IBM 
BRMS [2],[3] has the greatest business rules capabilities on 
the market. IBM BPM includes a customized version of 
IBM’s Operational Decision Manager (ODM) tool for its 
business rules, which incorporates tools such as Eclipse to 
give inexperience programmers the ability to create and 
modify rules. The well-known IBM BRMS, WebSphere 
ILOG JRules [4], which provides a flexible tool for rule 
modelling, is now part of IBM ODM. While IBM BRMS 
provides integrated environment with rich and flexible tools 
for business rule modelling, there are some notable 
limitations in relation to the ability to manage changes to 
business rules. There is no straightforward way to change 
rules that affect more than one process. Multiple changes to 
business processes will need to be applied even for the 
simplest business rule changes. This seriously limits the 
business agility that business rules are designed to provide. 
37
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

There is no separation of the various parts of the business 
rules components, i.e., Event, Condition and Action. This 
means a change made on the “condition” part of the rule will 
require invoking the whole rule. Separating rule components 
provides flexibility and increases performance, as only the 
part that needs changing is exposed on the business rule 
application. Henceforth, various parts of the rule need to be 
stored 
in 
appropriate 
structures 
to 
facilitate 
their 
management, similar to the existing structures for data in 
database systems. Rules are executed one by one in a 
procedural manner As a consequence, this creates additional 
work when the rules sequences change or when a separate 
rule in a particular sequence is changed. This complicates the 
ability to perform logical deduction hence its inability to 
manage changes to multiple business rule hierarchies [5]. 
C Language Integrated Production System (CLIPS) is 
specifically designed to facilitate the development of 
software to model human knowledge or expertise [6]. The 
CLIPS expert shell provides a platform where expert 
knowledge may be categorized as rules. To supplement its 
rules management capability, CLIPS works as an inference 
engine that enables it to perform the inference procedure 
whereby rules are interpreted to generate various actions as 
appropriate [7]. This mechanism employs the embedded pre-
existing rules-based knowledge as “facts” to drive the firing 
mechanism of the inference engine and thereby produce a 
recommended conclusion to a problem. Even though CLIPS 
provide an interactive, text oriented environment for 
modelling rules, there is no dedicated knowledge base and, 
thus, facts are volatile and are purged from its memory as 
soon as its execution is terminated. To overcome this 
fundamental limitation, an external rule-base system must be 
added for a seamless integration with CLIPS. This adds to 
the complexity and cost for managing rules. The problem 
becomes worse when rules are scattered and changing. 
Java Expert System Shell (JESS) [8][9] is another rule 
engine, originated from CLIPS and written entirely using 
Java. There is an extension called VISUAL JESS, which 
improves the comfort of using the tool. Pitfalls of JESS for 
dynamic systems are well documented [10].  
Oracle BRMS is a leading Business Rules product, 
probably one of the finest products in the market. Oracle 
offers a Rule Author, a web-based graphical authoring 
environment that enables creation of business rules. In 
addition, Oracle provides an embedded business rules engine 
to its BPM system. The Oracle BPM application can 
add/remove and change the state of business objects in the 
working memory, and allows the rule engine to reason and 
update processes by triggering events or invoking specific 
processes based on the outcome of the rules. Like IBM 
BRMS, it faces similar limitations - it remains impossible to 
specify the dependencies between the rules based on the 
relationships between BPM objects. This causes multiple 
changes to be necessary to adjust already configured 
processes and update existing business rules even in the case 
of a simple rule change. 
OpenRules [11], another powerful BRMS for rule-based 
application development, provides both complex Business 
Rule editor as well as a tool for building user interfaces. It 
allows the use of external tools such as MS Excel, Google 
Docs, and Eclipse IDE to create a complex, decision support 
system. OpenRules has similar limitations like the Oracle 
and IBM products. In this case it becomes even more 
complicated to deal with multiple changing rules as the rule 
management remains a tedious manual task. 
JBoss Drools BRMS [12] is a sophisticated open source 
BRMS and has a lot of functionalities, which allow users to 
write and validate business rules that can then be added to 
Java Applications. While Drools distinguishes the structural 
elements of the rules syntactically it does not treat them in a 
special way semantically. At the same time, the users are free 
to define, classify, and modify the rules according to their 
specific requirements. Business rule components (i.e., Event, 
Condition, Action, etc.) are not defined as objects. This 
brings 
additional 
complexity 
in 
terms 
of 
change 
management. Furthermore, this work does not address the 
aspects of rules relationships and dependencies. 
There are only a few proposals in the literature, which 
consider the business rules functionality and change [13]-
[15]. Their focus is on rule execution and they do not provide 
support for modelling business rules. As a rule, they do not 
address how changes of business rules are managed. There is 
no clear, well-structured definition of the rule components 
and relationships; a common drawback of all industrial rule-
based frameworks. We believe that a more flexible and 
efficient approach to manage business rule changes is 
required.  
The next section outlines the two-level architecture of 
rule-based BPM systems, which addresses the above issues. 
Presenting a flexible approach for defining rules as objects, 
attributes/properties and relationships enabling logic and 
object programming power during rule implementation.  
III. 
TWO-LEVEL ARCHITECTURE FOR BPM SYSTEMS 
The formal model presented here is based on the 
understanding of the actual BPM system as an event-driven 
and constantly evolving process, with two functioning levels. 
The first level is the Process level, which governs the 
execution of business processes, while the second level, the 
Rule level, is a meta-level that controls the actual business 
rules. Features that are considered on the first level are: 
business processes, information and material flows, events, 
conditions and actions, which comprise the business domain. 
The users may intervene only via events that can trigger 
activities prescribed by the business rules. This way we can 
model manual, automated and fully-automatic processes as 
part of the business workflows. The second level considers 
the relationships between rules and dependencies between 
them, classifications of the rules, and meta-rules.  The 
business rules are made up of events, conditions and actions, 
or the famous “When <event> If <condition> Then 
<action>” structure, whereas process execution level is made 
up of processes, steps, flows (material and information 
flows), roles, etc. For instance, if some events are observed 
during execution of a working process, then the 
corresponding business rules, which depend on these events, 
are triggered and lead to actions, which in turn perform the 
38
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

transition to a new step, which may execute other processes 
or amend the parameters of the current process. The model 
uses business rules to glue together processes in a business 
process workflow. The rule control level provides a level of 
abstract “independence” between the two levels, suggesting 
that the rules can be changed without affecting the 
workflows that have been completed. The rule level controls 
the execution of business workflows adding the business 
logic to them. The business rules appear at all stages of the 
workflow from initiation, to execution, to termination. Based 
on the distinct roles they play in the workflow development, 
they can be organised in a taxonomic hierarchy: the 
Execution rules are divided into Flow and Process rules; the 
Flow rules are divided into Sequence, Fork and Join rules; 
and, Process rules are classified into Time-based and Non-
Time-based rules.  
IV. 
BASIC CONCEPTS OF THE TWO-LEVEL RULE-BASED 
ARCHITECTURE 
This section presents basic concepts to support creation of 
objects, properties, and, relations for the model and meta- 
model. The concepts have been developed in a purely 
logical manner. 
  
 
Figure 1. Example of a Business Rule. 
 
Fig. 1 depicts a typical business rule. From such rule, the 
following concepts can be identified: 
A. Business Objects 
The business objects are the building blocks for 
implementing business rules and business processes. The 
following types of objects can be distinguished: 
• 
Processes: Building blocks of the business workflows. 
Examples: 
Process1 
(Manage 
Cabinet 
Space 
Availability) and Process2 (Order Cabinet). 
• 
Flows: Capturing data/material and information in and 
out the processes. Examples: Cabinet Capacity, Cabinet 
Utilization, New Equipment, etc.  
• 
Events: Asynchronously registered situations that 
trigger the rules. Example: Filling up the cabinet up to 
the max capacity 
• 
Conditions: 
Synchronously 
occurring 
situations. 
Example: Sufficient space in the cabinet to mount a new 
server 
B. Object Properties 
Formally described, the business rules and workflows 
can be constructed in terms of object characteristics. The 
object 
properties 
provide 
information 
about 
the 
characteristics of the objects. For example, the object 
“Process” may have properties such as: process id, name, 
status, creation date, etc. From the viewpoint of the 
conceptualization of our ontology, object properties can be 
classified into one of the following types:  
• 
Identification properties - examples are process id, 
name, type, etc.  
• 
Qualitative description properties - these are 
categorical or nominal properties, which can be 
described qualitatively only - for example status, 
deviation, trend, etc. 
• 
Quantitative description properties - these properties 
can be described using a fixed value that can be 
estimated quantitatively - for example, the number of 
closed processes, etc. 
In [16], object properties are described as a common 
approach to specify characteristics or attributes of a real-
world object instance, which in turn helps to understand how 
to interact with the object. By introducing property 
characterization for each object, our model can fulfil the 
requirements for flexibility and maintainability of the 
formulation of business rules to control processes.  
C. Business Rules 
The structure of business rules is based on the famous 
Event-Condition-Action paradigm [17]. Various business 
rule classifications exist in the literature [18]. In connection 
to BPM systems, the following rule classification were 
identified: 
• 
Initiation Rules  
Initiation Rules depicts rules that specifically initiate a 
process. Depending on the conditions of the rule, a 
process can be launched and thus continue execution. 
Some Initiation rules are driven by events only, these are 
known as Start Event. The business workflows can be 
started only by Initiation Rules after a suitable triggering 
event. The triggering events can be manually or 
automatically invoked. 
• 
Event or Process Rules 
Event or Process Rules group rules that are defined 
during the execution of a process. An example for such 
a rule is the filling up of a container which generates a 
warning about reaching the capacity limit. 
• 
Flow Rules  
Flow Rule formally depicts rules that control the flow of 
processes. Intermediate processes depend on Flow Rules 
(if this is a specific name then capitalize) to progress 
from one process to another.  
39
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

• 
Termination Rules  
The business process terminates based on a Termination 
Rule, which is triggered by suitable termination event 
AFTER the process is finished, or on process execution 
control rule DURING the process execution in the case 
of emergency. In Fig. 1, Execution Rule was used to 
check cabinet space availability. The decision to install 
new network switch onto a cabinet depends on such an 
execution rule. Some Termination Rules are driven by 
events only, hence known as End Event.  
 
Objects are building blocks and they are described in 
this section. Object properties, apart from characterizing the 
objects quantitatively and qualitatively are also the main 
vehicle for analyzing the dependencies between the rules 
which apply to them. The more sophisticated the properties, 
the more elaborate the dependencies that can be formulated. 
To allow the mapping and displaying of identified concepts 
into required classes and properties, a concise and intuitive 
notation such as EBNF [19] can be used. Although other 
notations are possible, EBNF is sufficient for the purpose. 
The term "structured" means that all direct or indirect 
relations between objects and their properties can be 
represented into AND/OR trees. The following is EBNF 
notation for Condition concept, based on the use of objects 
and their properties.  
 
V. 
FORMAL DEFINITION OF BUSINESS RULES, RULE 
RELATIONSHIPS AND DEPENDENCY TREES 
This section briefly presents the formal definition of 
relationships between rules. The section also exemplifies 
business rules dependency trees to map rule relationships. 
A. Business Rules Formal Definitions  
Consider a Business Rule set R containing a collection of 
rule samples controlling business processes. A Rule set R 
has one or more related rules that has been put together to 
guide the movement of processes. For instance, R may be 
made up of Initiation Rule, Flow Rule, Event or Process 
Rules and Termination Rule. Let every Rule in R be 
expressed in terms of {Ri,| i= 1,…, n}.  Each Rule definition 
Ri consists of a collection of Event (E), Condition (C) and 
Action (A). We refer to E, C and A to represent sets of 
Events, Conditions and Actions respectively, containing 
fragments of the Rule R. Now, let E be expressed in terms 
of {Ei,| i= 1,…, n}.  And C be expressed in terms of {Ci,| i= 
1,…, n}. Also A be expressed in terms of {Ai,| i= 1,…, n}.  In 
this research, we will use notation E1i(R1), C1i(R1) and 
A1i(R1) where E1i  E1, C1i  C1 and A1i  A1 to represent 
Business Rule basic definition. Note that for simplicity 
reasons, if a part of the Business Rule has no importance in 
a discussion then it will be omitted. For example, C1i(R1) 
and A1i(R1) will represent a Business Rule that contains 
Conditions and Actions only. 
B. Relations Between Business Rules 
The existence of a dependency between two rules 
expresses that communication occurs between components 
(Event, Condition, and Action) of the Business Rule. For 
example, one Business Rule action may trigger conditions 
of other Business Rules or condition of one Business rule 
may depend on an event of another Business Rule. 
Therefore, Business Rules relationships can be described by 
analyzing Business Rule components relationships. We 
consider the relationship between two rules to be 
represented by the symbol
. For example, R1 
R2 means Rule 1 relates to Rule 2. If one of R1 
action activates event for R2, we declare as A1i(R1) 
 
E2j(R2). Business Rules relationships can be analysed and 
declared in one of the following possible six ways: 
 
These relationships are defined based on Objects and 
Object properties involved in Condition, Event and Action 
components of the Rules. Moreover, relationship can be 
defined 
in 
terms 
of 
qualitative 
and 
quantitative 
characteristics of the object parameters. We examined six 
ways (i-vi) of representing rule relationships based on the 
partial order relationship. However, it is far simpler and 
natural, to apply the tree structure to the model and picture 
the relationships between rules. Therefore, tree structure and 
patterns to show relationship are introduced in the next 
section. Rule patterns are simple enough to represent number 
of rule relationships. However, in practice there can be 
hundreds, thousands or more rule relationships. In systems 
with substantial number of rule relationships, three or more 
rule dimensions are needed to clearly depict the relationship 
structure. This is one of the areas that need to be explored in 
future studies. 
C. Business Rules Dependency Tree 
In our approach the rule dependencies are defined after 
structuring them into dependency trees, which are in the 
form of AND-OR graphs corresponding to the mutual co-
existence of the rules. As the name suggests, the 
relationships will be of two kinds: AND relationships, which 
40
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

group several rules that can be fired simultaneously, and OR 
relationships, which group several rules that can be invoked 
alternatively. There are variants of AND/OR relationships: 
Direct AND Dependency, Direct OR Dependency, Indirect 
AND Dependency and Indirect OR Dependency.  
 
 
 
Figure 2. AND/OR Tree. 
 
The AND/OR tree on Fig. 2 combines all relationship 
patterns: 
• 
Precedence based dependencies 
• 
Level based dependencies 
• 
Path (Chain) based dependencies 
• 
Node based dependencies 
• 
Indirect node based dependencies 
The dependency trees make it easier to understand the 
relationship between rules. The dependencies will be used in 
construction of the algorithm for real-time inference within 
BPM system. Structuring of the rules into dependency trees 
would also allow implementing of more efficient algorithms 
for searching the rules. Different patterns of inclusion of the 
rules in the trees will provide additional information to 
control the flow of execution as the business processes 
progress. In addition, we can use the trees to analyse the 
process behaviour in real time. 
VI. 
PROTOTYPE IMPLEMENTATION 
The implementation of the model presented here is 
currently 
underway 
using 
the 
open 
source 
Rule 
Management System DROOLS [20]. Since it is still work-
in-progress, only preliminary developments are presented. 
DROOLS rule system comes from the area of knowledge 
representation.  The knowledge representation arena is 
concerned with formally representation of knowledgebase 
and reasoning. In DROOLS, a rule has two-parts 
represented using first order logic. The structure of a rule is 
usually WHEN-THEN that is IF-THEN providing logic 
statements. This means we can infer conclusions from rule 
facts stored in the knowledgebase. DROOLS rule system is 
also perfect for rule adaptation and forward chaining. In 
DROOLS, the implementation of business rules is carried 
out using three main components: firstly, the rule class (drl) 
containing the actual rules, second, the fact class (pojo) 
containing the data affected by the rule, and third, the tester 
class (main), which calls both data and rules for execution.  
To manage the rules and processes, our architecture 
implements Event, Condition, Action, Process and Flow 
(Information and Material) as separate fact classes. In 
addition, Initiation Rule, Event Rule, Flow Rule and 
Termination Rule are implemented as subclasses of the rule 
class which is instantiated in the main or tester class to 
allow runtime modification of the rules. This supports the 
reusability and allows adaptation of the rules and their 
components in the case of changes, as well as the definition 
of meta-rules using information associated with rule 
relationships. Fig. 3 illustrates the implementation of the 
Condition class using DROOLS. 
 
Figure 3. Condition class using our approach in DROOLS. 
 
41
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Condition class defines object related configurations 
such as object properties and methods. Its methods include 
getConditionProperty, setConditionProperty etc. as 
shown in Fig. 3. In a Rule class, we simply create an 
instance of a Condition class. This automatically inherits 
the default properties and methods of the Condition class. 
The other concepts of the ontology are implemented in a 
similar way. 
VII. CONCLUSION AND FUTURE WORK 
The paper presented a two-level architecture of BPM 
system, which supports efficient solution for adaptation of 
business rules, thanks to the incremental indexing of the 
rules and the formalisation of structural patterns of 
dependencies between them. This architecture supports BPM 
professionals and academics with adequate means for 
modelling of both business process workflows and business 
rules. In addition, it is the basis for a seamless integration of 
an efficient algorithm for adaptation of the business 
workflows to the changing conditions.  
A prototype of the above model is being implemented in 
DROOLS using object-oriented (OO) technology. In this 
approach both the business workflow processes and business 
rule components are implemented as objects. Two of the 
fundamental features of OO technology, the encapsulation 
and the inheritance, are used conveniently for implementing 
the architecture following a bottom-up strategy. This 
approach allows the build up of the indexing mechanism in 
an incremental manner. The plan, on the next stage, is to 
complete the implementation of two separate inference 
engines on top of the model: a forward chaining inference 
algorithm, which account the logics of business process 
workflows and controls their execution, and a backward 
chaining inference engine, which propagates the changes and 
adapts the rules in real-time. Work has already begun on a 
series of algorithms, which account for the relationships and 
the dependencies between the rules. Our focus here will be in 
exploring the structural patterns of the rule relationships and 
the influence on the inference on Rule level.  
The architecture presented here has wide potential for 
applying 
BPM 
systems 
in 
many 
areas, 
such 
as 
manufacturing, chemical process control, healthcare and 
anywhere, where the business processes can be described in 
terms of operational workflows. The big advantage of this 
architecture is the ability to modify the business rules logics 
without interrupting the business workflows. Moreover, by 
adding some meta-rules it could become possible to test the 
production rules and achieve consistency.  
Other issues, which may be beneficial to explore further 
involve the relationships between different components of 
the model, i.e., relationships between rules and user roles, 
relationships 
between 
processes 
and 
business 
data, 
relationships between processes and workflows, etc. 
ACKNOWLEDGMENT 
The work reported here has been partially sponsored by 
Vertiv Co, formerly Emerson Network Power. 
REFERENCES 
  [1] K. Ezekiel, V. Vassilev, and K. Ouazzane, “Adaptive Business 
Rules Framework for Workflow Management”, to appear. 
  [2] S. D. Hendrick, K. E. Hendrick, Business Value of Business 
Rules Management Systems, IDC #231195, 2012. 
  [3] A. Macdonald, The value of IBM WebSphere ILOG BRMS, 
2010, IBM. [Online]. Available from: https://www01.ibm.com 
/software/integration/business-rule-management/jrules-family/ 
retrieved: 12.2017. 
  [4] J. Boyer, H. Mili, Agile Business Rule Development: Process, 
Architecture, and JRules Examples, Springer Science and 
Business Media 1, 2011, ISBN: 9783642190407. 
  [5] P. Haley, Confessions of a production rule vendor, 2013. 
[Online]. Available from: http://haleyai.com/wordpress/ 2013/ 
06/22/confessions-of-a-production-rule-vendor-part-1/ 
retrieved: 12.2017. 
  [6] J. C. Giarratano, CLIPS User’s Guide, 2003. [Online]. 
Available from:http://www.ghg.net/clips/download/documen-
tation /usrguide.pdf; retrieved: 12.2017. 
  [7] J. Giarratano, G. Riley, Expert Systems: Principles and 
Programming, Course Technology, 2004, ISBN: 0534384471. 
  [8] E. Friedman-Hill, Jess in Action: Java Rule-Based Systems, 
Manning Publications, 2003, ISBN: 1930110898. 
  [9] A. Grissa-Touzi, A, H. Ounally, and A. Boulila, “VISUAL 
JESS: An expandable visual generator of oriented object expert 
systems”, Engineering and Technology, pp. 108–111, 2005. 
[10] R. Thirumalainambi, “Pitfalls of JESS for Dynamic Systems”, 
Art. Intelligence and Pattern Recognition, pp. 491-494, 2007. 
[11] J. Feldman, Creating, Testing, and Executing Decision Models 
with 
OpenRules, 
2011. 
[Online]. 
Available 
from: 
http://slideplayer.com; retrieved: 12.2017. 
[12] M. Salatino, M. De Maio, and E. Aliverti, Mastering JBoss 
Drools, Packt Publishing, 2016, ISBN: 1783288620. 
[13] Lijun, Introducing a rule-based architecture for workflow 
systems in retail supply chain management. MSc Thesis, 
University of Borås School of Business and IT, Sweden, 2012. 
[14] F. Rosenberg, C. Nagl, and S. Dustdar, “Applying Distributed 
Business Rules – The VIDRE Approach”, IEEE Int. Conf. on 
Services in Computing, Chicago, IL, USA, pp. 471–478, 2006. 
[15] M. Thirumaran et al., “Business rule management framework 
for enterprise web services”, Int.J. Web Service Computing, 
Vol.1, No.2, pp. 15-29, 2010. 
[16] Y. Sun, B. Liefeng, and D. Fox, “Learning to Identify New 
Objects”, IEEE Int. Conf. on Robotics and Automation, Hong 
Kong, pp. 3165 – 3172, 2014. 
[17] F. Bry, M. Eckert, P.-L. Patranjan, and I. Romanenko, 
“Realizing Business Processes with ECA Rules: Benefits, 
Challenges, Limits”, Int. Workshop on Principles and Practice 
of Semantic Web Reasoning, Springer, pp. 48-62, 2006. 
[18] P. Jayaweera, M. Petit, “Classifying Business Rules to Guide 
the Systematic Alignment of a Business Value Model to 
Business Motivation”, Proc. Int. Workshop on Business/IT 
Alignment 
and 
Interoperability, 
Collection 
CEUR, 
Amsterdam, Vol, 456, 2009. 
[19] International Organization for Standardization, Information 
technology. Syntactic metalanguage. Extended BNF, ISO/IEC 
14977:1996, pp. 1-22, 1996. 
42
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

 
Detecting Adverse Events in an Active Theater of War Using Data Mining 
Techniques 
 
 
Jozef Zurada 
University of Louisville 
USA 
WSB Gdansk 
Poland 
jozef.zurada@louisville.edu 
 
Donghui Shi 
Anhui Jianzhu University 
China 
sdonghui@gmail.com 
 
 
 
Waldemar Karwowski 
University of Central Florida 
USA 
wkar@ucf.edu 
 
Jian Guan 
University of Louisville 
USA 
jeff.guan@louisville.edu 
 
Erman Cakit 
Aksaray University 
Turkey 
ermancakit@aksaray.edu.tr
Abstract – This study investigates the effectiveness of data 
mining techniques in detecting adverse events based on 
infrastructure development spending, the number of project 
types, and other variables in an active theater of war in 
Afghanistan using data sets provided by the Human Social 
Culture Behavior program management (2002-2010) of the 
U.S. Department of Defense. The study first applies feature 
reduction techniques to identify significant variables, then uses 
five cost-sensitive classification methods and reports the 
resulting classification accuracy rates and areas under the 
receiver operating characteristics charts for adverse events for 
each method for the entire country and its seven regions. The 
results show that when analysis is performed for the entire 
country, there is little correlation between adverse events and 
project types and the number of projects. However, the same 
type of analysis performed for each of its seven regions shows a 
connection between adverse events and the infrastructure 
budget and the number of projects allocated for the specific 
regions and time periods. Among the five classifiers, the C4.5 
decision tree and k-nearest neighbor provided the best global 
performance. 
 
Keywords: active war theater; data mining; adverse events; 
prediction; classification 
 
I. 
INTRODUCTION 
 
The U.S. Department of Defense (DoD) uses the 
following definition for irregular warfare: "a violent struggle 
among state and non-state actors for legitimacy and 
influence over the relevant population(s)." Irregular warfare 
is a non-conventional warfare which includes non-
proportional force to subdue and coerce the civilian 
population in the regions in which opposite forces are not 
large and effective. The success of irregular warfare 
operations depends heavily on protecting the civilian 
population by the military as the civilian population is the 
primary target of irregular warfare [1]. Recognizing the 
challenges of the dynamic of irregular warfare among 
various actors, the U.S. military has made some changes and 
accommodations to its force structure. Also, the DoD 
initiated and developed the Human Social Culture Behavior 
(HSCB) modeling program. The main goal of the program 
was to guide and help the U.S. military in understanding 
different cultures while operating in overseas countries and 
to better organize and control the human terrain during 
irregular warfare. The military uses HSCB models to 
understand the behavior and structure of organizational units 
at the macro level (i.e., health, politics, energy, economics, 
security, water and sanitation, and social and cultural aspects) 
and at the micro level (i.e., terrorist networks, tribes, customs, 
and military units). These HSCB models are important and 
attract a great deal of attention with regard to current and 
future operational military and non-military requirements. 
These models are also very complex as they exhibit non-
linear and fuzzy behavior and are often ill-defined with 
respect to their socio-economic-cultural factors. 
 
II. 
PREVIOUS WORK 
 
Several studies have attempted to develop models of 
human behavior from patterns identified in the data in order 
to predict the effects of actions aimed at disrupting terrorist 
networks [2]. Since terrorist attacks are not random in space 
and time, it is possible to discover representative patterns and 
trends in adverse activity or behavior over time and space by 
analyzing the geospatial intelligence on reported incidents. 
The studies concluded that these patterns and trends could be 
used for prediction future attacks and that they might help 
decision-makers to allocate more resources and personnel to 
the places which are more likely to be attacked and also to 
try reduce the number of such attacks. These studies used 
43
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

 
fuzzy inference systems (FIS), adaptive neuro-fuzzy 
inference system (ANFIS) and wavelet neural networks to 
analyze terrorist attacks time series.  
Other studies built models based on the input variables 
such as infrastructure development spending projects, the 
number of projects, and population density. The models 
applied multiple linear regression, data mining and soft 
computing techniques such as neural networks, ANFIS, and 
FIS as well as fuzzy C-means and subtractive clustering for 
predicting four categories of adverse events, i.e., the number 
of killed, the number of wounded, the number of hijacked, 
and the number of events at month t in an active theater of 
war in Afghanistan [3]. These four categories of events are 
collectively called "adverse events". The studies performed 
analysis for the entire country and its seven regions and used 
variable reduction techniques to eliminate redundant 
attributes as well as implemented sensitivity analysis for the 
neural network to determine the cause and effect relationship 
between the input and output variables. However, due to the 
sparse nature of the input and output data (between 87% and 
98% of values for the four adverse events are 0ʹs, with a 0 
representing lack of events), the prediction errors generated 
by the models for the four adverse events were significantly 
high. Thus due to the unbalanced nature of the data precise 
prediction of the number of four adverse events was an 
extremely challenging and difficult task. 
 
III. 
DATA SETS 
 
The data sets for the five mentioned studies and this study 
were provided by the HSCB program management of the 
U.S. DoD. The time-dependent data were collected over the 
years 2002 through 2010 and represent more than 30,000 
records and over 100 variables. Among other variables, the 
data sets included the following input variables: the budgeted 
amount [$US] for 14 categories of infrastructure investments 
in the areas such as Agriculture and Health, the number of 
14 project types at years t-2, t-1, and t, as well as the 
mentioned four categories of adverse events at month t-1, 
seven regions, and the male and female urban and rural 
population densities. The output variables included the 
mentioned four categories of adverse events at month t. 
 
IV. 
DESCRIPTION OF THE STUDY AND 
RESULTS 
  
This study investigates the effectiveness of data mining 
techniques in detecting/classifying adverse events based on 
the infrastructure development spending in 14 project 
categories, the number of project types, and other variables 
in an active theater of war in Afghanistan using the same data 
sets that were used in the five mentioned studies. First, the 
study recodes the four output variables (the number of killed, 
wounded, hijacked, and events) representing adverse events 
into the binary representation, i.e. two classes. For example, 
killed (Yes or 1) or not killed (No or 0) or an event happened 
(Yes or 1) or did not happen (No or 0). Then it applies feature 
reduction techniques to identify significant variables. Next 
to compensate for class imbalances, the study uses five cost-
sensitive classifiers such as neural networks (NN), k-nearest 
neighbors (k-NN), C4.5 decision trees (DT), support vector 
machines (SVM), and random forest (RF) to detect adverse 
events. Finally, the study reports the resulting classification 
accuracy rates and areas under the receiver operating 
characteristics (AUROC) charts for the four adverse events 
for each classifier for the entire country and its seven 
regions. The AUROC values, which testify to the global 
performance of the classifiers, are measured on the [0.5, 1] 
scale, where 0.5 and 1 indicates a bad classifier and a good 
classifier, respectively. For example, the AUROC values for 
the entire country for the four adverse events were within the 
[.688, .805] range. The results show that the AUROC values 
for events are generally higher than the AUROC values for 
dead, wounded and hijacked; and that the AUROC values for 
hijacked are generally lower than the AUROC values for 
dead, wounded and events. The hijacked category was the 
most highly underrepresented in the data sets. 
The results show that when analysis is performed for the 
entire country, there is little correlation between adverse 
events and project types and the number of projects. 
However, the same type of analysis performed for each of its 
seven regions shows a connection between adverse events 
and the infrastructure budget and the number of projects 
types allocated for the specific regions and time periods. For 
example, for region Eastern the following variables (project 
categories) 
were 
identified 
as 
significant: 
Energy, 
Governance, Emergency Assistance, and Gender, as well as 
urban male and female population densities, rural female 
population density, killed at month t-1, and number of events 
at month t-1. Among the five classifiers, the DT and k-NN 
generated the best rates in terms of global performance. 
 
V. 
CONCLUSION 
 
The models presented in this study could support 
decision makers who analyze historical economic data on 
how regional funds allocation can best help minimize 
adverse events. Though the models used Afghanistan data, 
they may be applicable for other countries that are looking 
to build infrastructure while the threat of terrorist and 
military activities are present. 
 
REFERENCES 
 
[1]. J. Clancy, and C. Crossett. "Measuring effectiveness in 
irregular warfare", Parameters, 37(2), 88-100, 2007. 
[2]. D. Schmorrow, and D. Nicholson. Advances in Cross-cultural 
Decision Making. Boca Raton: CRC Press, Chapter 38, 374-384, 
2011. 
[3]. E. Çakıt, and W. Karwowski. "Predicting the occurrence of 
adverse events using an adaptive neuro-fuzzy inference system 
(ANFIS) 
approach 
with 
the 
help 
of 
ANFIS 
input 
selection". Artificial Intelligence Review, 48(2), 139-155, 2017. 
44
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

Revenue Optimization of Telecom Marketing Campaigns for Prepaid Customers
Maurus Riedweg
Consulteer AG, Switzerland
Email: maurus.riedweg@consulteer.com
Pavol Svaba and Gwendolin Wilke
Lucerne School of Information Technology, Switzerland
Email: {gwendolin.wilke, pavol.svaba}@hslu.ch
Abstract—The design and optimization of marketing campaigns
today usually still includes a high level of manual expert in-
volvement. This applies particularly to the prepaid mobile phone
sector of the highly competitive telecommunication industry.
Since prepaid telecom customers are characterized by highly
volatile and sparse usage data their future behavior is hard
to predict, and marketers often rely mainly on experience and
gut feeling when designing marketing campaigns, using only
simple data analysis tools. The project developed a methodology
and software prototype that helps marketers in this area to
exploit the full potential of real-time big data-driven analytics for
microtargeting, allowing them to make fact-based and informed
decisions. Speciﬁcally, it provides an interactive solution for the
semi-automated visual support of the design and optimization
of single-channel marketing campaigns. The developed solutions
bring a huge step towards the automation of the whole process
of optimizing marketing campaigns in the telecommunication
business, keeping the possibility of interactive interventions of
marketers to implement strategic management decisions or use
their expert knowledge. The system provides enough information
for marketer to comprehend the reasons for the decision and
by retracing it provides precious insights for the design of new
campaigns. The solution uses machine learning closed loop and
intuitive visualization based on nomograms and was prototypi-
cally implemented on Apache Spark big data stack and evaluated
on sample data from two real-world prepaid telecom use cases.
Keywords–telecom; churn prediction; predictive analytics; Na¨ıve
Bayes; Nomogram
I.
INTRODUCTION
Classical data mining for marketing campaigns is usually
a time consuming task. Despite the availability and use of
automated analytics algorithms, the overall process of design-
ing, adapting, executing and evaluating a marketing campaign
still includes a high level of expert involvement and manual
decision-making. In the age of big data, the increased volume
and velocity of available data allows for added business
insights and helps companies to stay competitive. At the same
time, it becomes increasingly difﬁcult to ﬁne-tune the de-
sign and management of analytics-based marketing campaigns
manually. Therefore, to fully tap the potential of real-time
big data marketing analytics, a high degree of automation is
needed.
The success rate of a marketing campaign depends on tar-
geting the right customer needs and preferences. Consequently,
there is an increased need for highly targeted marketing cam-
paigns that are tailored to the needs and preferences of speciﬁc,
possibly small customer groups (micro-segmentation) or even
individuals (direct marketing campaigns). This requires highly
agile marketing campaign management facilitating fast reac-
tion times to customer or market events despite continuously
increasing data volumes. Additionally, structural adaptations
that allow for reacting to changes in the market structure as a
whole must be accounted for. There is a trend in marketing to-
wards building long-lasting relationships with customers. Since
the cost for customer acquisition is much greater than the cost
of customer retention [1], marketers have become increasingly
interested in the latter [2]. Customer retention models aim at
furthering customer loyalty and preventing active customers
from changing to other providers (churning) via marketing
campaigns. To achieve this, tools that support the development,
management and application of customer retention models and
the corresponding marketing campaigns are required and are
essential Business Intelligence (BI) applications. In telecom
business, the customer churn term refers to the customer
turnover, i.e., loss of a service subscriber. The common reasons
for churn are dissatisfaction with an existing provider, the lure
of a lower price for equivalent service or better service for the
same price from a different provider. Churn rate, the proportion
of churned clients during a given time period, is one of the key
business metrics and an indicator for customer dissatisfaction.
Churn prediction modeling techniques attempt to under-
stand the precise customer behaviors and attributes, which
signal the risk of customer churn. In predictive analytics, the
typical approach to data-driven churn prediction is to use
a sufﬁciently large historical data set of customer records,
containing churning and non-churning customers. The records
contain customer attributes such as age, gender, tariff, average
call frequency, etc., together with the information if a customer
has churned at some point in time or not. The data set is used
as training set for supervised learning to construct a classiﬁer,
i.e., a predictive model that classiﬁes a customer as a potential
churner or non-churner based on the knowledge of the other
attribute values (the predictor variables). In an actual prediction
task, the model is applied to a new data record, where the
values of the customers predictor variables are observed while
the possible churn event lies in the future, so that value of
the class variable churn is unknown. The classiﬁcation done
by the model is used as a prediction of churn or non-churn
of the customer segment that corresponds to the respective
predictor attribute valuations (the feature vector). In case the
system has predicted a customer segment of likely churners,
a suitable marketing action can be launched to prevent these
customers from churning. To select such an appropriate action
the underlying reasons for their churning must be analyzable,
i.e., a proper insight on the reasons for churning is essential
in order to design effective retention methods.
Predictive analytics models traditionally used for data-
driven customer churn prediction are Decision Trees [3]–[8]
and Regression analysis [9] and [10], which have been com-
45
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

plemented with Na¨ıve Bayes classiﬁers and Artiﬁcial Neural
Networks [11]–[13] in recent years [14] and [15]. In Na¨ıve
Bayes classiﬁer (NBC) models, the attributes of a domain are
interpreted as random variables and are represented as vertices
in a probabilistic graphical model. Direct inﬂuences of the
observed predictor variables on an unobservable target variable
are represented as directed edges in the graph. The result is
a simple tree graph structure. The strength of inﬂuence of
input variables on the target variable is given by conditional
probability tables, which are learned from historic data. In a
prediction task, Bayes Rule [16] is used to infer from values
of the observed variables the probability of possible values
of the unobserved target variable. NBCs are thus map based
classiﬁcations with the additional assumption of conditional
independence of input variables. Examples of applications
using NBCs for churn prediction are Nath and Behara [17] and
and Kirui et. al. [18] or Shaaban et. al. [13]. Other predictive
models that have been used for churn prediction are, e.g., rule
based classiﬁers (Ripper, PART), Nearest Neighbor (KNN),
Self Organizing Maps [19], Genetic Algorithms [20], Linear
Discriminant Analysis [21], Support Vector Machines [13],
Sequential Pattern Mining and Market Basket Analysis [22]
or Rough Sets [23].
As a reaction to the increasing data volumes in the telecom-
munication sector, approaches towards churn prediction on
massive data sets have been investigated. For example, Kamal-
raj and Malathi [24] discuss the use of data mining techniques
on big data clusters in the telecommunication industry. Balle
et. al. [25] describe a prototype for churn prediction using
stream mining methods, which offer the additional promise
of detecting new patterns of churn in real-time streams of
high-speed data, and adapting quickly to a changing reality.
Different scalable algorithms have been developed and imple-
mented, e.g., Apaches open source libraries Mahout and MLlib
include, besides scalable regression models, decision trees and
customer segmentation models, also Na¨ıve Bayes Classiﬁer
learning.
The research presented in this paper was funded by the
Swiss Commission for Technology and Innovation CTI and is
part of the project done together with two expert partners, one
software developer with extended history in telecommunica-
tion business, and one large telecom service provider located
in Switzerland.
The paper is organized as follows. After short introduction
we describe the problem and its requirements in Section II.
In Section III, we address the conceptual model. Section IV
brings the insights of development of the predictive analytics
and visualization core based on data analysis done in statistical
environment R. Section IV shortly describes the implemented
big data solution. Section V summarizes the project results and
Section VI concludes its achievements.
II.
PROBLEM DESCRIPTION
The mission of this project is to develop a methodology and
a software prototype for semi-automated predictive marketing
analytics and campaign management. It will automate the
life cycle of the data mining and the campaign management
process as a whole, including feature selection, model learn-
ing, prediction and decision making; iterative dynamic model
adaptation over time will allow for closing the analytics loop.
In particular, our goal was to optimize campaign targeting
in prepaid segment of a large Swiss telecom company. We
aim on customer retention campaigns in general, or on churn
prevention in a special case. The existing system, and its
data, puts some critical constraints we have to deal with.
For example, the provided historical data are aggregated and
contain time series from the narrow time slots before and after
the campaign offer. Within a campaign, the existing system
chooses a ﬁxed number of customers each day and offers them
a particular bonus. The offer is valid for couple of days, and
expires afterwards. Some particular campaigns with a bonus
credit have been chosen as a proof of concept. The campaigns
run for a deﬁned period of days, their results are reported on
a daily basis, and summarized at the end of the campaign.
The designed solution must possess the following properties:
(1.) Big data-driven knowledge discovery allows for detection
of hidden patterns in the growing numbers of data records
and customer attributes available;
(2.) A high degree of automation in analytics, prediction,
decision-making and campaign execution allows for
timely reaction to events, and allows to cope with big
data sets;
(3.) Self-adaptability accounts for changes in the data sets and
the domain structure, as well as for lessons learned of past
campaigns, embedding them in a closed feedback loop for
autonomous campaign optimization;
(4.) Scalability of the algorithms to a distributed big data
processing environment provides the necessary computing
power to analyze the increasing amount and velocity of
(big) data sets in time;
(5.) A high degree of model accuracy allows for detailed or
even personalized marketing decisions;
(6.) A high degree of interpretability (comprehensibility) of
the domain model and derived predictions, decisions
and automatic campaign executions ensure acceptance by
marketers.
In the marketing ﬁeld, acceptance is of pivotal importance.
Campaign managers must be able to explore inﬂuence factors
and variable dependencies in order to design appropriate
marketing instruments (such as special offerings for young
adults with high sms-frequency). They also need to understand
the rationale for an automatically triggered marketing action
in order to be able to justify it to potential business customers
and to top management, e.g., by pinpointing relevant inﬂuence
factors. Even though the high degree of automatization is
desired, the choice of contents of a marketing campaign is
usually not entirely reducible to structured information and
often includes highly emotional contextual information (such
as the choice of a protagonist in a TV spot), therefore the
possibility of manual intervention of marketing campaign must
be guaranteed by any decision support system in the marketing
area. Particularly, it must allow the campaign manager to
integrate his experience and real-world background knowledge,
as well as to implement strategic managerial requirements.
The whole marketing effort serves the purpose of sustain-
able increase (or of interrupting decrease) of the usage of the
provider’s service, in extreme case, of preventing clients churn.
In contrast to postpaid segment, ultimately fewer resources
are used in prepaid segment on maintaining contacts with
customers and marketing analytic activities in general. This
lack of interest in prepaid segment is caused by its low market
share in Switzerland. As a consequence, we rely here purely
46
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

on data induced by the service usage. Further, due to its
nature, the service usage by prepaid customers is usually
not as homogeneous as by postpaid clients. Thus, the data
available on prepaid users are much more inadequate. This
adds to the complexity of modeling and causes a decrease
in the model accuracy. The existing marketing solution rates
the success of the campaign based on the acceptance ratio.
This clearly does not bring the desired effect. Customers,
which exploit the offered bonus but do not change their
usage habit, are counted as proﬁtable, whereby in real they
generate a loss. Additional requirements have been set on
the designed system and its models, such as simplicity and
easy interpretability, possibility of intervention, capability of
effective and manageable visualization, scalability, ease of use.
III.
CONCEPTUAL MODEL
Two use cases have been speciﬁed based on obtained
campaign data:
(1.) the semi-automatic support of optimized campaign tem-
plate instantiation for increased acceptance ratio, and
(2.) the semi-automatic support of campaign template design
for increased revenue creation.
Speciﬁcations included existing situation, description, use case
properties (actors, stakeholders, levels, triggers, frequency,
preconditions, post-conditions), data characteristics (names,
accessibility, type, volume, temporal resolution, heterogeneity,
inconsistency, constraints, analysis needs) and possible use
case extensions. They are the result of workshops with the
telecom partner prepaid marketing team and a number of
project internal workshops (HSLU-I and Consulteer AG).
The conceptual solution addresses the following issues:
(a.) automatization of the whole marketing process (from data
mining to decision making);
Closed loop architecture with constantly trained and up-
dated model has been speciﬁed and build for the de-
veloped prototype, containing modules for feature pre-
selection, prediction, evaluation and adaptation. This was
done in two stages. First, methods for modules have
been speciﬁed and veriﬁed with small data in statistical
environment and programming language R. Second, the
modules have been ported to big data stack (see Section
IV) and the loop has been closed. The automatization
of the whole marketing process remains a challenge, the
reasons are twofold: (a.) technical – the built prototype
has to be incorporated and veriﬁed within the live system;
(b.) non-technical – the existing marketing processes have
to be rebuild towards the high level of automatization and
utilizing the implemented solution.
(b.) allow interactive intervention of marketers (e.g., to imple-
ment strategic management decisions or experience based
knowledge);
The selected approach heads towards micro-segmentation
by categorization, i.e., ﬁltering out clients with particular
combination of input feature values, which evidence the
potential of revenue gain if those clients are presented
with the bonus. The decision making is presented to
marketer in a form of recommendation to select particular
small set of clients for the campaign (microtargeting). The
implemented solution utilizes the intuitive visualization
based on nomogram paradigm (see Section IV). It allows
marketer to manually override the automatically chosen
(optimal) feature combinations with preferred values and
presents the gain/risk predictions for the selected choice.
(c.) provide interpretability of the automatically determined
customer segments needed to support campaign design;
The solution based on NBC and nomogram visualization
satisﬁes stringent conditions set on the designed system,
such as scalability, easy interpretability, possibility of
intervention, etc. As opposed to black box models such
as neural networks (NN) or random forests, the Na¨ıve
Bayes Model provides enough necessary information for
marketer to comprehend the reasons for the selection of a
client for a campaign. By retracing the decision up to the
input features, and identifying those mainly responsible
for the classiﬁers choice, it provides precious insights
for the design of new campaigns and allows marketing
managers to justify and explain the algorithm’s results
to their management. This is in contrast to black-box
approaches such as deep NN. These have demonstrated
strong potential on ﬁnding hidden patterns in big data
collections, but do not comply with the requirement of
interpretability by marketing managers.
(d.) prediction algorithm needs to be able to deal with volatile
and sparse prepaid data;
Based on data analysis done with provided campaign data
and consistent with the campaign management restric-
tions, the methods have been chosen for data cleaning and
transformation, and feature selection, as well as suitable
prediction algorithm.
(e.) usage of behavior change instead of acceptance rate as
a success measure.
Through redesigning the metric for campaign success
towards more reliable and long-term change indicator (see
Section IV).
The prediction model comprises six parts:
(i.) data transformation methods;
(ii.) input feature construction method (based on data explo-
ration results);
(iii.) feature pre-selection method with dynamic binning;
(iv.) method for target variable construction (implementing a
new campaign success measure for behavior change);
(v.) Na¨ıve Bayes prediction (classiﬁcation);
(vi.) automated customer selection according to class predic-
tions.
Closed loop learning enables a predictive analytics solution
to automatically adapt to changing conditions. To achieve this,
feedback loops must be integrated in the model architecture.
For the given use cases, four levels of feedback have been
identiﬁed: (1.) Data Level Feedback, (2.) Campaign Level
Feedback, (3.) Causal Loops, and (4.) Model Level Feedback.
Two of the four feedback levels have been selected for imple-
mentation (Data Level and Campaign Level Feedback).
IV.
DATA ANALYTICS
In telecommunications, the usage data typically consists
of time series for different variables such as number of calls,
number of SMS sent, date of credit charge and charged value,
revenue, etc. The provided data contain for each customer and
each variable (called here accumulator) aggregated data for at
least three months before and one month after the offer. In
47
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

the Figure 1, we show single accumulator data of two chosen
customers. The UserX accepts the offer (at time denoted by
dashed line A) but does not change its behavior remarkably
(offer causes a small boost in his usage but it continues to
sink which is observable by the slope of the blue line); the
UserY shows the evident change of the behavior and generates
notable growth in the revenue compared to the predicted values
(green triangles). It is also possible that we witness the churn
prevention in the case of UserY.
G
G
G
G
GGGGGG
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
GG
G
G
G
G
G
GGG
GG
G
G
G
G
G
GGGG
G
GG
G
G
G
0
1
2
3
4
time
avg. revenue p.D.
−80
−60
−40
−20
0
10
20
30
40
50
60
70
80
A
UserX
G
G
G
G
GGGGGG
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GGG
GG
G
G
G
G
G
GGGG
G
GG
G
G
G
G
G
GGGGGGG
G
G
G
GGG
G
GG
G
GG
G
G
G
GGGG
G
GG
G
G
G
G
G
G
GGGGGGG
G
GGGGG
G
G
G
G
G
G
GG
GG
GGG
G
0
1
2
3
time
avg. revenue p.D.
−80
−60
−40
−20
0
10
20
30
40
50
60
70
A
UserY
GGGGGGG
G
G
G
GGG
G
GG
G
GG
G
G
G
GGGG
G
GGGGGGG
G
GGGGG
G
G
G
G
G
G
GG
GG
GGG
G
Figure 1. Customer Data Examples
The goal of the campaign is to motivate customers to
increase their usage of the service. This is even the weaker
condition on the selection of customers and a more general
case as the churn prevention. For this purpose, it is important
to classify which of the possible customers are likely to
generate more usage and thus more proﬁt for the company.
Blind assignment of the offers can lead to considerable money
loss. E.g., in the case, when a customer accepts the offer
but does not change its usage habit (e.g., only exploits the
bonus credit). Further, in the opposite case, which also has
to be taken into account, the customer feels harassed by the
offers he is not interested in, may even decide to churn. We
utilize the classical data-driven machine learning pipeline with
preprocessing phase (data cleaning and transformation), model
building phase (feature extraction, model training and testing
loop) and deployment phase. The trained classiﬁer chooses the
client for a campaign when it assumes that by doing so, he
will increase his usage of the service and thus revenue of the
company.
The current solution uses acceptance ratio as measure for
the campaign success. We propose conversion shift towards
more reliable and long-term behavior change indicator. We
regress predictions of the accumulator values after the offer
(green triangles, see Figure 1) and train the classiﬁer with the
new target variable set as the difference between predicted and
real (measured) value. The predictors are variables extracted
from the different accumulators, such as average number of
national calls, average revenue, etc. The correlation coefﬁcient
is used to ﬁlter-out the predictors with non-signiﬁcant inﬂuence
on the target. In order to provide the “possibility of interven-
tion”, “capability of visualization”, as well as the high degree
of “comprehensibility and interpretability” that is needed for
acceptance of an automated solution in the marketing daily
business, the Na¨ıve Bayes approach has been chosen.
NBC is simple and effective technique based on the
Bayesian theorem. Even though, the Na¨ıve Bayes is not
the preferred classiﬁcation method, its performance is often
underestimated. It is fast and space effective, not sensitive to
irrelevant features, and can handle streaming data well. For
more information, the reader is referred to [26] and [27]. In
particular, the binary NBC, where the target class can take
only two possible outcomes, allows very elegant visualization
with Nomograms [28]. (In case of multi-class classiﬁcation
problem, we create nomogram for one particular outcome
class and the union of remaining outcomes represent the
complement class.) The NBC nomogram provides a way to
visualize the strength of inﬂuence of each input feature to a
Na¨ıve Bayes classiﬁcation result, depending on its value or
category. It assigns point scores to every predictor variable
depending on the chosen feature value. The higher the range
of the feature score is, the higher is its inﬂuence to the classi-
ﬁcation result. It thus provides marketers with the possibility
to graphically explore how the choice of different feature
values (i.e., attributes of a customer) inﬂuence the overall class
probability (e.g., the probability of accepting a marketing offer
or probability of revenue increase). The point scores of all
features are easily summed up together and translated into
the resulting overall probability. By ﬁxing some attributes to
particular values, we effectively ﬁlter the target population to
microtargets. This allows us to optimize the target with respect
to its resulting class probability and size.
−Inf − −0.1
−0.1 −  0.0
 0.0 −  0.1
 0.1 −  0.2
 0.2 −  0.5
 0.5 −  Inf
arpuDf
−0.4
−0.2
0.0
0.2
 −Inf − −10.5
−10.5 −  −5.2
 −5.2 −  −2.9
 −2.9 −  −1.2
 −1.2 −   0.0
  0.0 −   Inf
arpuS
−0.3
−0.2
−0.1
0.0
0.1
0.2
0.3
0.4
−Inf − −5.0
−5.0 − −2.0
−2.0 − −0.8
−0.8 − −0.1
−0.1 −  0.4
 0.4 −  Inf
callNreS
−0.15
−0.10
−0.05
0.00
0.05
0.10
0.15
0.20
−Inf − −0.3
−0.3 −  0.0
 0.0 −  0.2
 0.2 −  0.3
 0.3 −  0.5
 0.5 −  Inf
relCoS
−0.6
−0.4
−0.2
0.0
0.2
0.4
Figure 2. NBC Nomogram with 4 accumulator attributes.
As an underlying infrastructure, big data technology stack
has been built based on the Apache SPARK framework for
cluster computing. The main components used are: MLlib with
Elasticsearch has been used to implement the Na¨ıve Bayes
prediction module, Cassandra for scalable storage and high
performance data access in closed loop learning, Kafka for
real-time streaming capability, and Kibana as a visualization
component.
48
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

V.
RESULTS
A conceptual model of a semi-automated visual campaign
design support module has been developed (based on nomo-
grams) that directly shows predicted economic gain/loss in
dependency of the selected input variables. More precisely,
the model is capable of (1.) visualizing the inﬂuence of indi-
vidual buckets and features on predicted economic gain/loss
(2.) visualizing the impact of manual bucket/feature choices
interactively, (3.) recommending optimal buckets for a given
feature set, and (4.) recommending optimal features for a
given campaign. The presumed adaptiveness provided by the
closed loop learning module could not be evaluated, since no
appropriate data has been provided.
The prediction module has been developed and evaluated
on small data using software environment and programming
language R (because of data availability). Yet, the used algo-
rithms are scalable and have been also implemented with the
equivalent Spark algorithms to provide a proof of concept (1.)
of the implemented big data stack itself (see Section IV) and
(2.) to validate the R test results for Spark.
The evaluation results of the developed approach show
that (1.) the accuracy of the developed prediction module
exceeds the accuracy of the two benchmark models used
(telecom expert decisions and classiﬁcation by a decision tree)
and (2.) intuitively and understandability of Visualization and
Decision Model are satisfactory and have been accepted by the
telecom company. Based on the NBC nomogram visualization
paradigm, a decision model and an interactive GUI prototype
has been developed that allow for semi-automated campaign
design support. While the prediction module provides rec-
ommendations for customer segmentation w.r.t. an existing
campaign, the design of a new campaign (or the optimization
of an existing one) additionally requires interpretability of
these recommendations in terms of strength of causal inﬂuence
of input variables and their multidimensional interaction. The
NBC nomogram paradigm provides this interpretability in
an intuitive way. With the GUI, the marketer is provided
with the opportunity (1.) to set manual constraints for an
automated optimizer (e.g., to comply with given management
strategies), (2.) to choose these constraints based on intuitive
multidimensional visualization of feature nomograms, and (3.)
to graphically inspect the predicted (simulated) effects of his
decisions in terms of expected revenue change.
Even though the performance of the utilized prediction
model is not astonishing, it meets the stringent requirements
speciﬁed on the designed system and still achieves satisfying
results if compared with the existing solution. We measured
the average gain per person for customers chosen by the
trained model and compare it with the existing campaign
results. The decreased performance of the prediction model
is caused by the low quality of provided data and lower
stability of service usage in prepaid segment in general as
mentioned in Section II. In Figure 3, we show veriﬁcation
results of 100 rounds performed by the trained classiﬁer. The
left three boxplots show the statistic of the data from the
chosen marketing campaign if present targeting approach is
used. The targeted users have been split into three groups:
those which accepted the marketing offer, those which did
not accept it (expire) and the control group (baseline). The
red boxplot shows average arpu per person and day if micro-
targeting with NBC model is used, the blue box on the right
summarizes the advantage of the new approach compared to
the actual targeting method.
0.0
0.2
0.4
0.6
arpu [pPpD]
baseline
expire
accept
prediction
0
50
100
150
200
Growth [%]
Figure 3. Testing results of 100 rounds.
A relatively new insight in telecom marketing has been
gained, that the customer response rate of a digital marketing
campaign usually does not appropriately reﬂect the business
value generated by the campaign, and therefore is not an
appropriate marketing success measure. More speciﬁcally, cus-
tomer response measures the short-term campaign success, but
disregards long term effects on return on investment induced
by the campaign. Based on the data-driven marketing paradigm
of customer retention beats customer poaching, a new model
for measuring the marketing success of digital campaigns
for volatile and sparse prepaid telecom data was developed.
The metric is a function of the campaign-induced long-term
behavior change of customers in the targeted segment and
returns an assessment of the overall revenue increase or decline
n months after the offer has been accepted. It is applicable to
different intended behavior modiﬁcations, e.g., churn preven-
tion, usage increase per channel, and can handle low quality,
highly volatile and sparse input data.
VI.
CONCLUSIONS
The goal of this research is twofold. To develop a method-
ology and a software prototype for semi-automated predic-
tive analytics and campaign management, and as a proof of
concept, to verify the designed system on a chosen use case
provided by a large telecom company in Switzerland.
The implemented prototype shows that machine learning
can be used to support decision makers in the telecommu-
nication business to optimize marketing campaigns through
microtargeting. Moreover, it shows that Na¨ıve Bayes is suitable
model if full control need to be granted over the decision
support system. A nomogram based decision module has been
developed that optimizes the size of micro segments w.r.t. pre-
dicted long-term revenue increase caused by stimulus-induced
behavior change. The use of long-term revenue increase as a
campaign success measure is in contrast to commonly used
customer response success measures, which do not take long
term effects into account and often disregard the intended
effect on return on investment. Besides fully automated op-
timization over all input features, the decision module also
allows the marketer to manually set constraints to the opti-
mizer, e.g., to implement higher-order management strategies
or to explore the customer structure in terms of effects on
revenue in a simulation run. In order to permit the decision
model to adapt to changing customer structures, a closed-loop
49
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology

learning approach has been used that continuously updates
the underlying prediction model and corresponding nomogram
values. The solution has been prototypically implemented
based on an Apache big data stack and tested on two real-
world use cases with prepaid customer data sets provided by
the telecom project partner. The prediction accuracy of the
Na¨ıve Bayes approach has shown to exceed a decision trees
approach as well as the currently used benchmark with mostly
manual and experience-based campaign deﬁnition.
The detailed description of the prototype developed in
this research is omitted in order to honor the non-disclose
agreement with the involved industry partners.
ACKNOWLEDGMENT
The research presented in this paper was funded by the
Swiss Commission for Technology and Innovation CTI under
grand number 18714.1PFES-ES.
REFERENCES
[1]
CIM,
“The
cost
of
customer
acquisition
versus
customer
re-
tention.”
2010,
URL:
http://www.camfoundation.com/PDF/Cost-of-
customer-acquisition-vs-customer-retention.pdf [retrieved: April, 2016].
[2]
K. N. Lemon, T. B. White, and R. Winer, “Dynamic customer relation-
ship management: incorporating future considerations into the service
retention decision,” Journal of Marketing, vol. 66, 2002, pp. 1–14.
[3]
J. R. Quinlan, C4.5: Programs for Machine Learning.
San Francisco,
CA, USA: Morgan Kaufmann Publishers Inc., 1993.
[4]
M. Mehta, R. Agrawal, and J. Rissanen, “Sliq: A fast scalable classiﬁer
for data mining,” in Proceedings of the 5th International Conference on
Extending Database Technology: Advances in Database Technology,
ser. EDBT ’96, 1996, pp. 18–32.
[5]
J. C. Shafer, R. Agrawal, and M. Mehta, “Sprint: A scalable parallel
classiﬁer for data mining,” in Proc. 22nd Intl. Conf. on Very Large Data
Bases.
Morgan Kaufmann Publishers Inc., 1996, pp. 544–555.
[6]
J. Gehrke, R. Ramakrishnan, and V. Ganti, “Rainforest - a framework
for fast decision tree construction of large datasets,” in Proc. 24th Intl.
Conf. on Very Large Data Bases.
Morgan Kaufmann Publishers Inc.,
1998, pp. 416–427.
[7]
R. Rastogi and K. Shim, “Public: A decision tree classiﬁer that
integrates building and pruning,” in Proc. 24th Intl. Conf. on Very Large
Data Bases.
Morgan Kaufmann Publishers Inc., 1998, pp. 404–415.
[8]
J. Gehrke, V. Ganti, R. Ramakrishnan, and W.-Y. Loh, “Boat – opti-
mistic decision tree construction,” in Proc. 1999 ACM SIGMOD Intl.
Conf. on Management of Data.
ACM, 1999, pp. 169–180.
[9]
G. Kraljevi´c and S. Gotovac, “Modeling data mining applications
for prediction of prepaid churn in telecommunication services,” Au-
tomatika, vol. 51, no. 3, 2010, pp. 275–283.
[10]
T. Simsek Gursoy, “Customer churn analysis in telecommunication
sector,” Istanbul University Journal of The School of Business Admin-
istration, vol. 39, no. 1, 2010, pp. 35–49.
[11]
C.-P. Wei and I.-T. Chiu, “Turning telecommunications call details
to churn prediction: A data mining approach,” Expert Systems with
Applications, vol. 23, no. 2, 2002, pp. 103–112.
[12]
J. Hadden, A. Tiwari, R. Roy, and D. Ruta, “Churn Prediction: Does
Technology Matter?” Int. J. Intell. Technol., vol. 1, no. 2, 2006, pp.
104–110.
[13]
E. Shaaban, Y. Helmy, A. Khedr, and M. Nasr, “A proposed churn
prediction model,” J. Eng. Res. Appl., vol. 2, no. 4, 2012, pp. 694–697.
[14]
J. Hadden, A. Tiwari, R. Roy, and D. Ruta, “Computer Assisted
Customer Churn Management: State-of-the-art and Future Trends,”
Comput. Oper. Res., vol. 34, no. 10, 2007, pp. 2902–2917.
[15]
N. Kamalraj and A. Malathi, “A survey on churn prediction techniques
in communication sector,” International Journal of Computer Applica-
tions, vol. 64, 2013, pp. 39–42.
[16]
S. J. Russell and P. Norvig, Artiﬁcial Intelligence: A Modern Approach,
2nd ed.
Pearson Education, 2003.
[17]
S. V Nath, D. Ravi, and R. Behara, “Customer churn analysis
in the wireless industry: A data mining approach,” 2003, URL:
http://download.oracle.com/owsf 2003/40332.pdf
[retrieved:
April,
2016].
[18]
C. K. Kirui, L. Hong, W. K. Cheruiyot, and H. Kirui, “Predicting cus-
tomer churn in mobile telephony industry using probabilistic classiﬁers
in data mining,” in Int. J. Comput. Sci. Iss. (IJCSI), vol. 10, no. 2, 2013,
pp. 165–172.
[19]
A. Ultsch, “Data mining and knowledge discovery with emergent self-
organizing feature maps for multivariate time series,” in Kohonen Maps.
Elsevier, 1999, pp. 33–46.
[20]
W.-H. Au, K. C. Chan, and X. Yao, “A novel evolutionary data mining
algorithm with applications to churn prediction,” Trans. Evol. Comp,
vol. 7, no. 6, 2003, pp. 532–545.
[21]
Y. Xie and X. Li, “Churn prediction with linear discriminant boosting
algorithm,” in Proc. 7th Intl. Conf. on Machine Learning and Cyber-
netics, ICMLC, vol. 1, 2008, pp. 228–233.
[22]
H. Farquad, R. Vadlamani, and R. Bapi, “Churn Prediction using Com-
prehensible Support Vector Machine: an Analytical CRM Application,”
Applied Soft Computing, vol. 19, 2014, pp. 31–40.
[23]
A. Amin, S. Shehzad, C. Khan, I. Ali, and S. Anwar, “Churn prediction
in telecommunication industry using rough set approach,” Studies in
Computational Intelligence, vol. 572, 2015, pp. 83–95.
[24]
N. Kamalraj and A. Malathi, “A survey on churn prediction techniques
in communication sector,” in Int. J. Comput. App. (IJCA), vol. 64, no. 5,
2013, pp. 39–42.
[25]
B. Balle, B. Casas, A. Catarineu, R. Gavald`a, and D. Manzano-
Macho,
“The
architecture
of
a
churn
prediction
system
based
on
stream
mining,”
in
Frontiers
in
Artiﬁcial
Intelligence
and
Applications,
vol.
256,
2013,
pp.
157–166,
URL:
http://www.lsi.upc.edu/ gavalda/papers/ccia2013Churn.pdf [retrieved:
November, 2017].
[26]
C. M. Bishop, Ed., Pattern Recognition and Machine Learning.
Springer, 2006.
[27]
T. M. Mitchell, Machine Learning.
WCB McGraw-Hill, 1997.
[28]
M. Moˇzina, J. Demˇsar, M. Kattan, and B. Zupan, “Nomograms for
Visualization of Naive Bayesian Classiﬁer,” in Proc. PKDD 8th Eur.
Conf. Principles and Practice of Knowledge Discovery in Databases,
2004, pp. 337–348.
50
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-614-9
BUSTECH 2018 : The Eighth International Conference on Business Intelligence and Technology
Powered by TCPDF (www.tcpdf.org)

