Multi-Episodic Dependability Assessments for Large-Scale Networks 
 
Andrew P. Snow and Andrew Yachuan Chen 
Ohio University 
School of Information and Telecommunication Systems  
Athens, Ohio 
e-mail: asnow@ohio.edu 
 
Gary R. Weckman  
Ohio University 
Department of Industrial and Systems Engineering  
Athens, Ohio 
e-mail: weckmang@ohio.edu 
 
 
Abstract— As a network infrastructure expands in size, the 
number of concurrent outages can be expected to grow in 
frequency. The purpose of this research is to investigate 
through simulation the characteristics of concurrent network 
outages and how they impact network operators’ perspective of 
network dependability. The dependability investigated includes 
network 
reliability, 
availability, 
maintainability 
and 
survivability. To assess this phenomenon, a new event 
definition, called an “impact epoch”, is introduced.  Epochs are 
defined to be either single, concurrent, or overlapping outages 
in time, which can be best assessed with new metrics and 
simulation. These metrics, Mean-Time-To-Epoch, Mean-Time-
to Restore-Epoch along with percentage time the network is 
not in an epoch state (Quiescent Availability) and Peak 
Customers Impacted, are investigated. A case study based upon 
a variable size wireless network is studied to see what insights 
can be garnered through simulation. The new proposed 
metrics offer network operators valuable insights into the 
management of restoration resources. Simulation proved 
invaluable in identifying multi-outage epochs, as modeling 
their occurrence, frequency, duration and size is analytically 
intractable for large networks. 
Keywords-simulation, 
survivability, 
reliability, 
maintainability, wireless network infrastructure  
I. 
INTRODUCTION 
Telecommunication networks have become critical 
telecommunication infrastructure as millions of people 
depend on these networks for daily communication and 
commerce. As demand increases, so does network size, 
challenging engineers and operators to maintain and not 
compromise network dependability. As a network grows in 
size, the sheer number of components grows also, increasing 
failure hazard. With such an increase in hazard, the chance 
of concurrent, or overlapping, outages also can be expected 
to increase. Dealing with these concurrent outages is 
challenging because network operators have to judge 
priorities in allocating limited repair resources to outages 
spatially distributed. If the response is consistently 
substandard, the operator’s ability to satisfy current and 
accommodate new customers could be adversely affected. 
Understanding the characteristics of concurrent outages as a 
function of network size and component failure and repair 
rates offers network operators valuable information in 
developing outage recovery strategies. The number of 
customers that could be impacted by network failures is 
another important factor for network operators to consider. 
If the probability distribution of impacted customer is 
known, thresholds highlighting critical events can be 
established. This paper investigates the characteristics of 
simultaneous network outages and attempts to identify the 
distribution of impacted customers through simulation.  
A. Dependability 
Dependability has a number of different attributes. 
According to Laprie [9], the concept of dependability 
includes 
attributes 
like 
availability, 
reliability, 
maintainability, safety, confidentiality and integrity. Others 
have included survivability as an additional network 
dependability attribute, since it is so important to measure 
the resiliency of the network to provide partial service to the 
population of users during network service disruptions [8]. 
The higher the survivability, the better chance a service 
provider has to satisfy customers in times of network stress 
due to component failures or traffic overloads. Integrity and 
confidentiality are not considered in the scope of this study. 
Rather, 
we 
consider 
ARMS 
attributes 
(availability, 
reliability, 
maintainability, 
and 
survivability) 
of 
dependability.  
B. Reliability  
Network reliability is defined as the probability that a 
network will perform its required functions over a specific 
period of time [10]. The reliability, for a network or a 
network component is expressed as the probability that a 
network or component will not fail over some specified time 
period of interest, given by [11]: 
t MTTF
t
e
e
t
R
/
( )
−
−
=
=
λ
                            (1) 
Where λ is expected failure rate and MTTF is the expected 
average time between failures. If the time-period of interest 
is reasonably short, MTTF is assumed to be constant, 
meaning that an assumption of a Homogeneous Poisson 
Process (HPP) can be made.  
C. Maintainability 
Network maintainability is defined as the ability of a 
network to recover from failures [11]. Maintainability can 
be determined from the Mean Time to Restore (MTR). 
Restore time is a random variable and typically consists of 
three parts – detection time, travel time to the outage 
441
ICN 2011 : The Tenth International Conference on Networks
Copyright (c) IARIA, 2011              ISBN:978-1-61208-113-7

location and the actual repair or replacement time. In this 
research, the lognormal distribution is used since travel time 
plays an important role.  
D. Availability 
Network availability is defined as the probability that a 
network is ready for use when needed [11].  Average 
availability can be expressed as: 
MTR
MTTF
MTTF
A
+
=
                                          (2) 
Availability is a good metric to assess the state when the 
network is experiencing no problems due to failures.  
E. Survivability 
Network survivability is defined as the ability of a 
network to provide services to most customers under partial 
failures.  Snow [13] defined Prime Lost Line Hour (PLLH) 
as an impact measure for wire-line network outages that 
take into consideration usage levels at the time of the 
outage. PLLH is the product of the estimated number of 
customers impacted and the duration of an outage. Total 
Line Hours (TLH) is the product of the total number of 
customers served by the network and the total hours in the 
time-period of interest, resulting in a network survivability 
calculation in Equation (3). 
TLLH
PLLH
NS
= 1−
                                           (3) 
The Telecommunication Committee T1, an ANSI certified 
standards organization, developed the “outage index” as a 
survivability metric that includes consideration of the size 
and duration of the outage, in addition to the importance of 
the services affected by the outage. This metric uses weights 
for each of these three dimensions, and has been shown to 
be a questionable metric [15, 14, and 4]. 
II. 
IMPACT EPOCH 
The focus of this research is on concurrent and time-
overlapping component outages as the network size scales. 
In order to describe the characteristics of concurrent or 
overlapping outages from a network operator perspective, a 
new concept called impact epoch is introduced.  An impact 
epoch starts when a network transfers from the state of no 
customers impacted to a state of having customers 
impacted; and it continues until the network returns to the 
state of having no customers impacted.  An impact epoch 
event includes single or multiple outages that overlap in 
time. The number of impacted customers during one impact 
epoch is not necessarily constant, since a single impact 
epoch may include more than one component outage due to 
nearly simultaneous failures in the network. An example of 
a single impact epoch, which consists of three overlapping 
outages, is shown in the Figure 1 in the form of an epoch 
profile. Time is represented by the X-axis and the Y-axis 
represents the percentage of customers that can be served in 
the network. Prior published research has not considered an 
epoch perspective; hence, this new methodology is 
investigated in this paper. 
 
 
Fig. 1. Single impact epoch due to three overlapping outages 
 
Since epochs are arrival events, MTTE is defined as the 
mean time to impact epoch in a network. MTTE offers 
insights into the average interval before operators can 
expect disturbances that render the network incapable of 
satisfying all customers. Longer MTTE implies that the 
network has higher reliability, or the capacity and 
performance to lesson congestion events.  Since epochs 
have duration, MTRE specifies the mean impact epoch 
restore time - a description of a network’s maintenance 
response, or ability to gracefully recover from congestion. 
Shorter MTRE implies that the network has better 
maintainability or recoverability. MTRE together with 
MTTE provides the average quiescent time (AQ), or the 
fraction of time the network, on average, is not undergoing a 
disturbance that impacts customers. Quiescent availability 
can be determined by the following equation: 
MTRE
MTTE
MTTE
AQ
+
=
                                     (4) 
Survivability from an epoch perspective can still be 
measured by Equation 3. However, in an environment where 
there may be concurrent or overlapping outages, peak 
customers impacted (PCI) may be of interest. For instance, 
in Figure 1, the PCI is 20%. 
The advantages of studying impact epochs instead of a 
single outage are that epochs: 
• Provide a better-detailed description of the cumulative 
time phased effect of network disturbances 
• Offer a new way to evaluate network dependability, 
providing a different perspective important to network 
operators 
• Provide insights into how characteristics such as 
frequency, duration, number of concurrent outages, and 
peak customers impacted might change as network size 
varies 
Table 1 illustrates the mapping between wireless network 
dependability attributes and the metrics developed in this 
paper to assess them. In this wireless network example, a 
Wireless Traffic Profile (WTP) is developed using empirical 
wireless 
traffic 
data 
from 
the 
literature, 
allowing 
computation of PCI and WPLLH (Wireless prime lost line 
hours). In this study, outages are due to component failures. 
In other words, this is a fault management rather than a 
performance management perspective -- operators are 
442
ICN 2011 : The Tenth International Conference on Networks
Copyright (c) IARIA, 2011              ISBN:978-1-61208-113-7

responding to outage events induced by component failures, 
and the need to restore or replace the faulty components. 
Therefore, this work presents conservative estimates of 
episodic occurrences. 
 
TABLE 1 
New Network Dependability Metrics 
Dependability 
Network Attribute Name 
Reliability 
Network Mean Time To Epoch (MTTE) 
Maintainability 
Network Mean Time Restore Time (MTRE) 
Availability 
Network Quiescent Availability (AQ) 
Survivability 
Peak Customer Impacted (PCI) 
 
III. 
WIRELESS NETWORKS 
Extensive research has been conducted over many years 
regarding the traditional wire-line telephone network, also 
called the Public Switched Telephone Network (PSTN). 
These research efforts helped wire-line networks offer very 
dependable services with a common quality metric of Five 
9’s availability [3]. On the other hand, research in the world 
of wireless communication, especially in cell phone 
networks, is by comparison relatively new. Research into 
wireless telephone network reliability did not receive much 
attention until the late 1990s. Over the last 15 years, the 
wireless network has grown at an amazing rate. According 
to the Cellular Telecommunications Industry Association 
(CTIA) wireless Quick Fact Sheet [18], cellular subscribers 
in the US surpassed 5 million in 1990 and doubled in just 
two years. By 2000, cellular subscribers exceeded 100 
million in the US and wireless penetration rate was over 
65%. There were over 182 million customers in the US as of 
May 11, 2005.  
In 1992, the FCC at first ruled that wire-line carriers had 
to report all outages that impacted more than 50,000 
customers for at least 30 minutes. This threshold was 
quickly lowered to 30,000 customers for 30 minutes in 1993 
[12]. Statistical failure data of wire-line local switches are 
publicly available from the FCC’s Automatic Reporting and 
Management Information System (ARMIS) database. 
However, starting January 2, 2005, the FCC ruled that 
wireless carriers also had to report their network outages to 
the FCC [6]. Meanwhile, the FCC established a four-year 
rollout plan for E911 phase II, which began in October 
2001. Phase II required wireless carriers to provide precise 
location information for wireless 911 calls, within 50 to 300 
meters in most cases [7].  
A. Wireless Network Infrastructure 
The general structure of a wireless network with most of the 
required functional components is shown in Figure 2 [5]. 
They include the network operation subsystem, base station 
subsystem and network switching subsystem. Each 
subsystem includes a number of components that are studied 
in this research. This is a 2G+ architecture that has some 
similarity to 3G/4G architectures from hierarchical and 
topological perspectives.  The Base Station Subsystem 
(BSS) is comprised of Base Stations (BS) and Base Station 
Controllers (BSC). A BS is essentially the radio station that 
broadcasts to and receives from the mobile station in a 
“cell”. A BSC is the controlling node for one or more cells 
or BSs and manages voice or data traffic and signaling 
messages for all the cells under its control. The BSS 
provides the transmission path including traffic and 
signaling between mobiles and the Network Service 
Subsystem (NSS) [5]. 
 
MSC
VLR
HLR
EIR
AuC
NSS
BSC
BSC
BS
BS
BS
BS
BS
BSS
MSC
VLR
BSC
BS
BS
BS
BSS
Anchor
Switch
PSTN
 
Fig. 2.  Wireless network infrastructure 
 
The NSS is the switching and control portion of the 
entire wireless network. It is comprised of the Mobile 
Switching Center (MSC) and three intelligent network 
nodes known as the Home Location Register (HLR), Visitor 
Location Register (VLR), Equipment Identity Register 
(EIR), and the Authentication Center (AuC) [5]. The MSC is 
the central heart of a wireless network. The failure of a 
MSC typically results in communication loss of all users 
that the MSC controls, since calls cannot be originated or 
terminated. Carriers pay close attention to the status of a 
MSC since it supports billing functions such as collecting 
Call Detail Records (CDR).  A typical MSC is engineered to 
be highly reliable. In A. Snow, [16], the authors introduced a 
wireless 
network 
infrastructure 
called 
the 
Wireless 
Infrastructure Block (WIB). The scope of the WIB is from 
the BS to the MSC including the HLR/VLR database. They 
also discussed how MTTF and MTR in a WIB might affect 
the network’s dependability [16]. The topology used in a 
WIB is the star topology. Large wireless infrastructures 
consist of multiple WIBs. 
B. Wireless Traffic 
Advantages of using the star topology include supporting 
modular expansion, and simplified monitoring and trouble-
shooting. The largest disadvantage of star topology is the 
443
ICN 2011 : The Tenth International Conference on Networks
Copyright (c) IARIA, 2011              ISBN:978-1-61208-113-7

creation of single point of failure, such as the MSC and 
database. Fortunately, these components are highly reliable. 
Table 2 indicates the number of components in a WIB along 
with the number of customers potentially impacted by each 
component. A WIB can serve up to 100,000 customers. How 
many subscribers are actually impacted depends on 
utilization, which can be related historically to time of day 
and day of week. This can be represented by a time factor, 
which is really a time phased traffic profile that reflects 
percentage utilization at a point in time [17]. According to 
historical statistics [13], heavy traffic load in wire-line 
networks occur between 9:00am and 4:00pm on weekdays.  
 
TABLE 2 
Number of Components in One WIB and Maximum Failure Impact 
Component 
Number in One WIB 
No. Customers 
Potentially Impacted
MSC 
1 
100,000 
VLR/HLR DB  
1 
100,000 
MSC-BSC link 
5 
20,000 
BSC 
5 
20,000 
BSC-BS link 
50 
2,000 
BS 
50 
2000 
Anchor-MSC Link 
N 
100,000 
Anchor Switch 
N 
N * 100,000 
Anchor Link 
N 
N * 100,000 
Note: N is the number of WIBs in the wireless infrastructure 
In this work, a new traffic profile for wireless networks is 
developed. The reason is that traffic patterns in wireless 
networks are different from that in PSTN. For instance, 
service charges in the PSTN are usually a flat monthly 
charge, while in a wireless networks there are more usage 
plans with differential charges based on time of day a call is 
placed. For example, many cell phone plans offer free calls 
at weekends and after 9:00pm on weekdays. Some people 
could wait until 9:00pm to place calls and take the 
advantage of this plan. Such phenomena results in different 
weekday and weekend traffic profiles in wireless networks. 
In Albaghdadi and Razvi [1], the authors studied an actual 
1320 cell GSM network. In this research, the results 
reported in this GSM network were used to develop five-day 
weekday traffic and weekend traffic profiles as shown in 
Figures 3. These profiles were developed to create a 
wireless PLLH outage impact metric, called hereafter the 
WPLLH. 
 
 
Fig. 3. Wireless Weekday Time Factor 
 
Because the interaction of reliability and maintainability 
attributes are expected to be complex when it comes to 
investigating multi-episodic events, three different scenarios 
are 
investigated 
as 
follows: 
 
nominal, 
degraded 
maintainability, and enhanced reliability and maintainability. 
The nominal scenario signifies that the network is operating 
within published reliability and maintainability norms where 
regular maintenance schemes are used and reliability is 
stable. The degraded maintainability implies that the 
maintainability of the network is not as good as nominal, 
which signifies higher restore times from component 
failures. The enhanced reliability/maintainability scenario 
indicates that component reliability and maintainability are 
improved over nominal (with higher MTTFs and lower 
MTRs).  
C. Network Component MTTF and MTR 
Transmission links can be deployed with protection 
channels, wherein if the primary link is disrupted, the 
system switches to a protection channel. The more 
customers affected, the more likely there is a protection 
channel. Table 3 details a complete list of component 
MTTFs used in this study.  
 
TABLE 3 
Component MTTF and MTRs Used In the Study 
Components
Name 
Nominal 
MTTF 
(Yrs) 
Enhanced 
MTTF 
(Yrs) 
Degraded
MTR 
(Hours) 
Nominal
MTR 
(Hours)
Enhanced 
MTR (Hours)
Anchor 
Link 
8.0 
8.0 
12.0 
4.00 
2.00 
MSC-
Anchor 
Link 
8.0 
8.0 
12.0 
4.00 
2.00 
MSC-BSC 
Link 
2.7 
4.0 
12.0 
6.00 
3.00 
BSC-BS 
Link 
1.7 
2.7 
12.0 
6.00 
3.00 
MSC and 
anchor 
switch 
7.5 
7.5 
0.51 
0.17 
0.12 
VLR/HLR 
database 
3.0 
4.5 
2.00 
1.00 
0.50 
BSC 
3.0 
6.0 
4.00 
2.00 
1.00 
BS 
2.0 
4.0 
4.00 
2.00 
1.00 
 
The nominal MTTF for other components was taken from 
[16]. As the MSC has become a very stable control and 
switch system over many years’ development and 
deployment, in this case, the nominal MTTF and enhanced 
MTTF of MSC are taken to be the same, which is 7.5 years 
based on the results derived from empirical local switch 
statistics in the Federal Communication Commission’s 
ARMIS database. 
A component’s maintainability is represented by its 
MTR. In order to understand the role that MTR plays in 
dependability, three MTR scenarios are used in the 
simulation: nominal, degraded and enhanced.  Nominal 
MTR was obtained from [16]. The degraded MTR was taken 
as three times the nominal MTRs except for switches. Table 
444
ICN 2011 : The Tenth International Conference on Networks
Copyright (c) IARIA, 2011              ISBN:978-1-61208-113-7

3 also lists the component MTRs used. The repair 
distributions are modeled based on a lognormal distribution, 
which is commonly used for long tailed distributions when 
travel time is involved. To summarize: 
• The nominal case uses reliability and maintainability 
levels from literature and empirical data 
• The enhanced case uses improved reliability and 
maintenance levels 
• The degraded case uses lower maintainability levels 
D. Simulation Model 
Inputs for the simulation include all component MTRs and 
MTTFs, wireless traffic profile, the network size and an 
operational time of one year.  Outputs from the program are 
network survivability, detailed outage information including 
start time, stop time, the number of customers impacted and 
the WPLLH for each outage. Other results like MTTE, 
MTRE, PCI and quiescent availability are derived from 
these simulation outputs using MS ExcelTM. Figure 4 
displays the input and output process of the simulation and 
the derived results. 
 
 
Fig. 4. Process of simulation and results 
 
The process was conducted separately for different size 
networks (size determined by the number of WIBs) based 
on three scenarios: nominal, degraded maintainability, and 
enhanced reliability and maintainability. The maximum 
deviation in the nominal scenario between the simulation 
output and the analytical result was 0.85% for 8 WIB’s, 
which was acceptable. This verified the simulation. Direct 
simulation program outputs include outage numbers, start 
time, end time, impacted customers, WPLLH and duration 
of each component outage.  An example of a simulation 
output is revealed in Table 4, showing four component 
outages, starting at 308.465 days into the year.  Figure 5 
illustrates the impact epoch over the simulation time. The 
Quiescent Time can be derived from direct outputs of the 
simulation program and is calculated as: 
∑
∑
=
=
−
=
=
n
i
i
n
i
i
t
TRE
TotalSimulationTime
TTE
Q
1
1
        (5) 
where n is the number of quiescent periods.  The sum of all 
TTEs and all TREs should equal the total simulation time, 
as shown in Figure 5.   
 
TABLE 4 
Simulation Output Example for A 10 WIB Network 
Failure Start Time 
(Days into Year) 
Failed 
Component 
WIB Number 
Duration 
(Hours) 
308.465 
Base Station 32 
6 
6.55 
308.694 
Base Station 15 
5 
1.50 
308.698 
Base Station 5 
4 
2.90 
309.292 
BSC-BC-Link 41 
10 
6.52 
 
 
 
Fig. 5. Relationship of TTE, TRE and simulation time 
 
Likewise, we expect the MTTE (mean of all times to epochs 
TTE), MTR (mean of all times to restore epochs TRE) and 
total simulation time to be: 
n
TTE
MTTE
n
i
i
∑
=
=
1
                                              (6) 
n
TRE
MTRE
n
i
i
∑
=
=
1
                                                     (7) 
n
MTRE
MTTE
Time
Simulaiton
Total
•
+
=
)
(
_
_
           (8) 
IV. 
RESULTS 
As expected, the number of impact epochs increases as 
the network expands in all three scenarios since newly 
added WIBs in a wireless infrastructure will contribute more 
component outages. Figure 6 illustrates the relationship 
between the total numbers of impact epochs at different 
network size for each scenario over a one-year interval.  
Remember, this also includes single outage epochs. The 
nominal and degraded scenarios both use nominal MTTF, 
therefore the expected number of single component failures 
in these two scenarios should be at the same level when the 
network size is small, such as 1 or 2 WIBs, since the 
number of impact epochs is approximately the same. As the 
network size increases, the nominal scenario has more 
impact epochs as compared to the degraded maintenance 
scenario since longer repair times mean fewer components 
online at any instant that can fail. As it turns out, the 
degraded case has less epochs, but more multi-outage 
epochs. Remember – a one WIB network serves 100,000 
customers while a ten WIB network serves 1,000,000. 
 
445
ICN 2011 : The Tenth International Conference on Networks
Copyright (c) IARIA, 2011              ISBN:978-1-61208-113-7

 
Fig. 6. Total number of impact epoch 
 
Figure 7 displays the actual number of multi-outage epochs 
for each network size scenario. The curve increases almost 
linearly for networks in the degraded and nominal scenarios 
after network size exceeds 2 WIBs. The rate of growth 
slows down significantly in the enhanced scenario. Table 5 
indicates that nearly 40% of the total impact epochs are 
multi-outage epochs in a 10 WIB network with the degraded 
scenario. This situation improves in the enhanced scenario, 
where less than 8% of total impact epochs include more 
than one outage. 
 
 
Fig. 7. Multi-outage epoch number 
 
TABLE 5 
 Multi-Outage Impact Epoch Composition 
# WIB 
2 or more concurrent outages 
3 or more concurrent outages 
Degraded Nominal Enhanced Degraded Nominal Enhanced
2 
9.8% 
4.6% 
1.6% 
1.1% 
0.3% 
0 
4 
20.1% 
9.5% 
3.3% 
4.6% 
1% 
0 
8 
33.5% 
18.3% 
6.3% 
12.7% 
4.2% 
0 
10 
39.5% 
23.2% 
7.7% 
17.9% 
6.5% 
<0.9% 
 
The difference between degraded and enhanced scenario is 
significant. The percentage of network epochs in the 
degraded scenario increases from 4.6% to 17.9% as it 
expands from 1 to 10 WIBs. The range is from 0.3% to 
6.5% for networks in nominal scenario. While in an 
enhanced scenario network, the 3 or more outage epoch 
virtually disappears. Notable differences occur among three 
scenarios involving the multi-outage epochs. In the 
enhanced scenario, impact epochs consisting of more than 2 
concurrent outages rarely happen, even when a network 
expands to serve 1 million customers. However, in the 
degraded scenario, when the network has 6 WIBs, the 
composition of impact epochs consisting of more than 2 
concurrent outages is 7%. When the network has 10 WIBs, 
the number is 18%. Concurrent outages become a huge 
challenge for network operators in the degraded scenario, 
especially when network size grows. 
The results of the network quiescent days for each 
scenario are shown in Figure 8.  As the network expands, its 
quiescent availability decreases, almost linearly. In the 
degraded scenario, the total non-episodic time of a one WIB 
network is 345 days over a one-year operation time. By 
contrast, for a 10 WIB network, the number is only 213 
days, which demonstrates that the network is in an episodic 
state 42% of the time.  In the nominal scenario, which has 
the same reliability as the degraded scenario, the total non-
episodic time of a 1-WIB network is 355 days, and 272 days 
for a 10-WIB network. This implies that 25% of the time the 
nominal network is in an episodic state for a 10 WIB 
network, which is approximately 30% improvement over 
the degraded scenario. 
 
 
Fig. 8. Percentage of quiescent availability 
 
The nominal and degraded scenarios use the same 
component reliability or MTTF. The difference is the 
component 
maintainability. 
Meanwhile, 
the 
nominal 
scenario is different from the enhanced scenario for both the 
component reliability and the maintainability. Figure 8, 
demonstrates that the nominal curve lies between the 
enhanced and degraded curves.  Thus, the component 
maintainability rather than reliability is more decisive to the 
network quiescent availability. Efficient management of 
maintenance resources seems to have a positive impact on 
sustaining a network and avoiding an episodic status. 
Figure 8 shows the quiescent availability of a network in 
different scenarios.  There are four important attributes of an 
impact epoch: MTTE, MTRE, PCI, and WPLLH. MTTE is 
the average time between two impact epochs, which is used 
to model the network’s reliability. MTRE is the average 
time to repair an impact outage in the network, which is a 
measure of the network’s maintainability. PCI and PLLH are 
used to model the wireless network’s survivability. 
A. Mean Time To Epoch and Mean Time to Restore Epoch 
Results demonstrate that MTTE decreases nonlinearly, as 
expected, as the network size increases for each scenario. In 
446
ICN 2011 : The Tenth International Conference on Networks
Copyright (c) IARIA, 2011              ISBN:978-1-61208-113-7

all three scenarios, MTTE decreases quickly as the network 
grows from 1 to 3 WIBs, and the rate of decrease slows after 
3 WIBs. The MTTE in degraded and nominal scenarios are 
very similar, as they have the same reliability. This is 
because single component outages are still dominant when 
the network is less than 3 WIBs. After that, as the network 
size increases, the overlapping phenomenon begins to play 
an important role in determining the total number of impact 
epochs. 
MTRE is expected to increase as outage overlapping 
occurs. How much overlapping affects MTRE depends upon 
the pattern of the overlapping. There are several different 
overlapping patterns that could occur, shown in Figure 9 A, 
B, C, D. Among these four patterns shown in Figure 9, 
pattern “A” does not increase TRE since repair time of the 
second outage totally occurred within the repair time of the 
first outage (TRE in pattern “A” equals to the MTR of 
component one). Pattern “B” has a small degree of overlap 
and effect on TRE while pattern “C” has a moderate impact 
on TRE.  Pattern “D” overlap is nearly sequential, having 
the largest impact on TRE.  All these types of overlapping 
patterns may impact MTRE. Figure 10 illustrates the 
simulation output of the MTRE changes due to network 
size. 
 
Fig. 9. Different Overlapping Patterns 
 
 
 
Fig. 10. MTRE in hours 
 
As expected, MTRE in the degraded maintainability 
scenario increased nonlinearly as the network expanded due 
to overlapping outages. As the network grows, more 
overlapping instances occurred and the chance of 
overlapping pattern “A” increased, thereby decreasing 
MTRE. The component maintainability in the degraded 
scenario is lower than that in nominal and enhanced 
scenarios. The MTRE of a 10 WIB network in the degraded 
scenario increased by approximately 28% (about 144 
minutes) from the single WIB network, while a 10 WIB 
network in the enhanced scenario increased by only 5.4 
minutes longer than the one WIB network. 
B. Peak Customers Impacted 
A question that a network operator may ask is “what is the 
chance an impact epoch affecting more than 10,000 
customers will occur in the next 30 days?” Understanding 
the distribution of peak customers impacted can provide 
insights into such questions.  The PCI for each simulation 
run was collected and the data was fitted to an Exponential 
Distribution [2] with a high degree of significance (p value 
= 0.0000).  This allowed easy calculation of probabilities of 
peak outages. Table 6 displays the probability of a PCI 
greater than or equal to 10,000 customers in 30 days for 
different scenarios and network sizes, along with the same 
results for a PCI greater than or equal to 5,000 customers. 
Larger networks have higher probabilities due to the 
additive nature of outages in epochs. 
 
Table 6 
Probability of PCI over 10,000 and over 5,000 customers in 30 days 
Scenario 
Name 
Number of WIB 
(over 10,000) 
Number of WIB 
(over 5,000) 
 
2 
4 
8 
10 
2 
4 
8 
10 
Degraded  
3.3% 
4.8% 
10.3% 
11.1% 
18.2% 
21.9% 
32.1% 
33.3% 
Nominal  
1.0% 
1.1% 
2.5% 
2.7% 
10.0% 
10.6% 
15.7% 
16.4% 
Enhanced  
1.0% 
1.1% 
2.3% 
2.4% 
10.0% 
10.7% 
15.1% 
15.6% 
 
Similarly, the distribution of WPLLH values for 
networks of different sizes and scenarios are illustrated in 
Table 7. These results can predict the probability of PLLH 
over a threshold for a given time-period.  
 
Table 7 
WPLLH mean 
Scenario 
Name 
Number of WIB 
2 
4 
8 
10 
Degraded 
13,867 
18,367 
25,094 
25,367 
Nominal 
6,409 
6,640 
8,088 
8,257 
Enhanced 
3,550 
3,735 
4,042 
4,506 
 
The chance of the PCI and the PLLH over a certain 
threshold is much higher in the degraded scenario than that 
in the nominal and enhanced scenarios. For example, the 
chance of an epoch in which the PCI is over 10,000 
customers over 30 days in the degraded network is three to 
five times than that of the enhanced scenarios.  Thresholds 
are useful for network operators in effectively monitoring 
networks, given that they filter out lower priority epochs. In 
this paper, three different WPLLH threshold levels are used 
as filters: 5K WPLLH, 10K WPLLH and 15K WPLLH.  A 
5K WPLLH denotes that the product of impacted customers 
and impacted duration in an epoch is 5,000.  For example, it 
447
ICN 2011 : The Tenth International Conference on Networks
Copyright (c) IARIA, 2011              ISBN:978-1-61208-113-7

could mean 5,000 customers are impacted for one hour or it 
could signify that 10,000 customers are impacted for half an 
hour. Figure 11 indicates the relationship between the 
numbers of impact epoch versus different thresholds, for the 
degraded scenario.  
 
Fig. 11. Number of impact epochs with filters in degraded scenario 
 
The growth rate of impact epochs over 5K WPLLH in all 
three scenarios increased rapidly as the network expands in 
size. At the size of 10 WIB, in the enhanced scenario, the 
number of impact epochs over 5K WPLLH is 52, while in 
the degraded scenario, the number is 223 (4 times more than 
enhanced scenario 
This implies that in any scenario where a network 
expands, the number of impact epochs over a lower 
threshold can be expected to grow quickly. A network in the 
degraded scenario has to deal with a large number of epochs 
over higher thresholds because they grow in number at a 
much faster rate than that in the enhanced scenario. These 
insights should aid in network operators’ ability to set 
efficient thresholds. Set too low, a threshold masks 
important outages; set too high, too many less significant 
outages are seen. 
V. 
CONCLUSION 
This work indicates that in large networks, the epoch 
perspective is useful in understanding the complex nature of 
ongoing concurrent failures. With these new metrics, 
operators can calculate such things as the probability of 3-
outage epoch over a time-period and the probability of an 
epoch exceeding a specified peak over a time-period. Such 
information is useful to operators in allocating resources.  
Significant contributions of this work include: 
• Defined the impact epoch as a new way to evaluate 
wireless network infrastructure’s dependability. 
• Developed new metrics for analyzing ARMS for large 
networks (MTTE, MTRE, Quiescent Availability, PCI 
and WPLLH). 
• Development of empirically derived wireless traffic 
profiles to determine number of customers impacted by 
component failures by time of day and day of week. 
Important conclusions include: 
• An impact epoch perspective gives key insights into 
network dependability. Lacking empirical outage data, 
these perspectives are best investigated with simulation. 
• Component maintainability has a large effect on a 
network’s quiescent availability. Effective monitoring 
and efficient management of repair resources can 
shorten the time when a network is in an episodic state. 
• The no. of small network impact epochs is not critical.  
With respect to the last point, network operators should be 
very careful when expanding their infrastructure in order to 
accommodate more customers. Results here indicate that the 
number of concurrent outage epochs is sensitive to 
component reliability or maintainability. Reliability and 
maintainability should not be degraded in the expanded 
network. Additionally, it may be necessary to increase 
reliability and/or maintainability in order to keep multi-
outage epochs to a minimum. 
REFERENCES 
[1] Albaghdadi M, and Razvi K (2005). Efficient transmission of periodic data that 
follows a consistent daily pattern. 9th IFIP/IEEE International Symposium on 
Integrated Network Management. IEEE Operations Center. Piscataway NY. pp 
511-526. 
[2]  BestFit 4.5, Palisade Corporation. 
[3] Cankaya H, Lardies A, and Ester G. (2004) Availability-aware analysis and 
evaluation of mesh and ring architectures for long-haul networks. Applied 
Telecommunications Symposium. pp. 116 – 121. 
[4] Carver Carol Y, and Snow Andrew P (1999) Assessing the Impact of a Large-
Scale Telecommunications Outage. Proceedings of the 7th International 
Conference on Telecommunication Systems. March: 483-489. 
[5] Dryburgh L, and Hewett J (2004) Signaling System No.7: Protocol, Architecture 
and Services. ISBN 1-58705-040-4 
[6] FCC Report (2005a) Report and Order and Future Notice of Proposed Rule 
Making, retrieve from http://www.fcc.org in August. 
[7]  FCC report, (2005b) http://www.fcc.gov/911/enhanced/, retrieved in August. 
[8] Knight J, Strunk E, and Sullivan K. (2003) Towards a Rigorous Definition of 
Information System Availability. Proceedings of the DARPA Information 
Survivability Conference and Exposition. IEEE. 
[9] Laprie J C (1995) Dependability – Its Attributes, Impairments and Means in 
Predictability Dependable Computing Systems. 
[10] Leemis L (1995) Reliability Probabilistic Models and Statistical Methods. 
Prentice Hall, Englewood Cliffs, NJ.  
[11]  Lewis E E (1987) Introduction to Reliability Engineering. ISBN 0-471-81199-8. 
[12] Malloy A D (2002) Design and Performance Evaluation of QoS-Oriented 
Wireless Networks. Georgia State University, Doctorial Dissertation. 
[13] Snow A (1998) A Survivability Metric for Telecommunications: Insights and 
Shortcomings. 1998 Information Survivability Workshop – ISW’98 IEEE 
Computer Society, FL, October. pp.135-138. 
[14] Snow Andrew P (2003) The Failure of a Regulatory Threshold and a Carrier 
Standard in Recognizing Significant Communication Loss. TPRC 2003. 
November. 
[15] Snow A P (2004) Assessing pain below a regulatory outage reporting threshold. 
Telecommunications Policy, Vol. 28/7-8. pp 523-536. 
[16] Snow A, Varshney U, and A Malloy (2000) Reliability and Survivability of 
Wireless and Mobile Networks. IEEE Computer Magazine. July. pp. 49-55. 
[17] T1A1.2 Working Group (1997) A Technical Report on Network Survivability 
Performance 
[18] WCB, (2005) The Wireless Industry and Its Contribution – A Presentation to 
Wire-line Competition Bureau. 
 
 
448
ICN 2011 : The Tenth International Conference on Networks
Copyright (c) IARIA, 2011              ISBN:978-1-61208-113-7

