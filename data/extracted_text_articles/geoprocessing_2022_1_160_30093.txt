Use of UAV-Based RGB Imagery and Vegetation Index for Early Detection of the 
Rabies of Chickpeas 
 
Lorena Parra1, 2, Barbara Stefanutti, David Mostaza-Colado1, Jose F. Marin3, Jaime Lloret 2, and Pedro V. Mauri1  
1Instituto Madrileño de Investigación y Desarrollo Rural, Agrario y Alimentario (IMIDRA), Finca “El Encin”, A-2, Km 38, 
2, 28805 Alcalá de Henares, Madrid, Spain 
2 Instituto de Investigación para la Gestión Integrada de Zonas Costeras Universitat Politècnica de València, Valencia, Spain 
3 Area verde MG Projects SL. C/ Oña, 43 28933 Madrid, Spain 
Email: loparbo@doctor.upv.es, barbara.stefanutti@madrid.org, david.mostaza@madrid.org, jmarin@areaverde.es, 
jlloret@dcom.upv.es, and pedro.mauri@madrid.org 
 
 
Abstract—Rainfed crops rarely include the application of 
phytosanitary products due to the high cost of their application 
and the reduced rentability of crops. Nonetheless, if diseases are 
early detected, phytosanitary application costs are heavily 
reduced. This paper presents a method of detecting rabies in 
chickpeas based on true-colour images gathered from drones. 
The methodology consists of applying a series of vegetation 
indexes and filters. In the proposed method, applied to several 
images, we include the detection of areas affected by rabies of 
chickpea but also their differentiation from other areas with 
lower vigour. The developed approach is tested with images 
obtained in different soil types and gathered at diverse flying 
heights. As vegetation indexes, we used well-known vegetation 
indexes and specific vegetation indexes developed for chickpeas. 
To evaluate the accuracy of the proposed methodology, the 
number and percentage of true positives and false positives are 
assessed. Moreover, a verification is done using a different 
picture in order to evaluate if the methodology might be applied 
in other scenarios. The results of the initial test and the 
verification test offer a number of true positives higher than 
85%. Thus, we can affirm that the proposed methodology can 
be useful for the differentiation between areas affected by rabies 
of chickpea and areas with low vigour due to the passing of 
machinery. 
Keywords-plant disease; aggregation; image processing; 
legume; biotic stress; crop 
I. 
 INTRODUCTION  
Chickpea is a legume crop with a high percentage of 
protein, which can help to reduce the dependence on meat. 
Spain is the main producer of chickpea in Europe. 
Nonetheless, the cultivation of chickpeas has declined in 
recent decades. As indicated by the Lonja de Sevilla, the main 
problems are the following: (i) Lack of support via Common 
Agrarian Policy, (ii) Little Research (little use of certified 
seed), (iii) Technical difficulties in crop management (high 
presence of weeds, diseases, such as rabies and harvesting 
problems) and (iv) Obstacles in marketing. The technical 
difficulties of crop management are the most important for 
farmers and those that can be solved by means of new 
technologies. Of these difficulties, the detection of weeds [1] 
and plants affected by diseases [2] are vital in chickpea 
cultivation and other crops. Among the chickpea diseases, 
rabies of the chickpea is one of the most problematic ones, 
causing a decrease in production and even the death of the 
plant. 
The use of a remote sensing approach is pointed out as one 
of the best methodologies for evaluating plant features. In 
other crops, the use of remote sensing for detecting plant 
diseases is common. According to [3], the most studied 
diseases are fungal diseases. They are generally studied in 
cereals such as wheat, rice and maise, followed by soya. No 
study evaluated the use of hyperspectral images in chickpeas 
or other rainfed legumes. In [4], a large collection of 
Vegetation Indexes (VI) developed by different authors for 
diverse crops is presented. The authors indicate that VI are 
highly correlated at plot level with the presence of plant 
diseases. Focusing on VI, RGB images are reported as 
adequate for cotton, sugar beet, grapefruit, and tobacco [5]. 
Although hyperspectral images provide more information 
than RGB images, the elevated cost of hyperspectral cameras 
precludes its use for real solutions.  
The main problem of applying VI to identify the areas to 
be treated is that we can find other zones characterised by low 
vigour in the fields, which can be confused with zones 
affected by rabies. These zones are usually the areas that have 
been stepped on by a tractor when entering to apply 
treatments. If the areas are not correctly differentiated, 
phytosanitary products will be wasted, increasing the 
treatment's cost and efficiency.  
The aim of this paper is to evaluate the use of RGB images 
taken by Unmanned Aerial Vehicle (UAV) to identify areas 
affected by rabies in different fields. The images have been 
taken at different heights and included affected and unaffected 
areas. The entire process includes the application of the VI 
(testing up to 5 different VI) and the use of different tools for 
image processing, such as aggregation or reclassification 
tools. Simple tools are selected to avoid the use of artificial 
inteligence or complex tools. This, ensures that the 
methodology can be applied in the fields in real time. Thus, 
the Unmanned Aerial Vehicle (UAV) will be able to identify 
the areas to be treated and apply the treatment at the same 
time.  
The rest of the paper is structured as follows; Section II 
outline the related work and the gap in the current solutions. 
The proposal is described in Section III. Section IV details the 
material and methods used in this research. The results and 
68
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-983-6
GEOProcessing 2022 : The Fourteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

main discussion are presented in Section V. Finally, the 
conclusion and future work are summarised in Section VI.  
II. 
RELATED WORK 
This section summarises the existing proposals for 
identifying diseases or abiotic stress in plants using RGB 
images and the existing VI developed for chickpeas.  
Regarding the use of RGB images to identify plant 
diseases, we can find a limited number of examples. More 
examples of using the RGB data to evaluate abiotic stress.  
In 2019, Marc Sancho-Adamson et al. [6], showed the use 
of 
RGB-based 
VI 
for 
evaluating 
the 
effects 
of 
VerticilliumWilt of olive. The authors used the following VI: 
Green Area (GA), Greener Area (GGA), normalised green-
red difference index and triangular greenness index. Their 
results indicate that GA was the one with the strongest 
correlations between the VI and chlorophyll and carotenoid 
extractions to identify the diseases. Nonetheless, the VI was 
applied in pictures done in the laboratory, not in the field.  
Another example of the application of RGB indexes to 
identify plat diseases was published in 2021 by Arturo Yee-
Rendon [7]. In their paper, the authors pointed out the 
possibility of using RGB-based VI for detecting tobacco 
mosaic virus and pepper huasteco yellow vein Virus in 
jalapeño pepper plants. New VI was proposed and evaluated. 
The authors combine the VI with a convolutional neural 
network to identify the affected leaves in their proposal. Their 
results indicate that the VI with better accuracy was the 
normalised green–blue vegetation index, which combines the 
green and the blue bands.  
In 2021, Brenon Diennevam Souza Barbosa et al. [8], 
assess the use of RGB images for monitoring a coffee farm. A 
total of 9 VI were used and correlated with the obtained Leaf 
Area Index (LAI). Results indicate that the index that best 
correlates with the LAI varies along the different phenological 
stages of the crop. Salima Yousfi et al. in 2022 [9] present the 
use of RGB-based VI for evaluating the hydric stress in 
turfgrasses. GA and GGA were used to assess the stress of the 
plants. Their results indicate that RGB-based VI obtained 
from aerial images explains the hydric stress of the plants 
better than terrestrial indexes, even better than the Normalised 
Difference Vegetation Index (NDVI). The NDVI is a well-
known VI that uses not only the RGB data but also the 
information of infrared light.  
Finally, a few examples of VI applied for chickpeas 
monitoring can be found. Particularly, in the case of RGB 
indexes, only two papers have been published. In the first 
example, the VI was used to differentiate soil from vegetation 
in order to assess the degree of establishment of different 
legumes, including lentils and chickpea [10]. In the second 
example, a tailored VI including a series of thresholds is used 
to differentiate species of legumes in intercropping. The 
included species were chickpea, lentil and ervil. The results of 
[11] pointed out that chickpea was the species that was easier 
to differentiate.  
As far as we know, no paper has shown the use of RGB-
based VI to identify chickpea's rabies nor to differentiate 
regions affected by the disease and the pass of machinery. 
Moreover, no example of RGB-based VI applied in the field 
is found. Thus, the methodology presented in this paper can 
be considered an advance over state of the art.  
III. 
PROPOSAL 
In this section, we outline the proposal developed in the 
paper. We will describe why the proposed methodology is the 
most suitable approach for the problem. 
The problem indicated in the introduction is differentiating 
areas affected by rabies of chickpea from areas with lower 
vigour due to the passing of machinery. Our proposal is to 
develop a methodology that allows an UAV to apply it in real-
time to smartly decide the areas to be treated with the 
phytosanitary product. The aim is to minimise the application 
of phytosanitary products treating only the area affected by the 
disease and its surroundings. Thus, it is essential to 
differentiate the areas in which crop has low vigour due to the 
rabies of the chickpea from the areas with low vigour due to 
the machinery, which does not need any treatment. 
Figure 1 shows an example of a picture in which, 
manually, we have differentiated the areas with low vigour 
and their cause. The proposed approach is depicted in Figure 
2, in which we represent the whole process to minimise the 
use of phytosanitary products. Applying the phytosanitary 
product to the affected area it is possible to reduce the cost and 
environmental impact of the treatment without minimizing its 
effectivity. Traditionally, the phytosanitary product is applied 
to the whole plot. 
 
 
Figure 1.  
Example of areas with low vigour. 
 
Figure 2.  
Representation of proposal. 
69
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-983-6
GEOProcessing 2022 : The Fourteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

IV. 
MATERIAL AND METHODS 
A. Studied area 
The study area is located in the autonomous community of 
Castilla la Mancha (Spain), in the municipality of Trijueque 
(40º46'45''N, 2º59'2''W) at an approximate altitude of 1,000 
m.a.s.l. The location of the studied area in Spain and the 
detailed image of the studied fields can be seen in Figure 3 a) 
and Figure 3 b). The studied area is composed of two 
production fields in which chickpea is grown. Different crop 
densities characterise the plots due to different soils. Plots are 
located on a plain close to a highway.  
Pictures were gathered on 25 May 2021 in cloudless sky 
conditions (clouds covered less than 20% of the sky). This 
ensures the light condition for picture collection.  
B. Studied crop 
In the studied plots, chickpeas are in the reproductive 
stage. More specifically, chickpea was in the phenologic state 
R6, with the seeds starting their formation in the pods. The 
plants have an approximate height of about 40 cm. The plats 
belong to a variety of chickpea with small seeds.  
At different points of the plots, it is possible to identify 
areas affected by rabies of chickpeas. Those areas can be seen 
in Figure 4. In addition to the affected areas, the use of 
agricultural machinery in the early stages has affected the 
correct development of plants.  
C. AUV details 
The images are taken by a drone (Parrot Bebop 2) that uses 
an RGB and thermal camera (FLIR ONE Pro thermal – RGB 
camera). In the present study, only the information from the 
RGB camera is used. Figure 5 depicts the data gathering 
process. 
 
a) 
  
b)
 
Figure 3.  
Location of the studied area, a) location in the 
Iberian Peninsula and b) image of the fields.  
 
Figure 4.  
Terrestrial picture of the studied area. 
 
Figure 5.  
Picture during data collection. 
D. Image processing 
The pictures were taken at a height of 8 to 10 m above the 
ground. 
The 
images 
are 
subsequently 
processed 
mathematically to identify the areas with low vigour, 
differentiating the areas affected by rabies from the areas 
affected by the passage of a tractor. In Figure 1, we can see an 
example of the images taken. In each of the images, the tractor 
pass (on the left of the image) and the area affected by rabies 
(on the right of the image) are identified. 
The problem that we need to solve is the possible false 
positives caused by the tractor tread area. It is expected that 
when applying the VI, both the areas affected by rabies and 
the areas trampled by the tractor will give similar results. 
The process used with the images to identify areas with 
low vigour and differences between areas affected by rabies 
or by the passage of a tractor is broken down into the 
following elements: (i) Application of the VI (created for this 
case), (ii) Tools to differentiate false positives from the 
machinery pass, and (iii) Aggregation techniques and 
mathematical operations between bands. The process is 
summarized in Figure 6. 
 
 
70
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-983-6
GEOProcessing 2022 : The Fourteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

 
Figure 6.  
Block diagram of the followed process, including the most relevant tools (VI, reclassification and aggregation technique). 
V. 
EVALUATION OF RESULTS AND DISCUSSION 
In this section, we analyse the results of the proposed 
methodology and the selected values for each of the conducted 
steps. Moreover, we display the code of the used tools. 
A. Followed process 
Figure 7 shows the results of the processes followed to 
identify the areas affected by rabies. An example of the raster 
obtained working with the image taken at 10 m is shown in 
each block diagram. 
As VI, we have evaluated the use of well-known VI as 
well as analysed the generation of tailored VI. The use of 
existing VI does not offer good results compared with the 
proposed VI. Among the proposed VI, five different VI were 
considered. Being the VI indicated in (1), the one with better 
results. Three of the proposed VI are based on blue and green 
bands (B3 and B2), another VI is based on red and green bands 
(B1 and B2), and the last one on the three bands (B1, B2 and 
B3). The VI with better results is the one based on the 
combination of the three bands.  
Another relevant finding of this paper is that the "Thin" 
tool was the one that allowed the differentiation of the areas 
with low vigour due to the machinery from the areas affected 
by rabies of chickpea. The differentiation is done based on the 
different shapes of affected areas (round for rabies of chickpea 
and linear for the machinery). This tool was selected among 
different spatial analyst tools. Other tools tested in this paper 
included Boundary clean, Expand, Nibble, and Shrink. 
Nonetheless, none of these tools has proven to be as accurate 
as of the Thin tool. 
We can identify it at the end of the block diagram. As a 
result, a raster in which combines the AIVR and ATIVRR (see 
(2)), the red spots indicate the area in which rabies of chickpea 
appear. The area in grey is discarded due to the fine tool since 
it indicates that the shape of the area cannot be considered as 
an area affected by rabies of the chickpea for the given flying 
height and camera. 
 
Aggregation
Technique
Tool to 
differentiate 
zones (which tool 
will be evaluated 
in the article)
True-colour image
(RGB)
Vegetation Index
(VI)
Classification
(RESULTS)
VI Reclassified
(VIR)
71
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-983-6
GEOProcessing 2022 : The Fourteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

 
Figure 7.  
Results of each step until reaching the result. 
B. Code and values for selected code 
Following the process described in Figure 7, we detail the 
included code. The first code to be described is used for the 
reclassification of VI to generate the VIR. This code can be 
seen in Figure 8. The selected ranges do not need to be adapted 
if the proposed methodology is applied under different 
conditions.  
In the next step, the use of the Thin tool is defined in 
Figrue 9. In this case, the selected parameters were round 
since the results are more accurate than with sharp and 50 as 
the maximum thickness in pixels. The number of maximum 
thickness should be reconsidered for other scenarios in which 
the pixel size changes, such as for using another flying height 
or a camera with a different resolution.  
The Aggregation processes for both IVR and ATVIR are 
defined in Figrue 10. The aggregation method (mean for IVR 
and summation for ATVIR) should not be modified. 
Nonetheless, the cell size should be reconsidered and adapted 
for other scenarios.  
 
Code 1 
# Code for Reclassify Operation (I) 
import arcpy 
from arcpy import env 
from arcpy.sa import * 
env.workspace = "C:/sapyexamples/data" 
outReclass1 = Reclassify("VI", "Value",  
RemapRange([[0,1,1],[1,255,2]])) 
outReclass1.save("C:/sapyexamples/output/VIR")  
Figure 8.  
Code for reclassify operation (i) 
Code 2: 
# Code for Thin 
import arcpy  
from arcpy import env  
from arcpy.sa import *  
env.workspace = "C:/sapyexamples/data"  
thinOut = Thin("VIR","ZERO", "FILTER","ROUND",50) 
thinOut.save("c:/sapyexamples/output/FIVR")  
True-color Imabe
(RGB)
Red Band
Green Band
Blue Band
VI
Thin IVR (TVIR)
Aggregated TIVR
(ATVIR)
AFIVR Reclassified 
((ATVIR) R)
Result
(AVIR) x ((ATVIR) R)
VIR
AVIR
72
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-983-6
GEOProcessing 2022 : The Fourteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

Figure 9.  
Code for thin operation 
Code 3: 
# Code for Aggregate Operation 
import arcpy 
from arcpy import env 
from arcpy.sa import * 
env.workspace = "C:/sapyexamples/data" 
outAggreg = Aggregate("IVR", 5, "MEAN", 
"TRUNCATE", "DATA") 
outAggreg.save("C:/sapyexamples/output/AVIR") 
outAggreg = Aggregate("TAVIR", 25, "SUMMATION", 
"TRUNCATE", "DATA") 
outAggreg.save("C:/sapyexamples/output/ATAVIR") 
Figure 10.  
Code for Aggregate Operation 
 
Finally, Figure 11 presents the reclassification of ATVIR. 
Again, no modification of this code is needed for different 
scenarios. The selected values for the aggregation are the ones 
that allow the assigment of pixel value = 1 in the result of the 
methodology when the area has rabies of chickpea. 
 
Code 4: 
# Code for Reclassify Operation (II) 
import arcpy 
from arcpy import env 
from arcpy.sa import * 
env.workspace = "C:/sapyexamples/data" 
outReclass1 = Reclassify("AFIVR", "Value",  
RemapRange([[0,1],[0,255,0]])) 
outReclass1.save("C:/sapyexamples/output/VIR")  
Figure 11.  
Code for reclassify operation (ii). 
 
(1) 
 
 
(2) 
 
C. Accuracy of obtained results and verification 
The proposed methodology makes it possible to identify 
the areas affected by rabies in 100% of the cases. After 
removing the positives that are on the edge of the photograph, 
there are 85.7% true positives versus 14.3% false positives. In 
the verification of the methodology, using a different picture, 
we identify 100% of affected areas by rabies of chickpea. In 
this case, we have 88.2% of true positives and 11.8% of false 
positives. The results in terms of true positives and false 
positives are summarised in Table 1.  
TABLE I.  
SUMMARY OF RESULTS AND VERIFICATION 
 
Accuracy 
Nº of positive 
pixels 
True positives 
False positives 
Test 
7 
6 (85.7 %) 
1 (14.3 %) 
 
Accuracy 
Nº of positive 
pixels 
True positives 
False positives 
Verification 
17 
15 (88.2 %) 
2 (11.8 %) 
A correct classification percentage of 80% is within what 
is expected in this type of case, especially for preliminary 
results [11]. The precision is lower than in other articles in 
which more advanced classification techniques are used, in 
which case the precision is close to 90% [12]. There are cases 
in which these advanced classification techniques have a 
precision of less than 80% [13]. 
D. Dicussion 
The obtained accuracy, in the verification is aligned with 
accuracies found in other similar models [10 and 11].  
Existing approaches to solve the identification of fungic 
diseases focused on machine learning are designed for other 
crops such as cotton [14], wheat [15], maize [16], oilseed rape 
[17] or strawberry [18], amon oghers. Nonetheless, it is not 
possible to fairly compare the results since the accuracy of the 
methodologies are calculated in a whyde range of forms 
(confusion matrices, R2 of the correlation, etc).  
Only one paper has been found in which the detection of 
fungic disease in chickpea is analyzed [19] Nevertheless, in 
[19] the authors do not offer the accuracy of the disease 
detection, results are referred to productivity. Another 
example of use of remote sensing in chickpea, aimed to detect 
water stress, obtained accuracies between 72 % and 83 %. In 
a posterior study, using convolutional neural network [21], the 
maximum accuracy reached 96%. The accuracy of porposed 
method is aligned with existing accuracies in proposals for 
chickpea. 
The main limitation of the proposed methodology is the 
use of RGB images. Probably, if infrared informtionis 
included the differentiation will be easier. However, to 
maintain the methodology as simple as possible and to avoid 
using high-cost resources, RGB images are the sole 
information source. 
VI. 
CONCLUSION 
In the presented work, we have evaluated a methodology 
to identify areas affected by rabies capable of not giving false 
positives in the areas through which agricultural machinery 
has passed. It is a preliminary methodology that will be 
evaluated within the framework of the GO TecnoGAR and 
Valvagar-Dron Guardes projects in the years 2022 and 2023. 
In the future, it is hoped that this information will be 
combined with specific treatments that will reduce the number 
of phytosanitary products used to treat large areas. Similar 
methodologies will also be used to identify weeds. 
ACKNOWLEDGMENT 
This research was partially funded by the “Programa 
Estatal de I+D+i Orientada a los Retos de la Sociedad, en el 
marco del Plan Estatal de Investigación Científica y Técnica 
y de Innovación 2017–2020” (Project code: PID2020-
114467RR-C31 and PID2020-114467RR-C33), and by 
73
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-983-6
GEOProcessing 2022 : The Fourteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

“Proyectos de innovación de interés general por grupos 
operativos de la Asociación Europea para la Innovación en 
materia de productividad y sostenibilidad agrícolas (AEI-
Agri)” in the framework “Programa Nacional de Desarrollo 
Rural 2014–2020”, GO TECNOGAR, and by Comunidad de 
Madrid, through IMIDRA” FP22-ValvaGAR. 
REFERENCES 
[1] J. Gai, L. Tang, L., and B. L. Steward, "Automated crop plant 
detection based on the fusion of color and depth images for 
robotic weed control," Journal of Field Robotics, vol. 37, no. 1, 
pp. 35-52, 2020. 
[2] J. D. Pujari, R. Yakkundimath, and A. S. Byadgi, "Image 
processing based detection of fungal diseases in plants," 
Procedia Computer Science, vol. 46, pp. 1802-1808, 2015. 
[3] N. Zhang, et al., “A review of advanced technologies and 
development for hyperspectral-based plant disease detection in 
the past three decades,” Remote Sensing, vol. 12, no. 19, pp. 
3188, 2020. 
[4] J. Zhang, et al., "Monitoring plant diseases and pests through 
remote sensing technology: A review," Computers and 
Electronics in Agriculture, vol. 165, pp. 104943, 2019. 
[5] N. K.Gogoi, B. Deka, and L. C. Bora, "Remote sensing and its 
use in detection and monitoring plant diseases: A review, " 
Agricultural Reviews, vol. 39, no. 4, pp. 307-313, 2018. 
[6] M. Sancho-Adamson, M. I. Trillas, J. Bort, J. A. Fernandez-
Gallego, and J. Romanyà, "Use of RGB vegetation indexes in 
assessing early effects of Verticillium wilt of olive in 
asymptomatic plants in high and low fertility scenarios," 
Remote Sensing, vol. 11, no. 6, pp. 607, 2019. 
[7] A. Yee-Rendon, I. Torres-Pacheco, A. S. Trujillo-Lopez, K. P. 
Romero-Bringas, and J. R. Millan-Almaraz, "Analysis of New 
RGB Vegetation Indices for PHYVV and TMV Identification 
in Jalapeño Pepper (Capsicum annuum) Leaves Using CNNs-
Based Model," Plants, vol. 10, no. 10, pp. 1977, 2021 
[8] B. D. S. Barbosa, et al. "Application of RGB Images Obtained 
by UAV in Coffee Farming," Remote Sensing, vol. 13, no. 12, 
pp. 2397, 2021. 
[9] S. Yousfi, J. Marín, L. Parra, J. Lloret, and P. V. Mauri, 
"Remote sensing devices as key methods in the advanced 
turfgrass phenotyping under different water regimes," 
Agricultural Water Management, vol. 266, pp. 107581, 2022. 
[10] L. Parra, et al., "Drone RGB images as a reliable information 
source to determine legumes establishment success," Drones, 
vol. 5, no. 3, pp. 79, 2021. 
[11] L. Parra, D. Mostaza-Colado, J. F. Marin, P. V. Mauri, and J. 
Lloret, "Methodology to Differentiate Legume Species in 
Intercropping Agroecosystems Based on UAV with RGB 
Camera," Electronics, vol. 11, no. 4, pp. 609, 2022. 
[12] Z. Iqbal, et al. "An automated detection and classification of 
citrus plant diseases using image processing techniques: A 
review," Comput. Electron. Agr., vol. 153, pp. 12-32, 2018. 
[13] J. Boulent, S. Foucher, J. Théau, and P. L. St-Charles, 
"Convolutional 
neural 
networks 
for 
the 
automatic 
identification of plant diseases," Frontiers in plant science, vol. 
10, pp. 941, 2019. 
[14] T. Wang, J. A. Thomasson, C. Yang, T. Isakeit, and R. L. 
Nichols, "Automatic classification of cotton root rot disease 
based on UAV remote sensing," Remote Sensing, vol. 12, no. 
8, pp. 1310, 2020. 
[15] U. Shafi, et al. "Wheat rust disease detection techniques: a 
technical perspective," Journal of Plant Diseases and 
Protection, pp. 1-16, 2022. 
[16] A. Loladze, F. A. Rodrigues Jr, F. Toledo, F. San Vicente, B. 
Gérard, and M. P. Boddupalli, "Application of remote sensing 
for 
phenotyping 
tar 
spot 
complex 
resistance 
in 
maize," Frontiers in Plant Science, vol. 10, pp. 552, 2019. 
[17] F. Cao, et al. "Fast detection of sclerotinia sclerotiorum on 
oilseed rape leaves using low-altitude remote sensing 
technology," Sensors, vol. 18, no. 12, pp. 4464, 2018. 
[18] Q. Liu, et al. "Information fusion of hyperspectral imaging and 
electronic nose for evaluation of fungal contamination in 
strawberries during decay," Postharvest Biol. Technol., vol. 
153, pp. 152–160, 2019. 
[19] C. Zhang, W. Chen, and S. Sankaran, "High-throughput field 
phenotyping of Ascochyta blight disease severity in 
chickpea," Crop Protection, vol.125, pp. 104885, 2019. 
[20] S. Azimi, T. Kaur, and T. K. Gandhi, "Water stress 
identification in chickpea plant shoot images using deep 
learning," The 17th India Council International Conference 
(INDICON), Delhi, India, 10-13 Decembre 2020, pp. 1-7. 
[21] S. Azimi, T. Kaur, and T. K. Gandhi, "BAT optimized CNN 
model identifies water stress in chickpea plant shoot images," 
In the 25th International Conference on Pattern Recognition 
(ICPR), Milano, Italy, 10-15 January 2021 pp. 8500-8506. 
74
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-983-6
GEOProcessing 2022 : The Fourteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

