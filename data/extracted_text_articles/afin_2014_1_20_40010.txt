Interconnected Multiple Software-Defined Network Domains with Loop Topology 
 
Jen-Wei Hu 
National Center for High-performance 
Computing &  
Institute of Computer and 
Communication Engineering 
NARLabs & NCKU 
Tainan, Taiwan 
hujw@narlabs.org.tw 
 
Chu-Sing Yang 
Institute of Computer and 
Communication Engineering 
NCKU 
Tainan, Taiwan 
csyang@mail.ee.ncku.edu 
 
Te-Lung Liu 
National Center for High-performance 
Computing 
NARLabs 
Tainan, Taiwan 
tlliu@narlabs.org.tw 
 
 
Abstract—With the trends of software-defined networking 
(SDN) deployment, all network devices rely on a single 
controller will create a scalability issue. There are several novel 
approaches proposed in control plane to achieve scalability by 
dividing the whole networks into multiple SDN domains. 
However, in order to prevent broadcast storm, it is important 
to avoid loops in connections with OpenFlow devices or 
traditional equipments. Therefore, one SDN domain can only 
have exactly one connection to any other domains, which will 
cause limitation when deploying SDN networks. Motivated by 
this problem, we propose a mechanism which is able to work 
properly even the loops occurred between any two controller 
domains. Furthermore, this mechanism can also manage link 
resources more efficiently to improve the transfer performance. 
Our evaluation shows that the transmissions between hosts 
from different areas are guaranteed even if the network 
topology contains loops among multiple SDN domains. 
Moreover, the proposed mechanism outperforms current 
method in transferring bandwidth. 
Keywords-Software-defined networking; OpenFlow; multiple 
domains; loop topology. 
I. 
 INTRODUCTION 
During the last decades, numbers of innovative protocols 
are proposed by researchers in network area. However, it is 
hard to speed up the innovation because network devices are 
non-programmable. The software defined networking (SDN) 
approach is a new paradigm that separates the high-level 
routing decisions (control plane) from the fast packet 
forwarding (data plane). Making high-speed data plane still 
resides on network devices while high-level routing 
decisions are moved to a separate controller, typically an 
external controller. OpenFlow [1] is the leading protocol in 
SDN, which is an initiative by a group of people at Stanford 
University as part of their clean-slate program to redefine the 
Internet architecture. When an OpenFlow switch receives a 
packet it has never seen before, for which it has no matching 
flow entries, it sends this packet to the controller. The 
controller then makes a decision on how to handle this 
packet. It can drop the packet, or it can add a flow entry 
directing the switch on how to forward similar packets in the 
future. 
Moving local control functionalities to remote controllers 
brings numerous advantages, such as device independency, 
high flexibility, network programmability, and the possibility 
of realizing a centralized network view [2].  However, with 
the number and size of production networks deploying 
OpenFlow equipments increases, there have been increasing 
concern about the performance issues, especially scalability 
[3]. 
The benchmarks on NOX [7] showed it could only 
handle 30,000 flow installs per second. However, in 
[2][4][5][6], authors mention fully physically centralized 
control is inadequate because relying on a single controller 
for the entire network might not be feasible. In order to 
alleviate the load of controller and achieve more scalability, 
there are several literatures proposed their solutions. 
DevoFlow [8] which addresses this problem by proposing 
mechanisms in data plane (e.g., switch) with the objective of 
reducing the workload towards the controller [6]. In contrast 
to request reducing in data plane, the other way is to propose 
a distributed mechanism in control plane. A large-scale 
network should be divided into multiple SDN domains, 
where each domain manages a relatively small portion of the 
whole network, such like that many data centers may be 
located on different areas for improving network latency. 
However, if separating to multiple SDN domains, we will 
lose the consistent centralized control. Currently, there is no 
protocol for solving this issue [9]. Thus, there are some 
proposed frameworks [4][5][6] in which create a specific 
controller to collect information (e.g., states, events, etc.) 
from multiple domain controllers. They all focus on solving 
controller scalability issues and facilitating a consistent 
centralized control among multiple controller domains. 
 
 
Figure 1.  Looped example of a topology with SDN devices and traditional 
equipments. 
 
8
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-377-3
AFIN 2014 : The Sixth International Conference on Advances in Future Internet

For increasing reliability and transmission rate, multiple 
links are often deployed between nodes in ordinary network  
design, which practically create loops in topology. The loop 
leads to broadcast storms as broadcasts are forwarded by 
switches out of every port, the switches will repeatedly 
rebroadcast the broadcast packets flooding the network. 
Since the Layer 2 header does not support a time to live 
(TTL) value, if a packet is sent into a looped topology, it can 
loop forever and bring down the entire network. Border 
gateway protocol (BGP) can handle the loop topology in 
current Internet, but it is designed based on Layer 3. Thus, 
BGP still cannot prevent broadcast storm. In order to tackle 
broadcast storm issue, the spanning tree protocol (STP) is 
usually used to only allow broadcast packets to be delivered 
through the STP tree. This design avoids broadcast storm but 
reduces the overall utilization of the links as a consequence. 
When in a single SDN domain, controller has the complete 
knowledge over the entire network topology, thus the tree 
can be easily built. However, when the SDN network has 
been split into SDN islands, with the traditional network in 
between, the controller no longer knows the complete 
topology, resulting in the inability to build an effective tree, 
as shown in Figure 1. In this case, the existence of the loop 
may block the communication between two islands because 
the broadcast packets transmitted in the topology would 
mislead the controller to believe that the two hosts in 
communication are from the same island, thus wrong flow 
entries are incorporated. The situation will become even 
more complex in multiple SDN domains. In this paper, we  
focus on Layer 2 and propose a mechanism which can work 
properly even the loops occurred between any two SDN 
domains. Furthermore, this mechanism can also more 
efficiently in using link resources to improve the transfer 
performance. 
The remainder of the paper is organized as follows. 
Section 2 presents a brief review of relevant research works 
focus on solving scalability issue in control plane. In Section 
3, we define the preliminaries which will be used in 
proposed mechanism. Then, we briefly describe proposed 
mechanism for solving the loop limitation between multiple 
SDN domains and traditional networks in Section 4. In 
Section 5, we evaluate our mechanism in a real environment 
among multiple SDN domains and experiments are reported 
and compared with the current approach. Finally, the paper is 
concluded. 
II. 
RELATED WORKS 
Onix [5] is a control plane platform, which was designed 
to enable scalable control applications. It is a distributed 
instance with several control applications installed on top of 
control plane. In addition, it implements a general API set to 
facilitate access the Network Information Base (NIB) data 
structure from each domain. However, authors mention this 
platform does not consider the inter-domain network control 
due to the control logic designer needs to adapt the design 
again when changed requirements. 
HyperFlow [4] is a distributed event-based control plane 
for OpenFlow. It facilitates cross-controller communication 
by passively synchronizing network-wide view among 
OpenFlow controllers. They develop a HyperFlow controller 
application and use an event propagation system in each 
controller. Therefore, each HyperFlow controller acts as if it 
is controlling the whole network. In addition, each 
HyperFlow controller processes and exchanges these self-
defined events and the performance gets poor when the 
number of controllers grows [4]. 
HyperFlow and Onix assume that all applications require 
the network-wide view; hence, they cannot be of much help 
when it comes to local control applications.  In Kandoo [6], 
authors design and implement a distributed approach to 
offload control applications over available resources in the 
network with minimal developer intervention without 
violating any requirements of control applications. Kandoo 
creates a two-level hierarchy for control planes. One is local 
controller which executes local applications as close as 
possible to switches in order to process frequent requests, 
and the other is a logically centralized root controller runs 
non-local control applications. It enables to replicate local 
controllers on demand and relieve the load on the top layer, 
which is the only potential bottleneck in terms of scalability. 
The above proposals focus on network control, which 
proactively create the inter-domain links. Thus, these 
approaches are able to provision cross-domain paths but the 
complexity of maintaining global proactive flow rules can be 
minimized. However, loops may form between controller 
domains and need to be dealt with carefully. To meet this 
requirement, we develop a mechanism which dynamically 
forwards packets in the reactive way and solves the loop 
limitation 
between 
multiple 
controller 
domains 
and 
traditional networks. 
III. 
PRELIMINARIES 
We assume the network topology as an undirected graph 
       , where             is a finite set, the 
elements of which are called vertices, and             
    is a finite set, the elements of which are called edges, 
where each edge    can be represented by        , with 
     . 
In order to limit the propagation of edge information that 
is only relevant in certain portions of the network and to 
improve scalability, we group vertices into structures called 
areas, denoted as  . Each vertex     is assigned a label 
        that is taken from a set L, describing the area that 
 belongs to. Each area has a controller which is charged to 
coordinate vertices to exchange edge information with its 
connected areas. As shown in Figure 2, vertices   ,   , and 
   are on the same area       .        is the controller of area 
      .  is a special label used to represent the area is not 
belong to any controller domains (e.g., legacy network). That 
is, area      do not have a controller which can 
communicate with other areas. There is another special area, 
called Root Area (RA), which is the root of all areas whose 
starting item of label match to the label of RA. The controller 
of RA, called Rooter, collects edge information from its area 
controllers. Therefore, the Rooter owns a global relationship 
among its controlled areas.  
9
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-377-3
AFIN 2014 : The Sixth International Conference on Advances in Future Internet

 
Figure 2.  A sample network where vertices have been assigned to areas, 
represented by rounded boxes. 
 
A vertex         incident on an edge      , such that 
        is called a border vertex for      , and is in charge 
of exchanging the edge information to other connected area 
     where          . If a vertex         has an edge 
     , such that        , we call   and   as inner vertices. 
An inner vertex only exchanges edge messages to other inner 
vertices in the same area. 
There are two different edge information messages. one 
is exchanged among border vertices in network        , 
called                   where     is the vertex 
which fires this edge information message;      is the port 
of vertex   that sends the outgoing edge information;      is 
the label of   to represent the area that   belongs to;   
(possible empty) is the set of parameters of the port (e.g., 
priority), which can realize some simple link utilization 
functions. Each area controller disseminates the other edge 
message, called                , to its Rooter. As 
illustrated in Figure 2,      will receive   from       ,       , 
and        respectively. There are three fields in  ,      is a 
composition field which includes vertex  , its label     , 
and port     ; the definition of      is same as      but the 
vertex     such that     is the end vertex;   is the set 
of parameters of the port. 
Each area controller owns full edge information of 
neighbor areas via all its border vertices. If there exists a port 
on one border vertex, it receives the edge information of a 
specific area which is the same as its area controller, we call 
this port of the vertex is a Representative Port (RP) for this 
neighbor area. As illustrated in Figure 2,         and 
        are two RPs for        in area       . 
IV. 
PROPOSED MECHANISM 
In this section, we describe the design philosophy and 
implementation of our approach. In general SDN network 
environment, all vertices in the same area will exchange edge 
information to each other. This is well defined in OpenFlow 
specification and all popular controllers have already 
implemented it. However, extended edge information that 
beyond this area will not be exchanged. In order to compose 
edge information across different areas, the border vertex 
which resides in one area will exchange the edge information 
to its neighbor border vertices that may be directly connected 
or through one or more area     . These operations are 
formalized in Figure 3. First of all, area controller       calls 
sub-procedure to update its data structures according to the 
received message     , such as its neighbor area list and 
connected edges list for areas. Then, it creates and 
disseminates edge information that are newly appeared in 
    . After processing all new edge information,       
disseminates edge information that updated the set of 
parameters. Finally, the procedure updates the list of RP, that 
is used when area controller determines which ports allowed 
to forward broadcast packets to all connected areas. Note that, 
in each area controller we have a background process to 
check the validity of edge information. If it exceeds the 
timeout         , all related information of edge will be 
removed. 
 
01: procedure UpdateBorderVertexEdges (      ) 
02:       is a set stores all border vertices and ports in       
03:    is the set of all connected areas in       
04:       is the map of                      that 
stores the information of all discovered border vertices 
according to different connected areas. 
05:        is the map of                    , where   
and   are resided in two different controller areas 
respectively. 
06:      is the map of                              
which represents if sending packets to specific area, we can 
choose one vertex-port pair in the list. 
07:   for each                                  
08:     UpdateElements (    ) 
09:     Initialize             to a configured edge timeout 
10:     Create a new   
11:                          
12:                         ;       
13:     Send   to the Rooter 
14:     UpdateRepresentativePorts (             ) 
15:   end for 
16:   for each                                  
17:     if                                
18:            is an updated instance of   in       
19:                  ;         ; renew          
20:       Send   to the Rooter 
21:     end if 
22:   end for 
23: end procedure 
24: subprocedure UpdateElements ( ) 
25:            }  
26:   if         
27:            } 
10
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-377-3
AFIN 2014 : The Sixth International Conference on Advances in Future Internet

28:   if                          
29:                             
30:   if                      ) 
31:                           
32: end subprocedure 
33: function UpdateRepresentativePorts (             ) 
34:   for each         
35:     for each     
36:       if                   
37:                            
38:     end for 
39:   end for 
40: end function 
Figure 3.  Algorithm used for updating the set       of known edge 
information in area controller       when the new edge information      
arrived at a border vertex  . 
 
As we described in Section 3, each area controller will 
send edge information   to its Rooter periodically. In our 
proposal, this Rooter is responsible for providing the 
network-wide topology. Although our mechanism works 
well even eliminating the Rooter, we still keep this element  
for preserving the control flexibility to network management 
system in the upper layer, such as altering the original flow 
path and so on.  We now illustrate the operations undertaken 
by Rooter to update its controlled area topology according to 
received  . These operations are formalized as the 
procedure UpdateAreaTopology (    ) in Figure 4. First of 
all, Rooter composes any newly edge information from area 
controllers. If received   is already existed in edge 
information of Rooter, this process only updates the new 
parameter and the timeout of this edge. Last, the Rooter will 
refresh the area topology according to updated         and 
keep this data structure for computing the area path in the 
future.  
Both area        and        in Figure 2 have two border 
vertices. Take vertex    in area         as an example, it 
receives edge information from area        by two paths, 
              and (         . Similar to vertex   , 
we also can discover two paths. Therefore,  four edges are 
discovered between the area        and        in Rooter. After 
processing the algorithm of Figure 4, the area topology is 
delivered, as shown in Figure 5. 
 
01: procedure UpdateAreaTopology (    ) 
02:     is a new or an updated edge from controlled areas 
03:   for each                               
04:     Update         by   and initialize            
05:   end for 
06:   for each                                  
07:     if                              
08:                  ; renew            
09:   end for 
10:   Refresh area topology according to         
11: end procedure 
Figure 4.  Algorithm to update area topology at Rooter according to the 
received edge information from controller areas. 
 
 
 
Figure 5. The logical network reduced from the sample network in Figure 2. 
 
We illustrate the operation undertaken by area controller 
      when it receives a broadcast packet from any vertex in 
the controlled domain. These operations are formalized in 
Figure 6. The input parameters are composed of all 
connected areas ( ), the representative port list generated in 
Figure 3, and the broadcast packet (packet). This procedure 
goes through the set   and checks how many vertex-port 
pairs in    of  . If there contains two or more pairs, it uses 
the 
function 
                              
 to 
determine the ports of border vertices for forwarding this 
broadcast packet. 
 
01: procedure HandleBroadcastPacket (            ) 
02:         is the flow tables of vertex   
03:     is the set of all connected areas in       
04:      is the map of                              
05:                                      in       
06:   for each     
07:     if size of         
08:                                                       
09:     else // only one item in       
10:                            
11:                          
12:                             
13:     end if 
14:   end for 
15: end procedure 
16: function PickForwardingVertexPorts (  ) 
17:      
18:  for each       
19:     if     or                    
20:            
21:  end for 
22:                      
23:  return   
24: end function 
Figure 6.  Algorithm used for determining which ports on its all border 
vertices will be used to forward broadcast packets to other areas. 
 
11
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-377-3
AFIN 2014 : The Sixth International Conference on Advances in Future Internet

 
 
Figure 7. An example of forwarding packets from a host to different areas. 
 
 
Figure 8. An example of selecting one port in the receiver according to 
arriving time of the same request packet. 
 
To show an example of process in Figure 6, we consider 
the area        in Figure 5 and let one host H1 connect to 
vertex    in this area, as shown in Figure 7. There are three 
vertex-port pairs on all border vertices in       , two of them 
can reach to area        and the other is for area       . 
Assume that H1 sends a broadcast packet (e.g., an ARP 
request). In the meantime, the area controller triggers the 
procedure HandleBroadcastPacket and determines the 
forwarding ports on its border vertices to transmit the packet 
to other areas. Note that we only consider RPs on all border 
vertices, if there exists other type of ports, such as access 
ports (e.g., connecting to hosts) or intra-switch ports (e.g., 
the port on    connecting to   ), our mechanism also 
forwards the broadcast packet to these ports.  
In Figure 7, both         and         are RPs for area 
      , according to the algorithm in Figure 6 we will select 
only one RP and forward the broadcast packet. As shown in 
Figure 7, we choose the port 3 of vertex    to forward the 
broadcast packet of host H1 to area        while using the 
port 2 of vertex    to transmit the same packet to area       . 
In this way, we can decrease the number of packets in 
network to offload area controllers. Moreover, selecting the 
forwarding edge from one single area can ensure the 
effective usage of the links without conflicts with other hosts 
which target the same area. We choose the RP to forward 
packets according to the parameter on the port (e.g.,  ). If 
this port is selected this time, it will adjust the priority to 
ensure we can choose another ports next time. This priority 
value will be restored when the flow is released. 
In addition, to ensure that the receiver H2 only replies via 
one of the ports, the arrival time of the request is recorded 
and used to determine the returning port of H2’s reply. In 
Figure 8, area        receives the same broadcast packet from 
three different ports, P1, P2, and P3. Assume the arrival 
times are tP1, tP2, and tP3 respectively and tP1 is the minimum 
of them. Thus, host H2 chooses the P1 to send its reply 
packet. 
V. 
EVALUATION  
In this section, we describe the performance evaluation of 
our mechanism. We simulate physical network connection 
between TWAREN and Internet2 as our experiment 
topology. There are 2 physical servers equipped with 64G of 
RAM and 2 Intel Xeon(R) L5640 CPUs. Each of them runs a 
Mininet [10] to emulate OpenFlow network topology in 
TWAREN (e.g.,       ) and Internet2 (e.g.,       ). In 
addition, we create another domain, called       , with 2 
physical OpenFlow switches and 1 physical host. There are 4 
controllers, one of them represents the Rooter controller and 
the others install Floodlight (version 0.9) and manage their 
own areas. The topology of our experiment is shown in 
Figure 9.  
 
Figure 9. Experiment topology. 
 
As we described in Section 1, controller can handle loop 
topology in a single domain but not multiple domains. 
Therefore, we only consider the loops across two or more 
domains. A loop is represented as a series of edge nodes 
(e.g., TP-CHI-LA-HC-TP). 
 
Figure 10. Ping successful rate with four different cases. 
 
12
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-377-3
AFIN 2014 : The Sixth International Conference on Advances in Future Internet

 
Figure 11. Comparison of transferring bandwidth in two methods when 
there are two pairs of hosts transferring packets at the same time. 
 
In the first experiment, we evaluate the ping successful 
rate of our method by comparing with the original 
forwarding method. There are four cases in this experiment. 
In each case, we run 10 times on host pairs (e.g., H1-H2, H1-
H3, H1-H4, and H1-H5) and compute the ping successful 
rate. In Case 1, we remove two inter-domain links (e.g., HC-
LA and V2-LA) to construct the topology with no loops. As 
depicted in Figure 10, both our proposed method and the 
original method have 100% ping successful rate. In the 
second case, we add the link HC-LA to form one loop (e.g., 
TP-CHI-LA-HC-TP) between domain        and       . We 
note that there are 2 paths from CHI to LA (e.g., CHI-SEA-
LA and CHI-SLC-LA). But in our experiment, we only 
require that there is at least one path between these 2 edge 
nodes to assure the connectivity in a single domain. Thus, 
this is not regarded as a failure. The Case 2 in Figure 10 
shows our method reaching 100% ping successful rate while 
the original method is only 40%. We add link V2-LA in the 
third case to create the topology with 3 loops (e.g., TP-CHI-
LA-HC-TP, HC-LA-V2-V1-TN-HC, and TP-CHI-LA-V2-
V1-TN-HC-TP). Our proposed method still reaches 100% 
but the original forwarding method is lower than 30%, 
shown as Figure 10. In the final case, we add an extra link 
between HC and LA. The result is illustrated in Case 4 of 
Figure 10. This experiment shows that our method is 
working regardless the number of loops in the topology. We 
can guarantee any host can successfully communicate with 
hosts in other areas. 
Next, we compare our proposal with STP protocol. We 
use two pairs of hosts (e.g., H1-H2 and H3-H4) to measure 
the performance when these hosts transferring packets at the 
same time. Host H2 and H4 are selected as the iperf servers 
while the others are the clients of iperf. In order to simulate 
STP protocol, we remove two inter-domain links (e.g., HC-
LA and V2-LA) to build a tree structure in a looped topology. 
Figure 11 demonstrates the throughput results from STP and 
our method. We observe our proposed is performing 
considerably better than STP, offering 77% increasing 
throughput compared to STP. The reason is STP has only 
one path between domain        and       . Thus, two pairs 
of hosts share this inter-domain link TP-CHI. But in our 
proposed method, if there exists another link between these 
two domains, they will all be used to improve the transfer 
performance. 
VI. 
CONCLUSION 
In this paper, we have proposed a mechanism for solving 
transmission problem among SDN domains with loops. The 
proposed algorithms select one port for each connected area 
to forward broadcast packets. It decreases the number of 
packets in network to offload area controllers. In addition, 
the area controller uses our method to filter the repeated 
broadcast packets at a border vertex and do not forward these 
packets to avoid broadcast storm. Besides, as compared with 
original forwarding method, our method can efficiently use 
multiple edges in loops topology to improve the transferring 
bandwidth. 
Our future work is to improve path compute by defining 
more granular parameters across multiple SDN domains. In 
addition, our proposal use the Advanced Message Queuing 
Protocol (AMQP) to exchange edge information between the 
Rooter controller and area controllers. We can integrate our 
approach with other event-based frameworks (e.g., Kandoo) 
for resources management among multiple domains. 
REFERENCES 
[1] N. McKeown et al., “Openflow: Enabling Innovation in 
Campus 
Networks,” 
ACM 
SIGCOMM 
Computer 
Communication Review, vol. 38, no. 2, pp. 69-74, Apr. 2008. 
[2] S. H. Yeganeh, A. Tootoonchian, and Y. Ganjali, “On 
Scalability of Software-Defined Networking,” IEEE Commun. 
Mag., vol. 51, no. 2, pp. 136-141, Feb. 2013. 
[3] D. Levin, A. Wundsam, B. Heller, N. Handigol, and A. 
Feldmann,  “Logically Centralized? State Distribution Trade-
offs in Software Defined Networks,” Proc. ACM Workshop 
Hot Topics in Software Defined Networks (HotSDN '12), 
Aug. 2012, pp. 1-6, ISBN: 978-1-4503-1477-0. 
[4] A. Tootoonchian and Y. Ganjali, “HyperFlow: A Distributed 
Control Plane for OpenFlow,” Proc. Internet Network 
Management Workshop/Workshop on Research on Enterprise 
Networking (INM/WREN '10), USENIX Association, Apr. 
2010, pp. 3-3. 
[5] T. Koponen et al., “Onix: A Distributed Control Platform for 
Large-scale Production Networks, ” Proc. USENIX Symp. on 
Operating Systems Design and Implementation (OSDI '10), 
Oct. 2010, pp. 351-364, ISBN:  978-1-931971-79-9. 
[6] S. H. Yeganeh and Y. Ganjali, “Kandoo: A Framework for 
Efficient and Scalable Offloading of Control Applications,” 
Proc. ACM Workshop Hot Topics in Software Defined 
Networks (HotSDN '12), Aug. 2012, pp. 19-24, ISBN:978-1-
4503-1477-0. 
[7] A. Tavakoli, M. Casado, T. Koponen, and S. Shenker, 
“Applying NOX to the Datacenter,” Proc. ACM Workshop on 
Hot Topics in Networks (HotNets '09), Oct. 2009, pp. 1-6. 
[8] A. R. Curtis et al., “DevoFlow: Scaling Flow Management for 
High-Performance Networks,” Proc. ACM SIGCOMM 2011 
Conference (SIGCOMM '11), Aug. 2011, pp. 254-265, ISBN: 
978-1-4503-0797-0. 
[9] H. Yin et al., “SDNi: A Message Exchange Protocol for 
Software Defined Networks (SDNS) across Multiple 
Domains,” IETF Internet-Draft, draft-yin-sdn-sdni-00, Jun. 
2012. 
[10] B. Lantz, B. Heller, and N. McKeown, “A network in a 
Laptop: Rapid Prototyping for Software-Defined Networks,” 
Proc. ACM Workshop on Hot Topics in Networks (HotNets 
'10), Oct. 2010, pp. 1-6. 
13
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-377-3
AFIN 2014 : The Sixth International Conference on Advances in Future Internet

