Computer Vision-based System for Impaired Human Vision Compensation
Povilas Daniuˇsis1, Andrius Budrionis1,2, Audrius Indriulionis1,3, and Darius Plikynas1
1Department of Business Technologies and Entrepreneurship
Gediminas Technical University
Vilnius, Lithuania
Email: {povilas.daniusis, darius.plikynas}@vgtu.lt
2Norwegian Centre for E-health Research
University Hospital of North Norway
Tromsø, Norway
Email: andrius.budrionis@ehealthresearch.no
3Laboratory of Quaternary Research
Nature Research Centre
Vilnius, Lithuania
Email: audrius.indriulionis@gmail.com
Abstract—This paper presents a high-level architecture of a
computer vision-based system for partial compensation of lost or
impaired human vision. It combines standard smartphone device,
external deep learning-based image processing infrastructure and
audio/tactile user interface. The proposed architecture is based
on input from user-centered design process, involving end-users
into system development. The paper discusses user needs and
expectations for electronic travelling aids for the blind and
highlights limitations of the existing solutions. The suggested
architecture may be used as a basis for developing computer
vision-based tools for visually impaired individuals.
Keywords–computer vision; deep learning; mobile application;
aid for blind and visually impaired; audio feedback; tactile feed-
back; impaired human vision compensation; user-centered design.
I.
INTRODUCTION
More than 250 million people worldwide have moderate
to severe vision impairment, while (≈ 36 million are blind)
[1]. During the past decades signiﬁcant effort was devoted to
develop computer vision and other sensor-based aids [2]-[5]
for helping the blind and visually impaired users to perceive
the surrounding world better. However, computer vision is a
rapidly evolving ﬁeld; the proposed systems become outdated
in a relatively short time and often lack accuracy and reliability
in real-world conditions in comparison to the state-of-the-
art technologies. In this article, we describe a system, which
employs modern computer vision techniques for compensation
of lost or impaired vision function in humans. Many of the
previously proposed electronic aids for the blind count on
highly specialized hardware, for instance smart glasses [5],
Microsoft Kinnect sensors [6], helmet-mounted photo sensors
and cameras [7]. Such systems often lack convenience and
bring additional complexity into daily tasks, which visually
impaired people currently manage by the help of a white cane.
This project puts major emphasis on developing a tool, which
seamlessly integrates with the hardware visually impaired per-
sons already have on-hand and are used to (a smartphone). This
paper describes a high level architecture of the technology aid
for visually impaired people utilizing the latest achievements
in the ﬁeld of computer vision.
This paper is structured as follows. Section II describes
the methodological foundations of this research. Section III
focuses on the architecture of the suggested human vision
compensation system. Finally, Section IV summarizes and
discusses the results of the conducted analysis.
II.
METHOD
Following the principles of the participatory design [8][9]
we included end-users into system design process. Two vi-
sually impaired persons participated in discussion and focus
group meetings together with the project team to identify
the key properties of the solution. During the six discussion
sessions, major attention was paid to:
1)
Functionality and key features of the solution address-
ing real-life scenarios of visually impaired persons;
2)
Technical feasibility, connecting user needs to the
latest development in computer vision and assistive
technologies;
3)
Interface between the visually impaired person and
the assistive technology;
4)
Appearance, usability and potential costs of the pro-
posed product.
Participatory design process included testing of assistive tech-
nologies for the blind and visually impaired people using
widely available mobile phone apps like Seeing AI, Blind-
Ways, Be My Eyes, Aipoly Vision, TapTapSee, etc. Semi-
structured tests were performed by visually impaired partic-
ipants and researchers from the project group. Strengths and
weaknesses of the existing solutions are reﬂected in the design
of the proposed system.
To have an overview on research progress on assistive
technologies for the blind, a systematic literature review in
three major research databases (Medline, IEEE xplore and
ACM DL) was conducted covering publication period of 10
years. It revealed increasing research interest in the ﬁeld (84
related publications were identiﬁed, while 17 were character-
ized as highly relevant for this project) and provided better
81
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-688-0
eTELEMED 2019 : The Eleventh International Conference on eHealth, Telemedicine, and Social Medicine

understanding of the state-of-the-art in applying computer
vision technologies to address the needs of the blind. The
review was extended to relevant patents and patent applica-
tions worldwide. The search was performed in the following
databases: EspaceNet, GooglePatents, USPTO, PatentScope,
FreePatentsOnline. It revealed several technological trends in
the ﬁeld of assistive technologies for the blind. Based on
the aforementioned reviews, our insights are presented in the
Results section.
III.
RESULTS
Participatory design sessions with visually impaired in-
dividuals and analysis of the aforementioned data sources
provided substantial input for the design of the proposed
system (Figure 1). The process made it clear that understanding
of the user needs was limited in the project team. Based
on the results of the participatory design sessions, scientiﬁc
publications and patent analysis, we present the outline of the
system for impaired human vision compensation.
The key part of the proposed system is a smartphone
device, connecting User Interface (UI) and data processing
components into a well-functioning ecosystem. Smartphone
was selected due to its high availability and capability to
perform required computations or forward visual information
for processing to the external server via the Internet.
UI consists of a wearable forehead belt, integrating RGB
and depth cameras, Inertial Measurement Unit (IMU), tactile
feedback device and bone conduction headphones. Although
more expensive than standard ones, bone conduction head-
phones do not block the ears and allow the user to perceive
audio information from the surrounding environment and the
system at the same time, which is essential for the visually
impaired people.
However, some types of guidance information may be more
efﬁciently perceived through tactile feedback device than audio
channel. An example would be indicating the direction of a
detected object with a vibromotor on the corresponding area on
the forehead belt, instead of providing audio description. More
research is needed to clarify how various types of guidance
information should be represented to the user in the most
convenient and efﬁcient way. For instance, detected objects
could be presented to the user in a conﬁgurable order, based on
the importance of particular object classes to the user or other
personalized criteria. USB and Wiﬁ data links can be used
for bidirectional data transmission between UI and smartphone
component.
Although modern high-end smartphones are powerful com-
putational devices, many computer vision algorithms require
hardware of signiﬁcantly higher processing capabilities (e.g.,
a server equipped with a high-end Graphics Processing Unit
(GPU)). Therefore, an external server is used for live video
data processing. In our design we envision a computational
server hosting a set of computer vision algorithms imple-
mented and exposed via web services: faster Regions with
Convolutional Neural Network features (faster R-CNN) object
detector [10], trained to detect important objects, Convolu-
tional Neural Network (CNN) and Recurrent Neural Network-
based (RNN) scene descriptor [11], which provides textual
annotation of a given RGB image, place recognition [12],
face detection and recognition [13], obstacle detection [14],
action recognition [15], among others. After video analysis is
completed on the server side, results are transmitted back to
Text­to­speach and
tactile interface
Image/video capture
Internet
Data
processing
Figure 1. Schematics of vision compensation device.
the smartphone component and represented to the user through
the aforementioned UI.
The user interacts with the system through gestures, voice
commands or speciﬁcally designed user interface on the
smartphone. User input is required for selecting operation
mode of the system, i.e., switching between Optical Character
Recognition (OCR), currency recognition, object detection,
navigation and other available functions.
Social networking functionality was highlighted as a po-
tentially important feature allowing other users/volunteers con-
tribute to up-to-date and high quality guidance (for instance,
temporary road works) with exact location and description.
Navigating around such obstacles can be challenging for
computer vision-based tools and may decrease user’s trust in
the tool. Integration of additional information from other users,
governmental organizations and volunteers could be of high
value for the visually impaired people.
Discussions on the key features of the proposed solution
revealed a gap between the functionality of the existing tools,
which are often focused on advanced navigation and scene
description features, and actual user needs. For example, the
end-users highlighted that a simplistic solution for navigating
is lacking. Major focus was put on an easy-to-use and mini-
malistic tool, which can be trusted. Reliable object detection,
direction and approximate distance to an obstacle were high-
lighted as the main requirements. To our surprise, only 5-8
distinctive object classes (for instance, buss stop, pedestrian
crossing, doors, stairs, etc.) were of major importance while
navigating. Additional information, such as type of a passing
car, blossoming ﬂowers in the park or a person riding a bicycle
was considered as overwhelming and distractive.
Computer vision technology is under a rapid development
and during the last years made a major breakthrough in terms
of performance and efﬁciency. Deep learning neural networks,
which are de facto standard in modern image/video processing
in many real-world problems allow to achieve accuracies,
similar to that of human decision (e.g., face recognition [13],
82
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-688-0
eTELEMED 2019 : The Eleventh International Conference on eHealth, Telemedicine, and Social Medicine

object detection [10], among others). These models are now
well supported by software libraries (e.g., [16]) and dedicated
processing hardware is built-in even in relatively low-power
computational devices, such as smartphones. These devices
are also equipped with 4G Internet connectivity and sufﬁcient
computational power to perform partial or, in certain cases,
even full visual data processing locally. The maturity of
the aforementioned technologies suggest that our proposed
computational aid for visually impaired persons may be highly
feasible.
The cornerstone of this project is the interface between the
visually impaired person and the assistive technology. We are
proposing a combination of audio and tactile feedback, which
may improve the interaction between the assistive technology
and the user. Similar interfaces were suggested by research
communities earlier [4][5], however, existing knowledge on
user preferences and evaluation of various options of tactile
feedback (types of actuators, placement on the body, frequen-
cies, strength, etc.) is limited.
Appearance, usability and cost of the end product are
of major importance to the users. The proposed assistive
technology should supplement the tool that visually impaired
persons used for decades - the white cane. Using a white
cane requires little to no training, has very low costs and is
relatively reliable. It provides information on the surrounding
objects 1-2 meters in front of the user. While the cane can
be used to detect nearby obstacles, electronic travel aid could
be beneﬁcial for longer range (2-10m) route planning and
object detection. However, it should integrate seamlessly into
the existing navigation practices of visually impaired people.
IV.
DISCUSSION
This paper outlined a high-level architecture of a computer
vision-based system, which may help to partially compen-
sate impaired or lost human vision. The main advantages of
therein suggested system are: ability to use efﬁcient (but still
computationally intensive) modern computer vision algorithms
via the Internet connection and present the output of the
video processing to the user through a combined audio/tactile
interface.
Similar functionality has been previously addressed by
research communities [2]-[5] and industry [17][18]. The main
difference between the aforementioned commercial solutions
and the proposed system is the ability to utilize external re-
sources for computationally intensive image and video process-
ing tasks. While the availability of the computational power
could be seen as the main advantage, it comes with high cost
- dependency on a well-functioning mobile broadband. The
development of the high-speed mobile networks (4G and 5G
in the nearest future) is likely to make this limitation obsolete,
especially in densely populated areas containing many hazards
for the blind. Moreover, increasing computational power of
smartphones may also allow to perform more advanced image
and video processing locally. Although not always technically
feasible, local processing is especially important in low con-
nectivity areas to ensure at least partial functionality of the
system. Combined audio/tactile interface may also be more
convenient for the users, allowing to provide feedback in
a more efﬁcient way than using tactile or audio interfaces
separately.
Both open source (e.g., [16]) and state-of-the-art commer-
cial computer vision software libraries (e.g., [19]) may be
applied implementing the suggested architecture. Moreover,
a detailed speciﬁcation of the suggested hardware compo-
nents, following open hardware approach (providing an open
repository containing a detailed list of electronic components,
schematics, and 3D models of mechanical parts) could provide
a solid basis for semi-standard reference platform for the
researchers in the ﬁeld.
It is important to emphasize that the suggested system does
not aim to replace the main travelling aid of visually impaired
people - the white cane. Instead, this computer vision tool
aims to enhance and push the perception of the surrounding
environment boundary from 1-2 meters (achieved by using the
white cane) to 2-10 meters. It may improve route planning and
identiﬁcation of objects of interest (for instance, doors, stairs,
elevators, bus stops, etc.). This functionality corresponds with
the main requirement for electronic travelling aids highlighted
by the end-users - direction and distance estimation to a
selected object. The proposed hardware architecture can be
used as a basis for various computer vision-based software
modules, aiming to assist visually impaired users in daily activ-
ities (e.g., outdoor/indoor navigation, object/face recognition,
obstacle detection, etc.).
The proposed architecture is based on several assumptions,
which may be characterized as limitations of the system. For
instance, technical feasibility of the solution is dependent on
the availability of high bandwidth Internet connection ensuring
access to high-power data processing components. Insufﬁcient
bandwidth may result in latency, which may not be tolerated
by the users.
Economical feasibility of the proposed solution may also be
questioned. Advanced technology (high-end smartphone, depth
camera, tactile feedback device, bone conduction headphones)
is needed to ensure reliable functioning of the system. A
combination of such components may be perceived as costly
by the end-users. More research is needed to demonstrate the
cost-beneﬁt analysis of the system in real-world scenarios.
ACKNOWLEDGMENT
This research is/was funded by the European Regional De-
velopment Fund according to the supported activity Research
Projects Implemented by World-class Researcher Groups under
Measure No. 01.2.2-LMT-K-718.
REFERENCES
[1]
R. R. A. Bourne et al., “Magnitude, temporal trends, and projections
of the global prevalence of blindness and distance and near vision
impairment: a systematic review and meta-analysis,” Lancet Global
Health, Vol. 5(9), pp. 888-897, 2017.
[2]
S. Caraiman et al., “A. Computer Vision for the Visually Impaired: the
Sound of Vision System,” IEEE International Conference on Computer
Vision Workshops, pp. 1480-1489, 2017.
[3]
A. Csap´o, G. Wers´enyi, H. Nagy, and T. A. Stockman, “A survey
of assistive technologies and applications for blind users on mobile
platforms: a review and foundation for research,” Journal on Multimodal
User Interfaces, Vol. 9, issue 4, pp. 275-286, 2015.
[4]
M. Poggi and S. Mattoccia, “A wearable mobility aid for the visually
impaired based on embedded 3d vision and deep learning,” Proceedings
of IEEE Symposium on Compututers and Communication, pp. 208-213,
2016.
[5]
P. A. Zientara at al., “Third Eye: A shopping assistant for the visually
impaired,” Computer, Vol. 50, Issue 2, pp. 16-24, 2017.
[6]
M. Owayjan, A. Hayek, H. Nassrallah, and M. Eldor, “Smart Assis-
tive Navigation System for Blind and Visually Impaired Individuals,”
2015 International Conference on Advances in Biomedical Engineering
(ICABME), pp. 162-165, 2015.
83
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-688-0
eTELEMED 2019 : The Eleventh International Conference on eHealth, Telemedicine, and Social Medicine

[7]
L. D. Dunai, I. L. Lengua, I. Tortajada, and F. B. Simon, “Obstacle
detectors for visually impaired people,” 2014 International Conference
on Optimization of Electrical and Electronic Equipment (OPTIM), pp.
809-816, 2014.
[8]
F. Kensing, J. Simonsen, and K. Bødker, “MUST: A Method for
Participatory Design,” Human Computer Interaction, Vol. 13, Issue 2,
pp. 167-198, Jun. 1998.
[9]
J. M. Carroll and M. B. Rosson, “Participatory design in community
informatics,” Design Studies, Vol. 28, Issue 3, pp. 243-261, May 2007.
[10]
Sh. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards
real-time object detection with region proposal networks,” Advances in
Neural Information Processing Systems, pp. 91-99, 2015.
[11]
Ch. Liu, J. Mao, F. Sha, and A. Yuille, “Attention Correctness in Neural
Image Captioning,” Proceedings of the Thirty-First AAAI Conference
on Artiﬁcial Intelligence, pp. 4176-4182, 2017.
[12]
E. Ohn-Bar, K. Kitani, and Ch. Asakawa, “Personalized Dynamics Mod-
els for Adaptive Assistive Navigation Interfaces,” arXiv:1804.04118,
[cs.LG], 2018.
[13]
B. Amos, B. Ludwiczuk, and M. Satyanarayanan, “Openface: A
general-purpose face recognition library with mobile applications,”
Technical report, CMU-CS-16-118, CMU School of Computer Science,
2016.
[14]
I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab,
“Deeper depth prediction with fully convolutional residual networks,”
Fourth International Conference on 3D Vision (3DV), pp. 239-248,
2016.
[15]
G. Varol, I. Laptev, and C. Schmid, “Long-Term Temporal Convolutions
for Action Recognition,” IEEE Transactions on Pattern Analysis and
Machine Intelligence. Vol. 40, Issue 6, pp. 1510-1517, 2016.
[16]
M. Abadi et al., “TensorFlow: Large-Scale Machine Learning on
Heterogeneous Distributed Systems,” arXiv:1603.04467, 2016.
[17]
Orcam. MyEye 2.0. [Online]. Available from https://www.orcam.com
[Retrieved: January, 2019].
[18]
Horus. Horus system. [Online]. Available from http://www.horus.tech
[Retrieved: January, 2019].
[19]
Neurotechnology,
VeriLook
SDK.
[Online].
Available
from
https://www.neurotechnology.com/verilook.html
[Retrieved:
January,
2019].
84
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-688-0
eTELEMED 2019 : The Eleventh International Conference on eHealth, Telemedicine, and Social Medicine

