Budget-aware Static Scheduling
of Stochastic Workﬂows with DIET
Yves Caniou∗, Eddy Caron∗, Aurélie Kong Win Chang∗, Yves Robert∗†
∗ENS Lyon, France
†University of Tennessee, Knoxville, TN, USA
emails: {yves.caniou
 eddy.caron
 aurelie.kong-win-chang
yves.robert}@ens-lyon.fr
Abstract—Previous work has introduced a Cloud platform
model and budget-aware static algorithms to schedule stochastic
workﬂows on such platforms. In this paper, we compare the
performance of these algorithms obtained via simulation and via
execution on an actual platform, Grid’5000. We focus on DIET, a
widely used workﬂow engine, and detail the extensions that were
implemented to conduct the comparison. We also detail additional
code that we made available in order to automate and to ease the
reproducibility of such experiments.
Keywords – DIET; static workﬂow scheduling; Cloud platform.
I. INTRODUCTION
Public Cloud has emerged as an interesting tool for scientists,
offering an infrastructure adaptable on demand, with a vari-
ety of available options and performances. Multiple workﬂow
engines for Cloud platforms have been designed, with more
and more functionalities [1] - [2]. These workﬂow engines
aim at helping users to pick the most appropriate resources for
their intended work, in terms of the number and characteristics
of the Virtual Machines (VMs) selected for execution. The
main objective is to enable easy-to-produce applications while
guaranteeing that, given a constrained budget, rented resources
are used up to the maximum of their capacity.
In [3], we described a Cloud platform model, and we de-
tailed and compared several static budget-aware scheduling
algorithms in an extensive simulation campaign. One major
objective of this work is to further validate the conclusions of
the simulation study by conducting real-life experiments with
the same scientiﬁc workﬂows on the French national validation
platform Grid’5000 [4] and assess the accuracy of simulation
results on a real-life platform. Additional contributions are the
evolution of DIET (Distributed Interactive Engineering Tool-
box) to handle static scheduling, and a tool that automatically
generates DIET code to execute the target workﬂows.
The rest of the paper is organized as follows. We ﬁrst study
related work in Section II. Then, in Section III, we introduce
the DIET middleware and describe the new DIET functionali-
ties. In Section IV, we overview the budget-aware algorithms
that we aim at comparing. Section V details the experimental
framework. Results are reported in Section VI, with a com-
parison analysis between real executions on Grid’5000 and the
corresponding simulations.
II. RELATED WORK
A. Workﬂow engines
Many scientiﬁc applications from various disciplines are
structured as workﬂows [5]. Informally, a workﬂow can be seen
as the composition of a set of basic operations that have to be
performed on a given input data set to produce the expected
scientiﬁc result. For a long time, the development of complex
middleware with workﬂow engines [2] [6] [7] has automated
workﬂow management. For example, the Pegasus Workﬂow
Management System [2] maps workﬂows on resources until
a given horizon beyond which it considers pre-scheduling is
inefﬁcient, and allows its users to plug their own scheduling
algorithms if needed. Steep [8] comes along its own way to
schedule workﬂows, submitting complete process chains to its
remote agents, and supports cyclic workﬂows graphs. Apache
Airﬂow [9] is more oriented on accessibility to most users,
hence its focus on the user interface and the use of Python
to describe workﬂows or interact with the engine. In [10], the
authors summarize the key features of four production-ready
WMSs: Pegasus, Makeﬂow, Apache Airﬂow, and Pachyderm.
For this work, we focus on DIET [11] because of its practicality,
and the possibility to extend it with additional modules.
Infrastructure as a Service (IaaS) Clouds raised a lot of
interest recently thanks to an elastic resource allocation and
pay-as-you-go billing model. A Cloud user can adapt the execu-
tion environment to the needs of his application on a virtually
inﬁnite supply of resources. While the elasticity provided by
IaaS Clouds gives way to more dynamic application models,
it also raises new issues from a scheduling point of view. An
execution now corresponds to a certain budget, which imposes
some constraints on the scheduling process. In [12], the au-
thors propose a performance-feedback autoscaler that is budget-
aware: using Apache Airﬂow, they tackle the same scheduling
problem as in this paper, but they focus on the auto-scaling
problem (allocating and de-allocating resources on the ﬂy).
B. Budget aware static algorithms
Scheduling scientiﬁc workﬂows in cloud is a well-studied
domain [13] [14]. To the best of our knowledge, the closest
papers to the budget-aware algorithms with stochastic execution
times that we designed in [3] and compare in this paper are [15]
and [16], which both propose workﬂow scheduling algorithms
(Budget Distribution with Trickling – BDT in [15], Critical
28
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-887-7
ADVCOMP 2021 : The Fifteenth International Conference on Advanced Engineering Computing and Applications in Sciences

Greedy – CG/CG+ in [16]) under budget constraints, but with
a simpliﬁed platform model. As in our previous work [17], we
have extended BDT and CG/CG+ to enable a fair comparison
with our algorithms. More recent scheduling algorithms exist,
but address different aspects of the problem. For example, [18]
aims at simultaneously minimizing the cost and makespan of
workﬂow executions, but they use a simpliﬁed platform model
without Cloud storage. Another study [19] uses a platform
closer to ours, but with a completely different objective, namely
the minimization of data transfers (which should eventually
reduce both makespan and cost too). Finally, the workﬂows
considered in [20] present an uncertainty in task durations,
but the authors schedule multiple workﬂows at the same time
instead of optimizing for a single workﬂow.
III. FRAMEWORK
Scientiﬁc workﬂows are represented with a DAG (Directed
Acyclic Graph) G = (V, E), where V is the set of tasks
to schedule and E is the set of dependencies between tasks.
In this model, a dependency corresponds to a data transfer
between two tasks. Workﬂows are scheduled on an heteroge-
neous platform representing an IaaS Cloud, with a tariﬁcation
depending on the performances of the used machines, on the
amount of data transferred to and out of the Cloud, and on the
amount of time during which each machine has been used (see
Section IV-B for cost instances).
A. DIET workﬂow engine
DIET is a well established workﬂow engine [1] [11]. A DIET
platform (Figure 1) consists of a hierarchy of agents [Master
Agents MA and Local Agents LA] scheduling the requests
addressed by a client and sending them to the appropriate
servers, the Servers Deamon (SeD). DIET users, following the
GridRPC paradigm, usually submit individual tasks, but it is
possible to submit a whole workﬂow.
In this case, the client sends the XML ﬁle which describes
its structure to a special agent, called a MADAG, which con-
sequently manages task dependencies and handles ready tasks
submissions to the related MA. The MA dynamically deter-
mines which server will execute a request. A server ﬁrst looks
for the ﬁles needed for the task execution, downloads them if
they are not already on the server, and then executes the request.
B. Extending the MADAG for static scheduling
DIET was able to schedule workﬂows dynamically. We im-
proved the MADAG to apply a static schedule given by a user.
From the user point of view, the client sends to the MADAG the
usual ﬁles needed for the execution of the workﬂow, plus ﬁles
describing the desired schedule:
• The Workﬂow Description File (WDF): the XML ﬁle
describing the workﬂow as in any DIET execution of
workﬂow,
• The Desired Schedule File (DSF): a ﬁle giving the desired
schedule. Each line gives the machine attributed to a task,
under the formulation <task> <server>. The order of the
tasks gives their priority, the ﬁrst ones having the highest
priority. The static schedule is also used to write the
code of the target servers, hence this ﬁle has to be made
available before launching execution,
• The Mapping File (MF): a ﬁle giving the equivalence
between the name of the platform servers and their coun-
terpart from the DSF. Each line gives a machine-server
equivalence, under the formulation <machine> <server>.
DIET will then follow the given schedule.
On a more technical level, it is not possible to specify in the
MADAG which server will receive a given task: the MADAG
leaves the duty of selecting a server to its MA, only sending it
the name of the services ready to be executed. To alleviate this
limitation, we exploit the fact that the MA uses the name of the
service to ﬁnd which server can run it. In order to let the user
to be able to change the task-server attribution according to the
information gathered by the DIET agents, we choose to have the
servers declare their services twice. The ﬁrst one is a regular
generic declaration, with a name identical to the one given in
the XML ﬁle describing the workﬂow, and the same for all the
servers able to run it. This allows the MADAG to get the list of
all the servers available to run a given service corresponding to
a task;The second one is speciﬁc to the new functionality and
has a name composed of the concatenation between the name
of the service as described in the XML ﬁle and the name of the
server. This local name is the one which will be used to send
the request once the server is chosen.
C. Experiment-oriented tools
Given the large number of runs needed by the experiments,
we have developed a couple of tools to automate the creation
of the workﬂows meant to be executed on Grid’5000. We ﬁrst
use a home-made simulator [21] to generate the schedules with
the selected algorithms. We then generate the corresponding
DIET workﬂows needed for our experiments with a home-made
workﬂow generator, Ogma [21].
Our simulator is based on simDAG [22]. It generates both
the static schedules given to DIET and the simulated results.
Basically, it needs a description of the platform, the ﬁle in DAX
format [23] describing the workﬂow, and the parameters of
the scheduling problem (budget, algorithms, etc.). It creates a
schedule, writes the mapping <task, VM> into a ﬁle intended
for Ogma, in the order of their attribution, as determined by
the chosen algorithm. Then, it simulates with simDAG the
execution of the given workﬂow on the given platform, using
the calculated mapping. Finally, it calculates and writes in a ﬁle
the resulting makespan, cost, and various other metrics.
Once the simulations are done and the schedules calculated,
we used Ogma to generate the elements needed for the exper-
iments on Grid’5000. Concretely, Ogma uses the given DAX
ﬁle describing the target workﬂow, along with information on
the focused platform and simulations, and the static schedule, to
write all ﬁles: the source ﬁles for the servers and the client, the
conﬁguration ﬁles needed for every DIET entities, placeholder
ﬁles for the workﬂow, the DSF, the MF and the WDF.
We point out that, as DAX ﬁles do not give any details about
the real content of the tasks or the ﬁles they describe, Ogma
29
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-887-7
ADVCOMP 2021 : The Fifteenth International Conference on Advanced Engineering Computing and Applications in Sciences

	  (a) Architecture 1: Complete schedule from the
Client.
	  (b) Architecture 2: Schedule is handled by the
MADAG
Figure 1: Two different architectures of the DIET workﬂow engine.
only creates placeholder ﬁles and tasks. If a user wants to use
Ogma as a tool to generate the raw structure of the workﬂow,
they will have to replace these placeholders with the actual code
for the tasks.
IV. BUDGET-AWARE SCHEDULING ALGORITHMS
Details on this section can be found in [3].
A. Workﬂow model
A workﬂow is represented with a DAG of stochastic tasks.
Tasks are not preemptive and must be executed on a single pro-
cessor. In our model, we only know an estimation of the number
of instructions for each task. For lack of knowledge about the
origin of time variations, we assume that all the parameters
which determine the number of instructions forming a task are
independent. This resulting number is the weight wi of task Ti
and follows a truncated Normal law with mean wi and standard
deviation σiwi. Finally, to each dependency (Ti, Tj) ∈ E is
associated an amount of data of size size(dTi,Tj).
B. Cloud platform model
There is only one datacenter, used by all processing units.
It is the common crossing point for all the data exchanges
between processing units: these units do not interact directly.
The processing units are Virtual Machines (VMs). They can
be classiﬁed in different categories characterized by a set of
parameters ﬁxed by the provider. Some providers offer param-
eters of their own, such as the number of forwarding rules. We
only retain parameters common to the three providers Google,
Amazon and OVH: a VM of category k has nk processors, one
processor being able to process one task at a time; a VM has
also a speed sk corresponding to the number of instructions
that it can process per time unit, a cost per time-unit ch,k and an
initial cost cini,k; all these VMs take an initial, and uncharged,
amount of time tboot to boot before being ready to process
tasks. Already integrated in the schedule computing process,
this starting time is thus not counted in the cost related to the
use of the VM. Without loss of generality (even if the VM is
paid for each used second), categories are sorted according to
hourly costs, so that ch,1 ≤ ch,2 · · · ≤ ch,nk. We expect speeds
to follow the same order, but do not make such an assumption.
Altogether, the platform consists of a set of n VMs of k
possible categories. Some simplifying assumptions make the
model tractable while staying realistic: (i) we assume that the
bandwidth is the same for every VM, in both directions, and
does not change throughout execution; (ii) a VM is able to store
enough data for all the tasks assigned to it: in other words,
a VM will not have any memory/space overﬂow problem, so
that every increase of the total makespan will be because of
the stochastic aspect of the task weights; (iii) initialization
duration is the same for every VM; (iv) data transfers take place
independently of computations, hence do not have any impact
on processor speeds to execute tasks; (v) a VM executes at most
one task at every time-step, but this task can be parallel and
enroll many computing resources (hence the execution time of
the task strongly depends upon the VM type).
We chose an “on-demand” provisioning system: it is possible
to deploy a new VM during the workﬂow execution. Hence,
VMs may have different startup times. A VM v is started at
time Hstart,v and does not stop until all the data created by its
last computed task is transferred to the Cloud storage, at time
Hend,v.
C. Scheduling costs and objective
Tasks are mapped to VMs and locally executed in the order
given by the scheduling algorithm, such as those described in
Section IV-D. Given a VM v, a task is launched as soon as (i)
the VM is idle; (ii) all its predecessor tasks have been executed,
and (iii) the output ﬁles of those predecessors mapped onto
others VMs have been transferred to v via the Cloud storage.
a) Costs: The cost model is meant to represent generic
features out of the existing offers from Cloud providers
(Google, Amazon, OVH). The total cost of the whole workﬂow
execution is the sum of the costs due to the use of the VMs and
of the cost due to the use of the Cloud storage CCS. The cost Cv
of the use of a VM v of category kv is calculated as follows:
Cv = (Hend,v − Hstart,v) × ch,kv + cini,kv
(1)
There is a startup cost cini,kv in Equation (1), and a term ch,kv
proportional to usage duration Hend,v − Hstart,v.
The cost for the Cloud storage is based on a cost per time-
unit ch,CS, to which we add a transfer cost. This transfer cost is
30
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-887-7
ADVCOMP 2021 : The Fifteenth International Conference on Advanced Engineering Computing and Applications in Sciences

computed with the amount of data transferred from the external
world to the Cloud storage (size(din,CS)), and from the Cloud
storage to the outside world (size(dCS,out)). In other words,
din,CS corresponds to data that are input to entry tasks in the
workﬂow, and dCS,out to data that are output from exit tasks.
Letting Hstart,first be the moment when we book the ﬁrst VM
and Hend,last be the moment when the data of the last processed
task have entirely been sent to the Cloud storage, we deﬁne
Husage = Hend,last − Hstart,first as the total platform usage
during the whole execution. We have:
CCS =
(size(din,CS) + size(dCS,out)) × ctsf
+Husage × ch,CS
(2)
Altogether, the total cost is Cwf = P
v∈RV M Cv +CCS, where
RV M is the set of booked VMs during the execution.
b) Objective: Given a budget B and a platform P, the
objective is to minimize total execution time while respecting
the budget.
D. The HEFTBUDG scheduling algorithm
HEFTBUDG (Algorithm 1) is a budget-aware exten-
sion of the Heterogeneous Earliest Finish Time algorithm
(HEFT) [24]. This extension accounts both for task stochas-
ticity and budget constraints, while aiming at makespan mini-
mization. Coping with task stochasticity is achieved by adding
a certain quantity to the average task weight so that the risk
of under-estimating its execution time is reasonably low, while
retaining an accurate value for most executions. We use a con-
servative value for the weight of a task T, namely wT + σT wT .
Algorithm 1 HEFTBUDG.
1: function HEFTBUDG(wf, Bcalc, P)
2:
s ← calcMeanSpeed(P)
3:
bw ← getBw(P)
4:
budgPTsk ← divBudget(wf, Bcalc, s, bw)
5:
LISTT ← getTasksSortedByRanks(wf, s, bw, lat)
6:
pot, newPot ← 0
7:
for each T of LISTT do
8:
host ← getBestHost(T, budgPTsk[T], P, newPot)
9:
pot ← newPot
10:
sched[T] ← host
11:
schedule(T, host)
12:
update(UsedVM)
13:
end for
14:
return LISTT, sched
15: end function
In the beginning HEFTBUDG calls divBudget() (Algo-
rithm 2): given the workﬂow wf, we ﬁrst get the maximum
of total work (getMaxTotalWork(wf)) and the total amount
of data transfers (getMaxTotalTransfData (wf)) required to
execute the workﬂow, and we reserve a fraction of the budget
to cover the cost of the Cloud storage and VM initialization;
then we divide what remains, Bcalc, into the workﬂow tasks.
To estimate the fraction of budget to be reserved, assuming that
Bini denotes the initial budget:
• For the cost of the Cloud storage, we need to estimate
the duration Husage =
Hend,last
−
Hstart,first of
the whole execution (see Equation (2)). To this purpose,
we consider an execution on a single VM of the ﬁrst
(cheapest) category, compute the total duration Wmax =
P
T ∈wf(wT + σT ) and let
Husage = Wmax
s1
+ size(din,CS) + size(dCS,out)
bw
(3)
Altogether, we pay the cost of input/output data several
times: with factor ctsf for the outside world, with factor
ch,CS for the usage of the Cloud storage (Equation (3)),
and with factor ch,1 during the transfer of data to and
from the unique VM. However, there is no communication
internal to the workﬂow, since we use a single VM.
• For the initialization of the VMs, we assume a different
VM of the ﬁrst category per task, hence we budget the
amount n × cini,1.
Combining these two choices is conservative: on the one hand,
we consider a sequential execution, but account only for input
and output data with the external world, eliminating all internal
transfers during the execution; on the other hand, we reserve
as many VMs as tasks, ready to pay the price for parallelism,
at the risk of spending time and money due to data transfers
during the execution. Altogether, we reserve the corresponding
amount of budget and are left with Bcalc for the tasks.
This reduced budget Bcalc is shared among tasks in a propor-
tional way: we estimate how much time tcalc,T is required to
execute each task T, transfer times included, and allocate the
corresponding part of the budget in proportion to the whole for
execution of the entire workﬂow tcalc,wf:
budgPTsk[T] = tcalc,T
tcalc,wf
× Bcalc
(4)
In Equation (4), we use tcalc,T = wT +σT
s
+ size(dpred,T )
bw
, where
size(dpred,T ) =
X
(T ′,T )∈E
size(dT ′,T )
(5)
is the volume of input data of T from all its predecessors.
Similarly, we use tcalc,wf =
Wmax
s
+ dmax
bw , where dmax =
P
(Ti,Tj)∈E size(dTi,Tj) is the total volume of data within the
workﬂow. Computed weights (wT + σT and Wmax) are di-
vided by the mean speed s of VM categories, while data sizes
(size(dpred,T ) and dmax) are divided by the bandwidth bw
between VMs and the Cloud storage. Again, it is conservative to
assume that all data will be transferred, because some of them
will be stored in-place inside VMs, so there is here another
source of over-estimation of the cost. On the contrary, using the
average speed s in the estimation of the computing time may
lead to an under-estimation of the cost when cheaper/slower
VMs are selected.
This subdivided budget is then used to choose the best VM
to host each ready task by calling getTaskssortedbyranks(): the
best host for a task T will be the one providing the best Earliest
Finish Time (EFT) for T, among those respecting the amount
of budget BT allocated to T (considering all already used VMs
plus one fresh VM of each category).
31
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-887-7
ADVCOMP 2021 : The Fifteenth International Conference on Advanced Engineering Computing and Applications in Sciences

Algorithm 2 Dividing the budget onto tasks.
1: function DIVBUDGET(wf, Bcalc, s, bw)
2:
Wmax ← getMaxTotalWork(wf)
3:
dmax ← getMaxTotalTransfData (wf)
4:
for each T of wf do
5:
budgPTsk[T] ← Bcalc ×
wT +σT wT
s
+
size(dpred,T )
bw
Wmax
s
+ dmax
bw
6:
end for
7:
return budgPTsk
8: end function
HEFTBUDG reclaims any unused fraction of the budget
consumed when assigning former tasks: this is the role of the
variable pot, which records any leftover budget in previous
assignments. For some tasks, getBestHost() do not return the
host with the smallest EFT, but instead the host with the
smallest EFT among those that respect the allotted budget. The
complexity of HEFTBUDG is O((n + e)p), where n is the
number of tasks, e is the number of dependence edges, and p
the number of enrolled VMs. This complexity is the same as
for the baseline versions, except that p is not ﬁxed a priori. In
the worst case, p = O(max(n, k)) because for each task we try
all used VMs, whose count is possibly O(n), and k new ones,
one per category.
E. Other scheduling algorithms
[3] details several budget-aware scheduling algorithms:
• MINMINBUDG,
a
budget-aware
extension
of
MINMIN [25], [26], the exact counterpart of HEFTBUDG.
• HEFTBUDG+ and HEFTBUDG+INV, aiming at exploit-
ing the opportunity to re-schedule some tasks onto faster
VMs, thereby spending any budget leftover by the ﬁrst
allocation, at a price of higher complexity. These reﬁned
variants differ by the order in which tasks are considered,
and recompute the schedule after processing each task.
• HEFTBUDGMULT, a trade-off version that reallocates the
leftover budget in a single pass: it ﬁnds makespans slightly
larger than those computed with HEFTBUDG+, but with
a time complexity close to HEFTBUDG.
Two competitors to the new budget-aware algorithms de-
scribed above have also been simulated in [3], for the sake
of comparison. These are Budget Distribution with Trickling
(BDT [15]) and Critical Greedy (CG [16]). Both BDT and CG
schedule deterministic workﬂows, and CG does not take into
account communication costs. In [16], CG also comes with a
reﬁned version CG+. BDT and CG/CG+ have been extended
to ﬁt the model, so as to enforce fair comparisons.
V. EXPERIMENTAL FRAMEWORK
We use the new DIET functionalities to execute the schedul-
ing algorithms (Section IV) on Grid’5000, a French national
testbed supported by a scientiﬁc interest group hosted by Inria
and including CNRS, RENATER and several Universities as
well as other organizations [4]. For each workﬂow execution,
we used 31 homogeneous nodes from the grisou cluster
in Nancy (Intel Xeon E5-2630 v3 CPU 2.4 GHz ×8 cores).
The three different types of VMs (see Table I) needed for the
experiments have been emulated: a VM two times slower than
another one executes twice as many computations. We also run
the corresponding experiments with our simulator.
TABLE I: PARAMETERS OF THE PLATFORM.
VM parameters
Categories
k = 3
Setup cost
cini,=$0.00056
Category 1
Speed s1 = 3.2 Gﬂops
(Slow)
Cost ch,1 = $0.118 per hour
Category 2
Speed s2 = 6.4 Gﬂops
(Medium)
Cost ch,2 = $0.236 per hour
Category 3
Speed s3 = 9.6 Gﬂops
(Fast)
Cost ch,3 = $0.354 per hour
Cloud storage
Cost per month
ch,CS = $0.022 per GB
Data transfer cost
ctsf = $0.055 per GB
Bandwidth
bw
1GBps
We used three types of workﬂows: MONTAGE, LIGO and
CYBERSHAKE, generated with [27]. Given the large makespan
of these workﬂows (e.g., 33 hours for one CYBERSHAKE of
60 tasks scheduled with CG/CG+ [3]), we only ran small
instances with 30 tasks.
A. Simulations
We used the simulator described in Section III-C to obtain
both the static schedules to be evaluated on Grid’5000 and the
simulated results, with characteristics of the platform (band-
width: 1Gb/s, performances of the VMs used as slowest type:
3.2Gf) experimentally measured on grisou.
B. Grid’5000 experiments
One needs to enforce that tasks have similar durations in the
simulations and in the experiments. In the workﬂow description
ﬁle, the amount of work of a task is given in seconds, when
the amount of data transferred between two tasks is given in
bytes. Thus for Ogma, a task consists of three phases: a phase
during which the input ﬁles are read, a phase during which an
amount of double ﬂoating-point additions is calculated based
on the task duration given in the DAX ﬁle and the information
provided about the used platform, and a phase during which the
output ﬁles are written.
SimDAG uses a ﬁxed (but arbitrary) number to calculate a
number of ﬂops based on the task duration in seconds given by
the DAX ﬁle. This arbitrary number did not correspond to the
VMs enrolled on Grid’5000. To preserve a similar ratio between
the time used to move data tdata,T and the time used to execute
a task tcalc_only,T in the real setup and in the simulation, we
modiﬁed the number of operations for each service according
to the observed characteristics of the platform.
In the simulator, the amount of time tcalc_only,T used to
execute the task T, composed of wT operations, on a reference
host of speed shost,simu operations per second, and without
counting data transfers, is equal to tcalc_only,T =
wT
shost,simu .
The amount of time tdata,T to transfer all the data needed to
execute T, for a total size of size(dpred,T ), with a bandwidth
32
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-887-7
ADVCOMP 2021 : The Fifteenth International Conference on Advanced Engineering Computing and Applications in Sciences

bw simu is equal to tdata,T =
size(dpred,T )
bw simu
. Hence the ratio to
preserve is rsimu =
wT
size(dpred,T ) ×
bw simu
shost,simu .
Similarly, in the Grid’5000 execution, the ratio for a band-
width bw g5000 and a reference host speed shost,g5000, with the
same amount of transferred data size(dpred,T ) and a task T
composed of wT,g5000 operations is rg5000 =
wT,g5000
size(dpred,T ) ×
bw g5000
shost,g5000 . Finally, we calculated the number of operations
needed to execute the task T on a reference host of Grid’5000
as wT,g5000 = wT,simu × shost,g5000
shost,simu × bw simu
bw g5000 . We used this
number of operations to generate workﬂow tasks of appropriate
duration.
In the case of LIGO and CYBERSHAKE, instead of running an
equally long workﬂow as described in the DAX ﬁles, we chose
to generate one with the same shape (same tasks, dependencies
between tasks, proportion between data transfers and amount
of work for tasks), but whose malespan is made shorter, with
the application of a ﬁxed coefﬁcient to decrease the time of
execution of the workﬂows. As seen in Section VI, this has
close to no impact on the performance of the algorithms.
As per the execution itself with DIET, we used one VM to
run the client task, the MA, the MADAG agent and omniNames
(the naming service on which DIET relies to operate communi-
cation between its components), and one VM per server.
C. Experimental campaign
We have generated schedules: (i) for three workﬂow
types: CYBERSHAKE, LIGO and MONTAGE; and (ii) for
ten scheduling algorithms: MINMINBUDG, HEFTBUDG,
HEFTBUDGMULT, HEFTBUDG+, HEFTBUDG+INV, BDT,
MINMIN, CG, CG+ and HEFT. We have selected a range of
budget values which have an actual impact on the makespan.
For some of these budget values, a few algorithms are failing to
produce valid schedules. Here we deﬁne a valid schedule as a
schedule which successfully enforces the budget constraint.
We have executed, on Grid’5000 and on the simulator, 30
experiments per combination (budget × algorithm × work-
ﬂow), and collected the makespan and cost of each execution.
The costs are calculated as in [3], with prices adapted to the
performances of the used VMs. The raw data, their treatment,
and the whole experimentation setup, are available in [21].
VI. RESULTS
We ﬁrst focus on the observation of makespan results from
real life experiments compared to their simulation. Even results
that spend more than the allocated budget are considered (in
other words, these results are produced by non-valid schedules).
Then we assess the accuracy of experimental costs conducted
with the MONTAGE workﬂow, with a comparison between real
life experiments and their simulation, and in this case we restrict
to results under the given budget.
The experiments have been carried so that the generated
workﬂows are equivalent to their simulated counterpart, with
a ﬁxed factor as only difference. For the sake of comparison,
and to highlight how similar the obtained results are between
the simulation and the execution on Grid’5000, we scale them
in the ﬁgures: MONTAGE makespans are 4.6 times lower
than their simulation, LIGO ones are 49.06 times lower, and
CYBERSHAKE ones 6.9 times lower.
In Figure 2, we report the makespans obtained for each
type of workﬂow as a function of the available budget. The
ﬁrst column represents the results obtained with the simulator.
The second column represents the results obtained from the
runs with DIET on Grid’5000. In most cases, the hierarchy
of algorithms in the DIET executions is the same as the one
in simulation. CG and CG+ obtain the highest makespans,
and HEFT, BDT and MINMIN the lowest ones. Among the
budget-aware algorithms, MINMINBUDG gets most of the
time the highest makespans, but is inferior to the ones ob-
tained with CG+. HEFTBUDG obtains the second highest
ones. HEFTBUDG+ and HEFTBUDG+INV ﬁnd the schedules
with the lowest makespans. HEFTBUDGMULT schedules have
makespans between HEFTBUDG and HEFTBUDG+ ones.
While the results of simulation and real execution are very
similar for MONTAGE and LIGO, there are some differences
for CYBERSHAKE. We guessed that these differences are due
to ﬁle transfers, and we reran the simulations with an inﬁnite
bandwidth, but this had no impact. Still, overall, there is a pretty
good correspondence between simulations and actual runs.
Next, we focus on MONTAGE workﬂows, and in Figure 3,
we report the costs and the percentage of valid solutions found
for MONTAGE executions. In Figures 3a and 3b, we see the cost
of the execution of MONTAGE workﬂows, both for simulations
and real executions, and they match almost perfectly. With
low budgets, two algorithms achieve very expensive schedules:
BDT and HEFT. They are followed by MINMIN. All the other
budget-aware algorithms, spend twice less budget to make a
schedule. In addition, the higher the budget, the more optimiza-
tion opportunities for budget-aware algorithms. For the highest
budgets used for our experiments, we make the following
observations: (i) HEFTBUDG+ achieves the most expensive
schedules, even higher than the ones from HEFT and BDT
(but recall from Figure 2 that HEFTBUDG+ needed a lower
initial budget than HEFT and BDT to ﬁnd valid makespans);
(ii) HEFTBUDG, HEFTBUDGMULT and HEFTBUDG+INV
have a cost similar to HEFT (but achieve lower makespans);
(iii) Similarly, MINMINBUDG and MINMIN schedules have
similar costs (and lower makespans for MINMINBUDG); and
(iv) The cheapest schedules come from CG and CG/CG+ (but
this is at the cost of a far larger makespan).
Figures 3c and 3d show the percentage of valid schedules
found by each algorithm, from simulations (left) and real-life
experiments (right). Only HEFTBUDG+INV differs on the two
lowest budgets without a 100% valid schedules in real-life. We
know from [3] that this algorithm reﬁnes its schedule, leaving
only a small leftover budget, which explains the difference. On
each graph, we see that for the lowest budget, BDT, HEFT
and MINMIN give no valid schedule, and for the second lowest
budget, BDT and HEFT still do not. All the other algorithms
give 100% valid solutions.
33
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-887-7
ADVCOMP 2021 : The Fifteenth International Conference on Advanced Engineering Computing and Applications in Sciences

G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
100
200
300
400
0.02
0.03
0.04
0.05
Initial Budget
Makespan (sec.)
Algorithm
G
G
BDT
CG
CG+
HEFT
HEFTBudg
HEFTBudg+
HEFTBudg+Inv
HEFTBudgMult
Min−Min
Min−MinBudg
(a) MONTAGE, simulation
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
100
200
300
400
0.02
0.03
0.04
0.05
Initial Budget
Makespan (sec.)
Algorithm
G
G
BDT
CG
CG+
HEFT
HEFTBudg
HEFTBudg+
HEFTBudg+Inv
HEFTBudgMult
Min−Min
Min−MinBudg
(b) MONTAGE, Grid’5000
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0
2500
5000
7500
10000
0.30
0.35
0.40
0.45
Initial Budget
Makespan (sec.)
Algorithm
G
G
BDT
CG
CG+
HEFT
HEFTBudg
HEFTBudg+
HEFTBudg+Inv
HEFTBudgMult
Min−Min
Min−MinBudg
(c) LIGO, simulation
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0
2500
5000
7500
10000
0.30
0.35
0.40
0.45
Initial Budget
Makespan (sec.)
Algorithm
G
G
BDT
CG
CG+
HEFT
HEFTBudg
HEFTBudg+
HEFTBudg+Inv
HEFTBudgMult
Min−Min
Min−MinBudg
(d) LIGO, Grid’5000
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
500
1000
0.05
0.10
0.15
0.20
Initial Budget
Makespan (sec.)
Algorithm
G
G
BDT
CG
CG+
HEFT
HEFTBudg
HEFTBudg+
HEFTBudg+Inv
HEFTBudgMult
Min−Min
Min−MinBudg
(e) CYBERSHAKE, simulation
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
600
800
1000
1200
0.05
0.10
0.15
0.20
Initial Budget
Makespan (sec.)
Algorithm
G
G
BDT
CG
CG+
HEFT
HEFTBudg
HEFTBudg+
HEFTBudg+Inv
HEFTBudgMult
Min−Min
Min−MinBudg
(f) CYBERSHAKE, Grid’5000
Figure 2: Makespans for Montage, Cybershake and Ligo workﬂows of 30 tasks, execution on simulation vs. GRID’5000.
VII. CONCLUSION
In this paper, we have introduced a new scheduling function-
ality for DIET, and provided the user with a set of tools to
implement, and experiment with, static scheduling algorithms
for workﬂows. We then used this new functionality to compare
the executions of ten static algorithms for scientiﬁc applications
from the Pegasus benchmark, using both a simulator and the
testbed Grid’5000. Both types of experiments gave similar
results, validating the results obtained during the simulations
executed in [5] and DIET improvements.
Further work will be devoted to better understand the be-
havior of budget-aware algorithms on larger and more diverse
workﬂows, using the insights gained from both the similarities
and differences found in simulations and actual executions.
REFERENCES
[1] E. Caron and F. Desprez, “DIET: A scalable toolbox to build network
enabled servers on the grid,” International Journal of High Performance
Computing Applications, vol. 20, no. 3, pp. 335–352, 2006.
[2] E. Deelman et al., “Pegasus: a workﬂow management system for science
automation,” Future Generation Computer Systems, vol. 46, pp. 17–35,
2015, funding Acknowledgements: NSF ACI SDCI 0722019, NSF ACI
SI2-SSI 1148515 and NSF OCI-1053575.
[3] Y. Caniou, E. Caron, A. Kong Win Chang, and Y. Robert, “Budget-aware
scheduling algorithms for scientiﬁc workﬂows with stochastic task
weights on infrastructure as a service cloud platforms,” Concurrency
and Computation: Practice and Experience, vol. 33, no. 17, p.
e6065, 2021, [retrieved: August, 2021]. [Online]. Available: https:
//onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6065
[4] F. Cappello et al., “Grid’5000: A large scale, reconﬁgurable, controlable
and monitorable grid platform,” in Proc. 6th IEEE/ACM Int. Workshop on
Grid Computing (Grid’2005).
IEEE Computer Society Press, 2005, see
https://www.grid5000.fr/w/Grid5000:Home.
34
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-887-7
ADVCOMP 2021 : The Fifteenth International Conference on Advanced Engineering Computing and Applications in Sciences

G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.015
0.020
0.025
0.030
0.02
0.03
0.04
0.05
Initial Budget
Cost ($)
Algorithm
G
G
BDT
CG
CG+
HEFT
HEFTBudg
HEFTBudg+
HEFTBudg+Inv
HEFTBudgMult
Min−Min
Min−MinBudg
(a) MONTAGE, simulation
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.01
0.02
0.03
0.02
0.03
0.04
0.05
Initial Budget
Cost ($)
Algorithm
G
G
BDT
CG
CG+
HEFT
HEFTBudg
HEFTBudg+
HEFTBudg+Inv
HEFTBudgMult
Min−Min
Min−MinBudg
(b) MONTAGE, Grid’5000
0
25
50
75
100
0.018
0.025
0.032
0.039
0.046
0.053
Initial Budget
% valid schedules
Algorithm
BDT
CG
CG+
HEFT
HEFTBudg
HEFTBudg+
HEFTBudg+Inv
HEFTBudgMult
Min−Min
Min−MinBudg
(c) MONTAGE, percentage of valid solutions, simula-
tion
0
25
50
75
100
0.018
0.025
0.032
0.039
0.046
0.053
Initial Budget
% valid schedules
Algorithm
BDT
CG
CG+
HEFT
HEFTBudg
HEFTBudg+
HEFTBudg+Inv
HEFTBudgMult
Min−Min
Min−MinBudg
(d)
MONTAGE,
percentage
of
valid
solutions,
Grid’5000
Figure 3: Costs for Montage workﬂows of 30 tasks, execution on grid’5000 vs. simulation, and percentage of valid solutions.
[5] S. Bharathi, A. Chervenak, E. Deelman, G. Mehta, M.-H. Su, and K. Vahi,
“Characterization of scientiﬁc workﬂows,” in SC’08 Workshop: The 3rd
Workshop on Workﬂows in Support of Large-scale Science (WORKS08)
web site.
Austin, TX: ACM/IEEE, Nov. 2008.
[6] E. Caron, F. Desprez, T. Glatard, M. Ketan, J. Montagnat, and
D. Reimert, “Workﬂow-based comparison of two distributed computing
infrastructures,” in Workﬂows in Support of Large-Scale Science
(WORKS10), In Conjunction with Supercomputing 10 (SC’10).
New
Orleans: IEEE, November 14 2010, hal-00677820, [retrieved: August,
2021]. [Online]. Available: https://hal.inria.fr/hal-00677820
[7] P. Couvares, T. Kosar, A. Roy, J. Weber, and K. Wenger, “Workﬂow Man-
agement in Condor,” in Workﬂows for e-Science, I. Taylor, E. Deelman,
D. Gannon, and M. Shields, Eds.
Springer, 2007, pp. 357–375.
[8] M. Krämer, “Capability-based scheduling of scientiﬁc workﬂows in
the cloud,” in DATA, S. Hammoudi, C. Quix, and J. Bernardino, Eds.
SciTePress, 2020, pp. 43–54.
[9] M. Beauchemin. (2014) Apache airﬂow project. [retrieved: August,
2021]. [Online]. Available: https://airﬂow.apache.org/
[10] R. Mitchell et al., “Exploration of workﬂow management systems emerg-
ing features from users perspectives,” in 2019 IEEE International Con-
ference on Big Data (Big Data), 2019, pp. 4537–4544.
[11] E. Caron, “Contribution to the management of large scale platforms:
the DIET experience,” HDR (Habilitation ‘a Diriger les Recherches),
École Normale Supérieure de Lyon, Oct.6 2010, hal number tel-
00629060, [retrieved: August, 2021]. [Online]. Available: https://hal.
inria.fr/tel-00629060
[12] A. Ilyushkin, A. Bauer, A. V. Papadopoulos, E. Deelman, and A. Iosup,
“Performance-feedback autoscaling with budget constraints for cloud-
based workloads of workﬂows,” CoRR, vol. abs/1905.10270, 2019,
[retrieved: August, 2021]. [Online]. Available: http://arxiv.org/abs/1905.
10270
[13] C. Lin and S. Lu, “Scheduling scientiﬁc workﬂows elastically for cloud
computing,” in Cloud Computing (CLOUD), 2011 IEEE International
Conference on.
IEEE, 2011, pp. 746–747.
[14] S. Smanchat and K. Viriyapant, “Taxonomies of workﬂow scheduling
problem and techniques in the cloud,” Future Generation Computer
Systems, vol. 52, pp. 1–12, 2015.
[15] V. Arabnejad, K. Bubendorfer, and B. Ng, “Budget distribution strategies
for scientiﬁc workﬂow scheduling in commercial clouds,” in IEEE 12th
Int. Conf. on e-Science (e-Science), Oct 2016, pp. 137–146.
[16] C. Q. Wu, X. Lin, D. Yu, W. Xu, and L. Li, “End-to-end delay minimiza-
tion for scientiﬁc workﬂows in clouds under budget constraint,” IEEE
Transactions on Cloud Computing, vol. 3, no. 2, pp. 169–181, April 2015.
[17] T. Risset and Y. Robert, “Synthesis of processor arrays for the algebraic
path problem,” Parallel Processing Letters, vol. 1, no. 1, pp. 19–28, Sep.
1991.
[18] X. Zhou, G. Zhang, J. Sun, J. Zhou, T. Wei, and S. Hu, “Minimizing cost
and makespan for workﬂow scheduling in cloud using fuzzy dominance
sort based heft,” Future Generation Computer Systems, vol. 93, pp. 278 –
289, 2019.
[19] S. Makhlouf and B. Yagoubi, “Data-aware scheduling strategy for scien-
tiﬁc workﬂow applications in iaas cloud computing,” Int. J. Interactive
Multimedia and Artiﬁcial Intelligence, vol. InPress, p. 1, 01 2018.
[20] J. Liu et al., “Online multi-workﬂow scheduling under uncertain task
execution time in iaas clouds,” IEEE Trans. Cloud Computing, pp. 1–1,
2019.
[21] A. Kong Win Chang. https://graal.ens-lyon.fr/~achang/Research/. [re-
trieved: August, 2021].
[22] SimDag, “Programming environment for DAG applications,” http:
//simgrid.gforge.inria.fr/simgrid/3.13/doc/group__SD__API.html, 2017,
[retrieved: 2017].
[23] Pegasus
Team,
https://pegasus.isi.edu/documentation/development/
schemas.html, [retrieved:2021].
[24] H. Topcuoglu, S. Hariri, and M. Y. Wu, “Performance-effective and low-
complexity task scheduling for heterogeneous computing,” IEEE Trans.
Parallel Distributed Systems, vol. 13, no. 3, pp. 260–274, 2002.
[25] T. Braun et al., “A comparison of eleven static heuristics for mapping
a class of independent tasks onto heterogeneous distributed computing
systems,” Journal of Parallel and Distributed Computing, vol. 61, no. 6,
pp. 810–837, 2001.
[26] P. Ezzatti, M. Pedemonte, and A. Martín, “An efﬁcient implementation of
the min-min heuristic,” Comput. Oper. Res., vol. 40, no. 11, 2013.
[27] Pegasus Team, “Code for the Pegasus generator,” https://github.com/
pegasus-isi/WorkﬂowGenerator, 2020, [retrieved: August, 2021].
35
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-887-7
ADVCOMP 2021 : The Fifteenth International Conference on Advanced Engineering Computing and Applications in Sciences

