Supporting Active Participation and Situated Use in Mobile Interaction Design 
Susanne Koch Stigberg 
Faculty of Computer Sciences 
Østfold University College 
Halden, Norway 
susanne.k.stigberg@hiof.no 
 
 
Abstract—Most 
mobile 
applications 
are 
designed 
for 
interaction only when a user is standing still and able to pay 
visual and mental attention to the device. New interaction 
techniques are needed to replace this “stop-to-interact” 
paradigm. But how can we design for novel non-idiomatic 
mobile interactions? To inform the mobile interaction design 
process, I propose a design methodology driven by situated use 
and active participation.  I draw upon a case study co-
designing mobile hand gesture interfaces with runners to 
illustrate how the use of participatory design workshop and 
field study in a coherent manner can support the design of 
novel mobile interfaces.  
Keywords-Co-Design; Situated Use; Active Participation; 
Mobile Interface; Hand Gesture Toolkit. 
I. 
 INTRODUCTION  
Mobile technology is changing our daily lives. This 
transformation is less about mobile devices and more about 
the activities we perform using these devices. That is why 
Suchman et al. [1] argue that “the study of how new 
technology emerges should shift from a focus on invention 
to an interest in ongoing practices of assembly, 
demonstration, and performance”. They request a shift from 
an analysis in terms of form and function to a performative 
account. Alike Dourish and Bell [2] call for a deeper 
understanding of how social and cultural practice is carried 
out in and around emerging information technologies. They 
claim that “the vision of ubiquitous computing technologies 
is already fulfilled”, but that we need to pay considerably 
more attention to just what it is being used to do and its 
effects. Here, I am especially interested in how mobile 
technology is used for running and how I can design 
meaningful mobile interfaces that support runners. 
The use of mobile phones for physical activity has 
become popular. There is a vast amount of mobile health 
apps available for Android in the Google Play Store. Several 
research prototypes [3][4] for mobile devices have been 
developed for investigating the effects of mobile technology 
on exercise motivation, obesity prevention, and on users’ 
overall fitness. These sport applications operate mainly as 
digital training diaries collecting performance data on the 
way, using multiple sensors, such as GPS, heart-rate 
monitors, and pedometers. They support four essential 
training functions: performance feedback, navigational 
means, 
competition, 
and 
entertainment 
[4]. 
Before 
exercising a sport setting must be chosen in the application. 
Performance feedback is given visually and as audio [3]–[5] 
directly at the mobile device, forcing the athlete to interact 
with the device during exercising, which disturbs the 
running movement [6].  
In this paper, I do not present a new sports application, 
instead I explore how a participatory and situated approach 
deployed as co-design workshop and field study involving 
runners during design time and use time can help to 
understand and design for better mobile interactions. The 
aim of the study has then been to investigate how can we 
apply an approach driven by situated use and active 
participation to inform the design of mobile non-idiomatic 
interfaces that support the running experience. The paper 
presents a design case of mobile interfaces for runners based 
on eyes-free hand gestures. Moreover, I discuss the concept 
of engaging mobile prototypes to utilize a participatory and 
situated approach in mobile interaction design supporting 
participants’ needs, empowering them as co-creators, and 
studying mobile experiences by integrating their prototypes 
into their practice. After outlining the challenge for current 
mobile interaction design (Section 2), I will describe how I 
responded to that challenge presenting a case study in 
Section 3. I summarize my findings and discuss the 
approach and the concept of engaging prototypes in Section 
4. 
II. 
A CHALLENGE FOR MOBILE INTERACTION DESIGN 
Research in mobile interaction design is concerned with 
the use of technology while being mobile. So far most 
mobile systems request the user to “stop-to-interact”, 
designed for interaction only when a user is standing still, 
paying visual and mental attention to the device [7]. 
Research has shown that the user’s ability to interact with 
technology in motion is decreased even for simple activities 
like walking [8][9]. Lumsden and Brewster [10] requested 
“a paradigm shift in terms of interaction techniques for 
mobile technology” already in 2003. Since then, mobile 
technology has been an important theme in HCI research. 
Liu et al. [11] published a review focusing on keyword 
analysis of CHI publications to understand how the 
landscape of the HCI field has evolved. They list mobile 
phone as the most frequently used keyword between 2004-
2013. Still Marshall and Tennent [7] claim that mobile 
interaction does not exist. They highlight four challenges 
222
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

designing interactions for mobile devices: cognitive load, 
physical constraints, terrain and other people. So how can 
we understand and design for mobile interaction? 
Löwgren [12] argues that mobile interaction design 
research explores interaction possibilities outside the 
established screen idiom, making design methods used for 
common UIs inadequate. New methods and techniques need 
to be explored to understand and design for mobile 
experiences of user-and-technology interplay over time. 
Moen [13] calls for a non-technological, people-centered 
point of view in order to create embodied and engaging 
interaction experiences. Suchman et al. [1] suggest a 
performative 
account 
using 
working 
artifacts 
and 
Shengdong Zhao [14] requests an interaction shift from 
device-centric to human-and-environment-centric. I see two 
major themes from these demands for mobile interaction 
design: users’ active participation and their situated use of 
technology. 
So far, Stigberg [15] describes two common ways of  
including people in mobile interaction design research, as 
informants at design time and evaluators at use time. Work 
by Ruiz [16], Kim [17], Feng [18] and Pakanen [19] invited 
participants in early design time, ideating about future 
interfaces. They stress the importance of participants as 
informants and co-designers and explore the design space 
resulting in design implications, such as end-user inspired 
gesture sets [16][17], device form factors [18] or design 
recommendations for wearables [19]. Even though these 
studies rely on user’s participation during early design time, 
they do not enable participants to experience their imagined 
mobile technologies in situated use. At use time previous 
research projects [6][20]–[25] invited participants to 
evaluate social acceptability [20][23], usability [21][25] or 
overall device performance [6][22][24] of commercial 
mobile technologies [6][20][22] and novel prototypes 
[21][23]–[25]. Participants are seen as evaluators of mobile 
technology most often in a context of use proposed by the 
researchers. The participants are not able to integrate these 
prototypes into their exercising practice to afford embodied 
and engaging interaction experiences. Stigberg’s review 
exposes a gap between, how mobile interaction design 
research is conducted so far, and what is suggested by 
Dourish and Bell [2], Suchman [1] or Zhao [14].   
III. 
SITUATED USE AND ACTIVE PARTICIPATION FOR 
ENGAGING INTERACTION DESIGN. 
There is a well-established body of knowledge on 
situated use to rely on, such as Rogers [26] concepts of a 
wild theory, or Crabtree’s [27] breaching experiments. The 
outcome of field studies in general demonstrate different 
results from those arising out of lab studies [28]. They show 
how 
people 
come 
to 
understand 
and 
appropriate 
technologies on their own terms and for their own situated 
purposes. They afford greater motivation for participating - 
“it is one thing for people to volunteer for a short-term 
experiment and another for them to integrate a novel 
technology into their lives” [26]. And the locus of control 
shifts from the researcher to the participant, making it more 
difficult to study specific effects. Equalizing power relations 
between researchers and participants is one of the major 
principles of participatory design [29]. Co-design or 
participatory design (PD) [30] refers to the activity of 
researchers and people not trained in design working 
together as equals in the design process. Often workshops 
and/or toolkits are deployed to engage participants in the 
design process [31][32].  Examples of PD at use time are 
less explored. Suchman et al. [1] describe three practices of 
design-in-use: incorporating an artifact into an existing 
infrastructure; re-configuration and customization of any 
actual technological solution; as well as co-operative 
prototyping. Dittrich et al. [33] explore how participatory 
design can evolve in the wild. They use the term design-in-
use to “capture practices of interpretation, appropriation, 
assembly, tailoring and further development of computer 
support in what is normally regarded as deployment or use” 
and request new methods for sustainable, distributed co-
constructive design processes. 
Summarizing I see three approaches from previous work 
on situated use and active participation to inform mobile 
interaction design:  
• 
Co-design to engage participants during design 
time. 
• 
Field studies or breaching experiments to 
experience technology in practice 
• 
Design-in-use to appropriate technology at use 
time. 
IV. 
RESPONDING TO THE CHALLENGE:  
AN EARLY EXPERIENCE 
To utilize active participation and situated use, I 
conducted a case study including a co-design workshop 
followed by a field study. The co-design workshop included 
activities for telling, making, and enacting [34]. The 
participants could tell their mobile interaction story, make 
their own mobile hand gesture interface, and enact their story 
using their created artifacts at design time. The artifacts are 
prototypes with working behavior and were used in the 
participants’ everyday workouts during a one-month 
evaluation period following the workshop. The participants 
could revise their prototype at use time as an ongoing 
practice of design-in-use as suggested by Suchman et al. [1]. 
In the following I describe toolkit, workshop and field study 
in more detail. 
A. Mobile Hand Gesture Toolkit 
The mobile hand gesture toolkit [35] consists of 
hardware, software and paper tools (Figure 1). A Myo 
Gesture 
Control 
Armband 
based 
on 
surface 
electromyography [36]  can recognize five different hand 
gestures (Figure 2). The armband communicates via 
Bluetooth with a LG Nexus 5 mobile phone running 
Android. 
223
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

The phone has a set of pre-installed applications: Tasker, 
Secure Settings, AutoInput, MyoTasker Plugin, and MYO 
Phone App are software utility tools needed for creating 
gesture interfaces; Nike Run Club, RunKeeper, Strava, 
Spotify, StopWatch, and BVR are software action tools 
providing desired functionalities for runners. Tasker is an 
application for Android, which performs tasks (sets of 
actions) based on contexts (application, time, date, location, 
event, gesture) in user-defined profiles [37]. Here Tasker 
provides a way of mapping gestures to tasks. I prepared a set 
of possible runners’ tasks in Tasker to provide examples, and 
to reduce the workload for the participants during the 
workshop. The toolkit is a result of designing the design 
space: providing possibilities to add new tasks, new devices, 
and new mappings during the workshop and in future use. 
 
 
Figure 2. Available hand gestures: Fist, Wave In, Wave Out, Fingers 
Spread, and Double Tap (from myo.com) 
B. Co-Design Workshop 
I conducted a co-design workshop with four runners (2 
male, 2 female) in their twenties to thirties recruited from a 
local running club in Sweden. All of them used mobile 
phones for exercising. To inform the participants before the 
workshop, they received an invitation one week before the 
date including three questions related to their activity: what 
digital equipment do you use; which functionalities do you 
use from your digital equipment; do you have any problems 
or limitations with your digital equipment?  
The design workshop was made up off two parts and 
lasted about 1.5 hours. The first part was a paper-based co-
design workshop with three activities using the available 
paper tools to create a design concept. The use of paper tools 
was a conscious design choice to lower the threshold for 
participation and to focus on the participants’ experiences 
and creativity. In the second part of the workshop, I utilized 
the hardware and software tools and the participants created 
their functional prototype. I expected this to be the most 
intense part of the workshop, introducing a new technology, 
the concept of end-user-development using Tasker, and 
helping participants to create their own working interfaces. 
In the first part, we started with a telling activity where 
the participants described a mobile interaction story from 
running using their notes from the invitation. Each 
participant defined one scenario that they would like to work 
on. We continued with a making activity there I presented 
paper cards with pictures of available gestures. Each 
participant could write functionality that should be 
accomplished by the illustrated gesture on the card, e.g. “take 
a picture” written on the fist gesture card. The first part 
ended with an enacting activity there the participants’ were 
asked to demonstrate their designed interface by showing the 
selected gestures and telling what should happen. 
In the second part of the workshop, we utilized the 
hardware and software tools and the participants created their 
functional prototype. After I handed out the hardware, I 
asked the participants to adapt the armband to fit their arm 
using the included plastic brackets and to try out the Myo 
Phone App to practice hand gestures.  Next, I introduced the 
participants to Tasker and I demonstrated how to match 
gestures to tasks by example (Figure 1). For this making 
activity participants could use the prepared tasks or create 
new ones. We concluded the workshop with another 
enacting activity there the participants demonstrated their 
working artifacts. After the workshop, the participants were 
interviewed about their prototype and their experiences from 
the workshop. 
C. Field Study 
To support experiencing this mobile technology in 
practice and explore design-in-use time, I asked the 
participants to use their prototypes in their everyday practice, 
at least three times, during one month. They received a how-
to sheet for using and altering their prototype and I 
encouraged them to adapt their interface, if they were 
unsatisfied. I communicated with the participants through 
phone calls, text messages and Facebook conversations. I 
used text messages to remind the participants to use the 
prototype once a week. The participants were asked to write 
down short experience notes after each workout and send 
them to the researcher. Further the participants were able to 
get technical support throughout the whole time. After one 
month I collected the prototypes and conducted a second 
interview with each participant. 
 
 
 
 
Figure 1. The mobile hand gesture toolkit including (a) Paper tools: invitation and gesture cards, (b) Hardware tools: Myo Gesture Control Armband and 
LG Nexus 5 phone, (c) Participant performing Wave In gesture 
 
 
 
224
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

D. Data Analysis 
During design time, I collected data from: 4 completed 
invitations, 4 sets of completed paper cards, 32 photos taken 
during the workshop, 6 pages of workshop observation 
notes, and 51 min audio recordings from interviews. The 
analysis started after the workshop and has been an on-going 
process throughout the field study. This continuous process 
allowed me to refine the questions and directions of the 
investigation as part of the field study. The collected data 
from the workshop worked as initial exploration of the 
domain. I understood what functionality is important to 
runners during their activity and how they imagine accessing 
that functionality through an eyes-free hand gesture 
interface. During the interviews, I reviewed with participants 
their prototypes. The field study in the wild was a necessity 
to explore participants’ understandings, practices and 
eventual uses of their prototypes. But it was difficult to 
observe when and how participants actually used their 
prototype. The data collection during the field study 
consisted of: 12 experience notes from participants, and 127 
min audio recordings from interviews. All audio recordings 
were transcribed and together with observation notes and 
experience notes were analyzed by me and a second 
researcher using the software TAMS Analyzer. We coded 
the data regarding three themes: participants’ needs for 
alternative mobile interfaces, their means to co-create a 
mobile prototype and their experiences from situated use 
with that prototype. Quotations from the interviews are 
reported in anonymous form using participant A-D.  
E. Findings 
I retrieved three main insights from the case study, (1) 
diverse 
individuals’ 
needs, 
(2) 
challenging 
mobile 
interactions, (3) the necessity of evaluating mobile interfaces 
in situated use, are summarized in the following. (1) When 
designing for mobile interactions, it is important to provide 
flexibility and tailorability for mobile technology both during 
design and use time. All four participants created diverse 
interfaces during the design phase and three of them adapted 
their interface during use. They appreciated the openness of 
the mobile hand gesture toolkit. As participant C expresses: 
“I felt that it gave me a sense of control to design the 
interface according to my own needs. It felt like, that only 
my imagination could set the limits. Nor did I feel that it was 
particularly difficult either.” 
(2) All four participants identified mobile interactions 
using the touch screen as problematic. They agreed that 
minimal hand gesture interactions as probed in this study are 
a better alternative to control the phone while in motion. 
Participant A explains: “It is usually not possible to press on 
the phone screen directly during exercise. Any type of 
gesture control for the mobile is really needed.”  
(3) The probed technology based on electromyography is 
non-optimal for gesture recognition in the wild. The 
participants felt, that their prototype worked much better 
during design time sitting down at the workshop compared to 
use time when they were in motion. Participant D expresses 
her disappointment: “It works well when you sit with the 
phone in front of you and you see what's happening. 
Feedback and technology work well if you sit still and 
concentrate. When training, everything needs to work 
straightaway. It must react directly and it did not.” 
In the following, I present the designed mobile gesture 
interfaces, findings from the workshop at design time and in 
the wild at use time.  
1) Mobile Gesture Interfaces 
Four participants created four different personal 
interfaces. They used expected functionality, such as call 
management, music and media control as well as activity and 
performance tracking. But the combination of functionalities 
into tasks and the mapping from gesture to task was varying 
between participants.  
Participant A created an interface for interval training for 
biking and running. The participant assumed that Fingers 
Spread gesture would be the only accessible gesture when 
holding a bike handle so Fingers Spread gesture starts a 
stopwatch application, sets a new lap, and reads out the 
duration of the last lap. 
Participant B’s interface is designed for trail running. 
Fingers Spread records 30 sec scenic videos using BVR. Fist 
pauses and resumes activity tracking in RunKeeper. Double 
Tap reads out the current activity time, distance and pace 
from RunKeeper. Wave In skips to previous song and Wave 
Out skips to next song from current Spotify playlist if 
playing. 
Participant C designed an interface for handling calls 
while runnig. Fingers Spread accepts incoming calls and 
stops current activity tracking in RunKeeper. Fist ends a call 
and resumes current activity tracking in RunKeeper. Wave In 
decreases the volume, and Wave out increases the volume.  
Participant D plans to use the designed prototype mostly 
as a music player control. Fingers Spread starts the music 
and activity tracking in RunKeeper. Fist pauses music and 
activity tracking in RunKeeper. Double Tap skips to the next 
song. Wave In decreases volume, Wave out increases 
volume. 
2) Design Time 
All four participants provided positive feedback towards 
the organization of the workshop. Participant C tells about 
his experience from the workshop: “The workshop worked 
well. It became clear what we would do. There was no 
question mark. It was fun to attend. An exciting idea was 
presented.” 
I handed out invitations in beforehand to prepare the 
participants for the workshop. Participants experienced the 
invitations as important for the success of the workshop. 
Participant D summarizes it as: “The invitation was easy to 
fill out. Good with some time for consideration. It’s nothing I 
think of all the time, so it was great to think about that before 
the workshop.” 
In the first part of the workshop, we started with a telling 
activity where the participants described a mobile interaction 
story from running and biking using their notes from the 
invitation. Each participant defined one scenario without 
problems. We continued with a making activity there I 
presented paper cards with pictures of available gestures. 
a) Gesture Mappings 
225
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

The participants had varying strategies for mapping 
gestures to tasks. Participant C and D thought that it was 
easy to come up with meanings. Participant D states: “I 
immediately came up with how I wanted to control the 
features. It was easy to write tasks on paper cards. The 
gestures were clear. It was like using sign language.” 
She explained her thoughts about the mapping as 
following: “When you open your hand you start something. 
That is easy. And then you close your hand, and it will stop.” 
Participant C agreed that mappings were intuitive to him: 
“It was quite obvious which task every gesture would have. 
They were easy gestures for simple tasks, making it easy to 
remember.”  
Participant D explained how she connected some 
gestures to previously known interactions: “I thought of the 
Apple's headphones when you switch a song you double tap. 
Easy to remember.”  
Participant B however felt that none of the mappings 
were obvious. She said: “There is no gesture task 
combination that feels completely natural. You just have to 
choose one.”  
Participants B, C and D had mappings for four to five 
gestures. Participant A had a different approach: “I try to use 
as few gestures as possible. Maybe I add more then. It's 
important to start with a little thing to remember and see if it 
works.” He felt limited by his bike handle to which gestures 
he was able to choose from:  “Especially with the bike, then I 
hold the handle bar; I do not want to let go of it to make a 
gesture.”  
b) Making the Prototype 
In the second part of the workshop, we utilized the 
hardware and software tools and the participants created their 
own functional prototype. I introduced the participants to 
Tasker and I demonstrated how to match gestures to tasks by 
example. For this making activity participants could use the 
prepared tasks or create new ones. Participants felt that it 
was important to them to learn to create their own interfaces. 
Participant A states: “It's important to try it out myself and 
do my own thing. It's always easy to listen to someone else 
but then often hard to do it myself.” 
Participant A, B and D felt that it was important to have a 
workshop. Participant D explains: “I received quite a lot of 
help from you to create new tasks, so it was easy. If I did it 
completely by myself, it would have been more difficult.” 
Participant C disagrees: “Would I sit with that by myself 
for a while, maybe I would have learned it too. But it was 
really easy and user-friendly in the workshop.” 
Participants felt a sense of mastering at the end of the 
workshop. They were proud that they managed to create 
their own interface. Participant B says: “I was pleasantly 
surprised that it actually worked, as I had imagined.” 
3) Use Time  
Participant B reports from a 15km trail run: “Then I was 
running in some scenic nature I wanted to record and did the 
double tap gesture to unlock and wave in to record. It 
vibrated twice and it felt that everything was all right.”  
The participants exercised with the armband between 5 to 
8 times each during one month. They reported back to the 
researcher after each use with an experience note via 
Facebook. I conducted final interviews with all participants 
at the end of the use time to discuss their experiences in more 
detail. After testing the technology and creating their 
personal prototype during the workshop participants’ 
expectations were high. Participant A described his 
experience with the prototype during the final interview: “It 
works ok, but exercising is delicate it becomes important that 
it has to work directly. It puts very high demands on 
technology.” 
Participant B and D expressed similar thoughts. Only 
participant C stated that he expected some bugs, and that one 
cannot assume perfection of a prototype. 
a) Breakdowns and Design-in-Use 
As anticipated by the researchers the participants 
experienced issues with the use of the Myo armband in the 
wild. Sathiyanarayanan and Mulling [38] conclude in a 
related project from 2015 that Myo has the potential to be 
used for controlling applications, but needs improvement of 
physical device as well as gesture recognition optimization. 
In this study Myo gesture control armband was the available 
technology to test mobile gesture interfaces and I do not 
focus on its technical issues. Instead I am interested in 
findings on how the participants’ experienced and coped 
with experienced breakdowns and how their prototype 
evolved during use time. I documented two types of 
breakdowns: 
when 
the 
Myo 
armband 
recognized 
unintentional gestures; and when the Myo armband did not 
recognize the performed gestures.  
Unintentional gesture detection was reported both before 
and under the workout for all participants. Participants 
adapted their interfaces and inactivated gestures that were 
frequently faulty. Participant A describes his experience 
while getting ready: “It reacts to movements that you haven’t 
thought of, so I started the time unwillingly, because I tied 
my shoes or pointed at something.” 
Participant D reports similar problems: “I connected the 
Myo to the phone without problems. Then I tied my shoes 
the Myo detected lots of gestures, started Runkeeper, paused, 
resumed, and went really nuts. So I thought next time I get 
ready before I connect to the Myo.” 
Participants reacted on breakdowns by adapting the 
interface as participant B described: “ Then I started running, 
I started Runkeeper by making a fist and started music by 
waving out. This worked fine. But I noticed that the Myo 
armband was vibrating the whole time even though I did not 
do a gesture. Runkeeper was paused and resumed several 
times and my tracking status was read up without me doing 
anything. So after about 500m I stopped and removed Fist 
and Double Tap gestures from Tasker, so that they would not 
trigger all the time. I restarted my run and now it was fine.” 
Participant D changed her interface and removed the 
double tap gesture that did not work as expected: “I used 
Myo on Saturday. Run about 5 km on a hilly track. I 
removed the function to change song (double tap) because it 
swapped the song unintentionally last time.” 
Participant C tried to come up with an explanation for 
these breakdowns: “I think it had problems to disguise my 
226
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

gestures from muscle tension and vibrations caused naturally 
while running.” 
Participants A and C were mostly effected by non-
respondents from the interface.  Participant A reports from 
his first use: “It worked fine to start the time. Looked at the 
phone and the time was set. I thought testing the armband 
when I was almost finished warming up. I did a double tap, 
but nothing happened.” 
Participant A found that gesture detection improved 
when he moistened his arm. After a couple of uses he stated 
that it gets better the more he uses it. Participant C describes 
his problems with the armband: “Everything worked great at 
first, but after a few hundred meters, it stopped working. I 
tried to reset by flexing out the hand as it should be done, 
then it vibrated and worked again as it should.” 
Participant C did not change his interface during use: “I 
found that all the gestures I chose from the beginning were 
easy to remember and logical.” 
While participants B and D removed parts of their 
functionality during use, participant A came up with new 
functionality that he added to his interface: “I just had start 
and stop time. During testing I came across more things I 
wanted to know, such as distance and cadence and so forth. 
So I added start and stop tracking in Strava.” 
V. 
REVISITING THE CHALLENGE  
I am aware that this case study only involved four 
participants. I did not collect enough empirical data to argue 
for 
gesture-based 
interfaces 
as 
alternative 
mobile 
interactions. However, my goal was to explore how situated 
use and active participation can be integrated in mobile 
interaction design to deepen user engagement and to afford 
a better understanding of what mobile interactions are and 
how to design for them. The findings reaffirm that mobile 
interactions are personal and situated, participants’ needs 
are diverse, and their gesture mappings are individual. I 
have observed that participants’ needs can be hidden at 
design time and emerge during use. I utilized flexible 
functional prototypes that participants engaged with both 
during design time and use time. I call them engaging 
mobile prototypes, they are: 
• 
personal: they are created by the participant 
• 
functional: they are experienced in situated use 
• 
changeable: they can be altered during use  
In the following, I discuss how these prototypes can be 
used to support active participation and situated use and 
how this approach informed the design of mobile non-
idiomatic interfaces supporting the running experience. 
A. The Use of Toolkits in Mobile Interaction Design 
There exist a number of making tools available to 
support mobile prototyping for programmers and interaction 
designers in the HCI literature [39]–[41]. Prototyping 
toolkits that enable non-programmers to design and 
experience novel interaction interfaces are not yet 
commonplace. One example is Mogeste [42], a mobile 
phone tool for users to create rapid, in-situ mobile gestures, 
however the tool does not support working interaction 
interfaces, since gestures cannot be coupled to phone 
functionalities. In this case study, I demonstrate how 
commercial mobile and wearable technology can be 
combined to enable non-programmers to be engaged in 
making their own functional prototypes inspired by 
participatory design toolkits [31] for low-fidelity prototypes. 
During the workshop the participants create, experience and 
revise their solution with little help of a researcher. 
Unlike mobile prototypes in previous research, the 
resulting prototypes are personal, created by and for each 
participant. They enable two dimensions of freedom: the 
type of support requested during running (functionality); 
and the mapping of gestures to this functionality. Even 
though the case study only involved four participants, the 
findings indicate that both functionality and gesture 
mappings seem to be personal. Ruiz et al. [16] created a 
gesture set for common mobile phone functionality using a 
consensus of user-defined gesture sets. They too, found 
tasks with poor agreement scores and recommend gesture 
toolkits to allow end-user customization to support different 
gesture mappings.  
However designers need to balance between complexity 
of the toolkit and openness of the design space defined by 
Alan Perlis as the turing tar pit and its inverse [43]. I limited 
the prototype to five possible gestures for one hand, based 
on the available hardware. Participants will design different 
user interfaces having support for more diverse gestures or 
two hands. Future research can explore alternative hardware 
components using the toolkit to allow other gesture sets.  
B. Active Participation in Mobile Interaction Design 
The goal of this study was to engage runners in the 
design and evaluating of a mobile interface based on eyes-
free hand gestures. Previous research [24][44] indicated that 
hand gestures are feasible for running interfaces and I 
explored  what functionality runners would choose, how 
they would access that functionality through an eyes-free 
hand gesture interface, and how they experience such an 
interface integrated into their use practice. I learned that 
participants answered each of these questions differently. 
For two participants the gesture functionality mappings 
were intuitively, the other two just “picked a combination”. 
The co-design workshop enabled all participants to ideate 
and build their personal engaging prototype despite different 
technology literacy. This type of active participation 
supported 
participants’ 
understanding 
of 
technology 
limitations and made them comfortable handling their 
prototypes at use time. “It's important to try it out myself 
and do my own thing” as participant A stated during the 
first 
interview. 
The 
active 
participation 
continued 
throughout use time. The participants reported on several 
occasions that they adapted their interfaces during use and 
explained in detail why and how they did these changes.  
227
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

C. Experiencing Mobile Prototypes in Situated Use 
The mobile interface must be meaningful for the 
runner; otherwise it is a needless gadget. The engaging 
prototypes contain only functionality that matter to the 
participants, spanning participants’ ideas from design 
time to use time. But despite creating personal interfaces at 
design time, situated use cannot be completely anticipated. 
Participants, at use time, discovered mismatches between 
their needs and their prototype, and unexpected technology 
behavior leading to breakdowns [43] and changed requests. 
Two types of breakdowns were reported: when the Myo 
armband recognized unintentional gestures; and when the 
Myo armband did not recognize the performed gestures. 
These breakdowns were discouraging factors during use 
time and participants adapted their prototypes to avoid them. 
Further, one participant uncovered hidden needs during use 
time and added more gestures to the prototype. One problem 
was data collection during use time, since the locus of 
control is shifted from researcher to participant [26]. I 
collected no use data from the prototypes; instead I relied on 
experience notes from the participants and interviews with 
them after the field study, since I was interested how the 
participants experienced their interface integrated into their 
use practice. For future research I consider using tools such 
as AWARE [40] framework, to register additional usage 
data for more insights during use time. 
VI. 
CONCLUSION 
Marshall and Tennent [7] claim that mobile interaction 
does not exist, that we are trapped in a “stop-to-interact” 
paradigm. Löwgren [12] argues that mobile interaction 
design research should explore interaction possibilities 
outside the established screen idiom, making common 
design methods inadequate. Inspired by participatory design 
and in the wild studies, I studied an approach driven by 
situated use and active participation to inform the design of 
mobile non-idiomatic interfaces that support the running 
experience. At design time, I conducted a co-designing 
workshop with four runners. They used a hand gesture 
toolkit to create their own personal mobile prototypes. At 
use time, I led a one-month field study integrating these 
prototypes into their running practice. Three of the 
participants changed their prototypes during use. I found 
that mobile interfaces ought to be individual and situated; 
affording different user needs and practices. My suggestion 
is to provide flexible and functional prototypes that can be 
experienced in situated use and altered by the participants 
during use. I believe that an approach of co-designing 
engaging mobile prototypes can inspire research of non-
idiomatic interfaces in other mobile contexts.  
REFERENCES 
[1] 
L. Suchman, R. Trigg, and J. Blomberg, “Working artefacts: 
ethnomethods of the prototype,” Br. J. Sociol., vol. 53, no. 2, 
pp. 163–179, Jun. 2002. 
[2] 
P. Dourish and G. Bell, Divining a digital future: mess and 
mythology in ubiquitous computing. Cambridge, Mass: MIT 
Press, 2011. 
[3] 
R. de Oliveira and N. Oliver, “TripleBeat: Enhancing 
Exercise Performance with Persuasion,” in Proceedings of 
the 10th International Conference on Human Computer 
Interaction with Mobile Devices and Services, New York, 
NY, USA, 2008, pp. 255–264. 
[4] 
E. Kurdyukova, “Inspire, Guide, and Entertain: Designing a 
Mobile Assistant for Runners,” in Proceedings of the 11th 
International Conference on Human-Computer Interaction 
with Mobile Devices and Services, New York, NY, USA, 
2009, pp. 75:1–75:2. 
[5] 
E. Preuschl, A. Baca, H. Novatchkov, P. Kornfeind, S. 
Bichler, and M. Boecskoer, “Mobile Motion Advisor — a 
feedback system for physical exercise in schools,” Procedia 
Eng., vol. 2, no. 2, pp. 2741–2747, Jun. 2010. 
[6] 
M. Seuter, M. Pfeiffer, G. Bauer, K. Zentgraf, and C. Kray, 
“Running with Technology: Evaluating the Impact of 
Interacting with Wearable Devices on Running Movement,” 
Proc ACM Interact Mob Wearable Ubiquitous Technol, vol. 
1, no. 3, pp. 101:1–101:17, Sep. 2017. 
[7] 
J. Marshall and P. Tennent, “Mobile Interaction Does Not 
Exist,” in CHI ’13 Extended Abstracts on Human Factors in 
Computing Systems, New York, NY, USA, 2013, pp. 2069–
2078. 
[8] 
J. Bergstrom-Lehtovirta, A. Oulasvirta, and S. Brewster, 
“The Effects of Walking Speed on Target Acquisition on a 
Touchscreen Interface,” in Proceedings of the 13th 
International Conference on Human Computer Interaction 
with Mobile Devices and Services, New York, NY, USA, 
2011, pp. 143–146. 
[9] 
B. Schildbach and E. Rukzio, “Investigating Selection and 
Reading Performance on a Mobile Phone While Walking,” in 
Proceedings of the 12th International Conference on Human 
Computer Interaction with Mobile Devices and Services, 
New York, NY, USA, 2010, pp. 93–102. 
[10] J. Lumsden and S. Brewster, “A Paradigm Shift: Alternative 
Interaction Techniques for Use with Mobile & Wearable 
Devices,” in Proceedings of the 2003 Conference of the 
Centre for Advanced Studies on Collaborative Research, 
Toronto, Ontario, Canada, 2003, pp. 197–210. 
[11]  Y. Liu et al., “CHI 1994-2013: mapping two decades of 
intellectual 
progress 
through 
co-word 
analysis” 
in 
Proceedings of the 32nd annual ACM conference on Human 
factors in computing systems, 2014, pp. 3553–3562. 
[12]  J. Löwgren, “On the Significance of Making in Interaction 
Design Research,” Interactions, vol. 23, no. 3, pp. 26–33, 
Apr. 2016. 
[13]  J. Moen, “From Hand-held to Body-worn: Embodied 
Experiences of the Design and Use of a Wearable 
Movement-based Interaction Concept,” in Proceedings of the 
1st International Conference on Tangible and Embedded 
Interaction, New York, NY, USA, 2007, pp. 251–258. 
[14]  “MUM 2017 - 16th International Conference on Mobile and 
Ubiquitous 
Multimedia.” 
[Online]. 
Available: 
http://www.mum-conf.org/2017/index.php?web=keynotes. 
[Accessed: 03-Jan-2018]. 
[15] S. K. Stigberg, “A Critical Review on Participation in 
Mobile Interaction Design Research,” in MUM 2017: The 
16th International Conference on Mobile and Ubiquitous 
228
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

Multimedia, November 26--29, 2017, Stuttgart, Germany, 
New York, NY, USA, 2017, pp. 161–166. 
[16]  J. Ruiz, Y. Li, and E. Lank, “User-defined Motion Gestures 
for Mobile Interaction,” in Proceedings of the SIGCHI 
Conference on Human Factors in Computing Systems, New 
York, NY, USA, 2011, pp. 197–206. 
[17]  K. Kim, D. Joo, and K.-P. Lee, “Wearable-object-based 
Interaction for a Mobile Audio Device,” in CHI ’10 
Extended Abstracts on Human Factors in Computing 
Systems, New York, NY, USA, 2010, pp. 3865–3870. 
[18]  C. Feng, “Designing Wearable Mobile Device Controllers 
for Blind People: A Co-Design Approach,” in Proceedings 
of the 18th International ACM SIGACCESS Conference on 
Computers and Accessibility, New York, NY, USA, 2016, 
pp. 341–342. 
[19] M. Pakanen, T. Lappalainen, P. Roinesalo, and J. Häkkilä, 
“Exploring Smart Handbag Concepts Through Co-design,” 
in Proceedings of the 15th International Conference on 
Mobile and Ubiquitous Multimedia, New York, NY, USA, 
2016, pp. 37–48. 
[20]  D. Ahlström, K. Hasan, and P. Irani, “Are You Comfortable 
Doing That?: Acceptance Studies of Around-device Gestures 
in and for Public Settings,” in Proceedings of the 16th 
International Conference on Human-computer Interaction 
with Mobile Devices & Services, New York, NY, USA, 
2014, pp. 193–202. 
[21]  E. Costanza, S. A. Inverso, and R. Allen, “Toward Subtle 
Intimate Interfaces for Mobile Devices Using an EMG 
Controller,” in Proceedings of the SIGCHI Conference on 
Human Factors in Computing Systems, New York, NY, 
USA, 2005, pp. 481–489. 
[22]  T. Mulling and M. Sathiyanarayanan, “Characteristics of 
Hand Gesture Navigation: A Case Study Using a Wearable 
Device (MYO),” in Proceedings of the 2015 British HCI 
Conference, New York, NY, USA, 2015, pp. 283–284. 
[23]  J. Rico and S. Brewster, “Usable Gestures for Mobile 
Interfaces: Evaluating Social Acceptability,” in Proceedings 
of the SIGCHI Conference on Human Factors in Computing 
Systems, New York, NY, USA, 2010, pp. 887–896. 
[24]  S. Schneegass and A. Voit, “GestureSleeve: Using Touch 
Sensitive Fabrics for Gestural Input on the Forearm for 
Controlling Smartwatches,” in Proceedings of the 2016 ACM 
International Symposium on Wearable Computers, New 
York, NY, USA, 2016, pp. 108–115. 
[25]  J. R. Wiliamson, A. Crossan, and S. Brewster, “Multimodal 
Mobile Interactions: Usability Studies in Real World 
Settings,” in Proceedings of the 13th International 
Conference on Multimodal Interfaces, New York, NY, USA, 
2011, pp. 361–368. 
[26]  Y. Rogers, “Interaction Design Gone Wild: Striving for Wild 
Theory,” interactions, vol. 18, no. 4, pp. 58–62, Jul. 2011. 
[27] A. Crabtree, “Design in the Absence of Practice: Breaching 
Experiments,” in Proceedings of the 5th Conference on 
Designing 
Interactive 
Systems: 
Processes, 
Practices, 
Methods, and Techniques, New York, NY, USA, 2004, pp. 
59–68. 
[28] Y. Rogers et al., “Why It’s Worth the Hassle: The Value of 
In-situ Studies when Designing Ubicomp,” in Proceedings of 
the 9th International Conference on Ubiquitous Computing, 
Berlin, Heidelberg, 2007, pp. 336–353. 
[29]  J. Greenbaum and F. Kensing, “Heritage: having a say,” in 
Routledge International Handbook of Participatory Design, 
Routledge, 2012, pp. 41–56. 
[30]  C. Mareis, M. Held, G. Joost, “Wer gestaltet die Gestaltung? 
Praxis, Theorie und Geschichte des partizipatorischen 
Designs”. Bielefeld: Transcript, 2013. 
[31]  E. B.-N. Sanders, “Generative Tools for Co-designing,” in 
Collaborative Design, Springer, London, 2000, pp. 3–12. 
[32]  D. Svanaes and G. Seland, “Putting the Users Center Stage: 
Role Playing and Low-fi Prototyping Enable End Users to 
Design Mobile Systems,” in Proceedings of the SIGCHI 
Conference on Human Factors in Computing Systems, New 
York, NY, USA, 2004, pp. 479–486. 
[33]  Y. Dittrich, S. Eriksén, and C. Hansson, “PD in the Wild; 
Evolving Practices of Design in Use,” PDC, pp. 124–134, 
Jan. 2002. 
[34]  E. B.-N. Sanders and P. J. Stappers, “Probes, toolkits and 
prototypes: three approaches to making in codesigning,” 
CoDesign, vol. 10, no. 1, pp. 5–14, Jan. 2014. 
[35]  S. K. Stigberg, “Mobile Hand Gesture Toolkit: Co-Designing 
Mobile Interaction Interfaces,” in Proceedings of the 2017 
ACM Conference Companion Publication on Designing 
Interactive Systems, New York, NY, USA, 2017, pp. 161–
166. 
[36]  “Myo Gesture Control Armband | Wearable Technology by 
Thalmic Labs.” [Online]. Available: https://www.myo.com/. 
[Accessed: 16-Mar-2018]. 
[37]  “Tasker 
for 
Android.” 
[Online]. 
Available: 
http://tasker.dinglisch.net/. [Accessed: 20-Mar-2018]. 
[38]  M. Sathiyanarayanan and T. Mulling, “Map Navigation 
Using Hand Gesture Recognition: A Case Study Using MYO 
Connector on Apple Maps,” Procedia Comput. Sci., vol. 58, 
no. Supplement C, pp. 50–57, Jan. 2015. 
[39]  R. Ballagas, F. Memon, R. Reiners, and J. Borchers, “iStuff 
Mobile: Rapidly Prototyping New Mobile Phone Interfaces 
for Ubiquitous Computing,” in Proceedings of the SIGCHI 
Conference on Human Factors in Computing Systems, New 
York, NY, USA, 2007, pp. 1107–1116. 
[40]  D. Ferreira, V. Kostakos, and A. K. Dey, “AWARE: Mobile 
Context Instrumentation Framework,” Front. ICT, vol. 2, 
2015. 
[41]  M. Raento, A. Oulasvirta, R. Petit, and H. Toivonen, 
“ContextPhone: a prototyping platform for context-aware 
mobile applications,” IEEE Pervasive Comput., vol. 4, no. 2, 
pp. 51–59, Jan. 2005. 
[42]  A. Parnami, A. Gupta, G. Reyes, R. Sadana, Y. Li, and G. 
Abowd, “Mogeste: Mobile Tool for In-situ Motion Gesture 
Design,” in Proceedings of the 2016 ACM International 
Joint Conference on Pervasive and Ubiquitous Computing: 
Adjunct, New York, NY, USA, 2016, pp. 345–348. 
[43]  G. Fischer and E. Giaccardi, “Meta-design: A Framework for 
the Future of End-User Development,” in End User 
Development, H. Lieberman, F. Paternò, and V. Wulf, Eds. 
Springer Netherlands, 2006, pp. 427–457. 
[44]  J. Karlsen, S. Koch Stigberg, and J. Herstad, “Probing 
Privacy in Practice: Privacy Regulation and Instant Sharing 
of Video in Social Media when Running,” in Proceedings of 
the 2016 International Conference on Advances in 
Computer-Human Interactions, Venice, Italy, 2016, pp. 29–
36. 
 
229
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

