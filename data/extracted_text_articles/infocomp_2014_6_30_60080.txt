Fast Person Identiﬁcation Using JPEG2000 Compressed ECG Data
Yi-Ting Wu∗, Hung-Tsai Wu†, and Wen-Whei Chang‡
Institute of Communications Engineering
National Chiao-Tung University
Hsinchu, Taiwan
e-mail: ∗jay79227@gmail.com, †htwu.nctu@gmail.com, ‡wwchang@cc.nctu.edu.tw
Abstract—The use of electrocardiogram (ECG) signals in biomet-
ric systems has been an active research topic for over a decade.
In wireless telecardiology applications, compressed ECG packets
are often required for efﬁcient transmission and storage purposes.
Nonetheless, compressed ECG data must be decompressed ﬁrst
before applying existing biometric techniques that work on the
original signal. To achieve a faster patient care, we propose a
new biometric technique which performs person identiﬁcation in
compressed-domain using one-lead ECG signals. First, we apply a
preprocessor which converts one-dimensional (1-D) ECG signals
to 2-D image matrices and compresses them by the JPEG2000
image coding standard. Features relating to ECG morphology
were extracted directly from the JPEG2000 code-stream and
then applied for indexing person identity by texture content in
a known enrollment database. Experiments on standard ECG
databases demonstrate the validity of the proposed compressed-
domain ECG biometric system with an accuracy of 95.72%.
Keywords–ECG Biometric; Person Identiﬁcation; JPEG2000
Image Coding Standard.
I.
INTRODUCTION
Electrocardiogram (ECG) signal is a recording of the
electrical activity of the heart and is a clinical diagnosis tool for
cardiac diseases. Typical features are linked to the peaks and
time durations of the P-QRS-T waves representing the heart
activity in terms of depolarization and repolarization of the
atria and ventricles [1]. In wireless telecardiology applications,
ECG data forwarded by the user to the hospital needs to be ver-
iﬁed and authenticated to guard against spoof attack. Recently,
some proposals have suggested the possibility of using ECG
as a new biometrics modality for person identiﬁcation [2]-[9].
Comparing with other biometric traits such as ﬁngerprints and
voice, the ECG of a human is more universal and secure. The
validity of using ECG biometric is supported by the fact that
ECG can only be obtained through a sensor placed around
the user and forgery is difﬁcult as the unique shape of the
ECG signal is affected by the physiological and geometrical
differences of the heart in different individuals.
Basically, ECG biometrics can be achieved by comparing
the enrollment ECG template and recognition ECG template.
Based on the features that are extracted from ECG signals,
we can classify ECG biometric methods as ﬁducial-based [2]-
[5], non ﬁducial-based [6]-[8], or a hybrid [9]. Among them,
Discrete Wavelet Transform (DWT) techniques have shown
effective for extracting discriminative features in ECG bio-
metric recognition [6][7]. Irrespective of underlying methods
used for the generation of the templates, most of the existing
ECG biometrics work on uncompressed raw ECG signals [2]-
[9]. However, in remote telecardiology scenarios, ECG data
are often kept in compressed format for efﬁcient transmission
and storage purposes. Many ECG data compression methods
have been proposed, including direct time-domain methods and
transformation methods [10][11]. Most of the ECG data com-
pression methods adopt one-dimensional (1-D) representations
for ECG signals and focus on the utilization of the intra-beat
correlation between adjacent samples. Since the ECG signals
have both intra- and inter-beat correlation, better algorithms
have been proposed to get the most of beneﬁt from both types
of correlation [12][13]. These methods generally start with a
preprocess which converts 1-D ECG signals to 2-D data arrays
through the combined use of QRS detection and period normal-
ization. The constructed 2-D ECG data arrays are then ready
to be further compressed by the vector quantization [12] or the
JPEG2000 image coding standard [13]. In such scenarios, then
the compressed ECG data must be fully decompressed before
applying existing biometric techniques. Apart from posing
threat to the emergency patient, delay in authentication can be
an unnecessary burden on the hospital system, as the hospital
may have thousands of enrolled patients and decompression of
all their ECG packets is an enormous work. This drawback can
be avoided by directly reading the compressed ECG data to
obtain unique features that can identify an individual. The key
advantage of the proposed compressed-domain ECG biometric
system is smaller template size and faster biometric matching
compared with existing biometric systems. In addition, most
of the literatures have concentrated their research on obtaining
healthy subjects in their experiments. By contrast, we will look
into the effects of diseased subjects on the recognition rate of
an ECG biometric system.
The rest of this paper is organized as follows. Section II
describes the ECG fundamentals and presents a preprocessor
which converts 1-D ECG signals to 2-D image matrices.
Section III gives an overview of the JPEG2000 encoding
algorithm. Details of the algorithms for the proposed ECG bio-
metric system are provided in Section IV. Section V presents
the simulation results for the healthy and diseased subjects
using standard ECG databases. Finally, Section VI gives our
conclusions.
II.
2-D ECG COMPRESSION
To begin, we apply a preprocessor, which can be viewed as
a cascade of two stages. In the ﬁrst stage, the QRS complex in
each heartbeat is ﬁrstly detected for segmenting and aligning 1-
D ECG signals to 2-D image matrices and in the second stage,
the constructed matrices are compressed by JPEG2000 [14].
Figure 1 shows the block diagram of the 2-D ECG compression
scheme proposed by Bilgin [13].
A. ECG Data Sources
The ECG data for the experiments are obtained from the
QT Database [15] that contains ECG recordings collected from
healthy subjects and patients with various heart diseases. First,
10 healthy subjects from the MIT-BIH Normal Sinus Rhythm
144
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

Figure 1.
Block diagram of the ECG JPEG2000 compression scheme [13].
Database are used in the experiments and denoted as dataset
D1. Subjects that are added to the system to determine the
effects of the diseased ECG consist of 10 records from the
MIT-BIH Arrhythmia Database, 10 records from the MIT-BIH
Supraventricular Arrhythmia Database, and 10 records from
the Sudden Cardiac Death Holter Database. For convenience,
these three groups of diseased subjects are denoted as dataset
D2, D3, and D4, respectively. Each of these records is 15
minutes in duration and are sampled at 250 Hz with a resolu-
tion of 11 or 12 bits/sample. The ECG waveform of a normal
heartbeat consists of a P wave, a QRS complex, and a T wave
[1]. The P wave corresponds to the sequential depolarization
of the right and left atria. The QRS complex is produced when
the ventricles depolarize and squeeze the blood from the right
ventricle to the aorta. The T wave occurs due to ventricular
repolarization. A typical representation of the ECG waveform
is shown in Figure 2. It can be seen that an ECG signal tends
to exhibit considerable similarity between adjacent heartbeats,
along with short-term correlation between adjacent samples.
Thus, by dividing an ECG signal into heartbeat segments with
lengths equal to the beats, there should be a large correlation
between individual segments.
B. 2-D Image Matrix Construction
ECG itself is 1-D in the time-domain, but can be viewed
as a 2-D signal in terms of its implicit periodicity. The
QRS complex is the most characteristic wave in an ECG
waveform and hence, its peak can be used to identify each beat.
First of all, raw ECG signals are ﬁltered to remove various
noises. Afterwards we apply the Biomedical Signal Processing
Toolbox [16] to detect the R peak of each QRS complex. Then,
an ECG signal is divided into heartbeat segments and each
segment is stored as one row of a 2-D data array. Having
constructed the data array as such, the intra-beat correlation
is in the horizontal direction of the array and the inter-beat
correlation is in the vertical direction. Since the heartbeat
segments may have different lengths, each row of the data
array is period normalized to a ﬁxed length of Np = 200
samples via cubic spline interpolation. This choice was based
on the observation that the average heartbeat length is about
0.8 second, which corresponds to 200 samples for a 250-
Hz sampling frequency. Accordingly, the original heartbeat
length was represented with 9 bits and transmitted as side
information. Finally, we proceed to construct image matrices
of dimension 200×200 by gathering together 200 rows of the
data array and normalizing the amplitude of each component
to an integer ranging from 0 to 255. A typical example of
an ECG image matrix is shown in Figure 3. The constructed
gray-scale ECG image matrices are then ready to be further
compressed by the JPEG2000 still image coding standard.
III.
JPEG2000 ENCODING ALGORITHM
Although the JPEG2000 coding standard was originally
developed for still image compression, its applicability to
Figure 2.
Typical ECG wave pattern in time-domain.
Figure 3.
ECG image matrix for record sel17453m of the QT Database.
ECG data compression has been proposed in [13]. Compared
with the original JPEG coding standard, the wavelet-based
JPEG2000 provides advanced features in scalability and ﬂexi-
bility. In addition, JPEG2000 supports Region of Interest (ROI)
coding so that different parts of an image can be coded with
different ﬁdelity. The JPEG2000 encoding process consists
of the following operations: 1) a preprocessing step which
includes tilling, DC-level shifting and color space transform,
2) 2-D DWT followed by quantization, and 3) entropy coding
and bit-stream organization. The fundamental building blocks
of JPEG2000 are shown in Figure 4. The encoder begins with
a preprocessor which divides the source image into rectangular
blocks called tiles. For each tile, the DC level of image samples
is shifted to zero and color space transform is performed to de-
correlate the color information. Then, the 2-D DWT is carried
out for each color component of a tile. Successive dyadic
decompositions are applied and each of these splits high and
low frequencies in the horizontal and vertical directions into
four subbands. Among them, the subband corresponding to
the lowest frequency in the two directions is used as a starting
point for the next decomposition. This process is repeated for
N levels until no signiﬁcant gains in compression efﬁciency
can be obtained. In total, (3N +1) subbands are obtained with
respect to an N-level 2-D wavelet decomposition.
The 2-D DWT can be viewed as a cascade of 1-D DWT in
the horizontal direction and 1-D DWT in the vertical direction.
Speciﬁcally, the 1-D DWT is based on the lifting scheme
described in [17]. First, source data x[n] are split into even
samples s0[n] = x[2n] and odd samples d0[n] = x[2n + 1].
Then, the detailed coefﬁcients di[n] and the approximation
145
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

Figure 4.
Fundamental building blocks of a JPEG2000 encoder [14].
coefﬁcients si[n] at the i-th iteration are calculated as follows:
di[n] =di−1[n]−
X
k
Pi[k]si−1[n−k], i = 1, 2, . . . , N −1
si[n] =si−1[n]+
X
k
Ui[k]di[n−k], i = 1, 2, . . . , N −1,
(1)
where Pi and Ui represent a predictor and an updater, respec-
tively. Finally, both the detailed and approximation coefﬁcients
are normalized. For notational convenience, let xj(m, n) rep-
resent the wavelet coefﬁcient located at position (m, n) in
the j-th subband. After the 2-D DWT, a uniform quantizer
is applied to quantize each wavelet coefﬁcient xj(m, n) by an
index ¯xj(m, n) as follows:
¯xj(m, n) = sgn(xj(m, n))
|xj(m, n)|
∆j

,
(2)
where ∆j denotes the quantizer’s step size. The last step
in JPEG2000 compression consists in entropy coding with
two tier encoders. In the tier-1 encoder, quantized indexes
of each subband are split into code-blocks, which are then
compressed using a context-based entropy coder. The tier-
2 encoder truncates the bit-streams of each code-block to
meet the targeted compression ratio, and combines them with
additional headers to form the JPEG2000 code-stream.
IV.
PROPOSED ECG BIOMETRIC ALGORITHM
Person identiﬁcation is essentially a pattern recognition
problem consisted of two stages: feature extraction and clas-
siﬁcation. Under the JPEG2000 framework, the person iden-
tiﬁcation problem is analogous to a Content Based Image
Retrieval (CBIR) problem. Concerning compressed-domain
techniques, the JPEG2000 code-stream is subjected to partial
decoding and then the energies from all subbands are used
as the feature set. We also introduce a new method for
feature extraction that involves the application of the Principal
Component Analysis (PCA) for dimensionality reduction. In
the classiﬁcation stage, the query ECG image is compared
with the enrollment database, and output the person identity
that best matches the query with respect to some distance
measurement criterion. Figure 5 shows the block diagram of
the proposed ECG biometric system.
A. Feature Extraction in DWT Domain
Feature extraction is the ﬁrst step in applying CBIR to ECG
biometrics and one that conditions all the subsequent steps
of system implementation. For arbitrary image databases of
natural scenes, color and texture features are considered most
important. Since 1-D ECG signals are converted to gray-scale
images, we shall focus on the texture features that characterize
Figure 5.
The proposed ECG biometric system.
smooth, coarse and regularity of the speciﬁc image. One
effective tool to texture analysis is the DWT as it provides good
time and frequency localization ability. Its multi-scale nature
also allows the decomposition of an ECG into different scales,
each of which represents particular coarseness of the signal.
Furthermore, DWT coefﬁcients in JPEG2000 can be obtained
without involving a full decompression, as partial decoding of
the JPEG2000 code-stream would sufﬁce the needs. This is a
favorable property as the inverse DWT and subsequent decod-
ing processes could impose intensive computational burden.
Different texture features such as energy, signiﬁcance map, and
intensity histogram at the output of wavelet ﬁlter-banks have
been successfully applied to wavelet-based image retrieval
[18]-[20]. In general, any measures that provide some degree of
class separation should be included in the feature set. However,
as more features are added, there is a trade-off between
classiﬁcation performance and computational complexity.
We began by using the energies from all subbands as a
ﬁrst step towards an efﬁcient characterization of texture in
compressed ECG images. This is because that the energy
distribution along the frequency axis over scale and orientation
has been shown effective for texture characterization in image
retrieval [19][20]. For each subband, the reconstructed wavelet
coefﬁcients ˜xj(m, n) are computed as follows:
˜xj(m, n) = [¯xj(m, n) + r · sgn(¯xj(m, n))] · ∆j,
(3)
where ¯xj(m, n) represents the decoded quantizer indexes and
the bias parameter r is set to be zero here. Then, the energy
of the j-th subband is deﬁned as
Ej =
1
MjNj
Mj
X
m=1
Nj
X
n=1
˜x2
j(m, n),
(4)
where Mj and Nj represents the row and column dimension,
respectively. Another feature of interest is the average time
elapse between the current and previous R peaks, referred to as
the RR-interval. Certain ECG arrhythmias, such as premature
ventricular contraction and atrial premature beats, are related
with premature heart beats that provide shorter RR-intervals
than other types of ECG signals. Changes in the RR-interval
plays an important role in characterizing these types of arrhyth-
mias. Notice that the RR-interval can be calculated from the
beat lengths which are transmitted as side information along
with the JPEG2000 code-stream. In total, (3N +2) features are
used to form a Biometric Identiﬁcation Vector (BIV), including
the RR-interval and (3N + 1) subband energies.
The second feature set is obtained by applying PCA on the
subband whose frequency is the lowest in both horizontal and
vertical directions. This choice is based on the fact that human
eyes are more sensitive to low frequency components than
high frequency components. As one of the most commonly
used dimension reduction techniques, PCA ﬁnds the most
146
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

representative set of projection vectors such that the projected
samples retain the most information about the original data
samples. To begin, the M wavelet coefﬁcients in the lowest
frequency subband of a training dataset are used to compute the
covariance matrix C. Following an eigenvalue decomposition,
we obtain an eigenvalue matrix D and its corresponding
eigenvector matrix V with its column vectors sorted in the
descending order of eigenvalues. Finally, column vectors of
V are used as a basis set for projection of the data in the
directions of sorted eigenvectors. Let xl represent the data
vector consisting of M wavelet coefﬁcients in the lowest
frequency subband taken from the l-th ECG image. The PCA
procedure can be expressed as
C =
Nt
X
l=1
(xl − ¯x)(xl − ¯x)T = V DV T ,
(5)
where Nt is number of ECG images used in the training
dataset and ¯x represents the mean vector of all xl. Then,
the data vector xl is projected on the eigenvectors by taking
the inner product according to yl = V T xl. In this work,
the PCA was applied on M = 49 coefﬁcients of the low-
est frequency subband and seven principal components were
selected corresponding to retaining approximately 99% of the
total variability in the training dataset. Together with the RR-
interval, only eight features are used to form a BIV for the
second feature set.
B. Enrollment and Recognition
The proposed person identiﬁcation system can be divided
into two stages: enrollment and recognition. During the enroll-
ment stage, a number of BIVs of each enrolled user were taken
as a representation of the user and enrolled into a database. In
the recognition stage, the ECG signals of an unknown subject
are acquired and then compressed by JPEG2000. Afterwards,
the feature extraction procedure results in a query BIV to
be compared with all the BIVs enrolled in the database.
Accordingly, the system outputs the person identity of an
enrolled BIV which best matches the query BIV with respect
to a distance measurement criterion. In this work, the Standard
Euclidean Distance (SED) is adopted to measure the respective
similarity between the query BIV q and each enrolled BIV
e. Mathematically, the SED is denoted by dq,e and can be
expressed as
dq,e =
v
u
u
t
L
X
i=1
(q(i) − e(i)
σi
)2,
(6)
where L is the length of the BIV, q(i) and e(i) denote the
i-th component in the corresponding BIV, and the scale factor
σi represents the standard deviation computed from the i-th
component of all enrolled BIVs. As the data belonging to the
same class should be close in the feature space, a Nearest-
Neighbor (NN) classiﬁer is used to search for the minimum
SED value and assigns its corresponding person identity as the
recognition result.
V.
EXPERIMENTAL RESULTS
Computer simulations were conducted to evaluate the
performance of the proposed compressed-domain approaches
for person identiﬁcation. Two ECG biometric systems based
on JPEG2000, denoted by PA1 and PA2, are presented and
investigated. They both applied a preprocessor for construction
of the 2-D ECG image matrices in the JPEG2000 format and
used an NN classiﬁer for subsequent person identiﬁcation.
Unlike the PA1 which was performed using subband energy-
based feature set, the PA2 extracted the wavelet coefﬁcients
from the lowest frequency subband and represented in a lower
dimensional space using PCA. 10 normal subjects and 30
diseased subjects from the QT Database are chosen to represent
a wide variety of QRS and ST-T morphologies. The JPEG2000
codec used here was the open-source software JasPer version
1.900.0 [21]. ECG images were compressed in a lossy mode
using Daubechies 9/7 ﬁlter with 5-level of decomposition,
while the dimension of each tile and code-block is set to be
200 × 200 and 64 × 64, respectively. Besides, the parameter
value of coding rate ρ is set to be 0.15 and 0.08 in order to
achieve the compression ratio of 10 and 20, respectively.
A preliminary experiment was ﬁrst conducted on normal
and diseased ECG signals to examine the performance of 2-
D compression by JPEG2000 [13]. Typically, the performance
is evaluated in terms of the compression ratio (CR) and the
percent root mean square difference (PRD). The CR is deﬁned
as
CR= Nori
Ncom
,
(7)
where Nori and Ncom represent the total number of bits re-
quired for the original and compressed ECG data, respectively.
The PRD is used to evaluate the reconstruction distortion and
is deﬁned by
PRD(%) =
v
u
u
t
PK
k=1[xori(k) − xrec(k)]2
PK
k=1 xori(k)2
× 100,
(8)
where K is the total number of original samples in the record
and xori and xrec represent the original and reconstructed ECG
signals, respectively. Table I presents the average results for
2-D compression of various ECG records using JPEG2000
with coding rate ρ = 0.15 and ρ = 0.08. As should be
expected, the PRD performance of the system is related to
the severity of disease. For the coding rate of ρ = 0.08, the
individual PRD performances of the ECG records vary from
5.11% to 7.31% depending on the characteristics of normal
and pathological ECG signals. Although the 2-D compression
method shows good results for normal ECG signals, it may
suffer from irregular rhythms mainly due to the QRS detection
stage. In order to exploit the inter-beat correlation, the ECG
signal is QRS detected and then segmented according to the
detected ﬁducial points. As a consequence, the performance of
2-D ECG compression algorithms is affected by the accuracy
of the QRS detection scheme. Compared with normal subjects,
the worse performance of diseased subjects may be attributed
to the fact that the number of QRS false detections may
increase signiﬁcantly in the presence of noise and varying QRS
morphology.
The next step is to evaluate the recognition performance of
the proposed ECG biometric system on different datasets. The
system performance is evaluated in terms of the recognition
rate, which is normally deﬁned as the ratio of the number
of correctly identiﬁed subjects to the total number of testing
subjects. First of all, the proposed biometric systems were indi-
vidually tested on datasets from D1 to D4. Table II presents the
recognition rate performances associated with various datasets
for the case where the ECG images are subjected to JPEG2000
147
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

TABLE I
AVERAGE CR AND PRD PERFORMANCES FOR JPEG2000 WITH RATE
ρ = 0.15 AND ρ = 0.08.
Rate ρ
D1
D2
D3
D4
0.15
CR
14.28
9.88
10.72
10.64
PRD
3.08%
3.54%
3.32%
3.10%
0.08
CR
21.66
17.36
19.70
19.20
PRD
5.11%
7.31%
7.24%
7.25%
TABLE II
RECOGNITION RATES (%) OF THE PROPOSED METHODS.
Dataset
JPEG2000 (ρ = 0.15)
JPEG2000 (ρ = 0.08)
PA1
PA2
PA1
PA2
D1
96.18
99.98
95.45
99.99
D2
87.66
91.95
87.73
91.53
D3
91.48
95.50
91.54
95.46
D4
84.87
91.85
83.95
92.18
D1, D2
90.19
94.97
90.25
94.20
D1, D2, D3
89.98
96.39
89.16
95.92
D1, D2, D3, D4
89.55
95.72
89.39
95.66
encoding with coding rate ρ = 0.15 and ρ = 0.08. Data in the
table have been averaged over 1000 trials per subject. For each
subject, we randomly selected four compressed ECG images
for feature extraction and one BIV results for each ECG image.
Among them, the ﬁrst two BIVs of each subject are used for
training in the enrollment stage, and the other two BIVs are
used for testing in the recognition stage. The results clearly
demonstrate the improved performance achievable using PA2
in comparison to that of PA1. By the method PA2 with
ρ = 0.15 applied individually on datasets from D1 to D4, the
recognition rate was 99.98% for normal subjects, 91.95% for
arrhythmia subjects, 95.50% for supraventricular arrhythmia
subjects, and 91.85% for sudden cardiac death subjects. To
elaborate further, we also investigate the performance of PA2
when normal and diseased subjects are jointly enrolled and
tested. With this in mind, the initial 10 normal subjects are
combined with an additional 10, 20, and 30 diseased subjects
and the system is retrained and tested again. The results in
Table II show a correct recognition rate of 94.97% when the
system PA2 is jointly tested on datasets D1 and D2. When 10
other subjects from dataset D3 were added into the database,
recognition rate was 96.39%. Lastly, 95.72% was achieved
with the entire database containing 10 normal subjects and 30
diseased subjects. As the table shows, the additional inclusion
of 30 diseased subjects only dropped the recognition rate by
4.26%. This clearly demonstrates that the proposed system
PA2 is robust enough to handle the inclusion of diseased ECG
in the biometric database.
VI.
CONCLUSIONS
This paper proposed a fast method of ECG biometric
recognition that directly uses the compressed ECG data in
JPEG2000 format. Under the JPEG2000 framework, the person
identiﬁcation problem is analogous to a context-based image
retrieval problem. In this work, we have compared the per-
formances of two feature sets for texture characterization in
compressed ECG images. The ﬁrst feature set uses the energies
of all subbands, whereas the second feature set is obtained
by applying PCA on the wavelet coefﬁcients of the lowest
frequency subband. Also, we look into the effects of diseased
subjects on the recognition rate of a compressed-domain ECG
biometric system. The proposed ECG biometric system has
been tested on standard ECG databases and high recognition
accuracy is achieved with a low feature dimension.
ACKNOWLEDGMENT
This research was supported by the National Science
Council, Taiwan, ROC, under Grant NSC 102-2221-E-009-
030-MY3.
REFERENCES
[1]
M. S. Thaler, The Only EKG Book You’ll Ever Need, 7th ed. Philadel-
phia, PA: Lippincott Williams & Wilkins, 2012.
[2]
L. Biel, O. Pettersson, L. Philipson, and P. Wide, “ECG Analysis: A
New Approach in Human Identiﬁcation,” IEEE Trans. Instrum. Meas.,
vol. 50, no. 3, Jun. 2001, pp. 808-812.
[3]
S. Israel, J. Irvine, A. Cheng, M. Wiederhold, and B. Wiederhold, “ECG
to identify individuals,” Pattern Recognit., vol. 38, 2005, pp. 133-142.
[4]
T. W. D. Shen, W. J. Tompkins, and Y. H. Hu, “Implementation of a
one-lead ECG human identiﬁcation system on a normal population,”
J. Eng. Comput. Innovations, vol. 2, no. 1, Jan. 2011, pp. 12-21.
[5]
I. Odinaka et al., “ECG biometric recognition: A comparative analysis,”
IEEE Trans. Inform. Forensic Secur., vol. 7, no. 6, Dec. 2012, pp. 1812-
1824.
[6]
C. C. Chiu, C. M. Chuang, and C. Y. Hsu, “Discrete Wavelet Trans-
form Applied on Personal Identity Veriﬁcation with ECG Signal,”
Int. J. Wavelets Multi., vol. 7, no. 3, May 2009, pp. 341-355.
[7]
S. A. Fattah, C. Shahnaz, A. S. M. M. Jameel, and R. Goswami, “ECG
Signal Based Human Identiﬁcation Method Using Features in Temporal
and Wavelet Domains,” in Proc. TENCON 2012, Nov. 2012, pp. 1-4.
[8]
D. Hatzinakos, K. Plataniotis, and J. K. M. Lee, “ECG Biometric
Recognition Without Fiducial Detection,” in Proc. IEEE Biometrics
Symposiums (BSYM), Sep. 2006, pp. 1-6.
[9]
Y. Wang, F. Agraﬁoti, D. Hatzinakos, and K. Plataniotis “Analysis of
human electrocardiogram ECG for biometric recognition,” EURASIP
J. Adv. Signal Process., vol. 1, Jan. 2008, pp. 1-6.
[10]
S. M. S. Jalaleddine, C. G. Hutchens, R. D. Strattan, and W. A. Coberly,
“ECG data compression techniques - a uniﬁed approach,” IEEE
Trans. Biomed. Eng., vol. 37, no. 4, Apr. 1990, pp. 329-343.
[11]
B. Wang and G. Yuan, “Compression of ECG data by vector quanti-
zation,” IEEE Eng. Med. Biol. Mag., vol. 16, no. 4, Jul./Aug. 1997,
pp. 23-26.
[12]
C. C. Sun and S. C. Tai, “Beat-based ECG compression using gain-
shape vector quantization,” IEEE Trans. Biomed. Eng., vol. 52, no. 11,
Nov. 2005, pp. 1882-1888.
[13]
A. Bilgin, M. W. Marcellin, and M. I. Altbach, “Compression of electro-
cardiogram signals using JPEG2000,” IEEE Trans. Consum. Electron.,
vol. 49, no. 4, Nov. 2003, pp. 833-840.
[14]
D. S. Taubman and M. W. Marcellin, JPEG 2000: Image Compression
Fundamentals, Standards, and Practice. Norwell, MA: Kluwer, 2001.
[15]
The QT Database. [Online]. Available from: http://physionet.org/
physiobank/database/qtdb/ 2014.05.04
[16]
M. Aboy et al., “A biomedical signal processing toolbox,” in Proc.
Biosignal 2002, Jun. 2002, pp. 49-52.
[17]
I. Daubechies and W. Sweldens, “Factoring wavelet transforms into
lifting steps,” J. Fourier Anal. Appl., vol. 4, no. 3, 1998, pp. 245-267.
[18]
J. R. Smith and S. F. Chang, “Transform features for texture classi-
ﬁcation and discrimination in large image databases,” in Proc. IEEE
Int. Conf. Image Process., Nov. 1994, pp. 407-411.
[19]
A. Teynor, W. M¨uller, and W. Kowarschick, “Compressed domain image
retrieval using JPEG2000 and gaussian mixture models,” Visual Infor-
mation and Information Systems, Visual Information and Information
Systems, Lecture Notes in Computer Science, vol. 3736, 2006, pp. 132-
142.
[20]
M. N. Do and M. Vetterli, “Wavelet-Based texture retrieval using gen-
eralized Gaussian density and Kullback-Leibler distance,” Proc. IEEE
Trans. Image Process., vol. 11, no. 2, Feb. 2002, pp. 146-158.
[21]
The JasPer Project Home Page. [Online]. Available from: http://www.
ece.uvic.ca/∼frodo/jasper/ 2014.05.04
148
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

