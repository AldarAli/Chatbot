Web Based E-learning Tool for Visualization and Analysis of 3D Motion Capture 
Data 
Andraž Krašček 
eŠola d.o.o. 
Slovenia 
andraz@easistent.com 
 
Kristina Stojmenova, Sašo Tomažič and Jaka Sodnik 
University of Ljubljana 
Faculty of Electrical Engineering 
Slovenia 
kristina.stojmenova@fe.uni-lj.si 
saso.tomazic@fe.uni-lj.si 
jaka.sodnik@fe.uni-lj.si
 
Abstract—In this paper, we propose an e-learning tool for 
visualization and manipulation of 3D data on a web platform. 
The data is streamed in real time from an optical motion 
capture system Qualisys consisting of eight infrared cameras 
and Qualisys Track Manager (QTM) software. A WebSocket 
protocol and WebGL application programming interface (API) 
are used to visualize and to interact with the data in a browser. 
The tool represents a web-based extension of QTM software 
providing also additional features and new possibilities to 
manipulate and analyze the data. We report also on a user 
study in which we evaluated the web based application and 
compared it with the original desktop-based application. The 
proposed application proved to be fast, effective and intuitive 
and can be used as an e-learning tool for demonstrating and 
teaching techniques for visualization and analysis of motion 
capture data. 
Keywords-motion capture; Qualisys; e-learning; 3D data; 
AIM model; WebGL; WebSocket. 
I. 
 INTRODUCTION 
Optical tracking systems enable motion capture and 
recording of motion parameters of a selected object in space. 
They are commonly used to track motion in the field of 
biomechanics, industrial ergonomics, the moviemaking and 
entertainment industry, etc. Such systems consist of passive 
or active reflective markers that are placed on the points of 
interest of the monitored object, and infrared cameras that 
observe the motion of these markers in space. Active 
markers are light emitters, usually light-emitting diodes 
(LEDs), while passive markers are only light reflectors. 
Infrared cameras detect the light reflected from the markers 
and acquire their two-dimensional (2D) position in the 
recorded image. The system then combines 2D positions 
from all cameras and calculates the exact 3D position of 
markers in space. The tracking process begins with 
calibration, which provides information on exact positions 
and orientations of the infrared cameras. With higher number 
of cameras, a more accurate 3D position can be determined. 
The goal of our research was to design and implement an 
e-learning tool for the visualization and analysis of motion 
capture data on multiple computers simultaneously. The 
analysis is of vital importance when dealing with motion and 
movements in sports. The tool we propose can for example 
be used for establishing the correlations between different 
segments of data (i.e., observing the amplitude of head 
movements in relation to knee angle when performing 
squats). Consequently, it needs to include numerous 
measurements, such as the length of selected bones, the 
angles between the bones, the velocity or acceleration of 
selected segments, etc.  
By e-learning, we refer to a web-based system that makes 
information or knowledge available to students and teachers 
disregarding their geographic proximity and time restrictions 
[1]. The users can analyse and manipulate complex data sets 
remotely and use the tool to demonstrate different methods 
and procedures in real time. E-learning web applications are 
also affordable and easy to distribute to a large number of 
users by simply using a compatible web browser [2].  
II. 
RELATED WORK 
Visualization of 3D data in web applications has been 
addressed in numerous studies in different domains. In many 
cases, the rendering process was based on isosurface 
polygonization [3, 4, 5] enabled by various plug-ins in the 
browser. The most common issues related to this problem 
were specialized and dedicated programming languages, 
plug-in requirements, limited portability across browsers, 
devices and operating systems, and advanced rendering 
support [6]. Today, the rendering process has been 
significantly simplified by the technology called WebGL [7]. 
It is a JavaScript API based on OpenGL ES 2.0 for 
manipulation of 3D graphics in a web browser. It uses the 
OpenGL shading language, OpenGL for Embedded Systems 
(GLSL ES), and can be cleanly combined with other web 
content layered on top or underneath the 3D content. It is 
ideally suited for dynamic 3D web applications in the 
JavaScript programming language, and has been fully 
integrated in all leading web browsers. 
Several researchers reported on using WebGL for 
monitoring and interaction of 3D graphics in a web browser. 
A lot of them exposed the benefits of using web-based 
applications in order to lose dependency of hardware. 
Conote, Segura, Kabongo, Moreno, Posada, Ruiz 
discussed performance and scalability of the volume 
rendering by WebGL in different application domains [3]. In 
their work, they presented how implementation of a direct 
volume rendering system for the web articulates in efficient 
manner the capabilities of WebGL, making the formerly 
unusable accelerated graphic pipeline available.  
131
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-382-7
ACHI 2015 : The Eighth International Conference on Advances in Computer-Human Interactions

WebGL with the ability to put hardware-accelerated 3D 
content in the browser represents a mean for creation of new 
web based applications that were previously exclusive for the 
desktop environment. C. Leung and A. Salga discussed how 
mid-level APIs can help develop web applications that do 
not only copy the desktop application but can contain unique 
3D content as well [9].  
3D monitoring of static objects has also been a subject of 
research. Museums and similar institutions show growing 
interest into showing their collections to a wider public 
trough the web. Schwartz, Ruiters, Weinmann and Klein 
have proposed a WebGL-based presentation framework, 
which does not only provide a 3G geometry, but a powerful 
material representation, capable of reproducing the full 
visual appeal of an object as well [10].  
In this paper, we propose a web based e-learning tool for 
visualization and manipulation of 3D motion capture data in 
real time and on high number of computers simultaneously. 
It also supports observation and analysis of various motion 
parameters of observed objects and models, as well as active 
collaboration between different web users. Our goal was not 
only to develop an application that will provide access to 
data for observed models but also to provide a positive user 
experience and good usability of the system [11]. Based on 
the positive experience with WebGL API reported in the 
Related Work section, we selected this technology to 
develop the user interface in a browser. 
 
A. Our research contribution 
To our knowledge, this is the first example of the 
visualization and manipulation of motion capture data on a 
web platform supporting a high number of simultaneous 
users. The research hypothesis of our work is that by 
enabling the instructors and students to work with motion 
capture 
models 
through 
web-based 
clients, 
the 
teaching/learning experience will be greatly enhanced. The 
main field of use of the application is academia and 
education where it can be used as collaborative tool for 
teaching motion tracking system techniques and data 
processing methods. 
In the rest of the paper, we describe the proposed system 
and the corresponding user interface. We also report on a 
user study performed to evaluate the usability and user 
experience of our web application in comparison to the 
original desktop motion capture software. The results of the 
experiment are presented and statistically analysed. The 
Discussion section summarizes the most important findings 
and proposes some ideas for future work. 
III. 
SYSTEM ARCHITECTURE 
A. Qualysis Track Manager 
In our research, we use the professional motion capture 
system Qualisys [12]. The system consists of eight high-
speed cameras, a set of passive markers and the proprietary 
tracking software called Qualisys Track Manager (QTM) 
[13].  
This is a complex desktop application, which calculates the 
exact 3D position based on separate 2D images from all 
cameras. It also takes care of infrared (IR) cameras 
calibration, motion capture recording, creating and editing 
models, analyzing data on models, streaming captured 
motion, etc. It runs on a standard PC and exchanges data 
with the cameras trough a standardized Ethernet protocol. 
QTM shows the 3D position of each marker in a 
Cartesian coordinate system as a coloured dot. Individual 
markers or a group of markers can be labelled and connected 
to a structure called “model”. Each pair of markers with 
constant inter-distance can be connected with a line called 
bone (due to its rigid structure). When tracking the motion of 
humans the QTM bones correspond to the bones of human 
body.  
The created model, which consists of a set of markers 
and bones, can be saved for future measurements as an 
Automatic 
Identification 
of 
Markers 
(AIM) 
model.  
However, the visualization and analysis of the stored AIM 
model can be done only on a single computer running the 
QTM software. Figure 1 shows a screenshot of QTM with an 
example of AIM model (the model represents an upper part 
of human body). 
 
 
 
Figure 1. The visualization of an AIM model in QTM software. 
 
QTM is a very complex tool, which supports a high variety 
of commands and features. The user interface is therefore 
rather complicated and not very intuitive. It is primarily 
intended for controlling the cameras and not for the analysis 
of the recorded data. The latter is rather limited and cannot 
be saved or exported in a way that it could be used in 
different application. Since it does not allow simultaneous 
work of a larger number of users it is inappropriate to use it 
as an academic tool. 
B. Web Based E-learning Tool 
The architecture of the web application is divided into 
three levels as shown in Figure 2 [14]. First, data on marker 
coordinates is streamed from QTM and stored to a special 
buffer on a Node.JS web server [15].  
132
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-382-7
ACHI 2015 : The Eighth International Conference on Advances in Computer-Human Interactions

 
Figure 2. The web application system architecture [14]. 
 
Communication between the QTM and the web server is 
based on real-time (RT) protocol (Qualisys’ proprietary 
protocol for streaming data in real time). Server acts as a 
hub and translates raw data to JavaScript Object Notation 
(JSON) and broadcasts it to all connected clients through a 
WebSocket protocol [16]. The latter allows full-duplex 
communication between a server and a client and its packet 
headers are smaller than HTTP's.  
The client side application is divided into two main 
modules. WebSocket module is used for communication 
with the server while the WebGL module is responsible for 
rendering and interaction with 3D space through navigation 
panels. The WebSocket module reads the server data stream 
and updates the local storage when new marker data is 
received. It is built to be as lean as possible because both 
modules run in the same thread and the goal is not to block 
the WebGL module while it is rendering 3D data. WebGL 
module on the other hand is constantly rendering 3D space 
and exposing it in canvas HTML5 element. Constant 
rendering is needed for smooth user interaction with 3D 
space. When user moves, jaws, pitches or zooms in/out the 
3D scene only the view matrix is recalculated and 
transformed. On each render loop, the markers’ coordinates 
are read from local storage and the bone data is loaded from 
the model. Finally, based on the view matrix the scene-
space is transformed to view-space. 
 
1) User interaction with 3D space 
The main goal of web application is the visualization of 
3D space with markers and corresponding AIM models. 
Markers are coloured in colours defined in QTM application 
to help the user to differentiate between groups of markers. 
User can interact with 3D space using the mouse. While 
holding a mouse button and dragging a cursor user can 
rotate 3D space. If user drags the cursor horizontally, 3D 
space rotates around Z axis. If user drags the cursor 
vertically, 3D space moves around X or Y axis depending of 
Z rotation. User can also zoom in or zoom out by turning a 
mouse wheel. This command can also be initiated by using 
keyboard keys “page up” and “page down” in case user's 
mouse lacks the wheel. User can select a single marker by 
clicking it or an array of markers by holding “ctrl” key on 
keyboard while clicking several individual markers 
consequently. The click on an already selected marker will 
deselect it. 
 
2) Creation of a model 
A model can be thought as an undirected graph where 
markers represent nodes and bones between markers 
represent edges. In the web application, we tried to design a 
process of creating models as seamless as possible. To 
create a bone a user must select two individual markers in 
3D space. The bone is initialized by clicking the BONE 
button on the right bottom section of the dashboard. Only 
one bone can exist between the two individual markers. If 
multiple markers are selected in a consequential order 
multiple bones will be created between these markers.  
When all the bones in the model are created a user can save 
the model by selecting the corresponding command in the 
models navigation panel and defining its name. Models can 
be saved, loaded to a set of markers in 3D space or deleted.  
The model navigation panel is positioned in the upper 
right corner of the screen and it is divided in two subpanels. 
It allows a quick overview of all saved models and 
interaction with models. It features all possible actions a 
user can execute on the currently loaded model. The 
screenshot of the application is shown in Figure 3. 
 
 
 
Figure 3. The web application with simple model and marker position 
analysis chart. 
 
3) Manipulation with models and analysis 
At each time, only currently available actions are visible 
in the panel depending on the current state of the model. 
The goal was to reduce the number of menus and settings in 
order to simplify the user interface and increase its 
intuitiveness. When, for example, only one marker is 
selected the only available action is the analysis of its 
position in space as a function of time. When two markers 
133
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-382-7
ACHI 2015 : The Eighth International Conference on Advances in Computer-Human Interactions

are selected the length between these two markers can be 
analyzed as well their individual positions in space. If two 
bones connected by a shared common marker are clicked 
and selected, it is possible to analyze an angle between those 
two bones as a function of time. 
Data from all types of analysis are presented in an 
interactive chart. The X-axis shows time frames and the Y-
axis shows the corresponding distance or angle (the unit is 
therefore expressed in millimetres or angular degrees). The 
exact value of the individual time frame can be extracted by 
dragging the timeline bar across time frames and locating 
the desired frame. Figure 4 shows two examples of analysis 
charts for an angle (in degrees) between two bones and for a 
position (in millimetres) of a selected marker in space 
respectively. 
 
 
 
Figure 4. An example of two analysis charts showing the angle 
between two bones (upper figure) and position of a marker in space (lower 
figure) respectively. 
 
All analysis’ results can be exported by selecting the 
EXPORT action. The supported output formats are CSV 
(Comma separated Values) and JPG image. The CSV 
format can then be imported by majority of available third 
party software. 
 
4) User collaboration and data synchronization 
The most important feature of the proposed e-learning 
tool is the possibility of simultaneous work and 
collaboration of high number of users. A typical scenario is 
when a lecturer demonstrates various motion capture 
techniques and scenarios by operating QTM software and 
IR cameras in real time.  Students participate in the 
experiment at the same location or remotely through an 
internet connection. They can use any stationary or mobile 
device with a web browser supporting WebGL API. They 
all work on the same stream of data (same set of markers) 
but the interaction with the content such as creation of 
models and corresponding analyses are individualized.  
Additional feature of the application is a collaboration 
tool, which provides methods for synchronization of users’ 
data. All users connected to the same session can 
synchronize their local AIM models. Each user can propose 
and send his or her model to other session members or load 
the model proposed by other members. In this way, students 
have an option to participate actively in the manipulation of 
data or to be just passive observers. In the second case, a 
model, which is a subject of analysis, is built and sent to 
their application by other user (e.g., the lecturer). However, 
they can still fully manipulate the view of the 3D scene and 
interact with the model. 
IV. 
USER STUDY 
In order to test the proposed e-learning tool and to 
evaluate its effectiveness and intuitiveness, we conducted a 
user study in which we compared the tool with the original 
QTM application. The two applications represented two 
independent variables of the experiment. The three 
dependent variables were:  
 
Task completion time  or the time required to  solve 
given tasks; 
 
Subjective evaluation of the applications assessed 
with 
User 
Experience 
Questionnaire 
(UEQ) 
questionnaire; 
 
General subjective remarks given by the participants.   
 
A. Participants 
A total of 15 students of multimedia, electrical 
engineering and computer science participated in the user 
study. The subjects ranged from 20 to 28 years of age 
(M=23.1 years, SD=2.4 years). All participants reported 
normal eye sight, except for one who was wearing glasses 
due to myopia. Eight participants had prior experience with 
the visualisation and manipulation of 3D data (applications 
such as Blender, Google SketchUp, SolidWorks, etc.).  
They were randomly distributed into two experiment 
conditions described bellow. None of them had any 
experience with motion capture systems whatsoever. 
 
B. Experiment design 
The experimental design was a between-subject, dividing 
participants into two groups to avoid sequence effects and 
confounds (due to the similarity of commands and controls 
in both application). The first group (8 test subjects) 
performed the set of selected tasks with the proposed web 
application while the second group (7 test subjects) 
performed same tasks with the QTM application. Prior to 
the experiment the participants were given a short 
explanation on how motion capture system works and about 
the purpose of the application they were about to use. They 
were also given approx. five minutes to get familiar with the 
application and its interface. Each participant conducted the 
experiment individually following the instructions of the 
experiment leader.  
In the first part of the experiment, each participant was 
asked to perform 3 different tasks: 
 
Build a model by connecting markers with bones 
(T1); 
 
Save and load the created model (T2); 
 
Perform an analysis on the built model (T3); This 
task was broken down to 5 subtasks: 
- 
Analyze position of the right elbow; 
- 
Analyze length of the right upper arm; 
134
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-382-7
ACHI 2015 : The Eighth International Conference on Advances in Computer-Human Interactions

- 
Analyze the angle of the right elbow; 
- 
Read the exact value of the previously 
analyzed angle for the specific time 
frame; 
- 
Save the analyzed data to a file in raw 
format. 
In the second part of the experiment, the participants 
from both groups had to fill in a standardized UEQ and 
evaluate the used application [11]. The UEQ questionnaire 
assesses the participants’ feelings, impressions and attitudes 
towards the tested application. The questionnaires were 
translated into Slovenian language – the native language of 
the participants.  
In the third and the final part of the experiment, the 
participants were asked to give their opinion and general 
remarks about the tested application. Any comment or 
remark about the application at any stage of the experiment 
was also registered by the experiment leader.  
All computers running QTM, NodeJS server and web 
browser were connected through a local network providing 
high bandwidth. As a consequence, the page response and 
loading times as well as latencies on the network were very 
low and did not affect evaluation procedure. 
 
V. 
RESULTS 
The three variables evaluated in the user study were the 
following: 
 Task completion times; 
 UEQ; 
 Subjective comments about the applications and 
user interfaces. 
A. Task completion times 
The time required to complete the individual task was 
measured manually by the experiment leader. A timer was 
started just after the experiment leader would read the 
instructions for the selected task and stopped after when the 
participant was comfortable with the result achieved for that 
task. Figure 5 shows all task completion times for both 
applications. The between subject ANOVA and the post-hoc 
Bonferroni tests with a 0.05 limit on family wise error rate 
were used for the comparison of data (the normal 
distribution of data was confirmed with Shapiro-Wilk test). 
The proposed web application seemed to be slower than 
the original QTM application for creating bones and 
building models (task T1). However, no statistically 
significant differences were found in this task (F(1,12) = 
3.733, p = 0.077). There was also no significant difference 
for T2 (F(1,12) = 2.037, p = 0.179) and for T3.1 (F(1,12) = 
4.505,  p = 0,055). For all the other tasks the proposed web 
application outperformed the QTM application: 
T3.2: F(1,12) = 20.618, p = 0.001; 
T3.3: F(1,12) = 15.826, p = 0.002; 
T3.4: F(1,12) = 43.153, p < 0.001; 
T.3.5: (F(1,12) = 14.182, p = 0.003; 
 
 
 
Figure 5. Average task completion times (in seconds) with confidence 
intervals. 
 
The standard deviation is smaller and the corresponding 
confidence intervals are narrower for all tasks performed 
with the proposed web application. We believe this reflects 
intuitiveness, reliability and robustness of the proposed web 
interface. 
B. UEQ 
After completing the set of tasks, the participants were 
asked 
to 
complete 
the 
UEQ 
questionnaire. 
The 
questionnaire consists of 26 individual statements, which 
are graded with a seven stage Likert scale to reduce the 
well-known central tendency bias for such types of items 
[18]. The results of these grades are grouped to 6 different 
categories 
(attractiveness, 
perspicuity, 
efficiency, 
dependability, stimulation and novelty).  Figure 6 shows 
average UEQ scores for individual categories.  
 
 
 
Figure 6. Average UEQ values for six categories. 
 
The mean scores of the proposed web application were 
higher in all six categories. Again due to the normal 
distribution of data, the ANOVA and post-hoc Bonferroni 
tests with a 0.05 limit on family wise error rate were used 
for comparison. The statistically significant difference 
between the applications was found only in the categories 
attractiveness, perspicuity and dependability:  
135
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-382-7
ACHI 2015 : The Eighth International Conference on Advances in Computer-Human Interactions

Attractiveness:  F(1,12) =11.703, p = 0.005; 
Perspicuity: F(1,12) = 11.456, p = 0.005; 
Dependability: F(1,12) = 12.928, p = 0.004; 
Efficiency: F(1,12) = 2.001, p = 0.183; 
Stimulation: F(1,12) = 3.123, p = 0.103; 
Novelty: F(1,12) = 0.795, p = 0.390; 
C. General remarks 
After each experiment, we interviewed the subjects 
about their experience. We were primarily interested in 
some general comments and remarks about the proposed 
web application. In this section, we list some comments and 
suggestions for improvement expressed by several test 
subjects: 
 markers were too small and hard to select; 
 there should be more keyboard shortcuts for some 
commonly used actions; 
 some descriptions of results of analyses are 
unclear; 
 double-click could be used to list available 
commands and actions at any time; 
 etc. 
 
Some participants who had experiences with 3D 
applications and 3D interfaces reported to have problems 
using the current interface due to new interaction methods 
and metaphors introduced in the software. 
VI. 
DISCUSSION AND CONCLUSION 
The main goal of this user study was an evaluation of the 
proposed and developed web application to reveal its 
advantages and disadvantages in comparison with an 
existing QTM application. We aimed to prove that users’ 
actions can be performed faster and more intuitively with 
such task-specific application compared to a general-use 
application. The final scope of functionalities and features 
for the analysis of the captured data is comparable in both 
applications as they both require similar complexity of 
analysis and manipulation of the recorded data. Since the 
proposed application runs on a web platform as an e-
learning tool, it supports simultaneous use of high number 
of users as well as their collaboration and exchange of 
knowledge. 
The user interface of a web application proved to be 
equally fast or faster for a majority of tasks. However, no 
significant difference was found in T1 in which users were 
asked to build bones and an AIM model for further analysis. 
This result was not expected since the new interface was 
improved with additional commands and tools enabling the 
creation of multiple bones at once. Since no specific 
instructions were given about these tools only a few users 
explored and effectively used these features.   
For the T2 and T3 the web application was faster than the 
QTM application, which was an expected outcome. Our 
application was built primarily to enable various analyses on 
a model, such as, for example: detection of marker position, 
identification of length of a bone, an angle between two 
bones, etc. The user interface was therefore optimized to 
support these actions and to make them intuitive. On the 
other hand, in the original QTM application the analyses 
and the corresponding actions are just a small set of 
available features and several users have difficulties finding 
them among all other actions and menu commands. 
The second evaluated parameter in the study was the 
subjective perception of both applications measured through 
UEQ test. The web application was judged to be 
significantly more attractive (attractiveness category), 
understandable and clear (perspicuity category) and 
dependable (dependability category). We believe these high 
scores reflect simple, clear and intuitive user interface, 
which adapts to user actions and changes its state and a set 
of available controls depending on what the user is currently 
doing. The lower score on the other hand were given in the 
category novelty.  There was no significant difference found 
between our web application and the original QTM 
application. We believe these scores reflect the facts that 
majority of test subjects had no or very few experiences 
with motion tracking techniques and visualizations of 3D 
data. The main part of the user interface was a simple 3D 
visualization of a Cartesian coordinate system with a set of 
coloured points (markers) at different positions, which the 
user did not find very novel or exciting. We also believe a 
more significant difference could be found if a within 
subject test was performed enabling the users a direct 
comparison of both systems and user interfaces. 
The collected set of subjective comments revealed some 
problems and ambiguities of the proposed interface as well 
as missing commands and potential extensions. Several 
comments referred also to visualization problems, which are 
related to the WebGL API and their improvement is out of 
our power. These comments will be used primarily to 
improve the proposed interface in terms of its efficiency and 
clearness and its upgrade with new features and commands. 
Our future goal is to extend the set of available features 
for the analysis of motion data in the applications as well as 
the features related to remote collaboration and exchange of 
data. Another important module which is currently not 
available and should be implemented in the future is a 
common platform supporting predefined learning processes 
and tasks, the authentication of users, the creation and 
storage of the users’ profiles, the monitoring of learning 
progress, etc.  
We believe our application demonstrates the high 
usability of modern web technologies for the development 
of new powerful and rich services in the e-learning domain. 
Real time streaming of complex 3D data and their 
visualization in an interactive scene in a browser are an 
excellent use case for many other similar services. In the 
future, the impact of the proposed e-learning software on the 
learning performance and methodology should also be 
evaluated. 
136
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-382-7
ACHI 2015 : The Eighth International Conference on Advances in Computer-Human Interactions

REFERENCES 
[1] P. Sun, R. J. Tsai, G. Finger, Y. Chen, and D. Yeh, 
“What drives a successful e-Learning? An empirical 
investigation of the critical factors influencing learner 
satisfaction,” Computers & Education, vol. 50, issue 4, 
May 2008, pp. 1183-1202. 
[2] A. 
Anyuru, 
Professional 
WebGL 
Programming: 
Developing 3D Graphics for the Web, John Wiley & 
Sons, Apr 23, 2012. 
[3] P.-A. Fayolle, B. Schmitt, Y. Goto, and A. Pasko, “Web-
based constructive shape modeling using real distance 
functions,” IEICE - Trans. Inf. Syst., vol. E88-D, no. 5, 
May 2005, pp. 828–835.  
[4] R. Cartwright, V. Adzhiev, A. A. Pasko, Y. Goto, and T. 
L. Kunii,“Web-based shape modeling with hyperfun,” 
IEEE Computer Graphics and Applications, vol. 25, no. 
2, 2005, pp. 60–69. 
[5] Q. Liu and A. Sourin, “Function-defined shape 
metamorphoses in visual cyberworlds,” Vis. Comput., 
vol. 22, no. 12, Nov. 2006, pp. 977–990. 
[6] J. Behr, P. Eschler, Y. Jung, and M. Zollner, “X3dom: a 
dom-based html5/x3d integration model” in Proceedings 
of the 14th International Conference on 3D Web 
Technology, 2009, pp. 127–135. 
[7] WebGL - OpenGL ES 2.0 for the Web. Available from: 
http://www.khronos.org/webgl/, 2014.10.03 
[8] J. Conote, A. Segura, L. Kabongo, A. Moreno, J. Posada, 
and O. Ruiz, “Interactive visualization of volumetric data 
with WebGL in real-time” in 3D Web Technology, 2011 
pp. 137-146. 
[9] C. Leung and A. Salga, “Enabling WebGL”, World wide 
web, 2010, pp.1369-1370. 
[10] C. Schwartz, R. Ruiters, M. Weinmann, and R. Klein, 
“WebGL-based streaming and presentation framework 
for bidirectional texture functions”, Virtual Reality, 
Archaeology and Cultural Heritage, 2011, pp. 113-120. 
[11] J. Nielsen, “Usability 101: Introduction to Usability”. 
Available from: 
http://www.nngroup.com/articles/usability-101-
introduction-to-usability/, 2014.10.10 
[12] Qualysis Motion Capture Systems. Available from: 
http://www.qualisys.com/, 2014.10.10 
[13] QTM – Qualisys Track Manager, User Manual, 2011. 
[14] A. Krašček and J. Sodnik, “Qualisys Web Tracker – A 
web-based visualization tool for real-time data of an 
optical tracking system”, ICIST 2014 Proceedings, 2014, 
pp. 155-160. 
[15] S. Tilkov and S. Vinoski. "Node. js: Using JavaScript to 
build high-performance network programs." IEEE 
Internet Computing, vol. 14, issue 6, 2010, pp. 80-83. 
[16] The 
WebSocket 
Protocol. 
Available 
from: 
https://tools.ietf.org/html/rfc6455, 2014.10.06 
[17] What is the UEQ? Available from:  http://www.ueq-
online.org/, 2014.10.06 
[18] B. Laugwitz, T. Held, and M. Schrepp, “Construction 
and Evaluation of a User Experience Questionnaire”. 
Available in: Holzinger, A. (Ed.): USAB 2008, LNCS 
5298, pp. 63-76. 
 
 
 
 
 
137
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-382-7
ACHI 2015 : The Eighth International Conference on Advances in Computer-Human Interactions

