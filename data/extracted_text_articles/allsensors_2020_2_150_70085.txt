Normal Distributions Transform-Based Mapping Using Scanning LiDAR  
Mounted on Motorcycle  
 
Kota Matsuo, Akihiko Yoshida 
Graduate School of Science and Engineering 
Doshisha University 
Kyotanabe, Kyoto, Japan 
e-mail: {ctwd0130, ctwf0158}@mail4.doshisha.ac.jp 
Masafumi Hashimoto, Kazuhiko Takahashi 
Faculty of Science and Engineering 
Doshisha University 
Kyotanabe, Kyoto, Japan 
e-mail: {mhashimo, katakaha}@mail.doshisha.ac.jp 
Abstract—This paper presents a 3D point cloud mapping 
method for Global Navigation Satellite Systems (GNSS)-denied 
and dynamic environments using a scanning multilayer Light 
Detection And Ranging (LiDAR) mounted on a motorcycle. The 
distortion in the scan data from the LiDAR is corrected by 
estimating the motorcycle’s pose (3D positions and attitude 
angles) in a period shorter than the LiDAR scan period based 
on the information from Normal Distributions Transform 
(NDT) scan matching and an Inertial Measurement Unit (IMU). 
The corrected scan data are mapped onto an elevation map. The 
static and moving scan data, which originate from static and 
moving objects in the environments, respectively, are classified 
using the occupancy grid method. Only the static scan data are 
applied to generate a point cloud map using NDT-based 
Simultaneous Localization And Mapping (SLAM). The 
experimental results obtained in an urban road environment 
demonstrate the qualitative effectiveness of the proposed 
method. 
Keywords—motorcycle; 
LiDAR; 
NDT-based 
SLAM; 
distortion correction; dynamic environment. 
I.  INTRODUCTION 
In recent years, numerous studies were conducted on the 
active safety and autonomous driving of vehicles and 
personal mobility devices, while many also focused on last-
mile automation by delivery robots. Important technologies 
from these studies include environmental map generation 
(mapping) [1]–[3]. In this study, we focus on mapping with a 
Light Detection And Ranging (LiDAR) mounted on a vehicle.  
Within the domain of Intelligent Transportation Systems 
(ITS), maps are generated using mobile mapping systems [4]. 
These maps are applied to the areas of autonomous driving 
and active safety for automobiles in wide road environments, 
such as highways and motorways. In this study, we focus on 
environment maps for the active safety and autonomous 
driving of personal mobility devices and delivery robots as 
well as for various social services such as disaster prevention 
and mitigation [5][6].  
In the process, we address generating 3D point cloud 
maps for narrow road environments, such as community 
roads and scenic roads in urban and mountainous areas, using 
LiDAR mounted on two-wheeled vehicles (e.g., bicycles and 
motorcycles) with higher maneuverability than four-wheeled 
vehicles (e.g., cars and buses). To generate 3D point cloud 
maps using LiDAR-based Simultaneous Localization And 
Mapping (SLAM), the LiDAR data captured in the sensor 
coordinate frame must be accurately mapped onto the world 
coordinate frame using the pose (i.e., position and attitude 
angle) information of the vehicle. Since LiDAR generally 
obtains data via laser scanning, all the scan data within one 
scan cannot be obtained at the same time when a vehicle is 
moving or is changing its attitude. Therefore, if such data are 
transformed based on the vehicle’s pose at the same time, 
distortion appears on the environmental maps.  
To reduce the distortion in the scan data, several methods 
were proposed [7]–[10]. Most conventional methods were 
aimed at correcting the distortion in the scan data from a 
LiDAR mounted on four-wheeled vehicles moving on flat 
road surfaces. To the best of our knowledge, very few studies 
exist that addressed distortion correction when vehicles 
change their poses drastically.   
Thus, we proposed a Normal Distributions Transform 
(NDT)-based SLAM for application in Global Navigation 
Satellite Systems (GNSS)-denied environments using a 
scanning LiDAR mounted on two-wheeled vehicle that 
change their pose drastically [11]. The pose of the two-
wheeled vehicle was calculated via NDT scan matching [12] 
using the LiDAR scan data obtained for each scan period. By 
estimating the vehicle’s pose in a period shorter than the scan 
period via an Inertial Measurement Unit (IMU), the distortion 
in the scan data was corrected. The corrected scan data were 
applied to 3D point cloud maps. The experimental results of 
the road-environment mapping by a scanning LiDAR 
mounted on a bicycle under a zigzag motion validated the 
efficacy of the proposed method. 
However, further improvement to our NDT-based SLAM 
is needed. While it was designed to be used in static 
environments, moving objects such as cars, buses, two-
wheeled vehicles, and pedestrians exist in practical 
environments. The presence of moving objects in practical 
dynamic environments deteriorates the mapping performance. 
This problem can be addressed by integrating SLAM with 
moving-object detection. Many SLAM methods were 
presented for use in dynamic environments [2][13]–[15]. 
However, most of the methods were based on the use of a 
sensor mounted on a four-wheeled vehicle.  SLAM with a 
sensor mounted on a two-wheeled vehicle in dynamic 
environments remains a challenging issue. 
Thus, in this paper, we present an NDT-based SLAM 
using a scanning LiDAR mounted on a motorcycle in GNSS- 
denied and dynamic environments. This paper is an extension 
of our previous paper [15] on NDT-based SLAM, which used 
a scanning LiDAR mounted on a four-wheeled vehicle within  
69
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-766-5
ALLSENSORS 2020 : The Fifth International Conference on Advances in Sensors, Actuators, Metering and Sensing

 
Figure 1.  Overview of experimental bicycle. 
 
similar environments. The rest of this paper is organized as 
follows. Section II describes the experimental system. 
Section III summarizes the scan-data mapping based on the 
NDT scan matching. Section IV explains the distortion 
correction method for the LiDAR scan data, and Section V 
presents the extraction method for the static scan data from 
the LiDAR. Section VI explains the experiments conducted 
to verify the proposed method, followed by the conclusions 
and future work in Section VII. 
II. EXPERIMENTAL SYSTEM 
Figure 1 shows the overview of the motorcycle (Honda 
Gyro Canopy). A scanning 32-layer LiDAR (Velodyne HDL-
32E) and an IMU (Xsens MTi-300) are mounted on the upper 
part of the motorcycle. The maximum range of the LiDAR is 
70 m, the horizontal viewing angle is 360° with a resolution 
of 0.16°, and the vertical viewing angle is 41.34° with a 
resolution of 1.33°. The LiDAR provides 384 measurements 
(the object’s 3D position and reflection intensity) every 0.55 
ms (at 2° horizontal angle increments). The time that the 
LiDAR beam takes to complete one rotation (360°) in the 
horizontal direction is 100 ms, and 70,000 measurements are 
obtained in one rotation.  
The IMU provides the attitude angles (roll and pitch 
angles) and the angular velocities (roll, pitch, and yaw 
velocities) every 10 ms with an attitude angle error of ±0.3° 
(typ.) and an angular velocity error of ±0.2 °/s (typ.). 
In this paper, one rotation of the LiDAR beam in the 
horizontal direction (360°) is referred to as one scan, while 
the data obtained from this scan is referred to as scan data. 
The LiDAR’s scan period (100 ms) is denoted as 
 and the 
scan data observation period (0.55 ms) as 
. The 
observation period (10 ms) of the IMU is denoted as 
IM U , 
which means the IMU data are obtained 10 times in one scan 
of the LiDAR (τ = 10ΔτIMU), while the LiDAR scan data are 
obtained 18 times within the observation period of the IMU 
(ΔτIMU =18Δτ). 
III. 
NORMAL DISTRIBUTIONS TRANSFORM  
SCAN MATCHING 
The scan data mapping is based on NDT scan matching 
[12]. For clarity, we explain the NDT scan matching using the 
LiDAR scan data in which the distortion is corrected. The 
distortion correction method is detailed in the following 
section. 
A voxel grid filter is applied to downsize the scan data. 
The voxel used for the filter is a tetrahedron with a side length 
of 0.2 m. In the world coordinate frame, 
W , a voxel map 
with a voxel size of 1 m is used for NDT scan matching. For 
the i-th (i = 1, 2, …n) measurement in the scan data, the 
position vector in 
b  is defined as 
bi
p  and that in
W  as 
ip . 
Thus, the relation is given by the following homogeneous 
form: 
1
)
(
1
bi
i
p
Τ X
p
                                    (1) 
where 
T
x y z
( , , , , , )
X
  is the motorcycle’s pose,  
x y z T
( , , )
  and 
T)
( , ,
  are the 3D position and attitude 
angle (roll, pitch, and yaw angles) of the motorcycle, 
respectively, in 
W  . T(X) is the following homogeneous 
transformation matrix: 
1
0
0
0
cos
cos
cos
sin
sin
cos
sin
sin
cos sin
cos
cos
sin
sin
sin
sin
cos
sin
sin
cos
cos sin
cos sin
cos
sin
sin
cos
cos
)
(
z
y
x
Τ X
 
The scan data obtained at the current time t  (t = 0, 1, 2, 
…), 
( )
2 ( )
1( )
( )
,
,
,
bn t
t
b
t
b
b t
p
p
p
P
  or 
,
{
1( )
( )
t
t
p
P
  
,
( ,)
p2 t
 
np (t)}
, are referred to as the new input scan, and the scan data 
obtained 
in 
the 
previous 
time 
before 
(t 1)
 , 
1)
(
(1)
(0)
,
,
,
P t
P
P
P
, are referred to as the reference scan. 
NDT scan matching involves conducting an NDT for the 
reference scan data in each grid on a voxel map and 
calculating the mean and covariance of the LiDAR 
measurement positions. By matching the new input scan at 
t  with the reference scan data obtained prior to 
)1
(t
, the 
motorcycle’s pose 
X (t)
  at t   can be determined. The 
motorcycle’s pose is used for conducting a coordinate 
transform with (1), the new input scan can then be mapped to 
W , and the reference scan is updated.  
In this study, we use the point cloud library for the NDT 
scan matching [16]. 
IV. 
DISTORTION CORRECTION OF LIDAR SCAN DATA  
A. Motion and Measurement Models 
As shown in Figure 2, the linear velocity of the 
motorcycle in 
b  is denoted as Vb (the velocity in the xb-axis 
direction), and the angular velocities around the xb-, yb-, and 
zb-axes are denoted as 
b , 
b , and 
b , respectively.  
If the motorcycle is assumed to move at nearly constant 
linear and angular velocities, a motion model can be derived 
as (A.1) in the Appendix. We express (A.1) in the following 
vector form: 
,
,)
(
1)
(
w
f ξ
ξ
t
t
                                  (2) 
70
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-766-5
ALLSENSORS 2020 : The Fifth International Conference on Advances in Sensors, Actuators, Metering and Sensing

 
Figure 2.  Notation related to motorcycle motion.  
 
 
Figure 3.  Flow of distortion correction. 
 
The attitude angle and angular velocity of the motorcycle 
obtained at time 
t IM U
by the IMU is denoted as 
( )
zIMU t
. 
The measurement model is then  
( )
( )
( )
IMU
IMU
IMU
t
t
t
z
H
ξ
z
                        (3) 
where 
zIMU
 is sensor noise, and HIMU is a measurement 
matrix. 
The motorcycle’s pose obtained at t
  using the NDT 
scan matching is denoted as 
( )
( )
ˆ t
NDT t
X
z
. The measurement 
model is then 
( )
( )
( )
NDT
NDT
NDT
t
t
t
z
H
ξ
z
                       (4) 
where 
zNDT
  is the measurement noise, and HNDT is the 
measurement matrix. 
B. Distrotion Correction 
Figure 3 shows the correction flow of the LiDAR scan 
data [11]. The LiDAR’s scan period 
  is 100 ms, the 
observation period 
IM U  of the IMU is 10 ms, and the scan 
data observation period 
 is 0.55 ms. When the scan data 
are mapped onto 
W  using the motorcycle’s pose, which is 
calculated for every LiDAR scan period, distortion appears 
on the environmental map. Therefore, the distortion in the 
LiDAR scan data is corrected by estimating the motorcycle’s 
pose using the Extended Kalman Filter (EKF) for every scan 
data observation period 
. 
The IMU data are obtained 10 times per LiDAR scan (τ = 
10ΔτIMU). The state estimate of the motorcycle and its error 
covariance obtained at the time (
1)
(
1)
IMU
t
k
, where k 
=1–10, using EKF are denoted as 
(
1) (
1)
ˆ k
t
ξ
  and 
(
1) (
1)
k
t
Γ
 , 
respectively. From these estimates, the EKF prediction 
algorithm gives the state prediction 
( /
1) (
1)
ˆ k k
t
ξ
 and the error 
covariance 
(
/
1) (
1)
k k
t
Γ
 at 
)1
(t
+
IMU
k
 by 
( /
1)
(
1)
( /
1)
(
1)
ˆ
ˆ
(
1)
[
(
1),0,
]
(
1)
(
1)
(
1) (
1)  
(
1)
(
1)
k k
k
IMU
k k
k
T
T
t
t
t
t
t
t
t
t
ξ
f ξ
Γ
F
Γ
F
G
QG
 
        (5) 
where F = 
ξ
f
/ ˆ
 , G = 
f / w
 , and Q is the covariance 
matrix of the plant noise w. 
At (
1)
IMU
t
k
, the attitude angle and angular 
velocity 
zIMU
 of the motorcycle are observed with the IMU. 
Then, the EKF estimation algorithm gives the state estimate 
(
) (
1)
ˆ k
t
ξ
 and its error covariance 
( ) (
1)
k
t
Γ
 as follows:  
( )
( /
1)
( /
1)
( )
( /
1)
( /
1)
ˆ
ˆ
ˆ
(
1)
(
1)
{
(
1)}
(
1)
(
1)
(
1)
k
k k
k k
IMU
IMU
k
k k
k k
IMU
t
t
t
t
t
t
ξ
ξ
K z
H
ξ
Γ
Γ
KH
Γ
   (6) 
where 
( /
1)
1
(
1)
(
1)
k k
T
t
t
IMU
K
Γ
H
S
 
 and 
( /
1) (
1)
k k
T
t
IMU
IMU
IMU
S
H
Γ
H
R
  . RIMU is the covariance 
matrix of the sensor noise ΔzIMU. 
In the state estimate 
(
) (
1)
ˆ k
t
ξ
 , the elements related to 
the motorcycle’s pose 
( , , , , , )
x y z
 are denoted as 
( ) (
1)
ˆ
k
t
X
. Since the observation period 
IM U  of the IMU is 
10 ms, and the scan data observation period 
 is 0.55 ms, 
the LiDAR scan data are obtained 18 times within the IMU 
observation period (ΔτIMU =18Δτ). Using the pose estimates 
(
1) (
1)
ˆ k
t
X
 and 
(
) (
1 )
ˆ
k
t
X
 obtained 
at 
(
t 1)
 
(
1)
IMU
k
and 
)1
(t
+
IMU
k
, respectively, the 
motorcycle’s pose 
(
1) (
1, )
ˆ k
t
j
X
 at 
)1
(t
+ (
1)
IMU
k
jΔτ, 
where j = 1–18, can be interpolated by 
( )
(
1)
(
1)
(
1)
ˆ
ˆ
(
1)
(
1)
ˆ
ˆ
(
1, )
(
1)
k
k
k
k
IMU
t
t
t
j
t
j
X
X
X
X
  (7) 
Using (1) and the pose prediction 
(
1) (
1, )
ˆ k
t
j
X
, the scan 
data 
(
1) (
1, )
k
t
j
pbi
 in 
b obtained at 
)1
(t
+ (
1)
IMU
k
jΔτ can be transformed to 
(
1) (
1, )
k
t
j
ip
 in 
W  as follows: 
(
1)
(
1)
(
1)
(
1, )
(
1, )
( ˆ
(
1, ))
1
1
k
k
k
i
bi
t
j
t
j
t
j
p
p
X
     (8) 
Since the IMU data are obtained 10 times per LiDAR scan 
(τ = 10ΔτIMU), the time t  is equal to (t-1)
+10 τIMU. Using 
the motorcycle’s pose estimate 
(10) (
1)
ˆ
t
X
 at t , the scan data 
(
1) (
1, )
k
t
j
ip
 in 
W  at 
)1
(t
+ (
1)
IMU
k
jΔτ are 
transformed to the scan data 
* ( )t
Pbi
  in 
at t
 by 
*
(
1)
(10)
1
( )
(
1, )
( ˆ
(
1))
1
1
k
bi
i
t
t
j
t
p
p
X
    (9) 
In 
such 
way, 
the 
corrected 
scan 
data  
*
*
*
*
( )
( )
( )
( )
1
2
,
,
,
t
t
t
t
b
b
b
bn
P
p
p
p
 within one scan (LiDAR beam 
b
71
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-766-5
ALLSENSORS 2020 : The Fifth International Conference on Advances in Sensors, Actuators, Metering and Sensing

rotation of 360° in a horizontal plane) are obtained and used 
as the new input scan for the scan matching to calculate the 
pose 
zNDT
 of the motorcycle at t
. Then, the EKF 
estimation algorithm is used to calculate the state estimate 
( )
ˆ t
ξ
 and its error covariance 
( )
Γ t
 of the motorcycle at t
 as 
follows: 
(10)
(10)
(10)
(10)
ˆ
ˆ
ˆ
( )
(
1)
( ){
( )
(
1)}
( )
(
1)
( )
(
1)
NDT
NDT
NDT
t
t
t
t
t
t
t
t
t
ξ
ξ
K
z
H
ξ
Γ
Γ
K
H
Γ
  (10) 
where, 
(10)
1
( )
(
1)
( )
T
t
t
t
NDT
K
Γ
H
S
,
S ( )t
  
(10) (
1)
T
t
NDT
NDT
NDT
H
Γ
H
R
, and 
RNDT
 is the covariance 
matrix of 
. 
The corrected scan data 
bP* ( )t
 are mapped onto 
W  using 
the pose estimate calculated by (10), and the distortion in the 
environmental maps can then be removed. 
V. REMOVAL OF MOVING SCAN DATA 
In dynamic environments where moving objects such as 
cars, motorcycles, and pedestrians exist, the LiDAR scan data 
related to moving objects (referred to as moving scan data) 
must be removed from the entire scan data, and only the scan 
data related to static objects (static scan data), such as 
buildings and trees, have to be utilized for the mapping. 
We previously studied moving-object detection and 
tracking in crowded environments [17][18]. This method is 
applied to extract the static scan data from the LiDAR scan 
data. 
To extract the static scan data, the LiDAR scan data are 
first classified into two types, scan data originating from road 
surfaces (road-surface scan data) and scan data originating 
from objects (object scan data), according to a rule-based 
method. The object scan data obtained in 
b  are mapped 
onto an elevation map represented in 
W  using the 
motorcycle’s pose calculated by (1). In this study, the cell of 
the elevation map is a square with a side length of 0.3 m.  
A cell containing scan data is referred to as an occupied 
cell. For moving scan data, the time to occupy the same cell 
is short, while for static scan data, the time is long. Therefore, 
using the occupancy grid method based on the cell occupancy 
time [17], the occupied cells are classified into two types of 
cell, moving and static, which are occupied by the moving 
and static scan data, respectively. Cells that the LiDAR 
cannot identify due to obstructions are defined as unknown 
cells, and their cell occupancy time is not counted.  
Since the scan data related to an object generally occupy 
multiple cells, adjacent occupied cells are clustered. Then, the 
scan data in clustered static cells are applied to the mapping. 
When moving objects such as vehicles pause (e.g., at a 
red light), the occupancy grid-based method often 
misidentifies their scan data as static scan data. To address 
this problem, the road-surface scan data are mapped onto the 
elevation map, and the cells in which the road-surface scan 
data are occupied for a while are regarded as the road-surface 
cells. If the road-surface cells contain the object scan data, the 
object scan data are always determined as moving scan data 
and are removed from the entire scan data. 
VI. EXPERIMENTAL RESULTS 
Mapping experiments were conducted within road traffic 
environments (Figure 4). Figure 5 shows photos of areas 1 and 
2, which are shown in Figure 4. The traveled distance of the 
motorcycle was around 2,886 m, while its maximum speed 
was 30 km/h. The motorcycle turned left six times and then 
tilted approximately 10° in a roll direction. 
 
 
 
Figure 4. Moved path of vehicle (top view).  
 
 
(a) Area 1 
 
 
(b) Area 2 
Figure 5. Photo of environment. 
 
 
Figure 6.  Mapping result (top view). 
NDT
z
72
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-766-5
ALLSENSORS 2020 : The Fifth International Conference on Advances in Sensors, Actuators, Metering and Sensing

 
(a) Photo 
 
 
(b) Case 1 
 
(c) Case 2 
 
 (d) Case 3 
 
Figure 7.  Mapping result of area 1 (bird’s-eye view). The red line in (a) 
indicates the movement path of the motorcycle. The blue and red dots in (b)–
(d) indicate the static and moving scan data, respectively.  
 
(a) Photo 
 
(b) Case 1 
 
(c) Case 2 
 
(d) Case 3 
 
Figure 8.  Mapping result of area 2 (bird’s-eye view). The red line in (a) 
indicates the movement path of the motorcycle. The blue and red dots in (b)–
(d) indicate the static and moving scan data, respectively. 
73
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-766-5
ALLSENSORS 2020 : The Fifth International Conference on Advances in Sensors, Actuators, Metering and Sensing

  
 
 
(a) Photo          (b) Case 1             (c) Case 2               (d) Case 3 
 
Figure 9.  Mapping result of a traffic signal in area 1. 
 
 
Figure 6 shows the mapping result using NDT-based 
SLAM in conjunction with distortion correction of the LiDAR 
scan data and the extraction of the static scan data.  
To evaluate the mapping performance in detail, enlarged 
maps of areas 1 and 2 in Figure 4 are compiled, as shown in 
Figures 7 and 8. Figure 7 presents the map when the 
motorcycle is turning left, while Figure 8 presents the map 
when the motorcycle is going straight. Figure 9 also shows the 
mapping result of a traffic sign in area 1, in which the 
motorcycle is turning left. In area 1, there are four cars and 
four pedestrians, and in area 2, there are four cars and three 
pedestrians. 
For comparison purposes, the maps were generated in 
terms of the following cases: 
Case 1: Mapping by the proposed method: NDT-based 
SLAM with the distortion correction of the LiDAR scan data 
and the extraction of the static scan data from the entire 
LiDAR scan data.  
Case 2: NDT-based SLAM with the distortion correction 
but without the extraction of the static scan data.  
Case 3: NDT-based SLAM without either method. 
 
As Figures 7 and 8 show, case 1 (proposed method) 
removes the track (red dots) of any moving objects (cars and 
pedestrians) more than cases 2 and 3. Meanwhile, as Figure 9 
shows, the mapping results obtained in case 1 are more crisp 
than those obtained in case 3. Thus, the conclusion exists that 
the proposed method provides a better mapping result than 
cases 2 and 3. 
 However, our method of moving-object detection exhibits 
one drawback. When, for example, cars slow down at an 
intersection, stop at a red light, or pause to turn left (or right), 
they are sometimes determined as static objects. Then, the 
LiDAR scan data that relate to cars partially remain on the 
mapping.  
VII. CONCLUSIONS AND FUTURE WORK 
This paper presented NDT-based mapping using a 
scanning LiDAR mounted on a motorcycle in GNSS-denied 
and dynamic environments. The distortion in the LiDAR scan 
data was corrected based on the information from the NDT 
scan matching and the IMU via EKF. The moving scan data 
were removed from the entire LiDAR scan data using the 
occupancy grid-based method, and the static scan data were 
applied to 3D point cloud maps using the NDT scan matching. 
The efficacy of the mapping was qualitatively demonstrated 
through experimental results obtained in road environments. 
We are currently conducting quantitative evaluations of the 
presented method in various environments.  
Some improvements to the presented method are 
required.   Since the distortion correction of the LiDAR scan 
data requires a great deal of computational time, Graphical 
Processing Unit (GPU) must be utilized in real-time 
operations. NDT-based SLAM degrades the mapping 
accuracy over time due to the accumulation error and must be 
integrated with Graph-based SLAM to reduce the drift. In 
addition, NDT-based SLAM with moving-object detection 
should be extended to SLAM with Detection And Tracking 
of Moving Objects (DATMO) for advanced rider assist 
systems. The SLAM DATMO approach will improve the 
mapping performance of both the mapping and the moving-
object tracking.  
APPENDIX: MOTION MODEL OF MOTORCYCLE 
( )
( )
( )
( )
1
(
1)
( )
( )
( )
( )
1
(
1)
( )
( )
( )
1
(
1)
2
3
4
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
cos
cos
cos
sin
sin
( )
( )
( )sin ( )
( )cos (
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
b
t
b
t
b
t
b
x
a
x
y
a
y
z
a
z
t
a
t
a
t
t
a
t
V
　
3
4
3
4
( )
( )
( )
( )
) tan ( )
( )
( )cos ( )
( )sin ( )
1
( )
( )sin ( )
( )cos ( ) cos ( )
b
b
b
b
b t
V
t
b
t
b
t
b
t
t
t
a
t
t
a
t
t
t
a
t
t
a
t
t
t
V
w
w
w
w
 
  (A.1) 
where ( , , )
x y z  and ( , ,
)  are the 3D position and attitude 
angle (roll, pitch, and yaw angles) of the motorcycle, (
b , 
b  , 
b  ) are the angular velocities  (roll, pitch, and yaw 
velocities ) of the motorcycle, and (
,
,
b
wVb
w
   
,
)
b
w b
w
  are 
the acceleration disturbances. 
1
b
a
V
2
/ 2
wVb
 , 
a 2
 
2
/ 2
b
b
w
 , 
2
3
/ 2
b
b
a
w
 , and 
a 4
   
b
 
2
/ 2
w b
.  
ACKNOWLEDGMENT 
This study was partially supported by the KAKENHI 
Grant #18K04062, the Japan Society for the Promotion of 
Science (JSPS). 
REFERENCES 
[1] C. Cadena et al., “Past, Present, and Future of Simultaneous 
Localization and Mapping: Toward the Robust-Perception 
Age,” IEEE Trans. on Robotics, vol. 32, pp. 1309–1332, Dec. 
2016, doi: 10.1109/TRO.2016.2624754. 
[2] G. Bresson, Z. Alsayed, L. Yu, and S. Glaser, “Simultaneous 
Localization And Mapping: A Survey of Current Trends in 
Autonomous Driving,” IEEE Trans. on Intelligent Vehicles, 
vol. 2, pp. 194–220, Sept. 2017, doi: 10.1109/TIV.2017. 
2749181. 
[3] B. Huang, J. Zhao, and J. Liu, “A Survey of Simultaneous 
Localization and Mapping,” eprint arXiv:1909.05214, 2019. 
74
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-766-5
ALLSENSORS 2020 : The Fifth International Conference on Advances in Sensors, Actuators, Metering and Sensing

[4] H. G. Seif and X. Hu, “Autonomous Driving in the iCity—HD 
Maps as a Key Challenge of the Automotive Industry,” 
Engineering, vol. 2, pp.159–162, June 2016, doi:10.1016/ 
J.ENG.2016.02.010. 
[5] K. Morita, M. Hashimoto, and K. Takahashi, “Point-Cloud 
Mapping and Merging using Mobile Laser Scanner,” Proc. of 
the third IEEE Int. Conf. on Robotic Computing (IRC 2019), 
pp.417–418, March 2019, doi: 10.1109/IRC.2019.00078. 
[6] D. Schwesinger, A. Shariati, C. Montella, and J. Spletzer, “A 
Smart Wheelchair Ecosystem for Autonomous Navigation in 
Urban Environments,” Autonomous Robot, vol. 41, pp. 519–
538, March 2017, doi: 10.1007/s10514-016-9549-1. 
[7] S. Hong, H. Ko, and J. Kim, “VICP: Velocity Updating 
Iterative Closest Point Algorithm,” Proc. of 2010 IEEE Int. 
Conf. on Robotics and Automation (ICRA 2010), pp. 1893–
1898, May 2010, doi: 10.1109/ROBOT.2010.5509312. 
[8] F. Moosmann and C. Stiller, “Velodyne SLAM,” Proc. of IEEE 
Intelligent Vehicles Symp. (IV2011), pp. 393–398, July 2011, 
doi: 10.1109/IVS.2011.5940396. 
[9] J. Zhang and A. Singh, “LOAM: Lidar Odometry and Mapping 
in Real-time,” Proc. of Robotics: Science and Systems, Jan. 
2014. 
[10] K. Inui, M. Morikawa, M. Hashimoto, and K. Takahashi, 
“Distortion Correction of Laser Scan Data from In-Vehicle 
Laser Scanner Based on Kalman Filter and NDT Scan 
Matching,” Proc. of the 14th Int. Conf. on Informatics in 
Control, Automation and Robotics (ICINCO), pp. 329–334, 
2017, doi: 10.5220/0006422303290334. 
[11] K. Tokorodani, M. Hashimoto, Y. Aihara, and K. Takahashi, 
“Point-Cloud Mapping Using Lidar Mounted on Two-Wheeled 
Vehicle Based on NDT Scan Matching,” Proc. of the 18th Int. 
Conf. on Informatics in Control, Automation and Robotics 
(ICINCO), pp. 446–452, 2019, doi: 10.5220/ 00079462044 
60452. 
[12] P. Biber and W. Strasser, “The Normal Distributions 
Transform: A New Approach to Laser Scan Matching,” Proc. 
of IEEE/RSJ Int. Conf. on Intelligent Robots and Systems 
(IROS 2003), pp. 2743–2748, Oct. 2003, doi: 10.1109/ 
IROS.2003.1249285. 
[13] J. P. Saarinen, H. Andreasson, T. Stoyanov, and A. J. Lilienthal, 
“3D Normal Distributions Transform Occupancy Maps: An 
Efﬁcient 
Representation 
for 
Mapping 
in 
Dynamic 
Environments,” Int. J. of Robotics Research, vol.32, pp.1627–
1644, Sept. 2013, doi:10.1177/0278364913499415. 
[14] X. Ding, Y. Wang, H. Yin, L. Tang, and R. Xiong, “Multi-
Session Map Construction in Outdoor Dynamic Environment,” 
Proc. of the 2018 IEEE Int. Conf. on Real-time Computing and 
Robotics (IRC2018), pp. 384–389, Aug. 2018, doi: 10.1109/ 
RCAR.2018.8621770. 
[15] M. Yamaji, S. Tanaka, M. Hashimoto, and K. Takahashi, 
“Point Cloud Mapping Using Only Onboard Lidar in GNSS 
Denied and Dynamic Environments,” Proc. of the Fifteenth Int. 
Conf. on Systems (ICONS 2020), Feb. 2020. 
[16] R. B. Rusu and S. Cousins, “3D is Here: Point Cloud Library 
(PCL),” Proc. of 2011 IEEE Int. Conf. on Robotics and 
Automation (ICRA 2011), 2011. 
[17] S. Sato, M. Hashimoto, M. Takita, K. Takagi, and T. Ogawa, 
“Multilayer Lidar-Based Pedestrian Tracking in Urban 
Environments,” Proc. of IEEE Intelligent Vehicles Symp. 
(IV2010), pp. 849–854, June 2010, doi: 10.1109/IVS.2010. 
5548135. 
[18] S. Kanaki et al., “Cooperative Moving-Object Tracking with 
Multiple Mobile Sensor Nodes -Size and Posture Estimation of 
Moving Objects using In-Vehicle Multilayer Laser Scanner-,” 
Proc. of 2016 IEEE Int. Conf. on Industrial Technology (ICIT 
2016), pp. 59–64, March 2016, doi: 10.1109/ICIT.2016. 
7474726. 
 
75
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-766-5
ALLSENSORS 2020 : The Fifth International Conference on Advances in Sensors, Actuators, Metering and Sensing

