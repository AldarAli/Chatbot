Facial Part Effects Analysis using Emotion-evoking Videos focused on Smile 
Expression Process 
 
 
Kazuhito Sato and Hirokazu Madokoro 
Department of Machine Intelligence and Systems 
Engineering, 
Faculty of Systems Science and Technology, Akita 
Prefectural University 
 Yurihonjo, Japan 
e-mail:{ksato, madokoro}@akita-pu.ac.jp 
Momoyo Ito 
Institute of Technology and Science, 
Tokushima University 
 Tokushima, Japan 
e-mail: momoito@is.tokushima-u.ac.jp 
 
 
Sakura Kadowaki 
 Smart Design Corp. 
Akita, Japan 
e-mail: sakura@smart-d.jp 
 
Abstract - This study specifically examines the expressive 
process of "happiness" related facial expressions after giving a 
stress stimulus. In addition, it presents a quantitative analysis 
of expressive tempos and rhythms using mutual information. 
By acquiring image datasets of facial expressions under states 
of pleasant-unpleasant stimulus for 20 participants, we 
calculated the information in three region of interests (ROIs): 
ROI 1, the whole face and the upper face; ROI 2, the whole 
face and the lower face; and ROI 3 between the upper face and 
the lower face. Additionally, we tried to express complexity 
and ambiguity objectively during facial expressions because of 
human psychological states. The results clarified the possibility 
of estimating the impression of facial expressions from the 
magnitude relation and order relation of mutual information 
of each ROI. More than male participants, female participants 
were able to create facial expressions of "happiness" easily and 
intentionally, and were less susceptible to discrepancy 
expressions. Finally, we discussed the differences of expressive 
paths between intentional and spontaneous facial expressions 
based on the order of the mutual information of the ROIs. As a 
result, we figured out the validity of our hypotheses concerning 
to the individual expressive path of each facial expression. 
 
Keywords - Psychological stress measures; Intentional facial 
expression; Machine learning approaches; Behavior modeling 
I. 
 INTRODUCTION 
Human faces are often described as a window by which 
one can discern information of various types such as the 
state of a person's mind and health condition. Especially, 
facial expressions can reveal aspects of internal psychology, 
reflective emotions such as delight, anger, sorrow, pleasure, 
and the existence of stress. In contrast, humans can feel 
rhythms from all of their personal surroundings that are 
moving, especially any emitting sound. Additionally, they 
feel rhythms from engaging in daily life, such as rhythms 
related to conversation and rhythms of human life. To 
clarify the relevance between psychological states and facial 
expressions, we have been studying a dynamical framework 
that specifically examines actions to repeat intentional facial 
expressions after giving a stress stimulus [1]. 
Attractive smiles attract people and represent a symbol of 
happiness, soothing another person’s mind. Smiles are 
therefore effective as a lubricant of human communication. 
According to a study [2] that analyzed geometric features 
with respect to charming smiles, the most attractive part of 
smiles in both men and women is perceived as the eye, 
followed by the mouth. In addition, facial parts associated 
with the eyes and mouth, such as the corners of the eyes and 
mouth, are reportedly more important as attractive factors of 
smiles. In attractive smiles, the existence of a golden ratio 
was observed in the aspect ratio of the expression rectangle. 
Furthermore, Yamada et al. [3] investigated the relevance 
between the whole and partial impression formed from 
facial parts and pointed out the following points. Eyes play 
an extremely important role in forming impressions of 
others. It is possible to some degree to illustrate the overall 
impression by adding and coupling the partial impression 
formed from each part. However, they suggest that 
individual differences exist in the information of the parts 
which are expected to be related to emphasis. Assessing 
male and female viewpoints of smile expressions 
specifically, women are said to tend to expose smiles more 
than men [4]. Moreover, smiles are natural for women: 
women are better at making smiles than men. Particularly, 
women have excellent skills to adjust positive emotional 
expressions. Such natural expressions can elicit positive 
effects on a person viewing the smile (recipient). 
Nevertheless, for the creation of intentional facial 
128
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

expressions, different facial muscles are said to move in 
conjunction with natural facial expressions [5]. Particularly 
examining the expressive process, the deformation degree, 
and operation timing of facial parts creating smiles are 
expected to vary slightly. 
To clarify the relevance between facial expressions and 
psychological states to date, as a result of verifying the 
relevance 
between 
psychological 
stress 
and 
facial 
expressions using the framework of Facial Expression 
Spatial Charts (FESCs), we demonstrated that the degree of 
stress accumulation can be easily ascertained from facial 
expression types and expressive processes [6] [7]. 
Additionally, we proposed a framework of rhythms and 
tempos that specifically examines actions to repeat 
intentional facial expressions after giving a stress stimulus 
[8]. We define one rhythm as one tempo repeated several 
times. In addition, regarding one tempo as the period during 
which facial expressions transform from a neutral face (i.e., 
expressionless) to the next neutral face, we found that the 
variation in unpleasant stimulus became greater than that in 
pleasant stimulus, addressing the variation of the number of 
frames constituting one tempo. Furthermore, using Bayesian 
networks, we constructed a graphical model of the relation 
between these three facial expressions and psychological 
stress factors. Results show that facial expressions 
displaying the effects of psychological stress easily were 
"happiness" and "sadness." Additionally, we showed the 
possibilities that facial parts (such as the eyes and mouth) 
easily differed by facial expression type [9] [10] [11]. 
In this study, particularly addressing the expressive 
process of "happiness" facial expression after giving a 
pleasant-unpleasant stress stimulus by emotion-evoking 
videos, we strove objectively to express complexity and 
ambiguity through facial expression because of human 
psychological states, by quantitative analysis of expressive 
rhythms from the viewpoint of mutual information. 
This paper is presented as follows. We review related 
work to clarify the position of this study in Section II. 
Section III presents a definition of a new framework of 
exposed rhythms and tempos for analyzing relations of 
psychological stress and facial expressions. Section IV 
describes a method to capture facial expression images, in 
addition to preprocessing, classification of facial expression 
patterns with self-organizing maps, integration of facial 
expression categories with fuzzy adaptive theory, and 
quantification 
of 
expressive 
rhythms 
using 
mutual 
information. We explain our originally developed facial 
expression datasets including stress measurements in 
Section V. In Section VI, based on the calculation results of 
mutual information in a time-series change of ELs for each 
facial region, we analyze the respective trends exhibited by 
men and women. Additionally, we discuss the effects of a 
pleasant-unpleasant stimulus which would give the 
expressive rhythm of facial expressions from the perspective 
of mutual information. Finally, we present conclusions and 
intentions for future work in Section VII. 
II. 
RELATED WORKS 
In spite of increasing or decreasing attractiveness of a 
"smile" with changes in expressive process, many 
conventional studies have examined the shape of a post-
expression face. Case studies examining the expressive 
process are few [12]-[15]. Regarding impression formation 
of friendly and thoughtful smile expressions, Ishi et al. [12] 
described the following. A continuous video presentation, 
such as expression levels from a neutral face become the 
maximum, is the most effective. Hanibuchi et al. [13] 
proposed a smile training method that specifically examines 
facial expressions process. Through impression evaluation 
experiments, they demonstrated the validity of goal setting 
with the actor's perspective. In addition, particularly 
addressing a natural smiling face, Fujishiro et al. [14] [15] 
investigated how eye, cheek, and mouth movements 
contribute to the impression formation of natural smiles in 
the 
expressive 
process. 
Results 
revealed 
moderate 
correlation between the behavioral termination of the eyes 
and cheek and the impression formation of natural smiles. 
Nevertheless, the authors did not report the psychological 
state of the actor when viewing a "natural smile" and 
"forced smile," such as a disagreement expression or 
expression suppression. Particularly, they were unable to 
come up to address impression formation based on the 
timing structure of facial parts. 
For a good impression on the face of a conversation 
partner, Kampe et al. [16] revealed that the good impression 
was more emphasized with matching of each other's eye-
gaze. Using anthropomorphic agents, Kuroki et al. [17] 
indicated the following. The combination of eye-gaze and 
facial expressions affects emphases of impressions. The 
impressive transmission of friendship properties can be 
emphasized particularly. Moreover, by analyzing brain 
activities using functional magnetic resonance imaging 
(fMRI) as physiological indices, an activation is observed in 
the prefrontal cortex responsible for higher cognitive 
functions such as emotional processing, motivation, and 
reasoning. Furthermore, the same activation is observed in 
the amygdala associated with emotions and rewards. 
Therefore, the formation of a good impression shows that 
the prefrontal cortex and the amygdala play mutually 
important roles [18]. However, impression evaluation has 
not been done subjectively for overall impressions of the 
face. Moreover, dealing with impression formation based on 
the timing structure of facial parts has not been achieved. 
III. 
FRAMEWORK OF EXPOSED RHYTHMS AND TEMPOS 
As an index for quantifying individual facial expression 
spaces, we proposed a framework of expression levels (ELs) 
[6]. The ELs include both features of the pleasure and 
129
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

arousal dimensions based on the arrangement of facial 
expressions 
on 
Russell’s 
circumplex 
model 
[19]. 
Specifically, we extract the dynamics of topological changes 
of facial expressions of facial components such as the eyes, 
eyebrows, and mouth. Topological changes show the 
structure defining the connection form of the elements in the 
set. The ELs obtained in this study are sorted to categories 
according to their topological changes in intensity from 
expressions that are regarded as neutral facial expressions. 
As discussed above, the ELs in this study include features of 
both pleasure and arousal dimensions. In Russell’s 
circumplex model, all emotions are constellated on a two-
dimensional space: the pleasure dimension of pleasure-
displeasure and arousal dimension of arousal-sleepiness. In 
the intentional facial expressions covered in this study, 
direct handling of the facial expressions for the influence of 
pleasure dimension is difficult. As a method of measuring 
the transitory stress response, we conduct an evaluation 
using the salivary amylase test during the task of watching 
emotion-evoking videos causing a pleasant-unpleasant state. 
Specifically examining the values of salivary amylase 
activity before and after watching videos, we can effectively 
perform stress measurements using salivary amylase tests to 
assess the stress state transiently. Consequently, we target 
the intentional facial expressions under pleasant and 
unpleasant stimulation states. 
In this study, using temporal variation of ELs, we intend 
to visualize rhythms and tempos of facial expressions that 
humans create. We defined one rhythm as a tempo that is 
repeated several times. One tempo is the period during 
which facial expressions are transformed from a neutral 
state to the next neutral state. Facial expressions exhibited 
intentionally by humans form an individual space based on 
the dynamic diversity and static diversity of the human face. 
Facial expression dynamics can be regarded as "topological 
changes in time-sequential facial expression patterns that 
facial muscles create." Static diversity is individual diversity 
that is configured by the facial component position, size, 
and location, consisting of the eyes, nose, mouth, and ears. 
In contrast, dynamic diversity denotes that a human can 
move facial muscles to express internal emotions 
unconsciously and sequentially or to express emotions as a 
message. After organizing and visualizing topological 
changes of face patterns by ELs, we attempt to use the 
framework of rhythms and tempos with expressions to 
examine ambiguities and complexities of facial expressions 
attributable to a psychological state. 
IV. 
PROPOSED METHOD 
Facial expression processes differ among individuals. 
Therefore, adaptive learning mechanisms are necessary for 
modification according to individual characteristic features 
of facial expressions. In this study, our target is intentional 
facial expressions. We use self-organizing maps (SOMs) 
[20] to extract topological changes of facial expressions and 
for normalization with compression in the direction of the 
temporal axis. After classification by SOMs, facial images 
are integrated using Fuzzy ART [21], which is an adaptive 
 
 
Figure 1. Overview of the procedures used for our proposed method.  
 
130
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

learning algorithm with stability and plasticity. In fact, 
SOMs perform unsupervised classification input data into a 
mapping space that is defined preliminarily. In contrast, 
Fuzzy ART performs unsupervised classification at a 
constant granularity that is controlled by the vigilance 
parameter. Therefore, using SOMs and Fuzzy ART, time-
series datasets showing changes over a long term are 
classified using a certain standard. Figure 1 presents an 
overview of the procedures used for our proposed method. 
In the following, we describe extraction of time-sequential 
changes of ELs, and also explain quantification of 
expressive tempos and rhythms by mutual information. 
A. Acquisition of Time-series Variation of ELs 
We set the region of interest (ROI) to 90 × 80 pixels, 
including the eyebrows, which all contribute to the 
impression of a whole face as facial feature components. 
With preprocessing, brightness values are normalized for 
time-series images of facial expressions. The influence of 
brightness values attributable to illumination conditions is 
thereby reduced. Moreover, smoothing the histogram is 
useful to adjust contrast and clarify the images. In addition, 
using the orientation selectivity of Gabor Wavelets filtering 
as a feature representation method, the facial parts 
characterizing the dynamics of facial expressions are 
emphasized, such as the eyes, eyebrows, mouth, and nose. 
By down-sampling (i.e., 10 × 10 pixels) time-series facial 
expressions converted with Gabor Wavelets filtering [22], 
the effects of a slight positional deviation when taking facial 
images were minimized. Then data size compression was 
conducted. 
First, SOMs are used to learn the time-series images of 
facial expressions with down-sampling. The face images 
showing topological changes of facial expressions that are 
similar are classified into 15 mapping units of SOMs. Next, 
similar units (i.e., Euclidean distances of the weight vectors 
are close) among 15 mapping units of SOMs are integrated 
into the same category using Fuzzy ART. By sorting the 
facial expression categories integrated by Fuzzy ART from 
neutral facial expression to the maximum of facial 
expression, we obtain ELs labeled as expressive intensities 
of facial expressions quantitatively. The integrated category 
sorting procedure is based on the two-dimensional 
correlation coefficient of the average image of the facial 
expression images classified into each category. Finally, we 
conduct correspondence of ELs with each frame of the 
facial images to assess a time-series dataset of variation of 
ELs. 
B. Quantification of Exposed Rhythms using Mutual 
Information 
Mutual information [23] [24] can express changes 
between signals with the entanglement and synchrony. It 
can be regarded as an amount that represents linear and 
nonlinear dependence between the two time-series datasets. 
Moreover, it represents information flows and dynamically 
coupled rings between two signals. Mutual information 
between these two signals is zero if the two systems for 
observation target differ completely from independent ones. 
Applying this scheme to the facial expression process, it is 
possible to quantify the synchronicity and functional 
connectivity between facial parts. Figure 2 presents one 
example of time-series changes of ELs in the "Whole face," 
"Upper part of face," and "Lower part of face" obtained in 
Section Ⅳ.A. In this study, three ROIs listed below are 
calculated as the mutual information among facial parts in 
the expressive process. The time-series changes of ELs with 
respect to the "Whole face," "Upper part of face," and 
"Lower part of face" respectively represent
R (t)
R
w
w 
, 
R (t)
R
u
u 
, and 
R (t)
R
d
d 
. Then, mutual information of 
each ROI is obtained as described below. 
Mutual information between the "Whole face" and 
"Upper part of face" is 
) :
;
(
I Rw Ru
 
 
 
Figure 2. Each mutual information among time-series changes of facial parts. 
 
131
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

)
,
(
)
(
)
(
)
;
(
u
w
u
w
u
w
R
H R
H R
H R
R
I R



 
(1) 
Mutual information between the "Whole face" and 
"Lower part of face" is 
) :
;
(
I Rw Rd
 
)
,
(
)
(
)
(
)
;
(
d
w
d
w
d
w
R
H R
H R
H R
R
I R



 
(2) 
Mutual information between the "Upper part of face" and 
"Lower part of face" is 
) :
;
(
I Ru Rd
 
)
,
(
)
(
)
(
)
;
(
d
u
d
u
d
u
R
H R
H R
H R
R
I R



 
(3) 
In that equation, 
)
(
H Rw
, 
)
(
H Ru
, and 
)
(
H Rd
respectively 
represent the entropy of 
Rw (t)
, 
Ru (t)
, and 
Rd (t)
. 
)
,
(
H Rw Ru
, 
)
,
(
H Rw Rd
, and 
)
,
(
H Ru Rd
 respectively denote the joint 
entropy of both. 
V. 
DATASETS 
For this study, we constructed an original and long-term 
dataset for the specific facial expressions of participants. 
Details of the experimental protocols are the following. One 
experiment comprises three steps: step 1 is conducted under 
a normal state; step 2 is done during viewing of a pleasant 
video; and step 3 is done during viewing of an unpleasant 
video. We gave participants the task of watching emotion-
evoking videos, causing a pleasant-unpleasant state, and 
took stress measurements by salivary amylase tests to assess 
the stress state transiently. In addition, the watching time is 
about 3 min for each emotion-evoking video. We prepared 
unpleasant videos (i.e., implant surgery and cruel videos) 
and pleasant videos (i.e., comedy videos of three types). The 
subjective assessment of five stages was also conducted at 
watching videos. For all participants, we fully explained the 
experiment contents in advance, based on the research ethics 
policy of our university, and also obtained the consent of 
experiment participants in voluntary writing of participants. 
Moreover, from each, we received agreement to publish 
facial images as part of their experimental participation. 
A. Facial Expression Images 
Open datasets of facial expression images are open to the 
public through the internet from universities and research 
institutes. However, the specifications vary among datasets 
because of imaging with various conditions. As static facial 
images, the dataset presented by Ekman and Friesen [25] is 
a popular dataset comprising collected various facial 
expressions used for visual stimulation in psychological 
examinations of facial expression cognition. As dynamic 
facial images, the Cohn-Kanade dataset [26] and Ekman-
Hager dataset [27] are used widely, especially in 
experimental applications. In recent years, the MMI Facial 
Expression Database presented by Pantic et al. [28] and the 
CK+ dataset [29] have become a widely used open dataset 
containing both static and dynamic facial images. These 
datasets contain a sufficient number of people as horizontal 
datasets. However, facial images are taken only once for 
each person. No dataset exists in which the same person has 
been traced over a long term. Therefore, we created original 
and longitudinal datasets that include collections of the 
specific facial expressions of the same person during a long 
term. 
The six basic facial expressions proposed by Ekman et al. 
[25] are "happiness," "anger," "sadness," "disgust," "fear," 
and "surprise." Among those six basic facial expressions, 
we specifically examined the facial expression of 
"happiness," which is believed to be most likely to be 
exhibited spontaneously. As the target facial expression of 
"happiness" under pleasant and unpleasant stimulation states, 
we acquired the facial expressions of 20 people. As a 
stimulation method, we pre-selected emotion-evoking 
videos that elicit pleasant or unpleasant emotions, with all 
participants expressing facial expressions of "happiness" 
immediately after viewing them. Participants, all of whom 
were university students, were 10 men, whom we 
designated as A-J (J was 20 years old; B, G, H, and I were 
21; A, E, and F were 22; C and D were 23) and 10 women 
whom we designated as K-T (K, M, O, and P were 20 years 
old; L, Q, R, S, and T were 21; N was 23). The imaging 
period was three weeks at one-week intervals for all 
participants. 
The 
imaging 
environment 
for 
facial 
expressions was an imaging space partitioned by a curtain in 
the corner of the room. We took frontal facial images with 
conditions including the head of the participant in each 
image. In advance, we instructed each participant to expose 
the facial expression with no head movement. Consequently, 
imaging the face region to fit within the scope was possible. 
However, with respect to extremely small changes caused 
by body motion, we used template-matching methods to 
trace the face region by setting the initial template to include 
facial parts. By consideration of the application deployment 
and ease of imaging in future studies, we used commercially 
available USB cameras (QcamOrbit; Logicool Inc. [30]). 
When taking images of each facial expression, the same 
expression was repeated three times based on the neutral 
facial expression during the image-taking period of 20 s. We 
had previously instructed all participants to express an 
emotion three times at their own timing according to a 
guideline for 20 s. One dataset consisted of 200 frames with 
the sampling rate of 10 frames per second.  
B. Stress Measurement Method 
Because types of psychological stress are regarded as 
affecting facial expressions, we assessed transient stress and 
chronic stress. Chronic stress is that which humans have on 
a daily basis, whereas transient stress is that caused by a 
temporary stimulus. To assess transient stress stimulus to 
the participants in this study, we applied the salivary 
amylase test, which measures transient stress reactions. As a 
biological reaction, salivary amylase activity is detected as a 
low value if one is in a pleasant state. In contrast, the value 
132
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

is high if one is in an unpleasant state. As stress reactions 
when subjected to external transient stimulus, Yamaguchi et 
al. [31] confirmed that salivary amylase activity is an 
effective means of stress evaluation. For this study, using 
emotion-evoking videos as an external transient stimulus, 
we used the salivary amylase test method to measure stress 
reactions immediately after participants watched the videos. 
We verified the validity of emotion-evoking videos, 
which give a pleasant-unpleasant stimulus. Using the 
salivary amylase test, we examined the validity of emotion-
evoking factor in watching the video used as a pleasant-
unpleasant stimulus. The following were shown for salivary 
amylase activity. The value of salivary amylase activity is 
reduced if in a pleasant state. In contrast, its value is 
increased if one is in unpleasant circumstances [31]. 
Accordingly, letting Snormal be the value of salivary amylase 
activity at normal state, and letting Sstimu be the value of 
salivary amylase activity after watching the video, then the 
difference of salivary amylase activity between the normal 
state and after watching video (Sdif) is defined by the 
following equation.  
normal
stimu
dif
S
S
S


 
(4) 
Sdif  0
 (i.e., after watching pleasant videos) 
 
Sdif  0
 (i.e., after watching unpleasant videos) 
 
 Figure 3 presents results of Sdif obtained for target to the 
20 subjects of A-T. As noted previously, this figure 
  
                               
 
Figure 3. Results of Sdif obtained for target to the 20 subjects of A-T.         Figure 4. Results of Sdif addressed only the score of 4 and 5 with 
subjective evaluations. 
 
 
 
Figure 5. NIRS signals at pleasant stimulus for whole subjects. 
 
 
 
Figure 6. Changes of NIRS signals at unpleasant stimulus for whole subjects. 
 
133
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

indicates each average value of three pleasant videos (i.e., 
comedy videos of three types) and three unpleasant videos 
(i.e., implant surgery and cruel videos). Generally, it is 
known that increase or decrease of the salivary amylase 
activity is well consistent with the enhancement and calm 
down of sympathetic nerve activity. If all of these videos 
might be effectively acting on each subject, the value of 
salivary amylase activity should be reduced if in a pleasant 
state. In contrast, its value should be increased if in an 
unpleasant state. However, we are unable to confirm such a 
significant tendency in Figure 3. In this case, the perception 
for the pleasant-unpleasant videos differs slightly among 
subjects, so this fact might cause the results of salivary 
amylase activity of C and B differ with previous studies [31]. 
Therefore, we decided to calculate the salivary amylase 
activity only for data for which subjective evaluation of the 
subject is high. The subjective evaluation receives a score of 
1-5, score 1 (i.e., not at all), score 5 (i.e., strong) at watching 
each emotional video. Figure 4 presents results of salivary 
amylase activity in the case of particularly addressing only 
the score of 4 and 5 because we consider that the emotional 
video is effectively working as a pleasant-unpleasant 
stimulus. Based on this result, the average of all Sdif 
indicates -2 [kIU/l] at a pleasant state, 5 [kIU/l] at an 
unpleasant state. 
Furthermore, using NIRS (Near-Infrared Spectroscopy: 
OEG-16; Spectratech Inc. [32]), we measured the activation 
states of brain for three periods: i.e., 1 minute before 
     
 
 
(a) Pleasant stimulus with emotion-evoking videos                              (b) Unpleasant stimulus with emotion-evoking videos 
 
Figure 7. Mutual information results among each facial part for female. 
 
  
 
 
(a) Subject K                                   (b) Subject M  
 
Figure 8. Time-series changes of smile facial expression with pleasant stimulus for specified subjects of female. 
 
134
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

watching each video, 3 minutes during watching each video, 
and 1 minute after watching each video. As a new functional 
brain analysis method, NIRS for measuring the cerebral 
blood volume changes locally non-invasively is also 
attracting attention as a stress measuring method, which 
indicates that the activity of the prefrontal cortex is changed 
significantly [33]. Generally, when the brain is activated, the 
following findings are reported: i.e., the oxygenated 
hemoglobin (Oxy-Hb) in the brain would increase. 
Conversely, the deoxygenated hemoglobin (Deoxy-Hb) 
should decrease [34]. In addition, because NIRS signals 
indicates the relative changes on the basis of the time of 
measurement start, we separated the noise to signal 
components by multiresolution analysis, and we carried out 
the standardized scoring (Z) using the following equation. 
Here, X is the NIRS signal, μ is the average, ρ is the 
standard deviation. 

Z  X  
 
(5) 
After standardized scoring, we calculated the average of 
all subjects. Then, we compared the activated state of brain 
at the pleasant-stimulus and the unpleasant-stimulus. These 
results are shown in Figure 5 and Figure 6. Although the 
signals of Oxy-Hb and Deoxy-Hb during watching videos 
are varied little by little, after watching videos, it indicates 
that the value of Oxy-Hb is higher than Deoxy-Hb 
   
     
(a) Pleasant stimulus with emotion-evoking videos                    (b) Unpleasant stimulus with emotion-evoking videos 
 
Figure 9. Mutual information results among each facial part for male. 
 
 
      
 
(a) Subject D                                      (b) Subject J  
 
Figure 10. Time-series changes of smile facial expression with pleasant stimulus for specified subjects of male. 
 
135
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

obviously, which represents the state of brain are activated. 
Consequently, we convinced that the emotion-evoking 
videos used as pleasant-unpleasant stimulus in this study 
might be act effectively. Particularly, we considered that the 
unpleasant videos would be more effective than the pleasant 
videos as a transient stress stimulus. Therefore, these results 
show that the emotion-evoking video functioned as a 
pleasant-unpleasant stimulus. 
VI. 
EXPERIMENT 
Based on the calculation result of mutual information in a 
time-series change of ELs for each facial region, we 
analyzed the respective male and female trends. Finally, we 
discussed the effects of a pleasant-unpleasant stimulus 
which would give the expressive rhythm of facial 
expressions from the perspective of mutual information. 
A. Analysis of Female Participants 
Figure 7 depicts the calculation results of mutual 
information of five cases of female participants K, L, M, O, 
and P. The results show the mutual information of the time-
series variation of ELs in each face region described in 
Section Ⅳ.B. Figure 7-(a) presents the calculation results 
obtained after giving a pleasant stimulus. Figure 7-(b) 
shows calculation results obtained after giving unpleasant 
stimulus. As an overall trend of female participants, we 
confirmed the following. For K, L, O, and P, the value of 
the mutual information is reduced to the order of "ROI 1: 
between the whole face and upper face," "ROI 2: between 
the whole face and lower face," and "ROI 3: between the 
upper face and lower face." The value of "ROI 2: between 
the whole face and lower face" is clearly larger than those 
for other ROIs in M. For K, M, and O, we were unable to 
recognize a marked change in the trend of mutual 
information by pleasant-unpleasant stimulus. However, for 
L and P, we detected a specific change in the trend of the 
mutual 
information 
after 
giving 
pleasant-unpleasant 
stimulus. Particularly, the tendency of L is remarkable. In 
pleasant stimulus, the value of the mutual information is 
reduced to the order of "ROI 1: between the whole face and 
upper face," "ROI 2: between the whole face and lower 
face," and "ROI 3: between the upper face and lower face." 
Otherwise, "ROI 2: between the whole face and lower face" 
shows a large value for the unpleasant stimulus. In the 
unpleasant stimulus, the value of the mutual information is 
reduced to the order of "ROI 1: between the whole face and 
upper face," "ROI 2: between the whole face and lower 
face," and "ROI 3: between the upper face and lower face." 
However, in pleasant stimulus, the order relation of mutual 
information of P is reversed with L because the value of 
"ROI 1: between the whole face and upper face" is reduced. 
Next, although the same trend is apparent for both pleasant 
and unpleasant stimuli, we compare K to M, for which the 
order relation of the mutual information in each facial 
region is markedly different. For K in both pleasant and 
unpleasant stimuli, the mutual information value of "ROI 1: 
between the whole face and upper face" is larger than "ROI 
2: between the whole face and lower face." In addition, 
particularly addressing "ROI 3: between the upper face and 
lower face," the value of K is larger than M. However, for 
M with both pleasant and unpleasant stimuli, the mutual 
information value of "ROI 2: between the whole face and 
lower face" is markedly larger than others. Furthermore, 
particularly addressing "ROI 1: between the whole face and 
upper face" and "ROI 3: between the upper face and lower 
face," the values of M are clearly smaller than those of K. 
For K and M, thumbnail images representing the time-series 
changes of "happiness" in pleasant stimulus are shown in 
Figure 8. Figures 8-(a) and 8-(b), respectively present 
thumbnail images of K and M. The top of each figure shows 
the characteristic section during exposed facial expression 
of "happiness." Comparing the thumbnail images shown in 
Figure 8 to the calculation result of mutual information 
shown in Figure 7, for K exposed "happiness," we can 
recognize the change of facial expression in the upper face 
such as the brow and the area around the eyes, and in the 
lower face such as the mouth. Otherwise, for M, we can not 
observe any change of facial expression in the upper face. 
However, only the corner of mouth in the lower face has 
changed significantly. Actually, K has the characteristics 
which the upper part and lower face change both 
synchronized during facial expressions. Staying on the 
subjective impression of the experimenter, the result for 
"happiness" looks more natural facial expressions. In 
contrast, for M, only the corner of the mouth in the lower 
face 
has 
been 
changed. 
Therefore, 
we 
have 
an 
uncomfortable feeling about the unnatural facial expression 
of "happiness." 
B. Analysis of Male Participants 
Figure 
9 
presents 
calculation 
results 
of 
mutual 
information of five cases of male participants. Figure 9-(a) 
presents the calculation results after giving pleasant stimulus. 
Figure 9-(b) shows the calculation results after giving 
unpleasant stimulus. As an overall trend of male participants, 
we confirmed the following. For D and F, the value of the 
mutual information is reduced to the order of "ROI 1: 
between the whole face and upper face," "ROI 2: between 
the whole face and lower face," and "ROI 3: between the 
upper face and lower face." The value of "ROI 2: between 
the whole face and lower face" is markedly larger than those 
of other ROIs in C, G, and J. For male participants C, D, F, 
G and J, we were unable to recognize a marked change in 
the trend of mutual information by pleasant-unpleasant 
stimulus. 
Next, regarding male participants, we compare D to J, for 
whom the order relation of the mutual information in each 
facial region is significantly different. For D, in both 
136
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

pleasant and unpleasant stimuli, the mutual information 
value of "ROI 1: between the whole face and upper face" is 
larger than "ROI 2: between the whole face and lower face." 
In addition, particularly addressing "ROI 3: between the 
upper face and lower face," the value of D is larger than that 
of J. However, for J in both pleasant and unpleasant 
stimulus, the mutual information value of "ROI 2: between 
the whole face and lower face" is markedly larger than 
others. In addition, particularly addressing "ROI 1: between 
the whole face and upper face" and "ROI 3: between the 
upper face and lower face," the values of J are clearly 
smaller than those of D. For D and J, the thumbnail images 
representing the time-series changes of "happiness" in 
pleasant stimulus are portrayed in Figure 10. Figures 10-(a) 
and 10-(b) respectively present thumbnail images of D and J. 
The top of each figure shows the characteristic section 
during exposed facial expression of "happiness." Comparing 
the thumbnail images shown in Figure 10 to the calculation 
result of mutual information shown in Figure 9, for D 
exposed "happiness," we can recognize the change of facial 
expression in the upper face such as the brow and around the 
eyes, and in the lower face such as mouth. Otherwise, for J, 
no change of facial expression can be observed in the upper 
face. Only the corner of the mouth in the lower face has 
changed substantially. Actually, D has characteristics by 
which the upper part and lower face change at the same time 
during facial expressions. Therefore, the exposing result of 
"happiness" looks more natural facial expressions. In 
contrast, for J, only the corner of the mouth in the lower 
face 
has 
been 
changed. 
Therefore, 
we 
have 
an 
uncomfortable feeling about the unnatural facial expression 
of "happiness." These results underscore a common 
tendency between male and female participants and can be 
anticipated as a new index for quantification of the 
impression during facial expressions based on the mutual 
information of the time-series change of each face region. 
C. Effects of Pleasant-unpleasant Stimulus on Mutual 
Information 
The discrepancy expression in facial expressions means 
to expose the emotions that do not match one’s own feelings 
when experiencing certain emotions, such as having a smile, 
even though one might be in a sad mood. In previous studies, 
being positive emotional expressions during negative 
emotional experiences has been shown to engender the 
following: an amplification of actor’s sympathetic nerve 
activities [35], an increase of subjective emotional 
experiences, and some memory loss [36]. The discrepancy 
expression can easily take cognitive loads for expressive 
person. Additionally, it can potentially give bad effects to 
the mental health of actors. Furthermore, the expressive 
suppression in facial expressions indicates an emotional 
suppression by facial expressions when experiencing a 
certain emotion, such as to stifle crying when in a sad mood. 
Expressive suppression is reportedly associated with social 
support, closeness with others, and reduction in social 
satisfaction [37]. In comparison to men, women are more 
skilled at making smiles and excellent adjustments of 
positive emotional expressions. Moreover, women show 
similar effects such as natural expressions to recipients [4]. 
Exposing facial expressions related to "happiness" after 
viewing an unpleasant video is equivalent to a discrepancy 
expression. In contrast, exposing the facial expression of 
"happiness" after viewing a pleasant video is a matching 
expression. For female participants K and M, such order of 
mutual information was markedly different; Figure 11 
presents their facial expression rhythms. In the impression 
analysis of Section VI.B, the smile of K gave us a natural 
impression. In contrast, we received an unnatural impression 
from the smile of M. Focusing on an expressive rhythm of 
each facial part, the expressive rhythm of K indicates a 
time-series change such as to work together in each facial 
    
[frame]
[frame]
[frame]
[ELs]
[ELs]
[ELs]
Whole face
Upper part of face
Lower part of face
  
[frame]
[frame]
[frame]
[ELs]
[ELs]
[ELs]
Whole face
Upper part of face
Lower part of face
 
(a) Subject K                                                                                          (b) Subject M 
 
Figure 11. Comparison of time-series changes of ELs with unpleasant stimulus. 
 
137
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

part. In contrast, we were unable to recognize cooperative 
movements at all in the expressive rhythm of M because the 
upper face and the lower face are independent. The mutual 
information of the ROIs (i.e., ROI 1, ROI 2, and ROI 3) 
effectively expresses the degree of similarity and 
synchronization of signal waveforms in facial expression 
rhythms. These mutual information values can be 
interpreted as quantified indices of the timing structure 
indicating the synchronization between the upper face (e.g., 
the eyebrows and eyes) and the lower face (e.g., mouth), 
which contribute the impression formation to the whole face. 
We should comprehensively consider the analysis results of 
Sections VI.B and VI.C. By particularly addressing the 
magnitude relation between ROI 1 and ROI 2 with respect 
to the mutual information, we were able to interpret "Eyes 
say things sufficient to mouth" quantitatively. Around the 
value of ROI 3 quantifying the timing structure between the 
upper face and the lower face, noting the magnitude relation 
and order relation between the values of ROI 1 and ROI 2, it 
is effective as an index for quantifying the degree of 
spontaneity 
and 
artificiality 
in 
facial 
expressions. 
Furthermore, more than male participants, the female 
participants easily created facial expressions of "happiness" 
intentionally. Then we assumed that result was only slightly 
affected by the discrepancy expression. 
D. Consideration for Differences of Expressive Paths 
From a viewpoint of the order of the mutual information 
values, we try to discuss differences of expressive paths 
between intentional and spontaneous facial expressions. 
Blair [38] has reported that, for facial expressions, four 
brain domains are mutually related: (1) parts producing 
feelings (insular cortex and amygdala), (2) parts forming 
facial expressions involuntarily (basal ganglia), (3) parts 
embellishing 
facial 
expressions 
according 
to 
the 
surrounding circumstances (prefrontal area), and (4) motor-
related areas actually moving mimic muscles. As presented 
in Figure 12, in cases where facial expressions are 
embellished intentionally or spontaneously, time-sequential 
differences exist based on the route through which facial 
expressions are revealed. According to specific brain waves 
of four brain area, nerve cells of each brain area are used to 
work cooperatively, in the case of the repetition process of 
facial expressions under a pleasant-unpleasant stimulus 
particularly. Mimic muscles are activated by coordination of 
nerve cells with different speed, a unique expression is 
exposed through the individual path of each facial 
expression, such as the intentional or spontaneous 
expressive path described in Figure 12-(a). Therefore, the 
order of the mutual information of the ROIs (i.e., ROI 1, 
ROI 2, and ROI3) is able to figure out the expressive paths 
between intentional and spontaneous facial expressions. 
Figure 12-(b) indicates the order of mutual information 
values corresponding to "Intentional expressive path." In 
contrast, Figure 12-(c) presents the order of mutual 
information 
values 
corresponding 
to 
"Spontaneous 
expressive path." Even an intentional smile, it seems to 
strike a natural impression (e.g., hospitality smile), if 
exposing through "Expressive path of hospitality and actor 
mind" described in Figure 12-(a). 
VII. CONCLUSION AND FUTURE WORK 
In this study, by quantitative analysis of expressive 
rhythms from the viewpoint of mutual information, 
particularly addressing expressive processes of "happiness" 
 
 
(b) The order of mutual information values corresponding to 
 "Intentional expressive path" 
 
 
 
(c) The order of mutual information values corresponding to 
 "Spontaneous expressive path" 
 
 
 
 
 
 
 
 
 
(a) Block diagram of expressive paths based on four brain domains 
 
Figure 12. Differences of expressive paths between intentional and spontaneous facial expressions 
 focusing on the order of the mutual information values, i.e.,M1, M2, and M3. 
138
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

facial expression after giving a pleasant-unpleasant stress 
stimulus by emotion-evoking videos, we objectively strove 
to ascertain complexity and ambiguity when making facial 
expressions because of human psychological states. Using 
evaluation experiments examining 10 participants (i.e., 5 
men, 5 women), we analyzed the information of time-series 
changes in ROIs (i.e., ROI 1, ROI 2, and ROI 3), revealing 
the following points. By particularly addressing the 
expressive rhythm of each face region, one can estimate the 
impression of facial expressions from the magnitude relation 
and order relation of mutual information of each ROI. 
Additionally, the mutual information of expressive rhythms 
is effective as an index for measuring degree of spontaneity 
and 
artificiality 
during 
facial 
expressions. 
Female 
participants were better able to create facial expressions of 
"happiness" easily and intentionally than male participants 
were. Moreover, they were less susceptible to discrepancy 
expressions. Finally, we discussed the differences of 
expressive paths between intentional and spontaneous facial 
expressions based on the order of the mutual information of 
the ROIs. As a result, we figured out the validity of our 
hypotheses concerning to the individual expressive path of 
each facial expression. In future work, by quantifying 
fluctuations of expressive tempos in facial parts upon the 
impression formation, and analyzing their timing structure, 
we intend to clarify differences of expressive paths between 
intentional and spontaneous facial expressions. 
ACKNOWLEDGMENT 
The authors thank the 20 students at our university who 
participated by letting us take facial images over such a long 
period. This work was supported by the Japan Society for 
the Promotion of Science (JSPS) KAKENHI Grant Number 
25330325 and the Cosmetology Research Foundation. 
REFERENCES 
[1] 
K. Sato, M. Ito, H. Madokoro, and S. Kadowaki, “Facial Part Effects 
Analysis using Emotion-evoking Videos: Smile Expression,” 
Proceedings of the Tenth International Multi-Conference on 
Computing in the Global Information Technology (ICCGI2015),  pp. 
30-39,  Oct. 2015. 
[2] 
T. Iguchi, “Geometrical Features of the Attractive Smile: -Attractive 
Production by Kansei X Technology = Kanseiweab-,” The Institute 
of Electronics, Information, and Communication Engineers, 
Technical Report, pp. 51-56, Mar. 2007. 
[3] 
T. Yamada and I. Sasayama, “A Study of the Correlation between 
the Impression Formed from Each Features and the Impression 
Formed from Face,” Bulletin of Fukuoka University of Education，
vol. 48, no. 4, 1999, pp. 229-239. 
[4] 
L. Ellis, “Gender differences in smiling: An evolutionary 
neuroandrogenic theory,” Physiology and Behavior, vol. 88, pp. 303-
308, 2006. 
[5] 
K. M. Prkachin, “Effects of deliberate control on verbal and facial 
expressions of pain,” Pain, vol. 114, pp. 328-338, 2005. 
[6] 
H. Madokoro, K. Sato, and S. Kadowaki, “Facial expression spatial 
charts for representing time-series changes of facial expressions,” 
Japan Society for Fuzzy Theory, vol. 23, no. 2, pp. 157-169, 2011. 
[7] 
H. Madokoro and K. Sato, “Facial Expression Spatial Charts for 
Representing of Dynamic Diversity of Facial Expressions,” Journal 
of Multimedia, vol. 6, no. 1, pp. 1-12, Jan. 2007. 
[8] 
K. Sato, H. Madokoro, and S. Kadowaki, “Transient Stress Stimulus 
Effects on Intentional Facial Expressions,” Japan Society for Fuzzy 
Theory, RJ-005, pp. 29-36, 2012. 
[9] 
K. Sato, H. Otsu, H. Madokoro, and S. Kadowaki, “Analysis of 
Psychological Stress Factors on Intentional Facial Expressions,” 
Japan Society for Fuzzy Theory, RJ-002, pp. 21-28, 2013. 
[10] K. Sato, H. Otsu, H. Madokoro, and S. Kadowaki, “Analysis of 
Psychological Stress Factors and Facial Parts Effect on Intentional 
Facial Expressions,” Proceedings of the Third International 
Conference on Ambient Computing, Applications, Services and 
Technologies, pp. 7-16, Oct. 2013. 
[11] K. Sato, H. Otsu, H. Madokoro, and S. Kadowaki, “Analysis of 
Psychological Stress Factors by Using Bayesian Network,” 
Proceedings 
of 
2013 
IEEE 
International 
Conference 
on 
Mechatronics and Automation, pp. 811-818, Aug. 2013. 
[12] H. Ishi, M. Kamachi, and J. Gyoba, “Effect of Facial Motion on 
Impression of Smile,” The Institute of Electronics, Information, and 
Communication Engineers, Technical Report, pp. 25-30, Dec. 2004. 
[13] S. Hanibuchi, K. Ito, and S. Nishida, “Analysis of Transformed 
Impression of Smile Process: - An Approach to Supporting Facial 
Expression Process Training -,” The Institute of Electronics, 
Information, and Communication Engineers, Technical Report, pp. 
35-40, Oct. 2009. 
[14] H. Fujishiro, A. Maejima, and S. Morishima, “Natural Smile 
Synthesis Considering Impression of Facial Expression Process,” 
The Institute of Electronics, Information, and Communication 
Engineers, Technical Report, pp. 31-36, Mar. 2011. 
[15] H. Fujishiro, A. Maejima, and S. Morishima, “Analysis of Relation 
between Movement of Smile Expression Process and Impression,” 
The Journal of the Institute of Electronics, Information, and 
Communication Engineers, vol. J95-A, no. 1,  pp. 128-135,  2012. 
[16] K. W. Kampe, C. D. Frith, R. J. Dolan, and U. Frith, “Reward value 
of attractiveness and gaze,” Nature, vol. 413, Oct. 2001. 
[17] Y. Kuroki, S. Shiraishi, N. Mukawa, M. Yuasa, and N. Fukayama, 
“Interaction between Human and Human-like Agent with Gaze and 
Facial Expression for Human Computer Interaction,” The Institute of 
Electronics, Information, and Communication Engineers, Technical 
Report, pp. 49-54, Mar. 2005. 
[18] Y. Kuroki, S. Shiraishi, N. Mukawa, M. Yuasa, and N. Fukayama, 
“Impression of Human-like Agent with Gaze and Facial Expression: 
-Brain Activity Analysis of HCI using fMRI-,” The Institute of 
Electronics, Information, and Communication Engineers, Technical 
Report, pp. 43-48, Mar. 2006. 
[19] J.A. Russell and M. Bullock, “Multidimensional Scaling of 
Emotional Facial Expressions: Similarity from Preschoolers to 
Adults,” Journal of Personality and Social Psychology, vol. 48, pp. 
1290-1298, 1985. 
[20] T. Kohonen, Self-organizing maps, Springer Series in Information 
Sciences, 1995. 
[21] G. A. Carpenter, S. Grossberg, and D.B. Rosen, “Fuzzy ART: fast 
stable learning and categorization of analog patterns by an adaptive 
resonance system,” Neural Networks, vol. 4, pp. 759-771, 1991. 
[22] M. Haghighat, S. Zonouz, and M. Abdel-Mottaleb, “Identification 
Using Encrypted Biometrics,” Computer Analysis of Images and 
Patterns, Springer Berlin Heidelberg, pp. 440-448, 2013. 
[23] T. Ikeda, H. Ishiguro, and M. Asada, “Moving Signal-Source 
Tracking Based on Mutual Information Maximization,” The 
Transactions of the Institute of Electronics, Information, and 
Communication Engineers D, vol. J90-D, no. 2, pp. 535-543, 2007. 
[24] T. Kikuchi, K. Kishi, and J. Miyamichi, “An Automatic Data 
Classification Algorithm Adjusted by Mutual Information, ” The 
Transactions of the Institute of Electronics, Information, and 
Communication Engineers D, vol. J82-D, no. 4, pp. 660-668, 1999. 
[25] P. Ekman and W. V. Friesen, “Unmasking the Face: A Guide to 
Recognizing Emotions from Facial Clues,” Malor Books, 2003. 
[26] T. Kanade, J. F. Cohn, and Y. L. Tian, “Comprehensive database for 
facial expression analysis,” Proc. of the Fourth IEEE Int. Conf. on 
Automatic Face and Gesture Recognition, pp. 46-53, 2000. 
139
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[27] M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski, “Measuring 
facial expressions by computer image analysis,” Psychophysiology, 
vol. 36, pp. 253-264, 1999. 
[28] M. Pantic, M.F. Valstar, R. Rademaker, and L. Maat, “Web-based 
Database for Facial Expression Analysis,” Proc. IEEE Int'l. Conf. 
Multimedia and Expo, Amsterdam, The Netherlands, Jul. 2005. doi: 
10.1109/ICME.2005.15214. 
[29] P. Lucey et al., “The Extended Cohn-Kanade Dataset (CK+): A 
complete expression dataset for action unit and emotion-specified 
expression,” Proc. of the Third Int. Workshop on CVPR for Human 
Communicative Behavior Analysis, pp. 94-101, 2010. 
[30] QcamOrbit; Logicool Inc., http://www.logicool.co.jp/ja-jp/webcam-
communications/webcams [retrieved: June, 2016] 
[31] M. Yamaguchi, T. Kanamori, M. Kanemaru, Y. Mizuno, and H. 
Yoshida, “Correlation of Stress and Salivary Amylase Activity,” 
Japanese Journal of Medical Electronics and Biological Engineering: 
JJME, vol. 39, no. 3, pp. 46-51, Sep. 2001. 
[32] OEG-16; 
Spectratech 
Inc., 
http://www.spectratech.co.jp/product/16me/ [retrieved: June, 2016] 
[33] I. Akirav and M. Maroun, “The role of the medial prefrontal cortex-
amygdala circuit in stress effects on the extinction of fear,” Neural 
Plast. Published online 2007 Jan. doi: 10.1155/2007/30837. 
[34] K. Yanagisawa, H. Tsunashima, Y. Marumo, S. Hirose, T. Shimizu, 
M. Taira, and T. Haji, “Measurement and Evaluation of Higher 
Brain Function Using Functional Near-Infrared Spectroscopy 
(fNIRS),” Journal of Human Interface Society, vol.11, no.2, pp.21-
29, 2009. 
[35] J. L. Robinson and H. A. Demaree, “Physiological and cognitive 
effects of expressive dissonance,” Brain and Cognition, vol. 63, pp. 
70-78, 2007. 
[36] W. Sato, M. Noguchi, and S. Yoshikawa, “Emotion elicitation effect 
of films in a Japanese sample,” Social Behavior and Personality, vol. 
35, pp. 863-874, 2007. 
[37] S. Srivastava, M. Tamir, K. M. McGonigal, O. P. John, and J. J. 
Gross, “The social costs of emotional suppression: A prospective 
study of the transition to college,” Journal of Personality and Social 
Psychology, vol. 96, pp. 883-897, 2009. 
[38] R.J.R. Blair, “Facial expressions, their communicatory functions and 
euro-cognitive substates,” Philos. Trans. R. Soc. Lond., B358, pp. 
561-572, 2003. 
140
International Journal on Advances in Software, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

