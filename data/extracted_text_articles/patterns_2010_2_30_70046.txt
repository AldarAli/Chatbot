A Benchmark Platform for Design Pattern Detection
Francesca Arcelli Fontana, Marco Zanoni, Andrea Caracciolo
Dipartimento di Informatica, Sistemistica e Comunicazione
University of Milano Bicocca
Milano, Italy
Email: {arcelli,marco.zanoni}@disco.unimib.it, a.caracciolo1@campus.unimib.it
Abstract—Design patterns detection is a useful activity in
reverse engineering to gain knowledge on the design issues of an
existing system, on its software architecture and design quality,
improving in this way the comprehension of the system and
hence its maintainability and evolution. Several tools have been
developed, but they usually provide different results analyzing
the same systems. Some works have been proposed in the
literature to compare these results, but a standard widely
accepted benchmark is not yet available. In this work we
propose our benchmark platform for design patterns detection,
based on a community driven evaluation.
Keywords-design pattern detection; benchmark
I. INTRODUCTION
Design pattern detection (DPD) is a topic which received
a great interest during the last years. Finding design patterns
(DPs) [1] in a software system can give very useful hints on
the comprehension of a software system and on what kind
of problems have been addressed during the development
of the system itself. Moreover, they are very important
during the re-documentation process, in particular when the
documentation is very poor, incomplete or not up-to-date.
Several DPD approaches and tools have been developed
both for forward and reverse engineering aims and involving
different techniques for the detection such as fuzzy logic,
constraints solving techniques, theorem provers, template
matching methods and classiﬁcation techniques (i.e., [2],
[3], [4], [5], [6], [7], [8], [9], [10]). In spite of the many
approaches proposed, the results obtained are often quite
unsatisfactory and different from one tool to the other.
Many tools ﬁnd many false positive instances but other
correct instances are not found. One common problem in
DPD is the so called variant problem: DPs can be im-
plemented in several ways, often very different from one
another. The main variants for each pattern are described
in the catalog of [1], others are applied when the context
of application requires it. These variations cause the failure
of most pattern instances recognition using rigid detection
approaches, which are based only on canonical pattern
instances.
Hence the comparison of the results provided by the
different tools is very important, in order to be able to
evaluate the best approach and technique. Some works that
have been proposed respect to the comparison of DPD tools
are described in the next section.
We analyzed and faced the problem related to the different
results provided by the DPD tools, since we are developing a
tool called MARPLE (Metrics and Architecture Reconstruc-
tion plug-in for Eclipse) [11] whose main aims are related to
DPD and software architecture reconstruction. The Marple
DPD module is characterized by the following steps:
• the detection of sub components or micro structures,
which give useful hints on the DP detection, with the
aim of mitigating the variant problem;
• the detection of the largest possible set of DP can-
didates performed by a module called Joiner, whose
results are characterized by very high recall values;
• the reﬁning of the previous results through data mining
techniques, in particular through a step of clustering
and a step of supervised classiﬁcation.
The aim of this work is to introduce and describe a
benchmark platform to be used to compare DPD tools. We
propose some mechanisms to obtain safer results and to
make them available to the DPD community in an easy way.
Our approach is characterized by:
• a general model for design pattern representation;
• a way of comparing results coming from different tools;
• the possibility to evaluate instances and discuss about
their correctness.
The adoption by the DPD community of a benchmark
can improve the cooperation among the researchers and the
reuse of tools written by other instead of the development
of new ones.
II. RELATED WORK
As we observed before, undertaken a comparison among
design pattern detection tools is certainly a difﬁcult task. Few
benchmark proposals for the evaluation of design pattern
detection tools have been presented in the literature. In
[12], we describe our ﬁrst proposal for a benchmark, that
we extend and ﬁnalize in this paper. In [13], the authors
present their work in progress towards creating a benchmark,
called DEEBEE (DEsign pattern Evaluation Benchmark
Environment), for evaluating and comparing design pattern
detection tools. Currently, the benchmark database contains
the results of three DP tools.
42
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

In [14], it has been deﬁned P-MARt, a repository of
pattern-like micro-architectures, with the purpose to serve as
baseline to assess the precision and recall of pattern identiﬁ-
cation tools. The repository contains the analysis of speciﬁc
version of some different open source projects. While in
[15] the authors compare different design patterns detection
tools and they propose a novel approach based on data
fusion, build on the synergy of proven techniques, without
requiring any re-implementation of what is already available.
In [16], the authors develop DPDX, a rich common exchange
format for DPD tools, to overcome limitations due to the
different output format. DPDX provides the basis for an
open federation of tools that perform comparison, fusion,
visualization, and-or validation of DPD results.
In our work we aim to provide an online tool to support
the comparison of DPD results, with the beneﬁt of having a
ﬂexible underlying model and a collaborative environment.
In future work we will experiment the correctness and
completeness of our approach exchanging data with the
above cited platforms and models.
III. DP REPRESENTATION
In order to work on design pattern instances we need a
way to represent them in some kind of data structure. In
[12], we presented a model for the representation of DP
deﬁnitions and instances. That model now stands behind all
the elaboration made by the benchmark platform.
The model speciﬁes that a design pattern can be deﬁned
from the structural point of view using the roles it contains
and the cardinality relationship between couple of roles.
A design pattern is deﬁned as a tree whose nodes are
called Levels; each Level has to contain at least one of the
roles of the pattern and it can contain other nested Levels,
recursively. In Figure 1 it is possible to see the tree structure
of the LevelDef class (representing the level deﬁnition), and
the RoleDefs it owns; ﬁnally, DpDef deﬁnes that a design
pattern deﬁnition is a tree having as root one LevelDef.
LevelDef
id : Integer
RoleDef
id : Integer
name : String
DpDef
id : Integer
name : String
roles
part_of
 [1..*]
children
parent_level
root
Figure 1.
DP Deﬁnition UML class diagram
When two roles are contained in the same level, they are
in a one-to-one relationship; instead when a role is in a
nested Level it means that for each instance of the roles in
the parent level, there can be many sets of roles of the child
level. The most common case is when a pattern deﬁnes that a
class must extend another class. In most cases we identify a
single instance of that pattern as the parent class connected
with all the children classes. Instances are modeled as in
Figure 2; the model is simply an extension of the deﬁnition,
as it models the instantiation of the concepts contained
in the deﬁnition: a RoleAssociation is the realization of a
RoleDef, a LevelInstance is the realization of a LevelDef,
and so on. The only complex detail is the splitting of Level
and LevelInstance; the explanation is that each LevelDef is
instantiated as a LevelInstance when the RoleAssociations
are ﬁlled, but to deﬁne a child Level we need to specify
which particular parent instance it belongs to.
LevelDef
id : Integer
RoleDef
id : Integer
name : String
DpDef
id : Integer
name : String
RoleAssociation
id : Integer
className : String
package : String
ﬁlePath : String
LevelInstance
id : Integer
Level
id : Integer
DPInstance
id : Integer
name : String
roles
part_of
 [1..*]
children
parent_level
level
 [1..*]
part_of
 [1..*]
is_in
 [0..*]
deﬁnition
 [0..*]
deﬁnition
root
root
Figure 2.
Model UML class diagram
A. XML format
The XML format for specifying design pattern instance is
modeled according to the representation model, so it follows
the same concepts. The XML Schema Deﬁnition, which the
43
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

submitted XML ﬁle must comply to, is available at [17];
moreover, the list of the pattern deﬁnitions the tool can
support is also available at [18].
In order to have more chances of being able to compare
results coming from different tools, currently users cannot
supply their own pattern deﬁnitions. When the platform will
be more tested and ﬁlled with data, we will add this func-
tionality. Meanwhile we accept and encourage suggestions
coming from the users about new deﬁnitions or mistakes in
the current ones.
In the following we report an XML ﬁle example that
represents an instance of an Abstract Factory design pattern.
Listing 1.
Example of Abstract Factory instance
<a n a l y s i s
xmlns=” h t t p : / /www. e s s e r e . d i s c o . unimib . i t /DPBWeb”
x m l n s : x s i =” h t t p : / /www. w3 . org / 2 0 0 1 / XMLSchema−
i n s t a n c e ”
x s i : s c h e m a L o c a t i o n =” h t t p : / /www. e s s e r e . d i s c o . unimib
. i t : 8 0 8 0 /DPBWeb/ r e s o u r c e s / DpAnalysis . xsd ”>
<p a t t e r n
name=” A b s t r a c t
F a c t o r y ”>
<p a t t e r n I n s t a n c e>
<r o l e
name=” A b s t r a c t
F a c t o r y ”
package =” org . foo . bar ”
c l a s s =” AbstractFactoryClassNa me ”
f i l e P a t h =” org / foo / bar / a1 . j a v a ”
/>
<l e v e l>
<l e v e l I n s t a n c e>
<r o l e
name=” Concrete
F a c t o r y ”
package =” org . foo . bar ”
c l a s s =” ConcreteFactoryClassName ”
f i l e P a t h =” org / foo / bar / b1 . j a v a ”
/>
</ l e v e l I n s t a n c e>
</ l e v e l>
<l e v e l>
<l e v e l I n s t a n c e>
<r o l e
name=” A b s t r a c t
Product ”
package =” org . foo . bar . baz ”
c l a s s =” AbstractProductClassName ”
f i l e P a t h =” org / foo / bar / baz / c1 . j a v a ”
/>
<l e v e l>
<l e v e l I n s t a n c e>
<r o l e
name=” Concrete
Product ”
package=” org . foo . bar . baz ”
c l a s s =” ConcreteProductClassName1 ”
f i l e P a t h =” org / foo / bar / baz / d1 . j a v a ”
/>
</ l e v e l I n s t a n c e>
<l e v e l I n s t a n c e>
<r o l e
name=” Concrete
Product ”
package=” org . foo . bar . baz ”
c l a s s =” ConcreteProductClassName2 ”
f i l e P a t h =” org / foo / bar / baz / d2 . j a v a ”
/>
</ l e v e l I n s t a n c e>
</ l e v e l>
</ l e v e l I n s t a n c e>
</ l e v e l>
<l e v e l>
<l e v e l I n s t a n c e>
<r o l e
name=” C l i e n t ”
package =” org / foo / bar / baz ”
c l a s s =” ClientClassName ”
f i l e P a t h =” org / foo / bar / baz / e1 . j a v a ”
/>
</ l e v e l I n s t a n c e>
</ l e v e l>
</ p a t t e r n I n s t a n c e>
</ p a t t e r n>
</ a n a l y s i s>
The ﬁle refers to the roles Abstract Factory, Concrete
Factory, Abstract Product, Concrete Product, each associ-
ated to a class name and to a package name, and organized
following the Abstract Factory deﬁnition.
IV. DPD PLATFORM
The platform is available at [19] and it is subdivided in:
• the documentation section contains some references
and guides to use the platform; in addiction the home
page brieﬂy introduces the system functionalities and
provides a step by step tutorial;
• the search section lets the user to ﬁnd the results of a
particular analysis, according to different parameters;
• the compare section allows to compare the instances
found by different tools on the same input project;
• the browse section provides a tree-like view of the
contents of the platform.
Through these different kinds of exploration the user
can obtain the detailed view of each pattern instance, that
includes two different types of graphic visualization and a
simple forum for the evaluation of the instance by the users.
All the above functionalities are visible to all users; if a
user wants to load new pattern instances into the platform
he must be registered, log into the platform, and submit the
XML ﬁle containing the instances and some metadata, as
shown in Figure 3. In our platform an analysis consists of
the combination of the set of the instances, its description,
the choice of the DPD tool and the analyzed project.
Tool:
DPD Tool 4.5
Project:
JHotDraw 5.1
Svn base URI :
Svn revision: 
XML file: 
Created:
06/24/2010
mm/dd/yyyy
Short description
Long description:
Upload
Upload Project Analysis
Uploaded Analyses
Tool
Project
description
DPD Tool 4.5
J HotDraw 5.1
regular scan with all DPs enabeled
View
DPD Tool 4.5
J Refactory 2.6.24
regular scan with all DPs enabeled
View
DPD Tool 4.5
J Unit 3.7
regular scan with all DPs enabeled
View
DPD Tool 4.5
Lexi 0.1.1 alpha
regular scan with all DPs enabeled
View
DPD Tool 4.5
MapperXML 1.9.7
regular scan with all DPs enabeled
View
DPD Tool 4.5
Nutch 0.4
regular scan with all DPs enabeled
View
DPD Tool 4.5
PMD 1.8
regular scan with all DPs enabeled
View
DPD Tool 4.5
QuickUML 2001
regular scan with all DPs enabeled
View
WOP 1.3
J HotDraw 5.1
regular scan with all DPs enabeled
View
WOP 1.3
J Unit 3.7
regular scan with all DPs enabeled
View
WOP 1.3
MapperXML 1.9.7
regular scan with all DPs enabeled
View
WOP 1.3
Lexi 0.1.1 alpha
regular scan with all DPs enabeled
View
WOP 1.3
Nutch 0.4
regular scan with all DPs enabeled
View
WOP 1.3
QuickUML 2001
regular scan with all DPs enabeled
View
Back to panel
http://essere.disco.unimib.it/svn/DPB/J HotDraw%20v5.1/src/
7
Figure 3.
Example of analysis loading
44
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

A. Search
The user can search a pattern instance according to the
chosen programming language, project, detection tool, and
design pattern. An example of a search section is shown in
Figure 4. In the user interface, whenever the user chooses
a ﬁlter, the page shows the available further ﬁlters. In each
selection list it is possible to choose more than one value,
to make the search more ﬂexible and personalized.
Search
C
C#
C++
Java
Perl
PHP
Python
Visual Basic
-> 
JHotDraw 5.1
JRefactory 2.6.24
JUnit 3.7
Lexi 0.1.1 alpha
MapperXML 1.9.7
Netbeans 1.0.x
Nutch 0.4
PMD 1.8
QuickUML 2001
-> 
DPD Tool 4.5
WOP 1.3
-> 
Abstract Factory
Adapter
Bridge
Composite
Decorator
Factory Method
Observer
Prototype
Proxy
Singleton
State
Strategy
Votes:
0
Stars:
***
Search
J HotDraw 5.1
MapperXML 1.9.7
 
Composite
Template Method
Composite
Template Method
 
T
F
T
F
T
F
T
F
DPD Tool 4.5
 
38
-
-
0
1
X
X
X
X
42
X
X
X
X
-
-
0
0
WOP 1.3
 
46
1
1
-
-
X
X
X
X
56
X
X
X
X
1
0
0
0
Figure 4.
Example of a pattern instance search result
Figure 4 shows an example of search parameters and
results, in the case the user looks for the analysis of project
written in the Java language, on the “JHotDraw 5.1” and
“MapperXML 1.9.7” projects, analyzed by the “DPDTool
4.5” and “WOP 1.3” tools and ﬁltered on the “Composite”
and “Template Method” design patterns.
The table (see Figure 4) shows the results with the tools
and their analysis on the rows and the projects with the
patterns on the columns. Each cell is the combination of an
analysis of a tool and a pattern belonging to a project, and it
contains the number of instances considered correct and the
number of the ones considered incorrect, respectively under
the column label “T” and “F”.
The bias value that allows the platform to choose if an
instance is correct or not is speciﬁed by the two combo
boxes named “Votes” and “Stars”. The details of this topic
are explained in Section IV-D.
There is a special case regarding the cell values: for
example, when an analysis does not contain instances of
a particular design pattern in a particular project, the corre-
sponding cells are rendered as “X” in light grey.
B. Comparison
The user can compare the results produced by two dif-
ferent analysis, obtained by two different tools, on the same
project and for the same single chosen pattern deﬁnition.
The comparison results (see Figure 5) are shown in a table
where the two instance sets, found by the two analysis, are
shown one on the rows and the other on the columns. The
cells of the table contain values indicating the similarity of
the corresponding couple of instances. The similarity is cur-
rently evaluated through a very simple algorithm described
below. The background color of each cell is proportional to
its percentage value, according to the selected color scheme
(red:0% to green:100% or white:0% to blue:100%).
25%
2%
94%
26%
59%
84%
84%
9%
49%
77%
17%
20%
50%
81%
83%
13%
Figure 5.
Example of a result comparison
Figure 5 shows an example of comparison: the user selects
to compare the instances of the Composite pattern found on
project “QuickUML 2001” by:
• the “DPD Tool 4.5” tool in the analysis named “regular
scan with all DPs enabled”;
• the “WOP 1.3” tool in the analysis named “regular scan
with all DPs enabled”.
The table shows that each tool found four instances, and
that for example the instances #1911 and #526 are very
similar, with a score of 94%, and instances #1911 and #515
are very different, having a score of only 2%. The numbers
in bold are the highest value of the rows: in fact the last
option selected is to highlight the highest value of each row;
it is also possible to do the same on the columns. This option
simpliﬁes the task of ﬁnding the instances that are more
similar in order to understand if they are really the same.
45
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

The view can be also ﬁltered removing all the rows or
columns having all the values less or equal to the one
speciﬁed, in order to simplify the table and to include only
meaningful results.
1) Comparison algorithm: In the current version of the
benchmark platform we implemented a comparison algo-
rithm we developed as a proof of concept. The algorithm
tries to express the similarity of two instances giving more
weight to the parent roles and less to the children roles, and
produces a number between 0 (total difference) and 1 (total
equivalence). The actual version of the algorithm is very
simple and any suggestions and contribution coming from
the DPD community, in order to populate the platform with
other comparison algorithms, is welcome.
Our algorithm gives a descending score to each level depth
in the pattern deﬁnition: for example in a deﬁnition with only
a parent level and a child level, the parent level depth takes
a score of 2 and the child level depth takes a score of 1;
with three level depths the ﬁrst one (the root) would take
weight 3, and so on. Then each level in the deﬁnition takes
the score of the depth it belongs to, and an overall score is
calculated as the sum of the score of all levels.
Finally a weight is assigned to each level dividing the
level score by the overall score. In this way the sum of all
the weights is 1.
When the weights are set, the algorithm compares a
couple of instances starting from the root and recursively
distributing the weight of each level on its level instances;
we consider that two level instances are equal if their sets
of role association are equal.
For example: let us say a level weights 1/2 and the two
levels to compare contain the ﬁrst one instance and the
second two instances; in addition the instance in the ﬁrst
level is equal to one of the two in the second one level.
Their direct comparison value is 1/2, because we have only
one match out of two, that is the maximum of the number
of the instances in each level. Then the overall score of the
comparison is 1/2∗1/2, because we have to weight the local
score with the global weight.
The overall score of the comparison is the sum of all the
local scores.
C. Browse
The Browse section offers a tree view of the content of the
platform. Basically it allows the user to ﬁnd all the analysis
made by a tool on a project: the user can choose as entry
point both the available projects or tools. Whenever the users
clicks on one of them, the page shows the analysis available
in combination with the other.
For example, clicking on the name of the analysis, the
platform shows the analysis details page containing the list
of the found DP instances, grouped by design pattern.
D. Evaluation
The evaluation phase allows the user to analyze the
reported instance. The instance can be graphically viewed
in two ways: in the ﬁrst way using a graph representation of
the tree representing the instance (it requires java working
in the browser), and in the second way using nested boxes
(see Figure 6) to represent the nested structure of the tree
representing the pattern instance.
AbstractHandle
by andrea @ 23/06/10 (16:18)
Hide
Abstract Class
Concrete Class
-
Uploaded by: admin
Project: J HotDraw 5.1
Tool: DPD Tool 4.5
Source code:
- Choose a file -
View  layout:
Nested Boxes
Evaluations
Could be correct but it lacks the concrete class(es)
[Post a comment
1 points
Figure 6.
Example of the visualization of an instance
The rest of the evaluation phase is concerned with the
discussion forum about the found instance. This forum is
dedicated to the evaluation of the correctness of the found
instance:
• it allows to express a score (the “Stars”) in the range
1-5, where 1 means the instance is fully incorrect, and
5 means it is correct; it is also possible to insert a
comment to argument the evaluation;
• it lets other users to express an agreement or disagree-
ment (the “Votes”) with previous evaluations.
The overall scoring is used as a parameter in the search
page (see Figure 4) to have an immediate idea about the
overall correctness of a found instance.
In fact, during the search phase, the user can specify
the minimum number of stars a pattern instance must have
in order to be considered a correct instance. In addition,
the user can specify the absolute number of agreements
(or disagreements) each evaluation must have to be safely
included in the overall stars computation. The overall stars
number of an instance is computed as the average star
number weighted with the agreement balance (the difference
between the number of agreements and disagreements), con-
sidering only the evaluations having the minimum number
of absolute agreements. Currently, the platform allows the
users to choose the number of stars in the range 1-5, and
the number of agreements (called “Votes” in the platform)
from 0 to 20.
46
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

We believe that this community driven system of classi-
fying found instances provides good common datasets, for
the test of new tools and the enhancement of the existing
ones.
Moreover, in the evaluation phase it is possible to inspect
directly the source code, selecting the ﬁle to inspect from the
combo box under the graphical representation of the pattern.
V. CONCLUSION
In this paper we presented a platform helping the design
pattern detection community having a way to compare the
results produced by the tools and the techniques that have
been proposed in the literature.
Our ﬁnal intent is not only the tool “competition” but
also the creation of a container for design pattern instances
that, through the users’ voting, will allow us to build a
large and “community validated” dataset for tool testing and
benchmarking.
For all these reasons we are convinced that this kind of
platform can be really valuable in our research area because
it allows the real sharing of information and knowledge
among all research groups interested in design patterns for
both reverse and forward engineering.
In future work we are interested in integrating different
comparison algorithms, maybe suggested and discussed with
the DPD community, in order to let the users to choose the
algorithm they think is the more appropriate; in addition,
we need to reﬁne and tune the platform settings. We are
also investigating the possibility to expose some kind of web
services in order to let registered users to make their tool able
to automate the loading of their analysis into the platform.
REFERENCES
[1] E. Gamma, R. Helm, R. Johnson, and J. Vlissides, De-
sign patterns: elements of reusable object-oriented software.
Addison-Wesley Professional, 1995.
[2] A. D. Lucia, V. Deufemia, C. Gravino, and M. Risi, “Design
pattern recovery through visual language parsing and source
code analysis,” Journal of Systems and Software, vol. 82,
no. 7, pp. 1177 – 1193, 2009.
[3] H. Huang, S. Zhang, J. Cao, and Y. Duan, “A practical pattern
recovery approach based on both structural and behavioral
analysis,” Journal of Systems and Software, vol. 75, no. 1-
2, pp. 69 – 87, 2005, software Engineering Education and
Training.
[4] M. von Detten, M. Meyer, and D. Travkin, “Reverse engineer-
ing with the reclipse tool suite,” in ICSE ’10: Proceedings of
the 32nd ACM/IEEE International Conference on Software
Engineering.
New York, NY, USA: ACM, 2010, pp. 299–
300.
[5] R. A. Olsson and N. Shi, “Reverse engineering of design
patterns from java source code,” in ASE ’06: Proceedings of
the 21st IEEE/ACM International Conference on Automated
Software Engineering.
Washington, DC, USA: IEEE Com-
puter Society, 2006, pp. 123–134.
[6] N. Tsantalis, A. Chatzigeorgiou, G. Stephanides, and S. T.
Halkidis, “Design pattern detection using similarity scoring,”
IEEE Transactions on Software Engineering, vol. 32, no. 11,
pp. 896–909, 2006.
[7] Y.-G. Gu´eh´eneuc, “Ptidej: Promoting patterns with patterns,”
in Proceedings of the 1st ECOOP workshop on Building a
System using Patterns, M. E. Fayad, Ed.
Springer Verlag,
July 2005, 9 pages.
[8] J. Niere, W. Sch¨afer, J. P. Wadsack, L. Wendehals, and
J. Welsh, “Towards pattern-based design recovery,” in ICSE
’02: Proceedings of the 24th International Conference on
Software Engineering.
New York, NY, USA: ACM, 2002,
pp. 338–348.
[9] J. Dietrich and C. Elgar, “Towards a web of patterns,” Web
Semantics: Science, Services and Agents on the World Wide
Web, vol. 5, no. 2, pp. 108 – 116, 2007, software Engineering
and the Semantic Web.
[10] Y.-G. Gueheneuc and G. Antoniol, “DeMIMA: A multi-
layered approach for design pattern identiﬁcation,” IEEE
Transactions on Software Engineering, vol. 34, pp. 667–684,
2008.
[11] F. Arcelli, C. Tosi, M. Zanoni, and S. Maggioni, “The
marple project - a tool for design pattern detection and
software architecture reconstruction,” in Proceedings of the
International Workshop on Advanced Software Development
Tools and Techniques (WASDeTT 2008), Paphos, Cyprus, July
2008.
[12] F. Arcelli, C. Tosi, and M. Zanoni, “A benchmark proposal for
design pattern detection,” in FAMOOSr 2008: Proceedings of
2nd Workshop on FAMIX and Moose in Reengineering, 2008.
[13] L. Fulop, R. Ferenc, and T. Gyimothy, “Towards a benchmark
for evaluating design pattern miner tools,” in Software Mainte-
nance and Reengineering, 2008. CSMR 2008. 12th European
Conference on, 1-4 2008, pp. 143 –152.
[14] Y.-G. Guhneuc, “Pmart: Pattern-like micro architecture repos-
itory,” in Proceedings of the 1st EuroPLoP Focus Group on
Pattern Repositories, M. Weiss, A. Birukou, and P. Giorgini,
Eds., July 2007.
[15] G. Kniesel and A. Binun, “Standing on the shoulders of giants
- a data fusion approach to design pattern detection,” in ICPC.
IEEE Computer Society, 2009, pp. 208–217.
[16] G. Kniesel, A. Binun, P. Heged˝us, L. J. F¨ul¨op, N. Tsantalis,
A. Chatzigeorgiou, and Y.-G. Gu´eh´eneuc, “A common ex-
change format for design pattern detection tools,” in CSMR
2010, March 2010.
[17] ESSeRE, “Design pattern analysis schema deﬁnition,” Web
Site, 2010, http://essere.disco.unimib.it:8080/DPBWeb/faces/
resources/DpAnalysis.xsd.
[18] ——, “Design pattern deﬁnitions documentation,” Web Site,
2010, http://essere.disco.unimib.it:8080/DPBWeb/faces/Doc
DpDef.jsp.
[19] ——, “Design pattern benchmark platform,” Web Site, 2010,
http://essere.disco.unimib.it/DPB/.
47
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

