Recognizing Physical Activities Using the Axivity Device 
Ali Mehmood Khan 
Universität Bremen 
TZi 
Bremen, Germany 
akhan@tzi.de
 
 
Abstract— Physical activity is a major part of a user's context 
for wearable computing applications. The system should be 
able to acquire the user's physical activities by using body 
worn sensors. We want to develop a personal activity 
recognition system that is practical, reliable, and can be used 
for health-care related applications. We propose to use the 
axivity device [1] which is a ready-made, light weight, small 
and easy to use device for identifying basic physical activities 
like lying, sitting, walking, standing, cycling, running, 
ascending and descending stairs using decision tree classifier. 
In this paper, we present an approach to build a system that 
exhibits this property and provides evidence based on data for 
8 different activities collected from 12 different subjects. Our 
results indicate that the system has an accuracy rate of 
approximately 92%.  
Keywords-component; Physical activities; accelerometer 
sensor; classifier. 
I. 
 INTRODUCTION  
Human activity recognition by using body worn sensors 
has received attention in recent years. Activity recognition 
systems in health care support especially in elder care, long-
term health/fitness monitoring, and assisting those with 
cognitive disorders [1, 2, 3] has been demanded. Therefore, 
recognizing human physical activities with body worn 
sensors is not a new research field; much research has 
already been done in this area. We can identify users' 
physical movements using a body movement suit [2]. We 
also have other research projects where researchers identify 
the users' physical activities using some sensors like [3, 4, 5, 
6, 7 and 8]. 
In some diseases like diabetes, heart problems, mentally 
disabled persons, elder patients are required to perform some 
physical activities in order to make them physically fit. 
Similarly, in some cases patients need to be monitored by 
nurses which is very time consuming and expensive. 
Modern day lifestyle has lead to various physical and 
mental diseases such as diabetes, depression and heart 
diseases as well. According to the World Health 
Organization, there are at least 1.9 million people dying as a 
result of physical inactivity annually [10]. 
Although, people are aware of the importance of exercise 
there is a lack of motivation due to their busy schedules. 
People need to be forced and reminded about physical 
activities. Probably automatic and personal reminders can be 
very helpful if it can monitor one’s physical activities and 
persuade people to perform them regularly. 
Activity recognition technology can tackle this problem 
as it is able to monitor an individual’s physical activities and 
their duration in order to estimate how much calories are 
being consumed on a daily basis. Those systems can also 
provide recommendations when they fail to complete enough 
exercise and it also encourages people to conduct more 
activities [12, 13 and 14]. 
In some cases, especially in heart diseases, physical 
activities are also required along with the physiological 
information for doctors in order to examine their patient's 
conditions when he is away from the doctor's clinic [19]. 
We want to develop a physical activity recognition 
system using a minimum amount of sensors which should be 
able to identify the basic activities like lying, walking, 
running, sitting, standing, cycling, ascending and descending 
stairs.  
In our research we want to prove that it is possible to 
identify the aforementioned activities for a specific user by 
using a 3D accelerometer. In next chapter, “related work” 
will be discussed, “hypothesis and research question” will be 
discussed in the 3rd chapter, “experimental methodology” 
will be discussed in the 4th chapter, “evaluation” will be 
discussed in the 5th chapter and “conclusion and future work” 
will be in the last. 
II. 
RELATED WORK 
There are several ways to recognize a person’s daily 
activities. One way is using cameras to visually detect 
people’s motion [15, 16]. 
The drawback of this solution is that a large number of 
cameras would be required in order to monitor a moving 
person. This system would also need to be designed to 
compute information from each camera and deal with other 
factors such as light, distance and angle, which make the 
system impractical. 
Researchers have identified various physical activities 
using wearable sensors like sitting[3,6,7,8], standing 
[3,6,7,8], lying [6], walking [3,4,5,6,7,8], climbing stairs 
[3,4,6,7,8], running [5,7,8], cycling [5,8], strength training 
[8] etc. However for their recognition system they have used 
more than one sensor. For example, some researchers 
identified around 20 activities using 5 sensor boards. They 
identified walking, walking carrying items, sitting & 
relaxing, working on computer, standing still, eating or 
drinking, watching TV, reading, running, bicycling, 
stretching, strength-training, scrubbing, vacuuming, folding 
laundry, lying down & relaxing, brushing teeth, climbing 
147
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-252-3
eTELEMED 2013 : The Fifth International Conference on eHealth, Telemedicine, and Social Medicine

stairs, Riding elevator and Riding escalator using Decision 
Table, IBL, C4.5 and Naive Bayes algorithms. They placed 
sensors on the limb positions and on the right hip [8]. 
Similarly researchers identified 12 activities using 3 sensor 
boards, they identified sitting, standing, walking, walking up 
stairs, walking down stairs, riding elevator down, riding 
elevator up, brushing Teeth[3], researchers identified 3 
activities; walking, climbing stairs and descending stairs 
using 9 tilt switches using K-means clustering and brute 
force algorithms, these sensors were worn just above the 
right knee [4]. 
In our work, we want to use only one 3D accelerometer 
sensor in order to identify a few activities. A few of these 
physical activities (lying, sitting, walking, running) have 
already been identified by using a single device [9] but in our 
research we want to identify more physical activities by 
using a single wearable 3D accelerometer sensor and we also 
want to use different locations on a person's back, as opposed 
to the approach presented in paper [9] where the focus was 
only on the lower part of a person's backbone. 
III. 
HYPOTHESIS AND RESEARCH QUESTION 
The acceleration measured by a 3 axis accelerometer 
(X,Y,Z) at a specific point (backbone), indicates which 
activity the person is performing (lying, sitting, walking, 
standing, cycling, running, ascending and descending stairs), 
using classifier algorithms (J48, AODE). 
In this paper, we investigate some practical aspects of 
creating an automatic, personal activity recognition system. 
Through our experiments, we want to find the answers of the 
following questions: 
• Is it possible to identify which activity the person is 
performing (lying, sitting, walking, running, standing, 
cycling, ascending and descending stairs) by using a 3D 
wearable accelerometer sensor on participants' backbone? 
• Which particular location on a person's backbone is 
better for identifying these activities? 
IV. 
EXPERIMENTAL METHODOLOGY 
We used AX3 data logger [1] in order to identify 
physical activities (as shown in Figure: 1).  
 
Figure 1: Axivity device 
It was worn on the participants' backbone and they wore 
it on three different locations of their backbone; lower, 
middle and upper part respectively (as shown in Figure: 2). 
Participants were required to perform each activity for two 
minutes; one minute was meant for training data and other 
minute was for test data. 
The AX3 data logger contains 3-axis of accelerometer 
with flash memory and clock. This device is small and 
easy to use, its dimensions are 6x21.5x31.5 mm and its 
weight is 9 grams. 
The device comes with pre-installed software with the 
possibility to configure its settings. For example, we can 
configure sample rate, gravity etc. It continuously logs 
contextual information (time; hh:mm:ss and axis; X, Y, Z) 
to its internal memory. We can also set the duration for 
logging this information. There is also a possibility to 
export the logged data from the device to a computer in 
CSV format. 
 
In order to attach this device on the participants’ back, 
we used sticky tape which was directly placed on the skin. 
We logged continuous data with 8G and the sample rate 
was 100 Hz.  
 
We implemented an application for ‘Pocket PC’ 
where we can state the starting and ending time for each 
physical activity during experiments. This application 
generates text files with this information for each physical 
activity for both training data and test data. It also stores 
the participants’ personal information i.e. age, gender, 
height and weight. We implemented another application in 
Java for analysis; we used WEKA APIs [17] in order to 
use machine learning algorithms. This application requires 
three input files: both training and test data from ‘Pocket 
PC’ as well as the CSV file from the axivity device. 
Firstly, it filters needed data from the CSV file based on 
the time stamp from the files from the ‘Pocket PC for each 
physical activity and generates training and test data files 
in ARFF format. Later, it applies J48 and AODE 
algorithms on training data for generating models from 
both machine learning algorithms. After-wards these 
models take data as an input in order to predict their values 
and compared with ground truth. 
 
We recruited 12 testers (7 males, 5 females) for our 
experiment setup. The range of participants' age was from 
20 to 30 and ranged in BMI (body mass index) [10] from 
18.7 to 28.7 (mean 23.1, SD 2.98). They performed each 
physical activity (Lying, Sitting, Standing, Walking, 
Running, Cycling, Ascending and Descending stairs) 
twice. Two of our testers could not participate in 'Cycling' 
activity. Participants' were continuously observed during 
experiments. 
 
148
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-252-3
eTELEMED 2013 : The Fifth International Conference on eHealth, Telemedicine, and Social Medicine

  
Figure 2: Backbone’s location for the axivity device 
V. 
EVALUATION 
At the end we collected data from 12 participants and 
each participant performed eight different physical activities 
(except two participants who performed only seven), each 
physical activity contains a data of two minutes with a 
sample rate of 100Hz which implies we gathered 
(100X60X2X8) 96,000 instances for each data-set except 
two where we managed only 84,000 instances. We divided 
each data-set into two parts; one part was for training data 
and other was for test data. We generated a model from 
training data and then applied it to test data in order to 
predict the values.  We got 100 values(X, Y, Z) from the 
axivity device for each second because the sample rate was 
set to 100 Hz and we also got 100 predictions for each 
second. We wanted to have a single prediction for each 
second, therefore the activity with the maximum number of 
instances per second was chosen. This resulted in a single 
value for each second, instead of the 100 values that are 
received from the axivity device every second, leading to a 
much easier analysis of the experiment. After-wards these 
single values were compared with the ground truth of the 
physical activity to realize the accuracy of our test.  
TABLE I.  
PREDICTED RESULTS FROM BACKBONE’S LOWER PART 
 
Min 
Max 
 
Avg 
 
SD 
 
J48 
(J48) 
(J48) 
(J48) 
AODE 
(AODE) 
(AODE) 
(AODE) 
Lying 
56.67 
100 
95.14 
12.58 
93.33 
100 
99.31 
1.94 
Walking 
100 
100 
100 
0 
88.33 
100 
98.75 
3.34 
Running 
81.67 
100 
97.3 
5.6 
90.91 
100 
98.97 
2.71 
Sitting 
65 
100 
96.14 
9.93 
73.33 
100 
96.97 
7.58 
Standing 
88.33 
100 
98.06 
3.95 
86.67 
100 
97.51 
4.52 
Cycling 
83.33 
100 
97 
5.26 
80 
100 
96.5 
6.16 
Ascending stairs 
0 
98.33 
84.4 
27.62 
0 
100 
84.63 
27.66 
Descending stairs 
18.33 
100 
82.15 
21.65 
11.67 
100 
80.89 
23.78 
Our results (Table 1) show that placement of the axivity 
device on the lower part of the backbone was able to predict 
all physical activities with the accuracy of more than 80%. 
Lying, Walking, Sitting, Standing and Cycling activities 
were predicted with the accuracy of more than 95%. 
Walking activity was predicted 100% by J48 classifier. 
TABLE II.  
PREDICTED RESULTS FROM BACKBONE’S MIDDLE PART 
 
Min 
Max 
Avg 
SD 
(J48) 
(J48) 
(J48) 
(J48) 
(AODE) 
(AODE) 
(AODE) 
(AODE) 
Lying 
40 
100 
92.08 
17.82 
93.33 
100 
99.16 
1.94 
Walking 
96.67 
100 
99.58 
1.04 
95.33 
100 
99.06 
1.59 
Running 
91.38 
100 
98.84 
2.81 
93.1 
100 
98.56 
2.44 
Sitting 
46.67 
100 
90 
18.33 
46.67 
100 
90 
18.35 
Standing 
30 
100 
85.83 
21.76 
35 
100 
86.66 
20.65 
149
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-252-3
eTELEMED 2013 : The Fifth International Conference on eHealth, Telemedicine, and Social Medicine

Cycling 
81.67 
100 
97.83 
5.8 
76.67 
100 
96.83 
7.3 
Ascending 
stairs 
6.67 
98.33 
80.7 
24.32 
23.33 
100 
83.05 
21.12 
Descending 
stairs 
73.33 
100 
84.75 
13.5 
63.33 
100 
88.35 
10.42 
Our results (Table 2) show that placement of the axivity 
device on the middle part of the backbone was able to 
predict all physical activities with the accuracy of more than 
80%. Walking activity was predicted 99% by J48 and 
AODE classifiers. 
TABLE III.  
PREDICTED RESULTS FROM BACKBONE’S UPPER PART 
 
Min 
Max 
Avg 
SD 
J48 
J48 
J48 
J48 
AODE 
AODE 
AODE 
AODE 
Lying 
60.67 
100 
95.75 
11.23 
96.67 
100 
99.72 
0.96 
Walking 
48.33 
100 
92.78 
15.1 
61.67 
100 
94.58 
11.42 
Running 
93.1 
100 
97.9 
3.45 
90 
100 
98.04 
3.26 
Sitting 
1.67 
100 
80.7 
37.59 
0 
100 
78.2 
37.57 
Standing 
58.33 
100 
92.08 
12.6 
61.67 
100 
90.97 
12.15 
Cycling 
96.67 
100 
99.33 
1.17 
95 
100 
99.17 
1.62 
Ascending stairs 
15 
100 
80.03 
24.37 
21.67 
100 
81.23 
22.54 
Descending stairs 
43.1 
93.33 
75.95 
20.34 
40 
100 
83.68 
17.34 
Our results (Table 3) show that placement of the axivity 
device on the upper part of the backbone was able to predict 
all physical activities with the accuracy of more than 75%. 
Cycling activity was predicted 99% by J48 and AODE 
classifiers. 
TABLE IV.  
COMPARISON WITH ALL BACKBONE’S 
LOCATIONS 
 
Low 
Mid 
Up 
J48 
(J48) 
(J48) 
AODE 
(AODE) 
(AODE) 
Lying 
95.14 
92.08 
95.75 
99.31 
99.16 
99.72 
Walking 
100 
99.58 
92.78 
98.75 
99.06 
94.58 
Running 
97.3 
98.84 
97.9 
98.97 
98.56 
98.04 
Sitting 
96.14 
90 
80.7 
96.97 
90 
78.2 
Standing 
98.06 
85.83 
92.08 
97.51 
86.66 
90.97 
Cycling 
97 
97.83 
99.33 
96.5 
96.83 
99.17 
Ascending stairs 
84.4 
80.7 
80.03 
84.63 
83.05 
81.23 
Descending stairs 
82.15 
84.75 
75.95 
80.89 
88.35 
83.68 
 
Our results (Table 4) show that “laying” activity was 
predicted with an accuracy of 99% by the AODE classifier 
from all locations, “Walking” was predicted with more than 
an accuracy of 98% from lower and middle parts of 
backbone, “running” was predicted with more than an 
accuracy of 97% from all locations, “sitting” was predicted 
with an accuracy of 96% from lower backbone, “cycling” 
activity was predicted with more than an accuracy of 96% 
from all locations, “ascending stairs” activity was predicted 
in the range of 80% to 85% by J48 and AODE classifiers 
from all locations and “descending stairs” activity was 
predicted in the range of 84% to 89% by J48 and AODE 
classifiers from middle part of backbone. 
 
 
150
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-252-3
eTELEMED 2013 : The Fifth International Conference on eHealth, Telemedicine, and Social Medicine

TABLE V.  
 BACKBONE’S LOCATION-WISE PERFORMANCE 
 
Min 
Max 
Avg 
SD 
J48 
J48 
J48 
J48 
AODE 
AODE 
AODE 
AODE 
Low_backbone 
82.15 
100 
93.77 
6.66 
0 
99.31 
94.19 
7.19 
Mid_backbone 
80.7 
99.58 
91.2 
7.13 
83.05 
99.16 
92.71 
6.43 
Up_backbone 
75.95 
95.75 
89.32 
9.06 
81.23 
99.72 
90.7 
8.6 
Our results (Table 5) show that our system was able to 
predict physical activities with better accuracy rate (in terms 
of average) if acceleration data is coming from lower part of 
the backbone. 
VI. 
CONCLUSISION AND FUTURE WORK 
Our system is able to recognize a high percentage of the 
physical activities with the help of the decision tree and 
AODE classifiers. Results have shown that one 3D 
accelerometer sensor is enough for identifying a few physical 
activities (sitting, standing lying, walking, running, cycling, 
ascending and descending stairs). For every user, the system 
needs to be trained with the sensor data so that it would be 
able to predict the physical activities using the axivity 
device. This prototype is only a "proof of concept" and our 
results show that a single 3D accelerometer sensor can 
identify the above mentioned physical activities independent 
of BMI (body mass index) and age group. The accelerometer 
sensor has to be fixed properly on the backbone of the tester 
in order to predict the tester’s movements successfully. To 
conclude our discussion we can safely lay claim to being 
able to identify the aforementioned physical activities by 
using a 3D wearable accelerometer sensor and our results 
show that lower part of the backbone can be a good location 
for the wearable 3D accelerometer sensor.  
We will put the accelerometer sensor on other parts of 
the body in order to identify some other physical activities 
and we will use it for online machine learning. 
ACKNOWLEDGMENT 
This research was funded by USEFIL (www.usefil.eu). 
REFERENCES 
[1] Axivity -  
http://axivity.com/v2/index.php?page=product.php&product=index
.php?page=product.php&product=ax3 (12.12.12) 
 
[2] Xsens MVN - http://www.xsens.com/en/general/mvn 
(12.12.12) 
[3] J. Lester, T. Choudhury, and G. Borriello, “A practical 
approach to recognizing physical activities,” Lecture Notes in 
Computer Science 3968 (2006): 1–16. 
 
[4] K. Van Laerhoven and A. K. Aronsen, “Memorizing what you 
did last week: Towards detailed actigraphy with a wearable 
sensor,” in Distributed Computing Systems 
Workshops, 2007. ICDCSW'07. 27th International  
Conference on, 2007, 47–47. 
 
[5] T. Choudhury et al., “The mobile sensing platform: An 
embedded 
activity 
recognition 
system,” 
IEEE 
Pervasive 
Computing (2008): 32–41. 
 
[6] N. Kern, B. Schiele, and A. Schmidt, “Multi-sensor activity 
context detection for wearable computing,” Lecture Notes in 
Computer Science (2003): 220–234. 
 
[7] Uwe Maurer, Anthony Rowe, Asim Smailagic and Daniel 
Siewiorek, “Location and Activity Recognition Using eWatch: A 
Wearable Sensor Platform,” Lecture Notes in Computer 
Science3864 (2006): 86. 
 
[8] L. Bao and S. S Intille, “Activity recognition from 
userannotated acceleration data,” Lecture Notes in Computer 
Science (2004): 1–17 
 
[9] Ali Mahmood Khan 2011, "Recognizing Physical Activities 
using Wii remote", ICIKM 2011; Haikou, China  
 
[10] 
World 
Health 
Organization: 
Move 
for 
Health, 
http://www.who.int/moveforhealth/en 
 
[11] Manson, J.E., Skerrett, P.J., Greenland, P., VanItallie, T.B.: 
The Escalating Pandemics of Obesity and Sedentary Lifestyle: A 
Call to Action for Clinicians. Arch. Intern. Med. 164(3), 249–258 
(2004) 
 
[12] Consolvo, S., et al.: Activity Sensing in the Wild: A Field 
Trial of UbiFit Garden. In: CHI 2008 (2008) 
 
[13] Lin, J., Mamykina, L., Lindtner, S., Delajoux, G., Strub, H.: 
Fish’n’Steps: Encouraging Activitiy with an Interactive Computer 
Game. In: Dourish, P., Friday, A. (eds.) UbiComp 2006. LNCS, 
vol. 4206, pp. 261–278. Springer, Heidelberg (2006) 
 
[14] Anderson, I., Maitlan, J., Sherwood, S., Barkhuus, 
L.,Chalmers, M., Hall, M., Brown, B., Muller, H.: Shakra: 
Tracking and Sharing Daily Activity Levels with 
Unaugmented Mobile Phones. Mobile Networks and Applications, 
185–199 (2007) 
 
[15] Pavan, T., Chellappa, R., Subrahmanian, V.S., Udrea, O.: 
Machine Recognition of Human Activities: A survey. IEEE 
Transactions on Circuits and Systems for Video Technology 
18(11) (2008)  
 
[16] Hu,W., Tan, T., Wang, L., Maybank, S.: A survey on Visual 
Surveillance of Object Motion and Behaviors. IEEE Transactions 
on Systems, Man, and Cybernetics. Part C: 
Applications and Reviews 34(3) (2004) 
151
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-252-3
eTELEMED 2013 : The Fifth International Conference on eHealth, Telemedicine, and Social Medicine

 
[17] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann, Ian H. Witten (2009); The WEKA 
Data Mining Software: An Update; SIGKDD Explorations, 
Volume 11, Issue 1. 
 
[18] Body Mass Index - http://www.nhlbisupport.com/bmi/ (6th 
August, 2012) 
 
[19] Ali Mahmood Khan. (2011). Personal state and emotion 
monitoring by wearable computing and machine learning. 
BCS-HCI 2012, Newcastle, UK 
152
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-252-3
eTELEMED 2013 : The Fifth International Conference on eHealth, Telemedicine, and Social Medicine

