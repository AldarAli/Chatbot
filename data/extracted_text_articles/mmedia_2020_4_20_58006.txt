A non-Invasive Approach to Extract the User’s Patterns of Visual Arts Exploration 
through Wearable Technologies Application: the NEFFIE Project 
Diana Trojaniello, Matteo Zardin, Marco Mura, Alberto Sanna  
Center for Advanced Technology in Health and Wellbeing 
IRCSS San Raffaele Hospital 
Milan, Italy 
e-mail: trojaniello.diana@hsr.it, zardin.matteo@hsr.it, mura.marco@hsr.it, sanna.alberto@hsr.it  
 
 
Abstract— 
Understanding 
human 
cognitive 
perception 
processes during visual art fruition represents both a 
neuroscientific and technological challenge. By addressing the 
cognitive processes behind the art appreciation and by 
employing the last generation technologies to analyze human 
bio signals, the NEFFIE project aims to propose a new 
approach to emphasize the visual art fruition experience while 
increasing the awareness of what we actually see. The project 
consists of four different experimental phases, including both 
In-Lab and Out-Lab evaluations: I) In-Lab images rating; II) 
In-Lab functional Magnetic Resonance Imaging fMRI -based 
study; III) In-Lab Wearable-based study; IV) Out-Lab 
Wearable-based study. Through these experimental steps, the 
NEFFIE project will develop a unique platform based on 
Artificial Intelligence human-centric algorithms to identify 
each person’s unique fingerprint of visual art perception and 
discovery. The current idea-paper aims to describe the above-
mentioned experimental phases. 
Keywords - visual art; wearable; fMRI; artificial intelligence; 
machine learning. 
I. 
 INTRODUCTION 
 In literature, Visual Arts (VAs) including paintings, 
sculptures and photography have been always defined as an 
aesthetic expression of interiority and of the human soul. 
VAs reflect the artist's opinions, feelings and thoughts in the 
social, moral, cultural, ethical and religious context of his 
historical period. Philosophers and semantic scholars, 
however, argue that an objective language exists that, 
regardless of the eras and styles, should be codified in order 
to be understood by everyone, but so far efforts to 
demonstrate this claim have been found unsuccessful.  
Thanks to the technology advancements, nowadays it is 
possible to understand more precisely how the subjective 
process of aesthetic appreciation of VA forms takes place. 
Recent studies showed that “aesthetic experience” involves 
brain areas devoted to different functions [1], such as the 
body representation and its movements, and the analysis of 
the hedonic value of perceived stimuli. The above-mentioned 
brain areas are activated automatically when people observe 
VA forms, even if they have not been asked to judge them 
critically. Functional Magnetic Resonance Imaging (fMRI) 
allow to identify the brain areas associated with the aesthetic 
experience. 
However, such an in-depth study has physical and non-
physical limits, e.g., the impossibility to present images for a 
longer time interval, necessary for the observer to ensure 
deep contemplation. This is why new technologies and 
sensors (i.e., wearable devices able to monitor physiological 
signals) could be employed to monitor physiological 
parameters in un-constrained environments thus allowing a 
longer VA form fruition in a quasi-real life context. 
Monitoring methods such as ElectroEncephaloGraphy 
(EEG), Eye-Tracking (ET), Face Recognition (FR). 
PhotoPletismoGraphy (PPG) and Galvanic Skin Response 
registration (GSR) allow to collect physiological data highly 
correlated with the brain activity and emotions through 
wearable devices mounted on the subjects [2]. For example, 
the usage of such methods through an integrated multi-
sensors platform allow to register the electrical activity 
produced by populations of neurons on the observer cerebral 
cortex (EEG), while detecting eye movements and fixation 
points (ET), to identify which aspects of the VA form 
capture the attention of the observer and reach his 
consciousness. In addition, by including the emotions as 
expressed by the face (FR) and electro dermal activity 
variation (GSR) it is possible to detect the emotional 
activation level of the observer.  
 The NEFFIE project will develop a unique multi-sensors 
platform based on Artificial Intelligence (AI) human-centric 
algorithms to identify each person’s unique fingerprint of 
visual art perception and discovery in un-constrained 
environments. The current idea-paper aims to describe the 
experimental activities flow that will allow to develop the 
NEFFIE platform. The paper is organized as following: in 
Section I the introduction has been reported; in Section II the 
project experimental phases have been described; in Section 
III conclusion and future works have been reported.  
II. 
PROJECT EXPERIMENTAL PHASES  
In the following sections (A, B, C, D), the four 
experimental phases of the NEFFIE project (In-Lab images 
rating, In-Lab fMRI based study, In-Lab Wearable based 
study, Out-Lab Wearable based study) have been briefly 
described.  
A. In-Lab images rating 
The first phase of the project consisted in an image 
(photo) rating study. A total of 218 images have been 
collected and divided in two groups of images, i.e., Neffie 
Group (N-G) and Control Group (C-G). The N-G images 
were characterized by an high presence of reflections and in 
53
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-772-6
MMEDIA 2020 : The Twelfth International Conference on Advances in Multimedia

general complex elements, while the C-G ones were 
characterized by a simpler content. Eighty healthy subjects 
have been included in the study and divided into 4 groups. 
Each group was asked to rate on a 7-points Likert scale the 
218 images, presented in a random way, with respect to one 
of the four following dimensions: a) presence of reflections; 
b) complexity; c) beauty; d) stimulating. The results obtained 
in this first study allowed to identify a sub-set of images 
(n=61) characterized by higher levels of complexity and able 
to arouse the observer. 
B. In-Lab fMRI-based study 
The second phase of the project consisted in a fMRI 
based study. The study has been designed on the basis of the 
results obtained in the previous phase by including the subset 
of 61 images identified in the In-Lab image rating study. 
Thirty-six healthy subjects will take part to the study and will 
be invited to receive a fMRI evaluation while observing the 
61 N-G images and 61 C-G matched ones. The main 
objective of this phase is to evaluate the BOLD signal 
variation through the fMRI technique in those brain area 
involved in the complex visual stimuli analysis (N-G 
images). In general, this phase of the project aims to 
establish a relation between the brain activity and the 
aesthetic experience of the subject while observing N-G and 
C-G images. In addition, the eye-tracking technique will be 
employed to measure the fixation points and the image area, 
which is more involved in the visual exploratory pattern of 
the subject. This study will start in the first part of 2020.  
C. In-Lab Wearable-based study 
The second phase of the project consisted in a wearable 
sensors based study performed in-lab environment under the 
supervision of a researcher. The main objective of this phase 
was to identify the minimum viable number of sensors able 
to describe in the most accurate way the aesthetic experience 
(i.e., emotion level through valence and arousal levels 
identification) of the subject while observing a VA form. A 
multi sensors platform has been developed including the 
following devices: EEG, ET, GSR and PhotoPletismoGraphy 
PPG. The platform allowed the researchers to register the 
subjects bio-signals synchronized with the visualization of a 
number of images extracted by the Nencki Affective Picture 
System (NAPS) database.  
 
 
Figure 1.  The In-Lab Wearable based study experimental setup. 
 
Figure 2.  The Out-Lab Wearable based study experimental setup. 
Forty-three subjects have been enrolled in this study. 
Bio-signals were recorded during the presentation of visual 
stimuli (baseline image, eliciting image) from NAPS, which 
were at the same time evaluated by the subjects (self-reports 
on arousal, valence and emotional label). Figure 2 shows the 
experimental setup adopted for the experiment. AI 
algorithms including Neural Networks and Support Vector 
Machines have been then applied in order to find the most 
accurate one to identify arousal and valence levels (i.e., with 
respect to the reference values associated to each image as 
reported in the NAPS database).  
D. Out-Lab Wearable-based study 
The last phase of the project consisted in the application 
of the previous phase in an un-constrained environment. The 
same setup as developed and used in the In-Lab wearable 
based study phase has been placed inside a photo booth 
provided by an external company (as shown in Figure 3). 
The photo boot has been equipped with a touch screen. The 
subjects will be invited to enter in the photo booth and to 
wear the wearable devices placed inside and then to start the 
experiment. A set of images among those 61 ones selected in 
the image rating study will be presented on the screen while 
the subject bio-signals will be registered. Once the image 
presentation will be concluded, the subject will be requested 
to choose among one of them to be printed. The printed 
image will be the reinterpretation of the observed image on 
the basis of the bio signals registered and analyzed.   
This study will start in March 2020.  
 
III. 
CONCLUSIONS AND FUTURE WORKS  
The present paper outlines the current experimental 
phases of the NEFFIE project. The In-Lab fMRI as well as 
the Out-Lab Wearable based studies will start in the first 
half of 2020. 
REFERENCES  
[1] G. Udovičić, J. Ðerek, M. Russo, and M. Sikora. 2017. 
Wearable Emotion Recognition System based on GSR and PPG 
Signals. In Proceedings of the 2nd International Workshop on 
Multimedia for Personal Health and Health Care.  
[2]  E. Vessel, G.G. Starr, N. Rubin, 2012. The brain on art: 
intense aesthetic experience activates the default mode network 
Front Hum Neurosci. 2012; 6: 66. 
54
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-772-6
MMEDIA 2020 : The Twelfth International Conference on Advances in Multimedia

