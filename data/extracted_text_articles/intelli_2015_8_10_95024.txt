Process State Observation Using Artiﬁcial Neural Networks and Symbolic Regression
Susanne Fischer
Intelligent Systems Research Group
Karlsruhe University of Applied Sciences
Karlsruhe, Germany
Email: susanne.fischer@hs-karlsruhe.de
Abstract—Process state observation is important for efﬁcient
automated online control in manufacturing. In this paper, we
propose a new concept for online observation of the process state.
First, we obtain state variables of physically-based numerical
simulations of a process. After that, we reduce the state variables
to a few characteristic features using artiﬁcial neural networks.
As a result, we have a process state representation in a lower
dimensional feature space. Using this representation, we apply
symbolic regression to ﬁnd a mathematical model that describes
the state in the feature space. By applying these methods to a cup
deep drawing process, we can describe 99% of the state variables’
variation using only 7 features instead of 400. For these 7 features,
we can ﬁnd mathematical descriptions that represent a reduced
process model which is used for process state observation.
Keywords–Observation; control; reduction; regression; manu-
facturing.
I.
INTRODUCTION
Process control needs information about the current process
state to adjust controllable process parameters. However, if
the process state is not directly - or only with large effort
- available during the process, a state observer is required
to gain and observe process state information from available
measurements (observables). For this reason, a process model
is necessary to predict the immeasurable process state.
Given that the process model is used for process ob-
servation, it is evident that the process model has to be
online-capable. For this purpose, black-box models or gray-
box models can be applied. Black-box models characterize
the process input-output relation. In addition to the input-
output relation, gray-box models determine the resulting model
structure.
One of the most used methods for implementing a reduced
model is the proper orthogonal decomposition (POD), inde-
pendently proposed by Kosambi [1], Karhunen [2], and Lo`eve
[3]. It is also known by Karhunen-Lo`eve theorem (KLT) and
principal component analysis (PCA). Rozza [4] covers a wide
range of applications in engineering. Using POD, the reduced
model uN(x, t) is represented as a linear combination of basis
functions φj(x):
uN(x, t) =
N
X
j=1
aj(t)φj(x),
(1)
where aj(t) are time-dependent model coefﬁcients and N is
the reduced dimension. It is possible to apply POD to nonlinear
problems, but the result leads only to a linear reduction [5].
Established methods for black-box modeling are Kriging
and artiﬁcial neural networks (ANN). Kriging is based on the
interpolation of the simulation results [6]. ANNs are trained
to emulate the system by mapping the process input to the
process output [7].
We intend to predict the process state and process dynamic,
automated and online-capable. When implementing an online-
capable model, one concern is the complexity of the process.
Physical-based numerical process simulations predict the pro-
cess state very accurately, but they are also computationally
expensive and hence not feasible for online control. However,
the resulting process state of the numerical simulation can be
used to create a reduced process model.
We obtain process state variables for each time step and for
different process parameters and use artiﬁcial neural networks
to apply a nonlinear dimensionality reduction to reduce the
obtained state variables to a few new state features. To achieve
a gray-box model, we apply symbolic regression to describe
the time evolution of the state in the reduced space. The
determined process model is considered for process state
observation by running the process model in parallel to the
process.
In Section II, we introduce the several components of
the state observation (Figure 1), including the dimensionality
reduction algorithm. The result of this algorithm is compared
process
measurement
correction
model
measurement
model
process
model
controller
−
s
ˆs
u
uo
uc
ˆo
o
Figure 1. Components of the observer (blue) in parallel to the process.
to the results of the PCA and the nonlinear principal compo-
nent analysis (NLPCA), introduced by [8]. The feasibility of
this process state observation and dimensionality reduction is
evaluated with data of a deep drawing ﬁnite element model in
Section III and discussed in Section IV.
142
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

II.
STATE OBSERVATION
Standard observers for linear systems are the Luenberger
observer [9] and the Kalman ﬁlter [10]. The concept for state
observation proposed here is based on the Luenberger observer.
We adopt the idea to run the observer in parallel to the process
(Figure 1) and feed back the error between true observables o
and predicted observables ˆo to adjust the process model. An
additional output is the predicted state ˆs to feed the controller
with the required information.
The observer components are: (1) the process model that
predicts the state for the controller; (2) the measurement
model that maps the predicted state to observables; and (3)
the correction model. The inner construction of the individual
components is explained in the following as well as the
required dimensionality reduction.
A. Dimensionality Reduction
The goal of the dimensionality reduction is to reduce the
number of state variables of physical-based numerical process
simulations to a few characteristic features. The result is a
representation of the state in feature space.
PCA is a widely used linear dimensionality reduction
method. It starts from the premise that the maximal variance in
the data corresponds to the highest information gain and hence
small variances constitute negligible information or even noise.
Thus, the feature space is described by the directions of the
highest variances.
NLPCA, a nonlinear dimensionality reduction method, is
based on an autoencoder — a bottleneck neural network where
the input has the same meaning as the training target. Here,
input and training target are the state variables. The activation
of the bottleneck layer represents the extracted features. The
NLPCA uses a hierarchical error function to achieve ordered
features.
In [11], we described a nonlinear dimensionality reduction
method based on artiﬁcial neural networks. This approach
extracts ordered features that represents the process state
in feature space. It has several individual bottleneck neural
networks (BNN) that are arranged sequentially (seqBNN), with
one bottleneck node each (Figure 2). The input X are the
state variables that get reduced through the mapping layer
to the bottleneck node. The activation of the bottleneck node
represents the extracted feature. We have one network for each
feature, so that we can examine each feature separately.
The training target of the ﬁrst network are the state vari-
ables Y1 = X. To construct the second feature, we keep the
input and subtract the predicted output ˆY1 from the true target
Y1 and set it as the training target of the second network. Thus,
the second feature is a reduction of the state variables in such a
way that the feature reconstructs the variation of state variables
that could not be deﬁned by the ﬁrst feature. This continues
until we ﬁnd n features that describe the state with only a
negligible residual.
Thus, the network is trained to reduce the state variables to
new features which in turn can reconstruct the state variables.
The sum of the predicted output of each network corresponds
to the reconstructed state variables. The state variables for the
ofﬂine training are known through numerical simulation using
ABAQUS (ﬁnite element analysis software).
B. Process Model
Symbolic regression [12][13], a ﬁeld of genetic program-
ming, can be used to identify process models. In contrast
−
−
X
ˆY1
Y1
X
ˆY2
Y2
X
ˆYn
Yn
z2
zn
z1
...
Figure 2. Architecture of the sequential BNN used for the dimensionality
reduction.
to conventional regression methods, symbolic regression es-
timates not only the parameters, but, in addition, the structure
of the required mathematical models. It has the advantage that
expert knowledge about the structure of the model can be
embedded and in turn the resulting model can be interpreted
by experts.
Symbolic regression arranges the mathematical model in
a binary tree (the so-called individuals) where constants,
variables, and/or operators are children of operators (Figure
3). Using an entire population of individuals and genetic
·
/
4
x
sin
·
2
x
Figure 3. Symbolic regression individual for 4
x · sin(2x).
operations (such as mutation, crossover, and selection), the
individuals are changed until an appropriate mathematical
model is found that describes the given data points best.
In our case, we are trying to ﬁnd a model that describes
the trajectory of the state in the extracted feature space. The
reduced state s depends on time t, the control parameter uc,
and noise ν: ⃗s(t, uc, ν). In this paper, we do not consider the
inﬂuence of the noise. Consequently, the trajectory of the state
⃗s can be described by:
⃗s(t, uc) =
n
X
i=1
zi(t, uc)⃗ei,
(2)
143
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

where zi(t, uc) is the i-th state feature depending on time t and
process parameter uc, ⃗ei the unit vector, and n the dimension.
C. Measurement Model and Correction Model
The measurement model maps the extracted features to the
predicted observables ˆo (Figure 1). Subsequently, the predicted
observables are compared to the true observables o.
Senn [14] shows that a three-layer artiﬁcial neural network
(input, hidden, and output layer) is sufﬁcient to predict state
variables from observables with a negligible error. We assume
that the inverse mapping, with the state variables as input
and the observables as output of the neural network, provides
similarly satisfying results.
The predicted reduced state variables of the process model
and true observables, known from the numerical simulations,
are used as the input and output, respectively. Thus, the
numbers of input nodes and output nodes are predeﬁned. To
determine the number of nodes in the hidden layer, we use the
following equation, based on [15]:
H = OS − O
α(I + O),
(3)
where I is the number of input neurons, O the number of
output neurons, S the number of samples, and α a problem-
dependent adjustable coefﬁcient.
We applied a sigmoid activation function for the hidden
layer to model the nonlinear relation between state and observ-
ables. The network is trained using the Levenberg-Marquardt
backpropagation algorithm [16] to update the weights of the
connection between the layers.
The input of the correction model is the difference between
the true and predicted observables. The correction model
converts this difference into information that the process model
can use to update itself. Thereby, the observer can react to its
own inaccuracies or noise.
III.
RESULTS
In this Section, we apply the presented method to a two-
dimensional deep drawing model. The simulation of deep
drawing is computed with ABAQUS. The dimensionality
reduction is applied to the data that we extract from the
simulations with different process parameters. We compare
our dimensionality reduction method to the PCA, as a linear
method, and NLPCA, as a nonlinear method. Using the results
of the dimensionality reduction, we deﬁne a reduced process
model using symbolic regression.
A. Application - Deep Drawing
As an example process, we used a two-dimensional ﬁnite
element model of cup deep drawing. Deep drawing is a notable
method regarding sheet forming. A metal sheet is clamped
between a die and a blank holder (Figure 4). The force of
the blank holder is adjustable. During the process the punch
presses the metal sheet into an opening of the die.
The numerical simulation of the two-dimensional deep
drawing model provides us with observables (such as displace-
ments and reaction forces) and state variables (such as von
Mises stress or strain). The simulation consists of 129 time
steps and we perform 200 simulations, each with a different
blank holder force. The blank holder force is time-invariant
and varies between 70 and 100 kN. As a result, we have 9
observables and 400 state variables at 129 time steps for 200
different blank holder forces.
Figure 4. Two-dimensional deep drawing model: drawn metal sheet (v. Mises
stress color-coded) clamped between blank holder and die.
We arrange this data in two matrices, one for the observ-
ables and one for the state variables. The size of each matrix
X is described by ST ×V , where S is the number of samples
(numbers of simulations with different blank holder force), T
the number of time steps and V the number of observables or
state variables, respectively.
We normalize the data of X using standard deviation:
X = X − mean(X)
std(X)
,
(4)
where mean(X) is the arithmetic mean, and std(X) is the
standard deviation.
B. Dimensionality Reduction
We reduced the state variables (von Mises stress) using
our seqBNN approach in Section II-A. The reconstructed state
variables are deﬁned by
ˆY = ˆY1 + ˆY2 + . . . ˆYn.
(5)
These results are compared to the results of PCA and NLPCA.
The reconstruction error, the mean square error (MSE) of
the true state variables and the predicted state variables (both
normalized by (4)), of the three methods is shown in Figure
5.
1
2
3
4
5
6
7
2 · 10−2
4 · 10−2
6 · 10−2
8 · 10−2
0.1
0.12
Number of Features
MSE
seqBNN
NLPCA
PCA
Figure 5. Comparison of reconstruction error (MSE) per feature of seqBNN,
PCA and NLPCA.
The results of the methods based on artiﬁcial neural net-
works, seqBNN and NLPCA, depend on the conﬁguration of
the network. The network conﬁguration for the seqBNN result
in Figure 5 is:
•
8 nodes in mapping and demapping layer each
•
2000 training epochs
144
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

•
160 training samples with 129 time steps each
•
40 test samples with 129 time steps each
and for the NLPCA result:
•
16 nodes in mapping and demapping layer each
•
7 nodes in the bottleneck layer
•
2000 training epochs
•
160 training samples with 129 time steps each
•
40 test samples with 129 time steps each
For both methods different conﬁgurations, especially with
regard to the number of nodes in mapping and demapping
layer, yield different results for the higher features. Running
different conﬁgurations has shown that there is no signiﬁcant
inﬂuence on the ﬁrst two features, as long as the conﬁguration
is reasonable.
It is obvious that with increasing number of features
the reconstruction error decreases. The error should converge
toward zero as the number of features approaches the number
of state variables. Furthermore, the diagram shows that feature
4 of the seqBNN could not explain relevant information that
would reduce the reconstruction error signiﬁcantly. The same is
true for the feature 5, 6, and 7 of the NPLCA. By comparison,
the seqBNN can reconstruct the data with only two features as
good as the PCA with four features or the NLPCA with three
features.
Figure 6 shows the coefﬁcient of determination R2:
R2 = 1 −
SSE
PM
i=1 (Yi − mean(Y ))2 ,
(6)
where SSE is the sum of the squared error between true and
predicted values, and M the number of samples and time steps.
R2 measures how much of the total variation in the data is
1
2
3
4
5
6
7
86
88
90
92
94
96
98
Number of Features
R2 in %
seqBNN
NLPCA
PCA
Figure 6. Comparison of R2 per feature (in %) of the seqBNN, PCA and
NLPCA.
described by the variation in the reconstructed state variables.
With only one feature for seqBNN we can explain 95.55%
of the true state variables variation (in the original domain -
without preprocessing), and seven features can explain 99.11%
of the example process.
Figure 7 shows the path of the reduced state within the
feature space of only the ﬁrst three features of the seqBNN
with a MSE of 0.0201 and R2 of 98.36%.
C. Process Model
Based on the extracted features, we intend to create a
process model using the symbolic regression software Eureqa R
⃝
−5
0
5
−4
−2
0
2
4
−5
0
5
feature1
feature2
feature3
0
0.2
0.4
0.6
0.8
1
Figure 7. State in three-dimensional feature space. The time is color-coded.
[12]. The quality of the symbolic regression result is mea-
sured by the complexity of the mathematical model and the
regression error. The complexity of each operation is deﬁned
by the user, depending on the prior knowledge, e. g., if it is
almost certain that the result contains no exp-function, then the
user can set the complexity of it to a higher number than the
complexity of already known operations or exclude it entirely.
Furthermore, it is common to set the complexity of basic
arithmetic operations to a lesser value than, e. g., trigonometric
operations. In our case, we assume no prior knowledge and
run the symbolic regression with each operation, constant, and
variable set to complexity 1.
For the following results the von Mises stress in each
integration point of the ﬁnite element model is used as state
variables. Figure 8 shows one result for the ﬁrst feature
(created in Section III-B with seqBNN). The resulting equation
time
z1
0.25
0.5
0.75
1
−10
−6
−2
2
6
10
Figure 8. First feature over time (black line) of one speciﬁc process
parameter uc and z1(t, uc) (red dotted line) using symbolic regression.
with a complexity of 37 and MSE of 0.0327 is:
z1(t, uc) = −54.29t · 8.60t + 1.91 · (8.60t)2
−
12.80
1.82 + (1823.35 · 108)t5
+ 257.45t3 + 81.93t − 4.69.
(7)
An other result with a lesser complexity of 21 but a higher
145
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

error of 0.0979 is deﬁned by
z1(t, uc) = −8.41 · (4.87 · 10−40)(132.16t2)
− 3.68t10.53 + 14.04t − 1.48.
(8)
In these results, the process parameter uc has no inﬂuence
on z1. We assume this is caused by the small range of the
variation of uc (see Section III-A). The obvious variation in
the data is caused by the deformation over time. The linear-
elastic part at the beginning of the process is obvious (Hooke’s
law) and so is the beginning of the plastic deformation.
If we consider the normal strain in x-direction in each
integration point of the ﬁnite element model as state variables,
then the extracted z1 represents the highest variation in this
quantity. Thus, we can assume that the derivative of z1 shows
similarities to the rate of change of the deformation. Hence,
we are also looking for differential equations:
dzi
dt = f(zi(t, uc), t, uc).
(9)
One symbolic regression result for the derivative of the ﬁrst
feature of the strain values is explained by:
dz1
dt = 7.53tz1 + 5.78 + 0.79z1
et+z1
− 0.01tez1,
(10)
with a complexity of 24.
D. Measurement Model and Correction Model
The three-layer artiﬁcial neural network is trained with 140
samples and tested with 60 samples with 129 time steps each.
We run 2000 training epochs with the extracted state features
as input and observables as output.
We experimentally determined the coefﬁcient α in (3),
starting with a minimum of 1. Table I shows the MSE and
R2 for different numbers of nodes in the hidden layer. With
35 nodes in the hidden layer, we can explain 99.97% of the
variation of the observables.
TABLE I. MSE AND R2 OF THE TEST SET FOR DIFFERENT NUMBERS OF
NODES IN THE HIDDEN LAYER.
9
12
18
26
35
MSE
0.0064
0.0043
0.0014
0.0006
0.0004
R2
0.9972
0.9973
0.9989
0.9996
0.9997
Afterwards, the predicted observables are compared to the
true observables and the correction model updates the process
model with new information. Note that building the process
model, i. e., running the symbolic regression algorithm, is not
online capable. Thus, for the presented online observation the
correction of the process model is only possible using an
additional summand.
IV.
CONCLUSION AND FUTURE WORK
In this paper, we introduced a new concept for observing
state variables of a manufacturing process with the help of a
process model, measurement model and correction model.
For this purpose, we reduced 400 state variables of
physical-based process simulations to 7 new characteristic
features that still describe 99% of the variation of the state
variables. This nonlinear dimensionality reduction approach
shows promising results for describing the process state in a
low-dimensional space.
Before the network training, the NLPCA needs a ﬁxed
number of features that have to be extracted. This may result
in too few or too many features in order to achieve a desired
residual. With our method, however, it is sufﬁcient to deﬁne the
desired residual before the training and if that has not yet been
reached, another feature will be extracted by an additionally
network.
We assume that each feature represents a particular area
(or part of it) of the metal sheet. This assumption is based on
visual comparison of the extracted features with the von Mises
stress curve for elements of a particular area, using the ﬁnite
element model. For example, the stress curves of the elements
in the area 1 in Figure 4 are similar to feature 1 and area 2 is
close to feature 2. The proof of this assumption is subject of
future work.
We proposed symbolic regression to ﬁnd a process model
in the extracted low-dimensional space. First results reveal that
a function of time and process parameter can be found for each
feature. We also showed ﬁrst results for learning differential
equations, but nevertheless expert interpretation is subject of
future work.
The three-layer artiﬁcial neural network (measurement
model) proved to be sufﬁcient to map the reduced state vari-
ables to observables. The variation in the predicted observables
describes 99% of the total variation of the true observables.
Note that each step:
•
dimensionality reduction
•
learning of the process model
•
regression: predicted state → predicted observables
introduces a minor error between predicted results and true
results. It has to be ensured that the eliminated information is
negligible or noise.
Future work will include the implementation of an optimal
process chain controller using the presented process observa-
tion for each discrete process in the process chain.
ACKNOWLEDGEMENTS
The funding of this project within the Research Training
Group 1483 “Process Chains in Manufacturing” by the German
Research Foundation (DFG) is gratefully acknowledged.
REFERENCES
[1]
D. Kosambi, “Statistics in function space.” Journal of Indian Mathe-
matical Society, vol. 7, 1943, pp. 76–88.
[2]
K. Karhunen, ¨Uber lineare Methoden in der Wahrscheinlichkeitsrech-
nung, ser. Annales Academiae scientiarum Fennicae: Mathematica -
Physica.
Universitat Helsinki, 1947, On linear methods in probability
and statistics.
[3]
M. Lo`eve, “Fonctions alatoires de second ordre,” in Processus stochas-
tiques et mouvement Brownien, P. Lvy, Ed.
Gauthier-Villars, 1948,
Random functions of the second order.
[4]
A. Quarteroni and G. Rozza, Eds., Reduced Order Methods for Mod-
eling and Computational Reduction, ser. MS&A, Modeling, Simulation
and Applications.
Springer, 2013.
[5]
G. Kerschen, J.-C. Golinval, A. F. Vakakis, and L. Bergman, “The
method of proper orthogonal decomposition for dynamical charac-
terization and order reduction of mechanical systems: An overview,”
Nonlinear Dynamics, vol. 41, no. 1-3, 2005, pp. 147–169.
[6]
M. Strano, “A technique for FEM optimization under reliability con-
straint of process variables in sheet metal forming,” International Journal
of Material Forming, vol. 1, no. 1, 2008, pp. 13–20.
[7]
O. Nelles, Nonlinear System Identiﬁcation, 1st ed.
Springer-Verlag
Berlin Heidelberg, 2001.
[8]
M. Scholz and R. Vig´ario, “Nonlinear pca: a new hierarchical ap-
proach.” in ESANN, 2002, pp. 439–444.
146
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

[9]
D. G. Luenberger, “Observing the state of a linear system,” IEEE
transactions on military electronics, vol. 8, no. 2, 1964, pp. 74–80.
[10]
R. E. Kalman, “A new approach to linear ﬁltering and prediction
problems,” Journal of Fluids Engineering, vol. 82, no. 1, 1960, pp.
35–45.
[11]
S. Fischer, O. Hensgen, M. Elshaabiny, and N. Link, “Generating low-
dimensional nonlinear process representations by ordered features,” in
15th IFAC Symposium on Information Control in Manufacturing, 2015.
[12]
M. Schmidt and H. Lipson, “Distilling free-form natural laws from
experimental data,” Science, vol. 324, no. 5923, 2009, pp. 81–85.
[13]
J. R. Koza, Genetic Programming: On the Programming of Computers
by Means of Natural Selection.
MIT Press, 1992.
[14]
M. Senn and N. Link, “Hidden state observation for adaptive process
controls,” in Proceedings of the Second International Conference on
Adaptive and Self-adaptive Systems and Applications (ADAPTIVE
2010).
IARIA, 2010, pp. 52–57.
[15]
W. C. Carpenter and M. E. Hoffman, “Selecting the architecture of a
class of back-propagation neural networks used as approximators,” Arti-
ﬁcial Intelligence for Engineering, Design, Analysis and Manufacturing,
vol. 11, 1 1997, pp. 33–44.
[16]
M. Hagan and M. Menhaj, “Training feedforward networks with the
marquardt algorithm,” IEEE Transactions on Neural Networks, vol. 5,
no. 6, Nov 1994, pp. 989–993.
147
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

