Augmented Reality Simulation for Testing
Advanced Driver Assistance Systems in Future
Automotive Vehicles
Michael Weber
Institute of Energy Efficient Mobility
Hochschule Karlsruhe -
University of Applied Sciences, HKA
Karlsruhe, Germany
email: michael.weber@h-ka.de
Tobias Weiss
Institute of Energy Efficient Mobility
Hochschule Karlsruhe -
University of Applied Sciences, HKA
Karlsruhe, Germany
email: tobias.weiss@h-ka.de
Franck Gechter
CIAD (UMR 7533)
Univ. Bourgogne Franche-Comte, UTBM
Belfort, France
LORIA-MOSEL (UMR 7503)
Universit´e de Lorraine
Nancy, France
email: franck.gechter@utbm.fr
Reiner Kriesten
Institute of Energy Efficient Mobility
Hochschule Karlsruhe -
University of Applied Sciences, HKA
Karlsruhe, Germany
email: reiner.kriesten@h-ka.de
Reiner Kriesten
Institute of Energy Efficient Mobility
Hochschule Karlsruhe -
University of Applied Sciences, HKA
Karlsruhe, Germany
email: reiner.kriesten@h-ka.de
Reiner Kriesten
Institute of Energy Efficient Mobility
Hochschule Karlsruhe -
University of Applied Sciences, HKA
Karlsruhe, Germany
email: reiner.kriesten@h-ka.de
Abstract—To take advantage of Advanced Driver Assistance
Systems (ADAS) testing in simulation and reality, this paper
presents a new approach to using Augmented Reality (AR)
in future automotive vehicles to test ADAS. Our procedure
creates a connection between simulation and reality and should
enable a faster development process for ADAS tests and future
mobility solutions, which will become increasingly complex in
the future. Complex automotive environmental conditions, such
as high vehicle speed and fewer possible orientation points on
an urban test track compared to using AR applications inside
a building, require high computing power for our approach.
Using Image Segmentation (IS), Artificial Intelligence (AI) for
object recognition and visual Simultaneous Localization and
Mapping (vSLAM), a three-dimensional model with accurate
information about the urban test site is generated. The use of
AI and IS aims to significantly improve performance, such as
calculation speed and accuracy for AR applications in complex
automobiles.
Index Terms—Artificial Intelligence, Augmented Reality, Ad-
vanced Driver Assistance Systems, Visual Simultaneous Local-
ization and Mapping.
I. INTRODUCTION
Advanced Driver Assistance Systems (ADAS) are more
and more spread in current and future cars because they are
increasing vehicle safety by recognizing road signs, keep-
ing the car in its lane, or regulating the speed adaptively.
These complex systems go through an extensive test phase,
resulting in the potential for optimization in terms of quality,
reproducibility, and costs. Due to the increasing complexity
of vehicle communication and the rising demands on these
systems in terms of reliability to function safely even in a
complex environment and to support the driver, test scenarios
for ADAS are constantly being further developed to meet
higher requirements. The European New Car Assessment
Program (Euro NCAP) has included a series of new safety
tests for ADAS in its program and has drawn up a roadmap
up to the year 2025 [1] [2].
Current testing methods of ADAS can be divided into
simulation and reality. The core idea of using simulation is
to transfer the behavior of the vehicle to the virtual test drives
as realistically as possible. The approach of using simulation
aims to use the advantages like reproducibility, flexibility, and
cost reduction. In this way, the specifications and solutions
derived from this should be able to be tested and evaluated
at an early stage of the development process. Using suitable
simulation methods enables the efficient design, development,
and application of vehicles and vehicle components. However,
simulation cannot yet replace real test drives in all respects.
Due to the complex physical conditions under which a vehicle
is handed over during ADAS tests, real test drives are still
required for the current status. For example, weather, road
surface conditions, and other influencing parameters play a
crucial role in the evaluation of ADAS road tests [3] [4].
However, the test and evaluation effort correlate with the
complexity of an ADAS. The more complex the system, the
greater the testing effort. The robustness, functional safety,
and reliability of the ADAS must be proven in increasingly
29
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-001-8
SIMUL 2022 : The Fourteenth International Conference on Advances in System Simulation

dynamic, complex, and chaotic traffic situations. This also
includes the interaction with different road users, each with
their natural movements, such as, e.g., the interaction of
road users with each other. Therefore, new and efficient test
methods are required to pave the way for future ADAS [5].
This paper describes the usage of Augmented Reality (AR) for
testing camera-based Advanced Driver Assistance Systems. In
Section II a description of ADAS basics is presented. Section
III describes the current methods used for testing ADASs.
Section IV, shows a possible usage of Augmented Reality and
its challenges. The paper is concluded by Section V, where
the research, results, and lessons learned are discussed.
II. ADVANCED DRIVER ASSISTANCE SYSTEMS
ADAS supports the driver when operating a vehicle. De-
pending on the type, they ensure more driving comfort,
increase safety, reduce energy consumption or enable more
efficient traffic flow. The systems record the driving situation
via sensors, process the collected information with powerful
computers, and give the driver optical, acoustic, or haptic
feedback. In some cases, they intervene automatically, semi-
autonomously, or autonomously in the control and operation
of the vehicle, for instance by accelerating, braking, signaling,
or steering. These can extend to fully autonomous driving. The
main requirements for ADAS are fast data processing in almost
real-time and high system reliability [6] [7]. A growing num-
ber of environment sensors, such as radar, camera, ultrasonic,
and lidar sensors enable the use of ADAS in modern vehicles
and related functions for autonomous driving. At the same
time, each sensor is severely limited in its scope and cannot
provide all of the information about the vehicle’s surroundings
that is required for safety functions. Only the combination of
data from different sensors (sensorfusion) results in a complete
environment model, a basic requirement for the reliability and
safety of driver assistance systems and autonomous driving
[8].
III. TESTING OF ADVANCED DRIVER ASSISTANCE
SYSTEMS
Simulated test procedures during the development process,
as well as real test procedures, are used to evaluate the
functionality of individual ADAS sensors and their joint
interaction in ADAS-relevant scenarios. While all test drive
components remain virtual in the first concept phase, virtual
components are successively exchanged for associated real test
elements over the various integration stages in the course of
development. Up to entirely real test drives with real drivers
and road users, the simulation components have entirely given
way to reality [9].
A. Testing Advanced Driver Assistance Systems in Simulation
The guiding principle of the virtual test drive is to transfer
the real test drive to the virtual world as realistically as possi-
ble. The aim is to use the characteristic strengths of simulation
in terms of reproducibility, flexibility, and cost reduction and to
establish a test and evaluation option for the specifications and
solutions derived from this early on in the vehicle development
process. Using suitable simulation methods enables vehicles
and vehicle components to be designed, developed, and used
more efficiently. They serve as a bridge and shorten the time
until real vehicle prototypes are available. With real test drives
and the reliability of real test results as a template, Using
simulation techniques is an optimization task in which the
modeling, parameterization, and simulation effort must be
matched to the efficiency achieved. The methods used for
this approach mainly come from the repertoire of integrated
mechatronic system development. Here, the methods: Model
in the Loop (MiL), Software in the Loop (SiL), Hardware in
the Loop (HiL), and Vehicle in the Loop (ViL) come into
question [3].
B. Testing Advanced Driver Assistance Systems in Reality
While vehicle dynamics control systems can still be vali-
dated in real driving tests with great effort, despite all their
complexity and variety, this is no longer economically viable
for ADAS today due to the complex system, the complexity
of the test cases, and the necessary scope of the tests. Even
if the tests are supposed to be performed in the same way, in
practice it is impossible to perform the tests under the same
conditions due to the many potential and sometimes unknown
or ignored influences. The reproducibility of the results is
therefore not given because on the one hand the functionally
relevant features can contain the necessary interaction of
several road users and on the other hand a complex interaction
of a general nature can be subject to conditions, such as
low glare, simultaneous sun, and reflection on a wet road
surface at a specific angle. Current ADAS functions access
information about the environment, sometimes collected from
multiple sensors with different functions and processed into a
representation of the environment [4] [10].
We use the Euro NCAP as the standard. That is a Europe-
wide standardized test procedure for ADAS with real driving
tests. The focus is on the behavior and reaction of the
vehicle in safety-critical situations. In simulated dangerous
circumstances, dummies in the form of vehicles driving ahead,
pedestrians, and cyclists are used to test the functionality
of ADAS systems. In a further step, the reaction time of
the hazard message for the driver is evaluated. Due to the
constantly increasing traffic safety, the test procedures of Euro
NCAP will also contain more complex test scenarios in the
future. Therefore, the roadmap up to 2025 also includes other
road users, such as scooters, motorcycles, and wild animals
to increase road user safety [11]. An Autonomous Emergency
Braking (AEB) test scenario in which a pedestrian (child or
adult as a dummy) crosses the street in which a car is turning is
shown in Figure 1. The test vehicle must detect the pedestrian
and avoid personal injury or property damage by braking [12].
C. Combining Virtual and Real Testing
To enable the testing of camera-based assistance systems in
real environments earlier in the development phase and thus
increase the quality of the systems, the use of AR as a link
30
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-001-8
SIMUL 2022 : The Fourteenth International Conference on Advances in System Simulation

Fig. 1. Pedestrian is crossing a road in which a car is turning [12]
between virtual and real testing lends itself to this. Using AR
to test camera-based systems combines the advantages of a
virtual environment and these of the real world: Reproducible,
complex scenes with realistic environmental conditions. AR
thus makes it possible to dispense with test dummies or second
vehicles including drivers even in the initial phases of testing.
This reduces the costs of the tests and increases the safety of
the test engineers. The combination of different test situations
is also possible: The display of several vehicles, lane markings,
and road signs allow the simultaneous testing of all camera-
based driver assistance systems. The unlimited variety of test
scenarios allows a significant increase in the depth of testing
at an early stage of development. That increases the quality of
the testing and thus of the overall system. In 2010, a Swedish
team led by Jonas Nilsson presented a software framework at
a conference that used AR to evaluate a pedestrian detection
system. The framework was able to augment the images
from the vehicle camera to include a walking pedestrian. The
resulting detection system results were comparable to test
results obtained with real obstacles. As summarized in this
paper, deeper investigations are needed to further advance an
AR test system [20]. In the following, an AR-approach will
be discussed.
IV. AUGMENTED REALITY SIMULATION IN ADVANCED
DRIVER ASSISTANCE SYSTEMS
Different criteria are required for the use of AR in testing
automotive ADAS than for conventional AR applications, such
as on a smartphone. This section describes the test criteria for
this approach.
A. Augmented Reality for regular Applications
According to Azuma’s proposal, AR can be defined as a
combination of three fundamental characteristics: the com-
bination of real and virtual worlds and the precise three-
dimensional registration of real and virtual objects, both in
a real-time interactive environment [13]. The basic principle
of AR is mainly known from the mobile game Pok´emon Go
[17]. Within this game, users can interact with digital creatures
through their smartphones. These creatures are placed virtually
Fig. 2. Pok´emon Go-App on the left side of the figure [17] and a
self-created Augmented Reality application showing a possible scenery on
the right side of the figure
in the user’s environment. One such AR application is shown
in Figure 2. Figure 2 shows also a self-created AR-App for
demonstrating a possible scenery with traffic signs and a
pedestrian. The three parts of the algorithms behind AR are
image analysis, 3D modeling, and augmentation.
Image analysis serves to identify points or areas of interest
within the given image. Feature detection, such as corner
detection is often used for this step [14]. A three-dimensional
model of the environment is created using the results of the
image analysis. The types of algorithms used for this step
vary depending on the type of AR application. Simultaneous
Localization and Mapping (SLAM) or Structure-of-Motion
(SfM) algorithms are often used for AR in unknown locations
[14]. The augmentation is based on the results of 3D mod-
elling. The scene model is typically provided as a positional
description of a plane or coordinate system that represents
the real world [14]. With this information, a virtual object
can be placed on the plane or in the coordinate system with
appropriate characteristics, such as size and orientation. After
object placement, the virtual content is combined with the real
image [14].
There are different versions of applications for AR. These
applications are very diverse in their fields, from the use of
AR in psychology [13] to use in hospital operating rooms [14]
to mobile games [14] to military applications [14]. What all
these apps have in common is that human reality is expanded.
With humans as users of AR, there are implications for the
application. One is that, in most cases, the human user is
forgiving of not accurately placed virtual objects, if the error
lies within a small margin. In addition, the speed of human
movement, and therefore the distance covered in a given time,
is limited. Because of these limitations, localization, mapping,
object placement, and runtime requirements are not as strict
and demanding as in the automotive environment given in this
paper.
31
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-001-8
SIMUL 2022 : The Fourteenth International Conference on Advances in System Simulation

B. Approach for Using Augmented Reality in Advanced Driver
Assistance Systems
With a focus on the camera-based ADAS sensors, the area
around the test field is recorded, as shown in Figure 3. The
path between the sensor fusion module and the Electronic
Control Unit for Advanced Driver Assistance Systems (ADAS-
ECU), which causes the vehicle to intervene, for example by
braking, has to be disrupted and a new path has to be found
through the Augmented Reality Electronic Control Unit (AR-
ECU). Within the AR-ECU, the captured environmental data
is augmented with virtual objects, such as traffic signs or lane
markings. The aim here, is a realistic and consistent behavior
of the ADAS-ECU as in real object detection. For the final
Fig. 3. Our approach for using Augmented Reality in Advanced Driver
Assistance Systems
augmentation of the virtual objects on the real image of the
sensor, a detailed 3D environment of the test environment
must first be created. For this purpose, a visual Simultaneous
Localization and Mapping (vSLAM) approach is chosen. The
vSLAM method uses only visual inputs to perform localization
and mapping. That means no vehicle sensors other than the
vehicle’s camera system are needed to create a 3D model
of the environment, making this approach more flexible than
lidar, radar, and ultrasonic. The vSLAM algorithm framework
mainly consists of three basic modules: initialization, tracking,
and mapping, and two additional modules: relocation and
global map optimization (including loop closure) [15]. Several
approaches with the vSLAM algorithm are available for using
vSLAM in automotive vehicles and the associated properties,
such as fast scene changes and low environmental textures,
which can be found in [16]. Based on [16], various vSLAM
approaches are compared in terms of accuracy and robustness,
among other things.
C. Oriented FAST and Rotated BRIEF (ORB) as approach for
visual Simultaneous Localization and Mapping
The approach ”Oriented Features from accelerated Segment
Test (FAST) and Rotated Binary Robust Independent Elemen-
tary Features (BRIEF) (ORB)-SLAM” was first introduced in
2015 and seems to be state-of-the-art because it has higher
accuracy than comparable SLAM algorithms [16]. Here, ORB-
SLAM represents a complete SLAM system for monocular,
stereo, and red-green-blue-depth (RGBD) cameras. The sys-
tem works in real-time and achieves remarkable results in
terms of accuracy and robustness in a variety of different envi-
ronments. ORB-SLAM is used for indoor sequences, drones,
and cars driving through a city. The ORB-SLAM consists of
three parallel main threads: Tracking, Local Mapping, and
Loop Closure. A fourth thread can be created to run the
Bundle Adjustment (BA) after a closed loop. This algorithm is
a feature-based approach that represents the detected feature
points in a three-dimensional MapPoint [16]. Figure 4 shows
a MapPoint on the left side created from internally acquired
image sequences. Detected Feature Points that are used creat-
ing the MapPoint are displayed on the right side of the figure.
The increase in accuracy of the MapPoint created by ORB-
Fig. 4. Selfcreated MapPoint on the left side of the figure and the detected
feature points (green rectangles) on the right side of the figure
Fig. 5. Original image on the left side of the figure and the recognized
objects using Image Segmentation (Pole, Ego Vehicle (Hood), Terrain,
Vegetation, Road and Sky) on the right side of the figure
SLAM with correspondingly recognized objects and surfaces
is made possible by using IS and AI. Figure 5 shows the object
detection. On the left is the original picture of the front view of
the car. Since the Euro NCAP tests are conducted in dedicated
test areas with low texture and therefore less information about
the environment (see Figure 1), the recognized classes for our
approach are pylons, own vehicle (hood), terrain, vegetation,
road, and sky. More classes should be added in the further
project progress. This means deriving plans from MapPoint,
recognizing the road surface, and realistically enlarging objects
on the virtual map. Another focus of this work is the realism of
magnification, such as occlusion, reflection, and shadows. The
vSLAM location mode is used to augment individual camera
pixels to present the vehicle with a real-life hazard situation.
D. Advantages of our Approach
Our approach will combine the specific advantages of
testing in simulation (reproducibility, flexibility and cost re-
duction) and reality (vehicle and environment complexity) and
thus represent a link between these test methods. It should
32
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-001-8
SIMUL 2022 : The Fourteenth International Conference on Advances in System Simulation

make it possible to test more complex scenarios and thus
increase the safety of road users. The lane crossing enables,
for example, a lane departure warning system to be tested
independently of the test site. Scenarios, such as the appear-
ance of temporary lane markings or missing sections, can be
tested in the same test area. Narrowing and widening of lanes
can be displayed, as well as international differences between
lane markings. Vehicles driving ahead can be superimposed
on the camera image to test traffic jam assistants. In the first
test phase, there is no need for a second vehicle including
driver, which reduces test costs and increases safety for test
engineers. In addition, test cases with traffic signs, pedestrians
and cyclists can be added quickly and situationally. It is also
possible to combine different test situations. The unlimited
variety of test scenarios enables a significant increase in
test depth at an early stage of development. This increases
the quality of the tests and thus of the system in general.
Due to the increasing number of ADAS and the constant
development towards autonomous driving, the application area
of the software program can be expanded as desired.
E. Challenges for our Approach
Various advances and improvements in terms of accuracy,
robustness, etc. can be found in later developments based
on this ORB-SLAM approach [16]. While ORB-SLAM’s
performance is impressive on well-structured sequences, error
conditions can occur on poorly-structured sequences, such as
Euro NCAP test scenarios or when feature points temporarily
disappear, for example due to motion blur [16]. In addition to
accuracy, the execution time of the global algorithm is also
of great importance. Camera systems in automotive vehicles
today work with a frame rate of 30 to 60 frames per second
[fps] [7]. For a successful evaluation of ADAS test scenarios,
the AR system must be able to orientate itself very precisely in
the environment [16]. One cause is the missing feedback about
the impact intensity of test dummies when crashing them.
Because of this, it is necessary to know the exact position of
the car on the test track in order to calculate the intensity of
the impact based on the braking distance. Using Euro NCAP
test scenarios, speeds up to 130 km/h, which is equal to
36.111 m/s , are proven [11]. The algorithm must have a faster
execution time compared to the speed of the camera system.
The distance d that the vehicle covers within one frame at a
given speed and frame rate can be calculated. At a speed of
130 km/h and using a camera framerate of 30 fps, the vehicle
travels
d = 36.111 [ m
s ]
30 [ frames
s
]
= 1.204
m
frame.
(1)
So at a frame rate of 60 fps at the same speed, a distance of
d = 36.111 [ m
s ]
60 [ frames
s
]
= 0.602
m
frame
(2)
is covered. A slowdown of one frame means that the test
results deviate from 0.602 to 1,204 meters. Due to the high
speed of the car and camera, as well as the need for high
precision in object placement, it is clear that the requirements
for this AR application are much stricter than the usual human
user application.
F. Visualisation for the Testdriver
The main task of the Human-Machine-Interface (HMI) is
to make the AR perceptible to the test driver in real time
that the ADAS functions of the test vehicle, as well as the
human interaction, can be evaluated. The acceptance of the
HMI as an interface for the experience plays an important
role. This depends for the most part on the quality of the
display, interaction and haptics [18]. For our approach, the
selection of a suitable HMI concept focuses on visualization
and interaction. To display AR visibly, the use of a suitable
HMI or a corresponding display is necessary. Possible screen
approaches are classified into feature classes based on their
properties. Displays that use a medium-direct view through
to the real environment in 3D belong to the class of see-
through (ST) displays. Monitor-based (MB) displays only
allow an indirect view of the real environment. Live or stored
videos (2D) are used for this technology. Indirect displays (3D
objects: video ST), which visualise AR in 3D using video,
also belong to the group of ST displays. The 3D concept is
crucial here. The processing of the 2D camera data of the
real environment used, through 3D scene modelling, makes it
possible in the first place to integrate the virtual objects in
the correct perspective (2D). Video-based ST displays (video
ST) are used if the recording and playback of this same
AR on an indirect display take place almost simultaneously.
Optical ST displays (3D-Objects:optical ST) are used when
the reproduction of the virtual objects in combination with the
direct view of the real environment is correctly integrated. The
visualisation of AR according to Azuma limits the AR-capable
displays to those that can display virtual 3-dimensional objects
correctly oriented in perspective [13]. For the identification
of suitable HMI approaches for testing camera-based ADAS,
only these ST displays fulfil the necessary criteria.
HMI approaches in which stationary displays are mechani-
cally fixed to the vehicle for the duration of the test belong to
the Head-Up-Display (HUD) group. Head-Up-Displays used
in automotive vehicles to show the driver the actual speed or
using the display of a smartphone or tablet belongs to this
category. Those in which the display is attached to the head
like when using Virtual Reality (VR)-glasses or AR-glasses
belong to the Head-Mounted-Display (HMD) group [19]. In
both HMD and HUD, HMI approaches of optical and video-
based ST displays are identified. In the further progress of the
approach to use AR as a visualization for the driver, different
evaluations must be carried out.
G. Further Thoughts about Using Augmented Reality Simula-
tion for Testing Advanced Driver Assistance Systems in Future
Automotive Vehicles
In the first step, the focus of our approach will shift to
camera-based sensors. Only a few ADAS functions, such as
traffic sign recognition or Lane Departure Warning (LDW),
access the camera’s sensors. To evaluate further tests and to
33
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-001-8
SIMUL 2022 : The Fourteenth International Conference on Advances in System Simulation

achieve the identical behavior of the ADAS-ECU (see Figure
3) in reality as when using AR, the integration of more sensors,
such as radar or lidar, is required. It should also be mentioned
that the Euro NCAP test scenarios are only carried out under
ideal conditions according to the current status of the position
of the sun at midday with little or no shadows and reflections,
no other road users, no rain, etc. [16]. Our approach aims to
further increase the complexity and realism of Euro NCAP
test scenarios.
V. CONCLUSION
In this article, we proposed an approach to using AR in auto-
mobiles. Using AR in ADAS aims to combine the advantages
of simulated test procedures, such as reproducibility and cost
savings, with the benefits of real test procedures (complexity of
the entire vehicle and the environment). We model the problem
of creating an urban environment to use AR for testing in
high-speed ADAS. Our approach is based on a combination
of vSLAM algorithms with AI and IS for object detection.
That should help get better overall performance in terms of
computational speed and accuracy. The creation of a virtual 3D
environment with a better understanding of individual objects
should make it possible in a later step to enrich other sensors,
such as car radar and lidar, with objects in addition to camera
data. That should further increase the overall performance
of the entire system. The lessons learned so far are mainly
regarding the SLAM-Algorithms. In automotive test fields, the
current state-of-the-art SLAM algorithms are not well suited
for environmental conditions such as low textured environment
and high camera velocity. Further problems are the software
runtime since the test system must work in real-time and the
reproducibility. We hope to overcome at least some of these
challenges, as mentioned beforehand, by combining neural
networks with modern SLAM algorithms. This approach is
intended not only to create a link between real and virtual test
procedures but also to increase the complexity of potential
test procedures, accelerate the development speed of ADAS
functions and improve safety for future mobility solutions.
REFERENCES
[1] K. Bengler et al., ”Three decades of driver assistance systems: Review
and future perspectives.” In: IEEE Intelligent Transportation Systems
Magazine vol. 6, no.4 , pp. 6–22, Winter 2014.
[2] F.
Schuldt,
F.
Saust,
B.
Lichte,
M.
Maurer,
and
S.
Scholz,
”Efficient
systematic
test
generation
for
driver
assistance
systems
in
virtual
environments
-
Effiziente
systematische
Testgenerierung f¨ur Fahrerassistenzsysteme in virtuellen Umgebungen.”
2013.
[Online].
Available
from:
https://publikationsserver.tu-
braunschweig.de/servlets/MCRFileNodeServlet/dbbs-derivate-
00031187/AAET-Schuldt-Saust-Lichte-Maurer-Scholz.pdf
Accessed
2022.07.29
[3] B.-J. Kim and S.-B. Lee, ”A study on the evaluation method of au-
tonomous emergency vehicle braking for pedestrians test using monoc-
ular cameras.” Applied Sciences 10, no. 13: 4683, July 2020, doi:
10.3390/sapp10134683
[4] C. Miquet et al., ”New test method for reproducible real-time tests of
ADAS ECUs: ”Vehicle-in-the-loop” connects real-world vehicles with
the virtual world.” In: 5th International Munich Chassis Symposium
2014, pp. 575-589, July 2014.
[5] J.E. Stellet et al., ”Testing of Advanced Driver Assistance Towards
Automated Driving: A Survey and Taxonomy on Existing Approaches
and Open Questions.” In: 2015 IEEE 18th International Conference on
Intelligent Transportation Systems, pp. 1455–1462, September 2015.
[6] M. Nagai, ”Research into ADAS with autonomous driving intelligence
for future innovation.” In: 5th International Munich Chassis Symposium
2014, pp. 779–793, January 2014
[7] H. Winner, S. Hakuli, F. Lotz, and C. Singer, ”Manual Driver
Assistance Systems - Basics, Components and Systems for Ac-
tive
Safety
and
Comfort
-
Handbuch
Fahrerassistenzsysteme
-
Grundlagen,
Komponenten
und
Systeme
fuer
Aktive
Sicherheit
und Komfort.” Springer Vieweg, Wiesbaden, March 2015. [On-
line]. Available from: https://link.springer.com/content/pdf/10.1007/978-
3-658-05734-3.pdf Accessed 2022.07.29
[8] M. Darms, ”A Basic System Architecture for Sensor Data Fusion
of Environmental Sensors for Driver Assistance Systems - Eine
basis-systemarchitektur zur Sensordatenfusion von Umfeldsensoren fuer
Fahrerassistenzsysteme.” PhD thesis, Technische Universit¨at Darmstadt,
2007. [Online]. Available from: https://tuprints.ulb.tu-darmstadt.de/914/
Accessed 2022.07.29
[9] T.M. Gasser, A. Seeck, and B.W. Smith, ”Virtual Integration” In:
”Framework Conditions for Driver Assistance Development - Rah-
menbedingungen fuer die Fahrerassistenzentwicklung”, pp. 27–54.
Springer Vieweg, March 2015.
[10] P. Seiniger and A. Weitzel, ”Testing Procedures for Consumer
Protection and Legislation - Testverfahren fuer Verbraucherschutz
und
Gesetzgebung.”
In:
Manual
Driver
Assistance
Systems
-
Basics, Components and Systems for Active Safety and Comfort
-
Handbuch
Fahrerassistenzsysteme
-
Grundlagen,
Komponenten
und Systeme fuer Aktive Sicherheit und Komfort, pp. 167–182.
Springer Vieweg, Wiesbaden, March 2015. [Online]. Available from:
https://link.springer.com/content/pdf/10.1007/978-3-658-05734-3.pdf
Accessed 2022.07.29
[11] R. Fredriksson, M.G. Lenn´e, S. van Montfort, and C. Grover, ”European
NCAP Program Developments to Address Driver Distraction, Drowsi-
ness and Sudden Sickness” November 2021. [Online]. Available from:
https://www.frontiersin.org/articles/10.3389/fnrgo.2021.786674/full Ac-
cessed 2022.07.29
[12] Euro
NCAP
”AEB
Pedestrian”.
[Online].
Available
from:
https://www.euroncap.com/en/vehicle-safety/the-ratings-
explained/vulnerable-road-user-vru-protection/aeb-pedestrian/ Accessed
2022.07.30
[13] R.T. Azuma, ”A Survey of Augmented Reality.” In: Teleoperators and
Virtual Environments, pp. 355–385, August 1997.
[14] A. State, G. Hirota, D. Chen, W. Garrett, and M. Livingston, ”Superior
Augmented Reality Registration by Integrating Landmark Tracking and
Magnetic Tracking.” In: SIGGRAPH ’96: Proceedings of the 23rd
annual conference on Computer graphics and interactive techniques, pp.
429-438, August 1996.
[15] T. Taketomi, H. Uchiyama, and S. Ikeda, ”Visual Slam Algorithms: a
Survey from 2010 to 2016.” In: IPSJ Transactions on Computer Vision
and Applications, Article number: 16, 2017.
[16] C. Campos, R. Elvira, J.J.G. Rodrıguez, M. Montie, and D. Tar-
dos, ”Orb-slam3: An Accurate Open-Source Library for Visual, Vi-
sual–Inertial and Multimap Slam.” In: IEEE Transactions on Robotics,
pp. 1–17, July 2020, doi: 10.1109/TRO.2021.2075644
[17] Pok´emon GO, ”Developer Niantic is working on a game for tourists -
Pok´emon GO: Entwickler Niantic arbeitet an einem Spiel f¨ur Touristen.”
[Online] Available from: https://mein-mmo.de/pokemon-go-entwickler-
app-touristen/ Accessed 2022.07.29
[18] J. Brade and A. Koegel, ”Presence in Virtual Reality - Key to Acceptance
and Transferability?!“ In: 5. Fachkonferenz zu VR/AR-Technologien in
Anwendung und Forschung, VAR² 2019, pp. 59-71, December 2019.
[19] R. Doerner, ”Fundamentals and Methods of Virtual and Augmented
Reality - Grundlagen und Methoden der Virtuellen und Augmentierten
Realitaet.” In: Virtual and Augmented Reality (VR/AR), Springer Viweg,
pp. 1-143, 2019.
[20] J. Nilsson et al., “Performance Evaluation Method for Mobile Computer
Vision Systems Using Augmented Reality”. In: 2010 IEEE Virtual
Reality Conference (VR), pp. 19–22, March 2010.
34
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-001-8
SIMUL 2022 : The Fourteenth International Conference on Advances in System Simulation

