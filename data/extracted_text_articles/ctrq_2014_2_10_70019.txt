An Algorithm for Combinatorial Entropy Coding
Stephan B¨arwolf
Integrated Communication Systems Group
Ilmenau University of Technology
Ilmenau, Germany
stephan.baerwolf@tu-ilmenau.de
Abstract—Entropy coding (esp. order-0) was one of the ﬁrst
techniques for lossless data compression, dating back to the
invention of modern information theory. Over such a long period
of time different schemes were invented and entropy coding
has experienced various improvements: Huffman published its
minimal tree structured codes and then Witten, Neal and Cleary
presented a scheme leading to even better results.
While entropy compression is still used today in most of recent
compression schemes, it has not lost its signiﬁcance. This paper
presents an encoding and its corresponding decoding algorithm
not using trees or intervals to do entropy compression. Instead it
derives permutations from the input which are mapped to natural
numbers. Furthermore this paper gives an impression about the
compression performance by comparing some ﬁrst results with
well known entropy compression schemes.
Keywords- entropy; coding; data compression
I.
INTRODUCTION
Today, nearly all lossless and even lossy compression
schemes are using at least a build-in order-0 entropy coder.
Because normally entropy coding is very easy to understand
and very effective in compressing non uniform distributed
inputs, it is a preferred “last-stage” compression technique
in such schemes. Since Shannon posted his ﬁrst ideas about
compression, known as Shannon-Fano, in his famous paper
[3], different concepts for entropy compression have emerged:
Some years after Shannon, David Huffman [4] improved
Shannons scheme. Still using the same concept of binary
trees, Huffman changed the way of constructing the tree
structure and proved it to be optimal. Finally, in 1987 a paper
[5] was published, which cleariﬁed an algorithm leading to
nearly always better compression results than Huffman. This
breakthrough was done by using successive bisection of an
interval instead of trees to generate code words.
As already mentioned, even if today’s compression schemes
use more advanced algorithms, one of the previous mentioned
entropy coders (often Huffman) is still part of them. For
example, the Microsoft LZX [7] extends the idea of LZ77
[6] by utilizing entropy compression for match-lengths and -
positions via Huffman codes.
An extreme example of entropy compression used today
are the Burrows-Wheeler transform (BWT) [8], its bijective
version BWTS [9], and other sort transforming modiﬁcations
[11]. Since the BWTs are only “transformations”, the whole
compression effect is done (after some intermediate processing
stages) in one ﬁnal entropy compression stage [10].
This paper presents a different concept for order-0 entropy
compression by mappings of permutations to enumerations and
vice versa. Instead of using trees or intervals, the compression
results of the presented technique therefore should never be
worse than the one compared to arithmetical coding.
The paper is structured as following. A simple algorithm for
encoding is presented and discussed in the ﬁrst section. After
this section the same is done for the decoding. In section 4,
some ﬁrst results are presented by using the usual compression
corpora ([14],[15],[16]). Finally, the paper will be closed with
a conclusion/future work chapter.
II.
ENCODING
The idea of encoding an input word “I” over the ﬁnite,
non-empty alphabet “A” (I
∈
A∗, (a0, a1, . . . , ad−1)
∈
A, a0
<
a1
<
· · ·
<
ad−1), is to enumerate its
represented permutation under its given symbol frequencies
α ([α[a0], α[a1], . . . , α[ad−1]] = α ∈ (N ∪ {0})k , α[ai] =
|I|ai, n = |I| = Pd−1
k=0 α[ak]).
Furthermore ak will be synonymous with k.
Because such an enumeration would be bounded by a
multinomial coefﬁcient (1), the enumeration could be stored
with only log2(

TABLE I.
ENUMERATIONS FOR A GIVEN α
enumeration
word
0
iiiimppssss
1
iiimippssss
...
99
mppiisisiss
100
imppsiisiss
...
1999
iippisisssm
...
32591
imssissippi
32592
mississippi
32593
msisissippi
...
34649
ssssppmiiii
Algorithm 1 shows a way to efﬁciently calculate such an
enumeration.
First for every symbol a = ai (see line 5) a separate, partial
enumeration (“codea”) is generated by just taking symbols
larger or equal to ai into account.
Because permutations of smaller symbols aj, aj < ai have
already been processed, they are ignored in further iterations.
Therefore, codea is a sum of binomial coefﬁcients

Algorithm 2 Retrieve word from its enumeration
Require: code ⇐ to be decoded number
Require: α ⇐ frequency of each character (byte) in decoded
message
Require: n ⇐ size of decoded message (n = P255
a=0 α[a])
1: // initialize output:
2: msg ⇐ each byte ﬁlled with value 255, |msg| = n
3: bytesleft ⇐ n
4: // process permutation of every char individually:
5: for a = 0 to 255 do
6:
// retrieve permutation code for positions of value a
7:
codea ⇐ code mod

TABLE II.
THE CANTERBURY CORPUS [16] COMPRESSION PERFORMANCE
org. size
huffman
arithmetical
presented
ﬁlename
(bytes)
size
%
size
%
size
%
alice29.txt
152089
87688
57.66
86837
57.10
86788
57.06
asyoulik.txt
125179
75806
60.56
75235
60.10
75187
60.06
cp.html
24603
16199
65.84
16082
65.37
16035
65.17
ﬁelds.c
11150
7026
63.01
6980
62.60
6936
62.21
grammar.lsp
3721
2170
58.32
2155
57.91
2126
57.14
kennedy.xls
1029744
462532
44.92
459971
44.67
459779
44.65
lcet10.txt
426754
250565
58.71
249071
58.36
249008
58.35
plrabn12.txt
481861
275585
57.19
272936
56.64
272880
56.63
ptt5
513216
106551
20.76
77636
15.13
77563
15.11
sum
38240
25645
67.06
25473
66.61
25353
66.30
xargs.1
4227
2602
61.56
2589
61.25
2559
60.54
TABLE III.
THE CALGARY CORPUS [15] COMPRESSION PERFORMANCE
org. size
huffman
arithmetical
presented
ﬁlename
(bytes)
size
%
size
%
size
%
README
2479
1492
60.19
1483
59.82
1457
58.77
bib
111261
72761
65.40
72330
65.01
72273
64.96
book1
768771
438374
57.02
435043
56.59
434981
56.58
book2
610856
368300
60.29
365952
59.91
365877
59.90
geo
102400
72556
70.86
72274
70.58
72117
70.43
news
377109
246394
65.34
244633
64.87
244555
64.85
obj1
21504
16051
74.64
15989
74.35
15868
73.79
obj2
246814
194096
78.64
193144
78.25
192971
78.18
paper1
53161
33337
62.71
33113
62.29
33058
62.18
paper2
82199
47615
57.93
47280
57.52
47228
57.46
paper3
46526
27275
58.62
27132
58.32
27084
58.21
paper4
13286
7860
59.16
7806
58.75
7768
58.47
paper5
11954
7431
62.16
7376
61.70
7334
61.35
paper6
38105
24023
63.04
23861
62.62
23808
62.48
pic
513216
106551
20.76
77636
15.13
77563
15.11
progc
39611
25914
65.42
25743
64.99
25687
64.85
progl
71646
42982
59.99
42720
59.63
42668
59.55
progp
49379
30214
61.19
30052
60.86
30000
60.75
trans
93695
65218
69.61
64800
69.16
64734
69.09
REFERENCES
[1]
D. Salomon, “Data Compression The Complete Reference,” 4th ed.
London: Springer, 2007.
[2]
D. J. C. MacKay, “Information Theory, Inference, and Learning Al-
gorithms,” version 6.0
Cambridge University Press, June 2003, pp.
32.
[3]
C. E. Shannon, “A Matematical Theory of Communication,” Reprinted
with corrections from The Bell System Technical Journal, vol. 27,
October 1948, pp. 379–423, 623–656.
[4]
D. A. Huffman, “A method for construction of minimum-redundancy
codes,” Proceedings of the I.R.E., September 1958, pp. 1098–1101.
[5]
I. Witten, R. Neal, and J. Cleary, “Arithmetic Coding for Data Com-
pression,” Communications of the ACM, vol. 30, no. 6, June 1987, pp.
520–540.
[6]
A. Lempel and J. Ziv, “A Universal Algorithm for Sequential Data
Compression,” IEEE transactions on information theory, May 1977, pp.
337–343.
[7]
Microsoft,
“Microsoft
LZX
Data
Compression
Format,”
version
4.71.410.0
Microsoft Cabinet SDK, , March 1997.
[8]
M. Burrows and D. J. Wheeler, “A block-soring Lossless Data Com-
pression Algorithm,” System Research Center, Palo Alto, USA, research
report, May 1994.
[9]
J. Gil and D. A. Scott, “A Bijective String Sorting Transform,” Is-
rael/USA, July 2009.
[10]
J. Abel, “Improvements to the Burrows-Wheeler Compression Algo-
rithm: After BWT Stages,” preprint
D¨usburg-Essen, Germany, March
2003.
[11]
M. Kuﬂeitner, “On Bijective Variants of the Burrows-Wheeler Trans-
form.”
Prague, Czech Republic: Proceedings of PSC 2009, pp. 65–79.
[12]
A. Salomaa, “Public-Key Cryptography,” 2nd ed.
Finland: Springer,
1990.
[13]
M. Lothaire, “Combinatorics on words,”
Reading, Massachusetts:
Addison-Wesley, 1983.
[14]
R. Arnolds and T. Bell, “A corpus for the evaluation of lossless
compression algorithms.”
Christchurch, NZ: University of Canterbury.
[15]
T. C. Bell and I. Witten, “Calgary compression corpus,” [retrieved:
December, 2013]. [Online]. Available: ftp://ftp.cpsc.ucalgary.ca/pub/
projects/text.compression.corpus/
[16]
M. Powell and T. Bell, “The Canterbury corpus,” [retrieved: December,
2013]. [Online]. Available: http://corpus.canterbury.ac.nz/
[17]
D. R. Richardson, “libhuffman - An Open Source Huffman Coding
Library in C,” [retrieved: December, 2013]. [Online]. Available:
http://huffman.sourceforge.net/
[18]
http://www.telekom-stiftung.de
22
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-322-3
CTRQ 2014 : The Seventh International Conference on Communication Theory, Reliability, and Quality of Service

