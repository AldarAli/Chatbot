Kinect Skeleton Coordinate Calibration for Remote Physical Training
Tao Wei, Yuansong Qiao, Brian Lee 
Software Research Institute, Athlone Institute of Technology 
Athlone, Ireland 
{twei, ysqiao}@research.ait.ie, blee@ait.ie 
 
 
Abstract—With the advent of the Microsoft Kinect sensor, 
skeleton coordinate systems have become an active part of 
interactive multimedia applications. The skeleton coordinate 
data captured by the Kinect sensor can be used to compare the 
similarity of remote users’ motions in remote training systems. 
However, this approach is limited in that the remote users’ 
initial positions have to be at the same position and face the 
sensor with the same angle. This paper proposes a Kinect 
Skeleton Coordinate Calibration (KSCC) algorithm to 
calibrate the remote user’s arbitrary initial positions, thereby 
removing the above limitations on the initial positions and 
angles. It collects the remote user’s initial position data, and 
calculates the initial centre coordinate of the user and initial 
angle between user and Kinect sensor. After the collection and 
calculation, all skeleton coordinates are transformed to a 
universal coordinate system according to the initial centre 
coordinate and rotated by a quaternion rotation. An evaluation 
test has been performed to assess the accuracy and limiting 
fields of the system. The results show that our approach is able 
to calibrate the Kinect skeleton coordinates with a high 
accuracy, with the requirements that the user’s initial positions 
only need to be in the detection zone of the Kinect sensor. 
Keywords - Kinect skeleton coordinate; calibration; remote 
physical training; interactive multimedia 
I. 
 INTRODUCTION 
With advances in human body recognition technologies, 
some 3D sensors have been able to capture and analyse the 
human body without marker-based systems, which require 
the users to wear obtrusive devices. One such device is the 
Microsoft Kinect [1], a marker-less motion capturing sensor, 
which can track a user skeleton and capture data at a rate of 
30 frames per second using the Microsoft Kinect for 
Windows SDK [1]. Each such tracked skeleton contains 
twenty joints‘ 3D coordinates [2]. 
Our goal is to compare the similarity of remote users‘ 
motions. Various approaches [3][4][5] have been proposed 
to use the skeleton coordinates captured by Kinect sensor to 
do body motion comparison. In these approaches, they 
compare two skeletons with recorded data. These recorded 
data have several limitations, such as the initial positions and 
angles of the users are the same; the length and the content of 
the recorded data are constant. It is easy to control their 
experiments using the recorded data. However, for a 
practical remote body motion comparison system, the users 
may not act exactly according to the recorded data which are 
used in experimental environment. The users may stand at 
different positions relative to Kinect sensor, i.e., different 
angles facing the Kinect and/or different distances from the 
Kinect. Users may thus get very different coordinate data for 
the skeleton position, even if they do the same motion. It is 
difficult to compare two user-motions using these data. 
This paper proposes a KSCC algorithm. The main 
features of this approach are to ―pull‖ the user‘s skeleton to 
the centre of the Kinect sensor and then rotate it to face the 
Kinect sensor. KSCC treats this position as the initial 
position in a universal coordinate system. In this way all 
users‘ initial positions are normalized in the universal 
coordinate system, irrespective of their original standing 
position and angle. After reconstruction in the universal 
coordinate system, all movements of the skeleton are 
referenced in the universal coordinate system. In this 
approach, the user needs to stand motionless for about four 
seconds to enable the KSCC system to collect the data of the 
user‘s initial position. This data are then used to calculate the 
initial angle between user and Kinect sensor (let us call it 
‗initial angle‘) and the initial centre coordinate of the user‘s 
skeleton. Then KSCC transforms the twenty joints‘ 
coordinates of the user‘s skeleton according to the initial 
centre coordinate, and rotates them about the initial centre 
point‘s y-axis by the initial angle to a universal coordinate 
system. Consequently, all the skeletons are transformed to 
the same location with the same angle relative to the sensor. 
As a result, when different users do the same motions, they 
can get the same coordinate data for their skeletons, even if 
their initial positions are different. 
The following experiment is designed to test the KSCC 
algorithm. As it is difficult for two people to do exactly the 
same motion at the same time, we use one person as 
experimenter. We setup two Kinect sensors to capture the 
user‘s skeleton separately, and then run two calibration 
systems to detect the person at the same time. The two 
skeleton images captured by the two calibration systems are 
drawn in the same canvas. It is intuitive to compare the 
calibrated results. Next, the coordinate data of left and right 
shoulders are recorded to find out the two calibration 
systems‘ differences of value. The experimental results show 
that the skeleton coordinate data are accurately calibrated by 
the KSCC algorithm and the differences of the value are less 
than 4cm, given that both initial angles of the skeleton are all 
less than 20°. 
The rest of the paper is organized as following. In 
Section 2, the related work is presented and compared. The 
Kinect system environment and details of the KSCC 
algorithm are described in Section 3. Section 4 presents the 
experiments that have been made to evaluate the KSCC 
algorithm. Section 5 is dedicated to the conclusion and the 
future work. 
66
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

II. 
RELATED WORK 
Multimedia devices have been widely applied in remote 
physical training. Huang et al. [6] present a Multiple-video-
based E-learning Platform for Physical Education. Their 
system records videos and voices in three different angles 
synchronously. Then, the user reviews the records to teach or 
learn the sports actions. Li et al. [7] propose a tennis e-
learning system. In that system, a Nintendo Wii Remote is 
used as the input device to capture the motion of a tennis 
swing. The system is aimed at differentiating different types 
of swings. Muller et al. [8] propose a pre-processing method 
substantially 
accelerating 
the 
cost-intensive 
classical 
dynamic time warping techniques for the time alignment of 
logically similar motion data streams. 
With the advent of the Microsoft Kinect sensor, a lot of 
attention has been focused on skeleton coordinate system. 
Tamura et al. [9] propose a three-dimensional motion capture 
and feedback system for flying disc throwing action learners. 
Their system captures learners‘ body movement, checks their 
skeleton positions in pre-motion/motion/post-motion in 
several ways, and displays feedback messages to refine their 
actions. However, they set presupposed motion in the 
system. It only supports throwing motion comparison. Essid 
et al. [3][4] propose a virtual dance performance evaluator 
based on 17 skeletal joints positions. It can be used to 
evaluate a student‘s performance and provide him/her with 
meaningful feedback to aid improvement. In their system, 
Kinect sensors are used to acquire a ―choreography‖ dance 
rating. Three choreography scores are calculated by 
considering the modulus of the Quaternion Correlation 
Coefficient for each pair of joint position signals. The 3D 
coordinates of each joint are used to be input data directly. 
The angular skeleton representation of Raptis et al. [5] is a 
good method to remove dependence on Kinect position. 
They treat the torso as a vertically elongated rigid body. 
Their approach is to fit the full torso with a single frame of 
reference, and to use this frame to parameterize the 
orientation estimates of both the first-degree and second-
degree limb joints. However, these approaches compare 
experimenter‘s skeleton coordinates with recorded data, not 
with the real people. 
Due to different users‘ skeleton coordinates belonging to 
different Kinect skeleton coordinate systems, finding the 
relationship between the two coordinate systems is useful. A 
closed-form solution [10][11] is to calibrate a number of 
points‘ coordinates in two different Cartesian coordinate 
systems. Their approach transforms the points from one 
coordinate system to another using a 4 × 4 transformation 
matrix. However, their solution is used to solve local 
multiple cameras fusion problems, i.e., the multiple cameras 
must shoot the same object, and the system needs to know 
the points‘ coordinates data from all cameras system before 
calibration. 
In this paper, a novel calibration algorithm KSCC that 
calibrates all remote users‘ skeleton coordinates into a 
universal coordinate system is presented. Unlike [3][4][5][9] 
the KSCC algorithm is used for a practical remote body 
motion comparison system rather than recorded data or 
presupposed motions. It reduces the limitation of the users‘ 
initial positions and angles which are constant in recorded 
data. Moreover, unlike the closed-form solution [10][11], the 
KSCC algorithm does not transform the skeleton coordinate 
from one skeleton to another. The remote calibration systems 
of both users do not need to know each other‘s skeleton 
coordinates data before calibration. As a result, the remote 
calibrated skeletons can be compared in a universal 
coordinate system for the practical remote body motion 
comparison system. 
III. 
SKELETON CALIBRATION 
A. Kinect Skeleton Coordinate System 
As shown in Fig. 1, the Kinect sensor has a practical 
ranging limit of 82cm~400cm [2]. The Kinect sensor also 
can maintain tracking through an extended range of 
approximately 70cm~600cm in the context of ignoring some 
accuracy. The field of view of the sensor is pyramid shaped. 
It has an angular field of view of 57° horizontally and 43° 
vertically, while the motorized pivot is capable of tilting the 
sensor up to 27° either up or down [12]. 
Skeleton data contain 3D position data for human 
skeletons. Each joint position in the skeleton space is 
represented as (x, y, z). The skeleton space coordinates are 
expressed in meters. As illustrated in Fig. 2, it is a right-hand 
coordinate system that places a Kinect sensor at the origin. 
More specifically, the positive x-axis extends to the left of 
the Kinect, and the positive y-axis extends upward. The 
positive z-axis is extending in the direction in which the 
Kinect is looking at. 
It assumes that the distance between the Kinect sensor 
and floor is 100cm. And the surface on which the Kinect 
sensor is placed parallels the floor. Also, the Kinect sensor is 
not tilted, i.e., the z-axis of the skeleton coordinate system 
also parallels the floor. 
In the context of detecting the whole body of the user, the 
available active area is an isosceles trapezoid area. The 
minimum distance Dmin between Kinect sensor and the user 
can be calculated by 

Dmin = h / tan ( θv / 2 )

where h is the distance between Kinect sensor and floor. h 
equals 100cm. θv is the vertical angular field of view. θv 
equals 43°. 
 
Figure 1.  Detecting range of Kinect [2] 
67
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

 
Figure 2.  Kinect skeleton space 
The minimum distance Dmin equals 254cm, and the 
maximum Dmax equals 400cm which is limited by the Kinect 
sensor. 
The two bases of the isosceles trapezoid area can be 
calculated by 

B = D × tan ( θh / 2 )

where θh is the horizontal angular field of view, θh equals 
57°. The short base is calculated when D equals Dmin; the 
long base is calculated when D equals Dmax. 
Finally, the available active area is a isosceles trapezoid 
whose height is 146cm (Dmax – Dmin), and two bases are 
138cm and 217cm respectively. 
B. Initial Data Collection and Calculation 
In order to calibrate the initial position of the user, the 
system collects the first 120 frames as initial data. These 
initial data are used to calculate the initial angle and the 
initial centre coordinate of the user. Since the Kinect 
products about 30 frames data per second [2], it suggests that 
the user remains standing still around four seconds. 
1) Initial Angle between User and Kinect Sensor 
KSCC assumes that all joints of the user are in the same 
plane when the user stands straight. Also, the line between 
left shoulder and right shoulder is treated as the horizontal 
line in the user‘s body plane. According to the initial data, it 
obtains the average coordinate values of left and right 
shoulders as: 

LS = (Xl, Yl, Zl)


RS = (Xr, Yr, Zr)

The initial angle has three situations: 
1. The user‘s body plane is perpendicular to the z-axis 
direction of the skeleton coordinate system. 
 
2. The user‘s body plane faces right direction (Fig. 3-(a)). 
 
3. The user‘s body plane faces left direction (Fig. 3-(b)). 
 
(a)                                                    (b) 
Figure 3.  (a) The left shoulder is closer to the Kinect; (b) the right shoulder 
is closer to the Kinect. 
In the first situation, as the initial angle θ is 0, i.e., the 
user parallels the Kinect, it is not necessary to consider the 
angle problem. The other two situations are shown in Fig. 3. 
First, when the body plane faces right, the z-axis value of left 
shoulder (LS) is less than the right shoulder (RS). Second, 
when the body plane faces left direction, the z-axis value of 
LS is larger than the RS. 
The lengths of (LS, P) and (RS, P) in the right triangles 
can be calculated by 

D = Zr – Zl

where D is (RS, P) in Fig. 3-(a); D is (LS, P) in Fig. 3-(b). 

W = Xr – Xl

where W is (LS, P) in Fig. 3-(a); W is (RS, P) in Fig. 3-(b). 
Then, the initial angle θ is: 

θ = Atan ( D / W )

where θ is positive in the situation 2, is negative in the 
situation 3, and equals 0 in the situation 1. 
2) Initial Centre of User’s Skeleton 
In order to get the initial centre coordinate of user‘s 
skeleton, the sum of all joints coordinates in one frame is 
calculated by 

S

(X, Y, Z) = ∑j J

(X, Y, Z),   j = 0,…, 19

where, j is the index of joints in a skeleton. 
Then, the average of the twenty joints‘ coordinates is 
treated as the centre of the skeleton in one frame: 

A

(X, Y, Z) = S

(X, Y, Z) / 20

Finally, the initial centre coordinate of the user‘s skeleton 
in the period time can be calculated by the average of the 120 
centre coordinates: 

C

(XC, YC, ZC) = {∑t A

(X, Y, Z)} / T,   t = 1,…,T 
where T is the total frames in the period, here T equals 120. 
68
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

 
(a)                                                         (b) 
Figure 4.  (a) User‘s skeleton position before calibration;   (b) user‘s 
skeleton position after calibration. 
C. Transform and Rotation 
After the collection and calculation of the initial position 
data, the initial angle θ and the initial centre coordinate 
vector C(XC, YC, ZC) are obtained. These two results are the 
foundation of the calibration, and will be utilized all the time, 
unless the Kinect sensor is moved or the system is restarted. 
Then, any joint coordinates can be transformed and rotated to 
a universal coordinate system that places the initial centre at 
the origin. 
As illustrated in Fig. 4, the calibration process can be 
regarded as pulling the original skeleton to the centre of the 
Kinect sensor, and rotating it to face the Kinect sensor. This 
is the initial position in a universal coordinate system. Thus, 
all users‘ initial positions are the same in the universal 
coordinate 
system, 
wherever 
they 
stand 
at. 
After 
reconstruction in the universal coordinate system, all 
movements of the skeleton are in the universal coordinate 
system. 
Firstly, all joints are transformed to the origin of the 
universal coordinate system according to the initial centre 
coordinate: 
P

j(XP, YP, ZP) = (Xj – XC, Yj – YC, Zj – ZC,), j = 0,…,19 
where Xj, Yj, Zj are coordinates of joint j; XC, YC, ZC are 
coordinates of the initial centre. 
Secondly, a quaternion rotation [13] is used to rotate the 
coordinate vector Pj(XP, YP, ZP) about the y-axis of the initial 
centre by the initial angle θ. The quaternion rotation is a right 
handed rotation. The thumb points the direction of unit 
rotation axis vector R which is the y-axis of the initial centre. 

R

 = (XR, YR, ZR)

where || R

|| = 1. Thus, XR = 0, YR = 1 and ZR = 0. 
The rotation quaternion [13] is defined to be: 
Q = cos( 2
 ) + XR sin( 2
 )i + YR sin( 2
 )j + ZR sin( 2
 )k 
The point P

j(XP, YP, ZP) is viewed as a quaternion 
without scalar part [14]: 

QP = 0 + XP i + YP j +ZP k

Then, to rotate QP about the axis R

 by the angle θ, the 
quaternion rotation function [13] is defined to be: 

QPR = Q × QP × Q-1


Q-1 = 
Q
Q
*
Q



where Q-1 is the reciprocal [14] of the rotation quaternion Q, 
Q* is the conjugation [14] of the rotation quaternion Q. 
The result of QPR is a quaternion without scalar part: 
QPR = 0 + (XP cosθ + ZP sinθ) i + YP j + (ZP cosθ – XP sinθ) k

Finally, the vector part of the quaternion QPR is the 
coordinate of the new point: 

NPR = (XP cosθ + ZP sinθ, YP, ZP cosθ – XP sinθ)

The system can utilize the calculated initial angle and 
initial centre coordinate as long as the Kinect remains in the 
same position. 
IV. 
EXPERIMENTAL RESULT 
In order to evaluate the accuracy of this system, a 
skeleton calibration experimental system (Fig. 5) is used to 
show and compare calibrated results from two Kinect 
calibration systems. The reason for using two Kinect to test 
one person is that it is difficult for two people to do the 
completely same motion at the same time. In our 
experimental system, for the two Kinect calibration systems, 
the person does the same motion all the time. Consequently, 
comparing the calibrated results of both systems is an 
effective method to test the KSCC algorithm.  
Firstly, when the two systems finish the initial data 
collection, one system starts to send the skeleton calibrated 
results to another system via TCP. According to the 
calibrated skeleton coordinates, two skeletons which come 
from two systems respectively are displayed in one canvas. 
Shown in Fig. 6 is an example of the image results after 
calibration. The left skeleton (Fig. 6-(a)) captured by the left 
Kinect is facing right. While the middle skeleton (Fig. 6-(b)) 
captured by the right Kinect is facing left. Fig. 6-(c) shows 
the result after calibrating the two skeletons. Both skeletons 
are calibrated to face the same direction, and the coordinates 
of corresponding joints are very close to each other. The 
result indicates that the KSCC algorithm is able to calibrate 
the Kinect skeleton coordinate system.  
 
 
69
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

TABLE I.  
AVERAGE DIFFERENCES OF TWO KINECT SKELETON CALIBRATION SYSTEMS 
Test Index 
Left Angle 
Right Angle 
Angle Gap 
Average Difference before Movement 
Average Difference after Movement 
X 
Y 
Z 
X 
Y 
Z 
Test1 
-3.4648° 
0.7889° 
4.2537° 
0.00cm 
1.00cm 
2.00cm 
2.00cm 
1.28cm 
3.78cm 
Test2 
16.3570° 
0.5013° 
15.7557° 
0.50cm 
0.00cm 
3.00cm 
1.71cm 
1.57cm 
2.92cm 
Test3 
21.2544° 
0.1923° 
21.0621° 
0.50cm 
1.00cm 
3.00cm 
1.21cm 
1.50cm 
2.14cm 
Test4 
26.4897° 
0.9105° 
25.5792° 
0.50cm 
1.50cm 
1.00cm 
1.85cm 
1.92cm 
3.87cm 
Test5 
14.9705° 
-12.1837° 
27.1542° 
0.50cm 
1.00cm 
1.00cm 
2.14cm 
1.50cm 
1.64cm 
Test6 
30.6423° 
2.0577° 
28.5846° 
0.50cm 
0.50cm 
2.50cm 
4.50cm 
1.50cm 
5.57cm 
Test7 
21.0891° 
-17.7214° 
38.8105° 
0.00cm 
0.50cm 
2.50cm 
2.00cm 
1.78cm 
3.21cm 
Test8 
24.5130° 
-19.8589° 
44.3719° 
1.00cm 
1.00cm 
1.50cm 
4.28cm 
1.64cm 
5.28cm 
Test9 
24.5566° 
-25.8503° 
50.4069° 
1.50cm 
0.50cm 
2.50cm 
6.35cm 
2.21cm 
6.00cm 
Test10 
29.3320° 
-26.0919° 
55.4239° 
0.00cm 
1.00cm 
2.00cm 
4.50cm 
3.21cm 
5.71cm 
Note: 1° = 1 degree of arc, 1cm = 1 centimetre.
 
 
Figure 5.  Skeleton calibration testing system 
 
 
         (a)                                      (b)                                       (c) 
Figure 6.  Original skeletons and calibrated skeletons:                                
(a) skeleton captured by left Kinect; (b) skeleton captured by right Kinect; 
(c) calibrated skeletons result 
 
Figure 7.  Average differences of 10 Tests 
 
In order to evaluate out the accuracy of the KSCC 
algorithm, the relative differences for the two calibrated 
skeletons are also evaluated by using the coordinate recorded 
for the left and right shoulders as estimation of the 
measurement. Ten different Kinect angle classes with a total 
of 420 pairs of recording are evaluated. Each Kinect angle 
class represents different pairs of initial angles. In each test, 
seven different locations of experimenter (initial location, 
move a step forward, move a step to the left, move a step to 
the right, move a step backward, move a step backward and a 
step to the left, and move a step backward and a step to the 
right. All movements start from the initial location) are used 
to record the coordinates. 
Table I shows ten tests‘ average differences of two 
Kinect skeleton calibration systems. The second column 
(Left Angle) is the initial angle between experimenter and 
the left Kinect; the third column (Right Angle) is the initial 
angle between experimenter and the right Kinect; the fourth 
column (Angle Gap) is the gap of the Left Angle and Right 
Angle; the following three columns are the average 
differences of coordinates (X, Y, Z) before movement, i.e., 
after calibration the experimenter still stand at the initial 
location; the last three columns are the average differences of 
coordinates (X, Y, Z) after movement, each value is 
calculated by fourteen pairs of recording (two shoulders and 
seven locations). We test the initial angle in a range from 
0.1923° to 30.6423°. Since the experimenter is human, it is 
difficult to find absolute 0 degree. We also test the maximum 
workable initial angle, when the Kinect can detect all twenty 
joints of the experimenter. The maximum angle is up to 50°. 
The variation of the maximum angle depends on different 
standing location of the experimenter. Standing at the middle 
of the Kinect detection zone has smaller maximum angle 
than standing at the edges. And the accuracy of the Kinect 
will be decreased when the angle is increased. Moreover, it 
will increase the probability of self-occluded other body 
parts [15][17]. 
The initial angle gap is a range from 4.2537° to 55.4239°. 
The average differences in Table 1 show that the differences 
are very small (1cm~3cm). After movement, the difference 
increases with the increasing of the initial angle gap. The 
70
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

average differences of Y are much smaller than X and Z, and 
more stable. The maximum average difference is 6.35cm 
(Test9-X). 
Test1 is intended to get the least differences, but due to 
the two Kinect sensors being very close in Test1, the 
interference with each other will result in a larger error [11]. 
The average differences of Test6 become large abruptly. It 
illustrates that the average differences not only depends on 
the initial angle gap, but also depends on the value of the 
Left Angle or Right Angle. 
A random error of depth measurement increases with 
increasing distance to a Kinect sensor, and ranges from a few 
millimetres up to about 4cm at the maximum range 
(82cm~400cm) of the sensor [15][16]. If setting 4cm as 
boundary in Fig. 7, there are two kinds of solutions. Solution 
1: the initial angle of one Kinect sensor is set to be around 0 
degree and the initial angle of another Kinect sensor must be 
less than around 26.4897° (Test4). Solution 2: the two initial 
angles (Left Angle and Right Angle) are all less than 
approximately 21.2544° (Test3). Consequently, in order to 
control the average differences to less than 4cm, the solution 
of satisfying the two situations is that the two initial angles 
must be all less than approximately 20°. 
V. 
CONCLUSION AND FUTURE WORK 
In this paper, we proposed a KSCC algorithm to calibrate 
the Kinect skeleton coordinate for remote physical training 
applications. The calibration approach is based on the user‘s 
initial position data detected by the Kinect sensor. The user‘s 
initial centre coordinate and initial angle can be calculated by 
the initial position data. Twenty joints‘ coordinates of the 
user are transformed according to this initial centre 
coordinate, and rotated by quaternion rotation to a universal 
coordinate system. An experiment is designed to assess the 
accuracy and limitation of the system. The experiment 
results show that the proposed method removes the common 
constraint in traditional motion comparison systems that the 
users‘ initial positions have to be at the same position and 
facing the sensor with the same angle. Instead, users are 
allowed to stand at any position in the detection zone of the 
Kinect sensor, and the initial angle is extended to 20° for a 
high accuracy result. This improves the consumer experience 
and gives users more freedom. 
As a foundation work for body motion comparison for 
remote physical training, the KSCC algorithm still needs 
improvement to reduce the constraints of user‘s position and 
motion. As mentioned previously, the accuracy of the Kinect 
will be decreased when the initial angle is increased. And 
some body parts often self-occluded due to the limitation of 
single Kinect sensor. In future work, multiple sensors can be 
utilized in a 360° environment to reduce the limitation of the 
single Kinect sensor and extend the available active area of 
the users. 
The Kinect based body motion comparison for remote 
physical training is not discussed in this paper. In the future, 
it will be shown that this can be done simply and easily with 
the KSCC algorithm. Finally, the system will be tested with 
real physical training motions under remote environment. 
REFERENCES 
[1] Microsoft 
Kinect, 
[Online] 
Available: 
http://www.microsoft.com/en-us/kinectforwindows/, 
December 2013. 
[2] J. Webb and J. Ashley, ―Beginning Kinect Programming with 
the Microsoft Kinect SDK,‖ , 2012, pp. 52, pp. 67-100. 
[3] S. Essid, et al., ―An advanced virtual dance performance 
evaluator,‖ IEEE Conference on Acoustics, Speech and 
Signal Processing (ICASSP), 2012, pp. 2269 - 2272. 
[4] D. Alexiadis, P. Daras, P. Kelly, N. E. O‘Connor, T. 
Boubekeur and M. B. Moussa, ―Evaluating a dancer‘s 
performance using Kinect-based skeleton tracking,‖ in ACM 
Multimedia, 2011, pp. 659-662. 
[5] M. Raptis, D. Kirovski and H. Hoppe, ―Real-Time 
clissification of dance gesture from skeleton animation,‖ 
Eurographics ACM SIGGRAPH Symposium on Computer 
Animation, 2011, pp. 147-156. 
[6] C. H. Huang, T. L. Won, C. Y. Liu, Y. D. Chen and Y. H. 
Chen, ―Multiple-video-based E-learning platform for physical 
education,‖ Pervasive Computing (JCPC), 3-5 Dec. 2009, pp. 
21-26. 
[7] K. F. Li, T. Kosuke and G. J. Mark, ―Motion tracking and 
processing for multimedia sport E-Learning,‖ International 
Conference 
on 
Broadband 
and 
Wireless 
Computing, 
Communication and Applications, 2011, pp. 75-82. 
[8] M. Muller, T. Roder, and M. Clausen, ―Efficient content-
based retrieval of motion capture data,‖ in ACM Siggraph, 
2005, pp. 677-685. 
[9] Y. Tamura, K. Yamaoka, M. Uehara, and T. Shima, ―Capture 
and feedback in flying disc throw with use of Kinect,‖ Word 
Academy of Science, engineering and Technology, Feb. 2013, 
pp. 313-317. 
[10] B. K. P. Horn, ―Closed-form solution of absolute orientation 
using unit quaternions,‖ Journal of the Optical Society of 
America, April 1987, Vol. 4, Issue 4, pp. 629-642. 
[11] M. Caon, Y. Yue, J. Tscherrig, E. Mugellini, and O. A. 
Khaled, ―Context-Aware 3D gesture interaction based on 
multiple Kinects,‖ The first International conference on 
Ambient 
Computing, 
Applications, 
Services 
and 
Technologies, October 2011, pp. 7-12. 
[12] Kinect 
Wikipedia, 
[Online] 
Available: 
http://en.wikipedia.org/wiki/Kinect, December 2013 
[13] C. H. John, K. F. George and H. K. Louis, ―Visualizing 
quaternion rotation‖, ACM Transactions on Graphics, July 
1994, Vol. 13, No. 3. 
[14] Quaternion 
Wikipedia, 
[Online] 
Available: 
http://en.wikipedia.org/wiki/Quaternion, December 2013 
[15] J. Shotton, et al, ―Real-time human pose recognition in parts 
from single depth images,‖ in CVPR, 2011, pp. 1297-1304. 
[16] K. Khoshelham and S. O. Elberink, ―Accuracy and resolution 
of Kinect depth data for indoor mapping applications,‖ 
Sensors (Basel) 2012, 12(2): 1437-1454. 
[17] S. Obdrzalek, et al, ―Accuracy and robustness of Kinect pose 
estimation in the context of coaching of elderly population,‖ 
Engineering in Medicine and Biology Society (EMBC), 2012 
Annual International Conference of the IEEE, Aug. 28, 2012-
Sept. 1, 2012, pp. 1188 - 1193. 
 
71
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

