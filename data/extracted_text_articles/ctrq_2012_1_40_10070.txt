 
 
Abstract—A single point of failure (SPOF) in system 
operations is a weak point of system reliability. Mean time to 
failure (MTTF) of system operations is equal to the shortage 
component’s MTTF in system. A Tier IV data center is designed 
to eliminate the SPOF. Data center system reliability is not only 
depended on the MTTF of each component in the system, but 
also relies on the mean time to repair (MTTR) of each 
component.  Researcher performed Tier IV DC power 
distribution 
systems 
(PDS) 
through 
simulating 
software, 
BlockSim7. The research question is tried to investigate how to 
improve system reliability. Component’s inherent characteristic 
(CIC) and system connectivity topology (SCT) are applied to 
improve the system reliability of Tier IV data center. The results 
demonstrated an increasing PDS reliability, site plus site, of Tier 
IV data center and improving survival probability of system that 
helps for future improvement on any critical system.   
 
Keywords—System Reliability; Mean Time To Failure; Mean 
Time To Repair; Probability density function (pdf). 
 
I. INTRODUCTION 
The redundancy represents a possible approach to 
enhancing system reliability. In a series-parallel design 
methodology, serial systems reduce reliability, while more 
parallel systems help increase it. The redundancy scheme 
helps enhance the overall system reliability. It, however, costs 
more. A data center consists of multiple hardware components 
that are bound to fail sooner or later. The Tier IV data center is 
a fault tolerant system that is designed to eliminate a single 
point of failure (SPOF) [3]. System downtime adversely 
affects not only recovery and lost opportunity costs, but also 
the company’s reputation and customers’ confidence. The 
reliability and cost trade-off becomes a controversial issue 
among all concerned, including top management, IT 
managers, and financial managers [8]. Different organizations 
have different levels of recovery time objective (RTO) and 
recovery point objective (RPO) subject to system failures and 
power outages to varying degrees of risks [4]. This paper 
employs the reliability block diagrams (RBD) with reliability 
information obtained from IEEE 493, or the so-called Gold 
Book, and component vendors’ field test data. The study 
attempts to improve the data center system reliability by 
integrating 2 parallel systems of TIA 942’s Tier IV data center 
[3]. The research question comes up with, how the reliability 
of two parallel systems (PS) is higher than two parallel load-
sharing (LS) systems and which once is the most benefits to 
investor subject to investment, efficiency, and system 
reliability. Research findings suggest the system connectivity 
topology, i.e., parallel topology, helps increase the system 
reliability of DC operations. 
II.  RELIABILITY BACKGROUND 
A. Reliability Factors on Data Center Failures 
System failure in data centers may be caused by many 
sources. 
1. Human error: daily operation, regular planned downtime 
for maintenance, and unplanned downtime [5], [9].  
2. Component’s inherent characteristics (CIC) and system 
failures are dependent on mean time to failure (MTTF), 
the complexity of system connectivity topology (SCT), 
and operational conditions. A component/system 
failure may propagate or activate other component/ 
system fault, error, or failure. This process or failure is 
similar to a chain-reaction that is affected from 
component to component, component to system, or 
system to system [5], [7]. 
3. Operational conditions are other factors that relates to 
system failures, e.g., humidity, temperature, altitude, 
and dust. 
4. Natural disasters; this is beyond the control of data center 
to handle. A design for a parallel site needs to be 
considered to compensate for downtime losses [8]. 
 
B. Reliability Determination 
All equipment reliability data is obtained from the IEEE 
493 Gold Book Standard, as shown in Table I. Fig. 1 depicts a 
single line diagram of a representative network for the Tier IV 
data center. The components shown in the networks are 
labeled with numbers, which correspond with the reference 
numbers in Table I. Network reliability analysis is performed 
with reliability data for referenced components taken from this 
table.  
This paper investigated the system reliability/availability 
of a Tier IV data center in terms of the frequency and duration 
of power outages. System availability depends on: 
1. Reliability and maintainability of its components: 
including mean time between failure (MTBF) and 
mean time to repair (MTTR) of component’s inherent 
characteristics (CIC) distribution, failure modes 
effects and criticality analysis (FMECA), and 
environmental effects [4], [7]. 
System Reliability of Fault Tolerant Data Center 
Montri Wiboonrat 
College of Graduate Study in Management 
Khon Kaen University, Bangkok Campus 
Bangkok, Thailand 
montwi@kku.ac.com, mwiboonrat@gmail.com  
19
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-192-2
CTRQ 2012 : The Fifth International Conference on Communication Theory, Reliability, and Quality of Service

 
 
2. System design or system connectivity topology (SCT) 
(configuration or topology, dependency, and failure 
detection). 
3. System operation behavior (operational characteristics, 
switching procedures, and maintenance services).  
The following assumptions apply to the proposed Tier IV 
data center system networks, as shown in Fig. 1: 
• Failure rates and repair times are exponentially 
distributed. 
• Actual power cable lengths may be indicated on the 
drawing. The cable failure rate is thus determined per 
the indicated actual cable length.  
• The generators are 2N redundant. 
• The power grids, generators and UPSs are 2(N+1) 
redundant, applicable to Tier IV. 
• The transformers, switchgears, automatic transfer 
switches (ATSs) and bus bars are redundant. 
• There are two paths of power distribution systems. 
• Terminations and splices, while normal for all systems, 
are not included on the drawing, and are not included in 
the calculations. 
• The assumed breaker failure modes are 50% open and 
50% short. 
III. DATA CENTER MODEL ASSUMPTION  
A data center is a complex system that consists of 16 
systems. Operations on business continuity strategy mode: 
primary and secondary sites require 2 data centers operating at 
the same time to back each other up, i.e., system-of-systems 
(SoS) or site-plus-site [3]. To limit the scope of this 
investigation, the researcher focused on data center Tier IV, 
power distribution systems (PDS) and power distribution 
system-of-systems (PDSS: both primary and secondary), 
parallel input, from incoming utility throughout loaded 
consumptions, e.g. server racks, storage racks, and networking 
equipment racks. This research assumes the external systems 
e.g. utility system, communication system or integrated 
service provider (ISP), are described to operation 100% 
uptime during this research.  
PDS is the most sensitive system for data center downtime. 
Thus, the research tested fault injection by the cutting of the 
main utility system as human error or unplanned downtime. 
UPSs will take action to recharge power back to the system 
immediately as long as the battery can handle the loaded 
points. Gen-Set will activate within 15 seconds to change 
power back to the UPSs to resupply on loaded points [10]. By 
TIA 942, Tier IV, Gen-Set availability is 96 hours for 
consecutive operation without interruptions [3].  
On this research simulation, the research defines the fault 
tolerance of data center system design as reliability of 2 
parallel system of Tier IV data center. The modification 
prototype in Fig. 2 is reproduced from original Tier IV data 
center from Uptime Institute, Fig.1. Each number in Fig. 2 is 
referred from IEEE 493 Gold Book [7]. Table I refers to 
characteristic of each number in Fig. 2 that identifies failure 
rate, MTBF, and MTTR of each number.  A failure rate (λ) of 
Primary data center system and Secondary data center system 
is assumed equivalent and each system has a constant (λ). 
 
3
1
10
UPS
UPS
10
PDU
Server
Racks
10
4
5
2
6
7
10
8
9
10
9
Mech System
A
4
6
8
16
11
16
9
9
9
9
9
9
16
16
STS
16
16
9
9
9
22
16
13
16
16
6
6
60
6
6
6
16
16
16
1
10
UPS
UPS
10
PDU
10
4
5
2
6
7
8
9
10
9
Mech System
B
16
11
16
9
9
9
9
9
9
16
16
STS
16
16
9
9
9
22
16
13
16
16
6
6
6
16
16
16
6
3
4
6
8
60
 
 
Fig. 2.  PDSS of Tier IV Data Center 
 
The reliability of data center system designs for the two 
parallel systems are reliable in parallel operations only when 
the failure of both systems results in system operation. On the 
other hand, for a parallel system to succeed, at least one of the 
two parallel systems in the whole system needs to perform 
successfully, or operate without failure, for the operating 
interval on the intended mission.   
The research proposes a simulation approach applied to a 
reliability block diagram (RBD) by BlickSim 7. The system 
reliability results from Fig.1 through RBD show the MTBF of 
Tier IV data center is 75,434.78 hours, and the failure rate (λ) 
is 14.0865x10-6 [5]. The exponential distribution is applied in 
Tier IV data center reliability analysis. The f(T) of the 
exponential distribution is extremely convenient, often used to 
describe a steady-state hazard rate, as shown in Fig. 3 [2].  
20
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-192-2
CTRQ 2012 : The Fifth International Conference on Communication Theory, Reliability, and Quality of Service

 
 
 
 
Fig. 3.  Relationship of pdf and Operation Time [2] 
 
The probability density function (pdf) of two parallel 
systems, that two systems are equal, when λ 1 = λ 2 = λ, is 
given by: 
dT
T
d R
T
f
SP
SP
( )]
[
( )
= −
                                 (1) 
The failure rate of two parallel systems is given by: 
 
T
T
T
T
PS
e
e
e
e
T
λ
λ
λ
λ
λ
λ
λ
2
2
2
2
2
)
(
−
−
−
−
−
−
=
                               (2) 
The MTBF of the PS is given by: 
 
λ
λ
λ
λ
+
−
+
=
1
1
1
MTBFPS
, 
or 
λ
5.1
MTBFPS =
, 
or 
m
MTBFPS
5.1
=
                 (3) 
Where m is the MTBF of each DC-PDSS unit. It must be 
observed that for parallel systems even with data center PDSS 
units that have a constant failure rate (λ).  
 
PS
PS
MTBF
T
1
( )
≠
λ
             (4) 
 
The precise determination of the SoS reliability, the 
change of failure rate, and PDSS reliability of the surviving 
system need to be properly taken into account. The reliability 
of the SoS of two load-sharing exponential units shown in Fig. 
4 is given by Kececioglu (1991) as follows: 
R(t): Probability that Primary site (PS or 1) and Secondary 
site (SS or 2) complete their mission successful  with pdf’s  
f1(T) and f2(T), respectively, or the probability that PS fails at 
t1 < t with pdf  f1(T), and SS functions till t1 with pdf  f2(T) and 
the functions for the rest of the mission, on in (t - t1), with pdf  
f2’ (T), or the probability that SS fails at t2 < t with pdf  f2(T)  
and  PS  functions till  t2  with  pdf  f1(T) and then functions 
with pdf  f1’ (T)  in (t – t2). 
This scenario is presented in Table II and depicted in pdf 
form in Fig. 4. when both of PDSSs are exponential. The 
system model of integration of two equal system failure rates 
and SCT as parallel is illustrated in Fig. 4. Fig. 4 (a) and (b) 
shows the overall failure rate of system integration decrease 
under the parallel topology concept [1]. Fig. 4 (a) is illustrated 
chronologically and shows sequence reaction for the total 
system when Primary data center fails at time t1   after starting 
the operation period. The indicator of changing factor is 
transformed by 
'2
2
λ ⇒ λ
 on Secondary data center. And, 
vice versa, the changing on Fig. 4 (b) is transformed by  
'1
1
λ ⇒ λ
   on Primary data center when Secondary data 
center failure during time t1 of operation period, when 
'
'2
'1
λ
λ
λ
=
=
, is given. 
 
TABLE II 
MATRIX OF SOS, FUNCTION MODES, PDF’S AND TIME  DOMAINS  FOR ALL 
PDSS SUCCESS FUNCTION MODES [1] 
 
 
    
G*: SoS is good throughout the designed mission. 
  B**: SoS fails before the designed mission 
 
This is age and mission dependent even through the units 
are exponential. The first age mission t = T. Mathematically, 
the reliability of the parallel system (PS) is given by: 
 
T
T
PS
e
e
T
R
λ
λ
2
2
)
(
−
−
−
=
          (5) 
It must be observed that 
λPS (T)
is not constant, but a 
function of age, although each system of data center has a 
constant λ 1 = λ 2 = λ and
'
'2
'1
λ
λ
λ
=
=
. 
 
System integration and operations of site plus site, normal 
operation of data center PDSS, of Tier IV data center is shown 
on Fig. 5. Utility is going to supply power throughout the 
PDS: Transformer, automatic transfer switch (ATS), bus-bar, 
UPSs, bus-bar, and loaded points. Calculation of SoS or 
parallel system (PS) of Tier IV data center is derived from 
Kececioglu (1991). Kececioglu defines MTBFPS of two 
systems equal to 1.5m, as referred to, in (3). Substitution λ = 
14.0865x10-6 and T = 43,800 hours of Tier IV data center to 
(2). The result is 
λPS (T)
= 8.88215x10-6. Substitution 
λPS (T)
 
(1), we will receive MTBFPS= 112,585.371417 hours. The 
result from substitution 
λPS (T)
=  8.88215x10-6 and T = 
43,800 hours to (5) shows the reliability R(T) of parallel 
system of Tier IV data center, SoS, during 5 years equal to 
89.6128152%. 
21
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-192-2
CTRQ 2012 : The Fifth International Conference on Communication Theory, Reliability, and Quality of Service

 
 
T
e
.
1
1
.
λ
λ
−
T
e
.
2
2
.
λ
λ
−
)1
'2
'
.(
2 .
t t
e
−
−λ
λ
'2λ
T
T
T
T
PS
e
e
e
e
T
λ
λ
λ
λ
λ
λ
λ
2
2
2
2
2
)
(
−
−
−
−
−
−
=
1λ
(T) 2λ
f
PS
λ
 
(a). A pdf’s of two parallel DC systems when primary DC fails 
)2
'1
'
.(
1 .
t t
e
−
−λ
λ
'1λ
T
e
.
1
1
.
λ
λ
−
T
e
.
2
2
.
λ
λ
−
T
T
T
T
PS
e
e
e
e
T
λ
λ
λ
λ
λ
λ
λ
2
2
2
2
2
)
(
−
−
−
−
−
−
=
PS
λ
1λ
(T) 2λ
f
 
(b). A pdf’s of two parallel DC systems when secondary DC fails 
 
Fig. 4. Comparing between pdf and operation time of DC-PDSS load-sharing 
system [1] 
 
The assumption on Fig. 4 shows
λ
λ
PS <
, we need to 
prove 
λ < λ'
 after Primary site failover to Secondary site, as 
shown in Fig. 5. Since in (4) derived results contrast with (1). 
We are given:  
MTBF t or t
2
1
 is MTBF of each data center PDSS system 
before failure of Primary or Secondary site, 
MTBF t or t
'2
'1
is MTBF of each data center PDSS after 
failure of Primary or Secondary site,  
λ  is failure rate of each data center PDSS before failure, 
'λ is failure rate of each data center PDSS after failure. 
t
t
t
t
or
or
PS
PS
MTBF
MTBF
and MTBF
'2
'1
2
1
'
>
>
<
<
λ
λ
λ
  (6) 
Now we have MTBFPS = 112,585.371417 hours and 
substitute to (3). We received m = 75,056.91 hours. When we 
compare with original MTBF from Tier IV data center that 
equal 
to 
75,434.78 
hours, 
that 
means 
t
t
t
t
or
or
MTBF
MTBF
'2
'1
2
1
>
 and vice versa 
λ < λ'
 as well for 
(1), as depicted in Fig. 4. Hence the assumption on (6) is 
correct.  
)
' 2 (
2
( )
'
2
2
t
t
t
LS
e
e
e
t
R
λ
λ
λ
λ
λ
λ
−
−
−
−
−
+
=
      (7) 
 
For the two parallel load-sharing (LS) of data center 
PDSS’s, if the data center PDSS’s are equal, i.e., they have the 
same pdf, then λ 1 = λ 2 = λ and
'
'2
'1
λ
λ
λ
=
=
, we will derive 
(7) at t = 5 years, as illustrated in Fig. 4. The reliability result 
is 85.3775%.  
When one data center PDSS fails before the mission is 
completed in LS, and the data center PDSS is exponential. The 
reliability of the existing Secondary data center PDSS when 
Primary site fails calculates from (8), as shown in Fig. 3(a). 
)
(
'
1
'2
'2
1
'2
1
1
1
'
2
'
2
( )
( )
)
(
t t
t
t
e
e
e
t
R
t
R
t
t
R
−
= −
=
=
−
=
λ
λ
λ
    (8) 
Assumption, the Primary site fail at time = t1, what is the 
reliability of the Secondary site during the left 3 year 
operation? 
Substitution 
t, 
t2, 
and
'
λ ; 
from 
previous 
calculation, and t= 43,800, t1 = 17,520 hours to (8). We will 
get R2’ (t) = 70.4593882%. 
Normal operation of data center PDSS is depicted in Fig. 5 
or on Primary site on left hand and Secondary site on the right 
hand. The utilities supply power throughout the PDSS: aerial 
cable, fuse, transformer, cable, automatic transfer switch 
(ATS), bus-duct, circuit breaker, cable, UPS, circuit breaker, 
cable, PDU, cable, and loaded points. After utility outage or 
PDSS fails on Primary or Secondary site, normal operation is 
degraded, when either of both sites is resume accomplishing 
the system will resume to normal operation, as illustrated in 
Fig. 6. Whenever, data center of Primary or Secondary site 
failure the services for external operations, end users, will not 
interrupt. The capacity to handle the transactions will reduce a 
half or increase double waiting queue or time for executing 
process. Absolute failure in case can be happened only when 
both sites are completely destroyed or malfunction at the same 
time [12]. 
IV.   DISCUSSION 
A reliability of the parallel Tier IV data center system for a 
critical mission of T duration before the first failure or 5 years 
on this simulation is 89.6128152%. This 89.6128152% 
implies that the probability of survival on normal operation 
will still perform function continuously during mission time 
over 5 years or 10.3871848% of the probability of the parallel 
Tier IV data center system which may fail before 43,800 hours 
of mission operation. The reliability of two parallel systems 
(PS) is higher than two parallel load-sharing (LS) systems; 
conversely the efficiency of LS system is better than the 
normal two parallel systems in terms of distributing 
transaction, service response times, and expanding capacity 
and capability.  
The research defines the Tier IV DC parallel system failure 
when both Primary data center and Secondary data center fail 
at the same time. MTTR and maintenance systems of each 
data center are the keys to keeping parallel systems more 
reliable and available. The critical condition, which is MTTR, 
must be less than the maximum tolerable period of disruption 
(MTPD) [11]. MTTF of equipment is depended on 4 
conditions; first, selected on CICs and designed on SCT of a 
data center system, second, operation of data center site 
conditions, third, related risks on daily operation of  human 
activities, and last, procedural maintenance [5]. The most 
reasons of system failures come from human omission, e.g., 
they do not follow the manual instruction step by step, or 
commission, e.g., the system crashes during installing system.
22
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-192-2
CTRQ 2012 : The Fifth International Conference on Communication Theory, Reliability, and Quality of Service

 
 
Utility Sw Gare
Gen Sw Gear
GEN
N
GEN
+1
UPS
N
UPS
+1
Utility Sw Gare
Mech Sw Gear
Critical MCC
Gen Sw Gear
GEN
N
GEN
+1
Mech Sw Gear
Critical MCC
Utility Sw Gare
UPS
N
UPS
+1
Utility Sw Gare
Critical Fans 
Or Pumps
Mech System
Loaded 
Points
PDU
PDU
HV
HV
Zone I
Zone III
Zone II
Zone 0
Utility A
Utility B
 
 
 
Fig. 1.  Original Tier IV Data Center Diagram [3] 
 
 
Transformer A
Transformer B
Utility A
Utility B
Gen-Set A
Gen-Set B
ATS
ATS
Safety
Interlock
Communication Channel A
FW
UPS
STS
Storages/ Servers/ Applications
UPS
FW
STS
Networking
Transformer A
Transformer B
Gen-Set A
Gen-Set B
ATS
ATS
Safety
Interlock
FW
UPS
STS
Storages/ Servers/ Applications
UPS
FW
STS
Networking
Communication Channel B
UZ
Controllable Zone
UZ
Primary DC
Secondary DC
Distance 200 miles
 
 
Fig. 5.  Parallel System Design of Site plus Site Tier IV Data Center 
 
23
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-192-2
CTRQ 2012 : The Fifth International Conference on Communication Theory, Reliability, and Quality of Service

 
 
TABLE I 
EQUIPMENT RELIABILITY DATA FROM IEEE 493 GOLD BOOK [7] 
 
 
 
 
 
24
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-192-2
CTRQ 2012 : The Fifth International Conference on Communication Theory, Reliability, and Quality of Service

 
Propagation
Activation
 
 
Fig. 6.  System Failure Life Cycle Model. 
 
 
During the design process, engineers, consultants, and 
designers need to understand throughout the transformation of 
a system failure cycle. The research results are derived from 
the root cause analysis of each system failure cycle to prepare 
the preventive actions and pre-planning for the corrective 
actions.   
V.  CONCLUSION 
A simulation results from reliability block diagram (RBD) 
helps consultants, data center designers, IT managers, and 
contractors to foreseen the output of reliability system design.  
This helps to save time and costs from trial and error processes 
which in real data center operations cannot be accepted. To 
improve the system reliability data center designer needs to 
understand the MTTF, MTTR, CIC, and SCT of each type 
design pattern to optimize between reliability and investment. 
As a result from equation (6),  
t
t
t
t
or
or
PS
PS
MTBF
MTBF
MTBF
and
'2
'1
2
1
'
>
>
<
<
λ
λ
λ
 is shown the MTBF of each 
λ'
λ
λ
<
PS <
 presented 
reliability MTBF; 89.6128152%> 85.3775% > 70.4593882%  
respectively, as in equation (6). This is implied that the 
reliability of two parallel systems (PS) is higher than two 
parallel load-sharing (LS) systems. 
A regular maintenance, monitoring, and automatically 
control system is not only preventive and alert the system 
before disaster occurs but also help increase system reliability, 
system operations on energy conservative mode, and extending 
operations life-cycle of all equipments. All of these key factors 
are contributing to data center project operations success.  
 
REFERENCES 
[1] D. Kececioglu, “Reliability Engineering Handbook,” Volume 2, Prentice 
Hall, Englewood, Cliffs, New Jersey, 1991. 
[2] D. J. Klinger, Y. Nakada, and M. A. Menendez, “AT&T Reliability 
Manual,” AT&T, Van Nostrand Reinhold, New York, 1st edition, 
February 3, 1999. 
[3] Turner IV, W. P., J. H. Seader, V. Renaud, and K. G. Brill, Tier 
Classification Define Site Infrastructure Performance, White Paper, The 
Uptime Institute, Inc. 2008. 
[4] M. Wiboonrat, “An Empirical IT Contingency Planning Model for 
Disaster Recovery Strategy Selection,” IEEE International Engineering 
Management Conference, June, 28 to 30, 2008, Estoril, Portugal, 
accepted for publication. 
[5] M. Wiboonrat, “An Empirical Study on Data Center System Failure 
Diagnosis,” 3rd International Conference on Internet Monitoring and 
Protection, IEEE ICIMP 2008, Romania, June 29-July 5, 2008, pp. 103-
108. 
[6] M. Wiboonrat, “An Optimal Data Center Availability and Investment 
Trade-Offs,” 9th International Conference on Software Engineering, 
Artificial Intelligence, Networking, and Parallel/ Distributed Computing, 
IEEE SNPD 2008, Thailand, August 6-8, 2008, accepted for publication. 
[7] M. Wiboonrat, “Beyond Data Center Tier IV Reliability Enhancement,” 
Power Conversion and Power Management, Digital Power Europe, 
November 13-15, Munich, Germany, 2007. 
[8] M. Wiboonrat, “Power Reliability and Cost Trade-Offs: A Comparative 
Evaluation between Tier III and Tier IV Data Centers,” Power 
Conversion and Power Management, Digital Power Forum, 10-12 
September 2007, San Francisco. 
[9] M. Wiboonrat, and C. Jungthirapanich, “A Taxonomy of Causal Factors 
and Success Criteria on Project Management Success and Project 
Success,” Eight Int. Conference on Opers. & Quant. Management, 
ICOQM 2007, October 17-20 2007. 
[10] M. Wiboonrat, “Dependability Analysis of Data Center Tier III,” 13th 
International Telecommunications Network Strategy and Planning 
Symposium, Networks 2008 in Budapest, Hungary, September 28 – 
October 2, 2008, accepted for publication. 
[11] IEEE Std. 1100-1999, (Revision of IEEE Std. 1100-1992), IEEE 
Recommendation Practice for Powering and Grounding Electronic 
Equipment, 22 March 1999. 
[12] M. Wiboonrat, “Transformation of System Failure Life Cycle,” 
International Journal of Management Science and Engineering 
Management, 
Vol. 
4, 
No. 
2, 
2008, 
pp. 
143-152.
 
25
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-192-2
CTRQ 2012 : The Fifth International Conference on Communication Theory, Reliability, and Quality of Service

