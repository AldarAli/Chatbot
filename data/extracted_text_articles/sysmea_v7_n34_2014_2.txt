201
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Optical, Mathematical, and Computational
Foundations of Lensless Ultra-Miniature Diffractive
Imagers and Sensors
David G. Stork and Patrick R. Gill
Computational Sensing and Imaging
Rambus Labs
1050 Enterprise Way, Suite 700
Sunnyvale, CA 94089 USA
{dstork,pgill}@rambus.com
Abstract—We describe the optical, mathematical and compu-
tational foundations for a new class of lensless, ultra-miniature
computational imagers and image sensors. Such sensors employ
phase gratings that have provably optimal optical properties and
are integrated with CMOS photodetector matrices. These imagers
have no lens and can thus be made extremely small (∼100 µm)
and very inexpensive (a few Euro cents). Because the apertures
are small, they have an effective depth of ﬁeld ranging from
roughly 1 mm to inﬁnity. The grating acts as a two-dimensional
visual “chirp” and preserves image power throughout the Fourier
plane; thus the captured signals preserve image information. The
ﬁnal digital image is not captured as in a traditional camera
but is instead computed from raw photodetector signals. The
novel representation at the photodetectors demands powerful
algorithms such as deconvolution, Bayesian estimation, or matrix
inversion with Tikhonov regularization be used to compute the
image, each having different bandwidth, space and computational
complexities for a given image ﬁdelity. Such imaging architectures
can also be tailored to extract application-speciﬁc information
or compute decisions (rather than compute an image) based on
the optical signal. In most cases, both the phase grating and
the signal processing can incorporate prior information about
the visual ﬁeld and the imaging or estimation task at hand.
Our sensor design methodology relies on modular parallel and
computationally efﬁcient software tools for simulating optical
diffraction, for CAD design and layout of gratings themselves,
and for sensor signal processing. These sensors are so small they
should ﬁnd use in endoscopy, medical sensing, machine inspection,
surveillance and the Internet of Things, and are so inexpensive
that they should ﬁnd use in distributed network applications and
in a number of single-use or disposable applications, for instance
in military, hazardous natural and industrial conditions.
Keywords: Computational sensing, phase grating, diffractive
imager, application-speciﬁc sensing, face detection, QR code read-
ing
I.
INTRODUCTION
Recent theoretical and computational advances provide a
foundation for a new class of computational optical image
sensor: one that forgoes the use of traditional optical ele-
ments such as lenses and curved mirrors and relies instead
upon diffractive optical elements [1], [2]. Whereas diffractive
methods have been employed in other wavebands, such as
millimeter-wave imaging, prior, traditional optical imaging
architectures have generally been based on the camera obscura
model—in which each point in the scene is imaged onto a
single point on a sensor or image plane. This model has
dominated the science and technology of imaging systems for
several millennia, at least for sources illuminated by incoherent
light. The Chinese philosopher Mo Ti traced an inverted image
produced by a pinhole camera to record an image in the ﬁfth
century B.C.E. [3] and Johannes Kepler traced a real image
projected by a converging lens onto paper in 1603. Chemical
recording of projected images, such as by mercury or silver
halide, was invented in 1826 and the ﬁrst true digital camera
was built in 1975 [4], all these exploiting the fundamental
camera obscura architecture.
As photodetector sensor technology has improved and pixel
pitches have become smaller, pixels can be made smaller than
the optical diffraction limit of systems such as commercial
cameras [5]–[7]. Pixels smaller than the diffraction limit, how-
ever, do not provide new image information. Instead, such sub-
diffraction-limit pixels provide opportunities to make “smart
pixels” with functionality beyond mere direct conversion of
photons to electric current [8].
The rise in digital imaging, where image processing can
be incorporated into the data chain, has enabled new imag-
ing architectures. Although related concepts were explored
in computational radar and x-ray astronomy, it was Cathey
and Dowski who took an early and conceptually important
step away from the traditional camera obscura model for
optical imaging by exploiting digital processing in a deep
way [9]. They designed a cubic-phase optical plate which,
when inserted into the optical path of a traditional camera,
led to an image whose (signiﬁcant) blur was independent of
the object depth: the image on the sensor plane did not “look
good” as it would in a traditional camera obscura. Subsequent
image processing sharpened the entire blurred image, thus
leading to enhanced depth of ﬁeld. Since then the ﬁeld of
computational imaging has explored imaging architectures
in which the raw signals do not superﬁcially resemble a
traditional image; instead, the ﬁnal image is computed from
such signals. More and more of the total imaging “burden” is
borne by computation, thereby expanding the class of usable
optical components. In this way, many optical aberrations
can be corrected computationally rather than optically. This
imaging paradigm has led to new conceptual foundations of
joint design of optics and image processing [10], as well

202
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
as a wide range of non-standard imaging architectures such
as plenoptic, coded-aperture and multi-aperture systems, each
with associated methods of signal processing [11]–[15].
Fig. 1. The left ordinate axis in red shows the resolution (in pixels) versus the
physical volume (in mm3) of representative lens- and mirror-based telescopes
and cameras (log-log scale). Notice there is a seven-order-of-magnitude range
in physical volume devoid of such cameras (the Valley of darkness). 1 Grand
Canaria telescope, 2 Hubble telescope, 3 1-m telescope, 4 30-cm telescope,
5 AWARE 2 camera, 6 Professional camera,7 Consumer DSLR, 8 iPhone
5 camera, 9 Pelican camera, 10 Miniature VGA, 11 Medigus camera, 12
Single photodiode (without lens). The right ordinate axis in blue indicates
the sales of representative imagers of different physical volumes in units/year
worldwide in 2011. (The unit sales ﬁgures are estimates based on historical
data and market reports and do not include research prototypes and unreleased
products.) There is a precipitous drop in sales at the Valley of darkness. Our
lensless integrated diffraction grating/CMOS imagers lie within this “valley.”
The economic pressures for miniaturization of electronic
devices, including cameras, arising in the mobile computing
market have led to smaller imager form factors [16]. Figure 1
shows the resolution, in total pixels per exposure, versus
physical volume of imaging systems in the traditional camera
obscura architecture (or curved mirror equivalent). While such
imagers span 22 orders of magnitude in physical volume and
15 orders of magnitude in pixel resolution, the smaller the
imager the greater the number sold commercially... but only
down to a scale of roughly 1 mm3. There is a conspicuous
gap of seven orders of magnitude in physical volume—the
“Valley of darkness”—between the smallest digital camera
and a single unlensed photoreceptor. It seems that the camera
obscura model has reached its physical limits and cannot
be scaled much smaller. A new imaging architecture—with
new optical, mathematical and computational foundations—is
required to span the Valley of darkness.
Recently, a new miniature imaging architecture has been
explored, one based on integrating optics with CMOS photo-
detectors [2], [17]–[19]. In brief, this architecture forgoes
lenses and relies instead on simple square-wave diffraction
gratings created in CMOS itself. The earliest designs in
this architecture relied on CMOS wires to act as amplitude
optical grating patches, the gratings producing a wavelet-
like representation of the scene on the sensor matrix. More
recently, square-wave phase gratings have also been explored
[20]. For a given image resolution, such diffractive elements
enable the construction of imagers much smaller than does
the basic camera obscura model. (We mention in passing that
related CMOS structures have been explored for integrated
spectroscopy as well [21].) Note too that as given by the trends
in resolution versus physical volume evident in Fig. 1, imagers
in the Valley of darkness will have nominal resolutions (pixels
per single frame) lower than roughly 105 pixels [22], [23].
Nevertheless, such low-resolution imagers—or high-resolution
sensors—should ﬁnd use in many applications, especially in
the Internet of Things (see Section V).
There are a number of limitations of such previous work.
First, amplitude gratings based on CMOS wires have poor
low-light sensitivity because most of the incident light never
strikes the photodetector. Second, regular diffraction gratings
are by their very nature wavelength sensitive, i.e., the pattern
of light on the photodetectors depends strongly upon the
wavelength of incident light. Third, such imagers are sensitive
to manufacturing defects—speciﬁcally a small deviation in
the thickness of the grating layer can lead to a large (and
difﬁcult to correct) alteration of the diffraction pattern on the
photodetectors [18].
The method we describe here, while based on integrated
silicate phase optics and CMOS image sensors, is fundamen-
tally different from prior work in a number of deep ways.
Our method relies on novel special phase anti-symmetric spiral
phase gratings, which overcome prior limitations and afford
new functionality [24]. Moreover, our new sensor architecture
enables the construction of new classes of ultra-miniature
sensors whose output is an estimation of some property of the
scene (e.g., visual motion) or a decision (e.g., face detection
or barcode reading).
We begin in Section II with a discussion of our fundamental
technology and turn in Section III to a short description of our
software design and analysis tools. We describe our ﬁrst hard-
ware devices in Section IV. The full results of our hardware
veriﬁcation of the theory and design will be presented at a
later date [25]. We mention a few application areas for such
sensors and imagers in Section V and conclude in Section VI
with some ﬁnal remarks.
II.
SENSOR OPTICS AND TECHNOLOGY
The following description of our sensor technology follows
the data path—from target source through diffractive optics to
photodetector to digital signal processing to ﬁnal digital image
or image estimation.
A. Optics of one-dimensional phase anti-symmetric gratings
The fundamental optical elements employed by our sensors
are based on a new type of phase grating having phase
antisymmetry. Figure 2 shows a cross section through a UV-
curable acrylate binary phase grating, here speciﬁed by three
free parameters, w0, w1 and w2 [26]. (Generalizations to more
free parameters and multiple thicknesses are straightforward.)
Consider point P lying on the grating’s plane of odd symmetry,
shown as a vertical dashed red line. The steps in thickness of
the acrylate grating correspond to a phase delay of π radians
of the typical wavelength used in imaging. Such a phase
difference means that light from each position on one side
of the plane is cancelled via destructive interference by light
from the symmetric position on the other side of the plane

203
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
because those waves arrive out of phase. Note especially that
such cancellation occurs regardless of the vertical depth of P;
as such, all points along the red dashed line are dark. We call
this plane of destructive interference an “optical curtain” or
simply “curtain” [27]. The location of the curtain on the sensor
matrix below does not change despite manufacturing errors in
overall grating thickness. Finally, as the angle of incidence of
the light changes, the curtains tip by the same angle (Fig. 3),
a transformation that makes calibration particularly simple
problem of estimating a spatial shift. In this way, the sensor
responses are invariant to variations in manufactured thickness
and wavelength of incident light (Fig. 4). Greater wavelength
invariance can be achieved by using an additional layer of
silicate with different index of refraction and dispersion coef-
ﬁcient than the primary grating, much as chromatic aberration
is corrected in classical lens-based imaging systems through
the use of multiple lenses with different indexes of refraction
and dispersion [6].
P
w2
w1
w0
w2
w2
w0
w1
w2
w1
w1
Fig. 2.
A cross section through a binary anti-symmetric phase grating, where
the plane of odd symmetry is marked with a vertical dashed red line. The
parameters w0, w1 and w2 describe the surface proﬁle. For the medium’s
index of refraction n, the step height is chosen to corresponds to optical
phase delay of π radians along the red dashed line or “curtain.”
For such
a phase anti-symmetic grating, curtains exist even if the incident light is not
normal.
Fig. 3.
A ﬁnite-difference wave simulation of the electric ﬁeld energy
density for monochromatic light incident at 3.5◦ passing through a phase
anti-symmetric grating where x denotes the position left-to-right and z the
depth within the silicate medium. The curtains lie beneath the points of odd
symmetry and are tipped at the same angle as the incident light. Such curtains
are invariant to the wavelength of incident light. The photodetector matrix,
shown as pixels in different colors, lies along the bottom.
B. Phase anti-symmetric spiral gratings
The scenes we seek to image are two-dimensional and
therefore the one-dimensional phase anti-symmetric grating
Fig. 4.
The response of a single photodetector (pixel) beneath a phase anti-
symmetric grating (such as P in Fig. 2) as a function of angle of incident light,
θ, and wavelength of light, λ. Notice that for normally incident light (θ = 0◦)
the response nearly vanishes at all wavelengths and that at each incident
orientation, the response is nearly invariant with respect to wavelength. The
speciﬁc form of this response function depends upon the proﬁles of the
grating (described by wis), which can be tailored to extract information most
appropriate to particular applications, including non-imaging applications.
and photosensor array just described must be generalized to
two dimensions. Speciﬁcally, two-dimensional gratings must
include segments at every orientation so as to sample the
Fourier domain uniformly (and possess no zeros) and thereby
enable computational reconstruction of the image from sen-
sor responses. Figure 5 shows two examples of basic spiral
grating tiles—having four-fold and six-fold chiral symmetry.
These spiral grating tiles are constructed by sweeping one-
dimensional phase anti-symmetric gratings perpendicularly
along the length of each spiral arm. The phase anti-symmetric
gratings are lengthened and made more complicated (use more
ws) to cover the full tile area and feasible Fourier domain. Both
spiral gratings pass information at all orientations and spatial
frequencies up to the Nyquist limit, and can be tiled to cover
a full photodetector matrix of arbitrary area (Fig. 6) [24]. In
actual sensors, incident light covers an area at least as large
as that of a full individual tile element.
The wave optics described above assumes the incident
illumination is plane-wave. In such a case the pattern of light
produced by a grating does not depend upon the distance of
the object, so long as the object is farther from the sensor than
roughly 10 times the spatial scale of the sensor itself. As such,
our sensor has extremely large effective depth of ﬁeld, from
roughly 1 mm to inﬁnity.
The pattern of light produced by the diffraction grating
strikes the CMOS photodetector matrix beneath and the signals
are sent off chip for digital processing.
C. Signal processing
Sensed signals in our sensor do not resemble an image in a
camera obscura but must be processed to yield a digital image.
We assume the overall forward imaging model is described by:

204
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Fig. 5.
The left column shows phase anti-symmetric spiral binary gratings,
the middle column the point-spread function each produces (both ﬁgures of
spatial extent D × D, for some distance D). The right column shows the
corresponding modulation transfer function (modulus of the Fourier transform)
of extent 1/P × 1/P, where P is the pixel pitch and determines the Nyquist
rate. The top row corresponds to four-fold chiral symmetry and the bottom
row corresponds to six-fold chiral symmetry.
Fig. 6.
The individual grating tiles of Fig. 5 can be packed to cover a
photodetector matrix of arbitrary area. Alternate approaches to tessellating a
sensor array with such individual grating designs are not as space efﬁcient.
y = Ax + n,
(1)
where y is the vector of photodetector pixel responses, x
is a vector of inputs from the scene, A the system matrix
describing the linear transformation performed by the two-
dimensional optical grating, and n is additive noise, which
describes photodetector noise, Poisson photon statistics, quan-
tization noise, etc. (Other models, such as simple multiplicative
noise, could also be assumed.) We let x be m-dimensional,
both y and n be n-dimensional; hence A has dimensions
m × n.
The regularized least-square estimation problem—that is,
the reconstruction of the image—can be expressed as ﬁnding
the image ˆx that minimizes the error or cost function
C = ∥Aˆx − y∥2 + ∥Γˆx∥2,
(2)
where Γ weights the different components of ˆx, for instance
to accommodate differences prior probabilities of pixel values
in the expected scenes. The image that minimizes the cost C
in (2) is [28]
ˆx =

205
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
x
y = Ax + n
ˆx
Fig. 7.
Image sensing and computational reconstruction of Leonardo’s Mona
Lisa from a lensless phase anti-symmetric spiral phase grating sensor. (Left)
The input image. (Middle) The simulated response on the photodetectors
due to the six-fold grating in Fig. 5, and (right) the reconstruction by
Eq. 4. This image estimate is of higher ﬁdelity than the estimate based
on traditional square-wave amplitude gratings and photodetector arrays of
comparable number of pixels and overall noise level described in earlier work.
III.
SIMULATION/DESIGN TOOLS AND METHODOLOGY
Our sensor system design and analysis methods are based
on a modular architecture comprising three software tools, all
written in Matlab and executed on a large network of PCs:
•
Optics of phase gratings: We simulate the interaction
of light with gratings, for instance by ﬁnite-difference
wave algorithms. These full-three-dimensional sim-
ulations reveal the electromagnetic energy density
throughout the silicate grating volume (see Fig. 3) and
predict the response of physical photodetector pixels
to light of different wavelengths and incident angles,
such as in Fig. 4.
•
CAD design of gratings and tiles: We design grat-
ings (spiral and otherwise) and their tilings starting
from a mathematical description of the grating, often
parameterized by the number of arms, arm chirality
and curvature function, and phase cross-section as a
function of distance from the center (i.e., the wi shown
in Fig. 2). The representation of our design is either
a Matlab-compatible ﬁle for wave optics simulations
or a gdsII ﬁle for silicon grating manufacture.
•
Sensor signal processing: We continue to write
our own image reconstruction, signal estimation and
pattern recognition software in Matlab, often using
standard libraries of matrix operations such as Moore-
Penrose pseudoinverse. In some research systems, we
incorporate free software such as QR code symbol
reading software.
We can employ Perl software wrappers for these compo-
nents in order to efﬁciently design and model the system’s
end-to-end performance. Such joint design methodology can
often lead to superior system performance (higher ﬁdelity
reconstruction, few optical elements, etc.) than sequential
design, where optics is designed ﬁrst and only then is the signal
processing designed [32].
IV.
HARDWARE IMPLEMENTATION
Our experimental hardware implementation of lensless
imagers and sensors is based on a single pixel-addressable
10 Mega-pixel sensor from Aptina, Inc., with a single large
grating platform comprising 40 experiments (Fig. 8). The
gratings are made of a 50-µm-thick layer of acrylic (known as
Ugoo)1 with grating steps of 1.5 µm afﬁxed to a 400-µm-thick
glass substrate. Figure 9 shows a micrograph of one portion of
the full grating. Input images are presented on an LCD display
under computer control, and signals are read directly from the
Aptina sensor and processed on a PC.
Fig. 8.
The Ugoo silicate 5.5 × 4 mm grating platform contains 40 grating
experiments. Some of the experiments involve tesselated areas for applications
with lenses, as shown in Fig. 6. Fiducial marks at the lower-left and upper-
right of the platform facilitate the estimation of the alignment of the grating
with the underlying photodetector matrix.
Physical instantiation of the sensor, calibration of its A ma-
trix, estimation of noise (photon and circuit), and development
of accurate and computationally efﬁcient image reconstruction
methods for the hardware as built—all to verify the above
theory—is in progress and will be presented separately [25].
Fig. 9.
A scanning electron micrograph of the grating at the lower left in
Fig. 8.
V.
APPLICATIONS
There are many promising applications for our compu-
tational image sensors, which fall into a number of general
1manufactured by Holographix, LLC

206
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
categories. It is important to note, though, that these imagers
were not designed to compete with high-resolution cameras
that are larger and more expensive. Just as most animal and
insect vision systems are fairly low resolution but numerous,
so too our sensors are designed for numerous applications
requiring only relatively simple vision and image analysis.
It is as convenient to consider our devices as high-resolution
sensors as it is low-resolution imagers.
Some general categories of applications follow.
A. Low-resolution imaging
The ultra-miniature size of our imagers and sensors make
them especially appropriate for very small environments in
medical and industrial endoscopy as well as traditional and
some novel mobile computing devices. There are many surveil-
lance applications that would proﬁt from low- to mid-level
resolutions as well. Because these sensors are so inexpensive
(in bulk)—each less expensive than a single frame of 35-mm
photographic ﬁlm—they could ﬁnd application in a number
of one-use imaging scenarios arising in military theaters,
hazardous industrial conditions (crash tests) and natural en-
vironments [32]. Another general area is inexpensive mobile
medical imaging and sensing of the form pioneered by Ozcan
and his colleagues [33]. A key design decision is where the
signal processing should be implemented—close to the sensor
itself, or instead on a host machine, possibly delayed from the
signal capture.
The sensor described above is panchromatic, that is, it
responds to any optical wavelength and yields a monochrome
(grayscale) image. There are a number of ways to extend the
lensless imaging architecture to yield color images. The most
direct method would be to have three separate sensors, each
optimized for a different optical wavelength—short, medium
and long wavelengths, corresponding to blue, green and red—
and integrating the component images.
B. Motion detection and estimation
The optical gratings and signal processing algorithms can
be tailored to broad image sensing applications. For instance,
because each pixel in such a sensor responds to light from
an extended region in the visual ﬁeld, only a few such pixels
need be monitored in order to detect a change in the image.
Therefore, such a sensor has very low power dissipation in its
waiting or sentinel model. Once an image change has occurred,
the full complement of pixels can be read so that an image can
be captured or motion estimated. This kind of functionality
is valuable for occupancy detection for controlled lighting,
motion (motion-activated devices), visual looming (pre-impact
automotive airbag deployment), interactive toys, and numerous
applications in support of the Internet of Things [34].
C. Pattern recognition
These sensors can extract informative visual information
for pattern recognition applications, such as face detection (au-
thentication), one-dimensional barcode and two-dimensional
QR code reading (Fig. 10), gesture recognition and many
others. Of course, the signal processing is then based on prin-
ciples of pattern recognition appropriate for the task at hand
[35], [36]. For instance, QR code symbol reading software
must determine the orientation or tip angle of a symbol, and
does so by ﬁrst locating the three ﬁducial concentric squares
visible in Fig. 10 a), c) and d). This ﬁrst step in QR symbol
reading cannot be performed on the raw sensor representation.
Moreover, because code analysis and error correction apply to
the spatial domain, any lensless diffractive QR code symbol
reader should ﬁrst compute the pixel image of the symbol.
a)
b)
c)
d)
Fig. 10.
a) A Version 2 (25×25) target QR code symbol with information
payload of 31 bytes. b) The raw signals in the 400 × 400 pixels array in
our computational sensor. c) The digital image computed from the sensor
signals using Tikhonov regularization. d) The ﬁnal digital image, rotated and
thresholded by line to yield roughly 50% white pixels. This ﬁnal image is
presented to ZXing QR code reading software, which decodes the image
to extract its 31-byte code. Note that these barcode images relied on a
grating designed for general imaging; a special purpose grating, designed to
extract straight lines and right angles, with corresponding digital processing,
would likely yield QR symbol images of higher ﬁdelity and higher barcode
recognition rates. Note the slight reconstruction errors in the upper-right pixels
in d). Despite such slight reconstruction errors, error correction in the symbol
reading algorithms ensured this symbol was decoded accurately.
Such a low-resolution sensor is unlikely to support high-
accuracy face recognition among many candidate identities
[37], but could be used to identify whether some face—any
face—is present. Such functionality would be valuable for
waking up appliances or other connected devices in the Internet
of Things. Figure 11 shows the results of realistic simulations
of such a face presence detector based on the sensor described
above. The classiﬁer is based on a nearest-neighbor algorithm
[35, Chapter 4]. The test images consisted of 168 grayscale
face images in various orientations and scales as well as
simple non-face images. All recognition and classiﬁcation was
performed in the raw sensor representation—no traditional
human-interpretable images were computed.
Let F denote the set of sensor patterns corresponding to
faces (including transformations of rotations and scaling), and
G the set of general (i.e., non-face) images. For each of the 168
3600-dimensional patterns x ∈ F, we computed the Euclidean
distance D(x, x′) to the nearest other face pattern x′ (̸= x) ∈
F. The histogram of such distances is shown at the front of

207
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Fig. 11 in green. Then for each such pattern x we compute
the Euclidean distance to the nearest non-face pattern x′′ ∈ G.
This histogram is shown in red. Of course, on-average such
inter-face distances are less than the distances from faces to
non-face patterns, i.e., D(x, x′) < D(x, x′′). Because there is
some overlap in the red and the green histograms, this face
detection error is not 0 but in fact roughly 0.09. The Bayes
classiﬁer based on this distance D error is shown along the
far-right face in Fig. 11.
The above analysis was repeated on the 1800, 900, 400,
200, 100 and 50 features, yielding the additional green and
red histograms in Fig. 11. As expected, all histograms shift to
smaller overall distance D in the subspaces and the overlap
increases; thus the face/non-face error increases as the feature
space has fewer and fewer dimensions. These simulation re-
sults show, however, that our computational diffractive imager
design should yield an acceptable single-frame detection error
rate of roughly 0.1 with as few as 100 features.
10
50
100
200
400
900
1800
3600
Number of
features
5
10
Log@DD
0
0.1
0.2
0.3
0.4
FacenonFace error
Fig. 11.
The performance of a lensless ultra-miniature diffractive sensor for
distinguishing faces from non-faces. The logarithm of the distance D in the
full 3600-dimensional space and in subspaces of lower dimension (as listed
at the left in blue) are shown. Along a blue line marking a given number
of features, each green histogram represents the number of face patterns that
have the indicated distance to other face patterns and each red histogram
represents the (larger, on average) distance from a face to a non-face. The
optimal classiﬁcation rule is based on the crossing point of the red and the
green histograms, and the overlap of the histograms represents the relative
face/non-face classiﬁcation error.
VI.
FINAL REMARKS
We have designed and veriﬁed through full end-to-end
system simulation a new class of lensless computational im-
agers based on phase anti-symmetric spiral gratings. We have
built the components and are moving towards full hardware
characterization of gratings and veriﬁcation of imaging func-
tionality. These imagers promise to be smaller (lower physical
volume) than any existing lens-based imagers of comparable
resolution, very inexpensive, and customizable to both imaging
and a wide range of sensing and image measurement tasks. A
full description of the hardware manufacture, calibration, and
imaging performance are presented elsewhere [25].
Practical ﬁelded applications will lead to many interesting
problems in efﬁcient application-speciﬁc algorithms, either on
special-purpose ASICs, on highly parallel graphics processor
units (GPUs), or on general-purpose central processor units
(CPUs). Networks of such sensors highlight several problems
and opportunities in power usage and bandwidth optimization.
ACKNOWLEDGMENTS
We thank Thomas Vogelsang and Michael Ching for help-
ful comments.
REFERENCES
[1]
D. G. Stork and P. R. Gill, “Lensless ultra-miniature computational
sensors and imagers,” in SensorComm 2013, Barcelona, Spain, 2013.
[2]
P. R. Gill, C. Lee, D.-G. Lee, A. Wang, and A. Molnar, “A microscale
camera using direct Fourier-domain scene capture,” Optics Letters,
vol. 36, no. 15, pp. 2949–2951, 2011.
[3]
T. Gustavson, Camera: A history of photography from Daguerreotype
to digital.
New York, NY: Sterling Publishing Co., 2009.
[4]
D. Wooters and T. Mulligan, A history of photography—from 1839 to
the present.
New York, NY: Taschen, 2005.
[5]
D. Falk, D. Brill, and D. G. Stork, Seeing the light: Optics in nature,
photography, color, vision and holography.
New York, NY: Wiley,
1986.
[6]
M. V. Klein, Optics.
New York, NY: Wiley Publishing, 1970.
[7]
D. J. Brady, Optical imaging and spectroscopy.
New York, NY: Wiley
and Optical Society of America, 2009.
[8]
T. Vogelsang, D. G. Stork, and M. Guidash, “Hardware validated uniﬁed
model of multibit temporally and spatially oversampled image sensors
with conditional reset,” Journal of Electronic Imaging, vol. 23, no. 1,
p. 013021, 2014.
[9]
W. T. Cathey and E. R. Dowski, Jr., “A new paradigm for imaging
systems,” Applied Optics, vol. 42, no. 29, pp. 6080–6092, 2002.
[10]
D. G. Stork and M. D. Robinson, “Theoretical foundations of joint
design of electro-optical imaging systems,” Applied Optics, vol. 47,
no. 10, pp. B64–75, 2008.
[11]
E. H. Adelson and J. Y. A. Wang, “Single lens stereo with a plenoptic
camera,” IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, vol. 14, no. 2, pp. 99–106, 1992.
[12]
A. Levin, R. Fergus, F. Durand, and W. T. Freeman, “Image and depth
from a conventional camera with a coded aperture,” ACM Transactions
on Graphics, vol. 26, no. 3, pp. 70:1–70:9, 2007.
[13]
D. L. Marks, D. S. Kittle, H. S. Son, S. H. Youn, S. D. Feller, J. Kim,
D. J. Brady, D. R. Golish, E. M. Vera, M. E. Gehm, R. A. Stack,
E. J. Tremblay, and J. E. Ford, “Gigapixel imaging with the AWARE
multiscale camera,” Optics and Photonics News, vol. 23, no. 12, p. 31,
2012.
[14]
D. L. Donoho, “Compressed sensing,” IEEE Transactions on Informa-
tion Theory, vol. 52, no. 4, pp. 1289–1306, 2006.
[15]
M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T. Sun,
K. F. Kelly, and R. G. Baraniuk, “Single-pixel imaging via compressive
sampling,” IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 83–91,
2008.
[16]
J. M. Kahn, R. H. Katz, and K. S. J. Pister, “Next century challenges:
Mobile networking for ‘Smart Dust’,” in Proceedings of the 5th An-
nual ACM/IEEE International Conference on Mobile Computing and
Networking (MobiComm 99), 1999, pp. 271–278.
[17]
P. R. Gill, C. Lee, S. Sivaramakrishnan, and A. Molnar, “Robustness of
planar Fourier capture arrays to colour changes and lost pixels,” Journal
of Instrumentation, vol. 7, pp. C01–61, 2012.
[18]
A. Wang and A. Molnar, “A light-ﬁeld image sensor in 180 nm CMOS,”
IEEE Journal of Solid-State Circuits, vol. 47, no. 1, pp. 257–271, 2012.
[19]
A. Wang, P. R. Gill, and A. Molnar, “Light ﬁeld image sensors based
on the Talbot effect,” Applied Optics, vol. 48, no. 31, pp. 5897–5905,
2009.
[20]
S. Sivaramakrishnan, A. Wang, P. R. Gill, and A. Molnar, “Enhanced
angle sensitive pixels for light ﬁeld imaging,” in IEEE International
Electron Devices Meeting (IEDM), 2011, pp. 8.6.1–8.6.4.
[21]
C. Peroz, S. Dhuey, A. Goltsov, M. Volger, B. Harteneck, I. Ivonin,
A. Bugrov, S. Cabrini, S. Babin, and V. Yankov, “Digital spectrometer-
on-chip fabricated by step and repeat nanoimprint lithography on pre-
spin coated ﬁlms,” Microelectronic Engineering, vol. 88, no. 8, pp.
2092–2095, 2011.

208
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[22]
O. Cossairt, M. Gupta, and S. Nayar, “When does computational imag-
ing improve performance,” IEEE Transactions on Image Processing,
vol. 22, no. 2, pp. 447–458, 2013.
[23]
O. Cossairt, M. Gupta, K. Mitra, and A. Veeraraghavan, “Performance
bounds for computational imaging,” Imaging and Applied Optics, 2013.
[24]
P. R. Gill and D. G. Stork, “Lensless ultra miniature imagers using odd-
symmetry phase gratings,” in Proceedings of Computational Optical
Sensing and Imaging (COSI), Alexandria, VA, 2013.
[25]
——, “Hardware veriﬁcation of an ultra-miniature computational
diffractive imager,” in Proceedings of Computational Optical Sensing
and Imaging (COSI), Kohala Coast, HI, 2014.
[26]
R. L. Morrison, “Symmetries that simplify the design of spot array
phase gratings,” Journal of the Optical Society of America A, vol. 9,
no. 3, pp. 464–471, 1992.
[27]
P. R. Gill, “Odd-symmetry phase gratings produce optical nulls uniquely
insensitive to wavelength and depth,” Optics Letters, vol. 38, no. 12,
pp. 2074–2076, 2013.
[28]
R. Penrose and J. A. Todd, “On best approximate solutions of linear
matrix equations,” Mathematical Proceedings of the Cambridge Philo-
sophical Society, vol. 52, pp. 17–19, 1956.
[29]
D. G. Manolakis, V. K. Ingle, and S. M. Kogon, Statistical and adap-
tive signal processing: Spectral estimation, signal modeling, adaptive
ﬁltering and array processing.
Norwood, MA: Artech, 2005.
[30]
T. Hastie, R. Tibshirani, and J. Friedman, Elements of statistical
learning: Data mining, inference, and prediction.
New York, NY:
Springer, 2009.
[31]
D. A. Fish, A. M. Brinicombe, E. R. Pike, and J. G. Walker, “Blind
deconvolution by means of the Richardson-Lucy algorithm,” Journal of
the Optical Society of America A, vol. 12, no. 1, pp. 58–65, 1995.
[32]
D. G. Stork, “Joint optics/signal processing design for computational
diffractive sensing and imaging,” in Computational Optical Sensing and
Imaging (COSI), Kohala Coast, HI, 2014.
[33]
D. Tseng, O. Mudanyali, C. Oztoprak, S. O. Isikman, I. Sencan,
O. Yaglidere, and A. Ozcan, “Lensfree microscopy on a cellphone,”
Lab on a chip, vol. 14, pp. 1787–1792, 2010.
[34]
H. Chaouchi, Ed., The Internet of Things: Connecting objects.
New
York, NY: Wiley, 2010.
[35]
R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classiﬁcation, 2nd ed.
New York, NY: Wiley, 2001.
[36]
D. G. Stork and P. R. Gill, “Reading QR code symbols with an
ultra-miniature computational diffractive imager,” in Proceedings of
Computational Optical Sensing and Imaging (COSI), Kohala Coast, HI,
2014.
[37]
S. Z. Li and A. K. Jain, Eds., Handbook of face recognition.
New
York, NY: Springer, 2005.

