 
 
 
Abstract—The work presented in this paper is part of our 
investigation in the ROBOSKIN project. One key research 
activity in the project was to explore tactile interactions of 
children with autism with the humanoid robot KASPAR in 
order to develop methods and mechanism to support robot-
assisted therapy for children with autism. This article presents 
a detailed taxonomical classification of tactile interactions of 14 
children with autism with the humanoid robot KASPAR. Our 
quantitative analysis confirms results from the literature 
highlighting the great variety of autistic children’s interaction 
capabilities. 
 
 Keywords-assistive technology; human-robot interaction; 
autism therapy; robot assisted therapy. 
I. INTRODUCTION 
 Physical touch is one of the most basic, but at the same 
time very important forms of communication. Tactile 
sensing can help to provide awareness of one’s own self and 
each other. On the playground, touch and physical contact 
are used by children to give and receive support and 
encouragement, to build trust, to communicate and to 
develop their social relationship. In therapy, the tactile sense 
can be used individually to increase self knowledge, body 
image, to achieve sense of stability, and build confidence. 
Touch of another person, when it happened, is seen also as a 
way of breaking through isolation [1, 2].  
 However, for people with autism, in addition to their 
inabilities to relate to other people, show little use of eye 
contact, and have difficulty in verbal and non-verbal 
communication, impairments in tactile interaction prevent 
them even further from social interaction with other people. 
Some people with autism might be hyposensitive and seem 
not to feel pain. They may not sense their touch of other 
people or objects appropriately, which could lead them to   
unintentionally hurt other people, or break objects. Other 
people with autism might have a hyper-tactility condition 
which is very common and results in overwhelming 
sensation. As touch can be excruciating to people with this 
condition it leads to fear of being touched. This fear could be 
so great, that it may cause a panic attack [3, 4]. 
 On the other hand, tactile interaction (if tolerated) might 
be an important means of communication for children with 
autism, as some do not have verbal skills and others use their 
verbal skills inadequately. Caldwell [5] suggests that 
problems with verbal skills and eye gaze in children with 
 
 
autism create the need for touch to replace these detrimental 
ways of communicating. 
     We argue that a ‘tactile’ robot can be used as a ‘buffer’  
that mediates between a person with autism and another 
person, by providing indirect rather than direct human-
human contact, until such time that the person builds enough 
strength and confidence to tolerate direct human contact. 
A robot with tactile applications could allow a person with 
autism  to feel safe and  build their confidence in tactile 
interaction where they can  explore touch in a playful way 
that could be completely under their control. Also, while 
inappropriate tactile interaction with another person will 
automatically lead to negative feedback (e.g., when hitting 
another person), interaction with a robot can provide a non-
judgemental environment where the child can safely explore 
tactile interaction, in a long-term process that involves 
reflection and feedback given to the child about his or her 
actions. 
 Several observational systems and taxonomies can be 
found in the literature that were created to help clinicians 
and researchers making inferences about play, based on the 
observation of behaviours, e.g., Knox play scale [6], or 
Bundy’s Test of Playfulness [7].  Taxonomies of children’s 
behaviour during play provide criteria to guide observation 
of behaviours in narrower categories that may be easier to 
observe, describe and explain.  
However, play behaviours at their core are thought to 
involve experiential characteristics that are typically difficult 
to observe directly, such as intrinsic motivation, enjoyment 
and active engagement, and it is important to remember that 
taxonomies often rely on observable behaviours which often 
cannot capture fully the person’s subjective experience [8]. 
This becomes an even more important factor to consider 
when working with a population for whom communication 
skills are one of the main areas of impairment (such as the 
case of children with autism - many have limited or no 
language skills at all) and it is very difficult, if not 
impossible, to interview the person about their experience. 
In addition to impaired communication, atypical sensory 
processing, motor difficulties, and cognitive impairment are 
other very common characteristics of autism. As children 
with autism may manifest these symptoms to varying 
degrees, this results in an extremely heterogeneous 
population [8], which in turn, makes the task of developing a 
taxonomy and classification of typical tactile  behaviour of 
children with autism very difficult. Although children with 
Investigating Child-Robot Tactile Interactions: 
A taxonomical classification of tactile behaviour of children with autism towards 
a humanoid robot. 
 
Ben Robins, Farshid Amirabdollahian, Kerstin Dautenhahn 
School of Computer Science 
University of Hertfordshire 
Hatfield,  U.K 
{b.robins, k.dautenhahn, f.amirabdollahian2}@herts.ac.uk 
89
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
autism share the same core difficulties, each child displays 
these in an individual way [9]. A series of 14 interaction 
sessions that were conducted in a recent study with autistic 
children and the robot KASPAR were used as a basis for our 
taxonomical classification. The following sections describe 
the robotic platform used and the trials set up and procedures 
and continue with the taxonomical classification of tactile 
interaction followed by discussion and future plans. 
II. THE ROBOTIC PLATFORM - KASPAR 
KASPAR is a child-sized minimally expressive robot 
which acts as a platform for HRI studies, using mainly 
bodily expressions (movements of the hand, arms and facial 
expressions) and gestures to interact with a human (Figure 
1). The robot has a static body (torso, legs and hands were 
taken from a child-sized commercially available mannequin 
doll) with an 8 DOF head and two 3 DOF arms.   
 The face is made from a silicon rubber mask that covers 
an aluminum frame. It has 2 DOF eyes fitted with video 
cameras; eye lids that can open and shut and a mouth 
capable of opening and smiling. These features enable 
KASPAR  to  show minimally expressive emotional states 
such as happiness, neutral, sadness and surprise (Figure 2). 
It has several pre-programmed behaviours that include 
various facial expressions, hand waving and drumming on a 
toy tambourine that is placed on its legs. KASPAR’s 
movements are either controlled remotely through a remote 
control device, or it can operate autonomously.  
 For a complete description of KASPAR’s design 
rationale, hardware, and application examples see [10].  
III. TRAILS SET-UP AND PROCEDURES 
 The study, which involved 14 children with autism, was 
designed to provide essential observational data on 
children’s behaviour during child-robot interaction including  
spontaneous tactile interaction. 
 The trials took place in a special needs school for children 
with moderate learning difficulties in the UK. With the 
objective to provide a reassuring environment, the trials 
were designed to allow the children to have free and 
unconstrained interaction with the robot and with the present 
adult (i.e., teacher, experimenter) should they wish to. The 
trials were conducted in a familiar room often used by the 
children for various activities. Before the trials, the 
humanoid robot was placed on a table, connected to a laptop. 
The investigator was seated next to the table. The robot was 
operated remotely via a wireless remote control (a specially 
programmed keypad), either by the investigator or by the 
child (depending on the child’s ability). The children were 
brought to the room by their carer and the trials stopped  
 
 
Figure 1. The robotic platform KASPAR. 
 
Figure 2. Some of KASPAR’s facial expressions and expressive gestures. 
 
when the child indicated that they wanted to leave the room 
or if they became bored. Two stationary video cameras were 
used to record the trials. 
IV.  TAXONOMICAL CLASSIFICATION OF TACTILE 
INTERACTION 
 All interaction sessions of the children and the robot were 
video recorded from different angles. 14 sessions (one for 
each child) were used in building the taxonomical 
classification reported here. The resulting videos were 
analysed in a coding process that included watching the 
videos and manually identifying touch types, locations, 
durations and estimated pressure by observing video 
sequences and coding start, end or onset of events. This 
entailed complete coding of all 14 videos using the Observer 
software (Noldus). Table 1 shows the  tactile features that 
were observed during the interaction. In addition, the 
duration of these features were detected and calculated.  
A. Preliminary visual inspection of the coded events 
 In order to provide a more detailed classification and 
taxonomical breakdown, events coded in these videos were 
further analysed. Figure 3 shows a visual representation of the 
events detected in 4 of these videos. As can be seen, these 
four cases present a very different result in terms of type, 
duration, and frequency of the tactile events observed. 
 In order to gain a fuller picture of these variations, number 
and duration of these touch events were plotted versus one-
another (Figure 4 & Figure 5). As different participants 
interacted with the robot for different lengths of time, the 
number of tactile events are normalized based on the 
durations. Thus a further plot is included presenting the rate 
of occurrence for these tactile events (Figure 6). 
 
TABLE 1 
TACTILE FEATURES: TOUCH TYPES, PRESSURES AND 
LOCATIONS 
 
Touch 
type 
 
Touch 
Pressure 
 
Touch 
Location 
Grasp 
 
Light 
 
Left arm 
Touch 
 
Medium 
 
Left leg 
Stroke 
 
Tight 
 
Right arm 
Poke 
 
 
 
Right leg 
Pinch 
 
 
 
Both arms 
 
 
 
 
Both legs 
 
 
 
 
Head 
 
 
 
 
Torso 
 
90
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
 
 
Figure 3. Four examples of tactile events detected. 
 
  In order to further investigate the extent of these 
observed differences, coded events were analysed 
statistically. 
A. Statistical analysis of the coded events 
 Comparing the number of touch events as shown in Figure 
3 can be misleading as longer sessions can include a larger 
number of tactile events. The occurrence rate of tactile 
events is a compound variable that consists of the number of 
specific tactile events and the total duration of each session 
thus 
allowing 
for 
comparison 
based 
on 
rate 
(number/duration). This parameter was analysed statistically 
in order to highlight differences in between touch types and 
individual participants.  
 
 
Figure 4. Comparison between number of tactile events detected for 
different cases. 
 
Figure 5. Comparison between the duration of the tactile events detected 
from different cases. Please note that poke and pinch were point events and 
did not have a duration attribute. 
 
 At a first glance, the boxplot presented in Figure 7 shows 
the extent of these observed variations as presented by the 
‘rate per minute’ variable. As can be seen, the grasp and 
touch events present higher rates of touch and also higher 
variability while pinch, stroke and poke present lower rates, 
and less variability. 
 
1) General linear model (two-way ANOVA) 
In order to identify the extent of inter-event variations, as 
well as variations between each touch type, a two-way 
ANOVA model was constructed using the PASW statistical 
analysis package. This package uses the general linear model 
for multivariate analysis including the two-way ANOVA.  
Model variable was the ‘Rate per minute’, while factors 
were ‘touch type’ and ‘participant’. This allows for 
identifying differences between different touch types and  
 
 
Figure 6. Occurrence rate per minute for each tactile event for different 
cases. These include normalised number of events over the duration of each 
session in minute. 
 
91
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
Figure 7. Boxplot comparing rate per minute for different tactile events. 
 
also how participants performed under each type of tactile 
event. 
 Table 2 highlights that in addition to significant variations 
between the touch events identified, there were significant 
variations between different participants and their rate and 
type of touch.  
 Table 3 presents the parameter estimates and their 
influences on the general linear model. It shows that 
different parameters (intercept, participant 6 and touch type 
2) had most influenced the linear model. Such a model is a 
type of linear best-fit line for the data points and here the 
model tries to fit a line to those data points presented by 
touch type and for different participants. Significant and 
influential parameters are shaded rows while close to 
significant values are shown by shading the cell only. 
 It is interesting to note that participant 6 and touch type 2 
are presented as the most dominant features seen in Figure 5, 
while participants 8 and 13 present the least visible features 
in this figure, highlighted by the negative slope value 
(column B in Table 3).  
 
 
TABLE 2 
TOUCH TYPE AND PARTICIPANT VARIATIONS SHOWN BY A 
GENERAL LINEAR MODEL 
Tests of Model Effects 
Source 
Type III 
Wald Chi-
Square 
df 
Sig. 
(Intercept) 
53.819 
1 
.000 
Participant 
39.104 
13 
.000 
TouchType 
44.198 
4 
.000 
Dependent Variable: Rate Per Minute 
Model: (Intercept), Participant, TouchType 
 
 
 
 
TABLE 3 
PARAMETER ESTIMATION AND IDENTIFICATION OF 
SIGNIFICANT PARAMETERS 
 
Dependent Variable: Rate Per Minute 
Model: (Intercept), Participant, TouchType 
a. Set to zero because this parameter is redundant. 
b. Maximum likelihood estimate. 
 
2)  General linear model for tactile location and touch   
       intensity 
We further extended the analysis by considering variations 
of touch across different body parts of the robot 
shown by TABLE 1. This is a more complex statistical 
model, which would allow investigating if there were 
similarities for touch event rates for a specific body part and 
specific touch intensities. Such a model requires a more 
detailed coding and calculation of rates per body location, as 
well as data structuring to allow for a two-way ANOVA, 
done under the PASW general linear model. The model 
constructed here uses the ‘rate per minute’ as its variable 
while factorising ‘participant’, and ‘body part; intensity’. 
The last parameter identifies the body part touched and 
observed intensity of the touch type. The box plot presented 
by Figure 8 highlights these results.  
The general linear model once more identified significant 
differences in rate per minute between participants and 
‘touch event; intensity’, which is evident in the above 
boxplot (Table 4). These differences are highlighted by 
Figure 9 and TABLE 5. 
 Further results are provided by the parameter estimates 
(TABLE 5). This table shows that when modeling based on 
observed variations, participant 6 still presents a strong 
positive influence while participant 8 continues to provide a 
negative contribution towards a common fitted line. Other 
participants, participant 9 and 10, also provide significant 
contributions towards the trend. Touch events ‘ba; 1’ and 
‘head; 1’ both present significant influences. The ‘ba; 1’  
92
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
 
 
Figure 8. Rate of tactile events based on touch location and intensity. 
 
indicates slight touch on both robot arms and can be seen on 
the boxplot to present small variations in rate per minute, 
while the light head touch identified by ‘head; 1’ shows 
stronger variations, thus contributing to the positive slope as 
reflected by column B.  
 
A. Discussion of results 
 
  The visual inspection and statistical analysis of different 
observed touch types for different participants present a  
Versatile picture varying across touch types, intensities and 
different participants. This was predicted prior to the start of 
this study as touch events are shown to be different in their 
features, locations and for different participants. This is 
evident in the statistical results showing significant 
differences of touch type and participant levels. This poses a 
potential 
challenge 
to 
taxonomical 
classification 
as 
interactive scenarios with low functioning children with 
autism often feature free or less-structured interactions. For 
example, often it is difficult to enforce a certain type of 
touch for a certain duration and thus studies of the results 
from such interaction are bound to have large inter/intra  
 
TABLE 4 
TOUCH LOCATION AND PARTICIPANT VARIATIONS PRESENTED 
BY THE GENERAL LINEAR MODEL 
Tests of Model Effects 
Source 
Type III 
Wald Chi-
Square 
df 
Sig. 
(Intercept) 
18.887 
1 
.000 
Participant 
104.189 
13 
.000 
TouchEvent 
60.008 
19 
.000 
Dependent Variable: Rate per minute 
Model: (Intercept), Participant, TouchEvent; intensity 
 
 
Figure 9. Rate per minute for participant and tactile event; intensity. 
 
TABLE 5 
PARAMETER ESTIMATES FOR THE TOUCH PER MINUTE 
BASED ON PARTICIPANT AND TOUCH-TYPE; INTENSITY 
 
Dependable Variable: Rate per Minute;  
Model: (Intercept), Participant, TouchType 
a. Set to zero because this parameter is redundant 
b. Maximum Likelihood estimate 
 
participant variations. As shown by statistical results as well 
as plots presented, the touch rate per minute varies at all 
factor levels (participant, touch type or touch intensity). 
93
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
 
This is a true reflection of the captured behaviour and it is  
in-line with what is confirmed by the literature, i.e., that 
children with autism are an extremely heterogeneous 
population and although they share the same core 
difficulties, each child displays these in an individual way.  
 This conclusion is also supported by observation analysis 
of trials where children with autism interacted with the robot 
in free play context. Figure 10 shows examples of the variety 
of interaction styles from forceful poking and grabbing (two 
images on the left)  to gentle stroking (image on the right). 
 
 In addition, literature suggests that children with autism 
may demonstrate more interest in parts of objects rather than 
the object itself as a whole (e.g., wheels on a toy car rather 
than the car itself) [11]. Sometimes object manipulation is 
focused on self-stimulatory use of toys rather than 
exploration of the properties of the object [12]. 
These behaviours present additional challenges and must be 
considered when building autonomous robots for cognitive 
social interaction with children with autism. 
V. CONCLUSION AND FUTURE WORK 
 The paper presented  a taxonomical classification of 
tactile interactions of 14 children with autism with the 
humanoid robot KASPAR. It described the experiments 
which were designed to observe the tactile interaction of the 
children with the robot and record the location and type of 
these interactions. It than presented the statistical analysis 
results comparing the observed interaction in term of rate per 
minutes between different users and different type of tactile 
interaction. The statistical results presented in the paper 
showed significant differences across  touch type intensities 
and participants.  These results support the literature 
suggesting that children with autism are an extremely 
heterogeneous population and although they share the same 
core difficulties, each child displays these in individual 
ways. The case study examples discussed in the paper 
highlight the need and possible benefit for future  modular 
play scenarios and  adaptive robots that can be flexible in 
their operation, i.e., capable  of autonomous operation and 
response to interaction but at the same time allow operation 
via a remote control where a teacher/therapist could work 
with the child alongside the robot, triggering additional robot 
behaviors adapted to the individual needs of the children. 
 
 
  
 
 
Figure 10.  Forceful and gentle interaction of children with KASPAR. 
 
 
ACKNOWLEDGEMENT 
 This work has been partially supported by the European 
Commission under contract number FP7-231500-RoboSkin. 
 
REFERENCES 
[1] 
M. 
Costonis, 
Therapy 
In 
Motion. 
Urbana: 
University of Illinois Press, 1978. 
[2] 
P. 
Bernstein, 
Theoretical 
approaches 
in 
dance/movement therapy, I & II. Dubuque, Iowa: 
Kendall Hunt, 1986. 
[3] 
O. Bogdashina, Sensory perceptual issues in autism 
and 
Asperger 
Syndrome: 
different 
sensory 
experiences – different perceptual worlds. London: 
Jessica Kingsley Publishers, 2003. 
[4] 
G. Gillingham, Autism: Handle with Care: 
Understanding 
and 
Managing 
Behaviour 
of 
Children and Adults with Autism. Arlington: TX. 
Future Education Inc, 1995. 
[5] 
P. Caldwell, Getting in Touch: Ways of working 
with people with severe learning disabilities and 
extensive 
support 
needs. 
Brighton: 
Pavilion 
Publishing Ltd 1996. 
[6] 
S. Knox, "Development and current use of the 
revised Knox preschool play scale," in Play in 
occupational therapy for children, Diane Parham 
and L. Fazio, Eds.: Mosby - Elsevier, 2008, pp. 55-
70. 
[7] 
G. Skard and A. Bundy, "Test of Playfulness," in 
Play in Coccupational Therapy for Children, D. 
Parham and L. Fazio, Eds.: Mosby - Elsevier, 2008, 
pp. 71-93. 
[8] 
D. Parham and L. Fasio, Play in Occupational 
Therapy for Children. St. Louis: Mosby-Elsevier, 
2008. 
[9] 
V. Cumine, J. Leach, and G. Stevenson, Autism in 
the Early Years: A Practical Guide London: David 
Fulton Publishers, 2000. 
[10] 
K. Dautenhahn, C. L. Nehaniv, M. L. Walters, B. 
Robins, H. Kose-Bagci, N. Assif Mirza, and M. 
Blow, 
"KASPAR- 
A 
Minimally 
Expressive 
Humanoid Robot for Human-Robot Interaction 
Research," Special Issue on "Humanoid Robots", 
Applied Bionics and Biomechanics, vol. 6(3): 369-
397, 2009. 
[11] 
American-Psychiatric-Association, "Diagnostic and 
Statistical Manual of Mental Disorders DSM-IV," 
American Psychiatric Association, Washington 
DC., 1995. 
[12] 
M. Black, B. Freeman, and J. Montgomery, 
"Systematic observation of play behaviourin 
autistic children," Journal of Autism and Childhood 
Schizophrenia, vol. 5, pp. 363-371, 1975. 
94
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

