Assessment of Differences in Human Depth Understanding 
Between Stereo and Motion Parallax Cues in Light-Field Displays   
Raymond Swannack, Oky Dicky Ardiansyah Prima 
Graduate School of Software and Information Science 
Iwate Prefectural University 
152-52 Takizawa, Iwate, Japan 
e-mail: g231s501@s.iwate-pu.ac.jp, prima@iwate-pu.ac.jp
 
 
Abstract‚ÄîAs Three-Dimensional (3D) digital content has 
become widely recognized, virtual and augmented realities have 
received a lot of attention. The Light-Field display (LFD), which 
allows users to view stereoscopic images from multiple 
viewpoints at the same time, provides a new 3D experience. 
LFDs are complicated to set up, but this display has been made 
available for personal use. This study aims to evaluate the 
differences in task accomplishment between stereo versus 
motion-parallax cues for users performing 3D interactions on a 
multi-view display. Our task scenario involves user tests for 3D 
alignment accuracy and questionnaires about the experience 
during the test. For each task, 3D contents are presented in 
stereo and motion parallax cue presentation, respectively, using 
the LFD ‚ÄúLume Pad‚Äù developed by Leia Inc. Results on six 
subjects showed that task alignment could be achieved with 
greater 
accuracy 
when 
stereo 
cues 
were 
available. 
Questionnaires showed that depth perception appeared to be 
easier with stereo cues. Future work will include observing 
whether LFDs provide better 3D perception than current 
Virtual Reality (VR) devices. 
Keywords-3D, Light-Field Display; 3D human perception; 
motion-parallax; stereoscopic vision. 
I.  INTRODUCTION 
The human ability to view and understand three 
dimensions allows us to interact with the world in detail. 
Translating this to the digital field has not been an easy task. 
A Two-Dimensional (2D) screen does not have true depth to 
it, so the human eye does not interact with objects on a screen 
the same as it would with an object in the real world [1]. 
There are nine widely agreed upon sources of information 
that the brain uses for perceiving depth. They are as follows, 
binocular disparity, convergence, occlusion, relative size, 
height in the visual field, relative density, aerial perspective, 
accommodation, and motion parallax [2]. To a greater or 
lesser extent, they are used in conveying depth in a 2D screen, 
and it is the manipulation of these sources that forms the basis 
of 3D displays. There are many different types of 3D displays, 
each being varied to suit different tasks.  
VR headsets are Head Mounted Displays (HMDs) that 
have had the most exposure in popular culture. Popularity of 
products such as the HTC VIVE and Facebook‚Äôs Oculus are 
seeing use both in entertainment as well as scientific research. 
Augmented Reality (AR) headsets, such as HoloLens by 
Microsoft, are becoming more well-known as well. VR and 
head-mounted AR displays use similar concepts where they 
display slightly different images to each eye. Both rely heavily 
on binocular disparity to create stereopsis, as well as motion 
parallax. 
Hand-held displays are largely composed of smartphones 
and tablets. Some are specifically designed as AR devices 
while others have apps, such as Pok√©mon Go and IKEA Place, 
that use the inbuilt camera to give the appearance of projecting 
their scenes into the real world. These rely largely on height 
in the visual field for the user to believe that what they are 
looking at is real. 
Another form of 3D display is LFDs. An LFD uses 
lenticular lenses to bend the light coming out of the screen of 
the display, giving a different view to each eye. In this way, 
they work similarly to head mounted displays but perform this 
job without the need for a headset. 
The rest of the paper is structured as follows. In Section II, 
we present the details of the LFD hardware used in the 
experiment. Section III details the experiment methodology, 
such as the design concept as well as the software and 
hardware used. The results for the experiment and the 
questionnaire are given in Section IV. Finally, we conclude 
our work in Section V, with our conclusion and future work. 
 
II. LUME PAD 
This research was performed on the Lume Pad, the LFD 
tablet by Leia Inc. This allows users to see the illusion of depth 
inside of a 2D screen by showing each eye a different image, 
creating stereopsis. The tablet boasts a 10.1-inch screen with 
a resolution of 2560x1600 pixels. To create the light field 
effect, the tablet displays four views at a time and uses the 
lenticular lenses to allow the user to see two of these images 
at a time. This gives the user the best of both the HMD-typed 
VR set up, as well as the tablet-typed AR device. The Lume 
Pad displays what looks like a real 3D image to multiple users 
at one time. 
To generate the different views, the Lume Pad generates 
four views in 2x2 grids. Each image has a resolution of 
640x400 pixels but is displayed in such a way as to be 
perceived as having a higher resolution [3]. 
The device dissects the images and displays them under 
lenticular lenses so that the user sees different images with 
each eye. This achieves stereoscopy and gives the user the 
perception of depth within the 2D screen. 
 
18
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-982-9
ACHI 2022 : The Fifteenth International Conference on Advances in Computer-Human Interactions

III. EXPERIMENT 
In this study, the test was conducted using the Lume Pad 
for the display and a Sony Dual Sense controller for user input. 
Subjects were given control of an arrow and needed to aim it 
at a target that appears at a point above the arrow. This target 
is at a set y position and is offset by some distance in the x and 
z directions. The right control stick controls the arrow, 
allowing the user to rotate it to be facing the target to the best 
of their ability. The left stick controls the scene rotation, but it 
can only rotate the scene-camera around the center point on 
the horizontal axis. The right shoulder button zooms the 
camera in while the left zooms the camera out. The southern 
button is used to fire the arrow while the western button 
changes the display from stereo to motion parallax mode and 
back again. The button layout can be seen in Figure 1. 
Subjects were forced to focus on horizontal angles because 
the scene could only be rotated horizontally. Since the Lume 
Pad has four horizontal and two vertical views, the decision 
was made to put more focus on the horizontal axis. To further 
increase the difficulty of the test, a large rock is placed behind 
the arrow so that the subject must view the scene from 
different angles to understand the direction that the arrow is 
facing. There are five trees that are placed within the scene, so 
that some sight lines will be blocked. In addition, the target 
starts relatively close to the center of the scene, near the 
arrow's starting point, and gets further away with each 
attempt. 
Our experiment was designed using the Leia Unity 
Software Development Kit (SDK) [4]. The SDK allows to 
utilize the Lume Pad‚Äôs features, such as the special lenticular 
camera, as seen in Figure 2. Moreover, the screen can be 
switched from stereo to motion parallax mode with a press of 
a button. 
The experiment was carried out as follows. First, the 
subject is presented with a scene where the target is in random 
positions x and z and the trees are all in random positions. This 
allows the subject to learn the controls of the experiment and 
familiarize themselves with how to look around the scene. The 
results of this first attempt are ignored so the subject was 
encouraged to take their time and ask questions. After this 
practice, the subject was asked to perform three more 
attempts. These attempts will be the same for every user, with 
no random elements. The layouts for these attempts can be 
seen in Figure 3.  
Each subject was seated at a desk, positioned between 45-
50 cm from the tablet. This is the distance that Leia Inc states 
is the best viewing distance for the Lume Pad tablet. The tablet 
was then angled towards the subject‚Äôs face to maximize the 
light-field effect.  
For the experiment, there were 12 subjects (three females 
and nine males). They ranged in age from 22 to 31 with an 
average age of 25. All had normal or corrected-to-normal 
vision. They were split into two groups, one group would see 
the scene in motion parallax first, followed by the scene in 
stereo. This group did their practice attempt on the motion 
parallax version and then after their three actual attempts, the 
display was switched to stereo mode for the final three 
attempts. The second group was shown the stereo version for 
their practice attempt and first three actual attempts, followed 
by the motion parallax version for the final three attempts. 
Accuracy is measured based on Euclidean distance, with 
a lower score being desirable.  
ùê∑ùëñùë†ùë°ùëéùëõùëêùëí = ‚àö(ùë•1 ‚àí ùë•2)2 + (ùëß1 ‚àí ùëß2)2 
(1) 
Here, x1 and z1 correspond to the position of the target 
while x2 and z2 to the final location of the arrow. The unit of 
measure for the experiment is Unity units (m). 
 
 
 
Figure 1. Sony Dual Sense Controller - Button layout. 
 
Figure 2. Scene-Camera layout for optimal Lume Pad experience [3]. 
 
 
 
(a) Top view. 
 
(b) Side view. 
Figure 3. Scenes from different angles used in the experiment. 
 
 
19
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-982-9
ACHI 2022 : The Fifteenth International Conference on Advances in Computer-Human Interactions

IV. RESULTS 
Figure 4 shows the results of the experiments. Each 
subject is displayed on their own line, where attempts 1, 2, 
and 3 are in blue, red, and yellow, respectively. The direction 
of the arrows in the figure indicates the order of the 
experiment performed by the subject. Group 1 shows the 
results for the motion parallax attempts followed by the stereo 
attempts, while Group 2 shows the results for the stereo 
attempts followed by the motion parallax attempts. Group 1 
was more accurate than Group 2. There is an argument to be 
made that Group 1 learned the test during the motion parallax 
attempts so it made sense that they would do better at the 
second round of attempts, the stereo portion of the test. The 
fact that Group two was more accurate during the stereo 
attempts as well, even though it is marginal, disproves this at 
least to some level. 
Subjects were also given a questionnaire to ascertain how 
well they believed they had understood the test as well as how 
the test made them feel. The questions were as follows: 
a. How well do you feel that you understood the scene? Did 
you know where everything was? 
b. How confident were you in your aim? Did you think you 
would be close to the target? 
c. How much discomfort did you feel? Did your eyes hurt? 
Did you feel sick? 
d. Could you see the 3D effect? Do you feel a 3D sensation?  
Subjects were given the same questions to answer after 
both the stereo and motion parallax tests. They were asked to 
give an answer on a Likert-typed scale from 1 to 5, where 1 
was not at all or no and 5 was completely. On average 
subjects felt like they understood the scene more in the stereo 
mode as well as feeling more confident in their accuracy. 
Some users felt discomfort or sickness from the test, 
specifically in the stereo mode. More subjects from Group 1 
felt discomfort than in Group 2. One subject stated that they 
felt dizzy after switching from motion parallax to stereo 
mode. Another subject said they could not see the 3D effect, 
though they had very high accuracy and moved the camera 
around frequently in the tests to make sure they were as 
accurate as possible. 
 
V. CONCLUSION AND FUTURE WORK 
In this study, we examined how accurate a user could be, 
given some constraints, in a stereo and a motion parallax 
environment. While the subjects were more accurate with the 
stereo attempts, it is not conclusive that the human brain 
understands distance in the light field display more so than 
on a motion parallax display. 
To improve on this test, the next step will be to add eye 
tracking. We did not measure eye movement and eye 
vergence in this test. Both would give us a better 
 
(a) Group 1. 
 
(b) Group 2. 
Figure 4. Experiment results. 
 
 
Subject
Subject
Distance
Distance
Motion Parallax 3D
Stereo 3D
Subject
Subject
Distance
Distance
Stereo
Motion Parallax 3D
20
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-982-9
ACHI 2022 : The Fifteenth International Conference on Advances in Computer-Human Interactions

understanding of how well individuals understood the scene 
they were interacting with [5]. 
Another concept which we plan to investigate is a 
multiscreen display. The pCubee [6] display is a cube shaped 
display, created by the University of British Colombia, that 
utilizes trackers to alter the display for a single user‚Äôs 
perspective. Combining this with the light field effect should 
make for a very believable 3D display. 
A point to be weary of is how much this technology relies 
on stereoscopic vision. There is an overabundance of depth 
cues that can confuse the human brain [7]. It is unclear if this 
affected the subjects in this test. 
 
REFERENCES 
[1] K. Kato and O. D. A. Prima, ‚Äú3D Gaze Characteristics in 
Mixed-Reality Environment,‚Äù eTELEMED 2021 : The 
Thirteenth International Conference on EHealth, Telemedicine, 
and Social Medicine, IARIA, pp.11-15, 2021. 
[2] J. E. Cutting and P. M. Vishton, ‚ÄúPerceiving Layout and 
Knowing Distances : The Integration, Relative Potecney, and 
Contextual Use of Different Information about Depth,‚Äù Percept. 
Sp. Motion, 22(5), pp. 69-117, 1995. 
[3] 3D Lightfield Experience Platform, https://www.leiainc.com/ 
[Retrieved at May 2022] 
[4] SDK and Developer Resources, www.leiainc.com/sdk. 
[Retrieved at 10 June 2022] 
[5] E. Mlot, H. Bahmani, S. Wahl, and E. Kasneci, ‚Äú3D Gaze 
Estimation Using Eye Vergence,‚Äù Proceedings of the 9th 
International Joint Conference on Biomedical Engineering 
Systems and Technologies (BIOSTEC 2016), 5, pp. 125-131, 
2016. 
[6] Stavness, I., Lam, B., & Fels, S. (2010, April). pCubee: a 
perspective-corrected handheld cubic display. In Proceedings 
of the SIGCHI Conference on Human Factors in Computing 
Systems (pp. 1381-1390). 
[7] T. S. Murdison, G. Leclercq, P. Lef√®vre, and G. Blohm, 
‚ÄúMisperception of Motion in Depth Originates from an 
Incomplete Transformation of Retinal Signals,‚Äù Journal of 
vision 19(12), pp. 21-21, 2019. 
 
21
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-982-9
ACHI 2022 : The Fifteenth International Conference on Advances in Computer-Human Interactions

