The Usability State of Nine Public Self-Service Applications in Denmark
Jane Billestrup, Jan Stage, Anders Bruun
Institute of Computer Science
Aalborg University
Aalborg, Denmark
{jane,jans,bruun}@cs.aau.dk
Abstract—Empirical usability research have documented
usability problems in public websites and  self-service
applications.  This  paper  uses  data  from  usability
evaluations  of  nine  Danish  public  self-service
applications from six self-service areas by five different
self-service  providers,  to  examine  similarities  in  the
usability problems found across self-service applications.
The study found that the types of usability problems are
present  across  self-service  applications,  self-service
areas,  and  self-service  providers.  However,  it  is  also
found that the total number of problems have decreased
significantly in  the self-service  applications that were
usability evaluated in 2016. In this paper, we have shown
that though the amount of found usability problems is
significantly  lower,  three  types  of  usability  problems
were  present  in  both  old  and  new  self-service
applications. These general types were button placement,
attaching of files, and meaning of concepts.  
Keywords-Digitalisation;  Usability;  Usability  Evaluation;
Self-Service Applications; User-Centred Design
I.
 INTRODUCTION 
European countries are currently developing digital self-
service  applications  for  their  citizens.  These  efforts  are
being launched to improve citizens' services and to reduce
costs [7]. The Digital Economy and Society Index (DESI)
measures  the  level  of  digitalisation  in  EU  countries.
According to DESI, Denmark is one of the leading countries
in regards  to digitalisation [7].  In 2012, a  digitalisation
process was launched in Denmark, with the goal that by the
end of 2015, 80% of all communication between citizens
and the municipalities should be conducted digitally; this
also included digital public self-service applications [17]. In
this paper, applications and self-service applications refer to
digital forms used for applying for e.g., a new passport, this
activity was until recent times conducted on paper, but has
been digitalized in recent years.
Having public self-service applications does not mean
that citizens are necessarily willing to use these applications.
The  usage  depends  on  whether  citizens  find  these
applications easy to use [5] as poor design can prevent
citizens from using these websites [15]. 
In Denmark self-service applications are developed by
different companies, and several companies are developing
similar self-service applications and competing about selling
their applications to the municipalities 98 municipalities.
The citizens do not experience that  the applications are
developed  by  different  self-service  providers  as  all
applications follow the same design style guide, though the
content and layout vary between the different companies.
To support the Danish initiative, the joint IT organisation
of the municipalities in Denmark (KOMBIT) developed two
sets of user-centred guidance materials in 2012 and early
2013, to support self-service providers in developing user-
friendly self-service applications [2]. Similar initiatives have
been  taken  in  countries,  such  as  the  United  States, the
United Kingdom, and South Africa [18] [22]. 
In this paper, we analyse the usability problems found
across self-service applications and self-service providers to
find commonalities in the usability problems. The purpose
was to ascertain if the usability of self-service applications
has  been  improved  and  if  there  are  general  usability
problems across self-service applications. The categories of
usability  problems  identified  here  have  previously  been
published. The purpose and the content of this paper differs
from [21]. 
In  the  following  section,  we  present  the  method  of
collecting and analysing the data for this study, Section III
describes the findings. Section IV discusses these findings,
and Section V presents the conclusion.
II.
METHOD
For  this  study,  we  use  lists  of  usability  problems
gathered  from  usability evaluations of nine Danish self-
service applications developed by five self-service providers
for six different self-service areas. These evaluations were
conducted between 2010 and 2016 [1] [3] [11]. 
A.
Case Companies
This  study  includes  three  of  the  largest  and  most
experienced companies in regards to developing public self-
service  applications  in  Denmark,  one  medium  sized
experienced company, and one small company with little
experience in developing public self-service applications.
Table  1  shows  the  year  the  usability  evaluations  were
conducted and the relation between companies and self-
service applications. 
Usability evaluations were conducted of nine different
public  self-service  applications  from  five  different  self-
265
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

service providers, in six different self-service areas. The
table also shows that the applications were evaluated in four
usability evaluations. 
TABLE I. 
SHOWS THE RELATION BETWEEN COMPANIES AND SELF-
SERVICE APPLICATIONS AND NUMBER OF USABILITY PROBLEMS FOUND
Year of
Evaluati
on
Tested Self-Service
Solutions
Number of
Test
Persons
Company and
Usability Problems 
Total number
of usability
problems
2010
Building permits
10
Company E  7/26/38  +4
75
2014
Assistive technologies
8
Company A  2/17/17
Company B  5/18/14
Company C  0/11/6
Company D  1/15/13
36
37
17
29
2014
Marriage certificates
4
Company B  1/3/2
6
2016
Address change
Rent subsidy
Medical practitioner
6
Company E  2/3/0
Company D  1/3/3
Company C  2/4/0
5
7
6
B.
Self-Service Applications
This section provides a description of each self-service
area included in this study.
1)
Building Permits (2010)
The self-service application for building permits is used 
when citizens apply for conducting construction work where
a building permit is needed, such as building a garage. The
evaluated  building  application  was  a  digitalised  paper
application developed by company E. The application was
developed before the approach of user-centred design was
introduced  in  the  development  of  public  self-service
applications.
2)
Assistive Technologies (2014)
The self-service application for procurement of assistive
technologies is used if a citizen needs to apply for assistive
technologies, such as a hearing aid. These applications were
developed just after the introduction of user-centred design
in public self-service applications by companies A, B, C and
D. 
3)
Marriage Certificates (2014)
The self-service application for marriage certificates is filled
out by citizen's wanting to get married either in a church or
by having a registry-office wedding. This application was
developed just after the introduction of user-centred design
in public self-service applications by company B.
4)
Address Change (2016)
The self-service application for an address change is used
when citizens are moving to a new address. This application
was developed more than two years after the introduction of
user-centred design in public self-service applications. This
self-service application was developed by company E. 
5)
Rent Subsidy (2016)
The  self-service  application  for  rent  subsidy  is  used  if
citizens  have  a  low  income  and  live  in  rented
accommodation. This application was developed more than
two years after the introduction of user-centred design in
public self-service applications by company D.
6)
Medical Practitioner (2016)
The  self-service  application  for  changing  medical
practitioner is used if a citizen wants to change to another
medical practitioner.  This application was developed more
than two years after the introduction of user-centred design
in public self-service applications by company C. 
C.
Usability Evaluations
All usability evaluations were conducted as think-aloud
evaluations on a PC in a Chrome browser. The building
application, assistive technology applications, and marriage
certificate  application  were  conducted  in  a  usability
laboratory. The applications of address change, rent subsidy
and medical practitioner were conducted at a student café.
1)
Test Persons
In  regards  to  the  number  of  test  persons  for  each
evaluation, we are aware that the correct number of test
persons  has  been  discussed  extensively  in  the  research
community  e.g.,  [6]  [10]  [13]  [16]  [20].   All  usability
evaluations in this study were conducted with between four
and ten test persons. All test persons in each test received
the same instructions and the same tasks. The tasks were
scenario  based  and  were  tasks  a  user  would  typically
complete in these systems. 
For  the  building  application,  the  evaluation  was
conducted in 2010 with ten test persons. All test persons
were  experienced  in  conducting  “do-it-yourself”  (DIY)
work. Their DIY experiences varied; two had only painted
their homes, and eight had either restored parts or all of their
homes.  Experience with filling out online forms for the
municipality varied from none to a few times for eight of the
participants.  The  two  remaining  participants  were  more
experienced and had filled out forms for the municipality
more than ten times. 
For  the  procurement  of  assistive  technologies,  the
evaluation was conducted in 2014 with eight test persons.
The  four  different  self-service  applications  in  this  self-
service  area  were  usability  evaluated  during  the  same
usability evaluation, where the four applications were given
to the users in a different order to even out bias. Seven
participants  had  experience  with  filling  out  public
applications, of these, three had experience with public self-
service applications; of these three, two had experience with
the public self-service application for assistive technologies.
For ordering of a marriage certificate, the evaluation was
conducted in 2014 with four test persons. Three test persons
had experience with public self-service applications, though
neither with this particular application. 
The three self-service applications for address change,
rent  subsidy,  and  changing  medical  practitioner  were
usability evaluated in one evaluation with the same test-
persons in 2016 with six test persons. All users had filled
out a self-service application for an address change in the
past; four participants had used the self-service application
for a rent subsidy and changing medical practitioner before
this usability evaluation.
266
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

2)
Data Analysis from Usability Evaluations
All data were analysed using the instant data analysis
(IDA) method [12] by researchers. All usability problems
were categorised as either critical, serious or cosmetic, in
regards  to  levels  of  confusion  and  frustration  of  the
participants, and whether they were able to fill out the forms
correctly [14].  
3)
Results of Usability Evaluations
All results were documented in a list describing and
categorising each usability problem. At least two people
took part in the characterisation of the problems. The lists of
usability problems across all nine self-service applications
consisted of a total of 218 usability problems (21 critical,
100 serious, 93 cosmetic, and four uncategorised problems);
no usability problems were removed prior to the analysis.
The distribution of the usability problems across self-service
applications,  self-service  providers  and  severity  can  be
found in Table 1.  The +4 by company E in the building
application means that there were two problems where the
severity was uncategorised as this categorisation required a
deeper knowledge of the domain than the researchers had
acquired; it was left to the case workers to conduct the
classifications of these four usability problems.
D.
Data Analysis
The  usability  problems  were  analysed  using  a
descriptive coding as described by Saldana [19].  All 218
problems found across the lists of usability problems were
coded  in  regards  to  the  character  of  the  problem.  The
descriptive coding provided us with a list of three categories
after removing all specific problems only found in one self-
service application. Subsequently, all the problems in each
category were discussed between two researchers to validate
the categories and ascertain if the problems in each category
were comparable across self-service applications and self-
service  providers.  All  problems  not  directly  comparable
were removed, leaving only the problems appearing across
self-service applications. The three categories were named:
button  placement,  attachment  of  files,  and  meaning  of
concepts.
III.
FINDINGS
In  this  section,  we  present  the  findings  from  the
categories  button  placement,  attachment  of  files,  and
meaning of concepts, respectively. We also compare the
results  of  the  usability  evaluations  between  applications
from each self-service provider.
A.
Button Placement
Usability problems in relations to button placement were
found in four of the six self-service areas. It was mainly the
placement  of  the  “next-button”  that  confused  the  test
persons.
In the building application, the buttons were placed at
the top of the application, which made the test persons
overlook the buttons, as this placement made these buttons
difficult for them to find. Similar problems were found in
two  assistive  technology  self-service  applications.  The
“next” button was hidden until the test person scrolled down
to the bottom of the page, in the self-service applications for
address change, and changing  medical practitioner. It was
later discovered that the buttons were only hidden in some
Internet  browsers.  Subsequently,  four  different  browsers
have been checked: Chrome, Firefox, Safari and Internet
Explorer.  In  Chrome  the  “next”  button  was  hidden,  in
Firefox  the  button  was  partly  visible  and  in  Internet
Explorer and Safari, the button was fully visible. The only
indication of hidden buttons was the scrollbar located on the
left side, which some test persons missed. 
B.
Attachment of Files
In the applications for building permits, rent subsidy, and
changing  medical practitioner, some test persons did not
understand how to attach a file. When the test persons had
chosen a file to attach, they did not understand that they
then had to press the “attach” button to get the file attached.
Instead, some test persons clicked the “next” button, which
meant that the document did not get attached. 
When trying to attach files, the test persons in both the
building application and two of the assistive technologies
applications  had  difficulties  seeing  that  a  file  had  been
attached.  When  a  test  person  experienced  problems
understanding how to attach a file they tried to follow the
guidelines; however, these were constructed to be browser
specific and did not match the actual flow in the Chrome
browser used for the usability evaluations.
C.
Meaning of Concepts
Meaning of concepts is used as a broad categorisation of
problems  in  regards  to  what  the  users  read  in  the
applications. This category covers wording, and term users
do not understand, consequences of a conducting a specific
action, like clicking yes or no, and unclear use of language,
meaning that users do not understand what is expected of
them. 
The test persons experienced  problems understanding
the wording and terms used in the applications for building
permits and all four applications for assistive technologies.
The test persons had insecurities about clicking yes or no, as
the consequences of choosing one or the other were not
clearly stated, e.g., the test persons had to decide whether
the municipality was allowed access to their medical file,
but neither an explanation as to why or the consequences of
choosing no was stated if they chose not to allow access. 
In one of the applications for assistive technologies, the
test persons had to click either yes or no in a radio button to
the  question  “Do  you  consent  to  this?”  The  wording
confused the test persons as they became insecure about
what “this” meant. In the marriage application, one section
had to be filled out by both parties which confused the test
persons as the wording made them believe both parties had
to be present in the same room to do that, which was not the
267
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

case.  In the application for address  change, several  test
persons did not understand when to use a power of attorney,
or what to use it for.
IV.
DISCUSSION
In this section, we discuss our findings. Each of the three
categories  of usability problems described in the results
section is compared to Nielsen's ten usability heuristics.
A.
Comparison of Applications From One Self-service 
Provider
We  have  only  evaluated  one  self-service  application
from company A, and both evaluations of the self-service
applications from company B was conducted in 2014, which
means that neither of these would be interesting to compare.
Self-service  applications  from  company  C  and  D  were
evaluated in 2014 and 2016, and self-service applications
from company E were evaluated in 2010 and 2016. In this
section, we will compare the number of usability problems
compared  to  the  type  of  applications  and  describe  the
tendencies  in  regards  to  the  design  of  self-service
applications. 
1)
Company C
In  2014  the  application  for  procurement  of assistive
technologies  was  usability  evaluated.  This  evaluation
provided 17 usability problems in total, of which none were
categorised as critical, 11 were categorised as serious, and
six were categorised as cosmetic problems.  This application
was developed as a digitalized paper application and was
part of a larger healthcare system. 
The application looked identical to a paper application
citizens used to fill out by hand, both in terms of format and
design. The application was developed to provide the basic
information  the  caseworkers  needed  to  handle  the
application and did not intend to ease the workload of the
caseworkers or to make the application process easier for
the citizens [1]. 
In  2016  their  application  for  changing  medical
practitioner was evaluated. This usability evaluation showed
six problems in total, of which two were critical, and four
were serious, and none were cosmetic. The application from
2016 was developed as a wizard and not as a digitalized
paper application.
A segment of the application from 2014 is shown in
Figure 1 on the top, and the application from 2016 is shown
at the bottom.
Figure 1. Shows parts of both applications from company C
2)
Company D
In 2014 the application for procurement  of assistive
technologies  was  usability  evaluated.  That  evaluation
provided 29 usability problems in total, of which one were
categorised as critical, 15 were categorised as serious, and
13 were categorised as cosmetic problems. This application
was developed as a digitalized paper application and was
part of a larger healthcare system. In 2016 the application
for rent subsidy was evaluated; this evaluation identified
seven  problems.  Of  these,  one  usability  problems  was
categorised  as  critical,  three  as  serious,  and  three  was
classified as cosmetic usability problems.
The application from 2016 was developed as a wizard
and not as a digitalized paper application. Segments of both
applications are shown in  Figure  2. The application from
2014 is shown at the top and the application from 2016 is
shown at the bottom of Figure 2.
Figure 2. Shows parts of both applications from company D
The application from 2014 is shown to the left, and the
application from 2016 is shown to the right.
3)
Company E
In  2010  the  application  for  applying  for  a  building
permit was usability evaluated. That evaluation provided 75
usability problems in total, of which seven were categorised
as critical, 26 were categorised as serious, and 28 were
categorised  as  cosmetic  problems.  This  application  was
developed as a digitalized paper application.
In  2016  the  application  for  changing  address  was
evaluated; this evaluation identified five problems. Of these
two  were  critical,  three  were  serious,  and  none  were
classified as cosmetic usability problems.
 The application from 2016 was developed as a wizard
and not as a digitalized paper application. Segments of both
applications are shown in Figure 3.  
Figure 3. Shows parts of both applications from company E
268
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

The application from 2010 is shown on the top, and the
application from 2016 is shown on the buttom of Figure 3.
B.
Button Placement
In the building application, the “next” button was placed
counter-intuitive  to  the  test  persons.  However,  most
problems  in  regards  to  button  placement  were  present
because the self-service applications were not optimised to
different browser types. 
Geminus Ranking logs Internet activity in Denmark to
give access to statistical data about technology and Internet
usage. According to Geminus, Chrome is the most used
Internet  browser  on  computers  in  Denmark  [8].   This
indicates  that  a  large  amount  of  Danish  citizens  would
experience  hidden  “next”  buttons,  which  could  lead  to
confused and annoyed citizens who might not be interested
in using self-service applications[5] [15].
C.
Attachment of Files
Our results showed two types of problems in regards to
file attachments. One type of problem was test persons not
understanding how to attach a file; they pressed the “next”
button instead of the “attach” button. Some tried to follow
the guidelines in the self-service application. However, the
guidelines were optimised for another browser, meaning that
the steps did not fit.
A citizen  experiencing  these  types  of  problems  will
likely lead to their inability to correctly attach a file. This
means that they will either need to ask for help or send an
application  that  might  be  incomplete.  If  they  press  the
wrong  button,  they  may  not  even  be  aware  that  their
application is incomplete.
The other problem type is users not seeing when a file
has  been  attached,  which  shows  that  the  relevant
information was either too small, or too much information
was on the screen meaning that there was too little focus on
the essentials, leading to users not noticing when a file had
been attached. 
D.
Meaning of Concepts
Test persons experiencing problems with understanding
meaning of concepts were found in all the evaluated self-
service applications. The wording used was mainly directed
at professionals or people with some amount of domain
knowledge,  and  was  not  necessarily  understandable  for
citizens. Or, the language was simply unclear. This problem
made  some  test  persons  confused  and  afraid  to  make
mistakes; as a result, some test persons stopped for a longer
period, trying to figure out the consequences of choosing
one option over the other.  Several test persons stated that
they would have given up and contacted the municipality by
phone if this was not a test and they experienced this kind of
doubt when filling out a public self-service application. 
E.
Usability Problems Across Self-Service Providers and 
Year of Evaluations
Table  2  shows  a  decrease  of  the  numbers  of  found
usability problems between 2014 and 2016 for company C
and D, and between 2010 and 2016 for company E. 
TABLE II. 
SHOWS THE NUMBER OF FOUND USABILITY PROBLEMS
FROM EACH SELF-SERVICE PROVIDER AND USABILITY EVALUATION
Company
Year
Critical
Serious
Cosmetic
Uncategorised
Total
number of
usability
problems
C
2014
2016
0
2
11
4
6
17
6
D
2014
2016
1
1
15
3
13
3
29
7
E
2010
2016
7
2
26
3
38
0
4
75
5
Billestrup  et.  al.  found  that  the  Danish  self-service
applications  for  procurement  of  assistive  technologies,
which were usability evaluated in 2014, were not developed
with a user-centred  approach,  though this approach  had
officially been implemented as guidelines by the joint IT
organisation of the municipalities during this period [1]. 
As  the  number  of  found  usability  problems  have
dropped significantly between the evaluations conducted in
2010 and 2014 to the ones conducted in 2016, this indicates
that  some  improvements  have  been  made;  this  could
indicate  that  a  more  user-centred  approach  has  been
enforced by companies during this period or simply that the
evaluated applications from 2016 have been developed as
wizards, designed to help the users. Also. In 2014 it was
decided that all new public self-service applications should
be usability evaluated which itself also could have had an
impact [4] as usability evaluations might have caught some
issues before the citizens had to use these applications. A
decreasing number of usability problems could also indicate
that citizens have increased their understanding of using
public self-service applications over the past few years.
V.
CONCLUSION
In this paper, we analysed the usability problems across
self-service  applications  and  self-service  providers.  The
purpose of this study was to gain a greater understanding of
the  broader  usability  issues  in  public  self-service
applications.  Our  results  show  three  types  of  usability
problems found across self-service applications and self-
service providers. 
We  have  shown  that  public  self-service  applications
need to be optimised for different browsers as this otherwise
can lead to usability problems for the users. This should also
include  optimisation  for  different  technologies  such  as
tablets and smartphones, as Geminous rankings show that
56% of Internet usage in Denmark is not conducted from a
computer but other devices, e.g., smartphones and tablets
[9].  
269
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

The first evaluation was conducted in 2010, two years
before the user-centred design approach was implemented
in the development of public self-service applications in
Denmark. In 2012 the user-centred design approach was
implemented, meaning that the systems usability evaluated
in 2014 and 2016 were evaluated after the introduction of a
user-centred focus. Though Billestrup et al. found that this
was not the case in with self-service applications developed
in 2014 [1].
This study showed that the number of problems has
decreased since the introduction of the user-centred design
approach, and we have shown an indication of more usable
self-service  applications,  as  the  number  of  usability
problems was significantly lower in 2016. However, we
have also shown that though the amount of found usability
problems was significantly lower, three types of usability
problems were present in both old and new self-service
applications. 
This means that the approach taken by focusing on a
user-centred  approach,  using  wizards,  and  conducting
usability evaluations has not been sufficient in eliminating
some  general  and  reoccurring  usability  problems  found
across self-service applications, self-service providers and
self-service areas. 
A.
Limitations and Future Work
This study is limited to a single country by its focus on
the Danish  self-service  applications and problems  found
across these self-service applications. As for future work, it
would be interesting to compare our findings to similar
studies from other countries. 
Another limitation is that the lists of usability problems
we have analysed for this study did not state how many test
persons experienced each of the listed problems. 
We  are  aware  that  many  unknown  factors  could
implicate the changes in the self-service applications besides
the  companies  using  a  more  user-centred  development
approach  these  unknown  factors  should  be  investigated
further.
REFERENCES
[1]
J. Billestrup, M. Larusdottir, and J. Stage, “A Case Study of
Four IT Companies Developing Usable Public Digital Self-
Service Solutions”. In proceedings the Ninth International
Conference on Advances in Computer-Human Interactions,
ACHI, 2016.
[2]
J. Billestrup, a. Bruun, and J. Stage, “UX Requirements to
Public  Systems  for  All:  Formalisation  or  Innovation”.
Proceedings of  INTERACT 15th IFIP TC. 13 International
Conference  on  Human-Computer  Interaction,  Vol.  22,  p.
2015407, 2015.
[3]
A. Bruun, J. J. Jensen, M. Skov, and J. Stage, “Usability
Evaluation  of  an  electronic  building  application”.  White
paper, 2010. 
[4]
User  Evaluation  of  public  self-service  evaluations,  2016.
http://www.kl.dk/Administration-og-digitalisering/
Brugertest-bidrager-til-oget-brugervenlighed-id211070/
[retrieved: January, 2017].
[5]
T.  Clemmensen  and  D.  Katre,  “Adapting  e-gov  usability
evaluation to cultural contexts”. In proceedings of Usability
of e-government systems, 2012. 
[6]
D. A. Caulton,  “Relaxing the homogeneity assumption in
usability testing”. Behaviour & Information Technology, vol.
20 no. 1, pp. 1-7, 2001.
[7]
DESI  Index,  2016.
 https://ec.europa.eu/digitalagenda
/en/scoreboard/ [retrieved: January, 2017].
[8]
Geminus
 
Rankings,
 
browsers,
 
2016.
 http://
rankings.dk/en/rankings/web-browsersgroups .html [retrieved:
January, 2017].
[9]
Geminous  Rankings,  devices,  2016.  http://rankings.dk/en/
rankings/pc-vs-nonpc.html [retrieved: January, 2017].
[10] W. Hwang and G. Salvendy, “Number of people required for
usability evaluation: The 10±2 rule”. Commun. ACM, vol. 53,
no. 5, pp. 130–133, 2010.
[11] K.B. Jørgensen and M. L. Stentoft, Usability Analysis of
Public Self-Service Applications. Student report, 2016.
[12] J.  Kjeldskov,  M.  B.  Skov,  and  J.  Stage,  “Instant  Data
Analysis:  Conducting  Usability  Evaluations  in  a  Day”.
Proceedings  of  the  third  Nordic  conference  on  Human-
computer interaction, NordiCHI, ACM,  pp. 233-240, 2004.
[13] E. L-C. Law, and E. Hvannberg, “Analysis of combinatorial
user  effect  in  international  usability test”.  Proceedings  of
CHI, pp. 9-16, 2004.
[14] R.  Molich,  “User-Friendly  Web  Design” (in  Danish).
Ingeniøren Books, Copenhagen, 2nd edition, 2003.
[15] S.  Wangpipatwong,  W.  Chutimaskul,  and  B.  Papastratorn,
“Understanding  citizen's  continuance  intention  to  use  e-
government  website:  A  composite  view  of  technology
acceptance  model  and  computer  self-efficiency”.  The
Electronic Journal of e-Government, vol. 6, no. 1,  2008.
[16] J.  Nielsen,  “10  Usability  Heuristics  for  User  Interface
Design”. Fremont: Nielsen Norman Group, 1995. 
[17] Digital  applications.  Organisation  of  the  Municipalities  in
Denmark,
 
2012.
 http://www.kl.dk/Administration-og-
digitalisering /Lov-om-obligatorisk-digital-selvbetjening-og-
di gital-post-er-vedtaget-id105354/ [retrieved: January, 2017]
[18] M. C. Pretorius and A. P. Calitz, “The South African user
experience maturity status for website design in provincial
governments”. Proceedings of the 12th European Conference
on eGovernment, ESADE, pP. 589-599, 2012. 
[19] J. Saldaña, “The coding manual for qualitative researchers”.
Sage,  2015.
[20] M. Schmettow, “Sample Size in Usability Studies”. Commun.
ACM vol. 55, no. 4, pp. 64–70, 2012.
[21] J. Billestrup, N.  Bornø, A. Bruun, and J. Stage, “Usability
problems found across public self-service applications and
self-service providers”. In: Proceedings of OzCHI. ACM, pp.
623-625,  2016
[22] B. Soufi, and M. Maguire, “Achieving usability within e-
government websites illustrated by a case study evaluation”.
In Symposium on Human Interface and the Management of
Information, Springer-Verlag, pp. 777-784, 2007.
270
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

