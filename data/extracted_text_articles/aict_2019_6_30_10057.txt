Implementation of Machine-Based Learning Solutions in Distance Education for
Pathologists in Ophthalmic Oncology
Denis Garri
A.I. Yevdokimov Moscow State
University of Medicine and
Dentistry
Moscow, Russia
Email: ldenisl@inbox.ru
Svetlana Saakyan, Inna
Khoroshilova-Maslova,
Alexander Tsygankov
Helmholtz Moscow Research
Institute of Eye Diseases
Moscow, Russia
Email: svsaakyan@yandex.ru
horoshilova@yandex.ru
alextsygankov1986@yandex.ru
Oleg Nikitin, Grigory Tarasov
Limited Liability Company
"Artificial Networks and
Technologies"
Moscow, Russia
Email: olegnikitin@mail.ru
grigoriy.tarasov.u@gmail.com
Abstract – Uveal melanoma is a malignant tumor originating
from melanocytes of an eye vascular tract. Depending on the
cellular composition, the tumor is classified as a spindle cell (A
or B), epithelioid cell or mixed cell. The presence of epithelioid
cells reflects an unfavorable vital prognosis. The study of the
cellular composition of the tumor is subjective and results in
disagreements about the type of individual cells in 13% of
cases among qualified pathologists. The discrepancies in
diagnoses are due to the use of different classifications, which
can lead to an incorrect assessment of the vital prognosis and
incorrect tactics of patient treatment. Machine learning can be
used to objectify the criteria of pathomorphological study of
uveal melanoma, but currently there are no published works
on machine analysis of pathomorphological images of this type
of tumors. Our solution is based on the use of conventional
neural network for the classification of images of uveal
melanoma cells. We obtained an average F-score value of 0.75
to differentiate spindle cells nuclei from epithelioid cells nuclei
and developed a visualization interface to explain differences
between various types of cells with color mark-up of cell nuclei,
probability of belonging to a certain class and deconvolution
maps.
Keywords-E-learning; artificial neural networks; pathology;
uveal melanoma.
I.
INTRODUCTION
Uveal
melanoma
is
the
most
common
primary
intraocular malignant tumor in the adult population [1].
Tumor cell dissemination is a frequent occurrence with
uveal melanomas. Even with complete removal of the
primary tumor, metastatic foci are detected in 50% of
patients. In case of tumor metastasis, the vital prognosis is
substantially worse, with the average survival rate during
the first year falling to 20% [2].
The tumor cell type is an important prognostic factor.
The McLean classification is used currently, including
spindle-A, spindle-B, epithelioid and mixed tumors [3].
Studies have shown that spindle cell tumors offer the best
vital prognoses, while for mixed cell tumors the outlook is
intermediate, and epithelioid cell tumors present the most
unfavorable prospects. A greater number of epithelioid cells
in the field of view is associated with a worse vital
prognosis [4]. The morphological characteristic of the tumor
composition is subjective, and the quantity of epithelioid
cells required to identify tumors as epithelioid or mixed type
has not yet been universally defined. Disagreements among
qualified pathologists regarding belonging of individual
melanoma cells to a certain type are on average 13%, which
is due to the lack of objectively measurable signs and the
presence of intermediate-type cells that have signs of
several cell types. McLean and co-authors found that
differences in the classifications used led to differences in
diagnosis in 32% of cases [3]. Machine learning can be used
to objectify the criteria of pathomorphological study of
uveal melanoma.
The article is organized as follows. In Section II, we
present the state of art concerning machine learning use in
digital pathology. Section III discusses our training set,
method specification and performance metrics. In Section
IV, the use of the trained network is discussed. Finally, the
paper is concluded in Section V.
II.
STATE OF ART
A.
Machine Learning
Machine learning is applied in every field of human
activity where digital data is used. Various articles have
been published recently concerning the use of artificial
intelligence for the purposes of classification, regression and
segmentation in medicine and particularly in pathology.
Machine learning and deep learning are self-learning
methods used to analyze complex data and find patterns and
interdependencies without explicit programming. Due to
this, they are sometimes called "artificial intelligence".
Machine learning includes models and algorithms that
mimic the architecture of biological neural networks.
Artificial neural networks are of great interest in the field of
111
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-727-6
AICT 2019 : The Fifteenth Advanced International Conference on Telecommunications

machine learning, particularly networks based on deep
learning.
This
is
due
to
their
capacity
for
working
effectively with complex and multidimensional databases,
along with the increasing availability of databases and the
performance of graphics processors.
B.
Digital Pathology
Recent developments in the field of digital pathology,
related to the access of medical institutions to digital
microscopes and slide scanners, allow us to carry out
scientific work with digital data, including gigapixel images
of pathological specimens. The availability of such data
allows the use of a range of machine learning methods to
process it and to obtain new unified diagnostic criteria and
prognoses for the passage of malignant diseases that are
unavailable in a classical pathology study [5]. Recent
studies have shown that convolutional neural networks
reveal a high accuracy in the identification of pathological
images of certain types of cancer, including the pathology of
the prostate gland, lung, mammary gland, large intestine,
and ovaries [6]-[10]. The unit of learning is usually a small
image of about 100x100 microns or larger. This approach is
convenient to identify tissue patterns in the images under
study and can be used to determine the predominant cell
population in the image, but to characterize individual cells,
the size of one image should be comparable to the size of
one cell – 10-20 microns.
No published articles devoted to the machine analysis of
pathomorphological
images
of
uveal
melanoma
are
available at the moment. The articles that are the closest to
our work in terms of purpose and methodology are devoted
to the study of images of melanoma of the skin, which,
despite the similarity of origin, has a different metastatic
potential, responds differently to treatment and has different
immunological and genetic characteristics. The primary
tumor focus of melanoma of the skin lies in the depth of the
tissues of epidermal origin, and uveal melanoma – in the
tissues of mesodermal origin, which results in their different
histological characteristics [11].
Effland
et
al.
provided
variational
networks
to
differentiate tumor nuclei from the nuclei of immune cells.
Tissue samples were tinted with immunofluorescent dyes,
giving a different color signal while interacting with CD45
antigen of immune cells, gp100 protein antigen of tumor
cells and adenine–thymine rich regions in nuclei, which
allowed to form a training sample without using a manual
marking process [12]. In [13], Rexhepaj et al. used Melan-A
dye for immune staining of melanoma cells; they created
training samples of tumor and non-tumor cells and analysed
them using support vector machine. This experiment can not
be used to differentiate different types of uveal melanoma
cells due to the fact that there are no dyes specifically
staining epithelioid or spindle cells.
Liu et al. used SetSVM – support vector machine
modification – to solve a number of diagnostic problems, in
particular, the differentiation of dysplastic nevus from
malignant melanoma of the skin. The approach provided the
use of cell nuclei features to categorize each case [14]. The
method showed high accuracy in the classification of groups
of homogeneous cells (up to 82.01%), but can not be used
for mixed cell cases as well as to characterize individual cell
elements.
Our contribution is to create and train an artificial
convolutional neural network that would allow the less
malignant cellular elements (spindle-shaped cells) to be
distinguished from the most malignant (epithelioid cells).
Determination of the cellular composition of a tumor is a
routine event, but it is difficult to characterize individual
cells, especially in mixed tumors. Our solution should help
to improve the diagnostic skills of students and pathology
specialists.
III.
ARTIFICIAL NEURAL NETWORK LEARNING PROCESS
The first stage of neural network training is the preparing
of the training set. For this purpose, 52 patients who
underwent enucleation from 2005–2006 were selected and
their pathology reports and clinical records were studied.
Being faced with the task of classification between two
groups of cells, we selected for the training sample only
those tumors that, according to the reports, were spindle or
epithelioid. Inclusion in the training sample of mixed
tumors would require labor-intensive process for marking
various cells in the tumor site. The use of tumors with a
homogeneous cell composition would allow the markup to
be applied to the tumor site as a whole. According to current
classifications, spindle cell tumors can contain up to 10% of
epithelioid cells and vice versa, so we decided to evaluate
the possibility of marking mainly homogeneous tumors after
digitizing their histological slides.
By excluding mixed cell tumors from the training set, we
had 23 patients and 37 histological slides for them. All
samples were digitized using a Leica ScanScope CS2 slide
scanner, producing 37 gigapixel images in .svs format.
After reviewing the digitized images, we decided to use the
following for further training set: 24 gigapixel images from
12 patients – spindle cell tumors; 4 gigapixel images from 3
patients – epithelioid cell tumors. In total, 28 images were
used. Nine images from 8 patients could not be used in the
training set since it proved impossible to isolate nodes of
homogeneous cell composition. We plan to use the images
not included in the training set for further control of the
classifier.
Each
gigapixel
image
was
marked
using
Aperio
ImageScope by a qualified pathologist. The marking was
performed by complete encirclement of the tumor node with
further exclusion from the marking area of non-tumor
tissues and empty spaces. Convolutional neural networks
require a large number of images in the training sample, so
the problem of the small number of cases was solved by
dividing the gigapixel images into smaller images of 240 x
240 pixels. The common term for such smaller images is a
patch. One .svs image gave the output of a number of
112
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-727-6
AICT 2019 : The Fifteenth Advanced International Conference on Telecommunications

patches from 2,573 to 57,554; the total number of patches
was 605,375, with the average number of patches per image
of 21,620.
The original files in .svs or .scn format are a set of
pyramidal images. The base layer is 240x240 pixel images
compressed using the libjpeg application library. The top
layers comprise 4–16 images combined of lower layers
with lost pixel density, up to the topmost image of
approximately 2000x2000
pixels.
Such
a
composition
allows prompt navigation due to simultaneous displaying of
approximately
10
small
images
instead of a number of images from the lower layer.
Individual patches are extracted from a pyramidal image file
using the same library.
For further processing, we scanned individual 240х240 
patches with a 48х48 pixel scanning window, in increments 
of 8 pixels, and with an overlay round mask. Neural
network ResNet-101 was used for image cell identification.
The weight for the neural network was taken from weights
which produced the best results on 2018 Data Science Bowl
for cell nucleus localization [15]. During training, 48х48 
pixel images with a round mask containing one cell were
fed to another neural network input. The second neural
network had 18,490 parameters and used the Adam(lr=0.01)
optimizer and categorical_crossentropy loss, as well as three
Conv2D blocks:

The first Conv2D block was composed of two
foldings 
with 
3х3 
kernels, 
relu 
activation, 
BatchNormalization and dropout functions. The
first Conv2D block had the convolution kernel size
of 8, the first convolution stride of 2, and dropout
rate of 0.2.

The second Conv2D block had the convolution
kernel size of 16, the first convolution stride of 2,
and dropout rate of 0.4.

The third Conv2D block had the convolution
kernel size of 16, the first convolution stride of 2,
and dropout rate of 0.4.
Also, there are two fully connected layers with 32
parameters, relu activation, BatchNormalization and dropout
functions. The last layer has two neurons and softmax
activation.
To validate the results, a 4-fold approach was used,
whereas the training set was randomly divided into 4 equal
parts. Alternately, 3 parts were submitted to the training,
and 1 was used for control.
When the training was complete, we calculated the F1-
scores for 4 folds of this model and obtained the following
values: 0.76, 0.82, 0.79, and 0.62, respectively. The mean
F1-score for 4 folds of our model was 0.75.
I.
USE OF TRAINED ARTIFICIAL NEURAL NETWORK
The marking in Aperio is stored for each gigapixel image
as a file in .xml format. A trained neural network allows to
mark each patch on a gigapixel image of uveal melanoma or
on a selected part of the image and present such machine-
aided marking as an .xml file. In the .xml file, each marking
color corresponding to a single class (predominant cell
elements) is represented as a list of polygons described as an
enumeration of its boundary points.
This marking method is very similar to the method used
by pathologists to assess the cell population ratios judging
by predominant cells in a number of fields of vision in the
light microscope (usually approximately 20), but it ensures
rough assessment of the ratio, taking into account the tumor
node as a whole.
The data to which the neural network responds may be
presented as mark-ups to patches and gigapixel images.
Mark-ups may be intensity maps, cells with their boundaries
and
the probability of belonging to
a certain class,
deconvolution maps with features marked which contributed
to the decision to include a cell into a certain class.
As seen from Figure 1, the areas of most probable nuclei
location are marked with red mark-up and those of least
probable
location
with
blue
mark-up.
Figure 2 shows another variant of a visual mark-up
Figure 1. Mark-up of most and least probable nuclei location
Figure 2. А color chart of the probability of cell belonging to a certain 
class
113
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-727-6
AICT 2019 : The Fifteenth Advanced International Conference on Telecommunications

presentation: a color chart of the probability of cell
belonging to a certain class, where a bluish color represents
the least probability and a reddish color corresponds to the
highest probability. The last but the most diverse variant of
data presentation is a map of contributions into the decision
to include a cell into a certain class. Figure 3 shows an
example of a deconvolution map for spindle-shaped and
epithelioid cells. The patches show that spindle-shaped cells
look like parallel folds rising above the plane of probability.
Epithelioid cells are broken rounded folds. While comparing
maps of contributions from parallel filters, it is obvious that
higher probability folds for one class correlate with lower
probability for the other. It can be assumed that this pair of
filters mirrors inherent cell geometry, where epithelioid
cells have a rounded nucleus and sphere-like shape and
spindle-shaped cells have elongated nucleus and shape.
A
Web-based
interface
where
anyone
can
upload
a
gigapixel image and see its machine-aided interpretation
with a mentioned mark-up is being developed.
II.
CONCLUSION AND FUTURE WORK
Even small groups of patients may be used in digital
pathology for training in convolution neural networks. This
is possible due to the fact that each gigapixel image contains
several thousand smaller images – patches, which in turn
have two-three dozens of cells – training units. Our further
work will be dedicated to training set extension and tests of
images not included in the training sample.
The mean fold F1-score was 0.75. The result may seem
unassertive if we do not take into consideration the fact that
images in training sample very often had cells from another
class. This aspect is very difficult to eliminate in pathology.
The fact that machine-aided cell marking quite often
contradicts the initial patches marking allows to assume that
the classification accuracy is higher than this figure.
Acquisition
of
more
images
with
uniform
cellular
composition will allow us to assess more accurately the
metrics of the classifier. In our opinion, the most appropriate
mark-up to explain the differences between various types of
cells is color mark-up with cell boundaries and the
probability of belonging to a certain class. Even an
experienced pathologist sometimes finds it difficult to
differentiate between spindle-shaped and epithelioid cells.
Thus, it can be assumed that there are a number of cell
subtypes which are similar to cell classes. The presentation
of each cell as corresponding to a certain class (with the
probability indicated) allows us to visualize this trend.
Mark-ups in the form of contribution maps are more
appropriate for a research work and advanced pathologists
training. Their non-obviousness makes their use in specialist
training possible only with explanations.
We assume that the best solution for processed data
demonstration is a Web-based interface in a browser with
two synchronized windows, where one window shows raw
data and the other window demonstrates the results of the
functioning neural network (predicted classes, cells with
their boundaries and the probability of belonging to a certain
class, maps of features which contributed to the decision to
include a cell into a certain class).
Although our work is devoted to the visualization of
cellular signs of uveal melanoma, this approach can be used
for
automated
pathomorphological
diagnosis
of
uveal
melanoma, which requires further study of the material base
and methodology.
REFERENCES
[1]
A. D. Singh, M. E. Turell, and A. K. Topham, “Uveal
Melanoma: Trends in Incidence, Treatment, and Survival,”
Ophthalmology,
Vol.
118(9),
pp.
1881–1885,
2011,
doi:10.1016/j.ophtha.2011.01.040
[2]
D. Lorenzo et al., “Prognostic Factors and Decision Tree for
Long-Term Survival in Metastatic Uveal Melanoma,” Cancer
Research and Treatment, Vol. 50(4), pp. 1130–1139, 2018,
doi:10.4143/crt.2017.171
[3]
I. W. McLean, W. D. Foster, L.E. Zimmerman, and J. W.
Gamel, “Modifications of Callender’s Classification of Uveal
Melanoma at the Armed Forces Institute of Pathology,”
American Journal of Ophthalmology, Vol. 195, pp. lvi–lx,
2018, doi:10.1016/j.ajo.2018.08.025
[4]
S. Kaliki, C. Shields, and J. Shields, “Uveal melanoma:
Estimating prognosis,” Indian Journal of Ophthalmology,
Vol. 63(2), p.93, 2015, doi:10.4103/0301-4738.154367
[5]
P. Khosravi, E. Kazemi, M. Imielinski, O. Elemento, and I.
Hajirasouliha, “Deep Convolutional Neural Networks Enable
Discrimination of Heterogeneous Digital Pathology Images,”
EBioMedicine,
2018,
Vol.
27,
pp.
317–328,
doi:10.1016/j.ebiom.2017.12.02662.
[6]
J. T. Kwak, S. M. Hewitt, S. Sinha, and R. Bhargava,
“Multimodal microscopy for automated histologic analysis of
prostate cancer,” BMC Cancer, 2011, Vol. 11(1), pp. 1-16,
doi:10.1186/1471-2407-11-6263.
[7]
P. W. Hamilton et al., “Automated tumor analysis for
molecular profiling in lung cancer,” Oncotarget, 2015, Vol.
6(29), pp. 27938-27952, doi:10.18632/oncotarget.4391.
[8]
L. W. Wang et al., “Computer-Based Image Studies on
Tumor Nests Mathematical Features of Breast Cancer and
Their Clinical Prognostic Value,” PLoS ONE, 2013, Vol.
8(12), p. e82314, doi:10.1371/journal.pone.0082314.
[9]
K. Bruno et al., “Deep learning for classification of colorectal
polyps on whole-slide images,” PLoS One, 2013, Vol. 8(12),
p. e8231466, doi: 10.4103/jpi.jpi_34_17
Figure 3. Deconvolution Map
114
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-727-6
AICT 2019 : The Fifteenth Advanced International Conference on Telecommunications

[10] A.
Janowczyk
et
al.,
“High-Throughput
Biomarker
Segmentation on Ovarian Cancer Tissue Microarrays via
Hierarchical
Normalized
Cuts,”
IEEE
Transactions
on
Biomedical Engineering, 2012, Vol. 59(5), pp. 1240–1252,
doi:10.1109/tbme.2011.2179546
[11] C. Belmar-Lopez et al., “Uveal vs. cutaneous melanoma.
Origins
and
causes
of
the
differences,”
Clinical
and
Translational Oncology, 2008, Vol. 10(3), pp. 137–142,
doi:10.1007/s12094-008-0170-4
[12] A. Effland et al., “Joint reconstruction and classification of
tumor cells and cell interactions in melanoma tissue sections
with synthesized training data,” International Journal of
Computer Assisted Radiology and Surgery, 2019, Vol. 14(4),
pp. 587-599, doi:10.1007/s11548-019-01919-z
[13] E. Rexhepaj et al., “A Texture Based Pattern Recognition
Approach to Distinguish Melanoma from Non-Melanoma
Cells in Histopathological Tissue Microarray Sections,” PLoS
ONE,
2013,
Vol.
8(5),
pp.
e62070,
doi:10.1371/journal.pone.0062070
[14] C. Liu et al., “SetSVM: An Approach to Set Classification in
Nuclei-based Cancer Detection,” IEEE Journal of Biomedical
and Health Informatics, 2019, Vol. 23(1), pp. 351-361,
doi:10.1109/jbhi.2018.2803793
[15] L. Wang, W. Li, and Y. Kang, “Data Fusion Network for
Instance Segmentation,” Lecture Notes in Computer Science,
2018,
pp.
175–182,
doi:10.1007/978-3-030-01078-2
115
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-727-6
AICT 2019 : The Fifteenth Advanced International Conference on Telecommunications

