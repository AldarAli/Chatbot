Pinbone Detection of Japanese Shime-Saba by Near-Infrared Imaging
Hisayoshi Ito     Oky Dicky Ardiansyah Prima     Takehiro Sasaki
Graduate School of Software and Information Science
Iwate Prefectural University
152-52, Sugo, Takizawa, Iwate, Japan
e-mail: {hito, prima}@iwate-pu.ac.jp, g231t201@s.iwate-pu.ac.jp
Abstract‚ÄîThere is a growing demand for new analytical 
techniques to manage the presence of fish bones more effectively. 
The detection of fish bones involves human inspection using 
touch and vision, which can lead to misjudgment. Many studies 
utilize X-ray machine vision approaches to effectively detect fish 
bones in quality control processes. However, this approach 
requires complex devices and carries the risk of radiation 
exposure. In this study, we attempted to detect the pinbone tip 
locations of shime-saba by using Near-Infrared (NIR) machine 
vision to highlight those features and quantify them based on 
image geometry and neural networks. The vinegar added to the 
shime-saba softens most of the bones, but the pinbones remain 
tough and need to be removed. Our approach is as follows. At
first, the fish fillet is photographed with NIR transmitted 
through the fish fillet. The resulting image is correlated with the 
Gaussian template image. Quadratic surface equations are
performed on the automatically defined Region of Interest 
(ROI) for this image and the convex-up shapes are selected as 
candidates for the pinbone tips. Rectangular areas (sub-images)
of ten candidates are extracted and a Convolutional Neural 
Network (CNN) is constructed using these sub-images to 
determine the presence of pinbones. In the experiment, 95 
samples of shime-saba fillets were captured in NIR, and 950 sub-
images (225 contained pinbones) were extracted to train the 
CNN model. As a result, the CNN model was able to determine 
the bone with 84.9% accuracy.
Keywords-fishbone; near-infrared imaging; bone detection;
image geometry; convolutional neural network.
I.
INTRODUCTION
There is an increasing need for improved quality 
inspection and visibility into the food supply chain to ensure 
food safety, quality, and traceability. This paper is an 
extension of our previous work on an automated method for 
detecting pinbones of shime-saba [1], a traditional Japanese 
fish dish.
Sushi and sashimi are among the most famous Japanese 
dishes. Such fishery products have had a significant influence 
on the Japanese diet for centuries. In addition to fresh fish, 
there have also been many seafood products developed for 
long-term 
preservation. 
Traditional 
Japanese 
seafood 
preparations include shime-saba (mackerel marinated in sugar, 
salt, and rice vinegar) and dried or salted fish. Hachinohe City 
in Aomori Prefecture is home to shime-saba, the first city in 
Japan to begin producing this fish dish in 1968. Although the 
number of mackerel landings is on the decline, production of 
shime-saba is on the rise. However, this growth is difficult to 
maintain in Japan due to a chronic shortage of human 
resources, requiring the automation of production processes.
To increase efficiency in the food industry, methods are 
being developed to automate production processes [2]. 
Robotics may play an important role as a solution. However, 
compared to other industries, the food industry has been slow 
to adopt robotics. The use of robots is expected to bring many 
tangible, intangible, social, and economic benefits [3]. The 
risks associated with sanitation and safety, as well as high 
labor and social costs, will promote the adoption of robots in 
the food industry.
The fisheries industry has traditionally been a labor-
intensive sector and required skilled staff to process the fish 
into consumable products. Processing included filleting, 
trimming, peeling, visual inspection for parasites, and quality 
control [4]. Another process of trimming is the removal of 
pinbones. The pinbone can be removed by hand because its 
attachment tends to weaken after rigor.
The Japanese food industry is actively introducing robots 
to the market. However, it is difficult for small and medium-
sized companies to introduce robots for processing typical fish 
on their own. In addition, the processing of shime-saba 
requires human handling to remove the pinbones that remain 
on the fish fillet. The vinegar added to the shime-saba softens 
most of the bones, but the pinbone remains tough and needs 
to be removed by hand. Since pinbones are difficult to see, this 
job is physically demanding because it involves touching the 
fish fillet, sensing the presence of pinbones, and then 
removing those bones with tweezers. It also requires more 
workers to ensure production capacity. The manual removal 
of the remaining pinbones from the shime-saba can be seen in 
Figure 1. These bones are in the middle of the cross-section of 
the filleted fish body. Since the cross-sectional height of 
shime-saba is only about 1-3 cm, deboning automatically from 
that position is considered challenging.
The purpose of this study is to improve on our previously 
developed image sensing system for detecting the pinbone tips 
of shime-saba to assist in deboning robots [1]. The new system 
automatically detects the tip of the bone by analyzing features 
from Near-Infrared (NIR) images of the fillet of shime-saba.
Pinbone detection is performed in two stages: selection of 
local convex-up regions based on mathematical quadratic 
surfaces and determination of the regions containing pinbone 
tips based on Convolution Neural Network (CNN). Finally, 
the implementation of this system is described and further 
improvements in detection accuracy are discussed.
166
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

The rest of this paper is organized as follows. Section II 
describes related works on fish bone detection using image 
sensing techniques. Section III describes our proposed 
methods for detecting pinbone tips from NIR transmitted 
images. Section IV describes our experiments and 
summarizes the results. Section V provides a discussion of 
improving the determination of areas that limit the analysis 
area for efficient pinbone tip detection. Finally, section VI
concludes our work.
II. RELATED WORKS
Traditionally, removing bones from fish fillets has 
involved the use of tweezers or needle nose pliers. Currently, 
hand-held pinbone removers are available and allow for easy 
deboning manually. To remove bones using these tools, the 
tip of the bone must first be located. Hence, finding the tip of 
the bone is an important step in automating the deboning 
process.
Various applications using image sensing inspection have 
been developed to ensure food safety. For detecting fish bones, 
image sensing techniques such as X-ray, Ultraviolet (UV), 
and NIR spectroscopy have been proposed. Mery et al.  
developed an X-ray machine vision approach to detect bones 
in fish fillets [5]. Their device is a digital radiography system 
consisting of an X-ray source and a flat panel detector. Filter 
banks including Discrete Fourier Transform (DFT), Discrete 
Cosine Transform (DCT), and Gabor were used to extract 
features from the resulting X-ray images. The results showed 
that by photographing the fish bones, which are arranged in 
strips and range from 14 mm to 47 mm in length, these bones 
can be detected with a high accuracy. Andriiashen et al. 
introduce a processing method for unsupervised foreign body 
detection 
based 
on 
dual-energy 
X-ray 
absorption 
measurements. Their method results in improved X-ray-based 
bone detection [6]. Wang et al. investigated the fluorescent 
properties of cod bone under UV irradiation and found that the 
optimum wavelengths of excitation and emission were 320 
nm and 515 nm. They were the first to develop UV 
fluorescence-assisted candling for detecting fish bones, but 
the detection accuracy was lower than that of X-ray-based 
techniques [7]. Wei et al. used infrared spectroscopy to 
identify fish bone contents in surimi. The absorption peak in 
the infrared spectrum at around 9,890 nm wavelength was 
observed from fish bones [8]. Song et al. proposed a fish bone 
detection based on Raman hyperspectral imaging technique to 
improve detection rate and achieve automatic detection [9].
This technique was found to effectively detect fish bones 
down to a depth of 2.5 mm.
The above optical sensing at various wavelengths has been 
used to bring up the feature values of fish bones to locate the 
position of the bones in the body of the fish. However, putting 
Figure 1. Manual removal of the pinbones remaining in the middle of the cross-section of the fillet.
Figure 2. Detection of pinbone tips for this study.
Input
Set the ROI
Calculate the 
Gaussian Response
Candidate selection
(local maxima)
Candidate re-selection
(geomorphometric)
Determination of
Pinbones (CNN)
Locations of
pinbone tips
167
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

these technologies to practical use in food processing facilities 
is problematic due to their high cost. Furthermore, while it can 
detect bones of a certain length, it is not capable of detecting 
objects where only the tip of the bone can be seen. In contrast, 
the NIR-based approach proposed by Prima et al. was able to 
highlight the presence of pinbone tips by image correlation 
between NIR and Gaussian template images [1].
III. METHODS
In this study, we focus on the broad application of NIR to 
food analysis, based on various sample presentation 
techniques [10]. In addition to reflection, NIR absorbed and 
transmitted from the sample may be used to detect the 
presence or absence of pinbones in the fish fillet. Here, we 
attempt to detect the tips of pinbones by photographing the 
NIR transmitted through the fish fillet.
Figure 2 shows an overview of our pinbone tip detection. 
After capturing a fillet of shime-saba, the region of interest 
(ROI) where the pinbone is likely to be located is estimated. 
Since the pinbone of the shime-saba remains in the center of 
the cross-section of the fillet, assigning this region as a ROI 
reduces the processing cost of its detection. This ROI
estimation involves locating the backbone of the fillet. The 
NIR image within the ROI is then correlated with a predefined 
Gaussian template image to produce a Gaussian response 
image. Regions with maxima in the response image are 
selected as candidates for those consisting of the pinbone tip. 
Geomorphometric features are calculated for these regions, 
and regions with convex-up features are selected as final 
candidates. Finally, a CNN model is built based on these 
regions, and the presence or absence of pinbone tips in each 
region is determined.
A. Image Acquisition
The device for NIR imaging in this study is shown in 
Figure 3. The camera can capture images in the wavelength 
range from visible spectrum to NIR spectrum. To obtain 
images in each spectrum, a dynamic infrared filter is attached 
to the camera lens. Arduino hardware and software were used 
to switch the filters. By switching these filters electronically, 
both visible and NIR images of the fillets can be taken at the 
same position and orientation. For the NIR source, eight 840
nm NIR LEDs were used. The LEDs were arrayed beneath the 
conveyor belt. The camera resolution is 1280x720 pixels.
Figure 4. Three bones in the shime-saba.
Visible Spectrum Image
NIR Image
#1
#2
#3
#1
#2
#3
Figure 3. The device for NIR imaging in this study.
168
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

As shown in Figure 4, there are a maximum of three 
pinbones to be deboned. The presence of these bones is not 
clear in the visible spectrum image, so deboning staff must
touch the fillet with a tweezer to check for their presence. In 
contrast, the NIR image reveals features at the tip of the
pinbone. The tips of the pinbone appear to be locally bright 
and to have a convex-up surface in the NIR image.
B. Determining the ROI
In the cross-section of the shime-saba fillets, the pinbone
tips to be detected are known to be located on the backbone of 
the fillet relative to the stomach. Since the most raised portion 
of the fillet is where the backbone was originally located, 
identifying this area will lead to the identification of the ROI.
To automatically identify this location, a vertical edge 
detection filter (Sobel) is applied to the near-infrared image of 
the fillet, and the resulting edge image is scanned from top to 
bottom for all columns to find points where pixels exceed the 
threshold value. A cubic polynomial
ùëì(ùë•) = ùê¥ùë•3 + ùêµùë•2 + ùê∂ùë• + ùê∑.
(1)
is applied to these points to estimate the shape of the upper 
part of the fillet. Here, x, y are the coordinates of the points 
obtained from scanning. A to D are the coefficients for the 
cubic polynomial calculated by the least squares method. If 
the location of the maximum of the curve of the cubic 
polynomial is on the left side of the fillet image, the stomach 
is considered to be on the right side, and vice versa.
The resulting ROIs are as shown in Figure 5. Despite the 
presence of some incorrectly detected upper boundary points 
of the fillet (red circles) as shown in Figure 5(b), the cubic 
curve approximately traces the fillet's boundary (yellow lines).
C. Gaussian Response
To enhance the features of the bone tips in the NIR image, 
this image is correlated with a Gaussian template image. The 
formula of a Gaussian function in two dimension is
ùëì(ùë•, ùë¶) =
1
‚àö2ùúãùúé2 ùëí
‚àíùë•2+ùë¶2
2ùúé2   .
(2)
where x, y is the distance from the origin in the horizontal and 
in the vertical axes, respectively. œÉ is the standard deviation of 
the Gaussian distribution. As shown in Figure 6, the response 
image obtained shows that the center of the image is bright in 
the region at the tip of the bone. If the brightness of this image 
is taken as an elevation, the region at the tip of the bone can 
be thought of as a convex-up surface. The extent of the 
convex-up structure can be adjusted by changing the size of 
the Gaussian template image. For this study, the size was 
empirically determined to be 45 x 45 pixels with œÉ = 5 pixels.
D. Rectangular Areas of Pinbone Tip Candidates
The rectangular area centered on the brightest spot of the 
response image is selected as the candidate area (sub-image)
for the pinbone tip. This spot is extracted by calculating the 
local maxima of the response image. A threshold is set as the 
distance between a point of maxima and the next point of 
maxima. Here, the size of the area is set to 45 x 45 pixels and 
the minimum distance between the points of maxima is set to 
20 pixels.
E. Geomorphometric Features of Pinbone Tip Candidates
We developed a morphometric characterization algorithm 
to determine geomorphometric features (e.g., peaks, pits, 
ravines, or ridges) from previously described rectangular
Figure 6. The response image obtained by pre-processing.
ÔÇ´
=
Gaussian
Template
(a) Proper detection of the upper boundary points of the fillet
(b) Incorrect detection of a part of the upper boundary points of the fillet
Figure 5. Determining the ROIs.
ROI
ROI
169
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

areas of the pinbone tip candidates. The quadratic surface is 
fitted to the image within a moving local analysis window 
using least-squares [11], with the general equation being
ùëß = ùëì(ùë•, ùë¶) = ùê¥ùë•2 + ùêµùë•ùë¶ + ùê∂ùë¶2 + ùê∑ùë• + ùê∏ùë¶ + ùêπ.
(3)
where (x, y) is the location‚Äôs coordinate, z is the pixel value 
calculated by the quadratic function, and A to F are the 
coefficients of the quadratic function calculated by the least 
squares method. By analyzing the second-order coefficients A
to C, the shape of the quadratic surface can be characterized 
as follows.
Elliptic paraboloid:       ùêµ2 ‚àí 4ùê¥ùê∂ < 0
(4)
Hyperbolic paraboloid: ùêµ2 ‚àí 4ùê¥ùê∂ > 0
(5)
Parabolic paraboloid:    ùêµ2 ‚àí 4ùê¥ùê∂ = 0
(6)
Here, if A=B=C=0 then the quadratic is a plane. Equation (2) 
divides the quadratic surface into convex-up and concave-up.
If the center of the convex-up surface is within the analysis 
window, this surface can be determined as the peak. As shown 
in Figure 4, the pixel of the pinbone tip is brighter than its 
neighbors, which means that this pixel represents the convex-
up (peak). Hence, this property can be used to determine the 
location of the pinbone tips from the response image.  
F. Determination of Pinbone Tip Using CNN
Regions with convex-up features do not necessarily 
contain pin bones. We introduce a CNN that determines the 
presence of pin bones among those regions. Our CNN
architecture is shown in Figure 7. This network was built 
using the Neural Network Console [12], an engineer-oriented 
deep learning framework developed by Sony Group Inc. The 
input to the CNN is a 45x45 rectangular area of the pinbone 
candidate. Signals are passed through a two-stage network 
consisting of convolution, maximum pooling, and tanh 
activation function. The first stage produces 32 maps and the 
second stage 16 maps. The results are fully concatenated and 
the presence or absence of pinbone is determined by binary 
cross entropy. The Mean Squared Error (MSE) is used as the 
loss function for the network optimization process.
To summarize, the process to build a dataset of pinbones 
in shime-saba fillets is as shown in Figure 8. The first process 
is to find the boundary points (white circles) between the top 
point of the fillet and the background based on the edge 
intensities calculated from the input NIR image. From these 
points, a cubic curve (yellow line) is approximated to find the 
highest point of the fillet. The ROI is then placed at the same 
height as the top point in the stomach direction. Ten 
rectangular areas of candidate pinbones are automatically 
extracted. These areas are assumed to have both local maxima 
Figure 7. The CNN used for this study.
Input
Convolution
Kernel: 5x5
Max Pooling
Kernel: 5x5
Tanh
Convolution
Kernel: 5x5
Max Pooling
Kernel: 5x5
Tanh
Affine
Tanh
Affine
Sigmoid
Binary Cross Entropy
1, 45, 45
32, 41, 41
32, 20, 20
32, 20, 20
16, 16, 16
16, 8, 8
16, 8, 8
10
10
1
1
1
Figure 8. A GUI created to build a dataset of pinbones in the shime-saba.
Gaussian
Response
ROI
Rectangular areas of 
pinbone candidates
Pinbone availability
Gaussian
template
170
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

and convex-up features, which are confirmed by quadratic 
equations.
Finally, 
using 
the 
GUI 
(slider), 
the 
presence/absence label of the pinbone tips are given to these 
rectangles, and they are constructed into a dataset learnable by 
the CNN. Here, samples with pinbone tips are classified as
positive and those without pinbone tips as negative in the 
dataset. OpenCV (opencv.org), an open-source computer 
vision software library, is used for this calculation.
IV. EXPERIMENTS AND RESULTS
For this experiment, 95 shime-saba fillets were used. 
These fillets were taken from the shime-saba production 
process before they were packaged. Fillets with less cracks 
were chosen for the experiment to avoid extra NIR light 
influence from the cracks.
A. Image Acquisition
The fillets are photographed in the order of NIR and 
visible spectrum, and then the pinbones are located with 
tweezers and the scene is photographed as well. For each fillet, 
the images are taken as shown in Figure 9, facilitating easy 
generation of the positive images in the dataset that contains 
pinbones. From the 95 fillets of shime-saba photographed in 
this study, 250 locations of pinbones could be identified 
manually with tweezers. This means that there are around two 
to three pinbones in each fillet.
B. Region of Interest (ROI)
The size of the ROIs was fixed at 200 x 400 pixels and 
located at 175 pixels away from the top of the approximated
curve of the fillet. Figure 10 shows the results of some typical 
ROIs. The blue dotted lines indicate the potential location of 
pin bones. The ROIs were determined for 95 fillets of shime-
saba, with 78 ROIs covering the area where the pinbone tips
are present. In addition, 14 ROIs partially covered the area 
where the pinbone tips are present. Only three ROIs did not 
correctly cover the pinbone tips. As the side of the fillet floats 
off the conveyor belt, the ROI falls off the pinbone positions.
In this study, ROIs that did not properly cover the pinbone 
were manually transformed to cover the area where the 
pinbone was present.
C. Ten Candidates for Pinbone Tips
From each fillet, ten rectangular areas where pinbones 
potentially exist were selected in order of their convexity
calculated by Equation (2). These rectangular areas were 
labeled for the presence or absence of pinbone tips by 
comparing them to locations indicated by the tweezers in the 
visible spectrum image of the fillet as shown in Figure 9. The 
top ten candidate rectangular areas were obtained from 95 
fillets, for a total of 950 rectangular areas. Of those, 225 
pinbones could be identified, but the remaining 25 could not.
Extracting the top ten pinbone tip candidates for each fillet 
means that 90% of the pinbone tips can be located. However, 
since only a maximum of three pinbone tips are present in the 
cross-section of each fillet, the ten candidates must be further 
narrowed down. Finally, a binary classification dataset was 
constructed with 225 rectangular areas with confirmed 
pinbone tips as positive and the remaining 725 rectangular 
areas as negative samples.
(a) NIR image.
(b) Visible spectrum.
(c)  Location of the pinbone #1 in visible spectrum.
(d) Location of the pinbone #2 in visible spectrum.
Figure 9. An example of images of a fillet taken for this experiment.
171
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

Figure 11 shows ten candidate rectangular areas extracted 
from seven fillets (I - VII). Yellow boxes indicate the presence 
of pinbone tips within the areas. Overall, the relationship 
between the rank of 10 candidate areas and the number of 
pinbones present is shown in Figure 12. Here, a higher rank 
can be interpreted as the presence of more pinbone tips. In 
other words, the presence of pinbone tips suggests that they
can be found in the NIR image of the fillet where the 
convexity is high.
D. Determination of Pinbone Tip Using CNN
Using the previously described binary classification 
dataset, we trained the CNN as shown in Figure 7. About 80% 
(760 rectangular areas) were randomly selected for training 
and 20% (190 rectangular areas) for validation. Here, the 
batch size was set to 32, the epoch to 100, and Adam was used 
as the optimizer. The cost function, training error, and 
validation error of the CNN measured after each epoch are 
(a)
ROIs covering the area where the pinbone tips are present.
(b)
ROIs partially covering the area where the pinbone tips are present.
(c)
ROIs does not cover the area where the pinbone tips are present.
Figure 10. ROIs automatically identified by this study.
ROI
ROI
ROI
ROI
ROI
ROI
ROI
ROI
Figure 11. Rectangular areas from seven fillets (I~VII)
Figure 12. The relationship between rank and pinbones.
172
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

shown in Figure 13. Up to the 80th epoch, the validation error 
decreases but begins to increase again. Therefore, at the 80th
epoch, the model weights were outputted since the loss is low 
and stable. Table I shows the confusion matrix of the learned 
CNN model. Accuracy was good at 84.9%, but precision, 
recall, and F-Measures were relatively low at just over 60%. 
The reason can be attributed to the small sample size of the 
dataset and to positive samples being only about 30% of the 
total number of negative samples. The results of this study, 
however, are encouraging as a first attempt in this effort, and 
we believe that the results can be improved further by 
increasing samples in the dataset and by improving the 
method of determining the ROI and the design of the CNN.
V. DISCUSSION
The ROI of this study, which limits the analysis area for 
efficient pinbone tip detection, is expected to not only reduce 
the computational cost but also reduce misdetections.
However, identifying the boundary points of the fillets by 
edge intensity and approximating those points with a cubic 
curve is susceptible to influence by noise. In addition, when 
multiple fillets are side-by-side, calculation of the cubic 
curves according to each fillet becomes more complicated.
For future development, we experimented with the use of 
CNN-based 
semantic 
segmentation 
to estimate 
the 
boundaries of fillets as shown in Figure 14. There are two 
main parts: a contraction pass consisting of a convolution 
layer unit and an expansion pass consisting of a 
deconvolution layer unit. Each convolution is followed by 
batch normalization and Rectified Linear Unit (ReLU). Input 
data consisted of NIR images of 95 fillets of shime-saba fillet 
used in our experiment and manually annotated fillet shapes. 
Figure 13. Learning curve of the CNN in this study.
TABLE I.  CONFUSION MATRIX
0.60
0.55
0.50
0.45
0.40
0.35
0.30
0          10           20          30          40          50          60          70          80           90          100 
Cost
Training Error
Validation Error
0.60
0.55
0.50
0.45
0.40
0.35
0.30
Error
Cost
Epoch
Accuracy
0.849
Precision
0.641
Recall
0.625
F-Measures
0.633
Accuracy
0.849
Precision
0.641
Recall
0.625
F-Measures
0.633
Negative
Positive
Negative
138
14
Positive
15
25
Real values
Predicted values
Figure 14. Neural network of the semantic segmentation for this study
Input
Augmentation
Convolution
Batch Norm.
ReLU
Convolution
Convolution
Convolution
Deconvolution
Deconvolution
Deconvolution
Concatenate
Concatenate
Concatenate
Convolution
Batch Norm.
ReLU
Convolution
Squared Error
3, 512, 512
3, 512, 512
11, 512, 512
11, 512, 512
11, 512, 512
16, 256, 256
16, 128, 128
16, 64, 64
16, 128, 128
32, 128, 128
16, 256, 256
32, 256, 256
16, 512, 512
27, 512, 512
8, 512, 512
8, 512, 512
8, 512, 512
3, 512, 512
3, 512, 512
Input
Max Pooling
Convolution
ReLU
Batch Norm.
Convolution
Batch Norm.
ReLU
Dropout (0.2)
Convolution
Batch Norm.
ReLU
Convolution
Batch Norm.
ReLU
Convolution
Convolution
Convolution
Convolution
ReLU
ReLU
ReLU
ReLU
Batch Norm.
Batch Norm.
Batch Norm.
Dropout (0.2)
1, 28, 28
1, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
16, 14, 14
1, 28, 28
1, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
16, 56, 56
Convolution Unit
Deconvolution Unit
Semantic Segmentation
173
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

LabelMe, a labeling and annotation tool, was used for 
annotation [13].
Figure 15(a) shows shime-saba fillets manually annotated 
using LabelMe (red areas). Since the boundary is not clear in 
the area of fillet touching the conveyor belt, the darker area 
toward the fillet was treated as the boundary.  To evaluate the 
derived model of semantic segmentation, we merged two fillet 
images horizontally and inferred the boundaries of each with 
the model. As shown in Figure 15(b), the two fillets can be 
separated, and the boundaries of each fillet (yellow lines) 
extracted are acceptable. The result is expected to enable the 
identification of the thickest part of the shime-saba fillet, 
which will facilitate the identification of areas of the pinbone 
tips.
VI. CONCLUSION
In this study, an image sensing system for detecting the 
pinbone tips of shime-saba was developed. The two-step 
processes of selecting local convex-up areas by mathematical 
quadratic surfaces and determining pinbone-containing areas 
by the Convolution Neural Network enable pinbone locations 
to be automatically identified. Pinbone detection accuracy 
was 84.9%, while Precision, Recall, and F-Measures were 
relatively low at just over 60%. As a first attempt, this result 
is acceptable. Further improvement in results can be expected 
by increasing the number of samples in the data set.
Since the area where pinbones are located is known to be 
in and around the middle of the fillet, the proposed cubic curve 
based on the edge information indicates that these areas can 
(a) Manually annotated shime-saba fillets.
(b) Fillet shapes detected using the CNN-based semantic segmentation
Figure 15. Manually annotated shime-saba fillets and fillet shapes detected using the CNN-based semantic segmentation.
174
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

be estimated. For a better method of obtaining the area, we 
found that automatic extraction of the fillet shape by semantic 
segmentation can also be used. This method enables 
simultaneous detection of pinbone tips from multiple fillets.
Future experiments will include the implementation of a 
deboning robot that will remove the detected pinbone tips. The 
results will be presented in our forthcoming paper.
ACKNOWLEDGMENT
This work was supported by the IoT Human Resource 
Development Support Project of Iwate Prefecture, Japan.
REFERENCES
[1]
H. Ito, O. D. A. Prima, and T. Sasaki, ‚ÄúDetection of Pinbones 
in Japanese Shime-saba,‚Äù The Fifteenth International 
Conference on Advances in Computer-Human Interactions,
ACHI 2022, pp. 22-25, 2022.
[2]
C. Alejandro, ‚ÄúFood Analysis: Present, Future, and Foodomics,‚Äù 
International Scholarly Research Network (ISRN) Analytical 
Chemistry, pp. 1-16, 2012.
[3]
P . Graham, ‚ÄúRobotic Equipment in the Meat Industry,‚Äù Meat 
Science, 49(1), pp. 297-307, 1998.
[4]
H. Einarsdottir, B. Guoundsson, and V. Omarsson, 
‚ÄúAutomation in the fish industry,‚Äù Animal Frontiers, 12(2), pp. 
32‚Äì39, 2022.
[5]
D. Mery, et al., ‚ÄúAutomated fish bone detection using X-ray 
imaging,‚Äù Journal of Food Engineering, 105(3), pp. 485‚Äì492, 
2011, https://doi.org/10.1016/j.jfoodeng.2011.03.007.
[6]
V. Andriiashen, R. van Liere,  T. van Leeuwen, K. J. Batenburg, 
‚ÄúUnsupervised Foreign Object Detection Based on Dual-
Energy Absorptiometry in the Food Industry,‚Äù Journal of 
Imaging, 7(104), pp. 1-18, 2021.
[7]
S. Wang, R. Nian, L. Cao, J. Sui, and H. Lin, ‚ÄúDetection of fish 
bones in cod fillets by UV illumination,‚Äù Journal of Food 
Protection, 
78(7), 
pp. 
1414‚Äì1419, 
2015. 
https://doi.org/10.4315/0362-028X.JFP-14-358.
[8]
W. Wei, et al., ‚ÄúEnhanced chemical and spatial recognition of 
fish bones in surimi by Tri-step infrared spectroscopy and 
infrared microspectroscopic imaging,‚Äù Spectrochimica Acta 
Part A: Molecular and Biomolecular Spectroscopy, 205, pp. 
186‚Äì192. doi:10.1016/j.saa.2018.07.031, 2018.
[9]
S. Song, et al., ‚ÄúDetection of fish bones in fillets by Raman 
hyperspectral 
imaging 
technology,‚Äù 
Journal 
of 
Food 
Engineering, 272, pp. 1-10, 2019.
https://doi.org/10.1016/j.jfoodeng.2019.109808
[10] B. G. Osborne, ‚ÄúNear-Infrared Spectroscopy in Food Analysis,‚Äù 
Encyclopedia of Analytical Chemistry, pp. 1‚Äì14, 2006. 
https://doi.org/10.1002/9780470027318.a1018
[11] D. Wang, S. W. Laffan, Y. Liu, and L. Wu, ‚ÄúMorphometric 
characterisation of landform from DEMs,‚Äù International 
Journal of Geographical Information Science, 24(2), pp. 305‚Äì
326, 2010. https://doi.org/10.1080/13658810802467969.
[12] T. Narihira,et al., ‚ÄúNeural Network Libraries: A Deep Learning 
Framework Designed from Engineers' Perspectives,‚Äù arXiv 
preprint arXiv:2102.06725, pp. 1- 12, 2021.
[13] LabelMe, https://github.com/wkentaro/labelme
[retrieved: December, 2022].
175
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

