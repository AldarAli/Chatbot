Cursor Control Trace 
Another look into eye-gaze, hand, and eye-hand pointing techniques  
 
Ricardo Sol 
Exact Sciences and Engineering 
Centre 
University of Madeira 
9000-390 Madeira, Portugal 
rsol@alumni.carnegiemellon.edu 
Mon-Chu Chen 
M-ITI, Madeira Interactive 
Technology Institute 
University of Madeira 
9000-390 Madeira, Portugal 
monchu@uma.pt 
José Carlos Marques 
Exact Sciences and Engineering 
Centre 
University of Madeira 
9000-390 Madeira, Portugal 
marques@uma.pt 
 
 
Abstract—We analyzed cursor control trace with respect to 
three cursor control methods: eye-gaze, hand, and eye-hand. 
First, we look into the mechanism that allows users to control 
cursor positions by hands and/or eyes. Second, we conducted 
an experiment in which subjects perform a searching, pointing 
and selection task in three different conditions (eye-only, hand-
only, and eye-hand). Third, we further studied the cursor trace 
and analyzed the moments when users switch between eye-
gaze-control and hand-control. Although different from a 
simpler Fitt’s pointing task, our results mostly corroborate 
previous work. In addition, the cursor traces analysis further 
shows why eye-hand is more efficient, and how users progress 
from an inefficient pointing behavior to an optimal one.   
Keywords; pointing; accuracy; gaze; eye; tracking 
I. 
 INTRODUCTION 
Hands move slower than eyes. Therefore having eye 
tracking instead of manual input as an interaction mechanism 
in digital devices can make interactions faster. However, the 
eye trackers developed so far have numerous drawbacks. 
First, current eye trackers are not precise enough to 
accomplish common tasks on applications or websites. Next, 
these systems have a delay in processing gaze position and 
calibration. A third problem is the 'Midas touch', a problem 
identified by Jacob in 1991, that occurs because it is 
challenging to distinguish a selection task (for example, a 
click) from the search task, using a purely eye-gaze approach 
[2]. Eye pointing is also typically associated with fixation 
(dwell) but this action is not stable and shows jitter. 
In order to address these issues, in 1999 Zhai et al. 
presented Mouse and Gaze Input Cascaded (MAGIC), a 
technique that uses a combination of gaze and manual input 
[8]. First, the user uses gaze to dynamically redefine the 
position of the cursor. After the cursor position is redefined 
the user then makes a small manual input action to select the 
target. 
The advantages of MAGIC are that it reduces manual 
stress and fatigue. In addition, MAGIC pointing is faster than 
just manual pointing. It must be noted however, that because 
the cursor movement is faster while controlled by the eye, 
the user may perceive all action to be faster even when it is 
not. For example the manual selection time might take 
longer, so overall the tasks might not be accomplished faster 
although the user may perceive them to be so.  
MAGIC has two approaches. In the first approach, 
referred to as the liberal approach, the cursor moves directly 
over (in front of) the target that the users looks at. In the 
second approach, the conservative approach, the cursor 
moves to the boundaries of the target. 
Zhai et al conducted experimental validations of the 
MAGIC technique. These experiments were conducted with 
a miniature isometric pointing stick [8]. The experimental 
task was basically a Fitts' pointing task. The factors 
manipulated in the experiment were: target size, target 
distance, and direction. Each subject performed the task with 
3 techniques: no gaze, liberal, and conservative. The liberal 
technique was found to be faster and preferred by users. 
While the MAGIC technique addresses some of the 
problems of preceding gaze input mechanisms, it is not 
without its drawbacks. For example, with MAGIC's liberal 
technique the cursor movement can be overactive, which 
could be distracting when reading. With MAGIC's 
conservative technique the user might tend to activate the 
cursor manually. 
Our broad research goal is to improve input techniques. 
In this work, our contribution adds to previous work by 
analyzing the trace before, during, and after that manual 
activation, and we try to find patterns in these traces. 
II. 
RELATED WORK 
In 2000 Salvucci and Anderson presented their intelligent 
gaze-added interfaces [6]. They addressed accuracy 
problems that we also face. In their work, any target 
positioned where the users' eye gaze is, is a highlighted 
target. Then a gaze key gives the user the chance to trigger 
the action. The system uses a probabilistic algorithm to try to 
guess the targets the user will look at. 
In 2003 McGuffin and Balakrishnan showed that 
expanding targets facilitates the pointing task [4]. Their 
results show that working with expanding targets can be 
accurately modeled by Fitts' law. They have also shown that 
targets that expand just as the user is about to reach them can 
be acquired approximately as fast as targets that are always 
in an expanded state. They specifically found strong 
evidence that the user performance is consistently aided by 
440
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
 
 
the target expansion. Similar to how we hypothesize for our 
work, they found that the performance is dependent on the 
final target size. 
In 2005 Miniotas and Spakov used an expansion of 
targets visible to the users [5]. To facilitate pointing they 
used dynamic target expansion for fixing the calibration of 
the eye tracker. This technique has 91% accuracy, a result 
not expected in our work. The drawback of this technique is 
an increase in selection time. 
In 2005 Ashmore and Duchowsky refined a fisheye lens 
to support eye pointing [1]. They simply hid the lens during 
visual search and obtained improvements in speed and 
accuracy. Fisheye interaction was evaluated by a visual 
search, and a Fitts’ pointing. Unlike MAGIC pointing, where 
the cursor was rapidly moved to the vicinity of one's gaze 
prior to mouse movement, here the lens is directly slaved to 
gaze position. Important to our work is their finding that in 
combined tasks it is impossible to distinguish the precise 
amount of time that search consumed prior to selection.  
In 2007 Kumar et. al presented EyePoint. EyePoint uses 
expansions of interactive targets, and uses a key for input [3]. 
When the key is pressed the gaze area is enlarged. When the 
key is released the selections are made according to where 
the eye gaze is. Similar as we hypothesize for our work, they 
found that it is possible to divide appropriate interaction 
techniques that use gaze without overloading the visual 
channel.   
III. 
METHOD 
A. Participants 
The experiment was performed on 10 participants with 
normal or corrected-to-normal vision. All participants were 
regular computer and mouse users. All participants had used 
track pads. 
B. Tasks 
Three conditions (eye-only, hand-only, and eye-hand) 
were presented that required searching and selecting one 
target. In the hand-only condition the participant used the 
track-pad to move the cursor until it was over the target. In 
the eye-only condition the participant used eye-gaze to move 
the cursor until it was over the target. In the eye-hand 
condition the participant used eye-gaze and the track-pad to 
move the cursor until it was over the target. The target 
consisted of a white vertical rectangle with a plus signal in 
its center. This rectangle could alternate in size (2% or 4% of 
the screen width), and distance to the center along the 
screen’s horizontal central axis (45% or 90% of half of the 
screen width), giving a total of four different possibilities. 
The target also alternated between sides of the screen. Each 
trial had one distractor with the same characteristics as the 
target without the plus signal.  All trails started with the 
cursor located at the center of the screen as in Fig. 1 and Fig. 
2. All trails finished with the participants pressing the space 
key while the cursor was over the target. Each trial had a 
maximum amount of time, 5 seconds, and if this expired 
without pressing space while on the target, the task failed.  
C. Apparatus 
Participants used a track-pad to complete the task (Apple 
Magic Trackpad). 
The experimental computer ran Mac OS X version 10.7.4 
and was connected to Tobii TX300 Eye-tracker. The system 
allowed for unconstrained head motion by seating 
participants approximately 65 cm (adjusted by Tobii studio 
running on a PC) in front of the Eye-tracker 23 inch screen 
(resolution 1920 x 1080 pixel). With this system the sample 
rate was 300 Hz. 
Figure 1.  Initial display of a trial that has the target (right side) with the 
biggest size at the farthest distance. 
 
Figure 2.  Initial display of a trial that has the target with the smallest size 
at the closest distance. 
 
Figure 3.  A zoom in of the initial display of a trial that has the target with 
the biggest size at the closest distance. 
441
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
 
 
Figure 4.  One trial of one task presented to the users.  In this case the 
target is small and is farther from the cursor’s original point. 
D. Procedure 
The user was first asked to sit in a non-mobile chair. 
Then the distance of the eyes to the screen was adjusted and 
the user was asked not to make large movements with his 
head and torso. After this, it was necessary to calibrate the 
user’s gaze to the system. This calibration was made by 
having the user looking each time for 3 seconds to a dot that 
appeared in 9 different pre-selected positions of the screen. 
Before data collection, the conditions and task were 
explained, and unlimited time to practice with the 3 
conditions was given. The data collection started with the 
conditions ordered randomly. Each participant then carried 
out 80 trials for each condition. Each experimental session 
lasted around 45 minutes including pauses between 
conditions.  
IV. 
RESULTS 
Our results show that Eye-only is significantly faster than 
Hand-only solely when the target has the largest size and is 
at the farthest distance. The variance of the response time is 
always higher in Eye-only. Eye-only has a hit on target rate 
of 72%. That is, participants successfully accomplished the 
task within the limit of 5 seconds in only 72% of the trials in 
the Eye-only condition. These hits on target were lower in 
the smallest size and farthest distance condition. 
The results also show that Eye-with-Hand is the 
significantly fastest solely when the target has the largest 
size and is at the closest distance. The variance of the 
response time in Eye-with-Hand is higher than in Hand-only 
and lower than in Eye-only, as Figure 4 shows. Eye-with-
Hand has a hit on target rate of 99.50%. This is similar to 
Hand-only that had a hit on target rate of 99.75%. 
Our results also show the trace of each trial for Eye-with-
Hand. In Figure 5 we can see an early trial trace. This user is 
not yet sufficiently familiarized with eye-gaze control. The 
target is situated in the right side of the screen, that is 0.9 on 
the vertical axis. The user first briefly looked to the left and 
immediately changed direction to the right and changed it 
again to the left all way. Then the user changed control to the 
hand bringing the cursor to slightly right of the center. The 
users’ finger then reached the right extremity of the track 
pad, so the user released the finger, passing automatically to 
the control of the eyes. The user subsequently acquired 
control with the track pad bringing the cursor to the extreme 
right of the screen. Then the users’ finger reached the right 
extremity of the track pad, so the user released the finger 
passing automatically to the control of the eyes. The user 
then looked to the center and had to repeat the process twice, 
failing to press space while the cursor was on top of the 
target, and not completing the task in the defined time.  
In the trial in Figure 6, the user started looking to the left 
and then looked to the right. 1 second after the beginning, the 
user started controlling the cursor with the hand and moved 
the cursor to the target and at 2.1 seconds pressed space. 
In Figure 7, the user started looking to the left and then 
looked further to the left, then looked back to center, and 
back to the left again. 1.4 seconds after the beginning the 
user started controlling the cursor with the hand, moved the 
cursor to the target and at 2.4 seconds pressed space. 
 
Figure 5.  Eye control and Hand control shown over Time as a function of 
Distance. Early trial. 
 
 
 
 
 
 
Figure 6.  Eye and Hand control over Time as a function of Distance. 
442
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
 
Figure 7.  Eye control and Hand control shown over Time as a function of 
Distance. 
 
V. 
CONCLUSION & FUTURE WORK 
We show support for MAGIC by stating with confidence 
that in at least in one condition, the Eye-with-Hand 
condition, the performance was faster. An explanation for 
this could be that this system had a more optimized transfer 
function for the movements. Another factor contributing to 
this could be the more optimized parameters in the gaze 
system's filter. In addition, we used a more recent eye-tracker 
as compared with those used in previous work and is 
therefore expected to be more accurate. 
Eye-gaze control is a novelty for users. Users’ lack of 
experience with eye-gaze can introduce delays in the control 
action. It is expected that with the dissemination of eye-
tracker these delays can diminish. However, controlling the 
cursor with Eye-gaze can give the user a sense of speed. 
The results help to further understand how people 
progress to master controlling cursor using the combination 
of the eye-gaze and mouse. 
We foresee that having a relatively short constant time 
for hand control will decrease the time taken to reach the 
target. More distance will be traveled during the fastest 
control, which is the control by the eye. We foresee that 
both, eye and hand control conditions are governed by Fitt's 
Law. We intend to perform more experiments in order to 
confirm this. 
Some users considered having the cursor in the gaze 
position distracting. While in MAGIC’s conservative 
approach the cursor stays in the boundaries of the target, this 
can still be a distraction. One potential solution for this 
problem might be to decrease the cursor's opacity. 
In a pointing task, having the cursor in the eye-gaze 
might not be so distracting. In a selecting task this distraction 
can be higher. However, it is during reading that we expect 
to find the cursor in the eye-gaze to be most distracting. In 
future work, we intend to perform further experiments in 
order to confirm these hypotheses. 
 
ACKNOWLEDGMENTS 
The authors acknowledge the financial support of the 
Portuguese governmental agencies ADI – Agencia da 
Inovacao, and FCT – Fundacao para a Ciencia e Tecnologia 
through 
PEst-C/CTM/LA0025/2011 
and 
project 
PTDC/EQU-EPR/114685/2009. 
REFERENCES 
 
[1] Ashmore, M., Duchowski, A. T., and Shoemaker, G. Efficient 
Eye Pointing with a Fisheye Lens. In Proceedings of Graphics 
interface 2005. ACM International Conference Proceeding 
Series, vol. 112, 203 – 210, 2005. 
[2] Jacob, R. J. K. What You Look at is What You Get: Eye 
Movement-Based Interaction Techniques. In Proc. CHI ’90, 
pages 11-18. ACM Press, 1990. 
[3] Kumar, M., Paepcke, A., and Winograd, T. EyePoint: 
Practical Pointing and Selection Using Gaze and Keyboard. In 
Proceedings of the SIGCHI Conference on Human Factors in 
Computing Systems CHI '07. ACM Press 421 – 430, 2007. 
[4] McGuffin, M. and Balakrishnan, R. Acquisition of Expanding 
Targets. In Proceedings of the SIGCHI Conference on Human 
Factors in Computing Systems CHI '02. ACM Press 57 – 64, 
2002. 
[5] Miniotas, D., Špakov, O., and MacKenzie, I.S. Eye Gaze 
Interaction with Expanding Targets. In CHI '04 Extended 
Abstracts on Human Factors in Computing Systems. CHI '04. 
ACM Press 1255 – 1258, 2005. 
[6] Salvucci, D. D., and Anderson, J. R. Intelligent Gaze-Added 
Interfaces. In Proceedings of the SIGCHI Conference on 
Human Factors in Computing Systems. CHI '00. ACM Press 
273 – 280, 2000. 
[7] Spakov, O., and Miniotas, D. Gaze-Based Selection of 
Standard-Size Menu Items. In Proceedings of the 7th 
international Conference on Multimodal interfaces. ICMI '05. 
ACM Press 124 – 128, 2005. 
[8] Zhai, S., Morimoto, C., and Ihde, S. Manual and gaze input 
cascaded (MAGIC) pointing. In Proc. of CHI ’99, 246–253. 
ACM, 1999. 
[9] Zhai, S., Conversy, S., Beaudouin-Lafon, M., Guiard, Y. 
Human On-line Response to Target Expansion. In Proc. of the 
SIGCHI, 177 – 184. ACM Press , 2003. 
 
443
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

