174
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
The Impact of Multi-Outage Episodes on Large-Scale Wireless Voice Networks 
 
Andrew P. Snow and Yachuan Chen 
Ohio University 
School of Information and Telecommunication Systems  
Athens, Ohio 
e-mail: asnow@ohio.edu; yc137604@ohio.edu  
 
 
Gary R. Weckman  
Ohio University 
Department of Industrial and Systems Engineering  
Athens, Ohio 
e-mail: weckmang@ohio.edu 
 
 
Abstract— Large wireless network infrastructures experience 
concurrent or overlapping service outages due to equipment 
and link failures. The frequency, duration, and impact of such 
episodes are of interest to users and network operators alike. 
Here, a research project which investigates through simulation 
the characteristics of concurrent network outages in large 
wireless 
network 
infrastructures 
is 
presented. 
The 
dependability attributes used to gain a perspective on this issue 
are network reliability, availability, maintainability and 
survivability. To assess these attributes in this setting, a new 
term, called an “impact epoch”, is introduced.  Epochs are 
defined as single, concurrent, or overlapping outages in time, 
consisting of n different outages. A wireless network is 
expanded in size and epochs observed as the network grows. 
The new proposed metrics offer valuable insights into the 
management of restoration resources. Simulations proved 
invaluable in identifying multi-outage epochs, as well as 
modeling their occurrence, frequency, duration, and size – 
results which are analytically intractable for assessing large 
networks. 
Keywords 
– 
RAMS; 
network 
outages; 
simulation; 
survivability; reliability; maintainability; wireless network 
infrastructure  
I. 
INTRODUCTION 
The larger the network, the greater the challenge for 
operators. 
Networks 
are 
critical 
telecommunication 
infrastructure, as millions of people depend on these 
networks for daily communication and commerce. As 
demand increases, so does network size, challenging 
engineers 
and 
operators 
to 
maintain—and 
not 
compromise—network dependability. As a network grows 
in size, the sheer number of components grows also, 
increasing failure hazard. With such an increase in hazard, 
the chance of concurrent, or overlapping, outages also can 
be expected to increase. Dealing with these concurrent 
outages is challenging because network operators have to 
judge priorities in allocating limited repair resources to 
outages spatially distributed. If the response is consistently 
substandard, the operator’s ability to satisfy current 
customers—as well as accommodate new ones—could be 
adversely affected. Understanding the characteristics of 
concurrent outages as a function of network size, component 
failure, and repair rates offers network operators valuable 
information in developing outage recovery strategies. The 
number of customers that could be impacted by network 
failures is another important factor for network operators to 
consider. If the probability distribution of impacted 
customers is known, thresholds highlighting critical events 
can be established.  
This 
paper 
investigates 
the 
characteristics 
of 
simultaneous network outages and attempts to identify the 
distribution of impacted customers through simulation. This 
phenomenon was first reported in [1], and this paper 
expands on and extends some of those preliminary findings. 
There is much interest in understanding the impact of 
outages. Hariri, et al [2] examined the impact of concurrent 
faults and attacks in large-scale networks, in particular the 
internet. However, the emphasis was on the effect of 
multiple transmission and switching outages to traffic, not 
predictions of the frequency of such phenomena. 
Alternately, Bassiri and Heydari [3] considered network 
survivability in the presence of regional outage scenarios. 
However, they concentrated on the effects of multiple 
switch and link outages in regional areas due to such 
phenomena as natural disasters, and  also concentrates on 
traffic in internet environments. Recently, others invented an 
outage management portal to coordinate response to single 
outages [4]. However, no studies could be found that 
examined the probabilistic frequency and severity of 
concurrent outages. Prior published research has not 
considered how often multiple outage epochs occur in large-
scale networks, how many simultaneous outage epochs can 
be expected, and how many users can be expected to be 
impacted for how long. 
A. Dependability 
Users count on networks. If a network is unreliable, hard 
to maintain, and has poor availability, it can hardly be 
deemed successful. Dependability has a number of different 
attributes. According to Avižienis, et al [5], the concept of 
dependability includes attributes like availability, reliability, 
maintainability, safety, confidentiality, and integrity. Others 
have included survivability as an additional network 
dependability attribute, since it is so important to measure 
the resiliency of the network to provide partial service to the 
population of users during network service disruptions [6]. 

175
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
The higher the survivability, the better the chance a service 
provider has to satisfy customers in times of network stress 
due to component failures or traffic overloads. Integrity and 
confidentiality are not considered in the scope of this study. 
Rather, 
we 
consider 
RAMS 
attributes 
(reliability, 
availability, 
maintainability, 
and 
survivability) 
of 
dependability.  
B. Reliability  
Reliability is a function of how often we might expect 
failure. Conversely, the formal definition of network 
reliability is the probability that it will perform its required 
functions over a specific period of time [7]. The reliability 
for a network, a network service, or a network component is 
expressed as the probability that a network or component 
will not fail over some specified time period of interest, 
given by [5]: 
 
 
 ( )              
⁄
 
(1) 
 
Where λ is expected failure rate and MTTF (mean time to 
failure) is the average time between failures. If the time-
period of interest is reasonably short, MTTF is assumed to 
be constant, meaning that an assumption of a Homogeneous 
Poisson Process (HPP) can be made.  
C. Maintainability 
Maintainability is a function of how fast we can expect to 
recover from a failure. Network maintainability is defined as 
the ability of a network to recover from failures [8]. 
Maintainability can be determined from the Mean Time to 
Restore (MTTR). Restore time is a random variable and 
typically consists of three parts – detection time, travel time 
to the outage location and the actual repair or replacement 
time. In this research, the lognormal distribution is used, as 
travel time plays an important role.  
D. Availability 
There are two forms of availability – instantaneous and 
average. Network instantaneous availability is defined as the 
probability that a network is ready for use when needed [8].  
Average availability can be expressed as: 
 
 
MTTR
MTTF
MTTF
A


 
(2) 
 
Availability, being the fraction of time a network or network 
service is up, is a good metric to assess the state when the 
network is experiencing no problems due to failures.  
E. Survivability 
Availability is not always a good indicator of network 
dependability, as networks are very rarely “all-up” or “all-
down”. Rather, networks are “mostly-up”—or said another 
way, “fractionally-up”. Survivability is a measure that can 
capture this phenomenon. Network survivability is defined 
as the ability of a network to provide services to most 
customers under partial failures.  Snow [9] defined Prime 
Lost Line Hours (PLLH) as an impact measure for wire-line 
network outages that take into consideration usage levels at 
the time of the outage. PLLH is the product of the estimated 
number of customers impacted and the duration of an 
outage. Total Line Hours (TLH) is the product of the total 
number of customers served by the network and the total 
hours in the time-period of interest, resulting in a network 
survivability calculation in Equation (3). 
 
 
TLH
PLLH
NS
1
 
(3) 
 
The Telecommunication Committee T1, an ANSI-certified 
standards organization, developed the “outage index” as a 
survivability metric that includes consideration of the size 
and duration of the outage, in addition to the importance of 
the services affected by the outage. This metric uses weights 
for each of these three dimensions, and has been shown to 
be a questionable metric [10], [11], [12]. 
The organization of this paper follows. In Section II the 
concept of impact epochs is introduced, which represent 
multiple outages in time. In Section III, wireless voice 
infrastructure is introduced. Additionally, equipment and 
link reliability and maintenance are quantified. Then 
architectural scenarios investigated in this paper are 
presented. In Section IV the paper research questions are 
presented and discussed, while Section V introduces the 
simulation model used to address the research questions, 
and the assumptions and limitations of the model. Lastly, 
Sections V and VII present the results and conclusions, 
respectively. 
II. 
IMPACT EPOCH 
This research examines episodes where multiple outages 
overlap in time. Single outages impact some fraction of 
users. When they are coincident, the impact increases and 
challenges network operators. The focus of this research is 
on concurrent and time-overlapping component outages as 
the network size scales. In order to describe the 
characteristics of concurrent or overlapping outages from a 
network operator perspective, a new concept called impact 
epoch is introduced.  An impact epoch starts when a 
network transfers from a state of no customers impacted to a 
state of having customers impacted. It continues until the 
network returns to the state of having no customers 
impacted.  An impact epoch event includes single or 
multiple outages that overlap in time. The number of 
impacted customers during one impact epoch is not 
necessarily constant, since a single impact epoch may 
include more than one component outage due to nearly 
simultaneous failures in the network. An example of single 

176
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
impact epochs, consisting of two non-overlapping outages, 
is shown in Fig. 1 in the form of an epoch profile. Note that 
time is represented by the X-axis, and the Y-axis represents 
the percentage of customers served in the network. Each 
outage has a duration and a maximum impact. 
 
 
Figure 1.  Non-Overlapping Outage Epochs 
 
Next, refer to Fig. 2, which shows three different 
combinations of these two outages. Note how different these two 
events are, depending on degree of overlap. In the top profile, the 
two outages do not overlap and are separate epochs. In the 
middle profile, the outages combine into a single epoch with the 
same duration, but with a larger impact. Lastly, note the bottom 
profile, which has a different duration and impact. 
 
 
Figure 2.  Different Perspectives of Two Overlapping Outages 
 
Individual epochs are arrival events, and MTTE is 
defined as the mean time to impact epoch in a network. 
MTTE offers insights into the average interval before 
operators can expect disturbances that render the network 
incapable of satisfying all customers. Longer MTTE implies 
that the network has higher reliability, or the capacity and 
performance to lessen congestion events.  Since epochs have 
duration, MTRE specifies the mean impact epoch restore 
time - a description of a network’s maintenance response, or 
ability to recover gracefully from congestion. 
Shorter MTRE implies that the network has better 
maintainability or recoverability. MTRE together with 
MTTE provides the average quiescent time (AQ), or the 
fraction of time the network, on average, is not undergoing a 
disturbance that impacts customers. Quiescent availability 
can be determined by the following equation: 
 
 
MTRE
MTTE
MTTE
AQ


 
(4) 
 
Equation 3 can still measure survivability from an epoch 
perspective. However, in an environment where there may 
be concurrent or overlapping outages, peak customers 
impacted (PCI) may be of interest. For instance, in Fig. 2, 
epoch 2 has a larger PCI than epoch 1. 
The advantages of studying impact epochs instead of a 
single outage are that epochs: 
 
Provide 
a 
better-detailed 
description 
of 
the 
cumulative 
time-phased 
effect 
of 
network 
disturbances 
 
Offer a new way to evaluate network dependability, 
providing a different perspective important to 
network operators 
 
Provide insights into how characteristics such as 
frequency, duration, number of concurrent outages, 
and peak customers impacted might change as 
network size varies 
Table 1 illustrates the mapping between wireless network 
dependability attributes and the metrics developed in this 
paper to assess them. In this wireless network example, a 
Wireless Traffic Profile (WTP) is developed using empirical 
wireless 
traffic 
data 
from the 
literature, allowing 
computation of PCI and WPLLH (Wireless Prime Lost Line 
Hours).  
 
TABLE 1.  New Network Dependability Metrics 
Dependability 
Network Attribute Name 
Reliability 
Network Mean Time To Epoch (MTTE) 
Maintainability 
Network Mean Time Restore Time (MTRE) 
Availability 
Network Quiescent Availability (AQ) 
Survivability 
Peak Customer Impacted (PCI) 
Wireless Prime Lost Line Hours (WPLLH) 
 
In this study, outages are due to component failures. In 
other words, this is a fault management, rather than a 
performance management, perspective -- operators are 
responding to outage events induced by component failures, 
and the need to restore or replace the faulty components. 
Therefore, this work presents conservative estimates of 
episodic occurrences. 

177
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
III. 
WIRELESS NETWORKS 
Like 
all 
telecommunications 
providers, 
wireless 
operators are reluctant to share statistics on service outages. 
Even so, extensive research has been conducted over many 
years regarding the traditional wire-line telephone network, 
also called the Public Switched Telephone Network (PSTN). 
These research efforts helped wire-line networks offer very 
dependable services with a common quality metric of Five 
9’s availability [13]. On the other hand, research in the world 
of wireless communication, especially in cell phone 
networks, is relatively new. Research into wireless 
telephone network reliability did not receive much attention 
until the late 1990s. Over the last 22 years, the wireless 
network has grown at an amazing rate. According to the 
Cellular Telecommunications Industry Association (CTIA) 
wireless Quick Fact Sheet [14], cellular subscribers in the 
US surpassed 5 million in 1990 and doubled in just two 
years. By 2012, cellular subscribers exceeded 300 million in 
the US and wireless penetration rate was over 65%. There 
were over 327 million customers in the US as of June, 2012.  
In 1992, the FCC at first ruled that wire-line carriers had 
to report all outages that affected more than 50,000 
customers for at least 30 minutes. This threshold was 
quickly lowered to 30,000 customers for 30 minutes in 1993 
[10]. Thresholds for RAMS attributes have also been shown 
to be important in wireless networks [15]. Statistical failure 
data of wire-line local switches are publicly available from 
the 
FCC’s 
Automatic 
Reporting 
and 
Management 
Information System (ARMIS) database. However, starting 
January 2, 2005, the FCC ruled that wireless carriers also 
had to report their network outages to the FCC [16]. 
Meanwhile, the FCC established a four-year rollout plan for 
E911 phase II, which began in October 2001. Phase II 
required wireless carriers to provide precise location 
information for wireless 911 calls, within 50 to 300 meters 
in most cases [17].  
A. Wireless Network Infrastructure 
Wireless networks consist of components, such as cable 
and equipment. Additionally, equipment consists of both 
hardware and software. The general structure of a wireless 
network with most of the required functional components is 
shown in Fig. 3. They include the network operation 
subsystem, base station subsystem, and network switching 
subsystem. Each subsystem includes a number of 
components that are studied in this research. This is a 2G+ 
architecture that has some similarity to 3G/4G architectures 
from hierarchical and topological perspectives.  The Base 
Station Subsystem (BSS) is comprised of Base Stations 
(BS) and Base Station Controllers (BSC). A BS is 
essentially the radio station that broadcasts to and receives 
from the mobile station in a “cell”. A BSC is the controlling 
node for one or more cells or BSs and manages voice or 
data traffic and signaling messages for all the cells under its 
control. The BSS provides the transmission path including 
traffic and signaling between mobiles and the Network 
Service Subsystem (NSS) [18]. 
 
MSC
VLR
HLR
EIR
AuC
NSS
BSC
BSC
BS
BS
BS
BS
BS
BSS
MSC
VLR
BSC
BS
BS
BS
BSS
Anchor
Switch
PSTN
 
Figure 3.  Wireless Network Infrastructure 
 
 
The NSS is the switching and control portion of the 
entire wireless network. It is comprised of the Mobile 
Switching Center (MSC) and three intelligent network 
nodes known as the Home Location Register (HLR), Visitor 
Location Register (VLR), Equipment Identity Register 
(EIR), and the Authentication Center (AuC) [18]. The MSC 
is the central heart of a wireless network. The failure of a 
MSC typically results in communication loss of all users 
that the MSC controls, since calls cannot be originated or 
terminated. Carriers pay close attention to the status of a 
MSC since it supports billing functions such as collecting 
Call Detail Records (CDR).  A typical MSC is engineered to 
be highly reliable. In A. Snow, [19], the authors introduced a 
wireless network 
infrastructure 
called 
the 
Wireless 
Infrastructure Block (WIB). The scope of the WIB is from 
the BS to the MSC, including the HLR/VLR database. They 
also discussed how MTTF and MTTR in a WIB might affect 
the network’s dependability [19]. The topology used in a 
WIB is the star topology. Large wireless infrastructures 
consist of multiple WIBs. 
B. Wireless Traffic 
Wireless traffic, like all telecommunications traffic, 
varies widely over a single day. If equipment fails, or 
transmission links are severed, users are impacted. For 
faster restoration, providers use redundancy in equipment 
and links, and a topology that minimizes restoral times. 
Advantages of using the star topology include supporting 
modular expansion, as well as simplified monitoring and 
trouble-shooting. The largest disadvantage of star topology 
is the creation of a single point of failure, such as the MSC 
and database. Fortunately, these components are highly 
reliable. Table 2 indicates the number of components in a 
WIB along with the number of customers potentially 
impacted by each component. A WIB can serve up to 

178
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
100,000 customers. How many subscribers are actually 
impacted depends on utilization, which can be related 
historically to time of day and day of week. This can be 
represented by a time factor, which is really a time phased 
traffic profile that reflects percentage utilization at a point in 
time [20].  
 
TABLE 2.  No. of Components in One WIB and Maximum Failure Impact 
Component 
Number in One WIB 
Number Customers 
Potentially Impacted 
MSC 
1 
         100,000 
VLR/HLR DB  
1 
         100,000 
MSC-BSC link 
5 
          20,000 
BSC 
5 
          20,000 
BSC-BS link 
50 
            2,000 
BS 
50 
            2,000 
Anchor-MSC Link 
1 
        100,000 
Anchor Switch 
n 
       n  x 100,000 
Anchor Link 
n 
       n  x 100,000 
Note: n is the number of WIBs in the wireless infrastructure 
 
The time factor accounts for time-of-day and day-of-
week usage by customers. The goal of network engineering 
for carriers is to establish an infrastructure that satisfies peak 
hour traffic loading. Similar to traffic on a highway, voice 
traffic volume in networks varies over a day. According to 
historical statistics for wireline voice traffic [20], heavy 
traffic load in the wire-line network occurs between 9:00am 
and 4:00pm on weekdays. Taking traffic estimates into 
account, a network component failure occurring at different 
times may impact a different number of users. For example, 
a one-hour outage at 10:00am has much a larger impact than 
a one-hour failure at 3:00am in the morning. The time factor 
values, or utilization, for wire-line networks are summarized 
in Table 3 from [20].  
 
TABLE 3. Time Factor for Wire-Line Network 
Spanned                  Time Period 
Time Factor 
Day               (8:00am to 4:59pm, Mon. ~ Fri.) 
   1.0 
Evening        (5:00pm to 10:59pm, Mon. ~ Fri.)                 0.3 
Night            (11:00pm to 7:59am, Mon ~ Sun.)                 0.1 
Weekend      (8:00am to 10:59pm, Sat. & Sun.)                  0.2 
 
Say there is a failure of central office with 50,000 lines that 
lasts one hour. The number of affected customers is 1 x 
50,000 = 50,000 if the outage started at 10:00am. However, 
if the outage started at 3:00am the number of affected 
customers is 0.1 x 50,000 = 5,000. The product of time 
factor and telecommunications capacity is the impact of the 
outage, in line hours. As the time factor are fractions of full 
utilization during the prime times of the day, this impact has 
been called prime lost line hours, or PLLH [9], [10], [11].  
In this work, a new traffic profile for wireless networks is 
developed. This is because traffic patterns in wireless 
networks are different from that in PSTN. For instance, 
service charges in the PSTN are usually a flat monthly 
charge, while in a wireless networks there are more usage 
plans with differential charges based on the time of day a 
call is placed. For example, many cell phone plans offer free 
calls on weekends and after 9:00pm on weekdays. Some 
people could wait until 9:00pm to place calls and take 
advantage of this plan. Such phenomena results in different 
weekday and weekend traffic profiles in wireless networks. 
In Albaghdadi and Razvi [21], the authors studied an actual 
1320 cell GSM network. In that research, the results 
reported in this GSM network were used to develop five-day 
weekday traffic and weekend traffic profiles as shown in 
Fig. 4 and 5 respectively. The data is from [21] while the 
solid lines are added for this research to create a wireless 
time factor. These wireless time factors were developed to 
create a wireless PLLH outage impact metric, called 
hereafter the WPLLH, where the W denotes wireless. 
 
 
 
Figure 4.  Wireless Weekday Time Factor 
 
 
Figure 5.  Wireless Weekend Day Time Factor 
 
Because the interaction of reliability and maintainability 
attributes are expected to be complex when it comes to 
investigating multi-episodic events, three different scenarios 
are 
investigated 
as 
follows: 
 
nominal, 
degraded 
maintainability, and enhanced reliability and maintainability. 
The nominal scenario signifies that the network is operating 
within published reliability and maintainability norms, 
where regular maintenance schemes are used and reliability 
is stable. The degraded maintainability implies that the 
maintainability of the network is not as good as nominal, 
which signifies higher restore times from component 

179
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
failures. The enhanced reliability/maintainability scenario 
indicates that component reliability and maintainability are 
improved over nominal (with higher MTTFs and lower 
MTTRs).  
C. Network Component MTTF and MTTR 
Transmission links can be deployed with protection 
channels, wherein if the primary link is disrupted, the 
system switches to a protection channel. The more 
customers affected, the more likely there is a protection 
channel. Table 3 details a complete list of component 
MTTFs used in this study.  
 
TABLE 3.  Component MTTF and MTTRs Used in the Study 
Component 
Name 
Nominal 
MTTF 
(Years) 
Enhanced 
MTTF 
(Years) 
Degraded 
MTTR 
(Hours) 
Nominal 
MTTR 
(Hours) 
Enhanced 
MTTR 
(Hours) 
Anchor 
Link 
8.0 
8.0 
12.0 
4.00 
2.00 
MSC/Anchor 
Link 
8.0 
8.0 
12.0 
4.00 
2.00 
MSC-BSC 
Link 
2.7 
4.0 
12.0 
6.00 
3.00 
BSC-BS Link 
1.7 
2.7 
12.0 
6.00 
3.00 
MSC/Anchor 
switch 
7.5 
7.5 
0.51 
0.17 
0.12 
VLR/HLR 
database 
3.0 
4.5 
2.00 
1.00 
0.50 
BSC 
3.0 
6.0 
4.00 
2.00 
1.00 
BS 
2.0 
4.0 
4.00 
2.00 
1.00 
 
The nominal MTTF for other components was taken from 
[19]. As the MSC has become a very stable control and 
switch system over many years’ development and 
deployment, in this case, the nominal MTTF and enhanced 
MTTF of MSC are taken to be the same, which is 7.5 years 
based on the results derived from empirical local switch 
statistics in the Federal Communication Commission’s 
ARMIS database. 
Derivation of link MTTFs are also based upon empirical 
failure data for fiber optic links, and are derived here. As 
suspected, the MTTFs are greatly affected by power 
failures. 
As seen in the multi-WIB architecture of Fig. 3, 
transmission systems include BS-BSC links, BSC-MSC 
links, MSC-Anchor links and the Anchor link to outside 
networks. Fiber cable is the transmission medium of choice 
for these link systems. Although microwave systems are 
sometimes used where fiber runs are not cost-effective, we 
assume the wireless infrastructure to be interconnected by 
fiber transmission capabilities. Fig. 6, 7 and 8 show the 
typical structure of link systems. Link systems can be 
generally classified as one of three cases:  
 
Case A is the single-fiber system with no backup 
(shown in Fig. 6).  
 
Case B has redundant fiber media backup. 
Redundant circuits are supposed to take different 
physical paths (shown in Fig. 7).  
 
Case C has fiber media, transceiver, and power 
backup, while transceivers are hot standby (shown 
in Fig. 8). 
 
 
Figure 6. Unprotected Link 
 
 
Figure 7. Partially Protected Link 
 
 
Figure 8. Fully Protected Link 
 
From the multi-WIB infrastructure in Fig. 3, it is seen 
that although all links are important to a network’s 
dependability, the reliability levels vary from one type link 
to the next. For example, the BS-BSC links are relatively 
less important than BSC-MSC links from the network 
operators’ point of view. Similarly, BSC-MSC links are not 
as important as MSC-Anchor links. Each of the three link 
categories shown  have different reliability or MTTF. 
In Fawaz [22] fiber cable system reliability is discussed 
in detail. Statistics from Telecordia are referred to in that 
paper, where the authors came to three conclusions:  
 
The frequency of failure occurrence in optical 
network is not negligible. 
 
Cable cuts are the dominant failure scenario for long 
optical fiber networks.  
 
Power reliability is important in link reliability. 
Table 4 shows their results. In this table, Failure In Time 
(FIT) is the average number of failures in 109 hours [22]. 

180
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
TABLE 4.  Optical fiber and Transceiver Failure Rate [22] 
 Cable Cut rate FIT per 1000 miles 
501142 
 Cable Cut rate per hour, 1000 miles 
0.0005011 
 Cable Cut rate per year, 1000 miles 
4.390 
 Cable Cut MTTF per 1000 miles (Yr) 
0.228 
 Cable Cut MTTF per mile (Yr) 
228 
 Transceiver Failure rate FIT 
15178 
 Transceiver MTTF (Yr) 
8.0 
 US Telecom Power failure rate per year 
0.1252 
 Power MTTF (Yr) 
8.0  
 
The MTTF for links are different mainly because of the 
varying fiber length. If we assume that the hazard due to 
fiber cut will decrease linearly, the failure rate shown in 
Table 4 can be used in this work. For fiber links less than 10 
miles, the transceiver and power systems become the 
dominant contribution to failure, rather than the fibers. This 
means that the total link system’s MTTF is comparatively 
low for unprotected links. The MTTF of a parallel system 
with four components is about 1.6 times of the MTTF in the 
single system [9]. For instance, the MTTF of a 10-mile link 
without protection is given by: 
 
 
power
fiber
r
transceive
MTTF
MTTF
MTTF
MTTF
2
1
2
1
10



 
(5) 
 
 
years
MTTF
8.1
0.8
2
.78
22
1
0.8
2
1
10




 
(6) 
 
Likewise, the MTTF of a 10-mile, partially protected link is 
given: 
 
 
power
fiber
r
transceive
p
MTTF
MTTF
MTTF
MTTF
2
*
6.1
1
2
1
10



 (7) 
 
 
years
MTTF
p
9.1
0.8
2
*22.78
6.1
1
5.7
2
1
10




  
(8) 
 
Lastly a fully protected 10-mile link MTTF is given by: 
 
 
years
MTTF
MTTF
f
9.2
6.1
10
10



 
(9) 
 
Table 5 shows MTTFs of fiber links under different 
protection schemes at different distances. From the 
calculation results, we can see that the MTTFs of fiber links 
at a distance between 10 to 20 miles are very similar. 
 
TABLE 5.  Optical Fiber Link MTTF 
Link 
Length 
(Miles) 
Optical 
Fiber 
MTTF 
(Years) 
Unprotected 
MTTF 
(Years) 
Partial 
Protect 
MTTF 
(Years) 
Fully 
Protect. 
MTTF 
(Years) 
1 
222.79 
3.9 
3.8 
6.3 
5 
 45.56 
1.9 
2.0 
3.1 
10 
 22.78 
1.8 
1.9 
2.9 
20 
 11.39 
1.7 
1.8 
2.7 
 
According to the statistic data from US Census Bureau 
[23], the number of persons per square mile ranges from 
several hundred to over a thousand in metropolises such as 
Los Angeles, New York, Atlanta, and Phoenix. In this study, 
the following assumptions on fiber length and protection 
lead to the following MTTFs for links: 
 
Fiber link of BS-BSC is at 20 miles. 
 
Fiber link of BSC-MSC is at 5 miles level. 
 
Fiber link of MSC-Anchor switch is very short, less 
than 1 mile. 
 
Fiber link of Anchor switch-PSTN is very short, less 
than 1 mile. 
A component’s maintainability is represented by its 
MTTR. In order to understand the role that MTTR plays in 
dependability, three MTTR scenarios are used in the 
simulation: nominal, degraded, and enhanced.  Nominal 
MTTR was obtained from [19]. The degraded MTTR was 
taken as three times the nominal MTTRs, excepting 
switches. Table 3 also lists the component MTTRs used. 
The repair distributions are modeled based on a lognormal 
distribution, which is commonly used for long-tailed 
distributions when travel time is involved. To summarize: 
 
The 
nominal 
case 
uses 
reliability 
and 
maintainability levels from literature and empirical 
data 
 
The enhanced case uses improved reliability and 
maintenance levels 
 
The degraded case uses lower maintainability levels 
IV. 
RESEARCH QUESTIONS 
In this section, four major research questions are 
presented and discussed. Additionally, the assumptions 
made in addressing the research questions are listed. 
 
Research Question 1: How will the number of impact 
epochs and their composition (number of concurrent 
component outages making up epochs) change as the 
network size, component reliability, and component 
maintainability change? 
 
As customer demand increases in an area, the network 
size increases, and more components (equipment and links) 
are used. We expect that more component outages will occur 
as the network grows. The wireless infrastructure studied in 
this research, as shown in Fig. 3, indicates that the total 

181
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
number of components failing is expected to scale with 
network size. We also expect impact epochs to grow along 
with network size—however, what is the relationship 
between the number of epochs and the network size? Will 
this number linearly scale as the network grows? Notice that 
impact epochs include both single and concurrent outage 
epochs, and as we count overlapping outages as one epoch, 
we expect the total number of impact epochs to grow 
nonlinearly. 
Over time, how many impact epochs consist of more than 
one outage? The answer depends on several factors. First, 
more components mean more possible failures. So as the 
network grows bigger, the probability of simultaneous 
outages increases. This probability increases nonlinearly as 
the network size increases. The second factor is component 
MTTF. As component MTTF decreases, more component 
outages occur over a period of time. We expect multi-outage 
impact epochs to increase in a network as component 
MTTFs decrease. The third factor is component MTTR. If 
the repair time for a single outage increases, the probability 
for other outages happening during this repair interval 
increases. Thus we expect multi-outage impact epochs will 
increase as the component MTTRs increase. Network size, 
reliability, and maintainability interact in ways that make it 
difficult to predict either linear or non-linear behavior with 
regard to the number of impact epochs. This research 
investigates the relationship based on network size, 
reliability, and maintainability scenarios. 
 
Research Question 2: What fraction of time is the network 
in a non-episodic state as network size, reliability, and 
maintainability change? 
 
The percentage of time in one year that a network is in the 
quiescent state and non-quiescent state is insightful. The 
average quiescent availability is an important issue to 
network operators. The total non-episodic time is the sum of 
time that a network is in quiescent state over one year. It is 
expected that as the network size increases, the total time the 
network will be in a non-episodic state will decrease. 
This question deals with how network size, component 
reliability, and component maintainability affect the total 
non-episodic time in a wireless network. More frequent 
failures and increasing repair times should decrease 
quiescent time. However, overlapping outages could increase 
quiescent time. How these factors combine to effect total 
quiescent time is not obvious.  
Research Question 3: How will the dependability 
characteristics of impact epoch change with the network 
size, component reliability, and component maintainability? 
Impact epochs have a number of characteristics such as 
MTTE, MTRE, and the peak number of impacted customers. 
As a system, a wireless network’s MTTF is dependent upon 
all of its component’s MTTFs. As the network size increases, 
the network component outages increase linearly.  
MTTE is the mean time to epochs, instead of component 
failures within a wireless infrastructure. Because each epoch 
may be a single- or multi-component failure, the probability 
that an epoch includes more than one failure nonlinearly 
increases as the network size becomes bigger. So for MTTE, 
we expect it to decrease in a nonlinear fashion as the network 
expands.  
The second attribute of impact epochs that is investigated 
in this work is MTRE. Due to increases in simultaneous 
component outages, MTRE increases as a network grows. 
How long MTRE lasts depends on how many impact epochs 
are multi-outage epochs. The higher the percentage of multi-
outage epochs, the longer the MTRE will be. We expect a 
nonlinear growth on MTRE as the network becomes bigger. 
The third impact attribute investigated in the research is 
Peak Customers Impacted (PCI). This factor shows how 
serious an impact epoch could be in the dimension of impact 
size. If we can find the distribution of PCI, we may be able 
to provide network operators the probability of an impact 
epoch impacting more than a set number of customers over a 
period of time. For example, we may calculate the 
probability of PCI exceeds 8000 customers. This could be 
valuable information to network operators. 
 
Research Question 4: How will different thresholds help 
network operators filter impact epochs in a network? 
 
Peak customers impacted (PCI) provides information of 
an epoch in only one dimension. Another perspective is one 
that considers size and duration of an epoch. In this research, 
the two-dimensional metric called WPLLH, discussed 
earlier, is also used to measure impact epoch. The WPLLH 
uses the wireless traffic profile developed earlier, rather than 
wire-line PLLH usage time factor.  
Thresholds are powerful tools in network management 
because network operators usually prioritize their activities 
to respond to more important events in their networks. In this 
thesis, three different thresholds are investigated—5000 
WPLLH, 10,000 WPLLH, and 15,000 WPLLH. The number 
of impact epochs over these thresholds is expected to grow 
as network size increases, or component reliability or 
component maintainability decreases. The number of epochs 
exceeding a threshold will change from one threshold to 
another. For example, the number over 15K WPLLH 
threshold may not change as fast as the number of epochs 
over a 5K WPLLH threshold. This is because epochs over 
15K WPLLH rarely occur in smaller networks. This applies 
to different scenarios. For example, the number of epochs 
over 15K WPLLH is certainly less than that in a degraded 
reliability and maintainability scenario. 
V. 
SIMULATION MODEL 
Fig. 9 displays the input and output process of the 
simulation and the derived results, while Fig. 10 shows the 

182
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
architecture simulated. Inputs for the simulation include all 
component MTTRs and MTTFs, wireless traffic profile, the 
network size, and an operational time of one year.   
 
 
Figure 9.  Process of Simulation and Results 
 
Outputs from the program are network survivability as well 
as detailed outage information including start time, stop 
time, the number of customers impacted, and the WPLLH 
for each outage. Other results like MTTE, MTRE, PCI, and 
quiescent availability are derived from these simulation 
outputs using MS ExcelTM. 
To fully investigate effects of different levels of reliability 
and maintainability for different size networks (size 
determined by the number of WIBs), we investigate three 
scenarios: nominal, degraded maintainability as well as 
enhanced reliability and maintainability. The maximum 
deviation in the nominal scenario between the simulation 
output and the analytical result was 0.85% for 8 WIB’s,  
which was acceptable. This verified the simulation. Direct 
simulation program outputs include outage numbers, start 
time, end time, impacted customers, WPLLH, and duration 
of each component outage.  An example of a simulation 
output is revealed in Table 6, showing four component 
outages, starting at 308.465 days into the year.  
 
TABLE 6.  Simulation Output Example for A 10 WIB Network 
Failure Start Time 
(Days into Year) 
Failed 
Component 
WIB Number 
Duration 
(Hours) 
308.465 
Base Station 32 
6 
6.55 
308.694 
Base Station 15 
5 
1.50 
308.698 
Base Station 5 
4 
2.90 
309.292 
BSC-BC-Link 41 
10 
6.52 
 
Fig. 11 illustrates the impact epoch over the simulation time. 
The Quiescent Time can be derived from direct outputs of 
the simulation program and is calculated as: 
 
 







n
i
i
n
i
i
t
TRE
TotalSimulationTime
TTE
Q
1
1
 
(10) 
 
where n is the number of quiescent periods.  The sum of all 
TTEs and all TREs should equal the total simulation time, 
as shown in Fig. 11.   
 
 
Figure 10.  Scalable Network Size 
 
Figure 11.  Relationship of TTE, TRE and Simulation Time 
 
Likewise, we expect the MTTE (mean of all times to epochs 
TTE), MTTR (mean of all times to restore epochs TRE) and 
total simulation time to be: 
 
 
n
TTE
MTTE
n
i
i



1
 
(11) 
 
 
n
TRE
MTRE
n
i
i



1
 
(12) 
 
n
MTRE
MTTE
Simulaiton Time
Total



)
(
_
_
 
(13) 
 
A. Further Model Verification 
The discrete time event simulation model was written in 
VC++. All components/links in the WIB(s) are in service 
simultaneously. Component times to fail are exponential, 
while repairs times are lognormal. Simulated failure counts 
were exhaustively compared to expected component failure 
counts, while simulated repair times were compared to fitted 
lognormal repair distributions. The null hypotheses of “no 
difference” were accepted at high degrees of inference, 
using chi-squared test statistics. 

183
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Next, network survivability was checked for consistency 
against the three different scenarios investigated, and 
compared to analytic calculations. As explained earlier, 
survivability is defined as the fraction of WPLLH offered 
over a one year operation:  
 
 
Network Survivability =  
TLH
n
WPLLH

1
 
(14) 
 
 
TLH
n
WPLLH
WPLLH
WPLLH
n
m
i
i
AL
A







1
)
(
1
 
(15) 
 
where:  
 
TLH is Total Line Hour for one WIB  
(365×24× 100,000); 
 
WPLLH is Wireless Prime Lost Line Hour; 
 
i is the number of outages in the network; 
 
n is the number of WIB in the wireless 
infrastructure; 
 
WPLLHA is the prime lost line hours because of the 
anchor switch outage; 
 
WPLLHAL is the prime lost line hours because of the 
anchor link outage; and 
 
WPLLHi is the prime lost line hours for the ith WIB 
in the network. 
Based on the infrastructure used in this research, each WIB 
has the same structure, reliability, and maintainability 
levels, meaning that same-type component MTTF and 
MTTR are the same for each WIB in the architecture. So we 
may expect that each WIB will generate similar numbers of 
outages, outage repair times, and WPLLH. From the above 
equation, we can see that factors affecting network 
survivability are WPLLHA and WPLLHAL. As any failure of 
the anchor link or anchor switch will impact the entire 
network no matter how many WIBs are in the infrastructure, 
we expect that the network survivability will stay relatively 
constant in each scenario, as survivability is the fraction of 
user hours available over a time period. However, nominal, 
degraded maintainability—as well as enhanced reliability 
and maintainability 
scenarios—will 
exhibit 
different 
network survivability levels. We expect that enhanced 
scenario to have the highest survivability because we expect 
the least outages. In contrast, the degraded maintainability 
scenario should have the lowest survivability because we 
expect the most outages.  
The network survivability simulation results for each of 
the three scenarios is seen in Fig. 12. As expected, the 
enhanced network has the highest survivability and the 
degraded network the lowest. Also, for each scenario the 
network survivability remains constant for different network 
sizes, as expected. It also indicates that the simulation is 
verified with the new wireless traffic profile. In addition, the 
survivability by scenario and size were compared to analytic 
predictions and compared by chi-squared statistics. 
 
 
Figure 12.  Network Survivability by Scenario 
B. Assumptions and Limitations 
 
The model is subject to the following assumptions and 
limitations: 
 
This work considers outages due to equipment and 
link failures (“components”), and does not focus on 
network disturbances due to traffic congestion. 
 
The 
wireless 
network 
under 
study 
is 
an 
infrastructure with an anchor switch as the gateway 
connecting to outside networks, such as the PSTN or 
other wireless networks. The anchor switch acts as 
the only interface to the outside world. All MSCs in 
this network will route their traffic with outside 
destinations to the anchor switch for further routing. 
 
An anchor switch is assumed to have the same 
dependability features as any other MSC in the 
network. The MSC has become very stable and 
reliable over many years of development.  
 
The network topology is a star topology, which is very 
popular in practice. The star topology distributes 
network functionalities geographically. It is assumed 
that there are no mesh topologies in the network. 
 
Structure and scale of all WIBs within the wireless 
infrastructure are the same. 
 
Nominal component MTTFs and MTTRs are based on 
published literature [10] and are not based on 
empirical data collected for this research. 
 
Component MTTF and MTTR are invariant over a 
one-year period. TTFs are exponentially distributed, 
consistent with a homogeneous Poisson process. TRs 
are lognormally distributed, consistent with long tail 
distributions to account for travel time. 
 
The impact on network dependability caused by 
anomalous propagation is not in the scope of this 
research as it relates to single outage. 
 
Fractional component failures are not considered in 
the research. 
 
Inter-WIB traffic is not modeled; however, impact 

184
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
epochs in the research do include both incoming and 
outgoing communication loss. 
 
Optimal reliability and maintenance strategies are 
not addressed, as cost is not part of this research. 
VI. 
RESULTS 
Research Question 1: How will the number of impact 
epochs and their composition (number of concurrent 
component outages making up epochs) change as the 
network size, component reliability, and component 
maintainability change? 
 
The number of impact epochs increases as the network 
expands in all three scenarios, since newly added WIBs in a 
wireless infrastructure will contribute more component 
outages. Fig. 13 illustrates the relationship between the total 
numbers of impact epochs at a different network size for 
each scenario over a one-year interval.   
 
 
Figure 13.  Total Number of Impact Epochs 
 
Remember, this also includes single-outage epochs. The 
nominal and degraded scenarios both use nominal MTTF; 
therefore the expected number of single component failures 
in these two scenarios should be at the same level when the 
network size is small (such as 1 or 2 WIBs), since the 
number of impact epochs is approximately the same. As the 
network size increases, the nominal scenario has more 
impact epochs as compared to the degraded maintenance 
scenario, since longer repair times mean that fewer 
components online at any instant can fail. As it turns out, the 
degraded case has less epochs, but more multi-outage 
epochs. Remember – a 1-WIB network serves 100,000 
customers, while a 10-WIB network serves 1,000,000. 
For larger networks that do not have enhancements in 
component reliability and maintainability, expanding a 
network presents challenges for network operators who 
must cope with impact epochs consisting of multiple 
outages that overlap in time. Repairing simultaneous 
outages is challenging in large networks especially because 
of geographic dispersion, which requires more maintenance 
staff, equipment, and vehicles. Component maintainability 
must be achieved even though there are simultaneous 
outages in the network. See Tables 7, 8, and 9 to see how the 
frequency of multi-outage epochs decreases as reliability 
and maintainability improve. 
 
TABLE 7. Frequency of Multi-Outage Epochs: Degraded Scenario 
 
TABLE 8.  Frequency of Multi-Outage Epochs: Nominal Scenario 
No. Outages        
in Epoch 
Number of WIB 
2 
4 
6 
8 
10 
1 
105 
1 
105 
1 
105 
2 
4 
2 
4 
2 
4 
3 
0 
3 
0 
3 
0 
4 
0 
4 
0 
4 
0 
5 
0 
0 
0 
0 
1 
6 
0 
0 
0 
0 
0 
7 
0 
0 
0 
0 
0 
 
TABLE 9.  Frequency of Multi-Outage Epochs: Enhanced Scenario 
No. Outages     
in Epoch 
Number of WIB 
2 
4 
6 
8 
10 
1 
94 
154 
183 
198 
205 
2 
9 
31 
49 
62 
70 
3 
1 
8 
12 
23 
30 
4 
0 
1 
5 
9 
17 
5 
0 
0 
1 
4 
7 
6 
0 
0 
0 
2 
4 
7 
0 
0 
0 
1 
2 
8 
0 
0 
0 
0 
0 
 
Research Question 2: What fraction of time is the network 
in a non-episodic state as network size, reliability, and 
maintainability change? 
 
The simulated number of multi-outage epochs for each 
network size and scenario is displayed in Fig. 14. The curve 
increases almost linearly for networks in the degraded and 
nominal scenarios after network size exceeds 2 WIBs. The 
rate of growth slows down significantly in the enhanced 
scenario. Table 10 indicates that nearly 40% of the total 
impact epochs are multi-outage epochs in a 10-WIB 
network with the degraded scenario. This situation improves 
in the enhanced scenario, where less than 8% of total impact 
epochs include more than one outage. 
 
No. Outages in 
Epoch 
 
Number of WIB 
2 
4 
6 
8 
10 
1 
65 
125 
189 
234 
281 
2 
1 
4 
9 
15 
20 
3 
0 
0 
0 
1 
1 
4 
0 
0 
0 
0 
0 

185
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
Figure 14.  Multi-Outage Epoch Number 
 
TABLE 10.   Multi-Outage Impact Epoch Composition 
# WIB 
≥ 2 concurrent outages 
≥ 3 concurrent outages 
Degraded Nominal 
Enhanced 
Degraded Nominal Enhanced 
2 
9.8% 
4.6% 
1.6% 
1.1% 
0.3% 
0 
4 
20.1% 
9.5% 
3.3% 
4.6% 
1% 
0 
8 
33.5% 
18.3% 
6.3% 
12.7% 
4.2% 
0 
10 
39.5% 
23.2% 
7.7% 
17.9% 
6.5% 
< 0.9% 
 
The difference between the degraded and enhanced 
scenarios is significant. The percentage of network epochs 
in the degraded scenario increases from 4.6% to 17.9% as it 
expands from 1 to 10 WIBs. The range is from 0.3% to 
6.5% for networks in nominal scenarios. While in an 
enhanced scenario network, the 3-or-more outage epoch 
virtually disappears. Notable differences occur among three 
scenarios involving the multi-outage epochs. In the 
enhanced scenario, impact epochs consisting of more than 2 
concurrent outages rarely happen, even when a network 
expands to serve 1 million customers. However, in the 
degraded scenario, when the network has 6 WIBs, the 
composition of impact epochs consisting of more than 2 
concurrent outages is 7%. When the network has 10 WIBs, 
the number is 18%. Concurrent outages become a huge 
challenge for network operators in the degraded scenario, 
especially when network size grows. 
The results of the network quiescent days for each 
scenario are shown in Fig. 15.  As the network expands, its 
quiescent availability decreases, almost linearly. In the 
degraded scenario, the total non-episodic time of a WIB 
network is 345 days over a one-year operation time. By 
contrast, for a 10-WIB network, the number is only 213 
days, which demonstrates that the network is in an episodic 
state 42% of the time.  In the nominal scenario, which has 
the same reliability as the degraded scenario, the total non-
episodic time of a 1-WIB network is 355 days, and 272 days 
for a 10-WIB network. This implies that 25% of the time the 
nominal network is in an episodic state for a 10-WIB 
network, which is approximately a 30% improvement over 
the degraded scenario. 
 
 
Figure 15.  Percentage of Quiescent Availability 
 
Research Question 3: How will the dependability 
characteristics of impact epochs change with the network 
size, component reliability, and component maintainability? 
The nominal and degraded scenarios use the same 
component reliability or MTTF. The difference is the 
component 
maintainability. 
Meanwhile, 
the 
nominal 
scenario is different from the enhanced scenario for both the 
component reliability and the maintainability. Fig. 15 shows 
the quiescent availability of a network in different scenarios.  
The nominal curve lies between the enhanced and degraded 
curves.  Thus, the component maintainability, rather than 
reliability, is more decisive to the network quiescent 
availability. Efficient management of maintenance resources 
seems to have a positive impact on sustaining a network and 
avoiding an episodic status. 
There are four important attributes of an impact epoch: 
MTTE, MTRE, PCI, and WPLLH. MTTE is the average 
time between two impact epochs, which is used to model the 
network’s reliability. MTRE is the average time to repair an 
impact outage in the network, and is a measure of the 
network’s maintainability. PCI and PLLH are subsequently 
used to model the wireless network’s survivability. 
A. Mean Time to Epoch and Mean Time to Restore Epoch 
Results demonstrate that MTTE decreases nonlinearly, as 
expected, as the network size increases for each scenario. In 
all three scenarios, MTTE decreases quickly as the network 
grows from 1 to 3 WIBs, and the rate of decrease slows after 
3 WIBs. The MTTE in degraded and nominal scenarios are 
very similar, as they have the same reliability. This is 
because single component outages are still dominant when 
the network is less than 3 WIBs. After that, as the network 
size increases, the overlapping phenomenon begins to play 
an important role in determining the total number of impact 
epochs. 
 

186
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
Figure 16.  Different Overlapping Patterns 
 
MTRE is expected to increase as outage overlapping 
occurs. How much overlapping affects MTRE depends upon 
the pattern of the overlapping. There are several different 
overlapping patterns that could occur, shown in Fig. 16 A, 
B, C, D. Among these four patterns, pattern “A” does not 
increase TRE, since repair time of the second outage 
completely occurred within the repair time of the first 
outage (TRE in pattern “A” equals the MTTR of component 
one). Pattern “B” has a small degree of overlap and effect 
on TRE while pattern “C” has a moderate impact on TRE.  
Pattern “D” overlap is nearly sequential, having the largest 
impact on TRE.  All these types of overlapping patterns may 
impact MTRE. Fig. 17 illustrates the simulation output of 
the MTRE changes due to network size. 
As expected, MTRE in the degraded maintainability 
scenario increased nonlinearly as the network expanded, due 
to overlapping outages. As the network grows, more 
overlapping instances occurred and the chance of 
overlapping pattern “A” increased, thereby decreasing 
MTRE. The component maintainability in the degraded 
scenario is lower than that in the nominal and enhanced 
scenarios. The MTRE of a 10-WIB network in the degraded 
scenario increased by approximately 28% (about 144 
minutes) from the 1-WIB network, while a 10-WIB network 
in the enhanced scenario increased by only 5.4 minutes 
longer than the 1-WIB network. 
B. Peak Customers Impacted 
A question that a network operator may ask is, “What is 
the chance an impact epoch affecting more than 10,000 
customers will occur in the next 30 days?” Understanding 
the distribution of peak customers impacted can provide 
insights into such questions.  The PCI for each simulation 
run was collected and the data was fitted to an Exponential 
Distribution [24] with a high degree of significance (p value 
less than 0.0001).  This allowed easy calculation of 
probabilities of peak outages. Table 11 shows the 
exponential PCI means.  
Table 12 displays the probability of a PCI greater than or 
equal to 10,000 customers in 30 days for different scenarios 
and network sizes, along with the same results for a PCI 
greater than or equal to 5,000 customers. Larger networks 
have higher probabilities due to the additive nature of 
outages in epochs. 
 
 
Figure 17.  MTRE in Hours 
 
TABLE 11.  Mean PCI per Epoch 
Scenario 
Number of WIB 
2 
4 
6 
8 
10 
Degraded 
2,932 
3,296 
3,509 
4,407 
4,549 
Nominal 
2,154 
2,227 
2,579 
2,702 
2,766 
Enhanced 
2,176 
2,246 
2,526 
2,654 
2,695 
 
TABLE 12.  Probability of PCI Over 10,000 and Over 5,000 Customers in 
30 Days 
Scenario 
Name 
Number of WIB 
(over 10,000) 
Number of WIB 
(over 5,000) 
 
2 
4 
8 
10 
2 
4 
8 
10 
Degraded  
3.3% 
4.8% 
10.3% 
11.1% 
18.2% 
21.9% 
32.1% 
33.3% 
Nominal  
1.0% 
1.1% 
2.5% 
2.7% 
10.0% 
10.6% 
15.7% 
16.4% 
Enhanced  
1.0% 
1.1% 
2.3% 
2.4% 
10.0% 
10.7% 
15.1% 
15.6% 
 
C. WPLLH  
Similarly, the distribution of WPLLH values for networks 
of different sizes and scenarios is illustrated in Table 13. 
These results can predict the probability of PLLH over a 
threshold for a given time period.  
 
TABLE 13.  WPLLH Mean 
Scenario 
Name 
Number of WIB 
2 
4 
8 
10 
Degraded 
13,867 
18,367 
25,094 
25,367 
Nominal 
6,409 
6,640 
8,088 
8,257 
Enhanced 
3,550 
3,735 
4,042 
4,506 
 
Research Question 4: How will different thresholds help 
network operators filter impact epochs in a network? 
 
The chance of the PCI and the WPLLH over a certain 
threshold is much higher in the degraded scenario than that 
in the nominal and enhanced scenarios. For example, the 
chance of an epoch in which the PCI is over 10,000 

187
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
customers over 30 days in the degraded network is three-to-
five times that of the enhanced scenarios.  Thresholds are 
useful for network operators in effectively monitoring 
networks, given that they filter out lower-priority epochs. In 
this paper, three different WPLLH threshold levels are used 
as filters: 5K WPLLH, 10K WPLLH and 15K WPLLH.  A 
5K WPLLH denotes that the product of impacted customers 
and impacted duration in an epoch is 5,000.  For example, it 
could mean 5,000 customers are impacted for one hour, or it 
could signify that 10,000 customers are impacted for half an 
hour. Fig. 18 indicates the relationship between the numbers 
of impact epochs versus different thresholds for the 
degraded scenario.  
 
 
 
Figure 18.  Number of Impact Epochs with Filters in Degraded Scenario 
 
The growth rate of impact epochs over 5K WPLLH in all 
three scenarios increases rapidly as the network expands in 
size. At the size of 10 WIB, in the enhanced scenario, the 
number of impact epochs over 5K WPLLH is 4 times more 
than enhanced scenario. 
This implies that in any scenario where a network 
expands, the number of impact epochs over a lower 
threshold can be expected to grow quickly. A network in the 
degraded scenario has to deal with a large number of epochs 
over higher thresholds because they grow in number at a 
much faster rate than that in the enhanced scenario. These 
insights should aid in network operators’ ability to set 
efficient thresholds. Set too low, a threshold masks 
important outages; set too high, and too many less 
significant outages are seen. 
 
VII. CONCLUSION 
New dependability metrics have been developed here to 
investigate concurrent multiple outage epochs. Results 
indicate that in large networks, the epoch perspective is 
useful in understanding the complex nature of ongoing 
concurrent failures. With these new metrics, operators can 
calculate such things as the probability of a 3-outage epoch 
over a time period and the probability of an epoch 
exceeding a specified peak over a time period. Such 
information is useful to operators in allocating resources.  
Significant contributions of this work include: 
• 
Defining the impact epoch as a new way to evaluate 
wireless network infrastructure’s dependability. 
• 
Developing new metrics for analyzing RAMS for 
large 
networks 
(MTTE, 
MTRE, 
Quiescent 
Availability, PCI and WPLLH). 
• 
Development of empirically derived wireless traffic 
profiles to determine the number of customers 
impacted by component failures by time of day and 
day of week. 
Important conclusions include: 
• 
An impact epoch perspective gives key insights into 
network dependability. Lacking empirical outage 
data, these perspectives are best investigated with 
simulation. 
• 
Component maintainability has a large effect on a 
network’s 
quiescent 
availability. 
Effective 
monitoring and efficient management of repair 
resources can shorten the time when a network is in 
an episodic state. 
• 
The number of small network impact epochs is not 
critical.  
With respect to the last point, network operators should be 
very careful when expanding their infrastructure in order to 
accommodate more customers. Results here indicate that the 
number of concurrent outage epochs is sensitive to both 
component reliability and maintainability. Reliability and 
maintainability should not be degraded in the expanded 
network. Additionally, it may be necessary to increase 
reliability and/or maintainability in order to keep multi-
outage epochs to an acceptable  minimum. 
REFERENCES 
[1] Andrew Snow, Gary Weckman, and Yachuan Chen, “Multi-Episodic 
Dependability Assessments for Large-Scale Networks”, ICN 2011: 
The Tenth International Conference on Networks, pp.  441 to 448, 
IARIA, St. Maarten, The Netherlands Antilles, January 23-28, 2011, 
ISBN: 978-1-61208-113-7. 
[2] S Hariri, S., et al, “Impact analysis of faults and attacks in large-scale 
networks”, Security & Privacy, IEEE Sept.-Oct. 2003, Volume: 1, 
Issue: 5, Page(s): 49 – 54. 
[3] B. Bassiri and S. Heydari, “Network Survivability in Large-Scale 
Regional Failure Scenarios”, Proceedings C3S2E '09 Proceedings of 
the 2nd Canadian Conference on Computer Science and Software 
Engineering, Pages 83-87, ISBN: 978-1-60558-401-0, ACM New 
York, NY, USA 2009.   
[4] J. Appleyard, “Outage management portal leveraging back-end 
resources to create a role and user tailored front-end interface for 
coordinating outage responses”, Patent US 8,171,415 B2, May 1, 
2012.  
 [5] Avižienis, A., Laprie, J.-C., Randell, B., & Landwehr, C. (2004). Basic 
Concepts and Taxonomy of Dependable and Secure Computing. IEEE 
Transactions on Dependable and Secure Computing, 1(1), (pp. 11–33). 
[6] J. Knight, E. Strunk, and K. Sullivan, “Towards a rigorous definition of 
information system availability”. Proceedings of the DARPA 
Information Survivability Conference and Exposition, IEEE, 2003. 

188
International Journal on Advances in Networks and Services, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/networks_and_services/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[7] A. Birolini, Reliability Engineering: Theory and Practice, Six edition, 
Springer, 2010. 
[8] M. Ayers, Telecommunications System Reliability Engineering, 
Theory, and Practice, IEEE Press Series on Network Management, 
John Wiley and Sons, 2012.. 
[9] A. Snow, “A survivability metric for telecommunications: insights and 
shortcomings”, 1998 Information Survivability Workshop – ISW’98 
IEEE Computer Society, FL, October, 1998, pp.135-138. 
[10] A. P. Snow, “Assessing pain below a regulatory outage reporting 
threshold”, Telecommunications Policy”, Vol. 28/7-8, 2004, pp. 523-
536. 
[11] Andrew P. Snow, “The failure of a regulatory threshold and a carrier 
standard in recognizing significant communication loss”, TPRC 2003, 
November 2003. 
[12] Carol Y. Carver and Andrew P. Snow, “Assessing the impact of a 
large-scale telecommunications outage”, Proceedings of the 7th 
International Conference on Telecommunication Systems, March, 
483-489, 1999. 
[13] H. Cankaya, A. Lardies, and G. Ester, “Availability-aware analysis 
and evaluation of mesh and ring architectures for long-haul networks”, 
Applied Telecommunications Symposium, pp. 116 – 121, 2004. 
[14] CTIA Wireless Quick Facts, retrieved from www.ctia.org in 
September 2012. 
[15] U. Varshney and A. Malloy, “Multilevel fault-tolerance for designing 
dependable wireless networks”, Proceedings of the 36th Annual 
Hawaii International Conference on System Sciences, HICCS 03, 
IEEE, Jan. 2003. 
[16] FCC , Report and Order and Future Notice of Proposed Rule Making, 
retrieved from http://www.fcc.org in August, 2005. 
[17] FCC, Public Safety and Homeland Security Bureau, 
www.fcc.gov/911/enhanced/, retrieved September 2012. 
[18] K.. Du, and M. Swamy, Wireless Communication Systems: From RF 
Subsystems to 4G Enabling Technologies, Cambridge University 
Press, 2010. 
[19] A. Snow, U. Varshney, and A Malloy, “Reliability and survivability of 
wireless and mobile networks”., IEEE Computer Magazine, July, 
2000, pp. 49-55. 
[20] Committee T1,  T1A1.2 Working Group on Network Survivability 
Performance, “Enhanced Network Survivability Performance”, 
Technical Report, November 2000. 
[21] M. Albaghdadi and K. Razvi, “Efficient transmission of periodic data 
that follows a consistent daily pattern”, 9th IFIP/IEEE International 
Symposium on Integrated Network Management, IEEE Operations 
Center, Piscataway NY, pp 511-526, 2005. 
[22] W. Fawaz, F. Martignon, K. Chen, and G. Pujolle, “A novel 
protection scheme for quality of service aware WDM 
networks”, IEEE, 0-7803-8939-5, 2005. 
[23] U.S. Census Bureau reports, retrieved from 
http://quickfacts.census.gov/qfd/states/ in September, 2012. 
[24] BestFit 4.5, Palisade Corporation. 
 
 

