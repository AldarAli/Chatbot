Elckerlyc goes Mobile
Enabling Technology for ECAs in Mobile Applications
Randy Klaassen, Jordi Hendrix, Dennis Reidsma, Rieks op den Akker
Human Media Interaction
University of Twente
Enschede, Netherlands
PO Box 217, 7500AE Enschede, Netherlands
{r.klaassen, j.k.hendrix, d.reidsma, h.j.a.opdenakker}@utwente.nl
Abstract—The fast growth of computational resources and
speech technology available on mobile devices makes it pos-
sible for users of these devices to interact with service sys-
tems through natural dialogue. These systems are sometimes
perceived as social agents and presented by means of an
animated embodied conversational agent (ECA). To take the
full advantage of the power of ECAs in service systems,
it is important to support real-time, online and responsive
interaction with the system through the ECA. The design of
responsive animated conversational agents is a daunting task.
Elckerlyc is a model-based platform for the speciﬁcation and
animation of synchronised multimodal responsive animated
agents. This paper presents a new light-weight PictureEngine
that allows this platform to embed an ECA in the user interface
of mobile applications. The ECA can be speciﬁed by using the
behavior markup language (BML). An application and user
evaluations of Elckerlyc and the PictureEngine in a mobile
embedded digital coach is presented.
Keywords-Mobile User Interfaces
I. INTRODUCTION
Advances in user interface technology — speech recog-
nition, speech synthesis and screen capacities — allow
more and more people to engage in spoken interaction with
services on their mobile phones. Examples of these services
are intelligent personal assistants in search applications,
persuasive systems or characters in games. It is well known
from user studies that the use of a talking head or an
embodied conversational agent (ECA) has a positive effect
on user experience when using these kind of services [1].
The presentation of a service agent by means of a persona
supports the idea of the computer as a social actor. Research
has shown that animation of human-like social behaviours
and expressions by means of a virtual human or embodied
conversational agent strengthens the impression that the
agent is present and engaged in the interaction. In human-
human conversations the one who has the speaker role is
monitoring his addressees while speaking. Listeners give
backchannels, short comments, and may also interrupt the
speaker. By his gaze behaviour the speaker shows his interest
with the addressee. By adjusting or stopping his speech he
Figure 1.
An example of a BML speciﬁcation for an ECA.
shows being responsive to the listeners comments and that
he is really engaged in the conversation. Gaze behaviour in
conversations is important for interaction management, in
particular for signaling that one wants to have the ﬂoor, that
the speaker wants to keep the ﬂoor or is willing to yield
the ﬂoor. Expressions of emotion are prime indicators of
engagement in what is going on in the conversation [2]. In
designing virtual humans that are able to show these social
signals and responsiveness one needs well designed model-
based speciﬁcation languages and tools.
The SAIBA framework [3] provides a good starting
point for designing interactive virtual humans. Its Behaviour
Markup Language (BML) deﬁnes a speciﬁcation of the form
and relative timing of the behaviour (e.g. speech, facial
expression, gesture) that a BML realizer should display
on the embodiment of a virtual human. An example of a
speciﬁcation in BML can be found in ﬁgure 1. Elckerlyc is
a state-of-the-art BML realizer. In [4] its mixed dynamics
capabilities are described as well as its focus on continuous
interaction, which makes it very suitable for virtual human
applications requiring high responsiveness to the behaviour
of the user.
The Elckerlyc platform can act as a back-end realizer for
different embodiments, like physical robots or realistic 3D
41
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

full kinematic virtual humans. Using a full 3D virtual human
on a mobile phone is too heavy in terms of processing power
and battery usage. To be able to use the Elckerlyc platform
on a mobile phone a light-weight animation embodiment
is needed. This paper presents the PictureEngine, a light-
weight animation embodiment that enables our SAIBA-
based BML realizer to be implemented and run on mobile
applications. Section II describes the Elckerlyc platform in
more detail. The PictureEngine will be discussed in Section
III, the Android implementation of the platform and the
PictureEngine in Section IV.
Research by e.g. Bickmore [1] showed that personiﬁcation
of the user interface of coaching systems can have positive
effects on the effectiveness of the coaching program. Real-
time animations do have a positive effect on the user expe-
rience. Compared to static pictures or prerecorded movies,
real-time animations are able to react immediately to the
user. Responsiveness increases the experience of engage-
ment of the agent. In section V a personalised context-
aware multi-device coaching application will be discussed.
The coaching application makes use of the mobile Elckerlyc
platform. The ECA developed for this application presents
feedback from the digital coach by animated spoken inter-
action. We conclude with with a description future work
on the development of the mobile embodied coach and
user evaluations of a coaching application that is using the
PictureEngine.
II. THE ELCKERLYC PLATFORM
In behaviour generation, at least two main aspects
can be distinguished. The ﬁrst aspect is the planning of
the actions and movements as means to a certain goal
that the agent intends to achieve. The second one is the
actual detailed realisation of the verbal and non-verbal
behaviours in terms of “embodiments” of the (graphical)
virtual human - including the generation of the speech by
a text-to-speech synthesizer. This distinction between intent
planning, behaviour planning and behaviour realisation
is the basis of the SAIBA1 framework [5]. According to
this framework the detailed behaviours are speciﬁed in the
Behaviour Markup Language (BML)[6].
The Elckerlyc platform is a BML realizer for real-time
generation of behaviours of virtual humans (VHs). The
Elckerlyc platform has been described and compared with
other BML behaviour realizers (for example EMBR [7] and
Greta [8]) in various papers [9], [10], [4].
Dependent on the application and task that the intelligent
system has, the virtual human presents for example the
character of a tutor, an information assistant, or a conductor.
The goal is to make these embodied conversational agents
look like believable and convincing communicative partners
1www.mindmakers.org
while interacting with humans. This requires the generation
and coordination of “natural” behaviours and expressions.
Reidsma and Welbergen [10] discusses several features
of the modular achitecture of Elckerlyc and relates each of
them to a number of use requirements. A general overview
of the Elckerlyc system can be found in Figure 2. The input
of the Elckerlyc platform is a BML speciﬁcation. “BML
provides abstract behaviour elements to steer the behaviour
of a virtual human. A BML realizer is free to make its
own choices concerning how these abstract behaviours
will be displayed on the embodiment. For example, in
Elckerlyc, an abstract ‘beat gesture’ is by default mapped
to a procedural animation from the Greta repertoire. The
developer may want to map the same abstract behaviour
to a different form, i.e., to a high quality motion captured
gesture.”[10]. Different Engines will handle their own parts
of the behaviour speciﬁcation and generate synchronised
instructions for realising i.e. speech output, body gestures,
postures and facial expression. The output of all the engines
is displayed on one embodiment, like a realistic 3D full
kinematic virtual human, the Nabaztag or a graphical 2D
cartoon like picture animation. Figure 3 shows three types
of embodiments supported by the Elckerlyc platform.
Not every embodiment is able to render all the behaviours
that can be speciﬁed in BML. This depends on what the
embodiment offers e.g. a robot that is not able to smile
or a picture animation that lacks a picture showing the
smiling face cannot render the requested smiling behaviour.
The interface between the output of Elckerlyc and the
embodiment occurs in a Binding. A Binding is an XML
description to achieve a mapping from abstract BML be-
haviours to PlanUnits that determines how the behaviour will
be displayed in the embodiment. Bindings can be customized
by the application developer.
This paper discusses how these Bindings were exploit.
A light-weight PictureEngine was developed that makes
it possible to run Elckerlyc on mobile Android platforms.
Elckerlyc allows for a transparent and adjustable mapping
from BML to output behaviours (rather than the mostly
hardcoded mappings in other realizers), and allows for
easy integration of new modalities and embodiments, for
example to control robotic embodiments, or full 3D em-
bodiments. To run Elckerlyc on mobile platforms a light-
weight PictureEngine was developed that allows rendering
of behaviours and expressions using layering of pictures.
III. THE PICTUREENGINE
A realistic 3D full kinematic virtual human embodiment is
not suitable for use on mobile devices for multiple reasons.
Not only do such devices lack the processing power to
render this kind of environment, but displaying a full scene
including a full body ECA on the relatively small screen of
a mobile device is quite impractical. The displayed size of
42
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

Figure 2.
Overview of the Elckerlyc architecture. BML input is processed by the Elckerlyc system by different engines. The result is combined into one
embodiement.
the ECA would make it so small that its expressions would
hardly be visible. The high processing demands would also
drain the device’s battery quickly. In order to avoid all these
problems, Elckerlyc uses a different graphical embodiment
on the Android platform, the PictureEngine.
The PictureEngine is a lightweight graphical embodiment
that uses a collection of 2D images in order to display the
ECA. While having a 2D image embodiment does present
some limitations, it also has its advantages. First of all, it
has low demands in terms of processing and memory. It also
allows for great variation in the design of ECAs. One could
for example design a cartoon ﬁgure ECA, an ECA based
on more lifelike illustrations, an ECA based on prerendered
3D images, or even an ECA based on photographic images
of a real person. This section discusses the most important
aspects of the PictureEngine.
A. Layers
In order to generate a dynamic ECA from a collection
of images, the PictureEngine uses a layer-based approach.
Different parts of the ECA are displayed on different layers
of the ﬁnal image, and can thus be in different states. For
example, one layer may contain the eyes, while another
contains the mouth. The base layer normally contains the
ECA in a base state, meaning that when the ECA is in a
neutral or passive state, the user sees only this base layer.
That means that while each (facial) feature of the ECA does
have its own layer, they are also present in the base layer.
This means that the base layer contains for example a full
face with a neutral expression, even though the eyes and
mouth may have their own layers. There can also be layers
containing features that are not visible in the base state, such
as hands that only move into the frame when executing
a gesture. By using this layer approach, different parts of
the ECA can be manipulated independently and combined
in order to generate different expressions. This also allows
the ECA to do several (connected or unconnected) things
at once, such as blink while also speaking and pointing at
something.
As noted earlier, the layer approach does present some
limitations. Because the features of the ECA are in separate
layers, the base onto which these features are displayed
43
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

(a) The Nabaztag rabit
(b) a 2D cartoon like picture animation
(c) a realistic 3D full kinematic virtual human
Figure 3.
Three types of embodiment used as back-end for the Elckerlyc platform
(usually a face, and possibly part of the body) is gener-
ally static. This means that any movement of the entire
ECA poses a problem. When an ECA has facial features
on different layers, the layered structure prevents it from
moving around. This also applies to smaller movements
such as nodding, shaking and tilting of the head. However,
because the PictureEngine is designed to be used on smaller
screens, the ECA will generally be displayed as a talking
head, a closeup of a face covering most of the available
screen space. In this kind of environment, having the ECA
perform locomotion is already impractical and, since there
is hardly any room for the ECA’s environment to contain
anything but itself, arguably unneccessary.
B. Animations
While single images may sufﬁce for portraying expres-
sions in many cases, there are other cases where an ECA
simply has to display some motion in order to come across
as believable. To make this possible, the PictureEngine also
allows the use of animations instead of single images. These
animations are deﬁned by using a simple XML format that
allows a number of images to be listed, together with the
duration for which they are to be displayed. While these
durations are speciﬁed in seconds, the nature of the BML
scheduler allows the duration of animations to be adjusted
according to the BML code that is being realised, causing the
animation to play faster or slower depending on the timespan
determined by the scheduler.
These animation XML ﬁles have an additional feature
that provides an advantage over using an already established
format for image animations: the possibility to include
synchronization information in the animation speciﬁcation.
This allows a synchronization point to be included in the
speciﬁcation between any two frames of an animation. These
synchronization points are available for use in the main BML
code. In this way, it is possible to e.g. synchronize the stroke
of a beat gesture animation with a certain word within a
speech element.
Figure 4.
PictureBinding entry for a smile.
C. PictureBinding
Like other Elckerlyc embodiment engines, the Pic-
tureEngine uses a Binding. This PictureBinding allows a
combination of a BML behaviour class and (optionally)
several constraints to be mapped to a certain PictureUnit (i.e.
an image or animation). It is possible to include anywhere
from zero constraints to all the constraints deﬁned for
the corresponding BML behaviour type. This allows the
designer of a PictureEngine ECA to reﬁne those behaviours
that are most relevant to the ECA, and implement any others
in a more general fashion.
The actual PictureBinding itself is deﬁned in an XML
ﬁle containing the behaviour classes and constraints and the
PictureUnits and parameters they are to be mapped onto (see
Figure 4 for an example). The accessibility of this format
allows an ECA to be designed or modiﬁed by someone who
does not have knowledge of the inner workings of Elckerlyc.
Only knowledge of BML and the available PictureUnits and
their parameters is required to be able to build a complete
PictureBinding.
D. Lipsync
In order to visually display the fact that the ECA is
speaking, the PictureEngine provides a rudimentary lipsync
facility. This lipsync feature is implemented in the same
way as the lipsync provided by the default AnimationEngine.
44
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

However, where the AnimationEngine provides a full map-
ping from visemes to animation units, the PictureEngine
lipsync currently does not make use of such a mapping
(although it could be added in the future). In its current state
the lipsync allows a single animation to be speciﬁed which
is played whenever the ECA is speaking. This animation is
repeated for the number of times it ﬁts into the duration of
the speech unit (and slightly adjusted so that the amount of
repetitions becomes a round number).
IV. ANDROID IMPLEMENTATION
Since the Elckerlyc platform is implemented almost en-
tirely in Java, all of its core elements run on Android
without any modiﬁcation. However, since Android has its
own environment for visual and audio output, some addi-
tions are required. This does not mean that the Android
application uses a modiﬁed version of the core Elckerlyc
platform. The fact that Elckerlyc uses an XML format to
deﬁne the loading requirements for a speciﬁc ECA allows
the Android application to simply load its own versions of a
few key components. This allows the core Elckerlyc system
to be used in the Android application as-is, so any changes
to the Elckerlyc core can be directly used in the Android
application without having to modify or port it ﬁrst. The
subsystems for which the Android application contains its
own versions are discussed here.
A. Graphical Output
Because the Android platform has its own graphical
environment, the engines which provide graphical output
use a modiﬁed component for printing their output in the
Android application. This goes for both the PictureEngine,
which handles the graphical display of the ECA, and the
TextSpeechEngine, which outputs speech elements to a text
area. Since PNG images can be handled without problems
by the Android graphical environment, the additional code
needed to replace the PictureEngine’s default output subsys-
tem with a version that works on Android is minimal.
B. SpeechEngine
In the case of the SpeechEngine (for the rendering of
spoken text using text-to-speech (TTS)) the differences with
Android are unfortunately more severe. The TTS engines
used in the PC version of the SpeechEngine contain several
dependencies on native PC systems and cannot be used on
Android without signiﬁcant changes. However, Android does
offer an internal TTS system. Using this internal system
avoids the costly process of porting a TTS engine and
any possible efﬁciency issues this may bring. In order to
make use of the internal Android TTS system, an Android
adaptation of the Elckerlyc SpeechEngine is needed. This
includes the module that loads and initializes the engine, as
well as the parts of the system dealing with the actual TTS
operations.
The main problem with the Android TTS system is that it
is not possible to obtain timing information for utterances,
meaning there is no way to ﬁnd out exactly at what time
a word is spoken. This causes the BML scheduler to be
unable to use synchronization points within utterances. This
makes it hard to precisely synchronize other behaviours
with speciﬁc words being spoken. A partial solution is that
utterances are presynthesized to a ﬁle in order to ﬁnd the
total duration of the utterance. This provides the crucial
information for the Elckerlyc scheduler. This ”preloading”
of utterances causes a delay at startup before the ECA starts
playback of the requested BML code.
Furthermore, the TTS also does not offer any viseme
information, making it impossible to use real lipsync on
Android. This is the main reason the PictureEngine does
not currently support true lipsync.
C. Subtitles
Because the PictureEngine can run on a mobile device, the
chances of the user having trouble hearing the text spoken by
the TTS on the Android system are quite high. This could be
caused by factors such as environment noise, low volume or
bad speakers. In order for the user to still be able to interact
with the ECA in these situations, the Android application
also offers an on-screen representation of any spoken text,
comparable to subtitling. The TextSpeechEngine (on-screen
text display) receives the text handled by the SpeechEngine
and displays this in a text area, synchronized (per utterance)
with the TTS.
V. APPLICATION
With the growing availability of online services and ubiq-
uitous computing capabilities it becomes easier to develop
systems that can support people in changing their behaviour
or lifestyle [11]. Sensor data and context information is
available anywhere. Many of these systems support people
in their daily life by providing support by means of a human
or digital coach. These systems can support users in coping
with chronic diseases like COPD [12] and diabetes, but also
to be more physically active [13] [14]. Persuasive systems
[15], and especially behaviour change support systems, are
information systems designed to form, alter or reinforce
attitudes, behaviours or an act of complying without using
deception, coercion or inducements [16].
In the EU Artemis project Smarcos we developed a
personal digital health coach that supports users in attaining
a healthy lifestyle by giving timely, context-aware feedback
about daily activities through a range of interconnected
devices. The two targeted user groups of the coaching system
are ofﬁce workers and diabetes type II patients. Ofﬁce
workers will receive feedback about their physical activity
level, while diabetes type II patients also receive feedback
about their medication intake. Physical activity is measured
by a 3D accelerometer and medication intake is tracked by
45
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

a smart pill dispenser. The pill dispenser uses the mobile
network to connect to the internet. The system is context-
aware and multi-device which means that the (digital) coach
can support the users in various contexts and on different
devices. GPS information is provided by the mobile phone
of the user. The system sends feedback to the mobile phone
of the user (iOS or Android), their laptop or PC, and their
television.
All input and output devices are connected to the Smarcos
cloud. User proﬁles and preferences, contextual information
and sensor data is uploaded to the cloud and stored in a
central database. The digital coach continuously keeps track
of all user data and contains coaching rules. When the
coach receives a trigger it starts to evaluate the coaching
rules. When one of the rules is true, it will select a
suitable message from the coaching content database and
send the message to the user through one of the available
output devices and through one of the available modalities.
Feedback can be presented using different output modalities
Feedback can be sent as a text message, can be presented in
a graph or can be presented by animated spoken interaction
with an ECA.
Personalisation of the user interface by means of ECA
may affect the effectiveness of the behaviour change pro-
gram and the user experience. Results from other studies
indicate that the use of an ECA in a persuasive system has
a positive effect on how the feedback is received by the user
and on the results of the coaching program [17], [18], [19].
A ﬁrst user evaluation with a basic version of the Smarcos
personal digital health coach compared two alternatives for
providing digital coaching to users of a physical activity
promotion service. Participants in the study (n=15) received
personalised feedback on their physical activity levels for a
period of six weeks. Feedback was provided weekly either
by e-mail or through an embodied conversational agent.
The messages by the ECA were prerecorded video messges.
Users’ perceptions of the digital coaching was assessed by
means of validated questionnaires after three weeks and
at the end of the study. Results show signiﬁcantly higher
attractiveness, intelligence and perceived quality of coaching
for the ECA coach.
VI. CONCLUSION AND FUTURE WORK
To take full advantage of the known beneﬁts of person-
iﬁcation of the user interface of service systems, a mobile
platform that is able to present embodied conversation agents
in mobile applications is presented. The platform makes use
of the Elckerlyc system. Because it is too heavy to render
realistic 3D virtual humans on mobile devices a light-weight
PictureEngine was developed. The PictureEngine makes it
possible to use the Elckerlyc system on the Android platform
and generate realtime animations of embodied conversa-
tional agents. The PictureEngine is used in the Smarcos
coaching application as a mobile embodied coach.
Long term user evaluations with the Smarcos coaching
platform, including the mobile emdodied coach, are planned
to investigate the effects of personiﬁed coaching feedback on
user experience, quality of coaching and effectiveness of the
coaching program.
During a six weeks user experiment 80 participants will
use the Smarcos coaching platform for physical activity.
Every participant has to meet the daily personal activity
goal. The participants will get feedback about their progress
on their mobile phone. The system will send feedback to
the users with reminders to be more physically active or to
upload the activity data, motivational message, tips and a
weekly overview of their coaching program.
The design of the user evaluation will be a between
subject design. The participants will be divided into two
groups of 40 participants each. One group will receive the
feedback presented in text, while the other group will receive
the feedback presented by the ECA.
The effects on the coaching program of the way of
presenting the feedback will be measured by means of
questionnaires and by observation of the performance of the
participants. During the experiments the progress towards
the goal of the user, the actual amount of physical activity
and the times the participant uploads their activity data is
logged by the system.
At the start, at the end and halfway the experiment the
participants will be asked to complete a questionnaire to
measure the user experience, the credibility towards the
system and the quality of coaching. User experience will be
measured by the AttrakDiff2 questionnaire [20], credibility
will be measured by the Source Credibility Questionnaire
[21] and the quality of coaching will be measures by the
Quality of Coaching questionnaire [22].
Although it is shown that the PictureEngine can run
on mobile Android devices it would be worth exploring
options for using a different TTS system in the future.
This would allow the application to regain the speech-
related functionality that is currently unavailable on Android,
such as synchronization within utterances and viseme-based
lipsync. A next step in the development of the PictureEngine
is looking for techniques to allow small movements by the
ECA, such as nodding and shaking of the head.
ACKNOWLEDGMENT
This work was funded by the European Commission,
within the framework of the ARTEMIS JU SP8 SMARCOS
project 100249 - (www.smarcos-project.eu).
REFERENCES
[1] T. Bickmore, D. Mauer, F. Crespo, and T. Brown, “Persuasion,
task interruption and health regimen adherence,” in Proceed-
ings of the 2nd international conference on Persuasive tech-
nology, ser. PERSUASIVE’07. Berlin, Heidelberg: Springer-
Verlag, 2007, pp. 1–11.
46
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

[2] M. Argyle, Bodily Communication, MethuenEditors, Ed.
Methuen, 1988, vol. 2nd.
[3] D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt,
“Multi-party, multi-issue, multi-strategy negotiation for multi-
modal virtual agents,” in Intelligent Virtual Agents, ser. Lec-
ture Notes in Computer Science, H. Prendinger, J. Lester, and
M. Ishizuka, Eds.
Springer Berlin / Heidelberg, 2008, vol.
5208, pp. 117–130.
[4] H. van Welbergen, D. Reidsma, Z. M. Ruttkay, and J. Zwiers,
“Elckerlyc - a BML realizer for continuous, multimodal
interaction with a virtual human,” Journal on Multimodal
User Interfaces, vol. 3, no. 4, pp. 271–284, August 2010.
[5] E. Bevacqua, K. Prepin, E. de Sevin, R. Niewiadomski, and
C. Pelachaud, “Reactive behaviors in SAIBA architecture,” in
Proc. of 8th Int. Conf. on Autonomous Agents and Multiagent
Systems (AAMAS 2009), S. Decker, Sichman and Castel-
franchi, Eds., May 2009.
[6] H. Vilhjalmsson, N. Cantelmo, J. Cassell, N. Chafai, M. Kipp,
S. Kopp, M. Mancini, S. Marsella, A. Marshall, C. Pelachaud,
Z. Ruttkay, K. Th´orisson, H. van Welbergen, and R. van der
Werf, “The behavior markup language: Recent developments
and challenges,” in Proceedings of the 7th International
Conference on Intelligent Virtual Agents, C. Pelachaud, J.-C.
Martin, E. Andr´e, G. Collet, K. Karpouzis, and a. p. D. Pel´e,
pages=90–111, Eds., 2007.
[7] A. Heloir and M. Kipp, “Real-time animation of interactive
agents: specifciation and realization,” Applied Artiﬁcial Intel-
ligence, vol. 24(6), pp. 510–529, 2010.
[8] E.
Bevacqua,
M.
Mancini,
R.
Niewiadomski,
and
C.
Pelachaud,
“An
expressive
ECA
showing
complex
emotions,”
in
AISB’07
Annual
convention,
workshop
“Language, Speech and Gesture for Expressive Characters”,
April 2007, pp. 208–216.
[9] D. Reidsma, I. de Kok, D. Neiberg, S. Pammi, B. van
Straalen, K. Truong, and H. van Welbergen, “Continuous
interaction with a virtual human,” Journal on Multimodal
User Interfaces, vol. 4, no. 2, pp. 97–118, 2011.
[10] D. Reidsma and H. v. Welbergen, “Elckerlyc in practice on
the integration of a BML realizer in real applications,” in
Proc. of Intetain 2011, 2011.
[11] M. Kasza, V. Sz¨ucs, A. V´egh, and T. T¨or¨ok, “Passive vs.
active measurement: The role of smart sensors,” in UBI-
COMM 2011, The Fifth International Conference on Mobile
Ubiquitous Computing, Systems, Services and Technologies,
2011, pp. 333 – 337.
[12] H. op den Akker, V. Jones, and H. Hermens, “Predicting
feedback compliance in a teletreatment application,” in Pro-
ceedings of ISABEL 2010: the 3rd International Symposium
on Applied Sciences in Biomedical and Communication Tech-
nologies, 2010.
[13] O. St˚ahl, B. Gamb¨ack, M. Turunen, and J. Hakulinen, “A mo-
bile health and ﬁtness companion demonstrator,” in Proceed-
ings of the 12th Conference of the European Chapter of the
Association for Computational Linguistics: Demonstrations
Session, ser. EACL ’09.
Stroudsburg, PA, USA: Association
for Computational Linguistics, 2009, pp. 65–68.
[14] G. Geleijnse, A. van Halteren, and J. Diekhoff, “Towards a
mobile application to create sedentary awareness,” in In Pro-
ceedings of the 2nd Int. Workshop on Persuasion, Inﬂuence,
Nudge Coercion Through Mobile Devices (PINC2011), vol.
722, 2011, pp. 90–111.
[15] B. Fogg, Persuasive Technology. Using computers to change
what we think and do.
Morgan Kaufmann, 2003.
[16] H. Oinas-Kukkonen, “Behavior change support systems: A
research model and agenda,” in Persuasive Technology, ser.
Lecture Notes in Computer Science, T. Ploug, P. Hasle, and
H. Oinas-Kukkonen, Eds.
Springer Berlin / Heidelberg,
2010, vol. 6137, pp. 4–14.
[17] O. A. B. Henkemans, P. van der Boog, J. Lindenberg,
C. van der Mast, M. Neerincx, and B. J. H. M. Zwetsloot-
Schonk, “An online lifestyle diary with a persuasive computer
assistant providing feedback on self-management,” Technol-
ogy & Health Care, vol. 17, p. 253–257, 2009.
[18] D. Schulman and T. W. Bickmore, “Persuading users through
counseling dialogue with a conversational agent,” in Persua-
sive ’09: Proceedings of the 4th International Conference on
Persuasive Technology.
New York, NY, USA: ACM, 2009,
pp. 1–8.
[19] D. C. Berry, L. T. Butler, and F. de Rosis, “Evaluating a
realistic agent in an advice-giving task,” Int. J. Hum.-Comput.
Stud., vol. 63, no. 3, pp. 304–327, 2005.
[20] M. Hassenzahl, “Funology,” M. A. Blythe, K. Overbeeke,
A. F. Monk, and P. C. Wright, Eds.
Norwell, MA, USA:
Kluwer Academic Publishers, 2004, ch. The thing and I:
understanding the relationship between user and product, pp.
31–42.
[21] J. C. McCroskey, “Scales for the measurement of ethos,”
Speech Monographs, vol. 33, no. 1, pp. 65–72, 1966.
[22] J. Cote, J. Yardley, J. Hay, W. Sedgwick, and J. Baker, “An
exploratory examination of the coaching behaviour scale for
sport,” AVANTE, vol. 5, pp. 82–92, 1999.
47
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

