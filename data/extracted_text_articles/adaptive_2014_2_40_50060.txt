Automated Fault Analysis and Filter Generation for Adaptive Cybersecurity
David J. Musliner, Scott E. Friedman, Jeffrey M. Rye
Smart Information Flow Technologies (SIFT)
Minneapolis, USA
email: {dmusliner,sfriedman,jrye}@sift.net
Abstract‚ÄîWe are developing the FUZZBUSTER system to auto-
matically identify software vulnerabilities and create adaptations
that shield or repair those vulnerabilities before attackers can
exploit them. Adaptive cybersecurity involves efÔ¨Åciently improv-
ing software security to minimize the window of attack, and also
preserving software functionality as much as possible. This paper
presents new tools that have been integrated into FUZZBUSTER
adaptive cybersecurity. These tools produce more general, ac-
curate adaptations, increase the efÔ¨Åciency of FUZZBUSTER‚Äôs
diagnoses and adaptation operations, and preserve the software‚Äôs
functionality. We report the results of FUZZBUSTER‚Äôs analysis of
16 fault-injected command-line binaries and six previously known
bugs in the Apache web server. We compare results over different
conÔ¨Ågurations of FUZZBUSTER to characterize the beneÔ¨Åts of the
new fuzz-testing tools.
Keywords-cyber defense; automatic Ô¨Ålter generation.
I. INTRODUCTION
Cyber-attackers constantly threaten today‚Äôs computer sys-
tems, increasing the number of intrusions every year [1], [2].
Firewalls, anti-virus systems, and patch distribution systems
react too slowly to newfound ‚Äúzero-day‚Äù vulnerabilities, al-
lowing intruders to wreak havoc. We are investigating ways to
solve this problem by allowing computer systems to automati-
cally identify their own vulnerabilities and adapt their software
to shield or repair those vulnerabilities, before attackers can
exploit them. Such adaptations must balance the safety of the
system against its functionality: the safest behavior might be
to simply turn the power off or entirely disable vulnerable
applications, but that would make the systems useless. To
make a Ô¨Åner-grained balance between security and function-
ality, adaptations must be:
‚Ä¢ General enough to shield the entire vulnerability (i.e., not
just blocking an overspeciÔ¨Åc set of faulting inputs).
‚Ä¢ SpeciÔ¨Åc enough to minimize the negative impact on
program functionality (e.g., by causing incorrect results
on valid inputs).
‚Ä¢ EfÔ¨Åciently-generated, to minimize the window of expo-
sure to vulnerability over time.
These considerations for adaptive cybersecurity pose several
challenges, including: how faults are discovered and diag-
nosed, with and without direct access to source code or
binaries; how adaptations are generated from the diagnoses;
how the many possible adaptations are assessed and chosen;
and how all of these operations are orchestrated for efÔ¨Åciency.
This paper describes strategies for automatically discovering
vulnerabilities, diagnosing them, and adapting programs to
defend against them. We have implemented these strategies
	 
 
 

!! 

 
 


 

 
 
  
 
!! 
 ! 
 
 
 
	

Fig. 1.
FUZZBUSTER automatically Ô¨Ånds vulnerabilities, reÔ¨Ånes its under-
standing of their extent, and creates adaptations to shield or repair them.
within the FUZZBUSTER integrated system for active cyber-
security [3], which includes metrics [4], and metacontrol [5]
for self-adaptative software immunity. FUZZBUSTER uses a
diverse set of custom-built and off-the-shelf fuzz-testing tools
and code analysis tools to develop protective self-adaptations.
Fuzz-testing tools Ô¨Ånd software vulnerabilities by exploring
millions of semi-random inputs to a program. FUZZBUSTER
also uses fuzz-testing tools to reÔ¨Åne its models of known
vulnerabilities, clarifying which types of inputs can trigger a
vulnerability. FUZZBUSTER‚Äôs behavior falls into two general
classes, as illustrated in Figure 1:
1) Proactive: FUZZBUSTER discovers novel vulnerabilities
in applications using fuzz-testing tools. FUZZBUSTER
reÔ¨Ånes its models of the vulnerabilities and then repairs
them or shields them before attackers Ô¨Ånd and exploit
them.
2) Reactive: FUZZBUSTER is notiÔ¨Åed of a fault in an ap-
plication (potentially triggered by an adversary). FUZZ-
BUSTER subsequently tries to reÔ¨Åne the vulnerability and
repair or shield it against attackers. Reactive vulnerabil-
ities pose a greater threat to the host, since these may
indicate an imminent exploit by an attacker.
FUZZBUSTER‚Äôs primary objective is to protect its host by
adapting its applications, but this may come at some cost.
For example, applying an input Ô¨Ålter or a binary patch may
create a new vulnerability, re-enable a previously-addressed
vulnerability, or otherwise negatively impact an application‚Äôs
usability by changing its expected behavior. This illustrates
a tradeoff between functionality and security, and measuring
both of these factors is important for making decisions about
adaptive cybersecurity.
We begin by outlining FUZZBUSTER‚Äôs process of discov-
ering, reÔ¨Åning, and repairing vulnerabilities in Section II,
56
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-341-4
ADAPTIVE 2014 : The Sixth International Conference on Adaptive and Self-Adaptive Systems and Applications

which motivates our research on adaptation metrics. We then
describe FUZZBUSTER‚Äôs novel diagnosis tools for adaptive
cybersecurity in Section III, and we summarize the results of
several experiments in Section IV.
II. BACKGROUND: FUZZBUSTER ACTIVE CYBERSECURITY
FUZZBUSTER tests and adapts multiple applications on a
host machine. When FUZZBUSTER discovers a fault in one
of these applications‚Äî or when it is notiÔ¨Åed of a reactive
fault triggered by some other input source‚Äî it represents
the fault as an exemplar that contains information about the
system‚Äôs state when it faulted, as shown in Figure 1. Note
that FUZZBUSTER is not responsible for fault detection; we
assume that other security and correctness mechanisms detect
the fault and notify FUZZBUSTER.
An exemplar includes information for replicating the fault,
such as environment variables and data passed as input to
the faulting application (e.g., via sockets or stdin). Some
of this data may be unrelated to the underlying vulnerability.
For instance, when FUZZBUSTER encounters a fault in the
Apache web server in Section IV, it captures all environment
variables (all of which are unnecessary to replicate the fault),
and the entire string of network input that was sent to the
application (most of which is unnecessary to replicate the
fault). FUZZBUSTER uses fuzz-testing tools to incrementally
reÔ¨Åne the exemplar, trying to characterize the minimal inputs
needed to trigger the fault. Since time and processing power is
limited, FUZZBUSTER uses a greedy meta-control strategy [5]
to orchestrate these tools.
ReÔ¨Ånement is an iterative process, where each task improves
the vulnerability proÔ¨Åle that FUZZBUSTER uses to characterize
the vulnerability. The reÔ¨Ånement process turns the initial
(often over-speciÔ¨Åc) vulnerability proÔ¨Åle into a more accurate
and general proÔ¨Åle. While reÔ¨Åning the Apache web server
vulnerabilities, FUZZBUSTER uses an environment variable
fuzzer to test and remove unnecessary environment variables
for replicating the fault, uses input fuzzers to delimit, test,
and remove/replace unnecessary network input, and thereby
develops a more accurate vulnerability proÔ¨Åle.
FUZZBUSTER has several general adaptation capabilities,
including input Ô¨Ålters, environment variable Ô¨Ålters, and source-
code repair and recompilation. These protect against entire
classes of exploits that may be encountered in the future.
FUZZBUSTER uses each of these by (1) constructing the adap-
tation, (2) assessing the adaptation by temporarily applying it
for test runs, and (3) applying the adaptation to the production
application if it is deemed beneÔ¨Åcial. FUZZBUSTER may apply
multiple adaptations to an application to repair a single under-
lying vulnerability. In the case of adapting the Apache web
server in Section IV, FUZZBUSTER creates input Ô¨Ålters based
on its vulnerability proÔ¨Åles: it extracts regular expressions that
characterize the pattern of faulting inputs, including necessary
character sequences (e.g., ‚ÄúCookie:‚Äù), length-dependent wild-
cards (e.g., ‚Äú.{256,}?‚Äù), and more. FUZZBUSTER then uses
these input Ô¨Ålters to identify potentially-faulting inputs and
then discard them or rectify them, based on the application
under test.
A. Assessing Adaptations
FUZZBUSTER cannot blindly apply adaptations, since they
might have a negative impact on functionality or, even worse,
they could create new faults altogether. Thus, FUZZBUSTER
uses concrete metrics to assess the impact of candidate adap-
tations on security and functionality.
FUZZBUSTER‚Äôs adaptation metrics are based on test cases:
mappings from application inputs (e.g., sockets, stdin,
command-line arguments, and environment variables) to ap-
plication outputs (e.g., stdout and return code). A faulting
test case terminates with an error code or its execution time
exceeds a set timeout parameter, while a non-faulting test case
terminates gracefully. FUZZBUSTER stores three sets of test
cases for each application under its control:
1) Non-faulting (reference) test cases are test cases that
were supplied with an application for regression test-
ing. FUZZBUSTER tracks which of these have correct
behavior (i.e., output and return code), and which have
different/incorrect behavior, given some adaptations.
2) Faulting test cases include exemplars that caused faults
on their Ô¨Årst encounter, and other faulting test cases
encountered while reÔ¨Åning the exemplar. FUZZBUSTER
tracks which of these have been Ô¨Åxed by the adaptations
created so far, and which are still faulting. There are two
speciÔ¨Åc types of faulting test cases:
a) Reactive faulting test cases: encountered by host
notiÔ¨Åcation and subsequent reÔ¨Ånement (see Fig-
ure 1). These pose more of a threat, since the
underlying vulnerability may have been caused by
an adversary.
b) Proactive faulting test cases: encountered by dis-
covery and reÔ¨Ånement (see Figure 1). These pose
less threat, since they were discovered internally
and FUZZBUSTER has no evidence that an adver-
sary is aware of them.
We can calculate two important metrics from these sets of
test cases over time:
1) Exposure is computed as the number of unÔ¨Åxed fault-
ing test cases over time. This represents an estimated
window of exploitability.
2) Functionality loss is computed as the number of incor-
rect non-faulting (reference) test cases over time. This
represents the usability that FUZZBUSTER has sacriÔ¨Åced
for the sake of security.
Before FUZZBUSTER has discovered faults or been notiÔ¨Åed
of faults, there are no faulting test cases for any application.
As FUZZBUSTER encounters proactive and reactive faults and
reÔ¨Ånes those faults (e.g., by experimenting with different
inputs), it will accrue faulting test cases. FUZZBUSTER then
applies and removes adaptations to Ô¨Åx these faulting test
cases. These adaptations ultimately protect the host against
adversaries.
57
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-341-4
ADAPTIVE 2014 : The Sixth International Conference on Adaptive and Self-Adaptive Systems and Applications

FUZZBUSTER‚Äôs assessment policy allows it to sacriÔ¨Åce
functionality to Ô¨Åx faulting test cases. The exact balance can
be tuned for different applications, but FUZZBUSTER‚Äôs default
priorities are:
1) Fixing reactive faulting test cases.
2) Fixing proactive faulting test cases.
3) Maintaining the behavior of non-faulting test cases.
This means that FUZZBUSTER will tolerate functionality loss
(i.e., by changing the behavior of non-faulting test cases) in
order to decrease exposure.
B. Pre-existing Tools for Discovery & ReÔ¨Ånement
Since this paper presents new tools for discovery and reÔ¨Åne-
ment (Section III), for the sake of comparison we Ô¨Årst review
the set of fuzz tools we used in previous work [5], [3], [4].
Those tools included a random string generator for discovering
faults (called Fuzz-2001) and various minimization (i.e.,
unnecessary character removal) tools for reÔ¨Åning faults.
Fuzz-2001 quickly constructs a sequence of printable
and non-printable characters and feeds it as input to the
program under test. This is effective for discovering some
buffer overÔ¨Çows, problems with escape characters, and other
such problems.
The minimization tools FUZZBUSTER uses to reÔ¨Åne vulner-
abilities include:
‚Ä¢ smallify: semi-randomly removes single characters from
the input string.
‚Ä¢ line-relev: semi-randomly removes entire lines from the
input string.
‚Ä¢ divide-and-conquer: Use a binary search to attempt to
remove entire portions of the input string.
Each of these tools is designed take a faulting test case as
input, and produce smaller faulting test case(s).
Minimization tools can operate in a black-box fashion,
where FUZZBUSTER does not have the source code or even
access to the binary. All they require is an output signal to
determine whether the program faulted.
III. NEW DISCOVERY & REFINEMENT TOOLS
We now discuss several new tools that we have incorporated
into FUZZBUSTER for discovering and reÔ¨Åning faults. We
then present empirical results comparing the new and existing
tools to characterize the effects on the host‚Äôs exposure to
vulnerabilities.
Both of these tools work with input Ô¨Ålter adapations; that
is, program adaptations that remove content from input data
before passing the data to the corresponding program.
A. Retrospective Fault Analysis
We implemented and tested Retrospective Fault Analysis
(RFA), a new tool for vulnerability discovery. RFA works by
Ô¨Ånding the most recent faulting test case such that:
‚Ä¢ The test case‚Äôs input is Ô¨Åltered by the most recent
adaptation applied, so some input data has been removed.
‚Ä¢ The test case still faults, despite its input being Ô¨Åltered.
RFA then uses the test case‚Äî with Ô¨Åltered input‚Äî as an
exemplar. This effectively allows FUZZBUSTER to Ô¨Åx test
cases that still fault, despite incremental adaptations.
To illustrate why this is important, consider the following
simpliÔ¨Åed example, where a program faults if it receives either
CRASH or fault in an incoming message. Some messages
may have more than one fault within them, e.g.:
‚Ä¢ Cookie: foo=...CRASH...fault...
‚Ä¢ Cookie: foo=...faCRASHult...
This means that FUZZBUSTER can automatically build a
Ô¨Ålter adaptation to address CRASH, but in both of the above
cases, there will still be a fault. Using RFA, FUZZBUSTER
will follow its CRASH adaptation with a retrospective investi-
gation of the remaining fault test case(s). This produces a
more complete analysis of problematic inputs, and it improves
the host‚Äôs exposure to vulnerabilities, as we demonstrate in our
experiments.
B. Input Generalization Tools
As described in Section II-B, minimization tools remove
unnecessary characters for a fault. Unfortunately, reÔ¨Åning
vulnerabilities based on removal alone will tend to produce
overspeciÔ¨Åc adaptations.
Consider the example of IP addresses within a packet
header: minimization tools might trim 192.168.0.1 to 2.8.0.1,
which might still produce the fault; however, an adaptation
based on this model will only be effective when 2, 8, 0, and
1 are all present in the address.
FUZZBUSTER‚Äôs new generalization tools go the extra step
of replacing characters and inserting characters to generalize
FUZZBUSTER‚Äôs regular expression model of the faulting input
pattern. This means that FUZZBUSTER will be able to substi-
tute the IP address‚Äô digits with other digits to develop a more
general, accurate adaptation.
We have implemented the following generalization tools:
‚Ä¢ replace-all-chars: replaces all characters with dif-
ferent characters, reruns the test case, and then general-
izes. This determines whether the test case is an instance
of a buffer overÔ¨Çow. For example:
ABCDEFGH ==> .{8,}
‚Ä¢ replace-delimited-chars: splits the input into
chunks, using common delimiters, removes and replaces
delimited chunks, and then generalizes. For example:
host: 1.1.1.1\nCookie ==> .{0,}?Cookie
‚Ä¢ replace-individual-chars:
removes
and
re-
places individual characters, sensitive to character classes
(e.g., letters, digits, whitespace, etc.), and generalizes. For
example:
GCOJR34A59S94H ==> .*C.*R.*A.*S.*H
‚Ä¢ insert-chars: inserts characters in-between consec-
utive concrete characters, to relax adjacency constraints.
For example:
CRASH ==> .*C.*R.*A.*S.*H
58
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-341-4
ADAPTIVE 2014 : The Sixth International Conference on Adaptive and Self-Adaptive Systems and Applications

‚Ä¢ shorten-regex: removes characters within wildcard
blocks to provide more accurate buffer overÔ¨Çow thresh-
olds. For example:
host: .{951,} ==> host: .{256,}
We conducted experiments on multiple programs to charac-
terize the effect of generalization tools and RFA. We discuss
these experiments and results next.
 0
 20
 40
 60
 80
 100
 120
 140
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 17, 27)
PATCH 2 (0, 34, 27)
PATCH 3 (0, 54, 27)
PATCH 4 (0, 80, 27)
PATCH 5 (0, 92, 27)
PATCH 6 (0, 105, 27)
PATCH 7 (0, 120, 27)
PATCH 8 (0, 139, 27)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
Fig. 2.
Results using RFA, minimization, and generalization.
IV. EXPERIMENTS
We conducted an empirical evaluation on different programs
to measure the effect of RFA and the new generalization fuzz-
tools. We divide this into four discussions: (1) a compara-
tive analysis of minimization, generalization, and RFA on a
single program; (2) an example of FUZZBUSTER sacriÔ¨Åcing
functionality in order to increase security; (3) a quantitative
comparison of minimization and generalization using FUZZ-
BUSTER to shield a web server against known vulnerabilities;
and (4) adaptation statistics across multiple programs using
FUZZBUSTER with generalization and RFA.
A. Comparative Analysis: Generalization, Minimization, RFA
For this experiment, we used a fault-injected version of
dc, a unix-based, stdin-based desktop calculator program.
The fault in dc was injected within the internal modulo (i.e.,
remainder) operation. This operation is reached by invoking
the % command with at least two numbers on the stack,
printing with a non-decimal output radix, changing the input
radix, or invoking base conversion.
We ran FUZZBUSTER in Ô¨Åve settings: with RFA using
both minimization and generalization tools (Figure 2); and
then with and without RFA, under either minimization or
generalization tools (Figure 3).
Each of these plots display the following important data for
adaptive cybersecurity:
‚Ä¢ The number of faulting test cases FUZZBUSTER has
identiÔ¨Åed through discovery and reÔ¨Ånement (solid light
red line).
‚Ä¢ The number of those faulting test cases that FUZZBUSTER
has Ô¨Åxed (dashed light red line).
‚Ä¢ Exposure to vulnerabilities (area between light red lines).
‚Ä¢ The number of reference (non-faulting) test cases FUZZ-
BUSTER has for the application (solid dark line).
‚Ä¢ The number of those non-faulting test cases whose return
code and output behavior is preserved in the patched
version (dashed dark line).
‚Ä¢ Loss of functionality (area between dark lines).
‚Ä¢ The patches that have been applied.
The comparison plots in Figure 3 illustrate the trade-
offs of generalization and RFA. Minimization tools (Fig-
ure 3, left) produce quick, overspeciÔ¨Åc patches. For instance,
PATCH 16 in the Figure 3 upper-left plot Ô¨Ålters the pattern
.*9.*5.*%.*. While this is a legitimate example of the
fault, it does not characterize the fault in its entirety. By com-
parison, the generalization patches are slightly more general.
Figure 3 also illustrates the effect of retrospective fault
analysis. In the RFA trials, the exposure (distance between
the light red lines) is signiÔ¨Åcantly reduced. This is because
FUZZBUSTER often deploys a Ô¨Ålter that addresses some ‚Äì but
not all ‚Äì problems in a faulting input, and then RFA allows
FUZZBUSTER to focus on the remainder of the problematic
input. For instance, if a single test case has both a modulo
operation and a base conversion, Ô¨Åltering out only one of these
operations will not repair the test case.
In the setting with both generalization and RFA, FUZZ-
BUSTER Ô¨Ålters against the entire vulnerability within 15 min-
utes; in the other cases, FUZZBUSTER does not level off for
over three hours.
Note that in all settings in Figure 3, FUZZBUSTER did not
lose functionality of the underlying application, as measured
by the correctness of the reference test cases.
Figure 2 shows the results of FUZZBUSTER with both
minimization and genralization enabled. It Ô¨Åxes the entire
vulnerability and levels off in 18 minutes, but it also destroys
the functionality of one of the reference test cases, since its
PATCH 5 was overgeneral.
B. SacriÔ¨Åcing Functionality to Increase Security
We ran another FUZZBUSTER trial on a different fault-
injected version of the dc binary. This version faulted when-
ever an arithmetic operation is invoked on an empty stack,
so for instance, the sequence ‚Äò‚Äò9 5 +‚Äô‚Äô would not fault,
but the inputs ‚Äò‚Äò+‚Äô‚Äô or ‚Äò‚Äò4 n +‚Äô‚Äô would fault due to an
empty stack (and ‚Äò‚Äòn‚Äô‚Äô pops the stack).
The results are shown in Figure 4. Using generalization
tools and RFA, FUZZBUSTER isolates individual arithmetic
operations and generates Ô¨Ålters for each, ultimately disabling
its arithmetic operations to prevent any faults. Note that
almost every adaptation has an adverse impact on program
functionality, but by design, these are acceptable losses to
increase safety of the host.
C. Adapting a Web Server
We conducted FUZZBUSTER experiments on known Com-
mon Vulnerabilities and Exposures (CVEs) on the Apache web
server. This demonstrates FUZZBUSTER working on larger
59
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-341-4
ADAPTIVE 2014 : The Sixth International Conference on Adaptive and Self-Adaptive Systems and Applications

 0
 20
 40
 60
 80
 100
 120
 140
 160
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 1, 27)
PATCH 2 (0, 7, 27)
PATCH 3 (0, 14, 27)
PATCH 4 (0, 21, 27)
PATCH 5 (0, 21, 27)
PATCH 6 (0, 22, 27)
PATCH 7 (0, 29, 27)
PATCH 8 (0, 29, 27)
PATCH 9 (0, 39, 27)
PATCH 10 (0, 39, 27)
PATCH 11 (0, 39, 27)
PATCH 12 (0, 46, 27)
PATCH 13 (0, 46, 27)
PATCH 14 (0, 52, 27)
PATCH 15 (0, 58, 27)
PATCH 16 (0, 67, 27)
PATCH 17 (0, 74, 27)
PATCH 18 (0, 76, 27)
PATCH 19 (0, 76, 27)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
 0
 50
 100
 150
 200
 250
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 13, 27)
PATCH 2 (0, 20, 27)
PATCH 3 (0, 29, 27)
PATCH 4 (0, 36, 27)
PATCH 5 (0, 48, 27)
PATCH 6 (0, 56, 27)
PATCH 7 (0, 68, 27)
PATCH 8 (0, 77, 27)
PATCH 9 (0, 87, 27)
PATCH 10 (0, 91, 27)
PATCH 11 (0, 97, 27)
PATCH 12 (0, 111, 27)
PATCH 13 (0, 116, 27)
PATCH 14 (0, 124, 27)
PATCH 15 (0, 135, 27)
PATCH 16 (0, 146, 27)
PATCH 17 (0, 151, 27)
PATCH 18 (0, 158, 27)
PATCH 19 (0, 172, 27)
PATCH 20 (0, 184, 27)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 4, 27)
PATCH 2 (0, 11, 27)
PATCH 3 (0, 17, 27)
PATCH 4 (0, 25, 27)
PATCH 5 (0, 32, 27)
PATCH 6 (0, 39, 27)
PATCH 7 (0, 39, 27)
PATCH 8 (0, 54, 27)
PATCH 9 (0, 62, 27)
PATCH 10 (0, 62, 27)
PATCH 11 (0, 72, 27)
PATCH 12 (0, 85, 27)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
 0
 50
 100
 150
 200
 250
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 10, 27)
PATCH 2 (0, 26, 27)
PATCH 3 (0, 37, 27)
PATCH 4 (0, 45, 27)
PATCH 5 (0, 57, 27)
PATCH 6 (0, 65, 27)
PATCH 7 (0, 78, 27)
PATCH 8 (0, 87, 27)
PATCH 9 (0, 94, 27)
PATCH 10 (0, 102, 27)
PATCH 11 (0, 108, 27)
PATCH 12 (0, 121, 27)
PATCH 13 (0, 127, 27)
PATCH 14 (0, 139, 27)
PATCH 15 (0, 148, 27)
PATCH 16 (0, 158, 27)
PATCH 17 (0, 169, 27)
PATCH 18 (0, 178, 27)
PATCH 19 (0, 191, 27)
PATCH 20 (0, 195, 27)
PATCH 21 (0, 201, 27)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
Minimization Fuzz-Tools
Generalization Fuzz-Tools
Without RFA
With RFA
Fig. 3.
Results comparing the exposure window of Retrospective Fault Analysis and minimization vs. generalization tools.
production-quality applications with real vulnerabilities, and
it shows the generality of FUZZBUSTER and its fuzz-tools.
For each trial, we initialized FUZZBUSTER with the Apache
web server as the only application under test. We then sent
a faulting message to the server‚Äî as dictated by the cor-
responding CVE‚Äî and FUZZBUSTER detected the reactive
fault and began its fuzzing. Table I reports how many minutes
FUZZBUSTER took to produce an input Ô¨Ålter adaptation (from
simulation start to patch time) for the corresponding CVE us-
ing only minimization tools (i.e., ‚ÄúMin.‚Äù), only generalization
tools (i.e., ‚ÄúGen.‚Äù), and the speedup provided by generalization
tools.
TABLE I
FUZZBUSTER‚ÄôS REACTION TIME ON CVES OF THE APACHE WEB SERVER.
CVE
RT (Min.)
RT (Gen.)
Speedup
2011-3192
96
4
24x
2011-3368-1
53
10
5x
2011-3368-2
32
10
3x
2011-3368-3
77
11
7x
2012-0021
36
3
12x
2012-0053
30
7
4x
Reaction times reported in minutes; speedup reported as quotient.
In addition to producing more general patches, the gener-
alization tools also yield a signiÔ¨Åcant speedup factor between
3x and 24x, and on average, produce useful adaptations in an
order of magnitude less time.
For these CVE trials, RFA was not necessary since FUZZ-
BUSTER Ô¨Åxes all faulting test cases with the Ô¨Årst patch it
produces.
D. Statistics Across Programs
We now present additional results from using FUZZBUSTER
with the generalization tools and retrospective fault analysis
on 16 fault-injected binaries.
We used GenProg [6], an evolutionary program repair
tool, to create faulty binaries from the source code of unix
command-line applications including dc, fold, uniq, and
wc. We achieved this by modifying the GenProg test cases‚Äî
which GenProg uses as a Ô¨Åtness function‚Äî to expect a fault on
certain inputs. This way, GenProg would generate selectively-
faulting binaries based on our speciÔ¨Åcations.
FUZZBUSTER automatically analyzed each faulty binary
for two hours, using a mix of proactive fuzz-tools (e.g.,
Fuzz-2001 and RFA), reÔ¨Ånement fuzz tools (e.g., the gen-
eralization fuzz-tools), and adaptation strategies (e.g., input
Ô¨Ålters).
Fuzzing leveled off (i.e., FUZZBUSTER patched the entire
injected fault, based on our manual analysis of patches) on
10/16 binaries. Of these leveled-off binaries, FUZZBUSTER
took an average of 5.87 minutes to level off, and it sacriÔ¨Åced
an average of 6% functionality (i.e., by changing the output of
60
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-341-4
ADAPTIVE 2014 : The Sixth International Conference on Adaptive and Self-Adaptive Systems and Applications

 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 1000
 1100
# Faulting Test Cases
Time (s)
PATCH 1 (6, 0, 30)
PATCH 2 (6, 0, 30)
PATCH 3 (6, 0, 30)
PATCH 4 (6, 1, 30)
PATCH 5 (6, 10, 30)
PATCH 6 (6, 10, 30)
PATCH 7 (6, 40, 30)
PATCH 8 (6, 40, 30)
PATCH 9 (6, 40, 30)
PATCH 10 (6, 62, 30)
PATCH 11 (6, 62, 30)
PATCH 12 (6, 78, 30)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
Disable
addition
Disable
division
Disable
subtraction
Disable
modulo
Disable
exponentiation
Disable
multiplication
Fig. 4.
FUZZBUSTER sacriÔ¨Åces functionality to protect the program against vulnerabilities.
non-faulting reference test cases). FUZZBUSTER retained full
functionality on 7 of the 10 leveled-off binaries.
Over all 16 fault-injected binaries, FUZZBUSTER created
an average of 8.2 adaptations and applied an average of 7.8,
which amounts to a 95% usage of the adaptations it created.
Over all binaries, FUZZBUSTER Ô¨Åxed an average of 82%
of the faulting test cases and sacriÔ¨Åced an average of 10%
functionality during each 2-hour trial. This suggests that when
FUZZBUSTER cannot generate a perfect adaptation, it still
manages to close the exposure window over time.
V. RELATED WORK
As previously noted, the FUZZBUSTER approach has roots
in fuzz-testing, a term Ô¨Årst coined in 1988 applied to software
security analysis [7]. It refers to invalid, random or unexpected
data that is deliberately provided as program input in order
to identify defects. Fuzz-testers‚Äî and the closely related
‚Äúfault injectors‚Äù‚Äî are good at Ô¨Ånding buffer overÔ¨Çow, cross-
site scripting, denial of service (DoS), SQL injection, and
format string bugs. They are generally not highly effective in
Ô¨Ånding vulnerabilities that do not cause program crashes, e.g.,
encryption Ô¨Çaws and information disclosure vulnerabilities [8].
Moreover, existing fuzz-testing tools tend to rely signiÔ¨Åcantly
on expert user oversight, testing reÔ¨Ånement and decision-
making in responding to identiÔ¨Åed vulnerabilities.
FUZZBUSTER is designed both to augment the power of
fuzz-testing and to address some of its key limitations. FUZZ-
BUSTER fully automates the process of identifying seeds for
fuzz-testing, guides the use of fuzz-testing to develop general
vulnerability proÔ¨Åles, and automates the synthesis of defenses
for identiÔ¨Åed vulnerabilities.
To date, several research groups have created specialized
self-adaptive systems for protecting software applications.
For example, both AWDRAT [9] and PMOP [10] used
dynamically-programmed wrappers to compare program ac-
tivities against hand-generated models, detecting attacks and
blocking them or adaptively selecting application methods to
avoid damage or compromises.
The CORTEX system [11] used a different approach, plac-
ing a dynamically-programmed proxy in front of a replicated
database server and using active experimentation based on
learned (not hand-coded) models to diagnose new system
vulnerabilities and protect against novel attacks.
While these systems demonstrated the feasibility of the self-
adaptive, self-regenerative software concept, they are closely
tailored to speciÔ¨Åc applications and speciÔ¨Åc representations of
program behavior. FUZZBUSTER provides a general approach
61
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-341-4
ADAPTIVE 2014 : The Sixth International Conference on Adaptive and Self-Adaptive Systems and Applications

to adaptive immunity that is not limited to a single class of
application. FUZZBUSTER does not require detailed system
models, but will work from high-level descriptions of com-
ponent interactions such as APIs or contracts. Furthermore,
FUZZBUSTER‚Äôs proactive use of intelligent, automatic fuzz-
testing identiÔ¨Åes possible vulnerabilities before they can be
exploited.
VI. CONCLUSION AND FUTURE WORK
FUZZBUSTER is designed to discover vulnerabilities and
then quickly reÔ¨Åne and adapt its applications to prevent
them from being exploited by attackers. This paper presented
two advances in FUZZBUSTER‚Äôs tools ‚Äî retrospective fault
analysis and generalization fuzz-tools ‚Äî aimed at improv-
ing the quality and efÔ¨Åciency of FUZZBUSTER‚Äôs adaptations.
We presented empirical results of FUZZBUSTER‚Äôs automated
analysis of fault-injected programs and real CVEs, using ob-
jective metrics for adaptive cybersecurity such as vulnerability
exposure, functional loss, and reaction time. When analyzing
fault-injected programs, the generalization fuzz-tools and RFA
reduced vulnerability exposure by a factor of Ô¨Åve on fault
injected programs, and allowed FUZZBUSTER to Ô¨Ålter out
more of the vulnerability in less time. When analyzing the
Apache HTTP server, the fault generalization tools yielded an
order of magnitude speedup in reaction time over the existing
fault minimization tools.
At present, FUZZBUSTER uses a wrapper around the pro-
grams it controls, and its wrapper Ô¨Ålters all incoming data
according to the current adaptations (e.g., input Ô¨Ålters) before
sending the data to the binary. One next step is to revise
the program‚Äôs binary directly, and embed the input Ô¨Ålters as
preprocessors.
The generalization fuzz-tools and RFA are all domain-
general strategies, and we demonstrated this by using them
to improve program analysis on command-line Ô¨Ålter pro-
grams (e.g., wc), state-dependent standard input programs
(e.g., dc), and grammar-speciÔ¨Åc web programs (e.g., Apache
HTTP server). The most domain-speciÔ¨Åc enhancement is the
replace-delimited-chars tool that uses common de-
limiters to analyze portions of data. This tool contributed sig-
niÔ¨Åcantly to the speedup of FUZZBUSTER‚Äôs analysis of HTTP
headers in the Apache HTTP server experiment. We believe
that we will see additional performance beneÔ¨Åts by adding
more domain-speciÔ¨Åc structures to FUZZBUSTER, including
input grammars (e.g., packet header structure) and deeper
application models (e.g., recording application command-line
options and values).
We anticipate using the adaptive cybersecurity metrics from
this paper (see also [5], [4]) to evaluate future design decisions
for FUZZBUSTER and other active cybersecurity projects.
ACKNOWLEDGMENTS
This work was supported by DARPA and Air Force Research
Laboratory under contract FA8650-10-C-7087. The views expressed
are those of the author and do not reÔ¨Çect the ofÔ¨Åcial policy or position
of the Department of Defense or the U.S. Government.
Approved
for public release, distribution unlimited.
REFERENCES
[1] T. Kellerman, ‚ÄúCyber-threat proliferation: Today‚Äôs truly pervasive global
epidemic,‚Äù Security Privacy, IEEE, vol. 8, no. 3, pp. 70 ‚Äì73, May-June
2010.
[2] G. C. Wilshusen, ‚ÄúCyber threats and vulnerabilities place federal systems
at risk: Testimony before the subcommittee on government management,
organization and procurement,‚Äù United States Government Accountabil-
ity OfÔ¨Åce, Tech. Rep., May 2009.
[3] D. J. Musliner, J. M. Rye, D. Thomsen, D. D. McDonald, and M. H.
Burstein, ‚ÄúFUZZBUSTER: A system for self-adaptive immunity from
cyber threats,‚Äù in Eighth International Conference on Autonomic and
Autonomous Systems (ICAS-12), March 2012.
[4] D. J. Musliner, S. E. Friedman, T. Marble, J. M. Rye, M. W. Boldt, and
M. Pelican, ‚ÄúSelf-adaptation metrics for active cybersecurity,‚Äù in SASO-
13: Adaptive Host and Network Security Workshop at the Seventh IEEE
International Conference on Self-Adaptive and Self-Organizing Systems,
September 2013.
[5] D. J. Musliner, S. E. Friedman, J. M. Rye, and T. Marble, ‚ÄúMeta-control
for adaptive cybersecurity in FUZZBUSTER,‚Äù in Proc. IEEE Int‚Äôl Conf.
on Self-Adaptive and Self-Organizing Systems, sep 2013.
[6] W. Weimer, T. Nguyen, C. L. Goues, and S. Forrest, ‚ÄúAutomatically
Ô¨Ånding patches using genetic programming,‚Äù Software Engineering,
International Conference on, vol. 0, pp. 364‚Äì374, 2009.
[7] B. Miller, L. Fredriksen, and B. So, ‚ÄúAn empirical study of the
reliability of unix utilities,‚Äù Communications of the ACM, vol. 33, no. 12,
December 1990.
[8] C. Anley, J. Heasman, F. Linder, and G. Richarte, The Shellcoder‚Äôs
Handbook: Discovering and Exploiting Security Holes, 2nd Ed.
John
Wiley & Sons, 2007, ch. The art of fuzzing.
[9] H. Shrobe, R. Laddaga, B. Balzer, N. Goldman, D. Wile, M. Tallis,
T. Hollebeek, and A. Egyed, ‚ÄúAWDRAT: a cognitive middleware system
for information survivability,‚Äù AI Magazine, vol. 28, no. 3, p. 73, 2007.
[10] H. Shrobe, R. Laddaga, B. Balzer et al., ‚ÄúSelf-Adaptive systems for
information survivability: PMOP and AWDRAT,‚Äù in Proc. First Int‚Äôl
Conf. on Self-Adaptive and Self-Organizing Systems, 2007, pp. 332‚Äì335.
[11] ‚ÄúCortex: Mission-aware cognitive self-regeneration technology,‚Äù Final
Report, US Air Force Research Laboratories Contract Number FA8750-
04-C-0253, March 2006.
62
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-341-4
ADAPTIVE 2014 : The Sixth International Conference on Adaptive and Self-Adaptive Systems and Applications

