The Signiﬁcance of Imaginary Points in Linear Least Square Approximation
Mikael Fridenfalk
Department of Game Design
Uppsala University
Visby, Sweden
mikael.fridenfalk@speldesign.uu.se
Abstract—The linear least square method constitutes one of the
most useful statistical methods in mathematics. This paper shows
that in this method, real, versus imaginary points may act as
minimizers, versus maximizers of the error. The use of imaginary
points provides thereby an additional degree of freedom in the
design of methods based on this statistical method.
Keywords-complex number; curve ﬁtting; imaginary number;
least square method
I.
INTRODUCTION
Previous research on complex numbers in conjunction with
the linear least square method has been restricted to applica-
tions, such as constrained phases [2], stochastic processes [5],
and complex monomial neural networks [1]. In this paper,
a new application is presented, using real, versus imaginary
points as error minimizers, versus maximizers. The initial
motivation for the development of the method presented in
this paper was to accelerate the backpropagation process in the
evaluation of the weights of a large-scale feedforward neural
network. An analytic solution was however found along the
way with the potential to replace backpropagation in large-
scale neural networks [4]. As a brief overview, in Section II,
the theory behind the linear least square is reiterated, along
with a curve-ﬁtting example. In Section III, a new and more
generalized method is proposed for linear least square ﬁtting,
including imaginary points, followed by experimental results
in Section IV for the veriﬁcation of the proposed method.
II.
STATE OF THE ART
To begin with, the theory behind the standard method is
reiterated. To reproduce a textbook example on the subject [3],
given the inconsistent linear equation system:
x1 + x2 = 4
(1)
2x1 + x2 = 8
(2)
x1 + 2x2 = 5
(3)
or alternatively expressed, using a matrix A of size M × N
(with M = 3 and N = 2), and the vectors x and b:
Ax = b
(4)
A =
" 1
1
2
1
1
2
#
= [ c1
c2 ] =
" r1
r2
r3
#
(5)
x =

x1
x2

(6)
b =
" 4
8
5
#
=
" b1
b2
b3
#
(7)
where c1 and c2 are the column vectors of A, and r1, r2,
and r3 the row vectors of A. It is possible to schematically
represent such system by Fig. 1, with:
b = p + q
(8)
where b denotes in this example a vector outside a plane V,
spanned by c1 and c2, and p denotes the orthogonal projection
of b on V, as shown in Fig. 1. Due to orthogonal projection,
the smallest distance from b to V is ϵ = |p−b|, and therefore
ϵ2 = |p − b|2 is minimal. The least square error may in this
example be expressed as:
ϵ2 = ϵ2
1 + ϵ2
2 + ϵ2
3
(9)
|p−b|2 = (r1 ·x−b1)2 +(r2 ·x−b2)2 +(r3 ·x−b3)2 (10)
O
V
c1
c2
b
p
q
Figure 1.
Orthogonal projection of b on plane V, spanned by c1 and c2.
or in the general case, given row m in A and element m in
b, with p = Ax, as:
ϵm = |rm · x − bm|
(11)
Given an arbitrary vector y in RN:
Ay = y1c1 + y2c2 + . . . + yNcN
(12)
Since Ay will always lie in V, and is, therefore, orthogonal
to q, the linear least square method can be derived by the
following equations:
(Ay) · q = (Ay)T q = 0
(13)
(Ay)T (Ax − b) = 0
(14)
yT AT (Ax − b) = 0
(15)
yT (AT Ax − AT b) = 0
(16)
Thus:
AT Ax = AT b
(17)
x = (AT A)−1AT b
(18)
71
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-354-4
ADVCOMP 2014 : The Eighth International Conference on Advanced Engineering Computing and Applications in Sciences

The estimated solution x, is for maximum evaluation speed,
preferably solved by the direct solution of (17). As an example
of a curve ﬁtting application, given M points (am, bm):
a =


a1
a2
...
aM

 , b =


b1
b2
...
bM


(19)
and a second degree polynomial:
b = x1 + x2a + x3a2
(20)
A =


1
a1
a2
1
1
a2
a2
2
...
...
...
1
aN
a2
N


(21)
Equations (19) and (21) with (17) give thus the least square
solution x for the polynomial in (20), that best approximates
the distribution of the sample points in (19).
III.
PROPOSAL
As an example of the extension of the standard method
described above for least square curve ﬁtting, we present here
a similar method, but with the addition of imaginary points.
The deﬁnition of an imaginary point is in this paper any row
m in A and element m in b, that has been multiplied with i,
as in √−1. In the following example, m = 2 is designated as
an imaginary point:
a =


a1
a2 · i
...
aM

 , b =


b1
b2 · i
...
bM


(22)
Given the polynomial equation in (20):
A =


1
a1
a2
1
i
a2 · i
a2
2 · i
...
...
...
1
aN
a2
N


(23)
and the deﬁnitions of the square matrix U = AT A and the
vector v = AT b:
U =


1
i
. . .
1
a1
a2 · i
. . .
aN
a2
1
a2
2 · i
. . .
a2
N




1
a1
a2
1
i
a2 · i
a2
2 · i
...
...
...
1
aN
a2
N

 (24)
v =


1
i
. . .
1
a1
a2 · i
. . .
aN
a2
1
a2
2 · i
. . .
a2
N




b1
b2 · i
...
bN


(25)
Since i2 = −1, both U and v will only consist of real numbers,
yielding the least square equation:
Ux = v
(26)
Given the standard least square method, based on M sample
points, the squared error may for any sample m be expressed
as:
ϵ2
m = (rmx − bm)2
(27)
or more explicitly:
ϵ2
m =



[ am1
am2
. . .
amN ]


x1
x2
...
xN

 − bm




2
(28)
Therefore, the total squared error is equal to:
ϵ2 =
M
X
m=1
(rmx − bm)2
(29)
The division of the M sample points into J real points (aj, bj),
versus K imaginary points (ak, bk), yields the following
equations for the squared errors:
ϵ2
j =
 
 1
aj
a2
j

" x1
x2
x3
#
− bj
!2
(30)
ϵ2
k =
 

i
ak · i
a2
k · i

" x1
x2
x3
#
− bk · i
!2
(31)
By extraction of i and the relation i2 = −1:
ϵ2
k = (rkx · i − bk · i)2
(32)
ϵ2
k = i2(rkx − bk)2
(33)
ϵ2
k = −(rkx − bk)2
(34)
ϵ2 =
J
X
j=1
(rjx − bj)2 −
K
X
k=1
(rkx − bk)2
(35)
Thus, imaginary points seem to reverse the direction or “po-
larity” of the least square method. However, since this method
is based on projection, and the parameter that is minimized is
|ϵ|, large error components may reverse the expected polarities
of sample points.
IV.
EXPERIMENTAL RESULTS
An equation solver was developed in C++ for the solution
of linear equation systems of the form Ux = v. To optimize
the evaluation speed of U and v, a specialized matrix multipli-
cation method was implemented, by the reversal of the signs
of the products that correspond to imaginary points. Using this
equation solver, Figs. 2-9 show the results of curve ﬁtting of
a third degree polynomial, based on real (•), versus imaginary
points (×).
To comment these ﬁgures, Fig. 2 presents a curve ﬁtting
experiment using a third degree polynomial, with four real
points, and an imaginary point, placed slightly above the
line formed by the real points, yielding due to symmetry, a
parabolic curve. As shown here, while the distance to the real
points is minimized, the distance to the imaginary point is
maximized. Figure 3 presents the same case as in previous
ﬁgure, but with an imaginary point placed on the same line as
the one formed by the real points, yielding as expected a ﬂat
72
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-354-4
ADVCOMP 2014 : The Eighth International Conference on Advanced Engineering Computing and Applications in Sciences

line. Figure 4 presents the same case as in previous ﬁgure, but
here with an imaginary point placed slightly below the line
formed by the real points. Figure 5 presents the inversion of
real versus imaginary points with respect to previous ﬁgure.
Since the least square method minimizes |ϵ|, even if the system
is reversed (with ϵ2 < 0, where ϵ is an imaginary number), as
expected by the derived theory, the result remains the same.
Figure 6 presents a least square curve ﬁtting of a third degree
polynomial based on seven real points. Figures 7-8 present the
same case as in Fig. 6, except for the replacement of a real
point with an imaginary. Regarding Fig. 9, in our experiments,
a borderline point such as this showed to nominally reverse the
polarity of an imaginary point. A borderline point seems thus
to be able to affect the error by a signiﬁcant amount.
×
Figure 2.
Curve ﬁtting using a third degree polynomial, with four real points
(•), and an imaginary point (×).
×
Figure 3.
Same as previous ﬁgure, but with an imaginary point placed on
the same line as the one formed by the real points.
×
Figure 4.
Same as previous ﬁgure, but here with an imaginary point placed
slightly below the line formed by the real points.
×
×
×
×
Figure 5.
Inversion of real versus imaginary points.
Figure 6.
Least square curve ﬁtting of a third degree polynomial based on
seven real points.
×
Figure 7.
Same as previous ﬁgure, except for the replacement of a real point
with an imaginary.
×
Figure 8.
Same as previous ﬁgure.
×
Figure 9.
A demonstration of the effect of a borderline point.
V.
CONCLUSION
The linear least square method is associated with the eval-
uation of coefﬁcients of linear equation systems, minimizing
errors. This paper shows that pure imaginary points (as deﬁned
in this paper) in a such system, tend nominally to act as
squared error maximizers instead of minimizers. According
to experimental results, the new method needs to be used with
caution, since crossover between real and imaginary errors
affects the behavior of the system.
REFERENCES
[1]
M. F. Amin, R. Savitha, M. I. Amin, and K. Murase, “Orthogonal
Least Squares Based Complex-Valued Functional Link Network”, Neural
Networks, Elsevier, vol. 32, 2012, pp. 257-266.
[2]
M. Bydder, “Solution of a Complex Least Squares Problem with
Constrained Phase”, Linear Algebra Appl., vol. 433, no. 10-11, 2010,
pp. 1719-1721.
[3]
C. H. Edwards and D. E. Penney, Elementary Linear Algebra, Prentice
Hall, 1988.
[4]
M. Fridenfalk, “The Development and Analysis of Analytic Method
as Alternative for Backpropagation in Large-Scale Multilayer Neural
Networks”, accepted for publication in the proceedings of The Eighth
International Conference on Advanced Engineering Computing and Ap-
plications in Sciences, ADVCOMP 2014, Rome, Italy, August, 2014.
[5]
K. S. Miller, “Complex Linear Least Squares”, SIAM Review, Society
for Industrial and Applied Mathematics, vol. 15, no. 4, 1973, pp. 706-
726.
73
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-354-4
ADVCOMP 2014 : The Eighth International Conference on Advanced Engineering Computing and Applications in Sciences

