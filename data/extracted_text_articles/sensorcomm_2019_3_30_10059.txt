Design and Development of Medical Sensor Networks for Differently Abled
Persons
Veeraprathap Veerabharaiah, Fazaluddeen Doddahosur Mohammad Jamal and Narendra Kumar Gurumurthy
Dept. of Electronics & Communication Engineering,
University Visvesvaraya College of Engineering, Bangalore University,
Bangalore, India
Email: veeraprathap2001@gmail.com, fazaluddeen.dm@gmail.com, gnarenk@yahoo.com
Abstract— 13% of the global population is elderly and that
percentage is estimated to increase to 16.4% by 2030 as life
expectancy keeps increasing due in a great part to modern
medical technologies. Serving the elderly and the paralytic
persons is a noble cause; it is our responsibility to respect,
care and fulfill their needs. Communication for a person
having
a
speech
disorder
thoroughly
relies
upon
the
movement of their fingers, hands and articulations. In this
paper, we propose a glove based Sign To Speech (STS)
system that uses gestures of fingers and hand to convert the
American Sign Language (ASL) into a Speech Signal
synthesized to adapt for English and different Indian
languages. To improve the services to the people in need of
mobility assistance, a Sensor Controlled Wheelchair (SCW)
is
proposed
with
capabilities
of
navigating,
detecting
obstacles and moving automatically by utilizing Gesture and
Ultrasonic Sensors that establish Ad-hoc Sensor Networks.
The wheelchair is intended for elderly, medically challenged
and paralytic patients, so they can lead a peaceful and
satisfying life.
Keywords-Sign to Speech; American Sign Language;
Sensor Controlled Wheelchair.
I.
INTRODUCTION
Wheelchair
assistive
technology
provides
the
opportunity for people with mobility impairments, such as
imperfect limbs, weakness, muscle atrophy, stroke and
other symptoms, to move through indoor and outdoor
environments [1]-[3]. Depending on the lifestyle and
mobility of the wheelchair user, there are many types of
wheelchairs available including basic, lightweight, folding,
multi-function, special types, etc. Wheelchairs can be
divided
into
manual
wheelchairs
and
automatic
wheelchairs. Sensor Controlled Wheelchairs designed as
traditional and automatic wheelchairs are not always
suitable for elderly people.
Muteness is commonly referred to as an inability to
speak often caused by speech disorders and hearing loss.
Normally, people have the ability to freely communicate
between each other vocally without any barriers but cannot
communicate
with
people
with
a
speech
disorder.
Researchers
have
been
focusing
on
hand
gestures
detections and have been developing applications in the
field of robotics [4]-[6], in the extended area of artificial or
prosthetic hands, that can mimic the behaviour of a natural
human hand using American Sign Language (ASL).
American Sign Language is a natural predominant sign
language that has the same linguistic properties as spoken
languages with grammar that differs from the English
expressed by movements of the hands and face. It is the
primary language of Deaf and Mute communities in the
United States and Canada. The dialects of ASL and ASL-
based creoles are used in many countries around the world.
The movement of fingers is converted into alphabets
using American Sign Language, as shown in Figure 1.
Figure 1.
American Sign Language Alphabets [6].
Gesture recognition is classified into a pair of main
categories: vision and glove based techniques. The vision
based system fails during no visible light and it includes
challenges in image and video processing in varying
lighting conditions, backgrounds, field of scan constraints
and occlusion.
Our proposed system adopted a glove based technique
and will efficiently translate each gesture into speech.
Thus, it can be used by patients who are not able to speak,
but are able to move their fingers and hand.
The paper is organized as follows. Section II presents a
brief literature review and its implications. Section III
presents the proposed system for differently abled persons
based on Medical Sensor Networks. The model presented
in this section includes block diagrams. Section IV
presents the system implementation. The model presented
in this section includes a flowchart of Sign to Speech and
Sensor Controlled Wheelchair. Section V presents some of
the results obtained from the hardware module described
in Section IV. Conclusion and future scope are presented
in Sections VI and VII.
II.
LITERATURE REVIEW
A. Hartman et al. [1] has proposed autonomous vehicle
technology in
the
field
of
medical mobility where
wheelchair users can advance their movement using a
smart human and machine interface. The research focused
on the design of such robotic wheelchair, which is a
multilayered task incorporating hardware and software
55
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-744-3
SENSORCOMM 2019 : The Thirteenth International Conference on Sensor Technologies and Applications

with sensor technology, computer processing and power
distribution.
The
system
described
the
design
and
development of a smart wheelchair for autonomous path
planning and high- performance computing for real time
data processing.
S. Umchid et al. [2] have developed a Voice Controlled
Automatic Wheelchair. In general, due to many reasons
such as injury from accident, age and health problems,
physical disability occurs. The wheelchair is needed for
handicapped
people
travelling
to
other
places
by
themselves;
however,
people
with
hands
and
arms
impairment find it difficult to use a typical wheelchair. The
system’s objective was to design, develop and construct a
voice controlled automatic wheelchair.
T. Gomi et al. [3] have developed an Intelligent
Wheelchairs
for
the
handicapped.
A
standardized
autonomy management system that can be installed on
readily available power chairs, which have been well-
engineered over the years, has been developed and tested.
A behavior based approach was used to establish sufficient
onboard autonomy at minimal cost and material usage
while
achieving
high
efficiency,
sufficient
safety,
transparency in appearance and extendibility.
S. Faiz Ahmed et al. [4] have described Electronic
Speaking Glove for Speechless Patients. The electronic
speaking
glove
was
designed
to
facilitate
easy
communication through synthesized speech for the benefit
of speech impaired patients. Commonly, a mute person
communicating through sign language is not understood
by the majority of people. The gesture of fingers of a user
is converted into synthesized speech to convey an audible
message to other people.
M. M. Abdel-Aziz et al. [5] have invented a Smart
Communication System as deaf and mute people find a
difficulty in communicating with normal people. The sign
language translator system is used for reducing the
communication barrier between the deaf-mute people and
the people who do not have this impairment. The system
deals with all these problems and supports the Arabic sign
language and Arabic vocal language as well as efficient
and friendly communication between deaf-mute people
and the rest.
A. Al Mamun et al. [6] have described a Flex Sensor
Based Hand Glove for deaf and mute people. As sign
language is the only way to communicate for listening and
talking with disabled people, the designed system uses a
hand
glove
that
will
make
the
sign
language
understandable to all. An Android mobile phone app is
used to receive a voice, which will generate a speech
signal and send it to the hand glove through a wireless
communication system.
III.
PROPOSED WORK
The proposed Medical Sensor Networks (MSN) based
system provides an opportunity for elderly people with
mobility impairment to move through indoor and outdoor
environments and enables communication for persons
having a speech disorder, i.e. mute people. The Medical
Sensor Networks based system has a Sign to Speech node
and Sensor Controlled Wheelchair node.
Figure 2.
Block diagram of Sign to Speech system.
The Sign to Speech system starts with obtaining a signal
proportional to the movement of fingers. Fingers can
interpret different hand gestures. Research showed that
Flex and MPU6050 sensors based on finger and hand
gesture are best suited to convert sign to the text as it is
more reliable and cost effective solution. However, to
convert the obtained text into a speech signal, the E-Speak
Synthesis Engine enables the user to get a speech signal in
English, or predefined different Indian language, as shown
in Figure 2.
Figure 3.
Block diagram of Sensor Controlled Wheelchair.
The block diagram of the developed sensor controlled
automatic wheelchair is shown in Figure 3. The MPU6050
based hand gesture of the user is transmitted through 434
MHz RF (Radio-Frequency) transceiver to operate and
move the Sensor Controlled Wheelchair in different
56
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-744-3
SENSORCOMM 2019 : The Thirteenth International Conference on Sensor Technologies and Applications

directions. The Ultrasonic Sensors are employed for
automatic obstacle detection to stop the wheelchair during
object detection.
A.
E-Speak Speech Synthesis Engine
E-Speak [11] is a compact open source software speech
synthesizer for English and other languages, for Linux and
Windows, and uses a "formant synthesis" method. E-Speak
coverts text into speech signal for the following languages:
Afrikaans, Albanian, Aragonese, Armenian, Bulgarian,
Cantonese, Catalan, Croatian, Czech, Danish, Dutch,
English, Esperanto, Estonian, Farsi, Finnish, French,
Georgian, German, Greek, Hindi, Hungarian, Icelandic,
Indonesian, Irish, Italian, Kannada, Kurdish, Latvian,
Lithuanian, Lojban, Macedonian, Malaysian, Malayalam,
Mandarin,
Nepalese,
Norwegian,
Polish,
Portuguese,
Punjabi, Romanian, Russian, Serbian, Slovak, Spanish,
Swahili, Swedish, Tamil, Turkish, Vietnamese, Welsh.
Methods of accessing E-Speak:

The
command
line
program
of
Linux
and
Windows to speak the text from a file or from a
standard input.

On Windows, a shared DLL library of E-Speak.
The features of E-Speak:

Includes many languages with different voices
whose characteristics can be altered.

It provides flexibility to generate WAV file for
Synthesized Speech Output.

Provides program and data memory of about 2
Mbytes.

Converts text to phonemes with pitch and length
information and can be used as a front-end to
MBROLA diphone voices.

Modifies as a front end for another speech
synthesis engine.

Used with screen-readers and other programs
supporting the Windows SAPI5 interface.

Supports Speech Synthesis Markup Language
(SSML) and HTML language and other platforms
including Android, Mac OSX and Solaris.
B.
Matrix Laboratory (MATLAB)
MATLAB
[12]
is
a
multi-paradigm
numerical
computing
environment
and
patented
programming
language developed by MathWorks. It allows matrix
manipulations,
plotting
of
functions
for
data,
implementation of algorithms, creation of user interfaces
and interfacing with programs written in other languages
including C, C++, C#, Java, Fortran and Python.
C.
Flex Sensor
The Flex Sensor, also known as Bend Sensor,
measures the amount of bending. The resistance of the
sensor element is proportional to the amount of bending; it
is
used
as
goniometer
and
often
called
flexible
potentiometer.
TABLE I.
CHARACTERISTICS OF FLEX SENSOR.
Range of
Resistance
1.5-40 kΩ depending on sensor type. Flex point claims 
a 0-250 kΩ resistance range. 
Range of
Temperature
-35 to +80 degrees Celsius
Lifetime
Lifetime greater than one million life cycles
Hysteresis
Hysteresis 7%
Voltage
Voltage 5 to 12 V
Size
Approximately 0.28" wide and 1"/3"/5"
The Flex Sensor contains carbon resistive elements
within a thin flexible substrate. The characteristics are
described in Table I. The sensor produces a resistance
output
relative
to
the
bend
radius.
Practically,
the
deflection in degrees of 0, 20, 40, 45, 50, 70 and 90 will
give 10 kΩ, 14.5 kΩ, 18.8 kΩ, 20 kΩ, 21.1 kΩ, 25.5 kΩ 
and 30 kΩ of resistances, respectively. 
D.
Motor Driver
The Motor Driver L293D is a Dual H-bridge IC; it
allows the DC motor to drive in either direction and can
control a set of two DC motors simultaneously in any
direction. The clockwise and anti-clockwise rotation of
the DC motor depends on the polarity of the applied
voltage. A 9-volt battery is used to power the motor driver
for driving the DC motors.
E.
MPU6050 Sensor Module
The MPU6050 Sensor Module is a 6-axis Motion
Tracking Device and combines 3-axis gyroscope, 3-axis
accelerometer and Digital Motion Processor in a single
package
with
the
additional
feature
of
on-chip
Temperature
Sensor.
It
has
I2C
bus
interface
to
communicate with the microcontrollers and auxiliary I2C
bus
to
communicate
with
the
sensor
devices
magnetometer, pressure sensor, etc.
F.
Ultrasonic Sensor
The Ultrasonic Sensor measures the distance of the
nearest object. HC-SR04 is employed for the automatic
obstacle detection and its range is from 2 cm to 3 meters.
Distance = (Echo Pulse Duration *.0343)/2
(1)
The Ultrasonic Sensor detects the nearest object by
emitting a short pulse and receiving the reflected echo. The
time spent by the pulse signal to reach the object and
return is used to determine the relative distance (1).
IV.
SYSTEM IMPLEMENTATION
The working principle of the developed Sign to Speech
system is presented in a flowchart, as shown in Figure 4.
After the hand glove is powered up, every fingers and hand
movement is analyzed each time to detect word or
character for its given American Sign Language pattern.
The gesture of fingers is obtained by Flex Sensor and hand
by MPU6050 Sensor. Depending on the degree of bending,
the Flex Sensor values are mapped onto eight different
values 0, 1, 2, 3, 4, 5, 6 and 7.
57
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-744-3
SENSORCOMM 2019 : The Thirteenth International Conference on Sensor Technologies and Applications

Figure 4.
A flowchart of Sign to Speech System.
The words and characters of the English language are
recognized based on values of Flex Sensor and orientation
of MPU6050. These are described in Tables II and III.
TABLE II.
WORD FORMATION.
Word
PF
RF
MF
IF
TF
MPU6050
Orientation
FOOD
2|3
2|3
4|5
4|5
2|3
X>5
NEWSPAPER
1|2
1|2
1|2
1|2
1|2
X>5, Y<5
WASHROOM
4|5
3|4
4|5
3|4
2|3
X>5
SICK
1|2
1|2
3
1|2
1|2
X>5
MEDICINE
1|2
1|2
4|5
1|2
1|2
X>5
EMERGENCY
5|6
4|5
4|5
4|5
4|5
X>5
WATER
4|5
1|2
1|2
1|2
4|5
X>5
The system provides flexibility to convert multiple
signs into text depending on the combination of American
Sign Language words and characters. However, the
obtained text is processed in MATLAB using E-Speak to
get a synthesized speech signal in English and other Indian
languages. This speech signal is used by a mute patient to
communicate with other people to inform or their needs,
desires and any other problems.
TABLE III.
CHARACTER FORMATION.
Character
PF
RF
MF
IF
TF
MPU6050
Orientation
A
5|6
5|6
5|6
5|6
1|2
X>5
B
1|2
1|2
1|2
1|2
4|5
X>5
C
2|3
2|3
2|3
2|3
2|3
X>5
D
3|4
3|4
3|4
1|2
2|3
X>5, Y>2
E
5|6
5|6
5|6
5|6
5|6
X>5
F
1|2
1|2
1|2
4|5
3|4
X>5
G
4|5
4|5
4|5
1|2
1|2
X>0, Y>5
H
4|5
4|5
1|2
1|2
2|3
X>0, Y>5
I
1|2
4|5
4|5
4|5
4|5
X>5, Y<5
J
1|2
4|5
4|5
4|5
4|5
X>5, Y>5
K
4|5
4|5
1|2
1|2
2|3
X>5
L
5|6
4|5
5|6
1|2
1|2
X>5
M
5
4
4
4
4
X>5
N
5
5
4
4
4
X>5
O
4
4
4
4
3
X>5
P
3|4
3|4
3|4
1|2
2|3
X<5, Y<5
Q
4|5
4|5
4|5
2|3
2|3
X<5, Y<5
R
4|5
4|5
1|2
3|4
4|5
X>5
S
4|5
4|5
4|5
4|5
4
X>5
T
4|5
4|5
4|5
4|5
3
X>5
U
4|5
4|5
2|3
1|2
4|5
X>5, Y<2
V
4|5
4|5
1|2
1|2
4|5
X>5, Y>2
W
4|5
1|2
1|2
1|2
4|5
X>5
X
5|6
4|5
5|6
3|4
4|5
X>5
Y
1|2
5|6
5|6
5|6
1|2
X>5
Z
5|6
4|5
5|6
1|2
2|3
X>5, Y<-2
Note: PF=Pinky Finger, RF=Ring Finger, MF=Middle Finger, IF=Index
Finger, TF=Thumb Finger, | = Logical – OR
The movement of the Sensor Controlled Wheelchair is
dependent on the orientation of the MPU6050 dictated in
Table IV. A single MPU6050 is shared between the nodes
in the Medical Sensor Network.
TABLE IV.
WORKING STATUS OF SENSOR CONTROLLED
WHEELCHAIR.
MPU6050
Orientation
RF Bits
Movement of Wheelchair
X<0
1000
FORWARD
X>0
0001
BACKWARD
Y>0
0100
RIGHT
Y<0
0010
LEFT
X=0 & Y=0
0000
STOP
58
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-744-3
SENSORCOMM 2019 : The Thirteenth International Conference on Sensor Technologies and Applications

The Sensor Controlled Wheelchair has five types of
motions, namely, moving forward, moving backward,
moving to the left, moving to the right and stop motion.
The Ultrasonic Sensors have been included to stop the
wheelchair during obstacle detection.
Figure 5.
A flowchart of Sensor Controlled Wheelchair.
The
working
principle
of
the
developed
Sensor
Controlled Wheelchair is presented in a flowchart, as
shown in Figure 5. The orientation of MPU6050 is
transmitted to the Sensor Controlled Wheelchair through
RF wireless communication includes RF Encoder and
Decoder to encrypt the data and remove noise. The Motor
Driver L293D controls the rotation of the DC motors in
clock and anti-clock wise direction so that the required
movement of the wheelchair is achieved.
V.
RESULT EVALUATION
The implementation of Medical Sensor Networks for
differently-abled persons has shown good results in
encouraging the patients during recovery transition.
Figure 6.
Sign for the English Alphabet ‘I’
Figure 6 shows the sign for English Alphabet letter ‘I’
based on American Sign Language, getting converted into
the corresponding text on the right-hand side by processing
the obtained values of Flex and MPU6050 Sensors.
Figure 7.
Sign for The English Word ‘NEED’
Figure 7 shows the sign for the English word ‘NEED’
based on the American Sign Language getting converted
into the corresponding text on the right-hand side.
Figure 8.
Sign for The English Word ‘MEDICINE’
Figure 8 shows the sign for the English word
‘MEDICINE’ based on American Sign Language, getting
converted into the corresponding text on the right-hand
side.
Figure 9.
Signs to Speech Conversion in the English Language
Based on the words and characters of American Sign
Language, the obtained text is synthesized in MATLAB
using Speech API to generate a Speech Signal in the
English language, as shown in Figure 9.
Figure 10. Signs to Speech Conversion in the Kannada Language
The obtained text in the English language is converted
into the Kannada language and then synthesized using E-
Speak to generate a Speech Signal in the Kannada
language, as shown in Figure 10.
59
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-744-3
SENSORCOMM 2019 : The Thirteenth International Conference on Sensor Technologies and Applications

Figure 11. Object Detection in Sensor Controlled Wheelchair
The
Sensor
Controlled
Wheelchair
gets
stopped
automatically as it finds any object in the path it moves
using
the
Ultrasonic
Sensors
and
displays
the
corresponding message on the right side, as shown in
Figure 11.
Figure 12. Moving Sensor Controlled Wheelchair in Forward Direction
The Sensor Controlled Wheelchair moving in forward
direction as the horizontal orientation of MPU6050 is less
than zero makes the RF bits equal to 1000, as shown in
Figure 12.
Figure 13. Moving Sensor Controlled Wheelchair in Left Direction.
The Sensor Controlled Wheelchair moving towards left
as the vertical orientation of MPU6050 is less than zero
makes RF bits equal to 0010, as shown in Figure 13.
Figure 14. Moving Sensor Controlled Wheelchair in Backward
Direction
The Sensor Controlled Wheelchair moving in the
backward
direction
as
the
horizontal
orientation
of
MPU6050 is greater than zero makes RF bits equal to
0001, as shown in Figure 14.
Figure 15. Moving Sensor Controlled Wheelchair in Right Direction
The Sensor Controlled Wheelchair moving towards the
right as the vertical orientation of MPU6050 is greater than
zero makes RF bits equal to 0100, as shown in Figure 15.
Figure 16. Sensor Controlled Wheelchair is Stopped
The Sensor Controlled Wheelchair gets stopped as the
vertical and horizontal orientation of MPU6050 is equal to
zero makes RF bits equal to 0000 and displays the
corresponding message on the right side, as shown in
Figure 16.
VI.
CONCLUSION
Our proposed system uses Sign to Speech, which
converts American Sign Language into a Speech Signal
useful for speech impaired elderly patients to fill the
communication
gap
between
patients,
doctors
and
relatives. The output speech can be in English or other
Indian languages for the mute patient to express their
needs by using gestures.
The Sensor Controlled Wheelchair operated by the
gestures of hand provides the opportunity for the old age
patients with mobility impairment to move through indoor
and
outdoor
environments.
The
automatic
obstacle
detection is implemented in the system to stop the
movement of the wheelchair automatically, providing easy
access for elderly people and automatic protection from
obstacle collision.
VII.
FUTURE SCOPE
The Flex Sensors based hand glove can be replaced by
an E-Textile make hand glove that is light weight and less
wiring is required; however, it can be very expensive.
60
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-744-3
SENSORCOMM 2019 : The Thirteenth International Conference on Sensor Technologies and Applications

The K-Nearest Neighbors Algorithm can be adopted as
the graphical view of the characters, words and sensor
values change with different persons.
REFERENCES
[1]
A. Hartman, R. Gillberg Jr, C. T. Lin, and V. K.
Nandikolla, “Design and development of an autonomous
robotic wheelchair for medical mobility,” IEEE, 2018.
[2]
S.
Umchid,
P.
Limhaprasert, S.
Chumsoongnern,
T.
Petthong,
and
T.
Leeudomwong,
“Voice
Controlled
Automatic Wheelchair,” The 2018 Biomedical Engineering
International Conference, 2018.
[3]
T.
Gomi
and
A.
Griffith,
“Developing
Intelligent
Wheelchairs for the Handicapped,” Springer, 2006.
[4]
S. Faiz Ahmed, S. M. Baber Ali, and S. S. Munawwar
Qureshi,
“Electronic
Speaking
Glove
for
Speechless
Patients - A Tongue to a Mute,” IEEE Conference on
Sustainable Utilization and Development in Engineering
and Technology, 2010.
[5]
M. M. Abdel-Aziz, M. M. Abdel-Masieh, and M. M.
Nasief, “Smart Communication System for Deaf- Dumb
People,” Int'l Conf. Embedded Systems, Cyber-physical
Systems, & Applications, 2017.
[6]
A. Al Mamun, M. Sarwar Jahan Khan Polash, and F.
Mashuque Alamgir, “Flex Sensor Based Hand Glove for
Deaf and Dumb People,” International Journal of Computer
Networks and Communications Security VOL. 5, NO. 2,
February 2017, 38–48
[7]
N. Ambika, G. K. Rama Murali, S. Sheka, and G. Narendra
Kumar, "TTS System for Coal Miners in MANET Based
Disaster Management System," 2nd Intl Multi-Conference
on Complexity, Informatics and Cybernetics, Mar 2011.
[8]
C. K. Huang, Z. W. Wang, G. W. Chen, and C. Y. Yang,
“Development of a Smart Wheelchair with Dual Functions:
Real-time
Control
and
Automated
Guide,”
2nd
International
Conference
on
Control
and
Robotics
Engineering, 2017
[9]
T. Arsan, “Sign Language Converter,” International Journal
of Computer Science & Engineering Survey (IJCSES)
Vol.6, No.4, August 2015.
[10] D. Wang and H. Yu, “Development of the control system
of
a
voice-operated
wheelchair
with
multi-posture
characteristics,” 2nd Asia-Pacific Conference on Intelligent
Robot Systems (ACIRS), 2017, pp. 151-155.
[11] E-Speak,
http://espeak.sourceforge.net/
[retrieved
Oct.
2019]
[12] MATLAB,
https://www.mathworks.com/products/matlab.html
[retrieved Oct. 2019]
61
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-744-3
SENSORCOMM 2019 : The Thirteenth International Conference on Sensor Technologies and Applications

