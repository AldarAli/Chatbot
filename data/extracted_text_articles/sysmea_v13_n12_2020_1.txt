1
International Journal on Advances in Systems and Measurements, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/systems_and_measurements/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Development of Front-End Readout Electronics System for the ALICE           
HMPID and Charged-Particle Veto Detectors 
 
Clive Seguna, Edward Gatt, Ivan Grech, Owen Casha  
Department of Microelectronics and Nanoelectronics  
University of Malta  
Msida, Malta   
e-mail: {clive.seguna, edward.gatt, ivan.grech,  
owen.casha}@um.edu.mt   
Giacinto De Cataldo 
Department of Physics 
University of Bari 
Bari Italy 
                       e-mail:Giacinto.de.Cataldo@cern.ch 
Yuri Kharlov, Artem Shangaraev 
Department of Physics 
Institute for High Energy Physics, Protvino 142281, Russia 
e-mail: {Yuri.Kharlov, Artem.Shangaraev}@cern.ch 
 
Abstract— Luminosity of lead-ion collisions at the Large 
Hadron Collider will be increased in the forthcoming Run 3 to 
6×1027 cm2s-1, corresponding to an average inelastic interaction 
rate of 50 kHz. At the same time, paradigm of data taking of the 
ALICE experiment changes aiming to collect and process all 
interaction data, which represents an increase in data sample 
rate by two orders of magnitude with respect to the present 
system. This requirement demands a reliable readout electronic 
system with an increase in data bandwidth, strict timing 
constraints, and low power consumption. This work presents 
the hardware architecture of a newly developed front-end 
readout electronic system for the Charged-Particle Veto 
detector, located in the Photo Spectrometer at the A Large Ion 
Collider Experiment situated at the largest European facility for 
Nuclear Research, CERN. The developed front-end hardware 
architecture enables the simultaneous readout of 23,040 cathode 
pad channels for amplitude analysis, contributing a ten-fold 
increase in bandwidth when compared to prior system. Main 
contributions to this achievement include the re-design of highly 
dense interconnect printed circuit boards, use of 3.125 Gbps 
data links and the implementation of a radiation tolerant 
firmware architecture using low power 28 nm field 
programmable gate arrays. Measurement results indicate that 
the newly developed data acquisition electronic system satisfies 
the target detector readout rate requirements. This paper 
discusses the firmware and hardware implementation details, 
followed by the presentation of the performance measurement 
results for the recently developed Charged-Particle Veto 
detector front-end readout topology when compared to other 
particle detector electronic systems. 
Keywords- electronics; readout; detector; FPGA; VHDL. 
I.  INTRODUCTION  
This journal paper is an extension of the work originally 
presented at 11th Conference on Advances in Circuits, 
Electronics and Micro-electronics CENICS 2018 [1]. This 
Section explains the overall system architecture and 
performance of the prior CPV readout electronic system 
operated in the ALICE experiment during the LHC Run 1 and 
Run 2 (2010-2018). Particle detectors such as Charged 
Particle Veto Detector (CPV) and High Momentum Particle 
Identification (HMPID) are required to find and possibly 
identify all the particles emerging from a scattering event. 
The design of such detectors for colliding beam experiments 
presents several challenges. These include minimization of 
materials because of limited space and services, low-power 
consumption and potentially high data rates and reliability 
when running a specific radiation tolerance level. Particle 
detectors rely on custom designed electronic hardware for 
identifying specific types of particles. Although detectors 
appear to be very different, basic principles of the readout 
apply to all. They consist of a signal current sensor whose 
output after integration yields to a charge proportional to the 
amount of energy collected. 
Every particle detector needs to a have a custom designed 
Front-End electronics (FEE) system to be able to detect 
specific beam types and particle characteristics, therefore, the 
equipment must be modular, and adaptable. Additionally, the 
design criteria for a particle detector depends on application, 
energy resolution, rate or timing capability requirements, and 
sensing positioning. Large-scale systems additionally impose 
compromises on power consumption, scalability and easy 
monitoring with a reduction in maintenance cost. Today, 
multi-channel systems are additionally required in many 
fields. In large systems power dissipation and size are critical, 
so systems are not necessarily designed for optimum noise, 
but adequate noise, and circuitry needs to be tailored to 
specific detector requirements. The present CPV detector 
FEE limits the present readout rate requirements and 
luminosity levels, therefore, this results in the need of 
developing a newly customized FEE system that meets 
specific design criteria. 
The CPV is a Multi-Wire Proportional Chamber 
(MWPC) with cathode-pad readout located in the A Large 
Ion Collider Experiment (ALICE) [2]. It is used to suppress 
detection of charged particles hitting the front surface of the 
Photon Spectrometer (PHOS) in order to improve photon 
identification [3]. CPV charged-particle detection efficiency 
is better than 99%. The spatial resolution of the reconstructed 
impact point is about 1.54 mm along the beam direction and 
1.38 mm across the beam.  The CPV pad electronics is 
identical to the one used for the A Large Ion Collider 
Experiment 
(ALICE) 
High 
Momentum 
Particle 
Identification (HMPID). A primary consideration for PHOS 
is that it has an optimal performance for measuring photons 

2
International Journal on Advances in Systems and Measurements, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/systems_and_measurements/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
in the energy range from few hundred MeV to 100 GeV. The 
PHOS contains 12,544 detection channels based on lead 
tungstate crystals. During the LHC Run 2, CPV consisted of 
one module installed on the top of one of the PHOS module. 
The CPV module consists of: 
 
• 
16 columns; 
• 
10 cards per column, where each card consists of 
three ASIC charge amplifier signal condition chips 
called Gassiplex07-3; 
• 
48 cathode readout pads per Gassiplex07-3 card; 
• 
7680 channels of amplitude analysis per module. 
The ALICE experiment is dedicated for studying 
properties of strongly interacting matter created in high-
energy heavy ion and proton collisions. The current system 
still leaves open physics questions that need to be addressed, 
and these questions relate to, among others, hadronization, 
nuclei, long range capability correlations and small x-proton 
structure [4]. CPV electronics consists of dedicated 
Application Specific Integrated Circuit (ASIC) devices in 
each column, Gassiplex for analogue signal processing and 
DiLogic for handling the digitized information. Every 
column consists of 10 cards with Gassiplex chips, called        
3-GAS cards interfaced directly on the backside of the 
MWPC cathode. A customized electronic board called 
DiLogic contains five channels of 12-bit Analogue-to-Digital 
Converter (ADC) modules and five DiLogic (5-DIL) 
processors [5].  
Each column contains 480 pads connected with two          
5-DIL cards and a group of Field-Programmable Gate Arrays 
(FPGAs) called column and segment controllers for 
processing various control signals, and additionally provide 
the necessary interface to the Data Acquisition (DAQ) 
module and Central Trigger Processor (CTP). Further, CTP 
is responsible for the generation of three trigger level signals 
L0, L1 and L2. At power-on an order of 1000 events are 
collected with the zero-suppression turned off so to measure 
the pedestal levels. For each channel, the thresholds are 
calculated as Thr(j) = Ped(j) + N(j)*Sig(j), where Ped(j) is the 
pedestal mean for channel j, Sig(j) is the corresponding r.m.s. 
value and N(j) is a parameter, usually set to 3. The pedestals 
and thresholds tables are downloaded into the DiLogic chip 
memory and finally the zero-suppression is turned on. The 
system is then ready for normal DAQ operation. The L0 
trigger is used to store the analogue information inside the 
Gassiplex chip and start the multiplexing sequence. Then the 
pulse height information for analogue channels above 
threshold is acquired after pedestal subtraction, and then 
stored in the DiLogic internal First-In-First-Out (FIFO) 
memory (512 x 18-bit words) together with the 
corresponding analogue channel address. The data from two 
5-DIL cards each having 5 digital signal processor chips 
connected in a daisy chain are collected in the column FIFO 
as shown in Figure 1. Finally, at arrival of the L2 trigger 
signal event data is transferred to the DAQ experiment 
through a 2.125 Gbps optical Detector Data Link (DDL) for 
further processing.  
A typical event size consists of 1.3 Kbytes for Pb-Pb 
particles. With just firmware upgrade the maximum possible 
event readout rate that the present detector electronics  can 
reach is 10 kHz for an occupancy of 1%, therefore, due to this 
technical limitation, a new front-end readout electronic 
system has been developed to collect more than 10 nb-1 of  
Pb-Pb collisions at luminosities of up to 6 x 1027 cm-2 s-1.  As 
shown in Figure 2 the busy time is the period, where the 
FPGA busy signal goes high, from the arrival of the L0 
trigger to the end of the transmission of an event. Therefore, 
it includes the waiting time of a L2 trigger (about 108 µs) and 
the transmission time depending on the number of words 
from the 32-bit data bus fbD[31..0]. The fixed part consists 
of headers and markers: 
            
• 
Common Data Header (CDH) (10 words, 40 bytes); 
• 
CPV header (5 words, 20 bytes); 
• 
Column headers (24 words, 96 bytes); 
• 
DILOGIC markers (240 words, 960 bytes); 
• 
Segment markers (3 words, 12 bytes). 
     The data block transmitted by the frontend readout 
electronics to DAQ system consists of a fixed part and 
another of variable length. 
                              
 
Figure 1. Block Diagram of Column Controller. 
 
                          
 
Figure 2. Timing diagram for the acquisition of a physics event-data, 
showing the relationship between various detector data and control signals. 
 

3
International Journal on Advances in Systems and Measurements, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/systems_and_measurements/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
     Therefore, each event consists of total of 1128 bytes, 
currently being transferred at a rate of not more than 5 kHz 
by readout electronics. The variable event rate depends on the 
number of activated channels. 
     The plot shown in Figure 3 illustrates that the detector 
event readout rate decreases with the number of transmitted 
or increased detector occupancy. On the prior system, the 
Frontend Electronics (FEEs) implements a full-duplex Serial 
Interface Unit (SIU). The main SIU task is to transmit event 
data, receive control commands from Read-out Receiver 
Card (RORC) or Destination Interface Unit (DIU) located on 
DAQ via DDL. 
                                                       
 
Figure 3. Prior System Readout rate versus Occupancy with   100 kHz trigger 
rate [6]. 
 
    The 2.125 Gbps DDL optical interface has two interfaces: 
the FEE-SIU interface and the RORC - DIU DDL interface. 
Data transfer from DIU to computer farm for further 
processing is done through PCI-X communication interface 
as shown in Figure 4. 
 
 
 
Figure 4. DDL components in data taking configuration between FEE and 
DAQ [7]. 
 
The present CPV readout architecture contributes to a low 
particle readout rate of 5 kHz or 200 µs busy time. The 
ALICE experiment is scheduled to start running with an 
interaction rate of 50 kHz of all Pb-Pb events in 2021 (Run3). 
Therefore, the installation of the newly produced CPV 
electronic system, which includes 3 modules, each having 8 
optimized FPGA column controller cards shall contribute to 
an increase in bandwidth, data rate and drastic reduction in 
busy time by ten-fold. In Run3, CPV will consist of                    
3 modules installed in front of 3 PHOS modules at sectors 
260-280°, 280-300°, 300-320°. The main goal of CPV is to 
improve photon identification in PHOS via charged cluster 
rejection, therefore, events taken by PHOS and CPV should 
be in complete synchronization. As PHOS plans to upgrade 
readout rate up to 40-50 kHz, CPV should not be slower than 
PHOS but possibly faster.  
Current readout rate of the old CPV electronics can be 
increased from 5 kHz to 10 kHz just by a firmware upgrade 
of the readout board. Further increase of readout rate is not 
possible without hardware upgrade. Therefore, new readout 
cards had to be designed and produced keeping the same data 
flow as the one implemented in the previous system but 
taking a chance to speed up by tenfold the read-out event rate.  
The rest of the paper is structured as follows. Section II 
gives an overview of the newly developed readout electronic 
hardware including two types of printed circuit boards: 
FPGA based Column controller and passive motherboard 
called Segment. Section III provides a description of the 
implemented and optimized column controller electronic 
hardware. Simulation and measurement results are shown in 
the following sections. Finally, Section X presents the 
novelty of this work, other conclusions and future work. 
II. NEW SYSTEM ARCHITECTURE 
     The evaluation of various FPGA-based electronic boards 
that are currently available in the market was performed, and 
it was concluded that no FPGA electronic card with the 
required features is available for the development of this new 
CPV detector readout electronics. Therefore, a complete          
re-design and implementation of all the detector electronic 
controller cards had to be completed.  
      As requested by ALICE collaboration, the objective is to 
preserve the charge-sensitive amplifiers (3-GAS cards) and 
Analogue-to-Digital Converter (ADC) together with the 5-
DIL processors, 96 cards in total, while upgrading the column 
controllers (CCs) (from16 CCs/module to 8 CCs/module).                          
       Every column controller shall simultaneously process 
two columns of 480 3-Gassiplex channels. In total leading to 
24 FPGA column controller cards for all the three CPV 
modules, with a total of 7680 3-Gassiplex channels per 
module. Additionally, segment motherboards shall be 
upgraded to eight per module leading to a total of 24 segment 
boards.  
      The upgrade for the new CPV readout electronic system 
includes the parallel readout of all column controllers via 
3.125 Gbps FPGA unidirectional transceiver serial links and 
integration with the ALICE Inner Tracking System Readout 
Unit (ITS-RU) [8]. The ITS-RU frontend electronic system is 
divided into a modular Readout Units (RU), identical for each 
layer. Each Readout unit controls an entire stave, including 
power to the sensors (through custom-made power units) [9]. 

4
International Journal on Advances in Systems and Measurements, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/systems_and_measurements/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
 
Figure 5. Block diagram for the upgrade of CPV and HMPID electronics. 
                          
   
 
Figure 6. Developed Readout electronic cards for one half of the CPV 
module (4 Segments, 4 CCs, 16 5-DIL cards and 1 ITS-RU). 
                     
      
 
 
Figure 7. CPV module for the simultaneous Readout of 7,680 analogue 
channels (8 CCs, 32 5-DIL cards and 8 Transceiver links). 
 
      The ITS-RU shown in Figure 8 will serve to 
simultaneously transmit event data from all 24 CPV/HMPID 
column controllers to the Online computing system (O2) 
using the SMA-based connectors Gigabit transceivers. All 
upgraded cards are shown in Figure 5 and Figure 6. 
 
III. OPTIMZED FPGA COLUMN CONTROLLER 
The layout of the optimized FPGA CPV column 
controller card is shown in Figure 9 and Figure 10. The card 
has a 364 pins High-Speed Mezzanine edge connector, 
includes a Cyclone V GX Intel FPGA, three power supply 
voltage regulators of 3.3 V, 1.1 V and 2.5 V, a Low-Voltage 
Differential Signalling (LVDS) Fire-Fly connector, and high-
speed full-duplex transceiver links for command and event 
data transfer between FPGA column controllers and ITS-RU. 
The newly adopted architecture allows the simultaneous 
readout of two columns, where each column contains 480 
Gassiplex channels, an improvement of two-fold when 
compared with the prior system. The two main simultaneous 
operations that are required to be performed by the newly 
developed FPGA-based controller card include frontend and 
backend operations. The various modes of operations are 
listed in Table I. The firmware and hardware of the CC allow 
to work with four 5-DIL cards in parallel. 
 
    
 
Figure 8. Block Diagram for ITS-RU. 
                                                              
 
Figure 9. Block Diagram for FPGA-based controller card [10].       
             
 
Figure 10. Manufactured FPGA column controller card for CPV Readout 
electronics. 
Four 5-DiLogic Cards 
 
ITS Readout Unit 
 
Column 
Controller Card 
 
3.3V, 2.5V, 
1.1V, Voltage 
Regulators 
 
Fire-Fly 
Connector 
 
   3.125 Gbps  
SMA Connectors 
 
Programmable  
Clock 
 
Cyclone V GX  
FPGA 
 
364-pins HSMC Edge 
Connector 
 

5
International Journal on Advances in Systems and Measurements, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/systems_and_measurements/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
                         
 
Figure 11. Manufactured Segment card for CPV Readout electronics. 
Figure 11 illustrates the main interfaces for the developed 
segment card, for interfacing column controller and four           
5-DIL processor cards, as shown in Figure 9.  
          TABLE I. FUNCTION CODES FOR FPGA CONTROLLER CARD. 
Operating  
Modes/Codes 
Description 
Type of 
Operation 
“1010” 
Analogue Readout 
Back-End 
“0111” 
Data Acquisition 
Front-end 
“1000” 
DiLogic Pattern Read Out  
Back-End 
“1110” 
DiLogic Write Configuration 
Back-End 
“1111” 
DiLogic Read Configuration 
Back-End 
“0000” 
Test Mode - DiLogic 
Front-End 
 
IV. 
COLUMN CONTROLLER CARD - FRONT-END 
OPERATIONS 
A. Data Acquisiton and Test mode 
 
As shown in Figure 12, prior starting a data acquisition 
the DiLogic processor should have the pedestal threshold 
memory filled with pedestal and operating threshold values 
for each of the 480 channels.   
 
 
  Figure 12. Simulation - Timing Diagram Data Acquisition.  
 
 
 
The frontend card must activate the CLR_N and RST for 
initializing DiLogic processor prior data taking. Every event 
starts with a pulse on TRIGN pin and several clock cycles on 
the C1_CLKD_N and CLK_ADC_N pins depending on the 
number of channels indicated by the binary value “10” on the 
C1_GX pin. An extra clock cycle is necessary to store the 
end-event word and turn off the readout of an event. Timing 
diagram for the complete data acquisition sequence is shown 
in Figure 12.  
Similar timings can be applied for Test-mode operation 
but instead of asserting the amplitude and channel address on 
the input pins, they should be filled through the I/O Digital 
bus, 
respectively 
with 
channel 
address 
on 
C1_DATA_BUS0/1 (17:12) on D17-D12 and Amplitude on 
C1_DATA_BUS0/1 (11:0). 
 
V. 
COLUMN CONTROLLER CARD - BACK-END 
OPERATIONS 
A. DiLogic Read and Write Configurations 
 
    The DiLogic threshold and offset memory can be read 
using the function code “1111”, where pins ENIN1N, 
ENIN5N are set to low and STRINnD1, STRINnD2 strobe 
cycles are applied.  
 
   
 
Figure 13. Simulation - Timing Diagram for DiLogic Read Configuration.  
   The data will appear on the data bus after the falling edge 
of strobe and will stay stable until the rising edge of 
STRINnD1, and STRINnD2 pins as shown in Figure 13. The 
threshold and offset memory of DiLogic chip can be loaded 
using the function code “1110” shown in Figure 14, where 
the ENIN1N and ENIN5N pins are set to low and 
STRINnD1, STRINnD2 strobe cycles are applied. 
Four PCI DiLogic 
Card  
Connectors 
 
JTAG  
10-pin header 
connectors 
 
FPGA Column Controller 
HSMC Interface 
 
Voltage Level Translators 
 

6
International Journal on Advances in Systems and Measurements, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/systems_and_measurements/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
  
 
Figure 14. Simulation - Timing Diagram for DiLogic Write Configuration.  
     The data should be stable on the data bus at the rising edge 
of STRINnD1 and STRINnD2 signals. A reset daisy chain 
must be applied at the end of the operation. 
 
B. DiLogic Analogue Readout 
 
      The DiLogic processor is configured in analogue readout 
mode using function code “1010” and setting ENIN1N and 
ENIN5N pins low as shown in Figure 15.  
 
 
Figure 15. Simulation - Timing Diagram Analogue Readout.  
Successive STRINnD1, and STRINnD2 cycles will cause all 
the daisy chained DiLogic chips to place their digitised data 
on the data bus one at a time, starting with the first module in 
the chain. 
 
C. DiLogic Pattern Readout 
 
      To perform the pattern readout of DiLogic chip Bit-Map 
memory, the operation code must be set to “1000” and both 
ENIN1N, ENIN5N pins must be low. While STRINnD1, and 
STRINnD2 are set to low, the patterns will appear on the data 
bus. The readout sequence will be the same as the analogue 
readout, it will be finished when the last module drives its 
ENOUT_N pin low. A reset daisy chain must be applied to 
turn off the ENOUT_N pins. As in the analogue readout 
mode a pattern delete can be performed. 
 
 
Figure 16. Simulation - Timing Diagram Pattern Readout.  
An enable signal is passed from the ENOUT_N pin to the 
ENIN_N pin of the next chip when the module has finished 
after transferring its Analogue data of one event on the 
C1_DATA_BUS (17:0) data bus pins. The MACK_1N and 
MACK_2N pins indicate the occurrence of the end-event 
word and the end of the analogue readout on that DiLogic 
chip. 
VI. 
TRANSCEIVER MEASUREMENT RESULTS 
 
     The eye and composite jitter diagram measurement results 
for the 3.125 Gbps transceiver links are shown in Figures 17 
and 18. Measurements were taken using Tektronix 6 GHz 
real-time scope. The implementation of a 16-bit PRBS 
generation logic has been applied for validating the physical 
quality of the high-speed transceiver printed-circuit board 
links. The implementation of this PRBS generator is based on 
the linear feedback shift register with the logic XOR and 
logic AND operations that produces a predefined sequence of 
1's and 0's. The measured eye diagram is a common indicator 
of 
the 
quality 
of 
signals 
in 
high-speed 
digital 
transmissions. The test was executed for a duration of 72 
hours using 5 m SMA cables. The measured differential input 
jitter, PRBS pattern at zero crossing is +/- 0.25 UI or 0.5 UI. 
Rise and Fall times of around 100 ps were measured, with a 
peak-peak jitter value of +/- 20 ps for a data rate of 3.125 
Gbps. The measured Bit-error rate (BER) from the illustrated 
Bath-Tub plot is 1 x 10-12, which is also an acceptable 
measurement value according to the digital recommendations 
standards. Gigabit receivers (3.125 Gbps) are AC coupled 
with OCT, and use 8b/10b encoder/decoder, byte ordering, 
and an automatic synchronization state machine. To avoid 
common-mode noise being generated from the 5m non-

7
International Journal on Advances in Systems and Measurements, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/systems_and_measurements/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
identical differential pair cable properties (i.e., unequal 
length, diameters, twisting or material) as a remedy for 
common mode signal, A.C. coupling is used, while for intra-
pair skew, can be adjusted through the transceiver slew-rate 
programmer. AC-coupling allows transceivers to operate 
with large common-mode offsets. 
 
 
 
Figure 17. FPGA Transceiver Eye diagram, for an associated 16-bit               
pseudo-random bit generator pattern. 
     
 
 
Figure 18. FPGA Transceiver composite jitter histogram for the associated 
16-bit pseudo-random bit generator pattern. 
 
 
 
Figure 19. FPGA Transceiver, Bathtub plot showing a 1 x 10-12 BER. 
 
    The 200 Mbps speed LVDS links are DC coupled with   
100-ohm termination resistors connected across the link pairs 
and placed as close as possible to the receiver. Since for 
LVDS we are using DC instead of AC coupling then the 
8b/10b encoding scheme was not required.  The LVDS peak-
peak jitter value of +/- 4 ps for a data rate of 200 Mbps. 
VII. 
READOUT CHAIN AND TIMING VERIFICATION 
     Gassiplex is designed to be connected to wire chambers as 
well as to silicon strip detectors. The 16-channel Gassiplex 
chip is an ungated asynchronous device composed of a charge 
sensitive amplifier (CSA) and signal-conditioning circuitry 
with a track and hold (T/H) input signal used to store the 
charges in sample-hold capacitors. Additionally, a burst of 
clock pulses is required by an external controller to operate 
the multiplexed readout of the stored charges on a single 
output line SWAN-OUT. The clear CLR-SROUT pulse is 
needed to restore the initial state of the sample switches as 
shown in Figure 20. 
  
 
Figure 20. 3-Gassiplex Architecture.  
 
    A T/H signal is generated by the FPGA on column 
controller card as soon as it receives the L0 trigger. When the 
T/H signal is active the Gassiplex reads the amplitude of the 
analogue input waveform and holds the value on the T/H 
buffer capacitor until it is read through a multiplexer. The 
FPGA provides as well a bunch of clock pulses to read the 
analogue value and convert it by ADC. The timing of 
analogue output and clock pulses can be observed from 
Figure 21. Thus, the maximum amplitude in the hit-channel 
or pedestal value is then stored in DiLogic processor 
memory. 
 
  
 
 Figure 21. Readout timing specification for 3Gassiplex chip.  
      
    The verification of the actual timing measurements is 
performed using Quartus Embedded Signal Tap Logic 

8
International Journal on Advances in Systems and Measurements, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/systems_and_measurements/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Analyzer synchronously through a global reference clock of 
156.25 MHz. Various FPGA registers and nodes were 
captured and logged on a workstation terminal through the 
Joint Test Action Group (JTAG) protocol using a 1 MHz 
clock signal. Figure 22 illustrates the DiLogic chip being put 
in the analogue readout mode with function code set to 
“1010” and EnIn_N is low. A burst of Successive StrIn_N 
cycles will cause all five DiLogic modules in the chain to 
place their digitised data on the data bus sequentially one at a 
time. The EnOut_N pin from each DiLogic chip, which is 
connected to the EnIn_N pin of the next chip is activated 
when the module has ended transferring its analogue data of 
one event on the data bus. The Mack_N pin of each DiLogic 
card indicates the occurrence of the end event word on the 
18-bits data bus so to indicate the end of the analogue readout 
on that DiLogic chip, and immediately start the readout of the 
next chip. The 18-bits of the end-event word contains the 
contents of 2 counters: 7-bits from D0 to D6 representing the 
number of channels above threshold and 11-bits from D7 to 
D17 indicate the event numbering. The end-event word is 
indicated in time by the Mack-N output, with the EnOut_N 
of the last chip going low indicating the termination of the 
analogue readout operation. All the 5-DiLogic chips must be 
reset by applying the reset daisy chain code “1101” and an 
extra Strin_N strobe. The timing diagram for the transfer of 
event-data between column controller card and ITS-RU is 
illustrated in Figure 23.  
 
 
Figure 22. Analogue Readout Timing diagram. 
 
    The column controller ACK_Col and rd_req control 
signals remains high during event data transfer between 
column controller card and ITS-RU. The end of data transfer 
is indicated by a pulse being issued on control signal 
rdyXCVR.  
 
VIII. 
COLUMN CONTROLLER FIRMWARE DESIGN AND 
DEVELOPMENT 
System firmware consists of a VHDL (VHSIC Hardware 
Description Language) top-entity FPGA controller module 
per four 5-DiLogic cards, or two columns. Each FPGA 
controller consists of four sub-entities DilCtrlC1D1, 
DilCtrlC1D2, 
DilCtrlC2D1, 
DilCtrlC2D2 
used 
for 
simultaneously controlling all 5-DiLogic cards. Further each 
DilCtrl controller module implements the control logic for 
the Gassiplex chips. The Track/Hold (T/H) signal is used to 
store charges in Gassiplex sampling capacitors using T/H 
switches.  
A burst of clock pulses triggered by the column controller 
FPGA device is then generated to operate the multiplexed 
readout of the stored charges on a single output line.  
 
 
 
Figure 23. Timing diagram for transfer of event-data between column 
controller card and ITS-RU. 
 
The ENCol control signal is set high to initiate reading and 
transfer of event data from the 5-DiLogic cards. Clock 
frequency for each DilCtrl module is set 10 MHz while that 
for transceiver is 156.25 MHz. Therefore, components                      
ColC1D1Ctrl, ColC1D2Ctrl, ColC2D1Ctrl, ColC2D2Ctrl 
implement the synchronization logic and data buffering using 
a 2kB First-In-First-Out (FIFO) data structure between 
DilCtrl and 3.125 Gbps transceivers. Additionally, a                     
200 kbps LVDS link is used to receive L0 CTP command 
word from ITS-RU unit via the Timing, Trigger Control 
system (TTC) and issues a Busy flag for the reduction of the 
overall dataflow.  
The Transceiver module performs serialization and de-
serializes of event data. The Busy flag is issued from the 
arrival of the L0 trigger to the end of the transmission of event 
data. Figure 24 illustrates the complete state-machine for the 
FPGA 
controller 
module. 
Synchronization 
between 
transceiver and FIFO memory is done via flags RDYC1D1, 
RDYC1D2, RDYC2D1, RDYC2D2, and ACKC1D1, 
ACKC2D1, ACKC2D2. A high level on SCN_RDY control 
line indicates the completion of event data transfer to ITS-RU 
from FIFO memory.  
Each event-word contains the selected channel address and 
digitised amplitude information that need to be transferred 
via the FPGA transceivers at a rate of 3.125 Gbps then finally 
to the ITS-RU module for further formatting and transfer to 
DAQ. DilCtrl Controller activates the respective EnCol pin 
to initiate data transfer for various functional modes and write 
to FIFO memory buffer.  
The ACK signals are set to ‘1’ to indicate the on-going 
progress of event-data transfer between FIFO memory and 
transceiver modules. When reading of data from memory is 
complete, then RDYx pin is set high by and ACK low. The 
SCN_RDY control signal indicates that all FIFO memory has 
been sequentially scanned, and therefore, setting back FPGA 
controller to IDLE state, waiting for the next event to be 
transferred to ITS-RU and DAQ.  

9
International Journal on Advances in Systems and Measurements, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/systems_and_measurements/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
IX. PRELIMINARY MEASUREMENT RESULTS  
The busy time of the data collection is mainly defined by 
the CTP waiting time for the completion of the readout 
electronics to transmit event data from FEE to DAQ server. 
The detector busy time due to readout in general depends on 
the event size. 
 
 
 
 
 
Figure 24. State-Machine for Timing diagram FPGA controller Top-entity. 
 
 
Figure 25. System firmware for FPGA column controller. 
With the current firmware and the detector occupancy of 
100% the estimated event size from one column controller is 
3.8 Kbytes. The corresponding busy time is 33 µs as shown 
in Table II, which allows to read data at the trigger rate up to 
30 kHz.  However, the target detector occupancy is 1%, 
leading to at least a two-fold increase in readout rate, of up-
to 70 kHz with enough margin above target requirements.                                           
This measurement result is above the required target for a 
detector occupancy of 1.2 Kbyte Pb-Pb collisions. The 
maximum event readout rate measurement of the prior 
system in Run 2 is estimated to be 5 kHz ten-fold slower than 
this work, to be increased up to 10 kHz just by firmware 
upgrade. The major contribution to such an improvement is 
due to the complete re-design of the new electronics hardware 
architecture leading to the parallel readout of all column 
controllers, including concurrent readout of 5-DiLogic cards 
and use of high speed 3.125 Gbps FPGA transceiver links. 
The location of the proposed new readout electronics 
presented in this work will be in the ALICE detector where 
the measured radiation doses are estimated to be 0.1 kRad 
and 1.9x1010 charged particles/cm2, which puts CPV 
electronics in a safe operating side by 3 to 4 orders of 
magnitude [11]. As described in [12], to detect and protect 
the system against errors caused by SEU in the FPGA 
memory cells, a threefold way is to be adopted: 
 
• 
an efficient error detection scheme based on parity 
check logic; 
• 
8/10 bits of data coding as part of the transceiver low 
level protocols; 
• 
a Cyclic Redundancy Check (CRC) will be 
accompanying data on its way between FEE and 
ITS-RU board.  
 
     The obtained preliminary measurement results shown in 
Table III indicate an event readout time of ~20 µs (50 kHz) 
for a detector occupancy of 55% as expected in Run3. 
Therefore, the newly developed CPV readout electronics 
contributes to a performance improvement in data transfer 
rate between column controllers and DAQ by almost a factor 
of two when compared with the present Scalable Readout 
Unit (SRU) (~21µs), Time Projection Chamber (TPC), 100µs 
for High Momentum Particle Identification (HMPID) readout 
detector electronics as reported in [13], [14], and [15], 
respectively. 
TABLE II.  MEASURED ELAPSED TIME FOR AN EVENT SIZE OF 3.8 KBYTES 
Total Busy 
Time 
Elapsed time 
for Readout of  
5-DiLogic 
Card  
@10MHz 
Elapsed time 
for Readout of  
Column 
Controller to 
ITS-RU 
@156.25MHz 
Detector 
Occupancy 
33µs 
23µs 
10 µs 
100% 
 
TABLE III. READOUT RATE COMPARISON WITH OTHER SIMILAR WORK. 
Detector 
Estimated Readout Rate (µs) 
(this work)  
20 
SRU [13] 
21 
TPC [14] 
33 
HMPID [15] 
100 

10
International Journal on Advances in Systems and Measurements, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/systems_and_measurements/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
         X. NOVELTY CONTRIBUTION AND FUTURE WORK 
 
   This paper presented the design of a new CPV Front-end 
Readout electronics system, which attains the ALICE 
Readout rate goal of 50 kHz. The preliminary prototype 
measurements indicate an estimated event Readout rate of at 
least 50 kHz, as per target value requirements for an 
occupancy of 1%.  
 
            Figure 26. Estimated event readout rate (Hz) for prior System. 
 
     The newly designed upgrade offers significantly 
improved electronics performance. Such an improvement in 
event readout rate when compared with the prior CPV, TPC, 
HMPID and SRU readout detector electronics is mainly due 
to the parallel readout and processing of column controllers 
and the adopted high-speed transceiver link speeds between 
DAQ and readout electronics of around 3.125 Gbps. 
Additionally, the integrated CRC hard Intellectual Property 
(IP) FPGA block, shall detect and correct errors due to SEU, 
thus ensuring a reliable operation of the newly developed 
CPV electronics. A further study to be considered is the 
evaluation of data reliability versus the improvement in 
readout trigger rates. Additionally, further reduction in power 
consumption and area space requirements will be done 
through the integration of DIL-5 and FPGA column 
controller functionality into same ASIC device. 
     After prototype testing and improving in the year of 2018, 
the full set of new Column controllers and Segment boards 
was produced and installed on three CPV modules.  So, the 
detector is now ready to work at 50 kHz trigger rate during 
Run 3. The next step is to replace the old 5-DiLogic cards 
with 700 nm technology with an ASIC chip for a better 
system performance, throughput and maintainability.     
ACKNOWLEDGMENT 
The authors gratefully acknowledge the support of the 
project under the Tertiary Education Scholarship Scheme 
(TESS) and the Malta College of Arts, Science, and 
Technology (MCAST).  
 
REFERENCES 
 [1] C. Seguna, E. Gatt, G. Cataldo De, I. Grech and O. Casha, "A 
New Front-End Readout Electronics for the ALICE Charged-
Particle Veto Detector" The Eleventh International Conference on 
Advances in Circuits, Electronics and Micro-electronics (CENICS 
2018) IARIA, Sep. 2018, pp. 11-15, ISSN 2308-426X, ISBN:978-
1-61208-664-4 
[2] S. Evdokimov et al., “The ALICE CPV Detector”, KEn, vol. 3, 
no. 1, pp. 260–267, Apr. 2018. 
[3] ALICE Collaboration, Technical Design Report of the Photon 
Spectrometer (PHOS). CERN/LHCC, 1999. 
[4] P. Riedler,"Upgrade of the ALICE Detector,"  in 2nd 
International Conference on Technology and Instrumentation in 
Particle 
Physics 
(TIPP), 
pp. 
164-169, 
June 
2011,                         
doi: 10.1016/j.phpro.2012.03.707. 
[5] J. C. Santiard, “The ALICE HMPID on-detector front-end end 
readout electronics,” Nucl. Instrum.Meth. vol. A518, pp. 498-500, 
April 2014. 
[6] F. Carena, “DDL, the ALICE data transmission protocol and its 
evolution from 2 to 6 Gb/s,” JINST, vol. 10, pp. 2-6, April 2015, 
doi: 10.1088/1748-0221/10/04/c04008. 
[7] J. C. Santiard, K. Maret, "The Gassiplex07-2 integrated front-
end analog processor for the HMPID and Dimuon spectrometer of 
ALICE” The Sixth Workshop on Electronics for LHC Experiments 
(CERN 2000), CERN/LHCC, Oct. 2000, pp. 178-182, ISSN 0007-
8328, ISBN 92-9083-172-3 
[8] P. Leitao et al., "Test bench development for the radiation Hard 
GBTX ASIC," JINST, vol. 10, pp. 1-26, January 2015, doi: 
10.1088/1748-0221/10/01/c01038. 
[9] ALICE Collaboration, Radiation Dose and Fluence in ALICE 
after LS2, ALICE-PUBLIC-2018-012, 2018. 
[10] C. Seguna, E. Gatt, G. Cataldo De, I. Grech and O. Casha 
"Proposal for a new ALICE CPV-HMPID front-end electronics 
topology," 
in 
13th 
Conference 
on 
Ph.D. 
Research 
in 
Microelectronics and Electronics (PRIME), June 2017, pp. 173-
176, doi: 10.1109/PRIME.2017.7974135.  
[11] C. Seguna et al., "A New FPGA-Based Controller Card for the 
Optimisation of the Front-End Readout Electronics of Charged-
Particle Veto Detector at ALICE," The Second New Generation of 
Circuits and Systems Conference (NGCAS), November 2018,             
pp. 45-48, doi: 10.1109/NGCAS.2018.8572296. 
[12] H. Witters, J. C. Santiard, Paolo Martinengno, "DILOGIC-2: 
A sparse data scan readout processor for the HMPID detector of 
ALICE," Proc. 6th Workshop on Electronics for LHC Experiments, 
Sep. 2000, pp. 179-183. 
[13] F. Zhang et al., “Point-to-point readout for the ALICE EMCal 
detector,” Nucl. Instrum. And Meth in Phys, January 2014, pp. 157-
162, doi: 10.1016/j.nima.2013.09.023. 
[14] A. Velure, “Upgrades of ALICE TPC Front-End Electronics 
for Long Shutdown 1 and 2," IEEE Transactions on Nuclear 
Science, vol. 62, pp. 1040-1044, June 2015. 
[15] ALICE Collaboration, Performance of the ALICE experiment 
at the CERN LHC, 2014. 
 
 
 
 
 
 
 
 

