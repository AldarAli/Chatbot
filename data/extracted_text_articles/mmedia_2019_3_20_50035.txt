Automatic Analysis and Musicological Interpretation
of Human Free Sorting of Musical Excerpts
Nicolas Dauban,
Christine S´enac,
Julien Pinquier
IRIT
University of Toulouse, UPS
Toulouse, France
nicolas.dauban@irit.fr
christine.senac@irit.fr
julien.pinquier@irit.fr
Pascal Gaillard,
Ludovic Florin
CLLE
University of Toulouse, UT2J
Toulouse, France
pascal.gaillard@univ-tlse2.fr
ludovic.florin@univ-tlse2.fr
Paul Albenge
IREMUS
University of Paris-Sorbonne
Paris, France
paul.albenge@gmail.com
Abstract—Most content-based music recommendation systems are
relying on audio features which do not always match with
musicological criteria. This paper describes the experimental
protocol and the results of a sorting experiment, which leads to
an ‘average categorization’ by volunteers. An automatic analysis
afterward aims at identifying relevant acoustic parameters based
on the obtained categories and sub-categories. A musicological
analysis was also done in parallel.
Keywords–Categorization; Music Information Retrieval; Recom-
mendation.
I.
INTRODUCTION
The role of music recommendation algorithms is to of-
fer new songs to users of online music listening platforms.
Research in Music Recommendation (MR) is very recent
and underdeveloped in the academic world, because of the
limitation -due to licensing issues- of access to the music
signal on a large scale. Because basing a recommendation
on simple metadata from collaborative ﬁlters is not always
relevant, more and more works are based on the expertise
acquired in Music Information Retrieval (MIR), which aims to
extract information from the signal at different scales (notes,
chords, sequence of notes, etc.) in order to characterize for
example an instrument or to calculate descriptors, such as the
tempo or the main melody. See [1] for a state of art on the
MIR.
Thus, some authors have tried to rely on the measure of
similarity between pieces of music [2]. While this approach is
relevant for genre classiﬁcation (the closest task to musical rec-
ommendations), it is quite disappointing for MR. Also, it was
natural to introduce, in parallel to content-based approaches,
information about user preferences [3] [4], or user behavior [5].
However, whatever the method, less known pieces (located in
the ‘long tail’) are never (or rarely) proposed: some works aim
to remedy this problem [6] [7].
One of the main difﬁculties raised by content-based meth-
ods is the selection of parameters. Indeed, among all the
parameters we can extract from an audio signal, which of them
can describe and explain the listeners’ liking? How can we
link these acoustic parameters to musicological and perceptive
criteria? These problems are the major issues of the project
in which this free categorization experience ﬁts. The purpose
of this experiment is to identify both acoustic parameters and
musicological or ‘non-expert’ criteria according to which the
subjects classify the pieces, starting from the assumption that
the tastes of a listener are linked to a ﬁxed combination of
parameters or criteria.
Section II describes the constitution of the corpus and the
experimental conditions. Section III presents the data generated
by the experiment and the way we processed it automatically
and how it was interpreted with a musicological point of view.
Section IV describes a method based on audio features which
aims at reconstruct the categorization made by volunteers.
II.
EXPERIMENTAL PROTOCOL
A. Corpus with musicological criteria
One of the ﬁrst steps of the project was to build a corpus,
which had to meet several requirements: (1) wide range of
musical genres; (2) good quality excerpts: Audio CD (stereo,
16 bits, 44.1 kHz); (3) long enought excerpts (at least 20s)
and in sufﬁcient numbers; (4) preferably with a copyright-free
access database.
The corpus has been built with a musicological approach.
First, we made a set of 15 criteria which can deﬁne the
music in the most comprehensive way possible without using
a commercial classiﬁcation like genres. The concepts and
lexicon used here rely mainly on the work of Pierre Boulez
and Gilles Deleuze in [8] and [9].
•
Recording Quality: perception of the support and
mean of recording (noises, sound spectrum, intensity,
etc.).
•
Prevalence of an Instrument: salience of a special
timbre.
•
Voice: presence or absence of voice, type of voice
(spoken, sung, declarative, repetitive, etc.).
•
Space: feeling and representation of a diffusion space,
deepness of the musical ﬁeld.
•
Memory Work: presence of one or several memorable
elements, repetition of an element (stricte or similar),
clear perception of a pattern or logic.
•
Dynamic: change of quantity/density of events, con-
trast in the musical development.
•
Narrative Development: evolution of musical ele-
ments, presence of different parts relatively distinct.
42
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-697-2
MMEDIA 2019 : The Eleventh International Conference on Advances in Multimedia

•
Smooth/Striated time: according to Boulez [10], pres-
ence or absence of beat, diversity of elements, varia-
tion in quantity of elements in a short moment.
•
Sensorimotor: instrumentalists’ music, mostly ani-
mated by a desire of gesture and a research of the
sensorial effect of the sound. As exemple, African mu-
sic, percussion improvisations, jazz solos, or concrete
pieces of music by Pierre Henry.
•
Representation: what represents by a visible or hidden
way the reality on the plastic plan, by trajectories,
speeds, impacts or event realistic noises (Gregorien,
occidental romanticism, many contemporary music).
•
Rules: any written music, whether written as a clas-
sical counterpoint or transmitted orally as Pygmy
polyphonies or M’Baka horns.
•
Energy: intensity, body implication, involvement of
the musician.
•
Level of Technicity: there are two levels, instrumen-
tal and composition. Perception of the assurance of
musician’s intentions and/or presence of structural
concepts.
•
Cultural Elements: reference to a socio-cultural class.
•
Chronological Situation: perception of elements spe-
ciﬁc to an era, such as type of recording, type of play,
reference to a particular aesthetic.
For each of these criteria, we empirically selected three sig-
niﬁcant pieces which contain different musical characteristics
in order to propose an eclectic ensemble. Although these would
have been selected to initially correspond to a speciﬁc criterion,
it is possible to ﬁnd characteristics of other criteria. The goal
here is not to recover this classiﬁcation in the experimental
results but to see which criteria were particularly relevant in the
volunteer’s sorting. For each selected track, we had to select
a short excerpt in order to keep the experiment from being
too long for the volunteers. Excerpts had to be still relevant
regarding to the corresponding criterion. In the end, a corpus
of 45 excerpts of 20 seconds was deﬁned (see Figure 1).
B. Experimental conditions
In order to limit the impact of the age of the participants
on the results, we used participants/volunteers from 20 to 25
years old (30 in number). For the experiment, we used the
TCL-labX tool [11] [12].
The interface (see Figure 2) was presented in an identical
way to all volunteers who were asked to freely sort excerpts
and thus form as many categories as they wished, based on
the similarities between the pieces. To do so, the users could
listen as many times as necessary excerpts and could move
and group them freely on the interface.
III.
FREE SORTING AND INTERPRETATION
A. MetaData
For each volunteer, the program generates a ﬁle in which
is indicated the distribution of the excerpts in the different
classes. The software also generates a ”cookie” ﬁle containing
the history of the operations performed by the user: moving
icons and listening to excerpts. We can replay all actions
performed by the volunteer. In addition, the software carries
out an automatic analysis of these ﬁles in order to extract
several statistics on the participants. The average duration
Figure 1. List of 20 seconds excerpts and their beginning time.
Figure 2. Interface presented at the beginning of the experiment (each
excerpt is represented by a numbered icon), and after the free sorting
(excerpts are grouped by volunteer).
of the experiment was 37 minutes, the maximum duration
exceeded one hour (1h 2min) and the minimum duration was
15 minutes. The standard deviation over the duration of the
experiment is 10 minutes. On average, participants formed 15.5
classes, the minimum being 8 classes and the maximum 20.
The standard deviation on the number of classes is 3.2.
B. Automatic Results Analysis
1) Matrices of co-occurrence and dissimilarity: to accom-
plish this task, we based ourselves on the work of [13]. First,
we built a co-occurrence matrix Ci for each participant i. A
co-occurence matrix is square and symmetrical, of dimension
N × N with N equal to the number of sorted excerpts. In
each cell, we indicate the distance between two excerpts: if
these two excerpts are in the same category, the distance is
considered as zero, otherwise we assign a unit distance.
Then, we calculate an average co-occurrence matrix of all
participants, called the D dissimilarity matrix. This dissimi-
larity matrix gives us a distance measurement for each pair of
43
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-697-2
MMEDIA 2019 : The Eleventh International Conference on Advances in Multimedia

musical excerpts, this distance being based on the classiﬁcation
by the n participants.
We have also computed a matrix of variance Mvar from
the matrices of co-occurrence Ci and the dissimilarity matrix
D (see equation 1). For a pair of excerpts, this variance is null
if the set of n participants sorted these two excerpts identically.
Conversely, this variance is maximal (varmax = 1) if half of
the participants put these excerpts together, and the other half
separately.
M j,k
var = 1
n
n
X
i=1
(Ci,j,k − Dj,k)2
(1)
with M j,k
var the cell of the line j and column k of the matrix
Mvar, Ci,j,k the cell (j, k) of the matrix Ci and Dj,k the cell
(j, k) of the matrix D.
Since the Ci matrices contain only binary values, for a
pair of excerpts, a null variance necessarily corresponds to a
dissimilarity of 1 or 0, and a unit variance necessarily corre-
sponds to a dissimilarity of 0.5. The closer the dissimilarity is
to an extreme value (1 or 0), the more these two excerpts will
be ‘unanimously’ put in their class by volunteers. The closer
the dissimilarity is to 0.5, the more these two extracts will
have ‘divided the opinion’ of the volunteers.
2) Dendrogram: from the dissimilarity matrix, we were
able to perform an ascending hierarchical classiﬁcation. We
have chosen to use the Ward’s method [14], which consists
in grouping the classes so that the increase of the inertia
interclasses, given by (2) is maximized. This, according to
the Huygens theorem, is equivalent to minimize the increase
of the intraclass inertia (see (3)) [15].
Ie = 1
n
k
X
i=1
ni × d2(gi, g)
(2)
Ia = 1
n
k
X
i=1
ni
X
j=1
d2(ej, gi)
(3)
We obtained the dendrogram of Figure 3. It tells us about
the links between excerpts put by users. The vertical axis
represents the distance between excerpts or groups of excerpts.
Thus, two excerpts that have been very often placed together
by the volunteers have a low link on the ﬁgure, such as on the
numbers 11 and 17 or the numbers 15 and 37.
C. Musicological Interpretation
The dendrogram obtained previously was interpreted from
a musicological point of view in order to understand how the
volunteers had made their classiﬁcation. The dendrogram has
been annotated with the criteria common to excerpts belonging
to the same branch, see Figure 3.
The name indicated under each node corresponds to a
presumed musicological criterion common to all the excerpts
which are below it. This name was choosen analysing the con-
tent of each category. The four main categories are described
as follows.
1) Audio-Tactile: Pieces of music in this category are all
very rhythmic and belong to the genre jazz or funk and more
generally to Africo-American. ”Audio-tactility” refers to a
particular relationship with the body. Within this category,
the excerpts have been distinguished by the predominant
instrument.
Figure 3. Dendrogram annotated by musicologists. y axis corresponds to the
distance between two excerpts or clusters of excerpts; excerpts to the x axis.
2) Art Music: this category is a hybrid grouping of aca-
demic Western Music, of music evoking a sacred or spiritual
aspect and ﬁnally of the excerpts where the voice is predom-
inant.
3) Non-conventional: this category includes music built
outside the conventional rules governing Western music, partic-
ularly based on melody and harmony. Therefore, we ﬁnd music
without pitch of precise notes and the non-Western music. Any
music that does not follow the hierarchies present in Western
codes is necessarily perceived as a group apart.
4) Electronic/Energetic: most of the excerpts in this cate-
gory were produced from electronic instruments. This category
also includes pieces with contrasts in terms of energy. How-
ever, the distance remains particularly high between these two
sub-categories.
The objective was not to recover, via the results of the
experiment, the free classiﬁcation created but to see which
dimensions are the most important while listening to music.
We can notice that the musicological criteria used to establish
the corpus are not found through the free classiﬁcation of non-
expert participants. This shows that there are different types
of musical analysis and valuation. The musicological criteria
allowed us to obtain a very varied corpus and the categories
identiﬁed by the participants reveal other more accessible
criteria for non-experts.
IV.
TOWARDS AN AUTOMATIC CLASSIFICATION
The aim here was to verify whether an automatic classiﬁ-
cation of musical extracts based on acoustic parameters could
approach the free classiﬁcation made by humans and based not
only on the signal, but also on knowledge. As music (in term of
production) commonly refers to a series of sound events (notes,
percussive sounds, voiced or unvoiced sounds) deﬁned by their
rythm, timbre, dynamics, and pitch, we have extracted some
of these parameters. So, we calculated 31 audio parameters
on each excerpt of the corpus using the MIR Toolbox [16].
The extraction of these parameters is described in detail in the
following subsection.
A. Extraction of Acoustic Parameters
1) Rhythm: it describes the temporal location of sound
events and their duration. Generaly, in conventional western
music, a regular pulse determines the beat, a measure being
composed of several beats. In a score, the rhythm (inside the
beat) is described by the different shapes of notes (crotchet,
quaver, semiquaver, etc.) and of silences (pause, minim, etc.)
as well as by the time signature.
44
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-697-2
MMEDIA 2019 : The Eleventh International Conference on Advances in Multimedia

•
Event Detection and Density: all the rhythmic parame-
ters are based initially on the temporal location of each
event. For this, we use a peak detection algorithm on
the signal envelope. Once the peaks are detected, we
can then calculate the number of events per second.
These features are extracted on 10 seconds window
without overlapping.
•
Tempo: the tempo calculation, which is based on a
detection of the periodicity of events, selects the high-
est peak. Periodicity detection is performed using the
autocorrelation function [16]. This feature is extracted
on a 3 second window with and overlap of 0.3 seconds.
•
Pulse Clarity: the pulse clarity can be calculated ac-
cording to the method detailed in [17]. This parameter
describes how much the beat is dominant in the
rhythm, or in other words, how much emphasis is
placed on the beats: for example, the clarity of the
beat is strong for disco rhythms, and is often low for
complex rhythms, like those of jazz. This feature is
extracted on a 5 second window with and overlap of
0.5 seconds.
2) Timbre: it describes the spectral composition of a note,
that is to say the amplitude of the harmonics and the variation
in time of these harmonics. This distinguishes, for example,
two notes played at the same pitch by a piano and a guitar.
These features (excepted the attack) are extracted on a 50ms
window with and overlap of 25ms.
•
Attack: the attack of a note describes the variation
of amplitude at the moment when it is played. It
is measured by its duration, its amplitude or by its
slope [18]. For example, struck strings of the piano
have a stronger attack than violin strings played with
a bow. When a note is detected, the beginning and
the end of the attack are marked, then the difference
of amplitude or the duration between the two points
is calculated (the slope is obtained using these two
informations).
•
Zero Crossing Rate: it is calculated on the original
signal by multiplying all successive pairs of samples,
and iterating a variable when the product is negative
(signal change). This variable is then divided by the
duration to obtain the rate [19].
•
Rolloff Frequency: it informs us about the amount of
energy present in the low frequencies. On a spectrum,
we calculate the frequency below which 85% of the
energy is contained. The lower the frequency, the more
energy is concentrated in the low frequencies.
•
Brightness: it informs us about the amount of energy
present in the high frequencies [20]. On a spectrum,
we calculate the amount of energy present beyond a
ﬁxed frequency (usually 1500 Hz).
•
Statistical Parameters of Spectral Distribution: it is
possible to calculate statistics as well as moments of
different orders on the spectrum, such as Centroid,
Spread, Skewness, Kurtosis, Flatness, as well as the
Entropy.
•
MFCC
(Mel
Frequency
Cepstral
Coefﬁcients):
MFCCs are cepstral coefﬁcients calculated by a
discrete
cosine
transform
applied
to
the
power
spectrum of a signal [21]. The different frequency
bands are determined according to the perceptive
logarithmic scale Mel, which is modeled on the
human hearing system.
•
Roughness: it describes the phenomenon of audible
beat in the presence of two near frequencies [22].
Two notes spaced half tone apart (or less) will gen-
erate strong roughness, which decreases as spacing
increases. Roughness is almost zero from 5 half tones.
•
Irregularity of a Spectrum: this is the degree of am-
plitude variation of two successive peaks (harmonics
or not) of the spectrum [16].
3) Dynamics: it describes the relative amplitude of differ-
ent sounds, which results in shades of intensity. On a score, the
dynamics is indicated by terms, such as ’pianissimo’ or ’forte’
that tell the musician to play relatively more or less loudly. In
signal processing and generally, dynamics describes the range
of variation of the different values taken by a signal. In music,
dynamics describes the ratio of sounds of strong and weak
amplitudes. These features are extracted on a 50ms window
with and overlap of 25ms.
•
RMS (Root Mean Square) level: the effective value of
an ergodic random signal over a time interval is the
square root of the square signal mean, or the square
root of its mean power. In practice, for a discrete time
signal, the RMS level is calculated on a ﬁnite number
of samples.
•
Low Energy Rate: it is the number of points whose
value is less than the RMS value of the signal. For a
signal with peaks at the high RMS level, this rate will
be high whereas for a signal at the RMS level rather
constant, this rate will be low.
4) Pitch: it describes the fundamental frequency of a sound
played by an instrument, which deﬁnes the note.
•
Note detection: the default method for detecting notes
is to decompose the signal into several frequency
bands, then calculate the auto-correlation and ﬁnally
detect the peaks in order to obtain an estimate of the
notes. This feature is extracted on a 46.4ms window
with and overlap of 10ms.
•
Harmonies Detection: from the detection of notes, it
is then possible to detect harmonies, that is to say
combinations of different notes. It is also possible to
calculate the key of an extract, as well as the temporal
evolution of all these parameters. This feature is
extracted on a 743ms window with and overlap of
74ms.
B. Selection of Acoustic Parameters
We averaged each parameter to obtain a matrix of the form
N ×P with N = 45 (excerpts) and P = 31 (parameters). Note
that by keeping only the mean, we lose the temporal evolution,
but this allows us to have only one scalar per excerpt and per
parameter. For each parameter, we computed the distance for
each pair of excerpts and thus form a dissimilarity matrix P i
for each parameter i. Then, we established a model of the
matrix of dissimilarity of the human free sorting from a linear
combination of the matrices of the parameters.
Mmodel =
31
X
i=1
aiP i
(4)
45
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-697-2
MMEDIA 2019 : The Eleventh International Conference on Advances in Multimedia

The values contained in each dissimilarity matrix were
normalized between 0 and 1 in order to remain consistent with
the matrix values of the free sorting.
Rather than using all P i matrices, we selected the most
relevant matrices by computing the correlation coefﬁcient of
each matrix of parameters with the matrix of the volunteers
and we selected the m more correlated. Indeed, the matrices
the most correlated with the matrix of dissimilarity established
during the free sorting are by deﬁnition the most ’similar’.
C. Regression
1) Complete matrix: with these ﬁrst m matrices, we used a
gradient descent algorithm to ﬁnd the best linear combination,
the criterion to be optimized being the quadratic error between
this linear combination and the dissimilarity matrix of the
free sorting. This algorithm therefore returns the coefﬁcients
ai by which the matrices of dissimilarity are multiplied in
order to obtain the matrix most resembling the dissimilarity
matrix formed by the set of results of the volunteers. These
coefﬁcients inform us of the importance of each parameter: if
a coefﬁcient is low then it is not inﬂuential for the volunteers
to sort the pieces, and vice versa.
To simplify the calculations, the dissimilarity matrices have
been transformed into Vp and Vd vectors of length L = 45 ×
45 = 2025.
For m used dissimilarity matrices of parameters, the
quadratic error equation is deﬁned by:
J =
L
X
j=1
" m
X
i=1
aiV i,j
p
!
− V j
d
#2
(5)
The gradient of this error is:
−−→
grad J =


∂J
∂a1
...
∂J
∂ak
...
∂J
∂aM


(6)
where:
∂J
∂ak
= PL
j=1
∂[(
Pm
i=1 aiV i,j
p )−V j
d ]
2
∂ak
= 2 PL
j=1

6) Electronic/Energetic: The method has been the most
efﬁcient for this group (see Figure 5). Except for the ﬁrst
excerpt, the dendrogram was well reconstituted. For each sub-
group i of size Ni, the initial indices of the excerpts were
replaced by indices ranging from 1 to Ni. If the dendrogram
of a sub-group has been reconstituted, the excerpts are placed
in ascending order. We obtained a total squared error of 2.8.
We used the following 9 parameters: Attack, MFCC3, MFCC8,
MFCC11, Rolloff, Brightness, MFCC4, Zero Crossing Rate,
MFCC0 (i.e. Energy).
Overall, the results are quite satisfactory because we were
able to reconstruct the dendrogram of each category with a
limited number of errors.
D. Classiﬁcation of new excerpts
The objective of this part was to ﬁnd a method to assign to a
‘new’ excerpt the right category. In all the methods that follow,
we successively considered each excerpt as a new individual,
taking care to remove it from the learning base (leave-one-
out cross-validation). The score for each method is therefore
between 0 and 45 (where all the excerpts were assigned to the
right categories). In addition, the parameters were centered
and reduced in order to eliminate the inﬂuence of the unit of
measure used for each of them.
The ﬁrst method consisted in calculating the center of
gravity of each category according to the 31 parameters, and
then assigning the new extract to the class with its nearest
barycenter: we thus obtained 30 correct assignments (67%).
In the second method, we retained a small number of
parameters: we observed which parameters were relevant for
the classiﬁcation in the sub-categories (section IV-C2) and
we kept only those which were the most correlated in the 4
classiﬁcations. They are: Change Harmony Detection, Attack,
Spectrum Entropy, Rolloff, MFCC0, and Irregularity. We thus
obtained 22 correct assignments (33%): the selected parame-
ters were therefore not particularly relevant.
Figure 6. Attribution score based on the number of parameters used.
y axis corresponds to the score obtained in function of the number of
parameters (x axis).
In the third method, we used the N most correlated
parameters in ranking the same way as in the section IV-C1.
In Figure 6, we see that the score increases globally with
the number of parameters but sometimes decreases when we
use a new one. The maximum score (71%) is reached for
25 parameters : all but 3rd, 4th and 6th MFCC, the Spectral
Flatness, Inharmonicity and Spectrum Kurtosis. This method
proved to be the best.
V.
CONCLUSION AND PROSPECTS
For this experiment, a corpus was built according to a wide
range of musicological criteria. Different audio parameters
were also computed on the excerpts of the corpus.
Volunteers have performed a free sorting task on this
corpus. Analysis of the experimental results let us establish an
average human classiﬁcation of excerpts by volunteers, which
has been represented in the form of a dendrogram in which
appear four main groups with sub-groups. We noticed that
these sub-groups were built according to some of the musi-
cological criteria but also according to ‘non expert’ criteria
such as genres.
In order to automatically reconstruct this human classi-
ﬁcation, we have established a hierarchy in the parameters
relevance depending on their correlation with the volunteers’
classiﬁcation. We saw that this automatic reconstruction is
more efﬁcient to distinguish sub-groups within a group instead
of groups between them. Finally, the identiﬁed parameters can
be selected for an application in music recommendation.
REFERENCES
[1]
M. Schedl, E. G´omez, and J. Urbano, “Music information retrieval:
Recent developments and applications,” Foundations and Trends R⃝ in
Information Retrieval, vol. 8, no. 2-3, 2014, pp. 127–261.
[2]
B. McFee, L. Barrington, and G. Lanckriet, “Learning content similarity
for music recommendation,” IEEE transactions on audio, speech, and
language processing, vol. 20, no. 8, 2012, pp. 2207–2218.
[3]
A. Van den Oord, S. Dieleman, and B. Schrauwen, “Deep content-based
music recommendation,” in Advances in neural information processing
systems, 2013, pp. 2643–2651.
[4]
E. J. Humphrey, J. P. Bello, and Y. LeCun, “Moving beyond feature
design: Deep architectures and automatic feature learning in music
informatics.” in ISMIR.
Citeseer, 2012, pp. 403–408.
[5]
M. Schedl and D. Hauger, “Tailoring music recommendations to users
by considering diversity, mainstreaminess, and novelty,” in Proceedings
of the 38th International ACM SIGIR Conference on Research and
Development in Information Retrieval.
ACM, 2015, pp. 947–950.
[6]
`O. Celma Herrada, “Music recommendation and discovery in the long
tail,” Ph.D. dissertation, 2009.
[7]
M. A. Domingues, F. Gouyon, A. M. Jorge, J. P. Leal, J. Vinagre,
L. Lemos, and M. Sordo, “Combining usage and content in an online
recommendation system for music in the long tail,” International Journal
of Multimedia Information Retrieval, vol. 2, no. 1, 2013, pp. 3–13.
[8]
R. Bogue, Deleuze’s way: Essays in transverse ethics and aesthetics.
Routledge, 2016.
[9]
G. Deleuze and F. Guattari, A thousand plateaus: Capitalism and
schizophrenia.
Continuum, 2004.
[10]
P. Boulez, Boulez on music today (trans. by Bradshaw, Susan and
Rodney Bennett, Richard).
London: Faber and Faber, 1971.
[11]
P. Gaillard, “Laissez-nous trier ! tcl-labx et les tˆaches de cat´egorisation
libre de sons.” Le sentir et le dire : Concepts et m´ethodes en psychologie
et linguistique cognitive, 2009, pp. 189–210.
[12]
http://petra.univ-tlse2.fr/tcl-labx, [Online; accessed 15-January-2019].
[13]
J. P. Barth´el´emy and A. Gu´enoche, Trees and Proximity Representa-
tions.
John Wiley and Sons, 1991.
[14]
J. H. Ward Jr, “Hierarchical grouping to optimize an objective function,”
Journal of the American statistical association, vol. 58, no. 301, 1963,
pp. 236–244.
[15]
G. Saporta, Probabilit´es, analyse des donn´ees et statistique.
Editions
Technip, 2006.
[16]
O. Lartillot, “Mirtoolbox 1.6.1 users manual,” 2014.
[17]
O. Lartillot, T. Eerola, P. Toiviainen, and J. Fornari, “Multi-feature
modeling of pulse clarity: Design, validation and optimization.” in
ISMIR, 2008, pp. 521–526.
[18]
J. M. Grey, An Exploration of Musical Timbre Using Computer-based
Techniques.
Department of Psychology, Stanford University., 1975.
[19]
F. Gouyon, F. Pachet, and O. Delerue, “On the use of zero-crossing rate
for an application of classiﬁcation of percussive sounds,” in Proceedings
of the COST G-6 conference on Digital Audio Effects (DAFX-00),
Verona, Italy, 2000.
47
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-697-2
MMEDIA 2019 : The Eleventh International Conference on Advances in Multimedia

[20]
P. Laukka, P. Juslin, and R. Bresin, “A dimensional approach to vocal
expression of emotion,” Cognition & Emotion, vol. 19, no. 5, 2005, pp.
633–653.
[21]
B. Logan, “Mel frequency cepstral coefﬁcients for music modeling.” in
ISMIR, vol. 270, 2000, pp. 1–11.
[22]
W. A. Sethares, Tuning, timbre, spectrum, scale.
Springer Science &
Business Media, 2005.
48
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-697-2
MMEDIA 2019 : The Eleventh International Conference on Advances in Multimedia

