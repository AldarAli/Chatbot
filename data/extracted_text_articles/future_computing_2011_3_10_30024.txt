The Hopfield-type Memory Without Catastrophic Forgetting 
 
Iakov Karandashev 
Boris Kryzhanovsky 
Leonid Litinskii 
Yakov.Karandashev@phystech.edu   
kryzhanov@mail.ru   
litin@mail.ru  
Center of Optical Neural Technologies of Scientific Research Institute for System Analysis  
Russian Academy of Science 
Moscow, Russia
Earlier some modifications of the Hebb matrix were 
proposed to eliminate the memory destruction [3]-[7]. As the 
result of such modifications an unlimited number of random 
patterns can be fearlessly written down into matrix elements 
one by one. However, the memory of the network is 
restricted. If as previously the maximum number of 
recognized patterns is denoted by
Abstract—We analyzed a Hopfield-like model of artificial 
memory that reproduces some features of the human memory. 
They are: a) the ability to absorb new information when 
working; b) the memorized patterns are only a small part of a 
set of patterns that are written down in connection matrix; c) 
the more the pattern was shown during the learning process, 
the better the quality of its recognition. We used the Hebb rule, 
but each pattern was supplied with its own weight. The weight 
is proportional to the frequency of the pattern showing during 
the learning of the network. As a result unlimited number of 
patterns can be written down in the connection matrix, and 
that would not lead to the memory destroying (as it has place 
in the standard Hopfield model). However, only the patterns 
that were shown rather frequently would be recognized: their 
weights have to be larger a critical value. For analyzed 
variants of the weights distribution the storage capacity was 
estimated as ~0.05N-0.06N, where N is the dimensionality of 
the problem. 
Mc
, for the models 
discussed in [3]-[7] 
0.05
Mc
N
≈
⋅
. All these models have 
the same weak point: only patterns that are the last written 
down in the connection matrix constitute the memory of the 
network. Let us explain the last statement. Let us number the 
patterns in the course of their appearance during the learning 
process: the later the pattern was shown to the network, the 
larger its number. Then it turns out that the real network 
memory is formed of patterns whose numbers belong to the 
interval 
[
c ,
]
M
M
M
μ ∈
−
. Patterns with order numbers less 
than 
c
M
− M
 are excluded from the memory irretrievably. 
That is the common property of the models [3]-[7].  
Keywords - Associative memory; catastrophic forgetting; 
quasi-Hebbian matrix.  
In our work, we succeeded in eliminating of the 
catastrophic forgetting of the Hopfield model. In our 
approach every pattern is supplied by an individual weight. 
Then in place of the Hebbian matrix we obtain a quasi-
Hebbian connection of the form  
I. 
 INTRODUCTION 
In the standard Hopfield model one uses the Hebb matrix 
constructed with the aid of M  random patterns [1], [2]. For 
definiteness we suppose that N-dimensional vector-row 
1
(1
)
M
ij
ij
i
j
J
r x x
μ
μ
μ
μ
δ
=
=
−
∑
. 
 
(1) 
1
2
(
,
,...,
N )
x
x
x
μ
μ
μ
μ
x =
 with binary coordinates ixμ = ±1
 is the 
μ -th pattern, the number of patterns is equal to M , and the 
Hebb connection matrix has the form 
Here the weights rμ  are positive and put in decreasing 
order: 
. It is natural to treat the weight r
1
2
...
...
0
r
≥ r
≥
≥
≥
μ  
as the frequency of the 
1
(1
)
M
ij
ij
i
j
J
x x
μ
μ
μ
δ
=
=
−
∑
μ -th pattern showing during the 
learning process.  
. 
The estimate for number of random patterns that can be 
memorized in the Hopfield model is well known: 
In previous papers [8], [9] with the aid of statistical 
physics methods the main equation for the Hopfield model 
with the quasi-Hebbian matrix was obtained. This equation 
generalizes the classical equation of the standard Hopfield 
model [1]. For a special distribution of the weights we 
succeeded in solution of the main equation: the case when 
only one coefficient differed from all other that were 
identically equal: 
0.138
Mc
N
≈
⋅
. If the number of patterns written down into 
connection matrix is larger than
c
M , the catastrophe takes 
place: the network ceases to recognize patterns at all.  
This catastrophic forgetting is a troublesome defect of the 
Hopfield model. Indeed, let us imagine a robot whose 
memory is based on the Hopfield model. It is natural to think 
that his memory is steadily filled up. When the robot sees a 
new image, it is written additionally to its memory. 
Catastrophic forgetting means that when the number of 
memorized patterns exceeds 
1r
=τ
2
3
...
1
M
r
r
r
=
=
=
=
, 
 was examined. 
Theoretical results were confirmed by computer simulations. 
(In what follows we use the notations introduced in [8], [9].) 
In this paper, we present the results of solving of the 
main equation in the general case. The main obtained result 
is as follows. For every weights distribution {
Mc
, the memory will be 
completely destroyed. Everything that was accumulated in 
the memory would be forgotten. This behavior is contrary to 
common sense. 
rμ }
 there is 
such a critical value 
 that only patterns whose weights are 
greater than 
 will be recognized by the network. Other 
cr
cr
57
FUTURE COMPUTING 2011 : The Third International Conference on Future Computational Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-154-0

patterns are not recognized. Now we know only the 
algorithm of calculation of the critical value
c . It is not 
clear, if it is possible to obtain an analytical expression for 
.  
r
cr
The case of the weights, which are the terms of a 
decreasing geometric series r
qμ
μ =
 (
) we discuss in 
details. For this weights distribution the number of the 
recognized patterns is ~ 0
. The results are confirmed 
by computer simulations. 
1
q <
.05 N
⋅
Note, for the first time the quasi-Hebbian connection 
matrix (1) was discussed many years ago. For this matrix the 
implicit form of the main equation (2) was obtained in [6]. 
However, the authors of [6] examined the case of the 
standard Hopfield model only (
). Our contribution is 
the solution of the main equation in the general form. 
1
rμ ≡
II. 
SOLUTION OF MAIN EQUATION 
The main equation for the k -th pattern, which we 
obtained in [8], [9], has the form: 
2
2
1
1
M
k
k
r
M
r
r
μ
μ
μ
γ
α
ϕ
≠
⎛
⎞
=
⎜⎜
−
−
⎝
⎠
∑
⎟⎟ . 
 
(2) 
It is supposed that M and
are very large:
,
N
,
M N >>1
α  is 
the load parameter: 
/
α = M N
,  
( )
y
γ
= γ
 and 
( )
y
ϕ
= ϕ
 are 
functions of an auxiliary argument 
0 : 
y ≥
2
2
( )
y
y
e
γ
π
−
=
,  
2
( )
( )
2
y
erf y
y
e
y
π
ϕ
=
. 
The function 
( )
γ y
 decreases monotonically, and 
( )
ϕ y
 
increases monotonically from the minimal value
(0)
1
ϕ
= . 
For what follows it is important that ( )
1
ϕ y
≥ . When all the 
weights are equal to each other, equation (2) turns into the 
equation for the standard Hopfield model [1], [2], [6].  
Let us transform Eq.(2) dividing the left hand and right 
hand sides by M . Moreover we tend the upper limit of the 
sum in r.h.s. of Eq. (2) to infinity. In fact, we pass to the 
model with infinitely number of patterns. The main equation 
takes form: 
( )( )
k
k
N
fμ
μ
∞
≠
= ∑
y , 
 
(3) 
where 
( )
fμ k
 are the functions of γ , ϕ  and rμ : 
2
( )
( )
( )
( )
(
)
k
k
k
t
f
y
t
μ
μ
μ
γ ϕ
⎛
⎞
= ⎜
⎟
⎜
⎟
−
⎝
⎠
, 
( )
k
k
r
t
r
μ
μ
=
,
μ ≠ k
.  
(4) 
Eq. (3) connects the dimensionality
 of the problem with 
number k  of the pattern. When we find the solution 
 of 
this equation, we calculate the overlap of the 
-th pattern 
with the nearest fixed point: 
N
0y
k
(
0 )
mk
erf
y
=
. 
 
 
(5) 
When 
, in the vicinity of the k -th pattern there is 
a fixed point of the network. This situation is interpreted as 
recognition of the k -th pattern by the network. If 
0
k
m ≈
, 
the 
-th pattern is not recognized by the network. 
k
( )
The values tμk
 are arranged in decreasing order. For 
what follows it is important that the first 
1
k −  of these 
values are larger than 1, and the other ones are less than 1: 
t( )
t( )
( )
( )
( )
1
2
1
1
2
...
1
....
k
k
k
k
k
k
k
k
t
t
t
−
+
+
>
>
>
> >
>
>
 (6) 
The r.h.s. of Eq. (3) is the result of summarizing over the 
set of functions 
( )( )
f k
y
μ
 (4).  It is easy to see that when 
 the denominator
 of any function 
y → ∞
( )
(
γ ϕ − tμk
( )( )
f k
y
μ
)
 
tends to 0. In other words, at the infinity each function 
( )( )
f k
y
μ
 increases unrestrictedly. As a result, at the infinity 
the r.h.s. of Eq.(3) increases unrestrictedly too. 
( )( )
f k
y
The behavior of the function μ
 for finite values of 
argument depends on the constant 
( )
k
tμ  in its denominator. If 
( )
1
tμk
<
( )( )
f k
y
, the function μ
 is everywhere continuous and 
limited. If 
, the function 
( )
1
tμk
>
( )( )
f k
y
μ
 has a singular 
point. In this case the denominator of the function 
( )( )
f k
y
μ
 
is equal to zero for some value 
( )
k
yμ of its argument:  
(
)
(
( )
( )
( )
1
( )
k
k
k
k
y
t
y
t
μ
μ
μ
μ
ϕ
ϕ −
=
⇔
=
) , 
where ϕ −1
 is the inverse function with regard to ϕ . We see 
that for every 
 the function 
( )
1
tμk
>
( )( )
f k
y
μ
 has the 
discontinuity of the second kind in the point 
( )
k
yμ . Since in 
the series (6) the first 
k −1
( )
 values of tμk
 are greater than 1, 
it is easy to understand that the r.h.s. of Eq.(3) has the 
discontinuities of the second kind in 
k −1
 points 
. At the infinity the r.h.s. of Eq.(3) 
increases unrestrictedly.  
( )
( )
( )
1
2
1
...
k
k
k
k
y
y
y
−
−
<
<
<
k
For simplicity let us go in Eq.(3) to inverse quantities: 
1
( )
( )
k ( )
k
k
F
y
f
y
μ
μ
−
∞
≠
⎛
= ⎜
⎝
⎠
∑
⎞
⎟
1
kF ( )
y
N =
, where 
. 
(7) 
kF ( )
y
It is evident that nonnegative function 
 in the r.h.s. of 
Eq.(7) is equal to zero in the points 
( )  < 
1
k
ky −
( )
2
k
ky − <…< 
. 
At the infinity 
( )
1
k
y
kF ( )
y  tends to zero. The typical behavior of 
the function 
kF ( )
y is shown in Fig.1. To the right of the 
rightmost zero 
, where the inequality 
 holds, 
the function 
( )
1
y k
( )
1
( )
k
y
t
ϕ
>
kF ( )
y at first increases, and then after reaching 
its maximum the function 
kF ( )
y  decreases monotonically.  
Let 
 be the coordinate of the rightmost maximum of the 
function 
( )
k
cy
kF ( )
y
kF ( )
y
. The value of 
 in the point 
determines the critical characteristics related to the 
recognition of the k -th pattern. Let us explain what it 
means.  
( )
k
cy
1
k
m ≈
58
FUTURE COMPUTING 2011 : The Third International Conference on Future Computational Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-154-0

Generally speaking, Eq.(7) has several solutions. Their 
number is equal to the number of intersections of the 
function 
kF ( )
y  with the straight line that is parallel to 
abscissa axis at the height 1 N   (see Fig. 1). These solutions 
correspond to stationary points of the solution of the saddle-
point equation [1], [2]. However, only one of these 
intersections is important. Its coordinate is to the right of the 
rightmost maximum
. This intersection corresponds to 
the minimum of the solution of the saddle-point equation. 
Other solutions of Eq.(7) can be omitted. 
( )
k
cy
As example in Fig.1 the behavior of the r.h.s. of Eq.(7) 
for the pattern number 5 (
) is shown for the weights 
that are the terms of harmonic series 
5
k =
1/
rμ
μ
=
. Four points 
<
<
<
 are zeros of the function 
4y(5)
3y(5)
2y(5)
1y(5)
5( )
F y . For 
 that are greater than 
 (
) the function 
y
1y(5)
(5)
1
y
> y
5( )
F y  at 
first increases up to its local maximum in the point 
 and 
then decreases monotonically. The dashed line that is 
parallel to the abscissa axis is drawn at the height 0.001. 
When the l.h.s. of Eq.(7) is equal to 0.001, we obtain 
. In other words, for this quasi-Hebbian matrix of 
the dimensionality 
 in the vicinity of the 5-th 
pattern there is a fixed point certainly. Since the solution of 
Eq.(7) is large enough,
, the overlap (5) of the 
pattern and the fixed point is close to 1.  
(5)
cy
1000
N =
1000
N =
0
3.5
y ≈
Let us little by little decrease the dimensionality 
. The 
dashed straight line will go up, and the solution 
 of 
Eq.(7) will shift in the region of smaller values. This will go 
on till 
 coincides with the critical value 
. Just this 
defines the minimal dimensionality 
 for which the 
fixed point in the vicinity of the 5-th pattern still exists. 
Since for 
 equation (7) has no solutions in the 
region 
, there is no fixed points in the vicinity of the 
5-th pattern. From the point of view of the neural network 
memory this means that when 
 the 5-th pattern is 
not recognized by the network. 
N
0(
)
y
N
0y
(5)
cy
min
N
min
N
N
<
(5)
1
y
y
>
min
N
N
<
Up to now we fixed the number of the pattern k  and 
decreased the dimensionality
. However, it is reasonable 
to fix the dimensionality N  and increase 
 little by little. 
We seek its maximal value for which Eq.(7) has a solution. 
It is easy to show that when k  increases the critical point 
 shifts to the right, and the value of the maximum of 
N
k
( )
k
cy
( )
(
k
k
F yc )
 decreases steadily. Then it is not difficult to find 
the maximal value of 
 for which Eq.(7) has a solution. By 
 we denote this maximal value. 
k
(
)
m
m
k
k
N
=
For given dimensionality 
 the pattern with the number 
 is the last in whose vicinity there is a fixed point. For 
 Eq.(7) has a solution in the region 
too. 
Consequently, these patterns will also be recognized. On the 
contrary, for 
 Eq.(7) has no solutions in the region 
. Consequently, the patterns with such numbers will 
not be recognized. 
m
k
k
>
( )
ck
y
y
>
 
Figure 1.  The behavior of the function 
kF ( )
y  defined by Eq.(7) when the 
weights  are equal to 
. Here 
, 
1/
rμ
μ
=
0y
k = 5
 is the solution of the 
equation (7) for 1
0.00
N =
N
m
k
m
k
< k
( )
ck
y
y
>
1 and 
(5)  is the critical value. 
cy
Let 
 be the weight corresponding to the pattern with 
the number 
: 
cr
m
k
m
c
k
r
= r
. Our consideration shows that only 
the patterns, whose weights are not less than the critical 
value 
 will be recognized. Patterns whose weights are less 
than the critical value 
 are not recognized in spite of the 
fact that these patterns take part in the construction of the 
quasi-Hebbian matrix. The memory of the network is 
limited, but the catastrophic forgetting does not occur.  
cr
cr
 
Figure 2.  Maximal number of the pattern 
 that can be recognized   
for the weights 
(
)
km
N
1/
rμ
μ
=
.  The daggers show the results of computer 
simulations 
exp .  
m
k
Let us show how this approach works, choosing as an 
example matrices whose weights are the terms of harmonic 
59
FUTURE COMPUTING 2011 : The Third International Conference on Future Computational Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-154-0

On the left panel of Fig.3 the dependence of the ratio 
 on 
 for three dimensionalities 
 is shown. 
We see that curves have distinct points of maximum, but for 
all cases the value of the all maximums are the same: 
1/
rμ
μ
=
. For the dimensionalities N =
, 
series 
103
5 103
⋅
, 
, 
 and 
 we obtained the maximal number 
 of the pattern that could be recognized solving Eq.(7) 
numerically. In Fig.2 the value of k
 as function of 
 is shown. The daggers are the results of computer 
simulation
. For the given N we generated a random 
matrix with the weights that are the terms of harmonic series 
and defined the maximal number of the pattern that was a 
fixed point. (Patterns with larger numbers differed notably 
from the nearest fixed points.) The number 
was obtained 
as a result of averaging over 10 random matrices. We see that 
the experimental values are in good agreement with the 
theoretical results. The numerical solution of Eq.(7) shows 
that the storage capacity of such a network  asymptotically 
tends to  zero: 
(
, )/
km
N q
N
q
N
104
2.5 104
⋅
5
10
(
)
km
N
(
m N)
lim
(
) /
0.05
N
k N
N
→∞
≈

.  
 
(10) 
log10
N
In other words, the maximal number of patterns that can be 
memorized by the network is expressed as 
exp
km
0.05
Mc
N
≈
⋅
. It 
is more than two times less than the storage capacity for the 
standard Hopfield model ( 0.13
), but the catastrophic 
destruction of the memory does not occur. Let us note that 
the optimal values 
 are rather close to 1: 
8 N
⋅
exp
km
(
)
qm
N
0.992, 0.9992
m
q =
 and 
  for 
=1000, 10000 and 
100000, respectively.  
N
0.99992
When the value of q  becomes larger than
, the 
number of recognized patterns decreases. It is clear that as 
far as the parameter 
 tends to 1, our model more and more 
resemble the standard Hopfield model for which the 
dimensionality 
 is finite and the number of the patterns 
m
q
. 
lim m
(
) /
~ lim 1/
ln
0
N
N
k
N
N
N
N
→∞
→∞
=
q
In the next section we examine in details a particular 
distribution of the weights. 
N
M is infinitely large. It is clear that when 
1
q =  the network 
memory is destroyed: patterns are not recognized by the 
network. It turns out that the destruction of the memory 
occurs even before q  becomes equal to 1.  
III. 
GEOMETRIC PROGRESSION AS WEIGHTS  
Let us discuss in details the case of the weights in the 
form of decreasing geometric progression r
qμ
μ =
, where 
. For the first the weight of such a type were 
mentioned in [5] and [6]. It is natural to assume that in Eq.(3) 
the first value of the summation index is equal to zero and 
.  Now Eq.(3) has the form 
q ∈(0,1)
Let 
denotes the critical value of q  under which only 
the first pattern (with the maximal weight) is recognized; it 
is clear, that 
. It is possible to obtain an analytic 
estimate for 
: 
cq
cq
q
>
0
r =1
m
cq
1
cq
= −δ
2
2
0
1
k
k
q
N
s
q
μ
μ
μ
γ
∞
= ≠
⎛
=
⎜
−
⎝
⎠
∑
⎞
⎟ , where 
( )
k
ks
q
ϕ y
=
. 
(8) 
Our interest is the solution of this equation for large values of 
the argument when the inequality 
 is 
fulfilled. In the r.h.s. of Eq.(8) we replace summation by 
integration. Then in both sides of the equation we pass to 
inverse quantities and obtain the analogue of equation (7): 
( )
1
k
ks
q
ϕ y
=
>
2
2
2
1
(
1)
(
1)
k ( )
N
y
γ
ϕ
ϕ
−
=
−
Φ
−1, 
 
(9) 
where 
1
1
ln
1
( )
ln
k
k
k
k
s
s
s
y
q
⎛
− ⎞
+
⎜
⎟
−
⎝
⎠
Φ
=
. 
When solving Eq.(9) numerically we can find the 
maximal number of the pattern 
 which is recognized by 
the network yet, 
. Our goal is to find such 
value of the parameter q  that corresponds to the maximum 
of the storage capacity of the network. In other words, we 
find the value of the parameter q  for which 
 is 
maximal: 
. It is evident that this 
optimal 
 have to exist; 
denotes its value. 
m
k
(
, )
m
m
k
k
N q
=
(
, )
km
N q
(
)
max
m
k (
, )
q
k N
N q
=

q
(
)
m
m
q
q
N
=
, where
δ ≈1/0.329N
. Up to 
now we do not succeeded in analytic estimation of 
. 
From the fitting of the results of the numerical solution of 
Eq.(9) we obtain: 
m
q
1
2.75
qm
δ
≈ −
⋅
.  
For the last pattern with the number 
 that can be 
recognized by the network, on the right panel of Fig.3 the 
dependence of the overlap on 
 is shown. In the point of the 
solution “breakdown” 
 all overlaps have approximately 
the same values 
m
k
q
cq
0.933
c
m ≈
. 
The results of this section were obtained by means of 
numerical solution of eq. (9). At the present time we try to 
obtain these results analytically. Moreover, it is necessary to 
compare our predictions with the computer simulations in 
the case when the weights are terms of geometrical 
progression. First experiments show good agreement with 
theoretical predictions. It is interesting to analyze other 
distributions of the weights rμ differ from a decreasing 
geometrical progression. Now these investigations are in 
progress. 
IV. 
CONCLUSIONS 
 The common experience indicates that the learning 
process is a continuous one. In the brain there are 
mechanisms allowing one to accumulate steadily the 
obtained information. The Hopfield model is an artificial 
model of the human ability of learning. For this reason the 
60
FUTURE COMPUTING 2011 : The Third International Conference on Future Computational Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-154-0

ACKNOWLEDGMENT 
catastrophic destruction of the memory due to too much 
number of patterns is absolutely inadmissible. An artificial 
memory of a robot also has to work even if it obtains new 
information continuously written down. 
The work was supported by the program of Russian 
Academy of Sciences “Information technologies and 
analysis of complex systems” (project 1.7) and in part by 
Russian Basic Research Foundation (grant 09-07-00159). 
The method of eliminating of catastrophic destruction of 
the memory, proposed in our paper, seems to be very 
attractive. It turns out that it is possible to learn the network 
uninterruptedly, even during the process of the patterns 
recognition. Indeed, each pattern that finds itself in the field 
of vision of a robot can be automatically added to the 
connection matrix. Each pattern modifies matrix elements 
according to the standard Hebbian rule. If the pattern is the 
same as the one written down previously, its weight 
increases by 1. If the pattern is new, it is written down into 
the connection matrix with the initial weight equals to 1. 
According to the statistics of the patterns appearance the 
weights distribution is online modified. There is no 
catastrophic destruction of the memory since every moment 
the real memory of the network consists of patterns, whose 
weights are larger than the current critical value 
.  
REFERENCES 
[1] D. Amit, H. Gutfreund, and H. Sompolinsky, “Statistical 
mechanics of neural networks near saturation,” Annals of 
Physics, vol. 173, pp. 30-67, 1987. 
[2] J. Hertz, A. Krogh, and R. Palmer, Introduction to the Theory 
of Neural Computation. Massachusetts: Addison-Wesley, 
1991. 
[3] G. Parisi, “A memory which forgets,” Journal of Physics A 
vol. 19, pp. L617-L620, 1986. 
[4] J.P. Nadal, G. Toulouse, J.P. Changeux, and S. Dehaene, 
“Networks of Formal Neurons and Memory Palimpsest,”  
Europhysics Letters vol.1(10), pp. 535-542, 1986. 
[5] J.L. van Hemmen, G. Keller, and R. Kuhn, “Forgetful 
Memories,” Europhysics Letters, vol. 5(7), pp. 663-668, 1988. 
[6] J.L. van Hemmen and R. Kuhn, “Collective Phenomena in 
Neural Networks,”. in Models of Neural Networks, E. 
Domany, J.L van Hemmen and K. Shulten, Eds. Berlin: 
Springer, 1992, pp. 1-105. 
cr
 Let the weight of a pattern be less than 
 but we need 
this pattern to be recognized by the network. It is sufficient 
to increase the weight of this pattern doing it to be larger 
than the critical value, and this pattern will be recognized. It 
is possible that at the same time some other patterns cease to 
be recognized. (They are those whose weights are only 
slightly exceeds the critical value 
). Such replacement of 
patterns by other ones does not contradict to the common 
sense. It corresponds to the general conception of the human 
memory. 
cr
[7] A. Sandberg, O. Ekeberg, and A. Lansner, “An incremental 
Bayesian learning rule for palimpsest memory,” preprint. 
[8] Ja. Karandashev, B. Kryzhanovsky, and L. Litinskii, “Local 
Minima of a Quadratic Binary Functional with a Quasi-
Hebbian Connection Matrix,” Proc. 20th International 
Conference on Artificial Neural Networks (ICANN’10) 
Springer, 2010, Part 3, pp. 41-50, LNCS 6352. 
cr
[9] Ya. Karandashev, B. Kryzhanovsky, and L. Litinskii, “Local 
Minima of a Quadratic Binary Functional with a Quasi-
Hebbian Connection Matrix,” arXiv:1012.4981v1. 
 
 
 
Figure 3.   For three dimensionalities N  we show: on the left panel the dependence 
on 
; on the right panel synchronous values of the pattern 
overlaps with the nearest fixed point. On both panels the solid line corresponds to the dimensionality N=1000, the dashed line corresponds to N=10000, the 
point line corresponds to  N=100000. 
km /
N
q
 
q
km /
N
q
m
61
FUTURE COMPUTING 2011 : The Third International Conference on Future Computational Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-154-0

