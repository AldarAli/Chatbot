Mix-matrix Method in Problem of Discrete Optimization 
Iakov Karandashev and Boris Kryzhanovsky 
Center of Optical Neural Technologies 
Scientific Research Institute for System Analysis, Russian Academy of Sciences 
Moscow, Russia 
Yakov.Karandashev@phystech.edu, kryzhanov@mail.ru
 
 
Abstract—The problem of a quadratic functional minimization 
in the configuration space of N binary states is considered. In 
order to increase the efficiency of the random-search 
algorithm, we suggest to vary the attraction area of the deepest 
minima of the functional by changing the matrix T it is based 
on. The new matrix M, called mix-matrix, is a mixture of T and 
T2. We demonstrate that this brings about changes of the 
energy surface: deep minima displace very slightly in the space 
(the Hemming distance of the shift is of about 0.01*N ), they 
become still deeper and their attraction areas grow 
significantly. The experiment shows that use of the approach 
results in a considerable displacement of the spectrum of 
sought-for minima to the area of greater depths, while the 
probability of finding the global minimum increases abruptly 
(by a factor of 103 in the case of a two-dimensional Ising model) 
Keywords-quadratic 
optimization; 
binary 
optimization; 
combinatorial optimization; area of attraction; local search; 
random search; energy landscape transformation; mix-matrix 
I.  INTRODUCTION 
The goal of this paper is to improve the efficiency of a 
random search procedure used to solve binary minimization 
problems. In this class of problems, the solution is reduced 
to 
the 
minimization 
of 
the 
quadratic 
functional 
constructed from a given N
matrix 
 in the 
dimensional 
configuration 
space 
of 
states 
with 
discrete 
variables 
( )
E S
× N
T
N
1
( , 2
,...,
N )
S
s s
s
=
is = ±1
, 
. Many discrete programming problems, such 
as graph partitioning, graph coloring, traveling salesman 
problem etc., are reduced to this problem [1-2]. In addition, 
this problem arises in condensed matter physics where the 
search of the ground state is important for understanding of 
a disordered system structure [2]. 
i = 1,2,...,
N
It is well known that there is no polynomial algorithm for 
solving this problem, i.e., it is impossible to find a global 
minimum in polynomial time (the problem is NP-hard).  
Attempts are usually made to improve the efficiency of the 
random search procedure by modifying the dynamics of a 
descent over the landscape [1–3] described by 
. In 
contrast to this approach, we propose not to change the 
dynamics of landscape descent but rather to transform the 
energy landscape itself so as to increase the radius of the 
attraction domain of the global minimum (and of other 
minima comparable in depth with the global one).  
( )
E S
In previous work [9], we consider the simplest 
transformation, namely, the raising of T  to the power 
k = 2,3,...
. This approach was found to be productive: due 
to the landscape transformation, the spectrum of found 
minima is strongly shifted towards the deep side and the 
probability of finding the global minimum increases by 
 
times. It was shown that the optimal value of power is 
3
10
k = 3
. 
But the algorithm is unstable at 
: for the most part 
(about 70% of instances) the probability of finding global 
minima increases more than by 3 orders of magnitude in 
average, but sometimes (the rest 30%) it may decrease up to 
zero.  
3
k ≥
In present paper, we suggest to use a mix-matrix M , i.e., 
a mixture of  T  and 
. We claim that this yields a more 
reliable approach.  
2
T
The efficiency of the algorithm proposed is rigorously 
substantiated only for “random” matrices, whose elements 
generated as independent random variables. The application 
of the algorithm to matrices of other types is heuristic.  
The paper is constructed as follows. Section 2 includes 
the problem definition. Some preliminaries concerned the 
energy landscape of quadratic functional are given in Section 
3. We describe the suggested idea regarding mix-matrix in 
Section 4.  In Section 5, it is shown how the mix-matrix 
transforms the energy landscape of quadratic functional.  
Section 6 contains the obtained results for matrices of two 
types (uniform matrices and matrices of 2D Ising model).  
II. PROBLEM DEFINITION AND MINIMIZATION PROCEDURE  
The standard statement of the binary minimization 
problem is as follows. Given an N
 matrix T , find an 
-dimensional configuration vector 
, 
N
×
N
1
2
(
,
,...,
)
m
m
m
mN
S
s
s
s
=
1
smi
= ±
, 
1,2,...,
i
N
=
, that minimizes the energy 
functional 
( ) : 
E S
 
 
2
1
1
1
( )
N
N
ij
i
j
i
j
T
E S
T s s
N
σ
=
=
= −
∑∑
, 
(1) 
where 
T
σ  is the standard deviation of the matrix elements 
. Functional (1) can be symmetrized. For this reason, 
without loss of generality, we assume that the matrix 
 is 
symmetric and its diagonal elements are zero (
ij
T
ij
T
0
ii
T =
).  
218
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-202-8
ICCGI 2012 : The Seventh International Multi-Conference on Computing in the Global Information Technology

 
 
The minimization procedure is based on the Hopfield 
model [4], which is the core of most binary minimization 
algorithms. This is a one-dimensional system of N  spins, 
whose interaction is defined by the energy functional 
. 
The standard (asynchronous) dynamics of the model can be 
described as follows (The full description is shown in Listing 
1). The local field 
acting on the arbitrarily 
chosen i-th spin is calculated as  
( )
E S
( ) /
ih
= −∂E S
∂ is
 
 
2
1
N
i
j i
T
h
N
σ
≠
=
∑
ij
j
T s
i
 
(2) 
If  
, the state of the spin is updated according to 
the decision rule 
0
i
i
h s <
is
= sign h
. This procedure is sequentially 
applied to all the neurons until the network converges to a 
stable state 
. This dynamics is a descent over the energy 
landscape 
, which is a complete analogue of the 
coordinate-wise gradient descent in a real space. 
Sm
( )
E S
 
Listing 1. The program code of the dynamics. 
≠
=
∑
1
2
N
i
i
ij j
j i
i i
i
i
Asynchronous Neural Network Dynamics
Initialize S = (s ,s...s ) s
= ±1
 i = 1 : N
h
=
T s         %
flip
1
(flip > 0)
flip = 0
 i = 1 : N
(h s
< 0) then
s
= -s       
calculate local fields
algorithm
begin
for
end for
while
for
if
≠
j
j
ij i
     %
 j = 1 : N,  j
i
h
= h
+ 2T s  %
flip = flip + 1
reverse spin
refresh fields
for
end for
end if
end for
end while
end
 
NP-complete problems are known to have a huge number 
of local minima. In order to find a global one we have to use 
the random search. The random search procedure is 
described as follows. Given an arbitrarily initial state of the 
network, the nearest local minimum is found. This procedure 
is repeated until a minimum with an acceptable depth is 
found. The efficiency of the random search procedure is 
evaluated by the probability of finding the global minimum, 
by the rate of finding a minimum with a given depth, or by 
the mean depth of the minima found.  
III. PRELIMINARIES 
Before transforming the energy landscape, we establish 
the basic relations associated with the depth of the global 
(local) minimum, which underlie the subsequent argument.  
The first relation is a constraint on the depth of the 
minimum. Let 
 be the configuration 
corresponding to the global minimum 
. We 
extract from T  the term 
 that is responsible for the 
formation of this minimum. To this end, T  is represented as  
0
01
02
0
(
,
,...,
N )
S
s
s
s
=
0
(
)
E
= E S
0
1
0T
 
 
0
T
T
T
=
+
   ,    T
r
 
(3) 
0
0
0
S S0
σ
+
=
T
The statistical weight 
 is found from the condition that 
the elements of 
 and 
 do not correlate. Calculating the 
covariance of the matrix elements and setting it equal to 
zero, we obtain 
0r
0T
1T
 
 
0
0
2
1
E
T
r
δ
δ
+
= −
,   
2
0
2
1
1
N
i
i
s
N
N
δ
=
⎡
⎤
⎛
⎞
=
−
⎢
⎥
⎜
⎟
⎝
⎠
⎢
⎥
⎣
⎦
∑
, 
(4) 
−
where T  is the mean of the elements of T  and δ  is a 
variable with a zero mean and a small standard deviation 
2 / N
σδ
=
. For simplicity, we set 
T = 0
 and 
δ = 0
 (the 
generalization to other cases is obvious). Then (4) yields the 
relation 
 
 
 
(5) 
0
E = − 0r
2
The variances of the elements of 
 and 
 are 
0T
1T
2
2
0
0
T
r
σ
σ
=
 and 
0
T
2
2
1
2
σ
= σ
−σ
. Therefore, we have managed 
to present the random matrix T  as the sum of two 
independent random matrices 
 and 
. Moreover, (3) and 
(4) imply that 
0T
1T
0
1
0
S T S + = 0
, which suggests that the 
contribution of 
 to 
 is strictly zero; i.e., the minimum in 
 is caused only by the contribution of 
.  
1T
0
E
S0
0T
Following [5], we continue decomposition (3) and 
represent the matrix as a weighted sum of exterior products 
of random vectors:  
0
T
m
m
m
T
r S
σ
∞
+
=
∑
S
,  
2
∑ mr =1
. 
For this type of matrices, it was shown in [6] that any of 
the vectors 
 present in the decomposition of T  is a 
minimizer of functional (1) if and only if its weight 
 is 
larger than the critical value  
m
S
mr
 
 
1
2 0.138
cr
N
=
 
(6) 
This assertion is concerned primarily with the point 
, 
which by definition is a minimizer of functional (1) and 
satisfies the relations 
0
S
 
 
, 
,     
0
1
c
r
r
≥
≥
0
1
Ec
≥ E
≥ −
Ec
cr
= −  
 
219
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-202-8
ICCGI 2012 : The Seventh International Multi-Conference on Computing in the Global Information Technology

 
 
The second necessary relation obtained in [7] is that, as 
the depth of minimum
0  increases, its width increases as 
well and, accordingly, the probability of finding this 
minimum grows as 
  
E
(
)
2
2
0
0
(
) ~ exp
c /
P E
NE
E
−
As a result, we have established the following two 
relations:  
− For a larger weight 
, the minimum 
 is deeper and 
the probability of finding it is higher. 
0r
0
E
− 
 can be a minimum only if 
; i.e., the depth of 
the minimum is larger than the critical value 
S0
0
c
r
r
≥
c
E .  
These relations suggest the direction of improving the 
efficiency of the random search algorithm: the energy 
landscape (1) has to be transformed so as to increase the 
depth of the global minimum and, accordingly, to increase 
the probability of finding it.  
IV. THE ALGORITHM 
In this section we describe the proposed minimization 
algorithm. The main idea underlying the algorithm is the 
transformation of energy landscape of the functional. The 
surface described by the quadratic form 
 can be 
transformed only by transforming the underlying matrix.  
( )
E S
Let us define the mix-matrix M as: 
 
 
2
2
1
T
T
z
z
M
T
T
σ
σ
−
=
+
,  
(7) 
where 
 is obtained by raising T  to the second power and 
setting the diagonal elements equal to zero, 
2
T
T
σ  and 
σ2T
 are 
the standard deviations of matrices T  and 
 respectively. 
Substitute the new matrix into (1). Changing the parameter 
 from 
 to 
, we pass from the matrix T  to 
. 
Accordingly, 
the 
landscape 
described 
by 
 is 
transformed into that described by:  
2
T
z
0
1
2
T
( )
E S
 
 
2
1
1
( )
N
N
z
ij
i
j
i
j i
M
E S
M s s
N
σ
=
≠
= −
∑∑
 
(8) 
where  
σM
 is the standard deviation of 
Mij
. Obviously, 
under the landscape transformation, the global minimum is 
shifted in space and its depth and the width of the attraction 
domain change as well.  
Accordingly, we propose the following minimization 
algorithm. Firstly, we choose a value z , then construct the 
mix-matrix (7) and accordingly the functional 
. Then 
we start the minimization procedure consisting of two steps: 
z ( )
E S
− At the first step, a descent over 
 is performed and a 
configuration 
z ( )
E S
Szm
 is found that minimizes 
z ( ) .  
E S
− The second step involves correction, namely, from the 
point 
Szm
, we descend over 
 to the nearest 
minimum 
 of 
.  
( )
E S
Sm
( )
E S
 
Listing 2. The program code of the proposed algorithm. 
≠∑
1
2
N
i
i
ij j
j i
Mix - matrix algorithm
Initialize S = (s ,s...s ) s
= ±1
Initialize the mix - matrix  with certain 
     %
 i = 1 : N
h
=
s         %
M
z
1.Descent over transformed landscape
M
calculate local fie
algorithm
begin
for
=
≠
i
i
i
i
j
j
ij i
flip
1
(flip > 0)
flip = 0
 i = 1 : N
(h s
< 0) then
s
= -s            %
 j = 1 : N,  j
i
h
= h
+ 2
s  %
flip = flip + 1
     %
lds
reverse spin
M
refresh fields
2.Descent over initial 
end for
while
for
if
for
end for
end if
end for
end while
≠
=
≠
∑
i
ij j
j i
i
i
i
i
j
j
ij i
 i = 1 : N
h
=
T s         %
flip
1
(flip > 0)
flip = 0
 i = 1 : N
(h s
< 0) then
s
= -s            %
 j = 1 : N,  j
i
h
= h
+ 2T s  %
fl
landscape
calculate local fields
reverse spin
refresh fields
for
end for
while
for
if
for
end for
ip = flip + 1
end if
end for
end while
end
 
The descent over 
 is performed as described 
above: we calculate the local field of the ith spin 
z ( )
E S
( )
( ) /
z
i
z
h
E
S
is
= −∂
∂  and, if 
, the state of the spin is 
updated according to the decision rule 
( )
0
z
i
i
h
s <
( )
z
i
i
s
= sign h
. The 
full description of the algorithm is given in Listing 2. 
In previous work [9] we consider the simplest 
transformation, namely, when 
k
M
= T
, 
. It was 
shown that the optimal value of power is 
2,3,4,5
k =
k = 3
. In this case 
the probability of finding global minima increases by 3  
orders of magnitude for the most part (about 70%  of 
instances). But sometimes (the rest 30
ones) it may 
decrease up to zero due to vanishing a minimum near 
.  
%
0
S
As a result of this, in present paper, we introduce a mix-
matrix (7), i.e., a mixture of T  and 
, and vary the 
parameter 
 from 
 to 
. This yields a more reliable 
approach. 
2
T
z
0
1
We 
will 
show 
that 
at 
 the 
proposed 
transformation leads to significant increase of the global 
minimum depth, while the shift from the minimum is smaller 
(1
2
0.5
z ≈
− %
 of 
N ) than  in case 
3
M
= T
( 3%  of 
). 
N
220
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-202-8
ICCGI 2012 : The Seventh International Multi-Conference on Computing in the Global Information Technology

 
 
V. CORRECTNESS OF THE ALGORITHM  
The algorithm is substantiated only for “random” 
matrices, whose elements are independent random variables. 
The application of the algorithm to matrices of other types is 
heuristic. 
A. 
The deepening of the minima. 
Let us show that the landscape transformation leads to a 
deeper minimum. Consider the energy 
 at the 
point 
. Following (3), the mix-matrix 
0
(
)
z
z
E
= E S
0
S0
M  is represented as 
2
2
0
1
0
0 1
1 0
1
2
(1
)
T
T
T
T
T
T T
TT
T
M
z
z
σ
σ
+
+
+
+
=
−
+
 
In view of  
 and 
, we then 
derive from (8) that  
0
1
0
S T S + = 0
2
2
(1
)
M
z
z
σ
=
−
+
2
 
 
0
0
z
Ez
= E
+ R  
(9) 
where 
 
2
0
0
0
2
2
(1
)
(1
)
z
z r
zr
N
E
z
z
−
+
= −
−
+
 
 
2
1
1
0
0
2
2
2
1
2
(1
)
1
(1
)
N
N
i
j
i
j i
T
T
ij
z T
zT
R
s s
N
z
z
σ
σ
=
≠
⎛
⎞
−
=
+
⎜
⎟
−
+
⎝
⎠
∑∑
>>
 
In the limit of N
1, 
z0
E  can be viewed as a normally 
distributed quantity with the mean value 
Ez0
 and the 
relatively small noise R  of standard deviation 
1/
R
N
σ
=
. 
The ratio: 
 
   
0
2
2
0
(1
)
(1
)
Ez
z
z
E
0
Nr
z
z
−
+
=
−
+
 
(10) 
shows how many times the average value of the modified 
functional at point S  more than the initial functional value 
at the same point. Taking into account
0
0
Nr ≈1.35
, it is 
obvious that at any value of 
expression (10) is larger than 
unit, hence when N
 one can be sure that the minimum 
becomes deeper. Fig. 1 confirms this. The largest deepening 
(
) is observed at 
. 
z
1
>>
0
E
( )
0
0
Pr{
0 |
0}
z
i
i
i
i
P
s h
s h
0
1.6
Ez
≈
0.6
z ≈
 
 
0
0
Ez
E
z
Figure 1.  The decrease of energy in the point 
(global minimum) due to 
energy landscape transformation (mix-matrix with T ). The dashed line is 
theoretical (10). Other lines are experimental for 50 random instances with 
uniform matrices.  
0
S
2
B. 
The shift of the minima. 
Let us estimate the shift of the minimum under the 
landscape transformation. The mean shift can be represented 
as  
  
,  
d
= N P
⋅
where 
=
<
>
 is the probability that the 
directions of the spin 
s0i
 and the local field 
( )
z
ih
 do not 
coincide. Omitting the unnecessary constants, the value 
)
(z
0i
ih
can be represented as  
s
 
( )
3/2
2
0
0
0
(1
)
z
i
i
h
s
z Nr
zN
r
H
=
−
+
+
 
(11) 
where 
 
2
0
1
1
1
0
0
1
2
(
)
(1
)
N
T
i
j
i
T
T
ij
z Nr
T
T
z T
H
s s
σ
σ
σ
=
⎛
⎞
+
−
=
+
⎜
⎟
⎝
⎠
∑
 
In view of (11), P  is expressed in terms of the error 
function: 
 
(
)
(
2
1 (
)
2
0
1
1
2 ( ) 2
x
P
dx e
γ
ασ
γ
π
∞
−
−
=
Φ
∫
x )
− Φ
, 
(12) 
where 
( )
Φ ⋅  is the probability integral and  
 
0 /
1.
Nr
γ
σ
=
≈
9  , 
 
2
2
1
1
0.7
N
i
i
T
h
N
σ
σ
=
=
≈
∑
 , 
 
0
1
z
Nr
z
α
−
=
+
  
221
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-202-8
ICCGI 2012 : The Seventh International Multi-Conference on Computing in the Global Information Technology

 
 
.
 
Figure 2.  The shift (in bits) of the global minimum as a function of 
 (mix 
with 
).  The curves with error bars were obtained by experiment for two 
types of matrices: matrix with uniformly distributed elements (solid line) 
and 2D Ising matrices (dashed lines). The dash-dot line is theoretical (12). 
z
2
T
Note that at 
 the functional 
coincides with 
initial
 and therefore the shift is absent, this agrees 
(
) with (12).  
z = 0
Ez 0( )
S
=
( )
E S
0
d
= N P
⋅
=
The formula (12) describes a monotone increase of the 
minimum shift with growing z  in view of enlarging 
functional transformation. This corresponds to a common 
sense and is proved by experiment (see fig. 2). 
Expressions (9)–(12) suggest the following conclusions. 
With a high probability, the landscape transformation leads 
to deeper minima and, as a result, to a higher probability of 
finding them. Moreover, the depth increase (10) is larger for 
a larger initial depth 
E0
≈ r
0
N =
…
 
Figure 3. 
of energy of local minima found with the 
. Th
In addition, some part of the running time is spent on 
ma
Nruns
=
 runs. Each run 
res
. In other words, deep minima 
become even deeper and the probability of finding them 
increases, while shallow minima become shallower (or 
disappear at all) and the probability of finding them is 
reduced. This means that the spectrum of minima found by 
the algorithm shifts considerably toward the global 
minimum, and the probability of finding the latter increases 
considerably. The spatial minima displacements caused by 
the transformation are relatively small: it follows from (12) 
that the smallest shifts are expected for the deepest minima. 
VI. RESULTS 
The efficiency of the two-step descent algorithm was 
verified for z  ranging from 
 to 
 for matrices of size 
 of two types:  
0
1
100,
500
− matrices with random elements uniformly distributed 
within 
( 1;1) ; 
−
− matrices of 2-dimensional Ising model with [2].  
During numerical experiments we built a mix-matrix for 
different values of z  from 
 to 
 equally spaced with 
. The results were averaged over 50 random 
instances of each size and type.  
0
1
0.05
z
Δ =
The computational complexity of the algorithm is 
. In experiments, we used the same algorithm 
realization both for sparse and dense matrices, although it is 
possible to reduce the complexity up to 
 in case of 2D 
Ising matrices.  
2
(
)
O N
(
)
O N
 
 The mean value 
0
Emean /
E
d
N
z
z
2d Ising
uniform
Emean
 
proposed two-step algorithm
e solid lines are for mix-matrices with 
2
T . 
The dashed lines are for mix-matrices with 
3
T . The curves are drawn 
r 
two types of matrices: uniform matrices (on top) and 2D Ising matrices. 
The value of 
Emean
is divided by the energy of global minimum 
0
E  and 
does not depen
 the problem dimension N . 
fo
d on
trix multiplication. Nevertheless, each our experiment 
took no more than one hour for 
N  500
. 
Each experiment included 
06
1
ulted in a local minimum. We chose two parameters to 
trace: the mean energy 
Emean
of the minima found and the 
probability of finding a m
um in the interval of energy 
close to global one 
[ 1; 0.99]
E
inim
∈ − −
, where 1
− corresponds 
to 
0
E . 
In experim
e
nly
x-matrix (7) 
but
merical results are shown in Fi
 and 4. 
of 
 comes near 
 with 
inc
nts, we try to use not o
 the mi
 also mix with 
3
T . In this case the mix-matrix was 
constructed in the s
e manner (7) but 
2
T  was replaced 
with 
3
T .  
The nu
am
gs. 3
Fig. 3 demonstrates how the mean value of energy 
Emean
 
minima found for different z changes. It is interesting that 
the value 
0
Emean /
E
 does not depend on the problem 
dimension bu
 of the matrix. 
As we can see from Fig. 3, 
mean
E
t the type
0
E
reasing z . We observe the m
um of 
0
Emean
E  at 
0.7
z
axim
/
≈
for mix-matrix with 
2
T  and the monoton
of 
0
E  for mix-matrix wit
3
T  up to 
1
z = . 
 shows how many tim s the probabilit
e growth 
y of finding 
min
Emean /
h 
Fig. 4
e
ima with energy differed from the global one less than 
1%  increases. For demonstration purpose, we chose the 
imal possible problem dimensions, which we can cope 
with. For 2D Ising matrices the probability of finding 
minima of energy
[ 1; 0.99]
E
max
∈ − −
 is not greater than 
7
1
3 10
P
−
=
⋅
 for N
=12 12
×
. For 
sion 
is 
uniform matrices the 
dimen
N = 500
 (the 
probability 
5
1
3 10
P
maximal 
−
=
⋅
). The probability 
with t e proposed 
as denoted by 
new
P
. As we can see from fig. 4, 
obtained 
h
algorithm w
222
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-202-8
ICCGI 2012 : The Seventh International Multi-Conference on Computing in the Global Information Technology

 
 
the difference between 
 and 
 turned out to be 
enormous – approximately 3 orders of magnitude. 
new
P
1P
An interesting fact is that for uniform matrices the 
 
and 
 curves almost coincide (see. fig. 3-4), and they start 
to diverge when 
 only. For Ising matrices, we have 
another picture: mix with 
 prevails over mix with 
 up 
to 
 and after that vice versa.  
2
T
3
T
0.7
z >
T 2
3
T
z ≈ 0.8
It can be also seen from Fig. 4 that with increasing 
 the 
dispersion rises, and this can lead to the instability of the 
algorithm, i.e., the transformation may change the search 
procedure for the worse in some cases.  
z
VII. CONCLUSION 
Finally, we formulate the minimization algorithm 
proposed.  
The preliminary phase consists of the following steps. 
The original matrix T  is symmetrized (if it is initially not 
symmetric) and its diagonal elements are set to zero. The 
matrix is raised to the kth power (
 or 
) and the 
diagonal elements in the resulting matrix 
 are set to zero. 
Afterwards the matrices T  and 
are normalized on unit 
dispersion and mixed in accordance with (7), and the mix-
matrix 
k = 2
3
k
T
k
T
M is obtained. It depends on the chosen parameter 
. The functional 
 is constructed from 
z ∈(0,1)
z ( )
E S
M  
according to (8).  
After the preliminary phase, the random search procedure 
based on the two-step descent algorithm is executed. 
Specifically, at the first step, a descent over surface
 is 
performed from a random initial configuration to the nearest 
local minimum 
z ( )
E S
Szm
 of 
. The second step involves 
correction: from the point we descend over surface 
 to 
the nearest local minimum 
 of 
, which is, as a rule, 
located near 
z ( )
E S
( )
E S
Sm
( )
E S
Szm
. 
The simplest Hopfield neural network dynamics [4] was 
chosen as a descent dynamics (nevertheless, it can be 
arbitrary). 
A comparison shows that the efficiency of the 
minimization algorithm is improved substantially due to the 
landscape transformation.  
It was shown that we succeeded in decreasing the value 
(difference between the mean energy of 
found minima and global one) by half when 
( 0
mean )
E
E
E
−
0
/
k = 2
and 
0.7  (see fig. 3).  
z =
Due to the proposed method the probability of finding 
suboptimal solutions with energy differed from the optimum 
less than 1%  increases by 
 orders of magnitude for 
uniform (full) matrices of dimension
 and by more 
than 
 orders for (sparse) matrices of Ising model of 
dimension 
2.5
500
N =
3
N =12 12
×
. 
Finally, it seems to be attractive to use the proposed 
algorithm as a preliminary stage of any sort of genetic 
algorithms. Indeed, one can run our algorithm with different 
values of z  to obtain a set of minima. Most of the minima 
must be deep and some of them may lie near the global 
minimum. Therefore, they are the good candidates for being 
parents. 
ACKNOWLEDGMENT 
The work was supported by the program of the Presidium 
of the Russian Academy of Sciences (project 2.15) and in 
part by the Russian Basic Research Foundation (grant 12-07-
00295). 
REFERENCES 
[1] B.W. Kernighan and S. Lin, “An Efficient Heuristic 
Procedure for Partitioning Graphs”, Bell System Tech. 
Journal, 49, pp. 291– 307, 1970. 
[2] A.K. Hartmann and H. Rieger, New Optimization Algorithms 
in Physics (Wiley, Weinheim), 2004. 
[3] C. Dang, W. Ma, and J. Liang, “A deterministic annealing 
algorithm for approximating a solution of the min-bisection 
problem”, Neural Networks 22 (1), pp. 58–66, 2009. 
[4] J.J. Hopfield, “Neural Networks and physical systems with 
emergent 
collective 
computational 
abilities”, 
Proc.Nat.Acad.Sci.USA. vol. 79, pp. 2554-2558, 1982. 
[5] B.V. Kryzhanovsky, “Expansion of a matrix in terms of 
external products of configuration vectors”, Optical Memory 
and Neural Networks  6 (4), pp. 187–199, 2007.  
[6] D.J. Amit, H. Gutfreund, and H. Sompolinsky, “Spin-glass 
models of neural networks”, Phys. Rev. A, vol. 32, pp. 1007-
1018, 1985; Annals of Physics, vol. 173, pp. 30-67, 1987 
[7] B.V. Kryzhanovskii, B.M. Magomedov, and A.L. Mikaelyan, 
“A Relation Between the Depth of a Local Minimum and the 
Probability of Its Detection in the Generalized Hopfield 
Model”, Doklady Mathematics, vol. 72, N3, pp. 986-990, 
2005.  
[8] B.V. Kryzhanovsky and V.M. Kryzhanovsky, “The shape of a 
local minimum and the probability of its detection in random 
search”, Lecture Notes in Electrical Engineering, Vol. 24, pp. 
51-61, 2009.  
[9] Y.M. Karandashev and B.V. Kryzhanovsky, “Transformation 
of 
Energy 
Landscape 
in 
the 
Problem 
of 
Binary 
Minimization”, Doklady Mathematics, v. 80, No. 3, pp. 927-
931, 2009. 
 
223
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-202-8
ICCGI 2012 : The Seventh International Multi-Conference on Computing in the Global Information Technology

 
 
1
lg(
/
)
new
P
P
 
1
lg(
/
)
new
P
P
uniform, N=500
2
2D Ising, N=12
 
Figure 4.  The common logarithm of the ratio of probabilities of hitting the energy interval 
[ 1, 0.99]
E ∈ − −
. The solid lines are for mix-matrices with 
. 
The dashed lines are for mix-matrices with 
. In the left panel the results for uniform matrices of
 (.
). In the right panel the results for 
2D Ising matrices of
 (.
). Note,  that when 
is too small, the algorithm does not find the global minimum in some instances, so the 
points are missed. 
2
T
T3
N = 500
5
1
3 10
P
−
≈ ⋅
N =144
7
1
2.6 10
P
−
≈
⋅
z
 
z
z
224
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-202-8
ICCGI 2012 : The Seventh International Multi-Conference on Computing in the Global Information Technology

