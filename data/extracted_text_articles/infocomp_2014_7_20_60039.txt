Computing Optimised Result Matrices for the
Processing of Objects from Knowledge Resources
Claus-Peter R¨uckemann
Westf¨alische Wilhelms-Universit¨at M¨unster (WWU),
Leibniz Universit¨at Hannover,
North-German Supercomputing Alliance (HLRN), Germany
Email: ruckema@uni-muenster.de
Abstract—The aim of this paper is to discuss and summarise
the main results on computing optimised result matrices from
the practical creation of long-term multi-disciplinary and multi-
lingual knowledge resources. Structuring big data is the essen-
tial process, which has to preceed creating and implementing
algorithms. The knowledge resources implement structure and
features and can be integrated most ﬂexibly into information
and computing system components. Main elements are so called
knowledge objects, which can consist of any content and con-
text documentation and can employ a multitude of means for
description and referencing of objects used with computational
workﬂows. Core attributes are a facetted universal classiﬁcation
and various content views and attributes. Developing workﬂow
implementations for various purposes requires to compute result
matrices from the objects and referred knowledge, e.g., from
geosciences, archaeology, physics, and information technology.
The purposes can require individual processing means, complex
algorithms, and a base of big data collections. Advanced discovery
workﬂows can easily demand large computational requirements
for High End Computing (HEC) resources supporting an efﬁcient
implementation. This paper presents some major methodologies
and statistics instruments, which have been developed and
successfully integrated. The combination of instruments and
resources allows to ﬂexibly compute optimised result matrices for
discovery processes in information systems, expert and decision
making components, search engine algorithms, and fosters the
further development of the long-term knowledge resources.
Keywords–Knowledge Processing; Result Matrix; Optimisation;
Computing; Statistics; Classiﬁcation; UDC; Big Data; High End
Computing; Knowledge Resources; Knowledge Discovery.
I.
INTRODUCTION
Knowledge resources are the basic components in complex
integrated systems. Their target is mostly to create a long-
term multi-disciplinary knowledge base for various purposes.
Request and selection processes result in requirements for
computing result matrices from the available information and
data. Optimisation in the context of result matrices means
“improved for a certain purpose”. Here, the certain purpose is
given by the target and intention of the application scenario,
e.g., requests on search results or associations. Therefore,
improving the result matrices is a very multi-fold process and
“optimising result matrices” primarily refers to the content
and context but in second order also to the workﬂows and
algorithms. The major means presented here contributing to
the optimisation are classiﬁcation and statistics, based on the
knowledge resources. The employed knowledge resources can
provide any knowledge documentation and additional informa-
tion on objects and knowledge references, e.g., from natural
sciences and decision making. Any data used in case studies is
embedded into millions of multi-disciplinary objects, including
dynamical and spatial information and data ﬁles.
It is necessary to develop logical structures in order to
govern the existing unstructured and structured big data today
and in future, especially in volume, variability, and velocity
and to keep the information addressable on long-term. Prepar-
ing and structuring big data is the essential process, which
has to preceed creating and implementing algorithms. The
systematic, methodological, and “clean” big data knowledge
preparation and structuring must generally be named as largest
achievement in this context and can be considered by far
the most signiﬁcant overall contribution [1]. The creation
and optimisation of respective algorithms is of secondary
importance, the more the data must be considered for long-
term knowledge creation as, e.g., the beneﬁts of most of those
implementations depend on a certain generation of computing
and storage architectures, which change all few 4–6 years.
Workﬂows based on these objects and facilities have been
created for different applications. The knowledge resources
can make sustainable and vital use of Object Carousels [2]
in order to create knowledge object references and modularise
the required algorithms [3]. This provides a universal means
for improving coverage, e.g., dark data, and quality within the
workﬂow. Secondary resources being available for data, infor-
mation, and knowledge integration, besides Integrated Infor-
mation and Computing System (IICS) applications, allow for
workﬂows and intelligent components on High End Computing
(HEC) and High Performance Computing (HPC) resources
[4], [5]. This paper presents the up-to-date experiences with
selected components for structures and workﬂows.
This paper is organised as follows. Section II introduces
the previous work with methodologies and components used,
Sections III and IV present the implemented means and statis-
tics fundamentals integrated. Sections V, VI, and VII discuss
the implementation environment and evaluate main results, and
summarise the lessons learned, conclusions and future work.
II.
METHODOLOGIES AND COMPONENTS EMPLOYED
The data used here is based on the content and context
from the knowledge resources, provided by the LX Foundation
Scientiﬁc Resources [6], [7]. The LX structure and the classi-
ﬁcation references based on UDC [8], [9] are essential means
156
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

for the processing workﬂows and evaluation of the knowledge
objects and containers. Both provide strong multi-disciplinary
and multi-lingual support.
An instructive example for an archaeological and geoscien-
tiﬁc use case, deploying knowledge resources, classiﬁcation,
references, and Object Carousels has been recently published
[2]. With this research the presentation complements the use
case by an important methodology, statistics for intermediate
result matrices, usable in any associated workﬂow. In order to
get an overview, the following practical example for a speciﬁc
workﬂow as part of an application component shows how
result matrices for requests can be computed iteratively.
1)
Application component request,
2)
Object search (i.e., knowledge objects, classiﬁcation,
references, associations),
3)
Creation of intermediate result matrices,
4)
Iterative and alternating matrix element creation (i.e.,
based on intermediate result matrices, object search,
referenced content, classiﬁcation, and statistics),
5)
Creation of result matrix,
6)
Application component response.
The workﬂow will mostly be linear if the used algorithms are
linear and the data involved is ﬁxed in number and content.
The knowledge objects are under continuous development
for more than twenty-ﬁve years. The classiﬁcation information
has been added in order to describe the objects with the
ongoing research and in order to enable more detailed doc-
umentation in a multi-disciplinary and multi-lingual context.
Classiﬁcation is state-of-the-art with the development of
the knowledge resources, which implicitly means that the
classiﬁcation is not created statically or even ﬁxed. It can
be used and dynamically modiﬁed on the ﬂy, e.g., when
required by a discovery workﬂow description. Representations
and references can be handled dynamically with the context of
a discovery process. So, the classiﬁcation can be dynamically
modelled with the workﬂow context. The applied workﬂows
and processing are based on the data and extended features
developed for the Gottfried Wilhelm Leibniz resources [10].
Mathematical statistics is a central means for data analysis
[11], [12]. It can be of huge beneﬁts when analysing reg-
ularities and patterns when used for machine learning with
information system components [13]. It is a valuable means
deployed in natural sciences and has been integrated in multi-
disciplinary humanities-based disciplines, e.g., in archaeology
[14]. The span of ﬁelds for statistics is not only very broad
but statistics itself goes far beyond a simple “tool” status [15].
Methodological means, which have been created in order to
be deployed for regular use are workﬂows improving result
quantity and result quality, various ﬁlters, universal classiﬁca-
tions, statistics applications, manually documented resources’
components, integration interfaces for knowledge resources,
comparative methods, combination of several means. The
methodologies with the knowledge resources are based on
computational methods, processing, classiﬁcation and struc-
turing of multi-disciplinary knowledge, systematic documen-
tation, long-term knowledge creation, vitality of data concepts,
sustainable resources architecture, and collaboration frame-
works. In the past, many algorithms have been developed
and implemented [6], [7] for supporting different targets, e.g.,
silken criteria, statistics, classiﬁcation, references and citation
evaluation, translation, transliteration, and correction support,
regular expression based applications, phonetic analysis sup-
port, acronym expansions, data and application assignments,
request iteration, centralised and distributed discovery, and
automated and manual contributions to the workﬂow.
A. Structure and classiﬁcation
The key issues for computing result matrices from knowl-
edge resources are that they require long-term tasks on efﬁ-
ciently structuring and classifying content and context. The
classiﬁcation, which has shown up being most important
with complex multi-disciplinary long-term classiﬁcation with
practical simple and advanced applications of knowledge re-
sources is the Universal Decimal Classiﬁcation (UDC) [16].
According to Wikipedia currently about 150,000 institutions,
mostly libraries and institutions handling large amounts of
data and information, e.g., the ETH Library (Eidgen¨ossische
Technische Hochschule), are using basic UDC classiﬁcation
worldwide [17], e.g., with documentation of their resources,
library content, bibliographic purposes on publications and
references, for digital and realia objects. Just regarding the
library applications UDC is present in more than 144,000
institutions and 130 countries [18]. Further operational areas
are author-side content classiﬁcations and museum collections.
UDC allows an efﬁcient and effective processing of knowl-
edge data. UDC provides facilities to obtain a universal and
systematical view on the classiﬁed objects. UDC in com-
bination with statistical methods can be used for analysing
knowledge data for many purposes and in a multitude of
ways. With the knowledge resources in this research handling
70,000 classes, for 100,000 objects and several millions of
referenced data then simple workﬂows can be linear but the
more complex the algorithms get the workﬂows will mostly
become non-linear. They allow interactive use, dynamical
communication, computing, decision support, and pre- and
postprocessing, e.g., visualisation. The classiﬁcation deployed
for documentation [19] is able to document any object with any
relation, structure, and level of detail as well as intelligently
selected nearby hits and references. Objects include any media,
textual documents, illustrations, photos, maps, videos, sound
recordings, as well as realia, physical objects, such as mu-
seum objects. UDC is a suitable background classiﬁcation, for
example: The objects use preliminary classiﬁcations for multi-
disciplinary content. Standardised operations used with UDC
are coordination and addition (“+”), consecutive extension
(“/”), relation (“:”), order-ﬁxing (“::”), subgrouping (“[]”), non-
UDC notation (“*”), alphabetic extension (“A-Z”), besides
place, time, nationality, language, form, and characteristics.
B. Statistics implementation for the knowledge resources
A vast range of statistics, e.g., mathematical statistics, can be
deployed based on the knowledge resources. The application
of mathematical statistics beneﬁts from an increased number
of probes or elements. Probes can result from measurements,
157
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

e.g., from applied natural sciences and from available material.
In many cases, without further analysis a distribution or result
may seem random. If the accumulation of an occurrence may
indicate a regularity or a rule then this may correlate with a
statistical method. Many cases require that statistical results
have to be veriﬁed for realness. This can be done checking
against experience and understanding and using mathematical
means, e.g., computing probabilities based on probes.
Statistics have been used for steering the development of
the resources. Classiﬁcation and keyword statistics support
the optimisation of the quality of data within the knowl-
edge resources. Counts of terms, references, homophones,
synonyms and many more support the improvement of the
discovery workﬂows. Comparisons of content with different
language representations increase the intermediate associated
result matrices for a discovery process. The created knowledge
resources’ architecture is very ﬂexible and efﬁcient because the
components allow a natural integration of multi-disciplinary
knowledge. The processes of optimising a result matrix differ
from a statistical optimisation by the fact that statistics is only
one of the factors within the workﬂows.
III.
IMPLEMENTED KNOWLEDGE RESOURCES’ MEANS
The goals for the combination of statistics and classiﬁcation
are, for example:
•
Creating and improving result matrices.
•
Decision making within workﬂows.
•
Further development of knowledge resources.
•
Extrapolation and prediction.
The implementation for the required ﬂexible workﬂow cre-
ation and levels is shown in the following sketch (Figure 1).
Workﬂow
Sub-workﬂow . . . Sub-workﬂow
Sub-subworkﬂow . . . Sub-subworkﬂow
[. . . any level . . .]
Algorithm . . . Algorithm
Resources interface . . .
Figure 1. Workﬂow-algorithm sketch of the implementation.
The architecture is non-hierarchical. Any workﬂows can
be applied in chains. Each workﬂow can use sub-workﬂows,
these can use sub-subworkﬂows and so on. Each workﬂow can
call or implement algorithms, e.g., for discovery processes,
evaluation, and statistics. The workﬂows and algorithms can
use or implement interfaces to the resources. The ellipses
indicate that any step can be called or executed in parallel on
HEC resources, e.g., in data-parallel or task-parallel processes,
in any number of required instances.
An example for this is a “multi-probe parallelised opti-
misation” workﬂow, which generates an intermediate result
matrix and uses the elements in order to create additional
results, all of which are combined for an overall optimised
result matrix. The intermediate result matrices are deploying
statistical, numerical methods, and various algorithms on base
of additional knowledge and information resources.
As implementations of statistics are based on counting and
numbers the statistics sub-workﬂows can deploy everything,
e.g., any feature or attributes, which can be counted. Sources
and means of statistics and computation are:
•
Dynamical statistics on the internal and external con-
tent and context (e.g., overall statistics, keyword-,
categories-, classiﬁcation-, and media-statistics).
•
Mathematics and formula on statistics from the content.
•
Elements’ statistics (structuring, content, references).
•
Statistics based on UDC classiﬁcation.
•
UDC-based statistics computed from comparisons and
associations of UDC groups and descriptions.
•
Statistics based on any combination of classiﬁcation,
keywords, content, references, context, and computation.
Workﬂows based on the statistics can be type “semi-manually”
or “automated”. Besides the major processing and optimisation
goals descriptive statistics can be done with each workﬂow
or sub-workﬂow. Any change of the means supported within
a workﬂow can contribute to the optimisation of the result
matrix. Suitable and appropriate means have to be determined
for best supporting the goals of the respective step in the
workﬂow. The implementation considers measuring the op-
timisation by quantity and quality of attributes and features,
on intelligence-based and learning processes. With either use
there is no general quality measure. Possible quality measures
depend on purpose, view, and deployed means. In addition, the
decision on these measures can be well supported by statistics,
e.g., comparing result matrices from different workﬂows on the
same request. Learning systems components can be used for
capturing the success of different measures. The knowledge
resources can contain equations and formulary of any grade
of complexity. Due to the very high complexity level of the
multi-disciplinary components it is necessary to use the basic
instances for a comparison in this context of matrix statistics.
The following passages show basic excerpts of statistics
objects (LATEX representation) being part of the implemented
knowledge resources. These statistics methods/equations are
selected and shown mainly for two reasons: The selected
methods are taken from the knowledge objects contained
in the resources. These methods are used for result matrix
calculations and compared with the evaluation in this research.
IV.
STATISTICS: FUNDAMENTALS AND APPLICATION
Statistics on itself can rarely give an overall decisive answer
on a question. Statistic means merely can be used as tools
for supporting valuations and decisions. Statistics, probability,
and distributions are valuable auxiliaries within workﬂows and
integrated application components, e.g., on numbers of objects,
spatial or georeferences, phonetic variations, and series of
measurement values. Probability and statistics measures are
used with integrated applications, e.g., with search requests,
with seismic components (e.g., Median and Mean Stacks),
which can also be implemented on base of the resources.
158
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

A. Basic algorithms applied with knowledge resources
The mean value, arithmetic mean or average M for n values
is given by
M = 1
n
n
X
ν=1
xν
(1)
Calculating the mean value is described by a linear operation.
The median value or central value is the middle value in a
size-depending sort order of a number of values. For making
a statement on the extent of a group of values, the variance
(“scattering”) can be calculated, with the mean deviation m
and the squared mean deviation m2.
m2 = 1
n
n
X
ν=1
(xν − M)2 = (x − M)2
(2)
For any value this holds m2(A) = m2 + (M − A)2 When
applying statistics, especially when calculating the propagated
error, the following deﬁnition of the variance is used:
m2 =
1
n − 1
n
X
ν=1
(xν − M)2
(3)
The mean deviation ζ(A) is deﬁned as:
ζ(A) = |x − A| for which holds ζ(A) = min. for A = Z (4)
The probable deviation or probable error ρ with the probable
limits Q1 and Q3 is deﬁned as:
ρ = Q3 − Q1
2
(5)
The relative frequency hi is deﬁned as:
hi = ni
n , then it holds
k
X
i=1
hi = 1
(6)
where ni is the class frequency, which means the number of
elements in a class of which the middle element is xi.
B. Distributions deployed with knowledge resources
A continuous summation results in the cumulative frequency
distribution
Hi =
i
X
j=1
hj
(7)
which gives the relative number for which holds x ≤ xi ·Hi is
a function discretly increasing from 0 to 1. The presentation
results in a summation line. With steady variables, for which at
an interval width of ∆x the quotient hi/∆x nears a limit, one
can calculate a frequency density h(xi) and for the summation
frequency H(x):
h(xi) = lim
∆x→0
hi
∆x
and
dH(x)
dx
= h(x)
(8)
With statistical distributions the Gaussian normal distribution
is of basic importance.
h(x) =
1
√
2π e
−1
2x2
(9)
H(x) can not be given “closed”. It can be shown that
K =
+∞
Z
−∞
h(x)dx =
1
√
2π
+∞
Z
−∞
e
−1
2x2
dx = 1
(10)
The Binominal distribution wk(s) is deﬁned by
wk(s) =
k
s

psqk−s
(11)
The sum of the two binominal coefﬁcients is equal to

k+1
s

.
This is described by Pascals’ Triangle. It holds:
M =
k
X
s=0
wk(s) · s = kp
and
m =
p
kqp
(12)
Accordingly, the mean error of the mean value decreases
proportional to 1/
√
k. This describes the error propagation law.
h(X) =
1
√
2πme
−1
2

V.
IMPLEMENTATION FOR THE RESULT MATRIX CASES
A. Measures for optimisation and purposes
The measures for optimisation are on the one hand object
of the services and workﬂows but on the other hand they can
be of concern for the knowledge resources themselves.
Conforming with the goals, measures for optimisation mean
ﬁtness for a purpose, e.g., search for a regularity with statistics
and result matrices. After a search for regularities any statisti-
cal procedure beneﬁts from checking against experiences and
associating the procedure and result with a meaning. In many
cases, e.g., “relevance” means numbers, uniqueness, proximity
for objects, content, and attributes, e.g., terms.
Optimisation can be achieved by various means, e.g., by in-
telligent selection, by self-learning based optimisation, and by
comparisons and statistics. The ﬁrst measures include manual
procedures and essences of results being stored for learning
processes. They can also deploy comparisons and statistics,
which also mean probability and distributions. This case study
is focussed on comparisons and statistics applied with the
knowledge resources. The subject of the statistics deals with
the collection, description, presentation, and interpretation of
data. Especially, the methodology can be based on computing
more than the minimal number of comparisons, computing
more than the minimal number of distributions, computing
result matrices considering the mean of several distributions or
extreme distributions. In the case of “relevance”, information
on weighting may come from sources of different qualities.
The general steps with the knowledge resources, including
external sources, can be summarised as: Knowledge resource
requests, integrating search engine results (e.g., Google),
integrating results more or less randomly, without explicit
considerate classiﬁcation and correlation between content and
request, comparing the content of search result matrix elements
with the knowledge resources result matrix containing classi-
ﬁed elements, statistics on an accumulation of terms, selecting
accumulated terms, elimination of less concentrated results,
selecting the appropriate number of search results.
B. Sources and Structure: Knowledge resources
The full content, structure, and classiﬁcation of the knowl-
edge resources have been used. In the context of the case
discussed here, the sources, which have been integrated and
referenced with the knowledge resources consist of:
•
Classical natural sciences data sources.
•
Environmental and climatological information.
•
Geological and volcanological information.
•
Natural and man-made factor/event information.
•
Data sets and compilations from natural sciences.
•
Archaeological and historical information.
•
Archive objects references to realia objects.
•
Photo and video objects.
•
Dynamical and non-dynamical computation of content.
The sources consist of primary and secondary data and are used
for workﬂows, as far as content or references are accessible
and policies, licenses, and data security do not restrict.
C. Classiﬁcation and statistics in this sample case
Table I shows a small excerpt of resulting main UDC
classiﬁcation references practically used for the statistics with
the knowledge resources in the example case presented here.
TABLE I. UNIVERSAL DECIMAL CLASSIFICATION OF STATISTICS
FEATURES WITH THE KNOWLEDGE RESOURCES (EXCERPT).
UDC Code
Description
UDC:3
Social Sciences
UDC:310
Demography. Sociology. Statistics
UDC:311
Statistics as a science. Statistical theory
UDC:311.1
Fundamentals, bases of statistics
UDC:311.21
Statistical research
UDC:311.3
General organization of statistics. Ofﬁcial statistics
UDC:5
Mathematics. Natural sciences
UDC:519.2
Probability. Mathematical Statistics
UDC:531.19
Statistical mechanics
UDC:570.087.1 Biometry. Statistical study and treatment of biological data
UDC:615.036
Clinical results. Statistics etc.
The small unsorted excerpts of the knowledge resources
objects only refer to main UDC-based classes, which for this
part of the publication are taken from the Multilingual Univer-
sal Decimal Classiﬁcation Summary (UDCC Publication No.
088) [8] released by the UDC Consortium under the Creative
Commons Attribution Share Alike 3.0 license [20] (ﬁrst release
2009, subsequent update 2012).
As with any object the statistics features can be combined
for facets and views for any classiﬁcation subject. On the other
hand statistics objects from the resources can be selected and
applied. The listing (Figure 2) shows an excerpt intermediate
object result matrix on statistics content.
1
ANOVA
[Statistics, ...]:
2
Analysis of Variance.
3
BIWS
[Whaling]:
4
Bureau of International Whaling Statistic.
5
GSP
[Geophysics]:
6
Geophysical Statistics Project.
7
Median
[Statistics]:
8
In the middle line.
9
s. also Median-Stack
10
Median-Stack [Seismics]:
11
Stacking based on the median value of
adjacent traces.
12
MSWD
[Mathematics]:
13
Mean Square Weighted Deviation.
14
MSA
[Abbbreviation, GIS]:
15
Metropolitan Statistical Area.
16
MOS
[Abbreviation]:
17
Model Output Statistics.
18
MCDM
[GIS, GDI, Statistics, ...]:
19
Multi-Criteria Decision Making.
20
SHIPS
[Meteorology]:
21
Statistical Hurricane Intensity Prediction
Scheme.
22
SAND
[Abbreviation]:
23
Statistical Analysis of Natural resource
Data, Norway.
Figure 2. Intermediate object result matrix on “statistics” content.
Learning from this: The classiﬁcations used for this inter-
mediate matrix are based on contributions from more than
one discipline. The elements themselves do not necessarily
have to contain a requested term because the classiﬁcation
160
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

contributes. Several steps may be necessary in order to improve
the matrix, e.g., selecting disciplines, time intervals on the
entries, references, and associations. Because different content
carries different attributes and features the evaluation can be
used in comparative as well as in complementary context.
The implemented knowledge resources means of statis-
tics and computation described above are integrated in the
workﬂows, including classiﬁcation, dating, and localisation of
objects. In addition, probability distributions, linear and non-
linear modelling, and other supportive tools are used within
the workﬂow components.
D. Resulting numbers on processing and computing
The processing and computational demands per workﬂow
instance result from the implementation scenarios. The follow-
ing comparison (Table II) results from a minimal workﬂow
request for a result matrix compared to a workﬂow request
for a result matrix supporting classiﬁcation views referring
to UDC, supporting references and statistics on intermediate
results. Both scenarios are based on the same number of
elements and entries and can be considered atomic instances
in a larger workﬂow. Views and result matrices can be created
manually and automated in interactive and batch operation.
TABLE II. PROCESSING AND COMPUTATIONAL DEMANDS: 2 SCENARIOS,
BASED ON 50000 OBJECT ELEMENTS AND 10 RESULT MATRIX ENTRIES.
Scenario Workﬂow Request for Result Matrix
Value
“geosciences archaeology” (minimal)
Number of elements
50,000
Number of result matrix entries (deﬁned)
10
Number of workﬂow operations
15
Wall time on one core
14 s
“geosciences archaeology” (UDC, references, statistics)
Number of elements
50,000
Number of result matrix entries (deﬁned)
10
Number of workﬂow operations
6,500
Wall time on one core
6,700 s
As the discussed scenarios are instances this means work-
ﬂows based on n of these instances will at least require n-
times the time for an execution on the same system. It must
be remembered that the parallelisation will have a signiﬁcant
effect when workﬂows are created based on many of these
instances when required in parallel. Without modifying the
algorithms of the instances, which mostly means simplifying,
the positive parallelisation effect for the workﬂows can be
nearly linear. Besides the large requirements per instance
with most workﬂows there are signiﬁcant beneﬁcial effects
from parallelising even within single instances as soon as the
number of comparable tasks based on the instances increases.
A typical case where parallelisation within a workﬂow is
favourable is the implementation of an application creating
result matrices and being used with many parallel instances,
e.g., with providing services. The number of 70,000 elementary
UDC classes currently results in 3 million basic elements
when only considering multi-lingual entries – without any
combinations. With most isolated resources only several thou-
sand combinations are used in practice each. The variety
and statistics are mostly deployed for decision processing,
increasing quantity, and increasing quality. Many of the above
cases require to compute more than one data-workﬂow set
to create a decision. A review and an auditing process are
mandatory for mission critical applications. The computational
requirements can increase drastically with the computation of
multiple workﬂows. Each workﬂow will consist of one or
more processes, which can contain different conﬁgurations and
parameters. Therefore, creating a base for an improved result
matrix starts with creating several intermediate result matrices.
With a ten process workﬂow, e.g., the possible conﬁgurations
and parameters can easily lead to computing a reasonable set
of thousands to millions of intermediate result matrices.
The objects and methods used can be long-term documented
as knowledge objects. Nevertheless, there is explicitely no
demand for a certain programming language. Even multiple
implementations can be done with any object. The workﬂows
and algorithms with the cases discussed here have been im-
plemented as objects in Fortran, Perl, and Shell. Anyhow, the
implementation of algorithms is explicitely not part of any core
resources. It is the task of anyone having an application to do
this and to decide on the appropriate means and methods.
VI.
CASE RESULTS AND EVALUATION
Computing result matrices is an arbitrary complex task,
which can depend on various factors. Applying statistics and
classiﬁcation to knowledge resources has successfully provided
excellent solutions, which can be used for optimising result
matrices in context of natural sciences, e.g., geosciences,
archaeology, volcanology or with spatial disciplines, as well
as for universal knowledge. The method and application types
used for optimisation imply some general characteristics when
putting discovery workﬂows into practice regarding compo-
nents like terms, media, and other context (Table III).
TABLE III. RESULTING PER-INSTANCE-CALLS FOR METHOD AND
APPLICATION TYPES ON OPTIMISATION WITH KNOWLEDGE DISCOVERY.
Type
Terms
Media
Workﬂow
Algorithm
Combination
Mean
500
20
20
50,000
3,000
Median
10
5
2
5,000
50
Deviation
30
5
5
200
20
Distribution
90
40
15
20
120
Correlation
15
10
5
20
90
Probability
140
15
20
50
150
Phonetics
50
5
10
20
50
Regular expr.
920
100
50
40
1,500
References
720
120
30
5
900
Association
610
60
10
5
420
UDC
530
120
20
5
660
Keywords
820
100
10
5
600
Translations
245
20
5
5
650
Corrections
60
10
5
5
150
External res.
40
30
5
5
40
Statistics methods have shown to be an important means
for successfully optimising result matrices. The most widely
implemented methods for the creation of result matrices are
intermediate result matrices based on regular expressions and
161
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

intermediate result matrices based on combined regular ex-
pressions, classiﬁcation, and statistics, giving their numbers
special weight. Based on these per-instance numbers this
results in demanding requirements for complex applications
– On numerical data: Millions of calls are done per algorithm
and dataset, hundreds in parallel/compact numeric routines.
On “terms”: Hundred thousands of calls are done per sub-
workﬂow, thousands in parallel/complex routines, are done.
Most resources are used for one application scenario only.
Only 5–10 percent overlap between disciplines – due to mostly
isolated use. Large beneﬁts result from multi-disciplinary
multi-lingual integration. The multi-lingual application adds
an additional dimension to the knowledge matrix, which can
be used by most discovery processes. As this implemented
dimension is of very high quality the matrix space can beneﬁt
vastly from content and references.
VII.
CONCLUSION AND FUTURE WORK
A number of structuring elements and workﬂow procedures
have been successfully implemented for processing objects
from knowledge resources, which allow optimising result
matrices in very ﬂexible ways.
First, long-term multi-disciplinary and multi-lingual knowl-
edge resources can provide a solid source of structured content
and references for a wealth of result matrices. The long-
term results conﬁrm that for the usability the organisation
of the content and the data structures are most important
and should have the overall focus compared to algorithm
adaptation and optimisation. Nevertheless, the computational
requirements may be very high but compared against the long-
term data creation issues, they should be regarded secondary
from the scientiﬁc point of view.
Second, employing a classiﬁcation like UDC has shown to
be a universal and most ﬂexible solution with statistics for
supporting long-term multi-disciplinary knowledge resources.
Third, computing optimised result matrices from objects of
universally classiﬁed knowledge resources can be efﬁciently
supported by various statistics and probability measures. With
the quality and quantity of matrix elements this can also
improve the decision making processes within the workﬂows.
The research conducted provided that advanced discovery
will have to go into depth as well as into broad surface of the
context of the multi-disciplinary and multi-lingual information
in order to effectively improve the quality for most workﬂows.
Many of these workﬂow processes can be very well paral-
lelised on HEC resources. A typical case where parallelisation
is required is the implementation of an application creating
result matrices and used with many parallel instances. This
introduces beneﬁts for the applicability of the discovery facing
big data resources to be included. The integration of the
above strategies and means has proven an excellent method
for computing optimised result matrices. Future work will be
focussed on the workﬂow processes and standardisation and
best practice for container and resources’ objects.
ACKNOWLEDGEMENTS
We are grateful to all national and international partners
in the GEXI cooperations for their support and contributions.
Special thanks go to the scientiﬁc colleagues at the Gottfried
Wilhelm Leibniz Bibliothek (GWLB) Hannover, especially
Dr. Friedrich H¨ulsmann, for proliﬁc discussion, inspiration,
and practical case studies.
REFERENCES
[1]
A.
Woodie,
“Forget
the
Algorithms
and
Start
Cleaning
Your
Data,”
Datanami,
2014,
March
26,
2014,
URL:
http://www.datanami.com/datanami/2014-03-26/forget the
algorithms and start cleaning your data.html [accessed: 2014-04-03].
[2]
C.-P. R¨uckemann, “Sustainable Knowledge Resources Supporting Sci-
entiﬁc Supercomputing for Archaeological and Geoscientiﬁc Informa-
tion Systems,” in Proc. Third INFOCOMP 2013, Nov. 17–22, 2013,
Lisbon, Portugal, 2013, pp. 55–60, ISSN: 2308-3484, ISBN: 978-1-
61208-310-0.
[3]
C.-P. R¨uckemann, “High End Computing for Diffraction Amplitudes,”
in Proceedings ICNAAM 2013, Rhodes, Greece, vol. 1558. AIP Press,
2013, pp. 305–308, ISBN: 978-0-7354-1184-5, ISSN: 0094-243X, DOI:
10.1063/1.4825483.
[4]
U. Inden, D. T. Meridou, M.-E. C. Papadopoulou, A.-C. G. Anadiotis,
and C.-P. R¨uckemann, “Complex Landscapes of Risk in Operations
Systems Aspects of Processing and Modelling,” in Proc. Third INFO-
COMP 2013, Nov. 17–22, 2013, Lisbon, Portugal, 2013, pp. 99–104,
ISSN: 2308-3484, ISBN: 978-1-61208-310-0.
[5]
P. Leit˜ao, U. Inden, and C.-P. R¨uckemann, “Parallelising Multi-agent
Systems for High Performance Computing,” in Proc. Third INFOCOMP
2013, Nov. 17–22, 2013, Lisbon, Portugal, 2013, pp. 1–6, ISSN: 2308-
3484, ISBN: 978-1-61208-310-0.
[6]
“LX-Project,”
2014,
URL:
http://www.user.uni-hannover.de/cpr/x/
rprojs/en/#LX (Information) [accessed: 2014-01-12].
[7]
C.-P. R¨uckemann, “Enabling Dynamical Use of Integrated Systems and
Scientiﬁc Supercomputing Resources for Archaeological Information
Systems,” in Proc. INFOCOMP 2012, Oct. 21–26, 2012, Venice, Italy,
2012, pp. 36–41, ISBN: 978-1-61208-226-4.
[8]
“Multilingual Universal Decimal Classiﬁcation Summary,” 2012, UDC
Consortium, 2012, Web resource, v. 1.1. The Hague: UDC Consortium
(UDCC Publication No. 088), URL: http://www.udcc.org/udcsummary/
php/index.php [accessed: 2014-01-12].
[9]
“UDC Online,” 2014, http://www.udc-hub.com/ [acc.: 2014-01-12].
[10]
C.-P. R¨uckemann, “Archaeological and Geoscientiﬁc Objects used with
Integrated Systems and Scientiﬁc Supercomputing Resources,” Interna-
tional Journal on Advances in Systems and Measurements, vol. 6, no.
1&2, 2013, pp. 200–213, ISSN: 1942-261x.
[11]
Y. Dodge, The Oxford Dictionary of Statistical Terms.
Oxford
University Press, 2006, ISBN: 0-19-920613-9.
[12]
B. S. Everitt, The Cambridge Dictionary of Statistics, 3rd ed.
Cam-
bridge University Press, Cambridge, 2006, ISBN: 0-521-69027-7.
[13]
C. M. Bishop, Pattern Recognition and Machine Learning.
Springer,
2006, ISBN: 0-387-31073-8.
[14]
R. D. Drennan, Statistics in Archaeology, 2008, in: Pearsall, Deborah
M. (ed.), Encyclopedia of Archaeology, pp. 2093-2100, Elsevier Inc.,
ISBN: 978-0-12-373962-9.
[15]
D. Lindley, “The Philosophy of Statistics,” Journal of the Royal Sta-
tistical Society, 2000, JSTOR 2681060, Series D 49 (3), pp. 293–337,
DOI: 10.1111/1467-9884.00238.
[16]
“Universal Decimal Classiﬁcation Consortium (UDCC),” 2014, URL:
http://www.udcc.org [accessed: 2014-01-12].
[17]
“Universal
Decimal
Classiﬁcation
(UDC),”
2014,
Wikipedia,
URL:
http://en.wikipedia.org/wiki/Universal Decimal Classiﬁcation
[accessed: 2014-01-12].
[18]
A.
Slavic,
“UDC
libraries
in
the
world
-
2012
study,”
uni-
versaldecimalclassiﬁcation.blogspot.de,
2012,
Monday,
20
August
2012, URL: http://universaldecimalclassiﬁcation.blogspot.de/2012/08/
udc-libraries-in-world-2012-study.html [accessed: 2014-01-12].
[19]
C.-P. R¨uckemann, “Integrating Information Systems and Scientiﬁc
Computing,” International Journal on Advances in Systems and Mea-
surements, vol. 5, no. 3&4, 2012, pp. 113–127, ISSN: 1942-261x.
[20]
“Creative Commons Attribution Share Alike 3.0 license,” 2012, URL:
http://creativecommons.org/licenses/by-sa/3.0/ [accessed: 2014-01-12].
162
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

