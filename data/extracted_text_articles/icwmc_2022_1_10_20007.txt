Performance Analysis of MIMO using Machine Learning in 5G Networks 
 
Christos Bouras 
Computer Engineering and Informatics Department 
University of Patras  
Patras, Greece 
e-mail: bouras@upatras.gr 
 
Apostolos Gkamas  
University Ecclesiastical Academy of Vella 
Ioannina, Greece 
e-mail: gkamas@aevellas.com 
Ioannis Prokopiou 
Computer Engineering and Informatics Department 
University of Patras  
Patras, Greece 
e-mail: st1059554@ceid.upatras.gr 
 
Vasileios Kokkinos  
Computer Engineering and Informatics Department 
University of Patras  
Patras, Greece 
e-mail: kokkinos@cti.gr
 
 
Abstract— Massive Multiple-Input Multiple-output (MIMO) is 
a high-potential radio antenna technology for mobile wireless 
networks, such as 5th Generation (5G). The use of hybrid 
analog and digital precoding to minimize the energy 
consumption as well as the hardware complexity of mixed 
signal components is an essential strategy. Machine Learning 
(ML) could be able to boost 5G technologies due to the rising 
difficulty of configuring cellular networks. More than ever, an 
ML computational framework focused on successfully 
processing the expected huge data generated normally by 5G 
networks with high subscriber cell density, is required. In the 
Ultra-Dense Networks (UDNs) of 5G and beyond high 
demanding networks paired with beamforming and massive 
MIMO technologies, ML struggles to define network traffic 
aspects distinctively, especially when they are projected to be 
much more dynamic and complicated. This paper presents a 
state-of-the-art analysis of the combined and multiple uses of 
ML along with MIMO technology in 5G Networks. 
Keywords-MIMO; Machine Learning; 5G; Deep Learning; 
Internet of Things; Big Data. 
I. 
 INTRODUCTION  
In recent decades, a rise in Internet traffic has been 
observed, which is projected to continue to grow 
exponentially in the near future. The reason for this is the 
widespread use of a wide range of User Equipment (UE), 
which includes everything from Internet of Vehicles (IoV) 
and Machine-to-Machine Communication (M2M) to Internet 
of Things (IoT), and so on. Network traffic management is 
expected to be a critical problem, especially in the 5th 
Generation (5G) and beyond cellular Ultra-Dense Networks 
(UDNs) and Heterogeneous Networks (HetNets), which are 
the main technologies that will host this traffic, reason being 
the significant congestion on wireless communication 
networks due to the amount of traffic generated from big 
data. The primary problem with the wireless network's 
ongoing growth is that to achieve the necessary area 
throughput, it must either increase bandwidth (spectrum) or 
densify the cells and increasing bandwidth or densifying the 
cells raises hardware costs and increases latency. 
Because of its unique performance and freedom, Massive 
Multiple-Input Multiple-Output (MIMO) is a critical method 
for 5th Generation and future mobile wireless networks. 
Massive MIMO is a type of MIMO that requires connecting 
a base station with hundreds or even thousands of antennas 
to be able to boost spectral efficiency and throughput. 
Massive MIMO makes use of huge antenna arrays at base 
stations and Access Points (APs). When combined with 
millimeter-wave 
(mm-Wave) 
communications, 
which 
employ a bigger spectrum, this architecture enables for 
enhanced cellular communications with increased spectral 
density and reduced complexity. Massive MIMO can 
perceive data from several sensors in real time thanks to its 
high multiplexing gain and beamforming capabilities, 
resulting in decreased latency and larger data rates for 
sensors. 
Artificial Intelligence (AI) has emerged as a cutting-edge 
method with the potential to make major advancements in a 
variety of telecommunications problems, thanks to the uses 
of Machine Learning (ML) and furthermore deep Learning, 
including network management, self-organization, self-
healing, and Physical Layer (PHY) improvements (DL). The 
communication system will be taught how to recognize 
emergent channel models and how to react to changing 
channel conditions by utilizing deep learning techniques, all 
while delivering a cutting-edge tool for maximizing end-to-
end efficiency. DL-based approaches are also perfect for 
operating on Graphics Processing Units (GPUs) to fully use 
parallel hardware because of the Deep Neural Network 
(DNN) framework with ways that can help manage big data 
and fast evolving scenes based on parallel processing 
architectures. The secure uses of AI can greatly optimize 
classical ways in most of the areas. To improve its 
performance, many machine learning methods have been 
applied to MIMO technology. 
In [2], a methodology is presented for producing channel 
realizations that depict 5G scenarios with transceiver and 
artifact mobility. In [3], researchers investigate MT 
localization in Distributed Massive MIMO (DM-MIMO) 
systems using the Apache Spark big data computing 
1
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-973-7
ICWMC 2022 : The Eighteenth International Conference on Wireless and Mobile Communications

framework and the RSS fingerprinting approach in 
conjunction with ML algorithms, with the goal of using it in 
microcells in metropolitan areas. The work in [1] explains 
how to utilize deep Long Short-Term Memory (LSTM) 
learning to produce localized traffic load estimates at the 
UDN base station, while [4] shows a Partial Learning (PL)-
based detection technique and [5] gives a comprehensive 
review of 5G communications research utilizing DL. In [6], 
the authors undertake a review of the evolution of DL 
solutions for 5G communication before providing efficient 
strategies for DL-based 5G scenarios, whilst in [7], they 
provide a complete overview of the primary enabling 
technologies 5G and 6G networks, with a focus on massive 
MIMO systems. For successful hybrid precoding, [8] 
proposes a deep-learning-enabled mmWave massive MIMO 
architecture, in which each precoder selection for getting the 
best decoder is considered as a DNN mapping link. We will 
include a study of how MIMO technology can benefit from 
the modern application of ML in this paper. 
We will present how the components of a single 5G 
network that uses this combination work, what has been 
researched so far, and how it might be enhanced in the 
future. The rest of this work is organized as follows: In the 
following section, we showcase the literature review of the 
most state-of-the-art combinations of MIMO and ML. In 
Section III, we evaluate the use of ML and MIMO in 5G 
networks and Section IV includes our conclusions and future 
applications. 
 
II. 
RELATED WORK 
A. MIMO 
We can think of communicating in a MIMO system as 
sending a matrix rather than a single vector. As a result, we 
can deliver a data stream in parallel to numerous recipients. 
The data to be delivered is encoded by the system, and the 
stream is sent via transmitters. MIMO is using multiple 
antennas to send data to a large number of wireless endpoints 
simultaneously. MIMO is a technique for doubling the 
capacity of a radio link by taking advantage of multipath 
propagation by using multiple transmission and receiving 
antennas. MIMO, being a radio antenna technology, uses 
many antennas at the transmitter and receiver to offer several 
signal channels for data transmission, effectively. Each 
antenna is associated with a distinct signal path, allowing for 
the usage of several signal paths. Massive MIMO is a new 
technology that scales up MIMO and provides significant 
energy economy, spectrum efficiency, resilience, and 
dependability benefits. 
To highlight the importance of massive MIMO, we look 
at the work that has been done in [9]-[11]. In [10], Massive 
MIMO as an enabling technology for future generation of 
networks, is being showcased as a novel technology that 
scales up MIMO and offers considerable benefits. It enables 
both the base station and the mobile unit to use low-cost 
hardware. Expensive and powerful but inefficient equipment 
is replaced at the base station with many low-cost, low-
power components that work together. The term "massive" 
refers to the utilization of the multiple antenna arrays to 
support a plethora of terminals at the same time-frequency 
resource. Comprehensively describing massive MIMO 
systems from several different perspectives in [11], the 
authors point out that, by expanding the capacity of Radio 
Frequency (RF) networks, MIMO provides a more reliable 
connection and reduces congestion. A base station's 
spectrum and energy efficiency can be considerably 
optimized by providing it with hundreds or even thousands 
antennas.  
MIMO can enhance data carrying capacity without 
requiring more bandwidth due to spatial multiplexing, 
however, when compared to the classic single antenna 
antenna-based system, the resource requirements and 
hardware 
complexity 
are 
higher. 
Investigating 
the 
performance constraints of developing "wireless-powered" 
communication 
networks 
using 
opportunistic 
energy 
harvesting from ambient radio signals or specialized wireless 
power transfer, the authors conclude at [9] that when 
developing MIMO systems, compromises must be made in 
order to make simultaneous information and energy 
transmission as efficient as possible. When allocating 
resources in terms of communication to provide optimal 
solutions for network interference levels for maximum 
information vs. energy transfer, there are a few nontrivial 
considerations to keep in mind. 
B. Machine and Deep Learning 
Machine learning is a subfield of AI that refers to when 
computers are using data for learning techniques. It's the 
intersection of computer science and statistics when 
algorithms are used to carry out a procedure without being 
specifically written. The learning process for these 
algorithms falls into two categories based on the variety of 
data hat are given as an input: supervised or unsupervised. 
DL algorithms are a mathematically more complex and 
advanced evolution of machine learning techniques. DL is a 
subset of machine learning that deals with algorithms that 
analyze data in a way similar to the human brain. 
The work that has been done in [3], [4] and [8] best 
describes the role of ML and DL in enhancing MIMO. The 
Partial Learning (PL)-based detection scheme that is 
proposed in [4] can achieve low Bit Error Rate (BER) with 
low computational complexity. They use non-linear 
techniques to have a more efficient BER while relying on 
linear methods to reduce computing complexity, which can 
be even more optimized by using neural network for linear 
detection. Because neural networks ensure that the signals 
are appropriately recognized at the start, this technique can 
achieve lower BER than standard techniques. The results of 
the evaluation of thirteen machine learning methods that is 
performed comparatively, in conjunction with fingerprint-
based MT localization for dispersed Massive MIMO 
topologies [3], reveals that a subset of the assessed ML 
systems may accurately anticipate the position of an MT. 
Finally, the K-Nearest Neighbor (KNN) has been proven to 
appear the best ML algorithm performance, second being the 
2
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-973-7
ICWMC 2022 : The Eighteenth International Conference on Wireless and Mobile Communications

Kernel Ridge Regression (KRR) and Random Forest (RF) in 
all scenarios evaluated. 
The deep-learning-enabled mmWave massive MIMO 
framework proposed in [8] presents a solution to the 
difficulty that already implemented hybrid precoding 
schemes have, which is that they are computationally 
complex and do not fully leverage geographical information. 
This method achieves successful hybrid precoding by 
treating each precoder choice as a associating relation in the 
DNN in order to achieve the best decoder, which is chosen 
by DNN training for optimization of the mmWave massive 
MIMO precoding process. The system model is a typical 
mmWave massive MIMO system with one BS and a modern 
DNN utilized to create a unique precoding framework. The 
suggested approach which has DL in it’s core is viewed as a 
operation that is performing the mapping, and a training 
mechanism to acquire the mmWave-based model's structural 
statistics. With the data fed dynamically changing in 
accordance with the channel circumstances, the DNN is 
trained. The computational complexity of this unsupervised 
learning training strategy is reduced as well. 
C. 5G Networks 
After 1G, 2G, 3G, and 4G networks, 5G is a new global 
standard that is taking over wireless communications. 5G is 
intended to provide data speeds many times faster than the 
previous classic networks, latency that is being characterized 
as “ultra-low”, enhanced dependability, massive network 
capacity, increased availability, larger bandwidth of up to 10 
gigabits per second (Gbit/s) ensuring a more consistent user 
experience for a larger number of people. AI along with the 
infrastructure of Internet of Things (IoT) enable higher 
performance and efficiency. In 2019, cellular phone 
companies began installing 5G networks around the world, 
which are the projected successors to the 4G networks that 
connect the majority of today's handsets. In 5G, the service 
area is separated into cells, which are small geographical 
areas. All 5G wireless devices are connected to the Internet 
and to the telephone network via radio waves via a local 
antenna in the cell. Massive New antennas will be employed 
by MIMO for the several transmitters and receivers to be 
able to transfer a larger amount of data at the same time. 
Observing [1], [2], [7] and [11], the authors take a close 
look at the fundamental technologies that will be critical for 
5G and beyond networks, with a particular focus on massive 
MIMO systems. They discuss some of the many and most 
important challenges in a massive MIMO system, such as 
pilot contamination, channel estimation, precoding, user 
scheduling, energy efficiency, and signal identification, as 
well as some cutting-edge mitigation measures, as seen in 
[7]. For massive MIMO systems, they discuss contemporary 
advances, such as terahertz communication, Ultra-Massive 
MIMO (UM-MIMO), Visible Light Communication (VLC), 
ML, and DL. They believe that MIMO is the solution to the 
massive increase in wireless data traffic because to achieve 
excellent spectrum and energy efficiency with very simple 
processing, antennas are used in combination at both the 
transmitter and the receiver ends. In [11], the authors 
conclude that DL models, such as DNN and Convolutional 
Neural Networks (CNN), while optimizing channel 
estimations and feedback for large MIMO, will dramatically 
improve BER performance and system capacity. Massive 
MIMO and Non-Orthogonal Multiple Access (NOMA) will 
give improved performance and lower internal power usage, 
resulting in overall energy efficiency gains. 
[1] describes the way to employ the deep LSTM learning 
method to produce traffic load based on location estimates at 
the base station of UDN, emphasizing how important it is for 
the 5G network operators to perform control on all the 
resources of the radio in an efficient manner. According to 
this study, traditional traffic control strategies are "reactive," 
meaning that if alike traffic circumstances arise, they are 
prone to congestion again. Predicting congestion incidents 
seeks to prevent them from happening in the first place. The 
study in [2] describes a system with a car traffic simulator 
along a raytracing simulator, in combination, to create 
channel realizations that reflect 5G scenarios and allow for 
the use of sophisticated traffic simulator features with 
mobility of transceivers as well as objects. The research then 
goes on to offer a dataset that may be used to investigate 
beam selection approaches for vehicle-to-infrastructure 
communication utilizing millimeter waves in mmWave 
MIMO. 
 
III. 
MACHINE AND DEEP LEARNING ALGORITHMS 
COMPARISON 
ML and DL methods’ dynamic nature may be 
advantageous for analysis of complex tasks while also 
conserving a substantial amount of processing power. 
Massive MIMO beamforming, channel estimation, signal 
detection, load balancing, and spectrum optimization can all 
benefit from ML and DL technology, according to [7]. 
During channel estimation, data coming from the channel 
can be assumed to be big, and a variety of ML methods can 
be used to predict massive MIMO channels. Massive MIMO 
will see a significant increase in throughput thanks to ML-
based channel prediction. In massive MIMO, ML was 
utilized in the past for effective beam alignment to track 
users, and numerous machine learning and DL approaches 
are also useful for uplink signal detection in massive MIMO. 
Despite its benefits, massive MIMO has many challenges, 
including 
pilot 
contamination, 
channel 
estimation, 
precoding, user scheduling, hardware impairments, energy 
efficiency, and signal detection, all of which need 
understanding and need to be applied in a real-world setting 
before their promised benefits can be realized. 
The work in [8] indicates that using DL to solve the 
channel feedback problem could be a promising path for 
addressing concerns like codebook size and feedback 
overhead. The work in [6] states that improvement can be 
found if the data set acquisition and selection of the model 
issues are overcome, while the explainable development of 
DL methods is progressing, and we will have to establish the 
standard data sets that individuals in the industry support. 
The work in [1] discovers opportunities for improvement by 
taking a large number of traffic parameters and tensor-
modeling them in order to create a learning framework that is 
3
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-973-7
ICWMC 2022 : The Eighteenth International Conference on Wireless and Mobile Communications

even more robust and that can adapt and forecast with even 
more precision. The two challenges encountered in [3] are 
the effect of different locations and terrain on accuracy in 
terms of localization and the quantity of training datasets, as 
well as the effect of employing other DM-MIMO antenna 
array shapes and their effect on MT localization. Future work 
from [2] will entail simulating many sites and scenarios, as 
well as testing some of them with measurements, because it 
is crucial to decrease the computing cost in addition to 
precise modeling. DL in 5G can be examined utilizing a 
systematic and repeatable technique via experimentation. 
All in all, finding effective techniques to decrease the 
pilot contamination effect is a crucial subject to research. A 
scheduling method that guarantees more efficiency and 
fairness that can deliver a higher rate of data while also 
ensuring fairness among users, to increase overall system 
performance is also an important topic of research, as is 
finding effective precoding techniques for massive MIMO. 
Finding a more effective and low-complexity uplink signal 
identification method is one of the most important topics of 
research. Designing a Massive MIMO that is able to work 
with today's 4G network is a fascinating topic to research 
(Figure 1). The following two algorithms show especially 
favorable results. [6] illustrates how DL models, like DNN 
and CNN for massive MIMO, may dramatically optimize the 
performance of BER and the capacity of the system while 
channel estimation and feedback are optimized. [4] proposes 
a neural network-based intelligent detection method to strike 
a balance between cheap computing complexity and low 
BER. 
 
 
Figure 1.  Massive MIMO 
 
IV. 
PERFORMANCE EVALUATION 
In this section, we discuss the performance evaluation of 
the works done in [1], [6] and [8]. We are observing 
interesting results in the use of ML and especially DL in 
combination with MIMO in 5G and beyond networks.  
In general, in [6], a DL-based communication 
framework's performance has been demonstrated for channel 
estimation, encoding and decoding in massive MIMO, even 
though no theoretical work has been derived in this work to 
further verify and improve the framework's performance 
through understanding. Specifically, three deep learning-
based frameworks, NOMA, massive MIMO, and mmWave 
hybrid precoding, are introduced and their performance 
evaluated with an emphasis on 5G. These models use 
extremely large parameters, a high level of memory and have 
increased time complexity. This suggests that we can create 
high-performance deep compression techniques and model 
compression strategies to increase the efficiency of the 
networks that utilize deep learning, making them simpler as 
well. As a result, in the future, the deep reinforcement 
learning-based wireless physical layer should be thoroughly 
researched in order to optimize critical resource management 
tasks and be capable of improving precoding performance, 
BER, SNR, and data rates by enhancing equipment 
performance, such as CSI, latency, and bandwidth 
management. Because the original input signals are 
frequently transformed into binary signals, one-hot vectors, 
modulated integers, and other styles of data representation 
for improving network performance in the DL area, it is 
unclear whether the most modern methods’ performance is 
able to be achieved in DL-based wireless communication 
frameworks while the representation data is varying. In the 
field of DL-based wireless physical layer, the principles of 
learning schemes are still unclear, and a mechanism for 
picking training instances has not been created, which is one 
of the challenges that must be investigated further.  
The authors in [8] compare the DNN-based scheme's 
BER performance to that of the SVD-based hybrid precoding 
scheme, fully digital SVD-based precoding method, fully 
GMD-based precoding method, and new GMD-based 
precoding scheme, demonstrating that the techniques with 
deep-learning at their core are more efficient than the 
traditional methods. In terms of BER, it is noticed that the 
deep-learning-based strategy's performance diminishes as 
batch size increases. In the DNN-based method, the 
performance of hybrid precoding is improved by using a 
lower rate of learning to guarantee a smaller validation error. 
The suggested hybrid precoding strategy surpasses prior 
strategies 
by 
achieving 
improved 
hybrid 
precoding 
performance thanks to DL's superior mapping and learning 
capabilities. The Mean Square Error (MSE) performance 
improves as the number of iterations increases, which is due 
to the fact that all of these algorithms approach conversion 
with more iterations. As a result, when compared to existing 
systems, the proposed DL-based methodology achieves 
improved performance in terms of hybrid precoding 
accuracy and conversion.  
By comparing the work in [1] to the conventional 
method, the technique showcased in [1] achieves a lower rate 
of packet loss than the conventional method. The resource 
allocation approach that is given clearly leads to increased 
throughput. This result demonstrates that, even with a huge 
number of UEs, the proposed strategy outperforms the 
standard way. The solution that is stated results in a much 
4
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-973-7
ICWMC 2022 : The Eighteenth International Conference on Wireless and Mobile Communications

lower packet loss rate, because it may make a localized 
prediction of future crowding and attempt to alleviate or 
completely eschew it ahead of time. Unlike the traditional 
methodology, the proposed method may employ DL to 
generate a localized forecast of future bottleneck, which 
would then be utilized to adjust the UL/DL settings to reduce 
congestion. When compared to the old technique, the UDN 
that utilizes the proposed method can achieve significantly 
higher throughput and lower rates of packet loss. 
Furthermore, in comparison to the traditional method, the 
proposal results in a higher throughput while, in terms of 
network efficiency, surpasses traditional solutions. 
To sum up, from DL-based communication frameworks 
to DL predictions, the DL is clearly the state-of-the-art 
technique that seems to have overtaken the way MIMO in 
5G and beyond networks work. 
 
V. 
CONCLUSIONS 
All in all, we conclude that the use of ML and DL in 
combination with MIMO in 5G and beyond networks 
(Figure 2) has a lot of benefits and better performance in the 
variety of the aspects that are being showcased. Especially, 
the most promising state-of-the-art techniques consist of the 
various uses of DL. It is obvious that such techniques, as 
well as all the principles of learning schemes, still have some 
unclear areas that can be further explored. In the future, more 
analysis needs to be conducted on DL-based wireless 
physical 
layer 
mechanisms, 
congestion 
optimization 
techniques 
and 
precoding 
strategies, 
expanding 
the 
comparison presented in Table I. It is safe to assume that 
with exploration and exploitation of the aforementioned 
artificial intelligence combinations, various benefits can be 
derived in terms of BER, energy consumption, complexity, 
throughput, congestion and, in general, overall efficiency. 
 
 
Figure 2.  Machine/Deep Learning – 5G - MIMO 
 
 
 
 
 
TABLE I.  
COMPARISON OF DL-BASED MECHANISMS, CONGESTION 
OPTIMIZATION TECHNIQUES AND PRECODING STRATEGIES 
Work 
Strategy 
Results 
[2] 
In mmWave MIMO, a dataset is used 
to examine beam selection algorithms 
for vehicle-to-infrastructure interaction 
Channel 
realizations that 
simulate 5G 
scenarios with 
transceivers and 
objects moving 
about. 
[10] 
Energy and spectrum efficiency, 
robustness, and reliability analyses 
Massive MIMO 
description 
[7] 
Overview of core issues in massive 
MIMO system 
MIMO as the 
solution to the 
massive increase in 
wireless data traffic 
[8] 
Deep-learning-enabled mmWave 
massive MIMO framework  
Successful hybrid 
precoding 
[6] 
Overcoming the dataset acquisition 
and model selection issues 
Better results with 
the progressing use 
of DL 
[1] 
Deep LSTM learning technique for 
localized traffic load predictions at the 
UDN base station 
Learn and forecast 
with even greater 
precision 
[4] 
Partial Learning (PL)-based detection 
scheme 
Low BER with low 
computational 
complexity 
[3] 
Comparative performance evaluation 
KNN was the best 
ML algorithm 
performance that 
could effectively 
forecast the 
position of an MT. 
[11] 
Comprehensively describing massive 
MIMO systems from several different 
perspectives 
Better BER 
performance and 
system capacity 
while optimizing 
channel estimates 
and feedback for 
massive MIMO and 
overall energy 
efficiency gains on 
NOMA 
[5] 
Overview of 5G communications 
research using DL 
DNN and CNN can 
increase BER 
performance and 
system capacity 
while optimizing 
channel estimates 
and feedback for 
massive MIMO 
[9] 
Investigating the performance 
constraints of developing "wireless-
powered" communication networks 
using opportunistic energy harvesting 
from ambient radio signals or 
specialized wireless power transfer 
To maximize the 
efficiency of 
simultaneous 
information and 
energy 
transmission, 
fundamental 
compromises must 
be made when 
developing wireless 
MIMO systems. 
 
 
5
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-973-7
ICWMC 2022 : The Eighteenth International Conference on Wireless and Mobile Communications

REFERENCES 
[1] Y. Zhou, Z. M. Fadlullah, B. Mao, and N. Kato, “A Deep-
Learning-Based Radio Resource Assignment Technique for 
5G Ultra Dense Networks,” in IEEE Network, vol. 32, no. 6, 
pp. 28-34, November/December 2018J. Clerk Maxwell, A 
Treatise on Electricity and Magnetism, 3rd ed., vol. 2. 
Oxford: Clarendon, 1892, pp.68–73. 
[2] A. Klautau, P. Batista, N. González-Prelcic, Y. Wang, and R. 
W. Heath, “5G MIMO Data for Machine Learning: 
Application to Beam-Selection Using MACHINE,” 2018 
Information Theory and Applications Workshop (ITA), 2018, 
pp. 1-9. 
[3] W. Y. Al-Rashdan and A. Tahat, “A Comparative 
Performance Evaluation of Machine Learning Algorithms for 
Fingerprinting Based Localization in DM-MIMO Wireless 
Systems Relying on Big Data Techniques,” in IEEE Access, 
vol. 8, pp. 109522-109534, 2020. 
[4] Z. Jia, W. Cheng, and H. Zhang, “A Partial Learning-Based 
Detection Scheme for Massive MIMO,” in IEEE Wireless 
Communications Letters, vol. 8, no. 4, pp. 1137-1140, Aug. 
2019. 
[5] A. Ly and Y. -D. Yao, “A Review of Deep Learning in 5G 
Research: Channel Coding, Massive MIMO, Multiple Access, 
Resource Allocation, and Network Security,” in IEEE Open 
Journal of the Communications Society, vol. 2, pp. 396-408, 
2021. 
[6] H. Huang et al., “Deep Learning for Physical-Layer 5G 
Wireless 
Techniques: 
Opportunities, 
Challenges 
and 
Solutions,” in IEEE Wireless Communications, vol. 27, no. 1, 
pp. 214-222, February 2020. 
[7] R. Chataut and R. Akl, “Massive MIMO Systems for 5G and 
Beyond Networks—Overview, Recent Trends, Challenges, 
and Future Research Direction. Sensors.” 20. 2753. 
10.3390/s20102753, 2020. 
[8] H. Huang, Y. Song, J. Yang, G. Gui, and F. Adachi, “Deep-
Learning-Based Millimeter-Wave Massive MIMO for Hybrid 
Precoding,” in IEEE Transactions on Vehicular Technology, 
vol. 68, no. 3, pp. 3027-3032, March 2019. 
[9] R. Zhang and C. K. Ho, “MIMO Broadcasting for 
Simultaneous Wireless Information and Power Transfer,” in 
IEEE Transactions on Wireless Communications, vol. 12, no. 
5, pp. 1989-2001, May 2013. 
[10] E. G. Larsson, O. Edfors, F. Tufvesson, and T. L. Marzetta, 
“Massive MIMO for next generation wireless systems,” in 
IEEE Communications Magazine, vol. 52, no. 2, pp. 186-195, 
2014. 
[11] L. Lu, G. Y. Li, A. L. Swindlehurst, A. Ashikhmin, and R. 
Zhang, “An Overview of Massive MIMO: Benefits and 
Challenges. in IEEE Journal of Selected Topics in Signal 
Processing”, 
vol. 
8, 
no. 
5, 
pp. 
742-758, 
2014.
 
6
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-973-7
ICWMC 2022 : The Eighteenth International Conference on Wireless and Mobile Communications

