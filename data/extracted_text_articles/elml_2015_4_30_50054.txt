|Ae|: an e-Learning Environment with Multimodal Interaction 
André Constantino da Silva 
Instituto Federal de São Paulo – Campus Hortolândia 
Hortolândia, São Paulo, Brazil 
andre.constantino@ifsp.edu.br 
 
 
Abstract— e-Learning Virtual Environments or Learning 
Content Management Systems provide tools to support 
teaching and learning activities by using the infrastructure of 
the Web to provide its functionality to users. Available on the 
Web, these environments are susceptible to access by a variety 
of devices and modalities such as touch and pen, two modalities 
not available on desktop computers, so the e-learning 
environments need to support different modes of interaction. 
Designing a tailored user interface for each device is not a 
trivial task and results in a high number of code lines that 
must be maintained, so we propose to apply multimodal 
interaction in the Ae e-learning environment to build an e-
learning environment with multimodal interaction: the |Ae|. 
Keywords- Multimodal Interaction; Usability; Information 
Systems; e-Learning Environment. 
I. 
 INTRODUCTION 
e-Learning Environments or Learning Management 
Systems (LMS) provide tools to support teaching and 
learning activities by using the infrastructure of the Web to 
provide its functionality to users. Their user interfaces were 
designed to have good usability using a desktop computer 
with keyboard and mouse as input devices and a high-
resolution medium-size display as output device. By using 
the Web as a means of access, these environments are 
susceptible to access by a variety of devices and a variety of 
modalities such as touch and pen, two modalities not 
available on desktop computers and so not considered in 
their design time. Shneiderman [1] describe that "The new 
computing technologies would include wall-sized displays, 
palmtop appliances, and tiny jewel-like fingertip computers 
that change your sensory experiences and ways of thinking". 
Kugler [2], supported by reports from Gartner Group, 
describes one of the big challenges for the Information 
Technology and Communication (ICT) field in the next 25 
years are non-tactile and natural interfaces, and automatic 
translation of speech. This challenge, coupled with the 
tendency to change the mouse gradually for emerging 
alternative interfaces for working with facial recognition, 
motion and gestures brings new challenges to Human-
Computer Interaction (HCI). 
Designing a tailored user interface for each device is not 
a trivial task and results in a high number of code lines that 
must be maintained. Therefore, our group proposes to apply 
multimodal interaction in an e-learning environment to allow 
users use the many modalities that the new devices can 
support instead of developing one user interface for each 
type of device. In this paper, we present the advances to 
produce a web-based multimodal interaction e-learning 
environment, the |Ae|. Our research goal is studying how the 
desktop hardware shaped the on-line courses and how 
multimodality can improve the teaching and learning 
activities. 
Section 
II 
presents 
a 
literature 
review 
about 
multimodality and e-learning environment. Section III 
presents the adopted methodology and analysis of our 
findings, and Section IV presents our conclusions and future 
works.  
II. 
LITERATURE REVIEW 
‘Modality’ is the term used to define a mode in which a 
user´s input or system´s output are expressed. Nigay and 
Coutaz [3] define modality as an interaction method that an 
agent can use to reach a goal; a modality can be specified in 
general terms as “speech” or in more specific terms such as 
“using microphones”. Several modalities have become 
research topics in recent decades; among them, we can 
mention the voice, handwriting recognition, touch, and 
gestures. Bernsen [4] says that there are no two equal 
modalities; each of them has its own strengths and weakness. 
Bernsen [4] defined systems that use the same mode for 
input and output as unimodal systems, and multimodal 
interaction system as a system that uses at least two different 
modes for input or output. According to Bangalore and 
Johnston [5], multimodal interfaces enable the user´s input 
and system´s output to be expressed in the way or in the 
ways they are better adjusted, given a task, user´s 
preferences, 
and 
physical 
and 
social 
environment 
characteristics 
where 
the 
interaction 
is 
happening. 
Multimodality can aim to an increase the usability, 
accessibility, convenience, and flexibility of an application 
[6]. To Bernsen [4] the main concern about multimodality is 
to create something new, because “when modalities are 
combined, we obtain new and emerging properties of 
representations that could not be considered individually by 
the modalities”. 
Fadel [7] says, “significant increases in learning can be 
accomplished through the informed use of visual and verbal 
multimodal learning”. Alseid and Rigas [8] say, “multimodal 
metaphors may help to alleviate some of the difficulties that 
e-learning users often encounter”, focusing their studies on 
usability and learning of e-learning tools. In this research, the 
authors study the efficiency, effectiveness, and user 
satisfaction for the e-learning process evaluating two e-
learning interfaces version: one interface with text and 
graphs (visual channel) and another one with sound, facial 
expressions, text and graphs (auditory and visual channel). 
69
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-385-8
eLmL 2015 : The Seventh International Conference on Mobile, Hybrid, and On-line Learning

However, the used input device was a mouse (just one 
modality) on a desktop computer, and the researches did not 
study the flexibility of devices. 
Sankey, Birch, and Gardiner [9] studied multimodal 
learning environments that allow instructional elements to be 
presented in more than one sensory mode (visual, aural, 
written). In this case, just the output multimodality was 
studied. Despite the many investigations about how 
multimodality on content and on interaction impacts 
positively on learning process, no e-learning environment 
system with input and output multimodality was found in the 
literature. Online systems that support e-Learning through 
the Web are called e-learning environment systems or 
Virtual 
Learning 
Environments 
(VLE) 
or 
Learning 
Management Systems (LMS). An e-learning environment 
system is an application that uses the Web infrastructure to 
support teaching and learning activities, designed to support 
a variety of users and learning contexts. This environment is 
composed of tools that allow users to create content, 
communicate with other users, and manage the virtual space, 
e.g., chat, forums, portfolios, and repositories. Examples of 
e-Learning environments are Moodle [10], SAKAI [11], and 
Ae [12]. 
III. 
METHODOLOGY AND ANALYSIS 
We adopted the Ae learning environment to develop our 
work. This environment is developed by the TIDIA-Ae 
Project (TIDIA-Ae is the acronym for “Tecnologia da 
Informação para o Desenvolvimento da Internet Avançada – 
Aprendizado 
Eletrônico”, 
in 
English 
“Information 
Technology for Development of Advanced Internet – 
Electronic Learning”). This project was initiated by FAPESP 
(the State of São Paulo Research Foundation) with the main 
goal of developing an e-Learning environment that can 
explore the potential of Advanced Internet and can provide 
support to different educational context needs. We chose this 
learning environment due to our experience in its 
development and the layered component-based software 
architecture [13] with specific layer for the user interface. 
We started our work with a literature review and 
investigation of the interaction problems that occur when 
accessing e-learning environments with modalities that have 
not been considered in the design process, e.g., accessing the 
learning environment with touchscreen devices [14]. We 
noticed that some usability problems happen and need to be 
corrected so users have a better interaction experience using 
a touchscreen device. One example of identified usability 
problem occurs when users have to choose a tool in the Ae 
environment browsing with a touchscreen device like a tablet 
or smartphone. Because the touched area is usually larger 
than the area pointed at by a mouse click, the users might 
have a problem triggering a certain tool; they might select 
something that is outside the desired selection. This problem 
is known as the fat finger problem [15]. Due to the several 
modalities available nowadays and our expertise, we are 
focusing our research on touch, pen, and gesture modalities.  
After the investigation about usability problems, we 
developed a tool that takes the benefits of pen input 
modalities and multi-touch: the InkBlog [16], a tool to 
handwrite or to sketch posts in pen-based devices by adding 
features to manipulate electronic ink into a blog tool. Figure 
1 shows a handwrite resolution using a smartphone with pen-
sensitive screen where the user use the recursion tree 
technique to demonstrate the complexity time of an 
algorithmic.   
We noticed that the system architecture must change to 
include components to receive data from new modalities and 
to treat the multimodality, so we proposed an architecture for 
e-learning environments with multimodal interaction [17]. 
Besides the client-server architecture model, a Web 
application can adopt another architecture model to define 
and structure the client or server components. Usually the e-
learning environments have functionalities to manage data 
about courses and users, so it is necessary to have 
components for course management, user management, user 
authentication, and session management. We considered too 
the W3C Multimodal Architecture [18], Web-Accessible 
Multimodal Interfaces architecture [19], and the architecture 
of multimodal systems [6] to define our architecture. So 
components to treat the multimodality have been added to 
the environment (input recognizers, fusion and fission 
machines, output synthesizers and others). In this in-progress 
research, we are codifying these components related with the 
multimodal interaction and their connection with the e-
learning environment components, to perform tests and study 
the impact of multimodality over the learning activities. The 
first implemented component was to treat electronic ink; this 
component was developed using part of the InkBlog code. 
 
Figure 1.  Using InkBlog to handwrite a post using a stylus in a 
smartphone with pen-sensitive screen and Android version 4.4.2. 
70
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-385-8
eLmL 2015 : The Seventh International Conference on Mobile, Hybrid, and On-line Learning

After user tests, we developed a second version of the 
InkBlog tool adding some features to improve the InkBlog to 
allow users to write messages in touchscreen devices. This 
changing generated another component that treats input data 
from touch interactions. 
About the identified usability problems due to the 
modality change, we are applying Responsive Web Design 
techniques and investigating its limitations. We are facing 
the following problems: send input data from more than one 
modality due limitations of Web architecture and actual 
browsers implementation; dispose available fusion and 
fission machines on a Web architecture; develop a 
framework to build easily tools for the environment with 
multimodal 
interaction; 
determine 
which 
course 
characteristics influence a modality and its adoption. 
We believe that the modalities (mouse, keyboard, and 
high-resolution medium-sized screen) shaped the activities 
done in the environment. Embracing new modalities can 
delivery other benefit: support a large number of educational 
contexts. Gay et al. [20] suggest that the introduction of 
wireless computing resources in learning environments can 
potentially affect the development, maintenance, and 
transformation of learning communities. We believe that the 
same can be said about multimodality and when the e-
learning environments are employed. 
IV. 
CONCLUSION 
Multimodal systems are present in the HCI literature to 
allow users to interact with more than one mode, supporting 
multimodality. We believe that multimodal interaction can 
be a solution for the necessity to allow the environment to be 
accessed by a variety of modalities, so we are developing an 
e-learning environment with multimodal interaction called 
|Ae|. In this paper, we described how we are developing this 
e-learning environment. We noticed that the impact of 
multimodality can go besides enabling access for various 
peripherals interaction and may emerge new functionalities 
that support the production of content that were difficult 
before, or impossible, in the environments. We perceived the 
needs in changing the architecture to treat multimodality and 
in the user interface to get a better usability in devices with 
modalities not considered in the design process. Supporting 
new modalities allows having new tools in the environments, 
perhaps affecting the learning activities. We want to analyze 
these on future works. 
REFERENCES 
[1] B. Shneiderman, Leonardo's Laptop: Human Needs and the 
New Computing Technologies, Cambridge: MIT Press, 2002. 
[2] L. Kugler, “Goodbye, Computer Mouse,” Communications of 
the ACM, vol. 51, n. 9, p. 16, September 2008. 
[3] L. Nigay and J. Coutaz, “A Generic Platform for Addressing 
the Multimodal Challenge,” Proc. of the 13th Conference On 
Human Factors in Computing Systems (CHI´95), ACM Press 
/ Addison-Wesley Publishing Co, September 1995, pp. 98-
105. 
[4] N. O. Bernsen, “Multimodality Theory,” Multimodal user 
Interfaces: From signal to interaction, D. Tzovaras, Ed. 
Berlin: Springer, pp. 5-28, 2008. 
[5] S. Bangalore and M. Johnston, “Robust Understanding in 
Multimodal Interfaces,” Computational Linguistic, vol. 35, 3, 
pp. 345-397, September 2009.  
[6] B. Dumas, D. Lalanne, and S. Oviatt, “Multimodal Interfaces: 
A Survey of Principles, Models and Frameworks,” Human-
Machine Interaction, D. Lalanne and J. Kohlas, Eds. Berlin: 
Springer Berlin / Heidelberg, pp. 3-26, 2009. 
[7] C. Fadel, “Multimodal Learning Through Media: What the 
Research Says,” San Jose, CA: Cisco Systems, 2008. 
[8] M. Alseid and D. Rigas, “An Empirical Investigation into the 
Use of Multimodal e-Learning Interfaces,” Human-Computer 
Interaction, Inaki Maurtua, Ed. Rijeka: InTech, pp. 85-100, 
ISBN: 978-953-307-022-3, DOI: 10.5772/7737.  
[9] M. Sankey, D. Birch, and M. Gardiner, “Engaging students 
through multimodal learning environments: the journey 
continues,”  Proc. of 27th Annual Conference of the 
Australasian Society for Computers in Learning in Tertiary 
Education (ASCILITE 2010), University of Queensland, Dec. 
2010, pp. 852-863. 
[10] Moodle Trust, Moodle.org: open-source community-based 
tools for learning. Available from: http://moodle.org. 
[11] SAKAI Environment, Sakai Project | collaboration and 
learning - for educators by educators. Available from: 
http://sakaiproject.org. 
[12] Ae Project. Ae - Aprendizado Eletrônico Environment. 
Available from http://tidia-ae.iv.org.br/. 
[13] D. M. Beder, A. C. da Silva, J. L. Otsuka, C. G. Silva, and H. 
V. da Rocha, “A Case Study of the Development of e-
Learning Systems Following a Component-based Layered 
Architecture,” Proc. of 7th IEEE International Conference on 
Advanced Learning Technologies (ICALT 2007), IEEE Press, 
Jul. 2007, pp. 21-25, doi: 10.1109/ICALT.2007.4. 
[14] A. C. da Silva, F. M. P. Freire, and H. V. da Rocha, 
“Identifying Cross-Platform and Cross-Modality Interaction 
Problems in e-Learning Environments,” Proc. of 6th 
International Conference on Advances in Computer-Human 
Interactions (ACHI 2013), IARIA, Feb. 2013, pp. 243-249. 
[15] D. Vogel and P. Baudisch, “Shift: A Technique for Operating 
Pen-Based 
Interfaces 
Using 
Touch,” 
Proc. 
of 
25th 
International Conference on Human Factors in Computing 
Systems (CHI’07), ACM Press, 2007, pp. 657–666, doi: 
10.1145/1240624.1240727. 
[16] A. C. da Silva and H. V. da Rocha, “InkBlog: A Pen-Based 
Blog Tool for e-Learning Environments,” Issues in Informing 
Science & Information Technology, vol. 10, pp. 121-135, 
2013. 
[17] A. C. da Silva and H. V. da Rocha, “Multimodal User 
Interface 
in 
E-Learning 
Environments: 
A 
Proposed 
Architecture,” Human-Computer Interfaces and Interactivity: 
Emergent Research and Applications, P. Isaías and K. 
Blashki, Eds. Hershey: IGI Global, pp. 33-49, 2014. 
[18] W3C. Multimodal Architecture and Interfaces. [Online]. 
Available from: http://www.w3.org/TR/2012/REC-mmi-arch-
20121025/  
[19] A. Gruenstein, I. McGraw, and I. Badr, “The WAMI Toolkit 
for Developing, Deploying, and Evaluating Web-Accessible 
Multimodal 
Interfaces,” 
Proc. 
of 
10th 
International 
Conference on Multimodal Interfaces (ICMI’08), ACM Press, 
Oct.  2008, pp. 141-148.  
[20] G. Gay, M. Stefanone, M. Grace-Martin, and H. Hembrooke, 
“The effects of wireless computing in collaborative learning 
environments,” International Journal of Human–Computer 
Interaction, vol. 13, n. 2, pp. 257–276, Nov. 2009, doi: 
10.1207/S15327590IJHC1302_10. 
 
71
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-385-8
eLmL 2015 : The Seventh International Conference on Mobile, Hybrid, and On-line Learning

