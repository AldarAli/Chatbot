Design Guidelines for Hybrid 2D/3D User Interfaces on Tablet Devices 
A User Experience Evaluation 
Katri Salo 
Intel and Nokia Joint Innovation Center 
Department of Information Processing Science 
University of Oulu, Finland 
katri.p.salo@gmail.com 
Leena Arhippainen, Seamus Hickey 
Intel and Nokia Joint Innovation Center 
Center for Internet Excellence 
University of Oulu, Finland 
leena.arhippainen@cie.fi, seamus.hickey@cie.fi 
 
 
Abstract— There is a growing proliferation of 3D based 
applications in tablet devices, but there is a lack of studies 
evaluating user experiences of these user interfaces. In 
particular, most of these applications use a mix of overlaid 2D 
controls and embedded 3D controls for user interactions and 
there is little current understanding on how users perceive and 
experience these controls. This paper presents the results of a 
user experience evaluation made for the user interface of four 
3D applications running on two different tablet devices. A 
number of results are obtained. First, the results show that 
users wish that applications had less 2D overlaid icons and 
more space for touch gesture interactions. Second, the number 
of on screen control elements complicates the activity and 
provides a more disappointing experience. Third, avatar 
control was difficult as there were no tips or clear cues on how 
to use them. Fourth, users expected to control and interact 
with the applications by using direct touch gestures in the 3D 
space. As a result, design guidelines for hybrid 2D/3D user 
interfaces were created. 
Keywords-3D; hybrid; tablet; user experience; user interface 
I. 
 INTRODUCTION 
There is an increasing number of products and services 
which are available for mobile tablet devices. In particular, 
there has been an increasing proliferation of three-
dimensional (3D) graphical-based applications, which have 
been developed for tablet devices. Dillon and Morris [1] state 
that users are often unwilling to use technology which would 
result in an unimpressive performance and this is particularly 
relevant to 3D applications which add complexity to the User 
Interface (UI). From the user's perspective, there needs to be 
more research in the touch screen field [2] and in the 3D 
implementation area [3][4].  
This paper presents the results of a User eXperience 
(UX) study of existing 3D applications that are commercially 
available on tablet devices such as the Apple iPad and 
Samsung Galaxy Tab. We studied the user experience of 
four 3D applications, three games and one map application. 
The focus was on the UI controls and not on the games' 
playability. Of these, the games make heavy use of an 
overlaid UI with 2D icons, as well as touch gestures and 
embedded objects in the 3D space. Each application takes a 
different approach to solve similar problems and this study 
evaluates these choices from the user experience point of 
view and identifies the principal design challenges. 
This paper is organized as follows. Section II presents 
research that is relevant to this research topic. Section III 
presents the research methods used. Section IV presents the 
results. Section V presents a discussion of the results. 
Section VI concludes the research paper. 
II. 
RELATED WORK 
In 1974, Elographics developed and patented the five-
wire resistive technology, the most popular touch screen 
technology, which is still in use today [5]. In 1982, the first 
multi-touch system was developed at the University of 
Toronto; this system allowed users more than one contact 
point at a time. Gestural interfaces have become popular for 
the mass market during the past decade. In 2007, Apple 
introduced the iPhone and iPod Touch demonstrating their 
touch screen capabilities. [5]  
Yee [6] summarizes five major criteria which are thought 
to contribute to the effectiveness of gestural interactions. 
First, the application or system interface should make it clear 
to the user that gestures can be used. Second, the gestures 
should be obvious and intuitive for the relevant tasks. Third, 
the users need to be allowed to gesture with minimal effort, 
e.g., interaction is simple to perform and without a need for 
unusual motor skills. Fourth, gestures should have a logical 
relationship with the application functionality that they 
represent, i.e., in the type of movement and type of 
interaction with objects that they are acting on. Fifth, 
gestures should be designed for repetitive use and minimal 
muscle stress. [6] 
With touch screen gestures it is easier for users to carry 
out certain actions, depending on the task they want to 
accomplish [7]. It has been recognized that the aesthetical 
appeal of the interface is important for users [8] and it can 
also improve the user performance.  
There is little to be found in literature on hybrid UIs; the 
available literature is more focused on virtual environments. 
Haan et al. [9] describe hybrid UIs as an incorporation of 2D 
UI elements in a 3D environment. This type of solution is 
often used in games. In a hybrid UI, the elements in use 
should not be unreasonably large, because they are intended 
as supplements rather than the main focus. The interface 
should be intuitive to use. Icons should provide tool tips so 
that it is also easy for inexperienced users to interact with. 
The icons should be provided with a symbol instead of text 
to save space for the embedded UI. [9] 
180
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

Yoon et al. [4] compared a conventional 2D image–based 
system and a web 3D system by using an online survey of 
furniture style preferences. The findings showed 3D to be 
superior to 2D for product examination and decision making.  
3D is expected to emerge from films to personal 
consumer electronics. 3D can provide a greater immersion 
experience for users, but only if this added value is not 
restrained by poor image quality. According to Jumisko-
Pyykkö et al. [10], an overall excellence of 3D is also 
influenced by the quality of the display. The final quality is 
determined by the users’ perception, which is influenced by 
their characteristics and the context of use. [10][3] 
III. 
RESEARCH METHOD 
The use of hybrid 2D/3D UIs were studied by conducting 
user 
experience 
evaluations 
for 
four 
different 
3D 
applications. Three were three games and one was a 3D map 
application. The selected games were Order and Chaos 
(O&C), Pocket Legend (PL) and Dungeon Defenders (DD), 
which are all mobile multiplayer online (MMO) games. 
These were selected because they have the same game 
objectives but different approaches to the user interface. The 
map application was the YesCitiz Barcelona (YB) demo 
version. The applications have an overlay UI, which is the 
layer where control widgets and other functions with 2D 
icons are found. The 3D space has an embedded UI with 3D 
objects and models. O&C and YB run on an Apple iPad, and 
PL and DD run on a Samsung Galaxy Tab. We selected two 
different tablets in order to observe how people hold the 
device and how they interact with the touch screen. For 
instance, we are particularly interested in which hand or 
fingers they use for holding and controlling the device. This 
paper discusses these issues when they are related to the 
viewpoint controls and both 2D and 3D embedded object 
interaction.    
A. Test Setting and Methods  
For studying how users experience the interaction with 
hybrid UIs, we used various methods; observation, 
interviews, user tests, a customized version of the product 
reaction cards [12] method and a discussion forum. After 
using each test application, the user was interviewed by 
using the product reaction card method. They selected five 
suitable adverbs from a list of 39, and gave a short argument 
for their selection. This provided information that either 
supported the observation or brought new findings. 
In the evaluation, participants used each application for 
around 10 minutes. During the use, no detailed tasks to 
accomplish were given to participants. However, users were 
asked to move the avatar, familiarize themselves with the 
game environment and interact with objects (other players, 
enemies, etc.) when they appear. Users were asked to think 
aloud and reply to questions concerning the use of 
applications and the tablet device. If there were problems in 
proceeding or performing a certain function, they were given 
tips. The observation focused on the touch gestures, the use 
of the overlay and embedded UI objects and also the hand 
and finger positions while holding the tablet and controlling 
the application.  
Before the actual tests, a pilot test was performed to 
identify potential problems, identify gaps and ensure timely 
evaluation completion. A pilot test was carried out and 
changes were made to the product reaction card method and 
the order in which the applications were shown to the users.  
It was decided to alternate the order of the applications in the 
future. O&C was the first game presented to eight users. This 
is because of different UI solution: the use of a hidden 
control widget. User evaluations were conducted in 
laboratory settings. The duration of the evaluations varied 
between 1-1,5 hours. All sessions were video recorded. 
B. Participants  
We had 12 participants, whose age varied from 23 to 34 
years, while the median age was 28 years. The ratio between 
males and females was 1:1. The age group for the evaluation 
was selected based on the target user group, according to 
tablet and mobile gaming demographics [13][14]. One 
participant was left handed, and the rest were right handed. 
Seven users had prior experience with touch screen mobile 
devices while five had none. Only one participant actively 
used a tablet device. Eleven participants had prior experience 
with 3D technology, for example from films or games, but 
one had not. When participants described their gaming 
experience, eight turned out to be active gamers on several 
different platforms. The remaining four had no experience or 
they had used rather dated games a long time ago. 
IV. 
RESULTS  
This chapter presents the results on each test application 
and tablet. Sections A and B discuss the map and avatar 
control. Section C describes the user interaction with 
embedded UI objects, and section D discusses switching 
between the overlay UI and the embedded UI. 
A. Map Controlling 
YB differs from the game applications by only having a 
single UI overlaid component which just changes the camera 
orientation to 'north'. The interaction is based on direct 
gestures to the embedded UI. The application opens with a 
bird's eye view over the area, and the user must then zoom 
closer to street view, where the environment is modeled in 
3D (Fig. 1). A pan gesture is needed to adjust the horizontal 
view and it was the most problematic for users due to its 
unfamiliarity. Six different touch gestures were tried. In 
addition, one user tilted the device in the air to pan the view, 
but this kind of feature was not possible. A few participants 
commented on the similarity of the rotate and pan gestures. 
The 3D modeling received positive comments and it was 
considered helpful. There were participants, who first tried to 
find the 2D control widget for controlling the UI, but 
eventually they understood that the application does not have 
any widget for zooming, rotating or panning. These 
participants had tested one or more of the games before the 
map application, so we suspect that they probably learned 
that 2D elements are used for interaction. One participant 
with no game experience said: “… the map looks so neat- 
maybe that’s why I was more eager to experiment with 
gestures compared to the games.” 
181
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

 
Figure 1.  YB: Pan gesture used (right hand) to tilt in street view. 
B. Avatar Control 
Avatar control was necessary in three of the applications, 
O&C, PL and DD. Table I shows the order (O) in which the 
applications were used by each user, the amount of time (S) 
they spent trying to locate the avatar control widget and 
whether they had game experience (GE) or not (NE). The 
order of the tested applications is important, because 
previous usage has influences on the expectations and 
assumptions for the next one. O&C was the first game in 8 
tests; PL was the second game in the 8 tests and DD as the 
third game in 6 tests. (Table I) 
1) Hidden Avatar Control in O&C 
The avatar control widget in O&C is located in the 
bottom left corner and is initially invisible until the user 
touches that area, at which point it becomes visible and 
controllable (Fig. 2). This was the most difficult gesture for 
the participants to figure out. In the beginning, most 
participants assumed that the avatar would be controlled by 
either tapping or sweeping the game space, or tapping an 
icon with a 'boot' symbol. Many said that a touch screen 
device mislead them into believing that the avatar is 
controlled in a similar manner as with a mouse and a 
keyboard-like point and click, or by a gesture in the game 
space. When participants opted to use their right hand for 
controlling the avatar, this blocked a lot of the view on 
screen (Fig. 2). They wondered why the widget could not be 
activated from the right side of the UI. A participant tried to 
solve the problem by flipping the tablet upside down, 
thinking that the UI would adapt differently. There were 
mainly negative comments on the use of an invisible widget 
as the participants did not feel it was necessary to hide it. In 
total, there were 11 different gestures that the participants 
tried to control the avatar, but in average they needed to try 
five gestures before finding the correct one. It took in 
average 67 seconds to find the correct gestures (Table I).  
After getting used to the hidden control widget in O&C, 
some users eventually gave positive comments on the 
unrestricted control area, which is demonstrated in Fig. 2. In 
O&C it is also possible to have the avatar move 
automatically to a new position by clicking on an object with 
a special marker. This marker comes in the form of a 2D 
yellow arrow icon, which momentarily appears on the 
TABLE I.  
SECONDS (S) FOR FINDING AVATAR CONTROL WIDGET AND 
THE ORDER (O) OF THE APPLICATION USE IN THE TEST. 8 USERS HAD PRIOR 
GAME EXPERIENCE (GE) AND 4 HAD NOT (NE) 
GE/NE 
Order & 
Chaos (O&C) 
Pocket 
Legends (PL) 
Dungeon 
Defenders (DD) 
S 
O 
S 
O 
S 
O 
1GE 
48 
1 
41 
2 
6 
3 
2GE 
28 
1 
3 
2 
9 
3 
3GE 
5 
3 
7 
1 
9 
2 
4NE 
23 
3 
175 
2 
51 
1 
5NE 
56 
1 
2 
2 
7 
3 
6GE 
31 
3 
58 
2 
57 
1 
7NE 
100 
1 
378 
2 
10 
3 
8GE 
152 
2 
8 
3 
70 
1 
9GE 
136 
1 
36 
3 
31 
2 
10NE 
116 
1 
64 
2 
14 
3 
11GE 
70 
1 
31 
2 
6 
3 
12GE 
35 
1 
28 
3 
35 
2 
Average S 
67  
 
69  
 
25  
 
 
 
Figure 2.  O&C: Avatar control widget dragged through the game space. 
 
Figure 3.  PL: Control widget (left) is used with press and drag gesture. 
embedded UI. None used this. One reason could be that it 
went unnoticed, since it typically appears on the bottom of 
the UI, where the other hand was using the camera control 
the overlay UI icons. It is possible that this arrow can be 
understood as a guide to a location as well. 
2) Overlaid and Embedded Avatar Control in PL 
To control the avatar in PL, the control widget has a symbol 
of a yellow, eight–pointed star. When pressed, the widget 
activates a yellow circle around it (Fig. 3). During the test, 
participants commented that this does not remind them of a 
control widget, but rather that it looks like a compass. Five 
different gestures were used. In average, it took three 
182
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

gestures and 69 seconds (Table I) to find it, one reason being 
the unclear symbol. The other reason is that it is possible to 
control the avatar navigation by tapping the game space 
which causes a 'cross' indicator to appear and the avatar 
walks towards it. Three participants found this in-space 
tapping mechanism first, before noticing the control widget. 
Four participants never found this control option for tapping 
the space. For the eight users who did eventually find this in-
space tapping option, it took an average of 144 seconds to 
notice it. These eight persons considered the in-space tapping 
action as a preferred choice to using the control widget. They 
stated that it was easy to use their thumb for this in-space 
navigation and this choice was affected by the smaller screen 
size of the Samsung Galaxy Tab. 
3) Overlaid Avatar Control in DD 
The DD avatar is controlled by a grey widget on the 
bottom left corner (Fig. 4), which was found rather quickly 
by all participants. The average time to find this widget was 
the shortest of all the games, 25 seconds (Table I). 
Participants experimented with different gestures to control 
the avatar. In total, five different types of gestures were tried, 
of which one is the correct gesture. In average, each user 
tried two incorrect gestures before finding the correct gesture 
to control the avatar. Tapping the game space was tried by 
eight, sweeping the game space by seven. A direct tap on the 
avatar was used three times and a tap for the overlay UI 
icons five times. Users felt that the symbol for the control 
widget was quite understandable (Fig. 5). Still many of them 
expected to have a possibility to control the avatar by 
touching the game space directly. 
C. User Interaction with Embedded UI objects  
In O&C, there are 2D icons such as talk bubbles or 
message scrolls appearing above the embedded UI objects to 
indicate interaction. If there was this kind of a cue, 
participants mainly tapped them, instead of tapping on the 
object (e.g., avatar) directly. A direct tap on an object is 
possible in this game, and three participants also tapped on 
objects (e.g., doors) which did not have any function. 
Another cue used in O&C for indicating interactable objects 
are colored circles below the avatar or enemy, which was 
easy to notice, and participants were then able to interact 
directly with them. It was observed that using two hands 
simultaneously was not comfortable for everyone. 
In PL, a bow icon in the overlay UI is quite large and 
noticeable compared to the size of other icons on the overlay 
UI. It was tested by three participants in an attempt to 
interact with an embedded UI object. However, the two 
correct gestures were quickly discovered. Six participants 
were running through portals with a text “magic portal” 
above them; eight participants used the tap on the embedded 
object. 
For the participants, the interaction mechanism with 
embedded UI objects in DD was confusing. There are three 
possible ways to interact, depending on the object. Also there 
are inactive objects with no functionality, such as doors or 
tables, yet two participants tapped these types of objects. 
Some objects such as caskets can be bumped into to open 
 
Figure 4.  DD: A tap used to interact with an embedded 3D object (under 
right thumb). 
 
Figure 5.  DD: A 2D icon (under right thumb) used to interact with a 3D 
object on game space. 
them, and this was used by three users. The most used 
gesture was to tap an object directly, which was eventually 
done by ten users. There is an option to interact with a tap on 
a corresponding 2D icon, which appears momentarily on the 
overlay UI. This was used by six participants. Some of them 
had already tried to tap the embedded object, but nothing 
happened due to the distance from the object, or the touch 
pressure was too light. From then on, they continued to 
interact through the corresponding 2D icon. When 
participants were given a tip to find a certain object and then 
to try interacting with it, four expected it to happen by using 
a sword icon and five tapped a menu icon. One participant 
tapped a blue arrow icon. 
D. Switching Between Overlay and Embedded Objects  
According to participants, the icons were mainly 
“misleading symbols”. A boot symbol in O&C was the most 
problematic; it was interpreted as a kick, walk and weapon to 
name a few. Also, the amount of icons was important; too 
many icons caused user confusion. The result was that the 
users focused more on the overlay UI icons than using what 
is available in the embedded UI. In an interview, it was 
remarked that: “… if this is a touch screen device, why do 
you need icons to do stuff you could use gestures for?” 
In the embedded UIs, there are different cues (color, text) 
to inform the gamer that one can interact with an object. 
Eleven participants playing O&C did not need a suggestion 
to try interacting with the game space. The O&C UI was also 
183
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

easy to learn and more intuitive. The objects are often 
animated or color cued (enemies, avatars), or cued with a 2D 
symbol above the embedded object. This raises a question on 
whether the choice of having 2D icons in the embedded 3D 
UI is consistent with the idea of a 3D space. It seemed that 
participants were more prone to interact through these 2D 
icons whenever they were available. It could be possible that 
since O&C was the game which has the least number of 
overlay UI icons, more interaction happened on the 
embedded UI. 
Even though PL has the most overlay UI icons, seventeen 
of them, the use of the embedded UI was active and quickly 
picked up. Since PL has the chance to control the avatar by 
tapping on the game space, it may have encouraged eight 
users to interact with objects. Of the overlay 2D icons, the 
icon with a large bow symbol was tried by three users. Also, 
the cues were simple, mainly text and animation. 
DD has 14 overlay UI icons, and when there is a 
possibility to interact with an object, a 2D icon appears on 
the overlay UI (Fig. 5). So the gamer can choose between the 
icon and the object directly. Six users were interacting with 
the embedded UI objects (Fig. 4) directly without help, the 
remaining six were either encouraged to find a way to do that 
or they never noticed the possibility. The different overlay 
icons tapped for interaction had symbols of a blue arrow, two 
swords and a blue menu. It was a common error that a 
participant did not react in any way to an animated, glowing 
large white stone with text on it (Fig. 4). It is a portal to the 
next level, yet they passed it multiple times until they were 
advised to tap it.  
V. 
DISCUSSION  
Participants expected to use the touch gestures instead of 
interacting with multiple 2D icons. Even though Chehimi 
and Coulton [11] mention that mobile games should be 
playable with a single button, it is not always possible, 
especially in the MMO genre. Still the UI should be 
simplistic to give more room and chances for the touch 
gesture controls. It was apparent that the test applications did 
not meet Yee's [6] recommendation to design interaction for 
repetitive use and minimal muscle stress. Many participants 
felt that the control widget use was tiring and had problems 
to operate with it. A similar symptom was however not 
apparent with YB use, where the user has greater freedom to 
perform gestures. 
In all the game applications, participants expected that 
the controls would happen with a direct touch on the game 
space, usually by tapping or swiping it. A few of the 
comments concerned the limited area for the touch, that 
when using a fixed control widget, one has to monitor where 
their fingers are. No one considered this input method 
particularly advanced. In PL, there is the possibility to use 
the tap gesture on the game space, which was preferred over 
the fixed control widget due to the unrestricted touch area 
and also because both hands could be in use.  
Another issue was the typical location of the control 
widget, which was on the left in each application. The 
majority of the participants were right handed, and often held 
the tablet with their left hand, controlling with the right hand. 
This blocked the view on the game space, and caused 
another complaint. When it was suggested that they try 
controlling with their left hand, some felt that they did not 
trust the strength of their thumb to press and drag. The users 
then tried to control with their left thumb, but they soon 
reverted back to their left or right index finger. Most of the 
participants expected that it would be possible to change the 
control widget to the right side of the UI. 
The biggest problem in games according to participants 
was the avatar control; there were no quick cues on how – to 
control the avatar movement. A brief animation of the 
gestures during the first steps of the application use could 
solve this. One commented that: “… if a game is supposed to 
be played with two thumbs, then there shouldn't be important 
objects in the middle of the screen, you can't reach them.” 
Similar observations were made during the tests when some 
female participants had problems with the Apple iPad and 
the simultaneous use of two hands. 
A. Proposed Design Guidelines 
Based on the findings from the user evaluations, we 
propose the following set of design guidelines to improve the 
user experience of the hybrid 2D/3D user interfaces in tablet 
devices: 
1) Controls 
 
Place an easily identifiable control widget in a 
logical location on the overlay UI 
 
Promote the use of direct on-screen touch gestures 
 
Provide quick and interesting guidance for the 
principal gesture use. 
 
Reduce the need for an overlay UI control use 
 
Avoid implementing gestures which are too similar 
2) 3D space 
 
Design clear and consistent cues for interactive 
embedded 3D objects 
 
Avoid use of 2D icons in 3D space 
 
Design location of interactive elements by taking 
into account user's hands and fingers positions  
3) Icons 
 
Give the user a possibility to customize the icons’ 
location 
 
Use simple and consistent icons in the overlay UI 
 
Minimize the amount of icons in the overlay UI 
 
Ensure that the size of the icons in the overlay UI is 
consistent 
 
Avoid placing icons too close to each other. 
  These guidelines are derived from the results of the 
study of the use of hybrid 2D/3D UIs. These can be applied 
when there is a need to use both 2D icons in the overlay UI 
and 3D objects embedded into space.  
VI. 
CONCLUSION AND FUTURE WORK 
A user experience study on 3D applications in tablet 
devices was conducted. Twelve participants used four 
applications in two tablet devices. The study found issues in 
a combined use of an overlay UI and an embedded UI, 
184
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

focusing in the control methods and interaction with 
embedded objects in a 3D space.  
2D icons on the overlay UI negatively affect the use of 
direct gestures in 3D space. These overlay UI elements 
often result in an unsatisfying interaction, and reduce the 
user’s interest on embedded objects in 3D space. 
Availability of 2D icons also distracts the user from 
interacting with embedded 3D objects. An application 
without a forced use of overlay UI controls gets users more 
interested in experimenting with the touch gestures and the 
3D space. 
Based on the study results, design guidelines for a 
hybrid UI was created with an emphasis on encouraging 
direct on-screen gestures and reducing the dominance of an 
overlay UI. In the future with 3D UI design, there might be 
a need to design only embedded UIs with 3D objects and 
totally avoid 2D overlay icons. Future studies will be 
conducted to validate these guidelines. 
ACKNOWLEDGMENT 
This work was carried out at the Intel and Nokia Joint 
Innovation Center as part of Tekes Chiru project. We want to 
thank Lotta Haukipuro from CIE for an excellent co-
operation with the PATIO test user recruitment. We would 
like to warmly thank all the participants and Terhi 
Koskipaasi for assisting with experiments. 
REFERENCES 
 
[1] A. Dillon and M. Morris, “User acceptance of new 
information technology: theories and models,” in Annual 
Review of Information Science and Technology, vol. 31, M. 
Williams (ed.), Medford, NJ: Information Today, 1996, pp. 3-
32. 
[2] A.A. Ozok, D. Benson, J. Chakraborty and A.F. Norcio, “A 
Comparative Study Between Tablet and Laptop PCs: User 
Satisfaction and Preferences,” International Journal of 
Human-Computer Interaction, vol 24, Mar. 2008, pp. 329-
352, doi:10.1080/10447310801920524. 
[3] S. Jumisko-Pyykkö, M. Weitzel and D. Strohmeier, 
“Designing for User Experience: What to Expect from Mobile 
3D TV and Video?” Proc. of the 1st International Conference 
on Designing Interactive User Experiences for TV and Video 
(UXTV ‘08), ACM Press, Oct. 2008,  pp. 183-192, 
doi:10.1145/1453805.1453841. 
[4] S. Yoon, J. Laffey and H. Oh, “Understanding Usability and 
User Experience of Web – based 3D Graphics Technology,” 
International Journal of Human – Computer Interaction, vol 
24, 
Mar. 
2008, 
pp. 
288-306, 
doi: 
10.1080/10447310801920516. 
[5] D. Saffer, Designing Gestural Interfaces. Sebastopol, CA: 
O’Reilly Media Inc., 2009. 
[6] W. Yee, ”Potential Limitations of Multi-touch Gesture 
Vocabulary: Differentiation, Adoption, Fatigue,” Proc. of the 
13th 
International 
Conference 
on 
Human-Computer 
Interaction (HCII 2009). Part II: Novel Interaction Methods 
and Techniques, Springer Berlin / Heidelberg, Jul. 2009,  pp. 
291- 300, doi: 10.1007/978-3-642-02577-8_32.  
[7] F. Balagtas-Fernandez, J. Forrai and H. Hussmann, 
“Evaluation of User Interface Design and Input Methods for 
Applications on Mobile Touch Screen Devices,” INTERACT 
’09: Proc. of the 12th IFIP TC 13 International Conference on 
Human-Computer: Part I, Springer Berlin / Heidelberg,  Aug. 
2009, pp. 243-246, doi: 10.1007/978-3-642-03655-2_30. 
[8] S.J.P. McDougall and I. Reppa, ”Why do I like it? The 
Relationships 
Between 
Icon 
Characteristics, 
User 
Performance and Aesthetic Appeal,” Proc. of the Human 
Factors and Ergonomics Society 52nd Annual Meeting, Sep. 
2008,  pp. 1257-1261. 
[9] G. Haan, E.J. Griffith, M. Koutek and F.H. Post, “Hybrid 
Interfaces in VEs: Intent and Interaction,” Proc. of the 12th 
Eurographics Symposium on Virtual Environments (EGVE 
2006), 
May 
2006, 
pp. 
109-118, 
doi:10.2312/EGVE/EGVE06/109-118. 
[10] S. Jumisko-Pyykkö, D. Strohmeier, T. Utriainen, and K. 
Kunze, ”Descriptive Quality of Experience for Mobile 3D 
Video,”  Proc. of the 6th Nordic Conference on Human-
Computer Interaction: Extending Boundaries (NordiCHI ‘10), 
ACM 
Press, 
Oct. 
2010, 
pp. 
266-275, 
doi:10.1145/1868914.1868947. 
[11] F. Chehimi and P. Coulton, “Motion Controlled Mobile 3D 
Multiplayer Gaming,” Proc. of the 2008 International 
Conference on Advances in Computer Entertainment 
Technology (ACE ’08), ACM Press, Dec. 2008, pp. 267-270, 
doi: 10.1145/1501750.1501813. 
[12] C. Barnum and L. Palmer, “More Than a Feeling: 
Understanding the Desirability Factor in User Experience,” 
Proc. of the 28th International Conference Extended Abstracts 
on Human Factors in Computing Systems (CHI EA ’10), Apr. 
2010, pp. 4703-4715, doi: 10.1145/1753846.1754217. 
[13] K. Crosett, "Mobile Game Marketing to Increase", 18.3.2011,  
Available: http://www.marketingforecast.com/archives/10608 
[14] Marketing Charts, "iPad, iPhone Users Trend Younger, 
Wealthier" 
2010, 
http://www.marketingcharts.com/direct/ 
ipad-iphone-users-trend-younger-wealthier-13778/nielsen-
ipad-user-profile-august-2010jpg. 
 
185
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

