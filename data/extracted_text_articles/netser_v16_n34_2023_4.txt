75
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Energy Consumption Minimization in Data Centers for Cloud-RAN
Line M. P. Larsen∗†, Simon Friis∗, Henrik L. Christiansen† and Sarah Ruepp∗
† Department of Mobile Innovation, TDC Net, Copenhagen, Denmark
∗ Department of Electrical and Photonics Engineering, Technical University of Denmark, Kgs. Lyngby, Denmark
Corresponding author: lmph@dtu.dk
Abstract—Data centers are to become a vital part of mobile
networks when exploring new architectures such as cloud radio
access network, which is seen as a more energy efficient alterna-
tive to today’s mobile network installations. However, the scale
of a data center affects its energy efficiency, with larger data
centers having more resources for energy-saving measures but
at the same time different challenges than those faced by data
centers of smaller scale. Hence, this work analyzes and compares
energy efficiency of small, medium, and large data centers, and
explores the energy minimization opportunities for cloud radio
access network data centers. When moving processing from the
radio access network to a data center to save energy in the radio
access network, the mobile network operator does not want to
move the energy consumption from the radio access network to
the data center. Hence, energy consumption must be reduced in
all segments of the network, and not moved to another segment.
In the case study provided, cloud radio access network data
centers are categorized as mid-scale and small-scale depending
on the size of area they cover due to strict latency requirements
of mobile network traffic flow. Thus, even though larger DCs
prove to be more energy efficient, other factors, which in the
case of mobile networks will be latency, will impact the size of
the data center. Hence, this is a major driver to consider how
energy consumption can be minimized in small data centers.
Keywords-Data center; cloud; C-RAN; green RAN; energy
efficiency.
I. INTRODUCTION
This work is an extended version of [1]. The Information
and Communications Technology (ICT) sector, including Data
Centers (DCs), communication networks and user devices, is
accounted for an estimated up to 6% of global electricity
use [2]. Thus, many initiatives aim at reducing the energy
consumption of the various different sub-parts of the ICT
sector. However, looking into solutions for reducing the energy
consumption in one part of the sector should not just move the
problem to another part of the sector. The solution must look at
the ICT sector from a holistic perspective. A starting point is to
trace the solutions and investigate what other implications they
will bring. Mobile networks, are widely used communication
infrastructure, and looking at their energy consumption, the
Radio Access Network (RAN) plays a significant role [3]. One
solution envisioned to reduce the energy consumption of the
RAN, is to utilize the long time discussed Cloud RAN (C-
RAN) architecture [4].
In C-RAN, the mobile network functions are divided into
three functional units; the Radio Unit (RU), Distributed Unit
Support from Innovation Fund Denmark, through grant no. 1045-00047B
and the Nordic University Hub on Industrial IoT, Nordforsk grant agreement
no. 86220, is gratefully acknowledged.
DU+CU
DU+CU
DU+CU
DU DC
DU DC
DU DC
CU DC
CU DC
CU DC
RU
RU
Figure 1. A traditional Radio Access Network (RAN) architecture [top] and
a Cloud-RAN (C-RAN) architecture where data processing is moved to Data
Centers (DCs) [bottom].
(DU) and Centralized Unit (CU). The RU, is located in the
antenna mast and contains part of the physical layer functions.
Whereas the DU and CU, containing the upper layer baseband
processing functions, can be virtualized and located in DCs.
A C-RAN installation and a traditional RAN installation are
compared in Fig. 1. The figure shows how all baseband
processing is located on site in the traditional RAN in top of
the figure, where baseband processing is divided into separate
DU and CU and moved to DCs in the C-RAN installation.
The concept is, to store processing from a number of sites in
the same Data Center (DC), where they can share physical
resources and exploit their different user movement patterns.
This being users gathering in different areas at different times
of day [4]. Hence, in order to make the RAN segment of
mobile networks more energy efficient, much of the processing
is moved into DCs.
A DC refers to a number of servers located in the same
building. Many different types of DCs exist and they are in
this work categorized into three different sizes ranging from
small, medium to large.
• Small DCs are categorized as having less than 1,000
servers, as well as less complex infrastructure, limited
storage and thus; consume less power compared to larger
DCs.
• Mid-scale DCs have a larger number of servers which
ranges between 1,000 to 10,000 [5] with more complex
infrastructure.
• Large DCs are defined as having more than 10,000

76
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
servers with an even more complex infrastructure [5].
Large DCs are typically used by large companies or
governments.
Energy efficiency in DCs is a crucial topic of modern DC
operations, as it can help to reduce energy costs and envi-
ronmental impact of the ICT sector. DCs are energy-intensive
facilities that consume a large amount of electricity to power
servers, storage systems and cooling equipment. The energy
consumption of DCs has become an increasing concern for the
industry, as well as businesses and organizations that operate
these facilities, as DCs are responsible for approximately 1%
of global electricity demand [2].
This paper investigates methods for energy efficiency in
DCs, with a focus on state-of-the-art technologies and tech-
niques, as well as how and why these are beneficial for
DCs of different scale. Furthermore, it is investigated how
DCs, handling mobile network data processing in the C-RAN
architecture, can become energy optimized. Section II presents
other related papers, research projects and features our contri-
bution to the topic. Section III explains the typical components
in a DC, to be used in section IV, which goes in-depth with
modern and commonly used strategies for minimizing DC
energy consumption. Section V elaborates on the use of DCs
in C-RAN mobile networks, while section VI discusses what
and why some methods are most commonly used in DCs of
certain sizes and their potential. Finally, the conclusion closes
this paper. A list of acronyms is provided after the conclusion.
II. STATE OF THE ART
This study combines two directions of energy efficiency
studies, namely DCs and mobile networks. There have been
several studies and research conducted on energy efficiency
in DCs in recent years. However, numerous surveys regarding
energy efficiency in DCs tend to be older than 5 years, such as
the work in [6], which presents an overview of energy-aware
resource management approaches with focus on basic archi-
tecture of cloud DCs and virtualization technology. The survey
in [7] investigates the green energy aware power management
problem for Megawatt-scale DCs and classifies work that
considers renewable energy and/or carbon emission. In [8],
authors discuss several state-of-the-art resource management
techniques, that claim significant improvement in the energy
efficiency and performance of ICT equipment and large-scale
computing systems, such as DCs. The work in [9] conduct
an in-depth study of the existing literature on DC power
modeling, covering more than 200 models. The concept of C-
RAN was first mentioned by companies IBM [10] and China
Mobile [11] and later explored in numerous surveys including
[4], [12] and [13]. However, the following section of related
work will focus on research conducted within the latest years.
A. Related work
Recent related work in the field of greening DCs includes
[14], where the approaches moving towards green computing
are investigated and categorized to help researchers and spe-
cialists within cloud computing expand green cloud computing
TABLE I. DATA CENTER REFERENCES BY SIZE
DC Size
References
Large scale
[6] [7] [8] [9] [14] [16] [17] [18] [19] [20]
[21] [22] [23] [24] [25] [26] [27]
Mid scale
[9] [14] [16] [18] [21] [22] [23] [24] [25]
[27] [28]
Small scale
[14] [16] [18] [21] [22] [23] [24] [25] [27]
[28] [29] [30] [31]
and improve the environment quality. The work in [15],
gives a brief overview of the state-of-the-art in green cloud
computing. Existing research in the area is examined and
categorized into different themes. Furthermore, challenges and
opportunities in the field are discussed to provide insights into
future directions for research. The paper provides valuable
background information and a significant understanding of the
current landscape of green cloud computing. In the survey
[16], the authors discuss different mechanisms for lowering the
power utilization in DCs. The survey provides in-depth details
about the various mechanisms that can be employed at the
hardware level so that the utilization of energy by component
can be reduced. Techniques that can be applied at network,
cluster of servers’ level along with the various dynamic power
management measures that can be employed at the hardware
or firmware level and can lead to energy efficient or green DCs
are also studied in detail [16]. Table I lists relevant research in
the field of energy improved DCs categorized into relevance
regarding the different DC sizes.
C-RAN has recently been surveyed in relation to energy
consumption improvements in [3], [32], [33], [34]. Which
all highlights the energy saving potential of shutting down
equipment not in use. Hence, in DCs, some equipment can
be shut down in low traffic periods such as during the night.
Another benefit of C-RAN is the opportunity to assign extra
capacity where it is needed. In the perspective of users being
in residential areas in the morning and evening, and goes to
work areas during the day [4]. Thus, current trends in C-RAN
research point in the directions of: Load consolidation [35],
[36], [37]; coordinated transmission [38], [39]; and with a
focus on the transport network [40], [41].
The work in [42] acknowledges the problem of low server
utilization when mobile network traffic is moved to DCs, and
proposes a framework for a higher utilization of the physical
machines. In [43], various methods for DC load balancing was
surveyed and compared not only for resource utilization, but
also in other parameters such as power usage and reaction
time.
The authors of this work have previously addressed the
topic of energy minimization of DCs in [1]. Which examined
various methods for minimizing DC energy consumption in
various sizes of DCs. To the best of our knowledge the
combination of C-RAN and energy efficient DCs has not yet
been explored.

77
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
2025
2024
2023
2022
2021
2020
2019
2018
2017
2016
2015
GREENDC
Data Centers
CATALYST
iCIRRUS
Cloud RAN
SOOGREEN
AINET
DATAZERO
DATAZERO2
SeDuCe
CloudRadioNet
5G-COMPLETE
HUB4CLOUD
Figure 2. A timeline overview of various research projects within energy efficient Data Centers (DCs) and Cloud-Radio Access Networks (C-RAN).
B. Related research projects
DC energy consumption is a topic that receives broad atten-
tion. The European Commission has created the group ”Data
Centres Code of Conduct”, to reduce DC energy consumption
[44]. In the United States (US), Berkely Lab has a ”Center of
Expertise for Energy Efficiency in Data Centers” [45]. Some
of the recent past and ongoing research projects in DC energy
efficiency are listed below, and summarized in Fig. 2.
• The ”Greening Datacenters”, GREENDC project, is a
recently finalized project where knowledge of DCs opera-
tions was transferred from industry to academic partners,
where simulation based optimization for best practice of
energy demand control was examined [46].
• The ”Converting DCs in Energy Flexibility Ecosystems”,
CATALYST project, investigated how to combine existing
and new DCs into flexible multi-energy hubs [47].
• The AINET project focuses on edge DCs to provide
enablers and solutions for high-performance services by
the use of Artificial Intelligence (AI), which also includes
methods for minimizing energy consumption [48].
• The ”DATAcenter with Zero Emission and RObust
management using renewable energy”, DATAZERO and
DATAZERO2 projects, investigate how a DC only oper-
ated by renewable energies alone can work. Where the
focus in DATAZERO2 is the operation and design of
cooperating DCs [49].
• The ”Sustainable Data Centers: Bring Sun, Wind and
Cloud Back Together”, SeDuCe project, aimed to design
an experimental infrastructure dedicated to the study of
DCs with low energy footprint. Resulting in a testbed for
research on thermal and power management in DCs [50].
• The ”The European Cloud Computing Hub to grow a sus-
tainable and comprehensive ecosystem”, HUB4CLOUD
project, aimed to magnify the impact and relevance of
cloud computing research, innovation, and policy-driven
efforts in Europe with the ambition of environmentally
sustainable cloud technologies and solutions [51].
For the sake of C-RAN, some of the recent past and ongoing
research projects are listed below, and summarized in Fig. 2.
• The ”Service-oriented optimization of Green mobile net-
works”, SooGreen project, investigated how to reduce
the energy consumption of services in light of the traffic
evolution and exploit new network architectures including
hybrid C-RAN [52].
• The ”intelligent Converged network consolIdating Radio
and optical access aRound USer equipment”, iCIRRUS
project, proposed an intelligent C-RAN solution bringing
together optical fibre technology, low-cost but highly
flexible Ethernet networking and wireless resource man-
agement [53].
• The ”Cloud Wireless Networks: An Information The-
oretic Framework”, CloudRadioNet project, aimed at
developing novel information theoretic concepts and tech-
niques and their usage, to identify the ultimate communi-
cations limits and potential of different C-RAN structures
[54].
• The ”Computational and stOrage resource Management
framework targeting end-to-end Performance optimiza-
tion for secure 5G muLti-tEchnology and multi-Tenancy
Environments”, 5G-COMPLETE project, utilizes the C-
RAN architecture to build an unified ultra-high capacity
converged digital/analog fiber-wireless transport network
for the RAN [55].
C. Our contribution
This current work explores numerous strategies for improv-
ing energy efficiency in DCs, while taking the different sizes
of DCs into consideration. Furthermore, we examine the size
of DCs to be used for C-RAN, to explore how these can
become the most energy efficient, which is a major additional
contribution of this paper compared to [1], as well as the
extended overview of related work. Thus, we provide an in-
depth overview of key challenges, opportunities and methods
for improving energy efficiency in all types of DCs, but with
a major focus area in DCs for C-RAN. Hence, we:
• Provide insights into effective ways to improve energy
efficiency in DCs by synthesizing the state-of-the-art
technologies and techniques for DCs of different sizes.

78
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
• Analyze previous surveys and papers on energy efficiency
in DCs, and provide an overview of current research
projects.
• Offer engineering guidelines for DCs in C-RAN mobile
networks.
• Examine recommendations towards achieving minimized
energy consumption in DCs for C-RAN.
III. DATA CENTER BASICS
This section introduces various types and components of a
typical DC, before exploring the energy minimization oppor-
tunities in the next chapter. Two types of DCs include private
enterprise DCs and public cloud DCs. As illustrated in Fig. 3,
the end-users gain access to the DCs to store and process data
through a network of computers, wireless Access Points (APs),
switches/routers and the Internet. The computers are connected
to both DCs through a switch or router, which directs the data
traffic between them.
An enterprise DC is located inside the same local network
as the users, while a cloud DC is located outside of the local
network. Cloud DCs are typically managed and owned by
third-party service providers and the services they provide are
accessed through the Internet. Users can access both types of
DCs by authenticating themselves and then proceed to transfer
data through the nodes in the network. Fig. 3 gives an overview
of a basic network architecture where users have access to both
an enterprise DC and a cloud DC. The benefits of connecting
to both a cloud DC and an enterprise DC for data access and
exchange are:
• The cloud DC enables remote, on-demand access to data
and application services from other providers through the
Internet.
• The enterprise DC provides more secure, local access to
other types of data and applications, which is beneficial
for vulnerable data.
The architecture of a DC plays a crucial role in its overall
energy efficiency. Several components make up a typical DC
architecture, including [18]:
• Server Racks: The servers themselves, as well as the
physical stations that house the servers in a DC and
consume energy for processing and cooling. Server racks
are designed to organize, store and manage numerous
servers, while optimizing floor space at the same time.
• Top of the Rack (ToR) Switches: Switches connected to
every server in a server rack and connects those to the
network. A ToR switch can be located at the top of each
server rack to provide the connection between the servers
and the network. They are responsible for forwarding data
packets between servers and the rest of the network. This
is however; depending on the chosen DC architecture
[56].
• Aggregation Switches: A centralized connection point for
assigned ToR switches. Responsible for collecting data
traffic from multiple servers and forward it.
• Load Balancers: Devices responsible for distributing net-
work traffic evenly across several servers, reducing the
PC
PC
Switch
Router
Wireless 
AP
The Internet
Cloud Data Center
Enterprise Data Center
Figure 3. A basic network with computers connecting to both a cloud Data
Center (DC) and an enterprise DC.
probability of network failures by lowering workload of
overwhelmed servers.
• Access Routers: A secure connection point for external
network traffic.
• Core Switches/Routers: Devices responsible for forward-
ing traffic at a high speed within nodes of a DC network.
• Edge Routers: Handles incoming and outgoing network
traffic by routing data from and to the DC.
The various components cooperate to distribute, forward and
transmit data traffic stored in a DC, and understanding the
purpose and role of each device is key when optimizing energy
efficiency in DCs. Fig. 4 illustrates the basic elements of a
DC architecture, designed to efficiently process and manage
data. It includes server racks connected to ToR switches which
forwards traffic to the aggregation switch. The data is then
directed to the load balancer which distributes the traffic
between the servers. The access router controls access to the
DC network and the core switch/router forwards to the edge
routers which serve as the bridge between the internal DC
network and the external network, being the Internet.
Table II provides an overview of the various DC components
and their appearance in the different DC sizes. Furthermore,
the table provides an overview of the energy consumption
of the various elements using numbers from [56]. However,
comparing these numbers to the work in [57], then the
energy consumption of servers, storage and communications
equipment only account for approximately 50% of the total
DC energy consumption. The cooling systems require 40%
and the power supply system require the remaining 10% [57].
Thus, in light of the total DC energy consumption the servers
will consume 35%, aggregation switches 5%, access routers
7.5% and core switches 2.5%. The break down of energy
consumption figures are illustrated in Fig. 5.

79
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
TABLE II. DATA CENTER COMPONENTS BY SIZE
Component
Small scale
Mid scale
Large scale
Energy consumption
Servers
<1,000
1,000-10,000
10,000<
70% [56]
ToR switches
<50 [58]
50-500
500<
Expected to be part of the aggregation
switch energy consumption
Aggregation switches
<25 [59]
25-250
250<
10% [56]
Access routers
<25 [60]
25-250
250<
15% [56]
Core switches
<12 [59]
12-120
120<
5% [56]
Server 
Rack
Server 
Rack
ToR Switch
ToR Switch
Core Switch/Router
Aggregation Switch
Edge Router
The Internet
Load Balancer
Server 
Rack
Server 
Rack
ToR Switch
ToR Switch
Core Switch/Router
Aggregation Switch
Load Balancer
Access Router
Access Router
ToR Switch
ToR Switch
Aggregation Switch
Load Balancer
ToR Switch
ToR Switch
Aggregation Switch
Load Balancer
Access Router
Access Router
Core Switch/Router
Core Switch/Router
Core
L3
L2
Figure 4. Key components of a basic data center architecture separated into
different layers.
Figure 5. Data Center (DC) key components’ energy consumption in relation
to the total DC energy consumption.
IV. ENERGY MINIMIZATION METHODS
Energy minimization methods refer to the numerous strate-
gies used to reduce the energy consumption of DCs with the
goal of minimizing energy consumption while maintaining
high levels of performance and reliability. Server utilization
in DCs are found to be under 20% most of the time and with
the servers still running fully, this results in very low energy
efficiency since servers still consume a significant amount of
energy even when not fully utilized [21]. A common tool
for measuring energy efficiency in DCs is the Power Usage
Effiectiveness (PUE) metric. It is calculated by dividing the
total amount of energy used by a DC, including all systems and
components, by the energy used by the IT equipment within
the DC [61].
This section will give a brief overview of multiple tech-
nologies and techniques as well as going in-depth with some
subcategories of these strategies, being: sleep state methods
and resource utilization in their own subsections. Fig. 6
illustrates where in a DC certain methods are utilized and what
components are involved by highlighting the energy efficiency
strategies with different colors:
• Green for load balancing and scheduling
• Blue for cooling systems optimization
• Yellow for sleep state methods
• Pink for Data Center Infrastructure Management (DCIM)
tools
Figure 6 can be used as an overview of in which compo-
nents the various energy consumption minimization strategies
belong. The strategies will be further elaborated below.
A. Trending methodologies
Energy efficiency in DCs can be achieved through a variety
of strategies. Examples of current research directions are:
• Advanced cooling systems
• Server virtualization
• DCIM tools
• Edge computing
• AI-driven DC Management
• Quantum computing
Advanced cooling systems are innovative technologies used
for mainly cooling servers and can result in notable energy
savings. Liquid cooling, free cooling and indirect cooling
are some of the advanced types of cooling systems [30].
However, opportunities to place DCs underwater are also being
investigated [62]. Another relevant method in this category is

80
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Server 
Rack
Server 
Rack
ToR Switch
ToR Switch
Core Switch/Router
Aggregation Switch
Edge Router
The Internet
Load Balancer
Server 
Rack
Server 
Rack
Server 
Rack
Server 
Rack
ToR Switch
ToR Switch
Core Switch/Router
Aggregation Switch
Load Balancer
Access Router
Access Router
Load Balancer
Server 
Rack
Server 
Rack
ToR Switch
ToR Switch
Core Switch/Router
Aggregation Switch
Access Router
Core Switch/Router
Aggregation Switch
Access Router
Load Balancer
Edge Router
Figure 6. Energy minimization strategies highlighted in color for involved key
components of a Data Center (DC). Hence, some of the components benefit
from more than one strategy and thus; they are represented in different colors.
The green colored components benefits from load balancing and scheduling.
The dark blue components benefit from cooling systems optimization. The
yellow components benefit from sleep state methods. The pink components
benefit from DC Infrastructure Management (DCIM) tools.
heat re-use, which refers to the process of utilizing waste heat
generated from one process or system and using it for another
purpose, rather than letting it go to waste [3]. Such purposes
could be to heat up greenhouses in cold regions [63] [64]
[65], houses or whole cities [64] [66], also swimming pools
and even laundries can make use of the heat generated in DCs
[64].
Server virtualization can lower the number of needed
servers in a DC by running multiple virtual servers on a single
physical server, resulting in lower power consumption [22].
The technique is an enabler of methods to improve resource
utilization, which will be examined later in this chapter
[43]. Servers can be virtualized by realizing the functions
in software by the use of either Virtual Machine (VM) or
container systems. Thus, they need an integrator between the
hardware and software, which can be a hypervisor for VMs
or a container engine for containers.
DCIM tools are used to monitor, measure, manage and/or
control DC utilization and energy consumption of DC
equipment such as servers, storage systems and network
switches/routers. This helps identify power-related issues and
improve DC performance and energy efficiency [67].
Edge computing refers to a range of networks and devices
at or near the users and enables processing data closer to
where it is being generated. Edge computing can reduce the
amount of data traffic that needs to be transmitted to a central
DC, resulting in potential energy savings [23]. However, this
represents a trade-off between centralization benefits for larger
DCs and energy savings in the transport infrastructure. Hence,
more centralized traffic in one DC will open up for more
opportunities for sharing existing resources, but the cost is a
comprehensive transport network, which is examined in [41],
[68]. Edge computing however, might prove to be most bene-
ficial for smaller DCs. Large DCs are much more centralized
and have a much greater power density which counteracts the
whole principle of edge computing. However, egde computing
results in more and smaller DCs rather than one or a few large
DCs and thus, if considering the amount of data processed,
then the potential for higher resource utilization increases by
the number of servers in the DC. Hence, more traffic will bring
a larger potential for utilizing the servers at different times of
the day. On the other hand, not all traffic can be transported
to distant DCs, including time critical operations for mobile
networks.
AI-driven DC Management is a method for automating
control and monitoring of DC resources. By improving DC
operations, energy efficiency improves as well [69].
Quantum computing defines super powerful computers that
uses quantum technology to create a 3D reference model
that significantly increases the computational performance
compared to a normal computer. Quantum computing has
the potential to increase energy efficiency in DCs by solving
complex problems at an incredible speed compared to tradi-
tional computing methods. However, it is a new technology
and still in its early stages [70]. Furthermore, the quantum
computing requires extremely low temperatures and thus; the
energy consumption of the cooling system is expected to be
higher than the energy consumption of the computers [71].
B. Sleep states
Sleep states can be implemented to shut down several server
components for a short period of time to reduce energy wasted
on un-used server capacity. Fig. 6 illustrates the components in
a DC that can be impacted by sleep state methods, being both
switches, servers and routers at various levels. When utilizing
sleep state methods, the components that are being powered
down are the Central Processing Unit (CPU), cores of the
CPU, memory and storage devices [21]. The devices and nodes
that are involved when utilizing this method are highlighted in
Fig. 6, marked by yellow. Modern processors support multiple
types of sleep states, primarily:
• Core C-states
• Package C-states
• P-states
• Dynamic Random-Access Memory (DRAM) power mode
Core C-states work by stopping executions on the core.
They range from C1-C6 and the differences between those
being the varied amounts of power savings and exit latency
costs, which will here be referred to as wake up time. C0 is the
active state, with no CPU power savings. C1 is the state with
the least power savings but with the shortest wake up time
whereas C6 is having the longest wake up time at a 133µs
transition time [21], however; this number is depending on

81
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
CPU power savings
Wake up time
C0/ active state
C1
C2
C3
C4
C5
C6
Figure 7. Illustration of the 6 Core C-states according to wake up time and
opportunities for power savings.
the protocol used. The core c-states and their relation between
wake up time and CPU power savings are illustrated in Fig.
7.
Package C-states are used when all cores are in state C1 to
C6, hence; the entire CPU is idle. In this state a whole package
of components turns off, such as shared caches, integrated
Peripheral Component Interconnect Express (PCIe), memory
controllers, and so on [24]. However, the concept is that
additional power is saved compared to the power saved with
the sub-components individually [24]. Package C-states can
significantly reduce energy consumption but has the side effect
of increasing the latency for cores going to or from low power
states [21]. Furthermore, package C-states can be problematic
because of high response times during re-activation when han-
dling traffic spikes. Additionally, having the memory and/or
storage of all servers to be available, even during times of light
load, can be very beneficial. Lower latency can be achieved
by AgileWatts (AW) [21] which is a deep idle core power-
state architecture that reduces the transition latency to/from
very low power states. AW has been proven to result in up to
71% power savings per core with a less than 1% end-to-end
performance decrease [21].
P-states changes the frequency and voltage of a part of the
system. This being the cores or other components such as a
shared Layer 3, the network layer (L3) cache [24]. P-state
is a module state affecting a collection of cores that share
resources [24]. The concept of P-states is that a CPU running
at lower frequencies requires lower performance and longer
latency to complete a certain amount of work. Thus, under
some circumstances, for example in low traffic periods, it is
possible to complete a required amount of work with lower
energy [24].
The DRAM power mode consists of two power-saving
methods which are the Self-refresh function and the Clock
Enable (CKE) mode. CKE sends a signal from the Memory-
Controller (MC) to the DRAM device, and when this signal is
no longer being sent, the DRAM is free to enter a low power
state. In the Self-refresh function, the MC sends the refresh
signal to the DRAM to ensure that the data is valid. DRAM
has the ability to start the Self-refresh process itself, which
can reduce the power consumption in the MC [72].
C. Resource utilization
DCs’ load rises when more requests are received, and
these requests can be received seasonally. Thus, the workload
demands of the servers are changing dynamically and are
determined by a real-time workload status. By balancing the
load on the servers carefully and properly, it is possible
to increase the energy efficiency of components in a DC.
Fig. 6 illustrates resource utilization techniques within a DC,
highlighted as green. This work examines two different ways
of increasing the resource utilization:
• Load balancing
• Scheduling
Load balancing can be explored using various methods.
Dynamic Time Scale based Server Provisioning (DTSP) is a
method which takes the variability of workloads into consid-
eration when providing servers for workload demands. For
DTSP to load balance properly, key information is gathered
constantly so that DTSP can accurately estimate workload
requirements on servers and specify the appropriate number
of servers for the dynamic workloads [19]. Irregular arrivals
of requests impact the accuracy of the expected workload.
To increase the estimation, the gathered information of in-
coming requests is standardized before it is used in later
calculations. When it comes to workload, the algorithm looks
at the three factors; arrival rate of previous requests, the
arrival rate of current requests and the mean service time
of current requests. With these factors, the algorithm is able
to figure out the intensity of previous workloads and reflect
the available remaining capacity for the unfinished waiting
workloads, as well as measure the intensity and time needed
for current workloads to complete. These factors are also
used when calculating the workload demand of incoming
requests and to determine how many servers are needed
to finish current and remaining workloads while satisfying
the Quality of Service (QoS) requirements [25]. DTSP has
been proven to be able to estimate the workload demands of
servers in a DC. By periodically adjusting service resources
to match workload demands, DTSP significantly improves and
maintains the system energy efficiency under an acceptable
QoS level [19]. The work in [43] explores five different load
balancing methods, the dynamic, predictive, energy-conscious
application scaling, energy efficient and generic algorithm
with population reduction. Results show that each of the
investigated methods have individual pros and cons when
evaluating them based on various parameters.
Scheduling can be used to prioritize which machines that
run what jobs or are higher utilized. A cloud system uses
virtualization technology to provide cloud resources such as
CPU and memory to users in the form of virtual machines.

82
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Tasks and job requests are assigned on these VMs for exe-
cution. The technique known as job scheduling is a method
used to assign a job to a VM based on classification. By
allocating jobs based on types and availability, it is possible
to increase energy efficiency by making better use of avail-
able resources. Minimizing the number of hosts used when
allocating resources reduces energy consumption. The Energy
Aware VM Available Time (EAVMAT) scheduling algorithm
does exactly this [26]. By categorizing jobs into three types
and then assigning jobs based on a predefined policy with
the earliest available resource. Energy consumption is then
reduced since less hosts are in an active state and resource
utilization is higher. This method has been tested and was
able to achieve up to 46% energy savings [26].
V. DATA CENTERS FOR CLOUD-RAN
The C-RAN mobile network architecture centralizes the
baseband processing of a number of sites in DCs, as illustrated
in Fig. 1. Hence, for each RU the associated baseband process-
ing is divided into DU and CU functions, which can remain on
the cell site or be centralized in a DC. The potential maximum
number of sites associated with one DC, will depend on several
factors, being:
• Cell density in covered area
• Type of installations in covered area
• Latency limit in transport network
• DC efficiency
The cell density is an important parameter when estimating
the amount of DUs and CUs in an area, and it requires
knowledge about the Inter-Site Distance (ISD) in the current
area. Hence, the ISD in an urban area is expected to be much
shorter than the ISD in a rural area, due to the higher capacity
requirement and more obstacles. More sites are equal to more
installations and thus; more CUs and DUs.
The type of installations will vary based on the specific area.
In an urban area more equipment is installed compared to a
rural area due to the higher capacity requirement.
The transport latency limit is set by the requirements of the
current network segment. The network segment connecting the
RU to the DU is referred to as the fronthaul network, where the
network segment connecting the DU to the CU is referred to
as the midhaul network. The functions located respectively in
the DU and CU, referred to as the high layer functional split
or High Layer Split (HLS), is already standardized by 3rd
Generation Partnership Project (3GPP) in [73]. However, the
low layer functional split or Low Layer Split (LLS), separating
the functions of the RU and DU is still a discussion topic
amongst various industry alliances and standardization bodies
[74].
The efficiency of the DC is also a parameter, since more
efficient equipment can handle higher traffic loads, and on the
contrary, if the capacity of the DC is not enough, it might
not be able to handle traffic from all of its potential coverage
area. Hence, if it is a private DC it will require more effort to
upscale than using a public cloud solution where capacity is
rented on demand. This is also a more difficult parameter to
evaluate since it will vary based on vendor capabilities, and
because this is an area in continuous development.
A. Engineering guidelines
In the light of the implementation factors mentioned, then
in order to determine the size of a DC for C-RAN, the Mobile
Network Operator (MNO) must be mindful about several
conditions:
• On-site installation
• Number of basebands or DU and CUs in the current area
• Server efficiency
• Size of area to deploy C-RAN
The on-site installation defines what equipment in terms of
RU, DU and CU are installed at the cell site. The type of RU is
defined by the LLS used. Following 3GPP recommendations,
for the HLS and a variety of LLSs, including the one used in
today’s installations; then the fronthaul transport delay must
be < 250 µs [73] and the midhaul transport delay must be
< 10 ms [73]. Thus, assuming a fiber propagation delay of
10 µs/km [3], then the maximum distance from the farthest
site position and RU is 25 km to the associated DU DC and
up to 1000 km to the associated CU DC [3]. These distances
provides an approximation of how large an area a DU DC and
a CU DC can cover, by assuming the DC covers a circular
area with the maximum distance as the radius. Hence, this is
corresponding to 1900 km2 for the DU DC, which is the size
of the Hawaiian island of Maui. On the other hand, the CU
DC can cover more than 3 million km2, which is larger than
the country of Argentina, or approximately 1/3 of Europe. A
MNO has three potential placement scenarios if they want to
centralize their processing in DCs:
• Scenario A: To leave the DU on the cell site and move
CU functions to a DC.
• Scenario B: To centralize the DU functions in one DC
closer to cell sites and centralize CU functions from
multiple DU DCs in one CU DC.
• Scenario C: To centralize both DU and CU in the same
DC.
The three scenarios are illustrated in Fig. 8, where the
various latency requirements are stated too. Hence, if the MNO
wants to install CU functions in the same DC as the DU, then
the DU transport latency requirements must be met.
The number of basebands will determine the size of the
DC. This number is depending on the type of area, since
an area with higher population or frequent visits by many
people, like a huge train station or a concert hall, require more
equipment and capacity. Thus, since more capacity can be
added to an area by deploying more sites, then areas requiring
high capacity will have a shorter ISD compared to areas with
lower capacity requirements.
The server efficiency is difficult to measure and is an area
in continuous development. In order to be able to compare the
efficiency of a COTS server and a proprietary baseband instal-
lation, the performance must be measured. The performance
can be quantified by examining the number of jobs executed in

83
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
DU + CU DC
DU DC
RU
DU DC
CU DC
RU
B
C
<25 km
<1000 km
<25 km
CU DC
RU
A
<1000 km
DU
Figure 8. Three Cloud-Radio Access Network (C-RAN) installation options:
Distributed Unit (DU) on the cell site and Centralized Unit (CU) centralized
in a Data Center (DC) [A], DU and CU in different DCs [B] and in the
bottom, DU and CU in the same DC [C].
Figure 9. The figure shows the capacity in terms of required servers as a
function of the latency required by the fronthaul network.
a certain time interval and the maximum load of one unit. In
order to make an estimate for this work it is expected that one
onsite DU+CU will correspond to one server, with the traffic
distribution 2/3 to DU and 1/3 to CU. This is a topic that
leaves room for further investigations, because how efficient is
a Commercial off the Shelf (COTS) server actually compared
to a proprietary baseband? However, if the efficiency of the
proprietary baseband and the server(s) running the DU and
CU functions are not 1:1 efficient then the number of COTS
servers required might be less or (more likely) more than
the proprietary installations in the traditional RAN. Hence,
the server efficiency will affect the number of servers in the
DC. The relationship between latency and capacity in terms
of required servers in the DC is explored in Fig. 9. The figure
shows the required number of servers for various ISDs when
complying with different requirements to fronthaul latency.
The size of the area determines how many DCs are required
to cover the current area in order to comply with the RAN
latency requirements. Furthermore, the placement scenario
selected will also determine the need for multiple smaller or
one larger DC.
B. Case study
This case study, investigates the number and sizes of DCs
required for the Danish MNO TDC Net [75] to convert to a
C-RAN installation in their mobile network. Thus, real life
numbers and approximations are used to evaluate the sizes
of required DCs. TDC Net provides 99% geographical 5G
coverage in the country of Denmark and utilizes approxi-
mately 4000 sites. The country of Denmark can be seen in
Fig. 10. The country of Denmark is approximately 43,000
km2 and thus; only one DC can according to the transport
latency requirements carry all CU data of the whole country.
According to transport latency requirements, then at least 23
DCs are necessary to handle the DU traffic. In this case we
assume 23+ DU DCs since more might be required due to
practical implementation specifics. In order to explore the C-
RAN DC opportunities for TDC Net two areas with a radius
of approximately 25 km are selected. One urban area with
high traffic loads and many users present at all times of the
day, and one rural area with the complete opposite capabilities.
Table III summarizes the parameters of the urban area, rural
area and the whole country, utilizing the parameters stated in
the engineering guidelines. As stated in the table, the capacity
illustrated by utilized spectrum in the current area, differs a lot
in the different areas. The capacity here includes both Long
Term Evolution (LTE) and New Radio (NR) cells. The rural
area chosen has a lower average capacity compared to the
average capacity in the whole country, where the urban area
used here have a much higher average capacity compared to
the average capacity for the whole country. In the following
subsections, scenarios A, B and C presented under engineering
guidelines, will be explored in the light of the case study.
1) Scenario A: Only one CU DC, is necessary for covering
all of Denmark’s CU traffic. This DC shall be able to handle
upper layer traffic from all 4100 sites covering the whole
country, with a total of 7100 basebands. Thus, expecting 1:1
performance of current installations and DC servers, with 1/3
traffic handled in the CU as described under DC efficiency in
chapter V. Then 2400 servers will be required to handle CU
traffic, corresponding to a mid-scale DC. Data is summarized
in Table IV.
2) Scenario B: 23+ DCs are nescessary to handle all DU
traffic. The DU DCs’ sizes will depend on the cell density and
installation types of the current area. Thus, two areas will be
considered, a rural and an urban area in Denmark. The areas
are compared in Table V considering an urban area with a
total of 900 sites (including macro, pico and indoor systems)
and a rural area covered by 50 sites, both areas covering
approximately 25 km from the area center. When expecting

84
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
TABLE III. AREA SPECIFICS
Parameter
Urban area
Rural area
Denmark
Type of RU
LLS8
LLS8
LLS8
Fronthaul latency limit
< 250 µs
< 250 µs
< 250 µs
Total macro sites in area
900
50
4100
Total basebands in area
1400
100
7100
Average capacity per site
109 MHz
77 MHz
83 MHz
Estimated server efficiency
1:1
1:1
1:1
Approximated size of area
1900 km2
1900 km2
43,000 km2
~400 km
~400 km
Figure 10. The country of Denmark. The image is a creative common under
license CC BY-SA.
TABLE IV. CU DC
Parameter
Denmark
Assumed traffic distribution
1/3
Servers in DC
2400
1:1 performance of current installations and DC servers, and
DU traffic corresponding to 2/3, then 940 servers will be
required to handle DU traffic in the urban area, corresponding
to a small-scale DC. For the rural case, only 60 servers will
be necessary to handle DU traffic, corresponding to a minor
small-scale DC. Data is summarized in Table IV.
3) Scenario C: If the DU and CU is both located in the
same DC, then the covered area will be limited by the latency
boundaries of the DU, and thus; 23+ sites are necessary to
cover the whole country of Denmark. Table VI summarizes
the required size of DC in a rural and an urban area. As the
table shows, then the rural area is still covered by a small-
scale DC. However, the urban area DC becomes a mid-scale
DC with 1400 servers required to handle DU and CU traffic.
TABLE V. DU DC EXAMPLES
Parameter
Urban
Rural
Assumed traffic distribution
2/3
2/3
Servers in DC
940
60
TABLE VI. DU AND CU DC EXAMPLES
Parameter
Urban
Rural
Assumed traffic distribution
1/1
1/1
Servers in DC
1400
100
VI. DISCUSSION
DCs being responsible for approximately 1.5% of global
carbon emission with an annual growth rate of 4.3% have
become an area of focus within the last decade [28]. However,
a lot of attention has been directed towards the larger DCs,
which are only responsible for a small portion of the overall
energy consumption of DCs in general, since small-/mid-
scale sized DCs are responsible for approximately 50% of
the energy consumption [28]. Due to the increased attention,
large-scale DCs have therefore advanced more than small-scale
DCs and have numerous energy efficient methods implemented
already. It is shown in [28], that energy efficient strategies such
as virtualization are adopted less in smaller DCs compared to
large DCs. Small DCs are in general behind on the energy
efficiency front with around 43% of them not having energy
efficiency objectives in place at all [28]. When energy opti-
mizing the mobile networks, the baseband processing of the
RAN is moved into DCs categorized as small-or mid-scale.
Thus, if the energy usage should not just be passed on to the
next segment of the network, ie. the DC, it is important to
consider methods for minimizing the energy consumption in
DCs for C-RAN as well.
The benefits of different strategies used for energy efficiency
in DCs varies depending on the size of the DC. Below
is recommended a set of guidelines for optimizing energy
efficiency in DCs and evaluated based on the three different
sizes/categories; small-, mid- and large-scale. However, it is
important to stress that techniques and technologies recom-
mended for small-scale DCs are also excellent methods for
larger DCs, whereas methods recommended for large-scale
DCs are not always realistic/beneficial options for smaller

85
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
DC Size
Energy Efficiency Potential in %
Virtualization
75%
50%
25%
0%
Power management
Load Balancing & Scheduling
Sleep State/ EE Hardware
Cooling
Highest energy saving potential per strategy
Quantum Computing
AI-Driven DC Management
Small
Mid
Large
Figure 11. The graph compares the energy efficiency improvement percentages
achieved through different energy efficiency strategies in DCs of different
scale.
DCs because of price and other circumstances such as the
scale. On the other hand, small-scale DCs might see greater
improvements when utilizing some of these strategies, since
they are size-wise easier to manage, which can result in
energy-efficient technologies and practices being adopted more
easily. Large DCs managing thousands of servers and hundreds
of server racks will likely achieve greater power savings by
investing in advanced cooling systems than small-scale DCs
managing less than hundred servers. Here, small-scale setups
might see greater benefits investing in other technologies and
techniques as described in the next subsection.
A. DC Strategies at Different Scale
Many different factors are decisive for how effective certain
strategies are when it comes to the energy efficiency for
DCs of various sizes. This makes it difficult to generalize
the different methods as all DCs differ in relation to in-
frastructure, scale and utilization, environmental factors and
what energy efficient technologies are already in place. Some
energy efficiency strategies can provide the best results for
smaller DCs compared to larger DCs, since small-scale DCs
have fewer resources available as well as generally not even
having implemented any energy efficiency strategies at all
[28]. Fig. 11 shows potential power savings of different
strategies for varying DC sizes. Furthermore, below various
opportunities for minimizing energy consumption in DCs of
different scale are examined:
Small-scale and mid-scale DCs benefit from energy ef-
ficiency strategies such as sleep state methods and power
management tools, as well as virtualization, load balancing
and energy efficient hardware. Additionally, small-scale DCs
can also benefit from design optimization including efficient
cooling systems and energy efficient infrastructure.
Large-scale DCs have access to more resources and can
allocate those towards many different energy-saving measures,
including advanced cooling systems, server virtualization, load
balancing as well as renewable energy sources. Having access
to additional resources opens up for other strategies such as
AI-driven DC management and quantum computing as large-
scale DCs also have more data traffic to handle. Modern energy
efficiency strategies such as advanced cooling systems have
proven to potentially achieve energy saving of up to 50%
[30], virtualization has proven possibilities of 30% [22], sleep
state methods can provide up to 34% energy savings [27], and
resource utilization methods can reduce energy consumption
by up to 46% [26]. All these strategies are beneficial for DCs
of all sizes but can vary in potential energy savings depending
on multiple different factors. DCIM and PUE are also excellent
methods for working towards more energy efficient DCs and
can provide beneficial tools for analysing DCs of all sizes.
That being said, as well as being able to utilize and implement
the technologies and techniques mentioned for smaller DCs,
large-scale DCs does also have other possible methods for
achieving greater energy efficiency. AI driven DC management
and quantum computing are both methods which will most
commonly be seen in large-scale DCs since the owners are
able to provide sufficient resources for these technologies to
be implemented and these methods are therefore recommended
for large-scale DCs, along the methods mentioned for smaller
DCs.
Numbers provided in section III illustrate the energy con-
sumption of the various components of the DC and table VII
shows an overview of the various methods for energy savings
examined throughout this paper and the savings they provide.
Furthermore, the table shows how large a reduction in the
overall DC energy consumption each of the proposed methods
will bring, as well as which of the components will save energy
by the current method. Finally, the table also shows whether
the various DC sizes can benefit from the different solutions.
The table leads to the clarification that small and mid scale
DCs can potentially minimize their energy consumption by up
to 23% by utilizing methods mentioned earlier in this section
and in the table, where large DCs can save up to 34% energy
consumption by utilizing the methods proposed.
B. Energy Minimization potential for C-RAN DCs
When moving baseband processing from mobile networks
into DCs, the network functions are already virtualized, other-
wise they could not operate on COTS hardware. Thus, multiple
DUs or CUs can run on the same physical hardware. This
opens up for opportunities in resource utilization including
load balancing and scheduling. By utilizing these methods
it is possible to shut down un-used hardware resources in
low traffic periods. Mobile traffic does vary by time and
is especially lower during the night, thus; this is a great
potential energy saver. Hence, during the day the users of
mobile networks will move around between different areas, for
instance residential and work areas, leaving one area under-
utilized. The take away points from the case study is that
even in urban areas, the C-RAN DC is still a minor mid-
scale DC. Hence, the strategies for small-scale DCs energy
minimization can be applied. By examining the strategies for
small-scale DCs energy minimization, it is possible to utilize

86
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
TABLE VII. DATA CENTER ENERGY EFFICIENCY BY COMPONENT
Method
Saving
Reduction
Component(s)
Small scale DCs
Mid scale DCs
Large scale DCs
Cooling
50% [30]
20%
Cooling
Yes
Yes
Yes
Virtualization
30% [22]
10.5%
Servers
Yes
Yes
Yes
Resource utilization
46% [26]
23%
Servers, aggregation switches,
access routers, core switches
No
No
Yes
Sleep states
34% [27]
17%
Servers, aggregation switches,
access routers, core switches
Yes
Yes
Yes
Power management
20% [76]
2%
Aggregation switches,
access routers, core switches
Yes
Yes
Yes
DU latency 
limit
CU latency 
limit
Figure 12. The graph illustrates the energy usage in small-, mid- and large
scale Data Centers (DCs). The latency boundaries from Fig. 9 for a DU
corresponding to ISD 0.5 km and a CU corresponding to ISD 5 km, are
outlined in the graph.
sleep modes, where core C-states can be used in different
ranges depending on the traffic pattern. Hence, the MNO must
be aware of the exit latency, which increases with the CPU
power savings. Furthermore, package C-states can be used
for longer idle periods. However, since much mobile traffic
is latency sensitive, P-states are not recommended.
Exploring how many DCs are actually beneficial to cover a
certain area will depend on the size of the area as well as the
chosen size of DC(s). From an energy efficiency perspective,
both many small and one large DC have different pros and
cons. Hence, looking at the various elements in the DC, as
presented in Fig. 5, then some components will remain the
same number when increasing the amount of DCs while others
will increase, this is stated in table VII. Figure 12 shows how
the various DC sizes are beneficial for different numbers of
sites in C-RAN. In the figure, the maximum DC sizes stated
in table II, are utilized, and when exceeding this number, more
DCs of the current size are added increasing the number of
cooling systems and power supplies, which are expected to
be only one per DC. Thus, the figure outlines how latency
requirements, as are a vital part of mobile network data flow,
will be the limiting factor for re-routing DU traffic to larger
DCs.
VII. CONCLUSION
This work investigated how the scale of a DC can impact
its energy efficiency where large DCs in particular face oppor-
tunities in terms of energy minimization. On the other hand,
the C-RAN trend in the RAN segment of mobile networks
requires smaller and local DCs. Small and mid-sized DCs,
can achieve notable energy-savings by improving design and
infrastructure, as well as improving resource utilization. On
the other hand, large-scale DCs can make use of the greater
amount of available resources to increase energy efficiency
in the same and other ways such as with AI driven resource
management, new cooling methods and quantum computing.
This work provided a set of engineering guidelines to be used
for determining the size of a DC for C-RAN. These guidelines
implicate the on-site installation, which particularly restricts
the latency between the current site and the DC handling the
mobile traffic. Furthermore, the size of the area and the number
of installations in the current area affect the size of the C-
RAN DC, and the server efficiency which is a yet greenfield
area of exploration. Thus, when exploring the case study two
candidate areas within the latency limit of one DU brought
an insight in the size of DC required to support the C-RAN
architecture if adopted in the mobile network, but highlighted
the limitations within the mobile traffic latency requirements.
ACRONYMS
3GPP
3rd Generation Partnership Project.
AI
Artificial Intelligence.
APs
Access Points.
AW
AgileWatts.
C-RAN
Cloud RAN.
CKE
Clock Enable.
COTS
Commercial off the Shelf.
CPU
Central Processing Unit.
CU
Centralized Unit.
DC
Data Center.
DCIM
Data Center Infrastructure Management.
DCs
Data Centers.
DRAM
Dynamic Random-Access Memory.
DTSP
Dynamic Time Scale based Server Provisioning.
DU
Distributed Unit.

87
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
EAVMAT The Energy Aware VM Available Time.
HLS
High Layer Split.
ICT
Information and Communications Technology.
ISD
Inter-Site Distance.
L3
Layer 3, the network layer.
LLS
Low Layer Split.
LTE
Long Term Evolution.
MC
Memory-Controller.
MNO
Mobile Network Operator.
NR
New Radio.
PCIe
Peripheral Component Interconnect Express.
PUE
Power Usage Effiectiveness.
QoS
Quality of Service.
RAN
Radio Access Network.
RU
Radio Unit.
ToR
Top of the Rack.
US
United States.
VM
Virtual Machine.
REFERENCES
[1] S. Friis, L. M. P. Larsen, and S. Ruepp, “Strategies for minimization
of energy consumption in data centers,” Proceedings of Twenty-second
International Conference on Networks, ICN, pp. 17–22, 2023.
[2] The Parliamentary Office of Science and Technology. (2022) Postnote
677 september 2022 energy consumption of ict. Accessed: 15/06/2023.
[Online]. Available: https://post.parliament.uk/research-briefings/post-
pn-0677/
[3] L. M. P. Larsen, H. L. Christiansen, S. Ruepp, and M. S. Berger, “Toward
Greener 5G and Beyond Radio Access Networks-A Survey,” IEEE Open
Journal of the Communications Society, vol. 4, pp. 768–797, 2023.
[4] L. M. P. Larsen, A. Checko, and H. L. Christiansen, “A survey of
the functional splits proposed for 5g mobile crosshaul networks,” IEEE
Communications Surveys and Tutorials, vol. 21, no. 1, pp. 146–172,
2018.
[5] S.
Moss.
(2022)
In
search
of
the
world’s
largest
data
center.
Accessed:
15/03/2023.
[Online].
Avail-
able: https://www.datacenterdynamics.com/en/analysis/in-search-of-the-
worlds-largest-data-center/
[6] X. Wang, X. Liu, L. Fan, and J. Huang, “Energy-aware resource
management and green energy use for large-scale datacenters: A survey,”
Advances in Intelligent Systems and Computing, vol. 255, pp. 555–563,
2014.
[7] F. Kong and X. Liu, “A survey on green-energy-aware power man-
agement for datacenters,” Acm Computing Surveys, vol. 47, no. 2, p.
2642708, 2014.
[8] M. Zakarya, “Energy, performance and cost efficient datacenters: A
survey,” Renewable and Sustainable Energy Reviews, vol. 94, pp. 363–
385, 2018.
[9] M. Dayarathna, Y. Wen, and R. Fan, “Data center energy consumption
modeling: A survey,” Ieee Communications Surveys and Tutorials,
vol. 18, no. 1, p. 7279063, 2016.
[10] Y. Lin, L. Shao, Z. Zhu, Q. Wang, and R. K. Sabhikhi, “Wireless network
cloud: Architecture and system requirements,” International Business
Machines, Tech. Rep., 2010.
[11] China Mobile Research Institute, “C-RAN The Road Towards Green
RAN,” China Mobile, Tech. Rep., 2011.
[12] D. A. Temesgene, J. N´u˜nez-Mart´ınez, and P. Dini, “Softwarization and
optimization for sustainable future mobile networks: A survey,” Ieee
Access, vol. 5, pp. 25 421–25 436, 2017.
[13] A. Checko, H. L. Christiansen, Y. Yan, L. Scolari, G. Kardaras, M. S.
Berger, and L. Dittmann, “Cloud ran for mobile networks - a technology
overview,” IEEE Communications Surveys and Tutorials, vol. 17, no. 1,
pp. 405–426, 2014.
[14] L. R. Jahangard and A. Shirmarz, “Taxonomy of green cloud computing
techniques with environment quality improvement considering: a sur-
vey,” International Journal of Energy and Environmental Engineering,
vol. 13, no. 4, pp. 1247–1269, 2022.
[15] M. H. M. Gavali, M. S. S. Patil, M. P. U. Patil, S. P. Mane, and M. K. N.
Rode, “Green cloud computing,” International Journal for Research in
Applied Science and Engineering Technology, vol. 10, no. 4, pp. 581–
583, 2022.
[16] A. Katal, S. Dahiya, and T. Choudhury, “Energy efficiency in cloud
computing data center: a survey on hardware technologies,” Cluster
Computing, vol. 25, no. 1, pp. 675–705, 2022.
[17] S. Mustafa et al., “Performance evaluation of energy-aware best fit
decreasing algorithms for cloud environments,” Proceedings - 2015
Ieee International Conference on Data Science and Data Intensive
Systems; 8th Ieee International Conference Cyber, Physical and Social
Computing; 11th Ieee International Conference on Green Computing
and Communications and 8th Ieee International Conference on Internet
of Things, Dsdis/cpscom/greencom/ithings 2015, pp. 464–469, 2015.
[18] L. A. Barroso, U. H¨olzle, and P. Ranganathan, “Data center basics:
Building, power, and cooling,” Datacenter As a Computer, pp. 75–98,
2019.
[19] C. Hu, Y. Guo, Y. Deng, and L. Lang, “Improve the energy efficiency
of datacenters with the awareness of workload variability,” Ieee Trans-
actions on Network and Service Management, vol. 19, no. 2, pp. 1260–
1273, 2022.
[20] M. Zakarya, “Energy, performance and cost efficient datacenters: A
survey,” Renewable and Sustainable Energy Reviews, vol. 94, pp. 363–
385, 2018.
[21] G. Antoniou et al., “Agilepkgc: An agile system idle state architecture
for energy proportional datacenter servers,” Proceedings of the Annual
International Symposium on Microarchitecture, Micro, vol. 2022-, pp.
851–867, 2022.
[22] M. S. B. M. Desa et al., “Energy efficient approach using server virtu-
alization in cloud data center,” 2018 Ieee 4th International Symposium
in Robotics and Manufacturing Automation, Roma 2018, p. 8986732,
2018.
[23] L. C. Yan, Y. Li, H. Song, H. D. Zou, and L. J. Wang, “Edge com-
puting based data center monitoring,” Proceedings - Ieee International
Conference on Edge Computing, vol. 2021-, pp. 17–24, 2021.
[24] C. Gough, I. Steiner, and W. Saunders, Energy efficient servers:
Blueprints for data center optimization.
Apress Media LLC, 2015.
[25] C. Hu, Y. Deng, G. Min, P. Huang, and X. Qin, “Qos promotion
in energy-efficient datacenters through peak load scheduling,” Ieee
Transactions on Cloud Computing, vol. 9, no. 2, pp. 777–792, 2021.
[26] S. Loganathan, R. D. Saravanan, and S. Mukherjee, “Energy aware
resource management and job scheduling in cloud datacenter,” Inter-
national Journal of Intelligent Engineering and Systems, vol. 10, no. 4,
pp. 175–184, 2017.
[27] V. Anagnostopoulou, S. Biswas, H. Saadeldeen, A. Savage, R. Bianchini,
T. Yang, D. Franklin, and F. T. Chong, “Barely alive servers: Greener
datacenters through memory-accessible, low-power states,” Design Tech-
nologies for Green and Sustainable Computing Systems, pp. 149–178,
2013.
[28] T. L. Vasques, P. Moura, and A. de Almeida, “A review on energy
efficiency and demand response with focus on small and medium data
centers,” Energy Efficiency, vol. 12, no. 5, pp. 1399–1428, 2019.
[29] B. Speitkamp and M. Bichler, “A mathematical programming approach
for server consolidation problems in virtualized data centers,” Ieee
Transactions on Services Computing, vol. 3, no. 4, pp. 266–278, 2010.
[30] Y. Gong, F. Zhou, G. Ma, and S. Liu, “Advancements on mechanically
driven two-phase cooling loop systems for data center free cooling,”
International Journal of Refrigeration, vol. 138, pp. 84–96, 2022.
[31] Geetanjali and S. J. Quraishi, “Energy savings using green cloud
computing,” Proceedings of the 2022 3rd International Conference
on Intelligent Computing, Instrumentation and Control Technologies:
Computational Intelligence for Smart Systems, Icicict 2022, pp. 1496–
1500, 2022.

88
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[32] F. Marzouk, J. P. Barraca, and A. Radwan, “On energy efficient
resource allocation in shared rans: Survey and qualitative analysis,” Ieee
Communications Surveys and Tutorials, vol. 22, no. 3, pp. 1515–1538,
2020.
[33] A. Israr, Q. Yang, and A. Israr, “Power consumption analysis of access
network in 5g mobile communication infrastructures — an analytical
quantification model,” Pervasive and Mobile Computing, vol. 80, p.
101544, 2022.
[34] M. Masoudi, M. G. Khafagy, A. Conte, A. El-Amine, B. Francoise,
C. Nadjahi, F. E. Salem, W. Labidi, A. Sural, A. Gati, D. Bodere,
E. Arikan, F. Aklamanu, H. Louahlia-Gualous, J. Lallet, K. Pareek,
L. Nuaymi, L. Meunier, P. Silva, N. T. Almeida, T. Chahed, T. Sjolund,
and C. Cavdar, “Green mobile networks for 5g and beyond,” Ieee Access,
vol. 7, pp. 107 270–107 299, 2019.
[35] M. Zhu, J. Gu, X. Zeng, C. Yan, and P. Gu, “Delay-aware energy-
saving strategies for bbu pool in c-ran: Modeling and optimization,”
Ieee Access, vol. 9, pp. 63 257–63 266, 2021.
[36] M. R. Aktar and M. S. Anower, “Improvement of energy efficiency
by dynamic load consolidation in c-ran,” International Journal of
Communication Systems, vol. 35, no. 6, p. e5087, 2022.
[37] M. Alemam, A. A. El-Sherif, and T. Elbatt, “Energy efficiency optimiza-
tion through rrhs on/off switching technique in c-ran,” Ieee Wireless
Communications and Networking Conference, Wcnc, vol. 2019-, p.
8885821, 2019.
[38] M. R. Aktar and M. S. Anower, “Improvement of spectral and energy
efficiency by coordinated transmission in c-ran,” 6th International
Conference on Computer, Communication, Chemical, Materials and
Electronic Engineering, Ic4me2 2021, p. 4 pp., 2021.
[39] F. Zanferrari Morais, C. Andr´e da Costa, A. M. Alberti, C. Bonato Both,
and R. da Rosa Righi, “When sdn meets c-ran: A survey exploring multi-
point coordination, interference, and performance,” Journal of Network
and Computer Applications, vol. 162, p. 102655, 2020.
[40] J. Francis and G. Fettweis, “Energy efficiency maximization in massive
mimo-aided, fronthaul-constrained c-ran,” Ieee International Symposium
on Personal, Indoor and Mobile Radio Communications, Pimrc, vol.
2019-, p. 8904159, 2019.
[41] D. Lopez-Perez, A. De Domenico, N. Piovesan, G. Xinli, H. Bao,
S. Qitao, and M. Debbah, “A survey on 5g radio access network energy
efficiency: Massive mimo, lean carrier design, sleep modes, and machine
learning,” Ieee Communications Surveys and Tutorials, vol. 24, no. 1,
pp. 653–697, 2022.
[42] S. Zhang, Y. Zhang, X. Gong, and R. Wang, “Freevm: A server release
algorithm in datacenter network,” Ieee International Conference on
Communications, p. 6 pp., 2021.
[43] A. Kaushik, G. Khan, and P. Singhal, “Cloud energy-efficient load
balancing: A green cloud survey,” Proceedings of the 2022 11th Inter-
national Conference on System Modeling and Advancement in Research
Trends, Smart 2022, pp. 581–585, 2022.
[44] Team
E3P,
European
Commission.
(2016)
Data
centres
code
of
conduct.
Accessed:
19/06/2023.
[Online].
Available:
https://e3p.jrc.ec.europa.eu/communities/data-centres-code-conduct
[45] Berkely
Lab.
(2023)
Center
of
expertise
for
energy
efficiency
in
data
centers.
Accessed:
19/06/2023.
[Online].
Available:
https://datacenters.lbl.gov/
[46] GREENDC.
(2023)
Green
dc
greening
datacenters.
Accessed:
19/06/2023. [Online]. Available: https://www.greendc.eu/
[47] CORDIS EU research. (2020) Converting dcs in energy flexibility
ecosystems
(catalyst).
Accessed:
19/06/2023.
[Online].
Available:
https://cordis.europa.eu/project/id/768739
[48] AINET project. (2022) Ai-net-ainara. Accessed: 26/06/2023. [Online].
Available: https://aniara.ai-net.tech/home/
[49] DATAZERO.
(2022)
Datazero
as
green
as
possible.
Accessed:
26/06/2023. [Online]. Available: https://www.irit.fr/datazero/datazero2/
[50] SeDuCe project. (2022) A testbed for research on thermal and power
management in datacenters. Accessed: 26/06/2023. [Online]. Available:
https://www.irit.fr/datazero/datazero2/
[51] HUB4CLOUD project. (2022) Improving cloud computing performance
and sustainability in europe. Accessed: 26/06/2023. [Online]. Available:
https://cordis.europa.eu/project/id/101016673
[52] SOOGREEN
project,
“Service-oriented
optimization
of
Green
mobile networks,” 2015, accessed: 19/06/2023. [Online]. Available:
https://soogreen.eurestools.eu/
[53] UNIVERSITY
OF
KENT,
“Intelligent
Converged
network
consol-Idating
Radio
and
optical
access
aRound
USer
equipment,”
2015,
accessed:
19/06/2023.
[Online].
Available:
https://cordis.europa.eu/project/id/644526
[54] CloudRadioNet project. (2023) Cloud wireless networks: An information
theoretic
framework.
Accessed:
26/06/2023.
[Online].
Available:
https://cordis.europa.eu/project/id/694630
[55] 5G-COMPLETE
project.
(2023)
5g-complete
concept.
Accessed:
26/06/2023. [Online]. Available: https://5gcomplete.eu/concept/
[56] D. Kliazovich, P. Bouvry, and S. U. Khan, “Greencloud: A packet-level
simulator of energy-aware cloud computing data centers,” Journal of
Supercomputing, vol. 62, no. 3, pp. 1263–1283, 2012.
[57] H. Rong, H. Zhang, S. Xiao, C. Li, and C. Hu, “Optimizing
energy consumption for data centers,” Renewable and Sustainable
Energy Reviews, vol. 58, pp. 674–691, 2016. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S1364032115016664
[58] L. Huff. (2015) Network architecture in the data center. Accessed:
04/08/2023. [Online]. Available: https://connectorsupplier.com/network-
architecture-in-the-data-center/
[59] S. M. Nabavinejad and M. Goudarzi, “Chapter five - communication-
awareness for energy-efficiency in datacenters,” in Energy Efficiency
in
Data
Centers
and
Clouds,
ser.
Advances
in
Computers.
Elsevier,
2016,
vol.
100,
pp.
201–254.
[Online].
Available:
https://www.sciencedirect.com/science/article/pii/S0065245815000698
[60] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri,
D. A. Maltz, P. Patel, and S. Sengupta, “Vl2: A scalable and flexible
data center network,” Communications of the Acm, vol. 54, no. 3, pp.
95–104, 2011.
[61] N. Horner and I. Azevedo, “Power usage effectiveness in data centers:
Overloaded and underachieving,” Electricity Journal, vol. 29, no. 4, pp.
61–69, 2016.
[62] J.
Roach.
(2020)
Microsoft
finds
underwater
dat-
acenters
are
reliable,
practical
and
use
en-
ergy
sustainably.
Accessed:
26/06/2023.
[Online].
Avail-
able:
https://news.microsoft.com/source/features/sustainability/project-
natick-underwater-datacenter/
[63] H. M. Ljungqvist, L. Mattsson, M. Risberg, and M. Vesterlund, “Data
center heated greenhouses, a matter for enhanced food self-sufficiency
in sub-arctic regions,” Energy, vol. 215, p. 119169, 2021.
[64] NeRZ
and
Eco,
“Utilization
of
Waste
Heat
in
the
Data
Center,”
whitepaper.
[Online].
Available:
https://international.eco.de/topics/datacenter/white-paper-
utilization-of-waste-heat-in-the-data-center/
[65] ”RI.SE”. (2023) Data center greenhouse - data collection plattform.
Accessed: 26/06/2023. [Online]. Available: http://seduce.menaud.fr/
[66] W. E. District project. (2023) Heating & cooling solutions. Accessed:
26/06/2023. [Online]. Available: https://www.wedistrict.eu/
[67] D. Huang, “Data center infrastructure management,” Data Center Hand-
book: Plan, Design, Build, and Operations of a Smart Data Center, pp.
627–644, 2021.
[68] L. M. P. Larsen, M. S. Berger, and H. L. Christiansen, “Energy-
Aware Design Considerations for Ethernet-Based 5G Mobile Fronthaul
Networks,” Proceedings of the Fourth International Conference on
Green Communications, Computing and Technologies, 2019.
[69] A. Garg and D. Shenkar, “Drive data center management and build better
ai with it devices as sensors,” Data Center Handbook: Plan, Design,
Build, and Operations of a Smart Data Center, pp. 669–673, 2021.
[70] J. Liu, C. T. Hann, and L. Jiang, “Quantum data center: Theories and
applications,” p. 24, 2022, accessed: 15/03/2023. [Online]. Available:
https://arxiv.org/abs/2207.14336
[71] M. J. Martin, C. Hughes, G. Moreno, E. B. Jones, D. Sickinger,
S. Narumanchi, and R. Grout, “Energy use in quantum data centers:
Scaling the impact of computer architecture, qubit performance, size,
and thermal parameters,” Ieee Transactions on Sustainable Computing,
vol. 7, no. 4, pp. 864–874, 2022.
[72] J. Haj-Yahya, Y. Sazeides, M. Alser, E. Rotem, and O. Mutlu, “Tech-
niques for reducing the connected-standby energy consumption of mo-
bile devices,” Proceedings - 2020 Ieee International Symposium on High
Performance Computer Architecture, Hpca 2020, pp. 623–636, 2020.
[73] 3GPP, “TR 38.801 V14.0.0: Study on new radio access technology:
Radio access architecture and interfaces,” 3GPP, Specification, 2017.
[74] L. M. P. Larsen, H. L. Christiansen, S. Ruepp, and M. S. Berger,
“Deployment Guidelines for Cloud-RAN in Future Mobile Networks,”
Proceedings of 11th International Conference on Cloud Networking, pp.
141–149, 2022.

89
International Journal on Advances in Networks and Services, vol 16 no 3 & 4, year 2023, http://www.iariajournals.org/networks_and_services/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[75] TDC NET. (2023) We connect Denmark. For everyone. Accessed:
02/08/2023. [Online]. Available: https://tdcnet.com/
[76] Panduit, “Real-Time Data Center Intelligence Pays Off,” accessed:
07/08/2023.
[Online].
Available:
panduit-capacity-management-via-
dcim.pdf

