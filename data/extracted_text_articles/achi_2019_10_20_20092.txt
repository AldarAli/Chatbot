You are the Mind of a Robot
Tele-existence for Adults and Children
Vladimir Estivill-Castro and Vladimir Sukhov
School of Information and Communication Technology
Grifﬁth University
Nathan Campus, Brisbane, 4111, QLD, Australia
Email: v.estivill-castro@griffith.edu.au vladimir.sukhov@griffithuni.edu.au
Abstract—We carried two experiments of tele-existence. We pro-
vide a real-time illusion to humans that they exist in another
place. The other place is the real world, although now their new
body is a robot. In our experiments, we used virtual-reality to
transport the subject to the body of a soccer-playing robot. In
this new visions of human-computer interaction, we differentiate
the experience between that experimented by adults and that
experimented by children. We observed that the experience
radically changes the adults’ views regarding tele-existence and
robotics. On the other hand, children become rapidly transported
to the body of the robot.
Keywords–Human-robot
interaction;
mixed-reality;
tele-
existence; experiments and applications; virtual reality; immersive
environments.
I.
INTRODUCTION
Minsky introduced the term tele-presence [1] emphasising
the importance of high-quality sensory feedback and suggest-
ing that in the future, human perception and action will be
no different from those transferred as from those interacting
with the technologies. Since the ﬁrst robotic-assisted remote
tele-presence surgery, its application continues to grow [2].
The iRobot AVA 500 has been heralded now for several years
as the ﬁrst ever self-driving business collaboration robot [3].
The market of robots that can automate or assist in a range
of environments, from ofﬁces to clinics has seen several
companies emerge, such as Anybots, Double Robotics, Man-
taro, Revolve Robotics, Vecna, Awabot, Inbot Technology and
iRobot. For ofﬁce work, robots that resemble a tilt-able tablet
on wheels are gaining popularity; for example, the products
by Amy Robotics, AXYN Robotique, MantaroBot, Suitable
Technologies, Double Robotics and Ava Robotics offer video
conference and mobility. In 2017, the market value for 2016
was estimated at $1.6 billion [4]. However, how much of these
tools create the transportation effect originally suggested by
Minsky? Many scholars [5] have pointed out that tele-presence
raises fundamental issues about the nature of existence. In
particular, one deﬁnition of presence is
“the perceptual illusion of non-mediation [6].”
Thus, Minsky’s tele-presence should be indistinguishable with
presence; and the quality of tele-presence is related to the
ability to deceive our perceptual system to take for “real” what
is not there.
We are interested in how humans consider the possibility
of tele-existence [7] [8]. There are pioneering studies on how
humans attribute social and ethical stance to robots [9] [10].
However, Kahn et al. [9] found the attitude towards robots is
fundamentally linked to the anthropomorphism of the robot.
Our research here is concerned with the person’s simulated
immersion in the body of the robot and the effect of this im-
mersion on the person’s understanding of possibilities for tele-
existence. Moreover, we propose here to explore in children
the attribution and transportation of one’s body to the body
of the robot: the departure of one’s own body and accept the
body of the robot as our interaction with the world. We suggest
this happens remarkably fast when the child is immersed in
the environment and controls the body of the robot via mixed
reality. Such tele-presence results in children rapidly adopting
not only control but a dialogue where children abandon in
their language anything related to their own physical body,
and they formulate sentences and actions with a semantics that
is now grounded in the body of the robot. Note that research
in cognitive sciences has revealed language inﬂuences thinking
and experience and body inﬂuence the resolution of ambiguous
terms in abstract thought [11].
Although some early work suggested that in a child’s
world there could be some confusion between reality and
fantasy [12], with tele-existence, there is nothing more real
than the mixed-reality environment and, we argue that their
new robotic body is authentic. Our study with adults departs
from most tele-presence studies which aim at understanding
how to improve tele-presence systems [13]. The dominant
approach to facilitate social interaction between humans is Mo-
bile Robotic Tele-Presence Systems (MRTPS) [13]. Typically,
such MRTPS could be considered a tablet on wheels; they
enable the technologies of computer tele-conference with some
mobility. However, the robot (and its pilot) can hardly inﬂuence
the environment. If the robot is a tele-android system [14],
its primary goal is human-to-human reproduction of realistic
face-to-face human communication. If the robot has arms [15],
the main purpose is gesturing to reinforce human-to-human
gesticulation. Here, our robots are Nao robots (humanoids) in
the environment of the RoboCup Standard Platform League.
Their pilot can navigate them following a ball or to a position
in the soccer ﬁeld. They can kick the ball with a choice
of leg and a choice of kick (we ask the reader to reﬂect
on what could “they” stand for, the robots, or the pilots:
while the robots physically kick the ball, the pilots command
when and how such kick happens). Moreover, our pilots (a
person who remotely connects to the robot via a computer
interface) have a signiﬁcantly immersive interface (as opposed
to MRTPS). For our pilots, our interface consists of a virtual
reality headset and room size tracking technology, giving the
pilot an opportunity to experience a simulated environment.
165
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

However, in this simulated environment, a large proportion
is the streaming video of the robot’s cameras. Therefore, our
metaphor is that what the pilot sees is what the robot sees. We
anticipate this forwarding of vision will transport the pilot to
the local environment of the robot, and into the robot’s body.
We aim to explore and contrast the thesis that our mixed
reality achieves a perceptual illusion of no-mediation, and that
the interface pragmatically disappears. Thus, our research is
probably best identiﬁed with the recent emerging term of
tele-existence [7]. We follow closely the idea of a master
pilot commanding naturally a real-world robot [16]. However,
rather than ﬁdelity of reproduction of the human motions with
arms and feet, we are interested in immersion by orienting
the pilot to achieve tasks. The tele-operated robot is no
longer an autonomous robot either. The International Feder-
ation of Robotics (IFR) and the Australian Robot Association
adopted the ISO standard vocabulary (ISO 8373) to describe
‘manipulating industrial robots operated in a manufacturing
environment’. We suggest here that the proposed tele-existence
blurs further the boundary of the machine relative to hu-
man. The fundamental robotic characteristic, that after being
programmed, a robot operates automatically, is not entirely
true for the mixed-reality tele-existence environment of our
experiments. Also, those descriptions of a robot that require
a control unit typically composed of a computer and software
would not adequately apply as the human pilot is signiﬁcantly
the controller and decision maker in our tele-presence world.
However, the pilot is liberated of controlling every joint and
motor on the robot by signiﬁcant autonomous behaviours in
the robot (if the robot were to fall, it would autonomously get
up).
The next section will provide the details of our methods;
in particular, how we secured participants, and how we set the
experiment; some of it mandated by requirements for ethical
approval. Section III gives some insight on how the visual
stream for the robot’s cameras was placed on the headset (or
visor) and how human participants could pilot the robot. We
summarise the results of our experiment in Section IV, and
we analyse possible validity threats in Section V. Conclusions
terminate the paper.
II.
THE METHODS
For our experiments, we applied the following method.
The research was performed by a series of demonstrations
of autonomous Nao robots (designed initially by Aldebaran
and now commercialised by SoftBank robotics)., Nao is a
humanoid robot 58 cm tall and weighs 4.3 kg wth 25 degrees of
freedom, and a relatively large set of sensors that includes two
cameras one above the other in the head, four microphones,
sonar, and IR and the V5 offers an atom processor. We
performed such demonstration on seven different days. These
events were six days of special events where participants
visited the campus, and the other two days were public displays
associated with Australia’s Science Week.
We conducted the activities in South-East Queensland,
Australia, with the furthest apart being 90km. The demon-
strations were conducted to audiences of children and adults.
However, we engaged children and adults in different voluntary
experiments. We aim to investigate the attitudes of children and
adults to tele-existence. For adults, we collected responses to
two questionnaires. These questionnaires were a pre-activity
TABLE I. EXPERIMENT SUMMARY.
Event
Adult Participants
Children Participants
STEM-6 Day One
0
22
STEM-6 Day Two
0
25
Open Day
23
1
Science Museum
3
1
Pup-Up-Science
0
47
STEM-6 Day Three
0
23
STEM-6 Day Four
0
23
GLO Logan 2018
3
14
Total
29
156
questionnaire and a post-activity questionnaire. For children,
we conducted speciﬁc directed language and measured speciﬁc
responses, collecting observational data. Table I summarises
the presentations and the involvement of participants (we
have far more subjects than the 6 participants in a similar
setting [7]). Whenever consent was given, photographs of
the session were taken. Presentations were set up for ap-
proximately six hours long. Although individual participants
engaged with the particular activity for strictly less than 2
minutes (nevertheless, the 180 seconds of our immersion is
much higher than approximately 30s [7] or 20s [16] of recent
research in a similar setting). That is, all session for all par-
ticipants immersed in the mixed-reality environment lasted 2
minutes. Displays about the activity were available for the full
opening hours of the demonstration. There was no reward or
any other incentive except the unique opportunity to experience
tele-existence (mixed reality, where the human mind drives
somehow the body of a robot). No advertisement or ﬂiers
were used. The off-campus displays were mostly promotional
events on programs and courses offered by Grifﬁth University.
Having visitors on campus is part of Grifﬁth University’s open
doors programs and also the STEM (Science, Technology,
Engineering, Mathematics) engagement programs with many
local high-schools. These students visit and are involved in
educational experiences on campus. All children subjects were
12 years old or older when participating in our experiments.
We use virtual reality devices to transport the participant
to a virtual world; however, the immersion world reﬂects the
vision of a nearby robot. This setting transports the person
into a tele-operator or pilot for the legged robot. During this
time, participants tele-operate a robot near a Standard Platform
League (http://spl.robocup.org) soccer goal from RoboCup.
Figure 1 shows our typical set up of the presentations outside
the campus. Our experiments are radically different than the
only two experiments we are aware of in a similar setting [7]
[16]. While those experiments were concern on the optimal
and most loyal reﬂection of the human pilots’ walk and
hand motion to the body of the robot in real time, and to
the feeding back to the human sensors the images of the
robot with maximum ﬁdelity, our work concentrates more on
the achievement of a meaningful mission. Thus, our human
participants face immediately the task of scoring a goal in
an environment with adversaries (those other experiments [7]
[16] focus on how precise is the reproduction of a straight
walk from the human in the robot, how accurate is a turn by
the human pilot on the robot, and how loyal is the imitation
of operators’ arm movements on the robot). Our approach is
rather different, no-one better at executing the kick on the
robot’s body than the robot itself with its onboard software.
Similarly, the best routine to get up from falling is in the
166
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

(a) Set up during Science Week.
(b) Set up at Pop-Up Science Day.
Figure 1. Typical set-up of a robot soccer ﬁeld and demonstration of
autonomous soccer playing robots.
software on the robot. We do not want human pilots to teach
their human bodies how to replicate such get-up motion or
kick motion so their surrogate robot can perform these tasks.
However, we do believe the humans would ﬁnd themselves
transported to the world of the robot.
During the activity, we requested participants to guide their
robot towards the ball, issue kick commands and if possible
score a goal (if they were to score the goal, they can chose
how to celebrate it, or shall we say, have their robot shall
celebrate it). We advised that their objective is to be efﬁcient
pilots of the robot, and score as many goals as possible in two
minutes. Most of the time, before engaging with the activity,
participants had been observing Nao robots playing robotic
soccer autonomously. On several occasions, participants had to
perform under the competitive circumstance of an autonomous
robot also approaching the ball.
Figure 3 shows a sample of the questions used in the pre-
activity questionnaire for adults. Figure 4 displays a sample of
questions in the post-activity survey.
For children, we will be assessing how rapidly they accept
language about the body of the robot as if it was their own
(a) Standing-up adult in indoors environment.
(b) Participant adults in indoors environment.
(c) Sitting-down child facing a goal.
(d) Sitting-down child behind goals of the soccer ﬁeld.
Figure 2. Adult participants were standing, children participants were
sitting. Participants’ hearing coincides between virtual world and real world.
167
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

1)
Have you ever experienced a virtual reality scenario?
2)
How often do you play video games?
3)
How often do you engage in competitive games against artiﬁcial
systems where the opponent has no human input (computer chess,
XBOXTM alone away from Internet)?
4)
How often do you engage in competitive games against other humans
(chess, XBOXTM where opponent is piloted by another person)?
5)
Rank your interest in engaging in a game with a robot that is the
surrogate of another person?
6)
Rank your interest in interested in engaging in a game with a robot
that is completely autonomous (no human pilot can inﬂuence the robot
during the game)?
7)
Are you familiar with physical presence scenarios such as “reality” TV
shows such as Survivor and Big Brother?
8)
In the ﬁlm The Truman Show, Truman (played by Jim Carrey) lives in
a television studio manufactured to look like the real world (and he is
unaware of this). Do you think this could happen to some person?
9)
The ﬁlm The Matrix presents the possibility that although we are in
control of our own consciousness, our bodies and the material world
that surrounds us are an artiﬁcial construction. Do you think it is
possible for humans to be immersed in such simulation?
10)
Avatar scenarios, such as the popular “life-simulating game” called
The Sims, are so called because players create characters, proﬁles and
control their lives. Would it be possible to have robots around us that
we “control” in such a way?
Figure 3. Sample questions from the pre-activity questionnaire.
body. So, rather than say “Make the robot kick with its left
foot”, we will evaluate the time it takes for them to accept
“Kick with your left leg”. For all minors, approval will be
requested from parents or guardians as this research was
conducted under Grifﬁth University Ethics Reference Number:
2018/846.
The setting could be considered a mix of a computer game
with a virtual reality headset and 3D hand-held controllers. We
have developed software that renders the camera video stream
of the robot to the participant’s headset. The 3D hand-held
controllers enable pointing at cubes (labelled with icons) in
the 3D-virtual environment and triggering an action. To trigger
an action, pilots must select a cube. Upon selection, the cube
spins in the 3D-virtual environment. The cube changes colour
and tilts when triggered (and the chosen action is forwarded).
The design of the 3D-virtual world is not a soccer ﬁeld; it is
a pilot’s room. Figure 5 illustrates the environment’s design.
That is, the human user is still doubled in a controller room,
and the cubes offer control to actions that impact the robot’s
body. The 3-D room has a large screen where the vision of the
Nao is presented. The Nao has cameras one above the other
(and not side to side as most mammals). So the presentation
is an image that reﬂects such upper and lower camera.
For children and adults, instructions were given regarding
the virtual world. We described, prior to placing the headset,
the 3D room of Figure 5. Participants were also introduced
to the hand-held controllers. These are visible in the virtual
world in extremely look-alike objects placed in proportionally
the same position relative to the headset in the real world. That
is, the hand-held controllers are common element between both
worlds. We indicated that the upper button would produce a
ray, which does not appear in the real world, but that selects
cubes in the virtual world. The trigger under the controller
sparks the corresponding action.
At least two assistants support the experiment. One pro-
vides instructions to participants, answers their questions, and
also plays a role of a bridge between being in the soccer ﬁeld,
and being with the participant in the virtual world by sustaining
1)
Did you felt at some point in the experience that your mind was purely
conﬁned within the simulated environment and there was no other
existence; that is, did it felt for at least an instant that your world
was the mixed reality experience?
2)
When trying to score a goal, were you aware other opponents were
completely autonomous or piloted somehow like your own; that is, did
it matter other robots degree of autonomy and simulation?
3)
Would you consider other technologies that directly interact with your
optical nerve or your senses and connect to your neural hardware for
such an experience?
4)
As technology improves, would this type of experiences be better if
everything was simulated, and no robot in the real world existed, but
still felt completely real?
5)
After this experience, re-rank your interest in engaging in a game with
a robot that is the surrogate of another person?
6)
After this experience, re-rank your interest in interested in engaging in
a game with a robot that is completely autonomous (no human pilot
can inﬂuence the robot during the game)?
7)
After this experience, and revisiting the hypothesis of the ﬁlm The
Truman Show, where Truman (played by Jim Carrey) living in a
television studio manufactured to look like the real world (and he is
unaware of this). Do you think this could happen to some person?
8)
After this experience, and revisiting the hypothesis of the ﬁlm The
Matrix (the possibility that although we are in control of our own
consciousness, our bodies and the material world that surrounds us are
an artiﬁcial construction). Do you think it is possible for humans to be
immersed in such simulation?
9)
After this experience, would you enjoy interacting with avatars (ar-
tiﬁcial robots that have some control or conﬁguration by humans),
for example as receptionists in hotels (and not as boring as vending
machines)?
10)
Avatar scenarios, such as the popular “life-simulating game” called
The Sims, are so called because players create characters, proﬁles and
control their lives. After the experience, re-rank your belief that it
would it be possible to have robots around us that we “control” in
such a way?
Figure 4. Sample questions from the post-activity questionnaire.
Figure 5. Design of the 3-D worlds the pilot sees.
a dialogue. This ﬁrst assistant can only see the soccer ﬁeld as
a spectator. The second assistant monitors the execution of the
application in a ﬂat monitor with a display similar to Figure 6.
The second assistant can monitor the time left, whether all
devices (hand-held controller, handset, tracking towers) are
operational, whether WiFi link to the robot is operational, and
a fraction of the participants view. For example, in Figure 6, we
see part of the icon on one of the cubes for waiving (celebrating
a goal) and the images of the robot’s cameras.
The sessions with a participant consisted of a protocol with
the following stages.
1)
Demonstration of robotic soccer. The participant en-
counters a clearly identiﬁed soccer ﬁeld, and hu-
manoid robots dressed in either red or black jerseys
engaging in a match that resembles the Standard Plat-
form League setting for RoboCup (refer to Figure 1).
168
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

Figure 6. Screen shot of the monitor running the application.
2)
Participants are invited to take part in the match. If the
participant is an adult, we present ethics information
sheet, and pre-activity questionnaire. If the participant
is a child, we secure parental consent. Adults are
allowed to pilot standing while children are requested
to sit (refer to Figure 2).
3)
Establishing the control. This stage consists of de-
scribing the headset and hand-held controls cloned
into the virtual room (in particular, the button used
and the trigger). A brief description of what to expect
(there are cubes in a room, some to the left, some
to the right, some in the middle). A description
of the task: to score as many goals as possible in
two minutes. The ﬁrst assistant indicates what robot
the human will pilot before setting up the headset
(typically, “you are the robot in black jersey 3”).
4)
Establish the language. From the beginning, the ﬁrst
assistant will purposely engage in a dialogue where
it refers to actions on the ﬁeld by the piloted robot as
if it was the body of the person. For instance, “you
should be able to see the ball ahead”, “walk forward
a bit still, the ball is still far from you”, “the robot
in red will challenge you for the ball”. However, on
occasion, guidance of the virtual environment would
be necessary, with advice such as “the cubes on your
right control your motions”, “if you want to kick,
you must use the cube with the icon of a foot”, “if
you want to stand-up, you need to activate the higher
cube” “if you want to sit down, you need to activate
the lower left cube”. There will also be perhaps
backward situations, such as “after you kicked, you
have fallen”, and intentionally all celebrations of
the goal ﬁnish with the robot kneeling down, so
the ﬁrst assistant would say “stand-up, to keep on
playing.” We record whether the child responds to
these commands using their own body, or continues
to engage the ﬁrst assistant as if their body is the
robot’s body.
5)
After two minutes, the activity stops. The headset
is removed, and the participant has completed the
experience.
6)
If the participant is an adult, collect completed post-
activity questionnaire.
Figure 7. Schema of the components of the application.
III.
ARCHITECTURE
Our application architecture displays a pilot’s room which
may seem a small cinema, where a projection screen is centred
and slightly above a theatrical stage. As we explained above,
the projection screen renders the video feed of the robot’s
cameras. Cubes in the centre of the virtual environment are
like GUI-buttons for the control of movement on the robot, so
that it walks forwards, backwards, spins to the right or to the
left. There are some behaviour templates, such as kick with
either foot, pass the ball with either foot, kneel and rest, stand
up, waive with left or right hand brieﬂy, or wave extensively.
We run our application on a Linux – Ubuntu 16.04LTS
WS, and virtual reality equipment of an HTC Vive set, we use
Unity 3D 2017.2 BETA Ubuntu, the OpenVR on SteamVR
tools set, and other software elements (refer to Figure 7). Unity
enabled high-level programming, and facilitated integration
with SteamVR, although C/C++ had to be wrapped into
C#. However, this wrapping enabled all C++ infrastructure
and model-driven behaviour with logic-labelled ﬁnite-state
machines (LLFSMs) to integrate smoothly [17]. Control com-
mands (captured by Unity) are delivered over the distributed
whiteboard, and the distributed object-oriented whiteboard
also achieves feedback from sensors in the robot. The local
whiteboards are shared-memory middleware that interface well
with the concurrent but reliable sequential scheduling of an
arrangement of LLFSMs. The distributed middleware operates
over idempotent control/status messages over UDP, which has
been shown to be more responsive that standards such as
ROS [18].
All messages are, therefore, C++ classes, and operate over
local whiteboards on the robot and on the host. However,
the video feed is a dedicated socket channel of compressed
jpeg images from the robot to the host. Therefore, the
development effort included several technologies, from the
169
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

Figure 8. Histogram of responses to ﬁrst question in pre-activity
questionnaire.
programming of C# scripting in Unity, to designing recorded
motion gestures in Nao’s Choreographe motion composing
system, extending the C++ classes/messages known to the
whiteboard, and developing ﬁnite-state machines that enable
behaviours in the robot. Also, some network programming and
Open-CV wrapping were required to forward the video stream
from the robot to the host.
IV.
THE RESULTS
We review the results for adults ﬁrst and then for children.
A. Adult subjects
The questionnaires were scanned and analysed to compare
answers. The ﬁrst part of the pre-activity questionnaire enables
us to judge whether the person is a suitable subject. Grifﬁth
University Ethics committee required us to minimise the risk
that participants experience adverse reactions to virtual reality
exposure (e.g., dizziness, nausea etc.). Therefore, we excluded
any individual we suspect was susceptible to a neurological
condition (e.g., epilepsy) or that had experienced adverse
reactions to virtual reality previously. Side effects of virtual
reality exposure are usually associated with long periods of
immersion [19]. All our subjects were limited to a period of
2 minutes.
Figure 8 shows that our sample of adult participants
(29 individuals) is almost divided evenly (13/16) among those
who had previous experience with VR and those who had not.
The pre-activity questionnaire shows that our sample of
adult participants holds signiﬁcant diversity. Figure 9a shows
that our subjects are signiﬁcantly familiar with video games,
engaging quite regularly with them. While they have some
engagement in competitive games against opponents known
to be artiﬁcial (Figure 9b), there is a slight predilection for
versing other humans (Figure 9c).
The after-experience questionnaire does not show a deﬁnite
illusion on the subjects that they were transported to a different
world. Figure 10a shows that 16 users out of 29 (more than
half) felt immersed in another world on occasions and although
this is more than 50% of the subjects, the other 13 were
not deﬁnite. We believe this is the participants’ perception,
and that is one measure which is affected by the lag in the
image, and the fact that sounds in the real world have not
a perceivable source in the immersed world. However, our
experiments with the children, later on, show that children were
signiﬁcantly immersed and one could say even consumed by
the task and the activity. Adults also show a slight predilection
for experiencing more similar immersions and more accurate
and directly reproduced virtual worlds into their senses, with
more than a third declaring they would enjoy too much
being immersed (Figure 10b). Whether a complete simulation
or transportation to another real world is preferable is also
undecided (Figure 10c). However, there is a slight preference to
have some robot and some reality over a complete simulation.
Now, we report on our analysis of the change in attitude or
belief from the adult participant from the pre-activity question-
naire to the post-activity questionnaire. The type of hypothesis
we formulate is that the 2-minute immersions results in a
positive change towards the technology or to the belief that
how humans perceive reality and existence can be opened to
new interpretation and possibilities.
Therefore, we start with the re-evaluation offered by Ques-
tion 5 in both surveys. Our hypothesis is that
(H) after the experience, participants are more inter-
ested in engaging in games with robots on a tele-
existence world.
The corresponding null hypothesis is that
(Hnull) after the experience participants have no
more preference (or potentially less interest) in en-
gaging in games with robots on a tele-existence
world.
Since we had a total of 14 responses out of 29 where
participants upgraded their interest, 14 maintained the same
interest and only one reduced their interest, we observe a
prevalence for our hypothesis. The result would not be statis-
tically signiﬁcant. Nevertheless, when we consider the change
in response to Question 6, we have that all participants, that is
the 29 of them, preferred to engage in contests challenged by
a robot that is totally autonomous. This outcome is naturally a
statistically signiﬁcant result that the experience changed the
views of the adult subjects. We also ﬁnd that the results seem to
contrast how our sample of participants engage in competitive
games. We note the contrast of Figure 9b with Figure 9c that
shows a preference to have humans as opponents, but for this
tele-existence, having purely artiﬁcially controlled robots is the
absolute preference!
If we evaluate the following hypothesis by the change in
response to the question regarding the ﬁlm The Truman Show,
we also observe a remarkable result. Our hypothesis is that,
(H) after the experience, participants believe more
strongly that human beings can be fooled long-term
about the reality they experience.
In this case, responses are such that 25 respondents increased
their ranking on the possibility of scenarios like in the ﬁlm
The Truman Show. If we make the null hypothesis as follows
(Hnull) after the experience participants have no
more belief (or potentially even less belief) on the
170
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

(a) Histogram for usage of video-games.
(b) Histogram for choosing and artiﬁcial
opponent.
(c) Histogram for selecting a human
opponent.
Figure 9. Illustrating the dispersion of personal preferences and habits by adults reﬂected in the pre-activity questionnaire.
(a) How transported to another world.
(b) Wishing to have more direct and
seamless immersion.
(c) Shall we keep a robot and keep it real or
should it be a complete simulation.
Figure 10. Post-activity histograms on responses showing slight preference for tele-existence experiences.
plausibility of a human being living on a staged
reality.
Then, the hypothesis testing using the binomial distribution
with 28 degrees of freedom show statistical signiﬁcance at the
99% level.
Since the thesis of the movie The Matrix is a direct illus-
tration of tele-existence, we expect to observe some change on
the belief for the plausibility of such a scenario. That is we
expect
(H) after the experience, participants believe more
strongly that human beings can have their nervous
system and brain directly linked to a virtual reality
and it would appear a completely real experience.
We found here that 18 out of 29 participants’ responses are
consistent with the hypothesis. This outcome is not statistically
signiﬁcant. Nevertheless, none of the responses decreased the
belief after the experience. It is not methodologically prudent
to change the hypothesis after observing the data, but it
remains an interesting observation that if we had formulated
the hypothesis
(H) participants would increase or maintain their
belief in the plausibility of scenarios like in the
movie the Matrix.
Then, the null hypothesis would be that they would decrease
their belief in such plausibility, and the data would have
rejected the null hypothesis. It is reassuring that our tele-
existence experiment did not have an effect to reduce the
chances of people imagining scenarios of tele-existence. That
is, our hardware/software immersion is convincing enough to
transport adult subjects to a tele-existence that sustains the
plausibility of tele-existence scenarios.
Finally, if we look at the change in belief for participants
contrasting on the existence scenario exempliﬁed by the life-
simulating game The Sims, we formulated the hypothesis as
follows.
(H) Participants would increase their belief in the
plausibility of scenarios like in the life-simulating
game The Sims.
Here 18 out of 29 participants increased their belief, and 11
left it unchanged. This outcome is not statistically signiﬁcant to
reject the null hypothesis. However, if we had formulated the
hypothesis as an increase or maintain, then the 29 participants
would have been consistent with the hypothesis and the result
would have been statistically signiﬁcant. However, this would
have the questionable methodological issue of revising the hy-
pothesis after inspecting the data. Nevertheless, it is remarkable
that none of the participants decreased their perception that
surrogate existence as exempliﬁed by The Sims is plausible.
B. Children subjects
Since we will measure how immersed are the children in
the tele-existence experience by their reaction to statements
like “to play you need to get up”, we have in our method a
mechanism to establish the baseline. Our approach is derived
from the requirement by our ethics approval to have the
children sitting during the experiment. For all participants,
the robot is always kneeling at the start of the experience.
171
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

With participants in the four days of STEM, we had at least
5 different groups in each day. That is, the STEM activities
partition the children in the day into 5 groups. Therefore, at
most 5 and typically 4 children are introduced to the activity.
The ﬁrst child to sit (from a group) is chosen randomly, and
the activity commences immediately with the remark by the
ﬁrst assistant: “to play you need to get up”. In total, we had 20
children (5 representatives of a group on 4 days) to which the
indication to stand-up was given at the start of the immersion.
We iterate that these children had not witnessed the activity,
and we suggest the indication is ambiguous as it could refer to
the robot’s body or the children’s body. Our results show that 8
out of 20 children stood up themselves, rather than operate the
cube that raises the robot. We take this as a strong indication
that early in the activity, the statement “to play you need to
get up” is ambiguous.
However, when a child pilot manages to score a goal and
celebrate it, the robot will purposely ﬁnish kneeling. So, the
ﬁrst assistant gives the indication “to continue playing you
need to get up”. We only had 44 out of 156 children succeeding
in scoring a goal. So, the situation where we evaluate the
reaction of children to the indication applies to those 44
participants only. However, 42 children used the control of
the virtual world to raise the robot and continue playing for
a second goal. Only 2 (two) out of 44 stood up from their
sitting position. There was no overlap between the 20 children
who received the instruction very early in the activity and
the 44 who received it after they succeed in scoring a goal.
However, there were 14 children who had witnessed another
child participate earlier, scored a goal and not require to stand
up themselves, just the robot. So, we exclude those and we are
left with 30 children, where 28 used the robot body rather than
their own to continue playing. We designed this experiment
with the following hypothesis in mind.
(H) The probability that a child participant interprets
the ambiguous statement about the body needing to
stand up as referring to the robot rather than his
own body is larger after being immersed than before
being immersed.
Note that our data for interpreting the statement very early in
the experience suggest that the probability of interpreting the
statement as the robot’s body is ˆp0 = 12/20 = 3/5 (This is the
Maximum Likelihood Estimator (MLE) of the corresponding
Binomial distribution). Since we had n = 20 trials, the Fisher
information is
I(p) =
n
p(1 − p).
Thus, the 95% conﬁdence interval is given by
CI0
=
ˆp0 ± 1.96
s
ˆp0(1 − ˆp0)
n0
=
3
5 ± 1.96
r
0.6(0.4)
20
=
0.6 ± 0.2147.
Now, after scoring a goal, the MLE probability of interpreting
the statement as the body of the robot is ˆp1 = 28/30 = 14/15.
And in this case, the 95% conﬁdence interval is
CI1
=
ˆp1 ± 1.96
s
ˆp1(1 − ˆp1)
n1
=
14
151.96
r
0.9333(0.0666)
30
=
0.9333 ± 0.0892.
Since the 95% conﬁdence do not overlap, we can say that at
the 95% level, we have a statistically signiﬁcant result for our
hypothesis. That is, we reject the null hypothesis at 95% and
accept that children interpret the language of your body as
referring to the robot.
V.
THREATS TO VALIDITY
Analysing the conditions and setting of our experiments we
can identify some challenges regarding external validity. The
subjects do not represent random samples of adults, neither
a random sample of children. The subjects have pre-selected
themselves as curious regarding technology, virtual reality
or at least robotics. They may be a correlation between the
promotion of information technology degrees (being performed
during the day), and the belief system of those that approach
the display and eventually participated in the activity. The
activity is robotic soccer as per the Standard Platform League
at RoboCup; it is possible that other challenges or competitive
environments of tele-existence deliver different results. There
were no awards and perhaps the setting does not constitute
realistic pressure on the participants; results could also be
different if reaching a speciﬁc target of goals would result in
receiving a prize. Similarly, if the participant were to perform
poorly and a penalty were to be applied, it could cause different
behaviour during the activity and different responses regarding
the enthusiasm to be involved in competitive settings with
physical artiﬁcial agents.
While we query about previous experience with virtual
environments, it is possible that training and previous practice
with the hand-held controllers and familiarity with wearing a
VR headset results in skilful tele-operation. Such easy tasks
and success with such tasks may impact self-awareness or self-
esteem; leading to considering humans more highly, and thus
superior than artiﬁcial systems.
We also need to consider other aspects of construct validity.
Is the pre-questionnaire or post-questionnaire measuring ade-
quately the impact of the exposure to a change in beliefs? Are
there other ways by which humans revise their belief system
about machines and artiﬁcial intelligence? Could the partici-
pants (despite answering anonymously) be anticipating what
are “normal” responses and trying to please those conducting
the research? It is not surprising that people adopt language
referring to an avatar as talking about themselves, even for
simple video games or massively networked games [20]. How-
ever, completely virtual avatars occasionally lead to identity
issues [20]. We felt here it was clear to all participants they
are the mind for just one body, they could not choose it, and
they can not clone it or vary it. All these are elements that
are distinctive from the construction of avatars, and would
prove interesting avenues to explore in further experimental
settings [21].
With respect to statistical validly, challenges could be
derived from violations to the statistical assumptions that
172
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

enable a particular analysis, low statistical power or low effect
size. We could have evaluated the results for children using the
observed value of the LR statistic [22, Section 12.4, Pages 156-
158] and we would establish even stronger (i.e. 99% level)
statistical signiﬁcance. In particular, we reject the hypothesis
that assuming the language is about the robot’s body is the
same before and after scoring a goal. The common MLE for
the probability is
12 + 38
20 + 30 = 40/50 = 4/5.
Thus,
Dobs
=
2

8 log 8
4 + 2 log 2
6 + 12 log 12
16 + 28 log 28
24

=
8.42.
and Prob{χ2
(1) ≥ 8.42} < 0.01. We felt the argument with
conﬁdence intervals is more transparent. Thus, we highlighted
those results where we could report statistical signiﬁcance.
VI.
CONCLUSIONS
The Oxford living dictionary deﬁnes tele-presence as
“The use of virtual reality technology, especially
for remote control of machinery or for apparent
participation in distant events. A sensation of being
elsewhere, created by virtual reality technology”.
We believe this deﬁnition captures the very distinctive notion
of tele-existence [7] when it suggests the individual is trans-
ported somewhere else. However, we have emphasised here
the distinction with a large number of tele-presence products
essentially equivalent to a tilt-able tablet on wheels. Such
products are closer to tele-conference infrastructure as they
offer little capability to inﬂuence the world one is transported
to. By creating a tele-existence environment with a Nao robot,
virtual reality headset and VR-controllers, we have transported
adults and children to co-exist with other robots in a soccer
match opposing other autonomous robots. We have evaluated
adults attitudes to the notion of tele-existence comparing it
with some scenarios discussed in the literature and exempliﬁed
by some widely known movies or video-games. We found
humans attitudes to engaging in competitive matches against
robots increased signiﬁcantly, and our setting does not reduce
the belief in humans for the plausibility of such scenarios.
In the case of children, they very rapidly adopt the robot’s
body as the body used in the natural dialogue with the
presenter of the activity. Our results open the exploration of
the emerging notion of tele-existence to reclaim the deﬁnition
of tele-presence.
There are many improvement opportunities for furthering
our current immersion. The camera relay typically has a lag of
at least 1.5s. Such a delay should be reduced. Several situations
should relay better feedback to the pilot, such as the robot
falling or when the waving of hands is ﬁnished (waving actions
were included to allow scoring celebrations). There is an open
ﬁeld of how much sensor information to forward to the human,
and how. The person must believe they are the mind of the
robot.
REFERENCES
[1]
M. Minsky, “Telepresence,” Omni, vol. 2, no. 9, 1980, pp. 45–52.
[2]
G. Ballantyne, “Robotic surgery, telerobotic surgery, telepresence, and
telementoring,” Surgical Endoscopy And Other Interventional Tech-
niques, vol. 16, no. 10, Oct 2002, pp. 1389–1402.
[3]
M. Reid, “Rethinking the fourth amendment in the age of supercom-
puters, artiﬁcial intelligence, and robots,” West Virginia Law Review,
vol. 119, no. 101, March 16th 2017, pp. 100–126.
[4]
E. T. Curtiss and S. Eustis, Telepresence Robots Market Shares,
Strategies, and Forecasts, Worldwide, 2017 to 2023.
Lexington,
Massachusetts: Wintergreen Research, Inc., 2017.
[5]
M. T. Jones, M. Lombard, and J. Jasak, “(tele)presence and simulation:
Questions of epistemology, religion, morality, and mortality,” Psych-
Nology Journal, vol. 9, no. 3, 2011, pp. 193–222.
[6]
M. Lombard and T. Ditton, “At the heart of it all: The concept of
presence,” Journal of Computer-Mediated Communication, vol. 3, no. 2,
September 1st 1997, pp. 201–213.
[7]
M. Elobaid, Y. Hu, J. Babic, and D. Pucci, “Telexistence and teleoper-
ation for walking humanoid robots,” 2018, arXiv:1809.01578.
[8]
S. Tachi, “Tele-existence,” Journal of Robotics and Mechatronics JRM,
vol. 4, no. 1, 1992, pp. 7–12.
[9]
P. H. Kahn, Jr., B. Friedman, and J. Hagman, “”I Care About Him As a
Pal”: Conceptions of robotic pets in online aibo discussion forums,” in
CHI ’02 Extended Abstracts on Human Factors in Computing Systems,
ser. CHI EA ’02.
New York, NY, USA: ACM, 2002, pp. 632–633.
[10]
P. H. Kahn, Jr., B. Friedman, D. R. P´erez-Granados, and N. G. Freier,
“Robotic pets in the lives of preschool children,” Interaction Studies,
vol. 7, no. 3, 2006, pp. 405 – 436.
[11]
L. Boroditsky and M. Ramscar, “The roles of body and mind in abstract
thought,” Psychological Science, vol. 13, no. 2, 2002, pp. 185–189.
[12]
R. Aylett, Robots: Bringing Intelligent Machines to Life.
Woodbury,
NY, USA: Barron’s Educational Series Inc., 2002.
[13]
A. Kristoffersson, S. Coradeschi, and A. Loutﬁ, “A review of mobile
robotic telepresence,” Adv. in Hum.-Comp. Int., 2013, pp. 3:3–3:3.
[14]
J.-M. Lu, C.-H. Lu, Y.-W. Chen, J.-A. Wang, and H. Y.-L., “TRiCmini
- a telepresence robot towards enriched quality of life of the elderly,” in
First Asia Paciﬁc eCare and TeleCare Congress, vol. 18, June 16th-19th
2011.
[15]
S. O. Adalgeirsson and C. Breazeal, “MeBot: A robotic platform for
socially embodied presence,” in Proceedings of the 5th ACM/IEEE
International Conference on Human-robot Interaction, ser. HRI ’10.
Piscataway, NJ, USA: IEEE Press, 2010, pp. 15–22.
[16]
A. Spada, M. Cognetti, and A. De Luca, “Locomotion and telepresence
in virtual and real worlds,” in Human Friendly Robotics, F. Ficuciello,
F. Ruggiero, and A. Finzi, Eds.
Cham: Springer International Publish-
ing, 2019, pp. 85–98.
[17]
V. Estivill-Castro, R. Hexel, and C. Lusty, “High performance relaying
of c++11 objects across processes and logic-labeled ﬁnite-state ma-
chines,” in Simulation, Modeling, and Programming for Autonomous
Robots, D. Brugali, J. F. Broenink, T. Kroeger, and B. A. MacDonald,
Eds.
Cham: Springer International Publishing, 2014, pp. 182–194.
[18]
D. Joukoff, V. Estivill-Castro, R. Hexel, and C. Lusty, “Fast mav control
by control/status oo-messages on shared-memory middleware,” in Robot
Intelligence Technology and Applications 4, J.-H. Kim, F. Karray,
J. Jo, P. Sincak, and H. Myung, Eds.
Cham: Springer International
Publishing, 2017, pp. 195–211.
[19]
K. Fagan, “Here’s what happens to your body when you’ve been in
virtual reality for too long,” Business Insider, March 4th 2918, www.
businessinsider.com.
[20]
Y. B. Kafai, D. A. Fields, and M. S. Cook, “Your second selves: Player-
designed avatars,” Games and Culture, vol. 5, no. 1, 2010, pp. 23–42.
[21]
S.-A. A. Jin, “The virtual malleable self and the virtual identity
discrepancy model: Investigative frameworks for virtual possible selves
and others in avatar-based identity construction and social interaction,”
Computers in Human Behavior, vol. 28, no. 6, 2012, pp. 2160 – 2168.
[22]
J. G. Kalbﬂeisch, Probability and Statistical Inference — Volume 2:
Statistical Inference, 2nd ed.
New York: Springer-Verlag New York,
1985.
173
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

