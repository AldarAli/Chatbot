 
Figure 1. Cone of Cause-Effect Relations and Propagation. 
 
Complex Landscapes of Risk in Operations Systems  
Aspects of Modelling and Processing  
Udo Inden 
Despina T. Meridou,  
Maria-Eleftheria Ch. Papadopoulou,  
Angelos-Christos G. Anadiotis 
Claus-Peter Rückemann 
 
Cologne University  
of Applied Sciences  
School of Electrical and Computer Engineering, 
National Technical University of Athens 
Leibniz Universität Hannover / 
Westfälische Wilhelms-Universität,  
Cologne, Germany 
Athens, Greece 
Münster, Germany 
udo.inden@fh-koeln.de 
{dmeridou, marelpap, aca}@icbnet.ece.ntua.gr 
ruckema@uni-muenster.de 
 
 
Abstract— Large, dynamic landscapes of interdependent risks 
are potentially existential challenges to industry. Substantiated 
by an example out of a variety of concrete industrial cases, we 
discuss in this paper concepts of modelling and managing such 
landscapes in a new way. The starting-points of the concept are 
managerial responsibility, the propagation of risk along 
dependencies in complex operations’ systems and resulting 
impacts, fundamental ambiguities of awareness, events’ 
classification and mitigation of impacts. For solving these 
problems, 
we 
suggest 
semantic 
technologies 
and 
the 
programming paradigm of multi-agent systems that for this 
reason are to be leveraged by effective parallel computing.  
Keywords–Operations’ Risk Management; Disambiguation; 
Semantic Technologies; High-End Computing. 
I. 
 INTRODUCTION: COMPLEX LANDSCAPES OF RISK 
For the purpose of this paper we shall reduce the scope of 
Risk Management (RM) as it is described in ISO 31000 
(2009) to is the task of managing the negative or positive 
impact of events under uncertainty [17]. The Event Risk (ER) 
is equal to a stochastically or statistically defined probability 
p, with 0 ≤ p ≤ 1, where 1 and 0 represent certainty of 
occurrence or non-occurrence respectively.  
In managerial contexts, the relevance of events is equal to 
its Economic Expectation Value (eEV), which is the product 
of event risk (p), impact (I, a monetary value) and awareness 
(A): eEV = p*I*A. Impacts may be positive or negative and 
will be experienced by at least one victim or beneficiary. We 
added the parameter of “awareness” to the model because it 
is an obvious prerequisite of managerial acting. So the 
awareness of a competitor’s attack may be achieved too late. 
The factor of awareness again depends on factors like 
implicitness, ambiguity, ignorance, taboos, hubris or 
unknown knowables and unknowables [1] that may 
antagonize managerial effort. Risk landscapes (RL) develop 
from interacting risks and the value of p may be a function of 
other incidents: the risk of a denial of service attack depends 
on the probability to hijack a sufficiently large number of 
computers. That way, forward chaining of events is 
represented as risk of transit and negative impacts may be 
mitigated or eliminated by other events (consider noise 
cancellation) or meet a well prepared “victim”. Accordingly, 
beneficiaries may not be that impressed by a price. 
Time in Figure 1 passes from left to right with cones 
representing the universe of past and future events. Those to 
the left are causes and those to the right effects of the one in 
the centre. Given a maximum speed of propagation events 
outside delimitations can neither be causes nor effects of the 
event in the focus. In managerial contexts, except in 
computer trading [3], propagation is determined by physical 
or organisational parameters of the system, not at least by the 
factor of awareness. Further the cause of an event matters. 
But when the event has arrived immediate managerial action 
needs to focus on appropriate response. The causes are 
subject of back-office analysis.  
Hence, landscapes of risk are (possibly very large) sets of 
interacting cause-effect cones with a system-specific 
behaviour. It is “the set of all (possible) events in the 
managerial universe” [4]. Complexity emerges from variety 
and resolution of object and time, the chance that more and 
more variant objects become source or target of more and 
more variant events. This reduces the control of a system’s 
behaviour: due to Ashby’s Law of Requisite Variety [5], 
controllers need to dispose of as much Discretion to Act 
(DTA) as the controlled system – but it should be the right 
ones.  
This paper explains an industrial use-case (Section II), 
the need and aspects of computing the model (Section III), a 
semantic model of a risk landscape (Section IV) and con-
cluded by a section on current and future work. 
II. 
THE ARUM SCENARIO 
The ARUM project [6], a project in the 7th European 
Research Framework Program, aims at improving ramp-ups 
of small series of complex products, chiefly in aviation 
industry. Delays and costs’ explosions of the Airbus A380 or 
Boeing B787 exemplify the problem [7]. Nightmares may 
develop from a component (e.g., a bracket later used for  
harnessing) showing a failure that has slipped through 
99
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-310-0
INFOCOMP 2013 : The Third International Conference on Advanced Communications and Computation

presence
time
E!
S
B1
B2
B3
B4
B5
B6
B0
x4
6.4
time
upstream
downstream
B1
Duration
of B1
B6
End of Main-Process B6
End of Product Assembly
2
3
1
2
3
1
2
3
1
 
Figure 2. Diagram for Cause-Effect Analysis (fictitious example). 
 
previous tests. To avoid “extrinsic hazards that can result in 
compromises in the quality of engineering” [1], production 
management is obliged to report failures. But only 
engineering is entitled to ascertain the failure and to decide 
conditions to continue work. Thus production is interrupted 
and solutions may change technology needing to revise 
previous assemblies. So delays may sum up to weeks and 
costs to Millions of euros.  
Landscapes of risk develop under such circumstances 
since at any time a variety of disruptions may occur at any 
node of operations. In short, the reasons are that competition 
pushes the technological envelops, that not all detail can be 
tested or that at some point in time capital intensity and small 
series 
enforce 
the 
start 
of 
production. 
After that 
immatureness of technology, resources, suppliers or pro-
cesses are left to ramp-up management, now required to 
identify and clear up the nature of the problem, to assess 
impact, to select or in situ elaborate and to coordinate 
system-wide policies that mitigate impact. But although the 
defined technological and economical constraints may fail, 
they provide a reference that, as far as possible, is expected 
to be recovered. It also is an advantage that most aspects are 
transformed into monetary values.  
The example of the faulty bracket mentioned above may 
lead to a formal declaration of ‘non-conformity to design’ 
and by this withdraw many brackets of this type from 
production. If buffers are exhausted, the problem propagates 
along technical dependencies, the more the longer it takes to 
solve the problem. Figure 2 shows such dependencies in a 
Fishbone Diagram (a tool of Failure Effect and Mode 
Analysis [8]) matching the breakdown of work in a 
production system and relates to its explosion of the bill of 
materials. For this paper a time-line and a red line 
representing the present are added. The fictitious plant hosts 
lines B1 – B6, together delivering, e.g., an aircraft. 
Substructures are indicated for line B6 (bottom right). All 
assemblies finished in point B6 meet time constraints. The 
example of line B1 shows the duration of an assembly.  
A station manager is expected to know “his” events’ 
cones. So downstream, an NC-event that occurs in station 4 
(marked in red) will disable the execution of subsequent 
work-orders in station 3 and following ones. Besides starting 
the NC-process, the station manager has only one obligation: 
mitigating negative impact by implementing a policy (pre-
structured tactics). In in economic operations, this decision is 
driven by economic relevance (the eEV): Mitigating action is 
justified by exceeding a threshold of economic impact and 
policies need to provide adequate positive effect compared to 
the case of doing nothing.  
In the case of the faulty brackets, a policy may be to 
implement an auxiliary solution, e.g. special cable ties, to be 
replaced later in time, but now applied in order to avoid far 
more expensive problems in station 3. However, further 
events may turn efficient policies into problems and so their 
effectiveness is to be tracked and changed: e.g., engineering 
needs more time and the problem propagates more than 
initially expected and planned. Alternatively, the policy may 
have to be suspended in order to save time and rework if 
station 3 is stopped for another reason, while the auxiliary 
solution is being implemented. 
III. EFFICIENT HANDLING OF LANDSCAPES OF RISK 
A. Managerial Concepts  
A variety of use-cases from airports, airlines, inflight 
catering, large-scale technological ventures, or small series 
production show that even small operations systems can 
become very complex and not call for both, significantly 
improved planning capabilities and augmented awareness of 
system behaviour and the propagation of impact.  
Effective policies are outcomes of operations’ intelli-
gence and deep knowledge about operations’ behaviour like 
indicators of sensitivity or criticality. Policies may be an idea 
of proficient workers, step by step building a library of 
policies. Some are limited to a station or to a set of 
proceedings agreed between neighbour stations, others may 
refer to the main process or the whole factory. To ease 
rework policies may be considered in the design of the 
product. Some are documented or even certified, others may 
be used more informally.  
In the simplest case an RL connects work-stations by 
sequential technical dependencies as indicated in Figure 2. 
But not all stations may be directly connected. A little more 
complicated model is shown in the Pert diagram [9] in Figure 
3: a failure in station 6 may affect station 7 by stopping work 
in station 8 and shared resources may open another path of 
propagation.  
Unplanned events with a serious impact switch the mode 
operations management from ‘maintain standard operations’ 
into ‘manage an exception’ with objectives different from 
standard proceedings. The background lies in the arguments 
of the formula eEV = p*I*A: If a substantial economic 
expectation value would have been identified already in 
planning, a proficient management would have planned for 
that contingency. If p or I are underestimated it arrives as 
unplanned event, and if A is (close to) 0 the event belongs to 
the class of unknown knowables [1].  
While contingency buffers enable standard processes to 
breath, policies are temporarily implemented processes that 
exchange failing standard processes with the objective to 
recover (possibly improved) standard operations as soon as 
possible. Therefore the lifetime of policies is either limited to 
the time it needs to find and implement a solution recovering 
the initial or, if need be, a new standard or until a new event 
requires to change the policy.  
100
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-310-0
INFOCOMP 2013 : The Third International Conference on Advanced Communications and Computation

Event
Station 
7
Station
8
Station 
9
Station 
10
Station 
11
End
Resource
Pool 1
Resource
Pool 2
sequential dependencies
shared resource 
dependencies
previous
stations
Station 
6
 
Figure 3. Pert-diagram of dependencies in Figure 2. 
 
 A policy has the planned impact to reduce a downside or 
to save an upside of the related unplanned event, both 
calculated from the eEV of the triggering event with p=1 and 
A=1 but still with an impact to be validated. As in real world 
the evaluation follows the way of propagation and, therefore, 
is calculated by the target stations until propagation is 
stopped. Considering first-level effects only, the impact of an 
unplanned event in station 6 in Figure 3 is equal to the sum 
of impacts in stations 7 to 11 and in the pools of resources 
(e.g., in terms of idleness). 
As a process a policy employs resources and has costs. 
These resources may be (a) implemental ones like cable-ties 
that temporarily replace proper brackets for cables, or (b) 
contingency buffers that may even have been allocated to 
another purpose. The management of policies may use any 
discretion to act (DTA): In the cable-ties’ example the 
chance to reduce rework was introduced as consequence of 
the actually negative event of failing operations in Station 3. 
In this way DTA belong to the ‘fabric’ of policies.  
So the industrial cases have common demands: (a) to 
provide technology that supports back- and the front-office 
analysis and planning, and (b) to provide simulation-based 
training of managerial awareness of events.  
With regard to the need for effective reasoning about the 
ambiguity of events, impacts and finally the value of 
policies, semantic technologies constitute the most promising 
approach and the integration of case-based, process-based 
and ontology-based reasoning. It should be noted that this 
concerns borders between conceptualisation strategies based 
on the (today most popular) root-concept of things on one 
side and on the root-concept of “process” (change) on the 
other side [13]. 
B. Draft of a Multi-Agent Approach of Handling RLs 
Agent-based modelling and simulation (ABMS) provides 
means to handle RLs with thousands of geographically or 
organisationally distributed nodes. In the example, each node 
of the landscape, e.g., a work-station and its share in the 
work breakdown (fishbones in Figure 2 and resources assig-
ned to it), pools of workers, inventories of resources etc. can 
be represented by one agent, or, if a deeper resolution of the 
network is required, further agents may represent elements of 
their substructures.  
The choice of algorithms coding the behaviour of agents 
belongs to the core aspect of modelling. Examples are 
algorithms to check eligibility of resources to serve in a 
particular process (workers, tools or components need speci-
fied skills) or economic algorithms to minimize idle times. 
Methods will also control the behaviour of agents depending 
on constraints: e.g., agents of components of the product may 
be passive in a phase of transfer as one of many shipments 
(transport) or as one of many stock keeping units (storage) 
that temporarily are represented by one agent.  
The impact of events and related activity is driven by 
communications between agents: affected by a statement of 
non-conformity (setting value of the attribute “eligible for” to 
zero) the withdrawal of respective resource(s) will cause a 
missing-resource event in the respective process, start evalu-
ation of impact and activate mitigating action and respective 
implemental resources. The value of risk in this context may 
be defined by the responsible managers or calculated on the 
base of simulations and statistics.  
But, also human factors and the variety or context sensiti-
vity of latency times of organisations (slow decision proce-
dures) or of IT-systems or material resources are to be 
modelled because each may substantially contribute to pro-
blems in synchronising activities and to disambiguate the 
character and control the impact of events.  
The computational support of managing complex RLs 
calls for High-end or High-performance capacity, while how-
ever current multi-agent systems (MAS) are hardly designed 
for running under the conditions of present HPC/HEC envi-
ronments and implementations, that again are less economic 
for applications imposing the burden of a high load of com-
munication that is typical for MAS.  
Technology 
arranging 
architecture 
and 
operations 
systems that provide high computational power and 
architectures of applications allowing high granularity and 
high adaptiveness to events are described in [14]: an example 
is the Repast HPC tool [16], offering both an editor for 
designing a multi-agent system and a platform organising 
and synchronizing interactions of agents in a way that is 
compatible to HEC/HPC environments. 
To estimate the computational scale of simulating an RL, 
similarities to existing HEC applications deliver an idea: an 
architecture of a weather model with a number of 
interdependent geographic cells may compare to a network 
of interdependent cells of managerial responsibility (e.g., in a 
factory).  
While the number of cells of the latter may be smaller, 
the variety of interactions is noticeably higher. And while 
weather models have clear inputs like temperature or 
humidity, managerial models may have to deal with the 
question “Is it the problem about humidity?”. The 
phenomenon of immediate impact of human behaviour 
(awareness) implies that the disambiguation of events, of 
propagation, of building of patterns is not an input but an 
expected output of computing. 
IV. 
A SEMANTIC MODEL OF EXCEPTION MANAGEMENT 
A. The Role of Ontologies in Operations’ Risk 
Management 
Ontology is a formal specification of shared concepts 
within a domain and the relationships between those con-
cepts and is used to improve communication between human 
beings and computers. With ontologies, a vocabulary captu-
ring concepts that underlie knowledge of a specific domain 
can be defined [13]. For example, an ontology of a manufac-
turing domain would capture concepts describing seman-
101
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-310-0
INFOCOMP 2013 : The Third International Conference on Advanced Communications and Computation

 
Figure 4. Ontology of Exception Management. 
 
 
 
tically the existing production processes, the resources that 
are required for or the goals that need to be achieved by these 
processes. A risk exception management ontology, as the one 
presented in the lower section, should include the basic 
concepts of an unplanned event, the impact and risk values of 
this event, the management processes and policies applied in 
case of an exception, etc.  
 
A major advantage offered by ontologies is the act of 
Reasoning or Inference. Reasoning is the process of deriving 
facts that are not explicitly stated in an ontology [14]. The 
process of inference can be achieved with a piece of software 
known as Semantic Reasoner. In the context of the RL of a 
factory, reasoning can be applied in order to identify the 
most appropriate policy to mitigate the impact of a negative 
unplanned event or to take advantage from the impact of a 
positive event that may follow a negative event. The reasoner 
should take into consideration the parameters of the context 
of the unplanned event, which include but are not limited to 
details of the work-order, the resources utilised during the 
process the event was raised, any other event that has prece-
ded and the policies, if any, that were applied in order to 
mitigate the impact of this event.  
Based on knowledge offered by the ontology, the need 
for the specification of a new policy, the application of an 
already existing one or even the re-evaluation of an already 
applied policy can be derived through the process of infe-
rence. In this manner, the semantic representation of policies 
in combination with the adoption of logic inference facilita-
tes the control of a complex system, such as a manufacturing 
system, autonomously, thus, reducing human error. 
Furthermore, a well-defined ontology can be used in 
combination with ontologies of different domains, providing 
the opportunity to build large-scale ontologies covering the 
concepts and the relations among them in a broader context. 
For example, in the semantic model presented in the lower 
section, the concept of time is planned to be expressed by 
means of a Time Ontology. Knowledge related to the engi-
neering domain is planned to be modelled in an Engineering 
Ontology. Additionally, a unified ontology, providing a 
shared and reusable vocabulary, can be effectively used to 
govern the operational behaviour of multi-agent systems, 
where agents operate using knowledge from different do-
mains. To sum up, the benefit of a unified semantic model is 
its flexibility, providing effective handling of heterogeneities, 
as well as its extensibility for additional semantic infor-
mation, maintaining the Integrity of the Specifications 
B. Ontological Model of Exception Management 
In order to capture the aforementioned concepts, the 
ontological model shown in Figure 4 was created. When an 
event is raised, an instance of the Event class is created being 
either a Planned Event or an Unplanned Event. Each is 
associated with an Economic Expectation Value, which is 
formed by the Event Risk value, the Impact value and the 
Awareness value of the respective event. The latter may be 
improved by Training, which is a type of Goal. A Goal is 
assigned to a Role or a set of Roles, implemented by an 
Actor. Generally, a Goal is specified by a KPI (Key 
Performance Indicator) Type, which may be quantitative or 
qualitative, and is associated with a KPI Value. It is pursued 
within events handling by the responsible Manager and may 
be modified by Relevance of events due to the Economic 
Expectation Value.  
Usually, a Goal may be either of type Standard Mana-
gement, denoting an everyday production goal related to 
Planned Events handling, or Exception Management, repre-
senting the goal of dealing with an exception, referring to an 
Unplanned Event. Specifically, the Exception Management 
process, which may consist of one or more sub-processes, is 
responsible for choosing the appropriate Policy. A Policy 
may be explicitly elaborated and filed into a Policy Library. 
Implicit Policies are implemented during the exception 
handling process, in order to provide an intermediate solution 
102
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-310-0
INFOCOMP 2013 : The Third International Conference on Advanced Communications and Computation

by reducing the impact of the unplanned event’s propagation 
and mitigating the impact of waiting time, during which it is 
necessary to find and implement an enduring solution.  
Furthermore, each Policy is implemented by a Process, 
which uses Resources that may be either Standard, i.e., 
resources that are used by default according to the type of the 
production process, or Implemental, i.e., complementary 
resources that are not included in the work-order but it is 
known that they are able to substitute the resource in 
question. The aforementioned process consumes the Planned 
Economic Budget, which is an argument of the Expected 
Policy Pay-Off, taking also into account the Economic 
Expectation Value of the unplanned event. Note that the eEV 
now is determined by the value of expected propagation, 
only because the event has occurred so that p=1 and since the 
work on finding a policy implies that awareness has the value 
a=1.  
Additionally, an appropriate type of the Operations Area 
performs a number of actions on different types of events. To 
be more specific, the Production department is responsible 
for the Missing Resource Events handling and the 
Engineering department for either Non-Conformity Events or 
Change 
Technology 
Events. 
Other 
Domains 
of 
a 
manufacturing company have responsibility for decision-
making in the context of Other Unplanned Events. All of 
these events are types of Unplanned Events. Irrespective of 
the reason, a Missing Resource Event is raised when a 
required resource is not available in planning of production 
or in life production. It may be triggered by a Non-
Conformity Event, a Change-Technology Event or an Other 
Unplanned Event, referring to sources that are not explicitly 
elaborated in the model. 
A Non-Conformity (NC) Event may lead to a Missing 
Resource Event, since at the moment a product is declared 
“non-conformant”, it is no longer available for use during 
production processes. An NC Event is considered to have 
ended when the final solution has been applied, since any 
intermediate solution can only mitigate the propagation of 
the event. A Technology Change Event, which may also 
trigger a Missing Resource Event, implies that a change of 
the used technology has been defined by an engineer and has 
to be implemented in the production line. In addition, each 
Operations Area specifies the responsibility of a Role, which 
may reflect the right or the obligation an actor with a specific 
role may have to act or decide.  
V. HEC AND HPC SUPPORT 
With many modern and often dynamical and interactive 
application scenarios, the term “high performance” is 
covering demanding applications that are on the one hand 
compute- and on the other hand data-centric. It is a common 
understanding that parts of the respective scenarios will 
support the exploitation of parallelism for their implemen-
tation. 
High End Computing (HEC) systems can range from a 
desktop computer, through clusters of servers and data cen-
tres up to high-end custom supercomputers. Resources can 
be physically close to each other, e.g., in a highly performant 
compute cluster, or the compute power can be distributed on 
a large number of computers as with most Grid and Cloud 
computing concepts. Mostly, these architectures are used for 
task-parallel problems in classical capacity computing. 
High Performance Computing (HPC) systems are based 
on architectures with a large number of processors, for 
exploiting massive parallelism. Commonly used models are 
Massively Parallel Processing and Symmetric Multi-
Processing, used with the concept of local islands. Due to 
physically shared memory usage and compute communi-
cation, the physical architectures with these HPC systems are 
different. 
Handling of RM processes will therefore focus on distri-
buted components. Due to the physically different structure 
of highly distributed and massively parallel resources, the 
following aspects can be considered.  
In the case of HEC, e.g., Cloud Computing, these com-
ponents can be system resources acting autonomously like 
servers, being connected by external network means, being 
the ideal resources for events processing at capacity level. 
HEC resources can provide efficient means for massively 
distributed tasks. The non-availability of resources can be 
handled on a job or task base. 
In the case of HPC, e.g., common with Scientific Com-
puting on Supercomputing resources, the components can be 
internal network resources only, compute nodes on the one 
hand, being controlled by a management network and 
software, and management nodes on the other hand. The 
communication intensive modelling especially for the overall 
results and visualisation as well as the pre- and post-
processing for the models will be suitable for use of HPC 
resources. In order to optimise the efficiency and economic 
use of the HPC resources and minimising the effects of job 
size fragmentation these resources should be used for a 
defined class of suitable large tasks within the workflow. 
Available resources can be configured as distributed HPC 
resources within the network provided for the described 
systems. Software Defined Networks (SDN) can provide 
modular solutions for this purpose. 
VI. DISCUSSION  
On corporate level risk is addressed by Enterprise Risk 
Management providing the integrating structure of strategies 
and proceedings as approached by actuarial sciences 
addressing both risk in terms of insurance business and of 
real industry with a comparing idea. “A major challenge here 
is a more substantial and realistic description and modeling 
of the various complex dependence structures between risks 
showing up on all scales” [15]. To our research, related work 
focuses on stochastic models not addressing the ambiguity of 
scenes and therefore neither takes a semantic approach nor is 
able of discrete, event-driven simulation or control. Although 
our work is still in a very early stadium, the industrial use-
cases provide confidence that the particular computational 
approach discussed above at least will add a promising 
strategy to risk management in real economy under 
exceptional circumstances.Conclusion and Future Work 
For operation and management, it has shown appropriate 
to focus on the RM concept and service levels [11] [12]. 
Therefore, MR or CT Events and related processes can be 
103
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-310-0
INFOCOMP 2013 : The Third International Conference on Advanced Communications and Computation

handled with less interference if services are defined and 
interfaces for the processes are created. This is especially 
important for the HEC, HPC, and communication resources 
required. For the HEC processes, this can be done on a 
service level Cloud base, whereas for the HPC resources 
available in research environments, this mostly will have to 
be assisted by service level agreement policies.  
In both fields of semantic modelling and computation of 
industrial landscapes of risk, further work is to be done. One 
issue is that hybrid models require the integration of 
semantic conceptualisation with the mathematics of the neo-
Bayesian school of probability [15]that significantly goes 
beyond the eEV-model used in this paper. Another aspect is 
that the relation between ontological and process-based 
reasoning (things and flows) may have to be revised [9].   
ACKNOWLEDGMENTS 
This work is partially supported by the EU FP7 
Programme, in the frame of the ARUM project, Grant 
Agreement No. 31405.  
REFERENCES 
[1] 
R. Bea, “Approaches to achieve adequate quality and 
reliability,” Technical report, University of California, 
Berkeley, 
2006, 
[Online], 
http://ccrm.berkeley.edu/pdfs_papers/bea_pdfs/quality-and-
reliability.pdf, [accessed: August, 2013].  
[2] 
J. 
D. 
Norton, 
“Spacetime,” 
[Online], 
http://www.pitt.edu/~jdnorton/teaching/HPS_0410/chapters/s
pacetime/, [accessed: August, 2013]. 
[3] 
N. Baumann, “Too fast to fail: is High-Speed Trading the 
next 
Wall 
Street 
disaster?,” 
[Online], 
http://www.motherjones.com/politics/2013/02/high-
frequency-trading-danger-risk-wall-street, [accessed: June, 
2013]. 
[4] 
R. 
Sagaldo, 
“Events 
and 
Spacetime,” 
[Online], 
http://www.phy.syr.edu/courses/modules/LIGHTCONE/even
ts.html, [accessed: March, 2013]. 
[5] 
W. R. Ashby, “An introduction to Cybernetics,” Chapman & 
Hall Ltd. London, pp. 206–212. 
[6] 
ARUM: 
Adaptive 
production 
management, 
[Online]. 
http://www.arum-project.eu/, [accessed: August, 2013].  
[7] 
U. Inden, N. Mehandjiev, L. Mönch, and P. Vrba, “Towards 
an Ontology for Small Series Production,” Industrial 
Applications of Holonic and Multi-Agent Systems, Lecture 
Notes in Computer Science, Springer Berlin Heidelberg, 
2013, pp 128-139, doi: 10.1007/978-3-642-40090-2_12.  
[8] 
K. Ishikawa, “Introduction to Quality Control,” 3A 
Corporation, 1990, ISBN: 4-906224-61-X. 
[9] 
J. A. Rooke, L. Koskela, and D. Seymour, “Producing things 
or production flows? Ontological assumptions in the thinking 
of managers and professionals in construction,” Construction 
Management and Economics, 25 (10), pp. 1077-1085. 
[10] P. Leitão, U. Inden, and C.-P. Rückemann, “Parallelizing 
Multi-gent Systems for High Performance Computing,” 
Proceedings of the International Conference on Advanced 
Communications and Computation (INFOCOMP 2013),  
November 17-22, Lisbon, Portugal, XPS Press, ISSN: 2308-
3484, ISBN: 978-1-61208-310-0 (to appear). 
[11] C.-P. Rückemann, “Enabling Dynamical Use of Integrated 
Systems and Scientific Supercomputing Resources for 
Archaeological Information Systems,” Proceedings of the 
International Conference on Advanced Communications and 
Computation (INFOCOMP 2012), October 2012, Venice, 
Italy, XPS Press, pp. 36-41, ISBN:978-1-61208-226-4. 
[12] C.-P. 
Rückemann, 
“Queueing 
Aspects 
of 
Integrated 
Information and Computing Systems in Geosciences and 
Natural Sciences,” Advances in Data, Methods, Models and 
Their Applications in Geoscience, InTech, 2011, Chapter 1, 
pp. 1-26, ISBN-13: 978-953-307-737-6, doi:10.5772/29337.  
[13] B. Chandrasekaran, J. R. Josephson, and V. R. Benjamins,  
“What are ontologies, and why do we need them?,” 
Intelligent Systems and their Applications, IEEE, vol.14, 
no.1, Jan/Feb 1999, pp. 20-26, doi:10.1109/5254.747902. 
[14] M. Obitko, “Translations between Ontologies in Multi-Agent 
Systems,” Dissertation thesis, Department of Cybernetics, 
Czech Technical University, Prague, 2007.  
[15] J. Neslehová and D. Pfeifer, “Modeling and generating 
dependent risk processes for IRM and DFA,” Astin Bulletin, 
vol. 
34, 
no. 
2, 
2004, 
pp. 
333-360, 
doi:10.2143/AST.34.2.505147.  
[16] N. Collier and M. North, “Repast HPC: A Platform for 
Large-scale Agent-based Modeling”, Large-Scale Computing 
Techniques for Complex System Simulations, W. Dubitzky, 
K. Kurowski, and B. Schott (eds.), Wiley, 2011, pp. 81-110. 
[17] ISO 31000:2009, Risk management -- Principles and 
guidelines, 
[Online], 
http://www.iso.org/iso/catalogue_detail?csnumber=43170,  
[accessed March, 2013]  
104
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-310-0
INFOCOMP 2013 : The Third International Conference on Advanced Communications and Computation

