Knowledge-driven User Activity Recognition for a Smart House.
Development and Validation of a Generic and Low-Cost, Resource-Efﬁcient System
Ismael Duque, Kerstin Dautenhahn, Kheng Lee Koay, lan Willcock and Bruce Christianson
Adaptive Systems Research Group, School of Computer Science
University of Hertfordshire
Hatﬁeld, Hertfordshire, UK
Email: {i.duque-garcia2, k.dautenhahn, k.l.koay, i.willcock, b.christianson}@herts.ac.uk
Abstract—Our core interest is the development of autonomous and
socially interactive robots that may support elderly users at home as
part of a smart home, i.e. a home equipped with a sensor network
that may detect activities of daily living such as preparing food in the
kitchen, having meal in the living room, watching the television, etc.
The current paper focuses on showing the design and implementation
of a low-cost, resource-efﬁcient activity recognition system that can
detect user activities without the necessity of collecting a large dataset
to train the system. Based on common-sense knowledge from activities
of daily living, we generated a set of rules for deﬁning user’s activities
in a home setting. These rules can be edited and adapted easily in
order to accommodate different environments and daily life routines.
The approach has been validated empirically with a pilot study in
the University of Hertfordshire Robot House. The paper presents
results from a study with 14 participants performing different daily
life activities in the house. The results are promising, and future work
will include the integration of this system in a Smart House used
for Human-Robot Interaction studies. This may help develop context-
aware robot companions capable of making better decisions to support
users in their daily activities.
Keywords-Activity Recognition; Smart Houses; Context-Aware
I. INTRODUCTION
In the ﬁeld of Human-Robot Interaction (HRI), many researchers
are interested in understanding how humans interact with robots
in different environments [1]. The incorporation of social skills
into robots’ responses to achieve smoother interaction with humans
remains a signiﬁcant challenge. Many studies (e.g. [2] [3] [4])
from the Adaptive Systems Research Group at University of
Hertfordshire have been carried out with the aim of gathering
ﬁndings that help us understand how people interact with robots in
a domestic environment, and hence to develop robots which exhibit
a greater awareness of context when interacting with humans. The
Robot House (see Figure 1) is the naturalistic environment used by
our research group to perform this variety of experiments.
Fong et al. [5] assume that humans tend to interact with robots
in ways that are similar to how they interact with other humans,
i.e. humans expect certain social characteristics from robots. For
instance, in the area of assistive robotics, the robots will become
part of people’s lives, so these social skills have to be enhanced
during interaction in order to increase robots’ acceptance in these
environments. Context-aware robot companions would have the
ability to detect users’ activities performed at home, but they
require additional modules such as human activity recognition
systems. These supply the necessary information to allow robots
to adapt their behaviour to the ongoing activity, and increase their
social skills aforementioned.
One of the current problems pointed out in the literature re-
garding these systems, is the large variety of datasets necessary to
create accurate activity recognition systems [6], and the difﬁculties
in recruiting participants for the experiments [7]. We therefore
developed a different method to avoid involving users in extensive
studies of data collection during the whole process of system
development. This point is particularly important when working
with elderly people or people with special needs, which are one of
the target user groups that our research is concerned with. Asking
e.g. elderly people to spend several days or weeks engaged in
certain activities to generate training data for the system puts a
huge burden on them. The Activity Recognition System (ARS)
that will be presented in this paper takes into account this issue.
The knowledge-driven approach [8] used allowed us to develop
the low-cost, resource-efﬁcient system, in which participants were
involved just during the validation stage.
Our research follows two well-deﬁned directions. Firstly, the
incorporation of social skills in robot companions to create more
natural human-robot interactions in living environments. Smart
homes’ facilities will help to develop these skills (e.g. the non-
intrusive sensor network installed in the Robot House). As Chan et
al. [9] mentioned, sensor-embedded houses provide context infor-
mation without disturbing users’ daily activities, creating greater
comfort and well-being. Secondly, we avoid the involvement of
users during the training phase of the development of the system
by the use of knowledge-driven approach. Following these two
directions, we have created a functional activity recognition system
that was tested with 14 participants in its validation stage.
The remainder of this paper is organised as follows: Section 2
discusses related work. Section 3 presents the research question
and goals. Section 4 describes how the activity recognition system
has been created, and the structure of the set of rules deﬁned
on our system. Section 5 describes design and procedure of
the experiments carried out. In Section 6, the analysis and the
evaluation of these experiments are depicted. Section 7 reviews
how the research questions have been accomplished. Finally, we
conclude this paper in Section 8.
II. RELATED WORK
The HRI ﬁeld as a distinct branch of academic activity ﬁrst
emerged in the mid 1990s, although the robot’s behaviour and
their consequences for humans have been studied in several ﬁelds.
Goodrich et al. [10] present a survey of current and historical
research into HRI. The ﬁeld is focused on studying robotic systems
that interact directly or indirectly with humans. The understanding,
evaluation and appropriate design of these systems should facilitate
satisfying and naturalistic social interaction between robots and
humans. For an assistive robot to be useful for its user at home,
the ability to recognize and respond to human activities is essential.
As we mentioned in Section I, the integration of tools such as
human activity recognition systems is a ﬁrst step towards the target
of naturalistic interaction between users and robots. In the ﬁeld of
Smart Houses, we can ﬁnd a huge variety of activity recognition
studies, but relatively few are oriented towards robot companions
and take into account the need for a reduction of time invested
by users in the development of such systems, or the realistic
experiments conditions pointed out by Logan et al. [11]. Our ARS
has been designed and evaluated based on these principles.
141
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

In the literature, two main categories can be found regarding
activity recognition systems [8]. The ﬁrst is based on visual
sensors, e.g. camera-based systems to monitor behaviours and
changes in the environment [12] [13]. This approach combines
computer vision techniques and pattern recognition. The second
category is based on sensor networks for monitoring activities in
Smart Houses. It can be subdivided into data-centric, logical or
semantic approaches. These approaches typically require extensive
data collection with potential users of such systems. The data is
then analysed using data mining or machine learning techniques to
build activity models, which can then form the basis for activity
recognition systems. The knowledge-driven, rule-based system
approach that we describe in this article belongs to this second
category. Similar approaches can be found in the literature [14]
[8], but in their evaluation stages participants were told to perform
certain activities following a sequence of actions. The approach
here presented is capable of recognizing user activities without
restricting the way in which users perform those. The use of the
non-intrusive sensor network installed helped us create the natural
environment the we were looking for. In our view, wearable sensors
could affect users’ comfort and seem particularly problematic for
elderly people.
Other issues have been taken into consideration as well. The
system was designed to be easy to move and install in other similar
environments without the necessity of specialized knowledge on
how these systems work and need to be set up. The rules and
sensors are deﬁned in the conﬁguration ﬁles (see Section IV-B),
followed by a natural language description in order to make the
system more understandable. A key advantage of this approach,
is that the rules are explicitly represented rather than implicitly
represented (e.g. within a Bayesian network [15] [16] or a Hidden
Markov Model implementation [17] [6]). This allows us to inspect
and manually change or update the rules if needed. As part of
the ACCOMPANY project [18], our research in the Robot House
will be incrementally developing more complex HRI scenarios for
home assistance, so it is important for us to be able to have a
system that can be extended and modiﬁed easily by non experts,
and at the time, keeping the development cost of the system down.
We argue that developing a low-cost and resource-efﬁcient system
(e.g. the ARS presented), is an important prerequisite for a possible
future use in real world applications.
III. RESEARCH QUESTIONS AND GOALS
The purpose of this article is to present the development and
implementation of the knowledge-driven ARS system and its ﬁrst
validation study. The comparison between the activities recognized
by the system, and the actual, observed activities performed by
the user during several sessions, will determine the accuracy level
of the system and its capacity to be integrated into future HRI
studies. The data collected in the ﬁrst validation study will be
used to improve the ﬁrst set of system parameters and to suggest
new features for future versions of the system. In addition, we
try to learn about users’ behaviour in a natural home situation,
and understand how robot companions could behave in such home
environments. Our research questions are:
• Q1. Is our ARS generic enough to detect different users’
activities without the system being individually trained for
the users?
• Q2. Can the ARS achieve an accuracy higher than 80% in the
controlled experiments?
• Q3. Can the ARS achieve an accuracy higher than 80% in the
uncontrolled experiment?
• Q4. What are the advantages and disadvantages of the ARS
presented in this paper?
Dining Area
21
20
43
Living Room
19
57
49
18
Bedroom
Office
15
16
Bathroom
Kitchen
Hall
14
12
11
13
55
1
2
37
36
42
59
23
35
52
33 32 31
50
40
39
38
6
7
10
8
9
44
54
56
51
3
4
5
25 24
30 29 28
26
27
34
53
17
Figure 1. The UH Robot House layout and sensor arrangement. 59 sensors
are available in the house, but only 52 were used and shown here. The two
cameras’ locations during the experiments are represented in this picture.
The percentages deﬁned in questions 2 and 3, have been set
at these values in order to validate the system with an adequate
conﬁdence level. This will ensure a reasonable reliability of the
environmental information that will be sent to robot companions
in future HRI experiments. An accuracy over 80% seems sufﬁcient
since robots’ behaviour will not solely be based on the information
received from the ARS, but supported by the Robot House’s system
that makes decisions based on further environmental information.
Therefore, we expect that this additional information supplied by
the ARS will help us improve the robot’s awareness of the situation
and thus further enhance its abilities when interacting with users
in a living environment.
IV. HUMAN ACTIVITY RECOGNITION FRAMEWORK
A. Robot House Sensor Network Description
Two different but complementary commercially available sensor
systems, the GEO System and ZigBee Sensor Network, were
installed in the Robot House. Both the GEO System and ZigBee
Sensor Network have a refresh rate of 1 Hz, which is deemed as
adequate to detect user activities.
The GEO System [19] is a real-time energy monitoring system
for electrical devices. It is used to detect the activation and
deactivation of electrical appliances by the Robot House’s users
(e.g. opening the refrigerator or boiling water in a kettle). The
status of the electrical appliances connected to this system can be
queried from the GEO System database.
The ZigBee Sensor Network [20] is used to detect user activity
that cannot be detected by the GEO System such as opening of
drawers and doors, occupation of chairs and sofa seat places,
opening of cold and hot water taps etc. The ZigBee Sensor Network
consists of ﬁve ZigBee Wireless modules, which are spread across
the Robot House. Together they transmit readings from a total of 26
reed contact sensors, 4 temperature sensors and 10 pressure mats
to a ZigBee gateway (XBee Gateway X4). The ZigBee gateway
forms an interface between ZigBee Sensor Network and the Robot
House Ethernet infrastructure, where the ARS resides.
142
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

Table I
BEHAVIOUR CODING SCHEME. ACTIVITIES CONSIDERED FOR THE
ACTIVITY RECOGNIZER.
B. Implementation
The ARS was developed in Java with a local MySQL database
for logging purposes. The software consists of the following four
different modules:
• ZigBee module. Manages sensor data from ZigBee Sensory
Network.
• GeoSystem module. Pulls sensor data from GEO System
Database.
• Activity Recognizer module. Analyses the sensory data re-
trieved from the ZigBee Module and GEO System Module to
determine the user’s activity.
• User Interface module. Displays and records the detected
user’s activities and sensory information to a local database
(MySQL) and external log ﬁles.
The ARS has been tested on both Linux and Windows systems,
with a local MySQL database for data logging purposes. The
system is conﬁgured by using two XML ﬁles. The ﬁrst conﬁg-
uration ﬁle contains the representation of the Robot House sensor
network (i.e. mapping sensors’ IDs to their symbolic names), and
the second conﬁguration ﬁle deﬁnes the semantic rules used by
the ARS to detect user’s activities in the Robot House (see Section
IV-C). These rules were set based on an initial set of trials and the
common-sense knowledge which activities of daily living (ADL’s)
are based on [8]. In future work, the parameters could be reﬁned
based on the information gathered after this study. We have to
consider that this ﬁrst experiment is part of the learning process
that we have to follow to achieve our ﬁnal research goals.
Two issues have to be pointed out in regards to the system.
Firstly, the ARS is intended to trigger and present an identiﬁcation
at the starting point of the activities studied (see Table I). We
consider that the beginning of each activity is the suitable moment
at which robot companions should interact with users to offer their
help. Secondly, the possibility of migrating the system to other
similar environments has been considered during the development
process, so that the editing, redeﬁnition or adaptation of these two
conﬁguration ﬁles would be sufﬁcient to run the system in these
new environments.
C. Rule Deﬁnition Example
In this section, we show brieﬂy how the ADL’s rules have
been deﬁned following common-sense knowledge which make the
system understandable to any researcher using it. We studied a
variety of activities that will be useful in assistive robotics scenarios
in future stages (see Table I). These activities can be described
as the combination of sensors activated in the environment, and
previously performed activities, namely context-activities. Thus,
the system manages two different kinds of activities. Low-level
activities are those that are detectable by a single ﬁxed sensor (e.g.
the user sitting on the sofa). High-level activities are those that can
only be detected by utilising a combination of different sensors, or
a combination of different sensors and low-level activities detected.
Based on that, each rule is deﬁned using the following tags:
• Duration: The maximum time the activity remains activated in
the system. Some activities, e.g. Using Computer Dining Area
and Sitting Living Room (described below), do not consider
this tag as they are deactivated based on their associated
context-activities or associated sensors’ status values.
• Location: The location where the activity is performed.
• Context: Set of activities that has to be fulﬁlled before the ac-
tivity is activated. Some activities, e.g. Sitting Living Room, do
not have any context-activity associated with them. Interval:
Time window in which the context-activity is relevant for the
detection of the activity. Status: The required context-activity’s
state for the activation of the activity.
• Threshold (Sensors’ attribute): Minimum value necessary to
consider the activity as activated. It is based on the accumu-
lated weight of the sensors triggered.
• Sensors: Each of the sensors involved directly in this activity.
They have a Status, NotLatching (True: The sensor’s weight
will be only added to the accumulated weight while it remains
on, otherwise, its weight is subtracted from the accumulated
weight; False: the sensor’s weight is added to the accumulated
weight once it is on regardless of its later state), and Weight
ﬁelds. Some activities, e.g. Using Computer Dining Area, do
not have any sensors associated with them.
We can see below the examples rule Using Computer Dining
Area and Sitting Living Room. More examples are available from
the author on request:
<Activity Name="Using_Computer_Dining_Area">
<Duration>Nil</Duration>
<Location>Dining_Area</Location>
<Contexts>
<Context Interval="0" Status="activated">
Sitting_Dining_Area</Context>
<Context Interval="0" Status="activated">
Computer_ON</Context>
</Contexts>
<Sensors Threshold="0.0"></Sensors>
</Activity>
<Activity Name="Sitting_Living_Room">
<Duration>Nil</Duration>
<Location>Living_Room</Location>
<Contexts></Contexts>
<Sensors Threshold="0.50">
<Sensor Status="on" NotLatching="true" Weight="50">
Sofa seatplace 0</Sensor>
<Sensor Status="on" NotLatching="true" Weight="50">
Sofa seatplace 1</Sensor>
/Sensors>
</Activity>
In the ﬁrst example, Using Computer Dining Area, the activity
depends on Sitting Dining Area and Computer On, but no sensors
are associated with the activity recognition. For this reason, Du-
ration and Threshold tags are not considered for this activity, as
the activity will be activated only when both context-activities are
activated. In the second example, the activity is associated with
143
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

Table II
THE OBSERVER XT FORMATTED OUTPUT (LEFT SIDE) AND THE ACTIVITY RECOGNIZER’S EVENT LOGS (RIGHT SIDE). THIS DATA REPRESENTATION
HELPED US ANALYSE THE RESULTS AND FIND BEHAVIOUR PATTERNS THAT WILL BE CONSIDERED IN FUTURE WORKS.
certain sensors, whose NotLatching ﬁeld makes their activation
compulsory to keep the activity activated as well. Therefore,
Duration is not considered for this activity, since the deactivation
of the associated sensors will deactivate the activity.
V. EXPERIMENTAL DESIGN AND PROCEDURE
A validation study was conducted by the Adaptive Systems
Research Group at University of Hertfordshire in May 2012 to
measure the accuracy of the framework previously explained. The
Robot House provides a naturalistic and ecologically acceptable
environment to carry out studies into ADL’s. The main aim was to
measure the accuracy of the system in both controlled and uncon-
trolled scenarios and collect data for future studies. A sample of
14 adults, unafﬁliated with the ongoing research, and aged between
23 and 54 was recruited from students and staff of the University
of Hertfordshire. All the subjects ﬁrst completed a consent form,
in which they were informed about the voluntary nature of the
experiments, before they performed a two-day experiment, one
session per day. Each session lasted approximately 20 minutes.
A. Experimental Setup
The experiments took place in the Robot House in which ARS
were installed and conﬁgured. All the experiments were recorded
on video and audio using two different cameras (see Figure 1)
rather than relying on self-reporting. One camera covered the
dining area and living room, and the other covered the kitchen.
Those were the only rooms where the participants performed the
experiments. The cupboards were labelled to make the participants
aware of every object’s location and create a more natural envi-
ronment in the sense of knowing where things are located, as they
would feel in their own houses. However, users got used to the
Robot House facilities after the introductory session as will be
explained in the next section.
The ARS generated two different log ﬁles for each participant,
one per session. The ﬁrst ﬁle stored information on all the sensors
activated and deactivated during the experiment, as well as the
decision-making process that the activity recognition algorithm was
doing in real time. The second ﬁle represents the raw sensory data
received from the system during the experiment. These raw data
can be used to simulate users living in the Robot House in future
experimental scenarios in which robots will be included.
B. Experimental Procedure
The experiments were led by the researcher, who introduced and
explained the procedure and the house’s facilities to each subject.
This section took approximately 10 minutes and was only provided
for the ﬁrst session. After this introductory part, during the ﬁrst
(controlled) session the participants were led by the researcher
for 20 minutes, while they were asked to perform a number of
speciﬁc common ADL’s using the Robot House’s facilities in
the way in which they felt most comfortable with. Thus, they
were told what activity to perform, but not how to perform it.
In the second (uncontrolled) session, we told the participants to
spent around 20 minutes simulating ’living’ in the house. They
were asked to perform whichever activity (based on the facilities
shown during the introductory session) they wished during this
period of time. Consequently, we exposed the system to two
different situations, controlled and uncontrolled, which would help
us measure the system’s accuracy and analyse human behaviour at
a home environment and discover details omitted in the system,
respectively. After each session, the participants were asked to
complete a questionnaire. They rated the scenarios and the activities
in which they were involved. Basic demographic information of
each participant was collected in this questionnaire as well. Note,
the order of the conditions was not counterbalanced, since the goal
of the study was not to compare the two conditions. Also, it seemed
important to ﬁrst expose participants to the controlled condition
which helped them to prepare themselves for the uncontrolled
condition.
VI. ANALYSIS AND EVALUATION
A. Behaviour coding
Relatively little work (e.g Logan et al [11]) has combined
behaviour coding with user activities in Smart Houses. However,
many examples of different data annotation studies can be found in
the ﬁeld of HRI, e.g. [21], and Psychology, e.g. [22]. The coding of
the video data of the participants activities helped us analyse each
session and identify the important events which we were interested
in. The Observer XT software supplied by Noldus Information
Technology [23] is a commercial software package used for coding,
analysis and presentation of observational data.
The ﬁrst author of this article was the ﬁrst coder of all the
video material. Additionally, following conventions of behaviour
coding, a second coder carried out the same process with 10% of
the analysed videos in order to perform the reliability test. The
Observer XT and the coding scheme shown in Table I were used
by both coders, who were asked to familiarize themselves with this
coding scheme before the annotation process. They were told to
code activities in which users interacted with some of the sensors
installed in the Robot House, in order to generate the sequence of
activities that each user had been performed. The outcomes were
144
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

exported to an Excel ﬁles in order to be compared to the events
generated by our ARS during the analysis stage.
1) Inter-rater Reliability Test: The Kappa Statistic [24] was used
to determine the level of agreement between the two different anno-
tations carried out by the two coders. The annotations were paired
in Observer XT, and the kappa value was generated automatically
for both sessions. The time windows for the reliability analysis was
deﬁned as one second. The kappa value for the combined analysis
was 0.75, with overall agreement of 76%. This result represents a
good agreement rate for both annotations [25].
B. Data Analysis
A ﬁnal Excel ﬁle was built based on the event lists created using
Observer XT and the events generated by our ARS (see Table II).
The left side of the table represents the events exported from the
software. On the other side, the activities recorded by the system
were written down together with their starting time. In this way, the
results were shown clearly, and allowed to distinguish ’recognized’,
’missed’, or ’extra-recognized’ activities more easily. The last
category represents those activities that fulﬁlled all the sensor’s
activations required but they were not performed by the user as
evident in the video data. In future experiments, the interaction
between the robot companion and the user will help us clarify the
real status of these kinds of activities. Moreover, additional tools to
support our ARS will be integrated into the Robot House’s system
during the ACCOMPANY project [18].
A total of 14 participants and two sessions per participants have
been considered for the data analysis. We will explain each session
separately. The system performance was calculated in terms of
precision, recall and accuracy [26] (see Figure 2).
Precision =
tp
tp + fp
Recall =
tp
tp + fn
Accuracy =
tp
tp + fp + fn
Figure 2.
Precision, recall and accuracy formulas. (tp = true positives or
’recognized’, fn = false negatives or ’wrongly recognized’ and fp = false
positives or ’extra-recognized’).
1) Session 1 (controlled): We have to remember that in this
scenario the user was lead by the researcher, as we described in
Section V. A total of 240 events were coded in all the experiments
carried out in this session. The average number of performed activ-
ities per user was 17. We got 239 correctly recognized activities, 1
missed activity, and 37 extra-recognized activities were triggered.
We obtained a precision of 86,59%, a recall of 99,58% and an
accuracy of 86,28%. We found some delay in the recognition
of the most complex activities, i.e. those activities involving a
major number of different sensors (e.g. preparing food or preparing
a beverage). The rest of the activities were recognized with an
average delay of two seconds, which is reasonably fast, taking into
account the operating system frequency 1Hz.
2) Session 2 (uncontrolled): In the second session, good overall
results were achieved too, even taking into account the openness of
the scenario which we exposed our system to. A total of 216 events
were coded in the experiments carried out during this session. The
average number of performed activities per user was 15. We got 200
correctly recognized activities, 16 missed activities and 23 extra-
activities were triggered. We obtained a precision of 89,69%, a
recall of 92,59% and an accuracy of 83,68%. As stated before,
some delay were found on the most complex activities. In Figure
3, we represent these averages delays per activity (e.g. Preparing
Hot Drink was recognized with a delay of 35 seconds). The rest
of activities were recognized with a similar average delay than in
Session 1. The data collected along this experiment will help us
understand human behaviour at home and improve our system.
VII. DISCUSSION
The results presented above allow us to answer the research
questions presented in Section III. The approach followed has
demonstrated the possibility of creating a low-cost, resource-
efﬁcient ARS and presenting it to real users without the necessity
of previous training. This is directly related to the reduction of
time spent by participants in HRI studies as it was mentioned
in Section I. The accuracy in both controlled and uncontrolled
sessions exceeded the 80% threshold previously deﬁned in our
research questions, which was considered as adequate for the
kind of study. Some of the advantages presented by this approach
are the creation of a non-restricted and naturalistic system that
allows users to behave as they would in their own houses. As we
mentioned, in other approaches experiments were typically much
more constrained. The use of hidden, non-intrusive sensors installed
around the Robot House helped us create this natural environment,
as we focussed on avoiding wearable sensors that could make users
uncomfortable. In addition, the system can be easily migrated and
setting in a similar environment without the necessity of specialized
knowledge. The rules and sensors were deﬁned using a natural
language in order to make the system more understandable.
On the other hand, the system does have some disadvantages.
Firstly, the types of sensors currently used do not allow to deter-
mine accurately where the user is located in the house. Therefore,
the recognition of activities for two or more users simultaneously
cannot be detected directly, as the system is not able to match
activities with users. An extra tool (e.g. a camera recognition
system) may solve the problem, so that it will be considered in
future work. However, this will increase cost and complexity of
the system and involve privacy issues. Secondly, the semantic rules
used by the ARS were deﬁned based on common-sense knowledge
of how a person would carry out the ADL’s. A module to modiﬁed
these initial deﬁnitions as the user interact with the system will be
considered in future stages of our research.
Once the ARS has been integrated into the Robot House system,
we will be able to create much richer scenarios in which robot
companions will be aware of users’ activities. This will allow
us to adapt robots’ behaviour to their needs in each situation,
 
00:00
00:04
00:09
00:13
00:17
00:22
00:26
00:30
00:35
Using Toaster
Using Kettle
Preparing Food
Preparing Cold Drink
Preparing Hot Drink
Computer ON
Using Computer
Sitting Dining Area
Laying Table
Having Meal Dining 
Area
Spare Time Dining 
Area
Watching TV
TV ON
Sitting Living Room
Spare Time Living 
Room
Having Meal Living 
Room
Cleaning Table
Delay (s)
Figure 3.
Overall delay per activity in the uncontrolled scenario.
145
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

and increase robot companions’ autonomy to make decisions. A
variety of challenging studies will be targeted in future stages of
our research.
VIII. CONCLUSION AND FUTURE WORK
We have presented the development and validation of a
knowledge-driven rule system to identify user activities in home
scenarios. We tried to build a low-cost, resource-efﬁcient and
easily understandable and re-conﬁgurable system that is accurate
enough to detect a set of ADL’s. This approach was evaluated
empirically by means of the studies carried out in the Robot House.
The experimental environment allowed participants to behave in a
similar way that they would in their own homes, as it was reported
in the questionnaires. Although the participants did not belong to
our target user group, i.e. elderly people, we claim that, due to the
general design of our system, the results can be generalized, and
if necessary, can be easily adapted to this users group. In future
work, the adaptation to individual users and their speciﬁc life styles
and routines may also be considered. The results achieved fulﬁl
our expectations and answer fully the research questions deﬁned
in Section III. These ﬁndings motivate us to progress towards our
ﬁnal research target of designing context-aware companion robots
for home environments. It can be concluded that the developed
ARS could be integrated into future experiments of our research.
ACKNOWLEDGEMENT
The research leading to these results has received funding from
the European Union’s Seventh Framework Programme (FP7/2007-
2013) under grant agreement number 287624, the ACCOMPANY
project [18].
REFERENCES
[1] K. Dautenhahn.
Socially Intelligent Robots: Dimensions of
Human–Robot Interaction.
Philosophical Transactions of the
Royal Society B: Biological Sciences, 362(1480):679–704, 2007.
[2] K. L. Koay, D. S. Syrdal, M. L. Walters, and K. Dautenhahn.
Living with Robots: Investigating the Habituation Effect in
Participants’ Preferences During a Longitudinal Human–Robot
Interaction Study. ROMAN’07, pages 564–569. IEEE, 2007.
[3] K. L. Koay, D. S. Syrdal, M. L. Walters, and K. Dautenhahn. Five
Weeks in the Robot House - Exploratory Human–Robot Interac-
tion Trials in a Domestic Setting. In Proceedings of the 2009 Sec-
ond International Conferences on Advances in Computer-Human
Interactions, ACHI’09, pages 219–226, Washington, 2009.
[4] M. L. Walters, K. Dautenhahn, R. Te Boekhorst, K. L. Koay,
D. S. Syrdal, and C. L. Nehaniv. An Empirical Framework for
Human-Robot Proxemics. AISB2009, pages 144–149, 2009.
[5] T. Fong, I. Nourbakhsh, and K. Dautenhahn. A Survey of Socially
Interactive Robots.
Robotics and Autonomous Systems, 42(3-
4):143 – 166, 2003.
[6] T. van Kasteren, A. Noulas, G. Englebienne, and B. Kr¨ose.
Accurate Activity Recognition in a Home Setting. In Proceedings
of the 10th international conference on Ubiquitous computing,
UbiComp ’08, pages 1–9, New York, NY, USA, 2008.
[7] Z. Z. Bien, H. E. Lee, J.-H. Do, Y. H. Kim, K.-H. Park, and
S.-E. Yang. Intelligent Interaction for Human-Friendly Service
Robot in Smart House Environment.
International Journal of
Computational Intelligence Systems, 1(1):77–94, 2008.
[8] L. Chen, C. D. Nugent, and H. Wang.
A Knowledge–Driven
Approach to Activity Recognition in Smart Homes.
IEEE
Transactions on Knowledge and Data Engineering, 24:961–974,
2012.
[9] M. Chan, D. Estve, C. Escriba, and E. Campo. A Review of Smart
Homes-Present State and Future Challenges. Computer Methods
and Programs in Biomedicine, 91(1):55 – 81, 2008.
[10] M. A. Goodrich and A. C. Schultz. Human-Robot Interaction:
a Survey. Found. Trends Hum.-Comput. Interact., 1(3):203–275,
January 2007.
[11] B. Logan, J. Healey, M. Philipose, E. M. Tapia, and S. Intille.
A Long–Term Evaluation of Sensing Modalities for Activity
Recognition. In Proceedings of the 9th international conference
on Ubiquitous computing, pages 483–500. Springer-Verlag, 2007.
[12] J. Hoey and J. J. Little.
Value–Directed Human Behavior
Analysis from Video Using Partially Observable Markov Decision
Processes.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 29(7):1118–1132, July 2007.
[13] T. B. Moeslund, A. Hilton, and V. Kr¨uger. A Survey of Advances
in Vision-Based Human Motion Capture and Analysis. Comput.
Vis. Image Underst., 104(2):90–126, November 2006.
[14] S. Holger, M. Becker, and M. Riedl.
Rule-Based Activity
Recognition Framework: Challenges, Technique and Learning. In
PervasiveHealth, pages 1–7. IEEE, 2009.
[15] E. Tapia, S. Intille, and K. Larson. Activity Recognition in the
Home Using Simple and Ubiquitous Sensors. In Alois Ferscha
and Friedemann Mattern, editors, Pervasive Computing, volume
3001 of Lecture Notes in Computer Science, pages 158–175.
Springer Berlin / Heidelberg, 2004.
[16] L. Bao and S. Intille. Activity recognition from user–annotated
acceleration data.
In Alois Ferscha and Friedemann Mattern,
editors, Pervasive Computing, volume 3001 of Lecture Notes in
Computer Science, pages 1–17. Springer Berlin, 2004.
[17] D. Sanchez, M. Tentori, and J. Favela.
Activity Recognition
for the Smart Hospital. Intelligent Systems, IEEE, 23(2):50 –57,
March 2008.
[18] ACCOMPANY: Acceptable robotiCs COMPanions for AgeiNg
Years. Project No. 287624.
http://www.accompanyproject.eu/,
October 2011. [Online; accessed 02.12.2012].
[19] GEO: Green Energy Options. http://www.greenenergyoptions.co.
uk/. [Online; accessed 02.12.2012].
[20] S. Farahani. ZigBee Wireless Networks and Transceivers. Newnes,
Newton, MA, USA, 2008.
[21] K. L. Koay, K. Dautenhahn, S. N. Woods, and M. L. Walters.
Empirical Results from Using a Comfort Level Device in Human-
Robot Interaction Studies. In Proceedings of International Con-
ference on Human Robot Interaction, pages 194–201, 2006.
[22] J. L. Flenthrope and N. C. Brady.
Relationships Between
Early Gestures and Later Language in Children With Fragile X
Syndrome. Am J Speech Lang Pathol, 19(2):135–142, 2010.
[23] Noldus
Information
Technology.
The
Observer
XT
Soft-
ware. http://www.noldus.com/human-behavior-research/products/
the-observer-xt/. [Online; accessed 02.12.2012].
[24] J. Sim and C. C. Wright.
The Kappa Statistic in Reliability
Studies: Use, Interpretation, and Sample Size Requirements.
Physical Therapy, 85(3):257–268, 2005.
[25] R. Bakeman and J. M. Gottman.
Observing Interaction: An
Introduction to Sequential Analysis. Cambridge University Press,
1997.
[26] D. L. Olson and D. Delen. Advanced Data Mining Techniques.
Springer Publishing Company, Incorporated, 1st edition, 2008.
146
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

