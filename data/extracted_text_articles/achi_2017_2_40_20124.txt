A Comparative Assessment of User Interfaces for Choreography Design
Tafadzwa Joseph Dube, G¨okhan Kurt, G¨okhan ˙Ince
Computer Engineering Department
Istanbul Technical University
Istanbul, Turkey
Email: dube@itu.edu.tr, kurt@itu.edu.tr, gokhan.ince@itu.edu.tr
Abstract—Choreography design is a vital creative art that beneﬁts
vastly from digital applications. In this study, we investigate the
effects of different interaction techniques on user experience for
a choreography generator interface. We develop an augmented
reality choreography generator and compare it with 1) a personal
computer based choreography generator and 2) a mobile applica-
tion for choreography generation. We evaluate user performance
and user experience on the interfaces in terms of task completion
times, as well as subjective criteria, such as mental stress, physical
stress, and pleasure experienced. Our research contributes to the
study of how different interaction methods of the same application
affect user experience. The paper also contributes to human-
computer interaction in education and training. The results verify
the effectiveness of augmented reality in developing training and
design applications.
Keywords–choreography; augmented reality; user experience;
natural user interfaces.
I.
INTRODUCTION
Choreography is a creative process that has evolved over
years. With the advent of technology, different ways that
embraced digital tools for choreographing emerged. Thus,
allowing choreographers to beneﬁt a lot from this evolution. It
is imperative to develop interfaces that are robust and suite
well for hand-held devices and computers. Furthermore, to
enhance user experience in choreographing tools it is important
to design human friendly user interfaces.
One emerging ﬁeld of Natural User Interfaces (NUI) shows
much promise shows much promise in the Human Computer
Interaction(HCI)ﬁeld. The approach put more emphasis on
developing interfaces that allow users to perform tasks in a
natural way, including touch based techniques, gestures and
voice; it runs away from the traditional approach of using key-
board and mouse [1]. In this study we develop interfaces that
utilize the traditional interactive approach and also interfaces
that seek aspects of NUI. NUIs have a pivotal role to play in
HCI in education and training.
Augmented Reality (AR) presents an interesting approach
to interface development. AR coupled with touch screen
presents aspects of NUI, which potentially improves user
experience. AR combines the real world and the virtual worlds,
it provides interaction with 3 dimensional objects superim-
posed onto a real world [2]. AR provides a simultaneous
way for users to interact with the virtual and real world.
With the surge in mobile devices, mobile AR is quickly
gaining momentum [3]. This growth in mobile AR applications
has been facilitated by the increasing processing power of
modern day mobile devices [4]. The ability to process intensive
applications has seen a swift shift from the bulky hardware that
has symbolized AR since the 20th century. AR promises to be a
major player in interface design in the 21st century. Whenever
3D objects appear superimposed onto the real world, a novel
experience is created that adds pleasure and creates amazement
and curiosity to the user [5]. This has seen AR being embraced
by the mobile gaming industry. The year 2016 saw a major
shift for AR in the gaming industry with the launch of
“Pokemon Go” that has revolutionized how people viewed
AR. New York Times suggests that Pokemon go represents the
moment AR breaks through the current position to something
bigger [6]. As the trend continues, it is imperative to embrace
AR in interface development. Technological advances have
allowed cheaper ways to develop AR applications.
Over the years different digital platforms have been devel-
oped for choreography generation. 3D animation is a preferred
choice for dance learning [5]. Furthermore, choreographers
have been fascinated by the use of digital tools when per-
foming their tasks. The use of these digital tools encourages
choreographers to observe the creative compositional process
as new through conﬁnes alongside new possibilities [7]. In
this study, we complement existing choreographing tools by
adding AR. Several researchers have suggested that AR can
aid reinforce motivation of students and trainees through
improving their educational realism [8]. AR applications in
mobile platforms provide a lot of promise with respect to
training and planning [9].
In this study, we design, implement and compare the
mobile AR interface with 1) the Personal Computer (PC)
based choreography generator and 2) the mobile application for
choreography generation. Our aim is to investigate the effects
of different interactive techniques of the same application on
user experience. We also investigate how AR affects user
experience on training tools. The results of the experiments are
important in understanding the impacts of different interaction
techniques on user experience, as well as understanding the
effects of AR on training applications. In this study, we
provide an important contribution presenting a choreography
application bridging the gap in the creative technological ﬁeld.
This study also contributes in evaluating aspects of NUI with
respect to user experience.
The rest of the paper is structured in this way; Section
2 discusses related work. Section 3, 4 and 5 discuss the PC
based, the mobile application and the AR based interfaces
respectively. In Section 6, we discuss the experiments that we
carried out and share the results obtained. Finally, in Section
7 we discuss the results, challenges and future work.
53
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

II.
RELATED WORK
As the digitization of choreographing continues, different
approaches have been explored to fully embrace technology for
this creative process. However, the lack of standards with re-
gards to development of choreograph applications slows down
the advances of the development process of these applications.
Therefore, as attempts to digitize choreography appear many
open questions in relation to the support tools and consensus
on the standards exist [7]. Standardizing choreography appli-
cations remains a major challenge. This has seen different
choreographing tools being developed. Lack of standards al-
lows creativity thereby encouraging more developers to come
up with different idea. However, at the same time this leads to
development of applications that lack a proper structure.
As early as the 1960s digital tools were introduced for
choreography generation. The ﬁrst choreographing computer
system was developed in 1967. It was inﬂuenced by the need
to create dance annotations without the need of a physical
space or the physical dancers [10]. The system developed made
use of a two dimensional interface, which utilized stick ﬁgure
representation of dancers displayed on the computer screen.
The choreographers crafted dance annotations using different
buttons and controls to control the stick ﬁgure representations.
The system played an important role in crafting a way for
choreographing in the digital space. It was initially developed
for ballet dancing, although with time it included other chore-
ographs. However, the major weakness of such a system is
dependence on 2D which hinders the natural feel of the inter-
face. Furthermore, early systems suffered from limited input
techniques and low processing power of computers. However,
the modern century has seen an increase in processing power
and the availability of different input techniques.
As digital tools evolved augmented reality has been in-
coperated in different applications. AR has gained a lot of
use in teaching, training and visualization for many different
institutions over the last decade [9]. Choreography has also
embraced AR and virtual reality in recent years. An interactive
mixed reality system for stage management and choreography
is mentioned in [11]. The system developed is a hybrid chore-
ography system that uses both virtual reality and augmented
reality to achieve the desired goals. The system facilitates plan-
ning for stage shows and events. It makes use of head mounted
devices to allow the choreographer to design choreographs for
3D generated characters. Furthermore, 3D pops can be used
in stage set up to visualize how the real stage will look like.
These 3D props include different stage set up items like drums
and guitars. Choreographers can deﬁne different choreographs
and play them on the miniature stage. Interior designers can
also use this mixed reality stage. However, the major downside
of this approach is the dependence on bulky and expensive
hardware. Furthermore, it requires a lot of space for setting
up the miniature stage, making it difﬁcult for mobile users.
However, this cumbersome approach to AR has been improved
by recent advances in mobile hand-held devices like smart
phones [12].
Another digital approach to dance learning that is based
on a Web3D environment is described in [13]. This interactive
system realizes dance animations for training and education.
Using the application a choreographer can compose various
dance annotations. The Web3D environment is effective for in-
teractive dance steps observation, slow movement of fast steps
and different angles of view. The choreographer is presented
with a rich interface that enables the creation of choreographs
with a good zooming level. The system enables the dancer
to learn the annotations deﬁned by the choreographer from
different angles and speed. This 3D animation environment
provides an easy way for dancers and choreographers to
perform their work without the need of a physical platform.
However, the interface is effective on desktop browsers. In this
approach portability is a challenge.
Much work has been conducted to compare different
interactive techniques. A study mentioned in [1] investigates
the usability of the mouse-based and touch based interactive
approaches in manipulating objects in a 3D virtual environ-
ment. In the study, they measure subjective aspects such as
fatigue, workload, and preference. The researchers used dock-
ing tasks on participants to accomplish the investigation. The
experiments were conducted in a well controlled environment,
which allowed the users to continuously give feedback. The
results were important in showing that the two approaches
provide relatively similar levels of precision, however time of
interaction differs. The subjective results also showed which of
the interactive techniques users preferred. The study conducted
contributed to the study of interactive techniques and usability
of applications. However, as per our knowledge there is no
work comparing the different interactive techniques on user
experience for choreographing applications which also takes
AR into consideration. Therefore, our study focuses mainly
on user experience on a choreographing tool. Furthermore, we
focus on AR for training and education. We seek to develop
a clear understanding on the effects of mobile AR on user
experience.
III.
PC BASED INTERFACE
We ﬁrst developed a desktop application for choreography
generation. We engaged various stakeholders in the arts ﬁeld
to obtain the necessary functionality of a choreography appli-
cation. This interactive process played a big role in crafting
the control for the characters in the choreographing scene. In
the application, we limit choreography animation to movement
only, for the sake of simplicity.
The interface is a 3D choreographing environment that a
choreographer uses to deﬁne different annotations. The pre-
sented stage is rich in props and presents a well modeled stage
that resembles real stages for theaters. The choreographer is
presented with controls to add dancers and props into the stage.
The props that can be added include trees and boxes. Figure 1
shows the PC based interface with dancers and props added.
The choreography generator interface can be customized to
suit well for different set ups ,this enables ﬂexibility to
choreographers. In this way users can deﬁne their stages in
different ways depending on the type of choreographing they
are working on. Furthermore, individual character proﬁle for
the dancer can be created when adding a character, the proﬁle
contains attributes such as name, age, gender and height. This
functionality allows the choreographer to deﬁne 3D virtual
characters with attributes that makes it easier for the real
performing artists to follow in order to understand their roles
in the choreography.
The stage can be viewed from different angles, allowing the
choreographer and the dancer to view the deﬁned annotations
from various views. The user can zoom into and out of the
54
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

Figure 1. A screen shot from the PC based interface
showing props added.
scene using a mouse, and also rotate the scene to view it from
different angles. The interface provides high levels of zooming
that permits both the choreographer and the dancers to view
the deﬁned annotations clearly.
Figure 2. PC based interface: draw path.
To add a dancer into the scene the user utilizes the “add
dancer” button, then selects the character from the presented
grid. Once the dancer is added into the scene the choreographer
can then deﬁne the path that the dancer must follow. The “draw
path” button is used to set the scene into a mode for deﬁning
the path. The user uses the mouse to drag the character along
the path to follow. A line trail is used to show the user the
path they are deﬁning as shown in Figure 2. After deﬁning
the choreography the “play” button is used to play the deﬁned
choreography. While the scene is playing the choreographer
can use a slider to control the speed of the characters.
Figure 3. PC based interface: text bubble.
Other animations include a bubble to keep track of any text
that a performing artist utters in the scene, and also the ability
to raise hands as a sign for different signals. The choreographer
can set different signals that dancers use in the scene, including
voice signals which come in the form of note bubbles on the
interface. The bubble carries a message that a character must
utter as shown in Figure 3. This is analogous to a lead dancer
hailing out instructions during the dance or an actor telling his
lines.
IV.
MOBILE APPLICATION INTERFACE
The PC based choreography generator functions as the
foundation for the mobile application for choreography gen-
eration and the AR application. Mobile touch screens such as
those of phones and tablets remove reliance on the keyboard
and mouse for interaction, and present the touch based in-
teraction technique. The touch based approach present a new
dimension of controlling the characters and interacting with the
interface. This interface is designed to function on hand-held
devices of different screen sizes.
Figure 4. Mobile application interface: draw path.
The application retains the same functionality and graphics
as the PC based application, but relies on the ﬁnger to deﬁne
different controls. The user can select the actors to add into
the scene by touching a speciﬁc button from the character grid
presented in the interface. Once the character has been added
the ﬁnger is used to draw the path the character follows. The
user uses his/her ﬁnger to drag the character towards a path to
follow. A line trailer utility is developed to set the trail behind
the path to show the user the path being deﬁned as shown in
Figure 4. Drawing the path using the ﬁnger allows a natural
way of performing this task. This enables an easy way for
the user to achieve the required result, in addition it facilitates
deﬁning paths that are difﬁcult to achieve using the mouse.
To play the scene the “play” button is used. Whilst playing
the choreography the choreographer can view the scene from
different angles and zooming levels, by utilizing the touch
screen. To zoom in or out the interface allows the simultaneous
use of two ﬁngers to open up or close down the interface.
Rotating the scene is achieved by dragging the view towards
the required viewing angle using just one ﬁnger. The mobile
application also allows the user to add different props to
decorate the scene. The underlying visual graphics for the
PC based interface and the mobile application interface are
generally the same with a little difference owing to the nature
of mobile devices’ screens.
V.
AR BASED INTERFACE
Mobile AR presents an interesting approach to interface
development. AR changes the users’ real view by super-
imposing computer generated graphics using a smart-phone
screen or a headset. AR is important in choreographing as
it takes away the need for physical resources to set up a
choreographing environment. Using AR a virtual stage can be
created anywhere and virtual 3D characters can be added.
The AR based interface presents the same functionality as
the two previous interfaces. However, the graphical interface of
55
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

the AR based application differs signiﬁcantly due to the nature
of AR. The AR application is a mobile based AR tool that
utilizes marker based detection. The application uses marker-
based augmented reality approach. The marker is used as the
stage set up required to initiate the interface. Marker based AR
uses a camera and a visual marker to determine the center,
orientation and range of its spherical coordinate system [3].
The marker used for this application is shown in Figure 5.
Figure 5. Template designed for the AR marker.
Using AR, different real world tangible objects can be
added on top of the marker apart from the computer generated
imagery. This aspect gives the choreographer ability to use
different real world tangible objects. Therefore, using AR
the stage is not a ﬁxed environment but rather it allows
the choreographer to creatively deﬁne different environments
for choreographing. AR’s inherent nature provides a unique
opportunity to create authentic extraordinary environments that
make use of both digital and physical material [14]. In Figure 6
two physical objects are utilized as props and an image as a
background scenery. In mobile or PC based interfaces only
computer generated imagery can be used in the background
for stage setup.
Figure 6. A screen shot from the AR interface, which
shows interactive buttons, physical objects as props
and an image as background scenery.
The user utilizes his/her mobile device’s camera to view
the marker and initiate the interface. To interact with the
interface the user also utilizes the touch screen of the mobile
device. Once the marker is detected the user is presented with
the application’s interface on the mobile device’s screen. The
interface presented has controls for adding a dancer, drawing
the path, adding props to decorate scene, controlling speed of
the characters and resetting the scene. To add the characters
into the scene the “add button” is utilized. The user can then
position the dancers anywhere on top of the marker. To deﬁne
the choreography the user drags the 3D character along the
path to follow; this is achieved using the ﬁnger. The line trailer
utility of unity is also utilized to draw a trailer behind the path
being deﬁned, so that the choreographer has a visual aspect of
the path being deﬁned. Figure 7 shows a simple path deﬁned
for a single character.
Figure 7. AR based interface: draw path.
Figure 8. A screen shot from the AR interface, which
shows characters and a prop added.
A play button to play the deﬁned choreography is presented
to the user as shown in Figure 7. Whilst playing the deﬁned
choreography, the user can control the speed of the characters,
view the choreography from different angles and zoom in
or out. To achieve different zooming levels the user moves
the mobile device closer or further from the marker, whilst
keeping the marker at a ﬁxed position in the screen. This
same functionality can also be achieved by moving the marker
closer to the device’s camera, whilst keeping mobile device
on a ﬁxed position. In order to view the scene from different
angles the user can move the camera around the marker. The
scene can also be reset to correct any errors that arise. This
functionality enables the choreographer to explore the interface
further without fear of making errors. Figure 8 shows the
implementation of the AR interface with a tree and four dancer
characters added into the scene.
VI.
EXPERIMENTS
Experiments were conducted to evaluate the performance
of users on the three developed interfaces, the PC based
version, the mobile application for choreography generation
and the AR based version. We investigated four of Nielsen’s
factors of usability: satisfaction, effectiveness, ease of learning
and efﬁciency [15]. We obtained satisfaction levels from the
post user questionnaire form. The effectiveness is shown by
how the test subjects completed the given tasks. The user’s
feedback on how easy it is to learn the system gives an
indication of the time needed to learn the different interfaces.
The time scores determines the efﬁciency of the interface.
A. Software and Hardware
To develop the three interfaces we used Unity3D software,
a cross-platform game engine used to develop video games
for computers, consoles, mobile devices and websites [16].
Unity is a powerful platform for developing 3D based graphical
applications with rich interfaces. The PC based interface is
available as a standalone desktop application or as a web based
application, but works only on browsers that support Unity3D
56
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

plugin. The user interacts with the interface using a keyboard
and a mouse.
The mobile application for choreography generation is
designed for hand-held devices. For the experiments we pub-
lished the application for Android devices. The users use the
touch screen to interact with the interface. The AR application
makes use of an additional plugin for Unity3D named Vuforia.
Vuforia facilitates for the creation of robust AR applications
on the Unity3D platform. It uses computer vision technology
to recognize and track planar images and simple 3D objects
in real-time [17]. Vuforia and Unity3D have a seamless com-
bination that enables the development of rich interfaces.
To test the PC application the participants accessed it as
a web application from the Mozilla Firefox web browser, and
also as a standalone desktop application. To test the mobile
AR application and the mobile application we used an LG G4
stylus and a Samsung S5. These devices have good cameras
that are very effective for AR applications.
B. Participants
We used a total of 15 unpaid test subjects for the ex-
periments. From the 15, 9 were males and 6 were females.
Their ages ranged between 19 and 28, with a mean age of
24. We selected the participants based on their ability to use
computers. The experiments were carried out primarily to
evaluate the usability of the applications therefore the use of
non-professional test subjects, this is a preliminary study. With
respect to expertise in AR, four participants had experience
with AR applications whilst the others stated that they had no
signiﬁcant previous experience.
C. Tasks
For this comparative study, we chose these simple tasks
that are easier to handle for users since all of them are ﬁrst
time users of a choreographing application. The participants
were expected to complete these tasks on the three interfaces.
The participants had to add a dancer to the left and to the
right of scene. After adding the dancer, the user then adds
a prop into the scene, the prop being the tree or the box for
decoration. After adding the prop the user then draws a simple
path for the dancer to follow. Finally, the user is expected to
play the deﬁned choreography. In addition to this task, users
were required to zoom in and out, and also increase speed of
characters in the scene. This was to allow users to explore the
interfaces further.
The four tasks that the users were expected to perform:
•
Task 1) Add dancer to the left and right of the
scene
•
Task 2) Add prop to the left and right of the
scene
•
Task 3) Draw a simple path from the back to
the front
•
Task 4) Play the choreography (free play time)
D. Procedure
The participants performed the four tasks in sequence on
each of the three interfaces. The participants were each given
a brief description of the task to be performed. During the
experiments, our participants used the interfaces in a random
order to avoid task adaptation and to obtain fair scores for the
three interfaces. We encouraged the users to think aloud during
the process and we recorded their feedback at each stage.
We recorded the time the users took to complete each
tasks to obtain the objective test results. We only recorded
completion time for the ﬁrst three tasks, whilst the last task of
playing the scene had a free time so as to allow the users to
explore the system more.
At the end of the experiment the users were presented with
post test questionnaire that assisted in obtaining subjective
analysis of the experiments. The users gave feedback on
satisfaction, the pleasure they experienced, time to learn the
interface, their frustration levels and they also gave feedback
on their preferred interface. Difﬁcult measures of using the
interfaces in the type of mental stress and physical stress scores
were also attained. Furthermore, we required our participants
to express their preferred interface from the three given inter-
faces. These results gave us a fair comparison of the interfaces
to have a conclusive analysis.
The user experience test conducted on the interfaces inves-
tigated the following issues:
•
Users’ awareness and experience with AR technology
•
Users’ preference on touch based or pointer based
interactive technique
•
Users’ reaction to the different zooming approaches
•
How the users interact with the two interfaces.
•
Time taken to complete tasks
To measure the different aspects of user experience we
used a scale 1 to 10. Satisfaction, pleasure, physical stress
and mental stress level were attained from this scale. We
also expected users to give a feedback on the effects of the
different interactive approaches on viewing the choreography
from different angles, and also the effects of the different
zooming interactive techniques.
E. Results
The results of the experiments are shown the Tables 1-6.
Table 1 gives mean time scores, Table 2 gives mean mental
stress scores, Table 3 gives mean physical stress scores, Table
4 gives mean satisfactions scores, Table 5 gives mean pleasure
scores and Table 6 gives mean frustration scores. Best scores
are highlighted in bold.
TABLE I. TIME RESULTS[IN SECONDS]
PC based
Mobile
application
AR interface
Task 1
9.3±1.2
8.3±1.0
7.5±1.2
Task 2
8.9±1.2
9.0±2.5
9.1±1.3
Task 3
12.4±2.10
7.2±1.2
6.5±0.8
Task 4
free time
free time
free time
Average
7.7±1.1
6.1±1.2
5.4±0.8
The results in Table 1 indicate that completing the tasks
was generally faster on the touch based interactive approaches
of the AR and the mobile application interface as compared
to the mouse approach of the PC based interface. The total
average time for completing the ﬁrst three tasks in sequence
is 30.6 seconds for the PC based interface, 24.5 seconds for
the mobile application interface and 23.1 seconds for the
AR based interface. The total average scores show that AR
57
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

TABLE II. MENTAL STRESS RESULTS [IN LIKERT SCALE 1-10]
PC based
Mobile
application
AR interface
Task 1
3.1±0.9
3.0±0.7
2.9±0.9
Task 2
1.5±0.8
1.4±1.0
1.7±1.1
Task 3
6.7±1.00
2.9±1.2
1.3±0.7
Task 4
1.5±1.1
2.0±1.0
1.3±1.2
Average
3.2±0.9
2.3±1.0
1.8±0.9
TABLE III. PHYSICAL STRESS RESULTS [IN LIKERT SCALE]
PC based
Mobile
application
AR interface
Task 1
1.5±1.3
2.0±0.7
4.3±1.0
Task 2
1.6±1.2
2.8±0.4
4.2±1.3
Task 3
2.9±0.9
1.7±3.6
1.35±0.7
Task 4
1.6±1.6
4.8±0.7
6.6±1.7
Average
1.9±1.2
2.8±1.4
4.1±1.1
TABLE IV. SATISFACTION RESULTS [IN LIKERT SCALE 1-10]
PC based
Mobile
application
AR interface
Task 1
7.0±1.1
6.5±1.2
6.8±0.8
Task 2
7.9±0.9
7.3±1.3
8.0±0.5
Task 3
6.7±2.1
7.64±0.2
8.2±0.8
Task 4
7.2±0.5
7.0±0.9
8.6±1.0
Average
7.2±1.2
7.1±0.8
7.8±0.8
TABLE V. PLEASURE RESULTS [IN LIKERT SCALE 1-10]
PC based
Mobile
application
AR interface
Task 1
7.0±1.1
6.7±1.6
7.4±1.0
Task 2
7.6±1.0
7.6±1.2
8.0±1.1
Task 3
6.7±1.2
8.2±1.3
8.6±0.9
Task 4
6.6±0.7
7.0±1.1
8.9±0.6
Average
6.9±0.9
7.4±1.5
8.2±0.9
TABLE VI. FRUSTRATION RESULTS [IN LIKERT SCALE]
PC based
Mobile
application
AR interface
Task 1
1.90±0.8
1.4±0.9
2.3±0.9
Task 2
1.4±1.30
1.8±1.1
1.8±0.9
Task 3
3.1±0.8
2.8±0.9
1.0±0.8
Task 4
3.0±0.9
4.10±0.8
1.8±0.9
Average
2.4±0.9
2.5±0.8
1.7±0.9
produces faster task completion rates compared to PC based
interface and the mobile application interface. Especially Task
3 drawing path shows signiﬁcant difference on the mean time
values. Drawing path on the mobile based application and
the AR interface is performed drastically faster than on the
PC based. The AR has 6.5 seconds , 7.2 seconds for the
mobile application and 12.4 seconds for the PC based interface.
Furthermore, we note from the standard deviation scores that
the divergence of AR results is rather small, showing that
the task was performed almost similar by all users. Drawing
the path on the touch based interfaces using the ﬁnger was
simpler and faster as compared to using the mouse to deﬁne
the path. The time scores generally show that the touch based
approach is more efﬁcient than the mouse based approach
as they produce faster task completion rates. Furthermore,
although time was not measured for Task 4 it was observed
that zooming in and out or changing the viewing angles was
faster in the AR based interface. This further highlighted the
efﬁciency of AR as it gives users a quicker way of achieving
different viewing positions.
The levels of mental stress on all the three interfaces
are generally low owed to the simplicity of the tasks and
the interfaces, as shown in Table 2. However, we note a
signiﬁcant difference on the result for the PC based interface
when drawing the path. This was a result of the approach
that required more time to grasp. The mental stress scores for
the PC based is 6.7, whilst it is 2.9 for the mobile application
interface and 1.3 for the AR interface. This shows that the users
experienced a lot of stress mentally in this task. Furthermore,
the standard deviation score of 1.0 shows that almost all
the users agree on the high mental effort needed to perform
Task 3 on the PC based interface. On the AR interface users
experienced the least mental stress on three of the four tasks.
Physical stress is signiﬁcantly high on the AR interface.
The AR interface has an average physical stress of 5.6 as
compared to 3.1 and 1.8 of the mobile application and PC
based interface respectively. Task 4 “playing the scene” shows
the highest level of physical stress experienced by the users
on the AR interface. Having a physical stress score of 6.60
as compared to 1.60 and 2.75 of the PC based interface
and mobile application interface respectively. This highlights
one big disadvantage of mobile AR on user experience as it
requires more physical effort over time. However, for Task 3
we observe that AR has the lowest physical stress. This result
highlights that users needed less physical effort when drawing
the path using the ﬁnger as compared to using the mouse. The
high levels of physical stress on the AR interface are due to
the movement of hands, as the user moves the device closer
and further from the marker to achieve different angles and
zooming levels. This effect is also the cause for the physical
stress scores on the mobile application as the ﬁngers need to
constantly interact with the interface for any activity. Thus, the
touch based interfaces require users to use more physical effort
to perform the tasks. From Table 3, best scores are achieved
most on the PC based interface highlighting that the mouse as
an interaction device requires low physical effort. The main
source of physical stress on the PC based interface is the
movement of the mouse through small movements of the hand
while the arm lies in rest-up position. This explains the low
levels obtained in this regard.
Users expressed high levels of satisfaction from completing
their tasks on all the interfaces. These results are attributed
to the simplicity of the interfaces themselves. The standard
deviation scores that are generally less than 1.0 also indicate
that users experienced almost similar effects. As shown in
Table 4 users expressed more satisfaction on the AR interface
as compared to the two other interfaces. For Task 4 the
satisfaction scores are 7.2 for the PC based interface, 7.0 for
the mobile application interface and 8.6 for the AR based
interface. These high scores are attributed to the fact that users
managed to complete all their tasks to a satisfactory level.
Users expressed satisfaction in the quicker and easier way they
managed to move around the scene using the AR interface.
Table 5 shows the scores recorded for the pleasure ex-
perienced by users when performing the tasks. The results
show signiﬁcant excitement when using the AR interface. The
main excitement is brought by the curiosity that the interface
introduces as stated by [8]. On all the tasks the highest pleasure
is recorded on the AR interfaces with scores of 7.4, 8.0, 8.6 and
58
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

8.9, for Task 1 up to Task 4 respectively. Of particular interest
are the results for Task 4 where the users were allowed a free
time to explore the interfaces. The pleasure results for the PC
based interface, the mobile application interface and the AR
interface are 6.6, 7.0 and 8.9 respectively. These results show
a signiﬁcant difference; on the AR interface users experienced
the greatest pleasure. The ability to zoom in and out of the
scene by only moving the mobile device closer or further
from the marker, brought about such a compelling effect to the
users when using the AR interface. This approach of changing
viewing position gave users an easier way to move around the
scene. This ability also reduces the time to achieve the required
zooming position or angle as compared to using the touch
screen to zoom in or using the mouse to zoom into the scene.
Therefore on the overall the AR interface is more effective
than the other two interfaces. The results also indicate that
users experienced more excitement on the mobile application
than on the PC based interface.
The mobile based interface produced high level of errors
when the user was zooming in and out of the scene, thus
more frustration as shown in Table 6. This was largely due
to the user rotating the scene instead of zooming. A number
of errors were also experienced on the PC based interface, but
this was less than the ones on the mobile based. This result is
attributed to the level of control a pointer device has on the
scene largely due to the small point of contact with interface
as compared to the ﬁnger’s surface. However, we note that
with the AR based interface this error does not exist as the
AR interface allows a more natural way of zooming which
is analogous to moving closer to an object to view it clearly.
The frustration scores for Task 4 demonstrate the effects of
the errors on the interface as the users were frustrated on
interfaces with many errors. The score for Task 4 for the PC
based interface, the mobile application and the AR interface
is 3.00, 4.10 and 1.81 respectively. This shows that users were
less frustrated on the AR interface whilst playing the scene.
This is largely due to the easiness of changing viewing angles
and zooming levels. However, we note that when adding the
characters the AR presents the highest frustration scores (2.3
compared to 2.1 and 1.43 of the PC based interface and mobile
application interface respectively). This result is caused by the
tilting effect of the mobile device whilst the user is positioning
the characters. Therefore, this makes it difﬁcult for the user to
correctly position the characters in the scene in the way they
wanted. On the contrary, in the PC based interface and the
mobile application this difﬁculty does not exist. Table 6 also
highlights that frustration levels differed on the tasks the users
were performing on the different interfaces. The AR interface
gave the least frustration levels on the last two tasks whilst the
PC based interface gave the least on Task 2 and for Task 1 the
mobile application interface gave best result. The main source
of frustration for the AR interface is the tilting of the device
whilst performing tasks that are more effectively done with a
static device, such as adding the character on the scene.
The simplicity of the interfaces is further shown by the
“easy to learn” results we obtained from the users, which
have scores of 8.4 for the AR based application, 8.01 for
the mobile application interface and 8.10 for the PC based
interface. The AR interface has fewer visual buttons than the
other two interfaces this makes it easier to learn as shown by
the results. Generally, the interfaces provided the same basic
functions and controls that explains why the scores for the
easiness to learn are very close to each other. It is of high
importance to develop interfaces that are easy to grasp for the
users, especially for applications that are used in training and
education. This was also demonstrated by the willingness of
the participants to explore the interfaces further. This also helps
the users retain over time.
After completing the experiments, we also asked which
interface for choreography generation the users prefer best
after having performed tasks on all the three interfaces. Seven
preferred the AR interface, four preferred mobile application
and four preferred the PC based interface. These results show
that the AR based interface was the preferred option by the
users. From the users that already had prior experience with
AR only one of the users did not choose the AR based appli-
cation as a preferred interface for choreography generation.
VII.
DISCUSSION
The experiments were carried out to complete the same
task on three different interfaces using different interactive
techniques: 1) the PC based application that uses pointer based
interaction approach utilizing a mouse and keyboard, 2) the
mobile application interface and 3) the AR application that
used touch based interaction approach. The results obtained
from the experiments contributes to the study of how different
interaction techniques affect user experience and also the
effects of augmented reality on user experience.
The touch based approaches produced faster task com-
pletion times compared to the mouse based approach. The
touch based interactive approach of the AR interface and the
mobile application showed faster times using the ﬁnger to
draw path demonstrating the power of interfaces that provide
a natural way of interaction to the users [1]. Using the ﬁnger
to deﬁne choreographs presented a natural approach to deﬁne
movement as compared to using the mouse, this explains
the signiﬁcant time differences. The AR interface gives the
best overall task completion rate compared to that of the PC
based and mobile application interfaces. However, the mobile
application and the AR interface show a small difference of 1.4
seconds since both approaches use the touch based interactive
approach. Furthermore, the ability to zoom in and out of
the scene by only moving the hand-held device closer to the
scene gave users a quicker way to complete the task. This
further demonstrates the effectiveness of having interfaces that
facilitates a natural way to complete the tasks. Thus, interfaces
which facilitate a natural way to complete tasks possess a great
advantage in as far as user experience is concerned.
The results showed that when adding the dancer and the
props into the scene the rate of errors was high in the AR
interface as compared to the other two interfaces. This result
is inﬂuenced by the changing position of the interface as
the phone tilts or is held in an unstable manner and also
the underlying design of the interfaces. Maintaining a mobile
device in a stable position during interaction is a big challenge
for all mobile AR applications. This is attributed to the natural
hand tremor [18]. This effectively undermines AR’s effective-
ness for mobile devices in applications that require accurate
positioning. Nonetheless, depending on the application these
errors can be tolerated. However, AR interface had no errors
associated with zooming or rotating the scene since this was
achieved by only moving closer or around the scene. The
59
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

mobile application had more errors in this regard compared to
the PC based application owing to the larger surface area of the
ﬁnger as compared to the mouse pointer. The mouse facilitates
an easier zooming approach compared to using the ﬁnger. The
mouse utilizes the scroll wheel to change the levels by scrolling
it towards the required direction. In addition, changing the
viewing angles by dragging the scene using the mouse pointer
produces faster and more accurate results, compared to using
the ﬁnger on the touch screen of the mobile device. This effect
is inﬂuenced by the small contact area of the mouse pointer
which gives the user more control to perform the required
task. The touch based approach is efﬁcient, however care
has to be given when dealing with interfaces that have many
controls in the same line. This gave many errors on the mobile
based application where a user was zooming but instead found
himself/herself rotating the scene. This is a drawback for the
touch approach. However, rarely experienced on the PC based
approach due to the small area of contact on the mouse pointer
as compared to the larger surface area of the ﬁnger.
It very important to develop interfaces that are easy to
learn. The simplicity of the graphical user interface goes a long
way in achieving faster “time to learn” scores as demonstrated
by the results from the experiments. Time to learn eventually
affects the usability of the interface, especially with regards
to training tools. Interfaces that are simple to understand also
help boost the users’ retention rate. In our experiments, the
interfaces were easy to learn, this made users more interested
in exploring different functionality of the interfaces. We also
noted that AR interface produced the best “time to learn” score.
This also led to the interface being the preferred choice of the
users, as they felt more relaxed while using the interface.
The results of the experiments also show that AR has
an important part to play in crafting training tools as stated
in [9]. It gives a compelling effect to the users and excitement
through the curiosity it creates. Furthermore, AR allows the
addition of physical objects into the scene in addition to the
computer generated objects. For training applications this helps
explore many objects to illustrate concepts without the need
of completely changing the graphical interface. Therefore, AR
allows ﬂexibility in interface design. The user preference re-
sults demonstrate that AR has a great promise, this reenforces
the ﬁndings in [14]. This demonstrates that AR is effective in
teaching and training.
The AR interface affords users the ability to interact with
the characters by only moving the phone around the marker.
The ability to move closer to the dancers on the scene and
easily shift the viewing angle provided pleasure and excitement
to the users. This is similar to moving closer to an object
or further from the object, which is a natural approach to
changing positions. This approach gave users an easier and
quicker way to move around the scene. However, mobile AR
for hand-held devices presents high levels of physical stress
on the hands of the user and also a high error rate when
interacting with the touch screen as the mobile device tilts and
shifts positions on the hand of the user. The interface requires
a lot of physical effort from the user’s hand to achieve the
desired results, this in turn leads to the high levels of physical
stress. Another contributing factor to physical stress in mobile
AR is the screen size of the device as limited screen size tend
to restrict the covered ﬁeld of view as stated in [19], thereby
forcing the user to continuous adjust viewing positions.
Therefore, wearable glasses or goggles can prove more
effective for AR in training applications. For example, using
wearable glasses like Google’s cardboard the user does not
need to continuously use the hand to position the mobile device
correctly, but rather by positioning the eyes in the required
position. This is an approach we leave as future work, it
will give an understanding on the effects of wearable devices
on AR and training tools. Furthermore, we intend to extend
the interactive approach for the AR interface by allowing
the choreographer to use virtual buttons on the marker and
draw on the marker. In this approach the mobile phone screen
becomes a merely viewing screen but no longer the means of
interaction. To achieve more of the natural interface approach
voice commands will be important in the future work. For
the time being, voice is used as a means to enter text into
speech bubbles using the Google speech API. Therefore, future
work will complement the current interfaces by having voice
commands to add and control characters especially on the AR
interface. This future work study focuses more on the natural
user interface aspect by having an application that combines
virtual buttons and voice commands to control characters in
a real environment superimposed with 3D graphics. It is also
imperative to extend the interfaces to include a virtual reality
interface and compare user experience too.
VIII.
CONCLUSION AND FUTURE WORK
In the study, we designed and implemented three interfaces
for choreography generation a PC based interface, a mobile
application interface and an AR based interface from which we
compared the performance of users in terms of user experience.
The interfaces allow a choreographer to deﬁne choreographs
and makes the choreographer in charge of the actors just like
in the real world. The results indicated that using touch based
approaches the users obtained faster task completion rates
as compared to using a mouse on the PC based interface.
Users were more comfortable completing tasks using natural
approaches, for example deﬁning choreographs using their
ﬁnger on the touch based interface. The result demonstrates
the need to develop natural user interfaces to improve user
experience. Excitement and pleasure obtained was highest on
the AR interface, followed by the mobile application interface
and lowest on the PC based interface.
The results of the experiment showed that AR has a big
role to play in the development of training and educational
applications. Researchers can build upon this to further inves-
tigate AR coupled with natural user interface aspects like voice
and gestures. Another aspect to be implemented in the future
includes virtual buttons and virtual drawing of paths where the
user only interacts with the marker and not the mobile device
screen. In this approach the mobile device screen becomes
just a screen to view the real world but the user only interacts
with the marker. We plan to add more forms of movement
and special animation for choreography generation and test
the interfaces on experts working in the arts domain.
ACKNOWLEDGMENT
The authors would like to thank G¨unes¸ Karababa, C¸ a˘gatay
Koc¸ and Ege Sarıo˘glu for the fruitful discussion sessions and
their invaluable ideas.
60
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

REFERENCES
[1]
L. Besanc¸on, P. Issartel, M. Ammi, and P. Isenberg, “Usability com-
parison of mouse, touch and tangible inputs for 3d data manipulation,”
CoRR, vol. abs/1603.08735, 2016, pp. 1–10, URL: http://arxiv.org/abs/
1603.08735/[accessed: 2017-02-01].
[2]
R. M. Yilmaz, “Educational magic toys developed with augmented
reality technology for early childhood education,” Computer Humam
Behaviour Journal, vol. 54, 2016, pp. 240–248, ISBN: 0747-5632, URL:
http://dx.doi.org/10.1016/j.chb.2015.07.040 [accessed: 2017-02-02].
[3]
A. B. Craig, Understanding augmented reality: concepts and applica-
tions.
Morgan Kaufmann, 2013, ISBN: 978-0-240-82408-6.
[4]
G. A. Lee, U. Yang, Y. Kim, D. Jo, K. Kim, J. Kim, and J. Choi,
“Freeze-set-go interaction method for handheld mobile augmented
reality environments,” in Proceedings of the 16th ACM Symposium
on Virtual Reality Software and Technology November 18–22, 2009,
Kyoto, Japan.
ACM, Nov. 2009, pp. 143–146, ISBN: 978-1-60558-
869-8, URL: http://doi.acm.org/10.1145/1643928.1643961/ [accessed:
2017-02-01].
[5]
K. R. Bujak, I. Radu, R. Catrambone, B. Macintyre, R. Zheng, and
G. Golubski, “A Psychological Perspective on Augmented Reality in
the Mathematics Classroom,” Computer Education Journal, vol. 68, Oct.
2013, pp. 536–544, ISSN: 0360-1315.
[6]
“Pok´emon Go Brings Augmented Reality to a Mass Audience,”
Jun.
2016,
URL:
http://www.nytimes.com/2016/07/12/technology/
pokemon-go-brings-augmented-reality-to-a-mass-audience.html? r=1
[accessed: 2017-02-02].
[7]
S. F. Alaoui, K. Carlson, and T. Schiphorst, “Choreography As Mediated
Through Compositional Tools for Movement: Constructing A Historical
Perspective,” in Proceedings of the 2014 International Workshop on
Movement and Computing June 16–17, 2014, Paris, France.
ACM,
Jun. 2014, pp. 1–6, ISBN: 978-1-4503-2814-2, URL: http://doi.acm.
org/10.1145/2617995.2617996/ [accessed: 2017-02-01].
[8]
G. Chang, P. Morreale, and P. Medicherla, “Applications of augmented
reality systems in education,” in Proceedings of Society for Informa-
tion Technology
Teacher Education International Conference March
29, 2010, San Diego, USA.
Association for the Advancement of
Computing in Education (AACE), March 2010, pp. 1380–1385, URL:
https://www.learntechlib.org/p/33549 [accessed: 2017-02-01].
[9]
L. K., “Augmented reality in education and training,” TechTrends: Link-
ing Research and Practice to Improve Learning, vol. 56, no. 2, 2012, pp.
13–21, ISSN: 8756-3894,URL: https://www.learntechlib.org/p/65711/
[accessed: 2017-02-01].
[10]
A. M. Noll and A. Hutchinson, “Choreography and computers,” Dance
Magazine, vol. 41, 1967, pp. 43–45.
[11]
W. Broll, S. Gr¨unvogel, I. Herbst, I. Lindt, M. M. J. Ohlenburg, and
M. Wittk¨amper, “Interactive Props and Choreography Planning with
the Mixed Reality Stage,” in Proceedings of the 3rd International
Conference Entertainment Computing (ICEC) , September 1–3, 2004,
Eindhoven, The Netherlands,.
Springer Berlin Heidelberg, Sep. 2004,
pp. 185–192, ISBN: 978-3-540-28643-1, URL: http://dx.doi.org/10.
1007/978-3-540-28643-1 25/ [accessed: 2017-02-01].
[12]
D. W. F. Van Krevelen and R. Poelman, “A survey of augmented
reality technologies, applications and limitations,” International Journal
of Virtual Reality, vol. 9, 2010, pp. 1–20, URL: http://citeseerx.ist.psu.
edu/viewdoc/download?doi=10.1.1.454.8190&rep=rep1&type=pdf/ [ac-
cessed: 2017-02-01].
[13]
D. Davcev, V. Trajkovic, S. Kalajdziski, and S. Celakoski, “Augmented
reality environment for dance learning,” in Proceedings of the Infor-
mation Technology Research and Education International Conference
(ITRE) August 11–13, 2003, New Jersey, USA, Aug. 2003, pp. 189–
193, URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=
1270600&isnumber=28360/ [accessed: 2017-02-01].
[14]
M. Dunleavy, C. Dede, and R. Mitchell, “Affordances and limitations
of immersive participatory augmented reality simulations for teaching
and learning,” Journal of Science Education and Technology, vol. 18,
2009, pp. 7–22, ISSN: 1573-1839, URL: http://dx.doi.org/10.1007/
s10956-008-9119-1/ [accessed: 2017-02-01].
[15]
J. Nielsen, Usability Engineering.
Morgan Kaufmann Publishers Inc.,
1993, ISBN: 0125184050.
[16]
“Unity3D,” URL: http://www.unity3d.com [accessed: 2017-02-02].
[17]
“Vuforia,” URL: http://www.vuforia.com [accessed: 2017-02-02].
[18]
T. Vincent, L. Nigay, and T. Kurata, “Handheld Augmented Reality:
Effect of registration jitter on cursor-based pointing techniques,” in
Proceedings of the 25th Conference on l’Interaction Homme-Machine
(IHM) November 12–15, 2013, Bordeaux, France.
ACM, Nov. 2013,
pp. 1–6, URL: https://hal.inria.fr/hal-00875440/ﬁle/1-1.pdf/ [accessed:
2017-02-01].
[19]
O. Bimber and R. Raskar, “Modern approaches to augmented reality,”
in ACM SIGGRAPH 2006 Courses July 30–August 03, 2006, Boston,
USA.
ACM, Jul. 2006, pp. 1–88, ISBN: 1-59593-364-6, URL: http:
//doi.acm.org/10.1145/1185657.1185796/ [accessed: 2017-02-01].
61
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-538-8
ACHI 2017 : The Tenth International Conference on Advances in Computer-Human Interactions

