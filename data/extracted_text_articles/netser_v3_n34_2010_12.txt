Trafﬁc Shaping via Congestion Signals Delegation
Mina Guirguis
Computer Science Department
Texas State University - San Marcos
San Marcos, TX 78666, USA
Email: msg@txstate.edu
Jason Valdez
Computer Science Department
Texas State University - San Marcos
San Marcos, TX 78666, USA
Email: jv1150@txstate.edu
Abstract—This paper presents a new architecture that en-
ables a set of clients to enforce trafﬁc shaping policies among
them through the delegation of congestion signals. When
congestion-aware Internet ﬂows share a bottleneck link, they
compete for bandwidth and must respond to congestion signals
promptly by decreasing their throughput. For clients running
real-time applications (e.g., gaming, streaming), this may im-
pose strict limitation on their achievable throughput over short
time-scales. To that end, this paper presents an architecture,
whereby a set of TCP connections (we refer to them as the
Stunts) sacriﬁce/trade their performance on behalf of another
TCP connection (we refer to it as the Free) by picking up
a delegated subset of the congestion signals and reacting to
them in lieu of the Free connection. This gives the Free
connection just enough freedom to meet speciﬁc throughput
requirements as requested by the application running on top,
without affecting the level of congestion in the network. We
present numerical model and analysis, which we validate by
extensive simulation as well as through a pluggable module
implementation for the Linux kernel.
Keywords-Service-oriented architecture; TCP; Congestion
Control; Trafﬁc Shaping; Control Theory;
I. INTRODUCTION
Motivation: Certain classes of applications (e.g., gaming,
audio and video streaming) need to acquire/maintain certain
guarantees in order to perform adequately. Due to the “best-
effort” nature of the Internet, it is very difﬁcult to ensure that
these guarantees are met or even to predict what possible
guarantees could be provided. Hence, these applications
are often left with unspeciﬁed guarantees on their quality
of service. Research efforts have addressed this problem
and proposed two major architectures, Integrated Services
(IntServ) and Differentiated Services (DiffServ). IntServ
architectures require every router to maintain per-ﬂow state.
Applications make reservations based on their needs. The
main problem with IntServ is that it does not scale to a
size that is as large as the Internet. Thus, it is limited
to small-scale deployments. DiffServ, on the other hand,
push trafﬁc management towards the edges of the network,
while keeping its core simple. Edge routers maintain and
classify ﬂows based on classes. Core routers still need to
maintain brief information on how to treat each class. In
both architectures, some modiﬁcations had to be made to
some routers.
The “Free and Stunts” Architecture: In this paper, we
propose a new end-host service architecture that provides
an application with soft throughput guarantees over a best-
effort network, without any modiﬁcation to routers. More-
over, this is achieved in a completely friendly manner to
the network through strictly adhering to the Transmission
Control Protocol (TCP) rules. In particular, we envision a
set of TCP connections (we refer to them as the Stunts con-
nections) that are willing to sacriﬁce their own performance
on behalf of another TCP connection (we refer to it as the
Free connection). This would enable the Free connection
to match its throughput to the target throughput from the
application. In [1], we have demonstrated the feasibility
of this idea through simulations only. In this paper, we
extend this work by introducing a dynamic model (along
with numerical solutions) as well as real implementation of
this architecture in the Linux kernel.
TCP employs congestion control mainly via the Additive
Increase Multiplicative Decrease (AIMD) mechanism that
seeks to constantly probe for available capacity while re-
maining fair to other TCP ﬂows [2], [3]. Congestion signals
(as in dropped/ marked packets) signal TCP senders to slow
down, by halving their congestion windows. The quantity
and the timing of these congestion signals may prevent the
Free connection from achieving any throughput guarantees.
The main idea behind our architecture is to allow the Free
connection to delegate a subset of those congestion signals to
the Stunt connections. Thus, the Stunt connections would be
the ones that decrease their sending rates instead of the Free
connection, which would be liberated (to a larger extent) to
match the guarantees requested from the application above.
It is important to note that this architecture does not increase
network congestion at the bottleneck link because it ensures
that the total decrease in throughput from all the Stunt
connections is at least as large as what the Free connection
would have decreased, if it were to observe those delegated
losses. For example, it is possible for a single packet loss
delegated from the Free connection to cause more than one
Stunt connection to back-off, in order to have the same
equivalent effect on the bottleneck link.
The choice of Stunt connections is important due the
462
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

dynamic nature of Internet trafﬁc. It has been evident
through many measurement studies that TCP connections
can be characterized into long-lived and short-lived ﬂows,
also known as elephants and mice, respectively [4], [5],
[6], [7]. Elephant ﬂows, while there are few of them, they
account for around 80% of the trafﬁc. These are long-lived
stable TCP ﬂows. Mice, on the other hand, while there are
plenty of them, they account for only around 20% of the
trafﬁc. These are short-lived ﬂows (simple HTTP requests
and they typically ﬁnish before leaving the slow-start phase
of TCP [8], [9]). In our proposed architecture, we limit the
choice of Stunt connections to elephants as they are more
stable and control the majority of Internet trafﬁc. Moreover,
delegating losses to mice would hurt their performance
signiﬁcantly, if they were chosen to act as Stunts.
Free (1)
Cross-traffic
(N)
Stunts (S)
Internet
R
S
S
S
Server
C
Client
C
C
Bottleneck link
Capacity C
Figure 1.
The Free and the Stunts setup. The proposed architecture is
implemented at the End-host at the server’s side.
Deployment Scenarios: This architecture is proposed to
be used by Internet Service Providers (ISPs) to enforce
differentiated service among its clients, by having some
behave as the Free connections, while others play the Stunt
roles. For example, by dropping appropriate packets (or
marking them if the use of the ECN bit is enabled), an ISP
(e.g., the ﬁrst-hop router) can delegate losses between the
Free and the Stunts. Such delegations could be based on a
marketplace in which clients’ agents agree upon what is fair
and efﬁcient for each one [10].
This architecture is also envisioned to be used by Internet
servers that serve different forms of media content to
different clients. With this architecture, a server can give
a particular ﬂow (say a media stream) the freedom to
achieve requested guarantees, while making other ﬂows
(say long bulky ﬁle transfers, i.e., “elephants”) behave as
Stunts. In this scenario, the architecture is implemented at
the end-host and thus the server can accurately identify
the elephant ﬂows based on the requested content (e.g.,
large ﬁles) from the clients. Figure 1 shows an example of
such deployment where the server implements the proposed
architecture to manage its ﬁrst-hop to the Internet (which is
the one that is typically prone to congestion).
Paper Organization: Section II puts this work in contrast
to other related work. Section III describes our proposed
architecture in more detail with all its components and
logistics. Section IV captures the dynamics involved through
numerical results based on a non-linear ﬂuid model. We
evaluate the performance of our proposed architecture with
extensive simulation experiments in Section V. In Section
VI we present results from our Linux implementation. We
conclude the paper in Section VII.
II. RELATED WORK
As hinted in the introduction, many Quality of Service
(QoS) mechanisms have been proposed that belong to the
general IntServ [11] or DiffServ [12] architectures. Due to
their reliance on modifying some in-network components
(whether core routers or edge routers), their acceptance for
deployment is difﬁcult. Moreover, some of them do require
the participation of all entities, which imposes a signiﬁcant
scaling and implementation issues.
The work in [13] focused on managing the end-to-end
behavior of TCP connections through sharing congestion
information among them. A congestion manager module
is used to regulate the transmission rates of the TCP con-
nections in order to achieve an overall better performance.
This method, however, does not aim to provide speciﬁc
guarantees to the TCP connections. In [14] a coordination
protocol (CP) is proposed which seeks to optimize cluster-to-
cluster communication of computing devices across a bottle-
neck aggregation point. Their proposal entails, among other
aspects, giving the ﬂows across the aggregation point the
ability to sense the network state and adapt at the end-points.
The authors in [15] propose a Rate Management Protocol
(RMP) that controls the rates of the ﬂows passing through
an aggregation point based on their QoS requirements. Once
the fair share of the ﬂows are decided by the RMP, a new
TCP sliding window is used to realize that fair share. This
method, however, requires the modiﬁcation of the current
architecture (aggregation and end points) to be realized. In
[16], the authors divide the problem of managing QoS into
two components; the ﬁrst one utilizes a probing scheme at
the IP layer and the other policies the rates at the edge
routers. This method also requires the modiﬁcation of edge
routers.
In [17] an elastic tunnel is created using a number of
regular TCP connections to provide soft bandwidth guaran-
tees. The number of connections that form the elastic tunnel
is adjusted dynamically in tandem with cross-trafﬁc, so
their aggregate throughput ensures the QoS guarantees. This
work only considered constant QoS guarantees. Moreover, it
required modiﬁcations to edge routes to manage the elastic
tunnels.
The authors in [18] present an adaptive Forward Error
Correction (FEC) scheme that aims to ensure a speciﬁc
end-to-end rate for video streaming applications using TCP
connections. The idea is to adjust the degree of redundancy
in packets based on the difference between the achievable
463
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

throughput and the required rate, without wasting network
bandwidth.
In addition to the above related work, we refer the readers
to [19], [20] that provide surveys on bandwidth adaptation
and control mechanisms for managing Internet trafﬁc.
III. THE ARCHITECTURE
In this section, we describe the main components and the
operation of our proposed service architecture.
A. The Components
We envision a setup composed of a single Free TCP
connection and s Stunt TCP connections. Those s + 1
TCP connections traverse a bottleneck link along with n
other TCP connections representing normal cross-trafﬁc.
Thus a total of (1 + s + n) TCP connections traverse that
bottleneck link. Figure 1 depicts this setup. The application
running on top of the Free connection requests its throughput
requirements through a trace ﬁle. This trace ﬁle is ﬁrst
checked by a preprocessor for feasibility. If any of the checks
fail, another feasible trace is created that is closest to the
original trace ﬁle; otherwise, the trace ﬁle is passed directly
to the controller. The controller compares the achievable
throughput to the requested throughput over every time
instant. Based on the difference, the controller adjusts the
ratio of congestion signals to be delegated from the Free
connection to the Stunts in order to match the achievable
throughput to the requested throughput. A monitor measures
the throughput achieved by the Free connection and reports
this value back to the controller. Figure 2 represents the
different components in the architecture.
Figure 2.
The Components of the proposed architecture.
The trace ﬁle: An application speciﬁes its requirements
via a trace ﬁle. A trace ﬁle describes the shape of the
throughput over time. It is composed of two-tuple entries
in the form of time and throughput. An entry in the form
< i, Ti > indicates a request of Ti throughput at time
instant i.
The preprocessor: The main goal of the preprocessor is to
check the feasibility of the trace ﬁle and to create another
feasible trace, if any of the checks fail. It performs two
checks, a slope check and a region check. The slope check
ensures that the requested throughput can be attained from
one time instant to the next based on the round-trip time
(RTT) of the Free connection. For any two successive time
instants, i and j, the requested throughput Tj at time instant
j, is bounded by:
Tj ≤ Ti + j − i
RTT 2
(1)
Since TCP increases its congestion window by 1 packet
every RTT, the congestion window at time j, cannot be more
than
j−i
RT T of the congestion window at time i. Dividing by
RTT to obtain the throughput leads to the above equation.
The region check prevents the Stunts from achieving zero
throughput. Thus for any time instant i, the requested
throughput is bounded by:
Tj ≤ s × ¯xs
(2)
where ¯xs is the average expected throughput per Stunt
connection. This is a preliminary check and a more strict
check is enforced online.
The controller: The controller decides which losses are
picked up by the Free connection versus those delegated
to the Stunts. The decision is based on the error signal be-
tween the current throughput and the requested throughput.
We have experimented with two difference controllers. An
On/Off controller and a Proportional Integral (PI) controller.
An On/Off controller, decides the delegation percentage gi,
at time i according to the following equation:
gi
=
½
0
xi > 1.3¯3Ti
1
otherwise
(3)
where xi is the instantaneous throughput of the Free con-
nection at time instant i. An On/Off controller will try to
delegate all the losses to the Stunts, whenever the current
throughput is not matching the requested throughput. Oth-
erwise, the Free connection will pick up its own losses
and react to them. Notice that we compare xi to 1.3¯3Ti as
opposed to Ti directly. This is due to the AIMD mechanism
and the operation of the controller at short time-scales. A
packet loss at 1.3¯3Ti will cause the throughput to drop to
0.6¯6, leading to the correct average value of Ti, assuming
that the RTT is kept constant.
The above controller may lead to oscillations, thus we
experiment with a PI controller that adjusts the delegation
percentage based on the following equation:
gi
=
gi−1 + K × (xi − 1.3¯3Ti)
(4)
where K is a constant the decides the aggressiveness of
the controller in reaction to the error signal between the
464
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

current throughput and the requested throughput. The higher
the value of K is, the more aggressive the controller would
react.
B. Delegation of Congestion Signals
The Free and the Stunts architecture capitalizes on the
fact that a delegation of a congestion signal from the
Free connection to the Stunts will both (1) allow the Free
connection to achieve a higher target data rate, and (2) it
will not violate any TCP congestion control rules.
Since the Free connection can delegate congestion signals,
it can continue sending as dictated by the Additive Increase
component of the AIMD mechanism, by increasing its con-
gestion window by 1 packet every RTT. Since the requested
throughput is slope-checked by the preprocessor, then it
should be able to achieve the requested target. However,
in some conditions (explained below), the Free connection
may not be able to delegate a congestion signal and thus it
would have to cut its congestion window in half.
It is very important to realize that the impact of a
congestion signal is not the same – as far as the network
is concerned – whenever it gets delegated, since one con-
nection may have a different congestion window size than
the other. Thus, the reduction in the sending rate will not be
equivalent.
In our particular case, to make sure that the network
sees an equivalent reduction, we ﬁrst check to see if the
total congestion windows (of all the Stunts) is larger than
that of the Free connection. If so, then we go through the
Stunts, one by one in a round robin fashion, and we halve
each one’s congestion window, until the total reduction is
at least as large as half the Free connection’s congestion
window size. If not, then we cannot delegate this loss and
the Free connection has to react to it in the normal way,
since we can only cause each Stunt connection to back-off
one time for a given loss. Note that the round robin algorithm
may cause the last Stunt connection to decrease its rate by
a bit more of what is actually required, since we do not
optimize to ﬁnd the best ﬁt among the Stunt connection’s
congestion window sizes that would sum to exactly the
Free connection’s congestion window size. This is wasted
bandwidth that is essentially given up by the Stunts to be
acquired by all connections. The effect of this is diminished
over time as all connections grab more throughput.
The reason we go in round robin fashion is to provide
some notion of fairness across the Stunts without hurting
any one or group of Stunt connection. Notice that TCP
fairness in our case is considered globally across the group
of Free and the Stunt connections, as they can be abstractly
considered as a single entity. The group of Free and
Stunt connections should not together be more aggressive
than an equivalent number of ordinary TCP ﬂows when
increasing their data ﬂow rate through the normal TCP rules.
Paying back the Stunts: In some situations, the Free
connection may not need a higher data rate. Either because
the requested throughput at some point in time may go under
its fair share or its data rate has increased to a point that
is above the requested target rate. In both cases the Free
connection can easily give up this bandwidth by having the
application send less data. However, this slack of bandwidth
will be naturally acquired by all connections (Stunts and
cross-trafﬁc). We have modiﬁed both controllers described
above to allow for the reverse loss delegation from the Stunts
to the Free connection. Reverse delegation helps the group
of Free and Stunt ﬂows to retain bandwidth as a whole,
as apposed to releasing it to the network. Furthermore,
it allows the Free connection to closely match its target
when the target is low (typically below its fair-share). Also,
as discussed above, delegation in this case would ensure
that the Free connection would have a larger congestion
window than the Stunt that is delegating. Otherwise, the
Stunt connection cannot delegate a loss and would have to
react to it.
We have chosen to delegate losses during the AIMD
behavior, since we focus in this paper on longer data
transfers with TCP. It is possible to delegate other behaviors
such as timeouts and slow-start, but we do not consider those
in this work for reasons having to do mostly with complexity
and rareness of those particular events, in comparison to the
AIMD behavior, on a well provisioned network.
IV. THE MODEL
We extended a nonlinear ﬂuid model, similar to those
proposed in [21], [22], [23], [24], to capture the performance
of m TCP ﬂows traversing a bottleneck of capacity C, where
m is equal to (1 + s + n) as depicted in Figure 1.
A. Model Derivations
The round trip time ri(t) at time t for connection i is
equal to the round-trip propagation delay Di between the
sender and the receiver for connection i, plus the queuing
delay at the bottleneck router. Thus ri(t) can be expressed
by:
ri(t)
=
Di + b(t)
C
(5)
where b(t) is the backlog buffer size at time t at the
bottleneck router. We denote the propagation delay from
sender i to the bottleneck by Dsib, which is a fraction αi
of the total propagation delay.
Dsib
=
αiDi
(6)
The backlog buffer b(t) evolves according to the equation:
˙b(t)
=
m
X
i=1
xi(t − Dsib) − C
(7)
465
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

which is equal to the input rate xi(.) from the m connections
minus the output link rate. Notice that the input rates are
delayed by the propagation delay from the senders to the
bottleneck Dsib.
We assume RED, as proposed in [25], is employed at
the bottleneck link as an active queue management scheme.
Thus, the congestion loss probability pc(t) is given by:
pc(t)
=



0
v(t) ≤ Bmin
σ(v(t) − ς)
Bmin < v(t) < Bmax
1
v(t) ≥ Bmax
(8)
where σ and ς are the RED parameters given by
Pmax
Bmax−Bmin
and Bmin, respectively, and v(t) is the average queue size,
which evolves according to the equation:
˙v(t)
=
−βC(v(t) − b(t)),
0 < β < 1 (9)
Notice that in the above relationship, we multiply β by C
since RED updates the average queue length at every packet
arrival, whereas our model is a ﬂuid model as indicated in
[21], [23].
The loss delegation between the Free and the Stunts
causes them to pick up different congestion signals than
those set by RED. In particular, the Free connection, upon
delegating g(t) of its congestion signals, would pick up:
q(t)
=
pc(t) − g(t)
(10)
Each Stunt connection would pick up:
q(t)
=
pc(t) + g(t)
s
(11)
The normal cross-trafﬁc are not affected and will simply
pick up:
q(t)
=
pc(t)
(12)
The throughput of TCP, xi(t) is given by
xi(t)
=
wi(t)
ri(t)
(13)
where wi(t) is the size of the TCP congestion window for
sender i .
According to the TCP Additive-Increase Multiplicative-
Decrease (AIMD) rule, the dynamics of TCP throughput for
each of the m connections can be described by the following
differential equations:
˙xi(t)
=
xi(t − ri(t))
r2
i (t)xi(t) (1 − q(t − Dbsi(t)))
−
xi(t)xi(t − ri(t))
2
(q(t − Dbsi(t)))
i
=
1, 2, .., m
(14)
where q(.) is the congestion signals observed by each
connection based on its type. The ﬁrst term represents the
additive increase rule, whereas the second term represents
the multiplicative decrease rule. Both sides are multiplied by
the rate of the acknowledgments coming back due to the last
window of packets xi(t−ri(t)). In the above equations, the
time delay from the bottleneck to sender i, passing through
the receiver i, is given by
Dbsi(t)
=
ri(t) − Dsib
(15)
Mode Assumptions: The model above makes the following
assumptions: (1) It ignores the effect of slow-start and
timeout mechanisms of TCP, since our main focus is on
the AIMD. (2) The delegation of some losses can be dis-
tributed in a linear fashion among the Stunts (as indicated in
Equation 11). In general, this does not hold except for small
value of losses, since the throughput is inversely proportional
to the square-root of the loss probability. Despite these
assumptions, however, the model above still captures the
main dynamics as we illustrate below.
B. Numerical Results
We instantiate the model above with speciﬁc parameters
and we solve it iteratively. We assume there is 1 Free
connection, 4 Stunts and 15 cross-trafﬁc, for a total of 20
connections. The bottleneck has a capacity 2000 packets/sec.
The RTT for each connection is chosen at random around
100 msec.
0
1000
2000
3000
4000
5000
6000
7000
8000
0
50
100
150
200
250
300
350
400
450
Time
Throughput
Free
Average Stunts
Average CrossTraffic
Target
Figure 3.
Numerical Results.
Figure 3 illustrates the performance of the Free connection
in matching a target trace that starts with constant throughput
at 200 packets/sec and then follows a sin wave. The ﬁgure
also shows the average throughput across the stunts as well
as the average throughput across the cross-trafﬁc connec-
tions. One can observe how the Stunt connections make
room for the Free connection to match the target throughput.
Notice also, how little the normal cross-trafﬁc is affected,
except for the initial startup time (the ﬁrst 3 seconds) where
the whole system is still in a transient behavior. One can
also see the impact of reverse delegation around time 6000.
466
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 0
 50
 100
 150
 200
 250
 300
 350
 400
MB / sec
Time
Stunt Avg
Free
Trace
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0
 50
 100
 150
 200
 250
 300
MB / sec
Time
Stunt Avg
Free
Trace
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 0
 50
 100
 150
 200
 250
 300
MB / sec
Time
Stunt Avg
Free
Trace
Figure 4.
Three simulation traces to assess the Free connection’s throughput in matching the target throughput.
Since the target throughput drops below the fair-share (100
packets/sec), the Stunts can delegate congestion signals to
the Free connection and thus they are able to increase their
throughput a bit above their fair-share.
V. SIMULATION EXPERIMENTS
We have implemented our proposed architecture in
NS-2 [26]. In this section, we study the performance of
our proposed architecture under differing environments
(topologies and trace ﬁles) and parameters.
The Setup: Figure 1 depicts the general topology of the
simulated network. It is composed of a single bottleneck
link that is traversed by the Free, Stunts, and cross-trafﬁc
connections. We assume all connections have an inﬁnite
supply of data to transmit. However, to study the impact
of different dynamics that arise in practice, a number of the
cross-trafﬁc connections are turned on and off at varying
times during a given simulation run. We also vary the
number of Stunt connections to demonstrate and examine
the behavior of our proposed architecture under different
congestion levels.
The bottleneck link is conﬁgured with RED [25]. The
queue size at the bottleneck link is chosen to be RT T ×C
√m
as
advocated in [27], where C is the bottleneck link capacity
and m is the total number of connections traversing the
bottleneck. This is because on fully utilized simulation
networks, those composed of only long lived TCP ﬂows,
the justiﬁcation for dividing by the square root of m breaks
down due to synchronization of network ﬂows [27] [28];
however, when randomized cross trafﬁc ﬂows are added
the premise is regained for the reduction in the buffer size.
The RED parameter Bmin is set to 0.25 the size of the
queue resulting in the distance between Bmin and Bmax
being three times Bmin. Other parameters were chosen to
encourage the stability of the average queue size.
Performance Metrics: To measure the effectiveness of our
proposed architecture in matching the achievable throughput
to the requested throughput, we propose a weighted variant
of the standard ”sum-of-squared errors” method. The main
problem with the standard ”sum-of squared errors” is that it
does not differentiate between the case where the achieved
throughput is above the target, versus the case where the
achieved throughput is below the target (since in both cases
we may get the same value). So we deﬁne the positive
variance V + to be:
V +
=
P(xi − Ti)2
C+
∀ xi > Ti
(16)
where C+ is the number of times (sample points) the
achieved throughput is above target. Similarly, we deﬁne
the negative variance V − to be:
V −
=
P(xi − Ti)2
C−
∀ xi < Ti
(17)
where C− is the number of times (sample points) the
achieved throughput is below target. To capture the overall
performance we use a weighted variance, deﬁned by:
V ∗
=
δ ×
P(xi − Ti)2
C+ + C−
(18)
where δ is a ratio that is given by:
δ
=
max(V +, V −)
min(V +, V −)
(19)
where δ is always greater than or equal to 1. If the matching
is achieved ideally, then δ would be 1. A larger value of δ
indicates a bias in the matching, either above or below the
target, and this would increase the weighted variance in turn.
The above metrics are computed over an entire simulation
experiment.
A. Matching the Target Throughput
This set of experiments assess the ability of the Free
connection in matching its throughput to different target
trace ﬁles.
Figure 4 shows representative results using three different
trace ﬁles with three different parameters. All results were
obtained using a PI controller and with 10 Stunt connections.
In each one, we plot the target trace, the throughput of the
Free connection and the average throughput across all Stunts.
Figure 4 (left) was obtained using a topology with 40 Mbps
467
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

bottleneck link capacity. In order to provide some variability,
8 of the cross-trafﬁc connections through the bottleneck
were randomized at ten second on/off intervals with the
exception of 2 cross-trafﬁc ﬂows, which were continuous.
This experimentation ensured that the ﬂows through the
bottleneck did not experience many timeouts.
Figure 4 (middle and right) were obtained using a topol-
ogy with 80 Mbps bottleneck link capacity. The number of
cross-trafﬁc links was kept constant at 20 connections. Over-
all, one can see that the Free connection does a fairly good
job in matching the target throughput while the throughput
achieved by the Stunts changes in tandem. Notice the larger
oscillations at higher data rates; these are expected due to
the normal behavior of the AIMD mechanism. We see the
opposite effect at lower data rates, due to a smaller decrease
in bandwidth.
Notice also that the reverse delegation of losses from the
Stunts to the Free connection allows the Stunts to achieve
higher rates than they would have achieved otherwise. The
slack of bandwidth given up by the Free connection goes
directly to the Stunts as opposed to going to the Stunts
and the cross-trafﬁc. This is evident from Figure 4 (right)
from time 150 until around 235 seconds. During this time
interval, the Stunts are achieving higher throughput than
their fair share, since the Free connection does not need
that throughput. This also conﬁrms our numerical results in
Figure 3.
This experiment makes it clear that the Free ﬂow can
acquire a variety of target waveforms. Experimentation
showed that the limiting factors were virtually all related
to network latency and congestion. We understand this to
be due to the fact that the architecture is designed around
TCP congestion signals. Naturally, the ability of the Free
ﬂow to achieve a requested target rate is dependent on the
ability of the network to support that link utilization. As was
mentioned earlier, Figure 4 (left) is an example of the Free
ﬂow simulated in a well provisioned network with moderate
cross-trafﬁc dynamics and link utilization. That ﬁgure also
shows very good ﬁtness with regard to the target waveform.
B. Impact of the Number of Stunt Connections
To study the impact of the number of Stunt connections on
the performance of the Free connection, we vary the number
of Stunts while holding all other parameters constant and we
plot the weighted variance (as given in Equation 18) versus
the number of Stunts. As mentioned in Section I, these Stunt
connections already exist due to the normal operation of
the server. We do not advocate creating them to make this
architecture work.
Figure 5 shows the results obtained (the non-random
cross-trafﬁc plot), where each point represent an independent
simulation run. One can observe that there is an optimal
number of Stunts (around 5 or 6) that minimizes the
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
weighted variance
stunts
non-random cross traffic
randomized cross traffic
Figure 5.
Impact of the number of Stunts on the weighted variance for
non-random cross trafﬁc and randomized cross-trafﬁc.
weighted variance. Increasing the number of Stunts further,
not only shows diminishing returns but also harms the
performance of the Free connection as indicated by a slight
increase in the weighted variance towards the higher number
of Stunts.
Figure 6 shows the exact performances with 2, 10 and
20 Stunts, respectively. These plots were generated on
topology of an 80 Mbps bottleneck link with 20 cross-trafﬁc
connections. Clearly, a very small number of Stunts (2) has a
noticeable degrading effect on the performance of the Free
connection, which improves with an increased number of
Stunts up to a point where is starts decreasing again.
The number of Stunts affects the overall efﬁciency of this
method because Stunts act more than just being a reservoir
of bandwidth for the Free connection. In particular, they
adjust the level of congestion in the network for the Free
connection to better match the target throughput. If their
number is very low, the Free connection would not be able to
delegate losses (since we strictly enforce the same reduction
in congestion windows from all Stunts). If their number is
very large, the network would be more congested and all
ﬂows would go into timeouts/slow-start, which may prevent
the Free connection to match the target throughput. One
approach to handle this case would be to delegate timeouts,
however, our focus in this paper was mainly on the AIMD
mechanism as explained earlier.
C. Impact of Cross-trafﬁc Dynamics
To study the impact of dynamics that arise in practice,
we allow a number of the cross-trafﬁc connections to be
turned on and off at random times during a given simulation
run. The cross-trafﬁc ﬂows are turned on and off every 10
seconds randomized with a uniform distribution.
Figure 7 shows the impact of varying the number of the
cross-trafﬁc connections, while keeping the number of Stunts
steady at 10. We plot the positive variance, the negative
variance, and the weighted variance to better explain a
unique behavior observed in this experiment.
468
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 0
 100
 200
 300
 400
 500
MB / sec
Time
Stunt Avg
Free
Trace
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 0
 100
 200
 300
 400
 500
MB / sec
Time
Stunt Avg
Free
Trace
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 0
 100
 200
 300
 400
 500
MB / sec
Time
Stunt Avg
Free
Trace
Figure 6.
Impact of the number of Stunt connections on the performance. Left plot with 2 Stunts, middle plot with 10 Stunts and right plot with 20
Stunts.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
 0.22
 0
 5
 10
 15
 20
metric value (low is better)
# of randomized cross-traffic flows
+ variance
- variance
weighted variance metric
positive variance
negative variance
Figure 7.
Impact of increasing the level of dynamic cross-trafﬁc connec-
tions.
It is clear that the higher the dynamics from the random-
ized cross-trafﬁc, the harder it is for the Free connection to
match the target. However, when we examined the exact
performance of the Free connection, we found that its
shape was rather intact than deformed, but it was above
the requested target. This was conﬁrmed visually in each
simulation run and is easy to see by examining the positive
and negative variance metrics. Notice how the positive
variance grows larger as the negative variance grows smaller.
Such divergence increases the weighted variance due to a
higher δ. The throughput of the Free connection was above
the target because with a larger number of randomized cross-
trafﬁc, their combined throughput decreased the utilization
at the bottleneck. This caused the Free connection to acquire
more than the target because the lower link utilization also
resulted in a lower packet loss probability.
Figure 5 (the randomized cross-trafﬁc plot) shows the
impact of the number of Stunts when most of the cross-
trafﬁc connections (19 out 20) were randomized on a 10-
second on/off intervals. The presence of dynamics, coupled
by an increase in the Stunts lead to a degraded matching
between the the throughput of the Free connection and the
target throughput.
D. On the Feasibility of the Free and the Stunts Architecture
As hinted in Section I, Internet measurement studies have
indicated that around 80% of the trafﬁc is controlled by a
small number of connections (elephants) while the majority
of connections (mice) control only around 20% of the trafﬁc
[4], [5], [6]. This is a direct effect of the heavy-tailed nature
of ﬁle sizes on the Internet.
To study the execution of our proposed architecture in
an environment with such trafﬁc properties, we used a
simulation network that is composed of 1 Free ﬂow and 3
Stunt ﬂows, all are elephant ﬂows. In addition, the cross-
trafﬁc connections consisted of 4 elephant ﬂows and 24
mice ﬂows. Although, we initially classiﬁed ﬂows as being
elephants versus mice, such classiﬁcation can be achieved
dynamically as indicated in [29]. The bottleneck link is
10 Mbit with 10 ms latency. All links into and out of
the bottleneck are 100 Mbit with 2 ms latencies. All the
ﬂows have an on/off state with a Pareto distribution with
shape 1.5. The elephants have a mean burst time of 120
seconds and idle time of 5 seconds. The mice have a mean
burst time of 1 second and idle time of 5 seconds. The
parameters were chosen to produce a close distribution of
trafﬁc between elephants and mice that is close to the 80%-
20% rule observed on the Internet. We limit the choice of
Stunts to elephant ﬂows.
Figure 8 (left) shows the results of an experiment run
using the above parameters. We observe that, even with
the presence of dynamics across all ﬂows, utilizing the
elephants as Stunts achieves a good matching between the
Free connection’s throughput and the target trace. That is
because they control a large percentage of the capacity and
are able to accept congestion signals delegations from the
Free connection. Figure 8 (right) shows the results of a
different experiment where the Free ﬂow is turned on and off
(we show the trace only in the on period of the Free ﬂow).
The elephant ﬂows have a mean burst time of 10 seconds
and idle time of 5 seconds. The mice ﬂows have a mean
burst time of 300 msec and 13 seconds. In this experiment,
there were 10 elephant connections (only 4 were used as
Stunts) and 50 mice connections. Again the parameters were
chosen to produce the 80%-20% rule between elephant and
mice. Utilizing few elephant ﬂows as Stunts results in a good
matching.
469
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 0
 100
 200
 300
 400
 500
 0
 200
 400
 600
 800
 1000
Kb / sec
Time
Stunt Elephant avg
Free Elephant
Trace
 0
 50
 100
 150
 200
 250
 300
 0
 200
 400
 600
 800
 1000
Kb / sec
Time
Stunt Elephant avg
Free Avg
Trace
Figure 8.
Simulation results with Pareto distribution. Elephants and mice consume around 80% and 20% of the bottleneck’s capacity, respectively. Right
plot considers the Free connection turning on and off as well.
VI. IMPLEMENTATION EXPERIMENTS
We have implemented the Free and the Stunts architecture
as a pluggable module for the 2.6.20 Linux kernel. We
have chosen to experiment with the On/Off controller since
the Linux kernel does not natively support ﬂoating point
operations (implementing the PI controller is harder since it
requires changing all math operations to ﬁxed-point ones).
Client (C)
100 Mbps Router
Client (X)
(cross-traffic)
10 Mbps Hub
Server (S)
Internet
Free (1)
Cross-traffic
Stunts (7)
Figure 9. The experimental setup used in our implementation experiments.
The Setup: Figure 9 shows the experimental setup used
in our implementation experiments. It is composed of a
Server (S) and two Clients (C and X). The Server runs Linux
and implements the Free and Stunts pluggable module. The
server runs an application that accepts TCP connections from
clients and serves them continuous ﬂows of data. We have
created a bottleneck link of 10 Mbps at the Server’s ﬁrst
hop. All other links are 100 Mbps. All experiments were
composed of 1 Free connection and 7 Stunt connections,
from Client (C) to the Server. To simulate cross-trafﬁc on
the bottleneck link, we have manually opened and closed
connections between Client (X) and different Internet web-
servers.
A. Matching the Target Throughput
Our ﬁrst set of experiments illustrates the performance of
the free connection in matching the requested target. Figure
10 shows two different traces. Each plot shows the average
throughput over four independent runs. We also plot the
requested target as well as the adjusted target (1.33 of the
requested target). One can observe that on average there
is a very close matching between the throughput and the
requested target.
 0
 100
 200
 300
 400
 500
 600
 100
 150
 200
 250
 300
 350
 400
Kb / sec
Time
Free
Stunt Avg
Adj-Target
Target
Figure 11.
Free connection Performance with cross trafﬁc introduced at
time 330.
Figure 11 shows the performance of a single Free connec-
tion in matching the target throughput. One can see that the
implementation does relatively well in matching the target.
At around 330, we manually opened several connections
from Client (X) to www.youtube.com over a period of 50
seconds and downloaded some videos in order to introduce
cross trafﬁc on the bottleneck link. One can see the effect
of those cross-trafﬁc connections on the performance on the
Free connection as indicated by few misses in matching the
target throughput.
B. Improving the Reverse Delegation
To improve the matching between the throughput of the
Free connection and the target rate, we have modiﬁed the
“reverse” delegation so that the Free connection does not
drop its congestion window more than necessary. Recall that
delegating a loss from a stunt connection to the free connec-
tion would cause the free connection to drop is congestion
window by half. Here we modiﬁed such adjustment so that
470
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 0
 100
 200
 300
 400
 500
 600
 700
 800
 0
 50
 100
 150
 200
 250
 300
KB/s
Time
avg-Free
target
adj-Target
avg-Stunts
 0
 100
 200
 300
 400
 500
 600
 700
 800
 0
 50
 100
 150
 200
 250
 300
KB/s
Time
avg-Free
target
adj-Target
avg-Stunts
Figure 10.
Implementation results for target matching.
the free connection would drop its congestion window by the
exact value the delegating stunt would have experienced, if
it were to pick up this congestion signal.
-20
 0
 20
 40
 60
 80
 100
 25
 75
 125
 175
 225
KB/s
Time
Avg-Err Modified
Avg-Err UnModified
Figure 12.
Improving the reverse delegation.
Figure 12 depicts the trending behaviors of the modiﬁed
reverse delegation method versus the usual method, on the
error signal obtained as the difference between throughput
achieved and the target. The plotted lines are the average
error values over several runs.
One can observe from Figure 12 that the trending of the
targeting error is higher in certain areas of the plot. These
areas are predictably located where reverse delegation has a
higher probability of occurring (for example, the throughput
of the Free ﬂow needs to be reduced to meet the target).
The increased error means that the Free ﬂows throughput
is trending closer to the adjusted target, rather than the
requested target. We have experimented with different target
traces and they show the same types of trends. This behavior
was expected since the 1.33 target adjustment was computed
based on the exact halving of the congestion window, due to
normal TCP Congestion Control AIMD behavior. One can
mitigate this effect by having a closer adjusted target to the
requested target.
VII. CONCLUSION
This paper describes an architecture that enables a set
of clients to delegate and trade congestion signals between
them in order to shape trafﬁc based on demands from the
applications. The architecture strictly adheres to TCP rules
and does not affect network congestion on the bottleneck
links. Moreover, no modiﬁcations is required to any devices
along the communication path (unless an ISP decides to
implement this architecture to manage ﬂows). We have
shown that this service architecture is capable of providing
a reasonably accurate targeting between the achieved rate
and the target rate with a small number of Stunts. We
have assessed the performance of our proposed architecture
through new metrics, using numerical solutions, extensive
simulation experiments and real implementation in the Linux
kernel.
REFERENCES
[1] J. Valdez and M. Guirguis, “Liberating TCP: The Free and the
Stunts,” in Proceedings of the 7th International Conference
on Networking (ICN 2008), Cancun, Mexico, April 2008.
[2] V. Cerf and L. Kahn, “A Protocol for Packet Network
Interconnections,” IEEE Transactions on Communications,
vol. 22, no. 5, June 1974.
[3] V. Jacobson, “Congestion Avoidance and Control,” in Pro-
ceedings of ACM SIGCOMM, Stanford, CA, August 1988.
[4] K. Thompson, G. Miller, and R. Wilder, “Wide-Area Internet
Trafﬁc Patterns and Characteristics,” IEEE Networks, vol. 11,
no. 6, 1997.
[5] S. Fred, T. Bonald, A. Proutiere, G. R´egni´e, and J. Roberts,
“Statistical Bandwidth Sharing: A Study of Congestion at
Flow Level,” in Proceedings of ACM SIGCOMM, San Diego,
CA, August 2001.
[6] C. Fraleigh, S. Moon, B. Lyles, C. Cotton, M. Khan, D. Moll,
R. Rockell, T. Seely, and S. Diot, “Packet-level Trafﬁc Mea-
surements from the Sprint IP Backbone,” Network, IEEE,
vol. 17, no. 6, pp. 6–16, 2003.
471
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[7] L. Guo and I. Matta, “The War between Mice and Elephants,”
in Proceedings of IEEE ICNP, Mission Inn, Riverside, No-
vember 2001.
[8] C. Barakat and E. Altman, “Performance of Short TCP
Transfers,” Lecture Notes in Computer Science, pp. 567–579,
2000.
[9] M. Mellia, H. Zhang, and D. di Elettronica, “TCP Model for
Short Lived Flows,” IEEE Communications Letters, vol. 6,
no. 2, pp. 85–87, 2002.
[10] J. Londono, A. Bestavros, and N. Laoutaris, “Trade and Cap:
A Customer-Managed, Market-Based System for Trading
Bandwidth Allowances at a Shared Link,” Technical Re-
port BUCS-TR-2009-025, CS Department, Boston University,
2009.
[11] R. Braden, D. Clark, and S. Shenker, “Integrated Services in
the Internet Architecture: An Overview,” RFC 1633, 1994,
June 1994.
[12] S. Blake, D. Black, M. Carlson, E. Davies, Z.Wang, and
W. Weiss, “An Architecture for Differentiated Services,” IETF
RFC 2475, 1998, December 1998.
[13] H. Balakrishnan, H. Rahul, and S. Seshan, “An Integrated
Congestion Management Architecture for Internet Hosts,” in
Proceedings of ACM SIGCOMM, Cambridge, MA, August
1999.
[14] D. Ott and K. Mayer-Patel, “An Open Architecture for
Transport-level Protocol Coordination in Distributed Mul-
timedia Applications,” ACM Transactions on Multimedia
Computing, Communications, and Applications (TOMCCAP),
2007, August 2007.
[15] Z. Rosberga, J. Matthewsa, and M. Zukerman, “A Network
Rate Management Protocol with TCP Congestion Control and
Fairness for All,” Elsevier Computer Networks, vol. 54, no. 9,
June 2010.
[16] Z. Rosberg, J. Matthews, C. Russell, and S. Dealy, “Fair
and End-to-End QoS Controlled Internet,” in Proceedings
of 3rd International Conference on Communication Theory,
Reliability, and Quality of Service (CTRQ), Athens, Greece,
June 2010.
[17] M. Guirguis, A. Bestavros, I. Matta, N. Riga, G. Diamant,
and Y. Zhang, “Providing Soft Bandwidth Guarantees Using
Elastic TCP-based Tunnels,” in Proceedings of the 9th IEEE
Symposium on Computer and Communications, Alexandria,
Egypt, July 2004.
[18] T. Tsugawa, N. Fujita, T. Hama, H. Shimonishi, and
T. Murase, “TCP-AFEC: An Adaptive FEC Code Control for
End-to-End Bandwidth Guarantee,” in Proceedings of 16th
International Packet Video Workshop, Lausanne, Switzerland,
November 2007.
[19] P. Siripongwutikorn, S. Banerjee, and D. Tipper, “A Survey of
Adaptive Bandwidth Control Algorithms,” IEEE Communica-
tions Surveys and Tutorials, vol. 5, no. 1, pp. 14–26, 2009.
[20] H. Wei and Y. Lin, “A Survey and Measurement-based
Comparison of Bandwidth Management Techniques,” IEEE
Communications Surveys and Tutorials, vol. 5, no. 2, 2009.
[21] C. Hollot, V. Misra, D. Towsley, and W. Gong, “A Control
Theoretic Analysis of RED,” in Proceedings of IEEE INFO-
COM 2001, Anchorage, AL, April 2001.
[22] F. Kelly, “Mathematical Modelling of the Internet,” Mathe-
matics Unlimited and Beyond, 2001, pp. 685–702, 2001.
[23] S. Low, F. Paganini, J. Wang, S. Adlakha, and J. Doyle, “Dy-
namics of TCP/RED and a Scalable Control,” in Proceedings
of IEEE INFOCOM, New York, NY, June 2002.
[24] S. Shenker, “A Theoretical Analysis of Feedback Flow Con-
trol,” in Proceedings of ACM SIGCOMM, Philadelphia, PA,
September 1990.
[25] S. Floyd and V. Jacobson, “Random Early Detection Gate-
ways for Congestion Avoidance,” Transactions on Network-
ing, vol. 1(4), pp. 397–413, August 1993.
[26] E. Amir and et al., “UCB/LBNL/VINT Network Simulator -
ns (version 2),” available at http://www.isi.edu/nsnam/ns/.
[27] G. Appenzeller, I. Keslassy, and N. McKeown, “Sizing Router
Buffers,” in Proceedings of ACM SIGCOMM, Portland, Ore-
gon, August 2004.
[28] L. Zhang, S. Shenker, and D. Clark, “Observations on the
Dynamics of a Congestion Control Algorithm: The Effects
of Two-Way Trafﬁc,” in Proceedings of ACM SIGCOMM,
Zurich, Switzerland, September 1991.
[29] K. Avrachenkovt, U. Ayesta, P. Brown, E. Nyberg, and
F. INRIA, “Differentiation between Short and Long TCP
Flows: Predictability of the Response Time,” in Proceedings
of IEEE INFOCOM, vol. 2, 2004.
472
International Journal on Advances in Networks and Services, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

