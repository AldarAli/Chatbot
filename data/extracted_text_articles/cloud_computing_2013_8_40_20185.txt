Forensics-as-a-Service (FaaS): Computer Forensic Workﬂow Management and
Processing Using Cloud
Yuanfeng Wen, Xiaoxi Man, Khoa Le and Weidong Shi
Department of Computer Science
University of Houston
Houston, Texas 77204-3010
e-mail: {wyf, xman, ktle, larryshi}@cs.uh.edu
Abstract—Digital forensics is a critical technology for obtaining
evidences in crime investigation. Nowadays, the overwhelming
magnitude of data and the lack of easy-to-deploy software are
among the major obstacles in the ﬁeld of digital forensics. Cloud
computing, which is designed to support large scale data pro-
cessing on commodity hardware, provides a solution. However,
to support forensic examination efﬁciently using cloud, one has
to overcome many challenges such as lack of understanding and
experiences on conﬁguring and using digital forensic analytic
tools by the investigators, and lack of interoperability among the
forensic data processing software. To address these challenges
and to leverage the emerging trends of service based computing,
we proposed and experimented with a domain speciﬁc cloud
environment for supporting forensic applications. We designed a
cloud based framework for dealing with large volume of forensic
data, sharing interoperable forensic software, and providing
tools for forensic investigators to create and customize forensic
data processing workﬂows. The experimental results show that
the proposed approaches can signiﬁcantly reduce forensic data
analysis time by parallelizing the workload. The overhead for the
investigators to design and conﬁgure complex forensic workﬂows
is greatly minimized. The proposed workﬂow management solu-
tion can save up to 87% of analysis time in the tested scenarios.
Keywords—cloud computing; digital forensics
I. INTRODUCTION
Digital forensics is a technology to collect, examine, ana-
lyze, but still preserve the integrity of the data in modern high-
tech crimes [1]. Digital forensics were conventionally used in
physical hardware analysis, such as hard-disk, ﬂash drives. As
the ever increasing computing and storage needs arising in the
Internet age, investigators in the public and private sectors are
facing the same growing challenge when dealing with com-
puter forensics [2], which is to examine an increasing number
of digital devices (e.g., GPS gadgets, smartphones, routers,
embedded devices, SD cards), each containing an immense
volume of data, in a timely manner and with limited resources.
At the same time, with proliferation of low cost and easy-
to-access anti-forensic techniques (sometimes open source as
well), offenders are becoming increasingly sophisticated and
skillful at concealing information.
Computer forensic investigators and examiners are con-
fronted with the problems of, (i) unacceptable backlog of
information waiting for examination; (ii) miss of critical time
window to follow the leads due to slowness of computer
forensic examination; (iii) lack of understanding of the com-
puter forensics and consequent incapability by the detectives
to take advantages of digital forensic techniques to advance
investigations; and (iv) overlook of relevant data and waste of
resources due to lack of understanding of crime investigations
by the forensic examiners.
The cloud computing model provides ideal opportunities to
solve these problems. Cloud computing is a rapidly evolving
information technology that is gaining remarkable success in
recent years. It uses a shared pool of virtualized and con-
ﬁgurable computing resources (both hardware and software)
over a network to deliver services, such as to host and analyze
large datasets immediately. These resources and services can
be rapidly provisioned and released with minimal manage-
ment effort or service provider interaction. Cloud computing
is almost everywhere. Governments, research institutes, and
industry leaders are quickly adopting the cloud computing
model to solve the increasing computing and storage demands.
This trend has signiﬁcant implications for digital forensic
investigations.
However, current forensic research related to the cloud is
mainly focused on the stage of data collection (e.g.,
[3]).
The examination and analysis on the data are still performed
on local machines instead of in the cloud. Extending the
services to the cloud often calls for the external assistance
and professional software/applications. Researchers have made
efforts to build a forensic cloud. Sleuth-Hadoop [4] tries
to integrate different forensic analysis tools into the cloud.
However, Sleuth-Hadoop doesn’t have the ﬂexibility for the
investigators to build and customize the desired analysis work-
ﬂow for speciﬁc forensic datasets.
The main contribution of our work is to ﬁll the gaps. We
propose a domain speciﬁc cloud environment for forensic
applications. We designed a cloud infrastructure framework
for dealing with large forensic datasets, sharing forensic
software, and providing a way for the investigators to build
workﬂows using a common interface. We proposed a schema-
based forensic analysis workﬂow framework. The framework
allows the forensic investigators to deﬁne their requirements
in XML conﬁguration ﬁles. Supported with a collection of
forensic applications, the framework can select the appropriate
applications, generate the corresponding map-reduce drivers,
and set up the workﬂow in the cloud, automatically for the
208
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-271-4
CLOUD COMPUTING 2013 : The Fourth International Conference on Cloud Computing, GRIDs, and Virtualization

users.
The rest of this paper is organized as follows. Section II
presents the system design of the forensic cloud. Section III
shows the experimental results. Related works are discussed
in Section IV. Section V concludes the paper.
II. BACKGROUND
Four categories of cloud computing are deﬁned by NIST
(National Institute of Standards and Technology) [5], i.e.,
private cloud, community cloud, public cloud, and hybrid
cloud. Currently, most research focuses on the community
cloud and public cloud.
In the community cloud study, there are many solutions pro-
posed for data sharing and collaborations. At Hewlett-Packard
Labs, Erickson et al. [6] use a cloud-based platform to provide
content-centered collaboration in the Fractal project. Social
sharing of workﬂows are studied by Roure et al. [7]. Globus
Online [8] focuses on data-movement functions to deal with
new challenges brought by data-intensive, computational, and
collaborative scientiﬁc research through cloud-based services.
Compared with these studies, our work mainly concentrates on
the workﬂow management in computer forensics and domain
speciﬁc cloud infrastructure. Various kinds of other community
cloud are also studied, e.g., volunteer cloud [9], [10], Nebula
cloud [11], social cloud [12]. However, none of those is specif-
ically designed for computer forensics. For domain speciﬁc
applications, the one size ﬁts all approach would not work
because the speciﬁc characteristics and requirements from
each application domain often demand customized solutions
built on top of the cloud infrastructure.
In the public cloud, since users have different purposes to
run their applications, studies mainly focus on the general-
purpose resource management. For example, public cloud such
as Amazon EC2[13] uses a scheduler in Xen hypervisor to
schedule virtual machines. Song et al. [14] proposed a multi-
tiered on-demand resource scheduling scheme to improve
resource utilization and guarantee QoS in virtual machine
based data centers.
One of the most popular programming models in the cloud
is MapReduce [15], which is for distributed processing of
large-scale data on clusters of commodity servers. Anantha-
narayanan et al. [16] proposed an optimized cluster ﬁle system
for MapReduce applications. They use metablock that is a
consecutive set of blocks of a ﬁle that are allocated on the
same disk instead of the traditional cluster ﬁle system. Apache
Pig [17] is a platform for analyzing large data sets using
MapReduce on the top of Hadoop.
Digital forensics are performed in four phases [2], i.e., col-
lection, examination, analysis and reporting. The investigators
will execute the following separately, 1) identifying, recording,
acquiring data from possible sources, while preserving the
integrity of the data; 2) processing the data with a combination
of manual and automated methods, and extracting data of
particular interest; 3) analyzing the results of the examination
with legally justiﬁable methods and techniques to derive useful
information; 4) describing the results of the analysis.
Forensic software provides many different kinds of tools
to investigate suspicious servers, desktops, and personal dig-
ital devices such as cell phones, GPS navigators, PDAs,
etc. The investigations mainly focus on discovering foren-
sic evidence, and identifying suspicious ﬁles and activities.
Bulk extractor [18] can scan suspicious ﬁles and email and
extract data from the disk images, ﬁles, and directories. Many
comprehensive tools, such as FTK [19], OSForensics [20],
Intella [21], etc., provide the investigation functions. However,
they are stand-alone software running on local machines.
Supports for inter-operations and large scale automated paral-
lelization are poor, or almost none. Open Computer Forensics
Architecture (OCFA) [22] is an automated system that can
extract metadata from ﬁles, create indices for the target disk
images and ultimately output a repository containing the ﬁles
and indices for further examination. OCFA is able to work
with other third part analysis software or data mining tools.
The limitation of the OCFA is that it is not integrated with
the cloud.
Sleuth Kit [23] has a cloud-based version, Sleuth Hadoop,
which integrates several forensic software and enables them
to run in the cloud. However, the analysis workﬂow is ﬁxed
in Sleuth Hadoop [4] without the capabilities to conﬁgure and
construct workﬂow dynamically. It doesn’t support collabora-
tive software development and workﬂow management.
III. SYSTEM DESIGN
A. System Overview
The forensic cloud infrastructure aims to deliver the services
that go beyond today’s models of “software-as-a-service” and
“infrastructure-as-a-service”, with the goal of providing not
only elastic computing resources for on-demand computer
forensic data processing, but also an environment for in-
telligent forensic workﬂow management, customization, and
collaboration.
The forensic cloud comprises two main layers: a service
layer and a physical resource layer, as shown in Figure 1. The
service layer has three major components, the forensic data
manager, the forensic application manager and the forensic
workﬂow manager. The physical layer is composed of physical
devices such as accelerators, physical servers, and storage
servers for supporting forensic data banks. A set of virtual
machines can be allocated for serving a particular forensic
data processing task.
B. Forensic Data Manager
Forensic data manager provides supports for uploading,
storing, and retrieving the large-scale forensic data in the
cloud. Forensic data are collected from diverse sources (e.g.,
disks, cellphones, embedded devices). With elastic storage
resources provided by the cloud, forensic investigators can
process, analyze, and archive forensic data with reduced cost,
improved efﬁciencies, and increased productivity.
Considering the scale of the data and the fact that most
applications in the cloud use MapReduce [15] for paral-
lelizing the applications and performing the analysis on the
209
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-271-4
CLOUD COMPUTING 2013 : The Fourth International Conference on Cloud Computing, GRIDs, and Virtualization

Global Resource Virtualization Layer
Global DFS
Customized 
Mapreduce for 
Forensic Tasks
Bigtable Like 
Data Model 
Support
Meta 
Data
Forensic Data 
Bank
Forensic App
Library/Store
Forensic Workflow Support
(Scripting/Management/Planning)
User 
Interface
Collaboratio
n Interface 
Developer 
Interface
Digital Forensic as a Service 
(Software Stack)
Digital Forensic Cloud
Data Manager
Application 
Manager
Physical 
Servers
Digital Forensic 
Tasks
Resource 
Plans
Workflow 
Manager
Storage 
Servers
Forensic Specific Components
General Cloud Services
VM Servers
Fig. 1.
Forensic Cloud Overview and Software Stack
data, the data manager uses HDFS (Hadoop Distributed File
System) [24] to store the data. HDFS is a distributed ﬁle
system designed to work on commodity hardware maintained
as a Hadoop subproject. HDFS stores all the data in blocks.
The block size is usually 64MB or 128MB. HDFS works
more efﬁciently if the single ﬁle size is larger than the block
size, which, however, is not necessarily always the case for
all the ﬁles in a target disk image. To avoid the small-ﬁle
problem, the data manager organizes the ﬁles in HAR ﬁles
or SequenceFile formats [25]. Creating a working copy, is
managed by the forensic data manager as well. The forensic
data manager also ﬂattens all the directory information, which
exports all the nested ﬁles into one folder. This can mitigate
the anti-forensic (AF) approach called, “circular references”.
The “circular references” exploit uses symbolic links to point
to a parent folder, which may make a search operation run for
ever.
In addition, the data manager also maintains the metadata of
the ﬁles in the HBase (an open-source, distributed, versioned,
column-oriented store modeled after Google’s Bigtable [26]).
The metadata contains useful data for the ﬁles, for instance, the
directory structure information before ﬂatting, the hash values
(MD5) of the ﬁles. The information is often used in analyzing
the forensic data. For example, National Software Reference
Library (NSRL) [27] provides a comprehensive database with
the hash values for almost all the commercially available
software. This provides a Reference Data Set (RDS) of in-
formation [27], which can be used as digital signatures of the
known, good software applications. Therefore, by comparing
the hash values of the ﬁles in a target disk with the database,
the investigators can ﬁlter out all the uninterested ﬁles. This
Known File Filter (KFF) operation can signiﬁcantly reduce the
sizes of the data that requires examination. All other similar
metadata are calculated by the data manager and stored in the
HBase. This is a default step when new ﬁles are uploaded to
the forensic cloud and to be ingested.
With the help of the universal management of the data,
forensic analysis and data mining experts who develop soft-
ware for forensic data processing only need to submit their
software to the cloud.
C. Forensic Application Manager
Forensic applications and software such as ﬁles/emails
search, image/videos analysis, etc. are created through collabo-
rative processes involving many forensic experts and computer
science researchers. To accelerate productivity and expedite
collaborations among them, it is necessary to reuse the soft-
ware and workﬂow. Forensic software vendors can distribute
the developed algorithms and software to a software/app
library, the “forensic app store” where forensic workﬂow can
be constructed using these software. Forensic examiners and
investigators can on-demand create, invoke, and deploy tasks
using the forensic software and workﬂow stored in the library.
Consequently, the infrastructure will accelerate dissemination
and deployment of new forensic techniques.
All the applications in the “forensic app store” are tagged
and categorized by the application manager. The application
manager periodically generates an XML schema and metadata
for all the available software. The schema is used to generate a
user-friendly front-end web page (maintained by the workﬂow
manager) and to validate the XML-based workﬂow conﬁgura-
tion ﬁle.
An example schema ﬁle and xml conﬁguration ﬁle are
shown in Figure 2. In the schema ﬁle on the left of Figure 2,
all the four applications available in the “app store” are listed.
The digital forensic front-end web page can read the schema
ﬁle and generate a drop-down list with these applications
when a forensic investigator selects the applications. The
investigators only need to click several buttons to generate
an XML conﬁguration ﬁle as shown on the right bottom of
210
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-271-4
CLOUD COMPUTING 2013 : The Fourth International Conference on Cloud Computing, GRIDs, and Virtualization

Digital Forensic Front-end
Forensic 
Investigator
Fig. 2.
An example of the schema XML generated by application manager and the XML workﬂow conﬁguration ﬁle generated by the workﬂow manager.
The ﬁle on the left is the schema XML listing all the four applications and the desired structure of the work conﬁguration ﬁle; the ﬁle on the right bottom is
the XML conﬁguration ﬁle with two tasks.
Figure 2. This conﬁgure ﬁle is used by the workﬂow manger
to generate MapReduce drivers and workﬂow assembly. For
more advanced investigators, they can directly write the XML
conﬁguration ﬁle and use the schema to valid the ﬁle. This
will reduce the chances of creating an invalid ﬁle. In reality,
there could be more categories than the example provided.
The application manager provides a set of default categories
of the applications, including FileIndexApp, KeywordSearch
App, ImageAnalysis App, etc. Users can also add customized
tags and categories into the cloud when uploading the new ap-
plications. In addition, more tags and supplementary categories
could be created by users. Users are allowed and encouraged
to rate the applications after using. The ratings are further
used for the application recommendation. The applications are
sorted from the highest rating to the lowest in the generated
XML ﬁle. Therefore, highly qualiﬁed applications will be
presented to users at the top of the candidate application
list. The user ratings are the key criteria to evaluate the
applications.
The application manager also provides recommendations.
Currently, it is community oriented. Each application will be
rated by all the users who have tried it. When the application
manager generates the schema ﬁle, the rating information will
be included. Therefore, when users select the application, they
are aware of the information that can be used to evaluate the
candidate applications.
D. Forensic Workﬂow Manager
Forensic investigators can send data processing jobs to the
cloud. For example, an investigator can specify, the objectives
of data processing, the input dataset (stored in the cloud
using forensic data manager), and other constraints. The cloud
can create a workﬂow by decomposing the user’s request
into multiple processing steps. The workﬂow manager is
responsible for setting up, optimizing, executing and reporting
the workﬂow.
1) Workﬂow Setup: The workﬂow manager represents a
workﬂow using an XML conﬁguration ﬁle. The structure of
this XML ﬁle is deﬁned in the schema ﬁle generated by
the application manager. Generally, the schema ﬁle contains
two kinds of information. One is for all the available ap-
plications or software in the “application store”, which are
deﬁned in a simple type (xs:simpleType) or a complex type
(xs:complexType); the other is the root element structure,
called “tasks”. The “tasks” may contain one or more “tasks”,
each of which needs the application name, input path, output
path, and parameters for execution. All the tasks on the same
level are independent and can be executed in parallel. If a user
would like to deﬁne the dependency between two tasks, the
second task should be conﬁgured as a “sequential task” of the
ﬁrst task. Figure 2 shows an example. Complex workﬂows
can be also described by assigning the subtasks, which can
be recursively built with arbitrary levels of dependencies.To
facilitate the procedure of setting up a forensic workﬂow, the
workﬂow manager uses the schema ﬁle to generate a user-
211
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-271-4
CLOUD COMPUTING 2013 : The Fourth International Conference on Cloud Computing, GRIDs, and Virtualization

Step 1:
Use National Software 
Reference Library  to 
filter files installed by 
the known-to-be-good 
commercial installer
Step 2:
Check the metadata 
information, filter the 
files by modification 
data, and only keep 
the files created/
modified in a specific 
range 
Step 2':
Check the EXIF 
(EXchangeable Image 
File) header of the 
output image to find 
suspicious images  
Step 3:
Build an inverted 
index on all the text 
files and search the 
specified keywords on 
the indexed files 
Fig. 3.
A Workﬂow Example Constructed by the Workﬂow Manager
friendly web portal, which allows forensic investigators to
design the workﬂow and select the desired applications. After
designing the workﬂow, the frontend will pass the workﬂow
to the backend engine. This engine will generate an XML
conﬁgure ﬁle and further generate the Map-Reduce drivers
for each step and the necessary synchronization codes (if
multiple steps are involved in the workﬂow) automatically for
the forensic investigators. The fewer lines of codes to write,
the less chance to generate errors.
2) Workﬂow Recommendation: Since each step could be
completed by multiple candidate software with data dependent
performance metrics, the workﬂow manager will try to make
optimal selection/recommendaton of software/workﬂow and
allocate resources accordingly with the objective of achieving
the best performance (result quality) for the input dataset with
the help of user ratings and the pre-deﬁned workﬂows. For
example, the workﬂow manager recommends building indices
before keyword search. Another example is that by default, the
workﬂow manager will select the National Software Reference
Library (NSRL) to ﬁlter out the typical contents created by
the commercial installer, such as dll, exe, static data. The
recommendations are based on the user ratings and evaluation.
An example is shown in Figure 3.
3) Workﬂow Execution: To execute the workﬂow, the work-
ﬂow manager allocates processing resources such as elastic
machine hours based on an optimized resource plan and
assigns workload to the allocated resources using the MapRe-
duce model customized for data intensive forensic compu-
tations. Then, the allocated resources execute the assigned
tasks on datasets retrieved from the cloud forensic data banks
administrated by the data manager. The workﬂow manager will
direct the workﬂow execution and track the status of each task
in the workﬂow.
4) Workﬂow Report: Finally, after ﬁnishing the workﬂow,
the workﬂow manager will generate a report to the users.
In addition, the workﬂow manager also stores the status and
report in its own database.
IV. EVALUATION
In this section, we present the results of a comprehensive
evaluation of our system.
A. Experimental Setup
During our evaluation, we deployed a forensic cloud
as described earlier using the Amazon’ Elastic Compute
Cloud(EC2) service. The deployment uses Medium Level-1
(M1) EC2 instances. According to Amazon, these are 64-bit
instances with 3.75 GB of memory, 410GB of harddisk and
one virtual core containing two EC2 compute units (ECU).
One ECU is equivalent to a 1.0-1.2 GHz Xeon processor. The
forensic cloud infrastructure is based on Hadoop 0.20 and
HBase 0.20, which is managed by Cloudear Manager [28].
The data from a volunteer’s hard drive image was uploaded
to the forensic cloud. Notice that, the uploading time is not
counted and evaluated in the following experiments. This is
because, as mentioned previously, the data used are collected
from different sources in a distributed way using the cloud
as well. We simpliﬁed the process by uploading a dedicated
image disk for studying purpose. Therefore, the uploading time
is not considered.
B. Experimental Results
First, we compared the system outputs and analyzed the
performance using the same disk image dataset, which is a
working disk image from volunteer users. Figure 4 shows the
forensic analysis time on the target image. The image size is
160GB. It shrinks to 10GB after applying the ﬁler operations
mentioned in the previous sections. The number of nodes used
in the experiment increases from 1 to 10. With more nodes
involved, the analysis time is reduced from 21 minutes to only
6 minutes, i.e., 71% of analysis time is saved. However, given
a ﬁxed size of test data, the analysis speed can’t be further
accelerated by adding more nodes. As shown in Figure 4, the
forensic cloud with more than 8 nodes has almost the same
performance. This is because when more nodes are involved,
some of the MapReduce tasks are not executed at the same
machine where the data are stored. Copying data between
nodes cuts down the beneﬁts. Figure 5 shows the percentage
of the MapReduce tasks running locally. The percentage drops
from 100% to 40% when the number of nodes changes from
2 to 10. This explains why the speedup of analysis time is
only 3. However, when the size of data to be analyzed keeps
increasing, more time can be saved, because more data blocks
can be processed locally. As shown in Figure 6, when the size
of the data increases by 200%, i.e., the size is tripled, the
analysis time only increases by 100%. This gives us the clue
that the forensic cloud can save more time when dealing with
large amount of data.
In the second set of experiments, we compared the lines
of codes (LoC) that is needed for the conﬁguration with
and without the workﬂow manager. Figure 7 shows how
much effort could be saved in terms of LoC. Workﬂows
with different sequential tasks are built up. Without workﬂow
manager, to conﬁgure one workﬂow task, on average 40 LoCs
212
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-271-4
CLOUD COMPUTING 2013 : The Fourth International Conference on Cloud Computing, GRIDs, and Virtualization

0
2
4
6
8
10
12
14
16
18
20
22
1
2
3
4
5
6
7
8
9
10
Analysing Time (min)
Number of Nodes
Fig. 4.
Analysis Time under Different Number of Nodes in the Cloud
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
2
3
4
5
6
7
8
9
10
Percentage of Data Processed Locally
Number of Nodes
Fig. 5.
Percentage of the MapReduce Tasks Processed Locally under
Different Numbers of Nodes
are needed, but only 4 LoCs are actually required for the
workﬂow XML ﬁle. The LoCs can be reduced by 90% when
using the workﬂow manager to conﬁgure a forensic data
processing task.
We further compared the performance with and without
optimization performed by the workﬂow manager. We have
ten similar tasks, i.e., searching for some keywords, in our
experiments. The workﬂow can intelligently add an extra step
of building indices before running all the ten tasks. As shown
in Figure 8, the analysis time increases linearly with the
number of tasks without the help of workﬂow management.
With the workﬂow management and optimization, the total
time is a little more than the time spent without the workﬂow
management if there is one task executed. However, the total
execution time increases slightly when more similar tasks are
executed. This is because when the indices are built, further
keyword search operations will be accelerated dramatically by
the indices stored in the HBase.
0.0%
20.0%
40.0%
60.0%
80.0%
100.0%
120.0%
0%
25%
50%
75%
100%
125%
150%
175%
200%
Analysing Time  Increase(%)
Disk Image Size Increase (%)
Fig. 6.
Percentage of the Increased Analysis Time under Different Increased
Image Sizes
0
50
100
150
200
250
300
350
1
2
3
4
5
6
7
8
9
10
Line of Codes
Total Number of Tasks
Without Workflow Manager
With Workflow Manager
Fig. 7.
Line of Codes Needed to Conﬁgure Different Tasks in a Workﬂow
w/ and w/o the Dataﬂow Manager
0
50
100
150
200
250
1
2
3
4
5
6
7
Total Time (Min)
Number of Similar Tasks
Without Workflow
With Workflow
Fig. 8.
Total Time Spent w/ and w/o the Workﬂow Manager’s Optimization
V. CONCLUSIONS
We proposed and implemented a domain speciﬁc cloud
environment for digital forensics. We designed a cloud based
framework for supporting automated forensic workﬂow man-
agement and data processing. A schema-based forensic work-
ﬂow framework is proposed. The experimental results show
that using the proposed forensic cloud services can save at
least 71% of the time with only 10 virtual machine nodes.
Meanwhile, the lines of codes for specifying a workﬂow are
also reduced to only 10% when using the proposed workﬂow
management approach. For the investigators, it could be even
easier by using the web-based portal, clicking buttons and
selecting the desired applications from the dropdown lists. The
automated and optimized workﬂow management approach can
save 87% of the analysis time in the tested scenarios. The
proposed framework provides valuable insights on designs of
domain speciﬁc cloud environments using computer forensics
as a target ﬁeld. It demonstrates that, in addition to providing
elastic computing resources, cloud can be used as an envi-
ronment for workﬂow management and coordinated software
development.
VI. ACKNOWLEDGEMENT
We would like to thank the reviewers for their comments
which signiﬁcantly improved the paper. This research is
partially supported by the Department of Homeland Secu-
rity (DHS) under Award Number N66001-13-C-3002, and
the National Science Foundation under Award Number CNS
1205708. The views and conclusions contained in this docu-
ment are those of the authors and should not be interpreted as
representing the opinions or policies of DHS or NSF.
213
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-271-4
CLOUD COMPUTING 2013 : The Fourth International Conference on Cloud Computing, GRIDs, and Virtualization

REFERENCES
[1] A. of Chief Police Ofﬁcers, “Good practice guide for computer based
electronic evidence,” ACPO, Tech. Rep.
[2] K. Kent, S. Chevalier, T. Grance, and H. Dang, “Guide to integrating
forensic techniques into incident response,” National Institute of Stan-
dards and Technology, Tech. Rep.
[3] J. Dykstra and A. T. Sherman, “Acquiring forensic evidence from
infrastructure-as-a-service cloud computing: Exploring and evaluating
tools, trust, and techniques,” Digital Investigation, vol. 9, 2012, pp. S90–
S98.
[4] “Sleuth Hadoop,” http://www.sleuthkit.org/tsk hadoop/, retrieved April
2013.
[5] P. Mell and T. Grance, “The NIST deﬁnition of cloud computing,” http:
//csrc.nist.gov/publications/nistpubs/800-145/SP800-145.pdf.
[6] J. Erickson, M. Rhodes, S. Spence, D. Banks, J. Rutherford, E. Simpson,
G. Belrose, and R. Perry, “Content-centered collaboration spaces in the
cloud,” IEEE Internet Computing, vol. 13, September 2009, pp. 34–42.
[7] D. D. Roure, C. Goble, and R. Stevens, “The design and realisation
of the myexperiment virtual research environment for social sharing of
workﬂows,” Future Generation Computer Systems, vol. 25, no. 5, 2009,
pp. 561 – 567.
[8] I. Foster, “Globus online: Accelerating and democratizing science
through cloud-based services,” Internet Computing, IEEE, vol. 15, no. 3,
May-June 2011, pp. 70 –73.
[9] S. Caton and O. Rana, “Towards autonomic management for cloud ser-
vices based upon volunteered resources,” Concurrency and Computation:
Practice and Experience, 2011.
[10] S. Distefano, V. D. Cunsolo, A. Puliaﬁto, and M. Scarpa, “Cloud@home:
A new enhanced computing paradigm,” in Handbook of Cloud Comput-
ing, B. Furht and A. Escalante, Eds.
Springer US, 2010, pp. 575–594.
[11] A. Chandra and J. Weissman, “Nebulas: using distributed voluntary
resources to build clouds,” in Proceedings of the 2009 conference on
Hot topics in cloud computing.
USENIX Association, 2009.
[12] S. Xu and M. Yung, “Socialclouds: Concept, security architecture and
some mechanisms,” in Trusted Systems, ser. Lecture Notes in Computer
Science, L. Chen and M. Yung, Eds.
Springer Berlin / Heidelberg,
2010, vol. 6163, pp. 104–128.
[13] “Amazon EC2,” http://aws.amazon.com/ec2/, retrieved April 2013.
[14] Y. Song, H. Wang, Y. Li, B. Feng, and Y. Sun, “Multi-tiered on-demand
resource scheduling for vm-based data center,” in Proceedings of the
2009 9th IEEE/ACM International Symposium on Cluster Computing
and the Grid, ser. CCGRID ’09.
Washington, DC, USA: IEEE
Computer Society, 2009, pp. 148–155.
[15] J. Dean and S. Ghemawat, “Mapreduce: simpliﬁed data processing on
large clusters,” Commun. ACM, vol. 51, Jan. 2008, pp. 107–113.
[16] R. Ananthanarayanan, K. Gupta, P. Pandey, H. Pucha, P. Sarkar,
M. Shah, and R. Tewari, “Cloud analytics: do we really need to reinvent
the storage stack?” in Proceedings of the 2009 conference on Hot topics
in cloud computing, ser. HotCloud’09.
Berkeley, CA, USA: USENIX
Association, 2009.
[17] “Apache Pig,” http://pig.apache.org//, retrieved April 2013.
[18] “Bulk
Extractor,”
https://github.com/simsong/bulk extractor/wiki/
Introducing-bulk extractor, retrieved April 2013.
[19] “FTK (Forensics Toolkit),” http://www.accessdata.com/, retrieved April
2013.
[20] “OSForensics,” http://www.osforensics.com/, retrieved April 2013.
[21] “Intella,” http://www.vound-software.com/, retrieved April 2013.
[22] E. Huebner and S. Zanero, Open Source Software for Digital Forensics.
Springer, 2010. [Online]. Available: http://books.google.com/books?id=
2gl7k8PbIFYC
[23] “The Sleuth Kit,” http://www.sleuthkit.org/, retrieved April 2013.
[24] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The hadoop dis-
tributed ﬁle system,” in Proceedings of the 2010 IEEE 26th Symposium
on Mass Storage Systems and Technologies (MSST), ser. MSST ’10.
Washington, DC, USA: IEEE Computer Society, 2010, pp. 1–10.
[25] “Apache Hadoop Wiki-Sequence File,” http://wiki.apache.org/hadoop/
SequenceFile, retrieved April 2013.
[26] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Bur-
rows, T. Chandra, A. Fikes, and R. E. Gruber, “Bigtable: a distributed
storage system for structured data,” in Proceedings of the 7th USENIX
Symposium on Operating Systems Design and Implementation - Volume
7, ser. OSDI ’06.
Berkeley, CA, USA: USENIX Association, 2006,
pp. 15–15.
[27] “National Software Reference Library,” http://www.nsrl.nist.gov/, re-
trieved April 2013.
[28] “Cloudera,” http://www.cloudera/, retrieved April 2013.
214
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-271-4
CLOUD COMPUTING 2013 : The Fourth International Conference on Cloud Computing, GRIDs, and Virtualization

