Automatic Labelling of Seeds Based on Saliency Detection and Edge Detection for 
Image Segmentation 
 
Cheng-Mao Wu 
Department of Computer Science 
National Tsing Hua University  
 Hsinchu, Taiwan 
email:s101062613@m101.nthu.edu.tw 
Long-Wen Chang 
Department of Computer Science 
National Tsing Hua University  
Hsinchu, Taiwan 
email:lchang@cs.nthu.edu.tw
 
 
Abstractâ€” In computer vision, image segmentation transforms 
an input image into a more meaningful form which is easier to 
analyze.   It can be used in the applications such as medical 
imaging, object detection, face recognition, etc. Generally, 
image segmentation can be distinguished as supervised and 
unsupervised categories. The result of supervised image 
segmentation is greatly affected by a user. Therefore, we 
propose an unsupervised method of image segmentation. We use 
saliency detection to label some informative and significant 
parts of the image, and then, we apply edge detection to label 
some details of the image and use the labelled image for image 
segmentation by Kimâ€™s method. In this way, we can 
automatically label the seeds to get the scribble and then 
segment the image into the foreground and the background. The 
simulation results show that our method is feasible for image 
segmentation. 
Keywords- segmentation;supervised segmentation; 
unsupervised segmentation; saliency detecion; edge detection. 
I. 
 INTRODUCTION  
In computer vision, image segmentation is the process of 
partitioning an image into several segments. The goal of 
image segmentation is to transform the input image into a 
more meaningful form which is easier to analyze. How to 
detect the objects which humans can recognize in the image is 
a big issue in image segmentation. Generally, image 
segmentation can be distinguished as supervised and 
unsupervised methods. 
Supervised image segmentation [1]-[3] needs the user to 
label some seeds as scribbles. However, the result is greatly 
affected by the user.  The result of segmentation image is 
shown in Figure 1 while using different scribbles in the Kimâ€™s 
algorithm [3]. Besides, supervised image segmentation is hard 
to evaluate the interactive time. Therefore, we propose an 
unsupervised method of image segmentation, which canâ€™t be 
affected by users.  
Because the supervised image segmentation methods have 
some above-mentioned drawbacks. Thus, our motivation is to 
improve the supervised methodâ€™s disadvantage and make our 
result of the unsupervised method close to that of the 
supervised method. Our goal is to transform the supervised 
method to the unsupervised method. We use saliency 
detection [7] and edge detection [8] to automatically label the 
seeds to get the scribble, and then we apply Kimâ€™s method to 
segment the image into the foreground and the background. 
 Recently, there are a lot of saliency detection method [4]-
[7]. Moreover, we also apply Canny edge detection [8] to 
detect the edge because the method is simple and efficient. By 
saliency detection [7] and edge detection [8], we can 
automatically label seeds for image segmentation. Finally, we 
apply our scribble image with Kimâ€™s method [3] to get the 
final segmentation result. We will compare our method with 
Kimâ€™s method (supervised method) and the method of Donoser 
et. al [10] which is also unsupervised and based on saliency. 
The results show that our approach is good for image 
segmentation. In the section 2, we will briefly discuss some 
related works. In section 3, we will address our proposed 
algorithm. In section 4, we will show some experiment results 
and finally, we give our conclusion. 
 
(a) 
(b) 
(c) 
(d) 
Figure 1. The result of segmentation image while using different scribbles in 
the Kimâ€™s algorithm.  (a) (b) The different scribbles labeled by different users. 
(c) The result of (a). (d) The result of (b) 
 
II. 
RELATED WORKS 
Image segmentation can be categorized into many 
categories: thresholding methods, clustering methods, region-
based methods, edge-based methods, etc. The thresholding 
methods is the simplest method of image segmentation. The 
input image is transformed into a gray-scale image, and then 
segmented by a threshold value to get a binary image [11]. 
The main concept of clustering [12],[13]  is to determine 
which components of a data set naturally â€œbelong togetherâ€. 
The region-based methods are widely used as well [14], [15]. 
Because segmentation consists on partitioning an image into 
8
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-452-7
MMEDIA 2016 : The Eighth International Conferences on Advances in Multimedia

a set of connected regions, we can find homogeneous regions 
according to a specific criterion (intensity value, texture). In 
addition, segmentation can also be done by edge detection 
techniques [8]. Donoser  et al. [10] proposed another method 
which is unsupervised and based on saliency. The method 
automatically find salient regions first, and then focus on one 
different salient part of the image each time; the method 
finally, merge all obtained results into a composite 
segmentation.  
We will compare the result of our method with that of 
Donoser  et al. [10]  because it  is unsupervised and based on 
saliency as well. Furthermore, we will also compare the result 
of our method with the result of Kimâ€™s method (supervised 
method) [3]. According to the results, it shows that our result 
is good and very close to the result of Kimâ€™s method. 
 
III. 
PROPOSED METHOD 
The main idea of our method is labeling the seeds 
automatically and efficiently. To reduce the userâ€™s influence, 
we use our method to get the scribble of the image. The 
scribble provides the seeds to the method of nonparametric 
higher-order learning for segmentation [3].  
First, given an input image, we detect the saliency of the 
image by Yangâ€™s method [7]. After saliency detection, we can 
automatically label the seeds. It contains three steps: 
 
1. The foreground seeds and the background seeds by the 
foreground and the background threshold finding. 
2. Locate the background seeds with four side examination. 
3. Combine the seeds from step 1 and step 2 with the seeds 
generated by edge and saliency detection.  
 
By these three steps, we can automatically and efficiently 
label the seeds to get a scribble image.  
 
In the step of FG/BG threshold finding, we calculate the 
number of each saliency value and get the histogram of pixelâ€™s 
saliency value. We find that the background region is at least 
occupy 1/b area of an image, where b is a parameter (i.e., the 
lowest ğ‘›ğ‘™ saliency pixels of an image). Thus, we can get a 
proper background threshold (ğµğ‘‡) as  
 
âˆ‘
Num(ğ‘ ğ‘£)
ğµğ‘‡
ğ‘ ğ‘£=0
â‰¥ nğ‘™ =
ğ‘
ğ‘,              (1) 
where N is total number of pixels in the image, sv is saliency 
value (sv = 0,1,2, â€¦, 255), Num(sv)ï¼šthe number of pixels 
whose saliency value is sv.  
Similarly, we find that the foreground region is at least 
occupy 1/f area of an image, where f is a parameter (i.e., the 
highest ğ‘›â„ saliency pixels of an image). Thus, we can get a 
proper foreground threshold (ğ¹ğ‘‡) as  
 
âˆ‘
Num(255 âˆ’ ğ‘ ğ‘£)
ğ¹ğ‘‡
ğ‘ ğ‘£=0
â‰¥ nâ„ =
ğ‘
ğ‘“,  (2) 
where N is total number of pixels in the image, sv is 
saliency value (sv = 0,1,2, â€¦, 255), Num(sv)ï¼šthe number of 
pixels whose saliency value is sv.  
Then, we label the FG/BG and remain some undetermined 
parts. For each pixel ğ‘¥i, and its saliency value  S(ğ‘¥i), if  S(ğ‘¥i) 
is larger than ğ¹ğ‘‡ or is equal to ğ¹ğ‘‡, label it as  the foreground 
pixels (red). If S(ğ‘¥i) is smaller than ğµğ‘‡ or is equal to ğµğ‘‡, label 
it as the  background pixels (green). If S(ğ‘¥i ) is larger than 
ğµğ‘‡but smaller than ğ¹ğ‘‡, do nothing (undetermined). Thus, we 
can get 3 parts, the foreground, the background and the 
undetermined.  
In addition to find proper the foreground/background 
thresholds, we also examine four sides of the image. By step 
1, weâ€™ve already known where the foreground is 
approximately located. We want to check the left, right, top, 
bottom regions, shown in Figure 2 and then label some 
background seeds. 
We locate 4 regions ğ‘Ÿğ‘ (ğ‘™),   ğ‘Ÿğ‘ (ğ‘Ÿ), ğ‘Ÿğ‘ (ğ‘¡), ğ‘Ÿğ‘ (ğ‘)  and their 
corresponding borders are ğ‘ (ğ‘™),   ğ‘(ğ‘Ÿ), ğ‘ (ğ‘¡), ğ‘ (ğ‘), respectively. 
We use a size parameter s to define four regions 
ğ‘Ÿğ‘ (ğ‘Ÿ),   ğ‘Ÿğ‘ (ğ‘™), ğ‘Ÿğ‘ (ğ‘¡), ğ‘Ÿğ‘ (ğ‘) as following. 
ğ‘Ÿğ‘ (ğ‘™) = {(ğ‘¥, ğ‘¦)|1 â‰¤ ğ‘¥ â‰¤ ğ‘¤
ğ‘  ,  1 â‰¤ ğ‘¦ â‰¤ â„} 
ğ‘Ÿğ‘ (ğ‘Ÿ) = {(ğ‘¥, ğ‘¦)| ğ‘¤(ğ‘  âˆ’ 1)
ğ‘ 
â‰¤ ğ‘¥ â‰¤ ğ‘¤,  1 â‰¤ ğ‘¦ â‰¤ â„} 
ğ‘Ÿğ‘ (ğ‘¡) = {(ğ‘¥, ğ‘¦)|1 â‰¤ ğ‘¥ â‰¤ ğ‘¤,  1 â‰¤ ğ‘¦ â‰¤ â„
ğ‘ } 
ğ‘Ÿğ‘ (ğ‘) = {(ğ‘¥, ğ‘¦)|1 â‰¤ ğ‘¥ â‰¤ ğ‘¤, â„(ğ‘  âˆ’ 1)
ğ‘ 
â‰¤ ğ‘¦ â‰¤ â„} 
We define their corresponding borders b (l),   b(r), b (t), b (b) as 
following: 
ğ‘(ğ‘™) = {(ğ‘¥, ğ‘¦)|1 = ğ‘¥,  1 â‰¤ ğ‘¦ â‰¤ â„} ,           (3)  
ğ‘(ğ‘Ÿ) = {(ğ‘¥, ğ‘¦)|ğ‘¥ = ğ‘¤,  1 â‰¤ ğ‘¦ â‰¤ â„},          (4)  
ğ‘(ğ‘¡) = {(ğ‘¥, ğ‘¦)|1 â‰¤ ğ‘¥ â‰¤ ğ‘¤,  1 = ğ‘¦},           (5)  
ğ‘(ğ‘) = {(ğ‘¥, ğ‘¦)|1 â‰¤ ğ‘¥ â‰¤ ğ‘¤, ğ‘¦ = â„}.           (6)  
 
The definition of 4 regions and their corresponding borders 
is shown in Figure 2. The left region and left border are 
shown and other regions can be shown similarly. Then, we 
examine four sides, respectively. We take left 1/s region 
ğ‘Ÿğ‘ (ğ‘™) as an example. If there are foreground seeds in ğ‘Ÿğ‘ (ğ‘™), 
retain  the original edge. If there are no foreground seeds in 
ğ‘Ÿğ‘ (ğ‘™), label  the background seeds to the left boundary ğ‘(ğ‘™), 
which is that region corresponding edge. Similarly, we 
examine the other three regions ğ‘Ÿğ‘ (ğ‘Ÿ), ğ‘Ÿğ‘ (ğ‘¡), ğ‘Ÿğ‘ (ğ‘) .  
 
9
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-452-7
MMEDIA 2016 : The Eighth International Conferences on Advances in Multimedia

 
Figure 2.The definition of 4 regions and their corresponding borders. 
 
After four sides examination, the extracted foreground still 
has some details which need to be improved. Thus, we apply 
Canny edge detection [8] to improve the segmentation. We 
find that the pixel in the edge image always belongs to the 
background in the saliency image. The green area is the 
background and the red area is the foreground. We use 
(ğ¹ğ‘‡âˆ’ğµğ‘‡)/t to determine whether label the edge or not, where 
t is a parameter. The decision rule is as followingï¼š 
 
For each pixel ğ‘¥ğ‘–  in the edge image by Canny edge 
detection and its neighboring pixelğ‘¥ğ‘–â€². S(ğ‘¥ğ‘– ) is the saliency 
value of pixel ğ‘¥ğ‘–, and S(ğ‘¥ğ‘–â€²) is the saliency value of pixel ğ‘¥ğ‘–â€².  
if S(ğ‘¥ğ‘–â€²)âˆ’S(ğ‘¥ğ‘–) is larger than (ğ¹ğ‘‡âˆ’ğµğ‘‡)/t or S(ğ‘¥ğ‘–â€²)âˆ’S(ğ‘¥ğ‘–) 
is equal to (ğ¹ğ‘‡âˆ’ğµğ‘‡)/t, ğ‘¥ğ‘–â€² is labelled ğ‘ğ‘   the foreground seed  
and xi is lağ‘ğ‘’ğ‘™led  ğ‘ğ‘  the background seed; otherwise, do 
nothing. 
  
After automatically seeds labelling, we carry out the 
method 
of 
nonparametric 
higher-order 
learning 
for 
segmentation [3].  
 
IV. 
EXPERIMENT RESULTS AND ANALYSIS 
 
We use our method to label the seeds efficiently and 
automatically. We compare with the method of nonparametric 
higher-order learning for interactive segmentation [3], which 
is greatly affected by users. The results of our method are very 
close to the results of nonparametric higher-order learning for 
interactive segmentation [3]. In addition, we also compare our 
method with the method [10], which also segment the image 
based on saliency. The parameters in our proposed method are 
presented in Table 1. Figure 3 and Figure 4 show the 
segmentation results of the image. We compare the result of 
our method with the result of Kimâ€™s method (supervised 
method) [3] first. According to Figure 3, it shows that our 
result is good and very close to the result of Kimâ€™s method 
(supervised method), which is a greatly affected by the users, 
while our method is unaffected by the users. Besides, we also 
compare the result of our method with the result of the method 
of Donoser et. al. [10]. According to Figure 4,  the result of our 
method is better than  that of the method [10], which is also 
an unsupervised segmentation method based on saliency 
because our method can label 2 bulls (foreground) only, but 
without  the line between the grass and the lake.  
V. 
CONCLUSION  
 
In this paper, we proposed an unsupervised method based 
on saliency detection and edge detection to automatically and 
efficiently label the seeds. It is convenient because it can 
segment the foreground and background automatically and it 
doesnâ€™t need user interaction, which is quite important to 
segment a large data base of images. 
 
References 
 
[1] L. Grady, â€œRandom walks for image segmentation,â€ IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 
28(11):1768â€“1783, 2006. 
[2] P. Kohli, L. Ladicky, and P. H. S. Torr,â€ Robust higher order potentials 
for enforcing label consistency,â€ Int J Computer Vision , 82:302â€“
324, 2009. 
[3] T.H. Kim, K.M. Lee, S.U. Lee, â€œNonparametric higher-order 
learning for interactive segmentation,â€ in: Computer Vision 
and Pattern Recognition   (CVPR), 2010. 
[4] H. Jiang, J. Wang, Z. Yuan, T. Liu, and N. Zheng,â€œ Automatic salient 
object segmentation based on context and shape prior,â€ In British 
Machine Vision Conference, , 2011, pages 110.1â€“110.12 . 
[5] Stas Goferman, Lihi Zelnik-Manor, and Ayellet Tal,â€ Context-
aware saliency detection,â€  In  CVPR, 2010,  pages 2376â€“2383.  
[6] K. Y. Chang, T. L. Liu, H. T. Chen, and S. H. Lai, "Fusing 
Generic Objectness and Visual Saliency for Salient Object 
Detection," in IEEE International Conference on Computer 
Vision, 2011, pp. 914-921. 
[7] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang, â€œSaliency 
detection via graph-based manifold ranking,â€ In CVPR, 2013. 
[8] Canny, J., A. "A Computational Approach To Edge Detection" 
IEEE Transactions on pattern analysis and machine 
intelligence, 1986, VOL. PAMI-8, NO. 6, Page 679 â€“ 698. 
[9] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. 
SÃ¼sstrunk, â€œSlic superpixels,â€œ 2010,  EPFL Technical Report 
no. 149300. 
[10] M. Donoser, M. Urschler, M. Hirzer, and H. Bishof, â€œSaliency 
driven total variational segmentation,â€ in Proc. of the IEEE 
Intâ€™l Conf. Computer Vision (ICCVâ€™09), 2009. 
[11] Baradez, M.O., McGuckin, C.P., Forraz, N., Pettengell, R., 
Hoppe, A.: â€˜Robust and automated unimodal histogram 
thresholding and potential applications,â€ Pattern Recognit., 
2004, 37, (6), pp. 1131â€“1148 
[12] A. K. Jain and R. C. Dubes,  Algorithms for Clustering Data. 
Prentice Hall, 1988.. 
[13] F. Kurugollu, B. Sankur, and A. Harmanci, â€œColor image 
segmentation using histogram multithresholding and fusion,â€ 
Image and Vison Computing,â€ 2001, 19(13):915â€“928. 
[14] Y. Deng, B. S. Manjunath,  â€œUnsupervised segmentation of 
color-texture regions,â€   IEEE Trans. on Pattern Analysis and 
Machine Intelligence, 2001, 23(8):800~810. 
[15] Y. Deng, C. Kenney, M. S. Moore , B. S. Manjunath, â€œPeer 
group filtering and perceptual color image quantization,â€ In:   
Proc. of the IEEE Intâ€™l Symp. on Circuits and Systems,  1999. 
21~24. 
 
 
 
  
 
 
10
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-452-7
MMEDIA 2016 : The Eighth International Conferences on Advances in Multimedia

 
FG/BG threshold finding  
b=3 
f=100 
Four sides examination 
s=8 
Combine edge/saliency detection 
 
Ïƒ (Standard Deviation) ï¼š3 
double thresholdï¼š0.1 and 0.2 
t=3 
 
Table 1. The parameters in our proposed method. 
 
 
 
 
 
(a) 
(b) 
(c) 
 
(d) 
 
(f) 
 
(e) 
 
(g) 
Figure 3. Segmentation result of the image. (a) Input image. (b) Result of 
saliency detection. (c) Result of edge detection. (d) Our scribble. (e) Our 
result. (f) Kimâ€™s scribble. (g) Kimâ€™s result.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(a) 
(b) 
 
(c) 
 
(d) 
 
 
(e) 
 
(f) 
Figure 4. Segmentation result of the image. (a) Input image. (b) Result of 
saliency detection. (c) Result of edge detection. (d) Our scribble. (e) the result 
of   Donoser et al. [10] (f) Our result.  
11
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-452-7
MMEDIA 2016 : The Eighth International Conferences on Advances in Multimedia

