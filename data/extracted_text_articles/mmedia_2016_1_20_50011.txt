Automatic Labelling of Seeds Based on Saliency Detection and Edge Detection for 
Image Segmentation 
 
Cheng-Mao Wu 
Department of Computer Science 
National Tsing Hua University  
 Hsinchu, Taiwan 
email:s101062613@m101.nthu.edu.tw 
Long-Wen Chang 
Department of Computer Science 
National Tsing Hua University  
Hsinchu, Taiwan 
email:lchang@cs.nthu.edu.tw
 
 
Abstract— In computer vision, image segmentation transforms 
an input image into a more meaningful form which is easier to 
analyze.   It can be used in the applications such as medical 
imaging, object detection, face recognition, etc. Generally, 
image segmentation can be distinguished as supervised and 
unsupervised categories. The result of supervised image 
segmentation is greatly affected by a user. Therefore, we 
propose an unsupervised method of image segmentation. We use 
saliency detection to label some informative and significant 
parts of the image, and then, we apply edge detection to label 
some details of the image and use the labelled image for image 
segmentation by Kim’s method. In this way, we can 
automatically label the seeds to get the scribble and then 
segment the image into the foreground and the background. The 
simulation results show that our method is feasible for image 
segmentation. 
Keywords- segmentation;supervised segmentation; 
unsupervised segmentation; saliency detecion; edge detection. 
I. 
 INTRODUCTION  
In computer vision, image segmentation is the process of 
partitioning an image into several segments. The goal of 
image segmentation is to transform the input image into a 
more meaningful form which is easier to analyze. How to 
detect the objects which humans can recognize in the image is 
a big issue in image segmentation. Generally, image 
segmentation can be distinguished as supervised and 
unsupervised methods. 
Supervised image segmentation [1]-[3] needs the user to 
label some seeds as scribbles. However, the result is greatly 
affected by the user.  The result of segmentation image is 
shown in Figure 1 while using different scribbles in the Kim’s 
algorithm [3]. Besides, supervised image segmentation is hard 
to evaluate the interactive time. Therefore, we propose an 
unsupervised method of image segmentation, which can’t be 
affected by users.  
Because the supervised image segmentation methods have 
some above-mentioned drawbacks. Thus, our motivation is to 
improve the supervised method’s disadvantage and make our 
result of the unsupervised method close to that of the 
supervised method. Our goal is to transform the supervised 
method to the unsupervised method. We use saliency 
detection [7] and edge detection [8] to automatically label the 
seeds to get the scribble, and then we apply Kim’s method to 
segment the image into the foreground and the background. 
 Recently, there are a lot of saliency detection method [4]-
[7]. Moreover, we also apply Canny edge detection [8] to 
detect the edge because the method is simple and efficient. By 
saliency detection [7] and edge detection [8], we can 
automatically label seeds for image segmentation. Finally, we 
apply our scribble image with Kim’s method [3] to get the 
final segmentation result. We will compare our method with 
Kim’s method (supervised method) and the method of Donoser 
et. al [10] which is also unsupervised and based on saliency. 
The results show that our approach is good for image 
segmentation. In the section 2, we will briefly discuss some 
related works. In section 3, we will address our proposed 
algorithm. In section 4, we will show some experiment results 
and finally, we give our conclusion. 
 
(a) 
(b) 
(c) 
(d) 
Figure 1. The result of segmentation image while using different scribbles in 
the Kim’s algorithm.  (a) (b) The different scribbles labeled by different users. 
(c) The result of (a). (d) The result of (b) 
 
II. 
RELATED WORKS 
Image segmentation can be categorized into many 
categories: thresholding methods, clustering methods, region-
based methods, edge-based methods, etc. The thresholding 
methods is the simplest method of image segmentation. The 
input image is transformed into a gray-scale image, and then 
segmented by a threshold value to get a binary image [11]. 
The main concept of clustering [12],[13]  is to determine 
which components of a data set naturally “belong together”. 
The region-based methods are widely used as well [14], [15]. 
Because segmentation consists on partitioning an image into 
8
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-452-7
MMEDIA 2016 : The Eighth International Conferences on Advances in Multimedia

a set of connected regions, we can find homogeneous regions 
according to a specific criterion (intensity value, texture). In 
addition, segmentation can also be done by edge detection 
techniques [8]. Donoser  et al. [10] proposed another method 
which is unsupervised and based on saliency. The method 
automatically find salient regions first, and then focus on one 
different salient part of the image each time; the method 
finally, merge all obtained results into a composite 
segmentation.  
We will compare the result of our method with that of 
Donoser  et al. [10]  because it  is unsupervised and based on 
saliency as well. Furthermore, we will also compare the result 
of our method with the result of Kim’s method (supervised 
method) [3]. According to the results, it shows that our result 
is good and very close to the result of Kim’s method. 
 
III. 
PROPOSED METHOD 
The main idea of our method is labeling the seeds 
automatically and efficiently. To reduce the user’s influence, 
we use our method to get the scribble of the image. The 
scribble provides the seeds to the method of nonparametric 
higher-order learning for segmentation [3].  
First, given an input image, we detect the saliency of the 
image by Yang’s method [7]. After saliency detection, we can 
automatically label the seeds. It contains three steps: 
 
1. The foreground seeds and the background seeds by the 
foreground and the background threshold finding. 
2. Locate the background seeds with four side examination. 
3. Combine the seeds from step 1 and step 2 with the seeds 
generated by edge and saliency detection.  
 
By these three steps, we can automatically and efficiently 
label the seeds to get a scribble image.  
 
In the step of FG/BG threshold finding, we calculate the 
number of each saliency value and get the histogram of pixel’s 
saliency value. We find that the background region is at least 
occupy 1/b area of an image, where b is a parameter (i.e., the 
lowest 𝑛𝑙 saliency pixels of an image). Thus, we can get a 
proper background threshold (𝐵𝑇) as  
 
∑
Num(𝑠𝑣)
𝐵𝑇
𝑠𝑣=0
≥ n𝑙 =
𝑁
𝑏,              (1) 
where N is total number of pixels in the image, sv is saliency 
value (sv = 0,1,2, …, 255), Num(sv)：the number of pixels 
whose saliency value is sv.  
Similarly, we find that the foreground region is at least 
occupy 1/f area of an image, where f is a parameter (i.e., the 
highest 𝑛ℎ saliency pixels of an image). Thus, we can get a 
proper foreground threshold (𝐹𝑇) as  
 
∑
Num(255 − 𝑠𝑣)
𝐹𝑇
𝑠𝑣=0
≥ nℎ =
𝑁
𝑓,  (2) 
where N is total number of pixels in the image, sv is 
saliency value (sv = 0,1,2, …, 255), Num(sv)：the number of 
pixels whose saliency value is sv.  
Then, we label the FG/BG and remain some undetermined 
parts. For each pixel 𝑥i, and its saliency value  S(𝑥i), if  S(𝑥i) 
is larger than 𝐹𝑇 or is equal to 𝐹𝑇, label it as  the foreground 
pixels (red). If S(𝑥i) is smaller than 𝐵𝑇 or is equal to 𝐵𝑇, label 
it as the  background pixels (green). If S(𝑥i ) is larger than 
𝐵𝑇but smaller than 𝐹𝑇, do nothing (undetermined). Thus, we 
can get 3 parts, the foreground, the background and the 
undetermined.  
In addition to find proper the foreground/background 
thresholds, we also examine four sides of the image. By step 
1, we’ve already known where the foreground is 
approximately located. We want to check the left, right, top, 
bottom regions, shown in Figure 2 and then label some 
background seeds. 
We locate 4 regions 𝑟𝑠(𝑙),   𝑟𝑠(𝑟), 𝑟𝑠(𝑡), 𝑟𝑠(𝑏)  and their 
corresponding borders are 𝑏 (𝑙),   𝑏(𝑟), 𝑏 (𝑡), 𝑏 (𝑏), respectively. 
We use a size parameter s to define four regions 
𝑟𝑠(𝑟),   𝑟𝑠(𝑙), 𝑟𝑠(𝑡), 𝑟𝑠(𝑏) as following. 
𝑟𝑠(𝑙) = {(𝑥, 𝑦)|1 ≤ 𝑥 ≤ 𝑤
𝑠 ,  1 ≤ 𝑦 ≤ ℎ} 
𝑟𝑠(𝑟) = {(𝑥, 𝑦)| 𝑤(𝑠 − 1)
𝑠
≤ 𝑥 ≤ 𝑤,  1 ≤ 𝑦 ≤ ℎ} 
𝑟𝑠(𝑡) = {(𝑥, 𝑦)|1 ≤ 𝑥 ≤ 𝑤,  1 ≤ 𝑦 ≤ ℎ
𝑠} 
𝑟𝑠(𝑏) = {(𝑥, 𝑦)|1 ≤ 𝑥 ≤ 𝑤, ℎ(𝑠 − 1)
𝑠
≤ 𝑦 ≤ ℎ} 
We define their corresponding borders b (l),   b(r), b (t), b (b) as 
following: 
𝑏(𝑙) = {(𝑥, 𝑦)|1 = 𝑥,  1 ≤ 𝑦 ≤ ℎ} ,           (3)  
𝑏(𝑟) = {(𝑥, 𝑦)|𝑥 = 𝑤,  1 ≤ 𝑦 ≤ ℎ},          (4)  
𝑏(𝑡) = {(𝑥, 𝑦)|1 ≤ 𝑥 ≤ 𝑤,  1 = 𝑦},           (5)  
𝑏(𝑏) = {(𝑥, 𝑦)|1 ≤ 𝑥 ≤ 𝑤, 𝑦 = ℎ}.           (6)  
 
The definition of 4 regions and their corresponding borders 
is shown in Figure 2. The left region and left border are 
shown and other regions can be shown similarly. Then, we 
examine four sides, respectively. We take left 1/s region 
𝑟𝑠(𝑙) as an example. If there are foreground seeds in 𝑟𝑠(𝑙), 
retain  the original edge. If there are no foreground seeds in 
𝑟𝑠(𝑙), label  the background seeds to the left boundary 𝑏(𝑙), 
which is that region corresponding edge. Similarly, we 
examine the other three regions 𝑟𝑠(𝑟), 𝑟𝑠(𝑡), 𝑟𝑠(𝑏) .  
 
9
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-452-7
MMEDIA 2016 : The Eighth International Conferences on Advances in Multimedia

 
Figure 2.The definition of 4 regions and their corresponding borders. 
 
After four sides examination, the extracted foreground still 
has some details which need to be improved. Thus, we apply 
Canny edge detection [8] to improve the segmentation. We 
find that the pixel in the edge image always belongs to the 
background in the saliency image. The green area is the 
background and the red area is the foreground. We use 
(𝐹𝑇−𝐵𝑇)/t to determine whether label the edge or not, where 
t is a parameter. The decision rule is as following： 
 
For each pixel 𝑥𝑖  in the edge image by Canny edge 
detection and its neighboring pixel𝑥𝑖′. S(𝑥𝑖 ) is the saliency 
value of pixel 𝑥𝑖, and S(𝑥𝑖′) is the saliency value of pixel 𝑥𝑖′.  
if S(𝑥𝑖′)−S(𝑥𝑖) is larger than (𝐹𝑇−𝐵𝑇)/t or S(𝑥𝑖′)−S(𝑥𝑖) 
is equal to (𝐹𝑇−𝐵𝑇)/t, 𝑥𝑖′ is labelled 𝑎𝑠  the foreground seed  
and xi is la𝑏𝑒𝑙led  𝑎𝑠 the background seed; otherwise, do 
nothing. 
  
After automatically seeds labelling, we carry out the 
method 
of 
nonparametric 
higher-order 
learning 
for 
segmentation [3].  
 
IV. 
EXPERIMENT RESULTS AND ANALYSIS 
 
We use our method to label the seeds efficiently and 
automatically. We compare with the method of nonparametric 
higher-order learning for interactive segmentation [3], which 
is greatly affected by users. The results of our method are very 
close to the results of nonparametric higher-order learning for 
interactive segmentation [3]. In addition, we also compare our 
method with the method [10], which also segment the image 
based on saliency. The parameters in our proposed method are 
presented in Table 1. Figure 3 and Figure 4 show the 
segmentation results of the image. We compare the result of 
our method with the result of Kim’s method (supervised 
method) [3] first. According to Figure 3, it shows that our 
result is good and very close to the result of Kim’s method 
(supervised method), which is a greatly affected by the users, 
while our method is unaffected by the users. Besides, we also 
compare the result of our method with the result of the method 
of Donoser et. al. [10]. According to Figure 4,  the result of our 
method is better than  that of the method [10], which is also 
an unsupervised segmentation method based on saliency 
because our method can label 2 bulls (foreground) only, but 
without  the line between the grass and the lake.  
V. 
CONCLUSION  
 
In this paper, we proposed an unsupervised method based 
on saliency detection and edge detection to automatically and 
efficiently label the seeds. It is convenient because it can 
segment the foreground and background automatically and it 
doesn’t need user interaction, which is quite important to 
segment a large data base of images. 
 
References 
 
[1] L. Grady, “Random walks for image segmentation,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 
28(11):1768–1783, 2006. 
[2] P. Kohli, L. Ladicky, and P. H. S. Torr,” Robust higher order potentials 
for enforcing label consistency,” Int J Computer Vision , 82:302–
324, 2009. 
[3] T.H. Kim, K.M. Lee, S.U. Lee, “Nonparametric higher-order 
learning for interactive segmentation,” in: Computer Vision 
and Pattern Recognition   (CVPR), 2010. 
[4] H. Jiang, J. Wang, Z. Yuan, T. Liu, and N. Zheng,“ Automatic salient 
object segmentation based on context and shape prior,” In British 
Machine Vision Conference, , 2011, pages 110.1–110.12 . 
[5] Stas Goferman, Lihi Zelnik-Manor, and Ayellet Tal,” Context-
aware saliency detection,”  In  CVPR, 2010,  pages 2376–2383.  
[6] K. Y. Chang, T. L. Liu, H. T. Chen, and S. H. Lai, "Fusing 
Generic Objectness and Visual Saliency for Salient Object 
Detection," in IEEE International Conference on Computer 
Vision, 2011, pp. 914-921. 
[7] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang, “Saliency 
detection via graph-based manifold ranking,” In CVPR, 2013. 
[8] Canny, J., A. "A Computational Approach To Edge Detection" 
IEEE Transactions on pattern analysis and machine 
intelligence, 1986, VOL. PAMI-8, NO. 6, Page 679 – 698. 
[9] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. 
Süsstrunk, “Slic superpixels,“ 2010,  EPFL Technical Report 
no. 149300. 
[10] M. Donoser, M. Urschler, M. Hirzer, and H. Bishof, “Saliency 
driven total variational segmentation,” in Proc. of the IEEE 
Int’l Conf. Computer Vision (ICCV’09), 2009. 
[11] Baradez, M.O., McGuckin, C.P., Forraz, N., Pettengell, R., 
Hoppe, A.: ‘Robust and automated unimodal histogram 
thresholding and potential applications,” Pattern Recognit., 
2004, 37, (6), pp. 1131–1148 
[12] A. K. Jain and R. C. Dubes,  Algorithms for Clustering Data. 
Prentice Hall, 1988.. 
[13] F. Kurugollu, B. Sankur, and A. Harmanci, “Color image 
segmentation using histogram multithresholding and fusion,” 
Image and Vison Computing,” 2001, 19(13):915–928. 
[14] Y. Deng, B. S. Manjunath,  “Unsupervised segmentation of 
color-texture regions,”   IEEE Trans. on Pattern Analysis and 
Machine Intelligence, 2001, 23(8):800~810. 
[15] Y. Deng, C. Kenney, M. S. Moore , B. S. Manjunath, “Peer 
group filtering and perceptual color image quantization,” In:   
Proc. of the IEEE Int’l Symp. on Circuits and Systems,  1999. 
21~24. 
 
 
 
  
 
 
10
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-452-7
MMEDIA 2016 : The Eighth International Conferences on Advances in Multimedia

 
FG/BG threshold finding  
b=3 
f=100 
Four sides examination 
s=8 
Combine edge/saliency detection 
 
σ (Standard Deviation) ：3 
double threshold：0.1 and 0.2 
t=3 
 
Table 1. The parameters in our proposed method. 
 
 
 
 
 
(a) 
(b) 
(c) 
 
(d) 
 
(f) 
 
(e) 
 
(g) 
Figure 3. Segmentation result of the image. (a) Input image. (b) Result of 
saliency detection. (c) Result of edge detection. (d) Our scribble. (e) Our 
result. (f) Kim’s scribble. (g) Kim’s result.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(a) 
(b) 
 
(c) 
 
(d) 
 
 
(e) 
 
(f) 
Figure 4. Segmentation result of the image. (a) Input image. (b) Result of 
saliency detection. (c) Result of edge detection. (d) Our scribble. (e) the result 
of   Donoser et al. [10] (f) Our result.  
11
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-452-7
MMEDIA 2016 : The Eighth International Conferences on Advances in Multimedia

