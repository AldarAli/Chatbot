The DCP Bay: Toward an Art-House Content
Delivery Network for Digital Cinema
Nicolas Bertrand, Jean Denis Durou and Vincent Charvillat
IRIT, UMR CNRS 5505
Universit´e de Toulouse, France
email: nicolas.bertrand@isf.cc, durou@irit.fr, vincent.charvillat@enseeiht.fr
Abstract—Cinema theaters have arrived in the digital era. The
Digital Cinema Initiatives has chosen Digital Cinema Package
(DCP) as format for the distribution of feature ﬁlms. No suitable
economical nor technological model is proposed for DCP content
delivery to art-house theaters. The existing solutions are too
expensive or not adapted. Therefore, we conduct this research
activity in cooperation with Utopia cinemas, a group of art-house
French cinemas. Utopia’s main requirement (besides functional
ones) is to provide free and open source software for DCP
distribution. In this paper, we present a Content Delivery Network
for DCP adapted to art-house. This network is operative since
mid 2014 and based on torrent peer-to-peer technology inside a
multi-point VPN.
Keywords–Digital cinema; Content Delivery Network; Peer-to-
peer; VPN.
I.
INTRODUCTION
Cinema theaters switched from 35 mm prints to digital
era. The Digital Cinema System Speciﬁcation (DCSS) [1],
provided by Digital Cinema Initiatives (DCI), is now a world-
wide standard. The speciﬁcation describes how to create,
distribute and project a Digital Cinema Package (DCP).
Our research is conducted in collaboration with Utopia
cinemas, a network of ﬁve independent theaters in France.
More independent theaters support this activity through the
Ind´ependants, Solidaires et F´ed´er´es (ISF) association. Those
theaters initiated this research because they need to understand
the implications of changing to digital. Primarily, they are
concerned about becoming dependent on a single company’s
technology for presentation, and want us to provide Free and
Open Source Software (FOSS) for Digital Cinema (DC). As
exhibitors, they are mainly concerned in two parts: a projection
system and a system for DCP reception in theaters in a
dematerialized way.
The ﬁrst topic is addressed in [2]. It is a full software pro-
jection system running under VLC, with JPEG2000 decoding
optimization for DC (the VLC-DCP part in Figure 1). This
paper is on the second part: the DCP transmission system.
We designed a Content Delivery Network (CDN), peer-to-
peer based, for DCP delivery to exhibitors. Our goal is to
deliver on time a DCP to be used in the projection system
(not to stream the content). A major requirement is also to
design a CDN which ﬁts independent distributors and theaters
ecosystem: deliver many contents for many theaters in a cost
effective way.
In Section II, we present the environmental reasons to
propose our CDN. In Section III the CDN architecture is
presented. In Section IV the CDN is evaluated, and in Section
V we analyse the evaluation results.
II.
WHY AN ALTERNATIVE CDN FOR ART-HOUSE
THEATERS?
Once a movie is realized by producers, the movie shall
be distributed. We study the case of distribution to theaters
(not the video distribution as DVD or VOD). The distributors
acquire the ﬁlm rights per country. They negotiate per theater
when and how long the movie will be screened (cf. Figure 1,
blue arrows for right owners negotiation, red for DCP data
ﬂow). Our work is based on ﬁlm distribution in France. Film
industry in France is ruled by the Centre National du Cin´ema
et de l’image anim´ee (CNC) who delivers permissions to dis-
tribute a ﬁlm. CNC distinguishes three classes of theaters (big,
medium and small) and sets labels to ﬁlms (art-house, research,
etc.). We focus on distribution to medium and small theaters,
with CNC art-house labels. Big (multiplex) theaters already
have distribution solutions (mainly via satellites) adapted to
their business (for instance distribution of more than 900 copies
to exhibitors). At the opposite 60 % of the movies in 2013 have
been screened in less than 100 theaters (CNC source) and the
great majority of these ﬁlms were art-house ones.
Technically, distributors don’t create (except for the very
small ones) DCPs themselves. DCP mastering is done by
cinema industry laboratories. Basically DCP mastering consists
in creating a DCP according to DCSS standard (3 big steps:
compress video, create container and encrypt DCPs). The DCP
can then be delivered to exhibitors. In France, 2012 is the
year which represents the switch from 35 mm prints to digital
copies. At this time DCPs were transported like 35 mm prints:
via mail transporters. The switch to dematerialized transfer is
currently in progress. The dematerialized transfer was initiated
by big Network Service Providers (NSP): in France, Orange
via Globecast and TDF via Smartjog.
Globecast, for some theaters (big or representative ones),
proposes free equipment (rent of 1, 2 or 4 ADSL network
links and reception box), the DCP transfers are cost free also.
The distributor is in charge to pay the transfers to theaters. The
transfer cost is expensive for medium and small distributors, so
a few switched to dematerialized. Taking en example, Utopia
Bordeaux theater is on the top 5 of French cinemas playing art-
house movies. The cinema was free of charge equipped with
Globecast system: after 3 years of usage, they can receive less
than 10 % of the screened ﬁlm via this system. The remaining
90 % are received by hard disk via traditional mail delivery.
23
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

Figure 1. Overview of DC system. The ﬁgure represents the DCP distribution from distributors to exhibitors.
Furthermore, depending on the theater, Globecast offer is free
or not (installation cost and DCP transfer cost).
Other issue: transporters try to deal with distributors for
exclusivity on delivery. A theater shall then be equipped with
several reception mechanisms to receive all DCPs. So, we
have here an inter-operability problem. Inter-operability is
one of the keywords of DCSS: creation of DCPs, projection
system, security are largely described in the document. The
transportation chapter is only one page long in a 160 pages
document.
Affordable costs for art-house cinema and inter-operability
are two main reasons to starting this project. In next section
we will present our CDN: The DCP Bay.
III.
THE DCP BAY
The fundamentals are to propose an open network design
for DCP delivery to solve inter-operability issues and build
it in a cost effective way for distributors and exhibitors. By
inter-operability, we mean a standard public protocol for DCP
transfer based on FOSS and independent (from NSP, from
distributors and from theaters projection system).
A DCP can grow up to 400 GB (3 hours of a movie with
a compression ratio of 250 Mb/s). The observed DCP mean
size is around 200 GB. So, DCP distribution is challenging
due to the large size of the content and on time delivery for
ﬁrst screening. The distributors can make available the DCPs
to distribute from one month to 24 hours before the projection.
We excluded solutions based on JPEG2000 scalability [3]
and multiple-description-based distributed system [4] because
the majority of DCPs to distribute are encrypted: it is not
possible to access JPEG2000 data.
Our transfer system is peer-to-peer (torrent) based. Usage
of torrent ﬁles is common and implementation is open. Torrent
allows transfer of large ﬁles and ensures also the integrity of
the system. For security and maintenance simpliﬁcation all
the peers are in a multipoint VPN called tinc. Tinc is a well
known open source VPN simple to set up and with numerous
functionalities [5].
A distributor delivers the DCPs to our platform (by secure
FTP or by torrent or by traditional mail delivery). When a DCP
is received in one of our servers, the integrity is veriﬁed (the
information for hash veriﬁcation is present in DCP meta-data)
and the torrent ﬁle is created. The DCP is replicated in other
servers to create seeders. Then, the DCP can be delivered to
theaters, the exhibitor selects the torrent to download in our
torrent tracker (also inside the VPN), and starts the transfer
via his torrent client interface. Once the DCP received, the
exhibitor can transfer it to the DC library or DC projector
server (usually via FTP). For running The DCP Bay (TDB)
in a cinema, the exhibitor needs a dedicated machine. We
do not impose to the theater a speciﬁc hardware, neither a
speciﬁc NSP, but we ask to use a dedicated machine to not be
dependent on any DC equipment provider.
Thanks to our VPN and peer-to-peer architecture, we
can propose aggregation of network links to increase theater
bandwidth. The aggregation solution is presented in Figure 2.
For each network link, we have a modem. In the TDB theater
Figure 2. TDB network link aggregation. The TDB machine can receive
torrent data from the 2 network links by specifying routes at VPN level. For
the torrent client all the machines are in the same VPN network
(10.10.10.XXX).
machine at VPN level, two connections are made to two TDB
servers, and we create separate routes via each modem. So,
we receive data from seeder 1 via modem A, and data from
seeder 2 via modem B. At torrent client level both seeders
are viewed, via the VPN tunnel, and download rate for each
seeder is limited by each network link. This mechanism can
be extended to multiple seeders by balancing the connections
between modems. This aggregation solution is easy to set up
in the theater and does not need any speciﬁc connection from
an NSP. We can even add network robustness by selecting
separates NSP for each network link.
TDB have server machines in data-centers. We work with
non proﬁt NSPs (tetaneutral, aquilenet, LDN). In tetaneutral
one machine, with 9 To of disk storage, is used for DCP re-
24
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

TABLE I. PROGRESSION OF TRANSFERRED DCPs
Month
Number of transfers
Transferred data (in GB)
Jun 2014
1
153.04
Jul 2014
24
3450.19
Aug 2014
12
1656.76
Sep 2014
33
4000.11
Oct 2014
76
10290.37
Nov 2014
91
10355.58
Dec 2014
66
9046.71
ception via FTP, integrity veriﬁcation, torrent creation, torrent
tracker hosting and seeding. The second machine replicates the
DCP of the ﬁrst one and acts as a seeder also. The third one
is a virtual machine for archiving ’old’ DCPs, and seeds only
the archived DCP (16 To of disk storage). The machines in
aquilenet and LDN act as cache machines. We send a DCP to
these machines when we know that it will be transferred to
many theaters at the same time.
IV.
THE DCP BAY EVALUATION
TDB is operative since June 2014. Table I presents the
number of DCPs transferred by month to the theaters. All the
connected theaters (from one to ﬁve screens) are art-house
ones but some of them have also mainstream movies. The
theaters are connected with different connection types (optic
ﬁber, VDSL, ADSL) or usage (shared, dedicated or aggregated
lines). They can be equipped only by TDB, but also by
others DCP transporters (Globecast, Smartjog, Cinego). This
represents heterogeneous bandwidth and according to theater
screening it implies several distribution scenarios.
A.
Tinc VPN evaluation
All the DCPs transit (except the FTP transfers) via VPN
and tunnels. All the trafﬁc is made on the same tunnel. Data
transit inside tunnels does not have the same performance as
direct network transfer. We connected two machines directly
via an Ethernet wire and both with gigabit connections and
running under the same Linux kernel. We performed transfers
of 1 GB ﬁles via wget commands. With direct connections we
measured rates of 960 MB/s. Via the tunnel we measured rates
of 220-250 MB/s. We deactivated the ciphering to not reduce
VPN performances and only evaluate the tunnel performances.
At a more macroscopic level, we measured also non
constant rates between distant machines via tinc connections.
This problem is probably due to NAT ﬁrewall rules on network
link modems as mentioned in [6].
B. Torrent client evaluation
The selected torrent client is transmission, because it is
a FOSS torrent application. And the software provides also
a web interface to manage the downloads. All the theaters
have their own torrent clients accessible via a web interface
(example http://32.cinema.tdcpb.org for Utopia Tournefeuille).
Compared to other FOSS torrent clients transmission is slower
(in download time) than rtorrent in high speed networks.
We measured the download bandwidth inside the tetaneutral
local network. We downloaded a DCP from a server to a
virtual machine. With transmission the maximum measured
bandwidth was 96 MB/s. With the same DCP we measured
with rtorrent download rates of 230 MB/s. After code reading
in transmission client there are some timer loops which explain
the slower bandwidth.
C. Global TDB evaluation
We use a torrent tracker, named XBTT, to log all the
transfers. We can have the start and end times for the transfer
of each torrent. A sample of DCP transfer is presented in
Table II, which corresponds to the time window of Mr Turner
transfer (2 days, 8:45:45). All the DCP transfers presented
in this table have been ingested in a machine (utopian7) in
the laboratory network and then transmitted to server tdcpb31
and to theaters (cineXX). In that case the DCPs are available
to download for all the theaters. Since a DCP is available to
download in utopian7, this machine can be considered as a
primary seeder. The machine tdcpb31 is where all the DCPs
are ﬁnally stored and located at tetaneutral. In Table II, DCPs
are transferred to ﬁve theaters (noted cine35, cine69, etc.).
The theaters have different download bandwidths, the cine36
is connected to 100 Mbps optical ﬁber. For cine36 in that
case the maximum reached rate is 21.30 Mbps, so 5 times
less than the theoretical bit-rate of 100 Mbps. This is partly
explained by the optic ﬁber NSP (we did not reach more than
80 Mbps in direct download tests), due also to several DCPs
downloaded at the same time for instance (Timbuktu), and
also due to VPN tunnel limitations. The machine tdcpb31 is
connected to a high speed network (with a 1Gbps Ethernet
card). It is downloading (and partially uploading to other
theaters) DCP from utopian7 (MrTurner, Timbuktu, Quand
vient la nuit) and at the same time seeding to theaters (Men
women, White God). The download rates of tdcpb31 are slow
(11.68 Mbps) regarding the expected 1Gbps connections. The
VPN tunnel does not support trafﬁc rates greater than 10/15
Mbps when trafﬁc is in both directions (upload and download).
In a simpler case: no upload trafﬁc in tdcpb31, and utopian7
is only uploading to tdcpb31. We measured download rate
of 30 Mbps (not represented in Table II) and if we add a
virtual machine in tetaneutral network, which also downloads
the same DCP as tdcpb31 from utopian7 we reached rate of
58 Mbps for tdcpb31. The limitation is in the tunnel, not in
the connection between utopian7 and tetaneutral.
V.
TDB EVALUATION ANALYSIS
We designed a CDN for DCP delivery. Functionally, the
system is fully operative, and in “beta-production” since 6
months. We found one experimental solution for DCP delivery.
Now the problem is to ﬁnd an optimized solution for an
increase of payload (more theaters connected and more DCPs
to transfer). Putting aside the problem of tunnel transfer
limitation, which shall be addressed at a more “network” level,
we need to evaluate if the system can distribute all the DCPs
to all the theaters at an expected date (the ﬁrst screening day).
Figure 3 represents 8 months of ﬁlm screening at cinema
Utopia Bordeaux. Each Wednesday new ﬁlms are projected.
The number (and the size) of projected movies change each
week. The blue bars represents in GB the amount of new
ﬁlms each week. The calculated size is the real one, sum
of DCP sizes. The size of a DCP varies according to ﬁlm
duration and compression ratio. The red line represents in GB
the total amount of data that can be transferred on a network
link per week. From the logged transfers we extract a typical
download payload of 8 Mbps. During the 8 months represented
25
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

TABLE II. DCP TRANSFER SAMPLE.
Machine
Start
End
Duration
Movie
Size (GB)
Rate (Mbps)
cine35
2014-12-04 12:49:43
2014-12-06 16:19:13
2 days,3:29:30
Men women
202.34
8.73
cine69
2014-12-04 21:45:31
2014-12-07 03:06:57
2 days, 5:21:26
White God
203.16
8.46
tdcpb31
2014-12-05 11:07:25
2014-12-06 11:26:57
1 day, 0:19:32
Timbuktu
127.80
11.68
tdcpb31
2014-12-05 12:56:53
2014-12-07 10:32:29
1 day, 21:35:36
Quand vient la nuit
186.73
9.10
cine36
2014-12-05 13:46:24
2014-12-06 10:24:20
20:37:56
Quand vient la nuit
186.73
20.11
tdcpb31
2014-12-05 16:01:45
2014-12-08 00:47:30
2 days, 8:45:45
Mr Turner
135.50
5.30
cine36
2014-12-05 19:48:15
2014-12-06 14:49:54
19:01:39
Timbuktu
127.80
14.93
cine36
2014-12-06 10:07:05
2014-12-07 00:15:25
14:08:20
Mr Turner
135.50
21.30
cine56
2014-12-06 23:23:40
2014-12-07 21:59:46
22:36:06
Mr Turner
135.50
13.32
cine34
2014-12-07 00:07:30
2014-12-08 09:42:40
1 day, 9:35:10
Mr Turner
135.50
8.97
cine36
2014-12-07 10:36:22
2014-12-09 23:00:05
2 days, 12:23:43
White God
203.16
7.48
Figure 3. Movies distribution at theater Utopia Bordeaux. 8 months of DCP
screening are represented (real situation). Each week new ﬁlms are
presented. We represent in the blue bars the amount in GB of DCP to
receive.
in Figure 3 in 10 cases we cannot send all the DCPs in a week.
A solution is some cases, can be to use the free payload of the
previous weeks to start sending DCPs. This solution is limited
in time by the availability of the DCPs from the distributor.
The availability of the DCPs can be short (from two weeks
to less than one week) for movies in ﬁrst screening week.
Another solution is to increase the transfer payload, which
can be achieved by adding a new network link and using the
TDB aggregation method. Adding a new link is not always
possible, it depends on theater localisation and infrastructure.
For TDB, our goal is to distribute all the movies to a
theater. If we cannot achieve all the transfers in dematerialized,
we will send the remaining DCPs by physical transporter,
which will increase our costs. A simple solution to minimize
the costs is to send by transporter the largest DCPs, to free
payload on the network link. For each theater, we have a
distribution to achieve in time and the same movie can be
distributed to multiple theaters at the same time. To achieve
this goal we can adapt the mathematical model from [7] to our
CDN model. The aim of the proposed model is to minimize the
network operational cost and respect bandwidth constraints and
download time. We will add to this model the DCP availability
from distributor as input parameter.
VI.
CONCLUSION
In this paper, we have presented our operative CDN for
DCP distribution. The CDN will continue growing by con-
necting more theaters. We have demonstrated that we can
create a FOSS CDN, which respects all the constraints of DC
content delivery. Our system does not depend on any NSP
provider and can be deployed in any network. And we also
propose a network link aggregation for theaters without usage
of dedicated NSP service.
We have raised some networking issues: the bandwidth
limitation in the VPN tunnel. We will continue to investigate
this issue to deeply understand tunnel limitations and increase
our CDN global bandwidth. The proposed solution shall not
(or slightly) increase the network operational cost and respect
the FOSS requirement.
Future work will be to adapt the proposed model by [7] to
peer-to-peer distribution and increase the storage capacity in
data-centers. We are conﬁguring a server with storage capacity
based on ceph [8] distributed ﬁle system and erasure coding
algorithms.
REFERENCES
[1]
DCI, Digital Cinema System Speciﬁcation, 1st ed., Digital Cinema
Initiatives, Mar. 2012.
[2]
N. Bertrand, J.-D. Durou, and V. Charvillat, “Lecture de DCP pour le
cin´ema num´erique avec le lecteur multim´edia VLC et libav/ffmpeg,” in
Actes de la conf´erence CORESA’13, vol. 1, Le Creusot, France, May
2013, pp. 185–190, in french.
[3]
H. Sparenberg, T. Joormann, C. Feldheim, and S. Foessel, “Adaptive
RAID: Introduction of optimized storage techniques for scalable media,”
in Proceedings of the 20th IEEE International Conference on Image
Processing, Sep. 2013, pp. 1826–1830.
[4]
N. Blefari-Melazzi, D. Di Sorte, M. Femminella, L. Piacentini, and
G. Reali, “Performance evaluation of a multicast-based solution for
wireless resources discovery,” in Proceedings of the IEEE International
Conference on Communications, vol. 5, May 2005, pp. 3254–3260.
[5]
S. Khanvilkar and A. Khokhar, “Experimental evaluations of open-source
Linux-based VPN solutions,” in Proceedings of the 13th International
Conference on Computer, Communications and Networks, Oct. 2004,
pp. 181–186.
[6]
G. Sliepen, “The difﬁculties of a peer-to-peer VPN on the hostile Inter-
net,” in Proceedings of the Free and Open Source Software Developers’
European Meeting - FOSDEM, 2010.
[7]
D. Di Sorte, M. Femminella, G. Reali, and L. Rosati, “Deﬁnition and
performance evaluation of a request routing algorithm to distribute digital
cinema contents,” in Proccedings of the 4th International Telecommuni-
cation Networking Workshop on QoS in Multiservice IP Networks, Feb.
2008, pp. 27–32.
[8]
Ceph. [Online]. Available: http://ceph.com
26
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

