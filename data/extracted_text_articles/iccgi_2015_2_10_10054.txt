Facial Part Effects Analysis using Emotion-evoking Videos: Smile Expression 
 
 
Kazuhito Sato 
Department of Machine Intelligence and Systems 
Engineering, 
Faculty of Systems Science and Technology, Akita 
Prefectural University 
 Yurihonjo, Japan 
e-mail: ksato@akita-pu.ac.jp 
Hirokazu Madokoro 
Department of Machine Intelligence and Systems 
Engineering, 
Faculty of Systems Science and Technology, Akita 
Prefectural University 
 Yurihonjo, Japan 
e-mail: madokoro@ akita-pu.ac.jp 
Momoyo Ito 
Institute of Technology and Science, 
Tokushima University 
 Tokushima, Japan 
e-mail: momoito@is.tokushima-u.ac.jp 
 
 
Sakura Kadowaki 
 Smart Design Corp. 
 
Akita, Japan 
e-mail: sakura@smart-d.jp 
 
 
Abstract-This study specifically examines the expressive 
process of "happiness" related facial expressions after giving a 
stress stimulus. In addition, it presents a quantitative analysis 
of expressive tempos and rhythms using mutual information. 
By acquiring image datasets of facial expressions under states 
of pleasant–unpleasant stimulus for 20 participants, we 
calculated the information in three region of interests (ROIs): 
ROI 1, the whole face and the upper face; ROI 2, the whole 
face and the lower face; and ROI 3 between the upper face and 
the lower face. Additionally, we tried to express complexity 
and ambiguity objectively during facial expressions because of 
human psychological states. Results clarified the possibility of 
estimating the impression of facial expressions from the 
magnitude relation and order relation of mutual information 
of each ROI. More than male participants, female participants 
were able to create facial expressions of "happiness" easily and 
intentionally, and were less susceptible to discrepancy 
expressions. 
 
Keywords–Psychological measures, stress; Intentional facial 
expression; Machine learning approaches; Behavior modeling 
I. 
 INTRODUCTION 
Attractive smiles attract people and represent a symbol of 
happiness, soothing another person’s mind. Smiles are 
therefore effective as a lubricant of human communication. 
According to a study [1] that analyzed geometric features 
with respect to charming smiles, the most attractive part of 
smiles in both men and women is perceived as the eye, 
followed by the mouth. In addition, facial parts associated 
with the eyes and mouth, such as the corners of the eyes and 
mouth, are reportedly more important as attractive factors of 
smiles. In attractive smiles, the existence of a golden ratio 
was observed in the aspect ratio of the expression rectangle. 
Furthermore, Yamada et al. [2] investigated the relevance 
between the whole and partial impression formed from 
facial parts and pointed out the following points. Eyes play 
an extremely important role in forming impressions of 
others. It is possible to some degree to illustrate the overall 
impression by adding and coupling the partial impression 
formed from each part. However, they suggest that 
individual differences exist in the information of the parts 
which are expected to be related to emphasis. Assessing 
male and female viewpoints of smile expressions 
specifically, women are said to tend to expose smiles more 
than men [3]. Moreover, smiles are natural for women: 
women are better at making smiles than men. Particularly, 
women have excellent skills to adjust positive emotional 
expressions. Such natural expressions can elicit positive 
effects on a person viewing the smile (recipient). 
Nevertheless, for the creation of intentional facial 
expressions, different facial muscles are said to move in 
conjunction with natural facial expressions [4]. Particularly 
examining the expressive process, the deformation degree, 
and operation timing of facial parts creating smiles are 
expected to vary slightly. 
To clarify the relevance between facial expressions and 
psychological states to date, as a result of verifying the 
relevance 
between 
psychological 
stress 
and 
facial 
expressions using the framework of Facial Expression 
Spatial Charts (FESCs), we demonstrated that the degree of 
stress accumulation can be easily ascertained from facial 
expression types and expressive processes [5] [6]. 
Additionally, we proposed a framework of rhythms and 
30
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-432-9
ICCGI 2015 : The Tenth International Multi-Conference on Computing in the Global Information Technology

tempos that specifically examines actions to repeat 
intentional facial expressions after giving a stress stimulus 
[7]. We define one rhythm as one tempo repeated several 
times. In addition, regarding one tempo as the period during 
which facial expressions transform from a neutral face (i.e., 
expressionless) to the next neutral face, we found that the 
variation in unpleasant stimulus became greater than that in 
pleasant stimulus, addressing the variation of the number of 
frames constituting one tempo. Furthermore, using Bayesian 
networks, we constructed a graphical model of the relation 
between these three facial expressions and psychological 
stress factors. Results show that facial expressions 
displaying the effects of psychological stress easily were 
"happiness" and "sadness." Additionally, we showed the 
possibilities that facial parts (such as the eyes and mouth) 
easily differed by facial expression type [8] [9] [10]. 
In this study, particularly addressing the expressive 
process of "happiness" facial expression after giving a 
pleasant–unpleasant stress stimulus by emotion-evoking 
videos, we strove objectively to express complexity and 
ambiguity through facial expression because of human 
psychological states, by quantitative analysis of expressive 
rhythms from the viewpoint of mutual information. 
This paper is presented as follows. We review related 
work to clarify the position of this study in Section II. 
Section III presents a definition of a new framework of 
exposed rhythms and tempos for analyzing relations of 
psychological stress and facial expressions. Section IV 
describes a method to capture facial expression images, in 
addition to preprocessing, classification of facial expression 
patterns with self-organizing maps, integration of facial 
expression categories with fuzzy adaptive theory, and 
quantification 
of 
expressive 
rhythms 
using 
mutual 
information. We explain our originally developed facial 
expression datasets including stress measurements in 
Section V. In Section VI, based on the calculation results of 
mutual information in a time-series change of ELs for each 
facial region, we analyze the respective trends exhibited by 
men and women. Additionally, we discuss the effects of a 
pleasant–unpleasant stimulus which would give the 
expressive rhythm of facial expressions from the perspective 
of mutual information. Finally, we present conclusions and 
intentions for future work in Section VII. 
II. 
RELATED WORKS 
In spite of increasing or decreasing attractiveness of a 
"smile" with changes in expressive process, many 
conventional studies have examined the shape of a post-
expression face. Case studies examining the expressive 
process are few [11]–[14]. Regarding impression formation 
of friendly and thoughtful smile expressions, Ishi et al. [11] 
described the following. A continuous video presentation, 
such as expression levels from a neutral face become the 
maximum, is the most effective. Hanibuchi et al. [12] 
proposed a smile training method that specifically examines 
facial expressions process. Through impression evaluation 
experiments, they demonstrated the validity of goal setting 
with the actor's perspective. In addition, particularly 
addressing a natural smiling face, Fujishiro et al. [13] [14] 
investigated how eye, cheek, and mouth movements 
contribute to the impression formation of natural smiles in 
the 
expressive 
process. 
Results 
revealed 
moderate 
correlation between the behavioral termination of the eyes 
and cheek and the impression formation of natural smiles. 
Nevertheless, the authors did not report the psychological 
state of the actor when viewing a "natural smile" and 
"forced smile," such as a disagreement expression or 
expression suppression. Particularly, they were unable to 
come up to address impression formation based on the 
timing structure of facial parts. 
For a good impression on the face of a conversation 
partner, Kampe et al. [15] revealed that the good impression 
was more emphasized with matching of each other's eye-
gaze. Using anthropomorphic agents, Kuroki et al. [16] 
indicated the following. The combination of eye-gaze and 
facial expressions affects emphases of impressions. The 
impressive transmission of friendship properties can be 
emphasized particularly. Moreover, by analyzing brain 
activities using functional magnetic resonance imaging 
(fMRI) as physiological indices, an activation is observed in 
the prefrontal cortex responsible for higher cognitive 
functions such as emotional processing, motivation, and 
reasoning. Furthermore, the same activation is observed in 
the amygdala associated with emotions and rewards. 
Therefore, the formation of a good impression shows that 
the prefrontal cortex and the amygdala play mutually 
important roles [17]. However, impression evaluation has 
not been done subjectively for overall impressions of the 
face. Moreover, dealing with impression formation based on 
the timing structure of facial parts has not been achieved. 
III. 
FRAMEWORK OF EXPOSED RHYTHMS AND TEMPOS 
As an index for quantifying individual facial expression 
spaces, we proposed a framework of expression levels (ELs) 
[5]. The ELs include both features of the pleasure and 
arousal dimensions based on the arrangement of facial 
expressions 
on 
Russell’s 
circumplex 
model 
[18]. 
Specifically, we extract the dynamics of topological changes 
of facial expressions of facial components such as the eyes, 
eyebrows, and mouth. Topological changes show the 
structure defining the connection form of the elements in the 
set. The ELs obtained in this study are sorted to categories 
according to their topological changes in intensity from 
expressions that are regarded as neutral facial expressions. 
As discussed above, the ELs in this study include features of 
both pleasure and arousal dimensions. In Russell’s 
circumplex model, all emotions are constellated on a two-
dimensional space: the pleasure dimension of pleasure–
displeasure and arousal dimension of arousal–sleepiness. In 
31
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-432-9
ICCGI 2015 : The Tenth International Multi-Conference on Computing in the Global Information Technology

the intentional facial expressions covered in this study, 
direct handling of the facial expressions for the influence of 
pleasure dimension is difficult. Therefore, as a method of 
measuring transitory stress response, we conduct an 
evaluation using the salivary amylase test. As a method of 
measuring the transitory stress response, we conduct an 
evaluation using the salivary amylase test during the task of 
watching emotion-evoking videos causing a pleasant–
unpleasant state. Specifically examining the values of 
salivary amylase activity before and after watching videos, 
we can effectively perform stress measurements using 
salivary amylase tests to assess the stress state transiently. 
Consequently, we target the intentional facial expressions 
under pleasant and unpleasant stimulation states. 
In this study, using temporal variation of ELs, we intend 
to visualize rhythms and tempos of facial expressions that 
humans create. We defined one rhythm as a tempo that is 
repeated several times. One tempo is the period during 
which facial expressions are transformed from a neutral 
state to the next neutral state. Facial expressions exhibited 
intentionally by humans form an individual space based on 
the dynamic diversity and static diversity of the human face. 
Facial expression dynamics can be regarded as "topological 
changes in time-sequential facial expression patterns that 
facial muscles create." Static diversity is individual diversity 
that is configured by the facial component position, size, 
and location, consisting of the eyes, nose, mouth, and ears. 
In contrast, dynamic diversity denotes that a human can 
move facial muscles to express internal emotions 
unconsciously and sequentially or to express emotions as a 
message. After organizing and visualizing topological 
changes of face patterns by ELs, we attempt to use the 
framework of rhythms and tempos with expressions to 
examine ambiguities and complexities of facial expressions 
attributable to a psychological state. 
IV. 
PROPOSED METHOD 
Facial expression processes differ among individuals. 
Therefore, adaptive learning mechanisms are necessary for 
modification according to individual characteristic features 
of facial expressions. In this study, our target is intentional 
facial expressions. We use self-organizing maps (SOMs) 
[19] to extract topological changes of facial expressions and 
for normalization with compression in the direction of the 
temporal axis. After classification by SOMs, facial images 
are integrated using Fuzzy ART [20], which is an adaptive 
learning algorithm with stability and plasticity. In fact, 
SOMs perform unsupervised classification input data into a 
mapping space that is defined preliminarily. In contrast, 
Fuzzy ART performs unsupervised classification at a 
constant granularity that is controlled by the vigilance 
parameter. Therefore, using SOMs and Fuzzy ART, time-
series datasets showing changes over a long term are 
classified using a certain standard. Figure 1 presents an 
overview of the procedures used for our proposed method. 
In the following, we describe extraction of time-sequential 
 
 
Figure 1. Overview of the procedures used for our proposed method.  
 
32
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-432-9
ICCGI 2015 : The Tenth International Multi-Conference on Computing in the Global Information Technology

changes of ELs, and also explain quantification of 
expressive tempos and rhythms by mutual information. 
A. Acquisition of Time-series Variation of ELs 
We set the region of interest (ROI) to 90 × 80 pixels, 
including the eyebrows, which all contribute to the 
impression of a whole face as facial feature components. 
With preprocessing, brightness values are normalized for 
time-series images of facial expressions. The influence of 
brightness values attributable to illumination conditions is 
thereby reduced. Moreover, smoothing the histogram is 
useful to adjust contrast and clarify the images. In addition, 
using the orientation selectivity of Gabor Wavelets filtering 
as a feature representation method, the facial parts 
characterizing the dynamics of facial expressions are 
emphasized, such as the eyes, eyebrows, mouth, and nose. 
By down-sampling (i.e., 10 × 10 pixels) time-series facial 
expressions converted with Gabor Wavelets filtering [21], 
the effects of a slight positional deviation when taking facial 
images were minimized. Then data size compression was 
conducted. 
First, SOMs are used to learn the time-series images of 
facial expressions with down-sampling. The face images 
showing topological changes of facial expressions that are 
similar are classified into 15 mapping units of SOMs. Next, 
similar units (i.e., Euclidean distances of the weight vectors 
are close) among 15 mapping units of SOMs are integrated 
into the same category using Fuzzy ART. By sorting the 
facial expression categories integrated by Fuzzy ART from 
neutral facial expression to the maximum of facial 
expression, we obtain ELs labeled as expressive intensities 
of facial expressions quantitatively. The integrated category 
sorting procedure is based on the two-dimensional 
correlation coefficient of the average image of the facial 
expression images classified into each category. Finally, we 
conduct correspondence of ELs with each frame of the 
facial images to assess a time-series dataset of variation of 
ELs. 
B. Quantification of Exposed Rhythms using Mutual 
Information 
Mutual information [22] [23] can express changes 
between signals with the entanglement and synchrony. It 
can be regarded as an amount that represents linear and 
nonlinear dependence between the two time-series datasets. 
Moreover, it represents information flows and dynamically 
coupled rings between two signals. Mutual information 
between these two signals is zero if the two systems for 
observation target differ completely from independent ones. 
Applying this scheme to the facial expression process, it is 
possible to quantify the synchronicity and functional 
connectivity between facial parts. Figure 2 presents one 
example of time-series changes of ELs in the "Whole face," 
"Upper part of face," and "Lower part of face" obtained in 
Section 4.A. In this study, three ROIs listed below are 
calculated as the mutual information among facial parts in 
the expressive process. The time-series changes of ELs with 
respect to the "Whole face," "Upper part of face," and 
"Lower part of face" respectively represent
R (t)
R
w = w
, 
R (t)
R
u = u
, and 
R (t)
R
d = d
. Then, mutual information of each 
ROI is obtained as described below. 
Mutual information between the "Whole face" and 
"Upper part of face" is 
:)
;
(
I Rw Ru
 
)
,
(
)
(
)
(
)
;
(
u
w
u
w
u
w
R
H R
H R
H R
R
I R
−
+
=
 
(1) 
Mutual information between the "Whole face" and 
"Lower part of face" is 
:)
;
(
I Rw Rd
 
)
,
(
)
(
)
(
)
;
(
d
w
d
w
d
w
R
H R
H R
H R
R
I R
−
+
=
 
(2) 
Mutual information between the "Upper part of face" and 
"Lower part of face" is 
:)
;
(
I Ru Rd
 
)
,
(
)
(
)
(
)
;
(
d
u
d
u
d
u
R
H R
H R
H R
R
I R
−
+
=
 
(3) 
 
 
Figure 2. Each mutual information among time-series changes of facial parts. 
 
33
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-432-9
ICCGI 2015 : The Tenth International Multi-Conference on Computing in the Global Information Technology

In that equation, 
)
(
H Rw
, 
)
(
H Ru
, and 
)
(
H Rd
respectively 
represent the entropy of 
Rw (t)
, 
Ru (t)
, and 
Rd (t)
. 
)
,
(
H Rw Ru
, 
)
,
(
H Rw Rd
, and 
)
,
(
H Ru Rd
 respectively denote the 
joint entropy of both. 
V. 
DATASETS 
For this study, we constructed an original and long-term 
dataset for the specific facial expressions of participants. 
Details of the experimental protocols are the following. One 
experiment comprises three steps: step 1 is conducted under 
a normal state; step 2 is done during viewing of a pleasant 
video; and step 3 is done during viewing of an unpleasant 
video. We gave participants the task of watching emotion-
evoking videos, causing a pleasant–unpleasant state, and 
took stress measurements by salivary amylase tests to assess 
the stress state transiently. In addition, the watching time is 
about 3 min for each emotion-evoking video. We prepared 
unpleasant videos (i.e., implant surgery and cruel videos) 
and pleasant videos (i.e., comedy videos of three types). The 
subjective assessment of five stages was also conducted at 
watching videos. For all participants, we fully explained the 
experiment contents in advance, based on the research ethics 
policy of our university, and also obtained the consent of 
experiment participants in voluntary writing of participants. 
Moreover, from each, we received agreement to publish 
facial images as part of their experimental participation. 
A. Facial Expression Images 
Open datasets of facial expression images are open to the 
public through the internet from universities and research 
institutes. However, the specifications vary among datasets 
because of imaging with various conditions. As static facial 
images, the dataset presented by Ekman and Friesen [24] is 
a popular dataset comprising collected various facial 
expressions used for visual stimulation in psychological 
examinations of facial expression cognition. As dynamic 
facial images, the Cohn–Kanade dataset [25] and Ekman–
Hager dataset [26] are used widely, especially in 
experimental applications. In recent years, the MMI Facial 
Expression Database presented by Pantic et al. [27] and the 
CK+ dataset [28] have become a widely used open dataset 
containing both static and dynamic facial images. These 
datasets contain a sufficient number of people as horizontal 
datasets. However, facial images are taken only once for 
each person. No dataset exists in which the same person has 
been traced over a long term. Therefore, we created original 
and longitudinal datasets that include collections of the 
specific facial expressions of the same person during a long 
term. 
The six basic facial expressions proposed by Ekman et al. 
[24] are "happiness," "anger," "sadness," "disgust," "fear," 
and "surprise." Among those six basic facial expressions, 
we specifically examined the facial expression of 
"happiness," which is believed to be most likely to be 
exhibited spontaneously. As the target facial expression of 
"happiness" under pleasant and unpleasant stimulation states, 
we acquired the facial expressions of 20 people. As a 
stimulation method, we pre-selected emotion-evoking 
videos that elicit pleasant or unpleasant emotions, with all 
participants expressing facial expressions of "happiness" 
immediately after viewing them. Participants, all of whom 
were university students, were 10 men, whom we 
designated as A–J (J was 20 years old; B, G, H, and I were 
21; A, E, and F were 22; C and D were 23) and 10 women 
whom we designated as K–T (K, M, O, and P were 20 years 
old; L, Q, R, S, and T were 21; N was 23). The imaging 
period was three weeks at one-week intervals for all 
participants. 
The 
imaging 
environment 
for 
facial 
expressions was an imaging space partitioned by a curtain in 
the corner of the room. We took frontal facial images with 
conditions including the head of the participant in each 
image. In advance, we instructed each participant to expose 
the facial expression with no head movement. Consequently, 
imaging the face region to fit within the scope was possible. 
However, with respect to extremely small changes caused 
by body motion, we used template-matching methods to 
trace the face region by setting the initial template to include 
facial parts. By consideration of the application deployment 
and ease of imaging in future studies, we used commercially 
available USB cameras (QcamOrbit; Logicool Inc. [29]). 
When taking images of each facial expression, the same 
expression was repeated three times based on the neutral 
facial expression during the image-taking period of 20 s. We 
had previously instructed all participants to express an 
emotion three times at their own timing according to a 
guideline for 20 s. One dataset consisted of 200 frames with 
the sampling rate of 10 frames per second. 
B. Stress Measurement Method 
Because types of psychological stress are regarded as 
affecting facial expressions, we assessed transient stress and 
chronic stress. Chronic stress is that which humans have on 
a daily basis, whereas transient stress is that caused by a 
temporary stimulus. To assess transient stress stimulus to 
the participants in this study, we applied the salivary 
amylase test, which measures transient stress reactions. As a 
biological reaction, salivary amylase activity is detected as a 
low value if one is in a pleasant state. In contrast, the value 
is high if one is in an unpleasant state. As stress reactions 
when subjected to external transient stimulus, Yamaguchi et 
al. [30] confirmed that salivary amylase activity is an 
effective means of stress evaluation. For this study, using 
emotion-evoking videos as an external transient stimulus, 
we used the salivary amylase test method to measure stress 
reactions immediately after participants watched the videos. 
34
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-432-9
ICCGI 2015 : The Tenth International Multi-Conference on Computing in the Global Information Technology

VI. 
EXPERIMENT 
Based on the calculation result of mutual information in a 
time-series change of ELs for each facial region, we 
analyzed the respective male and female trends. Finally, we 
discussed the effects of a pleasant–unpleasant stimulus 
which would give the expressive rhythm of facial 
expressions from the perspective of mutual information. 
A. Analysis of Female Participants 
Figure 3 depicts the calculation results of mutual 
information of five cases of female participants K, L, M, O, 
and P. The results show the mutual information of the time-
series variation of ELs in each face region described in 
Section 4.B. Figure 3-(a) presents the calculation results 
obtained after giving a pleasant stimulus. Figure 3-(b) 
shows calculation results obtained after giving unpleasant 
stimulus. As an overall trend of female participants, we 
confirmed the following. For K, L, O, and P, the value of 
the mutual information is reduced to the order of "ROI 1: 
between the whole face and upper face," "ROI 2: between 
the whole face and lower face," and "ROI 3: between the 
upper face and lower face." The value of "ROI 2: between 
the whole face and lower face" is clearly larger than those 
for other ROIs in M. For K, M, and O, we were unable to 
recognize a marked change in the trend of mutual 
information by pleasant–unpleasant stimulus. However, for 
     
 
 
(a) Pleasant stimulus with emotion-evoking videos                              (b) Unpleasant stimulus with emotion-evoking videos 
 
Figure 3. Mutual information results among each facial part for female. 
 
  
 
(a) Subject K                                   (b) Subject M  
 
Figure 4. Time-series changes of smile facial expression with pleasant stimulus for specified subjects of female. 
 
35
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-432-9
ICCGI 2015 : The Tenth International Multi-Conference on Computing in the Global Information Technology

L and P, we detected a specific change in the trend of the 
mutual 
information 
after 
giving 
pleasant–unpleasant 
stimulus. Particularly, the tendency of L is remarkable. In 
pleasant stimulus, the value of the mutual information is 
reduced to the order of "ROI 1: between the whole face and 
upper face," "ROI 2: between the whole face and lower 
face," and "ROI 3: between the upper face and lower face." 
Otherwise, "ROI 2: between the whole face and lower face" 
shows a large value for the unpleasant stimulus. In the 
unpleasant stimulus, the value of the mutual information is 
reduced to the order of "ROI 1: between the whole face and 
upper face," "ROI 2: between the whole face and lower 
face," and "ROI 3: between the upper face and lower face." 
However, in pleasant stimulus, the order relation of mutual 
information of P is reversed with L because the value of 
"ROI 1: between the whole face and upper face" is reduced. 
Next, although the same trend is apparent for both 
pleasant and unpleasant stimuli, we compare K to M, for 
which the order relation of the mutual information in each 
facial region is markedly different. For K in both pleasant 
and unpleasant stimuli, the mutual information value of 
"ROI 1: between the whole face and upper face" is larger 
than "ROI 2: between the whole face and lower face." In 
addition, particularly addressing "ROI 3: between the upper 
face and lower face," the value of K is larger than M. 
However, for M with both pleasant and unpleasant stimuli, 
   
     
(a) Pleasant stimulus with emotion-evoking videos                    (b) Unpleasant stimulus with emotion-evoking videos 
 
Figure 5. Mutual information results among each facial part for male. 
 
 
      
  
(a) Subject D                                      (b) Subject J  
 
Figure 6. Time-series changes of smile facial expression with pleasant stimulus for specified subjects of male. 
 
36
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-432-9
ICCGI 2015 : The Tenth International Multi-Conference on Computing in the Global Information Technology

the mutual information value of "ROI 2: between the whole 
face and lower face" is markedly larger than others. 
Furthermore, particularly addressing "ROI 1: between the 
whole face and upper face" and "ROI 3: between the upper 
face and lower face," the values of M are clearly smaller 
than those of K. For K and M, thumbnail images 
representing the time-series changes of "happiness" in 
pleasant stimulus are shown in Figure 4. Figures 4-(a) and 
4-(b), respectively present thumbnail images of K and M. 
The top of each figure shows the characteristic section 
during exposed facial expression of "happiness." Comparing 
the thumbnail images shown in Figure 4 to the calculation 
result of mutual information shown in Figure 3, for K 
exposed "happiness," we can recognize the change of facial 
expression in the upper face such as the brow and the area 
around the eyes, and in the lower face such as the mouth. 
Otherwise, for M, we can not observe any change of facial 
expression in the upper face. However, only the corner of 
mouth in the lower face has changed significantly. Actually, 
K has the characteristics which the upper part and lower 
face change both synchronized during facial expressions. 
Staying on the subjective impression of the experimenter, 
the result for "happiness" looks more natural facial 
expressions. In contrast, for M, only the corner of the mouth 
in the lower face has been changed. Therefore, we have an 
uncomfortable feeling about the unnatural facial expression 
of "happiness." 
B. Analysis of Male Participants 
Figure 
5 
presents 
calculation 
results 
of 
mutual 
information of five cases of male participants. Figure 5-(a) 
presents the calculation results after giving pleasant stimulus. 
Figure 5-(b) shows the calculation results after giving 
unpleasant stimulus. As an overall trend of male participants, 
we confirmed the following. For D and F, the value of the 
mutual information is reduced to the order of "ROI 1: 
between the whole face and upper face," "ROI 2: between 
the whole face and lower face," and "ROI 3: between the 
upper face and lower face." The value of "ROI 2: between 
the whole face and lower face" is markedly larger than those 
of other ROIs in C, G, and J. For male participants C, D, F, 
G and J, we were unable to recognize a marked change in 
the trend of mutual information by pleasant–unpleasant 
stimulus. 
Next, regarding male participants, we compare D to J, for 
whom the order relation of the mutual information in each 
facial region is significantly different. For D in both 
pleasant and unpleasant stimuli, the mutual information 
value of "ROI 1: between the whole face and upper face" is 
larger than "ROI 2: between the whole face and lower face." 
In addition, particularly addressing "ROI 3: between the 
upper face and lower face," the value of D is larger than that 
of J. However, for J in both pleasant and unpleasant 
stimulus, the mutual information value of "ROI 2: between 
the whole face and lower face" is markedly larger than 
others. In addition, particularly addressing "ROI 1: between 
the whole face and upper face" and "ROI 3: between the 
upper face and lower face," the values of J are clearly 
smaller than those of D. For D and J, the thumbnail images 
representing the time-series changes of "happiness" in 
pleasant stimulus are portrayed in Figure 6. Figures 6-(a) 
and 6-(b) respectively present thumbnail images of D and J. 
The top of each figure shows the characteristic section 
during exposed facial expression of "happiness." Comparing 
the thumbnail images shown in Figure 6 to the calculation 
result of mutual information shown in Figure 5, for D 
exposed "happiness," we can recognize the change of facial 
expression in the upper face such as the brow and around the 
eyes, and in the lower face such as mouth. Otherwise, for J, 
no change of facial expression can be observed in the upper 
face. Only the corner of the mouth in the lower face has 
changed substantially. Actually, D has characteristics by 
which the upper part and lower face change at the same time 
during facial expressions. Therefore, the exposing result of 
"happiness" looks more natural facial expressions. In 
contrast, for J, only the corner of the mouth in the lower 
face 
has 
been 
changed. 
Therefore, 
we 
have 
an 
uncomfortable feeling about the unnatural facial expression 
of "happiness." These results underscore a common 
tendency between male and female participants and can be 
anticipated as a new index for quantification of the 
impression during facial expressions based on the mutual 
information of the time-series change of each face region. 
C. Effects of Pleasant–unpleasant Stimulus on Mutual 
Information 
The discrepancy expression in facial expressions means 
to expose the emotions that do not match one’s own feelings 
when experiencing certain emotions, such as having a smile, 
even though one might be in a sad mood. In previous studies, 
being positive emotional expressions during negative 
emotional experiences has been shown to engender the 
following: an amplification of actor’s sympathetic nerve 
activities [31], an increase of subjective emotional 
experiences, and some memory loss [32]. The discrepancy 
expression can easily take cognitive loads for expressive 
person. Additionally, it can potentially give bad effects to 
the mental health of actors. Furthermore, the expressive 
suppression in facial expressions indicates an emotional 
suppression by facial expressions when experiencing a 
certain emotion, such as to stifle crying when in a sad mood. 
Expressive suppression is reportedly associated with social 
support, closeness with others, and reduction in social 
satisfaction [33]. In comparison to men, women are more 
skilled at making smiles and excellent adjustments of 
positive emotional expressions. Moreover, women show 
similar effects such as natural expressions to recipients [3]. 
37
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-432-9
ICCGI 2015 : The Tenth International Multi-Conference on Computing in the Global Information Technology

Exposing facial expressions related to "happiness" after 
viewing an unpleasant video is equivalent to a discrepancy 
expression. In contrast, exposing the facial expression of 
"happiness" after viewing a pleasant video is a matching 
expression. For female participants K and M, such order of 
mutual information was markedly different; Figure 7 
presents their facial expression rhythms. In the impression 
analysis of Section 6.B, the smile of K gave us a natural 
impression. In contrast, we received an unnatural impression 
from the smile of M. Focusing on an expressive rhythm of 
each facial part, the expressive rhythm of K indicates a 
time-series change such as to work together in each facial 
part. In contrast, we were unable to recognize cooperative 
movements at all in the expressive rhythm of M because the 
upper face and the lower face are independent. The mutual 
information of the ROIs (i.e., ROI 1, ROI 2, and ROI 3) 
effectively expresses the degree of similarity and 
synchronization of signal waveforms in facial expression 
rhythms. These mutual information values can be 
interpreted as quantified indices of the timing structure 
indicating the synchronization between the upper face (e.g., 
the eyebrows and eyes) and the lower face (e.g., mouth), 
which contribute the impression formation to the whole face. 
We should comprehensively consider the analysis results of 
Sections 6.B and 6.C. By particularly addressing the 
magnitude relation between ROI 1 and ROI 2 with respect 
to the mutual information, we were able to interpret "Eyes 
say things sufficient to mouth" quantitatively. Around the 
value of ROI 3 quantifying the timing structure between the 
upper face and the lower face, noting the magnitude relation 
and order relation between the values of ROI 1 and ROI 2, it 
is effective as an index for quantifying the degree of 
spontaneity 
and 
artificiality 
in 
facial 
expressions. 
Furthermore, more than male participants, the female 
participants easily created facial expressions of "happiness" 
intentionally. Then we assumed that result was only slightly 
affected by the discrepancy expression. 
VII. CONCLUSION AND FUTURE WORK 
In this study, to acquire image datasets of facial 
expressions under the states of pleasant–unpleasant stimulus 
for 20 participants (i.e., 10 men, 10 women), we used 
salivary amylase tests to validate emotional factors when 
viewing 
emotion-evoking 
videos. 
Additionally, 
by 
quantitative analysis of expressive rhythms from the 
viewpoint of mutual information, particularly addressing 
expressive processes of "happiness" facial expression after 
giving a pleasant–unpleasant stress stimulus by emotion-
evoking videos, we objectively strove to ascertain 
complexity and ambiguity when making facial expressions 
because of human psychological states. Using evaluation 
experiments examining 10 participants (i.e., 5 men, 5 
women), we analyzed the information of time-series 
changes in ROIs (i.e., ROI 1, ROI 2, and ROI 3), revealing 
the following points. By particularly addressing the 
expressive rhythm of each face region, one can estimate the 
impression of facial expressions from the magnitude relation 
and order relation of mutual information of each ROI. 
Additionally, the mutual information of expressive rhythms 
is effective as an index for measuring degree of spontaneity 
and 
artificiality 
during 
facial 
expressions. 
Female 
participants were better able to create facial expressions of 
"happiness" easily and intentionally than male participants 
were. Moreover, they were less susceptible to discrepancy 
expressions. In future work, by quantifying fluctuations of 
expressive tempos in facial parts upon the impression 
formation, and analyzing their timing structure, we intend to 
clarify differences of expressive paths between intentional 
and spontaneous facial expressions. 
ACKNOWLEDGMENT 
    
[frame]
[frame]
[frame]
[ELs]
[ELs]
[ELs]
Whole face
Upper part of face
Lower part of face
  
[frame]
[frame]
[frame]
[ELs]
[ELs]
[ELs]
Whole face
Upper part of face
Lower part of face
 
(a) Subject K                                                                                          (b) Subject M 
 
Figure 7. Comparison of time-series changes of ELs with unpleasant stimulus. 
 
38
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-432-9
ICCGI 2015 : The Tenth International Multi-Conference on Computing in the Global Information Technology

The authors thank the 20 students at our university who 
participated by letting us take facial images over such a long 
period. This work was supported by the Japan Society for 
the Promotion of Science (JSPS) KAKENHI Grant Number 
25330325 and the Cosmetology Research Foundation. 
REFERENCES 
[1] 
T. Iguchi, “Geometrical Features of the Attractive Smile: –Attractive 
Production by Kansei X Technology = Kanseiweab–,” The Institute 
of Electronics, Information, and Communication Engineers, 
Technical Report, Mar. 2007, pp. 51–56. 
[2] 
T. Yamada and I. Sasayama, “A Study of the Correlation between 
the Impression Formed from Each Features and the Impression 
Formed from Face,” Bulletin of Fukuoka University of Education，
Vol. 48, No. 4, 1999, pp. 229–239. 
[3] 
L. Ellis, “Gender differences in smiling: An evolutionary 
neuroandrogenic theory,” Physiology and Behavior, Vol. 88, 2006,  
pp. 303–308. 
[4] 
K. M. Prkachin, “Effects of deliberate control on verbal and facial 
expressions of pain,” Pain, Vol. 114, 2005, pp. 328–338. 
[5] 
H. Madokoro, K. Sato, and S. Kadowaki, “Facial expression spatial 
charts for representing time-series changes of facial expressions,” 
Japan Society for Fuzzy Theory, Vol. 23, No. 2, 2011, pp. 157–169. 
[6] 
H. Madokoro and K. Sato, “Facial Expression Spatial Charts for 
Representing of Dynamic Diversity of Facial Expressions,” Journal 
of Multimedia, Vol. 6, No. 1, Jan. 2007, pp. 1–12. 
[7] 
K. Sato, H. Madokoro, and S. Kadowaki, “Transient Stress Stimulus 
Effects on Intentional Facial Expressions,” Japan Society for Fuzzy 
Theory, RJ-005, 2012, pp. 29–36. 
[8] 
K. Sato, H. Otsu, H. Madokoro, and S. Kadowaki, “Analysis of 
Psychological Stress Factors on Intentional Facial Expressions,” 
Japan Society for Fuzzy Theory, RJ-002, 2013, pp. 21–28. 
[9] 
K. Sato, H. Otsu, H. Madokoro, and S. Kadowaki, “Analysis of 
Psychological Stress Factors and Facial Parts Effect on Intentional 
Facial Expressions,” Proceedings of the Third International 
Conference on Ambient Computing, Applications, Services and 
Technologies , Oct. 2013, pp. 7–16. 
[10] K. Sato, H. Otsu, H. Madokoro, and S. Kadowaki, “Analysis of 
Psychological Stress Factors by Using Bayesian Network,” 
Proceedings 
of 
2013 
IEEE 
International 
Conference 
on 
Mechatronics and Automation, Aug. 2013, pp. 811–818. 
[11] H. Ishi, M. Kamachi, and J. Gyoba, “Effect of Facial Motion on 
Impression of Smile,” The Institute of Electronics, Information, and 
Communication Engineers, Technical Report, Dec. 2004, pp. 25–30. 
[12] S. Hanibuchi, K. Ito, and S. Nishida, “Analysis of Transformed 
Impression of Smile Process: – An Approach to Supporting Facial 
Expression Process Training –,” The Institute of Electronics, 
Information, and Communication Engineers, Technical Report, Oct. 
2009, pp. 35–40. 
[13] H. Fujishiro, A. Maejima, and S. Morishima, “Natural Smile 
Synthesis Considering Impression of Facial Expression Process,” 
The Institute of Electronics, Information, and Communication 
Engineers, Technical Report, Mar. 2011, pp. 31–36. 
[14] H. Fujishiro, A. Maejima, and S. Morishima, “Analysis of Relation 
between Movement of Smile Expression Process and Impression,” 
The Journal of the Institute of Electronics, Information, and 
Communication Engineers, Vol. J95-A, No. 1,  2012,  pp. 128–135. 
[15] K. W. Kampe, C. D. Frith, R. J. Dolan, and U. Frith, “Reward value 
of attractiveness and gaze,” Nature, Vol. 413, Oct. 2001. 
[16] Y. Kuroki, S. Shiraishi, N. Mukawa, M. Yuasa, and N. Fukayama, 
“Interaction between Human and Human-like Agent with Gaze and 
Facial Expression for Human Computer Interaction,” The Institute of 
Electronics, Information, and Communication Engineers, Technical 
Report, Mar. 2005, pp. 49–54. 
[17] Y. Kuroki, S. Shiraishi, N. Mukawa, M. Yuasa, and N. Fukayama, 
“Impression of Human-like Agent with Gaze and Facial Expression: 
–Brain Activity Analysis of HCI using fMRI–,” The Institute of 
Electronics, Information, and Communication Engineers, Technical 
Report, Mar. 2006, pp. 43–48. 
[18] J.A. Russell and M. Bullock, “Multidimensional Scaling of 
Emotional Facial Expressions: Similarity from Preschoolers to 
Adults,” Journal of Personality and Social Psychology, Vol. 48, 
1985, pp. 1290–1298. 
[19] T. Kohonen, Self-organizing maps, Springer Series in Information 
Sciences, 1995. 
[20] G. A. Carpenter, S. Grossberg, and D.B. Rosen, “Fuzzy ART: fast 
stable learning and categorization of analog patterns by an adaptive 
resonance system,” Neural Networks, vol. 4, 1991, pp. 759–771. 
[21] M. Haghighat, S. Zonouz, and M. Abdel-Mottaleb, “Identification 
Using Encrypted Biometrics,” Computer Analysis of Images and 
Patterns, Springer Berlin Heidelberg, 2013, pp. 440–448. 
[22] T. Ikeda, H. Ishiguro, and M. Asada, “Moving Signal-Source 
Tracking Based on Mutual Information Maximization,” The 
Transactions of the Institute of Electronics, Information, and 
Communication Engineers D, Vol. J90-D, No. 2, 2007, pp. 535–543. 
[23] T. Kikuchi, K. Kishi, and J. Miyamichi, “An Automatic Data 
Classification Algorithm Adjusted by Mutual Information, ” The 
Transactions of the Institute of Electronics, Information, and 
Communication Engineers D, Vol. J82-D, No. 4, 1999, pp. 660–668. 
[24] P. Ekman and W. V. Friesen, “Unmasking the Face: A Guide to 
Recognizing Emotions from Facial Clues,” Malor Books, 2003. 
[25] T. Kanade, J. F. Cohn, and Y. L. Tian, “Comprehensive database for 
facial expression analysis,” Proc. of the Fourth IEEE Int. Conf. on 
Automatic Face and Gesture Recognition, 2000, pp. 46–53. 
[26] M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski, “Measuring 
facial expressions by computer image analysis,” Psychophysiology, 
Vol. 36, 1999,  pp. 253–264. 
[27] M. Pantic, M.F. Valstar, R. Rademaker, and L. Maat, “Web-based 
Database for Facial Expression Analysis,” Proc. IEEE Int'l. Conf. 
Multimedia and Expo, Amsterdam, The Netherlands, Jul. 2005. doi: 
10.1109/ICME.2005.15214. 
[28] P. Lucey et al., “The Extended Cohn–Kanade Dataset (CK+): A 
complete expression dataset for action unit and emotion-specified 
expression,” Proc. of the Third Int. Workshop on CVPR for Human 
Communicative Behavior Analysis, 2010, pp. 94–101. 
[29] QcamOrbit; Logicool Inc., http://www.logicool.co.jp/ja-jp/webcam-
communications/webcams [retrieved: July, 2015] 
[30] M. Yamaguchi, T. Kanamori, M. Kanemaru, Y. Mizuno, and H. 
Yoshida, “Correlation of Stress and Salivary Amylase Activity,” 
Japanese Journal of Medical Electronics and Biological Engineering: 
JJME, Vol. 39, No. 3, Sep. 2001, pp. 46–51. 
[31] J. L. Robinson and H. A. Demaree, “Physiological and cognitive 
effects of expressive dissonance,” Brain and Cognition, Vol. 63, 
2007, pp. 70–78. 
[32] W. Sato, M. Noguchi, and S. Yoshikawa, “Emotion elicitation effect 
of films in a Japanese sample,” Social Behavior and Personality, Vol. 
35, 2007, pp. 863–874. 
[33] S. Srivastava, M. Tamir, K. M. McGonigal, O. P. John, and J. J. 
Gross, “The social costs of emotional suppression: A prospective 
study of the transition to college,” Journal of Personality and Social 
Psychology, Vol. 96, 2009, pp. 883–897. 
39
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-432-9
ICCGI 2015 : The Tenth International Multi-Conference on Computing in the Global Information Technology

