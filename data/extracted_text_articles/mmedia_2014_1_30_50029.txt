Multiexposure Image Fusion Using Homomorphic Filtering and Detail 
Enhancement 
Hui-Chun Tsai    Jin-Jang Leou    Han-Hui Hsiao 
 
Department of Computer Science and Information Engineering 
National Chung Cheng University 
Chiayi 621, Taiwan 
{thc100m, jjleou, hhh95p}@cs.ccu.edu.tw 
 
 
Abstract—In this study, a multiexposure image fusion 
approach using homomorphic filtering and detail enhancement 
is proposed. First, the N input low dynamic range (LDR) RGB 
color images are transformed into the HSI color space. 
Intensity enhancement is achieved by homomorphic filtering, 
gamma correction is used to compensate the nonlinear 
response of display devices, and “cross-image” median filtering 
is used to generate the reference intensity image. Guided 
filtering and weighted least squares (WLS) optimization are 
used to perform local and global detail extractions on the N 
processed LDR images, respectively. The N weighting maps of 
the N processed LDR images are estimated by spatial and 
cross-image consistencies and then refined by cross bilateral 
filtering. Finally, the multiresulution spline based scheme is 
used to perform multiexposure image fusion. Based on the 
experimental results obtained in this study, the performance of 
the proposed approach is better than those of four comparison 
approaches. 
Keywords-low dynamic range (LDR) image; high dynamic 
range (HDR) image; tone mapping; homomorphic filtering; 
multiexposure image fusion 
I. 
 INTRODUCTION 
In the last decade, image fusion has been employed in 
different application areas [1-2]. Image sensors usually have 
a limited dynamic range and a low dynamic range (LDR) 
image usually contains some under-exposed or over-exposed 
regions. Additionally, a natural scene usually contains high 
dynamic range (HDR) contents. To cope with this problem, a 
series of LDR images with different exposures can be fused 
to obtain an HDR image, which will be displayed on LDR 
devices. There are two main types of HDR imaging, namely, 
typical HDR imaging and multiexposure image fusion [3]. 
HDR imaging consists of two main steps: HDR 
reconstruction and tone mapping. First, HDR reconstruction 
techniques [4] usually recover the camera response function 
(CRF) and combine the radiance maps via a weighting 
function from a series of LDR images. Second, tone mapping 
is to compress the dynamic range of HDR images in order to 
display on LDR devices. Existing tone mapping approaches 
can be classified into global and local operators [5-7]. 
Compared to HDR reconstruction, multiexposure image 
fusion usually consists of two steps: selection and blending 
[8]. “Selection” decides the best representative regions and 
exposures among all the input LDR images via assigning 
weights to the pixels of each LDR image. For blending, the 
selected regions from LDR images are fused according to 
their weights individually. 
Multiexposure image fusion is similar to alpha blending 
[9]. Li, Zheng, and Rahardja [10] introduced a new quadratic 
optimization based image fusion approach. In [3], a mostly 
detailed LDR image is synthesized directly from input LDR 
images by solving different optimization problems. Song et 
al. [11] proposed a probabilistic model to preserve the 
calculated image luminance levels and suppress reversals in 
image luminance gradients. 
On the other hand, in Mertens et al. [1], a weight for a 
pixel is determined by three quality measures: contrast, 
saturation, and well-exposedness. All LDR images are 
blended at multiple scales by using the Laplacian and 
Gaussian pyramidal image decompositions. Gu et al. [12] 
modified the gradient field iteratively with twice average 
filtering and nonlinearly compressing in multi-scales. Fused 
gradient field is derived from the structure tensor of LDR 
images based on multi-dimensional Riemannian geometry. 
Zhang and Cham [13] used the gradient information to 
accomplish multiexposure image composition in both static 
and dynamic scenes. Zhang and Cham [14] also proposed a 
multiexposure image fusion approach for both static and 
dynamic scenes using both temporal consistency and spatial 
consistency. Zeev et al. [15] introduced a new way to 
construct edge-preserving multi-scale image decompositions, 
based on weighted least squares (WLS) optimization. 
The paper is organized as follows. The proposed 
multiexposure image fusion approach is described in Section 
2. Experimental results are addressed in Section 3, followed 
by concluding remarks. 
II. 
PROPOSED APPROACH 
A. System Architecture 
As shown in Fig. 1, the proposed multiexposure image 
fusion approach for static scenes contains six stages. First, 
the N input LDR color images are transformed from the 
RGB color space into the hue, saturation, and intensity (HSI) 
color space so that the intensity and color (hue and saturation) 
components 
can 
be 
separately 
processed. 
Intensity 
enhancement is achieved by homomorphic filtering in the 
frequency domain and gamma correction [16] is used to 
14
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

compensate the nonlinear response of display devices. To 
eliminate under-exposed or over-exposed regions in LDR 
images, the “cross-image” median filter is used to generate 
the reference intensity image. The guided filter [7] and 
weighted least squares (WLS) optimization [15] are used to 
perform local and global detail extractions on the N 
processed LDR images with gamma correction, respectively. 
Based on the reference intensity image, spatial consistency 
and cross-image consistency involving five consistency 
measures are computed to estimate the N weighting maps of 
the N processed LDR images with gamma correction. The 
cross bilateral filter is used to refine the N weighting maps. 
Finally, the multiresulution spline based scheme [17] is used 
to perform multiexposure image fusion and generate the final 
HDR image. 
Color 
components
Intensity 
component
Final HDR image 
LDR images 
Weighting map
 estimation
Gamma correction
Image fusion
Reference image 
generation
Color space 
transformation
Homomorphic
filtering
Detail extraction
 
Figure 1.  The framework of the proposed approach. 
B. Homomorphic filtering 
In this study, considering nonlinear intensity perception 
of the human visual system (HVS), the homomorphic filter is 
used to perform intensity enhancement in the frequency 
domain. 
( , )
x y
I
in
 denotes the intensity of pixel 
( , )
x y
 in the 
n-th input LDR image, 
, )
, (
x y
I
n
i H
 denotes the intensity of 
pixel 
( , )
x y
 in the n-th homomorphic filtered image, and 
( , )
H u v
is the transfer function of the homomorphic filter. 
The homomorphic filter processes the illumination and 
reflection components separately in the frequency domain 
(u,v) via the logarithm function. Here, 
( , )
H u v
 is a modified 
Gaussian highpass filter defined as 
,
]
) 1[
(
, )
(
)
( , ) /
(
2
0
2
L
D
u v
D
c
L
H
r
e
r
r
H u v






 
(1) 
where the constant c controls the sharpness of the transition 
slop of the filter function between 
Lr and 
Hr , 
0
D  is a positive 
constant, and 
, )
(
2
D u v
 is the distance between a point (u,v) 
and the center (W/2,H/2) of the frequency rectangle. Here, 
Lr ,
 
Hr ,
 and 
,c
 are empirically set to 0, 1, and 1, 
respectively. 
 
C. Gamma Correction 
Gamma correction [16] is used to compensate the 
nonlinear response of display devices, which is defined as 
, ,
,
,
)
(
R G B
C
L
L
I
I
out
s
in
n
C
n
C
in



 
(2) 
where s denotes the gamma correction coefficient in the 
range [0,1], 
n
ICin
 and 
n
CI  are the color components of the n-th 
LDR image and the n-th processed LDR image with gamma 
correction, respectively, and Lin and Lout denote the 
luminances of 
nI
 and 
n
IH
 (the n-th homomorphic filtered 
LDR color image), respectively. 
 
D. Reference Intensity Image Generation 
In this study, the reference intensity image is generated 
by performing cross-image median filtering over the N 
homomorphic filtered LDR images to exclude under-
exposed or over-exposed regions in the N input LDR images. 
Cross-image median filtering is performed in a pixel-by-
pixel manner over the N homomorphic filtered LDR images 
to generate the “median” image of 
( , )
x y
I n
H
, 
,
,1 2,...,
N
n 
 
as the reference intensity image, i.e., 
( , )),
,
( , ),
( , ),
median(
, )
(
2
1
x y
I
x y
x y I
I
x y
I
N
H
H
H
R


 
(3) 
where 
( , )
I R x y
 denotes pixel 
( , )
x y
 of the reference 
intensity image and N is the number of homomorphic filtered 
LDR images. 
 
E. Detail Extraction 
Edge-preserving filters, such as the bilateral filter [18], 
weighted least squares optimization [15], and the guided 
filter [7], will not blur strong edges (without ringing artifacts) 
in the decomposition process. Using edge-preserving 
filtering, detail extraction is to decompose each processed 
LDR image with gamma correction into two (base and detail) 
layers and use the detail layer to compensate image details. 
1) Local detail extraction 
In this study, the guided filter [7], an edge-preserving 
filter, is used to decompose each processed LDR image with  
gamma correction into a base layer and a detail layer, i.e., 
,
ˆ
L
L
C
I
I
I


 
(4) 
where 
LI
 and 
LIˆ
 denote the base and detail layers, 
respectively. Here, the guided filter is applied to the three (R, 
G, B) color component images when edges or fine details 
are not discriminative in a single color component image. It 
is assumed that the filtering output 
LI
 is a linear 
15
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

transformation of the guidance image 
CI  in a local window 
  centered at pixel k, i.e., 
,
k
k
C
T
k
k
L
b
a I
I


 
(5) 
where   is a 3×3 sliding window, 
k
LI  and 
k
IC
 denote the   
3×3 pixels of 
IL
 and 
IC
 in window 
,
  T denotes the 
transpose operator, and 
k
a  and 
kb  are two matrices of 
constants in window  , which can be directly estimated by 
linear regression as 
),
) ( 1
(
1
U
U
Σ








q
k
k
k
k
C
k
k
P
I P
a


 
(6) 
,
k
T
k
k
k
a
P
b



U
 
(7) 
where P  is the input image, 
k
P  denotes the 3×3 pixels of 
P  in window 
,
  
kP  is the mean of the 3×3 pixels of P  in 
window 
,
  
k
  is the mean of the guidance image 
IC
 in 
window 
,
    is the number of pixels in window 
,
    
is a parameter empirically set to 0.16, 
k
Σ is the 3 × 3 
covariance matrix of the guidance image 
IC
 in window 
,
  
and U  is the 3×3 identity matrix. 
 
2) Global detail extraction 
In this study, WLS optimization [15] is used to 
decompose each processed LDR image with gamma 
correction and extract global details. Using matrix notation, 
we have 
),
(
)
) (
(
G
y
y
T
y
T
G
G
x
x
T
x
T
G
C
G
T
C
G
D
D
D
D
I
A
I
I
A
I
I
I
I
I





 
(8) 
where 
x
A  and 
y
A  are two diagonal matrices containing the 
smoothness weights 
)
(
ax IC
 and 
),
(
ay IC
 respectively, and 
matrices 
x
D  and 
y
D  are two discrete differentiation operators. 
The vector 
G
I  minimizing (8) can be uniquely determined as 
the solution of 
)),
(
(
y
y
T
y
x
x
T
x
G
C
D
D
D
D
A
A
U
I
I




 
(9) 
where U  is the identity matrix. 
 
F. Weighting Map Estimation and Refinement 
For multiexposure image fusion, weighting map 
estimation is used to form the desired HDR image by 
keeping only the “best” regions (parts) in input LDR images. 
Weighting maps are determined by giving weights to the 
pixels of all LDR images. Here, two quality measures of 
spatial and cross-image consistency are used to estimate the 
weighting map of each processed LDR image with gamma 
correction, which is then refined by the cross bilateral filter. 
1) Weighting map estimation of spatial consistency 
In this study, four image quality measures of spatial 
consistency, namely, contrast, saturation, well-exposedness, 
and saliency, are used to estimate the weighting map of each 
processed LDR image with gamma correction. Three quality 
measures of spatial consistency, namely, contrast, saturation, 
and well-exposedness [1], are defined as 
( )) ,
(
)
contrast(
p
L I
p
W
n
C
n

 
(10) 
,
3
( )))
(
( )
(
)
(
, }
,
{
2
saturation



R G B
i
n
C
n
C
n
p
avg I
p
I
p
W
i
 
(11) 
),
2
)5.0
exp( (
)
(
2
2
exposed




n
C
n
I
p
W
 
(12) 
where p denotes a pixel in the n-th processed LDR image 
with gamma correction,   denotes the absolute value, 
L()
 
is a Laplacian filter with window size 3×3, 
avg()
 denotes 
the average of the RGB color components, and   is 
empirically set to 0.2. The fourth quality measure of spatial 
consistency is saliency [4]. First, Laplacian filtering is 
applied to each processed LDR image with gamma 
correction to obtain the corresponding high-pass image 
.
H n
 
Then, the local average of the absolute value of 
n
H  is used 
to construct the saliency map 
n
Wsaliency  of 
,
H n
 which is 
computed as 
( ),
( )
)
saliency (
p
G
p
H
p
W
g
n
n
 

 
(13) 
where 
( ),
( )
( )
L p
p
I
p
H
n
C
n


 
)
(
Gg
 is a Gaussian filter of 
size 11×11, the standard derivation 
g
  is empirically set to 
5, and   is the convolution operator. As a summery, the 
weighting map of spatial consistency is determined as 
( ).
( )
( )
( )
( )
p
W
p
W
p
W
p
W
p
W
n
saliency
n
exposed
n
saturation
n
contrast
n
spatial




 
(14) 
2) Weighting map estimation of cross-image consistency 
Zhang and Cham [14] found that the gradient directions 
of the pixels in well-exposed regions are stable in different 
exposures. Therefore, the weighting map of cross-image 
consistency can be estimated by measuring gradient direction 
changes between each processed LDR image with gamma 
correction and the reference intensity image. Here, the first 
derivatives of a 2-D Gaussian function in x and y directions 
are used to extract the gradient information as 
),
( , )
, )
(
( , )
, )
(
arctan(
, )
(
x y
x G
x y
I
x y
y G
x y
I
y
x
d
d
n
C
n
C
n










 
(15) 
where 
( , )
x y
I
n
C
 denotes pixel 
( , )
x y
 in the n-th processed 
LDR image with gamma correction and the standard 
derivation 
d
  is empirically set to 0.5. The weighting map of 
cross-image consistency is estimated as the 1-D Gaussian 
function 
of 
the 
difference 
between 
( , )
y
x
 n
 and 
that
( , )
y
x
 ref
 of the reference intensity image, i.e., 
),
2
( , ))
( , )
(
exp(
2
2
t
ref
n
n
temporal
x y
x y
W






 
(16) 
16
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

where 
t
 controls the influence of gradient direction 
changes and 
t
  is set adaptively to the exposure quality of 
the reference intensity image as 







otherwise,
 ,
,
( , )
1 ,
( , )





x y
I
y
x
R
t
 
(17) 
where the well-exposed range is 
, ]
1[
 
 with the image 
range normalized to 
]1 ,0
[
. Here,   controls the influence 
of gradient direction changes detected based on the well-
exposed pixels of the reference intensity image and 
( )
 
 
is used to reduce the influence of the detected gradient 
direction changes. In this study, the three parameters 
,
  
,
  
and  are empirically set to 0.02, 0.9, and 0.9, respectively. 
 
3) Weighting map refinement 
The initial weighting map directly estimated as 
, 
n
temporal
n
spatial
n
initial
W
W
W


 
(18) 
may be noisy. To cope with this problem, a cross bilateral 
filter based refinement [19-20] is employed so that 
neighboring pixels having similar intensities will have 
similar weight values, i.e., 
,
( ))
( )
(
)
(
( )
( ))
( )
(
)
(
( )











q
n
C
n
C
q
n
initial
n
C
n
C
n
final
q
I
p
I
q g
p
g
q
q W
I
p
I
q g
p
g
p
W
r
s
r
s




 
(19) 
where p is a pixel in the n-th weighting map,  is a 3×3 
sliding window centered at p, and q is a pixel in window 
.
  
Here, the standard derivations 
s
  and 
t
  are empirically set 
to 5 and 5, respectively. 
 
G. Image Fusion 
Based on the N weighting maps of the N processed LDR 
images with gamma correction, a composite image is 
generated by fusing N processed LDR images with gamma 
correction. Using the multiresolution spline based scheme 
[17] to achieve seamless image fusion, the final HDR image 
FI  is obtained by integrating the composite image and the 
extracted detail image 
ˆ ,...,ˆ , ˆ ,...,ˆ }
{
1
1
n
G
G
n
L
L
detail
I
I
I
I
I

 as 
)),
exp(max(
}
{
}
} {
{
1
1
detail
N
n
n
final
N
n
n
final
n
C
F
I
W
G
G W
I
L
I








 
(20) 
where 
L{}
 and 
G{}
 denote the Laplacian and Gaussian 
pyramids, respectively, and   is a small constant to avoid 
singularity. 
III. 
EXPERIMENTAL RESULTS 
In this study, the proposed approach is implemented 
using Matlab 7.10.0 (R2010a) on Intel Core i7-2700K CPU 
3.5GHz-Microsoft Windows 7 platform with 8GB main 
memory. To evaluate the effectiveness of the proposed 
approach, four comparison approaches are employed, where 
the source codes of Mertens et al.’s approach [1] and Shen et 
al.’s approach [3] are directly employed, whereas and Zhang 
and Cham’s approach [14] and Li et al.’s approach [10] are 
implemented in this study. Here, nineteen LDR image 
sequences with different numbers of LDR images are 
employed. 
In this study, four objective image quality measures, 
namely, the structural similarity (SSIM) index [21] , the 
saturation, the blind image quality index (BIQI) [22], and the 
naturalness image quality evaluator (NIQE) [23], are 
employed. In terms of the SSIM index, saturation index, 
BIQI, and NIQE, the performance comparisons between the 
four comparison approaches and the proposed approach for 
the nineteen LDR image sequences are listed in Tables I~IV, 
respectively. The average performances of the proposed 
approach are better than those of four comparison 
approaches. To perform subjective evaluation, subjective 
scores, i.e., 1 (worst) up to 10 (best), are collected from 
eighteen people. Here, the final images of multiexposure 
image fusion of each LDR image sequence are shown on an 
EIZO LCD color monitor (S2402W) periodically (three 
seconds per image) and each viewer gives his subjective 
scores for different final images of each LDR image 
sequence. The subjective performance comparisons between 
the four comparison approaches and the proposed approach 
for the nineteen LDR image sequences are shown in Table V.  
As two illustrated experimental results shown in Figs. 2 and 
3, the overall image quality of the final images of 
multiexposure image fusion of the proposed approach is 
better than those of the four comparison approaches. In Fig. 
2, more texture details of windows are persevered in the final 
image of the proposed approach, whereas in Fig. 3, the 
contrast of books in the final image of the proposed approach 
is better than those of the four comparison approaches. 
IV. 
CONCLUDING REMARKS 
In this study, a multiexposure image fusion approach using 
homomorphic filtering and detail enhancement is proposed. 
Based on the experimental results obtained in this study, 
several observations can be found. (1) Based on Tables I~IV, 
on the average, the objective performance measures, namely, 
SSIM, saturation, BIQI, and NIQE, of the proposed 
approach are better than those of the four comparison 
approaches. (2) Based on Table V, the subjective evaluation 
of the final HDR images of the proposed approach is better 
than those of the four comparison approaches. (3) Based on 
Figs. 2-3, the final HDR images of the proposed approach 
are indeed better than those of four comparison approaches. 
ACKNOWLEDGMENT 
The work was supported in part by National Science 
Council, Taiwan, Republic of China under Grants NSC 102-
2221-E-194-028-MY2 and NSC 102-2221-E-194-041-MY3. 
 
REFERENCES 
[1] T. Mertens, J. Kautz, and F. V. Reeth, “Exposure fusion: a 
simple and practical alternative to high dynamic range 
photography,” Computer Graphics Forum, vol. 28, no. 1, pp. 
161-171, 2009. 
17
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

[2] S. Li,. X. Kang, and J. Hu, “Image fusion with guided 
filtering,” IEEE Trans. on Image Processing, vol. 22, no. 7, pp. 
2864-2875, July 2013. 
[3] R. Shen, I. Cheng, J. Shi, and A. Basu, “Generalized random 
walks for fusion of multi-exposure images,” IEEE Trans. on 
Image Processing, vol. 20, no. 12, pp. 3634-3646, Dec. 2011. 
[4] P. E. Debevec and J. Malik, “Recovering high dynamic range 
radiance maps from photographs,” in Proc. of ACM 
SIGGRAPH, 1997, pp. 369-378. 
[5] J. Duanm, M. Bressan, and C. Dance, “Tone-mapping high 
dynamic range images by novel histogram adjustment,” 
Pattern Recognition, vol. 43, no. 5, pp. 1847-1862, May 2010. 
[6] T. H. Wang, C. W. Fang, M. C. Sung, and J. J. Lien, 
“Photography enhancement based on the fusion of tone and 
color mappings in adaptive local region,” IEEE Trans. on 
Image Processing, vol. 19, no. 12, pp. 3089-3105, Dec. 2010. 
[7] K. He, J. Sun, and X. Tang, “Guided image filtering,” in Proc. 
of European Conf. on Computer Vision, 2010, pp. 1-14. 
[8] T. Kartalov, Z. Ivanovski, and L. Panovski, “Full automated 
exposure fusion algorithm for mobile platforms,” in Proc. of 
IEEE Int. Conf. on Image Processing, 2011, pp. 361-364. 
[9] S. Raman and S. Chaudhuri, “Bilateral filter based 
compositing for variable exposure photography,” in Proc. of 
Eurographics, 2009, pp. 1-4. 
[10] Z. G. Li, J. H. Zheng, and S. Rahardja, “Detail-enhanced 
exposure fusion,” IEEE Trans. on Image Processing, vol. 21, 
no. 7, pp. 1-6, July 2012. 
[11] M. Song, D. Tao, C. Chen, J. Bu, J. Luo, and C. Zhang, 
“Probabilistic exposure fusion,” IEEE Trans. on Image 
Processing, vol. 21, no. 1, pp. 341-357, Jan. 2012. 
[12] B. Gu, W. Li, J. Wong, M. Zhu, and M. Wang, “Gradient 
field multi-exposure images fusion for high dynamic range 
image visualization,” Journal of Visual Communication and 
Image Representation, vol. 23, no. 4, pp. 604-610, May 2012. 
[13] W. Zhang and W. K. Cham, “Gradient-directed multiexposure 
composition,” IEEE Trans. on Image Processing, vol. 21, no. 
4, pp. 2318-2323, April 2012. 
[14] W. Zhang and W. K. Cham, “Reference-guided exposure 
fusion in dynamic scenes,” Journal of Visual Communication 
and Image Representation, vol. 23, no. 3, pp. 467-475, April 
2012. 
[15] F. Zeev, F. Raanan, L. Dani, and S. Richard, “Edge-
preserving decompositions for multi-scale tone and detail 
manipulation,” ACM Trans. on Graphics, vol. 27, no. 3, pp. 1-
10, 2008. 
[16] J. Tumblin, J. K. Hodgins, and B. K. Guenter, “Two methods 
for display of high contrast images,” ACM Trans. on 
Graphics, vol. 18, no. 1, pp. 56-94, 1999. 
[17] P. J. Burt and E. H. Adelson, “A multiresolution spline with 
application to image mosaics,” ACM Trans. on Graphics, vol. 
2, no. 2, pp. 217-236, 1983. 
[18] F. Durand and J. Dorsey, “Fast bilateral filtering for the 
display of high-dynamic-range images,” ACM Trans. on 
Graphics (Proc. of ACM SIGGRAPH 2002), vol. 21, no. 3, pp. 
257-266, July 2002. 
[19] E. Eisemann and F. Durand, “Flash photography enhancement 
via intrinisic relighting,” ACM Trans. on Graphics, vol. 23, 
no. 1, pp. 673-678, 2004. 
[20] G. Petschnigg, R. Szeliski, M. Agrawala, M. Cohen, H. 
Hoppe, and K. Toyama, “Digital photography with flash and 
no-flash image pairs,” ACM Trans. on Graphics, vol. 23, no. 
1, pp. 664-672, 2004. 
[21] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, 
“Image qulaity assessment: from error visibility to structural 
similarity,” IEEE Trans. on Image Processing, vol. 13, no. 4, 
pp. 600-612, April 2004. 
[22] A. K. Moorthy and A. C. Bovik, “A two-step framework for 
constructing blind image quality indices,” IEEE Signal 
Processing Letters, vol. 17, no. 5, pp. 513-516, May 2010. 
[23] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a 
completely blind image quality analyzer,” IEEE Signal 
Processing Letters, vol. 22, no. 3, pp. 209-212, March 2013. 
 
 
(a)                    (b)                    (c)                     (d)                    (e) 
Figure 2.  The final images of multiexposure image fusion of the “Church” 
LDR image sequence: (a) Mertens et al. [1]; (b) Shen et al. [3]; (c) Zhang 
and Cham [14]; (d) Li et al. [10]; (e) proposed. 
 
(a)                                     (b)                                     (c) 
 
 (d)                                       (e) 
Figure 3.  The final images of multiexposure image fusion of the “Desk 
Lamp1” LDR image sequence: (a) Mertens et al. [1]; (b) Shen et al. [3]; (c) 
Zhang and Cham [14]; (d) Li et al. [10]; (e) proposed. 
TABLE I.  
IN TERMS OF SSIM, PERFORMANCE COMPARISONS 
BETWEEN THE FOUR COMPARISON APPROACHES AND THE PROPOSED 
APPROACH FOR THE NINETEEN LDR IMAGE SEQUENCES 
LDR image 
sequences 
Mertens 
et al. [1] 
Shen 
et al. 
[3] 
Zhang 
and 
Cham 
[14] 
Li  
et al. 
[10] 
Proposed 
Aloe 
68.2% 
70.0% 
71.0% 
61.2% 
89.7% 
Ardeshir 
76.5% 
78.5% 
77.6% 
75.1% 
84.3% 
Belgium 
40.6% 
47.6% 
46.6% 
44.6% 
63.4% 
Bridge 
80.1% 
82.6% 
77.9% 
79.2% 
86.4% 
Church 
70.3% 
70.7% 
70.9% 
63.6% 
67.7% 
Desk Lamp1 
80.8% 
81.6% 
80.5% 
76.0% 
79.6% 
Desk Lamp2 
77.6% 
79.0% 
72.5% 
72.8% 
75.1% 
Flower8 
63.5% 
64.7% 
63.5% 
60.0% 
72.8% 
GrandCanal 
62.2% 
64.2% 
61.5% 
59.3% 
73.6% 
Hall 
80.9% 
81.7% 
80.8% 
79.4% 
81.6% 
HDRLab3 
67.4% 
67.2% 
66.8% 
66.5% 
80.4% 
House 
42.1% 
43.2% 
41.9% 
40.4% 
51.1% 
Kitchen 
68.4% 
71.2% 
68.2% 
64.5% 
76.8% 
Landscape 
75.8% 
76.4% 
72.0% 
74.0% 
85.4% 
Lighthouse 
71.1% 
72.8% 
70.9% 
68.0% 
80.5% 
Mountain 
80.5% 
81.9% 
78.7% 
76.1% 
78.7% 
Sofa 
61.2% 
61.4% 
62.1% 
57.5% 
64.1% 
Tree 
70.1% 
70.0% 
70.4% 
67.0% 
65.0% 
Wall 
59.9% 
61.1% 
59.3% 
59.4% 
67.4% 
Average 
68.6% 
69.8% 
68.1% 
65.5% 
74.9% 
 
 
18
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

TABLE II.  
IN TERMS OF SATURATION, PERFORMANCE 
COMPARISONS BETWEEN THE FOUR COMPARISON APPROACHES AND THE 
PROPOSED APPROACH FOR THE NINETEEN LDR IMAGE SEQUENCES 
LDR image 
sequences 
Mertens 
et al. [1] 
Shen 
et al. 
[3] 
Zhang 
and 
Cham 
[14] 
Li  
et al. 
[10] 
Proposed 
Aloe 
0.33 
0.31 
0.38 
0.33 
0.45 
Ardeshir 
0.38 
0.32 
0.40 
0.38 
0.40 
Belgium 
0.30 
0.24 
0.33 
0.30 
0.32 
Bridge 
0.13 
0.12 
0.14 
0.13 
0.17 
Church 
0.64 
0.60 
0.65 
0.64 
0.69 
Desk Lamp1 
0.39 
0.38 
0.39 
0.39 
0.44 
Desk Lamp2 
0.28 
0.26 
0.24 
0.28 
0.31 
Flower8 
0.28 
0.24 
0.27 
0.28 
0.32 
GrandCanal 
0.22 
0.14 
0.22 
0.22 
0.25 
Hall 
0.30 
0.26 
0.31 
0.29 
0.33 
HDRLab3 
0.27 
0.24 
0.33 
0.27 
0.32 
House 
0.33 
0.25 
0.35 
0.33 
0.36 
Kitchen 
0.46 
0.39 
0.47 
0.46 
0.53 
Landscape 
0.18 
0.17 
0.16 
0.18 
0.22 
Lighthouse 
0.39 
0.34 
0.41 
0.39 
0.37 
Mountain 
0.15 
0.12 
0.15 
0.15 
0.18 
Sofa 
0.82 
0.78 
0.87 
0.81 
0.87 
Tree 
0.15 
0.13 
0.15 
0.15 
0.18 
Wall 
0.20 
0.11 
0.19 
0.20 
0.17 
Average 
0.33 
0.28 
0.34 
0.32 
0.36 
 
 
 
 
 
 
 
 
TABLE III.  
IN TERMS OF BIQI, PERFORMANCE COMPARISONS 
BETWEEN THE FOUR COMPARISON APPROACHES AND THE PROPOSED 
APPROACH FOR THE NINETEEN LDR IMAGE SEQUENCES 
LDR image 
sequences 
Mertens 
et al. [1] 
Shen 
et al. 
[3] 
Zhang 
and 
Cham 
[14] 
Li  
et al. 
[10] 
Proposed 
Aloe 
57.65 
69.12 
62.07 
39.78 
32.16 
Ardeshir 
24.31 
29.39 
21.71 
27.41 
23.47 
Belgium 
18.17 
28.97 
18.14 
12.50 
17.97 
Bridge 
31.33 
32.64 
31.53 
26.94 
23.14 
Church 
26.27 
31.60 
29.49 
21.36 
19.59 
Desk Lamp1 
21.97 
27.22 
27.23 
16.58 
17.33 
Desk Lamp2 
24.45 
27.33 
33.38 
16.41 
17.57 
Flower8 
29.51 
33.79 
29.76 
25.07 
20.49 
GrandCanal 
24.47 
29.27 
24.20 
31.19 
23.99 
Hall 
18.09 
29.80 
18.45 
26.21 
27.39 
HDRLab3 
29.90 
30.66 
31.22 
25.26 
30.84 
House 
27.54 
31.66 
27.98 
32.78 
27.80 
Kitchen 
26.65 
31.42 
27.38 
22.80 
21.22 
Landscape 
26.40 
25.32 
25.59 
26.74 
27.66 
Lighthouse 
11.04 
23.64 
11.36 
13.57 
11.98 
Mountain 
36.20 
41.07 
36.66 
23.77 
17.10 
Sofa 
38.55 
39.79 
32.61 
41.20 
39.97 
Tree 
26.06 
27.00 
27.91 
14.06 
13.65 
Wall 
23.68 
26.71 
24.30 
24.49 
22.26 
Average 
27.48 
32.44 
28.47 
24.64 
22.92 
 
 
 
TABLE IV.  
IN TERMS OF NIQE, PERFORMANCE COMPARISONS 
BETWEEN THE FOUR COMPARISON APPROACHES AND THE PROPOSED 
APPROACH FOR THE NINETEEN LDR IMAGE SEQUENCES 
LDR image 
sequences 
Mertens 
et al. [1] 
Shen 
et al. 
[3] 
Zhang 
and 
Cham 
[14] 
Li  
et al. 
[10] 
Proposed 
Aloe 
2.99 
2.57 
2.88 
2.77 
2.83 
Ardeshir 
3.86 
3.18 
3.38 
3.80 
3.38 
Belgium 
2.38 
2.45 
2.24 
2.19 
2.07 
Bridge 
2.42 
2.42 
2.36 
2.20 
2.13 
Church 
2.09 
1.90 
1.77 
1.98 
1.86 
Desk Lamp1 
2.61 
2.42 
2.53 
2.27 
2.25 
Desk Lamp2 
2.70 
2.54 
2.47 
2.34 
2.21 
Flower8 
1.87 
2.02 
1.86 
1.65 
1.66 
GrandCanal 
2.33 
2.27 
2.27 
2.16 
2.08 
Hall 
2.49 
2.39 
2.41 
2.39 
2.31 
HDRLab3 
2.98 
3.21 
2.98 
2.57 
2.74 
House 
2.52 
2.52 
2.53 
2.33 
2.41 
Kitchen 
3.08 
3.07 
2.74 
2.86 
2.65 
Landscape 
3.06 
2.15 
3.01 
2.79 
2.63 
Lighthouse 
3.47 
2.89 
3.44 
3.09 
3.34 
Mountain 
2.07 
2.14 
2.17 
2.03 
2.06 
Sofa 
3.29 
3.13 
3.11 
3.18 
3.05 
Tree 
1.93 
1.85 
1.87 
1.79 
1.75 
Wall 
2.55 
2.40 
2.15 
2.55 
2.29 
Average 
2.67 
2.50 
2.54 
2.47 
2.40 
 
 
TABLE V.  
IN TERMS OF SUBJECTIVE EVALUATION, PERFORMANCE 
COMPARISONS BETWEEN THE FOUR COMPARISON APPROACHES AND THE 
PROPOSED APPROACH FOR THE NINETEEN LDR IMAGE SEQUENCES 
LDR image 
sequences 
Mertens 
et al. [1] 
Shen 
et al. 
[3] 
Zhang 
and 
Cham 
[14] 
Li  
et al. 
[10] 
Proposed 
Aloe 
5.94 
4.72 
5.50 
7.67 
8.94 
Ardeshir 
8.00 
5.67 
5.56 
7.33 
6.78 
Belgium 
7.11 
5.11 
6.89 
7.22 
6.83 
Bridge 
4.94 
6.28 
5.78 
6.56 
8.89 
Church 
6.72 
5.33 
7.11 
6.33 
8.67 
Desk Lamp1 
6.44 
5.67 
5.61 
7.11 
9.11 
Desk Lamp2 
6.89 
5.83 
3.44 
7.28 
8.22 
Flower8 
6.17 
5.44 
6.00 
7.11 
8.64 
GrandCanal 
7.33 
5.61 
7.50 
8.33 
6.17 
Hall 
7.06 
6.11 
7.06 
7.61 
8.14 
HDRLab3 
6.17 
5.22 
7.28 
6.50 
7.89 
House 
8.06 
5.28 
7.28 
8.33 
8.06 
Kitchen 
7.67 
5.28 
6.22 
7.67 
8.78 
Landscape 
7.28 
6.06 
6.50 
7.89 
7.56 
Lighthouse 
7.50 
5.56 
7.22 
8.78 
7.00 
Mountain 
6.83 
5.78 
6.67 
8.33 
8.33 
Sofa 
7.50 
5.78 
6.06 
7.17 
8.33 
Tree 
5.56 
5.61 
6.22 
7.11 
8.89 
Wall 
6.94 
6.67 
6.39 
7.22 
8.61 
Average 
6.85 
5.63 
6.33 
7.45 
8.10 
 
 
 
19
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-320-9
MMEDIA 2014 : The Sixth International Conferences on Advances in Multimedia

