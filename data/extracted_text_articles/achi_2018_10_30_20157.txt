Detection and Classification of RBCs and WBCs in Urine Analysis
with Deep Network
Xingguo Zhang, Guoyue Chen, Kazuki Saruta and Yuki Terata
Faculty of Systems Science and Technology
Akita Prefectural University
Akita, Japan
e-mail: {xingguozhang, chen, saruta, terata}@akita-pu.ac.jp
Abstract—Urinary sediment examination is used to evaluate
the possible urinary tract diseases of patients. Currently,
numerous approaches are applied to automatically detect Red
Blood Cells (RBCs) and White Blood Cells (WBCs) from
urinary sediment images. However, it is still a challenging task
due to the cellular heterogeneity. Deep learning approaches
have been shown to produce encouraging results on image
detection in various tasks. In this paper, we investigate issues
involving Faster Regions with Convolutional Neural Network
(Faster R-CNN) for the construction of an end-to-end urine
analysis system. We propose an effective baseline for RBCs
and WBCs detection on urinary sediment images by using a
pre-train Faster R-CNN model. We evaluate our urine analysis
system on a large dataset of urinary sediment images which
consist of more than 6,000 annotated RBCs and WBCs images.
Our results show competitive accuracy and acceptable run
time. Prospectively, the proposed methods could provide
support to pathology practice in terms of quantitative analysis
of tissue constituents in whole-slide images, and it could
potentially lead to a better understanding of urinary tract
diseases. Code and dataset will be made publicly available.
Keywords- Urinary Sediment; RBC; WBC; Faster R-CNN;
Applications in Medicine.
I.
INTRODUCTION
Currently,
urinary
sediment
microscopy
plays
an
important role in the clinical diagnosis of urinary tract
diseases [1][2]. It is possible to diagnose a patient's disease
by identifying the type and amount of sediment in the urine
specimen which is effective on early detection and disease
control. The development of the Automatic Urinalysis
System has been attractive to scholars over the past few
years.
The higher clinical value of sediment in the urine mainly
includes RBCs and WBCs. The qualitative and quantitative
analysis of RBCs and WBCs in urine sediment can not only
be helpful in detecting the disease but also help to explore
various options for urethral diseases treatment.
Several solutions for urinary sediment microscopy using
computer vision methodology have been proposed by
different studies [3][4]. However, it is still a challenging task
due to the cellular heterogeneity.
In 2005, nephrologists began to report concerns about the
frequent discrepancies in some reported results of urine
sediment microscopy [5][6]. For instance, the diagnostic
accuracy rates of acute kidney injury based on reports by
medical technologists and nephrologists were <25% and 69–
92%, respectively [5]. These differences could be caused by
the number of dysmorphic red blood cells identified, which
tend to be underestimated by image processing technologists.
In addition, dysmorphic RBCs were often misclassified as
WBCs, besides, the reports issued by medical laboratory
technologists overlooked the description of dysmorphic
RBCs.
One
way
to
explore
these
cell
types
is
to
use
morphological clues in local neighborhoods to develop
automated cellular recognition via image analysis, which can
then be deployed for sophisticated tissue morphometry.
There are several factors that impede automatic detection
and classification of RBCs and WBCs. On one hand, the
inferior image quality may arise due to autofocus failure
during the digitization of slide. On the other hand, complex
tissue architecture, clutter of nuclei, and diversity of nuclear
morphology pose a challenging problem.
Particularly, in case of the large numbers of dysmorphic
RBCs appearing at urinary sediment, the detection accuracy
is significantly declined, due to the dysmorphic RBCs is very
similar to the WBCs in terms of cellular morphology (Figure
1). Dysmorphic RBCs are usually transformed by the red
blood cells being squeezed through the walls of the blood
vessels, causing the damage of cell wall. Furthermore, such
cases are not rare. Therefore, we need an effective approach
to distinguish the variety of objects from urine sediment,
especially isomorphic RBCs, dysmorphic RBCs and WBCs.
Figure 1. Isomorphic red blood cells (RBCs), dysmorphic RBCs
and write blood cells (WBCs).
194
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

Figure 2. Our urinary sediment detection pipeline.
Faster R-CNN [14] is a particularly successful method
for general object detection. It consists of two components:
region proposal and estimate object candidates, which is
generally considered to be more effective for smaller objects
detection. However, there are few studies on the use of
Faster R-CNN for urinary sediment detection.
In this paper, we would investigate issues involved Faster
R-CNN for construction of end-to-end urine analysis system.
In addition, we would propose an effective baseline for
RBCs and WBCs detection on urinary sediment images by
using a pre-train Faster R-CNN model. In here, we expect
both isomorphic RBC and dysmorphic RBC to be detected
as RBC.
The rest of the paper is organized as follows: Section 2
introduces the related works. Section 3 presents the pipeline
that we used for detecting RBCs and WBCs. Section 4 is
devoted to experimental evaluation, whereas conclusions are
drawn in Section 5.
II.
RELATED WORK
Cell and nucleus classification have been applied to
diverse histopathology related applications. Most existing
methods for cells detection share similar computation
pipelines:
thresholding
followed
by
morphological
operations, region growing, level sets, k-means, and graph-
cuts.
Cosatto et al. [7] proposed the detection of cell nuclei
using the difference of Gaussian (DoG) followed by Hough
transform to find radially symmetrical shapes. Vink et al. [8]
employed AdaBoost classifier to train two detectors, one
used pixel-based features and the other merged the results of
two detectors to detect the nuclei in immunohistochemistry
stained breast tissue images on the base of Haar-like features.
Dalle et al. [9] and Cosatto et al. [7] used shape, texture and
size of nuclei for nuclear pleomorphism grading in breast
cancer images.
Recently,
the
prevalent
success
of
deep
learning
approach
in
computer
vision,
such
as
Regions
with
Convolutional
Neural
Network (R-CNN) [10], Region
Proposal Network & Binary Forest (RPN_BF) [11] and
Spatially Constrained Convolutional Neural Network (SC-
CNN) [12] have shown good performance on a large number
of histopathological image datasets. The R-CNN method
[10] trains CNNs end-to-end to classify the proposal regions
into object categories or background. Fast R-CNN [13]
enables end-to-end detector training on shared convolutional
features and shows compelling accuracy and speed. In [11],
an RPN_BF approach has been proposed an RPN that
generates candidate boxes, convolutional feature maps, and a
Boosted Forest that classifies these proposals using these
convolutional features. Korsuk et al. [12] proposed a SC-
CNN classifier to detect colon cancer cells.
III.
CONVOLUTIONAL NEURAL NETWORK
CNN is one of the basic theories for deep learning,
therefore we briefly recap such network.
A CNN ݂ is a composition of a sequence of L functions
or layers (݂ଵ, … , ݂௅) that maps an input vector x to an output
vector y, i.e.,

ܻ = ݂(ݔ;ݓଵ, … ,ݓ௅)

= ݂௅(∙ ; ݓ௅)∘݂௅ିଵ(∙ ; ݓ௅ିଵ) ∘ …
∘ ݂ଶ(∙ ; ݓଶ) ∘ ݂ଵ(∙ ; ݓଵ)
whereݓ௅ is the weight and bias vector for the l the layer fl.
Conventionally, ݂௅ is defined to perform one of the
following operations: a) convolution with a bank of filters;
b) spatial pooling; and c) non-linear activation. Given a set
of N training data {(ݔ௜,ݕ௜)}௜ୀଵ
ே , we can estimate the vectors
ݓଵ, … ,ݓ௅ by solving the optimization problem

argmin௪భ,…,௪ಽ
ଵ
ே ∑
݈(݂(ݔ௜;ݓଵ, … ,ݓ௅),ݕ௜)
ே
௜ୀଵ


where l is an appropriately defined loss function. Numerical
optimization of (2) is often performed via backpropagation
and stochastic gradient descent methods.
IV.
RBC AND WBC DETECTION BASED ON FASTER R-CNN
In this section, we describe our RBCs and WBCs
detection pipeline for urinary sediment images by Faster R-
CNN [14].
Faster R-CNN consists of two components, as shown in
Figure 2: an RPN that generates candidate boxes as well as
convolutional feature maps, and a Fast R-CNN used for
object detection. RPN is used to compute candidate
bounding boxes, scores, and convolutional feature maps.
The candidate boxes are fed into Fast R-CNN for further
classification,
using
the
features
pooled
from
the
convolutional feature maps computed by RPN. Finally, non-
maximum suppression (NMS) is used to merge the similar
results and get the output. In here, we use a urinary sediment
image dataset to train the RPN and Fast R-CNN network.
195
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

A. RPN network
The
RPN
network
shares
full-image
convolutional
features with the detection network, thus enabling nearly
cost-free region proposals. An RPN is a fully convolutional
network that simultaneously predicts object bounds and
objectness scores at each position. The RPN is trained end-
to-end to generate high-quality region proposals, which are
used by Fast R-CNN for detection.
We fixed the aspect ratio of anchors (Region Proposal
Boxes) [14] as 1:1 (width / height). This is because that we
need a square box to mark the positions of the cells. This is
unlike the original RPN [14] for detect general object that
has anchors of multiple aspect ratios. In order to detect
multi-scale RBCs and WBCs, we use anchors of 3 different
scales, starting from 20 pixels length of square box side with
a scaling stride = 1.2.
Following [14], we adopt the VGG-16 net [15] pre-
trained on the ImageNet dataset [16] to initialize the network
parameters. The RPN is built on top of the Conv5_3 layer,
which is followed by an intermediate 3×3 convolutional
layer
and
two
sibling
1×1
convolutional
layers
for
classification and bounding box regression (more details in
[14]). The output layer of the RPN net provides confidence
scores and regression coordinate of the predicted boxes,
which can be used as the input for Fast R-CNN network.
B. Fast R-CNN network
For the detection process, we adopt Fast R-CNN
network as mentioned at [14]. To speed up the process, [14]
developed a technique that allows for sharing convolutional
layers between the two networks, rather than learning two
separate networks. For the convenience of the reader, we
briefly recap such approach.
A 4-step training algorithm has been adopted to learn
shared features via alternating optimization. In the 1st step,
the RPN network has been train. And this network is
initialized with an ImageNet pre-trained model and fine-
tuned end-to-end for the region proposal task. In the 2nd
step, they train a separate detection network by Fast R-CNN
using the proposals generated by the 1st step. In the 3rd
step, they use the detector network to initialize RPN
training, but fix the shared convolutional layers and only
fine-tune the layers unique to RPN. Finally, keeping the
shared convolutional layers fixed, and fine-tune the unique
layers of Fast R-CNN. As such, both networks share the
same convolutional layers and form the Faster R-CNN
network. In this solution, the RPN and Fast R-CNN
networks are merged into one network during training.
C. Implementation Details
For RPN and Fast R-CNN training, an anchor is
considered as a positive example if it has an Intersection-
over-Union (IoU) ratio greater than 0.7 with one ground truth
box, and otherwise consider as negative. For Fast R-CNN
training, we construct the training set by selecting the top-
ranked 100 proposals of each image by RPN network. At test
process, we only use the top 300 proposals in an image,
which are classified by the Fast R-CNN. We adopt NMS to
output the detect results.
V.
EXPERIMENT AND RESULTS
In this section, we describe our experimental setup details
and the results.
A. The dataset
This study involves electron microscope image. We
collected urine samples from 100 patients, and through urine
centrifuge to obtain urine sediment. It was then magnified
by an electron microscope 400 times and photographed with
a digital camera. All images have a common size of 1280 ×
1024 pixels.
Manual annotations of nuclei were conducted mostly by
an experienced pathologist and partly by a graduate student
under supervision of and validation by the same pathologist.
A total number of 5,215 RBCs and 4,828 WBCs were
marked. Figure 3 shows some examples of the urinary
sediment images in the dataset. We selected 3,000 random
images as the training samples. The test samples also
included 2,000 images selected from the rest of the dataset.
B. Evaluation metrics
The objective of this experiment is to detect all RBCs
and WBCs in an image by locating their positions, and
obtain their class labels. In particular, the performance of an
algorithm is evaluated in terms of the tradeoff between
precision, Recall, and F1 score.
First, a detected bounding box and a ground truth
bounding box (GT) are considered a true positive (TP) if the
area covered by their intersection ≥ 70%. A GT that does
not have a match is considered a False Negative (FN), or a
Miss. A detected bounding box that does not have a
matching GT is considered as a False Positive (FP). The F1
score (also F-score or F-measure) is a measure of a test's
accuracy. It considers both the precision p and the recall r of
the test to compute the score: p is the number of correct
positive results divided by the number of all positive results,
and r is the number of correct positive results divided by the
number of positive results that should have been returned.
Here, we use F1 score to quantitatively assess the detection
performance. The F1 score is computed by the following
equation:
ܨଵ = 2 ∙
௣௥௘௖௜௦௜௢௡ ∙ ௥௘௖௔௟௟
௣௥௘௖௜௦௜௢௡ ା ௥௘௖௔௟௟


Figure 3. Example images of urinary sediment images dataset used in this
experiment.
196
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions


(a)
(b)
(c)
Figure 4. Qualitative results for RBCs and WBCs detection in urine images. (a) Ground truth image. (b) Detection results of our approach. (c) Detection
results of RPN_BF.

C. Comparative Results
In this section, our final detectors were evaluated with
other state-of-the-art methods using our urinary sediment
dataset.
Figure 4 and Table 1 report the performance on
detection and classification for our dataset. Figure 4 are
qualitative results for RBCs and WBCs detection in urine
images. (a) Ground truth image. RBCs are shown as red
dashed Box, WBCs are shown as green dashed box. (b)
Detection results of our approach. (c) Detection results of
RPN_BF. Here, detected as RBCs are shown as red box,
and detected as WBCs are shown as green box. It can be
seen that our detector show better performance.
TABLE I.
COMPARATIVE RESULT FOR RBCS AND WBCS DETECTION
Method
Weighted Average F1 score
Our
0.914
RPN_BF
0.862
HOG
0.688
From the results of the images it can be seen that our
detector is competitive in terms of the detection quality with
respect to RPN_BF and provides significant improvement
over HOG+SVM.
In addition, we have observed the shapes of isomorphic
RBCs can change in response to the osmolarity of urine.
The isomorphic RBCs swell to spheres in urine with a low
specific gravity, and they shrink to the shape of a spiked
disk or a spiked sphere in urine with a high specific gravity.
The presence of dysmorphic RBCs leads to the difficulty
of distinguishing, which may seriously affects the accuracy
of the urinary sediment microscopy system. Therefore, a
precise count of dysmorphic RBCs is very important for
evaluating glomerular bleeding before renal biopsy.
We profiled the execution of our system on a desktop
architecture which features a 2.4GHz Intel i7 CPU, a
NVIDIA GTX1080 GPU and 32GB of RAM. The system
requires, on average, 96ms to process a frame at a resolution
of 1280×1080 pixels. It can be consider as an acceptable
running time.
VI.
CONCLUSION
In this paper, we investigate issues involving Faster R-
CNN for construction of end-to-end urine analysis system.
We proposed an effective baseline for RBCs and WBCs
detection on urinary sediment images, using a pre-train
Faster R-CNN model. Isomorphic, dysmorphic RBCs and
WBCs were successfully identified. We comprehensively
evaluate this method, the experiment results presenting
competitive accuracy and acceptable speed.
Prospectively, the proposed methods could benefit the
pathology practice in terms of quantitative analysis of tissue
constituents in whole-slide images, and could potentially
lead to a better understanding of urinary tract diseases.
However, this study only focused on type 2 cells in
urinary sediment, our current results will require additional
studies using a wider spectrum of cells and sediment, for
examples, Epithelial Cells, Bacteria, Yeast and Parasites. In
future work, more theoretical and experimental studies will
be conducted to analyze the performance.
REFERENCES
[1]
M. A. Perazella, “The Urine Sediment as a Biomarker of Kidney
Disease,” American Journal of Kidney Diseases, vol. 66, pp. 748–
755, 2015.
[2]
J. A. Simerville, W. C. Maxted, and J. J. Pahira, “Urinalysis: a
comprehensive review,” American family physician, vol. 71, pp.
1153–1162, 2005.
[3]
M. Yasuda, “Japanese guideline for clinical research of antimicrobial
agents
on
urogenital
infections,”
Journal
of
Infection
and
Chemotherapy, vol. 17, pp. 579–594, 2011.
[4]
R.
Davis,
et
al.
“Diagnosis,
evaluation
and
follow-up
of
asymptomatic
microhematuria
(AMH)
in
adults,”
,Journal
of
Urology, vol, 188, pp. 2473–2481, 2012.
[5]
J. J. Tsai, J. Y. Yeun, V. A. Kumar, and B. R. Don, “Comparison and
interpretation of urinalysis performed by a nephrologist versus a
hospital-based clinical laboratory,” American journal of kidney
diseases, vol, 46, pp. 820–829, 2005.
[6]
M. Kanbay, B. Kasapoglu, and M. A. Perazella, “Acute tubular
necrosis and prerenal acute kidney injury: utility of urine microscopy
in their evaluation a systematic review,” Int. Urol. Nephrol., vol. 42,
pp. 425–433, 2010.
197
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

[7]
E. Cosatto, M. Miller, H. P. Graf, and J. S. Meyer, “Grading nuclear
pleomorphism on histological micrographs,” Int. Conf. Pattern
Recognition, pp. 1–4, 2008.
[8]
J. P. Vink, M. Van Leeuwen, C. Van Deurzen, and G. De Haan,
“Efficient nucleus detector in histopathology images,” Journal of
microscopy, vol.249, no. 2, pp. 124–135, 2013.
[9]
J. R. Dalle, H. Li, C. H. Huang, and W. K. Leow, “Nuclear
Pleomorphism Scoring by Selective Cell Nuclei Detection,” IEEE
Winter Conf. on Applications of Computer Vision, pp. 1–6, 2009.
[10] R. Girshick, J. Donahue, T. Darrell, U. C. Berkeley, J. Malik, “R-
CNN: Region-based Convolutional Neural Networks,” Computer
Vision and Pattern Recognition, pp. 2–9, 2014.
[11] L. Zhang, L. Lin, X. Liang, K. He, “Is Faster R-CNN Doing Well for
Pedestrian Detection?” European Conference on Computer Vision,
pp. 1–15, 2016.
[12] K. Sirinukunwattana, S.E.A. Raza, Y.W Tsang, I.A. Cree, D.R.J.
Snead, N.M. Rajpoot,” Locality Sensitive Deep
Learning for
Detection and Classification of Nuclei in Routine Colon Cancer
Histology Images,” IEEE Transactions on Medical Imaging, pp.
1196–1206, 2016.
[13] R.
Girshick,
“Fast
R-CNN:
Fast
Region-based
Convolutional
Networks for object detection,” IEEE International Conference on
Computer Vision, pp. 1440–1448, 2016.
[14] S. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: Towards Real-
Time Object Detection with Region Proposal Networks,” Advances
in neural information processing systems, pp. 91–99, 2015.
[15] L. C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, A. L. Yuille,
“Semantic Image Segmentation with Deep Convolutional Nets and
Fully Connected CRFs,” International Conference on Learning
Representations , pp. 1–14, 2016.
[16] O. Russakovsky, et al. “ImageNet Large Scale Visual Recognition
Challenge,” International Journal of Computer Vision, vol. 115, pp.
211–252, 2015.
198
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

