An Approach to Controlled Crowd-sourced Activities 
Maria E. Orłowska, Adam Wierzbicki 
Polish-Japanese Institute of Information Technology 
Warsaw, Poland 
email: {omaria,adamw}@pjwstk.edu.pl 
Abstract - This paper is concerned with defining contribu-
tors’ selection for crowd-sourced task execution. Based on 
reputation measures that we introduce, a general mathemati-
cal model of controlled crowd-sourcing is presented. The mod-
el offers easy to manage, flexible selection of predefined trust-
worthy contributors based on their prior performance in simi-
lar activities. A simple choice of model parameters within 
specified range to suit user’s intended quality of the crowd 
involvement is introduced. The abstraction of the problem that 
we present can be tailored for applications in different do-
mains and crowd-sourced activities. Further extensions of 
presented concepts conclude the paper. 
Keywords - crowd-sourcing; reputation; credibility 
I.  INTRODUCTION 
Many activities normally performed by employees of a 
company or social organization need some form of assis-
tance from the outside. Since the concept of cooperating 
parties has became commonly acceptable and effective way 
to achieve business goals, the term of outsourcing as 
the contracting out of a business process or function to a 
third-party became a strategy in many domains of business.  
It is important to note that the concept of outsourcing is 
also used to exemplify the practice of delegating fragments 
of the overall activity on ad hoc bases to the third party 
without any obligation of persistence of such association. 
Outsourcing is not limited to a single country; it includes 
both foreign and domestic contracting, and recently often 
includes relocation of a business function to another coun-
try, but in all such cases the main player is aware of the 
subcontracting party, its competences and associated costs. 
In outsourcing situation, the relationship is covered by the 
formal agreements. The economical considerations are often 
the driving force for such business strategy but not always. 
The concept of crowd-sourcing is one but significant 
step further [15]. It is the process of obtaining required ser-
vices, ideas, or content by soliciting contributions from a 
large group of unidentified people, and especially from 
an online community, rather than from convention-
al employees or suppliers. It combines the efforts of many 
self-identified volunteers or part-time personnel, where each 
contributor of their own initiative adds a portion to the final 
result. Let us note that often many contributors perform the 
same task not knowing about each other. So, the final selec-
tion of acceptable results is additional function of the owner 
of such out contracted process. A most natural way to dis-
tinguish crowd-sourcing from outsourcing is fact that the 
completion of individual task comes from an undefined 
public rather than being made to order by a specific, named 
and bounded by initial agreements group. Understanding of 
associated issues, especially in relation to evaluation of the 
quality of the work completed is essential for the real appli-
cations leading to genuine business benefits [3][7][9][16]. 
      The paper is structured as follows. In Section II only 
relevant related work is presented, followed by discussion 
on general objects evaluations in Section III in order to 
build a perspective on the evaluation process as discussed in 
Section IV. The main contribution of this work is described 
in Section V where the formalization of the crowd-sourcing 
quality involvement is introduced. Finally, in the last Sec-
tion VI we indicate directions of future research work in 
this area. 
 
II. RELATED WORK 
The most frequently used example of a crowd-sourced 
work is Wikipedia. Another, but different in nature exam-
ples, can be associated with the Internet content evaluation 
or extensive testing of publically accessible e-service func-
tionality or a design task.  
Let us briefly discuss some related issues that occupy 
researchers recently. They are web content evaluation in 
general, its integrity, credibility and trustworthiness.   
The Internet became the first source of information for 
many users regardless of the investigated topic. On the oth-
er hand information presented on websites either of compa-
nies or private authors, social networks and social portals 
frequently doesn’t have any structured evaluation. Thus the 
credibility of content of web pages is an important issue for 
all the users and could serve here as a motivation example 
of work presented in this paper. However, the features of 
presented model are not limited to this application. 
Perhaps, a precise definition of the credibility may vary 
from case to case, depending on the purpose of the exami-
nation of the content. One could look at this issue as cross 
check of content with any other related source providing 
similar information. It is easy to observe that we already 
strike a problem – measure and identification of information 
similarity. If it is not identical (a copy) then, it must be syn-
tactically different but semantically may be similar. The 
issue of integrity of information, in general, is hard to de-
fine thus computing semantic similarity of two texts is not a 
tractable problem. There are numerous examples of cases 
when information on hand has different form but its content 
is comparable – for instance, financial data from stock ex-
changes, recorded temperature in the same geographical 
26
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-395-7
BUSTECH 2015 : The Fifth International Conference on Business Intelligence and Technology

locations but independently provided by many sources etc. 
In most cases of Web information we deal with a written 
text, for a specific audience of readers, to achieve a specific 
impact, at a specific time, generally, on a large group of 
receivers. For instance medical information could be in 
form of a public forum sharing personal experience, or a 
professional outline presented in a simple exploratory form 
to provide some health related information written by an 
expert. The problem of measuring quality of information 
has been identified in particular by [5][7][16]. 
Numerous analyses have been dedicated to study the 
web credibility assessment process. This process may in-
volve several problems that have been extensively studied 
in economic theory - for instance the problem of infor-
mation asymmetry, which may refer to a hidden quality [19] 
and so called "market-of-lemons" effect, or to a hidden type 
[3] and the occurrence of moral hazard. The problem of 
assessing the credibility of Web content may involve both 
cases of hidden information, i.e., hidden quality for static 
pages or hidden type for dynamic pages. Tanaka and 
Yamamoto [16] have identified six measurable factors re-
lated to the five main recognized features (i.e., accuracy, 
authority, objectivity, currency, and coverage of topic) for 
judging the credibility of web information, namely referen-
tial importance, social reputation, content typicality, topic 
coverage, freshness, and update frequency. Fogg et al. [5] 
utilized prominence-interpretation theory in order to explain 
the process of credibility assessment. There have also been 
other approaches to automatic credibility assessment. These 
methods aggregated the values of different features. For 
instance Metzger [7] used information about credentials, 
advertisements, web page design, type of website, date of 
update, sentiment analysis, pre-defined search engine page 
ranking, information commonality, source independence, 
prestige, experience with the source and authority of infor-
mation origin. On the other hand, Wierzbicki et al. [1][18] 
attempted to create a simple game-theoretic model that 
would capture the salient characteristics of web content 
credibility evaluation. 
Continuing with example of the content evaluation we 
should bring up the term of trustworthiness. Trustworthi-
ness of the Web content occupied many researches recently. 
Rapid publication of new Web content affects many aspects 
of everyday lives of millions of people regardless of geo-
location or political beliefs [8]. Moreover, Web content 
becomes the basis for the operation of digital economy [6], 
[9] and very often an essential source of information while 
making decisions concerning shopping, employment, edu-
cation, health (both self-diagnosis of disease and treatment 
selection), financial data, investments, etc. [2][20]. On the 
other hand, Dellarocas [4] and Thompson [17] notice that 
web content is increasingly often manipulated for the bene-
fit of the authors or content providers.  
In the case of crowd-sourcing where involvement of a 
large group of unidentified testers/evaluators offers inde-
pendent opinions, the quality of such assessments may de-
pend heavily on many factors such as the background of the 
participants, education level, willingness to collaborate with 
good intensions and many more. Thus deploying the crowd-
sourcing to such process requires special preparation of the 
final result compilation. The analysis of collected data may 
suggest ignoring some submissions and to favor the others.  
This brings us to the term of a controlled crowd-
sourcing; meaning a well justified selection of the contrib-
uted works from a larger collection of submitted results 
carried out for a specific crowd-sourced activity.  
The problem of selecting only credible contribution 
from reputable but unknown partners will be more and more 
important in future for large scale business processes. The 
concept of business workflows partially executed by public 
input must be properly supported with new workflow facili-
ty foreign to the current structured workflow management 
systems. The assignment of task to partners/workers, the 
methods and correctness of the process design, data flow, 
and time constraints for traditional workflows is extensively 
studied for number of years [10]-[14] by Orlowska et al. 
The new functionality of workflow services to accommo-
date crowd-sourced activities with ‘reasonable‘quality of 
individual tasks execution is a new direction requiring fur-
ther research. 
The purpose of this paper is to build a simple mathemat-
ical model of controlled crowd-sourcing when dealing with 
evaluation of a given set of physical objects. The objects 
could be selected websites, books, electronic services in-
tended for public use, e-learning platforms, and pieces of 
software or any publically accessible entity. The considera-
tion of credible contributions is dependent on several pa-
rameters that all individually can be controlled within a 
predefined range of values to suite user’s defined crowd 
involvement. 
The paper is constructed as follows, firstly we point out 
how multiple single indicators assigned to individual ob-
jects are used to rank (order) the set of such items. Then, we 
construct a linear model of selection only assignments that 
satisfy defined ‘quality’ conditions. The presented model is 
general and may well form a foundation for a construction 
of an evaluation environment appropriate for use in differ-
ent application domains, independently from the purpose of 
the crowd sourced involvement into processes.  
The conclusion and suggestions of further extensions of 
these concepts close this presentation. 
III.  OBJECTS EVALUATION IN GENERAL 
The needs to evaluate objects from a collection fre-
quently emerge in many domains of applications.  Typical-
ly, we identify collection of attributes (characteristics) that 
require to be evaluated independently. Normally, the set of 
values used to express the results of the evaluation process 
for each property is specified.  Often, it is a finite set with 
rather small cardinality. Such evaluation cross multiple 
attributes with the overall purpose of ranking the objects (in 
27
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-395-7
BUSTECH 2015 : The Fifth International Conference on Business Intelligence and Technology

contrast to many rankings on the basis of individual proper-
ties) is a simple version of a classical multi criterion optimi-
zation problem. 
Most ranking systems such as for example; ranking of 
universities, innovation summary index for ranking coun-
tries use exactly multi-criterion approach to rank the con-
sidered objects. 
For the purpose of further considerations, let us recall a 
few well known facts about ranking/ordering objects in a 
multi dimensional space defined as a Cartesian product of 
the domains of considered attributes.  
Let us assume that objects are evaluated with respect to 
several attributes, each having its scale of values to be used 
by evaluators to express their impression.  
To be able to make the ranking list, for instance to 
communicate the order from the best to the worst object, we 
must use some expression, a function from multi dimen-
sional space to the set of real values, a ‘shrinking” function. 
Very often the expression used is a weighted sum of the 
values assigned to the individual criteria. It is worth to men-
tion here, that regardless of the effort dedicated to the con-
struction of the “shrinking” function, any two points being 
far (in the sense of Euclidian metric) from each other in 
multidimensional space, they may become close in the line-
ar order resulting from the shrinking process.  The simplest 
way to demonstrate the above statement is the application of 
the sum of values as the shrinking function. For example, in 
two dimensional space - two attributes both with domains 
{1,…,9} are evaluated for each object; distanced points 
(1,9) and (9,1) after application of the summation they all 
get value 10.  
There is here also another aspect requiring aggregations 
of the raw data. Already summarized values submitted by 
different tester for the same object requires further “shrink-
ing” process. This aggregation function needs to be de-
signed to finally get a single indicator for each object based 
on many submissions to allow the final ranking process. 
Such an aggregation may take into account different 
weights for more experienced testers or higher weights for 
more credible examiners, etc. However, the masking pro-
cess illustrated above will also take place here.  
Concluding this brief discussion we can sum up it as fol-
lows. It is well understood that each classification or com-
parison procedure of objects requires two important phases. 
Firstly, an abstraction of the objects by selecting a number 
of their attributes (characteristics) and ranges of evaluation 
values assigned to each attribute must be provided.  Second-
ly, functions capable to express our intuitive comparison 
need should be constructed to shrink the whole multidimen-
sional task to a single, one dimensional comparison prob-
lem. Such a mapping will be called an aggregation. 
IV. EVALUATION PROCESS 
Let us formulate the problem a bit more precisely but 
still informally. 
Further, we assume that the following set of data and 
objects are accessible;   
1)  Set of comparable objects - the evaluated collection,  
2)  Predefined scope of the evaluation in the form of de-
fined objects’ attributes. For instance in the context of web-
sites content evaluations it could be indicated features such 
as reliability, correctness of the content expressed in an 
objective sense wherever it is possible, clarity, esthetics, 
usability  or similar, 
3) Experts’ evaluation for each given examination scope 
or attribute called an expert value assigned to each attribute 
for each object. 
To effectively crowd-source an evaluation process, we 
shall have a mechanism to identify reliable testers in a given 
domain for specific evaluation scope and compile the final 
evaluation result only on the bases of aggregations across 
such multiple values. It is important to note that the calcu-
lated values may substantially differ from the expert value. 
The weakness of such expert’s replacement approach is 
rather obvious. There are no two identical evaluation cases 
and there are no super experts. As we mentioned in the 
introduction remarks, it is difficult to think of a similarity 
function construction between the objects such as, for ex-
ample websites content or a design task. In other words, 
based on some prior data from evaluation experience, we 
select testers only from formerly credible group assuming 
that the current evaluation task may be a bit different.  Thus, 
in some cases, the direct comparison of recent evaluations’ 
results with given super values may indicate substantial 
difference even for perfect evaluators in the past. Thus, the 
fundamental question is how to sensibly identify credible 
results to the problem based only on the prior testers’ expe-
rience.   
This observation indicates the difficult nature of this of 
problems but not a total inability to formulate it more pre-
cisely. The problem is real one, thus a level of imprecision 
is unavoidable, and so we must be ready to accept some 
estimation of perfect results in practice. 
V.   A FORMAL MODEL DEFINITION 
For the simplicity of the presentation, but without losing 
generality, we assume that objects are evaluated with re-
spect to only one attribute and evaluated by many examin-
ers. This assumption reduces required aggregation process 
to a single one, only across the testers’ submitted values for 
each object. An extension to cover the evaluation with re-
spect to several attributes is conceptually simple and as such 
is deliberately omitted in this paper. Further, we assume that 
each object is evaluated by a different group of testers due 
to the voluntarial character of crowd-sourcing activities. 
Thus some objects may attract more opinions then others. 
At this stage, we assume that the experiment is done over a 
fixed period of time so there is no need to accommodate 
dynamic change of number of tests and involved testers. 
28
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-395-7
BUSTECH 2015 : The Fifth International Conference on Business Intelligence and Technology

We introduce formal notation in Table 1 below where: 
W = {wi, i  {1, 2,…, I} is a finite set of comparable ob-
jects, E = {ek, k  {1, 2,…, K} is a set of examiners with 
different competences, si is an expert value for each i  {1, 
2,…, I} (a single value due to the assumption above), fk,i  is 
the evaluation f by the k-th examiner for the i-th object. 
TABLE I. NOTATIONS INTRODUCTION 
Objects 
Expert 
value 
Tester e1 
Tester e2 
… 
Tester ek 
w1 
w2 
… 
wj   
s1 
s2 
… 
sj 
f1,1 
f1,2 
… 
f1,j 
- 
f2,2 
… 
f2,j 
 
fk,1 
- 
… 
fk,j 
 
In general, by the evaluation of an object by a tester we 
mean an assignment of a value from the predefined subset 
of natural numbers f:  (W, E)  {0, 1, … , v}. 
An extension of such notation for multiple attributes re-
quires several evaluations f1, f2, etc for a given object by a 
tester. As we assumed earlier, we consider a single attribute 
evaluation only, hence there is only one function f in this 
formalization. 
Further, for each tester k we assign two values [dk, zk], 
where dk, zk  {0,1,…, Bk,} dk is the count of so-called 
good evaluations submitted by the k-th evaluator, and corre-
spondingly, zk is the count of bad evaluations of this exam-
iner and Bk is the total number of submitted tests by the k-th 
examiner dk  + zk = Bk.. 
For the clarity of the presentation, we suggest the sim-
plest approach to separate the good from bad evaluations 
below.  
Formally, we define good and bad evaluations as fol-
lows; 
An evaluation of an object wi by a tester k is good if   si 
- fk,i ≤ c, is bad otherwise, meaning  
 si - fk,i > c for each  k  { 1, 2,…, K} where c is a con-
stant value  indicating acceptable distance to the expert 
evaluation. 
It is obvious that one could consider immediate general-
ization of this approach by introducing many levels of the 
goodness of evaluation, however for the sake of simplicity 
we consider further only one cutting point c.  
We illustrate graphically the concepts introduced. Let us 
consider the first quarter of the (D,Z) space where Z and D 
are sets of natural numbers. For a given period of time and 
given constant c, we depict collected data of k testers by 
assigning a single point [dk, zk] for each k. Hence, submit-
ting additional test by a tester k (for an object) moves its’ 
point either to the right or to the top, depending whether the 
evaluation is regarded as a good or bad one. Clearly, if all 
evaluators would submit the same number of tests t , mean-
ing that they all evaluated t objects but not necessarily the 
same set of objects as indicated by entries in the table 
above,  then all those points would be positioned on a 
straight line z = - d + t.  
It is easy to see that all points positioned on any straight 
line from the family z = a d, where a is a positive number, 
identify all testers with the same proportion of good to bad 
evaluations but with different total count of the completed 
tests. It is depicted on Figure 1 below. 
One may interpret those points as an image of history 
for submitted evaluations for all testers without indication 
which objects these individuals examined. 
Our first goal is to group testers of similar credibility in-
to classes. We provide a simple but flexible definition of 
such partitioning by introducing two constant values g1, g2.  
As before, one could consider more comprehensive model 
by introducing additional values for a finer partition of this 
space. However, for the purpose of this presentation, only 
two values will give required flexibility and natural parti-
tion of set of all testers into tree classes. 
 
 
Figure 1. Illustration of introduced classes of examiners 
 
Let us discuss in more details such a division. The two 
straight lines z=g1d and z=g2d divide the first quarter of 
space (D,Z) into three areas as depicted on Figure 1 above. 
We might call a tester credible if and only if in his history 
of evaluation process the count of the good evaluations was, 
for instance, 90%, implying the value of g1 = 0,1. Conse-
quently, the count of bad evaluations must be just 10% for 
this tester.  The graphical interpretation of this set is the 
lowest triangle in Figure 1. 
The other class of testers we might call almost credible. 
In this case, let for instance g2=0,25. Naturally, this is the 
set of points between the two formed lines.  
Finally, the third class we consider as not credible (not 
plausible) for all the points on lines z=g2d for all g2 > 0,25. 
It is important to remind us that joining the crowd-
sourcing evaluation process is on volunteer bases, so we 
cannot expect that all testers will evaluate the same set of 
objects and the same number of objects in general. Fre-
quently, some individuals are more active than others so 
there is a need to reflect this fact in our model.  
It is rather clear that if a tester has completed very few 
examinations then based on such limited activity its alloca-
tion to one of the classes might not be well justified. This is 
why, it is sensible to introduce a new constraint: a minimum 
29
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-395-7
BUSTECH 2015 : The Fifth International Conference on Business Intelligence and Technology

number of evaluations m completed before assigning the 
tester to the appropriate class.  
Formally, k-the tester’s experience related condition is 
following dk +zk > m. 
As before, only for the simplicity of the presentation, we 
consider only one cut-off point - the minimum m, but intro-
duction of several levels of experience m1, m2, ... is natural 
as shown on Figure 2. 
 
Figure 2. Illustration of introduction of minimum count condition 
Introduction of this parameter to the model gives us ad-
ditional flexibility of selecting only those results of evalua-
tions that come from a group of individuals’ satisfying our 
requirements for their general quality, as it is illustrated in 
Figure 2 above. Especially, in case of crowd-sourcing, a 
controlled selection of cooperators is vital for overall sense 
of sharing such activities. Let us then introduce this condi-
tion on our model. 
It is easy to see that introduction of parameters in our 
linear model such as  c, g1, g2, m give us opportunity to 
control the selection of set of points in this space in many 
ways. We can move those lines freely by changing the val-
ues for the parameters to make more or less strict pre-
selection of testers.  
The sum of all Bk for all k allows us to calculate the 
number of submitted results for our crowd-sourced task for 
all objects from W. However, till now, in our model we 
focused on testers’ reputation but there is no reference to 
the evaluation of individual object by any means.  Some 
objects could be evaluated by many testers, some by only 
few and some may not be evaluated at all. To assign the 
final evaluation value by combining opinion of several test-
ers of an object one would expect to have a minimal number 
of tests completed for this object. Then aggregation proce-
dure smoothes the differences of assigned values (marks) to 
offer a final and credible result. Theoretically, it is possible 
that the class of credible and experienced testers is sizeable 
having many elements thus satisfy model’s conditions but 
as far as object wi is concerned, is insufficiently rich. This is 
when none of the testers, or very few from this class, evalu-
ated object wi. In such case there is no raw data for the ag-
gregation function to be applied. 
This observation leads to requirement of introduction of 
subsequent control parameter v – the minimal number of 
tests for each object before the aggregation function can be 
applied. Subsequently, aggregation function combining 
results from at least v contributors satisfying required con-
ditions (selected values c, g1, g2 and m) can be applied.  
Let summarize the set of model parameters introduced 
earlier. They are: 
1. Constant value c measuring acceptable difference 
between the entry and object’s expert value,  
2. Two constants values g1, g2 defining set of testers 
classes  -  credibility conditions, 
3. Constant value m measuring minimal number of 
completed evaluations by a tester -  experience condition,  
4. Constant value v indicating minimal count of sub-
mitted results for the object by testers from selected class 
prior to the calculation of final result – object occurrence 
condition.  
This summary concludes our discussion on selection of 
credible examiners, let us then return to the main problem 
of objects evaluation. It can be completed systematically 
from now on. For each object, we apply the final aggrega-
tion function only on those entries that satisfy parameters 
defined at the process design phase. Thus the problem has 
the following formulation; for a given set of objects W, for 
an undefined and open set of evaluators/testers E, for  a 
given constants c, g1, g2, m and v of the model, for each 
object wi,  compute value of the aggregation function based 
on all fk,i  that satisfying credibility, experience and object 
occurrence conditions.  
In practice, it is possible that concurrent fulfillment of 
all the conditions may require some time. Only the conjunc-
tion of all specified conditions offers some expected level of 
quality of task execution. 
Presented construction of a flexible environment for the 
visualization of entries coming from the crowd sourced 
activity appears to be an interesting service. An interface 
allowing selection of the values for the model parameters 
and dynamic control of separation lines is envisaged to be a 
useful tool independent form the application domain and 
purpose of the application supported. The scalability of such 
a system needs to be carefully considered. Over period of 
time, in case of considerable number of new submissions, 
the data content will grow in size and change of its content 
where some players may gain reputation but some may 
loose their already gained status. This observation justify 
introduction of an additional dimension to the model – tem-
poral aspect. For the purpose of this preliminary presenta-
tion, it is sufficient to consider fixed length period of time 
for each session of the execution. Addition of continuous, 
dynamic observations requires more complex formalization 
but this is not a purpose of this article. 
VI. CONCLUSION AND FUTURE WORK 
Crowd-sourcing is getting a form of direct collaboration, 
often on a large scale, between the task provider and public 
contributors. There is a need to provide easy to manage 
30
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-395-7
BUSTECH 2015 : The Fifth International Conference on Business Intelligence and Technology

environment to support controlled crowd-sourcing involve-
ment. 
In this paper, a linear model of selecting reputable con-
tributors in the crowd sourced task execution has been pre-
sented. Model offers choice of parameters within predefined 
ranges to allow flexibility in selection of preferred submis-
sions from the large scale crowd sourced assignments.  
Automation of introduced control mechanism is a sim-
ple implementation task. The concept was tested on synthet-
ic data sets demonstrating potential usability on a large 
scale. The issue of user interface to such environment 
should be tested exploring several options. Effective visual-
ization of dynamically changing points’ positions maybe a 
useful tool in practice for a big scale crowd sourcing as-
signments. Visual observation of the points’ density and 
introduced functionality allowing continuous movement of 
the introduced lines may in return automatically compute 
the introduced parameters. Empirical examination of distri-
bution of points in different segments of the dedicated 
screen, forming a base for future selection of parameters 
values appears to be an interesting scope for applied study. 
The presented model may be extended in many different 
directions by imposing more conditions on the space de-
fined above. Firstly, a number of levels for all types of pre-
sented constraints will bring additional precision of the 
observed experimentation. Secondly, for the same data 
segment, for each object several types of aggregation func-
tions can be applied to tailor the best fit. Those extensions 
depend from the size of the problem, number of players, 
intended application and the domain of consideration and 
required expected precision. 
A subsequent stage of this work will cover the change of 
the linear model, where separation of space segments is 
done by straight lines, for a class of polynomial functions. 
An extensive testing and analysis of large real data sets may 
be a useful source of pointers for well justified extensions. 
The ultimate goal of technological support for collabora-
tion of standard business processes with a crowd accom-
plished activities is one solution for both types of partners. 
It is envisaged that the deployment of the controlled crowd 
sourcing functionality by new generation of workflows 
technology will form a suite of a novel technological solu-
tion for business support and expansion.  
                               REFERENCES 
[1] K. Abramczuk, P. Adamska, T. Papaioannou, A. Wierzbicki, 
andK. Aberer, ”Game-theoretic Models of Web Credibility”. 
Joint 
WICOW/AIRWeb 
Workshop 
on 
Web 
Quality 
(WebQuality '12), 2012, pp.27-34. 
[2] J. Chevalier and D. Mayzlin, “The effect of word of mouth on 
sales: Online book reviews. Journal of Marketing Research, 
43(3), August. 2006, pp.345–354. 
[3] C. Dellarocas, “Reputation mechanism design in online trad-
ing environments with pure moral hazard”. Info. Sys. Re-
search(16), 2005, pp.209–230. 
[4] C. Dellarocas, “Strategic Manipulation of Internet Opinion 
Forums: Implications for Consumers and Firms”. Manage-
ment Science, 52(10), October. 2006, pp.1577–1593. 
[5] B. J. Fogg, C.  Soohoo, D. R. Danielson, L. Marable, J. Stan-
ford, and E. R. Tauber, “How do users evaluate the credibility 
of web sites?: a study with over 2,500 participants”. Confer-
ence on Designing for user experiences, DUX '03. New York: 
2003, pp.1-15. 
[6] M. I. Melnik and J. Alm, “Does a seller's ecommerce reputa-
tion matter? evidence from ebay auctions”. Journal of Indus-
trial Economics, 50(3), September. 2002, pp.337–349. 
[7] M. J. Metzger, “Making sense of credibility on the web: 
Models for evaluating online information and recommenda-
tions for future research”. J. Am. Soc. Inf. Sci. Technol. (58), 
2007, pp. 2078–2091. 
[8] L. Rainie and N. Kommers, “Use of the Internet at Major Life 
Moments”. Washington D. C. Pew Internet and American 
Life Project, May. 2002. 
[9] P. Resnick, R. J. Zeckhauser, J. Swanson and K. Lockwood, 
“The value of reputation on ebay: A controlled experiment”. 
Experimental Economics, 9(2), 2006, pp.79–101. 
[10] S. W. Sadiq, O. Marjanovic and M. E. Orlowska, “Managing 
change and time in dynamic workflow processes”. Interna-
tional Journal of Cooperative Information Systems, 2000, pp. 
93-116. 
[11] S. W. Sadiq, M. E. Orlowska and W. Sadiq, “Specification 
and validation of process constraints for flexible workflows”. 
Information Systems , 30, 2005, pp.349-378. 
[12] S. W. Sadiq, M. E. Orlowska, W. Sadiq and C. Foulger, ”Data 
flow and validation in workflow modeling”. Proceedings of 
the 15th Australasian database conference-Volume 27, 2004.  
pp. 207-214.  
[13] S. W. Sadiq, W. Sadiq and M. E Orlowska, “Pockets of flexi-
bility in workflow specification”. Conceptual Modeling—ER, 
Springer Berlin Heidelberg. 2001, pp.513-526.  
[14] W. Sadiq and M. E. Orlowska, “Analyzing Process Models 
using Graph Reduction Techniques”. Information Systems 
(25), 2000, pp.117-134. 
[15] J. Surowiecki, “The wisdom of crowds: why the many are 
smarter than the few and how collective wisdom shapes busi-
ness, economies, societies and nations”.  Boston, MA: Little, 
Brown, 2004 
[16] Y. Tanaka and K. Yamamoto, “Enhancing credibility judg-
ment of web search results”. CHI'11. New York: ACM. 2011 
[17] N. Thompson. “More companies pay heed to their "word of 
mouse" reputation”. New YorkTimes, June 23. 2003. 
[18] A. Wierzbicki, P. Adamska,  K. Abramczuk, T. Papaioannou,  
K. Aberer and E. Rejmund, “Studying Web Content Credibil-
ity by Social Simulation”, Journal of Artificial Societies and 
Social Simulation, vol.17, issue 3, 2014, pp.165-191. 
[19] D. Wilson and R. Kreps, “Reputation and imperfect informa- 
tion” Journal of Economic Theory(27), 1982,  pp. 253–279. 
[20] Y. H. Weng, et al. “Increasing utilization of Internet-based re-
sources following efforts to promote evidence-based medi-
cine: a national study in Taiwa”. BMC Medical Informatics 
and Decision Making, 13:4, January. 2013. doi:10.1186/1472-
6947-13-4 
31
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-395-7
BUSTECH 2015 : The Fifth International Conference on Business Intelligence and Technology

