Developing a Formal Model of Conversational Agent 
 
 
Mare Koit 
Institute of Computer Science 
University of Tartu 
Tartu, Estonia 
mare.koit@ut.ee 
 
Abstract—Our model of conversational agent is based on the 
analysis of human-human telemarketing calls. We study 
negotiations where two participants are presenting arguments 
for and against of doing an action. The choice of an argument 
depends, on one hand, on the beliefs of the dialogue participant 
about the positive and negative aspects of the action and the 
needed resources, and on the other hand, on the result of 
reasoning affected by these beliefs. The dialogue initiator is 
using a partner model—the hypothetical beliefs of the partner 
who is aimed to do the action. At the same time, the partner 
operates with own beliefs. Both the models have to be upgraded 
during a dialogue. The notion of a communicative strategy is 
included in the model as an algorithm used by a dialogue 
participant for achieving his or her communicative goal. A 
limited version of the model is implemented on the computer. 
The computer attempts to influence the reasoning of the user by 
its arguments in order to convince the user to make the positive 
decision about the proposed action.  
Keywords—reasoning; 
beliefs; 
communicative 
strategy; 
negotiation; argument; conversational agent. 
I.  INTRODUCTION  
This article is an extended version of the conference paper 
[1]. Our aim is to develop a model of conversational agent 
that interacts with a user in a natural language and carries out 
negotiations.  
Negotiation is a form of interaction in which a group of 
agents, with a desire to cooperate but with potentially 
conflicting interests try to come to a mutually acceptable 
division 
of 
scarce 
resources 
[2]. 
Negotiation 
is 
simultaneously a linguistic and a reasoning problem, in which 
intent must be formulated and then verbally realized. A 
variety of agents have been created to negotiate with people 
within a large spectrum of settings including the number of 
parties, the number of interactions, and the number of issues 
to be negotiated [3]. Negotiation dialogues contain both 
cooperative and adversarial elements, and their modelling 
require agents to understand, plan, and generate utterances to 
achieve their goals [3][4]. 
We start with the analysis of human-human negotiation 
dialogues aiming to model the reasoning processes, which 
people go through when pursuing their communicative goals 
and coming to a decision.  
The remainder of the paper is organized as follows. 
Section 2 describes related work. In Section 3, we analyze a 
kind of human-human negotiation dialogues—telemarketing 
calls, in order to explain how do people reason and argue 
when negotiating about doing an action. In Section 4, we 
introduce our model of conversational agent that takes into 
account the results of the analysis of human-human 
negotiations, and an implementation—a simple dialogue 
system (DS). Section 5 discusses the model and the DS. In 
Section 6, we draw conclusions and plan future work. 
II. RELATED WORK 
A conversational agent, or DS, is a computer system 
intended to interact with a human using text, speech, 
graphics, gestures and other modes for communication. It will 
have both dialogue modelling and dialogue management 
components [5]. A dialogue manager is a component of a DS 
that controls the conversation. The dialogue manager reads 
the input modalities, updates the current state of the dialogue, 
decides what to do next, and generates output [6]. Four kinds 
of dialogue management architectures are most common—
plan-based, finite-state, frame-based, and information-state 
[7]. 
One of the earliest models of conversational agent is 
based on the use of artificial intelligence planning techniques. 
Using plans to generate and interpret sentences require the 
models of beliefs, desires, and intentions (BDI) [4][5]. Plan-
based approaches, though complex and difficult to embed in 
practical dialogue systems, are seen as more amenable to 
flexible dialogue behavior [7]. 
The simplest dialogue manager architecture, used in many 
practical implementations, is a finite-state manager. The 
states correspond to questions that the dialogue manager asks 
the user, and the arcs correspond to actions to take depending 
on what the user responds. Such a system completely controls 
the conversation with the user. It asks the user a series of 
questions, ignoring or misinterpreting anything the user says 
that is not a direct answer to the system’s question, and then 
going on to the next question [7]. 
Frame-based dialogue managers ask the user questions to 
fill slots in a frame until there is enough information to 
perform a data base query, and then return the result to the 
user. If the user answers more than one question at a time, the 
system has to fill in these slots and then remember not to ask 
the user the associated questions for the slots. In this way, the 
user can also guide the dialogue [5].  
More advanced architecture for dialogue management, 
which 
allows 
for 
sophisticated 
components 
is 
the 
information-state architecture [5][7]. An information-state 
approach combines the other approaches, using the 
advantages of each. An information state includes beliefs, 
assumptions, expectations, goals, preferences and other 
attitudes of a dialogue participant that may influence the 
176
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

participant’s interpretation and generation of communicative 
behavior. The functions of the dialogue manager can be 
formalized in terms of information state update [8]. Update 
and selection rules provide a more transparent, declarative 
representation of system behavior.  
Rahwan et al. [9] discuss three approaches to automated 
negotiation—game-theoretic, 
heuristic-based 
and 
argumentation-based.  
A dialogue game is a rule-based structure for conversation 
where arguments are exchanged between two participants 
reasoning together on a turn-taking basis aimed at a collective 
goal [10]. Heuristic methods offer approximations to the 
decisions made by participants. Agents exchange proposals 
(i.e., potential agreements or potential deals). Both game-
theoretic and heuristic approaches assume that agents’ 
preferences are fixed. One agent cannot directly influence 
another agent’s preferences, or any of its internal mental 
attitudes (beliefs, desires, goals, etc.) that generate its 
preference model. A rational agent only modifies its 
preferences if it receives new information.  
Attitudes are relatively enduring, affectively colored 
beliefs, preferences, and predispositions towards objects or 
persons [11]. Attitude is a psychological tendency that is 
expressed by evaluating a particular entity with some degree 
of favor or disfavor. Attitude change can mediate the impact 
of some influence treatment on behavioral compliance [12]. 
Argumentation-based approaches to negotiation allow 
agents to ‘argue’ about their beliefs and other mental attitudes 
during 
the 
negotiation 
process. 
Argumentation-based 
negotiation is the process of decision-making through the 
exchange of arguments [13].  
Hadjinikolis et al. [14] provide an argumentation-based 
framework for persuasion dialogues, using a logical 
conception of arguments, that an agent may undertake in a 
dialogue game, based on its model of its opponents. In 
negotiation, an argument can be considered as a piece of 
information that may allow an agent to (a) justify its 
negotiation state; or (b) influence another agent’s negotiation 
state. Amgoud and Cayrol define an argument as a pair (H, h) 
where (i) H is a consistent subset of the knowledge base, (ii) 
H implies h, (iii) H is minimal, so that no subset of H 
satisfying both (i) and (ii) exists. H is called the support and h 
the conclusion of the argument [15].  
Automated negotiation agents capable of negotiating 
efﬁciently with people must rely on a good opponent 
modelling component to model their counterpart, adapt their 
behavior to their partner, influencing the partner’s opinions 
and beliefs [16]. NegoChat is the first negotiation agent 
successfully developed to use a natural chat interface while 
considering its impact on the agent’s negotiation strategy [3]. 
A virtual human negotiating with a human helps people learn 
negotiation skills. For virtual agents, the expression of 
attitudes in groups is a key element to improve the social 
believability of the virtual worlds that they populate as well 
as the user’s experience, for example in entertainment or 
training applications [17][18][19]. Argumentation systems 
can be beneficial for students.  
Computers that negotiate on our behalf hold great promise 
for the future in emerging application domains such as the 
smart grid and the Internet of Things. An interesting and 
useful kind of DSs are embodied conversational agents 
[17][20][21].  
III. ANALYSIS OF HUMAN-HUMAN NEGOTIATIONS 
Our further aim is to implement a DS, which interacts 
with a user in a natural language (Estonian) and carries out 
negotiations like a human does. For that, we are studying 
human-human negotiations using the Estonian dialogue 
corpus [22]. All the dialogues in the corpus are recorded in 
authentic situations and then transliterated by using the 
transcription of Conversation Analysis [23]. A sub-corpus of 
telemarketing calls is chosen for the current study. In the 
dialogues, two official persons are communicating—a sales 
clerk of an educational company, he is the initiator of a call, 
and a manager or a personnel officer of another institution—
she is here a customer.  
The educational company offers training courses 
(management, sale, etc.), which can be useful for the 
employees of the customer’s institution. The communicative 
goal of a sales clerk is to convince the customer to decide to 
order a course.  
Several 
typical 
phases 
can 
be 
differentiated 
in 
telemarketing negotiations [24]: (1) preparing, (2) opening, 
(3) mapping the customer, (4) argumentation, (5) achieving a 
decision, (6) following activities. The first (1) and the last (6) 
phases take place outside of actual negotiation.  
Phase 1 is carried out by a sales clerk alone when he is 
planning his very first call to a pre-selected customer. The 
clerk, by using different open sources, tries to collect 
information about the customer’s institution—its background, 
financial situation, the number of employees, etc., in order to 
be ready to propose and argue for a suitable course. The last 
phase (6) will be initiated by a sales clerk after the customer 
has already passed a course. Then the clerk again calls to the 
customer asking her feedback. Still, our sub-corpus does not 
include such calls. Therefore, it is possible to recognise only 
the phases 2 to 5 in the dialogues of our sub-corpus.  
As a rule, several calls are needed before a customer 
makes her decision about the offered course. The decision 
can be positive (to take the course) or negative (to reject the 
clerk’s proposal). However, no decision is mostly made 
regarding the courses during a call—the calls usually end 
with an agreement to continue the negotiation after some 
time. The reason is that all the telemarketing calls in our 
corpus belong to the beginning stage of negotiations. 
The 
most 
important 
phase 
of 
negotiation 
is 
argumentation. A sales clerk (A) presents different arguments 
that take into account the actual needs of the customer (B) 
explained by him (A) before or during the call. A tries to bring 
out the factors that are essential for the customer, in order to 
convince her to make a positive decision (Example 1). If B 
accepts these factors then A will demonstrate how the 
proposed course will solve B’s assumed problems. In an ideal 
case, the customer will agree with the proof offered by the 
clerk and she will decide to take the course. 
The behaviour of sales clerks and customers is different 
when they are arguing for/against a course. A sales clerk 
when having the initiative provides his arguments for taking 
177
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

the course either asserting something (then a customer 
typically accepts the assertion, Example 1).  
 
Example 1 (transcription of Conversation Analysis is 
used in the examples) 
A: /---/(1.0) .hh sest loomu`likult et=ee `töökogemuste 
kaudu: õpib ka: alati aga .hh a `sageli ongi just `see (0.5) mt 
ee `kursused pakuvad sellise `võimaluse kus saab siis `teiste 
.hh oma hh `ala `spetsia`listidega samuti `kokku=ja `rääkida 
nendest `ühistest prob`leemidest ja samas siis ka .hh ee 
`mõtteid ja `ideid ee hh ee=`Tiritamme poolt sinna `juurde. 
because, of course, one can learn 
from experience but frequently training 
courses make it possible to meet other 
specialists in the field and discuss 
common problems; additional thoughts and 
ideas come from Tiritamm         argument 
(.) 
B: £ `jah?  
yes                              accept 
 
A customer, to the contrary, does not accept 
assertions/arguments of a sales clerk when arguing against 
taking a course (Example 2).  
 
Example 2 
/---/ B: aga jah ei mul on see läbi ´vaadatud=ja (.) 
´kahjuks ma pean ütlema=et (.) et ´teie (.) seda meile (.) ´ei 
suuda ´õpetada (.) mida (.)´mina: (.) tahan. 
but 
I 
have 
looked 
through 
your 
catalogue of courses and unfortunately, 
I have to say that you can’t teach what 
is needed for us            counter argument 
/---/ 
A: .h ja mida kon´kreetselt=ee ´teie tahate.  
and what do you want                            question 
(0.8) mida te ´silmas ´peate.  what do you have 
in view                                                                 question 
B: noo (0.2) ´meie (.) äri´tegevus on (.) ´ehitamine.  
well, our business is house-building 
                                                                              answer 
/---/ 
A: nüüd kas (0.2) näiteks (0.5) ´lepingute ´saamisel 
(0.5) mt ee ´tegelete te ka: läbi´rääkimistega.  
well, do you need to carry out 
negotiations 
in 
order 
to 
achieve 
agreements                                                         question 
B: noo ikka.  
yes, of course                                            answer 
(0.8) 
A: mt et see=on ka üks ´valdkond mida me: (0.2) 
´käsitleme.  
but that is one of our fields, which 
we cover                                                             argument 
  
The argumentation continues in a similar way. A 
attempts to convince B preparing his new arguments by 
questions. Either A constructs his arguments during the 
conversation or he chooses suitable arguments from a 
previously completed set of possible arguments collected in 
discussions with other customers.  
In Example 3, the customer takes over the initiative and 
explains that no courses are needed because her experience 
is unique.  
 
Example 3 
B: /---/ me ´müüme eksklu´siivset ´kodu´mööblit. 
we 
are 
selling 
exclusive 
home 
furniture. 
/---/  
noo siis ´see kauba´märk ka juba ütleb iseendast se=on 
´täpselt sama´moodi nagu on Mer´seedes=´Pens ja 
´Jaaguar.   
this trademark speaks for itself, 
like Mercedes Benz and Jaguar.  
[eksole]  
is it not?                                     counter argument 
A: [jaa. ] 
yes.  
seda ´küll?   
it is.                                                                  accept 
 
A sales clerk always accepts the counter arguments 
presented by a customer but he also brings out his own 
arguments for taking the course and tries to take over the 
initiative. The participants communicate cooperatively in 
the majority of the analyzed dialogues. The customer 
similarly can ask questions like the sales clerk does but her 
aim is rather to get more information about the course(s) 
than to dispute. 
When modelling negotiation, a good way seems to 
follow the sales clerks’ strategy: try to take and hold the 
initiative and propose ‘hard’ arguments for the strived 
action, i.e., the statements that do not provoke the partner’s 
rejection but accept. In order to have such arguments at 
disposal, it is necessary to know as possible more about the 
partner in relation to the goal action. That is the reason why 
mapping (i.e., explanation of the customer’s needs) is a 
necessary phase (3) in our analyzed telemarketing calls. 
In summary, telemarketing calls turn out to be good 
examples of argument-based negotiations. 
IV. MODELLING CONVERSATIONAL AGENT  
Our model of conversational agent is motivated by the 
results of the analysis of human-human negotiations. Let us 
consider negotiation between two (human or artificial) 
participants A and B where A is the initiator. Let A’s 
communicative goal be to bring B to the decision to do an 
action D. When convincing B, he is using a partner model 
(an image of the communication partner) that gives him 
grounds to believe that B will agree to do the action. A 
starts the dialogue by proposing B to do D. If B, after her 
reasoning, refuses, then A must influence her in the 
following negotiation, continuously correcting his partner 
model and trying to guess in which reasoning step B 
reached her negative decision. In this way a dialogue—a 
178
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

sequence of utterances will be generated together by A and 
B. 
A. Reasoning Model  
The initial version of our reasoning model is introduced 
in [24]. In general, it follows the ideas realized in the BDI 
model [25][4][5]. The reasoning process of a subject about 
doing an action D consists of steps where the resources, 
positive and negative aspects of D will be weighed. A 
communication partner can take part in this process only 
implicitly by presenting arguments to stress the positive 
aspects of D and downgrade the negative ones.  
Our reasoning model includes two parts—(1) a model 
of (human) motivational sphere that represents the beliefs 
of a reasoning subject in relation to the aspects of the action 
under consideration, and (2) reasoning procedures. 
1) Model of Motivational Sphere 
We represent the model of motivational sphere of a 
communication participant as a vector with (here: 
numerical) coordinates that express the beliefs of the 
participant in relation to different aspects of the action D:  
 
wD = (w(resourcesD), w(pleasantD), w(unpleasantD), 
w(usefulD), w(harmfulD), w(obligatoryD), w(prohibitedD), 
w(punishment-doD), w(punishment-notD)).  
 
The value of w(resourcesD) is 1 if the reasoning subject 
has all the resources needed for doing D, and 0 if some of 
them are missing. The value of w(obligatoryD) or 
w(prohibitedD) is 1 if the action is obligatory or, 
respectively, prohibited for the subject (otherwise 0). The 
values of the other coordinates can be numbers on the scale 
from 0 to 10—w(pleasantD), w(unpleasantD), etc, indicate 
the values of the pleasantness, unpleasantness, etc. of D or 
its consequences; w(punishment-doD) is the punishment for 
doing a prohibited action and w(punishment-notD)—the 
punishment for not doing an obligatory action.  
2) Reasoning Procedures 
The reasoning itself depends on the determinant which 
triggers it. With respect to the used (intuitive) theory, there 
are three kinds of determinants that can cause humans to 
reason about an action D: wish, need and obligation [26]. 
Therefore, 
three 
different 
prototypical 
reasoning 
procedures can be described—WISH, NEEDED, and 
MUST. Every procedure consists of steps passed by a 
reasoning subject and it finishes with a decision—do D or 
not. When reasoning, the subject considers his/her 
resources as well as different positive and negative aspects 
of doing D. If the positive aspects (pleasantness, usefulness, 
etc.) 
weigh 
more 
than 
negative 
(unpleasantness, 
harmfulness, etc.) then the decision will be “do D” 
otherwise “do not do D”. The reasoning subject checks 
primarily his/her wish, thereafter the need and then the 
obligation and he/she triggers the corresponding reasoning 
procedures. If no one procedure returns the decision “do D” 
then the reasoning ends with the decision “do not do D”.  
In Figures 1 and 2, we present two reasoning procedures 
that will be used in the following examples—WISH 
triggered by the wish of the reasoning subject to do the 
action D (i.e., doing the action is more pleasant than 
unpleasant for the subject), and NEEDED triggered by the 
need of the reasoning subject to do the action D (i.e., doing 
the action is more useful than harmful for the subject). The 
procedures are presented as step-form algorithms. We do 
not more indicate the action D. 
 
Presumption: w(pleasant)  w(unpleasant). 
1) Is w(resources) = 1? If not then 11. 
2) Is w(pleasant) > w(unpleasant) + w(harmful)? If not 
then go to 6. 
3) Is w(prohibited) = 1? If not then go to 10. 
4) Is w(pleasant) > w(unpleasant) + w(harmful) + 
w(punishment_D)? If yes then go to 10. 
5) Is w(pleasant) + w(useful) > 
w(unpleasant)+w(harmful) + w(punishment_D)? If yes then 
go to 10 else 11. 
6) Is w(pleasant) + w(useful)  w(unpleasant) + 
w(harmful)? If not then go to 9. 
7) Is w(obligatory) = 1? If not then go to 11. 
8) Is w(pleasant) + w(useful) + w(punishment_not) > 
w(unpleasant) + w(harmful)? If yes then go to 10 else 11. 
9) Is w(prohibited) = 1? If yes then go to 5 else 10. 
10) Decide: to do D. End. 
11) Decide: not to do D. 
Figure 1. Reasoning procedure WISH. 
 
 
Presumption: w(useful)  w(harmful). 
1) Is w(resources) = 1? If not then go to 8. 
2) Is w(pleasant) > w(unpleasant)? If not then go to 5.  
3) Is w(prohibited) = 1? If not then go to 7. 
4) Is w(pleasant) + w(useful) > w(unpleasant) + 
w(harmful) + w(punishment-do)? If yes then go to 7 else 8. 
5) Is w(obligatory) = 1? If not then go to 8. 
6) Is w(pleasant) + w(useful) + w(punishment-not) > 
w(unpleasant) + w(harmful)? If not then go to 8. 
7) Decide: do D. End. 
8) Decide: do not do D. 
Figure 2. Reasoning procedure NEEDED. 
 
We use two vectors wB and wAB, which capture the 
beliefs of communication participants in relation to the 
action D under consideration. Here wB is the model of 
motivational sphere of B who has to make a decision about 
doing D; the vector includes B’s (actual) evaluations 
(beliefs) of D’s aspects. These values are used by B when 
reasoning about doing D. The other vector wAB is the 
partner model that includes A’s hypothetical beliefs 
concerning B’s beliefs in relation to the action. It is used by 
A when planning the next turns in dialogue. We suppose 
that A has some preliminary knowledge about B in order to 
compose the initial partner model before making the initial 
proposal.  
179
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Both the models will change as influenced by the 
arguments presented by both the participants in negotiation. 
For example, every argument presented by A targeting the 
usefulness of D will increase the corresponding values of 
wB(useful) as well as wAB(useful).  
B. 
Communicative Strategies and Tactics 
A communicative strategy is an algorithm used by a 
participant for achieving his/her goal in communication [27]. 
The initiator A can realize his communicative strategy in 
different ways—he can entice, persuade or threaten the 
partner B to do (or respectively, not to do) D. We call these 
ways 
of 
realization 
of 
a 
communicative 
strategy 
communicative tactics. If A’s communicative goal is “B will 
do D” then by persuading, A tries to trigger B’s reasoning by 
the NEEDED-determinant (i.e., he tries to increase the 
usefulness of D for B as compared with its harmfulness). 
Respectively, when enticing, A tries to trigger B’s reasoning 
by the WISH-determinant (to increase the pleasantness) and 
when threatening, by the MUST-determinant (to increase the 
punishment for not doing an obligatory D). We call the 
affected aspect (respectively, usefulness, pleasantness, or 
punishment) the title aspect of the tactics. When choosing the 
communicative tactics, A believes that B’s reasoning 
triggered by this determinant, will give a positive decision in 
his partner model. Still, the participants can change their 
communicative tactics during negotiation. 
 
• 
Determine the initial wAB in relation to D 
• 
Choose an input determinant (WISH, NEEDED, 
or MUST), which determines a reasoning procedure 
depending on wAB  
• 
Choose the communicative tactics with the title 
aspect 
a 
(respectively, 
pleasantness, 
usefulness 
or 
punishment for not doing D if it is obligatory for B) 
• 
Implement the tactics to generate a proposal to B 
to do D 
REPEAT 
Analyze B’s utterance  
- - (1)  Choose a (counter) argument depending on the 
aspect of D indicated in B’s utterance 
- - (2)  Choose argument(s) to support a 
Update wAB  
Run the current reasoning procedure in updated wAB  
- - Response to B 
IF the decision generated with the reasoning procedure 
matches to GA THEN present the chosen argument(s) to B 
(- - A can optionally present both (1) and (2), OR only (2)) 
Change the communicative tactics? IF yes THEN 
choose the new tactics (with new a)  
UNTIL B agrees (GA achieved), OR B postpones the 
decision (GA not yet achieved), OR A decides to abandon 
GA, OR A does not have unused tactics and/or unused 
arguments (GA not achieved) 
 
Figure 3. Communicative strategy of the initiator A (GA =B will do D).  
 
 
Let us present two communicative strategies in Figures 
3 and 4, respectively, for A and for B. It is assumed here 
that their initial communicative goals are similar—that B 
decides to do D. Still, there can be some obstacles for B 
(resources are missing, or D is prohibited and its doing will 
be punished, etc.). In negotiation, A has to demonstrate how 
the obstacles can be crossed over. 
Both A and B can indicate that the finishing conditions 
are fulfilled: (1) the communicative goal is already 
achieved, (2) give up regardless of having new arguments 
or counter arguments, (3) there are no more arguments to 
continue the fixed tactics but no new tactics will be chosen 
regardless of having some tactics not implemented so far, 
(4) all the tactics are already implemented and all the 
possible arguments are used without achieving the 
communicative 
goal. 
We 
assume 
here 
that 
the 
communicative tactics and arguments can be used only 
once. 
 
 
• 
Determine the initial wB  in relation to D 
• 
Choose an input determinant (WISH, NEEDED, 
or MUST) which determines a reasoning procedure 
depending on wB  
REPEAT 
Analyze A’s utterance  
  CASE A’s utterance OF  
resources: increase resources 
pleasantness: increase pleasantness 
unpleasantness: decrease unpleasantness 
usefulness: increase usefulness 
harmfulness: decrease harmfulness 
punishment for not doing an obligatory D: increase 
punishment 
punishment for doing a prohibited D: decrease 
punishment 
  END CASE 
Update wB 
Change the reasoning procedure? IF yes THEN choose 
a new procedure  
Run the current reasoning procedure in updated wB  
Choose and present a new (counter) argument 
depending on the result of the reasoning procedure  
UNTIL B’s current reasoning procedure gives the 
decision, which matches to GB in the current wB (A and B 
achieved their joint goal), OR whether A or B abandoned 
their joint goal, OR B postpones the decision (A and B did 
not yet achieve their joint goal).  
Figure 4. Communicative strategy of the partner B  (GB= GA=B will do D). 
 
If A or B gives up then the communicative goal will be 
not achieved. If B postpones her decision at the end of 
dialogue then there are neither winners nor losers. 
Questions can be asked by participants in order to make 
choices among different propositions that can be used in 
argumentation. 
180
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

C. 
Information States in Negotiation Process 
If both A and B are conversational agents then let us 
assume the availability of following knowledge [27]: 
1) a set G of communicative goals where both 
participants choose their initial goals (GA and GB, 
respectively). In our case, GA = GB= “B decides to do D”  
2) a set S of communicative strategies of the 
participants. A communicative strategy is an algorithm used 
by a participant for achieving his/her communicative goal. 
This algorithm determines the activity of the participant at 
each communicative step 
3) a set T of communicative tactics, i.e., methods of 
influencing the partner when applying a communicative 
strategy. For example, A can entice, persuade, or threaten B 
in order to achieve the goal GA, i.e., A attempts to 
demonstrate that achieving this goal is, accordingly, more 
pleasant than unpleasant, more useful than harmful, or 
obligatory for B 
4) a set R of reasoning procedures, which are used by 
participants when reasoning (here: about doing an action 
D). A reasoning procedure is an algorithm that returns the 
positive or negative decision about the reasoning object (the 
action D) 
5) a set P of participant models, i.e., a participant’s 
depictions of the beliefs of himself/herself and his/her 
partner in relation to the reasoning object: P = {PA(A), 
PA(B), PB(A), PB(B)} 
6) a set of world knowledge  
7) a set of linguistic knowledge. 
 
A conversational agent passes several information states 
during interaction starting from the initial state and going to 
every next state by applying update rules. Information 
states represent cumulative additions from previous actions 
in the dialogue, motivating future actions. There are two 
parts of an information state of a conversational agent [8] 
—private (information accessible only for the agent) and 
shared (accessible for both participants). 
The private part of an information state of the 
conversational agent A (dialogue initiator) consists of the 
following information: (a) current model of the partner B, 
(b) communicative tactics tiA, which A has chosen for 
influencing B, (c) the reasoning model rj, which A is trying 
to trigger in B and bring it to the positive decision (it is 
determined by the chosen tactics, e.g., when persuading, A 
tries to increase B’s need to do D), (d) a set of dialogue acts 
DA={d1A, d2A, …, dnA}, which A can use, (e) a set of 
utterances for increasing or decreasing the values of B’s 
attitudes in relation to D (arguments for/against of doing D) 
U={ui1A, ui2A, …, uikiA}.  
The shared part of an information state contains (a) a 
set of reasoning models R={r1,…,rk}, (b) a set of 
communicative tactics T={t1, t2, …, tp}, and (c) dialogue 
history p1:u1[d1], p2:u2[d2],…, pi:ui[di] where p1=A; p2, etc. 
are A or B.  
A stack is used keeping (sub-)goals under consideration. 
In every information state, the stack contains an aspect of D 
under consideration (e.g., when A is persuading B then the 
usefulness is on the top). 
Two categories of update rules are at disposal of 
conversational agent for moving from current information 
state into the next one: (1) for interpreting the partner’s 
turns and (2) for generating its own turns. For example, 
there are the following rules for the initiator A in order to 
generate its turns: 
1) for the case if the title aspect of the used tactics is 
located on top of the goal stack (e.g., for the tactics 
of persuasion, the title aspect is usefulness) 
2) for the case if another aspect is located over the 
title aspect of the used tactics (e.g., if A is trying to 
increase the usefulness of D for B but B argues for 
unpleasantness, then the unpleasantness lies over 
the usefulness) 
3) for the case if there are no more utterances for 
continuing the current tactics (and new tactics 
should be chosen if possible)  
4) for the case if A has to abandon its goal  
5) for the case if B has made the positive decision 
and therefore, A has reached the goal. 
Special rules exist for updating the initial information 
state. 
D. Implementation 
A simple dialogue system is implemented that carries 
out negotiations with a user in a natural language about 
doing an action [27]. The participants can have different 
initial goals: e.g., the initiator (either DS or a user) tries to 
achieve the decision of the partner to do the action but the 
partner’s goal can be opposite. DS interacts with a user 
using texts in a natural language. There are two work 
modes. In one mode, the computer is playing A’s and in the 
other—B’s role. 
Both A and B have access to a common set of reasoning 
procedures. They also use fixed sets of dialogue acts and 
the corresponding utterances in a natural language which 
are pre-classified semantically, e.g., the set Pmissing_resources 
for indicating that some resources for doing a certain action 
D are missing (e.g., I don't have proper dresses, see 
Example 4), Pincreasing_resources for indicating that there exist 
resources (e.g., The company will cover all your expenses), 
Pincreasing_usefulness for stressing the usefulness of D (e.g., You 
can be useful for the company), etc. Therefore, no linguistic 
analysis or generation will be made during a dialogue in 
current implementation. The utterances will be accidentally 
chosen by conversational agent from the suitable semantic 
classes (in our implementation, every utterance can be used 
only once). However, these restrictions will bring along that 
the generated dialogues are not quite coherent.  
If A’s goal is “B will do D” then A, starting interaction, 
generates, by using his knowledge, a partner model wAB and 
determines the communicative tactics T, which he will use 
(e.g., persuasion), i.e., he accordingly fixes the reasoning 
procedure R, which he will try to trigger in B’s mind (e.g., 
NEEDED). B (if being another conversational agent) has 
her own model wB (which exact values A does not know). B 
in her turn determines a reasoning procedure RB that she 
181
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

will use in order to make a decision about doing D, and her 
communicative tactics TB. 
When attempting to direct B’s reasoning to the positive 
decision (do D), A presents several arguments stressing the 
positive and downgrading the negative aspects of D. 
Preparing an argument, A triggers the current reasoning 
procedure in his partner model wAB, in order to be sure that 
the reasoning will give the positive decision. B can use the 
same or a different reasoning procedure triggering it in the 
model of herself wB. After the changes made by both the 
participants in the two models during a dialogue, the 
models will approach each to other but, in general, do not 
equalise. Although, the results of reasoning in both models 
can be (or not be) equal. 
1) Upgrading the model wAB 
Let us consider a dialogue with our DS (Example 4). 
Here A is conversational agent playing the role of the boss 
of a company and B is the user playing the role of an 
employee of the company who is at the same time studying 
at the university. A presents arguments for doing D by B 
(D=travel to N. to conclude a contract). He succeeds to 
decline B’s counter arguments and convince B to accept its 
goal. 
 
Example 4 
1.A: The company offers you a trip to 
N. Our company needs to conclude a 
contract there.  
2.B: I don't have proper dresses.  
3.A: 
The 
company 
will 
pay 
your 
executive expenses. You can be useful 
for the company.  
4.B: I can have some problems at my 
university.  
5.A: It's all right–your examinations 
period will be extended. The company 
will evaluate your contribution.  
6.B: OK, I’ll do it.  
7.A: I am glad.  
 
Let us examine how the partner model is used in the 
dialogue. A will implement the tactics of persuasion and 
generates a partner model, let it be 
 
wAB={wAB(resources)=1, 
wAB(pleasant)=4, 
wAB(unpleasant)=2, 
wAB(useful)=5, 
wAB(harmful)=2, 
wAB(obligatory)=0, wAB(prohibited)=0, wAB(punishment-
do)=0, wAB(punishment-not)=0}. 
 
The reasoning procedure NEEDED (Figure 2 above) 
yields a positive decision in this model. A’s initial 
information state is as follows. 
 
Private part 
•initial partner model wAB = (1, 4, 2, 5, 2, 0, 0, 0, 0) 
•the tactics chosen by A–persuasion 
•A will use the reasoning procedure NEEDED, the 
presumption is fulfilled: wAB(useful) > wAB(harmful) 
•the set of dialogue acts at A’s disposal: {proposal; 
arguments for increasing/decreasing values of different 
coordinates of wAB; accept; reject} 
•the set of utterances for expressing the dialogue acts at 
A’s disposal: {The company offers you a trip to N, You can 
be useful for the company, etc.}. 
Shared part  
•the reasoning procedures WISH, NEEDED, MUST 
•the tactics of enticement, persuasion, threatening 
•dialogue history–an empty set. 
 
Let us suppose here that every statement (argument) 
presented in dialogue will increase or respectively, decrease 
the corresponding value in the model of beliefs by one unit. 
Still, this is a simplification because different arguments 
might have different weights for different dialogue 
participants. 
Table 1 demonstrates how the partner model is 
changing during the dialogue.  
Conversational agent A starts the dialogue with a 
proposal. Using the tactics of persuasion and attempting to 
trigger the reasoning procedure NEEDED in B, it adds an 
argument for increasing the usefulness to the proposal (turn 
1). At the same time, it increases the initial value of the 
usefulness in its partner model wAB by 1. The current 
reasoning procedure NEEDED still gives a positive 
decision in the updated model. A does not know the actual 
values of attitudes, which B has assigned in the model wB of 
herself. As caused by every counter argument that B will 
present, A has to update the partner model wAB.  
However, B’s counter argument (turn 2) demonstrates 
that B actually has resources missing (I don't have proper 
dresses) therefore, A has to decrease the value of 
wAB(resources) from 1 to 0 in its partner model. Now A 
must find an argument indicating that the resources are 
available: 
it 
selects 
an 
utterance 
from 
the 
set 
Pincreasing_resources (The company will pay your executive 
expenses) and following the tactics of persuasion it adds an 
argument for increasing the usefulness (You can be useful 
for the company) in turn 3. The value of wAB(resources) 
will now be 1 and the value of wAB(useful) will be increased 
by 1 in the partner model. The reasoning in the updated 
model gives a positive decision. 
Nevertheless, B has a new counter argument indicating 
the harmfulness of the action: I can have some problems at 
my university (turn 4).  
Now A has to increase the value wAB(harmful) in the 
partner model, it turns out that by 6 not by 1 as was 
assumed by default. Let us explain why. So far, A was 
supposing that D is not prohibited for B. This assumption 
proves to be wrong because otherwise it would be 
impossible for B to indicate the harmfulness of D (if she is 
applying the reasoning procedure NEEDED as A supposes, 
see Figure 2). Therefore, B supposedly compares the values 
of beliefs at the step 4 of the procedure and makes a 
negative decision. B can come to the step 4 only after the 
step 3 where she detects that D is prohibited and doing D 
involves a punishment (turn 4).  
182
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Table 1. Updating the partner model wAB by A in argumentation dialogue with B (A implements the reasoning procedure NEEDED)  
Dialogue history 
Partner model wAB 
Comments 
Re-
sour-
ces 
Pl
ea
sa
nt 
Un-
plea-
sant 
Use-
ful 
Ha
rm
ful 
Obli
ga-
tory 
Proh
ibite
d 
Puni
shm
ent_
do 
Puni
shm
ent-
not 
 
1 
4 
2 
5 
2 
0 
0 
0 
0 
Initial model 
1.A: The company 
offers you a trip to 
N.  
Our company needs to 
conclude an agreement 
there. statement for 
usefulness 
1 
4 
2 
5+1 
2 
0 
0 
0 
0 
A makes a 
proposal; 
usefulness 
increases 
2.B: I don't have 
proper dresses. 
statement of missing 
resources 
0 
4 
2 
6 
2 
0 
0 
0 
0 
Resources 
missing 
3.A: The company will 
pay your executive 
expenses. statement 
of existence of 
resources  
You can be useful for 
the company.     
statement for 
usefulness 
1 
4 
2 
6+1 
2 
0 
0 
0 
0 
Resources 
exist, 
usefulness 
increases 
once more 
4.B: I can have some 
problems at my 
university.    
statement for harm-
fulness 
1 
4 
2 
7 
2+
6 
0 
1 
1 
0 
Harmfulness 
has to be 
increased by 6 
because B’s 
reasoning has 
given a 
negative 
decision  
5.A: It's all right - 
your examinations 
period will be 
extended. statement 
against harmfulness   
The company will 
evaluate your 
contribution. state-
ment for usefulness 
1 
4 
2 
7+1 
8-
1 
0 
1 
1 
0 
Harmfulness 
decreases, 
usefulness 
increases 
once more 
6.B: OK, I’ll do it. 
agreement 
1 
4 
2 
8 
7 
0 
1 
1 
0 
Final model 
7.A: I am glad. 
accept 
 
 
 
 
 
 
 
 
 
A has 
achieved the 
goal 
 
Therefore, A changes the value of wAB(prohibited) from 0 
to 1 and increases the value of wAB(punishment-do) in the 
partner model at least by 1. (Being optimistic, A increases the 
value exactly by 1 and not more.) Now A checks, how to 
change the value of the harmfulness in the partner model in 
order to get the negative decision like B did. According to the 
reasoning procedure NEEDED A calculates that the value has 
to be increased (at least) by 6. Therefore, wAB(harmful) will 
be 2+6=8. Responding to B’s counter argument, A decreases 
the value of wAB(harmful) by 1 using the utterance It's all 
right - your examinations period will be extended, and 
increases the value of wAB(useful) once more using the 
utterance The company will evaluate your contribution (turn 
5). The reasoning procedure NEEDED gives a positive 
decision in the updated partner model. Now it turns out that B 
has made this same decision (turn 6). A has achieved its 
communicative goal and finishes the dialogue (turn 7). Table 
1 demonstrates how A is updating the partner model wAB in 
argumentation dialogue with B. As compared with the initial 
model, the values of four aspects have been increased: 
w(usefulness) from 5 to 8, w(harmfulness) from 2 to 7, 
w(prohibited) and w(punishment-do) from 0 to 1. 
183
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Table 2. Updating the model wB by B in argumentation dialogue with A (B implements the reasoning procedure WISH).  
Dialogue history 
The model wB 
Comments 
Re-
sour-
ces 
Pl
ea
sa
nt 
Un-
plea-
sant 
Us
e-
ful 
Ha
rm
ful 
Obli
ga-
tory 
Proh
ibite
d 
Puni
shm
ent_
do 
Pu-
nish-
ment
-not 
 
0 
6 
4 
3 
4 
0 
1 
0 
3 
Initial model; 
reasoning 
procedure WISH 
gives a negative 
decision 
1.A: The company 
offers you a trip to 
N in order to 
conclude a contract.  
proposal 
Our company needs to 
conclude an agreement 
there. 
0 
6 
4 
3+
1 
4 
0 
1 
0 
3 
A makes a 
proposal; 
usefulness 
increases 
2.B: I don't have 
proper dresses. 
statement of missing resources 
0 
6 
4 
4 
4 
0 
1 
0 
3 
Resources mis-
sing 
3.A: The company will 
pay your executive 
expenses. You can be 
useful for the 
company.  
1 
6 
4 
4+
1 
4 
0 
1 
0 
3 
Resources exist, 
usefulness 
increases once 
more 
4.B: I can have some 
problems at my 
university. statement for 
harmfulness 
1 
6 
4 
5 
4 
0 
1 
0 
3 
Harmfulness too 
big, B’s reasoning 
has given a 
negative decision  
5.A: It's all right - 
your examinations 
period will be 
extended. statement 
against harmfulness   
The company evaluates 
your contribution. 
1 
6 
4 
5+
1 
4-
1 
0 
1 
0 
3 
Harmfulness 
decreases, 
usefulness 
increases once 
more 
6.B: OK, I’ll do it. 
agreement 
1 
6 
4 
6 
3 
0 
1 
0 
3 
Final model; 
WISH gives a 
positive decision 
7.A: I am glad. 
accept 
 
 
 
 
 
 
 
 
 
The common goal 
is achieved  
 
 
2) 
Upgrading the model wB 
In argumentation dialogue, A is updating his partner 
model wAB. At the same time, B has to update the model wB of 
herself as caused by the arguments presented by A. Similarly 
with A, who does not know the exact values of B’s beliefs in 
wB, also B does not know the exact values of beliefs in the 
model wAB. Both participants can make inferences only from 
arguments presented by the partner. 
Does the final model wAB coincide with B’s actual model 
wB, i.e., has A correctly guessed all the actual weights of B’s 
beliefs? The answer is “no”. Let us discuss why. 
Let us again consider Example 4 and Table 1. Let us 
suppose that B also is a conversational agent (not a human 
user) and that B’s actual model is wB = (0, 6, 4, 3, 4, 0, 1, 0, 3) 
at the beginning of the dialogue (different from wAB as in 
Table 1). In addition, let us suppose that B’s communicative 
goal coincides with A’s one (is not opposite)—B has a wish 
to do D (doing D is more pleasant than unpleasant). It triggers 
a reasoning procedure WISH (Figure 1 above) in its model of 
beliefs in order to check the resources and other aspects of 
doing D and to make a decision. The decision will be 
negative because B does not have enough resources (I don't 
have proper dresses). 
Table 2 demonstrates the updates made by B in the model 
wB during the dialogue as affected by A’s arguments. We 
suppose here that A’s arguments will increase/decrease the 
corresponding weights by one unit (this same assumption was 
made for A in the case of B’s counter arguments). B uses the 
reasoning procedure WISH. The initial model wB gives a 
184
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

negative decision but the updated final model gives a positive 
decision. 
In this way, A is able to convince B to do D if he has 
enough arguments for doing D and his initial picture of B 
does not radically differentiate from B’s actual beliefs. Both 
the beliefs in the partner model wAB and B’s actual beliefs in 
the model wB of herself (if B is a conversational agent 
similarly with A) are changing during the dialogue as 
influenced by the arguments presented by the participants. 
Although the models wAB and wB do not necessarily coincide 
at the end of the dialogue, the proportions of the values of the 
positive (pleasantness, etc.) and negative aspects of doing D 
(unpleasantness, etc.) will be similar. Still, if B (or A) is a 
human user then she (or he) is not obliged to use the models 
and algorithms. 
V. DISCUSSION 
Our model of conversational agent is motivated by the 
analysis of human-human negotiations. We consider the 
dialogues where two participants A and B negotiate doing an 
action D. In the analyzed telemarketing calls, the 
communicative goal of a sales clerk of the educational 
company is to convince a customer to order and pass a 
training course offered by the company. The customer can 
either adopt or not this goal.  
If the participants are collaborative and one of them 
presents his/her argument then the partner mostly accepts it. 
If the participants are antagonistic then at least one of them 
does not agree with the opinion of the partner and presents 
his/her counterargument(s). The more the clerk knows about 
the customer, the more convincing arguments is he able to 
choose. Asking questions is a way to learn more about the 
communication partner. 
The most important phase of a telemarketing call is a 
clerk’s argumentation for taking a training course. Arguments 
of sales clerks are presented as assertions and customers can 
accept or reject them. It is remarkable that the customers 
usually accept assertions of clerks—it shows that the clerks 
succeed to choose ‘right’ arguments. Still, B’s accept is 
usually followed by additional information that can be 
interpreted as a counter argument. The argumentation chain 
looks like 
 
A: argument1 – B: accept1 + counter argument1 
… 
A: argumentn – B: acceptn + counter argumentn.  
 
The situation is different when B is steering to a negative 
decision (one single conversation in the whole analyzed 
corpus). Then B does not accept A’s assertion/argument and 
takes over the initiative starting to present assertions/counter 
arguments herself. A always accepts B’s assertions but he 
provides his arguments as additional information. The 
argumentation chain looks like 
 
A: argument1 – B: reject1 + counter argument1 
A: accept2 + argument2 – B: reject2 + counter argument2 
…  
A: acceptn + argumentn – B: rejectn + counter argumentn. 
 
When reasoning about doing an action, a subject is 
weighing different aspects of the action (its pleasantness, 
usefulness, etc.), which are included into his/her model of 
motivational sphere. In the model presented here, we evaluate 
these aspects by giving them discrete numerical values on the 
scale from 0 to 10. Still, people do not use numbers but rather 
words of a natural language, e.g., excellent, very pleasant, 
harm, etc. Further, when reasoning, people do not operate 
with exact values of the aspects of an action but they rather 
make ‘fuzzy calculations’, for example, they suppose/believe 
that doing an action is more pleasant than unpleasant and 
therefore they wish to do it. Another problem is that the 
aspects of actions considered here do not be fully 
independent. For example, harmful consequences of an action 
as a rule are unpleasant. In addition, if the reasoning object is 
different (not doing an action like in our case) then the beliefs 
of a reasoning subject can be characterized by a different set 
of aspects. 
When attempting to direct B’s reasoning to the desirable 
decision, A presents several arguments stressing the positive 
and downgrading the negative aspects of D. The choice of A’s 
argument is based on one hand, on the partner model, which 
captures A’s knowledge about B, and on the other hand, on 
the (counter) argument presented by B. Still, B is not obliged 
to present any counter argument but she can simply refuse 
(e.g., I do not do this action). When choosing the next 
argument supporting D, A triggers a reasoning procedure in 
his partner model depending on the chosen communicative 
tactics, in order to be sure that the reasoning will give a 
positive decision after presenting this argument. B herself can 
use the same or a different reasoning procedure triggering it 
in her own model. After the updates made both by A and B in 
the two models during a dialogue, the models will approach 
each to another but, in general, do not equalize. Nevertheless, 
the results of reasoning in both models can be similar, as 
demonstrated in Example 4. Therefore, A can convince B to 
do D even if not having a perfect picture of her. 
Our dialogue model considers only limited kinds of 
dialogue but nonetheless it illustrates the situation where the 
dialogue participants are able to change their beliefs related to 
the negotiation object and bring them closer one to another by 
using arguments. The initiator A does not need to know 
whether the counter arguments presented by the partner B 
have been caused by B’s opposite initial goal or are there 
simply obstacles before their common goal, which can be 
eliminated by A’s arguments. A’s goal, on the contrary is not 
hidden from B. Secondly, the different communicative tactics 
used by A are aimed to trigger different reasoning procedures 
in B’s mind. A can fail to trigger the pursued reasoning 
procedure in B but however he can achieve his 
communicative goal when having a sufficient number of 
arguments supporting his initial goal. 
185
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

In our implemented DS, the user interacts with the 
computer, choosing ready-made, semantically pre-classified 
sentences as arguments and counter arguments for and against 
doing a certain action. Nevertheless, we suppose that such 
kind of software is useful when training the skills of finding 
arguments for and against of doing an action. The computer 
can establish certain restrictions on the argument types and on 
the order in their use. Still, when interacting with the 
computer, a human user does not use neither a formal partner 
model, nor a formal model of herself, nor reasoning 
procedures. However, both implementation modes allow 
study how the beliefs of the participants are changing in 
negotiation. 
VI. CONCLUSION AND FUTURE WORK 
We analyse human negotiations in order to explain how 
arguments are used to convince a dialogue partner. We 
consider human-human telemarketing calls where a sales 
clerk of an educational company proposes training courses to 
a customer who possibly does not want to order any course. 
When starting a dialogue, the sales clerk determines a certain 
way to realize his communicative strategy—communicative 
tactics—and retains them during a dialogue. The customer 
can change her strategy during conversation. 
We study dialogues where one participant (initiator of 
interaction A) has a communicative goal that the partner (B) 
will decide to do an action D. B’s communicative goal can be 
similar or opposite (“do not do D”). When reasoning about 
doing D, B considers different positive and negative aspects 
of D. If the positive aspects weigh more the decision will be 
“do D”. If the negative aspects weigh more the decision will 
be “do not do D”. The initiator A chooses a suitable 
communicative strategy and tactics in order to influence B’s 
reasoning and achieve the positive decision: he stresses the 
positive and downgrades the negative aspects of doing D. 
Different arguments are presented in a systematic way, e.g., A 
stresses time and again the usefulness of D.  
Initial communicative goals of the participants can be 
similar or opposite. The partners present arguments for and 
against of doing D. The arguments of the initiator A are based 
on his partner model wAB whilst B’s arguments—on her model 
of herself wB. Both models include beliefs about the 
resources, positive and negative aspects of doing D that have 
numerical values in our implementation. Both models are 
updated during a dialogue.  
In our implementation, the user interacts with the 
computer, choosing ready-made, semantically pre-classified 
sentences as arguments and counter arguments for and against 
of performing a certain action. We believe that this kind of 
software is useful when training the argumentation skills—the 
programme can establish certain restrictions on the argument 
types, the order in the use of arguments and counter 
arguments, etc. It allows study how beliefs of the participants 
are changing in argumentation dialogue. 
Our further aim is to develop the DS concentrating 
foremost on the reasoning model. So far, we are using an 
intuitive (naïve) reasoning theory. However, there are several 
other approaches to model change of a person’s opinion, e.g., 
Elaboration Likelihood Model, Social Judgment Theory, and 
Social Impact Theory. Some of the theories can be better to 
model human reasoning. Our further research will explain this. 
We also plan to add NLP for Estonian in order to achieve more 
natural communication between DS and the user. The results of 
the study can be used in various domains of activity, when 
training people to carry out negotiations. 
ACKNOWLEDGMENT 
This work was supported by the European Union through 
the European Regional Development Fund (Centre of 
Excellence in Estonian Studies). 
REFERENCES 
[1] 
M. Koit, “Reasoning and Arguments in Negotiation. Developing 
a Formal Model,” Proc. of Cognitive 2022, ThinkMind, pp. 17-
22. 
Available 
from 
https://www.thinkmind.org/-
index.php?view=article&articleid=cognitive_2022_1_30_40017 
(5.12.2022) 
[2] 
I. Rahwan, P. McBurney, and L. Sonenberg, “Towards a theory 
of negotiation strategy (a preliminary report),” S. Parsons and P. 
Gmytrasiewicz (Eds.), Game-Theoretic and Decision-Theoretic 
Agents (GTDT), Proc. of an AAMAS-2003 Workshop, pp. 73–
80, 2003. Melbourne, Australia. 
[3] 
A. Rosenfeld, I. Zuckerman, E. Segal-Halevi, O. Drein, and S. 
Kraus, “NegoChat: A Chat-based Negotiation Agent,” Proc. of 
the International Conference on Autonomous Agents and Multi-
agent Systems AAMAS’14, pp. 525–532, 2014. 
[4] 
M. Lewis, D. Yarats, Y.N. Dauphin, D. Parikh, and D. Batra, 
“Deal or No Deal? End-to-End Learning for Negotiation 
Dialogues,” Proc. of the Conference on Empirical Methods in 
Natural Language Processing, pp. 2443–2453. Copenhagen, 
Denmark, Assoc. for Computational Linguistics, 2017. 
[5] 
D. Traum, “Computational Approaches to Dialogue,” The 
Routledge Handbook of Language and Dialogue, 2017. 
[6] 
J. Bos, E. Klein, O. Lemon, and T. Oka, “DIPPER: Description 
and Formalisation of an Information-State Update Dialogue 
System Architecture,” Proc. of the SIGDial Workshop on 
Discourse and Dialogue, pp. 115-124. Sapporo, 2003. Available 
from 
http://sigdial.org/workshops/workshop4/proceedings/11_SHOR
T_bos_dipper.pdf (5.12.2022) 
[7] 
D. Jurafsky and J.M. Martin, Speech and Language Processing: 
An Introduction to Natural Language Processing, Computational 
Linguistics, and Speech Recognition. Prentice Hall, 2009. 
[8] 
D. Traum and S. Larsson, “The Information State Approach to 
Dialogue Management,” Current and New Directions in 
Discourse and Dialogue, pp. 325–353, 2003. 
[9] 
I. Rahwan, S.D. Ramchurn, N.R. Jennings, P. Mcburney, S. 
Parsons, and L. Sonenberg, “Argumentation-based negotiation,” 
The Knowledge Engineering Review, vol. 18(4), pp. 343–375, 
Cambridge 
University 
Press, 
2004. 
Available 
from 
http://dx.doi.org/10.1017/S0269888904000098 (5.12.2022) 
[10] T. Yuan, D. Moore, C. Reed, A. Ravenscroft, and N. Maudet, 
“Informal 
Logic Dialogue 
Games in Human-Computer 
Dialogue,” Knowledge Engineering Review, 26(2), pp. 159-174, 
2011. 
[11] K.R. Scherer. “What are emotions? And how can they be 
measured?” Social Science Information, 2005. Available from 
http://journals.sagepub.com/doi/abs/10.1177/539018405058216 
(5.12.2022) 
186
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[12] R.E. Petty, P. Briñol, J. Teeny, and J. Horcajo, “The elaboration 
likelihood model: Changing attitudes toward exercising and 
beyond,“ B. Jackson, J. Dimmock, & J. Compton (Eds.), 
Persuasion and communication in sport, exercise, and physical 
activity, pp. 22-37, Abington, UK:  Routledge, 2018. 
[13] N.R. Jennings, S. Parsons, P. Noriega, and C. Sierra, “On 
argumentation-based negotiation,” Proc. of the International 
Workshop on Multi-Agent Systems, Boston, pp. 1–7, 1998. 
[14] C. Hadjinikolis, S. Modgil, E. Black, P. Mcburney, and M. 
Luck, “Investigating Strategic Considerations in Persuasion 
Dialogue Games,” Frontiers in Artificial Intelligence and 
Applications, vol. 241, STAIR, pp. 137–148, IOS Press, 2012. 
DOI: 10.3233/978-1-61499-096-3-137. 
[15] L. Amgoud and C. Cayrol, “A Reasoning Model Based on the 
Production of Acceptable Arguments,” Ann. Math. Artif. Intell. 
34(1-3), pp. 197-215, 2002. 
[16] C Hadjinikolis, Y Siantos, S Modgil, E Black, and P. 
McBurney, “Opponent modelling in persuasion dialogues,” 
Proc. of the Twenty-Third international joint conference on 
Artificial Intelligence, pp. 164-170, 2013. 
[17] B. Ravenet, A. Cafaro, B. Biancardi, M. Ochs, and C. 
Pelachaud, “Conversational Behavior Reflecting Interpersonal 
Attitudes in Small Group Interactions,” IVA, pp. 375–38,. 2015. 
[18] J. Gratch, S. Hill, L.-P. Morency, D. Pynadath, and D. Traum, 
“Exploring the Implications of Virtual Human Research for 
Human-Robot Teams,” R. Shumaker and S. Lackey (Eds.), 
VAMR’15, International Conference on Virtual, Augmented 
and Mixed Reality, pp. 186–196, Springer, 2015. Available from 
https://link.springer.com/chapter/10.1007/978-3-319-21067-
4_20 (5.12.2022) 
[19] M. Saberi, S. DiPaola, and U. Bernardet, “Expressing 
Personality Through Non-verbal Behaviour in Real-Time 
Interaction,” Frontiers in Psychology, vol 12, pp. 54-74, 2021. 
[20] S. Dermouche, “Computational Model for Interpersonal Attitude 
Expression,” ICMI, pp. 554–558, 2016. 
[21] K. Jokinen, “Exploring Boundaries among Interactive Robots 
and Humans,” D’Haro, L.F, Callejas, Z., Nakamura, S. (Eds.), 
Conversational Dialogue Systems for the Next Decade, LNEE, 
vol. 704, pp. 271-275, Springer, Singapore, 2020.  
[22] T. Hennoste, O. Gerassimenko, R. Kasterpalu, M. Koit, A. 
Rääbis, and K. Strandson, “From Human Communication to 
Intelligent User Interfaces: Corpora of Spoken Estonian,” Proc. 
of LREC’08, European Language Resources Association 
(ELRA), pp. 2025–2032, 2008, Marrakech, Morocco, 2008. 
Available from http://www.lrec-conf.org/proceedings/lrec2008/ 
(5.12.2022) 
[23] J. Sidnell, Conversation Analysis: An Introduction, London: 
Wiley-Blackwell, 2010. 
[24] M. Koit and H. Õim, “A Computational Model of 
Argumentation in Agreement Negotiation Processes,” Argument 
& Computation, 5 (2-3), pp. 209–236, Taylor & Francis Online, 
2014. 
Available 
from 
https://www.tandfonline.com/doi/abs/10.1080/19462166.2014.9
15233 (5.12.2022) 
[25] M.E. Bratman, Intention, Plans, and Practical Reason, CSLI 
Publications, 1999. 
[26] H. Õim, “Naïve Theories and Communicative Competence: 
Reasoning in Communication,” Estonian in the Changing 
World, pp. 211–231, 1996. 
[27] M. Koit, “Reasoning and communicative strategies in a model 
of argument-based negotiation,” Journal of Information and 
Telecommunication TJIT, 2, 14 p. Taylor & Francis Online, 
2018. 
Available 
from 
https://www.tandfonline.com/-
doi/full/10.1080/24751839.2018.1448504 (5.12.2022) 
 
187
International Journal on Advances in Intelligent Systems, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/intelligent_systems/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

