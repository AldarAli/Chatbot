Interactive Dynamic Simulations with Co-Located
Maglev Haptic and 3D Graphic Display
Peter Berkelman
Department of Mechanical Engineering
University of Hawaii at Manoa
Honolulu Hawaii, USA
Email: peterb@hawaii.edu
Sebastian Bozlee
Mathematics Department
Unversity of Portland
Portland Oregon, USA
Email: sebbyj@gmail.com
Muneaki Miyasaka
Department of Mechanical Engineering
University of Washington
Seattle Washington, USA
Email: muneaki@uw.edu
Abstract—We have developed an system which combines
realtime dynamic simulations, 3D display, and magnetic lev-
itation to provide high-ﬁdelity co-located haptic and graphic
interaction. Haptic interaction is generated by a horizontal
array of cylindrical coils which act in combination to produce
arbitrary forces and torques in any direction on magnets
ﬁxed to an instrument handle held by the user, according
to the position and orientation sensed by a motion tracking
sensor and the dynamics of a realtime physical simulation.
Co-located graphics are provided by a thin ﬂat screen placed
directly above the coil array so that the 3D display of virtual
objects shares the same volume as the motion range of the
handheld instrument. Shuttered glasses and a head tracking
system are used to preserve the alignment of the displayed
environment and the interaction handle according to the user’s
head position. Interactive demonstration environments include
rigid bodies with solid contacts, suspended mass-spring-damper
assemblies, and deformable surfaces.
Keywords-haptics, interaction, simulation
I. INTRODUCTION
The ideal of virtual reality and haptic interfaces is to phys-
ically interact with simulated objects with the highest possi-
ble ﬁdelity in both the graphical display and the kinesthetic
forces and torques sensed by the user during interaction.
Computer-generated graphics can produce highly realistic,
dynamic 3D imagery in real time, but haptic interfaces are
generally based on single point contact feedback, tactile
cues, and linkage devices which have various limitations
in their force and motion ranges, frequency response band-
width, and resolution.
Our system combines a graphical display with a large
range of motion magnetic levitation device, as shown in Fig.
1. The graphical display is placed directly above a horizontal
array of cylindrical coils and underneath the instrument
handle held by the user, so that electromagnetic forces and
torques can be generated on magnets embedded in the handle
as the instrument is moved by the user into contact with the
displayed simulated environment.
The magnets in the instrument handle, the coil array with
its current ampliﬁers, and the motion tracking sensor with
its infrared LED markers, function together as a magnetic
Figure 1.
Co-Located Maglev Haptic and 3D Graphic Display
levitation system. Its motion range is approximately 100x100
mm horizontally, up to 75 mm vertically, with unlimited yaw
and tilt up to 40 degrees.
A secondary, slower and less precise motion tracking
system tracks the position of the user’s head so that the
3D views are generated correctly according to the position
of each of the user’s eyes. A pair of shuttered glasses,
synchronized to the update rate of the graphics on the
monitor, is worn by the user so that each eye sees a
different image as the shutters alternate. In practice, this
head tracking system allows the user to observe the handheld
instrument and the 3D displayed environment together from
the side and from above, in a natural ergonomic position
for hand-eye coordination during dextrous manipulation of
a handheld instrument or tool. Examples of relevant dextrous
324
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

tool manipulation tasks include any writing, carving, or
cutting tasks, operation of wrenches or screwdrivers, and
medical needle manipulation for suturing, injections, and
biopsy.
The real-time haptic interaction and graphical display are
generated from a dynamic simulation which must perform
collision detection, ﬁnite element deformation, and haptic
rendering sufﬁciently quickly to support graphical updates
at 30-60 Hz and haptic interaction and magnetic levitation
at 800-1000 Hz. These tasks are sufﬁciently computationally
intensive to be the limiting factor regarding the resolution
and realism of the simulated environment.
II. BACKGROUND
The realization of our interactive system depends on the
performance and integration of technology in the areas of
maglev haptics, graphics, and physical simulation. Relevant
prior work in each of these areas is surveyed below.
A. Co-Located Haptics and Graphics
3D graphics and haptic force and/or torque feedback can
be generated at the same location by simply placing the 3D
display behind the haptic interaction device, however this
method has two drawbacks. First, the body of the haptic
interface device partially occludes the display, and second,
there may be a signiﬁcant difference produced between the
perceived location of the displayed imagery and the surface
of the screen, so that the convergence and focal distance of
the user’s eyes do not match, which is unnatural and may
cause discomfort to the user.
ReachIn, Immersivetouch, and SenseGraphics systems [1]
use a partially silvered mirror between the head and hand of
the user, so that the display can be moved out of the way and
the focal and convergence distances of the user’s eyes can
be matched. The haptic device and the user’s hand do not
occlude the 3D graphics behind them, but rather the real and
virtual environments are superimposed and semitransparent
due to the half-silvered mirror, which may be a distraction
to the user.
The ”what you see is what you feel” system [2] uses a
thin ﬂat display with a camera behind it. The video image
of the user’s hand is then extracted from the camera view
using a green screen chroma-key technique, and rendered in
the virtual environment. Holographic display [3] is another
method which has been used for haptic and graphic co-
location.
Comparative studies have shown [4], [5] evidence of im-
proved perception and performance from co-located haptic
interaction.
B. Haptic Magnetic Levitation
Hollis and Salcudean ﬁrst applied Lorentz force mag-
netic levitation devices [6] to haptic interaction and force-
feedback teleoperation. Lorentz force magnetic levitation
haptic interaction development continued with other more
specialized device designs [7] [8] and larger range devices
developed by Berkelman [9] [10].
The design and function of the magnetic levitation system
used here is described in [11]. This system uses a ﬁxed
planar array of cylindrical coils to levitate a platform of
one or more cylindrical magnets. The yaw of the levitated
platform is unlimited and its horizontal motion range is
determined by the size of the planar array. Vertical levitation
distances of up to 75 mm and tilt angles of 45 degrees are
achievable, depending on the mass of the levitated platform
and the dimensions of the magnets used.
Similar tabletop-scale large range magnetic levitation sys-
tems have been developed for suspension of models in wind
tunnels [12] and for micromanipulation using pole pieces to
shape magnetic ﬁelds [13].
C. Realtime Physical Simulation
Realistic software simulations of dynamic physical envi-
ronments have been developed by Baraff both for rigid [14]
and deformable [15] objects, including efﬁcient collision and
reaction force detection and surface friction. Freely available
physical simulation software packages include the SOFA
framework [16] [17], Bullet Physics, and the PhysX library
from NVIDIA. Higher resolution and performance can be
obtained by using precomputed deformation modes [18] and
6-DOF haptic rendering including torque feedback as well
as force on an interactive instrument can be integrated with
simulations [19].
Several software packages are freely available for hap-
tic rendering and realtime physical simulation. H3D [20]
and Chai3D [21] include driver interfaces for common
commercial haptic interface devices such as the Sensable
Technologies Phantom [22]. A programming interface is also
available with the magnetic levitation haptic interface from
Butterﬂy Haptics LLC [23].
III. IMPLEMENTED SYSTEM
The motion tracking, magnetic levitation control, haptic
rendering, physical simulation, and graphical display in our
current system are all executed in real time in separate
threads on a single quad-core PC in Linux 2.6. GNU C/C++
was used for all programming. An initial demonstration
concept of the system with a simulation of a single paddle
instrument and a ball rolling on a plane, an earlier magnet
and coil conﬁguration, and a conventional 2D display was
demonstrated previously [24]. The current system is shown
in Fig. 2, including the planar 3D display, haptic instrument
handle, current ampliﬁers, and head tracker.
A. Magnetic Levitation
The motion tracking of the handheld instrument in our
system is done using a Northern Digital Optotrak Certus
position sensor and three infrared Smart Markers. Motion
325
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

Figure 2.
Implemented system
tracking updates are provided at 860 Hz with a position
resolution of approximately 0.01 mm for each marker. Ac-
tuation forces and torques are generated by a closely packed
array of 27 cylindrical coils, each with 1000 windings, 25
mm diameter, and 30 mm height. Either a two-magnet or
four-magnet instrument handle can be used with the system;
the two-magnet 125 g instrument can provide greater haptic
forces and torques but is more massive and bulky, and the
smaller 75 g four-magnet instrument occludes the user’s
view of the display less due to its compact size. Forces are
limited to approximately 4 N due to heating of the actuation
coils, although higher momentary peak forces are possible.
The four-magnet instrument is shown in Fig. 3 levitated
above the 27-coil array at a height of 30 mm and a tilt
angle of 20 degrees. This coil array is underneath the 3D
display monitor shown in Fig. 2. The motion tracker for
the haptic instrument is mounted on a rigid frame at ceiling
level, looking downwards.
The design and evaluation methods used in the develop-
ment of the magnetic levitation system are described in [25].
B. 3D Display
The NVIDIA 3D Vision package was used with Linux
drivers to provide 3D display of the simulated environment.
This package uses shutter glasses which are synchronized
with the graphics card by an infrared emitter box. A Quadro
4000 graphics card was used with a ViewSonic vm2268
monitor with a 120 Hz update rate. OpenGL and GLUT
graphics libraries are used for the 3D graphics rendering.
The case of the monitor was removed and backplane
circuit boards and wiring were moved so that the monitor
backlight and display could be placed directly on the coil
array. The combined thickness is under 10 mm, so that haptic
forces and torques can be applied to the handheld instrument
up to a vertical height of at least 60 mm. Magnetic ﬁelds
Figure 3.
Levitated 4-magnet instrument
from the instrument magnets and coil array were not found
to interfere with the display, and there are no ferromagnetic
components in the display to interfere with the magnetic
levitation system. A thin sheet of plastic was placed on the
monitor screen for protection from impacts from the magnets
and instrument.
Head tracking was implemented using a Northern Digital
Polaris Vicra and passive reﬂective markers to produce
correct 3D display according to the position of each eye. The
spatial position and orientation of the shutter glasses from
the positions of four reﬂective markers ﬁxed to the glasses.
Position and orientation data were updated at a 10 Hz rate
with a resolution of approximately 0.1 mm for each marker.
It would be possible to track both the magnet instrument and
and the user’s head using a single motion tracking system,
but this would require using wired infrared markers on the
3D shutter glasses, slowing down the update rate of the
magnetic levitation localization due to the additional LED
markers on the glasses, and mounting the localizer at least
3.5 m high so that its sensing volume includes the location
of the glasses.
As both the Optotrak and Polaris motion trackers use
infrared position sensing, and 3D Vision systems uses in-
frared communication to synchronize display frames with
the shutter glasses, it is necessary to ensure that each
infrared system does not interfere with the others. In our
326
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

Figure 4.
Deformable simulation
system, each set of emitters and receivers are oriented in
orthogonal directions and positioned so that each emitter
is only visible to its corresponding receiver. The Optotrak
sensor is mounted above the table looking down at the LEDs
on the instrument, the Polaris is mounted on the side of the
table to track the reﬂective markers on the side of the glasses,
and the synchronization emitter is mounted at the front of
the tabletop.
C. Simulations
Basic interactive simulations which have been imple-
mented on our system at present include point, edge, and
face contacts between simple solid shapes such as square
peg-in-hole insertion, simple dynamic environments includ-
ing suspended masses and springs, and rolling objects. These
simple initial simulations allow the dynamics and contact
models of the environments to be modiﬁed and adjusted to
provide the most realistic haptic interaction while preventing
unstable dynamics.
A more sophisticated simulation which involves an in-
strument contacting a deformable surface is shown in Fig.
4. In this simulation, a virtual extension is added to the
actual haptic instrument handle, and the deformation of the
surface and reaction forces and torques on the instrument
are calculated at the haptic update rate. Damping is added
to the internal dynamics of the deformable body and the
surface dynamics during contact with the haptic instrument.
The MLHI library and programming interface, originally
0
5
10
15
20
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Haptic Force Feedback (N) 
0
5
10
15
20
0
20
40
60
80
100
120
Time (seconds)
Peg Position (mm)
x 
x 
y
z 
y
z 
Figure 5.
Interactive peg-in-hole simulation
from Butterﬂy Haptics, has been adapted for use with
our system, and can be used for haptic rendering and
communication between simulation and magnetic levitation
threads with a haptic update rate of 1000 Hz. Alternatively,
haptic rendering and dynamic simulation calculations can be
performed synchronously with the motion tracking at 860
Hz.
IV. RESULTS
Force and position experimental data in x, y, and z direc-
tions obtained during interactive simulations are presented
in Figs. 5 and 6. The position data was measured by the
position tracking system, and the force data are calculated
by the simulations and generated by the coil array of the
magnetic levitation system in real time. The commanded
forces were shown to be within 0.1 Newtons of force
sensor measurements throughout the range of the magnetic
levitation system in [11].
The Fig. 5 plots are from a haptic peg-in-hole simulation
in which a 25 mm square peg is controlled by the haptic
instrument handle and inserted into a 27x54 mm, 10 mm
deep square hole. The Fig. 6 plots are from a deformable
simulation in which a pointed virtual instrument contacts a
deformable object, as shown in Fig. 4. For both cases, haptic
forces and torques are zero while the instrument is moving
freely, contact forces are approximately proportional to the
327
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

0
5
10
15
20
−1
−0.5
0
0.5
1
1.5
2
2.5
3
Haptic Force Feedback (N)
0
5
10
15
20
−20
0
20
40
60
80
100
120
Time (seconds)
Instrument Position (mm)
z 
x 
y
x 
y
z 
Figure 6.
Interactive palpation of deformable surface
depth of contact, and haptic torques depend on each contact
force and the displacement between the contact point and
the center of the haptic instrument and simulated tool.
In the peg-in-hole simulation of Fig. 5, the peg is not in
contact with the hole or top surfaces at the 8-9 and 14-15
second intervals, the z coordinate is greater than 30, and
there is no haptic force feedback. As the peg is moved in
and out of the hole, the z position moves between 20 and
30 mm. The x position can vary between approximately 40
and 70 mm while the peg is in the hole, as the hole is more
than twice as wide as the peg in the x direction. Non-zero
x and y forces are present when the virtual peg is pushed
against any of the four sides of the virtual hole. Contact
stiffnesses are approximately 0.4 N/mm and the kinetic and
static friction coefﬁcents are 0.15 in the simulations.
For the deformable surface of Fig. 6, the probe is moved
across the surface during the 12-20 second interval, and
the surface is struck with the probe several times from 8-
12 seconds. The object was modeled with millimeter-scale
surface variations rather than a smooth ﬂat surface. This
surface texture therefore produces variable vertical (z) forces
in response to horizontal (x and y) motions of the intstrument
tip. Oscillations in both the position and force data can be
seen in the 12-20 second period due to sticking and slipping
of the sliding surface contact. Overall the force plots are
smoother in the deformable surface simulation than the peg-
in-hole simulation due to the compliance and lower friction
of the deformable surface.
V. FURTHER WORK
At present, the magnetic levitation and motion tracking
aspects of our system are fully developed, but the interactive
environments are at a preliminary stage. We plan to reﬁne
the detail and physical realism of the simulated environments
to a degree where they are useable and provide measureable
beneﬁts in medical training tasks such as surgery, intubation,
and needle driving. User studies will be conducted to eval-
uate the beneﬁts of colocated haptic and graphical training
of simulated medical procedures.
The complexity of the modelled environment and the
sophistication of the simulated dynamics can be improved
by using the graphics processor for additional numerical
computations, as a general purpose graphics processing unit
or GPGPU. NVIDIA provides the CUDA [26] programming
interface to utilize the parallel processing capabilities of the
GPGPU on the graphics cards used.
One more planned improvement to be made on the system
is to reconﬁgure the system to be simpler and more compact.
VI. CONCLUSION
Our co-located haptic and graphic interface system is
novel in that there is no hardware between the user and
the display other than the handheld interaction instrument.
The 3D environment is displayed close to the surface of the
monitor, so there is no conﬂict between visual convergence
and focal ranges. Electromagnetic force and torque actua-
tion is used for haptic interaction rather than a motorized
linkage, providing advantages in backdriveability, precision,
and response frequency bandwidths.
We have demonstrated the feasibility and function of our
system with the basic simulation environments described.
ACKNOWLEDGMENT
This work was supported in part by National Science
Foundation grants IIS-0846172 and CNS-0551515, and by
the University of Hawaii College of Engineering.
REFERENCES
[1] C. Luciano, P. Bannerjee, L. Florea, and G. Dawe, “Design
of the ImmersiveTouch: a high-performance haptic augmented
virtual reality system,” in 11th International Conference on
Human-Computer Interaction, Las Vegas, August 2005.
[2] Y. Yokokoji, R. Hollis, and T. Kanade, “WYSIWYF display:
A visual/haptic interface to virtual environment,” Presence,
vol. 8, no. 4, pp. 412–434, August 1999.
[3] P. Olsson, F. Nysjo, S. Siepel, and I. Carlbom, “Physically co-
located haptic interaction with 3d displays,” in IEEE Haptics
Symposium, Vancouver, March 2012, pp. 267–272.
[4] D. Swapp, V. Pawar, and C. Loscos, “Interaction with haptic
feedback and co-location in virtual reality,” Presence, vol. 10,
no. 1, pp. 24–30, April 2006.
328
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

[5] G. Jansson and M. Ostrom, “The effects of co-location of
visual and haptic space on judgements of form,” in Euro-
Haptics, Munich, June 2004, pp. 516–519.
[6] R. L. Hollis and S. E. Salcudean, “Lorentz levitation technol-
ogy: a new approach to ﬁne motion robotics, teleoperation,
haptic interfaces, and vibration isolation,” in Proc. 6th Int’l
Symposium on Robotics Research, Hidden Valley, PA, Octo-
ber 1993.
[7] S. Salcudean and T. Vlaar, “On the emulation of stiff walls
and static friction with a magnetically levitated input-output
device,” in ASME IMECE, Chicago, November 1994, pp.
303–309.
[8] P. J. Berkelman, R. L. Hollis, and S. E. Salcudean, “Inter-
acting with virtual environments using a magnetic levitation
haptic interface,” in Int’l Conf. on Intelligent Robots and
Systems, Pittsburgh, August 1995.
[9] P. J. Berkelman and R. L. Hollis, “Lorentz magnetic levitation
for haptic interaction: Device design, function, and integra-
tion with simulated environments,” International Journal of
Robotics Research, vol. 9, no. 7, pp. 644–667, 2000.
[10] P. Berkelman and M. Dzadovsky, “Extending the motion
ranges of magnetic levitation for haptic interaction,” in Eu-
rohaptics Conference and Symposium on Haptic Interfaces
for Virtual Environment and Teleoperator Systems, Salt Lake
City, March 2009, pp. 517–522.
[11] ——, “Magnetic levitation over large translation and rota-
tion ranges in all directions,” IEEE/ASME Transactions on
Mechatronics, vol. 18, no. 1, pp. 44–52, 2013.
[12] N. J. Groom and C. P. Britcher, “A description of a laboratory
model magnetic suspension testﬁxture with large angular
capability,” in IEEE Conference on Control Applications,
Dayton, September 1992, pp. 454–459.
[13] M. B. Khamesee and E. Shameli, “Regulation technique for
a large gap magnetic ﬁeld for 3d non-contact manipulation,”
Mechatronics, vol. 15, pp. 1073–1087, 2005.
[14] D. Baraff, “Interactive simulation of solid rigid bodies,” IEEE
Computer Graphics and Applications, vol. 15, pp. 63–75,
1995.
[15] D. Baraff and A. Witkin, “Dynamic simulation of non-
penetrating ﬂexible bodies,” in Computer Graphics (Proc.
SIGGRAPH), vol. 26.
ACM, July 1992, pp. 303–308.
[16] J. Allard, S. Cotin, F. Faure, P.-J. Bensoussan, F. Poyer,
C. Duriez, H. Delingette, and L. Grisoni, “Sofa
an open
source framework for medical simulation,” in Medicine Meets
Virtual Reality (MMVR’15), Long Beach, USA, February
2007.
[17] M. Marchal, J. Allard, C. Duriez, and S. Cotin, “Towards
a framework for assessing deformable models in medical
simulation,” in International Symposium on Computational
Models for Biomedical Simulation, London, July 2008.
[18] J. Barbic and D. James, “Six-dof haptic rendering of contact
between geometrically complex reduced deformable models,”
IEEE Transactions on Haptics, preprint published online,
June 2008.
[19] D. James and D. Pai, “A uniﬁed treatment of elastostatic
and rigid contact simulation for real time haptics,” Haptics-e,
vol. 2, no. 1, 2001.
[20] HAPI Manual, 1st ed., SenseGraphics Inc., May 2009.
[21] Http://www.chai3d.org/.
[22] T. Massie and K. Salisbury, “The PHANToM haptic interface:
A device for probing virtual objects,” in Proceedings of
the ASME Winter Annual Meeting, Symposium on Haptic
Interfaces for Virtual Environment and Teleoperator Systems,
Chicago, Illinois, November 1994.
[23] Magnetic Levitation Haptic Interface API Reference Manual,
1st ed., Microdynamic Systems Lab, Carnegie Mellon Uni-
versity, September 2008.
[24] P. Berkelman, M. Miyasaka, and J. Anderson, “Co-located 3d
graphic and haptic display using electromagnetic levitation,”
in IEEE Haptics Symposium, Vancouver, March 2012, pp.
77–81.
[25] P. Berkelman and M. Dzadovsky, “Novel design, characteri-
zation, and control method for large motion range magnetic
levitation,” IEEE Magnetics Letters, vol. 1, January 2010.
[26] Nvidia CUDA Compute Uniﬁed Device Architecture Program-
ming Guide 1.0, Nvidia, 2007.
329
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

