Abstract—This paper presents findings from a HRI user-
study  which  investigated  participants'  perceptions  and 
experiences playing a simple version of the classic game, 
stone-paper-scissors with a humanoid robot. Participants 
experienced the robot displaying one of four different robot 
faces and interacted with the robots using a gesture-based 
interface. Findings from the study indicated that the effects 
of  the  different  robot  faces  were  inter-related  with 
participants gender and ratings for overall enjoyment of 
the game experience. The usability and effectiveness of the 
gesture-based  interface  were  overall  rated  positively  by 
participants, though the use of a separate display for the 
game  interface  seems  to  have  distracted  participants 
attention from the robot's face.
Keywords;  HRI,  Human-Robot-Interaction,  Gesture-
Based User Interface, Interactive Game Robot
I.
INTRODUCTION 
Recent research aims towards developing robots for 
use  in  domestic,  office  or  other  human-oriented 
environments. These robots will interact with humans in 
these environments as a matter of course and should 
exhibit  behaviour  that  is  not  just  safe  and  socially 
acceptable for humans in the vicinity, but should also 
facilitate and enhance the usability of the robots.  Overall 
it  has  been  found  that  people  prefer  consistency,  in 
particular  regarding  robot  appearance,  capability  and 
functionality  [1][2]. The judgement  of consistency  of 
robot appearance, behaviour and function is subjective 
and it is likely that peoples' own idiosyncratic preferences 
and perceptions play a large role. However, some general 
characteristics regarding peoples preferences for robots 
have been discovered. Most people do not like robots to 
have realistic human appearances  [3], but prefer some 
degree of human-likeness. A sizeable minority strongly 
prefer  robots  with  non-human-like  (i.e.  machine-like) 
appearance [4][5]. It has been shown that people perceive 
human-likeness for robots as two main factors, physical-
likeness  and  expressive-likeness,  with  the  latter  (i.e. 
communication and interaction abilities) generally more 
desirable than physical human-likeness (i.e. appearance) 
[6]. Most people would prefer to interact with a robot by 
means of speech [7][8] and many previous studies (e.g. 
[9][10][11]) have indicated that robot speech capabilities 
hold  much  promise  with  regards  to  Human  Robot 
Interaction  (HRI).  However,  speech  recognition 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Michael L. Walters
Adaptive Systems Research Group,
University of Hertfordshire,
Hatfield, Herts, UK, 
AL10 9AB
M.L.Walters@herts.ac.uk
Samuel Marcos
CARTIF Foundation, 
Division of Robotics and Computer Vision, 
Parque Tecnológico de Boecillo, 
205, 47151, Boecillo, Valladolid, Spain
sammar@cartif.es
Dag Sverre Syrdal & Kerstin 
Dautenhahn
Adaptive Systems Research Group,
University of Hertfordshire,
Hatfield, Herts, UK, AL10 9AB
D.S.Syrdal@herts.ac.uk 
K.Dautenhahn@herts.ac.uk
An Interactive Game with a Robot: Peoples' Perceptions of Robot Faces and a 
Gesture-Based User Interface
123
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

in collaboration with international artists, Anna Dumitriu 
and  Alex May  [15]. These  venues were  attended  by 
relatively large numbers of  people, which provided the 
opportunity  to  carry  out  survey-based  research  in 
conjunction with these events  [6].  In the light of the 
findings,  technical  and  procedural  experience  gained 
from these events, it was decided to carry out a new HRI 
study to investigate in more depth some of the issues that 
had arisen. The areas chosen for further investigation 
were: Peoples' perceptions  and reactions to different face 
displays  and  to  evaluate  a  newly  developed  Kinect 
gesture based user interface. This was to be achieved by 
playing a simple interactive game with the robot. The 
main research questions were:
1)
Does the appearance of the robot's face display 
have any effects on users' perceptions or preferences for 
interacting with the robot?
2)
Is the Kinect based gestural interface a feasible 
way for users to interact and control a robot.
Section  II  describes  the  robot  and  game  system, 
Section III the user study method, Section IV presents the 
results and Section V the Discussion and Conclusions.
II.
CHARLY SYSTEM DESCRIPTION
CHARLY has a simplified human-like appearance 
(humanoid) and scale, and was developed as part of the 
EU  funded  project  LIREC  (LIving  with  Robots  and 
intEractive  Companions).  CHARLY  is  designed  with 
easily  replaceable  head  and  body  covers  so  that  the 
robot's appearance may readily be changed to facilitate 
HRI research. CHARLY was developed using as many 
(low cost) standard parts as possible and the arms are 
primarily used for making expressive gestures. The body 
is mounted on two 'legs' and the robot can assume either a 
bending, crouching or sitting position. The robot body is 
mounted on a Pioneer 3AX mobile robot base [16]. 
1)
CHARLY Server and Control
A  dedicated  Mini-PC  implements a simple  TC-IP 
based  server which allows remote  client programs to 
connect over a local (wired or wireless) LAN to control 
the  robot.  Remote  clients  can  be  written  in  any 
programming  language  that  supports  IP  sockets.  The 
underlying control software has obstacle detection and 
avoidance  continuously  active  in  normal  use,  so 
CHARLY has the capability to move safely in people 
orientated environments, either under direct control or 
autonomously. 
2)
Morphing Face Display System
A Face Morphing display  system was developed 
when  CHARLY  was  previously  shown  at  the  “My 
Familiar  Companion”  art  exhibit [15].  This  used  a 
Microsoft Kinect camera mounted on the robot's chest to 
detect and isolate the faces of anyone coming within 
range. All the detected faces are combined into a single 
image such that faces closer to the Kinect have more 
influence  over  the  resulting  image  than  ones  further 
away. The morphing face display program also acted as a 
client program for CHARLY and directly controlled the 
head  and  body  movements  for  the  “My  Familiar 
Companion” exhibit.
3)
CHARLY Stone Scissors Paper Game
 For the current Study an Interactive Game program 
was developed which used the Kinect sensor to interpret 
users' gestures and implement a simple game based on 
Stone, Paper, Scissors.  This program ran concurrently 
with the Face Morphing Program (only for one of the 
experimental conditions, see Section III) on CHARLY's 
laptop  PC  and  allowed  participants  to  interact  with 
CHARLY using only gestures.  
Initially it was planned to used CHARLY's hands to 
make appropriate gestures for Stone, paper or scissors, to 
use the Kinect to recognise hand gestures directly and to 
use  speech  to  provide  the  main  means  of  providing 
information to participants. However, though feasible in a 
quiet lab situation, it was found not robust enough in 
noisy and dynamic public areas.  It was therefore decided 
to use a second body-mounted touch-pad to provide a 
dedicated  display  where  the  game  progress  and  user 
interface was displayed (see Figure 3). Also, a method for 
controlling  the  game  and  robot  with  the  human 
participants' whole arm gestures was found to be much 
more  reliable  in  public  environments.  The  program 
interfaced to the CHARLY Server and initiated simple 
pre-scripted body, head and arm movements, gestures 
and speech at appropriate points of the game.  
4)
Game Interface and Description
The game interface is shown in Figure 3. The topmost 
part displays the current game score. The robot score is 
labelled as “Me” and the user score is labelled as “You”. 
The  middle  area,  also  labelled  as  “Me”,  displays  an 
image of the robot's selection. During the game, this 
image changed randomly over time giving the user a clue 
as to which option the robot will choose. The bottom area 
has three buttons that allow the user to select rock, paper 
or scissors. This area is labelled as “You”. Each button 
has an image of the hand gesture for scissors, paper and 
rock as used in the real game. The user makes a choice by 
moving their (left or right) arm up and down (Figure 4) 
then  pushing  their  hand  forward  when  their  desired 
choice icon is highlighted (Figure  5). When the user 
makes  a  selection,  the  image  freezes  to  indicate  the 
robot's choice. A complete game session consisted of 
three rounds, each of three individual games. The first 
player (robot or participant) to win two individual games 
in a round wins the round. The first to win two rounds 
wins  the  game.  A  complete  game  of  three  rounds 
typically lasted from 5 to 10 minutes. Depending on who 
won the game, the robot said “I won the game” or “You 
won the game” and either waved its arms in the air or 
adopted a head down posture respectively. 
B.
CHARLY Face Displays
The robot used four different face displays during the 
interactive game sessions. Each participant experienced 
only one face display from: Simple static face - A simple 
black  and  white,  cartoon-like  face  constructed  from 
ellipses and straight lines (Figure 6). Morphing user face 
display - The face display slowly changed to an image of 
the face of the the interacting person (see Figure 7 and 
section II.A.2 for more details). A fixed image of a real 
robot face - This robot face image was from a robot head 
we had used previously for live HRI studies  [17][18] 
(Figure 8a). Simple cartoon-like expressive face -  Three 
simple cartoon-like faces, constructed from ellipses and 
straight  lines,  formed  simple  'expressions'  which 
predicted the  game intention of the robot (Figure 8b-c). 
124
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

III.
EXPERIMENTAL SET-UP AND PROCEDURE
The study ran for 2 weeks in late November 2011 and 
was performed in the foyer area of the University of 
Hertfordshire  Learning  Resources  Centre  (LRC).  The 
LRC  is  open  to  students  and  staff  working  at  the 
University and provides open access to research, library, 
computing resources and social areas. A high level of 
casual and passing foot traffic occurs within the foyer 
area  and  this provided  a potentially large  number  of 
participants for taking part in the Interactive Game Study 
(IGS). CHARLY was set-up at one end of the entrance 
foyer area, with the robot operator, control and support 
equipment partially obscured by screens (see Figure 2). A 
separate  desktop  computer  ran  a  browser-based 
questionnaire to obtain participants' views on their IGS 
experience.  Brief  descriptions  of  the  questions  are 
provided  in  the  following  section  IV along  with  the 
questionnaire  results  and  analyses.  The  experimental 
procedure was as follows:
1.
Any passers-by that showed interest in the robot and 
game, were asked if they wanted to play a game with 
the robot. If the answer was positive, they were then 
asked for consent to take part in the study. Note, if 
they  only  wanted  to  play  the  game,  this  was 
acceptable. If they consented to take part in the study, 
they were asked to sign a consent form to allow data 
and/or video recordings to be collected and used for 
subsequent research purposes.
2.
The  game  was  described  and  the  gesture-based 
interface was shown to them. After a short practice, 
the game was restarted and up three rounds were 
played.  Many participants did not complete a full 
three rounds, either because a winner was declared 
after two rounds, or the participant wanted to finish. 
3.
The participant was then directed to the questionnaire 
computer  and  left  to  complete  the  online 
questionnaire. An experimenter was on hand in case 
of any difficulties with using the software, but did not 
lead or direct their responses to the questions.  
IV.
RESULTS
The  questionnaire  was  divided  into  the  following 
categories:
Demographic – Age, gender, occupation, handedness, 
robot  computer   and  games  experience,  and  Kinect 
(xbox) experience. The total number of participants was 
80, 54 males  (68%), 26 females (32%) and 7 (8%) were 
left handed. Their ages ranged from 17 to 60 (mean = 
27),  40 (50%) were  quite or very familiar with computer 
games but only 12 (15%) rated themselves as familiar 
with (toy or service) robots.  57 (71%) had not used a 
Kinect before.
Game Experience – Enjoy overall, like playing with 
CHARLY, who won?, like winning/losing, play again, 
understand  game,  game  difficulty,  like  CHARLY  as 
game partner and like game interface. 
CHARLY specific – Overall appearance, looking at, 
pleasing,  comfort,  distraction,  difficulty  focusing  on 
game. Face;  like, annoy, prefer another? Voice;  like, 
like talking?
A.
Principal Component Analysis (PCA): Global 
Liking, Face Type and Demographics. 
In order to categorise the response data, a series of 
PCA analyses were run. Variables loading less than .5 on 
any given factor were removed as part of the analyses. 
The predictors of Global Evaluation were assessed using 
a stepwise multiple regression. The variables entered into 
the initial model were: Gender, Age, Familiarity with 
Robots, Familiarity with the Xbox Kinect, Familiarity 
with Computer Games, Familiarity with Rock, Paper, 
Scissors Game, Face type (Simple Static Face, Simple 
Expressive Face, User Face and Robot Face). The final 
analyses are reported here:
1)
Global Evaluation of the Experience
The PCA on the variables intended to investigate a 
general (global) evaluation of the interaction as a whole 
found a unidimensional construct, formed of three factors 
Figure 3. Stone-paper-scissors game interface when the user is 
being tracked and selecting paper
Figure 4. The three areas related to the user's body where a detected 
hand is associated with the selection of one of the three buttons of the 
game
125
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

which had an eigenvalue of 2.22 and explained 73% of 
the variance.  Table 1 provides the factor loadings and the 
variables are presented below:
Table 1: PCA of the Global Experience Evaluation Variables
Variable
Factor Loading
How was your experience of playing the game with 
CHARLY? 
.87
Did you like playing with CHARLY?
.88
If you had the Opportunity, would you like to play 
with CHARLY again?
.83
2)
Game Usability
Table 2: PCA of the Robot Evaluation variables 
Variable
General
Voice
Distraction
Did you like CHARLY’s 
General Appearance?
.825
.050
.083
Did you find it pleasing looking 
at CHARLY ? 
.839
.284
.076
CHARLY distracted me from 
the game 
.119
.029
.828
CHARLY made it difficult to 
focus 
.010
.215
.800
Did you like CHARLY’s face? 
.767
.300
.116
I liked CHARLY’s Voice 
.015
.853
.057
Felt good talking with 
CHARLY
.123
.843
.105
Did you feel comfortable 
looking at CHARLY?
.638
.267
.360
The PCA on the variables intended to investigate the
usability  aspects  of  the  game  itself  found  a
unidimensional construct formed of three variables which 
had an eigenvalue of 1.69 and explained 57% of the 
variance as follows:
Table 3: PCA of the Robot Evaluation Variables
Variable
Factor Loading
Did you find it difficult to play the game?
.75
Did you like the interface of the game? 
.65
Did you find it difficult to understand the game?
.84
3)
Robot Evaluation.
The PCA on the variables intended to investigate the 
views of the robot in the interaction (Table 2) found three 
factors. The first factor had an eigenvalue of 2.63 and 
explained 33% of the variance. It was named General 
Robot Evaluation and is described below in terms of 
factor loadings. The Second Factor was named Robot 
Voice  Evaluation  with  an  eigenvalue  of  1.77  and 
accounted for 22% of the variance. The Third Factor was 
named Robot Distraction with an eigenvalue of 1.24 and 
accounted for 16% of the variance. In total these three 
factors accounted for 70.5% of the variance.
B.
Significant differences for the four face display 
conditions
There was no significant relationship between success at 
the game and the face display conditions (χ2(3)=2.10, 
p=.553), and also no relationship between success at the 
game and any of the Global, Game or Robot Measures.
 
 
 
 
 
 
 
 
 
 
Figure 5. Schema of the movement necessary to press the 
scissors button
Figure 7. The Morphing User face 
display
Figure 6. The Simple Static 
cartoon-like robot face display
126
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

than female participants and the opposite is true of the 
other display types. The effect was most pronounced for 
the Simple Expressive and the User Faces.
Table 4: ANOVA indicated significant differences between the four 
face conditions and participants' enjoyment of the interactive game 
experience (F(3,75=2.775, p=.047)
Group
N
Mean
Std Error
Simple Face
22
4.46
0.13
User Face
23
4.30
0.14
Robot Face
23
4.16
0.13
Expressive 
Face
12
3.86
0.22
There was no significant relationship between Face 
Display Condition along the other measures. There was, 
however a significant main effect for for Gender and the 
General  Robot  Evaluation  measure  (F(1,72)=4.27, 
p=.042). This effect suggested that female participants 
were more likely to rate the robot more positively along 
this dimension. The results also suggested that overall, 
participants rated the interaction positively.
C.
Relationship between Prior Use of the Kinect 
Interface and Measurements.
There was no relationship between Prior Use of the 
Kinect  and  the  Evaluation  measures  (F(1,  77)<.010, 
p>.99, η2<.001) and no relationship between Prior Use of 
the Kinect and Success at the Game.  In fact, there was a 
non-significant  trend  in  which  participants  with  no 
experience of the Kinect were  more likely to succeed 
(χ2(1)=.522, p=.470). 
Table 5: Correlation Matrix for Evaluation Measures
Measure
Global
Game
Robot 
General 
Robot 
Voice
Robot 
Distract
Global
1
Game
.197*
1
Robot 
General
.254**
.380**
1
Robot 
Voice
.191
-.039
.088
1
Robot 
Distract
.104
-.287**
.171
-.141
Note: *p<.01, **p<.05
This combined with the overall high scores given for 
Evaluation,  suggest  that  the  gestural  interface  was 
suitable for a wide section of participants, not just those 
accustomed to such interfaces from previous use. 
D.
Correlations between the Measures
The  different  measures  were  correlated  using 
Pearson's r, and the results are presented in Table 5 which 
suggests that there are results approaching significance 
between Global Evaluation, Game Evaluation and the 
General  Robot  Evaluation  measures.  In  addition,  the 
Distraction measure was negatively correlated with the 
Game Evaluation.
V.
DISCUSSION AND CONCLUSION
Regarding the first research question: While there was 
a difference between the Face Display conditions in terms 
of Global Evaluation, post-hoc tests found that this was 
caused  by  the  sample  as  a  whole  rating  the  simple 
expressive display less favourably than the other displays.
 Significant gender differences in the sample were 
found, with female participants being more positively 
inclined towards the Expressive, Robot and User Face 
conditions than the male participants. This  suggests that 
male participants overall preferred a simple, unchanging 
face  display,  while  female  participants  were  more 
positive towards a richer, more dynamic display. This 
finding echoes those from  [19] where male participants 
had a tendency to prefer robot behaviours that facilitated 
the  explicit  goal  of  the  interaction,  while  female 
participants attached more value to the more social and 
intrinsically rewarding aspects of the interaction. This 
also mirrors results found in [20] where female players of 
games seemed to value the non-competitive aspects of 
computer-mediated  games  in  comparison  with  male 
players. Also, the female participants may have been able 
to process the richer facial displays more efficiently than 
male participants as found in  [21]. This suggests that 
while some participants found it problematic to deal with 
split attention in such interactions, the use of rich and 
dynamic face displays will still be evaluated. However, 
the simple expressive face which predicted the robot's 
next game move was not highly rated by participants, and 
it is therefore likely that they were so concentrated on the 
game display, that they did not pay much attention to the 
face display. The problem of split attention has been 
remarked upon in some previous studies [22][23][24] and 
these findings seem to reflect a similar phenomenon.
For the second research question the data supports 
using the Kinect based gestural interface as a feasible 
way to interact and control the robot. Although half the 
participant  sample  had  previous  computer  game 
experience, most had no previous experience of playing a 
Kinect-based (computer) game. Even though participants' 
interaction time was typically less than 10 minutes, their 
ratings for usability were relatively high irrespective of 
their previous experience of the Kinect interface.
This study has shown that there are differences in 
how  participants  evaluate  the  use  of  a  robot.  It  has 
highlighted the trade-off in the current system between 
the potential problem of split attention with competing 
sources  of  expressive  and  game  task  feedback. 
Differences  were  found  between  male  and  female 
perceptions'  of  the  interaction;  game  interaction  and 
Figure 9. Interaction Effect for Gender and Face Display Conditions 
in terms of global evaluation.
127
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

control, versus the intrinsic reward of a richer, more 
dynamic face display. Future work should address the 
issues of incorporating a gestural interface integrated with 
social feedback mechanisms in a more seamless manner 
for service robots in human-oriented environments.
ACKNOWLEDGMENT 
Many thanks are due to Anna Dumitriu and Alex May for 
their work in developing the morphing face system and 
body form for CHARLY. This work was supported by 
the  LIREC  (LIving  with  Robots  and  intEractional 
Companions)  EU  project  (FP7-215554),  the  Junta  de 
Castilla y León (CCTT/10/VA/001/A.2-C.6), the EDRF 
programme and the Spanish Ministry of Economy and 
Competitiveness (DPI2011-25489, DPI2008-06739-C02-
01, IPT2011-0854-900000).
REFERENCES
[1]  A. Tapus,  M.  J. Matarić, "User Personality Matching with 
Hands-Off Robot for Post-Stroke Rehabilitation Therapy" 
in  Proceedings of the 10th International Symposium on 
Experimental Robotics (ISER-06). 2006.
[2]  S. Lee, I. Yee-man Lau, S. Kiesler,  C. Chiu, "Human 
Models of Humanoid Robots" in Proceedings of the 2005 
International  Conference  on  Robotics  and  Automation 
(ICRA 05). 2005. pp. 2767- 2772
[3]  T. Minato, K. F. MacDorman, M. Shimada, S. Itakura, K. 
Lee,  H.  Ishiguro,  "Evaluating  Humanlikeness  by 
Comparing  Responses  Elicited  by  an  Android  and  a 
Person"  in  Proceedings  of  the  Second  International 
Workshop on Man-Machine Symbiotic Systems.. 2004. pp. 
373-383
[4]  M. L. Walters, D. S. Syrdal, K. L. Koay, K. Dautenhahn,  R. 
te  Boekhorst,  "Human  Preferences  and  Perceptions  of 
Robot  Appearances"  in
 Proceedings  of  Artificial 
Intelligence  and  Simulation  of  Behaviour  Convention 
(AISB 09), HRI Symposium. 2009. pp. 136-143
[5]  D. S. Syrdal, M. L. Walters, K. L. Koay, S. N. Woods,  K. 
Dautenhahn, "Looking Good? Appearance Preferences and 
Robot Personality Inferences  at  Zero  Acquaintance" in 
AAAI  -  Spring  Symposium  2007,  Multidisciplinary 
Collaboration for Socially Assistive Robotics. 2007. pp. 86-
92
[6]  M. L. Walters, D. S. Syrdal, K. Dautenhahn, A. Dumitriu, A. 
May, B. Christiansen,  K.  L. Koay, "My Familiar Robot 
Companion: Preferences and Perceptions of CHARLY,  a 
Companion Humanoid Autonomous Robot for Living with 
You" in  In Proceeedings of the Conference of Towards 
Autonomous Robotics Systems (TAROS 2012). 2012.
[7]  K. Dautenhahn, S. N. Woods, C. Kaouri, M. L. Walters, K. 
L. Koay,  I. Werry, "What is a Robot companion - Friend, 
Assistant  or  Butler?"  in  Proceedings  of  IEEE  RSJ 
International  Conference  on  Intelligent  Robot  Systems 
(IROS'05). 2005. pp. 1488-1493
[8]  B. Bengtsson, J. K. Burgoon, C. Cederberg, J. Bonito,  M. 
Lundeberg, "The Impact of Anthropomorphic Interfaces on 
Influence,Understanding, and Credibility" in  Proceedings 
of  the  Thirty-Second  Annual  Hawaii  International 
Conference on System Sciences-Volume 1. 1999. pp. 1051-
1066
[9]  S. Lauria, G. Bugmann, T. Kyriacou, E. Klein, "Mobile 
Robot Programming Using Natural Language."  Robotics 
and Autonomous Systems, 2002, 38 (3-4).  pp. 171-181
[10]  C. Nass, S. Brave, Wired for Speech: How Voice Activates 
and Advances the Human-Computer Relationship.   The 
MIT Press, 2005.
[11]  A. J. N. van Breeman, K. Crucq, B. J. A. Krose, "A User-
Interface Robot for Ambient Intelligent Environments" in 
Proceedings of ASER 2003. 2003. pp. 132-139
[12] Microsoft Kinect. http://www.xbox.com/en-US/kinect.
[13]  H. B. Suay,  S. Chernova, "Humanoid Robot Control Using 
Depth  Camera"  in  Proceedings  of  the  6th  ACM/IEEE 
International  Conference  on  Human-Robot  Interaction 
(HRI 2011) . 2011. pp. 401-402
[14] S. M. Albrektsen, Thesis. Using the Kinect Sensor for 
Social
 
Robotics.
 
Norwegian University of Science and Technology, Faculty 
of Information Technology, Mathematics and Electrical 
Engineering,  Department  of  Engineering  Cybernetics. 
2011.  http://urn.kb.se/resolve?urn=urn:nbn:no:ntnu:diva-
13862
[15] My Robot Companion. An art exhibit commisioned by the 
Science Gallery, Dublin, and created in collaboration with 
the
 
University
 
of
 
Hertfordshire. 
http://www.myrobotcompanion.com/.
[16] Robotis Inc. Manufacturers of the Dynamixel series of robot 
servos. www.robotis.com.
[17] Mobile Robots. http://www.mobilerobots.com/.
[18] The OpenNI API Framework. www.openni.org.
[19]  Open  Computer  Vision  Library  (OpenCV). 
http://sourceforge.net/projects/opencvlibrary.
[20]  M. L. Walters, K. Dautenhahn, R. te Boekhorst,  K.  L. 
Koay, "Exploring the Design Space of Robot Appearance 
and  Behaviour  in  an  Attention-Seeking  'Living  Room' 
Scenario for a Robot Companion" in Proceedings of IEEE-
Artificial Life (Alife 07). 2007. pp. 341-347
[21]   M.  L.  Walters,  D.  S.  Syrdal,  K.  Dautenhahn,  R.  te 
Boekhorst,  K.  L. Koay, "Avoiding the Uncanny Valley – 
Robot  Appearance,  Personality  and  Consistency  of 
Behavior in an Attention-Seeking Home Scenario for a 
Robot Companion." Journal of Autonomous Robots, 2008, 
24(2).  pp. 159-178
[22]  H. Kose-Bagci, K. Dautenhahn, D. S. Syrdal,  C.  L. 
Nehaniv, "Drum-mate: Interaction Dynamics and Gestures 
in  Human-Humanoid  Drumming  experiments." 
Connection Science, 2010, 22(2).  pp. 103-134
[23]  T. Hartmann,  C. Klimmt, "Gender and Computer Games: 
Exploring  Females'  Dislikes."  Journal  of  Computer-
Mediated Communication, 2006, 11(4).  pp. Article 2
[24]  E. Hampson, S. M. Anders,  L.  I. Mullin, "A Female 
Advantage  in  the  Recognition  of  Emotional  Facial 
Expressions:  Test  of  an  Evolutionary  Hypothesis." 
Evolution and Human Behavior, 2006, 27.  pp. 401-416
[25]  M. L. Walters, M. Lohse, M. Hanheide, B. Wrede, D. S. 
Syrdal,  K.  L.  Koay,  A.  Green,  H.  Hüttenrauch,  K. 
Dautenhahn,  G.  Sagerer,   K.  Severinson-Eklundh, 
"Evaluating the Robot Personality and Verbal Behavior of 
Domestic Robots Using Video-Based Studies." Advanced 
Robotics, 2011, 25(18).  pp. 2233-2254
[26]  M. Lohse, M. Hanheide, B. Wrede, M. L. Walters, K. L. 
Koay,  D.  S.  Syrdal,  A.  Green,  H.  Hüttenrauch,  K. 
Dautenhahn,  G.  Sagerer,   K.  Severinson-Eklundh, 
"Evaluating extrovert and introvert behaviour of a domestic 
robot – a video study" in  Proceedings of the 17th IEEE 
International Symposium on Robot and Human Interactive 
Communication (RoMan 08). 2008. pp. 488-493
[27]  G. Zen, B. Lepri, E. Ricci,  O. Lanz, "Space speaks: 
towards socially and personality aware visual surveillance" 
in Proceedings of the 1st ACM international workshop on 
Multimodal pervasive video analysis. 2010. pp. 37-42
128
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

