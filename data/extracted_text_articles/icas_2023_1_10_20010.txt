Mobile 3D LiDAR-based Object and Change Detection in Production and 
Operations Management 
 
Taru Hakanen, Paul Kemppi and Petri Tikka 
Cognitive Production Industry 
VTT Technical Research Centre of Finland 
Tampere, Finland 
email: taru.hakanen@vtt.fi 
 
Abstract—As an ongoing robot-related trend, intelligent mobile 
robots can create an accurate digital three-dimensional (3D) 
model of the environment using Light Detection And Ranging 
(LiDAR) scanners. This paper presents a new solution concept, 
which enables change detection and thus more efficient visual 
3D data utilisation in production and operations management. 
As a result, use cases for mobile, real-time 3D LiDAR-based 
change detection are identified. In these cases, Autonomous 
Mobile Robots (AMRs) equipped with LiDARs detect changes 
automatically while moving around the factory in, e.g., internal 
logistics tasks. The solution studied may be useful for, e.g., 
detecting large items left in the wrong places, obstacles on 
AMRs’ routes and blocked fire doors and emergency exits. The 
as-is 3D model of the factory can also be used as a basis for 
factory 
renovation 
and 
modernisation 
and 
progress 
monitoring. 
Keywords-Autonomous mobile robot; AMR; LiDAR; object 
and change detection; real-time 
I. 
INTRODUCTION 
Modern factories are full of fixed sensors on production 
lines and machines, as well as in the building itself. There 
have been plenty of discussions and actions taken in smart 
factories in order to unleash the potential of Industry 4.0 and 
enormous amounts of data [1]. AMRs and digital twins are 
among central technologies when companies proceed in 
Industry 4.0 and 5.0 [2][3]. AMRs use LiDAR scanners in 
mapping their environment, localisation and autonomous 
navigation [4]-[8]. With 3D LiDARs, an accurate digital 
reconstruction can be created of the environment where the 
AMR is moving while conducting, e.g., internal logistics 
tasks. However, data gathered by LiDARs remain under 
utilised in factories, and new mobile LiDAR solutions can 
enable new use cases and benefits for production and 
operations management. 
The aim of this study was to study the way mobile 3D 
LiDAR-based object and change detection can enhance 
production and operations management (POM). The study 
consisted of two tasks: 1) development of a new mobile, 
real-time 3D LiDAR-based solution concept for object and 
change detection and related testing in real industrial 
environments and 2) identification of potential use cases for 
such solutions within the production and operations 
management domain. The findings of the study propose new 
potential use cases for object and change detection in, e.g., 
production, intra-logistics and security operations. The 
strengths of the solution are especially related to cases in 
which rather large visual objects and changes need to be 
detected. Then, for example, obstacles blocking AMR traffic 
or emergency exits can be pinpointed real-time leading to 
prompt reactions and thus ensuring smooth logistics flows 
and safety. 
This paper is organised as follows: Section 1 provides an 
introduction for the paper. In the Section 2, we provide a 
theoretical background regarding digital smart factories, 
AMRs, LiDARs and related applications, as well as methods 
for object and change detection. Section 3 describes the 
methodology of the study. Section 4 presents the study 
results: a) a technical solution concept for object and change 
detection and testing results in a real industrial environment, 
and b) potential use cases for the solution. Finally, in the 
Section 5, conclusions are drawn on the most potential use 
cases and the ways the solution may enhance POMS in the 
future. 
II. 
THEORY 
A. Intelligent production and operations 
Industry 4.0, also known as the Fourth Industrial 
Revolution, 
is 
drastically 
renewing 
manufacturing 
companies’ 
production 
and 
operations 
[2]. 
Digital 
technologies make machines more self-sufficient, highly data 
intensive and able to communicate and even “talk” to one 
another. Automation and data analytics emerge as major 
forces to enhance efficiency in operations management [9]. 
The ultimate aim is to enhance production and operations 
efficiency and business growth. 
Hand in hand with the phenomenon goes the trend of 
making all businesses smarter and more automated [9]. 
Manufacturing is in an increasing manner equipped with 
sensor and autonomous systems. Dull, dirty and dangerous 
labour involves in an increasing manner robots instead of 
people. Robotics and digital twins are two out of five central 
disruptive technologies in the Industry 4.0 and beyond [9]. 
Robots are more intelligent and autonomous and support 
automation of production and intra-logistics operations. In 
addition to robotics, holistically digitalised models of 
products and factories are developed for smart factories [10]. 
Industry 5.0 also includes edge computing, digital twins and 
collaborative robots also encompassing stronger human 
1
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-053-7
ICAS 2023 : The Nineteenth International Conference on Autonomic and Autonomous Systems

perspective in terms of critical thinking and decision making 
[3][9]. 
Industry 4.0 and 5.0 go hand in hand with the Big Data 
phenomenon [12], and huge amounts of various data from 
machines and operations are collected and used nowadays in 
factories. Data supports rehal-time monitoring and more 
intelligent decision making concerning, for example, 
production and inventory management [1][4][12]. Fifth-
generation (5G) network enables further advances in data 
gtransfer, remote monitoring and operation [13]. 
B. Autonomous mobile robots in smart factory 
Traditionally, motives for automation have been cost 
savings, quality improvements and safety [14]. Use of AMRs 
is increasing in production and intralogistics operations 
[5][14]. AMRs are typically equipped with a wide set of 
sensing technologies providing input data. Laser scanners, 
3D cameras, accelerometers, gyroscopes, ultrasound sensors 
and wheel encoders ensure accurate position and heading 
data at all times, thus enabling safe autonomous navigation 
[5][6][15][16]. Although huge amounts of data are often 
collected with fixed sensors in factories, data that AMRs 
collect while driving around remains underutilised. We 
suggest that data collected with laser scanners, 3D LiDARs, 
could also be used more in production and operations 
management. 
 
 
 
Figure 1. SPOT robot as an example of an AMR. 
 
During the past ten years, the use of 3D LiDARs has 
grown rapidly. LiDAR stands for Light Detection And 
Ranging. LiDARs uses light pulses, typically emitted by a 
laser, to measure distances of its surroundings. LiDARs 
provide a precise distance point cloud relative to the AMR of 
its environment, which is then used for Simultaneous 
Localisation And Mapping (SLAM) and collision avoidance 
[6]-[8][17]-[19]. As a result, a point cloud consisting of as 
many as millions of dots is created, and a digital twin of the 
environment is created (Figure 2). More knowledge is 
needed on, how point clouds formed by AMR 3D LiDARs 
could be used to accrue benefits for production and 
operations management. 
 
 
Figure 2. Point cloud created with 3D LiDAR scanner. 
 
C. 3D LiDAR applications in various business fields 
LiDARs are widely in use in various business fields. In 
the manufacturing industry, one of the most common 
applications of LiDARs is for autonomous navigation of 
AMRs and automated guided vehicles (AGVs) in internal 
logistics operations [4]-[8]. Accurate, as-built 3D models of 
industrial sites could be used more in the manufacturing 
sector [4] for, e.g., site modernisation projects, inspection, 
safety analysis, simulation and change detection. 
In addition to AMRs and AGVs in industrial use, another 
common application area for LiDARs is transportation. 
LiDAR data can be collected, e.g., by cars on the ground, by 
airplanes or drones in the air or by satellites in space [20]. In 
these cases, LiDAR data is mobile, whereas in some cases, 
data can also be collected statically, e.g., when modelling a 
building. In this paper, we concentrate on mobile laser 
scanning, which is LiDAR mounted on a mobile platform, 
moving on a factory floor or outdoor at a factory site. Mobile 
LiDAR scanning is the most common approach for 
collecting data in transportation applications, since roads and 
various features of urban environments can be captured with 
a high level of detail [20]-[22]. Further applications for 
LiDARs include, e.g., mapping mines [23][24], cities and 
buildings [25]-[28], forests [29], plants and fields [30] and 
historical landscapes in archaeological research [31]. 
D. 3D LiDAR-based object and change detection 
LiDAR sensors target the surrounding surfaces with a 
laser beam and produce range data by measuring the time for 
the reflected light to return to the receiver. For instance, 
Ouster OS0-128 LiDAR has a 360°×90° field-of-view and a 
measuring range from 0.5m up to 100m. These wide angle, 
high resolution LiDARs provide opportunities not only for 
localisation, navigation and mapping, but also for detection 
and monitoring temporal changes. 
3D-change-detection tasks aim to find differences in a 
scene or for particular types of objects using multiple 
acquisitions of 3D data. 3D LiDARs scan the environment at 
2
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-053-7
ICAS 2023 : The Nineteenth International Conference on Autonomic and Autonomous Systems

a high speed (e.g., 20 times per second), and every time, a 
point cloud consisting of as many as millions of dots is 
created. For instance, with an angular resolution of 0.2°×0.7° 
Ouster OS0-128 LiDAR is able to provide up to 2621440 
points per second, with a precision of ±1.5–5cm. Point 
clouds collected at different times are then compared for 
detecting changes. The changes in the 3D geometry can be 
further categorised as additions (previously free space is now 
occupied), subtractions (previously occupied space is now 
free) or discrepancies, where the apparent change is caused 
by sensor noise or other sources of error  [32]. In addition to 
data differentiation, change detection may also include 
identification of changes for meaningful objects. For 
instance, monitoring of an on-street car park included 1) 
vehicle detection, 2) classification in terms of defined 
categories of vehicles and 3) change detection in terms of 
whether a car had changed from a certain spot in order to 
estimate parking duration [33]. 
3D object detection can be divided into three kinds of 
methodologies: 1) fusion-based approaches, combining RGB 
images and 3D point cloud data, 2) two-dimensional (2D) 
detection-driven methods, based on 2D bounding boxes 
defined from RGB images and 3) point-cloud-based 
methods, exploring features and topology of points to detect 
3D objects [34]. In this paper, we concentrate on the latter 
one, point-cloud-based methods. 3D LiDARs can create a 
very accurate digital 3D model and map of the environment, 
even in environments where there are people [35]. When a 
mobile robot gathers 3D LiDAR data, object steric size and 
location information are included. Another benefit of 
LiDARs is that they are insensitive to natural light, which 
makes them a potential sensor for 3D detection in 
environments with varying or poor light conditions. In many 
cases, methods based on RGB images and image recognition 
are feasible for object and change detection [36]. However, 
while comparing LiDAR-based methods with the 2D object 
detection, LiDAR-based methods can provide new potential 
applications for production and operations management. 
One state-of-the-art point-cloud-based object and change 
detection method is a method called PointPillars, in which 
point cloud data is combined with voxel-based feature 
extraction [37]. First, it organises raw point clouds as pillars 
(voxels) and then uses PointNet to learn the representation of 
point clouds. Finally, a standard 2D convolutional is used to 
enable efficient real-time detection. This method has been 
used, e.g., in autonomous vehicles for pedestrian and cyclist 
detection [38][39]. 
E. Literature synthesis, research gap and research 
question 
3D LiDARs have been traditionally used for mapping the 
environment, robot localisation and autonomous navigation 
[4]-[8]. In recent years, the advances in LiDAR technology 
have improved their range, accuracy and resolution. At the 
same time, laser scanners’ prices have gone down 
significantly in relation to their improved capabilities. Object 
detection, classification and 3D change detection can be 
done in a more detailed level than before and new use cases 
can be identified in factories. Still, within the production and 
operations management domain, there is scarcity of research 
concerning future mobile 3D LiDAR-based concepts, 
solutions and potential use cases that can emerge in this 
context. Object and change detection done with 3D LiDARs 
may offer new benefits for the industry. It is one way in 
which mobile LiDAR data collected by AMR on site can be 
taken into more efficient use in so-called smart factories. 
Thus, the main research question of this paper is the 
following: “How can mobile 3D LiDAR-based object and 
change detection enhance production and operations 
management in smart factories?” 
III. 
METHODOLOGY 
There is quite limited research within the POMS domain 
regarding the way LiDAR-based object and change detection 
could enhance operations in smart factories. Therefore, this 
study applies a qualitative case study approach, which is 
suitable for increasing the understanding of a phenomenon 
previously under investigated and thus to answer “how” 
questions [40]. The studied case includes the development of 
a solution concept for LiDAR-based object and change 
detection, its testing and collection of qualitative data on the 
research topic from the companies participating in the same 
project. VTT Technical Research Centre of Finland 
developed the technical solution. The solution is based on a 
LiDAR point-cloud comparison and helps identifying 
changes between the point clouds scanned at different times. 
The solution can be used on a mobile platform, such as 
an AMR, which is moving around an industrial site anyway 
during its preliminary task (e.g., in intra-logistics). The aim 
is to develop a solution, which would enable mobile, real-
time object and change detection in factories both indoors 
and outdoors. Parts of the solution were tested, and 
experiences gathered already in the course of this study, but 
development work is still ongoing based on the results so far. 
However, the solution concept is presented in this paper. 
The solution was developed and tested in two phases. 
The first scanning round was done in real industrial 
environment outdoors on a factory yard. The changes that 
took place on the yard were related to the trash containers 
(present/absent), the factory doors (open/closed) and cars. 
The test results were gathered and used for further 
development. The version of the solution presented in this 
paper was finalized and tested again in office environment. 
In addition to technical development work, qualitative 
data 
were 
gathered 
in 
discussions 
with 
company 
representatives. Nine companies attended the same publicly 
funded “Multi-purpose service robotics as operator business” 
R&D project in 2021-2022. The companies represent 
robotics, software and end-user companies. They attended 
several workshops during the research project, and among 
other research topics, they gave their views also into LiDAR 
object and change detection. 
Tasks and results of this study are thus twofold: 1) 
Technical development work regarding the new mobile 
LiDAR-based object and change detection solution and its 
testing, and 2) identification of use cases in the production 
and operations management domain. Finally, conclusions 
were drawn combining onsite test results and discussions 
3
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-053-7
ICAS 2023 : The Nineteenth International Conference on Autonomic and Autonomous Systems

between the researchers and the company representatives on 
potential use cases and limitations of the solution.  
IV. 
RESULTS 
A. Solution concept for 3D LiDAR-based object and 
change detection 
The large quantity of produced points by high resolution 
LiDARs makes the point-cloud processing a challenging 
task, especially if this is done in real-time. To achieve real-
time change detection with this kind of high resolution 
LiDAR, we propose a solution that is based on an open-
source C++ library called Open Visual Database System 
(OpenVDB) [41]. OpenVDB incorporates a hierarchical data 
structure and tools for the efficient storage and manipulation 
of sparse volumetric data maintained in 3D grids. This 
volumetric database is typically referred to as VDB. The 
library has been used in the visual effects industry for 
simulation and rendering of water, fire and other effects that 
rely on sparse volume data. Despite the wide use in 
numerous movie production applications over the last 
decade, the robotics community has paid little attention to 
the library. The proposed solution requires a reference point 
cloud to be converted to a NanoVDB level set grid as 
described in the steps of Figure 3. 
 
Figure 3. Steps to convert point cloud to NanoVDB level set. 
 
NanoVDB [42] is a new addition to the OpenVDB 
library to leveragme the power of Graphics Processing Units 
(GPUs) in the processing of volumetric data, whereas 
OpenVDB was originally designed to be run Central 
Processing Unit (CPU) only. NanoVDB provides a 
simplified representation of the data structures being still 
completely compatible with the OpenVDB’s tree structure. It 
offers functionality to convert back-and-forth between the 
NanoVDB and the OpenVDB data structures and to create 
and visualise the data. NanoVDB’s compacted and linearised 
representation of the VDB tree structure is read-only. In 
other words, while values can be modified in a NanoVDB 
grid, its tree topology cannot. Despite this limitation, 
NanoVDB enables notably faster collision detection and 
raytracing compared to OpenVDB’s CPU implementation 
[43]. The proposed approach for change detection follows 
the pipeline defined in Figure 4. 
Figure 4. Pipeline for the change detection. 
A NanoVDB level set grid is created from an existing 3D 
point cloud that represents the reference map of the 
environment. Because the same 3D point cloud is also used 
to localise the autonomously moving robot, the pose of the 
LiDAR scan can be used to render a matching virtual LiDAR 
scan from the NanoVDB level set grid. 3D localisation is 
done based on combining wheel or LiDAR odometry to a 
scan matching module that tries to minimise the cumulative 
drift of the odometry by registering the latest LiDAR scan or 
a submap created from multiple consecutive scans to the 
reference map [44]. 
The computationally expensive process of rendering 
virtual scans from an existing map of the environment is 
performed using NanoVDB on a GPU. For each new scan, 
the change detection is apprehended by calculating the 
difference between the LiDAR scan and the virtual LiDAR 
scan. The difference is defined according to range: 
 
range < 0: positive change (insert to OpenVDB 
grid as occluding voxel) 
 
range > 0: negative change (insert to OpenVDB 
grid as newly revealed voxel) 
 
range == 0: no change (optionally only update 
to NanoVDB grid as 'visible' voxels) 
This way the positive and negative voxels are cumulated 
to the OpenVDB grid, done in the CPU as a NanoVDB grid 
cannot 
be 
modified. 
Afterwards, 
the 
changes 
are 
accumulated to OpenVDB grid via VDBMapping. In the 
following Figure 5 positive changes detected are presented: 
 
 
Figure 5. Positive changes detected at the industrial site. 
 
The novelty of the presented method is in its approach to 
be real-time. The traditional method creates point clouds and 
only afterwards registers these to the same coordinate system 
and performs the change detection accordingly. Real-time 
approach enables more precise change detection on-site. 
To the best of the authors’ knowledge, NanoVDB is 
deployed in the field of robotics only to implement a GPU-
accelerated mapping 
and simulation package called 
NanoMap [45]. This work extends the use of OpenVDB and 
NanoVDB to LiDAR-based change detection. Also, only in 
recent years, OpenVDB has been starting to appear in open 
source projects, e.g., to implement spatiotemporal occupancy 
4
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-053-7
ICAS 2023 : The Nineteenth International Conference on Autonomic and Autonomous Systems

grid map [46], probabilistic 3D mapping [47] and efficient 
Truncated Signed Distance Field (TSDF) integration of 
range sensor data [48]. The latter presents a 3D mapping 
library, called VDBFusion, which has a higher runtime, 
lower memory consumption and disc usage compared to 
older state-of-the-art 3D mapping libraries, like OctoMap 
[49] and Voxblox [50]. 
The presented work is evaluated on an embedded 
platform, NVIDIA Jetson AGX Orin, with data collected 
using two different robotic platforms, including SPOT robot 
(Figure 1) and ROVéo, ROVENSO’s wheeled robot (Figure 
6). 
 
 
Figure 6. ROVENSO’s ROVéo AMR. 
 
B. Identified use cases in production and operations 
management 
As results of this study, use cases for LiDAR-based 
object and change detection were identified. The emphasis is 
to complement data collected with fixed sensors in factories 
with LiDAR scans done continuously on a mobile platform. 
Real-time change detection is one central novelty value of 
the solution developed by VTT. When AMR detects relevant 
and defined changes in real-time, it can send an alarm for the 
respective organisation unit (e.g., security or production) 
with a location in which the change was detected so that the 
organisation can take the needed measures. 
Another selection criterion for potential use cases was 
that purely LiDAR-based detection would be beneficial 
compared to camera or LiDAR-RGB camera-based detection 
and image recognition. Cameras and image detection is a 
more suitable method for detection of smaller details and 
items. Compared to visual light cameras, a relatively 
accurate 3D model of the environment can be created, and 
rather large items and changes can be detected. Another 
strength of LiDARs is in their use in poor lighting 
conditions. LiDARs are already commonly used in AGVs 
and AMRs, so no new sensor or camera investments would 
be needed, but the idea would be to better utilise data 
collected by those moving around in factories. 
When identifying potential use cases for mobile LiDAR-
based object and change detection, the following criteria and 
features of LiDAR-based detection were considered: 
- 
Added value from AMR moving around the factory and 
factory site and detecting changes in real-time 
- 
Achieving relatively accurate and up-to-date digital 3D 
reconstruction of the environment of the AMR with 
repeated LiDAR scans 
- 
Visual detection of rather large physical items and 
changes 
- 
LiDARs work also in poor or limited lighting 
conditions. 
The next Table outlines identified use cases for LiDAR-
based object and change detection in production and 
operations management: 
TABLE I.  
INDUSTRIAL USE CASES FOR LIDAR-BASED OBJECT AND 
CHANGE DETECTION. 
Industrial 
operation 
Use cases 
Internal 
logistics 
- Pallets and other large items left in the wrong place 
- Blocked routes of AMRs and AGVs and sending 
information to the fleet management system or 
operators 
- Cars left in the wrong place (e.g., on AMR/AGV 
driving lanes) in the yard 
Warehouse 
- Pallets, trolleys or other large items left in the wrong 
place in the warehouse 
Production 
- Large items left in the wrong place on aisles or in 
production cells: links to lean, 5S and safety 
- Digital as-is 3D model as basis for machinery and 
production line modernisation 
Security 
surveillance 
- Blocked fire doors or large items left in the front of 
emergency exits that should be kept clean 
- Factory gate or open doors, e.g., at night when there 
or no people at work 
- Holes in factory area fence 
Building 
maintenance 
- As-is model of the factory as basis for factory 
renovation and modernisation and progress 
monitoring 
 
V. 
CONCLUSIONS 
RGB images and image recognition is often feasible for 
visual change detection [36]. However, in certain cases, an 
accurate digital 3D model of the environment created with 
LiDARs and detecting changes repeatedly in real-time 
accrue benefits for production and operations management. 
This study identified use cases for mobile, real-time LiDAR-
based object and change detection. The new solution 
developed and presented enables efficient visual 3D data 
utilisation in operations management. The study shows 
practical implications of Industry 4.0 and 5.0 in terms of 
utilising autonomous systems and digital twins. 
As the main results of the study, we identified potential 
use cases in production, internal logistics, security and 
maintenance 
operations 
for 
the 
developed 
solution. 
Identifying, e.g., obstacles on AGV and AMR routes and 
sending the information to the operators of fleet management 
systems helps AGVs and AMRs update their routes and 
continue their tasks smoothly. Thus, the solution may help in 
optimising traffic flows in factories. Detecting large items 
5
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-053-7
ICAS 2023 : The Nineteenth International Conference on Autonomic and Autonomous Systems

left in the wrong place and sending alarms for respective 
organisation units enhance intra-logistics operations, tidiness 
and safety. For security purposes, the solution identifies e.g. 
doors left open at nightime. 
The 
study 
results 
contribute 
to 
the 
operations 
management literature [4]-[8] by presenting a new technical 
enabler for object and change detection in factories. The 
main novelty value of the solution is the aim of detecting 
changes in real-time on repetitive rounds of AMRs. Real-
time approach enables more precise change detection on-site 
and provides many future development possibilities e.g. in 
terms of sending alarms of noted changes that might cause 
safety or other hazards. Thus, the results complement earlier 
studies on mobile change detection [33, 35]. Point cloud data 
collected with LiDARs will support more intelligent decision 
making in companies concerning, for example, production 
and intra-logistics [1][11][12]. 
As with any study, this one also has limitations. One 
limitation is that the technical solution is not ready yet, but is 
still under development. More testing and development will 
be needed in various industrial environments in order to 
ensure its reliability and accuracy. The accuracy of the 
change detection depends heavily on the accuracy of the 3D 
pose estimation of the LiDAR, which may be degraded due 
to measurement noise, motion distortion, and reflections 
from shiny surfaces. Moreover, major geometric changes in 
the environment (e.g., some walls removed) may also 
degrade the accuracy of the pose estimation. Another central 
limitation is that, in many cases, object and change detection 
is more feasible and cheaper to conduct with cameras. On the 
other hand, the use of LiDARs is already common in AGVs 
and AMRs, which reasons for wider utilisation of the data 
they collect. What remains important is to carefully evaluate 
which cases, e.g., RGB camera or RGB camera-LiDAR 
solution, are feasible and more reliable than a solution based 
solely on LiDAR. LiDARs’ strengths remain in their ability 
to make a relatively accurate digital twin of the environment. 
Use cases that benefit from those have the foremost potential 
for LiDAR-based solutions.  
Despite the limitations, this study inspires multiple 
interesting future R&D avenues. Within the production and 
operations management domain, autonomous systems, 
extensive data utilisation and so-called digital twins will 
provide much to study and develop in the future. For 
example, the fusion of multimodal data that AMRs collect 
can 
offer 
many 
possibilities 
for 
developing, 
e.g., 
manufacturing, intra-logistics and maintenance operations to 
be safer and more efficient. For example, one can integrate 
thermal camera and gas sensors onto a mobile robot 
platform, and data collected by those can be used in 
combination with LiDAR-generated maps for safety. More 
research will certainly arise from applying the new object 
and change detection method developed for various 
industries. Construction is one business field that would 
certainly benefit from real-time 3D modelling and change 
detection. 
 
 
ACKNOWLEDGMENT 
The MURO research project is funded by Business 
Finland, VTT and companies participating in the project. 
REFERENCES  
[1] B, Chen et al., “Smart factory of industry 4.0: Key 
technologies, application case, and challenges,” IEEE Access, 
vol. 6, pp. 6505-6519, 2017. 
[2] R. Morrar, H. Arman, and S. Mousa, “The fourth industrial 
revolution (Industry 4.0): A social innovation perspective,” 
Technology innovation management review, vol. 7, no. 11, 
pp. 12-20, 2017. 
[3] P. K. R. Maddikunta et al., “Industry 5.0: A survey on 
enabling technologies and potential applications,” Journal of 
Industrial Information Integration, vol. 26, pp. 1-19, 2022. 
[4] Z. M Bi and L. Wang, “Advances in 3D data acquisition and 
processing for industrial applications,” Robotics and 
Computer-Integrated Manufacturing, vol. 26, no. 5, pp. 403-
413, 2010. 
[5] G. Fragapane, R. De Koster, F. Sgarbossa, and J. O. 
Strandhagen, “Planning and control of autonomous mobile 
robots for intralogistics: Literature review and research 
agenda,” European Journal of Operational Research, vol. 294, 
no. 2, pp. 405-426, 2021. 
[6] V. De Silva, J. Roche, and A. Kondoz, “Robust fusion of 
LiDAR and wide-angle camera data for autonomous mobile 
robots,” Sensors, vol. 18, no. 8, pp. 2730, 2018. 
[7] T. Yang et al., “3D ToF LiDAR in mobile robotics: a review,” 
arXiv preprint arXiv:2202.11025, 2022. 
[8] D. Zhang, J. Cao, G. Dobie, and C. MacLeod, “A framework 
of using customized LIDAR to localize robot for nuclear 
reactor inspections,” IEEE Sensors Journal, vol. 22, no. 6, pp. 
5352-5359, 2021. 
[9] T. M. Choi, S. Kumar, X. Yue, and H. L. Chan, “Disruptive 
technologies and operations management in the Industry 4.0 
era and beyond,” Production and Operations Management, 
vol. 31, no. 1, pp. 9-31, 2022. 
[10] H. Lasi, P. Fettke, H. G. Kemper, T. Feld, and M. Hoffmann, 
“Industry 4.0. Business & information systems engineering,” 
vol. 6, no. 4, pp. 239-242, 2014. 
[11] D. Bertsimas, N. Kallus, and A. Hussain (2016), “Inventory 
management in the era of big data,” Production and 
Operations Management, vol. 25, no. 12, pp. 2006-2009, 
2014. 
[12] T. M. Choi, S. W. Wallace, and Y. Wang, “Big data analytics 
in operations management,” Prod. Oper. Manag. vol. 27, no. 
10, pp. 1868–1883, 2018. 
[13] K. Shafique, B. A. Khawaja, F. Sabir, S. Qazi, and M. 
Mustaqim, “Internet of things (IoT) for next-generation smart 
systems: A review of current challenges, future trends and 
prospects for emerging 5G-IoT scenarios,” Ieee Access, vol. 
8, pp. 23022-23040, 2020. 
[14] T. L. Olsen and B. Tomlin, “Industry 4.0: Opportunities and 
challenges for operations management,” Manufacturing & 
Service Operations Management, vol. 22, no. 1, pp. 113-122, 
2020. 
[15] E. Guerra, R. Munguía, and A. Grau, “UAV visual and laser 
sensors fusion for detection and positioning in industrial 
applications,” Sensors, vol. 18, no.7, pp. 2071, 2018. 
[16] P. Wei, L. Cagle, T. Reza, J. Ball, and J. Gafford, “LiDAR 
and camera detection fusion in a real-time industrial multi-
sensor collision avoidance system,” Electronics, vol. 7, no. 6, 
pp. 84, 2018. 
6
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-053-7
ICAS 2023 : The Nineteenth International Conference on Autonomic and Autonomous Systems

[17] C. Debeunne and D. Vivet, “A review of visual-LiDAR 
fusion based simultaneous localization and mapping,” 
Sensors, vol. 20, no. 7, pp. 2068, 2020. 
[18] W. Lu, Y. Zhou, G. Wan, S. Hou, and S. Song “L3-net: 
Towards learning based lidar localization for autonomous 
driving,” Proceedings of the IEEE/CVF Conference on 
Computer Vision and Pattern Recognition, pp. 6389-6398, 
2019. 
[19] D. Van Nam and K. Gon-Woo, “Solid-state LiDAR based-
SLAM: A concise review and application,” 2021 IEEE 
International Conference on Big Data and Smart Computing 
(BigComp), pp. 302-305, 2021. 
[20] S. Gargoum and K. El-Basyouny “Automated extraction of 
road features using LiDAR data: A review of LiDAR 
applications in transportation,” In 2017 4th International 
Conference on Transportation Information and Safety 
(ICTIS), pp. 563-574. IEEE, August 2017. 
[21] S. A. Gargoum and K. El Basyouny, “A literature synthesis of 
LiDAR applications in transportation: Feature extraction and 
geometric assessments of highways,” GIScience & Remote 
Sensing, vol. 56, no.6, pp. 864-893, 2019. 
[22] H. Guan, J. Li, S. Cao, and Y. Yu, “Use of mobile LiDAR in 
road information inventory: A review. International Journal of 
Image and Data Fusion”, vol. 7, no. 3, pp. 219-242, 2016. 
[23] T. Neumann, A. Ferrein, S. Kallweit, and I. Scholl, “Towards 
a mobile mapping robot for underground mines,” Proceedings 
of the 2014 PRASA, RobMech and AfLaT International Joint 
Symposium, Cape Town, South Africa, pp. 27-28, 2014. 
[24] R. Zlot and M. Bosse, “Efficient large-scale 3D mobile 
mapping and surface reconstruction of an underground mine,” 
Field and service robotics, pp. 479-493, Springer, Berlin, 
Heidelberg, 2014. 
[25] M. Awrangjeb, “Effective generation and update of a building 
map database through automatic building change detection 
from LiDAR point cloud data,” Remote Sensing, vol. 7, no. 
10, pp. 14119-14150, 2015. 
[26] J. L. Blanco-Claraco, F. A. Moreno-Duenas, and J. González-
Jiménez, “The Málaga urban dataset: High-rate stereo and 
LiDAR in a realistic urban scenario,” The International 
Journal of Robotics Research, vol. 33, no. 2, pp. 207-214, 
2014. 
[27] A. Börcs, B. Nagy, and C. Benedek, “Instant object detection 
in lidar point clouds,” IEEE Geoscience and Remote Sensing 
Letters, vol. 14, no. 7, pp. 992-996, 2017. 
[28] X. Meng, N. Currit, L. Wang, and X. Yang, “Detect 
residential buildings from lidar and aerial photographs 
through 
object-oriented 
land-use 
classification,” 
Photogrammetric Engineering & Remote Sensing, vol. 78, 
no.1, pp. 35-44, 2012. 
[29] J. Schreyer, J. Tigges, T. Lakes, and G. Churkina, “Using 
airborne LiDAR and QuickBird data for modelling urban tree 
carbon storage and its distribution—A case study of Berlin,” 
Remote Sensing, vol. 6, no. 11, pp. 10636-10655, 2014. 
[30] U. Weiss and P. Biber, “Plant detection and mapping for 
agricultural robots using a 3D LIDAR sensor,” Robotics and 
autonomous systems, vol. 59, no. 5, pp. 265-273, 2011. 
[31] A. S. Chase, D. Z. Chase, and A. F Chase, “LiDAR for 
archaeological 
research 
and 
the 
study 
of 
historical 
landscapes,” Sensing the Past, pp. 89-100. Springer, Cham, 
2017. 
[32] J. P. Underwood, D. Gillsj ̈o, T. Bailey, and V. Vlaskine, 
“Explicit 3d change detection using ray-tracing in spherical 
coordinates,” 2013 IEEE international conference on robotics 
and automation, pp. 4735–4741. IEEE, 2013. 
[33] W. Xiao, B. Vallet, K. Schindler, and N. Paparoditis, “Street-
side vehicle detection, classification and change detection 
using mobile laser scanning data,” ISPRS Journal of 
Photogrammetry and Remote Sensing, vol. 114, pp. 166-178, 
2016. 
[34] Y. Wu, Y. Wang, S. Zhang, and H. Ogai, “Deep 3D object 
detection networks using LiDAR data: A review,” IEEE 
Sensors Journal, vol. 21, no. 2, pp. 1152-1171, 2020. 
[35] I. Jinno, Y. Sasaki, and H. Mizoguchi, “3d map update in 
human environment using change detection from lidar 
equipped mobile robot,” 2019 IEEE/SICE International 
Symposium on System Integration (SII) (pp. 330-335). IEEE. 
[36] K. Simonyan and A. Zisserman, “Very deep convolutional 
networks for large-scale image recognition,” arXiv preprint 
arXiv:1409.1556, 2014. 
[37] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. 
Beijbom, “PointPillars: Fast encoders for object detection 
from point clouds,” In Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition, Long Beach, 
CA, USA, pp. 12697–12705, June 2019. 
[38] Y. Zhou et al., “End-to-end multi-view fusion for 3d object 
detection in lidar point clouds,” Conference on Robot 
Learning, pp. 923-932, 2020. 
[39] H. Kuang, B. Wang, J. An, M. Zhang, and Z. Zhang, “Voxel-
FPN: Multi-scale voxel feature aggregation for 3D object 
detection from LIDAR point clouds,” Sensors, vol. 20, no. 3, 
pp. 704, 2020. 
[40] R. K. Yin. Case study research—Design and methods. 
Thousand Oaks: Sage Publications, 2003. 
[41] K. Museth et al. “Openvdb: an open-source data structure and 
toolkit for high-resolution volumes,” in Acm siggraph 2013 
courses, pp. 1–1, 2013. 
[42] K. Museth, “Nanovdb: A gpu-friendly and portable vdb data 
structure for real-time rendering and simulation,” ACM 
SIGGRAPH 2021 Talks, pp. 1-2, 2021. 
[43] W. Braithwaite and K. Muset, “NVIDIA Technical Blog: 
Accelerating 
OpenVDB 
on 
GPUs 
with 
NanoVDB”, 
https://developer.nvidia.com/blog/accelerating-openvdb-on-
gpus-with-nanovdb/. Accessed: 2022-12-02. 
[44] D. Rozenberszki and A. L. Majdik, “LOL: Lidar-only 
Odometry and Localization in 3D point cloud maps”, IEEE 
International Conference on Robotics and Automation 
(ICRA), pp. 4379-4385, 2020. 
[45] V. Walker, F. Vanegas, and F. Gonzalez, “NanoMap: A gpu-
accelerated openvdb-based mapping and simulation hpackage 
for robotic agents,” Remote Sensing, voil. 14, no. 21, pp. 
5463, 2022. 
[46] S. Macenski, D. Tsai, and M. Feinberg, “Spatiotemporal 
voxel layer: A view on robot perception for the dynamic 
world,” International Journal of Advanced Robotic Systems, 
vol. 17, no. 2, pp. 1-14, 2020. 
[47] M. Grosse Besselmann, L.  Puck, L. Steffen, A. Roennau, and 
R. Dillmann, “Vdb-mapping: A high resolution and real-time 
capable 3d mapping framework for versatile mobile robots,” 
2021 IEEE 17th International Conference on Automation 
Science and Engineering (CASE), pp. 448–454. IEEE, 2021. 
[48] V. Ignacio, T. Guadagnino, J. Behley, and C. Stachniss. 
Vdbfusion, “Flexible and efficient tsdf integration of range 
sensor data,” Sensors, vol. 22, no. 3, pp. 1-24, 2022.  
[49] A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and 
W. Burgard, “Octomap: An efficient probabilistic 3d mapping 
framework based on octrees,” Autonomous robots, vol. 3, no. 
3, pp. 189-206, 2013. 
[50] H. Oleynikova, Z. Taylor, M. Fehr, R. Siegwart, and J. Nieto, 
“Voxblox: Incremental 3d euclidean signed distance fields for 
on-board mav planning,” IEEE/RSJ International Conference 
on Intelligent Robots and Systems (IROS), pp. 1366-1373. 
IEEE, 2017. 
 
7
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-053-7
ICAS 2023 : The Nineteenth International Conference on Autonomic and Autonomous Systems

