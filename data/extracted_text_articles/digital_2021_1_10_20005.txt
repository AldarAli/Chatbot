Dimensions of Fake News 
 
Paulina Schindler 
Chair of Business Informatics, 
Friedrich Schiller University Jena 
Jena, Germany 
e-mail: paulina.schindler@uni-
jena.de 
Marek Opuszko 
Department of Business 
Administration, Ernst-Abbe 
Hochschule Jena, University of 
Applied Sciences 
Jena, Germany 
e-mail: marek.opuszko@eah-
jena.de 
Meena Stöbesand 
Department of Business 
Administration, Ernst-Abbe 
Hochschule Jena, University of 
Applied Sciences 
Jena, Germany 
e-mail: meena.stoebesand@eah-
jena.de 
 
 
Abstract - “Fake news” has become a common buzzword in 
public, political, and scientific debates. Whereas the definition 
of the term and its political consequences are often highlighted, 
this paper seeks to provide an overview of the development, the 
most common dimensions of fake news, and their mode of 
action. Research shows that fake news can trigger and act in 
conjunction with numerous effects that influence recipients. A 
comprehensive overview of these effects is given in this paper. 
 Keywords 
- 
fake 
news; social 
media; 
misinformation; 
disinformation.  
I. INTRODUCTION 
In 1835, the New York Sun published a story by Richard 
Adam Locke saying that the renowned astronomer Sir John 
Herschel discovered life on the moon. The story was 
published for of a few days with new information about the 
discovery including the geography, lunar vegetation, and the 
inhabitants: bat-people. The story was reprinted by other 
papers and the New York Sun’s circulation increased from 
about 4.000 daily sales to 19.000 [1]. Of course, the Great 
Moon Hoax was made up by Locke without the knowledge 
of Herschel who was not amused to see his name used [2]. 
Locke later explained that he intended to write a satire and 
never meant it to be a hoax; his goal was to mock the 
gullibility of Americans and their belief in extraterrestrial life 
[2]. Locke’s famous news stunt would probably be called 
fake news today. In recent history, the term “fake news” is 
heavily associated with the emergence of social media or the 
role it played in the 2016 US general election [3]. Since the 
incident at a press conference in the White House on 
February 16, 2017, when the then President Donald Trump 
called CNN media representatives "fake news", refusing to 
allow any questions, the term has become a hot topic in the 
scientific discussion about modern and social media [4]. 
However, the frequent use and popularity of the term led to a 
more and more blurred understanding and vastly different 
interpretations among scholars and the general population, 
leaving it unclear as to what is considered fake news [5]. It is 
further unclear what mechanisms of action of fake news are 
prevalent in social media.  
In order to grasp fake news as a whole, a comprehensive 
understanding 
of 
the 
associated 
mechanisms 
and 
dissemination methods is important. Above all, this is 
necessary to recognize all aspects and occurrences of false 
and misinformation in social media. This work aims to 
provide a uniform view and to deliver a basic systematization 
of all dimensions of fake news and its prevalence in modern 
media.  
The rest of the paper is structured as follows. In order to 
classify the fuzzy concept of fake news, we will present the 
historical development of the term fake news in Section 2. In 
Section 3, we will discuss the methodology used to capture 
all dimensions of the phenomenon of fake news. We will 
present the results of a comprehensive literature and study 
analysis and characterize a total of 28 dimensions of fake 
news. In the final section, we evaluate the dangers of fake 
news, classify the results of this study, and provide an outlook 
for future studies. 
II. 
THE DEVELOPMENT OF FAKE NEWS 
The term "fake news" was recorded in lexicons in the 
USA as early as the late 19th century. Prior to that, the term 
"false news" was used [6]. Originally used to refer to made-
up or false news [7], today the term “fake news” is also used 
to refer to false news on social media, to undermine work by 
news outlets [8] [9] or to describe fabricated news in satirical 
contributions [10]. Fake news is often described as 
intentionally 
deceptive 
[3][11]. 
Other 
sources 
also 
acknowledge the possibility that the dissemination of fake 
news may also happen unintentionally [6][12]. Apart from 
the pure intention to deceive, other motivations, such as 
political ideologies or financial goals, are sometimes also 
attributed to creators of fake news [3]. Similarly, some 
sources define fake news as being written in a way that has 
news characteristics [9][13]. For some authors, online 
dissemination is an important aspect of fake news [14] or 
even a characteristic of it [15], while other sources do not pay 
particular attention to it [6]. Some definitions require fake 
news to be completely false, i.e., to have no basis of fact 
underneath [3][16], which raises the issue of classifying half-
truths and manipulating the context with a "core of truth". 
Tandoc et al. address this problem by distinguishing high and 
low levels of "facticity” [9]. Another approach to the 
argument is to call something "fake news" only if the 
intended deception has succeeded, otherwise, it is just fiction 
[9]. In a contrasting theory, fake news does not necessarily 
have to be believed in order to be considered as such [17]. In 
contrast to lies, fake news has less socially motivated 
1
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-927-0
DIGITAL 2021 : Advances on Societal Digital Transformation

purposes, such as protecting oneself or avoiding harm, but 
rather serves those who create it to achieve financial or 
political goals or to promote themselves [18].  
It becomes clear that the definition of the term determines 
the time of recognition and what is counted as fake news; 
therefore, historical examples or its different types and sub-
categories vary. In this work, the term fake news refers to the 
deliberate dissemination of erroneous information by the 
creator with the intent to deceive. 
Examples of fake news could be documented in the pre-
printing press era. While the invention of the printing press 
and the spread of literacy helped the spread of knowledge, it 
also resulted in its monetization [19].  Information became a 
commodity that could be produced, published, and sold in a 
high number of copies. Through the 17th century in France, 
false stories became very popular on printed broadsides: the 
so-called “canards” [19]. Progressing in history, mass media 
and the press have had an important role in the spread of 
hoaxes.  
Fake news today differs from the historical examples due 
to the instant and global distribution through the new media 
and the “systemic ways in which fake news mobilizes our 
cognitive biases and heuristics” [20]. The motivations did not 
change, but they increased on a global level: young 
Macedonian people spreading fake news for the US election 
2016 with no other interest than money, Donald Trump 
defining established mainstream media as “fake news“ or the 
famous so-called “pizzagate“ conspiracy theory which 
culminated in a shooting [10][20][21]. Spreading real or fake 
news through online media and social networks led to an 
enormous amount of information, making it more difficult to 
classify its validity. In addition, producers of fake news make 
use of the design of established news sources disguising their 
origin and intent [8]. Therefore, the consequences of fake 
news combined with social media are toxic and explosive 
because they make it possible for the creator to target an 
audience specifically and manipulate cognitive biases 
[20][22]. These dimensions of fake news, which can be direct 
effects of fake news or work in conjunction with them, are 
described next. 
III. 
THE DIMENSIONS OF FAKE NEWS 
To obtain information on the development and impact of 
fake news, first, a broad literature search was conducted via 
Google Scholar using the search terms "fake news” and “fake 
news effects”. Since this search term delivers over half a 
million search results, the search was further narrowed down 
to results that deal with fake news and its effects on people. 
After reviewing over 500 articles, 28 different effects or 
mechanisms of action could be identified. To examine these 
more closely, a snowball and depth-first search was then 
carried out for each effect. 
The creators of fake news often use various mechanisms 
or effects that work in conjunction with fake news and can 
make false information more effective for the recipient, 
amplify 
existing 
effects 
or 
immunize 
against 
counterarguments. These dimensions can be used by the 
creators of fake news. However, some dimensions that can 
play a significant role in the effectiveness of fake news are 
not necessarily used purposefully. Instead, these can result 
from the recipient's environment or handling of fake news. In 
TABLE I, common influential dimensions and effects in 
conjunction with fake news are listed and explained in 
alphabetical order according to their most common name (if 
available). 
 
TABLE I.  DIMENSIONS OF "FAKE NEWS". 
Dimension 
Explanation 
Astroturfing 
Astroturfing is an attempt to convey an incorrect 
impression of public opinion, e.g. by feigning 
that a large majority of people is in favor of a 
certain decision. In contrast to a “grassroots 
movement“, however, the population is not 
actually behind it, but it is organized by a covert 
initiator [23]–[27].   
Even though it is not a new phenomenon [28], 
astroturfing can spread more effectively by 
means of the Internet [29][30]. 
Availability 
Cascade 
Individuals tend to adopt the views of others 
when those views gain popularity in their social 
environment [31]–[33]. Informational cascades 
and reputational cascades can make this possible 
through different motivations and may occur 
together [32].   
Availability 
heuristic 
The probability of events is measured by how 
available a similar event is in memory. So a 
recent or frequent reporting of certain events 
ensures that they are considered more likely 
[34]–[40]. This also applies if these reports are 
purely thought-provoking [41].  
Backfire effect 
It was found that subjects believed even more 
strongly in the original, incorrect information 
after it had been corrected [42][43]. It is 
assumed that this effect only occurs in specific 
situations, since it could not be proven with 
another experimental setup [44]–[47]. If 
judgements are formed immediately during 
reception, backfire effects can be reduced [48]. 
Research suggests that emotions may be 
relevant in this process [49]. Additional research 
is needed [50]. This can also be called 
“boomerang effect” [51][52]. 
Bandwagon 
effect 
This refers to the assumption that if other people 
perceive something as good, it will also be 
judged good by oneself [53]–[55]. Own 
opinions are formed on the basis of other 
people’s opinions. This phenomenon has also 
been observed in online reviews, for example 
[55][56].  
Clickbait 
Information gaps created by news titles arouse 
the potential reader's curiosity for the rest of the 
article. Often a forward reference is used, which 
refers to further information in the article [57]–
[59]. This may increase the readership of an 
article but does not necessarily have further 
negative effects [60].  
Confirmation 
bias 
People unconsciously prefer information that 
coincides with their own opinion. If it does, they 
consider it more credible [61]–[66]. It is 
suspected that this contributes to the emergence 
of echo chambers and filter bubbles [67].  
Conservatism 
bias 
This refers to the tendency of individuals to 
inadequately adjust their attitudes when 
2
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-927-0
DIGITAL 2021 : Advances on Societal Digital Transformation

confronted with new information [64][68]–[70]. 
Thus, if a person already believes in fake news, 
their beliefs are difficult to correct. 
Continued 
influence effect 
Even the negation and correction of incorrect 
original information usually cannot completely 
reverse its effect. It continues to influence the 
recipient [71]–[76], even when warnings are 
given [77]. This effect is weakened if instead of 
a simple correction a suitable alternative 
explanation for a scenario is offered [78]. Partly 
this may be because recipients do not accept a 
correction [79]. This is also called "belief 
perseverance" [70][80]. 
Echo chamber 
effect 
If users mainly interact with other users or 
institutions that have a similar opinion to their 
own, an echo chamber is created. The users thus 
reinforce each other's opinions [81]–[83]. The 
confirmation bias works in a similar way. Often, 
however, users are not completely isolated, but 
continue to be confronted with opposing 
content, especially online [84]–[87]. Thus, this 
effect may not be as strong as initially 
perceived.  
Emotional 
memory 
enhancement 
Emotionally charged information is better 
retained than neutral information [88]–[92]. 
Suggestion has an even stronger effect than pure 
emotionality [92].  
Filter bubble 
This term refers to information bubbles that are 
created in social media in particular and in 
which algorithms select or pre-filter content that 
is then displayed to the user. This content often 
corresponds to existing interests. Users are often 
unaware of the filter bubble [93]–[97]. Thus, no 
contrary opinions are displayed that could 
invalidate fake news. This effect is similar to 
echo chambers. It is still debated whether filter 
bubbles exist and are problematic as some 
evidence points against it [87][95][98][99].  
Framing effect 
Small changes in context or in the way 
information is conveyed can lead to a major 
change in decision-making behavior [100]–
[102]. Emotions may be an important aspect of 
this [103][104]. The effect of framing can be 
reduced through warnings  [105][106].   
Google effect 
People tend not to remember information in 
itself, but instead where it can be found when 
needed [107][108]. Thus, insufficient 
background knowledge of a person might not be 
enough to counter fake news. 
Hostile media 
effect 
Biased subjects feel disadvantaged by media 
coverage, even if a large proportion of recipients 
perceive it as appropriate [109]–[114]. The 
disadvantage is perceived to be unfavorable of 
one's own opinion. This may reduce belief in the 
correction of fake news by major news outlets. 
Illusory 
truth 
effect 
Statements that are heard several times are 
attributed a higher truth value than statements 
that are heard for the first time [115]–[118]. 
This means that repetition increases the 
probability that a statement will be considered 
true. This is true even if the plausibility of the 
statement is low [119] or in the case of warnings 
against it [120]. This effect is also referred to as 
the "validity effect" [121].   
Implied 
truth 
effect 
If other news is recognized or labeled as fake 
news, but one is not, it is more likely to be 
considered true [122]. However, this effect may 
be small [123].  
Informational 
cascade 
People who lack complete information on a 
subject may rely on the perceived beliefs of 
others [32][124]–[126]. A decision is made 
based on the decisions of others, even ignoring 
personal knowledge, expecting the crowd to be 
right. This is called an “informational cascade” 
or “information cascade”. This way, fake news 
can be propagated through a network. The 
strength of ties of a person to the other people in 
their group may influence their decision-making 
behavior [127]. Self-corrections by further 
cascades are possible [128]. 
Misdirecting 
Misdirecting is employed when contextual 
hashtags are used in social media, but a 
completely different topic is being reported on 
[129]. To do this en masse, social bots can be 
used. This distracts from the actual topic and 
actual information is lost in the amount of news 
[130]. In another study, this could not be 
detected [131]. 
Misinformatio
n effect 
Untruthful reporting following an event 
damages the correct memory of that event 
[132]–[137]. Later corrections may be able to 
reduce that effect [138].   
Negativity bias 
People have a tendency to give more weight to 
negative information than to positive 
information [66][139]–[144].  
Primacy effect 
& 
Recency 
effect 
Information that a recipient takes in first has a 
stronger impact on them than the information 
that follows (primacy effect). Likewise, the 
information received last remains in the memory 
longer (recency effect) [145]–[149]. The 
primacy effect may be stronger than the recency 
effect [147]–[150].  
Reputational 
cascade 
Like with the informational cascade, people 
base their decisions on the decisions of their 
peers. However, here they do so regardless of 
their own thoughts because they are motivated 
to earn social approval and avoid disapproval 
[32][151][152]. Because of the perceived social 
pressure, this cascade may be more resilient 
than informational cascades [153]. 
Reputation 
heuristic 
Instead of checking the content of a source’s 
information, the source itself is checked for 
credibility. If the source has a good reputation 
or is considered credible, the information is 
more likely to be believed [73][154]–[157]. If 
fake news creators succeed in imitating a 
credible source, their credibility increases. 
Rumor 
refutation 
Rumors on social media that are incorrect take 
longer to be resolved than true rumors. 
Unverified rumors are often shared earlier and 
reach a larger user base than resolved rumors 
[158][159].  
Smoke 
screening 
Smoke screening works like misdirecting with 
the difference that at least similar content to a 
hashtag is posted [129]–[131].  
Tainted 
truth 
effect 
Warnings of false information issued 
erroneously in relation to truthful content can 
damage the credibility of the truthful 
information [137][160][161].  
3
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-927-0
DIGITAL 2021 : Advances on Societal Digital Transformation

Third 
person 
effect 
People tend to believe that mass media 
influence other people more strongly than they 
influence themselves [162]–[167]. As a result, 
the influence of fake news on oneself can be 
underestimated. 
 
Human memory can be affected by internal and external 
influences and is not infallible [92]. The effects of fake news 
that operate in this context are presented in this paper. Fake 
news can initially be an external influence with numerous 
associated effects on a person's perception. How strong the 
impact of these effects ultimately turns out also depends on 
the internal circumstances of this person. While some 
personal characteristics may support the effect of fake news, 
others weaken it. The susceptibility to fake news can be 
influenced, for example, by a tendency toward analytical 
thinking [168], skeptical attitudes [168], emotions [169], 
frequency of media use [3], conditions of one's own networks 
[3][168][173], age [170], and the culture from which 
someone comes [171][172]. Some effects may bypass some 
of these factors by operating at a low cognitive level [168].  
Numerous other, even previously undiscovered or 
unexplored internal possibilities of influence by a subject's 
personality or attitude may exist. Thus, in addition to further 
investigation of the effects of fake news, a closer look at the 
recipients of fake news and their circumstances also offers 
research potential for the future. 
IV. 
CONCLUSION 
Fake news has been with mankind for a long time and has 
made multiple appearances in the past. Although the 
phenomenon of fake news may not be new, it is crucial to 
understand that the latest developments are a danger to 
democratic societies. In this work, the basis for the 
understanding of various phenomena in the field of fake news 
is laid in order to ensure a holistic view of the topic for future 
research projects. Fake news can be spread particularly easily 
and quickly through modern technologies such as social 
media. Furthermore, it is evident that fake news and its 
impact should be considered within the respective cultural, 
social and political contexts [10]. This makes the dimensions 
with which fake news works even more relevant for current 
discussions, even more so when emotions are considered 
more valuable than facts [21]. Since it has been shown that 
fake news can influence a person's opinion formation in 
numerous ways, a danger to opinion formation in society as 
a whole is possible. Therefore, especially regarding the 
aspects of opinion formation and freedom of expression, 
attention should be paid to fake news and, if necessary, its 
spread should be curbed. The dimensions of fake news 
presented in this paper can be used in further work and serve 
as a reference standard to better classify and categorize fake 
news effects in social media, but also beyond. In this way, 
further studies could investigate which effects are 
particularly prevalent in the various social media. 
 
 
 
 
REFERENCES 
 
[1] 
B. Thornton, “The Moon Hoax: Debates About Ethics in 1835     
New York Newspapers”, Journal of Mass Media Ethics, vol. 
15, 
no. 
2, 
pp. 
89–100, 
2000, 
doi: 
10.1207/S15327728JMME1502. 
[2] 
S. W. Ruskin, “A Newly-Discovered Letter of J.F.W. 
Herschel concerning the ‘Great Moon Hoax’,” Journal for the 
History of Astronomy, vol. 33, no. 1, pp. 71–74, 2002, doi: 
10.1177/002182860203300108. 
[3] 
H. Allcott and M. Gentzkow, “Social Media and Fake News 
in the 2016 Election,” Journal of Economic Perspectives, vol. 
31, no. 2, pp. 211–236, 2017, doi: 10.1257/jep.31.2.211. 
[4] 
J. Acosta, “How Trumps's 'fake news' rhetoric has gotten out 
of control”. [Online]. Available: https://edition.cnn.com/
2019/06/11/politics/enemy-of-the-people-jim-acosta-donald-
trump/index.html (retrieved: October, 2021). 
[5] 
T. Quandt, L. Frischlich, S. Boberg, and T. Schatto‐Eckrodt, 
“Fake News,” in The International Encyclopedia of 
Journalism 
Studies, 
T. 
P. 
Vos, 
F. 
Hanusch, 
D. 
Dimitrakopoulou, M. Geertsema-Sligh, and A. Sehl, Eds.: 
Wiley, 2019, pp. 1–6. 
[6] 
C. McManus and C. Michaud, “Never Mind the Buzzwords: 
Defining Fake News and Post-Truth,” in Fake News - A 
Roadmap, King’s Centre for Strategic Communications and 
NATO Strategic Communications Centre of Excellence, Eds., 
Riga, London: King’s Centre for Strategic Communications; 
NATO Strategic Communications Centre of Excellence, 
2018, pp. 14–20. 
[7] 
C. Borchers, “‘Fake News’ Has Now Lost All Meaning”. 
[Online]. Available: https://www.washingtonpost.com/news/
the-fix/wp/2017/02/09/fake-news-has-now-lost-all-meaning/
?utm_term=.70fa3df16f17 (retrieved: August, 2020). 
[8] 
I. Stavre and M. Puntí, “Fake News, Something New?,” 
Sociology and Anthropology, vol. 7, no. 5, pp. 212–219, 2019, 
doi: 10.13189/sa.2019.070504. 
[9] 
E. C. Tandoc, Z. W. Lim, and R. Ling, “Defining ‘Fake 
News’,” Digital Journalism, vol. 6, no. 2, pp. 137–153, 2018, 
doi: 10.1080/21670811.2017.1360143. 
[10] H. Wasserman, “Fake news from Africa: Panics, politics and 
paradigms,” Journalism, vol. 21, no. 1, pp. 3–16, 2020, doi: 
10.1177/1464884917746861. 
[11] M. Sullivan, “It’s time to retire the tainted term ‘fake news’”. 
[Online]. 
Available: 
https://www.washingtonpost.com/
lifestyle/style/its-time-to-retire-the-tainted-term-fake-news/
2017/01/06/a5a7516c-d375-11e6-945a-76f69a399dd5_
story.html (retrieved: September, 2020). 
[12] C. Wardle and H. Derakhshan, Information Disorder: Toward 
an Interdisciplinary Framework for Research and Policy 
Making: Council of Europe report DGI(2017)09. [Online]. 
Available: https://rm.coe.int/information-disorder-toward-an-
interdisciplinary-framework-for-researc/168076277c 
(retrieved: August, 2020). 
[13] B. D. Horne and S. Adali, “This Just In: Fake News Packs a 
Lot in Title, Uses Simpler, Repetitive Content in Text Body, 
More Similar to Satire than Real News,” Mar. 2017. [Online]. 
Available: 
http://arxiv.org/pdf/1703.09398v1 
(retrieved: 
October, 2021). 
4
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-927-0
DIGITAL 2021 : Advances on Societal Digital Transformation

[14] D. Klein and J. Wueller, “Fake News: A Legal Perspective,” 
Journal of Internet Law, vol. 20, no. 10, 1, 6-13, 2017. 
[15] L. Bounegru, J. Gray, T. Venturini, and M. Mauri, A Field 
Guide To "Fake News" And Other Information Disorders: 
Zenodo, 2018. 
[16] D. Paskin, “Real or Fake News: Who Knows?,” The Journal 
of Social Media in Society, vol. 7, no. 2, pp. 252–273, 2018. 
[17] D. Fallis, “What Is Disinformation?,” Library Trends, vol. 63, 
no. 3, pp. 401–426, 2015, doi: 10.1353/lib.2015.0014. 
[18] V. Perez-Rosas, B. Kleinberg, A. Lefevre, and R. Mihalcea, 
“Automatic Detection of Fake News,” Proceedings of the 27th 
International Conference on Computational Linguistics, pp. 
3391–3401, 2018. 
[19] J. M. Burkhardt, Combating fake news in the digital age. 
Chicago IL: ALA TechSource, 2017. 
[20] A. Gelfert, “Fake News: A Definition,” Informal Logic, vol. 
38, no. 1, pp. 84–117, 2018, doi: 10.22329/il.v38i1.5068. 
[21] R. Jaster and D. Lanius, Die Wahrheit schafft sich ab: Wie 
Fake News Politik machen. Ditzingen: Reclam, 2019. 
[22] J. Gorbach, “Not Your Grandpa’s Hoax: A Comparative 
History of Fake News,” American Journalism, vol. 35, no. 2, 
pp. 236–249, 2018, doi: 10.1080/08821127.2018.1457915. 
[23] C. H. Cho, M. L. Martens, H. Kim, and M. Rodrigue, 
“Astroturfing Global Warming: It Isn’t Always Greener on the 
Other Side of the Fence,” J Bus Ethics, vol. 104, no. 4, pp. 
571–587, 2011, doi: 10.1007/s10551-011-0950-6. 
[24] F. B. Keller, D. Schoch, S. Stier, and J. Yang, “Political 
Astroturfing on Twitter: How to Coordinate a Disinformation 
Campaign,” Political Communication, vol. 37, no. 2, pp. 256–
280, 2020, doi: 10.1080/10584609.2019.1661888. 
[25] M. Kovic, A. Rauchfleisch, M. Sele, and C. Caspar, “Digital 
astroturfing 
in 
politics: 
Definition, 
typology, 
and 
countermeasures,” SComS, vol. 18, no. 1, 2018, doi: 
10.24434/j.scoms.2018.01.005. 
[26] K. Voss, “Grassrootscampaigning und Chancen durch neue 
Medien,” Aus Politik und Zeitgeschichte, vol. 19, pp. 28–33, 
2010. 
[Online]. 
Available: 
https://www.bpb.de/apuz/32777/grassrootscampaigning-und-
chancen- durch-neue-medien (retrieved: October, 2021). 
[27] T. Zerback, F. Töpfl, and M. Knöpfle, “The disconcerting 
potential of online disinformation: Persuasive effects of 
astroturfing comments and three strategies for inoculation 
against them,” New Media & Society, vol. 23, no. 5, pp. 1080–
1098, 2021, doi: 10.1177/1461444820908530. 
[28] C. W. Lee, “The Roots of Astroturfing,” Contexts, vol. 9, no. 
1, pp. 73–75, 2010, doi: 10.1525/ctx.2010.9.1.73. 
[29] T. Chen et al., “A Hidden Astroturfing Detection Approach 
Base on Emotion Analysis,” in vol. 10412, Knowledge 
Science, Engineering and Management, G. Li, Y. Ge, Z. 
Zhang, Z. Jin, and M. Blumenstein, Eds., Cham: Springer 
International Publishing, 2017, pp. 55–66. 
[30] K. Henrie and C. Gilde, “An Examination of the Impact of 
Astroturfing on Nationalism: A Persuasion Knowledge 
Perspective,” Social Sciences, vol. 8, no. 2, p. 38, 2019, doi: 
10.3390/socsci8020038. 
[31] . B. Barr, “An evidence based approach to sports concussion: 
confronting the availability cascade,” Neuropsychology 
review, vol. 23, no. 4, pp. 271–272, 2013, doi: 
10.1007/s11065-013-9244-3. 
[32] T. Kuran and C. R. Sunstein, “Availability Cascades and Risk 
Regulation,” Stanford Law Review, vol. 51, no. 4, p. 683, 
1999, doi: 10.2307/1229439. 
[33] T. Nogami, “Negative misconceptions about disaster 
behaviour through availability cascades: An examination of 
secondhand information and the moderating effect of trait 
anxiety on disaster myths,” J Community Appl Soc Psychol, 
vol. 30, no. 4, pp. 369–380, 2020, doi: 10.1002/casp.2441. 
[34] J. Bos, M. Frömmel, and M. Lamers, “FDI, terrorism and the 
availability heuristic for U.S. investors before and after 9/11,” 
Maastricht University School of Business and Economics 
Research Memoranda. Maastricht University School of 
Business and Economics, 2013. 
[35] V. S. Folkes, “The Availability Heuristic and Perceived Risk,” 
Journal of Consumer Research, vol. 15, no. 1, p. 13, 1988, doi: 
10.1086/209141. 
[36] M. Geurten, S. Willems, S. Germain, and T. Meulemans, 
“Less is more: The availability heuristic in early childhood,” 
The British journal of developmental psychology, vol. 33, no. 
4, pp. 405–410, 2015, doi: 10.1111/bjdp.12114. 
[37] L. H. Rubel, “The Availability Heuristic: A Redux,” Journal 
of Statistics Education, vol. 15, no. 2, 2007, doi: 
10.1080/10691898.2007.11889467. 
[38] N. Schwarz, H. Bless, F. Strack, G. Klumpp, H. Rittenauer-
Schatka, A. Simons, “Ease of retrieval as information: 
Another look at the availability heuristic,” Journal of 
Personality and Social Psychology, vol. 61, no. 2, pp. 195–
202, 1991, doi: 10.1037/0022-3514.61.2.195. 
[39] A. Tversky and D. Kahneman, “Availability: A heuristic for 
judging frequency and probability,” Cognitive Psychology, 
vol. 5, no. 2, pp. 207–232, 1973, doi: 10.1016/0010-
0285(73)90033-9. 
[40] H. Yin, J. Chen, and E. Michel-Kerjan, “Availability Heuristic 
and Gambler's Fallacy over Time in a Natural Disaster 
Insurance Choice Setting,” SSRN Journal, 2016, doi: 
10.2139/ssrn.2798371. 
[41] J. S. Carroll, “The effect of imagining an event on 
expectations for the event: An interpretation in terms of the 
availability heuristic,” Journal of Experimental Social 
Psychology, vol. 14, no. 1, pp. 88–96, 1978, doi: 
10.1016/0022-1031(78)90062-8. 
[42] A. Cook, J. Arndt, and J. D. Lieberman, “Firing back at the 
backfire effect: the influence of mortality salience and 
nullification beliefs on reactions to inadmissible evidence,” 
Law and human behavior, vol. 28, no. 4, pp. 389–410, 2004, 
doi: 10.1023/B:LAHU.0000039332.21386.f4. 
[43] B. Nyhan and J. Reifler, “When Corrections Fail: The 
Persistence of Political Misperceptions,” Polit Behav, vol. 32, 
no. 2, pp. 303–330, 2010, doi: 10.1007/s11109-010-9112-2. 
[44] U. K. H. Ecker, S. Lewandowsky, and M. Chadwick, “Can 
corrections spread misinformation to new audiences? Testing 
for the elusive familiarity backfire effect,” Cognitive research: 
principles and implications, vol. 5, no. 1, p. 41, 2020, doi: 
10.1186/s41235-020-00241-6. 
[45] B. Swire, A. J. Berinsky, S. Lewandowsky, and U. K. H. 
Ecker, “Processing political misinformation: comprehending 
5
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-927-0
DIGITAL 2021 : Advances on Societal Digital Transformation

the Trump phenomenon,” Royal Society open science, vol. 4, 
no. 3, p. 160802, 2017, doi: 10.1098/rsos.160802. 
[46] T. Wood and E. Porter, “The Elusive Backfire Effect: Mass 
Attitudes’ Steadfast Factual Adherence,” Polit Behav, vol. 41, 
no. 1, pp. 135–163, 2019, doi: 10.1007/s11109-018-9443-y. 
[47] K. Haglin, “The limitations of the backfire effect,” Research 
& Politics, vol. 4, no. 3, 205316801771654, 2017, doi: 
10.1177/2053168017716547. 
[48] C. Peter and T. Koch, “When Debunking Scientific Myths 
Fails (and When It Does Not),” Science Communication, vol. 
38, no. 1, pp. 3–25, 2016, doi: 10.1177/1075547015613523. 
[49] G. J. Trevors, K. R. Muis, R. Pekrun, G. M. Sinatra, and P. H. 
Winne, “Identity and Epistemic Emotions During Knowledge 
Revision: A Potential Account for the Backfire Effect,” 
Discourse Processes, vol. 53, 5-6, pp. 339–370, 2016, doi: 
10.1080/0163853X.2015.1136507. 
[50] B. Swire-Thompson, J. DeGutis, and D. Lazer, “Searching for 
the 
Backfire 
Effect: 
Measurement 
and 
Design 
Considerations,” Journal of applied research in memory and 
cognition, vol. 9, no. 3, pp. 286–299, 2020, doi: 
10.1016/j.jarmac.2020.06.006. 
[51] P. S. Hart and E. C. Nisbet, “Boomerang Effects in Science 
Communication,” Communication Research, vol. 39, no. 6, 
pp. 701–723, 2012, doi: 10.1177/0093650211416646. 
[52] D. J. Ringold, “Boomerang Effects in Response to Public 
Health Interventions: Some Unintended Consequences in the 
Alcoholic Beverage Market,” Journal of Consumer Policy, 
vol. 
25, 
no. 
1, 
pp. 
27–63, 
2002, 
doi: 
10.1023/A:1014588126336. 
[53] R. Nadeau, E. Cloutier, and J.-H. Guay, “New Evidence 
About the Existence of a Bandwagon Effect in the Opinion 
Formation Process,” International Political Science Review, 
vol. 
14, 
no. 
2, 
pp. 
203–213, 
1993, 
doi: 
10.1177/019251219301400204. 
[54] C. Marsh, “Back on the Bandwagon: The Effect of Opinion 
Polls on Public Opinion,” Brit. J. Polit. Sci., vol. 15, no. 1, pp. 
51–74, 1985, doi: 10.1017/S0007123400004063. 
[55] S. S. Sundar, A. Oeldorf-Hirsch, and Q. Xu, “The bandwagon 
effect of collaborative filtering technology,” in Proceeding of 
the twenty-sixth annual CHI conference extended abstracts on 
Human factors in computing systems - CHI '08, Florence, 
Italy, 2008, p. 3453. 
[56] S. Knobloch-Westerwick, N. Sharma, D. L. Hansen, and S. 
Alter, “Impact of Popularity Indications on Readers' Selective 
Exposure to Online News,” Journal of Broadcasting & 
Electronic Media, vol. 49, no. 3, pp. 296–313, 2005, doi: 
10.1207/s15506878jobem4903_3. 
[57] J. N. Blom and K. R. Hansen, “Click bait: Forward-reference 
as lure in online news headlines,” Journal of Pragmatics, vol. 
76, pp. 87–100, 2015, doi: 10.1016/j.pragma.2014.11.010. 
[58] Y. Chen, N. J. Conroy, and V. L. Rubin, “Misleading Online 
Content,” in Proceedings of the 2015 ACM on Workshop on 
Multimodal Deception Detection, Seattle Washington USA, 
2015, pp. 15–19. 
[59] M. Potthast, S. Köpsel, B. Stein, and M. Hagen, “Clickbait 
Detection,” in Lecture Notes in Computer Science, Advances 
in Information Retrieval, N. Ferro et al., Eds., Cham: Springer 
International Publishing, 2016, pp. 810–817. 
[60] K. Munger, M. Luca, J. Nagler, and J. Tucker, “The (Null) 
Effects of Clickbait Headlines on Polarization, Trust, and 
Learning,” Public Opinion Quarterly, vol. 84, no. 1, pp. 49–
73, 2020, doi: 10.1093/poq/nfaa008. 
[61] M. Jones and R. Sugden, “Positive confirmation bias in the 
acquisition of information,” Theory and Decision, vol. 50, no. 
1, pp. 59–99, 2001, doi: 10.1023/A:1005296023424. 
[62] C. S. Meppelink, E. G. Smit, M. L. Fransen, and N. Diviani, 
“"I was Right about Vaccination": Confirmation Bias and 
Health Literacy in Online Health Information Seeking,” 
Journal of health communication, vol. 24, no. 2, pp. 129–140, 
2019, doi: 10.1080/10810730.2019.1583701. 
[63] H. Mercier and D. Sperber, “Why do humans reason? 
Arguments for an argumentative theory,” The Behavioral and 
brain sciences, vol. 34, no. 2, 57-74; discussion 74-111, 2011, 
doi: 10.1017/S0140525X10000968. 
[64] R. S. Nickerson, “Confirmation Bias: A Ubiquitous 
Phenomenon in Many Guises,” Review of General 
Psychology, vol. 2, no. 2, pp. 175–220, 1998, doi: 
10.1037/1089-2680.2.2.175. 
[65] S. Schweiger, A. Oeberst, and U. Cress, “Confirmation bias in 
web-based search: a randomized online study on the effects of 
expert information and social tags on information search and 
evaluation,” Journal of medical Internet research, vol. 16, no. 
3, e94, 2014, doi: 10.2196/jmir.3044. 
[66] T. G. L. A. van der Meer, M. Hameleers, and A. C. Kroon, 
“Crafting Our Own Biased Media Diets: The Effects of 
Confirmation, Source, and Negativity Bias on Selective 
Attendance to Online News,” Mass Communication and 
Society, vol. 23, no. 6, pp. 937–967, 2020, doi: 
10.1080/15205436.2020.1782432. 
[67] D. Lazer et al., Combating Fake News: An Agenda for 
Research 
and 
Action. 
[Online]. 
Available: 
https://
shorensteincenter.org/wp-content/uploads/2017/05/
Combating-Fake-News-Agenda-for-Research-1.pdf 
(retrieved: October, 2021). 
[68] W. 
Edwards, 
“Conservatism 
in 
Human 
Information 
Processing,” in Judgment under uncertainty: heuristics and 
biases, D. Kahneman, P. SLOVIC, and A. Tversky, Eds., 
Cambridge: Cambridge University Press, 2001, pp. 359–369. 
[69] M. Hilbert, “Toward a synthesis of cognitive biases: how 
noisy information processing can bias human decision 
making,” Psychological bulletin, vol. 138, no. 2, pp. 211–237, 
2012, doi: 10.1037/a0025940. 
[70] D. S. Soper, “Informational Social Influence, Belief 
Perseverance, and Conservatism Bias in Web Interface Design 
Evaluations,” IEEE Access, vol. 8, pp. 218765–218776, 2020, 
doi: 10.1109/ACCESS.2020.3042777. 
[71] C. R. Brydges, G. E. Gignac, and U. K. Ecker, “Working 
memory capacity, short-term memory capacity, and the 
continued influence effect: A latent-variable analysis,” 
Intelligence, 
vol. 
69, 
pp. 
117–122, 
2018, 
doi: 
10.1016/j.intell.2018.03.009. 
[72] S. Connor Desai and S. Reimers, “Comparing the use of open 
and closed questions for Web-based measures of the 
continued-influence effect,” Behavior research methods, vol. 
51, no. 3, pp. 1426–1440, 2019, doi: 10.3758/s13428-018-
1066-z. 
6
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-927-0
DIGITAL 2021 : Advances on Societal Digital Transformation

[73] U. K. H. Ecker and L. M. Antonio, “Can you believe it? An 
investigation into the impact of retraction source credibility on 
the continued influence effect,” Memory & cognition, vol. 49, 
no. 4, pp. 631–644, 2021, doi: 10.3758/s13421-020-01129-y. 
[74] U. K. H. Ecker, S. Lewandowsky, O. Fenton, and K. Martin, 
“Do people keep believing because they want to? Preexisting 
attitudes and the continued influence of misinformation,” 
Memory & cognition, vol. 42, no. 2, pp. 292–304, 2014, doi: 
10.3758/s13421-013-0358-x. 
[75] S. Lewandowsky, U. K. H. Ecker, C. M. Seifert, N. Schwarz, 
and J. Cook, “Misinformation and Its Correction: Continued 
Influence and Successful Debiasing,” Psychological science 
in the public interest : a journal of the American Psychological 
Society, vol. 13, no. 3, pp. 106–131, 2012, doi: 
10.1177/1529100612451018. 
[76] L. Ross, M. R. Lepper, and M. Hubbard, “Perseverance in 
self-perception and social perception: Biased attributional 
processes in the debriefing paradigm,” Journal of Personality 
and Social Psychology, vol. 32, no. 5, pp. 880–892, 1975, doi: 
10.1037/0022-3514.32.5.880. 
[77] U. K. H. Ecker, S. Lewandowsky, and D. T. W. Tang, 
“Explicit warnings reduce but do not eliminate the continued 
influence of misinformation,” Memory & cognition, vol. 38, 
no. 8, pp. 1087–1100, 2010, doi: 10.3758/MC.38.8.1087. 
[78] H. M. Johnson and C. M. Seifert, “Sources of the continued 
influence effect: When misinformation in memory affects 
later inferences,” Journal of Experimental Psychology: 
Learning, Memory, and Cognition, vol. 20, no. 6, pp. 1420–
1436, 1994, doi: 10.1037/0278-7393.20.6.1420. 
[79] A. E. O'Rear and G. A. Radvansky, “Failure to accept 
retractions: A contribution to the continued influence effect,” 
Memory & cognition, vol. 48, no. 1, pp. 127–144, 2020, doi: 
10.3758/s13421-019-00967-9. 
[80] M. D. Cobb, B. Nyhan, and J. Reifler, “Beliefs Don't Always 
Persevere: How Political Figures Are Punished When Positive 
Information 
about 
Them 
Is 
Discredited,” 
Political 
Psychology, vol. 34, no. 3, pp. 307–326, 2013, doi: 
10.1111/j.1467-9221.2012.00935.x. 
[81] M. Cinelli, G. de Francisci Morales, A. Galeazzi, W. 
Quattrociocchi, and M. Starnini, “The echo chamber effect on 
social media,” Proceedings of the National Academy of 
Sciences of the United States of America, vol. 118, no. 9, 
2021, doi: 10.1073/pnas.2023301118. 
[82] E. Colleoni, A. Rozza, and A. Arvidsson, “Echo Chamber or 
Public Sphere? Predicting Political Orientation and Measuring 
Political Homophily in Twitter Using Big Data,” J Commun, 
vol. 64, no. 2, pp. 317–332, 2014, doi: 10.1111/jcom.12084. 
[83] D. Goldie, M. Linick, H. Jabbar, and C. Lubienski, “Using 
Bibliometric and Social Media Analyses to Explore the “Echo 
Chamber” Hypothesis,” Educational Policy, vol. 28, no. 2, pp. 
281–305, 2014, doi: 10.1177/0895904813515330. 
[84] P. Barberá, J. T. Jost, J. Nagler, J. A. Tucker, and R. Bonneau, 
“Tweeting From Left to Right: Is Online Political 
Communication 
More 
Than 
an 
Echo 
Chamber?,” 
Psychological science, vol. 26, no. 10, pp. 1531–1542, 2015, 
doi: 10.1177/0956797615594620. 
[85] E. Dubois and G. Blank, “The echo chamber is overstated: the 
moderating effect of political interest and diverse media,” 
Information, Communication & Society, vol. 21, no. 5, pp. 
729–745, 2018, doi: 10.1080/1369118X.2018.1428656. 
[86] R. 
K. 
Garrett, 
“The 
“Echo 
Chamber” 
Distraction: 
Disinformation Campaigns are the Problem, Not Audience 
Fragmentation,” Journal of applied research in memory and 
cognition, vol. 6, no. 4, pp. 370–376, 2017, doi: 
10.1016/J.JARMAC.2017.09.011. 
[87] F. J. Zuiderveen Borgesius, D. Trilling, J. Möller, B. Bodó, C. 
H. de Vreese, and N. Helberger, “Should we worry about filter 
bubbles?,” Internet Policy Review, vol. 5, no. 1, 2016, doi: 
10.14763/2016.1.401. 
[88] E. A. Kensinger and S. Corkin, “Memory enhancement for 
emotional words: are emotional words more vividly 
remembered than neutral words?,” Memory & cognition, vol. 
31, no. 8, pp. 1169–1180, 2003, doi: 10.3758/BF03195800. 
[89] T. Sommer, J. Gläscher, S. Moritz, and C. Büchel, “Emotional 
enhancement effect of memory: removing the influence of 
cognitive factors,” Learning & memory (Cold Spring Harbor, 
N.Y.), 
vol. 
15, 
no. 
8, 
pp. 
569–573, 
2008, 
doi: 
10.1101/lm.995108. 
[90] D. Talmi, “Enhanced Emotional Memory,” Curr Dir Psychol 
Sci, 
vol. 
22, 
no. 
6, 
pp. 
430–436, 
2013, 
doi: 
10.1177/0963721413498893. 
[91] D. Talmi and L. M. McGarry, “Accounting for immediate 
emotional memory enhancement,” Journal of Memory and 
Language, vol. 66, no. 1, pp. 93–108, 2012, doi: 
10.1016/j.jml.2011.07.009. 
[92] I. van Damme and K. Smets, “The power of emotion versus 
the power of suggestion: memory for emotional events in the 
misinformation paradigm,” Emotion (Washington, D.C.), vol. 
14, no. 2, pp. 310–320, 2014, doi: 10.1037/a0034629. 
[93] D. DiFranzo and K. Gloria-Garcia, “Filter bubbles and fake 
news,” XRDS, vol. 23, no. 3, pp. 32–35, 2017, doi: 
10.1145/3055153. 
[94] M. Emmer and C. Strippel, “Stichprobenziehung für Online-
Inhaltsanalysen: Suchmaschinen und Filter Bubbles,” 2015. 
[95] S. Flaxman, S. Goel, and J. M. Rao, “Filter Bubbles, Echo 
Chambers, and Online News Consumption,” Public Opinion 
Quarterly, 
vol. 
80, 
S1, 
pp. 
298–320, 
2016, 
doi: 
10.1093/poq/nfw006. 
[96] T. T. Nguyen, P.-M. Hui, F. M. Harper, L. Terveen, and J. A. 
Konstan, “Exploring the filter bubble,” in Proceedings of the 
23rd international conference on World wide web - WWW 
'14, Seoul, Korea, 2014, pp. 677–686. 
[97] E. Pariser, Filter Bubble: Wie wir im Internet entmündigt 
werden. München: Hanser, Carl, 2012. 
[98] A. Bruns, Are filter bubbles real? Cambridge, UK, Medford, 
USA: Polity, 2019. 
[99] M. Haim, A. Graefe, and H.-B. Brosius, “Burst of the Filter 
Bubble?,” Digital Journalism, vol. 6, no. 3, pp. 330–343, 
2018, doi: 10.1080/21670811.2017.1338145. 
[100] S. Almashat, B. Ayotte, B. Edelstein, and J. Margrett, 
“Framing effect debiasing in medical decision making,” 
Patient education and counseling, vol. 71, no. 1, pp. 102–107, 
2008, doi: 10.1016/j.pec.2007.11.004. 
[101] H. Bless, T. Betsch, and A. Franzen, “Framing the framing 
effect: the impact of context cues on solutions to the ‘Asian 
disease’ problem,” Eur. J. Soc. Psychol., vol. 28, no. 2, pp. 
7
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-927-0
DIGITAL 2021 : Advances on Societal Digital Transformation

287–291, 
1998, 
doi: 
10.1002/(SICI)1099-
0992(199803/04)28:2%3C287::AID-EJSP861%3E3.0.CO;2-
U. 
[102] V. Stocké, Framing und Rationalität: Die Bedeutung der 
Informationsdarstellung für das Entscheidungsverhalten. 
Zugl.: 
Mannheim, 
Univ., 
Dissertationsschrift, 
2000. 
München: De Gruyter Oldenbourg, 2002. [Online]. Available: 
http://www.degruyter.com/search?f_0=isbnissn&q_0=
9783486833263&searchTitles=true 
(retrieved: 
October, 
2021). 
[103] C. J. Gosling and S. Moutier, “Is the framing effect a framing 
affect?,” Quarterly journal of experimental psychology 
(2006), vol. 72, no. 6, pp. 1412–1421, 2019, doi: 
10.1177/1747021818796016. 
[104] S. Lecheler, A. R. T. Schuck, and C. H. de Vreese, “Dealing 
with feelings: Positive and negative discrete emotions as 
mediators of news framing effects,” Communications - The 
European Journal of Communication Research, vol. 38, no. 2, 
2013, doi: 10.1515/commun-2013-0011. 
[105] F.F. Cheng and C.S. Wu, “Debiasing the framing effect: The 
effect of warning and involvement,” Decision Support 
Systems, vol. 49, no. 3, pp. 328–334, 2010, doi: 
10.1016/j.dss.2010.04.002. 
[106] R. L. Nabi et al., “Can Emotions Capture the Elusive Gain-
Loss Framing Effect? A Meta-Analysis,” Communication 
Research, vol. 47, no. 8, pp. 1107–1130, 2020, doi: 
10.1177/0093650219861256. 
[107] B. Sparrow, J. Liu, and D. M. Wegner, “Google effects on 
memory: cognitive consequences of having information at our 
fingertips,” Science (New York, N.Y.), vol. 333, no. 6043, pp. 
776–778, 2011, doi: 10.1126/science.1207745. 
[108] D. M. Wegner and A. F. Ward, “How Google is changing your 
brain,” Scientific American, vol. 309, no. 6, pp. 58–61, 2013, 
doi: 10.1038/scientificamerican1213-58. 
[109] L. M. Arpan and A. A. Raney, “An Experimental 
Investigation of News Source and the Hostile Media Effect,” 
Journalism & Mass Communication Quarterly, vol. 80, no. 2, 
pp. 265–281, 2003, doi: 10.1177/107769900308000203. 
[110] L. Feldman, “Partisan Differences in Opinionated News 
Perceptions: A Test of the Hostile Media Effect,” Polit Behav, 
vol. 33, no. 3, pp. 407–432, 2011, doi: 10.1007/s11109-010-
9139-4. 
[111] A. C. Gunther and J. L. Liebhart, “Broad Reach or Biased 
Source? Decomposing the Hostile Media Effect,” Journal of 
Communication, vol. 56, no. 3, pp. 449–466, 2006, doi: 
10.1111/j.1460-2466.2006.00295.x. 
[112] A. C. Gunther and K. Schmitt, “Mapping Boundaries of the 
Hostile Media Effect,” Journal of Communication, vol. 54, no. 
1, 
pp. 
55–70, 
2004, 
doi: 
10.1111/j.1460-
2466.2004.tb02613.x. 
[113] S. A. Reid, “A Self-Categorization Explanation for the Hostile 
Media Effect,” vol. 62, no. 3, pp. 381–399, 2012, doi: 
10.1111/j.1460-2466.2012.01647.x. 
[114] R. P. Vallone, L. Ross, and M. R. Lepper, “The hostile media 
phenomenon: Biased perception and perceptions of media bias 
in coverage of the Beirut massacre,” Journal of Personality 
and Social Psychology, vol. 49, no. 3, pp. 577–585, 1985, doi: 
10.1037/0022-3514.49.3.577. 
[115] N. M. Brashier, E. D. Eliseev, and E. J. Marsh, “An initial 
accuracy focus prevents illusory truth,” Cognition, vol. 194, 
p. 104054, 2020, doi: 10.1016/j.cognition.2019.104054. 
[116] A. Hassan and S. J. Barber, “The effects of repetition 
frequency on the illusory truth effect,” Cognitive research: 
principles and implications, vol. 6, no. 1, p. 38, 2021, doi: 
10.1186/s41235-021-00301-5. 
[117] J. de Keersmaecker et al., “Investigating the Robustness of the 
Illusory Truth Effect Across Individual Differences in 
Cognitive Ability, Need for Cognitive Closure, and Cognitive 
Style,” Personality & social psychology bulletin, vol. 46, no. 
2, pp. 204–215, 2020, doi: 10.1177/0146167219853844. 
[118] E. J. Newman, M. C. Jalbert, N. Schwarz, and D. P. Ly, 
“Truthiness, the illusory truth effect, and the role of need for 
cognition,” Consciousness and cognition, vol. 78, p. 102866, 
2020, doi: 10.1016/j.concog.2019.102866. 
[119] L. K. Fazio, D. G. Rand, and G. Pennycook, “Repetition 
increases perceived truth equally for plausible and implausible 
statements,” Psychonomic bulletin & review, vol. 26, no. 5, 
pp. 1705–1710, 2019, doi: 10.3758/s13423-019-01651-4. 
[120] G. Pennycook and D. G. Rand, “Who falls for fake news? The 
roles of bullshit receptivity, overclaiming, familiarity, and 
analytic thinking,” Journal of personality, vol. 88, no. 2, pp. 
185–200, 2020, doi: 10.1111/jopy.12476. 
[121] L. E. Boehm, “The Validity Effect: A Search for Mediating 
Variables,” Pers Soc Psychol Bull, vol. 20, no. 3, pp. 285–293, 
1994, doi: 10.1177/0146167294203006. 
[122] G. Pennycook, A. Bear, E. T. Collins, and D. G. Rand, “The 
Implied Truth Effect: Attaching Warnings to a Subset of Fake 
News Headlines Increases Perceived Accuracy of Headlines 
Without Warnings,” Management Science, 2020, doi: 
10.1287/mnsc.2019.3478. 
[123] K. Clayton et al., “Real Solutions for Fake News? Measuring 
the Effectiveness of General Warnings and Fact-Check Tags 
in Reducing Belief in False Stories on Social Media,” Polit 
Behav, vol. 42, no. 4, pp. 1073–1095, 2020, doi: 
10.1007/s11109-019-09533-0. 
[124] L. R. Anderson and C. A. Holt, “Information cascades in the 
laboratory,” The American Economic Review, vol. 87, no. 5, 
pp. 847–862, 1997. 
[125] S. Bikhchandani, D. Hirshleifer, and I. Welch, “A Theory of 
Fads, Fashion, Custom, and Cultural Change as Informational 
Cascades,” Journal of Political Economy, vol. 100, no. 5, pp. 
992–1026, 1992, doi: 10.1086/261849. 
[126] M. Jalili and M. Perc, “Information cascades in complex 
networks,” Journal of Complex Networks, 2017, doi: 
10.1093/comnet/cnx019. 
[127] F. Wang, J. Wei, and D. Zhao, “A Quantifiable Risky 
Decision Model: Incorporating Individual Memory into 
Informational Cascade,” Syst. Res., vol. 31, no. 4, pp. 537–
553, 2014, doi: 10.1002/sres.2294. 
[128] J. K. Goeree, T. R. Palfrey, B. W. Rogers, and R. D. 
McKelvey, “Self-correcting information cascades,” The 
review of economic studies, vol. 74, no. 3, pp. 733–762, 2007. 
[129] Akademische Gesellschaft, How powerful are Social Bots? 
[Online]. 
Available: 
https://www.akademische-
gesellschaft.com/fileadmin/webcontent/Publikationen/
Communication_Snapshots/AGUK_
8
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-927-0
DIGITAL 2021 : Advances on Societal Digital Transformation

CommunicationSnapshot_SocialBots_June2018.pdf 
(retrieved: October, 2021). 
[130] N. Abokhodair, D. Yoo, and D. W. McDonald, “Dissecting a 
Social Botnet,” in Proceedings of the 18th ACM Conference 
on Computer Supported Cooperative Work & Social 
Computing, Vancouver BC Canada, 2015, pp. 839–851. 
[131] F. Brachten, S. Stieglitz, L. Hofeditz, K. Kloppenborg, and A. 
Reimann, “Strategies and Influence of Social Bots in a 2017 
German state election - A case study on Twitter,” Oct. 2017. 
[Online]. 
Available: 
http://arxiv.org/pdf/1710.07562v1 
(retrieved: October, 2021). 
[132] M. S. Ayers and L. M. Reder, “A theoretical review of the 
misinformation effect: Predictions from an activation-based 
memory model,” Psychon Bull Rev, vol. 5, no. 1, pp. 1–21, 
1998, doi: 10.3758/BF03209454. 
[133] H. Blank and C. Launay, “How to protect eyewitness memory 
against the misinformation effect: A meta-analysis of post-
warning studies,” Journal of applied research in memory and 
cognition, 
vol. 
3, 
no. 
2, 
pp. 
77–88, 
2014, 
doi: 
10.1016/j.jarmac.2014.03.005. 
[134] K. A. Braun and E. F. Loftus, “Advertising's misinformation 
effect,” Appl. Cognit. Psychol., vol. 12, no. 6, pp. 569–591, 
1998, 
doi: 
10.1002/(SICI)1099-
0720(1998120)12:6%3C569::AID-ACP539%3E3.0.CO;2-E. 
[135] E. Cowley and E. Janus, “Not Necessarily Better, but 
Certainly 
Different: 
A 
Limit 
to 
the 
Advertising 
Misinformation Effect on Memory,” J CONSUM RES, vol. 
31, no. 1, pp. 229–235, 2004, doi: 10.1086/383438.. 
[136] K. Lee, “Age, neuropsychological, and social cognitive 
measures as predictors of individual differences in 
susceptibility to the misinformation effect,” Appl. Cognit. 
Psychol., vol. 18, no. 8, pp. 997–1019, 2004, doi: 
10.1002/acp.1075. 
[137] M. Szpitalak and R. Polczyk, “Warning against warnings: 
Alerted subjects may perform worse. Misinformation, 
involvement and warning as determinants of witness 
testimony,” Polish Psychological Bulletin, vol. 41, no. 3, pp. 
105–112, 2010, doi: 10.2478/v10059-010-0014-2. 
[138] W. E. Crozier and D. Strange, “Correcting the misinformation 
effect,” Appl. Cognit. Psychol., 2018, doi: 10.1002/acp.3499. 
[139] R. F. Baumeister, E. Bratslavsky, C. Finkenauer, and K. D. 
Vohs, “Bad is Stronger than Good,” Review of General 
Psychology, vol. 5, no. 4, pp. 323–370, 2001, doi: 
10.1037/1089-2680.5.4.323. 
[140] Y.-X. Huang and Y.-J. Luo, “Temporal course of emotional 
negativity bias: an ERP study,” Neuroscience letters, vol. 398, 
1-2, pp. 91–96, 2006, doi: 10.1016/j.neulet.2005.12.074. 
[141] T. Ito and J. Cacioppo, “Variations on a human universal: 
Individual differences in positivity offset and negativity bias,” 
Cognition & Emotion, vol. 19, no. 1, pp. 1–26, 2005, doi: 
10.1080/02699930441000120. 
[142] P. Rozin and E. B. Royzman, “Negativity Bias, Negativity 
Dominance, and Contagion,” Pers Soc Psychol Rev, vol. 5, 
no. 
4, 
pp. 
296–320, 
2001, 
doi: 
10.1207/S15327957PSPR0504_2. 
[143] S. Soroka and S. McAdams, “News, Politics, and Negativity,” 
Political Communication, vol. 32, no. 1, pp. 1–22, 2015, doi: 
10.1080/10584609.2014.881942. 
[144] B. Wojciszke, H. Brycz, and P. Borkenau, “Effects of 
information content and evaluative extremity on positivity and 
negativity biases,” Journal of Personality and Social 
Psychology, vol. 64, no. 3, pp. 327–335, 1993. 
[145] H. A. Demaree, B. V. Shenal, D. E. Everhart, and J. L. 
Robinson, “Primacy and recency effects found using affective 
word lists,” Cognitive and Behavioral Neurology: Official 
Journal of the Society for Behavioral and Cognitive 
Neurology, vol. 17, no. 2, pp. 102–108, 2004, doi: 
10.1097/01.wnn.0000117861.44205.31. 
[146] J. A. Krosnick and D. F. Alwin, “An Evaluation of a Cognitive 
Theory of Response-Order Effects in Survey Measurement,” 
Public Opinion Quarterly, vol. 51, no. 2, p. 201, 1987, doi: 
10.1086/269029. 
[147] A. B. Morrison, A. R. A. Conway, and J. M. Chein, “Primacy 
and recency effects as indices of the focus of attention,” 
Frontiers in human neuroscience, vol. 8, p. 6, 2014, doi: 
10.3389/fnhum.2014.00006. 
[148] J. Murphy, C. Hofacker, and R. Mizerski, “Primacy and 
Recency Effects on Clicking Behavior,” J Comp Mediated 
Comm, vol. 11, no. 2, pp. 522–535, 2006, doi: 10.1111/j.1083-
6101.2006.00025.x. 
[149] C. Li, “Primacy effect or recency effect? A long-term memory 
test of Super Bowl commercials,” Journal of Consumer 
Behaviour, n/a-n/a, 2009, doi: 10.1002/cb.291. 
[150] P. H. Marshall and P. R. Werder, “The effects of the 
elimination of rehearsal on primacy and recency,” Journal of 
Verbal Learning and Verbal Behavior, vol. 11, no. 5, pp. 649–
653, 1972, doi: 10.1016/S0022-5371(72)80049-5. 
[151] R. C. Ellickson, “The market for social norms,” American 
Law and Economics Review: The Journal of the American 
Law and Economics Association, vol. 3, no. 1, pp. 1–49, 2001. 
[152] C. R. Sunstein and R. Hastie, “Making dumb groups smarter,” 
Harvard business review : HBR, vol. 92, no. 12, pp. 90–98, 
2014. [Online]. Available: https://hbr.org/2014/12/making-
dumb-groups-smarter (retrieved: October, 2021). 
[153] P. Lemieux, “Following the Herd: Why do some ideas 
suddenly become popular, and then die out just as quickly?,” 
Regulation : the Cato review of business and government, vol. 
26, no. 4, pp. 16–21, 2003. 
[154] R. Blom, “Believing false political headlines and discrediting 
truthful political headlines: The interaction between news 
source trust and news content expectancy,” Journalism, vol. 
22, 
no. 
3, 
pp. 
821–837, 
2021, 
doi: 
10.1177/1464884918765316. 
[155] T. Buchanan and V. Benson, “Spreading Disinformation on 
Facebook: Do Trust in Message Source, Risk Propensity, or 
Personality Affect the Organic Reach of “Fake News”?,” 
Social Media + Society, vol. 5, no. 4, 205630511988865, 
2019, doi: 10.1177/2056305119888654. 
[156] A. Kim, P. L. Moravec, and A. R. Dennis, “Combating Fake 
News on Social Media with Source Ratings: The Effects of 
User and Expert Reputation Ratings,” Journal of Management 
Information Systems, vol. 36, no. 3, pp. 931–968, 2019, doi: 
10.1080/07421222.2019.1628921. 
[157] M. J. Metzger, A. J. Flanagin, and R. B. Medders, “Social and 
Heuristic Approaches to Credibility Evaluation Online,” J 
9
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-927-0
DIGITAL 2021 : Advances on Societal Digital Transformation

Commun, vol. 60, no. 3, pp. 413–439, 2010, doi: 
10.1111/j.1460-2466.2010.01488.x. 
[158] S. Vosoughi, D. Roy, and S. Aral, “The spread of true and 
false news online,” Science (New York, N.Y.), vol. 359, no. 
6380, pp. 1146–1151, 2018, doi: 10.1126/science.aap9559. 
[159] A. Zubiaga, M. Liakata, R. Procter, G. Wong Sak Hoi, and P. 
Tolmie, “Analysing How People Orient to and Spread 
Rumours in Social Media by Looking at Conversational 
Threads,” PloS one, vol. 11, no. 3, e0150989, 2016, doi: 
10.1371/journal.pone.0150989. 
[160] G. Echterhoff, S. Groll, and W. Hirst, “Tainted Truth: 
Overcorrection for Misinformation Influence on Eyewitness 
Memory,” Social Cognition, vol. 25, no. 3, pp. 367–409, 2007, 
doi: 10.1521/soco.2007.25.3.367. 
[161] M. Freeze et al., “Fake Claims of Fake News: Political 
Misinformation, Warnings, and the Tainted Truth Effect,” 
Polit Behav, 2020, doi: 10.1007/s11109-020-09597-3. 
[162] W. P. Davison, “The Third-Person Effect in Communication,” 
Public Opinion Quarterly, vol. 47, no. 1, p. 1, 1983, doi: 
10.1086/268763. 
[163] A. C. Gunther and P. Mundy, “Biased Optimism and the 
Third-Person Effect,” Journalism Quarterly, vol. 70, no. 1, pp. 
58–67, 1993, doi: 10.1177/107769909307000107. 
[164] A. Lev-On, “The third-person effect on Facebook: The 
significance of perceived proficiency,” Telematics and 
Informatics, vol. 34, no. 4, pp. 252–260, 2017, doi: 
10.1016/j.tele.2016.07.002. 
[165] R. M. Perloff, “Third-person effect research 1983–1992: A 
Review and Synthesis,” International Journal of Public 
Opinion Research, vol. 5, no. 2, pp. 167–184, 1993, doi: 
10.1093/ijpor/5.2.167. 
[166] H. Rojas, D. V. Shah, and R. J. Faber, “For the good of others: 
Censorship and the third-person effect,” International Journal 
of Public Opinion Research, vol. 8, no. 2, pp. 163–186, 1996, 
doi: 10.1093/ijpor/8.2.163. 
[167] Y. Sun, L. Shen, and Z. Pan, “On the Behavioral Component 
of the Third-Person Effect,” Communication Research, vol. 
35, 
no. 
2, 
pp. 
257–278, 
2008, 
doi: 
10.1177/0093650207313167. 
[168] G. Pennycook and D. G. Rand, “Lazy, not biased: 
Susceptibility to partisan fake news is better explained by lack 
of reasoning than by motivated reasoning,” Cognition, vol. 
188, pp. 39–50, 2019, doi: 10.1016/j.cognition.2018.06.011. 
[169] B. E. Weeks, “Emotions, Partisanship, and Misperceptions: 
How Anger and Anxiety Moderate the Effect of Partisan Bias 
on Susceptibility to Political Misinformation,” J Commun, 
vol. 65, no. 4, pp. 699–719, 2015, doi: 10.1111/jcom.12164. 
[170] A. Guess, J. Nagler, and J. Tucker, “Less than you think: 
Prevalence and predictors of fake news dissemination on 
Facebook,” Science advances, vol. 5, no. 1, eaau4586, 2019, 
doi: 10.1126/sciadv.aau4586. 
[171] J. Yang, S. Counts, M. R. Morris, and A. Hoff, “Microblog 
credibility perceptions,” in Proceedings of the 2013 
conference on Computer supported cooperative work - CSCW 
'13, San Antonio, Texas, USA, 2013, p. 575. 
[172] S. M. Shariff, X. Zhang, and M. Sanderson, “On the credibility 
perception of news on Twitter: Readers, topics and features,” 
Computers in Human Behavior, vol. 75, pp. 785–796, 2017, 
doi: 10.1016/j.chb.2017.06.026. 
[173] M. Opuszko, S. Gehrke, and S. Niemz,. "Peer Influence and 
Centrality in Online Social Networks." In 2019 International 
Conference on Computational Science and Computational 
Intelligence (CSCI), pp. 1377-1382. IEEE, 2019, doi: 
10.1109/CSCI49370.2019.0025 
10
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-927-0
DIGITAL 2021 : Advances on Societal Digital Transformation

