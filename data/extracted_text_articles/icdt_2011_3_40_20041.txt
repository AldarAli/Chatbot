Linguistic Text Compression
Ondˇrej Kaz´ık
Charles University
Faculty of Mathematics and Physics
Prague, Czech Republic
Email: kazik.ondrej@gmail.com
Jan L´ansk´y
The University of Finance and Administration
Department of Computer Science
Prague, Czech Republic
Email: zizelevak@gmail.com
Abstract—Compression of texts written in natural language
can exploit information about its linguistic structure. We show
that separation of coding of part-of-speech tags of a sentence
(so called sentence types) from the text and coding this sentence
types separately can improve resulting compression ratio. For
this purpose the tagging method NNTagger based on neural
networks is designed. This article is focused on a speciﬁcation
and formalization of a compression model of texts written in
Czech. Language with such a complicated morphology contains
a great amount of implicit grammatical information of a
sentence and it is thus suitable for this approach. We propose
methods of constructing of initial dictionaries and test their
inﬂuence on resulting compression ratio.
Keywords-text compression; part-of-speech tagging; neural
networks
I. INTRODUCTION
It was pointed out in several works concerning text
compression (e.g., [22]) that knowledge of the structure
of a coded message can be exploited for more successful
compression of this message. It turned out that a set of basic
elements (words, syllables, characters) as well as a structure
of documents is strongly dependent on the used language.
This motivates us to exploit implicit linguistic information
contained in an input text.
We use during the compression knowledge about struc-
tures larger than words, so called sentence types – a
sequence of words’ tags in a sentence. We assume that
the separate coding of a sentence type and words of a
sentence with knowledge of their tag is a suitable supplement
of common compression methods based on words as the
elements.
A tagging method has to precede the compression itself.
Since compression is not sensitive to small errors, we will
seek for approximative method of assigning tags to the
words in a sentence.
Our goal it then to propose such model of sentences which
comprehends the sentence type. This model will serve for
an effective coding of document’s sentences.
In Section II, we provide a brief overview of tagging and
compression methods, other than the approaches described
in this article. In Section III we introduce the NNTagger,
an approximative part of speech tagger based on neural
networks. In Section IV we describe the model of a text,
sentence and other subelements and coding and decoding
algorithms which are based on them. In Section V we
compare results of our method with other compression
techniques. Finally, in Section VI we summarize our ﬁndings
and discuss possible future work.
II. RELATED WORK
Many nature language processing applications, e.g., syn-
tactic analysis, require as a preprocessing phase assigning
deﬁnite morphological information to each word in an
input text. This procedure is tagging and consists of a
morphological analysis, i.e., assigning of all possible tags
to a word alone, and disambiguation; i.e., resolving tag
ambiguity by the context of a word. Both these phases are
chosen with regard to the used language and tag set, which
is the range of the tagging method. The Czech language in
comparison with the English language is a highly inﬂecting
language, which needs a more sophisticated processing of
sufﬁxes and in many applications it requires a tag set,
which contains more detailed morphological information.
Also it is highly ambiguous. In the English language,
the relative small tag sets are used (e.g., Brown, Penn,
CLAWS, London-Lund [23]). By contrast, the Czech tag
sets (as the Prague Dependency Treebank positional tag
set [11] or the set used in morphological analyzer Ajka
[31]) represent broad morphological information containing
several thousands of possible tags. For the Czech language
there are correspondent morphological analyzers, e.g., the
tools Free Morphology [9] and Ajka [31]. There is a large
number of disambiguation methods suitable for different
languages and tagging sets, for instance visible [23] or
hidden Markov models [6], taggers based on a decision
tree TreeTagger [30], transformations [3], maximum entropy
model [26], exponential tagger [10], averaged perceptron-
based model ( [5] or MOR ˇCE [40] for the Czech language)
and artiﬁcial neural networks based taggers (NetTagger [29])
which is close to our approach, especially the context net of
NNTagger described later.
The text ﬁle compression is usually employed to save
hard drive space or bandwidth during their transfer. The
compression of texts can be classiﬁed according to elements
of coding, e.g., letters, words or syllables. The methods
64
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

based on words require dividing the input document into
a stream of words and non-words [24], [25]. Overview of
word methods is provided by [36]. There are word-based
variants of all main groups of compression algorithms: e.g.,
word-based Huffman encoding [13], [28], word-based LZW
[7], word-based Burrows-Wheeler Transformation [7], [15],
and word-based PPM [2], [24]. Syllables as source units
for compression can be successfully used in languages,
where words are naturally divided into syllables (e.g., Czech,
Russian, and German). This approach was ﬁrstly used in
[39]. Knowledge of regularities of the language, in which
an encoded message is written, can be exploited to im-
prove text compression. For English texts, there are several
methods of such improvement: e.g., speciﬁc order of letters
in the alphabet for lexicographic sorting [1], [4], replacing
common clusters (n-grams) of letters with one symbol [38],
static initialization of PPM method [35]. In order to manage
multilingual text ﬁles, the methods need to recognize the
language of a message and its encoding, like in modiﬁcations
of Word Replacing Transformation, TWRT [36]. Separation
of the parts of speech of each word is used as additional
information for compression of English texts and compared
with character and word based compression programs also
in [38]. Compression of small ﬁles is a separate subject of
research. One of such approaches is focused on compression
of short text ﬁles for mobile phones. The low-complexity
static partial matching model (PPM) is described in [27]. A
sequence of pruned sufﬁx trees is used in statistic model in
[17].
Lansky and Zemlicka [22] propose theoretical basis
of syllable-based compression, LZWL and HuffSyll al-
gorithms. Right conﬁguration of the characteristic words
and syllables dictionaries used by LZWL and HuffSyll
algorithms for initialization is emphasized in [21]. Various
approaches to small and medium text ﬁle compression
by means of Burrows-Wheeler Transformation (BWT) are
compared in [19]. Suitable inﬂation of the characteristic
syllables and words sets, which are used to initialize the
HuffSyll and LZWL methods is examined in [18]. XBW
Project [32], [33] studies the compression of large, non-well
formed XML ﬁles. Its basis is BWT and it implements many
other coding algorithms, which we utilize in this paper [20].
III. NNTAGGER (APPROXIMATIVE PART OF SPEECH
TAGGING)
Linguistics as a science investigates a language from
many viewpoints, e.g., syntax, morphology, lexicology etc.
The language we are interested in, the Czech language can
be described from these perspectives. The Czech language
has a free word order, a rich inﬂective morphology which
lays grammatical morphemes at the end of the word. Also
the sets of words belonging to different parts of speech
have different properties. For example, the inﬂective and
autosemantic parts of speech (e.g., nouns, verbs etc.) tend to
have longer words and greater variation both in a document
and among different documents than the non-inﬂective and
synsemantic ones (prepositions, conjunctions etc.). These
characteristics can be exploited in our approximative tagging
method NNTagger.
Every tagging algorithm has to solve three main prob-
lems: tokenization, i.e., division of the input text into basic
elements (words, sentences), morphological analysis, i.e., as-
signing of possible tags to a word alone, and disambiguation
which determines the tag according to a particular use of the
word in a sentence.
A. Pre-processing
Two questions must be solved before the tagging. First,
we focus on a deﬁnition of words, which will be the basic
element of processing. Second, we should consider a width
of the sequence of words, which will be available for the
tagging procedure, i.e., how to separate single sentences
from an input text.
1) Words: We are extending the deﬁnition of words from
[22], where authors, with regard to separability, introduced
these classes of words: words consisting of small letters
(small), those consisting of capital letters (capital), with
ﬁrst letter capital (mixed), numeric and other words. We
have taken into account characteristics of training data from
the PDT corpus [12]. There not only numeric and alpha
words but also punctuation marks are considered as the
basic elements on the word layer. This is consistent with
the Manning and Sch¨utze [23], where the information about
macrostructure of sentence contained in the punctuation is
emphasized.
2) Sentences: Now we need to decide, how long blocks
of words will be passed on to a tagging algorithm. Annotated
available training data (the PDT corpus [12]) contain texts
tagged divided into whole sentences. We gather an inspira-
tion from description of the algorithm in [23].
We are passing the sequence of non-special words from
left to right and inspecting them as candidates on a sentence
boundary. We use this heuristics:
• If the inspected word is “;” or “...”, it is marked as the
last word in the sentence.
• In the case of “?” or “!”, it is marked, if in the same
time is not followed by another “?” or “!”.
• The inspected word is “.”. It is marked only, if the
following word is capitalized but the preceding word
does not consist of one capital letter (abbreviation of a
ﬁrst name) or this word is not in a list of abbreviations
which usually is not placed at the end of a sentence
(“mgr”, “napˇr”, “tzn” etc.).
• If after the inspected word is end of line and next word
is not small or if after this word is more than one end
of line, we put there the sentence boundary. This is the
case of e.g., a heading.
65
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

• If immediately after the supposed sentence boundary is
a quotation mark, shift the boundary after it.
In the next section we will show that it is unsuitable to pass
these large segments to the compression algorithm directly
and thus they will be segmented to shorter sequences.
B. Tag Set
The decision about a tag set was limited by available
data. In our case it is the annotated corpus PDT with a
positional morphological tag system, where each position
corresponds to some morphological category. So the tag set
should be equivalent to a subset of these positions. The
decision was based on a presupposition that the tag should
carry an information about characteristics of the given word
form, its statistical features and its relation to other words
in a sentence. In the same time the sets of word forms
belonging to these tags should have minimal overlapping,
which disqualify some detailed categories (as the cases).
Hence we can focus on the position Part of Speech (POS, 12
values) and the position Detailed Part of Speech (SUBPOS,
75 values), from which we can also derive the part of
speech category. Due to various criteria for subsumption
under a part of speech, words from the same set can play
various roles in a sentence (e.g., Numerals or Pronouns).
The detailed part of speech category would partially reduce
this ambiguity. But we decided only for the ﬁrst category
(Part of Speech) because of the following reasons:
• The larger is the tag set, the more difﬁcult is to
determine the correct value of the tag. This is due to
ambiguity of the division criteria and to shortcomings
of the chosen tagging method.
• With an amount of information extracted from the text
complexity of the model of language and number of its
parameters increase.
• The larger alphabet of symbols, i.e., tag set, corre-
sponds to the larger set of words, which are compound
of this alphabet, i.e., sentence types, and also the higher
probability of occurrence of a new sentence type. This
would complicate the compression.
The selected set of part of speech tags contains thus 11
values: nouns (also marked by N), adjectives (A), pronouns
(P), numerals (C), verbs (V), adverbs (D), prepositions (R),
conjunctions (J), particles (T), interjections (I) and punc-
tuations (Z). This category carries according to the entropy
estimation about 25 % of the information of a word form.1
C. Morphological Phase
The input of this phase is a non-special word w with the
length m:
w = α1α2 . . . αm
1The entropy was measured on the PDT corpus. Shannons entropy of
the part of speech is 2.9 bits and of the whole word forms 11.9 bits.
The output is then the vector ym (w) with the length equal
to the size of the tag set n = |T |. It holds for elements of
this vector:
ym
t (w) ∈ [0, 1]
This value can be interpreted as the likelihood that the
word w has the tag t ∈ T on this level.
We deﬁne the auxiliary vector ˆy (t) which can be consid-
ered as a certainty that the word has the tag t.
ˆyt (t′)
=
1
if t = t′
=
0
otherwise
First, it is possible to tag the non-alpha words. If w is a
word containing a symbol from ΣZ, then w is a punctuation:
ym (w) = ˆy (Z). The procedure is then terminated.
Similarly, if the word w contains a symbol from ΣC, it
is a number and thus:
ym (w) = ˆy (C)
Next only alpha words are processed, so the small, the
capital and the mixed are distinguished. Lets the function
ψ : ΣM ∪ ΣV
→ {1, 2, . . . , 41} assigns to every letter
a natural number which corresponds to some basic Czech
small letter. 2 A size of letters is irrelevant for this mapping,
thus:
∀α ∈ ΣV : ψ (α) = ψ (µ (α))
For the letters which are not included to the basic Czech
alphabet (which have small probability in Czech texts)
it assigns some value from this alphabet. For example:
ψ (′¨a′) = ψ (′a′)
We decided to use neural nets for the next processing. This
decision was motivated by the amount of information, which
contains regular structures of word forms in such inﬂecting
languages as is the Czech language. Another advantage is
the absence of any dictionary. On the other side, we expect a
lower accuracy than by other methods of the morphological
analysis. This should not have a major inﬂuence on the
compression.
Due to a speed of learning and a number of parameters,
we decided to limit relevant letters for a morphological
analysis to a ﬁxed window. Symbols of the word w will be
inserted to this window justiﬁed to the right. This reﬂects
characteristic of the Czech language, where the major part
of morphological information carries an end of a word, i.e.,
sufﬁx and ending. We choose the size of the window to eight
characters.
The morphological net is based on the Back-propagation
net [37]. It includes input layer units, which form vector xm
of the length 331.
2We consider the basic Czech alphabet as a set of small letters together
with their possible variants (with the accent, wedge or circle), total 41
letters. The mapping µ is a bijection after a restriction on this set.
66
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

• In a ﬁrst part of the layer, every unit corresponds to a
possible symbol of the input window. It thus consists
of 8 · 41 = 328 symbols and these are set as follows:
∀i ∈ {1, . . . , 8}, j ∈ {1, . . . , 41} :
xm
41·(i−1)+j = 1
if j < |w| and ψ(α|w|−i+1) = j
xm
41·(i−1)+j = 0
otherwise
• The next two units are coding a size of the word w:
xm
359 = 0 and xm
360 = 0 if w is small
xm
359 = 1 and xm
360 = 0 if w is mixed
xm
359 = 1 and xm
360 = 1 if w is capital
• The last symbol contains a length of the word:
xm
361 = |w|
The output vector has the length 10, which is a length of
ym shortened by the position for punctuation (Z) handled
before. On an output of the morphological phase value 0
is being appended on this position. During the training, an
input word from the corpus is transformed according the
previous deﬁnition to the input vector, which is presented to
the morphological net and a result is computed. Next, this
result is compared to the desired output, i.e., vector ˆy (t),
where t is the right tag assigned to the word in the corpus.
Weights of the net are subsequently adjusted according to
the Backpropagation algorithm. [37]
We transformed the data from PDT [12] to a suitable
form, which includes beside word forms and their part of
speech tags the information about ends of sentences. The
data containing 670528 words were split to training, with
which the net was trained, and testing, on which an error
rate was measured in an approximate ratio 2 : 1.
We set in all cases the learning rate of the algorithm
α = 0.2. Momentum was not used, because there was no or
negative inﬂuence on a learning. Training data were passed
in six cycles by the net. This procedure was performed on
nets with different conﬁgurations. For nets with one hidden
layer it were 50 (i.e., 331 in the input, 50 in the hidden and
10 in the output layer), 100, 150, for nets with two hidden
layers then 50-25, 100-25, 100-50, 150-100.
On the testing data is then with estimated the tag of
each word as a maximal value in the output vector of the
morphological phase:
ˆtm (w)
=
arg max
t
ym
t (w)
We see that larger nets do not automatically lead to better
performance. Greater amount of parameters makes probably
the learning harder and longer. The relative simple net has
the best result with only one hidden layer containing 100
units.
D. Context Phase
The goal of the context phase is a processing of infor-
mation about the surrounding words, the actual context of a
word, in order to estimate its real tag. We took up a neural
net-based disambiguation method Net-Tagger, described in
[29]. Nevertheless there are several differences between this
and our approach. It is namely:
• Inputs of context net are not probabilities of tags
assigned to the word, which was looked up in a
dictionary of word forms, but an output vector of the
morphological phase.
• Context net contains a hidden layer. Computational
power of the nets without this layer is limited. [37]
On the other side, multilayer nets can model more
complicated functions.
We take into account for the actual word its left context
with size p and right context with size f. Every word in
this context is represented by |T | (i.e., 11) units expressing
likelihood that the word has a corresponding tag, based on
all available information. This means for the actual word wi
and words in the right context, for which context phase was
not yet processed, the outputs of the morphological phase,
i.e., vectors wi+1 . . . wi+f. For words in the left context,
previous outputs of the context layer are available. The input
layer thus contains 11·(p+1+f) neurons. If |S| is a length
of the sentence S, we set the input vector xc for the word
wi as follows:
∀j ∈ {−p, . . . , f}, t ∈ {1, . . . , 11} :
xc
11·(j+p)+t = yc
t(wi+j)
if j < 0 and 1 ≤ i + j
xc
11·(j+p)+t = ym
t (wi+j)
if 0 ≤ j and i + j ≤ |S|
xc
11·(j+p)+t = 0
otherwise
The output of the net is the vector yc (wi) with a length
|T |.
The input of the words from the left context brings a
recursion into the computation. This can complicate the
learning procedure, especially in the moment, when the
outputs are not yet correct. Hence in the time of learning
we replace the vector yc (wi−j), analogously to [29], by
weighted average of the previous output vector yc (wi−j) of
the context net and the output of the morphological phase
ym (wi−j):
(1 − eτ) · yc (wi−j) + eτ · ym (wi−j)
Here eτ is a coefﬁcient, which in the beginning, when
the error of the net is high, is near to 1 and with successful
learning falls. We used there for this value an exponential
moving average of an error of the net, i.e., ratio of wrong
assigned tags, if the error is greater than 0.1. In the other
case 0 is assigned to eτ.
The inputs from the training data were put to the net
during the learning according to the previous deﬁnitions and
67
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

Table I
ERROR RATE OF THE NNTAGGER DEPENDING ON A CONTEXT SIZE. IN
THE ROWS ARE SIZES OF A LEFT CONTEXT p, IN THE COLUMNS ARE
SIZES OF A RIGHT CONTEXT f.
0
1
2
3
0
4.72 %
4.74 %
4.67 %
4.61 %
1
4.72 %
4.53 %
4.42 %
4.45 %
2
4.72 %
4.51 %
4.40 %
4.41 %
3
4.72 %
4.48 %
4.43 %
4.37 %
the outputs were vectors ˆy (t), where t is a correct tag of the
word. The output of the best morphological net was used as
the input of the context net, i.e., the net 331-100-10. The
same data source as by the morphological phase was used
for the training. The training was processed six times with
a learning rate equal to 0.2 and without a momentum.
The output of the NNTagger method is thus a sequence
of tags induced from values of the vectors yc (w). Formally
speaking, the tag belonging to a word w is:
ˆt (w)
=
arg max
t
yc
t (w)
We show in the table I achieved error rates on the
remaining test data depending on sizes of the left (p) and
the right context (f).
The lowest relative accuracy of the tagger was by interjec-
tions which were never tagged. This part of speech occurs
in Czech texts only rarely. Greater error rate was also by
particles which have disputable morphological characteris-
tics. The highest absolute number of mistakes was between
nouns and adjectives in both directions.
IV. COMPRESSION OF SENTENCES
Any natural language utterance contains a great amount
of implicit information. Now we will show use of the
information about parts-of-speech in an input text for its
compression.
We deﬁne a type of a sentence (or of a sentence part),
which is determined by its non-special words w1, . . . , wn, as
a sequence of part-of-speech tags of this sentence. (T(S) =
tS
1,n )
A. XBW Project
We adopt the platform XBW [20] and exploit some of
its features in our work. The goal of the XBW project
is creation of an interface to test of different compression
methods with an accent on compression of texts in the XML-
format. It implements some of the elementary algorithms for
a number coding and decoding (Elias, arithmetic, Huffman
coding), string compression (Burrows-Wheeler, PPM, MTF,
RLE, LZ), XML ﬁles processing, dictionary compression,
some of auxiliary data structures (trie) and suggests their
collaboration.
B. Sentence Parts
We performed an experiment in order to ﬁnd out duplic-
ities of sentence types. We split a language corpus between
training and testing data in an approximate proportion 2:1.
A dictionary of the sentence types was created from the
training data. Then there were only 16 % overlapping of the
sentence types from the test data and those in the dictionary.
Variation of types of whole sentences is thus too high for
our purpose. Hence, we propose using shorter segments of a
sentence, called sentence parts. Due to a heuristic character
of the deﬁnition, sentence parts do not have to match with
sentence structure.
The parsing of an input sentence proceeds as follows.
After skipping of an initial sequence of punctuations, the
sentence is being passed from the left to the right and divided
in these cases:
• Put a boundary of a sentence part after occurrence
of punctuation marks “,”, “:”, “””, “(” or “)”. If it
is immediately followed by some other punctuation
marks, move the boundary after them. If there is not
a whitespace between the punctuation and a next word
(so this word immediately follows), move boundary of
sentence part before the punctuation.
• Put the boundary after “-”, if between it and the next
word is a white space (unlike the case of a composed
word or the Czech particle -li).
• Put the boundary before the Czech conjunctions “a”,
“i”, “ani”, “nebo” or “ˇci”. These conjunctions usually
have no commas before them.
After division of the sentence part the parsing continues on
the rest of the sentence until the end of sentence is reached.
Types of above deﬁned sentence parts have the duplicity
in the dictionary higher, approximately 37 %. Nevertheless
the number of new emerging types is relatively high. Thus
we will seek for methods of an effective coding of new
sentence types.
After the pre-processing we have input text decomposed
in sentence parts in this form:
SP = w1 · s1 · w2 · s2 · · · · · wn · sn
where n is length of the sentence, wi (for i = 1, . . . , n)
is a non-special word (alpha word, numeric word or punc-
tuation) and si is a special word (even with zero length),
which always follows wi. There is also an sentence type
(T(SP ) = tSP
1,n) for every sentence part SP .
C. Models
Lets deﬁne a model of text generation which will be used
for text compression and decompression.
Four models will be created for these layers of the text:
sentence part, sentence type, word, and symbol. Except the
ﬁrst one we can all of them subsume under a template. If
we have µ as a basic element (type, word or symbol), then
this model (template M) includes:
68
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

• Dictionary of the basic elements ∆. This is the injective
function, whose domain is a set of the known elements
and the range is the set of natural numbers.
• Probability distribution ρ of the known elements and
reserved value esc for elements, which are not found
in dictionary ∆. These structures are adaptive. If a
new element occurs, new values are added to the ∆
and ρ. After the occurrence of an element µ, relative
probability ρ(∆(µ)) increases. The implementation of
r depends on concrete compression method (in this
case arithmetic coding). Dictionaries and distributions
of elements could be initialized from frequent symbols
based on a text corpus. Details about the initialization
of dictionaries will be provided in the next section.
• Model of unknown elements M sub. If the element is
not found in the dictionary, it has to be coded on a
lower layer (the models of lengths, symbols, or parts-
of-speech).
For coding of the element µ to a bit string with knowledge
of the model M we will use this notation:
K(µ|M)
In most cases we abstract from the concrete coding algo-
rithm. So the code of symbol with index i and distribution
ρ (i.e., K(i|ρ) can be realized by any adaptive algorithm).
We introduce an operator of concatenation:
K = K1 · K2
which means that the bit string K consists of the code K1
followed by the code K2.
We do not describe the decoding explicitly. It follows
directly from the coding procedure, which is proposed as a
bijection between a set of possible texts and a set of possible
codes.
For every layer we have thus its own model structure:
a model of sentence parts M S, a model of sentence types
M T , model of words M W and model of symbols M C. In
addition, we have a model for lengths of words or sentences
M Lk, which describes generation of non-zero lengths. Some
of these structures can be multiple instantiated, as is the case
of the word models for each part-of-speech.
We will describe the compression procedure from above,
i.e., from higher structures as ﬁles and sentence parts to the
single characters.
The input ﬁle is decomposed after pre-processing into the
sequence of sentence parts S1, . . . , Sm. These sentence parts
are coded one by one and the code of empty sentence λ is
appended at the end.
1) Sentence parts:
Model: As we said, a sentence part with length n is
determined by a sequence of non-special words w1, . . . , wn,
a sequence of special words s1, . . . , sn, which follow them,
and its type, i.e., a sequence of tags, tSP
1,n. All of these
sequences have the lengths equal to n. Empty sentence
part (λ, zero length sequence) has a special meaning and
it represents an end of ﬁle.
Model of sentence parts consists of these submodels:
• Model of sentence types M T .
• For
every
part-of-speech
one
model
of
words
M WN , . . . , M WI. We have not ﬁnd any reason for
separate models of symbols for each part-of-speech,
because the basic distribution of letters differs in the
Czech words only slightly. So there is shared model of
symbols for each of them. On the other hand, there are
differences in average lengths of each part-of-speech,
so we proposed independent models of lengths.
• Model of punctuations M CZ.
• Model of numeric words M Wnum.
• Model of special words M WS.
The last two have their own models of lengths and
symbols.
Coding: Code of a sentence part consists of a code of
the sentence type and codes of each word of this sentence
part. Thus:
K(S|M S)
=
K(tS
1,n|M T ) · K(w1|M1) · K(s1|M WS) ·
· · · · K(wn|Mn) · K(sn|M WS)
where K(wi|Mi) = K(wi|M Wti) if ti is a tag of the
word wi, different from punctuation or numeral. If the word
is numeral, the alpha numeral must be distinguished from
numbers. It is accomplished by a one-bit token. If it is an
alpha numeral we use its model:
K(wi|Mi) = 0 · K(wi|M WC)
For a number the word is encoded by the model of
numbers:
K(wi|Mi) = 1 · K(wi|M Wnum)
On the other hand, a punctuation is a one-character word
wi = α, thus the coding is provided directly by the symbol
model of punctuations.
K(wi|Mi) = K(α|M CZ)
2) Sentence types:
Model: The model of sentence types contains a dictio-
nary ∆T of sentence types which were in an initialization set
or appeared in the input text. The dictionary is represented
by a trie (which maps a type to a natural number) over the
alphabet of tags and by a table (which maps a value to a
type). The next part of this model is a probability distribution
ρT of items of the dictionary ∆T , which is used for coding
and decoding. The dictionary and the distribution include an
element corresponding with the type of an empty sentence
λ.
A model of lengths M L30 is then needed because of the
coding of new types. Lengths of sentence types are at the
69
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

same time the lengths of whole sentence parts, so we do not
need to code this information on the higher level. Finally
we have to consider a model of generation of the unknown
type tags. The basic variant is to have an unconditional
probability distribution of tags occurrence ρP . However, we
decided to include the information about right context of
the actual tag. So, the resulting model is a bigram model.
Instead of one distribution, there is a distribution ρPt′ for
every possible tag t′ which is on the right side of the actual
position t. Together with them there is a distribution ρP∅
of the tags at the end of the type, which have no right
context. We used the entropy of tags and estimated that
the improvement on the new types would be approximately
21 %.
Coding: We have to divide the coding of sentence
types to two cases. In the ﬁrst case, the type t1,n is in the
dictionary ∆T . It is then possible to code corresponding
value according to the distribution ρT :
K(t1,n|M T ) = K(∆T (t1,n)|ρT )
The distribution is subsequently adjusted.
Otherwise, the type is not in the dictionary. Thus, the
escape symbol is generated and the new type must be coded.
This consists of coding of its length n and all of its tags from
the right to the left with knowledge of the right context.
3) Words:
Model: There are several instances of a model of
words in the general model of texts. There are models
corresponding to all part-of-speeches except punctuations
(alpha words: M WN , . . . , M WI), model of numbers (nu-
meric words M Wnum) and model of special words (M WS).
Each model M W is responsible for coding of a word w
(the string of symbols α1 · · · · · αm). The knowledge about
the position of the alpha word in a sentence part is essential
for coding of its size. From a measuring of word sizes on the
corpus we determined the most probable sizes of a word at
the beginning and on following positions of a sentence part.
The implicit value on the ﬁrst position is a mixed word (i.e.,
word with capital the ﬁrst letter and small other letters), on
the following positions are implicit small words.
Regarding this, there are again the dictionary ∆W and the
adaptive probability distribution ρW , both of them include
except the esc symbol for new word these function symbols:
• escM→C, i.e., the token which means that the word is
capital at the beginning of a sentence part.
• escM→S for small words at the beginning of the
sentence part.
• escS→C for capital words on the second or another
following position.
• escS→C for mixed word which is not at the beginning.
The dictionary and the distribution are moreover able to
code the empty word λ, as for instance the non-existent
white space between a word and the next punctuation. The
dictionary is represented by a pair of a trie over the alphabet
of UNICODE characters and a table which is maps from the
values to the words. In order to code new words, the M W
contains a model of length M L20 and a model of symbols
M C (in the case of alpha words representing small letters).
Coding: In the ﬁrst place, if the word is an alpha word,
we have to compare its size (small, mixed or capital) with the
predicted one and, if necessary, to emit an escape symbol.
Then the small variant of the word w′ = µg(α1) . . . µg(αn)
is taken into account.
• If the word is capital on the ﬁrst position:
K(w|M W ) = K(escM→C|ρW ) · K′(w′|M W )
• If it is a small word on the ﬁrst position:
K(w|M W ) = K(escM→S|ρW ) · K′(w′|M W )
• If it is a capital word on next positions:
K(w|M W ) = K(escS→C|ρW ) · K′(w′|M W )
• If it is a mixed word on next positions:
K(w|M W ) = K(escS→M|ρW ) · K′(w′|M W )
• In all other cases (including the non-letter words) the
size of word is known:
K(w|M W ) = K′(w′|M W )
Next, the w′ is coded. If the w′ is in the dictionary ∆W ,
the code can be emitted directly:
K′(w′|M W ) = K(∆W (w′)|ρW )
.
Otherwise, we generate the codes of the escape symbol,
the length of the word w′ and all of its symbols. Then the
dictionary and the distribution are adjusted.
4) Lengths:
Model: Model of lengths (M Lk ) contains only a
probability distribution ρL of k + 1 values. The values
1, . . . , k are the most frequent lengths. The value k + 1 is
reserved as a special symbol for higher lengths. As usually
the distribution is adaptive and after (de)coding of a length
l is the relative probability of occurrence of l increased. The
range of ρL is, on the contrary, ﬁxed and new values are not
added into the distribution.
Coding: There are two possibilities. First, if 1 ≤ l ≤ k,
we code it directly:
K(l|M Lk) = K(l|ρL)
Otherwise k < l, the auxiliary overﬂow symbol is emitted,
followed by the alpha code of a surplus value:
K(l|M Lk) = K(k + 1|ρL) · α(l − k)
5) Symbols:
70
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

Table II
COMPARISON OF VARIOUS METHODS COMPRESSION RATIOS TB.
Method
Compression Ratio
A1
4.60
A2
4.41
A3
4.19
A4
3.85
A5
3.34
gzip
4.43
bzip2
4.38
Model: Model of UNICODE-symbols generation is in
the sentence part model included in these instances:
• Model of small letters M CP shared by models of all
alpha part-of-speeches (M WN , . . . , M WI)
• Model of characters in numbers M Cnum (digits, deci-
mal separators, symbol “-”) which is part of model of
numbers (numeric words).
• Model of punctuations M CZ
• Model of special words symbols M CW (e.g., white
space)
Each of these models M C contains a symbol dictionary
∆C, consisting of two tables mapping frequent symbols
to natural numbers and back, and an adaptive probability
distribution ρC. This distribution includes among others a
special symbol esc for new characters.
Coding: The coding procedure of a symbol α has two
alternatives. If the symbol is in the dictionary ∆C, we emit
simply the corresponding code:
K(α|M C) = K(∆C(α)|ρC)
with increase of a relative frequency of ∆C(α).
If α is new, we put esc symbol on the output followed
by full 32-bit beta code of the character:
K(α|M C) = K(esc|ρC) · β32(α)
.
New element is subsequently added into the ∆C and the
ρC.
V. RESULTS
We test there our method in various conﬁgurations of
the initial dictionaries and the coding algorithm of basic
elements.
We
tested
the
mixtures
of
the
initial
dictionaries
(A1 . . . A5) on the testing set TB containing 100 shorter
documents (as newspaper articles). We show the results in
table II in bpc compared with commonly used methods bzip2
and gzip.
We see the expected improvement of a compression rate
for small documents. The variant A3 already overcame both
methods.
Next we present dependency of a compression rate on
a size of a compressed ﬁle. We split the set TA by size
Table III
COMPARISON OF COMPRESSION RATIOS FOR VARIOUS FILE SIZES
IN TA.
Method
10–100 kB
100–500 kB
500–1000 kB
A1
4.21
3.78
3.69
A2
4.00
3.61
3.51
A3
3.76
3.42
3.34
A4
3.47
3.22
3.16
A5
3.10
2.95
2.93
gzip
3.69
3.46
3.47
bzip2
3.35
2.71
2.57
into three categories and observed results within them. The
results are in table III.
There is again revealed that variant A5 achieves best
results for all sizes of an input ﬁle. Due to this initialization
the method overcame successful algorithm bzip2 based on
a Burrows-Wheeler transform.
We performed the compression and decompression of
the set TA concatenated into one ﬁle in order to ﬁnd out
time demands of the algorithm. The compression of such
concatenated ﬁle with size 4.76 MB with the variant A5
took about 82 seconds on the computer with processor unit
AMD Sempron 2800+, 1.60 GHz and 960 MB RAM. The
decompression took about 4 seconds. Here we can observe
the asymmetry between compression and decompression
phases.
VI. CONCLUSIONS AND FUTURE WORKS
In this paper we have introduced the method of extraction
of information from an input text which would allows better
modelling of this text and improve our ability to compress
it. Such knowledge has been for us the sequence of tags of
words in the text which represent the words’ belonging to the
parts of speech. The sequence of tags for a sentence is called
a sentence type. This makes possible for us to introduce
knowledge about the structure of a sentence and functions
of particular words in an utterance.
In order to get the sentence type, we have proposed the
approximative part-of-speech tagger for the Czech language,
the NNTagger, based on neural networks. Both procedures
– the morphological analysis and disambiguation – were
implemented by Backpropagation nets. Such an approach
was motivated by a great amount of information contained
in the regular structures of words. The NNTagger was
trained on the corpus of tagged texts and tested in various
conﬁgurations. The slightly lower accuracy of this method
is compensated by the absence of any dictionary in both
phases and justiﬁed by the observation that a compression
ratio is not sensitive to small errors.
We have constructed model structures of texts on different
layers (a symbol, word, sentence type, sentence part) with
respect to the additional knowledge (part of speech, kind
of symbol). The coding and decoding procedures have been
71
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

implemented based on these models and for that purpose a
novel notation has been introduced.
We proposed the methods of creating databases of fre-
quent words and sentence types from a set of Czech doc-
uments in order to set initially each of the models. Such
initialization can improve the resulting compression ratio
especially in domain of small documents. However, each
of the models contributes to this improvement with another
proportion due to its different function in the language.
Therefore, several possible mixtures of the initial dictionar-
ies have been compiled with respect to the improvement of
a compression ratio and the size of the databases.
The method has been tested in various conﬁgurations of
the initialization databases and results compared with both
commonly used compression programs and similar purely
word-based compression methods. These results proved an
advantage of coding of the sentence type as information
about syntactic structures separately from the words of a
sentence. The creation of the initialization mixtures shows
that the sets containing words of a part of speech have dif-
ferent properties, e.g., the variability both in a document and
among different documents, the distribution of word lengths
etc., and their models must be initialized differently. With
such an initialization, the algorithm overcame commonly
used compression algorithms gzip and bzip2 in compression
of small ﬁles up to 100 kB.
Features of our solution can be summarized as follows:
• Approximative tagger NNTagger had slightly lower
accuracy than usually used Czech tagging algorithms.
On the other hand, it does not need any additional
dictionary of word forms or lemmas.
• The method of exploitation of part-of-speech tags se-
quences as a linguistic information in a sentence has
been proposed which allows us better to grasp model
of text generation.
• Our text compression algorithm gained 17–27 % better
compression ratio in comparison with algorithms based
only on words with comparable settings and initializa-
tion.
The experiments with the linguistic compression in the
Czech language have shown some possible challenges for a
future work. The tagging method NNTagger has recorded
good results regarding the low costs on additional data;
however it has lower accuracy than other methods, par-
ticularly on non-inﬂective words. Thus some of the more
sophisticated methods should be used and the effect of a
tagger’s accuracy on the compression should be tested. Also
the frequency of unknown sentence types has been relatively
high during the compression, so we plan to implement other
string compression method, e.g., PPM [20]. Finally, an open
question is an application of analogous compression method
on languages with different characteristics. For instance,
due to less stress on the word morphology in the English
language, appropriate tagger has to be used, the morpholog-
ical phase of NNTagger should be replaced by dictionary-
based method, and another corresponding tag set has to be
chosen. On the other side, ﬁxed word order can cause an
increase in the signiﬁcance of sentence types and possibly
their better compression ratio. The symbol-based languages,
like Chinese, raise other challenges. In such languages, the
word segmentation is a hard problem which has to be solved
by non-trivial algorithm. Also the model of words would be
modiﬁed with respect to different relation between words
and characters.
Ackowledgement
This work has been supported by the Internal Grant
Agency of VˇSFS under project IGA VˇSFS 7721.
REFERENCES
[1] Abel J. and Teahan W. Universal Text Preprocessing for Data
Compression. In: IEEE Transactions on Computers, 54 (5),
pp. 497–507, (2005)
[2] Adiego J. and Fuente P. On the Use of Words as Source
Alphabet Symbols in PPM. In: Storer JA, Cohn M (Eds.).
Proceedings of 2006 IEEE Data Compression Conference,
IEEE Computer Society Press, Los Alamitos, California, USA
(2006) 435.
[3] Brill E. A Corpus-Based Approach to Language Learning,
dissertation, Department of Computer and Information Sci-
ence, University of Pennsylvania; Philadelphia, 1993.
[4] Chapin B. and Tate SR. Higher Compression from the
Burrows-Wheeler Transform by Modiﬁed Sorting. In: Storer
JA, Cohn M (Eds.). Proceedings of 1998 IEEE Data Com-
pression Conference, IEEE Computer Society Press, Los
Alamitos, California, USA (1998) 532.
[5] Collins M. Discriminative Training Methods for Hidden
Markov Models. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing. Morristown,
NJ, 2002; 1–8.
[6] Cutting D., Kupiec J., Pedersen J., and Sibun P. A Practical
Part-of-Speech Tagger. In Proceedings of the Third Confer-
ence on Applied Natural Language Processing. Trento, 1992.
[7] Dvorsk´y J., Pokorn´y J., and Sn´aˇsel V. Word-based Com-
pression Methods for Large Text Documents. In: Storer JA,
Cohn M (Eds.). Proceedings of 1999 IEEE Data Compression
Conference, IEEE Computer Society Press, Los Alamitos,
California, USA (1999) 523.
[8] Gailly J. The gzip home page.
http://www.gzip.org/ [1 March 2010].
[9] Hajiˇc J. Disambiguation of Rich Inﬂection (Computational
Morphology of Czech), Karolinum; Prague, 2004.
[10] Hajiˇc J. and Hladk´a B. Tagging Inﬂective Languages: Pre-
diction of Morphological Categories for a Rich, Structured
Tagset. In Proceedings of COLING-ACL Conference. Mon-
treal, 1998; 483–490.
72
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

[11] Hana J. and Zeman D. Manual for Morphological Annotation.
Revision for the Prague Dependency Treebank 2.0. ´UFAL
Technical Report No. 2005–27. Charles University; Prague,
2005.
[12] Honetschl¨ager V et al. The Prague Dependency Treebank,
´UFAL MFF UK; Prague, 2006.
http://www.ufal.mff.cuni.cz/pdt2.0/ [1 March 2010].
[13] Horspool RN. and Cormack GV. A general purpose data com-
pression technique with practical applications. Proceedings of
the CIPS Session 84, (1984) 138-141.
[14] Horspool RN. and Cormack GV. Constructing WordBased
Text Compression Algorithms. In: Storer JA, Cohn M (Eds.).
Proceedings of 1992 IEEE Data Compression Conference,
IEEE Computer Society Press, Los Alamitos, California, USA
(1992) 62-71.
[15] Isal RYK. and Moffat A. Word-based Block-sorting Text
Compression. In: Proceedings of the 24th Australasian con-
ference on Computer science, Gold Coast, Queensland, Aus-
tralia, (2001) 9299.
[16] Koch´anek J., L´ansk´y J., Uzel P., and ˇZemliˇcka M. Multi-
stream Compression. In: Storer JA, Marcellin MW (Eds.).
Proceedings of 2008 Data Compression Conference, IEEE
Computer Society Press, Los Alamitos, California, USA,
(2008) pg. 527.
[17] Korodi G., Rissanen J., and Tabus I. Lossless Data Com-
pression Using Optimal Tree Machines. In: Storer JA, Cohn
M (Eds.). Proceedings of 2005 IEEE Data Compression
Conference, IEEE Computer Society Press, Los Alamitos,
California, USA (2005) 348357.
[18] Kuthan T., and L´ansk´y J. Genetic algorithms in syllable based
text compression. In: Pokorn´y J, Sn´aˇsel V, and Richta K
(Eds.). Proceedings of the Dateso 2007 Annual International
Workshop on DAtabases, TExts, Speciﬁcations and Objects.
CEUR-WS, Vol. 235, (2007) 21-34.
[19] L´ansk´y J., Chernik K., and Vlˇckov´a Z. Comparison of Text
Models for BWT. In: Storer JA, Marcellin MW (Eds.).
Proceedings of 2007 IEEE Data Compression Conference,
IEEE Computer Society Press, Los Alamitos, California, USA
(2007) 389.
[20] L´ansk´y J., ˇSest´ak R., Uzel P., Kovalˇcin S., Kumiˇc´ak P., Urban
T., and Szab´o M. XBW - Word-based compression of non-
valid XML documents. http://xbw.sourceforge.net/ [1 March
2010].
[21] L´ansk´y J. and ˇZemliˇcka M. Compression of Small Text Files
Using Syllables. In: Storer JA, Cohn M (Eds.). Proceedings of
2006 IEEE Data Compression Conference, IEEE Computer
Society Press, Los Alamitos, California, USA (2006) 458.
[22] L´ansk´y J. and ˇZemliˇcka M. Text Compression: Syllables. In:
Richta K, Sn´aˇsel V, Pokorn´y J (Eds.). Proceedings of the
Dateso 2005 Annual International Workshop on DAtabases,
TExts, Speciﬁcations and Objects. CEUR-WS, Vol. 129,
(2005) 32-45.
[23] Manning C. and Sch¨utze H. Foundations of Statistical Natural
Language Processing, MIT Press; Cambridge, 1999.
[24] Moffat A. Word based text compression. In: Software-
Practice and Experience 19 (2), (1989) 185198.
[25] Moura ES., Navarro G., Ziviani N., and Baeza-Yates R. Fast
searching on compressed text allowing errors. In: Proceedings
of the 21st annual international ACM SIGIR conference on
Research and development in information retrieval. ACM
Press, New York, (1998) 298-306.
[26] Ratnaparkhi A. A Maximum Entropy Model for Part-of-
Speech Tagging. In Proceedings of the First Empirical Meth-
ods in Natural Language Processing Conference. Philadel-
phia, 1996; 133–141.
[27] Rein S., G¨uhmann C., and Fitzek F. Compression of Short
Text on Embedded Systems. Journal of Computers, Vol. 1,
No. 6, (2006).
[28] Ryabko BY. Data compression by means of a book stack.
Prob. Inf. Transm., 16(4), (1980). In Russian.
[29] Schmid H. Part-of-speech tagging with neural networks. In
Proceedings of the International Conference on New Methods
in Language Processing. Manchester, 1994; 44–49.
[30] Schmid H. Probabilistic Part-of-Speech Tagging Using Deci-
sion Trees. In Proceedings of the 15th COLING Conference.
Kyoto, 1994; 172–176.
[31] Sedl´aˇcek R. Morfologick´y analyz´ator ˇceˇstiny, diploma thesis,
FI MU; Brno, 1999. In Czech.
[32] ˇSest´ak R., and L´ansk´y J. Compression of Concatenated Web
Pages Using XBW. In: Geffert V et al. (Eds.). SOFSEM 2008,
LNCS 4910, Springer-Verlag Berlin, Heidelberg, Germany,
(2008) pg. 743–754.
[33] ˇSest´ak R., L´ansk´y J., and ˇZemliˇcka M. Sufﬁx Array for Large
Alphabet. In: Storer JA, Marcellin, MW (Eds.). Proceedings
of 2008 Data Compression Conference, IEEE Computer So-
ciety Press, Los Alamitos, California, USA, (2008) pg. 543.
[34] Seward H. bzip2: Documentation.
http://www.bzip.org/1.0.5/bzip2-manual-1.0.5.pdf [1 March
2010].
[35] Shkarin
D.
Durilca
Light
and
Durilca
0.4b.
http://compression.graphicon.ru/ds/ [3 February 2007].
[36] Skibinski P. Reversible data transforms that improve effec-
tiveness of universal lossless data compression. Doctor of
Philosophy Dissertation, University of Wroclaw, (2006).
[37] ˇS´ıma J., and Neruda R. Teoretick´e ot´azky neuronov´ych s´ıt´ı,
Matfyzpress; Prague, 1997. In Czech.
[38] Teahan W. Modelling English text. Ph.D. dissertation, Uni-
versity of Waikato, New Zealand, (1998).
[39] ¨Uc¸ol¨uk G., and Toroslu H. A Genetic Algorithm Approach
for Veriﬁcation of the Syllable Based Text Compression
Technique. Journal of Information Science, Vol. 23, No. 5,
(1997) 365-372.
[40] Votrubec J. Volba vhodn´e sady rys˚u pro morfologick´e
znaˇckov´an´ı ˇceˇstiny, diploma thesis, MFF UK; Prague, 2005.
In Czech.
73
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

