Concurrent Differential Evolution for Uncertain Optimization Problems
Kiyaharu Tagawa
School of Science and Engineering
Kinki University
Higashi-Osaka 577-8502, Japan
tagawa@info.kindai.ac.jp
Takashi Ishimizu
School of Science and Engineering
Kinki University
Higashi-Osaka 577-8502, Japan
takasi-i@info.kindai.ac.jp
Abstract—Multi-core CPUs, which have more than one
processor (core), have been introduced widely into personal
computers. Therefore, in order to utilize the additional cores
to execute various costly application programs, concurrent
implementations of them have been paid attention to. In this
paper, a concurrent program of the latest evolutionary algo-
rithm, i.e., differential evolution, is described. Furthermore,
the concurrent program of differential evolution, which is
called concurrent differential evolution, is revised to reduce
the computational time for solving optimization problems in
the presence of a wide range of uncertainties. Many real-world
applications can be formulated as an uncertain optimization
problem in which a probabilistic objective function has to be
evaluated by using Monte Carlo integration. Consequently, it
usually takes a long computational time to solve the uncertain
optimization problem. The results of the numerical experiments
conducted on two classes of uncertain optimization problems
show that the revised version can reduce the computational
time apparently comparing with the conventional version.
Keywords-evolutionary algorithm; concurrent program; uncer-
tain optimization problem; differential evolution.
I. INTRODUCTION
Differential Evolution (DE) is arguably one of the most
powerful stochastic real-parameter optimization algorithms
in current use [1], [2]. DE can be regarded as an evolutionary
algorithm. However, comparing with typical evolutionary
algorithms such as genetic algorithm, evolution strategy and
particle swarm optimization, it has been reported that DE
exhibits an overall excellent performance for a wide range
of benchmark problems [3], [4]. Furthermore, because of
its simple but powerful searching capability, DE has gotten
numerous science and engineering applications [4].
The procedure of evolutionary algorithms for updating the
individuals, i.e., tentative solutions, included in the popula-
tion is called “generation alternation model”. Evolutionary
algorithms usually employ either of two types of generation
alternation models [5]. The ﬁrst one is called “generational
model”, while the second one is called “steady-state model”.
The original DE proposed by R. Storn and K. Price has
been based on the generational model [1]. According to the
generational model, DE holds two populations, namely the
old one and the new one. After generating a new population,
a concurrent population is replaced all together by the
new population. On the other hand, a new DE based on
the steady-state model has been proposed lately [6], [7].
The new DE is sometimes called “Sequential DE” (SDE)
[6]. According to the steady-state model, SDE has only
one population. Then each individual in the population is
updated one by one. Comparing with the generational model,
the steady-state model is usually suitable for parallelized
evolutionary algorithms [10]. That is because evolutionary
algorithms with the steady-state model need not synchronize
the manipulation about any individual in each generation for
replacing the old population by the new population.
Recently, multi-core CPUs have been introduced widely
into personal computers. In order to utilize the additional
cores to execute costly application programs, concurrent
implementations of them are demanded [8]. In our previous
paper [9], a concurrent program of DE, which is called
Concurrent DE (CDE), was proposed. Exactly, the proposed
CDE was a parallelized version of the above SDE.
In this paper, CDE is revised for solving uncertain op-
timization problems by using multi-core CPUs effectively.
The revised CDE is called CDE for Uncertain optimiza-
tion (CDEU). In many real-world optimization problems, a
wide range of uncertainties have to be taken into account.
Therefore, various evolutionary algorithms including DE
for solving uncertain optimization problems have received
increasing attention in recent years [12]–[14]. However, the
conventional evolutionary algorithms applicable to uncertain
optimization problems have not been parallelized for multi-
core CPUs. Because the objective functions of uncertain
optimization problems are approximated using Monte Carlo
integration, the evaluation of them usually takes a very
long time. CDEU is a promising optimizer for solving
uncertain optimization problems with multi-core CPUs. The
performance of CDEU is demonstrated in a comparison with
CDE in two classes of uncertain optimization problems,
namely noisy and robust optimization problems.
The rest of this paper is organized as follows. Section II
describes SDE and a basic strategy of SDE used to generate
a new individual. Section III describes CDE. Section IV ex-
plains uncertain optimization problems. Section V proposes
CDEU for solving uncertain optimization problems. Section
VI demonstrates the performance of CDEU in comparison
with CDE. Finally, Section VII concludes this paper.
48
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-172-4
ADVCOMP 2011 : The Fifth International Conference on Advanced Engineering Computing and Applications in Sciences

II. SEQUENTIAL DE (SDE)
In this section, we describe a new DE based on the steady-
state model, i.e., SDE, and a basic strategy of SDE used to
generate a new individual, i.e., a tentative solution.
A. Representation
The optimization problem can be formulated as shown
in (1). The optimal solution of the optimization problem is
represented by a D-dimensional real-parameter vector x =
(x0, · · · , xD−1) that minimizes the value of the objective
function f(x). Besides, each xj ∈ IR is limited to the range
between the lower xj and the upper xj boundaries.
 minmize
f(x) = f(x0, · · · , xj, · · · , xD−1)
subject to
xj ≤ xj ≤ xj, j = 0, · · · , D − 1.
(1)
SDE is used to solve the optimization problem shown
in (1). A tentative solution of (1) is represented by a real-
parameter vector and called an “individual”. SDE holds
NP individuals within the population for each generation.
Therefore, the i-th individual xi ∈ P (i = 0, · · · , NP − 1)
arranged in the current population P is represented as
xi = (x0,i, · · · , xj,i, · · · , xD−1,i)
(2)
where, xj ≤ xj,i ≤ xj, j = 0, · · · , D − 1.
B. Strategy of SDE
In order to generate a new individual, SDE has borrowed
the strategy of DE [6], [7]. In this paper, a basic strategy of
DE named “DE/rand/1/exp” is described and used.
For each individual xi ∈ P (i = 0, · · · , NP − 1), which
is also called the target vector, three different individuals,
say xr1, xr2 and xr3 (i ̸= r1 ̸= r2 ̸= r3), are selected
randomly from the population P. Then a new individual
u called the trial vector is generated from them as shown
in (3). An index jr ∈ [0, D − 1] is selected randomly.
As a result, the trial vector u = (u0, · · · , uj, · · · , uD−1)
differs from the target vector xi at least one element ujr.
Besides, randj[0, 1] denotes the random number generator
that returns a uniformly distributed random number from
within the range between 0 and 1. Both the scale factor SF
(0 < SF ≤ 1) and the crossover rate CR (0 ≤ CR ≤ 1) are
control parameters speciﬁed by the user in advance.
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
j = jr;
do {
uj = xj,r1 + SF · (xj,r2 − xj,r3);
j = (j + 1) % D;
} while (randj[0, 1] < CR ∧ j ̸= jr)
while (j ̸= jr) {
uj = xj,i;
j = (j + 1) % D;
}
(3)
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
Randomly generate xi ∈ P (i = 0, · · · , NP − 1);
for (i = 0; i < NP ; i + +) Evaluate f(xi);
for (g = 0; g < GM; g + +) {
for (i = 0; i < NP ; i + +) {
Generate u from (3) and (4);
Evaluate f(u);
if (f(u) ≤ f(xi)) {
xi = u;
f(xi) = f(u);
}
}
}
Output the best xi ∈ P;
Figure 1.
Pseudocode of SDE
If an element uj of u comes out of the range [xj, xj] by
using the strategy in (3), it is returned to the range as
uj = (xj − xj) · randj[0, 1] + xj.
(4)
C. Procedure of SDE
Figure 1 shows the pseudocode of SDE. The stopping
condition is speciﬁed by the maximum number of genera-
tions GM. Since SDE is based on the steady-state model,
only one population P is used. If a newborn trial vector u
is not worse than the target vector xi ∈ P, the target vector
xi is replaced by u immediately. Therefore, the excellent
trial vector u can be used soon to generate offspring.
III. CONCURRENT DE (CDE)
A program is said to be concurrent if it can support two
or more tasks in process at the same time. On the other
hand, a program is said to be parallel if it can support two
or more tasks executing simultaneously [8]. The difference
between these deﬁnitions is the phrase in progress. The
concurrent program performs multiple tasks in parallel if
it runs on a multi-core CPU. Therefore, it can be expected
that the execution time of an algorithm is reduced by using
the concurrent program on the multi-core CPU.
The concurrent program of SDE is named Concurrent
DE (CDE) [9]. Figure 2 shows the pseudocode of CDE.
CDE employs a static allocation of tasks [8]. Therefore,
the population P is divided equally into NT chunks. Each
chunk can be regarded as a subpopulation including N P /NT
individuals. Then a task updating the individuals in one
chunk is assigned to one thread statically. By using a multi-
core CPU, one thread is executed by one core. Consequently,
NT tasks can be parallelized at the most by CDE.
CDE is based on MapReduce framework [11]. First of
all, an initial population P is generated randomly. Then, in
49
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-172-4
ADVCOMP 2011 : The Fifth International Conference on Advanced Engineering Computing and Applications in Sciences

⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
Randomly generate xi ∈ P (i = 0, · · · , NP − 1);
// Map phase
for all n in parallel do {
for (n = 0; n < NT ; n + +) Thread(n);
}
Barrier();
// Reduce phase
Output the best xi ∈ P;
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
Thread(n) {
for (i = 0; i < NP ; i + +) {
if (i % NT == n) {
Evaluate f(xi);
}
}
for (g = 0; g < GM; g + +) {
for (i = 0; i < NP ; i + +) {
if (i % NT == n) {
Generate u from (3) and (4);
Evaluate f(u);
if (f(u) ≤ f(xi)) {
xi = u;
f(xi) = f(u);
}
}
}
}
}
Figure 2.
Pseudocode of CDE
Map phase, Thread(n) (n = 0, · · · , NT − 1) are evoked.
Each Thread(n) method is assigned to one thread and
contracts a task for updating all individuals included in
one chunk. Barrier() denotes the method that waits until
all Thread(n) methods have been completed. Finally, in
Reduce phase, the best individual is selected from P.
IV. UNCERTAIN OPTIMIZATION PROBLEM
In many real-world optimization problems, a wide range
of uncertainties have to be taken into account. Therefore,
various evolutionary algorithms for solving uncertain op-
timization problems have received increasing attention in
recent years [12]–[14]. Generally, uncertain optimization
problems can be categorized into four classes. First, the ob-
jective function is noisy. Second, the decision variables may
change after optimization, and the quality of the obtained
optimal solutions should be robust against deviations from
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
Thread(n) {
for (i = 0; i < NP ; i + +) {
if (i % NT == n) {
Evaluate F(xi, N);
Evaluate D(xi, N);
}
}
for (g = 0; g < GM; g + +) {
for (i = 0; i < NP ; i + +) {
if (i % NT == n) {
Generate u from (3) and (4);
Evaluate f(u);
if (f(u) ≤ F(xi, N) + α · D(xi, N)) {
Evaluate F(u, N);
if (F(u, N) ≤ F(xi, N)) {
xi = u;
F(xi, N) = F(u, N);
Evaluate D(u, N);
D(xi, N) = D(u, N);
}
}
}
}
}
}
Figure 3.
Pseudocode of CDEU
the optimal point. Third, the objective function is approxi-
mated, which means that the objective function suffers from
approximation errors. Fourth, the optimum of the problem
to be solved changes over time. In this paper, the ﬁrst and
the second classes of optimization problems are discussed.
A. Noisy Optimization Problem
The evaluation of the objective function is subject to
noise. Noise in the evaluation of the objective function may
come from many different sources such as sensory mea-
surement errors or randomized simulations. Mathematically,
a noisy objective function can be described as follows:
F(x) =
 ∞
−∞
(f(x) + δ) p(δ) dδ
(5)
where, x is a vector of decision variables and f(x) is a time-
invariant objective function considered in (1). δ is additive
noise and p(δ) is the continuous density function of the noise
δ. The noise δ is often assumed to be normally distributed
with mean 0 and variance 1 such as δ = N(0, 1).
50
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-172-4
ADVCOMP 2011 : The Fifth International Conference on Advanced Engineering Computing and Applications in Sciences

In practice, the noisy objective function deﬁned in (5) is
approximated using Monte Carlo integration as
F(x, N) = 1
N
N

t=1
(f(x) + δt)
(6)
where, N is the number of random samples. The noise is
given as follows: δt = N(0, 1) (t = 1, · · · , N).
B. Robust Optimization Problem
The decision variables are subject to perturbations or
changes after the optimal solution has been determined.
Therefore, a common requirement is that a solution should
still work satisfactorily when the decision variables change
slightly. Such solutions are termed robust solutions. In order
to search for robust solutions, a robust objective function can
be described by using a vector of noises δ as follows:
F(x) =
 ∞
−∞
f(x + δ) p(δ) dδ
(7)
where, δ = (δ0, · · · , δj, · · · , δD), δj = N(0, 1).
In practice, the robust objective function deﬁned in (7) is
also approximated using Monte Carlo integration as
F(x, N) = 1
N
N

t=1
f(x + δt)
(8)
where, δt = (δ0,t, · · · , δj,t, · · · , δD,t), δj,t = N(0, 1).
V. CDE FOR UNCERTAIN OPTIMIZATION (CDEU)
CDE can be applied directly to uncertain optimization
problems if the objective function f(x) is replaced by the
uncertain one F(x, N) deﬁned by (6) or (8). However, in
order to solve uncertain optimization problems more effec-
tively, we propose a revised CDE. The revised CDE is called
CDE for Uncertain optimization (CDEU). The procedure of
CDEU is described in Figure 3. Since CDEU differs from
CDE in Thread(n) part, only Thread(n) is shown in
Figure 3. α is a control parameter. Besides, D(x, N) denotes
a standard deviation of N sampled objective function values,
namely f(x) + δt or f(x + δt) (t = 1, · · · , N).
In order to reduce the evaluation time of the expensive
F(u, N), CDEU prunes away the search for unpromising
trial vectors estimated by using the time-invariant function
f(u). Even though the values of f(u) and D(u, N) are
required for CDEU, N sampled objective function values,
f(x) + δt or f(x + δt), evaluated for calculating F(u, N)
can be reused for calculating D(u, N) again.
VI. EXPERIMENT
Through the numerical experiment conducted on two
classes of uncertain optimization problems, the performance
of the proposed CDEU is compared with that of CDE.
A. Test Function
For evaluating the performance of CDEU, the following
four test functions are used as time-invariant ones. All the
test functions have D = 20 dimensional real-parameters.
• Sphere function
f1(x) =
D−1

j=0
x2
j
− 100 ≤ xj ≤ 100, j = 0, · · · , D − 1.
• Schwefel’s function
f2(x) =
D−1

j=0
(
j

k=0
xk)2
− 100 ≤ xj ≤ 100, j = 0, · · · , D − 1.
• Rastrigin function
f3(x) =
D−1

j=0
(x2
j − 10 cos(2 π xj) + 10)
− 5.12 ≤ xj ≤ 5.12, j = 0, · · · , D − 1.
• Griewank function
f4(x) =
1
4000
D−1

j=0
x2
j −
D−1

j=0
cos
	 xj
√j

+ 1
− 600 ≤ xj ≤ 600, j = 0, · · · , D − 1.
B. Performance Criteria
In order to evaluate the performance of a parallel program
for a multi-processor machine, the speedup is often used.
The well-known deﬁnition of the speedup is the ratio of
serial execution time to parallel execution time. However,
in the assessment of a concurrent program, the conventional
deﬁnition of the speedup has to be modiﬁed [8]. In the multi-
core CPU, the executions of threads, which are evoked from
a concurrent program, are assigned to respective cores au-
tomatically by operating system. Although the programmer
can specify the number of threads in his program, he can
not specify the number of cores used by his program.
Furthermore, in order to evaluate the performance of
evolutionary algorithms such as CDE and CDEU, a single
execution of them is not statistically signiﬁcant. That is
because the evolutionary algorithm is a stochastic algorithm.
Therefore, a new speedup is deﬁned as follows:
S(NT , M) =
M

m=1
Tm(1)
Tm(NT )
(9)
where, Tm(NT ) (m = 1, · · · , M) is the execution time of
CDE or CDEU achieved by using NT threads. In an ideal
situation, speedup is equal to the number of threads.
51
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-172-4
ADVCOMP 2011 : The Fifth International Conference on Advanced Engineering Computing and Applications in Sciences

Table I
NOISY OBJECTIVE FUNCTION VALUE
Fp
F1
F2
F3
F4
CDEU
−0.242
−0.176
2.244
0.759
(0.024)
(0.038)
(1.165)
(0.034)
CDE
−0.284
−0.194
2.534
0.701
(0.024)
(0.034)
(1.439)
(0.030)
Table II
EXECUTION TIME FOR NOISY OPTIMIZATION PROBLEM
(a) Sphere function: F1
NT
1
2
4
6
8
CDEU
201
105
59
46
36
(10.8)
(8.3)
(10.6)
(5.0)
(7.5)
CDE
1085
565
306
244
189
(57.4)
(35.5)
(15.1)
(8.8)
(5.5)
(b) Schwefel’s function: F2
NT
1
2
4
6
8
CDEU
216
117
64
52
43
(9.1)
(7.8)
(5.5)
(7.4)
(6.9)
CDE
1116
579
315
255
193
(55.5)
(36.6)
(18.8)
(8.9)
(10.4)
(c) Rastrigin function: F3
NT
1
2
4
6
8
CDEU
262
136
74
62
49
(11.6)
(10.8)
(6.8)
(8.4)
(5.6)
CDE
1185
614
329
265
208
(55.0)
(31.3)
(19.1)
(7.0)
(10.2)
(d) Griewank function: F4
NT
1
2
4
6
8
CDEU
285
149
79
65
53
(7.2)
(10.4)
(4.7)
(7.9)
(7.6)
CDE
1198
619
333
271
210
(56.0)
(32.5)
(19.3)
(10.3)
(12.6)
C. Experimental Setup
CDEU and CDE were coded using the Java language,
i.e., a popular language supporting multiple threads, and
executed on a personal computer equipped with a multi-core
CPU (CPU: Intel® CoreTMi7 @3.33[GHz]; OS: Microsoft
Windows XP). The multi-core CPU has four cores that can
respectively manipulate two threads at one time.
CDEU and CDE were applied respectively to some
instances of uncertain optimization problems. During the
experiments, the control parameters of them were ﬁxed: the
population size NP = 96, the scale factor SF = 0.5 and
the crossover rate CR = 0.9. For the stopping condition, the
maximum number of generations was ﬁxed to G M = 103.
Besides, α = 0.1 is used for CDEU in all instances.
D. Result in Noisy Optimization Problem
CDEU and CDE were applied to four noisy optimization
problems, in which the noisy objective functions Fp(x, N)
were deﬁned by (6) using the above test functions f p(x).
The number of random samples was speciﬁed as N = 100.
Table III
ROBUST OBJECTIVE FUNCTION VALUE
Fp
F1
F2
F3
F4
CDEU
18.21
146.6
209.9
0.793
(0.159)
(6.191)
(0.856)
(0.006)
CDE
18.20
145.1
210.5
0.791
(0.172)
(4.830)
(0.814)
(0.004)
Table IV
EXECUTION TIME FOR ROBUST OPTIMIZATION PROBLEM
(a) Sphere function: F1
NT
1
2
4
6
8
CDEU
17857
9016
4601
3908
2963
(127.6)
(125.1)
(132.7)
(71.7)
(75.1)
CDE
19516
9814
5009
4185
3190
(132.8)
(139.6)
(138.6)
(84.1)
(70.5)
(b) Schwefel’s function: F2
NT
1
2
4
6
8
CDEU
13550
6870
3535
3195
2339
(289.1)
(155.9)
(127.9)
(90.0)
(82.8)
CDE
22434
11280
5795
4949
3772
(119.9)
(120.6)
(126.0)
(78.0)
(61.3)
(c) Rastrigin function: F3
NT
1
2
4
6
8
CDEU
24426
12295
6340
5612
4328
(198.2)
(161.6)
(133.0)
(81.6)
(48.9)
CDE
30424
15253
7858
6801
5191
(108.6)
(91.7)
(152.1)
(61.6)
(54.1)
(d) Griewank function: F4
NT
1
2
4
6
8
CDEU
24057
12103
6171
5297
3912
(308.2)
(222.9)
(175.4)
(95.6)
(72.2)
CDE
28688
14443
7320
6062
4623
(90.3)
(220.7)
(90.7)
(60.1)
(58.8)
Table I shows the smallest noisy objective function values
obtained by CDEU and CDE, which are averaged over M =
20 runs. The standard deviations of them also appear in
parentheses. Similarly, Table II shows the execution times
[ms] of CDEU and CDE using NT threads. From Table I
and Table II, CDEU has reduced the computational time
without losing the quality of obtained solutions.
Figure 4 plots the speedups of CDEU achieved with NT
threads in the four noisy optimization problems. Similarly,
Figure 5 plots the speedups of CDE. From Figure 4 and
Figure 5, the speedups of CDEU and CDE have increased
apparently with the increase in the number of threads.
E. Result in Robust Optimization Problem
CDEU and CDE were applied to four robust optimization
problems in the same way with the noisy optimization
problems. Table III compares the smallest robust objective
function values, while Table IV shows the execution times.
Figure 6 and Figure 7 also plot the speedups of CDEU and
CDE achieved in the four robust optimization problems.
52
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-172-4
ADVCOMP 2011 : The Fifth International Conference on Advanced Engineering Computing and Applications in Sciences

 0
 1
 2
 3
 4
 5
 6
 7
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
number of threads
speedup
Figure 4.
Speedups of CDEU in noisy optimization problems
speedup
 0
 1
 2
 3
 4
 5
 6
 7
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
number of threads
Figure 5.
Speedups of CDE in noisy optimization problems
 0
 1
 2
 3
 4
 5
 6
 7
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
number of threads
speedup
Figure 6.
Speedups of CDEU in robust optimization problems
 0
 1
 2
 3
 4
 5
 6
 7
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
number of threads
speedup
Figure 7.
Speedups of CDE in robust optimization problems
VII. CONCLUSION
CDEU was proposed for the noisy and robust optimization
problems. From the results of experiments, it was shown that
CDEU could reduce the computational time comparing with
CDE. Furthermore, CDEU could achieve the high speedup
and the quality of obtained solutions as well as CDE.
In our future work, we would like to apply CDEU to
real-world applications with various uncertainties.
ACKNOWLEDGMENT
The research was supported in part by the Grant-in-Aid
for Scientiﬁc Research (C) (No. 21560432) from JSPS.
REFERENCES
[1] R. Storn and K. Price, “Differential evolution - a simple
and efﬁcient heuristic for global optimization over contin-
uous space,” Journal of Global Optimization, vol. 11, no. 4,
pp. 341–359, 1997.
[2] S. Das and P. N. Suganthan, “Differential evolution: a survey
of the state-of-the art,” IEEE Trans. on Evolutionary Compu-
tation, vol. 15, no. 1, pp. 4–31, 2011.
[3] J. Vesterstrom and R. Thomson, “A comparative study of
differential evolution, particle swarm optimization, and evolu-
tionary algorithms on numerical benchmark problems,” Proc.
of IEEE Congress on Evolutionary Computation, pp. 1980–
1987, 2004.
[4] K. V. Price, R. M. Storn, and J. A. Lampinen, Differential
Evolution - A Practical Approach to Global Optimization,
Springer, 2005.
[5] G. Syswerda, “A study of reproduction in generational and
steady-state genetic algorithms,” Foundations of Genetic Al-
gorithms 2, Morgan Kaufmann Publ., pp. 94–101, 1991.
[6] V. Feoktistov, Differential Evolution in Search Solutions,
Chapter 6, Springer, 2006.
[7] K. Tagawa, “A statistical study of the differential evolu-
tion based on continuous generation model,” Proc. of IEEE
Congress on Evolutionary Computation, pp. 2614–2621,
2009.
[8] C. Breshears, The Art of Concurrency - A Thread Monkey’s
Guide to Writing Parallel Applications, O’Reilly, 2009.
[9] K. Tagawa and T. Ishimizu, “Concurrent implementation of
differential evolution,” Proc. of WSEAS Int. Conference on
Mew Aspects of System Theory and Scientiﬁc Computation,
pp. 65–70, 2010.
[10] B. D. Davison and K. Rasheed, “Effect of global parallelism
on a steady state GA,” Evolutionary Computation and Parallel
Processing Workshop, Proc. of Genetic and Evolutionary
Computation Conference Workshops, pp. 167–170, 1999.
[11] J. Dean and S Ghemawat, “MapReduce: simpliﬁed data
processing on large clusters,” Proc. of 6th Symposium on
Operating Systems Design and Implementation, pp. 137–149,
2010.
[12] Y. Jin and J. Branke, “Evolutionary optimization in uncertain
environments – a survey,” IEEE Trans. on Evolutionary
Computation, vol. 9, no. 3, pp. 303–317, 2005.
[13] S. Das, A. Konar, and U. K. Chakraborty, “Improved differ-
ential evolution algorithms for handling noisy optimization
problems,” Proc. of IEEE Congress on Evolutionary Compu-
tation, vol. 2, pp. 1691–1698, 2005.
[14] H. G. Beyer and B. Sendhoff, “Evolution strategies for
robust optimization,” Proc. of IEEE Congress on Evolutionary
Computation, pp. 1346–1353, 2006.
53
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-172-4
ADVCOMP 2011 : The Fifth International Conference on Advanced Engineering Computing and Applications in Sciences

