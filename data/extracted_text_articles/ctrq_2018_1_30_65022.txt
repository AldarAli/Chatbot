A Survey of Internet Protocol and Architectures in the Context of Emerging
Technologies
Kiran Makhijani, Renwei Li, Alexander Clemm, Uma Chanduri, Yigzhen Qu, Lin Han
Future Networks, America Research Center
Huawei Technologies Inc., Santa Clara, CA, USA
email: {kiran.makhijani, renwei.li, alexander.clemm, uma.chunduri, yingzhen.qu, lin.han}@huawei.com
Abstract—The Internet technologies need an overhaul to sup-
port next-generation of applications requiring communications
between machines and humans. This paper is a survey of the
state of current internetworking architecture and its engineering
properties. The purpose of this paper is to highlight the aging of
original design goals and motivations. We aim to formulate a new
set of guidelines that maybe used to postulate design principles
of the new network architectures.
Keywords–Internet architecture, Internet Protocol, Routing,
Switching; Ossiﬁcation, layering.
I.
INTRODUCTION
The Internet has grown remarkably since its foundational
work was published as A Protocol for Packet Network In-
tercommunication [1]. This speciﬁcation was developed into
Transmission Control Protocol/Internet Protcol (TCP/IP) in
compliance with the Internet design principles [2]. While
the Internet has proven to scale and support diverse set of
applications and users, the more recent technological advance-
ments such as Machine to Machine (M2M) communications,
connected or live Augmented Reality/Virtual Reality (AR/VR),
Vehicle to Anything (V2X) communications etc., impose new
requirements on connectivity that did not exist before. The
applications based on these technologies are far more stringent
about both network resource constraints and packet delivery
guarantees. The current architecture lacks several artifacts
to guarantee support for real-time, low latency and reliable
services. In this regard, several new network architectures have
been proposed with different motivations; however, none of
them have been attentive to strict quality of service constraints.
In this paper, we systematically analyse effects of current
architectural and engineering design choices (both adversely
and favorably) that can be used to understand speciﬁc gaps in
the context of emergent applications. These effects are identi-
ﬁed as: 1) the commercial effect, 2) layering, 3) addressing, 4)
Ossiﬁcation, and 5) services; They will be discussed in detail to
highlight their inﬂuence and stronghold on the current state of
the Internet. In this study we show that the current principles
of inter-networking are not sustainable to serve applications
built for the use of emerging technologies. The paper further
aims to achieve the following:
(a)
1)
Brieﬂy describe use cases catogarized as emerging
applications.
2)
Provide an analysis of original design principles and
corresponding engineering effects.
3)
Guidelines to be taken under consideration when
designing new or evolving the current Internet archi-
tecture.
The paper is organized as follows: Section II brieﬂy men-
tions future network architectures related work, while Section
III starts with the background and motivation for this paper,
in Section IV we analyse the original concept and design
goals of Internet architecture. Section V is a discussion on the
engineering effects of the Internet and their analysis in context
of emerging applications. Section VI proposes properties to be
taken in to account for designing new internet architecture. In
Section VII, we expose the factors that will drive the need for
new Internet architectures. We conclude with a summary of
this survey in Section VIII.
II.
RELATED WORK
This paper primarily analyses several published works of
Kahn, Cerf and Clark. Their insights and reﬂections on the
design of the Internet have been taken into consideration in
the context when analysing the current state of Internet.
The discussion for new architecture has come up several
times. In fact, immediately following the Internet impact,
guidelines for the future network architecture were produced
in RFC1287 [3]. It revealed several interesting shortcomings
relating to addresses, multi-protocol architectures, trafﬁc con-
trol and security. It also mentioned that service awareness
was necessary in general and speciﬁcally for voice, video and
teleconferencing type of applications.
There has been continuous effort in building next gen-
eration internet encompassing from evolutionary to clean-
slate approaches [4]. More recently, some of the large-
scale future internet initiatives eXpressive Internet Architec-
ture (XIA), Future Internet Research and Experimentation
(FIRE), Named Deﬁned Networks (NDN), Software Deﬁned
Networking (SDN), etc. [5] have been proposed to solve
known problems. None of these initiatives can be qualiﬁed
as either failed or successful projects since they did not
get deployed and tested in live environments. In principle,
the network community understands a need to upgrade the
Internet architecture and design, however, none of the efforts
have been able to stir a serious interest from commercial
sector. Several federated and national initiatives such as Future
Internet Architecture (FIA) [6], 4WARD [7], AKARI [8],
Study Group 13, Future Networks (SG13) [9] and many more
do not transition from research to commercial mainstream even
14
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

after having undergone thorough experimentation (FIRE [10],
Global Environment for Network Innovations (GENI) [11]).
An obvious reason is the growth in Internet and its ability
to absorb many motivations of new architectures. Another
possible reason may be that the new solutions focus on
a particular problem-domain instead of taking the holistic
approach along the lines of design principles. Our contribution
focuses on support for communication aspects of current and
future technological advances in medicine, manufacturing, city
planning and automating vehicles etc as a driver to review
current Internet design.
III.
BACKGROUND
Clark’s Internet design philosophy serves as the guiding
principles of the Internet architecture [2]. According to Kahn
[12], the reference architecture and TCP/IP as an implementa-
tion are often used interchangeably, but that was not the intent.
The reference design of the Internet was a logical framework
for interconnection of independent networks and TCP/IP is
one such instance that implemented it. Kahn also admits that
the reference architecture itself does not assume the idea of
linking different networks together will result into a single
system. The vision was to foster multiple implementations
serving different systems from the same abstract architecture.
With the TCP/IP, this generality of the design was lost which
prevents the evolution of Internet from its current state without
disruption [4]. The TCP/IP resists change and on-boarding new
services to support new applications is a difﬁcult task.
Traditionally, services with special constraints in networks
concern with delivery of data through Quality Of Service
(QoS) parameters that are represented by coarse grained means
of allocating network resources (e.g., buffers and bandwidth)
using code points [13] [14] on per hop or end-to-end [15] basis.
For example, a service characteristic such as lower latency
may be marked to code-point that indicate ’real-time’ trafﬁc.
In contrast, the emergent services for scenarios such as M2M,
V2X, AR/VR communications are associated with extremely
strict resource constraints and absolute guarantees of QoS
in the network. For example, industrial automation relies on
M2M communication to achieve reliable interaction between
different type of machines with a ﬁne-grained granularity in
delay variation. Any failure to deliver data in precise time-
interval could cause machines go into stall mode halting the
over all production. Similarly, in V2X scenarios, the infrastruc-
ture should be able to gather live information from multiple
sources such as approaching signals, road conditions, and other
vehicles to make real-time decisions about public safety and
streamlining trafﬁc ﬂow; while ensuring the decision is fed
to an autonomous vehicle instantaneously; any delay makes
information stale and unusable. Rest of the paper collectively
refers to these use cases as emerging applications.
IV.
FOUNDATIONS OF THE INTERNET ARCHITECTURE
The primary goal of the reference internet architecture is
to provide an effective technique to multiplex packet switched
data over interconnected networks. There were seven addi-
tional goals (see [2]) that had to be met at the time of the inter-
networking design. While these design principles are generally
accepted, as times change and technologies evolve, some of the
original principles cannot be followed as is.
The ﬁrst goal, ’Internet should continue to provide com-
munication service...’ is about suvivability and fate-sharing. In
networks, fate-sharing suggests that it is acceptable to lose the
state information of an entity, if the entity itself is lost. This
principle entirely takes away the responsibility of reliability in
the network which will require some knowledge of relevant
state.
There is an indirect consequence of this principle, that the
network is stateless with no knowledge besides forwarding
information of an entity. While, it is true that maintaining
an overall state of all the sessions in the Internet is un-
maintainable; there are speciﬁc scenarios where it provides
resilience, robustness through faster recovery and security.
There is also a question of what determines that an entity is
lost. Whether a session was withdrawn gracefully or due to
failure such as congestion or packet loss in the network can
not be determined by the network itself. In industrial interent,
M2M communication scenarios require bounded latency and
are sensitive to delays, such connections beneﬁt from having
state in the network. Relaxing this fate-sharing principle will
help determine fate of an entity. In Internet of Things (IoT)
communications entities would goto sleep mode but may still
have associated active state in the network for high reliability
scenarios. Therefore, future internet design goal may consider
fate-sharing to be optional or need-basis; Certain type of
services, such as those requiring zero packet loss, in-network
stateful buffering can help trigger retransmissions from a
nearer hop without involving end hosts.
Additionally, it is noted that the statelessness is already
diminishing in the Internet to a certain extent due to ever-
growing use of middle boxes that are largely stateful. The
middle boxes are generally considered to compromise network
transparency and break End-to-End (E2E) principle. Yet, in
practice they bring a lot of value to commercial enterprises by
performing Network Address Translation (NAT), ﬁrewall and
similar functions.
The second goal it should support, at the transport service
level, a variety of types of services manifested in to not making
any underlying assumption about the services in the datagrams.
Unfortunately, this behavior does not translate well in TCP/IP.
In the context of telecommunications and data communications
convergence, voice service in a telecom network outperforms
Voice over IP (VoIP). Support for real-time applications need-
ing low latency still cannot be assured. This is due to lack of
service awareness about the packets as it is transmitted through
the network. Clark had a broader view about the structure of
datagrams as building blocks that provide pieces of information
about services and corresponding resource requirements in
such a manner that each datagram is a self-describing con-
struct. This behavior rightfully, was too complex for that time
and did not make it to TCP/IP. While Type Of Service (TOS)
in IP is available, it is a) too generalized for emerging services
characterization, b) in practice, the interpretation and scope is
always within an internal network and has no signiﬁcance in
internetworking.
The third and seventh goals, ’the architecture must permit
distributed management of its resources’ and ’it must be
accountable’ are somewhat related to the cost. Distributed
management of network resources is realized through control
plane protocols. In this regard, the composition of services and
15
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

allocation of network resources, has been a difﬁcult problem.
This is because of the trusted domain concept and estab-
lishment of trust between transit networks happens outside
the
Internet Protocol (IP). In the absence of seventh goal
(accounting), there is no distributed way to convey explicit
business value in datagrams to obtain resources from transiting
networks. It would have been a simpler problem to solve if the
structure of datagram had permitted for presence of accounting
information. Hence, the third and seventh goals of original
design have remained unfulﬁlled.
The fourth goal, architecture must be cost effective pertains
to inefﬁciencies in packet transmission that are incurred either
due to header overheads or retransmissions. In the context of
IoT type of devices, large header related inefﬁciencies become
even more prominent and are handled through header compres-
sion schemes [16]. Back then (1980s), a retransmission rate
of 1 in 100 was tolerable. However, it is now unacceptable
for AR/VR applications that tolerate loss of 1 in 10000
packets [17], using current TCP throughput computations for
a 15200 Mbps stream with delay tolerance of 0.106 ms. For
such applications, retransmissions and packet loss have to be
absolutely avoided with the assumptions that end users are
willing to incur the cost of sych services.
Essentially, many of Clark’s goals are not sufﬁcient to meet
present-time service requirements as discussed in this section
and a reformation of the design principles are necessary. This
can possibly be achieved by reviving the concept of data-
grams carrying relevant information for use in the networks.
Emerging applications are in need of in-network state, service
awareness and resource control. The cost effectiveness varies
for different application environments and business demands.
To this effect Internet being cost-effective cannot be a funda-
mental design goal and applications should have choice to opt
for premium services.
V.
ENGINEERING INERTIAL EFFECTS
Over multiple decades, a lot of engineering effort has gone
into keeping the Internet stable and allowing it to scale. The
resulting Internet is rigid, that resists changes necessary in the
context of a wide variety of modern applications. The structure
of the present day Internet can be described through a set of
inertial effects since they provide means to maintain status quo
while avoiding substantial changes. An exploration of these
effects will reveal the trade-offs between their strengths and
shortcomings which may further help design next-generation
architectures.
A. Commercial Effect
Commercial aspect of the Internet manifested into three
characteristics viz. explosion of routing table, proliferation of
private networks through tunnels, and a surge in non-default
forwarding of the trafﬁc.
Firstly, as new websites or corporate sites are added,
replaced or merged, the global routing table is affected and
often misconﬁgured routes lead to the instability in Internet
backbones. This can become a cause of major outages on regu-
lar basis [18]. To minimize global routing updates, techniques
like damping [19] are employed. However, this method has
limitations such as loss of connectivity due to suppression of
correct updates, missing routes and the conﬁguration complex-
ity [20].
Secondly, commercialization also created a business case
for multi-site private networks. It became necessary for any
corporation to isolate and protect its digital assets when
traversing through the public Internet. Interestingly, the orig-
inal TCP/IP design was a single system with no notion
of network-to-network communication. To implement private
networks tunnels are deployed emulating a network as a
host (through a tunnel endpoint). It then requires a complex
instrumentation and an entirely independent stack of protocols
[21].
Thirdly, the measurement studies in [22] and related work
[23] discovered E2E path anomalies, i.e., not all packets
between the same source and destination were subjected to the
same path (non-default routing). This is because various com-
mercial features and business reasons have different service
requirements that are fulﬁlled by operator driven conﬁgurations
and/or route-policies.
The inertial aspect of this effect is that the current archi-
tecture implicitly resists change of any kind in favor stability,
overrides the notion of single system (through Virtual Private
Networks (VPN)s) and overrides default routing. A variety of
services requiring real-time, high bandwidth, zero packet loss
etc. resort to tricks such as path computations, route policies
and complex conﬁgurations just to get close to meeting their
service level objectives. They take away the dynamic nature
of forwarding which is not desirable traits of emerging appli-
cations where seamless connectivity and ubiquitous mobility
are essential requirements.
B. Layering and E2E Effect
The layering principle is honored when a) separation of
layers is not compromised or violated (layer independence), b)
there exists minimal layer crossing (services provided to next
higher layer only). The E2E principle creates transparency;
i.e., the network bears no knowledge of the contents and
remains non-discriminant about the applications. Both layering
and E2E principles are the foundation of the Internet and a
consequence of how TCP/IP got implemented.
Layering gets breached in the form of tunnels, overlays,
NATs, etc. through port blocking or ﬁltering techniques. It
is well known that many in-production routers do not allow
trafﬁc other than TCP and UDP [24] to pass through. This is an
obvious violation but is necessary for Internet Service Provider
(ISP)s to protect their network against spurious attacks. Lay-
ering helps scale different type of services but there are a few
drawbacks as well. Firstly, multiple levels of encapsulations
lead to bloating of the proverbial narrow waist in hourglass
structure of TCP/IP. Secondly, the encapsulated layer comes
with its own protocol, control and corresponding management
entities, thereby increasing network complexity.
From an architectural standpoint, layer abstraction is a
powerful concept, but in practice, it has resulted into a mecha-
nism to hide deﬁciencies in the structure of datagrams that do
not carry sufﬁcient control information. Similarly, E2E effect
has manifested into dumb networks and intelligent end points
making is impossible for networks to make informed decisions
unless middleboxes are deployed.
16
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

C. Network Ossiﬁcation Effect
Ossiﬁcation suggests both long-term survivability and as a
consequence rigidity in the network. It is believed that both
network [25] and transport [26] layers resist adoption of new
technologies. The ossiﬁcation is a consequence of gradual
building of resiliency and stability in TCP/IP technology over
a long period of time (also alluded to in Section V-A).
SDN has been positioned to mitigate effects of ossiﬁcation
and has produced several changes in network control through
programmability. However, SDN also brings an increased
complexity and scalability limitations due to central control for
programming the networks. In contrast, transport ossiﬁcation
is mainly a side-effect of use of middle boxes to bypass lack of
modularity in transport layer for service customizations. In an
E2E client server communication, any change to TCP, needs
coordination with all client instances. Over time, the structural
uniformity has become more rigid and most customizations
happen over HTTP instead (e.g., DASH, session management).
The ossiﬁcation of both network and transport layer implies
they cannot be changed. SDN only deals with the control plane
programmability, however, dataplane ﬂexibility is extremely
desirable for M2M communications requiring low latencies.
The emerging applications scenarios are exactly the kind
where application level session management will be inefﬁcient
and impractical, instead a network assisted packet processing
techniques will be necessary.
D. Addressing Effect
The Internet is ubiquitous and homogeneous because of
a uniform network addressing scheme in which a host is
understood in identical manner in each network. There are two
major factors regarding host addresses. First factor pertained
to the size; it was recognized that the 32-bit width will be
too small to cover every host on the Internet [3] [27]. Second
factor related to its structure limitations; that the early binding
aspects of an application and address turned out to be limiting
the functions of mobility, device portability and multi-homing.
This is also characterzed as location and identiﬁer separation
concept.
E.
Services Effect
Even though variety of services were anticipated in the
architecture, the earlier ones were primarily texts or static dig-
ital image formats. With digitization of audio and video, many
new applications started to emerge and Internet was suited for
many of those. For example, early attempts to implement VoIP,
which is a circuit-switched telecommunication service, over a
packet switched network were suboptimal, because the goal
of the internet was to support best-effort communication, but
voice performs the best as a circuit switched application. The
QoS markings that exists today are coarse-grained, therefore,
only a very narrow category of services can be supported.
Today, there are even more variety of such services but
with even more stringent requirements. M2M communications
take humans out of the loop; an application relies on each
machine-entity to function properly, respond, generate and
process events according to prescribed behavior. Any delay,
loss of packets in network could be misinterpreted as failures,
causing system to take serious recovery actions leading to loss
of productivity.
F. Summary of Inertial Effects
The runtime state of the Internet is a consequence of above
mentioned effects that emerged from the TCP/IP implemen-
tation. Due to limitation of space, and non-technical aspects
that vary for different countries, we do not discuss governance
aspect here; however, it is also relevant in shaping the Internet.
For interested reader we offer the following references [28]
[29].
The Internet is ’robust yet fragile’ [30]. It is well-
engineered in responding to predicable events, but unable
to handle unanticipated circumstances. Layering, addressing
and commercialization effects are virtue of the principle of
keeping network layer simple. The mechanisms adopted were
mainly based on conservative and cost-effective design choices
with the goal of scaling the Internet. Ironically, this has
brought comlpexity elsewhere in management, operations and
orchestration functions of the networks [31] [32]. In constrast,
Ossiﬁcation and services effects aimed to minimize variations
in the network thereby lacking customization mechanisms that
are needed for ﬁner granularity of control for certain classes
of applications.
The TCP/IP is an over simpliﬁed instance of the original
design and trade-offs made several decades ago were well-
suited for applications of that time. Not only that the state of
art routers and network nodes are more powerful now but the
networks play a much bigger role in all aspects. The emerging
applications driven by M2M communications will expose the
above mentioned limitation even further. A new balance has to
be struck between preserving the stability aspect of the Internet
and yet allow it to evolve for those applications.
VI.
PROPERTIES FOR TRANSITION FROM CLASSICAL TO
NEW INTERNET
We have discussed the architectural aging and engineering
effects in previous sections and call the structure as classical
Internet.
The Internet is diverifying in all facets, a new Internet
architecture must be deﬁned to be simultaneously public and
private, secure and open, social and commercial as well as both
human and machine centric. In the context of discussions in
previous sections, the properties for new network architecture
are proposed.
A. Multi-Instance Architecture
The idea of multi-instanced Internet should be explored.
Where an instance could be a special purpose and means to
connect with other instances if necessary. This could serve
new generation of technologies with speciﬁc type of network
resources better. This has already been noted as fragmented
Internet in [33] (as a warning, not a feature). Often general-
purpose solutions suffer from performance and complexity. In
contrast special purpose networks can be more efﬁcient but
limited. A single system is automatically prone to be rigid and
conservative, being a single point of failure. Having multiple
instances allow features to be experimented and withdrawn.
17
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

RFC1958 [27] discusses the possibility of at least two network
protocols to be in use to support gradual transition. Prevailing
encapsulation-based mechanisms suffer from bloating (Section
V-B). This approach could provide with ﬂexible and dynamic
bindings to information, in a tunnel-free manner.
B. Distributing Complexity
Section IV expains that the fate-sharing principle has led
to stateless design of the networks. The connectivity scenarios
are evolving rapidly as a large number of endhosts (e.g.,
wearables, sensors, appliances, etc.) are far less powerful than
the routers and switches. It makes sense for network to be
more aware of device behavior and the services they require
by distributing some of communication processing from end-
points into the networks. Several emerging applications (for
example, V2X, industrial automation and remote control etc.)
have strict service level criteria for normal operation in terms
of bounded latency or committed bandwidth. Without direct
sharing of such information, the network design becomes
inefﬁcient as an operator would need to understand require-
ments of each application and setup resources accordingly
through central control on per hop basis. Also this is possible
with SDN paradigm, maintenence of per ﬂow state is non-
trivial. In current architecture, the networks have evolved in a
manner that the intelligence lies with the applications or end
points. New design should identify mechanisms that distribute
intelligence into the networks, as in service awareness, runtime
state or behavior. This In effect, distributes complexity partially
from the end points in the network. Such considerations
are manadatory to meet service level objectives of absolute
guarantes .
Many technological advances have happened in the hard-
ware of network devices. The Network Processing Units
(NPU)s, Ternary Content Addressable Memories (TCAM)s
and port Application Speciﬁc Interface Circuit (ASIC)s are
much faster than before. New architecture can use advances
in hardware to their advantage while exploring solutions for
emerging applications.
C. Self-Sufﬁcing Datagrams
The datagram provides a basic building block out of which
a various types of service can be implemented. The notion
of a datagram carrying service-centric information can be an
extremely powerful concept to address several control and
management inefﬁciencies in network. A datagram should
be a self-sufﬁcient, self-describing entity comprising of user
payload and control information about the ﬂow or application
it is part of. Obviously, it comes at a cost of additional bits
on wire but a sensible structure and the framework could
be deployed. The information must be network centric and
should be detached from the transport aspects. As mentioned
before, hardware advancements can be used to deploy efﬁcient
processing in the networks.
D. Flexible Address Structure
While the uniformity of addresses need to be preserved
for the sake of reachability, a variable structure that is more
sensitive for IoT devices should be supported. This has already
been proposed in [34].
These are the main recommendations and can be used as
guiding principles for new architectures to make distinction
between the things in networks that should change (e.g.,
services, resources), and the things that provide stability ( e.g.,
uniformity of addresses and layering). Other guidelines are
possible based on speciﬁc choice of architectures but we only
mention the ones that can be added to any next-generation
architecture proposal.
VII.
FACTORS DRIVING THE NEED FOR NEW INTERNET
As mentioned earlier in Introduction, the need to change
is driven by applications. Had the communications remained
web-based online transactions or consuming streaming media,
the current Internet works just ﬁne. However, a ubiquity of
connectivity is emerging in several aspects. It is required to
think of the Internet as a fabric that interconnects humans,
services, sensors, devices etc.
It is well known that IoT space will grow to billions of
devices and each of them will have varying characteristics such
as identity (corresponding connectivity address and gateway),
its functionality (what purpose is it used for), energy efﬁciency
(to help determine right type of transport mechanisms) to state
a few. This high level of diversity is compounded by the
volume of data that is produced at varying intervals. As a
result current foundations of transport protocols do not apply
to IoT, new techniques to efﬁciently transfer information and
yet reducing or eliminating setup times are needed. 4
A fully automated vision of Industry 4.0 takes IoT to next
level in terms of M2M communications. Today manifacturing
networks are proprietory and purpose built. Looking ahead,
there are three factors that will drive integration of Industrial
network into mainstream Internet, a) combining Information
Technology (IT) and Operation Technology (OT), b) use of
common technologies, c) resource assurances. Even through
the manufacturing in a factory is automated, OT and IT are
managed as two separate networks. This requires a human to
integrate results from information technologies in to opera-
tions. To achieve complete automation, IT and OT must be
combined so that the results from complex analytics can be
fed into command center. Secondly, the investments in infras-
tructure can be reduced by using standard technologies, which
will not only incentivise manifactureres to automate at large
scale but also allow with modern cloud based infrastructure
solutions.
Inspite of the above compelling factors, it is a difﬁcult
task to change the incumbent Internet owing to its success.
Today, it is so big that even minor outages are unacceptable.
The Internet is IP based and most of the standardization
is driven by Internet Engineering Task Force (IETF). These
standards can only afford to bring segmented improvements in
a particular focus area such as operations, routing, transport
etc. Architectural changes related discussions often happen at
other Standards and Development Organization (SDO)s such
as European Telecommunications Standards Institute (ETSI),
International Telecommunications Union (ITU) involved in
study and evaluation of new network architectures. Perhaps
a close coordination among these SDOs will be necessary to
further the design of new architecture.
18
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

VIII.
CONCLUSION
In this paper, we surveyed two related topics; ﬁrst, the
design decisions that led to current network architecture and
were reasonable at that time. The second topic reﬂects upon the
consequences of ﬁrst in terms of its inertial effects that makes
it difﬁcult for the Internet to evolve from its current state. We
also establish that the foundations of Internet architecture have
been strong and were well engineered. However, in the context
of emerging applications and new types of communications,
some of the principles are outdated and must be revisited. This
can be achieved either through a new or evolved architectural
principles that balance both incumbant stability and adoption
of new features. Looking ahead at emerging applications,
Internet as a single system will be difﬁcult to scale, it is
simpler to evolve and adopt in multi instance environments.
Finally, to future-proof new architecture and design, datagram
building blocks are the key. They should be allowed to evolve,
be extensible and support ﬂexible mechansims for variety of
applications.
REFERENCES
[1]
V. Cerf and R. Kahn, “A protocol for packet network intercommunica-
tion,” IEEE Trans. Commun., vol. 22, pp. 637–648, May 1974.
[2]
D. D. Clark, “The Design Philosophy of the DARPA Internet Protocols,”
ACM Symposium proceedings on Communications architectures and
protocols, vol. 18, pp. 106–114, Aug. 1988.
[3]
D. D. Clark, L. Chapin, V. Cerf, R. Braden, and R. Hobby, “RFC
1287, Towards the Future Internet Architecture,” Internet Engineering
Task Force (IETF), Dec. 1991.
[4]
N. McKeown and B. Girod, “Clean-Slate Design for the Internet,
Whitepaper Version 2.0,” A Research Program at Stanford University,
2006.
[5]
J. Pan, S. Paul, and R. Jain, “A survey of the research on future internet
architectures,” IEEE Communications Magazine, vol. 49, p. 38, July
2011.
[6]
“NSF Future Internet Architecture Project.” http://www.nets-ﬁa.net,
2010. Last accessed 14 April 2018.
[7]
“EU,
Future
Internet
Initiative,
FP7
Project.”
”http://www.
4ward-project.eu”. Last accessed 14 April 2018.
[8]
“AKARI
Architecture
Design
Project.”
http://www.nict.
go.jp/publication/shuppan/kihou-journal/journal-vol62no2/
journal-vol62no2-03-00.pdf, 2006.
[9]
“Study Group 13 - Future networks.” https://www.itu.int/en/ITU-T/
about/groups/Pages/sg13.aspx. Last accessed 14 April 2018.
[10]
“Future Internet Research and Experimentation.” https://www.ict-ﬁre.
eu/ﬁre, 2010. Last accessed 14 April 2018.
[11]
“Global Environment for Network Innovations.” http://www.geni.net,
2010.
[12]
V. Cerf and R. E. Kahn, “Assessing the Internet: Lessons Learned,
Strategies for Evolution, and Future Possibilities,” ACM Turing award
lectures, 2004.
[13]
K. Nichols, S. Blake, F. Baker, and D. Black, “RFC 2474, Deﬁnition
of the Differentiated Services Field (DS Field) in the IPv4 and IPv6
Headers,” Internet Engineering Task Force (IETF), Dec. 1998.
[14]
S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang, and W. Weiss,
“RFC 2475, An Architecture for Differentiated Services,” Internet
Engineering Task Force (IETF), Dec. 1998.
[15]
J. Wroclawski, “RFC 2210, The Use of RSVP with IETF Integrated
Services,” Internet Engineering Task Force (IETF), Dec. 1997.
[16]
J. Hui and P. Thubert, “RFC 6282, Compression Format for IPv6
Datagrams over IEEE 802.15.4-Based Networks,” Internet Engineering
Task Force (IETF), Sept. 2011.
[17]
R.
Li,
A.
Sutton,
Ed.,
“Next
Generation
Protocols
-
Market
Drivers
and
Key
Scenarios,
Future
Internet
architecture.”
”http://www.etsi.org/images/ﬁles/ETSIWhitePapers/etsi wp17 Next
Generation Protocols v01.pdf”, Oct. 2016.
[18]
C. Labovitz, A. Ahuja, and F. Jahanian, “Experimental Study of
Internet Stability and Backbone Failures,” Digest of Papers. Twenty-
Ninth Annual International Symposium on Fault-Tolerant Computing
(Cat. No.99CB36352), pp. 278–285, 1999.
[19]
C. Villamizar, R. Chandra, and R. Govindan, “RFC 2439, BGP Route
Flap Damping,” Internet Engineering Task Force (IETF), Nov. 1998.
[20]
J.
Vahapassi,
“Internet
Routing
Stability,
ICL
Data
Oy.”
http://keskus.hut.ﬁ/opetus/s38130/k00/Papers/Topic17-Stability.doc.
[21]
E. Rosen and Y. Rekhter, “RFC 4364, BGP/MPLS IP Virtual Private
Networks (VPNs),” Internet Engineering Task Force (IETF), Feb. 2006.
[22]
M. Canbaz, K. Bakhshaliyev, and M. H. Gunes, “Analysis of path
stability within autonomous systems,” p. 38, 2017.
[23]
A. Zakaria, M. Alsarayreh, I. Jomhawy, and M. Rabinovich, “Internet
Path Stability: Exploring the Impact of MPLS Deployment,” IEEE
Global Communications Conference (GLOBECOM), pp. 1–7, 2016.
[24]
“Port
Blocking,
Broadband
Internet
Technical
Advisory
Group
Technical Working Group Report.” ”https://www.bitag.org/documents/
Port-Blocking.pdf”, Aug. 2013. Last accessed 14 April 2018.
[25]
National Research Council, “Looking Over the Fence at Networks, A
Neighbors View of Networking Research,” ”National Academy Press”.
[26]
J. S. Turner and D. E. Taylor, “Diversifying the Internet,” GLOBECOM
05. IEEE Global Telecommunications Conference, vol. 2, pp. 1–6, –
760, 2005.
[27]
B. Carpenter, ed., “Architectural Principles of the Internet,” Internet
Engineering Task Force (IETF), June 1996.
[28]
L. Solum and M. Chung, “The layers principle: Internet architecture
and the law,” Ssrn Electronic Journal, vol. 179, no. 1, p. 38, 2003.
[29]
J. Rexford and C. Dovrolis, “Future internet architecture,” Communi-
cations of the ACM, vol. 53, pp. 36–40, Sept. 2010.
[30]
R. Bush and D. Meyer, “RFC 3439, Some Internet Architectural
Guidelines and Philosophy,” Internet Engineering Task Force (IETF),
Dec. 2002.
[31]
M. Behringer and G. Huston, “A Framework for Deﬁning Network
Complexity,” Internet Engineering Task Force (IETF), Nov. 2012.
[32]
M. H. Behringer, “Classifying Network Complexity,” ACM Workshop
on Rearchitecting the Internet, ReArch 09, p. 13, Nov. 2009.
[33]
J. W. Drake, V. G. Cerf, and W. Kleinwchter, “Future of the Internet
Initiative White Paper, Internet Fragmentation: An Overview,” World
Economic Forum, p. 80, 2016.
[34]
P. Esnault, Ed., “GR NGP 004 - Evolved Architecture for mobility using
Identity Oriented Networks.” ”http://www.etsi.org/deliver/etsi gr/NGP/
001 099/004/01.01.01 60/gr NGP004v010101p.pdf”, Jan. 2018.
19
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

