Evaluating OpenFlow Controller Paradigms
Marcial P. Fernandez
Universidade Estadual do Cear´a (UECE)
Av. Paranjana 1700
Fortaleza - CE - Brazil
marcial@larces.uece.br
Abstract—The OpenFlow architecture is a proposal from
the Clean Slate initiative to deﬁne a new Internet architecture
where the network devices are simple, and the control and man-
agement plane is performed by a centralized controller. The
simplicity and centralization architecture makes it reliable and
inexpensive, but the centralization causes problems concerning
controller scalability. An OpenFlow controller has two oper-
ation paradigms: reactive and proactive. The performance of
both paradigms were analyzed in different known controllers.
The performance evaluation was done in a real environment
and emulation. Different OpenFlow controllers and distinct
amount of OpenFlow devices were evaluated. The analysis
has demonstrated the shortcoming of reactive approach. In
conclusion, this paper indicates the effectiveness of a hybrid
approach to improve the efﬁciency and scalability of OpenFlow
architecture.
Keywords-Openﬂow; OpenFlow Controller; Performance eval-
uation.
I. INTRODUCTION
The OpenFlow [1] architecture is a proposal from the
Clean Slate initiative to deﬁne an open protocol that sets up
forward tables in switches. It is the basis of the Software
Deﬁned Network (SDN) architecture, where the network
can be modiﬁed by the user. This proposal tries to use
the most basic abstraction layer of the switch, it is the
deﬁnition of forward tables, in order to achieve better
performance. The OpenFlow protocol can set a tuple of
condition-action assertion on switches like forward, ﬁlter
and also, count the packets that matches the condition.
The network management is performed by the OpenFlow
Controller maintaining the switches simple, only with the
packet forwarding function.
The OpenFlow architecture provides several beneﬁts:
(1) OpenFlow centralized controllers can manage all ﬂow
decisions reducing the switch complexity; (2) a central
controller can see all networks and ﬂows, giving global
and optimal management of network provisioning; and (3)
OpenFlow switches are relatively simple and reliable, since
forward decisions are deﬁned by a controller, rather than
by a switch ﬁrmware [2]. However, OpenFlow couples two
characteristics: a central controller and simple devices, that
result in scalability problems.
As the number of OpenFlow switches increases, relying
on a single controller for the entire network might not be fea-
sible for several reasons: (1) the amount of control messages
destined to the centralized controller grows with the number
of switches; (2) with the increase of network diameter, some
switches will have longer setup delay, independently where
the controller is placed [2]; and, (3) since the system is
bounded by the controller processor power, setup times can
grow signiﬁcantly when the number of switches and the size
of the network grow.
Two paradigms were implemented on some OpenFlow
controllers: the NOX/C++ Controller [3], the POX/Python
Controller [3], the Trema/Ruby Controller [4] and the Flood-
light/Java Controller [5]. In each controller and device the
reactive and proactive approaches were implemented and
they were evaluated.
The rest of the paper is structured as follows. In Section
II, we present some related work. Section III introduces
the OpenFlow architecture fundamentals; and in Section IV,
we present the evaluation methodology and tests. Section V
shows the results and Section VI concludes the paper.
II. RELATED WORK
Tootoonchian and Ganjali proposed the HyperFlow [6],
a mechanism that changes the controller paradigm from
centralized to distributed. HyperFlow tries to provide scal-
ability, using as many controllers as necessary to reach
it, but keeping the network control logically centralized.
The proposal uses a publish/subscribe messaging system
to propagate controller events to others, maintaining the
database consistent. However, this approach can only support
global visibility of rare events such as link state changes, not
frequent events such as ﬂow arrivals.
Another proposal, the DevoFlow [2], aims to deal with the
scalability problem by devolving network control to switches
with an aggressive use of ﬂow wildcard and introducing new
mechanisms to improve visibility. They introduced two new
mechanisms to be implemented on switches: rule cloning
and local actions.
The Source-Flow controller [7], proposed by Chiba, Shi-
nohara and Shimonishi, uses a similar approach. It tries
to reduce the number of ﬂow entries using a MPLS-like
tunneling approach in order do reduce the Ternary Content-
Addressable Memory (TCAM) used space.
151
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

In this paper, we evaluate some OpenFlow controller’s
performance working in reactive and proactive approach.
Then, we propose a new controller architecture performing
a hybrid approach, making better use of both paradigms.
III. OPENFLOW ARCHITECTURE
The OpenFlow architecture has several components: the
OpenFlow controller, the OpenFlow device (switch), and the
OpenFlow protocol. Figure 1 shows the components of an
OpenFlow architecture. The OpenFlow approach considers
a centralized controller that conﬁgures all devices. Devices
should be kept simple in order to reach better forward per-
formance and the network control is done by the controller.
Figure 1.
The OpenFlow architecture [1]
The OpenFlow Controller is the centralized controller of
an OpenFlow network. It sets up all OpenFlow devices,
maintains topology information, and monitors the overall
status of entire network. The OpenFlow Device is any
OpenFlow capable device in a network such as a switch,
router or access point. Each device maintains a Flow Table
that indicates the processing applied to any packet of a
certain ﬂow. The OpenFlow Protocol works as an interface
among the controller and the switches setting up the Flow
Table. The protocol should use a secure channel based on
Transport Layer Security (TLS).
The controller updates the Flow Table by adding and
removing Flow Entries using the OpenFlow Protocol. The
Flow Table is a database that contains Flow Entries asso-
ciated with actions to command the switch to apply some
actions on a certain ﬂow. Some possible actions are: forward,
drop and encapsulate.
Each OpenFlow device has a Flow Table with ﬂow entries
as shown in Figure 2. A Flow Entry has three parts: Rule,
Action and Statistics. The Rule ﬁeld is used to deﬁne the
match condition to a speciﬁc ﬂow; Action ﬁeld deﬁnes the
action to be applied to this ﬂow, and Stat ﬁeld is used to
count the rule occurrence for management purposes. When a
packet arrives to the OpenFlow Switch, it is matched against
Figure 2.
The OpenFlow Flow Entry [8]
Flow Entries in the Flow Table. The Action will be triggered
if the ﬂow Rule is matched and then, the Stat ﬁeld is updated.
If the packet does not match any entry in the Flow Table, it
will be sent to the Controller over a secure channel to ask
for an action. Packets are matched against all ﬂow entries
based on some prioritization scheme. An entry with an exact
match (no wildcards) has the highest priority. Optionally, the
Flow Table could have a priority ﬁeld (not shown in ﬁgure)
associated with each entry. Higher number indicates that the
rule should be processed before.
The Openﬂow Controller presents two behaviors: reactive
and proactive. In the Reactive approach, the ﬁrst packet of
ﬂow received by switch triggers the controller to insert ﬂow
entries in each OpenFlow switch of network. This approach
presents the most efﬁcient use of existing ﬂow table memory,
but every new ﬂow causes a small additional setup time.
Finally, with hard dependency on the controller, if the switch
loses the connection, it cannot forward the packet.
In the Proactive approach, the controller pre-populates the
ﬂow table in each switch. This approach has zero additional
ﬂow setup time because the forward rule is deﬁned. Now,
if the switch loses the connection with controller, it does
not disrupt trafﬁc. However, the network operation requires
a hard management, e.g., requires to aggregate (wildcard)
rules to cover all routes.
The OpenFlow Protocol uses the TCP protocol and port
6633. Optionally, the communication can use a secure chan-
nel based on TLS. The OpenFlow Protocol supports three
types of messages [8]:
1) Controller-to-Switch Messages: These messages are
sent only by the controller to the switches; they perform
the functions of switch conﬁguration, modifying the switch
capabilities, and also manages the Flow Table.
2) Symmetric Messages: These messages are sent in
both directions reporting on switch-controller connection
problems.
3) Asynchronous Messages: These messages are sent by
the switch to the controller to announce changes in the
network and switch state. All packets received by the switch
are compared against the Flow Table. If the packet matches
152
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

any Flow Entry, the action for that entry is performed on the
packet, e.g., forward a packet to a speciﬁed port. If there is
no match, the packet is forwarded to the controller that is
responsible for determining how to handle packets without
valid Flow Entries [8].
It is important to note that when the OpenFlow switch
receives a packet to a nonexistent destination in the Flow
Table, it requires an interaction with the controller to deﬁne
the treatment of this new ﬂow. At least, the switch will
need to send a message to the controller with regards to
the new packet received (message Packet-In). If the path
is already predeﬁned (there is an entry in Flow Table), this
procedure is not necessary, reducing the amount of messages
exchanged through the network and reducing the processing
at the controller.
Furthermore, the maintenance of unused Flow Entries
in the switch Flow Tables requires fast TCAM memory.
Therefore it is necessary to remove unused ﬂows using a
time-out mechanism. If a ﬂow previously excluded by time-
out restarts, it is necessary to reconﬁgure all switches on the
end-to-end path.
A. OpenFlow Device
An OpenFlow device is basically an Ethernet switch
supporting OpenFlow protocol. But there are different im-
plementation approaches: OpenFlow-enabled switch and
OpenFlow-compliant switch.
The OpenFlow-enabled switch uses off-the-shelf hard-
ware, i.e., traditional switches with OpenFlow protocol that
translate the rule according to the hardware chipset imple-
mentation. The OpenFlow-enabled switch re-uses existing
TCAM, that in a conventional switch has no more than only
few thousands of entries for IP routing and MAC table.
Considering that we need at least one TCAM entry per
ﬂow, in a current hardware, it would not be enough for
production environments. The Broadcom chipset switches
based on Indigo Firmware [9], e.g., Netgear 73xxSO, Pronto
Switch and many other, are example of this approach.
The OpenFlow-compliant switch uses speciﬁc network
chipset, designed to provide better performance to Open-
Flow devices. The OpenFlow philosophy relies on matching
packets against multiple tables in the forwarding pipeline,
where the output of one pipeline stage is able to modify the
contents of the table of next stage. Some example are devices
based on the EZChip NP-4 Network Processor [10]. But,
nowadays, there are few commercial OpenFlow-compliant
switches, one example is the NoviFlow Switch 1.1 [11].
B. OpenFlow Controller
All functions of the control and management plane are
performed by the controller. It has full network topology
information and the location of hosts and external paths
(MAC and routing tables). When a switch receives a packet
in which there is no entry in its Flow Table, it forwards
the message to the controller asking for the action to take
upon this new ﬂow. The controller can deﬁne the port that
the ﬂow must be forwarded to or take other actions, such as
dropping the packet. The controller must set the entire path
by sending conﬁguration messages to all switches from the
source to the destination.
Scalability and redundancy are possible using a stateless
OpenFlow control, allowing simple load-balancing over mul-
tiple devices [1]. Due to the OpenFlow centralized architec-
ture, controller scalability issues have received attention by
researchers. Many authors focus on distributed architectures
to improve the scalability problem.
The most common OpenFlow controller operation mode
is the reactive. In the reactive mode, the controller listens
to switches passively and conﬁgures routes on-demand. It
receives messages of connected hosts from the switches and
treats ARP message from hosts in order to maintain a global
MAC table. Upon receiving an ARP message, the controller
looks for the destination host location and sets the path by
sending OpenFlow messages to affected switches. After a
time out, an unused entry is excluded from the table. The
reactive behavior allows a more efﬁcient use of memory
(MAC table) at the cost of the re-establishing path later, if
necessary. In the proactive mode, paths are set up in advance.
This comes at the cost of lower memory space efﬁciency and
the requirement for a priori setup of all paths.
The controller performance is a central issue of an
OpenFlow architecture. A controller can only support a
limited number of ﬂow setups per second. In the former
Ethane experiment [12] with similar approach, i.e., only one
controller sets many switches, each controller could support
10K new ﬂows per second. Another work, Tavakoli et al.
[13] shows that one NOX controller can handle a maximum
of 30K new ﬂow installs per second maintaining a ﬂow
install time below 10 ms. From the network side, Kandula
et al. [14] measure on a 1500-server cluster datacenter the
creation of 100K new ﬂows per second, implying a need
for, at least, four OpenFlow controllers. As the OpenFlow
philosophy relies on a single controller, we can question
the feasibility of OpenFlow use in (not so big) production
datacenter.
Several approaches have proposed new OpenFlow Con-
troller architectures and implementations. One of the ﬁrst
approaches was the NOX Controller [3], a centralized con-
troller that implements a reactive and proactive approach.
The NOX default operation mode is the reactive, but offer
an API to allow users to set ﬂows in a proactive mode. The
NOX framework is used as the basis for many controller’s
implementation, for example, the POX controller. POX con-
troller is a pure Python controller, redesigned to improve the
performance compared to original Python NOX. The former
NOX was redesigned to provide a pure C++ controller.
The Floodlight [5] is Java-based OpenFlow Controller,
forked from the Beacon controller developed at Stan-
153
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

ford. The Floodlight controller is an open-source software
Apache-licensed, supported by a community of developers.
It offers a modular architecture, easy to extend and enhance.
The Trema [4] is an OpenFlow controller framework
developed in Ruby and C. It is basically a framework, includ-
ing basic libraries and functional modules that work as an
interface to OpenFlow switches. Several sample applications
are provided to permit execution of different controllers,
making easy the extension to new features.
IV. EVALUATING OPENFLOW CONTROLLER’S
PARADIGM
To evaluate the OpenFlow controller’s performance, two
scenarios were built: (1) a real network with Netgear
GSM-7328SO switches and (2) a virtualized network using
Mininet [15]. In each scenario, a host generates trafﬁc to
cross the entire network topology simulating a production
network.
On the server, the OpenFlow controller under evaluation
is installed. In the host, it runs the Cbench benchmark
software to stress the controller’s capacity. There is also
another host to generate real trafﬁc crossing the network
using Mausezahn software [16]. The evaluation does not
intend to compare the controller software; the main objective
is to compare the performance and the behavior of each
controller using the reactive and proactive approach.
For trafﬁc generation, Mausezahn software was used [16].
Mausezahn is an open-source trafﬁc generator written in C,
which allows to send nearly any possible packet. In order
to stress the OpenFlow controller, Mausezahn generates
packets with one million different IP addresses, forcing the
switch to send one million of Openlow requests to the
controller. IP address was used instead of MAC address,
to simulate a normal OpenFlow operation, where an ARP
request starts the route request to the controller.
To evaluate the controller performance, CBench software
was used [17]. CBench is a performance measurement
tool designed for benchmarking OpenFlow controllers. The
benchmarking measurement is the amount of ﬂows per
second that can be processed by the controller.
The controller code has been modiﬁed to implement
the reactive and proactive behavior. A special conﬁguration
message was used to set the switches to work in a reactive
or proactive way. The Indigo Firmware was also modiﬁed to
receive this message and set the switch to work in reactive
or proactive manner. The OpenVswitch code was changed
to implement the same behavior on virtualized Mininet
environment.
A. Evaluation Scenario
The tests were executed in two environment: real network
and emulated network. The real network was made with
Netgear GSM-7328SO switches running on modiﬁed Indigo
Firmware release 2012.03.19, as shown in Figure 3.
CBENCH
OpenFlow
Controller
NetGear GSM-7328SO 
OpenFlow Switch
Traffic 
generation
Mausezahn
Figure 3.
Real switch scenario
The second test environment chosen was the OpenFlow
emulation over virtual machine using Mininet [15]. This
model permits an evaluation in emulation environment using
only one computer. Mininet imposes a restriction on link
layer conﬁguration, e.g., we cannot specify link bandwidth
or error rate, but for the validation this environment was
suitable to obtain the results.
The experiment was built over VMware Workstation 8.
In the virtualized environment, Mininet was used [15].
Mininet is a network emulator used to create Software
Deﬁned Networks (SDNs) scenario in Linux environment.
The Mininet system permits the speciﬁcation of a network
interconnecting “virtualized” devices. Each network device,
hosts, switches and controller are virtualized and commu-
nicate via Mininet. The trafﬁc ﬂow used in this test is
generated by Mausezahn. A Python script is used to create
the topology in Mininet and the trafﬁc ﬂows setup are
received from a remote OpenFlow controller.
Therefore, the test environment implements and performs
the real protocol stacks that communicate with each other
virtually. The Mininet environment allows the execution of
real protocols in a virtual network. The virtual topology
created in Mininet platform is shown in Figure 4.
B. Evaluation Procedure
To deﬁne the experiment, initially it is necessary to specify
the hosts and network that will be used. The OpenFlow
controller has the responsibility to deﬁne the best path to
connect all hosts.
To evaluate the controller performance into the test topol-
ogy the Cbench [17] program was included, part of the
OpenFlow suite that creates and sends a large amount of
OpenFlow messages to the controller in order to test its
performance. These messages do not represent any real
154
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

OpenFlow
Controller
Mininet Environment
Traffic 
generation
CBENCH
Figure 4.
Virtual switch scenario
network topology, only “deceive” the controller that handles
and sends messages believing it is dealing a larger network.
As result, we obtain the number of OpenFlow messages the
controller can support per second, besides the messages sent
by real switches or virtualized switches in Mininet.
The tests with the Cbench, simulated the presence of
more than 100 switches, in addition to the 50, 100 and
200 virtualized switches topology created on Mininet. In
each round, 16 tests were performed, and in these tests the
average and the standard deviation were calculated. Finally,
the graph was plotted of average performance with 95%
conﬁdence interval. Since we are interested in studying the
system in equilibrium, we do not consider the ﬁrst 2 minutes
of data as the warm up period.
In a normal OpenFlow network, when a host performs an
ARP request to ﬁnd the destination address and the switch
has no such address in its Flow Table, the switch makes a
query to the OpenFlow controller. The controller will decide
what action would be applied to this ﬂow, e.g., choose the
path from origin to the destination to forward the packets
to destination host. To optimize the memory usage in the
switches, a timeout (in our experiment 10 seconds) sets the
removal of this entry on its ﬂow table, forcing the path
rebuild whenever it is necessary.
In the Proactive approach, the path is already predeﬁned
for the necessary time and therefore it is not necessary to ask
the controller to build the path from source to destination.
The proactive approach is implemented on controller’s and
switch’s code. The reduction of messages to discover a
new path among switches allows the increase of controller
performance, as shown on the results in Section V below.
V. RESULTS
The performance test results are shown in the following
graphs. They show the performance of each controller in
how many OpenFlow messages could be processed accord-
ing to the amount of additional switches which were created
in real scenario and in Mininet environment.
Figure 5 shows the performance of each OpenFlow con-
troller in real switch network, shown in Figure 3. We
can see that every controller, independently of architecture
and programming language, has better performance in the
proactive approach.
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 40000
 45000
 50000
FloodLight
NOX-C++
POX
Trema
Performance (Flows/sec)
OpenFlow Controller Paradigm Evaluation (3 Netgear Switch)
Reactive
Proactive
Figure 5.
Netgear switch network evaluation
Figure 6 shows the performance of each OpenFlow con-
troller in emulated Mininet environment, shown in Figure 4.
We can see similar results, proactive approach gives better
performance.
 2000
 4000
 6000
 8000
 10000
 12000
 14000
FloodLight
NOX-C++
POX
Trema
Performance (Flows/sec)
OpenFlow Controller Paradigm Evaluation (200 Mininet Switches)
Reactive
Proactive
Figure 6.
Mininet 200 switch network evaluation
Figure 7 shows the NOX-C++ controller performance
from 3 real switches to 200 virtual switches. The perfor-
mance measured by Cbench benchmark reduces according
to the increase of number of switches in network.
The improvement is due to proactive controller receives
less request message from switch because the path is already
set. Receiving fewer messages from “real” network allows
155
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

 5000
 10000
 15000
 20000
 25000
 30000
 35000
 40000
 45000
 50000
3
50
100
200
Performance (Flows/sec)
Number of Switches
OpenFlow Controller Paradigm Evaluation (NOX-C++)
Reactive
Proactive
Figure 7.
NOX-C++ controller evaluation
the controller to handle more “fake” messages from Cbench
program. We can also notice that increasing the number
of virtualized switches in Mininet, the performance will be
reduced.
The proactive operation improves the controller perfor-
mance. As the user pre-conﬁgures a path, all packets from
this ﬂow already have a Flow Entry on all switch’s Flow
Table, the switches do not need to send a message to the
controller. Then, the reduction of messages sent by switches
reduces the amount of messages received by the controller,
providing capacity to controller dealing more messages from
other on-demand ﬂows.
However, the proactive approach requires the controller
know the trafﬁc ﬂows in advanced to conﬁgure the paths
before it is used. The reactive approach reduces the con-
troller’s performance but requires less conﬁguration effort.
In reactive approach, it is necessary only to conﬁgure an IP
address to put the network in operation.
VI. CONCLUSION AND FUTURE WORKS
Although we consider that the OpenFlow architecture has
a prominent future due to its simplicity and suitability to new
technologies, its centralized architecture causes a scalability
problems. This problem has been studied by researchers
from several points of view.
This paper analyzes the performance of different Open-
Flow controller operating in reactive and proactive approach.
All evaluation’s results show the increase on controller
performance when it used the proactive approach. However,
we agree that reactive approach makes easy on the network
operation and management.
Our proposal tries to indicate a possible solution to this
problem, by adding a new intelligent switch that sets the
paths on-demand, improving the performance of proactive
approach but maintaining the facility of reactive approach.
The controller acts like a learning switch without manage-
ment interference but can perform a proactive set up using
Reinforcement Learning. The results show an improvement
in controller performance due to the fact of reduction on
control messages.
But some heuristics will be able to set the paths in advance
automatically, improving the performance maintaining the
simplest conﬁguration. The use of Reinforcement Learning,
proposed in Boyan and Littman [18], and also, by Peshkin
and Savova [19], can deﬁne the routes without the user
intervention.
As future work, it will be interesting to improve the sys-
tem manageability implementing the Reinforcement Learn-
ing mechanism to set the routes based on trafﬁc behavior.
Another proposal, is to adjust the OpenFlow switch Flow
Table time-out based on trafﬁc behavior.
ACKNOWLEDGMENT
We would like to thank Broadcom Corporation for their
support.
REFERENCES
[1] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,
L. Peterson, J. Rexford, S. Shenker, and J. Turner, “Open-
Flow: enabling innovation in campus networks,” ACM SIG-
COMM Computer Communication Review, vol. 38, no. 2, pp.
69–74, 2008.
[2] J. C. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma, A. R.
Curtis, and S. Banerjee, “Devoﬂow: cost-effective ﬂow man-
agement for high performance enterprise networks,” in Pro-
ceedings of the Ninth ACM SIGCOMM Workshop on Hot
Topics in Networks, ser. Hotnets ’10.
New York, NY, USA:
ACM, 2010, pp. 1:1–1:6.
[3] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado, N. McK-
eown, and S. Shenker, “Nox: towards an operating system for
networks,” SIGCOMM Comput. Commun. Rev., vol. 38, no. 3,
pp. 105–110, 2008.
[4] NEC, “Trema Openﬂow Controller,” Last accessed, Aug
2012. [Online]. Available: http://trema.github.com/trema/
[5] D. Erickson, “Floodlight Java based OpenFlow Controller,”
Last
accessed,
Aug
2012.
[Online].
Available:
http://
ﬂoodlight.openﬂowhub.org/
[6] A. Tootoonchian and Y. Ganjali, “HyperFlow: A distributed
control plane for OpenFlow,” in Proceedings of the 2010
internet network management conference on Research on
enterprise networking.
USENIX Association, 2010, p. 3.
[7] Y. Chiba, Y. Shinohara, and H. Shimonishi, “Source ﬂow:
handling millions of ﬂows on ﬂow-based nodes,” SIGCOMM
Comput. Commun. Rev., vol. 40, pp. 465–466, August 2010.
[8] B. Heller, “Openﬂow switch speciﬁcation, version 1.0.0,”
Last
accessed,
Dec
2011.
[Online].
Available:
www.
openﬂowswitch.org/documents/openﬂow-spec-v1.0.0.pdf
[9] D. Talayco, “Indigo OpenFlow Switching Software Package,”
Last accessed, Jun 2012. [Online]. Available: http://www.
openﬂowswitch.org/wk/index.php/IndigoReleaseNotes
156
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

[10] O. Ferkouss, I. Snaiki, O. Mounaouar, H. Dahmouni,
R. Ben Ali, Y. Lemieux, and O. Cherkaoui, “A 100gig
network processor platform for openﬂow,” in Network and
Service Management (CNSM), 2011 7th International Con-
ference on.
IEEE, 2011, pp. 1–4.
[11] NoviFlow, “NoviFlow Switch 1.1,” Last accessed, Sep
2012. [Online]. Available: http://www.noviﬂow.com/index.
asp?node=2&lang=en
[12] M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown,
and S. Shenker, “Ethane: taking control of the enterprise,”
in Proceedings of the 2007 conference on Applications,
technologies, architectures, and protocols for computer com-
munications, ser. SIGCOMM ’07.
New York, NY, USA:
ACM, 2007, pp. 1–12.
[13] A. Tavakoli, M. Casado, T. Koponen, and S. Shenker, “Ap-
plying NOX to the Datacenter,” in Proceedings of workshop
on Hot Topics in Networks (HotNets-VIII), 2009.
[14] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and
R. Chaiken, “The nature of data center trafﬁc: measurements
& analysis,” in Proceedings of the 9th ACM SIGCOMM
conference on Internet measurement conference.
ACM,
2009, pp. 202–208.
[15] B. Lantz, B. Heller, and N. McKeown, “A network in a
laptop: rapid prototyping for software-deﬁned networks,” in
Proceedings of the Ninth ACM SIGCOMM Workshop on Hot
Topics in Networks, ser. Hotnets ’10.
New York, NY, USA:
ACM, 2010, pp. 19:1–19:6.
[16] H. Haas, “Mausezahn Trafﬁc Generator Version 0.4,” Last
accessed, Jan 2012. [Online]. Available: http://www.perihel.
at/sec/mz/
[17] R.
Sherwood
and
K.-K.
Yap,
“Cbench
(controller
benchmarker),” Last accessed, Nov 2011. [Online]. Available:
http://www.openﬂowswitch.org/wk/index.php/Oﬂops
[18] J. Boyan and M. Littman, “Packet routing in dynamically
changing networks: A reinforcement learning approach,” Ad-
vances in neural information processing systems, pp. 671–
671, 1994.
[19] L. Peshkin and V. Savova, “Reinforcement learning for adap-
tive routing,” in Neural Networks, 2002. IJCNN’02. Proceed-
ings of the 2002 International Joint Conference on, vol. 2.
IEEE, 2002, pp. 1825–1830.
157
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

