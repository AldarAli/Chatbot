PolyPie: A Novel Interaction Techniques For Large Touch Surfaces With Extended
Wall Displays
Ihab Maged ∗, Michael Louis ∗, Mohamed Thabet ∗ and Ayman Atia †
∗ Faculty of computer science
MIU university , Cairo, Egypt
Email: ihab090178, michael.louis, mohamed.thabet @miuegypt.edu.eg
† HCI Lab, Faculty of computer science and information systems, Helwan University
Email: ayman@fci.helwan.edu.eg
Abstract—The paper presents new interaction techniques for
large touch tables and large-wall projected screens. The extended
projected screen is integrated as an extension to the touch table
where the user can grab the menu items on the projected screen
from the touch table or by interacting directly with the projected
screen using hand gestures. The system proposes multi-touch
controls and hand gestures that aim to minimize the user effort
and body movements while interacting with the large touch
table and the extended projected large-wall display. We present
”PolyPie” as a group of three touch interaction techniques for
large display touch surfaces and two hand gestures techniques for
interacting with the large-wall projected display. The proposed
touch techniques are the Dynamic Pie Magniﬁer, Poly-Fingers
Grab, Five Fingers Shadow Grab. We conducted a preliminary
study for the proposed interaction techniques. The results showed
that the proposed techniques helped the users to interact much
easier with large display table and wall screen. The users were
able to access distant ﬁles in acceptable time and more smoothly.
Keywords-Large Display Interaction;Gesture Interaction;Multi-
touch Tables;Interaction Techniques
I.
INTRODUCTION
Large Display Screens are becoming widely used after
recent progression of technology. Ball and North [1] showed
that by launching several windows on a single large screen
will facilitate different advantages for the users. The main
beneﬁt for working on those large displays is gaining the
usability of previewing relatively large sets of data. The main
challenge is to ﬁnd out the proper interaction techniques that
ﬁt with interacting with those displays. Tan et al. [2] stated that
large display screens save much time and effort when they are
compared to the normal display devices especially when they
are used in group-ware activities. Although those large displays
may be perfectly used by a single user but by the help of
certain interaction techniques. Also, the interaction techniques
designed for traditional computers, such as the trackball mouse
are not always appropriate for use with other form factors,
especially for large displays. The traditional trackball mouse
needs to be dragged from the bottom of the display up to the
end in order to a grab or select a single menu item. Khan et
al. [3] stated that in certain circumstances, the user spent a
signiﬁcant amount of time trying to ﬁnd out where the mouse
pointer was on the screen. The problem the users face is that
it is impossible for them to reach distant windows and keep
track of many launched windows. Vogel et al. [4] showed that
in some certain scenarios, users have to step back or move
from their place in order to access a speciﬁc item on a large
display. There are some tasks that are needed to be handled
from a distance in some speciﬁc environments, such as sorting
slides, photos and pages spread over a large display surface
or even a public place that contains a large mounted display
that is needed to be accessed. Radloff et al. [5] showed that
large displays in meeting rooms will assure a high level of
presentations and interactions between the users.
In this paper, we proposed some new interaction techniques
for working on a large touch table which is also extended to
another projected large-wall display. The main concern is to
embed the interaction techniques into the seamless interactions
ﬁgured out by the user in order to gain the maximum usability
and the minimum effort. Our motivation in this work is to
prevent the user from being frustrated while working on these
large extended displays, so the extended projected display and
the touchable surface must be logically integrated and accessed
as a single regular display with the proposed techniques. We
conducted a preliminary experiment concerning the time of the
proposed interaction techniques. We discussed the advantages
of each technique and its usage in different situations and
applications with large surfaces.
II.
RELATED WORK
Parker et al. [6] showed that large display screens are now
facing a problem in ﬁnding useful interaction techniques that
will help its users to reach any far object. There are some
techniques presented by researchers by using several input
method like laser pointers. Myer et al. [7] showed that using
laser pointers are problematic in terms of accuracy and speed.
Moreover, Laser pointer are used more effectively on wall
projected screens not a big display table.
Malik et al. [8] showed that there are several interaction
techniques implemented for accessing far distant desktop items
on a touchable surface to maximize the comfort of the user.
They implemented their interaction techniques on a touch pad.
A technique similar to the Vacuum [9] was implemented in
which all the menu items between the angles of two ﬁngers
are dragged to each ﬁnger. Also, on touching the touchable
48
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

surface with the ﬁve ﬁngers, the nearest ﬁve menu items are
dragged to each ﬁnger on the touch surface. Drag and drop is
also implemented as if the user is catching a menu item and
then leave the menu item to be dropped with his ﬁve ﬁngers.
Bezerianos et al. [9] conducted the Vacuum technique,
which works by drawing two projected lines till the end of the
screen from a single source point, a polygon will be drawn
and all the items covered in the polygon area will be dragged
to the source point. The Vacuum technique was compared
with the Drag-and-Pop technique and the experimental results
showed that the vacuum technique is relatively faster and more
accurate. The Vacuum technique has some limitations on the
touch table especially on changing the angle between the two
ﬁngers. The user needs to remove his hand and hold it one
more time on the touch table to change the angle.
Radloff et al. [5] discussed grouping of several large dis-
plays and how the user interacts with them. The user interacts
with the projected displays using a Wii remote where he/she is
able to manage and drag the menus and graphs instantly from
one screen to another. The paper stated that the interactions
with the projected screen by using a depth camera could be
more appropriate than the Wii remote.
Kim et al. [10] showed that there are some interaction tech-
niques used on the multi-touch screens where these techniques
have to be predeﬁned according to the number, position, and
movement of the ﬁngertips of the user. When the user touches
the screen an image is captured and preprocessed to remove
the noise and determine the movement of the ﬁngers. The
gesture commands were to manipulate the user interface. Bi
et al. [11] conducted the MagicDesk that works on integrating
touch capabilities with desktop work, which brings maximum
accuracy and speed for the user as the keyboard and mouse
technique is one of the fastest techniques but with integrating
some touch interactions for desktop work introduces a new way
for working on a desktop environment. The main contribution
in the paper is using an interactive tabletop by interacting with
touch techniques on the table and updating the monitor in real
time.
Potter et al. [12] used Leap motion for Australian sign
language. Leap Motion is a small usb device which can be
placed on any physical surface , facing upward . It uses two
monochromatic IR cameras and three infra-red LEDs , the
device can detect motion to a distance of 1 meter (3 feet).
The LEDs generate a 3D pattern of dots of IR light , the
IR cameras then generates almost 300 frames per second
of the reﬂected data. The leap motion controller software
synthesize 3D position data by comparing the 2D frames which
is generated by the IR Cameras. Marquardt et al. [13] showed
that the main idea of unifying touch and gestures on and above
a digital surface is to create a continuous interaction space
where the user moves any tangible objects in the space and
can also use touch and gestures techniques. The continuous
interaction space can be illustrated as follows; a 3D area above
the touch surface is conducted where the user interacts with his
gestures without even touching the touch surface. For example,
Mirrored Gestures for Redundancy is one of the techniques
used where the person interacts with the touch surface using
hand gestures instead of interacting directly on the surface. Kin
touch is a way of integrating touch capabilities with Kinect
[14] for facilitating the life of visually impaired people by
using tactile or interactive maps. The software works on two
applications, one for tracking ﬁngers with Kinect image and
the other for detecting the position of the ﬁngers on the touch
surface.
Shoemaker et al. [15] showed a new way for human
computer interactions dealing with 5m X 3m large display
wall depending on reality based interaction and whole body
interfaces. The major contributions of their work was pre-
senting a new body-centric approach speciﬁc to large display
walls. The ﬁrst step in their work was to sense the body by
a vision tracker, then modelling the body position and at last
analysing the interaction done by the user. Fikkert et al. [16]
proposed new hand gestures for interacting with large-wall
projected screens. They conducted their experiment on a large
projected display where the task is to allocate a speciﬁc town
on a large map. They introduced a set of gestures, such as the
point, select, deselect and three other gestures for activating,
deactivating and resizing.
Adrian et al. [17] implemented the SuperFlick, which is a
technique for long-distance object placement on digital tables.
SuperFlick is based on a remote Drag-and-Drop technique on
a thrown object. The user does not need to wait until the
ﬂow is ﬁnished and the system knows the ﬁnal position of
the object as soon as it is thrown. Drag-and-Pop and Drag-
and-Pick are techniques for accessing remote screen on touch
and pen operated systems. Patrick et al. [18] showed that the
users conducted some errors when the menu items are close
to each other or when they are removed from their home
screen. Doeweling et al. [19] implemented a new way for
representing the Drag-and-Drop technique for large display
touchable surfaces, which aims to optimize the short-backs
of the regular drag and drop technique. A fully interactive
proxy- targets operations are introduced and compared with
the regular Drag-and-Drop technique.
III.
SYSTEM OVERVIEW
Our system is composed of a large touch surface and a large
projected display integrated together, as shown in Figure 1. The
user interacts with the projected screens using a Depth camera
that is above the projected screen and in front of the user.
The projected display is continuously extended and integrated
to the touch surface. There are two scenarios for the user to
grab the far away menu items. The ﬁrst scenario, is working
with the touch techniques to grab the far away menu items
on the touch surface. However, this touch technique can work
as an extension towards the projected display in order to grab
all the menu items over the projected display and the touch
surface. The second scenario is that the user grabs multiple
items from the projected screen to the touch surface instantly
using some hand gestures. A continuous interaction space is
achieved between the projected display and the touch surface.
A. Hardware Conﬁgurations
The touch surface is created by Frustrated Total Internal
Reﬂection (FTIR) technique used by the multi-touch commu-
nity for building touch tables. A Stream of Infra-red LEDS is
surrounded within an acrylic sheet where the Infra-red light
enters the acrylic with a higher refractive index, at an angle
of incidence greater than a speciﬁc angle. The FTIR method
49
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

Figure 1.
Extended wall and large touch surface system overview
uses this principle of Total Internal Reﬂection in order to ﬂood
the Infra-red light inside the piece of acrylic. When the user
touches the surface, the light rays are frustrated, since they
can now pass through the contact material and the reﬂection
is no longer total at that point. This frustrated light is scattered
downwards towards an Infra-red camera in terms of blob
positions where the camera software can track and analyze
them.
A projector works as a displaying tool for the touch surface.
Basically, the projector is inside the table and it is directed and
positioned to a mirror. The mirror reﬂects the display with
a higher index to the acrylic surface that is covered with a
compliant layer. Our touch table dimensions are 150 X 120
X 100 cm where a single camera works on covering all the
acrylic sheet. We used a single graphics card with multiple
outputs for connecting the projector displaying the Projected-
Wall screen into the same PC connecting the touch surface
projector.
B. Software Conﬁgurations
Figure 2 shows the system’s overview. It is composed of :;
Input Layer, Intermediate Hardware Layer, Hardware Layer,
Software Server Layer, Software Client Layer, Application
Service Layer, Application Layer and Presentation Layer.
The Hand Gestures block is the Kinect Depth Camera
input layer while the Touch Input block is the touch surface
table input. The Intermediate Hardware Layer block contains
the FTIR Touch table components that works for reﬂecting
the IR light into the Camera. The Hardware Layer block
includes the PlayStation 3 (PS3) Eye-Cam block that captures
the reﬂected IR light and the Depth Camera block, which
is the Kinect that capture the body skeleton of the user. We
used the Community Core vision (CCV) as a Software Server
Layer, which is the camera software that works on ﬁltering
the captured image and calibrating the touch input with the
projected Screen. The TUIO protocol used for exchanging
messages between the software client layer and the software
server layer. The Microsoft Kinect Software Development Kit
works for analyzing the users hand gestures. Gesture was used
to select multiple items from projected display and other one
for selecting all items on the projected screen.
The Application Service Layer is the Gestures Recognition
Part, it implements the TUIO Client interface and the Microsoft
Figure 2.
Block Diagram: System Overview
Kinect interface in order to start analyzing the gestures and
compare the input gestures with predeﬁned gestures. The Ap-
plication layer works on animating and grabbing the selected
items for the user’s position based on the validated technique
received by the Gesture Listener in the Application layer and
displays the feedback to the presentation layer for the user.
IV.
INTERACTION TECHNIQUES
Interaction with large display screens requires some special
interaction techniques in order to grab the far away menu items
on the screen as it is impossible to select these items from
the user’s position. The user needs to move and stretch his
arm towards the touch surface in order to select a speciﬁc
item. The user cannot select the ﬁrst top left item from
his place. We implemented three touch interaction techniques
for large display touch surfaces. The touch techniques are,
the ”Dynamic Pie Magniﬁer”, the ”Poly-Fingers Grab” and
the ”Five Fingers Shadow Grab”. We used the Ray Tracing
algorithm for all the techniques that works on checking the
items inside a polygon.
A. Dynamic Pie Magniﬁer
The Dynamic Pie Magniﬁer is basically based on interact-
ing with the touch surface for grabbing multiple items from
any position on the screen by using two ﬁngers. The user holds
down the thumb ﬁnger on the touch surface, then points with
his second ﬁnger on the screen and start moving the second
ﬁnger to the right position in order to maximize the size of the
polygon. The shape of the drawn polygon is a cone polygon
in order to insert the highest number of items possible.
The polygon area is maximized as long as the user drags
his ﬁnger to the right and any allocated item inside the polygon
will be moved and inserted into the Pie Menu. The Pie Menu
appears on inserting the ﬁrst item into the polygon. The
50
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

Figure 3.
Touch surface: Dynamic Pie Magniﬁer Technique
Figure 4.
Extended Screen: Dynamic Pie Magniﬁer Technique
maximum amount of items that can be previewed on a Pie
Menu are eight items. The user can swipe the pie menu in
order to view more items. Figure 3 shows a single scenario for
grabbing the far menu items to the user using the Dynamic Pie
Magniﬁer Technique. The Dynamic Pie Magniﬁer technique
can also work for grabbing the menu items on the Wall-
Projected Screen to the touch surface as shown in Figure 4.
The items are dragged from the projected display to the touch
surface continuously and inserted into the Pie Menu.
B. Poly-Fingers Grab
The Poly-Fingers Grab works on holding down the two
ﬁngers of two different hands on the touch surface. A straight
line is drawn till the end of the touch surface from each ﬁnger
in order to display a polygon for the user. The user can change
the size of the polygon by dragging his two ﬁngers to the left or
right for maximizing and minimizing the polygon. The items
inside the polygon are animated and dragged to the user’s left
ﬁnger position. Other menu items are grabbed to the user as
long as the polygon is maximized.
Figure 5.
Touch surface: Poly-Fingers Grab Technique
Figure 6.
Extended Screen: Poly-Fingers Grab Technique
The user can change the angle of the drawn lines by
increasing the top position of one ﬁnger with a higher index
than the other ﬁnger. On the other hand, all the items that are
not anymore inside the polygon are dragged to their original
position as long as the user minimizes the distance between
the two ﬁngers. Figure 5 shows a single scenario for grabbing
the far menu items to the user using the Poly-Fingers Grab
Technique. The Poly-Fingers Grab technique can also work
for grabbing the menu items on the Wall-Projected Screen to
the touch surface as shown in ﬁgure 6. The items are dragged
from the projected display to the touch surface continuously
where the items are sorted at the user’s ﬁnger position. The
user can ﬁnally select the target item with on of his ﬁngers on
the screen.
51
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

C. Five Fingers Shadow Grab
Figure 7.
Touch surface: Five Fingers Shadow Grab Technique
The Five Fingers Shadow Grab technique simply works
when the user puts his ﬁve ﬁngers on the touch surface. The
user starts grabbing his hand to the lower border of the table
which is the user’s position as shown in Figure 7. On the
other hand, if the extended screen is activated, only the menu
items on the projected screen will be dragged to the touch
surface until they are close enough to the user. The technique
works for grabbing all the items on the screen. The user ﬁnally
selects one of the items from the grabbed items. The items are
returned to their initial position if the user did not select any
of the items for 2 seconds.
V.
EXPERIMENTS
Two Female and 8 Male volunteers participated on trying
the interaction techniques over the touch surface where their
age range is from 20 to 25 years old. They are all familiar
with the traditional mouse input device but they have never
interacted with a large touch screen or extended displays. We
conducted a training session for each user before performing
the real experiments. The experiment scenarios were ﬁxed
across all users to grab the most far menu items on the screen
to the user’s and select a target item (icons or ﬁles). We showed
the participants 16 items and ask them to select 2 item from
the ﬁrst row. The users repeat each scenario 3 times.
A. Experiment 1 - Large Touch Surface
The goal of this experiment is to measure the time for
the interaction techniques on grabbing the far away items on
the touch surface. Furthermore, we compared the interaction
techniques with the mouse device. The interaction techniques
on the touch screen results are shown in ﬁgure 8. After
conducting the experiments, it was shown that the Dynamic
Pie Magniﬁer is the fastest technique and achieve a near
result to the mouse device. The Five Fingers shadow Grab
conducted more time as it was frustrating for the user to drag
his ﬁve ﬁngers together at the same time for long distance
over the surface. The Mouse is faster than the other interaction
techniques as the user does not need to grab the items to
his position. Although, the mouse cannot be used as an input
device for the touch surface, but we compared our techniques
with the mouse input device to ensure that our techniques
does not waste time while interacting with the touch surface.
Moreover, the participants were more familiar with the mouse
device.
B. Results 1
Figure 8. Large Touch Surface Interaction Techniques average time in seconds
The Dynamic Pie magniﬁer scored the best results after
the mouse as all the selected items are inserted in a pie menu
around the user’s thumb. So, it was closer to the user and
faster than the Poly-ﬁngers grab, which works by grabbing the
selected icons and listing them in rows and columns depending
on the distance between the two ﬁngers.
C. Experiment 2 - Extended wall display
The goal of this experiment is to test the interaction
techniques on grabbing the projected display menu items to the
touch surface. We compared all the techniques we developed
in addition to mouse and hand gesture interaction. We tested
two hand gesture techniques for the projected display. First,
The ”Multiple Selection” hand gesture that is based on the
pause technique but the user has to raise his left hand in order
to enter the selection mode. Second, the ”Select All” Hand
Gestures technique works for selecting all the items in the
projected screen. The user has to raise his both hands in a
crossing position, the user drags these items till the end of the
screen coordinates and they are relatively dragged to the touch
surface.
D. Results 2
The experiment shows that the user performance while
interacting with the Poly-Fingers Grab is better than the
Dynamic Pie Magniﬁer unlike the ﬁrst experiment as shown
in ﬁgure 9. The Poly-Fingers Grab was more accurate as the
drawn straight polygon on the projected display was more
imaginable to the user, while the cone polygon was not very
familiar when it was extended to the projected display. The
angle between the two drawing lines in the Dynamic Pie
Magniﬁer relatively increases thus, lots of items are dragged
to the touch surface which conducts higher time range on
selecting the target item from the pie menu. On the other
hand, the Poly-Fingers Grab drags the items directly to the
touch surface and the user directly select the target.
52
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

Figure 9.
Extended Wall-Screen With Touch Surface Interaction Techniques
Results average time in seconds
The Five Fingers Shadow Grab still conducts a higher time
range for grabbing the items from the extended display and
user movement over the surface was more longer. We have
tested PolyPie with two different applications, ﬁrst was live
feedback presentation application for smart meeting rooms. We
get some users feedback about feeling interested that they can
access shared materials from the projected screen on their table
interface smoothly. The second application was for sharing
pictures between users sitting on the same large table. Users
were found the pie menu interface easy to use and was fast way
to access far items. Finally, the proposed interaction techniques
will increase the productivity of large display touch tables.
VI.
CONCLUSION AND FUTURE WORK
We presented ”PolyPie”, an interaction techniques for large
displays. Different touch interaction techniques are proposed
in order to minimize the user effort while interacting with large
display touch surfaces. The experiments showed that Dynamic
Pie Magniﬁer achieved acceptable results while grabbing the
items from the touch surface only. On the other hand, the
Poly-Fingers Grab technique was better when the user grabs
the items from the projected display to the touch surface. The
Five Fingers Shadow Grab was frustrated while dragging the
ﬁve ﬁngers concurrently. Our future work is to create some
techniques for grabbing the items from the touch table to the
extended display.
REFERENCES
[1]
R. Ball and C. North, “Analysis of user behavior on high-resolution
tiled displays,” in Interact 2005.
Berlin, Heidelberg: Springer-Verlag,
2005, pp. 350–363.
[2]
D. S. Tan, D. Gergle, P. Scupelli, and R. Pausch, “Physically large
displays improve performance on spatial tasks,” ACM Trans. Comput.-
Hum. Interact., vol. 13, no. 1, Mar. 2006, pp. 71–99.
[3]
A. Khan, J. Matejka, G. Fitzmaurice, and G. Kurtenbach, “Spotlight:
directing users’ attention on large displays,” ser. CHI ’05.
New York,
NY, USA: ACM, 2005, pp. 791–798.
[4]
D. Vogel and R. Balakrishnan, “Distant freehand pointing and clicking
on very large, high resolution displays,” in Proceedings of the 18th
annual ACM symposium on User interface software and technology,
ser. UIST ’05.
New York, NY, USA: ACM, 2005, pp. 33–42.
[5]
A. Radloff, A. Lehmann, O. Staadt, and H. Schumann, “Smart interac-
tion management: An interaction approach for smart meeting rooms,” in
Proceedings of the 2012 Eighth International Conference on Intelligent
Environments, ser. IE ’12.
Washington, DC, USA: IEEE Computer
Society, 2012, pp. 228–235.
[6]
J. K. Parker, R. L. Mandryk, and K. M. Inkpen, “Tractorbeam: seamless
integration of local and remote pointing for tabletop displays,” in
Proceedings of Graphics Interface 2005, ser. GI ’05.
School of
Computer Science, University of Waterloo, Waterloo, Ontario, Canada:
Canadian Human-Computer Communications Society, 2005, pp. 33–40.
[7]
B. A. Myers, R. Bhatnagar, J. Nichols, C. H. Peck, D. Kong, R. Miller,
and A. C. Long, “Interacting at a distance: measuring the performance
of laser pointers and other devices,” in Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, ser. CHI ’02.
New York, NY, USA: ACM, 2002, pp. 33–40.
[8]
S. Malik, A. Ranjan, and R. Balakrishnan, “Interacting with large
displays from a distance with vision-tracked multi-ﬁnger gestural input,”
in UIST 2005, ser. UIST ’05.
New York, NY, USA: ACM, 2005, pp.
43–52.
[9]
A. Bezerianos and R. Balakrishnan, “The vacuum: facilitating the ma-
nipulation of distant objects,” in Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, ser. CHI ’05.
New York,
NY, USA: ACM, 2005, pp. 361–370.
[10]
S.-G. Kim, J.-W. Kim, K.-T. Bae, and C.-W. Lee, “Multi-touch inter-
action for table-top display,” in Proceedings of the 16th international
conference on Advances in Artiﬁcial Reality and Tele-Existence, ser.
ICAT’06.
Berlin, Heidelberg: Springer-Verlag, 2006, pp. 1273–1282.
[11]
X. Bi, T. Grossman, J. Matejka, and G. Fitzmaurice, “Magic desk:
bringing multi-touch surfaces into desktop work,” in Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems, ser.
CHI ’11.
New York, NY, USA: ACM, 2011, pp. 2511–2520.
[12]
L.
E.
Potter,
J.
Araullo,
and
L.
Carter,
“The
leap
motion
controller: A view on sign language,” in Proceedings of the 25th
Australian Computer-Human Interaction Conference: Augmentation,
Application,
Innovation,
Collaboration,
ser.
OzCHI
’13.
New
York, NY, USA: ACM, 2013, pp. 175–178. [Online]. Available:
http://doi.acm.org/10.1145/2541016.2541072
[13]
N. Marquardt, R. Jota, S. Greenberg, and J. A. Jorge, “The continuous
interaction space: interaction techniques unifying touch and gesture on
and above a digital surface,” in IFIP TC 13 - Volume Part III, ser.
INTERACT’11.
Berlin, Heidelberg: Springer-Verlag, 2011, pp. 461–
476.
[14]
A. Brock, S. Lebaz, B. Oriola, D. Picard, C. Jouffrais, and P. Truillet,
“Kin’touch: understanding how visually impaired people explore tactile
maps,” in CHI ’12 Extended Abstracts on Human Factors in Computing
Systems, ser. CHI EA ’12.
New York, NY, USA: ACM, 2012, pp.
2471–2476.
[15]
G. Shoemaker, T. Tsukitani, Y. Kitamura, and K. S. Booth, “Body-
centric interaction techniques for very large wall displays,” in Proceed-
ings of the 6th Nordic Conference on Human-Computer Interaction:
Extending Boundaries, ser. NordiCHI ’10. New York, NY, USA: ACM,
2010, pp. 463–472.
[16]
W. Fikkert, P. van der Vet, G. van der Veer, and A. Nijholt, “Gestures
for large display control,” in Proceedings of the 8th international confer-
ence on Gesture in Embodied Communication and Human-Computer
Interaction, ser. GW’09.
Berlin, Heidelberg: Springer-Verlag, 2010,
pp. 245–256.
[17]
A. Reetz, C. Gutwin, T. Stach, M. Nacenta, and S. Subramanian,
“Superﬂick: a natural and efﬁcient technique for long-distance object
placement on digital tables,” in Proceedings of Graphics Interface 2006,
ser. GI ’06.
Toronto, Ont., Canada, Canada: Canadian Information
Processing Society, 2006, pp. 163–170.
[18]
P. Baudisch, E. Cutrell, D. Robbins, M. Czerwinski, P. Tandler, P. T,
B. Bederson, and A. Zierlinger, “Drag-and-pop and drag-and-pick:
techniques for accessing remote screen content on touch- and pen-
operated systems.”
Microsoft Research, 2003, pp. 57–64.
[19]
S. Doeweling and U. Glaubitt, “Drop-and-drag: easier drag & drop
on large touchscreen displays,” in Proceedings of the 6th Nordic
Conference on Human-Computer Interaction: Extending Boundaries,
ser. NordiCHI ’10.
New York, NY, USA: ACM, 2010, pp. 158–167.
53
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

