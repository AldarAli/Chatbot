Diffusion Recursive Least Square Adaptive Networks with Neighbor-Selection
Wael M. Bazzi
Electrical and Computer Engineering Department
American University in Dubai
Dubai, United Arab Emirates
Email: wbazzi@aud.edu
Vahid Vahidpour, Amir Rastegarnia, and Azam Khalili
Department of Electrical Engineering
Malayer University
Malayer 65719-95863, Iran
Email: {vahidpour,rastegarnia,khalili}@malayeru.ac.ir
Abstract—Constrained communication resources and limited
communication bandwidth are key issues for any task involv-
ing wireless sensor networks. This phenomenon motivated the
authors to examine diffusion networks where only a fraction
of neighbors participle in the communication process. In this
context, we modify the Diffusion Recursive Least Square (DRLS)
algorithm by allowing each node to receive intermediate esti-
mates from a subset of its neighbors, called neighbor-selection
DRLS. This results in signiﬁcant reduction in communication
overhead at the cost of some possible deterioration in the network
performance. We derive a theoretical expression for the steady
state Mean Square Deviation (MSD). Both numerical simulations
and theoretical ﬁndings are used to validate the effectiveness
of the proposed algorithm in providing a trade off between
communication burden and estimation performance.
Keywords–Adaptive network; diffusion; neighbor selection; re-
cursive least-squares.
I.
INTRODUCTION
Diffusion strategies are well-known techniques that enable
real-time learning and collaboration in adaptive networks [1]–
[3]. In these methods, information is gathered and processed
at all agents in a simultaneous fashion. This results in a
live sharing mechanism that ripples frequently through the
whole network [4]. Consequently, signiﬁcant improvements are
accrued in estimation performance of each network agent, in
comparison to the case in which nodes operate autonomously.
Notable properties of such networks are scalability and ro-
bustness to node/link failures. Power and bandwidth resources,
however, are the major constraints on performing a cooperative
task in an adaptive network. Communication is constrained by
the limited data transmission through radio links. Therefore,
the attained advantages of diffusion strategies in terms of inter-
node communications comes at an additional communication
cost [5].
Following on the discussion in the previous paragraph, it
is desirable to lower the level of internode communications as
much as possible, while maintaining the beneﬁts of coopera-
tion. There are some existing efforts related to reducing the
communication overheard, such as decreasing the dimension
of the estimates [6]–[8], selecting a subset of the entries of
the intermediate estimate vectors [9]–[12], and set membership
ﬁltering [13] [14]. In most earlier publications, it is assumed
that the degree of each node is ﬁxed and predeﬁned by the
network topology and, moreover, that every node senses data
that is affected by information diffused by all of its neighbors.
To the best of our knowledge, choosing a subset of neighboring
nodes was considered in [5], [15]–[19], but only in diffusion
least-mean-squares (LMS) networks. In this manuscript, we
consider the case where only a subset of agents participate in
the communication process. We focus on the scenario in [5], in
which every node consults with only a subset of its neighbors
and propose a novel reduced communication recursive least
square algorithm, called neighbor selection DRLS. In this
algorithm, which aims at further releasing the communication
density of DRLS, each node updates its estimate and sends
the intermediate estimate to only a subset of its neighbors.
Moreover, the total amount of internode communication in
the network is efﬁciently decreased with less performance
degradation in comparison to the diffusion LMS algorithm. We
derive a theoretical expression for the steady state MSD of the
Neighbor Selection DRLS algorithm and verify its accuracy
through numerical simulations.
The remainder of this paper is organized as follows: In
Section II, we recall a conventional DRLS algorithm and
formulate the proposed Neighbor Selection DRLS algorithm.
The performance analysis is examined in Section III. We
provide simulation results in Section IV and draw conclusions
in Section V.
Notation: We use plain lowercase letters to denote scalars,
lowercase bold letters to denote vectors and boldface uppercase
letters for matrices.
II.
ALGORITHM DESCRIPTION
A. Conventional Diffusion RLS
We consider a connected network of N nodes which aims
to determine an unknown vector, wo ∈ RM×1, in a distributed
manner. At every time instant i and each node k, scalar
measurements dk,i ∈ R are related to regression vectors,
uk,i ∈ R1×M, via the following linear regression model [20]:
dk,i = uk,iwo + vk,i
(1)
where vk,i denotes the additive noise process. The vector
wo denotes the parameter of interest that the agents wish to
identify.
We are then motivated to consider the following weighted
least square problem:
min
w ∥yk,i − Hk,iψψψ∥2
Λi
(2)
where yk,i and Hk,i are formed by stacking the history of
measurement and noise samples of node k up to time i as
follows:
yk,i = col {dk,i, . . . , dk,1, dk,0}
(3)
22
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-642-2
ICWMC 2018 : The Fourteenth International Conference on Wireless and Mobile Communications

Hk,i = col {uk,i, . . . , uk,1, uk,0}
(4)
where col {· · ·} denotes a column vector formed by stacking
its arguments on top of each other. The solution ψψψk,i from (2)
is given by [2]:
ψψψk,i =

III.
PERFORMANCE ANALYSIS
To proceed with the analysis, we make the following
assumptions:
Assumption:
1)
The regression data uk,i are temporally white and
spatially independent random variables with zero
mean and covariance matrix Ru,k ≜ E
h
uT
k,iuk,i
i
≥
0.
2)
The noise signal vk,i is temporally white and spatially
independent random variable with zero mean and
variance σ2
v,k.
3)
The regression data {um,i1}, and the model noise sig-
nals vn,i2, are mutually independent random variables
for all indexes {i1, i2, m, n}.
4)
For sufﬁciently large i, at any node k, we can replace
Pk,i and P−1
k,i with their expected values, E [Pk,i] and
E
h
P−1
k,i
i
, respectively.
5)
For a sufﬁciently large i, at any node k, we have
E [Pk,i] = E
h
P−1
k,i
i−1
A. Network update equation
Deﬁne M × 1 error vector as follows:
˜ψψψk,i ≜ wo − ψψψk,i
(16)
˜wk,i ≜ wo − wk,i
(17)
˜wi ≜ col { ˜w1,i, . . . , ˜wN,i}
(18)
Using the data model in (1) and subtracting wo from both
sides of the relation in (11), we get
˜ψψψk,i = ˜wk,i−1 − Pk,iuT
k,i [uk,i ˜wk,i−1 + vk,i]
(19)
Using the same procedure as stated in [12], the equation above
can be written in the following form:
˜ψψψk,i = λ ˜wk,i − (1 − λ) R−1
u,kuT
k,ivk,i
(20)
Moreover, subtracting both sides of (20) from wo gives
˜wk,i =
 
1 −
X
l∈Nk
alk,iclk
!
˜ψψψk,i +
X
l∈Nk
alk,iclk ˜ψψψl,i
(21)
which leads to
˜wi = λBi ˜wi−1 − BiΠsi
(22)
where
Π ≜ (1 − λ) diag
n
R−1
u,1, . . . , R−1
u,N
o
(23)
si ≜

uT
1,iv1,i, . . . , uT
N,ivN,i
	
(24)
Bi = Bi ⊗ IM
(25)
Bi =


B11,i
· · ·
B1N,i
...
...
...
BN1,i
· · ·
BNN,i


(26)
where
Bp,q,i =





1 − P
l∈Np apl,icpl
if q = p
apq,icpq
if q ∈ Np
0
otherwise
(27)
and ⊗ denotes the Kronecker product.
For any arbitrary symmetric nonnegative-deﬁnite matrix Σ,
using the alternative notation ∥x∥2
σσσ, where σσσ = vec {Σ}, to
refer to the weighted square quantity xT Σx and following
similar arguments to those in [3], we arrive at the following
variance relation:
E
h
∥ ˜wi∥2
σσσ
i
= E
h
∥ ˜wi−1∥2
λ2Φσσσ
i
+ vecT {G} Φσσσ
(28)
where
G = ΠE

sisT
i

Π
(29)
which, in view of the Assumptions, can be expressed as
G = (1 − λ)2 n
σ2
v,1R−1
u,1, . . . , σ2
v,NR−1
u,N
o
(30)
and
Φ = E
h
BT
i ⊗ BT
i
i
(31)
We arrive at the following expression for the network MSD
(η)
η = 1
N vecT {G} Φ

1
2
3
4
5
6
7
8
0
0.5
1
0
2
4
6
8
10
12
14
16
18
20
10
15
0
2
4
6
8
10
12
14
16
18
20
-25
-20
-15
Figure 1. Entries of wo, tr

Ru,k
	
, and
n
σ2
v,k
o
used in simulation.
0
500
1000
1500
-45
-40
-35
-30
-25
-20
-15
-10
-5
0
5
L = 0, Simulation
L = 1, Simulation
L = 3, Simulation
L = 5, Simulation
L = 7, Simulation
L = 0, Theory
L = 1, Theory
L = 3, Theory
L = 5, Theory
L = 7, Theory
Figure 2. Experimental and thoretical network MSD curves of the
neighbor-selection DRLS algorithm with different values K of when
λ = 0.95.
REFERENCES
[1]
F. S. Cattivelli and A. H. Sayed, “Diffusion LMS strategies for dis-
tributed estimation,” Signal Processing, IEEE Transactions on, vol. 58,
no. 3, 2010, pp. 1035–1048.
[2]
F. S. Cattivelli, C. G. Lopes, and A. H. Sayed, “Diffusion recursive
least-squares for distributed estimation over adaptive networks,” IEEE
Transactions on Signal Processing, vol. 56, no. 5, May 2008, pp. 1865–
1877.
[3]
A. H. Sayed, “Diffusion adaptation over networks,” Academic Press
Library in Signal Processing, vol. 3, 2013, pp. 323–454.
[4]
J. Chen and A. H. Sayed, “Diffusion adaptation strategies for distributed
optimization and learning over networks,” Signal Processing, IEEE
Transactions on, vol. 60, no. 8, 2012, pp. 4289–4305.
[5]
R. Arablouei, S. Werner, K. Dogancay, and Y.-F. Huang, “Analysis of a
reduced-communication diffusion LMS algorithm,” Signal Processing,
vol. 117, 2015, pp. 355–361.
[6]
M. O. Sayin and S. S. Kozat, “Single bit and reduced dimension dif-
fusion strategies over distributed networks,” Signal Processing Letters,
IEEE, vol. 20, no. 10, 2013, pp. 976–979.
[7]
——, “Compressive diffusion strategies over distributed networks for
reduced communication load,” Signal Processing, IEEE Transactions
on, vol. 62, no. 20, 2014, pp. 5308–5323.
[8]
S. Chouvardas, K. Slavakis, and S. Theodoridis, “Trading off com-
plexity with communication costs in distributed adaptive learning via
0
2
4
6
8
10
12
14
16
18
20
-48
-46
-44
-42
-40
-38
-36
-34
-32
L = 0
L = 3
L = 1
L = 5
L = 7
Figure 3. Theoretical and experimental steady-state MSDs of the
neighbor-selection DRLS algorithm at each node for different values of K
when λ = 0.99.
Krylov subspaces for dimensionality reduction,” Selected Topics in
Signal Processing, IEEE Journal of, vol. 7, no. 2, 2013, pp. 257–273.
[9]
R. Arablouei, S. Werner, Y.-F. Huang, and K. Dogancay, “Distributed
least mean-square estimation with partial diffusion,” Signal Processing,
IEEE Transactions on, vol. 62, no. 2, 2014, pp. 472–484.
[10]
R. Arablouei, K. Dogancay, S. Werner, and Y.-F. Huang, “Adaptive
distributed estimation based on recursive least-squares and partial
diffusion,” Signal Processing, IEEE Transactions on, vol. 62, no. 14,
2014, pp. 3510–3522.
[11]
V. Vahidpour, A. Rastegarnia, A. Khalili, W. M. Bazzi, and S. Sanei,
“Analysis of Partial Diffusion LMS for Adaptive Estimation Over
Networks with Noisy Links,” IEEE Transactions on Network Science
and Engineering, 2017.
[12]
V. Vahidpour, A. Rastegarnia, A. Khalili, and S. Sanei, “Analysis of
partial diffusion recursive least squares adaptation over noisy links,”
IET Signal Processing, 2017.
[13]
S. Gollamudi, S. Nagaraj, S. Kapoor, and Y.-F. Huang, “Set-membership
ﬁltering and a set-membership normalized LMS algorithm with an
adaptive step size,” Signal Processing Letters, IEEE, vol. 5, no. 5, 1998,
pp. 111–114.
[14]
J. R. Deller Jr, M. Nayeri, and S. F. Odeh, “Least-square identiﬁcation
with error bounds for real-time signal processing and control,” Proceed-
ings of the IEEE, vol. 81, no. 6, 1993, pp. 815–849.
[15]
O. L. Rørtveit, J. H. Husøy, and A. H. Sayed, “Diffusion LMS
with communication constraints,” in Signals, Systems and Computers
(ASILOMAR), 2010 Conference Record of the Forty Fourth Asilomar
Conference on.
IEEE, 2010, pp. 1645–1649.
[16]
C. G. Lopes and A. H. Sayed, “Diffusion adaptive networks with chang-
ing topologies,” in 2008 IEEE International Conference on Acoustics,
Speech and Signal Processing, 2008.
[17]
N. Takahashi and I. Yamada, “Link probability control for probabilistic
diffusion least-mean squares over resource-constrained networks,” in
Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE Inter-
national Conference on.
IEEE, 2010, pp. 3518–3521.
[18]
X. Zhao and A. H. Sayed, “Single-link diffusion strategies over adaptive
networks,” in Acoustics, Speech and Signal Processing (ICASSP), 2012
IEEE International Conference on.
IEEE, 2012, pp. 3749–3752.
[19]
J.-W. Lee, S.-E. Kim, and W.-J. Song, “Data-selective diffusion LMS
for reducing communication overhead,” Signal Processing, vol. 113,
2015, pp. 211–217.
[20]
A. H. Sayed, Fundamentals of Adaptive Filtering.
Wiley-IEEE Press,
Jun. 2003.
[21]
C. D. Meyer, Matrix analysis and applied linear algebra.
Siam, 2000,
vol. 2.
25
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-642-2
ICWMC 2018 : The Fourteenth International Conference on Wireless and Mobile Communications

