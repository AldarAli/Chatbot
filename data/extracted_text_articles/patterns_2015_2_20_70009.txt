A Review of Audio Features and Statistical Models Exploited for Voice Pattern Design
Ngoc Q. K. Duong
Technicolor
975 avenue des Champs Blancs
35576 Cesson S´evign´e, France
Email: quang-khanh-ngoc.duong@technicolor.com
Hien-Thanh Duong
Faculty of Information Technology
Hanoi University of Mining and Geology
Hanoi city, Vietnam
Email: duongthihienthanh@humg.edu.vn
Abstract—Audio ﬁngerprinting, also named as audio hashing,
has been well-known as a powerful technique to perform au-
dio identiﬁcation and synchronization. It basically involves two
major steps: ﬁngerprint (voice pattern) design and matching
search. While the ﬁrst step concerns the derivation of a robust
and compact audio signature, the second step usually requires
knowledge about database and quick-search algorithms. Though
this technique offers a wide range of real-world applications, to
the best of the authors’ knowledge, a comprehensive survey of
existing algorithms appeared more than eight years ago. Thus,
in this paper, we present a more up-to-date review and, for
emphasizing on the audio signal processing aspect, we focus our
state-of-the-art survey on the ﬁngerprint design step for which
various audio features and their tractable statistical models are
discussed.
Keywords–Voice pattern; audio identiﬁcation and synchroniza-
tion; spectral features; statistical models.
I.
INTRODUCTION
Real-time user interactive applications have emerged nowa-
days thanks to the increased power of mobile devices and their
Internet access speed. Let us consider applications like music
recognition [1][2], e.g., people hear a song in a public place
and they want to know more about it, or personalized TV
entertainment [3][4], e.g., people want to see more service and
related content on the Web in addition to the main view from
TV; both require a fast and reliable audio identiﬁcation system
in order to match the observed audio signal with its origin
stored in a large database. For these purposes, several research
directions have been studied, such as audio ﬁngerprinting [5],
audio watermarking [6], and timeline insertion [4]. While
watermarking and timeline approaches both require to embed
signature into the original media content, which is sometimes
inconvenient for the considered applications, ﬁngerprinting
technique allows directly monitoring the data for identiﬁcation.
Hence, audio ﬁngerprinting has been widely investigated in the
literature and already been deployed in many commercialized
products [1][7][8][9][10][11]. This technique has recently been
exploited for other applications such as media content syn-
chronization [12][13], multiple video clustering [14], repeating
object detection [15], and live version identiﬁcation [15].
A general architecture for an audio ﬁngerprinting system,
which can be used for either audio identiﬁcation or audio
synchronization purpose, is depicted in Fig. 1. The ﬁngerprint
extraction derives a set of relevant audio features followed by
an optional post-processing and feature modeling. Fingerprints
of the original audio collection and its corresponding metadata
(e.g., audio ID, name, time frame index, etc.) are systematically
stored in a database. Then given a short recording from the
user side, its feature vectors (i.e ﬁngerprints) are computed in
the same way as they were for the original data. Finally, a
searching algorithm will ﬁnd the best match between these
ﬁngerprints with those stored in the database so that the
recorded audio signal is labeled by the matched metadata.
Figure 1: General architecture of an audio ﬁngerprinting system.
In real-world recording, the audio signal often undergoes
many kinds of distortion: acoustical reverberation, background
noise addition, quantization error, etc. Thus, the derived ﬁn-
gerprints must be robust with respect to these various signal
degradations. Beside, the ﬁngerprint size should be as small
as possible to save memory resources and to allow real-
time matching. The details of general properties of the audio
ﬁngerprint was well-discussed in [1][16][17]. In order to fulﬁl
those requirements, audio sample signal is often transformed
into Time-Frequency (T-F) domain via the Short Time Fourier
transform (STFT) [16] where numerous distinguishable char-
acteristics such as high-level musical attributes, e.g., predom-
inant pitch, harmony structure, or low level spectral features,
e.g., mel-frequency cepstrum, spectral centroids, spectral note
onsets, etc., are exploited. To further compact the ﬁngerprints,
some approaches continue to ﬁt the spectral feature vectors
to a statistical model, e.g., Gaussian Mixture Model (GMM)
[18], Hidden Markov Model (HMM) [19], so that in the end
only the set of model parameters are used as ﬁngerprints.
32
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-393-3
PATTERNS 2015 : The Seventh International Conferences on Pervasive Patterns and Applications

Though diverse ﬁngerprinting algorithms have been pro-
posed in the literature, the number of review papers remains
limited where, to the best of the authors’ knowledge, a com-
prehensive review of ﬁngerprinting algorithms was presented
more than eight years ago [5][16], and a more recent survey
[20] only focusing on computer vision based approaches (e.g.,
methods proposed in [21][22]). In this paper, we present a
more up-to-date review of the domain, with particular focus
concerning the ﬁngerprint extraction block in Fig. 1, where
various audio spectral features and their statistical models are
summarized systematically. The presentation would particu-
larly beneﬁt new researchers in the domain and engineers in
the sense that they would easily follow the described steps to
implement different audio ﬁngerprints.
The structure of the rest of the paper is as follows. We ﬁrst
present a general architecture for ﬁngerprint design in Section
II, we then review various audio features, which have been
extensively exploited in the literature, in Section III. The detail
of some statistical feature models is introduced in Section IV.
Finally, we conclude in Section V.
II.
GENERAL ARCHITECTURE OF FINGERPRINT DESIGN
Fig. 2 depicts a general workﬂow of the ﬁngerprint design.
The purpose of each block is summarized as follows:
Figure 2: General workﬂow of the ﬁngerprint design.
•
Pre-processing: in this step, input audio signal is
often ﬁrst digitalized (if necessary), re-sampled to
a target sampling rate, and bandpass ﬁltered. Other
types of processing includes decorrelation and am-
plitude normalization [16]. Then the processed signal
is segmented into overlapping time frames where a
linear transformation, e.g., Fast Fourier Transform
(FFT), Discrete Cosine transform (DCT), or wavelet
transform [16], is applied to each frame. At this stage,
the input time-domain signal is represented in a feature
domain, and the most popular feature domain is time-
frequency representation given by the STFT.
•
Feature extraction: this is a major process since the
choice of ”which feature is used” will directly affect
the performance of the entire ﬁngerprinting system.
A great diversity of features have been investigated
targeting the reduction of dimensionality as well as
the invariance to various distortions. For summary,
most approaches ﬁrst map the linear time-frequency
representation given by the STFT to an auditory-
motivated frequency scale, i.e.,
Mel, Bark, Log, or
Cent scale, via ﬁlterbanks [2][23]. This mapping step
greatly reduces the spectrogram size since the number
of ﬁlterbanks is usually much smaller than the FFT
length. Then a feature vector such as Mel-Frequency
Cepstral Coefﬁcients (MFCC), spectral centroids of
all subbands, etc., are computed for each time frame.
In some systems, the ﬁrst and second derivatives
of the feature vectors are also integrated to better
track the temporal variation of audio signals [16][18].
Other types of feature that worth mentioning are e.g.,
time localized frequency peak [24], time-frequency
energy peak location [1], or those developed in im-
age processing based approaches such as top-wavelet
coefﬁcients computed on the spectral image [22] and
multiscale Gabor atoms extracted by Matching Pursuit
algorithm [14]. Recently, a general framework for dic-
tionary based feature learning has also been introduced
[25].
•
Post-processing: the feature vectors computed in the
previous step are often real-valued and the absolute
range depends on the signal power. Therefore when
Euclidean distance is used in the matching step, mean
substraction and component wise variance normaliza-
tion are recommended [26][27]. Another popular post-
processing is quantization where each entry of the
feature vectors is quantized to a binary number in
order to gain robustness against distortions and, more
importantly, to obtain memory efﬁciency [2][28][22]
[15]. In many existing system, ﬁngerprint is achieved
after this step.
•
Feature modeling: this block is sometimes deployed
in order to further compact the ﬁngerprint. In this
case, a large number of feature vectors along time
frames is ﬁtted to a statistical model so that an
input audio signal is well-characterized by the model
parameters, which are then stored as a ﬁngerprint
[29][18][30]. Popular model includes Gaussian Mix-
ture Model (GMM), Hidden Markov Model (HMM).
Other approaches used decomposition techniques, e.g.,
Non-negative MAtrix Factorization (NMF), to help
decreasing data dimension and therefore to reduce
the local statistical redundancy of the feature vectors
[31][32].
Since the pre-processing and post-processing steps are
quite straightforward, in the following of the paper we will
present more detail only on the feature extraction and the
feature modeling blocks.
III.
FEATURE EXTRACTION
Summarizing numerous types of audio features used for the
ﬁngerprint design so far will certainly go beyond the scope of
33
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-393-3
PATTERNS 2015 : The Seventh International Conferences on Pervasive Patterns and Applications

this paper. Thus in this section, we select to present the most
popular low level features in the spectral domain only.
A. MFCC
MFCC is one of the most popular feature considered in
speech recognition where the amplitude spectrum of input
audio signal is ﬁrst weighted by triangular ﬁlters spaced
according to the Mel scale, and DCT is then applied to
decorrelate the Mel-spectral vectors. MFCC was shown to
be applicable for music signal also in [33]. Examples of
ﬁngerprinting algorithms used MFCC feature are found in
[33][18]. In [34], MFCC was used also for clustering and
synchronizing large scale audio-video sequences recorded by
multiple users during an event. Matlab implementations for the
computation of MFCC are available [35][36].
B. Spectral Energy Peak (SEP)
SEP for music identiﬁcation systems was described in
[37][1] where a time-frequency point is considered as a peak
if it has higher amplitude than its neighboring points. SEP is
argued to be intrinsically robust to even high level background
noise and can provide discrimination in sound mixtures [38]. In
well-known Shazam’s system [1] time-frequency coordinates
of the energy peaks was described as sparse landmark points.
Then by using pairs of landmark points rather than single
points, the ﬁngerprints exploited the spectral structure of sound
sources. This landmark feature can also be found in [14]
and [39] for multiple video clustering. Ramona et al. used
start times of the spectral energy peaks, referred to as onsets,
for the automatic alignment of audio occurrences in their
ﬁngerprinting system [23][40].
C. Spectral Band Energy (SBE)
Together with spectral peak, SBE has been widely ex-
ploited in ﬁngerprinting algorithms. Let us denote by s(n, f) a
STFT coefﬁcient of an audio signal at time frame index n and
frequency bin index f, 1 ≤ f ≤ M. Let us also denote by b
an auditory-motivated subband index, i.e., in either Mel, Bark,
Log, or Cent scale, and lb and hb the lower and upper edges
of b-th subband. SBE is then computed, with normalization,
in each time frame and each frequency subband range by
F SBE
n,b =
Phb
f=lb |s(n, f)|2
PM
f=1 |s(n, f)|2 .
(1)
Haitsma et al. proposed a famous ﬁngerprint in [2] where
SBEs were ﬁrst computed in a block containing 257 time
frames and 33 Bark-scale frequency subbands, then each F SBE
n,b
was quantized to a binary value (either 0 or 1) based on its dif-
ferences compared to neighboring points. Other ﬁngerprinting
algorithms exploiting SBE feature were found for instance in
[41][18]. Variances of this subband energy difference feature
can be found in more recent approaches [28][21].
D. Spectral Flatness Measure (SFM)
SFM, also known as Wiener entropy, relates to the tonality
aspect of audio signals and it is therefore often used to
distinguish different recordings. SFM is computed in each
time-frequency subband point (n, b) as
F SFM
n,b =

A. GMM-based ﬁngerprint
GMM has been used to model the spectral shape of audio
signals in many different applications ranging from speaker
identiﬁcation [43] to speech enhancement [30], etc. It was also
investigated for audio ﬁngerprinting by Ramalingam and Kr-
ishnan [18], where spectral feature vectors Fn are modeled as
a multidimensional K-state Gaussian mixture with probability
density function (pdf) given by
p(Fn) =
K
X
k=1
αkNc(Fn|µk, Σk)
(5)
where αk, which satisﬁes PK
k=1 αk = 1, µk and Σk are the
weight, the mean vector and the covariance matrix of the k-th
state, respectively, and
Nc(Fn|µk, Σk) =
1
|πΣk|e−(Fn−µk)HΣ−1
k (Fn−µk)
(6)
where
H and |.| denote conjugate transpose and determi-
nant of a matrix, respectively. The model parameters θ =
{αk, µk, Σk}k are then estimated in the maximum likelihood
(ML) sense via the expectation-maximization (EM) algorithm,
which is well-known as an appropriate choice in this case, with
the global log-likelihood deﬁned as
LML =
N
X
n=1
log p(Fn|θ).
(7)
As a result, the parameters are iteratively updated via two
EM steps as follow:
•
E-step: compute the posterior probability that feature
vector Fn is generated from the k-th GMM state
γnk =
αkp(Fn|µk, Σk)
PK
l=1 αlp(Fn|µl, Σl)
.
(8)
•
M-step: update the parameters
αk = 1
N
N
X
n=1
γnk
(9)
µk =
PN
n=1 γnkFn
PN
n=1 γnk
(10)
Σk =
PN
n=1 γnk(Fn − µk)(Fn − µk)H
PN
n=1 γnk
.
(11)
With GMM, N d-dimensional feature vectors Fn are char-
acterized by K set of GMM parameters {αk, µk, Σk}k=1,...,K
where K is often very small compared to N. However,
since GMM does not explicitly model the amplitude variation
of sound sources, signals with different amplitude level but
similar spectral shape may result in different estimated mean
and covariance templates. To overcome this issue, another
version of GMM called spectral Gaussian scaled mixture
model (GSMM) could be considered instead. Though GSMM
has been used in speech enhancement [30] and audio source
separation [44], it has yet been applied in the context of
ﬁngerprinting.
B. HMM-based ﬁngerprint
HMM is a well-known model in many audio processing
applications [45]. When applied for audio ﬁngerprinting, pdf
of the observed feature vector Fn can be written as
p(Fn) =
X
q1,q2,...,qd
πq1bq1(Fn,1)aq1q2bq2(Fn,2)
...aqd−1qdbqd(Fn,d)
(12)
where πqi denotes the probability that qi is the initial state,
aqiqj is state transition probability, and bqi(Fn,i) is pdf for a
given state.
Given a sequence of observations Fn, n = 1, ..., N ex-
tracted from a labeled audio signal, the model parameters
θ = {πqi, aqiqj, bqi}i,j are learned via e.g., EM algorithm
(detail formulation can be found in [45]) and stored as a ﬁn-
gerprint. Cano et al. modeled MFCC feature vectors by HMM
in their AudioDNA ﬁngerprint system [29]. In [19] HMM-
based ﬁngerprint was shown to achieve a high compaction by
exploiting structural redundancies on music and to be robust
to distortions.
Note that when applying GMM or HMM for the ﬁngerprint
design, a captured signal at the user side is considered to be
matched with an original signal ﬁngerprinted by the model
parameter θ in the database if its corresponding feature vectors
bFn are most likely generated by θ.
C. NMF-based ﬁngerprint
NMF is well-known as an efﬁcient decomposition tech-
nique which helps reducing data dimension [46]. It has been
widely considered in audio and music processing, especially
for audio source separation [47][48]. When applying in the
context of audio ﬁngerprinting, a d × N matrix of the feature
vectors V = [F1, ..., FN] is approximated by
V = WH
(13)
where W and H are non-negative matrices of size d × Q and
Q × N, respectively, modeling the spectral characteristics of
the signal and its temporal activation, and Q is much smaller
than N. The model parameters θ = {W, H} can be estimated
by minimizing the following cost function:
C(θ) =
X
bn
dIS ([V]b,n|[WH]b,n) ,
(14)
where dIS (x|y)
=
x
y − log x
y − 1 is Itakura-Saito (IS)
divergence, and [A]b,n denotes an entry of matrix A at b-
th row and n-th column. The resulting multiplicative update
(MU) rules for parameter estimation write [49]:
H ← H ⊙
WT 
(WH).−2 ⊙ V

WT (WH).−1
(15)
W ← W ⊙

(WH).−2 ⊙ V

HT
(WH).−1 HT
(16)
where ⊙ denotes the Hadamard entrywise product, A.p being
the matrix with entries [A]p
ij, and the division is entrywise.
Fingerprints are then generated compactly from the basis
matrix W, which has much smaller size compared to the
feature matrix V.
35
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-393-3
PATTERNS 2015 : The Seventh International Conferences on Pervasive Patterns and Applications

NMF was applied to the spectral subband energy matrix
in [32] and to the MFCC matrix in [50]. The resulting ﬁn-
gerprint was shown to better identify audio clips than another
decomposition technique namely singular value decomposition
(SVD).
V.
CONCLUSION
In this paper, we presented a review of the existing audio
ﬁngerprinting systems which have been developed by numer-
ous researchers during the last decade for a range of practical
applications. We described a variety of audio features and
reviewed state-of-the-art approaches exploiting them for the
ﬁngerprint design. Furthermore, the use of statistical models
and decomposition techniques to reduce the global statistical
redundancy of feature vectors, and therefore to decrease ﬁnger-
print size, was also summarized. As a result, the combination
of different presenting features and/or the deployment of a
statistical feature model afterward are both applicable to obtain
a robust and compact audio signature.
REFERENCES
[1]
A. L.-C. Wang, “An industrial-strength audio search algorithm,” in Proc.
Int. Sym. on Music Information Retrieval (ISMIR), 2003, pp. 1–4.
[2]
J. Haitsma and T. Kalker, “A highly robust audio ﬁngerprinting system,”
in Proc. Int. Sym. on Music Information Retrieval (ISMIR), 2002.
[3]
M. Fink, M. Covell, and S. Baluja, “Social- and interactive-television.
applications based on real-time ambient-audio identiﬁcation,” in Proc.
European Interactive TV Conference (Euro-ITV), 2006.
[4]
C. Howson, E. Gautier, P. Gilberton, A. Laurent, and Y. Legallais,
“Second screen tv synchronization,” in Proc. IEEE Int. Conf. on
Consumer Electronics (ICCE), 2011.
[5]
P. Cano, E. Batlle, T. Kalker, and J. Haitsma, “A review of algorithms
for audio ﬁngerprinting,” in IEEE Workshop on Multimedia Signal
Processing, 2002, pp. 169–173.
[6]
H.J.Kim, Y.H.Choi, J.W.Seok, and J.W.Hong, “Audio watermarking
techniques,” in Intelligent Watermarking Techniques, 2004, ch. 8, pp.
185–218.
[7]
R. Macrae, X. Anguera, and N. Oliver, “Muvisync: Realtime music
video alignment,” in Proc. IEEE Int. Conf. on Multimedia and Expo
(ICME), 2010.
[8]
T. website, “http://www.tvplus.com/.”
[9]
I.-N. application from Yahoo, “http://www.intonow.com/ci/soundprint.”
[10]
M.-S. website, “http://media-sync.tv/.”
[11]
C. website, “http://www.civolution.com.”
[12]
N. Q. K. Duong, C. Howson, and Y. Legallais, “Fast second screen TV
synchronization combining audio ﬁngerprint technique and generalized
cross correlation,” in Proc. IEEE International Conference on Consumer
Electronics-Berlin (ICCE-Berlin), 2012, pp. 241–244.
[13]
N. Q. K. Duong and F. Thudor, “Movie synchronization by audio
landmark matching,” in Proc. IEEE Int. Conf. on Acoustics, Speech,
and Signal Processing (ICASSP), 2013, pp. 3632–3636.
[14]
C. V. Cotton and D. P. W. Ellis, “Audio ﬁngerprinting to identify
multiple videos of an event,” in Proc. IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing (ICASSP), 2010, pp. 2386–2389.
[15]
Z. Raﬁi, B. Coover, and J. Han, “An audio ﬁngerprinting system for
live version identiﬁcation using image processing techniques,” in Proc.
IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP),
2014.
[16]
P. Cano, E. Batlle, T. Kalker, and J. Haitsma, “A review of audio
ﬁngerprinting,” Journal of VLSI Signal Processing, no. 3, 2005, pp.
271–284.
[17]
G. Richard, S. Sundaram, and S. Narayanan, “An overview on percep-
tually motivated audio indexing and classiﬁcation,” Proceedings of the
IEEE, no. 9, 2013, pp. 1939–1954.
[18]
A. Ramalingam and S. Krishnan, “Gaussian mixture modeling of short-
time fourier transform features for audio ﬁngerprinting,” IEEE Trans.
on Information Forensics and Security, no. 4, 2006, pp. 457–463.
[19]
E. Batlle, J. Masip, E. Guaus, and P. Cano, “Scalability issues in
hmm-based audio ﬁngerprinting,” in Proc. IEEE Int. Conference on
Multimedia and Expo, 2004, pp. 735–738.
[20]
V. Chandrasekhar, M. Shariﬁ, and D. A. Ross, “Survey and evaluation
of audio ﬁngerprinting schemes for mobile query-by-example applica-
tions,” in 12th International Society for Music Information Retrieval
Conference (ISMIR), 2011, pp. 801–806.
[21]
Y. Ke, D. Hoiem, and R. Sukthankar, “Computer vision for music
identiﬁcation,” in Proc. Int. Conf. on Computer Vision and Pattern
Recognition (CVPR), 2005, pp. 597–604.
[22]
S. Baluja and M. Covell, “Audio ﬁngerprinting: Combining computer
vision and data stream processing,” in Proc. IEEE Int. Conf. on
Acoustics, Speech, and Signal Processing (ICASSP), 2007.
[23]
M. Ramona and G. Peeters, “Automatic alignment of audio occurrences:
application to the veriﬁcation and synchronization of audio ﬁngerprint-
ing annotation,” in Proc. DAFX, 2011, pp. 429–436.
[24]
E. Dupraz and G. Richard, “Robust frequency-based audio ﬁnger-
printing,” in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal
Processing (ICASSP), 2010, pp. 2091–2094.
[25]
M. Moussallam and L. Daudet, “A general framework for dictionary
based audio ﬁngerprinting,” in Proc. IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing (ICASSP), 2014, pp. 3077–3081.
[26]
J. S. Seo, M. Jin, S. Lee, D. Jang, S. Lee, and C. D. Yoo, “Audio
ﬁngerprinting based on normalized spectral subband centroids,” in Proc.
IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP),
2005, pp. 213–216.
[27]
K. Seyerlehner, M. Schedl, P. Knees, and R. Sonnleitner, “A reﬁned
block-level feature set for classiﬁcation, similarity and tag prediction,”
in Proc. Music Information Retrieval Evaluation eXchange (MIREX),
2011.
[28]
X. Anguera, A. Garzon, and T. Adamek, “Mask: Robust local features
for audio ﬁngerprinting,” in Proc. IEEE International Conference on
Multimedia and Expo (ICME), 2012, pp. 455–460.
[29]
P. Cano, E. Batlle, H. Mayer, and H. Neuschmied, “Robust sound
modeling for song detection in broadcast audio,” in Proc. 112th Audio
Engineering Society Convention (AES), 2002.
[30]
J. Hao, T.-W. Lee, and T. J. Sejnowski, “Speech enhancement using
gaussian scale mixture models,” IEEE Trans. on Audio Speech and
Language Processing, no. 18, 2010, pp. 1127–1136.
[31]
C. J. Burges, J. C. Platt, and S. Jana, “Distortion discriminant analysis
for audio ﬁngerprinting,” IEEE Trans. on Audio Speech and Language
Processing, no. 3, 2003, pp. 165–174.
[32]
J. Deng, W. Wan, X. Yu, and W. Yang, “Audio ﬁngerprinting based on
spectral energy structure and nmf,” in Proc. Int. Conf. on Communica-
tion Technology (ICCT), 2011, pp. 1103–1106.
[33]
B. Logan, “Mel frequency cepstral coefﬁcients for music modeling,” in
Proc. Int. Sym. on Music Information Retrieval (ISMIR), 2002.
[34]
A. Bagri, F. Thudor, A. Ozerov, and P. Hellier, “A scalable framework
for joint clustering and synchronizing multi-camera videos,” in Proc.
European Signal Processing Conference (EUSIPCO), 2013, pp. 1–5.
[35]
W. 1, “http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html.”
[36]
W. 2, “https://engineering.purdue.edu/malcolm/interval/1998-010/.”
[37]
C. Yang, “Macs: music audio characteristic sequence indexing for
similarity retrieval,” in Proc. IEEE Workshop on Applications of Signal
Processing to Audio and Acoustics (WASPAA), 2001, pp. 123–126.
[38]
J. Ogle and D. Ellis, “Fingerprinting to identify repeated sound events
in long-duration personal audio recordings,” in Proc. IEEE Int. Conf.
on Acoustics, Speech, and Signal Processing (ICASSP), 2011, pp. 233–
236.
[39]
N. J. Bryan, P. Smaragdis, and G. J. Mysore, “Clustering and syn-
chronizing multi-camera video via landmark cross-correlation,” in Proc.
IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP),
2012, pp. 2389–2392.
[40]
M. Ramona and G. Peeters, “Audio identiﬁcation based on spectral
modeling of bark-bands energy and synchronization through onset
detection,” in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal
Processing (ICASSP), 2011, pp. 477–480.
36
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-393-3
PATTERNS 2015 : The Seventh International Conferences on Pervasive Patterns and Applications

[41]
E. Allamanche, J. Herre, and O. Hellmuth, “Content-based identiﬁcation
of audio material using mpeg-7 low level description,” in Proc. Int. Sym.
on Music Information Retrieval (ISMIR), 2002.
[42]
J. Herre, E. Allamanche, and O. Hellmuth, “Robust matching of audio
signals using spectral ﬂatness features,” in Proc. IEEE Workshop on
Applications of Signal Processing to Audio and Acoustics (WASPAA),
2001, pp. 127–130.
[43]
D. Reynolds and R. Rose, “Robust text-independent speaker identiﬁca-
tion using gaussian mixture speaker models,” IEEE Trans. on Speech
and Audio Processing, no. 1, 1995, pp. 72–83.
[44]
L. Benaroya, F. Bimbot, and R. Gribonval, “Audio source separation
with a single sensor,” IEEE Trans. on Audio, Speech and Language
Processing, vol. 14, no. 1, 2006, pp. 191–199.
[45]
L. R. Rabiner, “A tutorial on hmm and selected applications in speech
recognition,” Proceeding of the IEEE, vol. 17, no. 2, 1989, pp. 257–286.
[46]
D. D. Lee and H. S. Seung, “Learning the parts of objects with non-
negative matrix factorization,” Nature, vol. 401, 1999, pp. 788–791.
[47]
D. El Badawy, N. Q. K. Duong, and A. Ozerov, “On-the-ﬂy audio
source separation,” in IEEE Int. Workshop on Machine Learning for
Signal Processing (MLSP), 2014, pp. 1–6.
[48]
N. Q. K. Duong, A. Ozerov, L. Chevallier, and J. Sirot, “An interactive
audio source separation framework based on nonnegative matrix factor-
ization,” in IEEE Int. Conf. on Acoustics, Speech, and Signal Processing
(ICASSP), 2014, pp. 1586–1590.
[49]
C. F´evotte, N. Bertin, and J.-L. Durrieu, “Nonnegative matrix factor-
ization with the Itakura-Saito divergence. With application to music
analysis,” Neural Computation, vol. 21, no. 3, 2009, pp. 793–830.
[50]
N. Chen, H.-D. Xiao, and W. Wan, “Audio hash function based on non-
negative matrix factorisation of mel-frequency cepstral coefﬁcients,”
IET Information Security, no. 1, 2011, pp. 19–25.
37
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-393-3
PATTERNS 2015 : The Seventh International Conferences on Pervasive Patterns and Applications

