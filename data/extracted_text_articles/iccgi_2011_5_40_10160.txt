Compiler-based Differentiation of Numerical Simulation Codes
Michel Schanen, Michael F¨orster, Boris Gendler, Uwe Naumann
LuFG Informatik 12: Software and Tools for Computational Engineering
RWTH Aachen University
Aachen, Germany
{schanen,foerster,bgendler,naumann}@stce.rwth-aachen.de
Abstract—Based on algorithmic differentiation, we present
a derivative code compiler capable of transforming imple-
mentations of multivariate vector functions into a program
for computing derivatives. The resulting values are accurate
up to machine precision compared to the common numerical
approximation by ﬁnite differences. This paper gives a short
mathematical background of algorithmic differentiation while
focusing on the user’s perspective of applying derivative gen-
eration tools on an already implemented code. This process is
illustrated by a one dimensional implementation of Burgers’
equation in a generic optimization setting using for example
Newton’s method. In this implementation, ﬁnite differences are
replaced by the computation of adjoints, thus saving an order
of magnitude in terms of computational complexity.
Keywords-Algorithmic Differentiation; Source Transforma-
tion; C/C++; Optimization; Numerical Simulation;
I. INTRODUCTION
A typical problem in ﬂuid dynamics is given by the
continuous Burgers equation [1]
∂u
∂t + u∂u
∂x = ν ∂2u
∂x2
,
(1)
describing shock waves moving through gases. u denotes
the velocity ﬁeld of the ﬂuid with viscosity ν. Similar
governing equations represent the core of many numerical
simulations. Such simulations are often subject to various
optimization techniques involving derivatives. Thus, Burg-
ers’ equation will serve as a case study for a compiler-based
approach to the accumulation of the required derivatives.
Suppose we solve the differential equation in (1) by
discretization using ﬁnite differences on a equidistant one-
dimensional grid with nx points. For given initial conditions
ui,0 with 0 < i ≤ nx we simulate a physical process by inte-
grating over nt time steps according to the leapfrog/DuFort-
Frankel scheme presented in [2]. At time step j we compute
ui,j+1 for time step j + 1 according to
ui,j+1 = ui,j−1 − ∆t
∆x (ui,j (ui+1,j − ui−1,j))
+ 2∆t
∆x2 (ui+1,j − (ui,j+1 + ui,j−1) + ui−1,j) ,
(2)
where ∆t is the time interval and ∆x is the distance
between two grid points. In general, if the initial conditions
ui,0 cannot be accurately measured, they are essentially
replaced by approximated values. To improve their accuracy
additional observed values uob ∈ Rnx×nt are taken into
account. The discrepancy between observed values uob
i,j and
simulated values ui,j are evaluated by the cost function
y = 1
2
nx
X
i=1
nt
X
j=1
(ui,j − uob
i,j)2
,
(3)
allows us to obtain improved estimations for the initial
conditions by applying, for example, Newton’s method [3] to
solve the data assimilation problem with Burgers’ equation
as constraints [4]. The single Newton steps are repeated until
the residual cost y undercuts a certain threshold.
In Section II, we introduce algorithmic differentiation as
implemented by our derivative code compiler dcc cover-
ing both the tangent-linear as well as the adjoint model.
Section III provides a user’s perspective on the application
of dcc. Higher-order differentiation models are discussed
in Section IV. Finally, the results of our case study are
discussed in Section V.
II. ALGORITHMIC DIFFERENTIATION
The minimization of the residual is implemented by
resorting to Newton’s second-order method for mini-
mization. In general, Newton’s method may be applied
to arbitrary differentiable multivariate vector functions
y = F (x) : Rn → Rm. This algorithm heavily depends on
the accurate and fast computation of Jacobian and Hessian
values, since one iterative step xi → xi+1 is computed by
xi+1 = xi − ∇2F(xi)−1 · ∇F(xi)
.
(4)
The easiest method of approximating partial derivatives
∇xiF uses the ﬁnite difference quotient
∇xiF(x) ≈ F (x + h · ei) − F (x)
h
,
(5)
for the Cartesian basis vector ei ∈ Rn and with x ∈ Rn,
h → 0. In order to accumulate the Jacobian of a multivari-
ate function the method is rerun n times to perturb each
component of the input vector x. The main advantage of
this method resides in its straightforward implementation;
no additional changes to the code of the function F are nec-
essary. However, the derivatives accumulated through ﬁnite
differences are only approximations. This represents a major
105
ICCGI 2011 : The Sixth International Multi-Conference on Computing in the Global Information Technology
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-139-7

drawback for codes that simulate highly nonlinear systems,
resulting in truncation and cancellation errors or simply
providing wrong results. In particular by applying the Taylor
expansion to the second-order centered difference quotient
we derive a machine precision induced approximation error
of
ǫ
h2 , with ǫ being the rounding error.
Algorithmic differentiation (AD) [5] solves this prob-
lem analytically, changing the underlying code to compute
derivatives by applying symbolic differentiation rules to
individual assignments and using the chain rule to propagate
derivatives along the ﬂow of control. The achieved accuracy
only depends on the machine’s precision ǫ. There exist
two distinct derivative models, differing in the order of
application of the associative chain rule. Let ∇F be the
Jacobian of F. The tangent-linear code
F

The important advantage of the adjoint model is that by
evaluating (10) only once we obtain the full gradient
∂y
∂x
in ¯xi = ¯vi for i = 0, . . . , n − 1. To achieve this we have
to initialize (¯xi)i=0,...,n−1 with zero and ¯y with one. As
mentioned above ¯x must be zero because it occurs not only
on the left-hand side in (7) and y is initialized with the value
of the Cartesian basis vector in R.
In (8), we assumed that the input code is given as a
SAC. This is an oversimpliﬁcation in terms of real codes.
The adjoint code has to deal with the fact that real code
variables are overwritten frequently. One way to simulate the
predicate of unique intermediate variables is to store certain
left-hand side variables on a stack during the augmented
forward section. Candidates for storing on the stack are
those variables that are being overwritten and are required
for later use during the computation of the local gradients
and associated adjoints. Before evaluating the corresponding
adjoint assignment in the reverse section the values are
restored from the stack.
For illustration purposes we consider Listing 1 show-
ing
an
implementation
of
the
non-linear
reduction
y(x) = Qn−1
i=0 sin(xi). dcc parses only functions with void
as a return type (line 1). All inputs and return values are
passed through the arguments, which in turn only consist
of arrays (called by pointers) and scalar values (called by
reference). Additionally we may pass an arbitrary number of
integer arguments by value or by reference. We assume that
all differentiable functions are implemented using values of
type double. Therefore, only variables of type double are
directly affected by the differentiation process.
1
void
f ( in t n ,
double ∗x ,
double &y )
2
{
3
in t
i =0;
4
y=0;
5
for ( i =0; i<n ; i ++) {
6
y=y∗ sin ( x [ i ] ) ;
7
}
8
}
Listing 1: dcc input code.
Using the command line dcc f.c -t, we instruct the
compiler to use the tangent-linear (-t) mode in order to
generate the function t1 f (tangent-linear, 1st-order version
of f) presented in Listing 2. The original function arguments
x and y are augmented with their associated tangent-linear
variables t1 x and t1 y. Inside a driver program this code has
to be rerun n times letting the input vector t1 x range over
the Cartesian basis vectors in Rn to accumulate the entire
gradient. Listing 3 shows how to use the generated code of
Listing 2 in a driver program. Lines 2 and 5 let input variable
t1 x range over the Cartesian basis vectors. By setting t1 x[ i ]
to 1 the function t1 f (line 3) computes the partial derivative
of y with respect to x[ i ].
The command line dcc f.c -a tells dcc to apply
the adjoint mode (-a) to f.c. The result is the function
1
void t1
f ( in t n ,
double∗ x ,
double∗ t1 x
2
, double& y ,
double& t1 y )
3
{
4
. . .
5
for ( in t
i =0; i<n ; i ++) {
6
y=y∗ sin ( x [ i ] ) ;
7
t1 y=t1 y∗ sin ( x [ i ] ) +y∗cos ( x [ i ] ) ∗t1 x [ i ] ;
8
}
9
. . .
10
}
Listing 2: Tangent-linear version of f as generated by dcc
1
for ( in t
i =0;
i<n ; i ++) {
2
t1 x [ i ]=1;
3
t1
f (n , x ,
t1 x , y ,
t1 y ) ;
4
gradient [ i ]= t1 y ;
5
t1 x [ i ]=0;
6
}
Listing 3: Driver for t1 f
a1 f (adjoint, 1st-order version of f) shown in Listing 4.
As in the tangent-linear case each function argument is
augmented by an associated adjoint component, here a1 x
and a1 y. As mentioned above we need a stack in the
adjoint code for storing data during the forward section.
The augmented forward section uses stacks to store values
that are being overwritten and to store the control ﬂow. The
actual implementation of the stack is not under consideration
here; therefore we replaced the calls to the stacks with macro
deﬁnitions for better readability. By default, dcc generates
code that uses static arrays which ensures high runtime
performance. There are three different stacks used in the
adjoint code. The stack called CS is for storing the control
ﬂow, FDS takes ﬂoating point values and IDS keeps integer
values. The unique identiﬁer of the two basic blocks [9] in
the forward section are stored in lines 6 and 9. For example,
after evaluating the augmented forward section of Listing 4,
the stack CS contains the following sequence
0, 1, . . ., 1
| {z }
n times
(11)
In line 10, variable y is stored onto the stack because it
is overwritten in each iteration although needed in line 21.
Hence, we restore the value of y in line 20. For the same
reason we store and restore the value of i in line 11 and
19. The reverse section consist of a loop that processes
the control ﬂow stack CS. The basic block identiﬁers are
restored from the stack and depending on the value, the
corresponding adjoint basic block is executed. For example,
the sequence given in (11) as content in the CS stack leads
to a n-times evaluation of the adjoint basic block one and
afterward one evaluation of the adjoint basic block zero.
The basic block one in line 9 to 11 has the corresponding
adjoint basic block in line 19 to 22. In contrast to (7), in
107
ICCGI 2011 : The Sixth International Multi-Conference on Computing in the Global Information Technology
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-139-7

1
void a1 f ( in t n ,
double∗ x ,
double∗ a1 x ,
2
double& y ,
double& a1 y )
3
{
4
in t
i =0;
5
/ /
augmented forward
section
6
CS PUSH(0 ) ;
7
y=0;
8
for
(
i =0;
i<n ;
i ++) {
9
CS PUSH(1 ) ;
10
FDS PUSH( y ) ;
y=y∗ sin ( x [ i ] ) ;
11
IDS PUSH( i ) ;
12
}
13
/ /
reverse
section
14
while (CS NON EMPTY) {
15
i f
(CS TOP==0) {
16
a1 y=0;
17
}
18
i f
(CS TOP==1) {
19
IDS POP( i ) ;
20
FDS POP( y ) ;
21
a1 x [ i ]+=y∗cos ( x [ i ] ) ∗a1 y ;
22
a1 y=sin ( x [ i ] ) ∗a1 y ;
23
}
24
CS POP;
25
}
26
}
Listing 4: Adjoint dcc output
line 22 the adjoint a1 y is not incremented but assigned.
This is due to the fact that y is on both hand sides of
the original assignment in line 10. This brings an aliasing
effect into play. This effect can be avoided with help of
intermediate variables; making this code difﬁcult to read.
For that reason we show the adjoint assignment without
intermediate variables. dcc generates adjoint assignments
with intermediate variables and incrementation of the left-
hand side as shown in (7). The dcc-generated code and the
one shown here are semantically equivalent. To accumulate
the gradient using the function a1 f, we again have to write
a driver, presented in Listing 5. It is sufﬁcient to initialize
the adjoint variable a1 y and call the adjoint function a1 f
only once to get the whole gradient (line 2), illustrating the
reduced runtime complexity of the adjoint mode.
1
a1 y=1;
2
a1 f (n ,
x , a1 x , y ,
a1 y ) ;
3
for ( in t
j =0;
j<n ; j ++)
4
gradient [ j ]= a1 x [ j ] ;
Listing 5: Driver for a1 f
IV. HIGHER ORDER DIFFERENTIATION
Numerical optimization algorithms often involve higher-
order derivative models. Thus, the need for Hessians is
imminent. With this in mind, dcc was designed to generate
higher-order derivative codes effortlessly using its reappli-
cation feature. dcc is able to generate jth-order derivative
code by reading (j −1)th-order derivative code as the input.
In this section we will focus on second-order models.
The tangent-linear mode reapplied to the ﬁrst-order
tangent-linear code (6) with m = 1 for scalar functions
yields the second-order tangent-linear code
˙F

n
250
500
1000
2000
f (s)
0.03
0.08
0.15
0.32
TLM (s)
33
109
457
1615
ADJ (s)
0.21
0.43
0.85
1.82
TLM-ADJ (s)
150
587
2286
8559
IDS size
7500502
15001002
30002002
60004002
FDS size
5000002
10000002
20000002
40000002
CS size
7500503
15001003
30002003
60004003
Table I: Time and memory requirements for gradient com-
putation
Hessian ∇2Fi, ˙x must be set to the i-th Cartesian basis
vector. As such, we have to rerun this model n times in
order to accumulate the whole Hessian, yielding only a
linear increase in runtime complexity of O(n) · cost (F).
The
desired
dcc
command
is
dcc -a -d 2
t1_foo.cpp resulting in the ﬁle a2_t1_foo.cpp. The
option -a instructs dcc to generate adjoint code.
V. CASE STUDY
As discussed in Section I, we run a test case on an inverse
problem based on Burgers’ equation (1). As a start we take
the code presented in [2] implementing the original function
with the signature of
1
void
f ( i n t
n ,
i n t
nt ,
double& cost , double∗∗
u ,
double∗ ui . . . )
2
{
3
. . .
4
}
Listing 6: Signature of Burgers’ function
Taking n grid points of ui as the initial conditions we
integrate over nt timesteps. The values are saved in the two
dimensional array u for each grid point i and time step j.
To solve the inverse problem we need the derivatives of
cost with respect to the initial conditions ui.
The results in Table I represent the runtime of one full
gradient accumulation as well as the memory requirements
in adjoint and tangent-linear mode. Additionally one Hessian
accumulation is performed using the tangent-linear over
adjoint model (13). Different problem sizes are simulated
with varying n. We also mention the different stack size
shown in Section III.
If we assume four bytes per integer and control stack
element plus eight bytes for a ﬂoating data stack element
we end up with a memory requirement of ≈ 610 MB for
the Hessian accumulation. The tests were running on a
GenuineIntel computer with Intel(R) Core(TM)2 Duo CPU
and with 2000.000 MHz cpu.
The execution time of the tangent-linear gradient compu-
tation is growing proportionally to the problem size nx and
the execution time of f:
FM : cost(F ′)
cost(F) ∼ O(n).
The single execuiton of t1 f takes approximately twice so
much as the execution of f.
The execution time of the adjoint gradient computation is
growing only proportional to the execution time of f:
AM : cost(F ′)
cost(F) ∼ O(1).
Finally we accumulate the Hessian using tangent-linear
over adjoint mode. Here, the runtime is growing linearly
with respect to n as well as f since the dimension of the
dependent cost is equal to 1.
FM − AM : cost(F ′′)
cost(F) ∼ O(n).
For scalar functions in particular, the runtime complexity
for accumulating the Hessian using AD is the same as the
runtime complexity of the gradient accumulation using ﬁnite
difference. This enables developers to implement a second-
order model where a ﬁrst-order model has been used so far.
VI. OUTLOOK & CONCLUSION
We have presented a source transformation compiler for
a restricted subset of C/C++. As such, dcc runs on any
system with a valid C/C++ compiler making it a very
portable tool. Its unique reapplication feature allows code
to be transformed up to any order of differentiation.
Additionally, several extensions were implemented. As
these programs run on cluster systems, they often rely on
parallelization techniques. The most widely used paralleliza-
tion method is MPI. Hence, there is a need for adjoint MPI
enabled code [10]. This feature has been integrated into dcc
using an adjoint MPI library [11]. Additionally there are
attempts to achieve the same goal with OpenMP [12]. For
the sake of brevity we did not mention the program analysis
dcc performs. For better efﬁciency, dcc uses activity and
TBR analyses [13].
REFERENCES
[1] D. Zwillinger, Handbook of Differential Equations, 3rd ed.
Boston, MA: Academic Press, 1997.
[2] E. Kalnay, “Atmospheric modeling, data assimilation and
predictability,” 2003.
[3] T. Kelley, Solving nonlinear equations with Newton’s method,
ser. Fundamentals of Algorithms.
Philadelphia, PA: Society
for Industrial and Applied Mathematics (SIAM), 2003.
[4] A. Tikhonov, “On the stability of inverse problems,” Dokl.
Akad. Nauk SSSR, vol. 39, no. 5, pp. 195–198, 1943.
[5] A. Griewank and A. Walter, Evaluating Derivatives. Prin-
ciples and Techniques of Algorithmic Differentiation (2nd
Edition).
Philadelphia: SIAM, 2008.
[6] G. Corliss and A. Griewank, Eds., Automatic Differentiation:
Theory, Implementation, and Application, ser. Proceedings
Series.
Philadelphia: SIAM, 1991.
109
ICCGI 2011 : The Sixth International Multi-Conference on Computing in the Global Information Technology
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-139-7

[7] G. Corliss, C. Faure, A. Griewank, L. Hasco¨et, and U. Nau-
mann, Eds., Automatic Differentiation of Algorithms – From
Simulation to Optimization.
New York: Springer, 2002.
[8] U.
Naumann,
“Dag
Reversal
is
NP-complete,”
Journal
of
Discrete
Algorithms,
vol.
7,
no.
4,
pp.
40–410,
2009.
[Online].
Avail-
able:
http://www.sciencedirect.com/science/article/B758J-
4THC1FD-2/2/7ddfc2eab484bbe184d4dcdf16d8e58a
[9] A. Aho, M. Lam, R. Sethi, and J. Ullman, Compilers.
Principles, Techniques, and Tools (Second Edition). Reading,
MA: Addison-Wesley, 2007.
[10] P. Hovland and C. Bischof, “Automatic differentiation for
message-passing parallel programs,” in Parallel Processing
Symposium, 1998. IPPS/SPDP 1998. Proceedings of the First
Merged International ... and Symposium on Parallel and
Distributed Processing 1998, mar-3 apr 1998, pp. 98 –104.
[11] M. Schanen, U. Naumann, and M. F¨orster, “Second-order
adjoint algorithmic differentiation by source transformation
of mpi code,” in Recent Advances in the Message Passing
Interface, Lecture Notes in Computer Science.
Springer,
2010, pp. 257–264.
[12] OpenMP Architecture Review Board, “OpenMP Application
Program Interface,” Speciﬁcation, 2008. [Online]. Available:
http://www.openmp.org/mp-documents/spec30.pdf
[13] L. Hasco¨et, U. Naumann, and V. Pascual, “To-be-recorded
analysis in reverse mode automatic differentiation,” Future
Generation Computer Systems, vol. 21, pp. 1401–1417, 2005.
110
ICCGI 2011 : The Sixth International Multi-Conference on Computing in the Global Information Technology
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-139-7

