Towards Makespan Minimization Task Allocation in Data Centers
Kangkang Li, Ziqi Wan, Jie Wu, and Adam Blaisse
Department of Computer and Information Sciences
Temple University
Philadelphia, Pennsylvania, 19122
Emails: {kang.kang.li, ziqi.wan, jiewu, adam.blaisse}@temple.edu
Abstract—Nowadays, data centers suffer from resource limita-
tions in both the limited bandwidth resources on the links and
the computing capability on the servers, which triggers a variety
of resource management problems. In this paper, we discuss
one classic resource allocation problem: task allocation in data
centers. That is, given a set of tasks with different makespans,
how to schedule these tasks into the data center to minimize the
average makespan. Due to the tradeoff between locality and load
balancing, along with the multi-layer topology of data centers,
it is extremely time consuming to obtain an optimal result. To
deal with the multi-layer topology, we ﬁrst study a simple case
of one-layer cluster and discuss the optimal solution. After that,
we propose our hierarchical task allocation algorithm for multi-
layer clusters. Evaluation results prove the high efﬁciency of our
algorithm.
Keywords–Task allocation; Data centers; Makespan.
I.
INTRODUCTION
Modern data centers comprise tens of thousands of comput-
ers interconnected between each other with commodity switch-
es. Nowadays, data centers suffer from resource limitations
in both the limited bandwidth resources on the links and the
computing capability on the servers, which triggers a variety
of resource management problems [1–6]. One classic issue is
the task allocation problem, which involve various constraints,
including performance, network, and cost.
Generally speaking, a task usually consists of two parts:
the computation workloads and the communication trafﬁc.
Different tasks running in the data center will compete for
computing capability on the servers and bandwidth resources
on the links. Obviously, to minimize the duration of commu-
nication trafﬁc, locality is one important factor that we need to
take into consideration. That is, we need to place the tasks as
close to each other as possible, in order to lower the hops of
communication between tasks. Furthermore, the tasks running
in the same server will not need communication trafﬁc between
each other due to the internal communication within the server.
However, the bandwidth capacity of each link is limited.
More tasks running under the same link will lead to the lower
average bandwidth allocated to each task, which will decrease
the communication speed and increase the communication
duration. What is worse, the computing capability of a server
is limited. If tasks are packed together, it will also reduce the
computing capability allocated to each task. With the degraded
computation speed, the computation workloads will take more
time to complete.
On the other hand, if we apply load-balancing and allo-
cate the tasks evenly to the servers, the computing speed of
each task can be maximized [6]. However, the geographically
separated tasks need more hops to communicate with each
other. With the limited bandwidth resources on the links, load-
balancing will considerably lengthen the communication time.
Therefore, there is a tradeoff between the locality and load
balancing.
The remainder of the paper is organized as follows: In
Section II, we introduce some related work. In Section III, we
present the task model discussed in this paper. In Section IV,
we formulate the task allocation problem in the data center.
In Section V, we study the simple case of a one-layer cluster
and discuss the optimal solution. Section VI focuses on the
multi-layer cluster and gives the hierarchical task allocation
algorithm. Section VII conducts the simulations to validate the
efﬁciency of our algorithm. Finally, conclusions are in Section
VIII.
II.
RELATED WORK
Task allocation has been an open research topic since the
traditional distributed computing era. Many task allocation
issues are NP-hard, thus, we need to ﬁnd a good heuristic
algorithm to solve the problem, such as the ﬁrst-ﬁt and best-ﬁt
greedy algorithm used by Xu and Fortes [3].
Take the Mapreduce jobs as an example. One Mapreduce
job consists of three tasks: map, shufﬂe and reduce. Many
MapReduce schedulers have been proposed to try maximizing
the resource utilization in the shared MapReduce clusters.
Zaharia et al. [7] introduced delay scheduling that specu-
latively postpones the scheduling of the head-of-line tasks
and ameliorate the locality degradation in the default Hadoop
Fair scheduler. In addition, Zaharia et al. [8] also proposed
Longest Approximate Time to End (LATE) scheduling policy
to mitigate the deﬁciency of Hadoop scheduler in coping
with the heterogeneity across virtual machines in a cloud
environment. Ahmad et al. [9] proposed a communication-
aware placement and scheduling of MapTasks and predictive
load-balancing for ReduceTasks to reduce the network trafﬁc
of Hadoop on heterogeneous clusters.
Virtual machine placement is similar to the task allocation
problem under the environment of cloud computing. As data
centers are becoming the mainframe of cloud services, the
virtual machine placement problem in data centers has been an
open research area, considering both the network and servers.
Piao et al. [1] gave a heuristic algorithm to satisfy both the
70
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-338-4
CLOUD COMPUTING 2014 : The Fifth International Conference on Cloud Computing, GRIDs, and Virtualization

communication demands and physical resource restrictions.
Meng et al. [2] proposes minimizing the trafﬁc cost through
virtual machine placement. Their objective is to place virtual
machines that have large communication requirements close to
each other, so as to reduce network capacity needs in the data
center.
Oktopus [4] uses the hose model to abstract the tenant’s
bandwidth request, including both virtual cluster and over-
subscribed virtual clusters. They propose a virtual machine
placement algorithm to deal with homogeneous bandwidth
demands. The virtual cluster provides tenants with guarantees
on the network bandwidth they demand, which, according
to Popa et al. [5], can be interpreted as the min-guarantee
requirements. However, this min-guarantee fails to consider
the potential growth of a tenant’s network demand. In order
to alleviate this problem, our previous work [6] proposed the
concept of elasticity, favoring the on-demand scaling feature
of cloud computing.
Our previous work [6] designed a recursive abstraction
scheme and hierarchical virtual machine placement algorithm,
which is similar to the task allocation scheme in this paper.
However, this paper focus on the objective of task makespan
minimization and does not consider the environment of cloud
computing. Furthermore, we focus on the study of tradeoff be-
tween locality and load balancing when doing task allocation.
The communication model between tasks is different from the
hose model for virtual machines’ communications used in [6].
III.
TASK MODEL
In our model, each task can be separated into three parts:
the pre-computation part, the computation part and the post-
computation part, as shown in Figure 1. Here, we study the
homogeneous task inputs. That is, all the tasks share the
same pre-computation, communication, and post-computation
workloads. To normalize these different types of workloads,
we consider the situation that there are two tasks running in
the data center and they are allocated to two servers under
the same switch. Then we deﬁne the normalized time for each
step as the time those two task go through each step, which
is noted as α, 2β, and γ time units, respectively. Here we use
2β for two tasks communicating at the same time. If there is
only one task fully using the bandwidth resource during the
communication period, then its communication time is β.
Obviously, when α + γ ≫ β, the communication time
can be neglected. Then the best choice is load-balancing
and to evenly divide the input tasks into the servers. On
the other hand, when α + γ ≪ β, the pre-computation and
post-computation time can be neglected. Therefore, the best
allocation scheme is to try to put all tasks into one server to
minimize the communication cost.
We assume that all the tasks will start their pre-computation
part at the same time. After ﬁnishing the pre-computation
part, a taski will try to communicate with another taskj.
However, due to the different completion times of the pre-
computation parts of different tasks, taskj might not ﬁnish
its pre-computation part. Then, taski must wait for the taskj
to complete its pre-computation, and then, they can carry on
the communication part with each other. After the communi-
cation part is ﬁnished, taski will start to complete its post-
Communication
Pre-Compute
Post-Compute
Figure 1: Task model
computation. Thus, the total makespan of a single taski is the
sum of pre-computation time, waiting time, communication
time, and post-computation time.
IV.
PROBLEM FORMULATION
In this section, we formulate the average makespan mini-
mization task allocation problem in data centers with the topol-
ogy of a multi-layer binary tree. The data center conﬁguration
is semi-homogeneous, as shown in Figure 2. Each server has
the same computing capability of C. Also, each link of the
same layer has the same bandwidth capacity: Lk (the kth layer
link bandwidth capacity). However, the upper layer links have
twice the bandwidth capacities than the lower layer links, i.e.,
L1 = 2L2 = 4L3. The links capacities only differ between
layers, we refer to this as the semi-homogeneous conﬁguration,
which is widely used to ease upper-layer link congestion. Our
objective is to minimize the average makespan of all the input
tasks, which can be expressed below:
makespan(i) = pre(i)+wait(i)+commun(i)+post(i) (1)
makespan =
PN
i makespan(i)
N
(2)
In Equation (2), makespan(i) is the makespan of taski,
pre(i) is the pre-computation time of taski, wait(i) is the
waiting time between pre-computation and communication,
commun(i) is the communication time of taski, post(i) is
the post-computation time of taski. N is the total number of
the tasks running in the system. Therefore, we tried to design a
good task allocation scheme to minimize the average makespan
of the input tasks.
Considering that the server’s computing capability is steady
and limited, then the speed of computation is inversely pro-
portional to the number of tasks computed at this time on this
server.
Considering that the bandwidth is equally shared by dif-
ferent tasks communicating through the link, then the commu-
nication speed is also inversely proportional to the number of
tasks communicating between the servers.
In this paper, we study a simple case, in which the
bandwidth resources are equally allocated to each task. In that
case, given the the link bandwidth capacity B, the bandwidth
allocated to each task is B
x , where x is the number of tasks
go through that link.
Take one server to analyze. Assume there are x tasks in the
server. Take f1(i, x) to be the time it takes taski to ﬁnish its
ﬁrst step in the server, where x is the number of tasks running
on this server. Then we have:
f1(1, x) = f1(2, x) = · · · = f1(x, x) = xα
(3)
71
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-338-4
CLOUD COMPUTING 2014 : The Fifth International Conference on Cloud Computing, GRIDs, and Virtualization

L2
L1
Tree
Core
Aggregation
Access
L3
L2
L3
Servers
Figure 2: Tree-based network topology
Switch
Physical 
Machine
Computing Capability: C
Link Capacity:  L
Request <N Tasks>
Figure 3: One-layer cluster
Equation (3) indicates that ﬁrst step ﬁnish time of the ﬁrst
task ﬁnished in the ﬁrst step is proportional to the the number
of tasks running on the server. This is due to the fact that, tasks
on the same server evenly sharing the computation resources.
It also shows the ﬁnish time of the ﬁrst step is equal for every
task when the pre-computation workload is identical.
Deﬁne f2(t, x) as the number of tasks that have ﬁnished
step 1 in the server at the time point of t. The we can get
f2(t, x) =
0
if t < αx
x
if t ≥ αx
(4)
t is the time from the the start of the pre-communication.
V.
A ONE-LAYER CLUSTER STUDY
With N tasks accepted into the cluster, consider the simple
case of two servers with a switch above them, as shown in
Figure 3. Since all the tasks are homogeneous, we assume that,
without loss of generality, we place x tasks in the left server,
and leave N − x tasks to the right server, and x ≥ N − x.
Due to the symmetry of binary-trees, there are, at most, ⌈ N
2 ⌉
different ways to allocate the tasks into two servers as the
value of x increase from ⌈ N
2 ⌉ to N.
Let p(t, x) be the probability of a task available for a
communication step at time point t, with x tasks in the server.
p(t, x) = f2(t, x)
x
(5)
10B
10B
5B
5B
1GHz
5B
5B
10B
10B
1GHz
1GHz
1GHz
2GHz
2GHz
 Abstraction
Figure 4: Abstraction process
We assume a task will randomly choose another task to
communicate with. Then the probability of taski and taskj,
both being available to communicate with each other at time
point t is pi(t)×pj(t). In the one-layer binary tree model, there
are only 2 possible relationships of taski and taskj, either in
the same server, or in the different servers. Let f3(t, x) be the
number of tasks able to start communication at time t in this
one-layer cluster, then we can ﬁgure out:
f3(t, x) = x × p(t, x) + (N − x) × p(t, N − x)
(6)
The internal trafﬁc of tasks running on the same server will
introduce no communication cost. If two tasks communicate
in the same server, then their communication time will be
ignored. Therefore, our focus is located on the trafﬁc between
tasks on different servers. Since the allocated bandwidth of
tasks on the two servers might be different, we adopt the
minimal one as the bottleneck of communication. However, if
there are only two servers in the cluster, all the trafﬁc outside
the servers go through both sides of the switch. Then the
outbound link bandwidth will be equally shared for both links,
when they have the same communication capacities.
Let f4(t, x) be the number of tasks the have ﬁnished the
communication part at time t, with x tasks in the left server,
and N−x tasks in the right server. The average number of tasks
communicating between the two servers at time t is f3(t, x)−
f4(t, x). For a taski communicating with a task on the other
server, let tsc(i) be the communication starting time of taski,
tfc(i) be the communication ﬁnishing time of taski. Then we
can get the constrains as follow
f3(tsc(i), x) = i
(7)
f4(tfc(i), x) = i
(8)
Z tfc(i)
tsc(i)
1
f3(t, x) − f4(t, x)dt =
Z β
0
1
1dt = β
(9)
Let f5(t, x) be the number of tasks that have ﬁnished all
the three parts ( pre-computation, communication and post-
computation) at time t. We further assume that f5L(t, x) is
the number of tasks that have ﬁnished on the left server at
time t, and let f5R(t, x) be the number of tasks that have
ﬁnished on the right server at time t. Similarly, we can split
f4(t, x) into f4L(t, x) and f4R(t, x). Likewise, let tspc(i) be
72
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-338-4
CLOUD COMPUTING 2014 : The Fifth International Conference on Cloud Computing, GRIDs, and Virtualization

the post-computation starting time of taski, and let tfpc(i) be
the post-computation time of taski, and taskj can be either
in the left or right server. Then, we can get
f4L(t, x) = f4(t, x) × x
N
(10)
f4R(t, x) = f4(t, x) × (N − x)
N
(11)
Z tfpc(i)
tspc(i)
1
f4L(t, x) − f5L(t, x)dt =
Z γ
0
1
1dt = γ
(12)
or
Z tfpc(i)
tspc(i)
1
f4R(t, x) − f5R(t, x)dt =
Z γ
0
1
1dt = γ
(13)
f5L(t, x) = f5L(t, x) + f5R(t, x)
(14)
Apparently, by solving f5(t, x) = i, i = 1, 2, ..., N, we
can get the ﬁnish time of each task, say t1(x), t2(x), ..., tN(x).
Then the object is to minimize the average makespan of tasks,
which is t1(x)+t2(x)+...+tN(x)
N
. Thus, given a one-layer cluster
with 2 servers and N tasks, traverse all the possibilities to
partition the N tasks into 2 servers. Since two servers are
identical, we just need to choose x from ⌈ N
2 ⌉ to N for one
server, and N − x for the other one. Since all the tasks are
identical, then the complexity will be O(N). Then, we choose
the best number to be put in the left server as x, remaining
N − x tasks will be put in the right server in the one-layer
cluster.
VI.
MULTI-LAYER CLUSTER STUDY
Given a binary tree multi-layer cluster with M servers and
N task requests, we can get the optimal allocation scheme by
traversing all the possibilities to partition the N tasks into M
servers; however, it will be extremely time-consuming. Due
to the NP-hardness of this problem [4], there is no optimal
solution in polynomial time. However, based on the optimal
results of the one-layer cluster, we can generalize this solution
to multi-layer clusters, which has a considerably low time
complexity.
For the bottom-layer access switches, we can view them as
the root of a one-layer binary cluster, and try to abstract it into
a single node. Each server under the access switch shares the
same computing capability of C, and the sum of computing
capabilities under the access switch is 2C. Since the upper-
layer links have twice the bandwidth resources as the lower
layers; therefore, we can view this 2C as the accumulative
computing capability of the abstraction node.
Based on this abstraction of the bottom-layer switch, we
are able to abstract the entire multi-layer cluster to a one-layer
cluster in a similar way. For each layer’s switch connecting
two sub-trees from the bottom to top, we can view it as the
root of a one-layer binary cluster, and try to abstract it into
a single node. Upon reaching the root switch at the top, the
whole multi-layer cluster is abstracted into a one-layer cluster.
We can see that our abstraction process misses no information.
For abstraction nodes with the same accumulative computing
capability, the inner structure of the original sub-trees are the
Algorithm 1 Hierarchical Task Allocation Algorithm
Input: The links capacity and servers computing capability;
Task requests ⟨N⟩
1: for layer i=1 to N do
2:
for all switches in layer i do
3:
Calculate the accumulative capacity for each switch
connecting two sub-trees in the layer
4: if input tasks could be accepted then
5:
for layer j=N to 1 do
6:
for all switches in layer j do
7:
Optimally allocate the given number of tasks to
this subtree
same. We can see that, in Fig. 4, two servers with two lower-
layer links are abstracted into a single node of accumulative
computing capability of 2GHz. Both abstraction nodes with a
computing capability of 2GHz share the same conﬁguration of
the original abstracted one-layer sub-tree.
1) Computing capability Sharing of Multi-layer Clusters:
The accumulative computing capability of the abstraction node
is equally divided by all tasks allocated in this node.
2) Bandwidth Sharing of Multi-layer Clusters: For multi-
layer clusters, the upper-layer link is connecting with a subtree
with multiple servers. The link bandwidth would be equally
divided by all tasks under that sub-tree.
3) Hierarchical Task Allocation Algorithm: With the ab-
straction of a multi-layer cluster into a one-layer cluster, we
can use the optimal solution for a one-layer cluster. Based
on that result, we propose our hierarchical task placement
algorithm.
Given N input tasks, our algorithm can be divided into two
steps. First, for each switch at each layer from bottom to top,
the accumulative computing capability of the abstraction node
rooted at that switch is calculated.
Second, for each switch connecting two sub-trees at each
layer from top to bottom, recursively allocate the input tasks
into the its two sub-trees according to our optimal one-layer so-
lution. Upon ﬁnishing the bottom-layer switch (access switch),
all the tasks are allocated into the servers. We summarize our
algorithm in Algorithm 1.
Our algorithm takes two loops. The ﬁrst is the abstraction
from bottom layer to top, and the second is the allocation
from top to bottom. For the ﬁrst loop, each switch at each
layer is abstracted. For a K layers cluster, the total number of
switch is PK−1
k=1 k. Suppose we have M servers at the bottom,
then M = 2K. Therefore, it takes O(M) for the ﬁrst loop of
abstraction process. In the second loop, for each layer, our
algorithm takes O(N) time to calculate the optimal solution,
based on the discussion in Section V. In the K-layer cluster,
it takes O(KN) for the allocation process. In sum, the total
time complexity of our algorithm is O(KN + M), which is
very efﬁcient.
VII.
EVALUATIONS
In this section, we evaluate our proposed algorithm in the
case of a 2-layer binary tree data center. We made comparisons
73
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-338-4
CLOUD COMPUTING 2014 : The Fifth International Conference on Cloud Computing, GRIDs, and Virtualization

0
2
4
6
8
10
12
14
16
10
15
20
25
30
35
40
45
50
55
60
65
70
75
80
85
average makespan /s
number of tasks
 optimal
 proposal
 random
(a) α = 2, β = 2, γ = 15
0
2
4
6
8
10
12
14
16
20
30
40
50
60
70
80
90
100
110
120
130
140
150
160
170
180
average makespan /s
number of tasks
 optimal
 proposal
 random
(b) α = 15, β = 2, γ = 15
0
2
4
6
8
10
12
14
16
40
60
80
100
120
140
160
180
200
220
240
260
280
300
320
340
360
380
400
average makespan /s
number of tasks
 optimal
 proposal
 random
(c) α = 45, β = 2, γ = 15
Figure 5: Performance comparisons of average makespan vs. the value of α
0
2
4
6
8
10
12
14
16
20
30
40
50
60
70
80
90
100
110
120
130
140
150
160
170
180
190
average makespan /s
number of tasks
 optimal
 proposal
 random
(a) α = 15, β = 1, γ = 15
0
2
4
6
8
10
12
14
16
20
30
40
50
60
70
80
90
100
110
120
130
140
150
160
170
180
average makespan /s
number of tasks
 optimal
 proposal
 random
(b) α = 15, β = 2, γ = 15
0
2
4
6
8
10
12
14
16
20
30
40
50
60
70
80
90
100
110
120
130
140
150
160
170
180
average makespan /s
number of tasks
 optimal
 proposal
 random
(c) α = 15, β = 4, γ = 15
Figure 6: Performance comparisons of average makespan vs. the value of β
with our proposed algorithm with the optimal brute force
algorithm and the random allocation algorithm. We produce the
optimal solution by programs that traverse all the possibilities
dividing the N inputs into M servers. The random allocation
algorithm is generated by putting random number of tasks into
different servers.
We evaluate the average makespan of our algorithm under
three groups of simulations on the average total completion
time. As shown in Figures 5, 6 and 7, our proposed algorithm
is very close to the optimal one, which is much better than the
random algorithm.
A. Simulation Settings
• Group 1: We select the α to be 2, 15 and 45 separately.
And we set β, γ as 2 and 15. The bandwidth of links in
each layer is equal. Task numbers go from 1 to 15.
• Group 2: We select the β to be 1, 2 and 4 separately. We
also set α, γ as 15 and 15. The bandwidth of links in
each layer is equal. Task numbers go from 1 to 15.
• Group 3: We select the γ to be 2, 15 and 45 separately.
We also set β, α as 2 and 15. The bandwidth of links in
each layer is equal. Task numbers go from 1 to 15.
B. Simulation Results
The results for the three groups of simulations are shown
in Figures 5, 6, and 7. From those, we can see that when the
number of tasks grows, the proposed algorithm’s performance
will deviate from that of the optimal one. However, the pro-
posed solution does not deviate far from the optimal solution,
and still follows the growing pattern of the optimal solution.
Besides that, we still have the following observations:
1) For different pre-computation times (α), as shown in Fig-
ure 5, when the pre-computation time grows, the proposed
algorithm’s performance will deviate farther from that of
the optimal one.
2) For different communication times (β), as shown in Fig-
ure 6, when the communication time grows, the difference
between the proposed algorithm’s average makespan and
that of the optimal one will grow slowly.
3) For different post-computation times (γ), as shown in
Figure 7, when the post-computation time grows, the the
proposed algorithm’s performance will be closer to that
of the optimal one.
74
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-338-4
CLOUD COMPUTING 2014 : The Fifth International Conference on Cloud Computing, GRIDs, and Virtualization

0
2
4
6
8
10
12
14
16
10
20
30
40
50
60
70
80
90
100
110
120
average makespan /s
number of tasks
 optimal
 proposal
 random
(a) α = 15, β = 2, γ = 2
0
2
4
6
8
10
12
14
16
20
30
40
50
60
70
80
90
100
110
120
130
140
150
160
170
180
average makespan /s
number of tasks
 optimal
 proposal
 random
(b) α = 15, β = 2, γ = 15
0
2
4
6
8
10
12
14
16
40
60
80
100
120
140
160
180
200
220
240
260
280
300
320
average makespan /s
number of tasks
 optimal
 proposal
 random
(c) α = 15, β = 2, γ = 45
Figure 7: Performance comparisons of average makespan vs. the value of γ
As it is shown, the post-computation does not have the
same inﬂuence as the pre-computation time or communica-
tion time does. One reason is that the post-computation is
independent with the waiting time. Notably, the waiting time
can inevitably increase the makespan of a task. Therefore, it
makes sense for the pre-computation time to contribute more
deviation than the post-computation time does. The smaller
portion of the pre-computation time the better performance will
be. The other reason is that our proposed algorithm will loss
some information about the bandwidth during the abstraction
step. Also, the more congestion in the bandwidth resources,
the larger the difference between the proposed solution and
the optimal solution will be. However, even the performance
varies with the ratio of α : β : γ, our proposed algorithm is
still much better than the random one.
VIII.
CONCLUSION AND FUTURE WORK
In this paper, we study the classic task allocation problem
in the data center, which is now suffering from both the limited
bandwidth resources and computing capabilities. Compared
to the previous work, we focus on the tradeoff between
locality and load balancing. To minimize the makespan of input
tasks into the multi-layer data center, we ﬁrst study the one-
layer cluster and discuss the optimal solution. After that, we
propose our hierarchical task allocation algorithm to deal with
the multi-layer cluster. The evaluation results show the high
efﬁciency of our algorithm.
In this paper, we only study the homogeneous task model
under the semi-homogeneous data center conﬁguration. In
our future work, we will ﬁrst extend the task model into
heterogeneous settings. That is, each task will have different
values of α, β, and γ.
Furthermore, the heterogeneous conﬁguration of data cen-
ters will also be studied. We will consider two sets of het-
erogeneous data center conﬁgurations. The ﬁrst is that, we
let the servers’ computing capabilities heterogeneous, while
keeping the links capacities semi-homogeneous as before. The
second will be that, we let all the links capacities, along
with the servers’ computing capabilities heterogeneous. We
will evaluate the efﬁciency of algorithm both theoretically and
experimentally.
REFERENCES
[1] J. T. Piao and J. Yan, “A network-aware virtual machine placement
and migration approach in cloud computing,” in Grid and Cooperative
Computing (GCC), 2010 9th International Conference on, pp. 87–92, Nov
2010.
[2] X. Meng, V. Pappas, and L. Zhang, “Improving the scalability of data
center networks with trafﬁc-aware virtual machine placement,” in INFO-
COM, 2010 Proceedings IEEE, pp. 1–9, March 2010.
[3] J. Xu and J. A. B. Fortes, “Multi-objective virtual machine placement in
virtualized data center environments,” in Green Computing and Commu-
nications (GreenCom), 2010 IEEE/ACM Int’l Conference on Int’l Confer-
ence on Cyber, Physical and Social Computing (CPSCom), pp. 179–188,
Dec 2010.
[4] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron, “Towards pre-
dictable datacenter networks,” in Proceedings of the ACM SIGCOMM
2011 Conference, SIGCOMM ’11, (New York, NY, USA), pp. 242–253,
ACM, 2011.
[5] L. Popa, A. Krishnamurthy, S. Ratnasamy, and I. Stoica, “Faircloud:
Sharing the network in cloud computing,” in Proceedings of the 10th
ACM Workshop on Hot Topics in Networks, HotNets-X, (New York, NY,
USA), pp. 22:1–22:6, ACM, 2011.
[6] K. Li, J. Wu, and A. Blaisse, “Elasticity-aware virtual machine placement
for cloud datacenters,” in Cloud Networking (CloudNet), 2013 IEEE 2nd
International Conference on, pp. 99–107, Nov 2013.
[7] M. Zaharia, D. Borthakur, J. Sen Sarma, K. Elmeleegy, S. Shenker, and
I. Stoica, “Delay scheduling: A simple technique for achieving locality
and fairness in cluster scheduling,” in Proceedings of the 5th European
Conference on Computer Systems, EuroSys ’10, (New York, NY, USA),
pp. 265–278, ACM, 2010.
[8] Z. Guo and G. Fox, “Improving mapreduce performance in heterogeneous
network environments and resource utilization,” in Cluster, Cloud and
Grid Computing (CCGrid), 2012 12th IEEE/ACM International Sympo-
sium on, pp. 714–716, May 2012.
[9] F. Ahmad, S. T. Chakradhar, A. Raghunathan, and T. N. Vijaykumar,
“Tarazu: Optimizing mapreduce on heterogeneous clusters,” SIGARCH
Comput. Archit. News, vol. 40, pp. 61–74, Mar. 2012.
75
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-338-4
CLOUD COMPUTING 2014 : The Fifth International Conference on Cloud Computing, GRIDs, and Virtualization

