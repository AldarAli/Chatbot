Using Neural Networks and Feature Selection Algorithms in the Identiﬁcation of
Protein Signatures for the Prediction of Alzheimer’s Disease
Lara Dantas and Mˆeuser Valenc¸a
Polytechnic School of Pernambuco
University of Pernambuco
Recife, Brazil
Email: {ldc, meuser}@ecomp.poli.br
Abstract—Alzheimer’s Disease is now considered the most com-
mon type of dementia in the population. Although, it is a degener-
ative and irreversible disease, if diagnosed early, medications may
be administered to slow the progression of symptoms and provide
a better quality of life for the patient. Ray et al., and G`omez
and Moscato conducted studies with classiﬁers contained in the
software Weka using a database with values of 120 blood proteins,
and they noticed that they could classify the patient may or may
not be diagnosed with AD with an accuracy rate of 93% and
65%, respectively. Thus, this study aims to use neural networks
such as Multi-layer Perceptron, Extreme-learning Machine and
Reservoir Computing to perform early diagnosis of a patient with
or without AD. This article also envisions to utilize the Random
Forest Algorithm to select proteins from the original set and,
therefore, create a new protein signature. Through experiments it
can be concluded that the best performance was obtained with the
Multi-layer Perceptron and the new signatures created achieved
better results than those available in the literature.
Keywords–Neural Networks, Alzheirmer’s Disease, Feature Se-
lection Algorithms.
I.
INTRODUCTION
Most developed countries are undergoing a major de-
mographic shift. The oldest segments of the population are
growing at a faster rate, and therefore, there is a constant in-
crease in age-related diseases, especially progressive dementia
disorders. First described by psychiatrist Alois Alzheimer in
1907, Alzheimer’s Disease (AD) is, today, the most common
cause of dementia in the elder population [1].
According to the Brazilian Institute of Geography and
Statistics (IBGE) and the World Health Organization (WHO),
there is 1.2 million people with AD in Brazil. It is believed
that only 5% of the patients developed the disease at an early
stage, i.e., before 65 years of age. In patients where the AD
started after 65 years old, it is estimated that between 10%
and 30% of these cases started after 85 their years old [2].
AD is a degenerative disease that causes irreversible death
of several brain cells, the neurons. The patient suffering from
this disease has a brain with microscopic pathologic lesions,
known as neuritic plaques, and neuroﬁbrillary tangles [3]. In
addition, the brain of a person with Alzheimer’s is much
smaller than the brain of a healthy person, as it is shown in
Fig. 1.
This disease develops in each patient in a unique way; how-
ever, there are several symptoms common to all of them, e.g.,
loss of memory, language disorders, depression, aggression,
among others. Initially, the patient loses episodic memory,
i.e., memory that holds information of events and their spatio-
temporal relations. Thus, the old facts and the facts that just
happened are easily forgotten. With the progress of the disease,
Figure 1. Brains illustrations. In the left side the one of a person with AD
and in the right side a brain of a healthy person.
semantic memory is also lost, i.e., lexical knowledge, rules,
symbols are forgotten and the patient begins to lose its cultural
identity [4].
The diagnosis of AD is often performed late since this
disease can be confused with several other types of dementia
and even the normal symptoms of aging. Although it is an
irreversible disease, if it is discovered in its early stage,
medications may be administered to slow the progression of
symptoms and prolong the patient’s welfare [5]. Thus, it is
extremely important that mechanisms are developed for an
earlier prediction of AD in the whole population.
Another common type of disease is the Mild Cognitive
Impairment (MCI). The MCI causes cognitive changes that are
noticed by the individuals experiencing them or to other people
that live with the patient. However, this changes are not severe
enough to interfere with daily life or independent function.
People who are diagnosed with MCI have an increased risk of
eventually developing AD.
In the literature, Ray et al. [6] conducted a study using
a database of 120 samples of proteins contained in plasma
of several patients. He concluded in his research that a com-
bination of 18 out of the 120 available proteins enabled the
realization of early diagnosis of AD with an classiﬁcation rate
of 91% using a set of tests with data from 92 patients who
were diagnosed with or without AD.
In addition, he also used another set of tests containing data
373
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

from 47 patients diagnosed with Mild Cognitive Impairment
(MCI). For this set, the classiﬁcation rate was 81%. These
values were calculated from the average success rates found
for all classiﬁers used in clinical trials for both sets [7].
Afterwards, G`omez and Moscato conducted a study using
20 different classiﬁers available in the software Weka and
deﬁned various signatures with 18, 10, 6 and 5 proteins. These
proteins were all contained in the set described by Ray et
al. The 10 proteins signature reached a classiﬁcation rate of
89% using the AD test set and 66% for the MCI test set.
Furthermore, the 5 proteins signature reached a classiﬁcation
rate of 93% using the AD test set and 65% for the MCI test
set. These success rates were also calculated from the average
values of the 20 classiﬁers used [7].
Recently, Dantas and Valenc¸a [1] made a signiﬁcant con-
tribution. In the work mentioned, it was used the Random
Forest Algorithm to create a new signature with 10 proteins
and test its accuracy with 2 topologies of neural networks:
Multi-Layer Perceptron and Reservoir Computing. After all the
experiments, it was statistically proven that the new signature
has a higher classiﬁcation rate for the diagnosis of AD and
MCI.
This paper aims to use another neural network topology to
calculate the classiﬁcation rate with the same signatures that
were described in the work performed by G`omez and Moscato
and Dantas [7][1]. To meet this goal, the Extreme-learning
Machine was the chosen technique.
Beyond that, this work also aims to use the Random Forest
Algorithm, intending to create new signatures with 5 proteins.
After that, this new signature will be tested using the Multi-
layer Perceptron [8], Extreme-learning Machine [9] and the
Reservoir Computing [10] and all results will be compared
with the one available in the literature [1][6][7].
This article is organized into several sections. The ﬁrst
contains information about the neural network topologies that
will be used. The next section describes the methodology used
throughout this work, i.e., what database is used and how it
is organized, the experiments and statistical analysis that was
performed. Finally, there is a section that displays the results
and the last one contains the conclusions obtained in this work.
II.
ARTIFICIAL NEURAL NETWORKS
The Artiﬁcial Neural Networks (ANN) are models that
intend to simulate the behavior of the human brain. The ANN
contains simple processing units that are interconnected in
order to process information. The knowledge of this model
is stored in the weight contained in each of the connections
between the artiﬁcial neurons.
The ANNs have been extensively used in many ﬁelds due
to their ability to approximate complex nonlinear functions.
These models have some advantages such as generalization,
adaptability, ability to learn from examples.
Over the years, many studies have emerged in this ﬁeld
and several ANN topologies were created. In this paper, some
of them will be discussed in the next sessions.
A. Multi-Layer Perceptron
One of neural networks topologies used in this paper
was the Muti-Layer Perceptron (MLP). This network has the
Figure 2. MLP with three layers
advantage of having intermediate layers. Thus, this character-
istic guarantees that this neural network can approximate any
continuous function as long as it has at least one intermediate
layer, or any mathematic function if the number of intermediate
layers is more than one.
All the connections between the neurons of the MLP have
a weight that, initially, have a random value but that during
the training will be optimized. These neurons are disposed in
three kinds of layers that is listed below:
•
Input layer: Represents the input variables of the
problem;
•
Intermediate or hidden layer: This layer is respon-
sible for the capacity of the MLP in solving non linear
problems;
•
Output layer: Represents the output variables of the
problem
Fig. 2 shows an example of this network.
As well as other topologies of neural networks, the MLP
also needs a training algorithm to optimize the weights of
the MLP. In this paper, it was used the Back-propagation, a
gradient-based algorithm [11].
1) Back-propagation algorithm: This algorithm is divided
in two phases. In the ﬁrst one, the forward phase, it happens
the progressive signal propagation, i.e., it goes from the input
layer to the output layer. In this phase, the weights are not
changed and the output of the neural network and the error is
calculated.
At this moment, the second phase, backward, of the al-
gorithm is initiated. During this stage, the back-propagation
error occurs, that is, based on the calculated output error, the
weights connecting the hidden layer to the output layer are
adjusted. This is accomplished using Equation (1) [11].
W m
i,j(t+1) = W m
i,j(t)+αδm
i f m−1(netm−1
j
)+β∆W m
i,j(t−1)
(1)
where
•
α: represents the learning rate of the algorithm;
•
β: represents the momentum rate;
374
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Figure 3. Single Layer Feedforward Network
•
δ: represents the sensibility that can be calculated
following Equation (2) to the neurons in the output
layer and using Equation (3) to the neurons in the
hidden layers [8].
δm
i = (di − yi)f ′(neti)
(2)
δm−1
j
= f ′m−1(netm−1
j
)
N
X
i=l
wm
ij , δm
i
(3)
More detailed information about the execution of this
algorithm can be found in the literature [11][12][8].
B. Extreme-learning Machine
With the increase in the use of neural networks in many
applications, it was noted that the learning speed of feedfor-
ward networks, such as MLP, usually was very slow. This
problem also makes it difﬁcult to use these neural networks
for problems that require real-time response .
The main reason for this speed problem was that almost
all ANNs were trained using gradient based algorithms. These
kind of algorithms have several issues, such as the slower speed
in the learning process, overﬁtting and local minimums.
In order to try to solve this problem, Huang et. al. created in
2004 the Extreme Learning Machine (ELM), a new training
algorithm for Single-hidden Layer Feedforward Neural Net-
works (SLFNs) [9]. This algorithm randomly chooses the input
weights and analytically determines the output weights of a
SLFN [13]. The architecture of a SLFN is shown on Fig. 3.
ELM can be modeled following the Equations (4) and (5)
[9]:
˜
N
X
i=l
βigi(xj) =
˜
N
X
i=1
βig(wi.xj + bi) = oj, j = 1, ..., N
(4)
˜
N
X
i=l
βig(wi.xj + bi) = tj, j = 1, ..., N
(5)
where
•
(xj, tj): N input patterns;
•
wi: Weight vector of the neuron i from the hidden
layer;
Figure 4. Pseudo-code of the ELM algorithm [13]
•
bi: bias of the neuron i from the hidden layer;
•
˜N: number of neurons in the hidden layer;
•
βi: Weight vector between the hidden neuron i and
the output layer.
The ELM also can be demonstrated in a matrix form as it
described in Equation (6) [13].
Hβ = T
(6)
where
•
H is expressed on Equation (7) [9];
•
β is expressed on Equation (8) [9];
•
T is expressed on Equation (9) [9];
H(w1, ..., w ˜
N, b1, ..., b ˜
N, x1, ..., x ˜
N) =


g(w1.x1 + b1)
· · ·
g(w ˜
N.x1 + b ˜
N)
...
· · ·
...
g(w1.xN + b1)
· · ·
g(w ˜
N.xN + b ˜
N)


(7)
β =


βT
1...
βT
˜
N


˜
Nxm
(8)
T =


tT
1...
tT
˜
N


Nxm
(9)
The ELM pseudo code training algorithm is available in
Fig. 4.
where H† is the Moore-Penrose Pseudo Inverse [9] and
needs to satisfy the following properties:
•
HH†H = H;
•
H†HH† = H†;
•
(HH†)T = HH†;
•
(H†H)T = H†H.
This pseudo inverse can be efﬁciently calculated using the
Single Value Decomposition (SVD) [13].
Furthermore, the ELM offers signiﬁcant advantages such as
ease of use, better generalization performance with much faster
learning speed and it can be used in real-time applications.
A remarkable point to be considered is that the gradient
based algorithms can be used for training neural networks
375
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

with multiple hidden layers. In contrast, the ELM can only be
used in SLFNs. However, as previously mentioned, with only a
single hidden layer, the ANN can approximate any continuous
function.
Thus, the use of this algorithm in this paper is valid, since
it is a problem that can be solved with SLFNs networks.
C. Reservoir Computing
Recurrent Neural Networks (RNN) were created to en-
able the solution of dynamic problems. This is accomplished
through a feedback of a neuron in a layer i to that found in
some previous layer, i−j. This neural network topology has a
better resemblance to the operation and behavior of the human
brain [14].
In 2001, a new approach for the design of the training of
a RNN was proposed by Wolfgang Mass called Liquid State
Machine (LSM) [15]. At the same time, but independently,
the same approach was described by Herbert Jaeger and called
Echo State Machine (ESN) [16].
Both ESN and LSM networks have the Echo State Property
(ESP) [17], i.e., due to the recurrent network connections,
information from previous entries are stored. However, these
data are not stored for an inﬁnite period of time, and as well
as the human brain, old information must be forgotten over
time. Thus, the neural network has a rich set of information
from the past and present therefore enhancing its applicability
to dynamic systems [18].
In 2007, Verstraeten coined the term Reservoir Computing
(RC) that uniﬁed the concepts described in ESN and LSM.
Since then, this term is used in literature to illustrate learning
systems which are represented by a dynamic recurrent neural
network [10].
The RC is composed of three parts: an input layer, which
as the MLP, represents the input variables of the problem, a
reservoir, which can be seen as a large distributed and dynamic
RNN with ﬁxed weights, and a linear output layer called
readout.
Fig. 5 represents the RC topology with two neurons in the
input layer, three in the reservoir and one neuron in the output
layer.
Although the problem addressed in this work is not dy-
namic, it was decided to test this neural network topology
in order to verify the processing time and the memory effect
of the RC and if it would have some positive impact on the
diagnosis of AD.
The RC used in this work was based on the ESN approach
and was developed in the Java programming language to
make use of the object-oriented paradigm. This framework
was created in order to solve classiﬁcation and prediction
problems and it was validated by three Benchmarks: Iris
species, Thyreoide, Cancer and Diabetes.
The training algorithm for the RC was the same as the
ELM, i.e., the outputs are calculated using the Moore-Penrose
generalized inverse and it is more detailed on the next section.
1) Construction and Simulation of RC: The ﬁrst step in
order to prepare the RC and perform the data set classiﬁcation
is conﬁgure its architecture. Thus, it is necessary to deﬁne the
number of neurons that will be used in the input, output and
in the reservoir layers.
Figure 5. RC architecture. The dashed lines represent the weights that
should be adjusted during the training of the network.
Furthermore, several RC parameters should also be deter-
mined. Being a recent methodology, there are no studies that
prove how many neurons in the reservoir are necessary so
that the neural network has better performance, or the rate of
connectivity between these neurons. Therefore, for this work,
values were deﬁned based on some empirical tests performed.
Once the architecture of the neural network is determined,
the next step is to generate the weight matrices connecting
the input layer to the reservoir, Win, and the matrix with the
weights between neurons in the reservoir, Wres. Both matrices
are generated with random values between -1 and 1.
Studies claim that the matrix Wres must have a spectral
radius equal to 1 to provide a more numerical stability [19],
i.e., when Wres is initialized, it must have its values changed
as follows:
•
Initially it must be decomposed into singular values;
•
Then, Wres should have its values changed until the
maximum value of the main diagonal of the eigenval-
ues matrix is less than or equal to 1.
To perform the simulation of the RC, the database is
divided into three sets: training, used to perform the update
of the states of the neurons of the reservoir, cross-validation,
used to stop the training of the neural network, and test set,
used to calculate the RC classiﬁcation rate [20].
The states of the neurons in the reservoir must be initialized
to zero. Since this is a recurrent network and RC stores
its states (Mest) in a matrix, it is necessary that the ﬁnal
values found by the network are not so inﬂuenced by this
initialization. Therefore, the literature suggests that before start
training, a set of cycles called warm up is executed in order
to perform updates in the states of the neurons in the reservoir
and overlook the inﬂuence of the initial value [10]. The states
are updated according to (10) [10]:
x[k + 1] = f(Wresx[k] + Winu[k])
(10)
where Winu[k] represents the matrix containing the result of
the product of the values derived from the input layer by the
376
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

weights connecting these neurons to the reservoir at a time k
and Wresx[k] is the matrix with the states of the neurons from
the reservoir at the same time k. The result will be assigned to
x[k + 1], i.e., the state of the neuron RC in an instant forward
will be the result of calculating the activation function of the
neuron from the sum of the two parcels described above. In this
work, the activation function used was the hyperbolic tangent
according to equation 11[8].
f(neti) = eneti − e−neti
eneti + e−neti
(11)
Where, y is the output value and neti is the weighted
average of the weights with the entries of the ith neuron.
Once the period of warm up is over, the training of the RC
can be initialized. The ﬁrst step should be to load the training
set and perform the update of the states of the reservoir, noting
that the matrices Win and Wres should not be changed. They
are randomly generated during construction of the RC, as
described in the previous section, and should not be adjusted.
Still during training, the weights matrix that connects the
neurons of the input layer to the output (Winout) and the one
that connects the reservoir to the output must be calculated
by the pseudo-inverse of Moore-Penrose. As they are non-
square matrices and their determinants can approach zero, it
is necessary to calculate the pseudo-inverse.
At the end of each training cycle, a cross validation cycle
should be initiated. This process should be repeated until the
stopping criteria is reached and the training set is ﬁnalized.
During the process of cross-validation, the matrices Winout
and Wout should remain being readjusted.
When the process of training is ﬁnished, the testing process
begins. The set of tests is presented to the RC and at this
time, all the weights matrices, Win, Wres, Winout and Wout,
should remain unchanged, as the matrix Mest. At this point,
the classiﬁcation error is calculated. These values will be used
in the future to make the necessary comparisons.
The behavior of the RC can be best viewed through the
algorithm described in Fig. 6[10].
III.
RANDOM FOREST ALGORITHM
Although, the Random Forest (RF) Algorithm is an ex-
cellent classiﬁer, this technique can be used to rank the
importance of variables in a classiﬁcation problem [21].
To measure the importance of each variable in a data set
Dn, it has to ﬁt a random forest to the data. The data set can
be expressed as it shown in Equation (12) [22].
Dn = (Xi, Yi)n
i=1
(12)
where
•
X is the training set;
•
Y is the responses set;
•
n is the number of the examples in the data set.
Fig. 7 shows the pseudo code used to calculate the Variable
Importance (VI) with the RF. It is important to mention that
this algorithm uses the Out-of-bag (OOB) error estimation in
the formula to measure the VI [21].
Figure 6. Reservoir Computing pseudo-code
Figure 7. Pseudo code of the calculus of VI using RF
With all errors calculated with the pseudo code described in
Fig. 7, the VI coefﬁcient must be calculated using the Equation
(13) [22].
377
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Figure 8. Pseudo code of the calculus of VI using RF
V I(Xj) =
1
ntree
j
X
t=0
(err
˜
OOB
j
t − errOOBt)
(13)
Since all VI have been calculated, now it is possible to
select the most important variables from the original data set.
To perform this action the steps described in Fig. 8 must be
followed [21].
In the case of this paper, the RF algorithm was applied
in the database and Fig. 9 shows the graph with all proteins
ordered by the VI coefﬁcient.
IV.
EXPERIMENTS
A. Database
The database used in the development of this work was the
same used by G`omez and Moscato et al. and Dantas in their
publication. It has values of 120 proteins found by analysis
of blood samples from different patients. The ultimate goal of
the database is to classify whether a patient can be diagnosed
or not with AD or MCI [7] [1].
In his work, G`omez and Moscato et al. subdivided the
database in 2 sets. The ﬁrst set contained the results of blood
samples of 83 patients. Of these 83 patients, 68 were allocated
to the training process of the chosen classiﬁer. The data for
the remaining 15 patients were used in the process of cross-
validation of the classiﬁer, i.e., a process that determines the
optimal point to stop its training [8].
The second set, used in the testing process of the classiﬁer,
has two options. It could be used to diagnosis AD and in this
case, this set will contain the samples related to the 92 patients
that could be diagnosed with AD. The second option is use
this set to perform diagnosis of MCI. In this other case, the
test set will contain blood samples related to 47 patients with
a possible diagnosis of MCI.
Recently, Dantas deﬁned two new signatures with 10
proteins with the objective to compare the results with the
one obtained by G`omez and Moscato et al. In this work those
two signatures will be used to verify the accuracy of the ELM
algorithm.
Figure 9. Application of the RF algorithm in the database used in this work
Besides that, aiming a comparison with the signatures
previously deﬁned by G`omez and Moscato [7], two new
signatures of 5 proteins were proposed. One of them used to
perform the diagnosis of Alzheimer’s Disease and the other
for MCI.
The Random Forest algorithm was executed 30 times and in
each of the simulations, a signature of 5 proteins was deﬁned.
After all these 30 simulations were over, the best signature
was chosen according to the gini metric.
It is important to mention that G`omez and Moscato [7]
used the same signature in both cases, that is, the signature
composed by 10 and 5 proteins is used for the AD and MCI
testing sets.
Table I shows the signatures of proteins that are contained
in G`omez and Moscato et al. work and which are used in this
study.
In this work, 8 databases were prepared in order to repro-
duce the experiments described by G`omez et al. [7] and Dantas
[1], using the MLP, RC and the ELM. They were:
•
1 database for testing the signature of 10 proteins
deﬁned by G`omez et al. with the AD set of tests,
called now by Database 1;
•
1 database for testing the signature of 10 proteins
deﬁned by G`omez et al. with the MCI set of tests,
called now by Database 2;
•
1 database for testing the signature of 10 proteins
deﬁned by Dantas with the AD set of tests, called
now by Database 3;
378
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE I. REPRESENTATION OF THE PROTEINS CONTAINED IN
EACH ONE OF THE SIGNATURES USED.
Abbreviation
Signature
Proteins
S1
10
proteins
sig-
nature deﬁned by
G`omez et al. [7]
CCL7/MCP-3,
CCL15/MIP-1d,
EGF, G-CSF, IL-1a, IL-3, IL-6,
IL-11, PDGF-BB, TNF-a
S2
10
proteins
sig-
nature deﬁned by
Dantas [1] for AD
test set
IL-1a,
TNF-a,
G-CSF
,PDGF-
BB, IGFBP-6, M-CSF, EGF, IL-3,
GDNF, Eotaxin-3
S3
10
proteins
signature
deﬁned
by Dantas [1] for
MCI test set
IL-1a, PDGF-BB, EGF, TNF-a,
RANTES, FAS, GCSF, MIP-1d,
FGF-6, IL-11
S4
5 proteins signa-
ture
deﬁned
by
G`omez et al. [7]
EGF, G-CSF, IL-1a, IL-3, TNF-a
S5
5 proteins signa-
ture deﬁned by the
Random Forest for
AD test set
IL-1a, TNF-a, G-CSF ,PDGF-BB,
M-CSF
S6
5 proteins signa-
ture deﬁned by the
Random Forest for
MCI test set
IL-1a, PDGF-BB, EGF, TNF-a,
RANTES
•
1 database for testing the signature of 10 proteins
deﬁned by Dantas with the MCI set of tests, called
now by Database 4;
•
1 database for testing the signature of 5 proteins
deﬁned by G`omez et al. with the AD set of tests,
called now by Database 5;
•
1 database for testing the signature of 5 proteins
deﬁned by G`omez et al. with the MCI set of tests,
called now by Database 6;
•
1 database for testing the signature of 5 proteins
deﬁned by Random Forest Algorithm with the AD set
of tests, called now by Database 7;
•
1 database for testing the signature of 5 proteins
deﬁned by Random Forest Algorithm with the MCI
set of tests, called now by Database 8;
All databases described above maintained the organization
used by G`omez et al. regarding the division of values for the
training, cross validation and testing set.
1) Pre-processing of data: To properly execute the training
of the neural network it is necessary that your data is nor-
malized, i.e., the input values of the neural network must be
contained in the same numerical range. This is important since
very different values can inﬂuence the training and generate a
loss in the generalization ability of the neural network [8].
One of the most commonly used normalization techniques
in literature is the linear transformation and it was the one
chosen for this work. Equation (14) is the formula used to
normalize the values of the database.
y = ((b − a) × (
x − xmin
xmax − xmin
)) + a
(14)
In (14), a and b represent the maximum and minimum
values that the data should take. In this work, it was used
the value of -0.85 for a and 0.85 for b, as the activation
function chosen for this neural network is the Hyperbolic
Tangent. Therefore, the values contained in the database must
be between -1 and 1.
B. Simulations
1) MLP Parameters: Although it is widely used in many
researches, several parameters of the MLP must to be con-
ﬁgured. The choice of each one directly inﬂuences the ﬁnal
outcome of the prediction.
Below are the main parameters of the MLP and Backprop-
agation algorithm:
•
Number of neurons in the input layer;
•
Number of neurons in the hidden layer (only one
hidden layer);
•
Number of neurons in the output layer;
•
Activation function of the hidden layer;
•
Activation function of the output layer;
•
Learning Rate;
•
Momentum.
During the experiments, the number of neurons in the input
layer was varied according to the amount of proteins in the
signature of each experiment. These values can be either 5 or
10.
This work aimed to perform early diagnosis of a patient
with or without AD or MCI, thus the number of neurons in
the output layer is 2.
The algorithm used to adjust the weights is the Backprop-
agation and its formula can be found in section II-A.
The activation function chosen of the hidden layer is the
hyperbolic tangent. This function returns values in the interval
[-1, 1] and is given by 11
Several tests were performed to deﬁne the learning rate,
momentum and number of neurons in the hidden layer. The
best results correspond to the values of 0.3 for the learning rate,
0.6 for the momentum and 20 neurons in the hidden layer.
The MLP used has been implemented in the JAVA pro-
gramming language and in the Eclipse development environ-
ment [23].
2) RC Parameters: As the MLP, the Reservoir Computing
technique has several parameters that require conﬁguration.
Taking into account that it is a recent area of research, the
choice of these settings can not be considered ideal and it is
often performed randomly. One way to do this is to evaluate
each chosen parameter value and determine if it was better or
worse for the network performance. This process is repeated
until a value is considered optimal, which does not necessarily
means the best.
In this work, these parameter values were changed and all
the classiﬁcation rates at the end of the trainings were stored
and then compared. The conﬁguration which presented the best
parameter values for the network performance was the one
chosen.
Below are the parameters whose settings were required to
be deﬁned during this project:
•
Number of neurons in the input layer;
•
Number of neurons in the output layer;
•
Number of neurons in the reservoir;
•
Activation function of the reservoir;
•
Activation function of the output layer;
379
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

•
Initialization of weights;
•
Connection rate of the reservoir;
•
Number of warm up cycles;
The number of inputs and outputs remain the same as used
in the MLP, since the goal is to compare the ANN’s techniques
using the same classiﬁcation scenario. It means the number of
neurons in the input layer can be 5 or 10, depending on the
signature used. The number of neurons in output layer is 2.
The number of neurons in the reservoir is one of the
parameters for which there is no ﬁxed criterion that deﬁnes
it. It was chosen randomly after checking the classiﬁcation
rate at the end of each training. It was observed that the ideal
number of neurons in the reservoir was 4.
As mentioned in Section II-C, the weights of the input layer
to the reservoir and the weights of the reservoir are randomly
generated with values between [-1, 1].
The reservoir states are initialized to zero (0). Due to this,
as also mentioned in Section II-C, it was decided to add to the
network a phase called warm up. During the warm up, it is
not necessary to calculate the weights of the output layer, or to
calculate an output value. This warm up phase is done just to
update the states of the reservoir and remove the dependence
on the initial state. The number of cycles chosen for warm up
was 100.
The connection rate of the reservoir neurons was 10%. That
is, only 10 % of the connections have weight values different
from zero associated to them.
The activation function chosen in the reservoir was the
hyperbolic tangent. In the output layer, the selected function
was linear one.
During this work, we implemented a Neural Network with
the technique of RC in the programming language Java and
the Eclipse development environment.
3) ELM Parameters: Finally, the ELM also needs to have
its parameters adjusted. Below are the parameters whose
settings were required to be deﬁned during this project:
•
Number of neurons in the input layer;
•
Number of neurons in the hidden layer (only one
hidden layer);
•
Number of neurons in the output layer;
•
Activation function of the hidden layer;
•
Activation function of the output layer;
The same number of neurons in the input and output layer
used in the two other techniques is maintained for the ELM.
That is, 5 or 10 inputs and 2 outputs.
The number of neurons in the hidden layer remains the
same used with the MLP which means 20 neurons.
The activation function of the hidden layer is the hyperbolic
tangent and of the output layer is the linear.
Table II summarizes the parameters used in all experiments
with the neural networks topologies mentioned above.
After deﬁning the settings of the MLP, ELM and RC, 30
simulations were performed with each of the databases in
each of the chosen neural network topologies in this work.
This number is considered ideal to perform more meaningful
statistical comparisons [24].
TABLE II. REPRESENTATION OF THE PARAMETERS USED FOR THE
SIMULATIONS WITH THE RC, MLP AND ELM.
Parameters
RC value
MLP value
ELM value
RC connectivity
10%
Not applicable
Not applicable
Number of neu-
rons in the input
layer
Depends of the
amount of pro-
teins in the sig-
nature
Depends of the
amount of pro-
teins in the sig-
nature
Depends of the
amount of pro-
teins in the sig-
nature
Number of neu-
rons in the RC
4
Not applicable.
Not applicable
Number of neu-
rons in the hid-
den layer
Not applicable
20
20
Number of neu-
rons in the out-
put layer
2
2
2
Number
of
warm up cycles
100
Not applicable.
Not applicable
Activation func-
tion of neurons
in the reservoir
or hidden layer
Hyperbolic
Tangent
Hyperbolic
Tangent
Hyperbolic
Tangent
Activation
function
of
neurons
in
the
output layer
Linear
Linear
Linear
Learning rate
Not applicable.
0.3
Not applicable
Momentum
Not applicable.
0.6
Not applicable
TABLE III. NULL AND ALTERNATIVE HYPOTHESIS FOR THE
SHAPIRO-WILK TEST.
Hypothesis
Description
Null Hypothesis
The sample is normally distributed
Alternative Hypothesis
The sample isn’t normally distributed
TABLE IV. NULL AND ALTERNATIVE HYPOTHESIS FOR THE F
TEST.
Hypothesis
Description
Null Hypothesis
The samples have variances statistically equal
Alternative Hypothesis
The samples don’t have variances statistically equal
C. Statistical Analysis
When all the simulations were completed, it was necessary
to perform a sequence of statistical tests in order to scientiﬁ-
cally validate the results. For this, it was used the R mathemat-
ical software, since it contains all the implementations of the
tests used. This software uses as default a level of signiﬁcance
(α) previously deﬁned with the value of 0.05.
Before using a parametric test on a data set it is necessary
to check whether the samples are normally distributed and if
they have statistically equal variances. If these two assumptions
are validated, one can apply a parametric test, otherwise it must
be used a non-parametric test.
In order to verify the ﬁrst assumption, that is, check if the
samples were normally distributed, the Shapiro-Wilk test was
applied with the hypothesis described in Table III.
After that, it is necessary to verify whether or not the
samples were drawn from the same population, i.e., if their
variances were statistically equal. The hypothesis for this test
are available on Table IV.
If these two premises were true, a parametric test can be
used. In this case, it was chosen the Student’s T-test. The
hypothesis are described in Table V.
In case of the ﬁrst two premises aren’t true at the same
time, it is necessary to use a non-parametric test, i.e., one that
380
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE V. NULL AND ALTERNATIVE HYPOTHESIS FOR THE
STUDENT’S T-TEST
Hypothesis
Description
Null Hypothesis
The average of the analyzed samples are statistically
equal
Alternative Hypothesis
The average of the analyzed samples aren’t statistically
equal
TABLE VI. NULL AND ALTERNATIVE HYPOTHESIS FOR THE
WILCOXON RANK-SUM TEST
Hypothesis
Description
Null Hypothesis
The median of the analyzed samples are statistically
equal
Alternative Hypothesis
The median of the analyzed samples aren’t statistically
equal
TABLE VII. REPRESENTATION OF THE AVERAGE ACCURACY
RATES AFTER THE 30 EXPERIMENTS.
Database
Average
classiﬁ-
cation rate with
RC
/
Standard
Deviation
Average
classiﬁ-
cation rate with
MLP / Standard
Deviation
Average
classiﬁ-
cation rate with
ELM / Standard
Deviation
Database 1
86.62% / 0.026
93.44% / 0.017
87.78% / 0.025
Database 2
69.29% / 0.024
68.15% / 0.018
68.45% / 0.027
Database 3
90.57% / 0.022
94.31% / 0.008
91.05% / 0.023
Database 4
76.59% / 0.047
78.86% / 0.031
74.32% / 0.050
Database 5
93.22% / 0.026
95.61% / 0.017
91.12% / 0.021
Database 6
65.44% / 0.024
69.14% / 0.018
66.81% / 0.019
Database 7
92.97% / 0.022
93.26% / 0.008
91.62% / 0.028
Database 8
72.90% / 0.047
73.33% / 0.031
73.88% / 0.040
makes no assumptions about the probability distribution of the
samples. It was chosen the Wilcoxon Rank-Sum Test with the
hypothesis in Table VI.
For all those cases, the p-value must be compared with
the signiﬁcance level adopted in the R software (α) . In case
of this value is less then α the null hypothesis should be re-
jected implying that the alternative hypothesis is automatically
accepted.
V.
RESULTS
After all simulations were performed with the databases, it
was calculated the arithmetic mean for each set of simulations
and Table VII displays those values found.
As none of the eight datasets met the two premises nec-
essary for the application of a parametric test at the same
time, it was not possible to perform the Student’s T-test. And
so, the Wilcoxon Rank-Sum Test was chosen, since it is a
non-parametric test, i.e., it makes no assumptions about the
probability distribution of the samples.
When applied the Wilcoxon Rank-Sum Test for each of
the eight cases, the results found were that the MLP has a
statistically better performance than the RC and ELM.
In order to verify the performance of the new proposed
signatures, simulations with RC, MLP and ELM topologies
were performed for both the diagnosis of AD and MCI. The
results were compared with those found by the same neural
networks when the signatures used were the ones deﬁned by
G`omez and Moscato et al.
In all cases, the classiﬁcation rate showed improvement
when the new signatures were used for all architectures. After
the Wilcoxon Rank-Sum test, this statement was conﬁrmed
TABLE VIII. COMPARISON OF THE RESULTS WITH THE NEW
PROTEIN SIGNATURE PROPOSAL OBTAINED WITH RC, MLP, ELM
AND THE ONES AVAILABLE IN LITERATURE.
Protein Sig-
nature
RC - Maxi-
mum value
MLP - Max-
imum value
ELM - Max-
imum value
Results
found
by
G`omez
and
Moscato
et
al.
New
10-
protein
signature
for
AD
proposed by
Dantas[1]
96.73%
95.65%
96.81%
93%
New
10-
protein
signature
for
MCI
proposed
by
Dantas
[1]
89.36%
82.97%
89.20%
66%
New
5-
protein
signature
for AD
98.15%
97.33%
95.29%
93%
New
5-
protein
signature
for MCI
73.50%
74.21%
74.49%
66%
statistically. The maximum and average values are also bigger
than those described by G`omez and Moscato et al.
Table VIII summarizes the maximum values found in the
simulations of the RC, MLP and ELM. Those results were
found using all new signatures proposed. Table VIII also
display the results obtained in the work of G`omez and Moscato
et al. using their own signature [7].
From Table VIII, it can be concluded that all neural
network topologies used obtained results consistent with those
described by G`omez and Moscato et al. and succeeded in
reaching a maximum value greater than the average found in
the literature.
VI.
CONCLUSION
Nowadays, Alzheimer’s disease is one of the most common
diseases in the elderly population. In recent years, the number
of patients has grown signiﬁcantly since the life expectancy in
most developed countries has increased.
AD is a degenerative disease, i.e., brain cells will deterio-
rate and there is no way to reverse the disease. However, the
earlier the drugs are administered, the better the quality of life
of the patient since the medication will slow the progression
of the symptoms.
Thus, this study aimed to verify the performance of MLP,
RC and ELM to early classify if a patient can be diagnosed
with AD or not. Moreover, another goal was to make a
comparison of the performance of the MLP with the RC and
ELM neural networks, and also with the results available in
the literature.
From the statistical tests and simulations, it can be con-
cluded that the MLP presented a superior performance in all
cases. It is also possible to conclude that the four new signa-
tures proposed achieved better results when compared to those
shown by G`omez and Moscato et al. Furthermore, they also
had better performance when compared to the results obtained
381
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

from the same neural network topologies when the signatures
used were the ones proposed by G`omez and Moscato et al.
As future work, it is intended to use other neural networks
topologies to make a comparison with those already used in
this work. Besides that, it is intended to invest in more variable
selection techniques in order to further optimize the results
and to reduce the number of proteins in the signatures used to
perform early diagnosis of Alzheimer and MCI.
REFERENCES
[1]
L. Dantas and M. Valenc¸a, “A comparative study of neural network
techniques to perform early diagnosis of alzheimer’s disease,” The
Sixth International Conference on Advanced Cognitive Technologies
and Applications, 2014, pp. 178–183.
[2]
H. Portal. Portal of the brazilian health. [retrieved: July, 2014].
[Online]. Available: http://portal.saude.gov.br/saude/ (2014)
[3]
R. Green, Diagnosis and Treatment of Alzheimer’s Disease and Other
Dementias.
Editora de Publicac¸˜oes Cient´ıﬁcas Ltda., 2001
[4]
P. Marques, Dementia of Alzheimer type: diagnostic, treatment and
social aspects.
Editora de Publicac¸˜oes Cient´ıﬁcas Ltda., 1997.
[5]
E. Giusti and V. Surdo, Alzheimer’s care and family counseling:
psychological needs and treatment of dementia.
Gryphus, 2010.
[6]
S. Ray, M. Britschgi, C. Herbert and A. Boxer, “Classiﬁcation and
prediction of clinical alzheimer’s diagnosis based on plasma signaling
proteins.” Nat. Med., vol. 13, 2007.
[7]
M. G´omez and P. Moscato, “Identiﬁcation of a 5-protein biomarker
molecular signature for predicting alzheimer’s disease,” PLos One,
vol. 3, 2008, p. 12p.
[8]
M. Valenc¸a, Fundamentals of Neural Networks: Examples in Java,
2nd ed.
Livro R´apido, 2009.
[9]
G. Huang, Q. Zhu and C. Siew, “Extreme learning machine: A new
learning scheme of feedforward neural networks,” International Joint
Conference on Neural Networks, 2004.
[10]
D. Verstraeten, “Reservoir computing : computation with dynamical
systems,” Ph.D. dissertation, Ghent University. Faculty of Engineering,
Ghent, Belgium, 2009.
[11]
P. Werbos, “Beyond regression: New tools for prediction and analysis
in behavioral sciences.” Ph.D. dissertation, Harvard University, 1974.
[12]
S. Haykin, Neural Networks: Principles and Practices. Bookman, 2007.
[13]
R. Rajesh and J. Prakash, “Extreme learning machines - a review and
state-of-the-art,” International Journal of Wisdom Based Computing,
vol. 1, 2011, pp. 35–49.
[14]
M. Valenc¸a, Applying Neural Networks: A Complete Guide, 1st ed.
Livro R´apido, 2005.
[15]
W. Maass, Motivation, theory, and applications of liquid state machines.
Imperial College Press, 2011.
[16]
H. Jaeger, “The echo state approach to analysing and trainning recurrent
neural networks,” German National Resource Center for Information
Technology, Tech. Rep., 2010.
[17]
M. Massar and S. Massar, “Mean-ﬁeld theory of echo state networks,”
PHYSICAL REVIEW E, vol. 87, 2013.
[18]
M. Lukoˇseviˇcius, A practical guide to applying echo state networks.
Springer Berlin Heidelberg, 2012, pp. 659–686.
[19]
A. Ara´ujo, “A method for design and training reservoir computing
applied to prediction of time series,” Ph.D. dissertation, Universidade
de Federal de Pernambuco, 2001.
[20]
M. Kulkarni and C. Teuscher, “Memristor-based reservoir comput-
ing,” in Proceedings of the IEEE/ACM International Symposium on
Nanoscale Architectures (NANOARCH’12), 2012.
[21]
A. Liaw and M. Wiener, “Classiﬁcation and regression by randomfor-
est,” R news, vol. 2, no. 3, 2002, pp. 18–22.
[22]
R. Genuer, J. Poggi and C. Malot, “Variable selection using random
forests,” Pattern Recognitions Letters 31, 2010.
[23]
S. Santos, “An application for prediction of water resources based on
neural networks and particle swarm,” Master’s thesis, Universidade de
Pernambuco, 2012.
[24]
N. Juristo and A. Moreno, Basics of Software Engineering Experimen-
tation.
Kluwer Academic Publishers, 2001.
382
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

