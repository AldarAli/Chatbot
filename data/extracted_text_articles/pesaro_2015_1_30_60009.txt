Estimation of Job Execution Time in MapReduce Framework over GPU clusters  
 
 Yang Hung, Sheng-Tzong Cheng 
Computer science and Information Engineering 
National Cheng Kung University 
Tainan, Taiwan 
e-mail: stcheng@mail.ncku.edu.tw 
Chia-Mei Chen 
 Information Management 
National Sun Yat-Sen University 
Kaohsuing, Taiwan 
e-mail: cmchen@mail.nsysu.edu.tw  
 
 
Abstract—The development of Graphic Processing Unit (GPU) 
makes it possible to put hundreds of cores in one processor. It 
starts a new direction of high-speed computing. In addition, in 
a Cloud computing environment, MapReduce over GPU 
clusters can execute graphics processing or general-purpose 
applications even much faster. It is crucial to manage the 
resources and parameters on GPU devices. In this paper, we 
study the execution time of MapReduce tasks over GPU clusters. 
We use Stochastic Petri Net to analyze the influence of GPU 
computing and develop SPN-GC model. The model defines 
formulas of every stage’s execution time and estimates the 
execution time under different input data size. Our 
experimental result presents the comparison between the 
estimated execution time and actual values under different input 
data size. The error range is found out to be within 10%. This 
paper can be a useful reference when a developer is tuning the 
program.  
Keywords-GPU; 
MapReduce; 
Stochastic 
Petri 
Net; 
Estimation of execution time. 
I. 
INTRODUCTION 
As data is growing at an incredible speed, it is not easy to 
handle a huge amount of data to make timely decisions. The 
scale and variety of big data now challenges traditional 
computing paradigms. That is why Cloud Computing could 
come to help. 
MapReduce is being considered as a programming model 
for large-scale parallel processing and an associated 
implementation for processing and generating large data sets. 
The benefits of this model include efficient resource 
utilization, improved performance, and ease-of-use via 
automatic 
resource 
scheduling, 
allocation, 
and 
data 
management. In addition, Graphic Processing Unit (GPU) 
was originally designed for graphics processing. But now, 
because of its parallel computing ability, GPU is more popular 
for scientific and engineering application. 
Many algorithms and applications can be speed up by 
MapReduce, with the power of GPU computing [5]. To 
develop MapReduce over GPU clusters [6], programmers 
have to give thought to some performance-related issues. 
When running a MapReduce job, programmers cannot obtain 
any information about how the performance of jobs will be 
under their testing environment. In the past, programs were 
tuned by running a series of configurations based of past 
experiences and then waiting for jobs to complete for several 
times. If jobs’ performance results can be estimated, 
programmers will be able to shorten their working hours by 
finding out the program’s behavior in advance under a 
particular hardware specification and node configuration. 
In Section II, Stochastic Petri Net [2] for MapReduce on 
GPU clusters model (SPN-GC) is developed to describe the 
detailed operations of MapReduce framework over GPU 
clusters [7]. SPN-GC estimates the execution time of 
MapReduce jobs with given parameters and returns the 
estimated results to programmers as a reference. In Section III, 
we validate the SPN-GC by running experiments. In Section 
IV, we conclude that programmers can use the proposed 
model to estimate and tune the programs in less time and with 
better performance.  
II. 
SPN-GC: MODELING MAPREDUCE OVER GPU 
CLUSTERS  
SPN-GC is divided into nine phases. Each phase 
demonstrates a specific operation in the MapReduce 
framework and is represented by the directed arcs along with 
the transitions from places to next places. Figure 4 illustrates 
a SPN-GC with three map functions and two reduce functions. 
1) Load user program/Split input: When user starts an 
application, a job is assigned to Hadoop jobtracker and 
initialized. Jobtracker of the master node copies user program 
to each tasktracker on worker nodes. According to the user 
program and information, the jobtracker breaks input data 
into splits.  
2) Read input: Every worker node runs its own 
tasktracker as a task manager. Tasktracker first reads input 
splits as input data of map function. Hadoop Disctributed File 
System (HDFS) always distributes input data over all nodes. 
In general, the map function will process the part of input 
splits that is stored on local disks for optimization. If the input 
split is on a remote server, a network transmission is 
initialized and the received data are stored, either temporarily 
in memory or on the disk if the data are too large, until the 
map function is completed. 
3) Map function M/Read into GPU device: Map function 
of the user application takes over Compute Unified Device 
Architecture (CUDA) kernel function is initialized because 
CPU instructs the process to GPU, and data are copied from 
main memory to GPU device memory. Let the number of 
15
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-401-5
PESARO 2015 : The Fifth International Conference on Performance, Safety and Robustness in Complex Systems and Applications

map tasks to be M. Note that map functions might not 
complete at the same time because every work node has its 
own processing speed and different delay time.  
4) GPU computing: CUDA performs data high-speed 
parallel computing on GPU at this phase. Every stream 
processor is assigned to a thread. By Nvidia, 32 threads is 
called a warp. Threads read and write data from shared 
memory on GPU device at the same speed of cache. 
5) GPU device to host: After CUDA finishes the work, 
data are copied from GPU device to host memory for the next 
process. 
6) Sort/ Spill: The map function finishes an input pair and 
has to deal with the outputs from the map function. The 
outputs are called intermediate files in the MapReduce 
process.  
7) Transmit/ Shuffle: After the first map task is finished, 
nodes of map function start to transmit intermediate files to 
nodes of reduce functions either locally or to remote nodes. 
Intermediate files are processed and sorted into the final 
output file for the use of the reduce tasks later. The final 
output file is separated into R partitions by a collector, and 
each partition is transmitted to the corresponding worker 
node that handles reduce tasks. 
8) Sort/ Merge: All the data for the reduce function are 
pre-processing. Partitions are collected and sorted as in the 
map phase. After partitions are downloaded, sorted, and 
merged concurrently, a temporary file is prepared as the input 
data for reduce function. 
9) Reduce function R: The reduce function is defined by 
the application requirements. According to the number of 
reduce tasks set by the user, R reduce functions should be 
executed in parallel, although all reduce functions may not 
start and end together because of varying processing speeds. 
All phases finish when the reduce tasks are completed.  
 
A. Model Formulation 
Let M to be the number of actual map tasks that is 
determined by the split size in Hadoop, and R to be the 
number of reduce tasks that can be configured directly in user 
program. The SPN-GC model can be defined as a marked 
stochastic Petri Net that is a 6-tuple (𝑃, 𝑇, 𝐼, 𝑂, 𝑀0, 𝐿), where  
𝑃 = {𝑝1, 𝑝2, … , 𝑝𝑛𝑝} is a finite set of 𝑛𝑝 places, and  
 
𝑛𝑝 = 5 ∗ 𝑀 + 3 ∗ 𝑅 + 2.

𝑇 = {𝑡1, 𝑡2, … , 𝑡𝑛𝑡} is a finite set of 𝑛𝑡 transitions, where 
 
𝑛𝑡 = 5 ∗ 𝑀 + 2 ∗ 𝑅 + 2.

𝐼 ⊆ { 𝑃 Χ 𝑇 } is a set of input arcs (flow relation), 𝑂 ⊆ { 𝑇 
Χ 𝑃 }  is a set of output arcs (flow relation), and 𝑀0 =
{𝑚1, 𝑚2, … , 𝑚𝑛𝑝} is the set of initial markings where the 
generic entry 𝑚𝑖 is the number of tokens in place 𝑝𝑖, L =
{λ1, λ2, … , λ𝑛𝑡} is an array of firing rates where λ𝑗 is the firing 
rate associated with  each transition 𝑡𝑗 . In SPN, each 
transition is associated with a random variable with 
exponential distribution that indicates the delay from the 
enabling to the firing of the transition. 
There are many selections for firing rate which produces 
the elapsed time at each stage. In this paper, delay on timed 
transitions takes exponential distribution to describe the 
occurrence of events as a Poisson process. The exponential 
probability density function is defined in (3), where 𝜆  is the 
rate parameter of the distribution,  
 
𝑓𝑋(𝑥) = 𝜆𝑒−𝜆𝑥,  𝑥 ≥ 0.

The mean or the expected value of an exponentially 
distributed random variable X for a timed transition  is given 
by (4), and can also be represented as the mean delay time in 
the set T on each timed transition, 𝑇𝑖 .  
 
𝐸[𝑋] = 1/𝜆.

After computing the mean delay time of each transition, 
the inverse can be obtained as a set of firing rates 𝐿 =
{𝜆1, 𝜆2, … , 𝜆𝑛𝑡}  where  𝜆𝑖 = 1/𝑇𝑖 , i = 1,2, … , 𝑛𝑡 , and the 
random time delay can be generated following an exponential 
distribution. 
A big difference between MapReduce application and 
CUDA application is in the splitting of the input data. 
Programmers can select how to cut their input data in 
different sizes or in different ways depending on purpose for 
the input data. In Hadoop, programmers can set the block size 
by configuration. When the job is initialed, input of every 
task will fit the block size based on configuration. Different 
block size may produce unequal map task stages and result in 
various performance. It is similar in CUDA that programmers 
also have to adjust the input size of threads for their specific 
purpose or algorithm.  
B. Model Analysis 
The number of map tasks, M, is related to the number of 
CPU cores on each server which runs tasktracker. According 
to the configuration of Hadoop, the number of map tasks, M, 
is split by the size of blocks, as (5) shows below.  
⌈Input data size (𝐷𝑖𝑛𝑝𝑢𝑡)/split size(𝐷𝑠𝑝𝑙𝑖𝑡)⌉  
Data in HDFS is stored on a data node in which a 
tasktracker resides. Depending on data locations, the speed of 
accessing data locally or to a remote data node might be 
different. In our work, a random variable takes into account 
the ratio of the number of replications to the number of data 
nodes. For each map task j, a random rate of map input on 
local disk, Aj, is defined as below. 
𝐴𝑗 =
𝑁𝑜.𝑜𝑓 𝑟𝑒𝑝𝑙𝑖𝑐𝑎𝑡𝑖𝑜𝑛𝑠
𝑁𝑜.𝑜𝑓 𝑑𝑎𝑡𝑎 𝑛𝑜𝑑𝑒𝑠 , where  𝑗 = 1,2, … , 𝑀;  0 ≤ 𝐴𝑗 ≤ 1. 
In order to estimate the total execution time for a 
MapReduce job over GPU clusters, we need to derive the 
execution time spent in each phase. Starting from phase 1 till 
phase 9, the estimated execution in each phase is studied.  
16
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-401-5
PESARO 2015 : The Fifth International Conference on Performance, Safety and Robustness in Complex Systems and Applications

Phase 1 is for loading program and splitting input data. 
Therefore, 𝑇𝑝ℎ𝑎𝑠𝑒1, the estimated execution time of phase 1, 
is derived in (6). After loading a program, SPN-GC estimates 
the time of data uploading to HDFS and splitting into blocks.  
𝑇𝑃ℎ𝑎𝑠𝑒1 = 𝑇𝑙𝑜𝑎𝑑𝑇𝑢𝑝𝑙𝑜𝑎𝑑
 
Phase 2 is for reading input from local disk and remote 
disk via network. Map-worker node download split from the 
input split locations. Every split data must be read from disk 
of HDFS and transmitted to map-worker node and then 
written into host memory. Therefore, 𝑇𝑃ℎ𝑎𝑠𝑒2 is the maximal 
downloading time of all the nodes that do the map tasks.  
𝑇𝑃ℎ𝑎𝑠𝑒2 = 𝑚𝑎𝑥
1≤𝑗≤𝑚 {
𝐷𝑠𝑝𝑙𝑖𝑡
𝐷𝑖𝑠𝑘_𝑟 𝐴𝑗
𝐷𝑠𝑝𝑙𝑖𝑡
𝐷𝑛𝑒𝑡𝑤𝑜𝑟𝑘1 − 𝐴𝑗 

𝐷𝑠𝑝𝑙𝑖𝑡
𝐷𝑖𝑠𝑘_𝑤1 − 𝐴𝑗

 
Phase 3 is about Mapping to GPU. Tasktrackers start map 
functions and copy input data into GPU devices. The 
execution time of map function can be estimated by the rate 
of data split size, CPU capability, and the time to copy data 
into GPU memory.  
𝑇𝑃ℎ𝑎𝑠𝑒3 =
𝐷𝑠𝑝𝑙𝑖𝑡
𝐷𝑖𝑠𝑘_𝑟 +
𝐷𝑠𝑝𝑙𝑖𝑡
𝑀𝑒𝑚_𝑟 + 𝐶𝑃𝑈𝑡𝑒𝑠𝑡
𝐶𝑃𝑈𝑖 m
𝐷𝑔𝑝𝑢_𝑏𝑙𝑜𝑐𝑘
𝐺𝑃𝑈_𝑀𝑒𝑚_𝑤  
Phase 4 accounts for GPU memory read and GPU 
computing. Input split data is being read into GPU device 
cache from GPU device memory. The execution time of GPU 
computing can be estimated by the rate of GPU block size 
and GPU capability.  
𝑇𝑃ℎ𝑎𝑠𝑒4 = 𝑚𝑎𝑥
1≤𝑖≤𝑛(
𝐷𝑔𝑝𝑢_𝑏𝑙𝑜𝑐𝑘
𝐺𝑃𝑈𝑚𝑒𝑚_𝑟 
𝐷𝑔𝑝𝑢_𝑏𝑙𝑜𝑐𝑘
𝐷𝑡𝑒𝑠𝑡
 
𝐺𝑃𝑈𝑡𝑒𝑠𝑡
𝐺𝑃𝑈𝑖   𝑇𝑚_𝐺𝑃𝑈 

wheren𝐷𝑠𝑝𝑙𝑖𝑡 / 𝐷𝑔𝑝𝑢_𝑏𝑙𝑜𝑐𝑘

Phase 5 is for GPU device to host. After GPU computing, 
output data is copied from GPU device memory to Host 
memory. Key-value pair is ready to sort.  
 
𝑇𝑃ℎ𝑎𝑠𝑒5 =
𝐷𝑠𝑝𝑙𝑖𝑡
𝐺𝑃𝑈_𝑀𝑒𝑚_𝑟
𝐷𝑠𝑝𝑙𝑖𝑡
𝐷𝑖𝑠𝑘_𝑤 +
𝐷𝑠𝑝𝑙𝑖𝑡
𝑀𝑒𝑚_𝑤

Phase 6 is Spill/ Merge. Spills generated either by the 
metadata buffer or by the sort buffer could reach a specific 
limit that can be evaluated. The metadata size is 16 bytes per 
key-value record, while 𝐷𝑚𝑒𝑡𝑎𝑑𝑎𝑡𝑎 is the metadata size for all 
records in the map tasks, which can be evaluated as 16 ∗
𝐷𝑖𝑛𝑝𝑢𝑡/(𝐷𝑡𝑒𝑠𝑡 ∗ 𝑀) ∗ 𝑀𝑎𝑝𝑠𝑝𝑖𝑙𝑙𝑒𝑟_𝑟𝑒𝑐𝑜𝑟𝑑𝑠
 bytes. 
In 
addition,
𝐷𝑚𝑒𝑡𝑎𝑏𝑢𝑓𝑓𝑒𝑟
 is 
equal 
to 
(sort.mb 
* 
sort.record.percent * sort.spill.percent). 
𝐷𝑑𝑎𝑡𝑎 is all key-value data size of the map task. 𝐷𝑑𝑎𝑡𝑎 is 
equal to (𝐷𝑖𝑛𝑝𝑢𝑡/𝐷𝑡𝑒𝑠𝑡) ) ∗ (𝐷𝑚𝑎𝑝/𝑀)  𝐷𝑚𝑒𝑡𝑎𝑑𝑎𝑡𝑎.  
𝐷𝑑𝑎𝑡𝑎𝑏𝑢𝑓𝑓𝑒𝑟 is equal to [sort.mb*(1-sort.record.percent)* 
sort.spill.percent]. While, 𝐷𝑠𝑝𝑖𝑙𝑙  is equal to (𝐷𝑚𝑒𝑡𝑎𝑑𝑎𝑡𝑎 +
𝐷𝑑𝑎𝑡𝑎), meaning that all data sizes must be processed in this 
phase. 
No. of spill(= 𝑁𝑠𝑝𝑖𝑙𝑙) = max ( ⌈ 𝐷𝑚𝑒𝑡𝑎𝑑𝑎𝑡𝑎
𝐷𝑚𝑒𝑡𝑎𝑏𝑢𝑓𝑓𝑒𝑟⌉,⌈
𝐷𝑑𝑎𝑡𝑎
𝐷𝑑𝑎𝑡𝑎𝑏𝑢𝑓𝑓𝑒𝑟⌉). 
No. of merges (= 𝑁𝑚𝑒𝑟𝑔𝑒) = ⌈
𝑁𝑠𝑝𝑖𝑙𝑙
𝑠𝑜𝑟𝑡.𝑓𝑎𝑐𝑡𝑜𝑟⌉. 
 
𝑇𝑃ℎ𝑎𝑠𝑒6 = 𝑁𝑠𝑝𝑖𝑙𝑙 ∗ (
𝐷𝑠𝑝𝑖𝑙𝑙
𝑀𝑅𝑖 +
𝐷𝑠𝑝𝑖𝑙𝑙
𝐻𝑊𝑖 ) + 𝑁𝑚𝑒𝑟𝑔𝑒 ∗ 𝐷𝑠𝑝𝑖𝑙𝑙 ∗ 
( 1
𝐻𝑅𝑖 +
1
𝐻𝑊𝑖 +
1
𝑀𝑅𝑖 +
1
𝑀𝑊𝑖)


Phase 7 is used to transmit and shuffle. The product of  
𝐷𝑖𝑛𝑝𝑢𝑡
𝐷𝑡𝑒𝑠𝑡∗𝑅 and 𝐷𝑚𝑎𝑝𝑜𝑢𝑡𝑝𝑢𝑡 equals the estimated map output size 
serving to be the input data of each reduce-worker node. The 
input data of reduce-worker nodes can be stored on a local 
disk (the reduce-worker node also acts as a map-worker node) 
or on remote map-worker nodes that must transmit data 
through the network.  
𝑇𝑃ℎ𝑎𝑠𝑒7 =
𝐷𝑖𝑛𝑝𝑢𝑡
𝐷𝑡𝑒𝑠𝑡∗𝑅 ∗ 𝐷𝑚𝑎𝑝𝑜𝑢𝑡𝑝𝑢𝑡 ∗ (
𝐴𝑗
𝐻𝑅𝑖 +
1−𝐴𝑗
𝑁𝑒𝑡𝑤𝑜𝑟𝑘𝑖 +
1−𝐴𝑗
𝐻𝑊𝑖)


Phase 8: Sort/ Merge. The data size of reduce input is 
expressed as 𝐷𝑠ℎ𝑢𝑓𝑓𝑙𝑒 , which is 
𝐷𝑖𝑛𝑝𝑢𝑡
𝐷𝑡𝑒𝑠𝑡 ∗ 𝐷𝑚𝑎𝑝𝑜𝑢𝑡𝑝𝑢𝑡 ∗ 𝑀𝑡𝑒𝑠𝑡
𝑅 . 
In shuffling, the downloaded map output is buffered in 
memory first. When memory buffer is filled at a certain level 
of usage, the data are written to the disk, as specified in 
spilling. The time to merge all reduce input data and sort them 
can be estimated as disk read-write and memory read-write 
on 𝐷𝑠ℎ𝑢𝑓𝑓𝑙𝑒 data size. Therefore,  
𝑇𝑃ℎ𝑎𝑠𝑒8 = 𝐷𝑠ℎ𝑢𝑓𝑓𝑙𝑒 ∗ ( 1
𝑀𝑅𝑖 +
1
𝑀𝑊𝑖) 
 + ⌈
𝐷𝑠ℎ𝑢𝑓𝑓𝑙𝑒
𝐻𝑒𝑎𝑝𝑟𝑒𝑑∗𝑠ℎ𝑢𝑓𝑓𝑙𝑒.𝑏𝑢𝑓𝑓𝑒𝑟.𝑝𝑒𝑟𝑐𝑒𝑛𝑡∗𝑠ℎ𝑢𝑓𝑓𝑙𝑒.𝑚𝑒𝑟𝑔𝑒.𝑝𝑒𝑟𝑐𝑒𝑛𝑡⌉ ∗ 
              𝐷𝑠ℎ𝑢𝑓𝑓𝑙𝑒 ∗ ( 1
𝐻𝑅𝑖 +
1
𝐻𝑊𝑖 +
1
𝑀𝑅𝑖 +
1
𝑀𝑊𝑖

The final phase, Phase 9 is about Reduce. In this phase, 
programs may use CUDA for GPU computing. Data in 
memory must be read first, then the elapsed time of reduce 
function is estimated by multiplying 𝑇𝑟  with the rate of 
𝐷𝑠ℎ𝑢𝑓𝑓𝑙𝑒 and the test map output-data size,𝐷𝑚𝑎𝑝𝑜𝑢𝑡𝑝𝑢𝑡.  After 
read and reduce, the output of the reduce function is written 
into HDFS. Hence,  
𝑇𝑃ℎ𝑎𝑠𝑒9 = 𝐷𝑠ℎ𝑢𝑓𝑓𝑙𝑒 ∗
1
𝑀𝑅𝑖 +
𝑚𝑎𝑥
1≤𝑖≤𝑛 ( 
𝐷𝑔𝑝𝑢_𝑏𝑙𝑜𝑐𝑘
𝐺𝑃𝑈_𝑀𝑒𝑚_𝑤
𝐷𝑔𝑝𝑢_𝑏𝑙𝑜𝑐𝑘
𝐷𝑡𝑒𝑠𝑡
 
𝐺𝑃𝑈𝑡𝑒𝑠𝑡
𝐺𝑃𝑈𝑖   𝑇𝑚_𝐺𝑃𝑈
𝐷𝑠ℎ𝑢𝑓𝑓𝑙𝑒
𝐺𝑃𝑈_𝑀𝑒𝑚_𝑟

𝐷𝑠ℎ𝑢𝑓𝑓𝑙𝑒
𝐷𝑚𝑎𝑝𝑜𝑢𝑡𝑝𝑢𝑡 ∗ 𝐶𝑃𝑈𝑡𝑒𝑠𝑡
𝐶𝑃𝑈𝑖 ∗ 𝑇𝑟 +
𝐷𝑖𝑛𝑝𝑢𝑡
𝐷𝑡𝑒𝑠𝑡∗𝑅 ∗ 𝐷𝑟𝑒𝑑 ∗ (
1
𝐻𝑊𝑖 +
1
𝑀𝑅𝑖)


C. Notations and Default Setting  
Major notations used in this paper are summarized in 
Table I. Default values are suggested as well. Detailed 
description of each parameter could be found in [9]. System 
17
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-401-5
PESARO 2015 : The Fifth International Conference on Performance, Safety and Robustness in Complex Systems and Applications

and application related notations used in this paper are 
summarized in Tables I and II, respectively. Default values 
are suggested as well. Detailed description of each parameter 
could be found in [9]. 
 
TABLE I.  
SYSTEM NOTATIONS AND SETTINGS 
Job Phase 
Parameter 
Notation 
Default 
Input split size 
𝐷𝑠𝑝𝑙𝑖𝑡 
64 MB 
No. of referred map tasks 
M_ref 
4 
No. of reduce tasks 
R 
1 
mapred.tasktracker.map.tasks.maximum 
Max_map 
4 
Map Phase 
Parameter 
Default 
Description 
sort.mb 
100 
(MB) 
The amount of buffer space 
to use when sorting streams. 
sort.spill.percent 
0.8 
The amount of sort buffer 
used before spilled to disk. 
sort.record.percent 
0.05 
The amount of metadata 
buffer used in spilling. 
sort.factor 
10 
The number of map output 
partitions to merge at a time. 
Reduce Phase 
Parameter 
Default 
Description 
Max Heap size of 
reduce task (𝐻𝑒𝑎𝑝𝑟𝑒𝑑) 
1024 
(MB) 
Max heap size that can be 
used by reduce task. 
parallel_copies 
5 
The number of map output 
partitions to merge at a time. 
shuffle.buffer.percent 
0.7 
The amount of buffer space 
to use when sorting streams. 
shuffle.merge.percent 
0.66 
The amount of the sort buffer 
used before data spill to disk. 
 
III. 
VALIDATION OF SPN-GC  
Platform-Independent Petri Net Editor 2(PIPE2) v4.3.0 [3] 
is an open-source tool that supports the design and analysis 
of Stochastic Petri Net models. PIPE2 uses the “xml” format 
that is easy to describe the form of a Petri Net. The SPN-GC 
is validated to conform the regulations of Stochastic Petri Net 
by PIPE2. Our SPN-GC simulator is constructed using Java 
and is expected to be released as a package in PIPE2 soon.  
A. Simulation Settings 
In our experimental environment, we run Hadoop-1.2.1 as 
MapReduce framework [1] of four worker nodes with one 
physical server each. Each physical server is equipped with 
GPU of model NVIDIA Tesla C2050 [4]. The details of 
hardware specification and software version can be found in 
[9].  
The speed of memory is measured by using the 
“dmidecode” command on Linux. The system measures the 
elapsed time when creating 100 of 1000 bytes blocks to read 
in and read out. The speed of disk is measured in the same 
way. GPU device information can be queried from NVIDIA 
system management interface.  
Three GPU computing benchmarks are studied in our 
experiments: converting side-by-side (SbS) video to depth 
video, matrix multiplication, and K-mean clustering. Due to 
the paper length, here we present the experimental results of 
3-D video case and K-means only. Interested readers are 
recommended to study further [8][9]. 
For 3-D video conversion, the program is to transfer side-
by-side video into depth video based on tremendous graphic 
processing. Every frame in the SbS video is composed of two 
almost-identical pictures except a little different angle of 
camera view. The depth video can be played on a 3-D monitor 
and delivers the 3-D visual effect. The data set was collected 
from Internet and the file format is any SbS video of .mp4 file.  
 
We use these SbS videos to compare the accuracy between 
the actual job execution time and the estimated execution time 
by SPN-GC. Since most of the GPU computing applications 
are not utilizing the MapReduce framework yet, we need to 
port the program written in CUDA to MapReduce framework. 
This justifies our major contribution in the area. To collect 
data, ten sets of different data size are fed in to each program 
to test the performance under different input configurations. 
The second experiment is about K-means clustering, 
which is a method of vector quantization, originally from 
signal processing. A popular cluster analysis in data mining, 
K-means clustering aims to partition n observations into k 
clusters in which each observation belongs to the cluster with 
the nearest mean, serving as a prototype of the cluster. The 
problem is known to be computationally difficult (i.e., NP-
hard). In our experiment, the observations are generated by 
random variables using time values as seed. 
Our SPN-GC simulator is following the nine-phase 
execution and calculating the mean delay time to estimate the 
mean elapsed time in each phase and return the total estimated 
execution time. Exponential distributed random variables are 
used. Every run of simulation was performed 2000 times. The 
average of all simulated values is returned as the estimated 
execution time for the program. 
B. Experimental Results 
Figure 1 shows the results of converting SbS videos into 
depth videos. The execution time of actual test and estimated 
TABLE II.  
APPLICATION RELATED NOTATIONS  
Parameter 
Notation 
Input data size of the estimated job 
𝐷𝑖𝑛𝑝𝑢𝑡 
Input data size of the test small job 
𝐷𝑡𝑒𝑠𝑡 
Program loading and data split time of the test small job 
𝑇𝑙𝑜𝑎𝑑 
Exec. time of map function in the test small job 
𝑇𝑚 
Exec. time of reduce function in the test small job 
𝑇𝑟 
Size after executing map function of the test small job 
𝐷𝑚𝑎𝑝 
Size after executing reduce function of the test small job 
𝐷𝑟𝑒𝑑 
Size of map output which is equal to reduce shuffle bytes  
𝐷𝑚𝑎𝑝𝑜𝑢𝑡𝑝𝑢𝑡 
Data size of GPU block bytes 
𝐷𝑔𝑝𝑢_𝑏𝑙𝑜𝑐𝑘 
18
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-401-5
PESARO 2015 : The Fifth International Conference on Performance, Safety and Robustness in Complex Systems and Applications

time by SPN-GC for input video length from 30, 60, 90, 120, 
150, 180, 210, 240, 270, 300 seconds are shown respectively. 
We can see that the average of estimated execution time and 
actual test values were very close.  
Figure 2 illustrates the results of the K-Means execution 
time of actual test and estimated execution time by SPN-GC.  
The input data size ranges from 0.5, 1, 2, 3, 4, 5, 6, to 7 
millions. Every block reads 20 thousands as input. The 
application randomly chooses 10 points as the centers, then 
partition n observations into 10 clusters with the nearest mean 
to the cluster. We can see that the estimated execution time of 
each input data is very close to the actual test value.  
To evaluate the significance of SPN-GC model, we 
compare the time taken by SPN-GC and by actually executing 
the test job, Time Cost Ratio is calculated to reflect how much 
time saving is obtained by SPN-GC estimation. SPN-GC is 
proved to be able to save lots of time in cluster selection or 
performance tuning.  
Time Cost Ratio =
𝑡𝑖𝑚𝑒 𝑠𝑝𝑒𝑛𝑡 𝑏𝑦 𝑆𝑃𝑁−𝐺𝐶 
𝐴𝑐𝑡𝑢𝑎𝑙 𝑗𝑜𝑏 𝑒𝑥𝑒𝑐𝑢𝑡𝑖𝑜𝑛 𝑡𝑖𝑚𝑒 ∗ 100%
 
 
 
Figure 1.  Job Execution Time of Side-by-side Videos. 
  
Figure 2.  K-Means Execution Time. 
Figure 3 draws the time cost ratio of SPN-GC to actual 
time cost for K-means clustering. The input range is from 0.5 
to 8 millions of observations. We can see that, as the data size 
grows, the ratio drops to 0.7% approximately. It can be 
identified that SPN-GC would be able to perform an accurate 
estimation of the execution time for a job with a very small 
time cost. The benefit becomes more significant when the size 
of the problem increases. 
 
Figure 3.  Ratio of SPN-GC Time Cost. 
IV. 
CONCLUSIONS 
In this paper, we develop the SPN-GC model to estimate 
the execution time of a job under the MapReduce framework 
over GPU clusters. Job execution time is an important 
performance indicator that provides crucial information for 
cluster evaluation. The considered environment is that input 
data are split into Hadoop block sizes and then spilt it again 
into the blocks for CUDA in GPU computing. There is also a 
problem that when GPU is computing graphic processing, 
every block gets different data and therefore each block has 
its own complexity. The data complexity and mean delay time 
are solved under the assumption of exponentially distributed 
random variables. In experimental results, SPN-GC is 
validated by PIPE2 and compared the estimation execution 
time with actual data test under three applications. The 
average error range of estimation execution time was found to 
be within 10%. SPN-GC can be a reference to evaluate GPU 
clusters performance. 
ACKNOWLEDGMENT 
This work is supported in part by Ministry of Science and 
Technology, Taiwan, R.O.C. under the grant number “102-
2221-E-006-086-MY3” and by Ministry of Economic Affairs, 
Taiwan, R.O.C. under the grant number “103-EC-17-A-02-
S1-201” respectively. 
 
REFERENCES 
[1] G. Wang, A. R. Butt, P. Pandey, and K. Gupta, “A Simulation 
Approach to Evaluating design decisions in MapReduce 
setups,” IEEE Symposium, Modeling, Analysis & Simulation 
of Computer and Telecommunication Systems, pp. 1-11, 2009. 
[2] M. K. Molloy, “Performance Analysis Using Stochastic Petri 
Nets,” IEEE Transactions, Computers, vol. C-31, issue 9, pp. 
913-917, 1982. 
0
200
400
600
800
1000
1200
1400
0
50
100
150
200
250
300
350
EXECUTION  TIME  (SECONDS)
LENGTH OF  INPUT VIDEO (SECONDS)
J O B  E X E C U T I O N  T I M E  O F  S B S  V I D E O S
Actual test
Estimation
0
100
200
300
400
500
600
700
800
0
1
2
3
4
5
6
7
8
EXECUTION TIME (SECONDS)
OBSERVATIONS (MILLIONS)
K - M E A N S  E X E C U T I O N  T I M E
Actual test time
Estimation
0.00%
0.20%
0.40%
0.60%
0.80%
1.00%
1.20%
1.40%
0
1
2
3
4
5
6
7
8
TIME COST RATIO (%)
OBSERVATIONS of K-MEANS (MILLIONS)
TIME COST RATIO OF SPN-GC TO ACTUAL TEST
19
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-401-5
PESARO 2015 : The Fifth International Conference on Performance, Safety and Robustness in Complex Systems and Applications

[3] N. J. Dingle, W. J. Knottenbelt, and T. Suto, “PIPE2: A Tool 
for the Performance Evaluation of Generalized Stochastic Petri 
Nets,” ACM SIGMETRICS, Performance Evaluation Review, 
vol. 36, issue 4, pp. 34–39, 2009. 
[4] E. Lindholm, J. Nickolls ,S. Oberman, and J. Montrym, 
“NVIDIA Tesla: A Unified Graphics and Computing 
Architecture,” IEEE conference, pp. 39-55, 2008. 
[5] Y. Guo, W. Liu, G. Voss, and W. Mueller-Wittig, “GCMR: A 
GPU Cluster-based MapReduce Framework for Large-scale 
Data Processing,” IEEE conference, High Performance 
Computing and Communications & Embedded and Ubiquitous 
Computing, 
pp. 
580-586, 
2013, 
doi:10.1109/tencon.2013.6719008. 
[6] J. A. Stuart and J. D. Owens, “Multi-GPU MapReduce on GPU 
Clusters,” IEEE Intl. Symp. Parallel & Distributed Processing, 
pp.1068–1079, 2011,.  
[7] H. Gao, J. Tang, and G. Wu, “A MapReduce Computing 
Framework Based on GPU Cluster,” IEEE Conference, High 
Performance Computing and Communications & Embedded 
and Ubiquitous Computing, pp. 1902-1907, 2013. 
[8] H. Wang, “Using Petri Net to Estimate Job Execution Time in 
MapReduce Model,” master thesis, National Cheng Kung 
University, Taiwan, 2013. 
[9] H. Yang, “Estimation of Job Execution Time in MapReduce on 
GPU Clusters,” master thesis, National Cheng Kung 
University, Taiwan, 2014. 
 
 
Figure 4.  Nine phases of SPN-GC (with M =3, R =2). 
 
20
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-401-5
PESARO 2015 : The Fifth International Conference on Performance, Safety and Robustness in Complex Systems and Applications

