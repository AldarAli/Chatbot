An Approach Towards Artistic Visualizations of Human Motion in Static Media 
Inspired by the Visual Arts 
 
Anastasia Rigaki, Nikolaos Partarakis, Xenophon 
Zabulis 
Institute of Computer Science, Foundation for Research and 
Technology – Hellas (FORTH), 
Heraklion, Greece 
e-mail: {rigaki, partarak, zabulis}@ics.forth.gr 
Constantine Stephanidis 
Institute of Computer Science, Foundation for Research and 
Technology – Hellas (FORTH), 
Department of Computer Science, University of Crete 
Heraklion, Greece 
e-mail: cs@ics.forth.gr
 
 
Abstract— The visualization of 3D human motion on a 2D 
canvas or display is employed by a wide spectrum of 
disciplines to abstract and provide insight on the motion of 
human subjects that is depicted by the 2D medium. Painters, 
illustrators 
and 
directors 
use 
motion 
lines, 
contrast, 
superimposition as well as juxtaposition of visual frames for a 
better conveyance of motion. The proliferation of digital 
cameras, motion sensors, combined with computer vision has 
enabled the 3D recording of human motion in a wide range of 
conditions. At the same time, applications of human motion 
visualization, 
such 
as 
illustrated 
safety 
or 
assembly 
instructions, still have a wide use in conventional depictions of 
human motion, namely 2D static depictions, whether these are 
presented on screen or on paper. Inspired by the depiction of 
human motion in the visual arts, we transfer pertinent visual 
approaches to the domain of human motion visualization. Our 
goal is to utilise these visualization techniques and create 
insightful visualizations of human motion recordings on static 
2D media. To that end, we propose the MotiVo system that 
dynamically integrates multiple tools for the visualization of 
human motion. Based on these tools, we study basic 
approaches of human motion visualization and abstraction. 
Keywords-Motion visualization; Artistic Visualization; Motion 
capture; Image processing;Computer Vision. 
I. 
 INTRODUCTION 
In visual arts, human motion and activity are often 
conveyed through still depictions or sculptures. Depiction 
of motion is an important part of artistic expression. Over 
the years, artists have depicted both motion (e.g., Claude 
Monet, En Plein Air, 1886) and lack of motion, (e.g., 
Johannes Vermeer's Woman Holding a Balance, 1664) as a 
way to stimulate interest [1]. We call visual abstraction, a 
drawing that encapsulates events lasting more than one 
moment and possibly occurring in more than one location. 
In essence, a visual abstraction is a manipulation of realistic 
imaging aiming to convey an understanding of the events 
occurring within a time-space interval. Motion is 
effectively conveyed in static media using superimposed 
and juxtaposed images. Pertinent techniques are based on 
the cognitive capability of the observer to “fill-in” missing 
information. In this way, the depiction encodes an event, 
taking place during a time interval rather than a 
photographic recording of a single  moment. Superimposed 
forms are employed in the visual arts to summarize motion 
within a short time interval, taking place at a location 
(Figure 1). 
Figure 1. (a) Man walking, Marey 1891 (b) Calder's Ascension, Head 
2017 (c) Cheval blanc monté, Marey 1886. 
Juxtaposed illustrations are used in comics [2] and 
illustrated instructions to convey motion. Visualizing 
motion, as a sequence of juxtaposed key pose [3] depictions, 
provides a clear understanding of the illustrated motion. 
Annotations, such as motion lines, provide visual cues to 
motion and facilitate understanding (Figure 2). 
  
 
Figure 2. Juxtaposed illustrations encoding motion. 
Representation of motion and activity in longer time 
intervals or scenes has been treated in art by manipulating 
time in the depiction, so that multiple time instances are 
seamlessly summarized, or “gracefully superimposed” 
without affecting the realism of depiction. For example, in 
Figure 3, it is the characteristic activity of each person 
depicted by each form in the painting rather than a 
photographic depiction of a moment. If the depiction would 
be literally considered, the depicted behaviours would 
probably not occur simultaneously. Instead, the painting 
264
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

summarizes the behavior of each character during the 
depicted event. The painter guides the observer to examine 
each form sequentially. The dominant stroke of light creates 
a salient visual path in the painting. Then, the attention [4] is 
guided by contrast changes in an elaborate visual path that 
visits the depicted characters and reveals the interaction 
among them. 
 
Figure 3. The Calling of St. Matthew, Caravaggio 1599-1600. 
Juxtaposition is also used in illustrated instructions (e.g., 
manuals) as an ordered representation of images combined 
with written information and graphical annotations, to direct 
the reader (Figure 4). 
 
 
 
Figure 4. Motion and action visualization in instruction manuals. 
 
Although this was conventionally a manual task for 
illustrators, nowadays technology is offering a plethora of 
tools for digital creativity. It is common for graphic 
designers and illustrators to use image-processing software 
in order to simplify authoring and enhance visualization. 
Though these tools are a commodity, they still require 
insight and art skills from the illustrator. The efficiency of 
communication and the abstraction of form is also noted as 
another inspirational aspect of story-telling visuals. Le 
Corbusier in a letter, he describes his project through 
suggestive drawings. The style reminds of the so-called 
“ligne claire” (clear line) [5], whose precursor is now 
recognized in Rodolphe Töpffer [6]. The technique is 
explained in: “Le Corbusier obsessively draws “after” 
photographs as in an attempt to remove anything 
superfluous”. The overlaps with Töpffer were particularly 
vivid in Le Corbusier’s sketches of human body actions, 
creating figures with a dynamism and liveliness (Figure 5). 
Le Corbusier’s trademark line style transformed his 
architecrural representations to a graphic narrative 
communication tool. 
 
 
 
Figure 5. Reduction of complex photographs into drawings. 
 
The goal of this research is to build over centuries of 
experience in the domains of the visual arts and implement 
a system that transfers artistic concepts in the digital world. 
Although there are techniques that offer a wide range of 
motion visualization tools, some of them focusing on 
Motion Capture (MoCap) whereas others work exclusively 
on motion visualizations, there is still need for an 
interactive and simple to use editor. Existing works are 
mostly targeted to one specific motion visualization 
technique that fits their work subject, nevertheless, there 
have not yet been presented works for general purposes. In 
this context, we present MotiVo, an interactive system that 
simplifies this process by offering a number of 
visualization tools and provides insightful and visually 
pleasant 
results 
requiring 
minimum 
expertise 
and 
knowledge from the user side. Using these tools, motion is 
visualized by parameters, such as the blending of key poses 
retrieved from an activity, the visualization of motion 
trajectories, the application of image filters to visualizations 
and their 3D and 2D combinations for hybrid depictions of 
motion. 
The rest of the paper is structured as follows. In Section 
II, we summarize the related work. In Section III, we 
present system’s input data and also give a brief description 
of how we acquired them. In Section IV, the MotiVo user 
interface and its tools are described. In Section V, there are 
presented some experimental results as well as we give 
some guidelines for a better experience according to an 
expert-based evaluation with the tools. Finally, we present 
our future work and draw some conclusions (Section VI). 
II. 
RELATED WORK 
A large number of existing studies in the broader 
literature have examined motion visualization techniques. 
Digital artists, such as T. Gremmler, use technology to 
produce artistic visualizations of motion. Gremmler has 
rendered a sequence of animations that illustrate the 
disciplined and defined movements of Kung Fu practice in 
the form of shapes, geometries, and abstract shapes. Human 
figures are reduced to a minimum of a simple sequence of 
lines, lines, and points, which adopt postures and pose 
throughout the video [7]. 
In this context, technical state-of-the-art is offering a 
number of alternative ways to digitize human motion, such 
as MoCap and Computer vision technologies [2][8]-[10]. 
 
265
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

These 
technologies 
provide 
accurate 
digitized 
representation of movement in 3D [11][12]. At the same 
time, in the domain of computer graphics, a wide range of 
visualization tools are available that allow the simplification 
of the production of 2D and 3D visualizations. 
Key Probe is a key-frame extraction technique, relied on 
an algorithm appropriate for rigid-body and soft-body 
animations that converts a skeleton based motion or 
animated mesh to a key frame-based representation [13]. To 
select a representative moment from a performance, they 
introduce “Action Snapshot”, a method based on information 
theory that automates the process of generating meaningful 
snapshots, by taking as input dynamic scenes as input and 
producing a narrative image as output [14]. 
3D visualization has also been proven valuable in the 
demonstration of Motion Capture (MoCap) data. Such an 
example is TooltY, a 3D authoring platform for 
demonstration of simple tool operations in 3D environments 
[15]. In sports, human motion visualization is used to display 
3D models of swimmers by digitizing their motions and 
creating personalized virtual representations [16]. Lucent 
Vision is a visualization system developed for tennis. It uses 
real-time video analysis to extract motion trajectories and 
provides a variety of visualization options [17]. 
Action summarization is prevalent in the human motion 
visualization community, as it can produce motion effects in 
still image frames. “Action Synopsis” takes as input human 
movements, encoded either as MoCap animations or videos 
and presents motion in still images [18]. The work [19] 
creates compact narratives from videos, by composing 
foreground and background scene regions into a single 
interactive image, using a series of spatiotemporal masks. 
Depth information of animations assists summarization 
of 3D animations in a single image. A method that extracts 
important frames from the animation sequence based on the 
importance of each frame is proposed, based on its 
contribution to overall motion-gradient [20]. 
Similar work has also been developed for the artistic 
motion visualization. In [21], simulates thick, dominant 
brush strokes, to place emphasis on important line features of 
an image. M.G Choi proposed an interface, where the user 
browses overall motion visualized by a unified medium in 
the form of 2D stick figure images [19]. 
All these proposals offer a wide range of motion 
visualization techniques but they lack in terms of intuition, 
interaction and ease of use. In this work, we identify a gap 
between motion digitisation and insightful, artistic motion 
visualization. We propose a bridging of these dimensions in 
a single workflow. To realise this approach, we implemented 
MotiVo, a 2D visualization editor. In this editor, two tools 
are presented for the production of motion visualizations. 
The first is based on video key frames. The second visualizes 
2D motion trajectories computed by visual tracking. To 
further assist users, we include post-processing tools that 
enable the application of image filters to input visual assets 
and manual annotation of upon these assets. Finally, we 
propose a method to use 3D information about human and 
object motion to enrich the produced visualizations. Our 
editor does not only focus on motion visualization effects, 
but also facilitates users to extend the tools through their 
combination in order to generate unique representations of 
motion visuals. 
 
III. 
ACQUIRING VISUALIZATION SOURCES DATA 
FROM IMAGE, VIDEO AND HUMAN MOTION 
For the purposes of this research work, two types of 
potential digital input are of interest. The first type is still 
images and image frames acquired from video and the 
second type is MoCap data. In order to collect data for our 
case study, we use the following methodology. Initially, we 
recorded in a video format of a person performing typical 
activities, such as waving. The video stream was segmented 
to video frames and frames of interest were then extracted 
from the video stream to produce a summarization in 
frames of movements. Furthermore, the video stream was 
used as a source for the Open Pose Computer Vision library 
[22]. Using the Open Pose output in this research work, we 
isolated specific joints of the skeleton in order to produce 
the trajectories path for our visualization algorithm. 
 
IV. 
THE MOTIVO APPROACH 
 
A. MotiVo tools 
1) Motion Blender 
Motion blender creates a directional motion effect by 
overlapping key poses into a united content. Specifically, as 
a dataset, each user is able to import multiple strong key 
pose images, they consider as the main motions of an 
activity. This dataset differs for every user. The 
combination of all the frames summarizes the overall 
action. Besides the visualization of motion direction, the 
user can emphasize on each frame with varying contrast 
and color intensity volumes. The contrast intensity of each 
key pose is determined by a value selected by each user via 
the tool’s User Interface (UI) sliders. The value range is 
between zero and one hundred, with zero to be the lowest 
contrast value whereas one hundred the maximum intensity. 
Depending on the nature of activity, the emphasis on the 
each image frame may be differentiated. In many cases the 
most significant action is the initial, middle or the last one. 
For example, on hammering a nail, the dominant action 
could be the last key pose, which indicates exactly how to hit 
the top of the nail, therefore this key frame is highlighted the 
most. In contrast to the previous scenario, in dance 
choreographies the motion sequence can be evenly defined 
as important, thus the relative intensity of each key pose is 
uniformly visualized. Users on runtime can view the 
resultant image. The output is a single image that can be also 
saved as an asset in the current project and then be loaded by 
another MotiVo tool for further processing. A color image I 
at coordinates x, y has pixel value c = I(x, y) where c has 
values {R, G, B}.We do not treat monochromatic (grayscale) 
differently. If such input is given, the monochromatic 
266
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

channel is replicated in all the RGB (Red Green Blue) bands 
and the image is treated as a color image. 
Let n be the number of key frames selected by the user. 
We denote by Ii the corresponding images, where i is in [1, 
n]. In case of even distribution between contrast and 
volume of the image frames, for each Ii we average the 
corresponding pixel RBG values from the color arrays of 
each selected image (1). This can be achieved in case of all 
the number values are set evenly. The combined result is a 
visualization of all the images demonstrating a motion 
sequence (Figure 6).  
 
 
 
 
       (1) 
 
 
Figure 6. Averaged Motion visualization. 
 
     An extended approach of the previous scenario is the 
weighted visualization of human motions. In this version, 
users alter the UI slider values thus setting different weight 
value wi for each image frame to denote the contrast intensity 
depicted in the final result (2). For lower weight values, the 
frames have low opacity volume in contrast to high weights 
values where the RGB values are greater. The effect of 
fading and overlapping motion allows the human eye to 
perceive easily the chronological order of the action (Figure 
7). 
  
 
(2) 
 
 
 
 
Figure 7. Weighted motion visualization. 
 
Average and weighted visualization of Motion Blender were 
implemented using the Windows Presentation Foundation 
(WPF). In order to rectify the problem of CPU overhead in 
pixel operations, the algorithms, we developed, were 
optimized using parallel loops and direct memory access. 
2) Motion Annotator 
 
The second tool of MotiVo editor is Motion Annotator. 
This tool takes as an input, an image frame depicting a 
human action, (e.g., the produced image from Motion 
Blender) as well as a trajectory file containing the specific 
joints of the skeleton body as (x, y) coordinates in 2D space. 
The trajectory files are generated by the OpenPose output 
for a specific joint of interest, thus isolating the movement 
of this joint for visualization purposes. These (x, y) 
coordinates are visualized on the canvas of the loaded input 
image. The algorithm we have developed, highlights with 
artistic designs the specific coordinates provided by the 2D 
trajectory file. There are multiple ways of hi-fidelity artistic 
representations that can be depicted based on the trajectories 
(i.e., bullets, simple lines, comic style lines). The annotated 
points on the image frame denote the direction of the 
motion. For example, in the case of hand waving, by 
annotating the image with the retrieved points, it highlights 
the human hand trajectory during the activity (Figure 8). 
 
 
 
 
Figure 8. Motion annotations using Motion Blender (top) and Motion 
Annotator with trajectories extracted by Open Pose (bottom). 
 
To smooth the visualized trajectories, we used composite 
Bézier curves. In computer graphics, a composite Bézier 
curve is a piecewise Bézier curve that is at least continuous. 
In other words, a composite Bézier curve is a series of 
Bézier curves joined end to end where the last point of one 
curve coincides with the starting point of the next curve 
[23]. 
 
3) Manual motion enhancer 
In some cases, there is need for manual annotation of 
motion, especially where the context information is to be 
added to the visualization. To that end, we exploit the 
techniques used to create juxtaposed illustrations in comics. 
The manual motion enhancer is an editing component that 
allows users to load an existing image result from the 
project assets and manually enhance it by attaching ready to 
use concepts and icon sets (i.e., arrows, lines, etc.), such as 
those in comics (Figure 9). 
 
4) Image filters 
 
This tool receives as input an image file and provides a list 
of image filters that can be used image similar to the ones 
267
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

used in popular image processing software [10][24]. 
Currently a wide array image transformations, color 
operations and artistic transformations are available. An 
example of edge detection is presented in Figure 10. 
 
 
 
Figure 9. Motion frames enhancement using icon sets 
 
 
 
Figure 10. Edge detection filter. 
 
5) Scene composer 
The scene composer was inspired by the depictions of 
crafts in art and photography (Figure 11). Its goal is to 
abstract human motion and tools to the minimum 
ingredients. 
 
 
 
 
 
  
Figure 11. Depiction of tools grip and usage in the visual arts (left, 
middle) and moment of interest for Scene Composer (right). 
 
As a result, scene composer is capable of depicting the 
essential parts of the craft so as to assist human perception 
and understanding and improve the development of 
captivating visualization for information and education. 
Scene Composer is a tool that takes as input an overview 
of the scene from a static moment in the course of a craft 
action, (e.g., passing the shuttle through a loom). This static 
motion frame is then used by a computer vision tracker, 
capable of tracking the position and orientation of hands and 
objects. An example of such a moment where both the hand 
posture and the tool orientation can be extracted (Figure 11). 
In both cases, we march from the hypothesis that the 
tracker has already the geometry and texture of the object to 
be tracked, (e.g., a generic model of a hand or a 3D model of 
the tool to be tracked). In both cases, the tracker is estimating 
the Rotation and Translation and Scale of the object tracked 
(i.e., hand, or tool) and overlays the model on top of the 
given frame for abstraction and emphasis on the spatial 
arrangement of the critical actors in the scene. Scene 
composer is inspired by visualization in robotics, where they 
are used to create a human-comprehensible illustration of the 
model of that the robot has built for its environment. In 
Figure 12, an example of such a visualization is shown for 
the case where a robotic manipulator grips objects upon a 
table top. The visualization illustrates the location and poses 
that the robot has estimated regarding the objects on the table 
top (blue) and its own manipulator (red). 
 
Figure 12. Illustration of object and robotic manipulator localization. 
 
In Figure 13, the process of integrating information in 
static frames using the scene composer tool is illustrated. In 
the illustrations, on the right is the 3D model of the tool that 
is tracked and on the right the inference of the position of 
the tool at the imaged moment. 
 
 
Figure 13. Visualization of tool usage. 
 
B. Extendable UI architecture 
The main requirement of the system-UI architecture of 
MotiVo was to use a plug-in based software development 
pattern where there is a distinction between the main system 
and visualization tools. This distinction was important so as 
to develop an extendable motion visualization system where 
new components are added as new visualization tools arise. 
In such a context, it was decided that the main system should 
support the creation of project files and the assignment of 
assets to these projects (images, videos, Biovision Hierarchy 
(BVH) files, image trajectories, etc.). All tools should be 
then loaded as window components and should be drag-drop 
268
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

enabled. Taking into consideration this main requirement, a 
dockable UI container architecture was selected, based on 
the one followed by Integrated Development Environments. 
In this architecture, components can be loaded and unloaded 
on the fly and new components can be loaded as plugins by 
integrating a new dockable window to the main window 
manager. Furthermore, this was considered a good option as 
one of the target groups of this tools are technical people 
with expertise on using dockable layouts. The only limitation 
is that typically such UIs may have a longer learning curve 
that simpler ones with the added value of extensibility and 
extensibility in the future. For the purposes presented above 
it was decided to structure the UI of MotiVo on top of the 
DevZest WPF Docking library [25]. 
 
V. 
EXPERIMENTS AND GUIDELINES 
 
A formative evaluation was conducted in the context of 
the Mingei Innovation Action under the Horizon 2020 
Programme of the European Commission [26]. The 
evaluation was based on images and videos of craft 
practitioner recordings in the context of the Mingei project. 
Based on these datasets, an expert-based evaluation was 
performed and several experiments were conducted with 
practitioners to assess our motion visualization strategies by 
using all the MotiVo tools. The outcome of those 
experiments formed a set of preliminary guidelines to 
address the needs of each individual tool. 
Guidelines 
In order the users to use MotiVo editor efficiently, we 
propose some guidelines based on an expert-based 
evaluation. 
1) Motion Blender 
After experimenting with weight values, we concluded 
that imaging settings play a significant role in the outcome. 
Guideline 1: Prefer image sequences acquired through 
static camera. 
Guideline 2: When working with a moving camera select 
a frame of reference where the camera is static and another 
one with multiple changes happening in the initial scene. 
This will improve the blending quality. 
2) Motion Annotator 
Guideline 1: The use of 2D trajectories can be sometimes 
be non-representative. The total number of Points (x,y) 
should be sufficient for the trajectory visualization to be 
precise. Despite the fact that we use Bezier splines to design 
curves, the input files content should be a good starting point 
for the Motion Annotator. 
3) Manual motion enhancer 
Guideline 1: Depending on the nature and style of the 
annotated image, users should choose graphic elements of 
similar style so as to fit the context. 
Guideline 2: For comic style images, annotations should 
be comic styled or even multicolored whereas in the case of 
simple and minimal images, artistic arrows or other 
elements are the appropriate ones. 
Guideline 3: Due to the fact that we are trying to 
visualize motion in 2D space, users are advised to adjust the 
projection on the annotation stickers so as to indicate the 
depth and direction of motion. 
4) Image Filters 
Guideline 1: Motion filters may be a powerful tool for 
post and pre-processing results. Experiment with motion 
filters to have an overview of the potential outcomes. 
Guideline 2: When a motion blending fails you can 
facilitate motion filters to simplify the input of motion 
blender and thus get better Visualization results. 
5) Scene Composer 
Guideline 1: Make sure that source frames have 
sufficient information regarding the visualized-tracked 
object or hand (i.e., it is clearly visible). 
Guideline 2: Avoid using frames where the position of 
the object can only be inferred through the position of the 
hand (non-occluded hand but occluded object). 
6) Combined usage of tools 
Guideline 1: Create richer motion Visualization by 
combining several MotiVo tools in the same Visualization 
project (Figure 14). 
 
 
 
Figure 14. Visualization of tool usage through the estimation of hand and 
3D model pose within the static motion frame 
 
VI. 
CONCLUSION AND FUTURE WORK 
This paper has presented an approach towards the 
visualization of data stemming from video recordings and 
visual tracking of human movement. Until now such 
visualizations were mainly targeted to the actual 
reproduction of motion in 3D or 2D space, such as for 
example the preview of MoCap output or the visualization 
of 2D pose estimations on top of the video or image 
sources. 
Inspired by motion visualization in art, cinema and 
design we marched into implementing MotiVo, a motion 
visualization editor that is comprised of four distinct tools 
built on top of a plugin-based architecture capable of being 
extended and enriched in the future. 
Based on the experience gained in this research work 
and the experiments performed in the context of 
implementing the visualization tools, it can be safely 
concluded that artistic visualization of human motion in 2D 
is technically feasible and can produce aesthetically 
pleasant results. Of course, human intervention is needed to 
orchestrate the appropriate selection of tools. This paper 
reports not only on the implementation of these tools, but 
269
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

also through the experience during implementation and 
experimentation provides a set of best practice guidelines so 
as to get the most 
[2] out of these tools. 
The authors plan to enhance their approach by exploring 
the advances on style transfer algorithms to provide even 
better visualizations. Furthermore, the integration of 3D 
information to the static motion frames will enable the use of 
3D data, in favor of motion analytics and visualization. 
Finally, video visualizations would be easily produced and 
even the isolation of the actors in a movement sequence 
would be facilitated, in order to reproduce them in another 
context, for example, in Virtual Reality (VR) training. 
ACKNOWLEDGMENT 
 
For this work, data stemming from the three pilot sites of 
the Mingei H2020 EU funded project (GA No. 822336) were 
used. The authors are grateful to project partner ARMINES 
for the acquisition of MoCap data. 
REFERENCES 
 
[1] S. Zeki, “An exploration of art and the brain,” in Inner 
Vision, 2000. 
[2] S. McCloud, “Understanding comics: The invisible art,” 
Northamp. Mass, 1993. 
[3] P. J. Kellman and T. F. Shipley, “A theory of visual 
interpolation in object perception,” Cognit. Psychol., vol. 
23, no. 2, pp. 141–221, 1991. 
[4] M. E. Chevreul, The Laws of Contrast of Colour, 1858. 
[5] L. Arana and L. Miguel, “La Ligne Claire de Le 
Corbusier. Time, Space, and Sequential Narratives,” 
presented at the Le Corbusier, 50 Years later, Valencia, 
2015. 
[6] C. L. Marcos, Graphic Imprints: The Influence of 
Representation and Ideation Tools in Architecture. 
Springer, 2018. 
[7] “Kung Fu Motion Visualization,” Vimeo. [Online]. 
Available: https://vimeo.com/163153865. [Accessed: 13-
Jan-2020]. 
[8] C. M. Brigante, N. Abbate, A. Basile, A. C. Faulisi, and 
S. Sessa, “Towards miniaturization of a MEMS-based 
wearable motion capture system,” IEEE Trans. Ind. 
Electron., vol. 58, no. 8, pp. 3234–3241, 2011. 
[9] M. Müller, T. Röder, M. Clausen, B. Eberhardt, B. 
Krüger, and A. Weber, “Documentation mocap database 
hdm05,” 2007. 
[10] D. A. Ross, J. Lim, R.-S. Lin, and M.-H. Yang, 
“Incremental learning for robust visual tracking,” Int. J. 
Comput. Vis., vol. 77, no. 1–3, pp. 125–141, 2008. 
[11] Dariush, M. Gienger, A. Arumbakkam, C. Goerick, Y. 
Zhu,  and   K.   Fujimura,   “Online   and   markerless 
motion retargeting with kinematic constraints,” in 2008 
IEEE/RSJ International Conference on Intelligent Robots 
and Systems, 2008, pp. 191–198. 
[12] C. Hecker, B. Raabe, R. W. Enslow, J. DeWeese, J. 
Maynard, and K. van Prooijen, “Real-time motion 
retargeting to highly varied user-created morphologies,” 
in ACM Transactions  on Graphics (TOG), 2008, vol. 
27, p. 27. 
[13] K.-S. Huang, C.-F. Chang, Y.-Y. Hsu, and S.-N. Yang, 
“Key probe: a technique for animation keyframe 
extraction,” Vis. Comput., vol. 21, no. 8–10, pp. 532–
541, 2005. 
[14] M. Wang, S. Guo, M. Liao, D. He, J.  Chang,  and  J. 
Zhang, “Action snapshot  with  single  pose  and 
viewpoint,”  Vis. Comput., vol. 35, no. 4, pp. 507–520, 
2019. 
[15] E.Stefanidi, N. Partarakis, X. Zabulis, P. Zikas, G. 
Papagiannakis, and N. Thalmann Magnenat, “TooltY: 
An approach  for  the  combination  of   motion   
capture and 3D reconstruction to present tool usage in 
3D environments,”  in Intelligent  Scene  Modelling  
and Human Computer Interaction, Springer. 
[16] C. Kirmizibayrak, J. Honorio, X.  Jiang,  R.  Mark,  and 
J. K. Hahn, “Digital Analysis and  Visualization  of 
Swimming Motion.,” Int. J. Virtual Real., vol. 10, no. 3, 
2011. 
[17] G.   Pingali,  A.  Opalach,    Y.    Jean,    and    I. 
Carlbom, “Visualization of sports  using  motion 
trajectories:  providing  insights     into     performance, 
style,   and    strategy,”    in Proceedings Visualization, 
2001. VIS’01, 2001, pp. 75–544. 
[18] J. Assa, Y. Caspi, and D. Cohen-Or,  “Action  synopsis: 
pose selection and illustration,” in ACM Transactions 
on Graphics (TOG), 2005, vol. 24, pp. 667–676. 
[19] M. G.  Choi,  K.  Yang,  T.  Igarashi,  J.  Mitani,  and  J. 
Lee, “Retrieval and visualization of  human  motion  
data via stick figures,” in Computer Graphics  Forum,  
2012, vol. 31, pp. 2057–2065. 
[20] H.-J.   Lee,   H.   J.   Shin,   and   J.-J.   Choi,   “Single 
image summarization of 3D animation using depth  
images,” Comput. Animat. Virtual Worlds, vol.  23,  no. 
3–4, pp. 417– 424, 2012. 
[21] H. Yang and K. Min, “Importance-based approach for  
rough drawings,” Vis. Comput., vol. 35, no. 4, pp. 609–
622, 2019. 
[22] “GitHub - CMU-Perceptual-Computing-Lab/openpose: 
OpenPose: Real-time multi-person keypoint  detection  
library for body, face, hands, and foot estimation.”  
[Online]. 
Available: 
 
 
https://github.com/CMU-
Perceptual-Computing- Lab/openpose. [Accessed: 13-
Jan-2020]. 
[23] E. V. Shikin and A. I. Plis, Handbook on Splines for the 
User. CRC Press, 1995. 
[24] N. Partarakis, M. Antona, E. Zidianakis, P. Koutlemanis, 
and C.Stephanidis, “Traditional Paintind Revided: The 
Ambient Intelligence Approch to Creativity”  
[25]  “GitHub   -   DevZest/WpfDocking:   A   docking   
library to integrate undo/redo-able tabbed docking, 
floating and auto hide window management into your 
application in minutes.” [Online]. 
                                     
Available: 
https://github.com/DevZest/WpfDocking. 
[Accessed: 
13-Jan- 2020]. 
[26] “The          Mingei           project.”           [Online]. 
Available: http://www.mingei-project.eu/. [Accessed: 
20-Jan-2020]
 
270
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

