Tissue Classiﬁcation from CT of Liver Volumetric Dataset
Using 3D Relational Features
Wan Nural Jawahir Hj Wan Yussof
Chair for Pattern Recognition and Image Processing
Albert-Ludwigs-Universit¨at Freiburg
Freiburg im Breisgau, Germany
yussof@informatik.uni-freiburg.de
Hans Burkhardt
Chair for Pattern Recognition and Image Processing
Albert-Ludwigs-Universit¨at Freiburg
Freiburg im Breisgau, Germany
Hans.Burkhardt@informatik.uni-freiburg.de
Abstract—This paper proposes an extension of two di-
mensional relational features into three dimensions. In two
dimensions, the relational features are extracted using a non-
linear kernel function. This function is applied to the values of
the points of two circles. To extract 3D relational features, we
represent the points on the two spheres. We aim at classifying
the tissue from Computed Tomography (CT) of liver datasets
into three classes; normal, abnormal and others(i.e., kidneys,
blood vessel, etc.). For this task, 100 known points from 6
CT datasets were used for training using the Support Vector
Machine (SVM) classiﬁer and 150 points from 10 CT datasets
were used for validation. The results presented in this paper
show that the relational features are promising.
Keywords-invariant features, relational kernels, Computed
Tomography (CT);
I. INTRODUCTION
Image data contains meaningful information that has to
be automatically extracted using computers or electronic
devices. Such image information are called image features.
Depending upon the particular task, the extracted features
capture morphological properties, color properties, or certain
textural properties of the image.
Based on the collection of texture deﬁnitions in [6],
formulated by different vision researchers, it is difﬁcult to
express the true meaning of texture. However, the utilization
of texture features is of increasing interest in many domains,
including in the medical domain. Major categories of texture
can be grouped into three subﬁelds: texture segmentation,
texture classiﬁcation and texture synthesis. Texture segmen-
tation works on partitioning the differently textured regions
in an image. Taking advantage of structural content from
a small digital sample image, one can construct a large
image. This process is called texture synthesis. In texture
classiﬁcation, the goal is to assign an unknown sample image
to one of the known texture classes. We attract the attention
of the reader to [6], as these categories have already been
discussed in detail by the authors.
This paper deals with the classiﬁcation problem and
presents a method for distinguishing normal tissue from
abnormal tissue based on three dimensional textures using
CT data recordings of liver datasets. The organization of
this paper is as follows: In Section 2, we will review the
role of texture features used in medical image processing. In
Section 3, we describe the proposed texture feature. Section
4 presents and discusses the results and ﬁnally we give the
conclusion in Section 5.
II. TEXTURE FEATURES IN MEDICAL APPLICATIONS
Texture features have been applied in many medical
image analysis and computer vision problems [1][2][11].
In general, the applications involve the automatic feature
extraction from the image. The features are then used for a
variety of medical tasks such as classiﬁcation, segmentation,
registration and medical images indexing and retrieval.
Kovalev et al. [11] proposed an extended co-occurrence
descriptor for three dimensional texture analysis of MRI
datasets. The method is based on extended multi-sort co-
occurrence matrices that combine intensity, gradient and
anisotropy image features. They have demonstrated that
their method is an efﬁcient tool in various MRI image
analysis tasks such as classiﬁcation of brain datasets and
segmentation of diffuse brain lesions.
For the registration task, Jarc et al. [1] extracted Laws
texture coefﬁcients and used them for computing registration
criterion functions. The purpose of their work is to ana-
lyze the importance of texture information for registration
of a digitaly reconstructed radiograph (DRR) and medical
electronic portal image (EPI). They computed a registration
criterion function directly from the intensity values, i.e.,
gray-values, for comparison to their proposed feature based
approach. Three observations have been done; the accuracy
of registration, the distinctiveness of local extrema and the
distinctiveness of a global extrema of the criterion functions.
These parameters are essential to achieve a correct image
alignment. From the given image modalities, a more robust
and correct registration can be expected using texture based
instead of using intensity based criterion functions.
Manduca et al. [2] used texture features for the predic-
tion of breast cancer from mammographic images. They
extracted ﬁve textural features. For each image, a total of
1,050 Markovian texture features, 112 run length features,
48
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

250 Laws features and 30 Wavelet features were extracted
from the central area of the breast where the thickness is
nearly uniform and is referred as constant thickness region
(CTR). A total of 41 Fourier features were calculated within
the CTR-box. The CTR-box was obtained from the largest
rectangular box that can be inscribed within the breast
region. For the evaluation they used 381 datasets for training
and 387 datasets for validation. The results consistently show
that texture features at low spatial frequencies provided the
strongest predictors of future breast cancer risk.
Tesar et al. [4] proposed a texture-based segmentation of
organs for disease diagnostic. They proposed the extension
of 2D Haralick texture features to 3D. For this work, they
calculated a separate co-occurrence matrix for each voxel
in the 3D image. The co-occurrence matrix is calculated
from all voxels in a small rectangular window around the
voxel. This makes it possible to segment given a 3D image
as opposed to calculating the feature for the pre-segmented
regions of an image. Consequently, such features can be used
to search for very small regions with different texture proper-
ties (like tumors). A set of abdomen CT images was used for
evaluation of the proposed approach. They used a Gaussian
Mixture Model for the segmentation and for learning the
parameters of the mixture models from the training dataset,
an expectation-maximization (EM) approach was used.
In medical retrieval applications, Glatard et al. [9] used
a bank of Gabor ﬁlters, one of the most popular texture
features for medical image indexing and retrieval with a
database of a cardiac magnetic resonance images. Each ﬁlter
was tuned to a speciﬁc orientation and spatial frequency.
Gabor ﬁlters with an angular spacing of 30◦ (corresponding
to the orientations 0◦, 30◦, 60◦, 90◦, 120◦ and 150◦) and a
frequency spacing of one octave (corresponding to the
frequencies
√
2, 2
√
2, ..., N
4
√
2 cycles per image, N being
the size of the image) were calculated. 42 Gabor ﬁlters were
used for image indexing and 16 Gabor ﬁlters were used for
segmentation assisted retrieval.
We discovered from the literature review that texture
features are an important property in many medical applica-
tions. However, among texture features, relational features
have not received much attention in this ﬁeld. In this paper,
our intention is to expose the potential of the proposed
texture features to the researchers who are enthusiastic in
medical image analysis.
III. RELATIONAL INVARIANT FEATURES
In this work, we focus on the classiﬁcation of liver
tissue from datasets based on extended relational features.
These features have been successfully used in the SIMBA
(Search IMages By Appearance) system1. The advantage of
using this method is that it is insensitive to image noise
and also invariant to a group of image transformations
1http://simba.informatik.uni-freiburg.de/
Figure 1.
CT scans showing the low-density metastases in the upper
right lobe of the liver. These tissues are labeled as 2 (abnormal). The
blood vessels which are labeled as 3 (others) also appear as if they have
been enhanced with contrast media and are brighter than 1 (normal) liver
tissues. Image on the right is the result after noise ﬁltering using anisotropic
diffusion from raw data on the left.
(e.g.,translation and rotation). Using a slice-by-slice two-
dimensional approach is still possible but it suffers from the
drawback of some important information loss. To beneﬁt
from all information in 3D space, we extend the relational
features to three dimensions. Note that, since CT data nor-
mally comes with low resolution and high noise, our method
does not directly work on a raw volume data. We ﬁltered
the data using 3D anisotropic diffusion (see Figure 1).
A. Invariant Features
In two dimensions, a gray value image, X are represented
as X(x, y), (0 ≤ x < M, 0 ≤ y < N) with X(x, y) be the
gray-value at pixel coordinate (x, y) and MxN is the image
domain. Let G be the transformation group of translation
and rotation with elements g ∈ G acting on the images.
The transformed images are gX. An invariant feature must
satisfy F(gX) = F(X), ∀g ∈ G. Integrating f(gX) over the
transformation group G constructs such invariant features as
follows:
I(X) = 1
|G|
Z
G
f(gX)dg
(1)
Eq. 1 becomes
IF(X) =
1
2πMN
Z M
x=0
Z N
y=0
Z 2π
θ=0
f(g(x, y, θ)X)dθdxdy
(2)
when applying the integration over all possible rotations and
translations (Haar integral over the Euclidean motion). In
discrete form the Eq. 2 is deﬁned as:
IF(X) ≈
1
qMN
M−1
X
x=0
N−1
X
y=0
q−1
X
j=0
f(g(x, y, θ = j 2π
q )X) (3)
where IF is approximated by choosing x and y to be integers
and by varying θ in a discrete manner producing q samples.
Interpolation is used to solve the problem of points that do
not lie on the image grid.
49
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

As can be observed, Eq. 3 can be computed locally by
applying the kernel function f, on the neighborhood of each
pixel in the image for all possible values of θ.
IFlocal(x, y) = 1
q
q−1
X
j=0
(f(g(x, y, θ = j 2π
q )X)).
(4)
The summation over the angle θ can be replaced with
histogramming [8].
IFlocal(x, y) = hist(f(g(x, y, θ = j 2π
q )X)), j = 0, ..., q − 1.
(5)
In this way, local information can be preserved and thus
increasing the discrimination capabilities of features.
B. Relational Features
The relational features are calculated similar to Local
Binary Pattern (LBP) texture features [10]. LBP thresholds
the neighborhood with the gray value of its center pixel and
represents the result as a binary pattern (0 and 1). Applying
this to all pixels in a circular neighborhood of the center
pixel, the binary pattern is then transformed into a unique
number as follows:
LBP =
n−1
X
i=0
s(vi − vc)2i, where
(6)
s(x) =
(
1
x ≥ 0,
0
x < 0
(7)
where vi and vc are the gray values at a neighboring pixel
and at the center pixel, respectively. The number of the pixels
in the circular neighborhood is denoted by n. The drawback
of LBP is that the discontinuity of the LBP operator (the s
function), since it maps to 0 or 1 which makes them sensitive
to noise. A small disturbance in the image may cause a big
deviation of the feature.
Using the following form
f(X) = rel(X(x1, y1) − X(x2, y2))
(8)
we construct an invariant feature by applying Eq. 8 onto
Eq. 4. Schael [5] has introduced a ramp function that extends
the step function in Eq. 7 giving values in the range of [0, 1]
as follows:
rel(η) =





1
if η < −ε
ε−η
2ε
if −ε ≤ η ≤ ε
0
if ε < η
(9)
where ε is a threshold parameter. Using a ramp function, it
is now more robust to image noise. Note that, if ε is set to
zero, then the rel function will reduce to the simple LBP
operator s.
(𝑥₁,𝑦₁)
(𝑥₂,𝑦₂)
(𝑥,𝑦)
𝜙
Figure 2.
Calculation of a set of relational features in two dimensions.
A feature is formed by applying the relational function to the gray-value
difference of the pixels lying on the speciﬁc distance and phase to the
reference point (i.e. center of the circles)
As opposed to LBP, 2D relational features use two circular
sets. Let (x, y) be the coordinates of central pixel in two-
dimension, taking into consideration a phase shift, φ, the
coordinates (x1, y1) and (x2, y2) in Eq. 8 are given by:
(x1, y1) = (x + r1 cos(θ), y + r1 sin(θ))
(10)
(x2, y2) = (x + r2 cos(θ + φ), y + r2 sin(θ + φ))
(11)
where r1 and r2 are the radii of the ﬁrst and second
circle, respectively. Local information at different scales and
orientations can be captured with different combinations of
r1, r2 and φ. The calculation of two dimensional relational
features is illustrated in Figure 2
C. Extension Relational Features to Three Dimension
In this paper, we present an extension of relational features
to three dimensions. The relational function in Eq. 8 for three
dimensions is very straightforward and is given by:
f(X) = rel(X(x1, y1, z1) − X(x2, y2, z2))
(12)
We have seen that LBP and 2D relational features use
circular sets to represent the neighborhood of a central pixel.
To extent relational features to three dimensions, a logical
way is to represent neighbors in unit sphere. Thus for a
central voxel with the coordinates (x, y, z), the coordinates
of (x1, y1, z1) and (x2, y2, z2) are given by:
(x1, y1, z1) = (x + r1 cos(θ) sin(ψ),
y + r1 sin(θ) sin(ψ), z + r1 cos(ψ)) (13)
(x2, y2, z2) = (x + r2 cos(θ + φ1) sin(ψ + φ2),
y + r2 sin(θ + φ1) sin(ψ + φ2), z + r2 cos(ψ + φ2)) (14)
where φ1 and φ2 denote the phase shifts, between the
corresponding points (x1, y1, z1) and (x2, y2, z2).
Given
n
set
of
parameters
with
i
=
0, . . . , n,
we
deﬁne
our
three
dimensional
relational
features,
R(x, y, z, r1, r2, φ1, φ2, q),
calculated
on
a
local
point
(x, y, z) as follows:
RFi = [R(x, y, z, r1, r2, φ1, φ2, q)]i
RFi = rel(X(x1, y1, z1) − X(x2, y2, z2))
(15)
50
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

Three dimensional invariant features in Eq. 4 becomes
IFlocal(i)|(x,y,z) = hist(fi(x, y, z, θ, ψ)X)),
(16)
where θ is an azimuthal coordinate running from 0 to 2π
(longitude) and ψ is a polar coordinate running from 0 to π
(colatitude). We end up with one dimensional feature vector
with t bins.
IV. RESULTS
For the evaluation of our three dimensional relational
features, 100 training points from 6 CT datasets and 150
test points from 10 CT datasets were used for the classiﬁ-
cation task. These points are labeled as 1 (normal tissue),
2 (abnormal tissue) and 3 (others, i.e., blood vessels, kid-
neys that could not be removed during segmentation). The
Support Vector Machine (SVM) classiﬁer2 was used for the
classiﬁcation task. Three tests were conducted in this work.
As SVM kernel, we used the radial basis function (RBF)
kernel for the classiﬁcation task with a pair of parameters
(C, γ). Here, C denotes the cost value that penalizes misclas-
siﬁcations and γ is the width-parameter in the RBF kernel.
We used four sets of parameters, (r1 = 0, r2 = 5, φ1 =
0), (r1 = 2, r2 = 3, φ1 = π/4), (r1 = 3, r2 = 6, φ1 = π/2)
and (r1 = 4, r2 = 8, φ1 = π), each with q = 20. For all
tests, we set φ2 = 0. This means only on longitude direction
a phase shift was applied. The rel threshold, ε = 0.196
was used. At each point, we obtained 20 ∗ 4 = 80 features.
Histogram relational features (hRF) were divided into 20
bins. The length of feature vector was reduced to 20.
For the ﬁrst test, we set C = 1000 and γ = 0.02. hRF
attains an accuracy of 86.67% for this test. Table I shows
the confusion matrix with the true positive (TP) and false
positive (FP) of the ﬁrst test. 71 out of 79 are correctly
classiﬁed as class 1 given 89.87% of TP and 7 out of
71 tissues from class 2 and 3 are incorrectly identiﬁed as
class 1 given 9.859% of FP. The TP and FP for class 2
are 93.1% and 7.438%, respectively. However, the TP for
class 3 is quite low (76.19%) since this class consists of
several different tissues, which might have different tissue
structures. Only 3.704% of tissues from class 1 and 2 are
wrongly classiﬁed as class 3.
Table I
CONFUSION MATRIX AND TRUE POSITIVE (TP) AND FALSE POSITIVE
(FP) FOR THE THREE CATEGORY CLASSIFICATION TASK
%
1
2
3
TP
FP
1
71
6
2
71/79 (89.87%)
7/71 (9.859%)
2
0
27
2
27/29 (93.1%)
9/121 (7.438%)
3
7
3
32
32/42 (69.05%)
4/108 (3.704%)
total :
130/150 (86.67%)
20/150 (13.33%)
In the second and third tests, we compared the result of
hRF with relational features that were computed by averag-
ing. We refer to the features as aRF. We also compared the
2http://lmb/lmbsoft/libsvmtl/svmtl.en.html
Figure 3.
Comparison of the proposed texture features with 3D GLCM
and 3D Gabor wavelets for the accuracy of different tissue classes.
results with the gray-level co-occurrence matrix (GLCM)
texture features [7] and Gabor wavelets [3]. We used the
same SVM kernel as the ﬁrst test for the classiﬁcation. The
GLCMs and Gabor wavelets used in this study were also
extracted in 3D space.
Eight Haralick statistic measurements to describe the 3D
GLCM were calculated using a window size of 5x5x5
in 13 directions((0◦, 45◦), (0◦, 90◦), (0◦, 135◦), (45◦, 45◦),
(45◦, 90◦), (45◦, 135◦), (90◦, 45◦), (90◦, 90◦),(90◦, 135◦),
(135◦, 45◦), (135◦, 90◦), (135◦, 135◦) and (−, 0◦)) for con-
trast, homogeneity, angular second moment, entropy, maxi-
mum probability, energy and correlation measurements. 3D
Gabor wavelets used in this study follow the formula given
in [3]. A set of Gabor wavelets of different frequencies fi
and orientations (θj, φk) was calculated with the following
representation:
{ψfi,θj,φk(x,y,z), fi = 0.5/(
√
2)i, θj = jπ/J, φk = kπ/K}
(17)
A window size of 25x25x25 was used to produce Gabor
ﬁlters and convolved with the same size of sub-image with
point being evaluated was located at the center of the sub-
image. The mean of convolution result was used to represent
Gabor feature. Since a set of Gabor wavelets {ψfi,θj,φk}
was used, IxJxK length of feature vector were obtained
with i = 0, ..., I − 1, j = 0, ..., J − 1 and k = 0, ..., K − 1.
We set I = J = K = 3. This produced a total of 27 Gabor
features (GF).
We performed the second test for comparison of classiﬁca-
tion accuracy for every class. On average, hRF outperforms
GLCM by 26.67% and GF by 24%. aRF attains an accuracy
of 80.67% in which hRF is better than aRF by 10%. See
the graph in Figure 3 for comparison. It can be seen from
the graph, the accuracy of GLCM and GF for class 1 and
class 3 are worse than relational features. For class 2, aRF
is the worst among all. However, hRF obtains the highest
accuracy for all classes.
In the third test, we have computed the accuracy of
51
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

Figure 4.
The accuracy of 3D relational features, 3D GLCM and 3D
Gabor wavelets using different γ with C = 1000.
Figure 5.
The accuracy of 3D relational features, 3D GLCM and 3D
Gabor wavelets using different cost values, C with γ = 0.01.
all features using different pair of parameters (C, γ) for
the RBF kernel. The result presented in Figure 4 was
obtained by setting γ
=
{0.01, 0.02, 0.05, 0.1, 0.2, 0.5}
and C = 1000. In Figure 5, γ was set with 0.01 and
C = {200, 500, 1000, 2000, 5000, 10000, 20000}. The re-
sults from both ﬁgures show that the relational features
outperform GLCM and GF on the task. As predicted, hRF
shows higher classiﬁcation accuracy than aRF for all pair
of parameters (C, γ) used in this test.
V. CONCLUSION AND FUTURE WORK
In this paper, we proposed an extension of relational fea-
tures to three dimensions. The purpose of this extension is to
extract texture features from CT volumetric dataset to beneﬁt
from the full 3D information. The results from our study
showed that the relational features achieved higher accuracy
than the GLCMs and Gabor wavelets texture features in clas-
sifying different types of liver tissue from CT datasets. This
indicated that, 3D relational features, especially hRF are
promising for many medical applications that exploit texture
features. For future work, we will consider nearby points to
progress in the accuracy of liver tissue classiﬁcation.
ACKNOWLEDGMENT
We would like to thank all the ﬁve anonymous reviewers
for their helpful comments. We also immensely grateful
to our colleague, Thorsten Schmidt for his comments and
suggestions on an earlier version of this paper. The work has
been partially sponsored by KPT/UMT under SLAI scheme.
REFERENCES
[1] A. Jarc, P. Rogelj and S. Kovacic, Texture Feature Based Image
Registration Biomedical Imaging: From Nano to Macro, 2007.
ISBI 2007. 4th IEEE International Symposium on , vol., no.,
pp.17-20, 12-15 April 2007.
[2] A. Manduc, M. J. Carston, J. J. Heine, C. G. Scott,
V. S. Pankratz, K. R. Brandt, T. A. Sellers, C. M. Vachon and
J. R. Cerhan, Texture Features from Mammographic Images
and Risk of Breast Cancer, Cancer Epidemiol Biomarkers Prev
2009, vol. 18(3), pp. 837–845, 2009.
[3] L. Shen and L. Bai, 3D Gabor wavelets for evaluating SPM
normalization algorithm, Medical Image Analysis, vol. 12,
pp. 375–383, 2008.
[4] L. Tesar, D. Smutek, A. Shimizu and H. Kobatake, 3D Exten-
sion of Haralick Texture Features for Medical Image Analysis
[5] M. Schael, Invariant grey scale features for texture analysis
based on group averaging with relational kernel function.,
Internal Report 01/01, University of Freiburg, 2001.
[6] M. Tuceryan and A. K. Jain, Texture Analysis, The Handbook
of Pattern Recognition and Computer Vision (2nd. Eds.) by
C. H. Chen, L. F. Pau and P. S. P. Wang (eds.), pp. 207–248,
World Scientiﬁc Publishing Co., 1998.
[7] R. M. Haralick, K. Shanmugam and I. Dinstein, Textural Fea-
tures for Image Classiﬁcation, IEEE Transactions on Systems,
Man, and Cybernatics SMC(6), pp. 610–621, 1973.
[8] S. Siggelkow, M. Schael and H. Burkhardt, SIMBA - Search
IMages By Appearance, DAGM, LNCS 2191, pp. 9–16, 2001.
[9] T. Glatard, J. Montagnat and I. E. Magnin, Texture based
medical image indexing and retrieval: application to cardiac
imaging, Proceedings of the 6th ACM SIGMM international
workshop on Multimedia information retrieval, pp. 135–142,
2004.
[10] T. Ojala, M. Pietik¨ainen and T. M¨aenp¨a¨a, Gray scale and rota-
tion invariant texture classiﬁcation with local binary patterns.
In Proceedings of the 6th European Conference on Computer
Vision, pp. 404–420, 2000.
[11] V. A. Kovalev, F. Kruggel, H. J. Gertz and D. Y. von Cramon,
Three-dimensional Texture Analysis of MRI Brain Datasets,
IEEE Transactions on Medical Imaging, vol. 20(5), pp. 424–
433, 2001.
52
PATTERNS 2010 : The Second International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-111-3

