Design and Development of Interactive Mirror for Aware Home 
Chidambaram Sethukkarasi, Vijayadharan SuseelaKumari HariKrishnan, Raja Pitchiah  
National Ubiquitous Computing Research Centre 
Centre for Development of Advanced Computing  
Chennai, India 
{ctsethu,  harikrishnans, rpitchiah}@cdac.in  
 
 
Abstract— This paper describes the design, development and 
deployment of an “Interactive Mirror”, an artifact augmented 
with intelligence to demonstrate personalized services for 
enhanced comfort. The mirror aims at recognizing the user 
based 
on 
image 
processing 
techniques 
and 
provides 
personalized services like emotion recognition, health progress 
representation, event reminder and mirror usage time. The 
prototype of the interactive mirror was deployed in laboratory 
environment and the user’s feedbacks were obtained. The face 
recognition and emotion recognition algorithms were tested 
and the results are discussed.  
 
Keywords - Ubiquitous Computing; Interactive mirror; Face 
Recognition; Emotion Recognition; Smart Artifact. 
I. 
 INTRODUCTION  
Ubiquitous 
computing, 
with 
wirelessly 
connected 
embedded devices that are being used in day–to–day 
activities, is changing and improving our quality of life. 
Based on ubiquitous computing and communication 
technologies, many devices/products are emerging to make 
users comfortable. We daily look at the mirror and interact 
with it psychologically to find out how we look and how our 
attire is. Generally everyone spends some time to select a 
suitable outfit for the day during mirror usage.  The 
interactive mirror is a development effort to augment the 
mirror with embedded intelligence for offering enhanced 
features such as face recognition, facial expression analysis, 
health progress reporting and selection of appropriate dress 
for the day. The ‘Interactive mirror’ is formulated as follows 
 
Interactive Mirror = Mirror + Sensors + Database + 
Intelligence + Display 
 
The Interactive Mirror comprises of a dielectric coated 
mirror mounted over a LCD Display, a camera for capturing 
the user’s image, load sensors for measuring user’s weight, 
Radio-frequency identification (RFID) reader and RFID tags 
for identifying the garment worn by the user. The mirror gets 
the inputs from all these sensors, processes it and provides 
output by displaying text and graphics as well as audio 
output. 
Our contributions are as follows: 1) Conceptualization, 
design and development of interactive mirror that provides 
personalized services based on biometric identification of the 
user. 2) Facial feature extraction for emotion recognition 3) 
Training face recognition and emotion recognition algorithm 
with the database images created by us 4) Test results of face 
recognition and emotion recognition algorithm in the 
deployed environment.  
The paper is organized as follows. Section 2 comprises of 
our comments about the related work, Section 3 details about 
the engineering of interactive mirror, Section 4 gives an 
overview of our proposed system and different modules 
involved in it, Section 5 presents the results obtained, and 
finally, Section 6 concludes the work. 
II. 
RELATED WORK 
Few investigations have been done in this area. By 
keeping the mirror usage in mind, certain efforts have been 
done to add the technologies in the mirror to do multiple 
tasks parallel at a time. How it will be if the mirror is talking 
to us. This concept has been realized in [2] using speech 
processing techniques. The mirror interacts with the user 
through verbal commands, functions like a good friend, 
listens to user’s question and responds them, provides 
relaxation and consolation.  It is essential to do the tasks 
parallel in our daily activities to save our time. Philips 
laboratory has come out with such a mirror [3] to assist the 
users in saving their time. One can watch news reports, TV 
channels, weather reports, etc., while brushing in bathroom. 
The bathroom lighting comprises of 50 light sources of 
different kind. Various light sources have been used which 
generates light of different color and temperature.  
A mirror [4] makes use of behavioral data in order to 
provide its user with continuous visual feedback on their 
behavior in a natural manner. It tells you how you will look 
after 5 years.  
An augmented setup [5] has been demonstrated targeting 
card game application. The system captures the image of the 
surrounding, detects multiple objects on it and provides 
augmented display on a LCD screen mounted on a table. 
A digital mirror that detects tracks and model the human 
face and expression on a computer screen is demonstrated in 
[6].  The system uses robust multiple face detector and 
tracker based on active infrared (IR) illumination, and 
developed a physics based face model to generate realistic 
graphics output, and tested the integration of both modules 
using an eye-contact application, that randomly changes 
facial expressions.       
Our system assists in leading a healthier life by 
measuring and providing health progress of basic health 
parameters, in addition to face recognition and facial 
expression analysis. The system also attempts to keep track 
1
Copyright (c) The Government of India, 2012. Used by permission to IARIA.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

of the garments used over a period of time and suggest an 
appropriate dress for a particular day. Even though the 
system is designed for user without any disabilities, visually 
impaired people might find it more useful. 
III. 
ENGINEERING OF INTERACTIVE MIRROR 
 
 
Figure 1.  Engineering of Interactive Mirror 
The dielectric coated mirror of thickness ¼” is mounted 
over the LCD display, so that the entire panel acts both as a 
mirror and display. When the display is ON, the dielectric 
glass will be transparent and when display is OFF, it will act 
as a normal mirror. A Creative web camera having VGA 
resolution and frame rate of 30fps is mounted on the top of 
the panel for capturing the images. The camera will be on 
sleep mode and triggered by the application for capturing 
mode when the user starts using the mirror. A weight 
measuring platform is formed with four load sensors from 
Loadstar. Each load sensor can carry weight up to 50lb, 
resulting in the maximum weight of the platform is 200lb. 
The weighing platform and the camera are mounted at 
suitable distance so that the entire face image should be 
covered by the camera Field of View (FOV). The distance 
between the weighing platform and the camera is kept 
constant to avoid scaling changes in the face image. The 
entire application has to be ported on atom processor which 
is placed inside the top frame of the panel. The omni 
directional, circular polarized Poynting UHF RFID antenna 
having 7dbi gain is positioned under the panel to detect the 
RFID tag attached in the garments. The antenna is connected 
to the SIRIT Infinity 510 RFID reader that is placed inside 
the wooden frame. The entire setup is shown in Figure 1. 
IV. 
SYSTEM OVERVIEW 
The system encompasses of several modules to 
implement the services shown in Figure 2. The modules are 
described as follows. 
A. Face Recognition 
The steps involved in face recognition module are shown in 
Figure 3. The image captured by the camera needs to be 
preprocessed in order to enhance the contrast. Generally, an 
image is a collection of pixels. The pixel value refers to the 
intensity. To improve the image contrast, the intensity value 
is analyzed from top left pixel to bottom right pixel. The 
intensity value which occurred frequently is distributed over 
the entire image using histogram equalization technique. 
 
 
Figure 2.  Services of Interactive Mirror. 
The first step of face recognition is face detection. The 
process of finding faces in an image is called as face 
detection. Processing an image, pixel by pixel is a time 
consuming process. The methods available for face detection 
and face recognition are surveyed and compared in [7]. The 
system is using Viola and Jones algorithm [1] for face 
detection. It processes the image based on haar-like features 
and not pixels. The image is scanned from top left to bottom 
right using a window. The scan looks for the presence of 
haar-like features. The scan window is passed through a 
chain of filters called as classifiers. The classifiers are all 
trained with both face and non face images. The window 
which passes through all the classifiers are classified as 
“face” and if a window is rejected by any of the classifiers in 
the chain is classified as “Non-Face”. 
 
 
 
 
 
Figure 3.  Steps involved in Face Recogntiion module. 
The second step is to recognize whose face is this? The 
detected face image is to be cropped and resized for further 
processing. Many algorithms exist for face recognition 
2
Copyright (c) The Government of India, 2012. Used by permission to IARIA.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

technique [13]. The face recognition technique using 
geometric approach and template matching have been 
compared in [12]. “EigenFace” Recognition method is used 
here [8][9][10] taking advantage of the actuality that the 
faces images will be frontal and upright while the user uses 
the mirror. This method works on training basis. The 
algorithm includes projecting the images over a lower 
dimensional space and finding the distances between them.  
The face images of the users are captured and stored in a 
face database after processing shown in Figure 4.  The 
algorithm needs to be trained with these standard face 
database images. 
 
 
Figure 4.  Face DataBase Images. 
As said earlier, processing an image pixel by pixel is a 
time consuming process. For example, an image of size 100 
* 100 is said to be a 10000 dimensional image. Hence, in 
order to reduce the dimension, principal component analysis 
(PCA) technique has been used. The Eigenvalues and 
Eigenvectors for all the images are calculated and projected 
over a lower dimension space.  
Then the distance between the input image and all the 
training images in the projected space needs to be calculated 
using “Euclidean Distance” method. The Euclidean distance 
between two point’s p1(x1, y1) and p2(x2, y2) is given by 
(1), 
 
)1 2
( 2
)1 2
(( 2
y
y
x
x
d
−
−
−
=
                    
(1) 
Based on the distance measured, the input image is 
recognized. If the input face is a known face, the user is 
authorized, otherwise the user is unauthorized. OpenCV, an 
image processing library developed by Intel Corporation has 
been used for image processing algorithms. 
B. Emotion Recognition 
Generally, a person’s emotional state is very important, 
since it has effect for whole day. Human Computer 
interaction by understanding the human’s emotion is a recent 
research topic.  Interactive mirror detects the user’s emotions 
based on their facial expressions while the user is interacting 
with the mirror. There exist various methods to recognize the 
user’s emotion. However, the system infers the emotions 
from the facial expressions as it get the input as face image.  
Facial expressions can be recognized from the facial features 
as per the steps shown in Figure 5. Facial features are the 
spatial position of the features and its displacements.   
 
 
Figure 5.  Steps inviolved in Emotion Recognition Module. 
The following assumptions have been made 1) Constant 
lighting and illumination 2) Front Face image 3) No scaling 
variations. 
The mouth features best describes the emotion of a 
person. Hence, mouth features like mouth width and height 
have been taken into consideration. Eyes also play a vital 
role in finding the emotion of the person. Thus eye features 
like eye width and eye height have also been extracted. The 
facial expression database images for happy, surprise and sad 
are shown in Figures 6, 7 and 8, respectively. 
 
          
 
Figure 6.  Happy Expression Database Images.        
          
.
 
Figure 7.  Surprise Expression Database Images. 
           
 
Figure 8.  Sad Expression Database Images. 
Integral Projection method is used to extract facial 
features [11][14]. This method is affected by the external 
conditions like illumination changes, and skin color.  
1) Mouth Feature Extraction: 
In order to extract the mouth features, the search region 
has to limited by segmenting the mouth part alone from the 
face image. There exist three approaches [11] to extract the 
face features (i) based on luminance, chrominance, facial 
geometry and symmetry [12] (ii) template matching [15] (iii) 
PCA. Facial geometry approach is being used in our 
application. The steps involved in extracting the mouth 
features are shown in Figure 9. As per the face geometry, the 
mouth region is segmented from the face image. The 
segmented image is preprocessed to improve its contrast and 
converted to binary image using image thersholding. The 
threshold value is selected dynamically by the algorithm as 
per the image brightness. The unwanted blobs as shown in 
Figure 10a) occurred because of illumination variation along 
the face region is eliminated using blob removal technique. 
The blob removed image is shown in Figure 10b). A 
minimum blob size and the connectivity of the pixels are 
analyzed to say whether the blob belong to mouth region or 
not. By applying the integral projection method on binary 
3
Copyright (c) The Government of India, 2012. Used by permission to IARIA.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

image, the exact position of the mouth corners and top point 
is located. 
 
 
Figure 9.   Steps involved in Mouth Feature Extraction 
Let I(x, y) be a gray value of an image.  
The Horizontal integral projection [11] of the image m x n is 
defined as  
 
∑
=
=
n
y
I x y
H y
0
( , )
( )
 
 
 
 
(2) 
The Vertical integral projection of the image is defined as  
 
∑
=
=
m
x
I x y
V x
0
( , )
( )
 
 
 
 
(3) 
 
 
 
 
 
 
Figure 10.  a) Binary Image b) Blob removed Image 
where m = no of rows and n = no of columns. 
 
Figure 11.  Mouth Feature Points. 
 
Figure 12.  Eye Feature Points 
 
The minimum value observed in the horizontal projection 
curve is the centre line of the upper and lower lip. Scan along 
the line both left and right to locate the corners. In vertical 
projection, the minimum value is the centre line of the mouth 
and scanning from top to bottom locates the top and bottom 
point of the mouth respectively. The mouth width is obtained 
from the two corner points and mouth height is obtained 
from the top point and the centre point of the mouth centre 
line shown in Figure 11.  
2) Eye Feature Extraction: 
As like mouth image segmentation, the same steps are 
followed for eye feature extraction also. The left and right 
eye region is separated from the face image as per the face 
geometry. The integral projection method is applied to 
locate the feature points as shown in Figure 12. The eye 
width and height is calculated from the featured points 
extracted.  The minimum value in both the projection curve 
denotes the iris centre point. 
3) k-NN Classifiaction Algorithm: 
There exist various classification algorithms [18] 
suitable for facial expression recognition; the system make 
use of k-NN classifier. Similar works using k-NN classifier 
are available at [17]. It is one of the simplest machine 
learning algorithms. An object is classified by the majority 
vote of its neighbors. K is a constant denotes the no of 
neighbors. The value k should not be too small and not too 
large. The different k values are used for testing the 
accuracy of our algorithm and the results were discussed in 
section IV. The classification algorithm is trained with the 
extracted mouth feature values for classifying emotions like 
happy, surprise, sad and normal. 
C. Health Progress Representation 
The basic health parameters like weight, height, BMI and 
BMR are measured and saved in the health database. The 
values measured are saved in the health database along with 
the date and time of measurement. The health database is 
secured so that only the authorized person can have access 
after proper authentication.   
The weighting platform mounted in front of the mirror 
starts giving the weight output when the user stands over it. 
The height information is obtained from the database 
currently; later the system will be integrated with the 
ultrasonic sensor to obtain height information of the user. 
Based on the weight and height information, the parameters 
like Body Mass Index (BMI) and Basal Metabolic Rate 
(BMR) have been calculated using the following formulas 
shown in equation 4 and 5/6 respectively. 
) 2
) /(
(
Height
Weight
BMI
=
 
 
 
      (4) 
Weight in Kilograms and Height in meter. 
For Women, 
day
xAge Kcal
xHeight
xWeight
BMR
/
)
7.4
) (
7.4
) (
655 ( .4 35
−
+
+
=
(5) 
For Men, 
day
xAge Kcal
xHeight
xWeight
BMR
/
)
8.6
) (
) (12 7.
66 ( .6 23
−
+
+
=
     (6) 
Weight in pounds, height in inches and age in years. 
 
 
Figure 13.  BMR Progress Representation. 
BMI determines a person’s weight according to his 
height and BMR is the measure of number of calories 
burned by the body when he/she is at rest. This helps in 
determining how much calories he need to intake inorder to   
maintain an energy balance and balanced diet condition. 
   a)                                            b) 
4
Copyright (c) The Government of India, 2012. Used by permission to IARIA.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

Both of these parameters are important to measure and 
monitor to ensure that the person is healthy. The measured 
BMI value is analyzed for the conditions such as normal, 
overweight and underweight. 
Drastic weight change over a short time period is the 
main symptom of identifying certain major diseases in 
human body. Hence the parameters measured have to be 
analyzed to monitor the change over a certain time period. 
Therefore the measured values are retrieved from the 
database and represented graphically to the user inorder to 
analyze any drastic change has occurred or not. For example, 
the BMR progress is shown in Figure 13. Similarly, the 
progress in weight and BMI are also analyzed and displayed 
to the user. The weight measurement accuracy depends on 
the accuracy of load sensor.  
D. Event Reminder 
In this busy world, it’s normal to forget certain things 
like bill payments, birthdays, and important dates etc., hence, 
there is a need to have reminders to remind all those things. 
Interactive mirror will do for us. It reminds the important 
dates in the calendars, birthday, bill payments, tour date, etc.  
The user has to enter the details which they want to be 
reminded using a GUI. When the user is using the mirror, the 
system will check for reminders against him/her at that 
particular time. If so, it will remind and also send a message 
to a hand held device of the user 
E. Garment  Description and Suggesstion 
This feature is proposed to be implemented. The system 
makes use of RFID technology to identify the garment 
worn. The RFID tags are attached to the garments. The tag 
is flexible, water and heat resistant. 
When a tag comes into the antenna coverage area, the 
reader detects the tag. Once the tag id detected by the 
reader, the details like when it was purchased, where 
purchased, price, color, material, how many times worn etc 
are retrieved from the database and displayed to the user. 
The suitable dress for the particular day is suggested by the 
mirror according to the presence of events like marriage 
function, birthday party, etc.  
In textile application, the mirror can suggest a suitable 
dress color based on the skin tone of the user. The first step 
is to segment the skin color pixels from the captured face 
image. Next, the skin color pixels are analyzed with a certain 
predefined values to find the skin tone of the user. 
RGB color space is used for skin color segmentation. The 
face image captured by the camera is scanned pixel by pixel 
from top left to bottom right. The pixels values are analyzed 
to classify whether the pixel belongs to skin color or not.   
The following condition is used for classification. 
A RGB pixel is classified as a skin color pixel, if the 
following condition is satisfied, the pixel is said to be skin 
color pixel.  
{
}
min
,
,
20 & &(max
40 & &
95 & &
−
>
>
>
R G B
B
G
R
 
{
}
B
R
G
R
R G
R G B
>
>
>
−
>
& &
15 & &
15 & &
)
, ,
 
Else the pixel doesn’t belong to skin color. 
Once the skin color regions are segmented, the values are 
analyzed and based on the values and the dress color 
availability with the particular user, a suitable dress is 
suggested for the particular day.  
F. Mirror Usage Time 
Interactive mirror sense the amount of time an individual 
uses the system and signals them if they are using for a long 
time. Based on the face recognition, the user’s profile is 
unlocked and their school/office is checked. It tells how 
much time is remaining for them to get ready. If they have 
enough time to get ready then make up tips will be provided.  
The weight measuring platform is used to find out the 
mirror usage time. The weight over the platform is 
monitored continuously. When anyone starts using the 
mirror, the weight will go up. That gives the start time. When 
the user leaves the load sensor platform, the weight will 
gradually decrease and then goes to zero and that time gives 
the end time. The time difference gives the mirror usage 
time.  
 
Figure 14.  Deployment of Interactive Mirror in Ubicomp Laboratory.  
The demo illustration of all these features in the 
interactive mirror is represented in Figure 14. 
V. 
RESULTS 
A. Face Recognition Algorithm Accuracy in Deployed 
Environment 
The face detection algorithm had been tested with CMU 
face database. The false detection percentage and missing 
face rate were found to be 7.56 and 9.18, respectively. For 
example, the background partition glass image (non-facial 
image) shown in Figure 16 was detected as face image by the 
face detection algorithm. 
Assume that the face images were frontal face images 
with no scaling variations taken under constant illumination.           
The users were asked to look at the camera and no other 
restrictions for them. There was no special experimental 
setup for capturing face image like dedicated light settings, 
user positioning with respect to camera, user’s viewing angle 
on a particular object, etc. This created a real life testing 
environment. Therefore our database images are not strictly 
front face images. The face images may be tilted, pose 
varied, with/without expressions, etc., but not with much  
5
Copyright (c) The Government of India, 2012. Used by permission to IARIA.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

   a)                                                                                                                                           b) 
 
Figure 15.  Training time and Recognition time versus number of training images for different Eigenvalues a)Training time b)Recognition time.
variation. The training set consists of 42 images of 9 
different subjects. The test set consists of known faces, 
unknown faces and non face images (false positives). 
Mahalanobis distance [19] method is better to use than the 
euclidean distance method. Inorder to recognize the 
unknown images and non face images, a proper threshold 
values have to be set for the distance calculation. These 
threshold values have to be calculated by experimentation for 
the chosen training set. Two different cases were considered 
for recognition. The one was to recognize the test image 
based on the first match alone and other based on the 
majority of first 3 matches. The accuracy of the algorithm for 
these two cases is listed in Table I. The role of number of 
eigenvalues (E) used in determining the algorithm accuracy 
was tested. It is noted that the accuracy decreases with 
number of Eigenvalues. 
Among the two cases, the accuracy was found to be 
better in the case of considering the first match alone for 
recognition.  
TABLE I.  
ALGORITHM  ACCURACY FOR DIFFERENT NUMBER OF 
EIGENVALUES 
Case 
E = 41 
E = 40 
E = 39 
E = 38 
E = 37 
First Match alone 
91 
77 
72 
71 
68 
Majority of first  
three matches 
53 
39 
27 
36 
23 
 
                                  
                                      
 
Figure 16.  False Positive Image 
The false positive result shown in Figure 16 had been 
eliminated out in face recognition algorithm as “Non face” 
image by setting up the proper threshold value.  
B. Face Recogntiion Algorithm Processing Time 
The training time (Tt) and recognition time (Tr) of the 
algorithm were obtained for different number of training 
images (T) and E. The results are represented in Figure 15a) 
and b). It was observed that both training time and 
recognition time increase with respect to number of training 
images. More number of training images leads to larger face 
space which in turn increases the training and recognition 
time. However the training time doesn’t affect the 
application much since the training has to be done only once 
at the initial stage. Later recognition process alone will run 
repeatedly.    
The role of number of eigenvalues used in determining 
the algorithm processing time was also examined and 
presented. It was observed that the fewer number of 
Eigenface reduces the training and recognition time. 
For achieving better accuracy, we were using maximum 
number of Eigenvalues.   
C. Emotion Recognition Algorithm Accuracy 
The Emotion recognition algorithm had been tested with our 
database images. The algorithm was trained with the mouth 
features like mouth width, mouth height, and left mouth 
corner to top point distance and right mouth corner to top 
point distance. The algorithm was tested with Yale facial 
expression database for different number of nearest 
neighbors (K). The results are presented in Table II. The 
algorithm was capable enough to recognize the happy 
expression when compared to other expressions. The 
extracted eye features has to be also included in the 
classification algorithm to improve the algorithm accuracy. 
The algorithm has to be tested in the deployed environment. 
TABLE II.  
EMOTION RECOGNITION ALGORITHM  ACCURACY  
K 
Happy 
Surprise 
Normal 
Sad 
3 
93.33 
53.33 
66.66 
33.33 
6 
100 
60 
46.6 
26.66 
Tt vs T
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0
10
20
30
40
50
60
T
Tt in Sec
E = T-1
E = T-2
E = T-3
E = T-4
Tr vs T
0
0.2
0.4
0.6
0.8
1
0
10
20
30
40
50
60
T
Tr in Sec
E = T-1
E = T-2
E = T-3
E = T-4
6
Copyright (c) The Government of India, 2012. Used by permission to IARIA.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

TABLE III.  
USER’S FEEDBACK ON INTERACTIVE MIRROR SERVICES
D. Weight Measurement Accuracy 
The weight measurement accuracy depends on the 
Weight vs Error Percentage
0
10
20
30
40
50
60
70
80
0.01
0.1
1
10
100
Weight (Kg)
Error Percentage (%)
 
Figure 17.  Weight versus error percentage. 
 
 
load sensor accuracy. The load sensor accuracy was tested 
with standard weights. The percentage error for various 
loads on a 50lb load sensor is shown in Figure 17. 
The test results conclude the following 1) The error 
percentage is very high at 10grams, gets decreased up to 
100g and finally very low and got stabilized after 1 Kg. 2) 
The load sensors should be loaded with maximum capacity 
to reduce the error percentage.  
The rest of the features like mirror usage time and 
event reminder were found to be useful.  
E. User Evaluation of the System 
The system is deployed in our UBICOMP laboratory. 
The feedback has been obtained from the users (age group 
24 to 32) for the services such as face recognition, emotion 
recognition, health progress representation, event reminder 
Face Recognition 
Emotion Recognition 
Health Progress 
Representation 
Event Reminder 
Mirror Usage Time 
 
Rating 
Comments 
Rating 
Comments 
Rating 
Comments 
Rating 
Comments 
Rating 
Comments 
User 
1 
4 
No comments 
3 
Should be 
Improved 
4 
No 
comments 
5 
No comments 
4 
 
User 
2 
3 
Sometimes the 
algorithm has 
reported as 
unknwon face, 
eventhough the 
information is 
already in 
database. 
4 
More 
emotions can 
be added to 
the database 
4 
No 
comments 
4 
How to input 
events to the 
system? Can we 
have some 
interaction where 
the suer can give 
inputs as well? 
4 
No 
comments 
User 
3 
3 
Sometimes good 
 
3 
Sometimes 
good 
 
4 
Good 
1 
Am not using yet 
 
5 
Good 
User 
4 
3 
Almost 60% it is 
Giving correct 
result 
3 
Same 
comments 
here 
 also 
1 
Not at all 
watching, 
you  
may make it 
colorful   
 and 
attractive 
1 
Didn’t use till 
now,  
so don’t know its  
capability 
2 
Not very 
sure 
how much 
it is  
getting 
reduced 
User 
5 
3 
Good only the  
environment is 
similar 
to training. 
Sometimes 
recognizing 
wrongly 
 
3 
Sometimes 
showing 
 wrongly 
4 
Good 
1 
I haven't used it 
yet 
 
5 
Good 
User 
6 
4 
Mostly 
recognizing  
correctly. Problem  
arises only when 
lighting varies 
 
2 
Difficult part 
 
5 
Good 
 
NA 
5 
Good 
User 
7  
3 
Still need complex 
algorithm of two 
or 
more algorithms 
running parallel 
for recognition 
 
4 
Eye and nose 
can also be  
considered in 
addition 
to lips for 
mood  
recognition 
2 
Instead of 
plain graph, 
info-
graphics 
can be 
 used 
 
NA 
3 
No 
comments 
7
Copyright (c) The Government of India, 2012. Used by permission to IARIA.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

and mirror usage time and listed in Table III. The rating 
has been done from 5 to 1 based on comfort/convenience 
level achieved in each service where 5 is the highest rate 
represents more comfortable and 1 represents not at all 
comfortable. The suggestions for improvements are being 
analyzed and suitable modifications will be carried out in 
the future development work.  
VI. 
CONCLUSION AND FUTURE WORK 
The Interactive Mirror system has been designed 
developed and deployed in our Ubiquitous computing 
laboratory. The features are all personalized using face 
recognition technique. The health parameter measurement 
and analysis help the user in leading a healthier life. The 
computation time and accuracy of the face recognition 
algorithm have been analyzed and discussed. The 
accuracy of emotion recognition algorithm is tested and 
results are presented. The system is targeted towards 
smart home application. It can be also used in Beauty 
Parlors, 
Textile 
Shops, 
and 
Hotels 
with 
some 
modifications in the offered features. The new user is 
trained automatically by the system when they use first 
time.  
The system can be made much more useful to the 
users by adding more functionality like integrating light 
settings, speech processing, etc. The accuracy of the 
emotion recognition algorithm can be improved by 
including the extracted eye feature. The system can be 
deployed in real life home settings and feedback from 
wider set of users could be obtained for further 
enhancement of the system.  
ACKNOWLEDGMENT 
The work is developed under National Ubiquitous 
Computing Research Centre, funded by Department of 
Information Technology, Government of India.  We would 
like to thank Department of Electronics and Information 
Technology for providing us an opportunity to develop 
this smart artifact.  
 
REFERENCES 
[1] 
Paul Viola and Michael J. Jones, “Rapid Object Detection using a 
Boosted Cascade of Simple Features,” IEEE Conference on 
Computer Vision and Pattern Recognition, 2001, pp. 511-518. 
[2] 
Jun-Ren Ding, Chien-Lin Huang, Jin-Kun Lin, Jar-Ferr Yang, and 
Chung-Hsien Wu, “Magic Mirror,” IEEE International Symposium 
on Multimedia, December 2007, pp. 176-185. 
[3] 
Tatiana Lashina, “Intelligent Bathroom,” Philips Research, 
Netherlands. 
[4] 
Ana C. Andrés del Valle and Agata Opalach, “The Persuasive 
Mirror: computerized persuasion for healthy living,” Accenture 
Technology Labs, France. 
[5] 
Tomoki Hayashi, Hideaki Uchiyama, Julien Pilet, and Hideo Saito, 
“An Augmented Reality Setup with an Omnidirectional Camera 
based on Multiple Object Detection,” International Conference on 
Pattern recognition, 2010, pp. 3171-3174. 
[6] 
Carlos Hitoshi Morimoto. “Interactive Digital Mirror,” IEEE 
Proceedings on Computer Graphics and Image Processing, 2001, 
pp. 232-236. 
[7] 
Hyun Hoi James Kim’ “Survey Paper : Face Detection and Face 
Recogntiion”. 
[8] 
W. Zhao, R. Chellappa, A. Rosenfeld, and P. Phillips, “Face 
Recognition : A Literature Survey,” 2000. 
[9] 
W.K. Teo, Liyanage C De Silva, and Prahlad Vadakkepat, “Facial 
Expression Detection and Recognition System,” Journal of the 
Institution Engineers, 2004, pp. 14-26. 
[10] Hua Gu, Guangda Su, and Cheng Du, “Feature points extraction 
from Faces,” Tsinghua University, China.  
[11] R. Brunelli and T. Poggio, “Face Recognition : features versus 
templates,” IEEE Transactions on Pattern Analysis and Machine 
Intelligence, Vol.15, No.10, Oct 1993, pp. 1042-1052. 
[12] R.S. Feris, T.E. de Campos, and R.M. Cesar Junior, “Detection and 
tracking of Face features in video Sequences,” Lecture Notes in 
Artificial Intelligence, 2000, 1793(4), pp. 129-137. 
[13] Mohamed Deriche, “A Simple Face Recognition Algorithm using 
Eigeneyes and a Class-Dependent 
PCA Implementation,” 
International Journal of Soft Computing 3(6), 2008, pp. 438-442. 
[14] M. Turk and A. Pentland, “Eigenfaces for Recogntion,” J. 
Cognitive Neuroscience, Vol.3, 1991, pp. 71-86. 
[15] Matthew A. Turk and Alex P. Pentland, “Face Recognition using 
Eigen Faces,” IEEE Computer Society Conference on Computer 
Vision and Pattern Recognition, 1991. pp. 586-591. 
[16]  M. Gargesha and S. Panchanathan, “A Hybrid Technique for 
Facial Feature Point Detection,” Fifth IEEE Southwest Symposium 
on Image Analysis and Interpretation, 2002, pp. 134-138. 
[17] F. Bourel, C. C. Chibelushi, and A. A. Low, “Robust Facial 
Expression Recognition Using a State-Based Model of Spatially – 
Localised Facial Dynamics,” Proceedings of the Fifth IEEE 
international Conference on Automatic Face and Gesture 
Recogntion, 2002, pp. 106-111. 
[18] Claude C. Chibelushi and Fabrice Bourel, “Facial Expression 
Recognition 
: 
A 
Brief 
Tutorial 
Overview,”  
http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/CHI
BELUSHI1/CCC_FB_FacExprRecCVonline.pdf [retrieved : Jan, 
2012] 
[19] http://onionesquereality.wordpress.com/2009/02/11/face-
recognition-using-eigenfaces-and-distance-classifiers-a-tutorial/ 
[retrieved : Jan, 2012] 
 
 
 
 
 
8
Copyright (c) The Government of India, 2012. Used by permission to IARIA.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

