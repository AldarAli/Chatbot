Proposal on operator-assisted E-Government Systems  
Yoshihiro Uda, Kazuhiro Yoshida and Yoshitoshi Murata 
Graduate School of Software and Information Science 
Iwate Prefectural University 
Iwate, Japan 
e-mail: g236i002@s.iwate-pu.ac.jp, kazuhiro.iwk@gmail.com, y-murata@iwate-pu.ac.jp 
 
 
Abstract—Japan is ranked 18th in the world in the 2012 
United Nation’s e-Government database. People experience 
poor system usability due to bureaucratic wording (jargon) 
and non-universal interfaces. For a better e-Government 
system experience, we propose an operator-assisted e-
Government system. In this system, operators at a call center 
assist applicants by giving application process guidance as well 
as taking over keyboard operation for people with low IT 
literacy. Jargon is a major obstacle in Web site usability; 
therefore, we propose an analysis tool for evaluating current 
Web sites based on a phrase difficulty index. Evaluation 
experiments on a prototype system suggested that operator-
assisted application shortened the process time by 20% 
compared to conventional solo application. Also, errors were 
negligible for operator-assisted application. Calculated process 
times obtained from the analysis tool showed good agreement 
with the experimental results. The proposed e-Government 
system will greatly help to accelerate system usage by the 
elderly and people with low IT literacy. 
Keywords-e-Government; call center; Web site usability;  
jargon; operator-assisted application. 
I. 
 INTRODUCTION 
The 
Japanese-Government 
has 
promoted 
an 
e-
Government system since 2006. However, the acceptance of 
the system is not as high as expected. According to the 
United Nation’s e-Government database in 2012, Japan 
ranked 18th in the world, while the Republic of Korea has 
remained on top for years [1]. Many studies on e-
Government system usability were carried out and major 
issues were pointed out such as bureaucrat wording (jargon) 
and the lack of uniform interfaces among systems. Instead of 
fully redesigning the current e-Government systems, 
usability improvement of current system is desired in view 
of economy and speed. 
 We propose an e-Government system with a  call 
center operator assistance for applicants to improve the user 
interface. The operator talks with applicants over the phone 
to help with the application process and takes over keyboard 
operation for applicants who are not familiar with Personal 
Computers (PCs) and the Internet. The proposed system’s 
effectiveness 
was 
examined 
through 
model 
system 
experiments to measure the process time and errors in the 
application contents.  
We use our proposed phrase difficulty rank evaluation 
tool for usability analysis of current Web sites. The 
evaluation tool is useful to determine the possibility of 
improving usability by adding the call center functionality 
for current e-Government systems. 
 Section 2 describes related work on e-Government 
systems and their usability issues. Objectives of this study 
are discussed in Section 3, and the model system 
experiments are explained in Section 4. The experimental 
results are given in Section 5, followed by the conclusions. 
II. 
RELATED WORK ON E-GOVERNMENT SYSTEM 
USABILITY ISSUES 
Web site usability issues have been discussed for years 
[2], [3]. At the beginning stage of the e-Government system, 
both governments and citizens expected the systems would 
be quickly and widely accepted. However, the adaption rate 
of e-Government systems is slower than e-commerce and 
other Web-site-based systems.  
Fuchs clearly pointed out substantial differences between 
e-commerce 
and 
e-Government 
systems, 
e.g., 
no 
competition for the same services, lack of uniform outlooks, 
and subdivided territorial levels due to broad public 
administration scopes [4]. These findings explain why 
current e-Government systems’ one-stop interfaces cannot 
be used for different applications (tax payment, passport 
application, etc.) and why system usability is usually lower 
than that of e-commerce systems [5-8]. 
Wording (jargon and technical terms) is a significant 
indication of low usability of e-Government Web sites [8]. 
Applicants are forced to take time to learn the jargon on the 
e-Government Web site pages. When jargon must be used in 
Web pages, proper explanation should be included. Also, 
application process guidance should be given on the Web 
page for applicants. We found that some e-Government Web 
sites have links to operation manuals, but most of applicants 
would not willingly spend time to either read the huge 
manuals in detail or even notice the attachments.  
Gauvin et al. found that age is a markedly higher 
demographic determinant of Internet usage than education, 
income, gender, and urbanity [10]. Thus, assisting the 
elderly with the Internet and e-Government system 
operations has become an important issue for governments 
to better serve their citizens in view of e-inclusion [11], [12], 
[13]. Kim reported that “mass digital literacy campaigns” for 
several tens of millions of elderly, government officials, and 
housewives were carried out as Information Technology (IT) 
education programs in Korea, which has been consistently at 
the top of the e-Government system usability ranking. This 
is one of the key success factors for Korean e-Government 
systems [14], but such an education system is not always 
applicable to the large population of senior people like in 
Japan. Moreover, Internet access platform expands from the 
fixed line communication by PCs to smart-phones and 
26
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-324-7
ICDS 2014 : The Eighth International Conference on Digital Society

tablets. Preparing education programs which cover all the 
different access platforms takes time and may not be ready 
when the evolution occurs on IT networks. This paper 
proposes a new user assistance system, which has better 
applicability than the user education.  
III. 
PROPOSED E-GOVERNMENT SYSTEM 
3.1 A proposed system function 
 Senior citizens and other people with low IT literacy 
require proper assistance to use an e-Government system. 
Therefore, call centers have been used as help desks to assist 
applicants with the e-Government system [15], [16]. Call 
center operators assist applicants by helping with the 
application process and answering applicants’ questions on 
jargon. The call center system offers the flexible service by 
training operators for the access platform evolution. On the 
other hand, data protection and applicants’ privacy protects 
are remaining issues for the call center system [13]. 
Our proposed system extends the call center operator 
function from verbally assisting applicants to taking over 
keyboard operation for applicants [17], [18]. This would 
greatly reduce the burden on low-IT-literacy applicants. This 
system is expected to reduce the application process time as 
well as minimize the operation and application errors. 
 As discussed in the previous section, e-Government 
systems which contain a large amount of jargon result in 
poor Web site usability and high possibility of making errors 
in the application process. Solo application process time 
may be longer through difficult Web sites than through 
operator-assisted application. Also, fewer errors in operator-
assisted application are expected than on solo application. 
From this viewpoint, we believe that e-Government Web 
site usability can be measured based on process time and 
application error rates. These assumptions were verified 
through experiments and explained in the following Section.    
We also propose a numerical analysis tool in terms of 
jargon difficulty in usability evaluation of current Web site. 
The jargon difficulty index definitions are given for the 
major phrases used on the Web pages. The difficulty levels 
are then applied to process time calculations for applications 
with and without operator assistance. By using the analyzed 
results (expected times and error rates), e-Government 
system owners can predict the call center effectiveness for 
current e-Government systems. 
 
3.2 The system configuration 
Operators located at call centers for e-Government 
systems talk with applicants over the phone (Fig. 1). During 
the application process, the applicant and operator share the 
Web pages of the application system through their respective 
PC displays (Fig. 2). The page sharing system is 
implemented by the following technologies: 
a. 
Applicant identification: The applicants are given 
separate identification numbers (e.g., phone 
numbers) to correspond with the operator over the 
phone and Web pages. 
b. Web page sharing between the applicant and the 
   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1.  Proposed configuration for operator-assisted e-Government 
system. 
 
 
 
 
 
 
Figure 2.  Example of operator’s page on operator’s display. 
operator: Two Web page sets are prepared from the current  
e-Government system database: (a) Web pages for 
applicants and (b) those for operators. During the operator- 
assisted application process, the operator works on the PC 
keyboard and fills in the application form on the operator 
Web pages based on the conversation with the applicant. 
The application contents are stored in the database at the call 
center. 
c. Confirmation of application contents: When the 
operator finishes keyboard work, the Web pages are 
displayed on the applicant’s PC so that the applicant can 
check if the contents are the same as what the applicant gave 
to the operator. After content confirmation, the applicant 
clicks on the register button on the Web page and finalizes 
the application form to be sent from the call center database 
to the e-Government system. 
 
3.3 An evaluation tool for Web site usability 
 There are many materials to determine the difficulty 
level of kanji (Chinese characters used in Japanese 
language), mainly prepared for non-Japanese speakers. Basic 
Japanese words are also classified for Japanese students and 
foreigners [19]. However, most of the jargon found in e-
Government Web sites are not included in the current basic 
word classifications and no difficulty levels are available. E-
Government jargon requires a much higher reading level 
ID input,
Register button click
Keyboard work,
Web page reload
Share Web pages 
with operator
Share Web pages with applicant
Talk over phone
Applicants
Operator
at call center
Application
Web server
RDBMS
Application
Web server
RDBMS
Application data 
and Register flags
Talk over phone
27
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-324-7
ICDS 2014 : The Eighth International Conference on Digital Society

than for students because they are not common in text books, 
newspapers, and magazines.  
We propose an evaluation tool to determine the phrase 
difficulty rank for jargon appearing in e-Government Web 
sites. The difficulty rank definition is given by assigning 
unique difficulty indexes as extended ranks in the Balanced 
Corpus of Contemporary Written Japanese (BCCWJ) [20]. 
The BCCWJ covers a wide range of popular phrases found 
in books, magazines, newspapers, and Web sites. 
The index assignments are ranked as follows. 
 Rank 1: Phrases found in “Kanji (Chinese characters) 2100 
[19].” This rank corresponds to the basic words at the 
reading level of junior high school students. 
Rank 2: Phrases not listed in “Kanji (Chinese characters) 
2100,” but have 100 or more search results in the BCCWJ. 
Rank 3: Phrases which have 10 to 99 search results in the 
BCCWJ. 
Rank 4: Phrases which have less than 9 search results in the 
BCCWJ or only some of the words in the phrases are found 
in the BCCWJ. 
 Analysis of Web site usability is carried out as follows. 
Step 1: List all the phrases used in an e-Government Web 
site. 
Step 2: Assign phrase ranks to the listed phrases per the 
above-mentioned indexes. 
Step 3: Obtain a summary of phrase ranks, which appear in a 
particular application theme (described below) before filling 
in one of the input boxes of the Web page.  
 The phrase ranks are also applied to the expected 
process time calculations by solo and operator-assisted 
applications, as described in Section V-3.  
IV. 
EVALUATION EXPERIMENTS 
4.1 Application themes 
Evaluation experiments were carried out to confirm the 
effectiveness of the proposed system for improving system 
usability, as discussed in Sections 2 and 3. The measured 
parameters for usability comparison between current 
application systems and the proposed one were processing 
time and the number of application errors.  
Based on current e-Government and e-application 
systems, six application themes were prepared on a Web 
server as a set of Web pages and databases. The themes were 
designed from simple to complicated processes as well as 
those which require good understanding of the process and 
jargon (Table 1). The design concepts for the themes are 
listed below. 
Theme A: Registration of applicant profile; Applicant’s 
name, address, etc. This theme focuses on the correct inputs 
for the application form. 
Theme B: Certificate of residence: Applicants are guided 
to register their new bike. One of the requested documents is 
the certificate of residence. This theme examines if the 
applicant can choose the proper document required for bike 
registration.  
Theme C: Family register certificate: Applicants are 
guided to change the legal domicile for their new passport.  
The theme examines if the applicant does not mix the old 
and new domiciles, as well as current living address (note:  
 
TABLE  I. NUMBER OF PHRASES IN APPLICATION THEMES CLASSIFIED 
BY PHRASE RANK. 
Rank 
A 
 
 Regis-
tration 
B 
 
Resi-
dence 
certifi-
cate 
C 
 
Family 
Regis-
ter 
D 
  
Confer
-ence 
E 
 
In-
come 
tax 
F 
 
Tax 
certifi-
cate 
1 
5 
0 
0 
0 
6 
4 
2 
2 
3 
2 
2 
7 
9 
3 
0 
2 
2 
8 
11 
13 
4 
0 
1 
0 
5 
11 
13 
 
in Japan, the legal domicile and the living address may not 
be the same).  
Theme D: Technical conference registration: Applicants 
are requested to fill in the registration form for a technical 
conference. The theme is prepared to examine if the correct 
options have been selected and calculate the registration fee 
under given conditions (member discount, etc.). 
Theme E: Income tax calculation: A simplified tax 
calculation system is provided. Applicants are requested to 
input the total income as well as deductions of life insurance, 
social insurance, and medical expense. The theme examines 
if the applicant can understand the deduction system 
described with a large amount of jargon and make correct 
calculations for the related deduction items. 
Theme F: Tax payment certificate: The applicants need 
the tax payment certificate for housing loan refinancing. The 
certificate application form is complicated and difficult to 
understand due to the jargon. The theme design concept is 
similar to Theme E, testing information access and correct 
calculations. 
 
4.2 The experiment design 
 Each test participant was given a separate identification 
number. The test participants were divided into either a solo 
application group or operator-assisted application group. 
The 
solo 
application 
group 
simulated 
conventional 
application systems. The operator-assisted application group 
was established to confirm the advantages of the proposed 
system. 
The test participant and operator had their own PCs and 
displays, but they could not look at the other’s. The 
applicant’s profile (name, address, birth date, etc.) was given 
as a fictitious identity and was commonly applied to all the 
test participants. 
 
4.3 The experiment process 
 Applicant action steps are summarized in Table 2. The solo 
application group tried to complete all the themes 
themselves. A test participant read a theme and understood 
what information and actions were necessary to complete the 
application. When the test participant could not understand 
28
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-324-7
ICDS 2014 : The Eighth International Conference on Digital Society

 
TABLE II. APPLICANT ACTION STEPS. 
 
the technical terms and jargon in the theme expression, 
he/she had to use Internet search engines for guidance and 
explanations and process the application the Web pages. 
A test participant in the operator-assisted application 
group directly talked with the operator (one of the authors) 
instead of making a phone call. After the test participant 
chose a theme, he/she asked the operator for guidance. With 
the operator’s guidance, the test participant gave information 
to the operator. The operator looked at the application Web 
page and worked on the keyboard. After finishing the input 
process, the operator told the test participant to update the 
Web page so that the test participant could confirm the given 
information correctly appeared in the application form. 
When the test participant confirmed the application form, 
he/she clicked on the “register” button on the Web page and 
the application was completed. 
 
V. 
EVALUATION OF EXPERIMENT RESULTS  
5.1 Test participants 
    Thirty-seven participants (20 students and 17 office 
workers) were divided into 27 solo applicants and 10 
operator-assisted applicants. The operator-assisted group 
was smaller than the solo-application group, because the 
preliminary experiments showed highly consistent results for 
both the process time and errors in the operator-assisted 
applications.  The participant profiles were classified by 
generation and years of PC experience (Fig. 3). 
 
5.2 Experimental results 
Both 
time 
and 
error 
comparisons 
suggest 
the 
effectiveness of operator assistance during the application 
process. 
Figure 4 shows the experimental results of the average 
processing times for both solo and operator-assisted 
applications. Solo application took 20% more time to finish 
the latter three themes (D, E and F) than operator-assisted 
one. Error rates are also compared between solo and 
operator-assisted applications (Fig. 5). Operator-assisted 
application significantly reduced errors compared to solo  
 
 
Figure 3.  Test participant profiles (generation and years of PC experience). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4.  Experimental results of average processing times for operator-
assisted and solo applications. 
 
 
 
 
 
 
 
 
 
Figure 5. Experimental results of error rates for operator-assisted and solo 
applications. 
 
 
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
Resigtration
Residence certificate
Family register
Conference
Income tax
Tax certificate
Themes
E rro r r ate
Assisted
Solo
0
2
4
6
8
10
12
14
16
10s
20s
30s
40s
50s
60s
< 1 year
5
10
15
> 20 years
Head counts
Generation                                                                                 Years of PC experience
0
100
200
300
400
500
600
700
800
Resigtration
Residence certificate
Family register
Conference
Income tax
Tax certificate
Themes
Proces sing  tim e  (se con ds )
Assisted
Solo
29
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-324-7
ICDS 2014 : The Eighth International Conference on Digital Society

application except the theme C, on which some participants 
confused the living address with the legal domicile and gave 
wrong information to the operator. 
 
5.3 Experimental result analyses 
 Process time calculation was carried out using the 
proposed analysis tool for each experiment theme. Table 4 
shows part of the analysis table for Theme E (Income tax 
calculation). In this example, the first input box is the 
medical expense tax deduction amount. Prior to filling in 
this box, the applicant has to understand the meaning of 
jargon such as “Medical expense tax deduction,” “Medical 
insurance supplementation,” and “Hospital expense grant”. 
The phrase ranks for the jargon are given based on the rules 
explained in Section 3.3. A time factor is then applied to 
each phrase based on Table 3. The time factors were 
optimized by fitting the measured times in the experiments 
to time factor parameter sets. The time factor sum was 
calculated, and calculated time was obtained for solo 
application by multiplying the sum and a unit time defined 
to the input process. Following this process for the entire 
theme table, the total calculated time was examined. 
Operator-assisted time is also provided in Table 4.  
The differences between solo application and operator-
assisted application are: 
a. 
Jargon search and time taken to understand was 
shorter 
for 
operator-assisted 
application 
than 
solo 
application because the operator could give proper advice to 
the applicants on the meaning of jargon and operation 
process. 
b. 
Time to fill in an input box for operator-assisted 
application was longer than solo application. This was due to 
the conversation between the applicant and the operator to 
transfer the necessary information to fill the input box. 
c. 
Verification process was added to the operator-
assisted application. After the operator completed filling in 
the input boxes, he/she had to ask the applicant to check the 
box contents and to verify the application form. This process 
is not necessary for solo application. 
The calculated times for six themes are shown in Fig. 6 
and compared to the experimental results both for solo and 
operator-assisted applications. The calculated times showed 
good agreement with the measured results in the 
experiments. Processing time calculation would be useful to 
examine current e-Government Web sites in terms of the 
effectiveness of call center operators to obtain better user 
interfaces. 
 
VI. 
CONCLUSION AND FUTURE WORK 
We proposed an e-Government system with extended 
call center functionality. Call center operators talk with 
applicants over the phone and assist them by helping with 
the application process and taking over keyboard operations 
for applicants who are not familiar with PCs and the Internet, 
such as senior citizens. The proposed system’s effectiveness 
was confirmed through model system experiments by 
measuring the process time and errors of the application 
contents for both solo and operator-assisted applications.  
The experimental results suggest that the process time 
for complicated Web sites can be shorten by 20% by 
operator-assisted application compared to solo application. 
Also, errors which occurred in solo application were 
negligible in operator-assisted applications. We also 
proposed a phrase difficulty rank evaluation tool usability 
analysis of current Web sites. Analyzed results showed good 
agreement with the measured processing time for all the 
themes in the experiments. 
These results indicate that the proposed e-Government 
system will greatly help to accelerate system usage by senior 
citizens and other people with low IT literacy.  
For the future work, we intend to develop the error rate 
calculation method on the current e-Government Web sites 
for further site usability analysis. Some other indexes would 
be considered for the error rate calculation in addition to the 
proposed phase difficulty rank.  
 
 
TABLE III. TIME FACTORS VS. PHRASE RANK. 
Phrase rank 
Time factor 
1 
1 
2 
1.3 
3 
1.6 
4 
1.9 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6. Comparisons of calculated and measured processing times for 
both solo and operator-assisted applications.
 
 
 
 
 
 
 
0
100
200
300
400
500
600
700
800
900
Resigtra tion
Residence certifica te
Fa mily register
Confe re nce
Income tax
Tax certificate
Themes
Processing time (seconds)
Calculated
Mea sure d
Ca lculated
Measure d
Assisted
Solo
30
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-324-7
ICDS 2014 : The Eighth International Conference on Digital Society

 
  
 
TABLE IV. PART OF ANALYS  TABLE FOR THEME E  (INCOME TAX CALCULATION) 
 
 
 
 
 
REFERENCES 
 
[1] United Nations Public Administration Network, “2012 Global 
E-Government 
Survey,” 
2013 
on 
http://www.unpan.org/egovkb/global_reports/08report.htm 
[retrieved: January, 2014]. 
[2] J. Nielsen, “Usability 101: Introduction to Usability,” 2012 on 
http://www.nngroup.com/articles/usability-101-introduction-
to-usability/ [retrieved: January, 2014]. 
[3] J. Iio and H. Shimizu, “Evaluation improvement method for 
business system usability,” Research Papers, Mitsubishi 
Research Institute, vol. 50, pp. 30-53, 2008 (in Japanese). 
[4] G. Fuchs, “Lost Youth? Attitudes Towards and Experiences 
Withe-Government: The Case of Germany University 
Students,“ Proc. of 12th. European Conference on e-
Government, pp. 251-258, 2012. 
[5] R. Schwester, “Examining the Barriers to e-Government 
Adoption,” Leading Issues in e-Government Research, pp. 32-
50, Academic Publishing International Ltd., 2011. 
[6] D. Evans and D. C. Yen, “E-Government: Evolving 
relationship of citizens and government, domestic, and 
international 
development,” 
Government 
Information 
Quarterly, vol. 23, pp. 207-235, 2006.  
[7] S. Elling, L. Lentz, M. De Jong, and H. Bergh, “Measuring 
the quality of governmental websites in a controlled versus an 
online setting with the ‘Website Evaluation Questionnaire’,” 
Government Information Quarterly, vol. 29, pp. 383-393, 
2012. 
[8] Z. Khabaziyan, H. Teimori, and M. Hekmatpanah, “Planning 
E-Citizen: A Step toward E-Society”, World Academy of 
Application steps
Jargon
Phrase
rank
Input items
Time
factor
Time
factor
sum
Solo
application
calculated
time
Operator
-assisted
application
calculated
time
Read theme and
understand/search
Medical expense tax dedction
2
1.3
 (Note 1)
Medical insurance supplementation
4
1.9
Hospital expense grant
4
1.9
Major medical expense
2
1.3
Family medical expense
3
1.6
One-off maternity benefit
3
1.6
9.6
96
30
Fill input box
Medical expense tax
deduction amount
10
20
Verification between
applicant and operator
(Note 2)
―
10
Read theme and
understand/search
Social insurance tax deduction
3
1.6
(Note 1)
National pension
1
1
National health insurance
2
1.3
Nursing-care insurance
2
1.3
Unemployment insurance
3
1.6
6.8
68
30
Fill input box
Social insurance
deduction amount
10
20
Verification between
applicant and operator
(Note 2)
―
10
Note 1: For solo application
Units in seconds
Note 2: For operator-assisted application
 
31
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-324-7
ICDS 2014 : The Eighth International Conference on Digital Society

Science, Engineering and Technology, vol. 59, pp. 2590-2593, 
2011. 
[9] P. Jaeger and M. Matteson, “e-Government and Technology 
Acceptance: The Case of the Implementation of Section 508 
Guidelines for Websites,” Leading Issues in e-Government 
Research, pp. 231-252, Academic Publishing International 
Ltd., 2011. 
[10] S. Gauvin, K. Granger, M. Lorthiois, and D. Poulin, “The 
Shrinking Digital Divide – Determinants and Technological 
Opportunities,“ Proc. of 12th European Conference on e-
Government, Jan. 2012, pp. 259-267. 
[11] C. W. Phang, J. Sutanto, A. Kankanhalli, Y. Li, B. C. Y. Tan 
and H-H Teo, ”Senior citizens’ acceptance of information 
systems: A study in the context of e-Government services,” 
IEEE Trans. on Engineering Management, vol. 53, no. 4, pp. 
555-569, November, 2006. 
[12] T. Molnar, “Best Practices for Improved Usability of e-
Government for the Ageing Population, “ Proc. of 12th. 
European Conference on e-Government, pp. 493-501, 2012. 
[13] E. Mordini et al. “Senior citizens and the ethics of e-
inclusion,” 
 
Springer 
on 
http://link.springer.com/article/10.1007%2Fs10676-009-
9189-7#page-1, April, 2009 [retrieved: January, 2014]. 
[14] S. Y. Kim, “Korea ICT/e-Gov History, Best Practices and 
Lessons,” presented at Georgian Cyber Security and ICT 
Innovation Conference 2011, Nov. 18, 2011. 
[15] A. K. Singh and R. Sahu, “Integrating Internet, telephones, 
and call centers for delivering better quality e-Governance to 
all citizens,” Government Information Quarterly, vol. 25, pp. 
477-490, 2007. 
[16] F. Bao and F. Zhao, “Study on the E-Government Call Center 
System Based on SOA,” Computer and Information Science, 
vol. 4, no. 4, pp. 120-122, July, 2011. 
[17] Y. Murata, Y. Sato, T. Takayama, and N. Sato, “E-
Government System Using an Integrated Call Center System 
and WWW,” Proc. of the 2008 IEEE/WIC/ACM International 
Conference on Web Intelligence and Intelligent Agent 
Technology – vol. 03, pp. 199-202, 2008.  
[18] Y. Uda, K. Yoshida, and Y. Murata, “A Proposal and 
Evaluation of the Operator-assisted E-Government System,” 
Proc. of Dicomo 2012 (Multimedia, Distributed, Cooperative, 
and Mobile Symposium), July, 2012, pp. 860-866 (in 
Japanese). 
[19] Y. Tokuhiro, “Kanji (Chinese characters) 2100, Listed 
according to Frequency and Familiarity,” Sanseido Co. Ltd., 
2008 (in Japanese). 
[20] Japanese 
Corpus 
project, 
“Shonagon,” 
2012 
on 
http://www.kotono.ha.gr.jp/shonagon/ 
(in 
Japanese) 
[retrieved: January, 2014]. 
 
 
32
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-324-7
ICDS 2014 : The Eighth International Conference on Digital Society

