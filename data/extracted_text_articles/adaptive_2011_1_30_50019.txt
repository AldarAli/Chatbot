An Adaptable Process Planning Tool  
A Tool for Information, Comunication, and Interaction in a Robot Cell 
 
Fredrik Danielsson  
University West 
Trollhättan, Sweden 
e-mail: fredrik.danielsson@hv.se 
Linn Gustavsson Christiernin  
University West 
Trollhättan, Sweden 
e-mail: linn.gustavsson@hv.se
 
 
Abstract—This study presents work in progress on how to 
develop a process-planning tool to handle interaction between 
human operators and robots within a robot cell. First, we 
introduce how to include human activities in the process flow; 
then, we turn to our ideas for communication and feedback 
systems inside a robot cell. A small example of how to design 
interactive and re-programmable screens is presented. 
Keywords-Process planning tool; robot cells; automation; 
interaction; human operators and interface design.  
I. 
 INTRODUCTION  
When producing automotive vehicles, a large part of the 
manufacturing process is automated. The vehicle parts go 
through different stations or cells to be adjusted and 
assembled and robots and humans have different, usually 
isolated, assignments. A typical robot cell might consist of a 
couple of robots up to as many as 20 robots. The entire cell is 
controlled and coordinated by a PLC (Programmable Logic 
Controller) or an industrial computer. A robot might be 
assigned a specific manufacturing process or to transport the 
product through the cell.  
A problem with this type of automated production is the 
time it takes to make changes. An update in the vehicle 
model or the introduction of a new vehicle in the 
manufacturing line could take up to a year or more from 
planning to implementation [9]. The goal with our work is 
therefore a quicker process with fewer steps involved and 
production cells with robots easy to reconfigure and adapt so 
the time for changeover can be reduced. As a first step to try 
to achieve this goal, a process planning tool, P-SOP (Product 
oriented Sequence of Operation), was introduced in our 
earlier work [5] where the operators could create a new flow 
of activities for a robot cell through dragging and dropping 
icons in the tool. The tool then generated code from the 
graphical flow chart, which could be uploaded to the robots 
and PLCs. The next step to achieve more flexible production 
was to include the operator in the process; create possibilities 
for humans and robots to interact. The human operators 
should be able to go in to the cell and help the robot or 
interact with the robot to make the assembly more efficient. 
This required new functionality in our tool and a new way of 
thinking in the robot cell, involving safety, interaction, and 
efficiency issues. 
This study will focus on how to reconfigure the software 
tool to be adaptable for human-machine interaction during 
the production. We will present our work in progress for how 
to 1) communicate to the robot that a part of an assignment is 
performed by a human operator, 2) give the human operator 
feedback on what to do and where and when to step in, 3) 
visualize the work flow, and 4) in real-time analyze the 
consequences of a human operator stepping in and 
performing a task. In this paper we will discuss how to 
change the software tool and how to create an adapting 
solution. The work is planned to be performed in two steps; 
firstly, create an adaptable tool, where the human interaction 
can be predetermined, and secondly, a fully adapting system 
where the human, on the fly, can go into the production line 
and assist the robot.  
Next, we will introduce a brief background followed by 
our suggested solution and a small discussion of our work in 
progress.  
II. 
BACKGROUND 
This project started a few years back when the industry 
of Europe required quicker changeover times for changing 
automated production cells. Through a better and quicker 
process for how to give the robots instructions and 
reconfigure a cell a first step towards reduced changeover 
times could be achieved. A team was put together and 
worked within the FLEXA project [5, 7] to develop a tool 
and work with process development.   
In the original process (also the process still used in most 
companies), a process planer creates a sketch or workflow 
for the activities a part should go through in a specific 
production cell. Information about the part, the material and 
different constraints are added to the sketch and the 
document is then sent to a subcontractor. They turn the 
sketch into instructions and code that is uploaded to the 
robots and PLC. They configure the cell to be “turn-key” 
ready for production. While this is done the process planer 
have sent work instructions to the operators of the cell for 
what to do and how. To speed up this process the idea is to 
exclude the subcontractor and create a tool where the process 
planner and operator can make the robots and PLC ready by 
themselves. Eventually the idea is to also exclude the process 
planer and only have an operator to perform the whole chain 
of activities.  
A software tool was invented [5], where an operator 
could create new sequences or flows of instructions by 
combining graphical icons representing different tasks in the 
15
ADAPTIVE 2011 : The Third International Conference on Adaptive and Self-Adaptive Systems and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-156-4

cell. The sequence is translated into code and downloaded to 
the robots and PLC. This made it possible to put together 
different tasks for the robot to perform with rather short 
notice; creating a more flexible production.  
Parallel to this project a second project called Flex Lean 
was initiated [6] where safety issues were addressed; due to 
safety regulations (HSG43 and EN ISO 13849-1) the humans 
are not allowed to be in the robot cell when active. If a 
human enters the robot cell during operation, sensors will 
activate the safety system. This will initiate the safety control 
system to cut power to the robots. Such a response will slow 
down production and halt the entire cell. In Flex Lean, these 
safety regulations have been further investigated and 
different ways for robots and humans to interact have been 
introduced within the safety regulations.  
These two projects create the foundation for our work in 
this study. Where we add more functionality to the tool and 
assume that the safety issues of the cell can be solved 
through cooperative solutions for robots such as SafeMove 
from ABB [2, 12]. 
A. Related work 
A more advanced process simulation tool and theories for 
how to best describe, model and simulate process planning 
activities are produced within the framework of FLEXA but 
by another research group [3, 19]. They have studied how to 
create and simulate the sequences in which tasks should be 
carried out but also how to include different constraints and 
organizational parameters when trying to optimize the 
process flow. 
Comparing the more advanced tool to the current one in 
this study; the current tool tries to focus more on how to 
quickly re-program a robot cell and the actual interaction and 
communication language between operators and robots while 
the other tool focus more on a understanding of the process it 
self and the work model. Their tool is used as a guide earlier 
in the process when the actual steps of the process are being 
determined; they also try to visualize the workflow. In 
current work we assume that the process steps have been 
already determined by a tool like theirs or similar. We start 
with a process plan that has been simulated in earlier steps 
and then try to look at how to handle rapid changes in that 
sequence and how to, eventually, include the operator in the 
process sequence. We will however, in later work, try to 
merge the ideas from booth tools.  
III. 
WORK IN PROGRESS - INTEGRATING HUMAN TASKS IN 
PROCESS FLOW 
To create a situation where the human operator and the 
robots or machines can cooperate we need to adapt 1) the 
configuration tool, 2) the interface of communication, 3) the 
feedback system, and 4) the physical environment. In the 
current study we have initiated work on no. 1, 2 and 3 while 
4 is considered in a parallel project and related studies. In our 
work we are very aware of the safety issues but also 
confident that results from related projects [2, 3, 6, 12, 19, 
21] will solve the physical challenges in the robot cell. To a 
get better understanding of the requirements for how to 
include the operator in the process, we are closely 
cooperating with a number of industrial partners [14].  
The work of further developing the configuration tool has 
been divided into two sub-steps. In the first step we create an 
adaptable tool where the human activity can be pre-
configured and planned, analyzed and implemented during 
setup of the cell. In the second step this should be possible to 
do in real-time from the cell while in action, resulting in 
changed activities for the robot during production. We are 
currently working on the first step of introducing human 
activities into the workflow.  
A. Adapting the configuration tool  
In Figure 1, an example of a process flow is illustrated, 
taken from one of our industrial partners. The flow could be 
described as a regular UML activity diagram [1], but we 
want to show both the physical robots and their activities, 
which is why we use this type of flowchart. The product, 
illustrated by a triangle, comes into the flow from the left and 
is then moved from station to station until it has passed 
through the sequence. Each rectangle in this example 
represents a machine or robot that performs a specific task. 
In the example in Figure 1, no human activities are included.  
If we now introduce activities performed by a human 
operator, it could look like the flow presented in Figure 2. 
The activity is marked as “human activity” during process 
planning and translated into information to the cell. A formal 
way to describe the details of the human activity and the 
interaction could be to use BPEL4People [11] or any other 
standardized modeling language. By clicking the rectangle 
marked as a “human activity” a new UML chart or flow 
chart should be visible showing each step of the task 
performed by the human. A tree view according to GOMS 
model (Goals, Operators, Methods, and Selection rules) [4] 
could be added to the chart to further expand the details. The 
goal would be the purpose of the human activity – what is 
the human expected to do. The Operators then describe the 
particular actions the human must perform to reach the goal. 
The order of the operators creates a sequence or a Method. 
There might be more than one viable sequence of actions 
available and if so the selection rules are used to establish 
when to use which method. In Figure 2 the human is 
expected to perform an inspection and the GOMS 
documentation should then provide information about how to 
perform the inspection, what type of inspection it is and if 
there are alternative ways depending on the product.  
From a software-tool perspective, this looks just like a 
small change but in reality this will have major 
consequences. The outcome of the human activity has to be 
communicated to the robots so that the next robot in the flow 
knows how to act. The communication could be through a 
broadcast signal or a message sent from the PLC system. The 
messages also have to be visualized to the operators in the 
screen. This can be done in several ways and is a matter for 
future testing.  
The overall PLC control system will be responsible for 
coordinating the work and keep track of the parts. By using a 
unified process description, the system can keep track of the 
parts in a sequence of activities.  
16
ADAPTIVE 2011 : The Third International Conference on Adaptive and Self-Adaptive Systems and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-156-4

 
Figure 1. An example of a sequence of activities in a production line, created in our process planning tool. 
 
 
Figure 2. An example of how a human have taken over one activity in the process flow. 
 
B. An interface for comunication between actors 
To solve parts of the communication problem we have to 
turn to the actual robot cell. It is here the interaction is being 
performed and also here the signals have to be sent from. In 
Figure 3 below we see a typical robot cell and a human 
operator waiting outside for an activity (left side). Next we 
see how the operator goes into the cell (right side) to perform 
the tasks. To communicate with the operator it has been 
concluded that we need a screen of some sort in the cell.  
According to our industrial partners the screen should 
have multiple purposes and communicate different types of 
information to the operator. The screen should provide the 
operator with support for the task at hand but also show the 
overall flow so the operator knows where in the flow they 
are involved and why.  
 
 
Figure 3. A robot cell where an operator is first waiting for 
the command to go in (to the left) and then goes into the cell 
(to the right) to solve a set of tasks. 
A GOMS document – A 
text description of what the 
human operator is supposed 
to do step by step including 
alternative methods – In 
this case an Inspection 
17
ADAPTIVE 2011 : The Third International Conference on Adaptive and Self-Adaptive Systems and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-156-4

It is beneficial to use symbols and icons the operators are 
used to as well as interaction styles that are adapted to the 
industrial environment [20]. By using an interface with 
different levels of information the operator can get access to 
information of the process from a meta level down to 
detailed information on materials and assemble instructions. 
The interface should show the process flow so that the 
operator can see what is going on in the cell in front of him 
or her. In Figure 4, an example of our current design 
suggestion is illustrated. When an activity in the flowchart is 
clicked a set of icons in a menu appear and the operator can 
select what to do by clicking one of the icons. In Figure 4 
the operator has selected the document mode and a group of 
documents, attached to the activity selected, have appeared 
in the lower part of the screen. It should then be possible to 
open up available documentation for that activity and 
detailed descriptions on the different steps. 
 
 
Figure 4. An example of what a screen for cell interaction 
and process information could look like. 
 
According to our background research, the screen should 
provide process completion information; time status of the 
activity. It should also signal to the operator that it is time for 
human activity within the cell and when to go in (see Figure 
5). When the operator has entered the cell, the screen should 
provide information about the tasks at hand, and be able to 
guide the operator through the tasks by providing text and 
image descriptions.  
 
 
Figure 5. An alert message telling the operator that it is time 
for him or her to enter the cell in a minute and a half.  
 
In Figure 5 an example of an alert message is displayed 
showing that an operator is needed within a minute and a 
half of time. In this screen we have also concluded that it 
could be helpful to show instruction for pre-conditions. A 
pre-condition could be material or a tool needed for the 
assembly, which the operator must fetch and bring with him 
or her into the cell before the task can be performed. 
C. Creating a feedback system for interacting actors 
When the robots have performed a task they signal to the 
PLC that the part can be picked up and moved to the next 
station. The screens described in previous section should not 
only function as information displays they should also adapt 
and provide possibilities for the actors to interact. The screen 
should be able to signal different scenarios to the operator 
but also be used to signal feedback to the robot and to the 
PLC. When an action has been performed or a problem has 
been solved the human must be able to signal back to the 
system. Figure 6 illustrates an example of our current design 
for what such a screen could look like. In the figure the 
action Inspection is selected and its work tasks are shown as 
a list. The list is clickable to provide details for each task. By 
clicking a task the operator also gets access to forms for 
measurements, error reports and feedback notes. The 
operator can use the checkboxes to signal when a task is 
completed. When all tasks in an action are performed and 
checked the “completed” button will light up and get 
available. The operator can then click it and through that 
signal to the control system that the action is complete and 
that the part can be moved to the next cell. Depending on the 
outcome of the activity the PLC will order different actions 
for the next step in the flow. If for example the part is 
discovered to be defect during inspection the part should be 
discard and the PLC will order the moving robots to do this 
instead of moving it to the next station in the flow. 
   
 
Figure 6. The process flow and instructions for the operator. 
IV. 
DISCUSSION AND CONTINUING WORK 
The next step of our work is to gather more information 
of how to design the interfaces. We will look at icons, click 
patterns, interactions styles, and colors, as well as contents 
and information presentation; using established research in 
the field [20]. The operators will then be involved in 
usability analyzes [16]. Later on, the human operator should 
be able to change the process flow during operation by 
accessing the tool from the cell; changing a “machine made” 
18
ADAPTIVE 2011 : The Third International Conference on Adaptive and Self-Adaptive Systems and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-156-4

activity into “performed by operator”. According to 
industry, it would be beneficial if a sensor grid could identify 
what the operator is doing and by automation change the 
activities in the process flow and tell the robot how to 
interact. It should be possible to signal to the system and the 
robot by hand signals or camera identification when a part 
for example has been assembled correctly and then update 
the status of that task in the tool. Research has been done [8, 
10, 13, 17-18] on this type of Human-Robot inter-action and 
should be possible to incorporate in our solution. We have 
also concluded that the process planning tool has to be able 
to in real-time analyze a change of procedure and estimate 
different consequences and produce a risk assessment for the 
operator. The idea is that the tool should be able to look at 
the process flow and estimate the consequences of different 
scenarios. It should be able to tell the operator if it is 
beneficial to enter the robot cell at that time or if he or she 
should wait a bit depending on how that interruption affects 
the robot at that time. The tool should also be able to signal 
for help when a robot gets overloaded and falls behind the 
work flow, so that an operator can go in and if possible assist 
that robot or reassign another robot in the line to help out.  
Similar problems have been tackled in research on 
holonic manufacturing systems [9, 15, 22, 23]. Here each 
robot, machine or operator is assigned with autonomous and 
cooperative properties and their activities are determined 
through cooperation instead of a centralized system. A large 
proportion of the work done within holonic systems is 
focused on the technical sides however and our work on 
including the operator could perhaps add knowledge to that 
domain. Current work in related projects on artificial 
intelligence could also render guidance in this matter. 
However, this is highly non-trivial and it will require 
solutions both from our project and parallel projects within 
FLEXA [18, 19] and Flex Lean [6, 7] as well as related work 
within the research community. The work from several 
projects will eventually be turned into one or two 
applications that will be used within our partner industries.  
ACKNOWLEDGEMENT  
The authors would like to thank Eric Lind for editorial help 
and our cooperating partners for sponsoring our project. 
REFERENCES 
[1] S. W. Ambler 2004, The object primer : agile model-driven 
development with UML 2.0, 3. ed. Cambridge: Cambridge 
Univ. Press, UK  
[2] K. Behnisch, “Taming the robot - Better safety without higher 
fences”, in Robotics Highlights ABB Review 4, 2006 
[3] K. Bengtsson et al. “Relations identification and visualization 
for sequence planning and automation design”, in Proc of   
2010 IEEE Conference on Automation Science and 
Engineering (CASE), 21-24 Aug. 2010, p. 841. 
[4] S. Card, T.P. Moran and A. Newell (1983). The Psychology 
of 
Human 
Computer 
Interaction. 
Lawrence 
Erlbaum 
Associates. 
[5] F. Danielsson, “A Flexible Process Planning Platform for 
Flexible Automation”, Project presentation, University West, 
2011, [Last access: 2011-05-14],  [Electronic] 
        http://www.cdti.es/recursos/doc/eventosCDTI/Aerodays2011/1D4.pdf 
[6] F. Danielsson, B. Svensson, and S Gustavsson, “A Flexible 
Lean Automation Concept for Robotized Manufacturing 
Industry”, In Proc. of 11th Middle Eastern Simulation 
Multiconference: Alexandria, Egypt, Dec. 2010, pp. 101-104.  
[7] FLEXA - Advanced Flexible Automation Cell, A reseach 
project within the European Seventh Framework Programme 
(FP7). [Electronic:] http://www.flexa-fp7.eu, [Last acess: 
2010-05-28]. 
[8] M. A. Goodrich  and A. C. Schultz,“Human-robot interaction: 
a survey”, In Foundations and Trends in Human-Computer 
Interaction Volume 1 Issue 3, February 2007.  
[9] L. Gou, P.B. Luh and Yuji Kyoya, “Holonic manufacturing 
scheduling: 
architecture, 
cooperation 
mechanism, 
and 
implementation”, in Computers in Industry 37, Elsevier, 
1998, pp. 213-231,  
[10] S.T. Hayes, E.R. Hooten, and J.A. Adams, “Multi-touch 
interaction for tasking robots”, In Proc. of the 5th ACM/IEEE 
international conference on Human-robot interaction (HRI 
'10), Osaka, Japan, Marsh, 2010, pp. 97-98.  
[11] T. Holmes, H. Tran, U. Zdun, and Schahram Dustdar, 
“Modeling Human Aspects of Business Processes – A View-
Based, Model-Driven Approach”. in Proc. of the 4th 
European conference on Model Driven Architecture: 
Foundations and Applications (ECMDA-FA '08), Berlin, 
Germany, June, 2008. 
[12] S. Kock, J. Bredahl, P.J. Eriksson, M. Myhr, and K. Behnisch, 
“Safe collaboration with ABB robots - Electronic Position 
Switch and SafeMove”, White Paper ABB Robotics AB, 2008  
[13] K. Nickel and R. Stiefelhagen, “Visual recognition of 
pointing gestures for human–robot interaction”, In Elsevier,  
Image and Vision Computing Volume 25, Issue 12, 3, Dec. 
2007, pp. 1875-1884. 
[14] The 
Production 
Technology 
Centre, 
[Electronic] 
http://www.innovatum.se/pages/ptc_uk_partners-3121.html 
[Last access: 2011-05-14] 
[15] A. Rosu, “PLC-based Holonic  manufacturing cell transport 
system”, U.P.B. Scientific Bulletiene, Series C, Vol. 73, Issue 
2, 2011, pp. 117-126. 
[16] J. Rubin, and D. Chisnell, (2008), Handbook of Usability 
Testing - How to Plan, Design, and Conduct Effective Tests, 
2nd Ed. WILEY publishing Inc. Indianapolis, USA. 
[17] E. Sato, T. Yamaguchi, and F. Harashima, “Natural Interface 
Using Pointing Behavior for Human–Robot Gestural 
Interaction”, In IEEE Transactions on Industrial Electronics, 
Volume 54, Issue 2, April 2007, pp. 1105-1112. 
[18] R. Stiefelhagen, et al., “Natural human-robot interaction using 
speech, head pose and gestures,” in Proc. IEEE/RSJ Int. Conf. 
Intell. Robots Syst., 2004. pp. 2422–2427. 
[19] N. Sundström, “Automatic Generation of Operations for the 
FLEXA Production System”, Master of Science Thesis in 
Automation, Depertment of signals and systems, Chalmers 
University of Technology, Göteborg, Sweden, 2010. 
[20] B. Shneiderman and C Plaisant (1998) Designing the User 
Interface-Strategies 
for 
Effective 
Human-Computer 
Interaction. 4th ed. Addison Wesley Longman, Inc. USA. 
[21] D.J. Smith (2010) The Safety Critical Systems Handbook, 
Butterworth-Heinemann, Oxford, UK.  
[22] H. Sun and P.K. Venuvinod, “The human side of holonic 
manufacturing systems”, in Technovation, Volume 21, Issue 
6, June 2001, pp. 353-360  
[23] J. Wyns, Concepts for Holonic Manufacturing, Katholieke 
Universiteit, 
Leuven, 
1996, 
[Electronic]: 
http://www.mech.kuleuven.ac.be/pma/project/goa/concepts.htm  
[Last access: 2011-05-14] 
 
19
ADAPTIVE 2011 : The Third International Conference on Adaptive and Self-Adaptive Systems and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-156-4

