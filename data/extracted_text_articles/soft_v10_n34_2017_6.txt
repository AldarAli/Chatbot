Watermarking Technique for Images Captured with Cameras Using Color–
Difference-Modulated Light 
 
Kazutake Uehira and Hiroshi Unno 
Kanagawa Institute of Technology 
Atsugi, Japan 
e-mail: uehira@nw.kanagawa-it.ac.jp 
 
Abstract— We propose a new optically written watermarking 
technique that can protect the portrait rights of real objects. It 
produces a watermarking pattern in the illumination light by 
modulating color differences. The illumination light that 
contains such watermarking is projected onto an object. An 
image of the object taken by a camera contains the same 
watermarking, which can be extracted by image processing. 
Therefore, this technique can protect the portrait rights of real 
objects. We discovered three findings through simulation 
where binary data were embedded as watermarking and we 
evaluated the accuracy with which the binary data were read 
out. The first was that the accuracy when color differences 
were modulated was higher than that when brightness was 
modulated. The second was that errors in reading out 
embedded binary data particularly tended to occur in dark 
areas, yellow areas, and areas that contained fine textures. The 
third was that we could satisfy both the invisibility and 
readability requirements of embedded data by using 
appropriate amplitudes of modulation.  
Keywords-Watermarking 
patterns; 
information 
embedding;  
portrait rights. 
I. 
 INTRODUCTION 
We recently proposed a digital watermarking technique 
that used illumination light whose color differences were 
modulated as a technique to embed information in a captured 
image of a real object [1] and we demonstrated its feasibility. 
This paper presents the detailed results obtained from 
research, application conditions, and advantages with other 
methods. 
Digital watermarking technology had originally been 
studied as a copyright protection technology for digital 
content. Copyright protection of digital content has become 
increasingly important as it has been progressively more 
distributed throughout various media because it consists of 
digital data that can easily be copied, which are exactly like 
that in the original. Digital watermarking is an effective way 
of protecting copyrights from being illegally copied and 
various techniques of digital watermarking for digital content 
have been developed [2]–[9].  
Out of all the techniques to process various kinds of 
content, those for images have been studied and developed 
the most. Digital watermarking of image content is 
embedded in digital images in various ways. Embedded 
watermarking is invisible when the images are displayed in 
most of these ways. Although it is invisible, it can be read 
out by applying digital processing to image data.  
Digital watermarking has also been used in printed 
images, where digital watermarking is embedded in the 
digital data before the images are printed [10]–[13]. This is 
to prevent the images from being copied from printed images 
by digital cameras or scanners. The watermarking in this 
case is read out from the image data produced by digital 
cameras or scanners. 
However, whether digital watermarking is in the digital 
data of an image, in a displayed image on an electronic 
display or in a printed image, conventional digital 
watermarking rests on the premise that people who want to 
protect the copyrights of their digital content, i.e., content 
creators or content providers, have the original digital data 
and they can embed watermarking in the original digital data 
by digital processing. 
However, this premise does not apply to some cases. Let 
us assume that a person took a picture of a painting at a 
museum with a digital camera.  Since recent digital cameras 
are highly advanced, captured images have very high levels 
of quality and if the painting is invaluable as a portrait, e.g., 
an artwork that has been painted by a famous artist, the 
captured image of such an irreplaceable painting may be 
extremely expensive. Therefore, the portrait rights of well-
known paintings should be protected. However, images 
captured with digital cameras do not have watermarking in 
this case because they have been captured by visitors to 
museums who are not interested in protecting portrait rights; 
therefore, they are susceptible to illegal use.  
The portrait rights or copyrights of such images should 
be protected. We previously proposed a technology that 
could prevent the images of objects from being used in such 
cases [14], [15]. It used illumination that contained invisible 
watermarking. 
As 
the 
illumination 
contained 
the 
watermarking, the images of photographs of objects that 
were illuminated by such illumination also contained 
watermarking. We demonstrated the feasibility of this 
technique and demonstrated that this technique could also be 
applied to objects with curved surfaces [16]. We produced 
watermarking by spatially modulating the brightness of the 
illumination.  
Accuracy in reading embedded information from the 
captured image of a real object is one of the most important 
evaluation indexes. Sufficient accuracy has not been 
obtained for various kinds of images when the amplitude of 
modulation has been small by using watermarking produced 
by modulating the brightness of illumination because the 
invisibility of watermarking in the degree of modulation 
231
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

needs to be as small as possible. We studied a technique of 
watermarking using color difference-modulated light in this 
study to improve accuracy in reading embedded information.   
Uchida et al. used color difference signals to produce 
watermarking in digital images [17]. This was different from 
their method that produced watermarking with lighting that 
illuminated real objects; on the other hand, they 
electronically generated it directly in the digital images.  
This paper is structured as follows. Section II explains 
the optical watermarking we propose and have studied thus 
far. Section III presents a new technique we used in this 
study that used color difference. Section Ⅳ describes 
simulations we conducted to evaluate the readability of the 
watermarking from captured images with the new technique. 
Section Ⅴ presents the results obtained from an experiment 
and discusses them. Section Ⅵ concludes the paper.   
II. 
EMBEDDING WATERMARKING IN 
ILLUMINATION AND RELATED WORK 
Figure 1 outlines the basic concept underlying our 
watermarking technique using illumination light to embed a 
watermark. An object is illuminated by projected light that 
Information to be embedded  
Read out 
Captured image 
  
Camera  
Light source 
Illumination light 
10010110 
Binary data 
Watermarking in light  
©2017UEHIRA 
©2017UEHIRA 
Figure 1. Basic concept underlying proposed technology 
Real object 
Figure 2. Procedure for producing optical watermarking 
(a) 
Block data in frequency 
d
i  
DC component 
Pattern for “1” 
(HC>0) 
Highest frequency component 
i-DCT 
(b) Block image data 


<
>
=
0)  for   "0"
(
HC
0)  for   "1"
HC(
)
7,7
(
F
0
( , )
u v =
F
(c) Whole image data 
SLM 
(d) Watermarking pattern  
Phases of these two patterns are opposite 
 
Pattern for “0” 
(HC<0) 
Illumination light 
232
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

contains an invisible watermark. A photograph taken of the 
object illuminated in this way would also contain the 
watermark. The watermark can be extracted in the same way 
as that used in conventional watermarking techniques for 
digital content. 
There are various ways of possibly producing optical 
watermarking. Figure 2 illustrates the procedure for 
producing optical watermarking in our previous studies. A 
watermarking pattern was produced by modulating the 
brightness of the illumination light. First, we produced a 
watermark pattern as image data. The whole image area that 
corresponds to the illumination area in Fig. 2 is divided into 
numerous blocks. Each block has 8 x 8 pixels. First, each 
block datum is expressed in the frequency domain. Each 
block only has a DC component and a highest frequency 
component (HC) in both the x and y directions. The DC 
components in all blocks have the same value and they 
provide the brightness of illumination. The absolute value of 
HCs is the magnitude of modulation in brightness. We 
express one-bit binary data to be embedded by the sign of the 
HCs, i.e., if an HC is positive, it is expressed as “1”  and if it 
is negative, it is expressed as “0”. After the HCs for all 
blocks have been set, data in the frequency domain are 
converted to those in the space domain as image data by 
inverse discreet cosine transformation (i-DCT), and they are 
then input to a space light modulator (SLM) and changed to 
illumination light that illuminates real objects, such as 
paintings. We could use a commercial projector as an SLM 
for this purpose. Fig. 2 (d) shows the watermarking pattern in 
the light. These two patterns are for the “1” and “0” of binary 
data and the phases of these two are opposite.  
The image of the object captured with a camera, I(x,y), is 
given as a product of the reflectance of the object surface, 
R(x,y), and the luminance of the projected light, L(x,y), as:  
{
0}
)
(
)
(
)
(
L
kR x,y L x,y
I x,y
+
=
,                (1) 
where L0 is bias luminance, such as that produced by room 
light and k is a constant.  
The captured image also has a high-frequency pattern 
produced by modulating brightness because, as derived in Eq. 
(1), it is given by the product of the reflectance of the object 
surface and the luminance of the illumination light that 
contains the high-frequency pattern. This means the captured 
image also contain watermarking.  
The watermarking pattern in the light and in the captured 
image cannot be seen by the human visual system because it 
is modulated at the highest frequency and the amplitude of 
modulation is small.  
Figure 3 illustrates the procedure for reading out 
embedded information from a captured image. A captured 
image is divided into blocks in similar ways as that in the 
original, and then the pixel data in each block are converted 
into data in the frequency domain by discreet cosine 
transformation (DCT). Finally, the embedded data are read 
out by checking the sign of the HC for each block.  
A technique based on a similar concept has been 
proposed by Zhou et al. [18] in related work. They 
temporally  modulated the brightness of light by using  a 
digital light processing (DLP) display. Although this was the 
same in terms of invisibly embedding information in 
projected light, luminance was temporally modulated it 
could not be applied to our purposes because our technology 
was targeted at shooting still images. Moreover, a method of 
using near infrared light has been studied [19] from the 
viewpoint of invisibly embedding information in light. 
Although this technique exploited the differences between 
sensory perceptions of humans and devices, it had the 
disadvantage that embedded information was eliminated by 
using an infrared cut filter. 
III. 
OPTICAL WATERMARKING USING COLOR–
DIFFERENCE MODULATION 
We evaluated a method of producing watermarking in this 
study by modulating color differences instead of brightness. 
Luminance, i.e., chroma-blue and chroma-red (YCbCr) 
signals, was used to produce the watermarking. The basic 
procedure for producing the watermarking was the same as 
that in our previous study where we produced watermarking 
by modulating the brightness, Y. First, we produced the 
Figure 3. Procedure for reading out embedded information from captured image 
Real object 
Block image  
Block data in frequency 
 
Check sign of highest frequency component 
 
Illumination light 
Camera  
i-DCT 
Captured image  
233
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

original data of the color difference, Cb, as the frequency 
domain data for each block, then converted them into a block 
image in the space domain by i-DCT, and combined all 
block images into one image. The Y and Cr components 
were constant. After the YCbCr were converted into a red, 
green, and blue (RGB) signal, the RGB signal was input to a 
projector, and the watermarking pattern was projected onto 
the object.  
A captured image was first converted into a YCbCr 
signal, and then the Cb component was divided into blocks 
and converted into data in the frequency domain by using 
DCT. Finally, the embedded data were read out by checking 
the sign of the HC of the Cb for each block in the same way 
as the method outlined in Fig. 3. 
IV. 
SIMULATION 
We evaluated the technique that used color difference 
modulation, which was explained in Section Ⅲ, by 
simulating the procedure in Fig. 3. The image data, I(x,y), of 
an object that was captured with a camera were obtained 
using Eq. (1).  
We used standard images as objects that had 512x512 
pixels, as shown in Fig. 4, i.e., we used RGB pixel values of 
standards images as the reflectance of the object surface, 
R(x,y), in Eq. (1).  
We first generated the initial data of Cb in the frequency 
domain for L(x,y) in Eq.(1), as shown in Fig. 2 (a). The data 
were generated for each block that had 8 x 8 components. 
Therefore, the L(x,y) used in this simulation is given as:  
(
)
16
1)
cos (2
16
1)
(2
cos
)
(
( ) ( )
4
1
7
0
v
7
u 0
vπ
n
uπ
m
C u C v F u,v
x,y
L
+
+
×
=
∑
∑
=
=
,        (2)  
where m and n are the pixel coordinates within the block and 
are given as:  
m = mod(512, y)
 and                           (3) 
n = mod(512,x)
.                                 (4) 
Here, mod (i,j) represents the remainder when i is divided by 
(a) 
Image A 
(b) Image B 
(c) 
Image C 
Figure 4. Images used as objects in simulation.  
(d) Image D 
(1) HC=5 
(2) HC=10 
(3) HC=15 
(4) HC=20 
(1) HC=5 
(2) HC=10 
(3) HC=15 
(4) HC=20 
(a) 
Modulating Cb 
(b) Modulating  Y 
Figure 5. Luminance distribution images of projected light 
L(x,y)   These are magnified images of block 
234
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

j. F(u,v) in Eq. (2) is given as:  



≠
=
=
=
 0 or 7
for
0
7
for
0
for
200
u,v
u,v
HC
u,v
F(u,v)
                     (5) 
The number of blocks of the initial data in the frequency 
domain was set to 64 x 64 (=4096) to make the number of 
pixels of L(x,y) equal the number of pixels of R(x,y), i.e., 512 
x 512. The signs of the highest frequency component (HC) 
for each block were determined depending on whether to 
embed “1” or “0” in that block. The same number of “1” and 
“0” were randomly embedded. The magnitude of HC in the 
original data was changed from one to 20 as an experimental 
parameter, while Y, Cr, and L0 were set to correspond to 
constant values of 200, 0, and 40. These values were the gray 
levels of image data whose maximum was 255. We 
embedded a watermarking pattern for reference by using our 
previous method, where we modulated Y. Here, Cb and Cr 
were set to zero, and L0 was set to 40. Figure 5 has images of 
L(y,x) when HC was 5, 10, 15 and 20.  These images are 
magnified images of a block. 
After I(x,y) was derived with Eq. (1) and it was converted 
into YCbCr format data, it was divided into 4096 (64 x 64) 
Figure 6. Accuracy in reading out binary data.  
Accuracy indicates percentage of data correctly read out from 4096 binary data. 
75 
80 
85 
90 
95 
100 
Accuracy (%) 
0 
5 
10 
HC 
Modulating Y 
Modulating Cb 
75 
80 
85 
90 
95 
100 
Accuracy (%) 
0 
5 
10 
HC 
75 
80 
85 
90 
95 
100 
Accuracy (%) 
0 
5 
10 
HC 
(a) 
Image A 
(c) 
Image C 
Modulating Y 
Modulating Cb 
Modulating Y 
Modulating Cb 
(b) Image B 
75 
80 
85 
90 
95 
100 
Accuracy (%)) 
0 
5 
10 
HC 
(d) Image D 
Modulating Y 
Modulating Cb 
235
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

blocks, each whose 8 x 8 pixels and Cb components were 
converted into data in the frequency domain by the DCT for 
each block.  
Embedded data were read out by checking the sign of the 
highest frequency component of the Cb for each block. The 
accuracy with which data were read out was evaluated based 
on the percentage of data that were correctly read out from 
4096 binary data. 
V. 
RESULTS AND DISCUSSION 
Figure 6 and Table I indicate the accuracy with which the 
binary data were read out. The results reveal that the 
accuracy when Cb was modulated is higher than that when 
Y was modulated for all four images. When Cb was 
modulated, the accuracy for Image A was over 99 % even at 
the HC of one, and it reached 100% when HC was three. It 
was over 99% for HC over two for Images B and C. That 
for Image D was over 99% for HC over four. The accuracies 
for all these four images reached 100% at HCs of 6–14. In 
contrast, the accuracy when Y was modulated was smaller 
by over 10% than that when Cb was modulated for the HC 
of one. They became 100% for HCs over 10. 
 It can be seen from Fig. 6 and Table I that the accuracy 
differs depending on images. Figure 7 indicates where the 
readout errors occurred in each image when color 
differences were modulated and presents the results when 
HC was set to one. The areas in red in the images are blocks 
in which errors had occurred. It can be seen from Fig. 7 that 
errors particularly tended to occur in dark, yellow, and other 
areas that contained fine textures. This is because the 
reflectance there was low and the high frequency 
component became so small that the sign was reversed 
under the influence of a slight amount of noise. It is also 
reasonable for errors to have tended to occur in finely 
textured areas because such areas contained large high 
frequency components and functioned as noise for 
watermarking. The main reason the errors occurred in 
yellow areas is that the Cb component of YCbCr was very 
small for these areas because yellow does not contain blue 
components.  
Figure 8 is a reference that shows blocks where readout 
errors occurred in images when watermarking was 
generated by modulating brightness Y. It can be seen that 
there are many errors that occurred on the pattern edge. Not 
many such errors can be seen when Cb was modulated, as 
indicated in Fig. 7. Blocks marked A to D, which are 
TABLE Ⅰ ACCURACY IN READING OUT BINARY DATA  
Magnitude of HC 
Accuracy in reading out binary data (%) 
Image A 
Image B 
Image C 
Image D 
Modulating 
Cb 
Modulating 
Y 
Cb 
Y 
Cb 
Y 
Cb 
Y 
HC=1 
99.1 
86.8 
95.2 
78.4 
95.9 
91.6 
79.6 
93.6 
2 
99.9 
92.5 
99.1 
87.8 
99.6 
97.9 
87.5 
97.0 
3 
100.0 
95.6 
99.7 
92.5 
99.9 
99.3 
90.7 
98.2 
4 
100.0 
97.5 
99.9 
95.6 
99.9 
99.6 
92.7 
99.0 
5 
100.0 
98.4 
99.9 
97.0 
99.9 
99.7 
94.3 
99.3 
6 
100.0 
98.7 
99.9 
97.9 
100.0 
99.9 
95.2 
99.4 
8 
100.0 
99.3 
100.0 
99.1 
100.0 
99.9 
96.8 
99.8 
10 
100.0 
99.4 
100.0 
99.7 
100.0 
100.0 
97.8 
99.9 
12 
100.0 
99.5 
100.0 
99.9 
100.0 
100.0 
98.6 
99.9 
14 
100.0 
99.6 
100.0 
99.9 
100.0 
100.0 
98.9 
100.0 
16 
100.0 
99.6 
100.0 
99.9 
100.0 
100.0 
99.1 
100.0 
18 
100.0 
99.7 
100.0 
99.9 
100.0 
100.0 
99.2 
100.0 
20 
100.0 
99.8 
100.0 
99.9 
100.0 
100.0 
99.3 
100.0 
 
236
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

surrounded by dashed circles in Fig. 8, are examples on 
pattern edges where errors occurred. The magnitude of the 
highest frequency component, F(7,7), in these blocks in the 
original image that was not modulated are summarized in 
Table II , where it can be seen that the highest frequency 
component for Y is larger than that for Cb. Therefore, when 
watermarking was generated by modulating Y, the influence 
of the high frequency component of the object image was 
more 
strongly 
received 
in 
the 
positive/negative 
determination of the highest frequency component of the 
modulated image. This is the main reason the accuracy 
when Cb was modulated was higher than that when Y was 
modulated. 
It is possible that these two methods can improve the 
accuracy with which the embedded data are read out by 
taking into consideration the results in Fig. 6, Fig. 7, and 
Table I . The first method involves embedding watermarking 
by avoiding error-prone areas, such as dark and yellow areas.  
Figure 7. Blocks where readout error occurred when HC was set to one. 
Red squares indicate blocks where readout error occurred. 
(a) 
Image A 
(b) Image B 
(c) 
Image C 
(d) Image D 
TABLE Ⅱ  EXAMPLES OF VALUE OF HIGHEST FREQUENCY 
COMPONENT OF BLOCK ON PATTERN EDGE 
 
Y 
Cb 
A 
-1.29 
0.41 
B 
0.75 
0.64 
C 
-0.29 
0.08 
D 
0.80 
0.08 
 
237
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

The second entails the use of error correction techniques, 
where many of these are used in the fields of communication. 
We used majority voting as an error correction method in our 
previous study [14], where we also used brightness 
modulation. It embedded the same 1-bit data into three 
blocks that were sufficiently separated from one another and 
when embedded data were read out, we decided whether to 
use majority voting if their readout data differed. Accuracy 
reached 100% even it was less than 90%. This technique of 
modulating color differences can be very accurate by 
combining it with error correction technologies. 
Figure 5 indicates that modulating Cb is superior to 
modulating Y in terms of invisibility. Consequently, the 
technique of modulating Cb is better than that of modulating 
Y for both the readability and invisiblity requirements of 
embedded watermarking. Figure 9 presents the captured 
images simulated by using Eq. (1) when HC was set to five. 
We could not see any watermarking patterns in the images. 
These results indicate that we could satisfy both the 
invisibility and readability requirements of embedded data 
by using appropriate HC ranges. 
Figure 8. Blocks where readout errors occurred when luminance was modulated (HC= 1). 
Red squares indicate blocks where readout errors occurred. 
(a) 
Image A 
(b) Image B 
(c) 
Image C 
(d) Image D 
A 
B 
C 
D 
238
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

VI. 
 CONCLUSION AND FUTURE WORK  
We developed a technique that could embed an invisible 
watermarking pattern into captured images of real objects 
using illumination that contained the pattern. We embedded 
the pattern into the illumination by modulating color 
differences. 
We discovered four main findings through simulation in 
this study. The first was that the accuracy when modulating 
Cb was higher than that when modulating Y. The second was 
that errors in reading out embedded binary data particularly 
tended to occur in dark, yellow, and other areas that 
contained fine textures. The third was that we could satisfy 
both the invisibility and readability requirements of 
embedded data by using appropriate HC ranges. The fourth 
was that we found that this technique of modulating color 
differences could be a very accurate method when combined 
with error correction technology.  
We intend to examine detailed conditions on the 
invisibility of watermarks in the future. Future research will 
also involve quantitatively finding what effects there are 
when error correction is added to the technique proposed in 
this research. Moreover, we will examine the effects of 
image compression on the accuracy with which embedded 
information could be read out because the captured image 
has been handled in compressed form in many cases.  
ACKNOWLEDGMENTS 
This study has been supported by a Japan Society for the 
Promotion of Science (JSPS) Research Institute Grant: No. 
16H02820. 
REFERENCES 
[1] K. Uehira and H. Unno, “Optical Watermark Pattern Technique using 
Color–Difference Modulation,” Proceedings of PATTERNS 2017, 
2017. 
Figure 9.Captured images simulated using Eq. 1 when HC was set to five 
(a) 
Image A 
(b) Image B 
(c) 
Image C 
(d) Image D 
239
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[2] I. J. Cox, J. Kilian, F. T. Leighton, and T. Shamoon, "Secure 
spread spectrum watermarking for multimedia,” IEEE Trans. 
Image Process., Vol. 6, No. 12, pp. 1673–1687, 1997. 
[3] M. D. Swanson, M. Kobayashi, and A. H. Tewfik, 
“Multimedia 
data-embedding 
and 
watermarking 
technologies,” Proc. IEEE, Vol. 86, No. 6, pp. 1064–1087,  
1998. 
[4] M. Hartung and M. Kutter, “Multimedia watermarking 
techniques,” Proc IEEE, Vol. 87, No.7, pp. 1079–1107, 1999. 
[5] G. C. Langelaar, I. Setyawan, and R. L. Lagendij, 
“Watermarking digital image and video data,” IEEE Signal 
Processing Magazine,  Vol. 17, No.5, pp. 20–46, 2000. 
[6] J. Haitsma and T. Kalker, “A Watermarking Scheme for 
Digital Cinema,” ICIP2001, Vol. 2, pp. 487–489, 2001. 
[7] Digital cinema system specification V1.2, Digital Cinema 
Initiatives, Mar. 2008. 
[8] S. Goshi, H. Nakamura, H. Ito, R. Fujii, M. Suzuki, S. Takai, 
and Y. Tani, “A New Watermark Surviving after Re-shooting 
the Images Displayed on a Screen,” KES2005, LNAI3682, pp. 
1099–1107, 2005. 
[9] K. Okihara, Y. Inazumi, and H. Kinoshita, “A Watermark 
Method that Improves the Relationship Between the Number 
of Embedded Bits and Image Degradation,” IEIJ, Vol. 58, No. 
10, pp. 1465–1467, 2004. 
[10] T. Mizumoto and K. Matsui, “Robustness investigation of 
DCT digital watermark for printing and scanning,” Trans. 
IEICE (A), Vol. J85-A, No. 4, pp. 451–459, 2002.  
[11] M. Ejima and A. Miyazaki, “Digital watermark technique for 
hard copy image,” Trans. IEICE (A), Vol. J82-A, No. 7, pp. 
1156–1159, 1999. 
[12] Y. Horiuchi and M. Muneyasu, “Information Embedding to 
the Printing Images Based on DCT,” Proc. of ITC-CSCC2004, 
No. 7F3P50-1-4, 2004. 
[13] Z. Liu, “New trends and challenges in digital watermarking 
technology: Application for printed materials” in Multimedia 
Watermarking Techniques and Applications, B. Furht and D. 
Kirovski, pp. 289–305, Auerbach Publications, 2006 
[14] K. Uehira and M. Suzuki, “Digital watermarking technique 
using brightness-modulated light,” Proceedings of the IEEE 
ICME2008, pp. 257–260, 2008. 
[15] Y. Ishikawa, K. Uehira, and K. Yanaka, “Practical Evaluation 
of Illumination Watermarking Technique Using Orthogonal 
Tranforms,” Journal of Display Technology,  Vol. 6, No. 9, 
pp. 351–358, 2010. 
[16]  M. Komori and K. Uehira, “Optical watermarking 
technology for protecting portrait rights of three-dimensional 
shaped real objects using one-dimensional high-frequency 
patterns,” Journal of Electronic Imaging, Vol. 22, No. 3, pp. 
033004-1 to 033004-7, 2013. 
[17] Y. Goto and O. Uchida, “Recognizable digital watermarking 
for printed materials embedding in hue component,” 
Proceedings of the IIEEJ Image Electronics and Visual 
Computing Workshop 2010, 2P-9, 2010. 
[18] L. Zhou, S. Fukushima, and T. Naemura, “Dynamically 
Reconfigurable Framework for Pixel-level Visible Light 
Communication Projector,” Proc. of SPIE Vol. 8979 89790J-1, 
2014. 
[19] T. Yamada, S. Gohshi, and I. Echizen, “Re-shooting 
prevention based on difference between sensory perceptions 
of humans and devices,” Proc. of the 17th International 
Conference on Image Processing, pp. 993–996,  2010. 
  
240
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

