Indoor Navigation Control System for Visually Impaired People
Mohit Sain, Dan Necsulescu
University of Ottawa
161 Louis Pasteur, Ottawa, Canada
e-mail: msain064@uottawa.ca; dan.necsulescu@uottawa.ca
Abstract — Blindness affects the perception of the surrounding
environmental conditions. The primary requirement of any
visual aid for mobility is obstacle detection. This work
proposes an indoor navigation system for the visually impaired
people. The system presented in this study is a robust,
independent and portable aid to assist the user to navigate with
auditory guidance. Computer-based algorithms developed in C
sharp for the Microsoft Xbox Kinect 360 sensor allows to build
a device for the navigational purpose. Kinect sensor streams
both colour and depth data from the surrounding environment
in real-time, which is then processed to provide the user with
directional
feedback
using
the
wireless
earphones.
The
effectiveness of the system was tested in experiments conducted
with six blindfolded volunteers who successfully navigated
across various indoor locations. Moreover, the user could also
follow a specific individual through the output generated from
the processed images.
Keywords - Indoor Navigation; Vision Assistance; Kinect
Camera; Collision Avoidance.
I.
INTRODUCTION
Visually impaired people often suffer from missing the
perception
of
the
environment,
which
affects
them
physiologically and psychologically. In the last few decades,
many solutions have been proposed and were made available
to the blind users through various means, such as white
canes, laser canes, binaural sensing aids, Braille, and guide
dogs.
The primary requirement is to detect the obstacles and
help avoid a collision while navigating to a goal. To help the
blind people navigate, the need is to find obstacles and
hazards which the regular aids cannot notice. Navigation
systems for blind people have been proposed to increase
their mobility. However, they were only concerned with
guiding the user along a predefined route as in the paper
presented by Loomis et al. [1]. In their study, they evaluated
the guidance performance in the virtual display mode that
allows providing enhanced guidance.
Electronic Travel Aids (ETA) have been considered in
the past, such as by Ram et al. [2] as an assistive device
which transforms the environment surroundings sensing into
signals for the blind. These aids brought a high level of
confidence to help visually impaired people navigate. These
devices helped to detect obstacles in the pathway of the user.
The sensors, software interface, and a feedback mechanism
are the three building blocks for any ETA. The sensors
communicate the data to the system, where it is further
processed using a specialised software and transmit to the
user the required information as real-time feedback so that
there are no hindrances in the user’s way.
Borenstein et al. [3] and Dodds et al. [4] used ultrasonic
sensors to augment the performance of the guidance cane.
These sensors helped the user to detect barriers and steer
accordingly,
which
proved
to
be better
compared
to
traditional cane, given that the guide cane provided the path
efficiently. Benjamin et al. [5] and Yuan et al. [6] used laser
and vision sensors that enhanced user's confidence while
navigating as a reasonable way for providing the information
in real time using a laser triangulation system.
S. T. Brassai
et al. [7] provided an overview of the literature on assistive
technologies for indoor and outdoor navigation in a dynamic
environment. They also provided the list of solutions
available for helping visually impaired people, such as
navigation
system,
obstacle
avoidance,
and
obstacle
localisation.
The indoor auditory navigation system [8] presented by
A. Zeb et al. assists visually impaired people in the
surrounding
environment
using
a
computer
vision
technology which detects the markers. The user navigates
using a webcam attached to the system which further
provides the user with valuable audio assistance whenever it
detects any markers in the environment that enables them to
navigate independently in the environment.
C. K. Lakde et al. [9] reviewed and further improved a
system [10] that guides blind people. This study presents an
idea to make a person aware of the available pathway and the
obstacles. The system proposed by the researchers here
consists of sensors (depth and Red, Green and Blue (RGB)
sensors) embedded
in
shoes,
a control board
and
a
communication system (vibration and voice assistance).
Another system, presented by T. Schwarze et al. [11],
consists of a wearable assistance system for helping visually
impaired
people
using
a
stereo
camera
and
acoustic
feedbacks. They used basic scene understanding, and head
tracking, to make the user aware of the surroundings and
allowing the user to walk in an unfamiliar environment and
to avoid obstacles safely by sonic signals. There have been
numerous solutions, but most of them are limited to specific
situations and only for specific areas.
54
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-621-7
ALLSENSORS 2018 : The Third International Conference on Advances in Sensors, Actuators, Metering and Sensing

II.
METHODOLOGY
A. System Design and Kinect Technology
In this study, a navigation system is proposed based on
computer vision technology. The system is much more than
a sensor itself; it includes three main components each
having a specific functionality. First and the foremost is
Microsoft Xbox 360 Kinect sensor, which is used for
accumulating the environmental information from the data
streams (both depth images and RGB images). The second is
the image processing algorithm programmed using C sharp
language, performed on a laptop, and the third component is
a feedback system for guiding the user using the auditory
information.
As soon as the system runs the algorithm, the Kinect
sensor starts capturing the depth and RGB data from the
video streams within the sensor range. The accumulated data
is then directed to the processing unit, which, in this case, is
a laptop running the Microsoft Visual Studio, which helps
with real-time image processing without any noticeable time
delay and provides the user with useful directional feedback.
To achieve the best out of the Kinect device and to provide
better viewing angles, the device is chest mounted using a
GoPro chest mount [12]. Mounting the device right from the
centre makes it more robust, and stable. The Kinect sensor is
powered by a rechargeable 6000 mAh Li-Ion, 12V DC
portable battery pack, which makes it portable and can run
the system for about 8 to 10 hours.
The primary aim of this study is to provide better vision
system as compared to the conventional low-priced sensors
as those cannot produce the same quality output as compared
to the much more advanced Kinect sensor. The prototype is a
robust system comprising of an RGB camera, infrared sensor
(IR), and an infrared projector. This device provides the user
with
an
improved
understanding
of
the
environment.
Moreover, it is a simple, reliable, and low-cost system which
helps the user to navigate by providing the right amount of
information.
Specific scenarios are tested using the Kinect, to help the
user navigate [12].
Three main scenarios tested in this study are:
1. Navigating indoors, such as in classrooms and
laboratories, guiding the visually impaired person through
obstacles, such as tables, chairs, lab partitions, other
individuals, and cabins.
2.
Detection of the doors of the classrooms and labs by
their names or numbers while in the hallways or corridors
and recognition of the stairway going up or down.
3.
Following a specific person out of a group of three in
the
lobby,
with
audio
guidance
through
Bluetooth
headphones.
For the above-mentioned testing scenarios, the system is
designed for two different modes of guidance to fulfil the
needs of the visually impaired people. First, the Normal
Mode of guidance in which the user can roam independently
indoors with the help of auditory feedback informing about
the obstacles, persons in their way, as well as stairs.
Moreover, if in some cases they do not receive precise
information, they are backed up with Quick Response Code
(QR) placed at various locations in the building. QR codes
can be scanned by the Kinect sensor much faster, and these
codes can store a substantial amount of data. Moreover, the
user can be provided with some additional information such
as stairs going down/up and the number of stairs, the location
of an elevator, and level information. The other mode of
guidance is the Follow Mode guidance in which the visually
impaired
people
can
follow
a
particular
person
for
navigational help without any physical contact, and it would
provide assistance which would not be altered even if anyone
else is in the range of the sensor.
The depth of the obstacles or objects is measured using
the triangulation method. The Prime sense chip sends a
signal to the IR projector to start emitting an invisible
electromagnetic light onto the object or the scene. It also
sends the signal to IR depth sensor to initialise and capture
the depth stream and this information is sent back to the chip
where the frame by frame depth stream is created for the
display.
The IR projector projects the laser pattern which gets
reflected by the object in the sensing range and IR camera
triangulates it for depth map by recapturing the emitted light,
as shown in Figure 1.
Figure 1.
Schematic representation of triangulation method
The distance zk, from the camera to the reference plane
is obtained from:
b d
f
z
z
zk





1


55
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-621-7
ALLSENSORS 2018 : The Third International Conference on Advances in Sensors, Actuators, Metering and Sensing

where d is the depth distance/observed disparity, zo is the
assumed position of the object on the reference plane, f is the
focal length of the Infrared Camera, and b is the baseline
between the IR camera and the IR projector. Equation (1)
denotes the observed disparity where zo, f and b can be
determined by calibration.
B. System Initialization and Implementation
Once the program starts, the very first step is to verify if
the Kinect sensor is connected to the computer or laptop. If
the system does not find any sensor connected, it will return
the program with error and exits. If the Kinect sensor is
connected to the system, the program proceeds normally.
Once the system is connected and can interact with the
Kinect, it will send a signal to both RGB and IR depth sensor
to start the streams for colour and depth frames, Figure 2.
Figure 2.
System data flow
If the Kinect sensor is connected to the system, the
program proceeds normally. Once the system is connected
and can interact with the Kinect, it will send a signal to both
RGB and IR depth sensor to start the streams for colour and
depth frames, Figure 2.
The RGB camera stream thus provides the QR code
scanning for the audio output to the user which is done with
the help of ZXing open source library that helps to scan the
code and read the information stored. The depth camera
stream uses the skeleton tracking and it also helps to provide
information about obstacles. The image processing for the
depth sensor is done using EmguCV, which is an open
source library that helps to find different contours, skipping
objects by different width ranges, can count people in the
sensor range and take all the necessary decisions which help
the user navigate. It also allows the speech synthesis which
further
provides
the
user
with
the
necessary
audio
information. Then, finally, both the streams are displayed on
the laptop screen running at 640 × 480 at FPS 30.
Figure 3.
Free Mode of guidance
Figure 3 shows the data flow process for the free mode of
guidance. Once the user starts navigating using this mode,
the very first thing the program looks is the QR code. In case
no QR code is detected by the colour camera, then the depth
camera will start to look for the surrounding obstacles and
provide the user with audio output necessary for the
navigation. In case, the camera detects the QR code, then, it
will communicate the information stored using the Bluetooth
earphones.
III.
EXPERIMENTAL RESULTS
A total of six blindfolded sighted individuals participated
in this experimental trials performed for both free mode and
Follow Mode guidance. Each performed two trials on the
specified path for the system testing and feedback. The
blinded person starts from a lab on the level 2 on the free
mode as shown in Figure 4 [12]. Then the individual must be
able to avoid all the obstacles making their way out of the lab
using the exit door. QR codes posted on the door gives the
information to the user saying exit. Once the user exit the
lab, they can either take the stairway or the elevators to reach
down to the level 1. QR codes help assist the user with the
number of stairs going up or down. The user then walks
through the hallways which have many classrooms doors.
QR codes posted will assist the user about their final
destination.
In Follow Mode guidance, the user will just keep on
following a particular person in the indoor environment. In
this mode, a few assumptions are made such as no more than
three individuals are walking in the hallway and following
the designated guiding person. Directional feedback guiding
the individual about every movement of the person being
followed helps them to reach their destination. The red box
highlights the person being followed, as shown in Figure 5.
This lets the system guide the user accordingly and, at the
same time, lets the user know about the obstacles.
56
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-621-7
ALLSENSORS 2018 : The Third International Conference on Advances in Sensors, Actuators, Metering and Sensing

For the Follow Mode of guidance, before iterations were
done, it is assumed that there is no interference between the
user and person being followed. In this case, the depth
camera will help to locate the motion of the subject being
followed
using
skeleton
tracking
and
guide
the
user
accordingly ignoring other humans in the surroundings.
After the feedback from the blindfolded test subjects, another
major iteration was done which improved the system.
Figure 4.
Experimental Environment for guidance
In this Follow Mode testing, instead of using the
skeleton tracking, it was decided to follow a person with QR
code at the back, and the sensor will keep on following that
QR even if there is an interference of another human which
would be ignored by the system. This change increased the
capabilities of the proposed system; with further algorithm
changes, it modified the way Follow Mode works as the part
of iteration after the test results. In this iteration, the visually
impaired person follows a QR code which was found to be
more accurate for the Follow Mode of guidance. Results for
this iteration are shown in Figure 5 and Figure 6. Whenever
the sensor detects the QR code is saying ‘Follow’, it will
keep on following that person even if any other person
interferes or passes across.
Figure 5.
Follow Mode using QR code in a lab
Further experimentation with people interfering during the
testing using Follow Mode was found to be better with the
use of QR code, as shown in the Figure 7. The results below
clearly show that the sensor is only tracking and following
the person with the QR code which is highlighted with the
red box and is not tracking the other two humans.
Figure 6.
Follow Mode using QR code in hallway
Another scenario is shown in Figure 8, where there is
another person in between the visually impaired person and
the person being followed. In this case, the sensor highlights
both the humans in the range for the normal Follow Mode,
but with the iteration which was done with the use of QR for
the following mode, now the sensor will guide the user to
follow only the person with the QR code at the back. The
accuracy of the test object through the test course helps to
measure the effectiveness of our device.
Figure 7.
Follow mode with interference
Figure 8.
Follow Mode with another person in between user and the
person being followed
The ability of the device in tracking all kinds of obstacles
and providing auditory outputs was found to be noteworthy
as compared to the previous studies discussed in the
57
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-621-7
ALLSENSORS 2018 : The Third International Conference on Advances in Sensors, Actuators, Metering and Sensing

literature review section. The success of the system is in fact
how it does the obstacle detection, avoidance, scanning QR
codes and tracking humans as well as in providing audio
information for the navigation purposes. The Kinect sensor
helped to develop a novel approach for navigation as
compared to the previously available devices in which there
were more components used but for a specific use.
IV.
CONCLUSIONS AND FUTURE WORK
This paper presented an indoor navigation system using a
Kinect sensor, for the Free Mode and Follow Mode
guidance. The prototype is durable, lightweight, and cost-
effective, and anyone can use it with ease without any
training because of its simple to operate. The real-time image
processing,
done
with
the
help
of
computer
vision
algorithms, helped to detect all kinds of obstacles such as
tables, chairs, humans, walls, doors, and stairs. The QR code
helped to enhance the accuracy of the system and improved
the results. Audio feedback is provided to the user whenever
the system scans any obstacle or a QR code, and in case of
the
Follow
Mode
guidance,
it
provides
directional
information according to the movement of the person being
followed. The system was evaluated by the feedback from
six blindfolded users for both navigation modes. The results
show the effectiveness and efficiency of the system in
helping visually impaired users in the indoor environment,
based on the navigation success of the users through the test
course.
To add to the prototype that can assist the blind or
visually impaired, further iterations and improvements can
be made in the current system. The further scope of
perfection and development can be achieved by designing a
more stable mount for the camera, which can provide better
viewing angles and even help in calibration. The current
Kinect sensor can be replaced by the newer version of Kinect
which has all new technology and better sensor quality
outputs. These changes would increase the scanning range of
the obstacles. There is also a possibility of using multiple
Kinect sensors that might help to provide more independence
while navigating. Further changes in the algorithm can be
made to make it more robust. Radio-frequency identification
(RFID) technology can also be incorporated into the current
system for better results in outdoor areas which will provide
another solution to help to navigate through public places
using RFID tag grids, canes or other various devices.
REFERENCES
[1]
J. M. Loomis, R. G. Golledge, and R. L. Klatzky,
“Navigation system for the blind: Auditory display modes
and guidance,” Presence Teleoperators Virtual Environ.,
vol. 7, no. 2, pp. 193–203, 1998.
[2]
S. Ram and J. Sharf, “The people sensor: a mobility aid for
the
visually
impaired,”
presented
at
the
Wearable
Computers, 1998. Digest of Papers. Second International
Symposium on, 1998, pp. 166–167.
[3]
J. Borenstein and I. Ulrich, “The guidecane-a computerized
travel aid for the active guidance of blind pedestrians,”
presented
at
the
Robotics
and
Automation,
1997.
Proceedings., 1997 IEEE International Conference on,
1997, vol. 2, pp. 1283–1288.
[4]
A. G. Dodds, “The Sonic Pathfinder: An Evaluation.,” J.
Vis. Impair. Blind., vol. 78, no. 5, pp. 203–6, 1984.
[5]
J. Benjamin, N. Ali, and A. Schepis, “A laser cane for the
blind,” presented at the Proceedings of the San Diego
Biomedical Symposium, 1973, vol. 12.
[6]
D.
Yuan
and
R.
Manduchi,
“Dynamic
environment
exploration using a virtual white cane,” presented at the
Computer Vision and Pattern Recognition, 2005. CVPR
2005. IEEE Computer Society Conference on, 2005, vol. 1,
pp. 243–249.
[7]
S. T. Brassai, L. Bako, and L. Losonczi, “Assistive
technologies for visually impaired people,” Acta Univ.
Sapientiae Electr. Mech. Eng., vol. 3, pp. 39–50, 2011.
[8]
A. Zeb, S. Ullah, and I. Rabbi, “Indoor vision-based
auditory assistance for blind people in semi controlled
environments,” presented at the Image Processing Theory,
Tools and Applications (IPTA), 2014 4th International
Conference on, 2014, pp. 1–6.
[9]
C. K. Lakde and P. S. Prasad, “Review paper on navigation
system for visually impaired people,” Int. J. Adv. Res.
Comput. Commun. Eng., vol. 4, no. 1, pp. 2278–1021,
2015.
[10]
C. K. Lakde and P. S. Prasad, “Navigation system for
visually impaired people,” presented at the Computation of
Power,
Energy
Information
and
Commuincation
(ICCPEIC), 2015 International Conference on, 2015, pp.
0093–0098.
[11]
T. Schwarze, M. Lauer, M. Schwaab, M. Romanovas, S.
Böhm, and T. Jürgensohn, “A camera-based mobility aid
for visually impaired people,” KI-Künstl. Intell., vol. 30, no.
1, pp. 29–36, 2016.
[12]
M. Sain, “Portable Monitoring and Navigation Control
System
for
Helping
Visually
Impaired
People.
Diss.
Université
d’Ottawa/University
of
Ottawa,
2017."
https://ruor.uottawa.ca/handle/10393/36869.
58
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-621-7
ALLSENSORS 2018 : The Third International Conference on Advances in Sensors, Actuators, Metering and Sensing

