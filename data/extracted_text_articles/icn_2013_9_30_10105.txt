A Reliability and Survivability Analysis of Local Telecommunication  
Switches Suffering Frequent Outages 
 
Andrew P. Snow1 
School of Information & Telecommunication Systems1 
Ohio University 
Athens, Ohio, USA 
e-mail: asnow@ohio.edu 
Julio Arauz1, Gary Weckman2, Aimee Shyirambere1 
Department of Industrial & Systems Engineering2  
Ohio University 
Athens, Ohio, USA 
e-mail: arauz@ohio.edu, weckmang@ohio.edu
 
 
Abstract— This paper presents a reliability analysis of local 
telecommunication switches experiencing frequent outages in 
the United States, based upon empirical data. Almost 13,000 
switch outages are examined and over 2,500 are found to 
originate with just 156 switches experiencing eight or more 
outages each over a 14-year period. Telecommunication switch 
outage statistics are analyzed for this multiyear period, 
allowing examination into switch failure frequency, causes, 
trends, and impacts. Failure categories are created by reported 
outage cause codes, including human error, design error, 
hardware failure, and external factor causality categories. 
Principal findings are that there are significant differences in 
the switch and outage characteristics for switches experiencing 
more frequent outages/failures. Additionally, time series 
analysis 
indicates 
significant 
reliability/survivability 
deterioration in switches experiencing more frequent outages.  
Keywords- telecommunication; reliability; local switches; 
mobile switching centers; public switched telephone network; 
wireless systems. 
I. 
 INTRODUCTION  
Historically, the Public Switched Telephone Network 
(PSTN) in the U.S. has been used predominantly for landline 
voice services. However, with the exponential growth of 
mobile voice services, the PSTN has been integrated with 
wireless systems. In fact, for calls outside of regional areas, 
wireless serves as radio interface technology, taking place of 
local loop connectivity. A call over hundreds or thousands of 
miles travels a very small percentage of the total distance 
over wireless infrastructure, as the wireless system connects 
to the PSTN for long haul transport. Both the PSTN and 
wireless systems use circuit switches manufactured by the 
same equipment suppliers. In fact, the switches are very 
similar.  As such, we expect local PSTN telecommunication 
systems to be very reliable and survivable, as they are the 
access nodes to transport services in voice networks, for both 
landline and wireless calls. As there are many thousands of 
these switches in the PSTN, monitoring and improving local 
switch reliability is of great importance. Also, wireline 
switch outage characteristics serve as a good proxy for 
wireless mobile switching centers. 
Continuous improvement of any communications device, 
such 
as 
local 
telecommunication 
switches, 
requires 
documenting today’s performance, and measuring against 
that baseline. It is important to know reliability trends, not 
merely to predict, but to influence the future in a proactive 
way. The key to managing highly reliable systems is the 
recognition of an important precept – a reliability trend does 
not have to be accepted and actions may in fact be taken to 
alter the trend. However, all failures cannot be prevented, as 
products are put into environments with hazards of all types. 
But understanding failure modes and how to avoid certain 
failures is important. Additionally, management must 
endeavor to decrease the chance of human induced errors 
throughout he lifecycle. This can be done by training, tools, 
and other support, but management must make reliability a 
priority and fund reliability programs, effectively managing 
reliability engineering [1]. In order to change a trend, we 
look for approaches that will offer insights into why failures 
are occurring. Telecommunication switch reliability is 
determined by the complex interaction between software, 
hardware, operators, traffic load, and a variety of 
environmental factors. By knowing failure causes, designers 
(switch vendors) and operators of telecommunication 
switches (service providers) may take corrective action to 
alter future trends.  Likewise, Barnard argues that modern 
reliability engineering must embrace the principal of 
continually improving products throughout the lifecycle, to 
include 
FRACAS 
(Failure 
Reporting, 
Analysis 
and 
Corrective Action System) before and during the operational 
phase of products [2]. 
 
There has been a paucity of published empirical research 
concerning the reliability of operational telecommunication 
switches in the US. Snow investigated local switch outage 
from 1992 through 1995, and documented reliability growth 
[3]. Later, Snow also noticed that some switches failed many 
times, while others failed infrequently [4]. This paper 
extends that early research over the years 1996 through 2009. 
II. 
RESEARCH QUESTIONS 
This Research will address the following questions 
regarding telecommunication local switch reliability and 
survivability over a 14 year period: 
1. Are the characteristics of switches failing more 
frequently different from those that are not? 
a. Switch size (lines) 
b. Rural or Urban location 
2. Are the event characteristics for switches failing more 
frequently different from those that are not? 
a. Causes 
b. Duration of outages and impact 
d. Time of day (TOD), Day of Week (DOW), and 
Month of Year (MOY) 
209
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

3. Are the failure trends for switches experiencing 
failures more frequently different from those that are 
not? Has there been reliability/survivability: 
a. Growth, 
b. Constancy, or 
c. Deterioration? 
 
Reliability is the probability a system will perform its 
intended function, in the intended environment and at a 
particular level of performance. Thresholds are very 
commonly used to declare a system as in either an 
“operational” or “degraded” mode [5]. Others define 
reliability as “conformance to specifications over time”[6]. 
If the system is in a degraded mode, there is a failure 
event.  Survivability is “The capability of a system to fulfill 
its mission, in a timely manner, in the presence of attacks, 
failures, or accidents.”  and is also a resiliency characteristic 
[7]. Outage frequency and impact, resulting from failures and 
accidents, are survivability measures. Therefore, for this 
paper we will principally analyze failure events to assess 
reliability, and scheduled/unscheduled outages to assess 
survivability. 
The 
data 
consists 
of 
local 
switches 
experiencing outages 2 minutes or more in duration. If a 
switch experiences a failure, it results in an outage, which 
has an impact until the failure is mitigated and service 
restored. 
III. 
CONTEXT AND IMPORTANCE 
The PSTN is a complex, distributed system, and its 
functions are executed by the close cooperation between 
switching, signaling and transmission entities.  These entities 
cooperate in order to provide circuit switching, or the 
establishment, maintenance and termination of temporary 
end-to-end connections between subscribers through a 
network, as shown in Figure 1. The switching entities are 
responsible for concatenating individual transmission links 
into an end-to-end circuit, while the signaling entities 
coordinate the establishment, maintenance and termination 
of the end-to-end circuit. Lastly, transmission entities 
provide links between switches. Local switches are defined 
as those having local loop access lines, including standalone, 
host, or remote local switches. Tandem switches that also 
have access lines, or access tandems, are also included in this 
study, but represent a small number of the total population. 
Tandem switch outages are beyond the scope of this 
research. 
 
 
Figure 1.  PSTN Infrastructure 
 
Wireless voice infrastructure also includes switches, as 
seen in Figure 2. These switches, called Mobile Switching 
Centers (MSC) or Mobile Switching Telephone Office 
(MTSO) , are very similar to the wire line switches studied in 
this research: 
“The mobile telephone office (MTSO) is the switch 
that serves a cellular system. It is similar in function 
to a class 5 end office switch….Prominent makers 
of MTSOs include Northern Telcom, Ericsson, 
Motorola, DSC, and Lucent Technologies”, the 
same manufacturers of local telecommunication 
switches.”[8] 
MSCs switch mobile calls in the wireless coverage area, and 
also interface to the PSTN if the call goes outside the 
wireless area being served. 
BSC
BSC
BSC
BSC
MSC
HLR
VLR
BSC
BSC
BSC
MSC
PSTN
SWITCH
SS7
STP
BS
BS
BS
BS
BS
BS
 
Figure 2.  Wireless Infrastructure 
 
 
 
 
 
 
210
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

In this work, the PSTN is viewed as a single system, 
made up of switching, signaling and transmission segments. 
The switching segment is made up of the tandem and local 
switch subsystems. The purpose of this paper is to 
investigate the reliability of the local exchange switching 
subsystem as a whole, by investigating the pooled failures of 
all individual local switches in the PSTN.   There are a large 
number of different manufacturers and models of local 
switches in this infrastructure. Even the same model switch 
varies substantially from serial to serial because of 
differences in customers served and features offered.  By 
pooling failures from different switches, we may assess the 
reliability of local switching as a whole, rather than the 
reliability of a single switch. 
IV. 
EMPIRICAL DATA 
Individual switch outage incidents of at least two minutes 
in 
duration 
have 
been 
reported 
to 
the 
Federal 
Communications Commission (FCC) by all price cap 
regulated local exchange carriers, accounting for over 90% 
of the wire line telephone access lines in the U.S.  This data 
is part of the quality-of-service statistics required by the 
FCC’s Automated Reporting and Management Information 
System (ARMIS) reports made to the FCC by the carriers.  
For each reported switch incident the date, time, duration, 
and outage cause are included, along with the number of 
access lines connected to the switch experiencing the outage. 
Very importantly, the reporting Carrier classifies each 
incident using one of fifteen different cause codes. This 
research presents a comprehensive reliability analysis of 
LEC local telecommunication switches in the United States 
using this public data over an extended period, January 1996 
through December 2009 [9]. 
Local switches are repairable systems in that they are 
repaired by means other than replacing the entire unit. 
Renewal processes are often used for modeling such 
phenonoma, as it is hoped the system is made “good as new” 
through modular replacement. However, switches involve 
software, wherein some repairs result in a slightly different 
switch as the software changes. This means failures are not 
independent or identically distributed. This is the case of the 
non-homogeneous Poisson process (NHPP), the most 
common model used for repairable systems. For such 
systems the failure rate changes over time, and is 
nonstationary [10].The two-minute reporting threshold is 
recognized as a reliability threshold in this study. An outage 
is different from a failure event, as the outage also has a 
duration (how long the switch failed) and a size (how many 
subscriber lines connect to the switch). An impact metric, 
called “Lost Line Hours” or LLH is a survivability metric, 
and is used here to assess outage and reliability deficit 
impact. If a 10,000 line switch experiences a 2 hour outage, 
that is equivalent to a 20,000 line switch down for 1 hour, or 
20,000 LLH. The results presented below make an important 
distinction between an outage and a failure event. Lastly, 
availability is another important aspect of switch quality-of-
service too, but not in the scope of this study. 
V. 
SUMMARY ANALYSIS 
As mentioned, the reporting carrier attributes an 
individual switch outage incident to one of fifteen different 
cause codes, as required by the ARMIS reporting 
instructions. It is important to note that only total switch 
outages are reported. A partially failed switch is not a 
reportable outage, irrespective of the size of the partial 
switch outage. Neither are outages less than two minutes. An 
abbreviated definition for each cause code and the number of 
outages reported for each category is shown in Table 1. From 
here on, a distinction is made between a failure and an 
outage. Cause code one is recognized as a planned 
maintenance outage, while cause codes two through fifteen 
are treated as failures resulting in outages.  From Table 1, 
note that the largest cause of outages was scheduled outages 
(about 30%) while the next largest was random hardware 
failure (about 23%), followed by roughly equal percentages 
of 8% for software design and acts of god.  
A. Causal Analysis 
Another way to summarize the failure data is to combine 
some of the codes into categories that might offer more 
insight into the reliability performance of local switches. The 
following categories are created by combining cause codes: 
• 
Human error:  Procedural errors made in installation, 
maintenance or other activities by Telco employees, 
contractors, switch vendors, or other vendors.  
• 
Design error:  Software or hardware design errors 
made by the switch vendor prior to installation. 
• 
Hardware error: A random hardware failure, which 
causes the switch to fail.   
• 
External circumstances: An event not directly 
associated with the switch, which causes it to fail or 
be isolated from the PSTN. 
• 
 Other/unknown: A failure for which the cause was 
not ascertained by the carrier. 
These categories, their composition, and the distribution 
of failures to each category are shown in Table 2, where 
scheduled outages are left out.  Note that the largest 
categories causing failures in about equal proportions are 
hardware failure and external causes. The next largest is 
procedural error and design error, each with about half the 
failures as either hardware failures or external causes.  
B. Time Series Analysis of All Switch Outages 
A time series analysis of outages is shown in Figure 3. 
From this figure there appears to be a period of reliability 
growth followed by a period of reliability deterioration. 
However, during this study period, the number of local 
exchange switches decreased somewhat, as shown in Figure 
4. From these results a time series of outage rate can be 
determined, as seen in Figure 5 (dividing the outage count 
per year by the number of switches per year). Here it is seen 
that the initial reliability growth is not as pronounced, and 
that the reliability deterioration is slightly more pronounced 
than that indicated by Figure 3. 
 
211
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

TABLE I.  
LOCAL SWITCH OUTAGE AND OUTAGE CAUSE 
DISTRIBUTION 
Code 
Description 
Number 
% 
1 
Scheduled 
3,885 
30.2% 
2 
Procedural error (Telco 
install./maintenance) 
 446 
3.5% 
3 
Procedural error (Telco 
non-install./maintain.) 
376 
2.9% 
4 
Procedural error (System 
vendor procedural error) 
315 
2.4% 
5 
Procedural error (Other 
vendor procedural error) 
257 
2.0% 
6 
Software design 
1,078 
8.4% 
7 
Hardware design 
136 
1.1% 
8 
Hardware failure 
2,951 
22.9% 
9 
Acts of god 
935 
7.3% 
10 
Traffic Overload 
17 
0.1% 
11 
Environmental 
83 
0.6% 
12 
External power failure 
896 
7.0% 
13 
Massive line outage, cable 
cut, other 
660 
5.1% 
14 
Remote - loss of facilities 
between host/remote 
309 
2.4% 
15 
Other/unknown 
516 
4.0% 
 
Total 
12,860 
100% 
TABLE II.  
LOCAL SWITCH FAILURE CAUSE CATEGORY DISTRIBUTION 
Codes 
Failure Category 
Numb. 
% 
2,3,4,5 
Human Proc. Error 
1,394 
15.5% 
6,7 
Design Error 
1,214 
13.5% 
8 
Hardware Failure 
2,951 
32.9% 
9 thru 14 
External Circumstances 
2,900 
32.3% 
15 
Other/unknown 
516 
5.7% 
2 thru 15 
Total 
8,975 
100% 
VI. 
SWITCHES WITH MORE FREQUENT 
OUTAGES/FAILURES 
Do some switches experience more outages than others? 
The logarithmic plot in Figure 6 indicates this is in fact the 
case. Here we see that 156 unique switches experienced 8 or 
more outages during the study period, while 5,976 switches 
experienced 7 or less outages. The selection of 8 or more 
outages is somewhat arbitrary, but partly selected because 
the data points at 8 or more outages deviate from the smooth 
curve formed by the data points for 7 or less outages per 
switch. 
 
 
 
 
 
 
0
200
400
600
800
1000
1200
1400
1600
1800
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Number of Outages
Year
 
Figure 3. Time Series of Switch Outages Over 
the Study Period 
 
0
2,000
4,000
6,000
8,000
10,000
12,000
14,000
16,000
18,000
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Number of Switches
Year
 
Figure 4. U.S. Local Switches  Over the Study Period 
 
0.000
0.020
0.040
0.060
0.080
0.100
0.120
0.140
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Outage Rate
Year
 
Figure 5. Switch Outage Rate Over the Study Period 
212
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

Total
≤ 7 Outages
≥ 8 Outages
Unique Switches
6,132
5,976
156
Outages
12,860
10,237
2,623
156 Unique Switches with ≥ 8 Outages Each
1
10
100
1000
10000
1
3
5
7
9 11 13 15 17 20 22 25 27 29 31 36 43 48 71 92
Number of Outages
 
Figure 6. Number of Unique Switches One or More Outages 
 
 
A. Causal Comparison of Switches with More Frequent 
Outages/Failures 
The percentages of outages by cause code and category 
are shown in Tables 3 and 4, respectively.  For a cause code 
comparison, note the following regarding switches with 
more failures compared to switches with less failures: 
• 
One-half the percent of scheduled outages 
• 
Double the percent for  
1. Acts of god, 
2. Line outage, 
3. Loss of connection to host switch, and  
4. Other/unknown 
For major cause categories, note the following regarding 
switches with more failures compared to switches with less 
failures: 
• 
One-third the human error 
• 
Double external circumstances 
 
 
B. Summary Analysis of Switches with Frequent 
Outages/Failures 
A summary comparison of switches is shown in Table 5. 
First note that although less frequently failing switches 
account for but 3% of unique failed switches, they represent 
25% of the switch failures. Also note that the less frequent 
failing total switch lines represent 10% of the total, they 
represent 22% of the total  duration. On a better note, the 
more frequently failing switches represent only 7% of the 
survivability deficit due to the outages induced by these 
failures (lost line hours, or LLH). Note that the more 
frequently failed switches are about one-third the size of the 
less frequently failing switches. However, note little 
differences in the average duration of outages (4.61 versus 
4.18 hours). Also note that the average LLH for the more 
frequently failed switches is about one-fourth of the less 
frequently failed switches. Also note that the median location 
for all failed switches is rural rather than urban (MSA stands 
for Metropolitan Statistical Area). 
 
Lastly, refer to Tables 5 and 6 for the following temporal 
comparisons: 
• 
Time of Day (TOD), where the day was divided 
into 6 timeslots 
• 
Day of Week (DOW), where 1 is Monday 
• 
Month of Year (MOY). Where 1 is January 
Note that although the differences in average TOD and 
DOW week shown in Table 5 are small, Table 6 results 
indicates statistically significant differences. No differences 
are indicated for month of year. 
TABLE III.  
COMPARISON OF CAUSE CODE DISTRIBUTION 
Cause Code 
≥ 8 Outages 
≤ 7 Outages 
1 
15.7% 
33.9% 
2 
1.9% 
3.9% 
3 
0.5% 
3.6% 
4 
0.5% 
2.9% 
5 
1.1% 
2.2% 
6 
10.8% 
7.8% 
7 
0.7% 
1.1% 
8 
27.4% 
21.8% 
9 
14.0% 
5.5% 
10 
0.2% 
0.1% 
11 
0.5% 
0.7% 
12 
8.5% 
6.6% 
13 
8.2% 
4.3% 
14 
4.0% 
2.0% 
15 
5.9% 
3.5% 
Total 
100.0% 
100.0% 
 
TABLE IV.  
COMPARISON OF CAUSE CATEGORY DISTRIBUTION 
Cause Category 
≥ 8 Outages 
≤ 7 Outages 
Scheduled (1) 
15.7% 
33.9% 
Human Proced. 
Error (2-5) 
4.0% 
12.6% 
Design Error  
(6-7) 
11.6% 
8.9% 
Hardware  
(8) 
27.4% 
21.8% 
External 
Circumst. (9-14) 
35.4% 
19.3% 
Other/Unknown 
(15) 
5.9% 
3.5% 
Total 
100.0% 
100.0% 
213
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

TABLE V.  
RELIABILITY AND SURVIVABILITY COMPARISON 
Cause 
Codes 
2-15 
Outages 
≥8  
Outages 
≤7  
Outages 
% 
 ≥8 
%  
≤7 
Number  
Outages 
8,975 
2,210 
6,765 
25% 
75% 
No. of 
Switches 
4,517 
154 
4,363 
3% 
97% 
Total   
Lines 
65.6 M 
6.6 M 
59.0 M 
10% 
90% 
Total Dur. 
(Hrs) 
41.3 K 
9.2 K 
32.1 K 
22% 
78% 
Total 
LLH 
307.8 M 
20.5 M 
287.3 M 
7% 
93% 
Avg Sw. 
Lines 
7,313 
3,000 
8,723 
 
 
Avg Dur.  
(Hours) 
4.61 
4.18 
4.75 
 
 
Average 
LLH 
34,295 
9,257 
42,475 
 
 
Median 
TOD 
10:57 
AM 
12:00  
PM 
10:34 
AM 
 
 
Mean 
TOD 
11:02 
AM 
11:54 
AM 
10:45 
AM 
 
 
Median 
DOW 
4.09 
4.29 
4.06 
 
 
Mean 
DOW 
4.22 
4.32 
4.19 
 
 
Median 
MOY 
6.89 
7.08 
6.82 
 
 
Mean 
MOY 
6.87 
6.90 
6.86 
 
 
Median 
MSA  
Rural 
Rural 
Rural 
 
 
 
 
VII. TIME SERIES ANALYSIS OF SWITCHES WITH 
FREQUENT OUTAGES 
Here the outage data is investigated for trends and arrival 
process assessment. The perspective is that the PSTN is 
viewed as a single repairable system, and that we are 
investigating the local switch subsystem as a whole. The first 
method in assessing a trend is visual, using the cumulative 
failures versus time plot. A linear plot means constant arrival 
process. This is a homogeneous-Poisson-process (HPP) if the 
time-to-failures are i.i.d. and exponentially distributed. If the 
events are i.i.d. and the distribution is other than exponential, 
then we may classify the process as renewal [10]. However, 
if the cumulative plot bends downward or upward, the 
failures are not from a common distribution, and we have 
either 
reliability 
growth 
or 
reliability 
deterioration, 
respectively. In this instance, the most common classification 
is expected to be the nonhomogeneous Poisson process 
(NHPP), where subsequent failures come from a different 
distribution [10]. This should be expected, as switches 
commonly receive new software versions and feature 
upgrades. 
The Cox-Lewis trend test (Laplace test) can be used to 
tease out whether subtle upward or downward bending of 
cumulative outage/failure plots are statistically significant 
cases of reliability deterioration or growth, respectively.  The 
Laplace test looks for trends where the homogeneous 
Poisson process (HPP) is the null hypotheses. The resulting 
test statistic rapidly converges to a normal score with very 
few data points [10]. However, the sample results presented 
here are visually convincing, with no need for formal trend 
testing to detect periods of reliability growth, constancy, and 
deterioration. 
 
TABLE VI.  
TEMPORAL COMPARISON FOR OUTAGES AND FAILURES 
T-TEST 
(Results 
Summary) 
≥ 8 
Outages 
≤ 7  
Outages 
Result 
TOD (All 
Cause Codes) 
11:41:23 
AM 
10:43:48 
AM 
Difference 
TOD (Cause 
Code 1) 
10:32:08 
AM 
10:39:36 
AM 
No 
Difference 
TOD (Cause 
Codes 2-15) 
11:54:19 
AM 
10:45:57 
AM 
Difference 
DOW (All 
Cause Codes) 
4.37 
4.25 
Difference 
DOW (Cause 
Code 1) 
4.68 
4.36 
Difference 
DOW (Cause 
Codes 2-15) 
4.32 
4.19 
Difference 
MOY (All 
Cause Codes) 
6.90 
6.93 
No 
Difference 
MOY (Cause 
Code 1) 
6.89 
7.06 
No 
Difference 
MOY (Cause 
Codes 2-15) 
6.90 
6.86 
No 
Difference 
 
 
Reliability growth, constancy, and deterioration can be 
examined through cumulative outage plots. A cumulative 
plot of all outages reported during the study period is seen on 
Figure 7. Note three general regions of the curve : 
• 
Region I: Reliability constancy – outage rate 
constant for years 1 to 4 
• 
Region II: Slight reliability growth for years 4 to 11 
• 
Region III: Slight reliability deterioration for years 
11 to 14 
Region I constant outage rate is about 1500 outages per year, 
indicating process stability. For Region II, the outage rate is 
demonstrably lower than Region I, and slowly decreases 
nonlinearly over years 4 to 12. The average outage rate in 
Region II is about  540 per year. Reliability growth is 
indicated from Region I to II. However, the reliability starts 
to deteriorate again in Region III, albeit not as bad as Region 
I reliability, with an average rate of about 1000 per year. 
Overall however, the Laplace trend test indicates very strong 
evidence of reliability growth over the entire 14 year period. 
 
 
 
214
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
11000
12000
13000
14000
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Cumulative Number of Outages
Years
All Outages
 
Figure 7. Cumulative Outage Plot: All Outages 
 
To compare switches with more frequent outages to those 
with less, refer to Figures 8 and 9. The switches with less 
outages exhibit a very strong reliability growth, with 
improvement starting about year 4. If we linearize into two 
regions, we see very significant reliability growth about 1400 
outages per year (years 1 to 4 years) to about 460 per year 
(years 4 to 14). There is a slight tailing up of outages in year 
14. However, for the switches with more frequent outages, 
from Figure 9 we see that there are three well defined 
regions of constancy, reliability growth, followed by a very 
strong region of reliability deterioration.  
 
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
11000
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Cumulative Number  of Outages
Years
≤ 7 Outages ‐ All Cause Codes 
 
Figure 8. Cumulative Outage Plot: All Outages 
(Seven or Less per Switch) 
 
Further insights into the switches with more frequent 
outages are shown in Figures 10 and 11. First, note very 
significant decreases in scheduled outages (Figure 10), and a 
very rapid increase in outages due to unplanned failures 
during the last three years of the study period (Figure 11). In 
Figure 11 the failure rate the first 11 years is about 55 per 
year, while for the last three years it is about 530. This is a 
tenfold increase and represents very severe reliability 
deterioration. 
0
200
400
600
800
1000
1200
1400
1600
1800
2000
2200
2400
2600
2800
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Cumulative Number of Outages
Years
≥ 8 Outages ‐ All Cause Codes
 
Figure 9. Cumulative Outage Plot: Outages 
(Eight or More per Switch) 
 
0
50
100
150
200
250
300
350
400
450
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Cumulative Number of Outages
Years
≥ 8 Outages ‐ Cause Code 1 
 
Figure 10. Cumulative Outage Plot: Scheduled Outages 
(Eight or More per Switch) 
 
0
200
400
600
800
1000
1200
1400
1600
1800
2000
2200
2400
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Cumulative Number of Outages
Years
≥ 8 Outages ‐ Cause Codes 2‐15
 
Figure 11. Cumulative Outage Plot:  
Outages Due to Failures 
(Eight or More per Switch) 
 
215
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

VIII. SUMMARY OF FINDINGS AND CONCLUSIONS  
Over the study period there was significant reliability 
growth for all local switches. However, the outage rate 
(number of switches with outages divided by number of 
switches) accelerated over the last four years of the study 
period. 
A summary of the more interesting and significant 
findings for switches that experience more frequent outages 
are: 
• 
There is very severe reliability deterioration in 
switches that go down more frequently while there is 
good reliability growth for switches that fail less 
frequently.  
• 
Switches that go down more frequently are decidedly 
smaller, more rural, and receive much less scheduled 
maintenance than those failing less frequently.  
• 
Human error induced failures are much less of a 
problem for the more frequently out switches. 
Perhaps this is due to their rural nature, where there 
are less visits by technicians. Or perhaps the rural 
switches are hosted to larger switches more often, 
and require less visits by technicians. 
• 
Scheduled outages occur less frequently for the more 
frequently out switches, perhaps indicating less 
frequency preventive maintenance. On the other 
hand, this could be an artifact of smaller switches 
hosted to large switches.  
• 
Acts of god, massive line outage, loss of connection 
to host switch, and other/unknown causes are much 
more of a problem for switches with more frequent 
outages. This could be suggestive of weaknesses in 
host-remote switch architectures and/or susceptibility 
of physical plant to natural disasters.  
• 
There are significant differences in the times-of-day 
and days-of-week between the more and less 
frequently out switches. This suggests a different 
maintenance/disaster-recovery approaches for large 
vs. small and/or rural vs. urban switches. 
• 
The more frequently out switches represent 2.5% of 
the switches with outages, but account for 7% of the 
lost line hours. This means that frequently out 
switches are about three times less survivable than 
switches that are out less frequently. 
In conclusion, there are demonstrable differences in (1) 
the causality of outages and (2) the characteristics of 
switches suffering outages, and (3) switch resiliency when it 
comes to switches out more and less frequently. Also, there 
is a slight uptrend in outages in the last several years of the 
14 year study period. Unfortunately, the FCC stopped 
collecting this data from carriers in 2009, masking the trends 
since then and in the future.  This research demonstrates that 
very pronounced reliability and survivability trends are 
identifiable, some of which are troublesome. This is a good 
example of retrospective quantitative research analysis, 
yielding important trends that can be investigated further and 
corrective action taken. 
 
 
REFERENCES 
[1] O’Conner, P.D.T., Practical Reliability Engineering, 4th 
edition, John Wiley & Sons, England, 2001. 
[2] R.W.A. Barnard, “Reliability Engineering : Futility and 
Error”, Second Annual Chapter Conference, South African 
Chapter, International Council on Systems Engineering 
(INCOSE), 31 August, 1 September 2004. 
[3] Snow, Andrew P., “The Reliability of Telecommunication 
Switches, 
Six 
International 
Conference 
on 
Telecommunications Systems: Modeling and Analysis 
(March 1997): 288-295. 
[4] Snow, Andrew P., “Internet Implications of Telephone 
Access”, IEEE Computer, Volume 32, Number 9, (September 
1999): 108-110. 
[5] Leemis, L., Reliability: Probabilistic Models and Statistical 
Methods, Prentice-Hall, Englewood Cliffs, NJ (1995). 
[6] Levin, M.A. and Kalal, T.T., Improving Product Reliability, 
Strategies and Implementation. John Wiley & Sons, England, 
2003. 
[7] R. J. Ellison, D. A. Fisher, R. C. Linger, H. F. Lipson, T. 
Longstaff, N. R. Mead, Survivable Network Systems: An 
Emerging Discipline, Carnegie-Mellon Software Engineering 
Institute Technical Report CMU/SEI-97-TR-013, 1997 
revised 1999. 
[8] 
Beddel, Paul, Cellular/PCS Management, McGraw-Hill, ISBN 
0071346457, 1999. 
[9] FCC Report 43-05, ARMIS Service Quality Report Table Iva, 
downloaded 
from 
http://transition.fcc.gov/wcb/armis/ 
September 2012. 
[10] Louit, D.M., Pascual, R., and Jardine, A.K.S, “A procatical 
procedure for the selection of time-to-failure models based on 
the assessment of trends in maintenance data”, Reliability 
Engineering and System Safety 94 (2009) 1618-1628. 
 
216
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-245-5
ICN 2013 : The Twelfth International Conference on Networks

