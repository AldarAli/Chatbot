The Changing Nature of Childhood Environments 
Investigating Children’s Interactions with Digital Voice Assistants in Light of a New Paradigm 
 
Janik Festerling 
Department of Education 
University of Oxford 
Oxford, UK 
e-mail: janik.festerling@education.ox.ac.uk 
 
 
Abstract — Based on the theoretical framework of the New 
Ontological Category Hypothesis (NOCH), this piece of doctoral 
research (work in progress) investigates the nature of children’s 
interactions with commercial Digital Voice Assistants (DVAs), 
such as Alexa, Google Assistant, or Siri. In a nutshell, NOCH 
challenges the notion of anthropomorphism and argues that 
intelligently behaving machines, such as voice assistants, could 
become ontological categories in their own right within 
children’s 
emerging 
understanding 
of 
the 
world. 
A 
methodological strategy is briefly outlined in order to explore 
NOCH with respect to children’s relative self-disclosure, that is, 
how children self-disclose personal insights when they interact 
with DVAs, on the one hand, and humans, on the other hand. 
Keywords – voice assistants; children-machine interaction; 
anthropomorphism; cognitive development; mixed methods. 
I. 
INTRODUCTION 
The spread of commercial Digital Voice Assistants 
(DVAs), such as Apple’s Siri, Amazon’s Alexa, or the Google 
Assistant, has gained extreme momentum within a few years, 
not only in terms of total numbers (i.e. number of households 
using DVAs), but also regarding individual usage intensities 
(i.e. number of DVA-devices per household), or third party 
developers that enter the DVA-market [1][2]. Although 
today’s DVAs are neither the only nor the most sophisticated 
manifestations of Artificial Intelligence (AI) in everyday life, 
these automated voice interfaces still remain one of the most 
tangible and recognizable embodiments of humanoid 
artificiality, and they are often present within the most 
intimate spaces of our home environments. 
Although empirical insights regarding the nature of child-
DVA interactions remain limited up to this point, a popular 
implicit or explicit theme in the growing body of preliminary 
research as well as journalistic commentaries is the 
anthropomorphism paradigm [3]–[8], which assumes that, as 
part of our human nature, we are sometimes inclined to treat 
and perceive non-human entities through a humanoid lens, 
either consciously or sub-consciously [9]–[12]. In the context 
of child-DVA interactions, this notion often translates into the 
general idea – not to say fear – that these machines could 
become children’s ‘imaginary’ friends (e.g. [13]). But the 
question remains: How much imagination is required when 
you talk to DVAs? The short answer is none, because the fact 
that you talk to Alexa et al. is as real as the fact that an actual 
humanoid voice responds to your request. In addition, 
claiming that children’s interactions with DVAs, in particular, 
and humanoid AI, in general, can be conceptually reduced to 
anthropomorphic behaviours and imaginative perceptions 
also means to ignore that children’s cognitive development 
might give rise to unprecedented forms of understanding and 
perception when it comes to the human-machine interactions. 
Hence, this piece of doctoral research, as it is outlined in 
this paper, argues that the anthropomorphism paradigm is not 
sufficient to grasp the evolving interactive relationship 
between children and DVAs on scientific grounds. Instead, 
the New Ontological Category Hypothesis (NOCH) by [14] is 
proposed as an alternative and exemplified in the 
contemporary context of DVAs. Lastly, a methodological 
strategy for its empirical investigation is briefly outlined. 
This paper is organised as follows: Section II briefly 
introduces the anthropomorphism paradigm and explains its 
theoretical implications. Section III raises major criticisms 
and shortages of the anthropomorphism paradigm in the 
context of child-DVA interactions. Section IV proposes and 
explains NOCH as an alternative paradigm. Section V 
concludes the discussion before the next steps for future 
research are outlined in the last section, Section VI, of this 
paper. 
II. 
ANTHROPOMORPHISM: AN OVERVIEW 
A prevailing paradigm in human-machine interaction and 
related disciplines is anthropomorphism, arguing that our 
behaviours and perceptions that characterise interactions with 
other humans can also be present when we interact with non-
human entities, such as machines. This section briefly 
summarises anthropomorphism’s theoretical substructure and 
implications, before turning the reader’s attention towards its 
shortages in the next section. 
A. Origins and mechanisms of anthropomorphism 
The human inclination to project some essence of 
humanness onto non-humanness, as firstly pointed out in early 
scholarly work by Charles Darwin, David Hume, or Sigmund 
Freud, remains a widely observable and reported phenomenon 
across different entities (e.g. animals, objects, or supranatural 
beings), and, of course, with varying degrees of prevalence 
and intensity throughout different historical, cultural, 
situational, and individual contexts [11][12]. Ever since the 
emergence of modern consumer technologies in the 20th 
186
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

century, this anthropomorphism paradigm has served as a 
popular framework to conceptualise those empirical 
observations in which human interactions with machines and 
media seemed to follow certain patterns of intra-human 
behaviours and perceptions (e.g. [9][15][16]). 
When it comes to the underlying psychological 
mechanisms that supposedly drive this widely observable 
inclination of human nature, anthropomorphism has been 
explained as an inductive inferential process: when we 
encounter a non-human entity with an uncertain or ambiguous 
inner state of being, we attempt to imbue its opaqueness with 
our introspectively acquired certainty about human life and 
mentality by adjusting our behaviour and perceptions as if it 
was human [11][12]. Notably, this process can already be 
present during infancy and early childhood, when children 
project their inner idea of human life and mentality – even 
though these ideas might still be pre-mature from a 
developmental perspective – onto the objects they play with, 
often according to their vivid imaginations and fantasies, 
which is referred to as ‘pretend play’ or ‘behaving-as-if-play’ 
[10]. 
B. Extending anthropomorphism: a thought experiment 
It must be emphasised that for children as well as adults 
anthropomorphism is not necessarily about confusing human 
and non-human entities; instead, it is about the creative control 
that is exercised over an entity that offers sufficient space for 
projection [10][17]. Hence, the reason why it would not make 
any sense to apply anthropomorphism to intra-human 
interactions is because, theoretically speaking, there is no 
space when one attempts to project humanness onto 
something that is indeed human. In other words, it would be 
odd to argue that when we interact with each other we do 
behave as if we were human, because, strictly speaking, we 
are. 
This yields an interesting theoretical thought experiment: 
if we extend the basic notion of anthropomorphism, one 
reasonable implication – similar to the reasoning of the 
original Turing Test [18] – would be that human interactions 
with machines should become more humanlike as technology 
develops, all the way up to the (theoretical) stage of perfect 
resemblance when AI would be able to emulate all domains 
of human intelligence. At this (theoretical) stage – which 
would also go beyond a potential uncanny valley – the 
anthropomorphism paradigm would suggest that human-
machine and human-human interactions follow (almost) 
indistinguishable patterns. Interestingly enough, this idea of 
‘perfect anthropomorphism’ matches the notion embedded in 
most pop cultural future visions that became famous 
throughout the 20th and 21st-century, such as the 
supercomputer ‘HAL 9000’ in Stanley Kubrick’s masterpiece 
‘2001: A Space Odyssey’, the crime-fighting car ‘Kitt’ in the 
TV-show ‘Knight Rider’, or, most recently, the charming 
virtual girlfriend ‘Samantha’ in Spike Jonze’s science-fiction 
romantic drama ‘Her’. 
However, even today, while we still wait for general AI 
and humanoid supercomputers to arrive (or not), the 
anthropomorphism paradigm and its theoretical implications 
seem problematic for several reasons, which are discussed in 
the next section. 
III. CHALLENGING ANTHROPOMOPHISM 
This section challenges anthropomorphism with three 
points of criticism related to its theoretical substructure as well 
as its applicability in the context of DVAs. 
A. The appreciation of non-human qualities 
The 
first 
general 
point 
of 
criticism 
against 
anthropomorphism is that, due to its simplistic theoretical 
substructure, it fails to consider an important aspect, namely 
how we as humans might appreciate machines due to their 
non-human qualities – and not despite of them. In other words, 
instead of arguing that our interactions with machines will 
become more intimate and intense as we see more humanness 
in them, the opposite could be true, because we might prefer 
machines over humans whenever we appreciate certain 
aspects about their inner absence of humanness. 
For instance, since the early 1970s, an extensive body of 
clinical research has shown how patients are more willing to 
self-disclose personal insights to a computer rather than a 
human physician [19]–[23], and a comprehensive meta-
analysis confirmed this tendency of humans to self-disclose 
more personal insights through a computer interface 
compared to face-to-face interviews [24]. Furthermore, more 
recent research has been able to extend these findings to 
virtual agents, which were often able to establish higher levels 
of rapport and elicit more personal insights from participants 
compared to the human baseline condition [25]–[27]. Another 
very recent piece of experimental research has shown that, 
across different domains of intelligence, participants 
prioritised predictions and assessments that were labelled to 
be of algorithmic origin compared somebody else’s or even 
one’s own prediction and assessment, which suggests, that, at 
times, humans might be willing to attribute higher levels of 
trust to the computational power of contemporary machines, 
even when they are unfamiliar with the machines’ inner 
working mechanisms [28]. 
Although both empirical aspects outlined above certainly 
allow for more than one theoretical explanation, this first point 
of criticism can be summarised as follows: contrary to the 
implicit notion of anthropomorphism, the breadth and depth 
of human interactions with machines might be enhanced by 
the perceived absence of humanness (e.g. moral judgement) 
and the presence of non-human machine qualities (e.g. 
superior computational power). 
B. DVAs’ limited scope for anthropomorphic projection 
The second point of criticism refers back to an issue raised 
earlier in the introduction: how much imagination is required 
when we talk to DVAs? The short answer remains none, 
because, in light of the previous discussion, it would be as odd 
to argue that, when we interact with DVAs, we behave as if 
these machines were talking to us, because, strictly speaking, 
they are. But, even if one argues that human-machine 
interactions, 
which 
are 
restricted 
to 
voice-only 
communication, offer plenty of room for anthropomorphic 
projection (e.g. [29]), one should keep in mind that DVAs are 
187
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

endowed with real interactive features and pre-programmed 
personalities, which may not prevent anthropomorphism per 
se, but they certainly constitute impeding factors by reducing 
the potential scope for the imaginative forces of creative 
control. In fact, exploratory empirical findings reported by 
[30] show how children systematically probe DVAs in order 
to understand the inner nature of the machine. Although some 
of these reported probing behaviours (e.g. asking for DVAs’ 
age or favourite colour, testing DVAs’ sense of humour), and 
children’s verbally expressed perceptions about DVAs (e.g. 
claiming DVAs possess emotions and feelings), seem 
anthropomorphic at first glance, the entirety of empirical 
findings by [30] does not suggest that children engage in 
strong pretend play, or behaving-as-if scenarios. Instead, 
children systematically attempt to reduce uncertainty by 
unfolding DVAs’ opaqueness. And even if children report 
firm perceptions about DVAs’ inner emotional states, this 
could be the result of a sincere experience-based judgement 
that a DVA has effectively communicated an emotional state, 
rather than expressing a pretended imagination of the DVAs’ 
anthropomorphic inner state of being. 
C. Developmental origins of what it means to be human 
Lastly, and most importantly, the argument of 
anthropomorphism skips a decisive step: claiming that 
humans are inclined to project their inner idea of human life 
and mentality onto non-human entities, raises the question 
where these subjective ideas come from, and the consecutive 
critical question, how the emergence of somebody’s 
internalised idea of what it means to be human may in itself 
be affected by interactions with non-human, but nevertheless 
humanoid entities, such as DVAs. As mentioned earlier (see 
Section II.A), our inner ideas of human life and mentality 
gradually mature as part of our development, when we learn – 
among other things – to recognise others as living kinds with 
intentions, mentality, intelligence, morality, emotions, or, in 
short, as humans [10]. However, cognitive development is 
subject to environmental stimuli, and therefore, changing 
childhood environments, that are increasingly characterised 
by humanoid manifestations of AI, could not only change how 
children think about these technologies, but also about 
themselves as humans [31]. This reasoning introduces the 
starting point of NOCH, which is discussed in the next section. 
IV. NEW ONTOLOGICAL CATEGORY HYPOTHESIS 
In a nutshell, NOCH argues that children, who grow up in 
highly technologised environments, might conceptualise 
humanoid intelligently behaving machines as hybrid beings, 
between the cognitive domains of living humans, on the one 
hand, and non-living machines, on the other hand, therefore 
forming an ontological category in its own right within 
children’s emerging understanding of the world. This section 
briefly summarises the theoretical substructure of NOCH and 
exemplifies how it can raise new perspectives around child-
machine interactions, in general, and child-DVA interactions, 
in particular. 
A. Developmental concept of ontologies 
In general, human cognitive development describes the 
iterative process of developing an experience-based 
understanding of the world by linking the sensual experience 
of the present with the conceptualised experience of the past 
[32]. Although these emerging and constantly refined mental 
representations of reality become more sophisticated, 
nuanced, and engrained throughout infancy and childhood, 
their complexity always remains subsumable under a single 
system of cognitive boundaries, referred to as ontology, which 
allows for a basic categorisation of entities along the lines of 
their perceived features and attributes [32][33]. Hence, 
ontological categories translate into foundational distinctions 
between the broadest classes of physical existence, such as 
living and non-living beings, human and non-human living 
beings, natural and artefactual non-living beings, and so on 
[17][31][33]. Although research has shown how children may 
struggle to categorise entities with ambiguous characteristics 
into ontological categories [35]–[37], the argument of NOCH 
goes one step further: if certain entities remain ‘lost’ within a 
child’s ontology, because they display characteristics that 
relate to multiple ontological natures from the perspective of 
the child (see Fig. 1), therefore preventing a clear 
categorisation, a new ontological category might be formed in 
order to overcome cognitive ambiguities [14]. 
In other words, from the perspective of children, who 
would have developed a new ontological category for 
intelligently behaving machines, the question whether Alexa 
et al. are humans or machines might seem as strange as the 
question whether an orange piece of paper is yellow or red, 
because, in both cases, the object (i.e. DVA) or quality (i.e. 
colour) in question would be perceived as something in its 
own right [38]. 
B. DVAs: A new ontological category? 
Peter H. Kahn and his colleagues, the original authors of 
NOCH, conceived their idea in light of the technological 
achievements of the early 2010s [14], but their work predates 
many of the recent AI breakthroughs, as well as DVAs’ 
tremendous commercial success, and, so far, there have been 
very few attempts to advance and apply their legacy (for one 
of the few exceptions see [5]), despite an array of intuitive 
reasons, why DVAs’ humanoid omnipresence could indeed 
introduce an perceptual ontological change of today’s 
childhood environments. Although an all-encompassing 
discussion is beyond the scope of this paper, the important 
point to make here is that, when it comes to the investigation 
of child-DVA interactions, NOCH urges us to at least consider 
that this unprecedented context – namely the permanent 
presence of a humanoid voice in a child’s home environment, 
starting at birth, and lasting into maturity – might also raise 
unprecedented questions. 
188
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

In particular, it could explain why children might 
appreciate DVAs’ humanoid (e.g. mastery of human 
language) as well as non-human qualities (e.g. absence of 
moral judgement) at the same time (see Section III.A), and 
why children’s interactions with DVAs might not display 
strong notions of anthropomorphism, such as pretend play or 
behaving-as-if scenarios. Furthermore, NOCH also urges us 
to question common assumptions and arguments. For 
instance, when children use human attributes to characterise 
DVAs (e.g. she/her, he/his/him), according to NOCH, this 
would not necessarily imply that children anthropomorphise 
by projecting humanness onto these machines, because, as a 
new hybrid ontological category, Alexa et al. simply draw 
upon certain linguistic qualities from one of the ontological 
‘parent’ categories, while remaining a distinct non-human 
(but nevertheless humanoid) category in their own right. 
Another example: the feminist criticism that DVAs’ 
female voices cause the reproduction of discriminating 
stereotypes in children (e.g. [39]), implicitly assumes that 
children directly associate and equalise the concept of human 
femininity with artificial femininity, as it is embodied by 
DVAs’ female voices. However, if Alexa et al. were hybrid 
beings in their own right, from an ontological perspective, this 
assumption would require empirical validation, since it cannot 
be inferred from theory alone. Or differently spoken, and in 
line with the metaphor used earlier: if you do not like the 
colour yellow, can we simply assume that you do not like the 
colour orange either? 
In sum, it may be left to the reader whether NOCH 
constitutes a paradigm shift that could overcome the implicit 
shortages of anthropomorphism. But it certainly raises new 
and potentially important questions in the context of child-
DVA interactions, which might be worth investigating. 
V. CONCLUSION 
This paper constitutes a critical review and discussion of 
the anthropomorphism paradigm, which remains a popular 
implicit or explicit theme in the literature on human-machine 
interaction, in general, and child-DVA interactions, in 
particular. This paper contributes to the literature by raising 
the argument that NOCH serves as a fruitful theoretical lens 
for the conceptualisation and empirical investigation of child-
DVA interactions, as they take place in children’s natural 
home environments. 
VI. FUTURE RESEARCH 
The next steps for future research are to develop a research 
design that allows for the empirical investigation of NOCH in 
the context of child-DVA interactions. 
In particular, the research will focus on the empirical 
exploration of NOCH with respect to one particular dimension 
of child-DVA interactions: a comparison of the nature of 
children’s self-disclosure when interacting with DVAs, on the 
one hand, and humans, on the other hand. This empirical focus 
seems to be of particular importance, because it sheds light on 
an important area of tension: as pointed out in the 
introduction, there is this concrete vision that children might 
develop close interactive relationships with intelligently 
behaving machines while growing up with them (e.g. 
[13][39]). If this notion is combined with the empirical 
findings reported earlier (see Section III.A), showing how 
humans are also inclined to appreciate machines’ non-human 
qualities (e.g. absence of human moral judgement), and the 
theoretical implications of NOCH that children might accept, 
appreciate, or even prefer a confidant of hybrid ontological 
nature over a human alternative, this issue seems worthwhile 
investigating. In addition, given that DVAs, in particular, and, 
arguably, even AI, in general, will continue to rely on the 
depth and breadth of insights we are willing to reveal, 
investigating children’s self-disclosure of personal insights 
would also have important implications for the future role of 
DVAs in our home environments, which is currently 
envisioned with a strong emphasis on leveraging user data 
through the customisation of services [42]. In order to 
compare the nature of children’s self-disclosure when 
communicating with DVAs, on the one hand, and humans, on 
the other hand, a mixed methods research design with two 
major design components is proposed.  
A. First design component 
The first design component is supposed to explore the 
nature of children’s self-disclosure by comparing how 
children share personal insights with a DVA, on the one hand, 
and with a real human, on the other hand. Although the 
collection of verbal data through DVAs has already been 
applied in preliminary research on DVA usage patterns [43]–
[47], the future research referred to in this paper attempts to 
contribute to the literature by using a researcher-designed 
DVA-application (work-in-progress). For the analysis, the 
transcribed verbal data are supposed to be explored and 
compared with computational methods of psychological text 
analysis [48]–[50]. The focus of the analysis is to explore and 
 
 
Figure 1. DVAs’ embeddedness in children’s ontological believe 
system according to NOCH (Source: compiled by the author, see [40])  
189
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

compare children’s self-disclosure patterns both within as 
well as between the DVA-condition and the human-condition 
(i.e. between subject comparison and within subject 
comparison). 
B. Second design component 
The second design component is supposed to complement 
the implicit weaknesses of the quantitative component by 
providing an in-depth panorama of selected cases, and in order 
to understand individual reasons and circumstances that might 
have caused the observed patterns of self-disclosure. Methods 
of data collection mainly include semi-structured interviews 
and observations during household visits, which are then used 
for qualitative means of data analysis in psychology, such as 
thematic analysis [51]. 
C. Additional remarks on future research 
The target population of this research consists of normally 
developing children in industrialised English-speaking 
countries, who are in the concrete operational stage of their 
cognitive development (i.e. ∽ 5 to 10 years), and with or 
without prior domestic DVA exposure at the beginning of the 
study. The intended sample sizes are n → 50 for the first 
component, and n → 10 for the second component. The 
beginning of the data collection is scheduled for spring 2021. 
ACKNOWLEDGEMENT 
I would like to thank my academic supervisors for their great 
support so far. 
 
REFERENCES 
[1] 
B. Kinsella and A. Mutchler, “Smart Speaker Consumer 
Adoption Report.” Voicebot & Voicify, 2019. 
[2] 
B. Kinsella and A. Mutchler, “Voice Assistant Consumer 
Adoption Report.” Voicebot, PullStrong & RAIN, 2019. 
[3] 
K. Wagner and H. Schramm-Klein, “Alexa, Are You Human? 
Investigating Anthropomorphism of Digital Voice Assistants 
- A Qualitative Approach,” in Fortieth International 
Conference on Information Systems, 2019, pp. 1–17. 
[4] 
A. Purington, J. G. Taft, S. Sannon, N. N. Bazarova, and S. 
H. Taylor, “‘Alexa is my new BFF’: Social roles, user 
satisfaction, and personification of the Amazon Echo,” in 
Proceedings of the ACM CHI Conference on Human Factors 
in Computing Systems, 2017, vol. Part F1276, pp. 2853–2859. 
[5] 
A. Pradhan, L. Findlater, and A. Lazar, “Phantom Friend or 
Just a Box with Information: Personification and Ontological 
Categorization of Smart Speaker-based Voice Assistants by 
Older Adults,” in Proceedings of the ACM on Human-
Computer Interaction, 2019, pp. 1–21. 
[6] 
J. Coughlin, “Alexa, Will You Be My Friend? When 
Artificial Intelligence Becomes Something More,” Forbes, 
2018. 
[Online]. 
Available: 
https://www.forbes.com/sites/josephcoughlin/2018/09/23/ale
xa-will-you-be-my-friend-when-artificial-intelligence-
becomes-something-more/. [Accessed: 25-Jan-2020]. 
[7] 
I. Lopatovska and H. Williams, “Personification of the 
Amazon Alexa: BFF or a Mindless Companion,” in 
Proceedings of the 2018 Conference on Human Information 
Interaction & Retrieval, 2018, pp. 265–268. 
[8] 
N. Motalebi, E. Cho, S. S. Sundar, and S. Abdullah, “Can 
Alexa be your Therapist?: How Back-Channeling Transforms 
Smart-Speakers to be Active Listeners,” in Conference 
Companion Publication of the 2019 on Computer Supported 
Cooperative Work and Social Computing, 2019, pp. 309–313. 
[9] 
B. Reeves and C. I. Nass, The media equation: How people 
treat computers, television, and new media like real people 
and places. Stanford, CA, US: CSLI Publications, 1998. 
[10] G. Airenti, “The Cognitive Bases of Anthropomorphism: 
From Relatedness to Empathy,” Int. J. Soc. Robot., vol. 7, no. 
1, pp. 117–127, 2015. 
[11] N. Epley, A. Waytz, and J. T. Cacioppo, “On Seeing Human: 
A Three-Factor Theory of Anthropomorphism,” Psychol. 
Rev., vol. 114, no. 4, pp. 864–886, 2007. 
[12] A. Waytz, J. Cacioppo, and N. Epley, “Who Sees Human? 
The Stability and Importance of Individual Differences in 
Anthropomorphism,” Perspect. Psychol. Sci., vol. 5, no. 3, 
pp. 219–232, 2014. 
[13] C. Biele et al., “How Might Voice Assistants Raise Our 
Children?,” in International Conference on Intelligent 
Human Systems Integration. IHSI 2019. Advances in 
Intelligent Systems and Computing, 2019, pp. 162–167. 
[14] P. H. J. Kahn et al., “The New Ontological Category 
Hypothesis in Human- Robot Interaction,” in HRI’11 - 6th 
Annual Conference for basic and applied human- robot 
interaction research, 2011, pp. 159–160. 
[15] C. Nass, J. Steuer, and E. R. Tauber, “Computers Are Social 
Actors,” in Proceedings of the SIGCHI Conference on Human 
Factors in Computing Systems, 1994, pp. 72–78. 
[16] C. Nass, B. J. Fogg, and Y. Moon, “Can Computers be 
Teammates?,” Int. J. Hum. Comput. Stud., vol. 45, no. 6, pp. 
669–678, 1996. 
[17] R. L. Severson and S. M. Carlson, “Behaving as or behaving 
as if? Children’s conceptions of personified robots and the 
emergence of a new ontological category,” Neural Networks, 
vol. 23, no. 8–9, pp. 1099–1103, 2010. 
[18] A. Turing, “Computing machinery and intelligence,” Mind, 
vol. 49, pp. 433–460, 1950. 
[19] J. H. Greist, M. H. Klein, and L. J. Van Cura, “A Computer 
Interview for Psychiatric Patient Target Symptoms,” Arch. 
Gen. Psychiatry, vol. 29, no. 2, pp. 247–253, 1973. 
[20] J. H. Greist et al., “A Computer Interview for Suicide-Risk 
Prediction,” Am. J. Psychiatry, vol. 130, no. 12, pp. 1327–
1332, 1973. 
[21] R. Robinson and R. West, “A comparison of computer and 
questionnaire methods of history-taking in a genito-urinary 
clinic,” Psychol. Health, vol. 6, no. 1–2, pp. 77–84, 1992. 
[22] M. Ferriter, “Computer Aided Interviewing in Psychiatric 
Social Work,” Comput. Hum. Serv., vol. 9, no. 1–2, pp. 59–
66, 1993. 
[23] P. Kissinger et al., “Application of Computer-Assisted 
Interviews to Sexual Behavior Research,” Am. J. Epidemiol., 
vol. 149, no. 10, pp. 950–954, 1999. 
[24] S. Weisband and S. Kiesler, “Self Disclosure on Computer 
Forms: Meta-Analysis and Implications,” in Proceedings of 
the SIGCHI Conference on Human Factors in Computing 
Systems, 1996, pp. 3–10. 
[25] D. DeVault et al., “SimSensei Kiosk: A Virtual Human 
Interviewer,” 
in 
13th 
International 
Conference 
on 
Autonomous Agents and Multiagent Systems (AAMAS 2014), 
190
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

2014, pp. 1061–1068. 
[26] G. M. Lucas, J. Gratch, A. King, and L.-P. Morency, “It’s 
only a computer: Virtual humans increase willingness to 
disclose,” Comput. Human Behav., vol. 37, no. C, pp. 94–100, 
2014. 
[27] K. Yokotani, G. Takagi, and K. Wakashima, “Advantages of 
virtual 
agents 
over 
clinical 
psychologists 
during 
comprehensive mental health interviews using a mixed 
methods design,” Comput. Human Behav., vol. 85, pp. 135–
145, 2018. 
[28] J. M. Logg, J. A. Minson, and D. A. Moore, “Algorithm 
appreciation: People prefer algorithmic to human judgment,” 
Organ. Behav. Hum. Decis. Process., vol. 151, pp. 90–103, 
2019. 
[29] V. Turk, “Home invasion,” New Sci., vol. 232, no. 3104–
3106, pp. 16–17, 2016. 
[30] S. Druga, R. Williams, C. Breazeal, and M. Resnick, “‘Hey 
Google is it OK if I eat you?,’” in Proceedings of the 2017 
Conference on Interaction Design and Children - IDC ’17, 
2017, pp. 595–600. 
[31] D. Bernstein and K. Crowley, “Searching for Signs of 
Intelligent Life: An Investigation of Young Children’s Beliefs 
About Robot Intelligence,” J. Learn. Sci., vol. 17, no. 2, pp. 
225–247, 2008. 
[32] S. A. Gelman, “Concepts in Development,” in The Oxford 
Handbook of Developmental Psychology (Vol 1): Body and 
Mind, vol. 1, P. D. Zelazo, Ed. New York, NY, USA: Oxford 
University Press, 2013, pp. 542–563. 
[33] I. Gaudiello, S. Lefort, and E. Zibetti, “The ontological and 
functional status of robots: How firm our representations 
are?,” Comput. Human Behav., vol. 50, pp. 259–273, 2015. 
[34] H. M. Wellman and S. A. Gelman, “Cognitive Development: 
Foundational Theories of Core Domains,” Annu. Rev. 
Psychol., vol. 43, pp. 337–375, 1992. 
[35] J. L. Jipson and S. A. Gelman, “Robots and rodents: 
Children’s inferences about living and nonliving kinds,” 
Child Dev., vol. 78, no. 6, pp. 1675–1688, 2007. 
[36] J. M. Kory-Westlund and C. Breazeal, “Assessing Children’s 
Perceptions and Acceptance of a Social Robot,” in 
Proceedings of the 18th ACM International Conference on 
Interaction Design and Children, 2019, pp. 38–50. 
[37] M. Scaife and M. Duuren, “Do computers have brains? What 
children believe about intelligent artifacts,” Br. J. Dev. 
Psychol., vol. 13, no. 4, pp. 367–377, 1995. 
[38] P. H. Kahn, H. E. Gary, and S. Shen, “Children’s Social 
Relationships With Current and Near-Future Robots,” Child 
Dev. Perspect., vol. 7, no. 1, pp. 32–37, 2013. 
[39] UNESCO, “I’d blush if I could,” 2019 [Online]. Available: 
https://en.unesco.org/Id-blush-if-I-could. [Accessed: 02-Mar-
2020]. 
[40] J. Festerling and I. Siraj, “Exploring Children’s Ontological 
Perceptions of Digital Voice Assistants: A Cognitive 
Developmental Perspective,” unpublished manuscript. 
[41] R. Gonzales, “Hey Alexa, What are you doing to my kid’s 
brain?,” 
WIRED, 
2018. 
[Online]. 
Available: 
https://www.wired.com/story/hey-alexa-what-are-you-
doing-to-my-kids-brain/. [Accessed: 25-Jan-2020]. 
[42] K. Hao, “Inside Amazon’s plan for Alexa to for Alexa to run 
your entire life,” MIT Technology Review, 2019. [Online]. 
Available: 
https://www.technologyreview.com/s/614676/amazon-alexa-
will-run-your-life-data-privacy/. [Accessed: 25-Jan-2020]. 
[43] M. Porcheron, J. E. Fischer, S. Reeves, and S. Sharples, 
“Voice Interfaces in Everyday Life,” in Proceedings of the 
2018 CHI Conference on Human Factors in Computing 
Systems - CHI ’18, 2018, pp. 1–12. 
[44] S. B. Lovato, A. M. Piper, and E. A. Wartella, “Hey Google, 
Do Unicorns Exist?: Conversational Agents as a Path to 
Answers to Children’s Questions,” in Proceedings of the 18th 
ACM International Conference on Interaction Design and 
Children, 2019, pp. 301–313. 
[45] E. Beneteau et al., “Communication Breakdowns Between 
Families and Alexa,” in Proceedings of the 2019 CHI 
Conference on Human Factors in Computing Systems, 2019, 
pp. 1–13. 
[46] A. Sciuto, A. Saini, J. Forlizzi, and J. I. Hong, “Hey Alexa, 
What’s Up?: A mixed-methods studies of in-home 
conversational agent usage,” in Proceedings of the 2018 
Designing Interactive Systems Conference, 2018, pp. 857–
868. 
[47] D. Beirl, N. Yuill, and Y. Rogers, “Using Voice Assistant 
Skills in Family Life,” in CSCL 2019 - 13th International 
Conference on Computer Supported Collaborative Learning, 
2019, pp. 96–103. 
[48] Y. R. Tausczik and J. W. Pennebaker, “The Psychological 
Meaning of Words: LIWC and Computerized Text Analysis 
Methods,” J. Lang. Soc. Psychol., vol. 29, no. 1, pp. 24–54, 
2010. 
[49] S. M. Mohammad and P. D. Turney, “Crowdsourcing A 
Word–Emotion Association Lexicon,” Comput. Intell., vol. 
29, no. 3, pp. 436–465, 2013. 
[50] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet 
Allocation,” J. Mach. Learn. Res., vol. 3, no. Jan, pp. 993–
1022, 2003. 
[51] V. Braun and V. Clarke, “Using Thematic Analysis in 
Psychology,” Qual. Res. Psychol., vol. 3, no. 2, pp. 77–101, 
2006. 
191
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

