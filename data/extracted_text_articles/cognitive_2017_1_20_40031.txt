Performance of Neural Clique Networks Subject to Synaptic Noise
Eliott Coyac, Vincent Gripon, Charlotte Langlais, and Claude Berrou
Electronics Department
IMT Atlantique
Brest, France
email: name.lastname@telecom-bretagne.eu
Abstract—Artiﬁcial neural networks are so-called because they
are supposed to be inspired from the brain and from the ways
the neurons work. While some networks are used purely for
computational purpose and do not endeavor to be a plausible
representation of what happens in the brain, such as deep learning
neural networks, others do. However, the question of the noise in
the brain and its impact on the functioning of those networks has
been little-studied. For example, it is widely known that synapses
misﬁre with a signiﬁcant probability. We model this noise and
study its impact on associative memories powered by neural
networks: neural clique networks and Hopﬁeld networks as a
reference point. We show that synaptic noise can in fact slightly
improve the performance of the decoding process of neural clique
networks by avoiding local minima.
Keywords–associative memories; neural clique networks; synap-
tic noise
I.
INTRODUCTION
There are multiple sources of noise in the brain. Indeed,
they can be molecular [1], [2] or due to external neurons [2].
Other factors include synaptic noise, the intermittent failure
of synapses, which seem to have a role outside of being just
noise [1], [2]. In this paper, we explore this particular type of
noise in details.
There are a lot of models of neural networks, which
either aim at modeling what happens in the brain or sim-
ply focus on efﬁciency at their speciﬁc purpose. The study
of the impact of noise on such artiﬁcial neural networks
focusing on performance is only relevant in the context of
electronical components, but that is obviously not the case
when considering neural networks that strive to be biologically
plausible. Such neural networks should not react adversely to
noise to be considered biologically plausible. In this paper, we
consider artiﬁcial neural networks that aim both at providing
efﬁcient solutions to real-world problems but also try to remain
plausible as a possible way the brain works, and study the
impact biological noise has on them. We focus on neural
networks working as associative memories [3]–[6], and study
how noise impacts their inner workings and performance.
Studies have already been conducted on the impact of noise
when implementing such neural networks on unreliable hard-
ware circuits [7], where the noise is caused by unreliable
components.
In this paper, we consider noise internal to the network,
and more speciﬁcally synaptic noise. We show how it can be
seen as an higher abstraction level than molecular noise and
that it can be easily modelled. The impact of synaptic noise
has been theoreticized in biological neural networks [8], but
never studied with regard to artiﬁcial neural networks that are
used for practical applications in computer science.
n2
prel
prel
prel
prel
prel
n1
Figure 1. Example of neuronal transmission from one neuron n1 to another
n2 with nsyn = 5. Each synapse has a probability prel = 0.5 of
stimulating n2 when n1 is activated.
The outline of the paper is as follows. We ﬁrst study how
synaptic noise can be represented in Section II. In Section
III, we introduce neural clique networks and discuss their
biological plausibility and applications. Finally, in Section
IV, we study the impact of synaptic noise on neural clique
networks, both theoretically and by running simulations. We
also brieﬂy depict the impact of synaptic noise on Hopﬁeld
networks, a classical form of neural network behaving as an
associative memory, for reference.
II.
SYNAPTIC NOISE IN THE BRAIN
In the brain, each neuron has numerous inputs from other
neurons and a single axon, which then branches to reach a
multitude of other target neurons. Even then, there is not a
single point of contact between the neuron and a target neuron,
but several. The axon not only branches to reach multiple
neurons, it also branches off in several synapses reaching the
same target neuron.
Generally, the connection between two neurons is com-
prised of 5 to 25 synapses [9]. One may ask why there
are so many synapses for a simple connection between two
neurons. Having a few is understandable for redundancy, but
there can be several tens of synapses. In fact, synapses are not
reliable [9], [10], and the probability of them working typically
ranges from 0.2 to 0.8 [9]. Such a conﬁguration of synapses
can help functioning when stressed under high frequency of
neuronal activation by spreading the load over the different
synapses [11], [12].
4
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-531-9
COGNITIVE 2017 : The Ninth International Conference on Advanced Cognitive Technologies and Applications

0
5
10
15
20
0.00
0.05
0.10
0.15
0.20
Stimulation intensity
Probability
prel = 0.2
prel = 0.5
Figure 2. Probabilities for different stimulation instensities with nsyn = 20.
Values for nsyn and prel fall within the model and are discussed at the end
of the paper.
In this paper, we consider the way failing synapses affect
the connection between two neurons, and then we show how it
translates to artiﬁcial neural networks. To make things simpler,
we consider that each connection between two neurons has
nsyn synapses of the same strength, normalized to 1, and
that they each have the same probability prel of working,
independently one from another. prel is named as such as it
represents the probability of releasing neuro-transmitters when
stimulated by the axon. The connection between two neurons
n1 and n2 is represented in Fig. 1. With this model, the
stimulation a neuron receives from another follows a binomial
law B(tries, psuccess) with the parameters nsyn and prel as
shown in Fig. 2,.
III.
NEURAL CLIQUE NETWORKS
Neural clique networks are associative memories with
error-correcting properties. They store binary patterns and use
binary connections. They have a high capacity, comparable to
binary storage. They also strive to achieve biological plausi-
bility.
A. Minicolumns and clusters
The smallest unit in a neural clique network is a fanal
and is based on a cortical minicolumn [13], which is a
pattern comprising around 100 neurons. This pattern has been
observed in humans and several other species. These fanals
are organized in clusters, and only one fanal can be active
at the same time in the same cluster, replicating the widely-
used winner-takes-all law and alleviating concerns of energy
efﬁciency.
A neural clique network is made of χ clusters containing
ℓ fanals each.
B. Storage
A message is stored as a group of several fanals belonging
to different clusters. A connection is established between two
fanals if they both belong to the same message, following
Hebb’s law [14]. A fanal can belong to a multitude of
messages. It is the connections between the fanals that deﬁne
the messages and contain the information. Thus, each message
is represented by a fully interconnected group of fanals, called
a clique. In full neural clique networks, the number of fanals c
making up a clique is equal to χ, the number of clusters in the
network. As such, each clique contains one fanal from each
cluster. As an example, a full neural clique network containing
3 messages is shown in Fig. 3.
Figure 3. Storing procedure illustration. The pattern to store (with thick
edges) connects units from 4 clusters of 16 units each (ﬁlled circles, ﬁlled
rectangles, rectangles and circles).
In sparse neural networks, we have 1 < c < χ. Stored
messages do not use all avalaible clusters.
C. Retrieval and performance
There are two forms of message retrieval. The network
can be asked if a message exists already, which is a simple
matter of testing if the fanals representing the message are
fully interconnected. If a message was stored, the network will
always say so, but false positives can be generated if two many
messages sharing fanals in common overlap.
The second form of message retrieval is providing a partial
message to the network with erasures (and possibly errors) and
retrieving the full message. The algorithm for retrieving the full
message simply consists of ﬁnding the fanals with the most
connections to the known fanals, at most one per cluster.
As far as performance goes, the network needs a binary
storage capacity of (χ · ℓ)2/2 bits to store the connections
between the different fanals, the adjacency matrix of the graph
representing the network. As shown in the examples further in
this paper, a full network of 8 clusters of 256 fanals can retrieve
15000 half-erased messages with an error rate of less than
2%. The binary storage needed for storing all the messages
without any error correcting mechanism would be 120 kB, and
the neural network uses 260 kB of storage for the adjacency
matrix, so the storage efﬁciency compared to raw binary is
46%, but with large resistance to message erasures.
D. Decoding algorithm
The algorithm that we use is suitable for retrieving partially
erased messages. Other algorithms can be applied ﬁrst to ﬁlter
out irrelevant inputs or with other purposes. However, that is
not our concern here.
5
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-531-9
COGNITIVE 2017 : The Ninth International Conference on Advanced Cognitive Technologies and Applications

1) Full networks: Each fanal gets a score, which is the
number of other activated fanals connected to it. The fanals
with the highest score in each cluster are activated for the
next iteration, all the others are deactivated. Known fanals
from the partially erased message are already provided with a
high score at each iteration so they are always the only fanal
activated in their own cluster. If there are several fanals with
the same highest score in a cluster, they all are activated for
the next iteration. The algorithm stops after several iterations,
which are enough to reach a stable state with the memory effect
introduced below. When the algorithm stops, one activated
fanal is picked from each cluster, at random if there are several
fanals activated.
A memory effect γ can be added, which increases per-
formance in the case of noise-free networks. It consists of
adding γ to the score of a fanal if it was activated the previous
iteration. Generally, γ = 1 is used, simulating each fanal being
connected to themselves. It is interpreted as an already active
fanal being easier to activate than an inactive fanal of the same
cluster.
2) Sparse networks: Sparse networks follow the same
principle as full networks. The difference is that only a few
clusters are to have a fanal activated for the next iteration
amongst all the clusters. There are two algorithms in order
to choose those activated clusters, the c-global-winners-take-
all and global-winners-take-all. The c-global-winners-take-all
takes the clusters with the highest score (determined by the
score of the fanal with the highest score inside the cluster) until
c clusters are chosen, with c being the size of the message.
Then it takes all the clusters that tie the chosen cluster with
the lowest score. All the chosen clusters are activated for the
next iteration.
The global-winners-take-all just stops at taking c clusters.
If several clusters have the same score, it only takes as many as
it needs to have c clusters, at random. The c-global-winners-
take-all is known to have better performance.
E. Applications
Neural clique networks have been used in various ap-
plication cases. In electronics, Boguslawski et al. [15] use
them to handle power management in multicore processors,
showing signiﬁcant improvements in energy usage compared
to existing solutions. In [16], the authors propose to use
neural clique networks to accelerate search in databases. They
provide a fully hardware implementation of their solution
using memristors, and obtain reduced energy consumption and
delays compared to classical solutions.
In [17], the authors propose to use neural clique networks
in combination with product quantization in order to accelerate
search in visual descriptors of images. The result is a gain of a
factor of about 100 in comparison to exhaustive search. Similar
work has been proposed in [18].
IV.
NEURAL CLIQUE NETWORKS WITH UNRELIABLE
CONNECTIONS
We study the impact of unreliable connections due to
synaptic noise on neural clique networks. Assuming messages
are independently and uniformly distributed, we propose a full
mathematical analysis of the retrieval of a partially erased
pattern after one iteration, which corresponds to the second
form of message retrieval previously discussed. For multiple
iterations, due to the complexity of the problem and the
multitude of variables we can only provide simulations, which
show us the retrieval rates with or without synaptic noise.
A. One iteration
We consider the probability of ﬁnding the correct version
of a partially erased message after one iteration.
1) Full network: Let’s consider a full neural clique network
(each cluster is used for each message). Let M be the number
of messages in the network. When we try to recover a message
in the network, let ck the number of known clusters and ce
(ce = c − ck) the number of erased clusters. The density of
the network, that is the probablity of a connection existing
between any two fanals, is [7]:
d = 1 −

1 − 1
ℓ2
M
(1)
Let’s consider an erased cluster. Let s0 be the correct fanal,
ns0 its score, s be an incorrect fanal, and ns its score. The
score of a fanal is the number of synapses connected to it
that released neurotransmitters. So a fanal connected to i other
fanals can get a score between 0 and i·nsyn. The correct fanal
s0 is obviously connected to the other ck known correct fanals.
So for x from 0 to nsyn · ck we have
P(ns0 = x) = P (B(nsyn · ck, prel) = x)
(2)
= pmf(x, nsyn · ck, prel)
(3)
We noted pmf the probability mass function of the bino-
mial law B. The incorrect fanals of the erased cluster can
have between 0 and ck connections to the known correct
fanals. First, we need to determine PE(i), the probability
that an incorrect fanal has i connections to known correct
fanals. In theory, existence of connections are not independent
events, which may lead to difﬁcult mathematical analysis [19].
In order to simplify the proofs, we make the assumption
they are independent, which has been reported to be a fair
approximation [6]. We ﬁnd
PE(i) =
ck
i

di(1 − d)ck−i.
(4)
Indeed, the probability of not being connected to any of
the known fanals is (1 − d)ck and the probability of being
connected to all the known fanals is dck. The probability of
being connected to only a speciﬁc known fanal is (1−d)ck−1d,
and to be solely connected to any one of the known fanals is
ck · (1 − d)ck−1d.
And with that, we can deduce the probability of an incor-
rect fanal getting a score x0 for any 0 ≤ x0 ≤ nsyn · ck:
P(ns = x0) =
ck
X
i=0
PE(i) pmf(x0, nsyn · i, prel)
(5)
P(ns ≤ x0) =
x0
X
x=0
ck
X
i=0
PE(i) pmf(x, nsyn · i, prel)
(6)
6
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-531-9
COGNITIVE 2017 : The Ninth International Conference on Advanced Cognitive Technologies and Applications

Now that we have this, we can write the probability that
the correct fanal is amongst the fanals with the highest scores:
Psucc(s0) =
nsyn·ck
X
x0=0
P(ns0 = x0)P(ns ≤ x0)ℓ−1.
(7)
The global probability of success, i.e., the probability that
in all erased nodes the correct node is amongst the winner is
Psucc = Psucc(s0)ce. The error rate is 1 − Psucc.
That approach is too lax, however. In practice, when
looking for a message of size c, we want c symbols as the
output of the network, not a set of size s (s ≥ c) containing
the c correct symbols. This means that if we have several fanals
with the highest score in the same cluster, we need to pick only
one of them. We then have a chance
1
k+1 of picking the correct
fanal in ambiguous cases, where k is the number of incorrect
fanals sharing the highest score with the correct fanal.
0
2000
4000
6000
8000
10000
12000
14000
Number of stored messages M
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Error rate
theory
simulation
theory (no noise)
Figure 4. Analytical and simulated results with
c = 8, ck = 4, ℓ = 256, nsyn = 10, prel = 0.5.
To take that into account, Psucc(s0) is rewritten. First, we
create the probability of success for the correct fanal if its
score is x0:
Psucc(s0,ns0 = x0) =
ℓ−1
X
k=0
1
k + 1
l − 1
k

P(ns = x0)kP(ns < x0)(ℓ−1−k)
(8)
and
Psucc(s0) =
nsyn·ck
X
x0=0
P(ns0 = x0)Psucc(s0, ns0 = x0)
(9)
The results are shown on Fig. 4.
2) Sparse networks: Sparse networks are harder to tackle,
due to the problem of spurious clusters. While what happens
in each cluster with a correct fanal doesn’t concern the other
clusters, all the correct fanals belonging to the erased clusters
must have a score higher than all the fanals of the erased
clusters and incorrect clusters.
It is possible to formalize this second relationship, a.k.a.
the lowest score of the correct fanals must be higher than the
highest score of the incorrect fanals. If we consider χ the total
number of clusters and c the size of a message, if χ >> c
then we only need to consider that second relationship. As if
each correct fanal has a score higher than the highest fanals
of the χ − c incorrect clusters, then the probability that they
have the highest score in their own cluster is close to 1.
Using the same logic as before, taking into account the
ce correct fanals and the (χ − c)ℓ fanals belonging to in-
correct clusters, we obtain the formula. First, we change
Psucc(s0, ns0 = x0) into Psucc(ℓc, ℓi|ns0 = x0), being the
probability that given that lc correct fanals have a score x0 and
the other correct fanals a higher score, all the correct fanals
are chosen. We denote ℓi = (χ − c)ℓ.
We get
Psucc(ℓc, ℓi|ns0 = x0) =
ℓi
X
k=0
1

0
5000
10000
15000
20000
25000
30000
Number of stored messages M
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Error rate, density
density
2 iterations
3 iterations
4 iterations
Figure 5. Error rate for a full neural clique network of parameters c = 8 and
ℓ = 256, with nsyn = 10, ck = 4 and prel = 0.5. The decoding process is
stopped after 2, 3 or 4 stable iterations.
0
5000
10000
15000
20000
25000
30000
Number of stored messages M
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Error rate, density
density
no noise
prel = 0.4
prel = 0.5
prel = 0.8
Figure 6. Error rate for a full neural clique network of parameters c = 8 and
ℓ = 256, with nsyn = 10, ck = 4 and prel ranging from 0.4 to 1.
noise is not involved, but as noise is involved it is more
beneﬁcial to not have any memory effect. As shown on Fig. 4,
the error rate after one iteration in a noisy network is important
even for a low number of messages, and the memory effect
would carry those mistakes onto further iterations. In order
to avoid that, a memory effect of γ = 0 is chosen for noisy
networks.
2) Results: As can be seen from Fig. 6 and Fig. 7, the
binomial noise seems to have more beneﬁcial effects on full
networks than on sparse networks. For prel = 0.5, there’s even
a better capacity than with no noise for error rates inferior to
10%. For prel = 0.8, the capacity seems better for virtually
all error rates.
Concerning sparse networks, a signiﬁcant degradation of
performance is observed for prel ≤ 0.5 compared to when
there is no noise. We observe a reduction of the capacity of the
network of approximately 20% to 30% for error rates ranging
from 2% to 10%, which is still a good result considering the
0
20000
40000
60000
80000
100000
120000
140000
160000
180000
Number of stored messages M
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Error rate, density
density
no noise
prel = 0.4
prel = 0.5
prel = 0.8
Figure 7. Error rate for a sparse neural clique network of parameters
χ = 100 and c = 12, with nsyn = 10, ck = 9 and prel varying.
unreliability of the network.
Those results can be attributed to avoiding local mimima
while still keeping a low deviation, with a principle loosely
similar to simulated annealing. The reason full networks give
better results than sparse networks would be that even if the
noise can send the decoding algorithm off track, it still keeps
to the same clusters in full networks.
C. Impact on Hopﬁeld Networks
Hopﬁeld networks [3] are artiﬁcial recurrent networks
functioning as associative memories. They are made up of
N neurons and can store binary messages of N bits, but the
connection weights are not binary. The number of messages
they can store is O(N/ log(N)) [20]. Each pattern stored is
an attractor, and when inputting data it shifts to the closest
pattern stored.
0
50
100
150
200
250
300
Number of stored messages M
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Error rate, density
no noise
prel = 0.5
Figure 8. Error rate for an Hopﬁeld network of 2048 neurons, with or
without noise, and 1
4 of the input erased.
We ran a simulation on Hopﬁeld networks to see how such
a model with precise synaptic weights would react to the large
ﬂuctuations introduced by the unreliable connections.
8
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-531-9
COGNITIVE 2017 : The Ninth International Conference on Advanced Cognitive Technologies and Applications

Fig. 8 shows the behavior of an Hopﬁeld network in
similar conditions to before, with 1
4 of the input erased, 2048
neurons, and 10 synaptic contacts at each connection with each
a probability of release of 0.5. The simulation stops when a
stable state over two iterations is reached.
We can see that there is only a minor increase in the error
rate. We can surmise that this is due to the high number
of nodes active at the same time, averaging the effects of
the binomial noise. Compared to full neural clique networks,
which can take advantage of the noise, Hopﬁeld networks seem
to suffer a little decrease in performance.
V.
DISCUSSION
The analysis in this paper was based on the supposition
of having 10 synaptic contacts per connection and a proba-
bility of release of the neurotransmitters of exactly 0.5 for
each synaptic contact at each iteration, independently of the
previous iterations.
The number of synaptic contacts per connection was cho-
sen to be nsyn = 10, but simulations show a much lower error
rate after one iteration for nsyn = 20, which is also a realistic
number.
As said in [9], synaptic contacts adapt with the help
of feedback, so it is wise to consider whether the release
probability could exceed 0.5 where strong connections are
concerned. Moreover, it is difﬁcult to imagine that in the case
of repeated stimulation of a synaptic contact, the probability of
release of neurotransmitters each time is independent from the
previous occurences. It makes sense that it is more probable
for a synapse to release neurotransmitters if it did not with
the previous stimulation, as it would be more ready. As such,
the variance of the probalistic law governing the stimuli would
be reduced, making the biological architecture — the brain —
more reliable.
VI.
CONCLUSION
The contribution of this paper is twofold. First, we show
the signiﬁcance of the noise generated by unreliable synapses,
which we refer to as synaptic noise, and model that noise.
We see it introduces randomness with high variance in the
stimulation a neuron receives from another neuron, which
can be represented by a binomial law depending on the
number of synapses nsyn the connection is made of and the
neurotransmitter release probability prel. We then study the
impact of this noise on associative memories that strive on
biological plausibility, to show if the models we have of neural
networks in the brain survive scrutiny. In particular, we show
the impact of synaptic noise on neural clique networks and
Hopﬁeld networks.
Suprisingly, we see that with the correct parameters such
synaptic noise can in fact increase the retrieval rate of partially
erased messages in neural clique networks. It is due to the
noise allowing the network to overcome the local minima in
its decoding process. Regarding Hopﬁeld networks, on which
a simulation is run as a reference, synaptic noise decreases
performance only very slightly. As such, both associative
memories sustain the test of synaptic noise and neural clique
networks even beneﬁt from it. As a future work, it would
be interesting to see the impact of this kind of noise on
feedforward neural networks, as they emulate the way the
visual cortex works.
ACKNOWLEDGEMENT
This work was funded in part by the European Research
Council under the European Union’s Seventh Framework Pro-
gramme (FP7/2007-2013)/ERC grant agreement n◦ 290901.
It was also funded in part by the CominLabs project Neural
Communications and the Future & Rupture program.
REFERENCES
[1]
J. A. White, J. T. Rubinstein, and A. R. Kay, “Channel noise in neurons,”
Trends in neurosciences, vol. 23, no. 3, pp. 131–137, 2000.
[2]
A. A. Faisal, L. P. Selen, and D. M. Wolpert, “Noise in the nervous
system,” Nature reviews neuroscience, vol. 9, no. 4, pp. 292–303, 2008.
[3]
J. J. Hopﬁeld, “Neural networks and physical systems with emergent
collective computational abilities,” Proceedings of the national academy
of sciences, vol. 79, no. 8, pp. 2554–2558, 1982.
[4]
D. J. Willshaw, O. P. Buneman, and H. C. Longuet-Higgins, “Non-
holographic associative memory.” Nature, pp. 960–962, 1969.
[5]
D. H. Ackley, G. E. Hinton, and T. J. Sejnowski, “A learning algorithm
for boltzmann machines,” Cognitive science, vol. 9, no. 1, pp. 147–169,
1985.
[6]
V. Gripon and C. Berrou, “Sparse neural networks with large learning
diversity,” Neural Networks, IEEE Transactions on, vol. 22, no. 7, pp.
1087–1096, 2011.
[7]
F. Leduc-Primeau, V. Gripon, M. Rabbat, and W. Gross, “Cluster-based
associative memories built from unreliable storage,” in ICASSP, pp.
8370–8374, May 2014.
[8]
A. Zador, “Impact of synaptic unreliability on the information trans-
mitted by spiking neurons,” Journal of Neurophysiology, vol. 79, no. 3,
pp. 1219–1229, 1998.
[9]
T. Branco and K. Staras, “The probability of neurotransmitter release:
variability and feedback control at single synapses,” Nature Reviews
Neuroscience, vol. 10, no. 5, pp. 373–383, 2009.
[10]
C. Allen and C. F. Stevens, “An evaluation of causes for unreliability
of synaptic transmission,” Proceedings of the National Academy of
Sciences, vol. 91, no. 22, pp. 10 380–10 383, 1994.
[11]
M. Fauth, F. Wörgötter, and C. Tetzlaff, “The formation of multi-
synaptic connections by the interaction of synaptic and structural plas-
ticity and their functional consequences,” PLoS Comput Biol, vol. 11,
no. 1, pp. e1 004 031, 2015.
[12]
A. Figurov et al., “Regulation of synaptic responses to high-frequency
stimulation and ltp by neurotrophins in the hippocampus,” Nature, vol.
381, no. 6584, pp. 706–709, 1996.
[13]
D. P. Buxhoeveden and M. F. Casanova, “The minicolumn hypothesis
in neuroscience,” Brain, vol. 125, no. 5, pp. 935–951, 2002.
[14]
D. O. Hebb, The organization of behavior: A neuropsychological
approach.
John Wiley & Sons, 1949.
[15]
B. Boguslawski, V. Gripon, F. Seguin, and F. Heitzmann, “Twin
neurons for efﬁcient real-world data distribution in networks of neural
cliques: Applications in power management in electronic circuits,” IEEE
transactions on neural networks and learning systems, vol. 27, no. 2,
pp. 375–387, 2016.
[16]
H. Jarollahi et al., “A non-volatile associative memory-based context-
driven search engine using 90 nm cmos mtj-hybrid logic-in-memory
architecture,” Journal on Emerging and Selected Topics in Circuits and
Systems, vol. 4, pp. 460–474, 2014.
[17]
D. Ferro, V. Gripon, and X. Jiang, “Nearest neighbour search using bi-
nary neural networks,” in Neural Networks (IJCNN), 2016 International
Joint Conference on.
IEEE, pp. 5106–5112, 2016.
[18]
C. Yu, V. Gripon, X. Jiang, and H. Jégou, “Neural associative memories
as accelerators for binary vector search,” in COGNITIVE 2015: 7th
International Conference on Advanced Cognitive Technologies and
Applications, pp. 85–89, 2015.
[19]
V. Gripon, J. Heusel, M. Löwe, and F. Vermet, “A comparative study of
sparse associative memories,” Journal of Statistical Physics, pp. 1–25,
2015.
[20]
R. McEliece, E. Posner, E. Rodemich, and S. Venkatesh, “The capacity
of the hopﬁeld associative memory,” IEEE Transactions on Information
Theory, vol. 33, no. 4, pp. 461–482, 1987.
9
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-531-9
COGNITIVE 2017 : The Ninth International Conference on Advanced Cognitive Technologies and Applications

