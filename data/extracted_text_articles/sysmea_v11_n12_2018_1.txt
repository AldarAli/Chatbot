1
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
A Comprehensive Framework for High Resolution
Image-based 3D Modeling and Documentation of
Crime Scenes and Disaster Sites
Sven Becker∗, Michael Spranger∗, Florian Heinke∗, Steffen Grunert∗ and Dirk Labudde∗†
∗University of Applied Sciences Mittweida, Germany
Faculty Applied Computer Sciences & Biosciences
Email: {name.surname}@hs-mittweida.de †Fraunhofer
Cybersecurity
Darmstadt, Germany
Email: labudde@hs-mittweida.de
Abstract—Serious crime scenes or disaster sites with many victims
after natural disasters, airplane crashes or terrorist attacks
require extensive and comprehensive investigations to clarify all
circumstances leading to the event, to identify the victims and
to ﬁnd the responsible people. The results of investigations do
not only serve to prosecute the perpetrators, yet are mainly used
to develop preventive strategies and to strengthen the resilience
of the society. 3D reconstructions of crime scenes and disaster
sites provide a quick and comprehensive method to support the
work of investigators digitally. The detailed reconstruction of the
scene allows not only a long term documentation, yet also the
simulation of alternative scenarios. To create a 3D reconstruction
a large number of parameters has to be taken into account
in order to make the necessary decisions. To this point there
is no standardized procedure for the reconstruction of serious
crime scenes or disaster sites. The framework presented in this
paper can serve as a simple guide to create a 3D reconstruction.
However, it can also be easily implemented in an already existing
forensic process chain of investigative services. In this sense,
it can form the basis for a standardization and thus ensure
the comparability of different models. Additionally, a detailed
description of an application of this framework for a real crime
scene is given.
Keywords–forensic; resilience engineering; framework; 3D re-
construction; photogrammetry
I.
INTRODUCTION
A. Background
Many forensic issues require the reconstruction of crime
scenes or desaster sites in order to allow a quick and detailed
investigation. In addition to identifying victims, perpetrators
and third parties involved, it is of particular interest to gain
knowledge in order to develop preventative strategies in the
context of resilience engineering. Furthermore, information
obtained from ad hoc digital models can help to control the
situation, especially in rough terrain, by providing a tool to
support the coordination of emergency services. Using the
example of the Germanwings aircraft crash in March 2015
in the French Alps, it was shown how aerial photogrammetry
can be used to create 3D models to manage the situation and
elucidating such major damage events [1]. Another area of ap-
plication is the reconstruction of crime scenes and subsequent
simulation of possible courses of events documented in way
that can be presented in front of a court of law. Previously
used methods are based on laser scanning technology, as
shown in [2]–[8]. Disadvantages of these technologies are high
acquisition costs as well as limited mobility. As a result the use
of these technologies is only reserved for a few special forces.
However, photo equipment and smartphones have become a
nationwide standard for all emergency services. It is therefore
obvious to use these devices for the reconstruction of crime
scenes and allows an analytical 3D examination of each crime
scene and event site.
B. Literature Review
In photogrammetry, an alternative to laser scanners, images
taken with a camera are used to create 3D models. With
a software these images are aligned and the corresponding
camera positions are calculated. In a next step, 3D point
clouds are created, which are then used to create a 3D
model. For taking these pictures several camera systems are
available, yet most commonly SLR cameras are used. Even
though these cameras are used for photogrammetry in several
areas, for example in the documentation of archeological sites
[9], underwater research [10], forensics like analysis of road
accidents [11], [12] and the documentation of autopsies [13],
of particular interest for this paper are 3D reconstructions of
larger outdoor areas and buildings based on pictures taken with
SLRs. Already in 2004 Kersten et al. showed the potential of
images taken with a single-lens reﬂex camera to create 3D
models using photogrammetry [14]. At that time, they used
a combination of automated and other methods, to conduct a
reconstruction of a castle. In a later study they once again used
images of buildings taken with an SLR camera, which were
processed using open-source, proprietary and web-based soft-
ware solutions. The results were compared to laser scans and

2
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
the authors found that SLR images processed with open-source
software can deliver 3D models of comparable quality [15],
[16]. Similar results were reported by Falkingham et al., who
took photographs of archaeological objects with an Olympus
Megapixel Camera and processed them with appropriate open-
source software [17]. Furthermore, Ziegler et al. were also able
to positively evaluate the use of SLR cameras in combination
with open source software for 3D modeling in their work from
2014 [18]. Additionally, several research groups analyzed the
suitability of SLR images as well as the use of photogrammet-
ric software solutions to calculate 3D models. Kersten et al.
and Falkingham et al. focused on terrestrial images, yet also on
how pictures taken from a helicopter or UAV may be used as
an additional source. Actually, pictures taken with the help of
helicopters or UAVs, for example drones, play an essential role
in the research on photogrammetric 3D models for example
of disaster sites. In 2008 Püschel et al. showed the potential
of taking images of the Landenberg castle using drones with
SLR cameras as payload [8]. Moreover, in the review of
UAVs for photogrammetry and remote sensing by Colomina
et al. it becomes clear to the reader, that there is a great
advantage of using drones and their corresponding payloads
for the creation of 3D models of large disaster sites [19].
Furthermore, Urbanova et al. discussed in their study the use of
drone-mounted cameras for the documentation of bodies and
show possible applications for the forensic sectors, especially
the digitalization of crime scenes. Additionally, in 2015 Resig
showed that in addition to drones, other ﬂying devices, for
example a kite, can be used to collect aerial images used for 3D
models [20]. However, for the reconstruction of large disaster
sites, not only buildings, yet also larger areas play a crucial
role. Bendea et al, Naidoo et al. and Barazzetti et al. discuss in
their studies the successful application of the aforementioned
methodologies for the reconstruction of disaster sites, accidents
and crime scenes as well as search and rescue operations
[21]–[23]. Even though the aforementioned studies all made
great contributions to the research of photogrammetry for the
creation of 3D models, they only focus on methodologies,
techniques and technologies and do not provide a consistent
approach starting with the planning process up until ﬁnishing
the 3D model. Hence, Kersten et al. present a straightforward
framework for creating 3D models using photogrammetry,
which can be used to plan a scene reconstruction, yet un-
fortunately more information about how to proceed beyond
the planning process is not given [15]. Again, Püschel et al.
specify in their study how collected data is processed, yet
keep their descriptions very general and do not provide more
detailed information [8]. Zancajo-Blazquez et al. present a
comprehensive workﬂow for the 3D reconstruction of crime
scenes, but focus in their studies rather on interiors than on
larger areas [24]. Similarly, in their framework Kim et al.
and González-Aguilera et al. describe the 3D reconstruction of
disaster sites and larger areas as well as crime scenes based on
images and open-source software, yet do not give information
about what parameters are available in the software applica-
tions, which makes it difﬁcult to derive a consistent approach
[25]–[27]. Even though Gindraux et al. give a basic approach
for the software Agisoft PhotoScan in their study from 2017,
including recommendations for settings of software-speciﬁc
parameters, it is only useful for a quick reconstruction and
if the user has enough knowledge about the equipment and
the image and procedural requirements. Therefore, there is no
universal description for the 3D reconstruction of crime scenes
and disaster sites, which gives sufﬁciently detailed information
about all necessary process steps and parameters. Frameworks
that provide an overview of necessary process steps from the
planning phase to the dissemination of results are already
be found in areas such as cultural inheritance as well as
intelligence and reconnaissance. Makantasis et al. present a
framework for the 3D documentation and reconstruction of
cultural monuments from data acquisition to the processed
model. They describe possible calculation errors of the 3D
models and approaches to optimize the results in detail.
Unfortunately, they provide only general information about
the remaining process steps [28]. Xu et al. describe in their
framework the 3D reconstruction of a cultural monument based
on video recordings. In addition to the description of data
acquisition using UAVs, information about the data processing
are given by means of software applications such as VisualSfM
in combination with PMVS. The framework provides a help-
ful overview of necessary process steps, but requires expert
knowledge in many places [29]. Napolitano et al. presented a
framework for the conservation of cultural heritage sites and
provide additional information about incurred costs and the
duration of individual process steps such as data collection.
Despite the necessity of this information, unfortunately, there
are only few details given about for example equipment for
planning or parameterization for data processing [30]. Torres-
Martínez et al. describe a comprehensive framework in the ﬁeld
of reconnaissance and remote sensing from the planning phase
up to visualization possibilities of ﬁnished 3D models. Despite
the clear and comprehensive information in some places only
a few details are given such as necessary parametrization
for processing collected data. Furthermore a lot of expert
knowledge is required. However, this framework provides a
basic overview of necessary process steps [31].
C. Objectives
To the knowledge of the authors up to this date, there is
no consistent approach on 3D-reconstruction in the context of
forensic investigations. Therefore, a comprehensive framework
addressing this issue is presented in this paper It relies on pre-
vious research and applications in the ﬁeld of photogrammetric
modeling, as described in the previous section. In Section
II-A the entire framework is presented followed by a detailed
description of all modules and sub-steps. Section III shows
the reconstruction of one test scenario and one cold case
as an examples of how this framework can be implemented
in practice. In addition, the inﬂuence of various parameters
on model quality and computation time is demonstrated. The
work concludes with a brief summary and gives an outlook on
further work.

3
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
II.
COMPREHENSIVE FRAMEWORK FOR CRIME SCENE
AND DISASTER SITE RECONSTRUCTION
A. General Framework
The process of reconstructing forensically relevant items and
complex crime scenes or disaster sites is similar, which is why
the same schema can be used for all of them. The process
usually consists of four to six modules (Fig. 1), which are
selected depending on the purpose of the reconstruction and
what data is used.
-
Planning: operational coordination, strategy and re-
source planning as well as technology choice.
-
Data Collection: parameterization, data collection, data
preprocessing to ensure a uniform and consistent data
base as a requirement for the following steps.
-
VSFM – CMPMVS – MeshLab/Agisoft Processing:
generation of 3D models
-
Data Integration: integration of detailed models to sup-
plement and complete a speciﬁc professional perspective
(for example biological traces).
-
Simulation: simulation of courses of events allow the
mutual exclusion of alternative courses as well as the
development of prevention strategies.
-
Documentation: ﬁling and court-proof documentation
of all relevant model parameters, source data and results
to ensure traceability of procedures and results
The starting point is always an incident or an inquiry by
the prosecution, the defense or a judge. Subsequently, in the
planning module decisions have to be made regarding what
resources, technologies and strategies should be employed.
Afterwards, within the data collection module, different ways
are presented to collect the data and prepare it, after setting
important parameters, for the creation of the 3D model. At
this point, the decision has to be made whether to create
the 3D model using open source or proprietary software.
This decision is affected by factors like cost, training time,
functionality, ﬂexibility, portability as well as limitations in
the image format. As a result of the respective processing
module, a basic 3D model is generated. If the scene to be
reconstructed as a 3D model is a more complex scene such as
a crime scene, it has to be decided whether additional items,
for example evidence, people or objects, should be added
iteratively as 3D models. Consequently, the reconstruction of
every item again starts with the planning module. Is the 3D
model such an item it can either be integrated in an already
existing scene or it can be documented as a separate 3D
model. If no additional items have to be added, a simulation
of the course of events can be done, if required. The result
of the simulation is a realistic, complex, physically correct
and fully parameterizable 3D model with integrated animations
showing the course of events. The collected data in the form of
images, the methodical procedures and techniques, which were
used, the software-speciﬁc parameterizations as well as the
calculated 3D models and simulations have to be documented
in a way they can be used before a court of law. This can be
achieved by following the steps of the documentation module.
In the following section a detailed description of each module
will be given.
Start
End
Planning
Planning
Data 
Collection
Data 
Collection
 VSFM
CMPMVS
MeshLab
 VSFM
CMPMVS
MeshLab
Framework
Agisoft 
Processing
Agisoft 
Processing
Documentation
Simulation
Data 
Integration
Simulation
No
Yes
Objects
No
Yes
Basic 3D 
Model
Scene
Yes
No
Extended 
3D 
Model
Scene
Integration
Yes
Figure 1. Framework for photogrammetric 3D reconstruction of crime scenes
and disaster sites.
B. Planning Module
The “Planning Module” as illustrated in Fig. 2 is constituted
of three general tasks:
1)
the description of the scenario,
2)
selection of an appropriate strategy, and
3)
the orchestration of necessary equipment.
At the beginning, it is important to describe the item that is
going to get reconstructed. The item can be a single object as
part of a complex scene or the scene itself. For the latter it is

4
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Start
Target 
Description
Terrestrial
Utility 
Selection
End
Flight Vehicle 
Selection
No
Flight 
Planning
Yes
Strategy 
Selection
Payload 
Selection
Figure 2. Description of the Planning Module.
important to determine, whether the site to be reconstructed is
indoors or outdoors and other aspects that might inﬂuence the
process of collecting the data and the reconstruction, including
vegetation, buildings and possible obstacles like high-voltage
lines. Furthermore, it is important to assess the accessibility of
the site in general. With the help of the detailed description it
is possible to choose a suitable strategy for taking the pictures
or making the video. Three different strategies are possible:
linear, contour-aligned or circular (Fig. 3). In most cases taking
the pictures or making the video in a circular manner is the
best option, especially for small items or aerial images as
long as the terrain allows to do so and the ﬂight path is not
affected by any obstacles. The linear technique is especially
suitable for the reconstruction of walls (inside or outside),
facades and the reconstruction of long stretches of ground
for example streets or rivers. However, when reconstructing
buildings the contour-aligned method has many advantages,
especially for buildings with a contorted layout. After choosing
the right technique, the decision has to be made, whether
the data should be collected from the ground or with the
help of a ﬂying device. If the data is collected from the
ground one can choose without further preparations, which
hardware to use such as photography/video equipment (like
camera, objective and ﬁlters) or an automatic rotator for taking
pictures of single objects. Experiments showed when using
an automatic rotator a homogenously colored wall in the
background in combination with a stationary camera ensures a
3D object of high quality. If aerial images/videos are required,
at ﬁrst the ﬂying device has to be chosen, for example a
helicopter or a ﬁtting drone type. Afterwards, one can choose
the respective payload (e.g., camera, thermal sensors or gas
sensors) depending on the loading capacity of the ﬂying device
and the data that needs to be collected. Finally, taking into
account the description, the chosen strategy to collect the
data and the chosen ﬂying device, one can start to plan
the ﬂight using a suitable software (manufacturer-speciﬁc or
open access sources like GoogleMaps) in consideration of the
environmental conditions.
Figure 3. Overview about various strategies for data collection.
C. Data Collection Module
The process of collecting data as illustrated in Fig. 4 is
one of the most important steps during the reconstruction
process, since the decisions made here, for example about the
parameterization of the hardware used, directly inﬂuence the
quality of the generated 3D models.
1)
Type: selection of the type of recording (video and
images),
2)
Parameter setting: parameterization of hardware used,
3)
Collection: actual data acquisition,
4)
Pre-Processing: conversion to an appropriate ﬁle format
for further processing steps

5
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Data Collection
Data Collection
Type
Type
Parameter Setting
Parameter Setting
Collection
Collection
Pre Processing
Pre Processing
Start
Recording 
Type
Images
Mode 
Selection
Parameter 
Settings
Yes
Auto 
Mode
No
Data 
Acquisition
Yes
No
Data
Images
Image 
Capturing
No
Conversion
File Format 
Conversion
Yes
Framework 
Selection
End
Yes
Yes
Capturing
No
Figure 4. Overview of the data collection module.
Depending on the device used, the image or video ﬁle
format must be chosen. It is recommended to use video
recordings, because the acquisition of data is much quicker,
and to only use photographs if time is not a concern.
When taking a sequence of pictures, one has to decide,
which image recording mode is most suitable (for example
day/night, landscape, macro, car etc.). In case of using the
automatic mode, the picture taking can be started without
any further adjustments. For video recordings or when using
a mode other than automatic, depending on the hardware
it may be necessary to set up other parameters, such as
the aperture or the shutter speed, before the actual picture
taking or video making can be carried out. Furthermore, it
is recommended to set the ISO value as low as possible to
prevent additional noise in the images and, correspondingly,
use a high f-number for a sufﬁcient depth of ﬁeld. Regardless
of the mode and the selected parameters, it is recommended
to use the highest camera resolution that is possible (full
HD resolution for videos). Additionally, for objects it is
recommended to use a mode that allows taking continued
shots in order to have images with a sufﬁcient overlap.
After collected the data it must be decided, which model
generation software (open source or Agisoft PhotoScan)
should be used. However, in the case of video recordings,
single images must be extracted beforehand. Depending on
what software was chosen it has to be decided whether any
preprocessing steps are necessary, such as ﬁltering, enhancing
the image quality or converting it to another image ﬁle format.
D. 3D Disaster-Site Reconstruction
1) VSFM – CMPMVS – MeshLab: Open source software
applications are a low-cost option to create 3D objects or
complex scenarios based on images. The process to create
models using photogrammetry as presented in Fig. 5 is based
on the applications VisualSFM, CMPMVS and MeshLab and
consists of the following two general tasks:
1)
Structure from Motion: 3D model generation of single
objects or complex scenes, and
2)
Post Processing: editing and optimization of generated
models.
The ﬁrst step in creating a 3D model is to calculate a
point cloud of the aligned and overlapping photographs. For
the calculation of the point cloud one can use the software
package VisualSFM, which is mentioned in several papers
[32] [33] and was used in previous work by the authors [1]
[34]. However, before calculating the point cloud necessary
parameters have to be set such as the maximal number of
calculated features or the maximal acceptable resolution of
the photographs used. Subsequently, the process step Execute
SIFT determines reference points in the pictures, which are
saved as key point descriptors and are the basis for creating
the point cloud images. Nonetheless, for the actual 3D Scene
Reconstruction a sparse point cloud, based on a minimal
number of features will be calculated ﬁrst. This sparse point
cloud can then be used to automatically create a textured model
using the software CMPMVS, which was mentioned in several
papers [35] [18]. The process consist of the following three
steps:
1)
Execute CMVS (Clustering Views for Multi-view
Stereo): fragmentation of the sparse point cloud into
single clusters for quicker processing,
2)
Execute PMVS (Patch-based Multi-view Stereo Soft-
ware): enrichment of the sparse point cloud and calcu-
lation of a dense point cloud, and
3)
Texturing: creation of a textured mesh
An alternative would be the sub steps CMVS und PMVS,
also included as a function in VisualSFM. In this case the
resulting model would be a dense cloud, which would have to
be transformed into a textured mesh with the software Mesh-
Lab [36]. Afterwards, the mesh can be used in the subsequent
process steps. However, our own experiments have shown that
it is difﬁcult to create good models with this method, because
of the enormous parametrization. The calculated 3D model can
be optimized in MeshLab (possible output formats: *.ply, *.stl,
*.off, *.obj, *.3ds, *.vrml 2.0, *.u3d, *.x3d and *.dae). Because
of insufﬁcient quality of the pictures, it is often necessary to
edit individual areas of the model, for example by deleting
artefacts or ﬁlling in holes.
2) Agisoft PhotoScan: An alternative to an open-source
software application is the software Agisoft PhotoScan, a
proprietary software for creating 3D models using photographs
or videos. The process of creating a 3D model is illustrated in
Fig. 6 and can be divided into two tasks:

6
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 VSFM – CMPMVS – MeshLab
 VSFM – CMPMVS – MeshLab
Structure from Motion
Structure from Motion
Post Processing
Post Processing
Parameter 
Settings
3D Scene 
Reconstruction
Execute 
CMVS
Execute SIFT
Start
Ende
Key Points 
Descriptors
Sparse 
Cloud
Execute 
PMVS
Mesh
Texturing
Optimization
Editing
Figure 5. Detailed VSFM – CMPMVS – MeshLab processing module.
1)
Pre-Processing: check image quality and camera cali-
bration,
2)
Model Generation: generation of fully textured model
After importing the photographs, they are displayed as
thumbnails, which allow a visual examination of the pho-
tographs regarding their completeness and quality. An advan-
tage of Agisoft PhotoScan is the option to quantify the quality
of the photographs in a range of [0 − 1] and the manufacturer
recommends to exclude all pictures with an estimated quality
of 0.5 or less. While the photographs are imported the software
will automatically check by means of the EXIF data, whether a
camera calibration – the compensation of the distortion of the
lens – has already taken place. If the compensation is missing,
it can be calculated mathematically by entering manually
the lens parameters such as the radial distortion coefﬁcient,
the distortion center and the tangential distortion coefﬁcient.
After setting some parameters that inﬂuence the quality, the
sparse point cloud is calculated using a similar concept as
described in section II-D1. After some further optimization
steps, for example removing redundant points or eliminating
projection errors, the sparse point cloud is densiﬁed into a
dense point cloud. Before the texture is transferred in a ﬁnal
step from the photographs onto the model, the points have to be
interconnected to create a 3D mesh (possible output formats:
*.obj, *.3ds, *.3ds, *.dae, *.ply, *.stl, *.dxf, *.fbx, *.u3d, *.wrl,
*.kmz, *.pdf).
E. Data Integration
In many cases, it is necessary to integrate additional objects
in the generated 3D scenes. In particular, for the simulation
of possible courses of events, 3D models of, for example,
evidence, victims or objects used in the commission of a crime
are often essential and indispensable. In general, small objects
can be added to larger scenes using the software already
Agisoft Processing
Agisoft Processing
Pre-Processing
Pre-Processing
Model Generation
Model Generation
Start
Loading 
Images
Calibrated 
Manual 
Calibration
No
Inspecting 
Images
Yes
Aligning
Dense Point 
Cloud
3D Mesh
Texture 
Mapping
Optimize Point 
Cloud
End
Figure 6. Detailed Agisoft processing module.
mentioned in the sections II-D1 and II-D2. However, in view
of an eventual simulation it is recommendable to integrate the
objects with the simulation software Blender. The simulation
of the course of events can be started as soon as all necessary
object are integrated in the extended 3D model. If no additional
objects are needed, one can skip this part and start directly with
the documentation.
F. Simulation and Documentation
For the analysis of possible circumstances of a crime the
reconstructed 3D scenes can be used to carry out computer-
aided simulations. The simulations can aid in comparing
alternative interactions between people (e.g., perpetrator and
victim), between people and objects (e.g., weapon and victim)
or between several objects (e.g., a projectile and a wall) and
thus may help in drawing conclusions about the actual course
of events. An additional application is the simulation of the

7
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
spread of biological or chemical pollutants in accidents or
investigations of environmental crimes. For the computer-aided
simulations, a variety of software applications is available.
At this point it is recommended to use the open-source
modeling software “Blender”. The integrated physics engine
allows to visualize complex physical processes n form of
three-dimensional simulations. After creating single objects,
complex 3D scenarios and simulations, all the steps taken
including all decisions made, what values for the parameters
were chosen and, which software applications were uses as
well as the results, must be documented in a way they can
presented in front of a court of law.
III.
PROTOTYPICAL APPLICATION
In this section two scenarios demonstrating the utilization
of the proposed framework are discussed. In addition a brief
summary on technical details of used hardware is given. The
ﬁrst scenario involves the employment of various imaging
techniques to the water tower located in Mittweida and its
surrounding area, which provides an ideal test ground due to
the size of the area and the tower as well as small details
aimed to be captured accurately in the reconstruction process.
The second scenario is a real-life application to a crime scene
in Jena Lobeda. In discussing both scenarios, more details on
program usage and parameterization are given, thus extending
the elucidations on the proposed frameworks towards their
utilization in practice.
A. Technical Details
1) Drone: The drone used in our study is a MikroKopter
MK-ARF OktoXL 6S12, an eight-blade rotary wing drone for
multi-purpose utilization. In our set-up the MK OktoXL 6S12
has a maximum slant range of 4,000 m and a maximum ceiling
of 5,000 m above sea level. With fully charged batteries and
optimal weather conditions, the drone achieves a maximum
ﬂight time of about 30 minutes. Besides present weather
conditions, maximum ﬂight time is reduced by hardware
additionally mounted to the drone, such as a ﬁxed SLR camera.
Furthermore, the drone is equipped with a CMOS camera
whose video feed can be received and post-processed on the
ground. Although not-movable around the yaw axes, both the
CMOS camera and SLR camera mount can be pitched and
rolled. In combination with automated pre-planned waypoint
ﬂight and POI focusing capabilities as well as automated
camera triggering, the user is thus able to obtain images made
in-ﬂight at pre-planned positions and altitudes. Waypoints and
trigger events can be set-up and uploaded to the drone using the
maintenance and control software MikroKopter-Tool (software
version used in this study: V2.12a).
2) Camera Equipment: If the used drone systems do not
have a built-in camera, as it is the case for the MK-ARF
OktoXL 6S12, additional camera equipment is needed. In prin-
ciple, any mountable camera can be used, but factors such as
resolution, parameterization of images and data format should
be considered. The systems used should support a minimum
resolution of 1920x1080 pixels, allow a parameterization of
aperture, shutter speed, ISO and other parameters, as well
as support data formats such as RAW or JPG. The camera
used in this study is a Nikon D7100 SLR camera. Due to the
compact design and the low weight of 765 grams, this camera
is ideal as a payload for the presented drone system. The Nikon
D7100 features an optical low pass ﬁlter and 24.1 megapixel
CMOS sensor. There are 51 measuring ﬁelds available for the
autofocus system. The camera allows continuous shooting at
6 frames per second with a maximum possible resolution of
6,000 x 4,000 pixels and video recordings with a maximum
of 1,920 x 1,080 pixels. Supported ﬁle formats are RAW and
JPG.
3) Motor-driven Turntable: For capturing smaller objects
such as ensured traces, the use of an automatic, motor-driven
turntable is recommended. In this study, a model of the
company stageonair was used, which allows a 360◦ object
imagery. The diameter of the turntable is 62.5 cm, the load is
designed for 150 kg. The associated software allows individual
adjustment of the direction of rotation and speed of the
turntable as well as the number of images, which can be
realized using the automatic camera release. If the possibility
using such systems is not be given, due to its practicability
in ﬁeld utilization, lighter but not motorized turntables can be
used as an alternative.
B. Test Scenario I — Mittweida Water Tower
The area in question is well-suitable for testing the proposed
framework. The tower is about 38 m high and has a base
diameter of 10 to 16 m. The surrounding area (about 150
m in diameter) is rather ﬂat with no obstructing obstacles
present. Small details (< 50 cm) on the tower surface provide
references for qualitative evaluation of resolution and accuracy
of obtained models. In Figure 7 models of the area are shown.
Both models are based on images obtained by a drone-mounted
SLR camera. As proposed, the drone was programmed to ﬂy
a circular ﬂight path (50 m radius) at constant height above
ground level. In this case an altitude of 50 m was chosen
in order to provide a 45◦ look-down camera angle. Images
were extracted from recorded HD video material every single
second, resulting to 111 images with a resolution of 1,920
x 1,080 pixels, and processed as proposed. Although major
details are discernible, smaller features with a size of about
less than one meter are difﬁcult to identify. On a standard
desktop machine (eight 3.5 GHz CPUs, 32GB RAM, GeForce
750 GTX Titan), the 3D reconstruction process required about
1.5 to 2 hours of computation time and 1.5GB of disk space
using the Open-Source software framework, about 2 to 2.5
hours and 2GB of disk space using the Agisoft PhotoScan
framework. In a previous study, the authors showed that, in a
trade-off between computation time and model quality, models
can be generated using a single notebook within rather short
time frames (less than 30 minutes) [1], allowing the process of
drone programming, image acquisition and model processing
to be conducted in situ in case of large-scale disaster events. In
addition to the aerial images, the base of the water tower was

8
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
recorded terrestrially on a circular path around it through 57
images with a resolution of 6,000 x 4,000 pixels to demonstrate
the depiction of details. Although conducted manually, the
images could have been made using a drone at low level
ﬂight as well. The data set was processed with the proposed
parameters and frameworks. The processing time of the open-
source framework was about 12 hours with a necessary storage
space of 25GB, with the Agisoft PhotoScan framework only
about 4 hours with 10 GB of storage space. Both models are
shown in Figure 8. Finally, high detail models like a shoe
illustrated smaller objects (Fig. 9) can be included to the
model using photogrammatric imaging utilizing motor-driven
turntables.
A
B
Figure 7. Watertower of Mittweida based on 111 UAV-images processed
using the open source (A) and Agisoft PhotoScan (B) software.
C. Test Scenario II — Crime Scene Reconstruction in Jena
Lobeda
The crime scene in question addressed in this scenario
is located in a remote area of Jena Lobeda city directly
adjacent to the Saale river. 3D reconstruction of the location
showed to be demanding due to the site being difﬁcult to
access. In addition multiple obstacles made optimal drone
ﬂight geometry impossible to achieve. Thus, the site had
to be inspected beforehand and careful planning had to be
carried out. For example obstacle heights were determined
during the inspection and conﬁrmed from estimations based
on areal images. Consistent visual contact to the drone had to
be ensured as well (as speciﬁed by German law). In Figure 10
a schematic of the area is shown (subﬁgure A) and a detail with
annotated minimum and maximum safe altitudes is shown. The
location of obstacles as well as the remoteness of the crime
A
B
Figure 8. Watertower of Mittweida based on 57 terrestrial images processed
using open source (A) and Agisoft PhotoScan (B) software.
scene only allowed the drone to be setup as well as to take off
and land on the western riverbank. Furthermore, a ﬂight plan
was chosen, which provided imagery of the area of interest by
ﬂying parallel to the eastern riverbank. Multiple ﬂights legs
were conducted during this ﬂight phase, each with individual
preprogrammed camera angles and ﬂight altitudes. The ﬁnal
ﬂight phase included a 45◦ look-down camera angle to the
crime scene from the other side of the area. In Figure 10B the
chosen ﬂight path is depicted schematically. A screenshot of
the MikroKopter Tool showing the actual ﬂight path is given
in Figure 10C. During ﬂight 120 single images were extracted
from HD video recordings taken with a drone-mounted SLR
camera. In addition a Eurocopter EC-145 helicopter equipped
with a StarSAFIRE HD camera (resolution: 1,280 x 720 pixels)
was provided by the Jena police department. 250 images
were extracted from the video ﬁle recording of the helicopter-
mounted camera. Laser scanning of the crime scene was
conducted as well, but no satisfying data could be gathered
due to dense and tall-grown vegetation and laser scattering
effects caused by prevalent humidity. Hence laser scanning
showed to be inferior to photogrammetric techniques under
the present conditions.
D. Software Utilization and Parameterization
1) VSFM-CMPMVS-MeshLab: Prior to importing the im-
ages to VisualSFM, it is important ensure that the images

9
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 9. Digitized shoe (based on 30 terrestrial images) as example for a
seized trace at a crime scene.
are of proper quality. Especially in case of extracting images
from video recordings, the imagery can be prone to containing
unwanted artifacts, such as motion blur or lense ﬂares. In this
respect, the VisualSFM GUI provides a convenient presenta-
tion interface of all images loaded to the workspace from,
which images lacking proper quality can be removed. With
all images being selected, parameterizations of point cloud
calculation is conducted. It should be highlighted that the
primary parameter in calculation is the maximum number of
underlying SIFT features to be considered. The point cloud
quality can thus be increased by raising the number of SIFT
features, whereas time and memory demands however increase
signiﬁcantly. Another aspect to address is that images with
resolution greater than 3,200 x 3,200 pixels are down-scaled
by default, decreasing point cloud quality. In all presented sce-
narios this setting was changed accordingly in order to avoid
downscaling. Again these selected settings are accompanied
with signiﬁcant computational demands. Upon SIFT feature
and point cloud calculation, storing the data in the appropri-
ate ﬁle format (option “CMPMVS”) provides a CMPMVS-
compatible initialization ﬁle, which in turn can be feed to the
CMPMVS executable. After computation the resulting model
is stored in a WRL ﬁle for further processing. As the next step,
the authors recommend to employ MeshLab for inspecting the
obtained model (Fig. 11, subﬁgure A and Fig. 12, subﬁgure A)
with respect to general quality and artifacts. MeshLab can also
be employed to perform ﬁrst model corrections and removal
of existing artifacts.
2) Agisoft PhotoScan: The framework (Fig. 6) for creat-
ing 3D scenarios using the commercial software application
Agisoft PhotoScan (Professional Edition, version used in this
study: V1.4) can be subdivided into the two consecutive
Figure 10. Steps involved in drone ﬂight planning for the Lobeda crime
scene reconstruction. A: Schematic overview of the area in question.
Multiple obstacles of varying height (f.e. trees and a high voltage power
line) hinder the application of a circular ﬂight path. The crime scene area
had thus to be inspected beforehand in this case. In (B) height information
gathered from inspection and areal images is annotated (red: maximum and
minimum safe obstacle heights, blue: minimum safe height areas) and the
ﬂight is planned accordingly. Black and yellow arrows indicate the drone
ﬂight path and camera angles respectively. C: Screenshot of the MikroKopter
Tool showing the loaded ﬂight path.
modules “Pre-Processing” and “Model Generation” , which in
turn consists of speciﬁc sub-processes. The initial phase of the
“Pre-Processing” module consists of the selection and import
of the previously obtained images. As with the utilization of
VisualSFM, the user has to ensure proper ﬁle formatting and
quality. In its current version Agisoft PhotoScan also supports
the direct processing of video data. Here convenient video
import and eventual image extraction is available. Similar
to VisualSFM images are displayed in a dedicated graphical
interface for quality assessment and selection. The information
about the resolution of the images and/or EXIF data are
essential to decide whether a camera calibration is necessary.
Camera positions are computed next. However, considering

10
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 11. Saale in Jena based on UAV-images processed using the open
source (A) and Agisoft PhotoScan (B) framework.
the data collection conditions and parameterizations presented
in the “Data Collection” framework, this processed (referred
to as a manual camera calibration) can be avoided in most
cases. In the forensic context it is important that the created
model has a high quality. This can be achieved in Agisoft
PhotoScan by using the maximum values for all parameters.
However, the resulting computation afford makes it necessary
to decrease some of the values while keeping the quality of
the model high. In own experiments sets of parameters (see
Table I) have been evaluated and used in creating models with a
sufﬁcient quality for subsequent simulations. After creating the
3D models, they were edited and textured (Fig. 11, subﬁgure
B and Fig. 12, subﬁgure B).
E. Results
It was shown that helicopters are only partly suitable for
the aerial photogrammetry (Fig. 12). Possible scenarios in,
which a helicopter can be of advantage are reconstructions of
huge areas without high level of degree. Disadvantages in the
A
B
Figure 12. Saale in Jena based on helicopter images processed using the
open source (A) and Agisoft PhotoScan (B) framework.
TABLE I. Recommended speciﬁc parameter sets for the generation of
models in Agisoft PhotoScan.
task
parameter
min
max
recommended
aligning
tie point limit
0
n
10,000
key point limit
0
n
400,000
dense cloud
accuracy
lowest
highest
high
mesh
surface type
Arbitrary
face count
low
custom
default
described scenarios were next to the low resolution of the cam-
era, especially the relatively high ﬂight altitude. Because the
drone can ﬂy in lower altitudes and the payload can be chosen
ﬂexibly a high level of degree can be ensured. Additionally,
the compact design allows its operation in many scenarios.
The only limiting factor besides the weather is its limited
battery service life. Doing a reconstruction using terrestrical
photographs is suitable for the reconstruction of objects with
an extremely high level of degree and a manageable size.
However, difﬁcult terrain and obstacles (e.g., branches or high
grasses) can lead to problems.

11
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
IV.
CONCLUSION AND FUTURE WORK
The 3D reconstruction of objects, complex crime scenes
or disaster sites provide a quick and comprehensive method
to support the investigators digitally. Photogrammetry-based
methods to creating 3D models are an inexpensive alternative
to laser scanners. Even though various approaches exist, to this
point there is no standardized process for the reconstruction
of crime scenes or disaster sites. Therefore, in this paper
the authors present a comprehensive framework for the 3D
reconstruction process using terrestrial and aerial photogram-
metry. The advantage of the presented framework is twofold.
It can serve as a simple guide to create a 3D reconstruction,
yet it can also be easily implemented in an already existing
process chain, for example as used by investigative services.
Additionally, the complete framework from the planning phase
up until the ﬁnal 3D model is presented by means of a test
scenario as well as one cold case. It could be shown by the
authors, that 3D models of sufﬁcient quality for subsequent
simulations can be generated using the presented framework.
With respect to the two presented frameworks we focused on
discussing model quality and processing parameterization. In
practical use one has to consider these aspects as a trade-
off of computational demands and resulting model resolution
and quality. Future work thus has to address the deﬁni-
tion and evaluation of quality measures as well as eventual
quantiﬁcation of best-practice standards, including guidelines
for optimizing modelling strategies. Future prospects should
also consider a quantitative comparison to established laser-
scanning strategies. Finally the adaption to a wide range of
practical usage scenarios by means of utilizing alternative
software packages and/or payloads (e.g., thermal imaging or
gas sensors) could be discussed.
ACKNOWLEDGMENT
The authors would like to thank the Free State of Saxony
for funding and the Jena police department for providing the
opportunity to work on a real case.
REFERENCES
[1]
M. Spranger, F. Heinke, S. Becker, and D. Labudde, “Towards drone-
assisted large-scale disaster response and recovery,” ACCSE, 2016, pp.
5–10.
[2]
A. Marcin, S. Maciej, S. Robert, and W. Adam, “Hierarchical, three-
dimensional measurement system for crime scene scanning,” Journal of
forensic sciences, vol. 62, no. 4, 2017, pp. 889–899.
[3]
E. Hołowko, K. Januszkiewicz, P. Bolewicki, R. Sitnik, and J. Mi-
cho´nski, “Application of multi-resolution 3d techniques in crime scene
documentation with bloodstain pattern analysis,” Forensic science in-
ternational, vol. 267, 2016, pp. 218–227.
[4]
D. Dustin, E. Liscio, and P. Eng, “Accuracy and repeatability of the laser
scanner and total station for crime and accident scene documentation,”
J Assoc Crime Scene Reconstr, vol. 20, 2016, pp. 57–68.
[5]
U. Buck, S. Naether, B. Räss, C. Jackowski, and M. J. Thali, “Accident
or homicide–virtual crime scene reconstruction using 3d methods,”
Forensic science international, vol. 225, no. 1-3, 2013, pp. 75–84.
[6]
V. Pagounis, M. Tsakiri, S. Palaskas, B. Biza, and E. Zaloumi, “3d laser
scanning for road safety and accident reconstruction,” in Proceedings
of the XXIIIth international FIG congress, 2006, pp. 8–13.
[7]
U. Buck, “Laserscanning in der kriminalistik,” Zeitschrift für Geodäsie,
Geoinformation und Landmanagement, vol. 135, 2010, pp. 190–198.
[8]
H. Püschel, M. Sauerbier, H. Eisenbeiss et al., “A 3d model of
castle landenberg (ch) from combined photogrammetric processing of
terrestrial and uav-based images,” Int. Arch. Photogramm. Remote Sens.
Spat. Inf. Sci, vol. 37, 2008, pp. 93–98.
[9]
M. Douglass, S. Lin, and M. Chodoronek, “The application of 3d
photogrammetry for in-ﬁeld documentation of archaeological features,”
Advances in Archaeological Practice, vol. 3, no. 2, 2015, pp. 136–152.
[10]
K. Porter, R. Simons, and J. Harris, “Comparison of three techniques
for scour depth measurement: photogrammetry, echosounder proﬁling
and a calibrated pile,” Coastal Engineering Proceedings, vol. 1, 01 2014,
p. 64.
[11]
A. Morales, D. Gonzalez-Aguilera, M. A. Gutiérrez, and A. I. López,
“Energy analysis of road accidents based on close-range photogramme-
try,” Remote Sensing, vol. 7, no. 11, 2015, pp. 15 161–15 178.
[12]
A.
Morales,
L.
J.
Sánchez-Aparicio,
D.
González-Aguilera,
P. Rodríguez-Gonzálvez, D. Hernández-López, M. A. Gutiérrez,
and A. I. López, “A new approach to energy calculation of road
accidents against ﬁxed small section elements based on close-range
photogrammetry,” Remote Sensing, vol. 9, no. 12, 2017, p. 1219.
[13]
P. Urbanova, P. Hejna, and M. Jurda, “Testing photogrammetry-based
techniques for three-dimensional surface documentation in forensic
pathology,” Forensic science international, vol. 250, 2015, pp. 77–86.
[14]
T. P. Kersten, C. A. Pardo, and M. Lindstaedt, “3d acquisition, mod-
elling and visualization of north german castles by digital architectural
photogrammetry,” in Proc. ISPRS XXth Congress, 2004, pp. 126–131.
[15]
T. P. Kersten and M. Lindstaedt, “Generierung von 3D-Punktwolken
durch kamera-basierte low-cost Systeme ? Workﬂow und praktische
Beispiele.” Terrestrisches Laserscanning 2012, vol. 69, 2012, pp. 25–
46.
[16]
——, “Image-based low-cost systems for automatic 3d recording and
modelling of archaeological ﬁnds and objects,” in Euro-Mediterranean
Conference.
Springer, 2012, pp. 1–10.
[17]
P. L. Falkingham, “Acquisition of high resolution three-dimensional
models using free, open-source, photogrammetric software,” Palaeon-
tologia electronica, vol. 15, no. 1, 2012, p. 15.
[18]
M. Ziegler, E. Gülch, and P. Rawiel, “3D-Rekonstruktion von Ob-
jekten mittels Structure-from-Motion aus einer photogrammetrischen
Aufnahme mit den Programmen VisualSFM und CMPMVS,” in DGPF
Tagungsband, vol. 23, 2014.
[19]
I. Colomina and P. Molina, “Unmanned aerial systems for photogram-
metry and remote sensing: A review,” ISPRS Journal of Photogramme-
try and Remote Sensing, vol. 92, 2014, pp. 79–97.
[20]
M. R. Resig, “Rapid 3D Scene Reconstruction from Kite-Based Aerial
Imagery Using Open Source Structure from Motion,” Master Thesis,
George Mason University, 2015.
[21]
H. Bendea, P. Boccardo, S. Dequal, F. Giulio Tonolo, D. Marenchino,
and M. Piras, “Low cost uav for post-disaster assessment,” The Inter-
national Archives of the Photogrammetry, Remote Sensing and Spatial
Information Sciences, vol. 37, no. B8, 2008, pp. 1373–1379.
[22]
Y. Naidoo, R. Stopforth, and G. Bright, “Development of an uav for
search & rescue applications,” in AFRICON, 2011.
IEEE, 2011, pp.
1–6.
[23]
L. Barazzetti, R. Sala, M. Scaioni, C. Cattaneo, D. Gibelli, A. Giussani,
P. Poppa, F. Roncoroni, and A. Vandone, “3d scanning and imaging
for quick documentation of crime and accident scenes,” in Sensors,
and Command, Control, Communications, and Intelligence (C3I) Tech-
nologies for Homeland Security and Homeland Defense XI, vol. 8359.
International Society for Optics and Photonics, 2012, p. 835910.
[24]
S. Zancajo-Blazquez, D. Gonzalez-Aguilera, H. Gonzalez-Jorge, and
D. Hernandez-Lopez, “An automatic image-based modelling method
applied to forensic infography,” PloS one, vol. 10, no. 3, 2015, p.
e0118719.
[25]
C. Kim, H. Moon, and W. Lee, “Data management framework

12
International Journal on Advances in Systems and Measurements, vol 11 no 1 & 2, year 2018, http://www.iariajournals.org/systems_and_measurements/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
of drone-based 3d model reconstruction of disaster site,” ISPRS
- International Archives of the Photogrammetry, Remote Sensing
and Spatial Information Sciences, vol. XLI-B4, 2016, pp. 31–
33. [Online]. Available: https://www.int-arch-photogramm-remote-sens-
spatial-inf-sci.net/XLI-B4/31/2016/
[26]
D. Gonzalez-Aguilera, L. López-Fernández, P. Rodriguez-Gonzalvez,
D. Hernandez-Lopez, D. Guerrero, F. Remondino, F. Menna, E. No-
cerino, I. Toschi, A. Ballabeni et al., “Graphos–open-source software for
photogrammetric applications,” The Photogrammetric Record, vol. 33,
no. 161, 2018, pp. 11–29.
[27]
D. Gonzalez-Aguilera and J. Gomez-Lahoz, “Forensic terrestrial pho-
togrammetry from a single image,” Journal of forensic sciences, vol. 54,
no. 6, 2009, pp. 1376–1387.
[28]
K. Makantasis, A. Doulamis, N. Doulamis, and M. Ioannides, “In the
wild image retrieval and clustering for 3d cultural heritage landmarks
reconstruction,” Multimedia Tools and Applications, vol. 75, no. 7,
2016, pp. 3593–3629.
[29]
Z. Xu, T. Wu, Y. Shen, and L. Wu, “Three dimentional reconstruction
of large cultural heritage objects based on uav video and tls data,” The
International Archives of Photogrammetry, Remote Sensing and Spatial
Information Sciences, vol. 41, 2016, p. 985.
[30]
R. K. Napolitano, G. Scherer, and B. Glisic, “Virtual tours and infor-
mational modeling for conservation of cultural heritage sites,” Journal
of Cultural Heritage, 2017.
[31]
J.
A.
Torres-Martínez,
M.
Seddaiu,
P.
Rodríguez-Gonzálvez,
D.
Hernández-López,
and
D.
González-Aguilera,
“A
multi-data
source and multi-sensor approach for the 3d reconstruction and web
visualization of a complex archaelogical site: The case study of “tolmo
de minateda”,” Remote Sensing, vol. 8, no. 7, 2016, p. 550.
[32]
C. Wu. SiftGPU Implementation of Scale Invariant Feature Transform.
[Online]. Available: http://cs.unc.edu/ ccwu/siftgpu/
[33]
C. Castillo, M. James, M. Redel-Macías, R. Pérez, and J. Gómez,
“Sf3m software: 3-d photo-reconstruction for non-expert users and its
application to a gully network,” Soil, vol. 1, no. 2, 2015, p. 583.
[34]
S. Becker, J. Dreßler, K. Thiele, and D. Labudde, “Gesichtsweichteil-
rekonstruktion mithilfe einer open-source-software,” Rechtsmedizin,
vol. 26, no. 2, 2016, pp. 83–89.
[35]
M. Jancosek and T. Pajdla, “Cmpmvs-multi-view reconstruction soft-
ware,” in Proceedings of the International Symposium on 3D Data
Processing, Visualization and Transmission (3DPVT), 2008.
[36]
P. Cignoni, M. Callieri, M. Corsini, M. Dellepiane, F. Ganovelli, and
G. Ranzuglia, “Meshlab: an open-source mesh processing tool.” in
Eurographics Italian Chapter Conference, vol. 2008, 2008, pp. 129–
136.

