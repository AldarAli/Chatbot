Haptic System for Eyes Free and Hands Free Pedestrian Navigation  
 
Nehla Ghouaiel                        
ESTIA-IRIT 
F-64210 Bidart, France 
n.ghouaiel@estia.fr 
Jean-Marc Cieutat                        
ESTIA-IRIT 
F-64210 Bidart, France 
j.cieutat@estia.fr 
Jean-Pierre jessel                       
IRIT 
UMR 5505, F-31062 Toulouse, 
France 
jean-pierre@irit.fr
 
 
Abstract—Until now, Augmented Reality was mainly 
associated with visual augmentation which was often 
reduced to superimposing a virtual object on to a real world.  
We present in this document a vibro-tactile system called 
HaptiNav, 
which 
illustrates 
the 
concept 
of 
Haptic 
Augmented Reality. We use the haptic feedback method to 
send users information about their direction, thus enabling 
them to reach their destination. To do so, we use a turn by 
turn metaphor which consists of dividing the route into 
many reference points. In order to assess the performances 
of the HaptiNav system, we carry out an experimental study 
in which we compare it to both Google Maps Audio and 
Pocket Navigator systems. The results show that there is no 
significant difference between HaptiNav and Google Maps 
Audio in terms of performance, physical load and time. 
However, statistical analysis of the mental load, frustration 
and effort highlights the advantages of HaptiNav compared 
to two other systems. In the light of the results obtained, we 
present possible improvements for HaptiNav and describe 
its second prototype, at the end of this paper.  
 
Keywords; haptic navigation; augmented reality; mobile 
computing; human computer interaction.   
I. 
INTRODUCTION  
 
Until now Augmented Reality was mainly associated with visual 
augmentation, which is often reduced to superimposing a virtual 
object onto a real object [5]. However, the concept of 
Augmented Reality is not limited to sight and could be extended 
to other senses, ie., hearing, smell and touch. Panagiotis et al. 
[10] classed user experience input and output, in an augmented 
reality environment. The input may be audio, visual, tactile or 
kinaesthetic, whilst the output may only be visual, haptic or 
audio. In B.Bayart [6] presented three different existing 
taxonomies for Augmented Reality, and studied their direct 
extension in terms of augmented haptics. As a continuation, the 
Haptic Augmented Reality taxonomy was introduced and 
separated into two categories: augmented haptics and haptic 
augmentation. As with the classification of Fuchs et al. [4], 
Haptic Augmented Reality systems can be used either to 
augment existing data or to add information, referred to as 
enhanced haptics and haptic enhancing respectively. Enhanced 
haptics is defined as when the haptic modality amplifies or 
modulates a haptic datum sent back to the user. In some 
applications, it may be important to be able to touch data which 
 
are not on a human scale and which are not perceptible by direct  
contact with one of the body parts. Thus, feeling holes and 
bumps which are no larger than on a mesoscopic scale, a sort of 
haptic microscope, is an example of this. The concept of haptic 
enhancing can be summarized as scenarios where the haptic 
modality is used to send additional information to the user. 
Some researchers have explored the possibilities of transferring 
emotions through haptic interfaces. Shneiderman [13] defined a 
(computer) icon as an image, a drawing or a symbol 
representing a concept, and in 2004 S. A. Brewster et al. [7] 
introduced the notion of tacton or tactile icon which is like 
visual icons, represents a tactile concept. After analyzing the 
possibilities provided by tacton vibrations in [8], the work 
presented in [16], endeavors to go further by trying to simulate 
emotions through tactile vibrations.  
In the study presented in this paper, we explore how to use 
Augmented Reality in its haptic modality, in order to guide 
pedestrian in a new urban environment. In this context, several 
questions are raised: Is the haptic modality efficient enough to 
guide pedestrians? Is it robust enough to allow hands free and 
eyes free navigation? To answer these questions we implement a 
vibro-tactile system which illustrates the concept of haptic 
augmented reality. We use the haptic modality to send users 
informations about their directions, enabling them to reach their 
destinations. In the first Section of this paper, we describe the 
prototype developed for HaptiNav. We then present the software 
structure of HaptiNav and the algorithm used. In Section 4, we 
detail the experimental study carried out to assess performances 
of HaptiNav system, in comparison with the standard Google 
Maps Audio and with another vibro-tactile system. All of these 
systems were tested without any visual support. At the end of this 
paper, we present possible improvements for HaptiNav and 
describe the second prototype for our system.  
 
II. 
    DESIGN OF THE HAPTIC INTERFACE  
 
The Figure 2 highlights three possible prototypes for our 
system. The selection criterion consists of finding the prototype 
which provides user friendly navigation, which enable users to 
navigate hands free and eyes free.  
We carry out a comparative study to choose the prototype 
which best corresponded to our selection criterion. The first line 
in Figure 2, shows a tactile tablet designed to be placed in the 
palm of the hand. Jin et al. [9] created a tactile tablet made up of 
12 panels, each panel contains a tactor (ie., a vibrating motor). 
Their tactile tablet contains 12 vibrators, forming a 4 lines and 3 
columns matrix. In order to send to user spatial and directional 
information, T-mobile system [9] combines three vibrators. For 
instance, T-mobile system makes tactors vibrate in the first line   
330
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
Figure 1. Possible prototypes. 
 
to show the north. The main disadvantage of this prototype is 
that it needs to be held in one of the user’s hands, therefore it 
prevents hands free navigation. The second prototype is based 
on a single vibrator integrated in a mobile phone. The Pocket 
Navigator system [3] uses this prototype; it codes the direction 
which must be followed by the user in different vibration modes, 
known as tactons. The approach presented in Pocket Navigator 
uses three different rhythms to tell the user to go straight on, 
turn right, turn left or stop. As shown in the figure below, the 
system translates the action of going straight on by two short 
successive pulses, turning left by a long vibration followed by a 
short one, turning right by a short vibration followed by a long 
one and turning back by three short pulses. 
 
 
 
Figure 2. Pocket Navigator tactile diagram [3]. 
 
 We chose to develop the vibro-tactile belt prototype, 
namely to make navigation more user-friendly. As previously 
highlighted, we want users to be able to reach their destinations 
without using vision or hands. The aim behind this choice is to 
enable users to concentrate on road traffic and obstacles, rather 
than on the navigation system. For people visiting a town for the 
first time, the advantage is that it guides them whilst at the same 
time, allowing them to fully concentrate on the new 
environment. Unlike ActiveBelt [15], our system has four 
vibrators. 
 
 
 
Figure 3. Prototype of HaptiNav.  
 
III. 
IMPLEMENTATION 
 
 Our system called HaptiNav, consists of an Android 
Smartphone Galaxy S2 and an Arduino BT electronic board [1] 
as shown in Figure 4. Arduino BT is an Arduino board with an 
integrated 
Bluetooth 
module, 
thus 
enabling 
wireless 
communication with the smartphone. In order to implement the 
system’s applicative aspect (deployed on the mobile and the 
Arduino board), we use Android SDK programming interface 
(API) and Arduino software. 
 The Android development environment consists of the 
Android development tool (ADT) integrated in Eclipse. We use 
Arduino freeware to develop the application loaded on the 
Arduino BT board, which controls the vibrators and the 
microcontroller. We chose this software because it is the 
proprietary platform of the Arduino electronic card. 
 To make the mobile application and the Arduino sketch 
communicate, we use Amarino software interface. It is 
developed as part of the Android meet Arduino project [2]. 
Amarino was launched by Bonifaz Kaufmann [2] in 2009, and 
developed at the University of Klagenfurt in Austria. There is 
another tool which enables Android and Arduino to 
communicate via USB: ADK (Android Open Accessory 
Development Kit). We are unable to adopt this solution since it 
is only available for Android version 13. However, Galaxy S2 
has Android version 11. The diagram below shows the structure 
of the applicative part of our system. It is divided into three 
modules: Android, Arduino and Android Arduino interface. The 
Android application constantly calculates the difference between 
the user’s orientation and the orientation of the route’s closest 
way point. The Android application sends difference in 
orientation angle to the Arduino sketch through the Amarino 
plug-in. Consequently, the Arduino sketch activates the belts 
vibrator corresponding to the direction sent. 
IV. 
ALGORITHM IMPLEMENTED 
 
 Unlike the compass metaphor used in ActiveBelt [11], we 
use another metaphor which we shall refer to it as turn by turn 
metaphor. With the compass metaphor, the difference between 
the user’s current orientation and the destination’s orientation is  
331
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
Figure 4. Structure of the system’s applicative part. 
 
instantly calculated and then, the user is constantly redirected 
towards the final destination. The turn by turn metaphor consists 
of dividing the route into many reference points. Thus, the 
system shows the user which direction to take for each way 
point. We refer to the user’s current orientation as OC and the 
desired orientation as OD. The desired orientation OD is 
extracted from the KML file (Keyhole Markup Language), 
generated by Google Maps. Desired orientation can be found in 
the tag <heading> of the kml file. A change in orientation OT 
(the angle required to go from the current orientation to the 
desired orientation) is obtained by calculating the difference 
between OD and OC. 
 
 
Figure 5. Change of orientation. 
 
 
OT = OD−OC 
Since 
0 <= OD < 360 
and 
0 <= OC < 360 
we therefore have : 
−360 < OT < 360 
Thus, two cases can be illustrated: 
(a) When the user has to move clockwise, 
(0 <= OT < 360) 
(b) When the user has to move anti-clockwise. 
(−360 < OT < 0) 
Both cases are illustrated in figure 5. 
V. 
EXPERIMENTAL STUDY 
 
There are several systems with which we can compare our 
system described above. We omit the work presented by Lin et 
al. [12], since their system indicates only two directions (left and 
right). We chose to compare our system with standard Google 
Maps Audio (pedestrian version) and with Pocket navigator [6].  
Both applications were installed in Samsung S2 equipped with 
Android version 2.3. Google Maps Audio is a very popular 
application. Pocket Navigator is a vibro-tactile system for 
pedestrian navigation whose principle is described in the first 
Section of this paper. 
 
 
 
Figure 6. Experiment with HaptiNav 
 
A. Protocol 
In the experimental study, we carry out three experiments. 
We vary the navigation mode between the three experiments. 
The first consists of navigating in audio mode using Google 
Maps Audio. The second experiment consists of navigating with 
our own system in a vibro-tactile mode. The third one consists 
of navigating with Pocket navigator in a vibro-tactile mode. We 
keep the same route (figure 7) in the three experiments, since 
each experiment has 12 different participants. The experiments 
take place at the School of Advanced Industrial Technologies 
located at Izarbel Science Park. Each subject takes part only in 
one of the three experiments and is followed by two 
experimenters. One experimenter managed the dashboard, the 
other managed the stopwatch. Subjects were not allowed to look 
at the Smartphone’s screen or ask the experimenters questions. 
They had to walk at their usual speed which is about 1 meter per 
second. There was no learning phase prior to the experiments. 
B. Participants 
36 unpaid subjects, 18 female and 18 male, take part in the 
experimental study. All are ESTIA students, trainees or 
employees. They are aged between 22 and 39 years old (average 
=30.5). The 36 subjects are divided into three groups, each with 
12 subjects. The first, second and third group take part 
respectively in the first, second and third experiment. It is worth 
noting that none of the subjects is involved in the research work 
presented in this paper. All the users have already used a map 
and 35 out of 36 are used to using electronic navigation systems, 
such as Tom-tom. Two subjects are unfamiliar with the 
experiment’s location; the others have already been there. 
However, knowing the location was not a significant factor since 
subjects only find out the route to destination at the end of the 
experiment. 
332
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
Figure 7. Izarbel Science Park route. 
 
C. Quantitative study  
 
 
 
 
Figure 8. errors and disorientations rates. 
 
The results of the experimental study show that the three 
techniques can be efficiently used for navigating. Figure 8 
shows the number of navigation errors as well as the number of 
disorientation events. An error is recorded when subjects did not 
take the right direction with regard to a way point (different 
from that given by the navigation system). A way point is 
schematized in Figure 7 by a blue arrow.  
A disorientation event is noted when a subject stops or 
deviates from the simplified route in figure 6 for more than 10 
seconds. A disorientation event is also noted when a subject 
indicates to the experimenter that he is confused. After a 
navigation error or a disorientation event, the subject is 
redirected towards the right direction. HaptiNav and Google 
Maps Audio have a very similar number of errors, 29 and 27 
respectively. However, disorientations with Google Maps Audio 
occur twice as often as disorientations with HaptiNav. They 
happen with Pocket Navigator three times as often as with 
HaptiNav. Navigation errors of Pocket Navigator occur more 
than twice as often as navigation errors of HaptiNav and Google 
Maps audio.  Navigation errors with HaptiNav and Pocket 
Navigator, take place when the vibro-tactile signal is not 
understood or due to errors relating to the GPS or digital  
compass. Navigation errors take place with Google Maps Audio 
due to GPS errors. Figure 7 shows the number of navigation 
errors and the number of disorientation events related to the 
three studied systems. 
Figure 9 shows the comparison of averages between the 
groups in terms of errors. The smallest error recorded for 
HaptiNav is 0. However, the smallest error recorded for Pocket 
Navigator is 4. 
 
UNITE [P=POIDS],  group -> errors
errors
6
5
4
3
2
1
0
group
1
2
3
Min, Max
Moy ± Ety
Moy
 
Figure 9. Diagram of averages relating to errors. 
 
Since the ANOVA test gives a value of p equal to 0.6641 
(p=0.6641>0.5), we conclude that there is no difference between 
the errors averages of the first and the second experiment. We 
establish the fact that the performances of HaptiNav are close to 
those of Google Maps Audio. The ANOVA test gives a p value 
equal to 0.0001(p=0.0001<0.5) for the second and third 
experiment. We therefore conclude that HaptiNav is better than 
Pocket Navigator in terms of navigation errors. 
 
UNITE [P=POIDS],  group -> disorient
disorient
5
4
3
2
1
0
group
1
2
3
Min, Max
Moy ± Ety
Moy
 
Figure 10. Diagram showing average number of disorientations. 
 
Figure 10 shows the comparison of averages between the 
groups in terms of disorientation events. Since the value of p (p 
= 0.0014 < 0.05) is below the significance level of 0.05, there is 
therefore a significant difference between HaptiNav and Google 
Maps Audio. This enables us to confirm the hypothesis that 
HaptiNav is more efficient than Google Maps Audio in terms of 
reducing disorientations during navigation. The ANOVA test 
highlights a significant difference between Google Maps Audio 
and Pocket Navigator (p =0.0028 <0.05). Hence, the 
PocketNavigator system generates more disorientations events 
than Google Maps Audio. 
Since the value of p (p = 0.0014 < 0.05) is below the 
significance level of 0.05, there is therefore a significant 
difference between HaptiNav and Google Maps Audio. This 
enables us to confirm the hypothesis that HaptiNav is more 
efficient than Google Maps Audio in terms of reducing 
disorientations during navigation. 
333
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

D. Qualitative study  
Subjective estimation of workload is measured using the 
Nasa TLX Load Index test. This is a multidimensional 
evaluation test in the form of weightings and it is applied to the 
measurement of six specific load factors: mental effort, time 
spent, frustration, physical load, performance and effort. 
Subjects note the system studied by attributing a mark from 0 to 
20 for each factor. No significant difference is detected between 
HaptiNav and Google Maps Audio in terms of physical load, 
time spent or performance (p =0.68> 0.05). However, statistical 
analysis of perceived mental load, frustration and effort 
dedicated to the task, highlights the advantage of HaptiNav (p < 
0.05) with regard to Google Maps Audio and Pocket Navigator. 
These results show that the mental effort perceived by subjects 
to understand HaptiNav’s vibro-tactile instructions, is on 
average equal to 4.5 points, which did not affect the frustration 
felt with this system. However, Pocket Navigator is rated with 
the highest level of frustration. The average of frustration with 
this system is 17.84 points. 
We ask the following question to the second group of 
subjects (subjects who take part in the navigation experiment 
with HaptiNav): ―Would you accept wearing this belt to find 
your way around a town which you are visiting for the first 
time?‖ 41 % say they would not, which means that 5 of the 12 
subjects questioned would refuse to wear the HaptiNav tactile 
belt system when visiting a town for the first time. These 
participants explain that they do not like to wear a belt in town 
because they do not want to be noticed by other pedestrians. 
They suggest that this system could be smaller. We ask the 
following question to the third group of subjects (subjects who 
take part in the navigation experiment with Pocket Navigator): 
―Would you accept using Pocket Navigator to find your way 
around a town which you are visiting for the first time?‖ More 
than 80% say that they would not because Pocket Navigator is 
very inaccurate. We ask this question to the first group of 
subjects (subjects who take part in the navigation experiment 
with Google Maps Audio): ―Would you accept using Google 
Maps Audio to find your way around a town which you are 
visiting for the first time?‖ More than 80% said that they would 
only if Google Maps Audio is turned in graphic mode in 
addition to audio mode. 
E.  Discussion  
The obtained results confirm that the HaptiNav system can 
be used for hands free and eyes free navigation. We consider 
improving the quality and the intensity of vibrations in the 
second prototype of HaptiNav system. Indeed, the participants 
notice that vibrations became difficult to distinguish when the 
system’s belt is worn on top of thick clothes. This issue explains 
some of navigation errors happened with HaptiNav.  
Some participants say that they refuse to wear HaptiNav 
because of the belt. So, we will replace the belt with a bracelet 
which can be worn around the wrist [14], in our System’s 
second prototype.  
 
VI. 
CONCLUSION AND FUTURE WORK 
In this paper, we have presented an haptic system 
―HaptiNav‖ which can be used to guide users to their 
destination.The aim of our research is to determine whether 
augmented reality in haptic modality can be used to ensure 
hands-free and eyes-free navigation in an urban environment. In 
order to answer this question, we test our system with people 
with normal vision. We compare it to Google Maps Audio and 
to Pocket Navigator systems, to evaluate its performance.  
With HaptiNav, all subjects manage to reach their 
destinations. HaptiNav and Google Maps Audio systems have 
approximately the same error rate. However, HaptiNav has the 
advantage of reducing the number of disorientations. In addition, 
a statistical analysis of the mental workload, frustration and 
effort highlights the advantage of HaptiNav, compared to 
Google Maps Audio and Pocket Navigator. These results show 
the performance of HaptiNav. Some of participants said that 
they refuse to wear HaptiNav because of the belt. To overcome 
this problem, we will replace the belt with a bracelet which can 
be worn around the wrist, in our System’s second prototype. We 
envisage developing our system, so that the new prototype can 
support navigation by the visually impaired, in an urban 
environment. We intend to add a proximity sensor to this 
system, in order to detect obstacles along the route for the 
visually impaired. We also plan to add a movement sensor to 
reintroduce the perception of movement lost by visually 
impaired people.  
 
ACKNOWLEDGMENTS  
We would like to thank Mr. Octavian Curea, ENERGEA 
team director at ESTIA Research and Mr. Guillaume Terrasson, 
member of the Mechatronics team. We would also like to thank 
Julien Conon and Simon Garde two students of ESTIA.  
 
REFERENCES  
[1]  http://arduino.cc/en/main/arduinoboardbluetooth/. February 
2012.  
[2]    http://www.amarino-toolkit.net/. February 2012.   
[3]    http://www.androidguys.com/2010/04/19/pocketnavigator-
tactilepedestrian-navigation/. February 2012.   
[4]    P. Fuchs, G. Moreau, S. Donikian, 38 auteurs, Le traité de 
la réalité virtuelle, troisième édition, cinquième volume : 
« Les humains virtuels ». Les Presses de l’Ecole Mines 
ParisTech. ISBN 978-2-911256-00-4, March 2009. 
[5]    R. Azuma, Y. Baillot, R. Behringer, S. Feiner, S. Julier, and 
B. Mac- Intyre. Recent advances in augmented reality. 
IEEE Comput.Graph.Appl. Volume.21, pages : 34–47, 
November 2001.  
[6] B. Bayart. Réalité augmentée haptique : théorie et 
applications. PhD thesis, Robotique, Université d’Evry, 
December 2007.  
[7]  S.A. Brewster and L.M. Brown. Tactons: structured tactile 
messages 
for 
non-visual 
information 
display. 
In 
Proceedings of the fifth conference on Australasian user 
interface. Volume .28, pages : 15–23,  January 2004.  
[8]  L.M. Brown, S.A. Brewster, and H.C. Purchase. A first 
investigation into the effectiveness of tactons. In 
Proceedings of the First Joint Eurohaptics Conference and 
Symposium on Haptic Interfaces for Virtual Environment 
and Teleoperator Systems. Pages : 167–176, March 2005.  
[9]  M.S. Jin and J.I. Park. Interactive Mobile Augmented 
Reality system using a vibro-tactile pad. In IEEE 
International Symposium on VR Innovation. Pages: 329 - 
330 , March  2011.  
[10]   A.S.G. Panagiotis, D. Ritsos and D.P. Ritsos. Standards for 
augmented reality: a user experience. International AR 
Standards Meeting. Pages:  1-9, February 2011.  
334
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

[11] M. Pielot, B. Poppinga, and S. Boll. Pocketnavigator:   
vibro-  tactile waypoint navigation for everyday mobile 
devices. In Proceedings of the 12th international conference 
on Human Computer Interaction with mobile devices and 
Services. Pages: 423–426, September 2010.  
[12] M.W. Lin, Y.M. Chang, W. Yu, and F.E. Sandnes. 
Investigation into the feasibility of using tactons to provide 
navigation cues in pedestrian situations.  In proceeding of 
the 
20th 
Australasian 
Computer-Human 
Interaction 
Conference, OZCHI 2008: Designing for Habitus and 
Habitat. Pages: 299-302, December 2008.  
[13]  B. Shneiderman. Designing the User Interface: Strategies 
for Effective Human-Computer Interaction. Addison-
Wesley Longman Publishing Co., Inc. ISBN:0-201-16505-
8, 1986.  
[14] S. Kammoun, C. Jouffrais, T. Guerreiro, H. Nicolau, H.    
Jorge and J. Guiding. Guiding blind people with haptic 
feedback. In Pervasive 2012 Workshop on  Frontiers in 
Accessibility for Pervasive Computing. June 2012.  
[15] G.H. Yang, M.S. Jin, Y. Jin, and S. Kang. T-mobile: 
Vibrotactile display pad with spatial and directional 
information for hand-held device. The 2010 IEEE/RSJ 
International Conference on Intelligent Robots and 
Systems. Pages: 5245–5250, October 2010.  
[16] T. Yliopisto, G. Evreinov, L. Vesterinen, A. Nyman, J. 
Kminen, J. Jokinen, and D. Mathew. vsmileys: Imaging 
emotions through vibration patterns. AlternativeAccess: 
Feelings Games. Pages: 75–80, May 2005. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
335
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

