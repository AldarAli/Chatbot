Supporting Environmental Analysis and Requaliﬁcation of Taranto Sea: an Integrated
ICT Platform
Floriano Scioscia, Agnese Pinto, Filippo Gramegna,
Giovanna Capurso, Danilo De Filippis, Raffaello Perez de Vera, Eugenio Di Sciascio
DEI - Politecnico di Bari
via E. Orabona 4, I-70125, Bari, Italy
Email: ﬂoriano.scioscia@poliba.it, agnese.pinto@poliba.it, ﬁlippo.gramegna@poliba.it,
giovanna.capurso@gmail.com, danilo.deﬁlippis@poliba.it, raffaello.perez@hotmail.it, eugenio.disciascio@poliba.it
Abstract—Pollution in Taranto Sea must be monitored constantly
to detect potential issues to public health, marine biology and
economic activities in the basin. A recent environmental anal-
ysis and intervention initiative is performing a systematic and
multidisciplinary research on the area. This paper describes
the support software platform, which is being devised for the
project. It is based on the crowdsourcing paradigm and on open
source software and open data formats. The platform collects
heterogeneous data from different research units and presents
them as multiple georeferenced and timestamped information
layers, which can be combined for advanced analysis. The overall
architecture, developed tools and prospected integration of Radio
Frequency Identiﬁcation (RFID) technologies for survey sample
tracking are discussed in detail.
Keywords - Environmental monitoring; OpenSeaMap;
Crowdsourcing; Geographic Information System.
I.
INTRODUCTION
Human settlements and activities have an impact on the
coastal and marine environment. Marine litter and industrial
waste impair the sea ecosystem and economic activities based
on it. The Taranto Sea is one of the most critical situations
in Italy and an internationally relevant case study [1], due to
the Taranto basin having low water circulation [2]. Pollution
can affect the health of the local population, as well as the
wildlife and the traditionally relevant seafood breeding activity
in the area. Authorities have set up a program [3] for the
requaliﬁcation of Taranto Sea. In order to accurately plan
the best and safest intervention strategies, systematic analysis
of seawater and seabed is needed with an interdisciplinary
approach, including geological, geotechnical, physical, chem-
ical and biological investigations. Borehole sampling of the
seaﬂoor is one of the most important and sensitive activities:
stocking samples and moving them to the various analysis
laboratories requires accurate and timely tracking. Further-
more, collected data need to be stored systematically and
shared among the different research units in order to allow
the discovery of relevant patterns and correlations providing
the needed insight to operate effectively and efﬁciently.
This paper presents an integrated Information and Commu-
nication Technology (ICT) platform supporting the ongoing
environmental analysis and intervention activities in Taranto
Sea. The proposal is based on the principle of information
crowdsourcing. In the last years, this paradigm has established
itself thanks to the ICT progress increasing large-scale infor-
mation sharing possibilities. Experience with crowdsourcing
has shown that a large, loosely coupled and heterogeneous
community of users is able to produce and maintain a data
or knowledge base, which is superior in size and quality
w.r.t. a narrow team of dedicated professionals. The key for
crowdsourcing success lies in three factors: (i) a motivated
community, which globally possesses the required skills; (ii) an
ICT support platform; (iii) a decentralized organization model
respecting individual autonomy but promoting shared policies
and mechanisms to maintain high quality of information as size
grows. This work proposes an integrated ICT support platform
based on open source software and open data formats. The core
of the platform consists of tools and technologies derived from
OpenSeaMap, a worldwide collaborative project for marine
cartography creation. It allows to collect heterogeneous data
about points of the Taranto Sea basin into a uniﬁed, general
data model, where results from the different investigations
appear as multiple layers of information, wildwhich are in-
dividually georeferenced and timestamped, in order to support
space-oriented, time-oriented and attribute-oriented queries. A
further capability is to allow cross-checking data from different
domains in order to perform interdisciplinary analysis and ﬁnd
hidden correlations. The whole platform is accessible through
a uniform and user-friendly interface.
Functional requirements of large-scale research projects go
beyond those of typical cartographers or Geographic Informa-
tion System (GIS) users. First of all, security was of paramount
importance, therefore a Virtual Private Network (VPN) link
protects all communications from/to the server hosting the pro-
posed platform. Access is granted only to the staff of involved
research units. Furthermore, the need to work on massive data
required specialized tools for (i) automating repetitive entry
and import operations, and (ii) performing advanced search and
data mining. Finally, integrating sample tracking management
with the platform was studied through the use of Radio
Frequency IDentiﬁcation (RFID) technologies. The developed
and proposed solutions constitute an integrated ICT platform
to support the whole workﬂow of environmental analysis and
monitoring, from ﬁeld to laboratory. The platform is under
use in the Taranto Sea marine environment requaliﬁcation
initiative, but it provides a general solution which can be
exported to a number of analogous scenarios with minimal
or no modiﬁcation.
The rest of the paper is as follows: the next section
discusses related work. Section III describes the core compo-
197
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-505-0
UBICOMM 2016 : The Tenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

nents of the integrated crowdsourcing platform, while Section
IV provides details on data management tools. RFID-based
traceability solutions are outlined in Section V, then conclusion
is in Section VI.
II.
RELATED WORK
GIS systems allow georeferencing data, performing various
kinds of analysis and producing chart-based reports. GIS tech-
nology is constantly improving and its adoption has been rising
for several years. They have been used successfully in a wide
range of scenarios, from urban planning [4] and transportation
[5] to epidemiology [6] and environmental monitoring [7].
Nevertheless, available solutions are usually based on closed
software, which is expensive to purchase and even more to
customize to the peculiar needs of a particular project. Even
though open standards exist for georeferenced data, not all
tools support them, so requiring format conversions with the
risk of losing valuable information.
Crowdsourcing approaches and platforms propose a rad-
ically different approach, based on the contribution of large
numbers of –often volunteer– participants to solve a complex
problem or collect large bodies of information [8]. Since the
concept is quite recent, deﬁning characteristics of the crowd-
sourcing paradigm is still openly debated [9]. Anyway, the
need to facilitate global-scale collaboration directs crowdsourc-
ing ICT platforms toward open data formats and often open
source software. This creates opportunities for customizations
and extensions in order to satisfy speciﬁc requirements.
Environmental collaborative monitoring networks were
proposed in [10], combining traditional environmental mon-
itoring with the principles of crowdsourcing. They were based
on three key elements: (i) motivated citizens, (ii) sensing de-
vices and (iii) a back-end information infrastructure. Although
that work is more limited in scope w.r.t. the one proposed here,
it shares the same basic perspective.
Among map-based crowdsourcing projects, OpenSeaMap
[11] and OpenStreetMap [12] (OSM henceforth) are likely the
largest and most successful. They are worldwide collabora-
tive initiatives for shared creation of cartographic corpuses,
respectively dedicated to sea and land mapping. They are
based on World Wide Web technologies and follow open data
license models, granting anyone the right to use, expand and
modify the data. The core software components were originally
designed and developed for OpenStreetMap since 2004 and
then adopted –with some adjustments– by OpenSeaMap in
2009. They are freely available through open source software
licenses.
The complete OpenSeaMap solution comprises several
components, including:
•
PostgreSQL [13], an efﬁcient and scalable database
management system with support for georeferenced
data;
•
Overpass [14], a geospatial query engine with a very
ﬂexible language and Application Programming Inter-
face (API);
•
a Web-based interface, mainly for information visual-
ization and exploration;
•
various editors for classical computers and mobile
devices to modify and add cartography data.
Among the editors, the Java OpenStreetMap editor (JOSM)
[15] is particularly relevant: it was developed in Java tech-
Figure 1. JOSM cartography editor
nology, making it compatible with Windows, Mac OS, Linux
and other UNIX-like operating systems. It has a user-friendly
interface, depicted in Fig. 1. Finally, it adopts a modular
extensible architecture: plug-ins add new functionality without
altering the whole software tool.
An important evolution of software platforms for environ-
mental data management is the integration with automated
information gathering devices and infrastructures. Wireless
sensor networks [16], Internet of Things technologies [17] and
robotics [18] have been adopted for this purpose in various
projects, exploiting their respective peculiarities. RFID can
be similarly integrated and is the most suitable technology
whenever there is the need to track physical objects such as
collected samples.
III.
REQUIREMENTS, STRATEGY, ARCHITECTURE
Supporting the characterization of Taranto Sea for moni-
toring and requaliﬁcation requires the development of a dis-
tributed information system capable of storing and retrieving
gathered data efﬁciently. All the relevant information for the
different research units should be stored along with geospa-
tial and temporal references for each point as a stack of
superimposed informative layers, in order to ensure full data
traceability.
The support platform should also facilitate the systematic
upload of information. This functional requirement posed the
ﬁrst signiﬁcant challenge for the project, due to the sheer
size and heterogeneity of the raw data at stake. Information
characterizing the probed seabed points actually pertains to
several disciplines, including geotechnics, hydraulics, hydroge-
ology, hydrology, chemistry, marine biology and biodiversity.
Data must be properly stored, through an annotation process
capable of making their nature and information content as
explicit as possible. A second challenge involves querying
the information system for systematic and interdisciplinary
investigation. Complex environmental analysis needs searching
techniques able to discover correlations and links between
different attributes, acquired by separate teams with diverse
tools and in variable time spans. The information system must
allow users to express complex queries in order to retrieve
the point(s) matching more closely a speciﬁc set of charac-
teristics within the area of interest. This requires data mining
198
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-505-0
UBICOMM 2016 : The Tenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

capabilities and cross-checking all information produced in the
investigations carried out by the various research units.
A preliminary analysis of necessities and desiderata of
the marine environmental monitoring tasks has elicited further
important non-functional requirements:
•
security and privacy of data and communications
between users and the system, allowing to share
information only among teams involved in the project;
•
support for open and interoperable data formats, due
to the diversity of tools and platforms to be used;
•
general and ﬂexible data storage models and schemes,
to support heterogeneous sources and allow integrated
analysis;
•
high performance scalability;
•
easily usable and accessible interface, as the main
users of the platform will be scientists but not nec-
essarily trained in computer science.
The above constraints have led to consider crowdsourcing
as the most appropriate information sharing model. Global
distributed collaborative projects have already shown it to be
scalable, reliable and effective. After an extensive survey of
the state of the art, the overall proposal was based on the
OpenSeaMap, with appropriate custom extensions to meet
the speciﬁc requirements of the project. A careful audit has
shown OSM technologies and tools fully meet the identiﬁed
requirements of generality, openness, reliability, scalability,
usability and security.
The OSM data model adheres to a simple and extensible
Extensible Markup Language (XML) Schema, comprising
three basic element types: (i) nodes, i.e., single geospatial
points; (ii) ways, representing ordered sequences of nodes;
(iii) relations, grouping logically multiple nodes and/or ways.
Each element includes latitude and longitude coordinates, a
unique identity code and versioning ﬁelds. The basic model can
be extended through optional informative tags, i.e., key-value
pairs of Unicode strings of up to 255 characters. The OSM
community has deﬁned a large number of tags to describe
a wide range of entities and attributes, but new ones can be
introduced freely to meet new and unforeseen use cases. The
base model was therefore extended exploiting tags to create
a general-purpose schema for environmental data and their
geographical and temporal metadata. It is structured as follows:
•
Key: unique preﬁx currently unused in OSM (accord-
ing to the community-managed Wiki [19]), concate-
nated with a timestamp of the data entry.
•
Value: concatenation of sub-ﬁelds, each with the same
key-value structure, including:
◦
sur: survey identiﬁer (ID);
◦
sid: sample ID;
◦
sts: survey extraction time;
◦
dmi: minimum depth;
◦
dma: maximum depth;
◦
a: attribute name;
◦
v: attribute value.
Including a timestamp in the key makes each data insertion
unique, so creating a traceable record of all editing operations.
Furthermore, as data concern samples extracted at different
values of marine or seabed depth, a depth range attribute was
added in order to evolve the basic two-dimensional coordinate
system of OSM into a three-dimensional one. Finally, general-
purpose attributes obtained through laboratory analysis are
stored individually for each point on the map and each depth
range, in every survey campaign (both historical and ongoing
ones). The above data model allows spatial, temporal and
attribute-oriented queries to support a wide range of use cases.
In order to further optimize the analysis of data from dif-
ferent sources distributed at large scale, ongoing developments
are assessing the possibility of using artiﬁcial intelligence
techniques. The adoption of automated techniques to extract
high-level knowledge from raw georeferenced data through
mining algorithms allows building knowledge-based tools for
environmental monitoring, decision support and control. A
promising direction is grounded on the combination of ma-
chine learning and knowledge representation techniques [20],
exploiting non-standard inference services for the analysis of
information streams [21].
IV.
DATA EDITING TOOLS
JOSM was selected as the main data editing tool in the
proposed integrated platform. Two extensions to the basic
editor developed for the project are discussed in this section.
They are devoted to massive data import and advanced search,
respectively.
A. Survey data importer
Population of the proposed environmental information sys-
tem involves high volumes of data. Therefore it is neces-
sary to provide tools to automate the process of information
migration and integration from existing sources. In order to
facilitate data entry, the proposed platform includes a JOSM
plug-in allowing users to import georeferenced data from
common formats, such as Comma Separated Values (CSV)
and Microsoft Excel. This enables importing in the proposed
system not only data gathered in past surveys, but also the
output of analysis processes currently used by the involved
research units. Such an approach removes time-consuming and
error-prone computerized data entry procedures, and enhances
automation in laboratory workﬂows.
After opening a data source ﬁle, the tool lets the user
choose the set of records to import and select the ﬁelds of
interest, through an assisted procedure, after which loading is
performed automatically in batch mode. The main steps are as
follows:
1)
determination of ﬁelds and records to be imported
from the data source, as shown in Fig. 2;
2)
selection of coordinates and reference system, which
are mandatory features, as depicted in Fig. 3,;
3)
selection of further optional features, concerning the
survey the imported records refer to (see Fig. 4).
At the end of the assisted procedure, individual nodes are
imported. When import is complete, nodes can be viewed
directly on the map as shown in Fig. 5. When a point is selected
on the map, its records are shown in the boxes on the right
hand side of the user interface.
Initially loading procedures will involve historical data,
collected by surveys carried out on Taranto Sea sites in the
past. Subsequently, in the same way new data will be uploaded
progressively during the environmental observation period.
B. Advanced query interface
A second JOSM plug-in was developed to support retrieval
and analysis in the environmental data management platform.
199
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-505-0
UBICOMM 2016 : The Tenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

Figure 2. Import plug-in: record selection
Figure 3. Import plug-in: geographical attributes selection
It was designed to enable the advanced search of points of
interest through the combination of multiple criteria. The built-
in search functionality in JOSM provides a very basic interface,
accepting just a query string as input. This poses a high
usability barrier, since the user is required to have a working
knowledge of the OSM data model, regular expressions and
logical operators. This cannot be taken for granted for typical
users, even for scientists and engineers working on environ-
mental data. Furthermore, the limited syntax does not allow
several kinds of complex queries. A different kind of search
interface is needed, to enhance both user-friendliness and query
ﬂexibility.
As depicted in Fig. 6, the devised tool allows to express
complex queries with:
•
a ﬁlter on depth range;
•
a ﬁlter on survey metadata, particularly useful for
historical analysis;
•
user-speciﬁed ﬁlters on any attribute stored in the
system, both for number and string types.
Individual ﬁlters can be joined through logical connectors in a
simpliﬁed, fully visual fashion. The tool hides the complexity
of query composition behind a straightforward user interface.
Users are not required to master the Overpass language in order
Figure 4. Import plug-in: optional attributes selection
Figure 5. Import plug-in: result panel
to be able to compose articulated queries. After conﬁrmation,
search starts. The tool displays in red on the map the points
of interest matching the query criteria.
V.
ENVIRONMENTAL SAMPLE TRACKING VIA RFID
As tracking of seabed samples is a challenging logistic task,
the proposed ICT platform is open to the integration of RFID
technologies. RFID allows the identiﬁcation and tracking of
Figure 6. Advanced query user interface
200
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-505-0
UBICOMM 2016 : The Tenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

objects equipped with transponders (tags), by means of ﬁxed or
mobile interrogators (readers) able to read and write data. Each
tag stores a unique code which identiﬁes the object throughout
its life cycle. Compared to traditional technologies such as bar
codes, RFID has a fundamental advantages, because reading
occurs at a distance, with a range depending on the device
technical speciﬁcations, and it does not require the optical
alignment of the reader with the tag. Each RFID tag can be
equipped with an enclosure capable of tolerating the operating
environment conditions, making this technology suitable for
processes with risk of exposure to liquids or acids and saline
vapors, wide temperature changes, mechanical vibrations and
shocks.
The RFID products market is very large and diversiﬁed:
specialized offerings exist for a wide range of industrial
sectors. A complete RFID solution typically includes: (i) a set
of tags suitable for the objects to identify and the environment
in which they will be used; (ii) a set of ﬁxed readers to be
placed appropriately, to detect objects crossing passages or
presence in conﬁned environments such as trucks or shelves;
(iii) a set of portable readers, for operators in the ﬁeld who
have to read and write tags; (iv) a back-end software receiving
real-time alerts of reading events from connected readers. A
typical reading event consists of at least three elements: the
identiﬁcation code read from the tag (which uniquely identiﬁes
an object); a reader identiﬁcation code (used to determine the
location of the reading); a timestamp. In this way, the back-end
software is able to build and manage a history of all handling
operations on objects equipped with RFID tags. On this data,
rules and queries of varying complexity can be speciﬁed to
meet business needs. Furthermore, semantic annotations could
be written into RFIDs attached to objects so that a meaningful
articulated description accompanies the item the tag adheres to
[22]. In such semantic-enhanced contexts, tagged objects act as
actual resources, revealing –in addition to their identiﬁcation
code– a semantic annotation to nearby RFID readers; this
allows them to describe themselves on the ﬂy even when a
central support infrastructure is not available. Semantic-based
sensory data dissemination and query processing technologies
could enable advanced solutions for environmental monitoring.
For marine environment analysis, RFID tags must be appli-
cable to the containers of samples extracted from the seabed.
The writing of the identiﬁcation code will be performed on
the pontoon hosting the extraction tools, as soon as a sample
is placed in its container equipped with an initially empty
RFID tag. From then on, each sample will be identiﬁable and
traceable along the following planned steps:
•
from the extraction workers to the logistics supervisor,
on the pontoon;
•
from the supervisor to a staff member of an analysis
laboratory which takes charge of the samples directly
on the pontoon;
•
from the logistics supervisor on the pontoon to the
warehouse;
•
from the warehouse to analysis laboratories, where
staff takes charge of the samples in their premises,
and back again to the warehouse;
•
from a laboratory to another laboratory directly, with
possible partitioning of a sample into smaller ones
(which have to be tracked individually thereafter).
In detail, a process analysis step revealed the following re-
quirements for the proposed RFID solution:
•
a set of tags attached to the body and the cap of
each container. The container and cap identiﬁcation
codes will be strongly correlated to allow verifying
the simultaneous presence of both elements. Tags must
be resistant to sea water also in the presence of any
pollutants;
•
portable readers for writing tags on the pontoon;
•
ﬁxed readers on each warehouse gates as well as on
shelves, to monitor the arrival and the departure of
samples and to check how many samples are currently
present;
•
ﬁxed readers at the gates of each laboratory to monitor
sample movements;
•
ﬁxed or portable readers for writing operations, ded-
icated to laboratories which must be able to divide a
sample into smaller ones, each with its own container.
Among the different available RFID tag families, the
EPCglobal Generation II Ultra High Frequency (Gen2 UHF)
standard from GS1 Consortium [23] emerges as the most ad-
visable, since it guarantees secure read/write operations and the
compatibility with the majority of readers and software tools.
Examples of different types of tags available on the market
are reported in Table I. They are all enclosed in a waterproof
plastic material resistant to dust and water immersion, and
equipped with user memory for the storage of additional data
beyond the Electronic Product Code (EPC) identiﬁcation code.
TABLE I. RFID TAG EXAMPLES (PRODUCT NAMES OMITTED)
Storage
Packaging
Reading
range
Operating
temperature
Cost
EPC 128 bit,
512
bit
of
user memory
Plastic (IP 68
certiﬁcation)
up to 9
meters
From
-40
to 80 °C
e450
for
100
tags
EPC 96 bit,
512
bit
of
user memory
Thermoplastic
(IP
68
certiﬁcation)
up to 6
meters
From
-40
to 85 °C
e60 for
10 tags
EPC 96 bit,
512
bit
of
user memory
Polypropylene
(IP
68
certiﬁcation)
up to 7
meters
From
-40
to 85 °C
e32 for
10 tags
Back-end software solutions are currently divided in two
main categories: full packages to be installed and run on one’s
own computing infrastructure; Platform-as-a-Service (PaaS)
cloud offerings, with elastic computing resources and pay-
as-you-go fees. Anyway, the general architecture of RFID
software compliant with GS1 standards is shown in Fig. 7.
The main elements are:
•
Low-Level Reader Protocol (LLRP), ensuring com-
patibility with interrogator hardware from multiple
manufacturers;
•
Application Level Events (ALE) compliant middle-
ware to catch and manage RFID reading events; it
embeds a rule engine for declarative speciﬁcation of
customized business rules;
•
Electronic Product Code Information Services (EP-
CIS) for describing, gathering and sharing data as-
sociated with tracked objects, also across computer
networks.
As back-end software tools support custom extensions, real-
time RFID event tracking can be integrated with the OSM-
based software solution described in Section III, in order
201
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-505-0
UBICOMM 2016 : The Tenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

Figure 7. RFID management software architecture
to provide a uniﬁed platform for the whole environmental
analysis workﬂow, from ﬁeld to laboratory.
VI.
CONCLUSION
The paper presented an integrated ICT platform supporting
analysis of the marine environment in Taranto Sea, Italy.
The proposal is based on principles of crowdsourcing and
comprises an information system –based on OpenSeaMap–
for managing georeferenced and timestamped data, as well as
specialized tools –as plug-ins for the JOSM editor– for massive
data import and advanced queries. Furthermore, the feasibility
of integrating RFID technologies to track survey samples in
all steps of their life cycle was evaluated.
Future work includes a full validation of the proposed
platform in all activities of the environmental analysis and
requaliﬁcation program. As the devised platform provides a
general solution for many analogous scenarios of environmen-
tal monitoring and decision support, it is possible to export it
to other contexts beyond the Taranto Sea, with minimal effort.
ACKNOWLEDGMENT
Authors acknowledge the project “Accordo con il Com-
missario Straordinario per gli interventi urgenti di boniﬁca,
ambientalizzazione e riqualiﬁcazione della citt`a di Taranto”.
REFERENCES
[1]
G. Alabiso, M. Giacomini, M. Milillo, and P. Ricci, “The taranto sea
system: 8 years of chemical–physical measurements,” Biol. mar. medit,
vol. 12, no. 1, pp. 369–373, 2005.
[2]
N. Cardellicchio, S. Covelli, and T. Cibic, “Integrated environmental
characterization of the contaminated marine coastal area of taranto,
ionian sea (southern italy),” Environmental Science and Pollution Re-
search, pp. 1–4, 2016.
[3]
Commissario Straordinario per la boniﬁca, ambientalizzazione e
riqualiﬁcazione
di
Taranto.
(last
access:
2016-09-20).
[Online].
Available: http://www.commissarioboniﬁcataranto.it
[4]
I.-A. Yeo, S.-H. Yoon, and J.-J. Yee, “Development of an environment
and energy geographical information system (e-gis) construction model
to support environmentally friendly urban planning,” Applied Energy,
vol. 104, pp. 723–739, 2013.
[5]
S. Erdogan, I. Yilmaz, T. Baybura, and M. Gullu, “Geographical
information systems aided trafﬁc accident analysis system case study:
city of afyonkarahisar,” Accident Analysis & Prevention, vol. 40, no. 1,
pp. 174–181, 2008.
[6]
H. M. Khormi and L. Kumar, “Assessing the risk for dengue fever
based on socioeconomic and environmental variables in a geographical
information system environment,” Geospatial health, vol. 6, no. 2, pp.
171–176, 2012.
[7]
E. Chuvieco, I. Aguado, M. Yebra, H. Nieto, J. Salas, M. P. Mart´ın,
L. Vilar, J. Mart´ınez, S. Mart´ın, P. Ibarra et al., “Development of a
framework for ﬁre risk assessment using remote sensing and geographic
information system technologies,” Ecological Modelling, vol. 221, no. 1,
pp. 46–58, 2010.
[8]
D. C. Brabham, “Crowdsourcing as a model for problem solving
an introduction and cases,” Convergence: the international journal of
research into new media technologies, vol. 14, no. 1, pp. 75–90, 2008.
[9]
E. Estell´es-Arolas and F. Gonz´alez-Ladr´on-De-Guevara, “Towards an
integrated crowdsourcing deﬁnition,” Journal of Information science,
vol. 38, no. 2, pp. 189–200, 2012.
[10]
C. Gouveia and A. Fonseca, “New approaches to environmental mon-
itoring: the use of ict to explore volunteered geographic information,”
GeoJournal, vol. 72, no. 3-4, pp. 185–197, 2008.
[11]
OpenSeaMap. (last access: 2016-09-20). [Online]. Available: http:
//www.openseamap.org/
[12]
M. Haklay and P. Weber, “Openstreetmap: User-generated street maps,”
IEEE Pervasive Computing, vol. 7, no. 4, pp. 12–18, 2008.
[13]
PostreSQL. (last access: 2016-09-20). [Online]. Available: https:
//www.postgresql.org/
[14]
Overpass API. (last access: 2016-09-20). [Online]. Available: http:
//wiki.openstreetmap.org/wiki/Overpass API
[15]
JOSM. (last access: 2016-09-20). [Online]. Available: https://josm.
openstreetmap.de/
[16]
G. Xu, W. Shen, and X. Wang, “Applications of wireless sensor net-
works in marine environment monitoring: A survey,” Sensors, vol. 14,
no. 9, pp. 16 932–16 954, 2014.
[17]
S. Fang, L. Da Xu, Y. Zhu, J. Ahati, H. Pei, J. Yan, and Z. Liu, “An
integrated system for regional environmental monitoring and manage-
ment based on internet of things,” IEEE Transactions on Industrial
Informatics, vol. 10, no. 2, pp. 1596–1605, 2014.
[18]
M. Dunbabin and L. Marques, “Robots for environmental monitoring:
Signiﬁcant advancements and applications,” IEEE Robotics & Automa-
tion Magazine, vol. 19, no. 1, pp. 24–39, 2012.
[19]
OpenStreetMap
Wiki
-
OpenSeaMap.
(last
access:
2016-09-20).
[Online]. Available: http://wiki.openstreetmap.org/wiki/OpenSeaMap
[20]
A. Pinto, F. Scioscia, G. Loseto, M. Ruta, E. Bove, and E. Di Scias-
cio, “A semantic-based approach for machine learning data analysis,”
Semantic Computing (ICSC), 2015 IEEE International Conference on,
pp. 324–327, 2015.
[21]
M. Ruta, S. Colucci, F. Scioscia, E. Di Sciascio, and F. Donini, “Finding
commonalities in rﬁd semantic streams,” Procedia Computer Science,
vol. 5, pp. 857–864, 2011.
[22]
R. De Virgilio, E. Di Sciascio, M. Ruta, F. Scioscia, and R. Torlone,
“Semantic-based RFID Data Management,” Unique Radio Innovation
for the 21st Century: Building Scalable and Global RFID Networks,
pp. 111–141, 2011.
[23]
GS1. (last access: 2016-09-20). [Online]. Available: http://www.gs1.org
202
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-505-0
UBICOMM 2016 : The Tenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

