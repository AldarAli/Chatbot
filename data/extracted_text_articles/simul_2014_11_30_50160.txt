A Uniﬁed Framework for Uncertainty and Sensitivity Analysis of Computational
Models with Many Input Parameters
Li Gu and C. F. Jeff Wu
H. Milton Stewart School of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, Georgia
Email: lgu@gatech.edu and jeffwu@isye.gatech.edu
Abstract—Computational models have found wide applications in
simulating physical systems. Uncertainties in input parameters of
the system can greatly inﬂuence the outputs, which are studied
by Uncertainty Analysis (UA) and Sensitivity Analysis (SA).
As the system becomes more complex, the number of input
parameters can be large and existing methods for UA and SA
are computationally intensive or prohibitive. We propose a uniﬁed
framework by using a hierarchical variable selection approach to
connect UA and SA with one design. By incorporating the effect
hierarchy principle and the effect heredity principle, the method
works well especially when the number of input parameters
is large. Since the procedure requires only one design, it is
economical in run size and computationally efﬁcient.
Keywords–Uncertainty analysis; Sensitivity analysis; Screening;
Effect hierarchy principle; Effect heredity principle; Polynomial
chaos expansions.
I.
INTRODUCTION
The understanding and analysis of complex physical sys-
tems relies increasingly on computer simulations. Simulations
are based on computational models with input parameters. In
many practical situations, there is uncertainty in the choice
of the input parameter values. Thus, the understanding of
uncertainties in the input parameters and their impact on the
output becomes essential. Two major tools, Uncertainty Anal-
ysis (UA) and Sensitivity Analysis (SA), are often employed
for this purpose.
UA studies how uncertainties in the input parameters can
be mapped to uncertainties in the outputs. A typical method for
UA is Monte Carlo simulation, which works as follows. The
inputs are determined by generating random samples, follow-
ing a distribution that characterize the uncertainties of the input
parameters. Then analyze the empirical cumulative distribution
function of the outputs. See [1] for a review. SA studies how
the total output uncertainty can be attributed to uncertainties
in each input parameter. It can be done locally or globally, but
we only focus on global SA in this paper. Global SA methods
can be gathered into two categories: direct computation and
metamodeling. Methods in the ﬁrst category directly use Monte
Carlo simulation to compute sensitivity indices, like FAST [2]
and Sobol’ indices [3]; Methods in the second category build a
metamodel to replace the computational model for subsequent
statistical analysis. Various metamodels were considered for
SA, such as linear regression model [4], Gaussian process
model [5], polynomial chaos expansions [6], and smoothing
spline [7].
SA becomes complicated when the number of input param-
eters is not small. Based on the effect sparsity principle [8],
only a few (but unknown) parameters are signiﬁcant among the
many candidates. Therefore variable selection, which selects
a short list of important parameters, is applied before SA to
reduce the computational burden, and to improve the accu-
racy. Variable selection methods usually need extra function
evaluations with the use of special designs, such as systematic
fractional replicate design [9], sequential bifurcation [10], or
elementary effect method [11].
One can choose the aforementioned methods to sequen-
tially conduct UA, variable selection, and SA. However, this
will require too many function evaluations because three sepa-
rate methods and designs are involved. Especially for variable
selection, existing methods are computationally intensive or
prohibitive when the number of input parameters is very large
and the computational model nonlinear.
In this paper, we propose a new framework, where only one
design is used for the three steps. For UA, we run Monte Carlo
simulation with Latin hypercube samples [12]. The Polynomial
Chaos (PC) expansions is used as the metamodel. By keeping
the same samples for UA, a variable selection approach
designed to handle many input parameters is employed for
selecting signiﬁcant linear, nonlinear and interaction effects.
Then, the sensitivity indices (Sobol’ indices) of the selected
parameters are computed analytically from the PC coefﬁcients
without any extra function evaluations [6]. Because we need
only one design in the three steps, the proposed approach can
drastically reduce the computational time in the simulation.
Also, it works efﬁciently for high dimensions (i.e., the number
of input parameters p is large). We focus on the selection of
input parameters, which is a bridge connecting UA and SA.
Details of UA and SA can be found in [1] [6] [13], and are
hence omitted.
The rest of the paper is organized as follows. Section
II reviews the PC expansions, which is the metamodel we
use. Two variable selection methods (the sure independence
screening and the lasso) are reviewed in Section III. Section
IV introduces a hierarchical variable selection approach which
can handle many input parameters. A numerical example is
given in Section V. Concluding remarks are given in Section
VI.
276
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

II.
POLYNOMIAL CHAOS EXPANSIONS
Computational models are black-box functions, which may
have complex relationships, such as nonlinear effects and
interactions. In most cases, the linear model does not work
well as a metamodel to approximate computational models.
We consider the PC expansions as the metamodel. A brief
review of the PC expansions is given in this section.
Denote the computational model by
y = f(x),
(1)
where f is the computer code, x ∈ Rp is the p-variate input,
and y is the output. Suppose the basis Ψ(x) is constructed
by the tensor product of orthogonal polynomials φ(xj), j =
1, . . . , p, as follows:
Ψα(x) = φα1(x1) · · · φαp(xp),
(2)
where α = (α1, . . . , αp), and αj corresponds to the order
of φαj. The order of Ψα is deﬁned by |α| = Pp
j=1 αj.
(The choice of orthogonal polynomials varies depending on
the distribution of x, which can be found in [14].)
Then, the PC expansions is
g(x) =
∞
X
k=0
βkΨαk(x),
(3)
where βk are unknown coefﬁcients. The order of g(x) is
deﬁned by maxk |αk|. Note that g(x) with a proper choice of
βk converges in quadratic mean to f(x) [15]. The truncated
PC expansions involving only a ﬁnite number of bases is used
for practical computations, and is denoted by
gP (x) =
P
X
k=0
βkΨαk(x),
(4)
where P is usually decided by restricting the order of the PC
expansions.
The coefﬁcients βk in (4) need to be determined. In
stochastic mechanics, stochastic spectral methods using a
Galerkin minimization technique that minimizes the residual
in the balance equation have been widely used [16]. Such
methods are intrusive, because an ad hoc modiﬁcation on
the computer code of each problem is required. Some non-
intrusive methods were recently proposed for estimating co-
efﬁcients, where the computation can be done with a set of
deterministic model evaluations. Sudret [6] used the regression
method. A projection method was discussed in [17]. Through-
out this paper, we follow [6] to estimate coefﬁcients. Suppose
we have n inputs, x1, . . . , xn. Then β = (β0, . . . , βP )T in (4)
are estimated by minimizing the sum of squares of residuals
as follows:
ˆβ = arg min
β
n
X
i=1
(
f(xi) −
P
X
k=0
βkΨαk(xi)
)2
.
(5)
The solution is
ˆβ = (ΨT Ψ)−1ΨT y,
(6)
where Ψ denotes the n×P matrix with (i, k)th entry Ψαk(xi),
and y = (f(x1), . . . , f(xn))T .
Variable selection is necessary when the number of bases
P is large. Note that selecting input parameters is equivalent
to selecting bases in the PC expansions. Blatman and Sudret
[13] proposed a method based on stepwise regression to ﬁt the
PC expansions sequentially. A similar idea using least angle
regression was discussed in [18]. However, these methods
work only if the number of input parameters is small. For
high dimensions, if we consider the PC expansions with order
L, the number of candidate bases P = (p+L)!/p!L! increases
rapidly, and hence the computation becomes burdensome.
III.
BRIEF REVIEW OF THE SURE INDEPENDENCE
SCREENING AND THE LASSO
In this section, we review two variable selection meth-
ods for linear models with high dimensions and moderate
dimensions respectively. These methods are embedded in the
proposed approach after suitable modiﬁcations of the selection
of bases. We should point out that the choice is not unique.
That is, other variable selection methods [19] [20] [21] that
work for linear models can perform the same function in the
proposed approach.
Fan and Lv [22] proposed the Sure Independence Screening
(SIS) on linear models for high dimensional variable selection
at a relatively low computational cost. We slightly modify
the SIS here to make it applicable on selecting bases, instead
of linear predictors. It works as follows. Denote Ψαk(X) =
(Ψαk(x1), . . . , Ψαk(xn))T . Compute the correlation between
Ψαk(X) and y, which is given by
ωk = corr(Ψαk(X), y),
for k = 1, . . . , P.
(7)
Then for any given γ ∈ (0, 1), the selected model is
Aγ = {1 ≤ k ≤ P : |ωk| is among the ﬁrst [γn] largest of all},
(8)
where [γn] is the integer part of γn. γn = n − 1 or n/ log(n)
were recommended.
When the dimension is moderate, the lasso [23] is com-
monly used for variable selection, which can be modiﬁed for
selecting bases as follows:
ˆβ = arg min
β
n
X
i=1
(
f(xi) −
P
X
k=0
βkΨαk(xi)
)2
+ λ
P
X
k=0
|βk|,
(9)
where λ is a tuning parameter that controls the number of
selected bases, and usually is determined by cross-validation.
The lasso does both variable selection and shrinkage due to
the use of the L1-penalty (last term in (9)). Note that the lasso
is more accurate but computationally much heavier than SIS,
and hence is more suitable for a moderate number of candidate
bases.
IV.
HIERARCHICAL SCREENING METHOD CONNECTING
UA AND SA
Using the PC expansions as the metamodel, we propose
a variable selection approach for computational models with
many input parameters. Since the method connects UA and SA
using one design, we call it Uncertainty-Sensitivity-Analysis
(or abbreviated as USA). The USA has a hierarchical struc-
ture with multiple layers. The following two principles give
justiﬁcation for the structure of the approach.
The effect hierarchy principle and the effect heredity prin-
ciple [8] are commonly considered in variable selection, in
277
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

order to reduce computational burden due to a large number of
candidate bases. The effect hierarchy principle states that lower
order effects are more likely to be important than higher order
effects, which gives a reasonable reference for the ranking of
importances of bases. By acknowledging the effect hierarchy
principle, it becomes possible to arrange the sequence of bases
under consideration. The bases that are more likely to be
important will be assigned in lower layers, which gives them
higher priorities to be tested. The effect heredity principle
states that an interaction can be active only if one or all of
its parent effects are also active. It can signiﬁcantly reduce the
number of bases to be considered in higher layers, by selecting
candidate bases given the selected bases in lower layers. See
[24] for a discussion on the principles.
The main idea of the USA is the following. Based on
the effect hierarchy principle, the hierarchical structure of
candidate bases is built to rank the priorities of the bases with
different orders. We do variable selection from lower layers
to higher layers. In each layer, depending on the number of
candidate bases, the SIS and the lasso are used alternately.
Then, the effect heredity principle transfers the selected bases
to the next layer, thus connecting any two adjacent layers. The
diagram of the USA is sketched in Figure 1.
SIS
lasso
SIS
lasso
SIS
lasso
heredity
heredity
heredity
hierarchy
Figure 1. Diagram of the proposed approach.
Let C = {αk : k = 1, . . . , P} be the set of the candidate
bases. (Note that k = 0 is the intercept and is always selected
in the model.) Using effect hierarchy, we only consider bases
with orders no more than l in the lth layer. Thus, let M(l) =
{αk : k = ik1, . . . , ikl; and |αk| ≤ l} be the set of selected
bases in the lth layer. Based on M(l−1), we can generate a
set of candidate bases C(l) for the lth layer. The details will
be discussed later. Then, the variable selection within the layer
works in the following way. Consider the response y. We apply
the SIS on C(l) to select a subset of bases A(l)
1 with a moderate
size. Apply the lasso on A(l)
1
to obtain the set of selected bases
S(l)
1 . In the mth step, we have S(l)
m−1 selected in the previous
steps. The response is changed to the residual rm−1, which is
a vector with ith entry f(xi) − P
αk∈S(l)
m−1
ˆβkΨαk(xi). ˆβ is
estimated by (5) and (6) with current selected bases. We then
apply the SIS on C(l) \ S(l)
m−1 to get A(l)
m . The set of selected
bases S(l)
m is obtained by using the lasso on S(l)
m−1∪A(l)
m . Keep
doing this until some pre-speciﬁed stopping rule is satisﬁed.
One reasonable choice is S(l)
m−1 = S(l)
m . The set of bases that
are selected when it stops is denoted by S(l).
We connect two adjacent layers by incorporating two ver-
sions of the effect heredity principle [8] [25]. For an interaction
to be active, at least one of its parents effects is required to be
active in weak heredity; while all of its parents effects have to
be active in strong heredity. Generally, there is no guideline on
which version should be used in each speciﬁc application [24].
In the USA, the two versions are jointly utilized in a natural
way for the purpose of simplifying screening without losing
much accuracy. The move between two layers consists of two
parts. First, at the beginning of each layer, the set of candidate
bases C(l) is generated by expanding M(l−1) following weak
heredity, which is given by
C(l)
=
{αk : ∃ α′ ∈ M(l−1), α′′ ∈ C s.t. α′ + α′′ = αk;
and |αk| ≤ l}.
(10)
The reason is that it is very likely that some of the parent
effects of signiﬁcant interactions are not signiﬁcant in lower
layers. Second, at the end of each layer, we expand S(l)
following strong heredity. Denote the expanded set by
D(l)
=
{αk : ∃ α′, α′′ ∈ S(l) s.t. α′ + α′′ = αk;
and |αk| ≤ l}.
(11)
Then use the lasso on D(l) to screen and ﬁt with the response
y. By doing so, the interactions between signiﬁcant bases,
which are likely to be signiﬁcant but have been screened out
by a quick variable selection method like the SIS can be re-
examined by a more elaborate and accurate variable selection
method like the lasso. Denote the set of the ﬁnal selected bases
by M(l), which will be used to generate C(l+1) in the next
layer. The USA stops when it ﬁnishes the highest layer L.
The choice of L is straightforward. Note that the per-
formance of the USA increases as L increases, so does the
computational burden. Thus, one can keep increasing the
number of layers until enough data is explained by the selected
model (which can be assessed by some criterion, such as the
coefﬁcient of determination, the root-mean-square error, and
so on) or the computational resource runs out. Based on our
experience, L = 3, 4, 5 are good choices.
The USA is summarized by the following pseudo-code.
Set M(0) = ∅
for l = 1, 2, . . . , L
Expand M(l−1) by (10) to obtain C(l)
Set r0 = y and S(l)
0
= ∅
for m = 1, 2, . . .
Set rm−1 to be the response
Apply the SIS on C(l) \ S(l)
m−1 to obtain A(l)
m
Apply the lasso on S(l)
m−1 ∪ A(l)
m to obtain S(l)
m
endfor if converge; denote the last S(l)
m by S(l)
Expand S(l) by (11) to obtain D(l)
Set y to be the response
Apply the lasso on D(l) to obtain M(l)
endfor
278
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

V.
A SIMULATION STUDY
We use the Morris function [11] to study the performance
of the USA.
y
=
β0 + P20
i=1 βiwi + P20
i<j βijwiwj
+ P20
i<j<l βijlwiwjwl + P20
i<j<l<s βijlswiwjwlws,
(12)
where
wi =

2(1.1Xi/(Xi + 0.1) − 0.5),
for i = 3, 5, 7,
2(Xi − 0.5),
otherwise,
(13)
and Xi ∼ U(0, 1). The βi are assigned as
βi = 20,
for i = 1, . . . , 10,
βij = −15,
for i, j = 1, . . . , 6,
βijl = −10,
for i, j, l = 1, . . . , 5,
βijls = 5,
for i, j, l, s = 1, . . . , 4.
(14)
The remaining ﬁrst- and second-order coefﬁcients are inde-
pendently generated from a standard Gaussian distribution; the
remaining third- and fourth-order coefﬁcients are zero.
The true model has 20 Xi input parameters, out of which
the ﬁrst ten are signiﬁcant. 980 dummy parameters were added.
So we had p = 1000 parameters in total. n simulations were
conducted each time. The design matrix was an arbitrary Latin
hypercube design with n rows and p columns.
For the SIS, [γn] was set to be [n/ log(n)] = 81. For
the lasso, the number of selected bases was chosen by cross-
validation. We use one simulation with n = 500 to illustrate
how the USA works. Step-by-step details are given in Table
I. For simplicity, only the results in the ﬁrst and the ﬁnal
iterations in each layer are listed. From Table I, all signiﬁcant
input parameters had been already selected in the third layer.
Although we can still try more layers to get better performance
of the PC expansions, the PC expansions with order three can
explain more than 90% variation of the data [13]. Thus, we
chose three as the highest layer L in the example. Another
important point is that strong heredity at the end of the second
and third layers played a key role in selecting signiﬁcant bases
and removing insigniﬁcant bases.
The results of 20 replications with n = 300, 500, 1000
are listed in Table II, which show the average values of the
numbers of true positives and false positives. Standard devi-
ations are given in parentheses. The sensitivities indices can
be computed analytically after all signiﬁcant input parameters
are found. See [13].
TABLE II. RESULTS FOR MORRIS FUNCTION WITH p = 1000
n
True Positives
False Positives
300
7.11(1.66)
9.58(4.05)
500
9.26(0.87)
2.95(2.83)
1000
10(0)
1.36(0.56)
By comparing the results with different n, it is clearly
seen that as n increases, the performance became better. With
n = 500 runs, the USA can identify almost all signiﬁcant
input parameters. The false positives, at the same time, were
very low. Moreover, the USA exactly identiﬁed all signiﬁcant
variables each time when n = 1000, which is still a light
computation.
We compare the performance of the USA with that of
the Morris method [11]. The Morris method requires at least
r(p + 1) function evaluations, where r is the number of levels
of each input parameter. Based on the results of 20 replications,
the Morris method identiﬁed 99.5% signiﬁcant variables when
r = 4, n = 4004. When r = 2, n = 2002 (the minimal
number of runs required), only 75.5% signiﬁcant variables
were identiﬁed. In contrast, the USA with n = 1000 performed
perfectly. Considering the design used in the Morris method
is a special design, which cannot be used for neither UA nor
SA, the USA is much better than the Morris method in terms
of saving functional evaluations.
VI.
CONCLUSION
In this paper, we have proposed a hierarchical variable
selection dubbed the USA, for computational models with
many input parameters. The major feature of the USA is
that it uniﬁes UA and SA with the use of one design, which
signiﬁcantly reduces the number of simulations required. By
incorporating the effect hierarchy principle and the effect
heredity principle, the number of candidate bases in each layer
is reduced to an acceptable level for variable selection. By
applying the SIS and the lasso alternately, the USA balances
the computation and the performance. The performance was
shown with a numerical example.
As we mentioned in Section II, the coefﬁcients in the PC
expansions can be estimated in several ways. Although we
only consider the regression method, the USA is independent
of the method for estimating coefﬁcients, and hence can be
extended to other methods, which is left for future research.
(Note: An expanded version of this paper, including an
application to building energy simulation, will be forthcoming
as a joint work with Yuming Sun and Godfried Augenbroe.)
ACKNOWLEDGMENT
This work is supported by NSF-EFRI-SEED Award
1038248 and DOE grant DE-SC 0010548.
REFERENCES
[1]
J. C. Helton and F. J. Davis, “Latin hypercube sampling and the
propagation of uncertainty in analyses of complex systems,” Reliability
Engineering & System Safety, vol. 81, no. 1, 2003, pp. 23–69.
[2]
R. Cukier, C. Fortuin, K. E. Shuler, A. Petschek, and J. Schaibly, “Study
of the sensitivity of coupled reaction systems to uncertainties in rate
coefﬁcients. i theory,” The Journal of Chemical Physics, vol. 59, no. 8,
1973, pp. 3873–3878.
[3]
I. M. Sobol’, “Sensitivity estimates for nonlinear mathematical models,”
Mathematical Modeling and Computational Experiment, vol. 1, no. 4,
1993, pp. 407–414.
[4]
A. Saltelli, K. Chan, and E. M. Scott, Sensitivity analysis.
Wiley New
York, 2000, vol. 134.
[5]
J. E. Oakley and A. O’Hagan, “Probabilistic sensitivity analysis of
complex models: a bayesian approach,” Journal of the Royal Statistical
Society: Series B (Statistical Methodology), vol. 66, no. 3, 2004, pp.
751–769.
[6]
B. Sudret, “Global sensitivity analysis using polynomial chaos expan-
sions,” Reliability Engineering & System Safety, vol. 93, no. 7, 2008,
pp. 964–979.
[7]
S. Touzani and D. Busby, “Smoothing spline analysis of variance
approach for global sensitivity analysis of computer codes,” Reliability
Engineering & System Safety, vol. 112, 2013, pp. 67–81.
[8]
C. F. J. Wu and M. S. Hamada, Experiments: planning, analysis, and
optimization.
John Wiley & Sons, 2009, vol. 552.
279
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

TABLE I. ILLUSTRATION OF THE USA WITH n = 500 AND p = 1000
Layer
Stage
Bases
1
Weak heredity
N/A; C(1) = 1000 bases
1
First: SIS
A(1)
1
= 81 bases
1
First: Lasso
S(1)
1
= 19 bases: 3, 5, 7, 8, 9, 10,
93, 203, 284, 286, 362, 434, 510, 511, 623, 815, 896, 940, 941
1
Last: SIS, Lasso
S(1) = 17 bases: 3, 5, 7, 8, 9, 10,
93, 203, 284, 286, 362, 434, 510, 511, 556, 896, 966
1
Strong heredity
N/A; M(1) = S(1)
2
Weak heredity
C(2) = 16711 bases
2
First: SIS
A(2)
1
= 81 bases
2
First: Lasso
S(2)
1
= 23 bases: 5, 7, 8, 9, 10, 286, 1&3, 1&5, 2&5, 3&4, 3&6, 4&6
161&284, 163&556, 167&896, 198&284, 198&544, 238&284,
284&858, 284&885, 510&592, 510&966, 673&792
2
Last: SIS, Lasso
S(2) = 19 bases: 5, 7, 8, 9, 10, 286, 1&3, 1&5, 2&5, 3&4, 3&6,
93&108, 161&284, 163&556, 167&896,
198&284, 238&284, 510&592, 510&869
2
Strong heredity
M(2) = 16 bases: 7, 8, 9, 10, 1&2, 1&3, 1&5, 1&6,
2&4, 2&5, 2&6, 3&4, 3&6, 4&6, 7&7, 163&556
3
Weak heredity
C(3) = 15744 bases
3
First: SIS
A(3)
1
= 81 bases
3
First: Lasso
S(3)
1
= 21 bases: 7, 8, 9, 10,
1&2, 1&3, 1&4, 1&5, 1&6, 2&4, 2&5, 2&6, 3&4,
3&6, 4&5, 4&6, 5&6, 7&7, 3&4&421, 5&574, 7&32
3
Last: SIS, Lasso
S(3) = 16 bases: 7, 8, 9, 10,
1&2, 1&3, 1&4, 1&5, 1&6, 2&4, 2&5, 2&6, 3&4, 3&6, 4&6, 7&7
3
Strong heredity
M(3) = 20 bases: 7, 8, 9, 10,
1&2, 1&3, 1&4, 1&5, 1&6, 2&4, 2&5, 2&6, 3&4,
3&6, 4&6, 7&7, 3&4&5, 3&5&5, 7&7&7, 10&10&10
[9]
S. C. Cotter, “A screening design for factorial experiments with inter-
actions,” Biometrika, vol. 66, no. 2, 1979, pp. 317–320.
[10]
B. Bettonvil, Detection of important factors by sequential bifurcation.
Tilburg University Press, 1990.
[11]
M. D. Morris, “Factorial sampling plans for preliminary computational
experiments,” Technometrics, vol. 33, no. 2, 1991, pp. 161–174.
[12]
M. D. McKay, R. J. Beckman, and W. J. Conover, “Comparison of
three methods for selecting values of input variables in the analysis of
output from a computer code,” Technometrics, vol. 21, no. 2, 1979, pp.
239–245.
[13]
G. Blatman and B. Sudret, “Efﬁcient computation of global sensitivity
indices using sparse polynomial chaos expansions,” Reliability Engi-
neering & System Safety, vol. 95, no. 11, 2010, pp. 1216–1229.
[14]
D. Xiu and G. E. Karniadakis, “The wiener–askey polynomial chaos
for stochastic differential equations,” SIAM Journal on Scientiﬁc Com-
puting, vol. 24, no. 2, 2002, pp. 619–644.
[15]
C. Soize and R. Ghanem, “Physical systems with random uncertainties:
chaos representations with arbitrary probability measure,” SIAM Journal
on Scientiﬁc Computing, vol. 26, no. 2, 2004, pp. 395–410.
[16]
R. G. Ghanem and P. D. Spanos, Stochastic ﬁnite elements: a spectral
approach.
Springer, 1991, vol. 41.
[17]
T. Crestaux, O. Le Maˆıtre, and J.-M. Martinez, “Polynomial chaos
expansion for sensitivity analysis,” Reliability Engineering & System
Safety, vol. 94, no. 7, 2009, pp. 1161–1172.
[18]
G. Blatman and B. Sudret, “Adaptive sparse polynomial chaos expan-
sion based on least angle regression,” Journal of Computational Physics,
vol. 230, no. 6, 2011, pp. 2345–2367.
[19]
J. Fan and R. Li, “Variable selection via nonconcave penalized like-
lihood and its oracle properties,” Journal of the American Statistical
Association, vol. 96, no. 456, 2001, pp. 1348–1360.
[20]
H. Zou, “The adaptive lasso and its oracle properties,” Journal of the
American Statistical Association, vol. 101, no. 476, 2006, pp. 1418–
1429.
[21]
E. Candes and T. Tao, “The dantzig selector: statistical estimation when
p is much larger than n,” The Annals of Statistics, 2007, pp. 2313–2351.
[22]
J. Fan and J. Lv, “Sure independence screening for ultrahigh dimen-
sional feature space,” Journal of the Royal Statistical Society: Series B
(Statistical Methodology), vol. 70, no. 5, 2008, pp. 849–911.
[23]
R. Tibshirani, “Regression shrinkage and selection via the lasso,”
Journal of the Royal Statistical Society: Series B (Methodological),
vol. 58, no. 1, 1996, pp. 267–288.
[24]
M. Yuan, V. R. Joseph, and Y. Lin, “An efﬁcient variable selection
approach for analyzing designed experiments,” Technometrics, vol. 49,
no. 4, 2007, pp. 430–439.
[25]
H. Chipman, “Bayesian variable selection with related predictors,”
Canadian Journal of Statistics, vol. 24, no. 1, 1996, pp. 17–36.
280
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

