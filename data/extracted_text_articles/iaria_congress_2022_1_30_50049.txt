Spatial and Temporal Registration of Asynchronous Multi-Sensors for Minimally 
Invasive Surgery Application 
Ali T. Alouani 
Department of Electrical and Computer Engineering 
Tennessee Technological University 
Cookeville, USA 
aalouani@tntech.edu
Uddhav Bhattarai 
Department of Electrical and Computer Engineering 
Tennessee Technological University 
Cookeville, USA 
ubhattara42@students.tntech.edu 
Abstract— Current Minimally Invasive Surgery (MIS)
technology, although advantageous compared to open cavity 
surgery in many aspects, has limitations that prevent its use for 
general purpose MIS. This is due to reduced dexterity, cost, 
and required 
complex 
training 
of 
the 
currently 
practiced technology. The main challenge in reducing the cost 
and amount of training is to have an accurate inner body 
navigation advisory system. As a first step in making 
minimally invasive surgery affordable and more user-
friendly, quality images inside the patient as well as the 
surgical tool location should be provided automatically and 
accurately in real-time.  The second step will be to provide the 
surgeon with an inner body Global Positioning System (GPS) 
like an advisory navigation system. This paper focuses on the 
first step: providing real-time information needed by the
surgeon.  This consists of real-time temporal and spatial 
calibration of heterogeneous asynchronous sensors that 
provide enough information needed to safely carry out MIS. 
The real-time asynchronous sensors registration algorithm 
has 
been successfully 
tested 
in 
the 
lab 
using 
a 
mannequin.  The experimental 
temporal 
and 
spatial 
registration 
showed promising success for real-time tracking 
of the surgical tool as well as real-time display of the 2D 
information provided by the videoscope. 
Keywords-Sensors registration, Spatio-temporal calibration, 
Computer-assisted surgery, Multisensor System.
I.
INTRODUCTION
MIS has distinct merits of faster recovery, shorter 
hospital stays, less pain, and decreased scarring. However, 
restricted visualization of the operative site, minimal 
accessibility, and reduced dexterity has increased the 
challenges of its implementation. Image Guided Surgery 
(IGS) during MIS will help solve such problems and 
improve safety and accuracy to a significant level [1]. Da 
Vinci surgical robot is the first surgical robot approved by 
FDA for commercial use in hospitals. This sophisticated 
high-end 
system 
has 
high procurement 
cost, and 
specialized training requirements for the surgeon [2]. 
Computed 
Tomography 
(CT) 
or 
Magnetic 
Resonance 
Imaging 
(MRI) 
provide 
high 
quality 
preoperative images of the inside of the patient body. 
Once the surgery starts, preoperative images can no 
longer 
be 
relied 
on 
because 
the 
intraoperative 
environment changes continuously due to manipulation 
by surgeon or organ movement. Therefore, the surgeon 
needs to rely on inner body real time images. An Inner Body 
GPS (IGPS) is also needed to safely and accurately help 
the surgeon guide the surgical tool to its desired surgery 
location. The IGPS requires pre-operative CT/MRI, real-time 
image from videoscope, and navigation sensor(s). The 
navigation sensors   locate the position of the surgical tool, and 
the videoscope helps the surgeon maneuver around body 
organs along the path predetermined using preoperative 
CT/MRI images. Since the videoscope and navigation sensors 
are heterogeneous, they have different data rate, and provide 
measurement/information in their local coordinate frame 
using their local time clock. Spatial and temporal registrations 
of such sensors are needed as a prerequisite to the success of 
MIS.  The spatial registration represents the spatial 
coordinates of all the sensors in an absolute coordinate frame 
while the temporal registration represents all the sensors data 
in a common time reference. 
Hybrid spatial calibration uses fusion of spatial 
information from more than one sensor to assist the surgeon 
during 
MIS 
operation. 
While 
implementing 
two 
heterogeneous 
sensors, 
researchers 
leveraged 
fused 
information from intraoperative images (Laparoscopic 
Ultrasound (LUS)/Endoscope) and either preoperative images 
(CT/MRI) [3] or navigation system (Electromagnetic 
Tracking System (EMTS)/Optical Tracking System (OTS)) 
coordinates [4] - [6], [10]. The information gathered from two 
sensors is not enough for MIS. Preoperative images are the 
only reference imaging technique to visualize the complete 
patient body. Surgeon always needs to have a visualization of 
where he/she is heading inside the patient body along with the 
position and orientation of the endoscope. On the other hand, 
it is necessary to have a navigation system connected to 
intraoperative imaging system to guide the endoscope in 
CT/MRI coordinate reference frame.  
From an information point of view, MIS requires at least 
three heterogeneous sensors:  Two for imaging (pre-operative 
and intra-operative) and one for navigation.  Fakhfakh et. al. 
proposed an automatic registration of pre- and intra-operative 
images with OTS embedded ultrasound probe, and 
preoperative CT scan [7]. The reconstructed 3D image from 
2D ultrasound slices was registered with the preoperative CT 
using principle axes of inertia and the Iterative Closest Point 
robust (ICPr) algorithm. ICP suffers from being trapped in 
local minima unless a good initial guess is provided. The use 
of Hand-Eye calibration for rigid registration among robotic 
arm, tracking devices (EMTS/OTS), and imaging devices 
(Endoscope/Laparoscopic camera) was reported in [6], [8]. 
Reference [8] implemented network time protocol (NTP) for 
9
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-017-9
IARIA Congress 2022 : The 2022 IARIA Annual Congress on Frontiers in Science, Technology, Services, and Applications

temporal data synchronization. In order to calculate the 
optimum spatial and temporal transformation, [9]-[11] 
integrated LUS, rigid oblique viewing endoscope, OTS, 
EMTS, and MRI using  linear least square and Levenberg-
Marquardt iterative algorithm.  Since the whole distortion 
correction from the metallic objects in EMTS was based on 
OTS, the magnetic distortion correction mechanism may 
provide false correction vector when the line of sight (LOS) 
for OTS is blocked [11]. Furthermore, the system was 
modeled for static distortion [10], [11]. Hence, the correction 
vector would be redundant if the distortion in the vicinity 
changes during surgery. Intraoperative imaging with LUS 
suffers from shadowing, multiple reflections, low signal-to-
noise ratio, and the requirement of expertise and training of 
surgeon [12].  For temporal calibration, it is assumed that the 
tracker with higher measurement rate has acquisition 
frequency multiple of lower one and the data is processed at 
the measurement speed of slower tracking device. Although, 
[6], [7], [13] used multiple asynchronous sensors, 
synchronization of data from such sensors has been 
overlooked.  This inhibits the correct and effective use of 
asynchronous sensors in high accuracy demanding MIS 
system in real-time.    
In addition to MIS, the use of heterogeneous sensors is 
desirable in different areas including robotics and automation 
where additional data complement and enhance the available 
information and assist to make more informed decisions. Such 
sensor systems may involve different imaging and navigation 
sensors such as cameras, Inertial Measurement Units (IMUs), 
LIDAR for robotic navigation, and object detection and 
tracking [14] - [16]. However heterogeneous sensors may use 
different data acquisition systems and may have a different 
data acquisition rate and use a dedicated local processor clock 
when reporting the measurements. A prerequisite for correct 
sensor fusion is the temporal alignment of different sensor 
data such that the data provided by the different asynchronous 
sensors are recorded using the same time reference.  
The main contribution of this paper is providing a real-
time spatial and temporal registrations of heterogeneous 
asynchronous sensors in an absolute spatial  
coordinate frame and a common time reference. The process 
will be called spatiotemporal registration. Performing 
temporal sensor registration is crucial given the dynamic 
changes of the surgery path to account for body organs 
movements in order to navigate safely toward the surgery 
location. In prior work, the authors performed offline spatial 
calibration between Laser Range Scanner (LRS), EMTS, and 
Camera with promising accuracy [17], [18]. LRS was used to 
emulate CT/MRI images. LRS-EMTS calibration was 
performed using Horn’s absolute orientation method [19], 
while the camera calibration was achieved using normalized 
Direct Linear Transform (DLT) algorithm [20]. These 
registrations are crucial for real time path planning.  
This paper is organized as follows. Section II discusses the 
proposed real-time spatial and temporal heterogeneous and 
asynchronous sensors registration. Section III discusses the 
accuracy obtained using experiments conducted in the lab. 
Section IV contains conclusions and discusses future work.    
II.
PROPOSED SPATIAL AND TEMPORAL CALIBRATION
The preoperative CT/MRI provides 3D images of the 
patient to determine the inner body 3D location where the 
surgery is to take place (desired destination). The preoperative 
images can also be used for 3D path planning to reach the 
desired destination using the shortest path that has minimal 
number of obstacles such as bones or body organs. In this 
work, LRS  [21] provides preoperative 3D scan , the 
videoscope provides real time high quality images, and the 
EMTS provides pose (position and orientation)of the surgical 
tool/videoscope inside the human body as shown in Figure 1. 
The navigation sensor used was NDI Type-2 6DOF sensor for 
Aurora EMTS with a measurement frequency of 40Hz [22]. 
According to NDI, the accuracy is 0.8 mm for position and 
0.7degree for orientation [22]. The EMTS has been 
thoroughly tested and it has been found that 300 series 
stainless steel, aluminum, and titanium does not affect EMTS 
performance [12]. The third sensor was the Go 5000C series 
color camera from JAI Corporation [23], with 5 amegapixel 
resolution and an image acquisition rate of 61.2 frames/sec. 
 
In this paper, the LRS coordinate frame was selected as 
the absolute coordinate reference frame so that spatial 
information from camera and EMTS can be transformed and 
analyzed in LRS coordinate system, as shown in Figure 1. 
The location of the videoscope in EMTS coordinate reference 
frame can be determined using the camera registration 
procedure [20]. The real time position of the camera, planned 
path, and the desired inner body destination location are 
represented in the LRS coordinate reference frame. The 
surgical tool can be moved to the destination correctly using 
real time feedback from the EMTS attached to the camera. It 
is worth noting that in clinical applications, a much smaller 
camera will be used instead of the Go 5000C.  However, the 
proposed sensors registration technique can be applied to 
accommodate any camera.  All information that is needed is 
their intrinsic and extrinsic parameters. While conducting the 
experiments it is assumed that the EMTS pose measurement 
was not interfered with by metallic objects in the room, 
camera system, or mount for the LRS system. It is also 
Figure 1.  Heterogeneous Asynchronous Sensors Used for Inner Body 
Navigation 
10
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-017-9
IARIA Congress 2022 : The 2022 IARIA Annual Congress on Frontiers in Science, Technology, Services, and Applications

assumed that the relative position of the camera, EMTS, and 
LRS system was not changed during offline calibration.  
A. Online Spatial Registration
The fusion of real-time information from LRS, Camera,
and EMTS should be able to guide the surgeon from the 
initial insertion point to the destination point inside human 
body. This was achieved by using the online sensors 
registration.  In the spatial registration, all the spatial sensors 
data was represented in the LRS (CT/MRI) coordinate 
reference frame. In the temporal registration, incoming data 
was timestamped based on arrival time  to the surgical PC 
processor. We developed a virtual camera model which 
replicates the position and orientation of the real camera in 
LRS coordinate frame and provides real-time image inside 
human body. 
Online spatial calibration accommodates the changes in 
registration parameters once the sensors start to move. 
Changing transformations between EMTS and camera 
coordinate frame can be computed by attaching an 
Electromagnetic Sensor (EMS) to the videoscope body, and 
calculation of fixed offset transformation parameters between 
EMS measurement and the camera center obtained from the 
camera calibration, as shown in Figure 2. Offline camera 
calibration provides the position and orientation of the 
camera in EMTS coordinate frame. The offset transformation 
between the computed camera center in EMTS coordinate 
frame and the EMTS sensor can be computed as 
(
)
1
*
CAM
offset
calibration
EMTS
T
T
T
−
=
(1) 
where, 
Toffset
 is the Offset transformation between EMS and 
camera frames, 
Tcalibration
 is the Position and Orientation of 
Camera obtained from Camera Calibration[27] and 
CAM
TEMTS
 is the transformation from the EMTS coordinate 
frame to the camera coordinate frame. The transformation 
between LRS and camera coordinate frames uses  the 
transformation form the EMTS to LRS frames  and  is given 
by 
*
LRS
LRS
cam
EMTS
offset
T
T
T
=
(2) 
Where, LRS
Tcam
 is the Transformation from camera to LRS 
coordinate frame; LRS
TEMTS
 is the Transformation from EMTS 
to LRS coordinate frame; 
Toffset
 is the  Offset transformation 
of the camera center to the EMTS coordinate frame. 
The guidance system consists of two separate display 
units inside single screen, as shown in Figure3. First display 
unit consists of plot of 3D LRS data, preoperative planned 
path, current path followed by real-time camera, and position 
and orientation of camera. Second unit displays real-time 
video feed from camera. First, 3D LRS data is displayed 
along with the preoperative planned path, start and 
destination point, as shown in Figure 3. Before real time 
processing, the command asks user to put camera at the 
specified start location of the preplanned path. The user is  
guided in real-time to follow the preplanned path without 
exceeding the user specified threshold deviation from the 
planned path. If an obstacle appears in real-time in the 
preplanned path, the surgeon can use their intelligence to 
avoid the real-time obstacle. During avoidance of obstacle, it 
is evident that the path followed by real-time camera may 
deviate from preplanned path. In such case, the system is 
designed in such a way that the processor searches for a 
shortest path for the camera to return to the original path. The 
point within the preoperative planned path having minimum 
distance from current camera location is called immediate 
goal for camera. The command system guides the user to 
take the necessary steps to reach the immediate goal. 
This provides infrastructure that can be used by surgeon such 
that the path can be automatically followed in real-time.  
One of the major challenges in implementation of 
minimally invasive surgery is to reach to the destination 
inside human body by minimally damaging the organs along 
the path. Previously we have designed an intrinsically 
actuated flexible robotic arm to be used for MIS [24]. To use 
flexible manipulators, one requires accompanying shortest  
path with minimum obstacle to reach to the destination. In 
addition to avoiding the obstacle, the planned path should be 
enough to accommodate the width of the manipulator. In this 
work, assuming the availability of preoperative planned path, 
we developed a semi-automatic system to guide surgeon in 
real time to reach to the destination inside human body. The 
Figure 3: Developed real-time guidance system. 
         Figure. 2: Offset between EMS and Camera Center
TEMS 
T Calibration 
TOffset 
11
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-017-9
IARIA Congress 2022 : The 2022 IARIA Annual Congress on Frontiers in Science, Technology, Services, and Applications

preplanned path and the possible return path are just the 
straight line connecting start and goal position in LRS 
coordinate frame. Rapidly Exploring Random Tree(RRT), 
RRT*, Probabilistic Roadmap(PRM), A* [25] are widely 
used sampling based path planning algorithms that can be 
implemented for path planning 
B. Temporal Calibration
Temporal 
Calibration 
is 
necessary 
for 
time 
synchronization of real-time data obtained from the camera 
and EMTS. EMTS and camera use different clock and report 
measurements at a rate of 40Hz and 61.2 Hz, respectively. A 
prerequisite for time synchronization of measurement is to 
timestamp the measurement with respect to the clock of a 
common processor clock.  In this application, the common 
processor clock is the clock of the computer, called here 
surgical PC, that is used by the surgeon to display the real time 
camera images and the location of the surgical tool(s) as 
shown in Figure 4. The PC processor is responsible for 
controlling, communicating, acquiring, and time stamping of 
the data from the camera and EMTS in real-time. Hence, 
instead of processing data with reference to the clocks of 
sensors themselves, they are processed on the basis of their 
arrival in the host PC. As soon as data arrive from EMTS and 
camera, they are timestamped and placed in a circular buffer. 
With the implementation of the circular buffer, the oldest 
measurement from the devices are overwritten by the most 
recently arrived data. This effectively solves the problem of 
memory leakage during real-time operation. The timestamped 
data are polled every 30 milliseconds. Polling measurement at 
every 30 milliseconds provide enough time for data 
processing without loss of information, as shown in Figure 4. 
III.
PERFORMANCE EVALUATION
The performance of the proposed sensor fusion system 
was evaluated in two-fold. First, the information from the 3D 
LRS was transformed into the EMTS coordinate frame 
followed by transformation to the camera coordinate frame. 
The transformation error was computed as an absolute 
difference between the computed coordinates obtained from 
the transformation and those extracted from the processed 
camera images. The sensors calibration and the accuracy 
evaluation were performed in an environment free of 
ferromagnetic material near the EM field generator. Tracking 
in the electromagnetic field generator is unaffected by the 
medical-grade stainless steel (300 series), titanium, and 
aluminum [12], [22]. The tabletop field generator also 
minimizes distortions produced from the patient table or 
materials located below it [22]. The artificial liver was placed 
inside the mannequin and attached with different colored 
objects on the top surface, as shown in Figure 5. The colored 
objects representing the liver tumor were used for accuracy 
evaluation.  
Figure 5: Setup for accuracy evaluation for spatio-temporal calibration 
system 
Given the start and goal position in LRS coordinate frame, 
the videoscope was navigated to reach to the destination 
points, Section II. For accuracy evaluation purpose, the 
destination points are the colored objects attached to the liver. 
Once the camera reaches to the vicinity of the accuracy 
evaluation points. The colored objects were extracted, and 
their centroids were calculated in LRS (see Figure 6), EMTS, 
and Camera coordinate frames. 
The extracted centroids were first transformed from LRS 
to EMTS coordinate frame. Once the set of points were 
transformed from LRS to EMTS they were projected to 
distortion corrected camera image. 
(
)
(
)
1
1
*
*
(3)
cam
cam
LRS
LRS
cam
LRS
LRS
LRS
cam
EMTS
offset
EMS
P
T
P
T
T
T
T
X
−
−
=
=
=
where, 
PLRS
is the accuracy evaluation point in LRS 
coordinate frame and 
cam
P
is the projected accuracy 
evaluation point in camera coordinate frame. 
As the transformation of the centroids were carried out in 
two phases, accuracy was also evaluated for LRS to EMTS 
coordinate 
transformation, 
and 
EMTS 
to 
camera 
transformation. The experiment was performed at least 12inch 
from the top surface of EMTS field generator to provide the 
room for placement of patient table. 
TABLE I.  ERROR ANALYSIS OF ONLINE CALIBRATION FROM LRS TO 
EMTS TO 2D IMAGE  
LRS to EMTS 
EMTS to Image 
X 
Y 
Z 
X 
Y 
Mean(mm) 
1.6315 
3.0157 
1.9214 
0.4154 
0.1845 
Figure 4: Polling of Timestamped Data in PC 
12
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-017-9
IARIA Congress 2022 : The 2022 IARIA Annual Congress on Frontiers in Science, Technology, Services, and Applications

S. D.(mm)
0.8765 
1.5239 
0.8912 
0.1435 
0.0656 
Range(mm) 
4.9123 
5.1455 
4.3190 
0.2675 
0.1265 
Table I summarizes the absolute positional error for the 
coordinate transformation from LRS to EMTS as well as 
EMTS to camera coordinate frame. Similar to our observation 
in offline calibration, the average error for LRS to EMTS 
coordinate transformation is minimum along the X, and Z axis 
while the Y coordinate is most affected by error with 
maximum standard deviation and range. There are two 
possible reasons for the error: the varying ability of LRS to 
correctly scan and replicate the scanned object at varying 
distance, and the error during data collection because of the 
non-planar surface of the liver. Previously it has been found 
that the performance accuracy of LRS significantly improved 
while scanning planner objects compared to non-planar 
objects [17]. The experimental error could further be 
minimized by taking multi-view scans from the LRS and 
fusing the 3D scans data together. The transformation 
parameters between EMTS and LRS is constant during the 
online and offline calibration. The evaluated accuracy for 
online calibration closely resembles to the offline calibration 
accuracy of 1.35±0.93mm, 2.60±1.52mm, 1.1325±0.9285mm 
along x, y, and z axis respectively. 
The transformation from EMTS to camera is the overall 
error associated with the hybrid tracking system. Although the 
error is in millimeter range, the is mainly due to the 
propagation of error associated with LRS to EMTS 
transformation, offset calculation, and EMTS to camera 
transformation. Compared to the offline calibration with the 
average error of 0.1081mm and 0.0872mm along x and y axis, 
the calibration error increased noticeably along both x and y 
direction. This might be because of the additional error 
introduced during offset calculation, and the motion of the 
camera-EMTS system.  
IV.
CONCLUSION
In this work, spatio-temporal registration  of three 
heterogeneous sensors to assist minimally invasive surgical 
applications was proposed. Laboratory testing showed that 
data fusion from three heterogenous and asynchronous 
sensors provide enough information to help the surgeon 
navigate to the surgery location by providing real-time 
surgical tool position and displaying quality images of the 
inner body. Accuracy evaluation  using a   mannequin and an 
artificial liver sample points localization showed promising 
accuracy for designing an inner body navigation system 
(IBNS). A low-cost camera was used to prove the concept. 
The spatial and temporal registration can be extended to any 
arbitrarily small size camera   as long as the  camera intrinsic 
and extrinsic parameters are provided.  
Currently, the real-time display consists of two display 
units: one for 3D scan and another for 2D images. Future 
efforts include the development of an augmented reality 
system to display an augmented view of human organs on the 
top of 3D scan for real-time simplified navigation. 
Furthermore, the outcome of this research will be used as a 
foundation to develop a comprehensive inner-body navigation 
advisory system. 
REFERENCES 
[1]
T. Peters and K. Cleary, Image-guided interventions:
technology and applications. Springer Science & Business
Media, 2008.
[2]
G. Aston, “Surgical robots: worth the investment?” Hospitals
& Health Networks, vol. 86, no. 4, pp. 38–40, 2012.
[3]
B. Marami, S. Sirouspour, A. Fenster, and D. W. Capson,
“Dynamic tracking of a deformable tissue based on 3d-2d mr-
us image registration,” in Medical Imaging 2014: Image-
Guided Procedures, Robotic Interventions, and Modeling, vol.
9036. SPIE, 2014, pp. 214–220.
[4]
D. Sindram, I. H. McKillop, J. B. Martinie, and D. A. Iannitti,
“Novel 3d laparoscopic magnetic ultrasound image guidance
for lesion targeting,” Hpb, vol. 12, no. 10, pp. 709–716, 2010.
[5]
C. S. Ng, S. C. Yu, R. W. Lau, and A. P. Yim, “Hybrid dynact-
guided electromagnetic navigational bronchoscopic biopsy,”
European Journal of Cardio-Thoracic Surgery, vol. 49, no.
suppl 1, pp. i87–i88, 2016.
[6]
C. Wengert, L. Bossard, A. Haberling, C. Baur, G. Székely,
and P. C. Cattin, “Endoscopic navigation for minimally
invasive suturing,” in International Conference on Medical
Image Computing 
and 
ComputerAssisted 
Intervention.
Springer, 2007, pp. 620–627.
[7]
H. E. Fakhfakh, G. Llort-Pujol, C. Hamitouche, and E. Stindel,
“Automatic registration of pre-and intraoperative data for long
bones in minimally invasive surgery,” in 2014 36th Annual
International Conference of the IEEE Engineering in Medicine
and Biology Society. IEEE, 2014, pp. 5575–5578.
[8]
M. Feuerstein, T. Reichl, J. Vogel, J. Traub, and N. Navab,
“Magnetooptical tracking of flexible laparoscopic ultrasound:
model-based online detection and correction of magnetic
tracking errors,” IEEE Transactions on Medical Imaging, vol.
28, no. 6, pp. 951–967, 2009.
[9]
M. Nakamoto, K. Nakada, Y. Sato, K. Konishi, M. Hashizume,
and S. Tamura, “Intraoperative magnetic tracker calibration
using a magnetooptic hybrid tracker for 3-d ultrasound-based
navigation in laparoscopic surgery,” IEEE transactions on
medical imaging, vol. 27, no. 2, pp. 255–270, 2008.
[10] K. Konishi et al., “A real-time navigation system for
laparoscopic surgery based on three-dimensional ultrasound
using magneto-optic hybrid tracking configuration,”
International Journal of Computer Assisted Radiology and 
Surgery, vol. 2, no. 1, pp. 1–10, 2007.
[11] K. Nakada, M. Nakamoto, Y. Sato, K. Konishi, M. Hashizume,
and S. Tamura, “A rapid method for magnetic tracker
calibration using a magneto-optic hybrid tracker,” in
International Conference on Medical Image Computing and
Computer-Assisted Intervention. Springer, 2003, pp. 285–293.
[12] W. Birkfellner, J. Hummel, E. Wilson, and K. Cleary,
“Tracking devices,” in Image-guided interventions. Springer,
2008, pp. 23–44.
[13] R. S. J. Estépar, N. Stylopoulos, R. Ellis, E. Samset, C.-F.
Westin, C. Thompson, and K. Vosburgh, “Towards scarless
surgery: an endoscopic ultrasound navigation system for
transgastric access procedures,” Computer aided surgery, vol.
12, no. 6, pp. 311–324, 2007.
[14] E. Mair, M. Fleps, M. Suppa, and D. Burschka, “Spatio-
temporal initialization for imu to camera registration,” in 2011
IEEE International Conference on Robotics and Biomimetics.
IEEE, 2011, pp. 557–564.
[15] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep
continuous fusion for multi-sensor 3d object detection,” in
13
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-017-9
IARIA Congress 2022 : The 2022 IARIA Annual Congress on Frontiers in Science, Technology, Services, and Applications

Proceedings of the European conference on computer vision 
(ECCV), 2018, pp. 641–656. 
[16] J. Marr and J. Kelly, “Unified spatiotemporal calibration of
monocular cameras and planar lidars,” in International
Symposium on Experimental Robotics. Springer, 2018, pp.
781–790.
[17] D. E. Ruehling, “Development and testing of a hybrid medical
tracking system for surgical use,” Ph.D. dissertation, Tennessee
Technological University, 2015.
[18] U. Bhattarai and A. T. Alouani, “Hybrid navigation
information system for minimally invasive surgery: Offline
sensors registration,” in Science and Information Conference.
Springer, 2019, pp. 205–219.
[19] B. K. Horn, “Closed-form solution of absolute orientation
using unit quaternions,” Josa a, vol. 4, no. 4, pp. 629–642,
1987.
[20] R. Hartley and A. Zisserman, Multiple View Geometry in
Computer Vision. 2003.
[21] Next Engine Inc., “Next engine 3d laser scanner.” Available at
http:// 
http://www.nextengine.com/products/scanner/specs
(2022/06/21).
[22] Northern Digital 
Inc., 
“Aurora,” Available at
http:// 
https://www.ndigital.com/electromagnetic-
tracking-technology/aurora/ (2022/06/21). 
[23] Jai Inc., “Go series go-5000c-usb compact 5 mp area scan
camera,” Available at http:// https://www.jai.com/products/go-
5000c-usb (2022/06/21).
[24] U. Bhattarai and A. T. Alouani, “Flexible semi-automatic arm
design for minimally invasive surgery,” in 2017 25th
International Conference on Systems Engineering (ICSEng).
IEEE, 2017, pp. 207–211.
[25] K. Karur, N. Sharma, C. Dharmatti, and J. E. Siegel, “A survey
of path planning algorithms for mobile robots,” Vehicles, vol.
3, no. 3, pp. 448– 468, 2021.
14
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-017-9
IARIA Congress 2022 : The 2022 IARIA Annual Congress on Frontiers in Science, Technology, Services, and Applications

