Self-Organized Potential Competitive Learning to Improve Interpretation and
Generalization in Neural Networks
Ryotaro Kamimura
IT Education Center, and
Graduate School of
Science and Technology
Tokai Univerisity
Email: ryo@keyaki.cc.u-tokai.ac.jp
Ryozo Kitajima
Graduate School of
Science and Technology
Tokai Univerisity
Email: 3btad004@mail.tokai-u.jp
Osamu Uchida
Dept. Human and
Information Science, and
Graduate School of
Science and Technology
Tokai University
Email: o-uchida@tokai.ac.jp
Abstract—The present paper proposes a new learning method
called “self-organized potential competitive learning” to improve
generalization and interpretation performance. In this method,
the self-organizing map (SOM) is used to produce knowledge
(SOM knowledge) on input patterns. By considering the poten-
tiality of neurons rather than stored information, it can be used to
train supervised learning. Highly potential neurons are supposed
to respond to as many input patterns and neurons as possible.
This property is, for the ﬁrst approximation, described by the
variance of connection weights. The method was applied to real
second language learning data (Japanese learners of English) and
showed improved generalization performance. In addition, two
important input neurons with high potentiality were detected,
both of which represented inanimate subjects. This implies
that Japanese students have difﬁculty dealing with inanimate
subjects when learning English as a second language. This ﬁnding
corresponds with the established knowledge on second language
learning. The present results afﬁrm the possibility of SOM
knowledge to be applied to many different situations.
Keywords–Self-organizing maps; Potentiality; Interpretation;
Generalization.
I.
INTRODUCTION
The present section shows that it is necessary to focus on
the main part of knowledge obtained by the self-organizing
maps for applying it to supervise learning.
A. Utility of SOM Knowledge
The self-organizing map (SOM) [1][2] is one of the most
important unsupervised techniques in neural networks. In par-
ticular, the SOM has good reputation for producing knowledge
(SOM knowledge) which can be used to clarify class structure
and visualize input patterns [3]-[13]. Because it has been
proved that the SOM can produce rich knowledge from input
patterns, SOM knowledge has been used for many different
purposes in addition to class clariﬁcation and visualization.
The present paper tries to show that SOM knowledge can
be used to train supervised neural networks. If it is possible
to use SOM knowledge in supervised learning, it has one
major merit compared with other supervised techniques. The
SOM has long been used to visualize complex data over
two-dimensional maps. Thus, supervised networks with SOM
knowledge can produce easily interpretable representations. It
is well-known that the black-box property of neural networks
is a major difﬁculty in extending them to practical problems.
To overcome this issue, a number of methods have been
developed. For example, some methods have tried to extract
rules from obtained connection weights [14]-[18]. However,
it is not easy to extract explicit rules when the connection
weights are complex. Methods with SOM knowledge can be
used to produce neural networks whose inference mechanisms
are more easily interpreted.
B. Potentiality of SOM Knowledge
The direct insertion of SOM knowledge into supervised
neural networks is particularly effective in decreasing errors
between targets and outputs. However, since the SOM is a form
of unsupervised learning, knowledge generated by the SOM
is not necessarily suitable for training supervised learning. In
this context, it is supposed that some form of enhancement of
SOM knowledge is necessary to adapt it for supervise learning.
More concretely, SOM knowledge needs to be modiﬁed before
entering the supervised leaning phase in order to make it
effective.
In the present paper, we suppose that the fundamental
parts of SOM knowledge can be used for general purposes,
including supervised learning. The main parts are supposed
to be related to as many different situations and patterns as
possible. On the other hand, the peripheral parts are exclusively
related to speciﬁc situations and input patterns. The main part
is related to the ability of neurons to respond appropriately
to as many new situations as possible. Linsker [19] stated
the concept of information in the same way and considered
the variance of neurons as one of the candidates for the
concept of information. Thus, the present paper adopts the
variance of neurons as the potentiality of neurons for the
ﬁrst approximation. Naturally, the variance itself is not always
useful in the improvement of performance. Thus, potentiality
refers to all processes of transforming variance into a useful
form for the sake of improved performance.
C. Paper Organization
In Section 2, we present how to compute the poten-
tiality and how it can be used for learning. In Section 3,
the experimental results on the second language learning is
presented. First, we show that the selective potentiality is
increased when the parameter is increased. Then, we compare
32
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

generalization performance by the present method with that by
the conventional learning methods. The present method show
better generalization performance compared with the other
conventional methods. In addition, connection weights into
the highly potential neuron represent the inanimate subjects,
corresponding to the established knowledge of the second
language learning.
II.
THEORY AND COMPUTATIONAL METHODS
In this section, we present how to compute the potentiality
and brieﬂy explain how to train supervised learning by this
potentiality.
A. Introducing Potentiality
Potentiality refers to how neurons respond differently to
as many situations as possible. For the ﬁrst approximation,
potentiality is measured by the variance of neurons. When
the variance of neurons becomes larger, the corresponding
potentiality becomes higher.
Figure 1 shows the three phases of potential learning.
In the ﬁrst potential determination phase in Figure 1(a), the
SOM is used to obtain connection weights from input to
hidden neurons. Then, the corresponding potentialities of input
and hidden neurons are computed. In the second potentiality
actualization phase in Figure 1(b), connection weights and
input and hidden potentialities transferred from the potentiality
determination phase are given as initial weights. Then, those
weights and potentialities are assimilated as much as possible
in learning. Finally, in the potentiality adjustment phase, the
connection weights obtained in the potentiality actualization
phase are slightly adjusted, speciﬁcally to eliminate the effects
of over-training.
B. Input and Hidden Potentiality
In the potentiality determination phase, ﬁrst the potentiality
is determined by using the variance of connection weights, and
then this potentiality is incorporated into the learning processes
to assimilate the potentiality. For this, we need to deﬁne the
potentiality of individual input neurons.
Let wjk denote connection weights from the kth input
neuron to the jth output neuron. Then, the variance is deﬁned
by
vk =
M
∑
j=1
(wjk − wk)2,
(1)
where M is the number of hidden neurons and
wk = 1
M
M
∑
j=1
wjk.
(2)
Then, the input potentiality is deﬁned by
ϕk =
(
vk
maxl vl
)r
,
(3)
where r denotes the potentiality parameter and r ≥ 0.
The hidden potentiality is deﬁned by
vj =
L
∑
k=1
(wjk − wj)2,
(4)
where L is the number of input neurons and
wj = 1
L
L
∑
k=1
wjk,
(5)
Then, the hidden potentiality is deﬁned by
ϕj =
(
vj
maxm vm
)r
,
(6)
C. Selective Potentiality
The number of highly potential neurons should be as
small as possible. For this, the selectivity of potentiality is
introduced. First, the input potentiality is normalized by
ϕnrm
k
=
ϕk
∑L
l=1 ϕk
.
(7)
and
H1 = −
L
∑
k=1
ϕnrm
k
log ϕnrm
k
.
(8)
Then, the selective potentiality is deﬁned by
SP1 = Hmax
1
− H1
Hmax
1
.
(9)
Finally, the hidden potentiality SP2 is obtained in the same
way.
D. Potentiality Actualization
The potentiality is used to modify connection weights
according its magnitude. The modiﬁcation is implemented for
connection weights from the input to hidden, and from hidden
to output neurons. For the input-hidden connection weights,
newwjk = ϕj
oldwjkϕk
(10)
and for the hidden-output connection weights,
newwij = oldwijϕj.
(11)
In the potential actualization phase, connection weights
weighted by the corresponding potentialities are given as initial
weights. Those initial and weighted connection weights guide
the learning processes in the actualization phase.
III.
RESULTS AND DISCUSSION
This section deals with an experimental result on the
second language learning, stressing that the main ﬁndings by
the present method correspond to those of the second language
learning.
A. Experimental Outline
Real second language learning data was used to test the
method. The numbers of input variables and patterns were 42
and 70, respectively. The number of hidden neurons was set to
12. The size was empirically determined for the SOM. The data
set was divided into the training (70%), validation (15%) and
testing (15%) data. All supervised learning used the default
parameter values of the Matlab neural networks package in
order to make it easy to trace the results.
The purpose of the experiment was to examine what differ-
entiates Japanese high school and university EFL students in
33
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

xk
s
wjk
wjk
wjk
Input
xk
s
wjk
*
(a2) Final state
(a1) Initial state
(a) Potentiality determination phase
(b2) Final state
(b1) Initial state
(b) Potentiality actualization phase
(c2) Final state
(c1) Initial state
(c) Potentiality adjustment phase
Input
svj
xk
s
so
wjk
W
Input
Output
xk
s
soi
i
wjk
*
Wij
ij
Input
svj
Output
xk
s
so
wjk
W
Input
Output
xk
s
soi
i
wjk
*
Wij
Wij
ij
Input
svj
Output
φ
φ
φ
φ
k
k
φk
j
φj
j
Figure 1. Concept of self-organized potential learning where the potentiality is determined in the potentiality determination phase, and the knowledge obtained
in this phase is transferred to the potentiality actualization phase. Finally, minor adjustments are made in the potentiality adjustment phase.
terms of their grammatical competence in writing. Thirty ﬁrst-
year high school and 40 ﬁrst-year university EFL students par-
ticipated in the experiment. Both the high school and university
students had started studying English in junior high school;
therefore, the former group had studied English for three years,
while the latter had studied for six years. None of them had
experience living or studying in English environments. Both
groups of students took a written grammar test consisting of
42 questions, each of which targeted different grammatical
structures. The questions were basically taken from model
sentences in different lessons in an English high school writing
textbook authorized by the Japanese Ministry of Education,
Culture, Sports, Science, and Technology. The 42 questions
comprised seven different grammatical categories, each of
which was further broken down into several questions: tense (8
questions), sentence patterns (11), inanimate subjects as agents
(2), auxiliary verbs (3), clauses (4), voice (2), non-ﬁnite verbs
(9) and comparative/superlative (3). For example, the category
”tense” included questions that asked about different tenses
such as past, present progressive, and present perfect. The two
groups of students took the test for 35 minutes in a classroom
without using a dictionary. For each question, the students
were given a Japanese sentence followed by scrambled English
words and phrases. Their task was to unscramble those words
and phrases to make a sentence that corresponded to the given
Japanese sentence.
B. Input and Hidden Selective Potentiality
Figures 2(a) and (b) show input and hidden selective
potentiality for the L2 data set. As can be seen in the ﬁgure,
the input selective potentiality increased to 0.7, while the
hidden selective potentiality only reached 0.4. In other words,
the input potentiality was easily increased compared with the
hidden potentiality.
Figure 3 shows the individual potentialities of input neu-
rons. When the parameter r was 0.1 in Figure 3(a1), the
34
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

1.1
2.4
3.7
5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Potentiality parameter r
1.1
2.4
3.7
5
Potentiality paraemer r
Input potentiality
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Hidden potentiality
(a) Input potentiality
(b) Hidden potentiality
Figure 2. Input potentiality (a) and hidden potentiality (b) for the L2 data set.
individual potentialities ﬂuctuated almost evenly. When the
parameter r increased from 1.1 in Figure 3(a2) to 5.0 in Figure
3(a4), the potentialities became gradually differentiated. In the
end, only two input neurons had higher potentialities, namely,
the 19th and 26th input neurons.
Figure 3 shows individual hidden potentialities. Because
of the SOM, periodic patterns could be observed. When the
parameter r increased from 0.1 in Figure 3(b1) to 5.0 in
Figure 3(b4), only two hidden neurons tended to have higher
potentiality, namely, the ﬁrst and seventh hidden neurons.
C. Generalization Performance
Figure 4 shows generalization errors when the parameter r
was increased from 0.1 to 5.0. In all cases, the errors by the
potentiality method were well lower than those by the BP and
the method without the potentiality. By the input potentiality
in Figure 4(a), when the parameter r was less than 2.4, the
generalization errors were lower than those by the BP and
the method without the potentiality. Then, the generalization
errors were larger than those by the conventional BP beyond
this point.
By using the hidden potentiality in Figure 4(b), the gen-
eralization errors were almost always below those by the
conventional BP. By using the input and hidden potentiality
in Figure 4(c), the generalization errors gradually decreased
when the parameter r increased to 2.4, and then began to
ﬂuctuate. Those results show that generalization errors by the
potential method were lower than those by the other methods.
In particular, by using the input and hidden potentiality, better
generalization performance could be obtained.
It should be stressed that the generalization error by the
method without the potentiality produced the worst errors
out of all the methods. The method without the potentiality
was one in which the SOM was directly connected with the
successive back-propagation networks. As mentioned in the
introduction section, direct insertion of SOM knowledge is
not useful for training supervised learning. The results show
clearly that modiﬁcation and enhancement by the potentiality
have the effect of transforming SOM knowledge to more useful
knowledge.
D. Connection Weights
Figure 5(a) shows connection weights in the potentiality
determination phase, namely, by the SOM. As can be seen
in the ﬁgure, many positive connection weights could be
seen, and it was difﬁcult to immediately detect any regularity
from those connection weights. Figure 5(b) shows connection
weights by the potentiality actualization phase with only
input potentiality. It could be seen that only two groups of
connection weights from the 19th and 26th input neurons
were strong. These two input neurons represented inanimate
subjects. Figure 5(c) shows connection weights with the hidden
neurons’ potentialities. As can be seen in the ﬁgure, two
groups of connection weights into the ﬁrst and seventh hidden
neurons had stronger positive weights. The connection weights
into both hidden neurons showed larger variance, as shown
in Figure 5(a). By using the input and hidden potentiality in
Figure 5(d), strong connection weights similar to those by the
input potentiality in Figure 5(b), and by the hidden potentiality
in Figure 5(c), were observed. However, the majority of them
became weaker and negative in red.
E. Summary of Results
Table I shows a summary of the experimental results in
terms of generalization performance. The bold face numbers
represent the best values. The method ”without” means the one
in which the SOM is directly connected with the supervised
component. As can be seen in the table, all potential methods
showed lower errors compared with those by the methods
without potentiality: BP and the support vector machines. By
the input potentiality, the generalization error was 0.2. Then,
by the hidden potentiality, the generalization error decreased
to 0.1909 and the minimum error became zero. By using
the input and hidden potentiality, the lowest error of 0.1818
was obtained. By the conventional BP, the error increased
to 0.2455, and by the ﬁne-tuned support vector machine,
the error further increased to 0.2818. Finally, without the
potentiality, the worst error of 0.4364 was obtained, meaning
that SOM knowledge did not contribute to the improvement
of generalization performance. The potentiality method was
essential in order to effectively utilize SOM knowledge.
The better generalization performance was due to the fact
that a smaller number of highly potential neurons was detected
in Figure 3. In addition, the better performance was due
to the connection weights by the SOM in Figure 5(a). The
potentiality method tried to those extract connection weights
with the largest variance created by the SOM.
Then, it was observed that connection weights were mod-
iﬁed only according to the potentialities in Figure 5(b). Only
35
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

10
20
30
40
0
0.5
1
10
20
30
40
0
0.5
1
10
20
30
40
0
0.5
1
10
20
30
40
0
0.5
1
(a1) r=0.1
(a2) 1.1
(a3) 2.4
(a4) 5
Input neurons
(a) Input potentiality
(b) Hidden potentiality
Potentiality
2
4
6
8
10
12
0
0.5
1
2
4
6
8
10
12
0
0.5
1
2
4
6
8
10
12
0
0.5
1
2
4
6
8
10
12
0
0.5
1
(b1) r=0.1
(b2) 1.1
(b3) 2.4
(b4) 5
Hidden neurons
Potentiality
Figure 3. Individual input (a) and hidden (b) potentialities for the L2 data set.
1.1
2.4
3.7
5
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Parameter values
Parameter values
Parameter values
Generalization errors
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Generalization errors
0.2
0.25
0.3
0.35
0.4
0.45
Generalization errors
(a) Input
(b) Hidden
(c) Input and hidden
BP
BP
BP
Without
Without
Without
0.1
1.1
2.4
3.7
5
0.1
1.1
2.4
3.7
5
0.1
Figure 4. Generalization errors by input (a), hidden (b) and combined (c) potentiality for the L2 data set.
TABLE I. Summary of experimental results for the L2 data set.
Method
Avg
Std dev
Min
Max
Input Potentiality
0.2000
0.1118
0.0909
0.3636
Hidden Potentiality
0.1909
0.1000
0.0000
0.3636
Input+hidden
0.1818
0.1134
0.0000
0.3636
Without
0.4364
0.1808
0.2727
0.8182
BP
0.2455
0.1054
0.0000
0.3636
SVM
0.2818
0.1088
0.0909
0.4545
two important and highly potential input neurons were de-
tected, both of which represented inanimate subjects. The
Japanese students had difﬁculty in using inanimate subjects,
which are not common in the Japanese language. This cor-
responds perfectly to already established knowledge in L2
literature [20][21].
IV.
CONCLUSION
The present paper proposed a new type of learning called
“self-organized potential learning”. This method aims to utilize
SOM knowledge to train supervised learning. The direct use
of SOM knowledge is not necessarily useful for supervised
training. Thus, SOM knowledge should be seen for its po-
tentiality in many different situations. If the knowledge can
be effective for many different situation or patterns, it can
have much potentiality. For the ﬁrst approximation to the
potentiality, the variance of neurons is adopted. If neurons have
larger variance and respond to input patterns differently, the
neurons’ potentiality becomes higher.
The method was applied to the actual data from the
second language learning. The method could extract a clear
result: that Japanese students had the most difﬁculty dealing
with inanimate subjects. This corresponds perfectly to second
language learning literature.
One of the main problems is that the quantities of the
selective potentiality of input and hidden neurons were dif-
ferent from each other. In the experiments, the input neurons
could increase the selectivity more so than the hidden neurons,
36
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

10
20
30
40
2
4
6
8
10
12
Input
HIdden
HIdden
HIdden
HIdden
10
20
30
40
2
4
6
8
10
12
Input
10
20
30
40
2
4
6
8
10
12
Input
10
20
30
40
2
4
6
8
10
12
Input
(a) Potentiality determination phase(SOM)
(b) Input
(c) Hidden
(d) Input and hidden
Inanimate subjects
Figure 5. Weights in the potentiality determination phase (a) and actualization phase (b)-(d) by the four methods for the L2 data set. Green and red colors
represent positive and negative weights.
as shown in Figure 2. This imbalance between input and
hidden potentiality may inﬂuence ﬁnal performance. Thus, it is
necessary to examine in more detail the relationship between
input and hidden potentiality. Finally, it is important to note
that though the present experiment was performed with a
small-sized but actual dataset, the method is simple enough
to be applied to large-scale data sets.
REFERENCES
[1]
T. Kohonen, Self-Organization and Associative Memory.
New York:
Springer-Verlag, 1988.
[2]
T. Kohonen, Self-Organizing Maps.
Springer-Verlag, 1995.
[3]
J. Vesanto, “Som-based data visualization methods,” Intelligent data
analysis, vol. 3, no. 2, pp. 111–126, 1999.
[4]
S. Kaski, J. Nikkil¨a, and T. Kohonen, “Methods for interpreting a self-
organized map in data analysis,” in In Proc. 6th European Symposium
on Artiﬁcial Neural Networks (ESANN98). D-Facto, Brugfes. Citeseer,
1998.
[5]
J. Mao and A. K. Jain, “Artiﬁcial neural networks for feature extraction
and multivariate data projection,” Neural Networks, IEEE Transactions
on, vol. 6, no. 2, pp. 296–317, 1995.
[6]
C. De Runz, E. Desjardin, and M. Herbin, “Unsupervised visual data
mining using self-organizing maps and a data-driven color mapping,”
in Information Visualisation (IV), 2012 16th International Conference
on.
IEEE, 2012, pp. 241–245.
[7]
S.-L. Shieh and I.-E. Liao, “A new approach for data clustering
and visualization using self-organizing maps,” Expert Systems with
Applications, vol. 39, no. 15, pp. 11 924–11 933, 2012.
[8]
H. Yin, “Visom-a novel method for multivariate data projection
and structure visualization,” Neural Networks, IEEE Transactions on,
vol. 13, no. 1, pp. 237–243, 2002.
[9]
M.-C. Su and H.-T. Chang, “A new model of self-organizing neural
networks and its application in data projection,” Neural Networks, IEEE
Transactions on, vol. 12, no. 1, pp. 153–158, 2001.
[10]
S. Wu and T. W. Chow, “Prsom: a new visualization method by
hybridizing multidimensional scaling and self-organizing map,” Neural
Networks, IEEE Transactions on, vol. 16, no. 6, pp. 1362–1380, 2005.
[11]
L. Xu, Y. Xu, and T. W. Chow, “Polsom: A new method for multidi-
mensional data visualization,” Pattern recognition, vol. 43, no. 4, pp.
1668–1675, 2010.
[12]
Y. Xu, L. Xu, and T. W. Chow, “Pposom: A new variant of polsom by
using probabilistic assignment for multidimensional data visualization,”
Neurocomputing, vol. 74, no. 11, pp. 2018–2027, 2011.
[13]
L. Xu and T. W. Chow, “Multivariate data classiﬁcation using polsom,”
in Prognostics and System Health Management Conference (PHM-
Shenzhen), 2011.
IEEE, 2011, pp. 1–4.
[14]
H. Kahramanli and N. Allahverdi, “Rule extraction from trained adap-
tive neural networks using artiﬁcial immune systems,” Expert Systems
with Applications, vol. 36, no. 2, pp. 1513–1522, 2009.
[15]
G. G. Towell and J. W. Shavlik, “Extracting reﬁned rules from
knowledge-based neural networks,” Machine learning, vol. 13, no. 1,
pp. 71–101, 1993.
[16]
R. Andrews, J. Diederich, and A. B. Tickle, “Survey and critique of
techniques for extracting rules from trained artiﬁcial neural networks,”
Knowledge-based systems, vol. 8, no. 6, pp. 373–389, 1995.
[17]
H. Tsukimoto, “Extracting rules from trained neural networks,” Neural
Networks, IEEE Transactions on, vol. 11, no. 2, pp. 377–389, 2000.
[18]
A. d. Garcez, K. Broda, and D. M. Gabbay, “Symbolic knowledge
extraction from trained neural networks: A sound approach,” Artiﬁcial
Intelligence, vol. 125, no. 1, pp. 155–207, 2001.
[19]
R. Linsker, “Self-organization in a perceptual network,” Computer,
vol. 21, no. 3, pp. 105–117, 1988.
[20]
T. Kamimura, Teaching EFL Composition in Japan. Senshu University
Press, 2012.
[21]
P. Master, “Active verbs with inanimate subjects in scientiﬁc prose,”
English for Speciﬁc Purposes, vol. 10, no. 1, pp. 15–33, 1991.
37
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

