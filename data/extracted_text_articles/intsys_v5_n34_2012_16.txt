427
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Involving All Stakeholders in the Development of TV Applications for Elderly 
José Coelho, Carlos Duarte, Tiago Guerreiro, Pedro 
Feiteira, Daniel Costa, David Costa, Bruno Neves, and 
Fernando Alves 
LaSIGE, Department of Informatics 
University of Lisbon 
Lisbon, PT 
{jcoelho, cad, tjvg, pfeiteira}@di.fc.ul.pt, {thewisher, 
dcosta, bneves, falves}@lasige.di.fc.ul.pt 
 
Pradipta Biswas and Patrick Langdon 
Department of Engineering 
University of Cambridge 
Cambridge, UK 
pb400@cam.ac.uk, pml24@eng.cam.ac.uk
 
 
Abstract- The development of new digital TV systems and the 
design practices adopted in the development of new TV based 
applications often isolate elderly and disabled users. By 
considering them as users with special needs and not taking 
their problems into account during the design phase of an 
application, developers are creating new accessibility problems 
or just keeping bad old habits. In this paper, we describe a 
novel adaptive accessibility approach on how to develop 
accessible TV applications, by making use of multimodal 
interaction techniques and without requiring too much effort 
from the developers. By putting user-centered design 
techniques in practice, and supporting the use of multimodal 
interfaces with several input and output devices, we confront 
users, developers and manufactures with new interaction and 
design paradigms. From their evaluation, new techniques are 
created capable of helping in the development of accessible TV 
applications, with special interest for a novel way of acquiring 
and providing knowledge from and to the users with an 
application called User Initialization Application. 
Keywords–multimodal; adaptation; developers; elderly; user 
initialization. 
I. 
 INTRODUCTION 
Ageing is certainly an obstacle to adequate human-
computer interaction, mostly because of physical and 
cognitive impairments. Traditional computational systems 
only provide keyboard and mouse interaction to users. This 
makes impossible, for example, for users with severe motor 
impairments to interact in any manner (at least effectively). 
Also, as recent developments are responsible for new 
television (TV) systems and applications, unimodal 
interaction is still being favored without accessibility 
concerns, excluding persons whom suffer from an 
impairment of the sensory channel needed to interact. This 
situation brings social exclusion and e-exclusion to the 
Human-Computer Interaction (HCI) world and new TV 
platforms, as it seriously restricts actions and information 
access to users with impairments (like the elderly), providing 
means of interaction exclusively for the so called “normal 
users”. However, multimodality can resolve this issue by 
offering the possibility of presenting content in many ways 
(audio, visual, haptic), and in the most suitable way to each 
user’s characteristics. Also, by offering users the possibility 
to use the inputs more adequate to them (or the context of 
interaction), in a single or combined manner, multimodal 
interaction can improve interaction efficiency and, more 
importantly, accessibility.  
Multimodal interfaces aim to provide a more natural and 
transparent way of interaction with users. They have been 
able to enhance human-computer interaction (HCI) in many 
numbers of ways, including: User satisfaction: studies 
revealed that people favor multiple-action modalities for 
virtual object manipulation tasks [14]; Oviatt [17] has also 
shown that about 95% of users prefer multimodal interaction 
over unimodal interaction; Robustness and Accuracy: “using 
a number of modes can increase the vocabulary of symbols 
available to the user” leading to an increased accessibility 
[15]. Oviatt stated that multiple inputs have a great potential 
to improve information and systems accessibility, because by 
complementing each other, they can yield a “highly 
synergistic blend in which the strengths of each mode are 
capitalized upon and used to overcome weaknesses in the 
other” [18]; Efficiency 
and 
Reliability: 
Multimodal 
interfaces are more efficient than unimodal interfaces, 
because they can in fact speed up tasks completion by 10% 
and improve error handling and reliability [16]; Adaptivity: 
Multimodal interfaces also oﬀer an increase in flexibility and 
adaptivity in interaction because of the ability to switch 
among diﬀerent modes of input, to whichever is more 
convenient or accessible to a user [15]. However, Vitense 
[20] illustrates the need of additional research in multimodal 
interaction, especially involving elderly people. This paper 
tries to extend this knowledge. 
Also, the majority of current approaches to the 
development of multimodal or adaptive systems, either 
addresses speciﬁc technical problems, or is dedicated to 
speciﬁc modalities. The technical problems dealt with 
include multimodal fusion [10], presentation planning [10], 
content selection [12], multimodal disambiguation [18], 
dialogue structures [3], or input management [9]. Platforms 
that combine speciﬁc modalities are in most cases dedicated 
to speech and gesture [19], speech and face recognition [11] 
or vision and haptics [13]. Even though the work done in 
tackling technical problems is of fundamental importance to 
the development of adaptive and multimodal interfaces, it is 
of a very particular nature, and not suited for a more general 
interface description. Also, frameworks supporting the 
development of interfaces for various devices exist; however, 

428
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
they do not consider the specificities of multimodal 
interaction in its design [5][6]; or they focus only on the use 
of the same modality in different devices [1]; or they ignore 
the possibility of adapting the components properties and 
features in run-time placing the burden on the designer [4]. 
In general, they do not consider in there architectures the 
introduction of modalities, and how they can be explored to 
achieve the goals of Universal Access. 
In the following, we first explain how European funded 
project GUIDE [7], aims to adapt interaction and UI 
presentation to fit each user’s characteristics and level of 
expertise. Also, resulting from specific user trials and 
discussions with developers, we also show how it makes use 
of a User Initialization Application to know and instruct its 
users, and how it supports adaptation by providing 
developers with solutions for UI modification, and tools for 
helping in the development of new user-centered and 
accessible applications. All this attending to user needs and 
differences, at the same time as it takes into consideration the 
developer’s interests.  
II. 
CHARACTERISTICS OF GUIDE PROJECT 
A. End-Users and Goals 
GUIDE [7] (figure 1) aims to achieve the necessary 
balance 
between 
developing 
multimodal 
adaptive 
applications for elderly and disabled users, and preserving 
TV and Set-Top Box(STB) developers/manufacturers design 
methodologies and efforts. Consequently, there are clearly 
two different end-users of this project: elderly and impaired 
users and developers of TV based applications. Creating a 
bridge between these two, we have also the STB 
manufacturers who dictate the rules about which type and 
which characteristics of applications can be used on a TV 
based environment. Firstly, for elderly and users with 
impairments, GUIDE has the goal of providing new ways of 
interacting with a TV, by applying multimodal interaction, 
supporting the use of different devices as well as different 
combinations of input and output techniques, and adaptation 
to each application’s UI an each user’s way of interaction. In 
other words, elderly or impaired users who are having 
difficulties interacting with modern TV systems because of 
their complexity, will be able to interact in a more intuitive 
way, using alternative modalities in a single or combined 
fashion, while each interface characteristics will also be 
adapted to fit user’s characteristics automatically. For all 
this, GUIDE has as a clear defined target environment, a 
STB connected to a TV in user’s home (and closed) 
environment. Secondly because developers tend to have no 
concerns 
about 
accessibility 
when 
designing 
TV 
applications, GUIDE has to be capable of reducing 
development effort in a radical manner. For that end, GUIDE 
wants to create a toolbox for accessible applications and UI 
design, shifting the design principle from a conventional 
user-centered design process to a GUIDE-assisted design and 
development process. Through all this, GUIDE also wants to 
ensure that developers (and also manufacturers) can maintain 
the control over the modifications made on their own 
applications UI. Meaning, the adaptation provided by the 
system for adapting interfaces to user characteristics must 
have boundaries that cannot be crossed. And these 
boundaries are defined by the developers. 
B. Multimodal Interfaces and Devices 
Input modalities to be supported in GUIDE are based in 
the more natural ways of communication for humans: speech 
and pointing (and gestures). Complementary to these 
modalities, and given the TV based environment, the 
framework should support the usage of remote controls and 
other devices capable of providing haptic input or feedback. 
As a result, GUIDE incorporates four main types of UI 
components (figure 1): visual sensing and gesture 
interpretation; audio; remote control; haptic interfaces and a 
 
content
Multimodal Adaptation
select!
click!
select!
select!
select!
Modal input
transformation
Modal output
transformation
Multimodal Input Fusion
Multimodal Output Fission
User model and profiles
ICT Application 
Logic
AAA
content
AAA
content
content
A
speech
commands
remote control:
buttons + gyroscopic
visual
gestures
adaptive
multitouch
control
video
rendering, 
adaptive GUI 
components
audio
rendering
haptic
feedback
speech 
guidance/
avatars
User simulation
GUIDE Framework
Fig 1. Features in GUIDE 

429
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
multi-touch tablet. In what concerns the output modalities, 
the framework should consider and integrate the following 
output components: video rendering equipment (TV); audio 
rendering equipment (Speakers); tablet supporting a subset 
of video and audio rendering and remote control supporting a 
subset of audio rendering and vibration feedback. A tablet 
may also be used to clone the TV screen or complement 
information displayed on the TV screen but essentially is 
used as a secondary display. The main user interface should 
be able to generate various configurable visual elements such 
as text (e.g., subtitles or information data), buttons for 
navigation purpose, images and video (e.g., video conference 
or media content). Additionally also a 3D avatar is generated 
and expected to play a major role for elderly acceptance and 
adoption of the GUIDE system, being able to perform non-
verbal expressions like facial expressions and gestures and 
giving the system a more human like communication ability. 
In order for the UI to be adapted to the user’s needs, 
these elements are necessarily highly configurable and 
scalable (vector-based). Size, font, location, and color are 
some attributes needed to maintain adaptability. These 
graphical elements enable the system communication with 
the users by illustrating, answering, suggesting, advising, 
helping or supporting them through their navigation. Also, 
both input and output modalities can be used in a combined 
manner to enrich interaction and reach every type of user. 
C. Discussion: What GUIDE needs to know 
For reaching its goals, GUIDE has to define a framework 
structure and collect information by asking and testing its 
end-users. So, the following questions have to be answered: 
What components the GUIDE framework has to have? What 
are the main preferences and typical behavior of elderly 
users when interacting with the system, and how to collect 
these preferences? How to perform automatic UI adaptation? 
How to help developers and manufactures in design process? 
III. 
LEARNING FROM END USERS 
To get answers to the questions above, we firstly derive 
end user requirements from results obtained through a 
quantitative and qualitative analysis of data recorded in 
comprehensive user trials [8]. Secondly, we organized focus 
group sessions with developers and used an online survey as 
qualitative 
research 
tools 
in 
gathering 
additional 
requirements from developers and STB platform providers 
A. Initial User Trials 
The GUIDE project pursues a User Centered Design 
(UCD) process, taking into account that one of the main 
principles that characterize UCD is iterative design. 
According to this principle the system is designed, modified 
and repeatedly tested. This iterative cycle allows the 
designers to think in the product design and include the 
changes needed depending on the users’ feedback. Following 
this approach, an initial study to elicit user requirements has 
been carried out. 
1) Main Objectives 
Additionally to the identification of viable usage methods 
(gestures, command languages) of novel traditional UI 
paradigms for the different impairments in the target groups 
via user studies in realistic user scenarios, this user trials also 
have the goal to generate quantitative and qualitative user 
data in order to establish and construct a generic user model. 
This user model will provide data representations for each 
user and will constitute the first step for adaptation in 
GUIDE, and will “virtualize” user impairments to try to 
capture the amount of knowledge needed for application 
design. 
 
 
Fig 2. Test Application used in the technical user trials. 
2) Organization and Setting 
The initial user studies carried out can be divided in two 
different categories; one survey session and one technical 
trials session. While the aim of the survey was to collect 
qualitative information about application acceptance, user 
habits and modalities of interaction, the objective of the 
technical trials was to gather both quantitative and qualitative 
data and observe the interaction between the elderly and the 
system, performing simple tasks in the context of TV 
interaction. In this Test Application (figure 2), the users had 
the opportunity to experiment the different modalities and 
devices of interaction (table 1), and the tests were divided in 
several scripts concerning different types of interaction, or 
different UI elements and GUIDE aspects (table 2).  
 
Device/Modality of 
Interaction 
Input and Output on the 
User Test Application 
Remote Control 
Button selection (input) 
Wii Remote + Wii Sensor 
Bar 
Pointing, gesture and 
button selection (input) 
Kinnect + Kinnect Sensor 
(originally a Led Camera 
Sensor) 
Pointing, gesture and 
button selection (input) 
Avatar Engine 
Audio and visual output 
Speech Synthesis 
Audio output 
Simulated Speech 
Recognition (Wizard of Oz) 
Audio input 
Tablet (Apple’s IPad) 
Touch screen input and 
visual and haptic output. 
Table 1. User interface components used. 

430
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
Type of tests 
Task to perform 
Devices 
(modalities) 
used 
Modalities 
and devices 
experimentati
on 
Answering to questions 
related with preferences 
of interaction, 
experimentation of each 
device and modality. 
Menu items selection 
and navigation. 
Input: Remote 
control, Wii 
remote, 
Kinnect.  
Output: Visual 
menus and 
Avatar 
Visual 
capabilities 
and 
preferences 
Answering to questions 
related with interface 
visual configuration 
(font size and colour, 
background colour and 
button size and location 
tests). Menu items 
selection and navigation. 
Input: One or 
more devices 
chosen by the 
user. 
Output: Visual 
menus 
Audio 
capabilities 
and 
preferences 
Answering to questions 
related with audio 
preferences (audio 
volume) 
Input: Speech 
Output: Audio 
Cognitive 
capabilities 
Localization of different 
items on the screen 
(cognitive scientific 
tests), Measuring time of 
response 
Input: Speech, 
Wii Remote, 
Kinnect 
Output: Visual 
menus and 
pictures 
Motor 
capabilities 
and 
preferences 
Performing gestures. 
Menu items selection 
and navigation. 
Interacting with the 
Tablet. Answering to 
questions related with 
motor preferences and 
pointing mechanisms. 
Input: Wii 
Remote, 
Kinnect, 
Tablet 
Output: Visual 
menus 
Avatar 
preferences 
Interacting with the 
Avatar. Answering to 
questions related with 
Avatar preferences. 
Input: One or 
more devices 
chosen by the 
user  
Output: Visual 
menus, Avatar 
Multimodal 
preferences 
Menu items selection 
and navigation. 
Simulation of 
application contexts of 
use. Answering to 
questions related with 
multimodal interaction 
and preferences. 
Input: One or 
more devices 
chosen by the 
user. 
Output: Visual 
and Audio 
menus, Avatar 
Table 2. Modalities, tasks and devices. 
 
In [8] you can find a more extensive description of the 
tests performed. 
B. Developer Focus Groups and Survey 
The GUIDE system is not exclusively focused on elderly 
users, but also centered in developers of TV based 
applications and manufacturers of STBs. For this reason, 
major discussions regarding subjects like adaptation, elderly 
user’s interaction, type of applications, and developers 
requirements for making possible the GUIDE ideas, has 
taken place in this evaluation, by performing both focus 
group with these end-users and by launching an online 
survey with the same user target. 
1) Main Objectives 
The general goal is to explore and understand the 
common practice among developers working on STBs. Thus 
the first objective is to gain data about current tools and APIs 
used in Set top box/connected TV platforms and to 
investigate how accessibility is currently perceived and 
applied in the industry. Secondly, exploring developer 
knowledge to identify which tools would developers need to 
efficiently integrate GUIDE-enabled accessibility features 
into their applications. Additionally, stimulate new ideas 
through discussions and to identify new relationships 
between objects embodying GUIDE concepts and objects 
embodying common practice. And finally, inform STB 
application development community about GUIDE. 
2) Organization and setting 
Developer Focus Groups: Two focus group sessions were 
carried out with connected TV platform providers and 
developers of applications and user interfaces deployed on 
STBs in a natural and interactive focus group setting. The 
sessions were conducted by two moderators (for ensuring 
progress and topic coverage) and each focus group session 
had between six and eight participants and lasted between 
120 and 150 minutes. Sessions were initiated with 
presentations of scripts containing development and use 
cases that cover different aspects of the GUIDE project and 
its concepts. Presentations of each development case script 
lasted 10 minutes and were followed by 30 minutes of 
interactive brainstorming, and discussions. 
Developer Online Survey: A questionnaire was designed 
to investigate how accessibility is currently perceived and 
applied in the industry. In addition, the survey was used as a 
medium to let respondents vote on the most important 
features of the envisaged GUIDE framework and toolbox. 
Both survey and focus group  were composed by the 
following participant types: STB test developers, STB 
experts in Innovative part, Flash application developers, 
HTML developers, middleware STB developers, architects 
in STB platforms, GUI developers for STB, project 
managers for STB projects, managers in Innovative projects 
for STB, product and marketing managers, research 
community, 
and 
standardization 
bodies 
and 
related 
organizations. In total, 81 participants from 16 countries, and 
30 companies all over the world, participated.  

431
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
C. Results and Conclusions 
From the realization of both initial user-trials and 
developers focus group (and online survey), we now 
summarize qualitative results which will work as starting 
points for the next section of this paper:  
1) User Survey Results 
The large numbers of variables contained in the data set 
were submitted to a two-stage process of analysis where 
correlations were made and a k-mean cluster analysis [2] was 
performed, reducing the results to only significant data (why 
we used the variables listed below are described in [9]). 
Resulting from this, 3 user profiles capable of discriminating 
differences between users were created – low, medium and 
high. These profiles were formed by combining and 
grouping all modalities simultaneously such that a specific 
grouping may represent capability on users perceptual, 
cognitive and motor capability ranges. The main differences 
noticed were the following measures: capability to read 
perfectly from close and distant vision; capability of seeing 
at night, and color perception; capability to hear sounds of 
different frequencies and to distinguish conversations in a 
noisy background; cognitive impairments; and mobility 
diagnosis like muscular weakness and tremors. Table 3 
shows all the identified variables in the three profiles. 
 
Vision 
LOW 
MED 
HI 
Close vision: level  able to 
read perfectly  
20/20 
20/60 
20/80 
Distant Vision: level able 
to read perfectly (metres) 
5 
5 
20 
general eyesight 
good 
excellent 
normal 
seeing at distance 
good 
poor 
poor 
seeing at night 
normal poor 
poor 
colour perception 
good 
bad 
bad 
Hearing 
LOW 
MED 
HI 
Able to hear a sound of 
500Hz? 
Yes 
Yes 
No 
Able to hear a sound of 
2Khz? 
Yes 
Yes 
Yes 
conversation from a noisy 
background 
excellent normal 
normal 
Cognition 
LOW 
MED 
HI 
TMT (seconds)  
Cognitive executive 
function 
30  
(no 
impairm
ent) 
49 
(low 
impairment) 
136  
(high 
impairment) 
Motor 
LOW 
MED 
HI 
mobility diagnosis 
none 
hernia / 
slipped disc 
none 
muscular weakness 
never 
A few 
occasions 
Frequently 
write 
No 
difficult
y 
No 
difficulty 
Mild 
difficulty 
Tingling of limb difficulty No  
Mild  
Mild 
Rigidity difficulty 
No  
Mild  
Moderate 
Table 3. Cluster centers 
 
2) Technical User Trials Results 
Big, centered and well-spaced buttons were preferred by 
users because they are easier to see and select (and elderly 
users typically have some kind of visual and motor 
impairments). Additionally, users prefer medium sized fonts 
and medium volumes for audio, but users with impairments 
tend to prefer bigger fonts and higher volumes. However, 
more than based on user abilities or preferences, both visual 
and audio elements configuration, depends on the interaction 
context and must be at all times modifiable and repeatable by 
the user. All the preferences described regarding visual 
components, reflect the low efficiency (lot of time needed for 
each selection) and accuracy (wrong target when selecting) 
registered when interacting with any type of pointing in these 
tests.  
Users clearly preferred gestures easier to make (swipe 
and pinch), and have no problem whatsoever interacting by 
gestures. It was also evident that alternative ways of 
interacting with the TV (speech and finger pointing) are 
preferred to the traditional way. Also, training makes any 
type of modality more efficient as the user learns what is 
required to perform each interaction. However, any type of 
interaction should not be imposed on the users, but be 
available as an intuitive option for interacting with the TV. 
Additionally, when not used by the user intuition, modalities 
of interaction should be explained to the user before he or 
she starts using the system.  
Every user is able to interact multimodally with the 
system and combine speech and pointing, even when they 
prefer only one modality. Users exhibited different 
multimodal interaction patterns during the trials and there is 
no specific interaction pattern for each user (a user can speak 
first and point afterwards, and in the next interaction do the 
opposite). Users can also change the way they interact 
depending on the type of feedback given while interacting. 
Regarding user preferences in input and output modalities, 
there are clear differences between what users say they 
prefer, and what users really ask for when interacting. In 
fact, 100% of the users want multimodal output every time 
information is presented to them, because every user who 
said to prefer only one type of feedback admitted differently 
when in specific interaction contexts. The same happened 
concerning input modalities, with almost half of the users 
admitting, when confronted with practical tasks, that they 
were wrong when they said to prefer only one modality.  
The results obtained in these trials enforce the need of a 
multimodal system and also the need for adaptation, as we 
can see in a more detailed fashion in [8].  
3) Developer Focus Groups and Survey Results 
Developers agree that if users are involved in every 
development phase of the applications (or in the maximum 
phases possible), the resulting UI will be more usable. It was 
concluded that for elderly people UIs should be maintained 
clear and simple, however without giving the impression that 
it has been designed for someone with impairments (not 
leaving the feeling of a “system for seniors”). Additionally, 
costs are the current major reason for reduced application of 
user-centered design in the industry (followed by time and 

432
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
lack of awareness). As the current most important device on 
interaction with STBs, the remote control must continue to 
have a central role in the interaction, and should only be 
relegated to a secondary role if that is a result of each user 
interaction preferences.  Gesture control and speech input are 
recognized 
as 
secondary 
technologies. 
In 
general, 
participants agree that automatic adaptation of user interfaces 
can help elderly users to access ICT services. However 
GUIDE adaptation mechanism should never change interface 
aspects unless it is mandatory for specific user interaction. 
Also, radical changes in the UI must be avoid so that the user 
feels he/she is in control and not get lost in the interface. If a 
radical change is indispensable the UI must inform the user 
of the proposed changes. Identified as the main obstacle to 
UI adaptation is the fact that elderly users present too many 
differences between each other. Therefore, for adaptation to 
fit each user, GUIDE has to first find a way to know his or 
her impairments, preferences or characteristics. This 
“discovery” will have to occur the first time the user interacts 
with the system, and will have to be short, not too much 
intrusive and entertaining to the user. The most important 
conclusion debated in this subject is the one saying GUIDE 
should support UI mark-up as interface between application 
and GUIDE adaptation. This way, developers will be 
allowed to keep tools and development environments and 
without too much additional effort, take a first step to 
accessible design. Web developers mostly use HTML editors 
as the most important tools in Web & TV development. 
However, having to learn new development processes will 
drive developers away from the GUIDE framework. So, 
developers should not be required to develop taken into 
consideration specificities of the multimodal operations but 
have a clear specification of how such devices interact with 
the framework. As it was already described in UI adaptation 
results, identification of UI components should be made 
using only mark-up language, however applications coded 
using dynamic HTML (through JavaScript) must continue to 
be able to change, remove or insert elements in the currently 
rendering page. Meaning, all changes in application 
presentation will need to be identified at run-time. For most 
participants connected TV platforms and STBs will be most 
relevant platforms in the future. Also, Web-based application 
environments will become more important for Web & TV. 
Manufactures stated increasing STBs capabilities cannot 
raise its price to much, or development will be more difficult 
and costly. Developers also pointed out GUIDE system must 
consider situations where multiple users are using the TV 
and services.  
IV. 
MULTIMODAL APPLICATION DEVELOPMENT 
From the results and implications reported in the 
previous section of this paper, we now derive GUIDE project 
solutions for giving answers to the same questions raised in 
the beginning of this paper.  
A. Multimodal and Adaptive Framework 
We now give an overview of the GUIDE framework [8] 
(figure 3) following an interaction cycle, starting from the 
Fig 3. Multimodal and Adaptive Framework architecture in GUIDE 

433
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
user input and going through the construction of the system’s 
output to be presented to the user. 
 A user provides input through multiple devices and 
modalities which can be used simultaneously. The signals 
from recognition based modalities are processed by 
interpreter modules (e.g., a series of points from the motion 
sensor go through a gesture recognition engine in order to 
detect gestures). The signals from pointing modalities go 
through input adaptation modules (e.g., in order to smooth 
tremors from the user’s hand). Both interpreter and 
adaptation modules base their decisions on knowledge stored 
in the user proﬁle, thus improving the efficiency of noise 
reduction in the input signals. Then, the multimodal fusion 
module receives, analyses and combines these multiple 
streams (outputs of input interpreters and input adaptation 
modules, or raw data that did not go through any of these) 
into a single interpretation of the user command based on the 
user, context and application models (abstract representation 
of the application). This interpretation is sent to the dialogue 
manager who decides which will be the application’s 
response, basing its decision on knowledge about the current 
application state and the possible actions that can be 
performed on the application in that state. The dialogue 
manager decision is fed to the multimodal ﬁssion module, 
which is responsible for rendering a presentation in 
accordance to which output to present (derived from the 
application itself and the application model), the user 
abilities (accessed through the user model) and the 
interaction context (made available through the context 
model). The ﬁssion module takes all this information and 
prepares the content to render, selects the appropriate output 
channels and handles the synchronization, both in time and 
space, between channels when rendering. This rendering is 
then perceived by the user, which reacts to it, and starts a 
new cycle by providing some new input. 
B. User Initialization Application 
In both technical user-trials and focus groups, it is the 
necessity of knowing every user characteristics, preferences 
and impairments from the first time he or she interacts with 
the system. This is mandatory because of the user’s 
differences and the necessity of adapting both UI 
components and interaction to fit each user, as well as the 
necessity of instructing the user about every possibility of 
interaction in order to reach the maximum efficiency when 
using the system. GUIDE adaptation begins through a User 
Initialization Application (UIA) (figure 4) that allows for the 
acquisition of primary assumptions about the user. So, 
knowing that each user model contain assumptions about 
interesting characteristics of user subgroups, after “going 
trough” the UIA, a user is assigned to a user model as certain 
preconditions are met. From that moment on, and for any 
GUIDE application the user interacts with, the system is 
“initially” adapted to him/her. It’s relevant to say that the 
UIA is presented to the user as a simple step-by-step 
configuration of a “general” interface. In each step, different 
types of contents and different contexts of interaction are 
presented, so the user can test different components and 
parameters, and the system learns the user characteristics, 
from his impairments to his preferences. Addressing the 
results from the developer focus groups, every UIA run as to 
be short in time, intuitive and transparent to the user and also 
Fig 4. Screenshots of the first version of the User Initialization Application 

434
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
serve as a “tutorial” for learning every modality of 
interaction available in the system. Additionally, the user 
must be recognized (facial or voice patterns) by the system 
so that the information provided can be stored in a profile 
and loaded every time the user interacts with the system.   
C. Simulation of User Impairments 
As developers need tools for helping saving time and cost 
in the development of inclusive TV base applications, 
GUIDE offers a simulator [2] which will allow the developer 
to perform accessibility tests based on virtual users, saving 
much time in comparison to tests with real users. So, 
evaluation as a typically expensive step in user centered 
design is supported in GUIDE by a simulation functionality 
allowing to illustrate to developers how users with typical 
impairment profiles will perceive or may interact with an 
application. The simulator can show how certain visual and 
strength impairments influence the way a user perceives and 
visualizes a certain UI (e.g., how an elderly color blind user 
sees a specific UI), and also what are the effects of those 
impairments in the user interaction (e.g., predicting cursor 
paths on the screen or task completion times). This simulator 
can be characterized as a tool for helping developers to take 
adaptation into consideration at design time. Figure 5, shows 
the simulation results on top of the User Initialization 
Application for both visual and motor impairments.  
D. Filtering 
As verified by the inefficient and erroneous use of 
pointing interaction when performing selections in the user 
trials, elderly users potentially have a wide range of 
impairments that hinder their ability to communicate their 
intentions to an application. In some cases these impairments 
can be severe, and significantly affect the speed and 
accuracy. This leads to an inefficient or even undesirable 
interaction with an application. The use of cursor smoothing 
techniques in GUIDE consists in processing the raw user 
input to obtain a filtered input (Input Adaptation Module 
described in the framework). This requires the usage of 
efficient statistical signal processing schemes to estimate the 
user’s intended operations in real time. Basically it consists 
in the application of corrective forces and forcing relatively 
smooth paths in a cursor interaction as well as assigning 
attraction fields to UI elements. Therefore, the following 
graphical UI filters can help improving pointing interaction 
within the GUIDE project:  
• 
Exponential averaging: this modification calculates 
the cursor position pi as pi = αxi + (1-α)pi-1, where 
xi is the user input, pi-1 is the previous cursor 
position and α ϵ [0,1] is a parameter determining 
how strong the user input influences the cursor 
position. This method produces smooth cursor traces 
but has the drawback that it can produce a delay 
between user’s intended position and the actual 
position;  
Fig 5. Applying simulation of User Initialization Application (top images show visual impairments - Macular Degeneration and Visual 
Acuity Loss -, bottom images show motor impairments - moderate motor impairment and Parkinson's Disease) 
 

435
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
• 
Damping: This method introduces a quadratic force 
that opposes the velocity of the cursor preventing 
sudden changes in directory or speed when 
interacting;  
• 
Gravity well: This method warps the cursor space, 
generating attractive basins to ease the selection of 
visual targets. This simplifies pointing interaction 
selection forcing the selection of buttons or UI 
elements that are more close to the location where 
the user is pointing (figure 7). 
Considering the different user characteristics and 
impairments, and the different UI element configuration, the 
existence of these filters make possible that motor impaired 
users can more easily interact with pointing and also makes 
possible the use of small and less spaced buttons in 
applications UIs avoiding errors in selection caused by the 
proximity of the buttons. All because pointing interaction 
accuracy is raised, and user intentionality when pointing is 
taken into consideration (figure 6). 
E. Semantic Programming and Run-Time Adaptation:  
The specification of TV based applications in GUIDE 
will be based on Web-based languages like HTML, CSS and 
JavaScript because of their wide acceptance among 
developers and compliance with STB specifications. 
However, in GUIDE exists the additional side-condition of 
specifying multimodal applications that needs to be merged 
with these web-based specification languages. This is made 
by specifying additional information about how an 
application is supposed to adapt in different modalities. For 
this semantic annotations are added to the HTML code, 
based on the WAI-ARIA draft specification of the 
W3C.Only by providing this type of supplementary 
information it is possible for the system to create an abstract 
representation of the application. Then, using an automatic 
application transformation module the system converts the 
annotated 
application 
description 
into 
a 
modality-
independent application representation, the Application 
Model described in the framework. Subsequently, and 
depending on the user interacting and on the level of control 
defined by the application developer, adaptation of UI 
components is performed.  
Developers can create their applications and UIs in an 
established manner, and GUIDE automatically adapts the UI 
to the user. This avoids having to design many user interface 
templates for various heterogeneous user groups. Therefore, 
GUIDE provides the application developers with two 
possible levels of adaptive control:  
Augmentation: presentation and interaction options 
taken by the developer are not subject of change. Instead, if 
the user model suggests that the presentation is insufficient 
for the user abilities, the presentation is augmented in 
different modalities (for example supplementing a visual 
interface with sound feedback). The multimodal fission 
mechanism 
renders 
the 
application 
output 
directly, 
augmenting or not the rendered presentation depending on 
the user model. Figure 8 shows an augmentation example;  
Adjustment: application rendering is adjusted to the 
abilities of the user (for example adjusting components of a 
visual interface to fit user characteristics, like raising font 
size or button size). The rendering changes can be achieved 
Fig 6. Process of detection of user intentionality by applying filtering 
Fig 7. An example of missed clicking (left) and clicking with the gravity well filter (right) 
 

436
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
through CSS manipulation. Adjustment can be combined 
with augmentation. Figure 9 shows an adaptation example. 
 
Fig 8. Example of Augmented adaptation 
 
Fig 9. Example of Adjusted adaptation 
 
V. 
DEVELOPING AND EVALUATING THE USER 
INITIALIZATION APPLICATION 
A. Development 
1) Selection of tasks and metrics 
The tasks and metrics chosen for the UIA are the ones for 
which the resulting data is the most capable to assign the 
more appropriate profile to the user profile. They were 
selected from an analysis of the extensive survey data, taking 
into account the feasibility of gathering the data. For those 
instances where it was not feasible to gather the data in a 
living room environment, alternative sources were selected 
and combined to estimate the required data. A description of 
these variables is listed below: Color Blindness: Plates 16 
and 17 of Ishihara Test [6] as it may classify among 
Protanopia, Deuteranopia and any other type of color 
blindness; Dexterity: We estimated Grip Strength and 
Active Range of Motion of wrist from age, sex and height of 
users following earlier Ergonomics research [2]; Tremor: 
We conducted earlier a test involving a Tablet device in 
horizontal position, and estimated tremor from the average 
number of times users need to touch the screen to select 
small buttons. Details of the study can be found in a separate 
paper [4]. Additionally, other tasks were chosen with the 
purpose of allowing users to personalize the system, while 
being a hands-on tutorial regarding new modality interaction 
and feedback configuration. The most relevant ones are the 
following: Modality Introduction: Self-explanatory videos 
of how to interact with each modality, followed by “do-it-
yourself” tasks; Button and Menu Configuration: Button 
size, and font and background color configuration; Cursor 
Configuration: Cursor size, shape and color configuration; 
Audio Perception: Hearing capabilities and preferences. 
 
 
 
Fig 10. Applying Simulation on the UIA 
 
2) Accessible Interface 
The UIA has a simple user interface, with a different 
screen for every task and metric identified above. Few 
buttons are presented per screen (preventing user confusion). 
Every screen preserves the same navigation model - an area 
with “next”, “previous” and “repeat” buttons, and another 
visually distinct area for presenting information and requests. 
For every metric to be measured, tests are presented as 
simple questions about preferences. Also, for every modality 
available in the system, a video introducing its use is 
presented, followed by the possibility for the user to try it 
out. A virtual character (Figure 11. first screen) accompanies 
the user through this process, offering explanations and 
assisting the user in the personalization. As the user goes 
through each task and preference setting, the UIA adapts 
itself to the preferences already manifested. For example, if 
user manifests preference for big, blue buttons with yellow 
text, all buttons will be presented with those settings from 
that moment onwards. It is worth pointing out that the results 
of our previous study are reflected in the UIA’s design: high 
contrast colors, big, centered and well-spaced buttons, etc.  
Also, the GUIDE simulator was also used in the design of 
the UIA, to ensure that users with visual and motion 
impairments could use it with high efficiency (figure 10).  

437
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
B. Evaluation 
1) Study description 
With the goal of evaluating the efficiency and acceptance 
of the User Initialization Application by elderly a study was 
conducted. First we want to measure the efficacy of this 
application in discovering the relevant characteristics of 
users and assigning user profiles; and secondly, we want to 
evaluate how understandable the UIA is in terms of its goals 
and the instructions it provides; and finally, how easy it is for 
elderly to interact with this application, or if they would do it 
if it was part of their daily lives.  
2) Participants (Pre-Survey) 
We recruited 40 elderly people (24 female and 16 male) 
with different age-related disabilities. Users were recruited in 
two countries, with 21 participants (14 female and 7 male) 
being recruited in Spain and 19 participants (10 female and 9 
male) in the UK. The average age was 70.9 years old and the 
different user profiles were assigned to the participants in the 
following manner: 14 users with profile A, 22 users with 
profile B, and 4 users with profile C. All users participated 
voluntarily and all activities involved in this study were 
safeguarded from the ethical point of view.  
3) Apparatus 
The study was conducted in two locations (Spain and 
UK). Efforts were directed to create similar environment and 
technical conditions in both labs. Trials were conducted by 
usability experts. Users were given freedom to interact (the 
trial conductor would only intervene when really needed, or 
user asked for help). In what concerns the technical setup 
and specification, different modalities of interaction were 
configured: pointing resorted to the use of a Microsoft 
Kinect; for speech recognition we used the Loquendo SR 
engine; a simplified remote control, with less buttons than 
traditional ones and capable of controlling pointer 
coordinates using a gyroscopic sensor was made available; 
an iPad was used for tablet interaction; and a full 1080p 
HDMI TV with integrated speakers and a 32’’ screen was 
used for visual and audio output. User interactions and 
answers were video recorded.  
4) Design and Analysis 
We used a within-subjects design where all users ran the 
UIA. Qualitative analysis was retrieved from pre, 
intermediate and post-questionnaires. Quantitative data was 
retrieved from the UIA (user profile and interface 
preferences). Herein, we discarded quantitative measures like 
trial errors and time as the trials followed a semi-supervised 
methodology: the participants were motivated to perform the 
tasks on their own but they were free to ask questions when 
they felt lost. For binomial measures, Mcnemar’s test was 
performed, and Cohen’s Kappa was used to assess the inter-
reliability of the profile ratings. 
Fig 11. Implemented Version of the User Initialization Application 
 
 

438
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
C. Results 
1) Discovering Elderly Profiles with UIA 
Our take for adaptation relies on a User Model fed by the 
UIA. All participants in our study performed both the pre-
survey and the UIA. Twenty-nine out of forty profile 
assessments were performed similarly by the two methods 
(74%). The interrater reliability between the profiles 
assigned with the pre-survey and the UIA was found to be 
Kappa = 0.58 (p <.0.001), revealing a moderate agreement 
[21]. It is relevant to notice that the UIA enables the user to 
input preference values, something that goes beyond ability 
profile. This is likely to explain part of the mismatch (e.g., a 
user with no visual impairments is likely to prefer a higher 
contrast button when he is confronted with such an 
hypothesis). Another source of uncertainty may be the under-
statements by part of the users in the pre-survey. Indeed, in a 
questionnaire it is likely that part of the users fail to 
acknowledge some limitations while they clearly state them 
when confronted with an interface with options to surpass it. 
A deeper understanding of the mismatches that are not 
created by these observed flaws can only be retrieved in a 
more extensive evaluation by analyzing how both 
methodologies enable the users to improve performance. 
2) UIA evaluation by the Elderly 
Users are not used to use something like UIA, so it is 
important to assess how the users see this component and if 
they are willing to use such a thing to improve their 
performance.  
 
Question about the UIA 
Median 
IQR 
Have you understood why we do the UIA? [1 - 
Yes ; 2 - No] 
1 
0 
If you have had the system at home, would you 
go through it or skip it?  [1 - Would do it ; 2 - 
Would skip it] 
1 
1 
Do you think the UIA is too long? [1- Yes;2 - 
Neutral;3 - No] 
3 
0 
Were the instructions easy enough to 
understand? [1 - Yes; 2 - No] 
1 
0 
Did you notice any changes in the application 
while you were using it? [1 - Yes ; 2 - No] 
1 
1 
Table 4: Subjective ratings to the UIA 
 
The participants took between 12 and 37 minutes to 
complete the UIA (M=22.8, SD=5.9). Once again, although 
they were discouraged to engage in long dialogues the 
participants were free to express their opinions and doubts 
during the UIA which increased the time to finalize the 
process. The UIA classified 16 people as profile A, 20 as 
profile B, and 4 as profile C. Table 4 presents the subjective 
ratings given by all the participants to the questions posed. 
Regarding the understanding of the purpose of the UIA 
(Question 1), 9 out of 40 (22%) did not understand the 
purpose of the UIA. This indicates that such a process should 
be better motivated or else it will be likely ignored by the 
users. In line with this, 11 out of 40 (28%) stated they would 
skip the process if they had the system at home (Q2). Five 
participants stated to find the process too long while four 
other were neutral about it (Q3) All the remaining thought it 
was neither too long nor tiring. Most users (35) thought the 
UIA was easy to follow and understand (Q4). Regarding the 
adaptations felt during the UIA (Q5), 26 participants stated 
to have noticed them. This is easily explained as 16 
participants were classified as profile A which means they 
had little or no adaptations done during the UIA. In sum, the 
users seem positive towards the UIA (Table 2) although it is 
clear that it should be well motivated and accompanied. 
D. Discussion 
Upon analyzing the UIA process and its impact on 
adaptation along with the usage of the GUIDE system and its 
underlying concepts, we answer our research topics as 
follows: 
1) Deriving a suitable user adaptation profile through 
the UIA.  
The UIA aims at creating a user profile by performing a 
simple set of questions and interactive tests. Results showed 
that the UIA is able to match profiles obtained with an 
extensive survey in 74% of the cases. Further, the UIA 
showed to be more realistic than its paper-based counterpart 
as data is likely to be more accurate when the users are faced 
with their limitations rather than just being questioned about 
them. Moreover, the UIA gives space for preference and 
subjectiveness. In sum, we consider that adapted TV 
applications based on simple initialization profiling are 
feasible 
and 
likely 
to 
improve 
over 
traditional 
methodologies.   
2) Acceptance of the UIA.  
The UIA took over 12 minutes, averaging around 23 
minutes. This amount of time can be discouraging for an 
elderly user if the benefits are not clear. Taking in 
consideration that it is supposed to be ran only once, the 
participants showed to be very positive about it. This is 
supported by the almost general understanding of the 
purpose of the UIA: they understood the benefits of such an 
application and perceived the adaptations during the process. 
Most participants (35) considered the application easy to 
follow which indicates that although the concepts underlying 
the creation of the user model are complex, the interface to 
generate it is not.  
VI. 
CONCLUSIONS 
New interaction paradigms, supported by new modalities 
and applications, are transforming a classical appliance that 
is the TV. If not handled properly, this transformation can 
increase the access barriers to TV content for elderly users.  
In this paper, we assessed several of the proposals that 
the GUIDE project puts forward in order to increase the 
accessibility of TV applications. GUIDE aims to provide 
application 
developers 
with 
a 
multimodal 
adaptive 
framework and a set of functionalities that will increase their 
products’ accessibility, without demanding major changes in 
their development process. The assessment was based on a 
user trial, with 40 participants from two different countries.  
The results obtained in this technical user-trials about the 
existence of disparity between what modalities users say they 

439
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
need, and what modalities they ask for when using the 
system, favors multimodality almost every time. This only 
helps to prove that the use of several input and output 
modalities is indispensable in the development of 
multimodal 
TV 
based 
applications 
for 
all. 
Also 
indispensable, are the components identified in the GUIDE 
framework, and the combined use of semantic programming 
and run-time adaptation mechanisms to fit UI components to 
each user characteristics. Additionally, the use of a simulator 
of user impairments can help developers understand at 
design-time how certain UI templates and components are 
perceived by different users with different impairments, 
preventing 
user 
exclusion 
and 
making 
accessible 
applications easier to design. 
Being an adaptive framework, it relies on knowledge and 
information about users. Essential for both providing and 
collecting knowledge, is the UIA, a process that streamlines 
user profile identification, based on short number of tasks 
and questions. We present an assessment of the efficacy of 
this process, concluding that it is possible to reliably identify 
user profiles, while also recognizing ways in which to further 
improve the process. From the user’s point of view, the 
process motivation was understood, and it was considered 
easy enough, although also here we were able to find ways to 
improve it. 
These results show a positive acceptance of the GUIDE 
concepts and their expected impact in the quality of life of its 
users, validate the approach followed so far and pave the 
road for the project’s future developments, which will be 
verified in a longitudinal trial for better assessing the effects 
of adaptation and multimodality. 
A. Future work. 
Regarding the use of modalities, speech interaction was 
singled out as the most attractive modality. In this study, a 
Wizard-of-Oz approach was used to replace the speech 
recognition engine, and as we question ourselves on how that 
might have contributed to the results, a follow-up study, 
where a real speech recognition engine is used, is necessary. 
It seems safe to say that speech plays an important role in 
promoting the adoption of these systems, and efforts to 
ensure its adequate operation are justified by the satisfaction 
it provides users with. Tablets, although not fully integrated 
with the system in this study, collected a positive response 
from participants, with 92% of them considering interacting 
with a TV using the Tablet. This tendency is also to be 
confirmed in the future with a study where users may be 
asked to execute tv-related tasks on a tablet. Finally, 
regarding the clustering process, by increasing the number of 
users available it will be possible to update the profiling 
process, resulting in a more accurate representation of the 
users’ characteristics and a more precise identification of the 
relative importance of each variable.  
ACKNOWLEDGMENT 
The research leading to these results has received funding 
from the European Union’s Seventh Framework Programme 
(FP7/2007-2013) under grant agreement nº 248893. 
REFERENCES 
[1] Coelho, J., Duarte, C., Feiteira, P., Costa, D. and Costa, 
D.. Building Bridges Between Elderly and TV 
Application Developers. In ACHI 2012 
[2] Balme, L., Demeure, A., Barralon, and N., Coutaz, J. & 
Calvary, 
G.. 
CAMELEON-RT: 
A 
Software 
Architecture 
Reference 
Model 
for 
Distributed, 
Migratable, and Plastic User Interfaces. In EUSAI’2004  
[3] Biswas, P., Robinson, P., and Langdon, P.: Designing 
inclusive interfaces through user modelling and 
simulation. International Journal of Human Computer 
Interaction. 
[4] Blechschmitt,E., and Strodecke,C.: An architecture to 
provide adaptive, synchronized and multimodal human 
computer interaction. In MULTIMEDIA ’02, NY, 
USA, pp. 287-290.  
[5] Bouchet, J., and Nigay, L.: ICARE: a component-based 
approach for the design and development of multimodal 
interfaces. In CHI ’04, NY, USA, pp. 1325-1328.  
[6] Calvary, G., Coutaz, J., and Thevenin, D.: A unifying 
reference framework for the development of plastic 
user interfaces. In EHCI ’01, London. UK, pp. 173—
192. 
[7] Calvary, G., Coutaz, J., Thevenin, D., Limbourg, Q., 
Souchon, 
N.,Bouillon, 
L., 
Florins, 
M., 
and 
Vanderdonckt, J. (2002). Plasticity of user interfaces: A 
revised reference framework. In TAMODIA ’02, 
Bucharest, pp. 127-134. 
[8] Coelho, J., and Duarte, C.: The Contribution of 
Multimodal Adaptation Techniques to the GUIDE 
Interface. HCII2011, Orlando, Florida, USA, pp. 337-
346. 
[9] Coelho, J., and Duarte, C., Biswas, P., Langdon, P. 
Developing Accessible TV Applications, Proceedings 
of ASSETS 2011, pp. 131-138. 
[10] Dragicevic, P., and Fekete, J.D.: The input conﬁgurator 
toolkit: towards high input adaptability in interactive 
applications. In AVI ’04, ACM Press, NY, USA, pp. 
244-247. 
[11] Elting, C., Rapp, S., Mohler, G., and Strube, M.:  
Architecture and implementation of multimodal plug 
and play. In ICMI ’03, ACM Press, NY, USA, pp. 93-
100. 
[12] Garg, A., Pavlovi´ c, V., and Rehg, J. (2003). Boosted 
learning in dynamic bayesian networks for multimodal 
speaker detection. Proceedings of  IEEE 91, pp. 1355–
1369. 
[13] Gotz, D., and Mayer-Patel, K.: A general framework 
for multidimensional adaptation. In MULTIMEDIA’04, 
NY, USA, pp. 612-619. 
[14] Harders, M., and Szekely, G. (2003). Enhancing 
human-computer interaction in medical segmentation. 
Proceedings of IEEE, 91, pp. 1430–1442.  

440
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[15] Martin, J.C., Julia, L., and Cheyer, A.:. A theoretical 
framework for multimodal user studies. In CMC98, 
Tilbur, Netherlands, pp. 104-110. 
[16] Oakley, I., Brewster, S. A., and Gray, P. D.: Solving 
multi-target haptic problems in menu interaction. 
CHI’01, Seattle, USA, pp. 357-358. 
[17] Oviatt S. L. Multimodal interactive maps: Designing 
for human performance. Human-Computer Interaction, 
1997, pp. 93-129 
[18] Oviatt, S. L., DeAngeli, A., and Kuhn, K. Integration 
and synchronization of input modes during multimodal 
human-computer interaction. CHI '97, New York, USA, 
pp. 415-422.  
[19] Oviatt, S.L.: Mutual Disambiguation of Recognition 
Errors 
in 
a 
Multimodal 
Architecture. 
CHI’99, 
Pittsburgh, USA, pp. 576-583.  
[20] Sharma, R., Yeasin, M., Krahnstoever, N., Rauschert, 
ICai, G., Brewer, I., Maceachren, A.M., and Sengupta, 
K. (2003). Speech-gesture driven multimodal interfaces 
for crisis management. Proceedings of IEEE, 91, pp. 
1327-1354  
[21] Vitense, H. S., Jacko, J. A., and Emery, V. K. 
Multimodal feedback: An assessment of performance 
and mental workload. Ergonomics 46,  2003, pp. 66-87.
 

