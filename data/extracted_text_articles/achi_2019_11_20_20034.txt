Lifelog Sharing System based on Context Matching
Jiaming Zhang
Graduate School of IPS
Waseda University
Fukuoka, Japan
Email: zhangjiaming777@toki.waseda.jp
Jiro Tanaka
Graduate School of IPS
Waseda University
Fukuoka, Japan
Email: jiro@aoni.waseda.jp
Abstract—In this paper, we propose a lifelog sharing mechanism
based on matching the situation context of audience users and
shared lifelogs. Sharing lifelogs can promote information sharing
among people and beneﬁt people who face similar situation.
However, it is difﬁcult and tedious for audience users to access
useful or interesting information from lifelog data taken in
chronological order. Our proposed lifelog sharing system pushes
appropriate shared lifelogs to audience users by matching the
context in real time. Audience users can get information in an
unobtrusive way by wearing a head-mounted display. The system
also allows audience users to give feedback and customize their
preferences. To collect lifelog data, we assume sharing users
to capture lifelogs with Autographer and Android smartphone.
When uploading captured data to share, sharing users need to
set sharing preferences to protect privacy. From the preliminary
evaluation, we have obtained a positive feedback.
Keywords–Lifelog; Wearable camera; Sharing mechanism; Sit-
uation context matching; Augmented reality.
I.
INTRODUCTION
Lifelogging is the pervasive activity that assists people in
recording their daily events in detail. In 1980, Steven Mann
built a wearable personal imaging system, which is equipped
with head-mounted display, cameras and wireless communica-
tions. The prototype system could capture images from ﬁrst-
person perspective [1]. The miniaturization enables devices to
be more unobtrusive and gain more social acceptance. More
and more commercial wearable devices have been produced
and entered the market.
Existing wearable cameras include SenseCam, Vicon Re-
vue and Autographer, which can capture photos passively
and continuously. For example, Autographer is a wearable
camera that has 6 built-in sensors [2]. The accelerometer
measures the change of speed when the camera is moving;
the color sensor is used to perceive light and brightness;
the passive infrared sensor detects moving objects before the
camera; the magnetometer detects the direction in which the
camera is facing; the temperature sensor measures environment
temperatures; and the integrated Global Positioning System
(GPS) locates the camera’s position. Autographer will capture
photos automatically after certain elapsed time periods, such
as 30 seconds. The sensor changes also can trigger the camera.
The lifelog data generated by lifelogging brings new op-
portunities for many research ﬁelds including quantiﬁed self,
healthcare, memory augmentation and so on. Nowadays, Social
Networking Service (SNS) is getting more and more popular,
which enables people to communicate and share knowledge
with each other. Sharing lifelogs can promote the information
sharing among people because lifelogging can record all the
details of our daily experiences and one’s experience can
beneﬁt other people who face similar situation or have common
interest. However, current SNS is not suitable for lifelog
sharing. For the users who view shared lifelogs, so-called
audience users, the accessing method is limited. It is difﬁcult
and tedious to access useful or attracting information among
the vast amount of lifelogs, because current systems manage
shared photos in chronological order mainly, and users can
only search photos with hashtags and location. However, the
format of hashtags that were added by sharing users are not
uniform.
Because the amount of data produced by electronic de-
vices is increasing, recommendation systems are available for
users to access relevant information from the vast amount of
information [3]. Also, context-aware recommendation system
has been researched in various domains, such as e-commerce,
multimedia, tourism, to provide a better personalized user
recommendation leveraging contextual information. The pos-
sibility of using context-aware computing in lifelog retrieving
has not been investigated yet.
Some previous research proposed approaches about how
to retrieve lifelog more efﬁciently [4]–[8], but less attention
is paid to how to share lifelog and how the users access
shared lifelog based on their current situation in real time.
For example, you are running outside for exercise and you
might want to view other’s nearby running record to motivate
yourself.
Our target is to propose a lifelog sharing system, which en-
ables audience users to access useful or attracting information
easily from shared experiences when they are facing speciﬁc
situation.
The rest of the paper is structured as follows. In Section II,
we describe the goal and approach. In Section III, the sharing
mechanism is presented. We provide an overview of the lifelog
sharing system in Section IV. Then, we describe user study and
results in Sections V. In Section VI, we discuss related work.
Finally, conclusion and future work is discussed in Section
VII.
II.
GOAL AND APPROACH
In this paper, we aim to propose a lifelog sharing system.
There are mainly two roles using the sharing system, including
sharing user and audience user (Figure 1).
Sharing user uses Autographer and Android smartphone
to capture the lifelog data. Autographer is used for taking
photos. Android smartphone is used for monitoring activities
185
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

Figure 1. Wearable devices for sharing user and audience user.
by analyzing the signals from multiple sensors embedded in the
Android device. After recording, sharing user should upload
the lifelog data to share. By leveraging computer vision service
and location-aware service, our system extracts context within
the shared lifelogs automatically, which can be used to match
the audience user’s situation in real time.
For the audience user, we introduce the Augmented Reality
(AR) and context-aware computing technology. Through the
head-mounted display, our system presents appropriate shared
lifelogs to the audience user by matching the context in an
unobtrusive way without interrupting what the user is doing. In
this work, we choose Epson Moverio BT-300, which contains
smart glass and controller.
III.
SHARING MECHANISM
We propose a new sharing mechanism for our system:
pushing appropriate shared lifelogs to audience user by match-
ing the situation context of user and shared lifelogs in real
time. By replaceing explicit request with active push, we aim
to save audience user’s effort in searching and ﬁltering useful
information.
To ﬁnd out valuable information in describing situation and
deﬁne the situation context in our research, we summarized
some previous work. Dey [9] proposed a generic deﬁnition of
context. Context is any information that can be used to charac-
terize the situation of an entity. An entity is a person, place, or
object. Grubert et al [10] categorized context into three high-
level categories, including human factors, environment factors
and system factors. Also, our previous research [11] found
several cues are important in describing an event, which can
help people understand what happened, including where and
when the event happened, what object the user interacted with.
Therefore, situation context in this research consists of human
and environment factors (Figure 2).
Human factors focus on the user, including activity and
preferences. Activity is the bodily movement, such as walking,
running, etc. Preferences has a different meaning for lifelogs
and audience users. For lifelogs, it means the sharing prefer-
ences that are set by sharing user; for audience users, it refers
to the objects that audience user prefers or has interest in, such
as food, park, etc.
Environment factors describe the surrounding of the user
in which the experience took place. Location and time mean
Figure 2. Situation context.
where and when the experience happened. Object is what
appeared in the user’s sight.
IV.
LIFELOG SHARING SYSTEM
In this section, we will describe our lifelog sharing system.
We will provide an overview and describe our state of the
development.
A. Usage Scenario
•
Scenario 1. A person is walking on the street in the
morning and approaching a bakery that he has never
been in. One other person shared lifelog photos that
were taken when he bought bread in this bakery. These
photos may be useful for this person. If he has interest
or thinks the bread looks delicious, he can go into the
bakery and buy some bread for himself.
•
Scenario 2. A person is running outside for exercise in
the evening. He might feel boring or tired. Some other
people also ran nearby and shared photos and running
record that were taken when they were running. The
person can view the photos and running record such
as the running speed of others via the head-mounted
display without stopping running, and he may be
motivated to run at a proper speed.
B. System Overview
Our proposed system mainly contains two parts (Figure 3).
The most important part is for audience users accessing shared
lifelogs. The AR-based viewer pushes appropriate shared lifel-
ogs to the audience user by matching the situation context of
audience user and lifelogs.
The other part is for sharing users capturing and uploading
lifelog data to share to others. The lifelog uploader will extract
Figure 3. System overview.
186
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

situation context, including location, object, time and activity,
from input data and store them into the database.
C. Sharing User’s Part
1) Activtiy Recorder: The activity recorder implemented
on Android is used to track and upload share user’s activity
records.
In our work, we make use of Moves API [12]. It can
automatically record any walking, cycling, and running the
user does and generate daily activity summaries, including
step count, distance, duration and consumed calories for each
activity.
2) Lifelog Uploader: The web-based lifelog uploader as-
sists sharing users in uploading the photos captured by Autog-
rapher (Figure 4). It is implemented based on Browser/Server
architecture and mainly uses the combination of the Spring
Boot and the Hibernate.
Before sharing, sharing users need to set the sharing
preferences, which consider two aspects. One is the scope of
visibility, sharing users should choose to share their lifelogs
with friends or all users. The other is to choose whether to
expose location or object information within the lifelog. For
location information, sharing users can set to share location at
country, city or street level.
The lifelog uploader extracts situation context from up-
loaded data according to the sharing preferences set by sharing
user by integrating with several existing computer vision
service and location-aware service, including Google Cloud
Vision API and Google Maps API (Figure 5). To protect
bystanders’ privacy, the system blurs the detected faces in
lifelog photos using Marvin Image Processing Framework.
D. Audience User’s Part
For audience users, we develop the AR-based viewer on the
Epson Moverio BT-300, which adopts Android as the operating
system. The viewer system mainly provides three functions:
viewing shared lifelogs, giving feedback to pushed photos and
customizing preferences.
1) Viewing Shared Lifelogs: The viewer system displays
appropriate shared lifelogs that match the users current situa-
tion (Figure 6). When there are more than one pushed lifelog
photos, audience users can view more by clicking the Next
button. If audience users don’t want to view any information
at present, they can click the Close button to hide the display
panel, which will appear again when the system get new
pushed lifelogs.
Figure 4. User interface of Lifelog Uploader.
Figure 5. Extracting context according to the sharing preferences.
Figure 6. Viewing pushed lifelogs.
To select appropriate lifelogs, we propose a push strategy
which pushes appropriate lifelogs to audience users automat-
ically by calculating the situation context similarity between
user and shared lifelog data (Figure 7).
The ﬁrst phase is detecting user’s activity to determine the
push frequency. The viewer system recognize activities using
HARLib, which is a human activity recognition library on
Android proposed by Yang et al [13]. For each activity, the
push strategy provides a default frequency depending on the
average moving speed of the activity. Especially for transport,
the system won’t push any information considering user’s
safety because the pushed information may interfere user when
he is driving. The second phase will get candidate lifelogs and
rank them by calculating the similarity score between user’s
situation context and lifelog data’s situation context, which
will be performed once after the time period corresponding
to current detected activity. Then, the system will push the
lifelogs that have the similarity score over the threshold, which
is set to 2.4.
187
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

Figure 7. Push strategy.
In order to deal with the huge data volume and improve
the query performance, we make use of Haversine formula
to ﬁlter out lifelogs that are created within 100 meters from
the audience user as the candidate lifelogs, and only calculate
similarity score for these candidate lifelogs.
The similarity value contains four situation context factors,
including location, activity, object and time, in the range (0,4]:
similarity = locationFit + activityFit +
objectFit + timeFit
(1)
where locationFit is determined by the distance between
audience user and lifelog’s creation location:
locationFit =

1,
lu = lp
x,
x = inverseDistance(lu, lp)
(2)
where inverseDistance is a negative exponential function of
distance in the range (0,1]. ActivityFit compares audience
user’s activity and the corresponding activity of lifelog photos:
activityFit =

1,
au = ap
0,
au ̸= ap
(3)
and objectFit evaluates the suitability between lifelogs and
user’s preferred objects, which is inferred from user’s liked
photos history in the range [0,1]. The offset is used to solve
the cold start problem. When audience user has not given any
feedback, the offset will be set to 0.6 to eliminate the impact
on the ﬁnal similarity score, and the value of offset will get
smaller with more feedback are given:
objectFit = intersection size(op, oul)
size(op)
+ offset
(4)
and timeFit evaluates the degree of difference in time in the
range (0,1]. We deﬁne 6am to 10am as morning, 10am to 2pm
as noon, 2pm to 6pm as afternoon, 6pm to 10pm as evening
and 10pm to 6am as night. The closer the time, the higher the
timeFit value.
2) Giving Feedback: Audience user can give feedback to
pushed lifelog photo by clicking the Like button (Figure 8),
which means the user has interest in this photo or the objects
that appear in it.
3) Customizing Preferences: The AR-based viewer allows
audience users to customize what kind of speciﬁc informa-
tion they want to view and how often they would get new
pushed lifelog. The customization panel contains three parts
(Figure 9). Audience user can select speciﬁc objects to reﬂect
their object preferences. They can also select the activity
to view corresponding activity record of the pushed lifelog
photo (Figure 10). To provide a better user experience, the
viewer system assists audience users in customizing the push
frequency instead of the default ones. This feature can adapt
to the situations where audience users want view more or less
information.
V.
PRELIMINARY EVALUATION
We conducted a preliminary user study to verify whether
audience users can get useful shared lifelogs easily with our
proposed system and evaluate the usability of the system.
Figure 8. Giving feedback to lifelog photos which contain Bread object.
188
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

Figure 9. User interface of customizing preferences.
Figure 10. Viewing the lifelog photos with corresponding activity record.
We need to collect lifelog data ﬁrst. We let 4 people
capture lifelogs for one day using Autographer and Android
smartphone and they were free to switch off the devices during
their private time. After capturing, we let these 4 people set
their sharing preferences and upload lifelogs to our system
to share. We got 784 lifelog photos in total. After collecting
shared lifelogs, the user study can be carried out.
A. Participants
We invited 8 participants to use our system as the audience
users, ranging in age from 23 to 25 and including 6 females
and 2 males. All of them have regular computer skills and they
were given a brief introduction of our system.
B. Method
Each participant needs to use our system to view shared
lifelogs by wearing the head-mounted display for at least half
an hour. To ensure all participants can get pushed lifelogs, dur-
ing they use the system, the range of activities of participants
should be within the area of shared lifelogs captured places.
After that, the participant will be asked to ﬁll in a question-
naire. The questionnaire has following 4 questions and these
questions use the 5-point Likert scale:
1)
Do you think pushed lifelogs are useful or interest-
ing?
2)
Do you think the preferences customization is helpful
in viewing shared lifelogs?
3)
Do you think the push frequency customization is
helpful in viewing shared lifelogs?
4)
Do you think the system is easy to operate?
C. Results
After collecting the questionnaire results from the 8 par-
ticipants, we calculated the average scores of each question
(Figure 11).
Question 1 is used to ask participants about the subjective
feelings of the pushed lifelogs. All participants used our system
for average 40 minutes and the average score of question 1 is
4.25. The results suggest that the participants generally found
the pushed lifelogs are useful or interesting in their speciﬁc
situation.
Question 2 and 3 are used to judge the design of the cus-
tomization function in our system and the average scores for
these two questions are 4.625 and 4.125. Results of question
2 indicate that each participant thought providing preferences
customization is helpful to reﬂect their interested objects. For
question 3, the results show that enabling users to customize
the push frequency is helpful. Most of the participants claimed
that customizing push frequency makes the lifelog displaying
much more ﬂexible.
Question 4 regards the ease of use of our system. It mainly
concerns whether it’s easy to use the controller to interact
with the system. The average score is 3.75. The results prove
that the system is easy to operate. Two participants considered
that the controller is not hands-free although the glasses can
superimpose digital information in an unobtrusive way. It was
difﬁcult to operate the system when participants were cycling.
Overall, we got a positive feedback through the preliminary
user study.
VI.
RELATED WORK
The most similar approach is the work of Memon [14],
which proposed a lifelog sharing framework which can identify
the target audience users who may ﬁnd shared lifelogs useful
based on locality. Sharing users of the system have to deﬁne
their sharing strategy by declaring the scope of visibility of
their lifelogs, which are, particular city, particular street or
location independent. For example, ‘particular city’ shared logs
are visible to the friends who visit that city. Audience users
Figure 11. Questionnaire results.
189
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

can retrieve friends’ shared lifelogs by sending a request to
the server with their current location.
This work focused on sharing lifelogs with friends and
the sharing strategy is only based on locality. In our work,
we consider more speciﬁc context by deﬁning the situation
context. In Memon’s work, Moodstocks API was applied to
read barcodes, QR-codes or identify objects. But it needed to
previously store the objects’ templates at Moodstocks server.
Our system makes use of computer vision service for object
recognition, which needs no preparation or deployment. An-
other difference is that our system is presented for the scenario
that the audience user is actually being in the speciﬁc situation.
User can get shared lifelogs in real time without any requests.
With augmented reality technology, user don’t need to interrupt
what he is doing.
Another important related work is the proactivity model
for mobile recommendation systems which is proposed by
Woerndl et al. [15]. The two-phase model can be used in a
proactive, context-aware recommendation system by utilizing
the available context information. In the ﬁrst phase, the system
determined whether the current situation needs a recommenda-
tion by calculating score of weighted combination of contexts.
The second phase dealt with the evaluation of candidate items,
and the system would push the items which are considered
good enough in the current context to the user. A prototype for
the gas station scenario was implemented, in which case that
the user context refers to the fuel level, trafﬁc is the temporal
context, the geographic context is the nearest gas station and
the social context corresponds to the number of persons in the
car.
Based on the proactivity model, we deﬁned the push
strategy in our system. Different from this work, the contexts
deﬁned in our system are more general to meet different
scenarios instead of the speciﬁc gas station scenario. Our
system considers not only adapting the content represented
by the system according to the context, but also system’s
conﬁguration, that is adapting the push frequency according
to the user’s activity to provide a better user experience.
VII.
CONCLUSION AND FUTURE WORK
In this work, we propose the lifelog sharing system with
the mechanism that matches the situation context of audience
user and shared lifelogs.
In our proposed system, there are sharing users and audi-
ence users. To collect lifelog data, the sharing users in our
system need to use Autographer and Android smartphone.
When uploading captured data, sharing users can set the
sharing preferences to protect privacy. The AR-based viewer
developed on head-mounted display pushes appropriate lifel-
ogs to audience users in real time, which allows audience users
to view pushed lifelogs, give feedback to pushed lifelog photos
and customize their preferences.
In the future work, the proposed system needs further
improvement. For example, we can incorporate other useful
contexts in our system and improve the push strategy to give
more suitable or desirable shared lifelogs to audience users.
So far, the system allows audience users to give feedback
to the pushed lifelog photos and sharing users can view the
feedback they get. The interaction between audience users and
sharer users can be improved to enhance the communication
and information sharing between people. After that, we plan
to perform user study which involves more participants to
get more convincing feedback. We also plan to compare our
system to current SNS to justify the performance for retrieving
useful lifelogs and displaying to the audience users.
REFERENCES
[1]
S. Mann, “Wearable computing: A ﬁrst step toward personal imaging,”
Computer, vol. 30, 1997, pp. 25–32.
[2]
K. C. Thoring, R. M. Mueller, and P. Badke-Schaub, “Ethnographic
design research with wearable cameras,” in Proceedings of the 33rd
Annual ACM Conference Extended Abstracts on Human Factors in
Computing Systems, ser. CHI EA ’15.
ACM, 2015, pp. 2049–2054.
[3]
e. a. Khalid Haruna, “Context-aware recommender system: A review
of recent developmental process and future research direction,” Applied
Sciences, vol. 7, 2017, p. 1211.
[4]
K. Aizawa, D. Tancharoen, S. Kawasaki, and T. Yamasaki, “Efﬁcient
retrieval of life log based on context and content,” in Proceedings of the
the 1st ACM workshop on Continuous archival and retrieval of personal
experiences, 2004, pp. 22–31.
[5]
L. Zhou, D.-T. Dang-Nguyen, and C. Gurrin, “A baseline search engine
for personal life archives,” in LTA ’17, 2017, pp. 21–24.
[6]
e. a. L. Zhou, “Organizer team at ImageCLEFlifelog 2017: Baseline
approaches for lifelog retrieval and summarization,” in CLEF, 2017,
pp. 154:1–154:11.
[7]
e. a. D.-T. Dang-Nguyen, “Overview of ImageCLEFlifelog 2017:
Lifelog retrieval and summarization,” in CLEF, 2017, pp. 10:1–10:23.
[8]
D.-T. Dang-Nguyen, L. Zhou, R. Gupta, M. Riegler, and C. Gurrin,
“Building a disclosed lifelog dataset: Challenges, principles and pro-
cesses,” in CBMI, 2017, pp. 22:1–22:6.
[9]
A. K. Dey, “Understanding and using context,” Personal Ubiquitous
Comput., vol. 5, 2001, pp. 4–7.
[10]
J. Grubert, T. Langlotz, S. Zollmann, and H. Regenbrecht, “Towards
pervasive augmented reality: Context-awareness in augmented reality,”
IEEE Transactions on Visualization and Computer Graphics, vol. 23,
2017, pp. 1706–1724.
[11]
J. Zhang, J. Liang, and J. Tanaka, “A lifelog viewer system supporting
multiple memory cues,” in Human-Computer Interaction. Interaction in
Context.
Springer International Publishing, 2018, pp. 638–649.
[12]
Moves, “Moves api,” https://www.programmableweb.com/api/moves/,
accessed Dec 25, 2018.
[13]
H.-C. Yang, Y.-C. Li, Z.-Y. Liu, and J. Qiu, “Harlib: A human activity
recognition library on android,” in Proceedings of 11th International
Computer Conference on Wavelet Active Media Technology and Infor-
mation Processing, 2014, pp. 313–315.
[14]
M. Memon and J. Tanaka, “Sharing life experiences with friends based
on individuals locality,” in Design, User Experience, and Usability. Web,
Mobile, and Product Design, vol. 8015, 2013, pp. 706–713.
[15]
W. W¨orndl, J. Huebner, R. Bader, and D. Gallego, “A model for
proactivity in mobile, context-aware recommender systems,” in RecSys,
2011, pp. 273–276.
190
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

