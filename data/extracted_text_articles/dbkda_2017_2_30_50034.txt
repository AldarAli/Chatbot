A Pseudometric for Gaussian Mixture Models
Linfei Zhou∗, Wei Ye∗, Bianca Wackersreuther∗, Claudia Plant† and Christian B¨ohm∗
∗ Institute for Computer Science, Ludwig-Maximilians-Universit¨at M¨unchen
† Department of Computer Science, University of Vienna
Email: ∗{zhou, ye, wackersb, boehm}@dbs.iﬁ.lmu.de, †claudia.plant@univie.ac.at
Abstract—Efﬁcient similarity search for uncertain data is a
challenging task in many modern data mining applications such
as image retrieval, speaker recognition and stock market analysis.
A common way to model the uncertainty of the objects is using
probability density functions in the form of Gaussian Mixture
Models (GMMs), which have the ability to approximate any ar-
bitrary distribution. However, there is a lack of suitable similarity
measures for GMMs. Hence, in this paper we propose a similarity
measure, Pseudometric for GMMs (PmG). The advantage of
PmG is that it is efﬁcient in computation because of its closed-
form expression for GMMs, and it fulﬁlls the triangle inequality
which is necessary for many techniques like clustering and
embedding. Extensive experimental evaluations of the proposed
similarity measure on various real-world and synthetic data sets
demonstrate a considerably better performance than that of the
existing similarity measures, in terms of run-time and result
quality in classiﬁcation and clustering.
Keywords–Gaussian Mixture Models, Similarity Measures, Met-
ric
I.
INTRODUCTION
Information extraction systems capable of handling un-
certain data objects is an actively investigated research ﬁeld.
Many modern applications like speaker recognition systems
[1, 2], content-based image and video retrieval [3, 4], biometric
identiﬁcation and stock market analysis can be supported by
uncertain data representation. As a general class of Probability
Density Functions (PDF), Gaussian Mixture Model (GMM)
consists of a weighted sum of univariate or multivariate Gaus-
sian distributions, allowing a concise but exact representation
of uncertain data objects [5]. A good example of using objects
represented by GMMs is managing multimedia data [6]. A
90 minutes movie contains about 130,000 single images. It
requires large storage capacities as well as enormous compu-
tational effort for content-based retrieval. Storing the movie
as GMMs will dramatically reduce the resource consumption
while guaranteeing a high accuracy of the search result.
Besides the modeling of uncertainty, the efﬁciency of
similarity search on uncertain data is another important aspect.
For data objects represented by GMMs, Rougui et al. [7] have
built a bottom-up hierarchical tree based on the calculation
of the complete similarity matrix for all GMMs. However, it
is only usable for static data sets, since it has no convenient
insertion and deletion strategy, which depends on a proper
similarity measure and requires a corresponding custom-built
structure. The suitable similarity measures of GMMs for the in-
dexing trees are yet to be developed and tested. A competitive
candidate for such a similarity measure has the competencies
to guarantee high efﬁciency in its computation and to facilitate
indexing and further analysis. As we will demonstrate, our
technique proposed in this paper is highly efﬁcient because
it enables closed-form computation. Moreover, our technique
has the property of being a pseudometric, thus indexing tech-
niques like VP-tree [8] can be applied for efﬁcient search and
embedding techniques like multidimensional scaling facilitate
the subsequent analysis of the data set. To our knowledge,
several studies [9]–[14] have dealt with deﬁning similarity
measures for GMMs, but only a few of them have closed-form
expressions and none of them is a metric or pseudometric.
The main contributions of this paper are:
•
We propose a pseudometric for GMMs (PmG), which
is a similarity measure for GMMs. We derive the
closed-form expression and prove that it is a pseudo-
metric. The closed-form expression has a great advan-
tage in calculation, and the properties of pseudometric
are required by many analysis techniques.
•
We deﬁne Normalized Matching Probability (NMP),
which can be constituted to form novel similarity mea-
sures that have closed-form expressions for GMMs.
•
Experimental evaluation demonstrates both the effec-
tiveness and efﬁciency of PmG.
The rest of this paper is organized as follows: Section II
gives the basic deﬁnition of GMMs, metric and pseudometric.
Section III deﬁnes NMP and PmG, and gives the proof of the
pseudometric properties of PmG. Section IV shows the exper-
imental studies for verifying the efﬁciency and effectiveness
of the proposed similarity measure. In Section V, we survey
the previous work. Finally, Section VI summarizes the paper
and presents some ideas for further research.
II.
FORMAL DEFINITIONS
In this section, we summarize the formal notations for
GMM. GMM is a probabilistic model that represents the prob-
ability distribution of observations. The deﬁnition of GMM is
shown as follow.
Deﬁnition 1: (Gaussian Mixture Model) Let x ∈ RD be
a variable in a D-dimensional space, x = (x1, x2, ..., xD). A
Gaussian Mixture Model G is the weighted sum of m Gaussian
functions, deﬁned as:
G(x) =
X
1≤i≤m
wi · Ni(x)
(1)
where P
1≤i≤m wi = 1, ∀i ∈ [1, m], wi ≥ 0, and Gaussian
component Ni(x) is the density of a Gaussian distribution with
37
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-558-6
DBKDA 2017 : The Ninth International Conference on Advances in Databases, Knowledge, and Data Applications

a covariance matrix Σi:
Ni(x) =
1
p
(2π)D|Σi|
exp

−1
2(x − µi)T Σ−1
i (x − µi)

Specially, when Σi is a diagonal matrix, Ni(x) can be refor-
mulated as:
Ni(x) =
Y
1≤l≤D
1
q
2πσ2
i,l
exp
 
−(xl − µi,l)2
2σ2
i,l
!
where σi,l is the l-th element on the diagonal of Σi.
Most of dissimilarities are distances, and they are also
metrics if the following deﬁnition is matched.
Deﬁnition 2: (Metric)
Given an nonempty set of objects P(RD), a mapping d :
P(RD) × P(RD) −→ R+ is a metric when the following
properties always hold for any object X, Y, Z ∈ P(RD).
•
Non-negativity: d(X, Y) ≥ 0
•
Identity of indiscernibles: d(X, Y) = 0 ⇔ X = Y
•
Symmetry: d(X, Y) = d(Y, X)
•
Triangle inequality: d(X, Y) + d(Y, Z) ≥ d(X, Z)
A pseudometric is a mapping that satisﬁes the axioms for
a metric, except that instead of the identity of indiscernbles,
d(X, X) = 0 but for some distinct objects X ̸= Y, possibly
d(X, Y) = 0.
The properties of the metric, especially the triangle inequal-
ity, are essential to index structures such as M-tree and VP-
tree for efﬁcient queries, and they are fully required by some
techniques like DBSCAN. For similarity measures without the
metric properties, specialized index and analysis methods are
needed to guarantee the efﬁciency and the applicability of
certain techniques.
III.
PSEUDOMETRIC FOR GAUSSIAN MIXTURE MODELS
In this section, we extend Matching Probability (MP) into
NMP, and derive its closed-form expression for GMMs. NMP
provides a fundamental closed-form calculation for GMMs,
and it can be used to deﬁne other similarity measures for
GMMs. Speciﬁcally, we deﬁne PmG, a pseudometric for
GMMs.
A. Normalized Matching Probability
MP considers all the possible positions of true feature
vectors, and sums up the joint probabilities of two PDFs. Here
we deﬁne NMP for GMMs.
Deﬁnition 3: (Normalized Matching Probability) Given t-
wo GMMs G1(x) and G2(x) with diagonal covariance matrices
in space RD, we deﬁne the NMP ⟨G1, G2⟩ as follows:
⟨G1, G2⟩ =
Z
RD G
′
1(x) · G
′
2(x)dx
(2)
MP
  
  
NMP
Figure 1: Demonstration of MP and NMP between GMM
objects U, V in a two-dimensional space.
where G
′(x) = P
1≤i≤m
Q
1≤l≤D N(µi,l, σ2
i,l/wi). m, µ and
σ are the parameters of GMM G (see Deﬁnition 1).
Figure 1 demonstrates MP and NMP between two GMM
objects U, V. As the measures of similarity, both MP and NMP
integrate the similar parts of Gaussian components, as shown
in the top of the ﬁgure. Because of the normalization operation,
NMP gains a greater values than MP and emphasizes the
shared parts.
For the closed-form expression of ⟨G1, G2⟩, we can derive
it from the following equation.
⟨G1, G2⟩
=
m1
X
i=1
m2
X
j=1
D
Y
l=1
√w1,iw2,j
2π
q
σ2
1,i,lσ2
2,j,l
Z
e
−
(x−µ1,i,l)2
2σ2
1,i/w1,i −
(x−µ2,j,l)2
2σ2
2,j,l
dx
=
m1
X
i=1
m2
X
j=1
D
Y
l=1
e
−
(µ1,i,l−µ2,j,l)2
2(σ2
1,i,l/w1,i+σ2
2,j,l/w2,j )
q
2π(σ2
1,i,l/w1,i + σ2
2,j,l/w2,j)
=
m1
X
i=1
m2
X
j=1
D
Y
l=1
N(µ1,i,l, µ2,j,l,
σ2
1,i,l
w1,i
+
σ2
2,j,l
w2,j
)
If two GMMs are very disjoint, NMP between them is
close to zero. To obtain a higher NMP, it is required that two
GMMs have similar shapes, i.e. similar parameters.
A closed-form expression is intrinsically valuable for com-
putation. It saves extra efforts to get a good approximation
by avoiding simulation methods like Monte-Carlo, which may
cause a signiﬁcant increase in computation time and the loss of
precision. Therefore, closed-form expressions are well received
in many applications, especially in real-time applications.
Based on NMP, we can get a set of similarity measures
with closed-form expressions for GMMs. For example, we can
deﬁne a distance as follows.
d(G1, G2) = 1 −
⟨G1, G2⟩
p
⟨G1, G1⟩⟨G2, G2⟩
38
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-558-6
DBKDA 2017 : The Ninth International Conference on Advances in Databases, Knowledge, and Data Applications

There are several similarity measures based on MP have
been proposed [10, 12, 13], and we can easily extend NMP on
them.
B. Pseudometric for GMMs
On the basis of NMP, we deﬁne a pseudometric for G-
MMs, PmG. Likewise, PmG determines the square differences
between normalized GMMs, sums them up by integration and
returns the root of the integration. The deﬁnition of PmG is
shown as follows.
Deﬁnition 4: (Pseudometric for GMMs) Given two GMMs
G1(x) and G2(x) with diagonal covariance matrices in space
RD, we deﬁne the PmG of them as follows:
dPmG(G1, G2) =
p
⟨G1, G1⟩ + ⟨G2, G2⟩ − 2 · ⟨G1, G2⟩
(3)
Obviously, PmG has a closed-form expression for GMMs
with diagonal covariances. Then we give the proof that PmG
fulﬁlls three properties of a metric.
Lemma 3.1: PmG is a pseudometric.
Proof: Non-negativity:
According to the deﬁnition of NMP, for any G1, G2,
⟨G1, G1⟩ + ⟨G2, G2⟩ − 2 · ⟨G1, G2⟩ ≥ 0. Thus dPmG(G1, G2) ≥ 0.
If G1 = G2, dPmG(G1, G2) = 0.
Symmetry:
Obviously, dPmG(G1, G2) = dPmG(G2, G1).
Triangle Inequality:
Since dPmG(G1, G2) can be reformed to the ℓ2 norm of G
′
1
and G
′
2, its triangle inequality can be proved easily.
Having the property of triangle inequality, metric or pseu-
dometric can employ various of metric trees to make accesses
to the data objects more efﬁcient. Otherwise, specialized index
and analysis methods are needed to guarantee the efﬁciency
and applicability of index, which means extra efforts.
IV.
EXPERIMENTAL EVALUATION
In this section, we provide experiments on both real-world
and synthetic data sets to show the effectiveness and efﬁcien-
cy of the proposed pseudometric for GMMs. Classiﬁcation
and clustering, which are the major subdivisions of pattern
recognition techniques, as well as the run-time of similarity
calculation are used in the evaluation.
For Kullback-Leibler (KL) divergence [15] based similarity
measures, only matching based approximation (KLm) [9] is
included in this paper since it is one of the best-performing
approximations [16].
All the experiments are implemented with Java 1.7, and
executed on a regular workstation PC with 3.4 GHz dual core
CPU equipped with 32 GB RAM. For all the experiments, we
use the 10-fold cross validation and report the average results
over 100 runs.
A. Data Sets
Synthetic data and four kinds of real-world data, including
activity data, image data, audio data, and weather data, are
used in the experiments. For the data objects, GMMs are
estimated using Expectation-Maximization (EM) algorithm
(implementation provided by WEKA).
Activity data [17] is collected from 15 participants per-
forming seven activities. Assuming that participants complete
a single activity in three seconds, we regard the 150 continuous
measurements of acceleration on three axes as one data object.
Thus 1083 objects are generated for participant 1.
Image data [18] is a collection of images taking under vari-
ous viewpoints. In this paper, we use the gray images recording
100 objects from 72 viewpoints. Every image (192×144) is
smoothed by a Gaussian ﬁlter with a standard deviation of
ﬁve pixels.
Audio data [19] consists of speech from ten speakers, the
names of who are shown as follows: Aaron, Abdul Moiz,
Afshad, Afzal, Akahansson, Alexander Drachmann, Afred
Strauss, Andy, Anna Karpelevich and Anniepoo. Every wav
ﬁle is split into ten fragments, transformed into frequency
domain by Fast Fourier Transform.
Weather data [20] is the historical weather data of 908
airports in Europe from 2005 to 2014. The features of Weather
data are temperature and humidity, and only the average values
of each day are used.
The synthetic data sets [21] are generated by randomly
choosing mean values between 0 and 100 and standard de-
viations between 0 and 5 for each Gaussian component. The
weights are randomly assigned, and they sum up to one within
each GMM. Since there is no intuitive way to assign class
labels for GMMs in advance, here we use the synthetic data
sets only for the run-time evaluation.
B. Effectiveness Evaluation
In the evaluation of classiﬁcation, we employ the simplest
and widely used algorithm k-Nearest Neighbors (k-NN), rather
than the other more complex techniques, to compare the
effectiveness of the similarity measures, since we are not
interested in tuning classiﬁcation accuracy to its optimum.
Varying k in k-NN and the number of Gaussian compo-
nents in each GMM, we start with experiments on Activity
data. As shown in Figure 2, PmG has a better performance
than the other similarity measures for different k. With the
increase number of Gaussian components in each GMM, the
classiﬁcation accuracies of PmG sightly increase, and generally
outperform the other measures. For all the four real-world data
sets, we ﬁx the numbers of Gaussian components and report
the results of 1-NN classiﬁcation accuracies in Table I, where
the highest accuracies of each column are marked in bold and
with •, while the second highest ones are just marked in bold.
We can see that PmG achieves the highest or second highest
accuracies among all the similarity measures.
To compare the usability of the proposed similarity mea-
sure for unsupervised data mining, we perform clustering
experiments on all four real-world data sets. Instead of k-
means algorithm, the k-medoids is used here since it works
39
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-558-6
DBKDA 2017 : The Ninth International Conference on Advances in Databases, Knowledge, and Data Applications

1
2
3
4
5
6
7
8
9
10
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0.88
0.9
0.92
Accuracy
k in k−NN
(a)
5
6
7
8
9
10
11
12
0.8
0.82
0.84
0.86
0.88
0.9
0.92
Accuracy
Number of Gaussian components
(b)
 
 
PmG
ES
C2
L2
KLm
SQFD
EMD
Netﬂow
Figure 2: Classiﬁcation results of Activity data. The numbers of Gaussian components in GMMs generated from data objects in
(a) are ﬁxed as ten. In (b), k in k-NN is ﬁxed as three.
TABLE II. CLUSTERING RESULTS OF k-MEDOIDS.
Activity (k=7)
Image (k=100)
Audio (k=10)
Weather (k=14)
Purity
NMI
FM
Purity
NMI
FM
Purity
NMI
FM
Purity
NMI
FM
PmG
.82±.03
.49±.04
.56±.07
.43±.00
.69±.00
.30±.01
.56±.04
.45±.03
.40±.03
.58±.01
.23±.01
.22±.03
ES
.73±.05
.37±.03
.39±.03
.40±.01
.67±.00
.28±.01
.49±.04
.37±.02
.33±.03
.55±.02
.18±.02
.21±.03
C2
.77±.06
.39±.07
.45±.07
.40±.00
.67±.00
.27±.00
.46±.05
.34±.03
.32±.03
.53±.02
.16±.02
.22±.04
L2
.77±.03
.37±.04
.44±.06
.40±.01
.66±.00
.27±.01
.48±.04
.36±.03
.33±.02
.54±.02
.17±.01
.20±.02
KLm
.74±.03
.41±.03
.50±.05
.40±.00
.66±.00
.27±.00
.46±.05
.38±.03
.33±.04
.47±.01
.15±.01
.26±.03
SQFD
.72±.04
.35±.03
.39±.04
.40±.01
.66±.00
.27±.00
.51±.04
.38±.03
.35±.03
.54±.01
.17±.01
.21±.02
EMD
.79±.04
.45±.05
.48±.07
.33±.01
.58±.00
.21±.01
.47±.03
.40±.03
.34±.03
.57±.01
.21±.01
.23±.03
Netﬂow
.74±.07
.36±.08
.44±.05
.39±.00
.66±.00
.27±.00
.49±.03
.45±.04
.38±.04
.56±.01
.20±.01
.19±.02
TABLE I. 1-NN CLASSIFICATION RESULTS OF REAL-WORLD DATA.
Activity
Image
Audio
Weather
(m = 10)
(m = 5)
(m = 5)
(m = 10)
PmG
.865±.030 •
.852±.010 •
.851±.044
.761±.032 •
ES
.834±.032
.827±.010
.804±.035
.740±.030
C2
.859±.032
.827±.010
.803±.039
.730±.033
L2
.853±.032
.826±.010
.802±.039
.737±.028
KLm
.793±.039
.823±.010
.825±.037
.717±.030
SQFD
.843±.030
.825±.014
.808±.028
.724±.035
EMD
.848±.033
.519±.014
.809±.032
.758±.047
Netﬂow
.838±.030
.813±.010
.857±.032 •
.757±.051
with arbitrary similarity measure, making it more suitable in
our situation. We evaluate the clustering results using three
widely used criteria, Purity, Normalized Mutual Information
(NMI) and F1 Measure (FM).
Table II illustrates the evaluation of clustering results when
using different similarity measures on four real-world data sets.
PmG achieves the best performance among all the measures
on all three criteria, except for FM on Weather data.
C. Efﬁciency Evaluation
Every similarity measure evaluated in this paper has a
time complexity of O(m1 · m2 · D), where m1 and m2 are
the numbers of Gaussian components in GMMs that are used
for similarity calculating, and D is the dimensionality of data
space. To support the theoretical time complexity, we provide
comparisons by scaling m and D on synthetic data sets.
We calculate distance matrices for all the synthetic data
sets, and as mentioned before, the average time cost of 100
runs are reported. Figure 3 shows the comparison of run-time
between all the similarity measures on synthetic data sets with
different numbers of Gaussian components in each GMM and
different data dimensionality. The run-time of all similarity
measures has a quadratic relation with the component number
and a linear dependence with data dimensionality. PmG has a
similar performance with ES, C2 and L2. With the increase of
components number, EMD and Netﬂow gain more than PmG
in time-cost. For varying dimensionality, the tendencies of all
the similarity measures are very similar.
Comparing the query efﬁciency of linear scan and metric
tree, we illustrate the time-cost of queries in Figure 4. Figure
4(a) demonstrates the linear relation between the query time
and the number of data objects in linear scan queries. When
40
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-558-6
DBKDA 2017 : The Ninth International Conference on Advances in Databases, Knowledge, and Data Applications

(a) Varying the number of Gaussian components.
(b) Varying data dimensionality.
Figure 3: Time cost of linear scan queries on synthetic data sets. In (a), the data dimensionality is ﬁxed as two. The numbers of
Gaussian components are set as ten in (b).
(a) Time cost of linear scan queries.
(b) Time cost of VP-tree queries.
Figure 4: Time cost of queries when varying the number of data objects. Each GMM used here has ten Gaussian components
in a two-dimensional space. The capacity of nodes in the VP-tree is set to 32.
using VP-tree, as shown in Figure 4(b), there are great im-
provements in query efﬁciency for all the similarity measures.
However, only PmG, EMD and Netﬂow can guarantee query
accuracies among them.
V.
RELATED WORK
This section gives a survey and discussion of similarity
measures for GMMs in previous work. Firstly we summarize
approximation approaches for GMMs, then we discuss simi-
larity measures that have closed-form expressions.
The Kullback-Leibler divergence [15] is a common way
to measure the distance between two PDFs. It is given by
dKL(f1∥f2) =
R
f1(x)log f1(x)
f2(x)dx. For the properties of metric,
the KL divergence only satisﬁes the non-negativity property,
although its symmetric version (| 1
2dKL(f1∥f2)+ 1
2dKL(f2∥f1)|)
also satisﬁes the symmetry property. Moreover, it has a closed-
form expression for Gaussian distributions, but no such expres-
sion for GMMs exists.
To compute the distance between GMMs by the KL di-
vergence, several approximation methods have been proposed.
A commonly used approximation to dKL(f1∥f2), the Gaussian
approximation, replaces f1 and f2 with two Gaussian distribu-
tions, whose means and covariance matrices depend on those
of GMMs. Another popular way is to use the minimum KL
divergence of Gaussian components that are included in two G-
MMs. Moreover, Hershey et al. [11] have proposed the product
of Gaussian approximation and the variation approximation,
but the former tends to greatly underestimate dKL(f1∥f2) while
the latter does not satisfy the positivity property. Besides,
Goldberger et al. [9] have proposed KLm and the unscented
transformation based KL divergence(KLt). KLm works well
when the Gaussian elements are far apart, but it cannot handle
the overlapping situations, which are very common in real-
world data sets. KLt solves the overlapping problem based
on a non-linear transformation. Cui et al. [16] have compared
the six approximation methods for KL divergence with Monte
Carlo sampling, where the variation approximation achieves
the best result quality, while KLm give a comparable result
with a much faster speed.
Besides the approximation similarity methods for GMMs,
several methods with closed-form expression have been pro-
posed. Hel´en et al. [10] have described a squared Euclidean
distance, which integrates the squared differences over the
whole feature space. It has a closed-form expression for
GMMs. Sﬁkas et al. [13] have presented a KL divergence
based distance C2 and a Bhattacharyya-based distance for
GMMs. Jensen et al. [12] used a normalized L2 distance to
41
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-558-6
DBKDA 2017 : The Ninth International Conference on Advances in Databases, Knowledge, and Data Applications

measure the similarity of GMMs in mel-frequency cepstral
coefﬁcients from songs. Beecks et al. [22] have proposed
SQFD for GMMs to model the similarities between images.
However, none of these similarity measures with closed-form
expression for GMMs obeys the triangle inequality.
VI.
DISCUSSION AND CONCLUSIONS
In this paper, we have proposed PmG for GMMs. As a
metric, PmG enables storing GMMs in any metric tree and
applying analysis techniques that require the properties of
triangle inequality. In our experimental evaluations on real-
world data sets, we have demonstrated the effectiveness of
the proposed similarity measure. PmG outperform the other
measures on different types of data sets in both classiﬁcation
and clustering.
Due to the potentially different number of Gaussian com-
ponents in GMMs, there is still not much specialized indexing
structure for GMMs exist. Using matching probability as the
similarity measure, B¨ohm et al. [23] and Zhou et al. [24] have
decomposed each GMM into its components to support the
indexing of GMMs. A specialized dynamic index for GMMs
using a metric or pseudometric is a promising perspective.
ACKNOWLEDGMENT
We thank Thomas Abeel for sharing the code implemen-
tations of k-medoids algorithm, Damien Di Fede for sharing
his implementation of Fast Fourier Transform, Mzechner for
his/her implementation of audio ﬁle processing.
REFERENCES
[1]
W. M. Campbell, D. E. Sturim, and D. A. Reynolds, “Support vector
machines using GMM supervectors for speaker veriﬁcation,” IEEE
Signal Process. Lett., vol. 13, no. 5, 2006, pp. 308–311.
[2]
D. A. Reynolds, T. F. Quatieri, and R. B. Dunn, “Speaker veriﬁcation
using adapted gaussian mixture models,” Digital Signal Processing,
vol. 10, no. 1-3, 2000, pp. 19–41.
[3]
P. KaewTraKulPong and R. Bowden, “An improved adaptive back-
ground mixture model for real-time tracking with shadow detection,”
in Video-based surveillance systems.
Springer, 2002, pp. 135–144.
[4]
Z. Zivkovic, “Improved Adaptive Gaussian Mixture Model for Back-
ground Subtraction,” in ICPR, 2004, pp. 28–31.
[5]
D. Reynolds, “Gaussian mixture models,” Encyclopedia of Biometrics,
2015, pp. 827–832.
[6]
S. Ou, C. Lee, V. S. Somayazulu, Y. Chen, and S. Chien, “Low
complexity on-line video summarization with gaussian mixture model
based clustering,” in ICASSP, 2014, pp. 1260–1264.
[7]
J. E. Rougui, M. Gelgon, D. Aboutajdine, N. Mouaddib, and M. Rziza,
“Organizing Gaussian mixture models into a tree for scaling up speaker
retrieval,” Pattern Recognition Letters, vol. 28, no. 11, 2007, pp. 1314–
1319.
[8]
P. N. Yianilos, “Data structures and algorithms for nearest neighbor
search in general metric spaces,” in ACM/SIGACT-SIAM SODA, 1993,
pp. 311–321.
[9]
J. Goldberger, S. Gordon, and H. Greenspan, “An efﬁcient image
similarity measure based on approximations of kl-divergence between
two gaussian mixtures,” in ICCV, 2003, pp. 487–493.
[10]
M. L. Hel´en and T. Virtanen, “Query by example of audio signals using
euclidean distance between gaussian mixture models,” in ICASSP (1),
2007, pp. 225–228.
[11]
J. R. Hershey and P. A. Olsen, “Approximating the kullback leibler
divergence between gaussian mixture models,” in ICASSP, 2007, pp.
317–320.
[12]
J. H. Jensen, D. P. W. Ellis, M. G. Christensen, and S. H. Jensen,
“Evaluation of distance measures between gaussian mixture models of
mfccs,” in ISMIR, 2007, pp. 107–108.
[13]
G. Sﬁkas, C. Constantinopoulos, A. Likas, and N. P. Galatsanos, “An
analytic distance metric for gaussian mixture models with application
in image retrieval,” in ICANN (2), 2005, pp. 835–840.
[14]
S. Zeng, R. Huang, H. Wang, and Z. Kang, “Image retrieval using
spatiograms of colors quantized by gaussian mixture models,” Neuro-
computing, vol. 171, 2016, pp. 673–684.
[15]
S. Kullback, Information theory and statistics.
Courier Dover Publi-
cations, 2012.
[16]
S. Cui and M. Datcu, “Comparison of kullback-leibler divergence
approximation methods between gaussian mixture models for satellite
image retrieval,” in IGARSS, 2015, pp. 3719–3722.
[17]
“Uci archive: Activity recognition data,” http://archive.ics.uci.edu/ml/
machine-learning-databases/00287/, accessed: 2017-03-27.
[18]
“Aloi image data,” http://aloi.science.uva.nl/, accessed: 2017-03-27.
[19]
“Speaker
recognition
data,”
http://www.repository.voxforge1.org/
downloads/SpeechCorpus/Trunk/Audio/Main/16kHz 16bit/,
accessed:
2017-03-27.
[20]
“Weather
of
airports
data,”
https://drive.google.com/open?id=
0B3LRCuPdnX1BZ0d4RXIxMDVzakE, accessed: 2017-03-27.
[21]
“Synthetic
data
sets,”
https://drive.google.com/open?id=
0B3LRCuPdnX1BMzlHaUJYSFlpU1U, accessed: 2017-03-27.
[22]
C. Beecks, A. M. Ivanescu, S. Kirchhoff, and T. Seidl, “Modeling image
similarity by gaussian mixture models and the signature quadratic form
distance,” in ICCV, 2011, pp. 1754–1761.
[23]
C. B¨ohm, P. Kunath, A. Pryakhin, and M. Schubert, “Querying objects
modeled by arbitrary probability distributions,” in SSTD, 2007, pp. 294–
311.
[24]
L. Zhou, B. Wackersreuther, F. Fiedler, C. Plant, and C. B¨ohm,
“Gaussian component based index for GMMs,” in ICDM, 2016, pp.
1365–1370.
42
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-558-6
DBKDA 2017 : The Ninth International Conference on Advances in Databases, Knowledge, and Data Applications

