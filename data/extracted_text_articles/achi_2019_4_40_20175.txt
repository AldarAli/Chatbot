Empirical Heatmap Decomposition — A Fresh
Look on Gaze Behavior
Saad Al-Baddai
Chair of Information Science, and CIML Group
Regensburg University
Regensburg, Germany
Email: saad.al-baddai@ur.de
Karema Al-Subari
and Elmar Lang
Institute of Biophysics
Regensburg University
Regensburg, Germany
Barbara Ströhl
and Bernd Ludwig
Chair of Information Science
Regensburg University
Regensburg, Germany
Abstract—The motivation for analyzing a person’s gaze be-
havior stems from the eye-mind-hypothesis that correlates
the observation of content by viewing it with its cogni-
tive comprehension. A standard approach to quantitative
analysis of gaze behavior is to analyze so called heatmaps
generated by an eye tracker. This technical process is
subject to noise. In order to compensate it, state-of-the-
art eye tracking software smooths raw data using linear
ﬁlters. In this paper, we present an alternative method
based on interpolation. We provide empirical data that
our method reproduces the actual gaze behavior more
precisely. Furthermore, we introduce Empirical Heatmap
Decomposition (EHD) to cluster eye movements into classes
of similar frequency and amplitude. For evaluation, we
present an analysis of gaze data that illustrates how EHD
can uncover details in the observed gaze behavior that
state-of-the-art heatmaps do not visualize.
Keywords–Empirical Mode Decomposition; Eye track-
ing; Gaze Behavior; Sifting process
I.
INTRODUCTION
Understanding the gaze behavior of users while they
read multimodal documents containing text, as well
as images, graphics, or sketches is of vital interest
for anybody engaged in designing digital content, e.g.,
of web pages, electronic product catalogs, or search
engine result pages. This understanding is of particular
importance if the digital content is to be generated
automatically and in real-time as necessary for, e.g.,
interactive web pages, digital guides for sightseeing or
museums. For these applications it is crucial to ensure
that all content is perceived by the users immediately
and provides positive user experience and joy of use.
A. Fixations and Saccades
For empirical investigations of the user behavior in
situations as sketched above, eye tracking is a state-
of-the-art method for monitoring the gaze of users. The
reading and comprehension of text has been investigated
using eye tracking, e.g., by [1], [2]. The analysis of
the gaze data allows to reconstruct in which way, in
which order, with which velocity, and in which regions
of the digital content users view the presented material.
From these observations, it can be concluded how users
process the observations cognitively. As reported by
[3], eyes could be attracted by some part of a visual
scene and ﬁxate these parts — named Areas of Focus
(AoFs) — for a longer period. The process of iden-
tifying AoFs, based on the duration of eye ﬁxations,
can help in advanced analyses of gaze behavior [4],
among others in the analysis of so called distractors:
Assuming that regular patterns for reading text do exist,
how do images or even animated material (e.g., videos
on a web page) inﬂuence and — in particular — disturb
the comprehension of the presented text? This question
was investigated — among others — by [5]. The authors
report empirical results that pictures, and in particular
those that are unrelated to the topic of the text, distract
and slow down the standard reading behavior. A similar
observation has been made by [6]. Unrelated material
when included in the presented digital content provides
negative user experience and leads to reading patterns
that are signiﬁcantly different from the standard ones.
The authors even use this observation to construct an
algorithm that predicts whether a user struggles with
the displayed content due to the presence of distractors.
While these results are highly relevant for the pur-
poses mentioned before, it has to be noted that most
analyses of gaze are based on heatmaps for ﬁxations
and plots for saccades that visualize all movements at
once. However, eye movements differ
• in their speed and
• in the ﬁxation duration
between subsequent movements [1], [2], [6]. However,
only few researchers develop mathematical models for
a quantitative analysis of such types of gaze (e.g., [7]).
This fact is quite astonishing as in other research
areas (e.g., in medical image processing), it is common
practice to decompose raw data using transformations
(e.g., Fourier or Wavelet) in order to identify different
causes for observations in a data set. More recently, a
more more data driven approach called Empirical Mode
25
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

Decomposition (EMD) was pioneered by [8]. Based on
an interpolation approach, EMD proved to reveal the
characteristics of the textures in raw data sets more
transparently and intuitively. As an elaboration of EMD,
[9] applied Green’s functions [10] for the interpolation
problem (GiT-BEMD) in order to avoid artifacts and
the immense computation load in the decomposition
process that presented a major problem to the original
version of EMD. GiT-BEMD works in real time and
can therefore be used in real-time applications.
B. Gaze Decomposition
GiT-BEMD (as will be explained later) is interesting
for gaze analysis as it provides an "all-in-one solution"
for processing gaze data: While the standard EMD
applies interpolation for separating sources in the signal,
GiT-BEMD provides a substitute for linear ﬁltering of
raw gaze data. State-of-the-art commercial eye tracking
software such as SMI’s BeGaze applies Gaussian ﬁlters
to reduce noise (see BeGaze Manual V 3.4, 2014, p.
204). However, this approach is problematic as the
Gaussian kernel may blur away interesting data points.
Recently, as an alternative, other ﬁlters, e.g., Savitzky-
Golay ﬁlter [11], have been employed in eye-tracking
area [12]–[14]. They are based on regression while GiT-
BEMD interpolates data points and in this way tends to
preserve original data as perfectly as possible.
C. Contribution
Even if we will provide data later that GiT-BEMD
outperforms ﬁlters used in state-of-the-art software,
adding the interpolation approach to smoothing gaze
data is a side contribution of the present paper. Our
main focus is on decomposing the smoothed signal. To
the best of our knowledge, this is the ﬁrst paper that
investigates the value of decomposition methods to the
analysis of gaze data. The algorithmic solution we de-
veloped is intended to be part of an open source tool box
for analyzing eye tracking data. Similar other tool boxes
have been implemented so far: eSeeTrack that focuses
on the analysis of patterns of sequential gaze recordings
[15], or imap [16] [17] which analyzes and compares
eye movements under different conditions, iComp [18],
ILAB [19], or GazeAlyze [20]. None of them however
decomposes eye tracking data and therefore GiT-BEMD
is an innovative extension to existing toolboxes.
To present our results, in Section II we introduce
our empirical setup. Then we introduce the GiT-BEMD
method in detail. In Section III we apply GT-BEMD on
gaze data and provide ﬁrst empirical results highlighting
the performance of the new approach in Section IV.
In Section V, we draw conclusions of the approach’s
impact on theoretical and practical issues of eye tracking
for gaze and viewing analysis.
Figure 1. Left: Actual eye movement of participants during reading
a Wikipedia page (scan paths observed by the eye tracker). Right:
Fixations at the start and end point of a scan path (circle radius
corresponds to the average ﬁxation time)
II.
SCENARIO
As [5], [6], we were interested in the effect of
images on the reading behavior. An example of the
digital content we investigated is the Wikipedia page
in Figure 1. In particular, we wanted to know whether
images that illustrate the textual information support the
memorability of the digital content. According to the
eye-mind-hypothesis [21], the gaze duration correlates
with the cognitive processing time and depth of the
perceived content. So it was surprising that participants
mentioned details of an image on the Web page even if
in the BeGaze heatmap no ﬁxations on the image were
registered (see Figure 4 left for an example).
In order to deeper analyze the collected gaze data
and understand whether the described phenomenon
was an artefact of the BeGaze’s method to generate
heatmaps, we applied four different methods on the
same data set:
• The ﬁrst method is the same as used by SMI’s
BeGaze analysis tool: the raw data is smoothed
with a Gaussian ﬁlter.
26
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

• The second method is based on EMD: after Gaus-
sian ﬁltering, the data is decomposed using EMD
(see [9]). It produces three heatmaps. Each of
them groups ﬁxations that are similar in terms of
frequency and amplitude.
• The third method is GiT-BEMD without decompo-
sition (and therefore analogous to the ﬁrst method).
• The fourth method is GiT-BEMD including decom-
position (analogous to the second method).
Our hypothesis is that EMD and GiT-BEMD reveal
the gaze behavior more precisely than pure ﬁltering.
In order to measure the effect of the decomposition, we
compute those areas of neighboring pixels in which all
pixels have an average ﬁxation time higher than to be
expected under the assumption of a uniform probability
distribution for ﬁxations over the whole digital content.
This is an unsupervised way to identify AoFs (area of
focus). Using this notion, our hypothesis can be stated
quantitatively. The decomposition based methods are
more precise than pure ﬁltering if they
• detect signiﬁcantly more AoFs and
• in each detected AoF, identify more pixels ﬁxated
longer than chance (i.e., with probability higher
than 1/pixels in image)
than the pure ﬁltering methods. Before presenting in
Section IV the results using the unsupervised metric, in
the next section we introduce the mathematical founda-
tions of our approach and the empirical data we used to
analyze the performance of GiT-BEMD to detect AoFs.
III.
METHOD
We collected gaze data using an SMI RED 250mo-
bile eye tracker at 250 Hz. In a controlled lab ex-
periment, BA students of an introductory course to
information science were asked to read a Wikipedia
page presented to them for two minutes. They knew in
advance that they had to answer fact retrieval questions
about the page’s content afterwards. We randomly chose
10 data sets from the described experiment to test the
GiT-BEMD method.
A. Smoothing Heatmaps
The raw data produced by an eyetracker contains a
list of subsequently ﬁxated pixels during recording eye
movements. Saccades can be calculated as difference
vectors of subsequently ﬁxated pixels. Due to noise
caused by technical constraints of the eyetracker, for
each ﬁxated pixel in the raw data there is a certain
chance that the observed ﬁxation is a artifact. To ac-
count for this issue, raw data is smoothed. As already
discussed above, the standard way to smoothing in
commercial eyetracking software is Gaussian ﬁltering
that essentially is a linear operation on data windows
to compute a weighted average from all pixels in the
window. A major disadvantage of averaging is that
isolated peaks in a window are blurred away. While
this effect is even welcome in image processing, in the
analysis of gaze data important ﬁxation are eventually
discarded leading to an erroneous reconstruction of the
actual gaze behavior.
As an alternative, GiT-BEMD instead of averaging
data locally, interpolates data globally. In this way,
smoothing is achieved by spline interpolation. Smooth-
ing then basically aims at ﬁnding the smoothest enve-
lope surface passing through a grid of irregularly spaced
extrema (i.e., the ﬁxation count or duration of pixels
from the raw data; see [9], [22], among others). The
boundary value problem for a spline that interpolates
ﬁxated pixels can be stated with appropriate conditions
for the spline’s derivatives [23]. The resulting system
of equations can then be solved using the family of
Green’s functions [10]. An envelope surface that fulﬁlls
all stated conditions can be expressed as
s(x) =
N
X
n=1
wnΦ(x, xn)
(1)
In this formula, xu denotes any point where the surface
is unknown, xn represents the n-th recorded ﬁxation,
Φ(x, xn) is the Green’s function and wn is the respec-
tive weight in the envelope representation. Calculating
s(x) for all pixels ﬁnally generates a smoothed heatmap.
An envelope surface is constructed in two steps: the
ﬁrst step estimates the weights w = [w1
The surface values [s(x1), . . . , s(xN)]T w2 . . . wP ]:
≡
c
=
[c1, c2, . . . , cN]T are known in a total of N pixels xn.
Employing (1) for each of the known points xn, a linear
system of N equations is obtained:
Gw = c
where n-th row of matrix G is the evaluation of the
Green function Φ(xn, xm), m = 1, 2 . . . N. We solve
the equation for the weights w = G−1c.
Corresponding slopes sm in directions ˆnm can be
obtained by evaluating the relations
sm =
N
X
m=1
wm∇Φ(xm − xn) · ˆnm
m = 1, . . . , N.
The second step estimates the interpolating envelope
surface: Using the weights w, the value s(xu) ≡ cu
of the envelope surface can be estimated at any point
xu by solving (1), which can be re-written as
cu = wT Φ.
(2)
The vector Φ = [Φ(x, x1) Φ(x, x2) . . . Φ(x, xN)]T
contains the Green’s function values of all distances
between the N data constraints and the considered
location.
GiT-BEMD implements the concept of smoothing
27
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

0
5
10
15
20
25
30
35
40
45
50
-0.2
0
0.2
0.4
0.6
0.8
1
Gaussian Kernel
Sifting by 2D S-filter
Raw data
Figure 2. The effect of Gaussian smoothing and the S-ﬁlter in 1D.
The y axis shows normalised ﬁxation durations.
described above with one extension: As eyetracking data
normally is sparse (i.e., for many pixels there is no ﬁx-
ation at all), adding white noise is required to decrease
the computational load of building an envelope surface
by interpolating from the extracted extrema. Without
added noise, zero crossings would be considered as local
minima and maxima that actually do not exist. After a
surface has been constructed by applying the described
concept, the artiﬁcially added noise can be ignored and
does not disturb the further analysis.
Figure 2 illustrates the approach and shows a ﬁrst
comparison to Gaussian smoothing. It is easy to see that
the GiT-BEMD aims to actually interpolate the raw data
points while the Gaussian smoothing calculates sort of
a "compromise". This is true for regions where a few
pixels have many ﬁxations as in the case of the leftmost
extremum. Such situations are typical for eyetracking
data and therefore we conclude that GiT-BEMD has the
potential to outperform Gaussian smoothing.
For the two dimensional case, the same effect is
illustrated in Figure 3. In the top left, one can see the
(sparse) map of actual ﬁxations while in the top right for
some examples the ﬁxation durations are plotted. Some
extrema are given explicitly. The map in the bottom
left illustrates how Gaussian smoothing inﬂuences the
relevance of these extrema while on the right the map
computed by GiT-BEMD is displayed. It is obvious
that Gaussian smoothing tends to build large prominent
regions (as the 70.2 ms) when the neighbors of a pixel
show similarly high ﬁxation durations. On the other
hand, isolated pixels such as the 59.6 ms are "smoothed
away" and considered as noise. GiT-BEMD however
preserves large, but also detects small regions.
B. Decomposition of Heatmaps
Gaze data is produced by eye movements of dif-
ferent velocity and ﬁxations (i.e., eye movements with
velocity v = 0) of different durations. Therefore, it is
reasonable to separate the gaze data into several compo-
nents for movements of similar velocity (i.e., frequency)
and analyze the durations (i.e., amplitudes). Such sepa-
rations are common and well-known algorithms for their
Figure 3. Effects of Gaussian smoothing and the GiT-BEMD. Top
left: map of actual ﬁxations. Top right: some ﬁxation durations.
Bottom left: Gaussian smoothing. Bottom right: GiT-BEMD
computation are Fourier or Wavelet transforms. How-
ever, as with Gaussian smoothing, Fourier or Wavelet
transforms are classical approaches, but not suited op-
timally for eyetracking data. Instead, the GiT-BEMD
decomposes data by constructing a series of surface
envelopes for a given data set. In essence, EHD it-
eratively applies the GiT-BEMD smoothing explained
above on the raw data without adding any artiﬁcial
noise. Once a surface function has been constructed, it
gets subtracted from the initial data, and the procedure
is repeated on the pixelwise difference. In this way,
EHD can decompose a heatmap H(m, n) into several
component heatmaps. This kind of decomposition can
be computed effectively by using a so called sifting
process [8], [24]–[27] as follows:
r−1(m, n) := H(m, n);
k := 0;
while rk−1(m, n) ̸= 0 or rk−1(m, n) is not monotone
do
i := 0;
Ik,i(m, n) := rk−1(m, n);
while Ik,i(x) has non-negligible local mean do
U(m, n) is a cubic spline through all local
maxima of Ik,i(m, n);
L(m, n) is a cubic spline through all local
minima of Ik,i(m, n);
meank,i(m, n) := 1
2 (U(m, n) + L(m, n));
Ik,i(m, n) := Ik,i(m, n) − meank,i(m, n);
i := i + 1;
end
IMFk(m, n) := Ik,i(m, n);
rk(m, n) := rk−1(m, n) − IMFk(m, n);
k := k + 1;
end
28
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

Figure 4. Heatmap (Gaussian smoothing) Components 1, 2, and 3 extracted by EHD. Boundary lines surround detected AoFs.
Figure 5. Heatmap (GiT-BEMD). Components 1, 2, and 3 extracted by EHD. Boundary lines surround detected AoFs.
The result of this process is the following decom-
position of the original heat map H(m, n) into the
approximative decomposed heatmap ˆH(m, n):
ˆH(m, n) =
k
X
j=1
IMFj(m, n)
(3)
Note that ˆH(m, n) depends on the parameters and stop
criteria. For any ˆH(m, n), in each extracted component
IMFj(m, n) different AoFs can be detected.
IV.
EVALUATION
In this section, we will present an example which
can illustrate the efﬁciency of the approach we are
advocating in this paper. In our Wikipedia experiment,
we are interested in analyzing the reading behavior. In
particular, we want to know whether test persons ﬁxate
the image that is a distractor for reading the text. This
issue has been investigated already in previous work [5],
[6], as outlined in the introduction.
A. Baseline: State-of-the-Art Eyetracking Software
In Figure 1, one can observe that there are ﬁxations
on the statue of liberty, but not many. We use this data
as ground truth in our evaluation. We ﬁrst normalize
all ﬁxation durations to the interval [0; 1]. While the
average duration on the whole Wikipedia page is 0.55,
the highest duration in the area of the statue of liberty
is 0.1869 and the ﬁxated pixels are quite isolated in
comparison to the pixels in the text area. However,
as we know from the post-test-questionnaires, all test
persons remembered the statue on the page although
they could not know about the image as the page was
designed just for the experiment imitating a Wikipedia
page. Therefore, according to the eye-mind hypothesis,
the test persons must have seen the image. This is in line
with the ﬁxations in the raw data, but contradicted by the
results of the BeGaze ﬁltering. It becomes obvious that
state-of-the-art heatmaps eventually provoke researchers
to draw wrong conclusions.
B. Smoothing by Interpolating raw data
However, as can be seen in Figure 4 on the left,
after Gaussian smoothing there is no AoF detected in
the area of the statue as the highest ﬁxation duration
is lower than elsewhere and the surrounding pixels do
not contribute ﬁxations. So, the information is regarded
as noise and discarded even if the area must have
been ﬁxated. GiT-BEMD’s output is shown in Figure
5. Here, AoFs are detected (see the areas enclosed by a
green line). In order to analyze this intuitive observation
systematically, for each of the ten test persons, we
computed the number of pixels ﬁxated longer than by
chance (i.e., their ﬁxation duration is 0.5 or more) and
how many of them had still a duration of 0.5 or more
after ﬁltering. In an ideal heatmap, the difference be-
tween both numbers should be 0. However, for Gaussian
29
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

smoothing the difference is 19.7 while for GiT-BEMD
it is 6.9 on average over all 10 trails. The difference
between both averages is signiﬁcant (p < 0.01): Even
without applying EHD, GiT-BEMD better preserves
ﬁxation details while still reducing noise in the raw data.
C. Decomposition of Heatmaps
The beneﬁt of the new EHD can be understood
when considering the three components displayed in
the middle of Figure 4. They are ordered from high
to low velocity of the eye movements. Component 2 in
the middle already highlights relevant ﬁxations on the
statue’s head. However, they are not signiﬁcant as other
areas in this component (see the bounding lines around
the named AoF). Component 3 then reveals a signiﬁcant
AoF on the head when only slow eye movements are
considered. We conclude that EHD allows us to identify
different for of gaze behavior caused by the nature of
the observed object: Recognizing text requires many
fast movements while identifying a known object by
retrieving it from memory can only be achieved by slow
movements scanning an AoF in detail.
EHD provides an even more exact analysis (see
Figure 5): signiﬁcant AoFs are found in all three com-
ponents. The new approach is obviously more reliable
in detecting as many actual ﬁxations as possible. This
claim can be backed by statistics: in component 1
(fast eye movements) the difference between actual
and detected ﬁxations is −1.7 for Gaussian smoothing
and −30.0 for GiT-BEMD (p < 0.001). Independently
of the ﬁlter used, EHD detects more ﬁxations than
could be expected assuming the same chance for each
pixel. Note however, that in this component the ﬁxation
duration is very low. Consequently, pixels can be ﬁxated
longer than by chance relatively easily. This component
therefore gives an overview of the parts of the page
the test persons ﬁxated while scanning the complete
digital content superﬁcially. For component 2 (average
velocity of eye movement) the differences are 12.7
and −13.8. Gaussian smoothing again detects fewer
ﬁxations (p < 0.001) and now already misses ﬁxations
that are relevant actual ﬁxations. Finally for component
3 the difference for Gaussian smoothing is 16.2, while
for GiT-BEMD it is 5.9. Again, Gaussian smoothing
misses more ﬁxations, however in this component the
difference is no longer signiﬁcant (p = 0.07687). In
this component, slow movements in particular during
reading the text are observed. Saccades are short and
therefore the Gaussian kernel removes fewer ﬁxations
as noisy compared to both other components. The
comparison of the trend of both approaches reveals no
signiﬁcant differences (25.7 vs. 24.3 with p = 0.7872).
Tab. I summarizes the comparison of both ap-
proaches. The row Original provides evidence that the
third method outperforms the ﬁrst one as hypothesized
earlier in Section II. The columns Gaussian and GiT
TABLE I. NUMBER OF DETECTED AoF PER COMPONENT
Heatmap
p-value
Gaussian
GiT
Original
p < 0.001
7.70
20.50
∗ ∗ ∗
Component 1
p < 0.001
29.10
57.40
∗ ∗ ∗
Component 2
p < 0.001
14.7
41.2
∗ ∗ ∗
Component 3
p < 0.01
11.20
21.50
∗ ∗
Trend
p > 0.05
1.70
3.10
indicate that the fourth method is superior to the second.
V.
DISCUSSION
In this paper, we presented EHD as an approach
to decompose gaze heatmaps according to the velocity
of eye movements. We showed that the approach can
be implemented effectively and even works in real-
time — an interesting fact for pervasive computing.
We validated the approach by analyzing gaze data from
an experiment in which users had to read a simulated
Wikipedia page and after the were tested which objects
on the page they could remember. For a quantitative
evaluation, we applied an unsupervised approach to
detect AoFs. We chose this metric, as it does not require
an expert to label AoFs. In our view, in this way we
could avoid biases stemming from the expert’s valuation
of the digital content. Assuming instead that each pixel
has the same chance to attract the focus of test persons,
therefore enables a fairer evaluation of different methods
to analyze gaze data. In future work, we will measure
the effects of EHD by comparing the ability of test
persons to recall certain details of the digital content and
correlate their performance with the results produced by
each analysis method. Assuming the eye-mind hypothe-
sis to be valid, better recall performance o content must
correlate statistically with a higher probability of the
respective AoF to have been viewed.
The evaluation results show that EHD outperforms
commercial state-of-the-art software packages for gaze
behavior. It can identify AoFs which are not present
in the heatmap if raw data is smoothed only, but not
decomposed. Furthermore, EHD can distinguish differ-
ent types of gaze behavior. Depending on the speed of
the eye movement and the duration of ﬁxations, EHD
detects different AoFs. Therefore, EHD allows us to
better understand how persons perceive content and in
which chronological order they process it cognitively.
VI.
FUTURE WORK
In the future, we will compare GiT-BEMD ﬁltering
to regression approaches such as the Savitzky-Golay
ﬁlter [11] and continue the comparison with the state
of the art in order to implement new toolboxes for
gaze data analysis which we will make available to the
public and use ourselves to investigate human reading
behavior.
30
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

REFERENCES
[1]
T. Vo, B. S. U. Mendi, and T. Gedeon, Gaze Pattern and
Reading Comprehension.
Berlin, Heidelberg: Springer Berlin
Heidelberg, 2010, pp. 124–131.
[2]
D. Fahey, T. Gedeon, and D. Zhu, Document Classiﬁcation on
Relevance: A Study on Eye Gaze Patterns for Reading. Berlin,
Heidelberg: Springer Berlin Heidelberg, 2011, pp. 143–150.
[3]
A. Poole and L. J. Ball, “Eye tracking in human computer
interaction and usability research: Current status and future
prospects,” In C. Ghaoui (Eds.), Encyclopedia of human com-
puter interaction, 2005, pp. 111–219.
[4]
D. D. Salvucci, “An interactive model based environment for
eye-movement protocol analysis and visualization,” In Pro-
ceedings of the Symposium on Eye Tracking Research and
Applications, 2000, pp. 57–63.
[5]
D. Beymer, P. Z. Orton, and D. M. Russell, An Eye Tracking
Study of How Pictures Inﬂuence Online Reading.
Berlin,
Heidelberg: Springer Berlin Heidelberg, 2007, pp. 456–460.
[6]
V. Navalpakkam, J. Rao, and M. Slaney, “Using gaze patterns
to study and predict reading struggles due to distraction,” in
CHI ’11 Extended Abstracts on Human Factors in Computing
Systems, ser. CHI EA ’11. New York, NY, USA: ACM, 2011,
pp. 1705–1710.
[7]
R. Rosenberg and C. Klein, “The moving eye of the beholder:
Eye tracking and the perception of paintings,” in Art, Aesthet-
ics, and the Brain, J. P. Huston, M. Nadal, F. Mora, L. F. Agnati,
and C. J. C. Conde, Eds.
Oxford: Oxford University Press,
2015, ch. 5, pp. 79–108.
[8]
N. E. Huang, Z. Shen, S. R. Long, M. L. Wu, H. H. Shih,
Q. Zheng, N. C. Yen, C. C. Tung, and H. H. Liu, “The empirical
mode decomposition and Hilbert spectrum for nonlinear and
non-stationary time series analysis,” Proc. Roy. Soc. London
A, vol. 454, 1998, pp. 903–995.
[9]
S. Al-Baddai, K. Al-Subari, A. Tomé, J. J. Solé-Casals, and
E. Lang, “A green’s function-based bi-dimensional empirical
mode decomposition,” Information Sciences, vol. 348, 2016,
pp. 305–321.
[10]
Y. Melnikov, , and M. Melnikov, Green’s Functions: Construc-
tion and Applications, ser. De Gruyter studies in mathematics.
De Gruyter, 2012.
[11]
S. J. Orfanidis, Introduction to Signal Processing.
Englewood
Cliffs, NJ: Prentice Hall, 1996.
[12]
H.-K. Ko, D. M. Snodderly, and M. Poletti, “Eye movements
between saccades: Measuring ocular drift and tremor,” Vision
Research, vol. 122, 2016, pp. 93–104.
[13]
W. Dai, I. Selesnick, J.-R. Rizzo, S. J. Rucker, and T. Hudson,
“A nonlinear generalization of the savitzky-golay ﬁlter and the
quantitative analysis of saccades,” Journal of Vision, vol. 17(9),
2017, pp. 1–15.
[14]
S. Al-gawwam and M. Benaissa, “Robust eye blink detection
based on eye landmarks and savitzky-golay ﬁltering,” Informa-
tion, vol. 9(4), 2018, pp. 1–11.
[15]
H. Y. Tsang, M. Tory, and C. Swindells, “eSeeTrack-visualizing
sequential ﬁxation patters.” IEEE Transactions on Visualization
and Computer Graphics, vol. 16(6), 2010, pp. 953–962.
[16]
R. Caldara and S. Miellet, “iMap: A novel method for statistical
ﬁxation mapping of eyemovement data,” Behavior Research
Methods, vol. 43, no. 3, 2011, pp. 864–878.
[17]
J. Lao, S. Miellet, C. Pernet, N. Sokhn, and R. Caldara, “imap4:
An open source toolbox for the statistical ﬁxation mapping
of eye movement data with linear mixed modeling.” Behavior
Research Methods, 2016, pp. 1–17.
[18]
J. Heminghous and A. T. Douchowski, “iComp: A tool for
scanpath visualization and comparison,” in Proc.of the 3rd
Symposium on Applied Perception in Graphics and Visualiza-
tion, 2006, pp. 152–152.
[19]
D. R. Gitelman, “ILAB: A program for post experimental eye
movement analysis,” Behavior Research Methods, Instruments,
& Computers, vol. 34, no. 4, 2002, pp. 605–612.
[20]
C. Berger, M. Winkels, A. Lischke, and J. Hoppner, “Gaze-
Alyze: a MATLAB toolbox for the analysis of eye movement
data.” Behavior Research Methods, vol. 44, 2012, pp. 404–419.
[21]
M. A. Just and P. A. Carpenter, “A theory of reading: from
eye ﬁxations to comprehension.” Psychological review, vol. 87,
no. 4, 1980, p. 329.
[22]
P. Wessel and J. M. Becker, “Interpolation using a generalized
green’s function for a spherical surface spline in tension,”
Geophysical Journal International, vol. 174, no. 1, July 2008,
pp. 21–28.
[23]
P. Wessel and D. Bercovici, “Interpolation with splines in
tension: A green’s function approach,” Mathematical Geology,
vol. 30, no. 1, 1998, pp. 77–93.
[24]
N. E. Huang, M. L. Wu, S. R. Long, S. Shen, w. Qu,
P. Gloersen, and K. Fan, “A conﬁdence limit for the empirical
mode decomposition and Hilbert spectral analysis,” The Royal
Society, 2003, pp. 2317–2345.
[25]
Z. Wu and N. E. Huang, “A study of the characteristics of
white noise using the empirical mode decomposition method,”
Proceedings of the Royal Society of London. Series A: Math-
ematical, Physical and Engineering Sciences, vol. 460(2046),
2004, pp. 1597–1611.
[26]
——, “Ensemble Empirical Mode Decomposition: a noise-
assisted data analysis method,” Adv. Adaptive Data Analysis,
vol. 1(1), 2009, pp. 1–41.
[27]
Z. Wu, N. E. Huang, and X. Chen, “The Multidimensional En-
semble Empirical Mode Decomposition Method,” Adv. Adap-
tive Data Analysis, vol. 1, 2009, pp. 339–372.
31
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

