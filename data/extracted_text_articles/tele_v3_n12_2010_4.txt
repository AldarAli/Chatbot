An Iterative Algorithm for Compression of
Correlated Sources at Rates Approaching the
Slepian-Wolf Bound: Theory and Analysis
F. Daneshgaran, M. Laddomada, and M. Mondin
Abstract—This paper proposes a novel iterative algorithm
based on Low Density Parity Check codes for compression of
correlated sources at rates approaching the Slepian-Wolf bound.
The setup considered in the paper looks at the problem of
compressing one source without employing the source correlation,
and employing the other correlated source as side information at
the decoder which decompresses the ﬁrst source. We demonstrate
that depending on the extent of the source correlation estimated
through an iterative paradigm, signiﬁcant compression can be
obtained relative to the case the decoder does not use the
implicit knowledge of the existence of correlation. Two stages of
iterative decoding are employed. During global iterations updated
estimates of the source correlation is obtained and passed on to
the belief-propagation decoder that performs local iterations with
a pre-deﬁned stopping criterion and/or a maximum number of
local decoding iterations. Detailed description of the iterative
decoding algorithm with embedded cross-correlation estimation
are provided in the paper, in addition to simulation results
conﬁrming the potential gains of the approach.
Keywords-Correlated sources; compression; iterative decoding;
low density parity check codes; Slepian-Wolf.
I. INTRODUCTION
Consider two independent identically distributed (i.i.d.)
discrete binary memoryless sequences of length k, X =
[x1, x2, . . . , xk] and Y = [y1, y2, . . . , yk], where pairs of com-
ponents (xi, yi) have joint probability mass function p(x, y).
Assume that the two sequences are generated by two transmit-
ters which do not communicate with each other, and that both
sequences have to be jointly decoded at a common receiver.
Slepian and Wolf demonstrated that the achievable rate region
for this problem (i.e., for perfect recovery of both sequences
at a joint decoder), is the one identiﬁed by the following set
of equations imposing constraints on the rates RX and RY by
which both correlated sequences are transmitted:



RX ≥ H(X|Y ),
RY ≥ H(Y |X),
RX + RY ≥ H(X, Y )
(1)
whereby, H(X|Y ) is the conditional entropy of source X
given source Y , H(Y |X) is the conditional entropy of source
Y given source X, and H(X, Y ) is the joint entropy of the
sources. A pictorial representation of this achievable region is
given in Fig. 1.
Fred Daneshgaran is with the ECE Dept., Calif. State Univ., Los Angeles,
USA. E-mail: fdanesh@calstatela.edu
Massimiliano Laddomada is with the Electrical Engineering Department of
Texas A&M University-Texarkana, USA. E-mail: mladdomada@tamut.edu.
Marina Mondin is with the Dipartimento di Elettronica, Politecnico di
Torino, Italy. E-mail: mondin@polito.it
Rx
Ry
H(X)
H(X|Y)
H(Y)
H(Y|X)
A
B
R  +
x
R  =H(X,Y)
y
Fig. 1.
Rate region for Slepian-Wolf encoding.
In this paper, which is an extended and thorough version
of [1], we focus on trying to achieve the corner points A
and B in Fig. 1, since any other point between these can
be achieved with a time-sharing approach [2]. In particular,
we focus on the architecture shown in Fig. 2 in which we
assume that one of the two sequences, namely X in our
framework, is independently encoded with a source encoder
that does not employ the correlation between the sources X
and Y . We assume that sequence Y is compressed up to its
source entropy H(Y ) and is known at the joint decoder as
side information, and our aim is at compressing sequence X
with a rate R1 ≤ RX as close as possible to its conditional
entropy R1 ≥ H(X|Y ) in order to achieve the corner point
A in Fig. 1. The decoder tries to decompress the sequence
X, in order to obtain an estimate b
X, by employing Y as side
information and without the prior knowledge of the amount
of correlation between the sequences. Indeed, it estimates this
correlation through an iterative algorithm which improves the
decoding reliability of X.
Obviously, our solution to joint source coding at point A is
directly applicable to point B by symmetry. The overall rate
of transmission of both sequences is greater than H(Y ) +
H(X|Y ) = H(X, Y ).
With this background, let us provide a quick survey of the
recent literature related to the problem addressed in this paper.
This survey is by no means exhaustive and is meant to simply
provide a sampling of the literature in this area.
In [3], [4], [5], the authors deal with applications of sensor
networks for tracking and processing of information to be sent
to a common destination. In particular, in [4], the authors
inspired by information theory concepts and in particular, the
39
International Journal on Advances in Telecommunications, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Source
Encoder
X
Z
Perfect
Channel
R1
Joint
Decoder
X^
Y
side information
Rx
Z
Fig. 2.
Architecture of the encoder and joint decoder for the Slepian-Wolf
problem.
Slepian-Wolf theorem [2], present the problem of distributed
source coding using syndromes. The approach is based on
the use of coset codes and signiﬁcant compression gains are
achieved using the proposed technique.
In [6], the authors show that turbo codes can allow to come
close to the Slepian-Wolf bound in lossless distributed source
coding. In [7], [8], [9], the authors propose a practical coding
scheme for separate encoding of the correlated sources for
the Slepian-Wolf problem. In [10], the authors propose the
use of punctured turbo codes for compression of correlated
binary sources whereby compression has been achieved via
puncturing. The proposed source decoder utilizes an iterative
scheme to estimate the correlation between two different
sources. In [11], punctured turbo codes have been applied to
the compression of non-binary sources.
The article [12] focuses on the problem of reducing the
transmission rate in a distributed environment of correlated
sources and the authors propose a source coding scheme for
correlated images exploiting modulo encoding of pixel values
and compression of the resulting symbols with binary and non-
binary turbo codes.
Paper [13] deals with the use of irregular repeat accumulate
codes as source-channel codes for the transmission of a mem-
oryless binary sequence with side information at the decoder,
while in [14] parallel and serial concatenated convolutional
codes are considered for the same problem. In [15], [16],
the authors propose a practical coding scheme based on Low
Density Parity Check (LDPC) codes for separate encoding of
the correlated sources for the Slepian-Wolf problem.
The dual problem of Slepian-Wolf correlated source coding
over noisy channels has been dealt with in [17]-[21].
In [22] the authors consider the problem of encoding
distributed correlated sources, demonstrating that separate
source-channel coding is optimal in any network in which
correlated sources do not interfere with each other, and that
the information on a network behaves as a ﬂow. Work [23]
proposes a distributed joint source-channel coding scheme for
multiple correlated sources.
Finally, in [24] the authors consider the design for joint
distributed source and network coding.
Almost all proposed works in the literature use soft-metrics
for improving the decoding performance. An excellent work
describing the ”mathematics of soft-decoding” is the paper by
Hagenauer et al. [25].
Relative to the cited articles, the main novelty of the present
work may be summarized as follows: 1) in [10] and [16] the
encoder and decoder must both know the correlation between
the two sources. We assume knowledge of mean correlation at
the encoder. The decoder has implicit knowledge of this via
observation of the length of the encoded message. It iteratively
estimates the actual correlation observed and uses it during
decoding; 2) our algorithm can be used with any pair of
systematic encoder/decoder without modifying the encoding
and decoding algorithm; 3) the proposed algorithm is very
efﬁcient in terms of the required number of LDPC decoding
iterations. We use quantized integer LLR values (LLRQ) and
the loss of our algorithm for using integer LLRQ metrics is
quite negligible in light of the fact that it is able to guarantee
performance better than that reported in [10] and [16] (where,
to the best of our knowledge, authors use ﬂoating point
metrics) as exempliﬁed by the results shown in table II below;
4) we utilize post detection correlation estimates to generate
extrinsic information, which can be applied to any already
employed decoder without any modiﬁcation; and 5) we do
not use any interleaver between the sources at the transmitter.
Using the approach of [10] in a network, information about
interleavers used by different nodes must be communicated
and managed. This is not trivial in a distributed network such
as the internet. Furthermore, there is a penalty in terms of
delay that is incurred.
This paper is an extended version of [1] and is organized as
follows. Section II deals with the deﬁnition of the encoding
algorithm for correlated sources, and presents the class of
LDPC codes which is the focus of our current work. In
section III, we present modiﬁcation of the belief-propagation
algorithm for decoding of LDPC codes, and follow up with
details of our algorithm for iterative joint source decoding of
correlated sources with side information and iterative corre-
lation estimation. Section IV deals with the issue of sensitiv-
ity of the cross-correlation function between two sequences
to residual errors after channel decoding, demonstrating the
relative robustness of the empirical cross-correlation measure
to channel induced errors. In section V, we present simulation
results and comparisons conﬁrming the potential gains that can
be obtained from the proposed iterative algorithm. Finally, we
present the conclusion in section VI.
II. ARCHITECTURE OF THE LDPC-BASED SOURCE
ENCODER
This section focuses on the source encoder used for source
compression. LDPC coding is essential to achieve performance
close to the theoretical limit in [2].
The LDPC matrix [26] for encoding each source is consid-
ered as a systematic (n, k) code. The codes used need to be
systematic for the decoder to exploit the estimated correlation
between X and Y directly.
Each codeword C is composed of a systematic part X, and
a parity part Z which together form C = [X, Z]. With this
setup and given the parity check matrix Hn−k,n of the LDPC
code, it is possible to decompose Hn−k,n as follows:
Hn−k,n = (HX, HZ)
(2)
whereby, HX is a (n − k) × (k) matrix specifying the source
bits participating in check equations, and HZ is a (n − k) ×
40
International Journal on Advances in Telecommunications, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(n − k) matrix of the form:
HZ =






1
0
. . .
0
0
1
1
0
. . .
0
0
1
1
0
. . .
. . .
. . .
. . .
. . .
. . .
0
. . .
0
1
1






.
(3)
The choice of this structure for H, also called staircase LDPC
(for the double diagonal of ones in HZ), has been motivated
by the fact that aside from being systematic, we obtain a LDPC
code which is encodable in linear time in the codeword length
n. In particular, with this structure, the encoding operation is
as follows:
zi =



hPk
j=1 xj · HX
i,j
i
(mod 2),
i = 1
h
zi−1 + Pk
j=1 xj · HX
i,j
i
(mod 2),
i = 2, ., n − k
(4)
where, HX
i,j represents the element (i, j) of the matrix HX,
and xj is the j-th bit of the source sequence X.
Source compression takes place as follows: considering the
scheme shown in Fig. 2, we encode the length k sequence
belonging to the source X and transmit on a perfect channel
only the parity sequence Z, whose bits are evaluated as in (4).
The rate guaranteed by such an encoder is
R1 = n − k
k
.
In relation to the setup shown in Fig. 2, the Slepian-Wolf
problem reduces to that of encoding the source X with a rate
R1 as close to H(X|Y ) as possible (i.e., R1 ≥ H(X|Y )).
Note that source correlation is not employed at the encoder.
The objective of the joint decoder as explained in the next
section, is to recover sequence X by employing the correlated
source Y (considered as a side information at the decoder),
and the estimates of the correlation between the sources X
and Y obtained in an iterative fashion.
Before explaining the iterative decoding algorithm, let us
brieﬂy discuss the correlation model adopted in this frame-
work. We consider the following model in order to follow the
same framework pursued in the literature [10], [15]:
P(xj ̸= yj) = p, ∀j = 1, . . . , k
(5)
In light of the considered correlation model, and noting that
the sequence Y is available losslessly at the joint decoder, the
theoretical limit for lossless compression of X is
R1 ≥ H(X|Y ) = H(p),
whereby H(p) is the binary entropy function deﬁned as
H(p) = −p log2(p) − (1 − p) log2(1 − p).
With this setup, the following holds:
R1 + RY ≥ H(X, Y ) → R1 + 1 ≥ H(p) + 1.
(6)
Note that the encoder needs to know the mean correlation so
as to choose a rate close to H(p). It does so, by keeping
k constant while choosing n appropriately. We use the term
mean correlation, because in any actual setting, the exact
correlation between the sequences may be varying about the
LDPC
decoder
L (X)
hard
estimator
X^
correlation
estimator
Y
(i)
(i)
Y
^
Z
α
(i)
Fig. 3.
Architecture of the Iterative Joint decoder of correlated sources.
mean value. Hence, it is beneﬁcial if the decoder estimates the
actual correlation value from observations itself. While no side
information about the rate is communicated to the decoder,
the decoder knows the mean correlation implicitly from the
knowledge of block length n.
III. JOINT ITERATIVE LDPC-DECODING OF CORRELATED
SOURCES
The architecture of the iterative joint decoder for the
Slepian-Wolf problem is depicted in Fig. 3. Its goal is to
determine the best estimate b
X of the source k-sequence X,
by starting from the received parity bit (n − k)-sequence Z.
We note that aside from the fact that the receiver may a-
priori presume some correlation between both sequences exist,
no side information on this correlation is communicated to
the joint decoder. Indeed, this correlation is estimated in an
iterative fashion.
With reference to the architecture of the joint decoder
depicted in Fig. 3, we note that there are two stages of
iterative decoding. Index i denotes a global iteration whereby
during each global iteration, the updated estimate of the source
correlation obtained during the previous global iteration is
passed on to the belief-propagation decoder that performs
local iterations with a pre-deﬁned stopping criterion and/or
a maximum number of local decoding iterations.
Based on the notation above, we can now develop the
algorithm for exploiting the source correlation in the LDPC
decoder. Consider a (n, k)-LDPC identiﬁed by the matrix
H(n−k,n) as expressed in (2). Note that we only make refer-
ence to maximum rank matrix H since the particular structure
assumed for H ensures this. In particular, the double diagonal
on the parity side of the H matrix always guarantees that the
rank of H is equal to the number of its rows, i.e., n − k.
It is well known that the parity check matrix H can be
described by a bipartite graph with two types of nodes as
shown in Fig. 4; n bit-nodes corresponding to the LDPC code
bits, and n−k check-nodes corresponding to the parity checks
as expressed by the rows of the matrix H. Following the
notation employed in [29], let B(m) denote the set of bit-
nodes connected to the m-th check-node, and C(n) denote
the set of check-nodes adjacent to the n-th bit-node. With this
setup, B(m) corresponds to the set of positions of the 1’s in
the m-th row of H, while C(n) is the set of positions of the
1’s in the n-th column of H. In addition, let us use the notation
C(n) \ m and B(m) \ n to mean the sets C(n) and B(m) in
which the m-th check-node and the n-th bit-node respectively,
are excluded. Furthermore, let us identify with λn,m(un) the
log-likelihood of the message that the n-th bit-node sends to
41
International Journal on Advances in Telecommunications, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

1u
2u
3u
ku
uk+1
n
u
systematic bit nodes
parity bit nodes
check nodes
1z
2z
iz
zi+1
n-k
z
Fig. 4.
Pictorial representation of the Tanner graph of a staircase LDPC
code.
the m-th check-node, that is, the LLR of the probability that n-
th bit-node is 1 or 0 based on all checks involving the n-th bit
except the m-th check, and with Λm,n(un) the log-likelihood
of the message that the m-th check-node sends to the n-th
bit-node, that is, the LLR of the probability that the n-th bit-
node is 1 or 0 based on all the bit-nodes checked by the m-th
check except the information coming from the n-th bit-node.
With this setup, we have the following steps of the belief-
propagation algorithm:
Initialization Step: each bit-node is assigned an a-posteriori
LLR L (uj) as follows:
(
log
³
P (xj=1|yj)
P (xj=0|yj)
´
= (2yj − 1) α(i),
j = 1, . . . , k
(2zj − 1) ,
j = k + 1, . . . , n
(7)
whereby,
α(i) = log
µ
p(i)
1 − p(i)
¶
is the correction factor taking into account the estimated
correlation between sequences X and Y at global iteration i.
Note that this term derives from the correlation model adopted
in this paper as expressed in (5), in which the correlation
between any bit in the same position in the two sequences X
and Y is seen as produced by an equivalent binary symmetric
channel with transition probability p.
In our setup, before the ﬁrst global iteration, α(0) = 1 since
the degree of correlation is not known a priori.
In summary, for any position (m, n) such that Hm,n = 1,
set:
λn,m(un) = L(un),
(8)
and
Λm,n(un) = 0.
(9)
(1) Check-node update: for each m = 1, . . . , n − k, and for
each n ∈ B(m), compute:
Λm,n(un) = 2 tanh−1


Y
p∈B(m)\n
tanh
µλp,m(up)
2
¶

(10)
(2) Bit-node update: for each t = 1, . . . , n, and for each m ∈
C(t), compute:
λt,m(ut) = L(ut) +
X
p∈C(t)\m
Λp,n(ut).
(11)
(3) Decision: for each bit node ut with t = 1, . . . , n, compute:
λt(ut) = L(ut) +
X
p∈C(t)
Λp,n(ut),
(12)
and quantize the results such that ut = 0 if λt(ut) < 0, and
ut = 1 otherwise.
If H · U T
=
0 then halt the algorithm and output
bxt = ut, t = 1, . . . , k as the estimate of the decompressed
source bits, X, corresponding to the ﬁrst source. Otherwise,
if the number of iterations is less than a predeﬁned maximum
number, iterate the process starting from step (1).
The architecture of the iterative joint decoder is depicted
in Fig. 3. Let us elaborate on the signal processing involved.
In particular, as before let x and y be two correlated binary
random variables which can take on the values {0, 1} and
let r = x ⊕ y. Let us assume that random variable r takes
on the values {0, 1} with probabilities P(r = 1) = pr and
P(r = 0) = 1 − pr.
The correction factor α(i) at global iteration (i) is evaluated
as follows,
α(i) = log
µ
pˆr
1 − pˆr
¶
,
(13)
by counting the number of places in which b
X(i) and Y differ,
or equivalently by evaluating the Hamming weight wH(.) of
the sequence bR(i) =
b
X(i) ⊕ Y whereby, in the previous
equation,
pˆr = wH( bR(i))
k
.
In the latter case, by assuming that the sequence bR = b
X ⊕ Y
is i.i.d., we have:
α(i) = log
Ã
wH( bR(i))
k − wH( bR(i))
!
(14)
where k is the source block size. Above, letters highlighted
with b. are used to mean that the respective parameters have
been estimated.
Formally, the iterative decoding algorithm can be stated as
follows:
1) Set the log-likelihood ratios α(0) to one (see Fig. 3).
Compute the log-likelihood ratios for any bit node
using (7).
2) For each global iteration i = 1, . . . , q, do the following:
a) perform a belief-propagation decoding on the par-
ity bit sequence Z by using a predeﬁned maximum
number of local iterations, and the side information
represented by the correlated sequence Y along
with the correction factor α(i−1);
b) Evaluate α(i) using (14);
c) If
¯¯α(i) − α(i−1)¯¯ ≥ 10−4 go back to (a) and
continue iterating, else exit.
Point c) in the previous code fragment is used in order to
speed-up the overall iterative algorithm. Extensive tests we
conducted suggested that the threshold value of 10−4 may be
used for this purpose. Obviously, one can keep iterating until
the last global iteration as well.
42
International Journal on Advances in Telecommunications, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

A. Overview of Integer-Metrics Belief-Propagation Decoder
In this section, we brieﬂy describe the LDPC decoder work-
ing with integer LLRs. This approach leads to efﬁcient belief-
propagation decoding. Following the rationales described in
[27], we begin by quantizing any real LLR (denoted LLRQ
after quantization) employed in the initialization phase of
the belief-propagation decoder in (7), using the following
transformation:
LLRQ =
½
⌊2qL(uj) + 0.5⌋ ,
j = 1, . . . , k
⌊2zj − 1⌋ · S,
j = k + 1, . . . , n
(15)
whereby ⌊·⌋ stands for rounding to the smaller integer in the
unit interval in which the real number falls, L(uj) is the real
LLR, S is a suitable scaling factor, and q is the precision
chosen to represent the LLR with integer metrics. In our
belief-propagation decoder, we use q = 3, which guarantees
a good trade-off between BER performance and complexity
of the decoder implementation [27]. The scaling factor S is
the greatest integer metric processed by the iterative decoder.
In our set-up, we use S = 10000. Note that such a scaling
factor depends on the practical implementations of the belief-
propagation decoder. Sufﬁce it to say that in our setup, S gives
high likelihood to the parity bits zj, ∀j = k + 1, . . . , n, since
they are transmitted through a perfect channel to the decoder.
All the summations shown in equations (11) and (12) are
performed on integer LLRs. Finally, (10) has to be modiﬁed
for working with integer metrics. To this end, we follow the
schedule proposed in [29] in order to avoid real calculations
and only use look-up tables and integer additions. This way,
the basic operation L(x ⊕ y) in the check node update is
approximated as follows:
L(x ⊕ y) ≈ sign(L(x)) · sign(L(y)) · min(|L(x)|, |L(y)|) (16)
+ log
³
1 + eL(x)+L(y)´
− log
³
eL(x) + eL(y)´
whereby, sign(·) is the function which evaluates the sign of
the enclosed LLR. The latter two log functions are evaluated
through look-up tables containing samples of the log func-
tions approximated with piecewise linear approximation as
suggested in [29].
IV. SENSITIVITY OF THE CROSS-CORRELATION TO
RESIDUAL ERRORS AFTER CHANNEL DECODING
In this section, we wish to demonstrate the relative robust-
ness of the empirical cross-correlation between sequences b
X
and Y to residual errors after channel decoding. To this end,
let X and Y be two binary vectors of length k. Correlation
between two binary streams can be measured in two ways. One
technique is to map a logic-0 to the integer +1, logic-1 to −1
and to take the inner product of the two vectors deﬁning our
data packets and divide by the vector length (assumed to be
the same for both packets). It is well known that the resulting
empirical correlation estimate lies in the range −1 ≤ ρ ≤ +1.
Positive value of ρ signiﬁes a similarity between X and Y ,
while a negative ρ signiﬁes a similarity between X and ¯Y ,
where ¯Y is the complement of vector Y . This traditional
measure of correlation is not of direct interest to us since we
wish to relate a correlation measure to an empirical probability
which is strictly positive.
To this end, let us deﬁne rn = xn ⊕ yn as the XOR of the
n-th component of the vectors X and Y . Similarly, we deﬁne
R = X ⊕ Y whereby R is obtained via componentwise XOR
of the components of the vectors X and Y . Let the number of
places in which X and Y agree be γ and deﬁne the empirical
cross-correlation between these two vectors as ρ = γ/k. Note
that with this deﬁnition, we may take ρ as the empirical
probability that rn = 0 and a value of for instance ρ = 0.7 and
ρ = 0.3 would have the same information content as measured
by the value of the binary entropy function. Notice that,
since intrinsic soft information is generated from probability
estimates, this measure of correlation is more useful to us than
the traditional deﬁnition.
Let us consider the Slepian-Wolf problem discussed above,
and suppose that what is available at the receiver is a noisy
version of X denoted
b
X. For instance,
b
X could be an
erroneous version of X obtained after transmission through
a noisy channel modelled as a Binary Symmetric Channel
(BSC) with transition probability pe. We assume that the error
events inﬂicting the sequence X are independent identically
distributed (i.i.d.). The joint decoder generates an empirical
estimate of the cross-correlation based on the use of the
sequences b
X and Y by forming the vector bR =
b
X ⊕ Y
and counting the number of places where bR is zero. Let us
denote this count as ˆγ. Clearly, ˆγ is a random variable. The
question is, what is the Probability Mass Function (PMF) of
ˆγ? Knowledge of this PMF allows us to assess the sensitivity
of our estimate of the cross-correlation to errors in the original
sequence X.
It is relatively straightforward to express the probability that
(ˆrn = rn) and (ˆrn ̸= rn) as P(ˆrn = rn) = P(ˆxn = xn) =
1 − pe, P(ˆrn ̸= rn) = P(ˆxn ̸= xn) = pe. Consider applying
a permutation to the sequences X and Y so that the permuted
sequences agree in the ﬁrst γ locations, and disagree in the
remaining (k − γ) locations. The permutation is applied to
simplify the explanation of how we may go about obtaining
the PMF of ˆγ and by no means impacts the results. It is
evident that the permuted sequence π(R) contains γ zeros in
the ﬁrst γ locations and (k−γ) ones in the remaining locations.
Now consider evaluation of the probability P(ˆγ = γ + ν)
for ν = 0, 1, .., (k − γ). We deﬁne π(R)γ to represent the
ﬁrst γ bits of π(R) and π(R)k−γ the remaining (k − γ) bits.
Similarly we deﬁne π( bR)γ and π( bR)k−γ. For a ﬁxed ν, the
event {ˆγ = γ+ν} corresponds to the union of the events of the
type: π( bR)k−γ differs from π(R)k−γ in (ν + l) positions for
some l ∈ {0, 1, .., γ}, π( bR)γ differs from π(R)γ in l positions,
and the remaining bits of π( bR) and π(R) are identical. The
probability of such elementary events are given by:
µ
γ
l
¶ µ
k − γ
ν + l
¶
(1 − pe)k−ν−2l(pe)ν+2l.
(17)
It can be shown that the probability of the event {ˆγ = γ+ν}
for ν = 0, 1, .., (k − γ) is given by:
P(ˆγ
=
γ + ν) =
γ
X
l=0
·µ γ
l
¶ µ k − γ
ν + l
¶
·
43
International Journal on Advances in Telecommunications, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

0.8
0.82
0.84
0.86
0.88
0.9
0.92
0.94
0.96
0.98
1
0
0.002
0.004
0.006
0.008
0.01
0.012
Empirical ρ
P(ρ)
ρ=0.9−pe=10−1
ρ=0.9−pe=10−2
ρ=0.9−pe=10−3
ρ=0.9−pe=10−4
ρ=0.9−pe=10−5
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0.88
0
0.002
0.004
0.006
0.008
0.01
Empirical ρ
P(ρ)
ρ=0.8−pe=10−1
ρ=0.8−pe=10−2
ρ=0.8−pe=10−3
ρ=0.8−pe=10−4
ρ=0.8−pe=10−5
Fig. 5.
Probability mass function (PMF) of ˆρ for three different values of
raw error rate p when the true cross-correlations between the data packets are
ρ = 0.8 and 0.9 at data block length of k = 16400.
·(1 − pe)k−ν−2l(pe)ν+2l¤
,
(18)
while, for m = 1, 2, .., γ we have:
P(ˆγ
=
γ − m) =
γ
X
l=m
·µ
γ
l
¶ µ
k − γ
l − m
¶
·
·(1 − pe)k−2l+m(pe)2l−m¤
.
(19)
In the next section, we shall present simulation results for
iterative joint decoding of correlated sources for data packet
size of k = 16400. Hence, let us look at representative results
associated with the estimation of ρ for this block length:
1) Fig. 5 depicts the PMF of ˆρ for ﬁve different values
of raw error rate pe when the true cross-correlations
between the data packets are ρ = 0.8 and 0.9 at data
block length of k = 16400. The following observation
is in order: there is a bias in the empirical estimate of
ρ as measured by the most probable value of ˆρ and the
true value of ρ. This bias is a strong function of pe.
2) As noted above, the most likely value of ˆρ, denoted
M(ˆρ) (i.e., the Mode), obtained from evaluation of the
empirical cross-correlation from noisy received vectors
is not necessarily the true value ρ. This is particularly
so at larger values of pe and for small and large values
of ρ. Fig. 6 captures this behavior for three values of
pe = 10−1, 10−2 and pe = 10−3 as a function of ρ
for block length k = 16400. In particular, this ﬁgure
shows the difference (ρ−M(ˆρ)) versus ρ obtained from
empirical evaluation of the cross-correlation from noisy
received vectors.
3) The standard deviation of ˆρ is a strong function of pe
itself. Fig. 7 depicts the standard deviation of ˆρ as a
function of pe for k = 16400 for three different values of
ρ. This ﬁgure shows that the standard deviation increases
slowly with increasing pe. Note that even at values of
pe as large as p = 0.1 this standard deviation is still
relatively small for ρ in the range ρ = 0.1 to ρ = 0.9.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.1
−0.08
−0.06
−0.04
−0.02
0
0.02
0.04
0.06
0.08
ρ
ρ−E[ρ]
pe=10−1
pe=10−2
pe=10−3
Fig. 6.
The difference (ρ−M(ˆρ)) versus ρ whereby M(ˆρ) denotes the most
probable value of ˆρ obtained from empirical evaluation of the cross-correlation
from noisy received vectors (block length k = 16400).
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
1
1.5
2
2.5
3
3.5
4
x 10
−3
pe
σρ
ρ=0.8
ρ=0.9
ρ=0.98
Fig. 7.
Standard deviation of ˆρ as a function of p (block length k = 16400).
V. SIMULATION RESULTS AND COMPARISONS
We have simulated the performance of our proposed iter-
ative joint source decoder. In order to compare our results
with others proposed in the literature, we follow the same
framework as in [10], [15], [16].
In the following, we provide sample simulation results
associated with various (n, k) LDPC codes designed with the
technique proposed in [28]. In particular, for a fair comparison
with the results provided in [16], we designed various LDCP
codes with k = 16400 and k = 16000. The details and the
parameters of the designed LDPCs are given in Tables I-II.
Parameters given in Table I are the source block length k,
the codeword length n, the rate R1 of the source, expressed
as
n−k
k
(i.e., inverse of the compression ratio), the average
degree dv of the bit nodes, and the average degree dc of the
check nodes of the designed LDPCs. Note that, the encoding
procedure adopted in our approach is different from the one
proposed in [16] in that we source encode k bits at a time
44
International Journal on Advances in Telecommunications, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE I
PARAMETERS OF THE DESIGNED LDPCS.
LDPC
k
n
R1
dv
dc
L1
16400
30750
0.875
2.8
6
L2
16400
26200
0.597
3
8
L3
16400
22400
0.365
3.21
12
L4
16000
20000
0.25
3.59
18
L5
16400
20300
0.237
3.45
18
L6
16400
20000
0.219
3.23
18
L7
16400
19300
0.176
3.0
20
L8
16400
19500
0.189
3.0
19
and transmit only n − k bits. In [16], the authors proposed
a source compression which encodes n source bits at a time,
and transmits n − k syndrome bits.
The degree distributions of the designed LDPC codes,
labelled L1 to L8 in Table I, are shown in Table II whereby,
the integer number multiplying any power of the indeterminate
xw identiﬁes the number of bit nodes in λLr(x), and check
nodes in ρLr(x) having degree w + 1 in the r-th designed
LDPC with r = 1, . . . , 8.
For local decoding of the LDPC codes, the maximum
number of local iterations has been set to 50, while the
maximum number of global iterations is 5, even though the
stopping criterion discussed in the previous section has been
adopted.
The results on the compression achieved with the proposed
algorithm are shown in Table III. The ﬁrst row shows the
correlation parameter assumed, that is, the probability p =
P(xj ̸= yj), ∀j = 1, . . . , k. The second row shows the
joint entropy limit as expressed in (6) for various values of
the correlation parameter p. The third and fourth rows show
the results on source compression presented in papers [10],
[16], while the last row presents the results on compression
achieved with the proposed algorithm employing a maximum
of 5 global iterations in conjunction with using the stopping
criterion noted in the previous section. As in [16], we assume
error free compression for a target Bit Error Rate (BER) 10−6.
Note that statistic of the results shown has been obtained by
counting 30 erroneous frames.
From Table III it is evident that signiﬁcant compression
gains with respect to the theoretical limits can be achieved as
the correlation between sequences X and Y increases. Notice
that as opposed to the algorithm proposed in [16], we do not
assume p is known at the decoder. Indeed, this parameter is
iteratively estimated as discussed in the previous section. A
similar approach has been pursued in [10], whereby an itera-
tive approach is employed for the estimation of the correlation
between the two correlated sequences, but employing turbo
codes.
Finally, note that we employ integer soft-metrics as ex-
plained in the previous section, while in [10], [16], to the
best our knowledge, the authors employ real metrics. The
algorithm working on integer metrics is very fast and reduces
considerably the complexity burden required by the two-stage
iterative algorithm (i.e., the local-global combination).
Fig. 8 shows the BER performance of the proposed iterative
decoding algorithm for various global iterations and as a
TABLE II
DEGREE DISTRIBUTIONS OF THE DESIGNED LDPCS.
λL1(x)
1 + 14349x + 15400x2 + 200x3 + 800x12
ρL1(x)
x4 + 14349x5
λL2(x)
1 + 9799x + 11000x2 + 4700x3 + 700x9
ρL2(x)
x6 + 9799x7
λL3(x)
1 + 5999x + 13700x2 + 1800x3 + 900x12
ρL3(x)
x10 + 5999x11
λL4(x)
1 + 3999x + 12600x2 + 1300x3 + 2100x9
ρL4(x)
x16 + 3999x17
λL5(x)
1 + 3899x + 12800x2 + 2400x3 + 1200x11
ρL5(x)
x16 + 3899x17
λL6(x)
1 + 3599x + 14400x2 + 1200x3 + 800x11
ρL6(x)
x16 + 3599x17
λL7(x)
1 + 2899x + 15400x2 + 750x3 + 250x11
ρL7(x)
x18 + 2899x19
λL8(x)
1 + 3099x + 15900x2 + 500x9
ρL8(x)
x17 + 3099x18
1.105
1.11
1.115
1.12
1.125
1.13
1.135
1.14
1.145
1.15
10
−8
10
−7
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
H(X,Y)
BER
Th.limit 1.1076
Th.limit 1.1171
Th.limit 1.1175
1−glob.it max
3−glob.it max
5−glob.it max
Fig. 8.
BER performance of the proposed iterative decoding algorithm for
various global iterations as a function of the joint entropy between sources
X and Y , when the stopping criterion for global iterations is applied. Results
refer to the LDPC labelled L8 in Tables I-II, guaranteeing a compression rate
of R1 = 0.189. For a target BER= 10−6, we also show the theoretical limit
as expressed by the Slepian-Wolf bound achieved by the iterative algorithm
applied to the LDPC L8 for increasing values of the maximum number of
global iterations.
function of the joint entropy between sources X and Y ,
when the stopping criterion for global iterations is applied
and the maximum number of global iterations speciﬁed in
the legend are used. The LDPC used for encoding is the one
labelled L8 in Tables I-II which guarantees a compression
rate of R1
=
0.189. At BER=
10−6, the overall rate
R = R1 + RY = 1.189 is within 0.075 of the theoretical
limit of 1.1175.
In order to assess the effects of the global iterations on
BER performances, we simulated the same LDPC L8 without
stopping criterion on the global iterations. The resulting BER
performance are shown in Fig. 9. Notice that 3- or 4- global
iterations sufﬁce to get almost all that can be gained from the
knowledge of the cross-correlation.
In Table IV, we show the average number of local iterations
45
International Journal on Advances in Telecommunications, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE III
COMPRESSION RATE PERFORMANCE OF THE ITERATIVE ALGORITHM FOR VARIOUS JOINT ENTROPIES.
p
0.01
0.02
0.025
0.02875
0.05
0.1
0.2
H(p) + 1
1.08
1.141
1.169
1.188
1.286
1.469
1.722
R [10]
-
-
1.31
-
1.435
1.63
1.89
R [16]
-
-
1.276
-
1.402
1.60
1.875
R = R1 + RY
1.176-L7
1.219-L6
1.237 -L5
1.25-L4
1.365-L3
1.597-L2
1.875-L1
1.105
1.11
1.115
1.12
1.125
1.13
1.135
1.14
1.145
1.15
10
−7
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
H(X,Y)
BER
Th.limit 1.1076
Th.limit 1.1171
Th.limit 1.1175
1−glob.it com
2−glob.it com
3−glob.it com
4−glob.it com
5−glob.it com
Fig. 9.
BER performance of the proposed iterative decoding algorithm for
various global iterations as a function of the joint entropy between sources
X and Y , without stopping criterion on global iterations. Results refer to
the LDPC labelled L8 in Tables I-II, guaranteeing a compression rate of
R1 = 0.189. For a target BER= 10−6, we also show the theoretical limit
as expressed by the Slepian-Wolf bound achieved by the iterative algorithm
applied to the LDPC L8 for increasing values of the maximum number of
global iterations.
performed by the joint decoder at the end of a given global
iteration, for various values of correlation between the sources
and when the stopping criterion on the global iterations is
not applied. Finally, we evaluated the average number of
global iterations performed by the iterative algorithm when
the stopping criterion on global iterations is employed dur-
ing decoding. Simulation results show that when the LDPC
decoder works at BER levels less than or equal to 10−5,
the average number of global iterations equals 1.2, thus
guaranteeing a very efﬁcient iterative approach to the co-
decompression problem. In other words, an overall average
number of 80 LDPC decoding iterations sufﬁces to obtain
good BER performance.
In the following set of simulation results, we adopted
another approach in order to test the proposed algorithm for
varying actual correlation levels. In particular, for any given
value of mean correlation p, we generate a uniform random
variable having mean value equal to the mean correlation
itself and with a maximum variation of ∆p around this
mean value. We used the following maximum variations:
∆p = 0.5, 0.2, 0.1%, and ∆p = 0.0% which refers to the
case in which the correlation value is not variable, but ﬁxed.
For each data block, we set the actual correlation equal
to the mean correlation plus this perturbation. The decoder
TABLE IV
AVERAGE NUMBER OF LOCAL ITERATIONS FOR LDPC L8 DECODING
WITH MAXIMUM LOCAL ITERATIONS PARAMETER SET TO 50, FOR
DIFFERENT GLOBAL ITERATION INDICES.
p
H(X, Y )
1-st it.
2-nd it.
3-rd it.
4-th it.
0.0213
1.1487
50.00
50.00
50.00
50.00
0.0204
1.1437
50.00
50.00
50.00
50.00
0.0195
1.1386
50.00
46.16
46.12
46.11
0.0186
1.1335
50.00
39.51
38.45
34.58
0.0177
1.1283
42.47
23.65
23.35
22.31
0.0168
1.1231
34.73
16.16
16.09
15.17
0.0159
1.1178
22.97
12.59
12.58
12.57
1.08
1.1
1.12
1.14
1.16
1.18
1.2
1.22
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
H(X,Y)
BER
p=0.015−∆ p=0.5%
p=0.015−∆ p=0.2%
p=0.015−∆ p=0.1%
p=0.015−∆ p=0.0%
p=0.015−∆ p=0.1%*
p=0.025−∆ p=0.5%
p=0.025−∆ p=0.2%
p=0.025−∆ p=0.1%
p=0.025−∆ p=0.0%
p=0.025−∆ p=0.1%*
Fig. 10.
BER performance of the proposed iterative decoding algorithm
for a maximum of 5 global iterations as a function of the joint entropy
between sources X and Y , when the stopping criterion for global iterations
is applied. Results refer to the LDPCs labelled L8 and L5 in Table I. The
legend shows the mean correlation value p and the maximum value of the
correlation variation with respect to the mean value. Curves labelled with
∗ refer to the ones obtained without the iterative paradigm, using the mean
correlation value.
iterates to estimate the actual correlation value which varies
around its mean value from one block to the next. In effect,
the parameter p is iteratively estimated as discussed in the
previous section. A similar approach has been pursued in [10]
for ﬁxed correlation level, whereby an iterative approach is
used for the estimation of the correlation between the two
correlated sequences, but employing turbo codes.
Fig. 10 shows the BER performance of the proposed itera-
tive decoding algorithm for a maximum of 5 global iterations
and as a function of the joint entropy between sources X and
Y , when the stopping criterion for global iterations is applied.
LDPCs used for encoding are the one labelled L5 and L8 in
46
International Journal on Advances in Telecommunications, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

1.27
1.28
1.29
1.3
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
H(X,Y)
BER
p=0.05−∆ p=0.5%
p=0.05−∆ p=0.2%
p=0.05−∆ p=0.1%
p=0.05−∆ p=0.0%
1.47
1.48
1.49
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
H(X,Y)
BER
p=0.1 −∆ p=0.5%
p=0.1 −∆ p=0.2%
p=0.1 −∆ p=0.1%
p=0.1 −∆ p=0.0%
Fig. 11.
BER performance of the proposed iterative decoding algorithm for
a maximum of 5 global iterations as a function of the joint entropy between
sources X and Y , when the stopping criterion for global iterations is applied.
Results refer to the LDPCs labelled L3 (left subplot) and L2 (right subplot)
in Table I. The legend shows the mean correlation value p and the maximum
value of the correlation variation with respect to the mean value.
Table I which guarantee compression rates of RX = 0.237
and RX = 0.189, respectively. LDPC labelled L5 is used at
mean values of p equal to 0.025, while LDPC L8 is adopted
for a mean correlation of 0.015. From Fig. 10 one clearly sees
that LDPC decoding does not converge when the decoder does
not iterate for estimating the actual value of p, but uses only
its mean value for setting the extrinsic information. Notice
also that the performances of the iterative decoder when the
correlation value is ﬁxed (curves labelled ∆p = 0.0 in Fig. 10),
are very close to the case in which the actual correlation value
varies within ∆p = 0.1% from the mean value.
Similar considerations can be deduced from Fig. 11 which
shows the BER performance of the proposed iterative decoding
algorithm when using LDPCs labelled L2 and L3 in Table I
which guarantee compression rates of RX
= 0.597 and
RX = 0.365, respectively. LDPC labelled L2 is used at mean
correlation equal to 0.1, while LDPC L3 is used with a mean
correlation of 0.05. Note that the performance degrades as
∆p increases since the encoder works further away from its
optimal operating point.
The results on the compression achieved with the proposed
algorithm are shown in Table V for the case in which the cor-
relation value is ﬁxed. The ﬁrst row shows the ﬁxed correlation
parameter assumed, namely, p = P(xj ̸= yj), ∀j = 1, . . . , k
in our model. The second row shows the joint entropy limit
for various values of the ﬁxed correlation parameter p. The
third row presents the results on compression achieved with
the proposed algorithm employing a maximum of 5 global
iterations in conjunction with using the stopping criterion
noted in the previous section. As in [16], we assume error free
compression for a target Bit Error Rate (BER) 10−6. Note that
statistic of the results shown has been obtained by counting
30 erroneous frames.
From Table V it is evident that signiﬁcant compression gains
TABLE V
COMPRESSION RATE PERFORMANCE OF THE ITERATIVE ALGORITHM FOR
H(p) = 0.112.
p
0.015
H(p) + 1
1.112
R = RX + RY
1.189 -L8
with respect to the theoretical limits can be achieved as the
correlation between sequences X and Y increases.
VI. CONCLUSION
In this paper we have presented a novel iterative joint de-
coding algorithm based on LDPC codes for the Slepian-Wolf
problem of compression of correlated information sources.
In the considered scenario, two correlated sources transmit
toward a common receiver. The ﬁrst source is compressed
by transmitting the parity check bits of a systematic LDPC
encoded codeword, without the knowledge of the correlation
among the sources. The correlated information of the second
source is employed as side information at the receiver and
used for decompressing and decoding of the ﬁrst source. The
correlation is not known a-priori, but is estimated through an
iterative paradigm. Both the iterative decoding algorithm and
the cross-correlation estimation procedure have been described
in detail.
Simulation results suggest that relatively large compression
gains are achievable at relatively small number of global
iterations specially when the sources are highly correlated.
VII. ACKNOWLEDGEMENTS
This work was partially supported by the PRIN 2007 project
Feasibility study of an optical Earth-satellite quantum com-
munication channel and by the CAPANINA project (FP6-IST-
2003-506745) as part of the EU VI Framework Programme.
REFERENCES
[1] F. Daneshgaran, M. Laddomada, and M. Mondin, “LDPC-based iterative
algorithm for compression of correlated sources at rates approaching the
Slepian-Wolf bound,” In Proc. of the Int. Conf. SPACOMM 2009, pp.74-
79, Colmar, France, July 2009.
[2] D. Slepian and J. Wolf, “Noiseless coding of correlated information
sources,” IEEE Trans. on Inform. Theory, vol.IT-19, no. 4, pp.471-480,
July 1973.
[3] L. J. Guibas, “Sensing, tracking, and reasoning with relations,” IEEE
Signal Processing Magazine, no.3, pp.73-85, March 2002.
[4] S. S. Pradhan, J. Kusuma, and K. Ramchandran, “Distributed com-
pression in a dense microsensor network,”
IEEE Signal Processing
Magazine, no.3, pp.51-60, March 2002.
[5] F. Zhao, J. Shin, and J. Reich, “Information-driven dynamic sensor col-
laboration,” IEEE Signal Processing Magazine, no.3, pp.61-72, March
2002.
[6] A. Aaron and B. Girod, “Compression with side information using turbo-
codes,” In Proc. of Data Compress. Conf., DCC’02, pp.252-261, April
2002.
[7] J. Bajcsy and P. Mitran, “Coding for the Slepian-Wolf problem with
turbo codes,” In Proc. of Global Telecomm. Conf., GLOBECOM’01,
vol.2, pp.1400-1404, Nov. 2001.
[8] J. Bajcsy and P. Mitran, “Coding for the Wyner-Ziv problem with turbo-
like codes”, In Proc. of Inter. Symposium on Infor. Theory, ISIT’02,
pp.91, 2002.
47
International Journal on Advances in Telecommunications, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[9] J. Bajcsy and I. Deslauriers, “Serial turbo coding for data compression
and the Slepian-Wolf problem”,In Proc. of Inform. Theory Workshop,
pp.296-299, March 31-April 4 2003.
[10] J. Garcia-Frias and Y. Zhao, “Compression of correlated binary sources
using turbo-codes,” IEEE Communications Letters, vol.5, no.10, pp.417-
419, October 2001.
[11] Y. Zhao and J. Garcia-Frias, “Joint estimation and compression of
correlated nonbinary sources using punctured turbo codes,” IEEE Trans-
actions on Communications, vol.53, no.3, pp.385-390, March 2005.
[12] A. D. Liveris, Z. Xiong, and C. N. Georghiades, “A distributed source
coding technique for correlated images using turbo-codes,” IEEE Com-
munications Letters, vol.6, no.9, pp.379-381, September 2002.
[13] A. D. Liveris, Z. Xiong, and C. N. Georghiades, “Joint source-channel
coding of binary sources with side information at the decoder using IRA
codes,” In Proc. of IEEE 2002 Multimedia Signal Processing Workshop,
December 2002.
[14] A. D. Liveris, Z. Xiong, and C. N. Georghiades, “Distributed com-
pression of binary sources using conventional parallel and serial con-
catenated convolutional codes,” In Proc. of IEEE Data Compression
Conference, 2003.
[15] A. D. Liveris, Z. Xiong, and C. N. Georghiades, “Compression of binary
sources with side information at the decoder using LDPC codes,” IEEE
Communications Letters, vol.6, no.10, pp.440-442, October 2002.
[16] A. D. Liveris, Z. Xiong, and C. N. Georghiades, “Compression of binary
sources with side information using low-density parity-check codes,” In
Proc. of IEEE GLOBECOM 2002, vol. 2, pp.1300-1304, 17-21 Nov.
2002.
[17] J. Garcia-Frias, “Joint source-channel decoding of correlated sources
over noisy channels,” In Proc. of DCC 2001, pp.283-292, March 2001.
[18] J. Garcia-Frias, W. Zhong, and Y. Zhao, “Iterative decoding schemes for
source and joint source-channel coding of correlated sources,” In Proc.
of Asilomar Conference on Signals, Systems, and Computers, November
2002.
[19] F. Daneshgaran, M. Laddomada, and M. Mondin, “Iterative joint channel
decoding of correlated sources employing serially concatenated convo-
lutional codes,” IEEE Transactions on Information Theory, vol.51, no.7,
pp.2721-2731, July 2005.
[20] F. Daneshgaran, M. Laddomada, and M. Mondin, “Iterative joint channel
decoding of correlated sources,” IEEE Transactions on Wireless Com-
munications, Vol. 5, No. 10, pp.2659-2663, October 2006.
[21] F. Daneshgaran, M. Laddomada, and M. Mondin, “LDPC-based channel
coding of correlated sources with iterative joint decoding,” IEEE Trans-
actions on Communications, Vol. 54, No. 4, pp.577-582, April 2006.
[22] J. Barros and S.D. Servetto, “The sensor reachback problem,” Sub-
mitted to IEEE Transactions on Information Theory, available online at
http://cn.ece.cornell.edu/publications/papers/20031112, November 2003.
[23] X. Zhu, L. Zhang, and Y. Liu, “ A distributed joint source-channel
coding scheme for multiple correlated sources,” Fourth International
Conference on Digital Communications and Networking in China, 2009,
pp. 1 - 6.
[24] W. Yunnan, V. Stankovic, Z. Xiong, and S. Kung, “ On practical design
for joint distributed source and network coding,” IEEE Transactions on
Information Theory, Vol. 55, No. 4, pp. 1709 - 1720, April 2009.
[25] J. Hagenauer, E. Offer, and L. Papke, “Iterative decoding of binary block
and convolutional codes,” IEEE Trans. on Inform. Theory, vol.42, no.2,
pp.429-445, March 1996.
[26] T.J. Richardson and R.L. Urbanke, “Efﬁcient encoding of low-density
parity-check codes,” IEEE Transactions on Information Theory, vol.47,
no.2, pp.638 - 656, Feb. 2001.
[27] G. Montorsi and S. Benedetto, “Design of ﬁxed-point iterative decoders
for concatenated codes with interleavers,” IEEE Journal on Selected
Areas in Communications, vol.19, No. 5, pp.871-882, May 2001.
[28] Xiao-Yu Hu, E. Eleftheriou, and D.M. Arnold, “Progressive edge-growth
Tanner graphs,” IEEE Global Telecommunications Conference, vol.2,
pp.995-1001, 25-29 Nov. 2001.
[29] Xiao-Yu Hu, E. Eleftheriou, D.-M. Arnold, and A. Dholakia, “Efﬁcient
implementations of the sum-product algorithm for decoding LDPC
codes,” IEEE Global Telecommunications Conference, vol.2, pp.25-29,
25-29 Nov. 2001.
48
International Journal on Advances in Telecommunications, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

