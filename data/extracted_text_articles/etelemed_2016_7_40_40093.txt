Wearable Recognition System for Emotional States Using Physiological Devices
Ali Mehmood Khan
TZi-AGKI, Universität Bremen
Bremen, Germany
e-mail: akhan@tzi.de
Michael Lawo
TZi-AGKI, Universität Bremen
Bremen, Germany
e-mail: mlawo@tzi.de
Abstract— Recognizing emotional states is becoming a major
part of a user's context for wearable computing applications.
The system should be able to acquire a user's emotional states
by using physiological sensors. We want to develop a personal
emotional states recognition system that is practical, reliable,
and can be used for health-care related applications. We
propose to use the eHealth platform [1] which is a ready-made,
light weight, small and easy to use device for recognizing a few
emotional states like ‘Sad’, ‘Dislike’, ‘Joy’, ‘Stress’, ‘Normal’,
‘No-Idea’, ‘Positive’ and ‘Negative’ using decision tree (J48)
classifier. In this paper, we present an approach to build a
system that exhibits this property and provides evidence based
on data for 8 different emotional states collected from 24
different subjects. Our results indicate that the system has an
accuracy rate of approximately 98%. In our work, we used
four physiological sensors i.e. ‘Blood Volume Pulse’ (BVP),
‘Electromyogram’ (EMG), ‘Galvanic Skin Response’ (GSR),
and ‘Skin Temperature’ in order to recognize emotional states
(i.e. stress, joy/happy, sad, normal/neutral, dislike, no-idea,
positive and negative).
Keywords- Emotional states; Physiological devices;
International Affective Picture System; Machine learning
classifier; User studies.
I.
INTRODUCTION
It is hard to express your own emotions; no one can
accurately measure the degree of his/her emotional state.
According to Darwin, “....the young and the old of widely
different races, both with man and animals, express the same
state of mind by the same movement” [16]. According to
Paul Ekman, there are seven basic emotions which are fear,
surprise, sad, dislike, disgrace, disgust and joy [14]. The
concept behind emotional states (also known as affective
computing) was first introduced by Rosalind Picard in 1995
[2]. Since then the affective computing group have produced
novel and innovative projects in that domain [3]. Emotional
states recognition has received attention in recent years and
is able to support the health care industry. Emotions and
physical health have a strong link in influencing the immune
system too [15]. Due to untreated, chronic stress; occurrence
of an emotional disorder is more than 50% [6]. According to
Richmond Hypnosis Center, due to stress; 110 million
people die every year.
That means, every 2 seconds, 7
people die [4].
According to American Psychological
Association, in 2011 about 53 percent of Americans claimed
stress as a reason behind personal health problems [5].
According to WebMD, intense and long term anger causes
mental health problems including anxiety, depression, self-
harm, high blood pressure, coronary heart disease, colds and
flu, stroke, gastro-intestinal problems, and cancer [6]. The
Occupational Safety and Health Administration (OSHA)
reported that stress is a threat for the workplace. Stress costs
American industry more than $300 billion annually [6].
According to Dr. Alexander G. Justicz, in the 21st century,
stress is a huge problem for men [9]. Stress affects our health
negatively, causing headaches, stomach problems, sleep
problems, and migraines. Stress can cause many mouth
problems,
the
painful
TMJ
(temporomandibular
joint)
syndrome, and tooth loss [7]. “Stress has an immediate effect
on your body. In the short term, that’s not necessarily a bad
thing, but chronic stress puts your health at risk” [8]. Long
term and intense anger can be caused of mental health
problems including depression, anxiety and self-harm. It can
also be caused of "high blood pressure", "cold and flu",
"coronary heart disease", "stroke", "cancer" and "gastro-
intestinal problems"[13]. “If you have a destructive reaction
to anger, you are more likely to have heart attacks” [12]
whereas “an upward-spiral dynamic continually reinforces
the tie between positive emotions and physical health"[17].
Modern day lifestyle has led to various physical and
mental diseases such as diabetes, depression and heart
diseases as well. Although the negative effects of stress are
known to people, they choose (deliberately or otherwise) to
ignore it. They need to be forcefully notified, that they must
shrug off negative emotions; either by sending them calls or
some video clips/text messages/games [10]. Emotions are the
feelings which influence the human organs. According to
number of studies, negative thinking or depression can
adversely affect your health [19].
Probably automatic and
personal applications can be very helpful if it can monitor
one’s emotional states and persuade people to come out of
negative emotional states. According to William Atkinson;
“The best way to overcome undesirable or negative thoughts
and feelings is to cultivate the positive ones” [18]. Emotional
recognition technology can tackle this problem as it is able to
monitor an individual’s emotional states. This kind of system
can also send an alarming call to a person when he is in a
negative emotional state for long time or notify the
caregivers or family members. The system can also log an
individual’s emotional states for later analysis. In some
cases, especially in heart diseases, emotional states are also
required along with the physical activities and physiological
information for doctors in order to examine their patient's
conditions when he is away from the doctor's clinic [11].
Emotional computing is a field of human computer
interaction where a system has the ability to recognize
emotions and react accordingly. We want to develop a
131
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-470-1
eTELEMED 2016 : The Eighth International Conference on eHealth, Telemedicine, and Social Medicine (with DIGITAL HEALTHY LIVING 2016 / MATH 2016)

system for recognizing emotional states using physiological
sensors which should be able to identify a few emotional
states like sad, dislike, joy, stress, normal, no-idea, positive
and negative. In our research we want to prove that it is
possible to recognize the aforementioned emotional states by
using physiological sensors.
II.
RELATED WORK
Recognizing
emotional
states
by
using
automated
systems
have
increased
in
recent
years.
Researchers
developed systems for recognizing emotional states using
speech [23, 24, and 25], facial expressions [26, 27, and 28]
and physiological devices [20, 21, 22, 29, and 30]. In this
research, we want to recognize different emotional states
using body worn physiological devices (EMG, BVP, GSR
and temperature). Researchers used physiological devices in
order to recognize for different emotional states like sad [20,
21, 22, 30], joy/happy [20, 21, 22, 30, 31], normal/neutral
[21, 30, 31], negative [29] etc. However, the aforementioned
researches have used different physiological devices in their
work. For example; some researchers recognized emotional
states using EEG (Electroencephalogram), GSR and pulse
sensor and they recognized joy, anger, sad, fear and relax.
Audio and visual clips were used as a stimulus for eliciting
the emotions [20]. Some researchers recognized emotional
states using ECG (Electrocardiography) and they recognized
'Happiness', 'Sad', 'Fear', 'Surprise', 'Disgust' and 'Neutral'.
Audio and visual clips were used as a stimulus for eliciting
the emotions [21]. Some researchers recognized emotional
states using ECG, EMG, skin conductance, respiration
sensor and they recognized Joy, anger, Sadness and Pleasure.
Music songs were used as a stimulus for eliciting the
emotions [22]. In another case, researchers gathered the data
from
the
“blood
volume
pulse”,
“electromyogram”,
“respiration” and the “skin conductance sensor”. They
conducted 20 experiments in 20 consecutive days, testing
around 25 minutes per day on each individual. They figured
out neutral, anger, hate, grief, love, romantic, joy and
reverence emotion states from the data. They got 81%
classification accuracy among the eight states [31]. Different
techniques can be used as a stimulus for eliciting the
emotions i.e. pictures, video clips, audio clips, games etc. In
our work, we used International Affective Picture System
(IAPS) for stimulation. IAPS is widely used in experiments
studying emotion and attention. The International Affective
Picture System (IAPS) provides normative emotional stimuli
for emotion and attention under experimental investigations.
The IAPS (pronounced eye-aps) is being produced and
distributed by the Center for Emotion and Attention (CSEA)
at the University of Florida [32]. In our previous work, we
took two physiological sensors (i.e. BVP and GSR) for the
analysis, IAPS were used as a stimulus and our system was
able to recognize few emotional states with good accuracy
[44]. In this paper, we used four physiological sensors in
order recognize few emotional states. The above mentioned
researchers used different parts of the body but in our
research we used only left arm for the sensor placement.
III.
HYPOTHESIS
The physiological data measured by wearable devices
(EMG,
blood
volume
pulse,
temperature
and
skin
conductance sensor) indicate a person’s emotional state
(‘Sad’,
‘Dislike’,
‘Joy’,
‘Stress’,
‘Normal’,
‘No-Idea’,
‘Positive’
and
‘Negative’)
using
a
machine
learning
classifier.
IV.
EXPERIMENTAL METHODOLOGY
We developed the following systems for the user study.
A.
eHealth platform and application
We used eHealth platform [1] in order to recognize
emotional states (Figure 1) and connected Raspberry Pi [41]
to eHealth platform as shown in Figure 2.
Figure 1. eHealth platform
Figure 2. Raspberri pi with eHealth platform
The eHealth sensor comes with few sensors like 2D
Accelerometer sensor, Blood pressure sensor (Breathing),
Pulse and oxygen in blood sensor, body temperature sensor,
airflow
sensor,
Electrocardiogram
sensor
(ECG),
Electromyography
sensor
(EMG)
and
Galvanic
skin
response sensor. We used Galvanic skin response sensor,
body temperature sensor, Electromyography sensor (EMG)
and we used another blood volume pulse sensor [40] as
shown in Figure 3.
Figure 3. Pulse sensor
We connected ‘GSR, ‘EMG’, ‘BVP’ and ‘body temperature
sensor’ to the board. We wrote a piece of code which reads
the values from the aforementioned sensors and writes it to
a network port.
132
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-470-1
eTELEMED 2016 : The Eighth International Conference on eHealth, Telemedicine, and Social Medicine (with DIGITAL HEALTHY LIVING 2016 / MATH 2016)

B.
Application for reading sensors from eHealth platform
We wrote an application in Java which reads the sensed
data from a network port and stores it to a text file with a
timestamp in the following structure for post analysis.
Time_stamp|emg, bvp, gsr, temperature
C.
IAPS and its application (Application for Stimulus)
We got access to IAPS [32] images and these images are
already used by several researchers for emotional
computing [33,34,35,36,37,38,39]. We implemented an
application in C#.net that shows participants’ IAPS
images in a sequence in order to change participants’
emotional states and also states the starting and ending
time for each IAPS image during experiments. After
showing participants five different images from each
group, our application used to ask participants about
their current emotional state by using the Likert scale
(Figure 4 (b)) approach. We chose 100 IAPS images
from different categories and presented them in the
order shown in Figure 4(a).
Figure 4(a). Chosen IAPS images
The images were shown as a slide show with a timer of
5 seconds for each image. For the questionnaire we used
radio buttons and participants had to choose one emotional
state. It also stores the participants’ personal information
i.e. age, gender, height and weight.
Figure 4(b). Questionnaire form
D.
Experiment setup
Experiments were conducted in a calm room with
normal temperature; there was no noise or distraction. To
make sure the readings from GSR were accurate we asked
the participants to dry their hands with a dryer before
beginning with the experiment. Since GSR measures sweat
glands as well, moist hands would result in an erroneous
result. To ensure full concentration from the participants,
the light in the room was kept very low and we also asked
them to turn off their mobile phones during experiments.
Participants were asked to wear sensors on their left arms,
palms and fingers (Figure 5). They were also required to
perform the experiments twice; the first experiment was
useful in getting the participants to familiarize themselves
with the setup, while the second attempt was actually used
for analyzing their data.
Figure 5. Participant is wearing sensors
We recruited 26 participants (21 males, 5 females) for
our experiment setup; two of them could not complete the
experiments so we ended up with 24 participants (19 males,
5 females). The range of participants' age was from 20 to 44
(mean 26.17, SD 5.14) and ranged in BMI (body mass
index) from 18.7 to 26.6 (mean 21.44, SD 2.17).Participants
were required to do it twice in different days.
E.
First experiment
As described earlier, the intention behind the first
experiment was only familiarization with the setup. This
was done to accommodate all first time participants, as they
were somewhat nervous due to physiological devices and
long cables and this could adversely influence our data. For
this reason, the results from the first experiment were never
used for analysis.
133
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-470-1
eTELEMED 2016 : The Eighth International Conference on eHealth, Telemedicine, and Social Medicine (with DIGITAL HEALTHY LIVING 2016 / MATH 2016)

F.
Second experiment
In the second experiment, all participants already knew
about the setup and they were not hesitating with the
sensors, they performed the task with confidence and their
data was stored for later analysis. We used same settings for
both experiments but IAPS images were different. We
showed participants different images (IAPS) for changing
their emotions to sad, dislike, joy and stress. After showing
a set of images; our application used to show them the
questionnaire forms for their emotional states. Physiological
data was logged to a laptop with a time stamp and on the
other
hand
image
application
was
also
logging
the
participants' feedback to the same laptop with timestamp.
After that we merged both files to generate a single file for
post analysis.
V.
RESULTS AND ANALYSIS
Our experimental setup was able to change participants’
emotional states; only four of the participants chose all of the
given emotional states. This was due to the fact that it was
hard for the participants to distinguish between sad, dislike
and stress. Also being asked to distinguish between joy and
normal during experiments was not a straightforward task.
That also explains why some emotional states were ignored
by participants. “As everyone knows, emotions seem to be
interrelated in various but systematic ways: Excitement and
depression seem to be opposites; excitement and surprise
seem to be more similar to one another; and excitement and
joy seem to be highly similar, often indistinguishable” [42].
Therefore,
we
generated
another
dataset
from
our
experimental data; we categorized emotional states into two
collections:

Positive {Joy, Normal}

Negative {Sad, Dislike. Stress}; ‘No-Idea’ is
excluded
Now, we have the following types of datasets:
•
Type1: It contains {Normal, Sad, Dislike, Joy,
Stress and No-Idea}
•
Type2: It contains {Positive and Negative}
Due to the fact that it was a huge dataset, it was not possible
for WEKA [43] application to process the data of all 24
participants together. Therefore, we divided our datasets
into six groups, each group consisting the data of four
participants (as shown in Table 2); we grouped the four
participants who chose all emotional states together and put
them in Group-1, others were assigned to remaining groups
in alphabetic order.
TABLE I.
GROUPS
Groups
Age
Gender
Chosen Emotional states
Group 1
25, 24,
25, 26
3
Males, 1
Female
Normal(4), Sad(4),
Dislike(4), Joy(4), Stress(4)
and No-Idea(4)
Group 2
24,25,25,
38
4
Males
Normal(0), Sad(3),
Dislike(4), Joy(4), Stress(4)
and No-Idea(2)
Group 3
24,24,25,
44
3 Males, 1
Female
Normal(3), Sad(3),
Dislike(4), Joy(4), Stress(4)
and No-Idea(1)
Group 4
20,25,25,
33
2 Males,
2 Females
Normal(2), Sad(4),
Dislike(4), Joy(4), Stress(2)
and No-Idea(1)
Group 5
22,24,24,
25
3
Males, 1
Female
Normal(3), Sad(3),
Dislike(4), Joy(4), Stress(3)
and No-Idea(2)
Group 6
24,25,25,
27
4 Males
Normal(2), Sad(4),
Dislike(4), Joy(4), Stress(3)
and No-Idea(0)
We received values from sensors i.e. EMG, BVP, GSR and
Temperature where the sample rate was around 650Hz. We
analyzed both types (i.e. Type 1 and Type 2) in the
following three different ways:
A.
Individuals
We applied J48 classifier [43] on the dataset of each
participant.
B.
Group-wise
We divided the participants in 6 groups (as shown in
Table 2) and applied J48 classifier on the dataset of each
group.
C.
Portioned data
As mentioned earlier due to the limitations of processing
huge datasets in WEKA toolkit, we chose small portions of
data randomly pertaining to each emotion from each
participant in Figure 6(a) and Figure 6(b) below.
Figure 6(a). Type 1
Figure 6(b). Type 2
D.
Analysis structure
We got two types of data i.e. “Two-Class” and “Six-
class”; each type was analyzed on “Individual”, “Group” and
“Portioned” basis.
We applied J48 classifier with 10-fold
cross validation.
134
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-470-1
eTELEMED 2016 : The Eighth International Conference on eHealth, Telemedicine, and Social Medicine (with DIGITAL HEALTHY LIVING 2016 / MATH 2016)

E.
Two-Class
1)
Individuals: The outcome from the J48 classifier
represents the average data of 24 participants where it
correctly classified the instances with the accuracy of 99.4%;
Min: 97.72%; Max: 99.67% and SD: 0.45.
a b <-- classified as
2169370 24518 | a = Positive
18038 4896951 | b = Negative
We took the confusion matrices from all participants and
summed them all. Our results show the summation of all
confusion matrices and accuracy of each emotional state
where
'Positive'
and
'Negative'
emotional
states
were
predicted with the accuracy of 98.88% and 99.63% by J48
classifier respectively.
2)
Group wise: We took an average of correctly
classified instances from all groups in order to figure out the
variation amongst them. Our result shows that there is not a
high variation among the groups and the average result was
99.3%; Min: 99.06%; Max: 99.45%; SD: 0.14.
a
b <-- classified as
2181321
29401|
a = Positive
19447 4
804414|
b = Negative
We took confusion matrices from each group and summed
them up. Our results show the summation of all confusion
matrices from the groups and accuracies of emotional states
where positive and Negative emotional states were predicted
with the accuracy of 98.67% and 95.6% by J48 classifier
respectively.
3)
Portioned data: Our results show that J48 was able to
correctly classify the instances with the accuracy of 99.33%
and it was also able to predict positive and Negative
emotional states with the accuracy of 98.56% and 99.67%
respectively.
a
b <-- classified as
409236 5982 |
a = Positive
3095 934177 |
b = Negative
We compared the accuracy between the categories i.e.
‘Individual’, ‘Group’ and ‘Portioned’ as shown in Figure 7
which shows that there is not much difference in results
among them.
Figure 7. Comparison
F.
Six-Class
1)
Individuals: The outcome from the J48 classifier
represents the average data of 24 participants where it
correctly classified the instances with the accuracy of
99.13%; Min: 98.39%; Max: 99.52% and SD: 0.25.
a
b
c
d
e
f <-- classified as
1298734
7019
3629
2091
962
571 |
a = Sad
6760 2152047
5540
3829
2211
931 |
b = Dislike
4074
5775 1455566
3288
1008
524 |
c = Joy
2139
3890
2896 1329053
1092
467 |
d = Stress
915
2267
974
1120 734834
377 |
e = Normal
885
1214
602
502
450 341361 |
f = NoIdea
We took confusion matrices from all participants and
summed them up. Our results show the summation of all
confusion matrices and accuracy of each emotional state
where ‘Sad’, ‘Dislike’, ‘Joy’, ‘Stress’, ‘Normal’ and ‘No-
Idea’ emotional states were predicted with the accuracy of
98.99%, 99.11%, 99%, 99.22%, 99.24% and 98.94% by J48
classifier respectively.
2)
Group wise: We took an average of correctly
classified instances from all groups in order to figure out the
variation amongst them. Our result shows that there is not a
high variation among the groups and the average result was
98.67%; Min: 98.29%; Max: 99.04%; SD: 0.26.
a
b
c
d
e
f <-- classified as
1293196
9506
4521
3550
1500
733 |
a = Sad
8791 2144771
7418
5817
3248
1273 |
b = Dislike
5020
8403 1449781
4449
1733
849 |
c = Joy
3484
6295
4432 1323013
1747
566 |
d = Stress
1619
3706
1749
1924 730989
500 |
e = Normal
1039
1708
897
684
609
340077 |
f = NoIdea
We took confusion matrices from each group and summed
them up. Our results show the summation of all confusion
matrices from the groups and accuracies of emotional states
where ‘Sad’, ‘Dislike’, ‘Joy’, ‘Stress’, ‘Normal’ and ‘No-
Idea’ emotional states were predicted with the accuracy of
98.49%, 98.78%, 98.61%, 98.76%, 98.72% and 98.57% by
J48 classifier respectively.
3)
Portioned data: Our results show that J48 was able to
correctly classify the instances with the accuracy of 98.47%
and it was also able to predict ‘Sad’, ‘Dislike’, ‘Joy’,
‘Stress’, ‘Normal’ and ‘No-Idea’ emotional states with the
accuracy of 98.24%, 98.75%, 98.41%, 98.34%, 98.62% and
97.99% respectively.
a
b
c
d
e
f <-- classified as
244303 2123 1016
800
278
168 |
a = Sad
1571 428263 1594 1481
507
288 |
b = Dislike
983 1977 275773 1020
303
162 |
c = Joy
913 1762 1142 250641
252
170 |
d = Stress
328
706
413
307 133139
107 |
e = Normal
210
509
241
199
130
62872 |
f = NoIdea
Figure 8. Comparison
We also compared the accuracy between the categories
i.e. ‘Individual’, ‘Group’ and ‘Portioned’ as shown in Figure
8 which shows that there is not much difference in results
among them.
135
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-470-1
eTELEMED 2016 : The Eighth International Conference on eHealth, Telemedicine, and Social Medicine (with DIGITAL HEALTHY LIVING 2016 / MATH 2016)

VI.
CONCLUSIONS AND FUTURE WORK
We used the following approaches for analyzing the data
1.
We took data of each participant and applied J48
classifier and then took an average of ‘Individual’
data.
2.
We took integrated data from six participants,
applied J48 classifier and then took an average of
‘Group’ data.
3.
We took a small portion of data randomly from
each participant and applied J48 classifier on the
data
We categorized data into the following collections:
•
Six emotional states i.e. ‘Sad’, ‘Dislike’, ‘Joy’,
‘Stress’, ‘Normal’ and ‘No-Idea’.
•
Two emotional states i.e. ‘Positive’ and ‘Negative’.
Our system was able to recognize the aforementioned
emotional states by using physiological devices and J48
(decision tree) classifier with high accuracy. Results have
shown that few physiological devices are enough for
recognizing required emotional states (‘Sad’, ‘Dislike’,
‘Joy’,
‘Stress’,
‘Normal’,
‘No-Idea’,
‘Positive’
and
‘Negative’). This prototype is only a "proof of concept" and
our results show that our approach can identify the above
mentioned emotional states independent of BMI (body mass
index) and age group. The physiological sensor has to be
fixed properly on the participants’ skin in order to predict
their emotional states successfully. We will conduct more
user studies where we will use physiological data and facial
expressions for recognizing these emotional states.
REFERENCES
[1]
Cooking Hacks: e-Health Sensor Platform V2.0 for Arduino and
Raspberry
Pi
[Biometric
/
Medical
Applications].
http://www.cooking-
hacks.com/documentation/tutorials/ehealth-biometric-sensor-
platform-arduino-raspberry-pi-medical#step4_9. [Last Accessed
24th March, 2016 ].
[2]
Professor Rosalind W. Picard.
http://web.media.mit.edu/~picard/index.php. Last Accessed 24th
March, 2016 ].
[3]
Affective Computing: Publications.
http://affect.media.mit.edu/publications.php .Last Accessed 24th
March, 2016 ].
[4]
Richmond Hypnosis Center.
http://richmondhypnosiscenter.com/2013/04/12/sample-post-
two/. Last Accessed 24th March, 2016 ].
[5]
American Psychological Association: The Impact of Stress.
http://www.apa.org/news/press/releases/stress/2011/impact.aspx
. Last Accessed 24th March, 2016 ].
[6]
WebMD: Stress Management Health Center.
http://www.webmd.com/balance/stress-management/effects-of-
stress-on-your-body.Last Accessed 24th March, 2016 ].
[7]
Krifka, S., Spagnuolo, G., Schmalz, G., Schweikl, H. A review
of adaptive mechanisms in cell responses towards oxidative
stress caused by dental resin monomers. Biomaterials.
2013;34:4555–4563.
[8]
Healthline : The Effects of stress on the Body.
http://www.healthline.com/health/stress/effects-on-body. Last
Accessed 24th March, 2016 ].
[9]
Miamiherald: Chronic stress is linked to the six leading causes
of death.
http://www.miamiherald.com/living/article1961770.html. Last
Accessed 24th March, 2016 ].
[10] Online Stress Reliever Games.
http://stress.lovetoknow.com/Online_Stress_Reliever_Games.
Last Accessed 24th March, 2016 ].
[11] Ali Mahmood Khan. (2011). Personal state and emotion
monitoring by wearable computing and machine learning. BCS-
HCI 2012, Newcastle, UK
[12] WebMD: Stress Management Health Center.
http://www.webmd.com/balance/stress-
management/features/how-anger-hurts-your-heart. Last
Accessed 24th March, 2016 ].
[13] BetterHealth: Anger - how it affects people.
http://www.betterhealth.vic.gov.au/bhcv2/bhcarticles.nsf/pages/
Anger_how_it_affects_people. Last Accessed 24th March, 2016
].
[14] Ekman, Paul (1999), "Basic Emotions", in Dalgleish, T; Power,
M, Handbook of Cognition and Emotion (PDF), Sussex, UK:
John Wiley & Sons.
[15] Health and Wellness: Are Happy People Healthier? New
Reasons to Stay Positive. http://www.oprah.com/health/How-
Your-Emotions-Affect-Your-Health-and-Immune-System.Last
Accessed 24th March, 2016 ].
[16] Darwin, C. (1872). The expression of the emotions in man and
animals. John Murray, London
[17] Kok, B.E., Coffey, K.A., Cohn, M.A., Catalino, L.I.,
Vacharkulksemsuk, T., Algoe, S., Brantley, M. & Fredrickson,
B. L. (2013). How positive emotions build physical health:
Perceived positive social connections account for the upward
spiral between positive emotions and vagal tone. Psychological
Science, 24(7), 1123-1132.
[18] Atkinson, William Walker (1908). Thought Vibration or the
Law of Attraction in the Thought World.
[19] Rush, A. J., Beck, A. T., Kovacs, M. & Hollon, S. D.
Comparative efficacy of cognitive therapy and pharmacotherapy
in the treatment of depressed outpatients. Cognit. Ther. Res. 1,
17-38.
[20] Remarks on Emotion Recognition from Bio-Potential Signals:
research paper (2004)
[21] Emotion Recognition from Electrocardiogram Signals using
Hilbert Huang Transform (2012)
[22] Emotion Pattern Recognition Using Physiological Signals
(2014)
[23] Walter Sendlmeier Felix Burkhardt, Verification of Acoustical
Correlates of Emotional Speech using Formant-Synthesis,
Technical University of Berlin, Germany.
[24] RECOGNIZING EMOTION IN SPEECH; Frank Dellaert,
Thomas Polzin and Alex Waibel
[25] RECOGNIZING EMOTION IN SPEECH USING NEURAL
NETWORKS; Keshi Dai, Harriet J. Fell and Joel MacAuslan
[26] Recognizing Emotion From Facial Expressions: Psychological
and Neurological Mechanisms Ralph Adolphs, University of
Iowa College of Medicine.
[27] Analysis of emotion recognition using facial expressions,
speech and multimodal information. Proceedings of the 6th
International Conference on Multimodal Interfaces, ICMI 2004,
State College, PA, USA, October 13-15, 2004
[28] Recognizing Emotions from Facial Expressions Using Neural
Network; Isidoros Perikos, Epaminondas Ziakopoulos, Ioannis
Hatzilygeroudis.
[29] Emotions
States
Recognition
Based
on
Physiological
Parameters
by
Employing
of
Fuzzy-Adaptive
Resonance
Theory; Mahdis Monajati, Seyed Hamidreza Abbasi, Fereidoon
Shabaninia, Sina Shamekhi.
[30] Classification
of
emotional
states
from
electrocardiogram
signals: a non-linear approach based on hurst; Jerritta Selvaraj,
Murugappan
Murugappan,
Khairunizam
Wan
and
Sazali
Yaacob
136
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-470-1
eTELEMED 2016 : The Eighth International Conference on eHealth, Telemedicine, and Social Medicine (with DIGITAL HEALTHY LIVING 2016 / MATH 2016)

[31] Jennifer Healey and Rosalind W. Picard (2002), Eight-emotion
Sentics
Data,
MIT
Affective
Computing
Group,
http://affect.media.mit.edu."
[32] THE CENTER FOR THE STUDY OF EMOTION AND
ATTENTION:
http://csea.phhp.ufl.edu/Media.html#topmedia.
Last Accessed 24th March, 2016 ].
[33] Cuthbert BN, Schupp HT, Bradley MM, Birbaumer N, Lang
PJ.; Brain potentials in affective picture processing: covariation
with autonomic arousal and affective report. Biol Psychol. 2000
Mar; 52(2): 95-111.
[34] Keil A, Bradley MM, Hauk O, Rockstroh B, Elbert T, Lang PJ.;
Large-scale
neural
correlates
of
affective
picture
processing.;Psychophysiology. 2002 Sep; 39(5):641-9.
[35] Lang PJ, Bradley MM, Fitzsimmons JR, Cuthbert BN, Scott JD,
Moulder B, Nangia V.; Emotional arousal and activation of the
visual cortex: an fMRI analysis. Psychophysiology. 1998 Mar;
35(2):199-210.
[36] Bradley MM1, Sabatinelli D, Lang PJ, Fitzsimmons JR, King
W, Desai P.; Activation of the visual cortex in motivated
attention.; Behav Neurosci. 2003 Apr; 117(2):369-80.
[37] Sabatinelli D, Bradley MM, Fitzsimmons JR, Lang PJ.; Parallel
amygdala
and
inferotemporal
activation
reflect
emotional
intensity and fear relevance.; Neuroimage. 2005 Feb 15;24
(4):1265-70. Epub 2005 Jan 7.
[38] Sabatinelli D, Lang PJ, Keil A, Bradley MM; Emotional
perception: correlation of functional MRI and event-related
potentials; Cereb Cortex. 2007 May; 17(5):1085-91. Epub 2006
Jun 12.
[39] Bradley MM, Codispoti M, Lang PJ.; A multi-process account
of
startle
modulation
during
affective
perception;
Psychophysiology. 2006 Sep; 43(5):486-97.
[40] Maker
Shed:
Pulse
Sensor
AMPED
for
Arduino.
http://www.makershed.com/products/pulse-sensor-amped-for-
arduino .Last Accessed 24th March, 2016 ].
[41] Raspberry Pi. https://www.raspberrypi.org/ .Last Accessed 24th
March, 2016 ].
[42] Russel and Bullock, Multidimensional scaling of emotional
facial expressions, Journal of Personality and Social Psychology
(1985), pp. 1290-1298
[43] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer,
Peter Reutemann, Ian H. Witten (2009); The WEKA Data
Mining Software: An Update; SIGKDD Explorations, Volume
11, Issue 1
[44] Ali Mehmood Khan, Michael Lawo 2016, "Recognizing
Emotion from Blood Volume Pulse and Skin Conductance
Sensor Using Machine Learning Algorithms", MEDICON 2016,
Paphos-Cyprus
137
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-470-1
eTELEMED 2016 : The Eighth International Conference on eHealth, Telemedicine, and Social Medicine (with DIGITAL HEALTHY LIVING 2016 / MATH 2016)

