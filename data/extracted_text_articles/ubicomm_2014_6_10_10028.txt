Compact Three-dimensional Vision for Ubiquitous Sensing 
 
Kumiko Yoshida  
Interdisciplinary Graduate School of Agriculture and 
Engineering 
University of Miyazaki 
Miyazaki, Japan 
E-mail: nc13004@student.miyazaki-u.ac.jp 
Kikuhito Kawasue 
Department of Environmental Robotics 
University of Miyazaki 
  
Miyazaki, Japan 
E-mail: kawasue@cc.miyazaki-u.ac.jp
 
 
Abstract—We herein propose two computer vision systems 
that make use of the Microsoft KINECT sensor for ubiquitous 
sensing in raising stock and in industrial fields. The first system 
is a three-dimensional (3D) thermo-sensing system that detects 
3D shape data and 3D temperature data simultaneously. These 
data are automatically combined, and the 3D shape and 
temperature distribution are reconstructed on the computer. 
The second system is a handheld 3D measurement system that 
uses a slit-ray projector in conjunction with the KINECT 
sensor. The 3D shape of the target is reconstructed on a 
computer using the detected data. These two systems are 
sufficiently compact and the measurement can be performed 
via online processing. As such, these systems will be useful in 
ubiquitous data acquisition systems in various fields. Typical 
applications of the proposed systems include environment 
sampling, health monitoring of animals, monitoring of facilities 
in raising stock, and industrial fields. The experimental results 
of the present study demonstrate the feasibility of the proposed 
system. 
 
Keywords-computer vision; KINECT; thermo-sensing; three-
dimensional sensing. 
I. 
 INTRODUCTION  
In realizing a ubiquitous society, the development of a 
compact system that can detect various data at a site is 
effective. Once they have been detected digitally, the data 
can be distributed through a network and can be used 
effectively. Recently, Charge coupled device (CCD) cameras 
have been made more compact and have been incorporated 
into a number of mobile phones. Indeed, mobile phones 
containing CCD cameras are considered to be the most 
familiar data acquisition system. Although CCD cameras 
capture primarily two-dimensional image data, three-
dimensional (3D) data (point cloud) are required for various 
applications.  
A point cloud is a set of vertices in a 3D coordinate 
system. Point clouds are used in Computer-Aided Design 
(CAD) data and robot vision systems. In recent years, 
inexpensive devices, such as the Microsoft KINECT sensor 
[1]-[4], which detects 3D point cloud data, have become 
available. The KINECT sensor is composed of a random dot 
projector and an Infrared (IR) camera. Random dots are 
projected from the laser projector, and the reflected light 
from the surface of objects is recorded by the IR camera. The 
dots recorded by the IR camera are triangulated in order to 
calculate the 3D position of an object based on the 
configuration of the laser projector and the IR camera. Such 
devices are useful for motion capture or modeling systems, 
which do not require high accuracy. Recently, The KINECT 
sensor has also been used in ubiquitous computing [5]-[8]. 
In using the KINECT sensor, there is no limitation on the 
measurement area size because individual 3D point cloud 
data sets recorded from different positions can be combined. 
The Iterative Closest Point (ICP) algorithm [9]-[13] is often 
used to combine data sets. This algorithm automatically 
determines the overlapping area between 3D point cloud data 
sets and constructs a single 3D image. The KINECT sensor 
can obtain thousands of point cloud data sets in real time and 
so is a very attractive sensor. We herein introduce two 
applications using the KINECT sensor. 
In 
recent 
years, 
thermal 
imaging 
measurement 
technology has developed rapidly. Infrared thermography 
has been used in animal sciences. Non-destructive evaluation 
in raising stock or in animal research is one example of such 
an approach. However, since images used in such 
evaluations 
are 
two-dimensional 
thermal 
images, 
quantitative information such as the area, the 3D shape, and 
the roughness of the heat source, cannot be obtained. In order 
to obtain the quantitative information, we herein propose a 
measurement system that uses the KINECT sensor in 
conjunction with thermography to produce 3D shapes and 
3D thermo-grams. Periodically evaluating the condition of 
Japanese black cattle during the growth process is important 
[14]. In addition to the weight and size of cattle, the body 
temperature should be measured as primary evaluation 
criteria. Quantitative measurement of 3D temperature and 
shape can be established using the proposed system. The 
proposed system can be a useful tool for monitoring the 
condition of cattle in breeding farms. 
Another application of the proposed system is the 
measurement of facilities in industrial fields, such as the 
chemical industry. However, the original data obtained by 
the KINECT sensor is not sufficiently accurate for industrial 
applications. On the other hand, the slit-ray projection 
method [15]-[17] (i.e., shape from structured light), which 
has high measurement accuracy, is widely used in industrial 
applications and robot vision systems. In this method, a laser 
slit is projected onto the surface of the target object and the 
laser streak generated on the surface is detected by a CCD 
157
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-353-7
UBICOMM 2014 : The Eighth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

camera. The 3D position data of the laser slit is estimated by 
triangulating the orientation of the laser projector and the 
CCD camera [18][19]. In the present study, we introduce a 
point cloud data acquisition system that uses slit ray 
projection and a KINECT sensor. The proposed system is 
sufficiently compact to allow its use as a hand-held device. 
During measurement, the user directs the laser slit ray at the 
target. The KINECT sensor then detects point cloud data 
while the CCD camera simultaneously detects the laser 
streak generated on the surface of the target. The user 
manually scans the system by directing the laser slit ray 
along the measurement pipe. The point cloud data obtained 
using the KINECT sensor is used to determine the movement 
of the system by adjusting overlapping data in consecutive 
frames using the ICP algorithm. The data obtained by the 
laser slit-ray projection method are more accurate than the 
original point cloud data obtained by the KINECT sensor 
because the data are obtained using a high-resolution camera. 
The pipe cross section is estimated based on data obtained by 
the slit-ray projection method. The 3D shape of the pipe is 
constructed on a computer from cross sections obtained from 
different positions. The proposed system enables onsite 
measurement in chemical plants and can be used in obtaining 
information on the current condition inside the plant, which 
can be used in plant evaluation and maintenance. 
Two systems that use the KINECT sensor are introduced 
in the present paper to use as effective tools for realizing a 
ubiquitous society. The remainder of the present paper is 
organized as follows. Section 2 describes the proposed 3D 
thermo-sensing system using the KINECT sensor, and 
Section 3 describes the application of the proposed system. 
Section 4 describes a pipe measurement system that uses the 
slit-ray projection method in conjunction with the KINECT 
sensor. Section 5 describes the pipe reconstruction results 
obtained using the proposed system. Finally, the paper is 
concluded in Section 6. 
II. 
THERMO-SENSING SYSTEM USING THE KINECT 
SENSOR 
The measurement system is shown in Figure 1. The 3D 
thermal-sensing 
system 
is 
established 
using 
a 
3D 
measurement sensor (KINECT sensor) and a thermograph 
(NEC, 320×240, 60 Hz). A flowchart of the measurement 
procedure is shown in Figure 2. The 3D shape measurement 
is performed using the KINECT sensor, while the 
thermograph simultaneously captures a thermal image. The 
corresponding temperature data obtained by thermography is 
then allocated to the reconstructed surface shape of the 
object on a computer. This allocation is established using a 
pre-determined conversion matrix, which is determined 
during the calibration process. 
Calibration is important in determining the coordinate 
conversion between global coordinates and thermography 
coordinates. Generally, the calibration process in a 3D 
measurement system is complicated and not universal. In 
the proposed system, the thermography coordinates have to 
be determined precisely in the calibration process because 
they influence the temperature mapping accuracy onto the 
surface of the object. A suitable calibration method for 
thermography is proposed.  
 
            
            
Figure 1. Measurement system. 
 
 
Figure 2. Measurement procedure. 
A standard cube is used to match the coordinates among 
the global coordinate system and the thermography 
coordinate system, because a thermal image can be recorded 
by thermography. The calibration setup is shown in Figure 3. 
The 3D shape of the standard cube is measured using the 
KINECT sensor, and the thermal image of the standard cube 
must be recorded simultaneously by thermography because 
the recorded thermal image is used to determine the 
calibration parameters.  
The relationship between the thermography coordinates 
(u, v) and the global coordinates (x, y, z) of a point is as 
follows: 
                                            
11
12
13
14
21
22
23
24
31
32
33
1
1
1
x
u
k
k
k
k
y
s v
k
k
k
k
z
k
k
k
 
 

  
 

  


 

  
 

  
 

  
                     (1) 
                                                  
where k11 through k33 are parameters that consider the 
rotation, scale, and displacement between the thermography 
coordinates and the global coordinates. These parameters 
are the elements of conversion matrices and are determined 
by inputting some corresponding positions between the 
thermography coordinates and the global coordinates. Here, 
k11 through k33 can be determined by inputting no less than 
Start of 
KINECT 
Start of  
thermography device 
3D Measurement 
of the object 
Allocating the corresponding temperature data 
to 3D shape of the object 
Capturing the thermal-
image of the object 
KINECT 
Thermography 
158
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-353-7
UBICOMM 2014 : The Eighth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

six corresponding points into Eq. (1).  
 
 
Figure 3. Calibration setup. 
 
Equation (1) can be rewritten as follows:  
                                 
11
12
13
14
31
32
33
21
22
23
24
31
32
33
(
) / (
1)
(
) / (
1)
u
k x
k y
k z
k
k x
k y
k z
v
k x
k y
k z
k
k x
k y
k z








 







    (2) 
                                           
Once k11 to k33 are determined, the thermography 
coordinates can be calculated using Eq. (2), and the 
corresponding temperatures can also be mapped on the 
reconstructed surfaces of the object.  
Based on the presented mathematical model, it is 
possible to determine the parameters that contain the 
position and posture of the IR camera. Once the 3D shape of 
the object is measured, the corresponding temperature is 
mapped from the thermal-image using Eq. (2). 
III. 
APPLICATION TO MEASURE THE TEMPERATURE IN 
CATTLE 
Periodically evaluating the condition of Japanese black 
cattle during the growth process is important. The weight, 
size, posture, body shape, and body temperature are 
measured as the primary evaluation criteria.  
 
 
Figure 4. Two-dimensional thermal image. 
Figure 5. Point cloud data of the face of a cow.  
 
       
 
 
Figure 6. Constructed 3D thermal image. 
 
A computer vision device can be a useful tool for 
evaluating the condition of cattle. Therefore, we used 3D 
thermo-sensing to measure the body shape and temperature 
of cattle.  
Figure 4 shows a two-dimensional thermal image 
recorded by thermography. The image in this figure contains 
the temperature distribution of a cow's head but not 
quantitative information such as the dimensions of the cow's 
head. Figure 5 shows the point cloud data detected by the 
KINECT sensor. The temperature data contained in the 
KINECT Laser projector 
Global coordinates  
Thermography coordinates 
A corresponding 
point P 
X 
Y 
Z 
u 
v 
IR camera 
Standard cube
Top view 
Side view 
Top view 
Side view 
159
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-353-7
UBICOMM 2014 : The Eighth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

thermal image is mapped to point cloud data. Figure 6 
shows the constructed 3D thermal image that was calculated 
using the proposed method. Since this result contains the 3D 
quantitative shape of the cattle along with temperature 
information, the information is useful for animal science, for 
example, in budget calculation. 
 
IV. 
APPLICATION FOR PIPE MEASUREMENT 
In the chemical plant, the shapes and the arrangement of 
existing pipes should be investigated before the replacement 
or the construction of new pipes. Generally, measurement is 
conducted manually using a metal tape measure and is a 
cumbersome task. Recently, A number of laser scanners 
have been introduced for use in the measurement of pipes. 
The laser scanner is a very attractive option because 
thousands of sets of point cloud data can be obtained in a 
short time. However, the volume of data obtained is very 
large, because the data includes unnecessary information 
such as ground data, wall data, and data related to other 
equipment. Extra work is required in order to extract the 
appropriate information from the huge volume of point cloud 
data for the design of new pipes. Therefore, the development 
of an effective method is required in order to extract the 
desired information from the point cloud data. In addition, 
certain pipes that are located at high positions that the laser 
cannot reach are not measured, because the laser scanner is 
fixed on stable ground. 
Figure 7 shows the measurement system. A high-
resolution CCD camera (resolution: 3,488×2,616) is 
attached to a Microsoft KINECT sensor. A laser slit 
projector (20 mW), which must be held within 300 to 500 
mm from the CCD camera, is also attached to the 
measurement system. During the piping measurement, the 
user directs the laser slit ray at the target. The KINECT 
sensor then detects point cloud data, while the CCD camera 
simultaneously detects the laser streak generated on the pipe 
surface. The cross-sectional shape is estimated from the 
image of the laser streak obtained by applying the slit-ray 
projection method (i.e., shape obtained from structured 
light). The user manually scans this system by directing the 
laser slit ray along the pipe. The movement data (i.e., the 
amount and direction of movement) are estimated from the 
point cloud data detected by the KINECT sensor. The 3D 
pipe shape can be constructed on the computer from two or 
more sets of cross-sectional shape data obtained from the 
laser streak. Figure 8 shows a flowchart of the measurement 
procedure. The measurement system is first directed at area 
1 on the target. The KINECT sensor and the laser slit 
measurement system are synchronized so that they are not 
affected by the movement of the system. The user can 
change the position of the measurement system by directing 
the system at the second measurement area (area 2). The 
KINECT sensor and the laser slit measurement system then 
detect data from this area. The point cloud data must include 
data from the overlap between areas 1 and 2 in order to 
estimate the movement of the system. Movement data (i.e., 
the amount of movement and the orientation of the 
measurement system) are estimated using the ICP algorithm 
[5]-[9]. The point cloud data is combined and a single pipe 
is constructed on the computer. The orientation of the cross 
section measured using the laser slit can also be determined 
by allocating the cross-sectional data of the constructed pipe. 
Thus, the shape of the entire pipe can be estimated from two 
or more cross-sectional data sets. 
 
 
Figure 7. System for piping measurement. 
 
Figure  8. Flowchart of the measurement procedure. 
KINECT with 
high resolution  
camera 
Laser slit 
Measurement 
pipe 
160
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-353-7
UBICOMM 2014 : The Eighth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

Figure 9 shows the calibration setup of the high-
resolution CCD camera. A calibration board is installed on 
the laser slit plane. An image of the scale on the calibration 
board is obtained by the CCD camera and is used to 
determine the calibration parameters. The relationship 
between the coordinates (u,v) of the CCD camera and the 
global coordinates (x,y) of the scale board is as follows: 
 
           































1
1
1
32
31
23
22
21
13
12
11
y
x
k
k
k
k
k
k
k
k
v
u
s
                               (3)      
              
where k11 through k32 are parameters that express the 
rotation, scale, and displacement between the camera 
coordinates and global coordinates. The global coordinate 
system is based on the measurement system and moves with 
the system. These parameters are determined by inputting 
corresponding positions between the camera coordinates 
and the global coordinates. These parameters are input using 
a mouse for the camera coordinates and a keyboard for the 
corresponding global coordinates.  
 
Parameters k11 through k32 are determined by inputting four 
or more corresponding points into Eq. (3). The function for 
converting camera coordinates to global coordinates is given 
as follows: 
 
 
 
























v
k
u
k
k
v
k
k
v
k
k
u
k
k
u
k
y
x
23
13
1
22
32
21
31
12
32
11
31
            (4)       
                                              
All points on the laser streak are converted to global 
coordinates, and the cross-sectional shape of the pipe is 
estimated. 
 
Figure 9. Calibration setup. 
 
Figures 10(a) and 10(b) show images of the laser streak 
on the pipe captured by the CCD camera and the KINECT 
sensor, respectively. The laser streak detected by the CCD 
camera is clearer and more stable than the point cloud data 
obtained using the KINECT sensor. The resolution of the 
KINECT camera is 640×480, whereas the resolution of the 
CCD used in the present is 1,280×1,024.  
The difference in the resolutions influences the 
measurement accuracy. The laser slit data in the point cloud 
data is replaced by the data of the slit-ray projection method.  
 
 
       
                            
                 
(a) CCD camera image     (b) KINECT image 
Figure 10. Point cloud data obtained by the CCD camera and the KINECT 
sensor. 
 
                                      
Figure 11. Estimated pipe cross section. 
 
 
Figure 12. Point cloud data sets captured from different positions. 
 
    
Figure 13. Connection of two point cloud data sets. 
   
The two-dimensional coordinates (x,y) of Eq. (4) are 
mapped to 3D coordinates (x,y,z) by this replacement 
because the point cloud data have 3D coordinates. 
The cross-sectional shape (ellipse) can be determined by 
applying the least-squares method to the data in Figure 10(a), 
as shown in Figure 11. 
More than two point cloud data sets were obtained at 
different positions, as is shown in Figure 12. Each data set 
contains data that overlaps with data from another data set, 
as indicated by the shaded regions in Figure 12. These point 
cloud data sets are connected using the ICP algorithm as 
shown in Figure 13. The ICP algorithm of the Point Cloud 
Library (PCL) open-source framework is used in the 
proposed system. 
 
 
Laser slit 
Calibration board 
161
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-353-7
UBICOMM 2014 : The Eighth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

V. 
RESULTS OF PIPE RECONSTRUCTION 
The position and orientation of the estimated cross-
section between images are then determined, and the pipe is 
constructed by the computer, as shown in Figure 14. In 
other words, the coordinates for each measurement position 
are converted to a common coordinate system (global 
coordinate system) by this connection. The proposed 
method can thus construct the shape of the entire pipe in a 
common coordinate system from point cloud data from 
different portions on the pipe. 
 
                         
Figure 14. Pipe reconstructed on a computer. 
 
Figure 15 shows a photograph of the pipe measured in 
the present experiment. The original point cloud data 
contains a great deal of redundant data. Six images were 
captured from different positions and were used to construct 
point cloud data on the computer. Each image includes a 
single slit-ray streak.  
  
  
 
 
Figure 15. Photograph of the measured pipe. 
 
 
         
 
 
                 Figure 16. Pipe reconstructed on the computer.  
 
Three pipes are reconstructed in Figure 16. Several sections 
are used to reconstruct each pipe. The measurement error in 
measuring the pipe size is less than 2 mm for distances 
under 2,000 mm and 0.3% over distances over 2,000 mm. 
 
VI. 
CONCLUSION 
Two computer vision systems that used the KINECT 
sensor for ubiquitous sensing in raising stock and in 
industrial fields were introduced. One of these systems is a 
3D thermo-sensing system that detects 3D shape data and 3D 
temperature data simultaneously. Measurement of the 3D 
temperature distribution was realized by mapping thermal 
data obtained by thermography to the 3D position obtained 
by the KINECT sensor. The 3D temperature distribution of 
the cow head is measured for one application of the proposed 
system.  
The other system is a handheld 3D measurement 
apparatus that uses a slit ray projector in conjunction with the 
KINECT sensor. The 3D shape of the target is reconstructed 
using the detected data on the computer. The measurement 
of pipe equipment is introduced in the present paper.   
These two systems are sufficiently compact, and 
measurement can be performed via online processing. As 
such, these systems can be useful as tools for ubiquitous data 
acquisition systems in various fields. The experimental 
results obtained in the present study demonstrate the 
feasibility of the proposed system in raising stock and in 
industrial applications. 
 
REFERENCES 
[1] J. Mahoney,  "Testing the goods: Xbox KINECT," 2010 
[2] A. Bigdelou, T. Benz, L. Schwarz, and N. Navab, 
"Simultaneous categorical and spatio- temporal 3d gestures 
using kinect,"  Proc. 3D User Interface, 2012, pp. 53-60. 
[3] C. Mutto, P. Zanuttigh, and G. Cortelazzo, "Time-of-Flight 
Cameras and Microsoft KINECT," Springer, 2012. 
[4] Z. Zhang, "Microsoft kinect sensor and its effect," IEEE 
multimedia, vol. 19, 2012, pp. 4-10. 
[5] D. Webster and O. Celik, "Experimental evaluation of 
Microsoft Kinect's accuracy and capture rate for stroke 
rehabilitation applications," Proc. Haptics Symposium IEEE, 
Feb. 2014, pp. 455-460. 
162
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-353-7
UBICOMM 2014 : The Eighth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

[6] R. Nagayama, T. Kazuma, T. Endo, and A. He, "A basic 
study of human face direction estimation using depth sensor," 
Proc. International Joint Conference on Awareness science 
and technology and Ubi-Media Computing, Nov. 2013, pp. 
644-648. 
[7] I. Lysenkov and V. Rabaud, "Pose estimation of rigid 
transparent objects in transparent clutter," Proc. IEEE 
international conference on Robotics and Automation, May. 
2013, pp. 162-169. 
[8] D. Surie, S. Partonia, and H. Lindgren, "Human sensing using 
computer vision for personalized smart spaces," Proc. IEEE 
International Conference on Ubiquitous Intelligence and 
Computing, Nov. 2013, pp. 487-494. 
[9] C. Kapoutsis, C. Vavoulidis, and I. Pitas, "Morphological 
techniques in the iterative closest point algorithm," IEEE 
Trans. on Image Processing, vol. 8, 1998, pp. 808-812. 
[10] G. Sharp, S.  Lee, and D. Wehe, "Icp registration using 
invariant features,"  IEEE Trans. on Pattern Analysis and 
Machine Intelligence, vol. 24 , 2002, pp. 90-102. 
[11] J. Feldmar, J. Declerck, G. Malandain, and N. Ayache, 
"Extension of the icp algorithm to nonrigid intensity-based 
registration of 3d volumes," Computer Vision and Image 
Understanding, vol. 66, 1997, pp. 193-206. 
[12]  B. Lee, C. Kim, and R. Park, "An orientation reliability matrix 
for the iterative closest point algorithm," IEEE Trans. on 
Pattern Analysis and Machine Intelligence, vol. 22, 2000, pp. 
1205-1208. 
[13] S. Gupta, K. Sengupta, and A. Kassim, "Compression of 
dynamic 3d geometry data using iterative closest point 
algorithm," Computer Vision and Image Understanding, vol. 
87, 2002, pp. 116-130. 
[14] K. Kawasue, T. Ikeda, T. Tokunaga, and H. Harada, "Three-
dimensional shape measurement system for black cattle using 
KINECT sensor," International journal of circuit and Signal 
processing, Issue 4, vol. 7, 2013, pp. 222-230. 
[15] Y. Wang, "Characterizing three-dimensional surface structure 
from visual images," IEEE Trans. on Pattern Analysis and 
Machine Intelligence, vol. 13, 1991, pp. 52-60. 
[16] D. Bhatnagar, A. Pujari, and P. Seetharamulu, "Static scene 
analysis using structured light,"  Image and Vision Computing, 
vol. 9 , 1991, pp. 82-87. 
[17] J. Aldon and O. Strauss, "Shape decomposition using 
structured light vision," Visual Form, 1992, pp. 11-20. 
[18] K. Kawasue, G. Uezono, Y. Gejima, and M. Nagata, “Three-
dimensional Measurement by Free Scanning of a CCD 
Camera and Laser,” WSEAS transactions on System, vol. 3, 
2004, pp. 143-147. 
[19] K. Kawasue, T. Komatsu, and K. Yoshida, "Handheld three-
dimensional pipe measurement system with a slit-ray 
projector," Proc. of SPIE vol. 8768, 2012.12, pp. 1-5. 
 
 
163
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-353-7
UBICOMM 2014 : The Eighth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

