A Study on Visualization of User Reviews
Eric Yang
Department of Digital Media Design
National Yunlin University of Science and Technology
Yunlin, Taiwan
E-mail: yangyueric@icloud.com
Abstract—User reviews is a research method with increasing
application in the application (‘app’) industry in recent years
because it allows development teams to better understand
users’ needs and ideas through user review analysis. However,
deciding how to interpret the massive daily incoming user
review data and how to sort them into useful information is a
challenge that development teams must address. This study
combined the architecture of an app with the excellent
information
communication
capability
of
information
visualisation and compiled a priority list of version releases
based on function areas mentioned in user reviews, allowing
development teams to create strategies for updates and
emergency resolution, as well as designing apps to meet users’
expectations, and to therefore achieve better user experience.
Subsequent to a heuristic evaluation, the three most important
suggestions were made: 1. Improve the enrichment of the
visualisation chart; 2. Increase follow-up evaluation; and 3.
Understand the reasons and context that lead users to like
apps. Experts that participated in the evaluation believed that
the
visualisation
chart
generated
in
this
study
allows
developers to better understand users’ real needs, to be able to
focus on the more serious issues related to system structure,
and assign them with higher priority for repair such that
product
evaluation
will
be
enhanced
resulting
in
more
download volume and profits. The experts also believed that if
visualisation charts can be enhanced with more flexibility and
user reviews will be followed up with ongoing evaluation, the
approach suggested in the present study will surely prove to be
very useful in the app market application.
Keywords-user
review
analysis;
structured
system;
information visualization; negative feedback.
I.
INTRODUCTION
As the mobile app market continues gaining momentum,
more developers are now paying attention to user reviews.
Consumers usually consult others for opinions to better
understand their products or services of interest and to
support their purchase decisions [17]. To allow consumers
the opportunity to preview an app before downloading it,
the user reviews and Feedback section is located on the app
product download page in each app store download platform
to provide reference to consumers before downloading. The
section also serves as a channel for developers to make
corrections to the app based on the user feedback and to
meet the needs of consumers in the market.
Vu et al. [15] pointed out that conducting manual
analyses of user reviews is a huge challenge because for a
popular app, there are as many as thousands or tens of
thousands of reviews daily, and therefore, it would be very
time-consuming to read through all the reviews. In addition,
user reviews are generally messy due to misspelling, the use
of acronyms, abbreviations, emojis, and others. C. Iacob et
al. [8] also agreed that users often use unconventional
expressions when writing reviews, which can be very brief
and unstructured and do not usually follow syntax and
punctuation rules. This implies that merely relying on
computer programming to sort out and analyse user reviews
is not feasible. Rather, a certain level of artificial judgment
is needed in the preliminary sorting of messy raw reviews
before further computer processing should be performed to
ensure the accuracy of the results that are generated. Thus,
the question of how to filter out invalid reviews to pinpoint
authentic errors for correction and set the stage for
generating
accurate
results
in
subsequent
computer
processing is an issue that development teams must address.
[7], [9] argued that product characteristics or product
functions identified in user reviews are conducive to
manufacturers
in
making
adjustments
to
respective
characteristics or functions. In [11], semantics used in user
reviews were analysed for classification based on directional
product characteristics in reviews. Subsequent evaluation
was then conducted to sort out the good reviews from the
bad
based
on
product
characteristics.
The
respective
experimental results showed that this methodology is
effective to developers, as keywords extracted from product
characteristics help developers quickly locate problem areas
and understand the situation. Linking user reviews with the
system functionality and characteristics of an app will
certainly help developers expedite the sorting of messy user
reviews to a certain extent, resulting in the ability to make
up for negative user reviews.
In addition, [10] revealed that among the review ratings
that included the keyword ‘shortcoming’ in the feedback,
the percentages of ratings from 5-star to 1-star were: 0%,
10.84%, 31.03%, 55.36%, and 50.00%, respectively; and
that among the review ratings that included the keyword
‘bug report’ in the feedback, the percentages of ratings from
5-star to 1-star were: 0%, 0%, 22.41%, 33.93%, and
46.38%, respectively. We can see that for the keyword
‘shortcoming’, there was a huge increase from 31% 3-star
180
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

ratings to 50.00% 1-star ratings and similarly, for the
keyword ‘bug report’, there was a big jump from 22.41% 3-
star ratings to 46.38% 1-star ratings. As the data showed, 3-
star was basically the cut-off in users’ app ratings, as the
majority of user reviews related to ‘shortcoming’ and ‘bug
report’ was accounted for in 3-star and below reviews.
Therefore, from the perspective of development teams, user
review analysis that focuses on 3-star and below reviews is
relatively cost-effective in app optimisation.
Appbot is a website exclusively designed to help
development teams further understand user reviews. Appbot
calculates the average star rating, provides rankings of
reviews based on the most frequently appearing words, and
combines star ratings with text to inform development teams
whether there is any need for imminent correction while
serving as correction support to development teams for the
next version release of the app. However, from the
perspective of development teams, information that only
presents flaws in the app is not sufficient for developers to
quickly identify the source of and reasons for the problem,
since development teams still need to spend a lot of time
identifying
the
exact
problem
area
in
the
complex
programming before they can further resolve the problem.
The discussion in the introduction section shows that
user favorability can be promoted through the creation of a
good review ecosystem, achieving a high download rate and
an increase in profits. However, most of the current analytic
tools used in user reviews focus on user emotions,
frequently used words and sentences, or custom searches for
specific words or meanings as a basis for analysis; a tool
that can help development teams clearly identify the specific
problem area and the version with ongoing problems is still
missing. Therefore, development teams need to spend a lot
of time analysing the huge volume of reviews in order to
pinpoint the function pain point before they can report to
engineers for repair.
Therefore, the goal of this study is to identify user pain
points from a huge volume of messy consumer reviews.
Combined with the system architecture of the app, the
system function areas identified as pain points were
analysed and presented in a visualization chart format. A
preliminary model was then created to provide development
teams with queries for fast judgment, serving as support for
decision-making in upgrade proposals.
This paper is organized as follows. In Section II, we
present the literature review. Section III introduces the
method and the content analysis study. Section IV reports
on the results and summarizes the related work.
II.
LITERATURE REVIEW
As a new genre of word-of-moth information, online
consumer
product
feedback
is
an
emerging
market
phenomenon that is playing a more significant role in
consumers’ purchase decisions. Online user feedback is
information
left
by
users
based
on
their
personal
experiences and can therefore be regarded as a novel type of
communication tool that helps consumers identify the most
appropriate products [3]. According to [4], [13], user
feedback
is
like
electronic
word-of-mouth
(eWOM)
marketing, and WOM marketing is widely recognised as a
very influential communication method. Prior to making a
purchase, consumers tend to refer to online feedback
systems. Therefore, from the perspective of providers of
goods and services, online feedback and feedback volume
directly impacts the sales volume of products and services.
Furthermore, user reviews and evaluations will form an
ecosystem; that is, bad ecosystems will slowly affect the
sales volume of products or sales and gradually deteriorate
into an even worse ecosystem, creating a huge roadblock
during product promotion and sale. Therefore, developers
must pay attention to the trend in user reviews at all times.
When reviews are found to be forming a bad cycle, timely
changes must be made to products and services to prevent a
bad ecosystem from forming. On the contrary, a good
ecosystem
will
continue
increasing
the
sales
volume
intangibly, resulting in a positive multiplying effect in sales;
good
mass
effect
will
create
widespread
benefits
to
development teams.
Platform service providers are the medium between
developers and users. Nowadays, all major software and
publishing platforms such as the Apple App Store and the
Google Play Store have provided a user reviews section on
the bottom of each product page that also shows the average
rating of the product, providing consumers with easy access
to other users’ tips and comments on their personal
experiences. Before downloading an app (especially for
apps that are not free), app users can easily check the ratings
of the respective app, hence cutting down the cost of time
spent searching. In addition, since reviews are made by
users who are also consumers of the same product, their
reviews are considered more credible than the information
provided by developers due to conflicts of interests. Ante
[1] pointed out that majority of consumers look up user
reviews and ratings before making a purchase. Because of
this, the consumer’s first impressions become the basis for
consumption. For developers, platform service providers not
only can bring in high subscription volume and browsing
volume but can also save development teams time on
product promotion. Online user reviews also facilitate
development teams to release faster updates on products and
services.
Vasa et al. [13] argued that although the user review is a
new concept in the app industry, user reviews have been
regarded as a business strategy in other industries for many
years. Negative reviews allow developers to focus on
specific
areas
that
need
to
be
corrected.
Therefore,
development teams can prioritise updates based on the
urgency expressed in the feedback, which will result in
better subsequent user reviews. According to [14], [16], in
the hotel industry, making appropriate analysis of reviews
will generate business opportunities. These two studies
found that prioritising updates on design or service process
that most consumers felt inappropriate makes consumers
feel
valued
by
the
business,
thereby
increasing
the
probability that consumers will make repurchases or leave
better reviews in the future, both of which are beneficial to
the service provider.
181
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

According to [12], user reviews are a form of value co-
creation. [2] additionally found that product evaluation
provided by consumers are perceived by fellow consumers
as more trustworthy than information provided by sales
personnel. One of the reasons why most consumers trust
consumer reviews was put forward in [3]: that consumer
reviews are based on personal usage experiences, leading to
the creation of a unique type of product information that can
help consumers identify products that they really need.
Therefore, online user review systems provide a venue for
consumers to share their opinions and experiences about
products. On the other hand, such venues also provide
valuable
information
resources
to
potential
buyers,
facilitating effective and rational purchase decisions. Users’
word-of-mouth is more convincing than traditional media
[4].
From the perspective of sellers, [6] indicated that
subsequent to service failure, the majority of service
providers will take remedial measures to improve consumer
experience, hoping to change consumers’ negative word-of-
mouth
into
positive
word-of-mouth.
In
terms
of
development, user’s negative feedback related to system
errors are often valuable information that can be collected
for analysis and upgrades, as well as the promotion of
satisfaction intensity in related products and services. [5]
also pointed out from research results that user reviews are
similar to users’ needs, ideas, or ways of improvement that
provide positive inputs to development personnel.
From the perspective of users, the majority of designs in
product promotion are focused on the strengths of products,
with shortcomings left off. However, user reviews are the
opposite: consumers will take a neutral stance when making
comments, pointing out both the strengths and shortcomings
of the product. This is in essence a type of promotion that is
co-created by a group of consumers, and is therefore of
more reference value and is important information for
decision-making by new consumers.
From the perspective of developers, launching new
versions and functionalities to maintain user freshness is
important, but user reviews will directly impact customer
retention rate such that excessive negative reviews will be a
threat to product promotion and sales, and therefore should
not be neglected. Thus, how to strike a balance between
retention rate and freshness intensity is a serious challenge to
development teams.
III.
METHODS
Based on the literature review, this study put forward
two areas of improvement that should be addressed in user
review analysis: 1. The volume of user reviews was too
huge making efficient classification impossible; 2. It was
impossible to pinpoint problem areas from user reviews. In
light of these two areas, this study adopted a system
structure and information visualization to simultaneously
resolve the above two problems. Taking advantage of
information
visualization
characteristics,
complex
data
would be converted into images that development team
members could understand, and related problems could then
be quickly clarified.
Thus, this study was divided into four stages: 1. User
review data collection; 2. Data arrangement and analysis; 3.
Visualization of analysed results; and 4. Heuristic evaluation,
as explained in Figure 1.
Figure 1.
Study Flowchart
Sample App Selection
The
two
appropriate
sample
apps
selected
for
examination (due to the fact that the maximum number of
user reviews iTunes allows for scraping is 500, app selection
criteria in this study was having more than 500 reviews) in
this study were Walkr (Figure 2) and OPUS: Rocket of
Whispers (Figure 3) after obtaining agreements from the two
related development teams
to provide assistance. The
following is a brief introduction of the two development
teams and the respective apps they have developed.
Walkr
Figure 2.
Walkr (captured on 13th October 2017), data source:iTunes
Walkr (Figure 2) is a game app developed by Fourdesire
in Taiwan that was released in August 2014 with nearly
30,000 user reviews on versions released in the United
States.
182
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

OPUS: Rocket of Whispers
Figure 3.
OPUS: Rocket of Whispers (captured on 13th October 2017),
data source:iTunes
OPUS: Rocket of Whispers was a game app developed
by Sigono in Taiwan that was released on 14th September
2017. Its user reviews in Taiwan reached 4,000 immediately
after release.
The following steps and pictures are presented using
Walkr as an example.
Step 1. User Review data collection
Since
it
was
time-consuming
and
error-prone
to
manually
enter
reviews,
Python
programming
was
employed because of its simple logic, fast download speed,
and ability to handle massive data. The development team
only needed to enter the review download addresses that
Apple iTunes allowed to open and define the data needed,
and they could then be quickly extracted. The reviews were
from the Apple iTunes stores in United States and Taiwan to
ensure languages used in post-hoc comparison were English
and Chinese only. The following five types of data were
also scraped: review date (Date), review title (Title), review
content (Content), Rating (Rate), and version (Version).
However, due to the fact that the maximum number of user
reviews Apple iTunes allowed for scraping in each app was
500, this study ranked the data based on the feature ‘Most
Helpful’. Users mark that they agree or disagree with other
users’ reviews, and the system would rank the reviews
based on number of users that agreed to determine the ‘Most
Helpful’ feature. That is, the more users who agreed with a
review, the higher the review would be ranked. This ranking
mechanism could eliminate invalid reviews to a certain
extent,
alleviating
the
burden
in
subsequent
data
arrangement. For each app, 500 user reviews would be
scraped as raw data, which were then directly exported to
Excel, as shown in Figure 4.
Figure 4.
Partial map of Walkr’s raw review data
Step 2. Data Arrangement and Analysis
This stage involved raw data arrangement and mapping
of the app framework. Raw data were sorted based on Rate
and reviews that were 3-stars and below with emojis or
garbled reviews were eliminated; only raw data refreshed
with text arrangement were retained.
Secondly, the app structure was drawn using mind
mapping, and all clickable buttons in the app were drawn
from left to right according to the page flowchart (Figure 5).
Lastly, Python programming was employed using the text
search function to search for the function areas mentioned in
the reviews, that is, all the reviews that mentioned a certain
function were included in the mind map (Figure 6 is a
magnified version of the area enclosed in red in Figure 5).
Orange text represents 3-star reviews and red text represents
1-star and 2-star reviews.
Figure 5.
Partial system structure of Walkr(1)
Figure 6.
Partial system structure of Walkr(2)
183
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

Step 3. Visualization of analysed results
App structure is often large, and although problems are
mostly concentrated in certain areas, with reviews included,
too much data and too big a page will not only make it
difficult to spot the problematic function area, but also make
the review text section impossible to read. Therefore, in the
visualization design stage, the functions mentioned in user
reviews were first sorted out in the structure using a version
timeline to sort on the timeline axis that could on the one
hand enhance readability, and on the other hand allow the
development team to gain a clear understanding of whether
the functional problem in the version was ongoing or
coincidental, and facilitate decisions on update priorities.
Figure 8 is a visualization chart.
Figure 7.
Partial system structure of Walkr(3)
Figure 8 is a visualization chart showing the user
reviews analyzed by Walkr and it is composed of six block
areas that allowed the development team members to easily
understand
and
analyze
the
results.
The
meanings
represented by the six block areas are presented in the
following accordingly.
1)
App name
2)
Icon: orange circle represents 3-star user reviews
and red circle represents 1- and 2-star reviews.
3)
Problematic functional structure (viewed together
with block area 4): the functional structure mentioned in
user reviews in the system structure in Figure 7. For
example, activity tracking was mentioned by users in Figure
6 and, therefore, activity tracking was included in this block
area to be viewed in conjunction with the version timeline.
4)
Version Timeline (viewed in conjunction with block
area 3): version that users reported problematic in the
related function. For example, under activity tracking in
Figure 6, there were 9 users reporting a problem in the
following 8 versions: 1.3.3, 1.3.5, 1.3.6, 2.0.1, 2.0.2, 2.1.8,
2.1.10, and 2.1.13, all of which were 3-star ratings and
were therefore denoted by orange circles. According to the
activity tracking
in Figure 8, the order was sorted
according to versions. Among them, version 2.02 appeared
twice and was therefore denoted with “x2” on top.
5)
Sentiment Analysis: the higher the number, the more
negative the user sentiment is. Red represents 1- and 2-star
user ratings; orange represents 3-star user ratings.
6)
Raw Data: the number of user reviews searched are
noted here; the number of 3-star, 2-star, and 1-star reviews
are also given; grey represents the number of 4- and 5-star
user reviews.
Figure 8.
Partial system structure of Walkr(3)
Method
Taking the epic function in Figure 8 as an example:
when users reported problems in version 2.1.2, which were
also reported in version 4.04, it could be deduced that such
functional problems might likely be coincidental and
perhaps had to do with hardware problems in users’ mobile
phones. In that case, developers could choose to repair
functions that were mentioned more frequently in reviews.
For example, for the function of Apple Watch (The sixth
item in block 3 of Figure 8) that users have repeatedly
mentioned about system failure from version 1.1 to version
4.0.4, development teams could intercept the programming
184
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

codes directly to test and repair the related area shown on
the picture.
As for system function (The last item in block 3 of
Figure 8), there were significantly more orange circles (3-
star) than red circles (1- and 2-star), indicating on the one
hand that there was not an urgent need for system
improvement. However, repairs targeted at maintenance
purposes such as system logistics, interface design, and logo
design would make the system more better and could
closely follow users’ needs. On the other hand, it can be
seen (In block 4 of Figure 8) that the development team has
actually made gradual system-related revisions as shown in
the increase in spacing between version 3 and version 4,
indicating that system stability and overall logistics design
were gradually recognised by users resulting in lower
probability of getting reviews of 3 stars (inclusive) and
below. This could be viewed as a sign of the gradual
maturity of the system.
Step 4. Heuristic Evaluation
This study adopted a heuristic evaluation for validation
by sending the development team the system architecture
and a design of the visualisation chart for review together
with two questionnaires that they were asked to fill out as
feedback.
There
were
7
true-false
questions
in
Questionnaire I aiming at understanding whether the results
of the present study were useful to development teams.
There were four short-answer questions in Questionnaire II,
the topics of which were focused on the following:
methodology adopted by the development team in previous
user review analysis, possible dilemmas development teams
have encountered, and the development team’s thoughts on
the recommendations made in the present study. Below is a
brief introduction of the background of the three experts
invited to participate in the evaluation (as shown in Table 1)
and the two questionnaires (Table 2 and Table 3) of
questions listed with respective reasons for question design.
The questions were designed not only to help the
researcher of this study obtain the development team’s
thoughts and recommendations on the results, but also (and
equally importantly) to share the development team’s
practical experience in handling issues and bottlenecks in
the past, the essential elements and application approaches
needed
for
user
review
behaviour
analysis,
and
the
identification of context from feedback while helping the
development team further understand the advantages and
shortcomings of apps so that the development team can
draw on the advantages for better performance in the future
while
making
corrections
to
deficiencies
for
further
improvement.
The three experts invited to take part in the heuristic
evaluation were team members in charge of user behaviour
studies. In Questionnaire I, all three experts raised the issue
that the system architecture or functions seemed irrelevant
to the overall system architecture. To address this issue, the
following explanation was presented to the development
team: the user review analysis approach provided in the
present study was designed exclusively for internal use
within
the
development
team.
Therefore,
during
architectural mapping, the development team could refer to
the files of the original system architecture design and edit
the design accordingly until reaching
full integration
without leaving any deviation. Regarding mapping function
keywords, the way the present study named and understood
the
keywords
might
be
different
than
that
of
the
development team. However, if the system functions chart
was created by the development team, not only could the
team focus on new functions for independent analysis, but
common function issues could also be identified for further
investigation, resulting in a higher level of accuracy. The
experts also mentioned that they hoped to have more
detailed user reviews.
TABLE I.
LIST OF EXPERTS
Expert
Position
Years of Experience
Affiliation
A
Sales
3.5 years
Fourdesire
B
Sales
4 years
Fourdesire
C
Product Manager
8 years
Sigono
TABLE II.
QUESTIONNAIRE I.
No
Reason for question
design
Question
1
To
understand
the
background
and
experience of the team
member.
What
is
your
position
in
the
development team? How many years
of related experience do you have?
2
To understand whether
the developer agrees
with the importance of
user reviews.
As a study of user behaviour, do you
think that User Review analysis will
help improve development projects?
3
To understand whether
negative reviews have
a higher priority for
consideration
of
handling
and
if
duration
is
an
important basis.
Did the ratings of 1-, 2-, and 3-star
help the development team quickly
adjust the priority list of issues to be
handled?
4
Was
information
related
to
the
problematic version and the duration of
problem an important indicator to the
development team?
5
To understand whether
the present study can
effectively
help
alleviate
the
developer’s burden in
analysing
user
reviews.
The
present
study
combined
user
reviews
with
system
function
architecture. Did this approach enhance
the development team’s precision in
identifying and handling the problem
areas?
6
To continue with the above, did the
problems located in the function area
that was identified in user reviews
(such as the area enclosed in red shown
in the figure below) help shorten the
time needed for the development team
in system optimisation?
7
To understand whether
the
results
of
the
present
study
are
accurate.
According to the development team’s
past experiences, were the problematic
functions listed in the visualisation
chart of the present study accurate?
(Here
accuracy
is
defined
as:
1.
whether the system functions pointed
185
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

No
Reason for question
design
Question
1
To
understand
the
background
and
experience of the team
member.
What
is
your
position
in
the
development team? How many years
of related experience do you have?
out in user reviews were accurate, and
2. whether the function actually existed
in user feedback.
This was due to the fact that iTunes’ limit on the number
of user review downloads allowed for each app was 500, so
the present study could not provide more to the team. The
general public is not allowed to obtain all user reviews for
any single app. However, the app’s developer is allowed to
download every single review. Therefore, if developers are
to take charge of data scraping from different versions,
especially scraping 1-star reviews, a certain degree of
improved operationality is guaranteed, meaning that results
will be more accurate.
TABLE III.
QUESTIONNAIRE II.
No
Reason for question design
Question
1
To understand the benefits
the development team has
gained from the results of the
present study.
What do you think is the most
helpful feature in the areas of
updating
and
optimisation
provided in the case study of the
present study?
2
To understand the past issues
encountered
by
the
development team and to try
to resolve them using the
results of the present study.
What are the most common
issues
or
bottlenecks
the
development
team
has
encountered
in
previous
versions?
3
To understand the important
features that the development
team will pay attention to
when studying user reviews.
How did your development team
handle user reviews in the past?
4
To understand the expert’s
suggestions to the results of
the present study.
Is there anything else or any
particular item the development
team
would
like
to
further
understand that has not been
discussed in the results?
5
Q&A
Lastly, if there are still any
questions or interest in the topic
of this study, you are welcome
to leave message here with your
contact
information. We
will
reply as soon as possible.
Feedback from the three experts on analysis made in the
present study is arranged and presented as follows:
1) After the new version release of a function, the
follow-up evaluation should be conducted on user reviews
related to this function.
2) Add more enrichment to the visualisation chart.
3) In terms of user reviews of higher ratings, in the case
of Walkr, which had relatively fewer 1- to 3-star reviews,
analysing 4- and 5-star reviews may help find out what
functions users like and why.
4) The suggested approach was able to expedite the
identification of problem areas and versions.
5) Visualisation design facilitated understanding of the
severity of issues and the frequency of occurrence.
6) To understand whether issues were resolved by
version iteration.
7) System architecture and function names did not fit
the overall system.
8) The number of reviews was not enough, and the
flexibility of the visualisation chart was limited.
Lastly, the questionnaires that were returned showed that
after eliminating the programming bugs that needed repair,
the development team did not know how to focus on the
issues raised in user reviews, nor did they know how to set
up a priority list of issues to be corrected. In other words,
team members did not have a dedicated set of analytical
tools that allowed them to handle the large volume of user
reviews from various sources, and they had not set a priority
list of issues to be corrected.
IV.
CONCLUSIONS
As seen from the analysis in the present study, user
reviews are a key factor in judging whether there is gradual
growth in an app. If there are enough user reviews, taking a
comprehensive view on user reviews helps developers
understand whether an app is on the right track for growth.
Visualisation charts also provide development teams easy
understanding in user review distribution and direction,
reducing the search time for identifying issues while quickly
understanding the real needs of users.
It is found from the two case studies that if an app only
has a few negative reviews, if user reviews are classified
into directional (clearly pointing out the area in system
function or architecture) system architecture review and
non-directional (mostly focusing on narrative of user
experience) user review, few directional user reviews mean
that the app currently does not have any issues in structural
functions. The majority of users are satisfied with the
system functions and have therefore switched focus to
expressing ideas and suggestions to the system, hoping that
the development team can follow the direction of the
reviews in updating. At this time, the development team can
follow the original plan to announce the new release of
functions to increase user freshness. On the contrary, having
more directional than non-directional user reviews means
that users think that there is problem in the system function.
Ignoring users’ requests at this time will result in user
churn. Thus, the development team must fix the problem as
soon as possible, rather than releasing new functions, to
calm down angry users and alleviate user churn.
The greatest challenge encountered in the course of this
study was in heuristic review because this study is still at
the model design stage; it needs verification and evaluation
from an outstanding and experienced development team in
the industry to help perfect the design process of the
research results. In addition, business secrets may be
involved in the research process, which most developers
cannot provide for assistance in evaluation. At the same
186
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

time, core members of the development team were not easy
to get in touch with; emails sent to development companies
were often declined by the support team, resulting in a
challenge in seeking experts to participate in the heuristic
evaluation.
REFERENCES
[1]
S. E. Ante, Amazon: Turning consumer opinions into  gold. Business
Week, 15, 2009.
[2]
B. Bickart and R. M. Schindler, "Internet forums as influential
sources of consumer information, " Journal of interactive marketing,
vol. 15, no. 3, pp. 31-40, 2001. 
[3]
Y. Chen, and J. Xie, "Online consumer review: Word- of-mouth as a
new element of marketing communication mix," Management science,
vol. 54, no. 3, pp. 477-491, 2008.
[4]
D. Godes, and D. Mayzlin, "Using online conversations to study
word-of-mouth communication," Marketing science, vol. 23, no. 4, pp.
545-560, 2004.
[5]
E. Guzman, and W. Maalej, "How do users like this feature? a fine
grained sentiment analysis of app reviews," 2014 IEEE 22nd
International Requirements Engineering Conference (RE), Karlskrona,
2014, pp. 153-162, doi: 10.1109/RE.2014.6912257.
[6]
C. W. Hart, J. L. Heskett, and W. E. Jr. Sasser, "The profitable art of
service recovery," Harvard Business Review, vol. 68, no. 4, pp. 148–
156, 1990.
[7]
M. Hu, and B. Liu, "Mining and summarizing customer reviews,"
Proc. 10th ACM SIGKDD international conference on Knowledge
discovery and data mining (KDD ‘04), ACM, New York, NY, USA,
2004, pp. 168-177, doi: 10.1145/1014052.1014073.
[8]
C. Iacob, and R. Harrison, "Retrieving and analyzing mobile apps
feature
requests
from
online
reviews,"
2013
10th
Working
Conference on Mining Software Repositories (MSR). IEEE Press,
2013, pp. 41-44, doi: 10.1109/MSR.2013.6624001.
[9]
X. Meng, and H. Wang, "Mining user reviews: from specification to
summarization,"
In
Proceedings
of
the
ACL-IJCNLP
2009
Conference Short Papers, pp. 177-180, 2009.
[10] D. Pagano, and W. Maalej, "User feedback in the appstore: An
empirical
study,"
2013
21st
IEEE
International
Requirements
Engineering Conference (RE), Rio de Janeiro, 2013, pp. 125-134, doi:
10.1109/RE.2013.6636712.
[11] G. Somprasertsri, and P. Lalitrojwong, (2010). "Mining Feature-
Opinion in Online Customer Reviews for Opinion Summarization,"
J.UCS, vol. 16, no. 6, pp. 938-955, 2010.
[12] F. T. C. Tan, and R. Vasa, "Toward a social media usage policy,"
Proc. ACIS 2011, pp. 84-89.
[13] R. Vasa, L. Hoon, K. Mouzakis, and A. Noguchi, "A preliminary
analysis
of
mobile app
user
reviews,"
Proc.
24th
Australian
Computer- Human Interaction Conference (OzCHI ’12), ACM, New
York, NY, USA, 2012, pp. 241-244, doi: 10.1145/2414536.2414577.
[14] I. E. Vermeulen, and D. Seegers, "Tried and tested: The impact of
online
hotel
reviews
on
consumer
consideration,"
Tourism
management, vol. 30, no. 1, pp. 123-127, 2009.
[15] P. M. Vu, T. T. Nguyen, H. V. Pham, and T. T. Nguyen, "Mining
user opinions in mobile app reviews: A keyword-based approach (t), "
2015 30th IEEE/ACM International Conference on Automated
Software Engineering (ASE), Lincoln, NE, 2015, pp. 749-759, doi:
10.1109/ASE.2015.85
[16] Q. Ye, R. Law, and B. Gu, "The impact of online user reviews on
hotel room sales, " International Journal of Hospitality Management,
vol. 28, no. 1, pp. 180-182, 2009.
[17] C. H. Yin, "網路口碑對線上應用程式的購買意圖之影響-以 Apple
App Store 為例" [ The Influence of eWOM on purchase Intention of
Online Applications: An Empirical Study of Apple App Store], 2011,
URL: http://hdl.handle.net/11296/t9tju5.
187
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

