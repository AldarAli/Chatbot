Machine Learning for Chemogenomics
on HPC in the ExCAPE Project
Tom Vander Aa and Tom Ashby
IMEC,
Leuven, Belgium
email:Ô¨Årstname.lastname@imec.be
Yves Vandriessche
Intel corp.
Belgium
email:yves.vandriessche@intel.com
Vojtech Cima, Stanislav B√∂hm and Jan MartinoviÀác
IT4Innovations, V≈†B ‚Äì Technical University of Ostrava,
Ostrava, Czech Republic
email:Ô¨Årstname.lastname@vsb.cz
Abstract‚ÄîThe ExCAPE project is a Horizon 2020 project
to advance the state of the art of machine learning (ML)
implementations on supercomputing hardware. We have adopted
bioactivity predictions for chemogenomics as a challenging use-
case to drive development. In this paper, we will give an overview
of the challenges in ExCAPE to use supercomputing efÔ¨Åciently.
We will touch on three key examples dealing with efÔ¨Åcient ML
workÔ¨Çow execution, support for multi-task learning using matrix
factorization methods and the challenges originating from the
large and very sparse datasets in ExCAPE.
Index Terms‚ÄîMachine Learning, High-Performance Comput-
ing, Collaborative Filtering, Distributed Task Scheduling
I. INTRODUCTION AND CONTEXT
Traditional users of High Performance Computing (HPC)
have mostly been concerned with simulation of physics of
one type or another and at various different scales. In the
last decade, a new breed of user of very large machines has
appeared, those concerned with Big Data. Carrying out sim-
ulations is mostly about doing large amounts of computation
to observe the behavior of a sophisticated model with few
parameters. Big Data problems, by contrast, usually deal with
less sophisticated models but with many more parameters,
and try to choose the model parameters by analyzing large
amounts of data with relatively little associated computation.
Folk wisdom in this Ô¨Åeld states that the ability to capture
and analyze more data is more valuable than making more
sophisticated models, and this works well when data is cheap
and easy to get. However, there are problems in this area for
which the data are very expensive to generate. In this case, it
becomes important to be able to use more sophisticated models
to be able to squeeze as much knowledge as possible out of the
data. Such problems are at the juncture of HPC and Big Data
in that they have large data sets to analyze, yet should exploit
more sophisticated models through computation to make the
most of the available data.
The ExCAPE project [1] is about how to tackle such
problems. The core of the project is about mathematics and
software and how they work on HPC machines. However, to be
able to advance the state of the art it helps to have a concrete
problem to tackle. For this, we take the chemogenomics
problem, that of predicting the activity of compounds in the
drug discovery phase of the pharmaceutical industry, leading
to the project name Exascale Compound Activity Prediction
Engines (ExCAPE). Making such predictive models belongs
to the Ô¨Åeld of Machine Learning.
More general than chemogenomics, we want to Ô¨Ånd meth-
ods and systems that can tackle large and complex machine
learning problems. This will require algorithms and software
that make efÔ¨Åcient use of the latest HPC machines. Creating
these, along with preparing the data to give the system
something to work on, is the main work of the project.
Many interesting open challenges need to be overcome to
be able to run machine learning efÔ¨Åciently at scale on HPC
hardware in all cases. In the following sections of this paper we
explain three very relevant and interesting example challenges,
namely:
‚Ä¢ how to execute machine learning workÔ¨Çows with many
dependent tasks (Section II);
‚Ä¢ how to take advantage of the fast interconnect (like
iniÔ¨Åniband) typically only found on true HPC hardware
for multi-node machine learning tasks (Section III);
‚Ä¢ how to support very large but sparse datasets (Section IV)
More details can be found in the referenced documents.
II. EFFICIENT WORKFLOWS USING HYPERLOOM
Solutions for scheduling problems on HPC systems exist
when dispatching tasks (computational units) of known du-
ration and resource requirements. However, real-world appli-
cations such as those in machine learning, encompass tasks
with no requirement annotations in addition to the overhead of
handling massive amounts of data only manageable by large-
scale distributed environments. To address these challenges,
we developed HyperLoom [2]. HyperLoom is a platform
for deÔ¨Åning and executing pipelines in large-scale distributed
environments. Unlike other scheduling systems, HyperLoom
is speciÔ¨Åcally tailored to work efÔ¨Åciently on high-performing
computing (or HPC) systems and offers a user-friendly repre-
sentation of tasks as acyclic computational graphs. An example
of such a scientiÔ¨Åc pipeline is shown in Figure 1.
Our resulting pipelines for both synthetic and real-world use
cases are successfully distributed leveraging HPC resources
covering up to a hundred thousand tasks, across ten or more
physical compute nodes. We analyzed HyperLoom perfor-
mance for both synthetic and real test cases scaling up to hun-
dreds of thousands tasks and tens of physical computational
nodes. HyperLoom signiÔ¨Åcantly outperforms Dask/Distributed
72
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

Figure 1.
Example of a scientiÔ¨Åc pipeline visualized as a directed acyclic
graph.
R
=
U
V
M
M
N
K
K
x
ùêæ ‚â™ ùëÄ
ùêæ ‚â™ ùëÅ
R is very sparse
Figure 2. Low-rank Matrix Factorization
ranging from 6.3√ó to 2.2√ó better performance for two test
cases [2].
III. SCALABLE MATRIX FACTORIZATION USING
ASYNCHRONOUS COMMUNICATION
While single-task learning methods for compound-activity
prediction result in many small tasks, which can easily be
solved using HyperLoom, multi-task learning methods com-
bine and solve multiple tasks at the same time [3], by
exploiting commonalities and differences across tasks. This
generally results in improved prediction accuracy but also in
much fewer and much larger tasks. Such tasks need to be
parallelized themselves to be able to run them efÔ¨Åciently.
Matrix Factorization (MF) is a class of multi-task methods
that have been successfully used. As sketched in Figure 2, the
idea of these methods is to approximate the compound-activity
matrix R as a product of two low-rank matrices U and V such
that R ‚âà U √ó V . In this way, U and V are constructed from
the known ratings in R, which is usually very sparsely Ô¨Ålled.
The recommendations can be made from the approximation
U √ó V , which is dense.
We have built a high-performance distributed implemen-
tation [4] of the popular and effective matrix factorization
algorithm called Bayesian Probabilistic Matrix Factorization
(BPMF [5]). We have shown that load balancing and low-
overhead asynchronous communication are essential to achieve
good parallel efÔ¨Åciency, clearly outperforming more common
synchronous approaches like GraphLab [6]. The achieved
speed-up allowed us to speed up machine learning for drug
discovery on an industrial dataset from 15 days for the initial
Julia-based version to 5 minutes using the distributed version
with Intel Threading Building Blocks (TBB) and the Global
Address Space Programming Interface (GASPI) [7].
IV. SUPPORT FOR LARGE BINARY SPARSE MATRICES
There is one class of routines that is ubiquitous for all
involved Machine Learning algorithms on ExCAPE-like data:
sparse linear algebra. In many cases, these operations dominate
the runtime of the computation, which means that even small
optimizations translate into large efÔ¨Åciency and performance
gains.
Unlike dense linear algebra routines, where ready-made
heavily optimized libraries are available, the performance of
sparse routines is determined by the nature of the data, which
makes it impossible to make a routine that is optimal in
general.
Sparse matrix-vector (SPMV) was identiÔ¨Åed as a key bottle-
neck low-level operation is the ExCAPE ML-algorithms. The
two main workloads we are analyzing are matrix-factorization
and neural network training on ExCAPE‚Äôs activity and the
compounds‚Äô Ô¨Ångerprint data.
After optimization [8] of those SPMV routines on ExCAPE
data [9] we can say that:
‚Ä¢ Linear algebra optimizations on the Compressed Storage
of Rows (CSR) format performs best from those formats
tested, but
‚Ä¢ although the Coordinate Format (COO) is less efÔ¨Åcient,
it gets close to CSR by sorting non-zero elements by
Hilbert-order [10], making it a good choice in case the
matrix contents are changing dynamically.
‚Ä¢ Optimizing for binary matrix data and parallel task
imbalances yields a 2.7√ó speedup over the more gen-
eral mkl_scsrgemv() Intel Math Kernel Library (MKL)
SparseBlas routine.
‚Ä¢ Operating on multiple right-hand sides obtains another
2.4√ó proportional speedup due to both vectorization and
improved cache locality.
V. CONCLUSIONS
This short paper provided insight into what is needed to
run large scale machine learning efÔ¨Åciently on HPC hardware
for three key examples. We have indicated the importance
of efÔ¨Åcient i) workÔ¨Çow execution, ii) support for multi-task
learning, and iii) low lever sparse algebra routines. Solutions
for all three will need to be combined for the project to
succeed.
73
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

ACKNOWLEDGMENTS
This work is partly funded by the European project ExCAPE
with reference 671555 and by the IT4Innovations infras-
tructure, which is supported from the Large Infrastructures
for Research, Experimental Development and Innovations
project ‚ÄúIT4Innovations National Supercomputing Center ‚Äì
LM2015070‚Äù.
REFERENCES
[1] The ExCAPE Consortium, ‚ÄúExCAPE: Exascale Compound Activity
Prediction Engine,‚Äù http://excape-h2020.eu/, retrieved: June 2017.
[2] V. Cima et al., ‚ÄúHyperLoom possibilities for executing scientiÔ¨Åc work-
Ô¨Çows on the cloud,‚Äù in Proceedings of the CISIS 2017 : The 11th In-
ternational Conference on Complex, Intelligent, and Software Intensive
Systems, 2017.
[3] R. Caruana, ‚ÄúMultitask learning,‚Äù in Learning to learn. Springer, 1998,
pp. 95‚Äì133.
[4] T. Vander Aa, I. Chakroun, and T. Haber, ‚ÄúDistributed bayesian prob-
abilistic matrix factorization,‚Äù in ICCS 2017: International Conference
on Computational Science, June 2017.
[5] R. Salakhutdinov and A. Mnih, ‚ÄúBayesian probabilistic matrix fac-
torization using Markov chain Monte Carlo,‚Äù in Proceedings of the
International Conference on Machine Learning, vol. 25, 2008, pp. 880‚Äì
887.
[6] Y. Guo, A. L. Varbanescu, A. Iosup, C. Martella, and T. L. Willke,
‚ÄúBenchmarking graph-processing platforms: A vision,‚Äù in Proceedings
of the 5th ACM/SPEC International Conference on Performance
Engineering, ser. ICPE ‚Äô14. New York, NY, USA: ACM, 2014, pp. 289‚Äì
292. [Online]. Available: http://doi.acm.org/10.1145/2568088.2576761
[7] D. Gr√ºnewald and C. Simmendinger, ‚ÄúThe GASPI API speciÔ¨Åcation and
its implementation GPI 2.0,‚Äù in 7th International Conference on PGAS
Programming Models, vol. 243, 2013, pp. 243‚Äì248.
[8] Y. Vandriessche and T. Vander Aa, ‚ÄúExCAPE deliverable D2.6: Simu-
lation report 1,‚Äù Tech. Rep., 2017.
[9] J.
Sun
et
al.,
‚ÄúExCAPE-DB:
an
integrated
large
scale
dataset
facilitating
Big
Data
analysis
in
chemogenomics,‚Äù
Journal
of
Cheminformatics, vol. 9, no. 1, p. 17, dec 2017. [Online]. Available:
http://jcheminf.springeropen.com/articles/10.1186/s13321-017-0203-5
[10] A. N. Yzelman, D. Roose, and K. Meerbergen, ‚ÄúSparse matrix-vector
multiplication: parallelization and vectorization,‚Äù in High Performance
Parallelism Pearls: Multicore and Many-core Programming Approaches,
J. Reinders and J. Jeffers, Eds.
Elsevier, 2014, ch. 27, p. 20.
74
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

