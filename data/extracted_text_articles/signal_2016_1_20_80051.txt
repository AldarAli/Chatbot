Tongue Recognition From Images
Ryszard S. Chora´s
Institute of Telecommunications and Computer Science
UTP University of Sciences and Technology
85-796 Bydgoszcz, Poland
Email: choras@utp.edu.pl
Abstract—This paper proposes a method of personal identiﬁca-
tion based on tongue image. Tongue images have many advantage
for personal identiﬁcation and veriﬁcation. In this paper a texture
tongue features are extracted based on Gabor ﬁlters, and WLD
(Weber Law Descriptors) transform. These features can be used
in forensic applications and with other robust biometrics features
can be combined in multi modal biometric system.
Keywords—Tongue image, Gabor ﬁlters, Weber Law Descriptors.
I. INTRODUCTION
Tongue recognition is attracting a great deal of attention
because of its usefulness in many applications. Traditional,
tongue recognition are often classiﬁed into two groups:
• Tongue recognition and analysis for the patient disease
diagnosis. Tongue recognition for diagnosis has played
an important role in traditional Chinese medicine (TCM)
and in this area most investigation has been focused on
extraction of chromatic features [11] [16], shape and
textural features [7] [8] [12].
• Tongue recognition for biometric personal identiﬁcation.
Our work concerns the biometric applications of the tongue
recognition and efﬁcient feature extraction.
Biometrics human identiﬁcations uses automated methods of
recognizing a person based on a physiological or behavioral
characteristics [6]. A biometric system is a pattern recognition
system that recognizes a person on the basis of a feature vector
derived from a speciﬁc physiological or behavioral characteris-
tics that the person possesses. Physiological Biometrics - also
known as static biometrics - is based on the data derived from
the measurement of a part of a person’s anatomy [9].
Tongue image analysis have received much attention in
image analysis and computer vision. Tongue texture has many
advantages for human identiﬁcation and veriﬁcation [10] [18].
The identiﬁcation of people can be based on the texture
features. As a biometric identiﬁer, tongue image has the
following properties:
• Tongue images are unique to every person. Texture fea-
tures of the tongue are distinctive to each person,
• Texture features of an individual tongue are stable and
unchangeable during the life of a person,
• The human tongue is well protected in mouth and is
difﬁcult to forge.
Tongue recognition system is presented in Figure 1 and
it involves ﬁve major modules: tongue image acquisition,
preprocessing, tongue feature extraction, visual features and
classiﬁcation.
In this paper, a tongue image feature extraction method is
proposed, which utilizes Gabor ﬁlters and local features such
as WLD, because these features are robust against some types
of geometric modiﬁcations. Weber local descriptors (WLD) is
a simple but powerful local descriptor, which simulates the
human visual perception.
Fig. 1. Tongue recognition system
Images which are considered in this paper are displayed in
Figure 2.
Fig. 2. Tongue images
The remainder of this paper is organized as follows. Section
2 brieﬂy describes the preprocessing operations. In Section 3,
we describe Gabor ﬁlters, propose a local descriptor called
WLD and brieﬂy describes the deﬁnition of Weber magnitude
and orientation. Conclusions and references are given there-
after.
6
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-487-9
SIGNAL 2016 : The First International Conference on Advances in Signal, Image and Video Processing

II. PREPROCESSING
Before performing feature extraction, the original tongue
images are subjected to some image processing operations,
such as:
1) Image stretching. The contrast level is stretched accord-
ing to
fout(x, y) = 255 ×

fin(x, y) − finmin(x, y)
finmax(x, y) − finmin(x, y)
γ
(1)
fout(x, y) is the color level for the output pixel (x, y)
after the contrast stretching process. fin(x, y) is the
color level input for data the pixel (x, y). finmax(x, y) -
is the maximum value for color level in the input image.
finmin(x, y) - is the minimum value for color level in
the input image, γ - constant that deﬁnes the shape of
the stretching curve.
2) Extraction of region of interest (ROI) from original
tongue images. The tongue images are normalized with
respect to position, orientation, scale, reﬂection, as fol-
lows.
The new invariant coordinates (x, y) of image pixels and
the old coordinates (x′, y′) are related by
[x, y, 1] = [x′, y′1] ×


1
0
0
0
1
0
−i0
−j0
1

 ×
×


1
δx
0
0
0
1
δy
0
0
0
1

 ×


cos β
sin β
0
− sin β
cos β
0
0
0
1


(2)
where x0, y0 is the centroid of image; δx and δy repre-
sent standard deviation relative to variable x, y; and β
is an angle between the major axis of an object and the
vertical line
tan 2β =
2 P
x
P
y(x − x0)(y − y0)
P
x(x − x2
0) − P
y(y − y2
0)
(3)
Next, the ROI’s tongue blocks are automatically selected
on the centroid of tongue normalized images. The size
of whole ROI is wx × wy where wx = (x0 + K
2 ) −
(x0 − K
2 ), wy = (y0 + K
2 ) − (y0 − K
2 ) where K = 128
pixels (Figure 3). Next, the ROI image is divided into
the four sub-blocks. The size of sub-block is K
2 × K
2
pixels (Figure 4).
III. FEATURE EXTRACTION
A. Gabor ﬁlters for feature extraction
Gabor ﬁlters are a powerful tool to extract texture features
and in the spatial domain is a complex exponential modulated
by a Gaussian function. In the most general the Gabor ﬁlters
are deﬁned as follows [3] [14] [15].
The two-dimensional Gabor ﬁlter is deﬁned as
Fig. 3. Tongue ROI
Fig. 4. Four ROI’s sub-blocks
Gab(x, y, W, θ, σx, σy) =
=
1
2πσxσy
e

− 1
2

( x
σx )
2+

y
σy
2
+jW (x cos θ+y sin θ)

(4)
where j = √−1 and σx and σy are the scaling parameters
of the ﬁlter, W is the radial frequency of the sinusoid and
θ ∈ [0, π] speciﬁes the orientation of the Gabor ﬁlters.
Figure 5 presents the real and imaginary parts of Gabor
ﬁlters.
Fig. 5. The real and imaginary parts of Gabor ﬁlters
In our work we use a bank of ﬁlters built from the real
part of Gabor expression called as even symmetric Gabor
ﬁlter. Gabor ﬁltered output of the image is obtained by the
convolution of the image with Gabor even function for each of
the orientation/spatial frequency (scale) orientation (Figure 6).
7
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-487-9
SIGNAL 2016 : The First International Conference on Advances in Signal, Image and Video Processing

Given an image F(x, y), we ﬁlter this image with
Gab(x, y, W, θ, σx, σy)
FGab(x, y, W, θ, σx, σy) =
=
X
k
X
l
F(x − k, y − l) ∗ Gab(x, y, W, θ, σx, σy)
(5)
The magnitudes of the Gabor ﬁlters responses are repre-
sented by three moments
µ(W, θ, σx, σy) =
1
XY
X
X
x=1
Y
X
y=1
FGab(x, y, W, θ, σx, σy)
(6)
std(W, θ, σx, σy) =
=
v
u
u
t
X
X
x=1
Y
X
y=1
||FGab(x, y, W, θ, σx, σy)| − µ(W, θ, σx, σy)|2
(7)
Energy =
X
X
x=1
Y
X
y=1
[FGab(x, y, W, θ, σx, σy)]2
(8)
By selecting different center frequencies and orientations,
we can obtain a family of Gabor kernels, which can then be
used to extract features from an image. The feature vector is
constructed using mean - µ(W, θ, σx, σy), standard deviation -
std(W, θ, σx, σy) and energy as feature components (Table I).
Fig. 6. Gabor images of tongue ROI’s
To reduce dimension of feature vector [13], [17], we use
the Principle Component Analysis (PCA) algorithm to keep
the most useful Gabor features.
Let X = [x1, x2, . . . , xn] denote an n-dimensional feature
vector. The mean of the vector X and the total scatter covari-
ance matrix of the vector X are deﬁned as: µ = 1
n
Pn
i=1 xi
and SX = Pn
i=1(xi − µ) · (xi − µ)t.
The PCA projection matrix S can be obtained by eigen-
analysis of the covariance matrix SX. We compute the eigen-
values of SX : λ1 > λ2 > · · · > λn and the eigenvectors
of SX : s1, s2, . . . , sn. Thus SXsi = λisi,
i = 1, 2, . . . , m.
si is the ith largest eigenvector of SX, m ≪ n and S =
[s1, s2, . . . , sm].
TABLE I. FEATURES OF TONGUE ROI’S.
ROI 1
Scale
Orientation
Energy
Mean
Std
2
45◦
1968755.0
480.65308
35.064735
2
90◦
1999251.1
488.09842
40.68314
2
135◦
1968758.9
480.65402
34.23929
2
180◦
1999252.2
488.0987
42.19372
8
45◦
3.181323E7
7766.903
669.66876
8
90◦
3.1827126E7
7770.2944
657.5264
8
135◦
3.181321E7
7766.897
762.5107
8
180◦
3.1827142E7
7770.2983
728.3237
ROI 2
Scale
Orientation
Energy
Mean
Std
2
45◦
1968755.0
480.65308
35.064735
2
90◦
1999251.1
488.09842
40.68314
2
135◦
1968758.9
480.65402
34.23929
2
180◦
1999252.2
488.0987
42.19372
8
45◦
3.181323E7
7766.903
669.66876
8
90◦
3.1827126E7
7770.2944
657.5264
8
135◦
3.181321E7
7766.897
762.5107
8
180◦
3.1827142E7
7770.2983
728.3237
ROI 3
Scale
Orientation
Energy
Mean
Std
2
45◦
1867638.8
455.9665
36.981506
2
90◦
1896568.8
463.02948
44.25503
2
135◦
1867638.6
455.96646
36.304142
2
180◦
1896567.2
463.0291
41.741203
8
45◦
3.0179278E7
7367.988
707.88116
8
90◦
3.0192456E7
7371.205
717.7458
8
135◦
3.0179264E7
7367.984
774.104
8
180◦
3.0192524E7
7371.221
614.612
ROI 4
Scale
Orientation
Energy
Mean
Std
2
45◦
1906920.6
465.5568
27.945246
2
90◦
1936458.5
472.7682
35.911766
2
135◦
1906919.8
465.55658
28.149256
2
180◦
1936459.8
472.7685
35.161488
8
45◦
3.0813984E7
7522.9453
786.99506
8
90◦
3.08275E7
7526.245
728.3674
8
135◦
3.0814002E7
7522.9497
562.7714
8
180◦
3.082751E7
7526.2476
521.4142
Any vector x can be written as a linear combination of the
eigenvectors (S is symmetric, s1, s2, . . . , snform a basis), i.e.
x = Pn
i=1 biui. For dimensionality reduction we choose only
m largest eigen values, i.e. x = Pm
i=1 biui. m is choose as
follows:
Pm
i=1 λi
Pn
i=1 λi > t where t is threshold.
By removing the principal components that contribute little
to the variance, we project the entire feature vector to a lower
dimensional space, but retain most of the information.
B. Weber Law Descriptors
In 1834 Ernst Weber stated that ”the ratio between the
smallest perceptual change in a stimulus △fmin and the back-
ground level of the stimulus f is constant e.g. △fmin
f
= k” [6].
Inspired by Weber’s Law, a robust and powerful Weber Local
Descriptor (WLD) is a recently developed for local feature
extraction. For each pixel of the input image, we compute two
joint descriptors: a differential excitation DE operator and
a gradient orientation GO descriptor. The DE is a function
of the ratio between two terms: one is the relative intensity
differences of a current pixel against its neighbors (e.g., 3 × 3
square region) and the other is the intensity of the current
8
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-487-9
SIGNAL 2016 : The First International Conference on Advances in Signal, Image and Video Processing

pixel. The orientation component is the GO of the current
pixel. Figure 7 show how the DE and GO are calculated [1].
Fig. 7. Illustration of the WLD algorithm
If f(x, y) is the center pixel of a 3 × 3 window, and f(x +
i, y + j); i = −1, 0, 1 j = −1, 0, 1 are the neighbors of the
center pixel, DE is calculated as
DE = arctan


1P
i=−1
1P
j=−1
f(x + i, y + j) − 8f(x, y)
f(x, y)


= arctan
W
V

(9)
where: f(x + i, y + j) i = −1, 0, 1; j = −1, 0, 1 is the
gray level intensity of the corresponding pixel. The positive
value of DE indicates that the current pixel is darker than
the neighboring pixel,while the negative value represents the
opposite.
The main purpose of the DE component is to extract the
local salient patterns from the image.
The GO of the center pixel f(x, y) is calculated as
GO = arctan
f(i, j − 1) − f(i, j + 1)
f(i + 1, j) − f(i − 1, j)

= arctan
 A
B

(10)
where the numerator is the intensity difference between the
left and the right of f(x, y), while the denominator is the inten-
sity difference between the below and the above of f(x, y).
Next, the GO are quantized into dominant orientations as
follows:
GO
′ = arctan 2
 A
B + π

(11)
arctan 2
 A
B

=
(12)
=







GO
A > 0 and B > 0
π − GO
A > 0 and B < 0
GO − π
A < 0 and B < 0
−GO
A < 0 and B > 0
The GO is then quantized into T dominant orientations.
For each dominant orientation, histogram, H, is calculated
using the DE (Figure 8) [1].
Fig. 8. Tongue feature extraction
Because not all the features are equally important, the
feature selection technique is used.
To compute the distance of two histograms chi-square χ2
distance is used
χ2(H1, H2) =
X
i
(h1
i − h2
i )2
h1
i + h2
i
(13)
where H1 and H2 are two histograms and h1
i , h2
i are the ith
bin of the histograms.
IV. CONCLUSION
In the paper, are presented some approaches for tongue
recognition from images. To evaluate the performance of
tongue recognition methods we use own tongue database that
consists 30 images. We proposed a method which combines
the recognition results of Gabor ﬁlters and WLD features
to tongue recognition. The WLD texture features are robust
against rotation and noise. The proposed system will be
evaluated on other tongue databases in the future study.
ACKNOWLEDGMENT
The research was supported by the UTP University of
Sciences and Technology by the Grant BS01/2014.
9
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-487-9
SIGNAL 2016 : The First International Conference on Advances in Signal, Image and Video Processing

REFERENCES
[1] J. Chen, S. Shan, C. He, G. Zhao, M. Pietikinen, X. Chen, W. Gao,
”WLD: A robust local image descriptor”, IEEE Transactions on Pattern
Analysis and Machine Intelligence, 32(9), pp. 1705 - 1720, 2010.
[2] R. S. Choras, ”Iris-based person identiﬁcation using Gabor wavelets and
moments”, Proc. 2009 Int. Conf. on Biometrics and Kansei Engineering
ICBAKE, CPS IEEE Computer Society, pp. 55-59, 2009.
[3] R. S. Choras, ”Lip-prints feature extraction and recognition”, Image
Processing & Communications Challenges 3, AISC 102, pp.33-41 ,
Springer-Verlag Berlin Heidelberg, 2011.
[4] R. S. Choras, ”Vascular Biometry”, Image Processing & Communi-
cations Challenges 6, AISC 313, pp. 21 -28, Springer-Verlag Berlin
Heidelberg 2015.
[5] R. S. Choras, ”Thermal face recognition”, Image Processing & Com-
munications Challenges 7, AISC 389, pp. 37-46, Springer-Verlag Berlin
Heidelberg 2016.
[6] A. K. Jain, Fundamentals of a Digital Signal Processing, Prentice-Hall,
Englewood Clifts, NJ., 1989.
[7] W. Huang, Z. Yan, J. Xu and L. Zhang, ”Analysis of the tongue fur
and tongue features by naive bayesian classiﬁer”, Proceedings of the
Int. Conf. Computer Application and System Modeling, Taiyuan, Oct.
2010, vol. 4, pp. 304-308.
[8] B. Huang, D. Zhang, Y. Li, H. Zhang and N. Li, ”Tongue coating
image retrieval”, Proceedings of the 3rd Int. Conf. Advanced Computer
Control, Harbin, Jan. 2011, pp. 292-296.
[9] A. K. Jain, R. Bolle and S. Pankanti, Biometrics: Personal Identiﬁcation
in Networked Society, Kulwer Academic, 1999.
[10] S. Lahmiri, ”Recognition of tongueprint textures for personal aAu-
thentication: A wavelet approach”, Journal of Advances in Information
Technology, vol.3 ,no.3, pp. 168 - 175, 2012.
[11] C. Li and P. Yuen, ”Tongue image matching using color content”, Pattern
Recogn. 35(2), pp. 407-419, 2002.
[12] W. Li, S. Hu, J. Yao and H. Song, ”The separation framework of tongue
coating and proper in traditional Chinese medicine”, Proceedings of
the 7th Int. Conf. Information, Communications and Signal Processing,
Macau, Dec. 2009, pp. 1-4.
[13] C. J. Liu and H. Wechsler, ”Gabor feature based classiﬁcation using the
enhanced Fisher linear discriminant model for face recognition”, IEEE
Transactions on Image Processing, vol. 11, no. 4, pp. 467- 476, 2002.
[14] D. H. Liu, K. M. Lam, and L. S. Shen, ”Optimal sampling of Gabor
features for face recognition”, Pattern Recognition Letters, vol. 25, no.
2, pp. 267-276, 2004.
[15] L. Shen and L. Bai, ”Face recognition based on Gabor features using
kernel methods”, Proc. 6th IEEE Conf. on Face and Gesture Recognition
Korea, pp. 170-175, 2004.
[16] Y. Wang, J. Yang and Y. Zhou, ”Region partition and feature matching
based color recognition of tongue image”, Pattern Recogn. Lett., 28(1),
pp. 11-19, 2007.
[17] B.C. Zhang, S.G. Shan, X.L. Chen and W. Gao, ”Histogram of Gabor
phase patterns(HGPP): a novel object representation approach for face
recogniton”, IEEE Trans. Image Processing, 16, pp. 57-68, 2007.
[18] D. Zhang, Z. Liu, J. Yan and P. Shi, ”Tongue-print: A novel biometrics
pattern”, ICB 2007, Springer-Verlag LNCS 4642, pp. 1174 - 1183, 2007.
10
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-487-9
SIGNAL 2016 : The First International Conference on Advances in Signal, Image and Video Processing

