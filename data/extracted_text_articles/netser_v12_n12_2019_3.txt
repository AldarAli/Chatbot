21
International Journal on Advances in Networks and Services, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/networks_and_services/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
Optimal Control of Multicasting Multimedia Streams in Cloud Data Centers 
Networks 
Nader F. Mir, Abhishek Rao, Rohit Garg, Mitesh Ambavale, and Raviteja Ainampudi 
Department of Electrical Engineering 
San Jose State University, California 
San Jose, CA, 95195, U.S.A 
email: nader.mir@sjsu.edu 
 
 Abstract - The objective of this paper is a thorough study on the 
optimal control of multicasting multimedia streaming over the 
components of data center networks.  Such a network control 
includes finding the best locations of traffic multicasting, assigning 
the quality of service (QoS) while multicasting, and the right 
network devices for this objective. Since streaming media accounts 
for a large portion of the traffic in networks, and its delivery 
requires continuous service, it is essential to analyze the traffic 
sources and investigate the network performance. IP multicasting 
technology adds tremendous amount of challenge to a network as 
the streamed media delivered to users must be multiplied in 
volume. In this paper, we first demonstrate a study of the 
complexity and feasibility of multicast multimedia networks by 
using different multicast protocols and video sources. In our 
proposed method, we then create peer-to-peer (P2P) and data 
center topologies in order to analyze the performance metrics. The 
implementation and evaluation of the presented methodology are 
carried out using OPNET Modeler simulator and the various built-
in models. Further, we implement performance tests to compare 
the efficiency of the presented topologies at various levels. At the 
end of the paper we analyze the optimal locations for multicasting 
multimedia streaming traffic.  
Keywords - video streaming; cloud data centers; multicast; 
multimedia; performance evaluation; video codecs; IP; MPLS.  
I. 
INTRODUCTION AND BACKGROUND   
In [1], an efficient video-based packet multicast method for 
multimedia control in cloud data center networks was 
presented.  The current paper extends that work and conducts a 
thorough study toward several other aspects, such as 
Comparison of throughput and delay at switching nodes on the 
control of multicasting multimedia streaming over the 
components of data center networks. 
Over the years, major development in the industry have been 
involved in the integration of various multimedia applications. 
Delivery of streaming media (video on demand), e-learning with 
minimum delay and highest quality has been one of the major 
challenges in the networking industry. Video service providers, 
such as Netflix, Hulu are constantly changing the architecture in 
order to service these needs. These service providers face stiff 
competition and pressure to deliver the next generation of 
streaming media to the subscribers. The next generation media 
can be divided into categories: real-time and non-real time. 
Examples of real time can be live streaming and video 
conferencing and non-real time can be e-learning and video on 
demand [2].  The next generation of streaming media [3] 
involves a large number of subscribers whose delivery is closer 
aligned with the latest protocols than with the traditional 
systems. In such cases, it is required that the service providers 
upgrade their infrastructure and support them [4].  
One of the main challenges in the multimedia industry that 
motivates us to look into it in this paper is multicasting the video 
streams. IP Multicast is one of the major techniques that can be 
used for efficient delivery of streaming multimedia traffic to a 
large number of subscribers simultaneously. Group membership, 
unicast and multicast routing protocols are mainly required for 
multicast communications [5]. Inter Group Membership 
Protocol (IGMP) utilized in our study maintains one of the most 
commonly used multicast protocols at user facility site. IGMP is 
used to obtain the multicast information in a network. Unicast 
routing protocols can be either distance vector or link state; the 
latter being preferred due to the dynamic reaction of these 
protocols to changes in topology. Multicast routing protocols can 
be integrated with the unicast routing protocol or can be 
independent of them. Protocols, such as the Multicast Open 
Shortest Path First (MOSPF), depend on the underlying unicast 
protocols used, whereas protocols such as Protocol Independent 
Multicast (PIM), are independent of the type of unicast routing 
protocols used. A combination of IGMP, MOSPF and PIM in 
sparse mode or dense mode can be used for successful 
implementation and efficient delivery of multimedia traffic in 
networks [6]. 
Multicast routing enables transmission of data to multiple 
sources simultaneously. The underlying algorithm involves 
finding a tree of links connecting to all the routers that contain 
hosts belonging to a particular multicast group. Multicast 
packets are then transmitted along the tree path from the source 
to a single destination or a group of receivers belonging to a 
multicast group. In order to achieve the multicast routing tree, 
several approaches have been adopted. Group-shared tree, 
source based tree and core based tree are some which are 
explained here.   
a. Group-based tree: In this approach a single routing tree 
is constructed for all the members in the multicast group;  
b. Source-based tree: This involves constructing a separate 
routing tree for each separate member in the multicast 
group. If multicast routing is carried out using source-

22
International Journal on Advances in Networks and Services, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/networks_and_services/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
based approach, then N separate routing trees are built 
for each of the N hosts in the group [7]; and  
c. Core-based tree: This is a multicast routing protocol, 
which builds the routing table using a group-shared tree 
approach. The tree is built between edge and core routers 
in a network, which helps in transmitting the multicast 
packets. 
MOSPF and PIM use one of the above mentioned 
approaches in the transmission of packets. As PIM is the 
multicast routing protocol used in the implementation, we 
discuss the working of PIM. 
PIM is a multicast routing protocol that is independent of the 
underlying unicast routing protocols used [8]. PIM works in 
two modes dense mode and sparse mode. In the former mode 
the multicast group members are located in a dense manner and 
the latter approach has the multicast group members distributed 
widely. PIM uses Reverse path forwarding (RPF) technique in 
dense modes to route the multicast packets. In dense mode, RPF 
floods packets to all multicast routers that belong to a multicast 
group whereas in a sparse mode PIM uses a center based 
method to construct the multicast routing table. PIM routers 
which work in sparse mode send messages to a center router 
called rendezvous point. The router chosen to be rendezvous 
point transmits the packets using the group based tree model. 
As seen in Fig. 1, the rendezvous point (RP) can move from a 
group-based tree model to a source-based approach if multiple 
sources are specified [9][10].  
 
A broadband network is a communication infrastructure that 
can provide higher bandwidth services. Regional networks are 
connected to such broadband backbone networks to form the 
Internet. Internet Service Providers own the regional broadband 
networks. A broadband network is required to support the 
exchange of multiple types of information such as, massive data 
storage access, voice over IP (VoIP), video streaming, and live 
multicasting, while satisfying the performance requirements of 
each application. In short, the delay and jitter should be minimum 
for better performance of these applications.  
 
Figure 1. Sample diagram of multicast routing 
 
The above-stated requirements are satisfied by employing 
broadband routers and switches, fiber optic cables, and tunneling 
mechanisms. Tunneling makes the communication faster in 
broadband networks compared to normal routing mechanism. 
Multiprotocol 
label 
switching 
(MPLS) 
is 
networking 
infrastructures used in a high-speed backbone network to provide 
a better quality of experience for broadband applications such as, 
video streaming and other real-time applications. 
In any MPLS network, the routers at the edge of the network 
are the most complex ones. In edge routers, user services such 
as policies, rate limiters, logical circuits, and address assignment 
are created. In a certain connection between a pair of users, edge 
routers keep a clean separation between a complex edge and 
create services while routers in the middle mainly do basic 
packet forwarding by switching MPLS packets from one 
interface to the other.  The MPLS header (also known as the 
label) is imposed between the data link layer (layer 2) header 
and network layer (layer 3) header.   
MPLS adds some traditional layer 2 capabilities and 
services, such as traffic engineering, to the IP layer. The 
separation of the MPLS control and forwarding components 
has led to multilayer, multiprotocol interoperability between 
layer 2 and layer 3 protocols. MPLS uses a small label or stack 
of labels appended to packets and typically makes efficient 
routing decisions. Another benefit is flexibility in merging IP-
based networks with fast-switching capabilities.  
In MPLS networks, any IP packet entering the MPLS 
network is encapsulated by a simple header called a label. The 
entire routing is therefore based on the assignment of labels to 
packets. The compelling point of MPLS is that this simple label 
is always processed for routing instead of the IP header in the 
network. Note that labels have only local significance. This 
fact removes a considerable amount of the network-
management burden.  
The rest of this paper is organized as follows: Section II 
provides a detail of our architecture and its functionality and 
Section III presents a performance analysis of the designed 
architectures. Finally, Section IV concludes the paper.  
 
II. NETWORK ARCHITECTURE  
Network architecture has been designed from service 
provider’s and user’s perspective. Network service providers 
are concerned with the available bandwidth and utilization of 
resources whereas end user’s main concern is with the delivery 
of streaming media with lowest time and maximum efficiency. 
In order to obtain the various parameters that are required for 
the best design of multimedia network, two network models 
were implemented and analyzed.  
A. Implemented Peer to Peer (P2P) Network Design 
Peer to peer network model is a distributed architecture where 
the application is transmitted between source and destination 
through peers. Applications such as music sharing, file sharing 
use peer-to-peer network model for transmitting the data. A 
peer-to-peer network was built using the values as shown in 
Table I. The network architecture shown below represents an 
organizational division where the admin department is the 
source of multimedia traffic, which is simultaneously streamed 
to the remaining departments namely the HR, finance and IT. 
The topology contains two backbone routers connected back-
to-back, a video streaming source is configured and stored in 
the admin department, where the video frames are encoded with 

23
International Journal on Advances in Networks and Services, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/networks_and_services/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
a H.264 codec and generating a frame rate of 15-20 frames per 
sec. The backbone routers are configured with PIM-DM as the 
multicast protocol that is responsible to carry multicast packets.  
 
TABLE I. CONFIGURATION PARAMETERS FOR A PEER TO PEER NETWORK 
DESIGN 
Link speed 
(in 
Mbps) 
Frame size 
Frame 
interarrival 
rate 
Video 
Codec 
Multicast 
protocol 
used 
100 
128x120 
10 fps 
H.264 
PIM-DM 
100 
128x240 
15fps 
H.264 
PIM-DM 
1000 
352x240 
30 fps 
H.264 
PIM-DM 
 
B. Implemented Data Center Topology  
The data center and its network testbed topology [12] 
implemented in this paper are shown in Fig. 2. A data center 
contains certain facilities for computing, data storage, and 
other technology resources, as shown by “server racks.” In the 
network testbed topology, the interconnections among the 
switches are regular. The figure shows a data center network 
using four layers of switches as two layers of core switches, one 
layer of aggregate switches, and one layer of edge switches. An 
edge switch, also called top-of-rack (ToR) switch, directly 
connect to end servers at server racks, and core switches directly 
connect to the routers such as Router1 and Router2 that are 
attached to the outside of the data center or directly to the 
Internet. In this figure, two groups of 6 destinations are 
considered in the testbed: group A destinations and group B 
destinations. 
 
 
Figure 2. Data center topology as the testbed for multimedia streaming 
 
The topology has been implemented taking into account 
redundancy at all levels, and responds dynamically to failures 
at link, path and device level. Scaling the number of nodes, both 
horizontally and vertically has been considered in order to 
analyze the performance metrics of the network. Streaming 
media content stored at the servers are configured for varying 
bit rates and varying frame sizes.  
The OPNET simulator has been used as the simulation tool 
for implementing and testing multicast multimedia traffic.  The 
detailed metrics that are used for data center implementation 
has been shown in the Table II.   
 
TABLE II. CONFIGURATION PARAMETERS FOR A DATA CENTER 
Number of servers per rack  
2 
Number of TOR switches used per rack  
2 
Number of distribution switches per rack  
2 
Number of core switches per rack  
1 
Total number of servers  
8 
Total TOR Switches  
8 
Total distribution switches  
4 
Total Core switches  
2 
Link speeds in data centers  
1000 Mbps 
Link speeds to WAN   
PPP DS3 
Video Application and codec used  
Video streaming, 
H.264 
Frame sixe  
Constant (5000) 
Bit rates  
Constant (10 fps)  
             
III. 
PERFORMANCE ANALYSIS  
The configuration parameters for used for the performance 
evaluation are shown below in Table III. Since backbone 
routers are majorly involved in the transmission of traffic over 
the internet, Ethernet load across these links has been 
considered. As the frame size increases load across the 
backbone links increases, which leads to increase in the 
delivery of media to destination. 
 
TABLE III. VIDEO CONTENT CONFIGURATION PARAMETERS 
Test 
Name  
Frame Size 
(in bytes)  
Video  
Codec  
Used  
Frame  
Inter-arrival 
Time  
Ethernet Load 
Across the Link 
(packets/sec)  
Video1  
   15,360 
H.264  
10 Frames/sec 
280  
Video2  
5,000  
H.264  
Exponential  
530 
 
A. Study on Ethernet Load 
In order to reduce the end to end delay, latency and prioritized 
traffic using quality of service (QoS) was implemented. Opnet 
simulator has various built-in QoS profiles, some of them being 
WFQ, FIFO, priority queueing.  Differentiated services code 
point based QoS is being used in this implementation wherein 
based on the priority of traffic delivery, a certain level of service 
is configured depending on which resources are allocated along 
the path of delivery.  

24
International Journal on Advances in Networks and Services, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/networks_and_services/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Now, we present the Ethernet load test – a performance 
metric which determines the amount of data packets that are 
carried by the network. Although each link in the network 
carries data packets WAN / core routers are chosen for analysis. 
In peer-to-peer topology mentioned earlier, the links connecting 
the backbone routers are considered, whereas in a data center 
topology core router links/WAN links have been chosen.  
The variation in the graph can be explained as follows. In 
this case the bit rate and frame size s, both have been kept as 
exponential increasing functions. From Fig. 3 it can be 
observed that in a two-node network since there is a single link 
connecting the backbone routers, Ethernet load across these 
links is considerably higher than that of a multi node model 
where, PIM builds a tree structure (source based, or center 
based) for sending the multicast packets. As a result, the load is 
distributed across various links thereby reducing the failure 
percentage. One more alternative that can be used is port-
channel can be configured to distribute the load across the links 
connecting the routers. Over the time considered it was 
observed that the load was higher in a two-mode network and 
lesser in a multi node network.   
 
  
Figure 3. Comparison of Ethernet load between two nodes and multi-mode 
cases  
Our next experiment is concerned with the queueing delay 
which is the amount of time that a packet waits in the router’s 
queue before being sent onto the network. This is one of the 
most important parameters for multicast networks as an 
increase in the queueing delay can cause significant delay in the 
transmission of packets across the network. Queueing delay can 
be due to many factors, such as buffer size in a router, router’s 
processing capacity, link speed used, number of hops from 
source to destination. In this analysis, the queueing delay has 
been analyzed for a two node and a multi-node environment. 
From the graphs shown in Fig. 4, it can be observed that 
although in a two node network links of higher speed were used, 
when packets of multiple applications arrive, a two-node 
network experienced significant queueing delay which led to 
the delay in the transmission of packets. Since no QoS was 
configured all the packets were serviced based on packet arrival 
times. The graph for a 2-node network shows peaks of highest 
queueing delay and lows of least queueing delay. This is due to 
the fact that when packets related to multiple applications arrive 
there has been peaks of high queue delay and when packets 
related to single applications arrive less queueing delay has 
been experienced. In order to have less queueing delay priority 
traffic can be classified based on QoS policies which helps in 
serving these packets better.  
 
  
Figure 4. Comparison of queueing delay between two nodes and multi-node 
cases 
Next, we consider a test on QoS, a mechanism which is used 
to analyze the performance of networks. QoS policies 
configured ensures traffic prioritization and reservation of 
resources along the path from source to destination. QoS plays 
a major role in multimedia networks where defining QoS 
policies defines the traffic priority when real time multimedia 
traffic and interactive media is involved. Since these types of 
traffic have rigid delay constraints defining QoS policies for 
these types can result in prioritizing them when requests for 
other traffic are in queue.  
Simulation results of QoS implementation is shown in Fig. 
5. Since real time interactive media could not be created in a 
simulation environment, two video sources (video1 and video2) 
were created and video1was configured with a WFQ QoS 
profile traffic group of video 1 being set to high priority and 
traffic group of video 2 being set to best effort with no QoS 
configured. From the plots in the figure, it can be observed that 
over a period of time when requests arrive for video1 and 
video2 packets requesting information, video1 is serviced with 
less packet delay than those packets for video2 while multicast 
flow is also included in the configuration. Since the IGMP 
convergence time was 2 min the QoS traffic servicing has 
started after the first few minutes.  
Finally, the latency is our last performance metric to focus 
on. The latency is the amount of delay involved in transmitting 
the data from source to destination. For calculating the Latency 
issues in network different pixel sizes were chosen for analysis. 
Three different Pixel sizes were configured over a period of 
time with link speed and other parameters being kept constant.   
The link speed was defined to be 100 Mbps and pixel sizes of 
352x240, 128X240 and 128X120 were defined with frame 
interarrival rates to be logarithmic. After several tests it was 

25
International Journal on Advances in Networks and Services, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/networks_and_services/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
observed that the latency in the transmission of a high-quality 
video was more compared to the latencies of the transmission 
of a video of lesser resolution as shown in Fig. 6. If a video of 
high quality has to be transmitted in minimum time, then 
separate channels can be used for high definition video where 
source specific trees can be used for routing thereby achieving 
successful routing of packets.  
 
  
Figure 5. QoS servicing of priority and non-priority traffic  
 
 
Figure 6. Comparison of latency of various frame sizes.  
B. Study on Network Infrastructure for Media Multicasting 
An MPLS network consists of nodes called label switch 
routers (LSRs). An LSR switches label packets according to 
its particular forwarding tables. The content of a forwarding 
table consists of labels to be assigned to a flow of traffic. An 
LSR has two distinct functional components: a control 
component and a forwarding component. The control 
component also facilitates the exchange of information with 
other LSRs to build and maintain the forwarding table. 
A typical advanced router or switch has routing tables in its 
control plane and an IP forwarding table in its data plane for 
routing purposes. The routing table of such a device receives 
signaling packets such as RIP or OSPF packets and must 
update its IP forwarding table occasionally. An LSR has a 
separate forwarding table to store labels. The MPLS 
forwarding table interacts with the IP forwarding table in order 
to arrange the conversion of IP address to labels or vice versa. 
Fig. 8 shows a basic comparison of streaming in IP and 
MPLS networks. In Fig. 7 (a), a typical IP network is shown 
where a source host, as host 1, connects to a destination host, 
as host 2. The generated IP packets enter a wide-area IP 
network at edge router R1 and pass over the network using all 
the relative routing protocols, and eventually reach edge router 
R2 from which they exit. In Fig. 7 (b), an MPLS-enabled 
network is contrasted with the IP network; any IP packet 
entering the MPLS network is encapsulated by a label. The 
entire routing is therefore based on the assignment of labels to 
packets. We will see in the next sections how this technique 
would substantially reduce the time for packet processing and 
routing. 
Assigning labels to each packet makes a label-swapping 
scheme perform the routing process efficiently and quickly. In 
Fig. 7 (b), the edge LSRs of the MPLS network are ingress 
LSR1 and egress LSR2. It can be seen that a label is indeed a 
header processed by an LSR to forward packets. The header 
format depends on the network characteristics. LSRs read only 
labels and do not engage in the network-layer packet headers. 
One key to the scalability of MPLS is that labels have only 
local significance between two devices that communicate. 
When a packet arrives, the forwarding component uses the 
label of the packet as an index to search the forwarding table 
for a match. The forwarding component then directs the packet 
from the input interface to the output interface through the 
switching fabric. 
 
Host 2
Host 1
IP Network
IP Packet
MPLS Tunnel
IP Packet
IP Packet
Host 2
Host 1
MPLS Network
IP Packet
IP Packet
Label
(a)
(b)
R1
R2
LSR1
LSR2
 
Figure 7. Operations of streaming in MPLS compared with IP infrastructures 
 
The advance mutual agreement between two adjacent LSRs on 
a certain label to be used for a route is called label binding. 
Once an IP packet enters an MPLS domain, the ingress LSR 
processes its header information and maps that packet to a 
forward equivalence class (FEC). Any traffic is thus grouped 
into FECs. An FEC indeed implies that a group of IP packets 
are forwarded in the same manner—for example, over the 
same path or with the same forwarding treatment. A packet can 
be mapped to a particular FEC, based on the following criteria:   

26
International Journal on Advances in Networks and Services, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/networks_and_services/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
▪ Source and/or destination IP address or IP network 
addresses  
▪ TCP/UDP port numbers  
▪ Class of service  
▪ Applications  
A label switched path (LSP) through the network must be 
defined, and the QoS parameters along that path must be 
established. An LSP resembles a tunnel. Label usage for 
identifying the next hop destination instead of the IP 
destination address adds superior capabilities like traffic 
engineering to the traditional IP routing. LSRs consist of two 
functional components; a control component and a forwarding 
component. The control component uses standard routing 
protocols to exchange information between the LSRs and 
facilitates the forwarding table formation. Based on the 
information in the forwarding table, the forwarding component 
of the LSR performs the switching of label packets.  
OPNET 
Modeler 
provides 
a 
global 
development 
environment capable of infrastructure networks, such as 
MPLS, modeling and performing discrete event simulations.  
Data collection and analysis, incorporated along with the 
design simulation, helps in evaluating and optimizing real life 
networking topologies. The advantages of OPNET Modeler 
over other modelers include a simple GUI interface and 
network modules integrating extensive protocol suite with 
queuing functionalities.  
The ‘System in the Loop’ module included in this Modeler 
provides an efficient method to capture live transmission and 
analyze the behavior of different real-life scenarios. 
Applications include LAN and WAN performance modeling, 
network planning and protocol research.  
The study on the type of network infrastructure for media 
streaming involves the performance evaluation of video 
streaming using MPLS and OPNET Modeler. The OPNET 
Modeler has the modules to implement the MPLS over the 
basic network topology. Creating dynamic and static LSPs, 
Traffic engineering, and Differential Service functionality 
could be achieved using the Modeler functionalities. The main 
steps involved in implementing MPLS using the simulator are: 
 
Configuring “MPLS config” Object 
 
Establishing MPLS Path Model 
 
Configuring MPLS parameter on MPLS routers 
 
Define Neighbor configuration 
 
Establish the Traffic mapping configuration 
 
Configuring IP Traffic Demand 
 
Configure Routing Protocol 
 
Update MPLS LSP details 
Fig. 8 shows a global network perspective for the study on 
the backbone networking infrastructure used in the simulator. 
The network topology consists of the following elements: 
LER1, LSR1, LSR2, LSR3, LSR4, LSR5, LSR6, LER2, and 
LER2 are Cisco 7200 series routers, which support MPLS 
protocol along with the standard routing protocols such as, Open 
Shortest Path First (OSPF) and Routing Information Protocol 
(RIP). 
 
 
Figure 8. Global network for the study on the backbone networking 
infrastructure 
 
Two subnets were considered in the simulator: San Francisco 
Bay area subnet and Bangalore subnet. The San Francisco Bay 
area subnet consists of a Cisco 4000 series router and an 
Ethernet workstation capable of video conferencing. The 
Bangalore subnet consists of a Cisco 4000 series router and an 
Ethernet workstation capable of video conferencing. Unique 
IPv4 addresses were assigned to each node in the network. All 
routers in the main subnet are connected via Point-to-Point DS3 
links with a data rate of 4.736 Mbps. The router and Ethernet 
workstation in each subnet are connected using 10Gbps 
Ethernet links. The simulation objects used in this project are 
the following: 
1. Application Configuration Object 
2. Profile Configuration Object 
3. MPLS Configuration Object 
4. IP Traffic Flow Object 
An application configuration object is used in the 
simulation to define the parameters for the video conferencing 
application. This object is a video conferencing application so 
that it receives the highest priority among all the other traffic 
in the network.  
In comparison with the MPLS traffic, we also used an IPv4 
traffic flow object that represents the IP layer traffic flow 
between a specified source and a destination. This object is 
used to create background traffic in the network. Using this 
object, a background traffic flow at the approximate rate of 
40.5 Mb/s is created. Fig. 9 shows the sample configuration for 
IP traffic flow. The background traffic flow starts when the 
simulation starts and continues at this rate for 3600 simulation 
seconds. This background traffic flow is created between all 
nodes in the parent subnet. Thus, the background traffic flows 
between the nodes LER1-R3 along the two paths LER1 - LSR1 
- LSR2 - LSR3 - LER2 - R3 and LER1 - LSR4 - LSR5 - LSR6 
- LER2 - R3.  
The profile configuration object is another object used to 
create a user profile named “video.” This profile was then 
specified on different nodes in the network to generate 

27
International Journal on Advances in Networks and Services, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/networks_and_services/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
application layer traffic. Sample configuration for the object. 
The video conferencing starts 150 seconds after the start of the 
simulation and continues until the end of the simulation. After 
every 60 seconds, a new video conferencing session will be 
created between the two workstations. 
 
 
Figure 9. Traffic flow configuration 
 
Finally, MPLS configuration object is used to define the 
Forward Equivalency Class (FEC) and traffic trunk profiles. 
The FEC defined for the video traffic is the Video_FEC. The 
Type of Service (ToS) or Differentiated Services Code Point 
(DSCP) is set to AF41 for this FEC. Hence, this FEC will be 
used only by the incoming IP datagram that has the ToS or 
DSCP field set to AF41. In this study, the video conferencing 
application has been configured in such a way that the hosts 
send the IP datagram for the application with DSCP bit set to 
AF41. Fig. 10 shows the sample configuration for the Trunk 
Profile. 
The delay increased rapidly from 0.5 seconds to 1.05 
seconds. After this point, the delay increased linearly and 
reached 2.3 seconds.  
The comparison of delay in video conferencing in MPLS 
and IP networks using OSPF specifications is shown in Fig. 11. 
When the video conferencing is done over traditional IP 
networks, the delay is found to be increasing linearly with time 
and reached a peak value of 10.6 seconds. When the same 
video conferencing session is simulated over MPLS network 
with single LSP, the delay increased gradually to 5 seconds and 
remained in this range until the end of the simulation. When 
two LSPs are used for video streaming, there is a significant 
decrease in the delay. The delay remained under two seconds 
for the most part of the simulation. 
 
C. Study on the Location of Multicasting Function 
Consider again the testbed of Fig. 2. In that figure, two groups of 
6 destinations are considered in the testbed: group A destinations 
and group B destinations. Group A destinations is 
 
 
Figure 10. Traffic trunk profile 
 
 
Figure 11. Comparison of end-to-end delay in video conferencing using IP and 
MPLS 
directly connected to the data center network passing through 
Router1, while group B destination is connected to the data 
center network after passing through a local router, indicated by 
“Router,” the Internet, and Router2 of the center.   
In the first scenario, a video conferencing between two 
workstations located in San Jose and Bangalore is simulated. 
The routing protocol used in the scenario is IP using Open 
Shortest Path First (OSPF) specifications and is configured to 
run on all nodes in the network.  Fig. 12 shows the end-to-end 
packet delay and jitter when video conferencing simulated over 
the network with the routing protocol OSPF/IP configured on 
all the nodes. Similarly, Fig. 13 shows the end-to-end packet 
delay and jitter when video conferencing simulated over the 

28
International Journal on Advances in Networks and Services, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/networks_and_services/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
network with the routing protocol OSPF/MPLS configured on 
all the nodes indicating an improvement over the OSPF/IP case 
on both delay and jitter. 
The Trunk profile used in this paper is AF41_Trunk. The 
AF41_traffic class is mapped on to this traffic trunk. The 
maximum bit rate and average bit rate are set to 44,000,000 
bits/sec. The maximum burst size and the peak burst size are 
set to 44,000,000 bits.  
 
 
Figure 12. Delay and jitter in video conferencing using OSPF/IP 
 
 
Figure 13. Delay and jitter in video conferencing using OSPF/MPLS 
 
Fig. 14 shows the result of performance evaluation on the 
throughput for group A destinations. We picked three 
destinations, Dest. 1, 2, and 3. Then we deployed three different 
experiments, each considering one of nodes 2, 3, and 5 as the 
location of the multicasting multimedia streaming. This study 
clearly shows, that the throughput of multicast at the highest 
switching nodes (node 5) is highest with an average of 900,000 
packets/sec, whereas the one for switching node 2 is the lowest 
with the average of 110,000 packets/sec. This experiment 
teaches the multicasting live multimedia must be carried out at 
the highest levels of the data center switching structures to 
return the best performance.    
The results of the second experiment to determine the best 
location of multicasting multimedia is shown in Fig. 14. In this 
experiment, we consider group B destinations in Fig. 3. The 
study is conducted on Router2 which is attached to the cloud 
center and Router which is attached to the destinations.  The 
study evidenced by Fig. 15 shows that the queueing delay (QD) 
at the Router is lower than the one in Router2.  What this result 
teaches us is that the streaming must also be multicast at the 
closest routing stage to the destinations.   
 
 
Figure 14. Comparison of throughput at switching nodes 2, 3, and 5 
 
 
Figure 15. Comparison of queueing delay (QD) at source, and 
intermediate routers 
IV. CONCLUSION  
In this paper, we conducted a thorough study on the control 
of multicasting multimedia streaming over the components of 
data center networks.  We focused on finding the best locations 
of traffic multicasting, assigning the quality of service (QoS) 
while multicasting, and the right network devices for this 
objective. we designed and implemented peer to peer and data 

29
International Journal on Advances in Networks and Services, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/networks_and_services/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
center topologies under QoS restrictions and multicast 
requirement of streaming traffic. The two topologies were 
implemented for various video streaming applications such as 
video conferencing and video streaming. The parameters of 
these video sources were changed in to measure the 
performance metrics of the multicast networks. Parameters 
such as video codecs, frame size, frame interarrival rate, link 
speed, QoS were changed for analysis. From the analysis it was 
observed that a multitier architecture connected to high speed 
links was best suited for high end real time traffic. Further it 
was observed that the QoS configuration for these real time 
traffic reduces the packet end to end delay and the latency of 
these packets was also less as compared to other packets. 
Building a multitier not only helped in better load distribution 
of traffic across links but also this type of topology was better 
equipped to handle failures at device, links and server levels. 
This paper also covered the different multicast routing 
protocols that can be used. At the end of the paper we analyzed 
the optimal locations for multicasting multimedia streaming 
traffic. We learned that the streaming must also be multicast at 
the closest routing stage to the destinations. 
REFERENCES  
[1] 
N. F. Mir, A. T. Rao, and R. Garg, “Efficient Video-Based Packet 
Multicast Method for Multimedia Control in Cloud Data Center 
Networks,” Proceedings of 13th International Conference on Systems 
and Networks Communications (ICNSC2018), Nice, France, October. 
14-18, 2018. 
[2] 
K. Oe, A. Koyama, and L. Batolli, “Performance evaluation of multicast 
protocols for multimedia traffic,” IEEE Xplore document, IEEE 27th 
International Conference on Advanced Information Networking and 
Applications (AINA), 2013. 
[3] 
T. Do, K. Hua, and M. Tantaoui, “P2VoD: Providing fault tolerant 
video-on-demand streaming in peer-to-peer environment.” Proc. of the 
IEEE ICC 2004. Paris: IEEE Communications Society, 2004.  
[4] 
B. Cheng, L. Stein, and J. Hetal, “Grid Cast: Improving peer sharing for 
P2P 
VOD,” 
ACM 
Transactions 
on 
Multimedia 
Computing, 
Communications, and Applications, vol. 4, 2008. 
[5] 
B. Quinn and K. Almeroth, “IP Multicast Applications: Challenges and 
Solutions.” RFC, 2001, ACM Digital Library. 
[6] 
“OPNET Technologies Inc. OPNET Modeller Product Documentation 
Release 14.5,” OPNET Modeller, 2008.  
[7] 
X. Sun and J. Lu, “The Research in Streaming Media OnDemand 
Technology based on IP Multicast,” 3rd International Conference on 
Computer and Electrical Engineering, 2010.  
[8] 
http://www2.ic.uff.br/~michael/kr1999/4-network/4_08-mcast.htm, 
Access date: April 2019.  
[9] 
Z. Bojkovic, B. Bakmaz, and M. Bakmaz, “Multimedia Traffic in New 
Generation Networks: Requirements,” Journal of Control and Modeling, 
2016.  
[10] S. Khanvilkar, F. Bashir, D. Schonfeld, and A. Khokhar, “Multimedia    
Networks and Communication,” 2015 
[11] X. Wang, C. Yu, and H. Schulzrinne, “IP Multicast Simulation in 
OPNET,” Semantic Scholar, 2015.  
[12] N. Mir, “Computer and Communication Networks,” Pearson Prentice 
Hall, 2nd Edition, 2014. 
. 
  

