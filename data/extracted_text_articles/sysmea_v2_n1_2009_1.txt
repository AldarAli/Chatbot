 
The Sampling Theorem for Finite Duration Signals 
 
 
 
Subhendu Das, CCSI, West Hills, California, subhendu.das@ccsi-ca.com 
Nirode Mohanty, Fellow-IEEE, CCSI, West Hills, California, nirode.mohanty@ccsi-ca.com 
Avtar Singh, San Jose State University, San Jose, California, avtar.singh@sjsu.edu 
 
 
Abstract 
 
The Shannonâ€™s sampling theorem was derived 
using the assumption that the signals must exist over 
infinite time interval. But all of our applications are 
based on finite time intervals. The objective of this 
research is to correct this inconsistency. In this paper 
we show where and how this infinite time assumption 
was used in the derivation of the original sampling 
theorem and then we extend the results to finite time 
case.  Our research shows that higher sample rate is 
necessary to recover finite duration signals. This 
paper validates, with detailed theory, the common 
industrial practice of higher sample rate. We use the 
infinite dimensionality property of function space as 
the basis of our theories. A graphical example 
illustrates the problem and the solution. 
 
Keywords: 
Sampling 
methods, 
Communication, 
Linear system, Wavelet transform, Modulation. 
  
1. Objective 
 
This paper is an extended version of [1]. It 
provides more details of the theories and presents 
many related ideas including the re-sampling process. 
The objective of this paper is to extend the original 
sampling theorem [2] to finite duration signals. It is 
shown here that the proof of the Shannonâ€™s sampling 
theorem assumed that the signal must exist for infinite 
time. This assumption came because the proof used 
Fourier transform theory which in turn uses infinite 
time. We give a new proof that does not require 
infinite time assumption and as a result of elimination 
of this assumption we get a new theory. 
Our research shows that more you sample more 
information you get about the signal when your signal 
measurement window is finite. We provide some 
theoretical analysis to justify our results. A very 
fundamental and well known concept in mathematics, 
infinite dimensionality of function space, is used as a 
basis of our research. Thus the main focus of the paper 
is on sampling theorem and on the number of samples. 
Since the result establishes a new view in signal 
processing, we apply the result to few other areas like 
signal reconstruction and up-down sampling. 
In engineering practice most of the applications 
use two to four times the Nyquist sample rate. In audio 
engineering much higher rate is used [3]. So the 
results of this paper are not new ideas in the practical 
world. However, this engineering practice also points 
out that there is something wrong somewhere in our 
theory. There is also this (mis)conception that higher 
sample rate provides redundant information. Therefore 
we examine the core issues and assumptions behind 
the original theory of [2], make some changes, and 
provide a theoretical proof of the high sample rate 
concept. It should be noted that the theory in [2] is not 
wrong, we are only changing one of the assumptions 
that is more meaningful in the present technology. 
Besides sampling theorem, another objective is to 
highlight the infinite time assumption behind the 
existing theories. This infinite time assumption is not 
practical in engineering. Thus we emphasize the 
infeasibility of the approaches based on transfer 
function and Fourier transform. All of them use 
infinite time assumption. In the past many engineers 
have rejected these approaches because they are useful 
for only Linear Time Invariant (LTI) systems. Now we 
have another reason â€“ the infinite time assumption. 
Interestingly enough, we show that LTI systems do not 
exist in engineering. 
This research leads us to realize that the concept 
of finite time duration of signals is the backbone of all 
our engineering systems. Therefore we need to do 
something about it, i.e., we should start a research in 
reducing these inconsistencies between the theory and 
the practice. Eventually, if we can successfully 
provide a new direction, then our technology will be 
more predictable and reliable. We may get significant 
product quality improvements. It may also be possible 
to reduce waste and thus help to create a greener 
technology [4]. 
In this paper our objective is not really to make a 
big jump in this new research on finite time direction 
1
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
but occasionally we have touched upon the various 
related topics, problems, and solutions. We believe 
that this is an important area of investigation even in 
mathematics. It should be noted though that all time 
domain approaches are closer to finite time reality. 
However unless we create or change some basic 
engineering definitions all our theories will remain 
somewhat inconsistent and unsatisfactory. 
During the publication process of this research 
many colleagues and reviewers have made many 
comments and questions on the subject of this paper. 
We have tried to include our answers to many of them. 
As a result, the paper got little bit defocused from its 
original goal and the contents got diluted. We hope the 
integration of all these subjects still maintains some 
coherency and novelty. 
The contents of this paper can be described using 
the following high level summary. We first show, in 
Section 2, that infinite time assumption is not really 
needed in engineering. Then we present a new 
modulation method, in Section 3, and show how we 
encountered this infinite time issue in a practical 
engineering problem. To solve the problem over finite 
time and to provide its theoretical foundation we 
discuss in details the concept of infinite dimensionality 
of function space in Section 4. Using this infinite 
dimensionality concept, in Section 5, we show that 
finite rate sample representations actually converge to 
the original function as rate increases to infinity. In 
Section 6, we provide new proofs of the original 
sampling theorem and provide a numerical example in 
Section 7. We also discuss briefly using a numerical 
example, in Section 8, how approaches based on 
analytical expressions rather than samples can help to 
resample a finite duration signal. Finally, in Sections 9 
and 10, we discuss the nonlinear nature of engineering 
systems and explain why time domain approach with 
high sample rates is more meaningful. 
 
2. Infinite Time 
 
In this section we show that the assumption of 
infinite time duration for signals is not practical and is 
not necessary for our theories. In real life and in all 
our engineering systems we use signals of finite time 
durations only. Intuitively this finite duration concept 
may not be quite obvious though. Ordinarily we know 
that all our engineering systems run continuously for 
days, months, and years. Traffic light signaling 
systems, GPS satellite transmitters, long distance air 
flights etc. are some common examples of systems of 
infinite time durations. Then why do we talk about 
finite duration signals? The confusions will be cleared 
when we think little bit and examine the internal 
design principles, the architecture of our technology, 
and the theory behind our algorithms. Originally we 
never thought that this question will be asked, but it 
was, 
and 
therefore 
we 
look 
here, 
at 
the 
implementations, for an explanation. 
The computer based embedded engineering 
applications run under basically two kinds of operating 
systems (OS). One of these OS uses periodic 
approaches. In these systems the OS has only one 
interrupt that is produced at a fixed rate by a timer 
counter. Here the same application runs periodically, 
at the rate of this interrupt, and executes a fixed 
algorithm over and over again on input signals of fixed 
and finite time duration. As an example, in digital 
communication engineering, these signals are usually 
the symbols of same fixed duration representing the 
digital data and the algorithm is the bit recovery 
process. Every time a symbol comes, the algorithm 
recovers the bits from the symbol and then goes back 
to process the next arriving symbol.  
Many core devices of an airplane, carrying 
passengers, are called flight critical systems. Similarly 
there are life critical systems, like pacemaker 
implanted inside human body. It is a very strict 
requirement that all flight critical and life critical 
systems have only one interrupt. This requirement is 
mainly used to keep the software simple and very 
deterministic. They all, as explained before, repeat the 
same periodic process of finite duration, but run 
practically for infinite time. 
The other kind of applications is based on the real 
time multi-tasking operating systems (RTOS). This 
OS is required for systems with more than one 
interrupts which normally appear at asynchronous and 
non-periodic rate. When you have more than one 
interrupts, you need to decide which one to process 
first. This leads to the concept of priority or 
assignment of some kind of importance to each 
interrupt and an algorithm to select them. The software 
that does this work is nothing but the RTOS. Thus 
RTOS is essentially an efficient interrupt handling 
algorithm. 
These RTOS based embedded applications are 
designed as a finite state machine. We are not going to 
present a theory of RTOS here. So to avoid confusions 
we do not try to distinguish among threads, tasks, 
processes, and states etc. We refer to all of these 
concepts as tasks, that is, we ignore all details below 
the level of tasks, in this paper. These tasks are 
executed according to the arrival of interrupts and the 
design of the application software. The total 
application algorithm is still fixed and finite but the 
work load is distributed among these finite numbers of 
tasks. The execution time of each task is finite also. 
These tasks process the incoming signals of finite time 
and produce the required output of finite size.  
2
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
An example will illustrate it better. A digital 
communication receiver can be designed to have many 
tasks â€“ signal processing task, bit recovery task, error 
correcting task etc. They can be interconnected by data 
buffers, operating system calls, and application 
functions. All these tasks together, implement a finite 
state machine, execute a finite duration algorithm, and 
process a finite size data buffer. These data buffers are 
originated from the samples of the finite duration 
signals representing the symbols.  
We should point out that there are systems which 
are combinations or variants of these two basic 
concepts. Most commercial RTOS provide many or all 
of these capabilities. Thus although all of the 
engineering systems run continuously for all time, all 
of them are run under the above two basic OS 
environment. Or in other words for all practical 
engineering designs the signal availability windows, 
the measurement windows, and the processing 
windows are all of finite time. For more details of real 
time embedded system design principles see many 
standard text books, for example [5, pp73-88]. 
The 
signals 
may 
exist 
theoretically 
or 
mathematically for infinite time but in this paper none 
of our theories, derivations, and assumptions will use 
that infinite time interval assumption.  
In the next section we describe the concept of a 
new 
digital 
communication 
scheme 
[6][7] 
to 
demonstrate the need for high sample rate. This 
scheme will also give the details of how finite time 
analysis can be used in our engineering systems. 
 
3. Motivation 
 
Almost all existing communication systems use 
sinusoidal functions as symbols for carrying digital 
data. But a sine function has only three parameters, 
amplitude, frequency, and phase. Therefore you can 
only transmit at most three parameters per symbol 
interval. That is a very inefficient use of symbol time. 
If instead we use general purpose functions then we 
can carry very large amount of information, thus 
significantly increasing the information content per 
symbol time. However, as we show below, these 
general purpose functions will require a large number 
of samples over its symbol time, and hence a high 
sample rate, to represent them precisely. We present a 
new digital communication system, called function 
modulation (fm) [6], to introduce the application of 
non-sinusoidal functions and the need for a new 
sampling theorem.  
Figure 1 shows an fm transmitter. The left hand 
side (LHS) vertical box shows four bits, as example, 
that will be transmitted using one symbol, s(t), shown 
in the right hand side (RHS) graph. Each bit location 
in the LHS box is represented by a graph or a general 
function. These functions, called bit functions, are 
combined by an algorithm to produce the RHS graph 
or function. A very simple example of the algorithm 
may be to add all the bit functions for which the bit 
values are ones and ignore whose bit values are 
zeroes. We call this algorithm a 0-1 addition 
algorithm. Since the bits in the LHS vertical box are 
continuously changing after every symbol time, the 
symbol s(t), t Ïµ [0,T], is also continuously changing.  
For this 0-1 addition algorithm we can write: 
 
s(t) = d1g1(t) + d2g2(t) + ..+dMgM(t)  
          (1) 
 
where di Ïµ {0,1} are the bit values. If we select {gi(t), 
tÏµ[0,T], i=1â€¦M} as a set of independent bit functions 
then we will be able to recover the bits if we know 
s(t). Here M is the number of bits to be transmitted 
using one symbol. This process of recovery of {di} 
from s(t) will require very precise knowledge of s(t). 
That can be achieved only by providing large number 
of samples for s(t) and for each member of {gi(t)}. 
Note that in (1) s(t), {gi(t)}, and {di}are all known 
quantities. In a later section we highlight the similarity 
of expression (1) with Fourier series and its 
consequences. 
The functions used in fm are not defined over 
infinite time interval; they are defined only over the 
symbol time, which are usually very small, of the 
order of microseconds or milliseconds, and should not 
be considered as infinite time intervals. The Nyquist 
rate will provide very few samples on these small 
intervals and will not enable us to reconstruct them 
correctly. We use these general classes of functions to 
represent digital data, because they have higher 
capacity to represent information compared to simple 
sine wave functions. Modern Digital Signal Processors 
(DSP) are ideally suited to handle them also. The DSP 
technology, high speed and high resolution Analog to 
Digital Converters (ADC), along with the analytical 
functions are quite capable of handling powerful 
10
10.000110.000210.000310.000410.000510.000610.000710.000810.0009 10.001
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
10
10.000110.000210.000310.000410.000510.000610.000710.000810.0009 10.001
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
10
10.000110.000210.000310.000410.000510.000610.000710.000810.0009 10.001
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
10
10.000110.000210.000310.000410.000510.000610.000710.000810.0009 10.001
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1 
0 
1 
1 
Figure 1.  fm Transmitter 
Data Space 
Bit-Function Space 
Symbol Space 
10
10.000110.000210.000310.000410.000510.000610.000710.000810.000910.001
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
s(t) 
Algorithm 
3
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
design methods, which cannot be implemented using 
hardware based concepts like voltage controlled 
oscillators or phase lock loops etc. 
The fm receivers are more complex than the fm 
transmitters. Mainly because the objective of the fm 
receiver is to decompose the received functions into 
the component bit functions that were used to create 
the symbol at the transmitter. The decomposition 
process is usually more complex than the composition 
process. However, if the bit functions are orthogonal 
then the decomposition process is very trivial [7]. 
Figure 2 shows the block diagram of a fm receiver 
based on orthogonal bit functions. Note however that 
the transmitter design is the same whether we use 
orthogonal or non-orthogonal bit functions. 
In Figure 2, the functions {gi(t)}are assumed to be 
orthogonal bit functions. They are M in number, the 
number of bits to be transmitted using one symbol. 
The received symbol r(t) is generated by the 
corresponding fm transmitter using 0-1 addition 
algorithm as shown in (1). We can write the symbol 
r(t) using the following relation: 
 
 r(t) = g1(t)x1 + g2(t)x2 + .. + gM(t)xM +  w(t)            (2) 
Where the set {xi} in (2) represents the unknown bit 
values and w(t) is the additive white Gaussian noise 
term. If the functions in {gi(t)} are orthogonal then we 
can get the estimate for {xi} using the following 
integration (3). Here wi is the projection of the noise 
on the i-th orthogonal function. 
 
 =  	
 + 


  
 
           (3) 
 
The integration process (3) is shown in the Figure 2, 
along with the thresholds for detecting the bits. 
Unlike similar figures in communication text 
books, here all parallel paths produce bit data. 
Therefore all integrations in all paths must be very 
precisely performed. This process will also require 
large volume of samples as well as powerful 
numerical integration methods, preferably based on 
analytical approaches. The receiver for non-orthogonal 
functions [6] is more complex and also demands large 
sample rate. Later we point out that the fm method is 
essentially an implementation of the concept behind 
the finite term Fourier series. 
The fm design provides a method for using 
general 
purpose 
functions 
for 
digital 
data 
communication. General functions can carry more 
information than sinusoidal functions. We highlight in 
many different ways the well known fact that any 
general continuous function defined over any finite 
time interval has infinite dimension and therefore can 
carry infinite amount of information. Intuitively this 
concept and its consequences in communication 
engineering may not be very clear, so we describe it in 
many details beginning with the following section. We 
also show that to extract this infinite information 
content we have to sample the functions, theoretically, 
at infinite rate. Thus the motivation for this research is 
to establish the theoretical foundation for the function 
modulation method. The engineering foundation of the 
fm method over real time voice band telephone line 
has already been demonstrated and presented [6]. 
 
4. Infinite Dimensionality 
 
We will use the following basic notations and 
definitions in our paper. Consider the space L2[a,b] of 
all real valued measurable functions defined over the 
finite, closed, and real interval [a,b]. We assume that 
the following Lebesgue integral is bounded: 
 
 || 
 < âˆž ,   âˆ€ âˆˆ [, ]


  
           (4) 
 
And then we define the norm:
 
 
 =   ||



!
"/
, âˆ€ âˆˆ [, ] 
           (5) 
 
Measurable functions form an equivalence class, 
in the sense that each function in this class has the 
same integral value. Two such functions in the same 
equivalent class differ on some countable discrete set 
whose measure is zero thus without affecting the 
integral value. We can always find a continuous 
function that can represent this equivalent class [8, 
pp418-427] in the sense of L2[a,b] norm. Thus for all 
engineering purposes we can think about continuous 
functions only [9, pp27-28]. 
The space L2[a,b] is a complete space. This 
completeness property ensures that every convergent 
sequence {fn} converges to a function f that belongs to 
L2[a,b] space. That is, L2[a,b] includes all the limit 
Figure 2.  fm Receiver for orthogonal functions 
g1(t)
gi(t)
gM(t)
r(t)
âˆ«
T â€¢
dt
0
)
(
âˆ«
T â€¢
dt
0
)
(
âˆ«
T â€¢
dt
0
)
(
0
1
0
1
1
0
=
â‡’
<
=
â‡’
>
b
b
0
0
1
0
=
â‡’
<
=
â‡’
>
ib
ib
0
0
1
0
=
â‡’
<
=
â‡’
>
bM
M
b


zi
 
4
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
points. The norm (5) is used to prove the convergence 
because it embeds the concept of distance between the 
elements of a sequence. 
We also define the inner product for L2[a,b] as: 
 
$, 	% =  	
    âˆ€, 	 âˆˆ [, ]


 
           (6) 
 
For L2[a,b] the integral in (6) exists and therefore the 
inner product is well defined. In a finite dimensional 
vector space the inner product of two vectors u = {&} 
and v = {'} is defined by 
 
&(' = &"'" + &' + â‹¯ + &*'*
 
 
Which is similar to (6) when you take the limit of the 
approximation given by the following: 
 
$, 	% =  	



  
â‰ˆ âˆ†["	" + 	 + â‹¯ + *	*]    
 
Thus a function space is very intimately linked with 
the concept of finite dimensional linear vector space 
when we look at it as nothing but a collection of 
infinite samples. 
Under the above conditions the function space, 
L2[a,b], is a Hilbert space. Hilbert space is defined as a 
complete inner product space. The inner product 
comes from the definition (6) and the completeness 
from the norm (5). The inner product helps to 
introduce the concept of orthogonality in the function 
space. We also define the distance between two 
functions in L2[a,b] space by: 
 

, 	 =  âˆ’ 	 =   | âˆ’ 	|



!
"/
      (7) 
 
The metric d in (7) defines the mean square distance 
between any two functions in L2[a,b]. 
One very important property of the Hilbert space 
[9, pp31-32] related to the communication theory, is 
that it contains a countable set of orthonormal basis 
functions. Let 
[ , ]}
2.1 ...,
( ),
{
a b
t
n t n
âˆˆ
=
Ï•
 be such a 
set of basis functions. Then the following is true: 
 
$./, .*% = 0/* = 10   3 4 â‰  6
1   3 4 = 6
8 
 
           (8) 
 
Also for any f Ïµ L2[a,b] we have the Fourier series 
 
 = âˆ‘
*.*,   âˆ€ âˆˆ [, ]
:
*;"
   
           (9) 
 
The above expression (9) really means that for any 
given Îµ > 0 there exists an N such that 
 
< âˆ’ âˆ‘
*.*
=
*;"
< < >, âˆ€? > A  
         (10) 
 
In this context we should also mention that the 
coefficients in (9) can be obtained using the following 
expression: 
 
* =  .*



, 6 = 1,2, â€¦     
         (11) 
 
In this paper we will consider only the continuous 
functions and their Riemann integrability. Riemann 
integration is the normal integration process we use in 
our basic calculus. We note that the continuous 
functions are measurable functions and the Riemann 
integrable functions are also Lebesgue integrable. For 
continuous functions the values for these two integrals 
are also same. Thus the Hilbert space theory (4-11) 
and the associated concepts will still remain applicable 
to our problems. We should point out though that the 
space of continuous functions is not complete for the 
L2[a,b] norm defined by (5). That means, there exists a 
sequence of continuous functions that does not 
converge to a continuous function under the L2[a,b] 
norm. However it will converge to a measurable 
function under L2 norm, that is, in the mean. 
Equality (9) happens only for infinite number of 
terms. Otherwise, the Fourier representation in (10) is 
only approximate for any finite number of terms. In 
this paper Îµ in (10) will be called as the measure of 
approximation or accuracy estimate in representing a 
continuous function. The Hilbert space theory ensures 
the existence of N in (10) for a given Îµ. The existence 
of such a countably infinite number of orthonormal 
basis functions (8) proves that the function space is an 
infinite dimensional vector space. This dimensionality 
does not depend on the length of the interval [a,b]. 
Even for a very small interval, like symbol time, or an 
infinite interval, a function is always an infinite 
dimensional vector. The components of this vector are 
the coefficients of (9). 
Hilbert space theory shows that a function can be 
represented by equation (9). The coefficients in (9) 
carry the information about a function. Since there are 
infinite numbers of coefficients, a function carries an 
infinite 
amount 
of 
information. 
Our 
digital 
communication theory will be significantly richer if 
we can use even a very small portion of this infinite 
information content of a function. The function 
modulation approach provides a frame work for such a 
system. The fm scheme essentially implements 
equation (9), for fm transmitter, where the coefficients 
used are zero or one instead of any real number. 
Similarly Figure 2, the fm receiver for orthogonal 
functions, implements expression (11). For an fm 
receiver (11) will produce zero or one as the values for 
{an}. It is clear that if we can find a band limited set of 
orthogonal functions then equations (9) and (11) will 
5
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
allow us to create a fm system with almost unlimited 
capacity [4]. 
It is not necessary to have orthonormal basis 
functions for demonstrating that the function space is 
infinite dimensional. The collection of all polynomial 
functions {tn, n=1,2,..}is linearly independent over the 
interval [a,b] and their number is also countable 
infinity. These polynomials can be used to represent 
any analytic function, i.e., a function that has all 
derivatives. Using Taylorâ€™s series we can express such 
a f(t) at t as: 
 
 = âˆ‘
DEF
*!
:
*;
  âˆ’ H*  
 
         (12) 
 
around the neighborhood of any point c. Thus the 
above polynomial set is also a basis set for the 
function space. Therefore using the infinite Taylor 
series expression (12), we prove that a function is an 
infinite dimensional vector over a finite interval. Here 
the information content of the function is defined by 
the derivative coefficients and the polynomial 
functions. Expression (12) also shows that the 
information content of a general function is infinity. 
The above two theories prove that the dimension 
of the function space is infinity. The number of such 
functions in this function space is also infinity, 
actually uncountable infinity. This is illustrated using 
the following logic. Consider any coefficient in the 
right hand side of (9). You will get a new function 
every time you change that coefficient. Since that 
coefficient can be adjusted to any value in the interval 
[0,1] you get a continuum of functions. Thus the 
cardinality of the function space is uncountable 
infinity whereas the dimensionality is countable 
infinity. 
We say that to represent a function accurately 
over any interval we need two sets of information: (A) 
An infinite set of basis functions, not necessarily 
orthogonal, like the ones given by (8) and (B) An 
infinite set of coefficients in the infinite series 
expression for the function, similar to (9). That is, 
these two sets completely define the information 
content in a mathematical function. In most cases the 
basis set described in (A) will remain fixed. We will 
distinguish functions only by their coefficients 
described in (B). Each function will have different 
coefficients in its expression for (9). 
We normally represent vectors as rows or 
columns with components as real numbers. As an 
example, a three dimensional vector has three real 
components. Similarly an n dimensional vector has n 
real components. Along that line an infinite 
dimensional 
vector 
will 
have 
infinite 
number 
components. We can represent a function by an 
infinite 
dimensional 
vector 
by 
selecting 
the 
coefficients of (9) as components of this vector. In this 
sense every function is an infinite dimensional vector. 
We will show later that the samples of a function can 
also be used to represent these components of an 
infinite dimensional vector. Thus these samples will 
bring this mathematics to engineering, because the 
ADCs can produce these samples. 
We now show that a band limited function is also 
infinite dimensional and therefore carries infinite 
amount of information. Consider a band limited 
function f(t), with bandwidth [-W,+W]. Then f(t) is 
given by the following inverse Fourier Transform [2]: 
 
 =
"
I 
JKLM

N:
O:
 
 
          
 
=
"
I 
JKLM

NIP
OIP
  
 
         (13) 
 
In (13) t is defined for all time in (â€“âˆž,+âˆž). But the 
frequency w is defined only over [â€“W,+W], and it can 
take any value: integer, rational, or irrational 
frequencies, within that range. 
The second line in expression (13) shows that the 
band limited function f(t) has uncountably infinite 
number of frequencies. That is, f(t) is created using 
infinite number of frequencies and therefore is an 
infinite dimensional vector. This is true even when we 
consider a small interval of time for the function f(t). 
In that small interval the function still has all the 
infinite frequency components corresponding to the 
points in [â€“W,+W]. This is another way of showing 
that a band limited function is an infinite dimensional 
vector over a finite measurement window.  
We have been talking about countable and 
uncountable infinities. To refresh our memory, 
countable infinity is the number of elements in the set 
of integers {1,2,â€¦} and the uncountable infinity is the 
number of points in the interval [0,1]. The set of 
rational 
numbers 
is 
also 
countable. 
Clearly 
uncountable infinity is larger than the countable 
infinity. However, one interesting fact is that any real 
number can be represented as a limit of a sequence of 
rational numbers. This fact is mathematically stated as 
the set of rational numbers is a dense set in the set of 
real numbers [8, pp43-45]. Therefore when we talk 
about uncountable infinity we can in many cases think 
in terms of countable infinity also. The relationship 
between 
measurable 
functions 
and 
continuous 
functions are similar as mentioned before. 
We point out here that a constant function f(t) = 
C, as an element of function space, is also an infinite 
dimensional vector. The only difference is that all 
sample values are same. In terms of Taylor series the 
coefficients for a constant function are {C,0,0,â€¦.}, 
which is an infinite dimensional vector. 
The infinite dimensionality idea of a function can 
be understood in another very interesting way. 
6
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
Consider the real line interval [0,1]. We know that it 
has uncountably infinite number of points. If we 
stretch this line to [0,2] we will still have all these 
uncountable number of points inside it. Now if we 
bend it and twist it all the points will still be there but 
the line will now become a function, a graph, in the 
two dimensional plane. Thus a function has 
uncountably infinite number of points. Every sample 
you take will have different coordinates and therefore 
different information. Therefore we can prove that a 
function can be exactly represented by this infinite 
number of samples and that more samples you take 
over 
this 
finite 
interval 
better 
will 
be 
the 
representation of the function. 
The definition of dimension should be clearly 
pointed out. The dimension of a vector space is the 
number of basis vectors of the space. For function 
space, both Fourier series (9) and the Taylor series 
(12) show, that the number of basis vectors is 
countable infinity. Therefore the dimension of the 
function space is countable infinity. Any element of 
this vector space will also have the same number of 
components in its representation as a vector. Therefore 
the total number of components in a vector is also 
called the dimension of the vector. 
We now show that samples can also be used to 
represent this infinite dimensionality. We prove that it 
is theoretically necessary to sample a function that is 
defined over finite time interval, infinite number of 
times, to extract all the information from the function. 
 
5. Sample Convergence 
 
Let f(t) be a continuous function defined over the 
real time interval [a,b]. Assume that we divide this 
finite time interval [a,b] into n > 1 equal parts using 
equally spaced points {t1, t2, .. tn, tn+1}.  Where t1 = a 
and tn+1 = b. Use the following notations to represent 
the t-subintervals 
 
âˆ† = Q [MR,MRST,
[ME,MEST],
    ;",,..,*O"
        ;*                    8  
 
Define the characteristic functions: 
 
V = Q",
,    Mâˆˆâˆ†MR
Mâˆ‰âˆ†MR
8    3 = 1,2, . . , 6  
 
         (14)          
 
In this case the characteristic functions, Xi(t) are 
orthonormal over the interval [a,b] with respect to the 
inner product on L2 [a,b], because 
 
VVX = 0, 3 â‰  Y, âˆ€ âˆˆ [, ]  
         (15) 
 
Also define the simple functions as: 
 
* = âˆ‘
V    âˆ€ âˆˆ [, ]
*
;"
 
         (16) 
 
Here f(ti) is the sampled value of the function f(t) at 
time t = ti that is, at the beginning of each sample 
interval âˆ†t. It is easy to visualize that fn(t) is a 
sequence of discrete step functions over n. Expression 
(16) is an approximate Fourier series representation of 
f(t) over [a,b]. This representation uses the samples of 
the function f(t) at equal intervals, fn(t) uses n number 
of samples.  
We show that this approximate representation 
(16) improves and approaches f(t) as we increase the 
value of n. Which will essentially prove that more you 
sample more information you get about the function.  
Thus the higher sample rate is meaningful and does 
not produce any redundant information. The following 
theorem is quite intuitive; its proof is also very simple. 
However, its consequence is very profound in the field 
of digital signal processing and in communication 
engineering. 
  
Theorem 1: fn â†’ f in L2[a,b] as
n â†’ âˆž
. 
 
First consider Figure 3, where we show the simple 
function fn(t) and the original continuous function f(t), 
between two consecutive sample points on the time 
line. It is geometrically obvious that the maximum 
error between the two functions reduces as the interval 
âˆ†t reduces. Mathematically, consider the error 
 
Z[* = maxM| âˆ’ *|, âˆ€ âˆˆ [, ] 
         (17) 
 
It is then clear that {âˆ†yn} is a monotonically 
decreasing sequence of n since the function f(t) is 
continuous over the closed interval [a,b]. Therefore, 
given any Îµ>0 we can find an N such that âˆ†[* â‰¤
>/` âˆ’     for all n > N. We can also verify that 
 
 âˆ’ * =   | âˆ’ *|



!
"/
 
          
 
=   | âˆ’ âˆ‘
V
*
;"
|



!
"/
   
         (18) 
 
Since V = 1 for all t we can rewrite the above 
expression without affecting the integral as 
 
=   |âˆ‘
V âˆ’ âˆ‘
V
*
;"
*
;"
|



!
"/
   
 
ti 
ti+1 
âˆ†t 
f(t) 
fn(t) 
Figure 3.  Simple function approximation 
 
Time 
Amplitude 
7
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
Rearranging the terms we can write 
 
=   aâˆ‘
b âˆ’ cV
*
;"
d




!
"/
                 
 
Now performing the squaring operation, noting that 
(15) holds, we can write the above as: 
 
=   [âˆ‘
[ âˆ’ ]V

*
;"
]



!
"/
  
          
 
â‰¤   [âˆ‘
[âˆ†[*]V

*
;"
]



!
"/
  
 
          
 
=  âˆ†[*
  âˆ‘
V

*
;"




!
"/
  
 
          
 
= [âˆ†[*
 âˆ’ ]"/ = ` âˆ’ âˆ†[* â‰¤ > 
 
Thus from (18) we see that âˆ€6 â‰¥ A  
 
 âˆ’ âˆ‘

*
;"
V â‰¤ f, âˆ€ âˆˆ [, ]             (19) 
 
Which means: 
 
 = âˆ‘
V , âˆ€ âˆˆ [, ]
:
;"
                    (20) 
 
This concludes the proof of Theorem 1.  
 
The expression (20) says that a function is an 
infinite dimensional vector and can be correctly 
represented by all the infinite samples, while the 
expression (19) can be used for approximate 
representation with accuracy given by Îµ. Essentially 
Theorem 1 proves that infinite sample rate is 
necessary to represent a continuous function correctly 
over a finite time interval. Another important 
interpretation of Theorem 1 is that the information 
content of a function is available in the samples of the 
function. Thus the amount of information these 
general purpose functions can carry is actually 
infinity. A communication system can be designed to 
extract a large amount of information from this infinite 
content. The fm system uses such a general class of 
function and can be used to carry more information 
than conventional designs. 
It is clear that the Theorem 1 does not depend on 
the bandwidth of the function f(t). However, for any 
given Îµ>0 the number N will depend on the 
bandwidth. 
Theorem 1 is similar to the one described for 
measurable functions in [8, pp389-391]. But the 
coefficients are not the sampled values in that 
theorem.  For measurable functions, samples are 
usually taken on the y-axis. Another proof can be 
found in [10, pp247-257] where the Bernstein 
polynomial has been used instead of the characteristic 
function. Although Bernstein polynomial functions are 
not orthogonal, like the characteristic functions used in 
Theorem 1, but they are defined over finite and closed 
interval, occasionally know as functions with compact 
support. We will see that it has an important 
consequence when we reconstruct the function using 
the samples. 
Theorem 1 shows that the approximating 
functions (16) converge in the mean, because we have 
used the L2 norm. In engineering we normally like 
pointwise, that is point by point convergence. We say 
that a sequence of functions, like {fn} in (16), 
converges uniformly to a function f on the closed 
interval [a,b] if for every Îµ > 0 there exists an N > 0, 
depending only on Îµ and not on t in the interval [a,b], 
such that |* âˆ’ | < > for all n >N, and for all t 
in [a,b]. It should be pointed out that uniform 
convergence implies pointwise convergence. 
Since the function f(t) is continuous over a closed 
interval, the sequence (16) is bounded, because the 
sample values are bounded. Therefore if we show that 
the supremum or the maximum of (16) converges then 
obviously the sequence will converge uniformly also 
[8, pp308-311]. That is we have to show that 
 
supjMj| âˆ’ *| < > , âˆ€6 > A 
         (21) 
 
Incidentally, the supremum and the maximum are 
the same thing for continuous functions over closed 
interval and also the difference really does not matter 
for engineering problems. Over every small interval 
âˆ†ti we can write 
 
| âˆ’ *| = |V âˆ’ V|           
 
= | âˆ’ |V    
 
 
          
 
According to the mean value theorem, there exists a ci 
within every interval âˆ†ti such that 
 
 =  +  âˆ’ (H 
 
          
 
Therefore using (16) we can write 
 
| âˆ’ *| = âˆ‘
| âˆ’ |V
*
;"
           (22) 
 
= âˆ‘
| âˆ’ (H|V
*
;"
  
 
          
 
= âˆ‘
| âˆ’ ||(H|
*
;"
V  
 
          
 
â‰¤ âˆ† k âˆ‘
V =
O
*
*
;"
k   
 
         (23) 
 
Where, M is the upper bound of the derivative of f(t) 
on the entire interval [a,b]. Since f(t) is a continuous 
function over a closed interval, M always exists. So in 
the above proof we have assumed that the function is 
differentiable. Thus the right hand side of (23) is 
independent of t and therefore is an uniform bound for 
all t for the left hand side of (22). Thus we can see that 
the difference expressed in the left side of (22) goes to 
8
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
zero as n goes to infinity. This shows that (20) is true 
for all t, thus proving the uniform convergence. The 
above derivation also proves the intuitive assertion 
made in (17). 
We have shown that the approximations generated 
by sample values of the original function f(t) 
converges to the original function. As it converges the 
number of samples increases, because that is the way 
we constructed the approximating function fn(t). Since 
the approximations improve, the fn(t) improves, 
confirming that more samples are better and does not 
generate 
redundant 
information. 
These 
infinite 
samples collectively define the function. A complete 
description of the function can only be obtained by 
these infinite numbers of samples, which can be 
considered as components of an infinite dimensional 
vector. The above basic tools now can help us to give 
theoretical proofs of sampling theorem. 
 
6. Sampling Theorem 
 
Consider the simple sinusoidal function 
 
s(t) = A sin (2 Ï€ f t + Î¸) 
 
 
         (24) 
 
and assume that it is defined only for one period for 
simplicity, although not necessary. We can think of 
this sinusoidal function as the highest frequency 
component of a band limited signal. So if we can 
recover this function by sampling it then we will be 
able to recover the entire original signal, because the 
other components of the band limited signal are 
changing slowly at lower frequencies. We try now to 
determine how many samples we need to recover the 
sine function. 
We can see from the above expression (24) that a 
sinusoidal function can be completely specified by 
three parameters A, f, and Î¸. That is we can express a 
sine function as a three dimensional vector: 
 
s = [A, f, Î¸] 
 
 
 
         (25) 
 
However (25) is very misleading. There is a major 
hidden assumption; that the parameters of (25) are 
related by the sine function. Therefore more precise 
representation of (25) should be: 
 
s = [A, f, Î¸, â€œsineâ€] 
 
 
         (26) 
 
The word sine in (26) means the Taylorâ€™s series, 
which has an infinite number of coefficients. 
Therefore when we say (25) we really mean (26) and 
that the sine function, as usual, is really an infinite 
dimensional vector. 
We can use the following three equations to solve 
for the three unknown parameters, A, f, and Î¸ of a 
sinusoidal function: 
s1 = A sin (2 Ï€ f t1 + Î¸) 
s2 = A sin (2 Ï€ f t2 + Î¸) 
 
 
         (27) 
s3 = A sin (2 Ï€ f t3 + Î¸) 
 
where t1 , t2 , t3  are sample times and s1 , s2 , s3 are 
corresponding sample values. Again a correct 
representation in terms of samples would be 
 
s = [(s1,t1), (s2,t2), (s3,t3), â€œsineâ€] 
 
          
 
Hence with sinusoidal assumption, a sine function can 
be completely specified by three samples. The above 
analysis gives a simple proof of the sampling theorem. 
We can now state the well known result: 
 
Theorem 2: A sinusoidal function, with the 
assumption of sinusoidality, can be completely 
specified by three non-zero samples of the function 
taken at any three points in its period. 
 
From (27) we see that if we assume sinusoidality 
then more than three samples, or higher than Nyquist 
rate, will give redundant information. However 
without sinsoidality assumptions more samples we 
take more information we get, as is done in common 
engineering practice. It should be pointed out that 
Shannonâ€™s sampling theorem assumes sinusoidality. 
Because it is derived using the concept of bandwidth, 
which is defined using Fourier series or transform, and 
which in turn uses sinusoidal functions.  
Theorem 2 says that the sampling theorem should 
be stated as fs > 2fm instead of fs â‰¥ 2fm that is, the 
equality should be replaced by strict inequality. Here, 
fm is the signal bandwidth, and fs is the sampling 
frequency. There are some engineering books [11, 
p63] that mention strict inequality.  
Shannon states his sampling theorem [2, p448] in 
the following way: â€œIf a function f(t) contains no 
frequencies higher than W cps, it is completely 
determined by giving its ordinates at a series of points 
spaced 1/2 W seconds apartâ€.  The proof in [2] is very 
simple and runs along the following lines. See also 
[12, p271]. A band limited function f(t) can be written 
as in (13). Substituting t = n/(2W) in (13) we get the 
following expression: 
 
 l
*
Pm =
"
I 
JKL E
no
NIP
OIP

    
         (28) 
 
Then the paper [2] makes the following comments: 
â€œOn the left are the values of f(t) at the sampling 
points. The integral on the right will be recognized as 
essentially the nth coefficient in a Fourier-series 
expansion of the function F(w), taking the interval â€“W 
to +W as a fundamental period. This means that the 
values of the samples f(n/2W) determine the Fourier 
coefficients in the series expansion of F(W).â€ It then 
9
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
continues â€œThus they determine F(w), since F(w) is 
zero for frequencies greater than W, and for lower 
frequencies F(w) is determined if its Fourier 
coefficients are determinedâ€.   
Thus the idea behind Shannonâ€™s proof is that from 
the samples of f(t) we reconstruct the unknown 
Fourier transform F(w) using (28). Then from this 
known F(w) we can find f(t) using (13) for all time t. 
One important feature of the above proof is that it 
requires that the function needs to exist for infinite 
time, because only then you get all the infinite samples 
from (28). We show that his proof can be extended to 
reconstruct functions over any finite interval with any 
degree of accuracy by increasing the sample rate. The 
idea behind the proof is similar, we construct F(w) 
from the samples of f(t). 
In this proof we use the principles behind the 
numerical inversion of Laplace transform method as 
described in [13, p359]. Let F(w) be the unknown 
band limited Fourier transform, defined over [-
W,+W]. Let the measurement window for the function 
f(t) be [0,T], where T is finite and not necessarily a 
large number.  Divide the frequency interval 2W into 
K smaller equal sub-intervals of width âˆ†w with 
equally spaced points {wj} and assume that the set of 
samples {F(wj)} is constant but unknown over that j-th 
interval. Then we can express the integration in (13) 
approximately as: 
 
 â‰ˆ
"
I âˆ† âˆ‘
KMLp
q
X;"
JX  
         (29) 
 
The right hand side of (29) is a linear equation in 
{F(wj)}, which is unknown. Now we can also divide 
the interval [0,T] into K equal parts with equally 
spaced points {tj} and let the corresponding known 
sample values be {f(tj)}. Then if we repeat the 
expression (29) for each sample point tj we get K 
simultaneous equations in the K unknown variables 
{F(wj)}as shown below: 
 
r
"

â‹®
q
t =
âˆ†L
I r
KMTLT   KMTLn â€¦ KMTLu
KMnLT   KMnLn â€¦ KMnLu
â‹®
KMuLT   KMuLn â€¦ KMuLu
t r
J"
J
â‹®
Jq
t (30) 
 
These 
equations 
are 
independent 
because 
the 
exponential functions in (29) are independent.  
We recall that a set of functions v = {	,
3 = 1 â€¦ k,  âˆˆ [0, x]} is called dependent over the 
interval if there exists constants ci, not all zero, such 
that 
 
	"H" + 	H + â‹¯ + 	zHz = 0 
 
for all t in [0,T]. If not, then it is independent [14, 
pp177-181]. The above expression is a linear 
combination of functions. Here the coefficients 
{H, 3 = 1 â€¦ k} are all real numbers. It essentially 
says that one function cannot be constructed using 
other functions. 
Therefore we can solve (30) for {F(wj)}.  
Theorem 1 ensures that the sets {F(wj)} and {f(tj)} can 
be selected to achieve any level of accuracy 
requirements in (13) for either f(t) or F(w). For 
convenience we assume that the number of terms K in 
(29) is equal to Tkfs = 2kWT. Here fs is the Nyquist 
sample rate and k > 1. We state the following new 
sampling theorem. 
 
Theorem 3:  Let f(t) be a band limited function 
with bandwidth restricted to [-W,+W] and be available 
over the finite measurement window [0,T]. Then given 
any accuracy estimate Îµ >0, there exists a constant k>1 
such that 2kWT equally spaced samples of f(t) over 
[0,T] will completely specify the Fourier transform 
F(w) of f(t) with the given accuracy Îµ. This F(w) can 
then be used to find f(t) for all time t. 
 
Note that we did not say that the function does not 
exist over the entire real line. We only said that our 
measurement window is finite. What happens to the 
function beyond the finite interval is not needed for 
our analysis. The main point of our paper is that we do 
not need to be concerned with the existence of our 
signals over the entire real line. 
We have given, as in (30), a very general 
numerical method of solving an integral equation. The 
method can be applied also to the case when f(t) in the 
left hand side of the equation (13) is unknown. The 
equation (13) itself can be generalized too. A well 
known [15] generalization is given below: 
 
 =  {, J



 
 
         (31) 
 
Here we have replaced the sinusoidal function by the 
kernel function K(t,w). Expression (31) represents a 
relationship between frequency components with the 
time functions for finite duration signals. In that sense 
it gives the bandwidth information of any given 
function f(t). It may even be possible to solve the 
equation (31) analytically over finite time and 
frequency ranges for some specific kernel functions. 
More we research in this very practical finite duration 
engineering problem better will be our definitions and 
theories and they will be closer to reality. We point out 
here that there is a need for extending the definition of 
bandwidth of a function from infinite time to finite 
time. 
10
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
In a sense Shannonâ€™s sampling theorem gives a 
sufficient condition. That is, if we sample at twice the 
bandwidth rate and collect all the infinite number of 
samples then we can recover the function. We point 
out that this is not a necessary condition. That is, his 
theorem does not say that if T is finite then we cannot 
recover the function accurately by sampling it. We 
have confirmed this idea in the above proof of 
Theorem 3. That is if T is finite we have to sample at 
infinite rate to get all the infinite number of samples. 
Or in other words more we sample more information 
we get. This is because a function is an infinite 
dimensional vector and therefore it can be correctly 
specified only if we get all the infinite number of 
samples.  
Shannon proves his sampling theorem in another 
way [2]. Any continuous function can be expressed 
using the Hilbert space based Fourier expression (9). 
He has used the expression (9) for a band limited 
function f(t), defined over infinite time interval. He 
has shown that if we use  
 
.* =
|*{ID}[MO*/D}]}
ID}[MO*/D}]     
 
         (32) 
 
Then the coefficients of (9) can be written as 
 
an = f(n/fs) 
 
 
 
        (33)  
 
Thus the function f(t) can be expressed as: 
 
 = âˆ‘
6/|
|*{ID}[MO*/D}]}
ID}[MO*/D}]
:
*;O:
    
         (34) 
 
Here fs = 2W, where W is the bandwidth of the 
function f(t). The set {Ï†n } in (32) is orthogonal only 
over (-âˆž,+âˆž).    
Observe that the above is very similar to our proof 
of theorem 1. Shannon used sinc functions as the 
orthogonal basis functions, whereas in our theorem 1 
we used rectangular pulses as the orthogonal basis 
functions. We know that the sinc function is the 
Fourier transform of the rectangular pulse. Only 
difference is that the sinc functions require infinite 
time interval. 
 
We make the following observations about (34): 
1. The representation (34) is exact only when 
infinite time interval and infinite terms are 
considered. 
2. If we truncate to finite time interval then the 
functions Ï†n in (32) will no longer be orthogonal, 
and therefore will not form a basis set, and 
consequently will not be able to represent the 
function f(t) correctly. 
3. If in addition we consider only finite number of 
terms of the series in (34) then more errors will be 
created because we are not considering all the 
basis functions. We will only be considering a 
subspace of the entire function space. 
 
We prove again, that by increasing the sample 
rate we can get any desired approximation of f(t), over 
any finite time interval [0,T], using the same sinc 
functions of (32). From calculus we know that the 
following limit holds: 
 
limÂ€â†’:
Â‚ÂƒÂ„ Â€
Â€
= 0  
 
 
         (35) 
 
Assume that fs is the Nyquist sampling frequency, i.e., 
fs = 2W.  Let us sample the signal at k times the 
Nyquist rate. Here k>1 is any real number. Then using 
(35), we can show that given any T and a small Î´ > 0, 
there exists an N such that  
 
Â…
|*I=D}M
I=D}M
Â… < 0, âˆ€? > A, âˆ€ â‰¥ x 
 
         (36) 
 
Thus these orthogonal functions (32) substantially 
go to zero outside any finite interval [0,T] for large 
enough sampling rate and still maintain their 
orthogonality property, substantially, over [0,T]. 
Therefore, for a given band limited function f(t), with 
signal capture time limited to the finite window [0,T], 
we can always find a high enough sample rate, kfs so 
that given any Îµ>0 the following will be true: 
 
Â† âˆ’ âˆ‘

*
=D}
|*{I=D}[MO*/=D}]}
I=D}[MO*/=D}]
q
*;
Â† < >          (37) 
 
âˆ€? > A, âˆ€ âˆˆ [0, x] 
 
The number of functions in the above series (37) 
is now K, which is equal to the number of samples 
over the period [0,T]. Thus K= kfsT = 2kWT. As k 
increases the number of sinc functions increases and 
the distance between the consecutive sinc functions 
reduces thus giving higher sample rate. See the 
numerical example given below. The original proof 
[2] for (32-34) still remains valid as we increase the 
sample rate. That is, the sinc functions in (32) still 
remain orthogonal. It can be shown using the original 
method that the coefficients in (33) remain valid and 
represent the sample values. Of course, the original 
proof requires the infinite time interval assumption. 
Thus the system still satisfies the Hilbert Space theory 
expressed by (4-11) making expression (37) justified. 
Now we can state the following new sampling 
theorem. 
 
Theorem 4:  Let f(t) be a band limited function 
with bandwidth restricted to [-W,+W] and available 
over the finite measurement window [0,T]. Then given 
any accuracy estimate Îµ there exists k>1 such that 
2kWT equally spaced samples of f(t) over [0,T] along 
11
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
with their sinc functions, will completely specify the 
function f(t) for all t in [0,T] at the given accuracy.  
 
We should point out, like in Theorem 2, that if we 
assume infinite time interval then faster than the 
Nyquist rate will also not give redundant information. 
This concept is also easily seen from the Fourier series 
expression (9). To solve for the coefficients of (9) we 
need infinite number of samples to form a set of 
simultaneous equations similar to (30). As we increase 
the sample rate the solution of (30) will only become 
better, that is, the resolution of the coefficients will 
increase and the unknown function will also get better 
approximations. 
For finite time assumption higher sampling rate is 
necessary to achieve the desired accuracy. The reason 
is same; the concept of infinite dimensionality must be 
maintained over finite time interval. That can be 
achieved only by higher sample rate. We also repeat, if 
you know the analytical expression then the number of 
samples must be equal to the number of unknown 
parameters of the analytical expression. This case does 
not depend on the time interval. 
A lot of research work has been performed on the 
Shannonâ€™s sampling theorem paper [2]. Somehow the 
attention got focused on the WT factor, now well 
known as the dimensionality theorem. It appears that 
people have [16][17] assumed that T is constant and 
finite, which is not true. Shannon said in his paper [2] 
many times that T will go to infinite value in the limit. 
No one, it seems, have ever thought about the finite 
duration issue. This is probably because of the 
presence of infinite time in the Fourier transform 
theory. The paper [15] gives a good summary of the 
developments around sampling theorem during the 
first thirty years after the publication of [2]. 
Interestingly [15] talks briefly about finite duration 
time functions, but the sampling theorem is presented 
for the frequency samples, that is, over Fourier domain 
which is of infinite duration on the frequency axis. 
Now we give a numerical example to show how 
higher rate samples actually improves the function 
reconstruction. 
 
7. A Numerical Example 
 
We illustrate the effect of sample rate on the 
reconstruction of functions. Since every function can 
be considered as a Fourier series of sinusoidal 
harmonics, we take one sine wave and analyze it. This 
sine function may be considered as the highest 
frequency component of the original band limited 
signal. The Nyquist rate would be twice the 
bandwidth, that is, in this case twice the frequency of 
the sine wave. We are considering only one period, 
and therefore the Nyquist rate will give only two 
samples of the signal during the finite interval of its 
period. We are also assuming that we do not know the 
analytical expression of the sine wave that we are 
trying to reconstruct.  
In this example we use the sinc functions of 
Shannonâ€™s theory, equation (37), to reconstruct the 
signal from the samples. In Figures 4-6 x-axis 
represents the angles of the sine function in terms of 
degrees or samples or time. The y-axis represents the 
amplitude of the sine function. Figure 4 shows the 
reconstruction process using two samples of the 
signal. In part (a) we show the sample locations and 
the corresponding sinc functions over the interval of 
one period. In part (b) we show how the construction 
formula (37) reproduces the function. Part (c) shows 
the error between the reconstructed sine function and 
the original sine function. We can see that the 
reconstruction really did not work well with two 
samples. Thus for finite time interval signals this 
process of recovering the function using expression 
(37) and the Nyquist rate provides very poor results. 
Therefore in all engineering application we cannot use 
the Nyquist rate. 
Figure 5. Reconstruction using three samples 
 
 
0.0002
0.0004
0.0006
0.0008
0.001
-0.2
0.2
0.4
0.6
0.8
1
(a)   3-terms sinc functions 
 
0.0002
0.0004
0.0006
0.0008
0.001
-1
-0.5
0.5
1
(b) 3-samples approximation 
 
0.0002
0.0004
0.0006
0.0008
0.001
-0.4
-0.2
0.2
0.4
(c) 3-samples error 
Time  
Time  
Amplitude  
Time  
Figure 4. Reconstruction using two samples 
 
 
0.0002
0.0004
0.0006
0.0008
0.001
-0.4
-0.2
0.2
0.4
(c)  2-samples error 
 
0.0002
0.0004
0.0006
0.0008
0.001
-0.2
0.2
0.4
0.6
0.8
1
 (a)  2-terms sinc functions 
 
0.0002
0.0004
0.0006
0.0008
0.001
-1
-0.5
0.5
1
(b) 2-samples approximation 
Time  
Time  
Time  
Amplitude  
12
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
In Figure 5 we repeat the process described for 
Figure 4 with three samples. In part (b) of Figure 5 we 
still have significant errors. The error is prominent at 
the two edges because the sinc functions do not stop at 
the end, they continue up to infinity. This is where 
Bernstein polynomials, with compact support, will fit 
better [10]. Usually Bernstein polynomial converges 
very slowly and thus will require large number of 
samples. We mention about the Bernstein polynomial 
because it has some good analytical qualities [10, 
pp247-258]. However when we have large number of 
samples we may have many other better options for 
reconstructing the functions as described in the next 
section. 
Figure 6 shows how the reconstruction process 
improves when we substantially increase the sample 
rate to six samples per period or three times the 
Nyquist rate. In this figure we still use the sinc 
function approach, i.e., expression (37). Notice that 
the errors at the edges are also reduced. This is 
because, as we increase the sample rate the sinc 
functions become narrower as predicted by the 
expression (36) and major part of the functions remain 
inside the signal interval. 
The graphs show that the error decreases as we 
increase the sample rate as predicted by the new 
sampling theory and infinite dimensionality of the 
function space. It is clear from the examples that, for 
the same number of samples, a different recovery 
function, instead of sinc function, will give different 
result. In the next section we discuss this different 
reconstruction approach. 
 
8. Re-sampling and Reconstruction 
 
In many applications in engineering we may 
require different sampling rates mainly to control the 
computation time of the processor. The theory 
presented in this paper essentially says that, sample as 
fast as you can. That will give you maximum amount 
of information about the signal you want to process. 
However, we may not be able to select the desired 
Analog to Digital Converters (ADC) to sample the 
signal at the rate we want, because of many reasons. 
The two most important of them are the cost and the 
power required by the ADC chips. 
After we have all the samples, collected at the 
highest feasible sample rate, then to reduce the sample 
rate we can simply drop few samples. This approach 
will maintain the quality of the signal representation. 
This is can be justified from the expression (30). Note 
that the Fourier series expression (9) can also be 
converted into a form given by (30). Normal 
decimation process changes the bandwidth thus losing 
the accuracy.  
However, if we want to increase the sample rate, 
after collecting all the samples from the ADC, then we 
have to interpolate the samples and resample the 
analytical expression, thus obtained, using the new 
higher rate.  
We emphasize the idea of using the analytical 
expression for the received signal in our algorithms. 
Instead of focusing on the samples we should focus on 
the mathematical expression and on the design of the 
algorithms around that mathematical expression. If we 
can achieve that then the number of samples will play 
a very minimal role. We will still have the complete 
expression of the signal even when we use very few 
samples.  
Since in this paper we are dealing with finite time 
intervals, we do not yet have the proper definition of 
bandwidth. We also cannot use the conventional filters 
because they use transfer function, which uses Laplace 
transform, and which in turn is defined only over 
infinite time interval. Classical linear system theory is 
inappropriate for finite time interval problems. Note 
that the normal decimation and interpolation methods 
used in digital signal processing techniques [18, 
pp303-316] cannot be used also, because they use 
bandwidth related and transfer function based filters. 
Thus all of our analysis must rely on the time domain 
approaches. 
We have described several analytical approaches 
for signal definition from the samples. Any method 
based on Fourier series (9), similar to (37), can be 
used as an analytical expression. Once you have the 
expression you can generate any number of samples 
from that expression. Approximation theory [10], a 
time domain approach, is also very rich in the area of 
interpolation using analytical expressions and can be 
used for re-sampling. This analytical expression 
approach requires that we use the entire batch of data. 
This batch allows us to see the entire life history of the 
system. This history can be more effective in signal 
processing than a sample by sample approach. 
Figure 6. Reconstruction using six samples 
 
 
0.0002
0.0004
0.0006
0.0008
0.001
-0.2
0.2
0.4
0.6
0.8
1
(a)   6-terms sinc functions 
 
0.0002
0.0004
0.0006
0.0008
0.001
-1
-0.5
0.5
1
(b) 6-samples approximation 
 
0.0002
0.0004
0.0006
0.0008
0.001
-0.03
-0.02
-0.01
0.01
0.02
0.03
0.04
6-samples error 
Amplitude  
Time  
Time  
Time  
13
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
We want to add another layer of information to 
our signal processing approach. All of the above 
methods assume that we do not have any total system 
level information about the origin of the signal we are 
trying to reconstruct. More specifically, in digital 
communication receiver, for example, we know how 
the received signal was constructed at the transmitter. 
We can use that information to reconstruct an analytic 
expression from the samples at the receiver and then 
go for re-sampling, if necessary. This will produce 
more realistic results than straight forward application 
of approximation theories to the samples. 
We give a numerical example to illustrate this 
global or system level concept of re-sampling and 
reconstruction 
of 
signal 
analysis. 
In 
function 
modulation [6] for example, at the transmitter, we 
used linear combination of a set of sinusoidal 
frequencies. We also used some constraints on the 
coefficients of this linear combination. The purpose of 
these constraints was to control the bandwidth 
(defined using the conventional sense) of the stream of 
concatenated symbols existing over infinite time. At 
the receiver we may not be able to use these 
constraints, but definitely we can interpolate using 
these specific frequency signals and achieve a higher 
level of accuracy in the reconstruction process. 
In our experiment, we transmitted the signal 
shown in Figure 7. We used the voice band telephone 
line to transmit the signal. The signal sample rate both 
at the transmitter and at the receiver was 16 kHz at 16 
bits resolution. The telephone companies sample the 
voice signals at 8 kHz rate and at 8 bits resolution. 
This sample rate difference or some other unknown 
reasons distorted the received signal very significantly 
as shown in the Figure 8. As we can see from the 
figures the received signal has two positive peaks as 
opposed to three positive peaks in the transmitted 
signal. As if the second trough of the transmitted 
signal got folded up in the received signal. 
It is clear that the conventional signal recovery 
methods, that use local concepts, no matter how many 
samples we take, cannot bring the received signal back 
to the transmitted form. However, a global approach 
or a systems approach, where we use the knowledge of 
the entire system can definitely help. We used the 
same sine wave frequencies of the transmitter, to 
interpolate the samples at the receiver. Here, of course, 
the high sample rate played an important role in the 
least square interpolation method. The details of the 
signal processing, is quite involved, and is not given 
here. The large sample rate and the systems approach 
helped us to bring the received signal back to a shape 
that is very close to the transmitted signal, as shown in 
Figure 9, which allowed us to detect the bits correctly. 
Thus we can see that a total system level or global 
approach in signal processing can perform miracles. 
At this end, we repeat again that all of the existing 
theories that are based on infinite time assumptions 
should be carefully reviewed, redefined, and recreated 
for their finite time applications. We do not want any 
infinite time assumptions behind any of our finite time 
applications. That is not theoretically correct. More 
research will be required to generate analytical results 
for finite time systems. We should design our theories 
based on the engineering constraints. Our technology 
has advanced significantly. We can now use many 
mathematical theories that we could not use before. 
Besides infinite time assumption most of our 
theories assume linearity also. We point out here in the 
next section how linearity concepts are deeply 
embedded in all our theories thus ignoring some basic 
engineering constraints. It is to be noted that the 
original sampling theorem used linearity assumption 
also, because it was based on Fourier theory. 
0.0002
0.0004
0.0006
0.0008
0.001
-1
-0.75
-0.5
-0.25
0.25
0.5
 
Figure 7.  Transmitted symbol for fm 
Time  
Amplitude  
20
40
60
80
100
120
140
-1
-0.75
-0.5
-0.25
0.25
0.5
0.75
 
Figure 9.  Best fit of the received symbol 
Time  
Amplitude  
2.5
5
7.5
10
12.5
15
17.5
-1
-0.75
-0.5
-0.25
0.25
0.5
0.75
 
Figure 8.  Received symbol for fm 
Time  
Amplitude  
14
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
9. Linearity Assumptions 
 
All of our engineering systems are nonlinear 
because of two very important reasons. We briefly 
discuss them here to point out in another section that 
the transform methods that are based on linearity 
assumptions cannot be effectively used in engineering 
problems. Also we want to raise the importance of this 
nonlinearity to create a concern for the validity of our 
theories for engineering. 
The most important reason for this nonlinearity is 
very well known though, but we probably never think 
about them. Hardly any text book [19, pp196-199] 
talks or provides any theory [20, pp167-179] for 
solving them. We call it saturation nonlinearity. Every 
engineering variable, like voltage, current, pressure, 
flow, etc. has some upper and lower bounds. They 
cannot go beyond that range. In terms of mathematical 
equation this situation can be described as 
 
4Â€ â‰¤  â‰¤ kÂ€,  
 
where x is any physical engineering variable, mx is the 
lower bound and Mx is the upper bound. Graphically 
the above equation can be represented by Figure 10. 
The figure shows that whenever the engineering 
variable x is within the range of [a,b] the output is 
linear and is equal to x. If x goes outside the boundary 
it gets clipped by Mx on the higher side and by mx on 
the lower side. Note that mx can be zero or negative 
also.  
Clearly Figure 10 shows severe nonlinearity and 
there is no escape from this in any engineering 
problems. We should not ignore its presence in 
engineering. These constraints are a kind of natural 
laws for our technology. Therefore we cannot treat our 
engineering problems using simple Linear Time 
Invariant (LTI) systems theory. Simply because there 
are no real LTI systems in engineering. If we do use 
such a theory then the performance of the system will 
be compromised. 
It should be pointed out that it is almost 
impossible to keep the variables in the linear region. In 
most practical engineering problems there are 
hundreds of variables and hundreds of equations. It is 
not feasible to ensure that all variables will be within 
the linear range all the time. Their relationships are 
very complex. In addition there are many transients in 
all systems that can significantly alter the domain size 
of the variables. There is yet another important case 
where we have to demonstrate that the systems we 
build must behave normally when it goes to these 
limits. In addition, many systems have strict 
requirements that variables must go to this limit and 
stay there, like actuators in airplane wings. The wing 
flaps must reach their maximum angular positions and 
stay there for certain period of time. 
In all engineering software, any conscientious 
programmer 
will 
always 
include 
the 
above 
nonlinearity test, usually called anti-windup, in their 
source code. And this code will automatically make 
the software, and hence our algorithm, nonlinear. A 
software engineer, barring few exceptions, does not 
know how mathematics works but knows how to make 
systems work. If you see any source code you will find 
many such patch work or kludge in the source code 
that are necessary to make the systems work. This 
necessity originates not only because of the lack of 
theoretical foundations of our algorithms but it will 
also be due to the RTOS and the interrupts of the 
background software which interferes with our theory. 
All electronic hardware automatically includes 
such saturation nonlinearity in their systems. An 
automatic gain control mechanism, for example, 
actually is nothing but a nonlinear method of keeping 
the variables inside their linear regions. Because of 
this non-linearity any application of the LTI theory in 
engineering will violate the mathematical assumptions 
of the LTI theory. All transfer functions based 
approaches, like Laplace, or Fourier, are inappropriate 
for all engineering methods. They cannot work, 
because 
they 
violate 
the 
basic 
engineering 
assumptions. The transform approaches not only 
assume infinite time, they also assume linearity. If we 
use correct theory consistent with engineering models 
then we will definitely get much better results from 
our systems. 
There is another very natural reason for using the 
non-linear theory in the design of our systems. Every 
engineer knows this one also but we want to mention 
it to strengthen our argument against the application of 
the LTI system theory. Most of the engineering 
requirements for todayâ€™s technology are very stringent 
and we have experienced that our technology in most 
cases can support them to some extent. Because of 
these highly advanced and sophisticated requirements 
a simple linear model of engineering systems cannot 
achieve the objectives. We must make use of advanced 
 
Figure. 10.  Saturation Non-Linearity 
x - input 
x - output 
m 
M 
a 
b 
15
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
nonlinear and dynamic or time varying adaptive 
system models. 
A very well know and well established example 
that engineers have developed during the last thirty 
years is the Inertial Navigation System (INS). Today 
our requirement says that the first missile will make a 
hole in the building and the second missile must go 
through that hole. That is a very sophisticated and 
precise demand. The INS development shows that 
simple Newtonâ€™s law of force equals to mass times 
acceleration cannot work. The equation has been 
extended to fill books of hundreds of pages. They 
derive starting with simple linear Newtonâ€™s equation 
[21, p4] a complete and very highly nonlinear set of 
equations [22, pp73-77] to describe the motion of an 
object. These equations include, among many other 
things, Coriolis forces, earthâ€™s geodetic ellipsoidal 
models etc. Even after including all the nonliniearities 
that we know of we still had to integrate the INS with 
a Global Positioning System (GPS) to satisfy many of 
our requirements.  
The simple Linear Time Invariant (LTI) system 
equation like the one given below: 
 
Â‡Â€M
Â‡M
= Âˆ + Â‰& 
 
         (38) 
 
cannot solve most of  our engineering problems, 
simply because the laws of nature are too complex and 
the demands from our technology are too precise. In 
engineering many nonlinear problems have been 
attempted using successive linearization methods with 
the application of theories of (38). These approaches 
do not work also, mainly because there are no theories 
with very general assumptions that establish their 
convergence, stability, and optimality. The above 
reason of nonlinearity is very well known to all of us.  
We have added this section to highlight the need 
for the signal processing approach presented in this 
paper. We are proposing a software radio approach 
with finite time batch data processing, very high 
sampling rate, time domain theories, and global 
system level simultaneous interactions. We believe 
that this direction, has some theoretical foundations, 
and can be augmented to many nonlinear dynamic 
approaches.  
The wavelets have become a very popular signal 
processing tools. It also has been used to extend the 
sampling theorem applications [23]. So we briefly talk 
about it in the next section. 
 
10. Wavelet theory 
 
The wavelet theory has many relations with the 
theories discussed in this paper. The Haar wavelet 
systems starts [24, p431] with the characteristic 
function of the [0,1] interval similar to the one defined 
by (14). These are also orthogonal functions as 
described in (15). The Shannonâ€™s scaling functions 
[24, p422] are the sinc functions sin Â‹/Â‹ similar to 
(32). Wavelets are very useful signal processing tools 
for many image and voice related problems. However 
it is still at its developmental phase. It has not been 
demonstrated yet that it can be integrated, similar to 
Fourier or Laplace methods, into dynamic systems 
governed by differential equations. 
The wavelet theory also gives a down sampling 
process [25, pp31-35]. Like Fourier theory the wavelet 
down sampling provides a lower resolution, as a 
consequence 
of 
the 
multi-resolution 
analysis, 
reconstruction of the original function. This is in 
contrast to the method presented in previous section, 
where the down sampled version still maintains the 
original resolution quality. The down sampling 
process is required for reducing the throughput 
requirements of the processor. Down sampling should 
not therefore reduce the quality of the signal. 
Continuous wavelet transform [24, p366] of a 
function f(t) has been defined by the following 
integral: 
 
ÂŒÂ[],  = ÂŽ Â,
ÂÂÂÂÂÂÂÂÂ
:
O:

 
Where the wavelet Â, âˆˆ Â‘ is defined by 
 
Â, =
1
`||
Â Â’ âˆ’ 

Â“ ,
,  âˆˆ Â‘,  â‰  0 
It is clear from the above definitions that the wavelet 
theory uses the infinite time assumption and therefore 
is not appropriate for signals of finite duration. It has 
the same application problem that the Fourier 
transform has. As mentioned before all practical 
problems are based on finite time assumptions. Since 
the scaling functions and the wavelets are used with 
translations on time axis, only very few and finite 
number of translations can be used over a finite 
duration interval. 
It is also well known that the wavelet transform 
uses the linearity assumptions [24, p378]. Therefore it 
has the same problems discussed in Section 9 above 
and should not be used for most engineering 
applications. 
Because many wavelets are orthogonal functions 
they will be very helpful in implementation of the 
function modulation systems described in Section 3. 
However it is not really known at this time how many 
band limited orthogonal wavelets can be constructed 
over a finite duration symbol time. Although wavelets 
are band limited but their bandwidth appears to be 
very high. 
16
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

 
11. Conclusion and Future Work 
 
We have given various proofs to show that k 
times, k>1, the Nyquist sample rate is necessary to 
improve the accuracy of recovering a function that is 
available only over finite time measurement window. 
We have shown that this k can be selected based on 
the required accuracy estimate Îµ.  
The foundation of our derivations used the infinite 
dimensionality property of the function space.  The 
concept essentially means that an infinite number of 
samples are necessary to precisely extract all the 
information from a function.  
We have pointed out that many of our existing 
definitions and theories depend on the infinite time 
assumptions. We should systematically approach to 
eliminate this requirement from all our theories to 
make them realistic for our engineering problems. 
 
12. Acknowledgement 
 
The first author wishes to express his sincere thanks to 
our friend and colleague Hari Sankar Basak for his 
extensive and thorough review of our original 
manuscript and for his many valuable questions and 
comments. 
 
13. References 
 
[1] S.Das, N.Mohanty, and A. Singh, â€œIs the Nyquist rate 
enough?â€, ICDT08, Bucharest, Romania, available at 
IEEE Xplore, 2008 
[2] C. E .Shannon, â€œCommunication in the presence of 
noiseâ€, Proc. IEEE, Vol 86, No. 2, pp447-457, 1998  
[3]   Xilinx, â€œAudio sample rate converter. Reference design 
for Xilinx FPGAsâ€, PN-2080-2, San Jose, California, 
2008 
[4] S.Das, N.Mohanty, and A. Singh, â€œCapacity theorem 
for finite duration symbolsâ€, ICN09, Guadalupe, 
France, available at IEEE Xplore, 2009 
[5] P.A.Laplante, Real-time systems design and analysis, 
Third Edition, IEEE Press, New Jersey, 2004 
[6] S.Das, 
N.Mohanty, 
and 
A.Singh, 
â€œA 
function 
modulation method for digital communicationsâ€, 
WTS07, Pomona, California, available at IEEE Xplore, 
2007 
[7] S.Das and N.Mohanty, â€œA narrow band OFDMâ€, VTC 
Fall 04, Los Angeles, California, available at IEEE 
Xplore, 2004 
[8] M.A.Al-Gwaiz and S.A.Elsanousi, Elements of real 
analysis, Chapman & Hall, Florida, 2007 
[9] Y.Eideman, V.Milman, and A.Tsolomitis, Functional 
Analysis, An Introduction, Amer. Math. Soc, Rhode 
Island, 2004 
[10] G.M.Phillips, Interpolation and Approximation by 
Polynomials, Can.Math.Soc., Springer, New York, 
2003 
[11] V.K.Ingle and J.G.Proakis, Digital Signal Processing 
using Matlab, Brooks/Cole, California, 2000 
[12] T. M. Cover and J.A.Thomas, Elements of Information 
Theory, Second Edition, John Wiley, New Jersey, 2006 
[13] R. Bellman, Introduction to Matrix Analysis, McGraw-
Hill, New York, 1970 
[14] J. Farlow, J. E. Hall, J. M. McDill, and B. H. West, 
Differential equations linear algebra, Prentice Hall, 
New Jersey, 2002. 
[15] A. J. Jerri, â€œThe Shannon sampling theorem, its various 
extensions and applications â€“ a tutorial reviewâ€, Proc. 
IEEE, Vol 65, No. 11, 1977 
[16] D.Slepian, â€œSome comments on Fourier analysis, 
uncertainty and modelingâ€, SIAM review, 1983 
[17] D. Slepian, â€œOn bandwidthâ€, Proc. IEEE, Vol. 64, No. 
3,  March 1976,  pp379-393 
[18] R.G.Lyons, Understanding Digital Signal Processing, 
Addison Wesley, Massachusetts, 1997 
[19]G.F.Franklin, 
J.D.Powell, 
and 
A. 
Emami-Naeini, 
Feedback Control of Dynamic Systems, Third Edition, 
AddisionWesley, Massachusetts, 1994 
[20]S.E.Lyshevski, 
Control 
Systems 
Theory 
with 
Engineering Applications, Birkhauser, Boston, 2001 
[21] A.B.Chatfield, Fundamentals of High Accuracy Inertial 
Navigation, AIAA, Vol 174, Massachusetts, 1997 
[22] R.M.Rogers, Applied Mathematics in Integrated 
Navigation Systems, Second Edition, AIAA, Virginia, 
2003 
[23] C.Cattani, â€œShannon wavelets theoryâ€, Mathematical 
Problems in Engineering, Vol. 2008, ID 164808, 
Hindawai Publishing Corporation, 2008. 
[24] L.Debnath, Wavelet Transforms & Their Applications, 
Birkhauser, Boston, 2002 
[25] C.S.Burrus, R.A.Gopinath, and H.Guo, Introduction to 
Wavelets and Wavelet Transforms - A primer, Prentice 
Hall, New Jersey, 1998 
 
 
 
 
 
 
17
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

