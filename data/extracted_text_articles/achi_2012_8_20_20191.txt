 
 
 
 
Christina Dicke 
relevantive AG 
Berlin, Germany 
christina.dicke@relevantive.de 
 
 
 
Abstract — In this paper, we propose a low cost, laboratory 
based testing framework for in-vehicle interfaces. Exemplified by 
a comparison between an auditory interface, a Head-up display, 
and a combination of both we show how task completion times, 
driving penalty points, mental workload, and subjective user 
evaluations of the interfaces can be collected through different 
logging systems and user questionnaires. The driving simulator 
used in the experiment enables the simulation of varying traffic 
conditions as well as different driving scenarios including a 
highway and a busy city center. Only some preliminary results 
are reported in this paper. 
 
Keywords-Human-computer interaction; auditory interface; 
head-up display; car simulator; driving performance. 
I.     INTRODUCTION 
Driver distraction is estimated to contribute to up to 25 
percent of vehicle crashes [1]. It is defined as the direction of 
attention away from safely handling the vehicle towards an 
object or event in the internal or external vehicle environment 
[2]. In many cases this distraction originates from in-vehicle 
tasks that are unrelated to driving such as making a phone call, 
sending text messages, or adjusting controls within the car. 
Distraction generated by interacting with smartphones is likely 
to increase as these devices are not only becoming more 
popular and affordable, but also offer services tailored towards 
in-vehicle use such as GPS (Global Positioning System) based 
navigation and real time traffic information [3].  
According to the multiple resource theory of attention [4], 
humans only have limited amounts of attention available at 
any given time. Different tasks can use different attention 
resources or share them. If resources are shared interferences 
may occur leading to decreased performance in all tasks. For 
example, driving a car demands a significant amount of visual 
attention. Operating a navigation system or mobile phone 
through a visual interface while driving competes for the same 
resource associated with visual perception and is therefore 
likely to cause distraction from the primary (driving) task [5] 
[6]. It has also been shown that physical and cognitive 
distraction significantly impair the driver’s visual search 
patterns, reaction times, decision-making processes, and the 
ability to maintain speed, throttle control, and lateral position 
on the road [1][7].  
Integrating the smartphone handset with in-car electronics 
and merging the access to all car and mobile device functions 
in one haptic interface, such as a multifunctional steering 
wheel, can help to reduce the haptic distraction. Head-up 
displays (HUD) have been proposed as a solution to reduce 
the frequency and duration of the driver’s eyes-off-the-road by 
projecting information on the windshield. HUDs, when  
Grega Jakus, Sašo Tomažič and Jaka Sodnik 
Faculty of Electrical Engineering 
University of Ljubljana, Slovenia 
grega.jakus@fe.uni-lj.si 
saso.tomazic@fe.uni-lj.si 
jaka.sodnik@fe.uni-lj.si 
 
compared to HDD (Head-Down Displays), have been shown 
to reduce the response times to unanticipated road events and 
lead to smaller variances in lateral acceleration and steering 
wheel angle [8]. However, they have also been shown to 
increase mental load as indicated by longer response times in 
high workload situations [9][10]. 
To reduce visual distraction, speech-based interface have 
been proposed [11][12] as they demand resources associated 
with auditory perception and are therefore less detrimental 
[13]. However, extensive user testing with particular emphasis 
on the evaluation of the distractive potential and the user 
experience of individual services or interfaces is crucial and 
can only be partially derived from prior research. Therefore, 
we propose a testing framework that allows for low cost, 
laboratory based usability testing. Using the example of a 
comparison between three different interfaces for interacting 
with 
an 
in-vehicle 
communication, 
navigation, 
and 
entertainment system while driving, a HUD display, an 
auditory interface, and a combination of both, we demonstrate 
how these interfaces can be prototypically realized and 
thoroughly evaluated. 
II.    USER STUDY 
The design rationale of this study is to investigate the 
impact of multimodal interfaces for in-vehicle control systems 
on driving and task solving performance. In particular, an 
audio only interface is compared to a visual only HUD and a 
combination of both. To simulate a realistic use case, the 
experiment is running in a driving simulator. Participants 
perform tasks of different complexity while they drive the 
simulated vehicle on different routes and with different traffic 
conditions. The driving simulator depicted in Figure 1 consists 
of a large projection screen, a steering wheel, accelerator and 
brake pedals, and the HUD shown in Figure 2. 
 
 
Figure 1. The car simulator consisting of a large projection screen, a steering 
wheel, foot pedals and a gear stick. 
On the Evaluation of Auditory and Head-up Displays While Driving
200
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

 
 
 
 
By means of a custom-made interaction device attached to 
the steering wheel (shown in Figure 3, left) participants can 
navigate through a menu structure as illustrated in Figure 3, 
right. 
 
 
Figure 2.  The visual interface implemented as a HUD. 
 
Three different experimental conditions are created by using 
three different user interfaces. The same menu structure is 
used with all three interfaces. There are up to 8 items on each 
level with the top level containing the following items: 
• Heating and Cooling 
• Entertainment 
• Communications 
• Navigation and Traffic 
• Trip computer 
 
 
 
Figure 3. The interaction device and its use for navigating the menu structure. 
III. EXPERIMENT DESIGN 
The study design is a within subjects 3 x 2 (Interface x Task 
Complexity) design with participants randomly allocated to 
one of six groups. To prevent learning effects each group has a 
different combination of route difficulty and conditions. 
For example, one group starts with the auditory interface in 
a low traffic situation, proceeds with the visual interface in a 
low traffic situation, and ends with the audio-visual interface 
in a high traffic situation. A second group starts with the 
audio-visual interface in a low traffic situation, proceeds with 
the visual interface in a high traffic situation, and ends with 
the auditory interface in a high traffic situation and so on. 
 
A.    Experiment Apparatus 
1) 
Computer Setup: Three different computers are used for 
the experiment running: 
• A driving simulator (DS), 
• User interaction (UI) application and 
• Management and logging (ML) software suite 
The computers are communicating via TCP/IP protocol 
stack in a private network. The DS machine is used for 
running the driving simulator and for logging driving errors. 
The UI machine is running the user interaction application 
and constantly reports the events connected with the 
interaction device (button clicks, mouse wheel turns) to the 
ML computer. 
The ML computer is used as a main machine for conducting 
the experiment. The operator of the ML machine can control 
the driving simulator using remote desktop software. The ML 
computer is also used for collecting driving and interaction 
device events, measuring task completion times, and for 
filling-in the user questionnaires. 
The software running on the UI and ML machines was 
developed on the Java platform. The input data required by the 
UI application are stored in an XML (eXtensible Markup 
Language) file. The file contains hierarchical menu structure, 
textual content that is displayed in visual interface, and 
references to pre-recorded sound clips for the auditory 
interface. 
 
 
 
Figure 4. Experiment setup 
 
2) Beamers: The visual interface and the image of the driving 
simulator are projected on a projection screen using two 
beamers. The beamer projecting the simulator image is 
mounted on the ceiling of the room, while the beamer 
projecting HUD is placed on the driver’s right side in the 
height of his/her shoulders. 
3) Car Simulator: The essential component of the experiment 
is a driving simulator software. We chose City Car Driving 
version 1.2 [14], which was projected on the big screen. It 
201
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

 
 
 
 
supports the simulation of multiple driving environments, such 
as different regions of a city center, a motorway or a highway. 
It also enables a variety of different driving routes and traffic 
intensity conditions. The Thrustmaster’s “RGT Force 
Feedback CLUTCH Racing Wheel” is used as the input 
device, which comprises of a steering wheel, three foot pedals 
and a gear stick. Car and environment sounds are played using 
Genelec’s “8030A bi-amplified monitoring system” consisting 
of two loudspeakers placed on both sides of the projected 
image. 
Test subjects are driving a left-handed Peugeot 206 CC with 
automatic gearbox. The traffic is right-handed. The route 
named “Motorway” with 10 percent traffic intensity is used 
for the “low traffic” condition while the route named “Modern 
district” with 50 percent traffic is used for “high traffic” 
condition. Test subjects are given loose navigation instructions 
ensuring comparable driving experience. 
B. 
Visual Interface 
The visual interface (Figure 2) is a 20 x 20 cm HUD 
projected to the right-central position of the windshield (above 
the car’s central LCD display which was not used in the 
study). The HUD displays five of the available items of the 
selected menu or submenu. When there are more than five 
items available, the user can access them by “scrolling” up or 
down in the current menu level. The menu is designed to be 
non-circular with the peg at both ends of the menu or 
submenu. The menu items are displayed in a high contrast 
yellow color. The selected item is highlighted with red fonts 
with slightly increased font size compared to non-selected 
items. On the top of the HUD a green colored title indicates 
the currently selected submenu. The setup was designed and 
programmed 
to 
allow 
for 
a 
quick adjustment 
and 
accommodate required changes of the menu structure, 
position, size, and content. 
C. 
Auditory Interface 
The auditory interface is based on prerecorded sound clips 
generated by AT&T Labs TTS Demo (text-to-speech) 
technology. A male voice called “Mike” is used for the main 
menu structure while other voices are used for imitating 
various tasks (voicemail messages, traffic report service, etc.). 
The OpenAL and JOAL libraries are used for the creation of 
dynamic sound sources. Sounds are played through two 
computer loudspeakers, which are placed at the usual position 
of car speakers mounted in vehicle doors. 
D. 
Interaction Device 
The navigation within the menu is enabled by using a 
custom-made interaction device – a small mouse attached to 
the steering wheel (Figure 3, left). The interaction device 
consists of two buttons and a scrolling wheel, which can also 
act as a third button if pressed. The scrolling wheel is used to 
navigate among the items available at a certain level of the 
menu structure. If the scrolling wheel is pressed while using 
the acoustic interface, the title of the current submenu is 
played. The other two buttons are used to confirm the current 
selection or to exit the current submenu and move up one level 
in the menu structure (cf. illustration in Figure 3). 
 
E. 
Experiment Conditions and Tasks 
Three different experiment conditions were defined. The 
conditions “A” and “V” are based on the acoustic and visual 
interfaces described in previous sections while the condition 
“AV” is based on the combination of both. Five tasks are 
performed within each experimental condition. Each group of 
tasks consists of three simple “atomic” and two “complex” or 
difficult tasks. The difficulty of the tasks is defined by the 
effort and physical activity required to finish the individual 
task (number of mouse clicks and wheel turns). A sample set 
of five tasks is listed in Table 1. 
TABLE I.      SAMPLE SET OF TASKS 
Atomic tasks 
Task 1 
Change the fan speed to “3”. 
Task 2 
Play the song “Yesterday” by The Beatles. 
Task 3 
Check your fuel level. 
Complex tasks 
Task 4 
You want to travel to New York City. Please 
check the traffic report for New York City and 
tell the name of the street mentioned in the report 
to the experimenter. 
Task 5 
Please set your navigation system to take you to 
New York City. When asked, verbally enter 
(dictate) the name of the street suggested by the 
traffic report into the navigation system. 
 
F. Logged Data & Questionnaires 
1) Driving errors: The driving errors and anomalies are 
recorded by the driving simulator software and saved into a 
database. The records in the database contain the 
information about the occurrence of an error, its description, 
and severity. The later is described using penalty points. The 
ML machine is responsible for organizing and archiving the 
driving data for each subject per experimental condition. 
 
2) Task Completion Times: Task completion times are 
measured manually by the operator of the ML machine using 
a logging application. The measurement starts when the 
instruction “Please start now!” is given to the test subject 
and it is completed when the task is completed successfully. 
 
3) Video Recordings: The entire user study is recorded with 
a HDR-XR105 Sony digital video camera. The recordings 
are used to perform an additional post-evaluation of driving 
performance and general safety. This allows for a recap and 
further analysis of situations in which severe driving errors 
were recorded by the system. 
 
4) NASA TLX: Hart and Staveland’s NASA Task Load 
Index (TLX) method assesses work load on five seven-point 
Likert scales [15]. It is a subjective, multidimensional 
assessment tool that rates perceived workload on six 
different subscales: Mental demand, physical demand,  
 
202
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

 
 
 
 
temporal demand, performance, effort, and frustration. An 
example of the TLX is given in Figure 5.  
 
 
Figure 5. Example of items in the NASA Task Load Index. 
 
5) User Experience Questionnaire (UEQ): The UEQ [16] is 
a tool for the user-driven assessment of software quality and 
usability. It consists of 26 bipolar items, each to be rated on 
a seven-point Likert scale. It has been developed to measure 
six 
factors: 
Attractiveness, 
perspicuity, 
efficiency, 
dependability, stimulation, and novelty. An example of three 
items is presented in the Figure 6. 
 
   Annoying 
{ { { { { { { enjoyable 
   Predictable 
{ { { { { { { unpredictable 
   Efficient   
{ { { { { { { inefficient 
 
Figure 6. Example of items in the User Experience Questionnaire. 
 
G. 
Experiment Procedure 
Before the experiment participants are given a thorough 
introduction to the driving simulator, the interaction device, 
and the menu structure. After they familiarize themselves with 
the simulator and the menu (approx. 20 minutes) they begin 
the experiment by either first using the visual, the audio-
visual, or the auditory interface depending on their random 
assignment to one of six groups. During the experiment, 
participants are asked to drive either on a motorway or 
through a busy city center while they are given first a set of 
atomic tasks followed by a set of complex tasks. After each 
condition, participants complete the electronic version of the 
NASA TLX followed by a complete UEQ to evaluate their 
experience of the particular interface they just used. After 
participants complete all three conditions they are asked to fill 
a short post-study questionnaire on their overall perception of 
the interfaces, the readability of the projection, the sound 
design, the realism of the driving simulator, and the task 
design.  
IV.   PRELIMINARY RESULTS 
A total of 30 test subjects participated in the experiment. 
The proposed experimental setup and apparatus have proven 
to be robust, flexible, and suited for evaluating the interfaces. 
The brief analysis of task completion times identified the 
audio-visual interface as the fastest and the audio interface as 
the slowest of all three. 
On the other hand, the best control of the car was noticed 
when using the audio interface and the worst when using the 
visual interface. We also noticed a higher compliance with the 
traffic rules when the test subjects were performing the tasks 
compared to just driving. This could partly be explained by  a 
reduction in driving speed when performing the tasks. 
Based on the preliminary results, the participants show an 
overall preference for the audio-visual combination. However, 
one fourth prefers only the audio or the visual interface 
respectively. The thorough analysis of all data collected in the 
experiment will bring more detailed insights that will enable 
further adjustments and user testing of the proposed interfaces. 
  
REFERENCES 
[1] D. L. Hibberd, S. L. Jamson, and O. M. J. Carsten, “Managing 
in-vehicle distractions: evidence from the psychological 
refractory period paradigm” Proc. of the 2nd International 
Conference on Automotive User Interfaces and Interactive 
Vehicular Applications, pp. 4-11, 2010. 
[2] M. A. Pettitt and G. E. Burnett, “Defining driver distraction,” 
Proc. of World Congress on Intelligent Transport Systems, San 
Francisco, USA, 2005.  
[3] S. Damiani, E. Deregibus, and L. Andreone, “Driver- vehicle 
interfaces and interaction: where are they going?” European 
Transport Research Review, vol. (2), pp. 87-96, 2009. 
[4] C. D. Wickens, “Processing resources in attention,” in Varieties 
of Attention (R. Parasuraman and R. Davies, eds.), pp. 63–102, 
New York, NY, USA: Academy Press, 1984. 
[5] P. Green, “Crashes induced by driver information systems and 
what can be done to reduce them,” in Proceedings of 
Convergence 2000, Soc. of Automotive Engineers, (Warrendale, 
PA, USA), pp. 26–36, Society of Automotive Engineers, 2000. 
[6] W. Wierwille and L. Tijerina, “Vision in vehicles vi,” in 
Modelling the Relationship Between Driver In-Vehicle Visual 
Demands and Accident Occurrence (A. Gale, I. Brown, C. 
Haslegrave, and S. Taylor, eds.), pp. 233–244, Elsevier, 1998. 
[7] K. L. Young, M. A. Regan, and M. Hammer, “Driver 
distraction: A review of the literature,” Tech. Rep. 206, Monash 
University Accident Research Centre, Victoria, Australia, 2003. 
[8] L. Yung-Ching, “Effects of using head-up display in automobile 
context on attention demand and driving performance” Displays, 
vol. 24(4-5), pp. 157-165, 2003. 
[9] E. Fisher, R.F. Haines, and T.A. Price, “Cognitive issues in 
head-up displays,” NASA Technical Paper 1711, NASA Ames 
Research Center, Moffett Field, CA, 1980. 
[10] C. D. Wickens, R. Martin-Emerson, and I. Larish, “Attentional 
tunneling and the head-up display,” in: R.S. Jensen (Ed.), 
Proceedings of the Seventh International Symposium on 
Aviation Psychology, Ohio State University,  Columbus,  pp. 
865 – 870, 1993. 
[11] J. Sodnik, C. Dicke, S. Tomazic, and M. Billinghurst, “A user 
study of auditory versus visual interfaces for use while driving,” 
Int. J. Human-Computer Studies, vol. 66(5), pp.  318–332, 2008. 
[12] M. Jeon, B. K. Davison, M. A. Nees, J. Wilson, and B. N. 
Walker, “Enhanced auditory menu cues improve dual task 
performance and are preferred with in-vehicle technologies,” 
Proceedings of the 1st International Conference on Automotive 
User Interfaces and Interactive Vehicular Applications, 2009. 
[13] J. D. Lee, B. Caven, S. Haake, and T. L. Brown, “Speech-based 
inter- action with in-vehicle computers: The effect of speech-
based e-mail on drivers’ attention to the roadway,” Human 
Factors: The Journal of the Human Factors and Ergonomics 
Society, vol. 43(4), pp. 631–640, 2001. 
[14] City Car Driving, Car Simulator, http://citycardriving.com/, 
accessed 14. November 2011 
[15] S. G. Hart, "NASA-Task Load Index (NASA-TLX); 20 years 
later," in Human Factors and Ergonomics Society 50th Annual 
Meeting San Francisco, CA, pp. 904-908, 2006.  
[16] B. Laugwitz, T. Held, and M. Schrepp, “Construction and 
evaluation of a user experience questionnaire,” in: Holzinger, A. 
(Ed.): USAB 2008. LNCS 5298, pp. 63-76, 2008. 
203
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

