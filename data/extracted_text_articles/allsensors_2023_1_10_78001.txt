Characterization of an IoT Stereo Image Sensor System for Weed Control
Bruno M. Moreno1,2, Paulo E. Cruvinel1,2
1 Embrapa Instrumentation (CNPDIA), P.O. Box 741, 13560-970, S˜ao Carlos, SP, Brazil
2 Postgraduate Program in Computer Science, Federal University of S˜ao Carlos, S˜ao Carlos, SP, Brazil
Emails: bruno.moreno@estudante.ufscar.br; paulo.cruvinel@embrapa.br
Abstract—Smart farming has emerged as a new option to
deal with the adversities of food generation in agriculture in the
face of growing demand, with the aim of increasing productivity
allied to the search for a more sustainable world. One of the
objects of study in precision agriculture is weed control and the
conscious use of inputs. This article presents the development
and characterization of an embedded stereo system using camera
sensors and Internet of Things principles, for future application
in the area of digital image processing. Concepts of validation
of lenses by Modular Transfer Function, calibration of intrinsic
parameters of sensors and 3D system and memory and energy
consumption management are analyzed, and implemented in the
construction of an equipment, which aim to control invasive
plants in crops.
Keywords—camera sensor; stereo vision; embedded platform;
weed control; agricultural industry.
I. INTRODUCTION
Agriculture is a very important source of food, feed, fiber
and even fuel. Despite this, agriculture currently faces the
challenge of increasing its production in response to the
demand of continued population growth, taking precautions
against the various adversities caused by the climate and
minimizing the impact of man on nature [1].
One of the approaches aimed at increasing productivity in
the field is the reduction of losses due to factors exogenous
to crops, such as competition resulting from the presence of
invasive plants. The presence of weeds in the cultivation area
can decrease crop yield by more than 50% just by competing
with the moisture present in the soil, causing more damage
than invasive animals, diseases and other pests [2]. Therefore,
weed control is essential so that the nutrients present in the
soil, the development space and the reception of sunlight
remain exclusively for the plant of interest [3].
Moreno and Cruvinel presented previous studies related to a
stereo camera’s system [4], and the development of a software
based on semantic computing concepts for the segmentation
of weed plants [5].
Although the use of pesticides has already been established
to deal with this problem, technological applications aimed at
the rational use of inputs are desired. Among such technolo-
gies, Computer Vision stands out, which works in two stages:
image acquisition and image processing. The acquisition is
made exclusively from camera sensors, capturing the environ-
ment and patterns present in digital images. Such sensors can
then capture the visible or thermal spectrum, and be coupled
to vehicles, devices, robots, drones and even satellites. On
the other hand, affordable single-board computers have made
onboard image processing possible [6].
Image processing can be summarized in five steps. In the
first, the raw data are pre-processed, removing noise and
selecting only the object of interest. In another step, pattern
features are extracted, whereas in the case of plant images,
such parameters are related to color, shape and texture. In
the third stage, the features go through a selection process,
decreasing the dimensionality of the data. Afterwards, the
data are classified, grouping them based on their similarities.
Finally, in the decision making stage, new input data can
be classified from the already trained model, thus identifying
which group it belongs to [7][8].
To ensure that the input data are of good quality, validating
and using good camera sensors have become extremely impor-
tant. Allied to this, other points of consideration in the appli-
cation of such techniques in agriculture are the management of
the volume of data generated, the data analysis techniques that
need to deliver interpretable and understandable results due to
the interdisciplinarity of workers in the area, and the mobile
systems that need to be able of handle scarce resources such
as limited battery life, low computational power and limited
bandwidths for data transfer [9].
As examples of the use of camera sensors in the field, there
are applications coupled to vehicles to operate during pre-
planting and analyze the height and density of vegetation from
the images [10] and identify the location of invasive plants for
manual control via weeding machine [11]. Plant images can
also be acquired to create a database on an external server for
further processing, for training a future classifier [12].
This paper is structured as follows. Section II presents the
materials and methods used, including the Internet of Things
(IoT) system description, camera sensor specifications, stereo
vision basics, and embedded board specifications. Section III
presents the results of the validation of the sensor and of the
stereo system, the power supply and memory limitations and
the final prototype, with the final conclusions in Section IV.
II. MATERIALS AND METHODS
The developed system aims to capture stereoscopic images
in a real environment of plantations, so that the presence and
concentration of weeds present in the region of interest can be
identified from an embedded algorithm. The capture of stereo
images requires two camera sensors, generating two images of
the same area that will be the input of the system. The images
are then processed and grouped into classes, and the data will
1
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-083-4
ALLSENSORS 2023 : The Eighth International Conference on Advances in Sensors, Actuators, Metering and Sensing

be prepared for sending to a module external to the system,
which will be responsible for spraying the site.
A. High-level IoT architecture
Embedded systems have a potential in agricultural use due
to their mobility, low cost and computational power, allowing
the performance of complex tasks in a more practical way.
Raspberry Pi (RPi) is being used in several applications and
it is the leading candidate for hardware implementation due
to its powerful processor, rich I/O interface and compatibility
that allows most projects to run on it [13]. Its wireless
communication also makes the RPi capable of working with
IoT projects, allowing objects to be sensed or controlled
remotely across existing network infrastructure and reducing
human intervention [14].
IoT systems in agriculture are separated into three modules:
farm side, server side and client side. The farm side usually
consists of detecting local agricultural parameters, identifying
the location and sensor data, transferring crop fields data
for decision making, decision support and early risk analysis
based on recent data, and action and control based on the
monitoring of the crop [15]. As can be seen from the block
diagram in Figure 1, the developed IoT Stereo System can
gather image data in the field, pre-process, segment, create
feature extraction and depth information vector, classificate
and interpret the collected data, while being controlled and
monitored via Bluetooth serial communication by a mobile
app.
On the server side, the network layer is responsible for
reliable transformation to the application layer. A Wireless
Personal Area Networks (WPAN) network can be mounted
on a single board computer, with its own unified control and
monitoring console for various wireless networks. Data trans-
port and storage become essential, with data that can be saved
on an external server or in the cloud, and then transferred to
other devices, including the equipment responsible for product
spraying on the plantation. The last module, the client side or
application layer, collects and processes information, provid-
ing an environment where users can monitor data processed
by the system via a web browser, anywhere and anytime.
B. Embedded System and Camera Sensor Specifications
RPi is a series of mini-embedded computers developed
in the United Kingdom by the Raspberry Pi Foundation in
association with Broadcom. The model used was the RPi 3 B+,
where its specifications can be seen in Table I. It is important
to note that board must be powered with a nominal voltage
of 5 V capable of delivering 2.5 A of current, with operating
temperature between 0 °C and 50 °C. The internal memory
is defined from a micro Secure Digital (SD) card, where the
kernel of the operating system is also present, being recom-
mended the use of at least 8 GB of memory. The RPi 3 B+,
unlike previous family models, enables BCM43438 wireless
Local Area Network (LAN) and Bluetooth Low Energy (BLE)
communication, allowing wireless data exchange.
Figure 1. High-level system architecture diagram.
TABLE I
RASPBERRY PI 3 MODEL B+ CHARACTERISTICS
Processor
BCM2837B0 Cortex-A53 (ARMv8) 64-bit
Clock
1.4 GHz
GPIO
40 pins
Memory
1 GB SDRAM
Gigabit Ethernet
1 connector
USB Port
4 USB 2.0
HDMI
1 connector
Camera serial interface (CSI)
Display serial interface (DSI)
Wireless (dual band)
Bluetooth 4.2/BLE
3,5mm 4 Jack output
Micro SD card slot
Support Power-over-Ethernet
Input DC 5V/2.5A
TABLE II
PI CAMERA CHARACTERISTICS
Size
25 x 24 x 9 mm
Resolution
5 MP
Video modules
1080p30, 720p60, 640x480p60/90
Sensor
OmniVision OV5647
Sensor resolution
2592 x 1944 pixels
Sensor image area
3.76 x 2.74 mm
Pixel size
1.4 µm x 1.4 µm
Optical size
1/4”
Full-frame SLR equivalent
35 mm
S/N Ratio
36 dB
Dynamic range
67 dB @ 8 times gain
Fixed focus
1 m - ∞
Focal length
3.60 ± 0.01 mm
Horizontal field of view (HFOV)
53.50° ± 0.13°
Vertical field of view (VFOV)
41.41° ± 0.11°
Focal ratio (F-stop)
2.9
The RPi has its own camera sensor alternatives, including
the Pi Camera v1, with specs shown in Table II. Among the
most important parameters, stand out the fixed focal length
of 3.60 mm, the maximum sensor resolution of 2592 x 1944
pixels, and the camera opening angle of 53.50º horizontally
and 41.41º vertically.
C. Modular Transfer Function as Camera Sensor Validation
The Modular Transfer Function (MTF) expresses how well
an optical system preserves the contrast of spatial frequencies
of the object in the image and is a well-established perfor-
mance method [16]. A popular way to estimate the MTF curve
for spatial frequency is called the inclined knife-edge method,
in which the curve is obtained from a region of the image
where there is a transition from a very dark tone to a very light
2
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-083-4
ALLSENSORS 2023 : The Eighth International Conference on Advances in Sensors, Actuators, Metering and Sensing

tone [17]. An Edge Spread Function (ESF) is calculated from
the recorded knife edge, giving the one-direction response
of the imaging system to an edge object. The Line Spread
Function (LSF) is obtained by the derivative of the ESF.
Then, the use of a camera sensor can be defined taking
into account the calculation of the LSF of the camera lens
and MTF, which represents the magnitude response of the
optical system to sinusoids of different spatial frequencies,
i.e., retrieved by Fourier transform of the LSF. Taking a linear
source the solution to measure the MTF is in 1D, orthogonally
to the direction of the line. This can be proven considering a
given source S(x, y) = δ(x).C, and a lens of diameter equal
to a, that means:
R(kx, ky) =
Z Z a/2
−a/2
δ(x)Cej(kxx+kyy)dxdy
(1)
The response of the objective can be expressed as the square
of the Fourier transform of the product of the source with
the aperture of the lens R2(kx, ky), with (kx, ky) the spatial
frequencies associated with the spatial coordinator (x, y).
Besides, looking for a solution of (1) and solving the integral
by parts is possible to reach:
R2(kx, ky)αsin2(aky)
(aky)2
(2)
Equation (2) corresponds to the LSF. The Fourier Transform
of the LSF then gives the 1D MTF in the yy-direction. In
stereo systems, usually the MTF system data are summarized
as a curve for each sensor or just the curve of the lowest
quality sensor [18]. Therefore, the images from the cameras
are convolved based on the multiplication of their MTFs in
the frequency domain.
D. Stereo Vision Principles
Stereo vision systems are usually based on the use of two
cameras with the aim of simulating the human vision system
and obtaining depth of objects, with the camera plane as
a reference. The depth is acquired through the comparison
of the object’s position between each captured image [19].
The simplest way of comparing both images is guaranteed
when the cameras are coplanar and aligned, as shown in
Figure 2. The variables defined by the camera system are the
baseline b and the focal distance f. The P(X, Y, Z) represents
a point that would be recorded by the two cameras and
uL = (XL, YL) and uR = (XR, YR) are the projections of
this point in each image. From the concepts of geometry and
similarity of triangles, it is possible to obtain:
Z =
bf
XL − XR
= bf
d
(3)
The d variable is called disparity. Thus, with two images
as inputs in a calibrated and synchronized stereo architecture,
depth information is obtained by finding the corresponding
pixels in both images (uL and uR) by a matching algorithm
and subtracting their X-axis coordinates. By performing this
operation for all paired pixels in the image, the disparity map
Figure 2. Stereo vision model.
is obtained, which contains all the depth information in the
image.
It is also important to note the distortion that variations
in the disparity map can cause in the depth estimation, i.e.,
verify the measurement obtained accuracy. So, for a variation
in depth, it is possible to find:
∆Z = Z −
bf
d + ∆d =
Z2∆d
bf + Z∆d ≈ Z2∆d
bf
(4)
During image capture, the focal length f and baseline b are
fixed, but the baseline distance can be adjusted to minimize
this distortion, taking into account the expected distance of
objects from the camera. Thereby, the system calibration can
be done from an image of a chessboard on which its pattern
allows to rotate and adjust the stereo images so that they
are lined up and the difference between them is only in
the horizontal dimension. Before calibrating the system, it
is necessary to find the calibration of each camera, which is
defined by two matrices, the camera matrix and the distortion
matrix [20]–[22]. The camera matrix, presented in (5), is
composed of intrinsic parameters as focal length fx and fy,
and optical centers cx and cy. On the other hand, (6) shows
the distortion matrix, where q1, q2 and q3 represent the lens
radial distortion coefficients and p1 and p2 the lens tangential
distortion coefficients.
camera matrix =


fx
0
cx
0
fy
cy
0
0
1


(5)
distortion coefficients =
q1
q2
p1
p2
q3

(6)
Finally, the calibration of the entire stereo vision system can be
summarized by (7), where Rx represents the rotation factor
and Tx the translation factor between the captured left and
right image [23][24].
Left Image = Rx ∗ Right Image + Tx
(7)
3
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-083-4
ALLSENSORS 2023 : The Eighth International Conference on Advances in Sensors, Actuators, Metering and Sensing

Note that unlike the camera matrix and distortion coeffi-
cients which depend only on the camera, the Rx and Tx
matrices must be recalculated if any stereo system settings
change such as, for example, the baseline distance.
III. RESULTS AND DISCUSSIONS
Experimental results were focused on the instrumentation’ s
characterization, i.e., including both the sensors and hardwares
associated with signal and image processing. So far, the images
for such a characterization were collected at laboratory level
only. The system is based on eight elements, as follows: 12
V battery; 12 Vdc to 220 Vac voltage inverter; Light Emitting
Diode (LED) lamp; 110-220 Vac to 5 Vdc rectifier; two RPis
and two Camera Pi, as the schematic presented in Figure 3. All
components are fixed on a metallic structure, with adjustable
distance between cameras, angle of inclination (0º, 90º, 180º,
270º) and height of the cameras in relation to the ground (10
to 100 cm). The built system can be seen in Figure 4.
The system is controlled by an Android App via Blue-
tooth serial communication, where commands can be sent:
synchronous image capture on the two RPis, send the images
to the cell phone to check the quality of the capture, check
the amount of images saved on memory, and board reboot
or shutdown command. The RPis also communicate with
each other via Bluetooth protocol, that supports up to 7
accessory devices, and uses Radio Frequency Communication
(RFCOMM) Bluetooth protocol in data transfer with the cell
phone. To ensure system security, it connects only to trusted
equipment on specific ports.
A. Energy consumption management
A RPi can has power consumption of up to 12.5 W, but
in laboratory tests the usual value during the application of
the image capture software was only 3 W. As the system
was designed with a inverter, the power consumed by this
equipment must also be considered for system evaluation and
possible improvements. In this case, the inverter in question
presented a spent power of around 8.4 W, significantly higher
than the sum of the RPis. To deal with such power, a battery
of 12 V and 60 Ah was chosen.
To measure the energy expenditure of the system, current
and power were calculated in different situations, according to
Table III, with battery voltage fixed at 12.0 V. To evaluate the
battery capacity, a test was carried out in the most extreme
situation, with the system in continuous operation with the
18 W LED lamp on, which resulted in the maintenance
of operation for approximately 15 hours. When the battery
was discharged to 11.7 V, the inverter stopped as a safety
precaution. It was observed that, in this operating mode, the
peak current at system startup was close to 3.2 A, while with
the same configuration but with the less potent LED lamp the
peak was 2.0 A.
B. Memory management
For each RPi a 32 GB SD memory card was selected. After
the initial settings, the necessary programs installed and the
Figure 3. Diagram of the connection between the components.
(a) Details of the camera, stereo rig
and lamp.
(b) Interior of protective case, with
RPis and rectifier.
Figure 4. Developed system.
TABLE III
SYSTEM POWER AT DIFFERENT SETTINGS
Mode of operation
Current (A)
Power (W)
Standard
1.3
15.6
With active camera sensors
1.4
16.8
With active camera sensors
1.9
22.8
and 4.5 W LED lamp
With active camera sensors
3.0
36
and 18 W LED lamp
capture algorithm developed, about 23.1 GB of memory was
free for general use. To ensure that the program can handle
the amount of data written and stored, it is good to know how
long the embedded system takes to save files. In testing, it
was found that the SD card sequential memory write rate is
14833 KB/s or 14.8 MB/s.
Such information is important to define the resolution in
which the images will be captured, as they define the size of
the files saved in memory. Following the dimension of the
camera sensor, it is preferable to define the resolution of the
captured images to take advantage of the entire sensor size,
that is, in which the 4:3 ratio is preserved. The maximum file
size can be calculated by multiplying the resolution by the
pixel depth, but since the Pi Camera doesn’t have the option to
format a RAW image file, the images are compressed, resulting
in smaller files. So, it was tested five resolutions, 640 x 480,
800 x 600, 1024 x 768, 1280 x 960 and the maximum 2592
x 1944. Early test results can be seen on Figure 5, where five
images in each resolution were taken and saved in the PNG
format.
Considering the future application in image processing, in
which the computational cost of operations tends to grow
4
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-083-4
ALLSENSORS 2023 : The Eighth International Conference on Advances in Sensors, Actuators, Metering and Sensing

Figure 5. Image file size experimentations.
(a) Left Camera.
(b) Right camera.
Figure 6.
Images of a calibration chessboard, captured synchronously and
without being processed.
exponentially according to the number of pixels present, and
the available SD memory, the resolution of 1280 x 960 was
then chosen. With this resolution, at least 6,000 images can be
saved in memory, although it is possible to store them later in
the cloud, from the system’s communication with an external
network, freeing up space on the board. It should be noted that
for future applications, if the maximum resolution is used, the
memory writing time must be taken into account as a limiting
factor.
C. Camera sensor validation
The first step in calculating the stereo MTF was to capture
an image of the chessboard with both cameras at the same
time, as can be seen in Figure 6. For each image, five random
regions were selected where there are knife edges recorded, in
the same location for both cameras. The normalized MTF was
calculated for each point and averaged between them, and after
that, the MTF of the stereo system was then calculated from
the multiplication of both MTFs, as can be seen in Figure 7.
The MTF value at the Nyquist frequency was then 14.31% for
the left camera, 8.97% for the right and 1.28% for the entire
system.
To evaluate the camera’s SNR ratio, only the regions of the
converted grayscale image where black blocks were presented,
which have a uniform color on the original chessboard, were
used, and the mean and standard variation of the signal were
evaluated. For the right camera, the calculated value was
Figure 7. MTF of each camera sensor and combined system.
Figure 8. Baseline distance disparity and distortion error evaluation.
19.7 dB, while for the left camera it was 17.9 dB, below the
36 dB specified by the manufacturer.
D. Stereo vision parameters
The first step in tuning the stereo system is to define the
baseline distance that will be used to capture the images.
The developed prototype has a minimum possible baseline of
6 cm and a maximum of 24 cm, which makes it capable of
simulating human vision, which has this value in the range
of 5.4 to 7.4 cm, in addition to allowing the exploration of
other scenarios. For this, considering (3) and (4), the expected
disparity for an object up to 1 m away from the camera and
the expected distortion error at such distance were calculated,
for four values of baseline, 6 cm, 12 cm, 18 cm and 24 cm,
as can be seen in the Figure 8, considering the resolution of
1280 x 960.
5
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-083-4
ALLSENSORS 2023 : The Eighth International Conference on Advances in Sensors, Actuators, Metering and Sensing

When setting the baseline distance, it is always preferable
to use the lower values to ensure greater interpolation between
the two generated images, which allows closer objects to have
their distance calculated. For example, according to the graph
shown, for b = 24 cm, objects up to 23.8 cm away from
the camera would not be present in both images, making it
impossible to calculate the disparity, while for b = 6 cm such
a situation is only valid for objects less than 5.9 cm away.
As for objects of up to 1 m, the distortion error proved to be
small for all cases, including for the scenario with the smallest
baseline, so it can be defined that the best use of the stereo
system occurs for values close to 6 cm.
Thus, for b = 6 cm and height of 1 m (value chosen
so that, due to the height of the growing plants, the object
under analysis is not too close to the sensors), the calibrated
parameters results of the left and right cameras, and of the
stereo system, were:
Left camera matrix =


736
0
582
0
735
464
0
0
1


(8)
Left distortion coefficients =


0.0589
−0.169
0.00139
0.00198
0.142


T
(9)
Right camera matrix =


1480
0
681
0
1480
480
0
0
1


(10)
Right distortion coefficients =


−0.0728
3.98
0.00117
0.00630
−22.6


T
(11)
Rx =


0.960
−0.0133
−0.281
0.0159
1.00
0.00721
0.280
−0.0114
0.960


(12)
Tx =


−0.787
−0.0670
5.65


(13)
Note that if the baseline distance is changed, it is necessary
to calibrate the system again, recalculating only the Rx and
Tx matrices, but it is expected that Rx will not change
significantly, as the mounted structure does not allow the
cameras to yaw, pitch or roll.
IV. CONCLUSION AND FUTURE WORK
The results showed a characterization process of an IoT
stereo image sensor system, capable of capturing and transfer-
ring validated images via wireless commands, ready to be used
in real agricultural field conditions. Such developed embedded
vision system can be useful for applications in 3D image
processing, with several variable parameters that allow the
adaptation of the system to different situations, although the
power supply can be simplified to reduce the weight and power
spent of the system, allowing the use of smaller batteries and
fewer components (for example, with only a 12 Vdc to 5 Vdc
converter and 9 W 12 V LED lamp).
For future steps, it is desired to carry out agricultural
analyzes, considering weed families, as well as the inclusion
of AI-based weed image process to identify plant species
for agricultural control. In addition, an expansion of system’s
connectivity with other devices will also be realized.
ACKNOWLEDGMENT
This work has been supported by Embrapa and Brazilian
research agency CAPES.
REFERENCES
[1] FAO, ”The future of food and agriculture – alternative pathways to 2050,”
Food and Agriculture Organization of the United Nations Rome, pp. 224,
2018.
[2] H. Abouziena and W. Haggag, ”Weed control in clean agriculture: a
review,” Planta daninha, SciELO Brasil, vol. 34, pp. 377-–392, 2016.
[3] S. C. Bhatla and M. A. Lal, ”Plant physiology, development and
metabolism,” Springer, 2018.
[4] B. M. Moreno and P. E. Cruvinel, ”Sensors-based stereo image system for
precision control of weed in the agricultural industry,” SENSORDEVICES
2018, The Ninth International Conference on Sensor Device Technologies
and Applications, pp. 69–76, 2018.
[5] B. M. Moreno and P. E. Cruvinel, ”Computer vision system for identify-
ing on farming weed species,” 2022 IEEE 16th International Conference
on Semantic Computing (ICSC), USA, pp. 287–292, 2022.
[6] M. I. Sadiq, S. M. P. Rahman, S. Kayes, A. H. Sumaita, and N. A.
Chisty, ”A review on the imaging approaches in agriculture with crop
and soil sensing methodologies,” 2021 Fifth International Conference On
Intelligent Computing in Data Sciences (ICDS), Morocco, pp. 1–7, 2021.
[7] M. Rajoriya and T. Usha, ”Pattern recognition in agricultural areas,”
Journal of Critical Reviews, vol. 7, pp. 1123–1127, 2020.
[8] J. W¨aldchen, M. Rzanny, M. Seeland, and P. M¨ader, ”Automated plant
species identification—trends and future directions,” PLoS computational
biology, vol. 14, pp. 1–19, 2018.
[9] M. P. Raj, P. R. Swaminarayan, J. R. Saini, and D. K. Parmar, ”Ap-
plications of pattern recognition algorithms in agriculture: a review,”
International Journal of Advanced Networking and Applications, vol. 6,
pp. 2495–2502, 2015.
[10] D. Mcloughlin, ”Image processing apparatus for analysis of vegetation
for weed control by identifying types of weeds,” EP1000540, May 17,
2000.
[11] J. Gao and Z. Jin, ”Bionic four-foot walking intelligent rotary tillage
weeding device, has weeding system installed on back side of machine
body, where gear in gear rotating mechanism transmits power to rotary
shaft that is provided with weeding cutter,” CN114794067, Jul 29, 2022.
[12] X. Jin, Y. Chen, and J. Yu, ”Precise weeding method for lawn and pasture
based on cloud-killing spectrum, involves receiving images uploaded
by each weeding robot, completing weed identification and outputting
spraying instructions, and collecting and organizing massive weed data
for big data applications,” CN113349188, Sep 7, 2021.
[13] S. E. Mathe, M. Bandaru, H. K. Kondaveeti, S. Vappangi, and G. S.
Rao, ”A survey of agriculture applications utilizing raspberry pi,” 2022
International Conference on Innovative Trends in Information Technology
(ICITIIT), Kottayam, India, pp. 1–7, 2022.
[14] C. Balamurugan and R. Satheesh, ”Development of raspberry pi and
IoT based monitoring and controlling devices for agriculture,” pp. 207–
215, 2017.
[15] K. A. Patil and N. R. Kale, ”A model for smart agriculture using IoT,”
2016 International Conference on Global Trends in Signal Processing,
Information Computing and Communication (ICGTSPICC), Jalgaon, India,
pp. 543–545, 2016.
6
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-083-4
ALLSENSORS 2023 : The Eighth International Conference on Advances in Sensors, Actuators, Metering and Sensing

[16] O. van Zwanenberg, S. Triantaphillidou, R. Jenkin, and A. Psarrou,
”Edge detection techniques for quantifying spatial imaging system perfor-
mance and image quality,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops, pp. 1871–1879,
2019.
[17] N. Kawagishi, R. Kakinuma, and H. Yamamoto, ”Aerial image reso-
lution measurement based on the slanted knife edge method,” in Optics
Express Vol. 28, pp. 35518–35527, 2020.
[18] A. A. Naumov, A. V. Gorevoy, A. S. Machikhin, V. I. Batshev, and V.
E. Pozha, ”Estimating the quality of stereoscopic endoscopic systems,” in
Journal of Physics: Conference Series Vol. 1421, pp. 012044, 2019.
[19] L. Yang, B. Wang, R. Zhang, H. Zhou, and R. Wang, ”Analysis
on location accuracy for the binocular stereo vision system,” in IEEE
Photonics Journal, vol. 10, no. 1, pp. 1–16, Art no. 7800316, Feb. 2018.
[20] Y. M. Wang, Y. Li, and J. B. Zheng, ”A camera calibration technique
based on OpenCV,” The 3rd International Conference on Information
Sciences and Interaction Sciences, Chengdu, China, pp. 403–406, 2010.
[21] J. Sun, X. Chen, Z. Gong, Z. Liu, and Y. Zhao, ”Accurate camera
calibration with distortion models using sphere images,” in Optics & Laser
Technology, Vol. 65, pp. 83–87, 2015.
[22] J. Salvi, X. Armangu´e, and J. Batlle, ”A comparative review of camera
calibrating methods with accuracy evaluation,” in Pattern Recognition,
Vol.35, Issue 7, pp. 1617–1635, 2002.
[23] S. Yang, Y. Gao, Z. Liu, and G. Zhang, ”A calibration method for
binocular stereo vision sensor with short-baseline based on 3D flexible
control field,” in Optics and Lasers in Engineering, Vol. 124, pp. 105817,
2020.
[24] Y. Wang, X. Wang, Z. Wan, and J. Zhang, ”A method for extrinsic
parameter calibration of rotating binocular stereo vision using a single
feature point,” in Sensors, Vol. 18, Art no. 3666, pp. 1–16, 2018.
7
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-083-4
ALLSENSORS 2023 : The Eighth International Conference on Advances in Sensors, Actuators, Metering and Sensing

