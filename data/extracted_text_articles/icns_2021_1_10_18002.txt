Comparison of Performance in Weed Detection with Aerial RGB and Thermal 
Images Gathered at Different Height 
 
Jose F. Marin1, David Mostaza-Colado2, Lorena Parra2, 3, Salima Yousfi2, Pedro V. Mauri2, and Jaime Lloret 3 
1 Area verde MG Projects SL. C/ Oña, 43 28933 Madrid, Spain 
2Instituto Madrileño de Investigación y Desarrollo Rural, Agrario y Alimentario (IMIDRA), Finca “El Encin”, A-2, Km 38, 
2, 28800 Alcalá de Henares, Madrid, Spain 
3 Instituto de Investigación para la Gestión Integrada de Zonas Costeras Universitat Politècnica de València, Valencia, Spain 
Email: jmarin@areaverde.es, david.mostaza@madrid.org, loparbo@doctor.upv.es, salima.yousfi@madrid.org, 
pedro.mauri@madrid.org, jlloret@dcom.upv.es 
 
Abstract— Weed detection is a crucial aspect of reducing the 
usage of phytosanitary products. Most studies about weed 
detection have been performed with linear crops; few studies 
can be found in crops with high soil coverage. In this paper, we 
have evaluated the effect of drone flying height on wild species 
detection. We have gathered images in a golf course from 4 to 
16 m above ground. A non-professional drone with a camera 
with 1.5 megapixels was used to gather the pictures. The images 
are composed of red, green, and blue bands. Images were 
gathered in three zones with a very high infestation, high 
infestation, and low infestation of Daucus carota. To evaluate the 
effect of flying height, we calculate the percentage of the affected 
area and compare the obtained rates for different height, 
assuming that the rate at 4 m is 100% of detection. To determine 
is a pixel represent the wild plant or the grass, a vegetation index 
is used. Our results indicate that the error in estimating the 
affected area is relatively low, from 8 to 10 m; in some cases, 
overestimation errors are detected. Nonetheless, the relative 
error beyond 12 m reaches up to 25% of relative error. In these 
cases, the error consists of an underestimation of the presence 
of a wild plant.  
Keywords-image processing; drone; turfgrass; green areas; 
vegetation indexes; wild plants. 
I. 
 INTRODUCTION 
The green areas are special agroecosystems characterized 
by the cultivation of a single species, grasses, to maintain a 
green cover in cities. Several authors pointed out the 
importance of green areas in cities and their potential benefits 
for air quality, quality of life of citizens, and enhancing social 
cohesion [1].  
One of the requirements for the green areas is having a 
homogenous coverage with a low or null incidence of wild 
plants. This is especially important in green areas intended for 
recreational purposes. To stop the proliferation of weed plants, 
periodic mowing and phytosanitary products can be applied. 
Nonetheless, considering the global efforts to reduce pesticide 
applications and the particular scenario for green areas, it is 
essential to minimize the application of phytosanitary.  
One of the best ways to reduce the amount of used product 
is to develop tools for early detection [2]. Thus, the infected 
area in which products must be applied is small, and a reduced 
amount of phytosanitary product is used. The use of image 
processing techniques acquired by different means has 
become essential in agriculture to identify the proliferation of 
pest, diseases and wild plants. Nevertheless, using these tools 
in green areas is reduced since most techniques cannot be 
directly applied due to the vegetation patterns. While in most 
cropping systems, the crops are schemed as lines, and the 
proliferation of green biomass outside that lines can be 
considered a wild plan, in green areas, there are no lines. 
Instead, the grass covers all the ground, and the wild plans 
might appear interspersed with the grasses.  
Most of the tools developed for agriculture are based on 
this linear scheme and the recognition through artificial 
intelligence of the wild plants. Nonetheless, this requires 
internet access, and the data cannot be processed in real-time 
in most cases. On the other hand, some indexes have been 
developed based on combining the Red-Green-Blue (RGB) 
components of an image. Initially, those are the most used 
components since not all drones incorporate thermal images. 
Moreover, in several cases, the optimal combination of RGB 
can be used to differentiate between the crop and the wild 
plants. Although those indexes are not as accurate as artificial 
intelligence, they can be applied in real-time and in scenarios 
without internet access. Moreover, this methodology can be 
used with the appropriate adaptations for other similar crops 
such as cereals or pastures.  
In recent papers, we have established different indexes, 
based on bands combinations, to determine the presence or 
absence of wild plants in a green area [3]. Another option is to 
use the edge detection technique to identify the wild plants [4]. 
Nevertheless, both techniques have been applied only with 
images gathered at low height and good spatial resolution. As 
far as we know, no tests have been done to determine the 
maximum flying height possible for gathering data with 
commercial drones. Even that the professional drones might 
have cameras with better spatial resolution, fixing an average 
limitation for commonly-used drones with regular cameras is 
needed. Once we identify the maximum flying height, it will 
be possible to estimate the required time to monitor an area. 
In this paper, we analyze the possibility of detecting wild 
plan in a green area using images gathered with Bebop 2 Pro 
drone at a flying height of 4 m to 16 m. We use pictures 
gathered in an area with the presence of Daucus carota L., a 
wild plant. The band combination is the technique used to 
determine the weed plant presence. First, we will evaluate the 
incidence of the wild plant as the percentage of pixels defined 
as a wild plant in a rectangular area. Then, we will compare 
the incidence in three areas, with diverse levels of incidence, 
at different flying heights to determine the maximum height 
that can be used for monitoring purposes.  
1
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-853-2
ICNS 2021 : The Seventeenth International Conference on Networking and Services

The rest of the paper is structured as follows; Section 2 
outlines the related work. The materials and methods are 
described in Section 3. Section 4 analyzes the results, 
highlighting the implications of the flying height. Finally, 
conclusions and future work are summarized in conclusion.  
II. 
RELATED WORK 
In this section, we are going to summarize the existing 
efforts for detecting wild species and the different options for 
grass monitoring with drones.  
First of all, we differentiate the options for remote sensing. 
According to Huang et al. [5], we can divide the remote 
sensing into four systems attending to the distance from the 
ground and the used equipment. Images gathered at least 5 m 
height are usually collected by ground-based systems and 
considered proximal remote sensing. Images gathered with 
drones are collected between 10 and 200 m. Each method has 
each own characteristics, restrictions and limitations. In our 
paper, we are using a drone, but according to the flying height, 
we are in-between both systems described by Huang et al.  
In [6], Hassanein and El-Sheimy used an Inspire 1 drone 
from DJI with an X3 RGB camera to determine the presence 
of wild species in crops (canola and bean). Authors shave 
selected as flying height 20, 40, 80, and 120 m. Their 
methodology consists of segmenting the image and generate 
vegetation indexes for each segment. Then, they apply a 
threshold to determine if in this grid there are wild plants or 
not. Their methodology offered good results at high height (80 
and 120 m). Nonetheless, the authors do not provide the used 
threshold or equations for detecting the wild plants in their 
paper. The employed camera has a higher resolution than our 
camera (they used a professional drone). Moreover, they do 
not indicate the size of the wild plant sports detected. In this 
case, their methodology can be applied in a large area where 
a high proliferation of wild plants is expected.  
Barrero and Perdomo proposed image fusion for 
gramineous detection in rice fields in [7]. They combine two 
cameras for obtaining images and fuse the data. The first 
camera was an RGB camera with 12.1 megapixels; the second 
one was a multispectral camera with 1.2 megapixels. Their 
methodology, fuse data using neural networks, was efficient 
with images gathered at 60 and 70 m. Although authors have 
proven their methodology at a higher height than us, they have 
better cameras and professional drones. In our case, the drone 
and camera stability are not as accurate as in their case since 
their drone has a gimbal. In addition, our camera has 1.5 
megapixels for RGB image.  
The effect of pixel size, which directly relates to flying 
height, over the accuracy of wild species detection was studied 
by Tamouridou et al. in [8]. Authors used machine learning to 
determine the presence or absence of wild plants in an 
abandoned field, previously used for cereals cropping. First, 
the authors applied the Maximum Likelihood classifier. Their 
images had a pixel size of 0.1 m. Next, they study the effect 
of reducing the pixel size on accuracy. Their results indicate 
that similar accuracy is found for the pixel size of 0.1 to 1.5 
m. Still, a substantial reduction in the accuracy is found for 2 
m (the largest evaluated pixel size).  
Another study, presented by Zou et al. [9], shows the 
accuracy of wild species detection in crops. The authors used 
a DJI, MAVIC 2. They have an image of the pixel size of 5 
mm obtained with a flying height of 20m. The authors 
obtained good accuracies by combining different artificial 
intelligence techniques. In our case, the acquired images have 
a pixel size of 3mm (at a flying height of 4 m) to 8.5 mm (at a 
flying height of 16 m).  
As far as we know, no paper has evaluated the effect of 
incrementing the flying height using non-professional drones 
in green areas. Most studies are developed for linear crops 
using professional drones, heavier gimbal, and higher spatial 
resolution.  
III. 
MATERIALS AND METHODS 
In this section, we are going to detail the procedure 
followed for gathering and processing the data, as well as the 
employed hardware and software to obtain the results.  
A. Data gathering process 
In order to test the effect of flying height on the 
quantification of infection by wild plants, images of grass 
were gathered in a golf course. The photos were taken in one 
of the green areas, which was suffering from an infection of 
Daucus carota L., one of the common wild plants. The green 
was composed of Agrostis stolonifera L. T1, mowed 
periodically at 3.8mm. A picture of the data gathering process 
can be seen in Figure 1 (a).  
 
 
 
 
 
(a) 
(b) 
(c) 
(d) 
Figure 1.  Images of the data gathering process, (a) picture taken during the data gathering, (b) to (d) data gathered for each studied area. 
2
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-853-2
ICNS 2021 : The Seventeenth International Conference on Networking and Services

Along the same green, three study areas were delimited 
according to different incidence levels of the wild plant. To 
delimit the areas, four golf balls were used to define the 4 
vertexes of a polygon. The three areas were described as 
“Very High Incidence”, see Figure 1 (b); “High Incidence”, 
see Figure 1 (c); and “Low Incidence” see Figure 1 (d).  
The area delimited by the four balls has an average surface 
of 3x7 m. Images were gathered at 5 different flying heights, 
from 4 m to 16m height. In Figure 1 (b) to 1 (d), we show the 
data gathered at 4 m height. All the images were collected on 
the same morning and with the same meteorological 
conditions. The data gathering process had a duration of less 
than 1 h 
B. Drone and image characteristics 
To gather the data, we have selected a commercial drone 
with typical characteristics. The chosen drone is the Parrot 
Bebop 2 Pro [10]. It has a flying autonomy of 20 min and a 
weight of 504 g. In addition, it has a peak velocity of 60 km/h 
and four helixes.  
The drone has two different cameras. One of them has a 
fixed position, and the second one can be moved. For these 
tests, we need to use the zenith pictures to use the second 
camera. It is essential to consider that for this drone, and this 
is the camera with lower resolution. The gathered images have 
1080x1440 pixels and 24-bit colour.  
C. Image processing 
To process the image, specific software (generally used for 
remote sensing) is used. We have selected ArcGIS 10.5 for its 
versatility. Once the photos are included in the ArcMap, from 
ArcGIS software [11], they are converted into 8-bit colour 
images to simplify the operations. 
First of all, a vegetation index based on RGB data can 
differentiate pixels with grass from pixes with the wild plant. 
The vegetation index is a linear combination of the two or 
more bands of the picture, which produce different values 
according to the presence or absence of wild plant; more 
information can be found in [3]. In past papers [3], we have 
developed vegetation indexes. Nonetheless, those indexes 
were used for mixed lawns composed of two or more grass 
species. In this case, as we have a single grass species, we will 
try to simplify the existing indexes. After calculating the 
vegetation index, the threshold to differentiate the type of 
cover is determined. Then, the image is reclassified [12], the 
pixels without wild plant are classified as 0, and the pixels 
which values indicate the presence of wild plant are classified 
as 1.  
Following, for each image, we generate a vectorial layer 
formed by a polygon that delimits the studied area. As the 
flying height increases, the size of the polygon (in pixels) 
decreases. To define the polygon, the inner extreme of each 
one of the balls is used.  
Once the image is classified as “Infected Pixel=0” and “No 
Infected Pixel=1”, the tool Zonal Statistic as a Table [13] is 
used to obtain the summary of data in each studied area. The 
most critical data are the summation, the mean, and the 
standard deviation from the statistics. The statistics summary 
of each polygon is exported to Excel. Finally, in Excel, some 
other parameters are calculated. The % of affection is 
calculated using the total number of pixels in the studied area 
and the number of pixels with values = 1 (obtained through 
the summation).  
We consider that the data gathered at 4 m is the most 
accurate one. Thus, we consider that this data has no error in 
their results. This is 100% of detection. The percentages of 
detection obtained at other height are compared to identify 
maximum height that offers accurate values.  
IV. 
RESULTS 
In this section, we are going to analyze the obtained data. 
First, we will see if previously used indexes might be used for 
this case or if indexes can be simplified in order to accelerate 
the calculations. Secondly, we compare the results in terms of 
accuracy at different heights.  
A. Obtention of vegetation index 
We have considered the two indexes presented in [3]. 
Among the proposed indexes, the first is the one that detects 
the wild plant. A modification of this index is proposed for 
this paper. The new index is defined in (1). The soil removal 
coefficient is removed from the index, and the mathematic 
operations have been modified.  
 
Vegetation Index= B1+B2-B3                (1) 
 
where B1 is the red band, B2 is the green band, and B3 is 
the blue band.  
Theoretically, the index can have values from -256 to 512. 
Nonetheless, the found values go from -29 to 260. After 
analyzing the vegetation index values of the 15 evaluated 
images, we have established the threshold as 190. While 
values higher than 190 are defined as will plant, the values 
lower equal or lower than 189 are defined as grass. This is a 
significant improvement concerning the previous indexes, in 
which the definition of wild plant or grass is based on natural 
breaks (or Jenks), and no fixed value can be used.  
In Figure 2, we can see the results of applying the index to 
the images gathered at 4 m in all the studied areas. We can see 
for each studied area the RGB picture, the results after 
applying the index, and the classified raster once the threshold 
is applied. The polygon that defines the studied area is visible 
for index images to facilitate the interpretation of results.  
We have compared the results of applying this index with 
the index defined in [3], and the results are similar. Thus, we 
keep with the index of (1).  
B. Comparison of incidence of wild species at different 
height 
This subsection will analyze the differences in the 
obtained incidence of images gathered at a different height. 
First of all, we include a short summary of the statistical 
information obtained for each image in Table II. The Zones 
are identified by the Id: 1 = Very High Infestation, 2 = High 
Infestation, and 1 = Low infestation. The height indicates the 
flying height at which the picture is obtained. The area and 
sum are the numbers of pixels of the studied area and the 
number of pixels classified as a wild plant. Those are the 
3
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-853-2
ICNS 2021 : The Seventeenth International Conference on Networking and Services

values used to calculate the % of the affected area. Finally, the 
Mean and the Standard Deviation (STD) indicate the 
variability of data in the studied areas. If there is no effect of 
flying height on a wild plant mean and STD detection, each 
zone should be similar. Nonetheless, we can see that the 
values decrease as the height increase. Therefore, we can 
affirm that there is an effect of flying height over the wild 
species detection.  
In order to identify the maximum flying height, we are 
going to consider not only the mean and the STD but also the 
error in the estimated affected area. For that purpose, we are 
going to compare the estimated affected area with each one of 
the images. Figure 2 shows the calculated affected area 
according to the number of pixels of the studied area and the 
number of pixels classified as a wild plant. We can see that the 
zones with Very High Infestation are the ones with a higher % 
of the affected area (3.66% to 2.04% from 4 to 16m). The zone 
with High Infestation has an affected area of 0.39 % to 0.12 
%, and the Zone with Low Infestation from 0.05% to 0%.  
TABLE I.  
REPRESENTATION OF RGB PICTURE, THE INDEX AND THE CLASSIFIED RESULT 
RGB Picture 
 
 
 
Index 
 
 
 
Index with threshold 
     Infected pixels = Red 
     Non-Infected pixels = Black 
 
 
 
 
 
4
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-853-2
ICNS 2021 : The Seventeenth International Conference on Networking and Services

TABLE II.  
SUMMARY OF OBTAINED INFORMATION OF EACH AREA 
Zone 
(Id) 
Height 
(m) 
Area 
Wild plant 
Mean 
STD 
(number of pixels) 
(pixel value) 
1 
4 
326956 
11953 
0.036558 
0.187675 
1 
8 
178129 
6585 
0.036968 
0.188682 
1 
10 
124936 
4135 
0.033097 
0.17889 
1 
12 
47070 
1251 
0.026577 
0.160845 
1 
16 
29356 
601 
0.020473 
0.141611 
2 
4 
245417 
958 
0.003904 
0.062356 
2 
8 
127101 
740 
0.005822 
0.076081 
2 
10 
67318 
339 
0.005036 
0.070784 
2 
12 
35319 
101 
0.00286 
0.053399 
2 
16 
19955 
25 
0.001253 
0.035373 
3 
4 
266248 
137 
0.000515 
0.022678 
3 
8 
147082 
31 
0.000211 
0.014516 
3 
10 
89412 
9 
0.000101 
0.010032 
3 
12 
35570 
5 
0.000141 
0.011855 
3 
16 
20203 
0 
0 
0 
Comparing the percentages at different heights, the data 
gathered at 4 and 8 m offer almost identical data in Zone 1, 
while the calculated areas at 10m are slightly lower. For Zones 
1 and 2, the percentage increase in some cases at 8 and 10 m 
due to the increment of size pixel and the reduced number of 
pixels that contain wild species. This is a common effect when 
the flying height increase and the number of interesting pixels 
(pixels classified as a wild plant) are low.  This effect was not 
found in Zone 3. 
We consider the Absolute Error (AE) in our estimations, 
assuming that the results of the image obtained at 4 m 
represents 100% of the affected area. Thus, we calculate the 
AE of each image having the % of affected areas at 4 m as a 
reference. The more evident results can be seen for Zone 1 
(Very High Infestation); in this case, the AE is almost null for 
8 m (-0.04%). The AE reaches 0.34, 0.99, and 1.6 1 % the 10, 
12, and 16 m. For the other areas, the errors are much lower 
since the affected areas are smaller.  
In Relative Error, see Figure 3, the errors can be compared 
among areas. Those errors with positive value indicate an 
overestimation of the infestation; meanwhile, errors with 
negative value indicate an underestimation. As detailed 
before, For Zones 1 and 2, there are some points, with low 
height, with an overestimation of the incidence. Nonetheless, 
after 12 m, in all the cases, there is an underestimation. Note 
that the error at 12 m is almost the same for Very High 
Infestation (-27.3%) and High Infestation (-26.7%). It is 
important to remark that with Low Incidence, there is no 
overestimation.  
Considering the negative effect of underestimation in the 
proliferation of wild species and the delay of phytosanitary 
products application, we consider that 10 m should be the 
established threshold for monitoring with this sort of drones.  
C. Limitations of proposed technic and implications of 
established threshold  
In this section, we are going to analyze the limitations of 
our study and the implications of the established threshold for 
the flying height.  
First of all, there is a bias in our tests given the used drone. 
Each camera has a different resolution parameter, which 
affects the resolution of the obtained pictures. Moreover, the 
fact of having the image in 8-bit to simplify the analyses affect 
the accuracy of our results. Nonetheless, these biases can be 
assumed since the specialized drones with cameras with high 
resolution (in terms of pixels) still have a high cost, and their 
weight can be a limiting factor according to the national or 
regional regulations. Moreover, cameras with higher 
resolution are used with professional drones, with a higher 
cost (almost 10 times more), which the enterprises cannot 
readily assume. The 8-bit constriction has an effect on the 
accuracy of the proposed vegetation index, but this is 
necessary if we want to keep the image processing in real-time 
or nearly real-time. 
It is essential to consider that this threshold, established for 
cameras with similar resolution than the used one, might be 
helpful in different wild species. Nonetheless, the threshold 
must be re-evaluated to detect grass diseases such as Fusarium 
or Dollar spot. The main reason is that the affected areas by 
the diseases are smaller than with wild species.  
Finally, we are going to analyze the impact over the flying 
time of modifying the threshold between 8, 10 and 12 m. The 
maximum covered areas with these flying thresholds are 
80x60 m for 4m high, 88x65m for 10 m high, and 96x70m for 
12 m high. The variation in the flying height increases a 20% 
and 40% (for 10 and 12 m, respectively). Considering the time 
required to fly over an area of 60x80 m, the drone will need 
20 min at 8 m, 17.5 min at 10 m, and 15 min at 12 m. This 
increment in the required time to cover a small area must be 
scaled when an entire golf course or green areas in cities must 
be evaluated. Thus, we are going to indicate the required time 
to cover different interesting and well-known areas for the 
different flying height (8 to 12 m). For example, in a golf 
course type Regulation Course with turfgrass areas from 
350,000 m2, the required flying time will be 24.30, 21.26, and 
18.23hours. For golf course type Par 35 or 36 with a Standard 
combination (average turfgrass surface of 120,000 m2, the 
required flying time will be 8.33, 7.29, and 6.25 hours. For 
other sports such as polo, which fields have an average surface 
of 40,134 m2, the time required is 2.7, 2.4, and 2 hours. For 
soccer or rugby fields, an average surface of 10,800 m2, the 
time of flying needed will be 45, 39.37, 33.75 minutes. In the 
case of the hockey field, an average of 5,027 m2, the flying 
time will be 21, 18.3, 15.7 minutes. Finally, for a tennis court, 
an average extension of 195 m2, less than 1 minute is required 
regardless of the flying height. 
 
 
Figure 2.  Comparison of estimated affected areas at each height 
0
1
2
3
4
0
4
8
12
16
20
Affected area (%)
Flying Height (m)
Very High Infestation
High Infestation
Low Infestation
5
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-853-2
ICNS 2021 : The Seventeenth International Conference on Networking and Services

 
Figure 3.  Relative Error of estimated affected area 
The other advantage of our proposed method is that 
controlling the infected areas with pictures will reduce the 
possible overestimation by the workers and reduce the 
consumed time. Currently, workers, particularly in green 
areas with high requirements, have to walk through the area 
periodically, searching for the apparition of diseases or wild 
plants.  
V. 
CONCLUSION AND FUTURE WORK 
In this paper, we have studied the effect of flying height 
on detecting the infestation of grass with a wild plant (Daucus 
carota). This is especially relevant in the management of 
green areas since a fast detection will save phytosanitary 
products, reducing the maintenance cost and increasing its 
sustainability.  
We need to find a balance between spatial resolution (low 
flying height had high resolutions) and flying time vs. covered 
area (high flying height had better relation) in selecting the 
flying height. We have evaluated the error in determining the 
percentage of infestation in a green area for different flying 
heights. Our results indicate that 10 m should be the maximum 
height given the characteristics of our camera.  
In future work, we will include thermal images to evaluate 
if having a combination of four bands allows having a better 
index with which we can have a higher threshold for the flying 
height. Regarding the index, we will work on its 
standardization for different undesired species in order to 
simplify the process in the future. In addition, we will compare 
the flying height for images captures with professional drones. 
If possible, we will compare the recommended threshold for 
wild species with a threshold for disease.  
ACKNOWLEDGMENT 
This research was funded by AREA VERDE-MG projects, 
by project PDR18-XEROCESPED, funded under the PDR-
CM 2014-2020 by the EU (European Agricultural Fund for 
Rural Development, EAFRD), Spanish Ministry of 
Agriculture, Fisheries and Food (MAPA) and Comunidad de 
Madrid regional government through IMIDRA, by European 
Union through the ERANETMED (Euromediterranean 
Cooperation through ERANET joint activities and beyond) 
project ERANETMED3-227 SMARTWATIR, and by 
Conselleria de Educación, Cultura y Deporte with the 
Subvenciones para la contratación de personal investigador 
en fase postdoctoral, grant number APOSTD/2019/04. 
REFERENCES 
[1] D. D’Alessandro, et al., “Green areas and public health: 
improving wellbeing and physical activity in the urban 
context”. Epidemiol Prev, 2015, 39(4), pp. 8-13. 
[2] L. Parra, et al., “Edge detection for weed recognition in lawns. 
Computers and Electronics in Agriculture”, 2020, 176, 
pp.105684. 
[3] L. Parra, V. Torices, J. Marín, P.V. Mauri, and J. Lloret, “The 
Use of Image Processing Techniques for Detection of Weed in 
Lawns”. In Proccedings of the Fourteenth International 
Conference on Systems (ICONS 2019), Valencia, Spain, 2019, 
pp. 24-28. 
[4] L. Parra, et al., “Comparison of Single Image Processing 
Techniques and Their Combination for Detection of Weed in 
Lawns”. International Journal On Advances in Intelligent 
Systems, 12 (3 and 4), 2019, pp. 177-190. 
[5] Y. Huang, K. N. Reddy, R. S. Fletcher, and D. Pennington, 
“UAV low-altitude remote sensing for precision weed 
management”. Weed technology, 32(1), 2018, pp. 2-6. 
[6] M. Hassanein and N. El-Sheimy, “An Efficient Weed 
Detection Procedure Using Low-Cost UAV Imagery System 
for Precision Agriculture Applications”. International Archives 
of the Photogrammetry, Remote Sensing & Spatial Information 
Sciences, 2018, pp. 1-7. 
[7] O. Barrero and S. A. Perdomo, “RGB and multispectral UAV 
image fusion for Gramineae weed detection in rice fields”. 
Precision Agriculture, 19(5), 2018, pp.809-822. 
[8] A. A. Tamouridou, et al., “Evaluation of UAV imagery for 
mapping Silybum marianum weed patches”. International 
journal of remote sensing, 38(8-10), 2017, pp. 2246-2259. 
[9] K. Zou, X. Chen, F. Zhang, H. Zhou, and C. Zhang, “A Field 
Weed Density Evaluation Method Based on UAV Imaging and 
Modified U-Net”. Remote Sensing, 13(2), 2021, pp. 310. 
[10] Specifications of Prarrot BeBop 2 Pro. Available at: 
https://s.eet.eu/icmedia/mmo_35935326_1491991400_5839_
16054.pdf. Last access in 16/04/2021. 
[11] Webpage 
of 
ArcGIS 
software. 
Available 
at: 
https://desktop.arcgis.com/en/. Last access in 16/04/2021 
[12] Description 
of 
Reclassify 
Tool. 
Available 
at: 
https://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-
analyst-toolbox/reclassify.htm. Last access in: 16/04/2021 
[13] Description of Zonal Statistic as Table Tool. Available at: 
https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-
analyst-toolbox/zonal-statistics-as-table.htm. Last access in: 
16/04/2021
 
-150
-100
-50
0
50
100
4
8
12
16
20
Relative Error (%)
Flying Height (m)
Very High Infestation
High Infestation
Low Infestation
Undestimation Errors
Overestimation Errors
6
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-853-2
ICNS 2021 : The Seventeenth International Conference on Networking and Services

