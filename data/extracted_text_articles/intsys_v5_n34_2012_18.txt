451
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Energy and Carbon Aware Scheduling in Supercomputing
Mikko Majanen, Olli Mämmelä
Autonomic Networking Team
VTT Technical Research Centre of Finland
Kaitoväylä 1, Oulu, Finland
Email: mikko.majanen@vtt.ﬁ, olli.mammela@vtt.ﬁ
André Giesler
Jülich Supercomputing Centre
Forschungszentrum Jülich
52425 Jülich, Germany
Email: a.giesler@fz-juelich.de
Abstract—At an early stage of information and com-
munications technology and high-performance com-
puting, performance and reliability were two impor-
tant factors in research and development. Energy
consumption was not considered as a serious topic,
since the technical characteristics of hardware and
software were limited and the amount of computing
nodes in a computing cluster, i.e., a data centre
was small. Gradually the situation has evolved a lot:
nowadays there are multiple data centres located
in geographically diverse locations and the software
has become more complex. Modern data centres are
equipped with a large amount of computing nodes
having vast computing power. Consequently, energy
consumption has become a major topic. This work
presents two algorithms for optimizing energy and
emissions in high-performance grid computing, in
which multiple data centres are interconnected to
each other. The algorithms are validated both in
simulation and testbed environments. The eﬀect of
various parameters to energy and emission savings
are studied and the performance of the algorithms
is compared to commonly used default algorithms.
Our simulation and testbed experiments show that
the developed algorithms are able to reduce energy
consumption and emissions drastically without signif-
icant increase in job turnaround or wait time.
Keywords-HPC; grid computing; energy; emissions;
testbed.
I. Introduction
The increased demand for IT applications and services
has encouraged the building of data centres worldwide.
However, data centres consume an enormous amount
of energy at an increasing ﬁnancial and environmental
cost. This has led to research eﬀorts in both industry
and academia to cut down data centre energy usage
and emissions. This work is a continuation to our prior
work in ENERGY 2012 [1] to save energy and reduce the
CO2 emissions in federated High-Performance Computing
(HPC) data centres. Previously, we have also researched
the energy saving potential inside single site data centres
[2].
In 2006, U.S. servers and data centres consumed around
61 billion kilowatt hours (kWh) at a cost of about 4.5
billion U.S. Dollars [3]. This is equal to about 1.5% of
the total U.S. electricity consumption or the output of
about 15 typical power plants. High energy consumption
naturally causes huge environment pollution. It has
been estimated that Information and Communications
Technology (ICT), as a whole, covers 2% of world’s CO2
emissions [4] and this amount looks set to grow at 6%
each year until 2020 [5]. Data centres were 14% of the
total ICT footprint in 2002 and 2007, and it is estimated
that the amount will rise to 18% in 2020.
In HPC, the ever-growing demand for higher perfor-
mance seems to increase the total power consumption,
even though more ﬂops per watt are achieved. In order to
provide even greater computing capabilities, HPC data
centres can be interconnected to each other to form larger,
federated or HPC grid data centres. The connection
is implemented by using special grid software (e.g.,
UNICORE (UNiform Interface to COmputing REsources)
[6]) that manages the job submissions to all data centres
belonging to the grid.
The energy consumption between the data centres may
vary radically due to the diﬀerent characteristics of the
centres. For example, the server hardware in each centre
may be diﬀerent and consume diﬀerent amount(s) of
energy. The centres may also locate geographically far
from each other and the surrounding climate can cause
large diﬀerences in the needed cooling, i.e., the Power
Usage Eﬀectiveness (PUE) [7] values between diﬀerent
centres may vary due to the surrounding climate. Also,
since the energy sources can diﬀer between the centres,
the CO2 emissions of the data centres may vary radically
depending on the available energy sources. The diﬀerences
between the data centres naturally enable optimizations
regarding energy consumption and CO2 emissions.
In our prior work [1] we introduced two algorithms for
selecting the data centre inside the grid in energy- and
CO2-aware manner. The performance of the algorithms
was studied by simulations and the results showed signif-
icant savings in energy consumption and CO2 emissions.
This work extends our previous work by a feasibility
study [8] of the algorithms in a testbed environment
that consists of three clusters: two located in Germany
and one in Finland. The testbed results conﬁrm the

452
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
possibilities for energy and emission savings achieved in
the simulations, and can be used as a basis for the design
of speciﬁc federated cluster environments when using
a developed software plug-in to enable an energy-aware
scheduling of the resources. Furthermore, we also consider
energy and CO2 emission savings inside single HPC
data centres by using energy-aware scheduling algorithms
presented in [2].
The rest of the paper is organized as follows: Section II
describes the related work. Section III introduces the
cluster selection algorithms. Section IV describes the sim-
ulation model and scenario, and presents the simulation
results. Testbed experiments are presented in Section V,
including the scenario and results. Conclusion and future
work are presented in Section VI.
II. Related work
As described in [2], several methods for saving energy
in single HPC data centres have been studied. The
methods include mainly the use of energy-eﬃcient or
energy proportional hardware (e.g., embedded low-power
chips), Dynamic Voltage and Frequency Scaling (DVFS)
techniques, shutting down idle hardware components
at low system utilizations, power capping, and thermal
management. Recently, there has also been approaches
to solve HPC energy issues in Graphics Processing Unit
(GPU) computing [9], [10], [11]. GPU computing aims at
combining the use of a GPU with a CPU to accelerate
general-purpose scientiﬁc and engineering applications.
The compute-intensive portions of the application are
oﬄoaded to the GPU, while the remainder of the code
still runs on the CPU. However, the energy consumption
is a major concern in these systems.
In our prior work [2], we used an energy-aware job
scheduler to schedule the jobs inside single data centres
and shut down idle computing nodes whenever possible.
We also noted that merely the choice of a diﬀerent
scheduling algorithm can aﬀect the energy consumption
of a data centre. Out of commercial HPC schedulers,
Moab oﬀers a Green Computing plug-in [12] that tries
to reduce power consumption and costs in a data centre
in a quite similar way as our energy-aware job scheduler.
In the Moab plug-in, it is possible to turn oﬀ idle nodes
that do not have reservations on them, and turn on
additional nodes when jobs require them. Moab uses a
MAXGREENSTANDBYPOOLSIZE parameter, where
users can specify a "green pool", which is the number of
nodes that are kept on and ready to run jobs (even if
some nodes are idle). Idle nodes that exceed the number
speciﬁed with the MAXGREENSTANDBYPOOLSIZE
parameter are turned oﬀ. The requirements for the Green
Computing plug-in are a license for green computing,
Moab 5.3.5 or later, a script that Moab can call to
programatically turn nodes on and oﬀ, and a resource
manager that can monitor and report power state. In
a test run, the Green Computing plug-in was able to
decrease the energy consumption by 8.2% with the
penalty of 7.5% increased workload completion time [13].
The savings with Moab Green Computing depend highly
on the workload.
In this paper we extend our scope from single HPC
data centres to HPC grid data centres and introduce two
algorithms for selecting the data centre inside the grid in
energy- and CO2-aware manner. Moreover, we provide
HPC grid simulation and testbed results, and new single
HPC data centre results.
Until recently, there has not been much previous
research that addresses the energy eﬃciency or CO2
emissions of the grids from the whole grid perspective;
mainly only optimizations inside a single data centre have
been studied. Perhaps the most similar approach to our
approach is the Heterogeneity Aware Meta-scheduling
Algorithm (HAMA) [14]. HAMA ﬁrst selects the most
energy-eﬃcient cluster for the job based on the power
consumption of the servers and the eﬃciency of the
cooling system. Additionally, when running the job,
DVFS is used to reduce the power consumption of the
CPU. The simulation results show that HAMA can
reduce up to 23% energy consumption in the worst case
and up to 50% in the best case as compared to other
algorithms (EDF-FQ, which prioritizes jobs based on a
deadline and submits jobs to resource sites in earliest
start time (FQ) manner with the smallest waiting time).
Without DVFS, HAMA can still result in power savings
of up to 21%.
Lynar et al. [15] have explored the eﬀect on energy
consumption by using diﬀerent resource allocation mech-
anisms, both in a cluster and in a grid. The results show
that diﬀerent resource allocation methods can result in
a signiﬁcantly diﬀerent energy usage while computing
a stream of tasks. The Pre-processed Batch Auction
(PPBA) and batch auctions almost always result in a
signiﬁcantly lower energy use than a random resource
allocation. By using a simple batch auction allocation
method, energy consumption can be reduced by up to
37.5%, and possibly even more by using the PPBA
method.
Patel et al. [16] have presented an energy-aware policy
for distributing computational workload in the Grid
resource management architecture. They introduce a data
centre energy coeﬃcient that is taken into account as
a policy when making allocation decisions for compute
workloads. This coeﬃcient is determined by the thermal
properties of each data centre’s cooling infrastructure
including regional and seasonal variations. The estimated
energy savings in case of three data centres located in two
diﬀerent time zones were large enough to give suﬃcient
reason for the economic viability of the approach.

453
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Shah and Krishnan [17] also analyze the climatic
conditions as a means to reducing cooling energy costs.
They show that dynamic optimization of the thermal
workloads based on local weather patterns can reduce
the environmental burden by up to 30% in their case
study. Additionally, the data centre operational costs
can be potentially reduced by nearly 35%. Due to the
variability of fuel mixes encountered in a global grid, they
also found that the use of pure energy consumption as
a metric for environmental sustainability — a common
practice in the ICT literature — can be erroneous.
The GREEN-NET framework [18] consists of an
ON/OFF model, which includes prediction heuristics
and green advice for the users and takes the decision
to switch on or oﬀ the nodes, and an adapted energy
eﬃcient Resource Management System (RMS) at the
grid level.
There has also been research on the feasibility of
powering data centres more by renewable energy ( [19],
[20], [21]) and studying the environmental potential of
Geographical Load Balancing (GLB) [22], in which pro-
cesses are shifted to data centres located in regions where
energy currently has low cost. In [23], a renewable and
cooling aware workload management plan is introduced.
The availability of renewable energy and IT demand is
predicted and IT resources are allocated according to
a time varying power supply and cooling eﬃciency. A
similar approach is taken in GreenSlot [24], which is a
parallel batch job scheduler powered by solar power and
the electrical grid (as a backup). It predicts the amount
of solar energy that will be available in the near future
and schedules the workload to maximize the use of green
energy without breaking any Service Level Agreement
(SLA).
III. Optimization in the HPC Grid
The optimization algorithm in the HPC grid focuses
on optimizing the scheduling process in the UNICORE
middleware [6]. The scheduling process is triggered by
submitting a job from the UNICORE Commandline
Client, or from the UNICORE Rich Client to the
UNICORE Workﬂow Engine. The UNICORE Workﬂow
Engine queries a UNICORE Service Orchestrator (USO),
on which cluster the job should be submitted. As a
default, the USO uses round-robin algorithm for choosing
the cluster. After cluster decision, the job is submitted
to the RMS of the chosen cluster. The RMS takes care
of executing the job according to the used scheduling
algorithm, e.g., FIFO (First In, First Out) or backﬁlling.
In this work, we focus on reducing the energy consump-
tion and the CO2 emissions. The CO2/energy related
optimizations should not aﬀect the current SLA or
Quality of Service (QoS) agreements, or alternatively,
a new green SLA [25] could be used. In HPC, there are
User
RMS
UNICORE service orchestrator
Server
Server
RMS
RMS
Cluster1
Cluster2
Cluster3
Submit job
Grid optimization algorithm
Get best cluster
Return best cluster
Submit job 
(to best 
cluster)
Server
Server
Server
Server
Energy-aware scheduler
Figure 1.
Job submission in a federated HPC data centre
no clear SLAs between users and data centres, but a
reasonable turnaround time can be seen as sort of a QoS
agreement. A possible green SLA for HPC data centres
could mean that the users allow certain delay for the
execution of their job. As a bonus, they will get some
extra computing time for free.
For decreasing CO2 emissions and/or energy consump-
tion in federated HPC data centres, the optimization
algorithm will be used for performing the cluster selection
in CO2/energy-aware manner. In addition to cluster selec-
tion algorithms, also energy-aware single site scheduling
algorithms will be used. As depicted in Figure 1, the
USO in UNICORE receives job requests coming from the
users. The jobs include the requirements for the needed
resources (e.g., number of nodes/cores, memory, etc.). If
the user wants to use the green SLA, it is also included
in the job requirements. The grid optimization algorithm
is used to select the most suitable cluster for the job
and the job is subsequently submitted to the RMS of
the selected cluster. The RMS uses energy-aware job
scheduling algorithms to schedule the job and power oﬀ
idle servers. The energy-aware job scheduling algorithms
for single site data centres were deﬁned in our previous
work [2]. The main principle of the single site algorithms
is to keep the system active with the lowest amount
of resources as possible. If a node is not needed for job
execution, it is shut down or placed into an energy saving
mode. Once the node is needed for the job execution
again, it is woken up.
A. Carbon Usage Eﬀectiveness
Carbon Usage Eﬀectiveness (CUE) is a sustainability
metric developed by the Green Grid organization [26].
The main purpose of the metric is to address carbon
emissions associated with data centres. The CUE can be
calculated as follows:

454
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Table I
Energy emission coefficient factors
Generation type
Conversion factor
(kgCO2 per kWh)
Closed cycle gas turbine
0.360
Coal
0.910
Electricity, France interconnector
0.083
Electricity, Ireland interconnector
0.699
Non pumped storage hydro
0.0
Nuclear
0.0161
Open cycle gas turbine
0.479
Oil
0.610
Pump storage
0.0
Other
0.610
CUE = SiteEmissions
ICTEnergy
,
(1)
where ICTEnergy is the energy consumpted by the ICT
equipment in the data centre. An alternative approach
for calculating the CUE is to multiply the Energy Source
Coeﬃcient (ESC) by the data centre’s PUE:
CUE = ESC ∗ PUE,
(2)
where PUE is a metric for deﬁning how eﬃciently the
power in the data centre is used, i.e., how much power
is actually used by the ICT equipment and how much
power is used for cooling and other equipment. ESC is
deﬁned as follows:
ESC =
X
ESP ∗ EEC,
(3)
where Energy Source Percent (ESP) indicates the percent-
age of the energy generation source, and Energy Emission
Coeﬃcient (EEC) indicates how many kilograms of CO2
are emitted per 1 kWh of energy. Example values of the
EEC can be found in Table I [27]. By using the formulas
described earlier and the values in Table I, we are able to
estimate how much emissions are caused by data centres
with diﬀerent energy sources:
SiteEmissions
=
CUE ∗ ICTEnergy
(4)
=
PUE ∗ ESC ∗ ICTEnergy.(5)
B. Algorithm description
This subsection describes the functionalities of the
default round-robin cluster selection algorithm, as well as
the two developed algorithms for optimizations: Fastest
possible (FP) that tries to minimize the wait time, and
Energy and CO2-aware (ECA) that tries to minimize the
energy or CO2 emissions.
1) Round-robin (RR): RR algorithm is generally used
in USO for selecting the cluster. It balances the number
of jobs between diﬀerent clusters by always choosing the
next cluster compared to the previous selection. After
the last cluster, the selection is started again from the
ﬁrst cluster.
2) Fastest possible (FP): FP cluster selection algo-
rithm tries to select the cluster that could possibly
execute the job with minimal wait time. For this, the
algorithm ﬁrst checks if there are enough idle nodes/cores
in some cluster for executing the job. If yes and the
cluster’s queue is also empty, the job is submitted to that
cluster. If not, an estimated wait time for the job in each
cluster is calculated by using the current status of each
cluster: number of nodes and cores, status of running
jobs, number of jobs in the queue, and walltimes of each
queued job. The cluster with the shortest estimated wait
time is then selected.
The algorithm relies on the dynamic cluster properties
(status of nodes and queues), which can be obtained by
a single site monitoring system. Otherwise, this dynamic
information is not available for the USO, so the normal
cluster selection algorithms can exploit only static cluster
information for the decision making.
It should be noted that the wait time can only be
estimated. The walltimes of the jobs are given by the users
and, in general, they are inaccurate [28], [29]. Also, the
used scheduling algorithm aﬀects in which order the jobs
are executed (especially backﬁlling). Thus, it is possible
to calculate only the maximum wait times for the jobs,
not the exact wait times.
3) Energy and CO2-aware (ECA): This algorithm tries
to ﬁnd the cluster with the smallest amount of estimated
energy consumption or CO2 emissions; the optimization
goal can be chosen by the user. The CO2 emissions of the
job can be calculated in the same way as for the whole
site in Equation (4):
JobEmissions = CUE ∗ ICTEnergyOfTheJob.
(6)
The simplest way is to select the cluster with the smallest
CUE value. This works if the clusters have signiﬁcant
diﬀerences in their CUE values (CUE = ESC * PUE).
If there are only small diﬀerences in the CUE values,
then additional estimations should be done, since the job
may consume diﬀerent amount of ICT energy in diﬀerent
clusters due to the diﬀerent computing node properties
(CPU, RAM, etc.), and this diﬀerence may become a
greater factor than CUE for the CO2 emissions. The
ICT energy of the job can be estimated by using the
job requirements (number of nodes/cores, walltime) and
cluster’s computing node properties (CPU, RAM, etc.)
as inputs for power consumption models such as those
described in [2] and [30]. Moreover, the job execution
time may diﬀer greatly between clusters because of

455
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
varying hardware parameters. Thus, it is very important
to try to estimate the execution time of the job on
diﬀerent clusters. Special application benchmarks for
estimating the clusters’ job execution times were chosen
for this purpose and they were used in the testbed
experiments. These benchmark applications are run
once on each cluster beforehand, and the execution
times are measured. Based on the measurements, the
data centre operators rank each cluster and benchmark
application combination. The higher the rank, the faster
the execution time is.
Thus, the ICT energy of the job is given by
ICTEnergy = ICTPower ∗ ExecutionTime,
(7)
where ICTPower is the power consumption of the job
and is calculated by using the power consumption models,
and ExecutionTime is calculated by using the walltime
estimate and the rank of the cluster.
However, selecting always the cluster with the least
amount of estimated CO2 emissions would cause huge
load and queue on the cluster with the least CO2
emissions. This would mean large delay for the users.
Thus, some form of load balancing is needed for this
algorithm. In the conducted simulations (described in
the next sections), we used a queue size limit: If the
queue exceeded its size limit, the job was submitted to
the cluster with the second least CO2 emissions, and
so on. In the case of green SLA, the users set a certain
deadline for the completion of their job. This limit can be
used for load balancing: The estimated completion time
for the job can be calculated as a sum of the estimated
wait time and walltime of the job. If this is in the limits,
the cluster can be chosen. If not, the same calculations
should be made to the cluster with the second least CO2
emissions, and so on. If the user sets too strict time
limit for the job that none of the clusters can fulﬁll,
the job should be either denied or the cluster should be
chosen by the Fastest possible algorithm. In the testbed
experiments (described later) we used this green SLA
method for load balancing, and FP algorithm in case of
too strict time limits. The ECA algorithm can be used for
selecting the cluster with minimal energy consumption,
too, by replacing CUE by PUE.
The ECA algorithm takes into account the dynamic
properties of the cluster and compute nodes. This in-
formation is stored in a meta-model, which is updated
accordingly if any of the parameters, such as PUE, CUE
or compute node hardware parameters are changed.
IV. Simulation studies
The simulation model for the HPC grid has been
developed with the OMNeT++ discrete event network
simulator [31] and the INET Framework [32]. The design
of the model is similar as in [2], except that the model
USO
Data Centre 2
Data Centre 1
Data Centre 3
Figure 2.
Network topology
is extended from a single site scenario to a federated site
scenario.
Figure 2 illustrates the network topology used in the
simulations. It consists of three backbone routers, three
gateway routers, three data centre modules, three clients
and a USO module. In this scenario, the clients send
HPC job requests to the USO, which is responsible for
choosing an appropriate data centre, i.e., an HPC cluster,
for executing the job. The USO has been adapted for the
simulation so that it is capable of using the developed
optimization algorithms and making decisions based on
the dynamic properties of the clusters. Normally, only
static information of the clusters is available for the USO.
For the decision making, the USO can query the status
and properties of each cluster from the corresponding
RMS. Once the cluster is chosen, the USO forwards
the job request to the RMS of the chosen cluster. The
RMS uses the policies and scheduling algorithms of the
cluster to choose suitable servers for job execution. When
the job execution ﬁnishes, the RMS informs the USO,
which again forwards the information to the client that
submitted the job for execution.
The data centre module can be seen in Figure 3, which
is similar as in the single site scenario used in our previous
work [2]. It contains a RMS, a ﬁxed number of servers and
a router between them. The RMS handles all incoming
job requests arriving to the data centre and allocates the
jobs to the servers for execution according to the selected
policies and algorithms. Thus, the RMS also functions
as a scheduler in the simulation. The RMS supports 6
diﬀerent scheduling algorithms: standard FIFO, Backﬁll
First Fit and Backﬁll Best Fit algorithms and their
energy-aware counterparts developed previously (see [2]
for more information on these).

456
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Server
Server
Server
Server
RMS
Figure 3.
Data centre module
The RMS module includes parameters for the PUE
and the CUE. By using these two values, the USO is
able to select a cluster that is the most energy-eﬃcient
or produces the least amount of CO2 emissions.
A. Simulation scenario
In this subsection, we describe the simulation scenario
and parameters. For evaluation we consider a scenario
that includes three data centres and 75 clients that are
sending job requests to the USO. The simulation is
stopped once 1500 jobs have been completed. During
the simulation we measure the energy consumed by each
data centre and present the obtained results in the next
section. General simulation parameters are presented in
Table II. Uniform(a,b) means randomly selected value
according to a uniform distribution between a and b. The
scheduling and cluster selection algorithms are shortened
as follows:
• FP = Fastest possible USO cluster selection algo-
rithm
• ECA = (Energy and) CO2-aware USO cluster selec-
tion algorithm set to minimize the CO2 emissions
• RR = Round-robin USO cluster selection algorithm
• FIFO = First In, First Out job scheduling algorithm
• BFF = Backﬁlling ﬁrst ﬁt job scheduling algorithm
• BBF = Backﬁlling best ﬁt job scheduling algorithm
• E-FIFO, E-BFF, E-BBF = energy-aware counter-
parts for the job scheduling algorithms (idle nodes
are powered oﬀ whenever possible)
In Table III, we can see the parameters for the three
clusters in the considered federated HPC data centre.
The clusters have diﬀerent characteristics, such as, the
number of servers, PUE, and ESC. The energy sources
(O = Oil, C = Coal, H = Hydro, N = Nuclear) for
the clusters were selected so that both extreme ends
in terms of ESC were represented in the simulations,
while the third one represents something in the middle
of them. Also, servers have diﬀerent operating systems
Table II
Simulation parameters
Parameter
Value
Simulation runs
10
Number of jobs
1500
Number of data centres
3
Number of clients
75
Number of gateway routers
3
Number of backbone routers
3
USO cluster selection algorithm
RR, FP, ECA
RMS scheduling algorithm
FIFO, BFF, BBF,
E-FIFO, E-BFF, E-BBF
Server memory
4 * 2 GB = 8 GB
Server cores per CPU
2
Server CPUs
2
Server CPU idle power
15 W
Server core voltage
1.2 V
Client job cores
1, 2, 4
Client job load
Uniform(30,99)
Client job nodes
Uniform(1,20)
Client job memory
Uniform(100MB, 2GB)
Client job run time
Uniform(600s, 86400s)
Table III
Data centre parameters
Parameter
Cluster 1
Cluster 2
Cluster 3
Servers
30
40
50
Energy source
C 50% H 20%
C 80%
O 20% H 40%
N 30%
O 20%
N 40%
PUE
1.5
1.8
1.3
ESC
0.45983
0.85
0.12844
CUE
0.689745
1.53
0.166792
OS
Linux
Windows
Linux
CPU arch.
AMD
Intel
Intel
(OS) and processor architectures. In the simulations, the
ECA algorithm optimization goal was set to minimize
the CO2 emissions.
B. Simulation Results
Figure 4 presents the total ICT energy consumption
of the three clusters for diﬀerent USO cluster selection
and job scheduling algorithms. As can be seen, RR
with normal job scheduling algorithms consumes the
most amount of energy. RR with normal job scheduling
algorithms represents a generally used, un-optimized
algorithm combination in federated HPC data centres.
Thus, it serves as a comparison point when calculating
the energy savings and CO2 emission reductions.
Figure 5 presents the energy savings achieved by using
Fastest possible and CO2-aware USO cluster selection
algorithms instead of the default RR algorithm, and by
using energy-aware job schedulers on each cluster. The
energy-aware job schedulers are compared to their normal
counterparts; for example, the ﬁrst bar (E-FIFO FP)
means the savings compared to FIFO RR. The last three
bars present the savings when using RR but with energy-
aware job scheduling. It can be seen that by using energy-

457
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
0
5E+10
1E+11
1,5E+11
2E+11
2,5E+11
ICT Energy (J)
Total ICT energy consumption
Figure 4.
Total ICT energy consumption. Black lines represent
the average value and the ﬂoating bars show the range of values
from minimum to maximum
0
5
10
15
20
25
30
35
40
45
Energy savings (%)
Algorithm
ICT energy savings compared to RR with FIFO, BFF, BBF
E-FIFO FP
E-BFF FP
E-BBF FP
E-FIFO ECA
E-BFF ECA
E-BBF ECA
E-FIFO RR
E-BFF RR
E-BBF RR
Figure 5.
ICT energy savings compared to un-optimized, generally
used RR with FIFO, BFF, and BBF
aware job scheduling, 22% to 35% energy savings can be
achieved. Together with FP and ECA cluster selection,
the savings are about 25% to 38%. When comparing the
energy-aware algorithms to their normal counterparts,
the savings with E-FIFO are the largest. This is because
backﬁlling exploits the idle nodes more eﬃciently by
running shorter jobs while with standard FIFO the nodes
that cannot be used for job execution are left in an idle
state and can thus be shut down by the energy-aware
scheduler.
However, if we only change the cluster selection
algorithm, and keep the normal job scheduling algorithms,
we can see from the Figure 6 that with FP we can save
17% to 30 %. Since the cluster selection is performed
before job scheduling, we can say that about 8 % of the
total savings are due to the energy-aware job scheduling,
while the rest is due to the FP cluster selection. When
comparing to RR with energy-aware job scheduling (as
depicted in Figure 7), we can see that FP and ECA
cluster selection algorithms can save additionally about
3% to 5%. For the explanation, we have to take a look
at the jobs’ average wait and turnaround times and the
simulation duration.
Figure 8 presents the average wait times of the jobs
(i.e., the average waiting times of the jobs in the queue) in
case of diﬀerent USO cluster selection and job scheduling
0
5
10
15
20
25
30
35
Energy savings (%)
Algorithm
ICT energy savings compared to RR with FIFO, BFF, BBF
FIFO FP
BFF FP
BBF FP
FIFO ECA
BFF ECA
BBF ECA
Figure 6.
ICT energy savings compared to RR with normal job
scheduling
0
1
2
3
4
5
6
Energy savings (%)
Algorithm
ICT energy savings compared to RR with E-FIFO, E-BFF, E-BBF
E-FIFO FP
E-BFF FP
E-BBF FP
E-FIFO ECA
E-BFF ECA
E-BBF ECA
Figure 7.
ICT energy savings compared to RR with energy-aware
job scheduling
algorithms. As can be seen, the average wait time is
clearly shorter with the FP USO algorithm. The ECA
USO algorithm with backﬁlling has about the same
average wait time as RR, even though RR with FIFO
clearly has the longest waiting time. Also, there are
basically no diﬀerences between RR with energy-aware
and normal job scheduling. This is true also in general,
as reported in [2]: energy-aware job scheduling does not
cause signiﬁcant increase in wait time.
0
50000
100000
150000
200000
250000
300000
350000
Wait time (s)
Average wait time
Figure 8.
Average job wait times

458
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
0
1000000
2000000
3000000
4000000
5000000
6000000
7000000
8000000
9000000
Duration (s)
Simulation duration
Figure 9.
Average simulation duration
0
50000
100000
150000
200000
250000
300000
350000
400000
Turnaround time (s)
Average turnaround time
Figure 10.
Average job turnaround time
Figure 9 depicts the simulation duration, i.e., how long
time it took to execute all the 1500 submitted jobs. The
graph shows the same as Figure 8: because the wait
times are longer with RR USO cluster selection, also the
simulation duration is longer.
Figure 10 presents the average job turnaround times
in case of diﬀerent scheduling algorithms. The story is
the same as in previous ﬁgures: RR is slower due to the
longer wait time.
Based on the results above, we can conclude that RR
cluster selection with normal job scheduling algorithms
can be very ineﬃcient in terms of energy. This is because
RR only balances the number of jobs among the clusters.
It does not take into account the diﬀerences in the
clusters (e.g., number of nodes/cores) or the diﬀerences
in the submitted job characteristics (e.g., number of
nodes/cores, walltime estimate). This can lead to a
situation where one cluster is over utilized with many jobs
waiting in the queue, while the other clusters can be under
utilized at the same time, with nodes running idle. The
energy-aware job schedulers (E-FIFO, E-BFF, E-BBF)
power oﬀ the idle nodes whenever possible, and this is why
a substantial amount of energy can be saved. On the other
hand, the FP cluster selection algorithm inherently takes
into account the diﬀerences in the clusters and submitted
jobs: it always selects the cluster with the estimated
minimal wait time, and thus balances the utilization
between the clusters. Then fewer nodes are running idle
0
5000
10000
15000
20000
25000
30000
35000
40000
45000
50000
CO2 emissions (kg)
Total CO2 emissions
Figure 11.
Total CO2 emissions
0
5
10
15
20
25
30
35
40
45
50
CO2 savings (%)
Algorithm
CO2 savings compared to RR with FIFO, BFF, BBF
E-FIFO FP
E-BFF FP
E-BBF FP
E-FIFO ECA
E-BFF ECA
E-BBF ECA
E-FIFO RR
E-BFF RR
E-BBF RR
FIFO FP
BFF FP
BBF FP
FIFO ECA
BFF ECA
BBF ECA
Figure 12.
CO2 savings compared to RR with FIFO, BFF, and
BBF
and energy is saved. Also ECA saves some energy even
though its goal was set to minimize the CO2 emissions.
Figure 11 presents the total CO2 emissions of the
federated HPC data centre. As can be seen, RR with
normal job scheduling causes the largest CO2 emissions.
Using energy-aware job scheduling reduces the emissions
due to the reduced energy consumption. Using FP cluster
selection reduces the energy consumption still a bit more
due to the better load balancing among clusters, and
thus the CO2 emissions are also smaller. ECA cluster
selection algorithm favours the cluster with the best
CUE value, i.e., least amount of CO2 emissions, and
hence achieves the greatest savings in CO2 emissions,
about 37% to 45% compared to RR with normal job
scheduling. Note that this requires also using the energy-
aware job scheduler; without energy-aware job scheduling
the emission reductions are smaller since ECA prefers
Cluster 1 over Cluster 2 due to the smaller CUE, which
in turn results in worse utilization in the bigger Cluster 2.
Energy-aware job scheduling turns oﬀ the idle nodes on
Cluster 2 and hence cuts the emissions. The CO2 savings
are depicted in Figure 12 as percentages.

459
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
V. Testbed experiments
The energy-aware job scheduling and cluster selection
algorithms were also implemented as part of the software
plug-in developed within the project FIT4Green [33]. The
developed plug-in [34] is a set of software components
that add energy management capabilities to a data
centre by interfacing with the existing management
and automation tools of the data centre. Its goal is to
dynamically optimize the deployment of the applications
and services hosted and running across the ICT resources
of a single or federated site data centre, in order to
minimize the energy consumption or CO2 emissions,
that is, trying to consolidate load so as some hardware
resources can be turned oﬀ or set to low-power state. One
core software component of the plug-in is the Optimizer
that contains the cluster selection and job scheduling
algorithms and uses them together with the information
on data centre resources for suggesting a list of actions
that can potentially lead to reduced energy consumption.
In particular, the plug-in communicates with the RMS
and UNICORE at the HPC data centre environment by
acquiring information about the data centre status: the
jobs in the queue, status of the nodes and the jobs that
are currently running. All this information is stored and
kept up-to-date in an XML-based meta-model. Based on
this information, the plug-in can send actions to the RMS
and UNICORE, such as, start job or shutdown node in
the single site scenario. In the federated scenario, the
plug-in decides to which cluster the job will be submitted
to.
The plug-in can be used also in other types of data
centres, i.e., in traditional service or enterprise portals,
or cloud computing data centres. However, these are out
of scope of this work. An interested reader can ﬁnd more
detailed information about the plug-in in [34]. The plug-
in’s source code is available at [35] as open source licensed
under the Apache License, Version 2.0.
A. Testbed scenario
This subsection describes the testbed scenario: the
clusters used in the tests, their conﬁguration and charac-
teristics, how the tests were conducted and what kind of
test workload was used.
1) Environment and conﬁguration: The testbed sce-
nario consists of three HPC clusters: Juggle, Juﬁt and
Dune. Juggle and Juﬁt are located at the Jülich Super-
computing Centre in Jülich, Germany, while Dune is
set up at the VTT Technical Research Centre in Oulu,
Finland. By having three testbed clusters located at
diﬀerent sites and countries, it is possible to analyse the
impact of diﬀerent CUE and PUE parameters of real
distributed systems. In Juggle and Juﬁt it is possible to
set the compute nodes to a low-power standby mode,
while in Dune it is only possible to shut down nodes.
Table IV
Operating numbers of the Juggle cluster
Parameter
Value
Processor type
Dual AMD Opteron
F2216 2.4GHz
Number of nodes
1 head node, 12 compute nodes
Cores per node
4
Overall number of cores
48
Main memory
8 GB per node
Network
InﬁniPath(QLOGIC),
Gigabit Ethernet
2 ﬁle servers
disk capacity: 6 TB
Power supply eﬃciency
83%
Operating system
SLES 10, Scientiﬁc Linux 5.2
RMS
Torque (PBS Scheduler)
Node power consumption
- standby
117W
- idle
162.5W
- maximum
230W
Juggle involves altogether 12 compute nodes (see
Table IV). This allows executing jobs requiring many
resources in parallel. The Juggle is a relatively old system
compared to the other clusters in the testbed. Intelligent
Platform Management Interface (IPMI) services and
monitoring tools if not already provided by the operating
system have been installed on each node of the system
to enable the monitoring of dynamic system parameters,
such as core voltage and frequency, memory load, fan
RPM, and disk read/write rates.
Juﬁt is a more modern system providing a more modern
generation of processors. It consists of two compute nodes
with 12 cores each (see Table V). Compared to Juggle,
Juﬁt is in general more energy eﬃcient in terms of CPU
power consumption and power supply eﬃciency. The
Juﬁt cluster has been used in the measurements for the
federated scenario.
The test environment at VTT consists of a Linux
cluster framework called Dune that includes four compute
nodes and a head node. All nodes are rackable Dell
PowerEdge R510 servers with equal characteristics as
can be seen in Table VI. There is no separate ﬁle server
available in the cluster, but instead all the nodes hold
adequate hard disk drives, with 1 TB of disk space.
The Torque RMS is installed on all clusters for
managing nodes and the scheduling and monitoring
of jobs. Furthermore, Target System Interface (TSI)
modules of the UNICORE middleware are installed on
the head nodes of the clusters to allow submitting jobs by
UNICORE, which is needed in the case for the federated
scenario.
All clusters are connected to Power Distribution Units
(PDUs), which measure the power consumption of each
single head and compute node. The results are requested
conveniently by clients through the Simple Network
Management Protocol (SNMP). The measurements were

460
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Table V
Operating numbers of the Jufit cluster
Parameter
Value
Processor type
Quad-core Intel Xeon
X5660 (Westmere), 2.6 GHz
Number of nodes
1 head node, 2 compute nodes
Cores per node
12
Overall number of cores
24
Main memory
24 GB per node
Network
InﬁniPath(QLOGIC),
Gigabit Ethernet
2 ﬁle servers
disk capacity: 6 TB
Power supply eﬃciency
91%
Operating system
OpenSuSE 11.3
RMS
Torque (Maui Scheduler)
Node power consumption
- standby
142W
- idle
175W
- maximum
232W
Table VI
Operating numbers of the Dune cluster
Parameter
Value
Processor type
2 x Quad-core Intel Xeon E5606
(Westmere), 2.13 GHz, 64-bit
Number of nodes
1 head node, 4 compute nodes
Cores per node
8
Overall number of cores
32
Main memory
4 x 16 GB (aggregate 64 GB)
Network
Gigabit Ethernet
Power supply eﬃciency
90%
Operating system
64-bit Rocks cluster distribution
(based on CentOS 5.6 Linux)
RMS
Torque (Maui Scheduler)
Disk space
1 TB SATA HDD on each node
Node power consumption
- oﬀ
0-2W
- idle
85-90W
- maximum
165-175W
updated every 3 seconds during the tests, so that the
values could be provided reliably.
The motherboards of the Juggle and Juﬁt compute
nodes support the ACPI [36] state S1, which is also
known as ‘standby’ state. As soon as the software plug-in
is generating a standby action, the appropriate compute
node will be set to standby mode. While on Juggle ‘wake-
on-lan’ is used to bring the machine back from standby
to normal state, the same result on Juﬁt is achieved by
an IPMI wake up command. The VTT testbed cluster
Dune is using instead the ACPI state S5 or better known
as ‘soft-oﬀ’, which requires a full reboot of the system.
Additionally, a DVFS feature has been enabled on
Juggle to analyse the impact of using DVFS instead of
ACPI energy saving mechanisms.
2) Testing methodology: The goal of the test mea-
surements was to compare the energy consumption of
the plug-in adapted supercomputing environments with
systems using default state-of-the-art solutions. The tests
tried to analyse the energy saving capabilities of the plug-
in software in two diﬀerent areas:
• Savings on a single cluster by using the energy-aware
job scheduler of the plug-in
• Savings in a federated cluster scenario by using the
cluster selection algorithms of the plug-in
For each of these investigation areas speciﬁc test ap-
proaches were carried out regarding the used benchmark
workloads, type of job submission, and energy metering.
All measurements depended on the available hard- and
software. The workloads stressing the testing environment
consisted of jobs that made use of the installed HPC
applications. The jobs were either submitted directly from
the test user’s home directory on the head node of the
cluster or alternatively from the UNICORE client, so that
the user did not need to be logged in to the cluster. Both
submission types are quite common in supercomputing
scenarios.
The single site scenario tests were performed on the
Juggle system, which provides a suitable number of
nodes for testing the energy saving potential by using
the plug-in’s energy-aware job scheduler. Firstly, this
scheduling mechanism schedules jobs in the queue of
the cluster to the particular nodes and cores of the
cluster, and, secondly, it sets nodes to standby if they are
completely idle. Alternatively, DVFS instead of ACPI
standby can be used to save energy. When having equal
workloads and comparing the energy-aware scheduler
with a default state-of-the-art scheduler, the best possible
energy consumption depends on how eﬃciently the
jobs can be scheduled without loss of time, so that as
many nodes as possible can be set to standby. In the
supercomputing testbed at FZJ the energy measurements
were analysed by comparing the default PBS scheduler
with the plug-in’s energy-aware scheduler. While the PBS
scheduler is based on an enhanced FIFO (ﬁrst in, ﬁrst
out) algorithm, the plug-in one used the backﬁll ﬁrst ﬁt
approach. The particular measurements were performed
with workloads generating diﬀerent system utilization
proﬁles on the Juggle cluster (0%, 40%, 60%, and 80%) to
simulate potential loads of real supercomputing machines.
The total energy consumption generated by a single
test workload has been calculated in Joule as a product
of the measured average power of all cluster nodes and
the elapsed time that was needed to run all jobs of
the workload. The elapsed time involves the total time
elapsed from the submission of the test user’s ﬁrst job
until the output ﬁles of the last executed job have been
stored where requested. So, this period includes the
time for transferring input ﬁles, the wait time in the
RMS queue, the actual execution time, and the time to
stage the output ﬁles to the requested locations. Each
measurement was stopped immediately after all jobs

461
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 13.
Diﬀerence of job submission in single and federated
benchmark tests
of the test workload were ﬁnished. This approach was
mandatory to measure the time that diﬀerent scheduling
strategies need to process the workload. So, the energy
of one measurement was calculated as follows:
ESingleSite = TW orkload ∗ PCluster,
(8)
where ESingleSite is the total energy of the site, TW orkload
is the elapsed time to process the workload, and PCluster
is the average power consumption in the cluster during
the workload processing.
Single site scenario tests were also performed on the
Dune cluster following the same testing methodology.
The only diﬀerences were that Dune supports only soft-
oﬀ instead of standby and DVFS, and the comparison
point was Maui scheduler instead of PBS scheduler.
When performing tests in the single site scenario, the
jobs of the workload were submitted locally from the
cluster’s head node by using provided shell commands
of the RMS on the dedicated cluster. This approach
is a common practice in supercomputing environments
when the dedicated target system of the job is already
known. In the federated scenario another job submission
approach was utilised. Since it is not known in advance
on which cluster the job should be executed, the test user
submits the jobs at ﬁrst to a UNICORE server, which
acts as a centralized entry point for all incoming jobs.
This UNICORE instance is connected to the installed
UNICORE TSIs on the testbed clusters, so that the
server is able to submit incoming jobs to the RMS of
an appropriate machine. Before that, the USO, service
of the UNICORE server, initiates a resource allocation
request to ask the plug-in for a suitable target machine
for the job. Figure 13 highlights the diﬀerence in terms of
job submission between the single and federated scenario.
In general, the workloads have been compared in each
test case by using the plug-in’s scheduling strategies as
well as default mechanisms for getting results of how
eﬃciently the plug-in is able to save energy. In the case
when the plug-in was not used, the existing state-of-the-
art mechanisms have been used to process the jobs on
the test clusters.
The approach of the energy consumption measurement
in the federated scenario is to consider only the elapsed
time, which is needed on each testbed cluster to run
the assigned jobs of the benchmark workload. This
incorporates that each involved cluster can produce in
one benchmark measurement a diﬀerent elapsed time
and diﬀerent average power consumption. So, the total
energy consumption EF ederated in one measurement is
the product of the elapsed time and the average power
of each cluster i:
EF ederated =
N
X
i=1
TClusteri ∗ PClusteri.
(9)
3) Test workload: The benchmark measurements on
the testbed were aimed at stressing the testbed as close as
possible to clusters in real supercomputing environments.
For that purpose diﬀerent typical HPC applications were
installed on the test clusters, namely LINPACK [37],
a collection of FORTRAN subroutines to solve linear
systems, and PEPC (Pretty Eﬃcient Parallel Coulomb
Solver) [38], which is used to run astrophysical N-body
simulations.
For the single site scenario the test workloads were cre-
ated by a conﬁgurable Perl script, which can parameterise
the jobs in terms of the used HPC application, the level
of computation intensity, the number of used nodes and
cores, as well as the planned walltime (also known as wall
clock time), which is the time elapsed until a job should
have been ﬁnished. In this way workloads were created
stressing the system with diﬀerent system utilization in
order to analyse the energy savings under those diﬀerent
loads. Also real world clusters working in production show
often varying system utilizations between entirely idle
and working to capacity. The utilization factor is deﬁned
here as the percentage of time when cluster resources are
stressed with jobs relative to the total elapsed time of the
workload. For instance, a system load of 90% means that
only on an average of 10% of the elapsed time cluster
nodes are able to be set to the energy saving standby
mode because they are otherwise busy with running jobs.
In case of the federated scenario the workloads of
the single site scenario were adapted as a template to
create workloads using the same HPC applications within
the graphical UNICORE Rich Client (URC) [39]. This
workload is embedded in a workﬂow from where the single
jobs are submitted in parallel to the UNICORE server,
which in turn initiates resource allocation requests to
the plug-in’s cluster selection algorithms, and forwards

462
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Table VII
Dune - Numerical results of single site measurements
System utilization [%]
0%
50%
80%
90%
Energy with
11.8
133.2
433.4
480.5
plug-in [kJ]
Elapsed Time [s]
517.8
543.2
1276.9
1276.7
Average Power [W]
22.8
250.8
339.6
376.6
Energy without
176.9
254.5
499.1
515.9
plug-in [kJ]
Elapsed Time [s]
515.7
648.0
1258.3
1268.0
Average Power [W]
343.4
393.0
396.8
406.9
Energy saving [%]
93.3
47.7
13.2
6.9
subsequently the jobs to the chosen cluster.
Roughly 90% of the workload jobs can run on both
clusters, while the requirements of always 10% of the jobs
can only be met on one of the clusters. This distribution
creates a base load on the clusters to map better real
world environments where jobs are usually not able to
run on every available target machine.
B. Testbed results
This subsection presents the results of the testbed
experiments for both single and federated site scenarios.
1) Single site scenario: In our previous work [2], we
showed the potential energy savings in a testbed that
considered the ACPI standby mechanism on the compute
nodes. In this work, we have performed additional single
site measurements on the new VTT testbed cluster
Dune, which is using ACPI S5 soft-oﬀ power shut down
to save energy on unused nodes, while on Juggle and
Juﬁt only ACPI standby is available. ACPI standby is
faster than soft-oﬀ in switching to the power-on state
but, on the other hand, does not have the same power
saving potential. The results can be found in Table VII.
Each workload measurement was repeated with several
iterations.
From the results it is possible to note that the energy
savings are highly dependent on the test workload and
the utilization of the whole system. The single site
optimization algorithm attempts to keep the system
active with the lowest possible amount of resources and
still providing suitable turnaround times for the jobs.
With lower utilization values the energy savings are
higher, but if the system is very busy there are not many
opportunities to shut down idle resources.
The single site scenario tests at FZJ were performed
on the Juggle system, which provides a suitable number
of nodes for testing the energy saving potential by
setting nodes to standby status. However, apart from
the diﬀerent ACPI mechanisms it was worth to evaluate
the energy saving potential of using the DVFS feature,
which works on the principle of setting unused nodes to
the powersave governor, i.e., the lowest CPU frequency
and core voltage is set on the node. The performance
governor is set again by the plug-in once the nodes are
requested again by jobs. That setting implies that the
maximum available frequency is set on the CPUs of the
nodes. On-demand governors were consequently not used
in the conﬁguration, since it is in general not desirable
to use frequency scaling when running CPU intensive
jobs. Administrators experienced performance drawbacks
with on-demand governors, which is a critical issue in
HPC environments. The software plug-in on Juggle has
thus been enabled to make use of diﬀerent energy saving
methods:
• ACPI power saving statuses (e.g., standby or soft-
oﬀ)
• DVFS (switching between powersave and perfor-
mance governor)
• ACPI standby + DVFS (observed slightly higher
savings on the testbed cluster when using both
mechanisms in parallel).
In general, the energy measurements have been anal-
ysed by comparing the PBS default scheduler with
the the plug-in’s energy aware job scheduler. These
measurements were performed with workloads generating
a diﬀerent total load on the cluster (0%, 40%, 60%, and
80%). Each workload measurement was repeated with
several iterations. The results in Table VIII show the
average values of those tests.
The energy consumption of a single workload has been
calculated in Joule as a product of the measured average
power of all cluster nodes and the elapsed time, which
was needed by the appropriate workload. The elapsed
times of each workload depend on the composition of the
workload to achieve certain system utilization, so there is
no correlation between the elapsed time values of diﬀerent
system loads. In contrast, the average power consumption
increases with more intensive workloads, since less idle
nodes can be set to an energy saving status.
When comparing the values of default scheduler
measurements with energy-aware scheduler tests it is
apparent that the elapsed time is most time slightly
higher with the energy-aware strategies, which is caused
by the overhead of the optimization process. In particular,
cluster information such as node and job statuses must
be read and analysed at the remote plug-in server, and
generated actions must be sent back to the RMS of the
cluster. However, this process has been optimized in
terms of performance during the implementation phases,
so that the time impact has been minimized. Overall,
the expected overhead in the single site scenario is round
about 2-3% compared to default mechanisms.
Taken into account similar elapsed time results for

463
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Table VIII
Juggle - Numerical results of single site measurements
System utilization [%]
0%
40%
60%
80%
Energy with
1300
2652
3018
4410
standby [kJ]
Elapsed Time [s]
1000
1634
1580
2143
Average Power [W]
1300
1623
1910
2058
Energy with
1400
2701
3033
4433
DVFS enabled [kJ]
Elapsed Time [s]
1000
1556
1558
2126
Average Power [W]
1400
1736
1947
2085
Energy with
1280
2616
3003
4389
standby+DVFS [kJ]
Elapsed Time [s]
1000
1620
1579
2144
Average Power [W]
1280
1615
1902
2047
Energy with
1960
3304
3345
4625
default scheduler [kJ]
Elapsed Time [s]
1000
1554
1552
2088
Average Power [W]
1960
2126
2155
2215
Energy saving with
33.7
19.7
9.8
4.6
standby [%]
Energy saving with
28.6
18.2
9.3
4.2
DVFS [%]
Energy saving with
34.7
20.8
10.2
5.1
standby + DVFS [%]
the single site measurements, the main factor for saving
energy is clearly the measured average power of the tested
cluster, which is proportional in the measurements to the
decreasing system load. While the default RMS scheduler
cannot make advantage of idle compute nodes, the plug-
in sets them in an energy saving standby mode, which
reduces noticeably the average power of the system.
When using only DVFS we could observe energy saving
between 28.6% and 4.2% compared to the default sched-
uler depending on the generated system utilization. When
enabling DVFS and ACPI standby together we could even
achieve between 34.7% and 5.1%, which conﬁrmed the
assumption that DVFS + standby generates a slightly
lower consumption than ACPI standby alone (between
33.7% and 4.6%). However, this behaviour could only
be detected on the used testbed hardware. It cannot be
stated as a general rule. Nevertheless, administrators
can check their systems if it makes sense to use both
mechanisms in parallel. On Juggle an additional gain of
about 1% saving is possible when using both features.
Using DVFS alone could be a beneﬁt when high job
ﬂuctuations can be expected on a system. Especially,
when the hardware is supporting only a full shut down
ACPI status, which would need 2-3 minutes for powering
the system on again, and a high job submission rate is
supposed, it could be more eﬃcient to use the supported
DFVS feature. That mechanism needs only one second
to switch between the governors.
2) Federated scenario: In the supercomputing feder-
ated scenario we wanted to measure the emission and
energy saving capabilities of the plug-in’s cluster selection
strategies. Jobs should be assigned to suitable cluster
resources in the most energy eﬃcient way. The plug-in’s
Optimizer implements two diﬀerent strategies to achieve
that resource allocation, namely the ECA and FP cluster
selection algorithms.
Section III gives already a detailed description of both
strategies. In short, the FP algorithm calculates the
wait time of a job on all potential suitable clusters and
submits the job to the system that provides the most
minimal estimated queue time. In contrast, the ECA
algorithm estimates at ﬁrst the energy or respectively
the emission (depending on the chosen objective), which
would be produced by a job on a particular cluster.
The energy/emission is calculated by considering the
PUEs/CUEs of the clusters as well as estimating the ICT
energy consumption of particular. In the supercomputing
testbed at FZJ, PUE and CUE indexes are equal for
both testbed clusters, since both machines are located in
the same data centre environment. However, the Dune
cluster of VTT is located at Oulu in Finland and provides
diﬀerent PUE and CUE speciﬁc values.
Additionally, the ECA algorithm checks if the user
deﬁned ‘latest job ﬁnishing time’ can be satisﬁed, i.e., we
used user deﬁned allowed delay value for load balancing
between clusters. This means that the plug-in veriﬁes if
the job can be executed and ﬁnished on a cluster within a
user deﬁned time limit. If not, the job cannot be scheduled
in the best energy eﬃcient way and the next cluster is
chosen, where the estimated wait time is smaller than the
user deﬁned value. By this mechanism, the user can set
a threshold value from where jobs must not be scheduled
energy eﬃciently anymore.
The crucial factor in terms of energy eﬃciency is
the power consumption of the testbed clusters over a
dedicated time. Furthermore, it was worth to analyse
the impact of an application benchmark parameter,
which takes into account the diﬀerent performance of
an application produced on a particular cluster.
Scheduling jobs in a federated environment of diﬀerent
supercomputers is not yet very common in HPC. Re-
source brokering in heterogeneous environments is still
an on-going research area in HPC. Meta-schedulers are
usually not yet deployed in real production environments
but rather in smaller research and test environments.
One of the main barriers is that HPC job requirements
are often strongly system-related so that the jobs can
only run in a dedicated hard- and software environment.
There is also a lack of dynamic resource information

464
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
on the broker level about node and queue statuses of
the bounded supercomputers. Usually, users have a clear
picture where to submit their jobs. Often these jobs can
only run on particular nodes, since only there the required
application software is installed.
Accordingly, there are no matured solutions of meta-
schedulers available that could be used as reference
software. So, in the testbed experiments we compared our
solution with a simple round-robin scheduling mechanism
provided by the UNICORE software. This solution does
not consider any dynamic system information about jobs
in queues or the statuses of nodes. A more intelligent
solution for scheduling jobs in federated environments is
the developed plug-in’s FP algorithm, which does not take
into account energy or emission aware parameters but
only the fastest execution time of jobs on the clusters. So
the focus of the federated scenario evaluation was not only
to compare ECA and FP with default RR algorithm, but
to check the impact of the provided scheduling parameters
on the results. In the federated tests the energy-aware
job scheduling used in the single sites assessment was
deactivated to set the focus on the energy saving potential
of the federated algorithms.
For analysing the impact of diﬀerent PUEs on the sites
we changed the PUE value of the FZJ site in each trial
and used a ﬁxed value on the VTT site where Dune is
located, so that we could analyse the system utilization
of the clusters while changing the PUE diﬀerence of
them. The PUE value on Dune, which was calculated by
VTT administrators, was set to 1.2. For the FZJ site we
actually calculated a value of 1.4. In the evaluation we
iterated the FZJ parameter from 1.0 to 1.8.
Figure 14 shows clearly the impact on the utilization
of the particular testbed clusters when iterating over the
FZJ PUE value. The graph of the Juggle is relatively
constant, since the Optimizer schedules only some default
jobs to the system, which can run only there (requesting
8 nodes per job). Apart from these jobs no others are
submitted to that machine, since the Optimizer considers
its high basic power consumption compared to the other
two clusters. In contrast, Juﬁt is getting at ﬁrst the most
jobs of the workload when having a lower PUE than
Dune which was set to 1.2. The consequence is at the
beginning higher system utilization on Juﬁt compared to
Dune.
When Juﬁt and Dune have the same PUE (1.2) Dune
is already preferred clearly by the Optimizer since it
has the most eﬃcient power consumption per compute
node in the testbed. Higher PUEs for FZJ-Juﬁt result
in even lower utilizations for Juﬁt and higher ones for
Dune. With more than 1.4 PUE on Juﬁt saturation on
the utilization of Dune can be detected. Dune is utilized
almost with 100% and also Juﬁt levels out at about 35%.
The CUE evaluation was performed in a similar way
0
20
40
60
80
100
120
1,00
1,20
1,40
1,60
1,80
2,00
Utilization [%]
FZJ-site-PUE 
PUE iteration 
Juggle
Jufit
Dune
Figure 14.
Impact on cluster utilization when iterating over PUE
0
20
40
60
80
100
120
0,40
0,50
0,60
0,70
0,80
0,90
1,00
Utilization [%]
FZJ-site-CUE
CUE iteration
Juggle
Jufit
Dune
Figure 15.
Impact on cluster utilization when iterating over CUE
as the PUE iteration. After changing the Optimizer
objective to CO2 emissions, the CUE parameter of the
available two sites was used for calculating the job
scheduling in the federated scenario tests. For Dune
a CUE value of 0.43 was calculated based on local
energy and emission characteristics. For the FZJ site
we ascertained a value of 0.8. Again, for analysing the
impact of diﬀerent CUEs, we iterated over diﬀerent CUEs
at the FZJ site.
The test results in Figure 15 show that the Juggle
utilization is again constant with diﬀerent CUEs since
the Optimizer schedules only some workload jobs to
Juggle which can only run on that machine. Apart from
these jobs no other ones were assigned to that cluster
since its basic energy consumption is too high compared
to the other testbed machines. Juﬁt shows a slightly
higher utilization than Dune when the CUE value is on
a similar level than the Dune one. However, this eﬀect
is reversed when the CUE value of FZJ site is increased.
The Optimizer calculates then a higher emission on Juﬁt
and generates therefore a higher utilization on Dune for
meeting the CO2 emission objective.
The runtime of a job on a dedicated machine can have
an important impact on the overall energy consumption.
Therefore, the plug-in’s application benchmark feature
was introduced to map the diﬀerent performance of
supercomputers when running jobs with known HPC

465
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
0
20
40
60
80
100
120
0
5
10
15
20
Nr of executed Jobs
Aplication Benchmark of Jufit 
Application Benchmarking iteration
Juggle
Jufit
Dune
Figure 16.
Impact of application benchmarking on job scheduling
applications. The mapping is implemented by adding an
application ID and a corresponding integer value, which
indicates how eﬃciently the application can be executed.
In the test trials we used the LINPACK application for
the evaluation of that parameter. LINPACK is installed
on all available testbed clusters. Test benchmarks showed
that all machines provide a diﬀerent performance when
executing jobs using the same LINPACK conﬁguration.
Finally, after measuring the execution time of the test
jobs we were able to deﬁne diﬀerent benchmark values for
all the clusters of the testbed. Nevertheless, we wanted
to evaluate not only this benchmark distribution but
the impact of the application benchmarking on the job
distribution and system utilization of the testbed. So, we
performed additional tests with changing the benchmark
parameter of the Juﬁt cluster and measuring the number
of executed jobs on the clusters when iterating over
diﬀerent parameters (5, 10, and 15).
Figure 16 shows the distribution of the jobs during
these diﬀerent iterations. The Juggle cluster receives
only a few jobs for all iterations. More revealing results
could be detected in that evaluation on Juﬁt and Dune.
When having similar benchmarks (Juﬁt 5, Dune 4) Dune
receives clearly most of the workload jobs, since there
is no big diﬀerence in that parameter between both
machines and Dune is preferred by the Optimizer because
of the node’s better power consumption. When increasing
the Juﬁt benchmark parameter from 5 to 15 it can be
detected that Juﬁt gets as more jobs from the Optimizer
as higher the benchmark parameter has been set. Finally,
a benchmark value at Juﬁt of 3 times higher than
the Dune value results that the actually more power
eﬃcient Dune machine is overpowered by the much better
benchmark eﬃciency of the Juﬁt machine, i.e., even if
Juﬁt consumes more power, it consumes less energy since
it is expected to execute the job much faster than Dune.
This shows that the benchmark application parameter
can have a strong impact on the job distribution, which
again aﬀects the energy consumption of the federated
clusters.
0
20
40
60
80
100
120
0
2000
4000
6000
8000
10000
12000
Nr of executed Jobs
Latest job finishing time [s]
Latest job finish time iteration
Juggle
Jufit
Dune
Figure 17.
Job distribution and latest job ﬁnishing time
The Optimizer checks additionally if the user deﬁned
‘latest job ﬁnishing time’ can be satisﬁed. This means
that it is checked if the job can be executed and ﬁnished
on a cluster within a user deﬁned limit. If not the job
cannot be scheduled in the most energy eﬃcient way and
the next cluster is chosen where the estimated wait time
is smaller than the user deﬁned value. By this mechanism
the user can set a threshold value as part of the given job
description from where on jobs must not be scheduled
energy eﬃciently anymore. It is assumed that users will
set a multiple value of the wall time of a job when
considering the latest job ﬁnishing time. For example,
if a job gets a walltime of 8 hours, it can be assumed
that the user expects in general some wait time before
his job can be executed. So, we analysed the impact of
the latest ﬁnishing parameters which were set between
three and ten times as high as the average walltime of
the submitted workload jobs. Figure 17 shows the results
of the job distribution when iterating the same workload
with diﬀerent job ﬁnishing parameters.
When relatively short latest job ﬁnishing times were
set by users the Optimizer is not able to schedule most of
the jobs in an energy eﬃcient way. Taking into account
the power estimation the Optimizer submits the jobs at
ﬁrst to Dune, calculates then that the overall ﬁnishing
time (wait time + runtime) of the other waiting jobs
would exceed their latest ﬁnish time, and thus will submit
new jobs to other clusters until Dune provides again time
slots for waiting jobs. In that way, it can be detected that
even the Juggle cluster, which is usually underutilized
by the federated plug-in scheduling algorithms because
of its bad energy eﬃciency, gets more jobs as usual with
low latest job ﬁnishing time parameters. With increasing
latest job ﬁnishing times the plug-in has a greater margin
to schedule jobs to energy eﬃcient clusters, which is the
Dune cluster at VTT in this evaluation.
In general, the latest job ﬁnishing time parameter is
very important to guarantee as much as possible a fair
sharing of the federated resources when there is high
system utilization. On the other hand this parameter

466
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
helps to submit jobs each time to the most eﬃcient
cluster as long as the user’s latest job ﬁnishing time
can be satisﬁed. This condition can be rather satisﬁed
in underutilized federated resources. Using additionally
some of the developed single site algorithms to save energy
on these underutilized resources would lead to further
energy savings.
In the ﬁnal assessment we analysed the overall energy
and emission saving potential of the plug-in’s federated
algorithms. For this purpose, we used the constructed
PUE and CUE values of the two testbed sites FZJ and
VTT. While at VTT a CUE of 0.43 and a PUE of 1.2
has been deﬁned, we set at Juﬁt and Juggle a CUE of
0.8 and a PUE of 1.4. For the LINPACK application
benchmark parameter we used the results of our test
measurements to detect the most eﬃcient benchmark
distribution, which is Juggle=1, Juﬁt=10, and Dune=4.
Concerning the latest job ﬁnishing time parameter
we compared the energy consumption of diﬀerent values
since this parameter reﬂects very reasonably the overall
utilization of the federated resources. The results were
compared at ﬁrst glance with the default round-robin
strategy of the UNICORE middleware. However, the
round-robin strategy is a bad algorithm in terms of energy
eﬃciency. It just submits the jobs by considering time
slots in the scheduling calculation and distributes thus
the jobs pretty equally to the resources without regarding
available idle resources, job queue information, and power
consumption.
A much better algorithm in terms of scheduling jobs
in federated environments without regarding the power
consumption, PUE, and CUE values is the FP algorithm,
which is a good reference point when comparing energy-
aware- with non-energy-aware-algorithms in federated
environments.
Table IX shows the results regarding energy consump-
tion and produced emissions when using the plug-in for
scheduling jobs eﬃciently in federated environments. The
ﬁrst column shows the measurement that was performed
with a relatively low latest ﬁnishing time parameter of
3600 s while running test jobs with an average walltime of
720 seconds. Compared to the USO round-robin strategy
one could save up to 52% energy when using the plug-in.
When the latest ﬁnishing time is increased to 10800 s,
this saving was even raised to 67%. This seems to be
a very high saving. The reason is that the round-robin
strategy submits in contrast to the plug-in a lot more
jobs to Juggle. This results obviously in a much higher
energy consumption of the Juggle nodes. Whereas more
jobs on Juﬁt or Dune only raise the energy consumption
moderately, on Juggle more jobs generate a signiﬁcantly
higher consumption.
The result of a measurement with the plug-in’s FP
algorithm is listed in the third column. Also that strategy
Table IX
Energy and Emissions in HPC federated measurements
ECA
ECA
FP
RR
Latest ﬁnish time [s]
3600
10800
10800
-
Juggle [kJ]
4668
1916
3705
12893
CUE
0.80
0.80
0.80
0.80
PUE
1.40
1.40
-
-
Jobs
16
5
11
43
Elapsed Time [s]
2979
1245
2434
8181
Average Power [W]
1567
1540
1522
1576
Utilization [%]
66
56
46
81
Juﬁt [kJ]
961
767
1051
962
CUE
0.80
0.80
0.80
0.80
PUE
1.40
1.40
-
-
Jobs
57
25
61
45
Elapsed Time [s]
2940
2536
3213
2838
Average Power [W]
327
304
327
339
Utilization [%]
65
49
93
45
Dune [kJ]
1442
2163
1426
1029
CUE
0.43
0.43
0.43
0.43
PUE
1.20
1.20
-
-
Jobs
62
106
63
47
Elapsed Time [s]
3163
4721
3211
2839
Average Power [W]
456
458
444
430
Utilization [%]
82
97
88
79
Total cluster [kJ]
7072
4846
6181
14884
Saving to FP [%]
-
21.59
-
-
Saving to USO [%]
52.49
67.44
-
-
Total cluster emissions
1.42
0.86
1.31
3.62
[kgCO2eq/kwh]
schedules some jobs to Juggle. However, in that case, the
waiting time of the queued jobs is checked continuously
by the Optimizer, so that much less jobs run at the end on
that machine. Moreover, considering the elapsed times of
the workload on the particular clusters it can be detected
that the total summarized elapsed time is shorter com-
pared to the energy aware algorithm. Nevertheless, the
FP algorithm is not as energy eﬃcient as the energy aware
strategy. At the end, the higher energy consumption of
the most ineﬃcient cluster is particularly disadvantageous
for the total workload energy consumption. So, compared
with the FP strategy and using same latest ﬁnishing
times, the ECA algorithm saves about 21% energy. In
the federated scenario, the energy-aware job scheduler
(if used) causes the same 2-3 % overhead as in single site
scenario. Additionally, FP and ECA algorithms cause
some more overhead compared to the simpler round-robin
algorithm that does not need any dynamic information
from the clusters. Clearly, the overhead is much smaller
than the achieved savings.

467
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
VI. Conclusion and Future Work
The results show that the generally used round-robin
cluster selection algorithm can lead to unbalanced uti-
lizations among clusters. This can be very ineﬃcient in
terms of energy consumption and CO2 emissions. Using
energy-aware job scheduling to power oﬀ idle computing
nodes whenever possible greatly enhances the energy-
eﬃciency. Load can also be balanced by replacing round-
robin cluster selection by the Fastest possible selection
algorithm. This leads to energy savings due to the better
utilization of clusters and shorter wait times. Using both
energy-aware job scheduling and FP cluster selection
simultaneously leads to greater energy savings than using
only one of them. The greatest CO2 emission savings can
be achieved by using ECA cluster selection algorithm
to favour the cluster with least CO2 emissions. The
actual savings in each case depends on the cluster and
job characteristics. In these simulations, for example,
the energy sources were chosen so that one cluster had
rather small CUE, another one rather big CUE, while
the third one was something between them. With smaller
diﬀerences in CUE, also the possible savings in CO2
emissions would be smaller.
Based on the simulation results presented above, we
propose to use FP cluster selection algorithm for the
jobs without green SLA, since it leads to energy and
CO2 emission savings due to the better utilization of
the clusters, and to better QoS due to the shorter wait
time. For the jobs with green SLA, we propose to use
the ECA cluster selection algorithm, since it can lead
to even greater CO2 emission savings than FP, while
keeping the QoS (in terms of time) at the user speciﬁed
level. It can be used also without green SLA, if some
other parameter (e.g., queue size limit) is used for load
balancing to prevent excessive load on the "greenest"
cluster.
The results of the single site testbed assessment, with
a focus on a DVFS feature, conﬁrmed the experiences of
our previous work [2] to the eﬀect that the energy saving
potential when using energy-aware scheduling increases
with decreasing system utilization. When having loads
above 80%, the saving potential is very limited (<10%).
On the other hand utilizations below 60% show more
reasonable savings (10% to 34%). The highest possible
energy saving on the Juggle cluster was 34.7%, which
happens when the system is completely idle. That value
is limited by the fact that a compute node that was
set to ACPI state standby cannot save more than 35%
energy. It remains to be stated that the higher the system
utilization the less compute nodes can be set to an energy
saving status.
For the federated testbed results, it must be stated that
the energy saving potential is absolutely dependent on the
available hardware. A slow and high-consumption cluster
as Juggle in a federated supercomputing environment
can have a major impact on the results. If we had
used three similar testbed clusters regarding their energy
consumption, it can be expected that the energy saving
potential would have been far smaller. In that case also a
round-robin strategy would have produced better results
and the FP strategy in the same way. Nevertheless, the
performed test studies revealed many new insights into
the saving potential of scheduling jobs energy eﬃciently
in federated supercomputing environments. The results
obtained can be used as a basis for the design of speciﬁc
federated cluster environments when using the developed
plug-in to enable an energy-aware scheduling of the
resources.
The simulation studies and testbed experiments were
performed with a diﬀerent set of parameters e.g., hard-
ware, type of jobs and cluster conﬁgurations. However,
it is possible to note from both results that there is an
energy saving potential in federated HPC environments
that is dependent on the available hardware and cluster
utilization. A poor utilization of clusters can ultimately
lead to an increase in energy and emissions.
Previous research in the energy-eﬃciency of HPC grid
computing has mainly focused on performing optimiza-
tions inside a single data centre. This work presented a
global view by taking into account the whole grid: the
characteristics of the data centres, compute nodes and
the computing hardware. The most comparable approach
to our work is HAMA, described in [14]. The results of
HAMA are similar to our approach: energy savings are
between 23% and 50%.
Recently, the plug-in’s energy-aware job scheduler has
been enhanced by adding several additional scheduling
policies that are available in commercial schedulers
like Moab. The new supported policies are mainly for
enhancing the QoS for the users, including for example
fair share and job exclusive policies. Fair share policy
prevents a single user from conquering all the nodes with
multiple jobs if there are also other users’ jobs waiting
in the queue. Job exclusive policy means that the job
will get all the resources of the entire node for itself;
only one job per node is then allowed. Our future work
include testing the eﬀect of these policies to the energy
and emission savings.
Acknowledgment
This work extends our earlier work [1]; the invitation
to submit this extended version is highly appreciated.
This work was supported by EU FP7 project FIT4Green
[33]. The authors would like to thank all the colleagues
that have worked in the project, as well as the anonymous
reviewers for their comments.

468
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
References
[1] M. Majanen and O. Mämmelä, “Optimization of Energy
and Emissions in High-Performance Grid Computing
Data Centres,” in The Second International Conference
on Smart Grids, Green Communications and IT Energy-
aware Technologies (ENERGY 2012), St. Maarten,
Netherlands Antilles, Mar. 2012.
[2] O. Mämmelä, M. Majanen, R. Basmadjian, H. Meer,
A.
Giesler,
and
W.
Homberg,
“Energy-aware
job
scheduler
for
high-performance
computing,”
Computer
Science
-
Research
and
Development,
vol.
27,
pp.
265–275,
2012.
[Online].
Available:
http://dx.doi.org/10.1007/s00450-011-0189-6
[3] Y. Liu and H. Zhu, “A survey of the research on power
management techniques for high-performance systems,”
Softw. Pract. Exper., vol. 40, pp. 943–964, October 2010.
[4] Gartner.
(2012,
Sep.)
Gartner
Estimates
ICT
Industry
Accounts
for
2
Percent
of
Global
CO2
Emissions.
[Online].
Available:
http://www.gartner.com/it/page.jsp?id=503867
[5] The Climate Group, “SMART 2020: Enabling the low
carbon economy in the information age,” Tech. Rep.,
2008.
[6] D. Erwin and D. Snelling, “UNICORE: A Grid Comput-
ing Environment,” in Euro-Par 2001 Parallel Processing,
ser. Lecture Notes in Computer Science.
Springer Berlin
/ Heidelberg, 2001, vol. 2150, pp. 825–834.
[7] C. Belady, “Green Grid Data Center Power Eﬃciency
Metrics: PUE and DCIE,” Green Grid, Tech. Rep., 2008.
[8] M. Di Girolamo and et. al., “Final evaluation report on pi-
lots of full-featured enhanced control plug-in and control
desk for federated data centre,” FIT4Green, Deliverable
D6.4 v2.0, Jul. 2012, http://www.ﬁt4green.eu/.
[9] S. Hong and H. Kim, “An integrated gpu power
and
performance
model,”
in
Proceedings
of
the
37th annual international symposium on Computer
architecture,
ser.
ISCA
’10.
New
York,
NY,
USA: ACM, 2010, pp. 280–289. [Online]. Available:
http://doi.acm.org/10.1145/1815961.1815998
[10] D. Li, S. Byna, and S. Chakradhar, “Energy-aware work-
load consolidation on gpu,” in 2011 40th International
Conference on Parallel Processing Workshops (ICPPW),
Sep. 2011, pp. 389 –398.
[11] K. Ma, X. Li, W. Chen, C. Zhang, and X. Wang,
“Greengpu: A holistic approach to energy eﬃciency
in gpu-cpu heterogeneous architectures,” in 2012 41st
International Conference on Parallel Processing (ICPP),
Sep. 2012, pp. 48 –57.
[12] Moab Green Computing. (2012, Dec.). [Online]. Available:
http://www.adaptivecomputing.com/resources/docs/
mwm/archive/6-0/18.0greencomputing.php
[13] F. Ehmke, “Moab evaluation,” University of Hamburg,
Project Report, Dec. 2011, http://wr.informatik.uni-
hamburg.de/_media/research/labs/2011/
2011-09-ﬂorian_ehmke-moab_evaluation-report.pdf.
[14] S. K. Garg and R. Buya, “Exploiting Heterogeneity in
Grid Computing for Energy-Eﬃcient Resource Alloca-
tion,” Seventeenth Annual International Conference on
Advanced Computing and Communications (ADCOM
2009), 2009.
[15] T. Lynar, R. Herbert, Simon, and W. Chivers, “Reducing
grid energy consumption through choice of resource
allocation method,” 2010 International Symposium on
Parallel & Distributed Processing, Workshops and Phd
Forum (IPDPSW), May 2010.
[16] C. Patel, R. Sharma, C. Bash, and S. Graupner, “Energy
aware grid: Global workload placement based on energy
eﬃciency,” Hewlett Packard, HP Laboratiories Palo Alto,
Tech. Rep., November 2002.
[17] A. J. Shah and N. Krishnan, “Optimization of global
data center thermal management workload for minimal
environmental and economic burden,” IEEE Transactions
on Components and Packaging Technologies, vol. 31, no. 1,
pp. 39–45, March 2011.
[18] G. Da Costa, J.-P. Gelas, Y. Georgiou, L. Lefevre, A.-
C. Orgerie, J.-M. Pierson, O. Richard, and K. Sharma,
“The GREEN-NET Framework: Energy Eﬃciency in
Large Scale Distributed Systems,” HPPAC 2009 : High
Performance Power Aware Computing Workshop in
conjunction with IPDPS 2009, May 2009.
[19] Z.
Liu,
M.
Lin,
A.
Wierman,
S.
H.
Low,
and
L. L. Andrew, “Geographical load balancing with
renewables,” SIGMETRICS Perform. Eval. Rev., vol. 39,
no.
3,
pp.
62–66,
Dec.
2011.
[Online].
Available:
http://doi.acm.org/10.1145/2160803.2160862
[20] K. Le, R. Bianchini, T. D. Nguyen, O. Bilgir, and
M. Martonosi, “Capping the brown energy consumption
of internet services at low cost,” in Proceedings of the
International Conference on Green Computing, ser.
GREENCOMP ’10.
Washington, DC, USA: IEEE
Computer Society, 2010, pp. 3–14. [Online]. Available:
http://dx.doi.org/10.1109/GREENCOMP.2010.5598305
[21] I.
Goiri,
K.
Le,
T.
D.
Nguyen,
J.
Guitart,
J. Torres, and R. Bianchini, “Greenhadoop: leveraging
green
energy
in
data-processing
frameworks,”
in
Proceedings of the 7th ACM european conference on
Computer Systems, ser. EuroSys ’12.
New York,
NY, USA: ACM, 2012, pp. 57–70. [Online]. Available:
http://doi.acm.org/10.1145/2168836.2168843
[22] Z.
Liu,
M.
Lin,
A.
Wierman,
S.
H.
Low,
and
L. L. Andrew, “Greening geographical load balancing,”
in
Proceedings
of
the
ACM
SIGMETRICS
joint
international conference on Measurement and modeling
of computer systems, ser. SIGMETRICS ’11.
New York,
NY, USA: ACM, 2011, pp. 233–244. [Online]. Available:
http://doi.acm.org/10.1145/1993744.1993767

469
International Journal on Advances in Intelligent Systems, vol 5 no 3 & 4, year 2012, http://www.iariajournals.org/intelligent_systems/
2012, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[23] Z. Liu, Y. Chen, C. Bash, A. Wierman, D. Gmach,
Z. Wang, M. Marwah, and C. Hyser, “Renewable and
cooling aware workload management for sustainable
data
centers,”
in
Proceedings
of
the
12th
ACM
SIGMETRICS/PERFORMANCE joint international
conference on Measurement and Modeling of Computer
Systems, ser. SIGMETRICS ’12.
New York, NY,
USA: ACM, 2012, pp. 175–186. [Online]. Available:
http://doi.acm.org/10.1145/2254756.2254779
[24] I. Goiri, K. Le, M. Haque, R. Beauchea, T. Nguyen,
J. Guitart, J. Torres, and R. Bianchini, “Greenslot:
Scheduling energy consumption in green datacenters,”
in 2011 International Conference for High Performance
Computing, Networking, Storage and Analysis (SC), Nov.
2011, pp. 1 –11.
[25] S. Klingert, T. Schulze, and C. Bunse, “GreenSLAs for
the eco-eﬃcient management of data centres,” in 2nd
International Conference on Energy-Eﬃcient Computing
and Networking 2011 (E-energy 2011), New York, NY,
USA, 2011.
[26] C. Belady, D. Azevedo, M. Patterson, J. Pouchet, and
R. Tipley, “Carbon Usage Eﬀectiveness (CUE): A Green
Grid Data Center Sustainability Metric,” Green Grid,
Tech. Rep., December 2010.
[27] RealtimeCarbon.org.
(2012,
Sep.)
CO2
conversion
factors.
[Online].
Available:
http://www.realtimecarbon.org/resources/
RealtimeCarbonMethodology.pdf
[28] W. Cirne and F. Berman, “A comprehensive model of
the supercomputer workload,” in IEEE International
Workshop on Workload Characterization, WWC-4. 2001,
dec. 2001, pp. 140–148.
[29] C. Bailey Lee, Y. Schwartzman, J. Hardy, and A. Snavely,
“Are user runtime estimates inherently inaccurate?” in
Job Scheduling Strategies for Parallel Processing, ser.
Lecture Notes in Computer Science.
Springer Berlin /
Heidelberg, 2005, vol. 3277, pp. 253–263.
[30] R. Basmadjian, N. Ali, F. Niedermeier, H. De Meer,
and G. Giuliani, “A methodology to predict the power
consumption of servers in data centres,” in Proc. of the
ACM SIGCOMM 2nd Int’l Conf. on Energy-Eﬃcient
Computing and Networking (e-Energy 2011).
ACM,
2011.
[31] OMNeT++.
(2012,
Sep.).
[Online].
Available:
http://www.omnetpp.org
[32] INET Framework. (2012, Sep.). [Online]. Available:
http://inet.omnetpp.org/
[33] FIT4Green project. (2012, Sep.) FIT4Green: Energy
aware ICT optimization policies. [Online]. Available:
http://www.ﬁt4green.eu
[34] V. Georgiadou, A. Salden, C. Dupont, A. Somov,
A. Giesler, J. C. L. Egea, P. Barone, G. Giuliani,
O. Abdelrahman, R. Lent, M. Kessel, T. Schulze, R. Bas-
madjian, H. D. Meer, M. Majanen, and O. Mämmelä,
“Enhanced control plug-in and control desk software
design speciﬁcations,” FIT4Green, Deliverable D-5.3 v1.0,
May 2012, http://www.ﬁt4green.eu/.
[35] FIT4Green plug-in source code. (2012, Sep.). [Online].
Available: https://github.com/ﬁt4green/FIT4Green
[36] Linux/ACPI
-
Documentation.
(2012,
Sep.).
[Online].
Available:
http://acpi.sourceforge.net/documentation/sleep.html
[37] LINPACK.
(2012,
Sep.).
[Online].
Available:
http://www.netlib.org/linpack/
[38] PEPC - Pretty Eﬃcient Parallel Coulomb Solver. (2012,
Sep.). [Online]. Available: www2.fz-juelich.de/zam/pepc
[39] Unicore Client Layer. (2012, Sep.). [Online]. Avail-
able: http://www.unicore.eu/unicore/architecture/client-
layer.php

