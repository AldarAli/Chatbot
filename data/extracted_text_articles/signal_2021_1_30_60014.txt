Identiﬁcation of Multilinear Forms Using
Combinations of Adaptive Algorithms
Ionut¸-Dorinel Fˆıciu, Laura-Maria Dogariu, Cristian-Lucian Stanciu, and Constantin Paleologu
University Politehnica of Bucharest, Romania
Email: {ldogariu, cristian, pale}@comm.pub.ro
Abstract—The tensor-based adaptive algorithms represent use-
ful tools for the identiﬁcation of linearly separable systems. These
algorithms are designed in the framework of multilinear forms
and exploit the decomposition of rank-1 tensors. In this paper,
we outline the idea of using different adaptive algorithms (with
different performance features) for the individual ﬁlters, which
represent the components of a rank-1 tensor. In this context, the
global scheme based on such combinations could inherit some
of the advantages provided by each category of algorithms, e.g.,
fast convergence rate and low computational complexity.
Index Terms—adaptive ﬁlter; multilinear forms; least-mean-
square (LMS) algorithm; recursive least-squares (RLS) algorithm;
tensor decomposition
I. INTRODUCTION
Many important system identiﬁcation problems can be efﬁ-
ciently solved using adaptive ﬁlters [1]. The real-time features
of these signal processing tools are advantageous especially in
time-varying environments. In this context, the performance
criteria mainly target fast convergence rate, accurate estima-
tion, and low computational complexity.
A more challenging scenario appears in the framework of
Multiple-Input Single-Output (MISO) systems. The adaptive
ﬁlters developed for this purpose should cope with the ex-
istence of a large parameter space. However, some of these
problems can be formulated in terms of linearly separable
systems. Such an approach can be exploited in different
applications, like array beamforming, nonlinear acoustic echo
cancellation, channel equalization, and source separation, e.g.,
see [2] and the references therein.
In this context, the tensor-based adaptive ﬁlters [2] represent
efﬁcient solutions. These adaptive algorithms rely on the
decomposition of rank-1 tensors, while the global solution
results using a combination of shorter adaptive ﬁlters. As
shown in [2], the tensorial approach can be applied using
the classical adaptive algorithms, e.g., the Least-Mean-Square
(LMS) and the Recursive Least-Squares (RLS).
In this work, we explore the idea of using different adap-
tation modes for the individual ﬁlters, aiming to inherit the
advantages of each category of algorithms. In other words, the
RLS algorithm is used for its fast convergence rate (paid by
a higher computational amount), while the normalized LMS
(NLMS) algorithm owns the low-complexity feature.
Following this introduction, Section II brieﬂy presents the
multilinear framework, while Section III describes the adaptive
algorithms and their combination. An experimental result is
provided in Section IV, followed by a conclusion in Section V.
II. IDENTIFICATION OF MULTILINEAR FORMS
In the framework of a real-valued MISO system, the output
signal (at discrete-time index n) is deﬁned as
y(n) =
L1
∑
l1=1
L2
∑
l2=1
· · ·
LN
∑
lN=1
xl1l2...lN (n)h1,l1h2,l2 · · · hN,lN ,
(1)
where hi = [hi,1 hi,2 · · · hi,Li]T are N individual channels,
each one of length Li, i = 1, 2, . . . , N, and the superscript
T denotes the transpose operator. The input signals can be
described in the tensorial form X(n) ∈ RL1×L2×···×LN ,
having the elements (X)l1l2...lN (n) = xl1l2...lN (n). Thus, the
output signal from (1) becomes
y(n) = X(n) ×1 hT
1 ×2 hT
2 ×3 · · · ×N hT
N,
(2)
where ×i, i = 1, 2, . . . , N denotes the mode-i product. As
we can notice, y(n) is a multilinear form, since it is a linear
function of each hi, i = 1, 2, . . . , N, when the other N − 1
components are ﬁxed.
Let us consider the rank-1 tensor H ∈ RL1×L2×···×LN ,
with the elements (H)l1,l2,...,lN = h1,l1h2,l2 · · · hN,lN , such
that
H = h1 ◦ h2 ◦ · · · ◦ hN,
(3)
where ◦ denotes the outer product. In addition, we have
vec (H) = hN ⊗ hN−1 ⊗ · · · ⊗ h1,
(4)
where vec(·) is the vectorization operation and ⊗ denotes the
Kronecker product. Hence, we can rewrite (1) as
y(n) = vecT (H) vec [X(n)] .
(5)
Furthermore, we can denote x(n) = vec [X(n)] and g =
vec (H). Here, x(n) is the input vector of length L1L2 · · · LN
and g plays the role of a global impulse response (of the same
length). Therefore, (1) ﬁnally becomes
y(n) = gT x(n),
(6)
while the reference signal usually results as
d(n) = gT x(n) + w(n),
(7)
where w(n) is the measurement noise, which is uncorrelated
with the input signals. The main goal is the identiﬁcation of the
global system g. Equivalently, the identiﬁcation problem can
be formulated in terms of recursively estimating the individual
components hi, i = 1, 2, . . . , N.
11
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-858-7
SIGNAL 2021 : The Sixth International Conference on Advances in Signal, Image and Video Processing

III. COMBINATIONS OF ADAPTIVE ALGORITHMS
Let us consider the estimated impulse responses of the
channels, bhi(n), i = 1, 2, . . . , N, and the estimated output,
by(n), such that the error signal result in N equivalent ways:
e(n) = d(n) − by(n) = d(n) − bhT
i (n − 1)xbhi(n),
(8)
for i = 1, 2, . . . , N, where
xbhi(n) =
[
bhN(n − 1) ⊗ bhN−1(n − 1) ⊗ · · · ⊗ bhi+1(n − 1)
⊗ ILi ⊗ bhi−1(n − 1) ⊗ · · · ⊗ bh2(n − 1) ⊗ bh1(n − 1)
]T
x(n),
with ILi, i = 1, 2, . . . , N denoting the identity matrices of
sizes Li × Li. Using a multilinear optimization strategy based
on the mean-squared error (MSE) criterion, the updates of the
N adaptive ﬁlters result in
bhi(n) = bhi(n − 1) + µi(n)xbhi(n)e(n),
(9)
where µi(n), i = 1, 2, . . . , N are the step-size parameters,
while the estimate of the global ﬁlter is obtained as
bg(n) = bhN(n) ⊗ bhN−1(n) ⊗ · · · ⊗ bh1(n).
(10)
In nonstationary environments, it is advantageous to follow
the line of the NLMS algorithm. In this context, the step-size
parameters of the tensor-based NLMS (NLMS-T) algorithm
are obtained as µi(n) = αi/
[
δi + xT
bhi(n)xbhi(n)
]
, with i =
1, 2, . . . , N, where 0 < αi ≤ 1 are the normalized step-sizes
and δi > 0 are the regularization constants.
Alternatively, we can apply the least-squares (LS) error
criterion [1] in the context of (7) and (8). Thus, the cost func-
tions can be formulated in N alternative ways, following the
optimization procedure of the individual impulse responses,
Furthermore, the minimization of these cost functions with
respect to bhi(n), i = 1, 2, . . . , N leads to a set of normal
equations, which result in the updates of the individual ﬁlters:
bhi(n) = bhi(n − 1) + ki(n)e(n), i = 1, 2, . . . , N,
(11)
where ki(n) are the Kalman gain vectors and e(n) is evaluated
based on (8). The Kalman gain vectors are
ki(n) =
R−1
i (n − 1)xbhi(n)
λi + xT
bhi(n)R−1
i (n − 1)xbhi(n),
(12)
where λi (i = 1, 2, . . . , N) are the individual forgetting
factors. Finally, the matrix inversion lemma [1] is used to
update the matrices R−1
i (n), i.e.,
R−1
i (n) = 1
λi
[
ILi − ki(n)xT
bhi(n)
]
R−1
i (n − 1),
(13)
for i = 1, 2, . . . , N. Summarizing, the tensor-based RLS
(RLS-T) algorithm is deﬁned by the relations (11)–(13).
In order to take advantage of the particular features of
the algorithms, we propose a combination of adaptive ﬁlters
that uses the RLS-T for the longest ﬁlter, while the rest of
them are updated as in the NLMS-T algorithm. In this way,
the resulting algorithm (namely RLS-NLMS-T) would inherit
Iterations
×104
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
NM (dB)
-45
-40
-35
-30
-25
-20
-15
-10
-5
0
5
NLMS-T
RLS-T
RLS-NLMS-T
Figure 1. NM of the NLMS-T, RLS-T, and RLS-NLMS-T algorithms.
the fast convergence features of the RLS-T algorithm, while
reducing the overall complexity due to the N − 1 ﬁlters that
are based on the NLMS-T algorithm.
IV. EXPERIMENT
In the following experiment, the input signals are AR(1)
processes, which are generated by ﬁltering white Gaussian
noises through a ﬁrst-order system with the pole 0.99. The
additive noise w(n) is white and Gaussian, with the variance
equal to 0.01. The order of the system used in the experiments
is N = 4, while the individual impulse responses hi, i =
1, 2, . . . , N are generated as in [2], but using L1 = 32, L2 = 8,
and L3 = L4 = 4. Thus, the global impulse response g has
the length 4096. The performance measure is the identiﬁcation
of the global impulse response using the normalized mis-
alignment NM [g, bg(n)] = ∥g − bg(n)∥2
2 / ∥g∥2
2, where ∥·∥2
is the Euclidean norm. In Figure 1, the main parameters of
the algorithms are set to αi = 0.25 and λi = 1 − 1/50Li,
for i = 1, 2, . . . , N. As we can notice, the RLS-T and RLS-
NLMS-T algorithms perform very similar, in terms of the
convergence rate/tracking and misalignment.
V. CONCLUSION
In this work, we have explored the idea of using a combi-
nation of adaptive ﬁlters for multilinear forms. The proposed
RLS-NLMS-T algorithm achieves a fast convergence rate,
while having a lower computational complexity as compared
to the RLS-T algorithm. Future works will investigate com-
putationally efﬁcient versions of the RLS-NLMS-T algorithm,
which could be based on the coordinate descent iterations [3].
ACKNOWLEDGEMENT
This work was supported by two grants of the Romanian Ministry of
Education and Research, CNCS–UEFISCDI, project no. PN-III-P1-1.1-PD-
2019-0340 and project no. PN-III-P1-1.1-TE-2019-0529, within PNCDI III.
REFERENCES
[1] S. Haykin, Adaptive Filter Theory. Fourth Edition, Upper Saddle River,
NJ, USA: Prentice-Hall, 2002.
[2] L.-M. Dogariu et al., “Tensor-based adaptive ﬁltering algorithms,” Sym-
metry, vol. 13, id 481 (27 pages), Mar. 2021.
[3] Y. V. Zakharov, G. P. White, and J. Liu, “Low-complexity RLS algo-
rithms using dichotomous coordinate descent iterations,” IEEE Trans.
Signal Processing, vol. 56, pp. 3150–3161, Jul. 2008.
12
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-858-7
SIGNAL 2021 : The Sixth International Conference on Advances in Signal, Image and Video Processing

