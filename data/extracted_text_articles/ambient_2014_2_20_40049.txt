Mobile Augmented Reality for Distributed Healthcare 
Point-of-View Sharing During Surgery 
Lauren Aquino Shluzas, Gabriel Aldaz,  
Joel Sadler, Shantanu Joshi, Larry Leifer 
Center for Design Research, Stanford University 
Stanford, California, USA 
{larenino, zamfir, jsadler, joshi4, larry.leifer}@stanford.edu 
David Pickham 
Office of Transdisciplinary Research 
Stanford Hospital & Clinics 
Stanford, California, USA 
dpickham@stanfordmed.org
 
 
Abstract—This 
research 
examines 
the 
capabilities 
and 
boundaries of a hands-free mobile augmented reality (AR) 
system for distributed healthcare. We use a developer version 
of the Google Glass head-mounted display to develop software 
applications to enable remote connectivity in the healthcare 
field; characterize system usage, data integration, and data 
visualization capabilities; and conduct a series of pilot studies 
involving medical scenarios. This paper presents the software 
development and experimental design for a pilot study that 
uses Glass for augmented point-of-view sharing during 
surgery. The intended impact of this research is to: (i) examine 
the use of technology for complex problem solving and clinical 
decision making within interdisciplinary healthcare teams; (ii) 
study the impact of enhanced visualization and auditory 
capabilities on team performance; and (iii) explore an AR 
system’s 
ability 
to 
influence 
behavior 
change 
in 
situations requiring acute decision-making through interaction 
between centralized experts and point-of-impact delivery 
personnel. 
Keywords—ambient; systems; distributed; healthcare; team-
based collaboration; head-mounted display, surgery; wearable 
computing. 
I. 
 INTRODUCTION  
In the healthcare field, the need for improved tools to 
enhance collaboration among patients and providers has 
become increasingly urgent – due, in large part, to a global 
rise in aging populations and chronic disease prevalence, 
coupled with increasing health care costs and physician 
shortages worldwide [1][2]. To address these challenges, this 
research examines the use of wearable mobile computing to 
mediate interdisciplinary communication and collaboration 
in healthcare. We first present an overview of collaborative 
technologies and interaction modalities for home-healthcare 
and hospital use.  These advances in mobile computing 
provide remote patient monitoring, automate simple 
knowledge-base procedures, and facilitate the delivery of 
interventions. We then present the experimental design for a 
pilot study that uses augmented point-of-view (POV) sharing 
during surgery through leveraging the camera and integrated 
sensors of Google Glass’ head mounted display (Figure 1).  
A. Remote Patient Monitoring 
For home healthcare applications, existing systems 
enable medical staff to remotely monitor patients suffering 
from advanced chronic disease and provide prompt support 
regarding health education and treatment compliance [3]. 
The Vsee video collaboration system (Sunnyvale, CA), for 
instance, simplifies patient-doctor interactions through web-
based video calling, coupled with medical device integration. 
 
Camera 
Photos – 5MP 
Videos – 720p 
High Resolution 
Display 
640x360 
Equivalent of 25in. Hi-def. 
screen from 8ft away 
Audio 
Bone Conduction 
Transducer 
Connectivity 
Wifi – 802.11 b/g 
Bluetooth 
Storage 
12 GB of usable 
memory, synced 
with Google cloud 
storage.  
16GB Flash total 
Battery 
1 full day of  
typical use 
Fit 
Adjustable nose 
pads and durable 
titanium frame 
 
Charger 
Includes Micro 
USB cable and 
charger 
Compatibility 
Any Bluetooth-capable phone. 
Enables GPS and SMS 
messaging. 
Touchpad 
Activates the 
display and selects 
actions 
Microphone 
Retina 
Fovea  
(sharp vision) 
Optic Nerve 
Prism 
Projector 
 
Figure 1. Google Glass (Mountain View, CA) wearable computer 
and head-mounted display features  
In the consumer health and wellbeing space, a variety of 
products provide self-patient monitoring to encourage 
behavior modifications aimed at promoting healthier 
lifestyles. These include activity monitors such as the Nike 
Fuelband and Fitbit Flex, and the LUMOback real-time 
posture feedback system. Yet, despite current advances, 
home monitoring technologies are often limited by the 
dependency on appropriate bandwidth, customized networks 
and high-cost equipment, as well as a lack of integration into 
electronic medical record (EMR) systems. 
In the hospital setting, remote monitoring systems, such 
as the tele-ICU, aid clinicians in the delivery of care to ICU 
patients. By collaborating with the bedside team, the tele-
ICU support care without distraction, while assisting in the 
delivery of timely interventions [4]. Yet, financial barriers 
associated with installing and operating such systems has 
limited widespread adoption. 
B. Automating Knowledge-based Procedures  
For patient-virtual agent interaction, the animated virtual 
nurse (VN) is an automated system that teaches patients their 
post-discharge self-care regimen directly from their hospital 
beds [5]. This system incorporates a VN who embodies best 
practices in health communication for patients with 
inadequate health literacy, and illustrates a growing field in 
mobile computing aimed at increasing universal healthcare 
access.  
34
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

 
Cognitive aids, such as dynamic checklists, present 
another example of tools to facilitate collaboration in the 
clinical setting, through automating knowledge-based 
procedures. A recent study involving dynamic checklists 
found that medical crisis care situations reveal “a physically 
complex information space, and relatively high-tempo time 
scales of gaze, action, and team-based coordination and 
communication” [6]. 
C. Facilitating the delivery of an intervention 
Collaborative technologies that facilitate the delivery of 
an intervention may include those in which (i) a clinician 
(expert) aids a non-expert in delivering an intervention, and 
(ii) a clinician delivers an intervention remotely through a 
robotic interface. With the commercialization of high-speed 
data networks, the implementation of these scenarios may be 
realized through the use of augmented reality (AR) systems. 
Many AR applications provide the benefit of visualizing 
three-dimensional data captured from non-invasive sensors, 
and range from remote 3D image analysis to advanced 
telesurgery [7][8]. 
Despite existing technologies, however, there is a 
growing need for new tools capable of augmenting a 
clinician’s knowledge base and his/her complex problem 
solving ability, while performing an intervention. Such 
augmentation 
can 
be 
accomplished 
if 
a 
system 
simultaneously connects the clinician (expert) to relevant 
medical databases, other experts, and a live telemetry of 
patients’ relevant vital statistics. The first order challenge 
required to accomplish these connections is the ability to 
manage the resulting high-bandwidth information flow 
between human and computer agents, and to enable agents to 
collectively work as a design team. 
In this paper, Section II describes the methods used to 
conduct this research study. In Section III, we provide an 
overview of point-of-view sharing during surgery, including 
software and hardware development, a usability assessment, 
and a proposed pilot study. Section IV provides a conclusion 
and discussion of future work.  
II. 
METHODS 
To address the challenges discussed above, this ongoing 
research effort examines the use of hands-free mobile AR for 
distributed healthcare collaboration.  We hypothesize that a 
mobile AR system (head-mounted display) will shorten 
communication cycles and reduce errors associated with 
point-of-care decision-making and distributed collaboration 
in healthcare scenarios. 
To test our hypothesis, we obtained four pairs of Google 
Glass as an initial platform for research.  Our methodology 
includes: (i) clinical needs finding to ground the study in the 
context of high-impact clinical problems; (ii) software 
development to create customized applications for the head-
mounted display that are specific to two or more clinical 
areas; and (iii) a series of pilot tests to characterize the AR 
system’s usage, data integration, and data visualization 
capabilities in medical scenarios.   
A. Clinical Needs Finding  
We conducted needs finding at Stanford Hospital and 
Clinics from October to December 2013. Two members of 
the research team interviewed ten nurses and two 
physicians, and shadowed four additional nurses. Each 
interview lasted approximately one hour, and nurses were 
shadowed from one to three hours at a time.  We recorded 
interviews using a digital recording device, and took hand-
written notes during each interview and clinical observation.    
From this process, we captured 135 clinical needs and 
grouped needs into 15 areas. To further narrow our research 
focus, we filtered needs based on degree of importance to 
the hospital, alignment with research interests, fit for Glass, 
and feasibility. We ordered our top three needs in order of 
increasing clinical risk.  These included: (i) wound and skin 
care photography, (ii) point-of-view sharing during surgery, 
and (iii) vital sign communication when patients are 
suffering from cardiac arrest.   
B. Initial Pilot Study  
We developed a Google Glass application to capture and 
document images for chronic wound photography, using the 
Android 4.0.4 (API 15) SDK and Glass Development Kit 
(GDK) add-on. The application leverages Glass’ camera, 
inertial measurement unit (IMU), infrared sensor, and 
microphone. The initial pilot study focused on the use of voice 
and gestural-based interaction commands for photo capture and 
documentation, and the transfer of annotated images to a 
patient’s EMR. The software development and pilot study 
for wound care management have been separately 
documented [9].  In this paper, we focus on the software 
application development and experimental design for a 
second pilot study to conduct point-of-view sharing in the 
operating room, using Google Glass.   
III. 
POINT-OF-VIEW SHARING DURING SURGERY 
A range of studies has demonstrated the use of head-
mounted cameras for clinical use and education. Bizzotto et 
al. [10], for instance, used the GoPro HERO3 in 
percutaneous, minimally invasive, and open surgeries with   
high image quality and resolution. Several reports document 
the live two-way broadcasts of patient surgeries by doctors 
wearing Glass in the operating room [11][12].  
 
 
Figure 2. Center for Immersive and Simulation-based Learning 
35
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

 
To build on existing work in this field, we plan to 
conduct a pilot study within Stanford’s Center for 
Immersive and Simulation-based Learning (CISL), as 
shown in Figure 2. For this study, we will simulate a 
medical scenario using augmented POV sharing for the 
treatment of an acute condition, in which relevant patient 
data is superimposed on the Glass display, within the 
surgeon’s field of view. The simulation will focus on 
complex problem solving with multiple potential solution 
paths (e.g., situations in which there is no optimal, context-
independent solution). 
A. Software Development and Hardware Enhancement 
The development of a software application for POV 
sharing during surgery focuses on three key areas [13]: 
voice commands, bi-directional communication, and EMR 
data transfer.  We intend to use voice commands to control 
the application in a hands-free manner, in order for 
clinicians to maintain heightened sterility while performing 
surgical procedures.  Bi-directional communication is 
required to enable collaboration between surgeons wearing 
Glass and remote colleagues, and to establish connectivity 
with remote sensors in the healthcare environment.  We are 
in the process of developing a communication interface 
between the wearable computer and remote sensors using 
WiFi and Bluetooth.  Using an Arduino microprocessor as a 
proxy for medical sensors, we have demonstrated that 
wireless connectivity via WiFi may be achieved by 
connecting Glass to remote sensors through Android’s WiFi 
Peer-to-Peer (P2P) API.  For Glass, this may also be 
achieved through the Google Mirror API, as an 
intermediary.  For Bluetooth connectivity, we demonstrated 
the ability to connect Glass with a remote sensor (Arduino 
microprocessor), pair the two devices, and transfer data 
between them.  With Glass, the connection may be launched 
manually using the device’s touch pad, physical gestures, or 
voice commands.  We aim to apply the connectivity 
protocols, demonstrated with an Arduino, towards capturing 
the actual vital signs of patients from monitoring systems in 
the hospital (e.g., the Phillips Intellivue Solution System).  
We also intend to build on existing work in wound care 
photography to wirelessly transfer surgical image and video 
data to a patient’s EMR [9].   
  Based on feedback from consulting surgeons, in 
addition to software development, we aim to physically 
enhance the Glass hardware for surgery in three ways.  We 
intend to add a transparent splash shield that surgeons may 
adhere to the front frame of the Glass display for protection 
from infectious disease, attach an optical loupe to the frame 
(in front of a surgeon’s left eye) in order to increase 
magnification for surgical procedures, and encase elements 
of the head-mounted display in a protective cover for 
improved cleaning and robustness during routine clinical 
use.  
B. Initial Feasibility and Usage Assessment 
Following 
software 
development 
and 
hardware 
enhancements, we will conduct an initial assessment of the 
AR system’s ability to (i) connect with remote study 
participants, (ii) capture a patient’s sensory data, and (iii) 
transmit information to/from a hypothetical EMR.  We 
intend to evaluate the system’s usage characteristics with 
10-15 individuals as they wear the Glass technology while 
performing a series of manual tasks.  We will evaluate each 
user’s ability to: (i) use and navigate the system through 
voice and tactile commands, (ii) integrate multiple audio 
and visual data inputs, and (iii) transfer data.   We will 
assess the time required for each user to complete a 
specified task, and the associated error rate.  
C. Pilot Study 
1) Experimental Set-up:  In a pilot study at Stanford’s 
CISL facility, an attending surgeon will wear the Glass 
head-mounted display, while conducting a hypothetical 
procedure in collaboration with one assisting surgeon and 
one nurse.  The local medical team will be in direct 
communication with a remote team of collaborators (in 
Germany and the Netherlands) via the Glass interface.  The 
team will be presented with a hypothetical medical scenario 
and asked to develop a solution in 20 minutes using 
standard operating room supplies.  A ‘patient’, e.g., 
computer driven mannequin that replicates physiologic 
parameters (pulse, heart and breath sounds, blood pressure, 
etc.) – will be used in the simulation. The patient’s sensory 
data will transmit to the remote team and to a hypothetical 
EMR, until the patient’s desired health state is achieved 
during the simulation.  Using voice commands, the 
attending surgeon will be able to switch views in the Glass 
display between EMR data and wound images with 
information overlays. The images within the Glass display 
will be projected on an adjacent wall, so that the assisting 
surgeons and nurse may see the procedure from the 
attending surgeon’s point-of-view.  A schematic illustration 
of the experimental set-up is shown in Figure 3.   
 
Projected View of EMR 
Doe, John 
MRN: 013465 
DOB: 02.13.41 
Picture Taken 
04.12.14 
11:35:10 
 
Projected View of 
Surgical Site   
Electronic Medical 
Record (EMR) 
HMD 
display 
Audio/Video sharing 
with Remote 
Collaborators: 
Trainees, Pathology, 
other Surgeons 
 
Figure 3. Point-of-View Sharing with Remote Collaborators  
We will repeat the simulation three times with different 
medical teams. Local study participants will include 
registered nurses and medical residents from Stanford 
Hospital & Clinics.  
2) Performance Measures:  We plan to assess team 
performance using the AR system based on a combination 
of qualitative and quantitative measures. The independent 
36
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

 
variables, based on a Likert 5-point scale, include: X1 – 
degree of team collaboration; X2 – degree of task 
coordination (e.g., the ability to co-locate activity with 
visual instruction, and switch between data inputs and 
verbal communication modes); and X3 – the degree of visual 
and auditory human perforance augmentation.   The 
dependent variables include: Y1 – the time for task 
completion and Y2 the error rate in completing each task.  
Secondary measures include (i) the most/least effective 
mode to enhance feedback capability; (ii) the most/least 
effective data visualization display method; and (iii) each 
participant’s percevied degree of user behavior change, 
based on the test conditions.  Scores will be ascertained 
through a survey administered to study participants, field 
observations conducted by the research team, and post-
experiment interviews with study participants.   
3) Data Analysis Methods:  We will calculate the mean 
time and survey scores for the three simulations.  
Multivariate statistical methods (e.g., ANOVA) will be used 
to determine if correlational relationships exist between the 
independent and dependent variables.  Qualitative data, 
including field observations and interviews, will be 
transcribed, coded, and analyzed using NVivo qualitative 
analysis software. 
IV. 
CONCLUSION AND FUTURE WORK 
Augmented point-of-view sharing during operating room 
procedures advances the state-of-the-art in surgery through 
enhancing a clinician’s knowledge base and decision-
making 
capabilities, 
while 
performing 
surgical 
interventions. Specifically, since several individuals are 
needed to perform a surgical procedure (e.g., the attending 
surgeon, assisting surgeons, an anesthesiologist, and 
multiple nurses), the incision site is frequently crowded and 
obstructed from one’s field-of-view.  With augmented POV 
sharing, images projected from an attending surgeon’s 
vantage point onto a remote screen provide visual clarity to 
individuals directly involved in the surgery, as well as 
remote participants and expert advisors.   It also helps 
trainees and surgical fellows to learn procedures more 
accurately, by viewing an intervention from the same 
perspective as an attending surgeon, rather than from the 
reverse perspective (e.g., standing on the opposite side of 
the table) - which is often the case today. Finally, the use of 
POV sharing with bi-directional communication capabilities 
and 
remote 
sensor 
connectivity 
enables 
real-time 
collaboration with a pathology lab or expert consultants 
while performing a surgical intervention.  Through visual 
overlays, POV sharing can co-locate one’s visual field with 
information critical to performing a procedure – such as 
vital sign information, procedural descriptions, or MRI 
scans.  
In the future, we aim to further enhance the robustness 
and reliability of a mobile AR system for acute care 
scenarios in the operating room and emergency room 
settings.  This involves developing applications with data 
security and privacy features that are in compliance with 
strict hospital security protocols.  
In the broader context of distributed collaboration for 
improved healthcare delivery, this research aims to examine 
the use of technology for complex distributed problem 
solving through interdisciplinary collaboration; gain an 
improved understanding of the benefits of human 
augmentation through enhanced visualization and auditory 
capabilities, on healthcare team performance; and explore 
an AR system’s ability to influence behavior change in 
situations requiring 
acute 
decision-making 
through 
interaction between centralized experts and point-of-impact 
delivery personnel. 
ACKNOWLEDGMENT 
We gratefully acknowledge the Hasso Plattner Design 
Thinking Research Program (HPDTRP) for their support of 
this work. We thank the clinical staff at Stanford Hospital & 
Clinics for their time and participation in this research study.   
REFERENCES 
[1] S. Mattke et al., Health and Well-Being in the Home: A 
Global Analysis of Needs, Expectations, and Priorities for 
Home Health Care Technology. Santa Monica, CA: RAND 
Corporation, Dec. 2010, pp. 1-39. 
[2] L. Magnusson, E. Hanson, and M. Borg, “A literature review 
study of information and communication technology as a 
support for frail older people living at home and their family 
carers,” Technology and Disability 16, Dec. 2004, pp. 223–
235. 
[3] L. C. Baker, S. J. Johnson, D. Macaulay, and H. Birnbaum, 
“Integrated telehealth and care management program for 
Medicare beneficiaries with chronic disease linked to 
savings,” Health Affairs 30, 9, Sept. 2011, pp.1689-1697.  
[4] S. F. Goran, “A second set of eyes: An introduction to Tele-
ICU,” Critical Care Nurse 30, 4, Aug. 2010, pp. 46-55.  
[5] T. W. Bickmore, L. M. Pfiefer, and B. W. Jack, “Taking time 
to care: empowering low health literacy hospital patients with 
virtual nurse agents,” CHI 2009, ACM Press, Apr. 2009, pp. 
1265-74, ISBN: 978-1-60558-246-7. 
[6] L. Wu et al., “Interactive cognitive aids:  maintaining shared 
mental models in anesthesia crisis care with nurse tablet input 
and large-screen displays,”  UIST’11, ACM press, Oct. 2011, 
pp. 71-72, ISBN: 978-1-4503-1014-7. 
[7] D. W. F. van Krevelan and R. Poelman, “A survey of 
augmented 
reality 
technologies, 
 
applications, 
and 
limitations,”  The International Journal of Virtual reality 9,2, 
Jan. 2010, pp. 1-20. 
[8] F. Zhou, H. B. L. Duh, and M. Billinghurst, “Trends in 
augmented reality tracking, interaction and display: a review 
of ten years of ISMAR,” Proc. 7th IEEE/ACM International 
Symposium on Mixed and Augmented Reality, (ISMR 2008), 
Sept. 2008, pp. 193-202. doi: 10.1109/ISMAR.2008.4637362. 
[9] G. Aldaz et al., “SnapCap: Improved hands-free image 
capture, tagging, and transfer using Google Glass,” 
unpublished.  
[10] N. Bizzotto, A. Sandri, F. Lavini, C. Dall’Oca, and D. Regis, 
“Video in operating room: GoPro HERO3 camera on 
surgeon’s head to film operations—a test,” Surgical 
Innovation, Dec. 2013, Published online before print, doi: 
10.1177/1553350613513514.  [retrieved: June, 2014] 
[11] B. J. Lutz, and N. Kwan, “Chicago surgeon to use Google 
Glass in operating room,” Dec. 2013. [Online]. Available 
37
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

 
from: 
http://www.nbcchicago.com/news/tech/google-glass-
surgery-237305531.html. [retrieved: June, 2014] 
[12] Medical Xpress. First US surgery transmitted live via Google 
Glass (with video). Oct. 2013. [Online]. Available from: 
http://medicalxpress.com/news/2013-08-surgery-transmitted-
google-glass-video.html. [retrieved: June, 2014] 
[13] L. Shluzas et al. A wearable computer with a head mounted 
display for hands-free image capture and bi-directional 
communication with external sensors. US Provisional Patent 
No. 61968857, Mar. 2014. 
 
38
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

