Mobile Robot Localisation and Terrain-Aware Path Guidance 
for Teleoperation in Virtual and Real Space 
 
Ray Jarvis  
Intelligent Robotics Research Centre  
Monash University 
Victoria, Australia 
ray.jarvis@monash.edu 
 
 
Abstract–This paper concerns the development of a 
force feedback enhanced teleoperation system for 
outdoor robotic vehicles navigating in rough terrain 
where true-colour 3D virtual world models of the 
working environment, created from laser and colour 
image scans collected offline, can be explored by walk-
throughs both before and during the robot navigation 
mission itself. In other words, the physical mission 
intended can be partially rehearsed in cyberspace[1]. 
Further, during a mission, the location and orientation 
(localisation) of the vehicle are continually determined 
and global collision-free paths to selected goal 
locations made available as advice to the operator, who 
can follow or ignore such advice at will. Live (real-
time) 3D laser range data also provides an up-to-date 
scan of the volume immediately surrounding the 
vehicle as it moves so that dynamic obstacles can be 
avoided. Local terrain-roughness is taken into account 
in the provision of local collision-free paths, the sub-
goals of which are operator determined. This live 
range data is matched with the pre-scanned range data 
to calculate the accurate robot vehicle localisation 
(position 
and 
orientation) 
which 
is 
provided 
continuously during the navigation mission. A force 
feedback 3D joystick reflects terrain roughness as a 
vibration in one axis and the other two axes are used to 
provide a 2D force to attract the operator towards 
following the local optimal collision-free path, but this 
attraction can be easily overridden by the operator. 
The instrumentation and methodologies used for 
localisation, 
path 
planning, 
force 
feedback 
teleoperation and 3D exploration are presented, 
together with some preliminary experimental results 
for large outdoor, natural environments. 
 
Keywords-Human/Machine 
Interaction, 
Teleoperation, 
Localisation, 
Cyberspace, 
Robot 
Navigation, Rough Terrain, Force Feedback. 
 
 
 
 
 
 
 
I. 
INTRODUCTION 
 
In the realm of mobile robot navigation, the 
research community has long held fully autonomous 
operation as the ultimate goal.  Yet, in many 
practical situations, this is not currently possible and, 
in some, not really justifiable or even sought after.  
Two examples where fully autonomous robot 
navigation is ether not sought for or infeasible are 
provided as follows: 
A severely disabled patient may be reliant on 
wheelchair navigation for his/her mobility needs [2]. 
Whilst providing sensor-based obstacle avoidance 
and safe-path guidance may contribute to the user's 
capacity to better engage the world of mobility, fully 
automating the process would impinge upon that 
person's freedom and also cause some reduction of 
capacities supporting independence still held to be of 
value in a quality of life sense.  The second example 
could be in a bush fire fighting situation [3], where 
an operator is available to provide human judgement 
and mission sub-goals but should not be in risk of 
physical injury or death.  Sensor informed feedback 
based teleoperation would suit that situation well.  
Again, some navigation support would be welcomed 
but full automation not really required (nor currently 
feasible). 
This paper concerns remote teleportation of 
robotic vehicles, possibly in fire- fighting or search 
and rescue operations in outdoor rough terrain 
situations, with sensory feedback and path guidance 
support.  The manner in which the human agency 
interacts with the system and interprets newly 
developing situations is considered critical to the 
quality of the navigation in the context of higher 
level mission goals. 
Robot navigation systems have three essential 
components and several more peripheral ones.  
Firstly, the location and orientation (pose) of the 
robot vehicle needs to be known in the context of its 
current working environment.  This is known as 
174
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

'localisation'.  This can be geometrical or topological 
in nature and may depend on the recognition of man-
made or natural landmarks. Various instruments such 
as global positioning systems (GPS), flux-gate 
compasses, wheel odometry, video cameras, laser 
range finders and inertial systems can be employed 
for this.  The second requirement is the availability 
of a map of the working terrain or the means of 
acquiring one whilst navigating.  In recent times, 
considerable research effort has been expended on 
simultaneously localising the robot and developing 
an 
environmental 
map 
(SLAM-Simultaneous 
Localisation and Mapping).  There are a number of 
difficulties using SLAM in the context of the 
application considered in this paper.  These will be 
touched upon later.  The third requirement is 
collision-free, low risk and somewhat optimal path 
planning.  Ideally the terrain properties, including 
roughness as well as obstacle structures, should be 
taken into account by the path planning strategy. 
In the SLAM approach [4,5], the environmental 
map takes quite some operational time to construct 
and optimal path planning cannot take place before 
the completion of the map, although piece-wise 
optimal strategies can be implemented within the 
context of partially known environmental spaces.  
There are also some problems with reliably 
recognising closures (places revisited) to distribute 
accumulated errors optimally. 
In this paper, an alternative approach has been 
adopted -that of acquiring, off- line, a detailed and 
accurate environmental map before, perhaps one of 
many, robot navigation missions are executed.  It is 
admitted that this may not be always practicable but, 
for many situations, the collecting of the map data 
can be treated like any other preparation step in 
anticipation of a crisis scenario which may eventuate 
later.  Clearly, for urban environments which could 
be subject to natural disasters like fires, floods and 
earthquakes, this precaution is very reasonable.  In 
bushland settings near homesteads this could also be 
seen as feasible.  Even entire farms with forest stands 
subject to fire risk could be pre-scanned in this way. 
Scanning instruments with quite large operational 
volumes are currently available. These are somewhat 
expensive, but one could imagine a bureau service 
providing the scan data for an affordable fee and 
even insurance companies reducing premiums for 
clients who have obtained this data. Besides, this 
technology will become less expensive with time. 
The remainder of this paper is structured as 
follows.  The next section describes, briefly, a 
number of outdoor vehicles instrumented for 
teleoperation as part of a research effort supporting 
bushfire fighting.  Any one of them could be 
operated using the navigation system which is the 
subject of this paper.  Next, the instrumentation, both 
for off-line mapping of the environment and the on-
board real-time laser range scanning, which are 
crucial for this work, is described. Then, a section on 
localisation and path planning using the results of 
scanning follows. The whole navigation system with 
force feedback for assisted teleoperation is then 
introduced.  Discussion and future work follows 
prior to the conclusions section. 
 
II 
ROBOTIC VEHICLES 
Figure 1 shows a number of standard (commercially 
available) vehicles which have been instrumented for 
teleoperation as part of a research project to support 
bush fire fighting, where the local Country Fire 
Authority (Victoria, Australia) was the industry 
partner.  The variety of vehicles represents a number 
of different, yet related, activities supporting bush 
fire fighting.  A four wheel drive farm 'bike' fitted 
with tracks [Figure 1(a)] is capable of climbing over 
fallen tree trunks up to 40cm thick and has been 
targeted mainly for forward scout forays to assess the 
severity and access possibilities along fire-break 
tracks prior to fire fighting itself [6].  It can also be 
used for very rough terrain search and rescue for 
firemen and property owners who may have became 
asphyxiated or have suffered smoke blindness.  The 
heavy tracks are extremely difficult to steer and a 
powerful chain linked hydraulic ram system has been 
employed for changing the steering direction of the 
front tracks.  Steering, braking and acceleration can 
all be operated by remote control via standard 
175
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

'hobby' style servo actuators and a radio control 
transmitter/receiver 
pair 
or, 
alternatively, 
by 
computer Ethernet links to serial line servers which 
can operate the servo actuators.  An excavator 
[Figure 1(b)] and a front loader [Figure 1(c)] are also 
teleoperable and are targeted for fire-break track 
clearing and smoothing for fire tanker access [7].  In 
both cases, in addition to mobility controls (steering, 
brake, accelerator), the buckets can also be 
teleoperated.  Figure 1(d) shows a 40 foot boom 
truck which can used both for search and rescue, 
with high vantage point views, and the capability of 
lifting a human up from behind a wall of fire and for 
directing a steam of water from the boom bucket [8].  
Finally [Figure 1(e)] there is a fire tanker [2] which 
can have 3000 litres of water and spray it at selected 
directions using a pan/tilt device aiming the water 
flow [Figure 1(f)]. 
Whilst all the above vehicles can be fitted with 
video and infra-red video cameras and laser range 
finders to assist teleoperation, the particular laser 
range finder instruments described next are the 
specific devices which support the main emphasis of 
this paper. 
 
 
Fig. 1(a). Four-Wheel Drive Farm  
 
 
Fig. 1(b) Excavator 
 
 
Fig. 1(c) Front Loader 
 
 
Fig. 1(d) 40 foot Boom Truck 
 
176
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
Figure 1(e) Fire Tanker 
 
Fig. 1(f) Water Spray Monitor 
 
FIG. 1. ROBOTIC VEHICLES 
 
 
 
 
III. 
CRITICAL LASER RANGE FINDER SCANNERS 
Two distinct laser range finder instruments are 
crucial in their support of this research.  The first 
collects pre-mission environmental data (range and 
colour) to build an accurate 3D cyberspace of the 
working environment and the second collects real-
time range data during the navigation mission itself. 
A Riegl LMS-Z420i [see Figure 2(a)] is an 
accurate time-of-flight laser range finder which can 
be fitted with a high resolution digital camera whose 
image data can be registered with the range data.  
This instrument can range up to 800 metres with an 
accuracy of 1cm, collecting range values at up to a 
11,000 samples per second rate.  A typical medium 
density scan from a fixed position takes between 15 
and 60 minutes, depending upon the settings used.  
Since not all aspects of a 3D scene are viewable from 
only one location, several fairly open locations are 
chosen for individual scans and these are later fused 
together 
under 
human 
supervision 
with 
computational support.  These separate scans should 
overlap to allow accurate registration during 
integration.  A two metre diameter 'dead zone' exists 
around the instrument since, up to this distance, the 
return timing is too short for the instrument to record 
correctly.  A typical view of a scanned space is 
shown in Figure 2(b). 
The second laser range finder is a Velodyne 
HDL-64E S2 [see Figure 3(a)] which spins at a rate 
of 5-15 Hz to collect range data up to 120 metres 
away (dependant upon the target surface albedo) at a 
date rate of up to 1.8 million samples/second at an 
accuracy of 2cm. 
The Velodyne contains 64 independent laser 
sources and sweeps 64 live scans around the axis of 
rotation, collecting data from +2° to -24.8° in 
elevation.  When mounted high on a vehicle it allows 
the volume that vehicle can move through to be 
analysed for obstacles and also permits the terrain 
undulations and holes to be analysed. A typical scan 
is shown in Figure 3(b). 
 
Fig. 2(a) Riegl Scanner 
 
177
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
Fig. 2(b) Typical Riegl Indoor Scan 
 
 
Fig. 3(a) Velodyne Range Scanner 
 
 
Fig. 3(b) Velodyne Outdoor Scan Example 
 
Both instruments can be powered by standard 
12Volt batteries and are connected to the controlling 
computer via Ethernet, but with the digital camera 
requiring a USB port. 
The range and colour imagery collected by the 
Riegl scanner and attached digital camera at various 
locations and subsequently combined, can be 
explored as a virtual world, moving, through it at 
ground level or from a 'fly over' elevated view.  This 
exploration 
can 
be 
used 
for 
pre-mission 
familiarisation and for noting specific aspects such 
as the location of dwellings or water sources, fences, 
gates etc. which may assist in the mission itself.  It 
can also be used to make judgements on tolerances 
for obstacle avoidance which should be used during 
the mission and where grass and bush may be 
navigable despite perhaps being regarded as obstacle 
space because of its height. 
 
IV. 
LOCALISATION AND PATH PLANNING 
The knowledge of the pose (position and 
orientation) of the robotic vehicle is an important 
requirement 
for 
efficient 
path 
planning 
and 
following, even if it were not strictly necessary for 
teleoperating a vehicle using on-board sensors alone 
(eg. cameras and range finders). 
A data-base of range 'signatures' is first extracted 
from height thresholded (between 0.5 and 1.0 
metres) range data from the Riegl scanner at 
intervals over a 0.1 metre grid over the working 
environment, associating the range to obstacles of 
180 radial rays at 2° intervals around the 360° 
sweep, with each ray length larger than 50 metres  
marked as 0 (keeping only values clearly within the 
range scope of the Velodyne).  This data-base is 
constructed off-line so its computational time cost is 
not crucial. Some local averaging is done to smooth 
the data to enable better spatial matching tolerances. 
In real-time, whilst the robotic vehicle is navigating, 
a similarly constructed 'signature' from height 
thresholded Velodyne range data and matched 
through searching the 'signatures' in the data base is 
used to determine the pose of the vehicle.  A rough 
match is followed by a more refined one to improve 
the efficiency of the method.  The robot vehicle can 
be localised, typically, within ~15cm of its actual 
178
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

location and ~1 degree of its actual orientation at the 
rate of 0.35 seconds per fix using a fast Intel i7 2.67 
GHz processor with 6 Gb of RAM.  Continuity 
constraints are used to limit the search requirements 
once the vehicle is initially localised, a complete 
initial search taking a number of seconds.  
The simple matching formula used is as follows: 
Given two ‘signatures’, one extracted from the 
current Velodyne range scan and one selected from 
the Riegl pre-scanned data base, S1 and S2, 
respectively, each with 180 range components. 
S=Sum[exp(-Abs(S[i]-S2[i])**2/(2*Sig)] 
over 
i=1 to 180 where Sig is a experimentally selected 
standard deviation and Abs the absolute value 
operator.  This produces a Gaussian weighted 
measure which downplays badly matching range 
rays. 
Then X= w+b*S where w and b are 
experimentally determined parameters.  The final 
score is calculated by Score= 1/(1+exp (-X)) which 
is between 0.0 and 1.0. The larger the score, the 
better the match. Further details can be found in [9]. 
Clearly, 
more 
sophisticated 
matching 
techniques can be developed but this first approach 
was found adequate for our purpose, since the terrain 
we used in our initial experiments was reasonably 
planar.  The pose data (position plus orientation) is 
exported continuously to a text file for the path 
planner to access when necessary, the most recent 
information overwriting the previous pose data. 
Figure 4 shows a coherent sequence of localisation 
traces (in real space with a physical vehicle) with the 
current location for each ‘screen shot’ showing the 
Velodyne range rays which were matched against the 
pre-scanned 
Riegl 
data 
to 
determine 
the 
location/orientation of the vehicle. The view of the 
cyberspace model obtainable from that point is also 
shown from ground level. One can identify 
correspondences between objects in that view and 
some structures in the plan map showing the 
localisation point. The vehicle is approaching a 
shack with a fire tanker (red) looming larger. The 
smoothness and continuity of the traces is clearly 
impressive and indicates a very high confidence in 
the reliability and accuracy of the methodology used. 
 
 
 
 
 
Fig. 4 Sequence of Localisation Traces and Virtual Reality Viewpoints 
for an Actual Physical Experiment. 
 
179
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

A number of path planning methodologies have 
been published [10,11,12].  Many treat the search 
space as a Euclidean geometry domain made up of 
points and lines with polygonally enclosed obstacle 
spaces.  Details can be found elsewhere [10].  An 
alternative approach is grid based, where the search 
space 
is 
made up 
of 
tessellated 
(generally 
rectangularly) cells which are either occupied by 
obstacle or not (free).  A path in such a space is a 
sequence connected free cells form a start cell to a 
goal cell.  The computational burden of such 
methodologies is highly related to the resolution 
chosen for the environment space representation.  A 
big advantage of the grid cell based approach is that, 
in addition to occupancy or not of obstacles, other 
cost structures can be represented in the cellular 
structure so that properties such as visibility or 
terrain roughness etc. can be accommodated in the 
path optimality calculations.  One can even include 
tolerance costs in relation to the proximity of 
obstacles so as to allow the robot to stray off its path 
to some extent without collision. 
A Distance Transform (DT) path planning 
strategy was used in this study as it has a number of 
advantages which suited the needs of the project [13] 
despite there being more recent and complex 
alternatives. 
It 
is 
simple 
to 
compute, 
can 
accommodate costs over the cell structure, including 
collision risk tolerance and probabilistic structures 
and can easily be extended into time/space for both 
deterministic and probabilistically estimated cost 
structures projected into the future.  It can include 
multiple goals and provides an optimal path from 
any cell in free space to the least cost acquirable goal 
simply by following a steepest descent trajectory in 
the DT space.  This last properly is particularly 
useful, since, if the robotic vehicle is driven off the 
currently mapped out path, a new optimal path from 
its new position is instantly available using a new 
steepest decent trajectory in the already calculated 
DT space. The details of the DT method can be 
found elsewhere [13] but an outline is provided here 
for completeness and for better being able to explain 
the path-guided teleoperation approach which is 
described later. 
First consider the simple case of an initially 
rectangularly tessellated N x N cell space with free 
cells marked '0' and obstacle cells marked '1' with 
only one goal. 
 
1.   Leave the goal as '0', putting a large number in 
all other free-cells (say >   N2) and mark the obstacle 
cells with computer infinity (say   232 1). 
2.   In raster order (left to right, top to bottom, one 
step at a time), skipping over obstacle cells, replace 
the free cell value with the least value (cost) of 
recently visited neighbours (3 x 3 region) plus 1 
(assuming that costs from entering the cell from any 
of its neighbours to be identical).  In fact only 4 
comparisons are needed (three cells in the previous 
line and the one to the left) but all can be used 
without error.  The goal cell should not be altered as 
it is zero cost from itself. 
3.   In reverse raster order (right to left, bottom to 
top, one step at a time) repeat the operation described 
in 2.  Now only the cells in the line below and to the 
right need to be looked at. 
4.   Repeat 2 & 3, above, alternatively until no 
further changes occur. 
5.   The resulting map is the Distance Transform and 
a steepest descent trajectory from any free-cell will 
lead to the goal with the least number of steps 
Some border conditions need to be set so the 
rasters, are usually carried out over a (N-1) x (N-1) 
grid. 
A simple example of a DT result is shown in 
Figure 5(a).  If the cost of a diagonal move is 
preferred to be  2  compared to a up/down or 
left/right cost of 1, the approximation of a weight of 
3 for diagonal moves and 2 for the others can be 
used [see Figure 5(b)].  In fact, 4:3 is even better and 
17:12 almost perfect.  In this case the candidate cell 
value is replaced by the least value of the sum of its 
neighbour's cost plus the cost of entering the cell 
from that neighbour.  If costs are to reflect distances 
as well as roughness, tolerance or probabilities, the 
180
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

same process can be used, as long as all costs are 
non-negative.  No local entrapment occurs using this 
strategy and the paths formed by steepest descent 
trajectories are truly global at all times. Only 
unreachable cells (enclosed by obstacle cells) are 
indeterminate. 
 
Fig. 5(a)  Simple DT Result 
 
Fig. 5(b).  Raster Ordering and Calculation 
 
 
Fig. 5(c)  Grown Obstacle Field 
 
A particularly elegant way of 'growing' 
obstacles to increase collision-free tolerance and/or 
to allow for the physical dimensions of the robotic 
vehicle, is to initially treat all obstacle cells as 
pseudo goals (set to 0) and carry out the DT 
computation which leaves all free-cells with values 
equal to their distance from their nearest obstacle 
cell.  Returning all values larger than a set threshold 
(say equivalent approximately to the radius enclosing 
circle of the vehicle, or more) to free-cell status are 
marking the remainder as obstacles will achieve the 
desired obstacle growth automatically [see Figure 
5(c).  Furthermore, the absolute difference of the 
value of cells (other than those set as obstacle cells  
after the DT process) from the maximum value over 
all non-obstacle cells can replace the cell value as a 
risk of collision cost which can be incorporated into 
the path planning process. The local maxima of the 
DT field provides a digital version of a Voroni 
construction and can represent safe 'roadways' 
through obstacle space. A more complex DT 
example is shown in Figure 5(d), showing the global 
qualities of the methodology. 
 
181
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
Fig. 5(d) More Complex DT Example 
 
Two levels of path planning using the DT 
methodology are used in this project, one applied to 
the obstacle field data from the integrated Riegl 
scans one for the local Velodyne live obstacle field 
data. For the Riegl data a path from the current 
location to a nominated goal is calculated. As the 
position of the robot vehicle is changed a new path is 
calculated. Note that the DT need only be calculated 
only once for each new goal specification in this 
case. The goal point can be changed at any time, the 
DT being  recalculated when required or simply 
continuously to avoid checking the goal change 
status. For the Velodyne obstacle field data case, the 
DT is always continuously recalculated (whether or 
not the goal status has changed),since dynamic 
obstacles may appear and , in any case, the robotic 
vehicle is moving. 
For this project, given that the raw Velodyne 3D 
range data provides terrain height data, a roughness 
factor was be calculated at each free cell location 
based on the sum of absolute height differences from 
the candidate cell to each of its eight neighbours and 
this sum was weighted into the cost of entering a free 
cell, with 3:2 distance component included as well. 
All obstacles were grown by a nominated number of 
cells beforehand as described earlier. 
 
 
V. 
TELEOPERATIONAL NAVIGATION SYSTEM 
WITH FORCE FEEDBACK CONTROL 
 
Figure 6 gives the block schematic for the 
whole teleoperational navigation system.  The off-
line Riegl data collection and localisation 'signature' 
data-base is entirely fixed and calculated prior to 
mission time.  The environment Virtual Reality (3D 
plus colour) model [see Figure 7(a)] can be explored 
in detail at any time either before or during physical 
navigation.  One may 'walk through' this virtual 
space at ground level or from any elevated 
viewpoint.  During navigation one can either explore 
at will or use the localisation fixes provided by the 
system to position the viewpoint (elevation can also 
be changed independently).  Live data from the 
Velodyne range scanner scan data, provided at 10 Hz 
rotation speed, is matched against the Riegl data-
base (signature matching) to provide the current 
robotic vehicle pose.  The local environment obstacle 
map derived from Velodyne range data is centred on 
this localisation fix with the vehicle direction of 
orientation always up on this map. The live 3D 
Velodyne data can be viewed simultaneously from a 
variable orientation view point and zoom. 
182
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
Fig. 6. System Schematic 
 
A global environment obstacle map is also 
provided [see Figure 7(b)].  The global goal can be 
selected via a text file or using the computer mouse.  
The current localisation position defines the start 
point of optimal path trajectories to the goal (or least 
cost goal if there is more than one goal).  The 
optimal (shortest) path shown on the global map is 
for grown static obstacle avoidance  alone.  
 
 
Fig. 7(a)  3D Virtual Reality Model 
 
 
Fig. 7(b) Global Collision-Free Path Planning 
 
The local obstacle/terrain roughness map shows 
live data updated from Velodyne data quite rapidly 
(e.g. at 0.5 second intervals).  The local path 
trajectory (using a DT which accommodates distance 
as well as terrain roughness after growing obstacles a 
specified amount) is for advice to the operator with 
the centre of the map representing the current robotic 
vehicle location [see Figure 8] and the local goal 
selected using a computer mouse. In Figure 8, two 
different goal positions are selected; for the second 
image, it can be clearly seen how rough terrain is 
avoided at the cost of a longer path. The operator is 
free to choose a local goal consistent with the global 
path trajectory shown in the global display but can 
select any local position if variations to check 
environment details are preferred. It would even be 
possible (but has not yet been done) to make the 
local goal some number of steps forward along the 
globally determined optimal path as a default.  Even 
when the local optimal path trajectory map reflects 
the operator's local goal selection the operator is free 
to ignore it.  Given that the Velodyne 3D range data 
is live, any dynamic obstacle will be taken into 
account in the local path trajectory (but can not be so 
accommodated in the global fixed data unless the 
183
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Velodyne data is made to temporarily overwrite the 
Riegl data which has not been done, so far). 
Now this is where the 3D Falcon force feedback 
joystick 
[see 
Figure 
9] 
comes 
in. 
 
The 
horizontal/vertical movements of the joy stick 
control the driving of the robot vehicle (off the 
planned path if so desired) but force is applied to the 
joy stick to pull it back towards following the local 
planned path. However, each excursion away for the 
path defines the starting point for a new path so the 
force field is continually changing.  Lightly holding 
the joystick allows the vehicle control to be 
consistent with the local planned path.  Also, the 
third degree of freedom of the joystick (in our case in 
and out) is vibrated by a magnitude proportional to 
the terrain roughness factor calculated as described 
earlier so that driving over rougher terrain can 
certainly be felt by the operator. Only full field trials 
(not yet carried out) will determine how best to 
provide the force controls described above. It may 
prove necessary to provide some smoothing filters in 
the force feedback loop to reduce overshooting 
jerkiness.  It would be hoped that the path preference 
and terrain roughness force feedbacks will give an 
intuitive feel to the operator and also effective 
navigation naturally without stress.  
 
 
 
 
Fig. 8  Local Terrain Roughness Aware Collision-Free Path Planning 
 
 
Fig. 9   Novint Falcon 3D Force-Feedback Joystick 
 
VI 
DISCUSSION AND FUTURE WORK 
 
The three central elements of the work 
described here are as follows: 
1.   The Riegl range and image data is used to build a 
virtual world [14] of the robotic vehicle's work space 
and provides the 'signature' data-base for localising 
the vehicle by scan matching.  The Virtual Reality 
world can be explored in detail at ground level or an 
elevated position either before or during navigation 
(in this latter case the current position and orientation 
184
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

can be used for viewpoint determination if so 
wished). 
2.  The Velodyne real-time 3D range not only 
provides 'signature' data for run-time localisation by 
scan matching against the Riegl 'signature' data-base 
but also provides dynamic local data on obstacles, 
ruts, moving objects, terrain roughness and the like 
whilst the robot is navigating and forms the basis of 
local path planning, force feedback navigation 
control and roughness vibration magnitude data in 
real-time. 
3.   The 3D Falcon force feedback joystick provides 
the operator with the capability of freely driving the 
robotic vehicle but with path planning guidance with 
preferred direction force feedback for driving and 
vibration feedback for terrain roughness monitoring. 
 
Whilst not mentioned explicitly earlier in the 
paper, once the Riegl data has been collected and 
integrated, the vehicle can be confidently navigated 
at night since the virtual environment world is lit and 
the Velodyne data needs no ambient lighting to 
collect.  Whether rain and/or smoke would seriously 
compromise this operation has not yet been 
investigated. 
Also, in the future, more sophisticated data 
matching for localisation in very rough terrain might 
be explored to provide accurate and reliable 3D 
localisation fixes.  Eventually, the navigation of 
smaller 
robotic 
vehicles 
in 
3D 
man-made 
constructions may be possible using this approach. 
 In some earlier work [15], it was shown that an 
‘appearance-based’ 
localisation 
method, 
where 
unwarped panoramic images collected on-board and 
compressed 
using 
Haar 
transformations 
were 
matched 
against 
visual 
signatures 
(similarly 
compressed) constructed off-line from the pre-
collected range/image Riegl data base, could yield 
acceptable localisation results without using an on-
board laser range finder. Particle filter methods were 
used to achieve approximate localisation. However, 
the accuracy achieved by this approach was not as 
good as that possible using range matching. The 
‘appearance-based’ results would have been worse in 
more sparse environments where less position/pose 
discriminating 
views 
could 
be 
extracted. 
Furthermore the ‘appearance-based’ approach would 
be inoperable in poor ambient lighting conditions (or 
at night) whereas the range based system can operate 
in any lighting conditions. Figure 10 shows two 
snapshots of a localisation trace being calculated. 
The central inserted panel show the unwarped 
current view from the on-board panoramic camera. 
The test environment is a partially covered outdoor, 
paved, flat environment with high visual business. In 
Figure 10(a), the initial spread of the particles over 
the environment indicates a wide search to find the 
starting position by image matching. In Figure 10(b), 
tracking based on continuity constraints allows the 
particle scatter to shrink; the weighted average point 
is taken as the calculated location. 
 
 
(a) 
 
 
(b) 
Fig. 10  Appearance Based Localisation Example 
 
VII 
CONCLUSIONS 
This paper has introduced the idea of terrain 
roughness and path planning guidance for the 
teleoperational control of a robotic vehicle in out-
185
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

door rough terrain with force feedback to assist 
control and terrain roughness monitoring with the 
added advantage of exploring a virtual world of a 3D 
visual model of the working environment either 
before or during a navigation mission.  Application 
areas such as bush fire fighting and search and 
rescue have been used to motivate this approach 
which allows both some degree of autonomous 
navigation to meld smoothly with teleoperational 
human guidance. Physical experiments to test the 
localisation methodology described in this paper 
have been successful in demonstrating the speed, 
accuracy and reliability of the approach, all of which 
were very satisfactory, even using the   simple 
matching formulations described. More work on 
human factors need to be carried out to properly 
gauge the value of this approach to bridging the gap 
between pure teleoperation and fully autonomous 
navigation. 
REFERENCES 
1. 
Jarvis, R. A., Terrain-Aware Path Guided Robot 
Teleoperation in Virtual and real Space, ACHI 2010,  
St. Maartins, Feb. 10-14, 
2. 
Jarvis, R. A., A Go Where You are Looking Semi-
Autonomous Rough Terrain Robotic Wheelchair,  
First International ICSC Congress on Autonomous 
Intelligent Systems, Deakin University, Geelong , 
Australia, 12-15 Feb. 2002. 
3. 
Jarvis, R. A., Sensor Rich Teleoperation Mode 
Robotic Bush Fire Fighting,  International Advanced 
Robotics 
Program/EURON 
WS 
RISE’2008,International Workshop on Robotics in 
Risky 
Interventions 
and 
Environmental 
Surveillance,7th to 8th Jan.,2008, Benicassim, Spain. 
4. 
Leonard, J.  J., and Durrant-Whyte, H. F. 
Simultaneous map building and localization for an 
autonomous mobile robot. In IROS-91 (Osaka, 
Japan, 1991), pp. 1442- 1447.  
5. 
Spero, D. (2007), “Simultaneous Localisation And 
Map building: the kidnapped way”.  PhD thesis. 
Monash University. 
6. 
Jarvis, R. A., Very Rough Terrain Robotic Vehicle 
for Bush Fire Fighting Support, Proc. 36th 
International Symposium on Robots (ISR 2005), 29th 
Nov.- 1st Dec. 2005, Tokyo, Japan. 
7. 
Jarvis, R. A., Virtual Reality Enhanced Excavator 
Teleoperation Proc. ISMCR'97 Workshop on Virtual 
Reality and Advanced Man-Machine Interfaces, 
Tampere, Finland, June 4-5, 1997, Proc. XIV 
IMEKO World Congress, Vol. IXB, pp.200-205. 
8. 
Jarvis, R. A., Four Wheel Drive Boom Lift Robot for 
Bush Fire Fighting, 10th International Symposium on 
Experimental Robotics (ISER 2006), July 6-10, 
2006, Rio de Janeiro, Brazil. Also in Experimental 
Robotics, Springer Tracts in Advanced Robotics 
239,Khatib, Kumar, Rus (Eds.)  ISBN 978-3-540-
77456-3, 2008, Springer Verlag Berlin Heidelberg. 
pp.245-255. 
9. 
Ray Jarvis and Nghia Ho, Robotic Cybernavigation 
in Natural Known Environments, Cyberworlds 2010 
International 
Conference, 
20-22 
Oct. 
2010, 
Singapore. 
10. Lozano-Perez', T.: Spatial planning: A configuration 
space approach, IEEE Trans. Comput. C-32(2) 
(1983), 108–120. 
11. LaValle, S. M. and Kuffner, J. J.. Rapidly-exploring 
random 
trees: 
Progress 
and 
prospects. 
In 
Proceedings 
Workshop 
on 
the 
Algorithmic 
Foundations of Robotics, 2000. 
12. Jarvis, R. A., Robot Path Planning: Complexity, 
Flexibility and Application Scope, International 
Symposium on Practical Cognitive Agents and 
Robots, 27-28 Nov., 2006, University of Western 
Australia, Perth. pp 3-14. 
13. Jarvis, R. A., On Distance Transform Based 
Collision-Free Path Planning for Robot Navigation 
in 
Known, 
Unknown 
and 
Time-Varying 
Environments, invited chapter for a book entitled 
'Advanced Mobile Robots' edited by Professor Yuan 
F. Zang World Scientific Publishing Co. Pty. 
Ltd.1994, pp. 3-31. 
14. Ho, Nghia and Jarvis, R. A. Large Scale 3D 
Environmental Modelling for Stereoscopic Walk-
Through 
Visualisation, 
submitted 
to 
3DTV 
Conference 2007, May 7-9, Kos Island, Greece. 
15. Ho, Nghia. and Jarvis, R. A., Vision based Global 
localisation Using a 3D Environmental Model 
Created by a Laser Range Scanner, Proc. IROS 
2008, Nice, France, Sept. 22-26, 2008, pp. 2964-
2969. 
 
 
186
International Journal on Advances in Intelligent Systems, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/intelligent_systems/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

