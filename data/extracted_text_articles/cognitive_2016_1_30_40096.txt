Effect on The Mental Stance of An Agent’s Encouraging Behavior in A Virtual Exercise
Game
Yoshimasa Ohmoto∗, Takashi Suyama∗ and Toyoaki Nishida∗
∗Department of Intelligence Science
and Technology
Graduate School of Informatics
Kyoto University
Kyoto, Japan
Email: ohmoto@i.kyoto-u.ac.jp, suyama@ii.ist.i.kyoto-u.ac.jp, nishida@i.kyoto-u.ac.jp
Abstract—Most of people think an agent is very diffrent from
human. The mental stance provides a critical barrier for an
agent to cross before it can be accepted as a social partner.
In this study, we focused on the situation in which an agent
encouraged performing a task. We experimentally investigated
how to inﬂuence the mental stance of human participants during
task performance by the encouraging behavior of the agent. We
implemented two agents: an “encouraging agent” that provided
motivational behavior to the participants and a “time-report
agent” that reported the passage of time to the end of the game.
We conducted an experiment to evaluate whether the behavior
model estimation had the potential to induce and maintain the
intentional stance in a variety of situations. As a result, the
agent could motivate the participants and induce the participants’
affective assiduities for the agent as that when they interact with
humans.
Keywords–Multi-modal interaction; human-agent interaction;
intentional stance.
I.
INTRODUCTION
Agents that perform collaborative tasks have been de-
veloped over a long period. It is expected that agents will
soon be developed that can replace humans in a variety of
roles, particularly short-term interactions such as front desks,
shopping counters, and information ofﬁces, where the quality
of the interaction between humans is “mechanical.” Agents are
usually regarded as multimodal interfaces that provide useful
information, rather than as social partners that can establish
relationships with humans [1]. To establish such social re-
lationships, people’s mental state with respect to humans or
agents is an important factor. The difference provides a critical
barrier for an agent to cross before it can be accepted as a
social partner.
The mental states that people infer when considering an
agent can be deﬁned as physical stance, design stance, and
intentional stance [2]. In the physical stance, we pay attention
to physical features such as the power of the motor and the
speciﬁcation of the display. In the design stance, we expect
that the agent will follow predeﬁned rules. In the intentional
stance, we assume that the agent has subjective thoughts and
intentions. When humans interact with each other, they usually
take the intentional stance, and they and their communication
partner respect each other. When humans interact with a
machine, they usually take the design stance. In this case,
they usually interact with the machine from a self-centered
perspective because they do not consider that the machine has
its own intentions. To establish social relationships between
a human and an artiﬁcial agent, the agent has to induce the
intentional stance in its human partner.
To induce such interactions, many previous researchers
have attempted to approximate the behavior model of an
agent to a generalized model of human behavior. For example,
Heider and Simmel [3] demonstrated that observers attribute
elaborate motivations, intentions, and goals to even simple
geometric shapes based solely on the purposeful pattern of
their movements. In the same way, when an agent exhibits
appropriate behavior, people who interact with the agent take
the intentional stance. However, in the course of a long-term
interaction, we expect that the behavior of the other entity
will be personalized as the interaction proceeds. This approach
is therefore not considered suitable for developing an agent
that can be regarded as a communicative social partner. There
are also differences in people’s mental states when engaging
with humans and with agents [4]. These differences provides a
critical barrier for an agent to cross before it can be accepted
as a social partner. It is important to ensure that the mental
state of people interacting with the agent is the same as that
when they interact with humans.
Goal-oriented behavior is one of the important factors in the
induction of the intentional stance [5]. In an earlier study, we
conﬁrmed that goal-oriented behavior was helpful in inducing
intentional stance during an interaction task [6]. In that study,
the apparent goal-oriented behavior of an agent was established
via trial-and-error. However, based on interaction analysis we
consider it more important to establish the behavior model of
an agent than to show goal-oriented behavior. In addition, if
only the goal-oriented behavior related to the immediate task
is used to induce the intentional stance, it becomes difﬁcult
to induce in long-term relationships, wherein many different
types of interaction are involved.
This study aims to investigate the method to inﬂuence the
mental stance of human participants during task performance
by making them estimate the behavior model when the behav-
ior of the agent is not directly related to the task itself. If such
a model of behavior can inﬂuence the mental stance of the
human participant, a more effective method for inducing the
intentional stance can be developed by combining this model
estimation approach with the goal-oriented behavior approach.
In long-term interactions, the agent can induce the intentional
stance using goal-oriented behavior in performing a particular
task and can maintain that stance using the model estimation
10
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

approach when the user switches to performing different tasks.
The paper is organized as follows. Section 2 brieﬂy
introduces previous work on the intentional stance. Section
3 outlines the proposed behavior model estimation approach.
Section 4 describes an evaluation experiment conducted to
investigate the effect on the mental stance toward the agent
and presents the results. Section 5 discusses the achievements
and the limitations of our approach. Section 6 concludes and
discusses future work.
II.
RELATED WORK
If an agent resembles a human or an animal in appear-
ance, people tend to spontaneously think that the agent has
intentions. Friedman et al. [7] reported that 42% members of
discussion forums about AIBO which was an animal robot
sold by Sony, a robotic pet, spoke of AIBO having intentions
or that AIBO engaged in intentional behavior. On the other
hand, [8] repored that an appropriate match between a robot’s
social cue and its task will improve people’s acceptance of and
cooperation with the robot. This means that we cannot induce
the intentional stance by the appearances alone.
Roubroeks [4] reported the occurrence of psychological
reactance when artiﬁcial social agents are used to persuade
people. In that study, participants read advice on how to
conserve energy when using a washing machine. The advice
was either provided as text-only, as text accompanied by a
still picture of a robotic agent, or as text accompanied by
a short ﬁlm clip of the same robotic agent. The results of
the experiment indicated that the text-only advice was more
accepted than either advice with the still picture of the robotic
agent or the advice with the short ﬁlm clip of the robotic agent.
Social agency theory proposes that more social cues lead to
more social interaction, but the result was the exact opposite.
This is caused by differences in people’s mental state with
respect to humans or agents.
From these researches, it is important that the mental stance
of people when they interact with the agents is the same as
that when they interact with humans. In our study, we tried to
inﬂuence the mental stance when the behavior of the agent is
not directly related to the task itself. Chen et al. [9] reported
that the perceived intent of the robot signiﬁcantly inﬂuenced
people’s responses when a robotic nurse autonomously touched
and wiped each participant’s forearm. They used the explicit
behavior which is directly related to the task to convey intent
of the robot. In our study, we focused on the motivational
behavior as agent’s behavior which was not directly related
to the task. Deci and Ryan [10] provided “self-determination
theory” which was a model to motivate people. This model
is applied in many situations (e.g., [11]). Readdy et al. [12]
reported that rewarded behaviors were not meaningfully con-
nected to successful performance. The rewarded behavior was
a kind of the motivational behavior. We thus considered the
motivational behavior was not directly related to the task. To
spontaneously make participants estimate the agent’s behavior
model, our proposed agent provided the motivational behavior
when the motivation of the participants were weakening.
III.
AN ENCOURAGING AGENT REFLECTING THE USER’S
STATE
In a previous study [6], we were able to induce the
intentional stance by presenting a goal-oriented, trial-and-error
process using multimodal behavior. However, the effect of
the method was low when participants were doing something
which was not directly related to the task. This suggested that
participants think the agent is only capable of producing ap-
propriate behavior directly related to performing the immediate
task. If participants just focus on the task performance, it is
hard to establish social relationships between a human and an
agent.
In this study, we tried to extend the method to induce the
intentional stance. For the purpose, we investigated whether the
agent’s behavior could improve and maintain the participant’s
active commitment to a task. The improvement is not directly
related to objectives of the task but important mental state
to performing the task. If the agent could do that, we think
participants represented a kind of affective attitudes towards
the agent.
The agent provided encouraging utterances in the task
when the agent judged that the participant’s commitment was
weakening. The agent’s behavior was caused by participant’s
behavior history and estimated current inner state. We expected
the participant to try to estimate the agent’s behavior model
because they could easily ﬁnd the agent had some rules to
interact with them but the behavior was not directly related to
performing the task. The estimation of the behavior model
is ﬁrst step to maintain the intentional stance in general
situations. In this section, we brieﬂy explain the architecture
of the “encouraging agent.”
A. Task description
In this study, we used a ﬁrst-person throwing game using
virtual balls in an immersive virtual space as the interactive
task. This game was designed for encouraging exercise. The
explicit objective of the participants was to win the game,
while the implicit objective was to improve the commitment
to the exercise. The encouraging behavior of the agent was
related to the participants’ implicit motivation, but did not
directly contribute to winning the game, as in some situations,
the winning strategy was for the participant to exit the game
(when the participant had a large point score or when the
remaining time was short). The encouraging behavior was used
to investigate the effect of the participant’s understanding that
the agent’s behavior is related to the implicit objective.
In the game task, the players (including the agent) shared
the basic rules and had the implicit and explicit objectives
as a common ground. This helped both partners estimate the
behavior model of the other. In the ﬁrst-person throwing game,
the players could not verbally share information because the
states of the game and of the players changed too quickly.
The agent therefore did not need to use detailed verbal
communication in the experiment. Use of a game task also
allows good data to be obtained because participants become
immersed in the game [13].
We set the following conditions on the exercise game task:
•
Multiple players joined the game, and most players
were humans.
•
Some objectives could be achieved without interacting
with other players.
•
Other objectives could be achieved only when players
interacted.
11
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

ObjecƟve
decision
Player state 
esƟmaƟon
Behavior 
decision
Expression
Game situaƟon 
coder
Behavior 
database
ReacƟon
ObjecƟve 
database
Agent 
management
Figure 1. The outline of the system architecture.
•
The only explicit reward was the game score. Other
rewards were implicit, and the player could not iden-
tify them.
•
The game session was short (about 10 min) to ensure
that the player concentrated on performing the task.
•
The game characters were controlled by the players’
body motions. This allowed the player to intuitively
control the game character.
•
All players had the same abilities and followed the
same rules.
More detailed rules were deﬁned depending on the target
state of the players, such as how strongly motivated they
appeared, how long the played the game, and how deeply
immersed they seemed. We expected that the rules would
allow us to investigate the effect of the agents’ behavior on
the human players’ mental stance.
B. Outline of the architecture
The outline of the system architecture, as shown in Figure
1, was based on a Belief-Desire-Intention (BDI) model. Each
component is brieﬂy described below.
Player state estimation:
This component estimated the user’s state in rela-
tion to task commitment based on the parameters
obtained from the exercise game and the prede-
ﬁned rules. The details of the parameters are given
below.
Game situation coder:
This component categorized different situations in
the game based on the parameters of the game
and the predeﬁned rules. The game parameters are
described below.
Objective database:
The database contained all the possible objectives.
Objective decision:
This component chose an objective from the ob-
jective database, based on the outputs of the player
state estimation component and the game situation
coder component.
Agent management:
This component calculated the state of the agent,
based on the same parameters as those of the
player.
Behavior decision:
This component chose a concrete behavior based
on the received values.
Expression:
This component produces the selected behavior.
The game parameters
Game score distribution:
The game was scored by the points accrued by
each player (including the agent) in line with the
game rules. The distribution compared the scores
of the players.
Remaining time:
This showed the time remaining until the game
was over.
Rate of accruing game score:
Each player had this parameter. This parameter
increased as the player accrued points and it
decreased with time.
The player parameters
Hate value:
Each player had more than one parameter for
each other player. This parameter measured how
strongly one player wanted to target the other
player. This parameter increased when the other
player hit the ball and it decreased with time.
Movement distance:
This parameter showed how far the player had
moved over the last 30 seconds.
Frequency of accruing game score:
This parameter showed how frequently the player
accrued points.
Some of the parameters decreased with time, reﬂecting the
observation of Wohl et al. [14] that the memory of past events
decreases with time.
The encouraging agent exhibited behavior designed to
motivate a player when the agent judged that the player’s
commitment to the game was falling. This was done under
the following conditions:
•
When the player’s movement distance parameter fell
below 75% of their movement distance at the start
of the game. Initially, the player’s commitment is
high, but he/she does not yet know what behavior
is appropriate when playing the game. We used this
initial player state as the benchmark for the behavioral
activity.
•
When the player’s frequency of accruing the game
score parameter was less than that of the other players,
including the agent, by two or more. A player’s
motivation drops when his/her ability to win the game
is poor [15]. When the player was in this state, the
agent assumed that commitment was low.
IV.
EXPERIMENT
To investigate the effects on mental stance when the
agent encourages the participant’s commitment to the exer-
cise task, we conducted an experiment using two agents: an
“encouraging agent” that provided motivational behavior to
the participants and a “time-report agent” that reported the
passage of time to the end of the game. We assumed that if
we could inﬂuence the mental stance of the participants using
this approach, the behavior model estimation has the potential
to induce and maintain the intentional stance in a variety of
12
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

situations.
To evaluate this, we analyzed the number of target actions
made toward the agent in the course of the experiment. The
target action meant that the participant tried to hit the ball to
another player, including the agent. We assumed that, when the
participants have an intentional stance toward the agent, they
unconsciously balance the target frequency, in a manner similar
to that when they want to balance the game score of each
participant in human-human interactions. The target actions
were counted automatically based on the behavioral data. In
addition, we asked the participants to complete a questionnaire
after the experiment. We compared the experimental results
between a group wherein participants interacted with the “en-
couraging agent” and a group wherein participants interacted
with the “time-report agent.”
A. Task
Two humans and an agent participated in the game task.
The task was a virtual ﬁrst-person throwing game. Each was
player assigned a different color. The player could change their
own ball to the color of another player’s ball by moving a game
object (a moving teddy bear) to a place corresponding to each
color. The players won a point when their ball hit another
player with the corresponding color. When the ball with the
player’s own color hit another player, the player received a
point. If the player hit a non-colored ball at another player,
that player stopped for 5 seconds and dropped their teddy bear
at that place. Other players could pick up the dropped teddy
bear. The player who was carrying the teddy bear could not
shoot a ball. The virtual ball was automatically refreshed after
5 seconds.
B. Experimental setting
The experimental setting is shown in Figure 2. We used an
Immersive Collaborative Interaction Environment (ICIE) [16]
and Unity3D [17] to construct the virtual environment and the
two agents. ICIE uses a cylindrical immersive display that is
composed of eight portrait orientation liquid-crystal-displays
(LCD) with a 65-inch screen size, arranged in an octagonal
shape. In this environment, participants could look around in
the virtual space with a low cognitive load, as in the real world.
A participant’s virtual avatar could be controlled by their body
motions using motion sensors placed on their dominant arm,
both feet, and waist. These sensors captured throwing motions,
stepping motions, and body orientation. The participant could
intuitively control the virtual avatar using body motions with
low physical constraints.
The speed of movement of the avatar was controlled
by the participant’s stepping motion. The minimum speed
was slower than the speed of the teddy bears and of the
game playing characters, while the maximum speed was faster.
Participants could achieve the maximum speed by adopting a
brisk walking pace and could throw the virtual ball with a
throwing motion. The speed of the ball was not dependent on
the throwing motion. The direction of movement and throwing
trajectory were determined by body orientation. To determine
the participant’s inner state, physical exertion was estimated
from the stepping motion. This information was sent to the
game system in real time.
The rules controlling the movement of the teddy bears
were simple and consistent. The teddy bear did not consider
Game control PC 
Motion data
Display the avatar motions
Producing utterances 
by an agent
Immersive display (ICIE)
Participant
Motion data
Game data
Game control PC
Participant
Immersive 
display (ICIE)
Motion data
Display 
the game data
Figure 2. The experimental environment.
the participant’s inner state or the game conditions (e.g., if
the score was high or if the previous strategy was the same
as the current strategy). The rules depended on the positional
relationships in the game and on whether the players had a
teddy bear.
C. Procedure
Two participants who were acquainted with each other
joined the game task. The interactive agent who joined
the game was randomly selected to display encouraging or
remaining-time-report behavior. The frequency of intervention
was the same for both agents. The frequency of intervention
by the encouraging agent was calculated from a preliminary
experiment. Neither of the agents could change their interac-
tion strategy in the game.
First, the participants were instructed on the experimental
procedures and the motion sensors were attached. After con-
ﬁrming the data from the sensors, the experimenter started the
video cameras and the game. The participant ﬁrst performed a
practice session and then performed three game sessions. Each
game session lasted 10 minutes, with 2-minutes rest intervals
between sessions. At the conclusion of the experiment, the
participant completed a questionnaire.
Ten pairs of participants (20 students, 16 males and 4
females) participated in the experiment. All participants were
students aged from 19 to 32 years (average 21.7 years). Ten
participants (8 males and 2 females) played the game with the
encouraging agent (E-group) and the rest played the game with
the time-report agent (T-group).
D. Result of interaction behavior analysis
The purpose of this analysis is to investigate whether
judgments about an agent’s behavior that is not directly
related to task performance inﬂuenced interaction behavior.
We calculated the ratio between the number of target actions
directed toward the agent and the number of target actions
directed toward the other participant. We expected that the
proportion would be around 0.5 when a participant took the
intentional stance, as the players tried to balance the game
score. In contrast, a participant who took the design stance
would either ignore the agent, assuming that the agent was
not a good player, or target only the agent, assuming that this
would be an easy way to improve their game score.
We compared the results from the E-group with those
13
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

0.24 
0.40 
0
0.1
0.2
0.3
0.4
0.5
0.6
E-group
T-group
Figure 3. The ratio between the number of target actions directed toward the
agent and the number of target actions directed toward the other participant.
2.7
3.3
3.6
3.3
6.3
4.5
2.9
2.2
4
5.5
1
2
3
4
5
6
7
How do you feel about
the agent's intenƟons
from the agent's
uƩerances?
How strongly do you
feel that the agent
strategically colored
the ball?
How strongly do you
feel that the agent paid
aƩenƟon to the
parƟcipants' game
scores?
How strongly do you
feel that the agent
wanted to win the
game?
How were you
moƟvated to play the
game?
E-group
T-group
Figure 4. Results of questionnaires.
from the T-group in the second and third sessions, and then
calculated the distances from 0.5. The results are shown in
Figure 3. A Mann-Whitney U test showed that the distance in
the E-group was signiﬁcantly less than that in the T-group (p
= 0.027). The average in the E-group was 0.23. This means
that a small difference arose once or twice in each session.
The results suggest that the participants took care to balance
the game score. These results suggest that the approach was
successful in inducing the intentional stance.
E. Result of questionnaire analysis
The purpose of this analysis is to investigate how agent
behavior not directly related to task performance inﬂuenced
the subjective impressions of the participants. The participants
rated the behavior of the agent on a seven-point scale, pre-
sented as ticks on a black line without numbers. We post-
coded these scores from 1 to 7. The results are shown in Figure
4. We performed Mann-Whitney U tests on the questionnaire
data. This shows the ﬁnal impressions of participants toward
the agents in the experiment.
•
How do you feel about the agent’s intentions from
the agent’s utterances?
This was used to conﬁrm the subjective impression
of the encouraging or time-report utterances of each
agent. The utterances of the remaining-time-report
agent were scored signiﬁcantly higher than those of
the encouraging agent (p = 0.028). This was an unex-
pected result. From observation of the video data, we
identiﬁed situations wherein the agent’s encouraging
behavior was inappropriate in the game context, for
example, encouraging a strategy immediately after the
same strategy had been performed. In these cases,
the participants could not understand the intentions of
the agent. In contrast, the time-report utterances were
always appropriate to the game context, and the par-
ticipants always understood the intention behind them.
This may be one of the reasons for this unexpected
result.
•
How strongly do you feel that the agent strategi-
cally colored the ball?
This was to conﬁrm whether the utterances that in-
duced the intentional stance caused the participants
to judge the meaning of the agent’s other behavior.
The Mann-Whitney U test showed that there was
no signiﬁcant difference between the groups (p =
0.53), suggesting that encouraging utterances did not
inﬂuence the participants’ judgments on the meaning
of the agent’s behavior.
•
How strongly do you feel that the agent paid
attention to the participants’ game scores?
This was to explore whether the participants were
aware of the implicit inner state of the agent. The
Mann-Whitney U test showed no signiﬁcant difference
between the groups (p = 0.16). Within the E-group,
there were large individual differences in awareness
of the agent’s inner state. If this approach is to be
applied to general situations, we need to ﬁnd ways to
reduce the individual variation through the presenta-
tion method.
•
How strongly do you feel that the agent wanted to
win the game?
Both agents had as an objective that ”the agent wants
to win.” The objective was very general but it was not
presented explicitly. This question was used to explore
whether the participants registered these objectives.
Again, the Mann-Whitney U test showed there was
no signiﬁcant difference between the groups (p =
0.34). Nor were there differences in the averages or
variances. This suggests that the participants did not
pay attention to objectives that were not presented
explicitly. This result was is a little disappointing. We
expected that the participants who had the intentional
stance would read objectives and intentions that were
not directly related to the information presented.
•
How were you motivated to play the game?
This question was asked to conﬁrm whether the agent
could motivate the participant to play the game. There
was a marginally signiﬁcant difference between the
groups (p = 0.081). The participants in the E-group
were more motivated than the participants in the T-
group. We suspect that there is a ceiling effect because
the virtual exercise game in an immersive environment
is itself motivating enough.
V.
DISCUSSION
In our previous study [6], we were able to induce the
intentional stance by presenting goal-oriented behavior, but
it proved challenging to induce the intentional stance in
situations wherein the relevance to task performance was low.
This study aims to induce the intentional stance in more
14
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

general situations than those in the previous study. For the
purpose, we made the participants estimate an agent’s behavior
model when performing the task. In the evaluation experiment,
when the participants interacted with an agent that presented
encouraging behavior (the E-group), participants focused on
the balance of the game score. They appropriately read the
meaning of the agent’s behavior, and their mental stance was
inﬂuenced by the agent’s interactive behavior.
A particularly important ﬁnding from our analysis was that
the encouraging agent’s behavior, while not directly related to
task performance, affected the behavior of the participants in
performing the task. The participants were obviously aware of
the meaning of the agent’s behavior (i.e., the agent encouraged
the participant’s commitment). Although the meaning was not
usually related to the balance of the game score which was
directly related to the task performance, the participants took
care to balance the game score. This suggests that the agent’s
behavior model induced affective effects. Humans naturally
show this kind of consideration even in competitive situations.
We think that this type of consideration is a ﬁrst step to
establishing a social relationship between humans and artiﬁcial
agents.
On the other hand, the proposed method did not affect
the participants’ judgment of those parts of the agent’s behav-
ior that were not related to the explicit behavior. We were
disappointed with this result because we expected that the
participants would be able to judge the agent’s behavior more
broadly. In future studies, we will investigate an interaction
model that allows the participant to judge a range of behaviors
in long-term interactions. We think that our previous studies
([6], [18]) provides the foundations for such an interactive
model.
VI.
CONCLUSIONS
In this study, we investigated how to inﬂuence the mental
stance of human participants during task performance when the
behavior of the agent is not directly related to the task itself.
For this purpose, we tried to make the participants estimate
the agent’s behavior model in human-agent interaction. We
adopted “encouraging behavior” as an estimated model of
the agent becuase the causal relationship between the agent’s
behavior and it’s intention was clear and presumable. We
implemented two agents: an “encouraging agent” that provided
motivational behavior to the participants and a “time-report
agent” that reported the passage of time to the end of the game.
We conducted an experiment to evaluate whether the behavior
model estimation had the potential to induce and maintain the
intentional stance in a variety of situations. As a result, the
agent could motivate the participants and they took care to
balance the game score. This is a kind of affective assiduities.
In future work, we will investigate an interaction model that
allows the participant to judge a range of behaviors in long-
term interactions.
ACKNOWLEDGEMENT
This research is supported by the Center of Innovation
Program from Japan Science and Technology Agency, JST,
AFOSR/AOARD Grant No. FA2386-14-1-0005, Grant-in-Aid
for Young Scientists (B) (KAKENHI No. 25870353), and
, Grant-in-Aid for Scientiﬁc Research (A) (KAKENHI No.
24240023) from the Ministry of Education, Culture, Sports,
Science and Technology of Japan.
REFERENCES
[1]
B. Shneiderman and P. Maes, “Direct manipulation vs. interface agents,”
interactions, vol. 4, no. 6, 1997, pp. 42–61.
[2]
D. C. Dennett, The intentional stance.
MIT press, 1989.
[3]
F. Heider and M. Simmel, “An experimental study of apparent behav-
ior,” The American Journal of Psychology, 1944, pp. 243–259.
[4]
M. Roubroeks, J. Ham, and C. Midden, “When artiﬁcial social agents
try to persuade people: The role of social agency on the occurrence
of psychological reactance,” International Journal of Social Robotics,
vol. 3, no. 2, 2011, pp. 155–165.
[5]
W. H. Dittrich and S. E. Lea, “Visual perception of intentional motion,”
Perception - London -, vol. 23, 1994, pp. 253–253.
[6]
Y. Ohmoto, J. Furutani, and T. Nishida, “Induction of intentional
stance in human-agent interaction by presenting agent behavior of
goal-oriented process using multi-modal information,” in COGNITIVE
2015: The Seventh International Conference on Advanced Cognitive
Technologies and Applications.
IARIA, 2015, pp. 90–95.
[7]
B. Friedman, P. H. Kahn Jr, and J. Hagman, “Hardware companions?:
What online aibo discussion forums reveal about the human-robotic
relationship,” in Proceedings of the SIGCHI conference on Human
factors in computing systems.
ACM, 2003, pp. 273–280.
[8]
J. Goetz, S. Kiesler, and A. Powers, “Matching robot appearance and
behavior to tasks to improve human-robot cooperation,” in Robot and
Human Interactive Communication, 2003. Proceedings. ROMAN 2003.
The 12th IEEE International Workshop on.
IEEE, 2003, pp. 55–60.
[9]
T. L. Chen, C.-H. A. King, A. L. Thomaz, and C. C. Kemp, “An
investigation of responses to robot-initiated touch in a nursing context,”
International Journal of Social Robotics, vol. 6, no. 1, 2014, pp. 141–
161.
[10]
E. L. Deci and R. M. Ryan, Intrinsic motivation and self-determination
in human behavior.
Springer Science & Business Media, 1985.
[11]
A. St George, A. Bauman, A. Johnston, G. Farrell, T. Chey, and
J. George, “Effect of a lifestyle intervention in patients with abnormal
liver enzymes and metabolic risk factors,” Journal of gastroenterology
and hepatology, vol. 24, no. 3, 2009, pp. 399–407.
[12]
T. Readdy, J. Raabe, and J. S. Harding, “Student-athletes ’perceptions
of an extrinsic reward program: A mixed-methods exploration of self-
determination theory in the context of college football,” Journal of
Applied Sport Psychology, vol. 26, no. 2, 2014, pp. 157–171.
[13]
K. Collins, K. Kanev, and B. Kapralos, “Using games as a method
of evaluation of usability and user experience in human-computer
interaction design,” in Proceedings of the 13th International Conference
on Humans and Computers.
University of Aizu Press, 2010, pp. 5–10.
[14]
M. J. Wohl and A. L. McGrath, “The perception of time heals all
wounds: Temporal distance affects willingness to forgive following
an interpersonal transgression,” Personality and Social Psychology
Bulletin, 2007.
[15]
C. S. Dweck, “Motivational processes affecting learning.” American
psychologist, vol. 41, no. 10, 1986, p. 1040.
[16]
Y. Ohmoto and et al., “Design of immersive environment for social
interaction based on socio-spatial information and the applications.” J.
Inf. Sci. Eng., vol. 29, no. 4, 2013, pp. 663–679.
[17]
“Unity,” http://unity3d.com/ (2016/02/01).
[18]
Y. Ohmoto, S. Horii, and T. Nishida, “The effects of extended estimation
on affective attitudes in an interactional series of tasks,” in CENTRIC
2015: The Eighth International Conference on Advances in Human-
oriented and Personalized Mechanisms, Technologies, and Services.
IARIA, 2015, pp. 62–67.
15
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

