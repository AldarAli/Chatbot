Adaptive Smart Environments: Detecting Human Behaviour from Multimodal
Observation
Rory O. Heffernan, Michael L. Walters, Neil R. Davey, Rene te Boekhorst, Kheng Lee Koay
and Kerstin Dautenhahn
Adaptive Systems Research Group, School of Computer Science
University of Hertfordshire, Hatﬁeld, Hertfordshire, UK
Email: {r.heffernan, m.l.walters, n.davey, r.teboekhorst, k.l.koay, k.dautenhahn}@herts.ac.uk
Abstract—It is desirable to enhance the social capabilities of
a smart home environment to become more aware of the
context of the human occupants’ activities. By taking human
behavioural and contextual information into account, this will
potentially improve decision making by the various smart
house systems. Full mesh Wireless Sensor Networks (WSN)
can be used for passive localisation and tracking of people
or objects within a smart home. By monitoring changes in
the propagation ﬁeld of the monitored area from the link
quality measurements collected from all the nodes of the
network, it is feasible to infer target locations. It is planned
to apply techniques from Radio Tomographic Imaging (RTI)
and machine vision methods, adapted to the idiosyncrasies of
RTI, which will facilitate real-time multiple target tracking in
the University of Hertfordshire Robot House (UHRH). Using
the Robot Operating System (ROS) framework, these data
may then be fused with concurrent data acquired from other
sensor systems (e.g.) 3-D video tracking and ambient audio
detection in order to develop a high level contextual data model
for human behaviour in a smart environment. We present
experimental results which could provide support for human
activity recognition in smart environments.
Keywords–radio tomography; device-free passive localisation;
wireless sensor networks; human-computer interaction; sensor
fusion.
I.
INTRODUCTION
There is a clear and obvious need for reliable person
location sensing systems in smart environments and assisted
living environments that make life more convenient and
more secure for people. The convenience of automated
control of lighting, heating and other electrical consumer
devices is increasingly being teamed with domestic or ser-
vice robots as elements within a smart home. Robots are
increasingly seen as one of the main ways by which the
smart home systems can interact face to face with the human
occupants, as well as directly perform assistive tasks. A
consequence of convergent multiple underlying technologies
is the emergence of the paradigm of ubiquitous computing,
also described as pervasive computing or ambient intelli-
gence or the "Internet of Things" [1], [2]. There are also sig-
niﬁcant beneﬁts to the wider global community derived from
more efﬁcient use of resources. In parallel with increasing
global pressure on dwindling energy reserves, we are also
witnessing the pressures that an ageing population places
on already over-stretched health and social welfare services.
Converting ordinary homes to smart living environments will
require the development of unobtrusive, passive, device-free
person tracking and localisation systems which are simple
to install, operate and maintain. Gathering human activity
information from a variety of sensor data coupled with
machine learning algorithms is critical to the development
of a wide variety of applications used to monitor and track
the functional status of residents. The eventual aim of this
current research is to enhance the social capabilities of a
smart environment so enabling it to become more aware
of the context of current human activities and to take this
contextual information into account in order to improve the
smart home systems’ Artiﬁcial Intelligence (AI) assistive
decision making.
The potential use of wireless sensor networks (WSN)
for passive human localisation and tracking systems affords
numerous desirable characteristics. In particular, people are
not required to carry or wear any electronic transceiver
equipment of any kind. Consequently the system perfor-
mance is not affected by such human attributes as forgetting
to carry or wear devices and is also able to accommodate
the presence of visitors (or intruders). The convergence
of advances in the key technology areas of digital micro-
electronics, micro-electro-mechanical systems (MEMS) and
wireless communications have led to an expanding ecosys-
tem of low cost, low-power sensor node devices which are
capable of forming self-organising cooperative ad-hoc short-
range radio frequency (RF) WSN [3]. WSN technology
deployed within a smart environment allows unobtrusive
person-tracking without the need for the person(s) being
tracked to carry a transmitting device. This is also known
as Device-free Passive Localisation (DfPL).
Objects moving through the space where the WSN
operates result in a perturbation of the radio links. Re-
ceived Signal Strength (RSS) is one of the metrics built
into 802.15.4-compliant wireless nodes, another being Link
Quality Indicator (LQI), both of which are discussed in
the technical note at Section 2 below. Radio Tomographic
Imaging (RTI) exploits these RSS ﬂuctuations caused by
multipath propagation of the radio signals as they deviate
from their line of sight path due to shadowing, reﬂection,
diffraction or scattering [4].
RTI technology is integral to the proposed sensor-based
research described here. A methodology summary can be
derived from the ﬁrst key paper describing the work of
Bocca et al. [5] in the development of RTI as a reliable
DfPL technology application for multiple target tracking (see
data processing steps in outline at Section 5 below). The
353
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

objective is to estimate the change in the propagation ﬁeld
of the monitored area from the RSS/(LQI) measurements
collected from all the links on the network.
Sensor fusion is also integral to this current research and
a methodology summary derived from the second key paper
describing the work of Brdiczka et al. [6] in the development
of multimodal smart environment observation is presented at
Section 3 below.
The remainder of this paper is organised as follows:
Section 2 discusses related work. Section 3 discusses re-
search challenges and goals. Section 4 describes how the
WSN has been created and an analysis and discussion of
initial studies are depicted. In Section 5 planned studies are
outlined. Finally we identify future research and conclude
this paper in Section 6.
II.
RELATED WORK
The eventual aim of the work presented here is the
recognition of everyday human activity, more formally re-
ferred to as Activities of Daily Living (ADL), for example,
through the use of multiple sensors installed in a smart home
environment. ADL include routines such as dressing/bathing,
eating, ambulating (walking), toileting and hygiene [7].
Associated with each activity there will typically be at least
one basic action such as opening or closing a drawer or
cupboard, fetching a utensil etc.
Cook and Das [8] deﬁne a smart environment as "a small
world where different kinds of smart device are continu-
ously working to make inhabitants’ lives more comfortable".
Smart environments are envisioned as the by-product of
inexpensive and pervasive computing, thus making Human-
Computer Interaction (HCI) with the system a pleasant and
intuitive experience.
A typical DfPL system currently employs a variety of
sensing devices including video camera systems and physi-
cal contact sensors [9] for people and object localisation and
tracking. Other DfPL systems make use of light sources and
sensors [10]. Fink and Beikrich [11] usefully classify DfPL
systems employing radio signals into three main categories
based on measurement principles:
i.
Radar-based systems which utilise trigonometry
(range and angle) derived from reﬂected and scat-
tered return signals [12], [13];
ii.
RF-based systems which analyse attenuation of re-
ceived signal strength (RSS) values on wireless links
between nodes. This technique, known as RTI, while
capable of monitoring large spatial volumes such as
warehouses, homes and ofﬁces, is limited to line of
sight (LOS) setups [4];
iii.
Variance-based RTI (VRTI) systems which analyse
RSS ﬂuctuations in multipath fading environments
and permit non line of sight (NLOS) localisation
[14].
In a smart environment, information about human be-
haviour and activities can be acquired from different types
of detectors, each with its own data acquisition framework
or ’modality’. Brdiczka et al. [6] describe their approach
to addressing learning and recognition of human behaviour
models from multi-modal observations in one such smart
home environment. Multi-modal observation is an innate
human concept where use of combined senses is essential
to the detection and discrimination of incoming signals
from the environment. Consequently some multi-modal ap-
plications are based on imitating aspects of natural multi-
modal sensing, with audio-visual being the most prevalent.
Sensor data fusion is the combining of sensor data derived
from disparate sources, such that the resulting information
has less uncertainty than would be possible when these
sources were used individually. Feature extraction is a key
component of 3-D video tracking, but is computationally
expensive as well as being context insensitive. A potential
alternative to 3-D video tracking is the use of RTI in
order to acquire changes in RSS and WSN link quality
information (LQI) caused by moving people and objects
in the monitored area [4]. RTI is a promising WSN-based
imaging technique developed by Bocca, Patwari, Wilson et
al. [4], [5] which is computationally less expensive than 3-D
video processing and can operate in real- time. Compared to
other sensing technologies applied in motion tracking, such
as infrared, ultrasonic range ﬁnders [15], ultra-wideband
(UWB) radios [16] and video cameras [17], WSN provides
several advantages: they work in the dark and can pene-
trate smoke and walls; they are less invasive in domestic
environments than video camera networks [18], they are
signiﬁcantly less expensive than UWB transceivers and their
installation and maintenance time is minimal. The current
work explores the possibility of using RTI for sensor fusion
and machine learning as a component in a multi-modal smart
environment. Bocca et al. [5] describe a state of the art
RTI system which operates in real time. Our intention is
to replicate or adapt aspects of their methodology, alongside
the methodology of Brdiczka et al. [6] and to incorporate it
into the experimental work. These data may then be fused
with, or evaluated against contemporaneous data acquired
from other sensor systems (e.g.) 3-D video tracking, speech
and ambient audio detection, reed switches, pressure mats,
etc. in order to develop a high level contextual data model
for human behaviour in a smart home environment.
A. Technical note concerning RSS & LQI
1) Received Signal strength (RSS): RSS decreases with
distance according to the following equation:
RSS = −(10nlog10d + A)
(1)
Where,
n = signal propagation constant (propagation exponent)
d = distance from sender
A = received signal strength at a distance of 1 meter
RSS is a measure of how the conﬁgured transmission
Power at the Transmitting device (PTx) directly affects the
received signal Power at the Receiving device (PRx). In
embedded RF devices, the RSS is converted to a Received
Signal Strength Indicator (RSSI) which is deﬁned as the
ratio of the received Power to the ReFerence power (PRF).
Typically the reference power represents an absolute value
of PRF = 1mW, RSSI is typically expressed in dBm:
RSSI = 10 ∗ log(PRX/PRF)
(2)
354
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

2) Link quality indicator (LQI): IEEE 802.15.4 radio
devices provide applications with information about the
incoming signal. The effect of distance and interference on
RSS can be measured by the packet success rate, RSSI
and LQI provided by the radio. LQI is a metric introduced
in IEEE 802.15.4 that measures the error in the incoming
modulation of successfully received packets (packets that
pass the Cyclic Redundancy Check (CRC) criterion).
LQI measures each successfully received packet and
the resulting integer ranges from 0x00 to 0xFF (0-255),
indicating the lowest and highest quality signals detectable
by the receiver (between -100dBm and 0dBm). The cor-
relation value of LQI ranges between 50 and 110 where
50 indicates the minimum value and 110 represents the
maximum. Software converts the correlation value to the
range 0-255:
LQI = (CORR − a) ∗ b
(3)
Where,
CORR = correlation value
a and b are found empirically based on Packet Error Rate
(PER) measurements as a function of CORR.
LQI is the preferred metric for the current work, rather
than the RSSI metric used by Bocca et al. [19]. In ZigBee-
compliant mesh network radios, the LQI is convenient to
obtain from neighbour request packets as the instantaneous
unidirectional link values are transmitted as the ﬁnal byte
of any message. By comparison, the RSSI value is more
difﬁcult to obtain simultaneously for all links at each sam-
ple time-point without an additional computational burden
inherent in time-stamp synchronisation.
III.
RESEARCH CHALLENGES AND GOALS
A. Design challenges
Smart environments have enabled computerised obser-
vation of human interaction. However using this acquired
information as contextual information for the purpose of
adapting the behaviour of an automated environment is less
well researched and is the main reason for undertaking this
research. The analysis of multiple targets provides informa-
tion about social context and enables computer systems to
track and anticipate human interaction. The latter is a non-
trivial problem because human activity is situation dependent
and not necessarily planned. Smart environments need to
make use of this situational information in context in order
to respond appropriately to human activity. "Context is key
for interaction without distraction." [6]
B. Detecting human behaviour from multimodal observation
Brdiczka et al. [6] in their 2009 paper address the
problem of learning and recognition of human behaviour
models from multimodal observation in a smart home en-
vironment, similar to the University of Hertfordshire Robot
House (UHRH). Their approach formed part of a framework
for acquiring a high-level contextual data model for human
behaviour in an augmented environment. In particular, their
work focused on the automatic acquisition of information
about social context and activity recognition from the anal-
ysis of the interactions between small groups of two or more
individuals in real-time. Their multi-modal set-up comprised
audio and video information. A 3-D video tracking system
was used to create and track persons. A speech analyser
determined if and when participants were speaking. Back-
ground environment noise was captured with an ambient
sound detector. An individual role detector derived person-
speciﬁc activities such as walking or interacting with an
object frame by frame from 3-D tracker data (posture,
speed and interaction distance). The association of audio
stream data with each individual person identiﬁed by the
3-D tracker was coded by hand prior to commencing each
recording. Several set-piece social situations were learned
and detected using Hidden Markov Models (HMM) based
on role and audio data. The authors propose a general
framework methodology for the recognition and learning
of the different components comprising a human behaviour
model. The learning and detection was achieved in a two-
part process involving ﬁrst ofﬂine analysis (learning) fol-
lowed by online detection of learned behaviour models.
C. Real-time object tracking and localisation
Real-time evaluation of video and audio streams is gen-
erally costly in terms of computing power. RTI in contrast
uses multiple target tracking algorithms to generate real time
images (every 13.3ms) of the change in the RF propagation
ﬁeld. The system is capable of running on a laptop having
a 2.50GHz Intel(R)CoreT M i5-2450P processor and 8GB of
RAM memory [5]. The application of RTI techniques for
multiple person tracking as an adjunct to the methodology
described in this section, may provide useful improvements
both to the problem of multi-person situation recognition
and to the online (real-time) detection problem. The RTI
system initially developed by Wilson and Patwari [4] and
later extended by Bocca et al. [5] provides a basis for
the current research. Their work targeted using changes
in the RSS of the links in WSN to localise and track
multiple people or other targets in real time without needing
them to carry or wear any electronic device. Their work
represents an important step in showing that the application
of methods informed by machine vision and adapted to
radio tomography could be used to generate images of the
changes in the propagation ﬁeld as if they were frames
of a video. Our current research seeks to build upon this
base, and eventually to extend it to use the sensor fusion
method initially developed by Brdiczka et al. [6]. Their
work was targeted at learning and recognition of human
behaviour models from multimodal observation in a smart
home environment. This work, however, was limited in two
respects. Firstly, it relied upon a computationally expensive
3-D video tracking system requiring extensive ofﬂine (and
therefore non real-time) processing in order to visualise hu-
man activities and to create and to track entities (people) in
a scene. Secondly, because the detection of group dynamics
and group formation, which is necessary for group situation
recognition in (informal) real settings, remains an open issue
within 3-D video tracking technology. This requires studying
the use of RTI as part of a framework for acquiring a high-
level contextual model for human behaviour in an augmented
environment. It is envisaged that ROS will be utilised as
a framework to implement processing nodes, which can
355
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

process and integrate data acquired from multiple sensor
types. Additionally, if evaluation metric testing between 3-
D video tracking and RTI suggests that RTI is at least as
reliable as the current video sensors, it may obviate the need
for video sensors in future multi-modal observation studies.
IV.
INITIAL STUDIES
We constructed a minimal full mesh network compris-
ing 5 nodes using XBee Series 2 radio modules running
the ZigBee protocol ﬁrmware [20]. The individual Router
nodes were labelled ’A’ to ’D’ respectively and the network
coordinator node ’BB’ Power supply is via a USB connector.
Transmit power was set to minimum for all nodes. Initial
simple, single person experiments were performed in a room
at the University of Hertfordshire. We deployed 5 sensors
spaced at approximately 2 meter intervals apart, each at a
height of about 1 meter forming 3 sides of a rectangle, with
the fourth side ’open’. The thick arrow indicates ’open’ side
walk path direction. Nodes ’A’ and ’BB’ not pictured but
mirror positioned to ’C’ and ’D’ respectively. (Figure 1).
Figure 1. Experimental setup: nodes ’B’, ’C’ and ’D’ are shown.
Evaluation was performed by walking along the length of
the open side. Two walks were performed: Walk ’A’ (walk
away-turn-walk back) and Walk ’B’ (walk away-turn then
pause for 10seconds-walk back). The system collected time-
series data sampled at the default rate of approximately 2
s-1. Fluctuating LQI data generated from static, rotational
and dynamic walk phases was captured (Figs. 2 & 3). In
the Walk ’A’ graph at Figure 2, the three individual phases
of the motion are clearly identiﬁable: the pre-motion phase
commences with a steady state, empty room ﬂat line signal
with no discernible noise and LQI of 255 representing the
maximum value. LQI values then fall during the ’walk away’
phase, bottom out during the ’turn’ phase, rising again during
the ’walk back’ phase, ﬁnally returning to the maximum
value and steady state. Data from other nodes has been
omitted for visual clarity. The Walk ’B’ graph at Figure
3 commences in similar fashion to the preceding graph.
However the three individual phases of the motion are less
clearly identiﬁable this time and there is less symmetry
between the ’walk away’ and ’walk back’ phases. LQI
values initially rise and return to roughly pre-motion values
during the quiescent ’turn, pause’ phase. During the ’walk
back’ phase, there is a brief dip followed by a gradual
return to pre-motion values before ﬁnally returning to the
maximum value and steady state. Again, data from other
nodes has been omitted for visual clarity. These early results
Figure 2. Walk ’A’ LQI ﬂuctuation for Node ’C’.
Figure 3. Walk ’B’ LQI ﬂuctuation for Node ’C’.
are promising and suggest that the use of the LQI metric is
adequately responsive to differentiate characteristic phases
of linear motion, rotational motion and no motion. Further,
visual analysis of the graphs derived from the sensor data
support the idea that speciﬁc activities produce uniquely
identiﬁable patterns which may be classiﬁed by machine
learning techniques.
V.
PLANNED STUDIES
The next stage of research involves undertaking formally
speciﬁed human motion studies which will be undertaken
to implement basic algorithms already developed by others.
The initial smart environment to develop and test the WSN
for human activity detection will be deployed in the UHRH
[21]. The UHRH is dedicated to Human-Robot Interaction
(HRI) research in an ecologically realistic, domestic environ-
ment. It has the appearance of an ordinary British suburban,
semi-detached house in a quiet residential street, with four
fully-furnished bedrooms and a sizeable garden (see Figure
356
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

4). The UHRH is also inhabited by different robots designed
for robot companion research. It also has a large number of
embedded sensors, most connect via a WSN, allowing the
recording of data from a range of different user activities.
A. Scaled WSN deployment
Initially, the number of nodes in the network will be
increased from 5 to 8 and deployment will be limited to
the ’living area’ of the UHRH in order to collect basic data
on a restricted set of activities. Later, in order to extend
the sensor area coverage to include additional areas with
different ADL, we intend to extend the area covered by the
network to include up to 20 nodes.
Figure 4. The University of Hertfordshire Robot House showing an HRI
experiment with a Care-O-Bot 3 robot.
B. Input data
The input data can be conceptualised as a sequence of
vectors mainly consisting of binary sensor data from the
simple sensors (reed switches, pressure mats, water ﬂow etc.)
and more complex data structures for the RTI, 3-D video and
audio streams.
C. RTI data processing pipeline
Following LQI data capture, digital signal processing
techniques may be applied to form radio tomographic images
in real-time according to the methodology of Bocca et
al. [5] in a ﬁve-stage pipeline: (i) image estimation, (ii)
image denoising, (iii) cluster heads selection, (iv) tracks
conﬁrmation and deletion, (v) target tracking.
VI.
CONCLUSION AND FUTURE WORK
Previous research at the UHRH has been undertaken
on the design and implementation of a low-cost, resource-
efﬁcient activity recognition system that can detect ADL
[22]. Following correlation with existing activity recognition
collected from previously published RH studies, we intend
to fuse data from existing sources/sensors in the UHRH with
the RTI data based on the method developed by Brdiczka et
al. [6], but using RTI data both separately and fused with 3D
Tracker data. This will permit us to compare and contrast the
technical performance of the two radio and optical imaging
systems. We have demonstrated that we have obtained data
from the ZigBee WSN, which has the potential to detect
human movements. The WSN has been constructed from
standard low-cost components of the type already used to
connect sensors to UHRH systems etc. This raw data capture
represents the essential ﬁrst stage prior to implementing
the RTI data processing pipeline methodology described by
Bocca et al. and summarised at Section 5 above. Further,
we have modiﬁed the approach taken by Bocca et al. and
exploited the availability of the LQI metric in order to
attempt an improvement in the reliability of the raw WSN
data in an otherwise noisy 2.4 GHz environment (see Section
2 above for a technical discussion of the RSSI/LQI metrics).
Graphical analysis of the initial data appears to demonstrate
stable waveforms which do ﬂuctuate appropriately in re-
sponse to motion but are otherwise unperturbed (ﬂat line,
LQI value close to 255). In the next stage of research we plan
to test the stability of the LQI sensor data in the noisy UHRH
environment. There are several areas to which this research
intends to make a contribution, namely HCI, Human-Robot
Interaction (HRI) and the application of artiﬁcial intelligence
(AI). The works of Patwari and Bocca have shown that
RTI as a localisation and tracking technology can be used
successfully to enable accurate real-time multiple target
tracking with RF sensor networks. Concomitantly, the work
of Brdiczka has shown that although a 3-D video tracking
system is capable of creating and tracking entities (persons)
in a smart environment, this is achieved not only at a
substantially higher capital and computational cost than RTI,
but also fails to adequately address the issue of the detection
of group dynamics and group formation, which is necessary
for group situation recognition. The proposed RTI methods
aimed at circumventing the performance problems of 3-D
video tracking systems have been shown to be successful
in other tracking and localisation situations at reducing
computation and memory costs.
The major unknown is to what extent RTI is capable
of supplanting current optical data acquisition technologies
at the machine learning level. A widely used technique for
the analysis of time series data typically involves the use of
feed-forward Artiﬁcial Neural Networks (ANN) employing a
sliding window technique applied over the input sequence.
However, before approaching the challenges of collecting
time series data and subsequent processing via an appro-
priately selected ANN, there is a requirement to identify
the optimal sample rate, embedding dimension and size of
the input window [23]. In our approach to the problem
of recognising activities we will consider implementing a
real-time activity recognition digital signal processing (DSP)
pipeline inspired by the methodology of [24]. Broadly, the
pipeline architecture is envisioned in ﬁve principal stages. In
the ﬁrst stage raw LQI data is acquired from the WSN. Next,
an aggregation step ﬁrst loads the acquired live data and then
initiates the downsampling, ﬁltering and segmenting opera-
tions. Live LQI data may be written to a structured text ﬁle
which may be conveniently parsed and loaded. In the third
stage, windows of interest are identiﬁed from the loaded
structured data by running a peak detection process. The
windows of interest are slices of the LQI data corresponding
to motion. Features are then extracted from the windows of
interest which are then used to describe particular forms of
357
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

motion (i.e. speciﬁc types of activities). In the penultimate
stage, these features of interest are employed to train a
recognition model for the movements detected and input.
It is intended to generate a number of recognition models
by exposing the data to differing classiﬁers in order to as-
certain the optimal classiﬁer for this time series motion data.
Examples of potentially suitable classiﬁer algorithms which
are currently under consideration include Support Vector
Machines, K-Nearest Neighbours, Decision Trees, Random
Forest and Growing Neural Gas. In the ﬁnal pipeline stage
classiﬁcation and validation operations will use classiﬁers
and the dataset to perform cross-validation. Initially we
intend to perform model training and classiﬁcation ofﬂine
by decomposing the dataset into a training set and a test
set. The training set is fed into the various classiﬁers. The
accuracy of each of the classiﬁers may be evaluated by
comparing the classiﬁer output predictions with the ground
truth represented by the test set data.
We intend to use the Python programming language to
initially implement all of the ﬁve stages of the pipeline de-
scribed above. Python provides a number of excellent mature
packages and libraries for DSP and machine learning includ-
ing NumPy [25] and scikit-learn [26]. In the longer term, it
would be desirable to develop the capability for our system
to avoid the ofﬂine feature extraction and classiﬁer training
phase. In this respect the use of unsupervised learning of
deep belief networks (Deep Learning) for generative pre-
training of stacked Restricted Boltzmann Machines followed
by supervised ﬁne-tuning may be of interest. The Theano
Python Library [27] offers tight integration with NumPy and
signiﬁcant performance gains through transparent use of a
GPU.
REFERENCES
[1]
M. Weiser, “The computer for the 21st century,” Scientiﬁc American,
vol. 265, no. 3, 1991, pp. 94–104.
[2]
M. Weiser, R. Gold, and J. S. Brown, “The origins of ubiquitous
computing research at parc in the late 1980s,” IBM systems journal,
vol. 38, no. 4, 1999, p. 693.
[3]
I. F. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci,
“Wireless sensor networks: a survey,” Computer networks, vol. 38,
no. 4, 2002, pp. 393–422.
[4]
J. Wilson and N. Patwari, “Radio tomographic imaging with wireless
networks,” Mobile Computing, IEEE Transactions on, vol. 9, no. 5,
2010, pp. 621–632.
[5]
M. Bocca, O. Kaltiokallio, N. Patwari, and S. Venkatasubramanian,
“Multiple target tracking with RF sensor networks,” Mobile Com-
puting, IEEE Transactions on, vol. 13, no. 8, 2014, pp. 1787–1800.
[6]
O. Brdiczka, M. Langet, J. Maisonnasse, and J. L. Crowley, “Detect-
ing human behavior models from multimodal observation in a smart
home,” Automation Science and Engineering, IEEE Transactions on,
vol. 6, no. 4, 2009, pp. 588–597.
[7]
University of Ottawa. Activities of daily living (adl). [Online]. Avail-
able: www.medicine.uottawa.ca/sim/data/Disability_ADL_e.htm [re-
trieved: March, 2016]
[8]
D. Cook and S. Das, “Smart environments: Technologies, protocols,
and applications,” Hoboken: John Wileyand Sons, 2005.
[9]
R. J. Orr and G. D. Abowd, “The smart ﬂoor: a mechanism for natural
user identiﬁcation and tracking,” in CHI’00 extended abstracts on
Human factors in computing systems.
ACM, 2000, pp. 275–276.
[10]
X. Mao, S. Tang, X. Xu, X.-Y. Li, and H. Ma, “ilight: Indoor device-
free passive tracking using wireless sensor networks,” in INFOCOM,
2011 Proceedings IEEE.
IEEE, 2011, pp. 281–285.
[11]
A. Fink and H. Beikirch, “Device-free localization using redundant
2.4 ghz radio signal strength readings,” in Indoor Positioning and
Indoor Navigation (IPIN), 2013 International Conference on. IEEE,
2013, pp. 1–7.
[12]
A. Lin and H. Ling, “Doppler and direction-of-arrival (DDOA) radar
for multiple-mover sensing,” Aerospace and Electronic Systems,
IEEE Transactions on, vol. 43, no. 4, 2007, pp. 1496–1509.
[13]
L.-P. Song, C. Yu, and Q. H. Liu, “Through-wall imaging (TWI)
by radar: 2-D tomographic results and analyses,” Geoscience and
Remote Sensing, IEEE Transactions on, vol. 43, no. 12, 2005, pp.
2793–2798.
[14]
J. Wilson and N. Patwari, “See-through walls: Motion tracking using
variance-based radio tomography networks,” Mobile Computing,
IEEE Transactions on, vol. 10, no. 5, 2011, pp. 612–621.
[15]
T. W. Hnat, E. Grifﬁths, R. Dawson, and K. Whitehouse, “Doorjamb:
unobtrusive room-level tracking of people in homes using doorway
sensors,” in Proceedings of the 10th ACM Conference on Embedded
Network Sensor Systems.
ACM, 2012, pp. 309–322.
[16]
Y. Kilic, H. Wymeersch, A. Meijerink, M. J. Bentum, and W. G.
Scanlon, “Device-free person detection and ranging in UWB net-
works,” Selected Topics in Signal Processing, IEEE Journal of, vol. 8,
no. 1, 2014, pp. 43–54.
[17]
F. Fleuret, J. Berclaz, R. Lengagne, and P. Fua, “Multicamera people
tracking with a probabilistic occupancy map,” Pattern Analysis and
Machine Intelligence, IEEE Transactions on, vol. 30, no. 2, 2008,
pp. 267–282.
[18]
G. Okeyo, L. Chen, H. Wang, and R. Sterritt, “Dynamic sensor data
segmentation for real-time knowledge-driven activity recognition,”
Pervasive and Mobile Computing, vol. 10, 2014, pp. 155–172.
[19]
S. J. Halder, J.-G. Park, and W. Kim, Adaptive ﬁltering for indoor
localization using ZIGBEE RSSI and LQI measurement.
INTECH
Open Access Publisher, 2011.
[20]
Digi
International
Inc.
Digi
docs:
Product
documentation
and
manuals
from
digi
international.
[Online].
Avail-
able:
www.digi.com/resources/documentation/digidocs/default.htm
[retrieved: March, 2016]
[21]
Lehmann, H., et al., “Artists as HRI pioneers: a creative approach
to developing novel interactions for living with robots,” in Social
robotics.
Springer, 2013, pp. 402–411.
[22]
I. Duque, K. Dautenhahn, K. L. Koay, L. Willcock, and B. Christian-
son, “Knowledge-driven user activity recognition for a smart house
– development and validation of a generic and low-cost, resource-
efﬁcient system,” in In Proc. Sixth International Conference on
Advances in Computer-Human Interactions.
Citeseer, 2013.
[23]
R. J. Frank, N. Davey, and S. P. Hunt, “Time series prediction and
neural networks,” Journal of intelligent and robotic systems, vol. 31,
no. 1-3, 2001, pp. 91–103.
[24]
T. Peters, “An Assessment of Single-Channel EMG Sensing
for
Gestural
Input.”
Dartmouth
College,
Computer
Science,
Hanover,
NH,
Tech.
Rep.
TR2015-767,
September
2014.
[Online]. Available: http://www.cs.dartmouth.edu/reports/TR2015-
767.pdf [retrieved: March 2016]
[25]
S. Van Der Walt, S. C. Colbert, and G. Varoquaux, “The numpy
array: a structure for efﬁcient numerical computation,” Computing
in Science & Engineering, vol. 13, no. 2, 2011, pp. 22–30.
[26]
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of
Machine Learning Research, vol. 12, 2011, pp. 2825–2830.
[27]
F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow,
A. Bergeron, N. Bouchard, and Y. Bengio, “Theano: new features
and speed improvements,” Deep Learning and Unsupervised Feature
Learning NIPS 2012 Workshop, 2012.
358
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

