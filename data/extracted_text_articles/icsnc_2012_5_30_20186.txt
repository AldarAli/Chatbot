Face Recognition Algorithm Using Muti-direction Markov Stationary Features and 
Adjacent Pixel Intensity Difference Quantization Histogram 
 
Feifei Lee, Koji Kotani*, Qiu Chen, and Tadahiro Ohmi 
New Industry Creation Hatchery Center, Tohoku University 
* Department of Electronics, Graduate School of Engineering, Tohoku University 
Aza-Aoba 6-6-10, Aramaki, Aoba-ku, Sendai 980-8579, JAPAN 
e-mail: fei@fff.niche.tohoku.ac.jp 
 
 
Abstract—We have proposed a robust face recognition 
algorithm 
using 
adjacent 
pixel 
intensity 
difference 
quantization (APIDQ) histogram combined with Markov 
Stationary Features (MSF) so as to add spatial structure 
information to histogram features in our previous work. We 
named the new histogram feature as MSF-DQ feature. In this 
paper, we extend original MSF to multi-direction MSF by 
generating co-occurrence matrices with orientations of 0, 45, 
90, 135 degrees, and then extract corresponding MSF-DQ 
features for every direction. Publicly available AT&T database 
of 40 subjects with 10 images per subject containing variations 
in lighting, posing, and expressions, is used to evaluate the 
performance of the proposed algorithm. Experimental results 
show face recognition using proposed multi-direction MSF-DQ 
features is more efficient compared with the original algorithm.  
Keywords-Face 
recognition; 
Adjacent 
pixel 
intensity 
difference quantization (APIDQ); Markov stationary feature 
(MSF); Multi-direction; Histogram feature. 
I. 
 INTRODUCTION 
As a more natural and effective person identification 
method compared with that using other biometric features 
such as voice, fingerprint, iris pattern, etc., a lot of face 
recognition algorithms have been proposed [1]-[14] in the 
last two decades. These algorithms can be mainly 
categorized into two groups, that is to say, structure-based 
and statistics-based. 
In the structure-based approaches [3][4], recognition is 
based on the relationship between human facial features such 
as eye, mouth, nose, profile silhouettes and face boundary. 
Statistics-based approaches [5][6][7] attempt to capture and 
define the face as a whole. The face is treated as a two 
dimensional pattern of intensity variation. Under this 
approach, the face is matched through finding its underlying 
statistical regularities. Principal component analysis (PCA) is 
a typical statistics-based technique [5]. However, these 
techniques are highly complicated and are computationally 
power hungry, making it difficult to implement them into 
real-time face recognition applications.  
In [18][19], a very simple, yet highly reliable face 
recognition 
method 
called 
Adjacent 
Pixel 
Intensity 
Difference Quantization (APIDQ) Histogram Method is 
proposed, which achieved the real-time face recognition. At 
each pixel location in an input image, a 2-D vector 
(composed of the horizontally adjacent pixel intensity 
difference (dIx) and the vertically adjacent difference (dIy)) 
contains information about the intensity variation angle () 
and its amount (r).  After the intensity variation vectors for 
all the pixels in an image are calculated and plotted in the r- 
plane, each vector is quantized in terms of its   and r values.  
By counting the number of elements in each quantized area 
in the r-  plane, a histogram can be created.  This histogram, 
obtained by APIDQ for facial images, is utilized as a very 
effective personal feature. Experimental results show a 
recognition rate of 95.7 % for 400 images of 40 persons (10 
images per person) from the publicly available AT&T face 
database [20].  
Li et al. [19] proposed Markov stationary feature (MSF), 
which can encode the relationships of intra-bin and inter-bin 
into histograms. Motivated by this consideration, we 
combine the APIDQ histogram with Markov stationary 
feature (MSF), so as to encode spatial structure information 
within and between histogram bins [17][18]. The MSF 
extends the APIDQ histogram features by characterizing the 
spatial co-occurrence of histogram patterns using the Markov 
chain models and improves the distinguishable capability of 
APIDQ features to extra-bin distinguishable level [19]. 
Experimental results demonstrated that the algorithm using 
the MSF-DQ features is more robust for face recognition 
evaluated by using the publicly available database of AT&T 
[20].  
Pixel pairs in all directions are counted to generate a 
single co-occurrence matrix in original MSF algorithm. 
Considering that the co-occurrence matrices have been 
widely used in as a feature in registration and segmentation 
problems [23][24][24], we extend original MSF to multi-
direction MSF by generating co-occurrence matrices with 
orientations of 0, 45, 90, 135 degrees, and then extract 
corresponding MSF-DQ feature for each direction. Therefore, 
more comprehensive personal feature information can be 
obtained by using multi-direction MSF-DQ features, which 
is named MDMSF-DQ. 
In Section II, we will first introduce Markov stationary 
feature (MSF) as well as the Adjacent Pixel Intensity 
Difference Quantization (APIDQ) histogram feature which 
had been successfully applied to face recognition previously, 
and then describe proposed face recognition algorithm using 
multi-direction MSF-DQ features (MDMSF-DQ) in Section 
113
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-231-8
ICSNC 2012 : The Seventh International Conference on Systems and Networks Communications

III. Experimental results will be discussed in Section IV. 
Finally, conclusions will be given in Section V. 
II. 
RELATED WORKS 
A. Markov Stationary features (MSF) 
The Markov stationary feature (MSF) [19] extends the 
APIDQ histogram features by characterizing the spatial co-
occurrence of histogram patterns using the Markov chain 
models and improves the distinguishable capability of 
APIDQ features to extra-bin distinguishable level. We will 
briefly introduce the MSF in this section. 
Let 
kp  be a pixel in image I, the spatial co-occurrence 
matrix is defined as 
K K
ijc
C


)
(
  where 
 
/) 2
|
|
,
#(
2
1
2
1
d
p
p
c
p
c
p
c
j
i
ij





, 
      (1) 
 
in which d (d=1 in this paper) indicates 
1L  distance 
between two pixels 
1p  and 
2
p , and 
ijc  counts the number 
of spatial co-occurrence for bin 
ic  and 
jc .  
The co-occurrence matrix 
ijc  can be interpreted in a 
statistical view. Markov chain model is adopted to 
characterize the spatial relationship between histogram bins. 
The bins are treated as states in Markov chain models, 
and the co-occurrence is viewed as the transition probability 
between bins. In this way, the MSF can transfer the 
comparison of two histograms to two corresponding 
Markov chains. 
  The elements of the transition matrix P are constructed 
from the spatial co-occurrence C by formula (2). 
 



K
j
ij
ij
ij
c
c
P
1
/
 
 
 
 
     (2) 
 
The state distribution after n steps is defined as
 (n)
, 
and the initial distribution is
(0)
, the Markov transition 
matrix obeys following rules. 
 
n P
n
( )
)1
(




, 
Pn
n
(0)
( )



;  
     (3) 
n
m
m n
P P
P


 
 
where 
(0)
 is defined as  



K
i
ii
ii
c
c
1
/
(0)
 
 
 
 
     (4) 
 
According to the formula (3), we can get a distribution 
of   called a stationary distribution which satisfies 
 
  P
 
 
 
 
 
     (5) 
 
The 
stationary 
distribution 
becomes 
the 
final 
representation of MSF. Obtaining the MSF of each image, 
the comparison of two histograms is transferred to the 
comparison of two corresponding Markov chains. 
 
B. Adjacent Pixel Intensity Difference Quantization 
(APIDQ) 
The Adjacent Pixel Intensity Difference Quantization 
(APIDQ) histogram method [15] has been developed for 
face recognition previously. Figure 1 shows the processing 
steps of APIDQ histogram method.  In APIDQ, for each 
pixel of an input image, the intensity difference of the 
horizontally adjacent pixels (dIx) and the intensity 
difference of the vertically adjacent pixels (dIy) are first 
calculated by using simple subtraction operations shown as 
formula (6). 
 
( , )
)1
( ,
( , )
( , )
)
,1
(
( , )
I i j
I i j
dIy i j
I i j
j
I i
dIx i j






     
 
     (6) 
 
A calculated (dIx, dIy) pair represents a single vector in 
the dIx-dIy plane. By changing the coordinate system from 
orthogonal coordinates to polar coordinates, the angle   and 
the distance r represent the direction and the amount of 
intensity variation, respectively. After processing all the 
pixels in an input image, the dots representing the vectors 
are distributed in the dIx-dIy plane. The distribution of dots 
(density and shape) represents the features of the input 
image.  
Each intensity variation vector is then quantized in the r-
  plane. Quantization levels are typically set at 8 in  -axis 
and 8 in r-axis (totally 50). Since dIx-dIy vectors are 
concentrated in small-r (small-dIx, -dIy) region, non-
uniform quantization steps are applied in r-axis. The number 
Low-Pass Filtering (2-D Ave.)
Adjacent Pixel Intensity 
Differentiation (dIx, dIy)
Coordinates Change (r-)
Quantization
Histogram Generation
Input Image
APIDQ
 
 
Figure 1.   Processing steps of APIDQ histogram method. 
114
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-231-8
ICSNC 2012 : The Seventh International Conference on Systems and Networks Communications

of vectors quantized in each quantization region is counted 
and a histogram is generated. In the face recognition 
approach, this histogram becomes the feature vector of the 
human face.  
The essence of the APIDQ histogram method can be 
considered that the operation detects and quantizes the 
direction and the amount of intensity variation in the image 
block. Hence the APIDQ histogram contains very effective 
image feature information.  The MSF extends histogram 
based features with spatial structure information of images, 
and transfer the comparison of two histograms to two 
corresponding Markov chains.  
 
III. 
PROPOSED FACE RECOGNITION ALGORITHM 
A. Generation of 4 directions co-occurrence matrices 
Pixel pairs in all directions are counted to generate a 
single co-occurrence matrix in original MSF algorithm. In 
this paper, we extend original MSF to multi-direction MSF 
by generating co-occurrence matrices with orientations of 0, 
45, 90, 135 degrees as shown in Figure 2.  
Because different MSF-DQ features are extracted with 
different direction co-occurrence matrices of the image, more 
comprehensive personal feature information can be obtained 
by combining multiple recognition results using 4 direction 
co-occurrence matrices.  
In this paper, we employ 4 direction co-occurrence 
matrices for the facial image to extract more powerful 
personal feature. As shown in Figure 2, after APIDQ 
processing is carried out, MSF-DQ features at different 
directions are extracted from corresponding co-occurrence 
matrices. Recognition results are firstly obtained using MSF-
DQ features at different directions separately and then 
combined by weighted averaging. 
 
B. Proposed algorithm 
The procedure of proposed face recognition algorithm 
using APIDQ histogram combined with MSF is shown in 
Figure 3. Low-pass filtering is first carried out before 
APIDQ using a simple 2-D moving average filter. This low-
pass filtering is essential for reducing the high-frequency 
noise and extracting the most effective low frequency 
component for recognition. Then original APIDQ is 
implemented and quantization region number corresponding 
to each 2x2 image block is calculated. As shown in Figure 3, 
because each 2x2 image block can be regarded as a pixel of 
color
ic , the co-occurrence matrix for APIDQ can be 
computed according to formula (1). But instead of counting 
pixel pairs in all directions to generate a single co-
occurrence matrix in original MSF algorithm, co-occurrence 
matrices at 4 different directions of 0, 45, 90, 135 degrees 
are generated. 
The Markov transition matrix P for each direction of co-
occurrence matrix is calculated by formula (2). Then the 
stationary distribution of corresponding direction can be 
approximated by the average of each row 
ia
  of
n
A  using 
formula (7). 
 




K
i
ia
K
1
1 /

, where 
T
k
n
a
a
A
]
,
,
[
1




, 
     (7) 
 
)
1(
1
2
n
n
P
P
P
I
n
A







  
     (8) 
 
0°
45°
90°
135°
0°
45°
90°
135°
 
 
Figure 2.   Extraction of Multi-direction  pixel pairs . 
Facial Image
APIDQ
Co-occurrence matrices generation 
for 4 directions (0,45,90,135 deg.)
Markov transition matrices 
generation
Initial and Stationary distributions 
calculation
Multi-direction MSF-DQ features
Low-Pass Filtering (2-D Ave.)
Database Matching (MD)
Database
Recognition Result
Recognition
Registration
 
Figure 3.   Proposed face recognition algorithm using multi-
direction MSF-DQ features (MDMSF-DQ). 
115
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-231-8
ICSNC 2012 : The Seventh International Conference on Systems and Networks Communications

  n =50 is used as same as in [19]. The initial distribution 
 (0)
 can be obtained by formula (4). As shown in formula 
(9), the Markov stationary feature in each direction is 
defined as the combination of the initial distribution 
 (0)
 
and the stationary distribution  after n steps.  
 
T
hMSF DQ
[ (0), ]





 
 
 
     (9) 
 
We call MSF extension of APIDQ histogram MSF-DQ 
feature. The MSF-DQ feature made from each direction is 
compared with those from the same direction in the database 
by calculating distances (di) between them using the same 
distance calculation formula as in [19]. Then the integrated 
distances (D) are obtained by weighted averaging as shown 
in the following formula (10).   
 
       



i
i
i
w
w d
D
      
 
 
 
     (10) 
 
where wi is weighting coefficient of the different directions, 
The best match is output as recognition result by searching 
the minimum integrated distance. 
IV. 
EXPERIMENTAL RESULTS AND DISCUSSIONS 
A. Data sets 
The publicly available face database of AT&T 
Laboratories Cambridge [20], [21]is used for the analysis 
and recognition experiments.  Forty people with 10 facial 
images each, (totaling 400 images), with variations in face 
angles, facial expressions, and lighting conditions are 
included in the database.  Each image has a resolution of 
92x112.  Figure 4 shows typical image samples of the 
database of AT&T Laboratories Cambridge.  From the 10 
images for each person, five were selected as probe images 
and the remaining five were registered as album images.  
Recognition experiments were carried out for 252 (10C5) 
probe-album combinations using the rotation method. 
B.  Experimental results 
Comparison of recognition results are shown in Figure 5.  
Recognition success rates are shown as a function of filter 
size.  The filter size represents the size of the averaging 
filter core.  A size of F3, for instance, represents the filter 
using a 3x3 filter core. Figure 5 shows the comparison 
between the recognition results using different direction 
MSF-DQ features separately and multi-direction MSF-DQ 
features. Average recognition rate is shown here. “bin 50 
(original DQ)” stands for the case that original APIDQ 
utilizes quantization table containing the number of bins of 
50 in [15][16]. “bin50_all_directions” stands for the case 
using pixel pairs in all directions counted to generate a 
single co-occurrence matrix in original MSF algorithm. 
“bin50_0:1:0:1”,  “bin50_1:0:1:0”, and “bin50_1:1:1:1”, 
stand for the cases using combination of various direction 
MSF-DQ features of 0, 45, 90, 135 degrees respectively, 
which weighting coefficient at each direction level is set as 
0 or 1. 
The best performance of the average recognition rate 
96.9% is obtained at original image size of 92x112 when 
using all-direction MSF-DQ features. By using multi-
direction MSF-DQ features with the weighting coefficient at 
each direction level of 1, highest recognition rate increases 
to 97.37%. It can be said that multi-direction MSF-DQ 
features is more robust than original MSF-DQ features. We 
notice that the case that only using the combination of 0, 90 
degrees or the combination of 45, 135 degrees can achieve 
similar recognition accuracy with that using 4 directions. 
Figure 5.   Comparison of results. Average recognition rate is    
shown here. 
 
 
Figure 4. Samples of the database of AT&T Laboratories 
Cambridge.  
Figure 6.   Comparison of results. Maximum average 
recognition rate is shown here. 
116
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-231-8
ICSNC 2012 : The Seventh International Conference on Systems and Networks Communications

      Figure 6 shows comparison results of the maximum 
average recognition rate using some combinations. 
Maximum of the average recognition rate 97.5% is achieved 
at the combination of weighting coefficients of 2:2:1:1 for 0, 
45, 90, 135 degrees.  
V. 
CONCLUSION AND FUTURE WORK 
In this paper, we improved our face recognition using 
multi-direction MSF-DQ feature by generating co-
occurrence matrices with orientations of 0, 45, 90, 135 
degrees, and then extract corresponding MSF-DQ feature 
for each direction multi-direction for the facial image to 
extract more powerful personal feature. Excellent face 
recognition performance as large as a 97.5% recognition 
rate has been achieved by using the publicly available 
database of AT&T. It can be said that multi-direction MSF-
DQ features is more is more robust for face recognition. 
Because AT&T database is not a large face database, we 
will evaluate our proposed algorithm for practical 
application by using large database of FERET in our future 
work. 
ACKNOWLEDGMENT 
This research was partially supported by the Ministry of 
Education, Culture, Sports, Science and Technology of 
Japan, Grant-in-Aid for Scientific Research (C), No. 
24500104, 2012-2015, and also by research grant from 
Support 
Center 
for 
Advanced 
Telecommunications 
Technology Research, Foundation (SCAT). 
 
REFERENCES 
[1] R. Chellappa, C. L. Wilson, and S. Sirohey, “Human and 
machine recognition of faces: a survey,” Proc. IEEE, vol. 83, 
no. 5, 1995, pp.705-740. 
[2] S. Z. Li and A. K. Jain, “Handbook of face recognition,” 
Springer, New York, 2005. 
[3] R. Brunelli and T. Poggio, “Face recognition: features versus 
templates,” IEEE Trans. on Pattern Analysis and Machine 
Intelligence, vol. 15, no. 10, Oct. 1993, pp. 1042-1052. 
[4] L. Wiskott, J. M. Fellous, N. Kruger, and C. Malsburg, “Face 
recognition by elastic bunch graph matching,” IEEE Trans. on 
Pattern Analysis and Machine Intelligence, vol. 19, no. 10, 
1997, pp. 775-780. 
[5] M. Turk and A. Pentland, “Eigenfaces for recognition,” 
Journal of Cognitive Neuroscience, vol. 3, no. 1, Mar. 1991, 
pp. 71-86. 
[6] W. Zhao, “Discriminant component analysis for face 
recognition,” Proc. in the Int’l Conf. on Pattern Recognition 
(ICPR’00), Track 2, 2000, pp. 822-825. 
[7] K.M. Lam and H. Yan, “An analytic-to-holistic approach for 
face recognition based on a single frontal view,” IEEE Trans. 
on Pattern Analysis and Machine Intelligence, vol. 20, no. 7, 
1998, pp. 673-686. 
[8] M.S. Bartlett, J.R. Movellan, and T.J. Sejnowski, “Face 
recognition by independent component analysis,” IEEE Trans. 
on Neural Networks, vol. 13, no. 6, 2002, pp. 1450-1464. 
[9] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, 
“Eigenface vs. Fisherfaces: Recognition using class specific 
linear projection,” IEEE Trans. on Pattern Analysis and 
Machine Intelligence, vol. 19, May 1997, pp. 711-720. 
[10] B. Moghaddam and A. Pentland, “Probabilistic visual 
learning for object representation,” IEEE Trans. on Pattern 
Analysis and Machine Intelligence, vol. 19, no. 7, 1997, pp. 
696-710. 
[11] S. G. Karungaru, M. Fukumi, and N. Akamatsu, “Face 
recognition in colour images using neural networks and 
genetic 
algorithms,” 
Int’l 
Journal 
of 
Computational 
Intelligence and Applications, vol. 5, no. 1, 2005, pp. 55-67. 
[12] Z. Liu and C. Liu, “Fusion of color, local spatial and global 
frequency 
information 
for 
face 
recognition,” 
Pattern 
Recognition, vol. 43, Issue 8, Aug. 2010, pp. 2882-2890. 
[13] H. F. Liau, K. P. Seng, L. M. Ang, and S. W. Chin, “New 
parallel models for face recognition,” Recent Advances in 
Face Recognition, Edited by K. Delac etc., InTech,  2008. 
[14] Q. Chen, K. Kotani, F. F. Lee, and T. Ohmi, “Face 
recognition using VQ histogram in compressed DCT 
domain,” Journal of Convergence Information Technology, 
vol. 7, no. 1, 2012, pp. 395-404. 
[15] K. Kotani, F. F. Lee, Q. Chen, and T. Ohmi, “Face 
recognition based on the adjacent pixel intensity difference 
quantization histogram method,” 2003 Int’l Symp. on 
Intelligent Signal Processing and Communication Systems, 
D7-4, 2003, pp. 877-880. 
[16] F. F. Lee, K. Kotani, Q. Chen, and T. Ohmi, “Face 
recognition 
using 
adjacent 
pixel 
intensity 
difference 
quantization histogram,” Int’l Journal of Computer Science & 
Network Security, vol. 9, no. 8, 2009, pp. 147-154. 
[17] F. F. Lee, K. Kotani, Q. Chen, and T. Ohmi, “A robust face 
recognition algorithm using Markov stationary features and 
adjacent pixel intensity difference quantization histogram,” 
Proc. in 7th Int’l Conf. on Signal Image Technology & 
Internet Based Systems (SITIS 2011), France, 2011, pp. 334-
339. 
[18] F. F. Lee, K. Kotani, Q. Chen, and T. Ohmi, “Face 
recognition 
using 
adjacent 
pixel 
intensity 
difference 
quantization histogram combined with Markov stationary 
features,” Int’l Journal of Advancements in Computing 
Technology, vol. 4, no. 12, 2012, pp. 327-335. 
[19] J. Li, W. Wu, T. Wang, and Y. Zhang, “One step beyond 
histograms: Image representation using Markov stationary 
features,” Proc. in the IEEE Conf. on Computer Vision and 
Pattern Recognition (CVPR’08) , 2008, pp. 1-8. 
[20] AT&T Laboratories Cambridge, The Database of Faces, at 
http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.
html. 
[21] F. Samaria and A. Harter, “Parameterisation of a stochastic 
model for human face identification,” 2nd IEEE Workshop on 
Applications of Computer Vision, 1994, pp. 138-142. 
[22] P. J. Phillips, H. Wechsler, J. Huang, and P. Rauss, “The 
FERET database and evaluation procedure for face 
recognition algorithms,” Image and Vision Computing J., vol. 
16, no. 5, 1998, pp. 295-306. 
[23] J. Malone, S. Prabhu, and P. Goddard, “The use of co-
occurrence features in medical imaging: an empirical study,” 
Visualization, Imaging, and Image Processing, 2005. 
[24] S. Hentschel, F. Kruggel, “Segmentation of the intracranial 
compartment: a registration approach,” Medical Imaging and 
Augmented Reality (Beijing), Lecture Notes in Computer 
Science vol. 3150, 2004, pp. 253-260. 
[25] P. Maji, M.K. Kundu, and B. Chanda, “Segmentation of brain 
MR images using Fuzzy sets and modified co-occurrence 
matrix,” IET Int’l Conf. on Visual Information Engineering 
(VIE 2006),  Sep. 2006, pp. 327-332. 
 
117
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-231-8
ICSNC 2012 : The Seventh International Conference on Systems and Networks Communications

