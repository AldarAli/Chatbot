An Intrinsic Difference Between
Vanilla RNNs and GRU Models
Tristan St¬¥erin
Computer Science Department
¬¥Ecole Normale Sup¬¥erieure de Lyon
Email: tristan.sterin@ens-lyon.fr
Nicolas Farrugia
Electronics Department
IMT Atlantique
Email: nicolas.farrugia@imt-atlantique.fr
Vincent Gripon
Electronics Department
IMT Atlantique
Email: vincent.gripon@imt-atlantique.fr
Abstract‚ÄîIn order to perform well in practice, Recurrent
Neural Networks (RNN) require computationally heavy archi-
tectures, such as Gated Recurrent Unit (GRU) or Long Short
Term Memory (LSTM). Indeed, the original Vanilla model fails to
encapsulate middle and long term sequential dependencies. The
aim of this paper is to show that gradient training issues, which
have motivated the introduction of LSTM and GRU models, are
not sufÔ¨Åcient to explain the failure of the simplest RNN. Using
the example of Reber‚Äôs grammar, we propose an experimental
measure of both Vanilla and GRU models, which suggest an
intrinsic difference in their dynamics. A better mathematical
understanding of this difference could lead to more efÔ¨Åcient
models without compromising performance.
Index Terms‚ÄîRecurrent Neural Networks; Gradient Backprop-
agation; Grammatical Inference; Dynamical Systems.
I. INTRODUCTION
Recurrent Neural Networks (RNN) [1] are a class of artiÔ¨Å-
cial neural networks that feature connections between hidden
layers that are propagated through time in order to learn
sequences. In recent years, such networks, and especially
variants such as Gated Recurrent Units (GRU) [2] or Long
Short-Term Memory (LSTM) networks [3] have demonstrated
remarkable performance in a wide range of applications in-
volving sequences, such as language modeling [4], speech
recognition [5], image captioning [6], and automatic trans-
lation [7]. Such networks often include a large number of
layers (i.e., deep neural networks), each containing many
neurons, resulting in a large set of parameters to be learnt.
The main reason explaining this challenge are vanishing or
exploding gradients while training parameters [8], [9], such
problems being likely to emerge when learning any large set
of parameters.
The conceptual leap from Vanilla RNN architectures to
LSTM or GRU was initially justiÔ¨Åed by the inability of Vanilla
RNN to learn long term sequential dependencies [3]. Theoret-
ically, it is not clear whether this limitation of Vanilla RNN
is indeed due to training issues, or to an intrinsic limitation
in their architecture. In this paper, we propose to revisit this
question by confronting GRU and Vanilla RNN by suggesting
the existence of intrinsic theoretical differences between two
models. We use the Embedded Reber Grammar and introduce
an experimental measure based on dynamical systems theory,
in order to highlight a limitation of Vanilla RNN architectures
without encountering vanishing or exploding gradient issues.
The remainder of the paper is organized as follows. In
Section II, we present the two RNN models that we evaluate,
Vanilla RNN and GRU. Section III presents the Embedded Re-
ber Grammar. In Section IV, we perform a set of experiments
using both models, and highlight limitations of the Vanilla
RNN while no issues with gradients can be demonstrated.
in Section V, we introduce a discrepancy measure and show
how it can be used to point out the limitations of the Vanilla
RNN model. Finally, we give conclusions and perspectives in
Section VI.
II. RECURRENT NEURAL NETWORK MODELS
In this section, we introduce the two models of recurrent
neural networks we discuss in this paper, namely Vanilla
RNNs and GRU.
A. Vanilla RNNs
Vanilla is the Ô¨Årst model of recurrent artiÔ¨Åcial neural
networks that was introduced [1]. Relying on very simple
dynamics, this neural network is described with the following
set of equations, indexed by time-step t:
ht = (Uhxt + Whht
output of such a neural network depends on both the input xt
and the hidden state ht
(xt)1
(xt)2
(xt)3
(xt)4
yt
Hidden
layer at t
W
Hidden
layer
at t-1
Wh
Input
layer
O
Output
layer
Fig. 1. Vanilla RNN with n = 4, k = 5 and p = 1.
Fig. 2. Reber‚Äôs grammar automaton.
V. THE (L,K)-DISCREPANCY
We now introduce a formal way to measure the discrepancy
capability of a recurrent neural network. More precisely, we
introduce a positive quantity that illustrates the ability a model
has to propagate long term dependencies.
DeÔ¨Ånition V.1 ((l,k)-discrepancy). Let us consider:
 Two integers l; k 2 N,
 A binary alphabet (n = 2), symbolically represented by
0 and 1 ,
 A RNN ‚Äì either Vanilla RNN or GRU model ‚Äì compris-
ing k neurons,
 The words u = 0
l
Fig. 4. Heatmaps of the hidden layer for a 5-neurons Vanilla on different
observed sequences of a Reber‚Äôs grammar. Corresponding automaton‚Äôs states
are also listed.
bzh/IARIA17 RNNs/tree/master/lk-discrepancy/.
It shows that as soon as l becomes large, the discrepancy
ability of Vanilla RNNs becomes very close to 0, meaning that
the two binary sequences are indistinguishable.
In comparison, Figure 11 depicts the discrepancy for the
GRU model for various values of l and k. As we can see
here, the discrepancy does not goes to 0 for large values of
l, suggesting the ability of the dynamics of GRU to maintain
long term information in the hidden state of the network.
These proÔ¨Åles provide an intuitive explanation of the results
for the Reber‚Äôs grammar: the Vanilla could succeed in the Ô¨Årst
problem because of the really short term memory required and
deÔ¨Ånitely failed in the embedded case because of out-of-range
dependencies.
VI. CONCLUSIONS AND FUTURE RESEARCH
Through Reber‚Äôs grammar example we saw that Vanilla and
GRU differenciate themselves more than just through learning
and gradients arguments. This observation motivates future
mathematical research to formally understand their intrinsic
difference in term of dynamical systems. A good starting point
could reside in the Echo States Network (c.f. [11]) theory and
the mathematical tools it develops. Understanding this differ-
ence could lead to simpler RNN models than GRU/LSTM, less
computationally heavy, and with better long term abilities.
ACKNOWLEDGEMENTS
This work was funded in part by the CominLabs project
‚ÄúNeural Communication‚Äù.
REFERENCES
[1] Pineda and F. J., ‚ÄúGeneralization of back-propagation to recurrent neural
networks‚Äù, Physical review letters, vol.59, pp.19, 1987.
[2] Cho K., Merri¬®enboer B., Bahdanau D. and Bengio Y., ‚ÄúOn the Properties
of Neural Machine Translation: Encoder-Decoder Approaches‚Äù, arXiv,
2014.
[3] Hochreiter S. and Schmidhuber J., ‚ÄúLong short-term memory‚Äù, Neural
Computation, vol.9, pp.1735-1780, 1997.
[4] Mikolov T., KaraÔ¨Å¬¥at, M., Burget, L., Cernock¬¥y, J. and Khudanpur S.,
‚ÄúRecurrent neural network based language model‚Äù, Interspeech, vol.3,
pp.2, 2010.
[5] Graves A., Mohamed, A. and Hinton G., ‚ÄúSpeech recognition with
deep recurrent neural networks‚Äù, 2013 IEEE international conference
on acoustics, speech and signal processing. , IEEE, 2013.
[6] Karpathy, A., and Fei-Fei, L., ‚ÄúDeep visual-semantic alignments for
generating image descriptions‚Äù, Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, IEEE, 2015.
[7] Sutskever, I., Oriol V. and Quoc V. Le., ‚ÄúSequence to sequence learning
with neural networks‚Äù, Advances in neural information processing
systems, 2014.
[8] Bengio Y., Sinard P. and Frasconi P., ‚ÄúLearning Long-Term Dependen-
cies with Gradient Descent is DifÔ¨Åcult‚Äù, IEEE transactions on neural
networks, vol.5, pp.2, 1994.
[9] Pascanu, R., Tomas M. and Bengio, Y., ‚ÄúOn the difÔ¨Åculty of training
recurrent neural networks‚Äù, International Conference on Machine Learn-
ing, vol.3, pp.28, 2013.
[10] T. Siegelmann H. and D. Sontag E., ‚ÄúOn the Computational Power of
Neural Nets‚Äù, Journal of computer and system sciences, vol.50, pp.132-
150, 1995.
[11] Jaeger H., ‚ÄúThe ‚Äùecho state‚Äù approach to analysing ang training recurrent
neural networks‚Äù, TechReport, 2001.
79
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-531-9
COGNITIVE 2017 : The Ninth International Conference on Advanced Cognitive Technologies and Applications

Fig. 5. A depiction of the automaton corresponding to the Embedded Reber‚Äôs grammar.
Fig. 6. Vanilla RNN prediction on several sequences of the Embedded
Reber‚Äôs grammar.
Fig. 7. GRU prediction on several sequences of the Embedded Reber‚Äôs
grammar. The expected letters are correct and reÔ¨Çect the fact the embedded
Reber‚Äôs grammar has been correctly inferred.
80
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-531-9
COGNITIVE 2017 : The Ninth International Conference on Advanced Cognitive Technologies and Applications

Fig. 8. Hidden layer of a 18-neurons Vanilla RNN after training on the
embedded Reber‚Äôs grammar.
RNN
GRU
Fig. 9. Gradient‚Äôs coefÔ¨Åcients for both Vanilla RNNs and GRU models
when learning the embedded Reber‚Äôs grammar from 2000 examples.
Fig. 10. Evolution of D(l; k) for a Vanilla RNN.
Fig. 11. Evolution of D(l; k) for the GRU model.
81
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-531-9
COGNITIVE 2017 : The Ninth International Conference on Advanced Cognitive Technologies and Applications

