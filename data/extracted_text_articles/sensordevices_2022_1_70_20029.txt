Simultaneous Localization, Mapping and Moving-Object Tracking Using  
Helmet-Mounted LiDAR for Micro-Mobility 
 
Ibuki Yoshida, Akihiko Yoshida 
Graduate School of Science and Engineering 
Doshisha University 
Kyotanabe, Kyoto, Japan 
e-mail: ctwg0157@mail4.doshisha.ac.jp 
Masafumi Hashimoto, Kazuhiko Takahashi 
Faculty of Science and Engineering 
Doshisha University 
Kyotanabe, Kyoto, Japan 
e-mail: {mhashimo, katakaha}@mail.doshisha.ac.jp 
 
Abstract— This paper presents a method of Simultaneous 
Localization, Mapping and Tracking of Moving Objects 
(SLAMTMO) using a Light Detection And Ranging sensor 
(LiDAR) mounted on a smart helmet worn by a vehicle rider. This 
technology can be used to active safety for micro-mobility, such as 
bicycles, e-bikes, and electric scooters, which are prioritized as 
personal commuters in the endemic society of coronavirus disease 
2019. Distortion in the scan data from the LiDAR is corrected by 
estimating the helmet’s pose (three-dimensional position and 
attitude angle) based on the information from Normal 
Distributions Transform (NDT)-based SLAM and an inertial 
measurement unit. The static and moving-object scan data, which 
originate from static and moving objects in the environments, 
respectively, are classified by subtracting the environment map 
generated by NDT-based SLAM from the LiDAR current scan 
data. The moving scan data are used for TMO based on a Bayesian 
filter, whereas the static scan data are used for point-cloud 
mapping. The experimental results in a road environment of our 
university campus show the effectiveness of the proposed 
SLAMTMO method. 
 Keywords—helmet LiDAR; SLAM; moving-object tracking; 
micro-mobility. 
I. INTRODUCTION 
There have been numerous studies on active safety and 
autonomous driving in the field of Intelligent Transportation 
Systems (ITS) [1]. In the field of last-mile automation, there has 
also been a flourishing study on delivery robots [2]. The 
environmental map building using Simultaneous Localization 
And Mapping (SLAM) technology [3, 4] and Tracking of 
Moving Objects (TMO), such as cars, cyclists and pedestrians 
[5, 6], are important issues for autonomous driving and active 
safety of vehicles and mobile robots. Many related studies have 
been using cameras, radars, and Light Detection And Ranging 
(LiDAR). In this paper, we focus on SLAMTMO with a 
vehicle-mounted LiDAR. 
To build a three-dimensional (3D) point-cloud map in 
community road environments, we presented a mapping 
method using car-mounted LiDAR based on Normal 
Distributions Transform (NDT)-Graph SLAM [7]. Using 
motorcycle-mounted LiDAR, we also proposed a method of 3D 
point-cloud mapping [8]. Moreover, we presented a TMO 
method utilizing car and motorcycle-mounted LiDAR [9, 10]. 
Coronavirus disease 2019 has caused people to be highly 
resistant to utilizing conventional means of urban transportation, 
such as crowded trains and buses. Hence, to escape the three Cs’ 
(closed spaces, crowded spaces, and close-contact settings), the 
use of single-seater “micro-mobility”, such as bicycles, e-bikes, 
electric scooters, and personal mobilities, are on the increase 
for short-distance travel in urban cities. The demand for micro-
mobility additionally increased in the endemic society. 
Even though the frequency of traffic accidents involving 
micro-mobility increases, the R&D related to active safety for 
micro-mobility is far behind. As a result, we are studying 
surrounding environmental sensing for micro-mobility, such as 
environmental map building and moving-object recognition for 
active safety in sidewalks and streets.  
In the case of micro-mobility systems, it is difficult to 
mount a large number of sensors on the vehicle body, as is the 
case with cars, because of size and theft concerns. Thus, it is 
desirable to mount small and easily detachable sensors on the 
handlebar of the micro-mobility or on the helmet worn by the 
micro-mobility rider. Our previous work [11] proposed a 
method of building a 3D point-cloud map in sidewalk and 
roadway environments using LiDAR attached to the rider's 
helmet (Helmet-Mounted LiDAR, HML) of a micro-mobility. 
This paper presents a SLAMTMO method using HML. The 
SLAMTMO can not only build a 3D point-cloud map in 
dynamic and Global Navigation Satellite System (GNSS)-
denied environments but also recognize moving objects, such 
as cars, two-wheelers, and pedestrians. This study is an 
extension of our previous works [10, 11] on SLAM-based 
environmental mapping using HML and TMO by motorcycle-
mounted LiDAR, and these methods are integrated into our 
HML system. 
Several studies have been accomplished on surrounding 
environmental sensing using HML. Indoor SLAM, in which 
people wearing helmets equipped with two-dimensional (2D) 
LiDAR or one-dimensional (1D) LiDAR walk around in 
building and factory environments, was presented [12–14]. 
Niforatos et al. [15] presented a method of skier detection using 
1D LiDAR attached to ski helmets to reduce the risk of 
accidents on ski slopes. To the best of our knowledge, no 
studies have been conducted on environmental sensing in 
sidewalks and roadways using 3D LiDAR attached to the 
rider’s helmet of a micro-mobility. Although there have been 
several studies on helmets with sensors (smart helmets) in the 
ITS fields [16], their use is limited to alcohol detection in 
motorcycle riders and collision-accident detection, as well as 
confirming rider safety after accidents. 
25
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-006-3
SENSORDEVICES 2022 : The Thirteenth International Conference on Sensor Device Technologies and Applications

The rest of this paper is organized as follows. Section II 
describes the experimental system. Section III overviews the 
SLAMTMO. Section IV explains the distortion correction 
method for the LiDAR scan data, and Section V presents the 
classification method for scan data related to static and moving 
objects. Section VI presents experimental results to verify the 
proposed method, followed by the conclusions and future 
works in Section VII. 
II. EXPERIMENTAL SYSTEM 
Figure 1 shows the overview of the smart helmet. The upper 
part of the helmet is equipped with a mechanical 64-layer 
LiDAR (Ouster, OS0-64) and an inertial measurement unit 
(IMU) (Xsens, MTi-300).  
The HML has a maximum range of 55 m, a horizontal field 
of view of 360 ° with a resolution of 0.35 °, and a vertical field 
of view of 90 ° with a resolution of 1.4 °. LiDAR can obtain 
1024 measurements (distance, direction, and reflected light 
intensity) every 1.56 ms (every 5.6 ° in the horizontal direction). 
Therefore, approximately 66,000 points of scan data are 
acquired in one rotation (360 ° observation) period (100 ms). 
Attitude angle (roll and pitch angles) and angular velocity 
(roll, pitch, and yaw angular velocities) of the helmet are 
obtained from the IMU every 10 ms. The measurement error 
for the attitude angle is less than ± 0.3 °, and that of the angular 
velocity is less than ± 0.2 °/s. 
The weight of the mechanical LiDAR is 0.5 kg, and the 
smart helmet is heavier and larger than usual helmets. Therefore, 
the LiDAR reduces the usability and practicability of the smart 
helmet. Moreover, it affects the performance of the helmet in 
the event of a crash. However, modern LiDAR technology [17] 
has been developing smaller, more lightweight, and lower 
power consumption solid-state LiDARs than mechanical 
LiDARs. The use of solid-state LiDARs will much improve the 
usability and practicability of the smart helmet. 
III. OVERVIEW OF SLAMTMO 
Figure 2 shows the sequence of SLAMTMO. For the 
environmental mapping and TMO, LiDAR scan data captured 
in the helmet coordinate system attached to the HML are 
mapped onto the world coordinate system using the self-pose 
(3D position and attitude angle) information of the helmet. For 
this, an accurate self-pose of the helmet is required. NDT-based 
SLAM [18] is utilized to estimate the self-pose in GNSS-denied 
environments. 
For the i-th measurement point (i = 1, 2, …n) in the LiDAR 
scan data, the position in the helmet coordinate system is 
denoted by 
(
,
,
)T
Hi
Hi
Hi
x y zHi
p
, and that in the world coordinate 
system by 
( , , )T
i
i
i
x y zi
p
. The following relationship is then 
represented by the homogeneous transformation: 
(
)
1
1
i
Hi
p
p
Τ X
 
(1) 
where 
( , , , , , )T
x y z
X
. 
x y z T
( , , )
 and ( , , )T  are the 3D 
position and attitude angle (roll, pitch, and yaw angles), 
respectively, of the helmet in the world coordinate system. T(X) 
is the following homogeneous transformation matrix: 
 
 
Figure 1.  Overview of the experimental smart helmet. 
 
 
 
 
Figure 2.  SLAMTMO sequence. 
 
cos cos
sin sin cos
cos sin
cos sin cos
sin sin
cos sin
sin sin sin
cos cos
cos sin sin
sin cos
( )
sin
sin cos
cos cos
0
0
0
1
x
y
z
Τ X
 
A voxel map with a cell size of 0.6 m per side is defined in 
the world coordinate system. In NDT-based SLAM, a normal 
distributions transformation is performed on the scan data 
obtained up to the previous time (referred to as environmental 
map) in each cell of the voxel map, and the mean and 
covariance of the scan data in each cell are calculated. 
The current self-pose (position and attitude angle) X of the 
helmet is calculated by matching the scan data obtained at the 
current time (referred to as current scan data) with the 
environmental map. The current scan data are mapped onto the 
world coordinate system by performing a coordinate 
transformation according to (1) using the pose X. They are then 
merged into the environmental map. By repeating this process 
every LiDAR scan period, the environmental map is built. 
The shape of the moving object in TMO is represented by a 
cuboid with a width W, a length L, and a height H, as shown in 
Figure 3. The width Wmeas and length Lmeas of the moving object 
are extracted from the scan data related to a moving object 
(moving scan data). With these values, W and L of the moving 
object are estimated [10] by  
 
26
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-006-3
SENSORDEVICES 2022 : The Thirteenth International Conference on Sensor Device Technologies and Applications

 
Figure 3.  Cuboid around the tracked object (car). 
 
( )
(
1)
(
(
1))
( )
(
1)
(
(
1))
meas
meas
W t
W t
G W
W t
L t
L t
G L
L t
              (2) 
where G is the gain.  
The height estimate H of the moving object is obtained from 
the height measurements of the moving scan data.  
The Kalman filter is used to estimate the position and 
velocity of the moving object in the world coordinate system 
based on the centroid position of the rectangle estimated from 
(2). When applying the Kalman filter, it is assumed that the 
object is moving at an approximately constant velocity. In 
crowded environments, the rule-based data association method 
[19] is used to accurately match multiple moving objects with 
multiple moving scan data. 
Because LiDAR scans laser in omnidirection, all the scan 
data within one scan cannot be obtained at a single location 
when the micro-mobility is moving or swinging, or when the 
rider’s body is swinging. Therefore, if all of the scan data 
within one scan is transformed using the pose information of 
the helmet at the same time, distortion arises in the LiDAR scan 
data mapped in the world coordinate system. Since distortion 
causes inaccurate results in SLAM and TMO, distortion 
correction of the LiDAR scan data is required. The distortion-
correction method is described in Section IV.  
Scan data relating to static and moving objects (static and 
moving scan data) are used for SLAM and TMO, respectively. 
For this, accurate classification of static and moving scan data 
from entire scan data is required. The classification method is 
described in Section V. 
IV. DISTORTION CORRECTION OF LIDAR SCAN DATA 
The helmet’s pose is determined every 100 ms (LiDAR scan 
period) using NDT-based SLAM. The scan data are acquired 
every 1.56 ms during one rotation of LiDAR. During LiDAR 
scanning, all the scan data within one scan cannot be obtained 
at a single location when the micro-mobility is moving or 
swinging, or when the rider’s body is swinging. If all the scan 
data within one scan is transformed using the pose information 
of the helmet at the same time, distortion appears in the 
mapping of the LiDAR scan data onto the world coordinate 
system. Therefore, the distortion in the scan data is corrected by 
estimating the helmet’s pose using the extended Kalman filter 
(EKF) every 1.56 ms, i.e., every LiDAR scan data are obtained.  
In the EKF for correcting the scan-data distortion, a constant 
velocity model is used as the helmet motion. As shown in 
Figure 4, the translational velocity of the helmet in the helmet 
coordinate system (OH-xHyHzH) is denoted by (
,
,
x
y
z
V V V ), and 
the angular velocity (roll, pitch, and yaw angular velocities) by 
(
,
,
H
H
H ). The following motion model of the helmet can be 
obtained assuming that the helmet moves at nearly constant 
translational and angular velocities: 
 
 
Figure 4.  Notation related to the helmet motion. 
 
( )
( )
( )
( )
1
( )
( )
( )
( )
( )
( )
2
( )
3
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
cos
cos
sin
sin
cos
cos
sin
cos
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
x
t
y
t
z
t
H
t
H
t
H
x
a
a
a
x
y
z
V
V
V
　　
　　
　
( )
( )
( )
( )
( )
( )
( )
( )
( )
1
( )
( )
( )
( )
( )
( )
2
( )
( )
( )
( )
( )
( )
3
( )
( )
( )
( )
( )
( )
1
2
( )
( )
3
sin
cos
sin
sin
cos
sin
sin
sin
sin
cos
cos
cos
sin
sin
sin
cos
sin
sin
cos
cos
cos
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
y
a
a
a
z
a
a
a
　　
　　
　　
( )
( )
( )
( )
( )
( )
( )
( )
4
5
6
( )
( )
( )
( )
( )
5
6
( )
( )
( )
( )
( )
5
6
( )
( )
( )
( )
( )
( )
( )
sin
cos
tan
cos
sin
1
sin
cos
cos
H
H
H
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
x t
Vx
y t
Vy
z t
Vz
t
H
t
H
t
H
a
a
a
a
a
a
a
V
w
V
w
V
w
w
w
w
  
       (3) 
where ( , , )
x y z  and ( , ,
)  are the position and attitude angle 
(roll, pitch, and yaw angles), respectively, of the helmet in the 
world coordinate system (Ow-xwywzw). (
,
,
,
,
H
Vx
Vy
Vz
w
w
w w
 
,
)
H
wH
w
is the acceleration disturbance. 
is the LiDAR scan 
period. 
2
1
/2
x
x
V
a
V
w
 , 
2
2
/2
y
Vy
a
V
w
 , 
2
3
/2
z
z
V
a
V
w
 , 
2
4
/ 2
H
H
a
w
 , 
2
5
/2
H
H
a
w
 , and 
6
H
a
2
/2
w H
.  
Figure 5 shows the sequence of distortion correction. The 
LiDAR scan period (100 ms) is denoted as τ, the IMU 
observation period (10 ms) as ΔτIMU, and the scan data 
observation period (1.56 ms) as Δτ. Here, the method for 
correcting the scan-data distortion obtained between time 
(
1)
t
 and t  is described [20]. 
Let us suppose that at the time (
1)
t
, the pose of the 
helmet is calculated by NDT-based SLAM and estimated via 
EKF. The IMU data are obtained 10 times per LiDAR scan (τ = 
10ΔτIMU). Using the IMU data obtained every ΔτIMU, the EKF  
27
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-006-3
SENSORDEVICES 2022 : The Thirteenth International Conference on Sensor Device Technologies and Applications

 
Figure 5.  Sequence of distortion correction. 
 
estimates the pose ˆ (
1, )
t
k
X
at the time (
1)
IMU
t
k
, 
where k = 0–10. Since the observation period 
 IMU of the IMU 
is 10 ms, and the scan-data observation period 
 is 1.56 ms, 
the LiDAR scan data are obtained six times within the IMU 
observation period (ΔτIMU =6Δτ). 
From 
the 
estimates, 
ˆ (
1, )
t
k
X
at 
the 
time
(
1)
IMU
t
k
 and ˆ (
1,
1)
t
k
X
 at 
)1
(t
+ (
1)
IMU
k
the interpolation algorithm predicts the helmet’s pose 
ˆ (
1, , )
t
k j
X
 at 
)1
(t
+
IMU
k
jΔτ  (where j = 1–5), at 
which the scan data are acquired.  
The scan data 
(
1, , )
Hi t
k j
p
 (where i = 1, 2, …, n) obtained 
at 
)1
(t
+
IMU
k
jΔτ in the helmet coordinate system are 
transformed to 
(
i t 1, , )
k j
p
 in the world coordinate system 
using Eq. (1) as follows: 
(
1, , )
(
1, , )
( ˆ
(
1, , ))
1
1
i
Hi
t
k j
t
k j
t
k j
p
p
Τ X
       (4) 
Using the pose estimate ˆ(
X t 1,10)
 at t
 (=
)1
(t
+
10
IMU ), the scan data 
(
i t 1, , )
k j
p
 obtained by Eq. (4) is 
again transformed into the scan data 
* ( )
pHi t
  in the helmet 
coordinate system at t
 by 
*
1
( )
(
1, , )
( ˆ
(
1,10))
1
1
Hi
i
t
t
k j
t
p
p
X
             (5) 
* ( )
pHi t
 is the scan data that correct distortion at t
. Using 
the corrected scan data, SLAMTMO is performed. 
V. CLASSIFICATION OF STATIC AND MOVING SCAN DATA  
For SLAM, the moving scan data have to be eliminated and 
the static scan data have to be extracted from the whole LiDAR 
scan data. TMO conversely requires the removal of static scan 
data and the extraction of moving scan data from the whole 
LiDAR scan data. For this, accurate classification of static and 
moving scan data is required. Although the classification is 
usually performed based on the occupancy grid method, in 
practical environments, LiDAR noises and outliers frequently 
cause misclassification.  
We utilize an environment map built by NDT-based SLAM 
to minimize the misclassification. We call this approach 
environment map subtraction (EMS)-based classification or 
dynamic background subtraction-based method [10]. Figure 6 
shows the EMS-based classification method. In this method, we 
subtract the environment map built by NDT-based SLAM from 
the current scan data to remove as much static scan data as 
possible from the entire LiDAR scan data. The scan data 
extracted using the EMS-based method are mapped onto a grid 
map. The cell on the grid map is a square with a side-length of 
0.3 m. A cell in which scan data exist is called an occupied cell. 
For the moving scan data, the time to occupy the same cell is 
short (less than 0.7 s in this paper), whereas for the static scan 
data, the time is long (not less than 0.7 s). Therefore, by using 
the occupancy grid method based on the cell occupancy time 
[19], cells occupied by moving scan data (or static scan data) 
can be detected as moving cells (or static cells).  
Because an object takes multiple cells, adjacent occupied 
cells are clustered. Then, clustered moving cells (or static cells) 
are obtained as a moving cell group (or static cell group). The 
scan data contained in the moving cell group are finally decided 
as the moving scan data. The static scan data are extracted by 
subtracting the moving scan data from the LiDAR current scan 
data.  
The LiDAR field of view also moves along with the micro-
mobility movement. Even though an object that recently enters 
the LiDAR field of view is static, it is misclassified as a moving 
object because the cell occupancy time is short. To address this 
problem, new-observation cells are defined on the grid map, 
which correspond to the new field of view of the LiDAR. The 
time of cells entering the LiDAR field of view (TNC) and the cell 
occupancy time (TOC) are measured, and the occupancy time 
rate (α) is calculated by α = TOC／TNC. Cells in which α is 10% 
or more are determined to be new-observation cells and then 
considered moving cells. This can minimize the false 
classification of static objects recently entering the LiDAR field 
of view as moving objects. 
The scan data in the environment map are sparser in the 
areas in front of the micro-mobility and in the occlusion areas. 
Therefore, the static scan data likewise exist in a sparse state 
 
 
 
Figure 6.  Sequence of EMS-based classification (top view). 
28
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-006-3
SENSORDEVICES 2022 : The Thirteenth International Conference on Sensor Device Technologies and Applications

when the environment map is subtracted from the current scan 
data. If the scan data, sparsely extracted based on the EMS-
based method, are mapped onto a grid map, it may be 
erroneously determined as a moving cell.  
To overcome this issue, the scan data removed by the EMS-
based method are additionally mapped onto the grid map as 
static cells. As a result, sparse static scan data that tend to be 
moving cells and static scan data that are removed by the EMS-
based method are both mapped onto the grid map. Neighboring 
cells, in which these static scan data are occupied, are clustered, 
and the cell group is then determined to be a static cell group. 
Accordingly, sparse static scan data are correctly determined as 
static scan data by the occupancy grid method. 
VI. FUNDAMENTAL EXPERIMENTS 
A micro-mobility was moved on our university-campus 
road, as shown in Figure 7, and SLAMTMO is performed. The 
traveling distance of the mobility is approximately 500 m, and 
the maximum speed is approximately 30 km/h. Figure 8 shows 
the attitude angle and angular velocity of the helmet during 
driving, which are observed by the IMU.  
Figure 8 shows the TMO results. In Figure 9(b), the blue 
rectangle indicates the assessed size of the moving object, and 
the blue stick indicates the moving direction of the moving 
object obtained from the velocity estimate. The black (or red) 
dots reveal the scan data removed (or extracted) from the 
LiDAR scan data using the EMS-based method.  
The micro-mobility is moved three times along the path 
shown in Figure 7 (a). Then, 111 moving objects (106 
pedestrians and five cars) are tracked. The TMO performance 
is examined under the following two cases: 
 
 
(a) Top view 
 
 
(b) Side view 
Figure 7.  Photo of experimental environment. 
 
(a) Roll (black) and Pitch(red) angles 
 
(b) Roll (black), pitch (red) and yaw (blue) angular velocities 
Figure 8.  Attitude angle and angular velocity of helmet. 
 
 
(a) Photo of area 1 
 
 
(b) Estimated track and size of the moving objects in area 1  
Figure 9.  Tracking result (top view). 
 
・Case 1: Tracking with distortion correction of LiDAR 
scan data and EMS-based classification method (proposed 
method) 
・Case 2: Tracking without using either method. 
As the result, in case 1, 109 objects (104 pedestrians and 
five cars) can be successfully tracked, and two pedestrians 
cannot be tracked. In case 2, conversely, 103 objects (98 
pedestrians and five cars) can be successfully tracked, and eight 
29
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-006-3
SENSORDEVICES 2022 : The Thirteenth International Conference on Sensor Device Technologies and Applications

pedestrians cannot be tracked. Pedestrians who are not being 
tracked are mistakenly classified as static objects. From these 
results, our proposed method gives better TMO accuracy. 
Figure 10 shows the SLAM result estimated by the 
proposed method (case 1). In the SLAM, the mapping accuracy 
can be evaluated by the pose accuracy. Therefore, the error of 
position estimate of the helmet at the goal position is measured 
by a GNSS/LiDAR positioning system set at the goal position. 
Table I reveals the result, in which the micro-mobility is moved 
 
 
(a) Overall map (top view) 
 
 
(b) Map in area 1 (bird’s-eye view) 
 
 
(c) Position estimate  
 
Figure 10.  SLAM result. 
 
TABLE I.  ERROR OF POSITION ESTIMATE OF HELMET AT GOAL POSITION. 
 
Case 1 
Case 2 
Exp 1 
0.23 m 
5.91 m 
Exp 2 
1.93 m 
15.10 m 
Exp 3 
0.71 m 
6.03 m 
 
TABLE II.  PROCESSING TIME OF SLAMTMO (CASE 1) 
 
Distortion 
correction and 
NDT SLAM   
Scan data 
classification 
Environment 
map update, 
moving-object 
tracking, and 
others 
Total 
Exp 1 
2180 ms 
801 ms 
33 ms 
3014 ms 
Exp 2 
2068 ms 
719 ms 
30 ms 
2817 ms 
Exp 3 
1815 ms 
659 ms 
33 ms 
2507 ms 
Mean 
2021 ms 
726 ms 
32 ms 
2779 ms 
 
TABLE III.  PROCESSING TIME OF SLAMTMO (CASE 2) 
 
NDT SLAM 
Scan data 
classification 
Environment 
map update, 
moving-object 
tracking, and 
others 
Total 
Exp 1 
1753 ms 
242 ms 
23 ms 
2025 ms 
Exp 2 
1832 ms 
285 ms 
27 ms 
2144 ms 
Exp 3 
1713 ms 
239 ms 
25 ms 
1977 ms 
Mean 
1766 ms 
255 ms 
27 ms 
2049 ms 
 
 
three times on the road shown in Figure 7. From the table, the 
proposed method (case 1) provides the SLAM accuracy better 
than case 2.  
In the experiments, LiDAR scan data are recorded, and 
SLAMTMO is executed offline by a computer. The 
specifications of the computer are as follows: Windows 10 Pro 
OS, Intel(R) Core (TM) i7-1065G7 @1.30GHz CPU, 16 GB 
RAM, and C++ software language. The point cloud library 
(PCL) [21] is used for NDT-based SLAM. Tables II and III 
show the processing time of SLAMTMO in cases 1 and 2, 
respectively.  
Although long computational time is currently required, as 
shown in Tables II and III, the computational time can be 
reduced by optimizing the program code and using a graphical 
processing unit for real-time operations. 
VII. CONCLUSIONS 
This paper presented a SLAMTMO method in dynamic and 
GNSS-denied environments utilizing LiDAR attached to a 
helmet worn by a rider of micro-mobility (helmet-mounted 
LiDAR). To accurately perform environmental mapping and 
TMO, the distortion of scanning LiDAR data was corrected 
using the self-pose information by NDT-based SLAM and IMU 
information via EKF. Furthermore, static and moving scan data 
were classified by the EMS-based classification and occupancy 
grid-based methods. The performance of the presented method 
was examined through experiments in a road environment of 
our university campus. 
Experiments in various sidewalk and roadway situations are 
now being carries out to thoroughly evaluate the proposed 
30
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-006-3
SENSORDEVICES 2022 : The Thirteenth International Conference on Sensor Device Technologies and Applications

method. In future works, we will detect obstacles on road 
surfaces, such as curbs, gutters, and steps, using LiDAR and 
accelerometer to reduce the falling risk of micro-mobility. 
Additionally, we will include the obstacle information in the 
environment map. 
REFERENCES 
[1] E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, “A Survey 
of Autonomous Driving: Common Practices and Emerging 
Technologies,” IEEE Access, vol. 8, pp. 58443–58469, 2020. 
[2] N. Boysen, S. Fedtke, and S. Schwerdfeger, “Last-Mile Delivery 
Concepts: A Survey from an Operational Research Perspective,” 
OR Spectrum, vol. 43, pp. 1–58, 2021. 
[3] B. Huang, J. Zhao, and J. Liu, “A Survey of Simultaneous 
Localization and Mapping,” eprint arXiv:1909.05214, 2019. 
[4] 
S. Kuutti et al., “A Survey of the State-of-the-Art Localization 
Techniques and Their Potentials for Autonomous Vehicle 
Applications,” IEEE Internet of Things Journal, vol.5, pp. 829–
846, 2018. 
[5] 
A. Mukhtar, L. Xia, and TB. Tang, “Vehicle Detection 
Techniques for Collision Avoidance Systems: A Review,” IEEE 
Trans. on Intelligent Transportation Systems, vol. 16, pp. 2318–
2338, 2015. 
[6] 
E. Marti, J. Perez, MA. Miguel, and F. Garcia, “A Review of 
Sensor Technologies for Perception in Automated Driving,” 
IEEE Intelligent Transportation Systems Magazine, pp. 94–108, 
2019.  
[7] S. Tanaka, C. Koshiro, M. Yamaji, M. Hashimoto, and K. 
Takahashi, “Point Cloud Mapping and Merging in GNSS-
Denied and Dynamic Environments Using Only Onboard 
Scanning LiDAR,” Int. J. on Advances in Systems and 
Measurements, vol. 13, pp. 275–288, 2020. 
[8] K. Matsuo, A. Yoshida, M. Hashimoto, and K. Takahashi, “NDT 
Based Mapping Using Scanning Lidar Mounted on Motorcycle,” 
Proc. of the Fifth Int. Conf. on Advances in Sensors, Actuators, 
Metering and Sensing, pp. 69–75, 2020. 
[9] S. Sato, M. Hashimoto, M. Takita, K. Takagi, and T. Ogawa, 
“Multilayer Lidar-Based Pedestrian Tracking in Urban 
Environments,” Proc. of IEEE Intelligent Vehicles Symp., pp. 
849–854, 2010. 
[10] S. Muro, I. Yoshida, M. Hashimoto, and K. Takahashi, “Moving-
Object Tracking by Scanning LiDAR Mounted on Motorcycle 
Based on Dynamic Background Subtraction,” Artificial Life and 
Robotics, vol. 26, issue 4, pp. 412–422, 2021. 
 
 
 
 
 
 
 
 
 
 
 
 
 
[11] A.Yoshida, I. Yoshida, M. Hashimoto, and K. Takahashi, “Point-
Cloud Mapping by Helmet-Mounted LiDAR Based on NDT 
SLAM,” Proc. of 23rd IEEE Int. Conf. on Industrial Technology, 
2022, to be presented. 
[12] Y. Cai, S. Hackett, G., Ben, F. Alber, and S. Mel, “Heads-Up 
Lidar Imaging with Sensor Fusion,” Electronic Imaging, The 
Engineering Reality of Virtual Reality 2020, pp. 338-1–338-7, 
2020. 
[13] B. Cinaz and H. Kenn, “Head SLAM - Simultaneous 
Localization and Mapping with Head-Mounted Inertial and 
Laser Range Sensors,” Proc. of 12th IEEE Int. Symp. on 
Wearable Computers, 2008. 
[14] H. Sadruddin, A. Mahmoud, and M. M. Atia, “Enhancing Body-
Mounted LiDAR SLAM using an IMU-based Pedestrian Dead 
Reckoning (PDR) Model,” Proc. of 2020 IEEE 63rd Int. 
Midwest Symp. on Circuits and Systems, 2020. 
[15] E. Niforatos, I. Elhart, A. Fedosov, and M. Langheinrich, “ s-
Helmet: A Ski Helmet for Augmenting Peripheral Perception,” 
Proc. of the 7th Augmented Human Int. Conf.,2016. 
[16]  A. Pangestu, M. N. Mohammed, S. Al-Zubaidi, S. H. K. Bahrain, 
and A. Jaenul, “An Internet of Things Toward a Novel Smart 
Helmet for Motorcycle: Review,” AIP Conf. Proceedings 2320, 
050026, 2021. 
[17] T. Raj, F. H. Hashim, A. B. Huddin, M. F. Ibrahim, and A. 
Hussain, “A Survey on LiDAR Scanning Mechanisms,” 
Electronics, 2020. 
[18] P. Biber and W. Strasser, “The Normal Distributions Transform: 
A New Approach to Laser Scan Matching,” Proc. of IEEE/RSJ 
Int. Conf. on Intelligent Robots and Systems, pp. 2743–2748, 
2003. 
[19] M. Hashimoto, S. Ogata, F. Oba, and T. Murayama, “A Laser 
Based Multi-Target Tracking for Mobile Robot,” Intelligent 
Autonomous Systems 9, pp. 135–144, 2006. 
[20]  K. Tokorodani, M. Hashimoto, Y. Aihara, and K. Takahashi, 
“Point-Cloud Mapping Using Lidar Mounted on Two-Wheeled 
Vehicle Based on NDT Scan Matching,” Proc. of the 16th Int. 
Conf. on Informatics in Control, Automation and Robotics, pp. 
446–452, 2019. 
[21]  R. B. Rusu and S. Cousins, “3D is here: Point Cloud Library 
(PCL),” Proc. of 2011 IEEE Int. Conf. on Robotics and 
Automation, 2011. 
 
31
Copyright (c) IARIA, 2022.     ISBN:  978-1-68558-006-3
SENSORDEVICES 2022 : The Thirteenth International Conference on Sensor Device Technologies and Applications

