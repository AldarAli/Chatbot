Moving with Style: Classifying Human and Robot Movement at Home
Trenton Schulz, Jo Herstad, Jim Torresen
University of Oslo
Postbox 1072 Blindern
0316 Oslo, Norway
Email: [trentonw|johe|jimtoer]@iﬁ.uio.no
Abstract—A robot moving in the home is a new experience
for most people. Classifying the different ways that humans
and robots move together can help in designing interactions.
In this paper, we ﬁrst put robots’ movements into two cate-
gories: global—where a robot changes position in the home—or
local—where a robot’s position does not change, but parts of it
move. We also look at the idea of animation and how it can
give a robot style. Building on these deﬁnitions, we present four
movement conditions to classify movement between a human and
robot at home. Using familiarity, we can recognize some of these
conditions from other interactions we have. Using animation, we
can give the robot a style that can make the robot’s movement
more familiar and easier to understand. We close the paper
with possible ways of using this classiﬁcation system for future
research.
Keywords–human-robot interaction; animation; style; move-
ment; familiarity; home
I.
INTRODUCTION
The “elderly wave” is an issue that many western countries
are examining [1]. The number of people that will be retiring
and needing care will be much larger than the people entering
the workforce for these jobs. Consequently, some countries
and areas have set goals that more elderly should live inde-
pendently at home longer. One way of addressing this issue
is to turn to welfare technology that can assist the elderly [2].
One example is using the Internet of Things and smart home
sensors for reporting and helping elderly complete tasks [3][4].
Another example is using these sensors to provide someone
with a warning when things go wrong, such as an elderly
person falling [5]. But all these sensors in the house may
transform it from a home to a place where elderly may feel
they are under surveillance with no privacy and little control
over their life.
Robots may be an alternative for helping out at home.
Robots can be mobile and customized for handling different
kinds of tasks. A robot may give the person a chance of feeling
in control and a feeling of privacy. For example, an elderly
person could tell the robot to leave the room. Robots cannot
replace a human in every context, but it can provide support
for issues when a person cannot be present or help contact
a person. Robots may help in ways that would otherwise
require another human to always be present, and have diverse
knowledge. For example, robots can collect data and use
algorithms to give early warnings about issues (e.g., falling
down, low blood pressure, or suffering from poor nutrition).
Robots have been making their way into the home. Do-
mestic robots cut the lawn and vacuum, but other robots have
been around to provide entertainment (the robot dog Aibo [6])
or stress relief (the Paro seal [7]). We are working to create
a robot that stays with the elderly at home and serves as a
safety alarm and perform other services that the elderly want.
The aim is to improve quality of life of the elderly at home.
It is an opportunity for collaboration between the robot, the
elderly at home, and the assisted living help.
Unlike other types of technology at home that is either
stationary or wearable, robots can move around and possibly
perform tasks on their own. This raises several questions: How
can we make the robot familiar? How do robots’ movement
affect our interaction with them? Are there better ways for this
movement to happen? We wish to examine these questions. We
will begin by trying to deﬁne animation, movement, and style
(Section II). Next, we propose a framework for classifying
movements between a person and a robot (Section III). Then,
we will look at how familiarity can help make this robot
motion familiar, and we will present how style created through
animation can help in this familiarity (Section IV). Finally,
we will look at some limitations with this framework and
ideas for future work (Section V) before concluding the article
(Section VI).
II.
BACKGROUND: MOVEMENT, ANIMATION, AND STYLE
First, we will deﬁne animation, movement, and style for
this paper. We will also look at some projects that have
involved robots and animation.
The physical idea of movement (or motion) is a change
in position over time that can work for some types of robot
movement we will call this global movement (Figure 1a). If
we were to imagine the robot in a house, global movement
would mean the robot moves in a room or moves to another
room. Let us deﬁne another type of motion where parts of the
robot move, but its global position does not change (Figure 1b).
This is local movement. Returning to our imaginary robot at
home above: its local movement would be moving parts on its
body (for example, rotating its body or moving an appendage
on its body) that do not move the complete robot in the room.
There is one situation left: no movement. That is, when a robot
standing still and not moving any parts of the body. To keep the
classiﬁcation system simple, let us assume that no movement is
a special case of local movement. Of course, local movement
and global movement can be combined.
There are many ways a robot’s movement can be accom-
plished. The robot can move at a constant speed, it can speed
up as it starts out and slow down, or it can back up to get
a running start and can abruptly stop when it gets to the
destination. We can think of these approaches as animating
the robot. Normally, we associate animation with cartoons and
ﬁlm, where one combines frames together to create movement
on screens, but the American Heritage Dictionary also deﬁnes
animation as, “the act, process, or result of imparting life,
interest, spirit, motion, or activity” [8].
188
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

x
y
(a)
(b)
Figure 1. Examples of global and local movement: in global movement (a), the robot moves in a two-dimensional plane; the Aibo laying down and waving (b)
is an example of local robot movement.
Another way we can think of animating robots is moving
with style. With style, we are using Gallaher’s deﬁnition: “the
way in which a behavior is performed” [9, p. 133]. Gallaher
pointed out that style can also be thought of as expressive
movement. Gallaher was looking at people’s style, but this
concept has been successfully applied to robots [10].
How does animation make the robot move with style?
Animation gives the robot an interesting way of moving.
This animated motion can make the robot seem likes it has
a personality. The motion can also help the robot to better
communicate what it is planning to do (the robot’s intention
for lack of a better term).
Are there any design principles or guidelines for adding
this style? Looking at the history of animation on ﬁlm,
Thomas and Johnston [11] documented twelve principles of
animation that animators at Disney used to create good ani-
mations. These principles include: (a) squash and stretch—an
animated object squashes and stretches its form, but never
truly loses its recognizable shape; (b) anticipated action—an
object needs to prepare itself before performing an action;
(c) follow through and overlapping action—actions are not
done in isolation, characters move seamlessly between them;
(d) arcs—limbs move in arcs, not straight up-down, left-right
motions; (e) secondary action—the object’s main action causes
other secondary actions to occur at the same time; and (f) exag-
geration—over-emphasizing an action helps people understand
a characters feelings.
Adopting principles of animation from ﬁlm can help in
making robots animated. Breazeal [12] references them when
working on the Kismet robot. Van Breemen [13] tried to apply
these principles in the facial expressions of the iCat [14].
His reason for doing this was to make the robot’s behavior
more natural and less machine-like. This would make the robot
easier to accept and have easier interactions.
As mentioned above, animation can make things “look
alive” or give them animacy. This may also give us some
feelings about them. Several experiments have been run where
a person works with an animated robot for a while, but then is
asked to “kill” it by, for example, destroying it with a hammer
or turning off its power to erase its memory [15]–[17].
There are several examples of using animation to communi-
cate a robot’s intention. Takayama, Dooley, and Ju [18] showed
how the animation principles can make it easier for a human
to understand and predict what a robot is doing. Gielniak and
Thomaz [19] found that creating anticipation for motion (i.e.,
one of the twelve principles of animation from above) made
it easier for people to predict what the robot was going to do.
In a later study, Gielniak and Thomaz [20] experimented with
exaggerating movement for a robot to tell stories. People that
saw the exaggerated movements remembered those parts of the
story better. These movements need not be big or elaborate.
For example, in a nod to the animation principle of secondary
action, a study with elderly people ran by Louie, McColl,
and Nejat [21] found that the participants enjoyed the “facial
expressions and different tones of voice” [21, p. 148]. Finally,
Baraka, Rosenthal, and Veloso [22] made the intentions of a
robot moving around independently more understandably by
adding animated lights to make the robot’s state more visible.
III.
CLASSIFYING HUMAN AND ROBOT MOVEMENT
Human-computer interaction (HCI) has a tradition of study-
ing the use, design, and evaluating ways interfaces and inter-
actions are taking place in different contexts between humans
and stationary computers in workplace settings, public places,
and home settings. Mobile computing raised the importance
of the context of use and interaction to researchers’ attention.
This lead to the research area of context aware computing [23].
Ubiquitous and ambient computing raise the idea of computers
in the home, but hidden from view and not moving.
The conditions for the interaction taking place between
humans and computers in a stationary and mobile situation
are similar; there is a stable spatial arrangement between
the people and computers. In both situations, humans and
computers are interacting in the same place, with a stationary
relationship in-between the human and the computer.
189
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

The spatial conditions change when robots enter the scene.
We may be used to moving things outside our home like
automobiles, buses, boats and trams. We are all living in a
shared world where we are used to other people moving around
at home and in public places. Yet in a home setting, we are
not familiar or used to things moving around except when
moving into a place, (e.g., renovating, or moving out of a
place). Furniture is moved, and there is movement of things
by residents and guests in the home, but very few things move
on their own.
In the home context, we can classify this movement:
(a) Things that we move around: furniture, peripherals,
clothes, machines like vacuum cleaners or furniture on wheels.
(b) Things moving themselves: domestic robots (robot vacuum
cleaners and robot lawn mowers) and our safety alarm robots.
If we examine the spatial arrangement for movement between
one human and one robot and classify the movement as
local and global from Section II, we get the following four
conditions (Table I):
1) Human moves locally and the robot moves locally,
2) Human moves locally and the robot moves globally,
3) Human moves globally and the robot moves locally, and
4) Human moving globally and the robot moving globally.
TABLE I. MOVEMENT CONDITIONS FOR HUMANS AND ROBOTS
Condition
Human
Robot
1
Local
Local
2
Local
Global
3
Global
Local
4
Global
Global
This framework for classiﬁcation also gives a way to
compare the human-robot movement with other objects. In
Condition 1 and Condition 3, when the robot is moving
locally (including being completely still as per Section II),
the human is either moving locally or globally. This is similar
to conditions for interacting with stationary computers. We
can see Condition 1 when a person watches TV, and we can
see Condition 3 when a person approaches a switch or walks
towards a remote control.
The other conditions are more unusual in the home before
robots. For example, Condition 2 happens when toys are
moving. But Condition 4 does not have good analogs other
than perhaps chasing a moving toy. These other conditions
also indicate something that is unfamiliar. Gibson and Ingold
[24] ﬁnd we are indeed familiar with movement, and they work
out the importance of movement on perception. But what we
are not used to is being around things that move—and not
being controlled or steered by other people! What can we do
to make this situation more familiar?
IV.
FAMILIARITY AND MOVING ROBOTS AT HOME
To make robots moving at home more familiar, we need
to examine what familiarity is. Once we have an idea what
familiarity is, we can look at how we can make a robot’s
movement familiar. We can also see how animation and style
can help in making these situations familiar.
A. Familiarity
Familiarity plays a role in how people interact and use
things and objects. The common sense meaning of familiarity
is trivial. The familiar is often what we are comfortable and
safe with, be it situations, technologies, relationships, activities
or other people. We are often unfamiliar with is things we do
not engage with, have no skills with or understanding of, or
are foreign to us.
These three concepts; involvement, understanding and
unity of user-world are, according to Turner and Walle [25],
ideas that we can apply to get a grip on familiarity. Turner
and Walle stated that familiarity unfolds over time. Hence,
familiarity points to activities of daily living where we are
engaged and skillful people going about our everyday lives.
When breakdowns or interruptions happens, i.e., something is
faulty, missing or in our way for us to proceed, the separation
between people and their world is taking place, and equipment
and activities become visible as objects for our analysis [26].
However, this is not the primordial way of being in the world.
Van de Walle, Turner, and Davenport claimed, “What is
observable are the outcomes: easiness, conﬁdence, success,
performance, which are all manifestations or signs of familiar-
ity,” [27, p. 467]. This shows that familiarity is subjective; it
can be described by observing activities or asked questions
in interviews. One way of investigating possible ways of
using robots in the home is to learn from what we already
are familiar with of movement. Harrigan and Rosenthal [28]
provided an introduction into non-verbal human behavior,
including proxemics. Hall [29] observed that human-social
spatial distances vary by degree of familiarity between the
people interacting and the number of people interacting. Hall
later provided a framework that identiﬁes the main social
spatial zones by interaction and situations. He estimated these
distances visually in terms of arms lengths, close contact and
threat/ﬂight distances—and researchers have since assigned
precise numerical values.
B. Making a robot’s movement more familiar
For all people, movement is a phenomenon that they are
familiar with. Moving within a place such as a home are
examples of the movement that we all experience in our
everyday life. We are familiar with seeing other people move.
We are familiar with seeing things move (for example, in the
house). We move about in concert with things such as phones,
watches, and footwear. There is nothing extraordinary with this
familiarity of movement of things and other people.
By focusing on familiarity of movement, we build on
people’s preexisting involvement, understanding and relation-
ship with the everyday world. We know how to move along
bicycles, automobiles, trains, trucks, metro cars as large objects
that move about. Even though we do not see the driver of a
metro car, we are familiar with the movement that unfolds.
In our homes, we are familiar with other people, and perhaps
animals, moving about the house. We are also familiar with
moving things around in our homes by ourselves, or by other
people. Moving a chair closer to the ﬁreplace, or carrying wood
to the stove are two examples that we are familiar with. If not
done by ourselves, it is at least something that we have seen
in pictures or on ﬁlm.
Walters, Dautenhahn, Te Boekhorst, et al. [30] have used
human-human proxemics for investigating interaction with
190
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

robots. Yet this is all based upon a model of distances
and proxemics that has human-human movement as its base.
Another possibility may be to use human-thing distances and
proxemics as the starting point. This would be grounded in
our familiarity with movement of things.
If we think of familiar movement where an object moves
with us, we can ﬁnd some examples: (a) navigating trafﬁc, with
cars, bicycle and public transport material, (b) walking with
a rolling suitcase, (c) operating a wheelchair, (d) operating a
walking stick, and (e) operating a walker. We are all familiar
with these movements, but there is no ﬁeld of literature to ﬁnd
out more about these types of movement. Yet the concept of
familiarity helps us ﬁnd these examples.
C. Making a robot more familiar by giving it style
In Section II, we posited that an animated robot moves
with style. Several of the robots from Section II do not move
from their location, but the way they move their parts makes
them appear more friendly and easier to relate to. Animation
also makes it possible to experiment with different kinds of
interaction depending on the animation style.
To jump back to HCI and graphical user interfaces, pro-
grammers can move items across the screen in many ways,
but animating can help people understand what is going on
when they are using a program. There is a different mood or
tone when a window minimizes by shrinking down to a small
area on the screen versus simply scaling the window. Just as
animated graphical user interface elements help explain what is
going on, the way a robot moves can be helpful in explaining
what is going on in an interaction with a robot. Naturally,
there are limitations—for example, robots must obey the laws
of physics and some types of motion put extra strain on the
robot [31]—but we can give a robot its own style by animating
it.
Animation can be present in all conditions in Section III.
For example, in Condition 1, the robot does not move globally,
but its local movement can still be animated by moving parts
of its body. This animation can give the robot a style, add some
personality, and give the effect of presence for the robot [32].
For example, if the person is asking a question or the robot
is providing feedback, animation can provide feedback to the
person about the robot’s state and other relevant information.
This does not have to be complex; a part of the robot rotating
can sufﬁce, or lights blinking to indicate the robot is listening.
A simple rotation that follows the person can help keep the
interaction going in Condition 3.
Condition 2 can build on top of the animation from
Condition 1. Here, the animation of parts of the robot’s body
can be combined with its global movement. For example, if
the person asks for some privacy, the robot can start moving
away. This can give the person a sense of what the robot is
going to do. Animation could also affect how fast the robot
moves, combining animations could make a robot “appear”
angry, sad, surprised, or happy.
Since these two conditions can build on top of each other,
animation can also help with the transition between them. This
can offer the human a cue to the robot’s intention (i.e., it is
about to move or about to stop as Gielniak and Thomaz [19]
researched). From the robot’s side, it can also try to determine
the human’s cue to get information if it too should start or
stop.
Condition 4 is still unfamiliar for most of us in indoor
settings. But animating the robot’s movements can give it a
style to make it seem like this condition is more familiar. The
way the robot moves can imitate another person or an animal.
These imitations can remind us of other situations where we
and something else move, and this can make a robot and human
moving at the same time more familiar.
Looking at proxemics, animation can aid in building a
rapport between the robot and the human. Mumm and Mutlu
[33] discuss how a rapport is necessary for people to be
willing to get (physically) close to a robot or answer personal
questions. Mumm and Mutlu also point out that until a rapport
is established, certain actions that signal a good rapport (like
maintaining eye contact) should be avoided. Obaid, Sandoval,
Złotowski, et al. [34] found different distances for an ap-
proaching robot based on the posture of the human (sitting
or standing).
A framework for looking at movement gives us a way
to animate this movement and give it style. The way these
movements are animated may inﬂuence how willing someone
is to interact with it. As Saerbeck and Bartneck [35] found
when looking at how people experienced motion of robots,
the speed and way a robot moved caused people to describe
the personality or mood of the robot. Building on this work,
Noordzij, Schmettow, and Lorijn [36] found people associated
negative and positive emotion to a simple robot simply by
adjusting how it accelerated. On the other hand, if a robot
moves too slow, people may assume that the robot can never
get anything done and simply will not interact with it. If we
desire interaction with a robot that moves, we need to make it
an inviting experience.
Finally, familiarity does not have to just be in the form
of the robot. A challenge we can ﬁnd from Hoffman and Ju
[37] is that robots that resemble something we are familiar
with (e.g., an animal or human) may bring expectations that
are difﬁcult to achieve with current technology. Instead, a robot
moving expressively can be used for clues to interaction. These
movements follow physical properties in the world that people
are already familiar with and give them a starting point for
their interaction.
V.
FUTURE WORK
There are limitations with this classiﬁcation as it only looks
at a speciﬁc case of one human and one robot, and we can
explore different directions of movement as well. Yet, even at
its simple level, it gives us many questions we can investigate:
how can the robot move to bring trust and assurance when the
person is working with the robot? What activities can a robot
do that are not available when a technology is stationary or
only handheld? What conditions are necessary so that people
and robots can work together? How are these interactions
affected by the animation, proximity, automation, control, and
delegation? We can also examine the transition between the
different classiﬁcations.
Moving with style can be helpful. But different people
prefer different styles, and some style may work well in some
situations than others. Finding styles that are compatible with
the robot, the people, and the situation will be a challenge.
Returning to the issue of having a robot at home with
the elderly. Another issue to look at is how the animation
191
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

can be tested. Many of the animation studies that we cited
in Section II were run in lab situations. This works well
for testing items in a controlled environment, but robots at
home need to work in more chaotic environments. Testing the
animations out in a home environment may be necessary to
see if the animation is helpful for the elderly.
Though we looked at movement conditions, an issue not
examined here is control. From our discussions in gathering
requirements from the elderly, people have different opinions
about a robot moving at home when they have control of its
movement versus it moving on its own. There is also a question
about what control means in a home situation with the elderly.
In Section I, we highlighted the idea of the elderly asking the
robot to leave, but are there points when the robot should stay?
Can it easily be called back?
As Chanseau, Lohan, and Aylett [38] found, people who
wanted a feeling of control also wanted robots to be more au-
tonomous. The size of the robot and a person’s anxiety towards
robots also inﬂuences proxemics. These issues are important
when introducing a robot—especially moving robots—in the
home of the elderly. Introducing a robot that can detect falls
beneﬁts no one if it moves around the home and becomes an
obstacle to stumble over in everyday life. Then, it is a fall
creator for the elderly instead of a detector.
Finally, this classiﬁcation focused on a single robot and
a single person at home. But there are many questions one
could explore to expand or apply this in other areas. Are
there other situations outside of home where this classiﬁcation
applies as well? What happens when you add more “moving
parts” like other people and robots? Does animating a robot
work in all situations? What about animating robots that have
limited movement? These are all questions to explore in future
research.
VI.
CONCLUSION
We have deﬁned animation, movement, familiarity, and
style. Using these deﬁnitions, we have looked at movement
of robots in the home and classiﬁed the movement in relation
to humans and their movements. We have found parallels with
other types of movement in the home and ways that having
a robot moving in the home may be unfamiliar to someone.
We have also suggested animating the robot will make it move
with style. This style can give the robot a personality and make
the robot more familiar to people living at home.
We have started our investigation with the elderly by run-
ning focus groups and discussing the issues of robots at home
and how a robot’s appearance and movement affects them.
The information and the elderly’s opinions have been helpful,
and they seem interested in what things robots can do. We
will be presenting this in future work and are integrating their
feedback into our future activities. We hope our classiﬁcation
of movement and incorporating animation can help in this.
ACKNOWLEDGMENTS
This work is partly supported by the Research Council of
Norway as a part of the Multimodal Elderly Care Systems
(MECS) project, under grant agreement 247697. Thanks also
to Tone Bratteteig, Hanne Cecilie Geirbo, Guri Verne, and
Diana Saplacan for comments and suggestions.
REFERENCES
[1]
G. Hagemann. (2015). Babyboom og eldrebølge, [Online].
Available from: http://www.norgeshistorie.no/oljealder- og-
overﬂod/hus- og- hjem/1915- babyboom- og- eldrebolge.html
[retrieved: 2018-01-17].
[2]
D. C. Sønderg˚ard, “Future Challenges and the Role of Welfare
Technology,” Nordic Center for Welfare and Social Issues,
Stockholm, Sweden, 2014.
[3]
T. Schulz, K. S. Fuglerud, H. Arfwedson, and M. Busch, “A
Case Study for Universal Design in the Internet of Things,”
in Universal Design 2014: Three Days of Creativity and
Diversity, H. Caltenco, P.-O. Hedvall, A. Larsson, K. Rassmus-
Gr¨ohn, and B. Rydeman, Eds., IOS Press, 2014, pp. 45–54,
ISBN: 978-1-61499-403-9.
[4]
T. Halbach and T. Schulz, “MobileSage – A Prototype Based
Case Study for Delivering Context-Aware, Accessible, and
Personalized On-Demand Help Content,” International Journal
of Advances in Intelligent Systems, vol. 7, pp. 267–278, 1 &
2 2014, ISSN: 1942-2679.
[5]
S. Kido, T. Miyasaka, T. Tanaka, T. Shimizu, and T. Saga,
“Fall detection in toilet rooms using thermal imaging sensors,”
in 2009 IEEE/SICE International Symposium on System Inte-
gration (SII), 2009, pp. 83–88. doi: 10.1109/SI.2009.5384550.
[6]
T. Makimoto and T. T. Doi, “Chip technologies for Entertain-
ment Robots - present and future,” in Digest. International
Electron Devices Meeting,, 2002, pp. 9–16. doi: 10 . 1109 /
IEDM.2002.1175768.
[7]
K. Wada, T. Shibata, T. Saito, and K. Tanie, “Psychological
and social effects of robot assisted activity to elderly people
who stay at a health service facility for the aged,” in 2003
IEEE International Conference on Robotics and Automation
(Cat. No.03CH37422), IEEE, 2003, 3996–4001 vol.3, ISBN:
0-7803-7736-2.
[8]
Animation, in American Heritage Dictionary of the English
Language, 5th, Houghton Mifﬂin Harcourt Publishing Com-
pany, 2011.
[9]
P. E. Gallaher, “Individual differences in nonverbal behavior:
Dimensions of style.,” Journal of personality and social psy-
chology, vol. 63, no. 1, p. 133, 1992.
[10]
J. E. Young, T. Igarashi, E. Sharlin, D. Sakamoto, and J. Allen,
“Design and Evaluation Techniques for Authoring Interactive
and Stylistic Behaviors,” ACM Trans. Interact. Intell. Syst.,
vol. 3, no. 4, 23:1–23:36, 2014, ISSN: 2160-6455. doi: 10.
1145/2499671.
[11]
F. Thomas and O. Johnston, The Illusion of Life: Disney Ani-
mation, 1st Hyperion ed. New York: Hyperion, 1995, 575 pp.,
ISBN: 978-0-7868-6070-8.
[12]
C. Breazeal, Designing Sociable Robots, ser. Intelligent
Robotics and Autonomous Agents. Boston, Massachusetts,
USA.: MIT Press, 2002, ISBN: 978-0-262-02510-2.
[13]
A. J. N. van Breemen, “Bringing robots to life: Applying
principles of animation to robots,” in Proceedings of Shap-
ping Human-Robot Interaction Workshop Held at CHI 2004,
Citeseer, 2004, pp. 143–144.
[14]
A. van Breemen, X. Yan, and B. Meerbeek, “iCat: An Ani-
mated User-interface Robot with Personality,” in Proceedings
of the Fourth International Joint Conference on Autonomous
Agents and Multiagent Systems, ser. AAMAS ’05, New York,
NY, USA: ACM, 2005, pp. 143–144, ISBN: 978-1-59593-093-
4. doi: 10.1145/1082473.1082823.
[15]
C. Bartneck, M. Verbunt, O. Mubin, and A. Al Mahmud,
“To Kill a Mockingbird Robot,” in 2007 2nd ACM/IEEE
International Conference on Human-Robot Interaction (HRI),
2007, pp. 81–87. doi: 10.1145/1228716.1228728.
[16]
C. Bartneck, M. van der Hoek, O. Mubin, and A. Al Mahmud,
““Daisy, daisy, give me your answer do!” switching off a
192
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

robot,” in 2007 2nd ACM/IEEE International Conference on
Human-Robot Interaction (HRI), 2007, pp. 217–222.
[17]
C. Bartneck, T. Kanda, O. Mubin, and A. Al Mahmud, “The
perception of animacy and intelligence based on a robot’s em-
bodiment,” in 2007 7th IEEE-RAS International Conference on
Humanoid Robots (Humanoids 07), IEEE, 2007, pp. 300–305,
ISBN: 978-1-4244-1861-9.
[18]
L. Takayama, D. Dooley, and W. Ju, “Expressing Thought:
Improving Robot Readability with Animation Principles,” in
Proceedings of the 6th International Conference on Human-
Robot Interaction, ser. HRI ’11, New York, NY, USA: ACM,
2011, pp. 69–76, ISBN: 978-1-4503-0561-7. doi: 10 . 1145 /
1957656.1957674.
[19]
M. J. Gielniak and A. L. Thomaz, “Generating anticipation
in robot motion,” in 2011 RO-MAN, 2011, pp. 449–454. doi:
10.1109/ROMAN.2011.6005255.
[20]
——, “Enhancing Interaction Through Exaggerated Mo-
tion
Synthesis,”
in
Proceedings
of
the
Seventh
Annual
ACM/IEEE International Conference on Human-Robot Inter-
action, ser. HRI ’12, New York, NY, USA: ACM, 2012,
pp. 375–382, ISBN: 978-1-4503-1063-5. doi: 10 . 1145 /
2157689.2157813.
[21]
W.-Y. G. Louie, D. McColl, and G. Nejat, “Acceptance and
Attitudes Toward a Human-like Socially Assistive Robot by
Older Adults,” Assistive Technology, vol. 26, no. 3, pp. 140–
150, 2014, ISSN: 1040-0435. doi: 10.1080/10400435.2013.
869703.
[22]
K. Baraka, S. Rosenthal, and M. Veloso, “Enhancing human
understanding of a mobile robot’s state and actions using
expressive lights,” in 2016 25th IEEE International Symposium
on Robot and Human Interactive Communication (RO-MAN),
2016, pp. 652–657. doi: 10.1109/ROMAN.2016.7745187.
[23]
P. Agre, “Changing places: Contexts of awareness in comput-
ing,” Human Computer Interaction, vol. 16, pp. 177–192, 2-3
2001.
[24]
K. R. Gibson and T. Ingold, Tools, Language and Cognition in
Human Evolution. Cambridge University Press, 1994, 504 pp.,
ISBN: 978-0-521-48541-8.
[25]
P. Turner and G. van de Walle, “Familiarity as a basis for
universal design,” Gerontechnology, vol. 5, no. 3, pp. 150–159,
2006, ISSN: 1569-111X. doi: 10.4017/gt.2006.05.03.004.00.
[26]
T. Winograd and F. Flores, Understanding Computers and
Cognition: A New Foundation for Design. Intellect Books,
1986, 236 pp., ISBN: 978-0-89391-050-1.
[27]
G. Van de Walle, P. Turner, and E. Davenport, “A study
of familiarity,” in Human-Computer Interaction-INTERACT,
vol. 3, 2003, pp. 463–70.
[28]
J. Harrigan and R. Rosenthal, New Handbook of Methods in
Nonverbal Behavior Research. Oxford University Press, 2008.
[29]
E. T. Hall, The Hidden Dimension, 1st. New York, NY, US:
Doubleday & Co, 1966, vol. xii, 201 pp.
[30]
M. L. Walters, K. Dautenhahn, R. Te Boekhorst, et al., “An
empirical framework for human-robot proxemics,” in Procs
of New Frontiers in Human-Robot Interaction, Edinburgh,
Scotland, 2009.
[31]
M. J. Gielniak, C. K. Liu, and A. L. Thomaz, “Secondary
action in robot motion,” in 19th International Symposium in
Robot and Human Interactive Communication, 2010, pp. 310–
315. doi: 10.1109/ROMAN.2010.5598730.
[32]
W. A. Bainbridge, J. Hart, E. S. Kim, and B. Scassellati, “The
effect of presence on human-robot interaction,” in RO-MAN
2008 - The 17th IEEE International Symposium on Robot and
Human Interactive Communication, 2008, pp. 701–706. doi:
10.1109/ROMAN.2008.4600749.
[33]
J. Mumm and B. Mutlu, “Human-robot Proxemics: Physical
and Psychological Distancing in Human-robot Interaction,” in
Proceedings of the 6th International Conference on Human-
Robot Interaction, ser. HRI ’11, New York, NY, USA: ACM,
2011, pp. 331–338, ISBN: 978-1-4503-0561-7. doi: 10.1145/
1957656.1957786.
[34]
M. Obaid, E. B. Sandoval, J. Złotowski, et al., “Stop! That
is close enough. How body postures inﬂuence human-robot
proximity,” in 2016 25th IEEE International Symposium on
Robot and Human Interactive Communication (RO-MAN),
2016, pp. 354–361. doi: 10.1109/ROMAN.2016.7745155.
[35]
M. Saerbeck and C. Bartneck, “Perception of Affect Elicited
by Robot Motion,” in Proceedings of the 5th ACM/IEEE In-
ternational Conference on Human-Robot Interaction, ser. HRI
’10, Piscataway, NJ, USA: IEEE Press, 2010, pp. 53–60, ISBN:
978-1-4244-4893-7.
[36]
M. L. Noordzij, M. Schmettow, and M. R. Lorijn, “Is an
Accelerating Robot Perceived As Energetic or As Gaining in
Speed?” In Proceedings of the 2014 ACM/IEEE International
Conference on Human-Robot Interaction, ser. HRI ’14, New
York, NY, USA: ACM, 2014, pp. 258–259, ISBN: 978-1-4503-
2658-2. doi: 10.1145/2559636.2559793.
[37]
G. Hoffman and W. Ju, “Designing Robots With Movement
in Mind,” Journal of Human-Robot Interaction, vol. 3, no. 1,
pp. 89–122, 2014, ISSN: 2163-0364. doi: 10.5898/JHRI.3.1.
Hoffman.
[38]
A. Chanseau, K. S. Lohan, and R. Aylett, “How motor speed of
a robot face can inﬂuence the “older” user’s perception of facial
expression?” In 2015 24th IEEE International Symposium
on Robot and Human Interactive Communication (RO-MAN),
2015, pp. 468–473. doi: 10.1109/ROMAN.2015.7333596.
193
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

