PATTERNS 2016 : The Eighth International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2016. ISBN: 978-1-61208-465-7
1
Learning Multi-Class Discriminative Patterns using
Episode-Trees
Eng-Jon Ong∗, Nicolas Pugeault†, Andrew Gilbert∗ and Richard Bowden∗
∗Centre for Vision, Speech and Signal Processing
University of Surrey, UK
Email: e.ong,a.gilbert,r.bowden@surrey.ac.uk
† College of Engineering, Mathematics and Physical Sciences,
University of Exeter, UK
Email: n.pugeault@exeter.ac.uk
Abstract—In this paper, we aim to tackle the problem of recognis-
ing temporal sequences in the context of a multi-class problem.
In the past, the representation of sequential patterns was used for
modelling discriminative temporal patterns for different classes.
Here, we have improved on this by using the more general
representation of episodes, of which sequential patterns are a
special case. We then propose a novel tree structure called
a MultI-Class Episode Tree (MICE-Tree) that allows one to
simultaneously model a set of different episodes in an efﬁcient
manner whilst providing labels for them. A set of MICE-Trees
are then combined together into a MICE-Forest that is learnt in a
Boosting framework. The result is a strong classiﬁer that utilises
episodes for performing classiﬁcation of temporal sequences. We
also provide experimental evidence showing that the MICE-Trees
allow for a more compact and efﬁcient model compared to
sequential patterns. Additionally, we demonstrate the accuracy
and robustness of the proposed method in the presence of
different levels of noise and class labels.
Keywords–Data Mining, Classiﬁcation, Episodes Patterns, De-
cision Trees
I.
INTRODUCTION
There are many problems in machine learning where the
data consists of temporal sequence of events. In this work,
we consider the problem where we have a collection of data
streams that we want to label according to a large number of
classes. Although there has been signiﬁcant work concerned
with mining frequently occurring patterns in data streams, few
studies have focused on the different task of classifying such
data streams. When solving this problem, we face several
challenges: 1) a large number of classes may lead to bloated
models, and large class confusion; 2) learning discriminative
sequences rather than frequently occurring ones; 3) relevant
sequences that contain discriminative power may be sparse in
the data stream; and 4) the presence of ambiguities in the
ordering of some parts of a discriminative sequence. In order
to address these problems, this article presents a theoretical
framework for learning of discriminative temporal sequences
based on episodes [1] that are structured efﬁciently within a
tree. We show how multiple episode-trees can be combined in
a Boosted framework to yield an accurate and robust classiﬁer.
There is a signiﬁcant amount of prior research that investi-
gate the discovery of frequently occurring temporal sequences,
represented using sequential patterns. There are two main
approaches for mining frequent sequential patterns: Apriori-
based methods [2], [3] and pattern growth methods [4], [5].
An alternative to sequential patterns is the representation of so-
called “episodes”. Episodes are a more generic representation
of temporal patterns proposed by Mannila et al.[1], [6] that
allow the formalisation of ambiguity in the sequencing of some
events in the pattern (whereas sequential patterns represent
strict ordering). They also proposed algorithms for ﬁnding
frequent episodes in data streams, in the limited case of either
serial or parallel episodes. More recently, two independent
groups have proposed algorithms for mining frequent episodes
in the general case [7], [8], [9].
The present work addresses a different problem: our aim is
the classiﬁcation of data streams, and therefore the sequence
of events are learnt for maximizing discrimination between the
classes. Hence, a pattern may be discriminative for a single
class amongst many without being frequent over the whole
dataset. An early attempt at learning discriminative sequential
patterns was proposed by Nowozin [10] and showed promising
results for action recognition, yet the proposed approach is
limited to binary discrimination. For problems containing more
than two classes, this entails learning a collection of 1 versus
1 classiﬁers within a voting framework—this approach is
clearly not scalable to problems with large number of classes.
Recently, Ong et al. [11], [12] proposed a scalable approach
to multi-class sequential pattern classiﬁcation that makes use
of boosted Sequential Pattern trees (SP-trees) for learning
discriminative sequential patterns. One essential property of
this approach is that the SP-trees allow for feature sharing
between sequential patterns that describe different classes. To
our knowledge these articles are the only attempts at learning
discriminative sequential patterns.
This article extends SP-trees to the more general formalism
provided by episodes. We will show that for certain speciﬁc
patterns, episode trees will allow for a more compact encoding
of discriminative temporal patterns. This article has four main
contributions: ﬁrst, we present a theoretic deﬁnition of a
tree structure called MultI-Class Episode Trees (MICE-Trees)
allowing for multiple classes to share sub-episodes; second, we
propose an efﬁcient algorithm for learning Boosted collections
of such multi-class trees; third we demonstrate that MICE-
Trees are similar to SP-Trees, but allow for a better and more
compact representation of certain type of temporal sequences;
and fourth, we show that the resulting classiﬁers can cope
with a large amount of signal noise while still providing good
classiﬁcation accuracy.

PATTERNS 2016 : The Eighth International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2016. ISBN: 978-1-61208-465-7
2
Figure 1. Illustration of a generic episode α, of the form
(a, c) → (b) → (c).
The rest of the paper is organised as follows: section 1
will provide the theoretical framework of episodes and some
important results; section 2 will present the MICE-Trees and
provide proofs that paths in such a tree are effectively episodes;
section 3 will describe an efﬁcient algorithm for learning such
MICE-Trees from a dataset of labelled streams; section 4 will
present an approach for learning a Boosted forests of MICE-
Trees; section 5 will present two synthetic dataset that are used
to evaluate the performance and robustness of the proposed
approach; and ﬁnally we will conclude in section 6.
II.
PROBLEM STATEMENT
Given a vocabulary of events E = {E1, . . . , E|E|}, we
deﬁne a data stream S ∈ SE (where SE is the set of all
possible data streams for a vocabulary E), as a sequence of
pairs
S =

(E1, t1), . . . , (E|S|, t|S|)

,
(1)
where (Ei, ti) denotes the occurence of the transient event Ei
at time ti—where the time labels ti are such that ti ≤ tj ∀i <
j.
Given a collection of data streams D = {(Si, λi)}i∈[1,|D|,
with associated class labels λi ∈ L, the task of stream
classiﬁcation can be seen as the function
F : SE → L,
(2)
that associates labels λi to data streams Si. Speciﬁcally, this
article proposes an approach for learning F from a set D of
labelled streams using discriminative episodes.
A. Episodes
Following from Mannila et al. [6], we deﬁne an episode as
Deﬁnition 1. An episode is deﬁned as α = (Vα, <α, gα),
where Vα =

v1, . . . , v|v|
	
is a collection of vertices, <α⊆
Vα × Vα is a strict partial order, and g:Vα → E is a mapping
between episode vertices and observed events.
where
Deﬁnition 2. The relation <α is a strict partial order, iff.
∀(a, b) ∈<α, we have:
a ̸= b
(3)
(b, a) /∈<α
(4)
(b, c) ∈<α⇒ (a, c) ∈<α
(5)
This is illustrated in Fig. 1. In this ﬁgure, each green
box corresponds to an episode vertex vi, and the letter in
the box corresponds to the mapped event Ej = gα(vi), with
Figure 2. Illustration of a parallel episode (left) of the form (a, b) and of a
serial episode (right), of the form (a) → (b) → (c).
an alphabet E = {a, b, c}. The blue arrows joining the boxes
represent the serial ordering between vertices enforced by the
strict partial order <α.
Moreover, we deﬁne sequential patterns as special cases,
coined serial episodes(see Fig. 2, right):
Theorem 1. If <+
α is a strict total order, then α = (Vα, <+
α
, gα) is a serial episode (i.e., a sequential pattern).
Proof: If <+
α is a strict total order, then ∀a, b ∈ Vα, a ̸= b,
we have
(a, b) ∈<+
α or (b, a) ∈<+
α,
hence there exists a sequence (β1, . . . , β|Vα|) such that
∀i, j ∈ [1..|Vα|], i < j ⇒ (vβi, vβj) ∈<α
(6)
and therefore (gα(vβi))|Vα|
i=1 is a sequential pattern.
and conversely we deﬁne parallel episodes (illustrated in
the left panel of Fig. 2):
Deﬁnition 3. If <α= ∅, then α is a parallel episode.
Finally we deﬁne a relation α ⊑ S which states that an
episode α occurs within a stream S ∈ S:
Deﬁnition 4. Let α be an episode and S ∈ S be a stream, we
deﬁne that α ⊑ S, iff. there exists a sequence β1, . . . , β|Vα|
such that ∀i, j ∈ [1..|Vα|], i < j ⇒ (vβi, vβj) ∈<α, and that
∃{ti}|Vα|
i=1 ,

(gα(vβi), ti)|Vα|
i=1

⊂ S.
This is illustrated in Fig. 3, where the red arrows indicate
the occurrences of the episode α’s vertices vi, mapped to
events a, b, c ∈ E in the stream S. Note that the sequence
of the occurrence of events a and b for the episode vertices v1
and v2 do not matter (they form a parallel episode).
B. MultIClass Episode Trees (MICE-Trees)
This section presents a deﬁnition of the MICE-Trees and
how paths to a MICE-Tree’s leaves model different episodes
αk.
First, we deﬁne a MICE-Tree as T = (N, L), where N =
{ni}|N|
i=1 is the set of all tree nodes and L = {li}|L|
i=1 the set of
all links, such that L ⊆ N × N—such a tree is illustrated in
Fig. 4. In the following section we will deﬁne the nodes and
links of the MICE-Trees and their speciﬁcity for the purpose
of learning and encoding episodes.
1) Tree nodes n ∈ N: MICE-Tree nodes have the double
purpose of storing episode information and the most likely
class label given the data. Formally:
Deﬁnition 5. A MICE-Tree node n ∈ N is deﬁned as the
tuple n = (Vn, gn, λn), where λn ∈ L is the most likely label
at this node, Vn is a set of vertices and gn is a mapping such

PATTERNS 2016 : The Eighth International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2016. ISBN: 978-1-61208-465-7
3
Figure 3. Illustration of how an episode α is matched in a data stream S, such as α ⊑ S.
that ∀vi ∈ Vn, ∃Ei ∈ E such that Ei = gn(vi) and {Ei}|Vn|
i=1
is a set of (unordered) events.
Without loss of generality, we enforce that the set of
vertices in all tree nodes be strictly disjoint:
Deﬁnition 6. For any two nodes n, n′ ∈ N, if n ̸= n′ then
V∗(n) ∩ V∗(n′) = ∅
Moreover, it follows from the deﬁnition that:
Property 1. A tree node n ∈ N models a (trivial) parallel
episode α = (V∗(n), ∅, g∗(n)).
For convenience we deﬁne the following accessor functions
for the properties of a node n = (Vn, gn, λn), namely: V∗(n) =
Vn, g∗(n) = gn and λ∗(n) = λn.
2) Tree links l ∈ L: Now that we have deﬁned the MICE-
Tree nodes as encoding the parallel components of episodes,
we will deﬁne how the tree links encode the serial constraints
in the episode, and whether these are satisﬁed or not by data
streams. Formally, we deﬁne a tree link as follows:
Deﬁnition 7. A MICE-Tree link l ∈ L is deﬁned as the tuple
l = (a, b, s) where a, b ∈ N are nodes in the tree and s ∈
{+, −} is the type of edge (positive or negative).
For convenience, we deﬁne the root of the tree as
Deﬁnition 8. We call root node of a tree T = (N, L) the
unique node n0 ∈ N that satisﬁes the condition ∄l ∈ L, n ∈
N, s ∈ {+, −} such that l = (n, n0, s).
and its leaves:
Deﬁnition 9. We say that a node n∗ ∈ N is a leaf node if
∄l ∈ L, n ∈ N, s ∈ {+, −} such that l = (n∗, n, s).
Without loss of generality, we will deﬁne that leaf nodes
are the only nodes in the tree containing no vertices, hence we
have:
Property 2. For any node n ∈ N, we have V∗(n) = ∅ iff. n
is a leaf node.
This property is ensured by the learning mechanism de-
scribed in section 3.
In this work, we restrict ourselves to using binary trees,
whereby every non-leaf tree-node (n ∈ N) will have exactly
two child tree-nodes n+, n− ∈ N, where (n, n+, +) ∈ L and
(n, n−, −) ∈ L. Also, as for the nodes we deﬁne for links
l = (pl, cl, sl) the accessor function s∗(l) = sl.
Property 1 showed that MICE-Tree nodes model parallel
episodes; conversely, we will now show that links can be
interpreted as the serial components of episodes. As a ﬁrst
step, we need to show that two nodes connected by a link
form a strict partial order (see deﬁnition 2).
Deﬁnition 10. We deﬁne a function ϕ between two sets of
vertices Va and Vb, such that ϕ(Va, Vb) = {(x, y) : x ∈ Va, y ∈
Vb}.
Theorem 2. If Va ∩ Vb = ∅ then ϕ(Va, Vb) is a strict partial
order.
Proof: Let (a, b) ∈ ϕ(Va, Vb), then from deﬁnition 10, we
have a ∈ Va and b ∈ Vb, and therefore Va ∩ Vb = ∅ implies:
(1) a ̸= b; (2) b /∈ Va and therefore ∀c ∈ Vb, (b, c) /∈ ϕ(Va, Vb)
From deﬁnitions 6, 7, 10 and theorem 2, we derive the
following property for linked nodes:
Corollary 1. A tree node n ∈ N models a parallel episode
α = (V∗(n), ∅, g∗(n)), and a pair of nodes n and n′ connected
by a link l models the general episode α′, such as:
α′ = (V∗(n)∪V∗(n′), ϕ(V∗(n), V∗(n′)), g∗(n)∪g∗(n′)). (7)
3) Tree Paths P ⊂ T: In the previous section we have
presented the MICE-Tree nodes and edges, we will now deﬁne
paths in the tree, and show that any path in a MICE-Tree
encodes an episode.
First we will deﬁne a tree path:
Deﬁnition 11. A path P is a sequence of node-edge pairs
P = ((n1, l1), . . . , (nK, lK)), where K is the length of the
path, and all the nodes Ni in the path are connected by their
edges. Formally, li = (ni, ni+1, si), ∀i ∈ [1, K −1], and lK =
(nK, n∗, sK), where n∗ is a leaf node.
and leaf nodes:
Deﬁnition 12. We say that a node n∗ = (∅, ∅, λ) ∈ N is a leaf
node if ∄l ∈ L, n ∈ N, s ∈ {+, −} such that l = (n∗, n, s).
Then we deﬁne the function ξ that generates an episode
from any MICE-Tree path:
Deﬁnition 13. Let P = ((n1, l1), . . . , (n|P |, l|P |)) be a path
in tree T, and let ψ(P) ⊂ P be the indices of positive links
in the path: ψ(P) = {i : i ∈ [1, |P|], s∗(li) = +}. Then we

PATTERNS 2016 : The Eighth International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2016. ISBN: 978-1-61208-465-7
4
deﬁne the function ξ(P) = (VP , <P , gP ), such that



VP
=
S
i∈ψ(P ) V∗(ni)
<P
=
S
i∈ψ(P )
S
j∈{k∈S(P ):k>i} ϕ(V∗(ni), V∗(nj))
gP
=
S
i∈ψ(P ) g∗(ni)
.
(8)
Lemma 1. If P is a path in a MICE-Tree gP is a map such
that gP : VP → E.
Proof: For all nodes n in the tree, we have g∗(n) a map
such that g∗(n) : V∗(n) → E, ∀n ∈ N (deﬁnition 5) and
V∗(ni) ∩ V∗(nj) = ∅ ∀ni, nj ∈ N, ni ̸= nj (deﬁnition 6),
therefore S
i∈ψ(P ) V∗(ni) is a map.
Lemma 2. If P is a path in a MICE-Tree <P is a strict partial
order.
Proof: From deﬁnition 10, ϕ(V∗(ni), V∗(nj)) is a strict
partial order for any pair of nodes ni, nj ∈ N, i ̸= j (i), and
deﬁnition 6 enforces that the sets of vertices of all tree nodes
are disjoint V∗(ni) ∩ V∗(nj) = ∅ if ni ̸= nj, ∀ni, nj ∈ N (ii),
therefore we have:
From deﬁnition 2, <P is a strict partial order iff:
(1) (a, b) ∈<P implies that a ∈ V∗(ni) and b ∈ V∗(nj)
with i ̸= j, and therefore because V∗(ni) ∩ V∗(nj) = ∅ from
(ii), we have a ̸= b
(2) According to deﬁnition 13, (a, b) ∈<P implies that
a ∈ V∗(ni) and b ∈ V∗(nj) with i < j, and therefore because
V∗(ni) ∩ V∗(nj) = ∅ we have (b, a) /∈<P .
(3) let (a, b) ∈<P , and (b, c) ∈<P , from deﬁnition 7 we
have a ∈ V∗(ni), b ∈ V∗(nj) and c ∈ V∗(nk) such that
i, j, k ∈ ψ(P), and from deﬁnition 13, i < j < k, and therefore
(a, c) ∈<P .
Therefore, from lemmas 1 and 2 we can draw the following
theorem:
Theorem 3. If P is a path in a MICE-Tree T, then ξ(P) =
(VP , <P , gP ) is an episode.
Proof: According to deﬁnition 1, the triplet (VP , <P , gP )
deﬁnes an episode iff. (1) gP : VP → E, proved by lemma 1;
and (2) <P is a strict partial order, proved by lemma 2.
An example of a MICE-Tree is then illustrated in Fig. 4. In
this ﬁgure, each green square denote a non-leaf node n ∈ N,
that code a parallel episode—the letters in the box indicate the
events Ei ∈ {g∗(n)(v) : v ∈ V∗(n)}. The blue boxes denote
leaf nodes n∗ associated to a valid label λ∗(n∗) ∈ {c1, c2, c3},
and below are indicated the episodes that correspond to each
leaf node’s path. Finally, the orange boxes denote rejection leaf
nodes where no assertion can be made on the episode’s class
and the blue arrows denote tree links l ∈ L.
C. Classifying Datastreams using MICE-Trees
In order to classify an input sequence S given a MICE
Tree T = (N, L), we deﬁne the following recursive function
CT : N × Sε → L:
CT (n, S) =



CT (n+, S)
if
ξ(Pn) ⊑ S, Vn ̸= ∅
CT (n−, S)
if
ξ(Pn) ̸⊑ S, Vn ̸= ∅
λn
otherwise
(9)
Figure 4. Example of a MICE-Tree.
where the tree node n is the triplet, n = (Vn, gn, λn),
and n+, n− are the positive and negative child-nodes of n
respectively: (n, n+, +), (n, n−, −) ∈ L. It is now possible
to deﬁne a multi-class classiﬁcation function for labelling an
input sequences given a MICE-Tree with root node r:
hT (S) = CT (r, S)
(10)
This is achieved in an computationally efﬁcient manner using
Algorithm 1.
Algorithm 1 MICE-Tree Classiﬁer: P = CT (S)
Input: Input datastream S =< (Si, ti) >|S|
i=1
Input: MICE-Tree T = (N, L), root node r.
Output: Label of S : λ ∈ L
Initialise current node to root node: ncur = r
The contents of current node: ncur = (Vcur, gcur, λcur)
Init start offset: e = 1
while ncur is not leaf-node do
For each k ∈ [1, |Vcur|] get:
Gk = {j : j ∈ [e, |S|], Sj = gcur(vk)}
Z = T
k∈[1,|Vcur|][min(Gk), |S|]
if Z = ∅ then
ncur = m, such that (ncur, m, −1) ∈ L
else
ncur = m, such that (ncur, m, +1) ∈ L
e = mink∈[1,|Vcur|](min(Gk))
end if
end while
Return λcur
III.
LEARNING MICE TREES
In this section, we propose a novel method for constructing
a MICE-Tree given a database of weighted and labelled data
streams. Here, the construction of a MICE-Tree is achieved
in a greedy and recursive manner, whereby the multi-class
training dataset is recursively partitioned into smaller and
smaller subsets that are distributed across different nodes of
the MICE-Tree. To achieve this, we theoretically show that a
node in the MICE-Tree does indeed induce a binary partition
on the training dataset in Section III-A. However, it is also

PATTERNS 2016 : The Eighth International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2016. ISBN: 978-1-61208-465-7
5
important that there is a method for measuring the optimality
of such a binary partition. To this end, a node-split criteria
based on the Gini impurity measure is described in Section
III-B. Finally, the MICE-Tree learning algorithm is described
in Section III-C
A. MICE-Tree Node Binary Partition
For the purpose of learning, we are provided with a training
data stream collection that we will denote as D (deﬁned in
Section II). Additionally, we are given a set of weights W =
(wi)|D|
i=1, where each example (Si, λi) ∈ D is associated with
the weight wi.
In order to construct a MICE-Tree T, we ﬁrstly introduce
a useful function D : Sε → 2N based on Eq. 9 that extracts
the set of nodes a datastream example X passes through when
it is classiﬁed by T (using Eq 9):
DT (n, S) = n∪



DT (n+, S)
if
ξ(Pn) ⊑ S, Vn ̸= ∅
DT (n−, S)
if
ξ(Pn) ̸⊑ S, Vn ̸= ∅
∅
otherwise
(11)
Both functions in DT (Eq. 11 and CT (Eq. 9) provides a
deterministic mechanism for classifying an input datastream:
Remark 1. Let S be an input datastream, T = (N, L) be a
MICE-Tree and r its root node, then there exists only one
unique path, whose nodes are DT (r, S), that will be used to
classify it. That is, a sequence S will always go through the
same path when C is used to classify it.
It can be observed that the properties of Eq. 11 will allow
us to see that each node in a MICE-tree “captures” a subset
of training datastreams from D: Since each example training
datastream can only “carve” a single unique path through a tree
during classiﬁcation (Rem. 1), then, only a subset of example
datastreams will pass through a particular node in a tree. More
speciﬁcally:
Deﬁnition 14. Let T = (N, L) be a MICE-Tree, D be the
labelled collection of datastreams. Let n ∈ N be a node and
m ∈ N be its parent node, with Pm being the path from the
root node to m. We then deﬁne the set of captured example
indices of n as the following set: In = {i ∈ [1, |D|] : ξ(Pm) ⊑
Si, n ∈ D(r, Si)}.
A consequence of the above deﬁnition is that a non-
leaf node n will induce a binary partitioning of its captured
example set In into a positive partition, I+
n , and negative
partition, I−
n where I = I+
n ∪ I−
n and:
I+
n = {i ∈ I : ξ(Pn+) ⊑ Si}
(12)
I−
n = {i ∈ I : ξ(Pn+) ̸⊑ Si}
(13)
The partitioning induced by a node n is clearly dependent
on its contents (i.e. Vn and gn). Thus, for learning purposes,
we deﬁne the following operation on a node:
Deﬁnition 15. Given a node n = (Vn, gn, λn) in a MICE-
Tree, a new (episode vertex,event) pair (v′, e′) can be added
into n, denoted as n′ = n + (v′, e′), where e′ ∈ ε and a v′ is
an episode-vertex. The new node is: n′ = (Vn′, gn′, λn′) with
Vn′ = Vn ∪ {v′}, gn′ = gn ∪ {(v′, e′)} and λn′ = λn),
Importantly, adding (episode-vertex, event) pairs into any
node has the effect of moving examples from its positive
partition to the negative partition.
Theorem 4. Let n = (Vn, gn, λn) be a node in a MICE-
Tree, In the index set of examples captured by n and I+
n , I−
n ,
its induced partition respectively. Then, adding an (episode-
vertex,event) pair, (v′, e′) to n will produce a new node, n′ =
n+(v′, e′), with a potentially smaller positive partition, I+
n′ ⊂
I+
n and potentially larger negative partition, I−
n′ ⊃ I−
n .
Proof: It is clear that the episode modelled by the path
from the root node r to n is a subsequence that from r to n′:
ξ(Pn) ⊑ ξ(Pn′). Then, by the Apriori rule, we have {i ∈ In :
ξ(Pn′)} ⊂ {i ∈ In : ξ(Pn)}
Algorithm 2 GetBestSplit Algorithm
Input: Train Set: S = {(Si, λi, wi)}|S|
i=1
Output: MICE-Tree Node, +ve partition, -ve partition, error
Let λS be the class with highest frequency in S.
Initialise empty node: n = (∅, ∅, λS).
Initialise the event set: E = E.
Initialise error: γbest = −1
while E ̸= ∅ do
Let v′ be an episode-vertex not present in any tree-node.
Find ebest ∈ E s.t. n + (v′, ebest) minimises γ from
Eq. 14 given weights wi.
if γbest < 0 or γbest > γ then
n = n + (v′, ebest)
E = E − {ebest} {Remove ebest from the set E}
else
break
end if
end while
Let the path from the root node to n be Pn.
S+
n = {(S, λ, w) ∈ S : ξ(Pn) ⊑ S}
S−
n = {(S, λ, w) ∈ S : ξ(Pn) ̸⊑ S}
Return (n, S+
n , S−
n , γbest).
B. Node-Split Criteria
The ﬁnal tool required for learning tree is a method of
evaluating how “good” a node split is. This is achieved using
an adapted Gini impurity that is popular in decision tree
learning [13]. Here, we have changed the criteria to account
for weighted training examples. Suppose we have found that
non-leaf node n has caused a partition I+ and I−. Suppose
the corresponding weights of these partitions are W ′
+ and
W ′
− respectively. Similarly, let the corresponding labels be Y ′
+
and Y ′
− respectively. We deﬁne the total positive and negative
partition weight coefﬁcient as: Z+ = P|W ′
+|
i=1 w′
+,i/ P|W ′|
i=1 w′
i
and Z− = P|W ′
−|
i=1 w′
−,i/ P|W ′|
i=1 w′
i respectively. Using both
the weights and labels, it is possible to compute a normalised
label histogram for each partition: F ′
+ and F ′
− respectively.
The node-split criteria is deﬁned as:
γ = Z+(1 −
C
X
i=1
f 2
+,i) + Z−(1 −
C
X
i=1
f 2
−,i)
(14)

PATTERNS 2016 : The Eighth International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2016. ISBN: 978-1-61208-465-7
6
Algorithm 3 MICE-Tree Learn Algorithm
Input: Training Set: {(Si, λi)}|S|
i=1, (wi)|S|
i=1
Output: MICE-Tree T = (N, L)
Make Offseted Train Set: S = {(Si, λi, wi, oi)}|S|
i=1, oi = 1
Queue element: (ParentNode, TrainSubset, Depth, LinkType)
Get optimal root node:
(r, S+, S−, ϵ) = GetBestSplit(S)
Initialise the queue:
Q = {(r, S+, 2, +), (R, S−, 2, −)}.
while Q ̸= ∅ do
Remove last item of Q: (ncur, Scur, Dcur, scur)
if |Scur| ≤ α OR Dcur ≥ β then
Get class (λ) with highest weighted freq. in Scur
λ = −1 if Scur = ∅.
m = (∅, ∅, λ)
N = N ∪ {m} {Add new tree-node}
L = L ∪ {(ncur, m, scur)} {Link to new tree-node}
break
end if
Get optimal current node:
(m, S+, S−, ϵ) = GetBestSplit(Scur)
N = N ∪ {m} {Add new tree-node}
L = L ∪ {(ncur, m, scur)} {Link to new tree-node}
if ϵ > 0 then
Q = Q ∪ {(m, S+, Dcur + 1, +), (m, S−, Dcur +
1), −}.
end if
end while
Return MICE-Tree: T = (N, E)
C. MICE-Tree Learning Algorithm
The algorithm for learning the MICE-Tree is given in Algo.
3, where a MICE-Tree is constructed in a greedy and recursive
manner. One key mechanism for constructing MICE-Trees
is given in Algo. 2. Here, episode-vertices are sequentially
added in a greedy fashion to the parent nodes of leaf nodes to
maximally improve the splitting criteria (Eq. 14). This process
terminates when the splitting criteria cannot be improved.
Algo. 3 then starts by constructing the root node using Algo
2. This induces a partitioning of the dataset into two training
subsets (i.e. +ve and -ve partitions). Two new children tree
nodes (+ve and -ve) are constructed and linked to the root
node. Each training subset is then passed on to its respective
child node. The algorithm then recursively applies Algo 2
to conﬁgure the contents of both child nodes.This recursive
process is performed via a queue-based system until one of
3 termination criteria is reached: 1) maximum tree-depth β is
reached; 2) training subset is smaller than minimum size α (set
here as 1); 3) The training subset is “pure” (i.e. only belongs
to a single class).
IV.
BOOSTING MICE FORESTS
In this section, we describe the method for learning and
combining the multiple MICE-trees in order to produce a
robust and accurate classiﬁer that generalises to unseen novel
sequences in the presence of noise. To this end, we propose a
novel machine learning method for learning strong classiﬁers
based on MICE-Tree within the Multi-class AdaBoost frame-
work [14]. A strong classiﬁer outputs a class label based on the
maximum votes cast by a number (S) of selected and weighted
weak classiﬁers:
H(I) = arg max
c
S
X
i=1
αiI(hi(I) = c)
(15)
In this paper, the weak classiﬁers hi are the MICE-Tree
classiﬁers deﬁned in Section II-C as Eq. 10. Each weak
classiﬁer hi is selected iteratively with respect to the following
error:
ϵi =
X
X
i=1
I(hi(Xi) ̸= yi)
(16)
Typically, in order to determine the optimal weak classiﬁer at
each Boosting iteration, the common approach is to exhaus-
tively consider the entire set of possible weak classiﬁers and
ﬁnally select the best weak classiﬁer (i.e. that with the lowest
ϵi). Such an exhaustive search will not be possible due to the
large search space of possible MICE-Trees. Thus, Algo. 3 is
used for choosing the appropriate MICE-Tree weak classiﬁer
instead given a set of training datastreams with associated
boosted weights.
The ﬁnal MICE-Tree Boosting algorithm is detailed in
Algo. 4. We have chosen to iteratively learn new MICE-Tree
based on the multi-class AdaBoost method. However, we are
not limited to this particular form of Boosting and it would be
easy to integrate the MICE-Tree learning algorithm (Algo. 3)
into other Boosting methods (e.g. GentleBoost, etc...).
Algorithm 4 MICE-Tree-Boost Algorithm
Initialise example weights: ∀wi ∈ W, wi = 1/X
for t = 1, ..., M do
Select (ht = hTbest) using Algo. 3
Obtain the classiﬁcation error ϵt for ht (Eq. 16)
Obtain the weight αt = ln 1−ϵbest
ϵbest
+ ln(C − 1)
Update weights: wi = wi exp(−αi [ht(Xi) ̸= yi])
Normalise weights: PX
i=1 wi = 1
end for
Return the strong classiﬁer:
H(X) = arg maxc
PM
i=1 αiI(hi(X) = c)
V.
EXPERIMENTAL EVALUATION
In this section we evaluate the proposed approach using
two different sets of generated data streams, that allow us
to control for properties of the encoded episodes. The ﬁrst
experiment, in section V-A was designed to illustrate the
limitations of the more common Sequential pattern framework
(as used by [10], [11]), and the advantages of the more generic
episodes framework presented herein. The second experiment,
in section V-B was designed to have a thorough evaluation of
the MICE-Trees robustness to noise in the data streams.
A. Exp 1: Comparison with SP-trees
The ﬁrst experiments evaluates the special case where
classes cannot be discriminated by purely serial episodes, and
therefore form a special challenge for SPs and should beneﬁt
from the greater generality of the episode model.
In order to assess this, we generated a speciﬁcally designed
dataset where all serial episodes are ambiguous. We achieved

PATTERNS 2016 : The Eighth International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2016. ISBN: 978-1-61208-465-7
7
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
Error (mean over 100 runs)
Boosting rounds
Performance comparison (max depth 1)
MICE-Trees
SP-Trees
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
Error (mean over 100 runs)
Boosting rounds
Performance comparison (max depth 2)
MICE-Trees
SP-Trees
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
Error (mean over 100 runs)
Boosting rounds
Performance comparison (max depth 3)
MICE-Trees
SP-Trees
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
Error (mean over 100 runs)
Boosting rounds
Performance comparison (max depth 4)
MICE-Trees
SP-Trees
Figure 5. Performance of the classiﬁcation for the dataset A (10 classes). The graphs show the mean classiﬁcation error over 100 randomised tests, for rounds
of Boosting from 1 to 100 and for maximal tree depths from 1 to 4. The error bars show the standard deviations for all curves.
this by generating 10 classes A, . . . , J, each characterised by
a unique episode of the form: αλ = (Eλ
1 , Eλ
2 ) → (Eλ
3 , Eλ
4 ),
such that there are no two class episodes that share any event.
Then we generated exhaustively a collection of streams that
satisfy each class λ, and injected noise in the resulting streams
by sampling from all possible sequence of events present in
other classes λ′ ̸= λ that do not satisfy any class episode.
For example, some streams of class A perturbated with events
from class B would be:
S1 =

(EA
1 , 1), (EA
2 , 2), (EB
1 , 3), (EB
2 , 4), (EB
4 , 5),(EA
3 , 6), (EA
4 , 7)

S2 =

(EA
2 , 1), (EA
1 , 2), (EB
1 , 3), (EB
4 , 4), (EB
3 , 5),(EA
3 , 6), (EA
4 , 7)

S3 =

(EA
1 , 1), (EA
2 , 2), (EB
2 , 3), (EB
3 , 4), (EB
4 , 5),(EA
4 , 6), (EA
3 , 7)

...
(17)
In this case, the EB
i
elements (shown in blue in Eq. 17),
injected in the middle, will contain all serial episodes that
deﬁne the episode αB—hence only the generic episodes are
unique to the classes we generated this way.
We then learnt a collection of 100 SP-Trees and 100
MICE-Trees from this data, using 500 training samples out
of a collection of 2,880 streams and testing on the rest. The
results, for different values of maximal tree depth are shown
in Fig. 5. There, we can see that SP-Trees require signiﬁcantly
mode complex models, in terms of both number and depth of
trees, to match the performance of MICE-Trees, conﬁrming
our assertion that the episode framework allow for a more
efﬁcient and compact encoding of certain classes of temporal
patterns.
B. Exp 2: Robustness to noise
The aim of this second experiment was to assess the
robustness of the MICE-Trees classiﬁcation for a large number
of classes (we used 100 classes) denoted by temporal patterns
corrupted by increasing amounts of noise.
1) Episode generation: In order to generate the dataset
we ﬁrst deﬁned a vocabulary E of 26 symbols. Then for
each of the classes we generated a signature episode by the
following steps: ﬁrst, we generated a single sequence of events
(E1, . . . , EN) ∈ EN, where N = 18 is the number of events
in the sequence; second, in order to transform this purely serial
episode in a generic one, we permuted randomly a number of
contiguous pairs of symbols. Hence if the original sequence
created was
(E1) → (E2) → (E3) → (E4) → (E5)
then after permutation of the leftmost (E1, E2) and rightmost
(E4, E5) adjacent pairs of events, we obtain the alternative
sequence
(E2) → (E1) → (E1) → (E4) → (E3)
and therefore this class is best described by the episode
(E1, E2) → (E3) → (E4, E5).
We use the same procedure to randomly generate unique
episodes for each of the 100 classes.
2) Episode corruption with temporal noise: All the patterns
generated at this point are perfect occurences of the class’
episode: there are no spurious events in the data stream. In
order to be able to control the proportion of spurious events in
the training and testing data streams, we propose modelled the
pattern generation as a Markov process. The process starts at
the beginning of the pattern and from there has a chance α to
move onto the next event in the noise-free pattern. Conversely,
the process has a 1 − α chance to generate a noisy event Eϵ.

PATTERNS 2016 : The Eighth International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2016. ISBN: 978-1-61208-465-7
8
ε
1-α
E_2
α
ε
1-α
E_3
α
ε
1-α
E_4
α
E_1
1-α
α
1-α
α
1-α
α
Figure 6. Illustration of the noise injection process in a noise-free sequence
(E1, E2, E3, E4). The generation is processed as a Markov process where
the chance to insert spurious events ε (randomly generated) is dependent on
a purity parameter α.
Hence, a purity parameter of α = 1 will generate a noise-free
sequence, and a purity parameter of α = 0 would generate an
inﬁnite sequence of randomised events. Moreover, the length
of the generated pattern increases with diminishing values of
α. This Markov process is illustrated in Fig. 6 for a simple
noise-free sequence.
Moreover, and in order to make the injected noise more
structured, spurious event Eϵ
k are not generated using a uniform
distribution, but using a noise generator that keeps a memory
of the previous noisy event generated, and produces the next
one using an a priori, randomly generated, Markov transi-
tion matrix encoding inhomogeneous transition probabilities
p(Eϵ
k|Eϵ
k−1)—this noise generation approach ensures that the
injected noise will be more likely to feature structure and
repeating patterns.
The usage of the α parameter and Markov chain allows the
pattern to be heavily corrupted, Figure 7 shows the corruption
of a pattern with noise for decreasing levels of α.
Figure 7. A example of the pattern corruption for increasing α
Note that the pattern length increases quickly with noise;
for values of α lower than 0.5 the proportion of noise elements
is greater that the original pattern.
3) Results: Figure 8 shows the performance of the MICE-
Trees classiﬁcation for 100 classes and values of α varying
between 20% and 100%. The full red line shows the classi-
ﬁcation error and the dashed blue line shows the Signal to
Noise Ratio (SNR) for this value of α. This graph shows that
the MICE-Trees yield excellent classiﬁcation results, providing
near-perfect classiﬁcation for values of α above 75%. For
values of α below 50%, the classiﬁcation performance drops
sharply, illustrating the fact that diminishing SNR values make
the learning extremely challenging.
Overall, this shows that the MICE-Trees can provide ex-
cellent classiﬁcation performance event in presence of large
amount of spurious events and can handle a large number of
classes.
VI.
CONCLUSIONS
In this work, we presented a theoretic deﬁnition of a
tree structure called MultI-Class Episode Trees (MICE-trees)
allowing for multiple classes to share sub-episodes whilst
providing the ability to classify datastreams. We then proposed
an efﬁcient algorithm for learning Boosted collections of such
multi-class episode trees. The performance of the proposed
model was then evaluated using two sets of experiments. The
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
Error
SNR
α
Robustness to stream noise
Error
SNR
Figure 8. This ﬁgure illustrates the robustness of the MICE-Trees to noise
injected in the data streams.
ﬁrst demonstrated that MICE-Trees allow for a better and more
compact representation of certain type of temporal sequences
when compared to sequential patterns. We also show that the
resulting classiﬁers can cope with a large amount of signal
noise while still providing good classiﬁcation accuracy.
REFERENCES
[1]
H. Mannila, H. Toivonen, and A. Verkamo, “Discovering frequent
episodes in sequences,” in Proc. of Int. Conf. on Knowledge Discovery
and Data Mining (KDD’95), 1995, pp. 210–215.
[2]
R. Agrawal and R. Srikant, “Mining sequential patterns: Generalizations
and performance improvements,” in Proc. of 5th International Confer-
ence on Extending Database Technology, 1996.
[3]
M. Zaki, “Spade:an efﬁcient algorithm for mining frequent sequences,”
Machine Learning, 2001.
[4]
J. Han, J. Pei, and Y. Yin, “Mining frequent pattern without candidate
generation,” in Proc. of International Conference on Management of
Data (SIGMOD), 2000.
[5]
J. Han, J. Pei, B. Mortazavi-Asl, and H. Zhu, “Mining access patterns
efﬁciently from web logs,” in Proc of Paciﬁc-Asia Conference on
Knowledge Discovery and Data Mining (PAKDD), 2000.
[6]
H. Mannila, H. Toivonen, and A. Verkamo, “Discovery of frequent
episodes in event sequences,” Data Mining and Knowledge Discovery,
vol. 1, 1997, pp. 259–289.
[7]
N. Tatti and B. Cule, “Mining closed strict episodes,” in Proc. of the
Int. Conf. on Data Mining (ICDM’2010), 2010.
[8]
——, “Mining closed strict episodes,” Data Mining and Knowledge
Discovery, vol. 25, 2012, pp. 34–66.
[9]
A. Achar, S. Laxman, R. Viswanathan, and P. Sastry, “Discovering
injective episodes with general partial orders,” Data Mining and Knowl-
edge Discovery, vol. 25, 2012, pp. 67–108.
[10]
S. Nowozin, “Discriminative subsequence mining for action classiﬁca-
tion,” in IEEE Int. Conf. in Computer Vision (ICCV’2007), 2007.
[11]
E. Ong, H. Cooper, N. Pugeault, and R. Bowden, “Sign language
recognition using sequential pattern trees,” in Proc. of the IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR’2012), 2012.
[12]
E. Ong, O. Koller, N. Pugeault, and R. Bowden, “Sign spotting using
hierarchical sequential patterns with temporal intervals,” in Proc. of the
IEEE Conf. on Computer Vision and Pattern Recognition (CVPR’2014),
2014.
[13]
L. Brieman, J. Friedman, R. Olshen, and C. Stone, Classiﬁcation and
regression trees.
Wadsworth & Brooks/Cole Advanced Books &
Software, 1984.
[14]
J. Zhu, H. Zou, S. Rosset, and T. Hastie, “Multi-class adaboost,”
Statistics and Its Interface, vol. 2, 2009, pp. 349–360.

PATTERNS 2016 : The Eighth International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2016. ISBN: 978-1-61208-465-7
9

PATTERNS 2016 : The Eighth International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2016. ISBN: 978-1-61208-465-7
10

