Behavior Risk: the Indeﬁnite Aspect at the Stuxnet
Attack?
Wolfgang Boehmer
Technische Universität Darmstadt
Email: wboehmer@cdc.informatik.tu-darmstadt.de
Abstract—In 2009, the Stuxnet virus was ﬁrst observed in
the wild and was considered as a novelty among the viruses.
The Stuxnet virus is classiﬁed as a game changer and so
we denote it causa Stuxnet. For the critical infrastructures, it
was inconceivable, that a speciﬁc virus has been developed for
industrial systems. Besides this novelty, the infection path was
diﬀerent from the typical patterns of attack and infection in
the ﬁeld of oﬃce communication. In this article, we focus only
on the infection path of Stuxnet. We use the Game Theory to
analyze the infection path. We found that the infection path is
one game in a complex multi-layer game. As a result, based
on a Nash equilibrium, a cooperative solution is proposed to
arm the existing IT security concepts against such infections.
Nevertheless, the existing IT security concepts are not useless,
but the behavioral risk has to be taken into account.
Index Terms—Event risks; behavioral risks; trust/investor game;
IT security concept; industrial control system
I. Introduction
In the past, the focus was placed on attacks and counter-
measures on purely IT systems in the business community
or, better, in the ﬁeld of oﬃce communication. However, it
had also previously been attacks on industrial facilities, such
as D. Denning analyzed in the article [1]. In their article,
a comprehensive analysis and categorization of attacks on
industrial facilities has been done. The Stuxnet virus was
called a game changer in their article,
In 2009, as N. Falliere et. al. from Symantec wrote [2], the
Stuxnet virus was ﬁrst observed in the wild and was considered
as a novelty among the viruses. For the critical infrastructures,
it was hitherto unthinkable that a speciﬁc virus has been
developed for industrial control systems. Besides this novelty,
the route of infection was diﬀerent from the typical patterns
of attack and infection in the ﬁeld of oﬃce communication.
Typically, an attack on IT systems is analyzed and described
by so-called attack trees. Attack trees are a good way to detect
possible attacks or mimic attack routes in advance.
In 1994, Edward Amoroso’s book Fundamentals of Com-
puter Security Technology described threat trees, a tree struc-
ture very similar to attack trees. By the late 1990s, papers were
beginning to appear describing the attack tree analysis process
in some detail. For instance, in the 1998 paper Toward a Secure
System Engineering Methodology [3] the authors describe a
mature, attack tree-based approach to analyzing risk. One of
the ﬁrst who dealt systematically with this type of technical
attacks was Bruce Schneier. In 1999, he published the idea of
attack trees, which were directed towards technical systems
[4]. The idea was taken further and improved by the company
Amenaza Tech. Ltd.
Game theory considered – very roughly speaking – con-
ﬂict situations between one or more Individuums. Fiona
Carmichael [5] wrote in her book (2005) on page 3: the idea
of Game Theory is to analyze situations where two or more
Individuums (or institutions) the outcome from action by one
of them depends not only on the particular action taken by
the individuums but also on the action taken by the other (or
others). In these circumstances the plans or strategies of the
individual concerned will be dependent on expectations about
what the other is doing. Thus, Individuums in these kinds of
situations are not making decisions in isolation, instead their
decisions making is interdependently related.
In essence, game theory is primarily not used for the
analysis of attacks [5] page 4. However, a certain relationship
exists between an attack tree and game theory in a particular
form of play (zero-sum game) of two players. In the article by
Kordy et al. [6] it was shown in a comparison that an attack-
defense tree and a strategic zero-sum game of two players to
their binary information are equivalent in the extensive form.
The extensive form denotes a game tree. In this context, a
strategic game is a scenario or situation where, for two or
more Individuums, their choice or behavior has an impact on
the other (or others). A Player is participant in a strategic
game and a strategy is a player’s plan of action for the game.
If only one player exist in a game, then this game is called a
game against nature, which is generally called decision theory
(rather than game theory). We will come back to this issue
later on.
Another type of attack has been perfected, e.g., by Kevin
D. Mitnick [7]. This type is classiﬁed as a social engineering
attack and has been discussed multiple times in the literature.
From attack-trees, insights are gained and then counter-
measures are designed to be armed for future attacks. However,
the mere development of measures is not very helpful for
an organization. These measures must be incorporated into
a security concept and coordinated with other measures.
Security concepts have always been established for safe-
guarding companies and industrial plants. It can be shown
that a good security concept cannot be reduced to a simple
list of measures. However, the measures listed in the security
concept must always result from a procedure or methodology.
In the literature, there are numerous articles on the develop-
ment of security concepts. But the objectives in the security
117
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-298-1
SECURWARE 2013 : The Seventh International Conference on Emerging Security Information, Systems and Technologies

concepts discussed in the literature are often very diﬀerent.
This paper addresses three protection goals, pursued from
diﬀerent perspectives. However, security concepts based on
the standards (e.g. ISO 27001) address three main protection
targets (availability, conﬁdentiality, integrity).
The relationship between security objective, systems and the
security concept can be produced with the following deﬁnition:
Def. 1: A security concept includes measures M to ensure
the security objectives of conﬁdentiality, availability and in-
tegrity of a system (ψ) aligned to a predeﬁned level.
The three protection goals (conﬁdentiality, availability and
integrity) behave as random variables in a probability space
and can counteract the risk of protection violation or deviation
of the predeﬁned level through appropriate security concepts.
The predeﬁned level is directly correlated with the risk appetite
of a company (see Figure 1). The reader should note that the
risk appetite must be deﬁned for each of the protection goals.
The risk appetite levels are set by the management for diﬀerent
activities or parts of the organization.
However, the diﬀerent types of risks that may lead to a
protection violation must be analyzed separately, because risks
can be divided into state risks, behavioral risks, and hybrid
risks (see Figure 2). Furthermore, not only the types of risks,
but also the underlying systems (ψ ∈ Ψ) are to be diﬀerentiated
in a security concept.
In general, for the term risk in this paper, we follow the def-
inition from the Circular 15/2009: Minimum Requirements for
Risk Management (MaRisk) issued by the Federal Financial
Supervisory Authority (BaFin).
Def. 2: Risk is understood as the possibility of not reaching
an explicitly formulated or implicitly deﬁned objective. All
risks identiﬁed by the management present a lasting negative
impact on the economic, ﬁnancial position or results of the
company may have to be considered as much as possible.
The research contribution results from the analysis of the
infection path of the Stuxnet virus [8] and combats it with
methods of Game Theory. The result evident from the analysis
is that only cooperative behavior between software manufac-
tures for SCADA systems (e.g. Siemens, ABB, AREVA) and
the software users (operator of the power plant) is suﬃcient for
a Nash equilibrium. For a Nash equilibrium it is characteristic
that actors cannot obtain a better position, if they deviate
from their strategy. The cooperation (Nash equilibrium) will
cause the software manufacturer for the SCADA systems,
as a ﬁrst step, to generate a signature in advance using the
software and provide the signature to the power plant operator
in advance, before any service technician arrives at the power
plant. The signature makes it possible to uncover an evolution
of the software (virus infection) in the IT equipment of service
technicians as a second step with only a little eﬀort. This
constructive solution to this type of behavioral risk is already
in the implementation stage at one power supplier in Germany.
It is also clear from the game analysis that the (current)
practice of disinfecting the infected systems retroactively only
represents the second best solution, because it is not ruled
out that modiﬁcations of the Stuxnet virus could infect the
sensitive control systems of industrial plants in the future.
Moreover, conventional virus scanners in SCADA (supervisory
control and data acquisition) is a type of industrial control sys-
tem (ICS) Systems are generally hardly used. This preventive
solution then enables, if the software manufacturer for SCADA
systems cooperates, future modiﬁcations of the Stuxnet virus
or a variant of a Stuxnet virus to be uncovered eﬀectively.
The rest of the article is divided into four sections. In the
next section, the underlying model equations are explained. In
the third section, case studies are discussed for the diﬀerent
types of risks; for example, hybrid risk analysis (see Fig. 2,
no. (2)). In better security concepts, this approach can be
found in the development of security measures. According
to, e.g., the ISO 27001 and ISO 27005, a scenario analysis
is required to create a security concept (statement of appli-
cability). Subsequently, behavioral risks are discussed on the
example of the Stuxnet virus. Such risks are based only on
the misbehavior of Individuums. These are marked with the
no. 3 in Fig. 2. With the Game Theory, the Causa Stuxnet is
analyzed. Here, the route of infection is analyzed as a partial
game in a complex multi-layered game. A Nash equilibrium is
achieved, the knowledge of which can eliminate general routes
of infection preemptively. However, measures derived from
the analysis of behavioral risks have rarely been included in
the security concepts. Also, methods of Game Theory, which
consider behavioral risks, have not previously been included
in any standard.
In the fourth section, we discuss the related work and in
the last section, there is a brief summary and an outlook on
further research. This article is an extended version of [9].
II. The model
In essence, we will deal in this article with hybrid risks
described in Figure 2, number (2) and the behavioral risks,
number (3) for analyzing the Stuxnet virus.
An IT/Inf.-Security concept reﬂects the complementary re-
lationship between security and risk for a system ψ ∈ Ψ.
This complementary relationship is that the lower the security
(Sec), the higher the risk (R) of a violation for a system ψ
of the three control objectives (cf. Deﬁnition 1); therefore risk
and security are negatively correlated.
Figure 1 illustrates this relationship qualitatively for the
protection goal availability.
To illustrate this negative correlation, risk and security
simpliﬁed are normalized by the interval [0, 1], with
Sec = 1 − R.
(1)
The risk (R) in the sense of operational risk is obtained as the
probability (Pr) of an event (E) on the impact on a system,
e.g., on an open system (ψ1), as a value chain with a negative
outcome (Loss, L) in monetary units (euro), in R+ [10]. This
relationship can be expressed as follows
R = PrE × L [R+].
(2)
118
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-298-1
SECURWARE 2013 : The Seventh International Conference on Emerging Security Information, Systems and Technologies

0%
100%
100%
0%
e.g accepted level of security
for the protection goal 
availability 
e.g. accepted level of risk
for the protection goal 
availability 
state of  risk is increasing for a system
state of security is increasing for  a system 
risk appetite
Figure 1: Security versus risk
At ﬁrst glance, these two deﬁnitions (Eq. 1, Eq. 2) do not
seem to attain anything. However, if the risk (R) is regarded
as a random variable in a probability space, then this is the
missing link in the chain of reasoning. For a random variable
let X : Ω → R be a measurable function in a probability space
deﬁned by the triple Ω, A(Ω), Pr, where A a σ-algebra, is a
certain subset in the probability space. By inserting (2) in (1)
we produce (3)
Sec = 1 −

PrE × L

.
(3)
Thus, security can be measured indirectly by measuring the
risk.
A quantiﬁcation of a random variable (X) is performed
formally by assigning a value (x) for a range of values (W)
using a certain event (E). For the random variable (X) the
image of a discrete probability space then applies to the
discrete result set Ω = {ω1, ω2, ..., } such that X : Ω → R.
For discrete random variables for the discrete value range that
is interpreted in the context of operational risk as a monetary
loss (L) (cf. Def. 2)
LX = WX := X(Ω) = {x ∈ R | ∃ ω ∈ Ω mit X(ω) = x}.
(4)
In the ﬁeld of operational risks, the probability (Pr) with the
random variable (X) which may accept certain values (WX) and
losses (LX) is of interest. For any event (E) with 1 ≤ i ≤ n
and xi ∈ N:
Ei := {ω ∈ Ω | X(Ω) = xi} = Pr[{ω ∈ Ω | X(ω) = xi}].
(5)
Since, in this context, only numerical random variables are
considered, each random variable can be assigned to two real
functions. We assign any real number (x) the probability that
the random variable takes that value or a maximum of such a
great value. Then the function fX with
fX : R → [0, 1], x 7→ Pr[X = x]
(6)
is called a discrete (exogenous) density (function) of X.
Furthermore, a distribution function (FX) is deﬁned with
FX : R → [0, 1], x 7→ Pr[X ≤ x]
X
x ∈ Lx : x′≤ x
Pr[X = x′].
(7)
The value (WX) can have both positive and negative values,
depending on which is discussed in the context, the density or
the distribution of values.
Now if the conﬁdentiality (Conf) and integrity (Int) are seen
as discrete sets of random variables in a probability space, it
is possible to describe these two security objectives (8), (9) as
the sets of a given indicator function.
Due to the binary property of the two subsets (Int, Conf),
with Int ⊆ X and Conf ⊆ X, for every x on [0, 1], which for
x ∈ X is 1 when x ∈ Int or x ∈ Conf, otherwise 0. It is
X → [0, 1], x 7→

1, if x ∈ Int
0, otherwise.
(8)
Also (8) can be used for the random variable Conf, if we used
Conf instead Int in (8), then we can derive (9)
X → [0, 1], x 7→

1, if x ∈ Conf
0, otherwise.
(9)
Thus, the binary properties of the two discrete random vari-
ables are formally described. In this paper we write 1Int to
the discrete indicator function, integrity, and 1Conf to use the
discrete indicator function conﬁdentiality.
It is diﬀerent with the availability (Av), which can be
formally described as a complete partial order, CPO. With
a CPO we can easily ﬁnd intermediate values in the interval
[0,1] in R+ which are the subject of a binary relation. A binary
relation over the set (Av) availability of all elements is a partial
order, if a, b ∈ Av and a ≤ b holds. We use the following
notation in this paper
(Av ≤) 7→ [a ≤ b] or short and sweet (Av ≤).
(10)
Finally, a security concept (SecCon (11)) is the illustration by
the measures (NMa) and with (8), (9) and (10) and the map
to the method M (cf. Def. 1)
SecCon (|NMa|) := M

(Av ≤), 1Int, 1Conf

7→ Ψ
(11)
for a system ψ ∈ Ψ.
However, the security concepts are not only the power of the
measures (|NMa|) to reduce the risk of a possible injury of the
three security objectives for a system, but it is necessary that
the measures NMa have been developed using a methodology.
This methodology is the function M in (11). The function
M must be able to map the diﬀerent risk types according
to the underlying (open, closed, isolated) systems. Thus, the
following deﬁnition is formulated for the measures.
Def. 3: The identiﬁed measures (NMa), included in a secu-
rity concept, based on the methodology (M).
The idea of the open (ψ1), closed (ψ2) and isolated systems
(ψ3) has been borrowed from thermodynamics, but can be
119
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-298-1
SECURWARE 2013 : The Seventh International Conference on Emerging Security Information, Systems and Technologies

easily transferred to computer science and business, too [11],
[12].
A broad representation of diﬀerent types of risks, relating to
systems all the way up to Individuums in Figure 2, has been
marked by the no. (1) - (3), illustrated by T. Alpcan [13] and
is the brainchild of N. Bambos.
System
Individuum
Nature
Malice 
Negligence
Policy breach
Power
failure
Conﬁguration errors
Natural
disaster
Design
failure
SPAM
Phishing
disgruntled
employee
DOS
Botnets
Malware,
Virus
Worms
Trjoaner
Portscanning
network 
attack
1
3
2
Figure 2: Diﬀerent types of states and risks, according to N.
Bambos [13]
Mark no. (1) denotes the event risk, for example, and this
risk can be related to closed and / or isolated systems. No. (3),
the purely behavioral risks, relates primarily to Individuums,
as does the hybrid risk indicated by the no. (2). These are
often considered using a scenario analysis. Hybrid risks are
common in open systems.
After the diﬀerent systems and risks are illustrated in Figure
2, the question becomes how to deal with the risks. It is not
mandatory that every identiﬁed risk must be eliminated.
How to deal with risks is important for an organization.
Risks generally could be reduced, avoided, transferred, ac-
cepted or eliminated (see Figure 3).
Economic aspects alone, and not technical aspects, are
crucial in dealing with the identiﬁed risks. The diﬀerent
treatment methods are described below with the numbers 1 to
4. Several decisions have to be made regarding which of the
risks can be avoided, reduced, transferred, or even accepted,
see Figure 3.
The following are the decisions to be taken.
1) R1 = Pn
i=1 Ravoid, number of risks that can be avoided
2) R2 = Pn
i=1 = Rmitigate, number of risks that can be
mitigated
3) R3
= Pn
i=1
= Rshift, number of risks that can be
transferred
4) R4 = Pn
i=1 = Rtaking, number of risks that can be accepted
Illustrated in Figure 3 are the upcoming decisions and these
decisions are designated as the security posture. The ratio
of overall risk and the various possibilities for reducing
operational risks is shown.
The orientation of the possibility of a reduction of the risk
depends on the cost. It is cheaper for the company to insure
certain risks than to invest in adequate measures.
Companies may decide the quantiﬁcation of risks according
to their budget and their business objectives and their risk
behavior (level of risk appetite). This turns risk management
into cost management.
identiﬁed   risks
accepted risks
unknown        risks
1. avoid
2. mitigate
3. shift
4. take a risk
- personal
- engineering
- housekeeping
Assurance
overall risk
reduction of risk
Figure 3: Aspects of risk treatment
It should be noted that in spite of a risk analysis un-
known risks still exist. The goal of any risk analysis must,
therefore, be to try to reduce the number of unknown risks
R5 = Pn
i=1 = Runknown as much as possible. The unknown risks
are not negligible, if we follow the groundbreaking book The
Black Swan from N. Taleb [14]. He argues that the Gaussian
bell curve can show probabilities, but the case of a devastating
security event (Black Swan) will be an outlier to the bell curve,
see Figure 4, and produce a worst case scenario.
III. Case study
Within this section, the next subsection (A) discusses hybrid
risks from the Game Theory perspective. We argue that it is a
game against nature. The second (B) and third (C) subsection
discuss the Causa Stuxnet and analyze it using the trust game
of the Game Theory. The solution achieved through a Nash
equilibrium of the game analysis of the trust game used is
presented in the fourth subsection (D).
A. Analyzing hybrid risks: a game against nature
The risks denoted by no. (2) in Figure 2 arise from both state
risk and behavior risks. For this type of risk analysis, statistical
methods and behavioral eﬀects are both considered. This
hybrid risk could be analyzed by the risk scenario technique
going back to the three-point estimation method [15]. This
three-point estimation method was used for the analysis of
hybrid risks in the area of power plants and speciﬁcally in the
ﬁeld of SCADA systems. It was studied experimentally at 29
power plants, as one can read in [16]. Based on this analysis,
a security concept for the SCADA system has been created.
Generally, a scenario is a possible event Ei, expressed
formally in (5). It is the attribution of a certain value of a
random variable (X(ω) = xi). In this context, an event Ei is
understood as a risk event (Rszς). Using the three-point (risk)
estimate method, diﬀerent loss probabilities (best case (BC),
most likely case (mc), worst case (wc), (see Figure 4) of a risk
event are identiﬁed. It relates the risk scenarios to the above
protection objectives (Av ≤), 1Int, 1Conf. The risk of incident is
related to an asset. The assumption is, an asset incorporates
both a resource and a role that interact in a business process
120
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-298-1
SECURWARE 2013 : The Seventh International Conference on Emerging Security Information, Systems and Technologies

[17]. (12) deﬁnes a risk event (RS z) with ς = {bc, mc, wc} as
a possible result of variations of the risk event.
X(ω) = Rszς B

ifς = bc | (xbc = xmc) ∧ Pr[X(ω) = xbc] → best case
ifς = mc | (xbc > xmc) ∧ Pr[X(ω) = xmc] → most likley case
ifς = wc | (xwc ≫ xbc ∧ xmc) ∧ Pr[X(ω) = xmc] → worst case
(12)
The possible result types (Rszς) of a risk event were estimated
by experts in workshops. An illustration of the stochastic
process of (12) is presented in Fig. 4. Furthermore, a distribu-
tion (Gaussian curve) using three points of the estimates was
created by a General Pareto Distribution [16]. This line shows
the distribution of possible losses to absorb. In this case, the
certain event Pr = 1 is no longer a stochastic event. In terms
of operational risks, only the grey shaded area of interest is
normally referred to as a downside risk with X(ω) = {1, −∞}.
Assuming a time interval (t1, t3) in the probability space (Pr),
the expected loss (VaR) can be determined for a conﬁdence
interval (α) using (13), which provides a lower bound
VaRα B min{x | (Pr[X ≤ x] > α)}.
(13)
The VaR is not a coherent risk measure, as demonstrated by
t1
t3
X(ω)
today
future
downsize 
0
1
0
Pr (t1 < T< t2)
t1
t2
(most likley case,mc )
(best case, bc)
(worst case, wc)
stochastic process
X(ω) ϵ T
Pr
Figure 4: Risk corridor for the time interval (T)
Artzner [18], but, for this risk estimation using the VaR, the
error made in this case is very small, because power plants use
the standard BS25999 for the very rare risk with a catastrophic
outcome [16].
After analyzing the risks created by the set NR, the elements
NR = {rRszς
1
, ..., rRszς
Rszς} have the cardinality |NR|. These can be
addressed through appropriate measures. There are diﬀerent
measures possible. On the one hand, actions may be identiﬁed,
when only one risk scenario works against one risk; on the
other hand, other measures can be identiﬁed that counteract
more than one risk. In general, measures are deﬁned with
the set NMa and the elements NMa = {mMa
1 , ..., mMa
Ma}. The
cardinality is given with |NMa|. These measures, based on
the three security objectives (8), (9, (10), create the security
concept according to (11).
Through risk analysis, using the risk scenario technology,
which refers to assets in a process (business process), both
a pure state risk and a behavioral risk are included, because
in the scenarios unconscious and conscious actions (misuse)
of an employee and its impact on the business process are
considered.
From the perspective of Game Theory, this is still a game
against nature. This is a decision problem D in strategic
form under risk. They are making decisions for actions
involving the diﬀerent probabilities (Pr) in the probability
space given for the environment states z ∈ Z. However,
the classical decision rules (MaxiMin rule, MaxiMax rule,
Laplace’s rule, etc.) are not used as strategies in this paper.
The decision maker who is responsible for developing a
security concept (see (11)) will choose a strategy s from the
set of all strategies S , to select those measures to (avoid,
decrease, transfer, eliminate, accept) a risk event (see (12)).
Five diﬀerent strategies, ˜s1, ..., ˜s5, can be used
˜s1 = Avoiding the outcome of the risk with a measure.
˜s2 = Decreasing the outcome of the risk with a measure.
˜s3 = Transferring the risk to an insurance company.
˜s4 = Eliminating the outcome of a risk with a measure.
˜s5 = Accept the risk.
Not all of the strategies listed above for ˜s are applied, because
each measure requires certain costs (U). In classical Game
Theory, u ∈ U is often understood as the pay-oﬀ (function
or utility function). We described U with the amount of the
costs of all measures and u is a cost function, the decision
problem D is a strategic decision
D =
 ˜S , Z, ς, U, u

(14)
under risk. (ς) represents a probability distribution on Z, the
environmental conditions. The creation of the decision space
(14) can be represented as follows
˜S × Z 7→ U(ς).
(15)
A decision matrix can be derived from the decision space,
as illustrated in Table I. The decision maker (security oﬃcer)
has to make a decision based on the decision matrix of Table
I that included the strategies, the environmental conditions
and the measures or the costs of the activities related to the
risk scenario (ς = {bc, mc, wc}), which should be provided in
the security concept (see (11)). The decision matrix is shown
above in Table I. Depending on the decision process by the
security oﬃcer (see (14) and Table I) this will meet the security
concept regarding the security objectives of conﬁdentiality (9),
availability (10) and integrity (8) of the identiﬁed risks with
appropriate measures. With the consideration of the hybrid
risks posed by the scenario technology (see (12)), consolidated
ﬁndings are gained to complete the security concept. However,
analysis of the hybrid risks with the scenario technology is not
ideal for analyzing the Stuxnet virus.
121
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-298-1
SECURWARE 2013 : The Seventh International Conference on Emerging Security Information, Systems and Technologies

Table I: CHANCE MOVES AGAINST NATURE
nature / environment
z1
z2
z3
˜s1
u(˜s1, bc)
u(˜s1, mc)
u(˜s1, wc)
˜s2
u(˜s1, bc)
u(˜s1, mc)
u(˜s1, wc)
security
˜s3
u(˜s1, bc)
u(˜s1, mc)
u(˜s1, wc)
oﬃcer
˜s4
u(˜s1, bc)
u(˜s1, mc)
u(˜s1, wc)
˜s5
u(˜s1, bc)
u(˜s1, mc)
u(˜s1, wc)
There is no analysis of the behavior (actions) of employees
or other service providers. This has led to the conclusion that
the existing security concepts in the power plants have a gap,
and that the infection of an authorized service technician –
albeit unwittingly – is possible.
In the next subsection, we analyze the infection of the
Stuxnet virus to compromise the protection target (1Int) with
the trust game of the Game Theory.
B. Game analysis before the Stuxnet virus arose
Pure behavioral risks (cf. no. (3) in Fig. 2), in contrast to
pure state risks (cf. no. (1) in Figure 2), could not be analyzed
with statistical methods.
Therefore, we consider the Causa Stuxnet and the behavior
between the service technician and the staﬀ (security oﬃcer)
causing the infection using the trust game (note:
The trust
game is a modiﬁed dictator game). from Game Theory. As
one of the ﬁrst, [19] deals with the trust game in reference
to a social environment. Typically, for the trust game, there
is a diﬀerent trust relationship (imbalance) between the two
players.
These behavioral risks are the types of decisions (strategies)
of the player (service technician / security oﬃcer) that caused
the infection.
Stuxnet
The infection of Stuxnet virus, in the area of crit-
ical infrastructure (SCADA) systems, has not been
an attack. The virus was transferred from a service
technician equipped with the necessary permissions un-
consciously, using an infected USB stick. The virus has
arrived without the knowledge of the service technician
on to his USB stick.
Subsequently we analyze this critical incident with the
Game Theory to derive a solution from the chance moves
of the game. This solution leads into a cooperation of the
software manufacturer with the power plant operator of the
SCADA systems.
In the analysis, we use a slightly modiﬁed version of the
trust game, because it cannot be ruled out that tomorrow
another service technician from another company with a
similar virus attends to the power plant.
Pure behavioral risks, which are designed to trust, could
be analyzed with the trust game [20]. We analyze the chance
moves of both players. Each player reacts to the behavior of
the other player. One of the basic ideas of Game Theory is to
study, analyze and evaluate the reciprocal response pattern of
the players. Reciprocal reaction patterns, so-called pay-oﬀs,
Table II: CHANCE MOVES BEFORE THE STUXNET
σ2
infect
not infect
σ1
trust
1, 0
3,3
don’t trust
1, 0
2, 2
are determined by distribution rules and play a signiﬁcant
role and in turn, depend on the incentives. It depends on the
distribution rules of legal, contractual, historical or political
power relations. Thus, a major diﬀerence is the probability
models, as these know no incentive mechanisms.
Game analysis of virus Stuxnet pursues a causal chain of
thought. First the chain of thought which was taken before the
Stuxnet virus (cf. Table II and Fig. 5) is followed and another
chain of thought follows after the onset of Stuxnet virus
(cf. Fig. 6 and Table IV). The concerned chance moves are
performed as a one-shot game. Thought chains are typically
illustrated in the form of branching trees, to represent the
individual moves. Another name for the game tree is the
extensive form as is noted in [5].
Formally, a strategy game Γ consists of a triple. With Σ,
the set of players σ is deﬁned, it is σ ∈ Σ. With S the set of
strategies is described and we have s ∈ S . This means that a
game can be characterized as follows
Γ = {Σ, S, U, u}.
(16)
U has the same intention as in (14). In the analysis of the
Stuxnet virus, are two players (σ1, σ2) in the space of action
A = Σ × S . The action space (cf. Fig. 5) for player σ1 ist Aσ1
= {t, nt} and Aσ2 = {i, ni} applies to player σ2. In this analysis,
pure strategies are postulated; therefore, a single pure strategy
is expressed in s. Strategies include decision rules that the
player implement to some beneﬁt (u) to obtain a pay-oﬀ. In
general, the trust game can be expressed as
Σ × S 7→ U(u).
(17)
Compared with (17), in (15) the players S now are in the place
of the environmental states Z.
Typically, games in the form of a bi-matrix, called the
simultaneous logical reasoning circular, are presented in a
sequential chain of thought the game tree. The Bi-Matrix in
the trust game between the service technician (player σ2) and
the security oﬃcer (player σ1) has been formulated in Table II.
In this situation, this game represents the situation before the
virus Stuxnet arrived. Before Causa Stuxnet, there was no dis-
trust due to player σ2 (service technician) and, consequently,
the chance moves (1, 3, 6) in Fig. 5 are typical chance moves.
To date, no incidents justiﬁed distrust of players σ1 (security
oﬃcer) toward players σ2. Also, it was inconceivable to date,
that a special virus [8] would be written which is used in
an area with property software for the small Program Logic
Controller (PLC), cf. with the Step 7 software. However, for
a suspicious player σ1 (security oﬃcer), the move (1, 2, 4)
of Fig. 5 is also conceivable, but impossible because thus far
virus infections were not encountered and therefore without
122
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-298-1
SECURWARE 2013 : The Seventh International Conference on Emerging Security Information, Systems and Technologies

consequence. It also ruled out the usual (normal) route of
infection in the power plant, due to the systematic separation
of networks and hermetic sealing of the internal systems to
the Internet and intranet. Thus, the combination (do not trust
/ not infected) is a Nash equilibrium. In a Nash equilibrium,
none of the two player could obtain a better position through
a change in their attitude.
In this respect, the cost function u (not much eﬀort, because
there are no security policies to follow) is greatest for the
two players when combining (trust / not infected) in Table II.
It leads also to a Nash equilibrium, since neither player can
achieve a better position by changing moves.
t
nt
ni
i
t = trust !2
nt = not trusting !2
i  = infect the system by !2
ni = not infect the system by !2
!1 
1
2
3
!2 
ni
i
!2 
4
5
6
7
Figure 5: Change moves in a game tree before the Stuxnet
virus arose
After the Stuxnet virus arose, this event changed the trust
relationship drastically between player σ1 (security oﬃcer)
and player σ2 (service technician). This change in position
of trust is analyzed in the next subsection.
C. Game analysis after the Stuxnet virus arose
After the Stuxnet arose, the perspective of player σ1 (se-
curity oﬃcer) has changed considerably. He does not know
whether or not the service technician σ2 brings an infected
USB stick. The result is the typical trust game (note: This
uncertainty could also be analyzed with a mixed strategy, a
probability distribution over the pure strategy. However, in this
investigation we use only pure strategies). situation because an
imbalance of trust has occurred.
After the Stuxnet virus, the security oﬃcer (σ1) could
continue to trust the service technician (σ2) or install com-
prehensive security policies. The behavior of σ1 trust (t) is
illustrated in the extensive form (see Fig. 6) in the right branch
(1, 3). The player σ2 then has the opportunity to infect the
system (1, 3, 7) or not (1, 3, 6) with the result that σ1 must
check the system (c, nc) or possibly report a virus (r, n).
The left branch illustrates, on the other hand, the behavior
strategy of σ1 for do not trust (nt), therefore the game play
(1, 2).
Security policies always increase the restrictions and the
workload involved for everyone (σ1, σ2). Furthermore, it is
evident that, when security policies are perceived as too
restrictive by the players, they bypass (σ1, σ2) all of the
policies. For the security policy for the USB stick, this leads to
(bc, nbc). This realistic situation is illustrated by the extensive
form in Fig. 6. If the security policies are not undermined,
there is the game branch (1, 2, 5, 10). However, the branch
with restrictions, imposed by the security policy, increased the
workload.
t
nt
ni
i
t = trust
nt = not trust
r = report a virus 
nr = not report a virus  
i = infect the system
ni = not infect the system
c = check the system
nc = not checking the system
bc = bypass the USB-Stick check
nbc = not bypass the USB-Stick check
c
nc
bc
i
ni
nbc
1
2
3
4
5
6
7
8
9
10
11
12
13
14
!1
!2
!2
!1
!1
!2
!2
c
nc
ni
r
nr
15
16
Figure 6: Change moves in a game tree after the Stuxnet virus
arose
The game branch (1, 2, 4) represents the case where the
service technician (σ2) bypasses the security policy unnoticed
and the security oﬃcer (σ1), driven by their distrust, reviews
the system for viruses (1, 2, 4, 9). This distrust does, however,
again cause an increased eﬀort for σ1. Otherwise, the strategy
(1, 2, 4, 8) is followed by σ1 and a signiﬁcant degree of
uncertainty remains about the state of the system. It may be
now, that the game play (1, 2, 4, 8, 16), or in the negative
case, an infection occurs (1, 2, 4, 8, 15). As a result it can be
stated that no Nash equilibrium can be achieved.
The game tree of Fig. 6 allows us to derive the bi-matrix of
the Table IV with the pay-oﬀ Table III and the key parameters
for the service technician (σ2) and security oﬃcer (σ1). In
the following statements, the key parameters for the service
technician (σ2) are listed.
GS
Beneﬁt for successful service
GB Beneﬁt to following the security policies (increasing of reputa-
tion)
KS
Investment given by checking the security controls
KR Penalty for ignoring the security policies
KU Penalty for procedure to disinfect the system
For the security oﬃcer (σ1), the following key parameters are
provided:
GE Beneﬁt given for the fact that the system was not infected by
the maintenance procedure
GN Beneﬁt given for the fact that the maintenance procedure was
done without any problems
GV Beneﬁt given for the fact that the maintenance procedure was
done successfully and in a safe manner
KV Penalty in the case of a violation of the security policy
The payoﬀs are taken from a real world assumption and reﬂect
observations in dealing with the service technician in a power
plant.
The payoﬀs of the game matrix for the security oﬃcers
and service technicians are illustrated in the Table III. If the
payoﬀs in Table III are taken into account in the bi-matrix,
123
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-298-1
SECURWARE 2013 : The Seventh International Conference on Emerging Security Information, Systems and Technologies

Table III: PAYOFF FOR BOTH PLAYERS
σ1
σ2
GS
6
GE
3
GB
6
GN
4
KS
3
GV
5
KR
8
KV
11
KU
2
Table IV: MOVES IN A GAME AFTER STUXNET VIRUS
σ2
infect
not infect
σ1
trust
GV − KV, GB − KS − KU
GE + GV, GS − KR
(-6, 1)
(8, -2)
don’t trust
GN, GS − KS − KV
GE, GS
(4, -9)
(3, 6)
the following Table IV is created. An appropriate strategy
for both players is not apparent. It is also clear that for the
current practice of using a virus scanner, no Nash equilibrium
is appropriate and that it, at best, is only a stopgap.
D. Game of cooperation to ﬁnd a solution for the Causa
Stuxnet
In the previous subsection, it appears that the use of a
virus scanner is only a stopgap measure in the sense of Game
Theory. The game situation has been changed. In this subsec-
tion we will therefore, in the analysis of the Cause Stuxnet,
initiate a modiﬁed game, which requires a collaboration of
players. Then, a Nash equilibrium could be established and
the utility function for the players is increased. Therefore, a
pure strategy was sought that minimized the amount of costs
(KS , KR, KV, KU) and correspondingly increased the value U.
The costs and beneﬁts of the diﬀerent values is still listed
in Table III. These values have not changed after using a
signature. Minimizing the cost and the strategy s is listed in
(18)
min U(s, Ks, KR, KV, KU).
(18)
As a pure strategy, in terms of a cooperation between the two
players, it means a lesser amount of work (beneﬁts) for both
players and the two companies. The eﬀort must keep both sides
balanced. This balance and the Nash equilibrium arise when
the software manufacturer creates the SCADA software with
a signature over a hash value and ensures that the signature
was generated using the original software. Then, the signature
was provided to the power plant operator. This change will
change the behavior of the players in the Table IV with the
same payments (beneﬁts) in the Table III. The behavior of the
two players with the appropriate response to the use of the
signature is given in Table V.
The diﬀerence between Table III and Table V is apparent
in the ﬁeld (don’t trust / not infect). The use of the signature
on both sides (producers and users) increases the beneﬁt and
the values (KV, KS , KR, KU) do not occur. Thus, it is possible
for the ﬁeld (12, 12) to obtain a Nash equilibrium. The game
tree in Fig. 7 displays the game. The moves (1, 2, 3, 4) show
the course.
Table V: MOVES IN A GAME AFTER STUXNET WITH
AN IMPLEMENTED SIGNATURE
σ2
infect
not infect
σ1
trust
GV − KV, GB − KS − KU
GE + GV, GS − KR
(-6, 1)
(8, -2)
don’t trust
GN, GS − KS − KV
GE + GN, GS + GB
(4, -9)
(12, 12)
nt
t = trusting !2
nt = not trusting !2
cs  = check the signature  !2
ncs  = not check the signature  !2
mt = maintain the system by !2
nmt = not-maintain the system by !2
i  = infect the system by  !2
ni  = not infect the system by  !2
!1 
1
2
cs
!2 
3
4
5
!1 
mt
nmt
6
t
7
ncs
!1 
!2 
8
9
ni
i
Figure 7: Moves in a strategic game in a game tree with a
pre-exchanged signature
Thus, the measure (use a signature) met the two above-
mentioned deﬁnitions 1 and 2, according to (11), and can be
included in a security concept.
In essence, the strategic moves of the game in Table V
and in Fig. 7 are only possible because a cooperative strategic
game between the software manufacturers of SCADA systems
and the power plant operators was recently initiated. A coop-
erative strategic game Γ consists of a tuple
Γ = {N, v}.
(19)
N = {1, 2} is understood as the software manufacture (1) and
the power plant (2). With
v ∈ V(N) := {f : 2N 7→ R | f(∅) = 0}
(20)
being the characteristic function. The coalition function maps
a value to each coalition. For example, if only one of the two
coalition partners applied the signature, the result is given by
v ({1}) = v({2}) = 0. If the signature is used as described above
by both, then v ({1}) = v({2}) = 1. Only when both partners
stick to the coalition is a beneﬁt obtained, as one can see in the
ﬁeld (don’t trust / not infect) in Table V. For the cooperative
game, the Nash equilibrium is achieved. Should it not come to
the coalition, the power plant operators may only use a virus
scanner.
IV. Related work
The behavior of attacks on IT systems were studied in
the Honeynet Project by M. Spitzer for ﬁrst time [21] and
has since received a lot of attention in the literature. The
Honeynet Project, at the time, sought a novel approach in
which the behaviors of the attacker were studied in order
to develop them into conclusions for the protection of IT
124
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-298-1
SECURWARE 2013 : The Seventh International Conference on Emerging Security Information, Systems and Technologies

systems. The behavior of the attacker was analyzed, but
not with the methods of Game Theory. In the paper by W.
Boehmer, human behavior was studied by entitling employees
to perform unauthorized actions if necessary in a company.
The method used is coupled to forensic analysis using data
mining techniques [22]. Proﬁling was performed to identify the
possible unlawful conduct by employees. The above examples
did not use a game-theory approach. In the area of networking,
T. Alpcan investigates attacks using game theory analysis.
This game theoretical approach gave deep new insight into
the defense of networks [13]. Based on the spy / inspector
game, evidence was obtained from Alpcan. However, game
theoretical methods have had hardly any or very little, but
not systematic use in the ﬁeld of IT security. In the case
of the Stuxnet virus, infection was performed by a certiﬁed
maintenance staﬀ unconsciously and thus deviates from the
usual spy / inspector game. All here above described methods
of analysis would not reﬂect the infection realistically. We
analyze the Causa Stxunet, or better the path of infection,
therefore, using the trust game, to reﬂect the realistic situation.
V. Conclusion and further investigations
In this paper, we have studied the Stuxnet virus using Game
Theory. In the multidimensional game of the Causa Stuxnet
we have analyzed a part of the whole game, especially – the
infection path with the Trust / Investor game – to compromise
the security objective (1Int) of the SCADA system. From
the game analysis, we derived that only a Nash equilibrium
can be established if a collaboration is sought between the
software manufacturer for SCADA systems and software users
in the power plant. The cooperation is made possible by the
implementation of a signature on the SCADA software from
the software manufacturer in his laboratories. This signature
can be checked very easily at the users site in the power plant,
when a service technician arrives. This solution presented by
the signature is a preventative solution and should be preferred
to the current reactive solution of the virus scanner.
The path of infection is only one part in the multi-
dimensional game, because there is an attacker in the back-
ground with the intention of damaging the power plants
(compromise the security objective availability (Av ≤), see eq.
10). Against this background, the game analysis of the path
of infection is only a part of a game in a multidimensional
game. The analysis of the entire game of Causa Stuxnet and
the embedding of this game into the whole game, will be part
of another investigation.
References
[1] D. E. Denning, “Stuxnet: What has changed?,” /Future Internet/, vol. 4
(3), pp. 672–687, published online: 16 July 2012 / last accessed 2013-
07-15/ doi:10.3390/ﬁ4030672.
[2] N. Falliere, L. O. Murchu, and E. Chien, “Symantec security response,
w32.stuxnet dossier, version 1.4.” http://www.symantec.org., 02 2011.
[3] C. Salter, O. S. Saydjari, B. Schneier, and J. Wallner, “Toward a secure
system engineering methodolgy,” in Proceedings of the 1998 workshop
on New security paradigms, NSPW ’98, (New York, NY, USA), pp. 2–
10, ACM, 1998.
[4] B.
Schneier,
“Attack
trees,
modeling
security
threats,”
http://www.schneier.com/paper-attacktrees-ddj-ft.html, 1999.
[5] F. Carmichael, Guide to Game Theory.
Pearson Education Limited,
ISBN 0 273684965, 2005.
[6] B. Kordy, S. Mauw, M. Melissen, and P. Schweitzer, “Attack-defense
trees and two-player binary zero-sum extensive form games are equiv-
alent,” pp. 245–256, 2010.
[7] K. D. Mitnick and W. L. Simon, The Art of Deception, Controlling the
Human Element of Security. Wiley, New York NY et. al., 2002.
[8] ENISA,
“Stuxnet
analysis,
http://www.enisa.europa.eu/media/press-
releases/stuxnet-analysis,” 10 / 2010.
[9] W. Boehmer, “Dynamic systems approach to analyzing event risks and
behavioral risks with game theory,” PASSAT/SocialCom 2011, Privacy,
Security, Risk and Trust (PASSAT), 2011 IEEE Third International
Conference on and 2011 IEEE Third International Conference on Social
Computing (SocialCom), Boston, MA, USA, 9-11 Oct., 2011, 2011.
[10] R. Giacometti, S. Rachev, A. Chernobai, and M. Bertocchi, “Aggregation
issues in operational risk,” Journal of Operational Risk, vol. 3, no. 3,
2008.
[11] F. Capra, The Turning Point: Science, Society, and the Rising Culture.
Bantam, 1984.
[12] F. Capra, The Web of Life: A New Scientiﬁc Understanding of Living
Systems. Anchor Books/Doubleday; 1st edition, 1996.
[13] T. Alpcan and T. Basar, Network Security - A Decision and Game-
Theoretic Approach. Cambridge University Press, 1. ed., 2011.
[14] N. Taleb, The Black Swan. Random House, 2007.
[15] G. Dukic, D. Dukic, and M. Sesar, “Simulation of Construction Project
Activities Duration by Means of Beta Pert Distribution,” in Information
Technology Interfaces, 2008. ITI 2008. 30th. International Conference
on, pp. 203 – 208, 2008.
[16] W. Boehmer, Information Security Management Systems Cybernetics.
in Strategic and Practical Approaches for Information Security Gover-
nance: Technologies and Applied Solutions, (ed.) M. Gupta, J. Walp, R.
Sharman, IGI Global publisher of the Information Science Reference,
2011.
[17] JTC 1/SC 27/WG 1, “ISO/IEC 27001:2005, Information technology
- Security techniques - Information security management systems -
Requirements.” Beuth-Verlag, Berlin, 10, 2005.
[18] P. Artzner, F. Delbaen, J.-M. Eber, and D. Heath, “Coherent measures
of risk,” Mathematical Finance, no. 9, pp. 203–228, 1999.
[19] J. Berg, J. Dickhaut, and K. McCabe, “Trust, reciprocity, and social
history,” Games and Economic Behavior, no. 10, pp. 122–142, 1995.
[20] T. Basar and G. J. Olsder, Dynamic Noncooperative Game Theory.
No. 23, Society for Industrial and Applied Mathematics, Academic
Press, New York, 2nd. ed., 1998.
[21] L. Spitzner, “Honeypots: Catching the insider threat,” in ACSAC ’03:
Proceedings of the 19th Annual Computer Security Applications Con-
ference, (Washington, DC, USA), p. 170, IEEE Computer Society, 2003.
[22] W. Boehmer, “Analyzing Human Behavior with Case Based Reasoning
by the help of Forensic Questions.” 24th IEEE International Conference
on Advanced Information Networking and Applications (AINA-2010),
03 2010.
125
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-298-1
SECURWARE 2013 : The Seventh International Conference on Emerging Security Information, Systems and Technologies

