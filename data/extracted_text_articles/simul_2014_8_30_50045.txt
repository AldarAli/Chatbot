Weights Decision Analysis on the Integration of Navigation Satellite System and 
Vision System for Precise Positioning 
Chi-Ho Park, Nam-Hyeok Kim 
IT Convergence Division 
DGIST  
Daegu, S.Korea  
{chpark, nhkim}@dgist.ac. 
 
 
Abstract— In this paper, we propose a precise and reliable 
positioning method for solving common problems, such as a 
navigation satellite's signal occlusion in an urban canyon and 
the positioning error due to a limited number of visible 
navigation satellites. This is an integrated system of the 
navigation satellites system and a vision system. In general, the 
navigation satellite positioning system has a fatal weakness in 
that it cannot calculate a position coordinate when its signal is 
occluded by some obstacle. For this reason, positioning by the 
navigation satellites system cannot be used for a variety of 
applications. Therefore, we propose a method to integrate both 
the navigation satellites system and a vision system using 
weights decision analysis for precise positioning. 
Keywords-GNSS; Vision; Integration; Positioning. 
I. 
 INTRODUCTION  
The Global Positioning System (GPS) was developed in 
the United States for military purposes. However, in the 
1990s, after being opened to the private sector, it has become 
widely used for vehicle navigation, aircraft, communications, 
science, agriculture, and exploration. In addition, the Soviet 
Union's navigation satellite system GLONASS was also 
opened to the private sector and this has allowed the Global 
Navigation Satellite System (GNSS) to be utilized for more 
purposes. Recently, there have been a lot of studies involving 
GNSS applications [1]-[3]. The advantages of this system 
include improved position accuracy, convenience, continuity, 
continuous usability, integrity, and so on. The most 
important feature of GNSS is its frequency band allocation. 
Consultations regarding spectrum allocation have assigned 
the L1 band to 1575.42MHz, the L2 band to 1227.60MHz, 
and the L5 band to 1176.45MHz in the WRC2000. An 
increase in the satellites and multi-frequency have solved 
problems, such as poor accuracy and continuity, however 
rapidly developing industrialization has increased many of 
these problems. In particular, the increased position error due 
to cycle slip and multipath exacerbates these problems. 
Further, many areas of the earth outdoors are still in a GPS 
shadow. These factors are the main cause of decreased 
reliability and continuity [4].  
Today’s navigation satellite system provides location 
information services within around 100 m (drms) accuracy at 
any time, regardless of the number of users. The system 
calculates a three-dimensional position by using triangulation, 
so it can be operated whenever the receiver can take signals 
from more than 4 satellites. However, locations in a city or in 
mountainous areas cannot always receive satellite signals 
because they may be blocked by high buildings or mountains. 
In such cases, the position error will be bigger or shaded 
areas will occur. Consequently, to solve these problems, 
many researchers have proposed techniques using the 
navigation satellite system and other applications [5]-[6].  
One representative research is an integration system 
involving the navigation satellite system and an Inertial 
Measurement Unit (IMU). This system is a device used by 
countries to detect the location of their own submarines, 
aircraft, missiles, etc. and to drive to a targeted destination. 
The operating principle involves calculating the moving 
variance by using accelerometers, after determining cardinal 
points by gyroscope. The moving object can always calculate 
its current position and velocity after being input with the 
initial position. The advantage of IMU is that it is not 
affected by weather or jamming. However, when moving 
over a long distance, errors are accumulated. So, GPS 
correction is needed. In the case that signals are not received 
from satellites, the errors of the IMU are exponentially 
increased. Therefore, factors which cause shaded areas and 
obstacles limit the uses of kinematic positioning [7]. Due to 
rapid industrialization, the areas and environments where the 
navigation satellite system can be used is being reduced by 
the increasing growth of urban canyons. Fig. 1 shows a 
general situation for navigation satellite system signal 
reception in an urban canyon. 
 
 
Figure 1. The situation for signal reception from the navigation satellite 
in an urban canyon. 
189
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

Recently, a vision system was reported at a DARPA 
unmanned vehicle conference in the United States, which, in 
part, dealt with obstacle recognition and detection [8]-[9]. In 
particular, one team reported that they had used only a vision 
system for detection and recognition. But the vision system 
had some constraints, such as a real time processing problem 
due to the large amounts of data, and a recognition error due 
to external lighting changes. 
Other recent researches using a vision system have made 
a lot of progress by improving the computer's performance 
and data processing time through the development of 
integration technology and the development of a broadband 
camera [10]-[11]. 
The advantages of the vision system are its wide range 
and long detection distance, the ease of data processing due 
to its similarity with the human visual system, and the 
provision of a variety of information. A stereo vision system 
that is capable of solving many of the problems of a 
monocular vision system uses a stereo matching algorithm 
for extracting a depth map, and an obstacle detection 
algorithm based on this depth map [12]. 
Therefore, in this paper, we propose a method to 
integrate both the navigation satellite system and the vision 
system using a weights decision analysis for precise 
positioning.  
This paper explains the problems of the existing fusion 
research and the limitations of positioning using a satellite 
navigation system in the Introduction. In the main part it 
proposes an equation and description for the fusion 
algorithm of the satellite navigation system and vision 
system. The performance analysis presents a comparative 
experiment and analysis of the integration of the navigation 
satellite system and vision system for precise positioning 
using weights decision. 
II. 
ALGORITHM 
A. Positioning using a navigation satellite system 
Usually, the position is obtained in point positioning 
mode by using a receiver chip that is mounted on the vehicle 
which receives the L1 C/A (Coarse/Acquisition) code from 
the navigation satellites. The C/A code observation equation 
for the navigation satellites system is given as follows. 
 


,1
,1
2
1
k
k
k
k
k
k
i
i
i
i
i
i
I
P
T
c dt
dt
e
f
 





 






2
2
2
i
k
i
k
i
k
ik
z
z
y
y
x
x







       (1) 
 
where, i and k denote receiver and satellite, respectively. 
,1
k
iP : L1 C/A code pseudorange between the receiver and 
the satellite (m); 
ik
: actual geometric distance between receiver and 
satellite (m); 
Ti k
: tropospheric delay error (m); 
2
1f
I k
i
: ionosphereric delay error (m); 
c : speed of light (m/s); 
i
dt : receiver clock error (sec); 
dt k
: satellite clock error (sec); 
k
ie 1
, : measurement error. 
 
Ionospheric delay effects and satellite clock errors are 
removed by a navigation message from the satellites. 
Tropospheric delay effects are removed by models that 
account for the dry and wet refractivity at the surface of the 
Earth. Multipath error is not assumed. Inter-frequency bias is 
ignored because of its small value. As a consequence, (2) can 
be used for the observation equation to compute the 
receiver’s position in 3-dimensional space. 
 
k
i
i
k
i
k
k
i
k
i
ik
e
cdt
cdt
f
I
T
PG
,1
2
1
,1







 
(2) 
 
We denote by 
,
k
i c
P in the left side of (2) and linearize (2) 
because it is a non-linear equation. After that, the Gauss-
Markov Model (GMM) [13] is applied. The result is (3). The 
satellites’ position coordinates are determined by using the 
navigation message. Unknown factors are the 3-dimensional 
position and receiver clock error. 
 


2
1
0
,
~ 0,
y
A
e
e
P





 
(3) 
 
Each item is shown below,  is calculated by the 
receiver's initial position
i 
i
i
x y z



,
,
. 
 
,0
,
,0
,
,0
,
k
k
i
i c
l
l
i
i c
q
q
i
i c
P
P
P
P
y
P
P








 







: Observation vector 







































c
c
c
z
z
y
y
x
x
z
z
y
y
x
x
z
z
y
y
x
x
A
q
i
i
q
q
i
i
q
q
i
i
q
l
i
i
l
l
i
i
l
l
i
i
l
k
i
i
k
k
i
i
k
k
i
i
k




























: Design matrix 
i
i
i
i
x
y
z
dt







 






: Unknown parameter vector 
190
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation


1
















n
q
i
l
i
k
i
e
e
e
e

: Measurement error vector 
 
The unknown that is calculated in (3) is the increment 
with respect to the initial value. 
 


1
ˆ
T
T
c
N
A PA
A Py



 
(4) 
 
The increment from (4) is added to the receiver's initial 
position and then the receiver's position is updated. This 
process is iterated until the increment is under the particular 
threshold value. After this process, the receiver's position is 
determined. 
 



































i
i
i
initial
i
i
i
update
i
i
i
z
y
x
z
y
x
z
y
x
 
(5) 
 
The variance component can be computed by using (6). 
Also, the variance-covariance matrix for the estimates can be 
obtained using (7). 
 
rkA
n
Pe
e T


~
~
ˆ 2
0
 
(6) 
 
where, 
ˆ
e
y
A


, n is the number of observations. 
 
 
2
1
0
ˆ
D
N




 
(7) 
B. Fusion positioning equations 
The vision system obtains observation values by 
recognizing objects. This means that the sizes of the 
observation vector and design matrix get larger as the 
number of observations increases. The distance from the 
receiver to the target object can be computed by using the 
vision system and the corresponding observation equation is 
as follows. 
 






2
2
2
i
a
i
a
i
a
a
i
a
i
a
i
a
i
z
z
y
y
x
x
e
PV










 
(8) 
 
PVi a
: distance estimated by the vision system from the 
specific object to the receiver (m). 
ia
: actual distance from the specific object to the 
receiver. 
a
a
a
z
y
x
,
,
: three-dimensional position of a specific 
object. 
i
i
i
z
y
x
,
,
:  three-dimensional position of the receiver. 
After linearization, (9) can be rewritten as follows. 
 


2
1
0
0
0
0
0
,
~
z
K
e
e
P





 
(9) 
 




1
1
0
_


a
i
PVi a
Z

: Observation vector 











 
0
a
i
i
a
a
i
i
a
a
i
i
a
z
z
y
y
x
x
A









: Design matrix 
i
i
i
i
x
y
z
dt







 






: Unknown parameter vector 
 
The distance measurement from the vision system can be 
used as an additional observation and then the Gauss-
Markov adjustment model with stochastic constraints is 
applied as shown in (10).  
 

 

1
0
0
0
ˆ
T
T
N
K P K
c
K P z





 
(10) 
 
The residual of the distance estimated by the vision 
system is 
0
0
e
z
K 


and the estimated variance 
component is (11).  
 
l
m
n
e P e
Pe
e
T
T




0
0
0
2
0
~
~
~
~
ˆ
 
(11) 
 
Here, n is the observed number of navigation satellites, 
m is the number of unknown parameters (coordinates 3, 
receiver's clock error 1), l is the number of the distance 
measurement obtained from the vision system. Also, the 
variance-covariance matrix for the estimates can be 
computed by using (12). 
 
 


1
2
0
0
ˆ
ˆ
T
D
N
K P K





 
(12) 
 
where, a is the specific object, i is a receiver, and each 
of the items are as follow. 
PVi a
: distance estimated by the vision system from the 
specific object to the receiver (m). 
ia
: actual distance from the specific object to the 
receiver. 
a
a
a
z
y
x
,
,
: three-dimensional position of a specific 
object. 
i
i
i
z
y
x
,
,
: three-dimensional position of the receiver. 
III. 
PERFORMANCE ANALYSIS 
Experiments were conducted to evaluate the reliability 
and stability of positioning based on the integration of the 
navigation satellites system and the vision system. The 
191
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

receiver of the navigation satellite system was the DL-V3 
Real Time Kinematic (RTK) of Novatel for the base station 
and the rover.  The antenna was GPS-702-GGL of Novatel, 
the RF modem was the PDL rover kit of 450 MHz, and the 
Inertial Measurement Unit (IMU) was CG-5100 of KVH. 
Also, a stereo camera was used as the image sensor. The 
focal length of the image sensor was 12 mm and the baseline 
of the stereo camera (x coordinate difference between the 
two cameras) was 300 mm.  
We compared the static and kinematic states of a vehicle 
in the experiments. 
The following are results of experiments in the static 
state. Figure 2 shows the sky plot of the static state. 
 
 
Figure 2. Sky plot. 
 
Tables I-VIII show the Horizontal Positioning Error 
according to the variance of number of satellites and vision’s 
target object. (In the tables, G means the number of visible 
navigation satellites and V means the number of target 
objects for the vision system. For example, G3V1 means that 
there are 3 navigation satellites and one vision target object.) 
Table I shows the Horizontal Position Error when there 
are three visible navigation satellites, and the number of 
target objects for the vision system is one. 
TABLE I.  
HORIZONTAL POSITION ERROR 
G3+V1 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
19.96 
1 
2 
19.89 
1 
3 
20.05 
1 
5 
20.41 
1 
10 
19.87 
2 
1 
19.83 
3 
1 
20.30 
5 
1 
20.22 
10 
1 
19.84 
Table II shows the Horizontal Position Error with three 
visible navigation satellites, and the number of target objects 
for the vision system is two. 
TABLE II.  
HORIZONTAL POSITION ERROR 
G3+V2 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
13.47 
1 
2 
16.59 
1 
3 
17.62 
1 
5 
18.18 
1 
10 
18.45 
2 
1 
10.29 
3 
1 
9.37 
5 
1 
8.90 
10 
1 
9.10 
 
Table III shows the Horizontal Position Error with three 
visible navigation satellites, and the number of target objects 
for the vision system is three. 
TABLE III.  
HORIZONTAL POSITION ERROR 
G3+V3 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
13.17 
1 
2 
16.11 
1 
3 
17.17 
1 
5 
17.85 
1 
10 
18.17 
2 
1 
10.34 
3 
1 
9.06 
5 
1 
7.69 
10 
1 
6.42 
 
Table IV shows the Horizontal Position Error with four 
visible navigation satellites, and the number of target objects 
for the vision system is zero. 
TABLE IV.  
HORIZONTAL POSITION ERROR 
G4 
Horizontal Error 
24.42 
192
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

 
Table V shows the Horizontal Position Error with four 
visible navigation satellites, and the number of target objects 
for the vision system is one. 
TABLE V.  
HORIZONTAL POSITION ERROR 
G4+V1 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
19.83 
1 
2 
20.04 
1 
3 
19.35 
1 
5 
19.62 
1 
10 
20.58 
2 
1 
19.46 
3 
1 
19.42 
5 
1 
19.26 
10 
1 
19.32 
 
Table VI shows the Horizontal Position Error with four 
visible navigation satellites, and the number of target objects 
for the vision system is two. 
TABLE VI.  
HORIZONTAL POSITION ERROR 
G4+V2 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
13.67 
1 
2 
18.01 
1 
3 
19.43 
1 
5 
20.52 
1 
10 
21.37 
2 
1 
10.26 
3 
1 
8.64 
5 
1 
7.77 
10 
1 
7.51 
 
Table VII shows the Horizontal Position Error with four 
visible navigation satellites, and the number of target objects 
for the vision system is three. 
TABLE VII.  
HORIZONTAL POSITION ERROR 
G4+V3 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
13.61 
1 
2 
15.68 
1 
3 
16.28 
1 
5 
16.73 
1 
10 
21.79 
2 
1 
10.63 
3 
1 
9.10 
5 
1 
7.60 
10 
1 
6.39 
 
Table VIII shows the Horizontal Position Error with five 
visible navigation satellites, and the number of target objects 
for the vision system is zero. 
TABLE VIII.  HORIZONTAL POSITION ERROR 
G5 
Horizontal Error 
28.15 
 
The following are results of experiments in the kinematic 
state. Figure 3 shows the kinematic state sky plot.  
 
 
Figure 3. Sky plot of kinematic state. 
 
 
193
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

Tables IX-XVI show the Horizontal Positioning Error 
according to the variance of number of satellites and vision’s 
target object. 
Table IX shows the Horizontal Position Error with three 
visible navigation satellites, and the number of target objects 
for the vision system is one. 
TABLE IX.  
HORIZONTAL POSITION ERROR 
G3+V1 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
21.62 
1 
2 
21.02 
1 
3 
21.96 
1 
5 
20.32 
1 
10 
20.33 
2 
1 
21.67 
3 
1 
20.99 
5 
1 
20.50 
10 
1 
20.34 
 
Table X shows the Horizontal Position Error with three 
visible navigation satellites, and the number of target objects 
for the vision system is two. 
TABLE X.  
HORIZONTAL POSITION ERROR 
G3+V2 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
23.95 
1 
2 
19.94 
1 
3 
19.68 
1 
5 
20.06 
1 
10 
20.25 
2 
1 
17.57 
3 
1 
20.65 
5 
1 
16.68 
10 
1 
17.43 
 
Table XI shows the Horizontal Position Error with three 
visible navigation satellites, and the number of target objects 
for the vision system is three. 
TABLE XI.  
HORIZONTAL POSITION ERROR 
G3+V3 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
18.07 
1 
2 
18.88 
1 
3 
19.29 
1 
5 
22.05 
1 
10 
19.93 
2 
1 
16.13 
3 
1 
18.96 
5 
1 
13.30 
10 
1 
12.75 
 
Table XII shows the Horizontal Position Error with four 
visible navigation satellites, and the number of target objects 
for the vision system is zero. 
TABLE XII.  
HORIZONTAL POSITION ERROR 
G4 
Horizontal Error 
20.95 
 
Table XIII shows the Horizontal Position Error with four 
visible navigation satellites, and the number of target objects 
for the vision system is one. 
TABLE XIII.  HORIZONTAL POSITION ERROR 
G4+V1 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
13.86 
1 
2 
23.18 
1 
3 
17.52 
1 
5 
20.26 
1 
10 
20.78 
2 
1 
24.86 
3 
1 
24.10 
5 
1 
13.24 
10 
1 
24.82 
194
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

Table XIV shows the Horizontal Position Error with four 
visible navigation satellites, and the number of target objects 
for the vision system is two. 
TABLE XIV.  HORIZONTAL POSITION ERROR 
G4+V2 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
22.86 
1 
2 
22.94 
1 
3 
21.67 
1 
5 
18.97 
1 
10 
20.66 
2 
1 
21.15 
3 
1 
19.21 
5 
1 
12.45 
10 
1 
11.49 
 
Table XV shows the Horizontal Position Error with four 
visible navigation satellites, and the number of target objects 
for the vision system is three. 
TABLE XV.  
HORIZONTAL POSITION ERROR 
G4+V3 
GNSS Weight 
Vision Weight 
Horizontal Error 
1 
1 
22.28 
1 
2 
22.64 
1 
3 
21.50 
1 
5 
18.65 
1 
10 
19.53 
2 
1 
19.56 
3 
1 
16.57 
5 
1 
7.45 
10 
1 
6.29 
 
Table XVI shows the Horizontal Position Error with five 
visible navigation satellites, and the number of target objects 
for the vision system is zero. 
TABLE XVI.  HORIZONTAL POSITION ERROR 
G5 
Horizontal Error 
16.33 
IV. 
CONCLUSION AND FUTURE WORK 
Tables I-VII show the Horizontal Positioning Error 
according to the variance of number of satellites and vision’s 
target object. The experimental results in Tables I-III and 
VIII-X indicate that the position could be determined with 
the addition of the vision system when there was less than 
four visible satellites. In Tables I-III, we can confirm that the 
results using three visible satellites with one, two, and three 
visual target objects are better than when using four visible 
satellites without the vision system. The performance of 
Horizontal Position Error was improved 19%, 64%, and 74%. 
In Tables V-VII, we can confirm that the results using four 
visible satellites with one, two, and three visual target objects 
are better than when using five visible satellites without the 
vision system. The performance of Horizontal Position Error 
was improved 32%, 74%, and 78%. In Tables IX-XI, we can 
confirm that the results using three visible satellites with one, 
two, and three visual target objects are better than using four 
visible satellites without the vision system. The performance 
of Horizontal Position Error was improved 0.4%, 21%, and 
40%. In Tables XIII-XV, we can confirm that the results 
using four visible satellites with one, two, and three visual 
target objects are better than when using five visible 
satellites without the vision system. The performance of 
Horizontal Position Error was improved 19%, 21%, and 40%. 
 
ACKNOWLEDGEMENT 
This work was supported by the DGIST R&D Program 
of the Ministry of Science, ICT & Future Planning of Korea. 
(14-IT-01) 
REFERENCES 
[1] J. B. Tsui , “Fundamentals of Global System Receivers, A 
Software Approach”, John Wiley & Sons, 2. 2000, pp. 10-27. 
[2] P. Ramjee and R. Marina, “Applied Satellite Navigation 
Using GPS, Galileo, and Augmentation Systems”, Artech 
House, 2005. 
[3] W. B. Hofmann, H. Lichtenegger, and J. Collins, “GPS Fifth, 
revised edition”, 2001. 
[4] D. K. Elliott, “Understanding GPS : Principles and 
Applications”, Artech House, 1996. 
[5] W. Jinling, G. Matthew, L. Andrew, J. W. Jack, H. Songlai, 
and S. David, “Intergration of GPS/INS/Vision Sensors to 
Navigate Unmanned Aerial Vehicles”, The International 
Archives of the Photogrammetry, Reomote Sensing and 
Spatial In formation Sciences, 37, pp. 963-969, Beijing, 2008. 
[6] S. Godha and M. E. Cannon, “Integration of DGPS with a 
Low Cost MEMS-based Inerial Measurement Unit(IMU) for 
Land Vehicle Navigation Application”, Proceedings of ION 
GPA-05, Institute of navigation, pp. 333-345, Long Beach, 
2005. 
[7] A. Broggi, C. Caraffi, R. I. Fedriga, and P. Grisleri, “Obstacle 
Detection 
with 
Stereo 
Vision 
for 
Off-road 
Vehicle 
Navigation”, Computer Vision and Pattern Recognition, IEEE 
Computer Society Conference on, 3, pp. 65-72, 2005. 
[8] Y. C. Lim, M. H. Lee, C. H. Lee, S. Kwon, and J. H. Lee, 
“Improvement of Stereo Vision-based Position and Velocity 
Estimation and Tracking using a Strip-based Disparity 
Estimation and Inverse Perspective Map-based Extended 
Kalman Filter”, Optics and Lasers in Engineering, 48, 9, pp. 
859-868, 2010. 
195
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

[9] M. Bertozzi, A. Broggi, M. Cellario, A. Fascoli, P. Lombardi, 
and M. Porta,  “Artificial Vision in Road Vehicles”, 
Proceeding of the IEEE,  90, 7, pp. 1258-1271, 2002. 
[10] N. Srinivasa, “A Vision-based Vehicle Detection and 
Tracking Method for Forward Collision Warning”, IEEE 
Intelligent Vehicle Symposium, pp. 626-631, 2002. 
[11] V. Lemonde and M. Devy, “Obstacle Detection with Stereo 
Vision”, Mechatronics and Robotics,  Germany, 2004. 
[12] E. J. Rossetter, J. P. Switkes, and J. C. Gerfes, “Experimental 
Validation of the Potential Field Lanekeeping System”, 
International journal of automotive technology, 5, 95, 2004. 
[13] R. Havard and H. Leonhard, “Gaussian Markov Random 
Fields”, 2007. 
196
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-371-1
SIMUL 2014 : The Sixth International Conference on Advances in System Simulation

