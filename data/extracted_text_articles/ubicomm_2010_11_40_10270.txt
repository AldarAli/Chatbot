SmartBuilding: a People-to-People-to-Geographical-Places Mobile System based on 
Augmented Reality 
Andrea De Lucia, Rita Francese, Ignazio Passero and Genoveffa Tortora 
Dipartimento di Matematica e Informatica 
University of Salerno 
84084 – Fisciano (SA) – Italy 
Tel:+39 (0) 89 963376 
adelucia@unisa.it, francese@unisa.it, ipassero@unisa.it, tortora@unisa.it 
 
 
Abstract— People-to-People-to-Geographical-Places systems 
connect places to communities. Generally, these systems 
require users to perform complex tasks using mobile phones. 
Thus, the creation of powerful interaction techniques and the 
design of effective and easy interfaces are key requirements. 
The system we propose in this paper, named SmartBuilding, is 
a mobile Augmented Reality system that supports the sharing 
of 
contextualized 
information 
in 
indoor 
environment, 
depending on the user location. This system shifts the 
interaction with the user from the classical desktop logical 
metaphors towards a more natural interaction style based on 
augmenting and annotating an indoor environment. It 
combines the world perceived by the phone camera with 
information concerning the user location and his/her 
community, enabling users to create several working areas and 
access to augmented content. SmartBuilding proposes an 
innovative interface that adopts the mobile device sensors to 
identify the content to be shown and to control the user 
interaction. Results on the usability of the proposed approach 
are positive. 
Augmented Reality; mobile user interfaces; context-
awareness; People-to-People-to-Geographical-Places systems. 
I. 
 INTRODUCTION 
Web 2.0 and social software are changing the way in 
which millions of users communicate. Users are not only 
receiver of content, but they contribute to the content 
creation in a bottom-up way, generating social networks and 
communities [3]. 
Changes in the people habits are also due to the great 
diffusion of mobile devices that enables users to be 
connected “anytime, anyway”. The adoption of these devices 
enables also the diffusion of localization technologies. The 
most adopted is the Global Positioning System (GPS), for 
outdoor environments, while, in indoors environments recent 
location sensing systems range from RFID, requiring explicit 
installation, to standard wireless networking hardware, see 
[1][9] for example, or Bluetooth tags, suffering of several 
problems, such as long time for device discovering and 
passing through walls. More recently, WiFi triangulation and 
video detection approaches have been proposed. 
According to Gartner, one of the ICT enterprise world 
leader, Location-Based services are at the second place in the 
first ten more required mobile devices applications in 2012 
[7]. Moreover, according to Gartner, the request of this 
service will strongly increase in the next years. Gartner 
predicts that Location Based users will increase from 96 
millions in 2009 to 526 millions in 2012. This kind of 
services is second in the top ten list for the high value that 
they have for users. In fact, they answer to several user 
necessities, from productivity to social network and 
entertainment needs.  
Several 
People-to-People-to 
Geographical-Places 
systems have been proposed in literature [10], aiming at 
connecting social networks and communities to physical 
places. Actually, new powerful devices enable systems to 
incorporate place and people in new and powerful ways.  
Augmented Reality (AR) is a technology that allows 
computer generated virtual imagery to exactly overlay 
physical objects in real time [22]. The integration of user 
localization, AR and of the feature offered by the top-of-the-
range devices (on-board camera, accelerometers, compass, 
GPS etc.) enables the device to combine the camera preview 
with AR information in real-time. Thus, a mobile device can 
be seen as a window onto a located 3D information space, 
enabling “to browse, interact, and manipulate electronic 
information within the context and situation in which the 
information originated and where it holds strong meaning” 
[5]. Following this approach, information can be provided 
and created considering the context and the user profile. 
Mobile devices are small, thus researchers have to face the 
challenge of designing usable interfaces for device screens 
with limited dimensions and invent new interaction 
modalities. 
In 
this 
paper, 
we 
propose 
a 
system, 
named 
SmartBuilding, which follows the metaphor of the 
“Cooperative Building” proposed in [19], .i.e. room elements 
with integrated information technology to support formal and 
informal communication. The approach we propose does not 
require specific hardware, except the user mobile devices, 
whose usage is largely diffused, and adopts innovative 
interfaces, which control the user interaction using the 
mobile device sensors. 
II. 
RELATED WORKS 
Several research projects, such as [4][14][16][19], 
investigate 
People-to-People-to-Geographically-Places 
systems displaying notes and messages [10].  
E-graffiti [4] is a context-aware application, which 
detects the user’s location and displays notes dependent on 
263
UBICOMM 2010 : The Fourth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-100-7

that location. The application is evaluated and results show 
that location-specific notes were appealing to users. 
GeoNotes [16] is a location-based messaging system, which 
allows users to provide their contents in order to create a 
social and dynamic information space. It does not allow 
remote access to notes. All these systems provide a 
traditional menu-based  interface.  
Augmented reality has been adopted by the Augmented 
Reality Post-It (AR Post-It) messaging system, which, 
similarly to our approach, uses the mobile phone as an 
augmented reality (AR) interface allowing users to view 
electronic messages in an AR context [19]. There is the need 
of a paper marker in each specific location where messages 
are available, i.e. on the fridge. 
Microsoft Research proposed Notescape, a tool that 
creates a “mixed reality” where virtual sticky notes 
appear to float in the physical space around the user's 
body [14]. The notes, using a mobile camera, follow the 
users as they move from place to place. 
An AR interface has been adopted by CAMAR (Context-
aware Mobile Augmented Reality), a system enabling users 
to interact with smart objects through personalized control 
interfaces on their mobile AR devices [15]. Similarly to our 
approach, it supports context-based contents augmentation 
and the sharing of contents among user communities. The 
main difference with the system we propose is that 
SmartBuilding does not need additional hardware: we 
associate the contents to a specific point of the room, while 
in CAMAR the interaction is limited to particular objects or 
markers. 
The adoption of onboard sensors, like orienteer and 
accelerometer, to intercept user interaction, enables to 
implement novel and natural user interfaces. Even if not 
strictly related to the mobile technology, it is important to 
underline how Nintendo adopts low cost accelerometers in 
Wii controllers to enrich user experience and augment game 
usability. Wii controllers are equipped with onboard sensors 
and speakers to keep the user gaming experience 
analogically real. User movements reproduce the real actions 
and are captured and replicated on the screen, keeping the 
user involved in the experience; the controller speaker gives 
the player a better sense of immersion. Recently, mobile 
phones are providing the same interesting features, 
sometimes offering a more powerful technological platform: 
they are capable of detecting orienteering and acceleration 
and have the computational power to augment the preview 
obtained by the onboard camera. These devices may shift the 
user interaction from the classical keypad or button input 
towards on the phone movement [8]. As an example, 
Labyrinth Light [12], an Android application, controls a ball 
on a plane by reading the on board devices sensor. In a more 
complex way, Google Sky Map projects the user in an AR 
sky space. The device sensors are used to understand where 
the user is pointing his/her cellular phone and augments the 
screen with information about stars and planets.  
Adopting the same technology, Layar [13] makes sets of 
data viewable on top of the camera of the mobile phone as 
one pans around a city and point at buildings. Layers are 
equivalent to web-pages in normal browsers. Real estate, 
banking and restaurant companies have already created 
layers of information available on the platform. The recent 
version 3.0 enables also to augment reality with 3D objects. 
ARToolKit uses computer vision techniques to calculate 
the real camera position and orientation relative to marked 
cards, allowing the programmer to overlay virtual objects 
onto these cards [1]. Differently, in our approach the marked 
cards are adopted only to initially localize the user. 
III. 
THE PROPOSED SYSTEM 
SmartBuilding aims at augmenting a physical building 
with spatially localized areas in which users can share formal 
and informal multimedia documents and messages. The 
mobile devices are capable of projecting the user in an 
augmented world of information, controlling the camera 
interaction with on-board sensors (i.e. accelerometer and 
orientation). By moving his/her device in the surrounding 
space the user abandons the 2D desktop metaphor (i.e. 
folders and icons) and adopts spatial movements for 
exploring a new information space. The device screen 
becomes the touchable interface of this world and the user 
position and his/her profile provide the context.  
SmartBuilding offers support for formal and informal 
communications. 
Indeed, 
it 
is 
possible 
to 
create 
administration areas, where formal communication is 
provided, as an example, describing some office procedure. 
Others information areas can be available for team work, or 
informal communication can be exchanged among specific 
user groups. Thus, the system enables each user to distribute 
his/her augmented content to specific colleagues, supporting 
selective content sharing and collaboration among people 
belonging to the same community. Voting and commenting 
features are also available and support information filtering. 
A. The informative space 
The device is used as a hand held lens giving a moving 
view on the AR scene. It is important to point out that the 
user needs to hold the device and, therefore, his/her 
maneuverability is quite reduced.  
Basing on these observations, we decided to adopt the 
Azimuth orientation sensor, creating, as a consequence, a 
360° space. This space constitutes a cylinder surrounding the 
user and ideally lays near the walls of his/her room. In our 
approach, the Azimuth is the main dimension of the 
augmented reality. The Pitch orientation sensor is adopted 
combined with the accelerometer, to detect how the camera 
is orientated in the space vertical dimension. This 
information enables to provide feedback to user by 
prospectively deforming the projected objects according to 
the device spatial orientation and is at the core of the area 
pagination system later presented. 
Each environment of the building is equipped with a 
“Quick Response” (QR) code [21], having a twofold use: (i) 
it univocally encodes the room and (ii) it enables to locate 
the user position in the environment. The environmental 
setup of the system requires the user to direct the camera 
towards the QR code by pointing a viewfinder visualized on 
this/her camera preview. The obtained resolution of the room 
264
UBICOMM 2010 : The Fourth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-100-7

marker enables us to deduct the shooting user-QR distance. 
In addition, the shooting angle, obtainable by comparing the 
shut QR side dimensions, allows us to determine more 
precisely the user position in the room. The devices also 
communicate the current state of the Wi-Fi signals from the 
various access points to the central server. Thus, it is possible 
to deduce each position variation by considering the azimuth 
variation, by integrating the detected acceleration variation 
and by interpolating the new Wi-Fi [18] signal, i.e. the 
strength of each access point carrier, of a user with his/her 
previous configurations and with those of the others. 
B. The augmented interfaces 
The environmental setup feature and the state variation of 
the device permit to reconstruct the user position. The 
augmented reality is corrected respect to the user orientation 
in the room obtaining the invariance to the user position for 
the projected objects in such a way that all users are able to 
find the information connected to the same physical place. 
This mechanism enables to associate each augmented 
information stream to a concrete area of the real 
environment.  
As an example, Figure 1 shows a portion of the 
augmented space of the Software Engineering Lab. The 
information concerning the location is depicted in the lower 
left-hand part of Figure 1, while information on the Software 
School and on the events concerning the lab is depicted on 
the left and on the right of this figure, respectively. The user 
can access the information using the interaction styles 
proposed in the next sub-section. Let us note that in the right-
hand lower corner of the screen depicts the position of the 
augmented areas inside the considered room [11]. This 
feature can be disabled.  
 
 
Figure 1.  A screenshot of an augmented informative space 
The interface in Figure 2 also displays the information 
concerning the owner of the area. A touch action on this area 
enacts the displaying of more details on the owner, if the user 
has the permissions, see Figure 2. 
Selecting an icon in the lower part of Figure 2, the user 
communicates with the owner activating a telephone call or 
by email, he/she can see the owner web site or, using Google 
Maps find his/her position on the hearth. This interface 
allows the user a quick passage from the AR modality to 
other applications. 
Another example of augmented area is shown in Figure 
3, where the list of the users working in a given room is 
associated to their office door, on the external side. Thus, it 
is possible to verify the user state, if they are or not in the 
room and to leave them a message that they will receive 
when they enter their office. 
 
 
 
Figure 2.  Contact information of an AR area owner 
A “Notes” area is available in each room registered with 
the system. This area represents a communication space 
related to the room and its occupants, displaying short 
messages painted directly on screen with fingers and stored 
as images, named Sticky Notes. Figure 4 depicts this 
augmented space populated by the users of a room 
concerning common interest facts. We noticed that the 
adoption of a paper background for messages provides a 
more realistic appearance.  
 
 
 
 
 
 
 
 
 
 
 
In addition to the “Room Users” and “Notes” areas, the 
system supports content sharing as follows: users can create 
areas to share documents with their colleagues, as an 
example to support group work. Each area can contain 
several contents: Sticky Notes, Text Notes and any other 
kind of files (including multimedia content) supported by the 
devices.  
 
Figure 3. A Room Users’ area 
265
UBICOMM 2010 : The Fourth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-100-7

The creation of a new area is activated through a menu 
choice: the system exposes to user a viewfinder and a button, 
to localize the new area position. In addition, during the 
creation phase, the user is required to provide the area name. 
When an area is no longer used, its contents can be saved on 
the supporting web site for further consultation. 
C. The new interaction style 
SmartBuilding offers two different user interaction 
styles: it is possible to directly manipulate the AR contents or 
use an indirect interaction style, based on SDK list selection. 
In the former case, the augmented areas are visually 
paginated.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Because the users’ movements are quite reduced when 
they hold the phone, there was the need of simplifying the 
user interaction. In particular, when the objects to show in 
the areas exceed the available size, a pagination mechanism 
is required. In the right-hand sides of Figures 3 and 4, the 
proposed pagination control is depicted. It is represented by a 
vertical segment and by a shorter horizontal segment, 
indicated by an arrow in Figure 4. 
The main characteristic of this mechanism is that a 
simple gesture enables the user to change the visualized 
objects: the user can request the objects located in the next 
page of the selected area by tilting upward his/her device. 
The symmetrical gesture takes his/her navigation backwards. 
Exploiting the phone orientation sensor, it is possible to 
detect the Pitch movements of the device for controlling the 
pagination system. When the user changes the pitch 
direction, an analogous movement is induced on the 
paginated objects. In this way, the user perceives the objects 
to move according to the inclination given to the device. The 
area shape is also prospectively deformed according to the 
phone inclination. 
Up and down movements cause the segment to 
accordingly move. The scrolling mechanism is activated 
only when the device inclination exceeds the thresholds 
represented by the “SCROLL UP” and “DOWN” markers. 
When a user clicks on an icon, the contextualized action 
is fired. As an example, in the case of “Notes” and user 
defined “Document” areas, the click on an object causes the 
corresponding editing application to be launched. When the 
user needs to create a new note or document, he/she is 
required to select a distribution list choosing among the users 
of the system. The creation of these objects is directly 
supported at interface level by the button “NEW”, as shown 
in Figure 4 in the case of “Notes”.   
 If the indirect manipulation is preferred, the classical list 
mechanism, exposed by the Android SDK, is adopted to 
drive the user action in choosing the objects. When the user 
touches an area, a scrolling list is presented.  
In addition to visual feedback, user involvement in the 
proposed experience may be enhanced by audio and haptic 
feedbacks. According to Henrysson et al. [8], we adopt the 
device speaker and its vibration to add multi sensorial 
feedbacks to user actions. 
IV. 
EVALUATION 
To assess the usability of the proposed system we carried 
out an evaluation study. In particular, following the 
traditional approach proposed in [11], we analyzed the user 
reactions to the functionalities provided by SmartBuilding.  
To evaluate a context-aware AR system it is also 
important to consider that AR representations combine 
rendered graphics with the real world environment and 
require a specific type of interaction among virtual artifacts 
and the real world.  
For example, user localization can be difficult if the user 
moves too fast, and, therefore, the system has problems in 
showing the appropriate contents in the appropriate location. 
Thus, we also evaluate the usability of the system following 
the directions proposed in [6], adapting it to the case of 
mobile phones. 
In the following, we illustrate the techniques used in 
these evaluation studies and we present the obtained results.  
A. 
Preparation and User tasks 
After a short introductory session on using the device 
(mainly focused on menu, back button and SDK menus) and 
the proposed system, the subjects, provided with the 
appropriate 
paper 
documentation, 
were 
required 
to 
accomplish the following tasks: 
 
• 
Task 1 – Users had to leave a text note on the Room 
Area on a door of the building.  
• 
Task 2 – Users had to read and comment the event 
information in the Software Engineering lab. 
• 
Task 3 – Collaboration. The subjects were structured in 
groups; each group owns a virtual area. Each group 
performs a simple collaborative session in which the 
group members upload on the Group Wall material 
concerning a lecture they have taken part (1.5 hour). 
Each group comments and votes the contribution of their 
peers of the other groups (1 hour). 
 
At the end of each task, we collected feedback from users 
about their experience with SmartBuilding by submitting 
them a task evaluation questionnaire. We followed the 
template After Scenario Questionnaire (ASQ) proposed by 
IBM [11]. It consists of three questions aiming at 
determining user satisfaction concerning the task completion, 
evaluating 
their 
satisfaction 
regarding 
the 
ease 
of 
 
Figure 4. The Notes Area associated to the room white-board. 
266
UBICOMM 2010 : The Fourth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-100-7

completing, the time taken and the support information 
available during the task. It is designed using a 7-point Likert 
scale anchored at 1 by Strongly Agree and at 7 by Strongly 
Disagree.  
Once the tasks were accomplished, the participants filled 
the Computer System Usability Questionnaire (CSUQ) [11] 
consisting in 19 questions evaluating user overall system 
usability focused on 3 subscales: System Usefulness, 
Information Quality and Interface Quality. Also in this case a 
7-point Likert scale anchored at 1 by Strongly Agree and at 7 
by Strongly Disagree has been adopted, but each answer 
contains an open “Comment” space to collect deeper details 
about user impressions.  
To integrate the evaluation of mobile augmented 
characteristics, we added to CSUQ questionnaire the 
additional questions proposed in [11], concerning the user 
perception of: System Lag, Image Disparity, Resolution, 
Rendering Quality, Maneuverability and Environmental 
Conditions. We also added some specific question tailored 
on the nature of our system. To evaluate in detail some 
aspects of the proposed interface, we formulated the 
following additional questions scored on a Likert scale 
anchored from 1, Very good to 7, Poor: 
 
1. The horizontal movement of augmented objects is …  
2. The vertical scrolling control is …  
3. The vertical inclination feedback is … 
4. The environment lighting affects my performance.  
 
In particular, we added question 4 to investigate if in case 
of a system mixing on the same screen an environmental 
preview with synthetic objects, the room lighting may 
disturb the user sight.  
B. 
Participants 
Subjects of the evaluation were eighteen students 
attending to the fundamentals of Computer Science course of 
the 
Environment 
Evaluation 
and 
Control 
program 
(University of Salerno).  
A user profile questionnaire has been proposed to the 
participants before the evaluation started. Ten participants 
have a good experience in the usage of mobile devices and 
no more than 30% of the sample had good computer skills.  
The participants were located in a didactic laboratory. 
The system was setup localizing the “Notes” area on the 
white-board of the laboratory, the “Room” area on the door 
of the course teacher and three separate group areas have 
been created in the lab for Task 3.  
C. 
Results 
Analyzing the result provided by ASQ questionnaires 
concerning Task 1 (Avg. score 3.33), Task 2 (Avg. score 
3.11) and Task 3 (Avg. 2.66) we noticed a diffused user 
satisfaction. In particular, subjects preferred Task 3, probably 
for its collaborative nature.  
In general, we noticed that the worst perception about the 
easiness of completing the tasks has been signaled by less 
computer skilled users and by less mobile expert ones. We 
separated the CSUQ questionnaire results from the 
Augmented Mobile specific ones, since the former is suitable 
to determine the overall system usability, while the latter 
provides a specific evaluation of usability aspects concerning 
the proposed augmented reality interface. Figure 5 reports 
the CSUQ results aggregated in three categories. 
The participants diffusely perceived the system as useful 
(average of SYS USE was 1.83). Analyzing the single 
question scores, it was evident that the great part of the 
subject sample felts confident to be able to accomplish the 
assigned tasks in an effective, quick and comfortable way. 
Just two subjects, not particularly technical skilled, 
expressed negative judgments.  
The system is also evaluated in terms of the quality of the 
information provided and robustness (INFO factor in Figure 
5). Also in this case the evaluation is positive. A specific 
question group of CSUQ questionnaire is devoted to evaluate 
the interface quality (INTERFACE). In particular, two 
specific questions required the users to score how the 
interface is pleasant and how much they liked it. Analyzing 
the individual answers, we noticed that, also the number of 
subjects not liking the interface was limited. 
The OVERALL usability factor provides the overall 
system rating and is defined by the full set of items of the 
three sub-factors. As depicted in Figure 5, this factor reaches 
a satisfying score of 2.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Concerning the Mobile Augmented Questionnaire, we 
aggregated the various questions proposed in [6] with the 
four additional ones proposed in Section 4.1, in the three 
categories described in the Legenda of Figure 6. Also in the 
case of Mobile Augmented evaluation, the user perception 
was positive. In particular, best results were obtained by the 
“GRAPH” category that evaluates the perceived graphical 
resolution and rendering qualities. Image disparity is a 
critical factor in augmented reality interfaces. Indeed, if the 
AR content reproduced on the camera is offset from the real 
world view, the user can be disoriented. User impressions on 
the proposed augmented reality interface were good for the 
most of subjects. The system lag (COMM/NET factor) did 
not affect the perceived system quality and did not impact on 
 
Legenda:  
SYS USE=”system usefulness”, INFO=”information quality”,  
INTERFACE=”interface 
quality”, 
OVERALL=“overall 
usability score” 
Figure 5. The CSUQ questionnaire results. 
267
UBICOMM 2010 : The Fourth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-100-7

the maneuverability (MAN), which is the movement of the 
user is not limited to match the augmented information. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Analyzing the “environmental conditions” questionnaire 
result category, it was also evident that the system is affected 
by changes in the environmental lighting. Indeed, we 
observed that, the proposed interface is suitable for typical 
working room lighting.  
V. 
CONCLUSION 
In this paper, we have presented SmartBuilding, a 
People-to-People-to-Geographical-Places system aiming at 
creating an informative space surrounding the user where 
contents are created and displayed using Augmented Reality 
interfaces. The SmartBuilding Augmented Reality interface 
tries to improve the user interaction providing pagination 
mechanisms based on the device sensors.  
The results of the usability evaluation of the proposed 
system have been positive. The evaluation revealed some 
problems with the environmental lighting, thus, we are going 
to develop new interfaces suitable for different lighting 
conditions (i.e. home working rooms or laboratories). At the 
present, we are investigating how to adopt the proposed 
technology to define collaborative methodologies to support 
work group and learning. Future work will also be devoted to 
make SmartBuilding, developed only for Android platform, 
portable on different mobile devices. 
REFERENCES 
[1] ARtoolkit, http://www.hitl.washington.edu/artoolkit/. Retrieved on 
May 2010. 
[2] P. Bahl and V.N. Padmanabhan, “RADAR: An In-Building RF-Based 
User Location and Tracking System”, In Proc. of IEEE INFOCOM 
2002, vol. 2, pp. 775–784, 2002.  
[3] T. Bemers-Lee, "Berners-Lee on the readlwrlte web", BBC, August 9, 
2005. 
[4] J. Burrell and G. Gay, “E-Graffiti: Evaluating real-world use of a 
context-aware system”, Interacting with Computers, vol. 14, n. 4, pp. 
301–312, 2002. 
[5] G. Fitzmaurice, “Situated Information Spaces and Spatially Aware 
Palmtop Computers”, Communications of the ACM, Vol. 36, no. 7, 
pp.39-49, July 1993. 
[6] D. Haniff and C. Baber, “User Evaluation of Augmented Reality 
Systems”, Proc. of the Seventh International Conference on 
Information Visualization (IV’03), 2003 
[7] Gartner, “Gartner Identifies the Top 10 Consumer Mobile 
Applications 
for 
2012”, 
http://www.gartner.com/it/page.jsp?id=1230413. Retrieved on May 
2010. 
[8] A. Henrysson, M. Billinghurst, M. Olilla, “Face to Face Collaborative 
AR on Mobile Phones”, 2005. Proc. of Fourth IEEE and ACM 
International Symposium on Symposium on Mixed and Augmented 
Reality,  pp. 80 - 89 , 2005. 
[9] H. Hile and G. Borriello, “Positioning and Orientation in Indoor 
Environments Using Camera Phones”, Computer Graphics and 
Applications, IEEE Volume: 28, Issue: 4, pp: 32-39, 2008. 
[10] Q. Jones and S.A. Grandhi, “P3 Systems: Putting the Place Back into 
Social Networks”, IEEE Internet Computing, 2005, pp. 38-46, 2005. 
[11] J. Lewis, “IBM Computer Usability Satisfaction Questionnaires: 
Psychometric Evaluation and Instructions for Use”, International 
Journal of Human-Computer Interaction, vol.7 no. 1, pp. 57-78,  
1995. 
[12] “Labirynth 
Light”, 
retrieved 
on 
May 
2010 
from 
http://www.android.com/market/free.html#app=labyrinth. 
Retrieved 
on May 2010. 
[13] “Layar”, http://layar.eu/. Retrieved  on May 2010.  
[14] “Notescape”, Microsoft Research,    http://research.microsoft.com/en-
us/projects/ncci/default.aspx. Retrieved on May 2010. 
[15] S. Oh and W. Woo, “CAMAR: Context-aware Mobile Augmented 
Reality in Smart Space,”” In Proc. of IWUVR 2009, pp. 48-51, 2009. 
[16] P. Persson, P. Fagerberg, “GeoNotes: A real-use study of a public 
location-aware community system”, Technical Report SICS-T–
2002/27-SE, SICS, University of Goteborg, Sweden, 2002. 
[17] C. Savarese, J. Rabaey, K. Langendoen, “Robust Positioning 
Algorithms for Distributed Ad-Hoc Wireless Sensor Networks”, In 
Proc. of the General Track: 2002 USENIX Annual Technical 
Conference, pp. 317–327, 2002. 
[18] A. Savidis, M. Zidianakis, N. Kazepis, S. Dubulakis, D. Gramenos, 
and C. Stephanidis, “An Integrated Platform for the Management of 
Mobile Location-aware Information Systems”, In Proc. of Pervasive 
2008. Sydney, Australia, pp. 128–145, 2008. 
[19] S. Singh, A. David Cheok, G. Loong Ng, F. Farbiz, “Augmented 
Reality Post-It System”, Proc. in Advances in Computer 
Entertainment Technology, p. 359, 2004. 
[20] N. A. Streitz, P. Tandler, C. Müller-Tomfelde, S. Konomi, 
“Roomware: Toward the Next Generation of Human-Computer 
Interaction Based on an Integrated Design of Real and Virtual 
Worlds”, In J. A. Carroll (Ed.): Human-Computer Interaction in the 
New Millennium, Addison Wesley, pp. 553–578, 2001. 
[21] Wikipedia 
QR-code, 
retrieved 
on 
March 
2010 
from 
http://en.wikipedia.org/wiki/QR_Code,  Retrieved on May 2010. 
[22] F. Zhou, H. Been-Lirn Duh, M. Billinghurst, “Trends in augmented 
reality tracking, interaction and display: A review of ten years of 
ISMAR”, Proc. of the 7th IEEE/ACM International Symposium on 
Mixed and Augmented Reality,pp. 193-202,  2008. 
 
COMM/NET GRAPH
MAN
ENV
1
2
3
4
5
6
7
Mobile Augmented Questionnaire
 
Legenda:  
COMM/NET=”system lag”, GRAPH=”image disparity, 
resolution and rendering quality”, MAN=”maneuverability”, 
ENV=“environmental conditions” 
Figure 6. The Mobile Augmented questionnaire results 
268
UBICOMM 2010 : The Fourth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-100-7

