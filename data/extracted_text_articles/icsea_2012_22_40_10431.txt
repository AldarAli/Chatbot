Simulation-Based Management for Software Dynamic Testing Processes 
Mercedes Ruiz 
Department of Computer Science and Engineering 
University of Cádiz 
Cádiz, SPAIN 
e-mail: mercedes.ruiz@uca.es 
Javier Tuya 
Department of Computing 
University of Oviedo 
Gijón, SPAIN 
e-mail: tuya@uniovi.es 
Daniel Crespo 
Department of Computer Science and Engineering 
University of Cádiz 
Cádiz, SPAIN 
e-mail: dani.crespobernal@alum.uca.es 
 
Abstract - Managing software development projects requires 
the coordination of different processes which may be 
performed by different teams, e.g., a development team and a 
separate testing team. This coordination aims at optimizing the 
trade-off between cost, schedule and delivered quality. 
Simulation models are a powerful tool to explore what-if 
scenarios that help managers to achieve this trade-off and to 
fine-tune different project parameters. This paper presents a 
simulation model based on a multi-paradigm approach which 
connects development and testing processes. The testing 
process model is based on the process model described in the 
ongoing standard ISO/IEC DIS 29119-2. The simulation model 
is built using two different methods: the discrete-event 
approach, to simulate the execution of the dynamic testing 
processes, and the agent-based approach, to in-depth simulate 
defects life cycle. Results show how the simulation model is 
used to optimize the efficiency of the testing process. 
Keywords - software testing process; multiparadigm 
simulation; management 
I. 
 INTRODUCTION  
Software testing is concerned with planning, preparation 
and evaluation of software products and related work 
products to: a) determine that they satisfy specified 
requirements, b) demonstrate that they are fit for purpose and 
c) detect defects [17]. In general, testing can be viewed as a 
means of improving the quality of a given product and 
mitigating risks due to poor quality.  
Testing can be carried on using different approaches 
(e.g., scripted or exploratory), at different levels (e.g., unit, 
system, integration or acceptance), using different techniques 
and tools and with different degrees of independency 
(ranging from testing performed by the producer to third 
party testing). When testing entails the execution of the 
system under test, it is often referred to as dynamic testing. 
Testing exists in an organizational context and is carried on a 
given project or service. Therefore, the testing activities are 
tightly interrelated with the development ones, and both shall 
be planned, monitored and controlled. Problems of quality of 
the system under test or delays in the development hamper 
the testing process. Conversely, an inadequate or delayed 
testing endangers the development process. If not managed 
properly, both development and testing processes may 
jeopardize the goals of cost, schedule and quality of a 
project. 
Both development and testing can be described as 
processes and take advantage of the use of simulation models 
for helping project and/or test managers in daily tasks of 
planning, monitoring and control. 
Informally, a simulation model can be considered as an 
abstract view of a complex system comprised of a set of 
rules that tell how to obtain the next state of the system from 
the current state. Those rules can be of many different forms: 
differential equations, state charts, process flowcharts, 
schedules, etc. The outputs of the model are produced and 
observed as the model is running. 
There is much research on simulation models of the 
software development process [12]. However there is lesser 
research on simulation models for the testing process, 
usually at the unit level. Furthermore, when testing is 
considered as part of a simulation model of the development 
process, it is often over simplified. The goal of this paper is 
to devise a multi-paradigm simulation model for the testing 
process to gain insights in how the testing process influences 
the goals of a given project.  
The main contribution of this work is a multi-paradigm 
simulation model of the dynamic testing processes which 
combines a discrete-event model and agent-based model. 
The model can be used to simulate the testing process at the 
system level and to help in decision-making in the test 
managing processes. 
The structure of the paper is as follows: Section II shows 
the works related to our proposal; Section III introduces the 
multi-layer process model proposed by the International 
Standards group upon which our simulation model is based; 
Section IV describes the simulation model; Section V shows 
some simulation runs. Finally, our conclusions and further 
work are given in Section VI. 
II. 
RELATED WORK 
The search string “simulation” AND “software testing 
process” AND “management” and others alike used in 
630
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-230-1
ICSEA 2012 : The Seventh International Conference on Software Engineering Advances

several digital libraries and citation databases of peer-
reviewed literature retrieves only a few number of papers. In 
many of the papers retrieved, the term “simulation” is 
frequently used to describe experiences in which simulation 
is used as a tool for the testing process. In other works, the 
term “simulation” makes reference to a set of formulas that 
are solved by analytical means.   
As an example of the first usage, in their collection of 
works, Lazić, Mastorakis and Velasěvić [7]-[11] aim at 
raising awareness about the usefulness and importance of 
computer-based simulation in support of software testing. In 
their works, simulation is used to ease the design and 
execution of the testing processes of real military and 
defense systems. 
Some analytical models of the software testing process 
can also be found. Zhang, Zhou, and Luo [19] propose a 
reward-Markov-chain-based 
quantitative 
model 
for 
sequential iterative processes and show how to use it to 
estimate the time for the software testing process. Similarly 
to this, Lizhi, Weiqin and Bin Zhu [12] propose an approach 
to model the testing process based on hierarchical time 
colored Petri Nets (HTCPN). However, while Petri-nets are 
good at modeling resources and parallel processing, 
simulation modeling models system components and their 
interactions, making it possible to conduct arbitrary time-
related performance analysis, something which is not easy 
using Petri-nets.  
Consequently to overcome the problems of analytical 
methods, simulation modeling can be applied in the context 
of testing processes mainly because: a) it enables to find 
solutions when analytical methods fail; b) it is a more 
straightforward process than analytical modeling since the 
structure of the simulation model naturally mimics the 
structure of the real system, and c) it is scalable, flexible, and 
easy to communicate since the modeling tools use visual 
languages. 
However, despite these advantages there is a small 
number of contributions of simulation modeling in the field 
of software testing processes. Saurabh [15] presents a system 
dynamics (SD) model of software development with a 
special focus on the unit test phase. This work is partially 
based on Collofello’s et al. [2] work about modeling the 
software testing process under the SD approach. 
The motivation of these works is closely related to ours, 
but the models are built under a different simulation 
approach. System Dynamics approach operates at high 
abstraction level and is mostly used for strategic modeling. 
Hence, since a simulation model can only be used at the 
abstraction level in which it has been created, such a highly 
abstract model is not adequate for the operational and tactical 
levels in which decision-making regarding the testing 
processes takes place. In our case, since our main interest is 
to simulate the testing processes the discrete event (DE) 
modeling, with the underlying process-centric approach, has 
been selected. Furthermore, we have also selected the agent-
based (AB) approach to be used together with the discrete-
event one resulting in a multi-paradigm simulation model.  
Generally, each simulation approach (SD, DE, AB) 
provides a set of different abstractions. If the system being 
modeled is complex enough, and software development is, 
then it is preferable to integrate different simulation methods 
than using one single approach, since the final model will 
represent the real system more realistically. 
When we used the search string (“multi-paradigm” OR 
“multi-method”) AND “simulation” AND “software testing 
process” and others alike in the digital libraries and citation 
databases, no single work was retrieved. Therefore, given 
the results of the systematic literature review performed, not 
fully documented here for space reasons, to the best of our 
knowledge our proposal is the first one that aims at using 
multi-paradigm simulation modeling to improve decision 
making in software testing management. 
III. 
MULTI-LAYER TEST PROCESS MODEL 
Testing processes include a variety of management and 
technical activities which are organized in a process model 
in part 2 of the draft ISO standard for software testing: 
ISO/IEC 29119-2 [4]. The purpose of this international 
standard is to define a generic process model for software 
testing that can be used by any organization when 
performing any form of software testing. Testing is 
structured in a multi-layer process model that defines the 
software testing processes at (1) the organizational level, (2) 
test management level and (3) dynamic test level. More 
specifically, the dynamic test level describes how dynamic 
testing is carried out within a particular phase of testing 
(e.g., unit, integration, system and acceptance) or type of 
testing (e.g., performance testing, security testing and 
usability testing). It is composed of four processes that are 
depicted in Figure 1.  
 
Test Design & Implementation Process: Describes 
how test cases and test procedures are derived; these 
are normally documented in a test specification, but 
may be immediately executed. 
 
Test Environment Set-Up & Maintenance Process: 
Describes how the environment in which tests are 
executed is established and maintained. 
 
Test Execution Process: Describes how the test 
procedures generated as a result of the Test Design 
& Implementation Process are run on the test 
environment established by the Test Environment 
Set-Up & Maintenance Process.  
 
Test Incident Reporting Process describes how the 
reporting of test incidents is managed.  
The Test Execution Process is run after the tests have 
been specified and the environment has been established 
which leads to a strong dependency on the previous 
processes. This process may need to be performed a number 
of times as all the available test procedures may not be 
executed in a single iteration. Additionally, this process 
must be reentered as a consequence of detected failures after 
the underlying defects have been corrected (retesting). 
 
631
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-230-1
ICSEA 2012 : The Seventh International Conference on Software Engineering Advances

 
Figure 1.  Dynamic Test Processes. 
Besides, the Test Design & Implementation Process, and 
the Test Environment Set-Up & Maintenance Process may 
be reentered whether additional tests are needed after 
execution or some problems are detected in the testing 
environment. The Test Incident Reporting Process may be 
also reentered as a result of: a) the identification of test 
failures, b) something unusual or unexpected occurred 
during test execution, or c) retest activities. 
IV. 
MODEL DESCRIPTION 
The simulation model developed is described below in 
terms of its scope, result variables, process abstraction and 
input parameters. The description is organized following 
Kellner’s proposal for describing simulation models [5]. 
A. Model Proposal and Scope 
To determine a model proposal, the key questions that 
the model needs to address need to be identified. Then, 
model scope is set so that it is large enough to fully solve the 
key questions posed. In the context of this work, model 
proposal is to help in decision-making in software testing 
process management. Accordingly, the scope for this model 
will be a portion of the life cycle, with a short time span (i.e., 
the months in which the testing activities take place), one 
software product and two teams (i.e., development and 
testing teams) organizational breadth.  
B. Result Variables 
The result variables are the information elements needed 
to answer the key questions regarding the purpose of the 
model. In our model, several process metrics have been 
identified to help us understand the simulated process 
capability. According to this, process metrics have been 
classified into effectiveness and efficiency process metrics. 
Effectiveness process metrics measure the extent to 
which a process produces a desired result [1]. 
The following result variables fall into this category: 
 
Defect Closure Period (DCP). The longer a reported 
defect takes to go from discovery to resolution, the 
higher the project risk associated with the underlying 
defect. Unresolved defects may: a) delay testing, b) 
make development less efficient or c) prevent the 
delivery of the software to the final customers. DCP 
measures the difference between the time required to 
repair a defect and the time required to confirm the 
defect is repaired. 
 
Defect Open Count. This measure tracks the number 
of times a defect report is opened. When the report is 
first submitted this count is set to one. This count is 
incremented each time the same defect report is 
reopened due to a failure in the confirmation test 
(retest). 
 
FixBacklog: Shows the percentage of defects closed 
per all the defects opened in a given time. 
 
Total Planned Test: The metric shows the evolution 
of the number of planned test cases along the testing 
project. 
 
Total Executed Tests: Shows the evolution of actual 
test cases which are executed along the project. 
 
Total Passed Tests: Shows the actual test cases 
which are executed and successfully passed (e.g., did 
not find any defects). 
 
Total Failed Tests: Shows the actual test cases that 
are executed and failed (e.g., did find defects). 
Efficiency process metrics measure the extent to which a 
process produces its desired results in a not wasteful way 
and, ideally, minimizing the resources used [1]. 
Result variables in this category follow: 
 
Actual test time: Shows the total length of the testing 
process.  
 
Total team size and number of people per activity: 
Shows the total size of the testing team and the 
number of resources allocated to each activity of the 
process, respectively. 
 
Process efficiency: Shows the ratio of the number of 
defects closed per the number of defects found. 
C. Process Abstraction 
When developing a simulation model, the key elements 
of the process, their inter-relationships, and behavior need to 
be identified. The focus should be on those aspects of the 
process that are especially relevant to the purpose of the 
model, and believed to affect the result variables [5].  
One of the decisions that need to be made in this phase is 
the simulation paradigm that it is going to be used to build 
the model. A simulation paradigm is a general framework for 
mapping a real world system to its model. The choice of 
paradigm should be based on the system being modeled and 
the purpose of the modeling. When modeling complex 
systems, it is frequent that different parts of the system are 
most naturally modeled using different paradigms. In this 
case, a multi-paradigm model is built. 
In order to build our model, the multi-paradigm approach 
has been selected. First, to model and simulate the dynamic 
testing processes, the paradigm selected has been the 
discrete-event or process centric approach. Under this 
approach, the system being modeled is considered as a 
process, i.e., a sequence of operations being performed 
across entities, and this makes this paradigm the most natural 
and adequate to build process simulation models. The model 
is specified graphically as a process flowchart, where blocks 
represent the operations to be done along the process.  
Although a simulation model following this approach 
allows us to analyze the evolution of the testing activities, 
632
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-230-1
ICSEA 2012 : The Seventh International Conference on Software Engineering Advances

the resource consumption and the number of defects 
detected, it would be interesting to add an extra functionality 
to the simulation model allowing the user to track the life of 
every defect since it is found until it is closed. It is important 
to notice that to achieve this aim the level of abstraction used 
needs to be changed from process-centric to individual-
centric. Agent-based modeling is a simulation approach that 
allows the modeler to build a model under a bottom-up 
perspective, that is, describing the behavior of individuals 
(e.g., agents) and, if needed, their interactions. Frequently, 
the behavior of an agent is formalized by means of a state 
chart-like diagram. Therefore, this approach seems to be 
most natural and adequate to describe the lifecycle of defects 
found during the testing phase. As a consequence, a multi-
paradigm simulation model was our choice for our modeling 
problem. 
In summary, the model consists of two connected 
models. A description of each of these models follows: 
1) Discrete event model (DE).  
The discrete event model represents the Dynamic Test 
Processes in ISO/IEC 29119-2 [4] previously described in 
Section III. 
The development process produces two main artifacts that 
are the input for the testing processes:  
1. The test basis, usually the software specification, 
which is modeled as a set of features. 
2. The executable code that is to be exercised by the 
tests. 
 The availability of the test basis enables the execution of 
the Test Design & Implementation Process which leads to a 
number of test cases. However, test cases are not ready to be 
executed until the test environment has been established 
(Test Design & Implementation Process) and the executable 
code released. Once the code is installed in the testing 
environment, the Test Execution Process can begin. Failed 
test cases are the input for the Test Incident Reporting 
Process and the results communicated to the development 
processes through the Agent-based model. Test execution 
reenters when previously detected defects have been fixed by 
development. 
2) Agent-based model (AB).  
During the software development process, each defect has 
a lifecycle in which it reaches different states. In order to 
simulate the different states that a defect reaches the agent-
based paradigm has been used. Under this approach, we 
formalize the defects found as agents and their behavior as a 
state chart that reflects the different states and transitions of 
defect lifecycle. A description of each state in which the 
agents can be follows: 
 
New: An agent reaches this state when a defect is 
reported by the tester for the first time and is yet to 
be approved. 
 
Analyzed: Once a defect is reported, the manager has 
to analyze it in order to approve it as a genuine 
defect, reject or defer it. The agent remains in this 
state during the time in which this activity takes 
place. When the activity is done, the information for 
deciding what to do with the defect is available, and 
so, the agent moves to the next state which can be 
one of the following: a) Rejected: If a defect is found 
to be invalid, b) Deferred: If a defect is decided to be 
fixed in upcoming releases, and c) Assigned: If a 
defect is found to be valid and assigned to a member 
of the development team to fix it. 
 
Fixed:  An agent moves to this state once the 
developer communicates the defect is fixed. The 
defect goes to the testing team for validation by 
injecting a task in the DE model to indicate that the 
test case that found this defect has to be executed 
again (retest). The result of this execution will 
determine the next state of the agent. 
 
Closed. If the tester finds that the defect is indeed 
fixed and is no more a cause of concern, the agent 
moves to the state Closed. Otherwise, if the defect is 
not fixed or partially fixed, the agent will go again to 
the state Assigned in which the work of a developer 
working on its fixing will be simulated again. 
D. Input Parameters 
The input parameters to include in the model largely 
depend upon the result variables desired and the process 
abstractions identified. Input parameters allow setting up 
different scenarios for simulation. The input parameters of 
the simulation model are the following: 
 
Software size: Size of the software product under 
development. 
 
FPA per Feature: Adjusted Functional Points per 
feature. 
 
Number of Test Cases per Feature: Number of test 
cases that need to be designed and executed per 
feature.  
 
Initial number of tasks in Environment Setup. Initial 
number of tasks that need to be done for the 
common and global environment setup. 
 
Estimated Time for Environment Setup. Time 
estimated to develop each environment setup task. 
 
Environment Setup Resources. Number of people 
allocated to the Environment Setup processes. 
 
Estimated 
Time 
for 
Test 
Design 
and 
Implementation. Time estimated to develop each 
task of the Test Design and Implementation 
processes. 
 
Test Design and Implementation Resources. Number 
of people allocated to the Test Design and 
Implementation process. 
 
Estimated Time for Test Execution. Time estimated 
to develop each task of the Test Execution processes. 
 
Test Execution Resources. Number of people 
allocated to the Test Execution processes. 
 
Estimated Time for Test Incident Reporting. Time 
estimated to develop each task of the Test Incident 
Reporting processes.  
 
Test Incident Reporting Resources. Number of 
people allocated to the Test Incident Reporting 
Processes. 
633
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-230-1
ICSEA 2012 : The Seventh International Conference on Software Engineering Advances

 
Estimated time to fix a defect. Time estimated to a 
fix a defect by a developer. 
 
Code released for Test Execution.  Indicates when 
the code is released for testing. This value is 
provided as a percentage of delay measured 
regarding the initial estimated time for the testing 
project. 
 
Probability of finding a defect per Test Case 
Execution. Probability that a Test Case finds a defect 
when the test case is executed the first time. 
 
Probability of finding a defect per Test Case in 
Retest Execution. Probability that a Test Case finds a 
defect when the defect has been reported as fixed. 
In order to achieve more realistic results, the model 
accepts a triangular distribution for most of the above input 
parameters. 
V. 
SIMULATION RUNS 
The model implementation and the simulation runs have 
been performed using AnylogicTM software [18] with the 
Enterprise Library. The model logic is written in Java.  
a) Base scenario (BS). In this scenario the base 
simulation is run to determine the values of the result 
variables and, analyze the results of the process. In order to 
obtain a set of reasonable parameters we have estimated the 
costs of the different activities using a set of ratios observed 
in average risk profiles [6]. We consider functional testing 
for a system test phase in a project with waterfall 
development, experienced builders and a structured test 
approach driven by risk. Ratios of functional design, 
realisation and functional test are 1:2:1. In relation with the 
test processes, the ratios of test design & implementation, 
execution, reporting and environment set-up are 50:40:5:5, 
respectively. The values of the input parameters in this 
scenario are displayed in TABLE I.  
TABLE I.  
SCENARIO CONFIGURATION 
Input parameter 
Value 
Software size 
800 FPA 
FPA per Feature 
5 
Number of Test Case per Feature 
(0.5, 2, 4) 
Initial number of tasks in Environment Setup 
5 tasks 
Estimated Time for Environment Setup 
(10, 14.4, 20) hours 
Environment Setup Resources 
1 person 
Estimated Time for Test Design and 
Implementation 
(3, 4.5, 6) hours 
Test Design and Implementation Resources 
4 people 
Estimated Time for Test Execution 
(1.5, 3.2, 4.5) hours 
Test Execution Resources 
4 people 
Estimated Time for Test Incident Reporting 
(1.5, 3, 4.5) hours 
Test Incident Reporting Resources 
1 person 
Estimated time to Fix a defect 
(3, 4.5, 6) hours 
Code released for Test Execution 
15% 
Probability of finding a defect per Test Case 
Execution 
(5%, 15%, 25%) 
Probability of finding a defect per Test Case in 
ReTest Execution  
(10%, 20%, 30%) 
Figures 2 and 3 contain a summary of the main process 
metrics. Figure 2 shows that 160 test cases were planned, 
which were able to find 116 defects (see Figure 3), 6 of them 
were rejected and 9 of them deferred. 101 defects were 
closed and 26 reopened. The Test Efficiency reached with 
this setting is 87%, which is reasonable in practice, showing 
the consistency of the model when using the above 
parameters.  
Figure 2.  Different process output metrics in base scenario 
b) Optimization. Even though simulation runs are 
useful to visualize the effect of different values of the input 
parameters in the process performance, that is, to execute 
what-if scenarios in managerial decision-making, a key 
benefit can be obtained when we use together simulation 
and metaheuristic optimization algorithms in a process 
called simulation optimization. In this case, it is possible to 
obtain which values need to take the input parameters in 
order to maximize  or   minimize  an  output  variable. To  
show  this  
Figure 3.  Number of defects in each state at the end of the simulation 
 
application in our field of study we run an optimization 
experiment to determine wheter it is possible to improve the 
efficiency of the test process by controlling the moment in 
which the executable code is available for testing. The 
optimization will determine the distribution of the human 
resources that maximizes test efficiency when the code is 
released for testing in an range that varies from 5% to 50% 
from the moment the testing process begin. The optimizer 
OptQuest® [14] built-in AnylogicTM has been used for this 
optimization. Table II shows the input values for the control 
parameters of the experiment, the constraints impossed and 
the results obtained in the optimized process compared with 
the base case. 
 
 
634
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-230-1
ICSEA 2012 : The Seventh International Conference on Software Engineering Advances

TABLE II.  
OPTIMIZATION  CONTROL PARAMETERS AND OUTPUTS 
COMPARED WITH BASE SCENARIO 
The results of the optimization experiment show that 
under the constraints imposed it is possible to achieve 97% 
of efficiency in the process allocating 7 people to the process 
and having a maximum delay of the code released for testing 
of 27% of the initial time estimated. This will result into a 
process which is 97% in closing defects but finishes one 
month later than the base scenario. The conclusion drawn 
from this particular experiment with regard to the base 
scenario is that if the project is adequately scheduled, it is 
possible to reduce the total number of test resources as well 
as increase the process efficiency. In other situations 
simulation can help to find the best input values for project 
schedule, resource allocation and quality objective from 
among all that lead to the optimization of the key process 
outputs. 
VI. 
CONCLUSION AND FURTHER WORK 
This paper presented a simulation model for the dynamic 
testing processes which allows a seamless integration 
between the testing and development processes. The model 
is devised as a multi-paradigm model composed by a discrete 
event simulation model, to simulate the execution of the 
dynamic test processes, and an agent-based simulation 
model, to in-depth simulate the defects life cycle. Results 
show that the simulation model can be effectively used to 
optimize different project parameters and then help managers 
to achieve a trade-off between cost, schedule and quality. 
This is a first step in the use of multi-paradigm 
simulation models for testing. Further work will include, 
although not limited to, the consideration of agent-based 
models to simulate parts of the dynamic test processes, the 
integration into a more complex project development 
simulation model [3]  and experimentation in different 
projects using different lifecycle models and including 
different test levels of testing. After calibrating and 
validating the model with historical data from the industry, it 
will be also possible to exploit it as an operating tool for 
decision-making in the industrial domain. 
ACKNOWLEDGMENTS 
This work has been partially supported by the Spanish 
Ministry of Science and Technology with ERDF funds under 
grants TIN2010-20057-C03-03 and TIN2010-20057-C03-01. 
REFERENCES 
[1] Black, R. Managing the Testing Process: Practical Tools and 
Techniques for Managing Hardware and Software Testing. Wiley 
Publishing, 2002. 
[2] Collofello J.S., Zhen Y., Tvedt J.D., Merrill D., and Rus, I. Modeling 
software testing processes (1996) Conference Proceedings - 
International 
Phoenix 
Conference 
on 
Computers 
and 
Communications, pp. 289-293. 
[3] Crespo D. and Ruiz M. Decision Making Support in CMMI Process 
Areas using Multiparadigm Simulation Modeling. 2012 Winter 
Simulation Conference. Berlin, December 9-12, 2012. Accepted. 
[4] ISO/IEC DIS 29119-2 Software and Systems Engineering - Software 
Testing – Part 2: Test Process. December 2011 (Draft International 
Standard). 
[5] Kellner M.I., Madachy R.J., and Raffo D.M. Software Process 
Modeling and Simulation: Why, What, How?, Journal of Systems and 
Software, Vol. 46, No. 2/3 (15 April 1999). 
[6] Koomen, T., van der Aalst, L., Broekman, B., and Vroon, M. TMap 
Next for result-driven testing. UTN Publishers, 2007. 
[7] Lazić L. and Mastorakis N. The use of modeling & simulation-based 
analysis & optimization of software testing (2005) WSEAS 
Transactions on Information Science and Applications, 2 (11), pp. 
1918-1933.  
[8] Lazić L. and Mastorakis N.  RBOSTP: Risk-based optimization of 
software testing process Part 2 (2005) WSEAS Transactions on 
Information Science and Applications, 2 (7), pp. 902-916.  
[9] Lazić L. and Mastorakis N. RBOSTP: Risk-based optimization of 
software testing process Part 1 (2005) WSEAS Transactions on 
Information Science and Applications, 2 (6), pp. 695-708. Cited 3 
times.  
[10] Lazić L. and Mastorakis N. Integrated intelligent modeling, 
simulation and design of experiments for Software Testing Process 
(2010) International Conference on Computers - Proceedings, 1, pp. 
555-567. 
[11] Lazić L. and Velasěvić D. Applying simulation and design of 
experiments to the embedded software testing process (2004) 
Software Testing Verification and Reliability, 14 (4), pp. 257-282. 
[12] Lizhi, C., Weiqin T., Bin Z., and  Zhang J. Modeling Software 
Testing Process Using HTCPN, Fourth International Conference on 
Frontier of Computer Science and Technology, 2009. FCST '09, pp. 
429-434, 17-19 Dec. 2009. 
[13] Madachy, RJ. Software Process Dynamics. John Wiley & Sons, Inc., 
2008. 
[14] OpTek Systems, Inc. OptQuest®. http:// www.opttek.com/  [retrieved: 
October, 2012] 
[15] Saurabh, K. Modeling unit testing processes a system dynamics 
approach (2008) ICEIS 2008 - Proceedings of the 10th International 
Conference on Enterprise Information Systems, 1 ISAS, pp. 183-186.  
[16] Soni R., Jolly A., and Rana A. Effect of residual defect density on 
software release management (2011) International Journal of 
Software Engineering and its Applications, 5 (4), pp. 151-158.  
[17] van Veenendaal, E (ed). Standard glossary of terms used in Software 
Testing, Version 2.1. International Software Testing Qualifications 
Board. Oct. 2010. 
[18] XJ Technologies. AnylogicTM. http://www.anylogic.com/ [retrieved: 
October, 2012] 
[19] Zhang W.M., Zhou, B.S., and Luo, W.J. Modeling and simulating of 
sequential iterative development processes (2008) Jisuanji Jicheng 
Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS, 
14 (9), pp. 1696-1703.  
Input parameter 
Control 
Input 
 
Result  
(Base 
scena.) 
Result  
(Optim.) 
Initial number of tasks in 
Environment Setup 
3-5 tasks 
5 
5 
Environment Setup Resources 
1-4 people 
1 
1 
Test Design and 
Implementation Resources 
1-4 people 
4 
2 
Test Execution Resources 
1-4 people 
4 
3 
Test Incident Reporting 
Resources 
1-4 people 
1 
1 
Code released  for Test 
Execution 
5% - 50% 
15% 
27% 
Constraints 
Value 
Testing Team Size 
<= 7 people 
Maximum Testing Time 
Overrun 
<= 1 month 
Process Efficiency obtained 
87% 
97% 
635
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-230-1
ICSEA 2012 : The Seventh International Conference on Software Engineering Advances

