Graph Neural Network for Accurate and Low-complexity SAR ATR
Bingyi Zhang‚àó, Sasindu Wijeratne‚àó, Rajgopal Kannan‚Ä†, Viktor Prasanna‚àó, Carl Busart‚Ä†
‚àóUniversity of Southern California ‚Ä†DEVCOM US Army Research Lab
e-mail: ‚àó{bingyizh, kangaram, prasanna}@usc.edu ‚Ä†{rajgopal.kannan.civ, carl.e.busart.civ}@army.mil
Abstract‚ÄîSynthetic Aperture Radar (SAR) Automatic Target
Recognition (ATR) is the key technique for remote sensing
image recognition. The state-of-the-art works exploit the deep
convolutional neural networks (CNNs) for SAR ATR, leading to
high computation costs. These deep CNN models are unsuitable
to be deployed on resource-limited platforms. In this work, we
propose a graph neural network (GNN) model to achieve accurate
and low-latency SAR ATR. We transform the input SAR image
into the graph representation. The proposed GNN model consists
of a stack of GNN layers that operates on the input graph to
perform target classification. Unlike the state-of-the-art CNNs,
which need heavy convolution operations, the proposed GNN
model has low computation complexity and achieves comparable
high accuracy. The GNN-based approach enables our proposed
input pruning strategy. By filtering out the irrelevant vertices
in the input graph, we can reduce the computation complexity.
Moreover, we propose the model pruning strategy to sparsify the
model weight matrices which further reduces the computation
complexity. We evaluate the proposed GNN model on the MSTAR
dataset and ship discrimination dataset. The evaluation results
show that the proposed GNN model achieves 99.38% and 99.7%
classification accuracy on the above two datasets, respectively.
The proposed pruning strategies can prune 98.6% input vertices
and 97% weight entries with negligible accuracy loss. Compared
with the state-of-the-art CNNs, the proposed GNN model has
only 1/3000 computation cost and 1/80 model size.
Keywords‚ÄîSynthetic aperture radar, automatic target recogni-
tion, graph neural network, low computation complexity, model
pruning
I. INTRODUCTION
Synthetic aperture radar (SAR) is capable of high-resolution
remote sensing and independent of weather conditions to
observe the targets on the earth ground. SAR automatic target
recognition (ATR) is the crucial technique to classify the target
in the SAR images and has been used in many real-world
applications, such as agriculture [1] [2], civilization [3] [4],
etc. SAR devices are typically mounted on moving platforms,
such as aircraft, spacecraft, and small/micro satellites [5]‚Äì
[9]. These moving platforms usually have limited computa-
tion resources and power budgets (e.g., 80-180W [10]). The
state-of-the-art works [11]‚Äì[15] develop complex convolu-
tional neural networks (CNNs) for SAR ATR to achieve high
classification accuracy. However, complex CNNs suffer from
high computation costs and large memory footprints, making
them unsuitable to be deployed on resource-limited platforms.
For example, to achieve real-time image classification using
CNNs, GPU is widely used. The power consumption of a state-
of-the-art GPU device (e.g., NVIDIA RTX3090 has a power
consumption of 450W) can exceed the power budget of the
small/micro satellites.
Objects in MSTAR dataset
Objects in ship discrimination dataset
Figure 1. The objects in the SAR images
We identify that CNNs have high computation costs due
to (1) heavy convolution operations and (2) CNNs do not
exploit the data sparsity in SAR images because CNNs need
to use the whole image as input. As shown in Figure 1, an
object in a SAR image usually has a small number of pixels,
and most pixels are irrelevant for classification. Recently,
Graph Neural Networks (GNNs) are proposed to operate
on graph data structure and have been successfully applied
to many graph classification tasks [16]‚Äì[18], such as point
cloud classification. [19] has proven that GNN can classify
a graph based on its graph structural information and vertex
features. Motivated by that, we propose to use GNN for
SAR ATR. First, we extract the image pixels of the target
object. We use these pixels to build a graph by constructing
the edge connections among the pixels. We exploit GNN to
operate on the input graph for target classifying. The proposed
GNN-based approach achieves significantly less computation
cost and comparable accuracy compared with state-of-the-art
CNNs. Moreover, we propose attention mechanisms, including
vertex attention and feature attention, to improve the model‚Äôs
accuracy. Our main contributions are:
‚óè We propose a novel GNN model for SAR ATR with atten-
tion mechanisms, including vertex attention and feature
attention, to achieve high accuracy with low computation
complexity.
‚óè We propose the input pruning strategy and the weight
pruning strategy to further reduce the computation com-
plexity with negligible accuracy loss.
‚óè We perform detailed ablation studies to evaluate (1)
various connectivity for constructing the input graph, (2)
various types of GNN layers, (3) the effect of the attention
25
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-079-7
GEOProcessing 2023 : The Fifteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

Input SAR Image (ship)
Graph Representation
Graph Construction + 
Input Pruning
Graph Neural Network
Classification Result
Figure 2. Overview of the proposed approach
TABLE I. NOTATIONS
Notation
Description
Notation
Description
G(V, E, X0)
input graph
vi
ith vertex
V
set of vertices
eij
edge from vi to vj
E
set of edges
L
number of GNN layers
hl
i
feature vector of vi at layer l
N (i)
neighbors of vi
mechanism, and (4) the impact of the proposed pruning
strategies.
‚óè We evaluate the proposed approach on MSTAR and
ship discrimination datasets. The evaluation results show
that the proposed GNN model achieves 99.38% and
99.7% classification accuracy on the above two datasets,
respectively. Compared with the state-of-the-art CNNs,
the proposed GNN model has only 1/3000 computation
cost and 1/80 model size.
The rest of the paper is organized as follows: Section II
presents the proposed GNN model for SAR ATR; Section III
describes the proposed pruning strategies for reducing com-
putation complexity; Section IV demonstrates the evaluation
results.
II. PROPOSED MODEL
Figure 2 depicts the overview of the proposed approach.
In Section II-A, we introduce the basics of the graph neural
network. In Section II-B, we cover the proposed graph repre-
sentation for the SAR images. In Section II-C, we introduce
the proposed GNN model architecture.
A. Graph Neural Network
We define GNN notations in Table I. Graph Neural Net-
works (GNNs) [20]‚Äì[22] are proposed for representation learn-
ing on graph G(V,E,X0). GNNs can learn from the structural
information and vertex features and embed this information
into low-dimension vector representation/graph embedding
(For example, hL
i is the embedding of vertex vi). The vector
representation can be used for many downstream tasks, such
as node classification [21] [20], link prediction [23], graph
classification [24], etc. As shown in Figure 3, GNNs follow the
message-passing paradigm that vertices recursively aggregate
information from the neighbors.
Input: Graph: G(V,E); vertex features: {h0
1,h0
2,...,h0
‚à£V‚à£};
Output: Output vertex features {hL
1 ,hL
2 ,...,hL
‚à£V‚à£};
1: for l = 1...L do
2:
for each vertex v ‚àà V do
3:
al
v = Aggregate(hl‚àí1
u
‚à∂ u ‚àà N(v))
4:
zl
v = Update(al
v,W l), hl
v = œÉ(zl
v)
Figure 3. GNN Computation Abstraction
B. Graph Representation
We transform the input SAR image into a graph represen-
tation G(V,E,X0), where each pixel in the SAR image is
mapped to a vertex v ‚àà V in the graph. The SAR signal value
of the pixel becomes the feature of the vertex. Each pixel
is connected to its neighbors as the edge connections E. As
shown in Figure 4, we propose the following two ways of
connecting a pixel to its neighbors and evaluate them in the
experiments:
‚óè 4-connectivity: Each pixel is connected to the four neigh-
bors: up (p2), down (p8), left (p4), and right (p6).
‚óè 8-connectivity: Each pixel is connected to the eight
neighbors: p1, p2, p3, p4, p6, p7, p8, p9.
p
p6
p4
p2
p8
p
p6
p4
p2
p8
p3
p1
p7
p9
4-connectivity
8-connectivity
Figure 4. Two types of connectivity for constructing input
graph
C. Model Architecture
The proposed model architecture is shown in Figure 5,
which consists of a stack of layers, including Graph Neural
Network layers, graph pooling layers, and attention layers. The
final Multi-layer Perceptron (MLP) generates the classification
result. For simplicity, vi,j denotes the vertex/pixel that locates
at ith row and jth column in original SAR image. The
input to layer l (1 ‚©Ω l ‚©Ω L) is the vertex feature vectors
{hl‚àí1
i,j ‚à∂ vi,j ‚àà Vl‚àí1} and edges {e ‚à∂ e ‚àà El‚àí1} that defines
26
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-079-7
GEOProcessing 2023 : The Fifteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

GNN Layer
Graph Pooling Layer
Attention Layer
GNN Layer
Graph Pooling Layer
Attention Layer
‚Ä¶‚Ä¶
MLP
Classification results
Feature
Attention
Vertex
Attention
x
x
+
1ùë†ùë° layer
2ùëõùëë layer
3ùëüùëë layer
4ùë°‚Ñé layer
5ùë°‚Ñé layer
Figure 5. Diagram of model architecture
the connectivity of the vertices in Vl‚àí1. The output of layer l
is the vertex feature vectors {hl
i,j ‚à∂ vi,j ‚àà Vl}.
Graph neural network (GNN) layer: A GNN layer follows
the Aggregate-Update paradigm as shown in Algorithm 3.
Using the Aggregate() function, each vertex aggregates the
feature vectors from the neighbors (line 3 of Algorithm 3).
Then, each feature vector is updated by the Update() function
to generate the updated feature vector (line 4 of Algorithm 3).
There are some representative Graph Neural Network layers,
such as GCN [20], GraphSAGE [21], GIN [19], and SGC [25].
Graph pooling layer: It downscales the input graph Vl‚àí1 into
a smaller output graph Vl. The pooling operaton is similar to
the pooling in the 2-D images:
hl
i,j = max(hl‚àí1
2i,2j,hl‚àí1
2i+1,2j,hl‚àí1
2i,2j+1,hl‚àí1
2i+1,2j+1)
(1)
where vl
i,j ‚àà Vl, and vl‚àí1
2i,2j,vl‚àí1
2i+1,2j,vl‚àí1
2i,2j+1,vl‚àí1
2i+1,2j+1 ‚àà Vl‚àí1.
Attention layer: We exploit the attention mechanism to im-
prove the accuracy. The attention layer consists of feature
attention that calculates the attention scores for each vertex
feature, and vertex attention that calculates the attention scores
for each vertex. The feature attention is calculated by:
Ffa = sigmoid(mean({hi,j ‚à∂ vi,j ‚àà V})W mean
fa
+
sum({hi,j ‚à∂ vi,j ‚àà V})W sum
fa
)
(2)
where hi,j,Ffa ‚àà Rc, W mean
fa
,W sum
fa
‚àà Rc√óc, and c denotes
the length of feature vector. fa[i] is the attention score for ith
feature. The vertex attention score is calculated using a GNN
layer:
{Œ±i,j ‚à∂ vi,j ‚àà Vl} = sigmoid(GNNL({hi,j ‚à∂ vi,j ‚àà Vl‚àí1})), (3)
Where Œ±i,j is the attention score for vertex vi,j. Then, the
output of the attention layer is calculated by:
{hout
i,j ‚à∂ hout
i,j = (1 + Œ±i,j)hin
i,j + hin
i,j ‚äó Ffa}
(4)
where ‚äó is element-wise multiplication.
Multi-layer Perceptron (MLP): After a sequence of layers,
all the feature vectors are flattened into a single vector, which
is sent to the MLP for classification. MLP has a stack of fully
connected (FC) layers.
III. PRUNING
This section covers the proposed pruning techniques, includ-
ing, input pruning (Section III-A), and weight pruning (Section
III-B).
A. Input Pruning
The key benefit of using GNN is that GNN is flexible
in accepting any graph structure as the input. Thereby, we
are able to exploit input pruning to reduce the computation
complexity. Theoretically, in a SAR image (See Figure 1),
the pixels not in the target do not affect the classification
results. As studied in [26], by properly setting up a constant
threshold Iv, we can filter out most irrelevant pixels since
the pixels that do not belong to the target usually have
negligible SAR signal magnitude. After constructing the input
graph from the SAR image, we prune the vertices that have
a magnitude smaller than Iv. The magnitude of a vertex
is calculated by
‚àö
x2
1 + x2
2 + ... + x2np where np denotes the
number of polarization of the SAR signal. For example, a
quad-polarization system has four kinds of polarization ‚Äì
horizontal-horizontal (HH), vertical-vertical (VV), horizontal-
vertical (HV), and vertical-horizontal (VH). After pruning the
vertices, all the edges connected to the pruned vertices are also
pruned. Due to the input pruning, the graph pooling operation
(Equation 1) is slightly modified:
hl
i,j = max(1l‚àí1
2i,2j ‚ãÖ hl‚àí1
2i,2j,1l‚àí1
2i+1,2j ‚ãÖ hl‚àí1
2i+1,2j,
1l‚àí1
2i,2j+1 ‚ãÖ hl‚àí1
2i,2j+1,1l‚àí1
2i+1,2j+1 ‚ãÖ hl‚àí1
2i+1,2j+1),
(5)
where 1i,j ‚àà {0,1} is the indicator that indicates the existence
of vertex vi,j. After input pruning, we can skip the compu-
tation for the pruned vertices, which greatly reduces the total
computation complexity.
B. Weight Pruning
As analyzed in [27], [28], the weight matrices in GNNs have
redundancy, and some weight entries can be pruned without
affecting the classification accuracy. Therefore, to reduce the
total computation complexity, we perform weight pruning by
training the model using lasso regression [29]. We add the L1
penalty to the loss function:
loss = l(y,y‚Ä≤) + Œª
W
‚àë
w
‚à£w‚à£
(6)
27
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-079-7
GEOProcessing 2023 : The Fifteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

where l(y,y‚Ä≤) is the classification loss, and Œª‚àëW
w ‚à£w‚à£ is the
L1 penalty term parameterized by Œª. The L1 penalty leads
to weight shrinkage during training. Thereby, some model
weights become zeros and can be eliminated from the model.
After training, we set a threshold Iw, and the model weights
with absolute values smaller than Iw are pruned.
IV. EVALUATION
We evaluate our approach on two widely used datasets:
‚óè MSTAR: The setting of the MSTAR dataset follows the
state-of-the-art work [11] [14] [15] [12]. MSTAR contains
the SAR images of ten classes of ground vehicles, with
2747 images in the training set and 2427 images in the
testing set.
‚óè Ship discrimination [30]: For the ship discrimination
dataset, we follow the setting in [31], which is a binary
classification task that identifies if a given SAR image has
a ship or not. The dataset contains 1596 positive image
samples and 1596 negative image samples.
A. Evaluation on MSTAR Dataset
1) Experimental Setting: For the MSTAR dataset, we use
the following setting. The proposed model consists of 12
layers. We develop the proposed model using Pytorch Geo-
metric. We use the cross-entropy loss as the classification loss
(Equation 6). We train the model using the Adam optimization
algorithm. The training batch size is set as 20, and the initial
learning rate is 0.02. Œª (for lasso regression) is set as 0.002.
The L2 weight decay is set as 0.08. We train the model for 150
epochs, and the learning rate is multiplied by 0.5 for every 10
epoch. We use the 8-connectivity to build the input graph. We
evaluate the three widely used GNN layers in the proposed
model ‚Äì GCN layer [20], GraphSAGE layer [21], and GAT
[22]. We train the proposed model using one NVIDIA RTX
A6000 GPU.
Performance metrics: We evaluate the proposed approach us-
ing the following metrics: classification accuracy, computation
complexity, and number of parameters.
TABLE II. THE ACCURACY ON MSTAR DATASET
GNN Layer
Type
Connectivity
Training
Accuracy
Testing
Accuracy
Training
Time
GCN
4
99.16%
90.06%
3.0 hours
8
95.44%
83.82%
4.0 hours
GAT
4
99.53%
92.21%
1.8 hours
8
82.71%
71.33%
1.9 hours
GraphSAGE
4
100.00%
97.81%
52 min
8
100.00%
99.38%
55 min
2) Classification Accuracy: The accuracy of the proposed
model (under various GNN layer types and connectivity) is
shown in Table II. We observe that using the GraphSAGE layer
as the GNN layer leads to the highest training/testing accuracy.
Using the GraphSAGE layer also leads to the lowest training
time. For the GraphSAGE layer, using 8-connectivity to build
the input graph can result in higher accuracy but slightly higher
training time than 4-connectivity. Table III shows that the
proposed GNN model achieves higher accuracy compared with
the state-of-the-art CNNs [11], [12], [14], [15] with negligible
computation complexity for inference.
TABLE III. COMPARISON WITH THE STATE-OF-THE-
ART CNNS ON MSTAR DATASET
Type
Accuracy
# of FLOPs
# of Para.
[11]
CNN
92.3%
1
12 √ó
0.5 √ó 106
[14]
CNN
97.97%
1
10 √ó
0.65 √ó 106
[15]
CNN
98.52%
1
3 √ó
2.1 √ó 106
[12]
CNN
99.3%
1√ó
(6.94 GFLOPs)
2.5 √ó 106
This work [after pruning]
(GraphSAGE layer,
8-connectivity)
GNN
99.1%
1
3000 √ó
0.03 √ó 106
TABLE
IV.
THE
IMPACT
OF
THE
ATTENTION
MECHANISM
(USING
GRAPHSAGE
LAYER
AND
8-CONNECTIVITY)
Vertex
Attention
Feature
Attention
Training
Accuracy
Testing
Accuracy
Training
Time
‚úó
‚úó
99.67%
93.77%
31 min
‚úó
‚úì
100.0%
98.51%
40 min
‚úì
‚úó
100.0%
99.26%
41 min
‚úì
‚úì
100.0%
99.38%
55 min
3) Ablation Study: We perform an ablation study to evalu-
ate the impact of the attention mechanism (using GraphSAGE
layer and 8-connectivity). The result is shown in Table IV.
Without vertex and feature attention, the model achieves
only 93.77% accuracy. With only vertex attention, the model
achieves 99.26% accuracy. With only feature attention, the
model achieves 98.51% accuracy. With both vertex and feature
attention, the model achieves 99.38% accuracy. The evaluation
result demonstrates that the attention mechanism can improve
classification accuracy without significantly increasing com-
putation complexity.
Training set
0
0.2
0.4
0.6
0.8
1
SAR signal magnitude
0
1
2
3
4
Occurence
10 5
Testing set
0
0.2
0.4
0.6
0.8
1
SAR signal magnitude
0
1
2
3
4
Occurence
10 5
Figure 6. The distribution of the SAR signal magnitude in the
training/testing set of MASTAR
4) Evaluation on the Pruning Strategy: We evaluate the
proposed input pruning and weight pruning strategies. We use
GraphSAGE layer and 8-connectivity as the setting of the
model.
28
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-079-7
GEOProcessing 2023 : The Fifteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

10-10
10-5
100
0
20
40
60
80
100
Accuracy
10-10
10-5
100
0
1
2
3
4
5
6
Number of Parameters
105
10-10
10-5
100
0.8
0.85
0.9
0.95
1
The pruning ratio
of weight matrices
Figure 7. Evaluation of proposed pruning strategy
Input Pruning: Figure 6 shows the data distribution of the
SAR signal magnitude of the image pixels in the training/test-
ing set. The SAR signal magnitude ranges from 0 to 16.
Since most pixels have a magnitude between 0-1, Figure
6 only shows the range 0-1. For experiment, we set the
pruning threshold Iv (See Section III-A) to be 0, 0.1, 0.2, 0.3
respectively. The image pixels that have a magnitude small
than Iv are pruned.
Weight Pruning: The weights in weight matrices can be
either negative or positive. We set the threshold Iw for weight
pruning (See Section III-B). The weights that have an absolute
value that is smaller than Iw are pruned. In the experiment,
we set Iw to be between 1 √ó 109 and 1 √ó 101.
The evaluation results for the pruning strategy are shown in
Figure 7. We have the following observations:
‚óè Without weight pruning, when Iv = 0.1, 93.4% input
vertices/pixels are pruned, the accuracy is dropped to
99.1%; when Iv = 0.2, 98.6% input vertices/pixels are
pruned, the accuracy is dropped to 98.5%; when Iv = 0.3,
99.1% input vertices/pixels are pruned, the accuracy is
dropped to 96.5%.
‚óè When weight pruning threshold Iw < 107, the accuracy
does not change w.r.t. to Iw. When Iw = 107, more than
95% weights are pruned. Therefore, most entries in the
weight matrices are redundant.
Therefore, by setting proper threshold Iv, Iw for input
pruning and weight pruning, most input pixels and weights
can be pruned without significantly dropping the accuracy.
Figure 7 shows the evaluation results for the pruning strategy,
97% weight entries are pruned, and the accuracy is 99.1%. By
skipping the computation for the pruned vertices and weights,
we can dramatically reduce the total computation complexity.
5) Experimental Setting: For ship discrimination dataset,
we follow the setting of [31] to conduct experiment for few-
shot learning. Since the ship discrimination is a binary class
task, the few-shot learning task can be formed as a 2-way-K-
shot-classification problem, where K = {1,2,..,10} denotes
the number of labeled training images for each class. We
train the model using the Adam optimization algorithm. The
training batch size is set as K
2 , and the learning rate is set as
0.001 ‚àó K. The L2 weight decay is set as 0.08.
6) Classification Accuracy: As shown in Figure 8, we
compare our accuracy with [31] (baseline) for the few-shot
learning on the ship discrimination dataset. Note that the
baseline [31] uses a convolutional neural network (CNN), and
the authors pretrained their CNN using the ship discrimination
dataset on the Electro-Optical (EO) domain. We do not pretrain
our network on any dataset. For various K, the proposed
model outperforms the baseline [31], which is a pretrained
deep CNN model.
0
2
4
6
8
10
K
80
85
90
95
100
Accuracy
Our Work
Baseline
Figure 8. The accuracy on the ship discrimination dataset
TABLE V. COMPARISON OF ACCURACY (%)
K
1
2
3
4
5
6
7
Baseline [31]
86.3
86.3
82.8
94.2
87.8
96.0
91.1
Our work
93.8
93.1
97.9
94.0
99.7
97.4
97.8
V. CONCLUSION AND FUTURE WORK
In this paper, we proposed a novel GNN-based approach
for SAR automatic target recognition. The proposed approach
uses the GNN layer as the backbone and uses the attention
mechanism to improve classification accuracy. We proposed
29
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-079-7
GEOProcessing 2023 : The Fifteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

pruning strategies, including input pruning and weight prun-
ing, to reduce the computation complexity. The evaluation
results on the MSTAR and ship discrimination datasets show
that the proposed model outperforms the state-of-the-art CNNs
in classification accuracy and computation complexity. In [32],
we designed a hardware accelerator for the proposed GNN
model. In the future, we plan to extend the proposed GNN
model to more SAR-related tasks, such as object detection.
ACKNOWLEDGMENT
This work is supported by the National Science Founda-
tion (NSF) under grants CCF-1919289 and OAC-2209563,
and the DEVCOM Army Research Lab (ARL) under grant
W911NF2220159.
REFERENCES
[1] L. Landuyt, A. Van Wesemael et al., ‚ÄúFlood mapping based on syn-
thetic aperture radar: An assessment of established approaches,‚Äù IEEE
Transactions on Geoscience and Remote Sensing, 2018.
[2] P. Zhan, W. Zhu, and N. Li, ‚ÄúAn automated rice mapping method based
on flooding signals in synthetic aperture radar time series,‚Äù Remote
Sensing of Environment, vol. 252, p. 112112, 2021.
[3] N. Li, Z. Guo et al., ‚ÄúCharacterizing ancient channel of the yellow river
from spaceborne sar: Case study of chinese gaofen-3 satellite,‚Äù IEEE
Geoscience and Remote Sensing Letters, vol. 19, pp. 1‚Äì5, 2021.
[4] T. Zhang, X. Zhang et al., ‚ÄúHyperli-net: A hyper-light deep learning
network for high-accurate and high-speed ship detection from synthetic
aperture radar imagery,‚Äù ISPRS Journal of Photogrammetry and Remote
Sensing, vol. 167, pp. 123‚Äì153, 2020.
[5] F. Bardi, W. Frodella et al., ‚ÄúIntegration between ground based and
satellite sar data in landslide mapping: The san fratello case study,‚Äù
Geomorphology, vol. 223, pp. 45‚Äì60, 2014.
[6] H. Septanto and O. Sudjana, ‚ÄúSimulation-based energy balance analysis
of sar micro-satellite,‚Äù in Sixth International Symposium on LAPAN-IPB
Satellite, vol. 11372.
International Society for Optics and Photonics,
2019, p. 113721W.
[7] Y. Yokota, Y. Okada et al., ‚ÄúNewly developed x-band sar system onboard
japanese small satellite ‚Äúasnaro-2‚Äù,‚Äù in Conference Proceedings of 2013
Asia-Pacific Conference on Synthetic Aperture Radar (APSAR).
IEEE,
2013, pp. 81‚Äì83.
[8] P. R. Akbar, H. Saito et al., ‚ÄúParallel-plate slot array antenna for
deployable sar antenna onboard small satellite,‚Äù IEEE Transactions on
Antennas and Propagation, 2016.
[9] K. Tanaka, H. Saito et al., ‚ÄúDevelopment of 1kw high power x-band sar
installed on small satellite for on-demand observation,‚Äù in Proceedings
of the International Astronautical Congress, IAC, 2018.
[10] M. Tsamsakizoglou, H. L¬®ofgren, and M. Gunnarsson, ‚ÄúMicrosatellite
power control and distribution unit for the innosat platform,‚Äù in E3S
Web of Conferences, vol. 16.
EDP Sciences, 2017, p. 18007.
[11] M. Zhang, J. An et al., ‚ÄúConvolutional neural network with attention
mechanism for sar automatic target recognition,‚Äù IEEE Geoscience and
Remote Sensing Letters, 2020.
[12] D. A. Morgan, ‚ÄúDeep convolutional neural networks for atr from sar
imagery,‚Äù in Algorithms for Synthetic Aperture Radar Imagery XXII,
vol. 9475.
SPIE, 2015, pp. 116‚Äì128.
[13] J. Hu, L. Shen, and G. Sun, ‚ÄúSqueeze-and-excitation networks,‚Äù in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2018, pp. 7132‚Äì7141.
[14] J. Pei, Y. Huang et al., ‚ÄúSar automatic target recognition based on
multiview deep learning framework,‚Äù IEEE Transactions on Geoscience
and Remote Sensing, 2017.
[15] Z. Ying, C. Xuan et al., ‚ÄúTai-sarnet: Deep transferred atrous-inception
cnn for small samples sar atr,‚Äù Sensors, vol. 20, no. 6, p. 1724, 2020.
[16] H. Zhu, X. Du, and Y. Yao, ‚ÄúConvsppis: identifying protein-protein
interaction sites by an ensemble convolutional neural network with
feature graph,‚Äù Current Bioinformatics, 2020.
[17] T. Zhao, Y. Hu et al., ‚ÄúIdentifying drug‚Äìtarget interactions based on
graph convolutional network and deep neural network,‚Äù Briefings in
bioinformatics, 2021.
[18] C. R. Qi, L. Yi et al., ‚ÄúPointnet++: Deep hierarchical feature learning on
point sets in a metric space,‚Äù Advances in neural information processing
systems, vol. 30, 2017.
[19] K. Xu, W. Hu et al., ‚ÄúHow powerful are graph neural networks?‚Äù arXiv
preprint arXiv:1810.00826, 2018.
[20] T. N. Kipf and M. Welling, ‚ÄúSemi-supervised classification with graph
convolutional networks,‚Äù arXiv preprint arXiv:1609.02907, 2016.
[21] W. L. Hamilton, R. Ying, and J. Leskovec, ‚ÄúInductive representation
learning on large graphs,‚Äù in Proceedings of the 31st International
Conference on Neural Information Processing Systems, 2017.
[22] P. VeliÀáckovi¬¥c, G. Cucurull et al., ‚ÄúGraph attention networks,‚Äù arXiv
preprint arXiv:1710.10903, 2017.
[23] M. Zhang and Y. Chen, ‚ÄúLink prediction based on graph neural net-
works,‚Äù Advances in Neural Information Processing Systems, 2018.
[24] R. Ying, J. You et al., ‚ÄúHierarchical graph representation learning with
differentiable pooling,‚Äù arXiv preprint arXiv:1806.08804, 2018.
[25] F. Wu and A. Souza, ‚ÄúSimplifying graph convolutional networks,‚Äù in
International conference on machine learning.
PMLR, 2019.
[26] H. Zhu, N. Lin et al., ‚ÄúTarget classification from sar imagery based
on the pixel grayscale decline by graph convolutional neural network,‚Äù
IEEE Sensors Letters, vol. 4, no. 6, pp. 1‚Äì4, 2020.
[27] M. Rahman, A. Azad et al., ‚ÄúTriple sparsification of graph con-
volutional networks without sacrificing the accuracy,‚Äù arXiv preprint
arXiv:2208.03559, 2022.
[28] H. Zhou, A. Srivastava et al., ‚ÄúAccelerating large scale real-time gnn
inference using channel pruning,‚Äù arXiv preprint arXiv:2105.04528,
2021.
[29] R. Tibshirani, ‚ÄúRegression shrinkage and selection via the lasso,‚Äù Jour-
nal of the Royal Statistical Society: Series B (Methodological), vol. 58,
no. 1, pp. 267‚Äì288, 1996.
[30] C. P. Schwegmann, W. Kleynhans et al., ‚ÄúVery deep learning for
ship discrimination in synthetic aperture radar imagery,‚Äù in 2016 IEEE
International Geoscience and Remote Sensing Symposium (IGARSS).
IEEE, 2016, pp. 104‚Äì107.
[31] M. Rostami, S. Kolouri et al., ‚ÄúDeep transfer learning for few-shot sar
image classification,‚Äù Remote Sensing, vol. 11, no. 11, p. 1374, 2019.
[32] B. Zhang, R. Kannan et al., ‚ÄúAccurate, low-latency, efficient sar auto-
matic target recognition on fpga,‚Äù in 2022 32nd International Conference
on Field-Programmable Logic and Applications (FPL), 2022, pp. 1‚Äì8.
30
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-079-7
GEOProcessing 2023 : The Fifteenth International Conference on Advanced Geographic Information Systems, Applications, and Services

