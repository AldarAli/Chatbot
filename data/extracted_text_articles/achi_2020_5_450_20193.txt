Rethinking the Fashion Show: 
A Personal Daily Life Show Using Augmented Reality
 
 
Shihui Xu1, Yuhan Liu1, Yuzhao Liu1, Kelvin Cheng2, Soh Masuko2, and Jiro Tanaka1 
 
1 Waseda University, Kitakyushu, Japan 
2 Rakuten Institute of Technology, Rakuten, Inc., Tokyo, Japan 
e-mail: shxu@toki.waseda.jp, liuyuhan-op@akane.waseda.jp, liuyuzhao131@akane.waseda.jp,  
kelvin.cheng@rakuten.com, so.masuko@rakuten.com, jiro@aoni.waseda.jp
 
 
Abstract—At present, most fashion shows are professional and 
exclusive in nature and are not easily accessible to the general 
public. Ordinary people are usually excluded from these events 
and have few chances to participate in fashion shows in their 
daily lives. In addition, setting up a fashion show can be 
frustrating because the garments for fashion shows are often 
specially designed and difficult to obtain for ordinary people. 
In this paper, we attempt to rethink the design of fashion 
shows and discuss how future fashion show could be 
redesigned. We proposed an Augmented Reality (AR) personal 
daily life show, which enables users to virtually attend a 
fashion show that is customized for themselves, and in their 
own environment. Personalized avatar is created for each user 
as a virtual fashion model in the fashion show. Furthermore, 
the avatar is enhanced with animations and can interact with 
physical objects in the real environment. We have conducted 
an evaluation and results show that such personal daily life 
show can improve user’s experience and narrow the gap 
between apparel show and user’s own life. 
Keywords-fashion show; Augmented Reality; personalized 
fashion. 
I. 
 INTRODUCTION  
Fashion show is a crucial part of the fashion industry and 
is defined as a showcase, presenting fashion designers' new 
collections of clothes and accessories to audiences of 
consumers, buyers, journalists, influencers, etc. [1]-[3]. The 
purpose of the fashion show is not only to exhibit fashion 
designers' collections to the public, but also to include its 
practicality as a distribution channel [1][4]. Since the first 
modern fashion show at Ehrich Brothers, New York City in 
1903, fashion shows have been adopted by upscale fashion 
brands as promotional events [3].  
Although fashion shows have superiority in distribution 
channel and advertising, they have their limitations. Firstly, 
consumers cannot participate at fashion shows themselves. 
Clothing is an involved product category, it is difficult to 
evaluate, and needs to be seen in person and tried on because 
it is highly related to personal ego [5]. Considering the 
purpose of fashion show is to present clothing to consumers, 
it is crucial to include general consumers in fashion shows so 
that they could see the clothing by themselves. Second, there 
is a gap between the current fashion show and daily life of 
the consumers. Current fashion shows prefer catwalks to 
showcase the apparels, which require specially built stages 
and are rarely seen in consumer’s daily life. In addition, 
apparels of fashion show are not widely applicable for 
general consumers. So that general consumers can hardly 
benefit from the current fashion show. 
Recently, fashion show has incorporated technologies 
such as Augmented Reality (AR) and Mixed Reality (MR), 
attracting the attention of more audiences. For example, UK 
wireless company Three teamed up with designers to show 
off a fashion show augmented with 3D special effects 
through network and AR head-mounted display at 2019 
London Fashion Week [6][7].  
However, these technologies are applied in merely eye-
catching strategies. The essential issue of how to effectively 
use these technologies to solve the problems of current 
fashion show to make fashion show a more extensive 
channel for general consumers has not been explored.  
Thus, to bring consumers into a completely new area of 
fashion show, it is important to rethink the design of fashion 
show and related technologies for meeting consumers' 
demands. Therefore, two research questions are proposed. 
First, how might we redesign the future fashion show? 
Second, what is the role of AR technology to facilitate 
consumers' shopping in the future fashion show?  
In this paper, we present a novel AR fashion apparels 
show system named personal daily life show to explore the 
above questions and to solve the existing problems of current 
fashion show. The novelties of the personal daily life show 
are: 
1. 
Generation of life-sized personalized 3-dimentional 
human avatar of the user self.  
2. 
3D apparel modeling from 2D shopping website 
images. 
3. 
Interaction of the personalized virtual avatar with 
the real physical environment. 
The contributions of this work include: 
1. 
The conceptual design and a proof-of-concept 
prototype implementation of personal daily life 
show. 
2. 
Quantitative and qualitative evaluation of personal 
daily life show and how it is effective in narrowing 
the gap between fashion show and user’s own life. 
399
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

The rest of this paper is organized as follows. Section II 
describes the related works on which our work is built. 
Section III describes the concept, design features and usage 
scenario of personal daily life show. Section IV goes into 
finer details with respect to the implementation of personal 
daily life show. Section V introduces the evaluation. Section 
VI gives the discussion of this work. The conclusions close 
the article. 
II. 
RELATED WORKS 
We have examined related works in terms of the AR and 
MR technologies in fashion industry and virtual avatar in 
fashion show. 
A. AR/MR Technologies in Fashion 
In fashion industry, the use of AR and MR technologies 
can be used to narrow the gap between online and brick-and-
mortar shopping experience [8]. For example, Virtual try-on 
system provides virtual garments trial experience for 
consumers through AR and VR technologies, overcoming 
the lack of fitting experience of online shopping [9]. The 
apparel company Gap created an app called DressingRoom 
[10], which allows customers to virtually try on clothing on a 
dummy human model through a smartphone. Combining 3D 
modeling technologies, the London College of Fashion has 
created an app named Pictofit [11], adopting virtual avatar of 
the users and enabling users to browse clothes with their own 
avatar in AR environment. In contrast with DressingRoom, 
in Pictofic consumers can use their own avatar to try on 
clothing, undergoing mental simulation of the fitting with the 
garments during the process, leading vivid mental imagery 
and high purchase intention. However, both DressingRoom 
and Pictofit are limited to the virtual static human model, 
lacking dynamic view of clothing try-on. Users can only see 
the standing virtual model, with no interaction other than 
changing clothes. 
The use of see-through type head-mounted displays 
(HMDs) with AR technology enables a more intuitive and 
immersive shopping experience compared to conventional e-
commerce systems. With spatial computing, AR has the 
potential to provide a sensory and immersive virtual 
boutique shopping experience wherever the user is. For 
example, H&Moschino and Warpin [12] created a virtual 
fashion shop using MR head-mounted display, where users 
were able to watch 3D videos and sounds surrounding 
different garments. Portal is an AR fashion show application 
launched by MESON [13], making it possible to watch a 
brand's new seasonal items fitted with hundreds of 3D virtual 
human models in real-world through smartphone, tablet or 
AR glasses. But Portal adopts professional models instead of 
consumers themselves, causing a low correlation with 
consumers. In addition, the models in Portal is static and 
clothes could not be changed. 
B. Avatar in Fashion Show 
Previous research has investigated the design and 
development of avatar in AR and VR system. Avatars have 
been widely used in most virtual worlds and games. An 
avatar represents a user self and it allows the user to 
experience the virtual world by manipulating the avatar 
[14][15].  
In this paper, we mainly focus on how people perceive 
their avatars in fashion show related system. For instance, 
Stephen Gray proposed a VR fashion show in which users 
can have a virtual model of the same shape as their own 
bodies [16]. The avatar can be fitted with virtual clothing and 
walk catwalks in a virtual environment. However, the avatar 
can only reflect the body shape of the user, without the face 
features of the user. Besides, the interactivity is limited. 
Users can only watch the performance of their avatar. 
Recently, virtual avatars have appeared in real-world fashion 
show using projection an AR technology. Central Saint 
Martins and Three launched a fashion show where a 3D life-
size avatar of a famous model could walk in AR on the 
runway at 2020 London Fashion Week [17]. The audiences 
can watch the catwalks of virtual avatar by smartphone. In 
this system, the avatar is embedded with the motion capture 
animations of model self. But it is generated by professional 
and expensive setup, so that there is only one avatar and one 
dress available.  
III. 
PERSONAL DAILY LIFE SHOW 
This section describes the concept and design of personal 
daily life show and the scenario of its usage. 
A. Concept 
One of our major design decisions was to not design for a 
runway catwalk fashion show. Instead, we focus on closing 
the gap between fashion show and the general consumers, 
bringing the fashion show into users' life and displaying the 
lifestyle of users. While we agreed that the runway catwalk 
fashion show has great commercial value for fashion 
industry, we decide to design a fashion show from which 
consumers can directly benefit. In our concept, we focused 
on making the fashion show available for users anywhere in 
their life, which means users can experience a fashion show 
in their real environment. 
Another major design decision was to let the user 
participate in the show instead of just being an audience. In 
order to allow users to see their own performances while 
participating in the show, we adopted users' personalized 
avatar as the show model. We think it is important to allow 
users to engage with the show, which could increase the 
engagement and enjoyment of users. 
Based on above decisions, we defined the personal daily 
life show enabling general consumers to watch the fashion 
apparels on their own life-sized personalized avatar, which is 
overlaid on user's real environment and can interact with the 
real environment using AR technology. Furthermore, instead 
of having models only walking and posing, as in current 
fashion shows, our personalized avatar can perform human-
like daily activities, such as walking around in the real-world, 
and interacting with real-world objects. A main consideration 
for the design of personal daily life show was to depict the 
scene of user's daily life, increasing the self-relevance of 
users during the show experience and facilitating consumers 
to imagine themselves fitted with fashion apparels in their 
own life.  
400
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

B. Design 
Based on the concept proposed above, we designed a 
proof-of-concept prototype of personal daily life show for 
further investigation. Figure 1 shows the pipeline of the 
personal daily life show. 
 
 
Figure 1. Pipeline of personal daily life show 
The novelties and features of personal daily life show 
include the following items. 
1) Adopting users' personalized avatar as models in the 
show. 
In order to create a fashion show with the user, while 
enabling the user to watch the show at the same time, we 
propose to make a life-sized personalized avatar for each 
user and adopt the user’s personalized avatar as the model of 
the fashion show. Thereby, users can watch the fashion 
apparels exhibited on their own personalized avatar and get 
the imagery of trying on the apparels on themselves. The 
personalized avatar is customized according to photos of 
user's body and facial appearance.  
2) Providing virtual 3D apparel models based on 2D 
apparel images from shopping website. 
The apparels of fashion show are usually difficult to 
obtain and purchase. To solve this problem, we provide users 
with 3D virtual apparels so that they could virtually fit the 
apparels on their personalized avatar. In addition, the 3D 
virtual apparels are generated according to the 2D apparels 
images from online shopping websites. In this way, users can 
easily purchase the apparels that appears in the fashion show. 
Besides, if they are satisfied with the apparel, they can 
purchase it using online shopping.  
3) Enabling user’s personalized avatar to interact with 
real environment using AR technology. 
To close the gap between fashion show and user’s daily 
life, we made use of AR technology to visualize and 
superimpose the personalized avatar in user’s real 
environment. And users can make the user’s personalized 
avatar interact with the real environment, such as walking 
and doing daily activities like a real person. By doing so, we 
intend to simulate the daily life of the user, giving user a 
fashion show experience that is associated with their own life 
and providing the user further information about how they 
will look like in their daily life. 
C. Scenario 
We propose a usage scenario illustrating how the 
personal daily life show can benefit the general consumers 
when online apparels shopping. And at the same time, users 
can also browse the clothes of online shopping website in a 
more intuitive and engaged way by the personal daily life 
show system. 
When the user uses the system for the first time, before 
the user starts shopping on the apparels website, she can 
upload the images of her face and body to create a 
personalized avatar which reflects the face appearance and 
body shape of the user. 
The user can then browse the shopping website of 
apparels to select the clothes she wants to view. Our system 
will generate the 3D model of the clothes selected by the 
user. Next, our system will provide a personalized avatar of 
herself fitted with the 3D clothes model to the user. 
By wearing a see-through type AR head-mounted display 
(HMD) with a depth camera, users can launch the show 
where they are. Once the show is launched, the real-world 
environment of the user will be mapped by the HMD using 
the built-in depth camera. After that user can see the life-
sized personalized avatar of herself superimposed onto the 
real environment through the HMD.  
The user can interact with the system to make the 
personalized avatar interact with the real environment, 
similar to what a real person would do in their daily activities.  
IV. 
IMPLEMENTATION 
In this section, the implementation details of the personal 
daily life show will be described. The implementation 
contains the hardware, development environment, pre-
processing and daily life show.  
A. Hardware 
The prototype system was built with a see-through type 
AR HMD, Microsoft HoloLens, with a 2GB CPU embedded 
[18]. HoloLens features four "environment understanding" 
sensors, a depth camera with a 120 * 120-degree angle of 
view, which is used for computing the real-world meshes in 
our system. 
B. Development Environment 
The software was developed using the Unity 3D game 
engine (2017.4). And a cross-platform toolkit for building 
Mixed Reality experiences for Virtual Reality (VR) and 
Augmented Reality (AR), MRTK, was used for building the 
application. This framework allowed us to rapidly prototype 
the personal daily life show system that supports spatial 
awareness and various interaction cues. 
C. Pre-processing 
After the pre-processing, a dressed personalized avatar 
with motions embedded will be available, which can be used 
in the fashion show as an apparel model. 
1) Generation of personalized avatar 
Figure 2 shows the process of the personalized avatar 
generation.  
401
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

Our approach to generate the personalized avatar is to 
model the user's face and body separately and combine the 
two parts together. For the face modeling, our approach only 
uses a single frontal image of the user's face. By making use 
of existing face modeling technology, Avatar SDK [19], the 
photorealistic face model can be generated. 
With body modeling, a front image and a side image of 
the user's body are required to build a body model of the user. 
By uploading the body images to 3DLOOK [20], the body 
model of the user can be generated. 
The face model and body model are combined by 
replacing the head part of generated body model with the 
face model. This is done in a 3D modeling software.  
 
 
Figure 2. Generation of personalized avatar 
2) Generation of 3D apparel model 
Methods used to generate garment models from images 
are widely used in virtual try-on systems. Our aim is to 
handle the clothe images from online apparel shopping 
websites. However, it is difficult to process all the images on 
such large collections. Therefore, we focused on dealing with 
simple clothes images that have a unique or similar 
background color. 
Our approach to generate garment model is to create 3D 
garment templates for each personalized avatar and map the 
2D garment images to the garment templates, which are 
created from online apparel websites. The 3D garment 
templates are created for each personalized avatar using 
Cloth Weaver. The 2D garment images are then mapped 
onto the generated 3D garment model templates in 3ds Max. 
In doing so, 3D garment models can be created that matches 
online shopping website images. 
3) Fitting apparel model to personalized avatar 
With the generated 3D garment model, we fitted the 
garment model to user’s personalized avatar by adjust the 
size and position of the 3D garment model within 3ds Max, 
which result in a dressed personalized avatar. 
4) Attaching motions to personalized avatar  
By embedding motions to the personalized avatar of 
users, they can be animated to mimic how we would normal 
move about in our daily activities. In order to embed motions 
to the personalized avatar, the personalized 3D avatar is 
rigged as a humanoid. We then select motion capture 
animations that mimics human actions, and attach them to 
the rigged personalized avatar. 
a) Motion capture animations 
Motion capture animations are the animation clips which 
record the movement of people or objects, widely used in 
games and movies. To make the personalized avatar act 
naturally and closer to real life, we chose to prepare motion 
capture animations that can reflect human’s daily activities, 
such as sitting, walking, and standing. 
The motion capture animations used in our system were 
sourced from Mixamo [21]. 
b) Rigging of Personalized Avatar 
Rigging is the process of creating a skeleton for a 3D 
model so it can animate. Before the motion capture 
animations can be used, the personalized avatar need to be 
rigged.  
The rigging process can be done in a 3D modeling 
software like 3ds Max or Maya, but it is time consuming 
using this method. Auto-rig software is currently available, 
which enables users to rig a 3D character by assigning 
several joints. We used the auto-rig function of Mixamo to 
rig the personalized avatar in our system, as the Figure 3 
shows. By aligning several joints (i.e., chin, two elbows, two 
wrists, two knees and groin) to the 3D personalized avatar, 
Mixamo will help do the rigging process automatically. We 
can also choose different levels of skeleton details to 
optimize the performance of our personalized avatar.  
 
 
Figure 3. Rigging of the personalized avatar 
c) Attaching the motion capture animation to rigged 
personalized avatar. 
After rigging, we attached the prepared motion capture 
animation to the personalized avatar via Unity Animator 
Controller, which allows humanoid models to utilize 
multiple kinds of motion capture animations. Figure 4 shows 
a few examples of personalized avatar with various kinds of 
motions. 
 
Figure 4. Personalized avatar with motions 
402
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

D. Daily Life Show 
The implementation of daily life show includes the 
avatar’s movement around the real environment and avatar’s 
interaction with real objects. 
1) Movement around the real environment 
In this section, the implementation of the avatar’s 
movement around the real environment is explained. As 
shown in Figure 5, the implementation of avatar’s movement 
includes specifying the destination, calculation of distance 
between avatar and destination, and avatar’s response to the 
environment (walking or standing). 
 
Figure 5. Avatar’s movement around the real environment 
a) Specifying the destination 
The movement of avatar is triggered by user’s specifying 
a destination on the floor by performing an “air-tap”. "Air-
tap" is a built-in gesture provided by HoloLens, which refers 
to straightening the index finger and then tapping down. The 
users can gaze on the floor and make an "air tap" gesture 
towards the floor. A virtual arrow will then appear as the 
indicator of destination of the movement.  
b) Calculation of distance between avatar and 
destination 
We calculate the distance between the avatar’s current 
position and the destination at every frame. The calculation 
is done by updating the position information of the avatar. 
c)  Response of avatar 
If the distance be between avatar and destination is larger 
than the pre-determined threshold value, which means the 
avatar need to move, we set the motion of the avatar to be 
“walking” motion. This is one of the animations that is 
already embedded in the avatar in pre-processing, and which 
sets the avatar in motion towards the destination. In this case, 
the avatar appears as walking towards the destination.  
If the distance between avatar and destination is less than 
the threshold value, the position of avatar will not be 
changed. However, the avatar will perform the standing 
motion, which is also embedded in avatar in pre-processing. 
2) Interaction with real objects 
The implementation of avatar’s interaction with real 
objects is illustrated in Figure 6. It consists of specifying real 
object for interaction, object detection in AR, and avatar’s 
interaction with real object. 
 
Figure 6. Avatar’s interaction with real objects 
a) Specifying real object for interaction 
Users can specify which real object to interact by "double 
taps" to trigger the personalized avatar's interaction with real 
objects. "Double taps" means performing the "air-tap" 
gesture twice. The users can make the virtual user interact 
with real object by gazing at the real objects and performing 
the "double taps" gesture towards the real object. When the 
"double taps" gesture is recognized by HoloLens, it captures 
a screenshot of current view of real world and record the 
current position of the cursor for next step’s object detection 
in AR. However, the limitation of "double taps" is that 
sometimes it will be misidentified as "air-tap" gesture 
because of the limited recognition speed of HoloLens. 
b) Object detection in the AR environment. 
In this step, we recognize the type of real object to 
interact with and compute the 3D position of the real object 
in the AR environment. To recognize the objects in the AR 
environment, we first recognize the objects’ type by using 
2D images object detection model, then calculate the 3D 
position of real object in AR environment.  
Before object detection in runtime, we first train the 
model used to recognize the objects in 2D images. As a 
proof of concept, we chose several everyday objects, i.e., 
chair, bed, sports mat, computer. For each real object, we 
captured 50 images from multiple perspective as the 
database to train model. The images were uploaded to Azure 
Custom Vision and trained on webserver. After training, we 
can publish the model and can access it through APIs. 
At runtime, after user performs double clicks, the 
captured 2D images of real-world view will be sent to the 
server for 2D object detection processing using Azure 
Custom Vision APIs. From this, we can get the type of 
object and its 2D position in the image. With the recorded 
3D cursor position combined with detected 2D position from 
captured image, we could calculate the 3D position of the 
real object in AR with depth information through 
Camera.ViewportPointToRay API. After getting the type 
and position of the real object in AR, our system could draw 
a cube with a type label in the AR environment to 
demonstrate object detection results for the users. For 
example, Figure 6 shows an example where a chair is 
detected, in which case a cube with a “chair” label is drawed 
at the position of the chair. 
c) Avatar’s interaction with real object. 
Knowing the 3D position and type of the real object, the 
avatar is then able to interact with the real object.  
The type of real object that is recognized determines the 
type of motion that the avatar should perform. Different 
kinds of real object correspond to different kinds of motion. 
For instance, chair type objects will trigger the personalized 
403
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

avatar to do a sitting motion, whereas bed type objects will 
trigger the personalized avatar to perform a laying down 
motion. The position of the personalized avatar is determined 
by the distance between the real object and the current 
position of the personalized avatar. If this distance is less 
than a threshold value, the personalized avatar’s position will 
be unchanged. If the distance is more than the threshold 
value, we set the position of the personalized avatar to the 
same position as the real object.  
 
Figure 7. Personal daily life show. (a) Walking in real environment. (b) 
Standing in real environment. (c) Sitting on a chair. (d) Typing on a 
computer. (e) Doing sit-ups on a sports mat. (f) Laying on a bed. 
With the movement and interaction of avatar, the users 
can conduct a fashion show in their real environment. Figure 
7 shows a system image of personal daily life show. 
Compared with traditional fashion show, the personal daily 
life show uses an avatar of the user self rather than a live 
fashion model. In addition, instead of walking on the 
catwalks and making poses on the runway, the avatar can 
move and perform daily activities within user’s physical 
environment.  
V. 
EVALUATION 
To assess our system, we conducted a user study in terms 
of quantitative and qualitative aspects. As previous work has 
already verified that the personalized avatar has positive 
effect on user's evaluation towards shopping experience, we 
will not re-evaluate personalized avatar redundantly. Instead, 
we are interested in evaluating the impact of interactivity of 
virtual avatar in AR fashion show system. 
A. Experimental Design 
The experiment follows a one-factorial design. In this 
case, our independent variable is the interactivity level of 
personalized avatar of user: static, in-situ animated and real-
world-interactive. The conditions are: 
1. 
Static personalized avatar: users can watch a static 
standing personalized avatar of themselves. 
2. 
In-situ animated personalized avatar: the personalized 
avatar can make different animations on the same spot 
in front of the user, such as walking, sitting, typing, 
doing sit-ups. User can change the animation of 
personalized avatar. 
3. 
Real-world-interactive 
personalized 
avatar: 
the 
personalized avatar can move around the real 
environment and make interaction with real objects, 
such as sitting on a chair, typing on a computer, doing 
sit-ups on a sports mat and so on.  
B. Experiment Environment and Set-up 
We set up our experiment in an office room with 
accommodation 
appliances, 
including 
tables, 
chairs, 
computers and a sports mat. The area of the room is about 50 
square meters. 
The device used in the experiment is see-through type 
head-mounted display HoloLens. 
C. Participants 
Twelve female students from a graduate school were 
recruited in the experiment. We focused on women 
participants because previous work [22] had examined 
women represent the largest apparel segment. All the 
participants had knowledge about fashion and had 
purchasing experience online, and 10 out of 12 had AR 
experience and learnt about human-computer interaction. 
D. Task and Procedure 
Before the experiment began, to reduce the impact 
caused by the unfamiliarity of device and hand gestures, each 
participant was required to be trained to use HoloLens and 
learn the basic gestures used in HoloLens. After training, 
each participant was informed about the experiment contents 
by a short video with explanations. 
The experiment was a within-subject design. Each 
participant was asked to experience the fashion show of the 
personalized avatar using all three conditions. During each 
condition, each participant had 10 minutes to browse 20 
apparel items. The clothes types were T-shirt, shirt, oversize 
T-shirt, skirt, tight skirt and pants. Participants were able to 
browse the next item by clicking on the personalized avatar. 
TABLE I.  
QUESTIONNAIRE FOR QANTITATIVE EVALUATION 
No. 
Question 
Aspect 
Description 
1 
Likeness of avatar 
I feel the avatar looks like me. 
2 
Affection to avatar 
I like the avatar. 
3 
Interest 
I think the show is interesting. 
4 
Engagement 
I feel engaged in the show. 
5 
Future application 
I would like to use the system in the 
future. 
6 
Assistance 
I think the system is helpful for 
viewing apparels. 
7 
Relevance to real 
life 
I feel the show is close to my life. 
 
To minimize the effect of learning and transfer across 
treatments, we randomly assigned the test order and apparel 
items of the 3 conditions for each participant.  
During the experiment, participants were asked to report 
their thoughts and feelings through think-aloud protocol. 
After completing all three conditions, each participant 
was asked to complete a questionnaire to evaluate the three 
conditions in terms of likeness of the personalized avatar, 
affection to the personalized avatar, interest, engagement, 
future application, assistance and relevance to users’ real life 
404
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

using 7-point Likert scale. The questions are described in 
Table 1. 
Afterwards, participants were interviewed about our 
system. The whole experiment was captured by voice 
recording and dictation notes. 
E. Results 
From the experiment, we were able to collect two kinds 
of data, quantitative data from the questionnaires and 
qualitative data from the transcriptions of think-aloud 
sessions and interviews. 
We conducted quantitative analysis for the former data 
and qualitative analysis for the latter data, which are 
described in detail below. 
1) Result 1: Quantitative Analysis 
In quantitative analysis, we aimed to examine if there 
was a significant difference between users’ evaluation of  
TABLE II.  
RESULTS OF ANOVA 
 
each condition and the way in which these differences could 
be explained. As each participant tested all three conditions 
(within-subjects design), we analyzed the quantitative data 
using a one-way repeated measures ANOVA, comparing the 
effect of each condition on the user experience of the 
interface. We also conducted a Tukey’s HSD test as a post-
hoc test for pairwise comparisons. 
The one-way ANOVA with repeated measures revealed 
significant effects of conditions on users’ ratings on user 
experience except for the likeness of avatar. TABLE II 
shows the results of ANOVA. Based on the results, we 
further conducted Tukey’s HSD test to investigate pairwise 
comparisons between each condition. The results are 
visualized in Figure 8. We categorized the results in terms of 
the main issues below.  
a) There is no significant influence of interactivity on 
the perceived likeness of the avatar, but the users tend to 
adore the real-world-interactive avatar (Q1, Q2).  
We tested interactivity level’s influence on users’ 
ratings of personalized avatar. From the comparisons, even 
though participants evaluated that the perceived likeness of 
the personalized avatar was not related to the interactivity 
level, we observed that the participants tended to prefer real-
world- interactive personalized avatar to the other two 
 
Figure 8. Visualization of quantitative analysis. (***p<0.001, **p<0.01, 
*p<0.05) 
conditions (static personalized avatar and in-situ animated 
personalized avatar). The received scores of real-world-
interactive avatars were significantly higher than static avatar 
(p = 0.016). The results showed that there was a trend that 
participants adored the personalized avatar with high 
interactivity level, especially the real-world-interactive 
avatar. 
b) Real-world-interaction 
positively 
affected 
participants’ interest and engagement (Q3, Q4).  
One of the most interesting results of the analysis was 
that the user experience of the apparels show could be 
significantly improved by real-world-interactive personalized 
avatar, while only adding daily life motions to the 
personalized avatar was not obviously effective. We 
evaluated user experience from participants’ interest and 
engagement, with significant differences found in both items. 
From a post-hoc test, we noticed the condition with real-
world-interaction was distinct from the other two conditions. 
In the case of interest, the real-world-interactive condition 
showed significantly higher scores than static condition (p < 
0.001) and in-situ animated condition (p = 0.029). In the case 
of engagement, participants felt more engaged in condition 
with real-world-interactive personalized avatar than static 
personalized avatar (p < 0.001) and in-situ animated avatar 
(p = 0.006). Besides, the post-hoc test suggested that 
participants did not perceive differences between static and 
in-situ animated conditions in terms of interest (p = 0.188) 
and engagement (p = 0.062). 
c) Showing apparels using avatar with daily life 
motions is more helpful and more likely to be chosen for 
future use (Q5, Q6). 
We also evaluated the effectiveness in terms of assistance 
and future application. The comparisons revealed that 
participants inclined to the conditions with motions, whether 
in-situ animated or real-world-interactive. From the 
assistance aspect, participants evaluated the condition with 
static personalized avatar as least helpful, compared with  
condition with real-world-interactive avatar (p = 0.011) 
and condition with in-situ animated avatar (p = 0.036). And 
from the aspect of future application, the post-hoc test 
showed that participants were more pleased to choose 
Aspect 
ANOVA 
F-value 
p-value 
Likeness of avatar 
0.084 
0.919 
Affection to avatar 
4.333 
0.021* 
Interest 
10.164 
<0.001*** 
Engagement 
16.148 
<0.001*** 
Future application 
6.547 
0.004** 
Assistance 
5.481 
0.009** 
Relevance to real life 
8.589 
0.001** 
405
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

condition with real-world-interactive avatar (p = 0.005) and 
condition with in-situ animated avatar (p = 0.020) for future 
purchase of apparels than condition with static avatar. There 
was no significant preference between in-situ animated 
condition and real-world-interactive condition (p = 0.864), 
and both are likely to be adopted by the participants in the 
future. 
d) Daily life motions can bring the feelings of 
participants’ own life (Q7). 
We identified that the conditions with real-world-
interactions received higher scores in the relevance of 
participants’ own life. The comparisons showed that 
participants felt the fashion show was significantly closer to 
their own daily life when the personalized avatars were 
enabled to interact with the real environment, compared with 
the static avatar (p = 0.001). Even though there is no 
significant difference between the condition with in-situ 
animated avatar and the condition with statically standing 
avatar, the received average score of the former condition 
was higher than the latter condition. This trend could also 
imply the daily life motions have a positive influence on 
user’s perception about the relevance of the show and their 
own life, especially when the daily life motions are 
companied with interactions with the real-world. 
2) Result 2: Qualitative Analysis                                                                                                                                                                     
The qualitative data from the think-aloud sessions and 
interviews were transcribed and analyzed. In the qualitative 
analysis, we aimed to investigate the users’ thoughts in more 
depth and discover the hidden characteristics beyond the 
quantitative analysis. The collated qualitative data is shown 
in TABLE III. 
a) Avatar's interaction with real environment is 
important, especially the movement around the real 
environment. 
As seen in the quantitative analysis, we also verified the 
importance of enabling avatar to interact with the real world 
in the qualitative analysis. Participants said that the 
personalized avatars’ interactions with real environment 
provided imagery of their own daily activities. For example, 
P8 said, “Looking my avatar walking around the room 
makes feel like walking by myself.” P2 also mentioned, “My 
avatar is typing on my computer, just like that I usually do in 
my office.” Making the avatar move around freely in the real 
environment is crucial because it could facilitate users to 
view avatar’s performance from different perspectives 
without having to move. P7 said, “I could imagine myself 
walking back and forth when instructing my avatar to walk. 
But I do not need to actually move at all.” P11 also said, 
“With the movement of avatar, I can observe the apparels 
from different degrees of view while just standing still at a 
point.” 
b) Users enjoy engaging with the show, rather than 
just watching the show as audience. 
TABLE III.  
QUALITATIVE RESULTS 
 
From the think-aloud process, we observed that 
participants were eager to interact with the show. Insufficient 
interactions may make users feel bored during the show. For 
example, P12 said, “I feel the time is too long when I can 
just look at a static model.” P11 also said, “I want to leave 
this session (static avatar) and go to next one.” We also 
found that adding user interaction to the show could bring 
hedonic value. Participants tended to seek fun during the 
interactive experience. P9 reported, “I feel enjoyable when I 
can interact with the show, and this kind of experience is like 
playing a game.” 
c) The interruptive interaction interface may hinder 
the performance of the real-world-interactive avatar. 
Despite verifying the real-world-interactive avatar in the 
apparels show, participants’ evaluations to the interaction 
were heavily influenced by the interaction interface. For 
example, participants were confused with the two kinds of 
hand gestures, leading to undesired effects. During the 
experiment, P3 said, “I want my avatar to do sports, but she 
just walks away.” The reason was that P3 used the wrong 
gesture. And in some cases, the time to wait for the avatar to 
respond could be longer than expected due to network delay. 
For instance, P4 said, “I feel hard to let the avatar sit on the 
chair. It did not respond. Is there something wrong?” 
Because of the above reasons, some participants felt difficult 
to make the avatar interact with the real environment, so that 
they could hardly perceive the hedonic value and practical 
value of the real-world-interaction.  
VI. 
DISCUSSION 
In this section, we summarize our findings of the study 
and discusses its implications for future fashion show design 
in general. We also report the plans of our future work as 
well as the limitations of current study. 
Keyword 
Description 
Avatar’s 
interaction 
“Looking my avatar walking around the room 
makes feel like walking by myself.” 
“My avatar is typing on my computer, just like that 
I usually do in my office.” 
“I could imagine myself walking back and forth 
when instructing my avatar to walk. But I do not 
need to actually move at all.” 
“With the movement of avatar, I can observe the 
apparels from different degrees of view while just 
standing still at a point.” 
Engageme
nt with the 
show 
“I feel the time is too long when I can just look at a 
static model.” 
“I want to leave this session (static avatar) and go 
to next one.” 
“I feel enjoyable when I can interact with the show, 
and this kind of experience is like playing a game.” 
Interface 
“The ‘click’ is easy to use.” 
“Using gestures is very interesting.” 
“I want my avatar to do sports, but she just walks 
away.” 
“I feel hard to let the avatar sit on the chair. It did 
not respond. Is there something wrong?” 
406
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

A. Avatar for apparels exhibition should be embedded 
with motions, and interaction with real world is 
preferable. 
As we have seen in both the quantitative and qualitative 
results, users prefer their personalized avatar animated. 
Avatars that are embedded with motions attract more 
affection from users, which may indirectly increase the 
fondness of using the whole system. Especially where 
motions of the avatar are related to the real environment, 
users show an obvious preference for the avatar. Furthermore, 
the real-world-interaction can close the gap between virtual 
apparels and users’ own real life. 
Therefore, when designing avatar for apparels exhibition, 
it is important to consider including real-world-interaction 
and animated avatar. When it is hard to enable the avatar to 
interact with real environment, having only in-situ animation 
is also effective to a certain degree.  
B. Let users interact with the show while avoiding 
complicated interactions 
From our analysis, we discovered that users are eager to 
participate in the show, such as interacting with the avatar 
and the environment. Users may feel tedious if there is no 
way to interact. Enabling users to participate more with the 
show, for instance manipulating and controlling the avatar, 
would improve the show experience. 
However, complicated interaction should be avoided in 
order to prevent users getting confused. When users are stuck 
by the complex interaction interfaces, they tend to give up, 
they would usually lack confidence to try again. Despite 
adding interactions is crucial, the interface should be 
designed very carefully. Simple and user-friendly interaction 
interfaces are necessary. Complicated interaction interfaces 
will hinder the benefit of interaction. 
C. Limitations and Future Work 
As a proof-of-concept, we designed only a limited set of 
real-world-interaction, which cannot represent all the daily 
activities in users’ daily life. 
As future work, we will continue to include more real-
world-interactions and investigate the interaction interface in 
the AR fashion apparel show to coordinate the interaction 
among users, virtual avatars and real environment. We are 
also interested in making the avatars aware of the real 
environment and react to it based on the user’s behaviors. 
VII. CONCLUSION 
In this paper, we proposed a novel fashion show system, 
personal daily life show, using AR technology. It has three 
key features: (1) Adopting users' personalized avatar as the 
fashion show model (2) Providing virtual 3D apparel models 
based on 2D apparel images from online shopping website (3) 
Enabling user’s personalized avatar to animate and interact 
with the real-world environment using AR technology.  
We conducted an evaluation using both quantitative and 
qualitative analysis to verify our system. The results showed 
that the real-world-interaction of personal daily life show can 
significantly improve the experience of the show and narrow 
the gap between user’s real life and the show.  
REFERENCES 
[1] 
K. W. Lau and P. Y. Lee, “The role of stereoscopic 3D virtual reality 
in fashion advertising and consumer learning,” Advances in 
Advertising Research, vol. 4, pp. 75–83, 2015. 
[2] 
Fashion show. https://en.wikipedia.org/wiki/Fashion_show. Accessed 
17 October 2020. 
[3] 
V. Pinchera and D. Rinallo, “Marketplace icon: the fashion show,” 
Consumption Markets & Culture, vol. 0, pp. 1-13, 2019. 
[4] 
S. Majima, “From   haute   couture   to   high   street: the   role   of   
shows   and   fairs   in twentieth-century fashion,” Textile History, vol. 
39, pp. 70-78, 2008. 
[5] 
A. K. Kau, Y. E. Tang, and S. Ghose, “Typology of online shoppers,” 
Journal of consumer marketing, vol. 20, pp. 139–156, 2003. 
[6] 
Magic leap & three UK power 5G Augmented Reality fashion show 
at 
London 
Fashion 
Week. 
https://magic-
leap.reality.news/news/magic-leap-three-uk-power-5g-augmented-
reality-fashion-show-london-fashion-week-0193914/. Accessed 17 
October 2020. 
[7] 
Magic Leap brings mixed reality to the catwalk for London Fashion 
Week. https://www.wareable.com/ar/magic-leap-fashion-show-gerrit-
jacob-london-fashion-week-6987. Accessed 17 October 2020. 
[8] 
M. Blázquez, “Fashion shopping in multichannel retail: the role of 
technology in enhancing the customer experience,” International 
Journal of Electronic Commerce, vol 18, pp. 97-116, 2014. 
[9] 
H. Lee and Y. Xu, “Classification  of  virtual  fitting  room  
technologies  in  the  fashion  industry:  from  the perspective of 
consumer experience,” International Journal of Fashion Design, 
Technology and Education, vol. 13, no. 1, pp. 1-10, 2020. 
[10] Gap tests new virtual dressing room. https://www.gapinc.com/en-
us/articles/2017/01/gap-tests-new-virtual-dressing-room. Accessed 17 
October 2020. 
[11] Pictofit. https://www.pictofit.com/. Accessed 17 October 2020. 
[12] H&Moschino 
AR 
Fashion 
Experience 
by 
Warpin. 
https://www.youtube.com/watch?v=vB22CQMfsOs. 
Accessed 
17 
October 2020. 
[13] PORTAL 
with 
Nreal. 
https://www.youtube.com/watch?v=2fVh9u8RBXI&list=PL-
VKm55vWiVZXQWOMsXbuP7dsJ8SW_Fdm&index=5&t=0s. 
Accessed 17 October 2020. 
[14] N. Ducheneaut, M. H. Wen, N. Yee, and G. Wadley, “Body and mind: 
a study of avatar personalization in three virtual worlds,” In 
Proceedings of the SIGCHI Conference on Human Factors in 
Computing Systems (CHI '09) ACM, April 2009, pp. 1151–1160. 
[15] R. Schroeder, “The social life of avatars: presence and interaction in 
shared virtual environments,” Springer Science & Business Media, 
2012. 
[16] S. Gray, "In virtual fashion," in IEEE Spectrum, vol. 35, no. 2, pp. 
18-25, Feb. 1998. 
[17] Fashion fuelled by 5G. http://www.three.co.uk/hub/fashion-fuelled-
by-5g/. Accessed 17 October 2020. 
[18] Microsoft 
HoloLens. 
https://en.wikipedia.org/wiki/Microsoft_HoloLens. 
Accessed 
17 
October 2020. 
[19] Avatar SDK. https://avatarsdk.com/. Accessed 17 October 2020. 
[20] 3DLOOK. https://3dlook.me/. Accessed 17 October 2020. 
[21] Mixamo. https://www.mixamo.com/. Accessed 17 October 2020. 
[22] A. Merle, S. Senecal, and A. S. Onge, “Whether  and  how  virtual  
try-on  influences  consumer responses  to  an  apparel  web  site,” 
International Journal of Electronic Commerce, vol. 16, no. 3, pp. 41-
64, April 2012. 
 
407
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

