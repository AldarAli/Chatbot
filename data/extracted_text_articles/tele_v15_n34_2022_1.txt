Reliability of Erasure-Coded Storage Systems with
Latent Errors
Ilias Iliadis
IBM Research Europe – Zurich
8803 R¨uschlikon, Switzerland
email: ili@zurich.ibm.com
Abstract—Large-scale storage systems employ erasure-coding re-
dundancy schemes to protect against device failures. The adverse
effect of latent sector errors on the Mean Time to Data Loss
(MTTDL) and the Expected Annual Fraction of Data Loss
(EAFDL) reliability metrics is evaluated. A theoretical model
capturing the effect of latent errors and device failures is
developed, and closed-form expressions for the metrics of interest
are derived. The MTTDL and EAFDL of erasure-coded systems
are obtained analytically for (i) the entire range of bit error rates,
(ii) the symmetric, clustered, and declustered data placement
schemes, and (iii) arbitrary device failure and rebuild time
distributions under network rebuild bandwidth constraints. For
realistic values of sector error rates, the results obtained demon-
strate that MTTDL degrades whereas, for moderate erasure codes,
EAFDL remains practically unaffected. It is demonstrated that,
in the range of typical sector error rates and for very powerful
erasure codes, EAFDL degrades as well. It is also shown that the
declustered data placement scheme offers superior reliability.
Keywords–Storage; Unrecoverable or latent sector errors; Reli-
ability analysis; MTTDL; EAFDL; RAID; MDS codes; stochastic
modeling.
I.
INTRODUCTION
Today’s large-scale data storage systems and most cloud
offerings recover data lost due to device and component
failures by deploying efficient erasure coding schemes that
provide high data reliability [1]. The replication schemes and
the Redundant Arrays of Inexpensive Disks (RAID) schemes,
such as RAID-5 and RAID-6, which have been deployed
extensively in the past thirty years [2-5] are special cases of
erasure codes. Modern storage systems though use advanced,
more powerful erasure coding schemes. The effectiveness of
these schemes has been evaluated based on the Mean Time to
Data Loss (MTTDL) [2-11] and, more recently, the Expected
Annual Fraction of Data Loss (EAFDL) reliability metrics [12-
16]. The latter metric was introduced, because Amazon S3
[17], Facebook [18], LinkedIn [19] and Yahoo! [20] consider
the amount of lost data measured in time.
The reliability level achieved depends not only on the
particular choice of the erasure coding scheme, but also on
the way data is placed on storage devices. The reliability
assessment presented in [4] demonstrated that, for a replication
factor of three, a declustered data placement scheme achieves a
superior reliability than other placement schemes. The declus-
tered placement scheme ensures that codewords are spread
equally across devices. This is the scheme that was originally
used by Google [21], Facebook [22], and Microsoft® Azure1
[23], but, to improve data reliability and storage efficiency
further, today they use erasure coding schemes that offer higher
efficiency [24-26].
The reliability of storage systems is further degraded by
the occurrence of unrecoverable sector errors, that is, errors
that can be corrected neither by the standard sector-associated
error correction code (ECC) nor by the re-read mechanism
of hard-disk drives (HDDs). These sector errors are latent,
because their existence is only discovered when there is an
attempt to access them. Once an unrecoverable or latent
sector error is detected, it can usually be corrected by the
erasure coding capability. However, if this is not feasible,
it is permanently lost, leading to an unrecoverable failure.
Consequently, unrecoverable errors do not necessarily lead
to unrecoverable failures. Permanent losses of data due to
latent errors are quite pronounced in higher-capacity HDDs
and storage nodes, because of the higher frequency of their
occurrence [27-30]. The risk of permanent loss of data rises
in the presence of latent errors.
Previous works have shown that actual latent-error rates
degrade MTTDL by orders of magnitude [8][11][28][30]. Does
this also apply to the case of the EAFDL metric given that,
when a data loss occurs, the amount of sectors lost due to
latent errors is much smaller than the amount of data lost due
to a device failure? What is the range of error rates that cause
EAFDL to deteriorate? This article addresses these critical
questions.
Analytical results for the MTTDL and EAFDL metrics in
the context of general erasure-coded storage systems, but in
the absence of latent errors, were obtained in [12-14]. The
first analytical assessment of EAFDL in the presence of latent
errors was presented in [15] for the case of RAID-5 systems by
presenting a comprehensive theoretical stochastic model that
captures all the details of the rebuild process. This model was
subsequently extended to a significantly more complex one for
the case of RAID-6 systems [16]. Clearly, extending this model
further to assess EAFDL in the presence of latent errors for
arbitrary erasure coding schemes seems to be a daunting task
because of its state explosion. The state of the model developed
in this article does not explode, because it takes into account
only the most significant details of the rebuild process.
To assess the reliability of erasure-coded systems, we adopt
the non-Markovian methodology developed in prior work [12-
1Microsoft is a trademark of Microsoft Corporation in the United States,
other countries, or both.
23
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

14] to evaluate MTTDL and EAFDL of storage systems and
extending it to assess the effect of latent errors. The validity
of this methodology for accurately assessing the reliability
of storage systems has been confirmed by simulations in
several contexts [4][9][12][31]. It has been demonstrated that
theoretical predictions of the reliability of systems comprising
highly reliable storage devices are in good agreement with
simulation results. Consequently, the emphasis of the present
work is on theoretically evaluating the reliability of storage
systems with latent errors.
The reliability results obtained by the model developed here
are shown to be in agreement with previous specific theoretical
and simulation results presented in the literature. We verify
its validity by comparing the results obtained for the cases
of RAID-5 and RAID-6 systems and showing that they match
with those derived by the detailed models in [16]. Furthermore,
we demonstrate that the model developed yields theoretical
reliability results that match well with the simulation results
obtained in [32], which studies the effect of erasure codes
deployed in a realistic distributed storage configuration. This
establishes a confidence for the model presented, the results
obtained, and the conclusions drawn. The model developed is
a practical one that takes into account the characteristics of
latent errors observed in real systems. It is realistic, because
it considers general device failure distributions including real-
world ones, such as Weibull and gamma. It can also be used
to assess system reliability when scrubbing is employed by
applying the methodology described in [8]. This is the first
work to study the effect of latent errors on EAFDL for general
erasure-coded storage systems.
Note that the storage model considered in this work is
relevant and realistic, because it properly captures the charac-
teristics of erasure coding and of the rebuild process associated
with the declustered data placement scheme currently used
by Google [24], Microsoft® Azure [26], Facebook [33], and
DELL/EMC [34]. Consequently, the theoretical results derived
here are important, because they can be used to assess the reli-
ability of the above schemes and also determine the parameter
values that ensure a desired level of reliability. It can also be
used to assess system reliability when scrubbing is employed
by applying the methodology described in [8].
The key contributions of this article are the following. We
consider the reliability of erasure-coded storage systems with
latent errors that was derived analytically for the MTTDL
and EAFDL reliability metrics for the entire range of sector
error rates, and for the symmetric, clustered, and declustered
data placement schemes [1]. In this article, we extend our
previous work by also presenting results for the additional
reliability metric of interest E(H), namely, the conditional
expected amount of lost user data, given that data loss has
occurred. We subsequently demonstrate that, in the range of
typical sector-error rates, unrecoverable failures are frequent,
which degrades MTTDL. However, in [1], it was shown that
the relative increase of the amount of data loss is negligible,
which leaves EAFDL practically unaffected in this range. In
the present work, we demonstrate that this result holds for
moderate erasure codes, but for very powerful erasure codes,
it may not be the case. We have confirmed that reliability
results obtained by the model developed here are in agree-
ment with previous specific theoretical and simulation results
TABLE I.
NOTATION OF SYSTEM PARAMETERS
Parameter
Definition
n
number of storage devices
c
amount of data stored on each device
l
number of user-data symbols per codeword (l ≥ 1)
m
total number of symbols per codeword (m > l)
(m, l)
MDS-code structure
s
symbol size
k
spread factor of the data placement scheme, or
group size (number of devices in a group) (m ≤ k ≤ n)
b
average reserved rebuild bandwidth per device
Bmax
upper limitation of the average network rebuild bandwidth
X
time required to read (or write) an amount c of data at an average
rate b from (or to) a device
FX(.)
cumulative distribution function of X
Fλ(.)
cumulative distribution function of device lifetimes
Pbit
probability of an unrecoverable bit error
seff
storage efficiency of redundancy scheme (seff = l/m)
U
amount of user data stored in the system (U = seff n c)
˜r
MDS-code distance: minimum number of codeword symbols lost
that lead to permanent data loss
(˜r = m − l + 1 and 2 ≤ ˜r ≤ m)
fX(.)
probability density function of X (fX(.) = F ′
X(.))
C
number of symbols stored in a device (C = c/s)
µ−1
mean time to read (or write) an amount c of data at an average rate
b from (or to) a device (µ−1 = E(X) = c/b)
λ−1
mean time to failure of a storage device
(λ−1 =
R ∞
0 [1 − Fλ(t)]dt)
Ps
probability of an unrecoverable sector (symbol) error
PDL
probability of data loss during rebuild
PUF
probability of data loss due to unrecoverable failures during rebuild
PDF
probability of data loss due to a disk failure during rebuild
Q
amount of lost user data during rebuild
H
amount of lost user data, given that data loss has occurred during
rebuild
S
number of lost symbols during rebuild
presented in the literature. We also assess the reliability of real-
world erasure coding schemes employed by enterprises. The
model developed provides useful insights into the benefits of
the erasure coding schemes and yields results for the entire
parameter space, which allows a better understanding of the
design tradeoffs.
The remainder of the article is organized as follows.
Section II reviews prior relevant work and analytical models
presented in the literature for assessing the effect of latent
errors on the reliability of erasure-coded systems. Section III
describes the storage system model and the corresponding
parameters considered. Section IV presents the general frame-
work and methodology for deriving the MTTDL and EAFDL
metrics analytically for the case of erasure-coded systems
and in the presence of latent errors. Closed-form expressions
for relevant reliability metrics are derived for the symmetric,
clustered, and declustered data placement schemes. Section V
presents numerical results demonstrating the adverse effect of
unrecoverable or latent errors and the effectiveness of these
schemes for improving system reliability. The reliability of
real-world erasure coding schemes employed by enterprises to
protect their stored data is assessed in Section VI. Finally, we
conclude in Section VII.
II.
RELATED WORK
The adverse effect of latent errors on the MTTDL reliability
metric of RAID-5, RAID-6, replication, and erasure-coded
systems has been demonstrated in [8][11][27-30]. Analytical
reliability expressions for MTTDL that take into account the
effect of latent errors have been obtained predominately using
Markovian models, which assume that component failure and
rebuild times are independent and exponentially distributed
24
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[8][11][28][29]. The effect of latent errors on MTTDL of
erasure-coded storage systems for the realistic case of non-
exponential failure and rebuild time distributions was assessed
in [30][35] for a limited range of error rates. In this article,
we consider the entire range of sector error rates and assess
the effect of latent errors not only on MTTDL, but also on the
amount of lost data for the realistic case of non-exponential
failure and rebuild time distributions.
Disk scrubbing has been used to mitigate the adverse
effect of latent errors on system reliability [8][36][37][38].
The scrubbing process identifies latent errors at an early stage
and attempts to correct them before disk failures occur. This
in effect reduces the probability of encountering a latent
error during the rebuild process. The resulting latent-error
probability was derived in [8] as a function of the scrubbing
and workload parameters. Subsequently, it was shown that
the reliability level achieved when scrubbing is used can be
obtained from the reliability level of a system that does not
use scrubbing by adjusting the probability of encountering a
latent error accordingly. The methodology presented in [8] for
deriving the adjusted latent error probability when scrubbing is
employed is also applicable for assessing the efficiency of other
scrubbing schemes, such as the adaptive scrubbing schemes
proposed in [37][38]. Moreover, this methodology can also be
applied in conjunction with the reliability results presented in
this article to assess the reliability of erasure-coded systems
when scrubbing is used.
A simulation analysis of reliability aspects of erasure-coded
data centers was presented in [39]. Various configurations were
considered and it was shown that erasure codes and redundancy
placement affect system reliability. In [32] it was recognized
that it is hard to get statistically meaningful experimental
reliability results using prototypes, because this would require
a large number of machines to run for years. This underscores
the usefulness of the analytical reliability results derived in
this article.
III.
STORAGE SYSTEM MODEL
To assess the reliability of erasure-coded storage systems,
we adopt the model used in [14] and extend it to cover the
case of latent errors. The storage system comprises n storage
devices (nodes or disks), where each device stores an amount
c of data such that the total storage capacity of the system
is n c. This does not account for the spare space used by the
rebuild process.
A. Redundancy
User data is divided into blocks (or symbols) of a fixed size
s (e.g., sector size of 512 bytes) and complemented with parity
symbols to form codewords. We consider (m, l) maximum
distance separable (MDS) erasure codes, which map l user-
data symbols to a set of m (> l) symbols, called a codeword,
having the property that any subset containing l of the m
symbols can be used to reconstruct (recover) the codeword.
The corresponding storage efficiency seff and amount U of
user data stored in the system is
seff = l/m
and
U = seff n c = l n c/m .
(1)
Figure 1. Clustered and declustered placement of codewords of length m = 3
on n = 6 devices. X1, X2, X3 represent a codeword (X = A, B, C, . . . , L).
Also, the number C of symbols stored in a device is
C = c/s .
(2)
Our notation is summarized in Table I. The derived param-
eters are listed in the lower part of the table. To minimize the
risk of permanent data loss, the m symbols of each codeword
are spread and stored on m distinct devices. This way, the
system can tolerate any ˜r − 1 device failures, but ˜r device
failures may lead to data loss, with
˜r = m − l + 1 ,
1 ≤ l < m
and
2 ≤ ˜r ≤ m .
(3)
Examples of MDS erasure codes are the replication, RAID-5,
RAID-6, and Reed–Solomon schemes.
B. Symmetric Codeword Placement
In a symmetric placement scheme, the system effectively
comprises n/k disjoint groups of k devices, and each codeword
is placed entirely in one of these groups. Within each group,
all

Figure 2.
Rebuild under declustered placement.
Figure 3.
Rebuild under clustered placement.
approximation, which considers only transitions from lower
to higher exposure levels [4][9][12][31][40]. This implies that
each exposure level is entered only once.
2) Prioritized Rebuild: When a symmetric or declustered
placement scheme is used, as shown in Figure 2, spare
space is reserved on each device for temporarily storing the
reconstructed codeword symbols before they are transferred to
a new replacement device. The rebuild process to restore the
data lost by failed devices is assumed to be both prioritized and
distributed. A prioritized (or intelligent) rebuild process always
attempts first to rebuild the most-exposed codewords, namely,
the codewords that have lost the largest number of symbols
[4][9][14][26][32]. At each exposure level u, it attempts to
bring the system back to exposure level u−1 by recovering one
of the u symbols that each of the Cu most-exposed codewords
has lost by reading l of the remaining symbols. In a distributed
rebuild process, the codewords are reconstructed by reading
symbols from an appropriate set of surviving devices and
storing the recovered symbols in the reserved spare space of
these devices. During this process, it is desirable to reconstruct
the lost codeword symbols on devices in which another symbol
of the same codeword is not already present.
In the case of clustered placement, the codeword symbols
are spread across all k (= m) devices in each group (cluster).
Therefore, reconstructing the lost symbols on the surviving
devices of a group would result in more than one symbol of
the same codeword on the same device. To avoid this, the lost
symbols are reconstructed directly in spare devices as shown
in Figure 3 and described in [14].
TABLE II.
NOTATION OF SYSTEM PARAMETERS AT EXPOSURE LEVELS
Parameter
Definition
u
exposure level
Cu
number of most-exposed codewords upon entering exposure level u
˜nu
number of devices at exposure level u whose failure causes an
exposure level transition to level u + 1
Vu
fraction of the most-exposed codewords that have symbols stored
on any given device from the ˜nu devices
Ru
rebuild time at exposure level u
αu
fraction of the rebuild time Ru still left when another device fails,
causing the exposure level transition u → u + 1
Pu→u+1
transition probability from exposure level u to u + 1
bu
average rate at which recovered data is written at exposure level u
3) Rebuild Process: A certain portion of the device band-
width is reserved for read/write data recovery during the
rebuild process, and the remaining bandwidth is used to serve
user requests. Let b denote the actual average reserved rebuild
bandwidth per device. The lost symbols are rebuilt in parallel
using the rebuild bandwidth available on each surviving device.
Let us denote by bu (≤ b) the average rate at which the
amount of data corresponding to the number Cu of symbols to
be rebuilt at exposure level u is written to selected device(s).
This rate depends on Bmax, the upper limitation of the average
network rebuild bandwidth [14]. Also, let 1/µ, fX(.), and
FX(.) denote the mean, the probability density function, and
the cumulative distribution function of the time X required
to read (or write) an amount c of data from (or to) a device,
respectively. The kth moment of X, E(Xk), and its mean
E(X) are then given by
E(Xk) =
Z ∞
0
tkfX(t)dt ,
for k = 1, 2, . . . ,
(5)
µ−1 ≜ E(X) = c/b .
(6)
4) Failure and Rebuild Time Distributions: The lifetimes
of the n devices are assumed to be independent and identically
distributed, with a cumulative distribution function Fλ(.) and
a mean of 1/λ. We consider real-world distributions, such as
Weibull and gamma, as well as exponential distributions that
belong to the large class defined in [31]. Note that, although
the model considered here does not account for correlated
device failures, their effect can be assessed by enhancing the
model according to the approach presented in [8]. This issue,
however, is beyond the scope of this article. The results in this
article hold for highly reliable storage devices, which satisfy
the condition [14][31]
µ
Z ∞
0
Fλ(t)[1 − FX(t)]dt ≪ 1,
with
λ
µ ≪ 1 .
(7)
This condition expresses the fact that the ratio of the mean time
1/µ to read all contents of a device (which typically is on the
order of tens of hours) to the mean time to failure of a device
1/λ (which is typically at least on the order of thousands of
hours) is very small, and in particular the fact that it is very
unlikely that a given device fails during a rebuild period.
When the devices are highly reliable, the MTTDL and
EAFDL reliability metrics of erasure-coded storage systems
tend to be insensitive to the device failure distribution, that
is, they depend only on its mean 1/λ, but not on its density
Fλ(.). They are, however, sensitive to the distribution FX(.)
of the device rebuild times [14].
26
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

5) Amount of Data to Rebuild and Rebuild Times at Each
Exposure Level: We denote by ˜nu the number of devices
at exposure level u whose failure causes an exposure level
transition to level u + 1 and Vu the fraction of the Cu most-
exposed codewords that have a symbol stored on any given
such device. Note that ˜nu depends on the codeword placement
scheme. The notation used here is summarized in Table II. Let
Ru denote the rebuild time of the most-exposed codewords at
exposure level u. At exposure level 1, the amount of data to
be recovered is equal to c. Given that this data is recovered
at an average rate of b1 and that the time required to write
an amount c of data at an average rate of b is equal to X, it
follows that the rebuild time R1 is given by
R1 = (b/b1) X .
(8)
Let αu be the fraction of the rebuild time Ru still left when
another device fails, causing the exposure level transition u →
u+1. In [41, Lemma 2], it was shown that, for highly reliable
devices satisfying condition (7), αu is approximately uniformly
distributed in (0, 1), that is
αu ∼ U(0, 1),
u = 1, . . . , ˜r − 1 .
(9)
We proceed by considering that the rebuild time Ru+1 is
determined completely by Ru and αu in the same manner
as in [13][14][40]. For the rebuild schemes considered, the
fraction of the Cu most-exposed codewords that were not yet
considered by the rebuild process upon the next device failure
is roughly equal to the fraction αu of the rebuild time Ru still
left. Therefore, upon the next device failure, an approximate
number αu Cu of the Cu codewords were not yet considered by
the rebuild process. Clearly, the fraction Vu of these codewords
that have symbols stored on the newly failed device depends
only on the codeword placement scheme. Consequently, the
number Cu+1 of the most-exposed codewords upon entering
exposure level u + 1 is
Cu+1 ≈ Vu αu Cu , for u = 1, . . . , ˜r − 1 .
(10)
Repeatedly applying (10) and using (4) and the convention that
for any sequence δi, Q0
i=1 δi ≜ 1, yields
Cu ≈ C
u−1
Y
i=1
Vi αi , for u = 1, . . . , ˜r .
(11)
Unconditioning (11) on α1, . . . , αu−1 yields
E(Cu) = C


u−1
Y
j=1
Vj

E


u−1
Y
j=1
αj
 level u was entered

 ,
for u = 1, . . . , ˜r .
(12)
6) Unrecoverable Errors: The reliability of storage systems
is affected by the occurrence of unrecoverable or latent errors.
Let Pbit denote the unrecoverable bit-error probability. Accord-
ing to the specifications, Pbit is equal to 10−15 for SCSI drives
and 10−14 for SATA drives [8]. Assuming that bit errors occur
independently over successive bits, the unrecoverable sector
(symbol) error probability Ps is
Ps = 1 − (1 − Pbit)s ,
(13)
with s expressed in bits. Assuming a sector size of 512 bytes,
the equivalent unrecoverable sector error probability is Ps ≈
Pbit × 4096, which is 4.096 × 10−12 in the case of SCSI and
4.096×10−11 in the case of SATA drives. In practice, however,
and also owing to the accumulation of latent errors over
time, these probability values are higher. Indeed, empirical
field results suggest that the actual values can be orders of
magnitude higher, reaching Ps ≈ 5 × 10−9 [42].
IV.
DERIVATION OF MTTDL AND EAFDL
The MTTDL metric assesses the expected time until some
data can no longer be recovered and therefore is lost forever,
whereas the EAFDL assesses the fraction of stored data that is
expected to be lost by the system annually. The reliability met-
rics are derived using the methodology presented in [12][13]
[14] and extending it to assess the effect of latent errors. This
methodology uses the direct path approximation [11], does not
involve Markovian analysis [4][9][12][31][40], and holds for
general failure time distributions, which can be exponential or
non-exponential, such as the Weibull and gamma distributions
that satisfy condition (7).
At any point in time, the system can be thought to be in one
of two modes: normal or rebuild mode. During normal mode,
all devices are operational and all data in the system has the
original amount of redundancy. A first device failure causes
a transition from normal to rebuild mode. A rebuild process
attempts to restore the lost data, which eventually leads the
system either to a data loss (DL) with probability PDL or back
to the original normal mode by restoring initial redundancy,
with probability 1 − PDL. Any symbols encountered with
unrecoverable or latent errors are usually corrected by the
erasure coding capability. However, it may not be possible
to recover multiple unrecoverable errors in a codeword, which
therefore leads to data loss.
Let T be a typical interval of a fully operational period,
that is, the interval from the time t that the system is brought to
its original state until a subsequent first device failure occurs.
For a system comprising n devices with a mean time to failure
of a device 1/λ, the expected duration of T is [12]
E(T) =
1
n λ ,
(14)
and MTTDL is
MTTDL ≈ E(T)
PDL
=
1
n λ PDL
.
(15)
The EAFDL is obtained as the ratio of the expected amount
E(Q) of lost user data, normalized to the amount U of user
data, to the expected duration of T [12, Eq. (9)]:
EAFDL ≈
E(Q)
E(T) · U
(14)
=
n λ E(Q)
U
(1)
= m λ E(Q)
l c
, (16)
with E(T) and 1/λ expressed in years.
The expected conditional amount E(H) of lost user data,
given that data loss has occurred, is determined by [12, Eq.
(8)]:
E(H) = E(Q)
PDL
.
(17)
It follows from (15), (16), and (17) that
EAFDL =
E(H)
MTTDL · U ,
(18)
with the MTTDL expressed in years.
27
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE III.
NOTATION OF RELIABILITY METRICS AT EXPOSURE
LEVELS
Parameter
Definition
u
exposure level
Pu
probability of entering exposure level u
PUFu
probability of data loss due to unrecoverable symbol errors at
exposure level u
PUF
probability of data loss due to unrecoverable symbol errors
PDF
probability of data loss due to ˜r successive device failures
PDL
probability of data loss
qu
probability that, at exposure level u, a codeword that has lost u
symbols can be restored
ˆqu
probability that, under instantaneous transitions from exposure level
1 to exposure level u, all of the Cu most-exposed codewords, which
have lost u symbols, can be restored
A. Reliability Analysis
At any exposure level u (u = 1, . . . , ˜r − 1), data loss
may occur during rebuild owing to one or more unrecoverable
failures, which is denoted by the transition u → UF. Moreover,
at exposure level ˜r−1, data loss occurs owing to a subsequent
device failure, which leads to the transition to exposure level
˜r. Consequently, the direct paths that lead to data loss are the
following:
−−→
UFu : the direct path of successive transitions 1 → 2 →
· · · → u → UF, for u = 1, . . . , ˜r − 1, and
−−→
DF : the direct path of successive transitions 1 → 2 →
· · · → ˜r − 1 → ˜r,
with corresponding probabilities PUFu and PDF, respectively.
The notation for the probabilities of the events that lead to data
loss is summarized in Table III.
1) Data Loss: The probability PUF of data loss owing to
unrecoverable failures is
PUF ≈
˜r−1
X
u=1
PUFu ,
(19)
where PUFu denotes the probability of data loss associated with
the direct path −−→
UFu. Also, it holds that
PUFu = Pu Pu→UF , for u = 1, . . . , ˜r − 1 ,
(20)
where Pu is the probability of entering exposure level u, which
is derived in Appendix A as follows:
Pu ≈ (λ c)u−1
1
(u − 1)!
E(Xu−1)
[E(X)]u−1
u−1
Y
i=1
˜ni
bi
V u−1−i
i
, (21)
and Pu→UF is the probability of encountering an unrecoverable
failure during the rebuild process at this exposure level.
In [11], it was shown that PDL is accurately approximated
by the probability of all direct paths to data loss. Therefore,
PDL ≈ PDF +
˜r−1
X
u=1
PUFu
(19)
≈
PDF + PUF .
(22)
Approximate expressions for the probabilities of data loss
PUFu and PDF are subsequently obtained by the following
proposition.
Proposition 1: For u = 1, . . . , ˜r − 1, it holds that
PUFu ≈ − (λ c)u−1 E(Xu−1)
[E(X)]u−1
 u−1
Y
i=1
˜ni
bi
V u−1−i
i
!
· log(ˆqu)−(u−1)
 
ˆqu −
u−1
X
i=0
log(ˆqu)i
i!
!
,
(23)
where
ˆqu ≜ q
C Qu−1
j=1 Vj
u
,
(24)
qu = 1 −
m−u
X
j=˜r−u
m − u
j

P j
s (1 − Ps)m−u−j ,
(25)
PDF ≈ (λ c)˜r−1
1
(˜r − 1)!
E(X ˜r−1)
[E(X)]˜r−1
˜r−1
Y
i=1
˜ni
bi
V ˜r−1−i
i
, (26)
where E(X ˜r−1) is obtained from (5).
Proof: Equation (23) is obtained in Appendix A. Equation
(26) is obtained from the fact that PDF = P˜r and, subsequently,
from (21) by setting u = ˜r.
The MTTDL metric is obtained by substituting (22) into
(15) as follows:
MTTDL ≈
1
n λ (PDF + P˜r−1
u=1 PUFu)
,
(27)
where PUFu and PDF are determined by (23) and (26), respec-
tively.
2) Amount of Data Loss: We proceed to derive the amount
of data loss during rebuild. Let Q, H, and S be the amount of
lost user data, the conditional amount of lost user data, given
that data loss has occurred, and the number of lost symbols,
respectively. Let also QDF and QUFu denote the amount of
lost user data associated with the direct paths −−→
DF and −−→
UFu,
respectively. Similarly, we consider the variables HDF, HUFu,
SDF, and SUFu. Then, the amount Q of lost user data is
obtained by
Q ≈





HDF ,
if −−→
DF
HUFu ,
if −−→
UFu , for u = 1, . . . , ˜r − 1
0 ,
otherwise .
(28)
Therefore,
E(Q) ≈ PDF E(HDF) +
˜r−1
X
u=1
PUFu E(HUFu)
(29)
= E(QDF) +
˜r−1
X
u=1
E(QUFu)
(30)
≈ E(QDF) + E(QUF) ,
(31)
where
E(QDF) = PDF E(HDF) ,
(32)
E(QUFu) = PUFu E(HUFu) , for u = 1, . . . , ˜r − 1
(33)
E(QUF) = PUF E(HUF) ≈
˜r−1
X
u=1
E(QUFu) ,
(34)
where QUF denotes the amount of lost user data due to
unrecoverable failures and HUF the conditional amount of lost
28
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

user data, given that data loss due to unrecoverable failures
has occurred.
Note that the expected amount E(Q) of lost user data is
equal to the product of the storage efficiency and the expected
amount of lost data, where the latter is equal to the product
of the expected number of lost symbols E(S) and the symbol
size s. Consequently, it follows from (1) that
E(Q) =
l
m E(S) s
(2)
=
l
m
E(S)
C
c .
(35)
Similarly,
E(QDF) = l
m E(SDF) s
(2)
=
l
m
E(SDF)
C
c ,
(36)
E(QUFu) = l
m E(SUFu) s
(2)
=
l
m
E(SUFu)
C
c .
(37)
Approximate
expressions
for
the
expected
amounts
E(QUFu) and E(QDF) of lost user data are subsequently
obtained by the following proposition.
Proposition 2: For u = 1, . . . , ˜r − 1, it holds that
E(QUFu) ≈ c l ˜r
m (λ c)u−1 1
u!
E(Xu−1)
[E(X)]u−1
 u−1
Y
i=1
˜ni
bi
V u−i
i
!
·
m − u
˜r − u

P ˜r−u
s
,
for Ps ≪
1
m − ˜r , (38)
E(QDF) ≈ c l
m (λ c)˜r−1
1
(˜r − 1) !
E(X ˜r−1)
[E(X)]˜r−1
˜r−1
Y
i=1
˜ni
bi
V ˜r−i
i
,
(39)
where E(Xu−1) and E(X ˜r−1) are obtained from (5).
Proof: Equations (38) and (39) are obtained in Appendix
B. Note that (39) can also be obtained from (38) by setting
u = ˜r, which is the same result as Eq. (25) of [14].
The EAFDL metric is obtained by substituting (30) into
(16) as follows:
EAFDL ≈ m λ [E(QDF) + P˜r−1
u=1 E(QUFu)]
l c
,
(40)
where E(QUFu) and E(QDF) are determined by (38) and (39),
respectively.
The conditional amounts E(H), E(HDF), E(HUFu), and
E(HUF) of lost user data, given that data loss has occurred,
are obtained from (17), (32), (33), and (34), respectively.
Remark 1: From (26), (32), and (39), it follows that
E(HDF) ≈
 
l
m
˜r−1
Y
i=1
Vi
!
c .
(41)
Note that when entering exposure level ˜r, for each of the
C˜r most-exposed codewords there are ˜r symbols permanently
lost. Consequently, the expected number of user-data symbols
permanently lost is C˜r (l/m) ˜r, which implies that, for a
symbol size of s, the expected amount E(HDF | C˜r) of user
data lost is
E(HDF | C˜r) = C˜r
l
m ˜r s .
(42)
Unconditioning (42) on C˜r yields
E(HDF) = E(C˜r) l
m ˜r s .
(43)
Combining (41), (43), and using (2), yields
E(C˜r) ≈
 ˜r−1
Y
i=1
Vi
!
C
˜r .
(44)
From (12) and (44), it follows that
E


˜r−1
Y
j=1
αj
 level ˜r was entered

 ≈ 1
˜r .
(45)
Remark 2: It turns out that when a data loss has occurred,
the variables α1, . . . , α˜r−1 are not distributed identically. More
specifically, for a rebuild time Ru, the uniform distribution of
αu in the interval (0, 1), given by (9), holds under the assump-
tion that there is a failure during this rebuild period, that is, an
exposure level transition u → u+1. However, conditioning on
the exposure level transitions u → u+1 → · · · → u′ → u′ +1
(u′ > u), αu is no longer uniformly distributed in (0, 1). This
is due to the fact that, conditioning on the fact that additional
failures occur during the rebuild times Ru+1, . . . , Ru′, it is
more likely that the Ru+1 period is long rather than short. In
this case, only α′
u is uniformly distributed in (0, 1). Assuming
that the system has entered exposure level u, we deduce from
(45) that
E


u−1
Y
j=1
αj
 level u was entered

 ≈ 1
u ,
for u = 2, . . . , ˜r .
(46)
Substituting (46) into (12) and using (4) yields
E(Cu) ≈
 u−1
Y
i=1
Vi
!
C
u ,
for u = 1, . . . , ˜r .
(47)
Remark 3: For small values of Ps, it holds that PUF → 0
and E(QUF) → 0. Therefore, from (22) and (31), for small
values of Ps, it holds that PDL → PDF and E(Q) → E(QDF).
Consequently, from (17), (32), and (41), it follows that
E(H) ≈ E(HDF) ≈
 
l
m
˜r−1
Y
i=1
Vi
!
c ,
for Ps → 0 .
(48)
When Ps is extremely small and an unrecoverable failure
occurs, this failure most likely occurs when rebuilding a
codeword after it has lost ˜r − 1 of its symbols owing to ˜r − 1
device failures and an unrecoverable error is encountered. In
this case, ˜r of its symbols are lost and therefore the expected
number of lost user symbols is equal to the product of the
storage efficiency l/m and ˜r, which implies that
E(HUF) ≈
l
m ˜r s
(2)
=
1
C
l
m ˜r c ,
for Ps → 0 .
(49)
Remark 4: For large values of Ps, it holds that
PDL ≈ PUF ≈ PUF1 ≈ 1 ,
for Ps → 1 ,
(50)
which, by virtue of (15), implies that
MTTDL ≈
1
n λ ,
for Ps → 1 .
(51)
29
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

It also holds that
E(Q) ≈ E(H) ≈ l c ,
for Ps → 1 .
(52)
From (18), and using (51) and (52), it follows that
EAFDL ≈ m λ ,
for Ps → 1 .
(53)
B. Symmetric and Declustered Placement
We consider the case m < k ≤ n. The special case k =
m corresponding to the clustered placement scheme has to
be considered separately for the reasons discussed in Section
III-C2. At each exposure level u, for u = 1, · · · , ˜r−1, it holds
that [13][14]
˜nsym
u
= k − u ,
(54)
bsym
u
= min((k − u) b, Bmax)
l + 1
,
(55)
V sym
u
= m − u
k − u .
(56)
The corresponding parameters ˜ndeclus
u
, bdeclus
u
, and V declus
u
for the
declustered placement are derived from (54), (55), and (56) by
setting k = n. which yields
˜ndeclus
u
= n − u ,
(57)
bdeclus
u
= min((n − u) b, Bmax)
l + 1
,
(58)
V declus
u
= m − u
n − u .
(59)
C. Clustered Placement
At any exposure level u (u = 1, . . . , ˜r − 1), it holds that
[13][14]
˜nclus
u
= m − u , bclus
u
= min( b , Bmax/l ) , V clus
u
= 1 . (60)
Remark 5: It follows from (60) that a system is not
bandwidth-constrained when Bmax
≥
l b. Then, bclus
u
=
min(b, Bmax/l) = b. In the case of RAID-5 and RAID-6, it
holds that m − l = 1 and m − l = 2 or, equivalently, ˜r = 2
and ˜r = 3, respectively, such that (22), (23), and (26) yield
P RAID-5
DL
≈ (m − 1) λ c
b
+ 1 − (1 − Ps)(m−1) C ,
(61)
which is the same result as Eq. (85) of [16] (with c
b = 1
µ), and
P RAID-6
DL
≈ 1 − qC
1 +

1 + 1 − qC
2
log(qC
2 )

(m − 1) λ c
b
+ (m − 1)(m − 2)
2
λ c
b
2 E(X2)
[E(X)]2 ,
(62)
where q1, q2 are determined by (25). This result is in agreement
with Eq. (243) of [16] (with c
b = 1
µ). Also, (30), (38), and (39)
yield
E(QRAID-5) ≈
l
m (m − 1)
 λ c
b + 2 Ps

c ,
(63)
TABLE IV.
TYPICAL VALUES OF DIFFERENT PARAMETERS
Parameter
Definition
Values
n
number of storage devices
64
c
amount of data stored on each device
20 TB
s
symbol (sector) size
512 B
λ−1
mean time to failure of a storage device
876,000 h
b
rebuild bandwidth per device
100 MB/s
m
symbols per codeword
16
l
user-data symbols per codeword
13, 14, 15
U
amount of user data stored in the system
1.04 to 1.2 PB
µ−1
time to read an amount c of data at a rate
b from a storage device
55.5 h
which is the same result as Eq. (105) of [16] (with c
b = 1
µ),
and
E(QRAID-6) ≈ l
m
(m − 1)(m − 2)
2
·
"λ c
b
2 E(X2)
[E(X)]2 + 3 λ c
b Ps + 3 P 2
s
#
c . (64)
This result is in agreement with Eq. (264) of [16] (with c
b = 1
µ).
V.
NUMERICAL RESULTS
Here we assess the reliability of the clustered and declus-
tered schemes for a system comprised of n = 64 devices
(disks) and protected by an erasure coding scheme with m =
16, which is the codeword length used by Microsoft® Azure
[26], and l = 13, 14, and 15. Each device stores an amount
of c = 20 TB, which is the capacity of the latest generation
of Seagate drives, and the symbol size s is equal to a sector
size of 512 bytes [43].
Typical parameter values are listed in Table IV. The an-
nualized failure rate (AFR) of HDDs for the year 2021 is in
the range of 0.11% to 4.79% [44], which corresponds to a
mean time to failure in the range of 180, 000 h to 8, 000, 000
h. The parameter λ−1 is chosen to be equal to 876, 000 h (100
years) that corresponds to an AFR of 1%, which is the average
AFR across all drive models [44]. Considering that 35% of
the maximum transfer rate of 285 MB/s [43] is allocated for
recovery operations, the reserved rebuild bandwidth b is then
equal to 100 MB/s, which yields a rebuild time of a device
µ−1 = c/b = 55.5 h. Also, it is assumed that the maximum
network rebuild bandwidth is sufficiently large (Bmax ≥ n b =
6.4 GB/s), that the rebuild time distribution is deterministic,
such that E(Xk) = [E(X)]k. The obtained results are accu-
rate, because (7) is satisfied, given that λ/µ = 6.3×10−5 ≪ 1.
First, we assess the reliability for the declustered placement
scheme (k = n = 64). The probability of data loss PDL is
determined by (22) as a function of Ps and shown in Figure
4. The probabilities PUFu and PDF are also shown, as obtained
from (23) and (26), respectively. We observe that PDL increases
monotonically with Ps and exhibits a number of ˜r plateaus. In
the interval [4.096 × 10−12, 5 × 10−9] of practical importance
for Ps, which is indicated between the two vertical dashed
lines, the probability of data loss PDL and, by virtue of (15), the
MTTDL are degraded by orders of magnitude. The normalized
λ MTTDL measure is obtained from (15) and shown in Figure
5. Increasing the number of parities (reducing l) improves
reliability by orders of magnitude.
The normalized expected amount E(Q)/c of lost user
data relative to the amount of data stored in a device is
30
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(a) l = 13
(˜r = 4)
(b) l = 14
(˜r = 3)
(c) l = 15
(˜r = 2)
Figure 4.
Probability of data loss PDL vs. Ps for l = 13, 14, 15; m = 16, n = k = 64 (declustered scheme).
(a) l = 13
(˜r = 4)
(b) l = 14
(˜r = 3)
(c) l = 15
(˜r = 2)
Figure 5.
Normalized MTTDL vs. Ps for l = 13, 14, and 15; m = 16, n = k = 64 (declustered scheme).
(a) l = 13
(˜r = 4)
(b) l = 14
(˜r = 3)
(c) l = 15
(˜r = 2)
Figure 6.
Normalized amount of data loss E(Q) vs. Ps for l = 13, 14, 15; m = 16, n = k = 64 (declustered scheme).
obtained from (30) and shown in Figure 6. The normalized
expected amounts E(QUFu)/c and E(QDF)/c are also shown
as determined by (38) and (39), respectively. The normalized
EAFDL/λ measure is obtained from (16) and shown in Figure
7. We observe that E(Q) and EAFDL increase monotonically,
but they are practically unaffected in the interval of interest,
because they degrade only when Ps is much larger than the
typical sector error probabilities. For the EAFDL metric too,
increasing the number of parities (reducing l) results in a
reliability improvement by orders of magnitude.
The normalized expected amount E(H)/c of lost user data,
given that a data loss has occurred, relative to the amount
of data stored in a device is obtained from (17) and shown
in Figure 8. The conditional amounts E(HDF) and E(HUF)
obtained from (32) and (34), respectively, are also shown. In
contrast to the PDL, EAFDL, and E(Q) metrics that increase
monotonically with Ps, we observe that E(H) does not do
so. The reason for that is the following. As shown in Figure
4, for Ps ≫ 10−14, data loss is more likely to be due to
sector errors than to device failures. Given that sector errors
result in a negligible amount of data loss compared with the
substantial data losses caused by device failures, when Ps
increases over the value of 10−14, the conditional amount of
lost data decreases. Clearly, this is reversed for high values of
Ps, and the conditional amount of lost data increases.
The expected amount E(HUF) of user data lost due to
unrecoverable failures, given that such failures have occurred,
is shown in Figure 8 by the dotted green line. For extremely
small values of Ps, and according to Remark 3, the value of
E(HUF) corresponds to a single corrupted codeword that loses
31
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(a) l = 13
(˜r = 4)
(b) l = 14
(˜r = 3)
(c) l = 15
(˜r = 2)
Figure 7.
Normalized EAFDL vs. Ps for l = 13, 14, and 15; m = 16, n = k = 64 (declustered scheme).
(a) l = 13
(˜r = 4)
(b) l = 14
(˜r = 3)
(c) l = 15
(˜r = 2)
Figure 8.
Normalized E(H) vs. Ps for l = 13, 14, and 15; m = 16, n = k = 64 (declustered scheme).
˜r of its symbols of which ˜r−1 are lost owing to device failures
and one is lost owing to an unrecoverable error. Consequently,
E(HUF) is independent of Ps, as indicated by the horizontal
part of the dotted green line. Let us now consider Figure
8(a). When Ps ≫ 10−10, E(HUF) increases, because there
are multiple such codewords, each of which loses ˜r symbols.
Subsequently, for ˜r ≥ 3 and when Ps ≫ 10−8, unrecoverable
failures may also be caused by a single corrupted codeword
that loses ˜r of its symbols, ˜r − 2 of which are lost owing
to device failures and two are lost owing to unrecoverable
errors. This in turn reduces the amount of lost data in the
interval (10−10, 10−8), as shown in Figure 8(a). Note that this
interval corresponds to that of the second plateau, as shown
in Figure 4(a). When Ps ≫ 10−6, E(HUF) increases again
owing to the occurrence of multiple such corrupted codewords.
Eventually, when Ps ≫ 10−5, unrecoverable failures are
encountered during rebuild prior to a second device failure
and are caused by corrupted codewords that lose ˜r of their
symbols, one of which is lost owing to the first device failure
and ˜r − 1 are lost owing to unrecoverable errors. This in
turn increases E(HUF), which eventually dominates E(HDF).
Similar observations apply in the cases of Figures 8(b) and
8(c).
The reliability metrics corresponding to the clustered place-
ment scheme (k = m = 16) are plotted in Figures 9, 10, 11,
12, and 13. We observe that the reliability achieved by the
clustered data placement scheme does not reach the reliability
level achieved by the declustered one.
In the cases considered, EAFDL is practically unaffected
by the presence of latent errors, as shown in Figures 7 and
12. Note, however, that for larger values of ˜r, EAFDL may be
affected by the presence of latent errors. For example, when
m = 24 and l = 12, which yields ˜r = 13, and for the case
of declustered placement scheme, not only MTTDL, but also
EAFDL is affected, as shown in Figure 14.
The performance of certain erasure coding schemes was
assessed in [32] by obtaining the probability of data loss
PDL using a detailed distributed storage simulator. The PDL
values corresponding to Ps = 4.096 × 10−12 (Pbit = 10−15)
for two of the configurations considered are indicated by the
squares in Figure 15. This figure also shows the probabilities
of data loss PDL that correspond to these two configurations
and obtained from (22) as a function of Ps. We observe that the
theoretical results are in agreement with the simulation results,
which confirms the validity of the model and the analytical
expressions derived.
VI.
REAL-WORLD ERASURE CODING SCHEMES
Here we assess the reliability of various practical systems
that store an amount of U = 1.2 PB user data on devices
(disks) whose capacity is c = 20 TB. This amount of user
data can therefore be stored on U/c = 60 devices. The system
comprises n devices, where n is determined using (1) as
follows:
n = U
c
m
l = 60 m
l .
(65)
Subsequently, we consider the following real-world erasure
coding schemes:
32
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(a) l = 13
(˜r = 4)
(b) l = 14
(˜r = 3),
RAID-6
(c) l = 15
(˜r = 2),
RAID-5
Figure 9.
Probability of data loss PDL vs. Ps for l = 13, 14, 15; n = 64, k = m = 16 (clustered scheme).
(a) l = 13
(˜r = 4)
(b) l = 14
(˜r = 3),
RAID-6
(c) l = 15
(˜r = 2),
RAID-5
Figure 10.
Normalized MTTDL vs. Ps for l = 13, 14, and 15; n = 64, k = m = 16 (clustered scheme).
(a) l = 13
(˜r = 4)
(b) l = 14
(˜r = 3),
RAID-6
(c) l = 15
(˜r = 2),
RAID-5
Figure 11.
Normalized amount of data loss E(Q) vs. Ps for l = 13, 14, 15; n = 64, k = m = 16 (clustered scheme).
(a) l = 13
(˜r = 4)
(b) l = 14
(˜r = 3),
RAID-6
(c) l = 15
(˜r = 2),
RAID-5
Figure 12.
Normalized EAFDL vs. Ps for l = 13, 14, and 15; n = 64, k = m = 16 (clustered scheme).
33
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(a) l = 13
(˜r = 4)
(b) l = 14
(˜r = 3),
RAID-6
(c) l = 15
(˜r = 2),
RAID-5
Figure 13.
Normalized E(H) vs. Ps for l = 13, 14, and 15; n = 64, k = m = 16 (clustered scheme).
(a) Normalized MTTDL
(b) Normalized EAFDL
(c) Normalized E(H)
Figure 14.
Reliability metrics vs. Ps for n = k = 64 (declustered scheme), m = 24, l = 12.
Figure 15.
PDL vs. Ps for n = k = 210, λ−1 = 3 years, µ−1 = 34 hours,
λ/µ = 0.0013, c = 15 TB, and s = 512 B. Reliability schemes: 3-way
replication and MDS(14,10).
1) the 3-way replication (triplication) scheme that was ini-
tially used by Google’s GFS, Microsoft® Azure, and
Facebook. In this case, m = 3, l = 1, with a corre-
sponding storage efficiency of seff = 33%. According to
(65), this scheme requires the employment of n = 180
devices.
2) the RS(9,6) erasure coding scheme employed by Google’s
GFS as well as QFS [24, 45], which for m = 9 and l = 6
achieves a storage efficiency of seff = 66% and requires
a number of n = 90 devices.
3) the MDS(16,12) erasure coding scheme akin to the
LRC(16,12) code used by Microsoft® Azure [26], which
for m = 16 and l = 12 achieves a storage efficiency of
seff = 75% and requires a number of n = 80 devices.
4) the RS(14,10) erasure coding scheme employed by Face-
book [25], which for m = 14 and l = 10 achieves a
storage efficiency of seff = 71% and requires a number
of n = 84 devices.
We proceed to assess the reliability of the four erasure
coding schemes for two data placement configurations: a
symmetric one where the system comprises 2 disjoint groups of
k devices, such that k = n/2, and a declustered one, such that
k = n. As we will see next, a superior reliability is achieved
by employing the declustered data placement scheme.
A. Symmetric Data Placement
First, we assess the reliability of the 3-way replication
(triplication) scheme that requires the employment of n = 180
devices, which in turn implies that each of the two groups
comprises k
= 90 devices. The reliability measures are
obtained for the parameter values listed in Table IV and shown
in Figures 16(a) through 20(a). We observe that MTTDL is
significantly degraded by the presence of latent errors. In the
interval [4.096 × 10−12, 5 × 10−9] of practical importance for
Ps, which is indicated between the two vertical dashed lines,
Figure 17(a) shows that MTTDL is degraded by three to six
orders of magnitude, whereas Figure 19(a) reveals that EAFDL
is practically unaffected in this range.
Second, we consider the MDS(9,6) erasure coding scheme
that requires a number of n = 90 devices, which in turn
implies that each of the two groups comprises k = 45 devices.
The corresponding reliability measures are shown in Figures
34
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

16(b) through 20(b). Figure 17(b) shows that, in the region of
interest for Ps, MTTDL is degraded by three to five orders
of magnitude, whereas Figure 19(a) reveals that EAFDL is
practically unaffected in this range.
Subsequently, we consider the MDS(16,12) erasure coding
scheme that requires a number of n = 80 devices, which in
turn implies that each of the two groups comprises k = 40
devices. The corresponding reliability measures are shown in
Figures 16(c) through 20(c). Figure 17(c) shows that, in the
interval of practical importance for Ps, MTTDL is degraded
by three to five orders of magnitude, whereas Figure 19(c)
reveals that EAFDL is practically unaffected in this range.
Finally, we consider the RS(14,10) erasure coding scheme
that requires n = 84 devices, which in turn implies that each of
the two groups comprises k = 42 devices. The corresponding
reliability measures are shown in Figures 16(d) through 20(d).
Figure 17(d) shows that, in the interval of interest, MTTDL
is degraded by three to five orders of magnitude, whereas
Figure 19(d) reveals that EAFDL is practically unaffected in
this range.
From the above, it follows that erasure coding schemes
corresponding to higher values of ˜r offer a higher level of
reliability. Thus, the MDS(16,12) and MDS(14,10) erasure
coding schemes, for which ˜r = 5, offer higher levels of
reliability compared with the MDS(9,6) and 3-way replication
schemes, for which ˜r < 5. In particular, MDS(14,10) achieves
a higher reliability than that of MDS(16,10), albeit at a lower
storage efficiency (71% vs. 75%).
B. Declustered Data Placement
Here, we assess the reliability achieved by the erasure cod-
ing schemes considered when the declustered data placement
scheme is used, such that k = n. The reliability results are
shown in Figures 21(a) through 25(a).
First, we assess the reliability of the 3-way replication
(triplication) scheme. Comparing Figure 22(a) with Figure
17(a), we deduce that MTTDL is roughly the same. However,
comparing Figure 24(a) with Figure 19(a), we deduce that
EAFDL improves by one order of magnitude.
Regarding, the reliability of the MDS(9,6) coding scheme,
comparing Figure 22(b) with Figure 17(b), we deduce that
MTTDL improves slightly by one order of magnitude, espe-
cially at smaller values of Ps. However, Figures 24(b) and
19(b) demonstrate that EAFDL improves by two orders of
magnitude.
For the MDS(16,12) coding scheme, Figures 22(c) and
17(c) show that MTTDL improves by two orders of magnitude
for values around the left vertical dotted line and by one order
of magnitude for values around the right vertical dotted line.
Also, from Figures 24(c) and 19(c), we observe that EAFDL
improves by three orders of magnitude.
Finally,
the
reliability
improvement
regarding
the
MDS(14,10)
coding
scheme
is
similar
to
that
of
the
MDS(16,12) coding scheme. Figures 22(d) and 17(d) show
that MTTDL improves by two orders of magnitude for values
around the left vertical dotted line and by one order of
magnitude for values around the right vertical dotted line.
Also, Figures 24(d) and 19(d) show that EAFDL improves by
three orders of magnitude.
C. Reliability Improvement
The reliability improvement of the erasure coding schemes
considered over the initial 3-way replication is shown in
Figures 26 and 27 for the two data placements, respectively.
Clearly, in the interval of practical importance for Ps, the
MDS(14,10) erasure coding scheme achieves superior reli-
ability for both symmetric and declustered data placement
schemes.
In particular, for the symmetric data placement, Fig-
ure 26(a) demonstrates that in the interval of interest, the
MDS(9,6) erasure coding scheme improves MTTDL by three
orders of magnitude for Ps values around the left vertical dot-
ted line and by four orders of magnitude for Ps values around
the right vertical dotted line. The MDS(16,12) erasure coding
scheme improves MTTDL by six orders of magnitude for Ps
values around the left vertical dotted line and by seven orders
of magnitude for Ps values around the right vertical dotted
line. Also, the MDS(14,10) erasure coding scheme improves
MTTDL by seven orders of magnitude for Ps values around
the left vertical dotted line and by eight orders of magnitude
for Ps values around the right vertical dotted line. On the
other hand, Figure 26(b) demonstrates that in the interval of
practical importance for Ps, the MDS(9,6), MDS(16,12), and
MDS(14,10) erasure coding schemes improve EAFDL by two,
five, and six orders of magnitude, respectively.
For the declustered data placement, Figure 27(a) demon-
strates that in the interval of interest, the MDS(9,6) erasure
coding scheme improves MTTDL by four orders of magnitude,
the MDS(16,12) erasure coding scheme improves MTTDL by
eight orders of magnitude, whereas the MDS(14,10) erasure
coding scheme improves MTTDL by nine orders of magnitude.
On the other hand, Figure 27(b) demonstrates that in the inter-
val of practical importance for Ps, the MDS(9,6), MDS(16,12),
and MDS(14,10) erasure coding schemes improve EAFDL by
three, seven, and eight orders of magnitude, respectively.
Figures 26(c) and 27(c) show the ratios of the E(H) met-
rics for the MDS(9,6), MDS(16,12), and MDS(14,10) erasure
coding schemes to the E(H) metric for the 3-way replication
scheme. In the region of interest for Ps, all three erasure coding
schemes result in greater amounts of lost user data, given that
data loss has occurred, compared to the conditional amount of
lost user data in the case of a 3-way replication. In particular,
for the symmetric data placement, the MDS(9,6), MDS(16,12),
and MDS(14,10) erasure coding schemes result in about 20,
82, and 33 times greater conditional amounts of lost user data
for Ps values around the left vertical dotted line and in about
58, 550, and 110 times greater conditional amounts of lost
user data for Ps values around the right vertical dotted line,
respectively. This is due to the fact that, for small values of
Ps and according to Remark 3, E(H) depends not only on the
values of the m and l parameters, but also on the number n
of devices in the system, which also varies. Accordingly, for
the declustered data placement, Figure 27(c) demonstrates that
the MDS(9,6), MDS(16,12), and MDS(14,10) erasure coding
schemes result in about 9, 18, and 7 times greater conditional
amounts of lost user data, respectively.
35
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(a) MDS(3,1)
(˜r=3)
(b) MDS(9,6)
(˜r=4)
(c) MDS(16,12)
(˜r=5)
(d) MDS(14,10)
(˜r=5)
Figure 16.
Probability of data loss PDL vs. Ps for various MDS coding schemes; symmetric data placement with k = n/2.
(a) MDS(3,1)
(b) MDS(9,6)
(c) MDS(16,12)
(d) MDS(14,10)
Figure 17.
MTTDL vs. Ps for various MDS coding schemes; symmetric data placement with k = n/2.
Figure 18.
Normalized amount of data loss E(Q) vs. Ps for various MDS coding schemes; symmetric data placement with k = n/2.
(a) MDS(3,1)
(b) MDS(9,6)
(c) MDS(16,12)
(d) MDS(14,10)
Figure 19.
EAFDL vs. Ps for various MDS coding schemes; symmetric data placement with k = n/2.
(a) MDS(3,1)
(b) MDS(9,6)
(c) MDS(16,12)
(d) MDS(14,10)
Figure 20.
Normalized E(H) vs. Ps for various MDS coding schemes; symmetric data placement with k = n/2.
36
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(a) MDS(3,1)
(˜r=3)
(b) MDS(9,6)
(˜r=4)
(c) MDS(16,12)
(˜r=5)
(d) MDS(14,10)
(˜r=5)
Figure 21.
Probability of data loss PDL vs. Ps for various MDS coding schemes; declustered data placement (k = n).
(a) MDS(3,1)
(b) MDS(9,6)
(c) MDS(16,12)
(d) MDS(14,10)
Figure 22.
MTTDL vs. Ps for various MDS coding schemes; declustered data placement (k = n).
Figure 23.
Normalized amount of data loss E(Q) vs. Ps for various MDS coding schemes; declustered data placement (k = n).
(a) MDS(3,1)
(b) MDS(9,6)
(c) MDS(16,12)
(d) MDS(14,10)
Figure 24.
EAFDL vs. Ps for various MDS coding schemes; declustered data placement (k = n).
(a) MDS(3,1)
(b) MDS(9,6)
(c) MDS(16,12)
(d) MDS(14,10)
Figure 25.
Normalized E(H) vs. Ps for various MDS coding schemes; declustered data placement (k = n).
37
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(a) MTTDL ratios
(b) EAFDL ratios
(c) E(H) ratios
Figure 26.
Ratios of the MTTDL, EAFDL, and E(H) metrics for the MDS(9,6), MDS(16,12), and MDS(14,10) schemes to those corresponding to the 3-way
replication scheme; symmetric data placement with k = n/2.
(a) MTTDL ratios
(b) EAFDL ratios
(c) E(H) ratios
Figure 27.
Ratios of the MTTDL, EAFDL, and E(H) metrics for the MDS(9,6), MDS(16,12), and MDS(14,10) schemes to those corresponding to the 3-way
replication scheme; declustered data placement (k = n).
VII.
CONCLUSIONS
The effect of latent sector errors on the reliability
of erasure-coded data storage systems was investigated. A
methodology was developed for deriving the Mean Time to
Data Loss (MTTDL) and the Expected Annual Fraction of
Data Loss (EAFDL) reliability metrics analytically. Closed-
form expressions capturing the effect of unrecoverable la-
tent errors were obtained for the symmetric, clustered and
declustered data placement schemes. We demonstrated that
the declustered placement scheme offers superior reliability
in terms of both metrics. We established that, for realistic
unrecoverable sector error rates, MTTDL is adversely affected
by the presence of latent errors, whereas EAFDL is not.
We considered several real-world erasure coding schemes and
demonstrated their efficiency. The analytical reliability expres-
sions derived enable the identification of storage-efficient data
placement configurations that yield high reliability.
Applying these results to assess the effect of network
rebuild bandwidth constraints is a subject of further inves-
tigation. The reliability evaluation of erasure-coded systems
when device failures, as well as unrecoverable latent errors
are correlated is also part of future work.
APPENDIX A
We consider the direct path −−→
UFu = 1 → 2 → · · · →
u → UF and proceed to evaluate PUFu(R1, ⃗αu−1), the prob-
ability of entering exposure level u through vector ⃗αu−1 ≜
(α1, . . . , αu−1) and given a rebuild time R1, and then en-
countering an unrecoverable failure during the rebuild process
at this exposure level. It follows from (20) that
PUFu(R1, ⃗αu−1) = Pu(R1, ⃗αu−2) · Pu→UF(R1, ⃗αu−1) . (66)
It follows from Eq.(111) of [13] by setting ˜r = u that
Pu(R1, ⃗αu−2) ≈ (λb1R1)u−1
u−1
Y
i=1
˜ni
bi
(Vi αi)u−1−i .
(67)
Given that the elements of ⃗αu−2 are independent random
variables approximately distributed according to (9), such that
E(αk
i ) ≈ 1/(k + 1), we have
E
 u−1
Y
i=1
αu−1−i
i
!
=
u−1
Y
i=1
E(αu−1−i
i
) ≈
u−1
Y
i=1
1
u − i =
1
(u − 1)!.
(68)
Unconditioning (67) on ⃗αu−2 using (68) yields
Pu(R1) ≈ (λb1R1)u−1
1
(u − 1)!
u−1
Y
i=1
˜ni
bi
V u−1−i
i
.
(69)
Unconditioning (69) on R1 and using (6) and (8) yields (21).
We now proceed to calculate Pu→UF(R1, ⃗αu−1). Upon
entering exposure level u, the rebuild process attempts to
restore the Cu most-exposed codewords, each of which has
m − u remaining symbols. Let us consider such a codeword,
38
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

and let Lu be the number of symbols permanently lost and Iu
be the number of symbols in the codeword with unrecoverable
errors. Owing to the independence of symbol errors, Iu follows
a binomial distribution with parameter Ps, the probability that
a symbol has a unrecoverable error. Thus, for i = 0, . . . , m−u,
P(Iu = j) =
m − u
j

P j
s (1 − Ps)m−u−j ,
(70)
≈
m − u
j

P j
s ,
for Ps ≪
1
m − u − j , (71)
such that
E(Iu) =
m−u
X
j=1
j P(Iu = j) = (m − u) Ps .
(72)
Clearly, the symbols lost due to the device failures can be
corrected by the erasure coding capability only if at least l of
the remaining m − u symbols can be read. Thus, Lu = 0 if
and only if Iu ≤ m−u−l or, by virtue of (3), Iu ≤ ˜r−1−u.
Thus, the probability qu that a codeword can be restored is
qu = P(Lu = 0) = 1 − P(Iu > ˜r − u) ,
(73)
which, using (70), yields (25).
Note that if a codeword is corrupted, then at least one of
its l user-data symbols is lost. Owing to the independence of
symbol errors, codewords are independently corrupted. Con-
sequently, the conditional probability PUF|Cu of encountering
an unrecoverable failure during the rebuild process of the Cu
codewords is
PUF|Cu = 1 − qCu
u
,
for u = 1, . . . , ˜r .
(74)
Substituting (11) into (74) and using (24) yields
Pu→UF(R1, ⃗αu−1) ≈ 1−q
C Qu−1
j=1 Vj αj
u
= 1−ˆq
Qu−1
u j=1 αj
. (75)
Substituting (75) into (66) yields
PUFu(R1, ⃗αu−1) ≈ Pu(R1, ⃗αu−2)

1 − ˆq
Qu−1
j=1 αj
u

.
(76)
Unconditioning (76) on ⃗αu−1 and using (67) yields
PUFu(R1) ≈ Pu(R1) − (λb1R1)u−1
 u−1
Y
i=1
˜ni
bi
V u−1−i
i
!
· E⃗αu−1
" u−1
Y
i=1
αu−1−i
i
!
ˆq
Qu−1
j=1 αj
u
#
. (77)
LEMMA 1: For αi ∼ U(0, 1) and for all q ∈ R, it holds
that
E
" u−1
Y
i=1
αu−1−i
i
!
q
Qu−1
i=1 αi
#
=
1
(u − 1)! + log(q)−(u−1)
 
q −
u−1
X
i=0
log(q)i
i!
!
.
(78)
Proof: It holds that
q
Qu−1
i=1 αi = e log(q) Qu−1
i=1 αi =
∞
X
j=0
log(q)j (Qu−1
i=1 αi)j
j!
,
(79)
which implies that
 u−1
Y
i=1
αu−1−i
i
!
q
Qu−1
i=1 αi
=
 u−1
Y
i=1
αu−1−i
i
! 

∞
X
j=0
log(q)j (Qu−1
i=1 αi)j
j!


=
∞
X
j=0
log(q)j Qu−1
i=1 αu−1−i+j
i
j!
.
(80)
Consequently,
E
" u−1
Y
i=1
αu−1−i
i
!
q
Qu−1
i=1 αi
#
=
∞
X
j=0
log(q)j Qu−1
i=1 E(αu−1−i+j
i
)
j!
≈
∞
X
j=0
log(q)j Qu−1
i=1
1
u−i+j
j!
=
∞
X
j=0
log(q)j
(u − 1 + j)! =
1
(u − 1)! +
∞
X
j=1
log(q)j
(u − 1 + j)!
(81)
=
1
(u − 1)! + log(q)−(u−1)
∞
X
i=u
log(q)i
i!
=
1
(u − 1)! + log(q)−(u−1)
 ∞
X
i=0
log(q)i
i!
−
u−1
X
i=0
log(q)i
i!
!
=
1
(u − 1)! + log(q)−(u−1)
 
elog(q) −
u−1
X
i=0
log(q)i
i!
!
From (69) and (78), (77) yields
PUFu(R1) ≈ − (λb1R1)u−1
 u−1
Y
i=1
˜ni
bi
V u−1−i
i
!
· log(ˆqu)−(u−1)
 
ˆqu −
u−1
X
i=0
log(ˆqu)i
i!
!
.
(82)
Unconditioning (82) on R1, and using (6) and (8), yields (23).
APPENDIX B
At exposure level u, when Iu ≥ m − u − l + 1 = ˜r − u,
the number Lu of lost symbols is Iu + u. Consequently, the
expected number E(Lu) of lost symbols is
E(Lu) =
m−u
X
i=˜r−u
(i + u) P(Iu = i) ,
(83)
where P(Iu = i) is given by (70). Considering approximation
(71), it follows that
E(Lu) ≈ ˜r
m − u
˜r − u

P ˜r−u
s
,
for Ps ≪
1
m − ˜r .
(84)
39
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

The expected number E(SU|Cu) of symbols lost due to
unrecoverable failures during the rebuild of the Cu codewords
at exposure level u is equal to Cu E(Lu), which yields
E(SU|Cu)
(84)
≈ Cu ˜r
m − u
˜r − u

P ˜r−u
s
, Ps ≪
1
m − ˜r .
(85)
Substituting (11) into (85) yields
E(SU|⃗αu−1) ≈ C


u−1
Y
j=1
Vj αj

 ˜r
m − u
˜r − u

P ˜r−u
s
.
(86)
Subsequently, the expected number E(SUFu|R1, ⃗αu−1) of
symbols lost due to unrecoverable failures encountered during
rebuild in conjunction with entering exposure level u through
vector ⃗αu−1, and given a rebuild time R1, is determined as
follows:
E(SUFu|R1, ⃗αu−1) = Pu(R1, ⃗αu−1) E(SU|⃗αu−1) .
(87)
Substituting (67) and (86) into (87) yields
E(SUFu|R1, ⃗αu−1) ≈ (λb1R1)u−1
"u−1
Y
i=1
˜ni
bi
(Vi αi)u−i
#
· C ˜r
m − u
˜r − u

P ˜r−u
s
,
Ps ≪
1
m − ˜r .
(88)
From (68), we have that E(Qu−1
i=1 αu−i
i
) = E(Qu
i=1 αu−i
i
) ≈
1/u!. Thus, unconditioning (88) on ⃗αu−1 yields
E(SUFu|R1) ≈ (λb1R1)u−1
 u−1
Y
i=1
˜ni
bi
V u−i
i
!
1
u! C ˜r
·
m − u
˜r − u

P ˜r−u
s
,
Ps ≪
1
m − ˜r .
(89)
Unconditioning (89) on R1, and using (6) and (8), yields
E(SUFu) ≈ (λ c)u−1 E(Xu−1)
[E(X)]u−1
 u−1
Y
i=1
˜ni
bi
V u−i
i
!
1
u! C ˜r
·
m − u
˜r − u

P ˜r−u
s
,
Ps ≪
1
m − ˜r .
(90)
Substituting (90) into (37) yields (38).
Remark 6: From (21), (47), (84), and (90), it follows that
E(SUFu) ≈ Pu E(Cu) E(Lu) .
(91)
Upon entering exposure level u, the expected number
E(SU|Cu) of symbols lost due to unrecoverable failures during
the rebuild of the Cu codewords is equal to Cu E(Lu), as
determined by (85). Consequently, upon entering exposure
level u, the expected number E(SU) of symbols lost due to
unrecoverable failures during the rebuild of the most-exposed
codewords is E(Cu) E(Lu). Therefore, the expected number
E(SUFu) of symbols lost due to unrecoverable failures at
exposure level u is obtained by also considering the probability
Pu of entering exposure level u, as determined by (91).
Note that when entering exposure level ˜r, for each of the
C˜r most-exposed codewords there are ˜r symbols permanently
lost. Therefore, the number of data symbols permanently lost
is C˜r ˜r. Consequently,
E(SDF) ≈ PDF E(C˜r) ˜r .
(92)
Substituting (92) into (36), and using (44), yields (39).
REFERENCES
[1]
I. Iliadis, “Reliability assessment of erasure-coded storage systems with
latent errors,” in Proceedings of the 14th International Conference on
Communication Theory, Reliability, and Quality of Service (CTRQ),
Apr. 2021, pp. 15–24.
[2]
D. A. Patterson, G. Gibson, and R. H. Katz, “A case for redundant arrays
of inexpensive disks (RAID),” in Proceedings of the ACM International
Conference on Management of Data (SIGMOD), Jun. 1988, pp. 109–
116.
[3]
P. M. Chen, E. A. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson,
“RAID: High-performance, reliable secondary storage,” ACM Comput.
Surv., vol. 26, no. 2, Jun. 1994, pp. 145–185.
[4]
V. Venkatesan, I. Iliadis, C. Fragouli, and R. Urbanke, “Reliability of
clustered vs. declustered replica placement in data storage systems,” in
Proceedings of the 19th Annual IEEE/ACM International Symposium
on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), Jul. 2011, pp. 307–317.
[5]
I. Iliadis, D. Sotnikov, P. Ta-Shma, and V. Venkatesan, “Reliability of
geo-replicated cloud storage systems,” in Proceedings of the 2014 IEEE
20th Pacific Rim International Symposium on Dependable Computing
(PRDC), Nov. 2014, pp. 169–179.
[6]
M. Malhotra and K. S. Trivedi, “Reliability analysis of redundant arrays
of inexpensive disks,” J. Parallel Distrib. Comput., vol. 17, no. 1, Jan.
1993, pp. 146–151.
[7]
A. Thomasian and M. Blaum, “Higher reliability redundant disk arrays:
Organization, operation, and coding,” ACM Trans. Storage, vol. 5, no. 3,
Nov. 2009, pp. 1–59.
[8]
I. Iliadis, R. Haas, X.-Y. Hu, and E. Eleftheriou, “Disk scrubbing versus
intradisk redundancy for RAID storage systems,” ACM Trans. Storage,
vol. 7, no. 2, Jul. 2011, pp. 1–42.
[9]
V. Venkatesan, I. Iliadis, and R. Haas, “Reliability of data storage
systems under network rebuild bandwidth constraints,” in Proceedings
of the 20th Annual IEEE International Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Aug. 2012, pp. 189–197.
[10]
J.-F. Pˆaris, T. J. E. Schwarz, A. Amer, and D. D. E. Long, “Highly
reliable two-dimensional RAID arrays for archival storage,” in Pro-
ceedings of the 31st IEEE International Performance Computing and
Communications Conference (IPCCC), Dec. 2012, pp. 324–331.
[11]
I. Iliadis and V. Venkatesan, “Most probable paths to data loss: An
efficient method for reliability evaluation of data storage systems,” Int’l
J. Adv. Syst. Measur., vol. 8, no. 3&4, Dec. 2015, pp. 178–200.
[12]
——, “Expected annual fraction of data loss as a metric for data storage
reliability,” in Proceedings of the 22nd Annual IEEE International
Symposium on Modeling, Analysis, and Simulation of Computer and
Telecommunication Systems (MASCOTS), Sep. 2014, pp. 375–384.
[13]
——, “Reliability evaluation of erasure coded systems,” Int’l J. Adv.
Telecommun., vol. 10, no. 3&4, Dec. 2017, pp. 118–144.
[14]
I. Iliadis, “Reliability evaluation of erasure coded systems under rebuild
bandwidth constraints,” Int’l J. Adv. Networks and Services, vol. 11,
no. 3&4, Dec. 2018, pp. 113–142.
[15]
——, “Data loss in RAID-5 storage systems with latent errors,” in
Proceedings of the 12th International Conference on Communication
Theory, Reliability, and Quality of Service (CTRQ), Mar. 2019, pp.
1–9.
[16]
——, “Data loss in RAID-5 and RAID-6 storage systems with latent
errors,” Int’l J. Adv. Software, vol. 12, no. 3&4, Dec. 2019, pp. 259–
287.
[17]
Amazon Web Services, ”Amazon Simple Storage Service (Amazon
S3),” 2022. [Online]. Available: http://aws.amazon.com/s3/ [retrieved:
December 7, 2022]
40
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[18]
D. Borthakur et al., “Apache Hadoop goes realtime at Facebook,” in
Proceedings of the ACM International Conference on Management of
Data (SIGMOD), Jun. 2011, pp. 1071–1080.
[19]
R. J. Chansler, “Data availability and durability with the Hadoop
Distributed File System,” ;login: The USENIX Association Newsletter,
vol. 37, no. 1, Feb. 2012, pp. 16–22.
[20]
K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop
Distributed File System,” in Proceedings of the 26th IEEE Symposium
on Mass Storage Systems and Technologies (MSST), May 2010, pp.
1–10.
[21]
S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google file system,”
in Proceedings of the 19th ACM Symposium on Operating Systems
Principles (SOSP), Oct. 2003, pp. 29–43.
[22]
D.
Borthakur.
HDFS
and
Erasure
Codes
(HDFS-RAID),
Aug.
2009. [Online]. Available: https://hadoopblog.blogspot.com/2009/08
[retrieved: December 7, 2022]
[23]
B. Calder et al., “Windows Azure Storage: a highly available cloud
storage service with strong consistency,” in Proceedings of the 23rd
ACM Symposium on Operating Systems Principles (SOSP), Oct. 2011,
pp. 143–157.
[24]
D. Ford, F. Labelle, F. I. Popovici, M. Stokely, V.-A. Truong, L. Barroso,
C. Grimes, and S. Quinlan, “Availability in globally distributed storage
systems,” in Proceedings of the 9th USENIX Symposium on Operating
Systems Design and Implementation (OSDI), Oct. 2010, pp. 61–74.
[25]
S. Muralidhar et al., “f4: Facebook’s Warm BLOB Storage System,”
in Proceedings of the 11th USENIX Symposium on Operating Systems
Design and Implementation (OSDI), Oct. 2014, pp. 383–397.
[26]
C. Huang, H. Simitci, Y. Xu, A. Ogus, B. Calder, P. Gopalan, J. Li,
and S. Yekhanin, “Erasure coding in Windows Azure Storage,” in
Proceedings of the USENIX Annual Technical Conference (ATC), Jun.
2012, pp. 15–26.
[27]
E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large
disk drive population,” in Proceedings of the 5th USENIX Conference
on File and Storage Technologies (FAST), Feb. 2007, pp. 17–28.
[28]
A. Dholakia, E. Eleftheriou, X.-Y. Hu, I. Iliadis, J. Menon, and K. Rao,
“A new intra-disk redundancy scheme for high-reliability RAID storage
systems in the presence of unrecoverable errors,” ACM Trans. Storage,
vol. 4, no. 1, May 2008, pp. 1–42.
[29]
I. Iliadis, “Reliability modeling of RAID storage systems with latent
errors,” in Proceedings of the 17th Annual IEEE/ACM International
Symposium on Modeling, Analysis, and Simulation of Computer and
Telecommunication Systems (MASCOTS), Sep. 2009, pp. 111–122.
[30]
V. Venkatesan and I. Iliadis, “Effect of latent errors on the reliability
of data storage systems,” in Proceedings of the 21st Annual IEEE
International Symposium on Modeling, Analysis, and Simulation of
Computer and Telecommunication Systems (MASCOTS), Aug. 2013,
pp. 293–297.
[31]
——, “A general reliability model for data storage systems,” in Proceed-
ings of the 9th International Conference on Quantitative Evaluation of
Systems (QEST), Sep. 2012, pp. 209–219.
[32]
M. Silberstein, L. Ganesh, Y. Wang, L. Alvisi, and M. Dahlin, “Lazy
means smart: Reducing repair bandwidth costs in erasure-coded dis-
tributed storage,” in Proceedings of the 7th ACM International Systems
and Storage Conference (SYSTOR), Jun. 2014, pp. 15:1–15:7.
[33]
K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and
K. Ramchandran, “A ”Hitchhiker’s” guide to fast and efficient data
reconstruction in erasure-coded data centers,” in Proceedings of the
2014 ACM conference on SIGCOMM, Aug. 2014, pp. 331–342.
[34]
DELL/EMC Whitepaper, ”PowerVault ME4 Series ADAPT Software,”
Feb. 2019. [Online]. Available: https://www.dellemc.com/ [retrieved:
December 7, 2022]
[35]
I. Iliadis and V. Venkatesan, “Rebuttal to ‘Beyond MTTDL: A closed-
form RAID-6 reliability equation’,” ACM Trans. Storage, vol. 11, no. 2,
Mar. 2015, pp. 1–10.
[36]
T. J. E. Schwarz, Q. Xin, E. L. Miller, D. D. E. Long, A. Hospodor,
and S. Ng, “Disk scrubbing in large archival storage systems,” in
Proceedings of the 12th Annual IEEE/ACM International Symposium
on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), Oct. 2004, pp. 409–418.
[37]
A. Oprea and A. Juels, “A clean-slate look at disk scrubbing,” in
Proceedings of the 8th USENIX Conference on File and Storage
Technologies (FAST), Feb. 2010, pp. 57–70.
[38]
B. Schroeder, S. Damouras, and P. Gill, “Understanding latent sector
errors and how to protect against them,” ACM Trans. Storage, vol. 6,
no. 3, Sep. 2010, pp. 1–23.
[39]
M. Zhang, S. Han, and P. P. C. Lee, “SimEDC: A simulator for the
reliability analysis of erasure-coded data centers,” IEEE Trans. Parallel
Distrib. Syst., vol. 30, no. 12, 2019, pp. 2836–2848.
[40]
V. Venkatesan and I. Iliadis, “Effect of codeword placement on the
reliability of erasure coded data storage systems,” in Proceedings of the
10th International Conference on Quantitative Evaluation of Systems
(QEST), Sep. 2013, pp. 241–257.
[41]
——, “Effect of codeword placement on the reliability of erasure coded
data storage systems,” IBM Research Report, RZ 3827, Aug. 2012.
[42]
I. Iliadis and X.-Y. Hu, “Reliability assurance of RAID storage sys-
tems for a wide range of latent sector errors,” in Proceedings of the
2008 IEEE International Conference on Networking, Architecture, and
Storage (NAS), Jun. 2008, pp. 10–19.
[43]
Seagate,
exos
x20,
data
sheet.
[Online].
Available:
https://www.seagate.com/products/enterprise-drives/exos-x/x20/
[re-
trieved: December 7, 2022]
[44]
Backblaze
drive
stats
for
2021.
[Online].
Available:
https://www.backblaze.com/blog/backblaze-drive-stats-for-2021/
[re-
trieved: December 7, 2022]
[45]
M. Ovsiannikov, S. Rus, D. Reeves, P. Sutter, S. Rao, and J. Kelly,
“The quantcast file system,” in Proceedings of the 39th International
Conference on Very Large Data Bases (VLDB), vol. 6, no. 11.
VLDB
Endowment, Aug. 2013, pp. 1092–1101.
41
International Journal on Advances in Telecommunications, vol 15 no 3 & 4, year 2022, http://www.iariajournals.org/telecommunications/
2022, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

