A Software Design Tool for the Modeling of Emotions in Autonomous Agents
Xavier Gonz´alez-Olvera, Luis-Felipe Rodr´ıguez, Luis A. Castro, and Ram´on Ren´e Palacio
Department of Computer Science and Design,
Instituto Tecnol´ogico de Sonora, M´exico
Email: alternx7@gmail.com, {luis.rodriguez, luis.caztro, ramon.palacio}@itson.edu.mx
Abstract—Cognitive agent architectures implement a series of
computational models of cognitive and affective functions to
enable the emergence of behaviors in autonomous agents. The
design phase of these computational models is usually based on
ﬁndings about the information processing in the human brain
and on principles and standards established for the development
of software systems. Software tools and methodologies to assist in
this design process are still limited and scarce. Moreover, although
available software tools cover most phases of the software devel-
opment cycle, these do not take advantage of theories and models
formulated in ﬁelds that study the brain information processing
and architecture. In this paper, we propose a software design tool
to assist in the speciﬁcation and design of computational models
of emotions for autonomous agents. We develop a preliminary
case study in order to demonstrate the use of the proposed tool.
This tool is intended to create consistent computational models
of emotions that conform to theories and models that explain the
process of human emotions.
Keywords–Autonomous Agent; Computational Model of Emo-
tion; Software Design.
I.
INTRODUCTION
It has been recognized in ﬁelds such as psychology and
neuroscience that human behavior is the result of the oper-
ation and interaction of brain processes such as perception,
emotions, and decision-making [1]. In particular, a variety of
brain structures and neural pathways have been identiﬁed as
the architectural components of these cognitive and affective
processes [2]. In the ﬁeld of computer science, the underlying
cognitive architectures (CA) of autonomous agents (AAs)
are usually designed to include components that imitate the
mechanisms of these cognitive and affective functions [3] [4].
The main assumption is that from the joint operation of these
types of computational models of brain processes (CMB) will
stem believable and human-like behavior.
Computational models included in CAs have a dual basis:
1) a theoretical support that validates the behaviors imple-
mented, and 2) a computational basis that ensures the quality
and adequacy of their development process. The design of
these models is based on theories that explain human cognitive
and affective functions [1] [2]. Moreover, their computational
implementation has to follow principles and standards estab-
lished to assist in the development cycle of software systems
[5]. Although there is a considerable number of software tools
and methodologies designed to assist in the development of
conventional software systems (e.g., those for industry) [5],
they do not seem totally appropriate to assist the construc-
tion of CMBs. These tools have been designed under the
assumption that all system requirements are established by
users. However, for computational models of cognitive and
affective processes additional types of requirements must be
considered. In this case, part of the requirements comes from
user’s observations of human behaviors, but the rest must be
formulated from formal and well-founded evidence about the
functioning of human processes. In this manner, nonsensical
speciﬁcations can be prevented by considering theories and
models developed in disciplines concerned with the under-
standing of the mechanisms and processes that underlie human
behaviors. Unfortunately, few tools recognize these needs in
the development of CMBs [6] [7].
In particular, the computational modeling of human emo-
tions can lead to inconsistencies, complications, or even con-
tradictions. This partly occurs because there is a wide variety
of theories that study human emotions in very different ways.
For example, the mechanisms assumed to be involved in the
process of human emotions vary depending on the theory
under consideration [8]. Nevertheless, despite these constraints
posed by the dual nature of CMBs, several computational
models of emotions (CMEs) have been proposed [9]. EMotion
and Adaptation (EMA) is a model of emotions based on
the appraisal theory and used in the development of virtual
humans [10]. A Layered Model of Affect (ALMA) is a model
that endows conversational agents with emotions, mood, and
personality [11]. Methodology for Analysis and Modeling of
Individual Differences (MAMID) is a model that associates
two concepts: a generic methodology to model the inﬂuences
of emotion on cognitive processing and an affective-cognitive
architecture that implements such methodology.
In this paper, we propose a software design tool (CME-
tool) for the construction of computational models of emotions.
In particular, this CME-tool is designed to assist in the
speciﬁcation and design phases of the development of this type
of computational model. The CME-tool acknowledges the dual
nature of the computational modeling of emotions by providing
a suitable framework to organize theories that explain human
brain processes, which in turn guide development of CMEs.
The paper is structured as follows. In Section II we present
the related work. Afterwards, in Section III we introduce the
proposed CME-tool and how it assists in the speciﬁcation and
design of integrative CMEs. Finally, we discuss future work
in Section IV and provide concluding remarks in Section V.
II.
RELATED WORK
Research on understanding and facilitating the computa-
tional modeling of emotions has led to the creation of new
tools and methodologies. The Emotion Markup Language
(EmotionML) is a general purpose annotation language used
for representing affective aspects in human-machine interactive
systems [6]. It is proposed by the World Wide Web Consortium
(W3C) Multimodal Interaction Working Group as an attempt
to standardize the description of emotions in three main
contexts: (1) manual annotation of texts, videos and anything
that involves emotional data; (2) accurate representations of
emotional aspects captured from user’s expressions, postures,
372
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

speech, etc.; and (3) for comprehensible generation of emo-
tional responses from user interfaces. Although a speciﬁcation
of the syntax of EmotionML is still in progress, several
elements may already be used.
Wang [12] proposed a set of denotational mathematics for
the rigorous and formal description of cognitive processes and
nature-inspired systems. These are expressive mathematical
means that emerge in the framework of cognitive informatics, a
discipline concerned with the internal information processing
mechanisms of natural intelligence. Two particular instances
of denotational mathematics are concept algebra and real-
time process algebra. The former is appropriate to rigor-
ously manipulate abstract concepts in a formal and coherent
framework, which leads to the construction and treatment
of more complex knowledge representations. The latter has
been developed as a coherent notation system and formal
methodology to algebraically denote and model the behaviors
and architectures of systems and human cognitive and affective
processes [12].
Cognitive Objects within a Graphical EnviroNmenT (CO-
GENT) [7] is a software tool with a visual environment for
the computational design and modeling of high-level cog-
nitive processes. COGENT allows the creation and testing
of cognitive models using box-arrow diagrams composed
of functional components with their respective interactions.
Conceptually, these components represent cognitive processes
such as memory systems, knowledge networks, and decision
procedures, which are embodied in computational structures
such as memory buffers, knowledge bases, and connectionist
networks. This tool provides an appropriate environment for
executing and testing the developed models.
Although the literature presents some proposals that can
be helpful in the construction of CMEs, these have primarily
been designed to represent and formally describe cognitive and
affective processes in general. Nevertheless, most proposals
seem complementary and appropriate to assist in certain phases
in the development of integrated models of emotions. In par-
ticular, the CME-tool proposed in this paper is focused on the
construction of models similar to those developed in COGENT,
but at a lower level. Whereas COGENT deals with high-level
cognitive functions, we are concerned with modeling their
structural basis supported on theories and models from ﬁelds
that study the brain functioning.
III.
THE CME-TOOL
Computational models of emotions can be included in CAs
using one of the following two schemas: as stand-alone models
or as integrated models. The former refer to components that
are separately developed and then included as extensions of
existing cognitive architectures, extending their functionality
by providing affective processing. The latter have to do with
emotional models that are designed and implemented as part
of these cognitive frameworks. They may not be implemented
as a single architectural component.
The beneﬁts and disadvantages of each approach vary. The
use of stand-alone models allows the rapid and easy integration
of emotional mechanisms in CAs using pre-tested components.
Architectures use these components by sending them raw data
and receiving back emotionally processed information. On the
other hand, models of emotions designed and implemented
within integrated environments such as CAs follow a more
TABLE I. BRAIN FUNCTIONS AND STRUCTURES USED TO CREATE
SPECIFICATION AND DESIGN DIAGRAMS IN THE CME-TOOL
Brain
Function
Description
Related brain structures
Perception
Emotions inﬂuence perceptual
processes to give a greater
interpretation to novel emo-
tional stimuli
Sensory System, Thalamus,
Hippocampus, Sensory Cor-
tex, Association Cortex
Learning
The emotional level of the
perceived stimuli is used to
provide a measure to learned
knowledge
Amygdala,
Hippocampus,
Sensory Cortex, Association
Cortex
Memory
Emotions inﬂuence the stor-
ing, retaining, and recalling of
knowledge in AAs
Amygdala,
Hippocampus,
Dorsolateral Cortex, Sensory
Cortex, Association Cortex,
Ventromedial Cortex
Attention
Emotions facilitate AAs to
concentrate on the salient ele-
ments of the environment and
discard the irrelevant ones
Amygdala,
Sensory Cortex,
Association Cortex
Emotions
This process encodes stimuli
in order to generate emotion-
ally driven behaviors in AAs
Thalamus,
Sensory
Cortex,
Association
Cortex,
Hippocampus, Amygdala
Planning
Emotions alter the creation of
sequences of actions that will
lead to an expected result
Orbitofrontal Cortex, Dorso-
lateral Cortex, Ventromedial
Cortex, Sensory Cortex, As-
sociation Cortex, Amygdala,
Hippocampus
Decision-
Making
Emotions assist to this process
to select the next action when
a “rational” decision cannot
be made
Orbitofrontal Cortex, Dorso-
lateral Cortex, Basal Ganglia,
Ventromedial Cortex
Motor-Action
Fast and survival reactions are
most of the time driven by
emotions
Basal Ganglia, Motor Cor-
tex, Association Cortex, So-
matosensory Cortex
natural design. These models are not seen as individual ar-
chitectural extensions, but as processes that emerge from the
joint operation of multiple mechanisms. In this case, given that
multiple processes and their interactions must be understood
to generate emotional data, their development becomes more
complicated. However, although this approach is more error
prone, many beneﬁts can be gained. For example, because
these models are built on the basis of the actual process of
human emotions, they can conveniently be adapted to include
other cognitive and affective processes.
The proposed CME-tool is developed to assist in the speci-
ﬁcation and design of integrative CMEs as part of CAs, which
are additionally restricted by ﬁndings about the functioning
and architecture of human brain processes. Table I shows
the brain functions and brain structures currently included as
functional and architectural components in the CME-tool for
the generation of speciﬁcation and design diagrams. They are
well documented in the literature as processes that inﬂuence
or are inﬂuenced by emotions [1] [2] [13].
From a high level perspective, the design process in the
CME-tool begins with the deﬁnition of users’ requirements for
an emotional autonomous agent (EAA). This process ends with
the architectural design of a CME to be included in the CA
of this EAA. Figure 1 shows the steps of this design process,
which are explained in the following subsections.
A. Speciﬁcation Phase
In this phase, all requirements are translated into a high-
level diagram composed of functional modules and their re-
spective interactions. Two main steps are followed to accom-
plish this: (1) user requirements are grouped (or decomposed)
so that they can be placed in predeﬁned modules that represent
affective and cognitive brain functions such as emotions,
373
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

Especiﬁcation 
environment 
of the 
CME-tool
Especiﬁcation Phase
Theoretical
support
Design 
environment 
of the 
CME-tool
High-level
Diagram
Theoretical
support
Design Phase
User 
requirements 
for an EAA
CME
Figure 1. Design phases of the CME-tool.
Decision 
making
Emotions
Figure 2. Speciﬁcation environment of the CME-tool.
perception, and attention; (2) in order to completely meet the
user’s speciﬁcations, explicit operations, interactions, and other
parameters should be deﬁned for each of these modules.
The CME-tool automates this procedure by providing a
series of pre-built functional modules representing speciﬁc
brain functions (see Table I). These functional modules are
selected by users to form a functional diagram that meets the
requirements of the deﬁned CME. The parameters of each
module are conﬁgured using available information provided
by theories that explain the functioning and architecture of
the brain process these modules represent. Figure 2 shows the
speciﬁcation environment of the CME-tool, which includes a
work space designed to organize the high-level diagram and a
series of ready-for-use icons that represent brain processes.
The following list shows the information provided to deﬁne
each component, which appears when a component is selected
in the work space (see left part of Figure 3):
1)
The role of this component: key aspects related to the
functioning of the brain process it represents.
2)
How emotions inﬂuence this component: roles that
emotions play on the brain function it represents.
3)
How this component inﬂuences emotions: how the
results of this component inﬂuence emotion process-
ing.
4)
Data this component receives: information necessary
for its functioning.
5)
Data this component sends: information sent to com-
municate emotion-related aspects.
6)
User parameters: open space for the user to introduce
information.
At the moment the user selects one of these attributes, a
window with more speciﬁc information is available. Given that
there are diverse theories that formulate different hypotheses to
explain the same phenomena, for each parameter the CME-tool
may provide several explanations. The displayed information
encourages the selection of theories that best ﬁt the user’
design goals. Thus, by following this procedure, all parameters
of the entire diagram are deﬁned.
Decision 
making
Figure 3. Left: attributes corresponding to the decision-making component.
Right: Additional information about the decision-making process.
To demonstrate how this procedure works in the CME-
tool, we consider a single requirement from the speciﬁcation
of an EAA whose decision-making should be inﬂuenced by
emotions: the EAA should be able to make decisions that
meet both emotional and rational rewards, thus maintaining
a general emotional balance. From this requirement, an initial
diagram with two components is drawn, see Figure 2.
In this diagram when the user selects the decision-making
component, its corresponding list of attributes appears (see
left part of Figure 3.) The right part of Figure 3 presents the
window that appears when the second parameter is selected; it
presents information to deﬁne such attribute. The format that
follows the data shown in this window has two ﬁelds for each
entry. One contains the explanation of the attribute and the
other the information of the referred theory.
Following this procedure, as the requirements in the speci-
ﬁcation of the EAA are analyzed, new components may be
incorporated, or even, only new interactions or operations
between the already considered modules may be necessary to
meet the whole speciﬁcation. In this manner, once the attributes
of all components are deﬁned, the speciﬁcation of the EAA
should be totally understood from the resulting diagram.
B. Design Phase
The CME-tool generates an architectural design by re-
ﬁning the functional diagram resulting in the speciﬁcation
phase. In the design phase, all components representing brain
functions in the speciﬁcation diagram are decomposed into
brain-inspired structures that embody mechanisms to allow the
emergence of speciﬁc behaviors.
The procedure to achieve the architectural design is similar
to that carried out in the speciﬁcation process. Once we
have the functional diagram completed, we proceed to open
it in the design environment included in the CME-tool (see
Figure 4.) This environment also provides pre-built modules
that take the role of brain structures, such as the amygdala,
hippocampus, and thalamus (see Table I). In this manner, when
a component of the functional diagram is chosen, related pre-
built modules are shown, allowing the user to select those that
meet its particular design goals and according to the parameters
established in the previous stage.
Each structural component should also be fully deﬁned.
The information needed to do that is provided and classiﬁed
in the same manner as with the functional components of the
speciﬁcation phase (see the list above and Figure 3). This
information is also based on theories, models, and concepts
addressing aspects related to the processing of human emo-
tions. This guides the decomposition of the modules in the
functional diagram in a coherent way.
374
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

Amygdala
Hippocampus
Thalamus
OrbitoFrontal
Cortex
DorsoLateral
Prefrontal C.
VentroMedial
Prefrontal C.
Visual
Cortex
Figure 4. Design environment of the CME-tool.
To continue with the example (Figure 2), when the emo-
tional module is selected, a number of components assuming
the role of brain structures are shown. They are those supposed
to be involved (directly or indirectly) in this particular process.
After that, users are able to select those structures that are
related to the processing of emotions in their particular design.
Similarly, when we select the decision making component,
structures involved with the functioning of this module appear,
which allows the user to follow the same procedure (the work
space in Figure 4 shows a possible architectural design for this
example).
The design process implemented by the CME-tool facil-
itates the construction of CMEs for AAs, which are based
on the brain mechanisms underlying human emotions. The
ﬁrst phase of the CME-tool allows users to put together
desirable characteristics and functions in EAAs in a simple
and organized way. The second phase assists in the translation
of these abilities to an architectural design, which enables the
emergence of proper emotionally-driven behaviors in EAAs.
This method promotes the design of AAs by modeling brain
functions, which emerge from the interaction of a number
of components that embody algorithms implementing mech-
anisms that imitate the brain functioning and architecture.
Moreover, since most of the initial user requirements for the
development of CMEs are not in terms of how emotions arise
or what are the components of the emotional process, but in the
form of emotions as inﬂuencing certain cognitive processes,
the approach of the CME-tool is adequate.
IV.
FUTURE WORK
The following list presents some future work:
1. Improvements for an accurate guidance in the com-
position of the functional and design diagrams are being
considered. The CME-tool will provide clues to prioritize
users’ requirements and to translate desired behaviors in AAs
to brain functions, as well as to assign precedence to compo-
nents constituting its functional diagram when they are being
decomposed into an architectural design.
2. We are considering to design a mechanism to extend
the CME-tool with information about the processes in which
emotions are not playing a main role. For example, the direct
operation between planning and decision making. To do that,
a more robust knowledge base may be implemented, which
allows users to include additional theories in the CME-tool in
a consistent and meaningful way, as well as to connect the
CME-tool with other available knowledge bases.
3. To consider the implementation process in the CME-tool,
architectural components in the design diagram may include
additional information of computational techniques to appro-
priately implement the process they are embodying, as well
as to include user’s ﬁelds to precisely indicate data structures,
functions, and other data useful for the implementation phase.
4. Finally, automatic generation of complementary docu-
mentation for an easier understanding of speciﬁcations and
designs is also to be implemented.
V.
CONCLUSION
In this paper, we proposed a new software tool to assist
in the speciﬁcation and design of computational models of
emotions as part of cognitive agent architectures. This tool
was designed taking into account three basic assumptions: (1)
it aims at developing integrated models, (2) it was inspired by
the functioning and architecture of the mechanisms underlying
human emotions, and (3) it was developed to address the
speciﬁcation and design phases of this type of computational
models. This paper emphasizes the importance of considering
the study on human mechanisms to design computational
models of brain processes, as well as the signiﬁcance of
implementing adequate computational tools to assist in this
complex task.
REFERENCES
[1]
E. R. Kandel, J. H. Schwartz, and T. M. Jessell, Principles of Neural
Science, 4th ed., J. Butler and H. Lebowitz, Eds.
New York, United
States of America: McGraw-Hill, 2000.
[2]
E. A. Phepls, “Emotion and cognition: Insights from studies of the
human amygdala,” Annual Review of Psychology, vol. 57, 2006, pp.
27–53.
[3]
P. Langley, J. E. Laird, and S. Rogers, “Cognitive architectures: Re-
search issues and challenges,” Cognitive Systems Research, vol. 10,
no. 2, 2009, pp. 141–160.
[4]
E. Hudlicka, “Beyond cognition: Modeling emotion in cognitive ar-
chitectures.” in In Proceedings of the International Conference on
Cognitive Modeling (ICCM)., 2004, pp. 118–123.
[5]
I. Sommerville, Software Engineering, 8th ed.
Addison Wesley, 2006.
[6]
P. Baggia, F. Burkhardt, A. Oltramari, C. Pelachaud, C. Peter, and
E. Zovato. Emotion markup language (emotionml) 1.0. [Online].
Available:
http://www.w3.org/TR/emotionml/
[retrieved:
December,
2015]
[7]
R. Cooper and J. Fox, “Cogent: A visual design environment for
cognitive modeling,” Behavior Research Methods, vol. 30, no. 4, 1998,
pp. 553–564.
[8]
A. Moors, “Theories of emotion causation: A review,” Cognition and
Emotion, vol. 23, 2009, pp. 625–662.
[9]
S. Marsella, J. Gratch, and P. Petta, “Computational models of emotion.”
in A Blueprint for Affective Computing: A Source Book, 1st ed., K. R.
Scherer, T. B¨anziger, and E. B. Roesch, Eds.
Oxford University Press,
2010, pp. 21–46.
[10]
S. C. Marsella and J. Gratch, “Ema: A process model of appraisal
dynamics,” Cognitive Systems Research, vol. 10, no. 1, 2009, pp. 70–
90.
[11]
P. Gebhard, “Alma: a layered model of affect,” in Proceedings of
the fourth international joint conference on Autonomous agents and
multiagent systems, 2005, pp. 29–36.
[12]
Y. Wang, Software Engineering Foundations: A Software Science
Perspective.
Auerbach Publications, 2008.
[13]
J. Panksepp, Affective neuroscience: the foundations of human and
animal emotions.
New York: Oxford University Press, 1998.
375
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

