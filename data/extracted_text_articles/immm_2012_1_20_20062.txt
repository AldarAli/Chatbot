Applying of Sentiment Analysis for Texts in Russian  
Based on Machine Learning Approach 
Nafissa Yussupova 
Faculty of informatics and robotics 
Ufa State Aviation Technical University 
Ufa, Russian Federation 
yussupova@ugatu.ac.ru 
Diana Bogdanova 
Faculty of informatics and robotics 
Ufa State Aviation Technical University 
Ufa, Russian Federation 
dianochka7bog@mail.ru
Maxim Boyko 
Faculty of informatics and robotics 
Ufa State Aviation Technical University 
Ufa, Russian Federation 
russian_max@inbox.ru 
 
 
Abstract—This paper considers the problem of Sentiment 
classification in text messages in Russian with using Machine 
Learning methods - Naive Bayes classifier and the Support 
Vector Machine. One of the features of the Russian language is 
using of a wide variety of declensional endings depending on 
the declination, tenses, grammatical gender. Another common 
problem of sentiment classification for different languages is 
that different words can have the same meaning (synonyms) 
and thus give equal emotional value. Therefore, our task was to 
evaluate on how the lemmatization affects the sentiment 
classification accuracy (or another, with endings and without 
them), and to compare the results for Russian and English 
languages. For evaluating the impact of synonymy, we used the 
approach when the words with the same meaning are grouping 
into a single term. To solve these problems we used 
lemmatization and synonyms libraries. The results showed that 
using lemmatization for texts in Russian improves the accuracy 
of sentiment classification. On the contrary, the sentiment 
classification of texts in English without using lemmatization 
yields better result. The results also showed that the use 
synonymy in the model has a positive influence on accuracy. 
In the "Introduction", we describe a place Sentiment Analysis 
in Data Mining. In the "Approaches to the Sentiment 
Analysis", we tell about the main approaches of Sentiment 
Analysis: linguistic approach, an approach based on Machine 
Learning, and their combination. In the "Description of 
algorithms for Sentiment Analysis", we state the problem of 
sentiment classification and describe methods for solving it 
using a Naïve Bayesian classifier, Bagging, Support Vector 
Machine. In the "Results of experiments", we describe aims of 
the experiment and the features of the implementation of the 
algorithm and report the results of the experiment. In the 
"Conclusion", we present the output from the results. 
Keywords-text analysis; analysis of tonality; sentiment 
analysis; machine learning. 
I. 
 INTRODUCTION 
The present stage of human development is characterized 
by rapid growth of information. One of the most common 
forms of storage is the text in natural language. Textual form 
of information is natural for human beings and they readily 
accept it. The development of information technologies is 
accompanied by intense growth in the number of websites, 
which currently stands at more than 285 millions, and as a 
consequence of increasing the volume of text data. The vast 
amount of information collected in numerous text databases 
that are stored in personal computers, local and wide area 
networks. Average user is becoming more difficult to work 
with huge amounts of data. Reading the texts of the volume, 
manual search and analysis of relevant information in giant 
arrays of text data are ineffective. To solve this problem and 
to 
automatically 
process 
the 
information, 
many 
developments were done in the areas of natural language 
processing, information retrieval, machine translation, 
information extraction, sentiment analysis and others. 
The article is devoted to the Sentiment Analysis of 
Russian text messages using Machine Learning [19]. 
Sentiment Analysis in the text is one of the directions in the 
analysis of natural language texts. Sentiment is the emotional 
score, which is expressed in the text. It can have one-
dimensional emotive space (two classes of sentiments) or 
multivariate (more than two). Foresight sentiment of the text 
lies in the fact that based on textual information, it allows 
you to evaluate the success of the campaign, political and 
economic reforms, to identify relevant press and media to a 
certain person, to an organization for the event, to determine 
how consumers relate to a particular product, to services to 
the organization. In [1], Boyko et al. consider applying 
Sentiment Analysis to the study opinions of consumer of 
different banks. 
Despite the promise of this direction, while it is not as 
actively used in text processing systems. The reasons are the 
difficulties of highlight the emotional vocabulary in the texts, 
a imperfection of the existing text analyzers, dependence on 
the domain. Therefore, the improvement and development of 
new analytical methods based on machine learning is an 
urgent task. 
8
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-227-1
IMMM 2012 : The Second International Conference on Advances in Information Mining and Management

The article presents the results of a study of Sentiment 
classification of texts in Russian with using Machine 
Learning. 
II. 
APPROACHES TO THE SENTIMENT ANALYSIS 
There are three approaches of Sentiment Analysis of text 
messages. 
1) Sentiment Analysis based on pre-defined dictionaries 
of tonality with linguistic analysis. Tonality dictionaries 
consist of elements such as words, phrases, patterns, each of 
which has its own emotional coloring. Tonality of the text is 
determined by the combination of emotive language found 
and evaluated in text. 
2) Sentiment Analysis based on methods of Machine 
Learning. The text presents in vector form; the classifier is 
trained according to the available training data. After that, it 
is possible to classify the sentiments in new text message. 
3) The combination of the first and the second 
approaches. 
The first approach is rather time-consuming because of 
the need for a tonality of dictionaries, a list of tonality 
patterns and the development of language parsers, but it is 
more flexible. The advantage of this approach is that it 
allows you to see the emotional vocabulary at the level of the 
sentence. 
In [2], Pazelskaya et al. present an algorithm for 
Sentiment Analysis based on the tonal dictionaries consisting 
of several steps: morphological analysis of text mark-up 
vocabulary lists for the tonality vocabulary, syntactic 
analysis, and directly determine the tonality. The algorithm 
can be estimated on the website [3]. 
In [4], Ermakov et al. developed the following algorithm 
for estimating the tonality of the text, which includes 
recognition of the object of tonality, parsing text, selection 
and classification of propositions that express the tonality, 
the assessment based on the general tonality of all the 
tonality propositions. 
Abroad, there was an active search to improve the 
analysis of tonality on the basis of tonality dictionaries and 
linguistic analysis, e.g , Nasukawa et al. [5]. This work 
describes the analyzer, which performs the following actions: 
1) remove the special terminology of the text, and 2) 
determine the tonality, and 3) analysis of the associative 
relationship. The analyzer uses two linguistic system: a dial 
tonality dictionary and database templates. 
The approach is based on using Machine Learning, 
presupposes the existence of pre-marked-up the training set 
of data. The purpose of training in Sentiment Analysis is to 
get the necessary and sufficient rules, which you can use to 
make a classification of tonality of the new text messages, 
similar to those that made up the training set. The drawback 
of algorithms based on Machine Learning is dependence on 
the quality and quantity of training data. This approach does 
not allow an in-depth analysis of the text, to identify the 
object and the subject of tonality. 
Machine Learning methods for solving the problem of 
Sentiment classification of messages are actively developing 
overseas. In the Russian practice of science are not yet 
known cases of successful application of Machine Learning 
to Sentiment Analysis. Therefore, we consider some of the 
work of foreign authors. 
A great contribution to the development of Sentiment 
Analysis of text messages was done by researchers from 
Cornell University B. Pang and L. Lee [6], [7], [8]. In 2008, 
Pang and Lee published the book «Opinion Mining and 
Sentiment» [6] devoted to modern methods and approaches 
to Sentiment Analysis in text messages. In [7], a Sentiment 
classification using Machine Learning was published and 
they showed that this approach is superior to a simple 
technique based on the compilation of dictionaries of 
commonly used positive and negative words. Pang and Lee 
[8] describe an algorithm that allows us to classify 
sentiments using only subjective sentences. Objective 
proposals generally do not have the emotional coloration, but 
create noise in the data. 
O'Keefe and Koprinska [9] consider the problem that 
from the training data extracts a very large number of terms. 
The authors describe methods for selecting the most 
informative terms, and evaluation of their tonality. 
To address the shortcomings of the above approaches is 
used to combine them. Thus, in [10], the method used is 
based on the extracted lexical rules; training with the 
participation of man and machine learning are combined into 
a sentiment classification algorithm. 
König and Brill  [11], from Microsoft  suggest ways to 
get sentiment patterns using proposed algorithm. The result 
is achieved through automatic extraction of informative 
patterns with subsequent evaluation of tonality, combining 
with Support Vector Machine (SVM). 
The combined approach is promising, as it combines 
advantages of the first two approaches. Here, an important 
task for the study is to determine how the Linguistic 
approach and Machine Learning should interact with each 
other. 
III. 
DESCRIPTION OF ALGORITHMS FOR SENTIMENT 
ANALYSIS 
In this paper, we consider algorithms which are based on 
using Machine Learning approach. As Machine Learning 
algorithms we chose a Naive Bayesian classifier [12] and 
Support Vector Machine [20]. For improving the accuracy 
of classification, we considered a Meta-Machine Learning 
algorithm [16] - Bagging for Naive Bayesian classifier. 
Mathematically, the problem of classifying of sentiment 
can be represented as follows. There are two classes - the 
class of positive messages,   , and class of negative 
messages,    , (1): 
}
,
{
C  c1 c2
,                                  (1) 
there is a set of messages (2): 
  
}
,...,
,
{
2
1
dn
D  d d
,  
              (2) 
and an unknown classification function (3): 
{ 1,0 }
:
F C D 
.                          (3) 
We need to build a classifier F' as close to the 
classification function F as possible. We have a labelled set 
of messages for learning (4). 
Dl
C
K


,                               (4) 
9
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-227-1
IMMM 2012 : The Second International Conference on Advances in Information Mining and Management

where 
l
D is learning set of messages. 
Feature space in the this problem can be represented 
using the vector model. Each text message is treated as a set 
of words (“bag” of words). This view of a text message is 
presenting as a point in multidimensional space. Points lying 
close to each other correspond to semantically similar 
messages. In this model, a sequence of words is ignored. For 
example, the "a good book" and "the book is good" is the 
same. Thus, the message is a "bag" with the words. 
A. Naive Bayes classifier 
Let us consider Naive Bayesian classifier for sentiment 
classification problem. Let each message   takes the values 
from the dictionary  , and is described by a set of words 
            . There is a set of classes          ,  
consisting of a class of positive messages and a class of 
negative messages. We need to find the most probable value 
of the corresponding class of the set of words (5): 
      
)
,...,
,
|
(
argmax
2
1
n
j
C
c
NB
w
w w
c
p d
c
j



     (5) 
It is known that the conditional probability of an event 
can be found using the Bayes theorem (6) [21]: 
)
,...,
,
(
)
(
)
|
,...,
,
(
)
,...,
,
|
(
2
1
2
1
2
1
n
j
j
n
n
j
w
w
w
p
c
p d
c
d
w
w
w
p
w
w
w
c
d
p






         (6) 
Then, the expression (5) takes the form (7): 
)
,...,
,
(
)
(
)
|
,...,
,
(
max
arg
2
1
2
1
n
j
j
n
C
c
NB
w
w w
p
c
p d
c
d
w
p w w
c
j





   (7) 
From the expression (7), we are interested only in the 
numerator, because the denominator does not depend from 
the class. Thus, the denominator is a constant and can be 
reduced. Assuming conditional independence of attributes, 
we obtain the expression (8) which is using for 
classification: 
)
(
)
|
, ...,
,
(
arg max
2
1
c
p d
c
d
w
w
p w
c
n
c C
NB





  (8) 
Naive Bayesian classifier operates under the following 
assumptions: 
 
words and phrases in the message are independent 
from each other; 
 
do not takes into account the sequence of words; 
 
do not takes into account the length of the message. 
There are two ways to implement a Naive Bayesian 
classifier – a Bernoulli model [12] and multinomial model 
[12]. The difference is that in the Bernoulli model is 
considering the presence of a word in a message. In the 
multinomial model, the number of occurrences of a word in 
the text is considered. Table 1 provides an example of a 
vector notation of the text. 
TABLE I.  
EXAMPLE OF VECTOR FORM 
 
Vector description 
Bernoulli model 
[0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0] 
Multinomial model 
[0, 0, 2, 1, 0, 3, 1, 2, 0, 0, 0] 
 
Let us consider the sentiment classification algorithm 
with the Bernoulli model, presented by Manning et al. [12]. 
In the Bernoulli model, the message is described by the 
vector consisting of the attributes with values 0 or 1. Thus, 
we consider only the presence or absence of words in the 
message; then, we ignore how many times it is repeated in 
the message. 
Given a vocabulary          
   ; then, the message    is 
described by the vector of length    , consisting of bits    . 
If a word    appears in the message    then      , if not 
then      . Then, the likelihood of belonging to a class      
of messages    can be calculated by the formula (9): 
)))
|
(
1(
)
1(
)
|
(
(
)
|
(
|
|
1
j
t
it
j
t
it
V
t
j
i
c
p w
b
c
p w
b
c
d
p








             (9) 
For learning a classifier it needs to find the 
probabilities          . Let there be a training set of 
messages        
    , which has labels of classes   , then it is 
possible to calculate estimates of the probabilities that a 
particular word occurs in a particular class ( 10): 








D
i
i
j
D
i
i
j
it
j
t
d
c
p
d
p c
b
c
w
p
1
1
)
|
(
2
)
|
(
1
)
|
(
            (10) 
A priori probabilities of classes can be calculated by the 
formula (11): 
 
|D|
|d )
p(c
)
p(c
|D|
i
i
j
j



1
                       (11) 
Then, the classification will be carried out by the 
formula (12). 
))]]
|
(
)(1
1(
)
|
(
log[
))
|
(
[log(
max
arg
)
|
(
)
(
max
arg
|
|
1
|
|
1
j
t
it
j
t
it
V
t
D
i
i
j
C
c
j
i
j
C
c
NB
c
p w
b
c
w
p
b
d
c
p
c
p d
p c
c
j
j
















       (12) 
From (10), it follows that the some probabilities will be 
zero, since that some words can be presented in one class of 
training data and can be absent in another. Difficulties arise 
with zero probabilities when they are multiplied in (12). In 
this case, the entire expression is zero and there is a loss of 
information. To avoid zero probability of obtaining used 
add-one, or Laplace smoothing [12], which consists of 
adding one to the numerator (13). 
          








D
i
i
j
D
i
i
j
it
j
t
d
c
p
d
p c
b
c
w
p
1
1
)
|
(
2
)
|
(
1
)
|
(
               (13) 
Sentiment classification algorithm using the Bernoulli 
model is shown in Figures 1 and 2. It consists of learning 
part and classifying part. In the learning part there are input 
parameters is a set of labelled messages and set of classes. 
In this part creates a dictionary of wordsV , that estimates 
      and         , sets the threshold value h which 
minimize the classification error. Output is a fully trained 
classifier with set parameters. Classifying part applies for 
new message, which sentiment must be determined. 
In the multinomial model, see Manning et al. [12], the 
message is a sequence of random selection of some word 
10
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-227-1
IMMM 2012 : The Second International Conference on Advances in Information Mining and Management

from the dictionary. This model takes into account the 
number of repetitions of each word in a one message, but 
ignores words that are absence in the message. 
Given a vocabulary          
   ; then, the message    
can be described by the vector of length    , consisting of 
words, which is taken from the dictionary with probability 
        . Then, the likelihood of belonging of messages     
to a class      estimates by formula (14). 
it
K
j
t
V
t
it
i
i
j
i
c
p w
K
d
p d
c
p d
)
|
(
!
1
!
)
(
)
|
(
1




,   (14) 
where     - is the number of occurrences of word    in the 
message   . 
For learning, the classifier also needs to find the 
probabilities         . Let there be a training set of 
messages         
   , which is distributed in classes    and 
we know the number of occurrences of words in the 
message    . Then, we can calculate estimates of the 
probabilities that a particular word occurs in a particular 
class (15). In this case, also apply smoothing add-one. 
 
Input: set of documents               , 
          set of classes           
1. Extract all terms from    to the vocabulary     
2. For each       do 
 
 
 
 
3. Count documents    in each class 
4. Calculate probability             
5. For each       do 
 
 
6. Count documents    
   in class containing 
word    
7. Calculate probability                 
  
            
8. Set threshold   with minimal classification error 
Output:  ,      ,         ,    
Figure 1.  Algorithm of learning NB Bernoulli model 
Input: document  ,  ,      ,         ,   
1. Extract all terms from    to the vocabulary     
2. For each      do 
 
 
 
 
3.                    
4. For each      do 
 
 
5. If       than                          
6. else                             
7. If                        than       else      
Output: tonality of document   
Figure 2.  Algorithm of classification NB Bernoulli model 
 
 









V
s
D
i
i
j
is
D
i
i
j
it
j
t
d
p c
K
V
d
p c
K
c
w
p
1
1
1
)
|
(
)
|
(
1
)
|
(
     
 (15) 
A priori probabilities of classes can be calculated by the 
formula (16). 
D
d
c
p
c
p
D
i
i
j
j
 

1
)
|
(
)
(
                (16) 
Then, the classification will be carried out by the 
formula (17). 
])
|
(
log
))
|
(
max [log(
arg
)
|
(
)
(
max
arg
1
1











V
t
j
t
it
D
i
i
j
j
j
i
j
j
NB
c
p w
K
d
c
p
c
p d
p c
c
      (17) 
Classification algorithm with the Multinomial Naïve 
Bayes model is shown in Figures 3 and 4. It consists of 
learning part and Sentiment classification part. In the 
learning part creates a dictionary of terms V , estimates 
probabilities        and         , set the threshold value of 
h, to minimize the classification error. Classifying part 
applies for new message, which sentiment must be 
determined. 
B. Bagging algorithm 
One of the algorithms for improving the quality of 
classification is called Bagging. It was proposed by L. 
Breinman and describes in [16], Breinman. Bagging 
algorithm is shown in Figure 5. 
From the initial training set of   of length     forms 
training subsets    of the same length     with the bootstrap 
- a random selection with returns. However, some messages 
will appear in a subset of a few times, some - not even once. 
Next, set the control messages by subtracting     . With 
using training subset    learns classifier   . Classification 
error    of    estimates by the control subset      and then 
compared with the admissible error of the classification of 
 . If the error is less than a classifier built admissible error, 
then it is added to the ensemble. Sentiment classification is 
produced with the ensemble of classifiers by a simple 
voting. 
C. Support Vector Machine 
The main idea of Support Vector Machine algorithm is 
to find separating hyperplane, represented by vector     
which minimize empirical error of classification and 
maximize margin between classes. SVM was proposed by 
V. Vapnik, C. Cortez and A. Chervonenkis [20]. SVM is a 
high effective in classification problems and has popularity 
among Machine Learning algorithms. In particular, it 
outperforms other algorithms of Machine Learning in text 
categorization. The finding of separating hyperplan 
corresponds to a constrained quadratic optimization 
11
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-227-1
IMMM 2012 : The Second International Conference on Advances in Information Mining and Management

problem. Let           be the class of document   ; then, 
the solution can be written as (18): 



j
j
j
j j
c d
w
0
, 

                  (17) 
where    are obtained by solving a dual optimization 
problem. Those     such that    is greater than zero are 
called support vectors, since they are the only document 
vectors contributing to   . Classification of message consists 
of determining which side of    hyperplan it fall on. 
The main disadvantage of Support Vector Machine is 
that it has cubic complexity in the size of dataset and 
requires a lot of computational resources. The cause is that it 
have to solve quadratic optimization problem with the 
number of parameters equal to number of data and to 
compute dot product many times. 
There are many modifications of SVM developed for 
reducing computational time. One of them is Sequentional 
Minimal Optimization algorithm [17], Platt. This algorithm 
is used in this work. It allowed receiving the. It allowed 
receiving the results in acceptable time. 
In this work realized to variants of SVM – first variant 
considers only a presents/absence of features and in the 
second variant considers the number of occurrences of 
features. 
 
Input: set of documents               , 
set of classes           
1. Extract all terms from    to the vocabulary     
2. For each       do 
 
 
 
 
3. Count documents    in each class 
4. Calculate probability             
5. For each       do 
 
 
6. Count number of occurrences     
   of word  
   in each class 
7. Calculate probability               
  
     
    
    
   
   
  
8. Set threshold   with minimal classification error 
Output:  ,      ,         ,    
Figure 3.  Algorithm of learning NB Multinomial model 
Input: document  ,  ,      ,         ,   
1. Extract all terms from    to the vocabulary     
2. For each      do 
 
 
 
 
3.                    
4. For each      do 
 
5. If        than                          
6. If                        than       else      
Output: tonality of document   
Figure 4.  Algorithm of learning NB Multinomial model 
Input:                          , where      – set of 
documents; 
             ,       if positive and       
negative polarity of documents; 
  – number of classifiers in ensemble; 
    - length of training data; 
  – admissible error of classification 
1. For           do 
 
2. Choose randomly     documents for constructing 
training dataset     from    
3. Construct control dataset from       
4. Construct classifier              
5. estimate error    of classifier    on dataset      
6. If      then add classifier      into ensemble  
Output:            
     
 
   
  final ensemble of 
classifiers 
Figure 5.  Bagging algorithm 
IV. 
RESULTS OF EXPERIMENTS 
In this research, we aimed at studying a few points: 
 
Evaluate 
the 
performance 
for 
Sentiment 
classification of text messages in Russian language; 
 
 Compare the performance with results obtained for 
text messages in English language; 
 
 Study the influence of lemmatization on the 
accuracy of classification; 
 
Study the influence of a length of word on the 
accuracy of classification, and 
 
Study the influence of the grouping words, which 
have equal semantic meaning on the accuracy of 
classification. 
According these aims, a program was developed «Text 
Analyzer» in the programming language C#. All listed 
algorithms of Sentiment classification were realized in this 
language.  
For learning and evaluation of the accuracy of the 
sentiment classification, we used the test set, consisting of 
customer reviews of a few Russian banks taken from the 
Internet site [13]. It includes 304 positive reviews and 850 
negative reviews in Russian. An example of review with a 
positive sentiment is: "An application for a loan designed to 
quickly, no questions asked, within 20 minutes." An 
example of negative review: "Consideration of the 
application took time for two months”. 
For evaluation of Sentiment classification for a text in 
English, we used dataset that includes 1000 negative and 
1000 positive reviews about films from IMDB [18].  
For study of influence of lemmatization in the pre-
processing text, lemmatization of all occurring words was 
entered. Lemmatization brings different words to their 
initial form; for example, the noun is the nominative case, 
singular. Motivation for lemmatization of the text is due to 
the fact that different forms of a word can often express the 
same meaning. In this regard, is justified to bring the words 
to a initial form. We used LemmaGen library written in C# 
12
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-227-1
IMMM 2012 : The Second International Conference on Advances in Information Mining and Management

and designed for lemmatization of words. These libraries are 
available on the website of the developer [14].  
To evaluate the generalization capability of the 
algorithm used by a sliding control or cross-validation we 
proceeded as follows. Fixed set consisting of 10 partitions 
of the original sample, each of which in turn consisted of 
two subsamples: the training and control. For each partition, 
configures the algorithm for the training subsample, and 
then evaluated its average error on the objects of the control 
subsample. Assessment of the sliding control was averaged 
over all partitions of the error on the control subsamples; for 
the bagging algorithm accepted allowable error of the 
classifier is equal to e = 25%. 
To evaluate the classification accuracy of each control 
unit we used the indicator "classification accuracy", which 
is calculated by the formula (18): 
100%





FN
FP
TN
TP
TN
TP
Accuracy
,       (18) 
where TP - the number of correctly classified positive 
messages; TN - the number of correctly classified negative 
messages; FP - the number of non correct classified positive 
messages; FN - the number of non correct classified 
negative messages. 
The results of computational experiments are presented 
in the Table 2. Accuracy of classification lies in range 85% - 
88,3%. For Naïve Bayes, the best results obtained by 
Multinomial model 86,83% (Bernoulli – 86,49%) or another 
words by considering number of occurrences of words. 
Using Bagging algorithm has a positive influence on the 
classification. It improved accuracy for NB Multinomial 
model by 0,86%.  
For Support Vector Machine with leaner core, the best 
results obtained by considering presence of word in the 
message (87,69% vs. 85%). Using of polinomial core gave 
86,73% of accuracy.  
For Sentiment classification, we received better results 
on dataset in Russian on an average 5%. This suggests that 
dataset in Russian is a more constrained domain - banking. 
In contrast, dataset in English has a wide range of different 
words, because most reviews have a description of film 
story. Analysis of results also indicates that SVM 
outperforms Naïve Bayes algorithm in two cases of 
language.  
For grouping words that have the same semantic 
meaning, we used a vocabulary of synonyms with 5371 
strings. For example, if in message occur to different words 
“borrow” and “lend” then it is equivalent occurring two 
words “borrow”. This modification allowed to improve 
accuracy by 0,1% for NB, and 0,08% for SVM. It is not so 
much but we hope that using more bigger and specific 
vocabulary of synonyms can give a more significant effect. 
Lemmatization has positive influence for Sentiment 
classification 
of 
text 
in 
Russian 
(87,07% 
without 
lemmatization, 87,69% with lemmatization). It could be 
explained that in Russian language words could have 
different endings. Lemmatization allows to group cognate 
words with one semantic meaning and different endings. In 
text 
in 
English 
the 
best 
result 
received 
without 
lemmatization (84,3% vs. 85,85%).  
TABLE II.  
RESULTS OF EXPERIMENTS WITH TEXTS IN RUSSIAN 
Naïve Bayes classifier 
NB Bernoulli model 
86,49% 
NB Multinomial model 
86,83% 
NB Multinomial model with synonyms 
86,93% 
NB Multinomial model, length > 2 
86,40% 
Bagging NB Bernoulli model  (e=25%) 
86,82% 
Bagging NB Multinomial model  (e=25%) 
87,69% 
Support Vector Machine 
SVM, presence, leaner 
87,69% 
SVM, occurance, leaner 
85,00% 
SVM, presence, leaner, without lemmatizator 
87,07% 
SVM, presence, leaner, length > 2 
88,21% 
SVM, presence, leaner, with synonyms 
87,77% 
SVM, presence, polinomial 
86,73% 
SVM, presence, leaner,  with synonyms, 
length > 2 
88,30% 
TABLE III.  
RESULTS OF EXPERIMENTS WITH TEXTS IN ENGLISH 
Naïve Bayes classifier 
NB Bernoulli model 
80,25% 
NB Multinomial model 
81,05% 
Support Vector Machine 
SVM, presence, leaner 
84,3% 
SVM, occurance, leaner 
83,15% 
SVM, presence, leaner, without lemmatizator 
85,85% 
 
By excluding prepositions and articles from feature 
words, the experiment considered words with length more 
than two letters. This modification gave a better result, with 
0,52% in SVM. But, in NB accuracy descended on 0,43%. 
The best result of 88,30% as obtained by SVM with 
lemmatization, grouping synonyms and length of word > 2. 
V. 
CONCLUSION 
Based on results of Sentiment classification of texts in 
Russian, we obtained the following conclusions: 
 
Machine Learning could provide accuracy of 
sentiment classification 85% - 88,3% for considered texts in 
Russian; 
 
SVM confirmed that it outperforms Naïve Bayes 
algorithm in two cases of language; 
 
Multinomial model surpasses Bernoulli model in 
NB; 
 
Bagging algorithm has a positive influence on the 
classification but little; 
 
presence feature of words surpasses number of 
occurance in SVM; 
 
using 
synonyms 
has 
positive 
influence 
on 
Sentiment classification but little; 
 
lemmatization has positive influence for Sentiment 
classification of text in Russian, but not for text in English. 
13
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-227-1
IMMM 2012 : The Second International Conference on Advances in Information Mining and Management

The task of sentiment classification of text messages has 
a complex nature and requires innovative approaches for 
solution. The complexity of its nature is that the initial data 
are the texts in natural language. Every word of this text has 
its meaning, and the combination of words is a complex 
interaction of the meaning of each word. At present there is 
no universal method of modeling such an interaction in the 
language of the machine or the language of numbers.  
Despite the complexity of the problem, it attracts a large 
number of researchers around the world. Searches in this 
area are actively maintained and there are some 
achievements. Many of the developed algorithms achieve 
classification accuracy greater than 85%. But keep in mind 
that these results were obtained on test data under 
experimental conditions. Unfortunately, there is no official 
information about the real successful practical application of 
systems to solve such problems. 
REFERENCES 
[1] M. Boyko, A. Hilbert, N. Yussupova, and D. Bogdanova, “Marketing 
research of cosumer opinions with using information technologies”, 
Proceedings of the 13th International Workshop on Computer Science 
and Information Technologies, Germany, Garmisch-Paterkirchen, 
September 27 - October 02, 2011, pp. 103-105. 
[2] A.G. Pazelskaya and A.N. Soloviev, “Method of the determination 
emotions in the lyrics in Russian”, Computer program linguistics and 
intellectual technologies, Issue 10 (17), 201, pp. 510-522. 
[3] The official site of "Эр Си О". On-line sentiment classification: 
http://x-file.su/tm/Default.aspx [retrieved: October, 2012]. 
[4] A.E. Ermakov and S.L. Kiselev, “Linguistic model for the computer 
analysis of key media of publications”, Computational Linguistics 
and the intellectual technology: proceedings of the International 
Conference Dialog'2005, 2005, pp. 172-177. 
[5] J. Yi, T. Nasukawa, W. Niblack, and R. Bunescu, “Sentiment 
analyzer: Extracting sentiments about a given topic using natural 
language processing techniques”, In Proceedings of the 3rd IEEE 
international conference on data mining, ICDM 2003 , pp. 427-434. 
[6] B. Pang and L. Lee, “Opinion Mining and Sentiment Analysis. 
Foundations and Trends in Information Retrieval”, Vol. 2, 2008, pp. 
1-135. 
[7] B. Pang and L. Lee, “Thumbs up? Sentiment Classification using 
Machine Learning Techniques”, Proceedings of the Conference on 
Empirical Methods in Natural. Language Processing (EMNLP), 
Philadelphia, July 2002, pp. 79-86. 
[8] B. Pang and L. Lee, “A Sentimental Education: Sentiment Analysis 
Using Subjectivity Summarization Based on Mini-mum Cuts”, 
Proceedings of the ACL, 2004, pp. 271-278. 
[9] T. O'Keefe and I. Koprinska, “Feature selection and weighting 
methods in sentiment analysis”, Australasian Document Computing 
Symposium, 2009, pp. 142-153. 
[10] R. Prabowo and M. Thelwall, “Sentiment analysis: A combined 
approach”, Journal of Informatics in 2009, pp. 143-157. 
[11] A. König and E. Brill, “Reducing the Human Overhead in Text 
Categorization”, Proceedings of KDD, 2006, pp.  598-603. 
[12] C. Manning, P. Raghavan, and H. Schuetze, “An Introduction to 
Information Retrieval”, Cambridge University Press. Cammbridge, 
England, 2009, pp. 1-544. 
[13] Internet portal dedicated to the Russian banks: http://banki.ru 
[retrieved: October, 2012]. 
[14] Portal dedicated to lemmatization and stemming: 
http://lemmatise.ijs.si/Software/Version3 [retrieved: October, 2012]. 
[15] Y. Freund and R. Schapire, “Experiments with New Boosting 
Algorithm”, Machine Learning: Proceedings of the Thirteenth 
International Conference, 1996, pp. 148-156. 
[16] L. Breinman, “Bagging Predictors”, Machine Learning, 24, 1996, pp. 
123-140. 
[17] J. Platt, “Fast training of support vector machines using sequential 
minimal optimization”, Advances in Neural Information Processing, 
Vol. 12, 2000, pp. 547-553. 
[18] Datasets for tests: http://www.cs.cornell.edu/People/pabo/movie-
review-data/ [retrieved: October, 2012] 
[19] R. Michalski, J. Carbonell, and T. Mitchell, “Machine Learning: An 
Artificial Intelligence Approach”, Tioga Publishing Company, ISBN 
0-935382-05-4, 1983, pp. 83-138. 
[20] C. Cortes and V. Vapnik, “Support-Vector Networks”, Machine 
Learning, 20, 1995, pp. 273-297. 
[21] T. Bayes and R. Price, “An Essay towards solving a Problem in the 
Doctrine of Chance. By the late Rev. Mr. Bayes, communicated by 
Mr. Price, in a letter to John Canton, M. A. and F. R. S”,  
Philosophical Transactions of the Royal Society of London 53, 1763,  
pp. 370-418. 
14
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-227-1
IMMM 2012 : The Second International Conference on Advances in Information Mining and Management

