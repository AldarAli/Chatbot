Semantic-based Technique for the Automation the 3D Reconstruction Process 
 
Helmi Ben Hmida, Frank Boochs 
Institut i3mainz, am Fachbereich Geoinformatik und 
Vermessung 
Fachhochschule Mainz, Lucy-Hillebrand-Str. 255128 
Mainz, Germany 
e-mail: {helmi.benhmida, boochs}@geoinform.fh-
mainz.de 
 
Christophe Cruz, Christophe Nicolle 
Laboratoire Le2i, UFR Sciences et Techniques 
 
Université de Bourgogne 
B.P. 47870, 21078 Dijon Cedex, France 
e-mail: {christophe.cruz, cnicolle}@u-bourgogne.fr 
 
 
Abstract—The reconstruction of 3D objects based on point 
clouds data presents a major task in many application field 
since it consumes time and require human interactions to yield 
a promising result. Robust and quick methods for complete 
object extraction or identification are still an ongoing research 
topic and suffer from the complex structure of the data, which 
cannot be sufficiently modeled by purely numerical strategies. 
Our work aims at defining a new way of automatically and 
intelligently processing of 3D point clouds from a 3D laser 
scanner. This processing is based on the combination of 3D 
processing technologies and Semantic Web technologies. 
Therefore, the intention of our approach is to take the human 
cognitive strategy as an example, and to simulate this process 
based on available knowledge for the objects of interest. First, 
this process introduces a semantic structure for the object 
description. Second, the semantics guides the algorithms to 
detect and recognize objects, which will yield a higher 
effectiveness. Hence, our research proposes an approach which 
uses knowledge to select and guide the 3D processing 
algorithms on the 3D point clouds.  
Keywords - Semantic web; knowledge modeling; ontology; 
3D processing; mixed strategy; 3D scene reconstruction; object 
identification 
I. 
 INTRODUCTION 
The laser scanning technology is a powerful tool for 
many applications; it has partially replaced traditional 
surveying methods since it can speed up field work 
significantly. This results in rich datasets with lots of useful 
and useless information. On one hand, the “manual” 
processing of such data set is efficient and robust since a 
human uses his own knowledge for detecting and identifying 
objects in point clouds, but this process is tedious, time-
consuming and expensive. On the other hand, the 
“automatic” processing of 3D point clouds can be very fast 
and efficient, but often it relies on significant interactions 
with the user for controlling algorithms and verifying the 
quality of the results. The WiDop project [24] aims at the 
automatic processing of 3D point clouds using the specialist 
knowledge in order to guide the reconstruction process. By 
this way, the point clouds quantification and qualification 
will not be processed via an intermediary step allowing the 
human intervention  (Figure 1). The principle of the WiDop 
project is a knowledge-based detection of objects in point 
clouds 
for 
AEC 
(Architecture, 
Engineering 
and 
Construction) engineering applications. 
 
 
Figure 1.  Automatic processing compared to the manual one. 
Funded by the German government, the partners of the 
WiDop project are the German railway company (Deutsche 
Bahn), the Fraport company (Frankfurt Airport manager), 
and the Metronome company specializing in 3D point cloud 
processing. The Fraport company main concerns are building 
and furniture management of the airport. The furniture’s 
position relative to the security gates and the trashes are 
constantly moving. In addition, updates are done on 
buildings such as new walls, destruction of walls, new holes 
in a wall, new windows, etc. This could be undertaken by the 
director of a new shop or by the technical employers in order 
to reorganize storerooms for instance. As a matter of fact, it 
is very difficult to keep up to date the plans of the airport. 
The motivation of the Deutsche Bahn Company is the 
management of railway furniture. The issue is closed to the 
Fraport Company because they have to face the management 
of the furniture which changed constantly. The cost of 
keeping these plans up to date is increasing. The solution 
consists to fix on a locomotive a 3D terrestrial laser scanner 
and to survey the surrounding landscape. After the first 
survey, the resulting data will be considered as a reference 
for comparisons with future surveys in order to detect 
changes. As a consequence, both companies will benefit 
from an automatic processing, because too much data has to 
be processed, and the amount of data leads to a tremendous 
management cost. 
Ten years ago, a new format, which seems to be very 
suitable for our purpose was developed by the IAI (The 
International Alliance for Interoperability) it is named the 
IFC format (IFC - Industry Foundation Classes). The 
191
SEMAPRO 2010 : The Fourth International Conference on Advances in Semantic Processing
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-104-5

specification is a neutral data format to describe exchange 
and share information typically used within the building and 
facility management industry. This norm considers the 
building elements as independent objects where each object 
is characterized by a 3D representation and defined by a 
semantic normalized label. Consequently, the architects and 
the experts are not the only ones who are able to recognize 
the elements, but everyone will be able to do it, even the 
system itself. For instance, an IFC door is not just a simple 
collection of lines and geometric primitives recognized as a 
door; it is an “intelligent” object door which has a door 
attributes linked to a geometrical definition. The building 
mock-up for instance is designed by engineers and it 
describes all concrete and abstract elements of a building. 
Thus, it allows each participant in a building project to share 
and exchange information with the standardized description. 
IFC files are made of objects and connections between these 
objects. Object attributes describe the “business semantic” of 
the object. Connections between objects are represented by 
“relation elements” [1]. This format and its semantics are the 
keystone of our solution. 
The following section covers background information on 
works and projects that aim at the reconstruction of 3D 
scenes from 3D point clouds. Section 3 presents a summary 
of the designed solution. Section 4 describes in detail the 
WiDop project. Section 5 focuses on the general model 
conception and the interaction management between the 
different created layers. It gives an overview of the different 
components of the reconstruction process and the basic 
theory of the "WiDop mixed strategy" presented by the 
combination of semantic web technology and 3D processing 
algorithms. Finally, conclusions and suggestions for future 
work are presented. 
II. 
RELATED WORK 
The reconstruction of 3D scene covers a wide area of 
computer vision; such reconstruction is based on the 3D 
processing algorithm extracted from the signal processing 
domain. Recent works aims to reconstruct a scene based on 
semantic networks describing the relationship between the 
scene objects. Based on these observations, this section will 
be articulated in two parts: the first one presents 
reconstruction 
methods 
based 
on 
signal 
processing 
algorithms while the second one describe methods based on 
semantic networks technology. Within a photogrammetric 
domain, there are three classes of methods for 3D scene 
reconstruction: Manual, semi-automatic and automatic 
methods.  
A. 3D Processing Methods  
Within the Terrestrial Scanning Laser (TSL) processing, 
three different main method classes are identified. These 
methods are classified based on their automatic rate. This 
section is articulated in three parts. The first part presents re-
construction methods based on manual processing of 3D 
point of clouds. The second presents the semantic based 
method to assist in the 3D scene reconstruction process and 
finally, the third one shows the automatic processing 
methods. 
1) Manual methods: are completely based on user 
interactions. Such methods allow the user to extract the 
scene elements, which are then converted into 3D models 
with the help of software’s packages.  
2) Semi-automatic methods: in these methods, the user 
initializes the process by some manual measurements based 
on which an algorithm tries to extract other elements. Such 
methods are based on user interactions and automatic 
algorithm processing. They support elements projection, 
affine, and Euclidean geometries [2] for the definition of 
constraints. When modeling buildings by constructive solid 
geometry, buildings can be regarded as compositions of a 
few components with simple roof shapes (such as flat roofs, 
gable roofs and hip roofs). In [3], Vosselman et al. tried to 
reconstruct a scene based on the detection of planar roof 
faces in the generated point clouds based on the 3D Hough 
transform. The used strategy relies on the detection of 
intersection lines and height jump edges between planar 
faces. Once done, the component composition is made 
manually.  
3) Automatic methods: these methods are processed 
without the need of any kind of user intervention. Manual 
methods have been established with the appearance of the 
need to reconstruct 3D scene long time ago and are 
available under a high end commercial feature c.f. Leica® 
[18] or low cost software Dista [16]. Automatic methods use 
various approaches but all are based on segmentation 
techniques to extract features. The methods of Pollefeys et 
al. [4] and Zisserman et al. [5] use the projective geometry 
technique. Pollefeys method divides the task of 3D 
modeling into several steps. The system combines various 
algorithms 
from 
computer 
vision, 
like 
projective 
reconstruction, auto-calibration and depth map estimation. 
The disparity calculation between point pairs makes it 
possible to get a depth map. The depth map is then 
transformed into a volume model composed of voxels. The 
surface estimation between the outer surface voxels and the 
interior surface voxels makes it possible to combine inner 
and outer object parts. The method developed is effective 
and obtains good results. The approach of Zisserman et al. 
[5] proceeds in two steps. First, a coarse surface model of 
the building is carried out. Then the coarse model guides the 
search of details (windows and doors) and refines the 
surface model. The reconstruction uses the detection of 
“vanishing points”, line correspondence, and the estimation 
of points and homologous lines. Vanishing points are 
necessary for the detection of planar primitives with the help 
of the plane-sweeping method. This method has strong 
constraints as it contains three perpendicular dominant 
directions. 
B. Knowledge-based Methods 
All the strategies outlined below are based on signal 
processing algorithms, in the other side, new strategies 
192
SEMAPRO 2010 : The Fourth International Conference on Advances in Semantic Processing
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-104-5

appeared recently. They are based on semantic networks to 
guide the reconstruction like the work of Cantzler et al. [9], it 
aim to improve the structural quality of a 3D model. 
Architectural features like orientation of wall are used. Then, 
the feature’s relationships are automatically extracted using a 
semantic network of the building mock ‘up. The whole 
strategy consists of three steps: the architectural feature 
extraction from triangulated 3D model. Then the automatic 
extraction of constraint out of the scene is carried out by 
matching the planes against a semantic network of the 
building mock ‘up by backtracking research tree. In this step, 
the semantic network concentrates the definition of the 3D 
objects and the relationships between them. The constraints 
such as parallel or perpendicular wall are exploited. The last 
step consists in applying the constraint to the model. 
Consequentially, the original model will be fitted to the new 
constraint model. Ansgar et al. [6] presents a new concept for 
the building reconstruction. Building model is reconstructed 
based on it is topology using Markov model technique. 
Stephane et al. [7], investigates this work into a model based 
reconstruction of complex polyhedral building roofs. The 
roof in question is modeled as a structured collection of 
planar polygonal faces. The modeling is done into two 
different regimes, one focus on geometry, whereas the other 
is rules by semantics. Concerning the geometry regime, the 
3D line segments are grouped into planes and furthers into 
faces using a Bayesian analysis. In the second regime, the 
preliminary geometric model is subject to a semantic 
interpretation. The knowledge gained in this step is used to 
infer missing parts of the roof model (by invoking the 
geometric regime once more) and to adjust the overall roof 
topology.  
C. Discussion 
The problem of automatic object reconstruction remains 
a difficult task to realize in spite of many years of research 
[8]. The major problems are the impact of the viewpoint onto 
the appearance of the object, resulting in changes with 
respect to geometry, radiometry, and existence of occlusions 
and the lack of texture. Strong variations in the viewpoint 
may destroy the adjacency relations of points, especially 
when the object surface shows considerable geometrical 
variations. This dissimilarity causes confusions within 
correspondence determination and is even worse, when 
partial occlusions result in a disappearance of object parts. In 
cases of weak texture, algorithms do not have sufficient 
information to correctly solve the correspondence problem. 
Consequently, the reconstruction fails to give a solution. 
Cantzler et al. [9] and Nüchte et al. [10] tried to solve these 
problems by using semantic information coupled to a scene.  
Planes found in the reconstruction phase are introduced 
into a semantic interpretation, which has to fit to a network 
model [11]. A tree of “backtracking” allows the finding of 
the best mapping between the interpretation of the scene and 
the semantic network model. A coherent labeling exists, if all 
surfaces are labeled. Relations between the nodes of the 
semantic network are used to define geometrical constraints 
between labeled surfaces. The model used and the relations 
between the elements of the model define the knowledge of a 
typical architectural scene. The interpretation of the scene 
then forms a semantic network, which is an instance of the 
architectural model. Actually, we argue that the pervious 
cited works and others do not take in account the context of 
the geometries, and the use of 3D processing algorithms. 
Based on these observations, the idea behind this work is to 
benefit from the knowledge related to the scene structure and 
the different characteristics of geometries mainly to select 
the most suitable 3D processing algorithm from a 3D 
processing algorithm collection. In addition, in order to 
resolve the ambiguities issue of the scene caused by the sited 
constraints, more than one interaction between the semantic 
network and the 3D point clouds data is required. 
In this paper, we claim that the domain of the semantic 
Web, and semantics technologies that it relies on, is of 
benefit for the definition of an automatic processing. One of 
the technologies is a language that helps to define ontologies; 
an evolved version of the semantic networks. Ontologies 
presents one of the most famous technology for knowledge 
modeling, where the basic ideas was to present information 
using graphs and logical structure to make computers able to 
understand and process it easily and automatically [12]. Our 
approach aims to structure knowledge, link geometrical 
objects to semantic information, create rules and finally 
guide the algorithms selection in 3D point clouds processing. 
The created knowledge will be structured in ontology. The 
produced ontology to orient the 3D object identification 
contain variety of data like the GIS data, images capture 
synchronized with the point clouds, information about the 
objects characteristics, the hierarchy of the sub elements, the 
geometrical topology, different processing algorithms etc. In 
the automatic process, the modeled knowledge will provide 
to the system relevant information aiming to orient the 
localization and the identification process. This purpose is 
reached by selecting the most suitable algorithm for the 
object detection and recognition. To achieve it, the ontology 
must contain information about objects characteristics like 
positions, geometrics information, images textures, etc. and 
also about the most suitable detection algorithms for each of 
existent objects.  
III. 
SYSTEM OVERVIEW 
As mentioned above, the automatic processing of 3D 
point clouds can be very fast and efficient, but often relies on 
significant interaction of the user for controlling algorithms 
and verifying the results. Alternatively, the manual 
processing is intelligent and very precise since a human 
person uses its own knowledge for detecting and identifying 
objects in point clouds, but it is very time-consuming and 
consequently inefficient and expensive. If human knowledge 
could be inserted into automatic detection and reconstruction 
algorithms, point cloud processing would be more efficient 
and reliable. However, such a solution involves a lot of 
questions and challenges such as: (1) How can knowledge be 
structured based on heterogeneous sources? (2) How to 
create a coarse model suitable for different applications? (3) 
How to allow a dynamic interaction between the knowledge 
model and the 3D processing part? 
193
SEMAPRO 2010 : The Fourth International Conference on Advances in Semantic Processing
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-104-5

In general, mathematical algorithms contain different 
data processing steps which are combined with internal 
decisions based on numerical results. This makes processing 
inflexible and error prone, especially when the data does not 
behave as the model behind the algorithm expects. We want 
to put these implicit decisions outside, make a semantic layer 
out of it and combine it with the object model. This approach 
is more flexible and can be easily extended, because 
knowledge and data processing are separated. 
The created knowledge will serve to guide the numerical 
algorithms for 3D point cloud processing, based on rules that 
have been created and formalized before. The knowledge 
will be organized in an ontology structure. Knowledge not 
only describes the information of the objects, but also gives a 
framework for the control of the strategies selected. For 
instance, it provides rules for the localization and 
identification process. These rules guide the selection of 
individual algorithms or sequences allowing the detection 
and recognition of the object to be searched for. Once the 
knowledge provides initial information about the structure of 
the scene and the objects, candidate regions can be 
determined. Then, the algorithms integrated in the 
knowledge will be guided to identify objects. In other cases, 
when the existence of objects in the scene is ambiguous, we 
will search them in the point cloud based on updated 
information in the knowledge model. Consequently, 
knowledge-based methods will enable the algorithms to be 
executed reasonably and adaptively on particular situations. 
This is where WiDOP project will try to make a step 
forward. 
IV. 
WIDOP PROCESSING CAPACITY 
The WiDop project aims at the development of efficient 
and intelligent methods for an automated processing of 
terrestrial laser scanner data. Figure 2 presents the general 
coarse architecture for the WiDop project, composed of 3 
parts: the knowledge part, the 3D processing part and the 
interaction management and control part labeled (WiDop 
Processing) ensuring the interaction between the above sited 
parts. In contrast to existing approaches, we aim at the 
utilization of previous knowledge on objects. This 
knowledge can be contained in databases, construction plans, 
as-built plans or Geographic Information Systems (GIS). 
Therefore, this knowledge is the basis for a selective, object-
oriented detection, identification and, if necessary, modeling 
of the objects and elements of interest in the point cloud. 
A. The knowledge processing 
Our approach aims at structuring and modeling the 
existing knowledge in order to represent objects from the 
geometrical and the semantic point of view and to integrate 
important feature characteristics, if necessary. In the second 
step, this knowledge base will guide the numerical 
algorithms for 3D point cloud processing, based on rules that 
have been created and formalized before. This approach also 
follows the concept of Semantic Web, while the knowledge 
will be organized in an ontology structure, where the basic 
idea is to present information in a logical structure to make 
computers able to understand and process it easily and 
automatically.  
 
 
Figure 2.  Overview system 
Our approach is intended to use semantic knowledge 
based on OWL technology for knowledge modeling and 
processing. Knowledge has to be structured and formalized 
based on IFC schema, XML files, etc., using classes, 
instances, relations and rules. An object in the ontology can 
be modeled as presented; a room has elements composed of 
4 walls, a ceiling and a floor. The sited elements are basic 
objects. They are defined by their geometry (plane, 
boundary, .), features (roughness, appearance, etc.), and also 
the qualified relations between them (adjacent wall, 
perpendicular, etc.). The object “room” gets its geometry 
from its elements and further characteristics may be added 
such as functions in order to estimate the existent sub 
elements. For instance a “classroom” will contain “tables”, 
“chairs”, “a blackboard”, etc. The research of the object 
“room” will be based on an algorithmic strategy which will 
look for the different objects contained in the point cloud. 
This means, using different detection algorithms for each 
element, based on the above mentioned characteristics, will 
allow us to classify most of the point region in the different 
element categories. This prior knowledge is modeled in a 
Coarse Model (CM). It corresponds to the spatial structure of 
a building and it is an instance of semantic knowledge 
defined in the ontology. This instance defines the rough 
geometry and the semantics of the building elements without 
any real measurement. For example, a CM may define the 
number of stages, the type of roof, the configuration of the 
walls, the number of rooms per floor, the number of 
194
SEMAPRO 2010 : The Fourth International Conference on Advances in Semantic Processing
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-104-5

windows and doors per wall. In a CM, images and point 
clouds may be used as entry parameters for the process of 
data collection trying to correct the CM. 
B. The 3D processing:  
Numerical processing includes a number of algorithms or 
their combination to process the spatial data. Strategies 
include geometric elements detection (straight line, plane, 
surface, etc.), projection - based region estimation, histogram 
matrices, etc. All of these strategies are either under the 
guidance of knowledge, or use the previous knowledge to 
estimate the object intelligently and optimally. Alongside 
with 3D point clouds various types of input, data sets can be 
used such as images, range images, point clouds with 
intensity or color values, point clouds with individual images 
oriented to them or even stereo images without point cloud. 
All sources are exploited for application to particular 
strategies. Knowledge not only describes the information of 
the objects, but also gives a framework for the control of the 
selected strategies. The success rate of detection algorithms 
using RANSAC [21], Iterative Closest Point [22] and Least 
Squares Fitting [23] should significantly increase by making 
use of the knowledge background. However, we are planning 
not only to process point data sets but also based on a surface 
and 
volume 
representation 
like 
mesh 
and 
voxels, 
respectively. These methods will be selected in a flexible 
way, depending on the semantic context. 
C. The WiDop processing: 
In order to manage the interaction between the 
knowledge part and the 3D processing part, a new layer 
labeled the WiDop processing is created. This layer ensures 
the control and the management of the information 
transaction and the decision taken, based on several steps, as 
outlined in Figure 3. The steps are: 
• 
The algorithmic strategy selection. 
• 
The update of the Coarse Model. 
• 
The topological search of new objects. 
• 
The semantic characterization of new objects. 
 
In the next section, the mixed strategy based on the 
WiDop processing layer is presented in detail in order to 
show the different interactions that takes place during the 
WiDop reconstruction process between the knowledge base 
and the 3D processing algorithms. 
 
V. 
THE INTERACTION MANAGMENT 
We propose a mixed strategy based on WiDop 
processing layer insuring the interaction between the 
knowledge base and the 3D processing algorithms (Figure 
2). It presents an intermediary between the semantic based 
strategy and the 3D processing one. In this section, the global 
view of our mixed strategy will be presented and the ultimate 
interaction between both of parts is described. 
As seen in Figure 3, the mixed strategy is based on two 
principles axes which are the geometric resolution based on 
the 3D processing domain and the semantic one based on the 
semantic web technology.  
 
 
Figure 3.  The mixed strategy, a system overview. 
Such strategy can be divided in two main steps: The first 
step is the geometric quantification, detection and 
recognition of the different existent objects in the coarse 
model. In this phase, the purpose of the processing is the 
detection of the defined objects in the coarse model. This is 
ensured by linking the high level semantic object definition 
in the coarse model and the correspondent portion of the 
point clouds. The second one aims at the semantic 
characterization of new objects in point clouds is based on 
the topologic relations. The inference will be based on the 
"Coarse model" CM and on the detected and localized 
objects. In this step, based on the relation´s interpretation and 
the interference rules management, new objects in the point 
cloud will be inferred and detected automatically (Figure 3). 
In order to focus on our method for the combination of 
the semantic web technology and the 3D processing 
algorithms, Figure 4 illustrates an UML sequence diagram 
that represents the general design of the proposed solution. 
Hence, the purpose is to create a more flexible, easily 
extended approach where algorithms will be executed 
reasonably and adaptively on particular situations. The 
system architecture is divided into four actors: the data base, 
the 3D processing, the WiDop processing and the knowledge 
base.  
To simplify the illustration, we will use a single data set 
type. In fact, we are limited in working on point clouds 
generated by a laser scanner. This does not mean that we will 
not profit from others resources like images, panoramic 
images, videos, etc. For this reason, the mentioned source 
presenting the fourth actor in the diagram is a laser scanner 
providing millions of point clouds. For the rest of the section, 
the real mechanism related to our solution will be 
disambiguated in details beginning by how our ontology is 
created, how knowledge are linked to the 3D processing 
algorithms arriving to how objects are detected and semantic 
model is updated. 
195
SEMAPRO 2010 : The Fourth International Conference on Advances in Semantic Processing
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-104-5

 
 
Figure 4.  The sequence diagram of interactions between the laser scanner, the 3D processing,the knwoldege processing and the knowledge base.  
 
A. The Ontology Creation 
The WiDop project deals with the creation of an ontology 
corresponding to the project requirements. In this field, two 
different strategies for the ontology creation can be used. In 
the first one, the ontology is created manually depending on 
our vision and on the business knowledge provided by the 
specialists of the domain. Such ontology will look like a 
bottom up ontology [13], [14], very precise and designed for 
a specific domain. In the second one, it can be automatically 
generated based on different sources like ontologies from 
different domains such as the transport, the railway and the 
geometric ones [15]. The generation of the ontology can be 
also done based on software’s packages thanks to many tools 
like the XML2OWL  [17], [19]. It serves to map XML files 
provided from Metronome Db Clear Suite software used for 
the 
management 
of 
the 
Deutsche 
Bahn 
point  
cloud´s, allowing a manual tagging of the different selected 
elements and describing the general structure for the railway 
domain to an OWL file. It can also be ensured by the 
IFC/XML tools mapping IFC files for the building 
management structure to OWL one. From our point of view, 
the WiDop ontology must respect the applied areas 
specification (railway or Fraport). Based on this observation, 
our ontology is created automatically in order to have a 
general model then adapted manually to respect the real 
scene characteristics. The schema extracted from the XML 
data base provided from the DB Clear suite software, will be 
exploited to facilitate the automatic population of our 
ontology. Once our knowledge base is created and 
populated, it will be used as an entry for the WiDop project 
(Figure 5). 
 
 
Figure 5. Portion of the developped ontology describing the "Algorithm" 
class 
B. Integrating Knowledge in 3D object detection  
The proposed approach couples the semantic web 
technology represented by the knowledge to the 3D 
processing one represented by the 3D processing algorithms. 
Let’s remember that the idea behind this project is to direct, 
adapt and select the most suitable algorithms based on the 
196
SEMAPRO 2010 : The Fourth International Conference on Advances in Semantic Processing
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-104-5

objects characteristics. In fact, one algorithm could not detect 
and recognize different existent objects in the 3D point 
clouds, since they are distinguished by different shapes, size 
and capture condition. The role of knowledge is to provide 
not only the object's characteristics (shape, size, color, etc.) 
but also object's status (visibility, correlation) to algorithmic 
part, in order to adjust its parameters to adapt with current 
situation, Table 1. Based on theses observation, we draw 
links from algorithms to objects based on the similar 
characteristics, as Figure 6 shows. 
 
 
 
Figure 6. Linking algorithms and objects. 
The knowledge part controls one or more algorithms for 
the detection of objects. In order to carry out this detection, 
we benefit from the experience of experts in 3D processing. 
This experience helps to find a match between the object’s 
characteristics and the algorithm’s characteristics. Actually, a 
certain algorithm can be used for the detection of a certain 
object in a certain context. The set of characteristics are 
determined by the object’s properties such as geometrical 
features and appearance. Then, the role of the knowledge is 
also to provide the algorithms that can detect and recognize 
these characteristics. These characteristics are considered as 
values and it can change the parameters of the algorithms. 
After the detection of an object, there is a module that gives 
a feedback about the status of the detected object according 
to the knowledge part and in order to adjust the algorithms to 
improve the robustness. Due to these frequent updates, the 
combination of knowledge and the 3D processing becomes 
relevant and flexible, c.f. Table1. 
TABLE 1. THE CHARACTERISTICS LIST OF ALGORITHM'S AND OBJECT'S 
INPUT 
No 
Characteristics 
1 
Geometry (plane, sphere, arc) 
2 
Corner 
3 
2D boundary 
4 
Size 
5 
Orientation 
6 
Appearance (colour, surface material) 
7 
Visibility 
8 
Correlative position 
C. The Geometry Processing 
The third part in this model is the digital treatments. This 
part will focus on the object detection based on the prior 
knowledge and the selected algorithm. As seen in Figure 4, 
once the algorithms are affected, the 3D processing layer will 
provide the generated point clouds from the laser scanners, it 
will also be provided with the different pieces of information 
relative to each object in the ontology. The 3D layer must 
have as information: 
• 
The object label 
• 
The object location coordinate 
• 
The object spatial coordinate 
• 
The eventual 3D shape of the object 
• 
The sub-elements composing the object 
• 
The object complexity rate 
• 
The most suitable detection algorithm to use 
• 
……. 
 
Depending on the object complexity, there are two 
possible scenarios. In the case of a low complexity rate, the 
objects can be detected automatically based on a template 
matching algorithm [20]. Else, the objects will be 
decomposed into elementary sub-objects as shown in the 
area B of Figure 4. Once detected, an evaluation process will 
estimate the detection quality rate (Figure 4, area C), and a 
topological reconstruction of the root element will be 
executed (Figure 4, area D). Once the coarse model elements 
are detected and recognized, (Figure 4, area E), the semantic 
research of new objects step is stimulated. 
D. The Semantic Qualification 
The described technique for the geometric qualification 
of the coarse model above aims at the detection of the 
maximum existing elements in the CM. Normally, a real 
scene should always contain extra object or unexpected one. 
To ensure a high detection quality rate, we suggest a second 
main module for our strategy aiming to identify new object 
in the coarse model. In fact, knowledge contains a reasoning 
capacity able to infer logical consequences from a set of 
asserted facts. Our model will be able to infer new objects 
and relations based on the coarse model topological relation 
and on the detected and identified elements (Figure 4, area 
F).  
VI. 
CONCLUSION AND FUTURE WORK 
The proposed approach for 3D object recognition in point 
clouds labeled "Mixed Strategy" present our initial work and 
vision on the project. It aims to improve the object 
localization and the scene reconstruction leading to a more 
robust and efficient processing of 3D point clouds and image 
data since it is based on interaction between two 
complementary domains, the semantic web and the 3D 
processing one via an intermediary layer labeled WiDop 
processing. 
The integration of knowledge into 3D processing is a 
promising solution. It could make the object detection 
algorithms more robust, flexible and adaptive in the different 
circumstances through the knowledge guidance via a new 
197
SEMAPRO 2010 : The Fourth International Conference on Advances in Semantic Processing
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-104-5

mechanism under construction named "3D processing rules". 
Such mechanism aims to connect ontology to 3d processing 
algorithm via new Built-Ins. Once executed, these rules will 
query the ontology and the point clouds via the activation 
and the instantiation of the most suitable 3D processing 
algorithm. 
ACKNOWLEDGMENT  
The work presented in this paper is part of the research 
project WiDOP – Wissensbasierte Detektion von Objekten in 
Punktwolken für Anwendungen im Ingenieurbereich, funded 
by the German Federal Ministry for Research and Education 
(grant no. 1758X09). Partners in the project are Metronom 
Automation GmbH and DB Netz AG (for the railway 
domain) and Fraport AG (for the facility management 
domain).  
REFERENCES 
[1] Vanlande, R., Nicolle, C., and Cruz, C. 2008. “IFC and building 
lifecycle management,” Elsevier, Automation in Construction, Vol. 
18, pp. 70-78. 
[2] Zitova, B., and Flusser, J. 2003. “Image registration methods: a 
survey,” Elsevier, Image and vision computing. Vol. 21, pp. 977-
1000. 
[3] Vosselman, G., and Dijkman, S. 2001. “3D building model 
reconstruction from point clouds and ground plans,” International 
Archives 
of 
Photogrammetry 
Remote 
Sensing 
and 
Spatial 
Information Sciences, Vol. 34, pp. 37-44. 
[4] Pollefeys, M. 2000. “Automated reconstruction of 3D scenes from 
sequences of images,” Elsevier, ISPRS Journal Of Photogrammetry 
And Remote Sensing, Vol. 55, pp. 251-267. 
[5] Hartley, R., and Zisserman, A. 2003. “Multiple view geometry in 
computer vision,”  Cambridge University Press New York, NY, USA. 
[6] Brunn, A. 2000. “A step towards semantic-based building 
reconstruction using markov-random-fields,” INTERNATIONAL 
ARCHIVES OF PHOTOGRAMMETRY AND REMOTE SENSING, 
v 33, pp. 117-124. 
[7] Scholze, S. Moons, T., and Van Gool, L. 2002. “A probabilistic 
approach to building roof reconstruction using semantic labelling,” 
Pattern Recognition. pp. 257-264. 
[8] Grimson, W E. 1986. “From images to surfaces: A computational 
study of the human early visual system,” MIT press Cambridge, 
Massachusetts. 
[9] Cantzler, H. Fisher, R., and Devy, M. 2002. “Quality enhancement of 
reconstructed 3D models using coplanarity and constraints,” Springer, 
Pattern Recognition, pp. 34-41. 
 
 
 
 
[10] Nuchter, A., Surmann, H., and Hertzberg, J. 2003. “Automatic model 
refinement for 3D reconstruction with mobile robots,” IEEE 
Computer Society, pp. 394-401. 
[11] Grau, O. 1997. “A scene analysis system for the generation of 3-D 
models,” First International Conference on Recent Advances in 3-D 
Digital Imaging and Modeling (3DIM '97) pp. 221. 
[12] Borst, WN., Akkermans, JM., and Top, JL. 1997. “Engineering 
ontologies,” International Journal of Human-Computer Studies, Vol. 
46, pp. 365-406. 
[13] Van der Vet, PE., and Mars, NJI. 1998. “Bottom-up construction of 
ontologies,” IEEE Transactions on Knowledge and data Engineering, 
Vol. 10, pp. 513-526. 
[14] Hare, JS., Sinclair, P. A. S., Lewis, P. H., Martinez, K., Enser, P. G. 
B., and Sandom, C. J. 2006. “Bridging the Semantic Gap in 
Multimedia Information Retrieval: Top-down and Bottom-up 
approaches,” Mastering the Gap: From Information Extraction to 
Semantic Representation / 3rd European Semantic Web Conference, 
12 June 2006, Budva, Montenegro. 
[15] Stumme, G., and Maedche, A . 2001. “Ontology merging for 
federated ontologies on the semantic web,” Proceedings of the 
International Workshop for Foundations of Models for Information 
Integration (FMII-2001), pp. 413-418. 
[16] Dista:  http://www.i3mainz.fh-mainz.de/Article68.html. The last 
access date:  04-08-2010.           
[17] Bohring, H., and Auer S. 2005. “Mapping XML to OWL ontologies,”  
Leipziger Informatik-Tage, Vol. 72 of LNI, GI, pp.147-156.  
[18] Leica®:  http://www.leica-geosystems.fr/fr/index.htm. The last access 
date: 04-08-2010. 
[19] Cruz, C., and Nicolle, C. 2008. “Ontology Enrichment and Automatic 
Population From XML Data,”  ODBIS. pp.17-20. 
[20] Kenue, S.K. “LANELOK: Detection of lane boundaries and vehicle 
tracking using image-processing techniques- part II: Template 
matching algorithms,” SPIE Conference on Mobile Robots. pp. 234-
245. 
[21] Lacey, AJ., Pinitkarn, N., and Thacker, N.A. “Faithful least-squares 
fitting of spheres, cylinders, cones and tori for reliable segmentation,” 
Proceedings of the British Machine Vision Conference (BMVC) 
2000.  
[22] Besl, P.J., and McKay, N.D. 1992. “A Method for Registration of 3-D 
Shapes,”  IEEE Trans. Pattern Analysis and Machine Intelligence, V 
14, pp. 239-256. 
[23] Arun, KS., Huang, T.S., and Blostein, S.D. “Least-squares fitting of 
two 3-D point sets,” IEEE TRANS. PATTERN ANAL, MACH. 
INTELLIG 1987. pp. 698-700. 
[24] Marbs, A., Boochs, F., Ben Hmida, H., and Truong, H. 2010. 
“Wissensbasierte Objekterkennung in 3D-Punktwolken und Bildern,” 
DGPF-Tagungsband, 3-Ländertagung D-A-CH Conference Wien, pp. 
220-227. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
198
SEMAPRO 2010 : The Fourth International Conference on Advances in Semantic Processing
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-104-5

