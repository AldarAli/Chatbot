A Comparison of Global and Saturated Probabilistic Approximations
Using Characteristic Sets in Mining Incomplete Data
Patrick G. Clark
Department of Electrical Engineering
and Computer Science,
University of Kansas
Lawrence, KS, USA
Email: patrick.g.clark@gmail.com
Jerzy W. Grzymala-Busse
Department of Electrical Engineering
and Computer Science,
University of Kansas,
Lawrence, KS, USA
Department of Expert Systems
and Artiﬁcial Intelligence,
University of Information
Technology and Management,
Rzeszow, Poland
Email: jerzy@ku.edu
Teresa Mroczek and Rafal Niemiec
Department of Expert Systems
and Artiﬁcial Intelligence,
University of Information
Technology and Management,
Rzeszow, Poland
Email: tmroczek@wsiz.rzeszow.pl
and rniemiec@wsiz.rzeszow.pl
Abstract—Data mining systems form granules of information
from data sets. Methods used to construct these granules can
signiﬁcantly impact the overall accuracy of the resulting model. In
this paper, we study incomplete data sets with two interpretations
of missing attribute values, lost values and “do not care”
conditions, to determine the best method between two approaches
and achieve the highest accuracy. For such incomplete data sets,
we apply data mining based on two probabilistic approximations,
global and saturated. The main objective of our paper is to
compare both approaches in terms of an error rate, evaluated by
ten-fold cross validation. Saturated probabilistic approximations
are closer to the concept than global probabilistic approximations,
so the corresponding error rate should be smaller. Using a
5% level of signiﬁcance, our main result shows that there are
differences between both approaches. However, in general neither
is better for all data sets and thus, both approaches should be
tried for each data set with the best selected for rule induction.
Keywords–Data mining; rough set theory; probabilistic approx-
imations; MLEM2 rule induction algorithm; lost values; “do not
care” conditions.
I.
INTRODUCTION
In this paper, we use two interpretations of a missing
attribute value: lost values and “do not care” conditions. Lost
values indicate that the original values were erased, and as a
result we should use only existing, speciﬁed attribute values
for rule induction. A lost value is denoted by “?”. “Do not
care” conditions mean that the missing attribute value may
be replaced by any speciﬁed attribute value. A “do not care”
condition is denoted by “*”.
We use for data mining probabilistic approximations, a
generalization of the idea of lower and upper approximations
known in rough set theory. A probabilistic approximation
is associated with a parameter (probability) α, if α = 1, a
probabilistic approximation is reduced to the lower approx-
imation; if α is a small positive number, e.g., 0.001, the
probabilistic approximation becomes the upper approximation.
Usually, probabilistic approximations are applied to completely
speciﬁed data sets [1]–[9]. Such approximations were gener-
alized to incomplete data sets in [10].
Characteristic sets were introduced in [11] for incomplete
data sets with any interpretation of missing attribute values.
One of the methods being used in this paper, global prob-
abilistic approximations, were introduced in [12]. This prior
work expanded the ideas of characteristic sets and also studied
their performance as a data mining tool. To further improve
our methodology, we introduce a new idea of saturated proba-
bilistic approximations. Our main objective is to compare both
approaches in terms of an error rate, evaluated by ten-fold cross
validation. The Modiﬁed Learning from Examples Module,
version 2 (MLEM2) [13] was used for rule induction.
This paper starts with a discussion on incomplete data
in Section II where we deﬁne attribute-value blocks and
characteristic sets. In Section III, we present two types of
probabilistic approximations, global and saturated. Section IV
contains the details of our experiments. Finally, conclusions
are presented in Section V.
II.
INCOMPLETE DATA
We assume that the input data sets are presented in the
form of a decision table. An example of a decision table is
shown in Table I. Rows of the decision table represent cases,
while columns are labeled by variables. The set of all cases
will be denoted by U. In Table I, U = {1, 2, 3, 4, 5, 6, 7,
8}. Independent variables are called attributes and a dependent
variable is called a decision and is denoted by d. The set of all
attributes will be denoted by A. In Table I, A = {Temperature,
Headache, Cough}. The value for a case x and an attribute a
will be denoted by a(x). For example, Temperature(1) = high.
The set X of all cases deﬁned by the same value of
the decision d is called a concept. For example, a concept
associated with the value yes of the decision Flu is the set {1,
2, 3, 4}.
For a variable a and its value v, (a, v) is called a variable-
value pair. A block of (a, v), denoted by [(a, v)], is the set
{x ∈ U | a(x) = v} [14]. For incomplete decision tables, the
10
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-723-8
INTELLI 2019 : The Eighth International Conference on Intelligent Systems and Applications

TABLE I. A DECISION TABLE
Attributes
Decision
Case
Temperature
Headache
Cough
Flu
1
high
yes
*
yes
2
*
no
no
yes
3
very-high
*
yes
yes
4
high
no
*
yes
5
normal
yes
no
no
6
high
*
no
no
7
?
no
*
no
8
normal
yes
yes
no
deﬁnition of a block of an attribute-value pair is modiﬁed in
the following way:
•
if for an attribute a and a case x we have a(x) = ?, the
case x should not be included in any blocks [(a, v)]
for all values v of attribute a;
•
if for an attribute a and a case x we have a(x) = ∗,
the case x should be included in blocks [(a, v)] for all
speciﬁed values v of attribute a.
For the data set from Table I, the blocks of attribute-value
pairs are:
[(Temperature, normal)] = {2, 5, 8},
[(Temperature, high)] = {1, 2, 4, 6},
[(Temperature, very-high)] = {2, 3},
[(Headache, no)] = {2, 3, 4, 6, 7},
[(Headache, yes)] = {1, 3, 5, 6, 8},
[(Cough, no)] = {1, 2, 4, 5, 6, 7}, and
[(Cough, yes)] = {1, 3, 4, 7, 8}.
For a case x ∈ U and B ⊆ A, the characteristic set KB(x)
is deﬁned as the intersection of the sets K(x, a), for all a ∈ B,
where the set K(x, a) is deﬁned in the following way:
•
if a(x) is speciﬁed, then K(x, a) is the block
[(a, a(x))] of attribute a and its value a(x);
•
if a(x) = ? or a(x) = ∗, then K(x, a) = U.
For example, for Table I and B = A,
K(1, Temperature) = [(Temperature, high)] = {1, 2, 4, 6},
K(1, Headache) = [(Headache, yes)] = {1, 3, 5, 6, 8}
and
K(1, Cough) = U
so KA(1) = {1, 2, 4, 6} ∩ {1, 3, 5, 6, 8} ∩ U = {1, 6}.
Similarly,
KA(2) = {2, 4, 6, 7},
KA(3) = {3},
KA(4) = {2, 4, 6},
KA(5) = {5},
KA(6) = {1, 2, 4, 6},
KA(7) = {2, 3, 4, 6, 7}, and
KA(8) = {8}.
III.
PROBABILISTIC APPROXIMATIONS
In this section, we will discuss two types of probabilistic
approximations: global and saturated.
A. Global Probabilistic Approximations
An idea of the global probabilistic approximation, though
restricted only to lower and upper approximations, was intro-
duced in [15][16], and presented in a general form in [12].
Let X be a concept, X ⊆ U. A B-global probabilistic
approximation of the concept X, based on characteristic sets,
with the parameter α and denoted by apprglobal
α,B
(X) is deﬁned
as the following set
[
{KB(x) | ∃ Y ⊆ U ∀ x ∈ Y, Pr(X|KB(x)) ≥ α}. (1)
In general, for given sets B and X and the parameter
α, there exist many B-global probabilistic approximations
of X. Additionally, the algorithm for computing B-global
probabilistic approximations is of exponential computational
complexity. Therefore, we decided to use a heuristic version
of the deﬁnition of B-global probabilistic approximation,
called a MLEM2 B-global probabilistic approximation of the
concept X, associated with a parameter α and denoted by
apprmlem2
α,B
(X), [12]. This deﬁnition is based on the rule
induction algorithm MLEM2 [17]. The MLEM2 algorithm
is used in the Learning from Examples using Rough Sets
(LERS) data mining system [17]–[19]. The approximation
apprmlem2
α,B
(X) is constructed from characteristic sets KB(y),
the most relevant to the concept X, i.e., with |X ∩ KB(y)|
as large as possible and Pr(X|KB(y)) ≥ α, where y ∈ U.
If more than one characteristic set KB(y) satisﬁes both
conditions, we pick the characteristic set KB(y) with the
largest Pr(X|KB(y)). If this criterion ends up with a tie, a
characteristic set is picked up heuristically, as the ﬁrst on the
list [12].
In this paper, we study MLEM2 B-global probabilistic
approximations based on characteristic sets, with B = A,
and calling them, for simplicity, global probabilistic ap-
proximations associated with the parameter α, denoted by
apprmlem2
α
(X). Similarly, for B = A, the characteristic set
KB(X) is denoted by K(x).
Let Eα(X) be the set of all eligible characteristic sets
deﬁned as follows
{K(x) | x ∈ U, Pr(X|K(x)) ≥ α}.
(2)
A heuristic version of the MLEM2 global probabilistic
approximation is computed using the algorithm speciﬁed in
Figure 1.
For Table I, all distinct MLEM2 global probabilistic ap-
proximations are
apprmlem2
1
({1, 2, 3, 4}) = {3},
apprmlem2
0.75
({1, 2, 3, 4}) = {1, 2, 3, 4, 6},
apprmlem2
0.6
({1, 2, 3, 4}) = {1, 2, 3, 4, 6, 7},
apprmlem2
1
({5, 6, 7, 8}) = {5, 8},
apprmlem2
0.5
({5, 6, 7, 8}) = {2, 4, 5, 6, 7, 8} and
apprmlem2
0.4
({5, 6, 7, 8}) = {2, 3, 4, 5, 6, 7, 8}.
11
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-723-8
INTELLI 2019 : The Eighth International Conference on Intelligent Systems and Applications

MLEM2 global probabilistic approximation algorithm
input: a set X (a concept), a set Eα(X),
output: a set T (apprmlem2
α
(X))
begin
G := X;
T := ∅;
Y := Eα(X);
while G ̸= ∅ and Y ̸= ∅
begin
select a characteristic set K(x) ∈ Y
such that |K(x) ∩ X| is maximum;
if a tie occurs, select K(x) ∈ Y
with the smallest cardinality;
if another tie occurs, select the ﬁrst K(x);
T := T ∪ K(x);
G := G − T;
Y := Y − K(x)
end
end
Figure 1. MLEM2 Global Approximation Algorithm
B. Saturated Probabilistic Approximations
Another heuristic version of the probabilistic approxima-
tion is based on selection of characteristic sets while giving
higher priority to characteristic sets with larger conditional
probability Pr(X|K(x)). Additionally, if the approximation
covers all cases from the concept X, we stop adding charac-
teristic sets.
Let X be a concept and let x ∈ U. Let us compute all
conditional probabilities Pr(X|K(x)). Then, we sort the set
{Pr(X|K(x)) | x ∈ U}.
(3)
Let us denote the sorted list of such conditional probabili-
ties by α1, α2,..., αn, where α1 is the largest. For any i = 1,
2,..., n, the set Ei(x) is deﬁned as follows
{K(x) | x ∈ U, Pr(X|K(x) = αi}.
(4)
If we want to compute a saturated probabilistic approxima-
tion, denoted by apprsaturated
α
(X), for some α, 0 < α ≤ 1,
we need to identify the index m such that
αm ≥ α > αm+1,
(5)
where m ∈ {1, 2, ..., n} and αn+1 = 0. Then, the saturated
probabilistic approximation apprsaturated
αm
(X) is computed us-
ing the algorithm speciﬁed in Figure 2.
For Table I, all distinct saturated probabilistic approxima-
tions are
apprsaturated
1
({1, 2, 3, 4}) = {3},
apprsaturated
0.75
({1, 2, 3, 4}) = {1, 2, 3, 4, 6},
apprsaturated
1
({5, 6, 7, 8}) = {5, 8} and
apprsaturated
0.5
({5, 6, 7, 8}) = {2, 4, 5, 6, 7, 8}.
Saturated probabilistic approximation algorithm
input: a set X (a concept), index m,
a set Ei(x) for i = 1, 2,..., n and x ∈ U,
output: a set T (apprsaturated
αm
(X))
begin
T := ∅;
Yi(x) := Ei(x) for all i = 1, 2,..., m and x ∈ U;
for j = 1, 2,..., m do
while Yj(x) ̸= ∅
begin
select a characteristic set K(x) ∈ Yj(x)
such that |K(x) ∩ X| is maximum;
if a tie occurs, select the ﬁrst K(x);
Yj(x) := Yj(x) − K(x);
if (K(x) − T) ∩ X ̸= ∅
then T := T ∪ K(x);
if X ⊆ T then exit
end
end
Figure 2. Saturated Probabilistic Approximation Algorithm
Note that apprmlem2
0.6
({1, 2, 3, 4}) covers the case 7 in spite
of the fact that the case 7 is not a member of the concept {1,
2, 3, 4}. The set {1, 2, 3, 4, 6, 7} is not listed among saturated
probabilistic approximations of the concept {1, 2, 3, 4}.
C. Rule Induction
Once the global and saturated probabilistic approximations
associated with a parameter α are constructed, rule sets are
induced using the rule induction algorithm based on another
parameter, also interpreted as a probability, and denoted by
β. This algorithm also uses MLEM2 principles [20], with the
algorithm details shown in Figure 3.
For example, for Table I and α = β = 0.5, using the satu-
rated probabilistic approximations, the MLEM2 rule induction
algorithm induces the following rules:
(Cough, no) & (Headache, no) → (Flu, no),
(Temperature, normal) → (Flu, no),
(Temperature, high) → (Flu, yes) and
(Temperature, very-high) → (Flu, yes).
IV.
EXPERIMENTS
The goal of this research is to select the best approach for
rule set induction in data mining. The data sets were chosen
to represent various types of data (symbolic or numeric) in an
attempt to ﬁnd the best method. For our experiments, we used
eight data sets that are available in the University of California
at Irvine Machine Learning Repository.
For every data set, a template was created. Such a template
was formed by replacing randomly 35% of existing speciﬁed
attribute values by lost values. The same templates were used
for constructing data sets with “do not care” conditions, by
replacing “?”s with “∗”s. The reason that these data sets were
selected is because they represent a reasonable distribution of
types of data and data set sizes to measure the impacts of
the experiments. Furthermore, 35% is selected as a missing
attribute percentage is because it is the maximum percentage
that is able to be replaced with all records having at least one
value speciﬁed.
12
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-723-8
INTELLI 2019 : The Eighth International Conference on Intelligent Systems and Applications

MLEM2 rule induction algorithm
input: a set Y (an approximation of X) and a parameter β,
output: a set T (a rule set),
begin
G := Y ;
D := Y ;
T := ∅;
J := ∅;
while G ̸= ∅
begin
T := ∅;
Ts := ∅;
Tn := ∅;
T(G) := {t | [t] ∩ G ̸= ∅};
while (T = ∅ or [T] ̸⊆ D) and T(G) ̸= ∅
begin
select a pair t = (at, vt) ∈ T(G) with maximum of
|[t] ∩ G|; if a tie occurs, select a pair t ∈ T(G)
with the smallest cardinality of [t]; if another tie occurs,
select ﬁrst pair;
T := T ∪ {t};
G := [t] ∩ G;
T(G) := {t | [t] ∩ G ̸= ∅};
if at is symbolic {let Vat be the domain of at}
then
Ts := Ts ∪ {(at, v)|v ∈ Vat}
else {at is numerical, let t = (at, u..v)} and Tn :=
Tn ∪ {(at, x..y) | disjoint x..y and u..v}∪
{(at, x..y) | x..y ⊇ u..v};
T(G) := T(G) − (Ts ∪ Tn);
end {while};
if Pr(X | [T]) ≥ β
then
begin
D := D ∪ [T];
T := T ∪ {T};
end {then}
else J := J ∪ {T};
G := D − ∪S∈T ∪J [S];
end {while};
for each T ∈ T do
for each numerical attribute at with
(at, u..v) ∈ T do
while (T contains at least two different pairs (at, u..v)
and (at, x..y) with the same numerical attribute at)
replace these two pairs with a new pair
(at, common part of (u..v) and (x..y));
for each t ∈ T do
if [T − {t}] ⊆ D then T := T − {t};
for each T ∈ T do
if ∪S∈(T −{T })[S] = ∪S∈T [S] then T := T − {T};
end {procedure}.
Figure 3. MLEM2 Rule Induction Algorithm
10
30
50
70
90
110
0
0.2
0.4
0.6
0.8
1
Error rate (%) 
Parameter beta
Global, ?
Global, *
Saturated, ?
Saturated, *
Figure 4. Error rate for the Bankruptcy data
20
40
60
80
0
0.2
0.4
0.6
0.8
1
Error rate (%) 
Parameter beta
Global, ?
Global, *
Saturated, ?
Saturated, *
Figure 5. Error rate for the Breast cancer data set
20
40
60
0
0.2
0.4
0.6
0.8
1
Error rate (%) 
Parameter beta
Global, ?
Global, *
Saturated, ?
Saturated, *
Figure 6. Error rate for the Echocardiogram data set
In our experiments, we used the MLEM2 rule induction
algorithm. In all experiments, the parameter α was equal to
0.5. Results of our experiments are presented in Figures 4–
11, where “Global” denotes a MLEM2 global probabilistic
approximation, “Saturated” denotes a saturated probabilistic
approximation, “?” denotes lost values and “∗” denotes “do
not care” conditions. In our experiments, four approaches for
mining incomplete data sets were used, since we combined two
options of probabilistic approximations: global and saturated
with two interpretations of missing attribute values: lost and
“do not care” conditions.
10
30
50
70
90
0
0.2
0.4
0.6
0.8
1
Error rate (%) 
Parameter beta
Global, ?
Global, *
Saturated, ?
Saturated, *
Figure 7. Error rate for the Hepatitis data set
13
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-723-8
INTELLI 2019 : The Eighth International Conference on Intelligent Systems and Applications

40
60
80
0
0.2
0.4
0.6
0.8
1
Error rate (%) 
Parameter beta
Global, ?
Global, *
Saturated, ?
Saturated, *
Figure 8. Error rate for the Image segmentation data set
10
30
50
70
0
0.2
0.4
0.6
0.8
1
Error rate (%) 
Parameter beta
Global, ?
Global, *
Saturated, ?
Saturated, *
Figure 9. Error rate for the Iris data set
These four approaches were compared by applying the
distribution free Friedman rank sum test and then by the
post-hoc test (distribution-free multiple comparisons based
on the Friedman rank sums), with a 5% level of signiﬁ-
cance. The null hypothesis H0 of the Friedman test, claim-
ing that differences between these approaches are insigniﬁ-
cant, was rejected for Breast cancer and Image recognition
as the only data sets. Results of the post-hoc distribution
free all-treatments multiple comparisons Wilcoxon-Nemenyi-
McDonald-Thompson test for the remaining six data sets are
30
40
50
60
0
0.2
0.4
0.6
0.8
1
Error rate (%) 
Parameter beta
Global, ?
Global, *
Saturated, ?
Saturated, *
Figure 10. Error rate for the Lymphography data set
10
30
50
70
0
0.2
0.4
0.6
0.8
1
Error rate (%) 
Parameter beta
Global, ?
Global, *
Saturated, ?
Saturated, *
Figure 11. Error rate for the Wine recognition data set
TABLE II. RESULTS OF STATISTICAL ANALYSIS
Data set
Friedman test results
(5% signiﬁcance level)
Bankruptcy
(Global, *) is better than (Global, ?)
(Saturated, *) is better than (Global, *)
(Global, *) is better than (Saturated, ?)
(Saturated, *) is better than (Saturated, ?)
Iris
(Global, *) is better than (Global, ?)
(Saturated, *) is better than (Global, ?)
(Global, *) is better than (Saturated, ?)
(Saturated, *) is better than (Saturated, ?)
Lymphography
(Global, *) is better than (Global, ?)
(Saturated, *) is better than (Global, ?)
(Global, *) is better than (Saturated, ?)
(Saturated, *) is better than (Saturated, ?)
Echocardiogram
(Global, ?) is better than (Global, *)
(Saturated, ?) is better than (Global, *)
Hepatitis
(Global, ?) is better than(Saturated, *)
(Saturated, ?) is better than (Saturated, *)
Wine recognition
(Global, *) is better than (Saturated, *)
presented in Table II. This table is divided into three parts.
In the ﬁrst part, for Bankruptcy, Iris and Lymphography data
sets, data mining approaches based on “do not care” conditions
are always better than approaches based on lost values. In
the second part, for Echocardiogram and Hepatitis, it is the
other way around. In the third part, for Wine recognition, the
only conclusion is that for “do not care” conditions global
probabilistic approximations are better than saturated ones.
Due to the varying characteristics of the data, as follows
from the experimental results, no particular combination was
identiﬁed as the best for all situations.
V.
CONCLUSIONS AND FUTURE WORK
We compared four approaches for mining incomplete data
sets, combining two interpretations of missing attribute values
with two types of probabilistic approximations. Our criterion
of quality was an error rate computed as a result of ten-fold
cross-validation. As follows from our experiments, there were
signiﬁcant differences between the four approaches. However,
the best approach, associated with the smallest error rate,
depends on a speciﬁc data set. Thus, for a given data set,
14
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-723-8
INTELLI 2019 : The Eighth International Conference on Intelligent Systems and Applications

the base approach to mining must be selected by running
experiments involved with all four approaches.
In future work, we will continue to study the experimental
effects of saturation in an effort to form a theoretical basis
for our ideas. Furthermore, these ideas will be extended to
Maximal Consistent Blocks [21][22] to include the concepts
of saturation with a thorough measurement of the experimental
performance.
REFERENCES
[1]
J. W. Grzymala-Busse and W. Ziarko, “Data mining based on rough
sets,” in Data Mining: Opportunities and Challenges, J. Wang, Ed.
Hershey, PA: Idea Group Publ., 2003, pp. 142–173.
[2]
Z. Pawlak and A. Skowron, “Rough sets: Some extensions,” Information
Sciences, vol. 177, 2007, pp. 28–40.
[3]
Z. Pawlak, S. K. M. Wong, and W. Ziarko, “Rough sets: probabilistic
versus deterministic approach,” International Journal of Man-Machine
Studies, vol. 29, 1988, pp. 81–95.
[4]
D. ´Sle¸zak and W. Ziarko, “The investigation of the bayesian rough set
model,” International Journal of Approximate Reasoning, vol. 40, 2005,
pp. 81–91.
[5]
S. K. M. Wong and W. Ziarko, “INFER—an adaptive decision sup-
port system based on the probabilistic approximate classiﬁcation,” in
Proceedings of the 6-th International Workshop on Expert Systems and
their Applications, 1986, pp. 713–726.
[6]
Y. Y. Yao, “Probabilistic rough set approximations,” International Jour-
nal of Approximate Reasoning, vol. 49, 2008, pp. 255–271.
[7]
Y. Y. Yao and S. K. M. Wong, “A decision theoretic framework for
approximate concepts,” International Journal of Man-Machine Studies,
vol. 37, 1992, pp. 793–809.
[8]
W. Ziarko, “Variable precision rough set model,” Journal of Computer
and System Sciences, vol. 46, no. 1, 1993, pp. 39–59.
[9]
——, “Probabilistic approach to rough sets,” International Journal of
Approximate Reasoning, vol. 49, 2008, pp. 272–284.
[10]
J. W. Grzymala-Busse, “Generalized parameterized approximations,” in
Proceedings of the 6-th International Conference on Rough Sets and
Knowledge Technology, 2011, pp. 136–145.
[11]
——, “Rough set strategies to data with missing attribute values,” in
Notes of the Workshop on Foundations and New Directions of Data
Mining, in conjunction with the Third International Conference on Data
Mining, 2003, pp. 56–63.
[12]
P. G. Clark, C. Gao, J. W. Grzymala-Busse, T. Mroczek, and
R. Niemiec, “A comparison of concept and global probabilistic ap-
proximations based on mining incomplete data,” in Proceedings of
ICIST 2018, the International Conference on Information and Software
Technologies, 2018, pp. 324–335.
[13]
P. G. Clark and J. W. Grzymala-Busse, “Experiments on rule induction
from incomplete data using three probabilistic approximations,” in
Proceedings of the 2012 IEEE International Conference on Granular
Computing, 2012, pp. 90–95.
[14]
J. W. Grzymala-Busse, “LERS—a system for learning from examples
based on rough sets,” in Intelligent Decision Support. Handbook of
Applications and Advances of the Rough Set Theory, R. Slowinski, Ed.
Dordrecht, Boston, London: Kluwer Academic Publishers, 1992, pp.
3–18.
[15]
J. W. Grzymala-Busse and W. Rzasa, “Local and global approxima-
tions for incomplete data,” in Proceedings of the Fifth International
Conference on Rough Sets and Current Trends in Computing, 2006,
pp. 244–253.
[16]
——, “Local and global approximations for incomplete data,” Transac-
tions on Rough Sets, vol. 8, 2008, pp. 21–34.
[17]
J. W. Grzymala-Busse, “MLEM2: A new algorithm for rule induction
from imperfect data,” in Proceedings of the 9th International Confer-
ence on Information Processing and Management of Uncertainty in
Knowledge-Based Systems, 2002, pp. 243–250.
[18]
P. G. Clark and J. W. Grzymala-Busse, “Experiments on probabilistic
approximations,” in Proceedings of the 2011 IEEE International Con-
ference on Granular Computing, 2011, pp. 144–149.
[19]
J. W. Grzymala-Busse, “A new version of the rule induction system
LERS,” Fundamenta Informaticae, vol. 31, 1997, pp. 27–39.
[20]
J. W. Grzymala-Busse, P. G. Clark, and M. Kuehnhausen, “Generalized
probabilistic approximations of incomplete data,” International Journal
of Approximate Reasoning, vol. 132, 2014, pp. 180–196.
[21]
Y. Leung and D. Li, “Maximal consistent block technique for rule
acquisition in incomplete information systems,” Information Sciences,
vol. 153, 2003, pp. 85–106.
[22]
P. G. Clark, C. Gao, J. W. Grzymala-Busse, and T. Mroczek, “Char-
acteristic sets and generalized maximal consistent blocks in mining
incomplete data,” in Proceedings of the International Joint Conference
on Rough Sets, part 1, 2017, pp. 477–486.
15
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-723-8
INTELLI 2019 : The Eighth International Conference on Intelligent Systems and Applications

