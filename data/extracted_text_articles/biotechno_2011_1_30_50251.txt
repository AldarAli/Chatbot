Bio-UnaGrid: Easing Bioinformatics Workflow Execution Using LONI Pipeline and 
a Virtual Desktop Grid 
 
 
Mario Villamizar, Harold Castro, David Mendez 
Department of Systems and Computing Engineering 
Universidad de los Andes 
Bogotá D.C., Colombia 
{mj.villamizar24, hcastro, 
dg.mendez67}@uniandes.edu.co 
Silvia Restrepo, Luis Rodriguez 
Department of Biological Sciences 
Universidad de los Andes 
Bogotá D.C., Colombia 
{srestrep, luisrodr}@uniandes.edu.co 
Abstract—Bioinformatics researches use applications that 
require large computational capabilities regularly provided by 
cluster and grid computing infrastructures. Researchers must 
learn tens of commands to execute bioinformatics applications, 
to coordinate the manual workflow execution and to use 
complex distributed computing infrastructures, spending 
much of their time in technical issues of applications and 
distributed computing infrastructures. We propose the Bio-
UnaGrid infrastructure to facilitate the automatic execution of 
intensive-computing workflows that require the use of existing 
application suites and distributed computing infrastructures. 
With Bio-UnaGrid, bioinformatics workflows are easily 
created and executed, with a simple click and in a transparent 
manner, 
on 
different 
cluster 
and 
grid 
computing 
infrastructures (line command is not used). To provide more 
processing capabilities, at low cost, Bio-UnaGrid use the idle 
processing capabilities of computer labs with Windows, Linux 
and Mac desktop computers, using a key virtualization 
strategy. We implement Bio-UnaGrid in a dedicated cluster 
and a computer lab. Results of performance tests evidence the 
gain obtained by our researchers. 
Keywords; Bio-UnaGrid; grid computing; cluster computing; 
bioinformatics; BLAST; mpiBLAST; desktop grid; UnaGrid; 
LONI Pipeline 
I. 
 INTRODUCTION 
Today genomic projects involve the use of two 
complementary approaches: cluster computing which allows 
to aggregate homogeneous computational resources for a 
specific research group or organization, and grid computing 
which aggregates heterogeneous computational resources of 
different organizations to support larger computational 
capabilities in e-Science projects. 
High performance computing (HPC) infrastructure, such 
as cluster or grid computing, provide results in shorter time, 
however when bioinformatics researchers want to execute 
applications, they face several consuming time problems: a) 
there are many application suites to run different analysis, so 
researchers must learn command line syntax (options, input 
and output files, distributed execution environment, etc.) for 
hundreds of applications. b) Researchers must learn 
commands to manage and use distributed computing 
infrastructures. c) Applications require specific and complex 
configurations to operate in distributed environments. d) 
Researchers frequently execute a set of applications in a 
sequential and/or coordinated manner, called pipelines or 
workflows. Workflows are regularly manually executed by 
researchers, so they must wait an application finishes, before 
executing the next. e) Very often researchers want to execute 
applications that require larger processing capabilities than 
those provided by dedicated computing infrastructures, so 
they must wait weeks or months before getting their results. 
Different approaches have been developed for solving 
these 
problems 
partially 
on 
dedicated 
computing 
infrastructures. In this work we propose and evaluate an 
integral infrastructure that allows bioinformatics researchers, 
requiring large computing capabilities, to focus in 
bioinformatics analysis and not on technical computing 
issues of distributed computing infrastructures. The 
infrastructure, called Bio-UnaGrid, allows researchers to 
define workflows using graphical user interfaces (GUIs) and 
drag and drop tools provide by the LONI Pipeline [1] 
application. Workflows are executed and distributed in a 
transparent manner on a grid infrastructure. To support larger 
processing capabilities to those provided by dedicated 
infrastructures, Bio-UnaGrid also uses the UnaGrid [2] 
desktop grid infrastructure. UnaGrid permanently takes 
advantage of the idle processing capabilities available in 
desktop computers, while students do their daily activities. 
Several existing bioinformatics application suites have 
been ported into Bio-UnaGrid. Performance tests were 
executed using the BLAST algorithm and its distributed 
implementations using query segmentation, provided by 
NCBI BLAST [3], and database segmentation, provided by 
mpiBLAST [4]. Test results show that Bio-UnaGrid allows 
to define workflows and to execute them on dedicated and 
desktop grid infrastructures, providing speedups 10 times 
higher than the speed on a desktop computer. 
This paper is organized as follows: section 2 presents the 
related works. Section 3 describes the BLAST application, 
the UnaGrid infrastructure and the LONI Pipeline 
application. Section 4 details the Bio-UnaGrid architecture, 
including its integration with MPI (Message Passing 
Interface) 
applications. 
Section 
5 
describes 
the 
implementation deployed on a university campus. Results 
and performance tests are described in Section 6. Section 7 
concludes and presents future work. 
12
BIOTECHNO 2011 :  The Third International Conference on Bioinformatics, Biocomputational Systems and Biotechnologies
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-137-3

II. 
RELATED WORKS 
Bioinformatics projects require the use of application 
suites for analysis that regularly require large computing 
capabilities. An analysis executed like a sequential 
computational job on a personal computer, may take weeks 
or months. Cluster computing infrastructures solve this 
problem aggregating the computational capabilities of 
several homogeneous computers, a cluster, to parallel 
execute a set of computational jobs. In a computational 
cluster there is a cluster master, a computer that submits a 
subset of jobs to other cluster computers: the slave nodes. 
Computational clusters are created by individual research 
groups to support their computational requirements. 
Grid computing emerged as a technology that allows 
different organizations with common goals to create a virtual 
organization (VO) to share resources such as data, hardware 
and software. A grid computing infrastructure can group a 
great number of heterogeneous resources to support the 
computational requirements of the VO. In a grid 
infrastructure a set of computational jobs can be distributed 
among clusters belonging to different administrative 
domains. A drawback of this approach is that grid 
infrastructures require complex management processes. 
To execute a bioinformatics analysis on a cluster or grid 
infrastructure, the application must be wrapped or designed 
to operate on these infrastructures. Wrapped application 
adapt an existing standalone application so it can be executed 
in parallel like a set of smaller and independent jobs, this 
approach is known as bags-of-tasks (BoT). In applications 
designed to operate on distributed infrastructures, a single 
job is automatically executed using several processes 
executed coordinately in a computational cluster or grid, 
using, regularly a MPI implementation. 
Like BLAST, other bioinformatics applications are 
frequently used by researchers to execute different analysis 
in a coordinated and dependent manner, called workflows or 
pipelines, which require HPC infrastructures. The manual 
and 
command-based 
workflow 
execution 
requires 
bioinformatics researchers to spend most of their time in: 
configuring 
application 
parameters, 
managing 
HPC 
infrastructures, managing scientific data, and linking partial 
results from one application to another. Several projects have 
been developed to facilitate the automatic workflow 
execution in other scientific fields. Projects like Khoros [5], 
3D Slicer [6], SCIRun/BioPSE [7] and Karma2 [8] for image 
processing; MAPS [9] for brain images; Trident [10] for 
oceanography; Kepler [11] and Swift [12] for agnostic area; 
MediGRID [13] for biomedical;  Pegasus [14], OpenDX 
[15], and Triana [16] for heterogeneous applications; 
Taverna [17] is a framework to executed bioinformatics 
applications in distributed environments using MyGrid [18] 
middleware. 
Most 
of 
workflow 
tools 
have 
limitations 
for 
bioinformatics applications such as: they require applications 
to be recompiled or modified, they support internal data 
sources with specific data structures; they operate with 
specific platforms, and cluster or grid middlewares; they are 
designed for solving needs of specific scientific fields; and 
they regularly require researchers to use complex commands 
involving consuming-time tasks. 
Although dedicated cluster and grid infrastructures 
provides large computational capabilities, in a university or 
enterprise campus there are tens or hundreds of desktop 
computers that are under-utilized. Some clusters or grid 
solutions take advantage of the idle computing capabilities 
for e-Science projects, these systems are called Desktop Grid 
and Volunteer and Computing Systems (DGVCSs). Several 
DGVCS solutions has been developed using different 
approaches, 
solutions 
and 
architectures, 
including  
SETI@home [19], BOINC [20], OurGrid [21], Integrade 
[22] and UnaGrid [2].  
From the conducted survey and our experience working 
on the field, we concluded that an infrastructure allowing 
existing bioinformatics applications to be easily incorporated 
without modifications in workflows created through GUIs 
and executed on different HPC infrastructures is needed. The 
infrastructure also must allow the use of different data 
sources (internal and external), and the incorporation of MPI 
applications commonly used in bioinformatics projects. For 
getting results faster the infrastructure should operate on 
dedicated computing infrastructures and on DGVCS that 
take advantage of the idle processing capabilities of tens of 
desktop computers with different operating systems. Bio-
UnaGrid integrates the capabilities of LONI Pipeline and 
UnaGrid to provide these features.  
III. 
LONI PIPELINE, UNAGRID AND BLAST 
A. LONI Pipeline 
LONI Pipeline [1] is a free framework used to execute 
neuroscience workflows (see [23]); however its design and 
implementation allows any command line application (like 
most bioinformatics applications) to be incorporated. From a 
computational perspective, LONI differs from other 
workflow tools  in several features: it does not require 
external application being recompiled, it supports external 
data storage sources, it is hardware platform independent, 
and it can be installed on different cluster or grid 
middlewares 
using 
the 
Pipeline 
plugin 
Application 
Programming Interface (API) [1]. From a research user 
perspective LONI was designed having in mind features like 
usability, 
portability, 
intuitiveness, 
transparency 
and 
abstraction of cluster or grid infrastructures. 
LONI Pipeline uses a client/server model. The server is 
installed on the master computer of a computational cluster, 
managed by a distributed resource management (DRM) 
system such as Oracle Grid Engine (OGE). LONI uses 
standard execution commands, so any applications executed 
through command line can be incorporated without requiring 
recompilations or new developments. An application is 
defined and loaded in the server through a GUI, defining an 
XML file, called a LONI Module that contains information 
about path application executable, and number and types of 
input and output parameters. 
LONI client is a lightweight and standalone Java 
application that can be executed on Windows, Linux or Mac 
desktops. A researcher connects through the LONI client 
13
BIOTECHNO 2011 :  The Third International Conference on Bioinformatics, Biocomputational Systems and Biotechnologies
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-137-3

with the LONI server, using a customizable authentication 
protocol (LDAP, Database, etc.). The LONI modules are 
downloaded into the client and the researcher can begin to 
create workflows using simple drag and drop tools provided 
by an intuitive GUIs. Once a workflow is created, the 
researcher can begin its validation and execution. The 
workflow is executed (no commands are required) on the 
HPC infrastructure available for the master of LONI 
Pipeline. During its execution the researcher can disconnect 
of the LONI Server, and reconnect to query the partial results 
and to monitor the workflow state. 
B. UnaGrid solution 
UnaGrid is a DGVCS, developed at Universidad de los 
Andes, which provides the processing capabilities required 
by applications of different research areas at a university 
through the use and deployment of Customizable Virtual 
Clusters (CVCs) composed dynamically on demand, and 
executed on conventional desktop machines with Linux, 
Windows or Mac operating systems. The UnaGrid solution 
does not require applications to be recompiled or rewritten, 
and it has been used in projects of different research areas for 
executing BoT applications [2]. 
UnaGrid is a DGVCS that takes advantage of the idle 
processing capabilities of desktop computers within 
computer labs, in a non-intrusive manner, through the 
execution of Customized Virtual Clusters (CVCs). A CVC is 
a set of commodity and interconnected physical desktops 
computers executing virtual machines (VMs). While a 
student do his/her activities, a VM, playing a slave role of 
the CVC, is executed as a low-priority and background 
process on each desktop computer used by a student. A 
dedicated machine for the CVC plays the role of the cluster 
master. All of these VMs in execution make up a CVC, 
which has the operating system (mainly Linux), applications, 
and middleware required by the research group. 
This model allows researcher to continue executing 
applications within their native environments, guaranteeing 
high usability of the infrastructure. Users access a CVC 
through a SSH connection to the CVC master. The use of 
virtualization tools such as VMware, Oracle Virtual Box, 
Citrix or Microsoft System Center, allows adding and taking 
advantage of the capabilities of tens or hundreds of machines 
in computer labs that have Windows, Linux or Mac 
operating systems, as well as the faculty to assign and limit 
the resources consumed by the VMs. 
When a research group requires its CVC, they can deploy 
it on demand using a novel Web application called GUMA 
(Grid Uniandes Management Application). GUMA allows 
deploying on demand a previously configured CVC. A 
researcher securely access GUMA (using a Web browser) 
and defines the size (number of VMs) of the CVC he/she 
requires. GUMA automatically deploy the VMs on selected 
desktops, hiding the complexities associated with the 
location, distribution and heterogeneity of computing 
resources, and providing an intuitive graphical interface. 
GUMA also provides services for selection, shutdown and 
monitoring of physical computers and VMs. GUMA offers 
high usability to the UnaGrid solution, using the on-demand 
approach. 
C. BLAST algorithmn 
BLAST is a heuristic based application widely used to 
search for similarities between a set of biological sequences 
S and sequences in a database D. Wrapped applications using 
the 
NCBI 
BLAST 
implementation 
use 
the 
query 
segmentation approach. In this case a sequence query set S is 
divided in n subsets of sequences Si. Each subset Si is 
compared with the complete database D. A number n of 
NCBI BLAST independent jobs are executed on a 
computational cluster. Each independent job, executed by a 
cluster slave, compares a subset Si with the database D. After 
the n jobs have been executed, the files generated by each 
job are merged in a single file containing all results. 
Query segmentation offers great performance when the 
complete database D can be stored on the RAM memory of 
each cluster node. mpiBLAST adds database segmentation. 
mpiBLAST divides a sequence database D in m subsets Dj, 
and the sequence set S is compared with each subset Dj in a 
coordinated manner. A search using mpiBLAST is executed 
through a single mpiBLAST job executed coordinately on m 
slave nodes (logical MPI cluster) of a physical computational 
cluster. Each logical slave node compares the sequence set S 
with a subset Dj, and partial results of all logical slave nodes 
are stored in a unique shared file. 
Several BLAST variations modify BLAST algorithm to 
execute relevant specific analysis and accelerate searches 
[24] such as PSI-BLAST, PHI-BLAST, Mega-BLAST 
MPBLAST, WU-BLAST2 and BLASTZ. Other BLAST 
solutions adapt or implement BLAST to operate efficiently 
on specific computing infrastructures [24]. BLAST solutions 
such as HGBS and FPGA-based BLAST require specialized 
hardware. BLAST implementation for dedicated cluster 
computing infrastructures are BeoBlast, NBLAST, Soap-HT-
BLAST, 
mpiBLAST, 
Hyper-BLAST, 
dBLAST 
parallelBLAST, 
BLAST.pm, 
Parallel 
BLAST++, 
ScalaBLAST, pioBLAST and avaBLAST. These solutions 
use processing scheduling systems like Condor, PBS, OGE 
or Torque, and distributed storage systems like NFS or 
PVFS. 
Solutions such as TurboBLAST, GBTK, GridBLAST 
CloudBLAST, G-BLAST, PackageBLAST and mpiBLAST-
PIO, operates on dedicated grid computing infrastructures 
with standard middleware like Globus. W.ND BLAST [25], 
BOINC BLAST [26], and BLAST on BitDew [27], use the 
processing capabilities of DGVCSs to execute searches. 
W.ND BLAST only operates on Windows desktops. BOINC 
and BitDew require every application being modified. 
IV. 
BIO-UNAGRID INFRASTRUCTURE 
We propose the Bio-UnaGrid infrastructure, which was 
designed to facilitate to bioinformatics researchers the use of 
bioinformatics applications, the automatic execution of 
workflows, and the use of HPC infrastructures. With Bio-
UnaGrid bioinformatics researchers can use the processing 
capabilities of dedicated computational clusters, and the idle 
processing capabilities provided by the UnaGrid DGVCS. 
14
BIOTECHNO 2011 :  The Third International Conference on Bioinformatics, Biocomputational Systems and Biotechnologies
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-137-3

U
a
a
m
th
h
a
A
s
a
b
R
P
U
p
b
S
p
L
th
(
d
M
w
s
th
s
P
e
d
e
R
w
L
Using the LON
and new bioinf
and used by 
modified. Curre
he execution 
however, Bio-U
applications. 
A. Bio-Unagri
Bio-UnaGri
solutions: LON
architecture is s
The LONI 
bioinformatics 
Researchers us
Pipeline Server
User Database
password. Afte
bioinformatics 
Server through
proceed to cre
LONI Module L
For each L
he researcher 
(which can tak
directories, num
Module is con
workflow. A si
several jobs, fo
he NCBI BLA
sequence query
Pipeline Server
each one com
database. Once 
execute it, se
Researcher can
workflow, and 
LONI Pipeline 
NI features, Bi
formatics appli
researchers 
ent version of L
of MPI appl
UnaGrid suppo
id Architecture
id is based on
NI Pipeline an
shown in Figur
Pipeline Clien
researchers to 
e the LONI cl
r, through an a
e. Each researc
er authenticatio
applications in
h the LONI Pi
ate the workfl
Library, drag an
LONI Module 
must define th
ke values suc
mbers and rela
nnected with 
ingle LONI M
or example if 
AST suite, rec
y files and a
r will execute 
mparing a sequ
a workflow ha
ending it to 
n monitor and
download part
Client.   
Figure 1.  Bio-Un
o-UnaGrid allo
ication can be 
without been 
LONI Pipeline 
lications such 
ort the executio
e 
n the integrati
nd UnaGrid. T
re 1.  
nt is the main
the Bio-UnaGr
lient to connec
authentication p
cher receives a
on the research
nstalled on the
ipeline Client,
flows, using th
nd drop, and ot
(a bioinforma
he input and o
ch as strings, 
ated lists), and 
other LONI 
Module may be
the BLASTn L
ceives as inpu
a database, lik
ten independen
uence query f
as been created
the LONI P
d visualize th
tial results, thro
naGrid architectur
ows that existi
easily integrat
recompiled 
does not supp
as mpiBLAS
on of this type 
ion of two ma
The Bio-UnaG
n entry point 
rid infrastructu
ct with the LO
process agains
a username and
her can view t
e LONI Pipeli
 and he/she c
he bioinformat
ther tools. 
tics applicatio
output paramete
path files, pa
how each LO
Modules of t
 used to execu
LONI Module
ut parameters t
ke nt, the LO
nt BLASTn job
file with the 
d, researchers c
Pipeline Serv
he status of t
ough GUIs of t
re. 
ing 
ted 
or 
ort 
ST, 
of 
ain 
rid 
of 
ure.  
ONI 
st a 
d a 
the 
ine 
can 
ics 
n), 
ers 
ath 
ONI 
the 
ute 
of 
ten  
ONI 
bs, 
nt 
can 
ver. 
the 
the 
 
The
master 
The LO
research
them, i
Master
running
UnaGri
desktop
The
availab
To pro
Slaves 
of the U
workflo
access 
Throug
and n
configu
the 
w
bioinfo
distribu
When t
contact
individu
A S
Module
input a
includin
Dedica
access 
executi
local fil
Client
automa
Shared 
For 
not hav
HPC in
using d
and run
execute
on ten
provide
research
using G
analysi
B. Bio
The
bioinfo
Bio-Un
1) I
on the 
from th
2) C
Pipelin
e LONI Pipelin
component of 
ONI Pipeline S
hers; divide the
in an order ma
r distributes the
g on a Dedicat
id, composed 
p computers, w
e Dedicated 
ble for research
ovide more pr
can be deploy
UnaGrid infras
ow or during 
the GUMA Po
gh GUMA Port
number of v
ured by the Un
workflow 
ex
ormatics VMs
uted along com
the CVC DRM
t the DRM Mas
dual jobs to CVC
Shared Storag
es, the binarie
and output data
ng genomic 
ated Processing
the Shared 
ions. During w
les of the perso
is executed
atically to the L
d Storage System
r executing wo
ve to worry abo
nfrastructures.
drag and drop t
un workflows 
ed faster and tr
ns or hundred
ed by UnaGr
hers can down
GUIs, and they
s. 
informatics ap
e 
process 
t
ormatics applic
naGrid infrastru
Installation an
Shared Storag
he LONI Pipeli
Creation of a L
ne Client for ea
ne Server is a d
f a DRM such 
Server receive
e workflows in
anner, to the D
e jobs to the D
ted Processing
of VMs exe
which we will c
Processing C
hers; however it
rocessing capa
yed on demand
structure. Befo
g the workflow
ortal using a u
tal researchers 
virtual machin
naGrid support 
xecution. 
GU
s on differe
mputers labs in
M Slaves are t
ster. The DRM
VC DRM Slaves
ge System is u
es of bioinform
a required for 
databases. T
g Cluster and
Storage Sys
workflow creati
onal computer 
d, and these 
LONI Pipeline
m. 
orkflows bioin
out application
They only ne
tools provided 
using a singl
transparently o
ds of commo
rid. When a 
nload the resu
y are ready to
pplications on B
to 
incorporat
cation suites, l
ucture are summ
nd configuratio
ge System: Thi
ine Server. 
LONI Pipeline
ach executable
dedicated serv
as OGE install
s the workflow
n individual job
DRM Master. 
DRM Slaves wh
g Cluster or on
ecuted on hete
all CVC DRM 
Cluster is pe
ts capabilities a
abilities, the C
d using the GU
ore a researcher
w execution, h
sername and a
define the exec
nes (VMs), 
team, he/she r
UMA 
Portal
ent desktop 
n the universit
turn on by GU
M Master begin
s. 
used to store 
matics applica
the workflow 
The DRM M
d the CVC DR
ystem during 
ion researchers
where the LON
files are 
e Server and st
nformatics rese
ns’ commands o
eed to define 
by LONI Pipel
le click. Work
on dedicated cl
dity desktop 
workflow is
ults of all LON
o do their bioi
Bio-UnaGrid
te 
existing 
like NCBI BL
marized in five
on of the applic
is installation i
e Module using
e of the suite: f
er with the 
led locally. 
ws sent by 
bs and send 
The DRM 
hich can be 
n a CVC of 
erogeneous 
Slaves.   
ermanently 
are limited. 
CVC DRM 
UMA Portal 
r execute a 
he/she can 
a password. 
cution time 
previously 
required for 
l
deploys 
computers 
ty campus. 
UMA, they 
s to submit 
the LONI 
ations, and 
execution, 
Master, the 
RM Slaves 
workflow 
s can select 
NI Pipeline 
transferred 
tored in the 
earchers do 
or complex 
workflows 
line Client, 
kflows are 
lusters, and 
computers 
s finished, 
NI modules 
informatics 
and 
new 
LAST, into 
e steps.  
cation suite 
is executed 
g the LONI 
for this it is 
15
BIOTECHNO 2011 :  The Third International Conference on Bioinformatics, Biocomputational Systems and Biotechnologies
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-137-3

n
e
th
P
b
s
a
a
w
a
c
s
b
B
C
M
a
m
M
s
c
a
A
L
o
r
D
th
L
s
s
S
D
b
B
r
th
A
p
S
W
S
im
c
im
d
[
necessary to id
executable, spe
3) Individu
he suite from t
4) Storage 
Pipeline Server
5) Using t
bioinformatics 
. This proc
system admini
application. A
application is 
workflows. Th
agilely incorp
complex workf
several applicat
be used from t
Bio-UnaGrid so
C. MPI applica
At the time
MPI applicati
applications, 
mpiJobManage
Management A
sending and m
cluster using a 
as the Oracle 
Application, su
LONI Module o
of MPI proce
received by th
DRM Slave (a 
he mpiJobMan
When an m
Linux script, c
script sends a
specifying the p
Server receives
DRM slaves. D
by the LONI 
Because of th
researchers can
he status of its 
Bio-UnaGri
Andes, involv
processing clu
Sciences, some
Windows desk
Systems and 
mplemented to
coffee, potato a
mprove coffee
different biolog
28] [29]. The i
 
dentify the inpu
ecifying its dep
al tests for ea
the LONI Pipel
of the LONI 
r. 
the modules 
researchers in
cess is execute
istrator only 
fter a LONI 
created, all re
his process al
orated, facilit
flows. Our exp
tion suites sho
the command 
olution.  
ations on Bio-U
e of this develo
ions. To allo
we 
develo
er, which use
Application AP
monitoring job
Distributed R
Grid Engine 
uch as mpiBLA
of the MPI app
sses to be us
he LONI Pipel
dedicated serv
nager wrapper. 
mpiJobManager
alled launcher
an MPI job t
process numbe
s the MPI job an
During its execu
Pipeline Serve
he design of 
n query the resu
individual pro
V. 
IMPL
id was implem
ving the curr
uster from th
e dedicated ser
ktop compute
Computing E
o support seve
and cassava, w
e, potato and c
gical organisms
implementation
ut and output p
pendences and d
ach LONI Pip
line Client. 
Pipeline Mod
from workflo
n the LONI Pip
ed by the HPC
once for each
Module for 
esearchers can
llows new ap
tating the cre
perience in th
w that any app
line can be in
UnaGrid 
opment, LONI
ow the exec
oped 
a 
w
es the Distri
PI (DRMAA). 
bs executed o
Resource Mana
(OGE). To e
AST, a research
plication to spe
sed. MPI LON
line Server and
ver or desktop 
  
r job is execut
rMPIJob. The 
o the LONI 
r of the job. Th
nd proceeds to
ution, the MPI 
er using the 
f the LONI 
ults of a whole
cesses.  
LEMENTATION 
mented at Un
rent bioinform
he Departmen
rvers and a co
ers from the 
ngineering. B
ral genomic pr
which seek gen
cassava produc
s that decrease
n is illustrated i
parameters of t
data types. 
peline Module
dule in the LO
ows created 
eline Client. 
C infrastructure
h bioinformat
a bioinformat
n use it in th
plications to 
eation of mo
e deployment 
plication that c
ntegrated into t
I did not supp
cution of M
wrapper, 
call
ibuted Resour
DRMAA allo
on computation
ager (DRM) su
execute an M
her uses the M
ecify the numb
NI Modules a
d executed on
computer) usi
ted, it executes
launcherMPIJ
Pipeline Serv
he LONI Pipeli
o execute it on t
job is monitor
mpiJobManag
Pipeline Serv
e MPI job but n
niversidad de l
matics dedicat
nt of Biologic
omputer lab w
Department 
Bio-UnaGrid w
rojects related 
nomic analysis 
ction affected 
e their producti
in Figure 2. 
the 
of 
ONI 
by 
e’s 
ics 
ics 
heir 
be 
ore 
of 
can 
the 
ort 
MPI 
led 
rce 
ws 
nal 
uch 
MPI 
MPI 
ber 
are 
n a 
ing 
s a 
Job 
ver, 
ine 
the 
red 
ger.  
ver, 
not 
los 
ted 
cal 
with 
of 
was 
to 
to 
by 
ion 
The
Master
dedicat
System
the Sha
on a de
dedicat
Science
X5560 
was ins
now ca
To t
the des
on a c
which h
RAM.
compon
from L
CPU co
through
mechan
OGE ca
Fou
on the 
2.2.20, 
mpiBLA
use 
o
implem
Module
BLAST
tBLAS
Search 
Coils, 
SuperF
mpiBLA
Res
applica
created
applica
shown 
Figure 2. 
e LONI Pipelin
r of OGE vers
ted server. We
m version 3 - N
ared Storage Sy
edicated serve
ted cluster us
es composed by
Quad Core pro
stalled the Slav
an process jobs 
test the perform
sktop computer
computer lab w
have an Intel i
On the CVC 
nent of OGE, s
LONI Pipeline 
ore and 4 GB 
h 1 GbE an
nisms were con
apabilities. 
ur bioinformati
Bio-UnaGrid i
HMMER vers
AST version 1
of 
an 
MPI
mentation vers
es were created
T 
(BLASTn,
Tx and MEG
and Calibrate
Gene3D, PI
Family, TIGR
AST.  
searchers now
ations from wo
d through the
ations, and ex
in Figure 3.  
 Bio-UnaGrid im
ne Server vers
sion 6.2 updat
e use an NFS
Network Attach
ystem. The Dat
er using MySQ
sed by the D
y 6 servers eac
ocessor of 2.8 
ve component 
sent from LON
mance of bioin
rs, a CVC wit
with Windows
i5 processor of
we installed D
so these VMs c
Clients. VMs 
of RAM. All 
nd 10GbE lin
nfigured for Bo
cs application 
implementation
sion 2.3.2, Inte
1.6.0. Because 
I 
implement
sion 1.2.7 w
d for the applic
, 
BLASTp, 
GABLAST); 3
e); 11 for Inte
IR, Panther, 
Rfam and fP
w can execut
orkflows. An 
e LONI Pipe
xecuted on the
mplementation. 
sion 5.0.2 and
te 5, were inst
Sv3-NAS (Ne
hed Storage) so
tabase User wa
QL version 5.1
epartment of 
ch one with an 
GHz and 4 GB
of OGE, so th
NI Pipeline Cli
nformatics appl
th 21 VMs wa
s 7 desktops 
f 3.46 GHz an
Debian 4.0 and
can also proces
 were configu
nodes are inte
nks and fault
oT application
suites have bee
n: NCBI BLA
erProScan versi
mpiBLAST r
tation, 
the 
was installed. 
cation suites: 6
BLASTx, 
3 for HMME
erProScan (Bla
Pfam, SEG, 
PrintScan) an
te these bioi
example of a
eline Client w
e HPC infras
 
d the DRM 
talled on a 
twork File 
olution like 
as installed 
1.5. On the 
Biological 
Intel Xeon 
B of RAM, 
hese servers 
ients. 
lications on 
as deployed 
computers, 
nd 8 GB of 
d the Slave 
ss jobs sent 
ured with a 
erconnected 
t tolerance 
ns using the 
en installed 
ST version 
ion 4.6 and 
equires the 
MPICH-2 
21 LONI 
6 for NCBI 
tBLASTn, 
ER (Build, 
astProdom, 
SMART, 
nd 1 for 
informatics 
a workflow 
with these 
structure is 
16
BIOTECHNO 2011 :  The Third International Conference on Bioinformatics, Biocomputational Systems and Biotechnologies
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-137-3

r
a
R
F
in
e
r
o
m
e
C
e
c
s
in
e
a
o
b
K
w
in
e
u
c
L
During 
th
researchers can
and of each 
Running, Queu
Figure 3, all file
n the Shared 
executed on th
researchers do n
operations are e
VI. 
P
Bio-UnaGri
machines; we
environments, i
CVC. We selec
execute 
these
characterization
segmentation) 
nfrastructures c
For testing
executing BoT 
application, var
of the sequen
between nucleo
KB) sequences
were executed
ndependent 
p
environments 
using a CPU c
complete nt dat
LONI Module, 
he 
workflow 
n see the statu
individual L
ued, and Waitin
es (query, datab
Storage Syste
he configured d
not have to use
executed using 
PERFORMANCE
id support ded
e executed p
in the dedicate
cted the high p
e 
tests. 
A 
n for the execu
and mpiBLA
can be found in
g the perform
applications w
rying the numb
nce set. We e
otide sets of 4
, and the nt dat
d using 5, 1
processes 
(B
each independ
core to compa
tabase. We use
for dividing th
Figure 3
execution 
s of the comp
LONI Module
ng). In the work
bases and FAS
em. This work
distributed infr
e any command
GUIs. 
E TESTS AND RE
dicated cluster
performance t
ed cluster and i
popular BLAS
detailed 
tim
ution of BLAST
AST in clus
n [30] and [4] r
mance of Bio
we used the N
ber of CPU cor
executed BLA
480 (487 KB) 
tabase (28 GB)
10, 15 and 
BoT 
BLASTn
dent process 
are a sequence
e query segmen
he sequence se
.  A bioinformtic
bioinformatics
lete workflow
e (Completed,
kflow shown in
STA) are stored
kflow is being
rastructure and
d; all workflow
ESULTS 
rs and desktop
tests in both
in the UnaGrid
T algorithm to
me-performance
T (using query
ster and grid
respectively. 
o-UnaGrid for
NCBI BLASTn
res and the size
ASTn searches
and 960 (954
). The searches
20, BLASTn
n). 
In 
both
was executed
e set with the
ntation, using a
ets in the same
s workflow execu
s 
, 
, 
n 
d 
g 
d 
w 
p 
h 
d 
o 
e 
y 
d 
r 
n 
e 
s 
4 
s 
n 
h 
d 
e 
a 
e 
numb
seque
A
avera
tests,
meas
time 
algor
seque
UnaG
using
were 
times
960 
13,14
12,28
T
cluste
show
Figu
uted on Bio-UnaGr
ber of proces
ences were use
All tests were e
age time. Two
, the execution 
sure that indica
of a parallel al
rithm executed
ential searches
Grid CVC and
g a standalone
e executed usin
s of the sequen
sequences on 
40 seconds re
88 and 19,488 
The execution t
er and the Un
wn in figure 4. 
ure 4.  Execution 
480 and 9
rid. 
sses; subsets 
ed for the 480 s
executed 3 tim
 metrics were 
time and the s
ates the impro
lgorithm when 
d in a seque
s were execut
d in a server 
e BLASTn sea
ng a single C
ntial searches 
the dedicated
espectively, an
seconds.  
times of the s
naGrid CVC, u
time of BoT BLA
960 sequences, and
 
of 96, 72, 4
sequence set. 
mes and we cal
used in the pe
speedup. The sp
ovement in the
it is compared
ential manner 
ted in a desk
of the dedicat
arch. Sequentia
CPU core. The
with the sets o
d server were 
nd on the des
earches on the
using BoT BL
ASTn searches betw
d the nt database.
48 and 24 
culated the 
erformance 
peedup is a 
e execution 
d with same 
[31]. The 
ktop of the 
ted cluster, 
al searches 
e execution 
of 480 and 
6,436 and 
sktop were 
e dedicated 
LASTn, are 
 
ween sets of 
17
BIOTECHNO 2011 :  The Third International Conference on Bioinformatics, Biocomputational Systems and Biotechnologies
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-137-3

s
n
w
p
s
th
s
r
p
U
1
F
th
s
o
u
e
m
c
th
e
2
s
p
m
o
T
F
 
F
 Results in F
searches in both
number (CPU 
with 960 sequ
processes). Tak
sequential sear
he speedups sh
search with 48
reduced from 6
providing resul
UnaGrid CVC
12,288 to 936 s
For the search 
he dedicated c
seconds when 
on the Bio-Un
using 20 proces
For testing 
environments, 
mpiBLAST. In
coordinated on 
he additional
executed in ano
21 OGE Slav
segmentation to
partitions 
usi
mpiBLAST, us
of these searche
The speedups fo
Figure 5.  Speedu
Figure 6.  Executi
an
Figure 4 show 
h environment
cores) is incr
uences on the 
king as referen
rches on each 
hown in Figure
80 sequences o
6,436 to 589 se
lts 10.9 times 
C the executio
seconds, using 
with 960 sequ
cluster was red
15 processors 
naGrid CVC fr
ssors (10x faste
the execution 
we execute
n these tests w
5, 10, 15 and
l process ca
other CPU cor
ves on Figur
o divide the d
ing 
the 
mp
sing a LONI M
es using mpiBL
for these search
ups of BoT BLAST
960 sequences, an
ion times of mpiB
nd 960 sequences, 
that the execut
ts, decrease wh
reased (except 
dedicated clu
nce the executi
environment, 
5.  The execut
on the dedicate
econds (using 2
faster (10.9x)
on time was 
20 processors 
uences, the exe
duced from 13
were used (6.
rom 19,488 to
er).  
of MPI applic
ed the same
we executed M
d 20 CPU core
alled mpiJobM
re (this is the r
re 2). We u
database in 5, 
piformatdb 
in
Module. The ex
LAST are show
hes are shown in
Tn searches betwe
nd the nt database. 
LAST searches be
and the nt databas
tion time of the
hen the process
in the search
uster using 20
ion time of the
we calculated
tion time of the
ed cluster was
20 processors)
). On the Bio-
reduced from
(13.1x faster)
ecution time on
3,140 to 2,152
1x faster), and
o 1,946 second
cations on both
e tests using
MPI processes
s. In each test,
Manager was
reason to show
used database
10, 15 and 20
nstalled 
with
xecution times
wn in Figure 6
n Figure 7. 
en sets of 480 and
 
etween sets of 480
e. 
e 
s 
h 
0 
e 
d 
e 
s 
, 
-
m 
. 
n 
2 
d 
d 
h 
g 
s 
, 
s 
w 
e 
0 
h 
s 
. 
 
d 
 
 
Figu
S
envir
when
dedic
480 s
when
UnaG
12,28
For t
the d
secon
the B
20 pr
R
can p
by a 
time
idle 
addit
dedic
13.1
work
W
allow
work
cluste
click
bioin
modi
analy
distri
of m
comp
proce
comp
W
comp
bioin
HMM
tests 
UnaG
13.1
ure 7.  Speedups o
960 s
Similar to prev
ronments, in m
n the number
cated cluster th
sequences was
n 15 processor
Grid CVC the
88 to 2,597 sec
the search with
dedicated clust
nds when 20 p
Bio-UnaGrid C
rocessors (3.7x
Results from Fi
provide speedu
a dedicated clu
when the num
processing 
tional processi
cated computa
times (13.1x)
kflows. 
VII. CON
We proposed
ws bioinforma
kflows using G
er and grid co
k. Bio-UnaGri
nformatics app
ifications. Wit
ysis of applica
ibuted comput
more processing
puting infrastru
essing capabil
puters common
We implemente
posed of 6 ser
nformatics app
MER, InterPro
with NCBI BL
Grid can reduc
times faste
of mpiBLAST sear
sequences, and the
vious tests, us
most tests the e
r of processo
he execution t
s reduced from
rs were used (2
e execution t
conds, using 20
h 960 sequence
ter was reduce
processors were
CVC from 19,4
x faster).  
igures 5 and 7
ups similar or 
uster, decreasi
mber of process
capabilities, 
ing capabilitie
ational clusters
) the execution
NCLUSIONS AND
Bio-UnaGrid,
atics research
GUIs and exe
omputing infra
id is highly 
plications can 
th Bio-UnaGri
ation results, n
ting infrastruct
g capabilities, 
uctures, Bio-U
lities of tens 
nly available in
ed Bio-UnaGr
rvers, and a co
plication suites 
oScan and m
LASTn and mp
ce the sequent
er. These re
rches between sets
e nt database. 
ing mpiBLAS
execution time 
or is increased
time for the s
m 6,436 to 2,8
2.3x faster). O
time was redu
0 processors (4
es, the executi
ed from 13,14
e used (3x fast
488 to 5,255 se
7 show that Bi
higher to thos
ing the result 
es is increased
Bio-UnaGrid 
es to those p
s, decreasing 
n time of bioi
D FUTURE WOR
, an infrastru
hers to easi
ecute them on
astructures with
extensible a
be incorporate
id, researchers
not on technica
tures. To take 
besides using
UnaGrid also u
or hundreds o
n computer labs
rid in a dedica
omputer lab w
such as NCB
mpiBLAST. Pe
piBLAST show
tial execution 
esults show 
 
s of 480 and 
ST on both 
is reduced 
d. On the 
search with 
15 seconds 
On the Bio-
uced from 
4.7x faster). 
on time on 
0 to 4,154 
ter), and on 
econd using 
io-UnaGrid 
se provided 
generation 
d. Using the 
provides 
provide by 
more than 
informatics 
RK 
ucture that 
ily define 
n different 
h a simple 
as existing 
ed without 
s focus on 
al issues of 
advantage 
g dedicated 
use the idle 
of desktop 
s.  
ated cluster 
with several 
BI BLAST, 
erformance 
w that Bio-
time up to 
promising 
18
BIOTECHNO 2011 :  The Third International Conference on Bioinformatics, Biocomputational Systems and Biotechnologies
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-137-3

opportunities for bioinformatics researchers to get results 
in shorter times using the idle processing capabilities 
available in computer labs using Windows, Linux or Mac 
desktops. 
As 
future 
work, 
we 
will 
incorporate 
more 
bioinformatics applications suites in Bio-UnaGrid and we 
will execute new performance tests with other applications 
such 
as 
HMMER, 
MPI-HMMER, 
ClustalW 
and 
ClustalW-MPI, in a larger grid deployment involving 
hundreds of heterogeneous desktop computers in different 
administrative domains. We will implement a shared 
storage system more scalable than NFSv3. We also plan to 
implement a fault tolerance mechanism for MPI 
applications. 
REFERENCES 
 
[1] 
I. Dinov et al., "Neuroimaging Study Designs, Computational 
Analyses and Data Provenance Using the LONI Pipeline," PLOS 
ONE, vol. 5, Sep. 2010, doi:10.1371/journal.pone.0013070. 
[2] 
H. Castro, E. Rosales, M. Villamizar, and A. Miller, "UnaGrid - 
On Demand Opportunistic Desktop Grid," Proc. 10th IEEE/ACM 
International Conference on Cluster, Cloud and Grid Computing, 
June 2010, pp. 661-666, doi:10.1109/CCGRID.2010.79. 
[3] 
S. Altschul et al., "Gapped BLAST and PSI-BLAST: a new 
generation of protein database search programs," Nucleic Acids 
Research, 
vol. 
25, 
July 
1997, 
pp. 
3389-3402, 
doi:10.1093/nar/25.17.3389. 
[4] 
A. Darling, L. Carey, and W. Feng, "The design, implementation, 
and evaluation of mpiBLAST," Proc. ClusterWorld 2003, June 
2003. 
[5] 
S. Kubica, T. Robey, and C. Moorman, "Data parallel 
programming with the Khoros data services library," Lecture Notes 
in Computer Science, vol. 1388, 1998, pp. 963-973, doi:10.1007/3-
540-64359-1_762. 
[6] 
S. Pieper, M. Halle, and R. Kikinis, "3D SLICER," Proc. IEEE 
International Symposium on omedical Imaging: Nano to Macro, 
April 2004, pp. 632-635, doi:10.1109/ISBI.2004.1398617. 
[7] 
R. MacLeod,  D. Weinstein, D. Germain, D. Brooks, C. Johnson, 
and S. Parker, "SCIRun/BioPSE: integrated problem solving 
environment for bioelectric field problems and visualization," 
Proc. IEEE International Symposium on Biomedical Imaging: 
Nano 
to 
Macro, 
April 
2004, 
pp. 
640-643, 
doi:10.1109/ISBI.2004.1398619. 
[8] 
Y. Simmhan, B. Plale, and D. Gannon, “Karma2: Provenance 
Management for Data Driven Workflows,” International Journal of 
Web Services Research, vol. 5, April 2008, pp. 1-22, 
doi:10.4018/jwsr.2008040101. 
[9] 
B. Lucas, B. Landman, J. Prince, and D. L. Pham, “MAPS: A Free 
Medical Image Processing Pipeline,” Proc. 14th Annual Meeting 
of the Organization of Human Brain Mapping, June 2008. 
[10] Y. Simmhan, R. Barga, C. Ingen, E. Lazowska, and A. Szalay, 
“Building the Trident Scientific Workflow Workbench for Data 
Management in the Cloud,” Proc. Third International Conference 
on Advanced Engineering Computing and Applications in 
Sciences, Oct. 2009, pp. 41-50, doi:10.1109/ADVCOMP.2009.14. 
[11] B. Ludäscher et al., “Scientific workflow management and the 
Kepler system,” Concurrency and Computation: Practice & 
Experience, 
vol. 
18, 
Aug. 
2006, 
pp. 
1039-1065, 
doi:10.1002/cpe.v18:10. 
[12] Y. Zhao et al., “Swift: Fast, Reliable, Loosely Coupled Parallel 
Computation,” Proc. IEEE Congress on Services, IEEE, July 2007, 
pp. 199-206, doi:10.1109/SERVICES.2007.63. 
[13] D. Krefting et al., “MediGRID: Towards a user friendly secured 
grid infrastructure,” The international journal of grid computing-
theory methods and applications, vol. 25, March 2009, pp. 236-
336, doi:10.1016/j.future.2008.05.00. 
[14] E. Deelman et al., “Pegasus: Mapping Scientific Workflows onto 
the Grid,” Lecture Notes in Computer Science, vol. 3165, 2004, 
pp. 131-140, doi:10.1007/978-3-540-28642-4_2. 
[15] D. Thompson, J. Braun, and R. Ford, “OpenDX: Paths to 
Visualization,” Proc. VIS, Inc. 
[16] D. Churches et al., “Programming scientific and distributed 
workflow with Triana services,” Concurrency and Computation: 
Practice & Experience, vol. 18, Aug. 2006, pp. 1021-1037, 
doi:10.1002/cpe.v18:10. 
[17] T. Oinn et al., “Taverna: lessons in creating a workflow 
environment for the life sciences,” Concurrency and Computation: 
Practice and Experience, vol. 18, Aug. 2006, pp. 1067-1100, 
doi:10.1002/cpe.v18:10. 
[18] MN Alpdemir et al., “Contextualised workflow execution in 
MyGrid,” Lecture Notes In Computer Science, vol. 3470, 2005, 
pp. 444-453, doi:10.1007/11508380_46. 
[19] D. Anderson, J. Cobb, E. Korpela, M. Lebofsky, and D. 
Werthimer, “SETI@home An Experiment in Public-Resource 
Computing,” Communications of the ACM, vol. 45, Nov. 2002, 
pp. 56-61, doi:10.1145/581571.581573. 
[20] D. Anderson, “BOINC: A System for Public-Resource Computing 
and Storage,” Proc. in 5th IEEE/ACM International Workshop on 
Grid, IEEE, Nov. 2004, doi:10.1109/GRID.2004.14. 
[21] B. Francisco and M. Rodrigo, “The OurGrid Approach for 
Opportunistic Grid Computing,” Proc. First EELA-2 Conference, 
Feb. 2009. 
[22] A. Goldchleger, F. Kon, A. Goldman, M. Finger, and G. Bezerra, 
“InteGrade: object-oriented Grid middleware leveraging the idle 
computing power of desktop machines,” Concurrency and 
Computation: Practice and Experience, vol. 16, March 2004, pp. 
449-459, doi:10.1002/cpe.824. 
[23] LONI Pipeline, "Laboratory of Neuro Imaging", [Online], 
http://pipeline.loni.ucla.edu. 
[24] M. Sousa, A. Melo, and A. Boukerche, “An adaptive multi-policy 
grid service for biological sequence comparison,” Journal of 
parallel and distributed computing, vol. 70, Feb. 2010, pp. 160-
172, doi:10.1016/j.jpdc.2009.02.009. 
[25] S. Dowd, J. Zaragoza, J. Rodriguez, M. Oliver, and P. Payton, 
“Windows.NET network distributed basic local alignment search 
toolkit (W.ND-BLAST),” BMC Bioinformatics, vol. 6, April 2005, 
doi:10.1186/1471-2105-6-93. 
[26] S. Pellicer, G. Chen, K. Chan, and Y. Pan, “Distributed Sequence 
Alignment Applications for the Public Computing Architecture,” 
IEEE Transactions on NanoBioscience, vol. 7, March 2008, pp. 
35-43, doi:10.1109/TNB.2008.2000148. 
[27] H. He, G. Fedak, B. Tang, and F. Cappello, “BLAST Application 
with Data-aware Desktop Grid Middleware,” Proc. 9th IEEE/ACM 
International Symposium on Cluster Computing and the Grid, 
IEEE, May 2009, pp. 284-291, doi:10.1109/CCGRID.2009.91. 
[28] A. Vargas, et al., “Characterization of Phytophthora infestans 
Populations in Colombia: First Report of the A2 Mating Type,” 
Phytopathology, Sep. 2009, pp. 82-88, doi:10.1094/PHYTO-99-1-
0082. 
[29] S. Restrepo et al., “Computational Biology in Colombia,” PLOS 
Computational 
Biology, 
vol. 
5, 
Oct. 
2009, 
doi:10.1371/journal.pcbi.1000535. 
[30] E. Afgan, and P. Bangalore, "Performance Characterization of 
BLAST for the Grid," Cluster Computing, vol. 13, Feb. 2010, pp. 
385–395, doi:10.1007/s10586-010-0121-z. 
[31] A. Shah, G. Folino, and N. Krasnogor, “Toward High-Throughput, 
Multicriteria Protein-Structure Comparison and Analysis,” IEEE 
Transactions on NanoBioscience, vol. 9, June 2010, pp. 144-155, 
doi:10.1109/TNB.2010.2043851. 
19
BIOTECHNO 2011 :  The Third International Conference on Bioinformatics, Biocomputational Systems and Biotechnologies
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-137-3

