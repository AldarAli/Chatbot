151
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Development of a VR Simulator for Speed Sprayer Operation Training  
 
Yu Tanaka, Oky Dicky Ardiansyah Prima, Katsuyoshi Hotta,  
Kanayo Ogura, and Koichi Matsuda 
Graduate School of Soft. and Info. Science, Iwate Prefectural University 
152-52 Sugo, Takizawa, Iwate, Japan 
email: g231t024@s.iwate-pu.ac.jp, prima@iwate-pu.ac.jp,  
g236q004@s.iwate-pu.ac.jp, {ogura_k, matsuda}@iwate-pu.ac.jp 
Shoichi Yuki 
 
YAMABIKO Corporation 
10-2 Sugo, Takizawa, Iwate, Japan 
email: s_yukixx@yamabiko-corp.co.jp 
 
Abstract— Speed (boom) sprayers, the agricultural chemical 
spraying vehicles, are generally used in orchards of grapes and 
apples to efficiently control pests. Speed sprayers require skill 
to operate, and improper operation can result in pesticides 
drifting off the field. It also requires significant time to train the 
operator, as adjustments to travel speed, pressure, and nozzle 
selection need to be considered. Meanwhile, to make the 
operation of the speed sprayer easier and safer, improvements 
to the control panel in the cockpit have become necessary. In 
this study, we developed a simulator of a speed sprayer based on 
Virtual Reality (VR) and conducted analysis of the driving and 
operation of the speed sprayer. Experiments using the 
developed simulator with ten subjects showed that the trend of 
the subjects’ steering wheel rotation angles was the same as that 
of an actual speed sprayer driver. Furthermore, the head 
posture when operating the spray button were also the same as 
that of the driver operating the actual speed sprayers. While 
there is room for improvement in terms of the real motion 
cueing and field of view provided to the user, this simulator 
allowed us to practice the basic operations necessary for 
pesticide spraying. 
Keywords-vr-simulator; speed sprayer; virtual reality; training 
simulator; deep feedforward neural network. 
I. 
INTRODUCTION 
Orchards have become an important core sector of 
agriculture in terms of production value, land use, and 
development of local agriculture in Japan. Many orchards are 
sprayed with pesticides. Speed sprayers (boom sprayers) have 
been introduced to improve the efficiency and labor saving of 
pesticide spraying in orchards. However, many orchardists are 
aging and depopulating, making succession training and 
securing labor a major social problem. This study extends our 
previous work on the analysis of the development of a Virtual 
Reality (VR) simulator for speed sprayers [1]. 
The first domestically produced speed sprayer was made 
in 1957 in Japan. Early speed sprayers were towed by tractors, 
but in 1965, they were replaced by self-propelled ones. There 
are several issues with the use of speed sprayers in terms of 
occupational safety and environmental protection. Speed 
sprayers used in orchards increase the amount of pesticides 
sprayed due to the large spray area. Since the Japanese 
Ministry of Health, Labor and Welfare introduced a positive 
list system for pesticide residues in food, feed additives, and 
veterinary drugs [2], measures to prevent pesticide drift into 
adjacent fields have become urgent.  
Many studies have been conducted to minimize the effects 
of pesticide drift. These studies include the improvement of 
spray nozzles, the development of remotely operated sprayers, 
and the development of unmanned sprayers. The low-drift 
nozzle could not compensate for the increase in spray drift due 
to the increase in sprayer speed [3]. The sprayer travel speed 
had a significant influence on the drift values. However, the 
coarse spraying did not result in that much higher drift [4]. 
While various unmanned pest control machines have been 
developed for more accurate spraying [5][6][7], they have not 
been widely adopted. 
To operate a speed sprayer, the relationship between travel 
speed, pressure, and nozzle selection must be considered. This 
operation, therefore, requires a skilled operator. Driving skills 
are expected in areas where the vehicle must pass through a 
narrow path between branches. On the other hand, the speed 
sprayer also needs to be improved as well, since it is difficult 
to see the surrounding from inside the vehicle and the 
operation panel is complicated. To solve these problems, there 
is a need to develop a speed sprayer simulator. This simulator 
will enable improvement in the work capacity of the spraying 
sequence and the interface for operating the sprayer. A more 
efficient interface needs to be developed to simplify tasks that 
require technical skills. 
Research and development of driving simulators are now 
common because they allow drivers to examine any scenario 
or situation. During the simulation of each driving task, 
sensory-based operation information can be analyzed. 
Attempts were made to develop a tractor driving simulator to 
prevent accidents during work [8][9]. Fujimoto et al. (2016) 
simulated the spray distribution of a speed sprayer using the 
coverage of water-sensitive paper placed in the path of travel 
[10]. By integrating these studies, the training simulator can 
be constructed to improve the operation skills of speed 
sprayers, but such an effort is still developing. 
In this study, we build a speed sprayer simulator using VR 
and analyze the driving and operation of the speed sprayer. To 
provide the user with the experience of operating a speed 
sprayer, the control buttons, steering wheel, and other control 
interfaces are made to resemble those of an actual speed 
sprayer. Information obtained from the VR simulator, such as 
button operations, head posture while driving, and steering 
wheel rotation, is recorded. Cameras were installed in the 
cockpit of the actual speed sprayer to obtain the same 
information through automatic image recognition. Finally, we 
analyze the data from the VR simulator and the actual field 
operation of the speed sprayer to evaluate the usability of the 
simulator. 

152
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
The rest of this paper is organized as follows. Section II 
discusses the speed sprayer in an orchard. Section III describes 
the proposed speed sprayer simulator. In Section IV, we 
describe our experiments with the simulator and the results. 
Finally, Section V summarizes the results of this study and 
discusses future perspectives. 
II. SPEED SPRAYER IN ORCHARDS 
Air blast sprayers are used to apply pesticides, plant 
growth regulators, and foliar nutrients to trees in orchards. 
Air-blast sprayers can be adapted to different orchard 
conditions by adjusting the fluid and air delivery systems [11]. 
A Speed sprayer is an air blast sprayer used for spraying in 
apple orchards, vineyards, etc. This type of sprayer sprays the 
pesticide close to the tree, thus reducing the loss of pesticide.  
The Speed sprayer is capable of adapting to the spray 
pattern of the tree. There are four types of tree patterns in 
orchards: the standing tree, the dwarf tree, the tunnel tree, and 
the slope types. Figure 1 shows the speed sprayer 
(SSV1091FSC) 
manufactured 
by 
the 
YAMABIKO 
Corporation, which can carry 1000L of pesticide [12]. The 
speed sprayer can spray pesticides on the entire tree by 
adjusting the nozzle and the plates that determine the spraying 
direction as follows.  
(1) Standing trees : Direct the nozzles and inner plates 
radially and the outer plate horizontally.  
(2) Dwarf trees 
: Direct the nozzles and inner plates to the 
top but close the top two nozzles. 
(3) Tunnel trees 
: Level the topmost plate to block the wind 
from moving upward and raise the outer plate. 
(4) Slope trees 
: Tilt the nozzle and plate in the direction 
of the slope. 
The operation of the speed sprayer is complex. The 
effective spraying operation depends on the skill of the 
sprayer operator. Skilled operators continuously check the 
condition of the trees and adjust the sprayer appropriately.  
Several studies have developed simulators to train 
operators to control agricultural vehicles in orchards [1][13]. 
Through the simulator, users can experience a variety of 
training courses, which is expected to improve their driving 
skills. There have also been attempts to improve the efficiency 
 
Figure 1. The electric speed sprayer (SSV1091FSC) manufactured by the YAMABIKO Corporation used in this study. 
 
Figure 2. Sensors used in our VR simulator. 

153
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
of spraying by introducing machine vision, and its 
effectiveness has been confirmed in spraying discrete targets 
[14]. 
 
III. SPEED SPRAYER SIMULATOR FOR OPERATION TRAINING 
In developing a simulator for speed sprayers, we first 
consider how to obtain information on operator behavior not 
only on the simulator, but also in the actual field. Firstly, the 
sensors must be able to track the driver's head movement and 
finger movement with Six Degrees of Freedom (6-DoF). 
Observing the driver's head movement is important because 
the driver needs to be aware of the trees to be sprayed while 
operating the nozzle and plates of the speed sprayer. By 
tracking the finger, we can confirm whether the control 
buttons for nozzles and plates have been pressed. Secondly, 
the driver must be able to operate the vehicle using the 
physical car controls. Finally, the driver must be able to feel 
the car controls in virtual and physical environments in the 
same way. The cockpit of the speed spray in the actual field 
has two camera sensors, one for the front and one for the rear, 
to detect the driver's behavior while driving and the steering 
wheel rotations.  
A. Sensors 
Sensors include wheel and pedal sensors, VR platform 
sensors, and cameras. All sensors used in this study are as 
follows. 
A.1. Logitech G27 Force Feedback Wheel and Pedal 
The Logitech G27 is a gaming racing wheel compatible 
with PlayStation 2 and 3. These devices were mounted on a 
cockpit frame manufactured by Rossomodello [15]. Figure 2 
shows the steering wheel and pedal of Logitech's G27 attached 
to the frame and the speed sprayer in the actual field. The 
steering is equipped with a dual-motor force feedback system 
that enables the steering to receive inputs from the unevenness 
of the road surface while driving. The cockpit is equipped with 
a seat that can be adjusted and reclined. The Logitech G27 
operates on Windows and Mac with the Logitech G27 driver.  
A.2. VR Platform: Oculus Quest 2 
Oculus Quest 2 is a head-mounted display (HMD)-based 
VR device developed by Facebook, featuring a 6-DoF angular 
and linear tracking system that can measure head pose and 
hand gestures. This system uses Inertial Measurement Units 
(IMUs) that assess linear acceleration and rotational velocity 
with low latency and cameras in the HMD that creates a 
Three-Dimensional (3D) map of the room space and hand 
landmarks of the user. The initial position of the HMD is pre-
calibrated against the position of the car steering wheel since 
TABLE I.  SENSORS USED IN THIS STUDY 
 
 
Sensors
Data
Output
Steering wheel rotation
-180.0° ~ 180.0°
Pedal:
- Accelerator
0.0 (low) ~ 1.0 (high)
- Brake
0.0 (low) ~ 1.0 (high)
Head pose
- Position
x , y , z (m)
- Pitch
-180.0° ~ 180.0°
- Yaw
-180.0° ~ 180.0°
- Roll
-180.0° ~ 180.0°
3D Hand landmark
19 points
GoPro Hero8
Image 
3,840x2,160px/60fps (max)
Oculus Quest 2
Logitech G27
 
(a) Exterior 
 
 
(b) Cockpit 
Figure 3. 3D model of the SSV1091FSC in our simulator. 
 
Figure 4. Mark of the steering wheel captured by the rear camera. 

154
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
no auxiliary devices, such as base stations, are used to obtain 
the absolute position of the driver. 
A.3. GoPro Hero8 Cameras 
Two cameras, as shown in Figure 2, are synchronized to 
capture videos of the cockpit of the speed sprayer in the actual 
field from two directions, front and rear. Videos are captured 
at 60fps with a resolution of 3,840 x 2,160px. The 60fps 
cameras are considered acceptable because there is no high 
speed movement in driving or button operation of the speed 
sprayer. The resulting video from the front camera is analyzed 
by image processing techniques to obtain the driver's head 
pose, whereas the video from the rear camera is to obtain the 
driver's finger movements and steering wheel rotation. 
Table I shows data collected from each sensor. The 
Logitech G27 outputs car driving information such as steering 
wheel rotation, gas and brake pedals, and sends vibrations 
from the steering wheel. The amplitude of the vibration can be 
set in proportion to the speed of the vehicle. The Oculus Quest 
2 tracks the driver's head pose and finger movements.  
B. Simulator 
Our simulator is built for the Unity [16] framework. We 
chose this framework because of its popularity in game and 
simulation developments. In addition, we can obtain the 3D 
data necessary to build the simulation scene from the Unity 
Asset Store, as well as the Software Development Kit (SDK) 
for the Logitech G27 used in this study, enabling us to work 
on the development in a short time.  
Figure 3 shows the 3D model of the SSV1091FSC placed 
in a virtual scene. The scale has been adjusted to give the 
landscape seen from the model the same appearance as the 
realistic landscape. Additionally, to simplify the operation of 
the buttons for spraying the pesticide, these buttons were made 
larger. The main-button opens or closes all spray nozzles, and 
the left-, center-, and right-buttons are for spraying the left, 
center, and right sides of the vehicle's direction of travel, 
respectively. The operator receives feedback from the sound 
and the color change of the buttons on pressing them. 
The initial position of the Oculus Quest 2 is pre-calibrated 
against the position of the steering wheel, and no auxiliary 
device such as a base station can be used to obtain the relative 
position between these devices. As the simulator is not in 
mixed-reality, the position of the simulated steering wheel on 
the VR need to match that of the Logitech G27. 
The Oculus Quest 2 runs at a 72fps by default, but to 
accommodate the reduced frame rate caused by our 
experimental program, all data was recorded at a sampling rate 
of 60fps. An Intel Core i7-7700 CPU 3.60GHz, RAM 32GB, 
NVIDIA GeForce GTX1070 8GB was used as the computing 
system to control the simulator. 
 
 
Figure 5. Our tool to semi-automatically specify the angle of the steering wheel marks. 

155
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
C. Field Measurement 
As with the simulator, the operator’s behavior and the 
rotation of the steering wheel during driving a real speed spray 
are measured using image processing techniques. The 
methods used to quantify these behaviors are described below. 
C.1. Head posture and hand movements 
To estimate the head posture from a front-facing camera, 
facial landmarks need to be extracted from the detected face 
area. There have been many studies on extracting facial 
landmarks [17], but we adopted the MediaPipe framework for 
this study [18]. This framework provides customized 
machine learning solutions for face detection, facial 
landmark extraction, and hand tracking. 
To extract the facial landmarks, we used the Face Mesh 
feature of the MediaPipe. From the 468 extracted landmarks, 
we selected 8 points: the corners of the eyes and mouth, as 
well as the tips of the nose and chin to determine the head 
posture. The Perspective-n-Point (PnP) solution was used to 
estimate the head posture of the 6DoF from these points [19]. 
The MediaPipe Hands tracks hands and fingers in real 
time [20]. This feature detects 21 hand landmarks. The 
tracked fingertip landmarks were used to detect pressed 
buttons and their frequency. 
C.2. Steering  wheel rotations 
Image recognition was used to estimate the rotation 
information of the steering wheel from its image. Since most 
vehicles have a mark on the steering wheel, its rotation can 
be estimated by measuring the rotation of the mark. For this 
study, we simply built a Deep Feedforward Neural Network 
(DFNN) to estimate the rotation of the steering wheel. Figure 
4 shows the mark of the steering wheel as it is captured by 
the rear camera. DFNN extracts the rectangular region of this 
mark as a Region of Interest (ROI) and estimates the rotation 
angle of the mark. 
To construct a DFNN, pairs of rotation angle data for the 
image of this ROI are required. Generating these pairs of data 
one by one manually would be a laborious task. For this study, 
we developed a tool to generate this data conveniently. This 
tool works as follows. First, it creates 360 rotated ROIs by 
 
Figure 6. Diagram of the deep feedforward neural network used in this study. 

156
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
rotating the ROI image by 1 degree 360 times. By finding a 
mark with a rotation angle of 0 degree in these images, the 
rotation angles of the images before and after this mark 
becomes ±1 degrees. Similarly, the mark with a rotation angle 
of 1 degree is followed by a mark with a rotation angle of 2 
degrees, and the mark with a rotation angle of -1 is preceded 
by a mark with a rotation angle of -2 degrees. Finally, the 
rotation angle can be specified for all 360 images in this 
manner. Figure 5 shows an example of a working interface of 
our tool. Taking a sufficiently large ROI enables the ROI 
image to be rotated in its complete shape. 
Our DFNN was built using Neural Network Console [21], 
a deep learning framework developed by Sony Group 
Corporation designed from the perspective of engineers. 
Figure 6 shows the diagram of our neural network. The input 
image is the previously described ROI images in 64x64px. 
The output is the value of the rotation angle of this image 
encoded to sine and cosine. This encoding is important when 
dealing with data with circular topology. We then apply 
convolution, batch normalization, and Rectified Linear Unit 
(ReLU) to the input image with the output filter set to 512. 
The next layer is the maximum pooling layer, defined by a 2x2 
pooling window. These procedures are repeated twice, but 
each time the output filter is reduced by half. Subsequently, 
the fully connected layer (Affine) is applied after convolution, 
batch normalization, and ReLU to obtain 4,098 outputs. Again, 
batch normalization, ReLU, and Affine are applied, and the 
absolute error between the inferred and true values are 
evaluated. The network was trained on 44 sets of ROI images 
corresponding to 1 to 360 degrees for a total of 15,840 images 
and validated with 18 sets for a total of 6,480 images. This 
network can infer the rotation angle of the steering wheel with 
an error of less than 2 degrees, which is comparable to the 
measurement accuracy of the steering wheel rotation angle of 
the simulator. 
IV. EXPERIMENT AND RESULT 
This experiment compares the operator’s behavior when 
operating the pesticide spraying in the orchard and when 
operating it on the simulator to verify whether unique 
operations are observable in both cases. A three-minute video 
of the spraying process was selected, and the nine actions 
taken during the process are summarized below. 
- Action 1: The operator moves the vehicle to the beginning 
trees to be sprayed, then presses the main-button to open all 
spray nozzles, followed by the right-button to start spraying 
to the right side of the vehicle. 
- Action 2: The operator presses the right-button again to stop 
the spraying when the vehicle passes the end point of the 
group of trees. An action is taken to confirm that the 
injection has been stopped via the right side mirror. 
- Action 3: When the next group of trees is reached, the 
operator presses the right-button again to spray towards the 
right side of the vehicle. When the next group of trees is 
reached, the operator presses the right button again to spray 
towards the right side of the vehicle. At that time, the 
operator confirms whether the spraying works correctly 
through looking at the side mirror. 
- Action 4: The operator presses the center-button to start 
spraying upward to the tall trees and confirms through the 
rear window glass. 
- Action 5: After passing over the tall trees, the operator 
presses the center-button to stop the upward spraying and 
confirms through the rear window glass. 
 
Figure 7. Operator behavior during speed spraying in the experiments of this study. 
 

157
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
- Action 6~9: The operator performs actions 2~5 to spray a 
group of trees having similar characteristics. 
The actions that occurred during the three-minute 
spraying task used in this experiment and the direction of the 
spraying are shown in Figure 7. The vehicle traveled straight 
at 3.6 km/h and there were few bumps in the path. 
To reproduce the above nine actions in the simulator, the 
operator is given voice guidance. In addition, the buttons in 
the simulator are virtual. Hence the operator is required to 
practice pressing the buttons properly while driving the 
vehicle.  
Ten subjects aged between 22 to 26 participated in the 
spraying experiment on a simulator. These subjects were 
familiar with operating content in VR or Mixed-Reality (MR) 
environments. Before starting the experiment, each subject 
was asked to practice a set of operations twice in about 10 
 
(a) Action 1 
 
(b) Action 2                                                                                                       (c) Action 3 
 
(d) Action 4                                                                                                       (e) Action 5 
Figure 8. The relationship between the vehicle and the tree for actions 1 through 5. 
 

158
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
minutes. However, since this experiment was intended to 
check how the operator would behave when performing all 
the operations, we did not require the operator to perform all 
the operations up to the exact timings. Figure 8 shows images 
of the relationship between the vehicle and the tree for actions 
1 through 5. The first action is to approach a parked speed 
sprayer to a tree and start spraying. The approach is left up to 
each subject. The images of actions 2 and 3 show that the 
trees are lined up about twice as far apart as the length of the 
vehicle. Finally, the images from actions 4 and 5 show the 
vehicles before and after passing through a tall tree. 
Data as shown in Table I was recorded from the simulator. 
Since the simulator can be played back using this data, the 
behavior can be confirmed in detail in the post-experimental 
analysis. This data was integrated into the EUDICO 
Linguistic Annotator (ELAN) [22] as time-series data, and 
the attribute information of the actions were also recorded as 
annotation labels. Likewise, in the actual speed sprayer, the 
operator's head posture, hand landmarks, and steering wheel 
rotation angles were measured from the video taken from the 
front and rear cameras in the cockpit and integrated into the 
ELAN. Figure 9 shows the pitch and yaw angles of the heads 
of ten subjects measured from the simulator during the same 
three-minute simulation apart from the actual speed sprayer 
operation. In this way, the operator's behavior data and the 
videos can be synchronized and displayed. 
A. Steering wheel rotation angle 
The rotation angle of the steering wheel in action 1 is 
shown in Figure 10. Overall, the trend of the subject's steering 
wheel rotation angle is the same as that of the actual speed 
spray operator, which show large variations in their steering 
wheel rotation angles. Since the approach steps were not 
specified in this experiment, some subjects went straight and 
then turned, while others turned and then went straight. After 
action 2, there was no significant variation in the steering 
wheel angles because the vehicle's travel path was linear. 
The speed sprayer used in this study is Four Wheel 
Steering (4WS), while the simulator vehicle used in this study 
is Two Wheel Steering (2WS), which resulted in differences 
in driving operation. We intend to analyze the effect of this 
difference as a simulator for training speed sprayer operation. 
B. Characteristics of button operations 
As described earlier, there are three operating buttons for 
spraying pesticides. We attempted to analyze the differences 
in gestures when operating the buttons by detecting both the 
operator's hand landmarks in the simulator and in the actual 
speed spray. However, we found that the operators of the 
simulator behaved in a fundamentally different way, as they 
operated the virtual buttons in the air while the actual speed 
sprayer operators placed their hands on the button platforms. 
To observe the differences in button operations, we focus 
on the head movements. In both the simulator and the actual 
vehicle, the operator faces in the direction of the button to be 
operated, hence we analyzed their head movements. Figure 
11 shows the operators' head movements in action 9. From 
this figure, we found that the trend of yaw and pitch angles 
of the head during button operation are similar even when in 
different environments.  
 
Figure 9. Three-minute of operator behavior data integrated into ELAN. 
 

159
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
There was no precise instruction for the button operation 
timing in this experiment, resulting in a difference in the 
timing of the button operation among the subjects. For 
example, the button to stop spraying must be pressed when 
the vehicle passes the end point of a group of trees, but the 
subject's judgment of the relative position of the rear-
mounted sprayer to the trees causes the button to be pressed 
at different timings. These situations appear as differences in 
the variation of the yaw angle of the head, as shown in Figure 
12. A positive yaw angle indicates that the subject is checking 
the relative position of the trees and the sprayer through the 
window, while a negative yaw angle indicates that the subject 
is checking the position of the button. The solid and dotted 
lines indicate subjects who pressed the button early (Group I) 
and late (Group II), respectively. 
 
V. CONCLUSION 
In this study, we have developed a speed sprayer simulator 
and analyzed the differences in driving between the actual 
speed sprayer and the simulator vehicle. From the video 
footage of the pesticide spraying in the apple orchard, we were 
able to analyze the operator's speed spray operation 
characteristics and reproduce each of them on the simulator. 
Button operations in the VR simulator were significantly 
different from those in the actual vehicle. However, we were 
able to see the characteristics of the button operations from the 
head movements because the face was always turned in the 
direction of the button before the button operation in our 
experiment. 
In developing the simulator, we also measured the steering 
wheel rotation and automatically detected the operator's 
behavior by image processing. In the future, we plan to 
improve the simulator by adding a function to measure the 
amount and percentage of pesticides wasted by the simulator 
operator and by integrating the control buttons into the 
steering wheel to reduce head movements. 
 
ACKNOWLEDGMENT 
This work was done in collaboration with YAMABIKO 
Corporation and the Faculty of Software and Information 
Science, Iwate Prefectural University. 
 
 
Figure 10. The rotation angle of the steering wheel in action 1. 
 
(a) Pitch angle                                                                                                                 (b) Yaw angle 
Figure 11. Operators' head movements in action 9. 
 

160
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
REFERENCES 
[1] Y. Tanaka, O. D. A. Prima, K. Ogura, K. Matsuda, and S. Yuki, 
“Toward the Development of a VR Simulator for Speed Sprayers,” 
ACHI 2021, The Fourteenth International Conference on Advances in 
Computer-Human Interactions, pp. 1-5, 2021. 
[2] The Japanese Ministry of Health, Labor and Welfare (MHLW), 
“Introduction of the Positive List System for Agricultural Chemical 
Residues in Foods,” 
https://www.mhlw.go.jp/english/topics/foodsafety/positivelist060228/
introduction.html [retrieved: September, 2021 
[3] J. C. Van de Zande, H. Stallinga, J. M. G. P. Michielsen, and P. van 
Velde, “Effect of Sprayer Speed on Spray Drift,” Annual Review of 
Agricultural Engineering, 4(1), pp. 129-142, 2005. 
[4] M. Lešnik, D. Stajnko, and S. Vajs, “Interactions between Spray Drift 
and Sprayer Travel Speed in Two Different Apple Orchard Training 
Systems,” International Journal of Environmental Science and 
Technology, 12(9), pp. 3017–3028, 2015. 
[5] M. Gonzalez-de-Soto, L. Emmi, M. Perez-Ruiz, J. Aguera, and P. 
Gonzalez-de-Santos, “Autonomous Systems for Precise spraying–
Evaluation of a robotized patch sprayer,” Biosystems Engineering, 146, 
pp. 165–182, 2016.  
[6] J. H. Han, C. H. Park, Y. J. Park, and J. H. Kwon, “Preliminary Results 
of the Development of a Single-Frequency GNSS RTK-Based 
Autonomous Driving System for a Speed Sprayer,” Journal of Sensors, 
pp. 1-9, 2019. 
[7] É. de C. C. Penido, M. M. Teixeira, H. C. Fernandes, P. B. Monteiro, 
and P. R. Cecon, “Development and Evaluation of A Remotely 
Controlled and Monitored Self-Propelled Sprayer in Tomato Crops,” 
Revista Ciencia Agronomica, 50(1), pp. 8–17, 2019. 
[8] D. O. Gonzalez et al., “Development and Assessment of a Tractor 
Driving Simulator with Immersive Virtual Reality for Training to 
Avoid Occupational Hazards,” Computer and Electronics in 
Agriculture, 143, pp. 111–118, 2017. 
[9] M. Watanabe and K. Sakai, “Development of a Nonlinear Tractor 
Model Using in Constructing a Tractor Driving Simulator,” 2017 
ASABE Annual International Meeting, pp. 1–6, 2017.  
[10] A. Fujimoto, T.  Satow, and T. Kishimoto, “Simulation of Spray 
Distribution with Boom Sprayer Considering Effect of Wind for 
Agricultural Cloud Computing Analysis,” Engineering in Agriculture, 
Environment and Food, 9(4), pp. 305–310, 2016. 
[11] P. E. Sumner, “Air Delivery Sprayer,” The University of Georgia 
Colleges of Agricultural and Environmental Sciences & Family and 
Customer Science Bulletin, 979, pp. 1-8, 2009. 
[12] Kioritz Speed Sprayer,  
https://www.yamabiko-corp.co.jp/kioritz/products/category/contents_type=59 
[retrieved: September, 2021] 
[13] D. O. Gonzalez et al., “Development and assessment of a tractor 
driving simulator with immersive virtual reality for training to avoid 
occupational hazards,” Computers and Electronics in Agriculture, 143, 
pp. 111–118, 2017. 
[14] H. Asaei, A. Jafari, and M. Loghavi, “Site-specific orchard sprayer 
equipped with machine vision for chemical usage management” 
Computers and Electronics in Agriculture, 162, pp. 431–439, 2019. 
[15] GTD Simulator, http://www.rossomodello.com/gtd/gtd-top.html 
[retrieved: September, 2021] 
[16] Unity Technologies, https://unity.com/ [retrieved: September, 2021] 
[17] V. Kazemi and J. Sullivan, “One millisecond face alignment with an 
ensemble of regression trees,” Proceedings of the IEEE conference on 
computer vision and pattern recognition, pp. 1867-1874, 2014. 
 
Figure 12. Variation in the yaw angle of the head, which resembles a difference in the timing of button operation. The solid and 
dotted lines indicate subjects who pressed the button early (Group I) and late (Group II), respectively. 

161
International Journal on Advances in Software, vol 14 no 1 & 2, year 2021, http://www.iariajournals.org/software/
2021, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[18] Lugaresi et al., “Mediapipe: A framework for building perception 
pipelines,” arXiv preprint arXiv:1906.08172, 2019. 
[19] F.Rocca, M. Mancas, and B. Gosselin, “Head Pose Estimation by 
Perspective-n-Point Solution Based on 2D Markerless Face Tracking,” 
Lecture Notes of the Institute for Computer Sciences, Social-
Informatics and Telecommunications Engineering, LNICST, 136 
LNICST, pp. 67–76. 2014. 
[20] Zhang et al., “Mediapipe hands: On-device real-time hand tracking,” 
arXiv preprint arXiv:2006.10214, 2020. 
[21] T. Narihira et al., “Neural Network Libraries: A Deep Learning 
Framework Designed from Engineers' Perspectives,” arXiv preprint 
arXiv:2102.06725, 2021. 
[22] ELAN (Version 6.0), “The Language Archive,”  
https://archive.mpi.nl/tla/elan  [retrieved: September, 2021]
 

