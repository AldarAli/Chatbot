Gesture Recognition for Humanoid Assisted Interactive Sign Language Tutoring 
 
 
Bekir Sıtkı ERTUĞRUL, Cemal GURPINAR, Hasan KIVRAK, Ajla KULAGLIC, Hatice KOSE 
Department of Computer Engineering 
Istanbul Technical University  
Istanbul, TURKEY 
{bsertugrul, gurpinarcemal, hkivrak , kulaglic, hatice.kose }@itu.edu.tr 
 
Abstract— This work is part of an ongoing work for sign 
language tutoring with imitation based turn-taking and 
interaction games (iSign) with humanoid robots and children 
with communication impairments. The paper focuses on the 
extension of the game, mainly for children with autism. Autism 
Spectrum 
Disorder 
(ASD) 
involves 
communication 
impairments, 
limited 
social 
interaction, 
and 
limited 
imagination. Many such children show interest in robots and 
find them engaging. Robots can facilitate social interaction 
between the child and teacher. In this work, a Nao H25 
Humanoid robot assisted the human teacher to teach some 
signs and basic upper torso actions which were observed and 
imitated by the participants. Kinect camera based system was 
used to recognize the signs and other actions, and the robot 
gave visual and audial feedback to the participants based on 
the performance.  
Keywords-Human-robot 
interaction; 
autism; 
imitation 
games; sign language 
I. 
 INTRODUCTION 
Communication is a vital requirement for human life.     
Language acquisition is an extremely crucial process for 
brain development and intelligence. Sign Language (SL) is 
an alternative way of communication for hearing impaired or 
autistic children who cannot communicate verbally. Sign 
Language is a visual language that is based on upper body 
movements (including hands, fingers, arms, upper torso, 
head and neck) and facial gestures. There are studies on 
visual recognition of sign language, and sign language 
tutoring with 2-D visual aids to hearing impaired people [1-
6]. Also several robots and robotic hands were utilized to 
implement sign language [7-8]. In the studies [9-11] visual 
games are employed for sign language tutoring.  
The studies introduced in this paper have been realized as 
part of an on-going research, which aims to utilize humanoid 
robots for assisting sign language tutoring due the lack of 
sufficient educational material. Also in terms of children’s 
sign language education 2-D instructional tools are found to 
be incompetent.  Therefore using humanoid robots as an 
assistive tool in sign language tutoring for children will be 
very beneficial. In the proposed system, it is intended that a 
child-sized humanoid robot is going to perform and 
recognize various elementary signs (currently basic upper 
torso gestures and words from sign languages) so as to assist 
teaching these signs to children with communication 
problems. This will be achieved through interaction games 
based on non-verbal communication, turn-taking and 
imitation that are designed specifically for robot and child to 
play together. We have used imitation based non-verbal 
interaction games with humanoid robots successfully with 
children and adults previously in [12-16]. 
Currently, American Sign Language (ASL) and Turkish 
Sign Language (TSL) are being implemented and tested 
within the project.   
In the first versions of the game, the robot was telling a 
short story verbally and through the story for some selected 
words, the robot was able to express word in the SL among a 
set of chosen words using hand movements, body and face 
gestures and having comprehended the word, the child was 
encouraged to give relevant feedback in SL or visually to the 
robot (using a colored card visualizing the word), according 
to the context of the game. The games were demonstrated 
with more than 100 preschool children with hearing ability 
and 7 preschool children from Special School for Hearing 
Impaired 
Children 
[17-22] 
(game 
demo 
videos: 
http://humanoid.ce.itu.edu.tr/). 
The current paper summarizes the attempt to extend this 
study to autistic children as a part of a PhD course entitled as 
Autism and Computational Aspects, which aims to bring 
researchers in computer science and robotics with non-
engineer experts from psychology, neurology, speech 
therapy, and autism therapists, to train students who want to 
work in this field, and for a long term brain storming event 
on possible computational solutions that can be used within 
autism therapy (recognition of autism and other related 
issues were not included in the course schedule due to time 
limitations). This paper presents one of the projects which 
are produced as an output of this collaboration, and it is 
planned to use the system and the game in the collaborative 
special schools on autism.  Autism Spectrum Disorder 
(ASD) involves communication impairments, limited social 
interaction, and limited imagination. Researchers are 
interested in using robots in treating children with ASD [23-
27]. Many such children show interest in robots and find 
them engaging. Robots can facilitate interaction between the 
child and teacher. Every child with autism has different 
needs. Robot behavior needs to be changed to accommodate 
individual children's needs and as each individual child 
makes progress. 
The game will be based on the visual cards, the cards will 
be shown to the robot to select among several signs from 
ASL and basic upper torso motion (hands side, forward, up 
etc.) Then the robot will perform the sign and wait for the 
child to imitate. The imitated action will be evaluated using 
an RGB-D camera (Kinect) and robot will give a motivating 
comment when the action is imitated with success. 
135
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
Figure 1 - Some of the cards used in these exercises 
 
II. 
SIGN RECOGNITION 
A. Hidden Markov Model 
Every hidden state in Hidden Markov Model (HMM) 
which models the hand motion is responsible for a specific 
part of given symbol sequence. In homogeneous hidden 
Markov models, the durations of segments are modeled with 
geometric distribution. These durations for every state are 
independent from each other. This constraint becomes 
important while types of hand movement and the number of 
different user increase. 
The generative models, like HMM, model the common 
probabilities between observations and states. In distinctive 
models, the probability distributions of states depend on the 
observations and it does not model the probability 
distributions of the observations subject to classes. It is 
hoped that distinctive models are more efficient in 
classification. Thus, in order to recognize hand movement, 
Conditional Random Fields (CRF), which is equivalent to 
HMM and its kinds were examined [28]. 
B. Hidden Conditional Random Fields(HCRF) 
CRF’s do not model the internal dynamics of the class, 
but inter-class dynamics and because of this constraint it is 
not suitable for the classification of time series. Therefore 
Hidden Conditional Random Field (HCRF) [29] and   
Hidden Dynamic Conditional Random Field (HDCRF) [30] 
were offered. While HDCRF models both the internal and 
external dynamics of classes, HCRF only models the internal 
dynamics and therefore it is more convenient for the isolated 
hand movement recognition problem. 
HCRF’s associate the observations with the state 
transitions instead of learning the state durations. This 
attribute increases the performance of positive samples, but it 
also increases the false acceptance rate. 
C. Input Output Hidden Markov Models(IOHMM) 
Input-Output HMMs as generative and discrete hybrid 
models show high performance in recognizing hand 
movements [31]. In IOHMM like HCRF, state transition 
probability distribution depends on the input sequence that 
consists of the function of observations [32]. 
In 
IOHMM, 
observations 
and 
state 
transition 
probabilities are calculated from input sequence using local 
models.  Radial basis functions or multi-layer perceptron’s 
can be used for local models. IOHMM’s are more 
complicated than HMMs and their training requires more 
samples than HMMs. 
 
D. Hidden Semi-Markov Models(HSMM) 
Although HSMMs are similar to the HMMs. HSMMs 
hidden states produces observation sequence from certain 
probability distributions instead of producing a single 
observation [33,34]. HSMM state creates a symbol sequence 
instead of a single symbol. Fixed-Term Models (FTM) is a 
kind of HSMM and it determines the exact staying duration 
at each state with the status of a plug-in counter. FTM solves 
the problem of the modeling of specific periods, but 
durations are still independent of each other unless being 
conditioned to the velocity and dimension.  
In order to recognize isolated hand movement, every 
action class must be modeled from positive samples. When 
an unclassified hand movement sequence comes, it is 
evaluated by all defined models in system and class 
likelihoods are calculated and the class of a model with the 
highest value is selected as a label. At [35], for evaluating 
the performance ratio and recognition rate of HMM, 
IOHMM, HCRF and FTM data set is collected. To ensure 
the independence of recognition rates from vision modules, a 
Kinect camera with infrared sensor is used. As a result of the 
experiments, it was found that performance rates of HCRF 
and IOHMM are higher than FTMs and HMMs, but they are 
slow for real time systems. For real time systems, FTMs that 
shows high performance more than HSMMs are offered. 
III. 
PROPOSED METHOD 
In the data collection phase, RGBD camera (Kinect 
Sensor) starts the input stream and sends every gestural 
motion data in the form of frame by frame. Different gestural 
motion data taken from RGBD camera (Kinect Sensor) can 
have different number of frames. Thus representation of 
every gesture pattern should be carefully modeled so that 
recognition process meets the performance criteria for robust 
recognition. 
As a first step, joint spatial coordinates (x,y,z) of skeletal 
structure for each joint are generated. Then, in order to 
provide robustness, every frame in the gestural motion data 
is expressed as a single vector of angle values (Roll, Pitch, 
Yaw) which is computed from spatial position values (x,y,z)  
for every joint node of skeletal model. Several image 
processing and computer vision techniques are used to detect 
the skeletal model of human successfully using RGBD 
cameras i.e. Kinect Sensor. Thus the job on image 
processing in determining a good feature for gesture 
classification became easier with the availability of Kinect. 
There are two main goals for this study. First, one is to 
represent gesture pattern (Sign Language (SL) word) using a 
suitable classification algorithm (K-Means). It provides 
clusters from every spatial motion data coming from the 
Kinect sensor corresponding to the related centroid of data. 
The generated probabilistic model (Hidden Markov Model) 
which is generated as a result of the system accepts this 
clustered discrete data.  
In the second phase, the recognizer cycle is started to 
provide recognition of this gesture so that recognized gesture 
patterns (SL words) are adaptively transferred to humanoid 
robot (Nao). In order to recognize the gesture, it generates a 
136
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

dynamic model for every distinct behavior (gesture). 
According to the clustered data coming from the K-Means 
algorithm, it determines hidden states (node) and observable 
variables (output labels). In the training section, data as a 
target vector (a collection of observation sequences) seeds 
into recognizer cycle to perform supervised training 
algorithm (e.g. Baum-Welch). Finally, recognizer model 
throws a unique distinct behavior as a label (related SL word/ 
gesture). 
IV. 
INTERACTIVE ISIGN GAME 
As stated before, as an outcome of the PhD level course 
on Autism and Computational Aspects, an imitation game 
was constructed using Kinect and Nao H25, based on the 
recommendations of autism therapists. Our main goal was to 
extent our studies on signing based interaction games with 
humanoid robots, to autism therapy.  
 
Figure 2 - “up”, and “side” actions. 
 
The sign imitation game is an extension of the sign 
language game; as an initial step, we used basic upper torso 
gestures, i.e. opening the arms sides, up, forward, waving 
hand, etc., in the long run, we plan to use signs from ASL 
and TSL as well. The aim of the game is to teach the children 
to recognize and imitate the gestures/signs, within a turn-
taking interaction game. The demonstrator will be the robot 
and the therapist will be able to manually assist the child, 
when the child fails to imitate the action successfully. Within 
this game it is possible to locate many of the exercises 
already being used as a part of the autism therapy. 
The game consists of 3 stages. In the first stage the child 
will learn how to play doing the gestures one by one, the 
sequence and the quality of the gestures were chosen by the 
therapist or the child. When they show a picture of the 
gesture to the robot, the robot does the gesture and waits for 
the child to repeat the action. Using a Kinect camera we can 
evaluate child’s actions and send the robot feedback. If the 
child can repeat the action then robot says “you did the 
action good”( The experts suggested us that we have to 
praise the action of the child, it is not enough to say “its 
good” or “congradulations”). Else the therapist helps the 
child manually to do the gesture. (The experts told us we 
should not let the child do the action wrong, because then the 
action will be learned wrong). 
In the second stage, the game is like a sports work out, 
each action/gesture is repeated several times without the 
picture display and the therapist get involved less. The child 
is assumed to learn each action by now. 
In the third stage, we turned the game into a musical 
play. Robot sings a song related to the actions and do the 
actions one by one and the child is expected to repeat the 
sequence of actions. 
 
Figure 3 - Demo setup with participant, Nao robot and Kinect Device (a 
second Kinect device was also employed for finger detection but not used 
within the demos) 
 
 
 
Figure 4 - Therapists help children in the first stage of the game 
 
The robot will record the success rate of the child and 
also the experimenter will record the therapist’s corrections 
and child’s success. 
These games were usually played with the therapist, or 
the video of the therapist, or another autistic child. The robot 
will act as a play mate in these games. 
V. 
GAME IMPLEMENTATION 
A. Data Collection 
Kinect and Microsoft Kinect SDK v1.5 was used to get 
human joint data from human motion. The human skeletal 
model of Kinect camera system is used to get 20 different 
joint data (X, Y, Z coordinates) over the SDK. Only upper 
torso information (shoulder, elbow, wrist coordinates) is 
used in this study, to classify the actions. Neck, head, face 
and fingers were no included within this study, but will be 
included in the overall project. 
137
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

As a start, 6 actions, namely, 3 basic sign language action 
(ASL and TSL) and 3 basic upper torso actions from 15 
university students were recorded with Kinect. For each 
action, every participant was asked to perform 4 times, of 
which 2 were used for training the system and 2 were used 
for testing. 30 frames/sec were recorded by the camera. 
Discrete HMM was used for sign recognition. The actions to 
be implemented were selected specially based on the advises 
of the therapists so that the tests can be also applied to the 
children with autism or mental disabilities.  Also the actions 
which can be implemented by the Nao robot (due to 
Kinematic constraints) were selected among these advised 
actions. In order to avoid boredom, confusion and fatique in 
children during the tests, number of the actions and the 
repeats were set to a minimum, as possible. 
The actions were introduced to half of the participants (6 
participants) using the real physical robot and the simulated 
robot, and by a real human to the rest of the participants. All 
of the participants were asked to stay on a special sign on the 
floor and implement the signs to make sure that they keep 
the same distance to the Kinect camera during the tests (1.5 
mt).  The participants were not given any information except 
the fact that they should follow the robot/human and imitate 
the actions afterwards, and their actions are recorded with the 
Kinect camera. 
First of all, a classification algorithm was used to enable 
the usage of discrete observation symbols and states on the 
motion data captured by Kinect. K-Means method was used 
to discretize the motion data and classified, then for every 
motion, in total 6 HMM was trained and the parameters were 
determined with Baum-Welch algorithm. 
Afterwards, the observation sequence was matched to the 
related action using the 6 different HMM which were trained 
before, and their probability to produce these observations 
was calculated using forward algorithm. The action matching 
the biggest probability was classified with this action. If it 
was trained with the same model in the training set, then the 
classification was successful. The complexity of the HMM 
should be detected according to the data used with the 
model. 
B. Aldebaran(NAO) Tools Used 
In this project, the goal is to empower teacher that works 
with children with ASD and to customize robot behavior to 
suit the needs of each child. NAO H-25 humanoid robot is 
used during the field studies, since it is a small size 
humanoid robot which is suitable to implement basic signs in 
the ASL and TSL, robust and safe to work with children. For 
further studies a bigger size humanoid robot platform with 5 
fingers and more DOF on arms will be used within the 
project. 
Aldebaran Robotics offer several software tools for use 
with the NAO robot. Choregraphe can be used for face 
detection, face recognition, speech, speech recognition, 
walking, recognizing special marks and dances, and 
individual control of the robot's joints. The movements can 
be performed in sequence or in parallel. Choregraphe needs 
to be used with a robot proxy, real or simulated.  
 
Figure 5 - Interaction of Aldebaran software for the NAO robot. Owners 
can use various software tools including Visual Studio to develop their own 
NAO software. 
 
The simulated proxy can be NAOqi or a sophisticated 
simulator such as NaoSim [36]. NAOqi is a piece of software 
that simulates the robot for Choreographe and tests it before 
trying on the actual robot. NAOsim is simulation tool that 
allows for robot simulation in an apartment having simulated 
furniture. Whereas NAOqi only simulates the robot, 
NAOsim simulates an environment with which the robot can 
interact. Monitor, which was called Telepathe until the 
current release, allows the user to access the robots memory, 
see through the robots two cameras and observe the 
environment as the robot senses it. Also, it is possible to use 
some of program languages, Python or C++ to program the 
NAO. 
During the implementation phase, different methods for 
the kinematic modelling of the humanoid robots are 
available such as [38]. 
C. Image Recognition 
To navigate through the exercises, the instructor used a 
set of cards with different images. Each image represents a 
different exercise. This requires image recognition software 
which was coded using C++ and OpenCV 2.3.1. The 
algorithms used in this project included SURF feature 
detection, Bag-of-Words, K-means clustering and support 
vector machines (SVM). 
OpenCV has many algorithms to detect and describe 
local feature. SURF was chosen for feature extraction and 
object detection. This is one of the most common methods. 
The Bag of words (BoW) approach is more commonly 
used in natural language processing. When used for image 
processing, an image and its graphical features are analogous 
to the document and to the words, respectively. After image 
representations had been obtained with BoW, SVM 
supervised learning was used for classification. SVM takes 
an input array which consists of data and a label. The label 
represents the class to which data belongs. There are many 
hyperplanes that could be used to classify the data sets. The 
best hyperplane is the one that gives the greatest separation 
or margin between the classes. 
The training database consisted of a scanned set of Walt 
Disney cartoon characters printed on cards. Every card was 
identified by a number which became the SVM label for that 
card. Training began by extracting the features from the 
images and computing image descriptors: 
138
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

Then, the dictionary of graphical features was determined 
with K-means clustering as shown below. The result set of 
BowKmeansTrainer was written to file in YAML format. 
That improves speed of image recognition. The last step in 
training was training the SVM. CvSVM, an implementation 
included in OpenCV was used. The SVM training results 
were also written in a file to improve performance. 
Once training was complete, the cartoon characters 
printed on the cards can be recognised by simply detecting 
the features of the images and sending them to the SVM for 
prediction. 
D. Test and Result 
6 Hidden Markov Models belonging to 6 signs were 
trained with 219 training sample and tested with 80 test 
samples. In this test every class has different state and event 
counts. The data which belongs to these classes, clustered 
corresponding to these event and state counts. The confusion 
matrix showing the tests with these parameters is shown in 
the Table 1. These state and event counts are the ones which 
gave the best solution in the test trials. At the tests, 8 state 
and 10 event were used for “side”, 8 state, 10 event were 
used for “forward”, 7 state, 9 event were used for “table”, 4 
state, 5 event were used for “car”, 4 state, 6 event were used 
for “up”, 6 state, 9 event were used for “dad”. 
The successfully matched classes were located in the 
diagonal of the table. 54 of the 80 test samples were 
classified successfully. When the unsuccessful groups were 
studied, it is observed that this is caused by the groups with 
similar features (similar actions). 
For example “table” was recognised as “forward” 1 
times, and “dad” and “up” signs were mislabeled. Another 
reason for the mislabeled signs, is that, some participants 
realized 
some 
actions 
wrong 
(not 
similar 
to 
the 
human/humanoid teacher), partly or as a whole. 
TABLE I.  
CLASSIFICATION MATRIX 
 
 
Predicted classification 
Actual 
classification 
  
Araba 
 (Car) 
Baba 
 (Dad) Forward 
Masa 
(Table) Side Up 
Araba (Car) 
13 
0 
0 
1 
1 
0 
Baba(Dad) 
0 
11 
1 
0 
1 
1 
Forward 
0 
0 
8 
0 
0 
5 
Masa(Table) 
3 
1 
1 
7 
0 
0 
Side 
3 
1 
3 
0 
6 
0 
Up 
1 
0 
2 
1 
0 
9 
In the future works, it is planned to improve the system 
recognition 
performance 
using 
probabilistic 
machine 
learning methods such as Hidden Conditional Random 
Fields, Input-Output Hidden Markov Models and Hidden 
Semi Markov Models. 
There were no statistically significant difference between 
the performance of the actions imitated from human teacher 
and humanoid teacher (simulated/physical). 
Within the studies, 1 child with normal developments, 
and 2 children with ASD (all in the age group 6-7) 
participated, as well as the university students. The children 
did not stay stable during the recording of data, which makes 
it very hard to get data. The child with the normal 
development and one of the children with ASD could finish 
all the actions, yet not exactly same with the adults. 
Unfortunately, since they did not stay still and were tired and 
leave the test before enough data was collected, the Kinect 
system could not be trained to recognize their actions. We 
are working on the necessary improvements to get data from 
children as fast and robust as possible. The system should be 
fast, and the time for calibration should be as short as 
possible. 
VI. 
CONCLUSION AND FUTURE WORK 
In this paper, we introduced a project on teaching 
children sign language by means of interaction games with 
humanoid robots. The extended versions of the studies are 
also being used with children with autism in teaching non-
verbal communication skills, imitation and turn-taking. 
Several types of media including robots with different 
embodiment, tablets, and web based applications are being 
used within the study. The experiments are being conducted 
with adults, sign language students, children with normal 
development, hearing impaired children and children with 
autism. The main aim of this interdisciplinary study is to 
build a bridge between the technical know-how and robotic 
hardware with the know-how from different disciplines to 
produce useful solutions for children with communication 
problems. Moreover we would like to increase the awareness 
among families and public. 
ACKNOWLEDGMENT 
This work was supported by The Scientific and 
Technological Research Council of Turkey under the 
contract TUBITAK KARIYER 111E283. Author Bekir S. 
Ertugrul is supported by TUBITAK BILGEM.  
REFERENCES 
[1] Staner, A. T., and A. Pentland,  “Real-Time American Sign 
Language Recognition from Video using Hidden Markov 
Models”, Technical Report TR-306, Media Lab, MIT. 
[2] Kadous, W., “GRASP: Recognition of Australian Sign 
Language Using Instrumented Gloves,” MSc. Thesis, 
University of New South            Wales, 1995. 
[3] Murakami, K. and H., Taguchi, “Gesture Recognition            
Using Recurrent Neural Networks,” Proceedings of CHI’91 
Human            Factors in Computing Systems, 1991, pp. 237-
242. 
[4] Aran, O., and L. Akarun, ―A Multi-class Classification 
Strategy for Fisher Scores: Application to Signer Independent 
Sign Language Recognition‖, Pattern Recognition, Vol. 43, 
no. 5, pp. 1717-1992, May 2010.  
[5] Aran, O., I., Ari, P. Campr, E. Dikici, M. Hruz, S. Parlak, 
L.Akarun, and M. Saraclar, Speech and Sliding Text Aided 
Sign Retrieval from Hearing Impaired Sign News Videos , 
Journal on Multimodal User Interfaces, vol. 2, n. 1, Springer, 
2008.  
[6] Caplier, A, .S. Stillittano, O. Aran, L. Akarun, G. Bailly, D. 
Beautemps, N. Aboutabit, and T. Burger, Image and video for 
hearing impaired people, EURASIP Journal on Image and 
Video Processing, Special Issue on Image and Video 
Processing for Disability, 2007.  
139
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

[7] Jaffe, D., Evolution of mechanical fingerspelling hands, for 
people who are deaf-blind, Journal of Rehabilitation Research 
and Development, 31, 236–244. 1994 
[8] Hersh, M. A. and M. A. Johnson (Eds.), Assistive Technology 
for the Hearing-impaired, Deaf and Deaf blind, 2003, XIX, 
319 p. 168 illus., Hardcover, ISBN: 978-1-85233-382-9 
[9] Adamo-Villani, N. A virtual learning environment for deaf 
children: Design and evaluation. IJASET – International 
Journal of Applied Science, Engineering, and Technology, 16, 
18-23, 2006  
[10] Lee, S., V. Henderson, H. Hamilton, T. Starner, H. Brashear, 
and S. Hamilton A Gesture-based American Sign Language 
(ASL) 
Tutor 
for 
Deaf 
Children, 
Proceedings 
of 
CHI(Computer-Human Interaction). Portland, OR. April 2005  
[11] Greenbacker, C., and K. McCoy. The ICICLE Project: An 
Overiew. First Annual Computer Science Research Day, 
Department of Computer & Information Sciences, University 
of Delaware, Feb 2008.  
[12] Shen, Q., J. Saunders, H. Kose-Bagci, K. Dautenhahn, “An 
Experimental Investigation of Interference Effects in Human-
Humanoid Interaction Games”, Proceedings of  IEEE RO-
MAN2009 ,  pp. 291 - 298  
[13] Kose-Bagci, H., K. Dautenhahn, C. L. Nehaniv, “Emergent 
Dynamics of Turn-Taking Interaction in Drumming Games 
with a Humanoid Robot”, Proceedings of. IEEE RO-MAN 
2008, pp. 346 – 353. 
[14] Kose-Bagci, H.,  E. Ferrari, K. Dautenhahn, D. S. Syrdal, and 
C. L. Nehaniv, “Effects of Embodiment and Gestures on 
Social Interaction in Drumming Games with a Humanoid 
Robot“, Special issue on Robot and Human Interactive 
Communication, Advanced Robotics vol.24, no.14, 2009. 
[15] Kose-Bagci, H, K. Dautenhahn, D. S. Syrdal, and C. L. 
Nehaniv, “Drum-mate: interaction dynamics and gestures in 
human–humanoid 
drumming 
experiments,” 
Connection 
Science, vol. 22, no. 2, pp. 103– 134, 2010. 
[16] Dautenhahn, K., C. L. Nehaniv, M. L. Walters, B. Robins, H. 
Kose-Bagci, N. A. Mirza, M. Blow  KASPAR - A Minimally 
Expressive Humanoid Robot for Human-Robot Interaction 
Research. Special Issue on "Humanoid Robots", Applied 
Bionics and Biomechanics 6(3): 369-397, 2009 
[17] Kose, H.,  R. Yorganci , H. E. Algan, and D.S. Syrdal, 
“Evaluation of the Robot Assisted Sign Language Tutoring 
using video-based studies”, SORO special issue on 
"Measuring 
Human-Robot 
Interaction, 
2012,   DOI:  10.1007/s12369-012-0142-2 
[18] Kose, H.,  R. Yorganci , and H. E. Algan, “Evaluation of the 
Robot  Sign Language Tutor using video-based studies”, 
Proceedings of 5th European Conference  on Mobile Robots  
(ECMR11),  pp. 109-114, 7-9 September, 2011. 
[19] Kose, H.,  and R. Yorganci , “Tale of a robot: Humanoid 
Robot Assisted Sign Language Tutoring”, 11th IEEE-RAS 
International Conference on Humanoid Robots, Bled, 
Slovenia (HUMANOIDS 2011) , pp 105 – 111, 2011,  
[20] Kose, H.,  R. Yorganci , and I.I. Itauma, "Robot Assisted 
Interactive Sign Language Tutoring Game", IEEE ROBIO 
Conference, Thailand, pp. 2247-2249, 7-11 december, 2011  
[21] Ertugrul, B.S., H. Kivrak, E. Daglarli, A. Kulaglic, A. 
Tekelioglu, S. Kavak, A. Ozkul, R. Yorgancı, H. Kose, 
“iSign: Interaction Games for Humanoid Assisted Sign 
Language Tutoring”, International Workshop on Human-
Agent Interaction (iHAI 2012), 11 October, 2012, Vilamoura, 
Algarve, Portugal, Accepted. 
[22] Kivrak,H., B.S. Ertugrul, R. Yorgancı E. Daglarli, A. 
Kulaglic, , H. Kose, , Humanoid Assisted Sign Language 
Tutoring, 5th International Workshop on Human-Friendly 
Robotics, Brussels, October, 2012 Accepted 
[23] Feil-Seifer, D. J., M. P. Black, E. Flores, A. B. St. Clair,E. K. 
Mower, C. Lee, M. J. Mataric, S. Narayanan, C. Lajonchere, 
P. Mundy, and M. Williams. Development of socially assistive 
robots for children with autism spectrum disorders. Technical 
Report CRES-09-001, USC Interaction Lab, Los Angeles, 
CA, 2009. 
[24] Goodrich, M. A., M. A. Colton, B. Brinton, and M. Fujiki.A 
case for low-dose robotics in autism therapy. In Proceedings 
of the 6th international conference on Human-robot 
interaction, HRI’11, pp: 143–144, New York, NY, USA, 2011.   
[25] Billard A., Robins B., Nadel J., Dautenhahn K. Building 
Robota, a Mini-Humaoid Robot for the  Rehabilitation of 
Children with Autism. RESNA Assistive Technology Journal, 
vol.19, 2006   
[26] Dautenhahn K. (2003) Roles and Functions of Robots in 
Human Society - Implications from Research in Autism 
Therapy. Robotica 21(4), pp. 443-452.  
[27] Kozima H., C. Nakagawa, and Y. Yasuda. Children-robot 
interaction: a pilot study in autism therapy. Prog Brain Res, 
164:385–400, 2007. 
[28] Lafferty J. D., A. McCallum, ve F. C. N. Pereira (2001), 
“Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data,” in Proceedings of 
the 
Eighteenth 
International 
Conference 
on 
Machine 
Learning. 2001, pp. 282–289,  
[29] Wang S. B., A. Quattoni, L. Morency, ve D. Demirdjian 
(2006), “Hidden conditional random fields for gesture 
recognition,” Proceedings of the 2006 IEEE Computer 
Society Conference on Computer Vision and Pattern 
Recognition, USA, 2006, pp. 1521–1527 
[30] Morency L., A. Quattoni, ve T. Darrell (2007), “Latent-
dynamic discriminative models for continuous gesture 
recognition,” Computer Vision and Pattern Recognition, IEEE 
Computer Society Conference on, vol. 0, pp. 1–8, 2007. 
[31] Keskin C. ve L. Akarun (2009), “Stars: Sign tracking and 
recognition system using input-output HMMs,” Pattern 
Recogn. Lett., vol. 30, pp. 1086–1095, September 2009. 
[32] Bengio Y. ve P. Frasconi (1996), “Input-output HMM’s for 
sequence 
processing,” 
IEEE 
Transactions 
on 
Neural 
Networks, vol. 7, no. 5, pp. 1231–1249, September 1996. 
[33] Yu S. ve H. Kobayashi (2006), “Practical implementation of 
an efficient forward-backward algorithm for an explicit-
duration hidden markov model,” IEEE Transactions on Signal 
Processing, vol. 54, no. 5, pp. 1947–1951, 2006. 
[34] Murphy K. P. (2002), “Hidden semi-markov models,” M.I.T. 
Technical Report, 2002. 
[35] Keskin C. (2011), Belirli süre modelleri ile izole el hareketi 
tanıma", IEEE Sinyal İşleme ve İletişim Uygulamaları 
Konferansı, Antalya, 2011. 
[36] Aldebaran 
Robotics 
Choregraphe. 
 
[Online].  
http://www.aldebaran-robotics.com/en/ 
, 
Available: 
13.02.2012 
[37] Wang J.G., and Y. Li , "A Cooperated-Robot Arm Used for 
Rehabilitation Treatment with Hybrid Impedance Control 
Method", Intelligent Robotics and Applications, Eds by H. 
Liu, H. Ding, Z. Xiong and X. Zhu, LNCS 6425, 2010, 
pp.451-462.
 
 
140
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

