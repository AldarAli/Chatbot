Text Document Clustering for Topic Discovery by
Hypergraph Construction
Wei-San Lin
Graduate Institute of Biomedical Informatic
Taipei Medical University
Taipei, Taiwan 110
Email: g658102003@tmu.edu.tw
Charles Chih-Ho Liu
Cathay General Hospital
Taipei, Taiwan 106
Email: chliu@cgh.org.tw
I-Jen Chiang
Graduate Institute of Biomedical Informatic
Taipei Medical University
Taipei, Taiwan 110
Email: ijchiang@tmu.edu.tw
Abstract—The paper presents a hypergraph model and HYPER-
GRAPH DECOMPOSITION ALGORITHM for text document
clustering. The experiments on three different data sets from
news, Web, and medical literatures have shown our algorithm is
signiﬁcantly better than traditional clustering algorithms, such as
K-MEANS, PRINCIPAL DIRECTION DIVISIVE PARTITION-
ING , AUTOCLASS and HIERACHICAL CLUSTERING.
Keywords–document
categorization/clustering;
hyper-
graph;association rules; hypergraph components decomposition
(HCD); hierarchical clustering (HCA); partition-based hypergraph
algorithm.
I.
INTRODUCTION
The exponential growth in the volume and the popularity
of Web information, such as news, social media, scientiﬁc
articles and discussion forums, induce a Big Data problem.
Since massive amounts of contents have generated everyday,
automatically discovering and organizing contextually relevant
text information are very challenging.
How to get web sophisticated information mining strategies
will be needed. Document clustering can deal with the diverse
and large amount of Web information and particularly is used
to discover latent concepts in a collection of Web documents,
which is inherently useful in organizing, summarizing, disam-
biguating, and searching through large document collections
[1].
Text document clustering is an unsupervised learning tech-
nique that has created a demand for a mechanism to discover
topics from heterogeneous information. Document clustering
aims to generate topic groups or clusters from a document
collections. According to a single document, the content
can mingle heterogeneous topics, the obtained topics from
document clustering methods sometimes do not necessarily
correspond to actual topics of interest and document clus-
tering methods do not provide descriptions that summarize
the clusters’ contents [2]. Many clustering methods, such as
k-means, hierarchical clustering (algorithms and non-negative
matrix factorization (NMF) have been performed on the matrix
to group the documents. However, these methods lack ability
of interpretation to each document cluster [3].
In what follows, we start by brieﬂy reviewing the related
work in Section II and deﬁning the frequent itemsets in a
collection of documents in Section III, and generating a graph
model of representing the concepts from the frequent itemsets
in Section IV, then presenting the topic based clustering
algorithm for partitioning documents into several semantic
topics in Section V. In Section VI, you can see each of which
represents a concept in the document collection, and docu-
ments can then be clustered based on the primitive concepts
identiﬁed by this algorithm. The three different experimental
data sets are also described in Section VII, and ﬁnally get
into the conclusion in the last section, which showed a novel
approach to document clustering which is compared with k-
means, HCA, AutoClass or the Principal Direction Divisive
Partitioning (PDDP). However, how to provide much more
guarantee on precision, even for detailed queries is still an
open research problem.
II.
RELATED WORK
The frequent itemsets (undirected association rules) can
demonstrate semantic topics and can be extracted from doc-
uments. A single item, i.e., word, does not carry much in-
formation about a document, yet a huge amount of items
may nearly identify the document uniquely. Therefore, ﬁnding
all meaningful frequent itemsets in a collection of textual
documents presents a great interest and challenge.
Feldman and his colleagues [4], [5], [6] proposed the KDT
and FACT system to discover association rules based on key-
words labelling the documents, the background knowledge of
keywords and relationships between them, but it is ineffective.
Therefore, an automated approach that documents are labelled
by the rules learned from labelled documents [7]. However,
several association rules are constructed by a compound word
(such as “Wall” and “Street” often co-occur) [8]. Feldman et al.
[4], [9] further proposed term extraction modules to generate
association rules by selected key words. It is beneﬁcial for us to
obtain meaningful results without the need to label documents
by human experts. Association rule hypergraph partition was
proposed in [10] to transform documents into a transactional
database form, and then apply hypergraph partitioning [11]
to ﬁnd the item clusters. Holt and Chung [12] addressed
Multipass-Apriori and Multipass-DHP algorithms to efﬁciently
ﬁnd association rules in text by modiﬁed the Apriori algorithm
[13] and the DHP algorithm [14] respectively. Those methods
did not consider to identify the importance of a word in
a document. Hence, they addressed two clustering methods,
103
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-420-6
SEMAPRO 2015 : The Ninth International Conference on Advances in Semantic Processing

CFWS and CFWM, to perform document clustering [15] by
considering the sequential aspect of word occurrences.
III.
FREQUENT ITEMSETS
Association rules was ﬁrst introduced by Agrawal et
al. [16] wherein two standard measures, called support and
conﬁdence, are often used. This paper only focuses on the
support; a set of items that meets the support will be called
the (undirected) association rules. The association rules are
thereby for the use of ﬁnding co-occurring frequent terms in
documents.
A. Feature Extraction
Feature extraction is to extract key terms from a collection
of documents; And various methods such as association rules
algorithms may be applied to determine relations between
features.
This paper considers only noun entities, especially some
representative entities. All NP chunkers extracted by part-of-
speech (POS) tagger are weighted with respect to the docu-
ments after NP chunkers have been recognised and extracted.
The simple and sophisticated weighted schema which is most
common used in IR or IE is TFIDF indexing, i.e., tf × idf
indexing [17], where tf denotes term frequency that appears in
the document and idf denotes inverse document frequency [18]
where document frequency is the number of documents which
contain the NP chunkers. It takes effect on the commonly used
NP chunker a relatively small tf ×idf value. Moffat and Zobel
[19] pointed out that tf × idf function demonstrates: (1) rare
NP chunkers are no less important than frequent NP chunkers
in according to their idf values; (2) multiple appearances of an
NP chunker in a document are no less important than single
appearances in according to their tf values. The tf×idf implies
the signiﬁcance of a term in a document, which can be deﬁned
as follows.
Deﬁnition 1: Let Tr be a collection of documents. The
signiﬁcance of a term, i.e., NP chunker ti in a document dj in
Tr is its TFIDF value calculated by the function tﬁdf(ti, dj),
which is equivalent to the value tf(ti, dj) × idf(ti, dj). It can
be calculated as
tﬁdf(ti, dj) = tf(ti, dj) log
|Tr|
|Tr(ti)|
(1)
where |Tr(ti)| denotes the number of documents in Tr in
which ti occurs at least once, and
tf(ti, dj) =
( 1 + log(N(ti, dj))
if N(ti, dj) > 0
0
otherwise
(2)
where N(ti, dj) denotes the frequency of terms ti occurs in
document dj by counting all its nonstop words.
For the purpose of document clustering, we only need to
consider when a set of terms that co-occur would become
a concept. The metric support is used for deﬁning the co-
occurred term association. All the documents that are com-
posed of those terms are able to organise a semantic cluster. Let
tA and tB be two terms. The support deﬁned for a collection
of documents is as follows.
Deﬁnition 2: Support denotes to the speciﬁc signiﬁcance
of the documents in Tr that contains both term tA and term
tB, that is,
Support(tA, tB) = tﬁdf(tA, tB, Tr)
|Tr|
(3)
where
tﬁdf(tA, tB, Tr) =
1
|Tr|
|Tr|
X
i=0
tﬁdf(tA, tB, di)
(4)
tﬁdf(tA, tB, di) = tf(tA, tB, di) log
|Tr|
|Tr(tA, tB)|
(5)
and |Tr(tA, tB)| deﬁne number of documents contained both
term tA and term tB.
The term frequency tf(tA, tB, di) of both chunkers tA and
tB can be calculated as follows.
Deﬁnition 3:
tf(tA, tB, dj) =









1 + log(min{N(tA, dj), N(tB, dj)})
if N(tA, dj) > 0 and N(tB, dj) > 0
0
otherwise.
(6)
A minimal support θ is given to ﬁlter the chunkers that
their TFIDF values are less than θ. It helps us to eliminate
the most common chunkers in a collection and the nonspeciﬁc
chunkers in a document.
Suppose that with regard to a query term “network”, the
underlying graph is generated as shown in Figure 1. Each edge
denotes the association between two terms is great than a given
threshold and illustrates a semantic concept.
Figure 1. A graph structure generated by the query term “network.”
IV.
GRAPH MODEL OF FREQUENT ITEMSETS
The set of all frequent itemsets of documents can form
a hypergraph of NP chunkers, and this hypergraph can rep-
resent the totality of thoughts expressed in this collection
of documents. A “simple component” of frequent itemsets
organizes hypergraph that represents semantic concepts inside
this collection of documents.
104
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-420-6
SEMAPRO 2015 : The Ninth International Conference on Advances in Semantic Processing

A. Preliminary
Let us brieﬂy introduce hypergraphs and deﬁne some
preliminaries for further descriptions.
Deﬁnition 4: A weighted hypergraph G = (V, E, W)
contains three distinct sets where (1) V is a ﬁnite set of
vertices, called ground set, (2) E = {e1, e2, · · · , em} is a non-
empty family of ﬁnite subsets of V , in which each subset is
called a n-hyperedge (where n + 1 is the cardinality of the
subset), and W = {w1, w2, · · · , wm} is a weight set. Each
hyperedge ei is assigned a weight wi.
Two vertices u and v are said to be r-connected in a
hypergraph if either u = v or there exists a path from u to v (a
sequence of r-hyperedge, (uj, u(j+1)), u0 = u, . . . , un = v).
A r-connected hyperedge is called a r-connected compo-
nent or r-topic.
B. Concept
For a collection of documents, we generate a hypergraph
of frequent itemsets. Note that because of Apriori conditions,
this hypergraph is closed. The goal of this paper is to establish
the following belief.
Claim
A connected component of a hypergraph repre-
sents a primitive concept in this collection of
documents.
Hypergraphs are a perfect method to represent association
rules. As seen in Figure 1, the vertex set V ={“network”,
“artiﬁcial”, “biological”, “car”, · · · } that represents the set
of key chunkers in a collection of documents, the edge set
E that represents term association rules in the graph. In the
graph, each circle represents a higer order association rules,
which is a hyperedge. Each circle is also a complete subgraph
that its support is bigger than a minimum support, so are all
the non-empty subsets of it. In a hypergraph, the universe of
vertices organizes 1-item frequent itemsets, the universe of 1-
hyperedge represents all possible 1-item and 2-item frequent
itemsets, and so on.
V.
TOPIC-BASED GRAPH MODEL
This section will introduce the algorithm to ﬁnd all frequent
itemsets in documents that is generated from the co-occurring
chunkers in a collection of documents.
A. Weighted Incident Matrix
The weighted incident matrix is
Deﬁnition 5:
a′
ij =

wij
if vi ∈ ej
0
otherwise.
(8)
where the weight wij denotes the support of an frequent
itemset.
Each vertex in V represent a chunker that have been re-
served (i.e., its support is greater that a given minimal support
θ), and each hyperedge in E is undirected that identiﬁes a
support incident with an itemset. Each edge-connector denotes
a topic, i.e., an undirected association rules. The number of
chunker in an edge-connector deﬁnes the rank of a hyperedge.
An edge-connector of a hyperedge with rank r is said to be a r-
hyperedge or r-connected component. As seen in Figure 1, for
instance, the set { “network”, “artiﬁcial”, “neural”, “computer”
} is an edge-connector of a 4-hyperedge that could represents
an “artiﬁcial neural network” topic.
B. Algorithm
A r-hyperedge denotes a r-topic, which is a r-frequent
itemset. If we say a frequent itemset Ii identiﬁed by a
hyperedge ei is a subset of a frequent itemset Ij identiﬁed
by ej, it means that ei ⊂ ej. A hyperedge ei is said to
be a maximal topic if no other hyperedge ej ∈ E is the
superset of ei for i ̸= j. Documents can be automatically
clustered based all maximal topics. Considering an example in
Figure 1, there are four maximal topics that both of them are
4-hyperedges in a hypergraph. One component is organized by
the hyperedge { “network”, “artiﬁcial”, “neural”, “computer”
}, and { “network”, “biological”, “neural”, “cell” } is another
generated hyperedge. The boundary of a concept deﬁnes all
possible term associations in a document collection. Both of
them share a common concept that can be taken as a 1-
hyperedge { “network”, “neural” }, which is an 2-item frequent
itemset. Since all connected components are convex hulls, the
intersection of connected components is nothing or a connected
component.
Property 1: The intersection of concepts is nothing or a
concept that is a maximal closed hyperedge belonging to all
intersected concepts.
Since there is at most one maximal closed hyperedge
in the intersection of more than one connected topics and
the dimension or rank of the intersection is lower than all
intersected hyperedges. It is convenient for us to design an
efﬁcient algorithm for documents clustering based on all
maximal connected components in a hypergraph not needed to
traverse all hyperedges. The algorithm for ﬁnding all maximal
connected components is listed as follows.
Require: V = {t1, t2, · · · , tn} be the vertex set of all reserved
NP chunkers in a collection of documents.
Ensure: E is the set of all maximal connected components.
Let θ be a given minimal support.
E ⇐ ∅
Let E0 = {ei|ei = {ti}∀ti ∈ V } be the 0-hyperedge set.
i ⇐ 0
while Ei ̸= ∅ do
while for all vertex tj ∈ V do
E(i+1) ⇐ ∅ be the i + 1-hyperedge set.
while for all element e ∈ Ei do
if e′ = e S{tj} with tj /∈ e whose support is no
less than θ then
add e′ in E(i+1)
remove e from Ei
end if
end while
end while
E ⇐ E S Ei
i ⇐ i + 1
end while
All the hyperedges in E are maximal connected compo-
nents. A hyperedge will be constructed by including all those
co-occurring terms whose support is bigger than or equal to
a given minimal support θ. An external vertex will be added
into a hyperedge if the produced support is no less than θ. It
is not necessary that the intersection of any two hyperedges
in E is empty because the intersection can be taken as the
common concept that both own as we have already stated.
According to the Property 1, when a maximal connected
105
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-420-6
SEMAPRO 2015 : The Ninth International Conference on Advances in Semantic Processing

component is found, all its subcomponents are also included
in the hyperedge.
The documents can be decomposed into several categories
based on its correspond concept that is represented by a
hyperedge in E. If a document consists in a concept, it means
that document highly equates to such concept, thereby all the
terms in a concept is also contained in this document. The
document can be classiﬁed into the category identiﬁed with
such concept. A document often consists of more than one
concept and it can be classiﬁed into multi-categories.
VI.
EXPERIMENTAL RESULTS
Experimental results are conducted to evaluate the cluster-
ing algorithm, rather than analytic statements.
A. Data Sets
Three data sets are involved in making the validation
and evaluating the performance of our model and algorithm.
Effectiveness is the important criterion for the validity of
clustering.
The ﬁrst dataset is Web pages collected from Boley et
al.[10]. 98 Web pages in four broad categories: business and
ﬁnance, electronic communication and networking, labor and
manufacturing are selected for the experiments. Each category
is also divided into four subcategories.
The second dataset is the “Reuters-21578, Distribution
1” collection consisted of newswire articles, which is a
multi-class, multi-labelled benchmark containing over 21000
newswires articles that are assigned 135 so-called topics. These
topics refer to ﬁnancial news related to different industries,
countries and other categories. In our test 9494 documents
are selected in which all multi-categorized documents were
discarded and the categories with less than ﬁve documents
have been removed.
The third dataset is 305 electronic medical literatures
collected from the journals, Transfusion, Transfusion Medicine,
Transfusion Science, Journal of Pediatrics and Archives of
Diseases in Childhood Fetal and Neonatal Edition. Those
articles are selected by searching from keywords, transfusion,
newborn, fetal and pediatrics. The MeSH categories have the
use of evaluating the effectiveness of our algorithm. It is best
for us to make external validities on the concepts generated
from our method by human experts.
B. Evaluation Criteria
The experimental evaluation of document clustering ap-
proaches usually measures their effectiveness rather than their
efﬁciency [20], in the other words, the ability of an approach
to make a right categorization.
TABLE I. THE CONTINGENCY TABLE FOR CATEGORY ci.
Category
Clustering Results
ci
YES
NO
Expert
YES
TPi
FNi
Judgment
NO
FPi
TNi
Considering the contingency table for a category (Table 1),
recall, precision, and Fβ are three measures of the effective-
ness of a clustering method. Precision and recall with respect
to a category is deﬁned as follows:
Precisioni =
TPi
TPi + FPi
(9)
Recalli =
TPi
TPi + FNi
(10)
The Fβ measure combined with precision and recall has
introduced by van Rijsbergen in 1979 as the following formula:
Fβ = (β2 + 1) × Precisioni × Recalli
β2 × Precisioni + Recalli
(11)
F1 measure is used in this paper, which is obtained when β is
set to be 1 that means precision and recall are equally weighted
for evaluating the performance of clustering. Because of many
categories that will be generated and the comparison reasons,
the overall precision and recall are calculated as the average
of all precisions and recalls belonging to some categories,
respectively. F1 is calculated as the mean of individual results.
It is a macro-average among categories.
In a non-overlapping scenario, each document belongs
to exactly one cluster. Three validation metrics: precision,
recall and F-measure, are proper to evaluate the performance
of crisp clustering algorithms. The overlapping clustering
schemes has been involved in a widely variety of application
domains because many real problems are naturally overlapped.
Information theoretic measures [21], [22], such as entropy and
mutual information, hence have been used to estimate how
much information is shared from the labelled instances in a
cluster, especially, for a hierarchical clustering schemes [23].
In order to compare effectiveness with other methods, two
different evaluation metrics, normalized mutual information
[24], [21], [25] and overall F-measure [26], [27], were also
used.
C. Results
The result of the ﬁrst experiment is presented in Table II.
The result of PDDP algorithm [10], is under consideration by
all non-stop words, that is, the F1 database in their paper,
with 16 clusters. The result of our algorithm, HCD, is under
consideration by all non-stop words with the minimal support,
0.15 by comparing with four algorithms, HCD, PDDP, k-
means and AutoClass. The PDDP algorithm splits the data into
TABLE II. THE PERFORMANCE COMPARISON ON THE FIRST
DATASET.
Method
HCD
PDDP
k means
AutoClass
HCA
Precision
68.3%
65.6%
56.7%
34.2%
35%
Recall
74.2%
68.4%
34.9%
23.6%
22.5%
F1 measure
0.727
0.67
0.432
0.279
0.274
two subsets hierarchically. Based on the principal direction,
i.e. principal component analysis, it also derives a linear
discriminant function. Principal component analysis often hurts
the results of classiﬁcation if with sparse and high dimensional
datasets, and induces a high false positive rate and false
negative rate. Based on the average of the conﬁdences of
the frequent itemsets with the same items, PDDP generates
the hyperedges. It is unfair that a possible concept would be
withdrawn if a very small conﬁdence of an itemset is existed
from an implication direction.
106
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-420-6
SEMAPRO 2015 : The Ninth International Conference on Advances in Semantic Processing

In the ﬁrst dataset, HCD generates 47 clusters, i.e., maxi-
mal connected components, as shown in Figure 2. It is larger
than the original 16 clusters. After performing on decreasing
the minimal support value to be 0.1, the number of clusters
reduces to be 23 and its precision, recall, and F1, become
63.7%, 77.3%, 0.698 respectively. The higher the minimal
support value is, the lower the number of co-occurred terms
in a hypergraph. Precision is worse than PDDP with lower
minimal support because the clustering constraints generated
from hyperedges are stronger to ﬁlter some documents that
should be included, which makes a high false positive rate.
Figure 3 demonstrates the performance on the ﬁrst dataset of
HCD.
Figure 2. The hypergraph generated from the ﬁrst dataset by using HCD.
Figure 3. The effectiveness of HCD on the ﬁrst dataset.
The evaluation was conducted for the cluster numbers
ranging from 2 to 10 on the Reuters data set. For each given
cluster number k, the performance scores were obtained by
averaging those k randomly chosen clusters from the Reuters
corpus in an one-run test. Some terms indicated a generic
category in Reuters classiﬁcations are not designated the same
category, so that the number of clusters is larger than the
number of Reuters’ categories. Table 3 indicates the evaluation
results using the Reuters dataset in Figure 4.
TABLE III. THE PERFORMANCE OF REUTERS DATASET BY HCD.
HCD
k=2
k=3
k=4
k=5
Precision
93%
90.8%
93.8%
86.1%
Recall
68%
63.5%
77.9%
76.2%
F1 measure
0.834
0.774
0.814
0.77
The MeSH categories (22 categories) have been taken to
evaluate the effectiveness of HCD on each individual category
of the third dataset. Document clustering is based on the
MeSH terms related to “Transfusion” and “Pediatrics”. The
effectiveness of all categories is shown in Figure 5. The MeSH
categories are a hierarchical structure that some categories
are the subcategories of the other categories. Many concept
categories are shared with the same terminology that induces
a high false negative rate by HCD on document clustering.
In this dataset, documents are not uniform distributed in all
categories, some categories only contain a few documents
that makes their latent concepts restricted by a few terms,
for example, the Anemia and the Surgery categories whose
precision are both below 70%.
VII.
CONCLUSION
Concept identiﬁcation from text documents is an open
research problem. While polysemy, phrases and chunker de-
pendency present additional challenges for search technology,
single chunker are often insufﬁcient to identify speciﬁc con-
cepts in a document. Discriminating NP chunker associations
naturally helps distinguish one topic from the others. A group
of solid chunker associations can clearly identify a concept.
While most methods, like k-means, HCA, AutoClass or PDDP
classify/cluster documents from the matrix representation,
matrix operations cannot discover all chunker associations.
Hypergraphs allow a efﬁcient way to ﬁnd chunker associations
in a collection of documents.
This paper presents a novel approach to document clus-
tering based on hypergraph decomposition. An agglomerative
method without the use of distance function is proposed.
A hypergraph is constructed from the set of co-occurring
frequent chunkers in the text documents. The r-hyperedges,
i.e., r-topics, can represent basic concepts in the document
collection. We presented a simple algorithm that can effectively
discover the maximal connected components of co-occurring
frequent chunkers. cluster documents. The proposed method
is compared with traditional clustering methods, such as k-
means, AutoClass and HCA, as well as the partition-based
hypergraph algorithm, PDDP, on three data sets in our experi-
ments. The hypergraph component decomposition algorithm
demonstrated superior performance in document clustering.
The results illustrate that hypergraphs are a perfect model to
denote association rules in text and is very useful for automatic
document clustering.
Our experiments also showed that the value of r is
dependent on the given minimal support. The r-connected
components represent the r-frequent itemsets with r different
107
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-420-6
SEMAPRO 2015 : The Ninth International Conference on Advances in Semantic Processing

Figure 4. The hypergraph generated from the second dataset with minimal
support, 0.1.
Figure 5. The effectiveness of HCD on the second dataset.
chunkers. The higher the minimal support value is, the lower
the value of r is. That is, the number of co-occurring chunkers
for organizing concepts in a collection of documents decreases
with a higher minimal support value. In other words, the
support for a more general concept is higher than the support
of a more speciﬁc concept. That is, a general concept is less
effective in classifying/clustering documents.
The strengths of our methods are: 1) an agglomerative
Web document hierarchical clustering is addressed by using
graph construction; 2) a hypergraph properly represents the
concept organized by the associations of terms in a collection
of documents; 3) considering the overlap of semantics between
documents, our method can provide more comprehensible
clustering results allowing concept overlap. However, as seen
in Figure 1, the hyperedge neural, network in the hypergraph
is an ambiguous concept. Not until the upper-leveled hyper-
edges, biological, cell, neural, network and computer, artiﬁcial,
neural, network have been generated we could clearly identify
these two distinct concepts. The weakness of our method is
lack of considering uncertainties within documents. We will
further consider to develop a fuzzy model on uncertainties.
REFERENCES
[1]
R. Kosala and H. Blockeel, “Web mining research: A survey,” SIGKDD
Explorations, vol. 2, no. 1, pp. 1–15, 2000.
[2]
H. Anaya-S´anchez, A. Pons-Porrata, and R. Berlanga-Llavori, “A
document clustering algorithm for discovering and describing topics,”
Pattern Recognition Letters, vol. 31, pp. 502–510, 2010.
[3]
D. Wang, S. Zhu, T. Li, Y. Chi, and Y. Gong, “Integrating document
clustering and multidocument summarization,” ACM Transactions on
Knowledge Discovery from Data, vol. 5, no. 3, pp. 14–26, 2011.
[4]
R. Feldman, Y. Aumann, A. Amir, W. Kl´osgen, and A. Zilberstien,
“Text mining at the term level,” in Proceedings of 3rd International
Conference on Knowledge Discovery, KDD-97, Newport Beach, CA,
1998, pp. 167–172.
[5]
R. Feldman, I. Dagan, and W. Kl´osgen, “Efﬁcient algorithms for mining
and manipulating associations in texts,” in Cybernetics and Systems, The
13th European Meeting on Cybernetics and Research, vol. II, Vienna,
Austria, April 1996.
[6]
R. Feldman and H. Hirsh, “Mining associations in text in the presence
of background knowledge,” in Proceedings of the Second International
Conference on Knowledge Discovery and Data Mining (KDD-96), 1996,
pp. 343–346.
[7]
B. Lent, R. Agrawal, and R. Srikant, “Discovering trends in text
databases,” in Proceedings of 3rd International Conference on Knowl-
edge Discovery, KDD-97, Newport Beach, CA, 1997, pp. 227–230.
[8]
M. Rajman and R. Besanon, “Text mining: Natural language techniques
and text mining applications,” in Proceedings of seventh IFIP 2.6 Work-
ing Conference on Database Semantics (DS-7), Leysin, Switzerland,
1997.
[9]
R. Feldman, M. Fresko, H. Hirsh, Y. Aumann, O. Liphstat, Y. Schler,
and M. Rajman, “Knowledge management: A text mining approach,”
in Proceedings of 2nd International Conference on Practical Aspects
of Knowledge Management, Basel, Switzerland, 1998, pp. 29–30.
[10]
D. Boley, M. Gini, R. Gross, E.-H. Han, K. Hastings, G. Karypis,
V. Kumar, B. Mobasher, and J. Moore, “Document categorization and
query generation on the world wide web using webace,” Artiﬁcial
Intelligence Review, vol. 13, no. 5-6, pp. 365–391, 1999.
[11]
G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar, “Multilevel hy-
pergraph partition application in vlsi domain,” Proceedings ACM/IEEE
Design Automation Conference, vol. 8, pp. 381–389, 1997.
[12]
J. D. Holt and S. M. Chung, “Efﬁcient mining of association rules in
text databases,” in Proceedings of CIKM, Kansas City, MO, 1999.
[13]
R. Agrawal and R. Srikant, “Fast algorithms for mining association
rules,” in Proceedings of the 20th International Conference of Very
Large Data Bases (VLDB), 1994, pp. 487–499.
[14]
J. S. Park, M. S. Chen, and P. S. Yu, “Using a hash-based method with
transaction trimming for mining association rules,” IEEE Transaction
on Knowledge and Data Engineering, vol. 9, no. 5, pp. 813–825, 1997.
[15]
Y. Li, S. M. Chung, and J. D. Holt, “Text document clustering based on
frequent word meaning sequences,” Data and Knowledge Engineering,
vol. 64, pp. 381–404, 2008.
[16]
R. Agrawal, T. Imielinski, and A. Swami, “Mining association rules
between sets of items in large databases,” in Proceedings of the ACM-
SIGMOD 1993 International Conference on Management of Data, May
1993, pp. 207–216.
[17]
G. Salton and C. Buckley, “Term weighting approaches in automatic
text retrieval,” Information Processing and Management, vol. 24, no. 5,
pp. 513–523, 1960.
[18]
K. Sparck Jones, “A statistical interpretation of term speciﬁcity and its
application in retrieval,” Journal of Documentation, vol. 28, no. 1, pp.
11–21, 1972.
[19]
A. Moffat and J. Zobel, “Compression and fast indexing for multi-
gigabit text databases,” Australian Computing Journal, vol. 26, no. 1,
p. 19, 1994.
[20]
F. Sebastiani, “Machine learning in automated text categorization,” ACM
Computing Surveys, pp. 1–47, 2002.
[21]
M. Meil˘a, “Comparing clusterings-an information based distance,”
Journal of Multivariate Analysis, vol. 98, pp. 873–895, 2007.
[22]
M. Sokolova and G. Lapalme, “A systematic analysis of performance
108
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-420-6
SEMAPRO 2015 : The Ninth International Conference on Advances in Semantic Processing

measures for classiﬁcation tasks,” Information Processing and Manage-
ment, vol. 45, pp. 427–437, 2009.
[23]
M. Aghagolzadeh, H. Soltanian-Zadeh, and B. N. Araabi, “Information
theoretic hierarchical clustering,” Entropy, vol. 13, pp. 450–465, 2011.
[24]
T. Cao, H. Do, D. Hong, and T. Quan, “Fuzzy named entity-based docu-
ment clustering,” in Proc. of the 17th IEEE International Conference on
Fuzzy Systems (FUZZ-IEEE 2008), Hong Kong, 2008, pp. 2028–2034.
[25]
W. Xu and Y. Gong, “Document clustering by concept factorization,”
in Proceedings of the 27th annual international ACM SIGIR conference
on Research and development in information retrieval, Shefﬁeld, United
Kingdom, 2004, pp. 202–209.
[26]
A. Dalli, “Adaptation of the f-measure to cluster based lexicon quality
evaluation,” in Proceedings of the EACL 2003 Workshop on Evaluation
Initiatives in Natural Language Processing: are evaluation methods,
metrics and resources reusable?, Budapest, Hungary, 2003, pp. 51–56.
[27]
K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram,
“A hierarchical monothetic document clustering algorithm for sum-
marization and browsing search results,” in Proceedings of the 13th
international conference on World Wide Web, New York, NY, 2004, pp.
658–665.
109
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-420-6
SEMAPRO 2015 : The Ninth International Conference on Advances in Semantic Processing

