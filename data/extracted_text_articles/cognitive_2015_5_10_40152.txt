Neural Associative Memories as Accelerators for Binary Vector Search
Chendi Yu,
Vincent Gripon
and Xiaoran Jiang
Email: name.surname@telecom-bretagne.eu
Telecom Bretagne, Electronics department
UMR CNRS Lab-STICC
Brest, France
Hervé Jégou
Email: name.surname@inria.fr
INRIA
IRISA, team Texmex
Rennes, France
Abstract—Associative memories aim at matching an input noisy
vector with a stored one. The matched vector satisﬁes a minimum
distance criterion with respect to the inner metric of the device.
This problem of ﬁnding nearest neighbors in terms of Euclidean
or Hamming distances is a very common operation in machine
learning and pattern recognition. However, the inner metrics
of associative memories are often misﬁtted to handle practical
scenarios. In this paper, we adapt Willshaw networks in order to
use them for accelerating nearest neighbor search with limited
impact on accuracy. We provide a theoretical analysis of our
method for binary sparse vectors. We also test our method
using the MNIST handwritten digits database. Both our analysis
for synthetic data and experiments with real-data evidence a
signiﬁcant gain in complexity with negligible loss in performance
compared to exhaustive search.
Keywords–Associative Memories; Binary Sparse Vector; Nearest
Neighbors Search; Willshaw Networks.
I.
INTRODUCTION
A
Ssociative memories are devices that store associations
between multiple patterns. They are considered a good
model for human memory for their ability to recall stored
messages given part of them. For example consider the query
of retrieving the word “neuron” from the partially erased query
“n*uro*”.
The literature on this subject is vast and many models
have been proposed, amongst which Hopﬁeld Neural Networks
(HNNs) [1] play a prominent role. HNNs store the empirical
covariance matrix associated with a set of d-dimensional
binary ({-1,1}) vectors. This simple design is appealing.
However, HNNs have a strong limitation on the number of
vectors they can store. This quantity, referred to as diversity,
is provably upper-bounded by d/(2log(d)) [2]. Other neural-
based methods [3] [4] focus on storing the co-occurrence
matrix instead, under the assumption that stored vectors are
c-sparse binary ({0,1}). With proper parameters, the diversity
of such networks is in the order of the square of d [5].
In machine learning and pattern recognition, many appli-
cations aim at matching an input vector to a collection of
other ones. In metric spaces, this operation is termed “nearest
neighbor search”. For high-dimensional vectors, nearest neigh-
bor search complexity is linear in both the number of vectors
in the collection and their dimension, as the naive strategy
of computing all the distances is the best [6]. To overcome
this issue and scale to larger collections, one has to resort to
approximate neighbor search techniques, which trade accuracy
against scalability [7] [8] [9].
This paper shows that binary neural networks can acceler-
ate nearest neighbor search with limited impact on accuracy,
in the case of sparse binary vectors. We support our claim
by providing both (i) a theoretical analysis with simulation on
synthetic data and (ii) experiments on real data carried out on
the gold-standard MNIST handwritten digits database.
The rest of the paper is organized as follows. Section II
introduces our method. In section III, we derive a theoretical
analysis of the performance, while Section IV presents our
real-data experiments. Section V concludes this paper.
II.
METHODOLOGY
Consider a set X of n binary sparse nonzero {0,1}-vectors
with dimension d. Given some input vector x0, we want to
retrieve the closest vector to the query with respect to the
Hamming distance:
x ∈ arg min
x′∈X dH(x0,x′) ,
where dH(x0,x′) denotes the Hamming distance (number of
distinct values) between x0 and x′. This problem is referred to
as nearest neighbor search.
Since computing distances between two d-dimensional
vectors is in O(d), the complexity of this problem is in O(dn)
with the naive approach, which turns out to be the best choice
for exact search: this complexity is tight in high-dimensional
spaces. Yet, approximate solutions exist to reduce it [7] [8] [9].
In this case, beyond CPU and memory complexity, the accu-
racy of the algorithm is a key characteristic when comparing
different approaches.
Since in our case, we focus on ﬁnding similar neighbors for
the Hamming distance and not the Euclidean one, according to
Andoni, “the best algorithm for the Hamming space remains
the one described in [10]”, that is a binary variant of Locality
Sensitive Hashing, which requires a lot of memory to be
effective in high-dimensional spaces.
The approach we propose below relies on a rather different
mechanism (and less memory-demanding) than the hashing-
based approach of LSH: neuro-inspired associative memories.
It consists of two steps: selection and check.
85
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

a) Selection: We partition X into subsets X1,...,Xq,
as
•
X = Sq
j=1 X j,
•
∀ j, j′, j ̸= j′,Xj ∩Xj′ = /0.
In the following, we conveniently consider that the split is
regular: |X1| = ··· = |Xq| = k, such that n = kq.
We then represent each subset as an associative memory,
and search for the input query x0 in each of them. Formally,
we use Willshaw networks. The Willshaw network associated
with Xj is deﬁned as:
W(X j) = max
x∈Xj
xx⊤ .
In order to have an estimation of whether some input vector
x0 resembles the content of X j or not, we use the following
formula:
s(x0,Xj) ≜ x⊤
0 W(X j)x0
(x⊤
0 x0)2
,
that we shall refer to as the score of x0 in Xj.
By construction of W(Xj), the score of x0 in Xj is
maximal if x0 ∈ Xj and equals one. Conversely, it may happen
that the maximal score is achieved even if x0 ̸∈ Xj, typically
when the matrix W(X j) is overﬁlled.
b) Check: Having selected all the associative memories
for which the score x0 is high, we exhaustively compare the
query to all the vectors stored in the corresponding subsets. We
therefore ﬁnd the nearest neighbor in a restricted subset of the
whole dataset. As we will see theoretically and experimentally,
it is likely to contain the desired neighbors to the query.
The complexity of computing the score of x0 in Xj is
in O(d2), and is independent of k. This is the reason that
motivates us for using such devices as accelerators for nearest
neighbor search. More formally, if the number of subsets that
are selected for ﬁne-grain search is N, the overall complexity
for performing nearest neighbor search is:
C = O(qd2 +Nkd) .
(1)
III.
ANALYSIS FOR SYNTHETIC DATA
A. Theoretical analysis
We consider n binary column vectors xi of dimension d
and L0 norm ∥xi∥0 = c, c being a constant. The vectors are
generated independently and randomly according to a uniform
distribution. They are then regularly split by group of k and
stored in q Willshaw networks in the way deﬁned in the
previous section.
Given an input vector x0, which is not necessarily con-
tained in the training set, we use this model to perform an
approximate search of its nearest neighbor. For simplifying
the demonstration, we assume that it has the same norm as
the training vectors, ∥x0∥0 = c. Let us denote by x its nearest
neighbor and W(Xz) the matrix it is stored into. We have
W(Xz) = x⊤x+R(Xz,x)
with R(Xz,x) = W(Xz)−x⊤x.
The score of x0 in Xz is written as:
s(x0,Xz) = x0⊤W(Xz)x0
(x0
⊤x0)2
= x0⊤x⊤xx0 +x0⊤R(Xz,x)x0
c2
=

c− dH(x,x0)
2
2
+x0⊤R(Xz,x)x0
c2
≥

c− dH(x,x0)
2
2
c2
.
If x0 = x, we have dH(x,x0) = 0 and x0⊤R(Xz,x)x0 = 0,
thus we rediscover the maximum score of 1.
Let us now consider any other matrix W(Xj), j =
1,...,q, j ̸= z. The score of x0 in W(X j) is calculated as:
s(x0,Xj) = ∑(l,m)∈M(x0) wlm
c2
with
M(x0) = {(l,m)|x0(l) = 1,x0(m) = 1}.
Obviously,
|M(x0)| = c2.
Let us make the assumption that the random variables
wlm are independent. Although this statement is false as a
stored vector adds multiple ones in the matrix, it is a fair
approximation [4] [5], considering c to be small compared to
d. According to the law of large numbers, this normalized sum
approaches the expectation E(wlm) when c is large enough.
Since wlm is taken within discrete values in {0, 1}, this is
nothing else than the probability that any coefﬁcient wlm is
equal to 1. We denote this probability d(W(Xj)), and term it
the density of the matrix. It can be expressed as:
d(W(Xj)) = Pr(wlm = 1)
= Pr(∃x′ ∈ Xj,x′(l)x′(m) = 1)
= 1−Pr(∀x′ ∈ X j,x′(l)x′(m) = 0)
= 1−

On the other hand, the complexity of an exhaustive nearest
neighbor search is written as:
Cknn = O(nd) .
An interesting value of k should satisfy:
nd ≥ n
k d2 +kd
We obtain:
k ≥ n−
√
n2 −4nd
2
≈ d,if d ≪ n.
(3)
Equations (2) and (3) gives the interval of k such that
the proposed model ﬁnds in expectation the nearest neighbor
without error and with a reduced complexity compared to
exhaustive nearest neighbor search. The existence of such a
value of k implies:
−d2
c2 ln dH(x,x0)
c
≥ d
We conclude:
dH(x,x0) ≤
c
ec2/d .
For c =
√
αd with α > 0, we have dH(x,x0) ≤ c
eα . Thus, for
sparse binary vectors of norm c that grows with the square root
of the vector dimension d, if a given vector is sufﬁciently near
from its nearest neighbor, the Willshaw networks can always
accelerate the nearest neighbor search without compromising
the performance in terms of error rate.
The complexity C should also be compared to that of
the Mount Approximate Nearest Neighbor (the Mount ANN)
search [7]:
CANN = O(d(1+ 6d
ε )d log(n)),
where ε is a constant.
In practical scenarios where the dimension is not smaller
than 10, it is reasonable to consider n ≪ dd. Adding the fact
that d ≤ k ≤ n, we easily show that C = o(CANN). Note that
in this case, the complexity of the Mount ANN is even larger
than that of exhaustive search.
B. Synthetic simulations
We consider a vector set X that contains n = 20000 ran-
domly generated sparse binary vectors of dimension d = 400
and c = 10 of them are 1s. The test vector is generated as a
modiﬁed version of any vector in the set X . To generate a test
vector x0, we randomly pick up a vector x ∈ X and permute
s 1s and 0s, s being smaller than c. Formally, we have
∥x∧x0∥0 = c−s ,
and
∥x∥0 = ∥x0∥0 = c .
If s is small enough, x is likely the nearest neighbor of x0
with Hamming distance dH(x,x0) = 2s. We take s = 4 for the
simulations.
Note that we still have not exploited the fact that vectors
are sparse in our estimation of algorithm complexity. As a
matter of fact, sparsity of vectors helps reducing complexity:
we only need to check the positions where are 1s, the number
of which is c, rather than looking at every coefﬁcients in a
vector of dimension d. Thus, we can update Equation (1) as
follows:
C = O(n
k c2 +αkc) ,
and
Cknn = O(αnc)
where α is some constant between 1 and 2. In fact, calculating
the Hamming distance between two sparse binary vectors of
norm c needs at most 2c operations if there is not any common
position that has coefﬁcient 1, and at least c operations if these
two vectors are perfectly identical. We then deﬁne the relative
complexity RC as the ratio between C and Cknn:
RC =
C
Cknn
= c
αk + k
n ≥ c
2k + k
n
Therefore, we obtain RCmin =
q
2c
n ≈ 3.2% in condition
that k = p nc
2 ≈ 316 if n = 20000 and c = 10. Nevertheless, in
terms of the error rate, k needs to be small enough to avoid
overﬁtting.
Simulated results are illustrated in Figure 1 with parameters
listed as follows: d = 400, c = 10, n = 20000, s = 4, N = 1.
Three measures are assessed in function of k, the number of
vectors that contains in each matrix: the red curve represents
the Hamming distance gap from the vector that obtains the
highest score in our method to the de facto nearest vector
obtained by the exhaustive search while the blue one indicates
the error rate and the black one signiﬁes the relative complexity
RC. For example, when k = 200, we obtain a negligible
error rate of 0.5% and a Hamming distance gap of 0.003 in
consuming only about 3.5% of the complexity of a classical
nearest neighbor search.
50
100
150
200
250
300
350
0
0.05
0.1
0.15
0.2
0.25
k: the number of vectors that we store in each matrix
Distance gap/ Error rate/ Relative complexity
 
 
Hamming distance gap
Error rate
Relative complexity
Figure 1. Performance evaluation for the use of associative memory to
accelerate nearest neighbor research.
One may notice that, the error rate and the Hamming
distance gap grow as k, the number of vectors stored in
each matrix increases, which is especially true when k passes
87
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

Figure 2. Procedure of the simulation.
a certain threshold (150 in this case). This degradation of
performance can be mainly attributed to the saturation of 1s
in matrices, which aggravates the noise interference in the
selection phase. Since the interference issue is closely related
to the matrix density, the performance depends on the length
of vectors and the number of vectors contained in each matrix.
IV.
REAL DATA
To evaluate our method on real data, we have carried out
experiments on the MNIST handwritten digits database [11].
Created by Yann Lecun, Corinna Cortes and Christopher J.C.
Burges, the MNIST database of handwritten digits consists of
a training set of 60,000 examples, and a test set of 10,000
examples. All the digits in the database are real-world data.
For the purpose of being more representative, the simula-
tion follows the steps of typical image recognition as is shown
in Figure 2.
In the simulation, Convolutional Neural Networks (CNNs)
are exploited to extract the proper features followed by a binary
quantization. These binary features will be subsequently used
as inputs for the proposed method. The nearest neighbor as
identiﬁed by the proposed method decides which class a test
input belongs to. The complexity and the corresponding error
rate are then calculated.
For the computation using CNNs, we adapted the code of
Rasmus Berg Palm [12]. A synthetic representation of the net-
work considered here is shown in Figure 4 (this representation
is inspired by [13]).
More precisely, given some 28×28 image input taken from
the MNIST database, we use a 5-layer deep network. The ﬁrst
and the third layers are convolutional layers while the second
and the forth layer are sub-sampling layers. The size of kernel
for the convolution layers is 5 and the scale of average pooling
for sum-sampling layers is 2. Six feature maps are applied for
the ﬁrst two layers and there are 12 for the third and the forth
layer. In the ﬁfth layer, we put all the feature maps already
present at the fourth layer (12×(4×4)) into a vector whose
dimension is 192×1. The parameter training rate is set as 1
and the parameter training times as 10.
For the binary quantization, we choose a pre-deﬁned
threshold t = 0.3. We observe that with this choice the
proportion of 1s reaches approximately 0.42 on average in
the output vectors. This threshold was tuned to obtain the
best performance when using nearest neighbor classiﬁcation
afterwards.
Then we use the proposed method. Let us denote by X D
the set of transformed training vectors corresponding to digit
D, D = 0,...,9. For each digit D, we split X D into 6000/k
subsets X D
X D = S j , each of them containing k vectors, and such that
j X D
j .
0
100
200
300
400
500
600
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.5
k: the number of vectors that we store in each matrix
P0:  the average proportion of ’0’s in matrices
Figure 3. The relation between k and the proportion p0 of 0s in the matrices.
Therefore, after the training phase, we have q = 60000
k
matrices in total. Then, at classiﬁcation time, given a vector x0
belonging to the test set, we calculate the score of this vector in
each set X D
j , s(x0,X D
j ) in the way deﬁned in Section II. The
N matrices with the highest scores will be chosen. If there is a
tie in matrices scores, we perform a random choice of which to
select. The vectors stored in these chosen matrices will be then
exhaustively searched to ﬁnd the nearest neighbor. The label
of the matrix that contains the nearest neighbor is returned as
the result of the classiﬁcation process.
The general expression of the overall complexity given
by Equation (1) should be adapted in this real data scenario.
In fact, the matrix W(X D
j ) contains much more 1s than 0s.
Clearly, as k grows, the number of 0s in the matrix decreases.
However, due to the non-uniformity of the vectors in MNIST
database, the speed at which 0s decreases is not as fast as for
synthetic data (see Section III). Figure 3 depicts the proportion
of 0s p0 in the matrices as function of k.
Therefore, we can exploit this fact to adjust the computa-
tion of complexity. In fact,
s(x0,X D
j ) =
x⊤
0 W(X D
j )x0

Input
28*28
C1:feature maps
6@24*24
S2:f.maps
6@12*12
C3:f.maps
12@8*8
s4:f.maps
12@4*4
Output
192*1
Convolutions
Subsampling
Convolutions
Subsampling
Full connection
Figure 4. Representation of the CNNs layers used to extract features from MNIST raw images.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
11
0.025
0.03
0.035
0.04
0.045
0.05
0.055
Relative Complexity
Error Rate
Figure 5. Error rate in function of the relative complexity. Simulations are
performed for the MNIST handwritten digits database.
V.
CONCLUSION
We proposed a method to perform approximate nearest
neighbor search using neural-based associative memories. This
method advantageously takes beneﬁt from the fact that the
neural network dimensions of the representation associated
with a set of k vectors is independent of k.
As a result, we prove that for synthetic data, one can expect
dramatically lower complexity with no reduction on accuracy.
We also evaluate our method using real data and prove that it
is possible to achieve a reduction of 70% of complexity with
a limited impact on the error rate.
In future work, we consider looking at alternative ways to
use Willshaw networks to really beneﬁt from their associative
capabilities [14], as well as perform experiments on more
challenging datasets.
ACKNOWLEDGMENT
This work was founded in part by the European Research
Council under Grant ERC-AdG2011 290901 NEUCOD.
REFERENCES
[1]
John J Hopﬁeld, “Neural networks and physical systems with emergent
collective computational abilities,” Proceedings of the national academy
of sciences, vol. 79, no. 8, pp. 2554–2558, 1982.
[2]
Robert J McEliece, Edward C Posner, Eugene R Rodemich, and San-
tosh S Venkatesh, “The capacity of the hopﬁeld associative memory,”
Information Theory, IEEE Transactions on, vol. 33, no. 4, pp. 461–482,
1987.
[3]
David J Willshaw, O Peter Buneman, and Hugh Christopher Longuet-
Higgins, “Non-holographic associative memory.,” Nature, vol. 222, pp.
960–962, 1969.
[4]
Vincent Gripon and Claude Berrou, “Sparse neural networks with large
learning diversity,” IEEE Transactions on Neural Networks, vol. 22,
no. 7, pp. 1087–1096, July 2011.
[5]
Judith Heusel, Matthias Löwe, and Franck Vermet, “On the capacity
of a new model of associative memory based on neural cliques,” arxiv
preprint.
[6]
Roger Weber, Hans-J. Schek, and Stephan Blott,
“A quantitative
analysis and performance study for similarity-search methods in high-
dimensional spaces,” in Proceedings of the International Conference of
Very Large Databases, 1998, pp. 194–205.
[7]
Sunil Arya, David M Mount, Nathan S Netanyahu, Ruth Silverman, and
Angela Y Wu, “An optimal algorithm for approximate nearest neighbor
searching ﬁxed dimensions,” Journal of the ACM (JACM), vol. 45, no.
6, pp. 891–923, 1998.
[8]
Sunil Arya, David M Mount, Nathan S Netanyahu, Ruth Silverman, and
Angela Wu, “An optimal algorithm for approximate nearest neighbor
searching,” in Proceedings of the ﬁfth annual ACM-SIAM symposium on
Discrete algorithms. Society for Industrial and Applied Mathematics,
1994, pp. 573–582.
[9]
Piotr Indyk and Rajeev Motwani,
“Approximate nearest neighbors:
towards removing the curse of dimensionality,” in Proceedings of the
thirtieth annual ACM symposium on Theory of computing. ACM, 1998,
pp. 604–613.
[10]
Aristides Gionis, Piotr Indyk, and Rajeev Motwani, “Similarity search
in high dimensions via hashing,” in Proceedings of the International
Conference of Very Large Databases, 1999, pp. 518–529.
[11]
“Mnist database,” 2014, URL: http://yann.lecun.com/exdb/mnist/ [ac-
cessed: 2014-01-02].
[12]
“Deeplearntoolbox,” 2014,
URL: https://github.com/rasmusbergpalm/
DeepLearnToolbox [accessed: 2014-01-02].
[13]
“Net5,” 2014, URL: http://eblearn.sourceforge.net [accessed: 2014-01-
02].
[14]
Ala Aboudib, Vincent Gripon, and Xiaoran Jiang, “A study of retrieval
algorithms of sparse messages in networks of neural cliques,”
in
Proceedings of Cognitive 2014, May 2014, pp. 140–146.
89
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

