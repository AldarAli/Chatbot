Lensless Ultra-Miniature CMOS Computational Imagers and Sensors
David G. Stork and Patrick R. Gill
Computational Sensing and Imaging
Rambus Labs
Abstract—We describe a new class of lensless, ultra-miniature
computational imagers and image sensors employing special
optical phase gratings integrated with CMOS photodetector
matrices. Because such imagers have no lens, they are ultra-
miniature (∼100 µm), have large effective depth of ﬁeld (1 mm
to inﬁnity), and are very inexpensive (a few Euro cents). The
grating acts as a two-dimensional visual “chirp” and preserves
image power throughout the Fourier plane (and hence preserves
image information); the ﬁnal digital image is not captured as in a
traditional camera but instead computed from raw photodetector
signals. The novel representation at the photodetectors demands
that algorithms such as deconvolution, Bayesian estimation, or
matrix inversion with Tikhonov regularization be used to compute
the image, each having different bandwidth, space and compu-
tational complexities for a given image ﬁdelity. Such imaging
architectures can also be tailored to extract application-speciﬁc
information or compute decisions (rather than compute an image)
based on the optical signal. In most cases, both the phase grating
and the signal processing can be optimized for the information
in the visual ﬁeld and the task at hand. Our sensor design
methodology relies on modular parallel and computationally
efﬁcient software tools for simulating optical diffraction, for CAD
design and layout of gratings themselves, and for sensor signal
processing. These sensors are so small they should ﬁnd use in
endoscopy, medical sensing, machine inspection, surveillance and
the Internet of Things, and are so inexpensive that they should
ﬁnd use in distributed network applications and in a number
of single-use scenarios, for instance in military theaters and
hazardous natural and industrial conditions.
Keywords: Computational sensing, phase grating, diffrac-
tive imager, application-speciﬁc sensing
I.
INTRODUCTION
The traditional camera obscura model—in which each
point in the scene is imaged onto a single point on a sensor
or image plane—has dominated the science and technology
of imaging systems for several millennia, at least for sources
illuminated by incoherent light. The Chinese philosopher Mo
Ti traced an inverted image produced by a pinhole camera
to record an image in the ﬁfth century B.C. [1] and Johannes
Kepler traced a real image projected by a converging lens onto
paper in 1603. Chemical recording of projected images, such as
by mercury or silver halide, was invented in 1826 and the ﬁrst
true digital camera was built in 1975, [2] all these exploiting
the fundamental camera obscura architecture.
The rise in digital imaging, where image processing can
be incorporated into the data chain, has enabled new imaging
architectures. Cathey and Dowski took an early and conceptu-
ally important step away from the traditional camera obscura
model by exploiting digital processing [3]. They designed
a cubic-phase optical plate which, when inserted into the
optical path of a traditional camera, led to an image whose
(signiﬁcant) blur was independent of the object depth: the
image on the sensor plane did not “look good” as it would
in a traditional camera obscura. Subsequent image processing
sharpened the entire blurred image, thus leading to enhanced
depth of ﬁeld. Since then the ﬁeld of computational imaging
has explored imaging architectures in which the raw signals
do not superﬁcially resemble a traditional image; instead,
the ﬁnal image is computed from such signals. In this way,
many optical aberrations can be corrected computationally
rather than optically. More and more of the total imaging
“burden” is borne by computation, thereby expanding the class
of usable optical components. This imaging paradigm has led
to new conceptual foundations of joint design of optics and
image processing, [4] as well as a wide range of non-standard
imaging architectures such as plenoptic, coded-aperture and
multi-aperture systems, each with associated methods of signal
processing [5]–[9].
1
2
3
45
7 6
8
9
10
10
11
11
12
12
10-9
10-5
0.1
1000
107
1011
1
1000
106
109
1012
1015
Volume Hmm3L
Resolution HpixelsL
Valley of
darkness
•
•
•
••
•
•
••
•
•
0.01
10
104
107
1010
Sales HunitsyearL
Fig. 1.
The resolution (in pixels) versus the physical volume (in mm3) of
representative lens- and mirror-based telescopes and cameras (log-log scale).
Notice there is a seven-order-of-magnitude range in physical volume devoid
of such cameras (the Valley of darkness). 1 Grand Canaria telescope, 2
Hubble telescope, 3 1-m telescope, 4 30-cm telescope, 5 AWARE 2 camera, 6
Professional camera,7 Consumer DSLR, 8 iPhone 5 camera, 9 Pelican camera,
10 Miniature VGA, 11 Medigus camera, 12 Single photodiode (without lens).
The blue points indicate the sales of representative imagers of different
physical volumes in units/year worldwide in 2013. (The unit sales ﬁgures
are estimates based on historical data and market reports and do not include
research prototypes and unreleased products.) Note that there is a precipitous
drop in sales at the Valley of darkness. Our lensless integrated diffraction
grating/CMOS imagers lie within this “valley.”
The economic pressures for miniaturization of electronic
devices, including cameras, arising in the mobile computing
market have led to smaller imager form factors [10]. Figure 1
shows the resolution, in total pixels per exposure, versus
physical volume of imaging systems in the traditional camera
obscura architecture (or curved mirror equivalent). While such
186
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-296-7
SENSORCOMM 2013 : The Seventh International Conference on Sensor Technologies and Applications

imagers span 22 orders of magnitude in physical volume and
15 orders of magnitude in pixel resolution, the smaller the
imager the greater the number sold commercially... but only
down to a scale of roughly 1 mm3. There is a conspicuous
gap of seven orders of magnitude in physical volume—the
“Valley of darkness”—between the smallest digital camera
and a single unlensed photoreceptor. It seems that the camera
obscura model has reached its physical limits and cannot be
scaled much smaller. A new imaging architecture is required
to span the Valley of darkness.
Recently, a new miniature imaging architecture has been
explored, one based on integrating optics with CMOS photo-
detectors [11]–[14]. In brief, this architecture forgoes lenses
and relies instead on simple square-wave diffraction gratings
created in CMOS itself. The earliest designs in this architecture
relied on CMOS wires to act as amplitude optical grating
patches, the gratings producing a wavelet-like representation
of the scene on the sensor matrix. More recently, square-wave
phase gratings have also been explored [15]. For a given image
resolution, such diffractive elements enable the construction
of imagers much smaller than does the basic camera obscura
model. (We mention in passing that related CMOS structures
have been explored for integrated spectroscopy as well [16].)
There are a number of limitations of such previous work.
First, amplitude gratings based on CMOS wires have poor
low-light sensitivity because most of the incident light never
strikes the photodetector. Second, regular diffraction gratings
are by their very nature wavelength sensitive, i.e., the pattern
of light on the photodetectors depends strongly upon the
wavelength of incident light. Third, such imagers are sensitive
to manufacturing defects—speciﬁcally a small deviation in
the thickness of the grating layer can lead to a large (and
difﬁcult to correct) alteration of the diffraction pattern on the
photodetectors [13].
The method we describe here, while based on integrated
silicate phase optics and CMOS image sensors, is fundamen-
tally different from prior work in a number of deep ways.
Our method relies on novel special phase anti-symmetric spiral
phase gratings, which overcome prior limitations and afford
new functionality [17]. Moreover, our new sensor architecture
enables the construction of new classes of ultra-miniature
sensors whose output is an estimation of some property of the
scene (e.g., visual motion) or a decision (e.g., face detection
or barcode reading).
We begin in Section II with a discussion of our fundamental
technology and turn in Section III to a short description of our
software design and analysis tools. We mention a few appli-
cation areas for such sensors and imagers in Section IV and
conclude in Section V with a brief summary and suggestions
for future research.
II.
SENSOR OPTICS AND TECHNOLOGY
The following description of our sensor technology follows
the data path—from target source through diffractive optics to
photodetector to digital signal processing.
A. Optics of one-dimensional phase anti-symmetric gratings
The fundamental optical elements employed by our sensors
are based on a new type of phase grating having phase anti-
symmetry. Figure 2 shows a cross section through our silicate
binary phase grating, here speciﬁed by three free parameters,
w0, w1 and w2 [18]. (Generalizations to more free parameters
and multiple thicknesses are straightforward.) Consider point
P lying on the grating’s plane of odd symmetry. Light from
each position on one side of the plane is cancelled via
destructive interference by light from the symmetric position
on the other side of the plane because those waves arrive out
of phase. Note that such cancellation occurs regardless of the
vertical depth of P. As such, all points along the red dashed
line are dark; we call this plane an “optical curtain” or simply
“curtain” [19], [20]. The location of the curtain on the sensor
matrix below does not change despite manufacturing errors in
overall grating thickness. Finally, as the angle of incidence of
the light changes, the curtains tip by the same angle (Fig. 3).
P
w2
w1
w0
w2
w2
w0
w1
w2
w1
w1
Fig. 2.
A cross section through a silicate binary phase anti-symmetric phase
grating, where the plane of odd symmetry is marked with a dashed red line.
The parameters w0, w1 and w2 describe the surface proﬁle. For the medium’s
index of refraction n, the step height is chosen to corresponds to optical phase
delay of π radians along the red dashed line or “curtain.” For such a phase
anti-symmetic grating, curtains exist even if the incident light is not normal
(Figs. 3 and 4).
Fig. 3.
A ﬁnite-difference wave simulation of the electric ﬁeld energy
density for monochromatic light incident at 3.5◦ passing through an phase
anti-symmetric grating where x denotes the position left-to-right and z the
depth within the silicate medium. The curtains lie beneath the points of odd
symmetry (purple) and are tipped at the same angle as the incident light. Such
curtains are invariant to the wavelength of incident light. The photodetector
matrix (not shown) lies along the bottom.
B. Phase anti-symmetric spiral gratings
Image sensors are two-dimensional and therefore the phase
anti-symmetric grating just described must be generalized to
two dimensions. Moreover, two-dimensional gratings must
include segments at every orientation so as to sample the
187
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-296-7
SENSORCOMM 2013 : The Seventh International Conference on Sensor Technologies and Applications

Fig. 4.
The response of a single photodetector (pixel) beneath a phase anti-
symmetric grating (such as P in Fig. 2) as a function of angle of incident light,
θ, and wavelength of light, λ. Notice that for normally incident light (θ = 0◦)
the response nearly vanishes at all wavelengths and that at each incident
orientation, the response is nearly invariant with respect to wavelength. The
speciﬁc form of this response function depends upon the proﬁles of the
grating (described by wis), which can be tailored to extract information most
appropriate to particular applications, including non-imaging applications.
Fourier domain uniformly (and possess no zeros) and thereby
enable computational reconstruction of the image from sen-
sor responses. Figure 5 shows two examples of basic spiral
grating tiles—having four-fold and six-fold chiral symmetry.
These spiral grating tiles are constructed by sweeping one-
dimensional phase anti-symmetric gratings perpendicularly
along the length of each spiral arm. The phase anti-symmetric
gratings are lengthened and made more complicated (use more
ws) to cover the full tile area and feasible Fourier domain. Both
spiral gratings pass information at all orientations and spatial
frequencies up to the Nyquist limit, and can be tiled to cover
a full photodetector matrix of arbitrary area (Fig. 6) [17]. In
actual sensors, incident light covers an area at least as large
as that of a full individual tile element.
The wave optics described above assumes the incident
illumination is plane-wave. In such a case the pattern of light
produced by a grating does not depend upon the distance of
the object, so long as the object is farther from the sensor than
roughly 10 times the spatial scale of the sensor itself. As such,
our sensor has extremely large effective depth of ﬁeld, from
roughly 1 mm to inﬁnity.
The pattern of light produced by the diffraction grating
strikes the CMOS photodetector matrix beneath and the signals
are sent off chip for digital processing.
C. Signal processing
Sensed signals in our sensor do not resemble an image in a
camera obscura but must be processed to yield a digital image.
We assume the overall forward imaging model is described by:
y = Mx + n,
(1)
Fig. 5.
The left column shows phase anti-symmetric spiral binary gratings,
the middle column the point-spread function each produces (both ﬁgures of
spatial extent D × D, for some distance D). The right column shows the
corresponding modulation transfer function (modulus of the Fourier transform)
of extent 1/P × 1/P, where P is the pixel pitch and determines the Nyquist
rate. The top row corresponds to four-fold chiral symmetry and the bottom
row corresponds to six-fold chiral symmetry.
Fig. 6.
The individual grating tiles of Fig. 5 can be packed to cover a
photodetector matrix of arbitrary area.
where y is the vector of photodetector pixel responses, x
is a vector of inputs, M the matrix describing the linear
transformation performed by the two-dimensional optical grat-
ing, and n is additive noise, which describes photodetector
noise, Poisson photon statistics, quantization noise, etc. (Other
models, such as simple multiplicative noise, could also be
assumed.)
Then estimation of the input—that is, the recon-
struction of the image—is given by:
ˆx =

parameters. Figure 7 shows the estimation of an image through
simple matrix inversion with Tikhonov regularization.
x
y = Mx + n
ˆx
Fig. 7.
Image sensing and computational reconstruction of Leonardo’s Mona
Lisa from a lensless phase anti-symmetric spiral phase grating sensor. (Left)
The input image. (Middle) The simulated response on the photodetectors due
to the six-fold grating in Fig. 5, and (right) the reconstruction by Eq. 2. This
image estimate is of higher ﬁdelity than the estimate based on traditional
square-wave gratings and photodetector arrays of comparable number of pixels
and overall noise level described in earlier work.
III.
SIMULATION/DESIGN TOOLS AND METHODOLOGY
Our sensor system design and analysis methods are based
on a modular architecture comprising three software tools, all
written in Matlab and executed on a large network of PCs:
•
Optics of phase gratings: We simulate the inter-
action of light with gratings, for instance by ﬁnite-
difference wave algorithms. These simulations predict
the response of physical photodetector pixels to light
incident at arbitrary angles.
•
CAD design of gratings and tiles: We design gratings
(spiral and otherwise) and their tilings. The output of
our design is either Matlab-compatible ﬁles for optics
simulations or GDSII for silicon grating manufacture.
•
Sensor signal processing: We continue to write
our own image reconstruction, signal estimation and
pattern recognition software in Matlab, often using
standard libraries of matrix operations.
We can employ Perl software wrappers for these compo-
nents in order to efﬁciently design and model the system’s
end-to-end performance.
IV.
APPLICATIONS
There are many promising applications for our sensors
which fall into a number of general categories. A small but
important subset of such categories follows.
A. Imaging
The ultra-miniature size of our imagers and sensors make
them especially appropriate for medical and industrial endo-
scopy as well as traditional and novel mobile computing
devices. There are many surveillance applications that would
proﬁt from low- to mid-level resolutions as well. Because these
sensors are so inexpensive (in bulk)—each less expensive than
a single frame of 35-mm photographic ﬁlm—they could ﬁnd
application in a number of one-use imaging scenarios arising in
military theaters, hazardous industrial conditions (crash tests)
and natural environments. Another general area is inexpensive
mobile medical imaging and sensing.
B. Motion estimation
The optical gratings and signal processing algorithms can
be tailored to broad image sensing applications such as oc-
cupancy detection for controlled lighting, motion (motion-
activated devices), visual looming (pre-impact automotive
airbag deployment), interactive toys, and numerous applica-
tions in support of the Internet of Things [23].
C. Pattern recognition
These sensors can extract informative visual information
for pattern recognition applications, such as face detection (au-
thentication), one-dimensional barcode and two-dimensional
QR code reading, gesture recognition and many others. Of
course, the signal processing is then based on principles of
pattern recognition appropriate for the task at hand [24].
V.
CONCLUSION
We have designed and veriﬁed through full end-to-end
system simulation a new class of lensless computational im-
agers based on phase anti-symmetric spiral gratings. These
imagers promise to be smaller (lower physical volume) than
any existing lens-based imagers of comparable resolution, very
inexpensive, and customizable to both imaging and a wide
range of sensing and image measurement tasks.
Hardware implementations of our computational sensors
are underway and practical ﬁelded applications will lead
to many interesting problems in efﬁcient application-speciﬁc
algorithms, either on special-purpose ASICs, on highly parallel
graphics processor units (GPUs), or on general-purpose central
processor units (CPUs). Networks of such sensors highlight
several problems and opportunities in power usage and band-
width optimization.
ACKNOWLEDGMENT
We thank Thomas Vogelsang and Michael Ching for help-
ful comments.
REFERENCES
[1]
T. Gustavson, Camera: A history of photography from Daguerreotype
to digital.
New York, NY: Sterling Publishing Co., 2009.
[2]
D. Wooters and T. Mulligan, A history of photography—from 1839 to
the present.
New York, NY: Taschen, 2005.
[3]
W. T. Cathey and E. R. Dowski, Jr., “A new paradigm for imaging
systems,” Applied Optics, vol. 42, no. 29, pp. 6080–6092, 2002.
[4]
D. G. Stork and M. D. Robinson, “Theoretical foundations of joint
design of electro-optical imaging systems,” Applied Optics, vol. 47,
no. 10, pp. B64–75, 2008.
[5]
E. H. Adelson and J. Y. Wang, “Single lens stereo with a plenoptic cam-
era,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 14, no. 2, pp. 99–106, 1992.
[6]
A. Levin, R. Fergus, F. Durand, and W. T. Freeman, “Image and depth
from a conventional camera with a coded aperture,” ACM Transactions
on Graphics, vol. 26, no. 3, pp. 70:1–70:9, 2007.
[7]
D. L. Marks, D. S. Kittle, H. S. Son, S. H. Youn, S. D. Feller, J. Kim,
D. J. Brady, D. R. Golish, E. M. Vera, M. E. Gehm, R. A. Stack,
E. J. Tremblay, and J. E. Ford, “Gigapixel imaging with the AWARE
multiscale camera,” Optics and Photonics News, vol. 23, no. 12, p. 31,
2012.
[8]
D. L. Donoho, “Compressed sensing,” IEEE Transactions on Informa-
tion Theory, vol. 52, no. 4, pp. 1289–1306, 2006.
189
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-296-7
SENSORCOMM 2013 : The Seventh International Conference on Sensor Technologies and Applications

[9]
M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T. Sun,
K. F. Kelly, and R. G. Baraniuk, “Single-pixel imaging via compressive
sampling,” IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 83–91,
2008.
[10]
J. M. Kahn, R. H. Katz, and K. S. J. Pister, “Next century challenges:
Mobile networking for ‘Smart Dust’,” in Proceedings of the 5th an-
nual ACM/IEEE International Conference on Mobile Computing and
Networking (MobiComm 99), 1999, pp. 271–278.
[11]
P. R. Gill, C. Lee, D.-G. Lee, A. Wang, and A. Molnar, “A microscale
camera using direct Fourier-domain scene capture,” Optics Letters,
vol. 36, no. 15, pp. 2949–2951, 2011.
[12]
P. R. Gill, C. Lee, S. Sivaramakrishnan, and A. Molnar, “Robustness of
planar Fourier capture arrays to colour changes and lost pixels,” Journal
of Instrumentation, vol. 7, pp. C01–61, 2012.
[13]
A. Wang and A. Molnar, “A light-ﬁeld image sensor in 180 nm CMOS,”
IEEE Journal of Solid-State Circuits, vol. 47, no. 1, pp. 257–271, 2012.
[14]
A. Wang, P. R. Gill, and A. Molnar, “Light ﬁeld image sensors based
on the Talbot effect,” Applied Optics, vol. 48, no. 31, pp. 5897–5905,
2009.
[15]
S. Sivaramakrishnan, A. Wang, P. R. Gill, and A. Molnar, “Enhanced
angle sensitive pixels for light ﬁeld imaging,” in IEEE International
Electron Devices Meeting (IEDM), 2011, pp. 8.6.1–8.6.4.
[16]
C. Peroz, S. Dhuey, A. Goltsov, M. Volger, B. Harteneck, I. Ivonin,
A. Bugrov, S. Cabrini, S. Babin, and V. Yankov, “Digital spectrometer-
on-chip fabricated by step and repeat nanoimprint lithography on pre-
spin coated ﬁlms,” Microelectronic Engineering, vol. 88, no. 8, pp.
2092–2095, 2011.
[17]
P. R. Gill and D. G. Stork, “Lensless ultra-miniature imagers using
odd-symmetry spiral phase gratings,” 2013, optical Society of America
Sensors Congress.
[18]
R. L. Morrison, “Symmetries that simplify the design of spot array
phase gratings,” Journal of the Optical Society of America A, vol. 9,
no. 3, pp. 464–471, 1992.
[19]
P. R. Gill, “Odd-symmetry phase gratings produce optical nulls uniquely
insensitive to wavelength and depth.”
[20]
——, “Odd-symmetry phase gratings produce optical nulls uniquely
insensitive to wavelength and depth,” 2013, submitted for publication.
[21]
D. G. Manolakis, V. K. Ingle, and S. M. Kogon, Statistical and adap-
tive signal processing: Spectral estimation, signal modeling, adaptive
ﬁltering and array processing.
Norwood, MA: Artech, 2005.
[22]
D. A. Fish, A. M. Brinicombe, E. R. Pike, and J. G. Walker, “Blind
deconvolution by means of the Richardson-Lucy algorithm,” Journal of
the Optical Society of America A, vol. 12, no. 1, pp. 58–65, 1995.
[23]
H. Chaouchi, Ed., The Internet of Things: Connecting objects.
New
York, NY: Wiley, 2010.
[24]
R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classiﬁcation, 2nd ed.
New York, NY: Wiley, 2001.
190
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-296-7
SENSORCOMM 2013 : The Seventh International Conference on Sensor Technologies and Applications

