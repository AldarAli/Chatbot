Perception of Utility in Autonomic VoIP Systems
Edward Stehle, Maxim Shevertalov, Paul deGrandis, Spiros Mancoridis, Moshe Kam
Department of Computer Science
Drexel University
Philadelphia, PA 19104, USA
Email: {edward, max, pd442, spiros} @drexel.edu and kam@minerva.ece.drexel.edu
Abstract
The transmission of voice-over-Internet protocol
(VoIP) network trafﬁc is used in an increasing
variety of applications and settings. Many of these
applications
involve
communications
where
VoIP
systems are deployed under unpredictable conditions
with poor network support. These conditions make
it difﬁcult for users to conﬁgure and optimize VoIP
systems and this creates a need for self conﬁguring
and self optimizing systems. To build an autonomic
system for VoIP communications, it is valuable to be
able to measure the user perceived utility of a system.
In this paper we identify factors important to the
estimation of user perceived utility in task dependent
VoIP communications.
Keywords-autonomic; VoIP; utility function;
1. Introduction
As the transmission of voice-over-Internet protocol
(VoIP) network trafﬁc becomes commonplace, VoIP
is used in an increasing variety of applications and
settings. Many current applications are outside the con-
text of simple social conversation across dependable
networks. Field applications, such as military opera-
tions, employ VoIP for task-speciﬁc communications
and require VoIP to operate under poor network con-
ditions. Emergency-response personnel may use VoIP
communications to complete tasks in disaster areas
where extreme weather or other adverse conditions
interfere with network performance. Operations may
be carried out in locations where there is little or no
communications infrastructure or where the communi-
cations infrastructure has been damaged. Under these
ﬁeld conditions VoIP needs to be served by small,
mobile, ad-hoc networks with limited resources.
VoIP systems for ﬁeld communications need to be
deployed quickly to minimize response time. In order
to deliver the best possible support to ﬁeld operations,
VoIP systems must be optimized to the ﬁeld conditions.
This creates a difﬁcult problem for the users of ﬁeld
VoIP systems. How do you quickly ﬁnd an optimal
conﬁguration for a VoIP network under adverse con-
ditions when little is known about these conditions
before the system arrives in the ﬁeld? How do you
optimally manage a VoIP network under changing ﬁeld
conditions? This is an ideal application for autonomic
systems. If we can produce a context aware VoIP
system that can self conﬁgure when deployed and self
optimize as ﬁeld conditions change, we can reduce
deployment time and improve overall performance in
unknown and unpredictable settings.
In order to build an autonomic system for ﬁeld
VoIP communications, we must have a way to measure
the performance of the system. Such an autonomic
system must be aware of user perceived utility of
the VoIP application. One approach when including
“black-box” applications in an autonomic system, is to
develop models for application utility estimation [2].
Autonomic systems using utility function policies [3],
[4] require an estimate of an application’s performance.
Previous work in the area of monitoring the health
of autonomic systems involved the use of a pulse to
estimate the health of speciﬁc autonomic elements [5],
[6], [7].
In this paper we look at methods to map network
conditions to user-perceived utility as a utility function.
We will review the ﬁndings of our earlier work [1].
We identify factors that need to be considered when
mapping network conditions to user perceived utility.
Speciﬁcally, we determine if the mapping from net-
work conditions to perceived utility is task dependent.
We also determine if the mappings for users perform-
ing different roles within the same task are affected
by their roles. We will compare the results of our
92
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

previous experiments [1] with the E-Model. Finally,
we wish to determine if perceived utility changes with
the continued repetition of a task.
This paper is structured as follows. First we present
previous work in calculating the user perceived utility
of VoIP applications (Section 2). Then we will present
the set up of our human subject experiments to ex-
plicitly determine user perceived utility of VoIP appli-
cations (Section 3). We will conclude by presenting
our results (Section 4), concluding remarks (Section
5), and an appendix of collected data (Section 6).
2. Previous Work
Existing approaches for predicting user perception
of utility in VoIP systems fall into two main categories.
Some approaches base predictions on the degrada-
tion of a reference signal and other approaches map
network conditions to perception of utility based on
subjective data gathered in human-subjects testing.
2.1. Reference Signal Approach
Objective systems such as the Perceptual Speech
Quality Measure (PSQM) [14] and the Perceptual
Assessment of Speech Quality (PESQ) [13] require a
speech sample to be sent across a VoIP network. The
original sample is then compared to the sample that
is received on the other end of the VoIP system. A
prediction of user utility is made based on the degree
to which the signal has degraded.
The main criticism of the existing objective ap-
proaches is that they only consider signal distortion
in one direction. They do not consider network im-
pairments such as delay and echo [12].
2.2. Subjective Testing based Approach (E-
Model)
The most common model for mapping network con-
ditions to user-perceived utility for voice applications
is the E-model [10]. During the mid-nineteen nineties
the International Telecommunications Union (ITU) de-
signed the E-Model to measure objectively the quality
of a public-switched telephony network (PSTN). The
E-Model was originally intended to be used by network
planners to predict the quality of PSTNs without the
need for expensive and time-consuming testing of
human subjects. It has since been adapted to cellular
communications and IP telephony [11], [9], [8].
The E-Model uses a transmission rating factor as a
measure of the predicted network quality. The trans-
mission rating factor R is the linear sum of various
Impairment factors and an expectation compensation
factor. This linear sum is described in Equation 1.
R = Ro − Is − Id − Ie + A
(1)
The ﬁrst variable Ro is the baseline of the model
for the given network. This is the E-Model value of
the unimpaired network. The most commonly used
baseline value for an ideal unimpaired network is one
hundred. If the network does not perform ideally in
the absence of impairment factors it may be given
a lower baseline value. The Is impairment factor is
deﬁned as simultaneous impairment, which is the sum
of impairments occurring simultaneous to voice. This
includes impairments such as inappropriate volume
and sidetone, which cannot be separated from voice.
Sidetone is any sound from the earpiece of a phone
that is picked up by the mouthpiece of the phone.
The primary effect of sidetone is echo. Most studies
that use the E-Model for evaluating VoIP calls do
not include simultaneous impairments since they are
”intrinsic to the voice signal itself and do not depend
on the transmission over the network” [8]. The Id
impairment factor is the impairment caused by the
round trip delay of the voice signal. Any Impairment
caused by the use of speciﬁc equipment is included
in the Ie factor. This factor includes distortion of the
original signal due to the codec, the packet loss in the
network and the packet loss in the playback buffer. The
ﬁnal factor A serves as a method to compensate for the
expectation or other advantages derived from using IP
telephony. For instance, most people expect that over
traditional telephone wire the call would be very good
but are a little more forgiving when speaking over a
mobile phone.
The E-Model has become a commonly used metric
to predict the quality of VoIP applications for several
reasons. Most models for objective quality measure-
ment require that the received signal be compared
to the sent signal. The E-Model is the only widely
recognized metric that does not require a reference
signal, making it computationally feasible for real
time applications. In addition, the E-Model correlates
well with subjective quality in situations where IP
telephony functions in the same fashion as PSTN;
for example in local VoIP networks where anomalous
trafﬁc conditions are minimized.
The E-Model transmission rating factor R can be
mapped to a Mean Opinion Score (MOS) by the use
of a function described by Cole and Rosenbluth [11].
MOS is a scoring system commonly used in tests
involving human subjects. Subjects using MOS rate
the quality of a VoIP system with a score from one to
93
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

ﬁve where one is the worst quality and ﬁve is the best.
The function for converting R is illustrated in Equation
2, Table 1 and Figure 1.
f(r) =











1
r≤0
1 −
7
103 r +
7
6250r2 +
7
106 r3
0<r<100
4.5
r≥100
(2)
Table 1. Mapping of Transmission Rating Factor
to the Mean Opinion Scale
Transmission Rating
User Satisfaction
Mean Opinion
Factor
Score
Scale of 0-100
Scale of 1-5
90-100
Best
4.34-4.5
80-90
High
4.03-4.34
70-80
Medium
3.60-4.03
60-70
Low
3.14-3.60
50-60
Poor
2.58-3.1
0-50
Worst
4.34-4.5
Figure 1. Mapping of Transmission Rating Factor
to the Mean Opinion Scale
There are however problems with using the E-Model
to predict user satisfaction with VoIP. Although the
E-Model correlates well with subjective quality in
situations where IP telephony functions in the same
fashion as PSTN, using the E-Model in the context
of the Internet greatly decreases such correlation. The
E-Model was not derived for this explicit purpose.
In fact the E-Model was not intended as a quality
assessment tool, but rather tool for planning circuit
switched networks. The E-Model was not meant to
be applied to IP networks. The impairment factors
that comprise it deal more with signal processing than
with IP networks. The E-Model does not consider the
differing expectations users may have toward delay
when using VoIP over the Internet. Delay in IP net-
works is greater than delay in PSTNs. When using
a large congested and unpredictable IP network such
as the Internet the delay can be much greater than in
PSTNs. Users who are used to dealing with delays
when using Internet applications may be more tolerant
of delay when using VoIP over the Internet. Although
the E-Model includes a variable to compensate for
user expectations it is independent of the impairment
factors. Internet VoIP users may be more tolerant of
delay, but not more tolerant of loss. This cannot be
captured by the expectation compensation factor A
in E-Model. The compensation factor adds a constant
independent of the impairment factors.
2.3. Problems with current approaches
Neither reference-signal based approaches nor sub-
jective test approaches consider the impact of task on
a perceived utility. Current models assume that, given
network conditions, users will always perceive utility
in the same manner regardless of what task they are
using VoIP to perform. In tests using circuit-switched
networks Kitawaki and Itoh concluded that speech
quality due to propagation delay greatly depends on
the kind of task [12]. Their tests showed that delay
has a greater effect on tasks that are more interactive.
3. Our Tests
In our tests, subjects rate the quality of VoIP under
varying network conditions. Each test involves one pair
of human test subjects. The subjects carry out a series
of similar tasks that require communication using a
VoIP application. For all of our testing we used Gnome
Meeting as the VoIP application and G.711 for our
audio codec. We vary the network conditions using a
FreeBSD application named Dummynet, which allows
us to set the bandwidth, latency and loss of the link
used by our test subjects. A single test point in our
experiment is a 3-tuple (bandwidth, latency, loss). Each
of these parameters can have one of ﬁve values. We
test across all combinations of these values, giving
94
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

Table 2. 3-tuple Parameters
Parameter
Values
Bandwidth
25, 40, 50, 65, 80 (kbps)
Latency
0,1000, 2000, 3000, 4000 (ms)
Loss
0, 12.5, 25, 50, 60 (percent)
us 125 points per subject. The possible values of the
parameters are listed in Table 2.
We have been performing three different types of
human subject tests, each with a different task. We be-
lieve that the relationship between network conditions
and user satisfaction is task dependent and that using
more than one test with different tasks will provide
data to support this belief. All of the tests have the
same basic structure. There are two roles that the sub-
jects play during a test. One subject is a questioner
and one subject is a responder. The actual duties of
the questioner and the responder vary between
the types of test. The subjects perform one task at
each of the 125 test points. After a task is completed
each subject votes on the quality of the communication.
Then the network conditions are changed to the next
point and the next task begins. The subjects rate the
quality on a scale of one to ﬁve where one is bad,
ﬁve is good, and three is okay. The subjects alternate
between the roles of questioner and responder after
each task. Each test collects 250 data points and takes
between 60 and 90 minutes to complete.
3.1. Simple Information Exchange Test
The ﬁrst VoIP test is designed to measure per-
ceived utility during tasks involving a simple exchange
of information. The tasks in this test involve the
the questioner asking a trivia question and the
responder answering it. Completion of this task in-
volves minimum back-and-forth conversation between
the subjects and does not have any time constraint. We
believe that this test is useful for modeling VoIP com-
munications where the users are simply exchanging
facts or instructions. For example, if VoIP is being used
to convey a military target’s position and instructions
for engaging the target, we expect the conversation to
be limited to conveying position, conveying instruc-
tions, and a conﬁrmation that the message has been
received.
In this test the questioner is given a trivia
question and the answer to the trivia question. A
screenshot of our testing application with a sample
question can be seen in Figure 2. The responder
is given a list of possible answers, one of which is
correct. A screenshot of the responders answers can be
seen in Figure 3. The questioner reads the question
to the responder. The responder picks an answer
from the list and reads it to the questioner. Then
the questioner records whether the question was
answered correctly. This requires both subjects to
receive a piece of information from the other and then
respond to that information.
We have conducted the simple information test with
thirty human subjects and collected 3750 data points.
Figure 2.
Simple Information Exchange Test
Questioner Screen
Figure 3. Simple Information Exchange Test Re-
sponder Screen
3.2. Time-Sensitive Collaboration Test
The second test is designed to measure perceived
utility during time-sensitive tasks that involve some
95
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

collaboration between subjects. The tasks in this test
involve a considerable amount of back-and-forth con-
versation between the two subjects in order complete
a time-constrained task. This test is intended to model
situations where users are not trying simply to convey
information but to perform some collaborative task. For
example, if two military commanders need to collabo-
rate on a plan for a time-critical task, we would expect
a considerable amount of back-and-forth conversation
and pressure to complete the plan quickly.
Figure 4.
Time-Sensitive Collaboration Test Re-
sponder Screen
In this test the questioner is given a word
that the responder must correctly guess, but the
questioner may not explicitly state the word.
Screenshots of the Time-Sensitive Collaboration Test
can be seen in Figures 4 and 5. The questioner can
only describe the word and answer the questions of the
responder. The responder can guess the word or
ask the questioner for speciﬁc information about
the word. Each task has a time limit of thirty seconds.
The task ends when the responder correctly guesses
the word or the time runs out.
We have conducted the time-sensitive collaboration
test with 30 human subjects and collected 3750 data
points.
3.3. Time-Sensitive Information Exchange
The third VoIP test is designed to measure perceived
utility during time constrained tasks involving the ex-
change of multiple pieces of information. The tasks in
Figure 5.
Time-Sensitive Collaboration Test Re-
sponder Screen Time Expired
this test involve the collaborative summing of a series
of small integers within a limited period of time. This
test is intended to model situations where users need to
collaborate and the collaboration is limited to a series
of simple exchanges of information. For example, in
order to coordinate the response of emergency workers
in separate locations of a disaster area these workers
may need to combine collected data such as the number
of disaster victims.
In this test the questioner and responder are
each given a list of integers. The questioner is
given a “starting number”, an “ending number” and
two “adding numbers”. The responder is given
three “adding numbers”. The starting number is an
integer from zero to ten, the adding numbers are
integers from zero to ﬁve, and the ending number is the
sum of the starting number and the adding numbers.
The questioner initiates the task by reading the
starting number to the responder. The responder
adds his ﬁrst adding number to the starting number
and reads the sum to the questioner. The exchange
continues with each subject adding one adding number
to the sum until all of the adding numbers have been
summed with the starting number. Once all of the
adding numbers have been summed with the starting
number the questioner checks the total against the
ending number and informs the responder that the
numbers have been summed correctly or incorrectly.
Each task has a time limit of thirty seconds.
We have conducted the time sensitive collaboration
96
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

test with 30 human subjects and collected 3750 data
points.
3.4. User-Adjustment Tests
User-adjustment tests were designed to measure
changes in perceived utility as a task is repeated. The
tasks in these tests are performed over a set of network
conditions, and then repeated over the same set of
network conditions. The results from the ﬁrst time
through the set of network conditions can then be
compared to the results from the second time through
the same set of network conditions. These tests are
designed to model situations where a user learns and
adjusts to tasks.
User adjustment tests were performed using the
three previously described tasks. These include the
tasks described in Section 3.1 (Simple Information
Exchange Test), Section 3.2 (Time Sensitive Collab-
oration Test), and Section 3.3 (Time Sensitive Infor-
mation Exchange). In their original form, each of the
previously described tests was performed over 125
network condition points. Repeating all points in a test
would yield a test with 250 data points that would
take two to three hours to complete. A test of this
length would tire the test subject. This would corrupt
the test results and create unnecessary stress for the
test subjects. In order to reduce the time required to
complete test trials the size of the set of network
settings was reduced. The possible values of network
condition parameters described in Table 3 were altered
so that only the highest bandwidth value was used.
A single test point in our user adjustment tests is a
2-tuple (latency, loss). Each of these parameters can
have one of 5 values, giving us 25 points. These points
are randomly ordered, and then repeated in the same
random order, giving us 50 points per subject. The set
of possible parameters for the user adjustment tests is
listed in Table 3.
Table 3. 2-tuple Parameters
Parameter
Values
Latency
0,1000, 2000, 3000, 4000 (ms)
Loss
0, 12.5, 25, 50, 60 (percent)
3.5. Our Test Bed
In order to carry out these tests we created a test bed
that allows two subjects to converse using VoIP while
we control the properties of the channel over which
VoIP is running.
Our test bed consists of one “subject computer” for
each of our two subjects, a switch partitioned into
two subnets, and one “bridge computer” that is used
to set the bandwidth, latency and loss of the channel
over which the two subject computers communicate.
Figure 6 illustrates the manner in which the test bed is
connected. Each of the subject computers is connected
to a different subnet and the bridge computer is
connected to both of the subnets. Communications
between the two subject computers are routed through
the bridge computer. The bridge computer employs
Dummynet to enforce the bandwidth, latency and loss
on the channel connecting the two subject computers.
The subject computers and the bridge computer are
also connected through a back channel, which is not
effected by Dummynet. This back channel is used to
send messages to the bridge computer instructing it to
change the Dummynet settings.
Figure 6. Architecture of the Test Bed
4. Results
The results of our experiments can be seen in Fig-
ures 7-12 found in the Appendix (Section 6). There are
two types of ﬁgures: tests in which the test points are
3-tuples (bandwidth, latency, loss) that are represented
by three-dimensional plots and tests in which the test
points are 2-tuples (latency, loss) that are represented
by two-dimensional plots.
The three-dimensional plots show the space deﬁned
by bandwidth, loss and latency measurements. Within
this space color is used to represent a user-satisfaction
rating. The darkest red represents the areas that were
rated best, and the darkest blue represents the areas
97
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

that were rated worst. In each of these ﬁgures our test
space is represented by three plots, each sliced along a
different axis. One is cut along bandwidth, one along
latency, and one along loss.
The two-dimensional plots show the space deﬁned
by our loss and latency measurements. The same color
convention is used to represent user satisfaction rating.
4.1. Different Tasks
In this section we present the results of our Simple
Information Exchange Test, Time-Sensitive Informa-
tion Exchange and Time-Sensitive Collaboration Test.
Descriptions of each of these tests can be found in
Section 3 and the results can be seen in Figures 7, 8 and
9. The average variance, minimum variance, maximum
variance and the variance of the variance for all test
points is shown in tables 4 through 6.
As expected, the results vary somewhat for different
tasks. One obvious difference between the results for
different tasks is the effect of latency on utility. In
the time-sensitive collaboration test and in the time-
sensitive information exchange test, latency had a
greater effect on perceived utility than in the simple
information-exchange test. These results make intuitive
sense. Tests in which the tasks are subject to time
constraints show a greater user reaction to latency. We
believe that this is caused not only by the addition
of the time constraints, but also by the collaborative
nature of the communication. During this type of
collaboration, subjects spend more time speaking back-
and-forth than they do during the simple information
exchange test. Greater latency can cause this back-
and-forth communication to fall out of sync, creating
additional difﬁculties in communication.
Another obvious difference is the effect of band-
width and loss. Bandwidth has the greatest effect on
the simple information-exchange test. We believe that
the collaborative nature of the time-sensitive tests helps
users adjust to poor voice quality. Because these tests
involve more back-and-forth communication, the users
have more opportunity to recognize poor quality. Once
poor voice quality is recognized, users may begin to
employ strategies such as repeating messages without
being asked. The back-and-forth communication also
gives users more opportunity to recognize conversa-
tional context. Recognizing conversational context can
be helpful for ﬁlling in portions of messages which
cannot be understood.
4.2. Different Roles in a Task
Within each of the tasks described in Section 3 one
test subject act as a questioner and one test subject acts
Table 4. Variance of User Perceived Utility for
Simple Information Exchange
Average Variance
0.732
Maximum Variance
2.193
Minimum Variance
0.216
Variance of Variance
0.080
Table 5. Variance of User Perceived Utility for
Time Sensitive Collaboration
Average Variance
0.610
Maximum Variance
1.140
Minimum Variance
0.127
Variance of Variance
0.040
Table 6. Variance of User Perceived Utility for
Time Sensitive Information Exchange
Average Variance
0.740
Maximum Variance
2.187
Minimum Variance
0.187
Variance of Variance
0.101
as a responder. Figure 10 shows the results of the Time
Sensitive Information Exchange test for both responder
and questioner, responder only, and questioner only.
When the results of our test are split into questioner-
only and responder-only plots it is clear that the role
played within a task has an effect on perceived utility.
Again, this is an expected result. Different roles within
a single test can be thought of as different sub-tasks,
and we have already illustrated that perceived utility is
task dependent.
4.3. User Adjustment
In the User Adjustment tests described in Section
3.4, we have subjects carry out tasks over the same
test points two times in a row. The purpose of tests if
to determine if the test subjects adjust to adverse net-
work conditions while performing tasks. The results of
subjects performing the Simple Information Exchange
test over the same set of 25 points two times in a row
can be found in Figure 11.
The results of our user adjustment tests show per-
ceived utility changes as users repeat a task. In each of
the tests the variance of the perceived utility decreased
during the second time through the test points. At
the same time the average perceived utility stayed
approximately the same. It appears that as users repeat
a task over different network conditions they “get used
to it”. They perceive fewer extremes in utility and tend
to perceive a larger portion of the test space as “okay”.
98
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

4.4. Comparison to E-model
The E-model equation for predicting user perception
of utility is described in Section 2.2. It is the most
commonly used tool for prediction of user utility in
VoIP systems. Figure 12 shows a comparison of the
E-model to our test results for the Simple Information
Exchange test.
Our results differ greatly from the predictions of he
E-model. We believe that this difference is due to task
oriented nature of our tests. The E-model was created
to predict user perceived utility in circuit switched
phone systems. These phone systems are designed to
handle not only task oriented communications, but also
social conversations. We believe a user given a task
to complete is less likely to dismiss a communication
session due to impaired quality than a user having a
social conversation. The user attempting to complete
a task is more likely to ﬁne utility in an impaired
connection that allows them to complete their task.
5. Summary and Conclusion
Knowledge of network conditions, such as band-
width, latency and loss, is not sufﬁcient to predict
the performance of a VoIP system adequately. The
predictor must also have knowledge of the task being
performed over the VoIP system. Our tests show that
user perceived utility may be very different for users
performing different tasks even if network conditions
are the same.
Many tasks performed over VoIP systems involve
multiple users playing different roles within the tasks.
Our tests show that perceived utility may be very
different for users performing different roles within a
task. When determining what network resources are
required to complete a task, it may be necessary to
base predictions on the most constrained role within a
task.
While carrying out a task, a user may adjust to
a task and network condition combination. Our tests
show that user perception of utility changes as a user
repeats tasks over the same network conditions. Users
may beneﬁt by starting to talk over a VoIP connection
before beginning a task. Users may also beneﬁt by
training over simulated bad network conditions.
References
[1] Stehle, E.; Shevertalov, M.; deGrandis, P.; Mancoridis,
S.; Kam, M., “Task Dependency of User Perceived
Utility in Autonomic VoIP Systems,” Autonomic and
Autonomous Systems, 2008. ICAS 2008. Fourth Interna-
tional Conference on , pp.248-254, 16-21 March 2008
[2] Karlsson, M.; Covell, M., “Dynamic Black-Box Perfor-
mance Model Estimation for Self-Tuning Regulators,”
Autonomic Computing, 2005. ICAC 2005. Proceedings.
Second International Conference on, pp.172-182, 13-16
June 2005
[3] Kephart, J.O.; Walsh, W.E., “An artiﬁcial intelligence
perspective on autonomic computing policies,” Policies
for Distributed Systems and Networks, 2004. POLICY
2004. Proceedings. Fifth IEEE International Workshop
on , pp. 3-12, 7-9 June 2004
[4] Walsh, W.E.; Tesauro, G.; Kephart, J.O.; Das, R., “Utility
functions in autonomic systems,” Autonomic Computing,
2004. Proceedings. International Conference on, pp. 70-
77, 17-18 May 2004
[5] Sterritt, R., “Pulse monitoring: extending the health-
check for the autonomic grid,” Industrial Informatics,
2003. INDIN 2003. Proceedings. IEEE International
Conference on , pp. 433-440, 21-24 Aug. 2003
[6] Sterritt, R; Bustard, D “A health-check model for auto-
nomic systems based on a pulse monitor” Knowl. Eng.
Rev., Cambridge University Press , vol.21, no.3pp.195-
204, 2006
[7] Hong-Linh Truong; Fahringer, T.; Nerieri, F.; Dustdar,
S., “Performance metrics and ontology for describing
performance data of grid workﬂows,” Cluster Computing
and the Grid, 2005. CCGrid 2005. IEEE International
Symposium on , vol.1, pp. 301-308 Vol. 1, 9-12 May
2005
[8] Markopoulou, A.P.; Tobagi, F.A.; Karam, M.J., “As-
sessment of VoIP quality over Internet backbones,” IN-
FOCOM 2002. Twenty-First Annual Joint Conference
of the IEEE Computer and Communications Societies.
Proceedings. IEEE , vol.1, pp. 150-159 vol.1, 2002
[9] Hall, T. “Objective Speech Quality Measures for Internet
Telephony” Proceedings of SPIE Voice over IP VoIP
Technology, vol. 4522, pp. 128-136, July 2001
[10] Johannesson, N.O., “The ETSI computation model: a
tool for transmission planning of telephone networks,”
Communications Magazine, IEEE , vol.35, no.1, pp.70-
79, Jan 1997
[11] Cole, R.G.; Rosenbluth, J.H., “Voice Over IP Perfor-
mance Monitoring” SIGCOMM Computer Communica-
tion Rev. 31, 2, Apr. 2001
[12] Kitawaki, N.; Itoh, K., “Pure delay effects on speech
quality in telecommunications,” Selected Areas in Com-
munications, IEEE Journal on , vol.9, no.4, pp.586-593,
May 1991
[13] Rix, A.W.; Beerends, J.G.; Hollier, M.P.; Hekstra, A.P.,
“Perceptual evaluation of speech quality (PESQ)-a new
method for speech quality assessment of telephone net-
works and codecs,” Acoustics, Speech, and Signal Pro-
cessing, 2001. Proceedings. (ICASSP ’01). 2001 IEEE
International Conference on , vol.2, pp.749-752 vol.2,
2001
99
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

[14] Kitawaki, N., “Perceptual QoS assessment methodolo-
gies for coded speech in networks,” Speech Coding,
2002, IEEE Workshop Proceedings. , pp. 80-82, 6-9 Oct.
2002
100
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

6. Appendix
Figure 7. Simple Information Exchange
101
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

Figure 8. Time Sensitive Collaboration
102
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

Figure 9. Time Sensitive Information Exchange
103
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

Figure 10. Time Sensitive Information Exchange Split by Questioner and Responder
104
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

Figure 11. Simple Information Exchange User-Adjustment Tests
105
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

Figure 12. E-Model vs Simple Information Exchange
106
International Journal On Advances in Intelligent Systems, vol 2 no 1, year 2009, http://www.iariajournals.org/intelligent_systems/

