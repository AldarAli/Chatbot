MoNA: Automated Identiﬁcation of Evidence in
Forensic Short Messages
Michael Spranger∗, Florian Heinke, Luisa Appelt, Marcus Puder and Dirk Labudde
Department of Applied Computer Sciences & Biosciences
University of Applied Sciences Mittweida
Mittweida, Germany
Email: {spranger∗, ﬂorian.heinke, luisa.appelt, marcus.puder, labudde}@hs-mittweida.de
Abstract—Mobile devices are a popular means for planning,
appointing and conducting criminal offences. In particular, short
messages (SMS) and chats often contain evidential information.
Due to the terms of their use, these types of messages are funda-
mentally different from other forms of written communication in
terms of their grammatical and syntactic structure. Due to the low
price of media storage, messages are rarely deleted. On one hand,
this fact is quite positive as potentially evidential information
is not lost. On the other hand, considering only SMSs, 15,000
and more stored only on one mobile phone is not uncommon. In
most cases of organized or gang crime, there is not one but many
devices in use. Analysing this large amount of messages manually
is time consuming and, therefore, not economically justiﬁable in
the cases of small and medium crimes. In this work, we propose a
process chain that enables to decrease the analysis and evaluation
time dramatically by reducing the amount of messages, that need
to be examined manually. We further present an implemented
prototype (MoNA, mobile network analyzer) and demonstrate its
performance.
Keywords–forensic; ontology; German; text processing; expert
system; text analysis; short messages
I.
INTRODUCTION
With our previous work [1] we try to close the gap between
backup and recovery of data and its content analysis in the
context of mobile forensics. The fast-growing mobile market,
constantly emerging or rapidly changing technologies and high
hardware diversity require rapid development of new forensic
tools. In recent years, many works have dealt with the backup
and recovery of data on a variety of platforms [2]. However,
there are few works that deal with the analysis of the textual
content.
Over the last decade, our understanding of communication
and its means have changed drastically. With the introduc-
tion of inexpensive messaging technologies and comfortable
usability driven by increasingly powerful smart devices, com-
munication has shifted towards conversing via chats and short
messages (especially short messaging service, SMS). Besides
rising computational power, mobile devices are also provided
with signiﬁcant amounts of memory that allow storing appli-
cation data, documents, images, and thousands of messages
exchanged with a multitude of conversation partners. Although
the number of exchanged messages can be in the thousands,
they account only for a small fraction of occupied space in
general. In consequence, chat and SMS logs are rarely deleted.
In the context of digital forensics, this aspect leads to
an ambivalent situation. On the positive side, it has become
more likely that conﬁscated devices yield information rele-
vant for the investigation process and could reveal additional
evidential aspects, such as identities of backers or other crime-
related intentions of the suspect. On the downside, these
information need to be extracted from the raw chat data,
which is, considering its scale, barely manageable by manual
means. In addition, with the growing amount of available
memory and the ongoing popularity growth of text messaging,
it can be postulated that manual perusing and annotating will
become practically impossible. Hence, there is a necessity in
developing computational (semi-) automated technologies that
can support the investigator in the process. To achieve this,
researches have to cope with a number of issues. Most notably,
messages are often enriched with typos and grammar mistakes
introduced by lowered language use standards observable in
casual text conversations. Such mistakes pose major problems
for text mining and computer linguistics.
Based on our previous work [1] a straightforward technique
for identifying individual conversations in SMS chat logs
is introduced in this paper and evaluated on a manually-
annotated SMS dataset. In addition, in Section II we ﬁrst
discuss general background and related work considered in
the MoNA development process. In Section III, we deﬁne and
characterise short message semantic analyses in the context of
forensics, providing detailed aspects involved in the motivation
of our work. Further, the SMS dataset used to develop and
evaluate MoNA is described. We emphasize that the dataset
in use has been relevant for a closed drug crime investigation,
thus actual information provided in this study are based on
a real-life application scenario. In this respect, in Section
IV we introduce measures for quantifying the potency of a
keyword dictionary provided by investigators that is used by
MoNA to classify and score identiﬁed conversations. These
measures additionally provide statistical ﬁgures that can be
used to further optimise and reﬁne the dictionary in use.
After discussing the evidential conversation detection process
utilised by MoNA (Section V), we demonstrate its performance
in Section VI and provide future prospects in Section VII.
II.
RELATED WORK
Compared with texts from industry, medicine or science,
relatively few works deal with the analysis of short messages.
Most of these works address the binary classiﬁcation problem
in terms of SPAM detection. For example, in a large-scale
study Skudlark [3] examined approaches to detect SPAM
activities. However, they rely on the presence of URLs in the
14
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

text body, which limits the applicability of these methods to
forensic short messages on very few cases of fraud, computer
sabotage or similar crimes. Ahmed et al. [4] presented an
SMS classiﬁcation approach based on Naive Bayes and a
priori algorithms. A further method has been discussed by
Xu et al. [5] that relies on content features for classiﬁcation.
Although this method yields reasonable results, an application
to most ﬁelds of forensics cannot only be based on meta
data. But this kind of data can be useful to enlarge the target
matching space. In the ﬁeld of multi class SMS classiﬁcation
Al-Talib et al. [6] introduced a technique using an improved
TF-IDF weighting, whereas Patel et al. exploit artiﬁcial neural
networks [7]. Another interesting work was presented in 2011
by Ishihara [8]. In this work, the author proposed a likelihood
ratio-based approach for SMS authorship classiﬁcation using
n-grams. The model was trained and evaluated using the NUS
SMS corpus [9]. Unfortunately, a similar corpus for German
forensic SMS is currently not available. The general problem
when trying to create such a corpus in the ﬁeld of forensics
is the availability of real-life data. This is the reason why
Ishihara considered non-forensic data, while the developed
classiﬁer has been trained and evaluated for forensic purposes.
Therefore, with respect to forensic short messages and their
special characteristics (which are discussed in Section III-B
in more detail), the applicability of such a classiﬁer and
its performance in the real-life context of forensics remains
unsettled. Furthermore, approaches for extracting information
from short messages, as discussed in [10], [11], are frequently
based on the presence of correct grammatical structures. How-
ever, these do not exist in most cases for short messages.
Knowledge-based approaches, such as proposed in [12], are
more promising since they can include a priori knowledge
of the investigator to support information retrieval as well as
information extraction processes. In the work presented by
Nebhi [12] Twitter posts are considered, which, however, are
similar to forensic SMS in some degree.
Thus, in general, there is no approach available that can
cope with the challenges posed by real-life short message
data. In addition, such an approach is required to be both
applicable to all the data, and to perform as reliable as required
by forensic investigation standards.
III.
BACKGROUND
A. Forensic Short Messages
The analysis of short messages is a particular challenge
in the context of forensic text analysis. The reasons for this
is the combination of forensic text characteristics and of high
information density, which is characterized by limitations in
the number of characters. Such limitations arise on the one
hand by technical reasons, on the other hand by the kind of
use. Thus, short messages are often used in terms of “by the
way messages”.
Deﬁnition 1 (Forensic Short Message): In this study, any
textual message having the properties of an incriminated text
and is sent or received using a short message service is
considered a forensic short message.
Looking at current surveys of the short message trafﬁc in
the Federal Republic of Germany (see Fig. 1), a turnaround
can be seen in the development starting in 2013. The reason
Figure 1: Number of SMS and IM messages (WhatsApp) sent
in Germany from 1990 to 2014 in millions per day (2015
estimated) [13].
for this is not a decrease in the mobile communications in
general, but in a shift to other convenient communication
services such as instant messaging services (e. g. Whats App).
Nevertheless, every citizen sends one instant message per
day on average. The outstanding role of text messages today
and in the future, both in general and forensics contexts,
was thoroughly discussed in [14]. Since the communication
behaviour is mainly inﬂuenced by the type of use, the change
of the medium has had only a relatively small impact on the
writing behaviour of the user. Thus, the results presented in this
work can be transferred in principle to other forms of mobile
communication in writing. In general, the forensic analysis of
incriminated texts is a big challenge for investigators—which
is especially the case for short messages. In addition to large
message quantities only sent by one individual, such messages
have a particular characteristic, which makes the analysis
difﬁcult even for experienced investigators. Considering the
amount of content that needs to be fully read, this effort is
probably justiﬁable only in cases of severe crime or crime
of high public interest. One of the current biggest limitations
during the development of an automated solution for forensic
purposes is the lack of a gold standard. Yet, an effective and
efﬁcient analysis in each relevant case is unthinkable without
the usage of computational solutions.
B. Characteristics of Forensic Short Messages
As already discussed in [1], forensic text’s structure and
quality regarding grammar, syntax and wording strongly de-
pends on the area of the crime committed by the offenders,
their level of education and their social environment. A more
detailed description of the general characteristics of forensic
texts can be found in [15]. Personalized SMS form the extreme
case of these characteristics. They are particularly marked by
frequent lack of correct grammatical structures. Therefore, it
is difﬁcult to use (lexico)-syntactic pattern as discussed in [16]
[17] for extracting information of criminalistic relevance. Fur-
ther, the usage of non-standardised emoticons, abbreviations,
emotionally intended character extensions and especially writ-
ten effects of language erosion caused by language-economic
processes make this task more difﬁcult and lead to a failing
of known techniques. The following list shows some example
texts to illustrate the problem:
•
”aber was ich mein[e] is[t] wir m¨uss[e]n wenn
15
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

wir weihnacht[e]n gefeiert hab[e]n ¨ubelst money
hab[e]n”
•
”Beruhig[e] dich ich zieh[e] denn das n¨achste ma[l]
rich[tig] fette ab! :))))))”
•
”Ich schreib jetzt wegen dir hab ich mein 12g nicht
bekommen Weil Du ne aus[ de]m knick gekommen
bist XD”
Missing characters are included in square brackets, whereas
additional characters are shown as strike-through text. Incor-
rect capitalisations are underlined. Slang-afﬂicted words and
phrases are printed in bold. The most challenging problem in
the considered context of SMS with criminalistic relevance is
the usage of slang-afﬂicted language combined with terms of
hidden semantics. Hidden semantics refer to one kind of a
steganographic code. Such a term is used in its common inno-
cent meaning but its actual semantic background is prearranged
by a narrow circle of insiders. For example, the question
”Bringst du ein Wernesgr¨uner mit?” (Can you bring a
Wernesgr¨uner?)
appears innocent and unsuspicious because the term Wer-
nesgr¨uner (a German beer brand) is used as in asking for a
bottle of beer. However, by considering the actual context, the
author of this message is actually asking for marijuana. Note
that in this example we intentionally do not use slang to avoid
misunderstandings. But commonly terms of slang are mixed
in regularly. These characteristics make it difﬁcult even for
criminalists and linguists with years of experience to read and
understand the semantics of forensic SMSs.
Thus, it becomes clear that any information not identiﬁed
as relevant by an automated system may be crucial in proving
the guilt or innocence of a criminal suspect. Eventually, it
can be stated that decisions concerning the evidential value of
forensic SMSs cannot be made by a machine.
C. Dataset under Consideration
The data used by the authors for the development of MoNA
is based on a dataset of a closed case of drug trafﬁcking
provided by a cooperating prosecutor for research purposes.
Nevertheless, the data is not publicly available. For this
purpose the legal framework has to be established, at least
in Germany. In the case under consideration a smart phone
of the suspect, an HTC Desire A9191, has been seized and
a physical image has been generated by using Cellebrite’s
UFED Physical Analyzer. The textual data contained in this
image was exported as an Excel Workbook and forms the
basis for all further investigations. This dataset includes 14,307
short messages (SMS) and 132,345 chat messages. During the
development of MoNA, only SMS messages have been used so
far. Through an ofﬁcial of a cooperating police department all
short messages were manually read and evidential ones were
labelled as relevant. Afterwards, the same work was performed
by a member of the research team without criminalistic back-
ground.
In summary, only half of the relevant messages were
correctly classiﬁed as evidential by the research member and,
on the contrary, messages considered as insigniﬁcant by the
investigators were classiﬁed as evidential. This shows that sub-
jectivity can introduce signiﬁcant errors in analyses processes
and emphasises the need for expert knowledge. This study
thus focuses on the prototypic implementation of MoNA as
a strategy for identiﬁcation and classiﬁcation of conversations
with respect to their relevance to the crime in question.
IV.
WORD DICTIONARY POTENCY
The majority of text mining and computer linguistic algo-
rithms rely on word dictionaries that provide the initial set
of words, which are screened against a given text dataset
in the process. In computer forensics, the investigator aims
at maximizing the number of identiﬁed messages contain-
ing evidential information, to which we refer to as signif-
icant messages in the following text. In general, the basis
for the successful identiﬁcation of signiﬁcant messages or
conversations in large message sets using string matching
techniques and phonetic algorithms predominantly requires a
potent word dictionary. Word dictionaries are subjected to two
major requirements: First, they have a signiﬁcant impact on
classiﬁcation performances of utilised methods and thus should
be optimally composed in this respect. Secondly, word dictio-
naries are required to be domain-speciﬁc, case-independent,
and generalized corpora of words—meaning that each should
be interchangeable and not be speciﬁcally tailored toward
the dataset in question. Especially in the ﬁeld of computer
forensics, it needs to be further emphasized that a dictionary
is considered to be speciﬁc for a certain time period and region
as well.
In this study, MoNA has been provided with a dictionary
of 90 words speciﬁc for the drug scene currently present in
the Chemnitz/west Saxony region of Germany. In this section,
measures of dictionary potency, which supply simple quan-
tiﬁcations of per-word performance, are introduced, demon-
strated and discussed on the available data. First, measuring
initial classiﬁcation power of dictionary words is demonstrated.
Subsequently, it is shown how word heterogeneity of obtained
word matches in the dataset can be measured. Word match
heterogeneity provides statistical ﬁgures on word diversity in
and between matching word sets in signiﬁcant and insigniﬁcant
(non-relevant) messages, which in turn can be used to represent
per-word speciﬁcity. Here, the investigator aims at maximizing
diversity between word sets and reduce heterogeneity within
the sets, thus reducing ambiguities of obtained matches.
A. Overview on Individual Dictionary Word Potency
Analyses of dictionary potency have been conducted
by employing string matching and two phonetic algorithms
(K¨olner phonetic [18] and Double Metaphone [19]) on the
provided SMS dataset. All three algorithms reported a total
of 11,665 matches in the dataset, of which 310 had been
annotated as signiﬁcant by the investigators. Interestingly, a
large fraction of 42 dictionary words matches exclusively to
either signiﬁcant or insigniﬁcant messages (see Table I). The
seven words only matching to signiﬁcant messages account
for eighteen of the 310 signiﬁcant matches. Furthermore, 35
dictionary words yield no signiﬁcant matches in the dataset,
classifying a total of 17% of the matches as insigniﬁcant.
In theory, a powerful dictionary yields signiﬁcant matches
only. However, the statistics obtained from actual data show
16
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

M+
M-
M+
M-
M-
w1
w2
w3
A
A
A
A
B
B
C
B
B
G
H
H
I
K
K
K
I
bufeln
Christel
Christina
Crystal
drehen
endspannendes
fix
Grünedaum
Schokolade
sechen
smoken
Schnee
XTC
Ecke
Steine
Pillen
high
Walther
Joint
Gutes
ruhig
Figure 2: a: Predictive performance evaluated by means of
Matthews correlation coefﬁcient (MCC) of 90 dictionary words
yielding matches in signiﬁcant and insigniﬁcant messages
(see Table I). b and c: Relative entropy (Hr) and relative
Kullback-Leibler divergence (KLr) are measures for assess-
ing word match set homogeneity respectively set divergence.
Three illustrative word match homogeneity scenarios for words
w1, w2, and w3 are depicted, with schematic plots of Hr
and KLr obtained from these scenarios shown on the right
(M + and M −: matching word sets of signiﬁcant respectively
insigniﬁcant messages). d: Plots of Hr and KLr of 21 words
yielding KLr > 0.4. Colour highlighting is in correspondence
to the three word matching scenarios shown in b and c,
indicating varying degrees of ambiguity and, thus, signiﬁcance
to dictionary power.
mixed power of individual dictionary words. The aspect that
thirty-ﬁve of 90 words are anti-correlated—yielding only in-
signiﬁcant matches—is rather surprising, as one would expect
most of the matches to be exclusively signiﬁcant or at least to
be matching to both cases. Although errors should be expected
in practice, large fractions of anti-correlated words, as observed
in this study, highlight that a given domain-speciﬁc dictionary
can produce unwanted effects caused by hidden ambiguities in
the data. Hence, a dictionary should always be considered as a
set of independent words—each of these with its own meaning
and power with respect to classiﬁcation performance.
TABLE I: Results of initial word dictionary testing. 90 domain-
speciﬁc dictionary words have been matched against the SMS
dataset using string matching, K¨olner phonetic and Double
Metaphone. If a matching message contains evidential infor-
mation, the match is considered signiﬁcant.
number of dictionary words
type of matches
% of all matches
7
only in signiﬁcant messages
0.2
35
only in insigniﬁcant messages
17.0
48
both
82.8
B. Measuring per-word Classiﬁcation Power
To demonstrate the varying power of words, classiﬁcation
performances of the 48 words matching to signiﬁcant and
insigniﬁcant messages have been analysed. Here, the Matthews
correlation coefﬁcients (MCC) [20] have been computed for
each word and each of the three used algorithms. The MCC
is deﬁned as follows:
MCC =
TP × TN − FP × FN
p
(TP + FP)(TP + FN)(TN + FP)(TN + FN)
.
(1)
The classiﬁcation statistics TP (true positives), FP (false posi-
tives), TN (true negatives), FN (false positives) correspond in
this context to the following numbers:
•
TP - number of reported matches corresponding to
signiﬁcant messages
•
FP - number of reported matches corresponding to
insigniﬁcant messages
•
TN - number of cases where the algorithm indicates
no match in insigniﬁcant messages
•
FN - number of cases where the algorithm indicates
no match in signiﬁcant messages
The MCC is in the range of -1 and +1, where +1 corresponds to
perfect classiﬁcation, whereas the number of FN and FP cases
is 0. In contrast an anti-correlated performance is indicated by
an MCC of -1. Furthermore, although the MCC is a more strict
measure of classiﬁcation performance in comparison to the
classic F1-measure, it is less prone to errors introduced by class
imbalance, which is present for the majority of investigated
words. The MCC thus supplies an intuitive representation of
dictionary potency, but also provides a measure that is ought
to be maximized. The computed individual MCC values are
shown in Fig. 2a. As depicted, ten words yield MCC values
> 0.5 for any of the three methods. These words include the
two phonetically similar words Christel (a German nickname)
and Crystal, as well as endspannendes (misspelled German
word for ’anything that is relaxing’). In case of these three
17
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

words, phonetic algorithms reported matches highly correlated
to signiﬁcant messages, however, no string matches have been
identiﬁed. This is simply due to spelling mistakes made by
the conversation partners. Here, phonetic algorithms have been
able to successfully identify correspondences missed by string
matching that had been proven to be case-relevant. Further, in
case of Schokolade, a domain-speciﬁc synonym for hashish,
anti-correlation is observed, yielding string matches only to in-
signiﬁcant messages. Manual inspection of string and phonetic
matches revealed spelling differences between signiﬁcant and
insigniﬁcant messages, where Schokolade is written correctly
in the latter, leading to reported anti-correlated string matches.
In signiﬁcant messages, however, a consistent miss-spelling,
Schockolade instead of Schokolade, is present. Although there
are only two cases of positive messages, it can be proposed
that this typing error is actually made on purpose—where the
additional ’c’ could abbreviate ’cannabis’ and therefore might
encode the actual meaning of the message in addition to using
the synonym. In summary, for the majority of these 48 words
classiﬁcation performance is insufﬁciently low. Even though
string matching seems to perform superior to the utilised
phonetic algorithms in this study, no signiﬁcant performance
difference is observable (one-sided Welch test, p = 0.24 for
K¨olner phonetic, p = 0.05 for Double metaphone).
C. Measuring per-word Match Ambiguity
Finally, if a dictionary word results to two sets of matching
words in signiﬁcant and insigniﬁcant messages (as in case of
forty-eight dictionary words in this study), it is at least desir-
able to obtain two divergent, homogeneous sets of matches.
Divergence indicates that the two sets differ in matching word
composition, whereas homogeneity indicates word diversity
within the sets. Thus, both measures in combination can pro-
vide quantiﬁcations of ambiguities present between and within
both sets. In order to avoid ambiguity, the researcher aims at
increasing divergence and homogeneity. Subﬁgures 2b and c
illustrate resulting hypothetical word match scenarios for three
imaginary dictionary words (w1, w2, w3). M + and M − corre-
spond to the sets of matching words in signiﬁcant respectively
insigniﬁcant messages. For measuring word set divergence
and homogeneity, relative Kullback-Leibler divergence (KLr)
and relative Shannon entropy (Hr) are here proposed. In
general, KLr corresponds to the normalized Kullback-Leibler
divergence, which is used to measure the difference between
probability distributions P and Q. A probability distribution
for a word set M can be simply deduced by considering the
relative frequency of each word in the set of unique words UM
derived from M. Thus
P(w ∈ UM) = f(w)
|M|
(2)
is obtained. For clarity, P(M −) is denoted as Q(M −) in the
following. From this, the Kullback-Leibler divergence between
sets M + and M − can be readily computed:
KL(M +||M −) =
X
w∈UM
P(w) log2
P(w)
Q(w),
(3)
where M = M + ∪ M −. However, divergence as deﬁned
here results to a non-symmetric measure, which also not
always strictly considers all unique words in M. In case of
a given word w being only present in M +, Q(w) = 0 and
the Kullback-Leibler divergence is not deﬁned. Due to these
drawbacks, the two-sided divergence is utilised in this study
instead:
KL(M +||M −) = KL(M +||M) + KL(M −||M).
(4)
Finally, KLr can be computed by normalizing the observed
divergence using the theoretical maximal divergence
KLmax(M +||M −) = log2
 |M|
|M +|

+ log2
 |M|
|M −|

(5)
KLr(M +||M −) =
KL(M +||M −)
KLmax(M +||M −).
(6)
Using the deﬁnition of word probability given in equation (2),
Shannon entropy can be computed:
H(M) = −
X
w∈UM
P(w) log2 P(w).
(7)
In order to obtain normalized comparable quantities Hr(M),
H(M) is weighted by taking into account the maximum
theoretical entropy, leading to
Hr(M) =
H(M)
log2(|UM|).
(8)
In Fig. 2 KLr and Hr are illustrated schematically for three
word match scenarios w1, w2 and w3 (see subﬁgures b and
c), which are observed for 21 dictionary words. Here, only
dictionary words yielding minimal ambiguity (KLr > 0.4) are
considered. As illustrated, if all unique words are uniformly
distributed over M + and M −, Hr is observed to be 1
(maximal) and KLr is 0 (minimal). In this case, ambiguity
is maximal and, simply by considering matching words, no
classiﬁcation can be achieved. The corresponding dictionary
word w is thus of low classiﬁcation potency. This case is
similar to word matching scenario w3, whereas four dictionary
words considered in this study yield similar values (highlighted
in green in Fig. 2a and d). Analogously, 11 and 6 dictionary
words can be assigned to scenario w1 (blue) and w2 (red),
respectively. Also, by taking into account MCC values of
these words, the dictionary words yielding good classiﬁcation
potency can be identiﬁed. In our study, the dictionary words
bufeln, Christel, Crystal, drehen, endspannendes, Gr¨unedaum,
sechen, smoken yield a low degree of ambiguity and good clas-
siﬁcation performance. Furthermore, dictionary words yielding
anti-correlation correspond to word matching scenario w1 as
well.
In summary, these statistics can be used to visualize indi-
vidual dictionary word potency, but also to provide measures
that can aid in identifying unknown correlations and selecting
additional potent words from obtained matches, or replacing
ambiguous less potent words. In this study, it is apparent
that a majority of words provided by the dictionary are of
low potency. A large fraction of words are not sensitive to
signiﬁcant messages or provide only low classiﬁcation potency
with respect to message relevance.
18
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

V.
IDENTIFICATION OF EVIDENTIAL SHORT MESSAGES
With respect to properties of forensic short messages
discussed in Section III-B, the identiﬁcation of crime-relevant
messages within a large message history is a classiﬁcation
problem that is difﬁcult to solve by means of computational
approaches as well as manual annotation. However, by taking
into account information extracted from related conversations
instead of individual messages, automated classiﬁcation strate-
gies could be developed and applied providing the investigator
with a list of conversations, which in turn can be manually
perused, put into context, and can thus aid in the investigation
process. As a positive side effect the context of the message is
maintained in such a preprocessing strategy, which facilitates
the understanding when manually perusing obtained classi-
ﬁcation results. Thus, an automated method for identifying
individual conversations in message histories is desirable. In
this section, a statistical approach is suggested, which aims at
addressing this problem. First, the initial strategy for extracting
statistical data from message logs is elucidated and mathe-
matical formalisms are introduced. Furthermore, a statistical
measure for quantifying conversation detection performance
is introduced and applied to the proposed strategy. In this
study, the conversation identiﬁcation strategy is applied to two
drug crime-related message histories both containing manually
annotated relevant (evidential) messages, whereas one further
contains a conversation index obtained from peruse. The latter
message history is eventually used to measure identiﬁcation
performance of the proposed strategy.
A. Conversation Detection
In the context of this study, a conversation is considered
as any amount of time- and semantically-coherent messages
between at least two people. Formally expressed let M be
the set of all messages, where m ∈ M is corresponding to
any message in M. Furthermore, M is in chronological order,
creating a temporal connection between the logged messages.
Therefore, the chronology of the exchange between the con-
versational members can be tracked. In addition, the response
times (the elapsed time between two sequential messages mi
and mi+1) can be derived. The strategy presented here is
based only on derived response times and follows a simple
hypothesis: the longer the response time between two messages
mi and mi+1, the lower the likelihood that both messages
belong to the same conversation. Based on this hypothesis,
the following approaches may gradually lead to the proposed
statistical strategy:
1)
Response times follow a statistical distribution (fre-
quency distribution). Short response times are thereby
more often observed than long response times.
2)
Given a sufﬁciently large dataset and obtained re-
sponse times, a probability distribution and hence
a probability density function can be estimated em-
pirically on the basis of the observed response time
frequency distribution.
3)
Given any response time t, the relative number of
expected response times ≤ t in a chat log can be es-
timated based on the approximated density function.
4)
The reversion of the above statement leads to an
approach for solving the problem and is as follows:
for which response time t is a given fraction p of
response times with ≤ t to be expected?
5)
If p is chosen sufﬁciently small (p = 0.05 is a
common statistical threshold), a critical response time
t can be determined. Thus, it is expected that for this
particular response time 95%(1 − p) of all messages
are answered within that time period. The remainder
of 5% is negligible in accordance to p.
6)
If the response time between two messages mi and
mi+1 exceeds the critical response time, the prob-
ability for both messages belonging to the same
conversation is expected to be low. Thus, it can be
postulated that both messages do not belong to the
same conversation.
7)
M can be split into different conversations solely
from the sequence of response times with respect the
estimated critical response time.
With the general hypothesis elucidated, the underlying for-
malisms resulting to the identiﬁcation strategy are now intro-
duced. Let δt = tmi+1 − tmi be the response time between
two sequential messages of two conversation partners. Then
∆T is the set of all response times which fall within interval
(t1, t2]; thus ∆T = {δt | t1 < δt ≤ t2}. The function of all
observed frequencies hi =∥ ∆Ti ∥ parallel over time gives
a characteristic frequency distribution, which is illustrated in
Fig. 3a for a message history containing 1,550 messages. The
bin interval is 5 seconds.
In this histogram it can be seen that the frequency distri-
bution follows an exponential decay of form ae−bt. Therefore,
short response times are frequently observed. A causal relation-
ship between the observed distribution and response time can
be explained by the responsiveness of two callers. However,
this responsiveness is not constant, but varies continuously
over time. To illustrate this underlying hypothesis, let t be the
time that has elapsed since receipt of the last message. The
recipient of the last message has not yet responded to this.
So the responsiveness is Bt. It should be noted, that Bt is
corresponding to a sum of several human factors, for example
for this readiness, the duration of the call and already discussed
aspects contribute to responsiveness variance. However, a
general decrease in responsiveness can be assumed considering
a general statement: if the recipient of the last message has
seen no reason to answer at time t, readiness to continue
the conversation does not increase at a later time t + dt.
This responsiveness is formally expressed as Bt+dt. Although
numerous human factors account to variance, the statement
Bt > Bt+dt is reasonable to assume in the general case.
Thus, the responsiveness decreases tendentiously. Here it can
be postulated that a constant reduction rate r describes the
reduction of B as a function of elapsed time since receipt of
the last message. This relationship can be formulated as:
dB
dt = −rB
(9)
Hence, the more time has passed since the receipt of a message,
the lower is the probability that a response will still be sent.
Equation (9) can be readily solved (equation (10)). Thus, from
these considerations time-dependent responsiveness Bt results
from initial standby responsiveness B0 and constant reduction
rate r. This aspect describes the exponential relation shown in
19
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Fig. 3a.
Bt = B0e−rt
(10)
Furthermore, equation (10) can be understood as a probability
density function obtainable by ﬁtting parameters B0 and r to
the observed response time distribution. Optimal ﬁtting can be
determined by regression errors (see equation (11)).
{ropt,B0opt } = arg min
r,B0
X
t
|Bcalc
t
− Bobs
t
|
(11)
Based on this approximated probability density function, a
threshold time tp is calculated corresponding to a sufﬁciently
low answer probability. If this time tp is exceeded, the con-
versation is considered as terminated. This probability can be
assumed to be sufﬁciently small and is hereinafter referred to
p. In this study p = 0.05 is observed to perform well on both
considered message histories. Finally, tp can be determined
by simply integrating the approximated probability density
function and corresponding rearranging of the equation:
F(B) =
Z tp
0
B0e−rtdt = 1 − p
(12)
The red line in Fig. 3a shows the result of the performed
regression on a message history consisting of 1,550 messages.
The dashed blue line illustrates the calculated tp for which
the probability of receiving a response to a sent message is
lower than p = 0.05. For this dataset tp is 217 seconds. In
summary, proposed statistical conversation identiﬁcation re-
quires to approximate the probability density function from the
conversation-characteristic response time distribution, estimate
a critical threshold time tp for a preselected p, and ﬁnally split
M into disjunct sets of messages, whereas |tmi −tmi+1| > tp.
To check if conversations containing evidential messages
are erroneously split by this approach, their response time
distribution has been investigated. As shown in the histogram
in Fig. 3b the response time for only one relevant message
exceeds the estimated tp. Hence, all conversations contain-
ing evidential information are not falsely split in this case.
However, this single message is a solitaire, meaning it is
a single unanswered message without contextual references.
The algorithm for detecting conversations is only applied to
phone numbers at a minimum of 7 digits without any country
code or at least 10 digits with country code, because shorter
numbers mostly belong to telephone services and, therefore,
are of less interest. Furthermore, the cut-off calculation as the
ﬁrst step during the conversation detection is only considered,
if the number of messages between two conversation partners
is at least 20. To avoid extremely short and, therefore, less
meaningful conversations a cut-off value of 2 minutes is set as
a minimum. These dialogs also include questions detected via
the question mark symbol (”?”), as well as in combination
with an exclamation mark (”!”). It was analysed whether
and to which extent at these points (between question and
answer/reaction) clustering occurs and if, therefore, coherent
conversations were cut undesirably. The analysis results on the
test dataset can be seen in Table II. Based on a manual review,
it could be found out that SMSs, which contain question marks
mostly (82.69 to 100%), follow up answers or reactions from
the other participant and, therefore, can be treated as a coherent
conversation. The cut-off value itself does not change, because
the clustering/conversation detection is following the cut-off
calculation.
absolute frequency
0
100
200
300
400
500
0
20
40
60
80
100
Figure 3: a: Response time histogram of a message history
containing 1,550 messages, of which 40 were evidential for
a recently completed drug crime investigation. Detecting con-
versations (grouping of chronologically ordered messages into
disjunct sets) statistically as proposed in this study utilises a
probability density function estimation (shown by red line)
based on response time distribution. Employing a probability
threshold p gives a critical response time tp, upon which two
consecutive messages are assigned to two separate conversa-
tions if the observed response time between said messages
exceeds tp. b: Frequency distribution of 40 evidential mes-
sages. As shown, utilising a tp of 217 seconds leads to a set
of conversations in which conversations containing evidential
messages are not erroneously split by the proposed approach.
By reference to Table II it would appear that special
treatment or ﬁltering of question marks has a major impact
on the number of clusters in the test dataset. Without question
mark treatment 384 clusters arise. If the clustering is extended
by the question mark treatment it results in 332 clusters
20
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE II: Comparison of the question mark consideration in
the conversation detection of the test dataset.
without ”?” treatment
with ”?” treatment
# matches
52
0
# conversations
384
332
(13.54% or alternatively 52 less clusters than before). In view
of these results, the question mark treatment is recommended.
B. Evaluation of Conversation Identiﬁcation
Evaluation of the proposed automated approach is carried
out by comparing the similarity of the generated chat log parti-
tioning with the partitioning obtained from manual annotations
using an information theoretic approach. More precisely, given
the two partitions normalized mutual information is computed,
which indicates information coupling between both and reports
the degree of uncertainty that the partitioning obtained by
the proposed approach is meaningful with respect to manual
annotations. A partitioning of a chat log corresponds to a set
C = c1, . . . , ck of k identiﬁed conversations. Let tE(ci) be the
time elapsed during conversation ci. Trivially, the sum of all
tE equals the elapsed time between all messages. According
to this the ratio p(ci) = tE(ci)/ P(tE(cj)) corresponds to the
probability of any randomly chosen message (or any arbitrary
point of time between t1 and tn) to belong to conversation
ci. In this respect, a pair of conversation sets obtained by
manual annotation Cman and automated identiﬁcation Cauto
can be compared by means of these probabilities. Considering
manual conversation identiﬁcation to be error-free, optimal
automated conversation identiﬁcation is achieved, if Cman =
Cauto. By considering underlying probability distributions
P(Cman) and P(Cauto), the statement holds analogously true
if P(Cman) = P(Cauto). In order to evaluate the quality
of the proposed approach, both P(Cman) and P(Cauto) can
be utilised to measure set similarity. However, since both
sets are not necessarily of equal size and there is no one-to-
one correspondence between conversations in both sets, rather
straightforward measures of set similarity, such as the Jaccard
index or the previously discussed Kullback-Leibler divergence
as well as correlation analyses of conversation indexes, have
to be considered as unsuitable. Due to these constraints,
normalized mutual information (NMI) is chosen for evaluation
instead. In general mutual information quantiﬁes the emitted
information (or dependence) between two variables X and Y
by means of scaling the joint distribution P(X, Y ) of both
using the distribution of marginal probabilities P(X)P(Y ).
This measure is still applicable in case when the sizes of
both sets are unequal, and also does not rely on one-to-one
relations. Further, mutual information can readily be normal-
ized (NMI ∈ [0, 1]) using the marginal entropies H(X)
and H(Y ) to obtain comparable quantities. The maximum
value of 1 is thus observed if the discrepancy between joint
distribution and marginal distribution is maximized— which
is only if P(X) = P(Y ). With respect to P(Cman) and
P(Cauto), a maximum value of 1 is only achieved, if both
sets are equal and, hence, perfect conversation identiﬁcation
is obtained. If Cauto is simply generated by chance and
the identiﬁed conversation set is thus expected to be of low
quality, the discrepancy between both distributions is observed
to be relatively small, leading to relatively low NMI. For
conversation identiﬁcation NMI is deﬁned as
NMI(Cauto, Cman) =
2MI(Cauto, Cman)
H(Cauto) + H(Cman),
(13)
where
MI(C1, C2) =
X
ci∈C1
X
cj∈C2
p(ci ∩ cj) log2
 p(ci ∩ cj)
p(ci)p(cj)

(14)
and
H(C) = −
X
ci∈C
p(ci) log2 (p(ci)) .
(15)
p(ci ∩ cj) corresponds to the time fraction of the overlap
between two conversations.
The proposed strategy was applied to a second message
history of 2046 messages. The history was manually perused
and 116 individual conversations were identiﬁed manually. The
statistical approach utilised on this dataset yielded a tp=0.05
of 2907 seconds. The corresponding NMI was computed and
compared to NMI values resulting for critical response times tc
in the range of 30 to 30,000 seconds. This comparison provides
a robustness test for the conversation identiﬁcation obtained
from tp=0.05. As shown in Fig. 4 NMItp is within the response
time interval (2000 s - 7000 s) that yields best performance
when considering a constant critical response time as proposed
here. Note that the NMI for small critical response times
(tc < 100 seconds) of about 0.95 corresponds to a message
history-characteristic baseline performance, which is the result
of marginal correlation between Cauto and Cman in this tc
range.
tp=0.05= 2907 s
50
100
200
500
1000
2000
5000
10000
20000
tc [s]
0.93
0.94
0.95
0.96
0.97
NMI
Figure 4: NMI values computed for a message history with
available Cman determined by peruse. NMI is obtained by
deriving Cauto for each corresponding critical response time
in the range of 30 to 30,000 seconds. The critical response time
tp=0.05 computed by the proposed statistical approach (2907
seconds) is here highlighted by a green line. Conversation
identiﬁcation obtained by proposed approach is within the
critical response time range resulting to best classiﬁcation.
C. Detecting Evidential Conversations
Given the set of identiﬁed conversations C = {c0, ..., cn},
the next step is to determine which of these are signiﬁcant
regarding the object of investigation. With respect to the
insights provided in Section III-B, we utilised a bag-of-words
model combined with a domain speciﬁc dictionary d to assign
a signiﬁcance value to each conversation and hence to each
21
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

person being part of it. This signiﬁcance value S can be
calculated depending on the frequency of domain-speciﬁc
terms (see equation (16)).
Si = bag(ci, d), ∀c ∈ C
(16)
These values form the basis of a heat scale we use to colour
the contacts in the contact network established using the
report data. Fig. 5 shows the overall process. The starting
point is a contact network based on the data gathered by
Physical Analyzer [21]. Exchanged coherent messages are
subsequently clustered into conversations as proposed. The
signiﬁcance value is calculated for each of these conversations.
Based on these values suspicious contacts and communications
are highlighted visually on the contact network using the
corresponding heat scale colours via the MoNA user interface.
suspect
Physical
Analyzer
innocent
Figure 5: The process of detecting suspicious communication.
As discussed in Section IV, the determining factor for
satisfactory results is a potent dictionary. A dictionary that
comprises local language conditions, as well as terms from
different categories of offences, is currently not available (at
least in Germany). Therefore, an appropriate dictionary for
each offence category and each local cultural circle is required
to be created before calculating conversation signiﬁcance.
D. Creating the Dictionary
We started dividing the corpus into signiﬁcant and non-
suspicious parts and performing a discriminant analysis involv-
ing stop-word elimination and stemming. Considering only the
frequency classes 1 and 2 (words exclusively in suspicious
texts and words relatively more frequent in such texts) we
identiﬁed 882 ”suspicious” terms. Using these terms in turn for
processing the whole dataset for evaluation we achieve 0.98
sensitivity with 1.0 precision. Looking at the distribution of
hits, we observed that the most of them are unique. The reason
for this is due to the high number of unique spellings, caused
by syntactic and typographical errors as well as deliberate word
extensions. However, these lists of terms can form a basis for
the dictionary, especially if more than one corpus is taken into
account and words are removed according to their frequency
within all corpora.
In addition, it is useful to integrate the knowledge of local
criminalists who deal with similar cases in a similar environ-
L[(SD)(DS)]{1}
E[cd]{1}ke
LDS
LSD
Edke
Ecke
pattern
generator
Figure 6: Generating a pattern dictionary by transforming
criminalist’s knowledge.
ment every day. This experiential knowledge is the best source
of information for both, slang and hidden semantics. Such
manually added terms need to be extended automatically, for
example, by twisting letters and transforming in patterns, e. g.,
regular expressions using an appropriate pattern generator (see
Fig. 6). Current work aims at improving dictionary potency by
applying a similar bootstrapping algorithm as presented in [15]
for the ﬁeld of categorising forensic texts in general.
Bong
breit
LSD
Ecke
Bong
breit
LSD
Ecke
speech
processor
speech
processor
Figure 7: Dictionary containing pronunciation proﬁles as a
basis for matching terms with high failure tolerance.
For testing the universality of the proposed process chain
and especially the dictionary additional corpora are required.
Fortunately, due to our cooperation with the local prosecutor’s
ofﬁce additional data is provided. Finally, initial development
of an algorithm, which aims at calculating the conversation
signiﬁcance value with a high failure tolerance as shown
in Fig. 7, is currently work in progress. Here, pronunciation
proﬁles are used as a basis for understanding special terms.
VI.
PERFORMANCE OF A PROTOTYPE
The implemented MoNA prototype show an F1 score of
about 0.80 for both string matching and phonetic matching
algorithms in relevance classiﬁcation of identiﬁed conversa-
tions. However, both algorithms show opposite performance
with respect to sensitivity and recall (string matching: 1.0
sensitivity, 0.67 recall, phonetic algorithms: 0.67 sensitivity,
1.0 recall). In performance testing, a dictionary of keywords
commonly used in the local drug scene of the western Saxony
22
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

area had been provided by investigators with expert knowledge.
As demonstrated in the Word Dictionary Potency section,
coverage and potency of the provided dictionary is rather low,
which is the cause for the discrepancy in recall, respectively,
sensitivity and leaves room for improvement. Thus, future
research and studies have to focus on keyword selection, dictio-
nary development and reﬁnement. Nevertheless, the workload
for manual peruse and annotation has been reduced signiﬁ-
cantly to 15% by integrating the MoNA prototype into the
investigation process chain.
Figure 8: MoNA user interface. The communication network
is visualised and highlighted by colour in accordance to scored
conversation relevance. Contact information and message his-
tories are further reported and interactively explorable. Sensi-
tive information regarding the closed criminal investigation is
disguised.
VII.
CONCLUSION AND FUTURE WORK
Manual forensic peruse and analyses of SMS and IM
messages is a time-demanding and error-prone process. In
addition, in case of minor or moderate offences and crimes,
such forensic investigations are not justiﬁable for economical
reasons. In recent work it has been shown that automated
strategies for information retrieval and mining in message
corpora is difﬁcult to realize due to information uncertainty
and ambiguity introduced by grammatical and semantic struc-
tures usually uncommon in well-written and error-free texts.
Existing computational text analyses approaches are predom-
inantly tailored towards a clearly deﬁned semantic domain
and are employed to domain-speciﬁc corpora of semantically
and grammatically correct texts. Successful utilisation of such
techniques is thus often limited or even impossible in the
context of forensic SMS and IM message analyses.
In this work, a computational approach is proposed that
aims at reducing the amount of messages prior to manual
peruse by identifying conversations in message histories, which
might contain evidential information relevant in investigation.
This approach initially identiﬁes conversations in message
histories based on statistical analyses of the characteristic be-
haviour of text communication between participants. Individual
identiﬁed conversations are subsequently scored with respect
to predicted crime-related relevance based on a key word
dictionary deduced from practical knowledge of investigators.
This evaluation is further used in conversation reporting and
visualization within the communication network. As demon-
strated, the implemented prototype, MoNA, shows acceptable
performance in this respect. Although widely applied software
(such as Oxygen Forensics [22], XRY Physical [23] and
UFED Touch Ultimate [24]) provide valuable means for data
extraction and visualization, the process of data exploration,
annotation and peruse is still required to be conducted manu-
ally. Here, as a tool for case-based forensic semantic analyses,
MoNA could provide a valuable missing link in the process
chain. Furthermore, MoNA currently features a data interface
to process results and data derived by means of UFED soft-
ware packages. In the near future, here presented approaches
are ought to be reﬁned. Implementations of additional data
interfaces compatible with software listed above are currently
work in progress.
ACKNOWLEDGMENT
The authors would like to thank the members of the crimi-
nal investigation department and the prosecutorial Chemnitz
(Germany). We acknowledge funding by the Free State of
Saxony and the University of Applied Sciences Mittweida.
REFERENCES
[1]
M. Spranger, E. Zuchantke, and D. Labudde, “Semantic tools for
forensics: Towards ﬁnding evidence in short messages,” in Proc. 4th.
International Conference on Advances in Information Management and
Mining, IARIA.
ThinkMind Library, 2014, pp. 1–4.
[2]
K. Barmpatsalou, D. Damopoulos, G. Kambourakis, and V. Katos,
“A critical review of 7 years of mobile device forensics,” Digital
Investigation, vol. 10, no. 4, 2013, pp. 323–349.
[3]
A. Skudlark, “Characterizing SMS Spam in a Large Cellular Network
via Mining Victim Spam Reports,” International Telecommunications
Society (ITS) Biennial Conference, Tech. Rep., December 2014.
[4]
I. Ahmed, D. Guan, and T. C. Chung, “Sms classiﬁcation based on na¨ıve
bayes classiﬁer and apriori algorithm frequent itemset,” International
Journal of Machine Learning and Computing, vol. 4, no. 2, April 2014,
pp. 183–187.
[5]
Q. Xu, E. W. Xiang, Q. Yang, J. Du, and J. Zhong, “Sms spam
detection using noncontent features,” IEEE Intelligent Systems, Novem-
ber/December 2012, pp. 44–51.
[6]
D. G. A. Al-Talib and H. S. Hassan, “A study on analysis of sms
classiﬁcation using tf-idf weighting,” International Journal of Computer
Networks and Communications Security, vol. 1, no. 5, October 2013,
pp. 189–194.
[7]
M. B. Deepshikha Patel, “Mobile sms classiﬁcation: An application
of text classiﬁcation,” International Journal of Soft Computing and
Engineering, vol. 1, no. 1, March 2011, pp. 47–49.
[8]
S. Ishihara, “A forensic authorship classiﬁcation in sms messages:
A likelihood ratio based approach using n-gram,” in Proceedings of
the Australasian Language Technology Association Workshop 2011,
Canberra, Australia, December 2011, pp. 47–56. [Online]. Available:
http://www.aclweb.org/anthology/U/U11/U11-1008
[9]
T. Chen and M.-Y. Kan, “Creating a live, public short message service
corpus: the nus sms corpus,” Language Resources and Evaluation,
vol. 47, no. 2, 2013, pp. 299–335.
[10]
D. H. W. Dannis Muhammad Mangan, “Information extraction from
short text message in bahasa indonesia for electronics,” Jurnal Sarjana
Institut Teknologi Bandung bidang Teknik Elektro dan Informatika,
vol. 1, no. 1, April 2012, pp. 29–32.
[11]
S. Cooper, R.L.and Manson, “Extracting temporal information from
short messages,” in British National Conference on Databases, Glasgow,
July 2007, LNCS 4587.
LNCS, Springer, 2007, pp. 224–234.
23
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[12]
K. Nebhi, “Ontology-based information extraction from twitter,” in
Proceedings of the Workshop on Information Extraction and Entity
Analytics on Social Media Data.
Mumbai, India: The COLING
2012 Organizing Committee, December 2012, pp. 17–22. [Online].
Available: http://www.aclweb.org/anthology/W12-5502
[13]
VATM,
“Number
of
SMS
and
MMS
sent
in
Germany
from
1999
to
2014*
(in
millions
per
day),”
2016,
URL:
http://www.statista.com/statistics/461700/number-of-sms-and-mms-
sent-per-day-germany/ [accessed: 2016-01-03].
[14]
G. Evans and V. Gosalia, “The coming storm: Companies must be
prepared to deal with text messages on employee mobile devices,”
Digital Discovery & e-Evidence, 2015.
[15]
M. Spranger and D. Labudde, “Semantic tools for forensics: Approaches
in forensic text analysis,” in Proc. 3rd. International Conference on
Advances in Information Management and Mining (IMMM), IARIA.
ThinkMind Library, 2013, pp. 97–100.
[16]
R. Cooper and S. Ali, “Extracting data from short messages,” in Natural
Language Processing and Information Systems, LNCS 3513.
LNCS,
Springer, 2005, pp. 388–391.
[17]
E. Riloff, “Automatically constructing a dictionary for information
extraction tasks,” in Proceedings of the Eleventh National Conference
on Artiﬁcial Intelligence, ser. AAAI’93.
AAAI Press, 1993, pp. 811–
816.
[18]
H.-J. Postel, “Die K¨olner Phonetik - Ein Verfahren zur Identiﬁzierung
von Personennamen auf der Grundlage der Gestaltanalyse.” IBM-
Nachrichten, vol. 19, 1969, pp. 925–931.
[19]
L. Philips, “The Double Metaphone Search Algorithm.” C/C++ Users
Journal, vol. 18, no. 6, 2000, pp. 925–931.
[20]
B. W. Matthews, “Comparison of the predicted and observed secondary
structure of t4 phage lysozyme.” Biochim Biophys Acta, vol. 405, no. 2,
Oct 1975, pp. 442–451.
[21]
Cellebrite Mobile Synchronization LTD. UFED Physical Analyzer -
Mobile Daten ermitteln, dekodieren und bereitstellen. [Online]. Avail-
able:
http://www.cellebrite.com/Mobile-Forensics/Applications/ufed-
physical-analyzer [accessed: 2016-03-01]
[22]
Oxygen
Forensics,
Inc.
Oxygen
Forensics.
[Online].
Available:
http://www.oxygen-forensic.com/de/ [accessed: 2016-03-01]
[23]
MSAB.
XRY
Physical.
[Online].
Available:
https://www.msab.com/products/xry/#physical [accessed: 2016-03-01]
[24]
Cellebrite
Mobile
Synchronization
LTD.
UFED
Touch
-
Eine
hochleistungsf¨ahige
L¨osung
f¨ur
hochleistungsf¨ahige
Ger¨ate.
[Online].
Available:
http://www.cellebrite.com/de/Mobile-
Forensics/Products/ufed-touch [accessed: 2016-03-01]
24
International Journal on Advances in Security, vol 9 no 1 & 2, year 2016, http://www.iariajournals.org/security/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

