Expected Annual Fraction of Entity Loss as a Metric
for Data Storage Durability
Ilias Iliadis
IBM Research Europe – Zurich
8803 R¨uschlikon, Switzerland
email: ili@zurich.ibm.com
Abstract—Data protection schemes are designed into storage
systems to ensure data durability and accessibility in the presence
of device failures. The effectiveness of these schemes has been
evaluated based on the Mean Time to Data Loss (MTTDL) and
the Expected Annual Fraction of Data Loss (EAFDL) metrics. The
EAFDL metric assesses data loss at a low data processing unit
level whereas durability refers to losses at a higher entity, say file
or object, level. To evaluate the durability of storage systems we
introduce the following reliability metric: the Expected Annual
Fraction of Entity Loss (EAFEL), that is, the fraction of entities
that is expected to be lost by the system annually. The general
methodology that was applied to assess the MTTDL and EAFDL
metrics is extended to obtain the EAFEL metric analytically
for erasure-coding redundancy schemes and for the clustered,
declustered, and symmetric data placement schemes. The theo-
retical model developed considers the effects of device failures,
latent errors, and lazy rebuilds. For realistic values of sector
error rates, the results obtained demonstrate that MTTDL and
EAFEL degrade, but the EAFEL degradation is more pronounced
when entities are large. It is also shown that the declustered data
placement scheme offers superior reliability.
Keywords–Storage; Unrecoverable or latent sector errors; De-
ferred recovery or repair; Reliability analysis; MTTDL; EAFDL;
RAID; MDS codes; stochastic modeling.
I.
INTRODUCTION
Data protection schemes are designed into storage systems
to ensure data durability and accessibility in the presence of
device and component failures [1-8]. The replication schemes
and the Redundant Arrays of Inexpensive Disks (RAID)
schemes, such as RAID-5 and RAID-6, which have been
employed extensively in the past thirty years [1-4] are special
cases of erasure codes. The reliability of storage systems is
adversely affected by the latent or unrecoverable sector errors
that are discovered when there is an attempt to access these
sectors. Permanent losses of data due to latent errors are quite
pronounced in higher-capacity hard-disk drives (HDDs) and
storage nodes [9-11]. To cope with the repair problem due to
the increased network traffic needed to repair data lost due to
device failures a lazy rebuild scheme was proposed in [12]
to reduce the amount of data transmitted during rebuilds. The
reliability level achieved by this scheme was assessed in [13].
The Mean Time to Data Loss (MTTDL) metric has
been widely used to assess the reliability of storage sys-
tems [14][15]. For a comprehensive reliability evaluation, it
is imperative to assess not only the frequency of data loss
events, which is obtained by the MTTDL metric, but also the
amount of data loss, which is captured by the Expected Annual
Fraction of Data Loss (EAFDL) metric [16-21]. The EAFDL
metric is meant to complement, not to replace the traditional
MTTDL metric, as these two metrics provide a useful profile
of the magnitude and frequency of data losses.
The MTTDL and EAFDL metrics have been evaluated in
parallel in a common general theoretical framework using the
direct-path-approximation methodology presented in [3, 22,
23], which, for highly reliable storage devices, enables the
derivation of analytic expressions that accurately assess the
reliability metrics of interest [16][22][24]. This methodology
does not involve Markovian analysis and holds for general
device failure time distributions, which can be exponential or
non-exponential, such as Weibull and gamma.
The EAFDL metric was introduced in [16], because Ama-
zon S3 [25], Facebook [26], LinkedIn [27] and Yahoo! [28]
consider the amount of lost data measured in time. However,
system durability reflects losses at an entity, say file or object,
level whereas the EAFDL metric assesses data losses at a lower
data processing unit level. For this reason, in this article we
introduce the following reliability metric: the Expected Annual
Fraction of Entity Loss (EAFEL), that is, the fraction of entities
that is expected to be lost by the system annually.
The key contributions of this article are the following. The
general non-Markovian methodology that was applied in prior
work to assess the MTTDL and EAFDL metrics is extended
to obtain the new EAFEL metric analytically for erasure-
coding redundancy schemes and for the clustered, declustered,
and symmetric data placement schemes. The validity of this
methodology for accurately assessing the reliability of storage
systems has been confirmed by simulations in several con-
texts [3][16][22][29]. It has been demonstrated that theoretical
predictions of the reliability of systems comprising highly
reliable storage devices are in good agreement with simulation
results. Consequently, the emphasis of the present work is
on theoretically assessing the effect of the entity size on the
reliability of storage systems. The theoretical model developed
considers the effects of device failures, latent errors, lazy
rebuilds, and can also be used to assess system reliability when
scrubbing is employed [14]. We subsequently use these results
to demonstrate the effect of latent errors and system parameters
on system reliability.
The remainder of the article is organized as follows.
Section II describes the storage system model and the cor-
responding parameters considered. Section III presents the
general framework and methodology for deriving the MTTDL
and EAFEL metrics analytically for the case of erasure-
coded systems that employ a lazy rebuild scheme. Closed-
form expressions for relevant reliability metrics are derived
for the symmetric, clustered, and declustered data placement
schemes. Section IV presents numerical results demonstrating
1
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-040-7
CTRQ 2023 : The Sixteenth International Conference on Communication Theory, Reliability, and Quality of Service

TABLE I.
NOTATION OF SYSTEM PARAMETERS
Parameter
Definition
n
number of storage devices
c
amount of data stored on each device
l
number of user-data symbols per codeword (l ≥ 1)
m
total number of symbols per codeword (m > l)
(m, l)
MDS-code structure
d
lazy rebuild threshold (0 ≤ d < m − l)
s
symbol size
k
spread factor of the data placement scheme, or
group size (number of devices in a group) (m ≤ k ≤ n)
J
number of codewords per entity (J ≥ 1)
b
average reserved rebuild bandwidth per device
Bmax
upper limitation of the average network rebuild bandwidth
X
time required to read (or write) an amount c of data at an average
rate b from (or to) a device
FX(.)
cumulative distribution function of X
Fλ(.)
cumulative distribution function of device lifetimes
Pbit
probability of an unrecoverable bit error
seff
storage efficiency of redundancy scheme (seff = l/m)
U
amount of user data stored in the system (U = seff n c)
˜r
MDS-code distance: minimum number of codeword symbols lost
that lead to permanent data loss
(˜r = m − l + 1 and 2 ≤ ˜r ≤ m)
C
number of symbols stored in a device (C = c/s)
NE
number of entities (NE = (n C)/(m J))
µ−1
mean time to read (or write) an amount c of data at an average rate
b from (or to) a device (µ−1 = E(X) = c/b)
λ−1
mean time to failure of a storage device
(λ−1 =
R ∞
0 [1 − Fλ(t)]dt)
Ps
probability of an unrecoverable sector (symbol) error
PDL
probability of data loss during rebuild
PUF
probability of data loss due to unrecoverable failures during rebuild
PDF
probability of data loss due to a disk failure during rebuild
Y
number of lost entities during rebuild
G
number of lost entities, given that data loss has occurred during
rebuild
the effectiveness of erasure coding schemes for improving
system reliability as well as the adverse effect of increased
entity sizes. Finally, we conclude in Section V.
II.
STORAGE SYSTEM MODEL
Here, we briefly review the operational characteristics of
erasure-coded storage systems. To assess their reliability, we
adopt the model used in [13] and extend it to cover the case
of entity rebuilds. The storage system comprises n storage
devices (nodes or disks), where each device stores an amount
c of data such that the total storage capacity of the system
is n c. This does not account for the spare space used by the
rebuild process.
User data contained in entities is divided into blocks (or
symbols) of a fixed size s (e.g., sector size of 512 bytes)
and complemented with parity symbols to form codewords,
as shown in Figure 1. Maximum Distance Separable (MDS)
erasure codes (m, l) that map l user-data symbols to codewords
of m symbols are employed. They have the property that any
subset containing l of the m codeword symbols can be used to
reconstruct (recover) a codeword. The corresponding storage
efficiency seff and amount U of user data stored in the system
is
seff = l/m
and
U = seff n c = l n c/m .
(1)
Also, the number C of symbols stored in a device is
C = c/s ,
(2)
and, therefore, the number of symbols stored in the system
is n C and the number of codewords stored in the system is
n C/m. Assuming fixed-size entities with each one containing
J codewords, an entity contains J l user-data symbols and
J (m − l) parity symbols for a total of J m symbols. Conse-
quently, the entity size (including parity symbols) is J m s and
the number NE of entities in the system is
NE = n
m · C
J
(2)
=
n
m · c
J s .
(3)
Our notation is summarized in Table I. The derived pa-
rameters are listed in the lower part of the table. Two different
ways (A and B) for storing the J l user-data symbols of an
entity in J codewords are shown in Figure 1. Note that Sj,i
denotes the ith symbol of the jth codeword. Thus, S1,2, which
is the second symbol of codeword C-1, in the case of A is
the second symbol of the first chunk, whereas in the case of
B is the first symbol of the second shard. In the case of A,
successive symbols of a chunk are stored on different devices
whereas in the case of B, successive symbols of a shard are
stored on the same device. To minimize the risk of permanent
data loss, the m symbols of each of the J codewords are spread
and stored successively in a set of m devices that constitute a
placement group. This way, the system can tolerate any ˜r − 1
device failures, but ˜r device failures may lead to data loss,
with
˜r = m − l + 1 ,
1 ≤ l < m
and
2 ≤ ˜r ≤ m .
(4)
Examples of MDS erasure codes are the replication, RAID-
5, and RAID-6 schemes. In terms of encoding operations,
MDS erasure codes are either bitwise exclusive-OR (XOR)
or non-XOR. The computation complexity of the non-XOR-
based codes, such as Reed–Solomon, is much higher than that
of the XOR-based ones. Additional categories of erasure codes
are the Minimum Bandwidth Regenerating (MBR) codes that
aim to reduce the amount of data transferred over the storage
network during rebuilds, and the Minimum Storage Regener-
ating (MSR) codes that aim to reduce storage overheads [30].
The system comprises n/k groups of k devices. These
groups are disjoint in that codewords are stored within, but
not across groups. Within each group, the number of placement
groups is at most

Figure 1.
Formation of codewords from entity data contents and their placement on storage devices. Two methods (A and B) for identical formation of
codewords. Clustered and declustered placement of codewords of length m = 3 on n = 6 devices. X1, X2, X3 represent codewords of an entity (X = A, B, C,
. . . , L).
A. Rebuild Process for Entity and Codeword Reconstruction
When storage devices fail, codewords lose some of their
symbols, and this reduces data redundancy. The system at-
tempts to maintain its redundancy by reconstructing the lost
codeword symbols using the surviving symbols of the affected
codewords. As the times to detect device failures are much
shorter than rebuild times, we assume that failures are detected
instantaneously. When a lazy rebuild scheme is used, the
rebuild process is not triggered immediately, but is delayed
until additional device failures occur that result in d additional
symbol losses within some of the codewords. Consequently,
the rebuild process is initiated when codewords have lost
1+d symbols. To avoid permanent data losses, the number of
symbols lost within codewords should be less than the MDS-
code distance ˜r, that is, this number should not exceed ˜r − 1,
which implies that d + 1 ≤ ˜r − 1 = m − l. Thus, we have
l < m ≤ k ≤ n
(n/k ∈ N)
and
0 ≤ d ≤ m − l − 1 . (5)
When a codeword of an entity loses ˜r or more symbols,
this codeword, and consequently the entity is permanently lost.
Note that the reconstruction of successive codewords leads to
the successive reconstruction of entities.
1) Exposure Levels: The system is at exposure level u (0 ≤
u ≤ ˜r) when there are codewords that have lost u symbols
owing to device failures, but there are no codewords that have
lost more symbols. These codewords are referred to as the
most-exposed codewords. Transitions to higher exposure levels
are caused by device failures, whereas transitions to lower ones
are caused by successful rebuilds. We denote by Cu the number
of most-exposed codewords upon entering exposure level u,
(u ≥ 1). Upon the first device failure it holds that
C1 = C ,
(6)
where C is determined by (2). The reliability metrics of interest
are derived in Section III using the direct path approximation,
which considers only transitions from lower to higher exposure
levels [3][16][22][23][29]. This implies that each exposure
level is entered only once.
2) Prioritized Lazy Rebuild: When a symmetric or declus-
tered placement scheme is used, as shown in Figure 2 of
[18][20][21], spare space is reserved on each device for
temporarily storing the reconstructed codeword symbols before
they are transferred to a new replacement device. The rebuild
process to restore the data lost by failed devices is assumed to
be both prioritized and distributed. A prioritized (or intelligent)
rebuild process always attempts first to rebuild the most-
exposed codewords, namely, the codewords that have lost the
largest number of symbols [3][5][7][12][18][29]. According
to the lazy rebuild scheme, no recovery actions are performed
at exposure levels u not exceeding the threshold d. However,
3
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-040-7
CTRQ 2023 : The Sixteenth International Conference on Communication Theory, Reliability, and Quality of Service

when the system enters a higher exposure level u, the rebuild
process is triggered and attempts to bring the system back to
exposure level u − 1 by reading l symbols and recovering one
of the u symbols that each of the Cu most-exposed codewords
has lost. In a distributed rebuild process, the codewords are
reconstructed by reading symbols from an appropriate set of
surviving devices and storing the recovered symbols in the
reserved spare space of these devices. During this process, the
lost codeword symbols of successive entities are reconstructed
on other appropriately chosen placement groups, which ensures
that the lost codeword symbols are reconstructed on devices
in which another symbol of the same codeword is not already
present. In the case of clustered placement, the lost symbols
are reconstructed directly in spare devices as described and
shown in Figure 3 of [18]. As an entity contains J codewords,
it is assumed that the number Eu of entities associated with
the Cu codewords is approximately Cu/J.
3) Rebuild Process: A certain portion of the device band-
width is reserved for read/write data recovery during the
rebuild process, and the remaining bandwidth is used to serve
user requests. Let b denote the actual average reserved rebuild
bandwidth per device. Lost symbols are rebuilt in parallel using
the rebuild bandwidth b available on each surviving device. The
amount of data corresponding to the number Cu of symbols
to be rebuilt at exposure level u is written at an average rate
bu (≤ b) to selected device(s). For the time X required to read
(or write) an amount c of data from (or to) a device it holds
that
µ−1 ≜ E(X) = c/b .
(7)
4) Failure and Rebuild Time Distributions: The lifetimes
of the n devices are assumed to be independent and identically
distributed, with a cumulative distribution function Fλ(.) and a
mean of 1/λ. The results in this article hold for highly reliable
storage devices, which satisfy the condition [18][22]
µ
Z ∞
0
Fλ(t)[1 − FX(t)]dt ≪ 1,
with λ
µ ≪ 1 .
(8)
5) Amount of Data to Rebuild and Rebuild Times at Each
Exposure Level: We denote by ˜nu the number of devices
at exposure level u whose failure causes an exposure level
transition to level u + 1, and Vu the fraction of the Cu most-
exposed codewords that have a symbol stored on any given
such device. Note that ˜nu depends on the codeword placement
scheme. Let Ru denote the rebuild time of the most-exposed
codewords at exposure level u, and αu be the fraction of the
rebuild time Ru still left when another device fails, causing
the exposure level transition u → u + 1. For u ≤ d, no
rebuild is performed and therefore αu = 1. For u > d, αu
is approximately uniformly distributed in (0, 1) [31, Lemma
2]. Therefore,
αu ≊
 1 ,
for u = 1, . . . , d
U(0, 1) ,
for u = d + 1, . . . , ˜r − 1 .
(9)
We proceed by considering that the rebuild time Ru+1 is
determined completely by Ru and αu in the same manner as in
[13][17][18][23]. For the rebuild schemes considered, it holds
that [13, Eq. (10)]:
Cu ≈ C
u−1
Y
i=1
Vi αi , for u = 1, . . . , ˜r ,
(10)
with the convention that for any integer j and for any sequence
δi , Q0
i=j δi ≜ 1. Unconditioning (10) on α1, . . . , αu−1 yields
E(Cu) = C


u−1
Y
j=1
Vj

E


u−1
Y
j=1
αj
 level u was entered

 ,
for u = 1, . . . , ˜r .
(11)
It also holds that [13, Eq. (49)]:
Rd+1 ≈


d
Y
j=1
Vj


b
bd+1
X .
(12)
6) Unrecoverable Errors: The reliability of storage systems
is affected by the occurrence of unrecoverable or latent errors.
According to the specifications of enterprise quality HDDs, the
unrecoverable bit-error probability Pbit is equal to 10−15 [14].
Assuming that bit errors occur independently over successive
bits, the unrecoverable sector error probability Ps is
Ps = 1 − (1 − Pbit)s ,
(13)
with s expressed in bits. For a sector size of 512 bytes, we have
Ps ≈ Pbit × 4096 = 4.096 × 10−12. In practice, however, and
also owing to the accumulation of latent errors over time, these
probability values are higher. Recent investigations show that
Pbit can be orders of magnitude higher, reaching Pbit ≈ 10−12
[32], which results in Ps ≈ 4.096 × 10−9. Moreover, latent
errors are found to exhibit spatial locality and they occur in
bursts of multiple contiguous sector errors. The mean burst
length ¯B reported in [21] is 1.0291. In the next section, we
show that the correlation of latent errors affects both MTTDL
and EAFEL reliability metrics.
III.
DERIVATION OF EAFEL
The EAFEL reliability metric is derived using the direct-
path-approximation methodology presented in [16-21]. At any
point in time, the system is in one of two modes: non-
rebuild or rebuild mode. Note that part of the non-rebuild
mode is the normal mode of operation where all devices
are operational and all data in the system has the original
amount of redundancy. In the context of lazy rebuild, when the
first device fails, the system does not enter the rebuild mode.
Subsequently, we refer to the device failure that causes the
transition from non-rebuild to rebuild mode as an initial device
failure, which should not be confused with the first device
failure. Consequently, an initial device failure triggers a rebuild
process that attempts to restore the lost data, which eventually
leads the system either to a Data Loss (DL) with probability
PDL or back to the original normal mode by restoring initial
redundancy, with probability 1 − PDL.
Let T be a typical interval of a non-rebuild period, that is,
the time interval from the time the system is brought to its
original state until a subsequent initial device failure occurs
that causes the system to enter exposure level d + 1. It then
holds that T = Pd
u=0 Tu, where T0 denotes the time interval
from the time the system is brought to its original state until
the first device failure, and Tu denotes the time that the system
spends at exposure level u. For a system comprising n devices
with a mean time to failure of a device equal to 1/λ, it holds
that E(T0) = 1/(n λ) [22]. Given that the number of devices
at exposure level u whose failure causes an exposure level
4
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-040-7
CTRQ 2023 : The Sixteenth International Conference on Communication Theory, Reliability, and Quality of Service

transition to level u+1 is ˜nu, it holds that E(Tu) = 1/(˜nu λ).
From the above, it follows that
E(T) =
d
X
u=0
E(Tu) =
 d
X
u=0
1
˜nu
! 
λ ,
where ˜n0 ≜ n ,
(14)
where ˜nu is determined by (58) or (61).
The MTTDL metric is then obtained by [16, Eq. (5)]:
MTTDL ≈ E(T)
PDL
.
(15)
The EAFEL is obtained as the ratio of the expected number
E(Y ) of lost entities, normalized to the number NE of entities,
to the expected duration of T:
EAFEL ≈
E(Y )
E(T) · NE
(3)
= m J s E(Y )
n c E(T)
.
(16)
where E(T) is determined by (14) and expressed in years.
The expected number E(G) of lost entities, given that data
loss has occurred, is determined by
E(G) = E(Y )
PDL
.
(17)
It follows from (15), (16), and (17) that
EAFEL =
E(G)
MTTDL · NE
,
(18)
with the MTTDL expressed in years.
A. Reliability Analysis
The EAFEL is evaluated in parallel with MTTDL using the
theoretical framework presented in [13]. At any exposure level
u (u = d + 1, . . . , ˜r − 1), data loss may occur during rebuild
owing to one or more unrecoverable failures, which is denoted
by the transition u → UF. Moreover, at exposure level ˜r − 1,
data loss occurs owing to a subsequent device failure, which
leads to the transition to exposure level ˜r. Consequently, the
direct paths that lead to data loss are the following:
−−→
UFu : the direct path of successive transitions 1 → 2 →
· · · → u → UF, for u = d + 1, . . . , ˜r − 1, and
−−→
DF : the direct path of successive transitions 1 → 2 →
· · · → ˜r − 1 → ˜r,
with their respective probabilities PUFu and PDF determined by
virtue of Proposition 1 of [13] and Eq. (32) of [21] as follows:
PUFu
≈ −

λ c
d
Y
j=1
Vj


u−d−1
E(Xu−d−1)
[E(X)]u−d−1
 u−1
Y
i=d+1
˜ni
bi
V u−1−i
i
!
· log(ˆqu)−(u−d−1)
 
ˆqu −
u−d−1
X
i=0
log(ˆqu)i
i!
!
,
(19)
where
ˆqu ≜ q
fcor C Qu−1
j=1 Vj
u
,
(20)
qu = 1 −
m−u
X
j=˜r−u
m − u
j

P j
s (1 − Ps)m−u−j ,
(21)
fcor ≜
 1 ,
for independent latent errors
1
¯
B ,
for correlated latent errors, and
(22)
PDF ≈
(λ c Qd
j=1 Vj)˜r−d−1
(˜r − d − 1)!
E(X ˜r−d−1)
[E(X)]˜r−d−1
˜r−1
Y
i=d+1
˜ni
bi V ˜r−1−i
i
.
(23)
Remark 1: Independently of threshold d, it holds that [21]
qu ≈





1−
 
m − u
˜r − u
!
P ˜r−u
s
, for Ps ≪
1
m−u <

The MTTDL metric is obtained by substituting (32) into
(15) as follows:
MTTDL ≈
E(T)
PDF + P˜r−1
u=d+1 PUFu
,
(34)
where E(T), PUFu and PDF are determined by (14), (19), and
(23), respectively. Note that PDL and, consequently, MTTDL
do not depend on the entity size.
We proceed to derive the number of lost entities during
rebuild. Let Y and G be the number of lost entities and
the conditional number of lost entities, given that data loss
has occurred, respectively. Let also YDF and YUFu denote the
number of lost entities associated with the direct paths −−→
DF and
−−→
UFu, respectively. Similarly, we consider the variables GDF
and GUFu. Then, the number Y of lost entities is obtained by
Y ≈





GDF ,
if −−→
DF
GUFu ,
if −−→
UFu , for u = d + 1, . . . , ˜r − 1
0 ,
otherwise .
(35)
Thus,
E(Y ) ≈ PDF E(GDF) +
˜r−1
X
u=d+1
PUFu E(GUFu) (36)
= E(YDF) +
˜r−1
X
u=d+1
E(YUFu) ,
(37)
≈ E(YDF) + E(YUF) ,
(38)
where
E(YDF) = PDF E(GDF) ,
(39)
E(YUFu) = PUFu E(GUFu) , u = d + 1, . . . , ˜r − 1 ,
(40)
E(YUF) = PUF E(GUF) ≈
˜r−1
X
u=1
E(YUFu) ,
(41)
where YUF denotes the number of lost entities due to unrecov-
erable failures and GUF the conditional number of lost entities,
given that data loss due to unrecoverable failures has occurred.
Proposition 1: For u = d + 1, . . . , ˜r − 1, it holds that
E(YUFu) ≈ C
J
Pu
u − d


u−1
Y
j=1
Vj

 ˜qu ,
(42)
where
˜qu ≜ 1 − q fcor J
u
,
(43)
E(YDF) ≈ C
J
PDF
˜r − d
˜r−1
Y
j=1
Vj ,
(44)
and Pu is determined by (29).
Proof: Equation (42) is obtained in Appendix A. Equation
(44) is obtained from (42) by setting u = ˜r and recognizing
that q˜r = 0, ˜q˜r = 1, and P˜r = PDF.
Remark 2: It follows from (24) and (43) that
˜qu ≈





fcor J
 
m − u
˜r − u
!
P ˜r−u
s
,
for Ps ≪ [ fcor J

From (11) and (54), it follows that
E


˜r−1
Y
j=1
αj
 level ˜r was entered

 ≈
1
˜r − d .
(55)
Remark 5: It turns out that when a data loss has occurred,
the variables αd+1, . . . , α˜r−1 are not distributed identically.
More specifically, for a rebuild time Ru, the uniform distribu-
tion of αu in the interval (0, 1), given by (9), holds under the
assumption that there is a failure during this rebuild period,
that is, an exposure level transition u → u + 1. However,
conditioning on the exposure level transitions u → u + 1 →
· · · → u′ → u′ + 1 (u′ > u), αu is no longer uniformly
distributed in (0, 1). This is due to the fact that, conditioning
on the fact that additional failures occur during the rebuild
times Ru+1, . . . , Ru′, it is more likely that the Ru+1 period
is long rather than short. In this case, only α′
u is uniformly
distributed in (0, 1). Assuming that the system has entered
exposure level u, we deduce from (55) that
E


u−1
Y
j=1
αj
 level u was entered

≈
1
u − d , u = d+1, . . . , ˜r.
(56)
Substituting (56) into (11) and using (9) yields
E(Cu) ≈
 u−1
Y
i=1
Vi
!
C
max(u − d, 1) ,
u = 1, . . . , ˜r . (57)
Remark 6: From (42) and considering (57), it holds that
E(YUFu) ≈ [E(Cu)/J ˜qu] Pu, that is, the expected number of
lost entities at exposure level u is the product of the expected
number of lost entities at exposure level u, given that the
system has entered exposure level u, and the probability of
entering exposure level u. Similarly, from (44) and considering
(54), it holds that E(YDF) ≈ [E(C˜r)/J ] PDF, given that at
exposure level ˜r, all of the E(C˜r) exposed codewords and
corresponding E(C˜r)/J entities are lost.
B. Symmetric and Declustered Placement
We consider the case m < k ≤ n. The special case k = m
corresponding to the clustered placement has to be considered
separately for the reasons discussed in Section II-A2. At each
exposure level u, for u = 1, · · · , ˜r − 1, it holds that [17][18]
˜nsym
u
= k − u ,
(58)
bsym
u
= min((k − u) b, Bmax)
l + 1
,
(59)
V sym
u
= m − u
k − u .
(60)
The corresponding parameters ˜ndeclus
u
, bdeclus
u
, and V declus
u
for the declustered placement are derived from (58), (59), and
(60) by setting k = n.
C. Clustered Placement
At each exposure level u, for u = 1, · · · , ˜r − 1, it holds
that [17][18]
˜nclus
u
= m − u , bclus
u
= min( b , Bmax/l ) , V clus
u
= 1 . (61)
Remark 7: From (42), (44), (19), and (23), and considering
expressions (58) through (61), it follows that E(YUFu) and
PUFu are mainly determined by the term (λ c/b)u−d−1, and
E(YDF) and PDF by the term (λ c/b)˜r−d−1. According to (8),
λ c/b ≪ 1, such that, for fixed values of ˜r and u, increasing
d causes these parameters to increase. Therefore, by virtue
of (34) and (51), increasing d causes EAFEL to increase and
MTTDL to decrease. Consequently, for fixed values of m and
l, deferring rebuilds degrades reliability.
D. Equivalent Systems
We call equivalent systems those that employ a given
codeword length m and have the same number m − l − d
of exposure levels at which the rebuild process is active. In
this case, it holds that l + d = z, and from (4) and (5), it
follows that
0 ≤ d < z < m
and
d + 1 ≤ u ≤ m − z + d + 1 .
(62)
Next, we compare the EAFEL of equivalent systems. From
(42) and (45), it follows that
E(YUFu+1 |d+1)
E(T |d+1)
E(YUFu |d)
E(T |d)
≈
E(T|d)
E(T|d + 1) ·
u
Y
i=d+1
Vi · z − d − 1
m − u
· A ,
(63)
where
A =
(
(
z−d
z−d+1)u−d−1 ,
for symmetric placement
m−u
m−d−1 ,
for clustered placement .
(64)
From (14), it follows that
E(T |d)
E(T |d+1) < 1. Consequently,
from (51), (62), and (63), and recognizing that A < 1 and
E(YDF) = E(YUF˜r), it follows that
EAFEL(d + 1)
EAFEL(d)
< 1 .
(65)
It also holds that [13, Eqs. (46) and (47)]:
MTTDL(d + 1)
MTTDL(d)
> 1 .
(66)
Remark 8: Within the class of equivalent systems, accord-
ing to (65) and (66), deferring rebuilds improves reliability,
despite the fact that rebuilds are performed at the same number
of exposure levels. This is because increasing d amounts to
decreasing l, and therefore at a reduced number of symbols
read at each exposure level. This in turn results in reduced
vulnerability window and therefore improved reliability.
IV.
NUMERICAL RESULTS
Here, we assess the reliability of the clustered and declus-
tered schemes for a system comprised of n = 64 devices
(disks) and protected by an erasure coding scheme with m =
16, which is the codeword length used by Microsoft® Azure
[7], and l = 13, 14, and 15. Each device stores an amount of
c = 20 TB, which is the capacity of the latest generation of
Seagate drives, and the symbol size s is equal to a sector size
of 512 bytes [33].
Typical parameter values are listed in Table II. The An-
nualized Failure Rate (AFR) of HDDs for the year 2021 is
in the range of 0.11% to 4.79% [34], which corresponds to a
mean time to failure in the range of 180, 000 h to 8, 000, 000
h. The parameter λ−1 is chosen to be equal to 876, 000 h (100
years) that corresponds to an AFR of 1%, which is the average
AFR across all drive models [34]. Considering that 35% of
the maximum transfer rate of 285 MB/s [33] is allocated for
7
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-040-7
CTRQ 2023 : The Sixteenth International Conference on Communication Theory, Reliability, and Quality of Service

(a) k = 64 (declustered data placement scheme)
(b) k = 16 (clustered data placement scheme)
Figure 2.
Normalized MTTDL vs. Ps for various MDS(m, l, d) codes; n = 64, λ/µ = 0.00006, c = 20 TB, and s = 512 B.
TABLE II.
TYPICAL VALUES OF DIFFERENT PARAMETERS
Parameter
Definition
Values
n
number of storage devices
64
c
amount of data stored on each device
20 TB
s
symbol (sector) size
512 B
λ−1
mean time to failure of a storage device
876,000 h
b
rebuild bandwidth per device
100 MB/s
m
symbols per codeword
16
l
user-data symbols per codeword
13, 14, 15
U
amount of user data stored in the system
1.04 to 1.2 PB
µ−1
time to read an amount c of data at a rate
b from a storage device
55.5 h
recovery operations, the reserved rebuild bandwidth b is then
equal to 100 MB/s, which yields a rebuild time of a device
µ−1 = c/b = 55.5 h. Also, it is assumed that the maximum
network rebuild bandwidth is sufficiently large (Bmax ≥ n b =
6.4 GB/s), that the rebuild time distribution is deterministic,
such that E(Xk) = [E(X)]k, and that sector errors are
correlated with ¯B ≈ 1, which implies that fcor ≈ 1. The
obtained results are accurate, because (8) is satisfied, given
that λ/µ = 6.3 × 10−5 ≪ 1.
First, we assess the reliability for the declustered placement
scheme (k = n = 64) for the MDS-coded configurations
considered in [13] with m = 16 and varying values of l and
d. These configurations are denoted by MDS(m,l,d) and the
corresponding results are shown in Figures 2, 3, 4, 5, and 6
by solid lines for d = 0 (no lazy rebuild employed), dashed
lines for d = 1 and dotted lines for d = 2. Six configurations
are considered: MDS(16,13,0), MDS(16,13,1), MDS(16,13,2),
MDS(16,14,0), MDS(16,14,1), and MDS(16,15,0), for each of
the declustered and clustered data placement schemes. In par-
ticular, for the clustered placement scheme, the MDS(16,15,0)
and MDS(16,14,0) configurations correspond to the RAID-5
and RAID-6 systems.
The normalized λ MTTDL measure, which does not de-
pend on the entity size, is obtained from (34) as a function of
Ps and shown in Figure 2(a) for the declustered data placement
scheme. We observe that MTTDL decreases monotonically
with Ps and exhibits m − l − d plateaus. In the interval
[4.096 × 10−12, 4.096 × 10−9] of practical importance for
Ps, which is indicated between the two vertical dashed lines,
MTTDL is degraded by orders of magnitude. Increasing the
number of parities (reducing l) improves reliability by orders of
magnitude. By contrast, and according to Remark 7, employ-
ing lazy rebuild degrades reliability by orders of magnitude.
Moreover, for equivalent systems, such as MDS(16,15,0),
MDS(16,14,1) and MDS(16,13,2), and according to Remark
8, MTTDL increases as d increases.
The normalized λ MTTDL measure for the clustered data
placement scheme is shown in Figure 2(b). We observe that the
declustered placement scheme achieves a significantly higher
MTTDL than the clustered one.
In contrast to MTTDL that is not dependent on the entity
size, the other reliability metrics depend on it. Its effect is as-
sessed by considering the following three cases: entity sizes of
6.656 KB, 6.656 MB, and 6.656 GB that correspond to entities
containing 1, 1000, and 1,000,000 codewords, respectively.
The normalized EAFEL/λ measure corresponding to the
declustered data placement scheme is obtained from (51) and
shown in Figure 3 for various entity sizes. We observe that
EAFEL increases monotonically, but for small entity sizes, it
is practically unaffected in the interval of interest, because
it degrades only when Ps is much larger than the typical
sector error probabilities, as shown in Figure 3(a). However, for
medium and large entity sizes, Figures 3(b) and 3(c) reveal that
EAFEL degrades in the interval of interest and the larger the
entity size, the more pronounced the degradation. By contrast,
for clustered data placement and in the interval of interest,
Figure 4 reveals that EAFEL degrades only when entities are
very large.
For the EAFEL metric too, increasing the number of
parities (reducing l) results in a reliability improvement by
orders of magnitude. By contrast, employing lazy rebuild
degrades reliability by orders of magnitude. Moreover, for
equivalent systems, such as MDS(16,15,0), MDS(16,14,1) and
MDS(16,13,2), and according to Remark 8, EAFEL decreases
as d increases, but according to Figures 3 and 4 the EAFEL
reduction for the clustered placement is much less than that for
the declustered placement. We observe that for both MTTDL
8
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-040-7
CTRQ 2023 : The Sixteenth International Conference on Communication Theory, Reliability, and Quality of Service

(a) Entity Size: 6.656 KB
(b) Entity Size: 6.656 MB
(c) Entity Size: 6.656 GB
Figure 3.
Normalized EAFEL vs. Ps for various MDS(m, l, d) codes; declustered data placement, n = 64, λ/µ = 0.00006, c = 20 TB, and s = 512 B.
(a) Entity Size: 6.656 KB
(b) Entity Size: 6.656 MB
(c) Entity Size: 6.656 GB
Figure 4.
Normalized EAFEL vs. Ps for various MDS(m, l, d) codes; clustered data placement, n = 64, λ/µ = 0.00006, c = 20 TB, and s = 512 B.
(a) Entity Size: 6.656 KB
(b) Entity Size: 6.656 MB
(c) Entity Size: 6.656 GB
Figure 5.
Normalized E(G) vs. Ps for various MDS(m, l, d) codes; declustered data placement, n = 64, λ/µ = 0.00006, c = 20 TB, and s = 512 B.
(a) Entity Size: 6.656 KB
(b) Entity Size: 6.656 MB
(c) Entity Size: 6.656 GB
Figure 6.
Normalized E(G) vs. Ps for various MDS(m, l, d) codes; clustered data placement, n = 64, λ/µ = 0.00006, c = 20 TB, and s = 512 B.
9
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-040-7
CTRQ 2023 : The Sixteenth International Conference on Communication Theory, Reliability, and Quality of Service

and EAFEL reliability metrics, the reliability level achieved
by the declustered data placement scheme is higher than that
of the clustered one.
The normalized expected number E(G)/NE of lost enti-
ties, given that a data loss has occurred, relative to the total
number of entities is obtained from (17) and shown in Figures
5 and 6 for the declustered and clustered data placement
schemes, respectively. In contrast to the PDL, EAFEL, and
E(Q) metrics that increase monotonically with Ps, we observe
that E(G) does not do so. The reason for that is the following.
From Figure 2, it follows that for Ps ≫ 10−15, data loss
is more likely to be due to sector errors than to device
failures. Given that sector errors result in a negligible number
of lost entities compared with the substantial number of lost
entities caused by device failures, when Ps increases over
the value of 10−15, the conditional number of lost entities
decreases. Clearly, this is reversed for high values of Ps, and
the conditional number of lost entities increases.
Also, in the interval [4.096×10−12, 4.096×10−9] of practi-
cal importance for Ps, and by contrast to MTTDL and EAFEL,
employing lazy rebuild does not affect E(G) significantly.
Moreover, for equivalent systems, such as MDS(16,15,0),
MDS(16,14,1) and MDS(16,13,2), and for higher values of d,
E(G) is lower for the declustered data placement scheme, but
it is not significantly affected for the clustered one. However,
Figures 5(c) and 6(c) reveal that this difference is reduced
when entities are very large.
V.
CONCLUSIONS
We considered the Expected Annual Fraction of Entity
Loss (EAFEL) metric, which assesses the durability of dis-
tributed and cloud storage systems and reflects losses at an
entity, say file or object, level. This metric, together with
the traditional MTTDL metric, provide a useful profile of the
size and frequency of data losses. The methodology developed
for deriving the Mean Time to Data Loss (MTTDL) and the
Expected Annual Fraction of Data Loss (EAFDL) reliability
metrics was extended to derive analytically the EAFEL metric
for erasure-coding redundancy schemes. Closed-form expres-
sions capturing the effect of unrecoverable latent errors and
lazy rebuild were obtained for the symmetric, clustered and
declustered data placement schemes. We established that, for
realistic unrecoverable sector error rates, MTTDL is adversely
affected by the presence of latent errors, whereas EAFEL is
adversely affected only when entities are large. It was also
shown that the declustered data placement scheme offers su-
perior reliability. The analytical reliability expressions derived
here can identify lazy rebuild schemes that reduce the volumes
of repair traffic and at the same time ensure a desired level of
reliability.
An extension of the analytical framework developed to
consider also the cases of fixed-size entities that do not contain
an integer number of codewords and of variable-size entities
is a subject of further investigation.
APPENDIX A
Proof of Proposition 1.
Upon entering exposure level u (u ≥ d + 1), there are Cu
most-exposed codewords to be recovered. As an entity contains
J codewords, the number Eu of entities to be recovered is
Eu ≈ Cu
J ,
for u = d + 1, . . . , ˜r − 1 .
(67)
Furthermore, according to Eq. (98) of [21], the probability
of recovering an entity is q fcor J
u
, where qu is the probability
of restoring a codeword and is determined by (21), and fcor
accounts for the correlation of latent errors and is determined
by (22). Consequently, the probability ˜qu that an entity is lost
is determined by (43).
Let us define the variable Wu,i as follows:
Wu,i =
 1 ,
when entity Eu,i is lost
0 ,
when entity Eu,i is recovered, i = 1, . . . , Eu ,
(68)
which implies that
E(Wu) = E(Wu,i) = P(entity Eu,i is lost) = ˜qu ,
for i = 1, . . . , Eu .
(69)
Let us also define by YU the number of lost entities
at exposure level u during the rebuild process of the Cu
codewords. Then it holds that,
YU =
Eu
X
i=1
Wu,i ,
(70)
which implies that
E(YU|Cu) = E
 Eu
X
i=1
Wu,i
!
=
Eu
X
i=1
E(Wu,i)
(69)
=
Eu
X
i=1
E(Wu) = Eu ˜qu
(67)
≈
Cu
J
˜qu .
(71)
Substituting (10) into (71) yields
E(YU|⃗αu−1) ≈ C
J


u−1
Y
j=1
Vj αj

 ˜qu .
(72)
Subsequently, the expected number E(YUFu|Rd+1, ⃗αu−1)
of lost entities due to unrecoverable failures encountered
during rebuild in conjunction with entering exposure level u
through vector ⃗αu−1, and given a rebuild time Rd+1, is
E(YUFu|Rd+1, ⃗αu−1) = Pu(Rd+1, ⃗αu−1) E(YU|⃗αu−1) ,
(73)
where Pu(Rd+1, ⃗αu−1) is the probability of entering exposure
level u through vector ⃗αu−1 ≜ (α1, . . . , αu−1) and given a
rebuild time Rd+1. This probability is determined by [13, Eq.
(47)] for u = d + 1, . . . , ˜r − 1:
Pu(Rd+1, ⃗αu−2) ≈ (λbd+1Rd+1)u−d−1
u−1
Y
i=d+1
˜ni
bi
(Vi αi)u−1−i.
(74)
Note that Pu(Rd+1, ⃗αu−1) = Pu(Rd+1, ⃗αu−2), which implies
that Pu(Rd+1, ⃗αu−1) is not dependent on αu−1.
Substituting (74) and (72) into (73) yields
E(YUFu|Rd+1, ⃗αu−1) ≈ (λbd+1Rd+1)u−d−1
" u−1
Y
i=d+1
˜ni
bi (Vi αi)u−i
#
· C
J
 
d
Y
j=1
Vj
!
˜qu .
(75)
10
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-040-7
CTRQ 2023 : The Sixteenth International Conference on Communication Theory, Reliability, and Quality of Service

Given that the elements of ⃗αu−1 are independent random
variables approximately distributed according to (9), such that
E(αk
i ) ≈ 1/(k + 1) for i ≥ d + 1, we have
E
 u−1
Y
i=d+1
αu−i
i
!
=
u−1
Y
i=d+1
E(αu−i
i
) ≈
u−1
Y
i=d+1
1
u − i + 1 =
1
(u − d)! .
(76)
Unconditioning (75) on ⃗αu−1 and using (76) yields
E(YUFu|Rd+1) ≈ (λbd+1Rd+1)u−d−1
 u−1
Y
i=d+1
˜ni
bi V u−i
i
!
1
(u − d)!
· C
J
 
d
Y
j=1
Vj
!
˜qu .
(77)
Unconditioning (77) on Rd+1, and using (7), (12), and (29)
yields (42).
□
REFERENCES
[1]
D. A. Patterson, G. Gibson, and R. H. Katz, “A case for redundant
arrays of inexpensive disks (RAID),” in Proc. ACM Int’l Conference
on Management of Data (SIGMOD), Jun. 1988, pp. 109–116.
[2]
P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson,
“RAID: High-Performance, reliable secondary storage,” ACM Comput.
Surv., vol. 26, no. 2, Jun. 1994, pp. 145–185.
[3]
V. Venkatesan, I. Iliadis, C. Fragouli, and R. Urbanke, “Reliability of
clustered vs. declustered replica placement in data storage systems,”
in Proc. 19th Annual IEEE/ACM Int’l Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Jul. 2011, pp. 307–317.
[4]
I. Iliadis, D. Sotnikov, P. Ta-Shma, and V. Venkatesan, “Reliability of
geo-replicated cloud storage systems,” in Proc. 2014 IEEE 20th Pacific
Rim Int’l Symposium on Dependable Computing (PRDC), Nov. 2014,
pp. 169–179.
[5]
D. Ford et al., “Availability in globally distributed storage systems,”
in Proc. 9th USENIX Symposium on Operating Systems Design and
Implementation (OSDI), Oct. 2010, pp. 61–74.
[6]
A. G. Dimakis, K. Ramchandran, Y. Wu, and C. Suh, “A survey on
network coding for distributed storage,” Proc. IEEE, vol. 99, no. 3,
Mar. 2011, pp. 476–489.
[7]
C. Huang et al., “Erasure coding in Windows Azure Storage,” in Proc.
USENIX Annual Technical Conference (ATC), Jun. 2012, pp. 15–26.
[8]
S. Muralidhar et al., “f4: Facebook’s Warm BLOB Storage System,”
in Proc. 11th USENIX Symposium on Operating Systems Design and
Implementation (OSDI), Oct. 2014, pp. 383–397.
[9]
A. Dholakia, E. Eleftheriou, X.-Y. Hu, I. Iliadis, J. Menon, and K. Rao,
“A new intra-disk redundancy scheme for high-reliability RAID storage
systems in the presence of unrecoverable errors,” ACM Trans. Storage,
vol. 4, no. 1, 2008, pp. 1–42.
[10]
I. Iliadis, “Reliability modeling of RAID storage systems with latent
errors,” in Proc. 17th Annual IEEE/ACM Int’l Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Sep. 2009, pp. 111–122.
[11]
V. Venkatesan and I. Iliadis, “Effect of latent errors on the reliability
of data storage systems,” in Proc. 21th Annual IEEE Int’l Symposium
on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), Aug. 2013, pp. 293–297.
[12]
M. Silberstein, L. Ganesh, Y. Wang, L. Alvisi, and M. Dahlin,
“Lazy means smart: Reducing repair bandwidth costs in erasure-coded
distributed storage,” in Proc. 7th ACM Int’l Systems and Storage
Conference (SYSTOR), Jun. 2014, pp. 15:1–15:7.
[13]
I. Iliadis, “Effect of lazy rebuild on reliability of erasure-coded storage
systems,” in Proc. 15th Int’l Conference on Communication Theory,
Reliability, and Quality of Service (CTRQ), Apr. 2022, pp. 1–10.
[14]
I. Iliadis, R. Haas, X.-Y. Hu, and E. Eleftheriou, “Disk scrubbing versus
intradisk redundancy for RAID storage systems,” ACM Trans. Storage,
vol. 7, no. 2, 2011, pp. 1–42.
[15]
I. Iliadis and V. Venkatesan, “Rebuttal to ‘Beyond MTTDL: A closed-
form RAID-6 reliability equation’,” ACM Trans. Storage, vol. 11, no. 2,
Mar. 2015, pp. 1–10.
[16]
——, “Expected annual fraction of data loss as a metric for data storage
reliability,” in Proc. 22nd Annual IEEE Int’l Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Sep. 2014, pp. 375–384.
[17]
——, “Reliability evaluation of erasure coded systems,” Int’l J. Adv.
Telecommun., vol. 10, no. 3&4, Dec. 2017, pp. 118–144.
[18]
I. Iliadis, “Reliability evaluation of erasure coded systems under rebuild
bandwidth constraints,” Int’l J. Adv. Networks and Services, vol. 11,
no. 3&4, Dec. 2018, pp. 113–142.
[19]
——, “Data loss in RAID-5 and RAID-6 storage systems with latent
errors,” Int’l J. Adv. Software, vol. 12, no. 3&4, Dec. 2019, pp. 259–
287.
[20]
——, “Reliability of erasure-coded storage systems with latent errors,”
Int’l J. Adv. Telecommun., vol. 15, no. 3&4, Dec. 2022, pp. 23–41.
[21]
——, “Reliability evaluation of erasure-coded storage systems with
latent errors,” ACM Trans. Storage, vol. 19, no. 1, Jan. 2023, pp. 1–47.
[22]
V. Venkatesan and I. Iliadis, “A general reliability model for data storage
systems,” in Proc. 9th Int’l Conference on Quantitative Evaluation of
Systems (QEST), Sep. 2012, pp. 209–219.
[23]
——, “Effect of codeword placement on the reliability of erasure coded
data storage systems,” in Proc. 10th Int’l Conference on Quantitative
Evaluation of Systems (QEST), Sep. 2013, pp. 241–257.
[24]
I. Iliadis and V. Venkatesan, “Most probable paths to data loss: An
efficient method for reliability evaluation of data storage systems,” Int’l
J. Adv. Syst. Measur., vol. 8, no. 3&4, Dec. 2015, pp. 178–200.
[25]
Amazon Web Services, ”Amazon Simple Storage Service (Amazon
S3),” 2022. [Online]. Available: http://aws.amazon.com/s3/ [retrieved:
December 7, 2022]
[26]
D. Borthakur et al., “Apache Hadoop goes realtime at Facebook,” in
Proc. ACM Int’l Conference on Management of Data (SIGMOD), Jun.
2011, pp. 1071–1080.
[27]
R. J. Chansler, “Data availability and durability with the Hadoop
Distributed File System,” ;login: The USENIX Association Newsletter,
vol. 37, no. 1, Feb. 2012, pp. 16–22.
[28]
K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop
Distributed File System,” in Proc. 26th IEEE Symposium on Mass
Storage Systems and Technologies (MSST), May 2010, pp. 1–10.
[29]
V. Venkatesan, I. Iliadis, and R. Haas, “Reliability of data storage
systems under network rebuild bandwidth constraints,” in Proc. 20th
Annual IEEE Int’l Symposium on Modeling, Analysis, and Simula-
tion of Computer and Telecommunication Systems (MASCOTS), Aug.
2012, pp. 189–197.
[30]
A. Chiniah and A. Mungur, “On the adoption of erasure code for
cloud storage by major distributed storage systems,” EAI Endorsed
Transactions on Cloud Systems, vol. 7, no. 21, 2022, pp. 1–11.
[31]
V. Venkatesan and I. Iliadis, “Effect of codeword placement on the
reliability of erasure coded data storage systems,” IBM Research Report,
RZ 3827, Aug. 2012.
[32]
Y. Li, X. Chen, N. Zheng, J. Hao, and T. Zhang, “An exploratory study
on software-defined data center hard disk drives,” ACM Trans. Storage,
vol. 15, no. 3, May 2019, pp. 1–22.
[33]
Seagate,
exos
x20,
data
sheet.
[Online].
Available:
https://www.seagate.com/products/enterprise-drives/exos-x/x20/
[re-
trieved: March, 2023]
[34]
Backblaze
drive
stats
for
2021.
[Online].
Available:
https://www.backblaze.com/blog/backblaze-drive-stats-for-2021/
[re-
trieved: March, 2023]
11
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-040-7
CTRQ 2023 : The Sixteenth International Conference on Communication Theory, Reliability, and Quality of Service

