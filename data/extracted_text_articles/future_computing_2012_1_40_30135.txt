Parallel Approaches for Mining Fuzzy Orderings based Gradual Patterns
Malaquias Quintero Flores∗, Federico Del Razo†, Anne Laurent∗, Nicolas Sicard‡
∗ University Montpellier 2 CNRS - LIRMM, Montpellier, France
Email: {quinteroﬂ, laurent}@lirmm.fr
† Toluca Institute of Technology, Toluca, Mexico
Email: delrazo@ittoluca.edu.mx
‡ EFREI - AllianSTIC, Paris, France
Email: nicolas.sicard@efrei.fr
Abstract—Mining gradual patterns invokes a number of
iterations for generating, adjusting, measuring, and comparing
gradual tendencies between numeric attributes of imprecise
or uncertain databases. Gradual tendencies are complex cor-
relations of the form {The hight/lower X, the hight/lower
Y}. Automatic extraction of such gradual patterns involves
huge amounts of processing time, load balance, and high
memory consumption. When managing large databases, taking
this into account is challenging. In this paper, we show a
framework and an algorithm based on rank correlation and
fuzzy orderings for mining gradual patterns from imprecise
or uncertain data. We also present an approach to improve
performance of the algorithm using the parallel programming
model of OpenMP and the Yale Sparse Matrix Format to
reduce memory consumption. Through an experimental study,
we show the performance of our approach with respect to the
number of attributes of the databases and the number of cores
available.
Keywords-Scaling fuzzy models; Fuzzy orderings; Fuzzy grad-
ual dependencies; OpenMP; Parallel programming.
I. INTRODUCTION
Data mining is often deﬁned as the formulation, analysis,
and implementation of an induction process proceeding
from speciﬁc data to general patterns that facilitates the
non-trivial extraction of implicit, unknown, and potentially
useful
information
[20].
Data
mining
techniques
are
employed in traditional scientiﬁc discovery disciplines,
such as biological, medical, biomedical, chemical, physical,
social research, and other knowledge industries.
Mining gradual patterns (MGP) is an important task for
knowledge discovery (KDD) and data mining (DM). In
MGP, the goal is to ﬁnd in numeric databases interesting
and complex correlations of the form {X{≥|≤}, Y{≥|≤}},
interpreted as {The hight/lower X, the hight/lower Y} and
named gradual dependencies.
Gradual dependencies express a relation (tendency or
correlation) among the variation of the values of attributes
of a gradual pattern. For example, in the database shown
in Table I containing data about fruit characteristics,
such as A1:Size, A2:Weight and A3:Sugar Rate, the
gradual pattern {the higher the weight, the hight the sugar
Table I
A SMALL DATABASE OF SOME FRUIT CHARACTERISTICS
Id
A1 : Size
A2 : Weight
A3 : SugarRate
t0
6
6
5.3
t1
10
12
5.1
t2
14
4
4.9
t3
23
10
4.9
t4
6
8
5.0
t5
14
9
4.9
t6
18
9
5.2
t7
23
10
5.3
t8
28
13
5.5
rate}, means that as the weight of a fruit increases, its
sugar rate tends to increases. Accordingly {the lower
the weight, the lower the sugar rate}, means that as the
weight of a fruit decreases, its sugar rate tends to decreases.
This new way to analyse the degree in which a pattern is
present in a transaction, called gradual pattern or gradual
dependency, was ﬁrst proposed in [10], thereafter studied
in [1][13][16], and more recently in [5][12][18].
There
exist
some
algorithms
for
MGP
that
focus
on ﬁnding gradual patterns from precise and certain
data [12][22]. Unfortunately, this is not always so, there are
situations where databases contain imprecise and uncertain
data (e.g., meteorological data, river pollution data, indicator
economic data, biological data, and so on) due to human
errors, instrument errors, recording errors, noisy data,
variables with imprecise and uncertain behaviour, and so
on [2][6][22].
In this paper, we present a framework and an algorithm
based on rank correlation and fuzzy orderings for MGP
(fuzzy-MGP) [18] from imprecise or uncertain data. We
also present an approach to improve performance of fuzzy-
MGP algorithm using the parallel programming model of
OpenMP [19][21] and a dedicated technique for handling
sparse matrices to reduce memory consumption.
The outline of the paper is as follows: In Section II,
we present the deﬁnitions of gradual dependency, fuzzy
orderings, fuzzy γ rank correlation and related works. In
13
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

Section III, we explain our parallel fuzzy orderings for fuzzy
gradual pattern mining algorithm (parallel fuzzy-MGP). Sec-
tion IV presents experiments and main results. Lastly, in
Section V, we present our conclusions and perspectives of
research.
II. DEFINITIONS AND RELATED WORKS
A. Gradual dependency: Some Background
In the context of fuzzy data mining, the concept of grad-
ual dependency was ﬁrst proposed to perform a regression
analysis between the change of the presence of fuzzy items
in a transaction [10].
In [1], a gradual dependence is deﬁned as a rule
of the form (v1, X, A)
→
(v2, Y, B) holds in D iff
∀(x, y), (x
′, y
′) ∈ D, meaning that A(x)v1A(x
′) implies
B(y)v2B(y
′), where X, Y are two attributes of database
D containing pairs of values (x, y) ∈ X × Y , two fuzzy
sets A, B deﬁned on the domains of X, Y respectively, and
v1, v2 ∈ {<, >}, represent two variations the less (<) and
the more (>).
In [16], Molina et al. propose a particular deﬁnition
of fuzzy gradual dependence from an speciﬁc approach
to fuzzy association rules, where gradual dependencies
represent tendencies in the variation of the degree of
fulﬁlment of properties in a set of objects, and also
introduce the notion of degree of variation associated to a
pair of objects.
In [13], Laurent et al. present an approach for extracting
gradual itemsets, where combines the interpretation of
gradual dependency of Berzal in [1], with a rank correlation
measure and the concept of binary matrices for representing
the sets of concordant couples.
Koh and H¨ullermeier [12] present a framework for
mining fuzzy gradual dependencies, in which the strength
of association between itemsets is measured in terms of a
fuzzy rank correlation coefﬁcient.
B. Gradual Pattern: Concepts and Deﬁnitions
Given a DB (database), constituted of n data record
(transactions) T={t1, t2, ..., tn}, described by m numeric
attributes A={A1, A2, ..., Am}, each record ti is represented
as a vector with m values, ti ={A1(ti), A2(ti), ..., Am(ti)},
were A2(ti) is the value of ti for the attribute A2, similarly
for each record tj.
A GP(gradual pattern) deﬁnes a relation of simultaneous
variation between values of the attributes of two or more
gradual items, different approaches have been deﬁned
according to the interpretation of the concept of gradual
dependency, a GP is a combination of two or more gradual
items of the form GP={A1≥A2≥A3≤} interpreted as {The
hight A1, the hight A2, the lower A3}, where the size (k) of
a GP is deﬁned as the number of gradual items contained
in the GP, such that k∈{2, 3, 4,..., m}.
A gradual item is deﬁned as the variation v associated
to the values of a attribute Al∈DB denoted as Alv, where
v can be ascending (≥|+) if the attribute values increase,
descending (≤|−) if the attribute values decrease, i.e.,
{Al≥} ≃ {Al(ti)<Al(tj)} and {Al≤} ≃ {Al(ti)>Al(tj)}
for i=1, 2, ..., n, for j=i+1, ..., n, i̸=j and l∈{1,2, ..., k}.
A concordant couple (cc) is an index pair cc(Ii,Ij)
deﬁned in (1), where the records (ti, tj) satisfy all the
variations v expressed by the involved gradual items in
a given GP of size k, e.g., let GP={A1≥A2≥A3≤} with
size k=3, an index pair cc(Ii,Ij) is a concordant couple if
((A1(ti)<A1(tj) implies A2(ti)<A2(tj)) implies A3(ti)>
A3(tj)), where Ii is deﬁned in (2) and Ij in (3).
cc(Ii, Ij) =





1
if cc(Ii, Ij) is concordant couple
0
in otherwise.
(1)
(
Ii = ((A1(ti), A2(ti)), (A1(ti), A3(ti))) or
= ((A1(ti), A2(ti)), A3(ti))
(2)
(
Ij = ((A1(tj), A2(tj)), (A1(tj), A3(tj))) or
= ((A1(tj), A2(tj)), A3(tj))
(3)
In MGP a minimal support is used to choose interesting
GP from gradual items which frequently occur together. A
GP is an interesting pattern if support(GP) is greater than
or equal to the user-predeﬁned minimal support named
minimum threshold.
In order to compute the support of a GP, different
approaches have been deﬁned according to the interpretation
of the concept of gradual dependency, such as based on fuzzy
implication interpretation [4][7][9], regression analysis [10],
induced rankings correlation [1][13], ranking-compliant
data subsets [15], strengthening quality criteria [5], and
so on. An interesting analysis of such interpretations is
considered in [5][13].
In the framework of the interpretation of gradual depen-
dency based on induced rankings correlation and concordant
couple concept, the support of a GP is computed as
support(GP) =
Pn
i=1
P
j̸=i cc (Ii, Ij)
n(n−1)
2
(4)
14
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

C. Fuzzy Orderings-Based ˜γ Rank Correlation Measure
Fuzzy orderings is a concept that have been introduced
with the aim to model human-like decisions by taking the
graduality of human thinking and reasoning into account.
Within the framework of a process of making decisions
fuzzy orderings allow express and evaluate preferences
among a set of available alternatives [2][24].
A fuzzy relation L : X2 →[0,1] is called fuzzy ordering
with respect to a t-norm T and a T-equivalence E : X2
→[0,1], for brevity T-E-ordering, if and only if the
following three axioms are fulﬁlled for all x, y, z ∈ X:
(i)
E-Reﬂexivity: E(x,y)≤ L(x,y)
(ii)
T-E-Antisymmetry: T(L(x,y),L(y,x))≤ E(x,y)
(iii)
T-Transitivity: T(L(x,y),L(y,z))≤ L(x,z).
The result of combining fuzzy orderings and gamma rank
correlation measure is a robust rank correlation coefﬁcient
ideally suited for measuring rank correlation for numerical
data perturbed by noise [3]. This innovative correlation co-
efﬁcient is known as fuzzy ordering-based rank correlation
measure ˜γ. Its formal deﬁnition is:
˜γ = CT − DT
CT + DT
(5)
CT =
n
X
i=1
X
j̸=i
˜C(i, j)
(6)
DT =
n
X
i=1
X
j̸=i
˜D(i, j)
(7)
˜C(i, j) = ⊤(RX(xi, xj), RY (yi, yj))
(8)
˜D(i, j) = ⊤(RX(xi, xj), RY (yj, yi))
(9)
RX(xi, xj) = 1 − LX(xj, xi)
(10)
LX(x1, x2) = min(1, max(0, 1 − (x1 − x2)
r
))
(11)
RY (yi, yj) = 1 − LY (yj, yi)
(12)
LY (y1, y2) = min(1, max(0, 1 − (y1 − y2)
r
))
(13)
⊤(a, b) = max(1, a + b − 1)
(14)
{(xi, yi)n
i=1|xi ∈ X and yi ∈ Y }
(15)
where ˜C(i, j) is the degree to which (i, j) is a concordant
pair and ˜D(i, j) is the degree to which (i, j) is a discordant
pair, for i=(xi, yi) and j=(xj, yj), such that
i=1, 2,...,n,
j=1, 2,...,n, i̸=j and n ̸= CT + DT. Given n≥2 pairs of
numeric observations, deﬁned in (15)
RX(xi, xj) is a strict TL − EX − ordering on X deﬁned
in (10), RY (yi, yj) is a strict TL − EY − ordering on Y
deﬁned in (12), LX(xi, xj) is a strongly complete TL −
Er − ordering on X deﬁned in (11), and LY (yi, yj) is a
strongly complete TL −Er −ordering on Y deﬁned in (13)
(assume r>0).
⊤
is
a
⊤−equivalence
relation
and
denotes
a
Lukasiewicz t-norm deﬁned in (14) for
a=RX(xi, xj)
and b =RY (yi, yj). For more information we recommend
consulting [2][3][12].
D. Problem statement
An important challenge in gradual pattern mining is to
analyze the correlation between the variation of numerical
attribute values perturbed by noise, and to consider when
such a small difference between two values is meaningful. In
this context, we present a fuzzy orderings-based framework
for mining fuzzy gradual patterns, where we propose to
compute the support of a GP, as in (16) based on compute of
the degree to which (Ii, Ij) is a concordant pair as is deﬁned
in (8) and according to the deﬁnition of concordant couple
given in (1), (2), and (3).
fuzzsupport(GP) =
Pn
i=1
P
j̸=i ˜C(Ii, Ij)
n(n − 1)
(16)
Huge amounts of processing time, load balance and
high memory consumption are important problems observed
in gradual pattern mining algorithms. We addressed these
problems making use of a parallel programming model and
an efﬁcient technique to reduce memory consumption.
E. Related Work
Recently, in [14] and [15], Laurent et al. have presented
PGP-mc a multicore parallel approach for mining gradual
patterns where the evaluation of the correlation and support
is based on conﬂict sets and precedence graph approaches.
PGP-mc was implemented using the g++ 3.4.6 and 4.3.2
with POSIX threads, on two different workstations: i) COY-
OTE machine, with 8 AMD Opteron 852 processors (each
with 4 cores), 64 GB of RAM with Linux Centos 5.1 And
ii) IDKONN machine, with 4 Intel Xeon 7460 processors
(each with 6 cores), 64 GB of RAM with Linux Debian
5.0.2. The experiments were led on synthetic databases
automatically generated by a tool based on adapted version
of IBM Synthetic Data Generation Code for Associations
and Sequential Patterns.
15
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

Database 
List of gradual 
patterns, and 
 support (GPk,p). 
Fuzzification/ 
Normalization 
Fuzzy 
Database/ 
Normalized 
Database 
MinSupport 
Generation of 
candidates
(Apriori 
algorithm) 
Parallel fuzzy 
orderings-based 
 evaluation of fuzzy 
concordance 
degrees C(i, j). 
Module 
of: 
Yale 
sparse matrix format  
of fuzzy concordance 
degrees C(i,j). 
Figure 1.
Parallel fuzzy orderings-based extraction of fuzzy frequent
gradual dependencies: General structure.
An efﬁcient parallel mining of closed frequent gradual
patterns, named PGLCM, has been proposed by Do et
al. [8]. This approach is based on the principle of the
LCM algorithm for mining closed frequent patterns, an
adaptation of LCM named GLCM in order to mine closed
frequent gradual patterns, and parallelization of the GLCM
algorithm named PGLCM based on the Melinda parallelism
environment. The comparative experiment is based on
synthetic databases produced with the same modiﬁed
version of IBM Synthetic Data Generator for Association
and Sequential Patterns. All the experiment have been
conduced on a 4-socket server with 4 Intel Xeon 7460 with
6 cores each and 64 GB of RAM.
III. PARALLEL FUZZY ORDERINGS FOR FUZZY GRADUAL
PATTERN MINING
In this section, we present an approach to improve
the performance of our method of automatic extraction of
frequent gradual patterns on the basis of fuzzy orderings [18]
and the idea of storing the fuzzy concordance degrees
C(i, j) in sparse matrices.
Figure 1 shows the general structure of our optimization
approach which has two purposes: A) Reduce memory
consumption
and
B)
Improve
execution
time
via
parallelization.
A. Memory consumption
In order to reduce memory consumption, we represented
and stored each matrix of concordance degrees C(i, j)
according to the Yale Sparse Matrix Format [23], such as
only non-zero coefﬁcients are retained. Because we generate
candidates from the frequent k−patterns, only matrices of
the (k-1)−level frequent gradual patterns are kept in memory
while being used to generate the matrices of the (k)−level
gradual pattern candidates, e.g., Figure 2 illustrates the
extraction of gradual patterns, from the data set of Table
I, with a minimum threshold= 0.15, for level k=2 and k=3,
where, if support of a gradual pattern (GP) is less than
minimum threshold then the GP is pruned and its matrix of
Gradual 
Items 
{A1+} 
{A1−} 
{A2+} 
{A2−} 
{A3+} 
{A3−} 
K=2 
GP 
Support 
{A1+A2+} 0.347 
{A1+A2−} 0.08 
{A1+A3+} 0.278 
{A1+A3−} 0.13 
{A1−A2+} 0.08 
{A1−A2−} 0.347 
{A1−A3+} 0.13 
{A1−A3−} 0.278 
{A2+A3+} 0.329 
{A2+A3−} 0.11 
{A2−A3+} 0.11 
{A2−A3−} 0.329 
X 
X 
X 
X 
X 
X 
Frequent GP 
Support 
{A1+A2+A3+} 
0.245 
{A1−A2−A3−} 
0.245 
K=3 
Frequent 
GP 
{A1+A2+} 
{A1+A3+} 
{A1−A2−} 
{A1−A3−} 
{A2+A3+} 
{A2−A3−} 
“+”   ∼   “≥” 
−   
positive variation 
“−”   ∼   “≤” 
−   
negative variation 
Pruned infrequent GP  
X 
Support(GP) < minimum threshold  
minimum threshold=0.15 
Figure 2.
Extraction of gradual patterns from data set of Table I for level
k=2 and k=3.
0.00 1.00 0.00 1.00 0.00 1.00 1.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00  
0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00  
0.00 1.00 0.00 1.00 0.00 1.00 1.00 1.00 1.00  
0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 1.00  
0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  
{A1+A2+} <- {{A1+},{A2+}} - supp = 0.347 
25 non-zero values : 127 / 93 bytes 
0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.67 1.00 1.00 1.00  
0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00  
0.00 0.66 0.00 0.00 0.00 1.00 1.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.66 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
{A1+A3+} <- {{A1+},{A3+}} - supp = 0.278 
21 non-zero values : 115 / 93 bytes 
Level k>2 : 
Level k=2 : 
0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00  
0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00  
0.00 0.66 0.00 0.00 0.00 1.00 1.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  
{A1+A2+A3+} <- {{A1+A2+}, {A1+A3+}} - supp = 0.245 
18 non-zero values : 106 / 93 bytes 
minimum threshold=0.15 
Figure 3.
Examples of matrices of fuzzy concordance degrees C(i,j) of
three gradual patters of Figure 2 with ascending variation.
JA 
7 
8 
7 
8 
3 
6 
7 
8 
8 
1 
5 
A 
1.0 
1.0 
1.0 
1.0 
1.0 
1.0 0.72 
1.0 
1.0 
0.66 
1.0 
IA 0 
2 
4 
8 
9 
14 
16 null null 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
0 
1 
2 3 
4 
5 
6 
7 
8 
{A1+A2+A3+}          {{A1+A2+},{A1+A3+}}  
Gradual pattern    GP3,1= 
Compressed matrix of fuzzy concordance degrees C(i,j):  
18 non-zero values : 106 / 93 bytes 
JA 
6 
7 
8 
7 
8 
7 
8 
A 
1.0 
1.0 
1.0 
1.0 
1.0 
1.0 
1.0 
11 
12 
13 
14 
15 
16 
17 
Figure 4.
Example of representation of a matrix of fuzzy concordance
degrees C(i,j) in the Yale Sparse Matrix Format.
16
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

F
O
R
K 
J
O
I
N 
F
O
R
K 
J
O
I
N 
Parallel evaluation of concordance degrees C(i, j) in: 
a) Fuzzy frequent gradual 
dependencies level k = 2 
Master  
thread 
Team of parallel  
threads  
{1, 2, 4, 6, …, 32} 
Team of parallel  
threads  
{1, 2, 4, 6, …, 32} 
b) Fuzzy frequent gradual 
dependencies level k > 2 
Figure 5.
Parallel evaluation of fuzzy concordance degrees in: a) Gradual
dependency of level k = 2, b) Gradual dependency of level k > 2.
fuzzy concordance degrees C(i,j) is removed.
Figure
3
shows
the
matrices
of
fuzzy
con-
cordance
degrees
C(i,j)
of
the
gradual
patters
GP2,1={A1+A2+},
GP2,3={A1+A3+}
of
level
k=2,
and GP3,1={A1+A2+A3+} of level k=3. Notation “+”
represents an ascending variation “≥” and notation “−”
represents an descending variation “≤”. Figure 4 shows the
representation of the matrix of fuzzy concordance degrees
C(i,j) in the Yale Sparse Matrix Format for the gradual
patter GP3,1.
B. Parallelization of algorithm
Load balance, huge amounts of processing time and high
memory consumption are important problems observed
in
gradual
pattern
mining
algorithms.
We
addressed
these problems using the shared memory architecture
API of OpenMP, which is ideally suited for multi-core
architectures [19][21].
Figure 5 gives an overall view of our approach to
parallelize the automatic extraction of frequent gradual
patterns of level k=2 and the automatic extraction of
frequent gradual patterns of level k>2.
Algorithm 1 shows the pseudocode of the master thread,
where DB is a database, m is the number of attributes in
DB, n is the number of records in DB, {Am} represents
the identiﬁers of the m attributes.
The variable named minSupp is the minimum threshold,
v←{+} denotes a positive (ascending) variation in each
index pair of an attribute, v←{-} denotes a negative
(descending) variation in each index pair of an attribute,
ListGIs represents the list of gradual items level k=1, Fk=2
is the set of gradual patterns/dependencies of level k=2, Fk
is the set of gradual patterns/dependencies of level k>2,
FF GD are all fuzzy frequent gradual dependencies and Nt
is the number of threads of each parallel region.
Algorithm 1: Main Thread for Fuzzy Gradual Depen-
dency Mining
Data: Database(DB), # Attributes(m), # Records(n),
Data: Id. Attributes{Am}, minSupp, # Threads (Nt).
Result: Fuzzy Frequent Gradual Dependencies FF GD
FF GD ← ∅; v ← {+, −};
ListGIs = build gItems({Am} × v);
k ← 2;
/* Parallel extraction of fuzzy frequent gradual
dependencies level k = 2*/;
Fk=2 = GradualPatterns(ListGIs, Ds, minSupp);
FF GD ← FF GD ∪ {Fk=2};
k + +;
repeat
/* Parallel extraction of fuzzy frequent gradual
dependencies level k > 2*/;
Fk=GradualPatterns(Fk−1, level(k), minSupp);
FF GP ← FF GP ∪ {Fk};
Delet(Fk−1.M);
k++;
until FF GP does not grow any more;
The pseudo-code of the Algorithm 2 corresponds to the
parallel extraction of frequent gradual dependencies of
level k = 2, where Sdck=2 is the set of gradual patterns of
size k = 2, Gdc is a pattern gradual candidate ∈ Sdck=2,
Gdcq.M
represents
the
matrix
of
fuzzy
concordance
degrees C(i, j) computed by fuzzy orderings.
The pseudo-code of the Algorithm 3 corresponds to the
parallel extraction of frequent gradual dependencies of level
k > 2, where {Fk1} is the set of frequent gradual patterns
of level (k − 1), Ck,q.M represents the matrix of fuzzy
concordance degrees C(i, j) of a gradual pattern candidate
of level k (computed by T − norm) and Fk is the set of
frequent gradual patterns of level k.
IV. EXPERIMENTS AND MAIN RESULTS
A. Experiments
We
present
an
experimental
study
of
the
scaling
capacities of our approach on several cores, for the database
C500A100 with 500 records and 100 attributes, and
database C500A150 with 500 records and 150 attributes,
which were used in [8][14] and produced with the IBM
Synthetic Data Generator for Association and Sequential
Patterns.
Our experiments were performed on a workstation with
up to 32 processing cores, named COYOTE, with 8 AMD
Opteron 852 processors (each with 4 cores), 64 GB of
RAM with Linux Centos 5.1, GCC OpenMP 3.1.
17
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

Algorithm 2: Parallel Fuzzy Orderings for Gradual Pat-
tern Mining: Level k = 2
Data: List of gradual Items(List gIs), minSupp
Data: Attribute values (aValues), # Threads (Nt)
Result: Fuzzy Frequent Gradual Dependencies (Fk=2)
Fk=2 ← ∅;
q ← 1;
/*Each thread computes the support of its fuzzy
frequent gradual dependency of level k = 2*/;
for all(thread in(1, 2, 4, 6, 8, ..., Nt = 32)) do
Sdck=2 ← GenCand({List gIs}, level(k = 2));
Gdc ← FirstxCandidate ∈ Sdck=2;
foreach Gdc ∈ Sdck=2 do
Gdcq.M = FuzzOrderings(Gdc, aV alues);
Support(Gdcq) = EvalSupport(Gdcq.M);
/* minSupp stands for a user-speciﬁed
minimum support value */;
if Support(Gdcq) ≥ minSupp then
Critical section :
>>>
Fk=2 ← Fk=2 ∪ {Gdcq};
q + +;
Gdcq ← NextCandidate ∈ Sdck=2;
Algorithm 3: Parallel Fuzzy Orderings for Gradual Pat-
tern Mining: Level k > 2
Data: Fuzzy Frequent Gradual DependencyFk−1,
Data: minSupp, Level(k>2), # Threads (Nt).
Result: Fuzzy Frequent Gradual Dependencies (Fk)
Fk ← ∅;
q ← 1; k1 ← k − 1;
/*Each thread computes the support of its fuzzy
frequent gradual dependency of level k > 2*/;
for all(thread in(1, 2, 4, 6, 8, ..., Nt = 32)) do
Fk1,a ← GenFirstFather1({Fk1});
Fk1,b ← GenFirstFather2({Fk1});
foreach {Fk1,a, Fk1,b} ∈ {Fk1} do
Ck,q.M = T −norm({Fk1,a.M}, {Fk1,b.M});
Support(Ck,q.M) = EvalSupport(Ck,q.M);
/* minSupp stands for a user-speciﬁed
minimum support value */;
if Support(Ck,q.M) ≥ minSupp then
Critical section :
>>>
Fk ← Fk ∪ {Ck,q};
Fk1,a ← GenNextFather1({Fk1});
Fk1,b ← GenNextFather2({Fk1});
q + +;
0
500
1000
1500
2000
2500
3000
3500
0
5
10
15
20
25
30
35
Elapsed time (sec.) 
Database of 500 records 
and 100 attributes 
Minimum threshold = .375 
Number of threads 
Minimum threshold = .38 
Number of patterns found= 186994 
Number of patterns found= 121154 
32 
Figure 6.
Threads vs. elapsed time with a database of 500x100 and
minSupp=.375 and .38, using uncompressed binary matrices of concordance
degrees.
0
5
10
15
20
25
0
5
10
15
20
25
30
35
Database of 500 records 
and 100 attributes 
Number of threads 
Minimum threshold = .375 
Minimum threshold = .38 
Speedup 
32 
Number of patterns found= 186994 
Number of patterns found= 121154 
Figure 7.
Speedup with a database of 500x100 and minSupp=.375 and
.38, using uncompressed binary matrices of concordance degrees.
B. Main Results
The ﬁrst experiment involves a database with 500 lines
and 100 attributes, Figures 6 and 7 depict the execution
time and speed-up related to: 1, 2, 4, 6, 8, 10, 12, 14,
16, 18, 20, 22, 24, 26, 28, 30, and 32 threads, on the test
database, from which were found 186994 frequent gradual
patterns for a minimum threshold of 0.375, and 121154
frequent gradual patterns for a minimum threshold of 0.38.
In the ﬁrst case we reach a memory consumption of 36.2%
using uncompressed binary matrices of concordance degrees
and of 14.4% using compressed matrices of concordance
degrees (Yale Sparse Matrix Format). While for the second
case we obtained a memory consumption of 24.7% with
uncompressed binary matrices of concordance degrees
and of 10.3% with compressed matrices of concordance
degrees. Within this experimental framework, Figures 8 and
9 illustrate the execution time and speed-up of our approach
using compressed matrices of concordance degrees.
The second experiment involves a database with 500 lines
and 150 attributes, from which were found 100834 frequent
18
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

0
200
400
600
800
1000
1200
1400
1600
1800
0
5
10
15
20
25
30
35
Elapsed time (sec.) 
Database of 500 records 
and 100 attributes 
(CM) 
Minimum threshold = .375 
Number of threads 
Minimum threshold = .38 
Number of patterns found= 186994 
Number of patterns found= 121154 
32 
Figure 8.
Threads vs. elapsed time with a database of 500x100 and
minSupp=.375 and .38, using compressed matrices of concordance degrees.
0
2
4
6
8
10
12
14
16
18
20
0
5
10
15
20
25
30
35
Database of 500 records 
and 100 attributes 
(CM) 
Number of threads 
Minimum threshold = .375 
Minimum threshold = .38 
Speedup 
32 
Number of patterns found= 186994 
Number of patterns found= 121154 
Figure 9.
Speedup with a database of 500x100 and minSupp=.375 and
.38, using compressed matrices of concordance degrees.
gradual patterns for a minimum threshold of 0.40, where we
reach a memory consumption of 24.5% using uncompressed
binary matrices of concordance degrees and of 10.8% using
compressed matrices of concordance degrees (Yale Sparse
Matrix Format). Within this experimental framework, Fig-
ures 10 and 11 depict the execution time and speed-up of
our approach, related to: 1, 2, 4, 6, . . . , to 32 threads.
V. CONCLUSION AND FUTURE WORK
In this paper, we presented the parallelization of our
fuzzy gradual pattern mining named fuzzy orderings for
fuzzy gradual pattern mining. Our approach consists in
parallelizing the extraction of gradual dependencies of
level k = 2 and in parallelizing the extraction of gradual
dependencies of level k > 2. The programming parallel
model used was the shared memory with the API of
OpenMP - C++.
In order to reduce memory consumption, each matrix
of concordance degrees C(i, j) is represented and stored
according to the Yale Sparse Matrix Format, such as only
non-zero coefﬁcients are retained. Because we generate
0
500
1000
1500
2000
2500
3000
3500
4000
0
5
10
15
20
25
30
35
Elapsed time (sec.) 
Database of 500 records 
and 150 attributes 
Minimum threshold = 0.40 
Number of threads 
Number of patterns found= 100834 
32 
Using compressed matrices 
Using uncompressed matrices 
Figure 10.
Threads vs. elapsed time with a database of 500x150 and min-
Supp=0.40, using uncompressed and compressed matrices of concordance
degrees.
0
2
4
6
8
10
12
14
16
18
20
0
5
10
15
20
25
30
35
Database of 500 records and 150 attributes 
Number of threads 
Speedup 
32 
Minimum threshold = 0.40 
Number of patterns found= 100834 
Using compressed matrices 
Using uncompressed matrices 
Figure 11.
Speedup with a database of 500x150 and minSupp=0.40, using
uncompressed and compressed matrices of concordance degrees.
candidates from the frequent k−itemsets, only matrices
of the (k − 1)−level frequent gradual patterns are kept in
memory while being used to generate the matrices of the
(k)−level gradual pattern candidates.
The experimental work reported in this document was
conducted on two databases, the ﬁrst with 500 records and
100 attributes and the second with 500 records and 150
attributes.
In general, the performance of our Parallel Fuzzy Gradual
Dependency Mining is signiﬁcantly improved with the use
of compressed matrices, mainly with databases containing
a large number of attributes. We recommend testing our
algorithm with databases with a larger number of records
and real world databases.
REFERENCES
[1] F. Berzal, J.C. Cubero, D. Sanchez, J. M. Serrano, and M.
A. Vila, An alternative approach to discover gradual depen-
dencies, Int. Journal of Uncertainty, Fuzziness and Knowledge-
based Systems, vol. 15, no. 5, pp. 559-570, (2007).
19
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

[2] U. Bodenhofer and F. Klawonn, Towards robust rank correla-
tion measures for numerical observations on the basis of fuzzy
orderings, in 5th Conference of the European Society for Fuzzy
Logic and Technology, pp. 321 - 327, Ostrava, Czech Republic,
University of Ostrava, Institute for Research and Applications
of Fuzzy Modeling, (Septembre, 2007).
[3] U. Bodenhofer and F. Klawonn, Roboust rank correlation
coefﬁcients on the basis of fuzzy orderings: Initial steps, in
Mathware & Soft Computing 15, 5-20 (2008).
[4] B. Bouchon-Meunier and S. Despr`es. Acquisition num´erique
/ sysmbolique de connaissance graduelles, in 3`emes Journ´ees
Nationales du PRC Intelligence Artiﬁcielle. Herm`es, pp. 127-
138 (1990).
[5] B. Bouchon-Meunier, A. Laurent, M.-J. Lesot, and M. Rifqi,
Strengthening fuzzy gradual rules through “all the more”
clauses, in Proc. WCCI 2010 IEEE World Congress on Com-
putatinal Inteligence, FUZZ-IEEE 2010, pp. 2940-2946 (2010).
[6] T. Calders, B. Goethals, and S. Jaroszewicz, Mining Rank-
Correlated Sets of Numerical Attributes, in Proc. KDD’06
ACM, pp. 96-105, (2006).
[7] D. Dubois and H. Prade, Gradual inference rules in approx-
imate reasoning, Information Sciences, vol. 61, no. 1-2, pp.
103-122 (1992).
[8] T. D. T. Do, A. Laurent, and A. Termier, PGLCM: Efﬁcient
Parallel Mining of Closed Frequent Gradual Itemsets, in Proc.
International Conference on Data Mining (ICDM), pp. 138-147
(2010).
[9] E. H¨ullermeier, Implication-based fuzzy association rules, in
Proceedings PKDD’01, pp. 241-252, (2001).
[10] E. H¨ullermeier, Association rules for expressing gradual
dependencies, in Proceedings PKDD 2002 Lecture Notes in
Computer Science 2431, pp. 200-211 (2002).
[11] E. H¨ullermeier, Fuzzy Sets in Machine Learning and Data
Mining, in Applied Soft Computing Journal, vol. 11, pp. 1493-
1505, (2011).
[12] H-W. Koh and E. Hullermeier, Mining gradual dependencies
based on fuzzy rank correlation, in Combining Soft Comput-
ing and Statistical Methods in Data Analysis. Advances in
Intelligent and Soft Computing, Vol. 77, pp. 379-386. Springer
Heidelberg (2010).
[13] A. Laurent, M.-J. Lesot, and M. Rifqi, GRAANK: Exploiting
rank correlations for extracting gradual itemsets, in FQAS’09,
LNAI 5822, Springer Berlin Heidelberg, pp. 382-393, (2009).
[14] A. Laurent, B. Negrevergne, N. Sicard, A. Termier, PGP-
mc: Towards a multicore parallel approach for mining gradual
patterns, in: Proc. DASFAA, pp. 78-84, (2010).
[15] A. Laurent, B. Negrevergne, N. Sicard, A. Termier, Efﬁcient
parallel mining of gradual patterns on multi-core processors,
in AKDM-2, Advances in Knowledge Discovery and Manage-
ment. Vol. 2. Springer, (2010).
[16] C. Molina, J. M. Serrano, D. Sanchez, and M. A. Vila, Mining
gradual dependencies with variation strength, in Mathware &
Soft Computing, Volume 15, pp. 75 - 93 (2008).
[17] P. L. Nancy, and C. Hao-en, Fuzzy Correlation Rules Mining,
in Proceedings of the 6th WSEAS International Conference on
Applied Computer Science, Hangzhou, China, pp 13-18, April
15-17 (2007).
[18] M. Quintero, A. Laurent, P. Poncelet, Fuzzy Ordering for
Fuzzy Gradual Patterns, in FQAS 2011, LNAI 7022, Springer-
Verlag Berlin Heidelberg, pp. 330-341, (2011).
[19] T. Rauber, G. Rnger, Parallel Programming: for Multicore
and Cluster Systems, Springer-Verlag Berlin Heidelberg (2010)
[20] V. Stankovski, Special section: Data mining in grid comput-
ing environments, in CienceDirect Elsevier, Future Generation
Computer Systems, vol. 23, pp. 31-33, (2007).
[21] R.
Van
der
Pas,
An
Overview
of
OpenMP,
In
the
OpenMP
API
speciﬁcation
for
parallel
programming,
http://openmp.org/wp/resources/#Tutorials, [retrieved: May,
2012].
[22] D.-H. Weng, and Y.-L. Chen, Mining fuzzy association rules
from uncertain data, in Knowl Inf Syst, vol. 23, Springer, pp.
129-152, (2010).
[23] D. Wilhelm, Sparse Matrix Formats, in ECE 250 Algorithms
and Data Structures, Department of Electrical and Computer
Engineering, University of Waterloo, Ontario, Canada, [re-
trieved: May, 2012].
[24] L. A. Zadeh, Similarity Relations and Fuzzy Orderings, in
Information Sciences. Volume 3, Issue 2, pp. 177 - 200, (April,
1971).
20
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

