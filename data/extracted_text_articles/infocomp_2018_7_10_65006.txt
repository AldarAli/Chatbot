A Simplex Algorithm with the Smallest Index Rule
for Concave Quadratic Programming
Mohand Bentobache, Mohamed Telli and Abdelkader Mokhtari
Laboratory of Pure and Applied Mathematics
University of Laghouat, 03000, Laghouat, Algeria
Email: m.bentobache@lagh-univ.dz
Abstract—In this work, we propose a new algorithm called
”Simplex Algorithm with the Smallest Index Rule” for ﬁnding
a local minimum of a concave quadratic function subject to
linear equality and nonnegativity constraints. First, we present
and prove a new sufﬁcient and necessary condition for local
optimality, then we describe the developed algorithm and we
give a numerical example for illustration purpose. In order
to prove the efﬁciency of our algorithm, we developed an
implementation using MATLAB, then we conducted numerical
experiments on randomly generated and Rusakov’s concave
quadratic test problems. The obtained numerical results show
that our algorithm outperforms the branch and bound algorithm
suggested by Rusakov in terms of CPU time and it gives the global
optimal solution for the Rusakov’s test problems. Furthermore, it
gives the global optimum for some generated test problems and
it ﬁnds, for other problems, a local minimizer which can be used
to initialize global optimization algorithms.
Keywords–Concave quadratic programming; Local minimum;
Global minimum, Simplex algorithm, Numerical experiments.
I.
INTRODUCTION
The Concave Quadratic Programming (CQP) problem
consists in minimizing a concave quadratic function under
a convex polyhedron delimited by linear constraints. This
optimization problem has important theoretic and practical
aspects. Indeed, many practical problems are modeled as CQP
problems, we can cite the quadratic assignment problem [1],
missile ﬂight testing [2], etc.
Unlike the convex quadratic programming problem, this
problem is difﬁcult to solve since a local optimal solution
is not in general a global one. Therefore, in many research
articles, the authors developed algorithms for approximate the
global optimum of the problem. The ﬁrst algorithm for solving
the problem is suggested by Tuy [3]. The principle of the
Tuy’s algorithm is to compute a new linear constraint, called
Tuy cut, in order to eliminate points in feasible region, which
can not be global optimal solutions. Later, many algorithms
are developed: branch and bound algorithms [4][5], cutting
plane algorithms [6], successive underestimating method [7],
metaheuristic algorithms [8], etc.
The majority of the proposed global optimization algo-
rithms starts by a local optimal solution. It is proved in [9] that
a local optimal solution of the problem is an extreme point of
the convex polyhedron corresponding to the linear constraints.
Hence, in this work we suggest a new algorithm called
”Simplex Algorithm with the Smallest Index Rule” (SASIR)
for ﬁnding an extreme point, which is a local minimum for the
considered problem. The principle of our algorithm is similar
to the one of the simplex algorithm of linear programming
[10]: it starts by an initial extreme point obtained using some
existing initialization technique of the simplex method, then it
moves in each iteration from one extreme point to a new one
having a better value of the quadratic objective function and
ﬁnally it stops when a local optimality condition is satisﬁed.
In order to test the efﬁciency of our method, we have
implemented it in MATLAB and conducted numerical exper-
iments on Rusakov’s test problems [5] inspired from prac-
tical problems arising in the area of missile ﬂight testing
and a set of randomly generated test problems with known
global minimum and size varying from 100 constraints and
120 variables up to 200 constraints and 240 variables. The
obtained numerical results are very encouraging. Indeed, our
algorithm gives a local optimum which is also global for
Rusakov’s test problems and it outperforms the Rusakov’s
algorithm implemented in his software [11] in terms of CPU
time. Furthermore, SASIR ﬁnds the global optimum for some
randomly generated test problems in reasonable amount of
time and it gives, for other test problems, a local minimizer
which can be used as initial point for global optimization
algorithms.
The paper is organized as follows: in Section II, we
present the problem, we give some deﬁnitions and we recall
some fondamental results of concave quadratic programming.
In Section III, we present and prove the suggested local
optimality criterion. In Section IV, we describe and justify
the suggested algorithm. Moreover, we illustrate our approach
with a numerical example. In Section V, we present some
numerical results in order to compare our algorithm with the
branch and bound algorithm of Rusakov [5] which uses the
Tuy cut. Finally, Section VI concludes the paper and gives
some future works.
II.
PRESENTATION OF THE PROBLEM AND DEFINITIONS
The concave quadratic programming problem with equality
and nonnegativity contraints is presented in the following form:
min f(x) = 1
2xT Dx + cT x,
subject to
Ax = b, x ≥ 0,
(1)
where D is an (n × n) real symmetric negative semideﬁnite
matrix, c and x are n-vectors; A is a matrix of dimension
m × n, with Rank(A) = m < n.
• Let us deﬁne the following sets of indices:
I = {1, 2, . . . , m}, J = {1, 2, . . . , n}, J = JB ∪ JN,
88
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-655-2
INFOCOMP 2018 : The Eighth International Conference on Advanced Communications and Computation

JB ∩ JN = ∅, |JB| = m, K = {1, . . . , n − m}.
We can partition the vectors x, c and the matrix A as follows:
x =

xB
xN

, xB = (xj, j ∈ JB), xN = (xj, j ∈ JN),
c =

cB
cN

, cB = (cj, j ∈ JB), cN = (cj, j ∈ JN),
A = (aij, i ∈ I, j ∈ J) = (aj, j ∈ J), aj =



a1j
...
amj


 ,
A = (AB, AN), AB = A(I, JB), AN = A(I, JN).
• We denote the feasible region of problem (1) by
S = {x ∈ Rn : Ax = b and x ≥ 0}.
• A vector x ∈ S is called a feasible solution for the problem
(1).
• Let JB ⊂ J be a subset of indices such that |JB| = |I| = m.
The matrix AB = A(I, JB) is said to be a basis matrix if
det(AB) ̸= 0. Then the feasible solution x =

xB
xN

, with
xB = A−1
B b ≥ 0 and xN = 0 is called a Basic Feasible
Solution (BFS).
• A BFS x is said to be nondegenerate if xj > 0, j ∈ JB.
• Let AB = A(I, JB) be a basis matrix, JN = J \ JB and
x the corresponding BFS. Let j0 ∈ JN, j1 ∈ JB be two
indices, and ¯JB = (JB \ {j1}) ∪ {j0}, ¯AB = A(I, ¯JB), such
that det( ¯AB) ̸= 0. Let ¯x be the BFS corresponding to the
new basis matrix ¯AB. Hence, we say that the basic feasible
solutions x and ¯x are adjacent.
• Let x∗ be a feasible solution for problem (1). We say that
x∗ is a local minimizer if it exists a neighborhood N(x∗) of
x∗, such that ∀x ∈ N(x∗) ∩ S, f(x∗) ≤ f(x). The vector x∗
is said to be a global minimizer if f(x∗) ≤ f(x), ∀x ∈ S.
Let us recall the following fundamental result [9]:
Theorem 1. Let f be a concave function deﬁned on the
bounded, closed convex set Ω. If f has a minimum over Ω,
then it is achieved at an extreme point of Ω.
• Since D is negative semideﬁnite, the quadratic function f
is concave. Therefore, the global minimizer is achieved at an
extreme point of the convex polyhedron S. This leads us to
give the following deﬁnitions: let A∗
B a basis matrix and x∗
the corresponding BFS, we denote by N(x∗) the set of all
basic feasible solutions, which are adjacent to x∗. We say that
x∗ is a local minimizer for problem (1), if it satisﬁes f(x∗) ≤
f(x), ∀x ∈ N(x∗). We say that x∗ is a global minimizer for
problem (1), if for any BFS x ∈ S, we have f(x∗) ≤ f(x).
• Let JB be a set of basic indices for problem (1) and JN =
J \ JB. We deﬁne the following vectors and matrices:
¯b = (¯bi, i ∈ I) = A−1
B b, ¯A = (¯ak, k ∈ K) = −A−1
B AN, (2)
Z =

¯A
In−m

and Q = (qij, i, j ∈ K) = ZT ¯DZ,
(3)
where In−m represents the identity matrix of order n − m
and ¯D = P T DP, P is the permutation matrix obtained by
permuting columns of the identity matrix In with respect to
the partition (JB, JN). Note that the matrix Q is negative
semideﬁnite. Indeed, ∀y ∈ Rn−m, we have
yT Qy = yT ZT ¯DZy = (Zy)T ¯D(Zy) ≤ 0.
Moreover, we note that the diagonal elements of a negative
semideﬁnite matrix are less than or equal to zero. Indeed,
∀y ∈ Rn−m : yT Qy ≤ 0. Particularly, for y = ek, where ek
is the vector with all its components equal to zero except
for the kth component, it is equal to 1. Hence, we get
yT Qy = qkk ≤ 0.
• A vector d ∈ Rn is said to be a feasible direction for
problem (1) if it satisﬁes Ad = 0.
III.
INCREMENT FORMULA OF THE OBJECTIVE FUNCTION
Using results presented in [12][13][14] on linear and
convex quadratic programming, we can deduce the increment
formula of the objective function for the concave quadratic
programming problem (1), when we move from a BFS to an
adjacent one.
 Let AB be a basis matrix for problem (1) and x =
xB
xN

=
 ¯b
0

the corresponding BFS. Let ¯x =

¯xB
¯xN

be an arbitrary feasible solution (not necessarily basic) and
f(¯x) the value of the objective function at ¯x.
Since ¯x is feasible, we can write:
AB¯xB +AN ¯xN = b ⇔ ¯xB = A−1
B b−A−1
B AN ¯xN = ¯b+ ¯A¯xN.
(4)
The objective function value at ¯x is
f(¯x) =

cB
cN
T 
¯xB
¯xN

+ 1
2

¯xB
¯xN
T
¯D

¯xB
¯xN

(5)
By replacing the expression of ¯xB in equation (5), we get
f(¯x)
=
cT
B¯b + (cT
N + cT
B ¯A)¯xN + 1
2
 ¯b
0
T
¯D
 ¯b
0

+
 ¯b
0
T
¯D
 ¯A¯xN
¯xN

+1
2
 ¯A¯xN
¯xN
T
¯D
 ¯A¯xN
¯xN

=
cT
B¯b + 1
2
 ¯b
0
T
¯D
 ¯b
0

+
"
cT
N + cT
B ¯A +
 ¯b
0
T
¯D

¯A
In−m
#
¯xN
+1
2 ¯xT
N

¯A
In−m
T
¯D

¯A
In−m

¯xN.
=
f(x) +
"
cT
N + cT
B ¯A +
 ¯b
0
T
¯DZ
#
¯xN
+1
2 ¯xT
NZT ¯DZ¯xN.
Thus, the objective function increment takes the following ﬁnal
form:
f(¯x) − f(x) = lT ¯xN + 1
2 ¯xT
NQ¯xN,
(6)
89
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-655-2
INFOCOMP 2018 : The Eighth International Conference on Advanced Communications and Computation

where
lT = cT
N + cT
B ¯A + vT Z, with vT =
 ¯b
0
T
¯D.
(7)
Remark 1.
• If we denote by ¯D = ( ¯dij, i, j ∈ J) and v = (vj, j ∈ J),
then the components of the n-vector v are computed as follows:
vj =
m
X
i=1
¯bi ¯dij, j = 1, 2, . . . , n.
• When D = 0, the problem (1) becomes a linear program
and the increment of the objective function becomes:
f(¯x)−f(x) = lT ¯xN = (cT
N+cT
B ¯A)¯xN = (cT
N−cT
BA−1
B AN)¯xN.
So, the vector l is equal to the reduced costs vector in the
simplex method of linear programming.
Let us denote by
JN = {j1, j2, . . . , jn−m} and JB = {j′
1, j′
2, . . . , j′
m}.
So, if k ∈ K, then jk represents the index of position k in
JN, and if s ∈ I, then j′
s represents the index of position s in
JB.
We introduce the following notations:
ajk = (aijk, i ∈ I) = −A−1
B ajk.
(8)
θik = min
i∈I θk
i , with θk
i =
(
−¯bi
¯aijk ,
if ¯aijk < 0;
+∞,
otherwise.
(9)
θk
0 =







−2lk
qkk ,
if qkk < 0;
−∞,
if qkk = 0 and lk > 0;
+∞,
otherwise.
(10)
The following theorem gives us a sufﬁcient and necessary
condition for the local optimality of the BFS x.
Theorem 2. The condition
∀k ∈ K : θk
0 ≥ θik
(11)
is sufﬁcient for the local optimality of the BFS x and it is also
necessary when x is nondegenerate.
Proof:
Sufﬁcient condition. Let ¯x be an arbitrary adjacent BFS to x
and assume that the basis matrix corresponding to ¯x is ¯AB =
A(I, ¯JB), where ¯JB = (JB \ {j′
s}) ∪ {jr}.
We assume that condition (11) holds. We have
f(¯x) − f(x) = lT ¯xN + 1
2 ¯xT
NQ¯xN.
However, for the BFS ¯x, we have ¯xjk = 0, k ̸= r and ¯xjr =
θs ≥ 0. Since θr
0 ≥ θs, we get
f(¯x) − f(x) = lr¯xjr + 1
2qrr¯x2
jr = lrθs + 1
2qrrθ2
s ≥ 0.
Necessary condition. Assume that x is nondegenerate and the
condition (11) is not satisﬁed, i.e.,
∃r ∈ K : θr
0 < θir
(12)
Thus, we can move to a new BFS ¯x, such that f(¯x) < f(x). In-
deed, we can improve the point x =

xB
xN

=

A−1
B b
0

,
by increasing the value of the component xjr by a positive
number θ and letting the other nonbasic components equal to
zero. Thus, we obtain a new point ¯x =

¯xB
¯xN

, such that
¯xN = xN + θer = θer,
¯xB = ¯b + ¯A¯xN = ¯b + ¯A(θer) = ¯b + θ¯ajr,
where er represents the (n−m)-vector of zeros except for the
component r, it is equal to 1. The positif number θ can be
chosen in such a way that the new point ¯x remains feasible:
¯xN = θer ≥ 0, ¯xB = ¯b + θ¯ajr ≥ 0,
and the objective function decreases:
f(¯x) − f(x) = lrθ + 1
2qrrθ2 < 0.
Indeed, using the nondegeneracy assumption (¯bi > 0, i ∈ I),
we get θs = θir = mini∈I{θr
i } > 0, then condition θr
0 < θs
implies that the number θ can be chosen in the interval ]0, θs]
if θr
0 ≤ 0, or in the interval ]θr
0, θs], otherwise. Hence, we get
¯x ≥ 0 and f(¯x) < f(x). Therefore, the BFS x is not a local
minimizer.
IV.
AN ITERATION OF SASIR
Let x be an initial BFS of the problem (1). An iteration
of the algorithm SASIR consists in moving from the BFS x
to a new BFS ¯x, with f(¯x) ≤ f(x) following the descent
feasible direction used in the the simplex method for linear
programming. The algorithm stops when the local optimality
criterion (11) is satisﬁed.
A. Computing the feasible descent direction
We deﬁne the following set of indices:
K∗ = {k ∈ K : θk
0 < θik}.
(13)
Two cases can occur:
Case 1. If K∗ = ∅, then the algorithm stops. The BFS x is a
local minimizer.
Case 2. If K∗ ̸= ∅, then we choose an index jr ∈ JN that
satisﬁes the smallest index rule, i.e., we choose the index r
that satisﬁes
r = min{k, k ∈ K∗}
and we compute the feasible descent direction d as follows:
dN = er, dB = ¯ajr,
(14)
where jr corresponds to the index of position r in JN.
Note that the direction d is a feasible direction because it
satisﬁes Ad = 0.
We move along the direction d with a steplength θ∗ = θir,
to achieve a new BFS ¯x = x + θ∗d, with a better objective
function value:
f(¯x) = f(x) + lrθ∗ + 1
2qrr(θ∗)2 ≤ f(x).
90
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-655-2
INFOCOMP 2018 : The Eighth International Conference on Advanced Communications and Computation

Then, we proceed to the change of the basis:
¯JB = (JB \ {j′
s}) ∪ {jr}, s = ir.
We start a new iteration with the new BFS ¯x and the new basis
matrix ¯AB = A(I, ¯JB).
B. Algorithm SASIR
(1)
Compute ¯b, ¯A, Z, Q and the vector l with formu-
las (2)-(3) and (7);
(2)
(Computing the entering index)
For every index k in K, compute θik and θk
0 with
relations (9)-(10); then compute the set K∗ with
relation (13);
Cas 1. If K∗ = ∅, then the algorithm stops with
a local minimizer x.
Cas 2. If K∗ ̸= ∅, then
compute the entering index jr corresponding to
the index of position r in JN, with the smallest
index rule, i.e., r = min{k, k ∈ K∗}, and set the
leaving index s = ir;
(3)
(Computing the direction and the steplength)
Compute the direction d with formula (14);
set the steplength θ∗ = θs = θir;
(4)
(Change of the current solution)
Set
¯x = x + θ∗d, f(¯x) = f(x) + lrθ∗ + 1
2qrr(θ∗)2;
(5)
(Change of the current basis)
Set
¯JB = (JB \ {j′
s}) ∪ {jr};
(6)
Set x = ¯x, JB = ¯JB and go to step (1).
Remark 2.
• Under the nondegeneracy assumption, our algorithm moves
from one BFS x to a new one ¯x, with f(¯x) < f(x). Since
the number of extreme points of the convex polyhedron S is
ﬁnite, our algorithm ﬁnds a local optimum in a ﬁnite number
of steps.
• We can choose the entering index jr with the best improve-
ment rule:
∆fr = max{∆fk, k ∈ K∗}, with ∆fk = lkθik + 1
2qkkθ2
ik.
This rule gives the maximal local improvement of the objective
function. In other words, we move from the current extreme
point to the adjacent one with the best objective function value.
However, some preliminary numerical experiments show that
this version of the algorithm consumes much time than the
version SASIR presented above.
C. Numerical example
Consider the following concave quadratic program [14]:
min
f(x) = −x1 − 2x2 − x2
1 − 3x2
2,
subject to
−x1 + x2 + x3 = 3,
x1 − x2 + x4 = 6,
x1 + 2x2 + x5 = 12,
xj ≥ 0, j = 1, 5.
We have
I = {1, 2, 3}, J = {1, 2, 3, 4, 5}, K = {1, 2}.
c =





−1
−2
0
0
0




 , D =





−2
0
0
0
0
0 −6
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0




 ,
A =
 −1
1
1
0
0
1 −1
0
1
0
1
2
0
0
1
!
, b =
 3
6
12
!
.
We start SASIR with the following initial BFS:
JB = {3, 4, 5}, JN = {1, 2}, xT
B = (3, 6, 12),
xT
N = (0, 0), xT = (0, 0, 3, 6, 12), f(x) = 0.
First iteration :
We have
AB = I3, AN =
 −1
1
1 −1
1
2
!
, cT
B = 0R3, cT
N = (−1, −2).
Computing the vectors ¯b, l and the matrices ¯A, Z, Q:
¯b = A−1
B b = b, ¯A = −A−1
B AN =
 1 −1
−1
1
−1 −2
!
,
Z =

¯A
I2

=





1 −1
−1
1
−1 −2
1
0
0
1




 ,
¯D =





0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 −2
0
0
0
0
0 −6




 , Q = ZT ¯DZ =

−2
0
0 −6

,
v =
 ¯b
0
T
¯D = 0R5, lT = cT
N + cT
B ¯A + vT Z = (−1, −2).
Computing the entering and leaving indices:
We compute θik and θk
0 for k ∈ {1, 2}:
θi1 = min{θ1
1, θ1
2, θ1
3} = min{+∞, 6, 12} = θ1
2 = 6,
θi2 = min{θ2
1, θ2
2, θ2
3} = min{3, +∞, 6} = θ2
1 = 3,
θ1
0 = −2l1
q11
= −1, θ2
0 = −2l2
q22
= −2/3.
So,
K∗ = {k ∈ K : θk
0 < θik} = {1, 2}.
We choose r with the smallest index rule, we get
r = min{k, k ∈ K∗} = min{1, 2} = 1 ⇒ r = 1, s = i1 = 2.
So, the entering index is the index of position r = 1 in JN,
i.e., j1 = 1 and the leaving index is the index of position s in
JB, i.e., j′
2 = 4.
91
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-655-2
INFOCOMP 2018 : The Eighth International Conference on Advanced Communications and Computation

Computing the feasible descent direction and the steplength:
We have
dN = (0, 1)T , dB = ¯a2 = (−1, 1, −2)T ,
dT = (1, 0, 1, −1, −1), θ∗ = θi1 = 6.
Computing the new solution ¯x and the new value of the
objective function f(¯x):
¯x = x + θ∗d = (6, 0, 9, 0, 6)T , f(¯x) = −42 < f(x).
Change of the basis:
¯JB = (JB \ {4}) ∪ {1} = {3, 1, 5}, ¯JN = {4, 2}.
Second iteration :
We have
JB = {3, 1, 5}, JN = {4, 2}, xT = (6, 0, 9, 0, 6),
f(x) = −42, AB =
 1 −1
0
0
1
0
0
1
1
!
, AN =
 0
1
1 −1
0
2
!
,
cB = (0, −1, 0)T , cN = (0, −2)T .
Computing the matrices ¯A, Z, Q and the vector l:
Z =





−1
0
−1
1
1 −3
1
0
0
1




 , Q =

−2
2
2 −8

, l =

13
−15

.
We compute θik and θk
0, k ∈ {1, 2}:
θi1 = {9, 6, +∞} = θ1
2 = 6, θ1
0 = 13,
θi2 = {+∞, +∞, 2} = θ2
3 = 2, θ2
0 = −15/4.
So,
K∗ = {k ∈ K : θk
0 < θik} = {2}.
We choose r with the smallest index rule, we get
r = min{k, k ∈ K∗} = 2 ⇒ r = 2, s = i2 = 3.
So, the entering index j2 = 2 and the leaving index is j′
3 = 5.
Computing the feasible descent direction and the steplength:
dT = (1, 1, 0, 0, −3), θ∗ = θi2 = 2.
Computing the new BFS ¯x and the new value of the objective
function f(¯x):
¯x = x + θ∗d = (8, 2, 9, 0, 0)T , f(¯x) = −88 < f(x).
Change of the basis:
¯JB = (JB \ {5}) ∪ {2} = {3, 1, 2}, ¯JN = {4, 5}.
Third iteration :
We have
JB = {3, 1, 2}, JN = {4, 5}, xT = (8, 2, 9, 0, 0),
f(x) = −88, AB =
 1 −1
1
0
1 −1
0
1
2
!
, AN =
 0
0
1
0
0
1
!
,
cB = (0, −1, −2)T , cN = (0, 0)T .
Computing the matrices Z, Q and the vector l:
Z =





−1
0
−2/3 −1/3
1/3 −1/3
1
0
0
1




 , Q =

−14/9
2/9
2/9 −8/9

,
l = (20/3, 31/3)T .
We compute θik and θk
0, k ∈ {1, 2}:
θi1 = {9, 12, +∞} = θ1
1 = 9, θ1
0 = 60/7,
θi2 = {+∞, 24, 6} = θ2
3 = 6, θ2
0 = 93/4.
So,
K∗ = {k ∈ K : θk
0 < θik} = {1}.
We choose r with the smallest index rule, we get
r = min{k, k ∈ K∗} = 1 ⇒ r = 1, s = i1 = 1.
So, the entering index is j1 = 4 and the leaving index is
j′
1 = 3.
The feasible descent direction and the steplength are:
dT = (−2/3, 1/3, −1, 1, 0), θ∗ = θi1 = 9.
Computing the new BFS ¯x and the new value of the objective
function f(¯x):
¯x = x + θ∗d = (2, 5, 0, 9, 0)T , f(¯x) = −91 < f(x).
Change of the basis:
¯JB = (JB \ {3}) ∪ {4} = {4, 1, 2}, ¯JN = {3, 5}.
Fourth iteration:
We have
JB = {4, 1, 2}, JN = {3, 5}, xT = (2, 5, 0, 9, 0), f(x) = −91.
AB =
 0 −1
1
1
1 −1
0
1
2
!
, AN =
 1
0
0
0
0
1
!
,
cB = (0, −1, −2)T , cN = (0, 0)T .
Computing the matrices Z, Q and the vector l:
Z =





−1
0
2/3 −1/3
−1/3 −1/3
1
0
0
1




 , Q =

−14/9 −2/9
−2/9 −8/9

,
l = (22/3, 37/3)T .
We compute θik and θk
0, k ∈ {1, 2}:
θi1 = min{9, +∞, 15} = θ1
1 = 9, θ1
0 = 66/7,
θi2 = min{+∞, 6, 15} = θ2
2 = 6, θ2
0 = 111/4.
So,
K∗ = {k ∈ K : θk
0 < θik} = ∅.
Therefore, the local optimal BFS and the corresponding ob-
jective function value are:
x∗ = (2, 5, 0, 9, 0)T , f(x∗) = −91.
Let us remark that the local minimizer found by our algorithm
in this example is also global.
92
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-655-2
INFOCOMP 2018 : The Eighth International Conference on Advanced Communications and Computation

V.
NUMERICAL EXPERIMENTS
In order to compare our algorithm with the algorithm of
Rusakov, we have developed an implementation with MAT-
LAB2017a on a PC with a processor Intel Pentium Dual-Core
2.20 GHz and 4 Go of RAM. The Rusakov’s algorithm is
a branch and bound algorithm which uses the Tuy cut. It
is implemented in his free software [5][11]. Note that the
Rusakov’s test problems have one constraints and n bounded
variables and the available free version of the software is
limited to one constraint and 20 bounded variables, that is
why we have done numerical experiments on small size test
problems. Numerical results on Rusakov’s test problems are
presented in Table I.
In order to test the performance of our algorithm on
medium size test problems, we have generated concave
quadratic test problems with known global optimum using the
generation procedure presented in [15], see Table II.
The notations in the ﬁrst row of Tables I and II : m,
n, niterj, optj, cputj represent respectively number of con-
straints, number of variables, number of iterations, the opti-
mum found, the CPU time for Algorithm j, where Algorithm
1 is the SASIR algorithm and Algorithm 2 is the Rusakov’s
algorithm. In Table II, gopt represents the global optimum
of the corresponding test problem and ”Mean” represents the
average CPU time and the average number of iterations for
each problem size.
TABLE I: NUMERICAL RESULTS ON RUSAKOV’S TEST PROBLEMS.
n
Algorithm 1 (SASIR)
Algorithm 2 ( Rusakov)
niter1
opt1
cput1 (s)
opt2
cput2 (s)
5
4
-11.0694
0.0031
-11.0694
1.1298
10
6
-40.8382
0.0041
-40.8382
2.0481
15
9
-88.9456
0.0051
-88.9456
6.1074
18
10
-127.0726
0.0075
-127.0726
9.7121
20
11
-156.1234
0.0092
-156.1234
11.5236
TABLE II: NUMERICAL RESULTS ON RANDOMLY GENERATED
TEST PROBLEMS.
Prob
m × n
niter1
opt1
gopt
cput1 (s)
1
100 × 120
40
-9.5894
-6.2500
0.1439
2
100 × 120
2
-4.3225
-0.0625
0.0217
3
100 × 120
30
-1.5625
-1.5625
0.2773
4
100 × 120
39
-4.0272
-4.0272
0.5287
5
100 × 120
3
-2.2500
-0.1600
0.0247
6
100 × 120
6
-1.9492
-1
0.0330
7
100 × 120
14
-2.9417
-2.2500
0.0564
8
100 × 120
2
-23.6580
-0.3600
0.0240
9
100 × 120
18
-3.0625
-3.0625
0.0552
10
100 × 120
20
-1.7778
-1.7778
0.0786
Mean
100 × 120
37.2
-
-
0.0988
11
200 × 240
2
-12.2500
-0.1600
0.0426
12
200 × 240
2
-11.6224
-0.3600
0.0394
13
200 × 240
18
-10.5901
-4.0000
0.4594
14
200 × 240
2
-9.8212
-0.4900
0.0386
15
200 × 240
2
-15.4605
-0.3600
0.0387
16
200 × 240
2
-57.4326
-0.0400
0.0389
17
200 × 240
27
-4
-4
0.5332
18
200 × 240
2
-11.3498
-0.6400
0.0454
19
200 × 240
5
-39.3321
-0.1600
0.0824
20
200 × 240
2
-1.4400
-0.2304
0.0409
Mean
200 × 240
6.4
-
-
0.1359
We have started our algorithm by the extreme point, with
objective function equal to zero. Table I shows clearly that
our algorithm has succesfully found the global optimum for
the Rusakov’s test problems. Moreover, the simplex algorithm
with the smallest index rule outperforms the branch and bound
algorithm of Rusakov in terms of CPU time. Table II shows
that our algorithm has found the global optimum for 5 gener-
ated test problems in a short CPU time (test problems 3,4,9,10
and 17). However, for other test problems, our algorithm gives
a local minimizer which can be used for the initialization of
the global optimization algorithms.
VI.
CONCLUSION
In this work, we have adapted the simplex algorithm of
linear programming for ﬁnding a local optimum of a concave
quadratic function subject to linear and nonnegativity con-
traints. In order to stop the algorithm, we suggested a simple
sufﬁcient and necessary condition for local optimality of the
current extreme point. Numerical experiments on Rusakov and
randomly test problems show that our algorithm is very fast
and can ﬁnd the global optimum for some problems. In a future
work, we will combine our algorithm with an existing global
optimization algorithm in order to ﬁnd the global optimum
of medium and large-scale concave quadratic programming
problems.
REFERENCES
[1]
E. L. Lawler, “The Quadratic Assignment Problem,” Management Sci.,
vol. 9, no. 4, 1963, pp. 586–599, ISSN: 1526-5501.
[2]
A. I. Rusakov, “An Improved Reduction Algorithm to Check Hypothe-
ses for the Multicollinear Regression Model,” Automation and Remote
Control, vol. 62, no. 5, 2001, pp. 762–771, ISSN: 1608-3032.
[3]
H. Tuy, “Concave Programming under Linear Constraints,” Dokl. Akad.
Nauk SSSR English translation in Soviet Math. Dokl., vol. 5, 1964, pp.
1437–1440, ISSN: 1531-8362.
[4]
R. Horst, “An Algorithm for Nonconvex Programming Problems,”
Math. Programming, vol. 10, 1976, pp. 312–321, ISSN: 1436-4646.
[5]
A. I. Rusakov, “Concave programming under the simplest linear
constraints,” Computational Mathematics and Mathematical Physics,
vol. 43, no. 7, 2003, pp. 951–960, ISSN: 0044-4669.
[6]
H. Konno, “Maximization of a Convex Quadratic Function under Linear
Constraints,” Math. Programming, vol. 11, 1976, pp. 117–127, ISSN:
1436-4646.
[7]
K. L. Hoffman, “A Successive Underestimating Method for Concave
Minimization Problems,” Ph.D. thesis, The George Washington Univer-
sity, 1975.
[8]
H. Konno, C. Gao, and I. Saitoh, ”Cutting plane/tabu searh algorithms
for low rank concave quadratic programming problems”.
Journal of
Global Optimization, vol. 13, no. 3, 1998, pp. 225–240, ISSN: 1573-
2916.
[9]
D. G. Luenberger and Y. Ye, Linear and nonlinear programming. Third
edition, New York, NY, USA: Springer-Verlag, 2008, ISBN: 978-0-387-
74502-2.
[10]
G. B. Dantzig, Linear Programming and Extensions.
Princeton
University Press, Princeton, N.J., 1998, ISBN: 978-0-691-05913-6.
[11]
A.
I.
Rusakov,
“Concave
software
manual,”
URL:
http://www.rusakov.donpac.ru/index1.htm [accessed: 2018-05-02].
[12]
N. Ikheneche, Support Method for the Minimization of a Convex
Quadratic Function.
Master thesis, University of Bejaia (in french),
2004, (in french).
[13]
M. Bentobache and M. O. Bibi, Numerical Methods of Linear and
Quadratic Programming: Theory and Algorithms.
French Academic
Editions, Germany, 2016, ISBN: 978-3-8416-4112-0 (in french).
[14]
A. Chikhaoui, B. Djebbar, A. Belabbaci, and A. Mokhtari, ”Optimiza-
tion of a quadratic function under its canonical form”.
Asian journal
of applied sciences, vol. 2, no. 6, 2009, pp. 499–510, ISSN: 1996-3343.
[15]
N. V. Thoai, “On the construction of test problems for concave
minimization algorithms,” Journal of Global Optimization, vol. 5, no. 4,
1994, pp. 399–402, ISSN: 1573-2916.
93
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-655-2
INFOCOMP 2018 : The Eighth International Conference on Advanced Communications and Computation

