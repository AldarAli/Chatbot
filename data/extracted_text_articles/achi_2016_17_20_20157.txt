Usability Analysis in the Liquid Galaxy platform
Ismael Arroyo∗, Francesc Gin´e†, Concepci´o Roig‡
Distributed Computing Group, University of Lleida
Lleida, Spain
Email: ∗ismael.arroyo@udl.cat, †sisco@diei.udl.cat, ‡roig@diei.udl.cat
Toni Granollers§
Research Group on Human-Computer
Interaction and Data Integration, University of Lleida
Lleida, Spain
Email: §antoni.granollers@udl.cat
Abstract—With the falling price of the technology and the growing
trend of cluster display-walls, User Experience (UX) becomes a
topic to be taken into account with this new kind of systems.
In this context, we present a study of a speciﬁc cluster display-
wall, named Liquid Galaxy and developed by Google, that covers
the three main aspects of usability: effectiveness, efﬁciency and
satisfaction. The study was done by means of a set of tasks that
users completed during UX tests, with post-task and post-tests
questionnaires, while the system performance was measured. The
results show positive feelings from users while using the system. In
addition, we relate both system performance and user behavior
to estimate whether the system will perform satisfactorily for
users. According to our observations, we can see that, in general,
there is an intrinsic relationship between the goodness of feelings
and the quality of the performance metrics. This relationship can
facilitate the inclusion of cluster display-wall systems in a broad
range of scenarios with right levels of UX just by analyzing their
computational characteristics.
Keywords–Liquid Galaxy; User Experience; Cluster Display-
Wall.
I.
INTRODUCTION
Cluster-based solutions for creating large displays have
recently generated a lot of interest [1]. Such displays have
the potential to put high-performance visualization within the
reach of more users. The most valuable potential of cluster
display-walls is their pixel density, as the images are FullHD
on every monitor, providing the system with an incredible
overall resolution given the low cost of the hardware required.
These systems consist of a number of commodity PCs that
are interconnected over a LAN or via low-latency networks
like the Myrinet [2]. Thus, visualization of the images across
screens in cluster display-walls is carried out by synchronizing
the data between the computers in the cluster. Nowadays,
besides the fact that conventional PCs can be equipped with
powerful consumer graphics cards with multiple outputs, the
availability of software packages for clusters makes setting up
cluster display-walls affordable. Thus, cluster-based displays
are affordable, scalable in resolution, and easy to maintain.
This opens a wide range of new possibilities for their use in
everyday scenarios in such areas as entertainment or education.
This rising trend for cluster display-walls caught our atten-
tion and brought up some questions about how the end-user
would feel when using and experiencing these new visualiza-
tion systems, and how we could estimate the user experience
acknowledging the system parameters. In order to answer both
questions, we performed user tests to know their behavior
by gathering satisfaction, effectiveness and efﬁciency data.
Furthermore, we found a relation between them that enabled
us to estimate how users would behave with a cluster display-
wall with speciﬁc performance characteristics. We noted that
there are some works in the literature [3][4] that deal with the
Figure 1. Liquid Galaxy.
desired results separately, but we want to go one step further
by relating both metrics.
In this work, we were interested in testing the usability re-
lated aspects of a particular cluster display-wall system, named
Liquid Galaxy [5], especially on acquiring data from real
users. The Liquid Galaxy technology is a cluster display-wall
hemisphere infrastructure developed by Google, running the
well-known Google Earth application [6], which is an example
of a master-slave application. The Liquid Galaxy system is
usually made up of three, ﬁve or eight displays, each connected
to a computer node, and is designed to provide an immersive
geographic visualization. Figure 1 shows the Google Liquid
Galaxy infrastructure installed in the Technological Park in
the city of Lleida (Spain) [7]. The Figure also shows the most
commonly used controller, named 3D Space Navigator.
With the Liquid Galaxy infrastructure, in the present
work, we carried out a pre-prepared test with 27 people. It
consisted of navigating to some well-known places around
the world. Additionally, everyone answered some questions
about their experience while using the system. Then, we
related these answers, which made up a set of qualitative
performance metrics, to the quantitative performance metrics
that we measured during the test. In order to measure this
quantitative metrics, we have used the Visualization Rate (VR)
performance parameter which has been presented in previous
works [8][9]. VR is a relation between the CPU time used by
the system to load all the multimedia data and the total time
of the visualization.
In general, the results show that the vast majority of the
participants were satisﬁed and had positive feelings using the
system throughout the test. Additionally, we have analyzed
the relationship between the user’ satisfaction levels and the
VR performance metric. The obtained correlation shows that
the usability of the system can be roughly estimated by using
only the VR in any given ﬁeld test. This relationship can be
345
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

determinant in facilitating an extensive use of such kind of
systems, given that the knowledge of system characteristics,
directly related to system performance, can give knowledge of
what the UX [10] of using the system will be.
The paper is structured as following. Section II enumerates
different approaches that other authors developed and describes
last works that are and improvement above them. In Section
III is presented the case study of Liquid Galaxy. In Section
IV is described the method and results obtained for the tests.
Section V summarizes and relates the obtained data from the
previous tests. Finally, VI concludes the paper and discusses
future directions.
II.
BACKGROUND AND RELATED WORK
Within the extensive literature on large displays, we fo-
cused on studies about User Experience on cluster display-
walls. In general, research into this kind of system is focused
on the study of the UX, such as the ones presented next.
Bi and Balakrishnan [3] focused on user behavior in
large-scale displays and demonstrated that the users preferred
this kind of system over single desktops. The authors made
participants perform everyday work in a week-long study and
compare it with single or dual-monitor desktops. The results
are all about subjective opinions and observations from the
users, with notes about how they used the system. This study
is focused on the preferences of the users while ignoring the
system performance.
Tan et al. [11] studied how their system, called Infocockpit,
can make information memorable. Infocockpit is a projector-
based display-wall with ambient visual and auditory displays
that engage human memory for location. In their work, they
made users complete semantic tasks that consist in remember-
ing pairs of words and then recalling them. The results are
both quantitative and qualitative, as they studied how many
words the users could remember in Infocockpit compared with
on a desktop computer. Also, participants answered a user
satisfaction questionnaire after the tasks. Nevertheless, this
approach does not try to relate both metrics using the system
performance.
Ball and North [4] studied the effectiveness of a 3x3
large tiled display compared with two smaller displays. They
concluded that display-walls that use physical navigation sig-
niﬁcantly outperform smaller displays that use pan and zoom
navigation. They tested both environments with a task with
quantitative results based on ﬁnding targets of different sizes.
Also, they introduced observations that the users made during
the test, adding a subtle qualitative study into the research.
Despite this, they did not take the system performance into
account.
Despite the growing literature on qualitative or quantitative
views of the use of a cluster display-wall [12] [13] [14], to
the best of our knowledge, there are no studies that focus on
establishing a relationship between both metrics. In general,
these studies are focused on the results of speciﬁc tasks, like
completion time or scores, but not on system performance and
how it affects user behavior and/or experience.
In previous works, we analyzed the performance of the Liq-
uid Galaxy System in relation to different system parameters
such as the scalability, heterogeneity, CPU, RAM, Network
requirements, etc. However, we did not tackle neither the
quality of use from the user point of view nor its relation
Figure 2. Measuring Usability.
with the system performance.
In this paper, our goal is to try to relate the Usability
aspects of the User Experience (satisfaction, effectiveness and
efﬁciency [15]) of the Liquid Galaxy cluster. Thus, it will
enable us to predict before-hand if a system will perform
well enough from the user point of view without the need
of carrying out any user test.
III.
LIQUID GALAXY
Liquid Galaxy [5] is a cluster display-wall that started as
a Google project, made up of a custom number of computers,
where every node has a single monitor. This system was origi-
nally built to run Google Earth [6] and to create an immersive
experience for the user. Liquid Galaxy lets you navigate around
the globe with its 6-axis controller, allowing you to instantly
zoom in, zoom out, and turn in a completely ﬂuid motion.
Likewise, the immersive visualization environment of Liquid
Galaxy opens up this kind of system to be used by a wide
range of applications that can beneﬁt from this feature. Some
examples of applications that can be run in this system are
WebGL applications like Aquarium [16] or Peruse-a-rue [17],
video streaming [18] and video-games like Quake 3 Arena
[19].
In the Liquid Galaxy system, every node is connected to the
same network and the nodes share a distributed cache named
Squid [20], included in the Liquid Galaxy repository. This is
used to cache http objects for repetitive use, thus improving
throughput, as Internet data requests would be reduced.
Depending on the application to be used, the interaction
with the system can be carried out using devices such as a
Mouse, Keyboard, Leapmotion, Myo or 3D Space Navigator
(as in our case), among others.
IV.
USER STUDY
We had the necessity to study the Liquid Galaxy to know
how users react to the system and how to try to predict an
estimated overall satisfaction by relating it to the performance
metrics. In order to achieve this, the system was tested by
means of the usability attributes of satisfaction, effectiveness
and efﬁciency [15]. To meet this aim, users had to answer
some post-task questions about how they felt after completing
each task and a post-test questionnaire about their feeling while
using the system. At the same time, the system performance
was monitored throughout the tests. This objective is depicted
in Figure 2, where it can be seen how effectiveness and
efﬁciency can be monitored using the performance parameter
VR, and satisfaction is obtained by using questionnaires that
users had to answer.
346
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

Figure 3. 3D Space Navigator Movement Options.
With these measurements, system performance and users
satisfaction, we aimed to relate both metrics to be able to
predict the average usability values for a speciﬁc cluster
display-wall with given hardware characteristics.
The different aspects involved in the usability tests that
were carried out to analyze the system performance and the
users satisfaction are exposed next.
A. Environment
1) Cluster Display-Wall: The environment on which the
tests were performed was an ofﬁce equipped with a Liquid
Galaxy system made up of 3 nodes. The monitors used in
the cluster were a triplet of 32” monitors set vertically in a
semi-hemispheric way. The interaction controller used was a
3D Space Navigator (Figure 3), which is the most widely-used
controller in all Liquid Galaxy setups. This device is able to
displace the view but also rotate it on all 3 axes, making it the
most suitable controller for navigating through 3D scenarios.
2) Facilitator:
A facilitator was running the test and
guiding the participants through it. He/she attended to the
participants, explained the goals of the test, the consent form
and everything to make them feel comfortable. Once the test
started, the facilitator could only answer speciﬁc questions
or give subtle advice when the participant was struggling to
complete a task for a long time.
B. Participants
The set of participants that carried out the tasks included a
range of professional proﬁles, ages and skills with controllers.
There were 27 participants in the test, of whom 15 were
females and 12 males. Their age ranged from 12 to 68.
Table I shows some information about the participants
grouped by age ranges. This corresponds to information about
their occupation and the navigation skills they think to have
before the test in a range from 0 to 10.
C. Test
Each participant was asked to do a series of four user tasks.
The test used to study the User Experience of the system was a
semi-guided tour, using Google Earth, around different places
across the globe.
The device used to carry out the tasks of the Liquid Galaxy
was the 3D Space Navigator of Figure 3. As we knew that this
is a different and more complex controller than a common
mouse, we dedicated some minutes helping the participants to
use the device before starting the test to avoid any initial fear.
The users were also informed that all the information would
be given through the monitors during the test and that they will
have to answer the questions from the facilitator by voice,
who wrote them down instead of the participant to avoid
interference with the test.
For each participant, the test was composed of four tasks,
each one corresponding to navigating to one of the following
well-known places in the world:
Figure 4. Tour Flowchart - T1a Statue of Liberty from above - T1b Statue of
Liberty’s crown - T2a Barcelona Football Club stadium - T2b The stadium’s
screen - T3a Sydney Harbour - T3b Luna Park - T4a Lleida bridge. Each
picture is the composition of the 3 screens of the system.
347
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

TABLE I. BASIC PARTICIPANT INFORMATION.
Age range
Participants
Avg Skill Level
Occupations
12 to 16
4
7
4 Students.
16 to 21
3
9
2 Computer Science Programmers, 1 Computer Science Engineer.
22 to 35
9
6
3 Computer Science Students, 1 Pre-School Ed. Student, 1 Economics
Student, 1 High School Student, 1 Restorer, 1 Industrial Engineer, 1
Computer Science Engineer.
36 to 68
11
4
5 Teachers, 4 management staff, 1 Customer Service Worker, 1 Shop
Assistant.
TABLE II. DESCRIPTION OF TEST TASKS.
Task
City
Sub-task
Positioning
Questions
T1
New York
T1a
Automatic
How many vertices does the base of the Statue of Liberty have?
T1b
Manual
How many points does the crown of the Statue of Liberty have?
T2
Barcelona
T2a
Automatic
What does it say on the Barcelona Football Club stadium stands?
T2b
Manual
What brand is the screen in the Barcelona Football Club stadium?
T3
Sydney
T3a
Automatic
How many buildings is the Sydney Opera House made of?
T3b
Manual
Find a structure nearby with a clown’s face on it. What does it say
above it?
T4
Lleida
T4
Manual
How many buses are crossing the bridge in front of the cathedral in
Lleida?
TABLE III. POST-TEST QUESTIONNAIRE.
Questions
Description
FQ1
”From 0 to 10, mark your personal skill at
using joysticks and remote control devices”
FQ2
”From 0 to 10, mark the ease of use when
using the system”
FQ3
”From 0 to 10, mark how much you think
that you had to learn to use the system”
FQ4
”From 0 to 10, mark how much technical
help you think would be needed to use the
system”
FQ5
”From 0 to 10, mark if you believe that
everyone could learn the system quickly”
FQ6
”From 0 to 10, mark your personal satis-
faction when using the system”
FQ7
”From 0 to 10, mark how secure you felt
when interacting with Liquid Galaxy”
FQ8
”From 0 to 10, mark the degree of difﬁculty
that you found for using this system”
FQ9
”From 0 to 10, mark your perception of
system complexity”
•
T1: New York (USA): Statue of Liberty.
•
T2: Barcelona (Spain): Barcelona Football Club Sta-
dium.
•
T3: Sydney (Australia): Bay of the Opera House.
•
T4: Lleida (Spain): City where the participants live.
Tasks T1, T2 and T3 were composed of two sub-tasks. In the
ﬁrst sub-task (Tia), the system positioned itself automatically
Figure 5. Emotional Choices (source LemTool).
at the ﬁrst place in the city, so that participants had to answer
the related question. In this way, the participants did not use the
controller for the ﬁrst part of each task. However, in the second
sub-task (Tib), they had to use the controller to reposition
the view to be able to answer the related question. Task T4
consisted of a free ﬂight from Sydney (T3b) to a speciﬁc point
of interest in Lleida city. Figure 4 shows the pictures of the 7
sub-tasks that correspond to the ﬂow of activities in the test.
Table II shows the questions that users had to answer to
complete each sub-task, thus completing each of the tasks.
Even though the real answers were not so important, they
were a way of forcing the user to interact with the system and
show interest when doing the tasks. The tasks were designed to
increase the difﬁculty gradually on each step and thus help the
users’ ability to control the 3D Space Navigator to progress.
After each task, with the aim of acquiring information to
evaluate the UX with the minimum set of questions, we issued
the same two questions to the participants:
348
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

TABLE IV. RESULTS OF Q1 QUESTION.
Tour
Joy
Desire
Fascination
Satisfaction
Sadness
Disgust
Boredom
Dissatisfaction
NY
7.1%
7.1%
35.7%
42.9%
7.1%
0%
0%
0%
BCN
7.1%
14.3%
21.4%
42.9%
0%
7.1%
0%
7.1%
SYD
7.1%
0%
28.6%
57.1%
0%
0%
0%
7.1%
LL
7.1%
14.3%
35.7%
42.9%
0%
0%
0%
0%
TABLE V. RESULTS OF Q2 QUESTION.
Tour
0
2
4
6
8
10
NY
0%
0%
0%
14.3%
42.9%
42.9%
BCN
7.1%
7.1%
7.1%
28.6%
42.9%
7.1%
SYD
0%
7.1%
7.1%
7.1%
50%
28.6%
LL
0%
0%
7.1%
14.3%
35.7%
42.9%
•
Q1: ”From the following drawings, mark which one
better explains how you felt when performing the
task”. The drawings (Figure 5), based on the LemTool
emotional tool [21], are designed to acquire the user’s
emotional state just after solving each task. Among
others, LemTool is an auto-report tool that can be
used during the interaction with the interface for its
evaluation. It allows the interface to be related to the
emotion evoked. The tool consists of eight ﬁgures that
represent four positive and four negative emotions,
combining facial expressions and body postures.
•
Q2: ”From 0 to 10, mark your satisfaction with the
load timeout of the images”. The possible answers
were ”0, 2, 4, 6, 8 or 10. The goal of this question
is to obtain a fast and a ﬁrst-hand opinion related to
the satisfaction with the image loading time which
is directly related to the VR parameter as will be
explained in Section IV-E.
Finally, after ﬁnishing all four tasks, participants had to
answer the post-test questionnaire, shown in Table III. This
last questionnaire is inspired by the System Usability Scale
(SUS), which has long been accepted as an industry standard
[22] and adapted to ﬁt in our tests with the Liquid Galaxy
system.
D. Satisfaction Results
By processing the answers from the questions of the
participants (see Tables II and III), we obtained a set of
qualitative measures of the test that are presented next.
Table IV shows the level of satisfaction (question Q1) when
performing the tests in New York (NY), Barcelona (BCN),
Sydney (SYD) and Lleida (LL), while Table V shows the
feeling of the users about the waiting time for each task
(question Q2). We can see that the majority of the participants
provided positive responses when using the system while doing
the tests whenever everything was functioning and the waiting
times were short. In general, 90% of the participants gave
positive feeling responses to the tours of NY, SYD and LL,
having a few cases of discomfort. However, this percentage
was lower in BCN because the user had to move the Google
Earth view into a view showing a signiﬁcant portion of
the city buildings, thus, making the application download a
considerable amount of data. Because of this, some people felt
that they had to wait much longer than in other tasks, especially
the most demanding users, including the youngest participants
or those with more technology knowledge. As a consequence,
the results of BCN in Table IV shows 14.2% of participants as
having negative feelings. This correlates with the waiting time
shown in Table V, where in the case of BCN, 21.3% considered
the waiting time unacceptable (values lower than 5). Despite
some people being frustrated by the wait, they answered more
positively in the question Q1 than expected. This leads us to
think that they were enthusiastic about the system as it was new
and fun for them. We can also say that many of them found the
system challenging, but not impossible, forcing themselves to
perform better and reward themselves with pride, which could
affect the results by giving higher values.
Figure 6. Mean and Error bars from Post-Test questions - Skilled Group.
Figure 7. Mean and Error bars from Post-Test questions - Less Skilled
Group.
The answers obtained for the post-test questionnaire de-
scribed in Table III can be divided into two main groups:
a young and more skilled proﬁle corresponding to the 12
349
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

to 35 age range in Table I, whose answers, in average, are
presented in Figure 6, and older and less skilled people, whose
answers are presented in Figure 7. This skill difference can be
observed with the answer to question FQ1 in both Figures.
Taking the answers to FQ2 and FQ3 into account, both groups
stated that they did not feel a need for prior knowledge and
that the system was easy to learn and use, regardless of their
experience. The main difference was about how secure they felt
while using the system (FQ7), where the second group, less
experienced, felt less secure as well as noting that they needed
more technical help. Also, the differences between both groups
in their response to the question about whether they thought
technical help would be needed (FQ4) were remarkable, which
is correlated with their technical skill level.
In relation to the deviation of the answers, we can see that,
in general, this was higher in Figure 7 due to the differences
between participants in this group, both in age and occupation.
Likewise, it is worth pointing out that the question about how
complex they felt the system was (FQ9) is the one with a
wider range of opinions in both groups. This is because some
of the participants did not know what to answer as it was
a new system and it lead them to think that it would be too
complex for others. Thus, some answered thinking about other
people with less or similar experience than themselves. This
behavior was expected as the questions were designed to take
into consideration other people evaluating the system, and not
how they used it.
E. Effectiveness and Efﬁciency Results
As stated in the introduction, one of the goals of the present
work was to establish a relationship between the satisfaction
(associated with the subjectivity of the participants) and the
effectiveness and efﬁciency measures, related to the system
performance (with no subjectivity associated). In our context,
we understand the effectiveness as the ability of the system
to load all the images while running the application, and
the efﬁciency as the time required to load these images. In
order to get the effectiveness and efﬁciency of the system,
the Visualization Rate (VR) metric was used. This metric was
used in our previous works to measure the Liquid Galaxy
performance [8][9].
Taking into account that the CPU usage tells us when the
system has loaded all the visual elements, VR is calculated as
the average CPU idle time for a cluster of n nodes with the
following equation:
V R = 100
n
n
X
i=1
T idlei
T total,
(1)
where T total is the total time of the test and T idlei is the
time when the CPU load of node ni is below a minimum
threshold. Note that a CPU load below this threshold means
that the CPU is idle and the images have been fully loaded.
This procedure can is illustrated in Figure 8, where the blue
line near 5% CPU Usage is the threshold for this particular
case. The peaks depicted in this Figure corresponds to when
the CPU is processing the imagery and the values that drop
below the threshold represent when the CPU becomes idle.
The CPU usage information was gathered every second from
the information given automatically by the system monitor.
Note that when the VR is near 100%, it denotes a high
visualization rate and, so, good user perception as images are
TABLE VI. VR RANGES FOR T1a, T2a AND T3a SUB-TASKS.
Tour
0-5%
5-10%
10-15%
15-30%
NY
28.6%
42.9%
21.4%
7.1%
BCN
57.1%
35.7%
7.1%
0%
SYD
14.3%
57.1%
21.4%
7.1%
fully visualized, while having a VR equal to 0% means that the
data has not been fully loaded and, thus, it has been ineffective.
Any value between 0% and 100% indicates how efﬁciently the
system has performed. Given that the best feature of the system
is its high pixel density visualization, if the system reports VR
results near 0%, it will mean that the user does not take all
the potential of the cluster.
The system performance metric was monitored throughout
the test, but only the tasks where comparisons could be made
were recorded. Those tasks are the ones that are guided,
because Google Earth follows the same path from one place
to another in a guided task independently of the user, thus
providing fully comparable values. This is not applicable when
the user moves between locations manually, as it is almost
impossible to have the same ﬂight path for the same step,
even with the same user. This means that only tasks T1a, T2a
and T3a, depicted in Figure 4, were monitored to calculate the
VR.
Table VI shows the VR values obtained by different users
for NY, BCN and SYD for the ﬁrst part of their respective tasks
(Tia). For simplicity, these values are categorized in ranges
which are shown as columns in the Table. For the case of NY
and SYD, the highest VR values were near 25%, but for BCN,
the VR achieved lower values. This is due to the fact that in
BCN, in Task 2a, a lot of 3D buildings had to be rendered
in order to view the desired place and, as a consequence,
the loading time was higher and the users did not need to
have the images fully loaded to answer the question. Another
point to highlight is that all VR values were below 30%. This
is because, in our test, the user only wanted to answer the
questionnaire, and he/she was not interested in the speciﬁc
imagery. In a free ﬂight around points of interest by a given
user, he/she would spend more time looking at a speciﬁc point,
which would increase the VR metric.
V.
DISCUSSION
User tests were carried out on a Liquid Galaxy cluster in
order to study the relation between the system performance
and the personal satisfaction of the users when using it. In
general, the results showed that there is a relation between
VR values and the users’ satisfaction level. From our study,
we can see that SYD had the highest VR and also the highest
satisfaction level, whereas BCN had the lowest VR and level
of satisfaction. Despite this, people were generally satisﬁed
and happy to use the system even though the VR values were
rather low. This behavior is reﬂected in the results obtained
from BCN. Table IV shows that the majority were satisﬁed
and fascinated (71%), while Table VI shows that the VR of
this city was the lowest, always below 15%. These data give
us a hint that there might be a relation between VR and the
Q1 and Q2 questions. Therefore, the possible relation between
the performance and the satisfaction parameters were studied
under a statistical point of view.
350
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

Figure 8. VR: CPU Usage.
TABLE VII. VARIANCES AND CORRELATIONS FOR Q1, Q2 AND VR.
Tour
σ2
Q1
σ2
Q2
σ2
V R
σQ1,V R
σQ2,V R
rQ1,V R
rQ2,V R
NY
0,88
1,40
6,48
3,70
5,59
0,65
0,62
BCN
5,34
1,91
3,04
2,49
1,73
0,15
0,30
SYD
1,23
1,69
4,16
2,67
4,43
0,52
0,63
Table VII shows the variances (σ2), covariances (σ) and
linear correlations (r) for the answers to Q1 and Q2, and the
VR performance metric. As for the correlation, values closer
to 1 or -1 means that there is a strong relation between the
metrics, while having values closer to 0 means that there
is no relation between them. We can see that, in general,
there is a positive correlation in all the cases. Likewise, as
it was expected, both correlations (rQ1V R and rQ2V R) in NY
and SYD are very strong, while correlations in BCN are the
weakest. The reason of this lowest correlation in the BCN
case is that in one hand it had the worst VR values due to the
high number of 3D buildings to be loaded, but, on the other
hand, people maintained a high interest and satisfaction when
ﬂying above Barcelona because it was the most well known
tour for the spanish users. In addition, it is worth pointing out
that, in general, the correlation rQ2V R is slightly stronger than
the rQ1V R. This behavior is normal given that the Q2 question
asked to the users about their feelings in relation to the loading
time and the VR metric was calculated from the same loading
time.
These correlation results lead us to think that the VR metric
constitutes an orientative value to know the minimum required
performance to guarantee a right satisfaction to the user with
the system.
VI.
CONCLUSION
This paper is focused on the User Experience study of
a speciﬁc cluster display-wall, named Google Liquid Galaxy,
by using the well-known usability standard deﬁnition which
enclosures the usability as satisfaction, effectiveness and efﬁ-
ciency. We studied the effectiveness and efﬁciency using the
quantitative metric VR, and the satisfaction using two post-
tasks questions about satisfaction using LemTool and a post-
test questionnaire based on the SUS and adapted to suit in
our study. Additionally, we carried out an analysis to ﬁnd the
relationship between these measurements.
The satisfaction analysis was based on a series of tasks
carried out with real users with different characteristics of age,
occupation and skills. The results showed that the majority
had positive feelings while using the system, even though
they noticed a lack of speed in loading images in some parts
of the test. Moreover, there was a clear difference between
younger and older people. The ﬁrst group are more experienced
and tend to be more impatient, as they are already into this
kind of technology, whereas the second group usually are less
experienced and tend to be more impressed.
To acquire knowledge of system performance, we used the
Visualization Rate (VR) metric that indicates the average CPU
idle time of system nodes, in such a way that the time intervals
when the CPU is idle (under a certain threshold) means that
the images were fully loaded. A VR value equal 0% means
that the system is ineffective and any value above 0% describes
the percentage of efﬁciency of the system. During the tests, we
observed that the VR was generally below 30% and above 0%.
Although this could seem a low performance, it was due to the
behavior of the participants when completing the tasks. Some
users had only to wait to load partially the image to be able
to answer the questions (usually the center screen, excluding
the rest of screens) and for this reason the VR values were
relatively low.
Finally, we analyzed the existing relationship that could
be established between the results of the study. We were
able to conﬁrm that the VR constitutes an orientative value
of what the satisfaction level will be when using a speciﬁc
system infrastructure. This was conﬁrmed by the statistical
correlation between VR and the questionnaires. It shows how
both satisfaction post-task questions are related to the VR
values achieved in those tasks. Additionally, satisfaction levels
can be higher than expected, taking the VR metric into account,
whenever the tour was of high interest for the users.
Our results reveal that with the knowledge of the system
performance, which can be found with objective metrics, we
can estimate the suitability of a cluster display-wall to be
used under certain user requirements. This is very encouraging
results for us, as it can facilitate the spread of the use of the
Liquid Galaxy platform to a broad number of users and ﬁelds
351
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

(education, professional, research, etc.).
Future work includes mathematical modeling of the pre-
dictable system performance, speciﬁcally the VR metric, of
any given hardware and conﬁguration. Also, we plan to run
a similar test in an everyday context such as a travel agency,
where participants (potential clients) will use the system to
extend the information of their travel plan. We think that this
test context will provide us with more realistic data as the
users will not perform predeﬁned tasks and, thus, they will
try to achieve their personal objectives. Thus, the VR values
probably would be different in relation to the ones obtained in
our test. One step further could be applying this methodology
of measuring the usability in other systems by ﬁnding their
own VR parameter.
ACKNOWLEDGMENT
*This work was supported by MEyC-Spain under contract
TIN2012-37826-C02-02 and TIN2014-53234-C2-2-R
REFERENCES
[1]
Nirnimesh, P. Harish, and P. Naraayanan, “Garuda: A scalable tiled
display wall using commodity pcs.” IEEE Transactions on Visualization
and Computer Graphics., vol. 13, no. 5, 2007, pp. 864–877.
[2]
N. J. Boden et al., “Myrinet: A gigabit-per-second local area network,”
IEEE micro, no. 1, 1995, pp. 29–36.
[3]
X. Bi and R. Balakrishnan, “Comparing usage of a large high-resolution
display to single or dual desktop displays for daily work,” in Pro-
ceedings of the SIGCHI Conference on Human Factors in Computing
Systems.
ACM, 2009, pp. 1005–1014.
[4]
R. Ball and C. North, “Effects of tiled high-resolution display on basic
visualization and navigation tasks,” in CHI’05 extended abstracts on
Human factors in computing systems.
ACM, 2005, pp. 1196–1199.
[5]
Google, “Liquid Galaxy live demo at TED,” 2010, lecture Notes in
Computer Science, Springer.
[6]
——,
“Google
Earth,”
2013,
retrieved:
March,
2016.
[Online].
Available: http://www.google.com/earth/index.html
[7]
Ponent 2002, “G-liquid galaxy,” 2012, retrieved: March, 2016. [Online].
Available: http://www.g-liquidgalaxy.com
[8]
I. Arroyo, F. Gine, C. Roig, and M. Gonzalez, “User experience
on heterogenous liquid galaxy cluster display walls,” in 2014 IEEE
15th International Symposium on a World of Wireless, Mobile and
Multimedia Networks (WoWMoM).
IEEE, 2014, pp. 1–3.
[9]
I. Arroyo, F. Gin´e, C. Roig, and T. Granollers, “Analyzing google
earth application in a heterogeneous commodity cluster display
wall,” Multimedia Tools and Applications, 2015, pp. 1–26. [Online].
Available: http://dx.doi.org/10.1007/s11042-015-2859-z
[10]
ISO DIS, “9241-210 (2008): Ergonomics of human system interaction-
part 210: Human-centered design for interactive systems,” International
Organization for Standardization, 2008.
[11]
M. Czerwinski et al., “Toward characterizing the productivity beneﬁts
of very large displays,” in Proceedings of INTERACT, vol. 3, 2003, pp.
9–16.
[12]
B. Yost, Y. Haciahmetoglu, and C. North, “Beyond visual acuity: The
perceptual scalability of information visualizations for large displays,”
in Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems, ser. CHI ’07.
New York, NY, USA: ACM, 2007,
pp. 101–110. [Online]. Available: http://doi.acm.org/10.1145/1240624.
1240639
[13]
A. Endert, C. Andrews, Y. H. Lee, and C. North, “Visual encodings
that support physical navigation on large displays,” in Proceedings of
Graphics Interface 2011, ser. GI ’11.
School of Computer Science,
University of Waterloo, Waterloo, Ontario, Canada: Canadian Human-
Computer Communications Society, 2011, pp. 103–110. [Online].
Available: http://dl.acm.org/citation.cfm?id=1992917.1992935
[14]
C. Andrews, A. Endert, and C. North, “Space to think: Large
high-resolution displays for sensemaking,” in Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems, ser.
CHI ’10.
New York, NY, USA: ACM, 2010, pp. 55–64. [Online].
Available: http://doi.acm.org/10.1145/1753326.1753336
[15]
ISO DIS, “9241-11 (1998): Ergonomic requirements for ofﬁce work
with visual display terminals (vdts)-part 11-guidance on usability,”
International Organization for Standardization, 1998.
[16]
Greggman and H. Engines, “Aquarium WebGL,” 2009, retrieved
at 2015-06-07. [Online]. Available: http://webglsamples.org/aquarium/
aquarium.html
[17]
Google,
“Liquid
galaxy
website,”
2013,
retrieved:
March,
2016.
[Online].
Available:
http://blog.endpoint.com/2013/11/
liquid-galaxy-and-its-very-own-street.html
[18]
“Panoramic
Video
Streaming,”
2012,
retrieved
at
2015-06-07.
[Online].
Available:
https://code.google.com/p/liquid-galaxy/\\wiki/
PanoramicVideo
[19]
OpenArena Team, “OpenArena (Quake 3 Arena),” 2005, retrieved at
2014-12-01. [Online]. Available: www.openarena.ws
[20]
Squid Team, “Squid web proxy cache,” 2002, retrieved at 2015-06-07.
[Online]. Available: http://www.squid-cache.org
[21]
G. Huisman and M. Van Hout, “The development of a graphical emotion
measurement instrument using caricatured expressions: the lemtool,”
in Emotion in HCI–Designing for People. Proceedings of the 2008
International Workshop.
Citeseer, 2008, pp. 5–8.
[22]
J. Brooke, “Sus: a retrospective,” Journal of Usability Studies, vol. 8,
no. 2, 2013, pp. 29–40.
352
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

