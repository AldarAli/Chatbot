234
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Classifying Human and Robot Movement at Home and Implementing
Robot Movement Using the Slow In, Slow Out Animation Principle
Trenton Schulz, Jo Herstad, Jim Torresen
University of Oslo
P.O. Box 1080 Blindern
0316 Oslo, Norway
Email: [trentonw|johe|jimtoer]@iﬁ.uio.no
Abstract—We examine how robot movement can help human-
robot interaction in the context of a robot helping people over
60-years old at home. Many people are not familiar with a robot
moving in their home. We present four movement conditions
to classify movement between a human and robot at home.
Using phenomenology and familiarity, we recognize some of these
conditions from other interactions people have with other moving
things. Using techniques from animation in movies, we give to
the robot a distinctive style that can make the robot’s movement
more familiar and easier to understand. Further on, we examine
animation and present how to implement the animation principle
of slow in, slow out with a research robot that can control its
speed. We close the paper with future work on how to use
the classiﬁcation system, how to build on the slow in, slow out
principle implementation for animated robots, and an outline for
a future experiment.
Keywords–human-robot interaction; animation; style; move-
ment; slow in, slow out.
I.
I N T RO D U C T I O N
In previous work [1], we saw that projections for people
over 60-years old who will not be working (hereafter “the
elderly”) will be larger than the number of people working [2].
As people age, they tend to accumulate different aches, pains,
diseases, and disabilities. The elderly will need assistance to
continue to live independently with these acquired health issues.
This aid could be a robot with sensors that could help monitor
and assist the elderly person staying at home. If robots will
be in homes, elderly and other people need to easily interact
with the robots. We posit that making robots move distinctively
using techniques from animation could make this interaction
easier.
Previously, we used phenomenology to examine movement
and classiﬁed robot movement in the home into classes [1].
We also discussed robot movements in the frame of proxemics
[3], people’s familiarity with robot movement, and animation
techniques that could help make the movement more familiar.
In this paper, we build on the previous work [1] by further
exploring the topics of familiarity and proxemics, before
introducing a formalized version of robot movement and a
possible way to animate it using the animation principle of
slow in, slow out. This contributes a combination of the
phenomenological and the formalized exploration of moving a
robot using an animation technique. This gives us a starting
point for building future work on human-robot interaction
(HRI), such as experiments or user evaluations.
We ﬁrst present the context by examining robots for helping
the elderly and robot’s movement in the home (Section II).
Then, we discuss robot movement and what animation and style
means for robots and HRI (Section III). To make it easier to
look at robots and human movement, we present a framework
for classifying movement relations between a person and a
robot (Section IV). We use this framework to aid in looking at
the concept of familiarity and how robot motion compares to
the motion of other objects people encounter in everyday life
(Section V) and how animation can help with this familiarity.
Then, we present a formalized version of robot movement and
how to derive slow in, slow out movement from it (Section VI).
Finally, we present ideas for future work (Section VII) before
concluding the article (Section VIII).
II.
R E S E A R C H C O N T E X T: RO B OT S AT H O M E
Western countries are examining the issue of the “elderly
wave” [2]: the number of people who will be retiring and
needing care will be larger than the people entering the
workforce for these jobs. There is a need for the elderly to
live independently at home longer. Living at home as long as
possible is also the wish of many people. One way of addressing
this goal is to use welfare technology that can assist the elderly
[4]; this includes technology like the Internet of Things and
smart home sensors for reporting and helping elderly complete
tasks [5][6]. Sensors can also provide a warning when things
go wrong, such as an elderly person falling [7].
Instead of mounting the sensors all over in the house, we
can mount the sensors on robots. Robots are mobile and can
be customized for handling different tasks. This idea is the
basis of our larger research project, Multimodal Elderly Care
System (MECS), but let us ﬁrst examine what other projects
have done.
A. Other Projects Looking at Elderly and Robots
Several robots have been built to help the elderly. One
example is Care-o-bot [8], [9] that can assist in multiple tasks
for the elderly at home. The Paro seal robot has been used to
look at how elderly and people with dementia react to a robot
in a nursing home context [10]–[12]. Others have investigated
how the elderly interact with robots. One study looked at a
robot that interacted with the elderly in social situations and
during card games [13].
The European Commission has ﬁnanced several projects that
investigate the elderly and robotics. The Acceptable robotiCs
COMPanions for AgeiNg Years (ACCOMPANY) project
modiﬁed the Care-o-bot to provide emotional and social support
for the elderly [14]. ACCOMPANY also examined viewpoints
of what the robot should do when the older people disobey
the robot’s recommendations [15]. The Managing Active and
healthy aging with use of caRing servIce rObots (MARIO)

235
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
project used a service robot to help address the issues of the
elderly’s feelings of loneliness, isolation, and dementia [16].
The Giraff robot was used in multiple projects. In the Enabling
SoCial Interaction Through Embodiment (ExCITE) project, the
Giraff robot was used for telepresence of other family members
in the elderly’s home [17], and in the GiraffPlus project, the
Giraff robot was upgraded to include monitoring [18].
A recent review of healthcare robotics pointed out that
robots can ﬁll gaps and help overloaded care workers, but that
there is no one-size-ﬁts-all solution to most health issues [19].
If robots shall succeed, different groups need to work together.
From asking the elderly, a survey found the elderly wanted
help for speciﬁc things like recovering from a fall and fetching
and reaching objects [20]. However, a report on the progress of
robots for use in helping elderly live independently found that
current robots must provide more help and services if they will
truly aid people to live independently longer at home; these
robots must be more than a tablet on wheels [21].
These are all points that we consider when we are working
with robots in the home of the elderly in the MECS project.
In addition, we have also sought the advice and cooperation
of members from some of these previous projects.
B. The MECS Project
We are investigating collaboration between human and
robots in the Multimodal Elderly Care Systems (MECS) project.
This multidisciplinary project is funded by the Research Council
of Norway and is examining helping the elderly at home by
offering safety alarm functionality in a robot. The project
investigates algorithms and sensor data to help predict abnormal
behavior by checking the presence of the person at home,
checking the person’s breathing, or noticing if someone is
unstable and may fall soon.
We are concerned that the elderly do not feel that they are
under constant surveillance. We are investigating data protection
issues and having the robot using privacy-preserving sensors
like thermal sensors [7] or ultra wide-band sensors [22]. A
robot at home may let the person feel in control and give the
person some privacy. For example, an elderly person could tell
the robot to leave the room so the person could be alone.
Robots cannot replace a human in every context, but they
can provide support for issues when a person cannot be present
or contact a person for assistance. The robot can also assist
by taking over tasks of drudgery. This allows visitors more
meaningful interaction with the residents in the home. Robots
may help in ways that would otherwise require another human
to always be present and have diverse knowledge. For example,
robots can collect data and use algorithms to give early warnings
about issues (e.g., falling down, low blood pressure, or suffering
from poor nutrition).
In MECS, we work with Kampen Omsorg+, a program
in the City of Oslo that aims at helping elderly people live
longer at home. Kampen Omsorg+ provides modern apartments
with common areas for residents to socialize. Currently, most
residents have a Scandinavian background. This setting provides
a good context for understanding the residents’ needs, designing
robots and sensors that can be helpful for the residents, and
evaluate these robots and sensors over a long-term period in
the residents’ apartments.
Having a robot at home means that the residents will have
to interact with the robot. To aid in observation, the robot
will move between the rooms and with people. One of the
areas we are investigating is how we can have the robot move
in the home and improve interactions between residents—the
elderly—and the robots.
III.
M OV E M E N T, A N I M AT I O N, A N D S T Y L E
It is important to deﬁne terms related to this phenomenon.
This section examines movement, animation, and style.
A. Global and Local Movement
Physical movement (or motion) is a change in position over
time. We call this global movement (Figure 1a). If we were to
imagine the robot in a house, global movement would mean
the robot moves in a room or moves to another room. Local
movement is when parts of a robot move, but its global position
does not change—for example, a robot at rest and waving at a
person (Figure 1b). For simplicity, we will also deﬁne when
no parts of the robot move and no change in global position as
a special case of local movement. Of course, local movement
and global movement can be combined.
B. Animation and Style in HRI
There are many ways a robot can move. The robot can
move at a constant speed, speed up quickly as it starts out, and
slow gradually down when it reaches its destination. Or it can
reverse to gather a running start or brake abruptly to signal its
arrival. All of these different movements can be programmed.
In movie animation, animators use software, pencils, or pens
to “program” the movement of their objects on a screen. So, one
could argue a robot’s movement could be animated. However,
if animation was solely movement, then any movement would
be animation. For animation, it is not the movement itself we
are interested in, but how the movement is done and how the
movement is perceived by the people interacting with the robot.
Animation in movies shares these concerns. Some animation
appears to audiences as smooth and believable, while other
animation appears to the audience as jerky, quickly-assembled,
and not believable. This implies some craft is necessary.
So, animation in HRI has two parts. The ﬁrst part is using
techniques from animation in movies or computer graphics
(or inspiration from them) to specify how a robot moves. The
second part of animation and HRI is the human side. How is
this animation perceived by the humans that are interacting
with the robot? If there is no HRI, then there is little reason
to do the animation and instead optimize movement for other
factors such as maximizing or conserving power.
We posit that animation can improve people’s interaction
with a robot. One way to improve the interaction is by using
animation techniques to give the robot style. Style in this context
means the way “a behavior is performed” [23, p. 133]. Style
can also be thought of as expressive movement. Gallaher looked
at people’s style, and this concept has been successfully applied
to robots [24], [25]. Animation gives the robot an interesting
way of moving, a style. This animated motion can make the
robot seem like it has a personality. The motion can also help
the robot to better communicate what it is planning to do.

236
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Human
x
y
Pose: (x, y, !) 
Twist vectors
(a)
(b)
Figure 1. Examples of global and local movement: in global movement (a), the robot moves in a two-dimensional plane; the Aibo laying down and waving (b) is
an example of local robot movement.
C. Principles of Animation in Previous HRI Studies
Thomas and Johnston [26] documented twelve princi-
ples of animation that animators at Disney used to create
their animations. These principles include: (a) squash and
stretch—an animated object squashes and stretches its form,
but never truly loses its recognizable shape; (b) anticipated
action—an object needs to prepare itself before performing
an action; (c) follow through and overlapping action—actions
are not done in isolation, characters move seamlessly between
them; (d) arcs—limbs move in arcs, not straight up-down,
left-right motions; (e) secondary action—the object’s main
action causes other secondary actions to occur at the same time;
(f) exaggeration—over-emphasizing an action helps people
understand a characters feelings; and (g) slow in, slow out—the
speed of motion is not the same the entire time, but slower at
the beginning and the end.
Previous work in HRI has adopted some of these principles
when creating robots. The principles were referenced when
creating the movement and emotional reactions for the Kismet
robot [27]. This made Kismet’s reactions easily recognized
by participants in the study. Van Breemen [28] advocated to
use these principles for robots, and he applied some of them
to make facial expressions of the iCat more natural and less
machine-like [29].
Animation can make things “look alive” or give them ani-
macy. This can cause people to treat the robots as if they were
alive. For example, in several experiments, participants worked
with an animated robot for a while. Then, the participants were
asked to destroy the robot by turning off its power to erase
its memory [30],[31]. The animated nature of the robot and
its perceived intelligence made some participants hesitate to
destroy the robot.
Applying animation principles has aided participants’ in-
teraction with a robot in several other studies. For example,
animation principles can make it easier for a human to
understand and predict what a robot is doing [32]. Using the
principle of anticipated action made it easier for participants to
predict what the robot was going to do [33]. Another example is
using the principle of exaggeration on a robot telling stories. The
robot’s exaggerated motion resulted in participants remembering
those speciﬁc parts of the story better [34].
So, using animation principles with robots has changed
people’s interaction with the robots. To examine this in the home
environment, let us classify a human’s and robot’s movement in
the home and see how this relates to other types of movements.
Then, we can see how animation techniques can be applied to
make robots’ movements more familiar and provide a possible
implementation of the animation principle of slow in, slow out.
IV.
C L A S S I F Y I N G H U M A N A N D RO B OT M OV E M E N T
Traditionally, human-computer interaction (HCI) was the
study of the use, design, and evaluation of people interacting
with interfaces in different contexts such as stationary computers
in workplace settings, public places, and home settings. Mobile
computing raised the importance of the context of use and
interaction to researchers’ attention. This lead to the research
area of context aware computing [35]. Ubiquitous and ambient
computing raise the idea of computers in the home, but hidden
from view and not moving.
The conditions for the interaction taking place between
humans and computers in a stationary and mobile situation are
similar; there is a stable spatial arrangement between the people
and computers. In both situations, humans and computers are
interacting in the same place, with a stationary relationship
in-between the humans and the computers.
The spatial conditions change when robots enter the scene.
We may be used to moving things outside our home like
automobiles, buses, boats and trams. But in a home setting, we
are not familiar with things moving around on their own.
In the home context, we can classify this movement:
(a) Things that we move around: furniture, peripherals,
clothes, machines like vacuum cleaners or furniture on wheels.
(b) Things moving themselves: domestic robots (robot vacuum
cleaners and robot lawn mowers) and other types of robots.
If we examine the spatial arrangement for movement
between one human and one robot and classify the movement

237
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
as local and global from Section III, we ﬁnd the following
four conditions (Table I):
1) Human moves locally and the robot moves locally,
2) Human moves locally and the robot moves globally,
3) Human moves globally and the robot moves locally, and
4) Human moving globally and the robot moving globally.
TABLE I. M OV E M E N T C O N D I T I O N S F O R H U M A N S A N D RO B OT S
Condition
Human
Robot
1
Local
Local
2
Local
Global
3
Global
Local
4
Global
Global
This framework for classiﬁcation also gives a way to
compare the human-robot movement with other objects. In
Condition 1 and Condition 3, when the robot is moving locally
(including being completely still), the human is either moving
locally or globally. This is similar to conditions for interacting
with stationary computers. We can see Condition 1 when a
person watches TV, and we can see Condition 3 when a person
approaches a switch or walks towards a remote control.
The other conditions are more unusual in the home before
robots. For example, Condition 2 happens when toys are moving.
But Condition 4 does not have good analogs other than perhaps
chasing a moving toy. These other conditions also indicate
something that is unfamiliar. Gibson and Ingold [36] ﬁnd we
are indeed familiar with movement, and they work out the
importance of movement on perception. Let us investigate the
phenomenon of familiarity and how moving robots in the home
might become more familiar to the elderly at home.
V.
FA M I L I A R I T Y A N D M OV I N G RO B OT S AT H O M E
We can examine the phenomenon of familiarity using
phenomenology; that is we look at how people experience
what is familiar and unfamiliar. Once we have an idea what
familiarity is to humans, we can look at how we can make
a robot’s movement familiar. We can also see how animation
and style can help in making these situations familiar.
A. Familiarity
Familiarity plays a role in how people interact and use things
and objects. The familiar is often what we are comfortable and
safe with, be it situations, technologies, relationships, activities
or other people. We are often unfamiliar with things we do not
engage with, things we do not understand, or things that are
foreign to us.
These three concepts; involvement, understanding, and unity
of user-world are, according to Turner and Walle [37], ideas that
we can apply to understand familiarity. Turner and Walle stated
that familiarity unfolds over time. Hence, familiarity points to
activities of daily living where we are engaged and skillful
people going about our everyday lives. When breakdowns or
interruptions happen—for example, something is faulty, missing
or in our way for us to proceed—the separation between people
and their world is taking place, and equipment and activities
become visible as objects for our analysis [38]. However, this
is not the primordial way of being in the world.
Van de Walle, Turner, and Davenport claimed, “What is
observable are the outcomes: easiness, conﬁdence, success,
performance, which are all manifestations or signs of familiar-
ity,” [39, p. 467]. This shows that familiarity is subjective; it
can be described by observing activities or asked questions in
interviews. One way of investigating possible ways of using
robots in the home is to learn from what we already are
familiar with of movement. Harrigan, Rosenthal, and Scherer
[40] provided an introduction into non-verbal human behavior,
including proxemics. Hall [3] observed that human-social
spatial distances vary by the degree of familiarity between
the people interacting and the number of people interacting.
Hall later provided a framework that identiﬁes the main social
spatial zones by interaction and situations. He estimated these
distances visually in terms of arms lengths, close contact and
threat/ﬂight distances—and researchers have since assigned
precise numerical values.
B. Making a robot’s movement more familiar
As Gibson and Ingold [36] claimed, we are all familiar
with movement. Moving within a place, such as a home, is
an example of movement that we all experience daily. We
are familiar with seeing other people move. We are familiar
with seeing things move. We move about in concert with
things such as phones, watches, and footwear. There is nothing
extraordinary with this familiarity of movement of things and
other people. By focusing on the familiarity of movement, we
build on people’s preexisting involvement, understanding and
relationship with the everyday world.
The concept of human-to-human proxemics has human-
human movement at its base and has been used when designing
interactions with robots [41]. This use of human-human
proxemics has been developed further to take the context of the
activity and the person’s location into account in how the robot
should approach the person [42]. All of this is dependent on
people wanting to interact with a robot as though the robot was
a human. Some people assume that robots are simply things
and approach a robot much closer than they would another
person [41]. So, depending on how people will interact with the
robot, another possibility may be to use human-thing distances
and proxemics as the starting point instead of human-human
proxemics. This would be grounded in our familiarity with the
movement of things.
If we think of familiar movement where an object moves
with us, we can ﬁnd some examples: (a) navigating trafﬁc, with
cars, bicycle and public transport material, (b) walking with
a rolling suitcase, (c) operating a wheelchair, (d) operating a
walking stick, and (e) operating a walker. We are all familiar
with doing or observing such movements, but there is no distinct
research ﬁeld literature to ﬁnd out more about these types of
movement. However, the concept of familiarity helps us ﬁnd
these examples.
C. Making a robot more familiar by giving it style
In Section III, we posited that an animated robot moves
with style. Several of the robots from Section III do not move
from their location, but the way they move their parts makes
them appear more friendly and easier to relate to. Animation
also makes it possible to experiment with different kinds of
interaction depending on the animation style.

238
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
In HCI and graphical user interfaces, programmers can
move items across the screen in many ways, and animating
user interface elements can help people understand what is
going on when they are using a program [43]. There is a
different mood or tone when a window minimizes by shrinking
down to a small area on the screen versus simply scaling the
window [44]. Just as animated graphical user interface elements
help explain what is going on, the way a robot moves can be
helpful in explaining what is going on in an interaction with
a robot. Naturally, there are limitations—for example, robots
must obey the laws of physics and some types of motion put
extra strain on the robot [45]—but we can give a robot its own
style by animating it.
Animation can be present in all conditions in Section IV.
For example, in Condition 1, the robot does not move globally,
but its local movement can still be animated by moving parts
of its body. This animation can give the robot a style, add some
personality, and give the effect of presence for the robot [46].
For example, if the person is asking a question or the robot
is providing feedback, animation can provide feedback to the
person about the robot’s state and other relevant information.
This does not have to be complex; a part of the robot rotating
can sufﬁce, or lights blinking to indicate the robot is listening.
A simple rotation that follows the person can help keep the
interaction going in Condition 3.
Condition 2 can build on the animation from Condition 1.
Here, animating parts of the robot’s body can be combined
with its global movement. For example, if the person asks for
some privacy, the robot can start moving away. This can give
the person a sense of what the robot is going to do. Using
animation techniques could also affect how fast the robot moves,
combining several animations techniques could make a robot
“appear” angry, sad, surprised, or happy.
Since these two conditions can build on each other, anima-
tion techniques can also help with the transition between them.
This can offer the human a cue to the robot’s intention. From
the robot’s side, it can also try to determine the human’s cue
to get information if it too should start or stop.
Condition 4 is still unfamiliar for most indoor settings.
Animating the robot’s movements can give it a style to make
it seem like this condition is more familiar. The way the robot
moves can imitate another person or an animal. These imitations
can remind us of other situations where we and something else
move, and this can make a robot and human moving at the
same time more familiar.
There is familiarity in motion and there is familiarity in
forms. Hoffman and Ju [47] posit that robots that resemble
something we are familiar with may bring assumptions and
expectations that are difﬁcult to achieve given current technol-
ogy. Instead, a robot that does not resemble a human or animal
can move expressively to provide clues for interaction. These
movements follow physical properties in the world that people
are already familiar with and give them a starting point for
their interaction.
Returning to proxemics, animation techniques can aid in
building rapport between robot and human. One study has
found that rapport is necessary for people to be willing to get
physically near to a robot or answer personal questions [48];
until a rapport is established, certain actions that signal a good
rapport (like maintaining eye contact) should be avoided. A
different study found different distances for an approaching
robot based on the posture of the human (sitting or standing)
[49].
This framework for investigating movement gives insight in
how to give this movement style through animation techniques.
The way these movements are animated may inﬂuence how
willing someone is to interact with it. A previous study found
the speed and way a robot moved caused people to describe
the personality or mood of the robot [50]. Building on this
work, Another study found people associated negative and
positive emotion to a simple robot simply by adjusting how
it accelerated [51]. A proper balance needs to be found. For
example, a robot moving too fast may prove frightening, and
if a robot moves too slow, people may assume that the robot
can never get anything done. If we desire interaction with a
robot that moves, we need to make it an inviting experience.
This is where using animation principles like slow in, slow out
(Section III-C) may better mimic familiar movement of other
objects. Let us explore how this can be done.
VI.
U S I N G T H E P R I N C I P L E O F S L O W I N, S L O W
O U T O N A RO B OT
Having explored robots’ movement and familiarity by using
the theory of phenomenology, we discuss how to make a robot
move following the animation principle of slow in, slow out.
This focuses on global movement, but it can be applied to
local movement as well. First, we start by describing robot
motion formally and the robot’s generic velocity proﬁle. Then,
we derive a new velocity proﬁle based on the slow in, slow
out principle. Finally, we discuss how this works for robots in
the real world.
A. Poses, Twists, and the Velocity Proﬁle
Robot motion is described in terms of poses and twists [52]
(Figure 1a). A pose provides the position and orientation of the
robot. If we are on a two-dimensional plane, a pose is normally
recorded as a tuple (x,y,ψ) where (x,y) is the position of the
robot in a room, and (ψ) is the robot’s orientation, i.e., which
direction the robot is facing. A twist provides information
about the different velocities the robot is traveling. For a robot
that moves on the ground, these velocities are the angular
velocity—the velocity that the robot is turning and the linear
velocity—the velocity in a line.
When a robot moves, it has a velocity proﬁle. A velocity
proﬁle is a graph of the robot’s velocity versus the distance
that it travels. If we assume a robot moving in a straight line
in ideal, non-friction conditions, the idealized velocity proﬁle
looks like a trapezoid (Figure 2a). The robot accelerating from
a velocity of zero to its cruising velocity makes one of the
diagonal lines (aRampUp). The constant cruising velocity (vCruise)
forms a parallel line with the distance axis. Finally, the robot’s
deceleration down to zero as it nears its ﬁnal location forms
the other diagonal (aRampDown).
There is also a special case when the distance to travel is
shorter than the distance needed to reach the robot’s cruising
speed. The robot accelerates up to a speed (vPeak), but then
slows down as it approaches its ﬁnal spot. This case results in
a triangle velocity proﬁle where acceleration and deceleration
form the legs of the triangle (Figure 2b).
We can formalize the different parts of these variables in
terms of time (t), distance (d), and the different velocities (v).

239
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
aRampUp
vCruise
aRampDown
Distance
Velocity
0
dCruise
dRampUp
dRampDown
dTravel
(a)
aRampUp
vPeak
aRampDown
Distance
Velocity
0
dCruise=0
dRampUp
dRampDown
dTravel
(b)
Figure 2. Examples of velocity proﬁles, a plot of velocity over distance. (a) The trapezoid proﬁle is normally used for long distance movement. (b) The triangle
proﬁle is a special case of the trapezoid when the cruising distance is zero (adapted from Newman [52]).
dCruise = dTravel −dRampUp −dRampDown
The cruising distance (dCruise) is the total distance traveled
(dTravel) minus the distance traveled during ramp up (dRampUp)
and ramp down (dRampDown).
∆tCruise = dCruise
vCruise
The time spent at cruising speed (vCruise) is the cruising
distance (dCruise) divided by vCruise.
∆tRampUp = vCruise
aRampUp
The time spent in the ramp up (∆tRampUp) is the cruising
speed (vCruise) divided by the acceleration at ramp up (aRampUp).
∆tRampDown =
vCruise
aRampDown
Similarly, the time spent in the ramp down (∆tRampDown) is
the cruising speed (vCruise) divided by the acceleration at ramp
down (aRampDown).
∆tMove = ∆tRampUp +∆tCruise +∆tRampDown
The time spent in movement (∆tMove) is the sum of the
time spent in ramp up (∆tRampUp), the time cruising (∆tCruise),
and the time spent in ramp down (∆tRampDown). All of these
equations allow us to deﬁne a distance function (Equation (1)).
d(t) =













































1
2aRampUp(t −t0)2,for 0 ≤ t −t0 ≤ ∆tRampUp
dRampUp +vCruise(t −∆tRampUp),
for
∆tRampUp
≤ t −t0
< ∆tRampUp
+∆tCruise
dTravel−
1
2|aRampDown|(∆tMove −(t −t0))2,
for
∆tRampUp
+∆tCruise ≤ t
−t0 ≤ ∆tMove
(1)
The velocity proﬁle implies that the acceleration is constant;
that is, the velocity changes at a constant rate until it reaches
the maximum speed. This constant acceleration and speed gives
us the mechanical movement that we associate with a robot. If
we change the acceleration and the speed, we may be able to
apply some principles from animation with the robot’s motion.
B. Deriving Slow In, Slow Out for the Robot’s Movement
When animating something in movies or in computer
graphics, the movement of the object is controlled by drawing
the object at a certain position for each frame that is shown on
the screen. This gives the animator a great deal of control in
the speed of the object. For example, if an animator changes
the position only a small amount for each frame, the object
will appear to move slow. The reverse is also true, a large
change in position of an object between frames creates a fast
moving object. If an animator wants to use the slow in, slow
out principle, both of these techniques must be used.
A programmatic way to accomplish the movement is to
use an easing curve (example curve in Figure 3). An easing
curve speciﬁes a time-distance curve that goes from zero to
one for both the time and the distance. This way the animator
needs to know only the starting point for the movement, the
end point for the movement, and the total time to complete the
movement to plot the animation.

240
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Then, for each frame of animation, the animator calculates
the frame’s time as a percentage of the total time to complete
the movement and ﬁnds out the percentage of the distance that
should be complete. This technique is easy to automate, but
requires someone to decide the initial inputs. An additional
advantage is that different easing curves will create different
effects. For example, an easing curve that goes over then under
the distance of 1.0 before ending at 1.0 will appear to “bounce
around” its end point before stopping.
The slow in, slow out animation principle states that an
object should slow speed up to its top speed and then quickly
slow down as it arrives at its ﬁnal location. The slow in can be
simulated by a curve like t3 and the slow out can be simulated
by the negative version (t −1)3 +1. To combine them together
into one curve that goes from zero to one, the equation is:
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Distance
Time
Figure 3. Easing curve for a cubic growth for the ﬁrst half of the journey and
cubic decline for the second half (Equation 2).
d(t) =







(2t)3
2
for 0 ≤ t ≤ 0.5
(2t −2)3 +2
2
for 0.5 < t ≤ 1.0
(2)
The graph would look like Figure 3. (2) is noticeably dif-
ferent than (1), but (2) does not have to take into consideration
acceleration.
This works ﬁne when an animator sets the position of
an object on a screen and worries only about how often a
frame is shown. For robots, there are physical limitations
such as how fast parts of the robot can move, friction, and
inaccuracies of sensors and actuators. Rather than setting the
position directly, the robot controls its acceleration or velocity,
which are complementing ways of expressing motion.
From calculus, we know that the derivative of a distance
function is a velocity function. This means that we can ﬁnd
the velocity at any point in time by taking the derivative of
Equation (2). The derivative (graph in Figure 4) is:
v(t) = d′(t) =
(
12t2
for 0 ≤ t ≤ 0.5
12t2 −24t +12
for 0.5 < t ≤ 1.0
(3)
Equation (3) gives us slow in, slow out movement for short
travel conditions. The curve does not go from zero to one
(it goes from zero to three), but, as Equation (2) gives the
position for a speciﬁc point in time, Equation (3) can be scaled
to give us the velocity we need at a certain point in ∆tRampUp
and ∆tRampDown. With no cruising velocity in Equation (3)—the
curves up and down of Figure 4 resemble the straight lines of
Figure 2b.
Since the triangle velocity proﬁle is a special case of a
trapezoid velocity proﬁle, we can create a similar version for
the trapezoid case. Conceptually, to make this proﬁle similar to
Figure 2a, the speeding up and slowing down should be split
at t = 0.5, and the cruising speed should be put in between the
split. Formally, it makes sense to divide things up into three
parts. During ∆tRampUp, a quadratic curve is used to accelerate
the robot. During ∆tCruise, the robot maintains its cruising speed.
Finally, during ∆tRampDown, a reverse quadratic curve is used.
 0
 0.5
 1
 1.5
 2
 2.5
 3
 0
 0.2
 0.4
 0.6
 0.8
 1
Velocity
Time
Figure 4. The derivative of the easing curve shown in Equation 3.
C. Implementing Slow In, Slow Out on a Robot
We implemented this algorithm for use with the “Burger”
variant of TurtleBot3 (Figure 5). TurtleBot3 is a research robot
from the Open Source Robotics Foundation [53]. The Burger
variant has two wheels driven by servos and a ball bearing to
keep its balance. Using the servos, the robot can go forward,
backward, and turn itself around using skid-steer techniques.
The algorithm is a C++ node for the Robot Operating
System (ROS) [54]. ROS functions as middleware where
different nodes communicate by publishing and subscribing
to different topics, such as twist commands. These nodes can
be located on any machine or robot in the network. In this
case, we are publishing twist commands about the angular and
linear velocity the robot should be running on a topic called
cmd_vel. The TurtleBot3 subscribes to the topic and adjusts
the speed of the servos accordingly.
The node works by taking parameters for going forward
and turning. For moving forward the distance to be traveled
(dTravel), the top speed of the robot (vCruise), and the time it
takes to accelerate to achieve the top speed (∆tRampUp) can be
adjusted. Once the parameters are set, the node publishes twist
commands periodically until the motion is complete. During the
ramp up time, the node publishes twist commands that follow
the curve 3t2. Once the robot reaches its cruising speed, the
node publishes twist commands at the cruising speed until it is
time to start slowing down. Then, it publishes twist commands
that follow the curve 3(t −1)2 until the ramp down is completed.
With the robot at its ﬁnal destination, the node publishes a
twist command with no angular or linear velocity to ensure the

241
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
robot is stopped. For distances that are under the maximum
velocity, the node ﬁnds a VPeak by recursively reducing speed
until it can create a curve that can accommodate the distance.
For turning, the parameters are: the number of degrees to
turn (positive for left, negative for right) and the time to use on
turning. The node then publishes commands for speeding up
and slowing the robot according to Equation (3). Like the linear
motion, it also publishes a twist command with no angular or
linear velocity to stop the robot once the turn is complete.
Figure 5. The TurtleBot3 “Burger” model that was used for testing slow in,
slow out motion.
This node was tested against a simulation of a TurtleBot3
Burger robot. This was done with the “fake node” (a node
that responds to the same messages as the real robot) and
the Gazebo simulator (a simulator that includes gravity and
friction). In both cases, the simulations of the robot show a
difference between the regular constant movement and slow in,
slow out movement.
Moving from a simulated TurtleBot3 robot in a simulated
world to an actual TurtleBot3 in the real world revealed some
limitations. First, the speed of the servos in the real-world are
limited to 0.22 meters per second (m/s); that speed is much
less than most people walking. However, this is only really
an issue if you ask for a speed higher than 0.22 m/s. In those
cases, an acceleration curve was generated for the requested
velocity, but acceleration stopped once the TurtleBot3 reach
its maximum speed and you would not see slow in, slow out
movement. Regardless, even when using the correct speed the
difference in the linear and slow in, slow out movement is
visible, but less pronounced.
To see if this is an issue with physics in the real world or
just the difference in speed, we have since tried the movement
with a robot, a Fetch Robot (Figure 6), that can move at 1 m/s.
This results in a visible difference in how the robot speeds
up and slows down when using slow in, slow out and linear
acceleration.
Another issue to explore is the number of times per second
the node should publish new speeds. Originally, this was done
30 times per second. This works ﬁne in a simulator, where the
updates happen nearly instantaneously. In the real world, there
is a small delay between broadcasting the signal, to receiving
the command, and telling the servos to change speed. The
result is that it is hard to know how many twist commands are
actually processed by the TurtleBot3. Sending less commands,
for example 20 times, 15 times, or even as low as ﬁve times
per second still results in a noticeable change in the robot’s
movement.
Figure 6. The Fetch Robot navigation stack was modiﬁed to provide slow in,
slow out movement.
This node blindly sends out its twist commands. So, a
mistakenly calculated distance may have the robot crash into a
wall, fall off a table, or worse. A robot in the real world needs
to be aware of its environment, and this node must be integrated
into the navigation system. This means that the robot uses slow
in, slow out to move while also being aware of obstacles and
ﬁnding its own way to a destination. We have a preliminary
plugin that can be used by the Fetch robot’s navigation code.
This makes it possible to run evaluations of the different ways
of movement with people interacting with the robot in a home
environment.
VII.
F U T U R E W O R K
There are limitations with movement classiﬁcation from
Section IV, since it only looks at a speciﬁc case of one human
and one robot. There are opportunities to explore different
directions of movement as well. However, even at its simple
level, it gives us many questions we can investigate: how
can the robot move to bring trust and assurance when the
person is interacting with the robot? What activities can a robot
do that are not available when a technology is stationary or
handheld? What conditions are necessary so that people and
robots can collaborate together? How are these interactions

242
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
affected by the animation, proximity, automation, control, and
delegation? We can also examine the transition between the
different classiﬁcations.
Moving with style can be helpful. However, different people
prefer different styles, and some styles may work better in some
situations than others. Finding styles that are compatible with
the robot, the people, and the situation will be a challenge.
Another issue is how the animation can be tested. Many
of the animation studies that we cited in Section III were
run in lab situations. This works well for testing items in a
controlled environment, but robots at home need to work in
dynamic environments. Testing the animations out in a home
environment may be necessary to see if the animation is helpful
for the elderly.
We did not examine who controls the robot in the home
situation. From our discussions in gathering requirements from
the elderly, people have different opinions about a robot moving
at home when they have control of its movement versus it
moving on its own. There is also a question about what control
means in a home situation with the elderly. In Section II-B, we
highlighted the idea of the elderly asking the robot to leave, but
there are also situations when the robot should stay or come
back quickly to join the elderly person autonomously.
As Chanseau, Lohan, and Aylett [55] found, people who
wanted a feeling of control also wanted robots to be more au-
tonomous. The size of the robot and a person’s anxiety towards
robots also inﬂuences proxemics. These issues are important
when introducing a robot—especially moving robots—in the
home of the elderly. Introducing a robot that can detect falls
beneﬁts no one if it moves around the home and becomes an
obstacle to stumble over in everyday life. Then, it is a fall
creator for the elderly instead of a fall detector.
The movement classiﬁcation could be expanded and applied
in other areas. Are there other situations outside of home where
this classiﬁcation applies as well? What happens when you
add more “moving parts” like other people and robots? Does
animating a robot work in all situations? What about animating
robots that have limited movement? These are all questions to
explore in future research.
As to the implementation of the slow in, slow out movement,
since a robot using the implementation can now navigate in an
area with humans, we are working on creating an experiment
in the home context where people interact with a the robot
and it moves using either a regular linear velocity curve or a
slow in, slow out velocity curve. Our goal is to see how slow
in, slow out velocity curves affect participants perceptions of
the robot. Preparations for this experiment are underway and
we hope to begin gathering data in the near future. If they are
successful, we hope to repeat the experiment in other contexts
or other robots to see if the slow in, slow out principle can be
applied in multiple cases.
VIII.
C O N C L U S I O N
We investigated robot movement in the home and classiﬁed
the movement in relation to humans and their movements.
We have used the phenomenon of familiarity to link familiar
movement outside the home with the unfamiliar movement of
a robot inside the home. We also suggested that animating the
robot will make it move with a distinctive style. This style
can give to the robot a personality and make the robot more
familiar to people living at home.
Further, we showed how we could apply one of the
principles of animation (slow in, slow out) to a robot. We
accomplished this by taking an easing curve from computer
animation and deriving a formula that would be useful to a robot
that can control its speed. This formula has been implemented
as an algorithm in a node in ROS and tested both in simulation
and in the real world with a TurtleBot3.
We are working with the elderly by running focus groups
and discussing the issues of robots at home and how a
robot’s appearance and movement affects them. The information
and the elderly’s opinions have been helpful, and they seem
interested in what things robots can do. We will be presenting
this in future work and are integrating their feedback into
our future activities. We will also be using the results from
future experiments in our implementation to see how animation
techniques can give the robot a distinctive way of moving.
AC K N O W L E D G M E N T S
This work is partly supported by the Research Council of
Norway as a part of the Multimodal Elderly Care Systems
(MECS) project, under grant agreement 247697. Thanks to
Tone Bratteteig, Hanne Cecilie Geirbo, Guri Verne, and Diana
Saplacan for comments and suggestions. Thanks also to the
University of Hertfordshire and Patrick Holthaus for allowing
me to test my code on the Fetch Robot at the University of
Hertfordshire’s Robot House.
R E F E R E N C E S
[1]
T. Schulz, J. Herstad, and J. Torresen, “Moving with Style:
Classifying Human and Robot Movement at Home,” in ACHI
2018, The Eleventh International Conference on Advances in
Computer-Human Interactions, IARIA, 2018, pp. 188–193,
ISBN: 978-1-61208-616-3.
[2]
United Nations, Department of Economic and Social Affairs,
Population Divisio, “World Population Ageing 2017,” United
Nations, General ST/ESA/SER.A/408, 2017.
[3]
E. T. Hall, The Hidden Dimension, 1st. New York, NY, US:
Doubleday & Co, 1966, vol. xii, 201 pp.
[4]
D. C. Sønderg˚ard, “Future Challenges and the Role of Welfare
Technology,” Nordic Center for Welfare and Social Issues,
Stockholm, Sweden, Research Report, 2014, p. 14.
[5]
T. Schulz, K. S. Fuglerud, H. Arfwedson, and M. Busch, “A
Case Study for Universal Design in the Internet of Things,” in
Universal Design 2014: Three Days of Creativity and Diversity,
H. Caltenco, P.-O. Hedvall, A. Larsson, K. Rassmus-Gr¨ohn,
and B. Rydeman, Eds., IOS Press, 2014, pp. 45–54, ISBN:
978-1-61499-403-9.
[6]
T. Halbach and T. Schulz, “MobileSage – A Prototype Based
Case Study for Delivering Context-Aware, Accessible, and
Personalized On-Demand Help Content,” International Journal
of Advances in Intelligent Systems, vol. 7, pp. 267–278, 1 & 2
2014, ISSN: 1942-2679.
[7]
S. Kido, T. Miyasaka, T. Tanaka, T. Shimizu, and T. Saga, “Fall
detection in toilet rooms using thermal imaging sensors,” in
2009 IEEE/SICE International Symposium on System Integra-
tion (SII), 2009, pp. 83–88. doi: 10.1109/SI.2009.5384550.
[8]
M. Hans and W. Baum, “Concept of a hybrid architecture for
Care-O-bot,” in Proceedings 10th IEEE International Workshop
on Robot and Human Interactive Communication. ROMAN
2001 (Cat. No.01TH8591), 2001, pp. 407–411. doi: 10.1109/
ROMAN.2001.981938.

243
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[9]
B. Graf, “Reactive navigation of an intelligent robotic walking
aid,” in Proceedings 10th IEEE International Workshop on
Robot and Human Interactive Communication. ROMAN 2001
(Cat. No.01TH8591), 2001, pp. 353–358. doi: 10.1109/ROMAN.
2001.981929.
[10]
L. Giusti and P. Marti, “Interpretative Dynamics in Hu-
man Robot Interaction,” in ROMAN 2006 - The 15th IEEE
International Symposium on Robot and Human Interactive
Communication, 2006, pp. 111–116. doi: 10.1109/ROMAN.
2006.314403.
[11]
K. Wada and T. Shibata, “Robot Therapy in a Care House -
Change of Relationship among the Residents and Seal Robot
during a 2-month Long Study,” in RO-MAN 2007 - The 16th
IEEE International Symposium on Robot and Human Interactive
Communication, 2007, pp. 107–112. doi: 10.1109/ROMAN.
2007.4415062.
[12]
K. Wada, Y. Takasawa, and T. Shibata, “Robot therapy at
facilities for the elderly in Kanagawa prefecture - A report on
the experimental result of the ﬁrst week,” in 2013 IEEE RO-
MAN, 2013, pp. 757–761. doi: 10.1109/ROMAN.2013.6628404.
[13]
J. Hoeﬁnghoff, A. R.-v. der P¨utten, J. Pauli, and N. Kr¨amer,
““Yes Dear, that Belongs into the Shelf!” - Exploratory Studies
with Elderly People Who Learn to Train an Adaptive Robot
Companion,” in Social Robotics, ser. Lecture Notes in Computer
Science 9388, A. Tapus, E. Andr´e, J.-C. Martin, F. Ferland,
and M. Ammi, Eds., Springer International Publishing, 2015,
pp. 235–244, ISBN: 978-3-319-25553-8. doi: 10.1007/978-3-
319-25554-5 24.
[14]
F. Amirabdollahian, R. op den Akker, S. Bedaf, et al.,
“Accompany: Acceptable robotiCs COMPanions for AgeiNG
Years - Multidimensional aspects of human-system interactions,”
in 2013 6th International Conference on Human System
Interactions (HSI), IEEE, 2013, pp. 570–577, ISBN: 978-1-
4673-5635-0.
[15]
S. Bedaf, H. Draper, G.-J. Gelderblom, T. Sorell, and L. de
Witte, “Can a Service Robot Which Supports Independent
Living of Older People Disobey a Command? The Views of
Older People, Informal Carers and Professional Caregivers on
the Acceptability of Robots,” Int J of Soc Robotics, vol. 8, no. 3,
pp. 409–420, 2016, ISSN: 1875-4791. doi: 10.1007/s12369-
016-0336-0.
[16]
H. Felzmann, K. Murphy, D. Casey, and O. Beyan, “Robot-
assisted care for elderly with dementia: Is there a potential for
genuine end-user empowerment?” In The Emerging Policy and
Ethics of Human Robot Interaction, Portland, Oregon, USA,
2015.
[17]
A. Cesta, G. Cortellessa, A. Orlandini, and L. Tiberio, “Long-
Term Evaluation of a Telepresence Robot for the Elderly:
Methodology and Ecological Case Study,” Int J of Soc Robotics,
vol. 8, no. 3, pp. 421–441, 2016, ISSN: 1875-4791. doi: 10.
1007/s12369-016-0337-z.
[18]
J. Gonz´alez-Jim´enez, C. Galindo, and J. R. Ruiz-Sarmiento,
“Technical improvements of the Giraff telepresence robot based
on users’ evaluation,” in 2012 IEEE RO-MAN: The 21st IEEE
International Symposium on Robot and Human Interactive
Communication, 2012, pp. 827–832. doi: 10.1109/ROMAN.
2012.6343854.
[19]
L. D. Riek, “Healthcare Robotics,” Commun. ACM, vol. 60,
no. 11, pp. 68–78, 2017, ISSN: 0001-0782. doi: 10.1145/
3127874.
[20]
L. Pigini, D. Facal, L. Blasi, and R. Andrich, “Service robots in
elderly care at home: Users’ needs and perceptions as a basis
for concept development,” Technology and Disability, vol. 24,
no. 4, pp. 303–311, 2012, ISSN: 1055-4181. doi: 10.3233/TAD-
120361.
[21]
S. Bedaf, G. J. Gelderblom, and L. de Witte, “Overview and
Categorization of Robots Supporting Independent Living of
Elderly People: What Activities Do They Support and How Far
Have They Developed,” Assistive Technology, vol. 27, no. 2,
pp. 88–100, 2014, ISSN: 1040-0435. doi: 10.1080/10400435.
2014.978916.
[22]
M. Tømmer, K. G. Kjelg˚ard, and T. S. Lande, “Body coupled
wideband monopole antenna,” in 2016 Loughborough Antennas
Propagation Conference (LAPC), 2016, pp. 1–5. doi: 10.1109/
LAPC.2016.7807483.
[23]
P. E. Gallaher, “Individual differences in nonverbal behavior:
Dimensions of style.,” Journal of personality and social
psychology, vol. 63, no. 1, p. 133, 1992.
[24]
J. E. Young, T. Igarashi, E. Sharlin, D. Sakamoto, and J. Allen,
“Design and Evaluation Techniques for Authoring Interactive
and Stylistic Behaviors,” ACM Trans. Interact. Intell. Syst.,
vol. 3, no. 4, 23:1–23:36, 2014, ISSN: 2160-6455. doi: 10.
1145/2499671.
[25]
H. Knight and R. Simmons, “Expressive motion with x, y
and theta: Laban Effort Features for mobile robots,” in The
23rd IEEE International Symposium on Robot and Human
Interactive Communication, 2014, pp. 267–273. doi: 10.1109/
ROMAN.2014.6926264.
[26]
F. Thomas and O. Johnston, The Illusion of Life: Disney
Animation, 1st Hyperion ed. New York: Hyperion, 1995, 575 pp.,
ISBN: 978-0-7868-6070-8.
[27]
C. Breazeal, Designing Sociable Robots, ser. Intelligent
Robotics and Autonomous Agents. Boston, Massachusetts,
USA.: MIT Press, 2002, ISBN: 978-0-262-02510-2.
[28]
A. J. N. van Breemen, “Bringing robots to life: Applying
principles of animation to robots,” in Proceedings of Shapping
Human-Robot Interaction Workshop Held at CHI 2004, 2004,
pp. 143–144.
[29]
A. van Breemen, X. Yan, and B. Meerbeek, “iCat: An Animated
User-interface Robot with Personality,” in Proceedings of the
Fourth International Joint Conference on Autonomous Agents
and Multiagent Systems, ser. AAMAS ’05, New York, NY,
USA: ACM, 2005, pp. 143–144, ISBN: 978-1-59593-093-4.
doi: 10.1145/1082473.1082823.
[30]
C. Bartneck, M. van der Hoek, O. Mubin, and A. Al Mahmud,
““Daisy, daisy, give me your answer do!” switching off a robot,”
in 2007 2nd ACM/IEEE International Conference on Human-
Robot Interaction (HRI), 2007, pp. 217–222.
[31]
C. Bartneck, T. Kanda, O. Mubin, and A. Al Mahmud, “The
perception of animacy and intelligence based on a robot’s
embodiment,” in 2007 7th IEEE-RAS International Conference
on Humanoid Robots (Humanoids 07), IEEE, 2007, pp. 300–
305, ISBN: 978-1-4244-1861-9.
[32]
L. Takayama, D. Dooley, and W. Ju, “Expressing Thought:
Improving Robot Readability with Animation Principles,” in
Proceedings of the 6th International Conference on Human-
Robot Interaction, ser. HRI ’11, New York, NY, USA: ACM,
2011, pp. 69–76, ISBN: 978-1-4503-0561-7. doi: 10.1145/
1957656.1957674.
[33]
M. J. Gielniak and A. L. Thomaz, “Generating anticipation
in robot motion,” in 2011 RO-MAN, 2011, pp. 449–454. doi:
10.1109/ROMAN.2011.6005255.
[34]
——, “Enhancing Interaction Through Exaggerated Motion
Synthesis,” in Proceedings of the Seventh Annual ACM/IEEE
International Conference on Human-Robot Interaction, ser. HRI
’12, New York, NY, USA: ACM, 2012, pp. 375–382, ISBN:
978-1-4503-1063-5. doi: 10.1145/2157689.2157813.
[35]
P. Agre, “Changing places: Contexts of awareness in computing,”
Human Computer Interaction, vol. 16, no. 2-3, pp. 177–192,
2001.
[36]
K. R. Gibson and T. Ingold, Tools, Language and Cognition in
Human Evolution. Cambridge University Press, 1994, 504 pp.,
ISBN: 978-0-521-48541-8.
[37]
P. Turner and G. van de Walle, “Familiarity as a basis for
universal design,” Gerontechnology, vol. 5, no. 3, pp. 150–159,
2006, ISSN: 1569-111X. doi: 10.4017/gt.2006.05.03.004.00.

244
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[38]
T. Winograd and F. Flores, Understanding Computers and
Cognition: A New Foundation for Design. Intellect Books,
1986, 236 pp., ISBN: 978-0-89391-050-1.
[39]
G. Van de Walle, P. Turner, and E. Davenport, “A study of
familiarity,” in Human-Computer Interaction-INTERACT ’03,
2003, pp. 463–70.
[40]
J. A. Harrigan, R. Rosenthal, and K. R. Scherer, Eds., New
Handbook of Methods in Nonverbal Behavior Research. Oxford
University Press, 2008, ISBN: 978-0198529620.
[41]
M. L. Walters, K. Dautenhahn, K. L. Koay, et al., “Close
encounters: Spatial distances between people and a robot
of mechanistic appearance,” in 5th IEEE-RAS International
Conference on Humanoid Robots, 2005., 2005, pp. 450–455.
doi: 10.1109/ICHR.2005.1573608.
[42]
K. L. Koay, D. Syrdal, R. Bormann, et al., “Initial Design,
Implementation and Technical Evaluation of a Context-aware
Proxemics Planner for a Social Robot,” in Social Robotics,
A. Kheddar, E. Yoshida, S. S. Ge, et al., Eds., vol. 10652, Cham:
Springer International Publishing, 2017, pp. 12–22, ISBN: 978-
3-319-70021-2. doi: 10.1007/978-3-319-70022-9 2.
[43]
B.-W. Chang and D. Ungar, “Animation: From Cartoons
to the User Interface,” in Proceedings of the 6th Annual
ACM Symposium on User Interface Software and Technology,
ser. UIST ’93, New York, NY, USA: ACM, 1993, pp. 45–55,
ISBN: 978-0-89791-628-8. doi: 10.1145/168642.168647.
[44]
K. K. Chow and D. F. Harrell, “Active Animation: An Approach
to Interactive and Generative Animation for User-Interface
Design and Expression,” in Proceedings of the 2009 Digital
Humanities Conference, 2009.
[45]
M. J. Gielniak, C. K. Liu, and A. L. Thomaz, “Secondary action
in robot motion,” in 19th International Symposium in Robot
and Human Interactive Communication, 2010, pp. 310–315.
doi: 10.1109/ROMAN.2010.5598730.
[46]
W. A. Bainbridge, J. Hart, E. S. Kim, and B. Scassellati, “The
effect of presence on human-robot interaction,” in RO-MAN
2008 - The 17th IEEE International Symposium on Robot and
Human Interactive Communication, 2008, pp. 701–706. doi:
10.1109/ROMAN.2008.4600749.
[47]
G. Hoffman and W. Ju, “Designing Robots With Movement
in Mind,” Journal of Human-Robot Interaction, vol. 3, no. 1,
pp. 89–122, 2014, ISSN: 2163-0364. doi: 10.5898/JHRI.3.1.
Hoffman.
[48]
J. Mumm and B. Mutlu, “Human-robot Proxemics: Physical
and Psychological Distancing in Human-robot Interaction,” in
Proceedings of the 6th International Conference on Human-
Robot Interaction, ser. HRI ’11, New York, NY, USA: ACM,
2011, pp. 331–338, ISBN: 978-1-4503-0561-7. doi: 10.1145/
1957656.1957786.
[49]
M. Obaid, E. B. Sandoval, J. Złotowski, et al., “Stop! That
is close enough. How body postures inﬂuence human-robot
proximity,” in 2016 25th IEEE International Symposium on
Robot and Human Interactive Communication (RO-MAN), 2016,
pp. 354–361. doi: 10.1109/ROMAN.2016.7745155.
[50]
M. Saerbeck and C. Bartneck, “Perception of Affect Elicited
by Robot Motion,” in Proceedings of the 5th ACM/IEEE
International Conference on Human-Robot Interaction, ser. HRI
’10, Piscataway, NJ, USA: IEEE Press, 2010, pp. 53–60, ISBN:
978-1-4244-4893-7.
[51]
M. L. Noordzij, M. Schmettow, and M. R. Lorijn, “Is an
Accelerating Robot Perceived As Energetic or As Gaining in
Speed?” In Proceedings of the 2014 ACM/IEEE International
Conference on Human-Robot Interaction, ser. HRI ’14, New
York, NY, USA: ACM, 2014, pp. 258–259, ISBN: 978-1-4503-
2658-2. doi: 10.1145/2559636.2559793.
[52]
W. S. Newman, A Systematic Approach to Learning Robot
Programming with ROS. Boca Raton: CRC Press, Taylor &
Francis Group, 2017, 502 pp., ISBN: 978-1-138-09630-1.
[53]
Open Source Robotics Foundation. (2018). TurtleBot, [Online].
Available from: https://www.turtlebot.com/ [retrieved: 2018-06-
14].
[54]
——, (2018). About ROS, [Online]. Available from: http://
www.ros.org/about-ros/ [retrieved: 2018-06-14].
[55]
A. Chanseau, K. S. Lohan, and R. Aylett, “How motor speed
of a robot face can inﬂuence the “older” user’s perception of
facial expression?” In 2015 24th IEEE International Symposium
on Robot and Human Interactive Communication (RO-MAN),
2015, pp. 468–473. doi: 10.1109/ROMAN.2015.7333596.

