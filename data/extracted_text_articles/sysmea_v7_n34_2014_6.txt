Dynamic Pattern Development for UAV Navigation Support 
 
Florian Segor, Igor Tchouchenkov, Sebastian Friedrich, Anna Nehaichik, and Chen-Ko Sung 
IAS – Department Interoperability and Assistance Systems 
Fraunhofer IOSB 
Karlsruhe, Germany 
{florian.segor, igor.tchouchenkov, sebastian.friedrich, anna.nehaichik}@iosb.fraunhofer.de 
chen-ko.sung@sung.de 
 
 
 
Abstract—Electrically operated Vertical Takeoff and Landing 
(VTOL) Unmanned Aerial Vehicle (UAV) systems are used for 
aerial situation awareness and reconnaissance for civil security 
because they can be controlled easily on account of the simple 
handling 
and 
the 
good 
maneuverability 
even 
during 
applications in urban areas. The applications of such systems 
for rescue purposes strongly increase and, therefore, the need 
for professional support systems arises steadily. Takeoffs of a 
VTOL UAV system and in particular the landing have no 
meaning for the quality of a reconnaissance operation, but 
require the undivided attention of the operator. To automate 
takeoff and landing, the concept of a dynamic ground pattern 
for position correction and communication is suggested. The 
developed procedure is drafted and the advancement of the 
basic pattern projecting technology to two different working 
prototypes is described. The suitability of the prototypes is 
examined and reviewed. Main focus, in this occasion, is on the 
comparison of the different pattern projecting technologies to 
provide a statement about their strengths and weaknesses.  
Keywords-automatic UAV guidance; pattern projector; 
pattern detection; visual communication; civil rescue forces. 
I. 
 INTRODUCTION 
As already illustrated in [1], there are various systems 
and sensors to support rescue forces in their work to manage 
natural or manmade disasters. One focus of the research 
done at Fraunhofer IOSB is the application of modern 
sensors and sensor carriers to support police and rescue 
forces in such situations. The project AMFIS [2] is 
concerned with developing an adaptable modular system for 
managing heterogenic mobile, as well as stationary sensors. 
The main task of its ground control station is to serve as an 
ergonomic user interface and a data integration hub between 
multiple sensors mounted on light UAVs, Unmanned 
Ground Vehicles (UGVs), stationary platforms (network 
cameras), ad hoc networked sensors, etc. and a super-
ordinated control center. 
Within the amount of different sensor carriers already 
integrated in the laboratory test bed, micro UAVs, especially 
small VTOL systems, play a special role. An application of 
multi-rotor systems within rescue or security scenarios had 
become more realistic in recent years because of the rising 
usability and higher levels of automation. The further 
extension of the application ability and the computer-guided-
control of these sensor carriers is also within the focus of 
research done in the AMFIS project. The aim is a ground 
control station permitting a single operator to control a 
complex heterogeneous reconnaissance system, not only 
sequentially by dealing with one sensor carrier at a time, but 
in parallel with reduced workload and supported by a high 
level of automation. 
Our experiments in the past have shown that the achieved 
level of automation is sufficient in most cases for the 
automated application of multiple sensor carriers with a 
minimum of operator interaction [3][4][5]. 
Though, the automatic take off process of a GPS 
supported VTOL UAV is possible without supervision, 
however, this flight sequence is far away from an absolutely 
secure procedure and can be further improved therefore. 
The landing process needs the unlimited attention of the 
user or a manual steering pilot because the navigation based 
on GPS and pressure sensors is in most cases not precise 
enough for a secure, unattended, automatic landing when 
space is the limiting factor. 
To remove these restrictions and to protect the aircraft as 
well as the personnel and the material near the lift off and 
landing site, procedures were developed to provide an on 
board visual detection of  ground pattern to use this 
information for an exact automatic landing [6]. 
However, using a static pattern, some problems and 
limitations have to be considered. Flying on different 
altitudes, the size of a static pattern varies and a partial 
coverage of the pattern is inevitable on low altitudes making 
it hard to provide robust pattern detection. To cope with 
these problems we extended the concept of using a visual fix 
point to provide a safe landing by introducing a dynamic 
pattern that can changes its representation in size and 
content. Therefore, it can be adapted to the altitude of the 
UAV and reduces the detection of false positives by an 
addition logical level within the detection process. In 
addition, dynamic patterns can be used as a communication 
channel to control the UAV. 
For these reasons, the developed basic detection 
algorithms were designed to be capable of detecting different 
patterns and to extract additional information from the 
ground pattern as for example deviation from the approach 
path or the direction and speed of a potential movement of 
the landing platform (if, e.g., mounted on a vehicle). 
258
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

The Introduction will be followed by review of related 
research in the field of automatic UAV landing facilitating 
pattern detection systems. Section III is describing the 
application scenario and the addressed problems in detail, 
followed by the subsumed results in Section IV on the 
original pattern recognition. Section V is introducing the 
main topic of this paper dealing with the development of 
different dynamic pattern techniques to create an advanced 
test bed that allows an intense validation of the overall 
concept. This is succeeded by an assessment of the created 
pattern systems in Section VI. Finally, the results are 
recapitulated in Section VII followed by conclusion and 
future work.  
 
II. 
RELATED WORK 
With the advance of the technological progress, UAVs 
can be successfully used for more and more applications. 
Hence, during the last 10 years, varied research results 
concerning 
UAV-swarming, 
independent 
navigation 
behavior, sense-and-avoid procedures and also work within 
the topic of automatic landing and lift off were published.  
Within the field of research about the automatic landing 
of a VTOL UAV, the principle of using a ground pattern and 
visual pattern recognition for navigation and position 
extraction has been treated extensively. This application of 
visual extraction poses a special problem within the field of 
image exploitation. Procedures for the processing and 
recognition of structures in a video stream are used in 
different areas professionally. For example number plate 
recognition or the automatic detection of deposit bottles in 
sorting machines should be mentioned. However, in most 
applications position, distance and orientation of the pattern 
to detect can be forecasted very exactly reducing the 
complexity of the application. This does not apply when 
using pattern recognition as a navigation support on board a 
moving UAV. The pattern can become visible in different 
distances, dimensions and rotations and, hence, poses a more 
complicated problem in the field of image exploitation. 
Nevertheless, the usability and applicability of this approach 
is undoubted according to the achieved success.  
S. Sharp et al. [7] presented a test bed for onboard 
detection of a defined ground pattern using Commercial Of 
The Shelf (COTS) camera and hardware components. 
Saripalli examines a very interesting application in [8] 
using a pattern detection algorithm on board of a small 
unmanned rotary aircraft. A theoretical approach to track and 
to land the UAV on a co-operative moving object is 
presented. 
Zhou et al. [9] as well as Yang et al. [10] examined the 
possibilities of an autonomous landing on a static "H"-
shaped pattern. Especially, Yang pays special attention to the 
high noise immunity and the rotation independence of the 
detection algorithm. 
Xiang et al. [11] describe a very interesting set up with 
low-cost COTS components (IR Cam of the Wii remote). 
The components are used to build an active IR pattern for the 
positioning system of a multi-rotor UAV. 
Lange et al. [12] also address the landing of an UAV on a 
ground pattern. They concentrate on handling the problem of 
the discrete scaling of the pattern independent of the 
different flight altitudes of the UAV by introducing a special 
designed circular ground pattern. Through different circles, 
which are becoming smaller to the centre of the pattern, the 
algorithm is capable of detecting the landing site also during 
the final flight stage of an approach without the need to adapt 
the absolute magnitude of the pattern. 
A similar approach is followed by Richardson et al. in 
[13], describing the landing of an autonomous UAV on a 
moving ground platform by using a pattern detection 
algorithm in co-operative surroundings. As in [12], a 
multistage pattern, which enables the complete visibility of 
the pattern for on board recognition also at a low flight level, 
is used.  
All these researchers have shown good success in 
addressing very similar purposes. However, the suggested 
solutions suffer from some limitations as for example the 
restrictions due to the missing discrete pattern scaling during 
landing and takeoff. Additionally, each static pattern 
approach can react on a pattern-like natural or man-made 
structure with miss-interpretation or detection errors.  
The dynamic pattern introduced in this research allows 
the construction of an additional communication link to the 
UAV and, besides, solves problems, which are not handled 
yet. 
 
III. 
APPLICATION SCENARIO AND MOTIVATION 
One of the central application scenarios of the AMFIS 
system is to deal with the support of rescue forces in 
disasters or accidents. The varied application of different 
sensors on board of a UAV can be used to acquire important 
reconnaissance information to make the work of the people 
in the field more safe and efficient. Derived from the 
experiments done with the AMFIS system, the missing 
capability of the UAVs used within these scenarios to 
precisely take off and land automatically on a designated 
position was identified as one of the main challenges for the 
professional application – especially when multiple UAVs 
are deployed at the same time. 
The endurance of electrically operating UAVs is limited 
and in most cases several take offs and landings become 
necessary in order to fulfill the mission. In these flight 
phases the UAV must be supervised and neither the operator 
nor the UAV can contribute to the mission’s target. To 
automate these flight phases the navigation exactness needs 
be improved. A visually extracted geographical fix point at 
the landing position is, on this occasion, a promising start. 
The here presented draught is based on already achieved 
success with visually extracted patterns and extended to use 
dynamic pattern recognition with the aim to receive a more 
stable and reliable navigation support. 
A dynamic pattern is not necessarily compelling for the 
solution of the primary problem and quite good results were 
achieved using non-dynamic, static patterns. Indeed, a 
dynamic pattern offers additional advantages which extend 
the application possibilities of such a system. Just by using 
259
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

   
 
 
 
the access to an, in principle, almost unlimited pool of 
different signs and symbols, the abilities of a pattern concept 
can be clearly enlarged. By that, the detection capability of 
the algorithm is not limited to the pure localization of the 
pattern any more. It can be extended by the functionality to 
extract information content hidden within a detected pattern. 
Besides, a dynamic pattern still offers some other 
advantages. As already Lange et al. [12] stressed out, an 
essential problem within using ground patterns originates 
from the detection of a static pattern at different flight 
altitudes. Even when using a fish-eye lens during an 
approach of the sensor to the pattern, the probability rises 
that parts of the pattern are not grasped by the sensor because 
of the limited aperture angle and the increasing appearance 
of the image or pattern. The use of a dynamically adaptable 
pattern allows resizing the shown pattern. Thus, the size of 
the pattern can be adjusted matching the current flight 
altitudes raising the chance that the sensor is capable of 
viewing the shape completely. Though, the algorithm is 
designed to be rotation and scale independent, nevertheless, 
the result quality of the detection algorithm could possibly be 
further improved by aligning the orientation of the pattern 
with the direction of the UAV as well as considering its point 
of view and distorting its perspective. An optimized 
projection of the pattern considering not only distance but 
also the orientation and view angles is assumed to potentially 
reduce the load on the low-power on-board processor. 
However, the introduction of an additional visual 
communication channel provides even more advantages. 
Unfortunately, the widely used radio data connections 
between UAVs and their dedicated ground stations can be 
very easily disturbed - intentionally or unintentionally. The 
detection of a used radio frequency can be done using COTS 
systems and even if it is not so easy to break into the 
communication to take over the UAV, in most cases it can be 
overlaid leading to a complete communication breakdown 
between the ground control and the aerial system. Using a 
visual 
communication 
system, 
interfering 
with 
the 
communication becomes more difficult because a potential 
disrupter stays hardly unnoticed if applying a permanent 
influence on the pattern providing ground platform. 
 
IV. 
ONBOARD DETECTION CHAIN 
The basic functions for adaptive pattern recognition on 
board the UAV have been reported in [5]. The implemented 
on board detection chain basically consists of two major 
tasks. 
The first task is the separation and extraction of possible 
pattern sub images from image sequences as pattern 
candidates for the recognition and interpretation of manmade 
landmarks. The implemented process chain with an adaptive 
threshold operation for this task works well and has not been 
modified for the present investigation. 
The second task is the recognition of patterns or 
manmade landmark images from the identified candidates. 
The challenge of this task is that the onboard process for 
image evaluation must be robust, non-compute-intensive, 
expandable and fast. For that reason, we developed a so-
called "zigzag" method, which analyzes how many binary 
values of relevant parts of an object image are correlated 
with the expected values within the selected region identified 
as a possible pattern. 
The previous investigation has shown that the methods 
and the complete on board detection chain is stable, easy to 
extend and provides good results on detecting the patterns on 
the ground in different conditions. 
Figure 1.  In-flight detection of shape "H" and "L" marked by colored 
circles at the center of the pattern ("H"  is marked red; "L" is marked green) 
camera: GoPro Hero 2, altitude: 30 metres. 
An important part in the first task of the process chain is 
the recalculation of identified possible patterns. These sub 
image regions are translated into a standard region. The 
algorithm inherits therefore some serious advantages, as for 
example the rotation and scaling independence necessary for 
an UAV application (see detection in Figure 1). 
At the same time the designed is not limited to only 
detect a pattern on the ground to calculate correct and GPS 
independent navigation information, but also to extract 
information from the different pattern sequences. The used 
"zig-zag" method has great advantages because of the fast 
and simple logic, used to recognize a single pattern. The 
procedure is quick and efficient and, hence, suited to deliver 
usable results with limited hardware capacity onboard, which 
has been proven in the past attempts. Using the detection of 
different signs in different sequences for creating a pattern 
language allows the transmission of reduced information 
form the ground to the aerial system. 
Figure 2.  Examples of used patterns. 
To achieve a sufficient information density, the number 
of different patterns has to be enlarged to reach the capability 
to transmit more complex information by combining 
symbols (see Figure 2). 
This can be seen as one other the key features of the 
dynamic pattern detection beside the improvement of the 
navigational information for the automatic landing. As 
already mentioned above, different patterns are shown at the 
same projection plane sequentially and can be recognized on 
board the UAV. On the one hand, by flipping the patterns, 
260
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

   
 
errors occurring due to the detection of similarly looking 
natural structures should be avoided in future, because the 
system expects a regular change in the detected area. On the 
other hand, dedicated information will be linked to the single 
symbols. Orders or important information, as for example 
the current wind direction or a possible movement of the 
ground platform, can be encoded and transferred using the 
pattern sequences. 
Therefore, 
the 
palette 
of 
used 
symbols 
was 
complemented with additional signs to extend the capability 
of encoding more complex information into a pattern 
sequence by switching between the introduced signs. 
Nevertheless, the used pattern pool is held small at the 
present time, because for every new introduced pattern the 
algorithm needs to be adapted in order to "learn" the new 
shape and to recognize it during the detection sequence. 
Additionally, an enlargement of the pattern pool also 
requires more logical operations during the scan process of 
possible pattern blobs found in the images, which leads 
directly to an enlargement of process time and workload 
during the classification of the pattern in flight. It remains to 
optimize the balance between size of the pattern pool (for 
information encoding) and duration of the pattern 
classification process. 
 
V. 
ADAPTIVE PATTERN DEVELOPMENT 
The currently used setup for development, evaluation and 
demonstration of the conceptual design was based on 
different simple pattern projectors to evaluate the concept 
and its functionality. The identified technologies that can be 
used to set up a working pattern projector needed to be 
consolidated in order to create a more flexible, adaptable test 
bed. The central object for further development is therefore 
the technological realization of the dynamic ground platform 
to create a complete working prototype, which will be 
integrated into the AMFIS communication backbone for 
information exchange and to receive control commands from 
the system in the future (see Figure 3). 
Figure 3.  Sktech of the final target system. 
For the initial non-dynamic testing of the algorithm, a 
static ground pattern with the shape of a white "H" on a 
black background was used. This test setup was designed to 
experimentally deploy the developed algorithm in a realistic 
test scenario under real conditions and environmental factors 
(e.g., sunshine). However, on account of the long-term aim 
of developing and applying a dynamic pattern, the 
adaptability and expandability of the detection and the 
interpretation algorithms was emphasized. Hence, the 
developed dynamic pattern should show the same static 
pattern (a white sign on black background) as exactly as 
possible to achieve the highest possible contrast in the first 
experiments. 
Because the detection should be functional under bad 
lighting conditions and the missing possibility to introduce 
new or adapted patterns in the future, a mechanical solution 
with flipping parts was excluded. It has been assumed that 
the final working system could need an extension on the 
pattern alphabet or a change within the available patterns 
when new demands arise. A simple solution to display 
different symbols or patterns in different representations and 
scaling needed to be found. To cope with this, different 
Light-Emitting Diode (LED) matrices were examined and 
tested for their suitability. 
 
The experimental used technologies for dynamic ground 
patterns are all slightly different in technology and size. The 
originally used prototype based on single low cost LED 
panels and reached a size of 65 x 65centimeter. Tested under 
realistic conditions, it shaped up that the low cost image 
display matrix, which provides control over every single 
LED, is not suitable on account of the used Pulse Duration 
Modulation (PDM) and the low fixed refresh rate. The PDM 
controlled LED cause a flickering not visible for the human 
eye, but for the camera. Experiments showed that this 
flickering troubles the algorithm in detecting possible blobs 
for the pattern in the video. 
Figure 4.  Illuminated and non-illuminated ground pattern. 
To reach a non-flickering representation, small 3x3 
illumination LED matrices were used and assembled to an 
18x21 experimental matrix even smaller than the original test 
system (see Figure 4). This pattern matrix turned out to be 
absolutely flickering free and can, therefore, be detected by 
the algorithm as one structure without any problems. The 
second advantage is that the assembled platform was 
luminous strong and provided the capability to see and detect 
the ground pattern even in bright sun light. 
 
261
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

The 
functionality 
of 
these 
different 
projection 
technologies were tested under different circumstances. In 
[1] it was shown that the developed algorithms in 
combination with the two described technological diverse 
pattern projectors are applicable for pattern supported 
navigation. Nevertheless, the validation of the overall 
concept for a final pattern projection technology is a central 
precondition for further advancements. Because different 
draughts for pattern projectors were pursued it is important 
to consolidate this technology and to transfer the knowledge 
from the validation process into a final technical draught. 
Based on the results of the present technological experiments 
two technology demonstrators were developed and tested. 
Both systems are based on matrix LEDs that can project 
different patterns. Indeed, they differ in the way the 
representation of the single patterns are generated as well as 
in their technical construction. 
 
A.  Large Pixel Pattern Projector (L3P) 
The L3P (see Figure 5) is based on technical 
specifications of the 3x3 LED illumination matrixes also 
facilitated in the projector in Figure 4. The main difference 
to other tested matrixes is that the control of single LEDs to 
visualize certain forms or pictures is not their scope of 
application, but a constant full-area backlight illumination.  
Figure 5.  Large Pixel Pattern Projector (L3P). 
The single modules are equipped with 9 LEDs, which 
can be either fully activated or deactivated. Based only on 
this technology a true-dynamic pattern projector cannot be 
realized. Hence, for the active pattern a projector module 
was developed, which includes several of the lighting 
modules, which can be switched on or off computer-
controlled. The so designed pattern module consists of a total 
of 36 lighting modules and permits all possible permutations 
of illuminated and deactivated light fields controlled by the 
integrated hardware. Every single light field is separated 
with footbridges from the neighboring fields to allow a clean, 
sharp-edged projection. The projection screen is concluded 
with a diffusor, which compensates the relatively big 
distance between the single LEDs and prevents the covering 
of partial LED segments when the approach angles of an 
UAV are getting sharper. 
In contrast to a fully adaptable pattern projector the 
ability of scaling the image is decreased by the size of the 
single pixels and the interconnected low overall resolution. 
On the other hand, originating from the diffusor and the size 
of the single pixels, it was assumed that less detection 
problems will arise during final or low flight phases. 
 
B. Flexible Advanced Pattern Projector (FlAPP) 
Beside design and construction of the L3P a second 
solution for a fully adaptable projection technology was 
developed. In opposite to the reduced scaling capabilities of 
the L3P the FlAPP should provide a high flexible pattern 
projection. An exact control of single LEDs is essential for a 
visualization of patterns in different scaling. For this purpose 
different high end LED panels were examined. As a main 
problem, on this occasion, it turned out that most LED 
screens suffer from a too low refresh rate. As a result of the 
flickering representations of the patterns the algorithms 
could not recognize the content and, hence, failed. 
Figure 6.  Flexible Advanced Pattern Projector (FlAPP). 
The FlAPP was conceived as a LED panel build from 
SMD LEDs, which refresh rates were heavily raised with 
additional LED control technology to eliminate these 
problems.  
 
VI. 
ADAPTIVE PATTERN ASSESSMENT 
To further improve the development of a test system for 
the pattern-recognition-supported precision landing, the L3P 
and FlAPP had to be comparatively tested. By these tests 
under 
equal 
conditions 
both 
technologies 
become 
comparable to each other and can support a final technology 
decision or lead to a new development cycle to improve the 
test bed. Both draughts have their advantages and 
disadvantages, which were known partially in advance or 
were discovered in the draught-related test studies. 
The L3P distinguishes itself by high contrast and angle 
independence by the accordingly scattering diffusor. 
However, it is limited in its scaling possibilities because the 
 
 
262
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

single pixels cannot fall short of a minimum of 45 x 45 
millimeter design dependent. Therefore, the resolution is low 
with ca. 386 pixels per square meter. The big advantages are 
the directly supplied LED modules, which are not controlled 
by a cyclic refresh process and provide a non-flickering 
representation independent of the used camera system. 
However, in comparison the FlAPP provides by its more 
than 40-times higher pixel density of 15,683 pixels per 
square meter a better adaptability in the representation of 
single patterns and also in their scaling. In addition, the 
available test system is equipped with RGB pixels, so that 
test series in different frequency bands of the visible light 
spectrum become possible. 
For the comparison tests of the developed pattern 
systems the demands for a functional projector were 
gathered and realistic scenarios within the scope of the 
common takeoff and landing routines were extracted. 
Regardless of the type of control (manually or computer-
controlled), the approach on the landing position in present 
flights occur accordingly to the same workflow. The UAV 
stays on a safe flight altitude, which can be assumed to be 
free from any obstacles within the operation area. 
If emergency procedures after a communication loss or a 
low energy alarm are disregarded, the UAV returns for 
landing to its starting point or another geographical position 
specified by the user. If the UAV has reached its landing 
position on a safe flight altitude, the pilot or the computer 
reduces the thrust and the UAV is approaching the ground. 
None or only GPS based course corrections are occurring in 
the computer-controlled mode, while a manual flying pilot 
can adapt the descent in speed as well as in horizontal 
direction to provide a safe landing. Hence, the direct vertical 
approach to the ground pattern arises as a primary test 
scenario. 
Beside the recognition of the pattern, the scalability of 
the patterns is one essential factor to be tested. Dependent on 
the selected EO sensors the minimum size of the projected 
pattern in different distances has to be determined in order to 
selcet a suitable scaling. 
Beside the maximum height or distance between sensor 
and projector, the minimal possible distance is of big 
relevance. Due to the used algorithm the projected shape of 
the pattern must have an interconnected structure. If the 
shape falls into pieces because of a too big pixel distance, the 
algorithm cannot recognize the pattern anymore and the 
pattern detection fails. This happens because of the adaptive 
threshold operation when the algorithm is searching for 
possible pattern blobs in the image. If parts of the pattern are 
disconnected to the rest, they will be detected as stand-alone-
blobs. The detection tasks will try to recognize them and will 
fail. Particularly for the application of the FlAPP this 
problem is of central importance as a diffusor is absent and 
perhaps would have to be subsequently mounted to close 
possible appearing gaps at short distances between camera 
and projector. But also the L3P design has caused narrow 
dividing footbridges between the pixels that could limit the 
detection robustness. 
Beside the primary task of validating the pattern 
technology concerning a functional direct vertical landing, 
the enlarged abilities of the draught are also to be examined. 
The above described scenario implies a low angle divergence 
during approach. However, if the possibilities of the used 
UAVs to adapt the optics horizontally as well as vertically 
are taken into account, sharper angles of approach need also 
to be considered. This scenario slightly adjusts the demands 
for the pattern technology concerning the homogeneous 
radiation of the LEDs or the diffusor. It was assumed that the 
L3P will provide a clearly steadier image projection on 
account of the diffusor whereas the FlAPP could suffer from 
color and intensity changes in different views. Hence, the 
experiments were extended to achieve a simple comparison 
between the projectors on account of different view angles. 
The perspective distortion of the patterns was neglected and 
is of minor importance as the algorithm is scale and rotation 
invariant. 
Figure 7.  Test set up: FlAPP (1), L3P (2), mobile plattform (3). 
In preceding test cases, enlarged flight experiments had 
already proved basic functionality of the concept facilitating 
illuminated but non-dynamic patterns. The knowledge and 
results from these experiments influenced the development 
of the L3P and the design of the FlAPP. 
Particularly the development of a suitable diffusor that 
provides enough dispersion on the one hand and a low 
damping rate on the other hand, so that recognition is still 
possible under direct solar irradiation, is decisive for the 
functional L3P. 
All initial test series were conducted under the premise of 
realistic application surroundings. Therefore, the pattern 
projectors were installed horizontally on the ground. All test 
recordings were done on board of a UAV with direct solar 
irradiation on the pattern. This modus operandi allowed 
checking and validating the design and functionality of the 
approach (see Figure 1). 
The subsequent test series were focused on the 
applicability of the selected cameras as well as on the 
evaluation of the different pattern projector technologies. 
In order to be able to compare the well-chosen electro-
optical sensors, the image recordings must be done at the 
same time from the same position in identical distance and 
 
263
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

lighting conditions. As the UAV chosen as target platform is 
not capable of carrying all cameras and their recording 
equipment, the experimental set-up was transferred for 
simplicity reasons from a vertical test bed into a horizontal 
one. 
For this purpose the FlAPP was installed upright on a 
mobile platform. This was not necessary for the L3P, 
because it can be moved easily from hand due to its small 
size and weight (see Figure 7: FlAPP (1), L3P (2), mobile 
platform (3)). A test track of a maximum of 50 meter in 
length was set up where the FlAPP as well as the L3P were 
recorded by the different electro-optical sensors in different 
distances. Beneath the direct view at the sensor, additional 
approach angles to the pattern projector were simulated by 
panning the mobile platform. 
 
VII. RESULTS 
Essential topics for the further advancement of the 
technology could be identified by the evaluation of the test 
series and the recorded data. The functional limits defined by 
design referred to an operational distance of 0 to 100 meters 
between projector and image sensor. The tested set up 
covered a maximum distance of 50 meters, the results for 
distances beyond 50 meters where calculated. For the 
distance tests the FlAPP was used as reference system 
because of its size and scalability. 
Based on the acquired data the main restriction identified 
for the chosen approach is that the complete application 
range cannot be covered by a single camera system fix fixed 
optics under the addressed conditions. In average, using 
different image sensors and distances the pattern was 
recognized down to a lower border of 6% of the side lengths 
of the original image resolution. In dependence from sensor, 
optics and the size of the pattern, the possible maximum 
distances for a successful detection can be calculated 
therefore. 
The scaling possibilities of the FlAPP allow adapting the 
pattern to the flight altitude of the approaching UAV. 
Particularly during deep flight phase this is vital, because it 
covers the most critical part of a final approach. Hence, 
within 0 – 5 meters above ground, special demands for the 
image sensor and the optics arise. Though, the pattern is 
reduced in size, however, for a successful detection it should 
not exceed 60% of the image until shortly before landing. A 
wide-angular optics is suitable particularly for the final flight 
phase. At heights of 20 meters and more above ground, these 
camera systems fail in delivering a suitable image for 
detecting the pattern. Hence, the application of a telephoto 
lens is unavoidable when the functionality should be also 
guaranteed in higher operation levels.  
Based on the minimum side length of 6% and 60% as a 
maximum value, 10% and 50 % were used for the 
calculation of the final optics. The considered buffer should 
permit a safe detection even at the outer bounds of the 
specification.  
A vario zoom optic is not always possible because of its 
weight and the low payload capacity of the UAV. 
Based on the test results of different camera systems a 
camera of the company IDS-IMAGING, the UI-2230SE was 
selected for further testing. Based on the performance data 
the necessary focal length can be calculated. The sensor size 
of the UI-2230SE is 1/3” (B), 3.6 millimeter to 4.8 
millimeter and 6.0 millimeter diagonal. Image distance (b), 
object distance (g), focal length (f) and object (G): 
 
 
 
With the restriction of the minimum and maximum 
picture ratio a theoretical focal length of 24.4 millimeter 
arises for the distance up to 15 meter and 81.2 millimeter 
focal length for distances of 20 – 50 meter. A continuous 
coverage for 0 – 100 meter is not possible with these 
restrictions. Pushing it to the edge using 6% image cover as 
determined during the test, the full distance up to a flight 
altitude of 100 meter is covered. 
TABLE I.  
FOCAL LENGTH FOR DISTANCE 
 
Attempts in the infrared spectrum of light have proven 
that detection of the patterns is possible but not effective. As 
it has been expected, the radiation of the used LEDs in the 
infrared spectrum is near zero. Merely the up-warming 
electronic modules were recognized with a big delay. A 
change of the pattern projection needs therefore several 
minutes to become visible to the IR sensor. After switching 
off of the pattern the last indicated symbol is still detectable 
for some time. Using IR for the pattern projection is 
interesting but would need a complete redesign of the pattern 
projection technology. Available LED panels are equipped 
with LEDs for the visual spectrum of the light due to their 
application purposes. For a fully working IR panel the LEDs 
need to be changed into special LEDs emitting light in the 
infrared spectrum.  Further experiments with IR are therefore 
expulsed. 
 
The comparative test of the developed projectors L3P 
and FlAPP could be used to evaluate the basic design as well 
as the special stages of development. Besides, both pattern 
technologies could show their strength. However, the 
identification of possible weak spots and problems was 
important. As illustrated in Figure 8, unexpected side effects 
were detected on the FlAPP during the measuring campaign. 
Partly heavy Moiré effects could be observed in some 
recordings in dependence of the used camera, certain 
distances and view angles. Though the effects of the image 
Focal 
length 
Distance 
0 - 15 
15-30 
30-45 
45-60 
60-75 
75-
100 
100 + 
100.00 
 
 
 
 
 
 
 
 
80.00 
 
 
 
 
 
 
 
 
 
65.00 
 
 
 
 
 
 
 
 
50.00 
 
 
 
 
 
 
 
 
15.00 
 
 
 
 
 
 
 
264
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

interferences turned to be acceptable to the algorithm, or 
could be removed by known procedures like, for example, a 
combination of image dilatation and image erosion, 
nevertheless, such effects need to be avoided if possible to 
provide a more robust detection and to keep the workload of 
the on-board hardware as low as possible. 
Figure 8.  Moiré effects on the FlAPP. 
We assume that the Moiré effects are originating from 
the overlapping of the matrix structure of the FlAPP by the 
matrix of the digital sensor. Therefore, the appearances of 
these effects are depending on distance and angle between 
camera and matrix LED. This phenomenon is strongly 
dependent to the combination of used image sensor, distance 
and angle. Hence, the appearance of such image 
interferences is difficult to avoid just by changing the sensor. 
The L3P does not show these effects on account of the fixed 
projection of the single large pixels and the distant mounted 
diffusor. Because of the pixel size and the steady light 
emission of the L3P the matrix is not filigree enough to 
generate Moiré effects by an overlapping with the raster of 
the image sensor. Tests have shown that the application of 
the same diffusor used on the L3P reduces the Moiré effects 
on the FlAPP to a minimum.  
 
As expected, the scalability of the patterns proved to be 
the central functionality that can guarantee successful 
detection during the final landing approach. The L3P showed 
here its weaknesses, because the display of the pattern is of 
limited scalability. To deal with these problems, this 
technology requires the implementation of a special solution 
for the final approach sequence like introducing a new 
pattern consisting of a single white square (a single Pixel of 
the L3P when scaled to the minimum).  
 
In addition to the internal factors, problems with the 
brightness of the projectors could be identified in the test. 
Originally it was assumed that detection problems will arise 
mainly in bright sunlight. The tests have not confirmed these 
concerns. But changing the conditions towards a poorer 
external lighting, some image sensors tend to catch a blurry 
representation of the pattern, especially at larger distances 
between projector and sensor. The pattern becomes indistinct 
to a single spot and thus cannot be detected anymore. The 
smaller the pattern (the greater the distance), the more 
intense is this effect, since fewer image pixels are 
accordingly covered by the pattern. At close range, this 
effect also occurs, but because the pattern is sufficiently 
large, the effect on the detection is low. The FLAPP is 
already equipped with an ambient sensor that can adjust the 
brightness to the external influence, but the sensor was not 
considered in the current test series. The L3P does not have 
such a sensor and therefore, needs to be upgraded. 
 
Both pattern projectors have shown their strength and 
weaknesses during the test series. Based on the results the 
further development will focus on the application on the 
FlAPP as a final technology. But, because of its simplicity, 
the good handling and the low price, the L3P could also be 
updated and considered in future test set-ups. 
 
VIII. CONCLUSION AND FUTURE WORK 
In this paper the activities of Fraunhofer IOSB in the area 
of civil security and their relevance for a supporting 
application in emergency situations were explained. For this 
work the applicability of small VTOL UAV systems to 
support rescue forces with local reconnaissance were brought 
into focus; the importance of a further improved automation 
was described. The essential restrictions of this technology 
for a realistic application concerning the critical flight phases 
of take-off and landing were discussed. As a solution for 
these problems the application of pattern recognition on 
board of an UAV in combination with a dynamic pattern 
projector on the ground was suggested.  Besides, this works 
is built on diverging scientific research in the area of pattern 
based VTOL UAV landing, the essential difference is the 
introduction of a dynamic, adaptive ground pattern, which 
can visualize different patterns in different scaling. 
Therefore, central problems of pattern-supported navigation 
can be solved with the proposed approach. The likelihood of 
a false positive on the basis of natural structures similar to 
the pattern can be drastically lowered when a pattern is 
confirmed only within a detected structured sequence of 
different patterns. Missing the pattern in low flight altitudes 
due to dimension problems are avoided until the touch-down 
because the patterns can be adapted in their size according to 
the flight altitude of the UAV. In addition, the pattern 
sequences can be used for a low rate data exchange. Thus, 
relevant information can be transferred to the approaching 
UAV, for example, a divergence of the landing path or 
special alignments or course corrections. 
To 
develop 
a 
dynamic 
pattern, 
different 
LED 
technologies were examined and checked on their 
applicability. The functionality of the draught was checked 
by successful system demonstrations. The identified 
functional LED technologies were further examined and two 
operational prototypes were developed for extended 
operational tests. These prototypes were operated in parallel 
and recorded with different IO sensors. On the set up test-
range, sensors and projectors were evaluated in defined 
distances. Based on this data and the detection results, 
statements about the future technologies concerning cameras 
and ground pattern were made and necessary changes in the 
approach were identified. In particular, the quality increases 
by a distant mounted diffusor, as well as the better luminous 
  
 
265
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

performance of the FlAPP will affect future works. 
Regardless of the pattern technology the detection algorithm 
is to be extended by the still missing pattern recognition for 
the new introduced patterns. Additionally, the development 
of a suitable pattern language as well as the safe ground 
pattern identification on base of pattern sequences has to be 
concluded. 
 
ACKNOWLEDGMENT 
The authors would like to thank their colleagues and 
students, who have contributed to the work presented in this 
paper. 
REFERENCES  
[1] F. Segor, C. K. Sung, R. Schoenbein, I. Tchouchenkov, and M. 
Kollmann, “Dynamic Pattern Utilization for Automatic UAV Control 
Support,” The Ninth International Conference on Systems ICONS, 
pp. 140-144, 2014. 
[2] S. Leuchter, T. Partmann, L. Berger, E. J. Blum, and R. Schönbein, 
“Karlsruhe generic agile ground station,” Beyerer J. (ed.), Future 
Security, 2nd Security Research Conference, Fraunhofer Defense and 
Security Alliance, pp. 159-162, 2007. 
[3] F. Segor, A. Bürkle, M. Kollmann, and R. Schönbein, “Instantaneous 
Autonomous Aerial Reconnaissance for Civil Applications - A UAV 
based approach to support security and rescue forces,” The 6th 
International Conference on Systems ICONS, pp. 72-76, 2011. 
[4] A. Bürkle, F. Segor, and M. Kollmann, “Towards Autonomous Micro 
UAV Swarms,” Journal of Intelligent & Robotic Systems 61, pp. 339-
353, 2011. 
[5] E. Santamaria, F. Segor, I. Tchouchenkov, and R. Schönbein, “Path 
Planning for Rapid Aerial Mapping with Unmanned Aircraft 
Systems,” The Eighth International Conference on Systems, pp. 82-
87, 2013. 
[6] C.-K. Sung and F. Segor, “Onboard pattern recognition for 
autonomous UAV landing,” Proc. SPIE 8499, Applications of Digital 
Image Processing XXXV, 84991K, October 2012. 
[7] S. Sharp, O. Shakernia, and S. Sastry, “A Vision System for Landing 
an Unmanned Aerial Vehicle,” Proc. of IEEE International 
Conference on Robotics and Automation, pp. 1720-1728, 2001. 
[8] S. Saripalli, “Vision-based Autonomous Landing of an Helicopter on 
a Moving Target,” AIAA Guidance Navigation and Control 
Conference, August 2009. 
[9] Y. Zhou, T. Wang, J. Liang, C. Wang, and Y. Zhang, “Structural 
target recognition algorithm for visual guidance of small unmanned 
helicopters,” IEEE International Conference on Robotics and 
Biomimetics (ROBIO), pp. 908-913, December 2012. 
[10] S. Yang, S. A. Scherer, and A. Zell, “An onboard monocular vision 
system for autonomous takeoff, hovering and landing of a micro 
aerial vehicle,” Journal of Intelligent and Robotic Systems 69(1-4), 
pp. 499-515, 2013. 
[11] W. Xiang, Y. Cao, and Z. Wang, “Automatic take-off and landing of 
a quad-rotor flying robot,” IEEE 24th Chinese Control and Decision 
Conference (CCDC), pp. 1251-1255, May 2012. 
[12] S. Lange, N. Sünderhauf, and P. Protzel, “Autonomous Landing for a 
Multirotor UAV Using Vision,” Workshop Proc. of SIMPAR 2008 
International Conferrence on Simulation, Modeling and Programming 
for Autonomous Robots, pp. 482-491, 2008. 
[13] T. S. Richardson, C. G. Jones, A. Likhoded, E. Sparks, A. Jordan, I. 
Cowling, and S. Willcox, “Automated Vision‐based Recovery of a 
Rotary Wing Unmanned Aerial Vehicle onto a Moving Platform,” 
Journal of Field Robotics 2013, pp. 667-684, 2013. 
[14] C.-K. Sung and F. Segor, “Adaptive Pattern for Autonomous UAV 
Guidance," Proc. SPIE 8856, Applications of Digital Image 
Processing XXXVI, 88560P, September 2013. 
 
 
 
266
International Journal on Advances in Systems and Measurements, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/systems_and_measurements/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

