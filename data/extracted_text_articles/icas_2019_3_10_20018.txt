A Navigational System for Quadcopter Remote Inspection of Offshore Substations
Elisabeth Welburn∗, Hassan Hakim Khalili†, Ananya Gupta‡, Joaquin Carrasco§ and Simon Watson¶
School of Electrical and Electronic Engineering
The University of Manchester, Manchester, UK M14 9PL
∗Email: Elisabeth.Welburn@manchester.ac.uk
† Email: Hassan.Hakimkhalili@manchester.ac.uk
‡ Email: Ananya.Gupta@manchester.ac.uk
§ Email: Joaquin.Carrasco@manchester.ac.uk
¶ Email: Simon.Watson@manchester.ac.uk
Abstract—Effective and safe maintenance of offshore infrastruc-
ture is hampered by its remote location. Robotic inspection can
provide a retroﬁt solution, improving safety for human personnel
by removing them from a potentially hazardous environment, and
also reduce operational costs. There are three primary challenges
for navigation around an offshore substation: low visibility, high
electromagnetic ﬁelds and the absence of Global Positioning
System (GPS) signals. This paper details a navigational system
that enables Unmanned Aerial Vehicles (UAVs) to operate within
a dark and GPS-denied environment.
Keywords–Robotics In Hazardous Fields; Aerial Robotics;
SLAM; Sensor-based Control.
I.
INTRODUCTION
The remote inspection and asset management of offshore
wind farms and the connection to shore will be worth up to 2
billion pounds annually by 2025. However, current methods of
inspection are dangerous for human personnel and introduce
high costs for the industry as a whole [1].
Currently, Supervisory Control and Data Acquisition
(SCADA) systems and thermal imaging inspections are being
used in data management to inform substation operations
and maintenance. However, the limited number of qualiﬁed
inspectors coupled with the high demand leads to common
unexpected failures. Automation could potentially alleviate this
by increasing inspection frequency and standardizing proce-
dures [2].
Robotic inspection platforms have the potential to ensure
the maintenance of vital infrastructure, reducing associated
expenditure and hazards [1]. However, this endeavour presents
unique challenges that must be overcome for it to become a
viable commercial method. Considering the offshore substation
environment in the context of a navigational system, the
inherently symmetrical nature coupled with the occlusion of
GPS signal may attribute to difﬁculty ascertaining an accurate
estimation of the robot’s global 6 Degree of Freedom (DoF)
pose. The high electromagnetic ﬁelds necessitate the use of
shielding, limiting external sensor hardware [3]. The presence
of high electromagnetic ﬁelds could potentially interfere with
the nominal operation of the propulsion motors [4]. To cir-
cumvent this, the use of a magnetometer within the proposed
navigational system will be neglected. Moreover, the sensor
payload must also be minimal to extend battery life, facilitating
the implementation of autonomous capabilities in this remote
location. Also, the absence of visible light limits the use of
vision-based odometry.
Three levels of autonomy can be deﬁned: pure tele-
operation, safe-guarded tele-operation and autonomous navi-
gation [5]. The navigational system presented within this paper
provides a method of remote tele-operation. However, the aim
is to extend this system to full autonomy in the future with
more sophisticated obstacle avoidance capabilities that account
for the electromagnetic ﬁelds.
This paper presents a navigational system for UAVs to op-
erate inside a High Voltage Direct Current (HVDC) valve hall.
The quadcopter is equipped with two 2D Light Detection and
Ranging (LiDAR) devices that are mounted perpendicularly to
each other, the combination of which provides a 3D estimation
of the robot pose. However, this estimation is, in part, based
upon relative movement of the surrounding landmarks between
frames and so is subject to a certain amount of drift. This is
further exacerbated by the repetitive and symmetrical nature of
these landmarks. To remedy this, the implementation of Quick
Response (QR) codes were investigated as global reference
points to correct for this accumulated error. Sensor fusion
was accomplished with the use of an Extended Kalman Filter
(EKF).
The remainder of this paper is structured as follows:
Section II of this paper will consider related works and Section
III will detail the system architecture, while Section IV will
analyse the collated results and several conclusions will be
drawn concerning further extensions of this work and the
viability of this system within industry.
II.
RELATED WORK
To inform system design, the state-of-the-art navigational
techniques were considered for UAVs as well as ground
vehicles, with the view of adapting these methods for UAV
navigation of a GPS-denied and dark environment.
A. Current Navigational Techniques
In [6], an autonomous navigational system was developed
for a ground vehicle deployed within a GPS-denied green-
house. This system used the Hector Simultaneous Localisa-
tion and Mapping (SLAM) Robot Operating System (ROS)
package, that is also used within this system, and combined
this with a potential ﬁelds path planning algorithm. Structural
changes due to the growth of crops were accounted in the path
planning algorithm while being safe to operate in the presence
of humans. This is reminiscent of faults occurring and causing
a ﬂuctuation of electromagnetic ﬁelds inside the HVDC valve
hall environment, changing the required clearances to maintain
nominal operation of the UAV.
In [7], a UAV was deployed into a GPS-denied, dark tunnel
where a perception system comprising of a near-infra-red
32
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-712-2
ICAS 2019 : The Fifteenth International Conference on Autonomic and Autonomous Systems

stereo camera, ﬂashing LEDs, inertial sensors, and a 3D depth
sensor to derive the geometry of the environment. A horizon-
based planner accounted for the system’s uncertainty during
mission execution and generated collision-free exploration
paths. However, the environment here was unknown and static,
whereas the valve hall geometry is known and faults within the
racks can cause a ﬂuctuation of the electromagnetic ﬁelds.
In [2], a robot transverses substations with the use of a
rail system and collects IR and visible images, positioning,
time and component description and transmits this, as well
as energy, to a control centre with the use of the rails.
This mitigates faults caused by intermediate electromagnetic
interference between the robot and the control centre. Also,
magnetic references on the rails negate the need for markings
implemented onto the substation infrastructure. Also, the use
of the commercial voltage for Brazil facilitates installation
in other locations. However, the rail-based robot requires the
installation of an extensive rail system in existing substations
to operate [2]. The system proposed within this paper provides
a retro-ﬁt solution that could potentially accomplish the same
task.
B. Vision-Based Odometry Techniques
The dark nature of the valve hall restricts the perception
within the visible spectrum. However, a Near-Infrared (NIR)
camera will be ﬁtted to the drone for fault detection purposes
and also a LED spotlight can provide limited ambient lighting
in the immediate vicinity.
In general, though visual odometry is useful for local posi-
tion control and stability, these methods often suffer from long-
term drifts and are not suitable for building large-scale maps
[8]. RGB-D cameras provide both a colour image and per-
pixel depth estimates and are prominent within mobile robotic
platforms due to their richness of the data collected coupled
with their reducing cost. In [8], a system for the navigation
of a micro-air vehicle within a cluttered, GPS-denied indoor
environments with the use of an on-board RGB-D camera
and an Inertial Measurement Unit (IMU) was developed. This
system periodically corrected for the drift present within the
local state estimation based upon visual odometry with results
from the RGB-D mapping algorithm [9]. However, this system
was unsuitable for real-time situations as the loop closing and
SLAM algorithms were not sufﬁciently fast to be run on an
on-board processor.
The use of Quick Response codes within absolute local-
isation methods for indoor mobile robots is widespread due
to their large data storage capabilities, small size, low cost
and simple implementation. A possible issue with their use is
that the recognition rate is reduced if the QR code is small
within the camera’s ﬁeld of view or the robot moves too
fast. Considering this application in real-time, the processing
resources are sufﬁciently low to enable use of the QR codes,
as it was found within [10], that the time taken to calculate
the relative position of the robot was between 20 to 30 ms.
Within [10], an industrial camera was mounted onto a
mobile ground robot pointing upwards in order to identify
QR codes mounted to the ceiling. Meanwhile, a laser range-
ﬁnder was used for object detection as well as the construction
of a 2D map with the use of a Rao-Blackwellized particle
ﬁlter. The Dijkstra algorithm, as well as the Dyanmic Window
Approach were used to implement both local and global path
planning capabilities [10]. However, this system is not usable
in situations where the QR codes were occluded from the
camera’s ﬁeld of view due to sheltering obstacles or ambient
light. Odometry data was used to compensate for the drift
occurring within the short time interval travelling between the
QR codes, whereby the error accumulation was mitigated with
the use of additional sensor inputs [10].
In [11], a tailored extended H∞ ﬁlter (EHF) was imple-
mented. This ﬁlter fused both odometry and gyroscope data
with pose estimates based upon QR code landmarks. However,
this method is more computationally expensive compared to an
EKF, taking longer to converge on an accurate estimate [11],
which is paramount when instructing real-time control as in
this scenario.
III.
SYSTEM ARCHITECTURE
Within this section, the architecture of the navigational
system as depicted in Figure 1 is discussed.
Figure 1. Software Architecture for the Proposed Navigational System.
A. Mobile Robotic Platform
The quadcopter utilised for the proof-of-concept system is
the Hector quadrotor Robot Operating System (ROS) package
[12] due to its pre-existing and well-documented integration
with the Gazebo simulator. The visual geometry was written
within COLLADA format and the collision geometry was
modelled as a STL mesh. A low polygon count reduced the
demand from rendering the model, allowing simulations to be
ran at a higher percentage of real-time. The propellers were
represented by actuator discs to facilitate the maintenance of
boundary conditions [12]. The hector quadrotor is depicted
below in Figure 2.
Figure 2. The Hector quadrotor rendered within the Gazebo simulator.
Image taken from [12].
The CAD model of an offshore substation, as shown in Fig-
ure 3, with the correct clearances as the real-time environment
was constructed and used to collate results.
33
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-712-2
ICAS 2019 : The Fifteenth International Conference on Autonomic and Autonomous Systems

Figure 3. CAD model of the HVDC valve hall
B. 2D SLAM Algorithm
Low-cost laser range-ﬁnders are prevalent in autonomous
robot applications due to their low price and ability to trace
terrains and structures in the contiguous area, while consuming
relatively little power [13].
In the proposed system, two Hokuyo utm-30lx LiDARs
were mounted perpendicularly to each other. It was assumed
that the z-axis was out-of-plane relative to the ground plane
and the x-axis was pointing in the forward direction of the
quadcopter. The horizontal, planar LiDAR was used within the
2D SLAM algorithm to construct a map of the surroundings.
A ROS node, Hector Mapping [14], was selected as the 2D
SLAM algorithm, of which the only requirement was a high
frequency laser scanner, such as the Hokuyo utm-30lx LiDAR
in this scenario.
C. Floor Extraction
For height estimation and greater spatial understanding, a
secondary LiDAR was mounted, perpendicular to the primary
LiDAR, onto the underside of Hector quadrotor. The vertical
LiDAR produced a 2D vertical laser scan of the environment.
A split-and-merge algorithm [15] was then implemented to
differentiate the walls and ﬂoor using the relative angle of
the incident laser endpoints. The roll and pitch recorded by
the EKF was processed and the calculated relative angles
of the identiﬁed line segments were rotated to avoid falsely
recognising the walls as the ﬂoor during operation.
A laser pointing vertically downwards was also considered
as the method of height estimation, however this is a less
robust method than the aforementioned secondary LiDAR.
This is because if the quadrotor turned near the boundaries
of the space, the singular laser point could potentially rotate
to be incidental on walls or substation racks. This could be
mitigated with the use of fusion with the orientation from
the IMU device to account for the laser rotation. However,
IMU data suffers from drift and so a secondary LiDAR was
used in the implementation to provide more information of the
transformation of the laser scan points relative to the quadrotor.
D. QR Codes as Global Landmarks
Vision-based odometry is generally computationally inten-
sive and also suffers from robustness under varying lighting
conditions [11]. However, in this scenario there is an absence
of visible light and so ambient light levels are constant. Also,
vision-based odometry was implemented with the view to
periodically correct for drift in the 2D SLAM algorithm pose
estimations.
A FLIR One Near-Infrared (NIR) camera of a spectral
range between 8 - 14 µm will be utilised to enable simultane-
ous QR code detection and faults within the infrared spectrum.
An infrared LED emitting light between 750 - 950 nm will be
mounted on top of the camera, illuminating the proximal ﬁeld
of view. However, for the purposes of this simulation, a generic
camera is created within a virtual world lit by ambient lighting
to ensure an accurate estimation of the nominal accuracy of
the vision-based odometry.
The QR codes were generated with the use of the open-
source library, arUco markers. These were then placed on
the racks within the virtual substation environment at regular
intervals. The ROS package, ﬁducial SLAM [16], was used to
both identify the unique identiﬁer of the QR code as well as
produce a 6 DoF pose estimation of the drone using the known
global poses of the QR codes.
The QR codes could potentially be used in two capacities
during drone operation. Correct identiﬁcation of a unique QR
code indicates the drone is within the correct general vicinity
of the rack. These could form the basis of a command interface
to set the goal destination that determines the generated path.
The unique identiﬁers of visible QR codes in the camera ﬁeld
of view are shown in Figure 3.
Figure 4. The virtual UAV inspecting a substation rack. Inset is the
camera ﬁeld of view.
Alternatively, the pose of the visible QR marker in a known
location can be processed to output a 6 DoF pose estimation
of the drone that could have been later fused with the other
sensor measurements within the EKF. However, this was found
to produce erroneous estimations of 6 DoF pose, as discussed
later.
IV.
SENSOR FUSION
Sensor fusion was necessary within this system to identify
the optimal estimate of the UAVs pose. Considering the sensor
measurements modelled in this section, (xs, ys, zs, φs, θs, Ψs)
was produced from the 2D SLAM performed using the planar
LiDAR, z from the height measurement using the perpendicu-
lar LiDAR and, ﬁnally, (x, y, z, φ, θ, Ψ) was produced from the
vision-based odometry system based upon QR codes visible to
the on-board camera.
First, the orientation measurements must be converted from
a quarternion in the local frame to Euler angles that are
relative to the global frame. A function available within the
ROS python library tf was used for this conversion. For the
purposes of this system, the starting pose of the spawned robot
34
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-712-2
ICAS 2019 : The Fifteenth International Conference on Autonomic and Autonomous Systems

was assumed to be the global frame in terms of the way-
point commands that were converted into command velocities.
However, this coordinate frame was mapped onto the 2D
occupancy grid constructed to utilise the A* path planning
algorithm.
The measurements were taken at different unsynchronised
rates. To accompany this, each sensor measurement was sam-
pled with each new IMU measurement at a rate of 100 Hz. In
this way, a sufﬁcient sample rate was ensured.
Kalman ﬁlters are algorithms for the estimation of dynamic
state variables by combining state predictions with measure-
ments. For discrete systems, the future values of the state
variables can be predicted using Kalman ﬁlters.
The EKF can overcome the linearity assumption of the
Kalman Filter that both the motion model and sensor model are
linear Gaussian [13]. Within this system, an extended Kalman
ﬁlter was implemented, where the non-linearity is introduced
with the continuously-variable rotation relative to the global
frame.
For time-invariant systems, the function f computed the
predicted state from the previous estimate, and the h function
computed the output. The variables, wk and vk, represented
the process and observation white noises, respectively, i. e.
xk+1 = f(xk, uk) + wk
(1)
zk = h(xk) + vk
(2)
The white noises wk and vk were assumed to be zero mean
and covariances Qk and Rk, respectively.
For the purposes of this Kalman ﬁlter, all variables were
within the global frame. The values used for initialisation of
the Kalman ﬁlter were the coordinates of spawning the robot
model.
In the case of the Hector quadcopter, the state variables
were updated with the use of inertial measurements from
the IMU unit. The state vector, X, comprised of these state
variables:
X = [x
y
z
˙x
˙y
˙z
φ
θ
Ψ]T
(3)
where x, y, z were the positions on the X, Y, and Z axes and
φ was the roll, θ, the pitch and Ψ, the yaw of the quadrotor.
The global displacement, s, in each of the X, Y and
Z axes was modelled using dead reckoning with the initial
displacement, s0, the rotation matrix that transforms between
the body frame to the inertial frame, R, the initial velocity at
the start of the time interval v0, IMU acceleration within the
IMU frame of reference, a, the gravitational constant, g, and
the length of the time interval, t.
With some abuse of notation, this relationship was encap-
sulated in the dynamic matrix, f, that described how the state
evolves to the next time step, as below:
f =


s0 + ˙s△T + 1
2R(a − g)△T 2
˙v0 + △T

R(a − g)
!
α0 + △TΘ

 ,
(4)
where Θ was the mapping of the angular velocities in the body
frame (p, q, r) to the changes in the Euler angles within the
inertial frame [17], e.g.
Θ =


1
sin φ tan θ
cos φ tan θ
0
cos φ
− sin φ
0
sin φ
cos θ
cos φ
cos θ

 .
(5)
The measurement function, h, was given by
h = [x
y
z
φ
θ
Ψ]T .
(6)
The linearisation of (1) provided the state transition matrix,
Fk, by computing the Jacobian of the dynamic matrix f with
respect to the state vector. Similarly, the observation matrix
Hk was also be deﬁned as the Jacobian of the measurement
matrix, h with respect to the state vector.
The control inputs into the system were assumed to be the
linear accelerations and the angular velocities as measured by
the IMU, i.e.
u =

¨x
¨y
¨z
˙φ
˙θ
˙Ψ
T
(7)
The time update of the EKF algorithm was given by
ˆxk|k−1 = f(xk−1, uk−1),
(8)
Pk|k−1 = FkPk−1|k−1F T
k + Qk.
(9)
The measurement update steps were then computed to
adjust the Kalman gain, Kk and to update the estimate with
the actual measurement, zk, and to update the error covariance,
Pk|k. The measurement residual, ey as well as the covariance
residual, Sk were also calculated.
eyk = zk − h(ˆxk|k−1)
(10)
Sk = HkPk|k−1HT
k + Rk
(11)
Kk = Pk|k−1HT
k S−1
k
(12)
ˆxk|k = ˆxk|k−1 + Kkeyk
(13)
Pk|k = (I − KkHk)Pk|k−1
(14)
This method was less computationally expensive in com-
parison to other methods, such as the H∞ ﬁlter (EHF) [11].
One of the drawbacks of an extended Kalman ﬁlter is that it is
not an optimal estimator. Moreover, if the initial state vector is
wrong, the ﬁlter will quickly diverge due to its linearisation. As
a result, the EKF requires extensive tuning of these parameters.
V.
PATH PLANNING ALGORITHM
The ﬁnal pose estimation from the EKF was fed into an A*
path planning module [18]. The robot radius was considered
greater than the nominal dimensions, ensuring clearances from
the high electromagnetic ﬁelds present within the substation
racks were maintained.
Prior to this, a 2D occupancy grid of the substation plan
was constructed using the known dimensions of the CAD
model. In terms of command way-points, height correction was
performed ﬁrst to adjust the drone to the speciﬁed goal height
because of the largely constant geometry of the environment
within the vertical plane. Then, an A* path planning algorithm
was then used to generate the path shown in Figure 5 within
the substation.
35
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-712-2
ICAS 2019 : The Fifteenth International Conference on Autonomic and Autonomous Systems

Figure 5. A 2D occupancy grid and path generated by the A* star
algorithm
TABLE I. ERROR IN EKF OUTPUT
Global Axis
Distance Travelled (m)
Maximum Error (m)
Maximum Error (%)
x
39
0.65
1.67
y
5.5
0.3
5.45
z
5
0.015
0.3
This produced the optimal trajectory consisting of 0.5 m
increments between the start position and goal position. After
this, alterations to the orientation of the drone were made to
enable 360 degree inspection.
VI.
RESULTS
To ascertain a baseline and compute the errors of the
constituent algorithms, the standard deviations of each of the
measurements were calculated. These standard deviations were
then used as a baseline for tuning of the Q and R matrices
within the EKF.
The pose estimation generated from the EKF during the
mapped trajectory in Figure 5 was used to gauge the viability
of the proposed navigational system. The IMU measurements,
as well as the hector mapping 2D pose and extracted ﬂoor
height were fused by the EKF. The 3D position of the UAV is
compared to the ground truth within Figure 6. The error present
within the EKF output is tabulated in Table I. An error of
1.67% in the x-axis throughout the course of a twenty-minute
mission is tolerable. However, an error of 5.45% in the y-axis
is unsatisfactory and further tuning of the EKF is required to
alleviate this. The height estimation algorithm was found to
produce the least error, with a 0.3% throughout the length of
the mission. The average battery life of a UAV is between ten
to ﬁfteen minutes, depending on payload and so these ﬁgures
represent a probable overestimation of the drift present within
the EKF.
The camera stream was also recorded, whereby visible QR
codes unique identiﬁers were overlaid, as seen in Figure 4. The
6 DoF pose estimation from the visible QR codes was also
collated to evaluate whether this data should be incorporated
into the EKF. However, as can be seen by Figure 7, these pose
estimations are extremely erratic and will not contribute to the
overall stability of the EKF upon fusion.
The inaccurate 6 DoF pose estimation produced from the
fiducial slam could potentially be due to the 2D nature of
the QR codes hindering precise depth perception. It also may
be due to the monocular nature of the camera.
Figure 6. EKF pose estimation
Considering Figure 7, ultimately direct pose estimation
from ﬁducial slam was not implemented within the EKF.
However, the unique identiﬁers displayed within the camera
stream could potentially facilitate inspection of substation
racks by providing a visual veriﬁcation of the current vicinity
of the UAV.
Figure 7. EKF pose estimation with the ﬁducial slam results
In summary, these set of results suggest that this framework
could potentially be adapted for implementation into a real-
world system.
VII.
CONCLUSION
In conclusion, the proposed, proof-of-concept, navigational
system paves the way for UAV navigation within dark, GPS-
denied environments. This was acheived with the fusion of
IMU data with processed LiDAR measurements. Possible
mechanisms of correction via vision-based odometry upon
the identiﬁcation of QR codes within the environment were
explored and it was concluded that though the QR codes
provide visual cues of the drones current position they fail to
act as reference points to generate an accurate 6 DoF pose. This
system provides a retro-ﬁt solution for the remote inspection
of substations, merely requiring the careful placement of QR
codes within the environment.
Future work includes the implementation of this naviga-
tional system onto a drone within an indoor, conﬁned and
dark environment. The computation and sensing required for
36
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-712-2
ICAS 2019 : The Fifteenth International Conference on Autonomic and Autonomous Systems

local position control will be performed on-board the vehicle,
reducing the dependence on unreliable wireless links [8]. The
path planning capabilities will also be expanded to account for
the presence of electromagnetic ﬁelds with the implementation
of a modiﬁed potential ﬁelds algorithm. Moreover, this system
could potentially pave the way for the use of thermal imaging
to identify faults within the substation infrastructure. This is
advantageous in comparision to existing methods because it
involves non-contact precision temperature measurements and
non-destructive testing [5].
ACKNOWLEDGMENTS
This work was supported by the Holistic Operation and
Maintenance for Energy from Offshore Wind Farms (HOME
Offshore) project (EPSRC Grant Number: EP/P009743/1) and
the Robotics and Artiﬁcal Intelligence for Nuclear (RAIN)
project (EPSRC Grant Number: EP/R026084/1). The authors
would like to thank both Dr. Andrew West and Dr. Thomas
Wright of the University of Manchester for their continued
support.
REFERENCES
[1]
E. M. Barnes et al., “Technology Drivers in Windfarm Asset Manage-
ment Position Paper,” 2018, pp. 1–46.
[2]
B. P. Silva et al., “On-rail solution for autonomous inspections in
electrical substations,” Infrared Physics & Technology, vol. 90, May
2018, pp. 53–58. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S1350449517307247
[3]
M. Heggo et al., “Evaluation and mitigation of high electrostatic ﬁelds
on operation of aerial inspections vehicles in hvdc environments,” in
EERA DeepWind19, Jan 2019.
[4]
M Heggo et al., “Evaluation and mitigation of offshore hvdc valve hall
magnetic ﬁeld impact on inspection quadcopter propulsion motors,” in
EERA DeepWind19, Jan 2019.
[5]
P.
Rea
and
E.
Ottaviano,
“Design
and
development
of
an
Inspection Robotic System for indoor applications,” Robotics and
Computer-Integrated Manufacturing, vol. 49, Feb 2018, pp. 143–
151. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/
S0736584517300613
[6]
E. H. Harik, A. Korsaeth, E. H. C. Harik, and A. Korsaeth, “Combining
Hector SLAM and Artiﬁcial Potential Field for Autonomous Navigation
Inside a Greenhouse,” Robotics, vol. 7, no. 2, May 2018, p. 22.
[Online]. Available: http://www.mdpi.com/2218-6581/7/2/22
[7]
C. Papachristos, S. Khattak, and K. Alexis, “Autonomous exploration
of visually-degraded environments using aerial robots,” in 2017
International Conference on Unmanned Aircraft Systems (ICUAS).
IEEE, Jun 2017, pp. 775–780. [Online]. Available: http://ieeexplore.
ieee.org/document/7991510/
[8]
Huang et al., “Visual Odometry and Mapping for Autonomous Flight
Using an RGB-D Camera.”
Springer, Cham, 2017, pp. 235–252. [On-
line]. Available: http://link.springer.com/10.1007/978-3-319-29363-914
[9]
P. Henry, M. Krainin, E. Herbst, X. Ren, and D. Fox, “RGB-D Mapping:
Using Depth Cameras for Dense 3D Modeling of Indoor Environments.”
Springer, Berlin, Heidelberg, 2014, pp. 477–491. [Online]. Available:
http://link.springer.com/10.1007/978-3-642-28572-133
[10]
H. Zhang, C. Zhang, W. Yang, and C.-Y. Chen, “Localization and
navigation using QR code for mobile robot in indoor environment,”
in 2015 IEEE International Conference on Robotics and Biomimetics
(ROBIO).
IEEE, Dec 2015, pp. 2501–2506. [Online]. Available:
http://ieeexplore.ieee.org/document/7419715/
[11]
P. Nazemzadeh, D. Fontanelli, D. Macii, and L. Palopoli, “Indoor
Localization of Mobile Robots Through QR Code Detection and Dead
Reckoning Data Fusion,” IEEE/ASME Transactions on Mechatronics,
vol. 22, no. 6, Dec 2017, pp. 2588–2599. [Online]. Available:
http://ieeexplore.ieee.org/document/8066377/
[12]
J. Meyer, A. Sendobry, S. Kohlbrecher, U. Klingauf, and O. von
Stryk, “Comprehensive simulation of quadrotor UAVs using ROS and
gazebo,” in Simulation, Modeling, and Programming for Autonomous
Robots.
Springer Berlin Heidelberg, 2012, pp. 400–411. [Online].
Available: https://doi.org/10.1007%2F978-3-642-34327-8 36
[13]
W.-C. Jiang, M.-Y. Ju, Y.-J. Chen, and W.-C. Jiang, “Implementation
of Odometry with EKF in Hector SLAM Methods,” International
Journal of Automation and Smart Technology, vol. 8, no. 1, Mar
2018, pp. 9–18. [Online]. Available: http://www.ausmt.org/index.php/
AUSMT/article/view/1558
[14]
“Hector
mapping,”
accessed:
2019-05-09.
[Online].
Available:
\url{http://wiki.ros.org/hector mapping}
[15]
“Laser line extraction,” accessed: 2019-05-09. [Online]. Available:
https://github.com/kam3k/laser line extraction
[16]
“Fiducial slam,” accessed: 2019-05-09. [Online]. Available: https:
//github.com/UbiquityRobotics/ﬁducials
[17]
T. I. Fossen, Handbook of Marine Craft Hydrodynamics and Motion
Control.
Chichester, UK: John Wiley & Sons, Ltd, Apr 2011.
[Online]. Available: http://doi.wiley.com/10.1002/9781119994138
[18]
“Pythonrobotics,” accessed: 2019-05-09. [Online]. Available: https:
//github.com/AtsushiSakai/PythonRobotics
37
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-712-2
ICAS 2019 : The Fifteenth International Conference on Autonomic and Autonomous Systems

