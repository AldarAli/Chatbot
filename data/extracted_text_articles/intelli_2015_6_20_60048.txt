Prediction of Golden Time Using SVM for Recovering SIS in Severe Post-LOCA 
Circumstances 
 
Kwae Hwan Yoo, Dong Yeong Kim, Ju Hyun Back, Man Gyun Na 
 Dept. Nuclear Engineering of Chosun University 
Chosun University 
Gwangju, Republic of Korea 
 e-mails: yooqh@naver.com, doo891221@naver.com, magyna@chosun.ac.kr, bjh4210@naver.com 
 
 
Abstract— After the Fukushima accident, the nuclear power 
plant (NPP) accident that occurred as a result of the East 
Japan Great Earthquake, the safety problem of NPPs has 
emerged as a global concern. As a result, many countries using 
nuclear energy are conducting research to improve the safety 
of NPPs. In this study, we predicted the golden time of safety 
injection system (SIS) recovery for accomplishing the reactor 
cold shutdown and preventing reactor vessel (RV) failure. The 
support vector machine (SVM) was used to predict the golden 
time for the SIS recovery in loss-of-coolant accident (LOCA) 
circumstances. If the golden time of SIS for accident recovery 
is predicted, the core will not be exposed through appropriate 
action. Also, the RV failure will be prevented by the cooling 
water injection even if the reactor core is exposed. These 
various golden time data are thought to be very useful to 
quickly deal with the actual accident. 
Keywords-Golden time, Support vector machine, Loss of 
coolant accident, Core uncovery, Reactor vessel failure. 
I. 
 INTRODUCTION 
After the Fukushima accident, the nuclear power plant 
(NPP) accident that occurred as a result of the East Japan 
Great Earthquake, the safety problem of NPPs has emerged 
as a global concern. As a result, many countries using 
nuclear energy are conducting research to improve the safety 
of NPPs. In addition, the interest in severe accidents in 
nuclear power plants has been increasing. Nuclear power 
plants are designed in consideration of design basis accidents 
(DBAs). DBAs such as loss-of-coolant accident (LOCA) in 
NPP may lead to serious accidents that exceed the DBAs due 
to failure of safety systems. If the heat removal system is not 
working properly, the core uncovery and the reactor vessel 
(RV) failure may be possible [1][2]. Several researches 
acquiring important information under severe accident using 
artificial intelligence methodologies have been conducted 
[3]–[5]. 
In this study, the golden time of SIS recovery for 
accomplishing the reactor cold shutdown and preventing RV 
failure according to LOCA break sizes were predicted by 
using the support vector machine (SVM) model when safety 
injection system (SIS) was not operating normally. The data 
was obtained by simulating severe accident scenarios for the 
Optimized Power Reactor 1000(OPR1000) using MAAP 
code. 
Section II explains the methodology of SVM and its 
optimization. Section III describes accident scenarios applied 
in this study. Section IV shows the prediction performance 
of the SVM model and its results.  
II. 
GOLDEN TIME PREDICTION USING SVM MODEL 
The SVM was used to predict the golden time for the SIS 
recovery in LOCA circumstances. The SVM model can be 
applied to classification problem and regression analysis. 
A. SVM method 
The SVM model is an algorithm for learning linear 
classifiers. SVM is a learning system using a high 
dimensional feature space. It yields prediction functions that 
are expanded on a subset of support vectors. Support vector 
regression (SVR) is the most common application form of 
SVMs. SVR model is to map nonlinearly the original data x  
into higher dimensional feature space and to conduct linear 
regression. Hence, given a data set {
} 1
(
,
)
N
m
i
i
i
y
R
R
= ∈
×
x
 where 
ix  is the input vector to an SVR model, 
iy  is the actual 
output value, N  is the total number of data points used to 
develop the SVR model, the SVR is based on the following 
regression function [6]: 
 
1
( )
( )
( )
N
T
i
i
i
y
f
w
b
b
φ
=
=
=
+
=
+
∑
x
x
w
φ x
 
(1) 
where 
 
1
2
[
 
] ,
N T
w w
w
=
⋅⋅⋅
w
1
[  2
 
 
N ]T
φ φ
φ
=
⋅⋅⋅
φ
 
 
The function 
iφ  is called feature, and parameters w  and b  
are support vector weight and bias. After the input vector   
x  is mapped into vector ( )
φ x
 of a high dimensional kernel-
induced feature space, the nonlinear regression model is 
turned into a linear regression model in the feature space. 
These parameters can be calculated by minimizing the 
following regularized risk function: 
 
1
1
( )
(
)
2
N
T
i
i
i
R
y
f
ε
λ
=
=
+
−
∑
w
w w
x
 
(2) 
118
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

where 
 
0
(
)
(
)
i
i
i
i
y
f
y
f
ε
ε

−
= 
−
−

x
x
(
)
i
i
if y
f
otherwise
ε
−
<
x
 (3) 
The first term of (2) is weight vector norm which 
characterizes the complexity of the SVR models and the 
second term is an estimation error. The parameters λ  and ε   
are user-defined parameters, and 
(
)
i
i
y
f
ε
−
x
 is called the 
ε -insensitive loss function [7]. The loss equals zero if  the 
predicted value 
( )
f x  is within an error level  ε , and for all 
other predicted point outside the error level ε , the loss is 
equal to the magnitude of the difference between the 
predicted value and  the error level  ε  (refer to Figure. 1). 
Increasing the insensitivity zone means a reduction in the 
requirement for accuracy of the estimation and a decrease in 
the number of support vectors (SVs), leading to data 
compression. In addition, as understood from Figure 2, 
increasing the insensitivity zone has smoothing effects on 
modeling on highly noisy polluted data. 
The regularization parameter λ  of (2) is used to ensure 
good generalization of the SVR model. An increase in the 
regularization parameter penalizes larger error, which leads 
to a decrease in the estimation error. The decrease in the 
estimation error can also be achieved easily by increasing the 
weight vector norm of the first term of (2). However, an 
increase in the weight vector norm does not ensure good 
generalization of the SVR model. This generalization 
property is of particular interest to data-based model 
development because a good model is not a model that 
performs well on only training data but a model that 
performs well even on other data that is no training data. 
In classical support vector regression, the proper value 
for the parameter ε  is difficult to determine beforehand. 
Minimizing the regularized risk function of (2) is equivalent 
to minimizing the following constrained risk function: 
 
*
1
1
( , ,
)
(
)
2
N
T
i
i
i
R
λ
ξ
ξ
∗
=
=
+
+
∑
w
w w
ξ ξ
 
(4) 
 
ε
ε
−
ε
L
f (x)
y −
 
Figure 1.  Linear sensitivity loss function. 
y
x
Regression function
y=f(x)
iξ
j
ξ ∗
iy
jy
ε
observed point
observed point
 
Figure 2.  Parameters for the SVR models [8]. 
subject to the constraints 
 
*
*
( )
,
1,2,
,
( )
,
1,2,
,
,
0,
1,2,
,
T
i
i
T
i
i
i
i
y
x
b
i
N
x
b
y
i
N
i
N
ε
ξ
ε
ξ
ξ ξ

−
−
≤
+
=
⋅⋅⋅

+
−
≤
+
=
⋅⋅⋅


≥
=
⋅⋅⋅

w
w
φ
φ
 
(5) 
The parameters 
[
]
1
2
T
N
ξ ξ
ξ
=
⋅⋅⋅
ξ
 and 
1
2
T
N
ξ ξ
ξ
∗
∗
∗
∗


=
⋅⋅⋅


ξ
 are 
the slack variables that represent the upper and lower 
constraints on the output of the system, and are positive 
values (refer to Figure. 2). 
The constrained optimization problem of (4) can be 
solved by applying the Lagrange multiplier technique to (4) 
and (5), and using a standard quadratic programming 
technique [8], [9]. Finally, the regression function of (1) is 
derived as 
 
*
1
1
( )
(
)
(
) ( )
( ,
)
N
T
i
i
i
i
N
i
i
i
y
f
b
K
b
α
α
β
=
=
=
=
−
+
=
+
∑
∑
x
x
x
x x
φ
φ
 
(6) 
where 
( ,
)
(
) ( )
T
i
i
K
=
x x
x
x
φ
φ
 is known as the kernel 
function and the coefficient 
iβ  is expressed as the Lagrange 
multiplier 
i
α  and 
i*
α . In this study, the SVR model uses the 
following radial basis kernel function [8]: 
 
2
(
) (
)
( ,
)
exp
2
T
i
i
i
K
σ


−
−
=
−



x
x
x
x
x x
 
(7) 
Many of the coefficients 
iβ  are nonzero values, and the 
training data points 
ix  corresponding to the nonzero values, 
which are known as SVs, have an estimation error greater 
than or equal to the insensitivity zone. 
119
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

B. Optimization of SVM model 
It is important to use good data and input variables 
because the SVR model is a data-based model. Therefore, we 
are required to select the input variables and optimize the 
related parameters in the SVR model. In this study, GAs 
were used to select the input variables and optimize the 
parameters of the SVR model: the insensitivity zone ε , 
regularization parameter λ , and radial basis kernel function 
parameter σ . 
GA is a search algorithm based on the mechanics of 
natural selection and natural genetics. In GA, the term 
chromosome typically refers to a candidate solution to a 
problem, generally encoded as a bit string. Because the GA 
is used to select the input variables and optimize the SVR 
parameters, the chromosome consists of a part of the 
parameter optimization and a part of the input variable 
selection. An allele in a bit string is either 0 or 1. The 
genotype of an individual in a GA using a bit string is simply 
the configuration of bits in that individual’s chromosome. 
Each chromosome can be thought of as a point in the search 
space of candidate solutions. The GA processes populations 
of chromosomes, successively replacing one such population 
with another. The GA requires a fitness function that assigns 
a score (fitness) to each chromosome in the current 
population. The fitness of a chromosome depends on how 
well that chromosome solves the problem at hand [11]–[13]. 
A fitness function to evaluate the appropriate level is 
proposed as follows: 
 
(
)
1
1
2
2
exp
F
E
E
µ
µ
=
−
−
 
(8) 
where 
1μ  and 
2
μ  are weighting factors, and 
1
E  and 
2
E  are 
the root-mean-square (RMS) error and maximum absolute 
error, respectively. 
1
E  and 
2
E  can be described as follows: 
 
(
)
ˆ
N
2
1
k
k
k=1
1
E =
N ∑ y - y
 
(9) 
 
{
}
2
ˆ
max
k
k
k
E
y
y
=
−
 
(10) 
In (9), N  is the number of data points, and 
ky and ˆ ky  
are the target values and estimated values, respectively. The 
GA minimizes the weighted sum of the RMS error and the 
maximum absolute error. 
III. 
ACCIDENT SCENARIOS 
It was assumed that there were a variety of situations for 
SIS failure. Accident scenarios were proposed according to 
break sizes (270 beak sizes) relative to the double ended 
guillotine break (DEGB), and High and Low Pressure Safety 
Injection systems (HPSI, LPSI) actuation status in hot-leg 
LOCA and cold-leg LOCA. It was assumed that safety 
injection tank (SIT) and Containment Spray System (CSS) 
were normally actuated [10]. 
The SIS including SIT, HPSI, and LPSI actuates 
automatically as soon as the safety injection actuation signal 
(SIAS) is generated based on low pressurizer pressure and 
high containment pressure. If the SIT that is a passive system 
actuates normally but the HPSI and LPSI systems do not 
actuate due to failure, the reactor core will be uncovered and 
then the RV will rupture. 
Through the MAAP simulations, a total 540 data of 
severe accident scenarios were obtained. This data was 
composed of 270 pieces of hot-leg LOCA and 270 pieces of 
cold-leg LOCA. Simulation scenarios were assumed for the 
four cases. It was assumed for case 1 that the LPSI system 
was failed and the HPSI system was not operated at first but 
operated late in hot-leg break. For case 2, it was assumed 
that the LPSI system was failed and the HPSI system 
actuation was delayed in hot-leg break. For case 3 it was 
assumed that the LPSI system was failed and the HPSI 
system actuation was delayed in cold-leg break. Finally, for 
case 4 it was assumed that the LPSI system failed and the 
HPSI system actuation was delayed in cold-leg break. 
Simulations were conducted according to LOCA break size 
for each case (Table I). The purpose of this study was to 
predict the golden time for recovering the SIS to prevent the 
core uncovery and RV failure when SIS actuation is delayed 
due to problems. The scenarios are similar to accident 
scenarios in a previous study [2]. 
IV. 
DETERMINING THE SIS GOLDEN TIME  
A. Prediction performance of the SVM model 
Table II summarizes the prediction performance results 
of the SVM model (HPSI delay). This table shows that the 
RMS errors for training data are approximately 12.1%, 
0.57%, 18.3% and 0.35% for the two LOCA positions, and 
for the core uncovery and RV failure, respectively. The RMS 
errors for the test data are approximately 10.86%, 1.02%, 
21.37% and 0.56%. 
Table III summarizes the prediction performance results 
of the SVM model (LPSI delay). This table shows that the 
RMS errors for training data are approximately 1.62%, 0.6%, 
1.66% and 2.85% for the two LOCA positions, and for the 
core uncovery and RV failure, respectively. The RMS errors 
for the test data are approximately 1.69%, 0.74%, 1.88% and 
2.78%. 
 
TABLE I.  
SIMULATION CASES 
Case 
Location 
SIT 
Operation 
CSS 
Operation 
HPSI 
Operation 
LPSI 
Operation 
1 
Hot-leg 
Success 
Inj & Rec 
Delay Inj 
& Rec 
N/A 
2 
N/A 
Delay Inj 
& Rec 
3 
Cold-leg 
Success 
Inj & Rec 
Delay Inj 
& Rec 
N/A 
4 
N/A 
Delay Inj 
& Rec 
120
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

TABLE II.  
PREDICTION PERFORMANCE OF SVM MODEL (HPSI 
DELAY) 
Break 
position HPSI delay 
Training Data 
Test data 
Maximum 
Error(% ) 
RMS 
Error(%) 
Maximum 
Error(%) 
RMS 
Error(%) 
Hot-leg 
LOCA 
(Case1) 
Core 
uncovery 
53.26 
12.1 
15.16 
10.86 
RV failure 
3.17 
0.57 
2.29 
1.02 
Cold-leg 
LOCA 
(case3) 
Core 
uncovery 
84.88 
18.3 
34.74 
21.37 
RV failure 
1.95 
0.35 
1.48 
0.56 
TABLE III.  
PREDICTION PERFORMANCE OF SVM MODEL (LPSI 
DELAY) 
Break 
position 
LPSI 
delay 
Training Data 
Test data 
Maximum 
Error(%) 
RMS 
Error(%) 
Maximum 
Error(%) 
RMS 
Error(%) 
Hot-leg 
LOCA 
(Case2) 
Core 
uncovery 
9.5 
1.62 
2.39 
1.69 
RV failure 
1.68 
0.6 
1.13 
0.74 
Cold-leg 
LOCA 
(case4) 
Core 
uncovery 
12.95 
1.66 
89.51 
1.88 
RV failure 
19.31 
2.85 
15.72 
2.78 
B. Results of SVM model 
Figures 3-10 show the predicted golden time using the 
SVM model. And these show the comparison of the SVR 
model and MAAP data. Figures 3 and 4 show the HPSI 
golden time prediction of the case 1 for the severe accident 
scenario of hot-leg LOCA. Figures 5 and 6 show the LPSI 
golden time prediction of the case 2 for the severe accident 
scenario of hot-leg LOCA. Figures 7 and 8 show the HPSI 
golden time prediction of the case 3 for the severe accident 
scenario of cold-leg LOCA. Figures 9 and 10 show the LPSI 
golden time prediction of the case 4 for the severe accident 
scenario of cold-leg LOCA. The results show that using the 
SVM model, it is possible to accurately predict the golden 
time. 
0
10
20
30
40
50
60
70
80
90
0
5000
10000
15000
20000
25000
30000
35000
 
 
Golden time(sec)
break size
 target
 predictedl
 
Figure 3.  Golden time prediction of case 1 (HPSI delay - core uncovery).  
0
20
40
60
80
100
120
140
160
180
200
7400
7500
7600
7700
7800
 
 
Golden time(sec)
break size
 target
 predicted
 
Figure 4.  Golden time prediction of case 1 (HPSI delay - RV failure).  
20
30
40
50
60
70
80
90
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
 
 
Golden time(sec)
Break size
 target
 predicted
 
Figure 5.  Golden time prediction  of case 2 (LPSI delay - core uncovery).  
60
80
100
120
140
160
180
200
220
240
260
280
300
7300
7400
7500
7600
7700
7800
 
 
Golden time(sec)
Break size
 target
 predicted
 
Figure 6.  Golden time prediction of case 2 (LPSI delay - RV failure).  
121
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

-10 0
10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160
0
10000
20000
30000
40000
 
 
Golden time(sec)
Break size
 target
 predicted
 
Figure 7.  Golden time prediction of case 3 (HPSI delay - core uncovery).  
140
160
180
200
220
240
260
280
9300
9400
9500
9600
9700
9800
9900
 
 
Golden time(sec)
Break size
 targetl
 predicted
 
Figure 8.  Golden time prediction of case 3 (HPSI delay - RV failure).  
10
20
30
40
50
60
70
80
90 100 110 120 130 140 150 160
0
2000
4000
6000
8000
10000
12000
 
 
Golden time(sec)
Break size
 target
 predicted
 
Figure 9.  Golden time prediction of case 4 (HPSI delay - core uncovery).  
140
160
180
200
220
240
260
280
9300
9400
9500
9600
9700
9800
9900
 
 
Golden time(sec)
break size
 target
 predicted
 
Figure 10.  Golden time prediction of case 4 (LPSI delay - RV failure).
 
V. 
CONCLUSION 
In this study, the golden time according to LOCA break 
sizes was analyzed by using the MAAP code when SIS was 
not operating normally. In addition, the golden time 
prediction model was developed in LOCA circumstances by 
using SVM model. In summary, the results of this study 
suggest that the SVM model can accurately predict the 
golden time. If the golden time of SIS for accident recovery 
is predicted, the core will not be exposed through appropriate 
action. Also, the RV failure will be prevented by the cooling 
water injection even if the reactor core is exposed. These 
various golden time data are thought to be very useful to 
quickly deal with the actual accident. Also, it will be possible 
to more efficiently manage accidents beyond design basis for 
accident recovery. 
ACKNOWLEDGMENT 
This work was supported by a Nuclear Research & 
Development Program of the National Research Foundation 
of Korea (NRF) grant funded by the Korean government 
(MSIP), 
(Grant 
Nos. 
2012M2B2B1055611, 
2014M2A8A4044966). 
 
REFERENCES 
[1] S. J. Han, H. G. Lim, and J. E. Yang, “Thermal Hydraulic 
Analysis Aggressive Secondary Cooldown in Small Break 
Loss of Coolant Accident with Total Loss of High Pressure 
Safety Injection,” Proc. of KNS Autumn Mtg, Seoul, Korea, 
Oct. 5-11, 2003. 
[2] K. H. Yoo, D. Y. Kim, G. P. Choi, J. H. Back, and M. G. 
Na, “Prediction of Golden Time for Recovering the Safety 
Injection System in Severe LOCA Circumstances” Proc. of 
KNS spring Mtg, Jeju, Korea, May 6-8, 2015 
[3] D. Y. Kim, J. H. Kim, K. H. Yoo, and M. G. Na, 
“Prediction of hydrogen concentration in containment 
during severe accidents using fuzzy neural network,” Nucl. 
Eng. Technol., vol. 47, no. 2, pp. 139-147, Mar. 2015. 
[4] S. H. Park, D. S. Kim, J. H. Kim, and M. G. Na, “Prediction 
of the reactor vessel water level using fuzzy neural 
networks in severe accident circumstances of NPPs,” Nucl. 
Eng. Technol., vol. 46, no. 3, pp. 373-380, Jun. 2014. 
[5] S. H. Park, J. H. Kim, K. H. Yoo, and M. G. Na, “Smart 
sensing of the RPV water level in NPP severe accidents 
using a GMDH algorithm,” IEEE Trans. Nucl. Sci., vol. 61, 
no. 2, pp. 931-938, Apr. 2014. 
122
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

[6] V. Kecman, Learning and Soft Computing. Cambridge, 
Massachusetts: MIT press, 2001. 
[7] V. Vapnik, The Nature of Statistical Learning Theory. New 
york: Springer, 1995. 
[8] M .G. Na, I. J. Hwang, and Y. J. Lee, “Inferential sensing 
and monitoring for feedwater flowrate in pressurized water 
reactor,” IEEE Trans. Nucl. Sci., vol. 53, no. 4, pp. 2335-
2342, Aug. 2006 
[9] D. P. Bertsekas, Constrained Optimization and Lagrange 
Multiplier Methods. New York: Academic Press, 1982. 
[10] D. S. Kim, S. H. Park, J. H. Kim , K. H. Yoo, and M. G. Na, 
“Determination of the Recovery Time of Unhealthy SISs in 
LOCA,” Proc. of KNS spring Mtg, Gwangju, Korea, May 
30-31, 2014. 
[11] M. G. Na, Y. R. Sim, K. H. Park, S. M. Lee, D. W. Jung, 
and S. H. Shin, “Sensor monitoring using a fuzzy neural 
network with an automatic structure constructor,” IEEE 
Trans. Nucl. Sci., vol. 50, no. 2, pp. 241-250, Apr. 2003.  
[12] D. 
E. 
Goldberg, 
Genetic 
Algorithms 
in 
Search, 
Optimization, and Machine Learning, Boston: Addison 
Wesley, 1989. 
[13] M. Mitchell, An Introduction to Genetic Algorithms. 
Cambridge, MA: MIT Press, 1996. 
 
 
123
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-437-4
INTELLI 2015 : he Fourth International Conference on Intelligent Systems and Applications

