Locating Multiple Camera Sensors and Wireless Access
Points for a Generalized Indoor Positioning System
Jaime Duque Domingo,
Carlos Cerrada
and J.A. Cerrada
Departamento de Ingenier´ıa de Software y
Sistemas Inform´aticos
ETSI Inform´atica, UNED,
C/Juan del Rosal, 16. 28040 Madrid, Spain
Email: ccerrada@issi.uned.es
Enrique Valero
School of Energy, Geoscience, Infrastructure and Society,
Heriot-Watt University,
Edinburgh EH14 4AS, United Kingdom
Email: e.valero@hw.ac.uk
Abstract—This work illustrates the generalization of a previously
developed Indoor Positioning System (IPS) based on the com-
bination of WiFi Positioning System (WPS) and depth maps.
This generalization extends the use of the proposed system
to scenarios containing multiple rooms and several people, in
contrast to the more simpler initial version. The combination
of both technologies improves the efﬁciency of existing methods,
based uniquely on wireless positioning techniques, for estimating
the location of people. Users just require the use of smart-phones,
besides the installation of RGB-D devices in the sensing area.
But some problems arise when multiple RGB-D sensors and
access points must be located in a large area composed of several
rooms. The paper exposes how the necessary devices are placed
minimizing the total uncovered area. Experimental results for an
ofﬁce space composed by nine differently sized rooms are shown.
Keywords–indoor positioning; IPS; WPS; RGB-D sensors;
Kinect; WiFi; ﬁngerprint; trajectory; skeletons; depth map.
I.
INTRODUCTION
Indoor Positioning Systems (IPSs) are used to obtain the
position of people or objects inside a building [1]. This work
extends a previously developed method for indoor positioning
inside a room [2], in which WiFi Positioning System (WPS)
and RGB-D sensors are combined. Object recognition can be
considered as a part of the core research area of computer
vision, and an important number of authors have reported
methods and applications for people detection and positioning.
The generalization of the previous IPS will deliver the position
of users in complete scenarios where there are several people
interacting with the environment. Trajectories of users will
be obtained by means of the two considered information
sources: WPS values and trajectories of the skeletons of users
in the depth map. While location of wireless sensors does
not represent any real problem, special care must be taken
in the location of the depth cameras in order to minimize
the total uncovered area. In fact, several Kinect v2 sensors
should be used simultaneously during experiments to capture
the skeletons of users evolving through different rooms. This
work shows how coordinates obtained by these sensors placed
in different rooms are transformed into a Universal Coordinates
System (UCS).
The paper is structured as follows: Section II explores
existing solutions concerning positioning, based on WPS,
RGB-D sensors, and using both technologies in a joint manner.
Section III is devoted to analyze the proposed system and
compare it to the previous one. Special attention is given to
describe how the multiple cameras should be located in a
large area as considered, and how their respective coordinate
systems should be related to the universal coordinate system.
Section IV presents the layout in which the experiments were
performed and analyzes the main obtained results. Finally,
Section V states the conclusions of the work.
II.
OVERVIEW OF RELATED WORK
WPS is founded on the ﬁngerprinting technique [3]. This
technique creates a map of the environment recording the
Received Signal Strength Indication (RSSI) in each point.
RSSI is a reference scale used to measure the power level of
signals received from a device on a wireless network (usually
WiFi or mobile telephony). This map is used to obtain the
position of a user in real-time, comparing the values received
from the user’s portable device to those stored in the map.
A recent comparison between WiFi ﬁngerprint-based indoor
positioning systems have been presented in [4]. Regarding
the use of advanced techniques, authors explain how to make
use of temporal or spatial signal patterns, user collaboration,
and motion sensors. Also, authors discuss recent advances on
reducing ofﬂine labor-intensive survey, adapting to ﬁngerprint
changes, calibrating heterogeneous devices for signal collec-
tion, and achieving energy efﬁciency for smartphones.
In the ﬁeld of people and objects detection, other technologies
based on computer vision (e.g., RGB-D sensors) have been
increasingly used. Authors, in [5], developed a method to
detect and identify several people that are occluded by others
in a scene. In [6], authors propose a smart-cane for the visually
impaired that, with the help of a Kinect sensor, facilitates
the location of objects. And the method Kinect Positioning
System (KPS) is analyzed in [7] aiming to obtain the user
position. These positioning techniques have also been used in
Robotics, such as the work presented in [8], where several
Simultaneous Localization and Mapping (SLAM) algorithms
are proposed for building maps with robots. By means of
51
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-572-2
ICWMC 2017 : The Thirteenth International Conference on Wireless and Mobile Communications

SLAM, the environment is built recording the measurements
RSSI in each point. Also, in this ﬁeld of research, the use of
different technologies improves positioning systems as shown
in [9], where authors analyze how to generate a ﬁngerprint
map with an RGB-D sensor mounted on a robot.
The work in [10] proposes a hybrid indoor positioning system
where WiFi and Global System for Mobile communication
(GSM) are combined for indoor positioning. Three positioning
algorithms from the Nearest Neighbor (NN) family are used for
simulations. An architecture for improving indoor positioning,
by means of the combination of WiFi and RFID, is presented
in [11]. WiFi is used for coordinating the RFID readers when
accessing the channel for retrieving tag identiﬁcations. This
avoids, in the presence of multiple readers, the so-called reader
collision problem that RFID suffers from. This problem is
caused by the inability for direct communication among them.
In [12], a robot is located by using three different systems:
a laser rangeﬁnder, a depth camera, and RSSI values. Each
system is independently used according to the zone where the
robot is located.
The present work extends a previously developed method
for indoor positioning inside a room [2], which considers a
scenario as represented in Figure 1. That work combined two
known technologies: WPS, widely used for indoor positioning,
and computer vision by means of RGB-D sensors. However,
experiments were carried out just in a room, where the system
was set up.
Figure 1. Scenario of the previous work
III.
ANALYSIS OF THE GENERALIZED SYSTEM
The generalization of the previous IPS will deliver users
identiﬁcation and position in more complex scenarios (i.e.
with more rooms) where several users are navigating. This
system can be set up in departments or companies where
monitoring the position of employees becomes useful to im-
prove the efﬁciency of business. The trajectories of users will
be calculated by combining the two mentioned information
sources: WPS data and trajectories of the skeletons of users
from the depth map. Skeletons are obtained by means of the
techniques presented in [13] and [14], where authors propose
new algorithms to quickly and accurately predict 3D positions
of body joints from depth images. Those methods form a core
component of the Kinect gaming platform.
As previously mentioned, the proposed system has been
conceived to work in a scenario composed of various rooms
(see Figure 2) where there are several people, each of them
using a smartphone. Two or more RGB-D sensors are situated
in each room to obtain the coordinates of users by means
of their skeletons. From these skeletons, neck coordinates are
extracted aiming to position people in the environment. This
part of the body is chosen because it is less prone to be
occluded by elements in the scenario.
Figure 2. Scenario of the generalized system
The coordinates of the obtained skeletons are considered
as illustrated in Figure 3. The distance from the sensor to the
skeletons is measured along the ZRGB−D axis, while YRGB−D
axis represents the heights of the users.
Figure 3. XRGB−D, YRGB−D, and ZRGB−D axis regarding the RGB-D
sensor direction
As one unique Universal Coordinates System (UCS) is
considered, the coordinates obtained by the RGB-D sensors are
transformed to that this reference system. Only 2D coordinates
are considered in this positioning system, as YRGB−D axis is
ignored and ZRGB−D axis corresponds to YP OS axis by means
of a transformation.
In order to transform the coordinates obtained by each
camera into the UCS, it is necessary to use the angle of the
camera and its distance to the universal center point P(0, 0).
52
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-572-2
ICWMC 2017 : The Thirteenth International Conference on Wireless and Mobile Communications

Each RGB-D sensor has a different angle respect to the UCS
and is situated at a different position.
In this work, Kinect sensors have been used as RGB-D sensors.
They have a limited angle range (see Figure 4). For this reason,
they are turned to obtain a 70◦ angle from the wall. This
implies that it is necessary to calculate the coordinates they
deliver into the UCS. These processes are described in the
following subsections.
The use of more than one RGB-D sensor reduces the
problem of uncovered areas. Since Kinect sensors obtain depth
maps with a limited angle of 70◦, the proposed system uses
two or more Kinect sensors to detect users in all positions. In
this manner, the parts which are not covered by a sensor are
recorded by another one. In some cases, it is necessary to use
more than two sensors because of the geometry of the scene.
In addition, the system can discern if two RGB-D sensors
are detecting the same user establishing a minimal distance
between them. If one Kinect detects a user and this is located
at 40cm of another user detected by another Kinect, then both
users are considered the same.
A. Position of RGB-D sensors in a room
RGB-D sensors are placed drawing a maximum angle from
the walls. As previously mentioned, in the case of Kinect v2,
the sensor is able to obtain a 70◦ angle. To obtain the maximum
coverage, the vision line must start at the opposite corner
respect the sensor. The values of XP OS and YP OS represent
the position of the sensor (see Figure 4). Kinect v2 is 25 cm
wide. L represents the length of the wall.
Figure 4. Position of the Kinect v2 sensor
Using the law of sines, it is possible to relate both triangles
and obtain the value of XP OS with respect to L. The ﬁrst
triangle, the one described between the sensor and the wall,
which is represented in light blue, is solved by (1).
25
sin 90◦ = XP OS
sin ε
(1)
where 90◦ + ε + δ = 180◦. Therefore, (1) is simpliﬁed in
(3).
25
sin 90◦ =
XP OS
sin(90◦ − δ)
(2)
XP OS = 25 · sin(90◦ − δ)
(3)
The second triangle is the one formed between the sensor
and the opposite corner, represented in light yellow (see Figure
4) and it is solved by (4).
L − XP OS
sin 55◦
= 12, 5
sin γ
(4)
where 55◦ + β + γ = 180◦ and β + δ = 180◦. Therefore,
γ = δ − 55◦ and (4) can be modiﬁed as (6).
L − XP OS
sin 55◦
=
12, 5
sin(δ − 55◦)
(5)
XP OS = L − 12, 5 · sin 55◦
sin(δ − 55◦)
(6)
According to (3), it is possible to extract the value of δ
(see (7)).
δ = 90◦ − arcsin
h x
25
i
(7)
Replacing δ in (6), XP OS is related to L (see (8)).
L = XP OS +
12, 5 · sin 55◦
sin(35◦ − arcsin
 XP OS
25

)
(8)
For example, if a Kinect v2 sensor is placed in a room with
430 cm wide (L = 430), XP OS will take the value 13, 79 using
(8). YP OS is obtained by Pythagoras’ Theorem as seen in (9).
X2
P OS + Y 2
P OS = 252
(9)
Therefore, YP OS = 20, 85. So, the Kinect sensor has to be
placed at 13, 79cm from the corner where the 70◦ angle starts
and at 20, 85cm from the adjacent wall.
B. Position of RGB-D sensors respect to the UCS
The middle point of the Kinect sensor, (CX, CY ), has to
be considered for the displacements in the UCS. This point is
calculated using again the law of sines (see (10)).
25
sin 90 = XP OS
sin ε = YP OS
sin δ
(10)
ε = arcsin
XP OS
25

(11)
δ = arcsin
YP OS
25

(12)
sin(δ) = CY
12, 5
(13)
53
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-572-2
ICWMC 2017 : The Thirteenth International Conference on Wireless and Mobile Communications

sin(ε) = CX
12, 5
(14)
Then, (15) is obtained and CX and CY are calculated. Con-
sidering x = 13, 79cm and y = 20, 85cm, then CX = 10, 43
and CY = 6, 90.

CX = 12, 5 · sin

TABLE I. TABLE OF PARAMETERS FOR KINECT PLACEMENT
Kinect
Corner
Corner
Sensor
Sensor
CX
CY
RGB-D
to UCS
to UCS
to corner
to corner
Camera
XCORNER
YCORNER
XP OS
YP OS
A
0,00
6,10
0,21
0,14
0,07
0,10
B
3,60
9,10
-0,22
-0,13
0,06
0,11
C
3,60
7,10
0,13
0,21
0,06
0,11
D
5,10
9,10
-0,13
-0,21
0,06
0,11
E
3,60
7,10
0,13
-0,21
0,07
0,11
F
6,10
7,10
-0,13
-0,21
0,07
0,11
G
6,10
5,70
0,13
0,21
0,07
0,11
H
8,30
9,10
-0,14
-0,21
0,07
0,10
I
1,70
6,10
-0,13
-0,21
0,06
0,11
J
0,00
3,90
0,13
0,21
0,06
0,11
K
1,70
4,70
0,14
0,21
0,07
0,10
L
5,00
5,70
-0,14
-0,21
0,07
0,10
M
5,00
4,70
0,14
0,21
0,07
0,10
N
8,30
5,70
-0,14
-0,21
0,07
0,10
O
0,00
3,90
0,21
-0,14
0,07
0,10
P
2,60
0,00
-0,21
0,14
0,07
0,10
Q
4,90
4,70
-0,13
-0,21
0,07
0,11
R
2,60
0,00
0,13
0,21
0,07
0,11
S
4,90
0,00
0,14
0,21
0,07
0,10
T
9,20
0,00
-0,14
0,21
0,07
0,10
TABLE II. TABLE OF PARAMETERS FOR KINECT PLACEMENT
Kinect RGB-D
L
λ
XUCS
YUCS
Camera
A
3,00
1,37◦
XCORNER + Y ′
RGB−D
YCORNER + X′
RGB−D
B
3,40
1,10◦
XCORNER − Y ′
RGB−D
YCORNER − X′
RGB−D
C
1,50
2,63◦
XCORNER + X′
RGB−D
YCORNER + Y ′
RGB−D
D
1,50
2,63◦
XCORNER − X′
RGB−D
YCORNER − Y ′
RGB−D
E
2,50
1,61◦
XCORNER + X′
RGB−D
YCORNER − Y ′
RGB−D
F
2,50
1,61◦
XCORNER − X′
RGB−D
YCORNER − Y ′
RGB−D
G
2,20
1,82◦
XCORNER + X′
RGB−D
YCORNER + Y ′
RGB−D
H
3,20
1,26◦
XCORNER − X′
RGB−D
YCORNER − Y ′
RGB−D
I
1,70
2,33◦
XCORNER − X′
RGB−D
YCORNER − Y ′
RGB−D
J
1,70
2,33◦
XCORNER + X′
RGB−D
YCORNER + Y ′
RGB−D
K
3,30
1,23◦
XCORNER + X′
RGB−D
YCORNER + Y ′
RGB−D
L
3,30
1,23◦
XCORNER − X′
RGB−D
YCORNER − Y ′
RGB−D
M
3,30
1,23◦
XCORNER + X′
RGB−D
YCORNER + Y ′
RGB−D
N
3,30
1,23◦
XCORNER − X′
RGB−D
YCORNER − Y ′
RGB−D
O
3,90
1,04◦
XCORNER + Y ′
RGB−D
YCORNER − X′
RGB−D
P
4,70
0,87◦
XCORNER − Y ′
RGB−D
YCORNER + X′
RGB−D
Q
2,30
1,74◦
XCORNER − X′
RGB−D
YCORNER − Y ′
RGB−D
R
2,30
1,74◦
XCORNER + X′
RGB−D
YCORNER + Y ′
RGB−D
S
4,30
0,94◦
XCORNER + X′
RGB−D
YCORNER + Y ′
RGB−D
T
4,30
0,94◦
XCORNER − X′
RGB−D
YCORNER + Y ′
RGB−D
 XE,UCS = 3, 60 + X′
YE,UCS = 7, 10 − Y ′E,RGB−D
E,RGB−D
(23)
where X′
E,RGB−D and Y ′
E,RGB−D are obtained by (24).



X′
E,RGB−D = 0, 07 + dE,RGB−D · cos θ
Y ′
E,RGB−D = 0, 11 + dE,RGB−D · sin θ
(24)
dE,RGB−D and θ are calculated with (25) and (26) respec-
tively.
dE,RGB−D =
q
Z2
E,RGB−D + X2
E,RGB−D
(25)
θ = 35 − 1, 61 − arcsin
XE,RGB−D
dE,RGB−D

(26)
Finally, (27) shows how to obtain XE,UCS and YE,UCS
coordinates from the Kinect coordinates XE,RGB−D and
YE,RGB−D.













XE,UCS = 3, 67 + dE,RGB−D
· cos

33, 39 − arcsin

XE,RGB−D
dE,RGB−D

YE,UCS = 6, 99 − dE,RGB−D
· sin

33, 39 − arcsin

XE,RGB−D
dE,RGB−D

(27)
The combination of WPS and RGB-D trajectories has been
performed using the Synchronized Euclidean distance [15], as
detailed in [2]. The developed experiments have shown an
important advance in terms of indoor positioning. The system
is able to locate more than 10 people with a success over 95%
(see Figure 6) in a scenario with multiple rooms.
Figure 6. Results for a different number of users
V.
CONCLUSIONS
This work presents an extended method for indoor position-
ing based on a previously developed algorithm, which worked
exclusively with one room. This new method allows obtaining
indoor positioning inside a complete ﬂoor or building. Twenty
RGB-D sensors have been used to obtain the depth maps
and, subsequently, the skeletons of users. The combination of
wireless networks with skeletons is a simple and economical
method to increase the performance of WPS in interiors.
The generalization of the previous IPS allows obtaining the
user positions in large areas where there are several users.
The trajectories of users can be obtained by combining the
two considered information sources: the WPS trajectories and
the trajectories of the skeletons of the users in the depth
map. Although the positioning of the wireless sensors in the
complete scenario does not represent a real problem, special
attention has been paid to the location of the depth cameras.
The purpose of this has been to avoid blind areas, minimizing
the total uncovered area. The work has also shown how
coordinates obtained from the depth cameras at different rooms
are transformed into a universal coordinates system (UCS). At
this point, the previous technique can be applied to obtain
55
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-572-2
ICWMC 2017 : The Thirteenth International Conference on Wireless and Mobile Communications

users positions and trajectories. The position of the sensors
is followed by [16], where authors consider the use of this
system to obtain the positions of the users in a museum, being
this an approach to enrich the indoor users experience.
CONFLICTS OF INTEREST
The authors declare that there is no conﬂict of interest
regarding the publication of this manuscript.
ACKNOWLEDGMENTS
This work has been developed with the help of the re-
search projects DPI2013-44776-R and DPI2016-77677-P of
MICINN. It also belongs to the activities carried out within
the framework of the research network CAM RoboCity2030
S2013/MIT-2748 of Comunidad de Madrid
REFERENCES
[1]
G. Deak, K. Curran, and J. Condell, “A survey of active and passive in-
door localisation systems,” Computer Communications, vol. 35, no. 16,
2012, pp. 1939–1954.
[2]
J. Duque Domingo, C. Cerrada, E. Valero, and J. Cerrada, “Indoor
positioning system using depth maps and wireless networks,” Journal
of Sensors, vol. 2016, 2016.
[3]
W. Liu, Y. Chen, Y. Xiong, L. Sun, and H. Zhu, “Optimization of
sampling cell size for ﬁngerprint positioning,” International Journal of
Distributed Sensor Networks, vol. 2014, 2014.
[4]
S. He and S.-H. G. Chan, “Wi-ﬁ ﬁngerprint-based indoor positioning:
Recent advances and comparisons,” IEEE Communications Surveys &
Tutorials, vol. 18, no. 1, 2016, pp. 466–490.
[5]
G. Ye, Y. Liu, Y. Deng, N. Hasler, X. Ji, Q. Dai, and C. Theobalt, “Free-
viewpoint video of human actors using multiple handheld kinects,”
Cybernetics, IEEE Transactions on, vol. 43, no. 5, 2013, pp. 1370–
1382.
[6]
H. Takizawa, S. Yamaguchi, M. Aoyagi, N. Ezaki, and S. Mizuno,
“Kinect cane: object recognition aids for the visually impaired,” in Hu-
man System Interaction (HSI), 2013 The 6th International Conference
on.
IEEE, 2013, pp. 473–478.
[7]
Y. Nakano, K. Izutsu, K. Tajitsu, K. Kai, and T. Tatsumi, “Kinect
positioning system (kps) and its potential applications,” in International
Conference on Indoor Positioning and Indoor Navigation, vol. 13, 2012,
p. 15th.
[8]
C. K. Schindhelm, “Evaluating slam approaches for microsoft kinect,”
in Proc. 2011 The Eighth International Conference on Wireless and
Mobile Communications (ICWMC 2012), Venice, 2012, pp. 402–407.
[9]
P. Mirowski, R. Palaniappan, and T. K. Ho, “Depth camera slam on
a low-cost wiﬁ mapping robot,” in Technologies for Practical Robot
Applications (TePRA), 2012 IEEE International Conference on. IEEE,
2012, pp. 1–6.
[10]
J. Machaj and P. Brida, “Impact of optimization algorithms on hybrid
indoor positioning based on gsm and wi-ﬁ signals,” Concurrency and
Computation: Practice and Experience, 2016.
[11]
A. Papapostolou and H. Chaouchi, “Integrating rﬁd and wlan for indoor
positioning and ip movement detection,” Wireless Networks, vol. 18,
no. 7, 2012, pp. 861–879.
[12]
J. Biswas and M. Veloso, “Multi-sensor mobile robot localization for
diverse environments,” in RoboCup 2013: Robot World Cup XVII.
Springer, 2014, pp. 468–479.
[13]
J. Shotton, T. Sharp, A. Kipman, A. Fitzgibbon, M. Finocchio, A. Blake,
M. Cook, and R. Moore, “Real-time human pose recognition in parts
from single depth images,” Communications of the ACM, vol. 56, no. 1,
2013, pp. 116–124.
[14]
A. Barmpoutis, “Tensor body: Real-time reconstruction of the human
body and avatar synthesis from rgb-d,” Cybernetics, IEEE Transactions
on, vol. 43, no. 5, 2013, pp. 1347–1356.
[15]
C. T. Lawson, S. Ravi, and J.-H. Hwang, “Compression and mining
of gps trace data: New techniques and applications,” Technical Report.
Region II University Transportation Research Center, Tech. Rep., 2011.
[16]
J. Duque-Domingo, P. J. Herrera, E. Valero, and C. Cerrada, “Deci-
phering egyptian hieroglyphs: Towards a new strategy for navigation in
museums,” Sensors, vol. 17, no. 3, 2017, p. 589.
56
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-572-2
ICWMC 2017 : The Thirteenth International Conference on Wireless and Mobile Communications

