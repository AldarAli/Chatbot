Method for Analytic Evaluation of the Weights of a Robust Large-Scale
Multilayer Neural Network with Many Hidden Nodes
Mikael Fridenfalk
Department of Game Design
Uppsala University
Visby, Sweden
mikael.fridenfalk@speldesign.uu.se
Abstract—The multilayer feedforward neural network is presently
one of the most popular computational methods in computer
science. The current method for the evaluation of its weights is
however performed by a relatively slow iterative method known
as backpropagation. According to previous research, attempts
to evaluate the weights analytically by the linear least square
method, showed to accelerate the evaluation process signiﬁcantly.
The evaluated networks showed however to fail in robustness
tests compared to well-trained networks by backpropagation, thus
resembling overtrained networks. This paper presents the design
and veriﬁcation of a new method, that solves the robustness issues
for a large-scale neural network with many hidden nodes, as an
upgrade to the previously suggested analytic method.
Keywords-analytic; FNN; large-scale; least square method;
neural network; robust; sigmoid
I.
INTRODUCTION
The artiﬁcial neural network constitutes one of the most
interesting and popular computational methods in computer
science. The most well-known category is the multilayer
Feedforward Neural Network (FNN), where the weights are
estimated by an iterative training method called backpropaga-
tion [7][9]. Although this iterative method is relatively fast
for small networks, it is rather slow for large ones, given
the computational power of modern computers [1][2]. To
accelerate the training speed of FNNs, many approaches have
been suggested based on the least square method [3]. Although
the presentation on the implementation, as well as of the data
on the robustness of these methods may be improved, the
application of the least square method as such seems to be
a promising path to investigate [8][10].
What we presume to be required for a new method to
replace backpropagation in such networks, is not only that it
is efﬁcient, but also that it is superior compared to existing
methods and is easy to understand and implement. The goal
of this paper is, therefore, to investigate the possibility to ﬁnd a
robust analytic solution (i.e., with good generalization abilities
compared with a well-trained network using backpropagation,
but without any iterations involved), for the weights of an
FNN, that is easily understood and that may be implemented
relatively effortlessly, using a mathematical application such
as Matlab [6].
In a previous work [4], an analytic solution was proposed
for the evaluation of the weights of a textbook FNN. This
solution was found to be signiﬁcantly much faster, and for
H
= N − 1 (where H denotes the number of hidden
nodes and N, the number of training points), more accurate
than solutions provided by backpropagation, but at the same
time signiﬁcantly less robust compared with a well-trained
x1
x2
hk
x3
1
vk1
vk2
vk3
vk4
Figure 1. An example with three input nodes (M = 3), hk = S(vku) =
S(vk1x1 + vk2x2 + vk3x3 + vk4), using a sigmoid activation function S.
x1
h1
y1
x2
h2
y2
x3
y3
y4
1
1
V
W
Figure 2. A vectorized model of a standard FNN with a single hidden layer,
in this example with M = 3 input nodes, H = 2 hidden nodes, K = 4
output nodes and the weight matrices V and W, using a sigmoid activation
function for the output of each hidden node. In this model, the biases for the
hidden layer and the output layer correspond to column M + 1 in V versus
column H + 1 in W.
network using backpropagation, why the analytic solution was
considered to lack robustness for direct use.
Further experiments showed, however, that even small
measures, such as an increase in the input range of the network
by doubling the size of the training set with the addition of
374
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

perturbation, led to signiﬁcant improvement of the robust-
ness of the evaluated network. As a ﬁrst systematic attempt
to address the issue of robustness, this paper presents the
derivation, implementation and veriﬁcation of a new method,
based on the expansion of the training set of an FNN, with
addition of perturbation, but in practice without any impact
on the execution speed of the original method. As a brief
overview, in Section II, a recap is made of the theory behind
the analytic method presented in [4], which is the foundation of
the theory presented in this paper. In Section III, a new method
(or upgrade) is derived for the improvement of the robustness
of the original method. In Section IV, the experimental setup
is brieﬂy described, and in Section V, the new method is
experimentally veriﬁed by comparison with the performance
of the original one.
II.
ANALYTIC SOLUTION
In [4], a textbook FNN is vectorized based on a sigmoid
activation function S(t) = 1/(1 + e−t). The weights V and
W of such system (often denoted as WIH versus WHO), may
be represented by Figures 1-2. In this representation, deﬁned
here as the normal form, the output of the network may be
expressed as:
y = Wh = W

S(Vu)
1
"
, u =

x
1
"
(1)
where x = [x1 x2 . . . xM]T denotes the input signals, y =
[y1 y2 . . . yK]T the output signals, and S, an element-wise
sigmoid function. In this paper, a winner-take-all classiﬁcation
model is used, where the ﬁnal output of the network is the
selection of the output node that has the highest value. Since
the sigmoid function is constantly increasing and identical for
each output node, it can be omitted from the output layer,
as max(y) results in the same node selection as max(S(y)).
Further on, presuming that the training set is highly fragmented
(the input-output relations in the training sets were in our
experiments established by a random number generator), the
number of hidden nodes is preferred to be set to H = N − 1.
Deﬁning a batch (training set), the input matrix U, may be
expressed as:
U =
2
66664
x11
x12
· · ·
x1N
x21
x22
· · ·
x2N
...
...
...
...
xM1
xM2
· · ·
xMN
1
1
· · ·
1
3
77775
(2)
where column vector i in U, corresponds to training point i,
column vector i in Y0 (target output value) and in Y (actual
output value). Further, deﬁning H of size N ⇥N, as the batch
values for the hidden layer, given a training set of input and
output values and M + = M + 1, the following relations hold:
U =

X
1T
"
: [M + ⇥ N]
(3)
H =

S(VU)
1T
"
: [N ⇥ N]
(4)
Y = WH : [K ⇥ N]
(5)
To evaluate the weights of this network analytically, we need to
evaluate the target values (points) of H0 for the hidden layer. In
this context, the initial assumption is that any point is feasible,
as long as it is unique for each training set. Therefore, in this
model, H0 is merely composed of random numbers. Thus,
the following evaluation scheme is suggested for the analytic
solution of the weights of such network:
VT = (UUT )−1UHT
0 : [M + ⇥ H]
(6)
WT = (HHT )−1HYT
0 : [N ⇥ K]
(7)
where a least square solution is used for the evaluation of each
network weight matrix. Such equation is nominally expressed
as:
Ax = b
(8)
with the least square solution [3]:
x = (AT A)−1AT b
(9)
Since the mathematical expressions for the analytic solution of
the weights of a neural network may be difﬁcult to follow, an
attempt has been made in Figure 3 to visualize the matrix
operations involved. While a nonlinear activation function
(such as the sigmoid function) is vital for the success of such
network, the inclusion of a bias is not essential. It is for
instance possible to omit the biases and to replace H0 with
an identity matrix I. Such a conﬁguration would instead yield
the following formula for the evaluation of V and H (where
UI can further be simpliﬁed as U):
VT = (UUT )−1UI : [M + ⇥ N]
(10)
H = S(VU) : [H ⇥ N]
(11)
III.
PROPOSAL
To start with, we expand the input training set U in (2),
by the addition of perturbation to the input signal, given the
deﬁnition ⇥ = UUT , with ✓M +M + = N in:
⇥ =
2
66664
✓11
✓12
. . .
✓1M
✓1M +
✓21
✓22
. . .
✓2M
✓2M +
...
...
...
...
...
✓M1
✓M2
. . .
✓MM
✓MM +
✓M +1
✓M +2
. . .
✓M +M
N
3
77775
(12)
Further, an extended matrix ˜U is introduced, where:
˜U =
⇥ ˜U1
˜U2
. . .
˜UN
⇤
(13)
with:
Uj =
2
66664
u1j + ∆
u1j − ∆
u1j
u1j
u2j
u2j
u2j + ∆
u2j − ∆
...
...
...
...
uMj
uMj
uMj
uMj
1
1
1
1
· · ·
u1j
u1j
· · ·
u2j
u2j
...
...
...
· · ·
uMj + ∆
uMj − ∆
· · ·
1
1
3
77775
(14)
375
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

h ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤
i
VT
=
0
@
h ⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
1 1 1 1 1 1
i
2
4
⇤ ⇤ 1
⇤ ⇤ 1
⇤ ⇤ 1
⇤ ⇤ 1
⇤ ⇤ 1
⇤ ⇤ 1
3
5
1
A
−1
h ⇤ ⇤ ⇤
⇤ ⇤ ⇤
⇤ ⇤ ⇤
i
(UUT )−1
2
4
h ⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
1 1 1 1 1 1
i
2
4
⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤
3
5
3
5
h ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤
i
UHT
0
2
4
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
1 1 1 1 1 1
3
5
H
=
2
6666666664
S
0
B
B
B
@
" ⇤ ⇤ ⇤
⇤ ⇤ ⇤
⇤ ⇤ ⇤
⇤ ⇤ ⇤
⇤ ⇤ ⇤
#
V
h ⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
1 1 1 1 1 1
i
U
1
C
C
C
A
[ 1 1 1 1 1 1 ]
1T
3
7777777775
2
4
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
3
5
WT
=
0
@
2
4
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
1 1 1 1 1 1
3
5
2
4
⇤ ⇤ ⇤ ⇤ ⇤ 1
⇤ ⇤ ⇤ ⇤ ⇤ 1
⇤ ⇤ ⇤ ⇤ ⇤ 1
⇤ ⇤ ⇤ ⇤ ⇤ 1
⇤ ⇤ ⇤ ⇤ ⇤ 1
⇤ ⇤ ⇤ ⇤ ⇤ 1
3
5
1
A
−1
(HHT )−1
2
666664
2
4
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
1 1 1 1 1 1
3
5
H
2
4
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
3
5
YT
0
3
777775
2
4
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤
3
5
HYT
0
 ⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
"
Y
=
 ⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
"
W
2
4
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
⇤ ⇤ ⇤ ⇤ ⇤ ⇤
1 1 1 1 1 1
3
5
H
Figure 3. A visual representation of the evaluation of weights V and W by the analytic method presented in the related work [4], and the actual output Y, in
this example as a function of six training points, N = 6, the training input and output sets U and Y0, with two inputs, M = 2, four outputs, K = 4, and ﬁve
hidden nodes, H = N − 1 = 5. In this ﬁgure, an asterisk denotes a ﬂoating-point number. To facilitate bias values, certain matrix elements are set to one.
and where ∆ is deﬁned as the amplitude of the perturbation.
Thus, for the right hand side of (8), ˜⇥ = ˜U ˜UT , or more
explicitly:
˜⇥ = 2M
2
66664
d1
✓12
. . .
✓1M
✓1M +
✓21
d2
. . .
✓2M
✓2M +
...
...
...
...
...
✓M1
✓M2
. . .
dM
✓MM +
✓M +1
✓M +2
. . .
✓M +M
N
3
77775
(15)
with di = ✓ii + ↵, where ↵ = N∆2/M, or:
˜⇥ = 2M [⇥ + diag(↵, ↵, . . . , ↵, 0)]
(16)
where diag(d1, d2, . . . , dM +), denotes a diagonal matrix of
size M + ⇥ M + (where M + = M + 1), with the diagonal
elements d1, d2, . . . , dM +. Similarly, for the left hand side
of (8),  and ⇤ are deﬁned as:
HT
0 =  =
2
664
 11
 12
. . .
 1H
 21
 22
. . .
 2H
...
...
...
...
 N1
 N2
. . .
 NH
3
775
(17)
⇤ = UHT
0 =
2
664
λ11
λ12
. . .
λ1H
λ21
λ22
. . .
λ2H
...
...
...
...
λM +1
λM +2
. . .
λM +H
3
775
(18)
and thereby,  and  j as:
 =
2
664
 1
 2
...
 N
3
775
(19)
 j =
2
664
 j1
 j2
. . .
 jH
 j1
 j2
. . .
 jH
...
...
...
...
 j1
 j2
. . .
 jH
3
775
(20)
with ˜⇤ = ˜U :
˜⇤ = 2M⇤
(21)
This transforms (8) into:
˜U ˜UT X = ˜U 
(22)
376
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

or:
˜⇥X = ˜⇤
(23)
Thus:
2M [⇥ + diag(↵, ↵, . . . , ↵, 0)] X = 2M⇤
(24)
Given the matrix equation:
AX = B
(25)
since, given a scalar c 2 R:
c · (AX) = (c · A)X = c · B
(26)
thereby:
[⇥ + diag(↵, ↵, . . . , ↵, 0)] X = ⇤
(27)
This yields thus, the ﬁnal expression:
⇥
UUT + diag(↵, ↵, . . . , ↵, 0)
⇤
X = UHT
0
(28)
Hence, the expansion of U into a perturbation matrix ˜U of size
M + ⇥ 2MN, and similarly of HT
0 into a matrix  of size
2MN⇥H, is according to (28), equivalent to the reinforcement
of the diagonal elements of the square matrix ⇥ = UUT ,
by the addition of a factor ↵ = N∆2/M to each diagonal
element, except for the last one, which as a consequence of
the use of bias in the network, is left intact.
IV.
EXPERIMENTAL SETUP
The experiments presented in this paper are based on a
minimal mathematical engine that was developed in C++,
with the capability to solve X in a linear matrix equation
system of the form AX = B, where A, B, and X denote
matrices of appropriate sizes, since it is computationally more
efﬁcient to solve a linear equation system directly, than by
matrix inversion. In this system, the column vectors of X are
evaluated using a single Gauss-Jordan elimination cycle [3],
where each column vector xi in X corresponds to the column
vector bi in B. Backpropagation was in these experiments,
for high execution speed (and a fair comparison with the new
methods), also implemented in C++, using the code presented
in [5] as a reference.
V.
RESULTS
The experimental results presented in this paper are shown
in Figures 4-9, measuring average success rate, and Table I,
measuring execution speed. Each experiment is based on ten
individual experiments (with different random seeds), using a
single CPU-core on a modern laptop computer. In Table I,
¯tbp denotes the execution time for backpropagation based
on 10000 iterations, which applies to all backpropagation
experiments presented in this paper. Similarly, ¯tnew denotes the
execution time for the original analytic method in [4], and ¯tnew+,
the execution time for the new method presented in this paper,
using diagonal reinforcement.
In these experiments, the input values to the FNN consisted
of the integers {0, 1, 2}, and the output values of a binary
number, {0, 1}. To avoid inconsistencies (or repetition) in any
training set, identical input values were replaced by unique
values. For the addition of noise, a random value (with uniform
distribution) in the range of ±∆, was added to each input
value. However, although according to our derivation of (28),
↵ = N∆2/M, ↵ had in practice to be retuned to 105·N∆2 for
good results in the experiments in Figures 4-6 (H = N − 1),
and to 104 · N∆2 in Figures 7-9 (few hidden nodes).
TABLE I. AVERAGE EXECUTION TIME
Figure
M
N
H
K
¯tbp
¯tnew
¯tnew+
4
10
25
24
10
92.5 ms
688 µs
668 µs
5
20
50
49
20
332 ms
4.84 ms
4.86 ms
6
40
100
99
40
1.27 s
37.0 ms
37.1 ms
7
10
25
10
10
43.3 ms
212 µs
202 µs
8
20
50
20
20
144 ms
1.33 ms
1.31 ms
9
40
100
40
40
535 ms
9.35 ms
9.38 ms
VI.
CONCLUSION
The upgrade proposed in this paper, showed to solve the
robustness issues of the analytic solution of the weights of
a large-scale FNN with H = N − 1 nodes, and in practice
without any impact on the execution speed of the solution.
Since according to [4], the original method was not considered
to be ready for direct use until the robustness issues had been
solved, this upgrade provides hereby a method that, given
access to a linear equation solver, while considerably faster, is
for large-scale networks with many hidden nodes, comparable
in robustness to a well-trained FNN by backpropagation.
REFERENCES
[1]
P. De Wilde, Neural Networks Models: An Analysis, Springer, 1996,
pp. 35-51.
[2]
R. P. W. Duin, “Learned from Neural Networks”, ASCI2000, Lommel,
Belgium, 2000, pp. 9-13.
[3]
C. H. Edwards and D. E. Penney, Elementary Linear Algebra, Prentice
Hall, 1988, pp. 220-227.
[4]
M. Fridenfalk, “The Development and Analysis of Analytic Method
as Alternative for Backpropagation in Large-Scale Multilayer Neural
Networks”, The Proceedings of ADVCOMP 2014, Rome, Italy, August
2014, in press.
[5]
M. T. Jones, AI Application Programming, 2nd ed., Charles River, 2005,
pp. 165-204.
[6]
Matlab, The MathWorks, Inc. <http://www.mathworks.com/> [retrieved:
August 23, 2014].
[7]
S. Russell and P. Norvig, Artiﬁcial Intelligence: A Modern Approach,
3nd ed., Prentice Hall, 2009, pp. 727-736.
[8]
B. Widrow and M. A. Lehr, “30 Years of Adaptive Neural Networks:
Perceptron, Madaline, and Backpropagation”, The Proceedings of IEEE,
vol. 78, no. 9, 1990, pp. 1415-1442.
[9]
B. J. Wythoff, “Backpropagation Neural Networks: A Tutorial”, Chemo-
metrics and Intelligent Laboratory Systems, vol. 18, no. 2, 1993,
pp. 115-155.
[10]
Y. Yam, “Accelerated Training Algorithm for Feedforward Neural
Networks Based on Least Squares Method”, Neural Processing Letters,
vol. 2, no. 4, 1995, pp. 20-25.
377
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

∆
Average Success Rate ¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100% •
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
⇥
⇥
⇥
⇥
⇥
⇥
Figure 4. Backpropagation (⇥), analytic method (◦), versus new method using
diagonal reinforcement (•), with M = 10 (input nodes), N = 25 (training
points), H = 24 (hidden nodes), and K = 10 (output nodes).
∆
¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100% •
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
⇥
⇥
⇥
⇥
⇥
⇥
Figure 5. M = 20, N = 50, H = 49, and K = 20.
∆
¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100% •
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
⇥
⇥
⇥
⇥
⇥
⇥
Figure 6. M = 40, N = 100, H = 99, and K = 40.
∆
¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100%
•
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
⇥
⇥
⇥
⇥
⇥
⇥
Figure 7. M = 10, N = 25, H = 10, and K = 10.
∆
¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100%
•
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
⇥
⇥
⇥
⇥
⇥
⇥
Figure 8. M = 20, N = 50, H = 20, and K = 20.
∆
¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100% •
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
⇥
⇥
⇥
⇥
⇥
⇥
Figure 9. M = 40, N = 100, H = 40, and K = 40.
378
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

