Towards 3D Data Environments using Multi-Touch Screens
Francisco Ortega, Naphtali Rishe
School of Computing and Information Sciences
Florida International University
Miami, FL. USA
Forte007@ﬁu.edu, NDR@acm.org
Armando Barreto, Malek Adjouadi
Electrical and Computer Engineering Department
Florida International University
Miami, FL. USA
BarretoA@ﬁu.edu, Adjouadi@ﬁu.edu
Abstract—The increase in availability of multi-touch devices
has motivated us to consider interaction approaches outside the
limitations associated with the use of a mouse. The problem
that we try to solve is how to interact in a 3D world using a
2D surface multi-touch display. Before showing our proposed
solution, we brieﬂy review previous work in related ﬁelds that
provided a framework for the development of our approach.
Finally, we propose a set of multi-touch gestures and outline
an experiment design for the evaluation of these forms of
interaction.
Keywords-Multi-Touch; 3D Interaction ;3D Navigation; Ges-
tures.
I. INTRODUCTION
This paper presents the initial development of our ap-
proach to work with 3D interaction worlds using a multi-
touch display. We introduce our emerging methods in the
context of important previous work, resulting in our pro-
posed translation and rotation gestures for multi-touch in-
teraction with 3D worlds. In this same process, we try to
answer two important questions and provide an evaluation
path to be implemented in the near future.
3D navigation and manipulation are not new problems in
Human-Computer Interaction (e.g., [1], [2]) as will be shown
in our brief review of previous work. However, with the
availability of multi-touch devices such as the iPad, iPhone
and desktop multi-touch monitors (e.g., 3M M2256PW 22”
Multi-Touch Monitor) new concepts have developed in order
to help the transition to a post-Windows-Icon-Menu-Pointer
(WIMP) era. This gives rise to important questions such as:
(1) What is the most appropriate mapping between the 2D
interface surface and the 3D world? and (2) Can previous
techniques used with other devices (e.g., joystick, keyboard
and mouse) be used in 3D navigation?
To begin answering these questions, we ﬁrst endeavor
to understand touch interactions and previous multi-touch
work. We believe that all those aspects create a foundation
that is necessary for the development of a sound post-WIMP
framework [3]. After the related work section, we cover our
proposed solution, discussion and future work.
II. BACKGROUND
A. Understanding Touch Interactions
A common option for multi-touch interaction is to use the
set of points corresponding to n touches in a direct manner.
However, to achieve a more natural interaction between the
screen and the user, studies like [4]–[8] provide a different
take on how touch information can be used. For example,
[4] studies ﬁnger orientation for oblique touches which gives
additional information without having extra sensors (e.g.,
left/right hand detection.) In another example, Benko and
Wilson [8] study dual ﬁnger interactions (e.g, dual ﬁnger
selection, dual ﬁnger slider, etc.) Additional work dealing
with contact shape and physics can be found in [6], [7]. In
a very comprehensive review of ﬁnger input properties for
multi-touch displays [5] provides suggestions that have been
used already in [4].
One aspect that is important to have in mind is whether
to keep rotations, translations and scaling separate [9] or
combined [10]. If the latter is chosen, the user’s ability to
perform the operations separately may become a problem
[9].
One very important point found in [11], [12] is that one-
hand techniques are better for integral tasks (e.g., rotation)
and two hands perform better with separable tasks. For our
particular work, one can think of using one hand to perform
common rotations and translations, and using two hands
when special rotations need to be performed, utilizing the
second hand to indicate a different point of reference.
B. Multi-Touch Techniques
We believe that all of the related work dealing with multi-
touch, regardless whether it was designed for manipulation
of objects or navigation of 3D graphical scenes, can con-
tribute to the set of unifying ideas that serves as the basis
for our approach.
Some of the work in 3D interactions has been speciﬁc for
multi-touch, which is our focus as well. In [10], Hancock
et al. provide algorithms for one, two and three-touches.
This allows the user to have direct simultaneous rotation
and translation. The values that are obtained from initial
touches T1, T2 and T3 and ﬁnal touches T ′
1, T ′
2 and T ′
3 are
118
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

∆yaw, ∆roll and ∆pitch, which are enough to perform the
rotation in all three axes and ∆x, ∆y, ∆z to perform the
translation. A key part of their study showed that users prefer
gestures that involve more simultaneous touches (except for
translations). Using gestures involving three touches was
always better for planar and spatial rotations [10].
A different approach is presented in RNT (Rotate ’N
Translate) [13], which allows planar objects to be rotated and
translated using opposing currents. This particular algorithm
is useful for planar objects and it has been used by 3D
interaction methods (e.g., [14]).
A problem that occurs when dealing with any type of
3D interaction in multi-touch displays, is the one of spatial
separability [9]. To address this problem, in [9], the authors
proposed different techniques that will allow the user to
perform the correct combination of transformations (e.g.,
scaling, translation + rotation, scaling + rotation + trans-
lation, etc.). The two methods that were most successful
were Magnitude Filtering and First Touch Gesture Matching.
Magnitude Filtering works similarly to snap-and-go [9].
This method has some differences from normal snapping
techniques because it does not snap to pre-selected values or
objects. In addition, the authors introduce a catch-up zone
allowing“continuous transition between the snap zone and
the unconstrained zone.” [9]. The latter method, First Touch
Gesture Matching, works by minimizing “the mean root
square difference between the actual motion and a motion
generated with a manipulation subset of each model” [9]. To
select the most appropriate model, each prospective model
creates two outputs, best-ﬁt error and magnitude of the
appropriate transformation. These outputs are given to an
algorithm that decides which models to apply.
III. PROPOSED SOLUTIONS
A. Set up
1) Camera: To test our work, we use OpenGL and
perform the visualization through a virtual camera developed
in [15] and described by [16], as shown in Figure 1. One can
see that the UP vector indicates which way is up, the EYE
(or ORIGIN) vector indicates the position of the camera
and the AT (or FORWARD) vector indicates the direction
in which the camera is pointing.
2) Multi-Touch: We are using Windows 7 multi-touch
technology [17] to test our proposed solutions using a 3M
M2256PW Multi-touch Display. Windows 7 provides two
ways of using the touches from the device. The ﬁrst one is
gesture-based, identifying simple pre-deﬁned gestures and
the second is the raw touch mode which provides the ability
to develop any type of interaction. We chose the latter be-
cause our end goal is to create custom user interactions and
for that we preferred to work at the lowest level possible that
is available to us. Each touch has a unique identiﬁcation (ID)
that is given at the moment of TOUCHDOWN, to be used
during the TOUCHMOVE, and to end when TOUCHUP has
Figure 1: Camera View [16]
been activated. The ID gives us a very nice way to keep track
of a trace, which is deﬁned as the path of the touch from
TOUCHDOWN to TOUCHUP.
3) Visual Display: As a test case, we have created a world
with 64 by 64 by 64 spheres, in a cubic arrangement, drawn
in perspective mode, where each sphere has a different color.
This allows the user to test the 3D navigation provided by
our gestures while having a visual feedback. It is important
to note that we colored the spheres in an ordered fashion
using the lowest RGB values in one corner and the highest
values in the opposite corner of the cube of spheres.
B. Gestures
We have decided to develop separate gestures for transla-
tion and rotation to understand what combinations are more
efﬁcient for the user. This means that when rotating, the
user will be rotating by a speciﬁc axis and translations will
be performed using two axes. We also decided to provide
simple gestures for our initial design to see the interaction
of the users. Once we have collected more data about the
interaction, we can create more complex gestures, if needed.
Before we discuss translation and rotation techniques, we
will clarify the notations used. Lowercase x, y and z refer
to 3D axes, Xscreen and Yscreen refer to 2D axes in the
screen (using OpenGL view mode,) and uppercase X,Y and
Z refer to a point in the x, y, z axes.
1) Translation: In order to translate the camera, we
decided to combine the x & y axes and leave z by itself. The
algorithm for the y axis is similar to Algorithm 1, replacing
the variable X with the variable Y. The algorithm for the z
axis is similar to the y axis with the exception that it uses
3 touches. In general, the user can perform simultaneous
translation by the x and y axes using one ﬁnger or translate
by the z axis using three ﬁngers. All the movements can be
executed with a single hand.
2) Rotations: To address rotations, we have to think of
rotation about x, y and z independently, given that is our
belief that separating the rotation will demand a lower cog-
nitive load from the user. This expectation is also supported
by [9], [11], [12]. In addition, all the rotations are designed
to use only one hand, which is preferable, as demonstrated
119
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

Algorithm 1 Translation over X
Require: TouchCount = 1
if Point.X < PrevPoint.X then
MoveRight(delta)
else if Point.X >= PrevPoint.X then
MoveRight(−delta)
end if
Figure 2: Rotation about the 3D x axis (see Figure 1)
in [9]. To keep the constraint of using only one hand, the
algorithm checks that the touches are within a cluster.
The gesture for rotation about x, as shown in Figure 2,
merits to be described in more detail because the other two
rotations about y and z use very similar algorithms to those
already described for the translations. The only difference is
that y and z rotations require two touches each. The gesture
for rotation about x begins with T1 and T2, which form an
angle α with the horizontal axis. The user’s ﬁnal state is
represented with T ′
1 and T ′
2, forming an α′ angle with the
horizontal. Then, the difference between α′ and α gives the
rotation angle to be applied, about x.
IV. DISCUSSION
A. Gestures
We believe that the set of gestures that we are proposing
based on the literature reviewed in the background section
and our own preliminary testing will give a starting point to
ﬁnd the most natural gestures to interact in a 3D world using
a multi-touch device. Even after ﬁnding the most natural
gestures for 3D navigation, one will have to compare with
other devices such as the ones found in Bowman et al. [18].
As we will outline in the next section, we suggest to make
the comparison with a 3D mouse, such as the one shown in
Figure 3.
The ﬁrst question asked in the introduction was: What
is the most appropriate mapping between the 2D interface
surface and the 3D world? We have proposed a simple
solution to the problem, through the set of gestures described
above. Deﬁning and implementing the most natural mapping
between this 2D multi-touch interface and the 3D world may
still require additional work, but the concepts advanced in
Figure 3: 3D Mouse
this paper may provide an interesting direction towards the
solution of this ongoing challenge.
The other question asked in the introduction was: Can
previous techniques used with other devices (e.g., Joystick,
keyboard and mouse) be used in 3D navigation? We propose
that the answer is yes. We can build upon existing work that
was developed for the mouse or other interfaces, adapting
it for use with multi-touch displays whenever possible. An
example of this is The Virtual Sphere [2]. We could take
The Virtual Sphere and create a similar device for use with
multiple ﬁngers to allow a pure 3D rotation and translation,
even emulating the 3D mouse shown in Figure 3. However,
those considerations would be outside the scope of this
paper.
In general, we ﬁnd that multi-touch displays can work
efﬁciently for achieving a more natural user 3D interaction
and 3D manipulation.
B. Proposed Evaluation Technique
The considerations presented above inform our current
process of planning the experimental protocol and data
analysis methods we will use for evaluating our approach. To
answer our research questions and test the proposed gestures,
we will recruit at least 30 subjects from the college student
population at our university. The reason for our choice of
target population is that we believe that all students will have
a good grasp of basic mouse interaction, which will facilitate
the completion of the experimental tasks by the subjects.
The actual experiment, after allowing the user to become
familiarized with the interface, will consist of a set of tasks
to test translation and rotation gestures (independently) using
our 3M 22” Multi Touch Monitor (Model M2256PW) and
the 3D mouse made by 3DConnexion, which is shown in
Figure 3. For each of the tasks, we will measure the time
of execution to complete the task, and the accuracy of the
movement. For the completion time, we will use an external
game controller to start and stop the time, and for the
accuracy of the movement, we will automatically record
the initial and ﬁnal positions. In addition to the automated
recording of performance data, we will ask the subjects to
complete a short usability questionnaire [19].
V. CONCLUSION AND FUTURE WORK
Recently, multi-touch displays have become more widely
available and more affordable. Accordingly, the search for
120
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

protocols that will simplify the use of these devices for inter-
action in 3D data environments has increased in importance.
In this paper we have outlined some of the most valuable
previous contributions to this area of research, highlighting
some of the key past developments that have emerged
in the 3D-interaction community. This review of pertinent
literature provides a context for the presentation of the core
elements of the solution we propose for the interaction in
3D environments through a multi-touch display.
Speciﬁcally, we proposed a set of multi-touch gestures
that can be used to command translations and rotations in 3
axes, within a 3D environment. Our proposed solution has
been implemented using a 3M M2256PW 22” Multi-Touch
Monitor as the interaction device. This paper explained the
proposed gestures and provided pseudo-code segments that
indicate how these gestures are to be captured using the
information provided by the device. In our deﬁnition of
the proposed multi-touch gesture set we have established
independent gestures for each type of translation and also
for each type of rotation. We decided to proceed in this
way so that we can study how users prefer to combine or
concatenate these elementary gestures.
The next step in the development of our approach is
to evaluate its efﬁciency in a comparative study involving
other 3D interaction mechanisms, such as a 3D mouse. The
ongoing process of planning the experiments for evaluation
takes into account the nature of the devices and general
principles of design of experiments, in an effort to minimize
the presence of confounding effects, such as subject fatigue,
etc. Our experiments may lead us to deﬁne alternative
gestures to allow more innovative means of interaction.
ACKNOWLEDGMENTS
This work was sponsored by NSF grants HRD-0833093,
and CNS-0959985. Mr. Francisco Ortega is the recipient of a
GAANN fellowship, from the US Department of Education,
at Florida International University.
REFERENCES
[1] G. Nielson and D. Olsen Jr, “Direct manipulation techniques
for 3D objects using 2D locator devices,” Proceedings of
the 1986 workshop on Interactive 3D graphics, pp. 175–182,
1987.
[2] M. Chen, S. Mountford, and A. Sellen, “A study in interactive
3-D rotation using 2-D control devices,” ACM SIGGRAPH
Computer Graphics, vol. 22, no. 4, p. 129, 1988.
[3] R. Jacob, A. Girouard, L. Hirshﬁeld, M. S. Horn, O. Shaer,
E. T. Solovey, and J. Zigelbaum, “Reality-based interaction:
a framework for post-WIMP interfaces,” Proceeding of the
twenty-sixth annual SIGCHI conference on Human factors in
computing systems (CHI ’08), pp. 201–210, 2008.
[4] F. Wang, X. Cao, X. Ren, and P. Irani, “Detecting and
leveraging ﬁnger orientation for interaction with direct-touch
surfaces,” Proceedings of the 22nd annual ACM symposium
on User interface software and technology, pp. 23–32, 2009.
[5] F. Wang and X. Ren, Empirical evaluation for ﬁnger input
properties in multi-touch interaction.
ACM, Apr. 2009.
[6] A. Wilson, S. Izadi, O. Hilliges, A. Garcia-Mendoza, and
D. Kirk, “Bringing physics to the surface,” Proceedings of
the 21st annual ACM symposium on User interface software
and technology, pp. 67–76, 2008.
[7] X. Cao, A. Wilson, R. Balakrishnan, K. Hinckley, and S. Hud-
son, “ShapeTouch: Leveraging contact shape on interactive
surfaces,” Horizontal Interactive Human Computer Systems,
2008. TABLETOP 2008. 3rd IEEE International Workshop
on, pp. 129–136, 2008.
[8] H. Benko, A. Wilson, and P. Baudisch, “Precise selection
techniques for multi-touch screens,” in In Proceedings of the
SIGCHI conference on Human Factors in computing systems
(CHI ’06), 2006, pp. 1263–1272.
[9] M. A. Nacenta, P. Baudisch, H. Benko, and A. Wilson, “Sepa-
rability of spatial manipulations in multi-touch interfaces,” in
GI ’09: Proceedings of Graphics Interface 2009.
Canadian
Information Processing Society, May 2009.
[10] M. Hancock, S. Carpendale, and A. Cockburn, “Shallow-
depth 3d interaction: design and evaluation of one-, two-
and three-touch techniques,” Proceedings of the SIGCHI
conference on Human Factors in computing systems, p. 1156,
2007.
[11] K. Kin, M. Agrawala, and T. DeRose, Determining the
beneﬁts of direct-touch, bimanual, and multiﬁnger input on
a multitouch workstation.
Canadian Information Processing
Society, May 2009.
[12] T. Moscovich and J. Hughes, “Indirect mappings of multi-
touch input using one and two hands,” Proceeding of the
twenty-sixth annual SIGCHI conference on Human factors
in computing systems (CHI ’08), pp. 1275–1284, 2008.
[13] R. Kruger, S. Carpendale, S. Scott, and A. Tang, “Fluid
integration of rotation and translation,” Proceedings of the
SIGCHI conference on Human Factors in computing systems,
pp. 601–610, 2005.
[14] J. Reisman, P. Davidson, and J. Han, “A screen-space for-
mulation for 2D and 3D direct manipulation,” Proceedings of
the 22nd annual ACM symposium on User interface software
and technology, pp. 69–78, 2009.
[15] R. S. Wright, N. Haemel, G. Sellers, and B. Lipchak,
OpenGL SuperBible, ser. Comprehensive Tutorial and Ref-
erence.
Addison-Wesley Professional, Jul. 2010.
[16] J. Han and J. Kim, 3D Graphics for Game Programming.
Chapman & Hall, Feb. 2011.
[17] Y. Kiriaty, L. Moroney, S. Goldshtein, and A. Fliess, Intro-
ducing Windows 7 for Developers.
Microsoft Pr, Sep. 2009.
[18] D. A Bowman, “3D user interfaces: theory and practice,” p.
478, Jan. 2005.
[19] J. Lazar, D. Jinjuan Heidi Feng, and D. Harry Hochheiser, Re-
search Methods in Human-Computer Interaction, Jan. 2010.
121
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

