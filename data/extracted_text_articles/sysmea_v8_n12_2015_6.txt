A Rare Event Method Applied to Signalling Cascades
Benoˆıt Barbot, Serge Haddad and Claudine Picaronny
LSV, ENS Cachan & CNRS & Inria,
61, avenue du Pr´esident Wilson Cachan, France
{barbot,haddad,picaronny}@lsv.ens-cachan.fr
Monika Heiner
Brandenburg University of Technology,
Walther-Pauer-Strasse 2, Cottbus, Germany
monika.heiner@b-tu.de
Abstract—Formal models have been shown useful for analysis
of regulatory systems. Here we focus on signalling cascades, a
recurrent pattern of biological regulatory systems. We choose
the formalism of stochastic Petri nets for this modelling and
we express the properties of interest by formulas of a temporal
logic. Such properties can be evaluated with either numeric or
simulation based methods. The former one suffers from the
combinatorial state space explosion problem, while the latter
suffers from time explosion due to rare event phenomena. In this
paper, we demonstrate the use of rare event techniques to tackle
the analysis of signalling cascades. We compare the effectiveness
of the COSMOS statistical model checker, which implements
importance sampling methods to speed up rare event simulations,
with the numerical model checker MARCIE on several properties.
More precisely, we study three properties that characterise the
ordering of events in the signalling cascade. We establish an
interesting dependency between quantitative parameters of the
regulatory system and its transient behaviour. Summarising, our
experiments establish that simulation is the only appropriate
method when parameters values increase and that importance
sampling is effective when dealing with rare events.
Keywords–rare event problem; importance sampling; regulatory
biological systems; stochastic Petri nets.
I.
INTRODUCTION
Signalling cascades. Signalling processes play a crucial
role for the regulatory behaviour of living cells. They mediate
input signals, i.e., the extracellular stimuli received at the cell
membrane, to the cell nucleus, where they enter as output
signals the gene regulatory system. Understanding signalling
processes is still a challenge in cell biology. To approach this
research area, biologists design and explore signalling net-
works, which are likely to be building blocks of the signalling
networks of living cells. Among them are the type of signalling
cascades which we investigate in our paper. In particular, we
complete the analysis performed in [1].
A signalling cascade is a set of reactions that can be
grouped into levels. At each level a particular enzyme is
produced (e.g., by phosphorylation); the level generally also
includes the inverse reactions (e.g., dephosphorylation). The
system constitutes a cascade since the enzyme produced at
some level is the catalyser for the reactions at the next level.
The catalyser of the ﬁrst level is usually considered to be
the input signal, while the catalyser produced by the last
level constitutes the output signal. The transient behaviour of
such a system presents a characteristic shape, the quantity of
every enzyme increases to some stationary value. In addition,
the increases are temporally ordered w.r.t. the levels in the
signalling cascade. This behaviour can be viewed as a signal
travelling along the levels, and there are many interesting
properties to be studied like the travelling time of the signal,
the relation between the variation of the enzymes of two
consecutive levels, etc.
In [2], it has been shown how such a system can be
modelled by a Petri net, which can either be equipped with
continuous transition ﬁring rates leading to a continuous Petri
net that determines a set of differential equations or by
stochastic transition ﬁring rates leading to a stochastic Petri
net. This approach emphasises the importance of Petri nets
that, depending on the chosen semantics, permit to investigate
particular properties of the system. In this paper, we wish to
explore the inﬂuence of stochastic features on the signalling
behaviour, and thus we focus on the use of stochastic Petri
nets.
Analysis of stochastic Petri nets can be performed either
numerically or statistically. The former approach is much faster
than the latter and provides exact results up to numerical
approximations, but its application is limited by the memory
requirements due to the combinatory explosion of the state
space.
Statistical evaluation of rare events. Statistical analysis
means to estimate the results by evaluating a sufﬁcient number
of simulations. However, standard simulation is unable to
efﬁciently handle rare events, i.e., properties whose probability
of satisfaction is tiny. Indeed, the number of trajectories to
be generated in order to get an accurate interval conﬁdence
for rare events becomes prohibitively huge. Thus, acceleration
techniques [3] have been designed to tackle this problem
whose principles consist in (1) favouring trajectories that
satisfy the property, and (2) numerically adjusting the result
to take into account the bias that has been introduced. This
can be done by splitting the most promising trajectories [4]
or importance sampling [5], i.e., modifying the distribution
during the simulation. In previous work [6], some of us have
developed an original importance sampling method based on
the design and numerical analysis of a reduced model in order
to get the importance coefﬁcients. First proposed for checking
“unbounded until” properties (e.g., a quantity of enzymes
remains below some threshold until a signal is produced) over
models whose semantics is a discrete time Markov chain, it has
been extended to also handle “bounded until” properties (e.g.,
a quantity of enzymes remains below some threshold until a
signal is produced within 10 time units) and continuous time
Markov chains [7].
Our contribution. In this paper, we complete the analysis of
the signalling cascade performed in [1] with a new family of
properties and we detail the algorithmic features of our impor-
tance sampling method. So, we consider here three families of
properties for signalling cascades that are particularly relevant
for the study of their behaviour and that are (depending on a
69
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

scaling parameter) potentially rare events. From an algorithmic
point of view, this case study raises interesting issues since the
combinatorial explosion of the model quickly forbids the use
of numerical solvers and its intricate (quantitative) behaviour
requires elaborated and different abstractions depending on the
property to be checked.
Due to these technical difﬁculties, the signalling cascade
analysis has led us to substantially improve our method and
in particular the way we obtain the ﬁnal conﬁdence interval.
From a biological point of view, experiments have pointed out
interesting dependencies between the scaling parameter of the
model and the probability of satisfying a property.
Organisation. In Section II, we present the biological back-
ground, the signalling cascade under study and the properties
to be studied. Then, in Section III, after some recalls on
stochastic Petri nets, we model signalling cascades by SPNs.
We introduce the rare event issue and the importance sampling
technique to cope with in Section IV. In Section V, we develop
our method for handling rare events. Then, in Section VI, we
report and discuss the results of our experiments. Finally, in
Section VII, we conclude and give some perspectives to our
work.
II.
SIGNALLING CASCADES
In technical terms, signalling cascades can be understood as
networks of biochemical reactions transforming input signals
into output signals. In this way, signalling processes determine
crucial decisions a cell has to make during its development,
such as cell division, differentiation, or death. Malfunction
of these networks may potentially lead to devastating con-
sequences on the organism, such as outbreak of diseases or
immunological abnormalities. Therefore, cell biology tries to
increase our understanding of how signalling cascades are
structured and how they operate. However, signalling networks
are generally hard to observe and often highly interconnected,
and thus signalling processes are not easy to follow. For this
reason, typical building blocks are designed instead, which are
able to reproduce observed input/output behaviours.
The case study we have chosen for our paper is such a
signalling building block: the mitogen-activated protein kinase
(MAPK) cascade [8]. This is the core of the ubiquitous
ERK/MAPK network that can, among others, convey cell
division and differentiation signals from the cell membrane to
the nucleus. The description starts at the RasGTP complex,
which acts as an enzyme (kinase) to phosphorylate Raf, which
phosphorylates MAPK/ERK Kinase (MEK), which in turn
phosphorylates Extracellular signal Regulated Kinase (ERK).
We consider RasGTP as the input signal and ERKPP (ac-
tivated ERK) as the output signal. This cascade (RasGTP
→ Raf → MEK → ERK) of protein interactions is known
to control cell differentiation, while the strength of the effect
depends on the ERK activity, i.e., concentration of ERKPP.
The scheme in Figure 1 describes the typical modular
structure for such a signalling cascade, see [9]. Each layer
corresponds to a distinct protein species. The protein Raf in
the ﬁrst layer is only singly phosphorylated. The proteins in
the two other layers, MEK and ERK, respectively, can be
singly as well as doubly phosphorylated. In each layer, for-
ward reactions are catalysed by kinases and reverse reactions
by phosphatases (Phosphatase1, Phosphatase2, Phosphatase3).
The kinases in the MEK and ERK layers are the phospho-
rylated forms of the proteins in the previous layer. Each
phosphorylation/dephosphorylation step applies mass action
kinetics according to the pattern A + E ⇌ AE → B + E.
This pattern reﬂects the mechanism by which enzymes act:
ﬁrst building a complex with the substrate, which modiﬁes
the substrate to allow for forming the product, and then
disassociating the complex to release the product; for details
see [10].
Figure 2 depicts the evolution of the mean number of
proteins with time. At time zero there is only a hundred
RasGTP proteins. Then we observe the transmission of the
signal witnessed by the decreasing of the number of RasGTP
proteins successively followed by the increasing of the number
of RafP, MEKPP and ERKPP proteins. In this ﬁgure, the
temporal order between the increasing of the different types
of proteins is clear. However, this ﬁgure only reports the mean
number of each protein for a large number of simulations
(300,000). It remains to check this correlation at the level of
a single (random) trajectory.
Having the wiring diagram of the signalling cascade, a
couple of interesting questions arise whose answers would
shed some additional light on the subject under investigation.
Among them are an assessment of the signal strength in each
level, and speciﬁcally of the output signal. We will consider
these properties in Sections VI-A and VI-B. The general
scheme of the signalling cascade also suggests a temporal order
of the signal propagation in accordance with the level order.
What cannot be derived from the structure is the extent to
which the signals are simultaneously produced; we will discuss
this property in Section VI-C.
III.
PETRI NET MODELLING
a) Stochastic Petri nets: Due to their graphical repre-
sentation and bipartite nature, Petri nets are highly appropri-
ate to model biochemical networks. When equipped with a
stochastic semantics, yielding stochastic Petri nets (SPN) [11],
they can be used to perform quantitative analysis.
Deﬁnition 1 (SPN): A stochastic Petri net N is deﬁned by:
•
a ﬁnite set of places P;
•
a ﬁnite set of transitions T;
•
a backward (resp. forward) incidence matrix Pre (resp.
Post) from P × T to N;
•
a set of state-dependent rates of transitions {µt}t∈T
such that µt is a mapping from NP to R>0.
A marking m of an SPN N is an item of NP . A transition t
is ﬁreable in marking m if for all p ∈ P m(p) ≥ Pre(p, t). Its
ﬁring leads to marking m′ deﬁned by: for all p ∈ P m′(p) =
m(p)−Pre(p, t)+Post(p, t). It is denoted either as m
t−→ m′
or as m
t−→ omitting the next marking. Let σ = σ1 . . . σn ∈
T ∗, then σ is ﬁreable from m and leads to m′ if there exists
a sequence of markings m = m0, m1, . . . , mn such that for
all 0 ≤ k < n, mk
σk
−→ mk+1. This ﬁring is also denoted
m
σ−→ m′. Let m0 be an initial marking, the reachability set
Reach(N, m0) is deﬁned by: Reach(N, m0) = {m | ∃σ ∈
T ∗ m0
σ−→ m}. The initialised SPNs (N, m0) that we consider
do not have deadlocks: for all m ∈ Reach(N, m0) there exists
t ∈ T such that m
t−→.
70
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Raf
RafP
MEKP
MEKPP
MEK
ERKP
ERKPP
ERK
Phosphatase3
Phosphatase1
Phosphatase2
RasGTP
Figure 1. The general scheme of the considered three-level signalling cascade; RasGTP serves as input signal and ERKPP as output signal.
An SPN is a high-level model whose operational semantics
is a continuous time Markov chain (CTMC). In a marking m,
each enabled transition of the Petri net randomly selects an
execution time according to a Poisson process with rate µt.
Then the transition with earliest ﬁring time is selected to ﬁre
yielding the new marking. This can be formalized as follows.
Deﬁnition 2 (CTMC of a SPN): Let N
be a stochastic
Petri net and m0 be an initial marking. Then the CTMC
associated with (N, m0) is deﬁned by:
•
the set of states is Reach(N, m0);
•
the transition matrix P is deﬁned by:
P(m, m′) =
P
m
t−→m′ µt(m)
P
m
t−→ µt(m)
•
the rate λm is deﬁned by: λm = P
m
t−→ µt(m)
b) Running case study:
We now explain how to
model our running case study in the Petri net framework.
The signalling cascade is made of several phosphoryla-
tion/dephosphorylation steps, which are built on mass/action
kinetics. Each step follows the pattern A+E ⇌ AE → B+E
and is modelled by a small Petri net component depicted
in Figure 3. The mass action kinetics is expressed by the
rate of the transitions. The marking-dependent rate of each
transition is equal to the product of the number of tokens in
all its incoming places up to a multiplicative constant given
by the biological behaviour (summing up dependencies on
temperature, pressure, volume, etc.).
The whole reaction network based on the general scheme
of a three-level double phosphorylation cascade, as given in
Figure 1, is modelled by the Petri net in Figure 4. The input
signal is the number of tokens in the place RasGTP, and the
output signal is the number of tokens in the place ERKPP.
This signalling cascade model represents a self-contained
and closed system. It is covered with place invariants (see
section VI), speciﬁcally each layer in the cascade forms a P-
invariant consisting of all states a protein can undergo; thus the
model is bounded. Assuming an appropriate initial marking,
the model is also live and reversible; see [2] for more details,
where this Petri net has been developed and analysed in the
qualitative, stochastic and continuous modelling paradigms. In
our paper, we extend these analysis techniques for handling
properties corresponding to rare events.
We introduce a scaling factor N to parameterize how many
tokens are spent to specify the initial marking. Increasing the
scaling parameter can be interpreted in two different ways:
either an increase of the biomass circulating in the closed
system (if the biomass value of one token is kept constant), or
an increase of the resolution (if the biomass value of one token
inversely decreases, called level concept in [2]). The kind of
interpretation does not inﬂuence the approach we pursue in
this paper.
Increasing N means to increase the size of the state space
and thus of the CTMC, as shown in Table I, which has been
computed with the symbolic analysis tool MARCIE [12]. As
expected, the explosion of the state space prevents numerical
model checking for higher N and thus calls for statistical
model checking.
Furthermore, increasing the number of states means to
actually decrease the probabilities to be in a certain state,
71
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

0.0001
0.001
0.01
0.1
1
10
100
0
10
20
30
40
50
60
70
80
90
100
Number of proteins
Time
RasGTP
RafP
MEKPP
ERKPP
Figure 2. Transmission of the signal in the signalling cascade.
N
RasGTP
3N
Phase1
4N
Raf
Raf RasGTP
RafP
RafP Phase1
k1
k2
k3
k4
k5
k6
2N
Phase2
4N
MEK
MEK RafP
MEKP
MEKP Phase2
MEKP RafP
MEKPP
MEKPP Phase2
k7
k8
k9
k16
k17
k18
k10
k11
k12
k13
k14
k15
3N
Phase3
3N
ERK
ERK MEKPP
ERKP
ERKP Phase3
ERKP MEKPP
ERKPP
ERKPP Phase3
k19
k20
k21
k28
k29
k30
k22
k23
k24
k25
k26
k27
Figure 4. A Petri net modelling the three-level signalling cascade given in Figure 1; ki are the kinetic constants for mass action kinetics, N the scaling
parameter.
as the total probability of 1 is ﬁxed. With the distribution of
the probability mass of 1 over an increasingly huge number
of states, we obtain sooner or later states with very tiny
probabilities, and thus rare events. Neglecting rare events is
usually appropriate when focusing on the averaged behaviour.
But they become crucial when certain jump processes such as
mutations under rarely occurring conditions are of interest.
IV.
STATISTICAL MODEL CHECKING WITH RARE EVENTS
A. Statistical model checking and rare events
c) Simulation recalls: The statistical approach for eval-
uating the expectation E(X) of a random variable X related to
72
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

r2
r1
r3
A
AE
B
E
Figure 3. Petri net pattern for mass action kinetics
A + E ⇌ AE → B + E.
Table I. Development of the state space for increasing N.
N
number of states
N
number of states
1
24,065 (4)
6
769,371,342,640 (11)
2
6,110,643 (6)
7
5,084,605,436,988 (12)
3
315,647,600 (8)
8
27,124,071,792,125 (13)
4
6,920,337,880 (9)
9
122,063,174,018,865 (14)
5
88,125,763,956 (10)
10
478,293,389,221,095 (14)
a random path in a Markov chain is generally based on three
parameters: the number of simulations K, the conﬁdence level
γ, and the width of the conﬁdence interval lg (see [13]). Once
the user provides two parameters, the procedure computes
the remaining one. Then it performs K simulations of the
Markov chain and outputs a conﬁdence interval [L, U] with
a width of at most lg such that E(X) belongs to this interval
with a probability of at least γ. More precisely, depending on
the hypotheses, the conﬁdence level has two interpretations:
(1) either the conﬁdence level is ensured, or (2) is only
asymptotically valid (when K goes to inﬁnity using central
limit theorem). The two usual hypotheses for providing an
exact conﬁdence level rather than an asymptotical one are:
(1) the distribution of X is known up to a parameter (e.g.,
Bernoulli law with unknown success probability), or (2) the
random variable is bounded allowing to exploit Chernoff-
Hoeffding bounds [14].
d) Statistical evaluation of a reachability probability:
Let C be a discrete time Markov chain (DTMC) with two
absorbing states s+ or s−, such that the probability to reach
s+ or s− from any state is equal to 1. Assume one wants to
estimate p, the probability to reach s+. Then the simulation
step consists in generating K paths of C, which end in an
absorbing state. Let K+ be the number of paths ending in state
s+. The random variable K+ follows a binomial distribution
with parameters p and K. Thus, the random variable K+
K has a
mean value p and since the distribution is parametrised by p, a
conﬁdence level can be ensured. Unfortunately, when p ≪ 1,
the number of paths required for a small conﬁdence interval
is too large to be simulated. This issue is known as the rare
event problem.
e) Importance sampling: In order to tackle the rare
event problem, the importance sampling method relies on a
choice of a biased distribution that will artiﬁcially increase the
frequency of the observed rare event during the simulation.
The choice of this distribution is crucial for the efﬁciency
of the method and usually cannot be found without a deep
understanding of the system to be studied. The generation of
paths is done according to a modiﬁed DTMC C′, with the same
state space, but modiﬁed transition matrix P′. P′ must satisfy:
P(s, s′) > 0 ⇒ P′(s, s′) > 0 ∨ s′ = s−
(1)
N
Abstraction
N •, f
Structural
Analysis
Λ
τ
Fox-Glynn
truncation
{cn}n+
n− n+, n−
Computation of
the embedded DTMC
C•
Λ
Numerical
evaluation
Simulation with
importance sampling
{µ•
n}n+
n−
Conﬁdence interval
generation
Figure 5. Principles of the methodology
which means that this modiﬁcation cannot remove transitions
that have not s− as target, but can add new transitions. The
method maintains a correction factor called L initialised to 1;
this factor represents the likelihood of the path. When a path
crosses a transition s → s′ with s′ ̸= s−, L is updated by
L ← L P(s,s′)
P′(s,s′). When a path reaches s−, L is set to zero.
If P′ = P (i.e., no modiﬁcation of the chain), the value of
L when the path reaches s+ (resp. s−) is 1 (resp. 0). Let Vs
(resp. Ws) be the random variable associated with the ﬁnal
value of L for a path starting in x in the original model C
(resp. in C′). By deﬁnition, the expectation E(Vs0) = p and
by construction of the likelihood, E(Ws0) = p. Of course, a
useful importance sampling should reduce the variance of Ws0
w.r.t. to the one of Vs0 equal to p(1 − p) ≈ p for a rare event.
V.
OUR METHODOLOGY FOR IMPORTANCE SAMPLING
A. Previous work
In [6], [7], we provided a method to compute a biased
distribution for importance sampling: we manually design an
abstract smaller model, with a behaviour close to the one of the
original model, that we call the reduced model and perform
numerical computations on this smaller model to obtain the
biased distribution. Furthermore, when the correspondence of
states between the original model and the reduced one satisﬁes
a good property called the variance reduction guarantee, Ws0
is a binary random variable (i.e., a rescaled Bernoulli variable)
thus allowing to get an exact conﬁdence interval with reduced
size. We applied this method in order to tackle the estimation
of time bounded property in CTMCs when it is a rare event,
that is the probability to satisfy a formula aU [0,τ]b: the state
property a is fulﬁlled until an instant in [0, τ] such that the
state property b is fulﬁlled. Let us outline the different steps
of the method that is depicted in Figure 5.
Abstraction of the model. As discussed above, given a SPN
N modelling the system to be studied, we manually design an
appropriate reduced one N • and a correspondence function f
from states of N to states of N •. Function f is deﬁned at the
net level (see Section VI).
Structural analysis. Importance sampling was originally pro-
posed for DTMCs. In order to apply it for CTMC C associated
73
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

with net N, we need to uniformize C (and also C• associated
with N •), which means ﬁnding a bound Λ for exit rate of
states, i.e., markings, considering Λ as the uniform exit rate
of states and rescaling accordingly the transition probability
matrices [15]. Since the rates of transitions depend on the
current marking, determining Λ requires a structural analysis
like invariant computations for bounding the number of tokens
in places.
Fox-Glynn truncation. Given a uniform chain with initial
state s0, exit rate Λ, and transition probability matrix P, the
state distribution πτ at time τ is obtained by the following
formula:
πτ(s) =
X
n≥0
e−Λτ(Λτ)n
n!
Pn(s0, s).
This value can be estimated, with sufﬁcient precision, by
applying [16]. Given two numerical accuracy requirements α
and β, truncation points n− and n+ and values {cn}n−≤n≤n+
are determined such that for all n− ≤ n ≤ n+:
cn(1 − α − β) ≤ e−Λτ(Λτ)n
n!
≤ cn
and
X
n<n−
e−Λτ(Λτ)n
n!
≤ α
X
n>n+
e−Λτ(Λτ)n
n!
≤ β
Computation of the embedded DTMC. Since N • has been
designed to be manageable, we build the embedded DTMC C•
Λ
of N • after uniformization. More precisely, since we want to
evaluate the probability to satisfy formula aU [0,τ]b, the states
satisfying a (resp. ¬a ∧ ¬b) are aggregated into an absorbing
accepting (resp. rejecting) state. Thus, the considered proba-
bility µτ(s•) is the probability to be in the accepting state at
time τ starting from state s•.
Numerical evaluation. Matrix P′ used for importance sam-
pling simulation in the embedded DTMC of N to evaluate
formulas aU [0,n]b for n− ≤ n ≤ n+, is based on the
distributions {µ•
n}0<n≤n+, where µ•
n(s•) is the probability
that a random path of the embedded DTMC of N • starting
from s• fulﬁlls aU [0,n]b. Such a distribution is computed by a
standard numerical evaluation. However, since n+ can be large,
depending on the memory requirements, this computation can
be done statically for all n or dynamically for a subset of such
n during the importance sampling simulation.
Simulation with importance sampling. This is done as for a
standard simulation except that the random distribution of the
successors of a state depend on both the embedded DTMC
CΛ and the values computed by the numerical evaluation.
Moreover, all formulas aU [0,n]b for n− ≤ n ≤ n+ have to
be evaluated increasing the time complexity of the method
w.r.t. the evaluation of an unbounded timed until formula.
Generation of the conﬁdence interval. The result of the
simulations is a family of conﬁdence intervals indexed by
n− ≤ n ≤ n+. Using the Fox-Glynn truncation, we weight
and combine the conﬁdence intervals in order to return the
ﬁnal interval.
Algorithmic considerations. The importance sampling sim-
ulation needs the family of vectors {µ•
n}0<n≤n+. They can
be computed iteratively one from the other with overall time
complexity Θ(mn+) where m is the number of states of
N •. More precisely, given P• the transition matrix of C•
Λ
(taking into account the transformation corresponding to the
two absorbing states with s•
+ the accepting one):
∀s• ̸= s•
+ µ•
0(s•) = 0, µ•
0(s•
+) = 1 and µ•
n = P• · µ•
n−1
Algorithm 1. One can perform this computation before start-
ing the importance sampling simulation. But for large values
of n+, the space complexity to store them becomes intractable.
However, looking more carefully at the importance sampling
speciﬁcation, it appears that at simulation time n one only
needs two vectors {µ•
n} and {µ•
n−1} [7]. So depending on the
memory requirements, we propose three alternative methods.
Algorithm 2. Let l(< n+) be an integer. In the precomputation
stage, the second method only stores the ⌊ n+
l ⌋ + 1 vectors µ•
n
with n multiple of l in list Ls and µ•
l⌊ n+
l ⌋+1, . . . , µ•
n+ in list
K (see the precomputation stage of the algorithm). During the
simulation stage, at time n, with n = ml, the vector µ•
n−1
is present neither in Ls nor in K. So, the method uses the
vector µ•
l(m−1) stored in Ls to compute iteratively all vectors
µ•
l(m−1)+i = P •i ·µ•
l(m−1) for i from 1 to l−1 and store them
in K (see the computation stage of the algorithm). Then it
proceeds to l consecutive steps of simulation without anymore
computations. We choose l close to
√
n+ in order to minimize
the space complexity of such a factorization of steps.
Algorithm 3. Let k = ⌊log2(n+)⌋ + 1. In the precomputation
stage, the third method only stores k + 1 vectors in Ls.
More precisely, initially using the binary decomposition of n+
(n+ = Pk
i=0 an+,i2i), the list Ls of k + 1 vectors consists of
wi,n = µ•Pk
j=i an,j2j, for all 1 ≤ i ≤ k+1 (see the precomputa-
tion step of the algorithm). During the simulation stage at time
n, with the binary decomposition of n (v = Pk
i=0 an,i2i), the
list Ls consists of wi,n = µ•Pk
j=i an,j2j, for all 1 ≤ i ≤ k + 1.
Observe that the ﬁrst vector w1,n is equal to µ•
n. We obtain
µ•
n−1 by updating Ls according to n − 1. Let us describe the
updating of the list performed by the stepcomputation of the
algorithm. Let i0 be the smallest index such that an,i0 = 1.
Then for i > i0, an−1,i = an,i, an−1,i0 = 0 and for i < i0,
an−1,i = 1. The new list Ls is then obtained as follows.
For i > i0 wi,n−1 = wi,n, wi0,n−1 = wi0−1,n. Then the
vectors for i0 < i, the vectors wi,n−1 are stored along iterated
2i0−1 −1 matrix-vector products starting from vector wi0,n−1:
w(j, v − 1) = P •2jw(j + 1, n − 1).
The computation at time n requires 1 + 2 + · · · + 2i0−1
products matrix-vector, i.e., Θ(m2i0). Noting that the bit i
is reset at most m2−i times, the complexity of the whole
computation is Pk
i=1 2k−iΘ(m2i) = Θ(mn+ log(n+)).
Algorithm 4. The fourth method consists in computing vector
µ•
v from the initial vector at each step. In this method, we only
need to store two copies of the vector.
74
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Algorithm 1
Precomputation(n+, µ•
0, P •) Result: Ls
// List Ls fulfills Ls(i) = µ•
i
Ls(0) ← µ•
0
for i = 1 to n+ do
Ls(i) ← P •Ls(i − 1)
Algorithm 2
Precomputation(n+, µ•
0, P •) Result: Ls, K
// List Ls fulfills Ls(i) = µ•
i·l
l ← ⌊√n+⌋
w ← µ•
0
for i from 1 to ⌊ n+
l ⌋l do
w ← P •w if i mod l = 0 then
Ls( i
l) ← w
// List K contains µ•
⌊ n+
l ⌋l+1, . . . , µ•
n+
for i from ⌊ n+
l ⌋l + 1 to n+ do
w ← P •w K(i mod l) ← w
Stepcomputation(n, l, P •, K, Ls)
// Updates K
when needed
if n mod l = 0 then
w ← Ls( n
l − 1)
for i from ( n
l − 1)l + 1 to n − 1 do
w ← P •
0 w K(i mod l) ← w
Algorithm 3
Precomputation(n+, µ•
0, P •) Result: Ls
// Ls fulfills Ls(i) = µ•Pk
j=i an+,j2j
k ← ⌊log2(n+)⌋ + 1
v ← µ•
0
Ls(k + 1) ← v
for i from k downto 0 do
if an+,i = 1 then
for j from 1 to 2i do
w ← P •w
Ls(i) ← w
Stepcomputation(n, l, P •, Ls)
// Ls is updated
accordingly to n − 1
i0 ← min(i | an,i = 1)
w ← Ls(i0 + 1)
Ls(i0) ← n
for i from i0 − 1 downto 0 do
for j = 1 to 2i do
w ← P •w
Ls(i) ← w
Algorithm 4
Stepcomputation(n, µ•
0, P •) Result: v
// Vector v equal to µ•
n
v ← µ•
0
for i = 1 to n do
v′ ← P •v v ← v′
B. Tackling signalling cascades
The reduced net that we design for signalling cascades
does not satisfy the variance reduction guarantee. This has
Table II. Compared complexities.
Complexity
Algorithm 1
Algorithm 2
Algorithm 3
Algorithm 4
Space
mn+
2m
√
n+
m log n+
2m
Time
for the
Θ(mn+)
Θ(mn+)
Θ(mn+)
0
precomputation
Additional time
for the
0
Θ(mn+)
Θ(mn+ log(n+))
Θ(m(n+)2)
simulation
two consequences: (1) we can perform a much more efﬁcient
importance sampling simulation and (2) we need to propose
different ways of computing “approximate” conﬁdence inter-
vals. We now detail these issues.
Importance sampling for multiple formulas. Using uni-
formisation, the computation of the probability to satisfy
aU τb in the CTMC, is performed by the computation of
the probability to satisfy aU [0,n]b for all n between n−
and n+ in the embedded DTMC. A naive implementation
would require to apply statistical model checking of formulas
aU [0,n]b for all n, but such a number can be large. A more
tricky alternative consists in producing all trajectories for time
horizon n = n+ with the corresponding importance sampling.
Simulation results are updated at the end of a trajectory for
all the intervals [0, n] with n− ≤ n ≤ n+ as follows. If the
trajectory has reached the absorbing rejecting state s− then it
is an unsuccessful trajectory for all intervals. Otherwise, if it
has reached the absorbing accepting state s+ at time n0, then
for all n ≥ n0 it is a successful trajectory and for all n < n0 it
is unsuccessful. Doing this way, every trajectory contributes to
all evaluations, and we signiﬁcantly increase the sample size
without increasing computational cost. With the same number
of simulations the accuracy of the result is greatly improved.
For example, the estimation of the ﬁrst property (with N = 5)
of the signalling cascade leads to n+ − n− = 759, inducing a
reduction of the simulation time by three orders of magnitude.
However, this requires that the importance sampling associated
with time interval [0, n+] is also appropriate for the other
intervals and in particular with time interval [0, n−]. It is not
true when the reachability probability in n− and n+ steps
differ by several orders of magnitude. In this case, the interval
[n−, n+] must be split into several intervals such that, the
reachability probability for each trajectory inside an interval
is of the same order of magnitude. Figure 6 illustrates this
idea by splitting the interval [n−, n+] in subintervals of width
l. For each subinterval, k trajectories are simulated. The naive
algorithm corresponds to l = 1. In the case of our experiments
(see Section VI) l = n+ −n− was sufﬁcient to obtain accurate
results.
Conﬁdence interval estimation. The result of each trajectory
of the simulation is a realisation of the random variable
Ws0
= Xs0Ls0 where the binary variable Xs0 indicates
whether a trajectory starting from s0 is succesful and the
positive random variable Ls0 is the (random) likelihood. Ob-
serve that E(Ws0) = E(Ls0|Xs0 = 1)E(Xs0). Since Xs0
follows a Bernoulli distribution, an exact conﬁdence interval
can be produced for E(Xs0). For E(Ls0|Xs0 = 1) several
approaches are possible among them we have selected three
possible computations ranked by conservation degree.
1)
The more classical way to compute conﬁdence inter-
75
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

n+
n−
0
n
. . .
(µn′(s0))n+
n′=n+−l
k
. . .
(µn′(s0))n+−l
n′=n+−2l
. . .
(µn′(s0))n+−2l
n′=n+−3l
. . .
. . .
(µn′(s0))n−+l
n′=n−
l
l
Figure 6. Parallel simulation estimating (µn(s0))n+
n=n− with reuse of
trajectories.
vals is to suppose that the distribution is Gaussian;
this is asymptotically valid if the variance is ﬁnite,
thanks to the central limit theorem.
2)
Another method is to use a pseudo Chernoff-
Hoeffding bound. Whenever the random variable is
bounded, this method is asymptotically valid. In our
case we will use the minimal and maximal values
observed during the simulation as the bounds of Ls0.
3)
The last method, which is more conservative than the
previous one, consists in returning the minimal and
maximal observed values as the conﬁdence interval.
VI.
EXPERIMENTS
We have analysed three properties, the last two are in-
spired by [2]. Recall that the initial marking of the model is
parametrized by a scaling factor N. For the ﬁrst two properties,
the reduced model is the same model but with local smaller
scaling factors on the different layers of phosphorylation.
Every state of the initial model is mapped (by f) to a state
of the abstract model which has the “closest” proportion of
chemical species. For instance, let N = 4, which corresponds
to 16 species of the ﬁrst layer, a state with 6 tokens in Raf and
10 tokens in RafP is mapped, for a reduced model with N = 3,
to a state with 4 = ⌊6×3/4⌋ tokens in Raf and 8 = ⌈10×3/4⌉
tokens in RafP (see the later on for a speciﬁcation of f).
All statistical experiments have been carried out with our
tool COSMOS [17]. COSMOS is a statistical model checker for
the HASL logic [18]. It takes as input a Petri net (or a high-
level Petri net) with general distributions for transitions. It
performs an efﬁcient statistical evaluation of the stochastic
Petri net by generating a code per model and formula. In the
case of importance sampling, it additionally takes as inputs
the reduced model and the mapping function speciﬁed by a C
function and returns the different conﬁdence intervals.
All experiments have been performed on a machine with
16 cores running at 2 GHz and 32 GB of memory both for the
statistical evaluation of COSMOS and the numerical evaluation
of MARCIE.
A. Maximal peak of the output signal
The ﬁrst property is expressed as a time-bounded reacha-
bility formula assessing the strength of the output signal of
the last layer: “What is the probability to reach within 10
time units a state where the total mass of ERK is doubly
phosphorylated?”, associated with probability p1 deﬁned by:
p1 = Pr(True U≤10(ERKPP = 3N))
Table III. Computational complexity related to the evaluation of p1.
N
COSMOS
MARCIE
Reduction factor
time
memory
time
memory
1
-
-
-
4
514MB
2
38
20,072
3,811MB
326
801MB
3
558
15,745
15,408MB
43,440
13,776MB
4
4667
40,241
3,593MB
Out of Memory: >32GB
5
27353
51,120
19,984MB
Table IV. Numerical values associated with p1.
N
COSMOS
MARCIE
Gaussian CI
Chernoff CI
MinMax CI
Output
1
2.07 E−12
2 [3.75E−27,5.88E−26] [3.75E−27,4.54E−25] [3.75E−27,1.57E−23] 8.18E−26
3 [4.34E−42,1.72E−39] [4.34E−42,1.82E−38] [4.43E−42,1.87E−37] 2.56E−39
4 [1.54E−57,8.54E−56] [1.54E−57,1.98E−55] [1.78E−57,7.05E−55]
-
5 [3.97E−73,2.33E−70] [3.97E−73,7.30E−70] [5.44E−73,2.24E−69]
-
The inner formula is parametrized by N, the scaling factor
of the net (via its initial marking). The reduced model that
we design for COSMOS uses different scaling factors for the
three layers in the signalling cascade. The ﬁrst two layers of
phosphorylation, which are based on Raf and MEK, always
use a scaling factor of 1, whereas the last layer involving ERK
uses a scaling factor of N. The second column of Table III
shows the ratio between the number of reachable states of the
original and the reduced models.
1) Experimental Results: We have performed experiments
with both COSMOS and MARCIE. The time and memory con-
sumptions for increasing values of N are reported in Table III.
For each value of N we generate one million trajectories with
COSMOS. We observe that the time consumption signiﬁcantly
increases between N = 3 and N = 4. This is due to a change
of strategy in the space/time trade-off in order to not exceed
the machine memory capacity. MARCIE suffers an exponential
increase w.r.t. both time and space resources. When N = 3,
it is slower than COSMOS and it is unable to handle the case
N = 4.
Table IV depicts the values returned by the two tools:
MARCIE returns a single value, whereas COSMOS returns three
conﬁdence intervals (discussed above) with a conﬁdence level
set to 0.99. We observe that conﬁdence intervals computed
by the Gaussian analysis neither contain the result, the ones
computed by Chernoff-Hoeffding do not contain it for N = 3,
and the most conservative ones always contain it (when this
result is available).
Figure 7 illustrates the dependency of p1 with respect to
the scaling factor N. It appears that the probability p1 depends
on N in an exponential way. The constants occurring in the
formula could be interpreted by biologists.
2) Mapping function: We describe here formally the re-
duction function f. The reduction function must map each
marking of the Petri net to a marking of the reduced Petri net.
First, we observe that the signalling cascades SPN contains
three places invariants of interest:
•
The
total
number
of
tokens
in
the
set
of
places
{Raf,Raf RasGTP,RafP Phase1,RafP,
MEK RafP, MEKP RafP} is equal to 4N.
•
The number of tokens in the set of places {MEK,
MEK RafP, MEKP Phase2, MEKP, MEKP RafP,
76
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

10−80
10−70
10−60
10−50
10−40
10−30
10−20
10−10
1
2
3
4
5
p1
N
y = 800(3 · 10−15)x
lower bound for p1
upper bound for p1
Figure 7. Highlighting an exponential dependency.
10−44
10−42
10−40
10−38
10−36
10−34
10−32
10−30
10−35
10−34
10−33
10−32
Distribution of trajectories impact
Distribution of trajectories
Figure 8. Distribution of trajectories and their contribution.
MEKPP Phase2,
MEKPP,
ERK MEKPP,
ERKP MEKPP} is equal to 2N.
•
The number of tokens in the set of places {ERK,
ERK RafP, ERKP Phase2, ERKP, ERKP RafP,
ERKPP Phase2, ERKPP} is equal to 3N.
We also introduce three subsets of places, one per layer of
phosphorylation.
•
S1 = {Raf,Raf RasGTP,RafP Phase1,RafP}
•
S2 = {MEK, MEK RafP, MEKP Phase2, MEKP,
MEKP RafP, MEKPP Phase2,MEKPP}
•
S3 = {ERK, ERK RafP, ERKP Phase2, ERKP,
ERKP RafP, ERKPP Phase2, ERKPP}
Let us remark that a marking of the SPN N is uniquely
determined by its values on places in S1, S2 and S3.
We deﬁne a function g such that: for all positive integer
m, positive real number p and vector of integers of size k,
v = (vi)k
1, g (p, m, v) is the vector of integers of size k, u =
(ui)k
1, deﬁned by: for all i > 1,
ui = min
 
⌈vi · p⌉ , m −
k
X
l=i+1
ul
!
and u1 = m −
k
X
l=2
ul
One can see that the g is properly deﬁned and that the sum of
the components of u are equal to m.
The reduction function f for the two properties is a
mapping from the set of states of SPN N to the set of states of
the reduced SPN N •. This function takes as input the marking
of a set of places that uniquely deﬁne the state. This set can
be decomposed on the three layers of phosphorylation, that is
S1 for the ﬁrst layer, S2 for the second layer and S3 for the
last layer.
Recall that layers are not independent one from the others
because proteins of one layer are used to activate the following
layer; this can be seen on the invariant that contains places of
the following layer. The mapping function that we construct
preserve these invariants.
Roughly speaking, on each layer Si, this function f applies
a function of the form g(pi, mi, −).
More precisely, given a scaling factor N and a scaling
factor for each of the three layers of the reduced model,
respectively N1, N2 and N3, the reduction function f maps
the marking m on the marking m• deﬁned as follow:
• (m•(p))p∈S3 = g

Table V. Numerical values associated with p2.
N
L
COSMOS
MARCIE
conﬁdence interval
time
result
time
memory
2
2
[2.39·10−13 , 1.07·10−9]
31
5.55·10−10
90
802 MB
2
3
[2.18·10−10 , 6.92·10−8]
110
6.64·10−8
136
816 MB
2
4
[9.33·10−8 , 3.54·10−5]
256
3.01·10−6
276
798 MB
2
5
[1.16·10−5 , 6.08·10−4]
1000
7.16·10−5
759
801 MB
2
6
[5.42·10−4 , 1.21·10−3]
5612
1.27·10−3
3180
804 MB
3
5
[1.82·10−12 , 9.78·10−9]
459
Time > 48 hours
3
6
[3.41·10−10 , 9.66·10−8]
1428
3
7
[1.81·10−8 , 2.23·10−6]
7067
3
8
[8.72·10−7 , 2.71·10−6]
4460
3
9
[1.42·10−6 , 4.59·10−5]
4301
3
10
[2.69·10−4 , 9.34·10−4]
6420
4
10
[5.12·10−9 , 2.75·10−8]
8423
Memory > 32GB
4
11
[8.23·10−8 , 2.97·10−7]
7157
4
12
[9.84·10−7 , 1.86·10−6]
18730
increase and reach 4N, when starting in a state where the
concentration is for the ﬁrst time at least L?”. This is a special
use case of the general pattern introduced in [2].
p2 = Prπ((RafP ≥ L) U (RafP ≥ 4N))
where π is the distribution over states when satisfying for the
ﬁrst time the state formula RafP ≥ L (previously called a
ﬁlter).
The presented method only deals with time bounded
reachability and not general “Until” formula. One way to
generaliqe it, is to build an automaton encoding the formula,
then the product of the automaton with the Markov chain and
ﬁnally compute the probability to reach an accepting state of
the automaton. However, this approach has two drawbacks:
ﬁrst, the size of the state space increases proportionally to
the number of states of the automaton. Second, most of the
simulation effort will be spent to reach a state satisfying ﬁrst
part of the formula, which is not a rare event. We use a more
efﬁcient approach: the system is simulated without importance
sampling until one reaches a state where the ﬁrst part of the
formula holds. Then, importance sampling is used to compute
the reachability probability of the second part of the formula.
This method is sound as it is equivalent to use an importance
sampling only on a part of the system.
This formula is parametrised by threshold L and scaling
factor N. The results for increasing N and L are reported
in Table V (conﬁdence intervals are computed by Chernoff-
Hoeffding method). As before, MARCIE cannot handle the case
N = 3, the bottleneck being here the execution time.
It is clear that p2 is an increasing function of L. More
precisely, experiments point out that p2 increases approxima-
tively exponentially by at least one magnitude order when L is
incremented. However, this dependency is less clear than the
one of the ﬁrst property.
The reduced model is the one used for the ﬁrst property
except for the values of the following parameters: here we
choose N1 = 1, N2 = N and N3 = 0.
C. Signal propagation
To demonstrate that the increases of the signals are tempo-
rally ordered w.r.t. the layers in the signalling cascade, and by
this way proving the travelling of the signals along the layers,
we explore the following property: “What is the probability
Table VI. Experiments associated with p3.
N
L
COSMOS
MARCIE
conﬁdence interval
time
result
time
memory
2
2
[0.8018,0.8024]
4112
0.8021
75
730MB
2
3
[0.4201,0.4209]
7979
0.4205
137
723MB
2
4
[0.1081,0.1086]
10467
0.1084
163
725MB
2
5
[0.0122,0.0124]
11122
0.0123
123
725MB
2
6
[6.20·10−4,6.61·10−4]
11185
6.32·10−4
129
725MB
2
7
[1.02·10−5,1.61·10−5]
11194
1.24·10−5
156
725MB
3
6
[0.0136,0.0138]
14648
0.0137
17420
10.3GB
3
7
[1.45·10−3,1.51·10−3]
14752
1.48·10−3
18155
10.3GB
3
8
[9.99·10−5,1.17·10−4]
14739
1.06·10−4
18433
10.3GB
3
9
[3.53·10−6,7.36·10−6]
14734
4.86·10−6
18353
10.3GB
3
10
[1.03·10−8,9.27·10−7]
14743
1.29·10−7
18355
10.3GB
3
11
[0 ,5.30·10−7]
14766
1.48·10−9
18047
10.3GB
4
8
[1.47·10−3,1.53·10−3]
17669
Out of Memory
4
9
[1.52·10−4,1.73·10−4]
17628
4
10
[9.99·10−6,1.59·10−5]
17656
4
11
[1.54·10−7,1.57·10−6]
17632
4
12
[0 ,5.30·10−7]
17664
5
8
[6.92·10−3,7.06·10−3]
20367
5
9
[1.13·10−3,1.19·10−3]
20421
5
10
[1.46·10−4,1.67·10−4]
20419
that, given the initial concentrations of RafP, MEKPP and
ERKPP being zero, the concentration of RafP rises above
some level L while the concentrations of MEKPP and ERKPP
remain at zero, i.e., RafP is the ﬁrst species to react?”. While
this property has its focus on the beginning of the signalling
cascade, it is obvious how to extend the investigation by further
properties covering the entire signalling cascade.
p3 = Pr((MEKPP = 0) ∧ (ERKPP = 0))U(RafP > L))
This formula is parametrized by L. Due to the lack of space
only some values of L in [0, 4N[ are reported. The results
for increasing N and L are given in Table VI. As can be
observed, the probability to satisfy this property is not a rare
event thus no importance sampling is required. Instead results
are obtained by a plain Monte Carlo simulation generating 10
millions of trajectories. For N > 3 MARCIE requires more
than 32GB of memory thus the computation was stopped. On
the other hand, the memory requirement of COSMOS is around
50MB for all experiments.
We also observed that as expected the probability exponen-
tially decreases with respect to L.
VII.
CONCLUSION AND FUTURE WORK
We have studied rare events in signalling cascades with the
help of an improved importance sampling method implemented
in COSMOS. As demonstrated by means of our scalable case
study, our method has been able to cope with huge models that
could not be handled neither by numerical computations nor by
standard simulations. In addition, analysis of the experiments
has pointed out some interesting dependencies between the
scaling parameter and the quantitative behaviour of the model.
In future work, we intend to incorporate other types of
quantitative properties, such as the mean time a signal needs
to exceed a certain threshold, the mean travelling time from the
input to the output signal, or the relation between the variation
of the enzymes of two consecutive levels. We also plan to
analyse other biological systems for which the evaluation of
78
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

tiny probabilities might be relevant like mutation rates in grow-
ing bacterial colonies [19]. This kind of properties requires to
specify new appropriate importance sampling methods.
REFERENCES
[1]
B. Barbot, S. Haddad, M. Heiner, and C. Picaronny, “Rare event
handling in signalling cascades,” in Proceedings of the 6th International
Conference on Advances in System Simulation (SIMUL’14), A. Arisha
and G. Bobashev, Eds.
Nice, France: XPS, Oct. 2014, pp. 126–131.
[2]
M. Heiner, D. Gilbert, and R. Donaldson, “Petri nets for systems and
synthetic biology,” in SFM 2008, ser. LNCS, M. Bernardo, P. Degano,
and G. Zavattaro, Eds., vol. 5016.
Springer, 2008, pp. 215–264.
[3]
G. Rubino and B. Tufﬁn, Rare Event Simulation using Monte Carlo
Methods.
Wiley, 2009.
[4]
P. L’Ecuyer, V. Demers, and B. Tufﬁn, “Rare events, splitting, and quasi-
Monte Carlo,” ACM Trans. Model. Comput. Simul., vol. 17, no. 2,
2007.
[5]
P. W. Glynn and D. L. Iglehart, “Importance sampling for stochastic
simulations,” Management Science, vol. 35, no. 11, 1989, pp. 1367–
1392.
[6]
B. Barbot, S. Haddad, and C. Picaronny, “Coupling and importance
sampling for statistical model checking,” in TACAS, ser. Lecture Notes
in Computer Science, C. Flanagan and B. K¨onig, Eds., vol. 7214.
Springer, 2012, pp. 331–346.
[7]
——, “Importance sampling for model checking of continuous time
Markov chains,” in Proceedings of the 4th International Conference on
Advances in System Simulation (SIMUL’12), P. Dini and P. Lorenz,
Eds.
Lisbon, Portugal: XPS, Nov. 2012, pp. 30–35.
[8]
A. Levchenko, J. Bruck, and P. Sternberg, “Scaffold proteins may bipha-
sically affect the levels of mitogen-activated protein kinase signaling
and reduce its threshold properties,” Proc Natl Acad Sci USA, vol. 97,
no. 11, 2000, pp. 5818–5823.
[9]
V. Chickarmane, B. N. Kholodenko, and H. M. Sauro, “Oscillatory
dynamics arising from competitive inhibition and multisite phosphory-
lation,” Journal of Theoretical Biology, vol. 244, no. 1, January 2007,
pp. 68–76.
[10]
R. Breitling, D. Gilbert, M. Heiner, and R. Orton, “A structured
approach for the engineering of biochemical network models, illustrated
for signalling pathways,” Brieﬁngs in Bioinformatics, vol. 9, no. 5,
September 2008, pp. 404–421.
[11]
M. Ajmone Marsan, G. Balbo, G. Conte, S. Donatelli, and G. Frances-
chinis, Modelling with generalized stochastic Petri nets.
John Wiley
& Sons, Inc., 1994.
[12]
M. Heiner, C. Rohr, and M. Schwarick, “ MARCIE - Model checking
And Reachability analysis done efﬁCIEntly ,” in Proc. PETRI NETS
2013, ser. LNCS, J. Colom and J. Desel, Eds., vol. 7927.
Springer,
2013, pp. 389–399.
[13]
L. J. Bain and M. Engelhardt, Introduction to Probability and Mathe-
matical Statistics, Second Edition.
Duxbury Classic Series, 1991.
[14]
W. Hoeffding, “Probability inequalities for sums of bounded random
variables,” Journal of the American Statistical Association, vol. 58, no.
301, 1963, pp. pp. 13–30.
[15]
A. Jensen, “Markoff chains as an aid in the study of markoff processes,”
Skand. Aktuarietidskr, 1953.
[16]
B. L. Fox and P. W. Glynn, “Computing Poisson probabilities,” Com-
mun. ACM, vol. 31, no. 4, 1988, pp. 440–445.
[17]
P. Ballarini, H. Djafri, M. Duﬂot, S. Haddad, and N. Pekergin, “HASL:
An expressive language for statistical veriﬁcation of stochastic models,”
in Proceedings of the 5th International Conference on Performance
Evaluation Methodologies and Tools (VALUETOOLS’11), Cachan,
France, May 2011, pp. 306–315.
[18]
P. Ballarini, B. Barbot, M. Duﬂot, S. Haddad, and N. Pekergin, “HASL:
A new approach for performance evaluation and model checking from
concepts to experimentation,” Performance Evaluation, 2015.
[19]
D. Gilbert, M. Heiner, F. Liu, and N. Saunders, “Colouring Space -
A Coloured Framework for Spatial Modelling in Systems Biology,” in
Proc. PETRI NETS 2013, ser. LNCS, J. Colom and J. Desel, Eds., vol.
7927.
Springer, June 2013, pp. 230–249.
79
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

