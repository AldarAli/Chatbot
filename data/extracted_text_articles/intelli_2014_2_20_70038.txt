Fuzzy Logic Control for Gaze-Guided Personal 
Assistance Robots  
 
Carl A. Nelson 
Department of Mechanical and Materials Engineering 
University of Nebraska-Lincoln 
Lincoln, NE, USA 
 
 
 
Abstract—As longer lifespans become the norm and modern 
healthcare allows individuals to live more functional lives despite 
physical disabilities, there is an increasing need for personal 
assistance robots.  One of the barriers to this shift in healthcare 
technology is the ability of the human operator to communicate 
his/her intent to the robot.  In this paper, a method of 
interpreting eye gaze data using fuzzy logic for robot control is 
presented.  Simulation results indicate that the fuzzy logic 
controller can successfully infer operator intent, modulate speed 
and direction accordingly, and avoid obstacles in a target 
following task relevant to personal assistance robots.  
Keywords—gaze tracking; fuzzy logic; autonomous robot; 
obstacle avoidance; personal assistance robot 
I. 
INTRODUCTION 
With lifespans increasing worldwide due to advancements 
in healthcare and related technologies, the importance of care 
for the elderly and disabled is increasing.  In particular, there is 
a shifting emphasis in technology development towards 
improving quality of life in the face of diminishing physical 
capabilities.  One of the burgeoning areas of this trend is 
personal assistance robotics.  In a typical scenario, a robot 
assistant may be present in the home to help with basic day-to-
day tasks (e.g., object retrieval), especially those tasks 
requiring navigation throughout the home, since age- or 
disability-related mobility limitations may keep an individual 
from performing all these tasks personally.  In extreme 
circumstances, it can even be challenging to give instructions 
to the robotic assistant, as in the case where the individual is 
not physically able to type, speak, or otherwise provide clear 
inputs to the human-robot interface.  Here, we present 
preliminary progress designing a robotic assistance system 
which relies on gaze tracking, including eye blinking patterns, 
to infer a person’s intent and thereby create instructions for the 
robot.  In this paper, we specifically focus on the intelligent 
inference of intent based on gaze and blinking input. 
This problem is an extension of the task of robotic target 
following and path planning.  Significant work has been done 
in this area of service robotics, where a robot is to follow a 
moving target.  For instance, some have used computer vision, 
using optical flow algorithms to track the target [1][2].  Other 
computer vision-based approaches have used Kalman filters for 
improving the accuracy of tracking [3].  Other tracking 
methods include the use of depth images with verification via a 
state vector machine [4], or following acoustic stimuli [5].  
Control approaches in these target-following scenarios include 
potential field mapping [6] and a variety of other techniques.  
Of particular interest are fuzzy logic controllers [5][7][8], 
which tend to be used primarily for steering.  Here, we will 
describe a fuzzy logic controller which not only determines the 
robot’s heading based on the location of the target, but also 
avoids obstacles and modulates speed based on the perception 
of intent from the combined gaze direction and blink frequency 
inputs.  This is conceptually based in part on recent work 
demonstrating how such a combined input using operator gaze 
could be used for automatic control of endoscope positioning 
in surgical tasks [9] using a commercially available eye 
tracking system, which is also similar to the work described in 
[10].  This approach extends beyond the most typical uses of 
eye gaze, which tend to be for two-dimensional human-
computer interfaces [11]. 
The remainder of this paper is organized as follows.  In 
section II, the eye gaze data and the fuzzy logic controller are 
described.  In section III, simulation results are presented.  
Section IV includes conclusions and recommendations for 
future work. 
II. 
METHODS 
A. Test Dataset and Simulation 
A gaze dataset was artificially generated to have 
spatiotemporal characteristics similar to those described in [9], 
in a planar workspace.  The data were arbitrarily assumed to be 
sampled at 10 Hz and included a logical blink data channel in 
addition to the x and y gaze target channels on the interval [-0.5 
0.5], providing a total of over 23 seconds of simulated robot 
tracking.  Due to the noisy nature of gaze data, the target X was 
determined by a linear weighted average of the previous n data 
points P, with n = 20: 
   
 
 ∑
(  
   
 )   
 
     
.   
 
(1) 
In this particular dataset, there are five intended target 
locations, characterized by dwelling gaze and higher blink 
frequency, and it is assumed that a supplementary action such 
as object placement or retrieval would follow target acquisition 
(although this supplementary action is beyond the scope of this 
preliminary study).  Within the workspace, three round 
obstacles were defined to test the ability of the simulated robot 
25
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

to avoid obstacles while seeking a target.  The data were 
imported into MATLAB (The MathWorks, Natick, MA) for 
simulation of gaze-based robotic target tracking. 
B. Fuzzy Logic Controller 
A Mamdani-type fuzzy logic controller [12] with five 
inputs and three outputs was created using the Fuzzy Logic 
Toolbox in MATLAB; the Mamdani-type model handles 
multi-input, multi-output problems better than the Sugeno-type 
alternative.  The inputs, shown in Table I, were intended to 
take into account the distance to the target, the degree of 
uncertainty of the target’s position, and the presence of 
obstacles in the path from the robot’s position to the target.  
The outputs, also shown in Table I, were used to control the 
speed and heading of the robot, including steering adjustments 
for obstacle avoidance.  All of the membership functions were 
triangular, as shown in Fig. 1. 
TABLE I.  
FUZZY CONTROLLER VARIABLES AND THEIR TRIANGLUAR 
MEMBERSHIP FUNCTIONS EXPRESSED IN MODAL FORM [LOWER BOUND, 
MODE, UPPER BOUND] 
Input/ 
Output 
Variables 
Name 
Units 
Membership Functions 
I 
Target x 
distance 
negative [-1, -0.5, 0] 
zero [-0.1, 0, 0.1] 
positive [0, 0.5, 1] 
I 
Target y 
distance 
negative [-1, -0.5, 0] 
zero [-0.1, 0, 0.1] 
positive [0, 0.5, 1] 
I 
Target variability 
distance 
zero [-0.1, 0, 0.1] 
low [0.05, 0.25, 0.45] 
high [0.35, 1, 1.4] 
I 
Blink frequency 
(normalized) 
- 
zero [-0.4, 0, 0.4] 
low [0.1, 0.5, 0.9] 
high [0.6, 1, 1.4] 
I 
Obstacle distance 
distance 
zero [-0.2, 0, 0.2] 
low [0, 0.3, 0.6] 
high [0.35, 1, 1.4] 
O 
Speed 
distance/ 
time 
zero [-0.4, 0, 0.4] 
low [0.1, 0.5, 0.9] 
high [0.6, 1, 1.4] 
O 
Heading 
rad 
up [0.125, 0.25, 0.375] 
up/right [0, 0.125, 0.25] 
right [-0.125, 0, 0.125] 
down/right [0.75, 0.875, 1] 
down [0.625, 0.75, 0.875] 
down/left [0.5, 0.625, 0.75] 
left [0.375, 0.5, 0.625] 
up/left [0.25, 0.375, 0.5] 
O 
Heading 
adjustment 
rad 
zero [-0.4, 0, 0.4] 
low [0.1, 0.5, 0.9] 
high [0.6, 1, 1.4] 
 
 
Fig. 1.  Membership functions for target variability (zero, low, and high). 
 
The target was determined using a weighted average of the 
gaze data as in (1), the target and obstacle distance variables 
were then calculated using the Pythagorean theorem, and target 
variability was represented by the standard deviation of the 
gaze input data over the averaging window.  Blink frequency 
was normalized to the interval [0 1] by assuming that four 
blink events within the 20-sample averaging window was high 
(achieving a value of 1), and lower blinking rates in the same 
window of time receive a proportionally smaller membership 
value.  If no obstacles were detected in the direct path between 
the robot and target, the obstacle distance was set to its 
maximum value of 1. 
Concerning the output variables, the maximum speed was 
constrained to a value of 0.25 (covering one-fourth the 
workspace in one second at maximum speed), and the 
maximum heading adjustment for obstacle avoidance was set 
at ±100°.  The heading variable was scaled to allow the robot 
to steer within the full 360° range. 
Fifteen rules were defined to characterize the influence of 
the five input variables on the three outputs.  In particular, four 
rules capture the influence of the inputs on the output variable 
speed, eight rules accommodate the division of heading into 
eight regions in polar coordinates, and the remaining three 
rules govern obstacle avoidance.  The rules defining the fuzzy 
logic controller are as follows: 
1. IF blink IS high THEN speed IS high 
2. IF target x IS positive OR  target x IS negative OR  
target y IS positive OR  target y IS negative THEN 
speed IS high 
3. IF target x IS zero AND  target y IS zero THEN 
speed IS zero 
4. IF target variability IS high OR  blink IS low THEN 
speed IS low 
5. IF target x IS positive AND target y IS zero THEN 
heading IS right 
6. IF target x IS positive AND target y IS positive 
THEN heading IS up/right 
7. IF target x IS positive AND target y IS negative 
THEN heading IS down/right 
8. IF target x IS negative AND target y IS zero THEN 
heading IS left 
9. IF target x IS negative AND target y IS positive 
THEN heading IS up/left 
10. IF target x IS negative AND target y IS negative 
THEN heading IS down/left 
11. IF target x IS zero AND target y IS positive THEN 
heading IS up 
12. IF target x IS zero AND target y IS negative THEN 
heading IS down 
13. IF obstacle distance IS zero THEN heading 
adjustment IS high 
26
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

14. IF obstacle distance IS low THEN heading adjustment 
IS low 
15. IF obstacle distance IS high THEN heading 
adjustment IS zero 
The first four rules govern the robot’s speed.  Higher blink 
rates imply a more focused operator intent and cause increased 
speed (rule 1).  Conversely, high gaze variability or low blink 
rate imply a less sure target and lead to lower speed (rule 4).  
The higher the distance to the target, the higher the necessary 
speed to reach it in a timely manner, and speed should drop to 
zero as the target is reached (rules 2-3).  It should be noted that 
lower speeds are sometimes desirable to conserve energy either 
when the goal is unclear or has been reached. 
Rules 5-12 pertain to heading.  These are relatively 
straightforward and use the four cardinal directions and the 
four semi-cardinal directions to navigate in the planar map 
based on the relative target distance in the x and y directions. 
The remaining three rules constitute the robot’s obstacle 
avoidance behavior.  The closer the obstacle, the larger the 
heading adjustment applied to go around it.  Whether this 
adjustment is added or subtracted from the heading variable is 
determined by whether the obstacle centroid is to the right or 
the left of the straight line along the robot’s heading. 
III. 
RESULTS 
Simulation in MATLAB revealed the ability of the fuzzy 
logic controller to simultaneously determine human intent from 
the combined gaze location and blink data, use this intent to 
modulate robot speed, follow a moving target, and avoid 
obstacles.  In Fig. 2, it can be observed that the robot (whose 
position is indicated by red diamond markers) can start at a 
location somewhat removed from the initial target, quickly 
acquire the target, and then follow it consistently without 
colliding with obstacles in the workspace. 
 
Fig. 2.  Target following behavior: robot (red diamond markers) follows target 
(blue circles) while avoiding fixed environmental obstacles.  Green circles 
indicate target location with a blink event.  Targets of definite interest (based 
on dwell duration and blink frequency) are at approximately (-0.3, 0.3), (0.1, 
0.3), (0.3, 0), (-0.1, -0.3), and (0, 0).  Raw gaze data are shown as black dots. 
 
Fig. 3.  Output speed as a function of blink membership function value: a 
positive correlation is noted. 
 
 
Fig. 4.  Output speed as a function of target distance: a positive correlation is 
noted. 
 
Fig. 5.  Output speed as a function of distance to current gaze location: 
correlation is much less pronounced. 
-0.5
0
0.5
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
robot start
target start
x
y
end
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
blink value
speed
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
target distance
speed
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
gaze point distance
speed
trend 
trend 
27
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

 
Fig. 6.  Modulation of heading adjustment based on obstacle distance 
demonstrates effective obstacle avoidance. 
The more interesting outcomes of the simulation are 
highlighted in Figs. 3-6.  In Fig. 3, one can see that robot speed 
tends to increase with blink frequency, as intended.  High 
speed at low blink can be attributed to the effects of target 
distance (particularly at the beginning of the simulation).  Note 
that the results in Fig. 3 are striated at discrete levels, since 
blinking is a discrete, logical event; this could be smoothed by 
applying an averaging method similar to that used in target 
determination.  Target distance also has an important effect on 
speed, as shown in Fig. 4.  In contrast, Fig. 5 illustrates that the 
relationship between robot speed and distance from the robot to 
the actual gaze point is less pronounced, since the target is 
based on a weighted average of the gaze point and is thus a less 
noisy signal.  The interdependence of speed on multiple input 
parameters is evident in Figs. 3 and 4.  The effectiveness of the 
obstacle avoidance behavior is shown in Fig. 6 by the clean 
heading adjustment curve. 
 
IV. 
CONCLUSIONS 
In this paper, a new technique for gaze-based guidance of 
personal assistance robots has been illustrated.  Fuzzy logic 
allows the robot to simultaneously manage multiple behaviors, 
practicing energy conservation when appropriate but pursuing 
the target when human intent to do so is clear.  Combined use 
of the eye gaze point and blinking data is a pivotal feature of 
the fuzzy logic controller.  Basic obstacle avoidance is 
demonstrated as an integrated behavior within this controller. 
The preliminary results presented in this paper suggest 
promise for additional future work.  The fuzzy controller 
should be tested using actual gaze data acquired from human 
users using an eye tracking system, with and without a real-
time robot presence, to determine how visual feedback between 
the robot and human may affect human gaze input.  The 
controller should also be tuned for improved performance, and 
some of its more basic rules may be replaced by a more 
sophisticated steering and obstacle avoidance rule set.  More 
advanced work will focus on detailed implementation for a 
broader variety of personal assistance tasks (e.g., object pick-
and-place, operating on a static object) in a true 3D 
environment. 
ACKNOWLEDGMENT 
Contributions by X. Zhang to the motivating concept of this 
paper, and support for this research from US NSF award 
1264504, are gratefully acknowledged. 
 
REFERENCES 
[1] 
J. Woodfill, R. Zabih, and O. Khatib, 1994, “Real-time motion vision 
for robot control in unstructured environments,” Proc. ASCE Robotics 
for Challenging Environments, pp. 10-18. 
[2] 
P. K. Allen, A. Timcenko, B. Yoshimi, and P. Michelman, 1993, 
“Automated tracking and grasping of a moving object with a robotic 
hand-eye system,” IEEE Transactions on Robotics and Automation, vol. 
9, no. 2, pp. 152-165. 
[3] 
T.-S. Jin, J.-M. Lee, and H. Hashimoto, 2006, “Position control of 
mobile robot for human-following in intelligent space with distributed 
sensors,” International Journal of Control, Automation, and Systems, 
vol. 4, no. 2, pp. 204-216. 
[4] 
J. Satake and J. Miura, 2009, “Robust stereo-based person detection and 
tracking for a person following robot,” Proc. IEEE International 
Conference on Robotics and Automation 2009, Workshop on People 
Detection and Tracking, Kobe, Japan, May 2009. 
[5] 
J. Han, S. Han, and J. Lee, 2013, “The tracking of a moving object by a 
mobile robot following the object’s sound,” J Intell Robot Syst, vol. 71, 
pp. 31–42. 
[6] 
C.-H. Chen, C. Cheng, D. Page, A. Koschan, and M. Abidi, 2006, “A 
moving object tracked by a mobile robot with real-time obstacles 
avoidance capacity,” Proc. of the 18th International Conference on 
Pattern Recognition (ICPR'06), 4 pp. 
[7] 
M. Mucientes and J. Casillas, 2005, “Learning fuzzy robot controllers to 
follow a mobile object,” International Conference on Machine 
Intelligence, Tozeur, Tunisia, Nov. 5-7, 2005, pp. 566-573. 
[8] 
M. Abdellatif, 2013, “Color-based object tracking and following for 
mobile service robots,” International Journal of Innovative Research in 
Science, Engineering and Technology, vol. 2, no. 11, pp. 5921-5928. 
[9] 
X. Zhang, S. Li, J. Zhang, and H. Williams, 2013, “Gaze Contingent 
Control for a Robotic Laparoscope Holder,” J. Med. Devices, vol. 7, no. 
2, pp. 020915.1-020915.2. 
[10] D. P. Noonan, G. P. Mylonas, A. Darzi, G.-Z. Yang, 2008, “Gaze 
Contingent Articulated Robot Control for Robot Assisted Minimally 
Invasive Surgery,” IEEE/RSJ International Conference on Intelligent 
Robots and Systems, Nice, France, Sept. 22-26, 2008, pp. 1186-1191. 
[11] A. Leonel, F. B. de Lima Neto, S. C. Oliveira, H. S. B. Filho, 2011, “An 
Intelligent Human-Machine Interface Based on Eye Tracking to Afford 
Written Communication of Locked-In Syndrome Patients,” Learning 
and Nonlinear Models, vol. 9, pp. 249-255. 
[12] P. Hájek, 1998, Metamathematics of Fuzzy Logic, Kluwer Academic 
Publishers, Dordrecht, The Netherlands. 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
obstacle distance
heading adjustment
trend 
28
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

