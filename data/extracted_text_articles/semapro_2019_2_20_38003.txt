Static and Dynamic Haptograms to Communicate Semantic Content 
Towards Enabling Face-to-Face Communication for People with Deafblindness 
Sándor Darányi, Nasrine Olson 
Swedish School of Library and Information Science 
University of Borås 
Borås, Sweden 
e-mail: {sandor.daranyi, nasrine.olson}@hb.se
Marina Riga, Efstratios Kontopoulos, Ioannis 
Kompatsiaris 
Information Technologies Institute (ITI/CERTH) 
Thessaloniki, Greece 
e-mail: {mriga, skontopo, ikom}@iti.gr
Abstract—Based on the ontology developed in the ongoing 
SUITCEYES EU-funded project to bridge visual analytics for 
situational awareness and navigation with semantic labelling of 
environmental cues, we designed a set of static and dynamic 
haptograms to represent concepts for two-way communication 
between deafblind and non-deafblind users. A haptogram cor-
responds to a tactile symbol drawn over a touchscreen, its dy-
namic nature referring to the act of writing or drawing, where 
the touchscreen can take several forms, including a smart tex-
tile screen designated for specific areas on the body. In its cur-
rent version, our haptogram set is generated over a 4 x 4 ma-
trix of cells and is displayed on the back of the user, tested for 
robustness at the receiving end. The concepts and concept se-
quences simulating simple questions and answers represented 
by haptograms are focused on ontology content for now but 
can be scaled up.  
Keywords-deafblind 
communication; 
conceptual 
hapto-
grams; word and sentence semantics; ontology. 
I.
INTRODUCTION
Communication with and between users with deafblind-
ness is constrained by the medical nature of this disability, 
ranging from congenital to acquired deafblindness, including 
worsening sight or worsening hearing or both over time, 
plus, ultimately, symptoms of ageing as well. This renders 
parties with and without this condition in a difficult position. 
Below we focus on the severest case, congenital deafblind-
ness, and propose a novel solution for improving the com-
munication between user and trainer, but with the hope in 
mind that it can be used in the future between two such users 
too. In this use case, a new model of mutual understanding 
between the partners must be developed practically from 
scratch. 
To this end we took inspiration from Lahtinen [1] and 
Lahtinen et al. [2], whose approach, while being expanded 
over the decades, basically reproduces ideograms on differ-
ent regions of the body by a combination of hand strokes, 
gestures, pressure, etc. Branded as the social-haptic mode of 
communication, by default this is a rich tactile language with 
its own syntax and vocabulary of so-called haptemes, built 
from phoneme-like haptices, and tailored to a range of situa-
tions and topics of high practical importance including envi-
ronmental descriptions, different situations, behaviour, the 
arts and advertisements to sum up a quick sampling. At the 
same time, due to its consensual nature, it is idiosyncratic 
and in need of being applicable in distance mode as well. 
This constraint makes it an ideal candidate for testing within 
the framework of the SUITCEYES EU-funded project [3], 
which is aimed at improving the quality of life for people 
with deafblindness through intelligent haptic technologies 
[4]. 
Another important parallel is McDaniel’s PhD thesis [5], 
where he describes a different approach. As in a situation of 
sensory overload, touch is a promising candidate for messag-
ing given that it is our largest sensory organ with impressive 
spatial and temporal acuity, there is need for a theory that 
addresses the design of touch-based building blocks for ex-
pandable, efficient, rich and robust touch languages that are 
easy to learn and use; moreover, beyond design, there is a 
lack of implementation and evaluation theories for such lan-
guages. To overcome these limitations, he proposed a uni-
fied, theoretical framework, inspired by natural, spoken lan-
guage, called Somatic ABC’s for Articulating (designing), 
Building (developing) and Confirming (evaluating) touch-
based languages. To evaluate the usefulness of Somatic 
ABC’s, its design, implementation and evaluation theories 
were applied to create communication languages for two 
very unique application areas: audio-described movies and 
motor learning. It was found that Somatic ABC’s aided the 
design, development and evaluation of rich somatic lan-
guages with distinct and natural communication units. 
Because the mission of SUITCEYES is to deploy a pro-
totype which is wearable, combines situational awareness, 
visual analytics and face-to-face communication by the same 
ontology, and works in distance mode by default, our below 
approach is conceptual. Instead of haptemes to reproduce 
phonemes by graphemes by a combination of consecutive 
dots, dashes and strokes as in [2], we propose to use hapto-
grams where the limited size and resolution of a body part as 
screen is counterbalanced by evolving patterns, i.e., the dy-
namics of signs. Our effort is in line with the approach by 
Israr and Poupyrev [6], building on their Tactile Brush ap-
proach, but focusing on language design by means of an on-
tology-compliant vocabulary vs. grammar, where the latter 
implements relational contextualization and sign sequencing. 
Thus, it belongs to the category of a priori defined spatial-
temporal patterns in the semiotic vein. 
The rest of the paper is structured as follows: Section 2 
starts with an account of related research approaches, fol-
16
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-738-2
SEMAPRO 2019 : The Thirteenth International Conference on Advances in Semantic Processing

lowed by Section 3 that provides a background on hapto-
grams. Section 4 then introduces the SUITCEYES ontology 
that will play the role of the unified model for semantic inte-
gration of information from the environment, while Section 5 
presents our approach for designing the haptogram vocabu-
lary. Finally, Section 6 concludes the paper and gives insight 
into our future work directions. 
II.
RELATED RESEARCH
The phonemic approach to a haptic language by Lahtinen 
et al. [2] finds support from the study by Chen et al. [7], 
where they investigated that decomposing spoken or written 
language into phonemes and transcribing each phoneme into 
a unique vibrotactile pattern enables people to receive lexical 
messages on the arm. A potential barrier to adopting this new 
communication system is the time and effort required to 
learn the association between phonemes and vibrotactile 
patterns. However, their study was limited to the learning of 
100 patterns by different methodologies, displayed on the 
arm, and the concepts were not connected to an ontology.  
On the other hand, Reed et al. [8] experimented with a 
new tactile speech device based on the presentation of pho-
nemic-based tactile codes. The device consisted of 24 tactors 
under independent control for stimulation at the elbow to 
wrist area. Using properties that included frequency and 
waveform of stimulation, amplitude, spatial location, and 
movement characteristics, unique tactile codes were de-
signed for 39 consonant and vowel phonemes of the English 
language. The participants, 10 young adults, were then 
trained to identify sets of consonants and vowels, before be-
ing tested on the full set of 39 tactile codes. 
Walker and Reed [9] investigated several haptic interfac-
es designed to reduce mistakes in Morse code reception of 
12 characters. Results concluded that a bimanual setup, dis-
criminating dots/dashes by left/right location, reduced the 
amount of errors to only 56.6% of the errors compared to a 
unimanual setup that used temporal discrimination to distin-
guish dots and dashes. 
Very much in line with what we would like to achieve, 
Israr and Poupyrev in [6] proposed Tactile Brush, an algo-
rithm that produces smooth, two-dimensional tactile moving 
strokes with varying frequency, intensity, velocity and direc-
tion of motion. The design of the algorithm was derived from 
the results of psychophysical investigations of two tactile 
illusions, apparent tactile motion and phantom sensations. 
Combined together they allowed for the design of high-
density two-dimensional tactile displays using sparse vi-
brotactile arrays. In a series of experiments and evaluations, 
they demonstrated that Tactile Brush is robust and can relia-
bly generate a wide variety of moving tactile sensations for a 
broad range of applications.
III.
BACKGROUND ON HAPTOGRAMS
Haptograms as a concept were introduced by Korres and 
Eid [10]. In their approach, “Haptogram” is a system de-
signed to provide point-cloud tactile display via acoustic 
radiation pressure. A tiled 2-D array of ultrasound transduc-
ers is used to produce a focal point that is animated to pro-
duce arbitrary 2-D and 3-D tactile shapes. The switching 
speed is very high, so that humans feel the distributed points 
simultaneously. The Haptogram system comprises a software 
component and a hardware component; the software compo-
nent enables users to author and/or select a tactile object, 
create a point-cloud representation, and generate a sequence 
of focal points to drive the hardware.  
Our haptograms, on the other hand, are conceptual, and 
correspond to ideograms and logograms in the tactile do-
main, using evolving dot patterns instead of tactile shapes. 
Further, we distinguish between stable vs. changing patterns 
and call them static vs. dynamic haptograms in a communi-
cation context. Their purpose in our framework is to imple-
ment an ontology-constrained messaging language to convey 
visual analytics results, situation awareness assessments, and 
everyday conversation raw material outside of the scope of 
the above two areas. As these haptograms are to be mapped 
to the back of a vest made of smart textile, i.e., use that body 
area to display semantic content, the resolution of this screen 
places a limit on the conceptual vocabulary. Since screen 
resolution goes back to the number of actuators in a rectan-
gular grid, as a proof of concept, in the current arrangement 
we designed a test vocabulary of both static and dynamic dot 
patterns conveyed to the body by vibration, pressure, heat, 
stimulated position, their combinations, and combination 
sequences to map  short messages from an external sender. 
This approach can be scaled up either by increasing actuator 
density, or by generating virtual actuators [6]. 
IV.
THE SUITCEYES ONTOLOGY
The key aim of the SUITCEYES ontology is to semanti-
cally integrate information coming from the environment 
(via sensors), and from the system’s analysis components 
(e.g., visual analysis of camera feed). In this sense, the on-
tology is primarily focused on semantically representing 
aspects relevant to the users’ context, in order to provide 
them with enhanced situational awareness and augment their 
navigation and communication capabilities. More important-
ly, the proposed ontology also serves as the bridge between 
environmental cues and content communicated to the user 
via the haptograms described in the next section. 
In ontology engineering, it is common practice to reuse 
existing third-party models and vocabularies during the de-
velopment of a custom ontology. We also followed this ap-
proach, in order to rely on previously used and validated 
ontologies. We, thus, adopted the semantic representation of 
objects and activities from the Dem@Care ontology [11], 
[12], which contains a set of descriptions of every-day activi-
ties and common objects used in an every-day context that 
are highly relevant to our goals. Moreover, we are relying on 
SOSA/SSN [13] for representing sensors and the respective 
observations, and on the Friend-Of-A-Friend (FOAF) speci-
fication [14] for representing persons and social associations. 
Finally, we integrated the SEAS (Smart Energy Aware Sys-
tems) Building Ontology [15], which is a schema for de-
scribing the core topological concepts of a building, such as 
buildings, building spaces and rooms. 
17
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-738-2
SEMAPRO 2019 : The Thirteenth International Conference on Advances in Semantic Processing

A.
Ontology Conceptualisation 
Figure 1 displays an overview of the core ontology clas-
ses based on the Grafoo ontology visualization notation [16]: 
the yellow rectangles represent classes, while the green ones 
represent data properties (i.e., properties that take a raw data 
value, like, e.g., integers and strings). The prefixes in front of 
some of the class names indicate the namespace of the re-
spective third-party ontologies, as mentioned above. Classes 
and properties that have no prefix belong to the core 
SUITCEYES ontology.  
Figure 1. Overview of the core classes of the SUITCEYES ontology. 
As indicated in the figure, class Detection is fundamen-
tal and refers to environmental cues (detected by the sensors) 
that have been instantiated in the ontology. A Detection
instance may be associated with persons, objects, activities, 
and semantic spaces (more details on the latter follow next). 
The respective information is communicated to the user via 
class Output and its specializations: Alert, Message, Warn-
ing. 
Figure 2. Semantic spaces and spatial contexts in the SUITCEYES 
ontology. 
An entity that occupies space (e.g., persons, objects) is 
considered a Spatial Entity and the occupied space (e.g., 
a room or a location) belongs to the Semantic Space repre-
sentation. These two aspects formulate the respective entity’s
Spatial Context, which provides information regarding the 
entity’s relationship to the semantic space it is located in. 
Examples include: in, on, left, right, far, close, etc. The 
aforementioned concepts are depicted in Figure 2. 
B.
Sample Usage 
Based on the ontological concepts presented above, Fig-
ure 3 illustrates a sample instantiation resembling an activity 
detected by the system’s camera. The activity involves two 
people speaking to each other, one of them is known to the 
user (i.e., john) and the other is unknown. Moreover, these 
two people are currently located in the kitchen (i.e., 
in_room_spatial_context), and the respective message is 
communicated to the user via a textual description, which is 
then converted to haptograms as described in the next sec-
tion. 
Figure 3. Sample instantiation of an activity involving two people 
discussing in the kitchen. 
This flexible ontology-based representation described 
thus far allows the system to convey various types of infor-
mation to the user. Below is an indicative list: 

Who is involved in an activity? 

Where is my mobile phone located? 

In which room am I now located? 

What objects are on the table? 

Which objects are observed on my left side? 

Which are the objects I am closer to/farther from? 

Alert! An obstacle (e.g., stairs) is in front of you!  
V.
HAPTOGRAM VOCABULARY DESIGN
Although in the next phase two-way communication will 
be our goal, in the current stage of the project our hapto-
gramic vocabulary was designed for in-principle receiver 
testing over a 4 x 4 actuator grid. We were interested in find-
ing out if the ontology and such a haptic conceptual vocabu-
lary can be aligned, and how pattern sequences reminiscent 
of sentences can implement the transmission of more com-
plex semantic content. 
A.
Examples 
In our approach, haptograms can be static or dynamic, 
and can represent both word meaning and sentence meaning. 
Figure 4 illustrates the basic idea of the former version which 
18
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-738-2
SEMAPRO 2019 : The Thirteenth International Conference on Advances in Semantic Processing

was derived from the ASCII code table, where in our matrix 
cells, instead of characters, concepts are encoded. 
Figure 4. Sample static haptograms over a 4 x 4 actuator grid. 
In Figure 5, we illustrate two sample dynamic hapto-
grams. Above, in the matrix cells, the numbers indicate the 
firing sequence of the actuators for concepts (a) and (b), 
meaning “stand” and “door”. Below the completed shape of 
the dynamic haptograms is indicated. 
Figure 5. Unfolding sequences of two patterns over a 4 x 4 actuator grid, 
yielding different dynamic haptograms: (a) “stand”; (b) “door”. 
Moving over to sentence meaning, in Figure 6 we show 
how a simple statement, “An unknown person is standing 
by/at the door”, can be made by concatenating static and 
dynamic haptograms. The statement begins with a single- 
blink sign, indicating the start of a new message, and finishes 
with a double-blink meaning end of transmission. It can be 
accompanied by a separate alert sign to add weight to the 
communicated content. Apart from this example, our test 
included questions and exclamations to enable a future dia-
logue between two users with deafblindness or a user with 
deafblindness and her/his trainer, family member, etc. Fur-
ther, the vocabulary is both aligned with the ontology, and is 
including concepts and parts of speech not covered by the 
current version, i.e., indicates expansion opportunities. 
Likewise, e.g., logical operators, numbers, signs for opera-
tions etc. can be added following the same line of thought. 
VI.
CONCLUSION AND FUTURE WORK
We plan to update the current pattern generator to a max-
imum of 9 x 9 actuator size matrices, subject to feasibility 
evaluation by psychophysics to make sure users are able to 
easily and consequently distinguish between the communi-
cated haptograms, a prerequisite of noise-free or low-noise 
communication. This could include adding numbers and 
ways of calculation to the haptogram kit for example. Paral-
lel to this, new concepts and relations from ongoing ontology 
development will be mapped to a more systematically de-
signed, structured set of static and dynamic haptograms so 
that their semantics, including statements and limited argu-
mentation, can be easier to follow by users. A mobile sender 
unit will be added to the receiving kit to enable two-way 
communication, and we aim to extend the framework to 
sending messages over a distance as well. 
Figure 6. Sample statement constructed from static and dynamic 
haptograms over a 4 x 4 actuator grid: “Unknown person stand(ing) at/by 
(the) door”. 
In more detail, the current approach has its constraints by 
design, limiting incoming sensations and pattern recognition 
to the back of the individual. Given that this area is one of 
the less sensitive body interfaces for pattern recognition, an 
obvious way ahead will be to add more parts of the body as a 
screen, and combine pattern construction with diverse recep-
19
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-738-2
SEMAPRO 2019 : The Thirteenth International Conference on Advances in Semantic Processing

tion areas to extend the grammatical functionality of hapto-
grams, while increasing the richness of the conceptual vo-
cabulary. According to Lemmens et al. [17], for the torso, up 
to 62 sensors and actuators can be considered. This, com-
bined with a more granular pattern generator, opens up new 
opportunities for a more systematic next effort, adding scala-
bility to the approach, inviting knowledge graphs to replace 
ontologies, and increasing the number and complexity of 
situations to be described. One of the subsequent new chal-
lenges will be to match haptogram drawing on a mobile de-
vice by the sender over a much more sensitive surface, and 
its translation to the body. 
ACKNOWLEDGMENTS
The authors are grateful to Sarah Woodin (University of 
Leeds) and Astrid Kappers (Eindhoven University of Tech-
nology) for helpful suggestions. This work has been partially 
funded by the European Union’s Horizon 2020 research and 
innovation programme under grant agreement No 780814 
SUITCEYES. 
REFERENCES
[1]
R. M. Lahtinen, Haptices and Haptemes: A Case Study of 
Developmental Process in Social-haptic Communication 
of 
Acquired 
Deafblind People. PhD dissertation, 
University of Helsinki, 2008. 
[2]
R. M. Lahtinen, R. Palmer, and M. Lahtinen, 
Environmental Description: For Visually and Dual 
Sensory Impaired People. A1 Management UK, 2010. 
[3]
http://suitceyes.eu/. 
[4]
O. Korn, R. Holt, E. Kontopoulos, A. M. Kappers, N. K. 
Persson, and N. Olson, “Empowering Persons with 
Deafblindness: 
Designing 
an 
Intelligent 
Assistive 
Wearable in the SUITCEYES Project,” Proc. 11th
PErvasive 
Technologies 
Related 
to 
Assistive 
Environments Conference, ACM, 2018, pp. 545-551. 
[5]
T. L. McDaniel, Somatic ABC’s: A Theoretical 
Framework for Designing, Developing and Evaluating the 
Building Blocks of Touch-based Information Delivery. 
PhD dissertation, Arizona State University, 2012. 
[6]
A. Israr, and I. Poupyrev, “Tactile Brush: Drawing on 
Skin with a Tactile Grid Display,” Proc. SIGCHI 
Conference on Human Factors in Computing Systems, 
ACM, 2011, pp. 2019-2028. 
[7]
J. Chen, R. Turcott, P. Castillo, W. Setiawan, F. Lau, and 
A. Israr, “Learning to Feel Words: A Comparison of 
Learning Approaches to Acquire Haptic Words,” Proc. 
15th ACM Symposium on Applied Perception, ACM, 
2018, p. 11. 
[8]
C. M. Reed et al., “A Phonemic-Based Tactile Display for 
Speech 
Communication,” 
IEEE 
Transactions 
on 
Haptics, 12(1), pp. 2-17, 2018. 
[9]
M. Walker, and K. B. Reed, “Tactile Morse Code Using 
Locational Stimulus Identification,” IEEE Transactions 
on Haptics, 11(1), pp. 151-155, 2017. 
[10] G. Korres, and M. Eid, “Haptogram: Ultrasonic Point-
cloud Tactile Stimulation,” IEEE Access, Vol 4, pp. 
7758-7769, 2016. 
[11] G. Meditskos, S. Dasiopoulou, and I. Kompatsiaris, 
“MetaQ: A Knowledge-driven Framework for Context-
aware Activity Recognition Combining SPARQL and 
OWL 2 Activity Patterns,” Pervasive and Mobile 
Computing, Vol 25, pp. 104-124, 2016. 
[12] G. Meditskos, and I. Kompatsiaris, “iKnow: Ontology-
driven Situational Awareness for the Recognition of 
Activities of Daily Living,” Pervasive and Mobile 
Computing, Vol 40, pp. 17-41, 2017. 
[13] K. Janowicz, A. Haller, S. J. Cox, D. Le Phuoc, and M. 
Lefrançois, “SOSA: A Lightweight Ontology for Sensors, 
Observations, Samples, and Actuators,” Journal of Web 
Semantics, Vol 56, pp. 1-10, 2019. 
[14] D. 
Brickley, 
and 
L. 
Miller. 
FOAF 
Vocabulary 
Specification 
0.99. Namespace 
Document. 
[Online]. 
Available from: http://xmlns. com/foaf/spec/ 2019.08.13. 
[15] M. Lefranois, “Planned ETSI SAREF Extensions based 
on 
the 
W3C&OGC 
SOSA/SSN-compatible 
SEAS 
Ontology 
Patterns,” 
Workshop 
on 
Semantic 
Interoperability and Standardization in the IoT, SIS-IoT, 
Amsterdam, Netherlands, 2017. 
[16] R. Falco, A. Gangemi, S. Peroni, D. Shotton, and F. 
Vitali, “Modelling OWL Ontologies with Graffoo,” Proc. 
European Semantic Web Conference, Springer, Cham, 
2014, pp. 320-325). 
[17] P. Lemmens, F. Crompvoets, D. Brokken, J. van den 
Eerenbeemd, and G. J. de Vries, “A body-conforming 
Tactile Jacket to Enrich Movie Viewing,” Proc. 3rd Joint 
EuroHaptics Conference and Symposium on Haptic 
Interfaces for Virtual Environment and Teleoperator 
Systems, 2009, p. 7. 
20
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-738-2
SEMAPRO 2019 : The Thirteenth International Conference on Advances in Semantic Processing

