Dynamic Pattern Utilization for Automatic UAV Control Support 
 
Florian Segor, Chen-Ko Sung, Rainer Schönbein, Igor Tchouchenkov, Matthias Kollmann 
IAS – Interoperabilität und Assistenzsysteme 
Fraunhofer IOSB 
Karlsruhe, Germany 
{florian.segor, rainer.schoenbein, igor.tchouchenkov, matthias.kollmann}@iosb.fraunhofer.de 
chen-ko.sung@sung.de 
 
 
Abstract—Within the research done at Fraunhofer IOSB in 
Karlsruhe in the area of civil security, various types of 
sensor/sensor systems, ground vehicles and Unmanned Aerial 
Vehicles (UAVs), have been used in the project AMFIS for 
some years. When it comes to aerial situation overview and 
reconnaissance, the 
research is focused 
primarily on 
electrically operated Vertical Takeoff and Landing (VTOL) 
systems which can be operated easily by police or rescue forces 
on account of the simple use and the good maneuverability 
even during applications in urban areas. One of the main 
research intentions of AMFIS is the further reduction of 
workload for the operator in scenarios where multiple and 
complex networks of different sensors and sensor carriers are 
used. That leads directly to the need for a high level of 
automation of the single sensor carriers. To further improve 
this automation the use of a dynamic and adaptable ground 
pattern as well as the detection and extraction of the 
information content of the displayed ground pattern onboard a 
flying vehicle is examined. The central objective of this 
investigation is the technical advancement of the dynamic 
ground pattern and the evaluation of the present test results as 
well as the use limits and the possibilities of the presented 
solution.  
Keywords-automatic 
UAV 
guidance; 
adaptive 
pattern 
detection; security and reconnaissance; visual communication; 
civil rescue forces 
I. 
 INTRODUCTION 
As for the technological advance today, there are many 
systems and sensors to support rescue forces in their work to 
manage natural or manmade disasters. One focus of the 
research done at Fraunhofer IOSB is the application of 
modern sensors and sensor carriers to support police and 
firefighters in such situations. The project AMFIS [1] is 
concerned with developing an adaptable modular system for 
managing heterogenic mobile, as well as stationary sensors. 
The main task of its ground control station is to serve as an 
ergonomic user interface and a data integration hub between 
multiple sensors mounted on light UAVs or Unmanned 
Ground Vehicles (UGVs), stationary platforms (network 
cameras), ad hoc networked sensors, and a super-ordinated 
control center. 
Within the amount of different sensor carriers already 
integrated in the laboratory test bed, micro UAVs, especially 
small VTOL systems, play a special role. An application of 
multi-rotor systems within rescue or security scenarios had 
become more realistic in recent years because of the rising 
usability and higher levels of automation and has in some 
cases already become reality. The research done in AMFIS 
focuses also on the extension of the application ability and 
automation of these sensor carriers. The aim is a ground 
control station permitting a single operator to control a 
complex heterogeneous reconnaissance system, not only 
sequentially by dealing with one sensor carrier at a time, but 
in parallel with reduced workload and supported by a high 
level of automation. 
Our experiments have shown that the achieved level of 
automation is sufficient in most cases for the automated 
application of multiple sensor carriers with a minimum of 
operator interaction [2], [3], [4]. 
Only the landing process needs the unlimited attention of 
the user or a manual steering pilot because the navigation 
based on GPS and pressure sensors is not precise enough for 
a secure unattended automatic landing when space is the 
limiting factor. 
Though, the automatic take off process of a GPS 
supported VTOL UAV is possible without supervision, 
however, the flight sequence is far away from an absolutely 
secure procedure and can be further improved therefore. 
To remove this restriction and to protect the aircraft as 
well as the personnel and the material near the lift off and 
landing site, procedures were developed to provide an on 
board detection of a ground pattern to use this information 
for an exact automatic landing [5]. 
To use a static pattern, some problems have to be 
considered. Flying on different altitudes, the size of the 
pattern varies and a partial coverage of the pattern is 
inevitable on low altitudes making it hard to provide constant 
pattern detection. In addition, we wanted to use the visual 
information to add a new communication channel to control 
the UAV. For these reasons, the basic detection algorithms 
were designed to be also capable of detecting different 
patterns and to extract additional information from the 
ground pattern as for example deviation from the approach 
path or the direction and speed of a potential movement of 
the landing platform (if, e.g., mounted on a vehicle). 
II. 
RELATED WORK 
With the advance of the technological progress, UAVs 
can be successfully used for more and more applications. 
140
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-319-3
ICONS 2014 : The Ninth International Conference on Systems

Hence, during the last 10 years, varied research results 
concerning 
UAV-swarming, 
independent 
navigation 
behavior, sense-and-avoid procedures and also work within 
the topic of autonomous or automatic landing and lift off 
were published. Within the field of research about the 
automatic landing of a VTOL UAV, the principle of using a 
ground pattern and visual pattern recognition for navigation 
and position extraction has been treated extensively. The 
usability of this approach is undoubted according to the 
achieved success and the application ability of such a system 
is beyond all questions.  
S. Sharp et Al. [6] presented a test bed for onboard 
detection of a defined ground pattern using Commercial Of 
The Shelf (COTS) camera and hardware components. 
Saripalli examines a very interesting application in [7] using 
a pattern detection algorithm on board of a small unmanned 
rotary aircraft. A theoretical approach to track and to land the 
UAV on a co-operative moving object is presented. Zhou et 
al. [8] as well as Yang et al. [9] examined the possibilities of 
an autonomous landing on a static "H"-shaped pattern. 
Especially, Yang pays special attention to the high noise 
immunity and the rotation independence of the detection 
algorithm. Xiang et Al. [10] describe very interesting set up 
with low-cost COTS components (IR Cam of the Wii 
remote). The components are used to build an active IR 
pattern for the positioning system of a multi-rotor UAV. 
Lange et al. [11] also address the landing of an UAV on a 
ground pattern. They concentrate on handling the problem of 
the discrete scaling of the pattern independent of the 
different flight altitudes of the UAV by introducing a special 
designed circular ground pattern. Through different circles, 
which are becoming smaller to the centre of the pattern, the 
algorithm is capable of detecting the landing site also during 
the final flight stage of an approach without the need to adapt 
the absolute magnitude of the pattern. A similar approach is 
followed by Richardson et al. in [12], describing the landing 
of an autonomous UAV on a moving ground platform by 
using a pattern detection algorithm in co-operative 
surroundings. As in [11], a multistage pattern, which enables 
the complete visibility of the pattern for on board recognition 
also at a low flight level, is used.  
All these researchers have shown good success in 
addressing very similar purposes. However, the suggested 
solutions suffer from some limitations as for example the 
restrictions due to the missing discrete pattern scaling during 
landing and takeoff. Additionally, we assume that each static 
mark approach will react on a pattern-like natural or man-
made structure with miss-interpretation or detection errors. 
The dynamic pattern introduced in this research allows the 
construction of an additional communication link to the 
UAV and, besides, solves problems which are not handled 
yet and therefore differs from the present proposals. 
III. 
APPLICATION SCENARIO AND MOTIVATION 
One of the central application scenarios of the AMFIS 
system is to deal with the support of rescue forces in 
disasters or accidents. The varied application of different 
sensors on board of a UAV can provide support for the 
rescue forces and make their work more safe and efficient. 
Derived from the experiments done with the AMFIS system, 
the missing capability of the UAVs used within these 
experiments to precisely take off and land automatically on a 
designated position was identified as one of the main 
challenges for the professional application – especially when 
multiple UAVs are deployed at the same time. To deal with 
this problem the pattern recognition was developed and 
tested with a UAV system experimentally. 
A dynamic pattern is not necessary compelling for the 
solution of the primary problem and quite good results were 
achieved using non-dynamic static patterns. Indeed, a 
dynamic pattern offers additional advantages which extend 
the application possibilities of such a system. Just by using 
the access to an, in principle, almost unlimited pool of 
different signs and symbols, the abilities of a pattern concept 
can be clearly enlarged. By that, the detection capability of 
the algorithm is not limited to the pure localization of the 
pattern any more. It can be extended by the capability to 
extract information content hidden within a detected pattern. 
Besides, a dynamic pattern still offers some other 
advantages. As already Lange et al. [11] stressed out, an 
essential problem within using ground patterns originates 
from the detection of a static pattern at different flight 
altitudes. Even when using a fish-eye lens during an 
approach of the sensor to the pattern, the probability rises 
that parts of the pattern are not grasped by the sensor because 
of the limited aperture angle and the increasing appearance 
of the image or pattern. The use of a dynamically adaptable 
pattern allows resizing the shown pattern. Thus, the size of 
the pattern can be adjusted matching the current flight 
altitudes raising the chance that the sensor is capable of 
viewing the shape completely. Though the algorithm is 
designed to be rotation and scale independent, nevertheless, 
the result quality of the detection algorithm can be further 
improved by aligning the orientation of the pattern with the 
direction of the UAV as well as considering its point of view 
and distorting its perspective. 
However, the introduction of an additional visual 
communication channel provides even more advantages. 
Unfortunately the widely used radio data connections 
between UAVs and their dedicated ground stations can be 
very easily disturbed - intentionally or unintentionally. The 
detection of a used radio frequency can be done using COTS 
systems and even if it is not so easy to break into the 
communication to take over the UAV, in most cases it can be 
overlaid leading to a complete communication breakdown 
between the ground control and the aerial system. Using a 
visual 
communication 
system, 
interfering 
with 
the 
communication becomes more difficult because a potential 
disrupter stays hardly unnoticed if applying a permanent 
influence on the pattern providing ground platform. 
IV. 
ADAPTIVE PATTERN AND ONBOARD DETECTION 
CHAIN 
The currently used setup for development, evaluation and 
demonstration of the conceptual design consists of the 
mobile AMFIS system as a control station, on the one hand, 
and the dynamic ground platform and the camera on board of 
the UAV for visual information extraction, on the other 
141
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-319-3
ICONS 2014 : The Ninth International Conference on Systems

   
 
hand. A central object for further development is the 
technological realization of the dynamic ground platform 
which will be integrated into the AMFIS communication 
backbone for information exchange and to receive control 
commands in the future (see Figure 1). 
 
 
Figure 1.  Sktech of the final target system. 
For the initial non-dynamic testing of the algorithm, a 
static ground pattern with the shape of a white "H" on a 
black background was used. This test setup was designed to 
experimentally deploy the developed algorithm in a realistic 
test scenario under real conditions and environmental factors. 
However, on account of the long-term aim of developing and 
applying 
a 
dynamic 
pattern, 
the 
adaptability 
and 
expendability of the detection interpretation algorithm was 
emphasized. Hence, the developed dynamic pattern should 
show exactly the same static pattern (a white sign on black 
background) to achieve the highest possible contrast in the 
first experiments. Because the detection should be functional 
under bad lighting conditions also, a mechanical solution 
with flipping parts was excluded. An additional requirement 
was the demand for a simple solution to display different 
symbols or patterns. To cope with this, different Light-
Emitting Diode (LED) matrices were examined and tested 
for their suitability. The experimental used ground patterns 
are all slightly different in technology and size. The 
originally used prototype based on single low cost LED 
panels and reached a size of 65cm x 65cm. Tested under 
realistic conditions, it shaped up that the low cost image 
display matrix, which provides control over every single 
LED, is not suitable on account of the used Pulse Duration 
Modulation (PDM) and the low fixed refresh rate. The PDM 
controlled LED cause a flickering not visible for the human 
eye, but for the camera. Experiments showed that this 
flickering troubles the algorithm in detecting possible blobs 
for the pattern in the video. 
To reach a non-flickering representation, small 3x3 
illumination LED matrices were used and assembled to an 
18x21 experimental matrix even smaller than the original test 
system (see Figure 2). This pattern matrix turned out to be 
absolutely flickering free and can, therefore, be detected by 
the algorithm as one structure without any problems. The 
second advantage is that the assembled platform was 
luminous strong and provided the capability to see and detect 
the ground pattern even in bright sun light. 
But, on account of the restrictions of the used 3x3 LED 
pluggable modules as missing control technology, limited 
displaying possibilities, difficult handling and the need for a 
more flexible test bed, the current research in this project is 
focusing on the use of a commercial high-end LED display 
for outdoor application, which is suitable to solve the 
problems of the low-cost display systems and can be 
deployed as a fully dynamic pattern projecting ground 
platform. For the new testbed, a panel of 1.57 square meter 
of high end SMD (Surface-Mounted Device) LEDs was 
purchased and is about to be included in the experimental 
setup. This more advanced system is designed to allow also a 
detection of the optical signals at higher flight levels of 
approx. 30 – 80 meters due to its size. 
In addition, the panel offers the possibility to control the 
single LEDs again. This allows to scale the shown pattern 
and to adapt it to the flight altitude of the drone. The full 1.5 
sqm can be used for maximum scaling and, therefore, 
reaches a size that allows the pattern to be recognized at an 
altitude of about 80 meters. It remains to be examined 
whether a pattern extraction is still possible in this distance. 
If the UAV approaches the pattern or reduces its flight 
altitude over the pattern, the scaling can be adapted and the 
size of the shown pattern is reduced. Therefore, the full 
visibility of the pattern can be guaranteed on a lower flight 
level. Nevertheless, a short distance between camera and 
LED screen, as it happens on every final landing approach, 
can be seen as critical to functionality, because the low 
distance  to the projection screen (d<1 meter to the surface) 
leads to the detection of single LEDs or rows of LEDs by the 
camera. In this case, the process chain is no longer capable 
of finding a coherent pattern area on which a verification and 
classification of the pattern is possible. This leads directly to 
the conclusion, that a workaround or an extension of the 
process chain is necessary to obtain the precise navigation 
during the last seconds of the final landing approach. 
Figure 2.  Illuminated an non-illuminated ground pattern. 
The basic functions for adaptive pattern recognition have 
been reported in [5]. For the pattern recognition, there are 
two major tasks which must be solved. One task is the 
separation and extraction of possible pattern sub images from 
image sequences as pattern candidates for the recognition 
 
142
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-319-3
ICONS 2014 : The Ninth International Conference on Systems

and interpretation of manmade landmarks. The implemented 
process chain with an adaptive threshold operation for this 
task works well and has not been modified for the present 
investigation. Another task is the recognition of patterns or 
manmade landmark images. The challenge of this task is that 
the onboard process chain for image evaluation must be 
robust, non-compute-intensive, expandable and fast. For that 
reason, we developed a so-called "zigzag" method which 
analyzes how many binary values of relevant parts of an 
object image are correlated with the expected values. The 
present investigation has shown that the method is easy to 
extend. 
The algorithm inherits some serious advantages, as for 
example the rotation and scaling independence (see detection 
in Figure 4). At the same time it was designed not only to 
detect a pattern on the ground to calculate correct and GPS 
independent navigation information, but also to extract 
information from the different pattern sequences. The used 
"zig-zag" method has great advantages because of the fast 
and simple logic, used to recognize a single pattern. The 
procedure is quick and efficient and, hence, suited to deliver 
usable results with limited hardware capacity onboard which 
has been proven in the past attempts [5]. 
To achieve a sufficient information density, the different 
patterns have to be enlarged to reach the capability to 
transmit more complex information (see Figure 3). 
 
 
Figure 3.  Examples of used patterns. 
This can be seen as a key feature of the dynamic pattern 
detection beside the improvement of the navigational 
information for the automatic landing. As already mentioned 
above, different patterns are shown at the same projection 
plane sequentially and can be recognized by the camera / 
algorithm on board the UAV. On the one hand, by flipping 
the patterns, errors occurring due to the detection of similarly 
looking natural structures should be avoided in future, 
because the system expects a regular change in the detected 
area. On the other hand, dedicated information will be linked 
to the single symbols. Orders or important information, as 
for example the current wind direction or a possible 
movement of the ground platform, can be encoded and 
transferred using the pattern sequences. 
Therefore, 
the 
palette 
of 
used 
symbols 
was 
complemented with additional signs to extend the capability 
of encoding more complex information into a pattern 
sequence by switching between the introduced signs. 
Nevertheless, the used pattern pool is held small at the 
present time, because for every new introduced pattern the 
algorithm needs to be adapted in order to "learn" the new 
shape and to recognize it during the detection sequence. 
Additionally, an enlargement of the pattern pool also 
requires more logical operations during the scan process of 
possible pattern blobs found in the images which leads 
directly to an enlargement of process workload during the 
classification of the pattern in flight. It remains to optimize 
the balance between size of the pattern pool (for information 
encoding) and duration of the pattern classification process. 
V. 
RESULTS 
With the application of a static pattern, the functionality 
of the algorithm and its suitability has been proven for the 
integration onboard the UAV under the aspect of the limited 
computing capacity within this mobile system. The work 
based on these results has shown furthermore that even a 
simple active pattern which is reduced in its adaptability and 
displaying capacities is capable of improving the detection 
process. In [13], the test construction was described proving 
that the theoretical concept is functional and such a system 
could be applied successfully. The main objectives of these 
tests focused on the suitability of the concept mainly under 
strong external light influence and the enlargement of the 
detection capability to more than a single pattern as well as 
the differences in the detection results of the algorithm under 
differently strong self-illumination of the ground pattern (see 
Figure 2). 
These experiments have shown that the used matrix can 
cope also with direct solar irradiation and emits enough light 
to produce a homogeneous pattern detectable by the 
algorithm.  
Because of the promising results, the next step is to 
improve the possibilities of the ground pattern. Due to some 
restrictions of the used LED matrices like a slightly too big 
pixel distance and the difficult control of the LED sub 
elements of the pattern, there is a need for a platform with a 
higher usability. Therefore, the current step is to change the 
technology of the ground platform to a highly efficient 
commercial LED display that comes with the capability of 
high brightness and low pixel distance as well as multi-color 
representation 
possibilities. 
This 
promises 
a 
further 
improvement of the test bed and allows extending the 
experiments to test a bigger number of patterns without 
additional expenditure. 
For the onboard component a fish eye lens was selected 
in the presented work to increase the detection area on one 
hand and to generate a stronger distortion in the picture 
particularly in the edge areas with the aim to test the 
algorithm also under more complicated conditions on the 
other hand. We assumed that the pattern must always be 
clearly visible from the image sensor, independent of the 
flight level of the UAV during the in-flight detection process. 
Numbers and characters are good land marks, because they 
have a system behind them, and can be encoded with high 
information content. To test the generality of our algorithm 
and process chains for the landmark detection and pattern 
recognition, two ground patterns "H" and "L" were used. 
These two ground pattern consists of the same hardware and 
were assembled currently by manually re-plugging the 3x3 
LED modules. 
Figure 4 shows the results of the pattern recognition 
while maneuvering the sensor carrier over the landing site. 
The width-to-height ratio of the ground pattern and even 
 
143
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-319-3
ICONS 2014 : The Ninth International Conference on Systems

   
 
 
their shape may be strongly distorted by the fish-eye and 
rotation motion. The process chain works also well with the 
ground pattern "L" that is also captured with the fish-eye 
lens. To avoid ambiguity in the pattern recognition, a 
minimal size of the detected object image was used. This 
means that small object images can still be detected and 
extracted, but they are not suitable for pattern recognition. 
Figure 4.  In-flight detection of shape "H" and "L" marked by colored 
circles at the center of the pattern ("H"  is marked red; "L" is marked green) 
Beside the fact that the algorithm cannot recognize the 
ground pattern in some frames the experiments showed that 
structures similar to the pattern can falsify the results. 
Though the used patterns were chosen that natural 
counterparts are rare, nevertheless, the attempts with static or 
partially static patterns showed that faulty detections are 
possible. 
This leads to great danger for the flight system and 
ground crew and is therefore an essential point which needs 
to be solved in future works by introducing the final fully 
dynamic pattern projector. 
VI. 
CONCLUSION AND FUTURE WORK 
On account of the conceptual change from a luminous-
strong but not very easily to adapt matrix LED to an outdoor 
suited high-resolution LED display, the test system can be 
essentially extended in close future. 
New possibilities are arising to improve the abilities of 
the algorithm in its detection quality and further extend its 
error tolerance as well as to develop completely new 
draughts for intercommunication between platform and 
UAV. For this purpose, the introduction of an initial 
recognition sequence which is visualized cyclic before each 
information transmission on the platform should be also 
examined. 
Essential research topics will cover investigations in 
resolution and adaptation of the projected pattern in 
dependence of the altitude of the UAV. The focus will be to 
find the optimum way of scaling the pattern and identifying 
the thresholds at which a homogeneous pattern projection is 
no longer possible and how this problem can be avoided. 
That includes also tests about the practicability of different 
geometrical projections and rotations of the pattern and to 
determine the lowest possible angle of view to admit the 
most precipitous angles of approach. On account of the 
changed pattern technology a renewed test sequence to 
determine the efficiency of the high end panels with regard 
to the environmental conditions in particular to the solar 
irradiation has to be conducted. Due to the extended 
capabilities new possibilities of interaction have to be 
evaluated. The investigation of color coding as well as a 
negotiation or automatic calibration of the pattern can 
provide interesting new capabilities. For example, the 
adaptation of the luminous strength would be conceivable as 
a reaction to the current environmental conditions taking into 
account the feedbacks of the drone forwarding the 
information which kind of pattern coding can be recognized 
with the highest success rate. 
ACKNOWLEDGMENT 
The authors would like to thank their colleagues and 
students, 
especially 
Sebastian 
Friedrich, 
who 
have 
contributed to the work presented in this paper. 
REFERENCES  
[1] S. Leuchter, T. Partmann, L. Berger, E. J. Blum, and R. Schönbein, 
“Karlsruhe generic agile ground station,” Beyerer J. (ed.), Future 
Security, 2nd Security Research Conference, Fraunhofer Defense and 
Security Alliance, pp. 159-162, 2007. 
[2] F. Segor, A. Bürkle, M. Kollmann, and R. Schönbein, “Instantaneous 
Autonomous Aerial Reconnaissance for Civil Applications - A UAV 
based approach to support security and rescue forces,” The 6th 
International Conference on Systems ICONS, pp. 72-76, 2011. 
[3] A. Bürkle, F. Segor, and M. Kollmann, “Towards Autonomous Micro 
UAV Swarms,” Journal of Intelligent & Robotic Systems 61, pp. 339-
353, 2011. 
[4] E. Santamaria, F. Segor, I. Tchouchenkov, and R. Schönbein, “Path 
Planning for Rapid Aerial Mapping with Unmanned Aircraft 
Systems,” The Eighth International Conference on Systems, pp. 82-
87, 2013. 
[5] C.-K. Sung, F. Segor, “Onboard pattern recognition for autonomous 
UAV landing,” Proc. SPIE 8499, Applications of Digital Image 
Processing XXXV, 84991K, October 2012. 
[6] S. Sharp, O. Shakernia, and S. Sastry, “A Vision System for Landing 
an Unmanned Aerial Vehicle,” Proc. of IEEE International 
Conference on Robotics and Automation, pp. 1720-1728, 2001. 
[7] S. Saripalli, “Vision-based Autonomous Landing of an Helicopter on 
a Moving Target,” AIAA Guidance Navigation and Control 
Conference, August 2009. 
[8] Y. Zhou, T. Wang, J. Liang, C. Wang, and Y. Zhang, “Structural 
target recognition algorithm for visual guidance of small unmanned 
helicopters,” IEEE International Conference on Robotics and 
Biomimetics (ROBIO), pp. 908-913, December 2012. 
[9] S. Yang, S. A. Scherer, and A. Zell, “An onboard monocular vision 
system for autonomous takeoff, hovering and landing of a micro 
aerial vehicle,” Journal of Intelligent and Robotic Systems 69(1-4), 
pp. 499-515, 2013. 
[10] W. Xiang, Y. Cao, and Z. Wang, “Automatic take-off and landing of 
a quad-rotor flying robot,” IEEE 24th Chinese Control and Decision 
Conference (CCDC), pp. 1251-1255, May 2012. 
[11] S. Lange, N. Sünderhauf, and P. Protzel, “Autonomous Landing for a 
Multirotor UAV Using Vision,” Workshop Proc. of SIMPAR 2008 
International Conferrence on Simulation, Modeling and Programming 
for Autonomous Robots, pp. 482-491, 2008. 
[12] T. S. Richardson, C. G. Jones, A. Likhoded, E. Sparks, A. Jordan, I. 
Cowling, and S. Willcox, “Automated Vision‐based Recovery of a 
Rotary Wing Unmanned Aerial Vehicle onto a Moving Platform,” 
Journal of Field Robotics 2013, pp. 667-684, 2013. 
[13] C.-K. Sung and F. Segor, “Adaptive Pattern for Autonomous UAV 
Guidance," Proc. SPIE 8856, Applications of Digital Image 
Processing XXXVI, 88560P, September 2013. 
144
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-319-3
ICONS 2014 : The Ninth International Conference on Systems

