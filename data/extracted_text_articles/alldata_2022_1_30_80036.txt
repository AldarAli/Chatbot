Big Data for Monitoring Mobile Applications 
Collection and Indexing Results for Analysis and Decision 
 
Mourlin Fabrice  
Algorithmic, Complexity and Logic 
Laboratory 
UPEC University 
Cretéil, France 
e-mail: fabrice.mourlin@u-pec.fr        
Djiken Guy Lahlou 
Applied Computer Science 
Laboratory 
Douala University 
Douala, Cameroon 
e-mail: gdjiken@fs-univ-douala.cm 
 
 
Laurent Nel 
Leuville Objects 
Paris-Saclay University 
Paris, France 
e-mail: laurent.nel@universite-paris-
saclay.fr
 
 
Abstract—Behavior monitoring is an activity that cuts across all 
software. It ensures that the application proceeds as expected. 
Our Big Data workflow supports data streams of varying rates. 
The collected data sometimes contains errors that we want to 
explain. To this end, we seek to trace the important events of our 
calculations in order to qualify the anomalies in our processing 
and then easily trace the origin of the problem. We have 
implemented a monitoring layer within mobile applications in 
order to perform smart control over a set of mobile devices. We 
have defined a Big Data workflow to collect, index and store the 
log data in order to submit it to an Artificial Intelligence (AI) 
model. We detect behavioural anomalies through the analysis of 
software logs deployed on embedded devices. Based on the 
patterns recognised in the logs, our AI model provides us with a 
sequence of system operations. These operations are scheduled 
to re-deploy a service, change a driver, perform a library 
update, etc. The critical points concern the management of the 
Android APIs with respect to the deployed software; we must 
manage with precision the software updates with respect to the 
firmware versions, among other things. In the end, management 
reports are built every week and issued to the maintenance 
team. These documents are the record of maintenance activities. 
They provide an explanation for periods of non-availability of 
equipment and for the withdrawal of obsolete equipment. 
Keywords— Big Data; indexing; log analysis; distributed 
application; AI model; storage server; anomaly detection. 
I. 
 INTRODUCTION 
With multiple sources of information, the supervision of 
mobile 
applications 
becomes 
essential 
and 
requires 
administration of the different applications. Software 
administration is an essential facet of any application these 
days, from application servers to embedded applications. We 
want to detect anomalies by information visualization, 
borderline behavior or even fraudulent actions. In order to 
have this information, developers use log Application 
Programming Interfaces (APIs) to transmit all behavioral 
data. To make the information usable, log messages generally 
have a format as well as a level or hierarchy of severity. The 
application administrator manages the level of expressiveness 
per module in order not to suffer from too much information. 
There are many areas of application for log messages, 
from system and network aspects to business applications. 
Database servers like Postgresql have log files containing the 
history of activity, from the creation of databases and triggers 
of application connections. Embedded applications share the 
same need. An Android application has access to a log API 
whether it is written in Java, Kotlin or C ++. An Android 
logger corresponds to a variable in memory. It can be stored 
in a file or sent to a socket. 
Log messages play a key role in several phases, not only 
at runtime but also in all the test phase: unit, integration, 
system and functional. During the development phase, they 
provide a view of the state of the application, in terms of 
network, security, business, etc. In the case of an on-board 
application, these data cannot be displayed because the 
peripheral does not necessarily have a screen and it is useful 
to redirect the information into a persistence unit (memory 
card, memory stick, etc.). During the debugging phase, these 
same messages have a tag that allows them to be filtered, or 
even to specify a different level of severity from one package 
to another to configure the feedback. As part of this study, this 
effort is focused on the analysis of log messages in use. 
The difficulties associated with log analysis relate first to 
the volume of messages received. Indeed, it grows rapidly 
with the number of sources. Thus, in the case of on-board 
application monitoring, when the fleet of peripherals is 
enriched with new equipment, it is no longer possible in 
human terms to analyze the logs with serenity. The 
automation of this process is required. A second difficulty 
arises with the flow of these messages, which depends on the 
use of the monitored applications. When the number of users 
grows, then it is necessary to set up information sampling. 
Finally, it is not uncommon for each embedded application to 
have its own log format, even if it is generated by the same 
API. In this case, it is necessary to consider a standardization 
of formats during the collection in order to be able to extract 
information from different sources and put them in causal 
relation. 
All the properties linked to the processing of log messages 
naturally lead us to consider this work as a Big Data workflow 
applied to a time scale. Indeed, it is essential to react to the 
15
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-945-4
ALLDATA 2022 : The Eighth International Conference on Big Data, Small Data, Linked Data and Open Data

problems detected before they become even more serious. In 
this document, we present the results of our work, which 
began more than a year ago with the Big Data prototyping of 
a solution to the anomaly prediction linked to Android 
applications. These mobile applications are used for taking 
pictures of experiments in biology. Users can annotate each 
photo and rearrange the photos within a document linked to 
an experiment. Users export their final document from the 
mobile device to a Web server accessible from the WiFi 
network at the place of the experiment. 
The rest of this paper is structured as follows. Section II 
describes the works close to our domain. Section III provides 
a precise description of our use case. Section IV addresses the 
software architecture of our distributed platform. Section V 
goes into finer details on our streaming approach, which 
includes an indexing step. Section VI focuses on our results 
and 
the 
impact 
on 
the 
maintenance 
task. 
The 
acknowledgement and conclusions close the article. 
II. 
RELATED WORKS 
Predictive log analysis is a widely studied topic. Part of 
the work focuses on enhancing the information itself. A 
second part concerns the use of this data to react, alert, and 
more generally automates a process. 
A. Log analysis methods 
Adam Oliner et al. describe, through several use cases, the 
information useful to report during execution for software 
monitoring [1]. They stress, among other things, the 
importance of adopting a consistent format throughout an 
application. They make the analogy between the follow-up of 
manufacturing on one meeting on a production line and the 
follow-up of the software activity, which is the subject of this 
work. 
T. Yen et al. describe how to leverage distributed 
application logs for the detection of suspicious activity on 
corporate networks [2]. Their work highlights the use of the 
beehive tool for extracting information and producing easily 
exploitable messages. Analysis against a signature database is 
then possible. 
S. He et al. present six methods for log analysis of 
distributed systems: three of them are supervised and three 
others are unsupervised. The authors make a comparative 
evaluation of these methods on a significant volume of log 
messages. They emphasize the strengths of software 
monitoring task automation [3]. 
In more constrained fields such as real time, log analysis 
systems must be able to detect an anomaly in a limited time. 
B. Debnath et al. present LogLens that automates the process 
of anomaly detection from logs with minimal target system 
knowledge [4]. LogLens presents an extensible index process 
based on new metrics (term frequency and boost factors). The 
use of temporal constraint also intervenes in the recognition 
of behavior pattern. Therefore, abnormal events are defined as 
visible in a time window while other events are not. This 
allows semi-automatic real-time device monitoring. 
B. Log analysis and machine learning 
The development of machine learning has greatly 
impacted the use of logs. Depending on the work, studies lead 
to the detection of anomalies or the discovery of software 
attacks. 
Q. Cao et al. presented a work on web server log analysis 
for intrusion detection and server load reduction. The use of 
two-level AI model allowed them to increase the efficiency of 
their detection system. In this approach, the use of decision 
trees structures the log data [5].  
W. Li considers that logs are a complement to the 
software-testing phase. Since the time allocated to testing is 
insufficient, he presents a failure diagnosis strategy based on 
the use of an AI model [6]. He provides a comparative study 
between several models.   
There is a large body of work on network log analysis for 
various protocols including HTTP [7] or data-centric 
protocols such as Named Domain Networking (NDN) [8]. In 
all cases, the strategy is based on formatted messages where 
part of the information is filtered and then submitted to a 
model for prediction. 
C. Reporting of artificial intellgence prediction model 
In order to obtain a set of guidelines for the use of 
predictive machine learning models, it is essential to build 
regular reports on the quality of predictions. In the context of 
clinical experiments, W. Luo et al. published a rulebook for 
AI model development [9].  
P. Henderson et al. present a systematic reporting of the 
energy and carbon footprints of machine learning. The 
authors’ goal is to adapt an efficient reinforcement learning 
strategy and explain the reinforcement learning events [10]. 
L.M. Stevens et al. present a recommendation for 
transparent and structured reporting of Machine Learning 
(ML) analysis results specifically directed at clinical 
researchers [11]. Their goal is to convince many clinicians and 
researchers who are not yet familiar with evaluating and 
interpreting ML analyses. 
D.P. Dos Santos et al. take a similar approach to the 
analysis of radiological images. Their quality is uneven and it 
becomes difficult to provide a reproducible analysis approach. 
It then becomes essential to build reports to explain the state 
of the AI model that led to certain predictions. The authors 
explain how to structure to help build a post analysis 
explanation [12]. 
III. 
USE CASE DESCRIPTION 
A. Context Description 
In biology trainings, many experiments are done where 
students are asked to prepare, perform and follow up. In this 
context, mobile devices are provided to take pictures, record 
sounds, or even use the device's sensors to collect data. To 
save different documents in the memory of the mobile device, 
a software suite is installed. It allows the authentication of the 
user, the dating of each collected information and the transfer 
at the end of the experiment to a server for validation. 
During an experiment, all the peripherals are connected to 
the laboratory WiFi network. This connection authorizes data 
16
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-945-4
ALLDATA 2022 : The Eighth International Conference on Big Data, Small Data, Linked Data and Open Data

exchange with the laboratory server, which will receive all the 
students' data at the end of the experiment for validation by 
the supervisor. This network connection is also used to send 
log messages to monitor the activity of each mobile device. 
This concerns the capture of information: taking a picture or 
recording a single comment or a short video. This type of 
recording is not often used during an experiment because 
several students are monitoring the same experiment and this 
leads to noise pollution for the other participants. 
The analysis of an experiment by a group of twenty 
students takes place over a period of one-day maximum in the 
same experimentation room. This means that the connection 
is made with the same access point for all devices. Even if the 
batteries are initially charged, it is possible at any time to have 
a recharging point in the room. 
Laboratory observation sessions can be short in the early 
grades, such as showing the release of gas bubbles by an 
aquatic plant. Students construct a document to highlight the 
conditions of this phenomenon and then make a video to 
support their comments. Then, they observe the role of light 
and measure its value with the light sensor of the Android 
tablet. A second video will show the release of gas bubbles by 
an aquatic plant in the presence of light. In the absence of 
light, the students make a comparison with pictures. 
In the lab room, a group of students follows an experiment 
with one tablet per student. A typical scenario consists of one 
WiFi access point per room, a set of mobile devices and a 
remote storage server for document backups at the end of the 
session. This scenario is to be multiplied by the number of 
groups, possibly in parallel in different lab rooms. Two 
properties are thus highlighted: on the one hand, a local 
authentication phase on the mobile device, on the other hand 
a centralized storage server (see Figure 1). In addition, the lab 
room has a laptop for the teacher and a shared printer. 
 
 
Figure 1. Network diagram of a laboratory room. 
B. Scenario description 
In order to describe a nominal scenario more precisely, let 
us take the case of a student from the beginning of a lab 
session to the submission of a document at the end of the 
session. 
Figure 2 describes the general flow of the scenario in the 
Unified Modeling Language (UML) notation; it concerns an 
observation session (Lab Session). The biology teacher 
manages this session. Each student manages their own 
documents locally on their local device. Thus, the student 
takes notes, photos, videos and measurements via the 
available sensors. When his work is finished or the teacher has 
closed the session, a student prepares his final document, signs 
it and deposits it on the storage server. 
 
 
 
Figure 2. Nominal scenario during an experiment in a lab room. 
IV. 
SOFTWARE ARCHITECTURE 
If the software architecture of the business part is very 
simple, it is only the entry point of the information collection, 
which gathers the log data on the storage server. The analysis 
of these logs is more complex because it takes into account 
additional constraints: the arrival of log data continuously, the 
need to impose a data schema to index the information, refine 
the search for information and the detection of anomalies. 
A. Client application 
In order to collect information from the activity of the 
actors in the scenario in Figure 2, the log system of the devices 
is used by the students and the teacher. Our goal is to collect 
and cross-reference information from the various sources. 
Thus, it is essential to monitor the events related to the 
management of the laboratory sessions. In addition, any event 
related to an information capture or modification is useful. 
1) Mobile application 
The MobileApp instance in Figure 2 is an Android 
component installed on each tablet. The set of class is written 
in Java using the log API specific to this system. In the 
business part, we have defined a message format in order to 
17
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-945-4
ALLDATA 2022 : The Eighth International Conference on Big Data, Small Data, Linked Data and Open Data

easily extract the information. The creation of the log 
messages is done by using the android.util.Log class, which 
allows not only to prefix with a semantic tab, but also to add 
a severity level. Thus, from a set of messages, a regular 
expression filters the relevant results to focus on the essential 
data. 
In addition to the business events, in this effort, we have 
traced the memory events provided by the garbage collector, 
the transmissions and receptions of information from the http 
network. Moreover, we used the Android Mobility 
Management API to define usage profiles such as Student 
profile. It allows business apps and data to be stored in a 
separate, self-contained space within a device. The teacher has 
full management control over the applications, data, and 
Student profile settings on the device, but has no visibility or 
access to the device's personal profile. This strong separation 
allows teachers to control MobileApp data and security 
without compromising student privacy if they are using 
applications other than those intended for the biology course. 
We have developed a Device Policy Controller (DPC), 
which logs network activity. Network activity logging helps 
us detect and track the spread of malware on tablets. Our DPC 
uses network logging APIs to report the Transmission Control 
Protocol (TCP) connections and Domain Name System 
(DNS) lookups from system network calls.  
To further process the logs on our Big Data cluster, we 
have configured DNS deny lists to detect and alert for 
suspicious behavior. We have enabled Android network 
logging to record every event from the MobileApp 
application. It uses the system's network libraries. Network 
logging records two types of events: DNS lookups and 
network connections. The logs capture every DNS query that 
resolves a host name to an IP address. Other supporting DNS 
queries, such as name server discovery are not logged. The 
Network Activity Logging API presents each DNS lookup as 
a DnsEvent instance. Network logging also records an event 
for each connection attempt that is part of a system network 
request. The logs capture successful and failed TCP 
connections, but User Datagram Protocol (UDP) transfers are 
not recorded. The Network Activity Logging API presents 
each connection as a ConnectEvent instance. All this network 
log configuration is complex, but grouped in a specific 
concrete 
class 
named 
DevicePolicyManager. 
The 
configuration is taken into account asynchronously and it is 
important to validate it before distributing the tablets to 
students at each software update. 
2) Mobile component 
The component deployed on the teacher's laptop is a 
traditional Java component also configured with a message 
format and a log level. This provides a trace of important 
events that take place on this workstation. Log analysis is the 
fastest way to detect what went wrong, which makes logging 
in Java essential to ensure the performance and health of our 
distributed application. The goal is to minimize and reduce 
any downtime, to reduce the mean time to repair. 
We used the slf4j library because it represents a simple and 
highly configurable API. In particular, we have configured the 
directory where the log messages are saved as well as the 
expression to generate the file names with the date. The size 
of the messages is voluntarily limited, so that the subsequent 
collection is always done within a reasonable time. In 
addition, the stack trace is provided for any anomaly. Finally, 
the structure of all logging events follows a pattern consistent 
with the MobileApp component. We have added a log 
converter to hide some information such as student IDs. It is 
important that sensitive information is not traced because this 
data is then transmitted to our Big Data cluster for analysis. 
B. Server application 
The server application part is deployed on the storage 
server. This component, also written in Java, contains the 
implementation of Web service allowing on the one hand to 
receive the documents of the students but also to acknowledge 
the receptions. This part is developed with the Spring Boot 
library. We use intensively the Spring configuration for the 
logs, but also for the persistence aspects. The database is 
Postgresql version 10. As in the previous section, the location 
of the log files for our server component or for the Postgresql 
server is imposed. As an example, we record the trace of any 
http request received by our Web services. The headers are 
kept as well as the response headers. The version of the http 
protocol used is http/1.1. In the same way as for the Laptop 
component, we have imposed a log message format. 
C. Big Data architecture 
This section focuses on describing our Big Data workflow 
from collection to building our AI model. We wanted to 
automate our approach as much as possible because any 
human intervention leads to blockages or even loss of 
information. 
1) Data collection 
This part deals with the collection of log files in order to 
send them to a Kafka queue. These Kafka files are the entry 
points of our Big Data cluster. Because there are 3 different 
types of components, our best choice was to build an event-
based collection based on scenario actions. For the 
MobileApp part, the logs are recorded locally on the device. 
The sending of the information to a Kafka topic is done when 
the student sends his final document to the storage server. This 
approach reduces the number of accesses to the Kafka topic 
server. Thus, the access point of the lab room has been used 
to send an http request with an attachment part (the 
document). This sending is also present in the logs so that the 
next time only a request is sent, not the same data but only the 
new ones. 
The same approach is used for the laptop component. 
When the lab session is closed, the logs on the laptop are sent 
to a Kafka file of the same topic. The message volume is 
lower, but the information is essential when associating with 
the logs of the mobile devices. 
For the storage server, a repetitive task was our best bid 
because this central point does not reveal any particular 
interaction but a continuous flow of data. A cron table was 
used to collect logs from the Postgresql server and the server 
component to a Kafka file of the concerned topic. Data are 
18
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-945-4
ALLDATA 2022 : The Eighth International Conference on Big Data, Small Data, Linked Data and Open Data

automatically routed to the Kafka server where the topics are 
managed. 
2) Big Data analysis 
A Big Data cluster that is built from Hortonworks Data 
Platform (HDP) 3.0.1 virtual machines is used to perform log 
analysis. This solution offers the advantage of deploying 
software from the Hadoop ecosystem while remaining open to 
other installations. Moreover, the Ambari console allows a 
simple configuration of servers such as Kafka for topics or 
Flume for routes. Our software architecture for Big Data is 
based on two software routes from Kafka topics to the 
persistence system. 
Figure 3 shows the layer components of our project. 
Deployed on the lab room platforms, we developed Kafka 
producers for the peripherals, the teaching laptop and the 
storage server. All these producers issue log messages in a 
Kafka topic that is partitioned on the server. This improves the 
access time to the information. 
 
Figure 3. Big Data Software architecture. 
The topic partition is the unit of parallelism in Kafka. On 
both the producer and the broker side, writes to different 
partitions are done fully in parallel. At the output of the Kafka 
topics, two Flume routes have been defined within this 
experiment, each managed by an agent. A first route (red on 
Figure 3) consumes the messages in order to transform them 
for some residual format differences and store them in a 
column-oriented database, HBase, installed on the cluster. A 
second route (green on figure 3) consumes the messages to 
index them according to a Solr data schema. Each persistence 
system has its own role: HBase keeps the log data and Solr 
keeps the indexes on this data to enrich the searches. We 
consider HBase and Solr as two data sources accessible from 
Spark components. The Spark SQL API is easily used to write 
to HBase column families on a Hadoop cluster. In contrast, 
our Spark to Solr consumer does not have such an easily 
accessible API and we used Solr Cloud REST services for our 
updates. 
The data indexed by Solr enables our system to classify 
the messages in order to carry out maintenance operations on 
the various materials. A relevant option here was a linear 
classifier with margin calculation. In fact, in several 
evaluations of AI models, it is established that in the category 
of linear classifiers, the Support Vector Machine (SVM) are 
those that obtain the best results. Another advantage of SVMs, 
and one that is important to note, is that they are very efficient 
when there is little training data: while other algorithms would 
fail to generalize correctly, SVMs are observed to be much 
more efficient. However, when there is too much data, the 
SVM tends to decrease in performance. 
In order to understand the HBase events and their 
distribution on the cluster, we have defined a report template 
to generate a pdf report. It summarizes the activities by table, 
their events, in particular the use of locks. The use of a 
template guarantees the scalability of these reports according 
to the evolutions of the consumer SQL Spark. We added a 
page header with a table name and the current edition date and 
a page footer with the page number. The column header band 
is printed at the beginning of each detail column with the 
column names in a tabular report. This means the part name 
of a log message. 
3) 
Log Data storage 
A first Spark consumer (named "Spark SQL consumer") 
has an essential task to recognize and process the contents of 
the file and load them into an SQL table in memory, perform 
filter operations and put them in a common format. Then, the 
route continues with a backup of these data in HBase tables. 
The role of this Flume route is to store structured information 
in a column-oriented database (the red route in Figure 3). In 
this effort, we experimented keeping software routes with 
Flume for event routing and defined Kafka topics to ensure 
decorrelation between components. This makes it possible to 
simplify the management of components, among other things 
for software updates. In addition, the Kafka API allows more 
controls on the management of messages associated with a 
topic; for example time management. We have added rules to 
ensure that a received message is processed within an hour 
(from a configuration file). In that case, the system raises an 
alert and the data saved in the local file system. 
A Flume agent is an independent daemon process, which 
manages the red route. The Flume agent ingests the streaming 
data from the Kafka topic source to the Spark SQL sink. The 
channel between the source and the sink is a temporary 
storage. It receives the events from the Kafka source and 
buffers them until they are consumed by Spark sinks. It acts 
as a bridge between the source and the sinks. We have added 
a Flume interceptor to decide what sort of data should pass 
through to the channel. It plays first a filter role in case of 
unsuitable data from the Kafka source and inserts the time in 
nanosecond into the event header. If the event already contains 
a timestamp, it will be overwritten with the current time. 
We wrote the script for creating tables structured in 
families of columns to keep the information from the log files. 
The column families are logical and physical groups of 
columns. The columns in one family are stored separately 
from the columns in another family. We assign that data to 
separate column family when they are not often queried. 
Because the column families are stored in separate HFiles, we 
keep the number of column families as small as possible. A 
HFile is a specific map file implementation for HBase. It 
contains key/value data. Moreover, one of our objectives was 
to reduce the number of column families to reduce the 
frequency of mem-store flushes, and the frequency of 
compactions. Moreover, by using the smallest number of 
column families possible, we improve the load time and 
reduce disk consumption. 
19
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-945-4
ALLDATA 2022 : The Eighth International Conference on Big Data, Small Data, Linked Data and Open Data

Our Spark SQL consumer uses the Spark SQL module to 
store data in an HBase database whose schema is structured in 
family of columns. The labels of these families of columns are 
involved in the data schema of the second Spark consumer. 
HBase is a database distributed on the nodes of our Hadoop 
cluster, which allows having a persistence system where the 
data are highly available because the replicated rate on 
separate nodes is set to three. 
4) 
Log Data indexing 
In parallel, another route has the role of indexing the data 
from the logs (green route in Figure 3). From the same Kafka 
source, a second Spark consumer (named "Spark Solr 
consumer") takes care of data indexing while respecting the 
Solr schema. The index is updated for the query steps and then 
the use of a model for the prediction of maintenance tasks. 
Solr Cloud is the indexing and search engine. It is completely 
open and allows us to personalize text analyzes. It allows a 
close link with HBase, database so the schemas used by both 
tools are designed in a closely related way. On our Big Data 
cluster, the Solr installation is also distributed. In that context, 
we have four shards with a replication rate equals to three. 
This allows us to distribute operations by reducing blockages 
due to frequent indexing. We have configured, not only the 
schema, but also the data handlers (schema.xml and 
solrconfig.xml files).  
Our schema defines the structure of the documents that are 
indexed into Solr. This means the set of fields that they 
contain, and define the datatype of those fields. It configures 
also how field types are processed during indexing and 
querying. This allows us to introduce our own parsing strategy 
via class programming. 
The Spark Solr consumer uses the Spring Data and SolrJ 
library to index the data read from the Kafka topic. It splits the 
data next to the Solr schema where the description of each 
type includes a "docValue" attribute, which is the link to the 
HBase column family. For each Solr type, our configuration 
provides a given analyzer. We have developed some of the 
analyzers in order to keep richer data than simple raw data 
from log files. Finally, the semantic additions that we add in 
our analysis are essential for the evaluation of Solr query. 
Likewise, we store the calculated metrics in HBase for 
control. SolrCloud is deployed on the cluster through the same 
Zookeeper agents. Thus, the index persistence system is also 
replicated. We therefore separate the concepts of backup and 
search via two distinct components. This reduces the 
blockages related to frequent updates of our HBase database 
[13]. 
At the beginning of our Solr design, we have built our 
schema based on our data types. Some of them were already 
defined, but some others are new. In addition, we have 
implemented new data classes for the new field types. For 
example, we used RankFieldType as a type of some fields in 
our schema. It allows us to manage enumeration values from 
the log message. Then, it becomes a sub class of FieldType in 
our Solr plugin. We have redesigned Solr filters so that they 
can be used in our previous setups. Our objective was to 
standardize the values present in the logs coming from 
different servers. Indeed, the messages provide information of 
the form <attribute, value> where the values certainly have 
units. However, the logs do not always provide the same units 
for the same attribute calculation. The analysis phase is the 
place to impose a measurement system in order to be able to 
compare the results later. The development pattern proposed 
by SolrJ is simple because it proposes abstract classes like 
TokenFilter and TokenFilterFactory then to build inherited 
classes. Then we have to build a plugin for Solr and drop it in 
the technical directory agreed in the installation of the tool 
[14]. 
5) 
Model factory 
In Artificial Intelligence, Support Vector Machine (SVM) 
models are a set of supervised learning techniques designed to 
solve discrimination and regression problems. SVMs have 
been applied to a large number of fields (bioinformatics, 
information research, computer vision, finance, etc.) [15]. 
SVM models are classifiers, which are based on two key ideas, 
which allow to deal with nonlinear discrimination problems, 
and to reformulate the ranking problem as a quadratic 
optimization problem. In our project, SVMs can be used to 
decide to which class of problem a recognized sample 
belongs. The weight of these classes if linked to the Solr 
metrics on these names. This amounts to predicting the value 
of a variable, which corresponds to an anomaly. 
All filtered log entries are potentially useful input data if it 
is possible that there are correlations between informational 
messages, warnings, and errors. Sometimes the correlation is 
strong and therefore critical to maximizing the learning rate. 
We have built a specific component based on Spark MLlib. It 
supports binary classification with linear SVM. Its linear 
SVMs algorithm outputs an SVM model [16]. We applied 
prior processing to the data from our HBase tables before 
building our decision modeling. These processes are grouped 
together in a pipeline, which leads to the creation of the SVM 
model with the configuration of its hyper-parameters such as 
weightCol. Part of the configuration of these parameters 
comes from metrics calculated by our indexing engine (Figure 
2). Once created and tested, the model goes into action to 
participate in the prediction of incidents. We use a new 
version of the SVM model builder based on distributed data 
augmented. This comes from an article written Nguyen, Le 
and Phung [17]. 
6) 
Report generation 
Jasper Report library allows us to build weekly graphical 
reports on indexing activity. HBase events are collected for 
display. The goal is to correlate the volumes of data saved in 
the database with the updates of the AI model. We would like 
to refine this report template in order to have metrics to decide 
on the model update. Currently, only HBase movements are 
represented graphically. Based on an HBase handler, we 
handle the change events at runtime and send data beans to the 
Jasper Report Server. 
V. 
DATA STREAMING PART  
A. Filtered log strategy 
Our component called Spark SQL Consumer contains a 
Kafka receiver class, which runs an executor as a long-running 
task. Each receiver is responsible for exactly one input 
discretized stream (called DStream). In the context of the first 
20
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-945-4
ALLDATA 2022 : The Eighth International Conference on Big Data, Small Data, Linked Data and Open Data

Flume route, this stream connects the Spark streaming to the 
external Kafka data source for reading input log data.  
Because the log data rate is high, our component reads 
from Kafka in parallel. Kafka stores the data logs in topics, 
with each topic consisting of a configurable number of 
partitions. The number of partitions of a topic is an important 
key for performance considerations as this number is an upper 
bound on the consumer parallelism. If a topic has N partitions, 
then our component can only consume this topic with a 
maximum of N threads in parallel. In our experiment, the 
Kafka partition number is set to four. 
Since log data are collected from a variety of sources, data 
sets often use different naming conventions for similar 
informational 
elements. 
The 
Spark 
SQL 
Consumer 
component aims to apply name conventions and a common 
structure. The ability to correlate the data from different 
sources is a crucial aspect of log analysis. Using normalization 
to assign the same terminology to similar aspects can help 
reduce confusion and error during analysis [17]. This case 
occurs when log messages contain values with different units 
or distinct scales. The log files are grouped under topics. We 
apply transformations depending on the topic the data come 
from. The filtered logs are cleaned and reorganized and then 
are ready for an export into an HBase instance. 
In the next step, the Spark SQL Consumer component 
inserts the cleaned log data into memory data frames backed 
to a schema. We have defined a mapping between HBase and 
Spark tables, called Table Catalog. There are two main 
difficulties of this catalog. 
a) The row key definition implies the creation of a specific 
key generator in our component.  
b) The mapping between table column in Spark and the 
column family and column qualifier in HBase needs a 
declarative name convention.  
The HBase sink exploits the parallelism on the set of 
Region servers, which are under control of the HBase master. 
The HBase sink treats both Put operation and Delete operation 
in a similar way, and both actions are performed in the 
executors. The driver Spark generates tasks per region. The 
tasks are sent to the preferred executors collocated with the 
region server, and are performed in parallel in the executors to 
achieve better data locality and concurrency. By the end of an 
exportation, a timed window a log data are stored into HBase 
tables. 
B. Index construction and query 
The strategy of the Spark Solr Consumer component deals 
with the ingestion of the log data into Apache Solr for search 
and query. The pipeline is built with Apache Spark and 
Apache Spark Solr connector. Spark framework is used for 
distributed in memory compute, transform and ingest to build 
the pipeline.  
The Apache HBase role is the log storage and the Apache 
Solr role is the log indexing. Both are configured in cloud 
mode Multiple Solr servers are easily scaled up by increasing 
server nodes. The Apache Solr collection, which plays the role 
of a SQL table, is configured with shards. The definition of 
shard is based on the number of partitions and the replicas rate 
for fault tolerance ability. The Spark executors run a task, 
which transforms and enriches each log message (format 
detection). Then, the Solr client takes the control and send a 
REST request to Solr Cloud Engine. Finally, depending on the 
Solr leader, a shard is updated. 
We use also Solr Cloud as a data source Spark when we 
create our ML model. We send requests from Spark ML 
classes and read results from Solr (with the use of Solr 
Resilient Distributed Dataset (SolrRDD class). The pre 
statement of the requests is different from the analysis of the 
log document. Their configuration follows another analysis 
process.  With Spark SQL, we expose the results as SQL 
tables in the Spark session. These data frames are the base of 
our ML model construction. The metrics called Term Factor 
(TF) and Inverse Document Frequency (IDF) are key features 
for the ML model. We have also used boost factor for 
customizing the weight of part of the log message. 
VI. 
RESULTS AND TASK MAINTENANCE 
We have several kinds of results. A part is about our 
architecture and the capacity to treat log messages over time. 
Another part is about the classification of log messages. The 
concepts behind SVM algorithm are relatively simple. The 
classifier separates data points using a hyperplane with the 
largest amount of margin. In our working context, the margin 
between log patterns is a suitable discriminant. 
A. Data features 
For our tests, we used previously saved log files from a 
month of application server and database server operations. 
We were interested in the performance of the two Spark 
consumers: For Spark SQL Consumer, the volume of data to 
analyze is 102.9 M rows in HBase. To exploit this data, we 
used a cluster of eight nodes on which we deployed Spark and 
HBase. The duration of the tests varies between 32 minutes 
and 3 hours and 30 minutes. 
 
Figure 4. Spark consumer runtime versus number of partitions. 
 
For Spark Solr Consumer, the volume of data indexed is 
100.5M rows indexed in about an hour. The number of 
documents indexed per second is 34k. We only installed Solr 
on four nodes with four shards and a replication rate of three. 
We have seen improved results by increasing the number of 
Spark partitions (RangePartitioner). At runtime for our data 
set based on a unique log format, the cost of Spark SQL 
consumer decreases when the partitioning of the dataset 
increases, an illustrated in Figure 4. The X-axis represents the 
partition number and the Y-axis represents the time 
21
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-945-4
ALLDATA 2022 : The Eighth International Conference on Big Data, Small Data, Linked Data and Open Data

consuming. We have to oversize the partitions and the gains 
are much less interesting. 
SVM offers very high accuracy compared to other 
classifiers such as logistic regression, and trees. There are 
several modes of assessment. The first is technical; it is 
obtained thanks to the framework used for the development 
(Spark MLlib). The second is more empirical because it 
relates to the use of this model and the anomaly detection rate 
on a known dataset. The analytical expression of the features 
precision, recall of retrieved log messages that are relevant to 
the find: Precision (1) is the fraction of retrieved log messages 
that are relevant to the find: 
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 
|{𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 log𝑚𝑒𝑠𝑠𝑎𝑔𝑒𝑠} ∩{𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 𝑙𝑜𝑔 𝑚𝑒𝑠𝑠𝑎𝑔𝑒𝑠}|
|{𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 log𝑚𝑒𝑠𝑠𝑎𝑔𝑒𝑠}|
               () 
Recall (2) is the fraction of log messages that are relevant to 
the query that are successfully retrieved: 
𝑟𝑒𝑐𝑎𝑙𝑙 = 
|{𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 log 𝑚𝑒𝑠𝑠𝑎𝑔𝑒𝑠} ∩{𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 𝑙𝑜𝑔 𝑚𝑒𝑠𝑠𝑎𝑔𝑒𝑠}|
|{𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 log 𝑚𝑒𝑠𝑠𝑎𝑔𝑒𝑠}|
       () 
𝐹𝛽 = (1 + 𝛽2) ∗ 
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛∗𝑟𝑒𝑐𝑎𝑙𝑙
(𝛽2∗𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛)+𝑟𝑒𝑐𝑎𝑙𝑙                               () 
In Table 1, we have four classes and for each class we 
compute three metrics: true positive (tp), false positive (fp) 
and false negative (fn). For instance, for the third class, we 
note these numbers tp3, fp3 and fn3. From these values, we 
compute precision by label, recall by label and F-score by 
label. 
TABLE I.  
SVM MODEL MEASURES 
Class 
number 
Metrics 
Precision by label 
Recall by label 
F1 score by label 
0.000000 
0.815846 
0.890100 
0.896616 
1.000000 
0.911000 
0.981000 
0.991000 
2.000000 
0.854461 
0.785714 
0.851481 
3.000000 
0.852446 
0.7589148 
0.833129 
Our prediction models are similar to a multiclass 
classification. We have several possible anomaly classes or 
labels, and the concept of label-based metrics is useful in our 
case. Precision is the measure of accuracy on all labels. This 
is the number of times a class of anomaly has been correctly 
predicted (true positives) normalized by the number of data 
points. Label precision takes into account only one class and 
measures the number of times a specific label has been 
predicted correctly normalized by the number of times that 
label appears in the output. The last observations are: 
• 
Weighted precision = 0.901742 
• 
Weighted recall = 0.931803 
• 
Weighted F1 score = 0.981731 
• 
Weighted false positive rate = 0.040009 
Our results for four classes are within acceptable ranges of 
values for the use of the model to be accepted. 
The test empirical phase on the SVM model was not 
extensive enough to be conclusive. However, our results 
suggest that increasing the number of log patterns deteriorates 
the performance. In addition, we defined a finite set of log 
patterns for a targeted anomaly detection approach. 
B. Reporting 
We have created a custom data source to connect to 
Apache Solr, therefore we are able to retrieve data and provide 
them back in following the JRDataSource interface of Jasper 
Report. With this access point, we have extracted metrics 
about the document cache and Query result cache. Both give 
an overview of the Solr activities and is meaningful for the 
analysts. We have deployed the CData JDBC Driver on Jasper 
Reports to provide real-time HBase data access from reports. 
We have found that running the underlying query and getting 
the data to our report takes the most time. When we generate 
many pages per report, there is overhead to send that to the 
browser.  
For the reporting phase, we have developed two report 
templates based on the use of a JDBC adapter. With system 
requests, we collect data about the last events (Get, Put, Scan, 
and Delete). From these HBase view, we have designed the 
report templates with cross tables. For the storage phase, we 
compute and display the number of Put events per timed 
window or grouped over a period. We periodically updated 
the data across report runs. We export the PDF files to the 
output repository where a web server manages them. 
VII. CONCLUSION AND FUTURE WORK 
We have presented our approach on log analysis and 
maintenance task prediction. We showed how an index engine 
is crucial for a suitable query engine. We have developed 
specific plugin for customizing the field types of our 
documents, but also for filtering the information from the log 
message. Because indexing and storage are the two sides of 
our study, we have separated the storage into a Hadoop 
database. We have stressed the key role of our Spark 
components, one per data source. The partition management 
is the key concept for improving the performance of the Spark 
SQL component. The data storage into data frames during the 
micro batches is particularly suitable for the management of 
flows originating from Kafka files. We observed that our 
approach supported a large volume of logs. 
From the filtered logs, we presented the construction of 
our SVM model based on work from the Center for Pattern 
Recognition and Data Analytics, Deakin University, 
(Australia). We were thus able to classify the recognized log 
patterns into classes of anomalies. This means that we can 
identify the associated maintenance operations. Finally, to 
measure the impact of our distributed analysis system, we 
wanted to build automatically reports based on templates and 
highlight indexing and storage activity. 
Our study also shows the limits that we want to push back, 
such as the management of log patterns. The use of an AI 
model is not the guarantee of an optimal result. We want to 
make more use of indexing metrics to give more weight to 
some information in the analyzed logs. We are, therefore, 
thinking of improving the classification model of log data. 
22
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-945-4
ALLDATA 2022 : The Eighth International Conference on Big Data, Small Data, Linked Data and Open Data

A first perspective will be to improve the indexing process 
based on a custom schema. We think that the use of DisMax 
query parser could be more suitable in log requests where 
messages are simple structured sentences. The similarity 
detection makes DisMax the appropriate query parser for 
short structured messages. 
The log format has a deep impact on the Solr schema 
definition and on the anomaly detection. We are going to 
evolve our approach. In the future, we want to extract 
dynamically the log format instead of the use of a static 
definition. We think also about malicious messages, which 
can perturb the indexing process and introduce bad requests in 
our prediction step. The challenge is to manage a set of 
malicious patterns and the quarantine of some message 
sequences. 
 
 
REFERENCES 
 
[1] A. Oliner, A. Ganapathi, and W. Xu, “Advances and challenges 
in log analysis,” Communications of the ACM, 2012, 2nd ed., 
vol. 55, pp. 55-61. 
[2] T. F. Yen et al., “Beehive: Large-scale log analysis for 
detecting suspicious activity in enterprise networks,” In 
Proceedings of the 29th Annual Computer Security 
Applications Conference, pp. 199-208, December 2013. 
[3] S. He, J. Zhu, P. He, and M. R. Lyu, “Experience report: System 
log analysis for anomaly detection,” In 2016 IEEE 27th 
International Symposium on Software Reliability Engineering 
(ISSRE), pp. 207-218, October 2016. 
[4] B. Debnath et al., “Loglens: A real-time log analysis system,” 
In 2018 IEEE 38th International Conference on Distributed 
Computing Systems (ICDCS), pp. 1052-1062, July 2018. 
[5] Q. Cao, Y. Qiao, and Z. Lyu, “Machine learning to detect 
anomalies in web log analysis,” In 2017 3rd IEEE International 
Conference on Computer and Communications (ICCC), pp. 
519-523, December 2017. 
[6] W. Li, “Automatic log analysis using machine learning: 
awesome automatic log analysis version 2.0.,” 3 edition, 
November 2013. 
[7] A. Juvonen, T. Sipola, and T. Hämäläinen, “Online anomaly 
detection using dimensionality reduction techniques for HTTP 
log analysis,” Computer Networks 91, pp. 46-56, November 
2015. 
[8] J. Dongo, C. Mahmoudi, and F. Mourlin, “Ndn log analysis 
using big data techniques: Nfd performance assessment,” In 
2018 IEEE Fourth International Conference on Big Data 
Computing Service and Applications (BigDataService), pp. 
169-175, March 2018. 
[9] W. Luo et al., “Guidelines for developing and reporting machine 
learning predictive models in biomedical research: a 
multidisciplinary view,” Journal of medical Internet research, 
12 ed., vol.18, 2016. 
[10] P. Henderson et al., “Towards the systematic reporting of the 
energy and carbon footprints of machine learning,” Journal of 
Machine Learning Research, 2020, 248 ed., vol. 21, pp. 1-43. 
[11] L. M. Stevens, B. J. Mortazavi, R. C. Deo, L. Curtis, and D. P. 
Kao, “Recommendations for reporting machine learning 
analyses in clinical research. Circulation: Cardiovascular 
Quality and Outcomes,” 2020, 10 ed., vol. 13. 
[12] D. P. Dos Santos and B. Baeßler, “Big data, artificial 
intelligence, and structured reporting,” European radiology 
experimental, 2018, 1rd ed., vol. 2, pp. 1-5. 
[13] K. Koitzsch, “Advanced Search Techniques with Hadoop, 
Lucene, and Solr,” Pro Hadoop Data Analytics, Apress, 
Berkeley, CA, 2017, pp. 91-136.  
[14] J. Kumar, “Apache Solr search patterns,” Packt Publishing Ltd, 
2015. 
[15] M. F. Ghalwash, D. Ramljak, and Z. Obradović, “Early 
classification of multivariate time series using a hybrid 
HMM/SVM model,” 2012 IEEE International Conference on 
Bioinformatics and Biomedicine, IEEE, pp. 1-6, 2012.  
[16] M. Assefi, E. Behravesh, G. Liu, and A. P. Tafti, “Big data 
machine learning using apache Spark MLlib,” 2017 IEEE 
International Conference on Big Data (Big Data), 2017, pp. 
3492-3498. 
[17] T. D. Nguyen, V. Nguyen, T. Le, and D. Phung, “Distributed 
data augmented support vector machine on Spark,” 23rd 
International Conference on Pattern Recognition (ICPR), 2016, 
IEEE. 
 
 
23
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-945-4
ALLDATA 2022 : The Eighth International Conference on Big Data, Small Data, Linked Data and Open Data

