Environmental Map Building and Moving Object Tracking Using  
Helmet-Mounted LiDAR and IMU for Micromobility 
 
Ibuki Yoshida, Akihiko Yoshida 
Graduate School of Science and Engineering 
Doshisha University 
Kyotanabe, Japan 
e-mail: {ctwh0151, ctwj0112}@mail4.doshisha.ac.jp 
Masafumi Hashimoto, Kazuhiko Takahashi 
Faculty of Science and Engineering 
Doshisha University 
Kyotanabe, Japan 
e-mail: {mhashimo, katakaha}@mail.doshisha.ac.jp 
 
Abstract— This paper presents a method of environmental map 
building and moving object tracking using Light Detection And 
Ranging (LiDAR) and Inertial Measurement Unit (IMU) mounted 
on a smart helmet worn by a micromobility rider. This presented 
method can be used for active safety for micromobility, such as 
bicycles, e-bikes, and electric scooters. Distortion in scan data 
from LiDAR is corrected by estimating the helmet pose (three-
dimensional (3D) position and attitude angle) based on 
information obtained from normal distributions transform scan 
matching and IMU. The corrected LiDAR scan data are classified 
into scan data on road surfaces, road boundaries, stationary 
objects, and moving objects in the environments. Moving object 
scan data are used for moving object tracking. Stationary object 
and road boundary scan data are used to represent 3D stationary 
objects and road obstacles, such as curbs, gutters, and steps. Road 
surface scan data from LiDAR are in conjunction with IMU 
acceleration, and small road unevenness, such as potholes and 
humps, is detected to reduce the falling risk of micromobility. 
Furthermore, road surface conditions are identified by integrating 
IMU acceleration data, and a road surface condition map is 
constructed 
to 
provide 
safety 
and 
comfortability 
for 
micromobility riders in environments. Experiments conducted on 
a road on our university campus demonstrate the effectiveness of 
the proposed method. 
Keywords-helmet LiDAR/IMU; micromobility; environmental 
map building; moving object tracking; road boundary detection; 
road unevenness detection; road surface condition estimation. 
I. INTRODUCTION 
This paper is an extended and improved version of an earlier 
paper presented at the IARIA Conference on Sensor Device 
Technologies and Applications (SENSORDEVICES 2022) [1] 
in Lisbon. 
Numerous studies on active safety and autonomous driving 
in the field of Intelligent Transportation Systems (ITS) have 
been conducted [2]. In the field of last-mile automation, there 
has also been flourishing study on delivery robots [3]. The 
environmental map building [4, 5] and tracking of moving 
objects, such as cars, cyclists and pedestrians [6, 7], are 
important issues for autonomous driving and the active safety 
of vehicles and mobility robots. Many related studies have been 
conducted using cameras, radars, and Light Detection And 
Ranging (LiDAR) [8, 9]. In this paper, we focus on 
environmental map building and moving object tracking with 
vehicle-mounted LiDAR. 
To reduce carbon emissions and resolve congestion, the use 
of single-seater micromobility, such as bicycles, e-bikes, 
electric scooters, and standing-type personal mobilities, has 
been attracting attention as a means of short-distance 
transportation in urban cities [10]. The coronavirus disease 
2019 has made people highly resistant to using conventional 
means of urban transportation, such as crowded trains and buses. 
Therefore, micromobilities have become more prevalent as a 
means of reducing the risk of infection in future endemic 
societies. 
Micromobilities can be driven on a road, a bicycle lane, or 
a section of sidewalk. In addition, micromobilities are prone to 
tipping over due to unevenness in road surfaces, and there is 
also the issue that pedestrians in the vicinity who are “doing 
something while walking” will not notice the approach of 
micromobilities.  
Although the frequency of traffic accidents involving 
micromobility has increased recently, study on active safety for 
micromobility is far behind. As a result, this paper explores 
environmental 
sensing 
for 
micromobility, 
such 
as 
environmental map building and moving object recognition for 
active safety for micromobility.  
In the case of micromobility systems, it is difficult to mount 
many sensors on the vehicle body, as is the case with cars, 
because of size and theft concerns. Thus, it is desirable to mount 
small and easily detachable sensors on the handlebar of a 
micromobility or the helmet worn by the micromobility rider. 
This paper presents a Helmet-Mounted LiDAR (HML)-based 
environmental map building and the tracking of moving objects, 
such as cars, two-wheelers, and pedestrians, in dynamic and 
Global 
Navigation 
Satellite 
System 
(GNSS)-denied 
environments. Moving object tracking and map building related 
to stationary 3D objects and road obstacles, such as curbs, 
gutters, and steps, are essential for preventing collision 
accidents of micromobilities. 
The detection of small road unevenness, such as potholes and 
humps, as well as the estimation of road surface conditions, is 
also required to prevent the falling risk of micromobility and 
improve rider comfort. However, it is difficult to detect small 
road unevenness and estimate road surface conditions using 
only LiDAR scan data. In this paper, these are performed using 
acceleration information from a helmet-mounted Inertial 
Measurement Unit (IMU). 
The rest of this paper is organized as follows. Section II 
presents an overview of related work, and Section III describes 
the 
experimental 
system. 
Section 
IV overviews 
the 
40
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

experimental map building and moving object tracking. Section 
V explains the estimation method of the helmet self-pose and 
the distortion-correction method of LiDAR scan data, and 
Section VI presents the method for classifying LiDAR scan 
data into scan data on road surfaces, road boundaries, stationary 
objects, and moving objects. Section VII presents the method 
for estimating the road boundaries and tracking moving objects, 
and Section VIII presents the method for detecting small road 
unevenness and estimating road surface conditions using IMU 
acceleration. Section IX presents experimental results to verify 
the proposed method, followed by the conclusions and future 
works in Section X. 
II. RELATED WORK 
Many studies on environmental mapping and moving object 
tracking have been conducted [4–7]. We previously presented 
an environmental map-building method using car and 
motorcycle-mounted LiDAR based on Normal Distributions 
Transform (NDT)-Graph Simultaneous Localization And 
Mapping (SLAM) [11, 12] to build a three-dimensional (3D) 
point cloud map in community road environments. We also 
presented a method of moving object tracking using car and 
motorcycle-mounted LiDAR [13, 14]. 
Recently, we proposed a method of building a 3D point-
cloud map in sidewalk and roadway environments using 
LiDAR attached to the rider's helmet (HML) of a micromobility 
[15]. This study is an extension of our previous works  [14, 15] 
on environmental map building using HML and moving object 
tracking by motorcycle-mounted LiDAR, and these methods 
are integrated into our HML system. 
Several studies have been conducted on surrounding 
environmental sensing using HML. Indoor SLAM, where 
people on helmets equipped with two-dimensional (2D) or one-
dimensional (1D) LiDAR walk around in building and factory 
environments, has been reported [16–18]. Niforatos et al. [19] 
presented a method of skier detection using 1D LiDAR attached 
to ski helmets to reduce the risk of accidents on ski slopes.  
Apart from map building and moving object tracking, many 
studies on road surface condition estimation have been 
proposed using car and bicycle-mounted sensors, such as 
LiDAR, accelerometer, and smartphone sensors [20–24]. 
However, the estimation of road surface conditions using a 
helmet-mounted IMU remains a challenge. 
To the best of our knowledge, no studies have been 
conducted on environmental sensing including environmental 
map building and moving object tracking in sidewalks and 
roadways using 3D LiDAR and an IMU attached to the rider 
helmet of a micromobility. Although there have been several 
studies on helmets with sensors (smart helmets) in the ITS field 
[25], their use is limited to alcohol detection in motorcycle 
riders and collision-accident detection, as well as confirming 
rider safety after accidents. 
III. EXPERIMENTAL SYSTEM 
Figure 1 shows an overview of the smart helmet. The upper 
part of the helmet is equipped with a mechanical 64-layer 
LiDAR (Ouster, OS0-64) and an IMU (Xsens, MTi-300).  
The HML has a maximum range of 55 m, a horizontal field 
of view of 360° with a resolution of 0.35°, and a vertical field 
of view of 90° with a resolution of 1.4°. LiDAR can obtain 1024 
measurements (distance, direction, and reflected light intensity) 
every 1.56 ms (every 5.6° in the horizontal direction). Therefore, 
approximately 66,000 scan data points are acquired in one 
rotation (360° observation) period (100 ms). 
The attitude angle (roll and pitch angles), angular velocity 
(roll, pitch, and yaw angular velocities), and three-axis 
acceleration of the helmet are obtained from the IMU every 10 
ms. The measurement error for the attitude angle is less than ± 
0.3°, that of the angular velocity is less than ± 0.2°/s, and that 
of the acceleration is less than ± 5 mG. 
The weight of the mechanical LiDAR is 0.5 kg, and the 
smart helmet is heavier and larger than usual helmets. Therefore, 
the LiDAR reduces the usability and practicability of the smart 
helmet. Moreover, it affects the performance of the helmet in 
the event of a crash. However, modern LiDAR technology [26] 
has been developing smaller, more lightweight, and lower 
power consumption solid-state LiDARs than mechanical 
LiDARs. The use of solid-state LiDARs will significantly 
improve the usability and practicability of smart helmets. 
IV. OVERVIEW OF ENVIRONMENTAL MAP BUILDING AND 
MOVING OBJECT TRACKING 
Figure 2 shows the sequence of environmental map building 
and moving object tracking. LiDAR scan data captured in the 
helmet coordinate system attached to the HML are mapped onto 
the world coordinate system using the self-pose (3D position 
and attitude angle) information of the helmet. For this, an 
accurate self-pose of the helmet is required. NDT scan 
matching [27] is employed to estimate the self-pose in GNSS-
denied environments. 
Because LiDAR scans lasers in the omnidirection, all scan 
data within one scan cannot be obtained at a single location 
when the micromobility is moving or swinging, or when the 
rider’s body is swinging. Therefore, if all scan data within one 
scan is transformed using the pose information of the helmet 
simultaneously, distortion arises in the LiDAR scan data 
mapped in the world coordinate system. Because distortion 
leads to inaccurate results in map building and moving object 
tracking, distortion correction of the LiDAR scan data is 
required. The distortion-correction method [15] is briefly 
described in Section V.  
 
 
 
Figure 1.  Overview of the experimental smart helmet. 
 
41
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
Figure 2.  Sequence of environmental map building and moving object tracking. 
 
The distortion-corrected LiDAR scan data are classified 
into scan data on road surfaces, road boundaries, stationary 
objects, and moving objects. Scan data on road surfaces 
(referred to as road surface scan data) are used to recognize 
areas where micromobility can travel. Scan data on stationary 
objects in the environment (stationary object scan data) are used 
to build a stationary object map. Scan data on moving objects 
(moving object scan data) are used for moving object tracking. 
Scan data relating to road boundaries (road boundary scan data) 
are used to detect road obstacles, such as fallen objects on the 
road, and boundaries of the road, e.g., grooves and curbs. The 
classification method of road surface, road boundary, stationary 
object, and moving object scan data from entire LiDAR scan 
data is described in Section VI. The methods for accurately 
estimating road boundaries from road boundary scan data and 
for accurately tracking moving objects from moving object scan 
data are explained in Section VII. 
Although the position determination of small unevenness on 
road surfaces, such as potholes, cracks, and manhole covers, is 
necessary for preventing fall accidents from micromobilities, it 
is difficult to detect them with only LiDAR. Therefore, such 
small road unevenness is detected using acceleration data from 
IMU. Furthermore, the road surface condition is estimated 
using IMU acceleration data, and the related map is built to 
predict the safety of micromobility and the comfort of the rider. 
These methods are described in Section VIII. 
Maps for stationary objects, road boundaries, and road 
unevenness, as well as moving object tracking, are built 
recursively after each LiDAR scan data and IMU acceleration 
data are obtained, whereas maps for road surface conditions are 
built in batch after a micromobility run is complete. 
V. SELF-POSE OF HELMET AND DISTORTION CORRECTION OF 
LIDAR SCAN DATA 
This section briefly explains the self-pose (position and 
attitude angle) estimation of the helmet using NDT scan 
matching and Extended Kalman Filter (EKF)-based distortion 
correction of LiDAR scan data.  
A. Estimation of Helmet Self-Pose 
For the i-th measurement point (i = 1, 2, …n) in the LiDAR 
scan data, the position in the helmet coordinate system is 
denoted by 
(
,
,
)T
Hi
Hi
Hi
Hi
x
y z
p
, and that in the world coordinate 
system by 
( , , )T
i
i
i
x y zi
p
. The following relationship is then 
represented by the homogeneous transformation: 
(
)
1
1
i
Hi
p
p
Τ X
 
(1) 
where 
( , , , , , )T
x y z
X
. 
x y z T
( , , )
 and ( , , )T  are the 3D 
position and attitude angle (roll, pitch, and yaw angles), 
respectively, of the helmet in the world coordinate system. T(X) 
is the following homogeneous transformation matrix: 
cos cos
sin sin cos
cos sin
cos sin cos
sin sin
cos sin
sin sin sin
cos cos
cos sin sin
sin cos
( )
sin
sin cos
cos cos
0
0
0
1
x
y
z
Τ X
 
A voxel map with a cell size of 0.2 m per side is defined in 
the world coordinate system. In NDT scan matching, a normal 
distributions transformation is performed on the scan data 
obtained up to the previous time (referred to as reference scan 
data) in each cell of the voxel map, and the mean and covariance 
of the scan data in each cell are calculated. 
The current self-pose X of the helmet is calculated by 
matching the scan data obtained at the current time (referred to 
as current scan data) with the reference scan data. The current 
scan data are mapped onto the world coordinate system by 
performing a coordinate transformation according to (1) using 
the pose X. These data are then merged into the reference scan 
data. By repeating this process every LiDAR scan period, a 
LiDAR-based map is built. 
B. Distortion Correction of LiDAR Scan Data  
The helmet pose is determined every 100 ms (LiDAR scan 
period) by NDT scan matching. Scan data are acquired every 
1.56 ms during one rotation of LiDAR. During LiDAR 
scanning, all scan data within one scan cannot be obtained at a 
single location when the micromobility is moving or swinging, 
or when the rider’s body is swinging. If all scan data within one 
scan are transformed using the pose information of the helmet 
simultaneously, distortion appears in the mapping of the 
LiDAR scan data onto the world coordinate system. Therefore, 
the distortion in the scan data is corrected by estimating the 
helmet’s pose using the EKF every 1.56 ms, i.e., every LiDAR 
scan data are obtained.  
Figure 3 shows the sequence of distortion correction. The 
LiDAR scan period (100 ms) is denoted as τ, the IMU 
observation period (10 ms) as ΔτIMU, and the scan data 
observation period (1.56 ms) as Δτ. Here, the method for 
correcting the distortion in scan data obtained between time 
(
1)
t
 and t  is described [15]. 
Let us suppose that at the time (
1)
t
, the pose of the 
helmet is calculated by NDT scan matching and estimated using 
the EKF. IMU data are obtained 10 times per LiDAR scan (τ = 
10ΔτIMU). Using the IMU data obtained every ΔτIMU, the EKF 
estimates the pose ˆ (
1,
)
t
k
X
at the time (
1)
IMU
t
k
, 
where k = 0–10. Because the observation period 
 IMU of the 
IMU is 10 ms, and the scan-data observation period 
 is  
 
42
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
Figure 3.  Sequence of distortion correction of LiDAR scan data. 
 
1.56 ms, the LiDAR scan data are obtained six times within the 
IMU observation period (ΔτIMU =6Δτ). 
From 
the 
estimates, 
ˆ (
1,
)
t
k
X
at 
the 
time
(
1)
IMU
t
k
 and ˆ (
1,
1)
t
k
X
 at 
)1
(t
+ (
1)
IMU
k
, 
the interpolation algorithm predicts the helmet’s pose 
ˆ (
t 1, , )
k j
X
 at 
)1
(t
+
IMU
k
jΔτ  (where j = 1–5), at 
which the scan data are acquired.  
The scan data 
(
Hi t 1, , )
k j
p
 (where i = 1, 2, …, n) obtained 
at 
)1
(t
+
IMU
k
jΔτ in the helmet coordinate system are 
transformed into 
(
i t 1, , )
k j
p
 in the world coordinate system 
using (1) as follows: 
(
1, , )
(
1, , )
( ˆ
(
1, , ))
1
1
i
Hi
t
k j
t
k j
t
k j
p
p
Τ X
     (2) 
Using the pose estimate ˆ(
X t 1,10)
 at t
 (=
)1
(t
+
10
IMU ), the scan data 
(
i t 1, , )
k j
p
 obtained by (2) is again 
transformed into the scan data 
* ( )
pHi t
  in the helmet coordinate 
system at t
 by 
*
1
( )
(
1, , )
( ˆ
(
1,10))
1
1
Hi
i
t
t
k j
t
p
p
X
             (3) 
The 
* ( )
pHi t
 denotes the scan data corrected for distortion at 
t
. Using the corrected scan data, environmental map building 
and moving object tracking are performed. In the EKF for 
correcting scan data distortion, a constant velocity model is 
used as the helmet motion (see Appendix). 
VI. CLASSIFICATION OF LIDAR SCAN DATA  
The distortion-corrected LiDAR scan data are classified 
into scan data on road surfaces, road boundaries, and 3D objects 
using a rule-based method. Following that, the 3D object scan 
data are classified into stationary and moving object scan data 
using 
map 
subtraction 
(MS)-based 
classification 
and 
occupancy grid methods. 
A. Classification of Road Surface, Road Boundary, and Object 
Scan Data 
As shown in Figure 4 (a), 64 scan data points obtained for 
each 0.35° (vertical resolution) at a horizontal angle of the 
LiDAR laser beam, are arranged in order of decreasing 
elevation angle 1
2
3
r,r ,r .... First, we obtain the angle 
 of the line 
connecting points r1 and r2 relative to the xy  plane in the world 
coordinate system. If 
10°, r2 is considered road surface 
scan data. This process is performed sequentially in other 
measurements r3, …, rn. When 
10°, the scan data r4, r5, r9, 
and r10 in Figure 4 (a) are considered convex object data. Then, 
the scan data r3 and r8 are considered the road boundary scan 
data. On the other hand, if 
-10°, as r4 in Figure 4 (b), the 
scan data are considered concave object data, and r3 is 
considered road boundary scan data. This process is performed 
on all LiDAR scan data, and only the object scan data are used 
for subsequent processes.  
 
 
 
 
(a) Convex obstacle 
 
 
(b) Concave obstacle 
 
Figure 4.  Classification of LiDAR scan data. The green, red, and blue circles 
indicate scan data on road surfaces, road boundaries, and objects, respectively. 
43
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Because the boundary between the road surface and a 3D 
object can be obtained for fallen objects on the road, the method 
described above can recognize obstacles on the road. 
The angle threshold to classify the scan data is set to 10°. If 
it is small, road slopes will be mis-detected as 3D objects. 
Generally, the slope of steep roads for vehicles is approximately 
6°. Thus, a threshold value of 10°, which is larger than 6°, is set. 
B. Classification of Stationary and Moving Object Scan Data 
For map building, moving object scan data have to be 
eliminated, and stationary object scan data have to be extracted 
from the entire LiDAR scan data. Moving object tracking 
conversely requires the removal of stationary object scan data 
and the extraction of moving object scan data from the entire 
LiDAR scan data. For this, accurate classification of stationary 
and moving object scan data is required. Although, in our 
previous work [15], the classification was performed using the 
occupancy grid method, LiDAR noises and outliers frequently 
result in misclassification.  
To mitigate the misclassification, we compare the current 
object scan data with stationary object scan data in a map built 
up to the previous time. We call this approach MS-based 
classification or dynamic background subtraction-based 
method [14]. Figure 5 shows the MS-based classification 
method. In this method, we subtract the stationary object scan 
data in the map from the current object scan data to remove as 
much stationary scan data as possible from the object scan data.  
The scan data extracted using the MS-based method are 
mapped onto a grid map. The cell on the grid map is a square 
with a side length of 0.3 m. A cell with scan data is called an 
occupied cell. For the moving object scan data, the time to 
occupy the same cell is short (less than 0.8 s in this paper), 
whereas for the stationary object scan data, the time is long (not 
less than 0.8 s). Therefore, by using the occupancy grid method 
based on the cell occupancy time [28], cells occupied by 
moving object scan data (or stationary object scan data) can be 
detected as moving cells (or stationary cells).  
Because an object can occupy multiple cells, adjacent 
occupied cells are clustered. Then, clustered moving cells (or 
stationary cells) are obtained as a moving cell group (or 
stationary cell group). The scan data contained in the moving 
cell group are finally determined as the moving object scan data. 
The stationary object scan data are extracted by subtracting the 
moving object scan data from the current object scan data.  
The LiDAR field of view also moves along with the 
micromobility movement. Although an object that has recently 
entered the LiDAR field of view is stationary, it is misclassified 
as a moving object because the cell occupancy time is short. To 
address this problem, new-observation cells are defined on the 
grid map, which corresponds to the new LiDAR field of view. 
The time of cells entering the LiDAR field of view (TNC) and 
the cell occupancy time (TOC) are measured, and the occupancy 
time rate ( ) is calculated by  = TOC／TNC. Cells with  of 10% 
or more are determined to be new-observation cells and then 
considered moving cells. This can minimize the false 
classification of stationary objects that recently entered the 
LiDAR field of view as moving objects. 
The scan data in the map are sparser in the areas in front of 
the micromobility and the occlusion areas. Therefore, the 
stationary object scan data likewise exist in a sparse state when  
 
 
Figure 5.  Sequence of MS-based classification (top view). 
 
the map is subtracted from the current object scan data. If the 
scan data, sparsely extracted using the MS-based method, are 
mapped onto a grid map, they may be erroneously determined 
as moving cells.  
To overcome this issue, the scan data removed using the 
MS-based method are also mapped onto the grid map as 
stationary cells. As a result, sparse stationary object scan data 
that tend to be moving cells and stationary object scan data 
removed by the MS-based method are both mapped onto the 
grid map. The neighboring cells, which these stationary object 
scan data occupy, are clustered, and the cell group is then 
determined to be a stationary cell group. Accordingly, sparse 
stationary object scan data are correctly determined as 
stationary object scan data using the occupancy grid method. 
In our preliminary experiments, LiDAR could correctly 
detect objects located within a range of approximately 50 m 
from the helmet. A grid map is therefore set up in ±35 m squares 
from the helmet; the distance from the helmet to the vertex of 
the square is approximately 50 m. Considering the resolution 
(0.35°) of the horizontal viewing angle of LiDAR, the cell size 
of the grid map is set to 0.3 m so that at least one measurement 
of LiDAR could be occupied in a cell 50 m away from the 
helmet. Assuming that the width and length of a pedestrian are 
0.4 m and the walking speed is greater than 1 m/s, the time that 
the LiDAR measurements related to the pedestrian occupying a 
cell is less than 0.8 s. Therefore, the threshold of the occupation 
time to determine stationary and moving cells is set to 0.8 s. 
VII.   ROAD BOUNDARY ESTIMATION AND MOVING OBJECT 
TRACKING   
In this section, the methods for estimating road boundaries 
and tracking moving objects are described using road boundary 
and moving object scan data, respectively.  
44
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

A. Road Boundary Estimation 
Although large boundaries between motor roads and 
sidewalks, such as grooves and curbs, can be detected every 
LiDAR scan period (100 ms), misdetection due to sensor errors 
and disturbances frequently occurs. Therefore, detection 
accuracy for road boundaries is improved using the following 
method. A grid map is created with the helmet position in the 
xy-plane in the world coordinate system as its center. Here, the 
cell on the grid map is a square with a side length of 0.5 m. 
Road boundary scan data obtained every LiDAR scan period 
(100 ms) are mapped onto the cell (referred to as road boundary 
cell) of the grid map.  
The occupancy time (ROC) is measured when road boundary 
scan data occupy each road boundary cell, and the unoccupied 
time (RUOC) is also measured when they are unoccupied. If the 
total time (ROC + RUOC) is greater than 2 s and the ratio of 
occupation time to the total time, ROC/(ROC + RUOC) is greater 
than 70 %, the scan data in the road boundary cell are 
considered road boundary scan data.  
Because the width of grooves is generally 0.3–0.5 m, the 
size of the boundary cell is set to 0.5 m. 
B. Moving Object Tracking [29] 
In moving object tracking, the shape of the moving object 
is represented by a cuboid with a width W, a length L, and a 
height H, as depicted in Figure 6. The width Wmeas and length 
Lmeas of the moving object are extracted from the moving object 
scan data. With these values, W and L of the moving object are 
estimated by  
( )
(
1)
(
(
1))
( )
(
1)
(
(
1))
meas
meas
W t
W t
G W
W t
L t
L t
G L
L t
              (4) 
where G represents the gain.  
The height estimate H of the moving object is obtained from 
the height measurements of the moving object scan data.  
The Kalman filter is used to estimate the position and 
velocity of the moving object in the world coordinate system 
based on the centroid position of the rectangle estimated from 
(4). In crowded environments, the rule-based data association 
method is used to accurately match multiple moving objects 
with multiple moving scan data. 
For the Kalman-filter-based tracking of moving object, it is 
assumed that the object moves at an approximately constant 
velocity. The motion model of the object is then given by 
2
2
( )
(
1)
(
1)
/ 2
0
1
0
0
0
0
1
0
0
0
0
1
0
/ 2
0
0
0
1
0
t
t
t
x
x
x
    (5) 
where
x x y y T
( , , , )
x
. ( , )T
x y  and ( , )T
x y
 are the position and 
velocity, 
respectively. 
y T
x
)
,
(
x
 is 
an 
unknown 
acceleration (plant noise).  
If the object moves with various different motions, such as 
moving at a constant speed, going or stopping suddenly, or 
turning suddenly, the use of multi-model-based tracking, such  
 
Figure 6.  Cuboid around the tracked object (car). 
 
 
Figure 7.  Threshold for road unevenness detection. 
 
as an interacting-multiple-model estimator [30], will improve 
the tracking performance. 
VIII.   DETECTION OF ROAD UNEVENNESS AND ESTIMATION OF 
ROAD SURFACE CONDITION 
In this section, the methods for detecting small road 
unevenness and estimating road surface conditions with 
acceleration data from IMU are described. The related 
information can be used to prevent falling risks of 
micromobility and to improve rider comfort. 
A. Detection of Small Road Unevenness 
Because it is difficult to detect small unevenness on roads 
with LiDAR, such as dents, cracks, and manhole covers, they 
are detected using IMU data. The acceleration obtained from 
the IMU in the helmet coordinate system is denoted by 
(
)T
H
Hx
Hy
a ,a ,aHz
a
. Because the acceleration data include the 
gravitational acceleration G, the acceleration 
H
ˆa , where the 
gravitational acceleration is removed, is given by 
sin
ˆ
sin
cos
cos
cos
H
H
G
a
a
                    (6) 
where 
 and 
 represent the roll and pitch angles, respectively, 
of the helmet obtained from the IMU in the world coordinate 
system. 
Then, the acceleration 
(
)T
W
Wx
Wy
Wz
ˆ
ˆ
ˆ
ˆ
a ,a ,a
a
in the world 
coordinate system is given by 
cos
sin
sin
cos sin
ˆ
ˆ
0
cos
sin
sin
sin
cos
cos
cos
W
H
a
a
 
(7) 
When 
ˆawz
  exceeds a threshold, it is assumed that the 
micromobility encounters small road unevenness. The 
threshold of unevenness detection is determined using a 
Hampel filter [31]. As shown in Figure 7, the acceleration data 
45
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

wz
ˆa  are separated by a sliding-time window (window size of 5 s 
in this paper), and median (M) and standard deviation (
) are 
calculated for the acceleration data in this window. From the 
calculation, the points 
3
ˆawz
M
 or 
3
ˆawz
M
 are detected 
as road unevenness.  
The locations of road unevenness in the world coordinate 
systems are determined based on the self-pose of the helmet. 
B. Estimation of Road Surface Condition 
Micromobilities travel on various roads, including non-
paved roads, such as gravel roads, and paved roads that are not 
suitably maintained. Because the road surface condition is 
closely related to the safety of the micromobility, as well as 
rider comfort, it is also estimated from acceleration data to build 
a related map. 
The International Roughness Index (IRI) [32] is used as an 
evaluation index for road surface conditions. To obtain IRI 
values, the vertical displacement caused by road unevenness, is 
calculated by integrating the acceleration data 
wz
ˆa . Because of 
the low-frequency noise of acceleration data, double integration 
of acceleration data with respect to time significantly leads to 
drift errors in the estimation of the vertical displacement.  
To accurately obtain the vertical displacement, the 
acceleration data are integrated in the frequency domain, as 
follows. First, the acceleration data are passed through a 
Hanning window to remove sidelobes, and Fourier transform is 
performed. Following this, low-frequency noise is removed 
using a high-pass filter, and double integration is performed. 
The vertical displacement is then obtained by inverse Fourier 
transform.  
The vertical displacement calculated from the acceleration 
data, which is obtained every IMU observation period (10 ms), 
is denoted by di. Then, the IRI value, J, for every S (10 m in this 
paper) of the traveled distance of the helmet is given by 
1
1
n
i
i
J
d
S
                                  (8) 
where S is obtained from the self-pose of the helmet.  
Note that the units of di, S, and J are mm, m, and mm/m, 
respectively. The higher the IRI value J, the rougher the road 
surface condition.  
Because the IMU is mounted on the helmet, the head motion 
of the rider may affect the estimation of the vertical 
displacement. To reduce the effect of the rider’s head motion as 
well as the low-frequency noise of acceleration data, the cut-off 
frequency of the high-pass filter is set to 10 Hz. 
IX. FUNDAMENTAL EXPERIMENTS 
In this section, the performance of our method is evaluated 
through experiments in a road environment on our university 
campus. 
A. Results of Map Building and Moving Object Tracking 
A micromobility was moved on our university campus road, 
as shown in Figure 8, and environmental map building and 
moving object tracking with LiDAR scan data were performed. 
The traveled distance of micromobility is approximately 500 m, 
and the maximum speed is approximately 30 km/h. Figure 9 
shows the attitude angle and angular velocity of the helmet 
during movement, which are observed by the IMU.  
The experiments are conducted in the following two cases: 
 Case 1: Map building and moving object tracking with the 
distortion correction of LiDAR scan data and the MS-based 
classification method (the proposed method) 
 Case 2: Map building and moving object tracking without 
using either method. 
 
 
 
(a) Top view 
 
              
 
(b) Side view                                                 (c) Curbstone. The longitudinal offset from road is 150 mm. 
Figure 8.  Photo of experimental environment. 
46
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
        
 
 (a) Roll (black) and Pitch(red) angles                                                     (b) Roll (black), pitch (red) and yaw (blue) angular velocities 
Figure 9.  Attitude angle and angular velocity of helmet. 
 
  
 
(a) Overall map (top view) 
 
(b) Enlarged map of area 1 (bird’s-eye view) 
 
Figure 10.  Map-building result. The black dots indicate stationary object scan data, and the red dots indicate estimated road boundaries. 
 
In case 2, although the distortion of LiDAR scan data is not 
corrected, the self-pose of the helmet is estimated using the 
EKF.  
Figure 10 shows the result of map building obtained using 
the proposed method (case 1). This figure shows that the 
proposed method can build an environmental map.  
In our SLAM-based-map building method, the accuracy of 
map building is equivalent to that of the self-pose estimate of a 
helmet. Therefore, to evaluate the accuracy of map building, the 
error of position estimate of the helmet at the goal position is 
measured using a GNSS/LiDAR positioning system set at the 
goal position. Table I shows the result, where the micromobility 
is moved thrice on the road shown in Figure 8. According to the 
table, the proposed method (case 1) provides better map-
building accuracy than case 2.  
Figure 11 presents the result of moving object tracking in 
area 2 shown in Figure 8. In (b), the blue rectangle indicates the 
assessed size of the moving object, and the blue stick indicates  
47
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE I.  ERROR IN POSITION ESTIMATE OF HELMET AT GOAL POSITION. 
 
 
 
(a) Photo of area 2 
 
 
(b) Estimated position and size of moving objects in area 2  
Figure 11.  Result of moving object tracking (top view). 
 
the moving direction of the moving object obtained from the 
velocity estimate. The black (or red) dots indicate the scan data 
removed (or extracted) from the LiDAR scan data using the 
MS-based method.  
In experiments 1–3, there are 111 moving objects (106 
pedestrians and five cars). As a result, in case 1 (proposed 
method), 109 objects (104 pedestrians and five cars) can be 
successfully tracked, and two pedestrians cannot be tracked. 
Conversely, in case 2, 103 objects (98 pedestrians and five cars) 
can be successfully tracked, and eight pedestrians cannot be 
tracked. From these results, our proposed method achieves 
better accuracy in moving object tracking. 
Pedestrians who are not being tracked walk close to trees. 
In case 2, moving cells related to pedestrians are merged with 
adjacent stationary cells related to trees using the occupancy 
grid method, and pedestrians are falsely detected as stationary 
objects. On the other hand, because, in case 1, the stationary 
cells related to trees are removed using the MS-based 
classification method, pedestrians are detected correctly. 
Consequently, untracking of pedestrians occurs more often in 
case 2 than in case 1. 
B. Results of Detection of Road Unevenness and Estimation 
of Road Surface Condition 
A micromobility was moved on an asphalt-paved road on 
our university campus, as depicted in Figure 12. Four 
unevenness (three curbs and a crack) on the road shown in 
Figure 12 are detected, and road roughness condition is 
estimated. It is expected that the accuracy of detecting road 
unevenness and estimating road surface conditions is affected 
by rider weight, vehicle speed, vehicle dynamics, and so on. 
Therefore, the experiments are conducted under conditions of 
different rider weights (55, 65, and 76 kg) and micromobility 
velocities (10 and 15 km/h).  
Figure 13 shows the vertical acceleration 
wz
ˆa  calculated 
from IMU data obtained while driving of the micromobility. 
Table II shows the success rate of detecting road unevenness. 
From the results, when the rider weight is light, the vertical 
acceleration to road unevenness becomes small, and the success 
rate of detection decreases. 
 
 
 
                 
 
a) Curb (+-15mm)                                  b) Crack (-6mm) 
 
Figure 12. Moved path of micromobility, and curb (a) and crack (b) on road. 
Numerals indicate the longitudinal offsets of the road unevenness. 
 
 
(a) Rider weight of 75 kg, and micromobility velocity of 15 km/h 
 
 
(b) Rider weight of 55 kg, and micromobility velocity of 15 km/h 
 
Figure 13.  Vertical acceleration. The “a” and “b” mean the times when the 
micromobility encounters on curb and crack, respectively, shown in Figure 12. 
 
Experiment 1 
Experiment 2 
Experiment 3 
Case 1 
0.38 m 
1.98 m 
0.68 m 
Case2 
5.91 m 
15.10 m 
6.03 m 
48
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
TABLE II.  SUCCESS RATE OF ROAD UNEVENNESS DETECTION 
 
Road Unevenness 
a: curb 
b: crack 
Rider  
Weight  
55 kg 
44.4 % (4/9)  
66.7 % (2/3) 
65 kg 
83.3 % (10/12) 
100 % (4/4) 
75 kg 
100 % (9/9) 
66.7 % (2/3) 
Total 
76.7 % (23/30) 
80.0 % (8/10) 
*The “c” and “d” in c/d indicate the number of detected unevenness and the 
number of times when the micromobility encounters the unevenness, 
respectively. 
 
 
 
Next, as shown in Figure 14, the path traveled by the 
micromobility is divided into 79 sections of 10 m each, and the 
IRI values are obtained, where the road unevenness (curb and 
crack) shown in Figure 12 are in sections 4, 44, 50, and 75. 
Figure 15 (a) shows the results. Because the IRI values for the 
rider with a weight of 55 kg are small and lead to inaccuracy, 
the mean of the IRI values for riders with weights of 65 and 75 
kg is calculated. The mean is shown in Figure 15 (b). Figure 16 
shows a map of the road surface condition, which is drawn 
based on the mean of the IRI values in Figure 15 (b).  
 
 
Figure 14. Section for IRI value estimation. 
 
   
 
(a) IRI value by different rider weights and micromobility velocities                                                               (b) Mean of IRI value 
Figure 15. Estimation result of road surface conditions. In (a), black solid line (weight of 75 kg and velocity of 15 km/h), black dashed line (75 kg and 10 km/h), 
red solid line (65 kg and 15 km/h), red dashed line (65 kg and 10 km/h), light blue solid line (55 kg and 15 km/h), and light blue dashed line (55 kg and 10 km/h). 
 
 
Figure 16. Map of road surface conditions. Different colored circles indicate IRI levels (white:0–3 mm/m, light blue: 3–6, and yellow:6–9). 
 
49
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
             
          
          
 
(a) Section 25 (6.28 mm/m)                         (b) Section 52 (6.40 mm/m)                        (c) Section 70 (6.52 mm/m)                     (d) Section 77 (7.29 mm/m) 
 
         
          
          
 
(e) Section 13 (5.59 mm/m)                         (f) Section 49 (2.57 mm/m)                        (g) Section 61 (5.13 mm/m)                       (h) Section 66 (4.56 mm/m) 
Figure 17. Photo of road surfaces in eight sections. Numerals in brackets indicate the estimated IRI values. The IRI level of (a)–(d) is high (yellow circle in Figure 
16), and that of (e)–(h) is low (white and light blue circles in Figure 16). 
 
 
Because most of the sections on which the micromobility 
travels are flat asphalt-paved roads, the IRI values are small in 
most sections. Figure 17 shows the photos of road surfaces and 
the estimated IRI values in eight of 79 sections. The IRI values 
in sections 4, 44, 50, and 74, where there is road unevenness 
(curb and crack), as shown in Figure 12, are 6.78, 6.94, 8.49, 
and 6.90, respectively.  
Because we currently do not have any professional 
instruments to accurately measure IRI values on roads, such as 
a car-mounted laser pavement scanner, the true IRI value is 
unknown, and the performance of our road surface condition 
estimation cannot be evaluated quantitatively.  
The root mean square (RMS) values of the IRI values shown 
in Figure 15 (a) are 5.48 mm/m (rider weight of 75 kg and 
micromobility velocity of 15 km/h), 5.30 mm/m (75 kg and 10 
km/h), 4.77 mm/m (65 kg and 15 km/h), 4.13 mm/m (65 kg and 
10 km/h), 1.93 mm/m (55 kg and 15 km/h), and 1.98 mm/m (55 
kg and 10 km/h). These RMS values and Table II show that the 
accuracy of the road surface condition map built using 
acceleration data is affected by rider weight and micromobility 
velocity. Eliminating those effects and improving map 
accuracy could be performed using a machine learning-based 
method, where the results of road unevenness detection and 
road surface condition estimation are collected from many 
micromobilities traveling on the same road. A related study in 
this regard is one of our future works. 
The building of maps related to stationary objects, road 
boundaries, and road unevenness, as well as tracking of moving 
objects, should be performed in real time after each LiDAR 
scan data and IMU acceleration data are obtained. In our 
experiments, LiDAR scan data and IMU data are recorded, and 
map building and moving object tracking are performed offline 
using a computer. The specifications of the computer are as 
follows: Windows 10 Pro OS, Intel(R) Core (TM) i7-1065G7 
@1.30GHz CPU, 16 GB RAM, and C++ software language. 
The point cloud library [33] is used for NDT scan matching.  
The RMS values of the processing times are as follows: 
 Distortion correction and NDT scan matching: 4048 ms 
 Classification of LiDAR scan data: 2153 ms 
 Map update of stationary objects and road boundaries: 39 
ms 
 Moving object tracking: 247 ms 
 Detection of road unevenness: 3 ms 
 Total: 6317 ms 
Although a long computational time is currently required 
for map building and moving object tracking, the computational 
time can be reduced by optimizing the program code and using 
a graphical processing unit for real-time operations. 
X. CONCLUSION AND FUTURE WORK  
This paper presented a method of experimental map 
building and moving object tracking using LiDAR and IMU 
attached to a helmet worn by a rider of micromobility. To 
accurately perform LiDAR-based environmental mapping and 
moving object tracking, the distortion of scanning LiDAR data 
was corrected using the self-pose information by NDT scan 
matching and IMU information using the EKF.  
The distortion-corrected LiDAR scan data were classified 
into different data types to build a map composed of scan data 
relating to stationary objects and road boundaries and to track 
moving objects. Furthermore, road unevenness was detected, 
and road surface conditions were estimated using acceleration 
information from the IMU, and the related map was built to 
reduce the falling risk of micromobility and to provide comfort 
50
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

of the rider. The performance of the presented method was 
examined through experiments in a road environment on our 
university campus. 
In future works, experiments in various sidewalk and 
roadway environments will be conducted to thoroughly 
evaluate the proposed method. The accuracy of the 
environmental map will be improved by collecting and 
processing 
map 
information 
obtained 
from 
many 
micromobilities. In addition, the realization of environment 
map building and moving object tracking using small and 
lightweight solid-state LiDAR instead of the mechanical 
LiDAR used in this paper is an important future direction. 
APPENDIX: MOTION MODEL OF HELMET 
As shown in Figure A, the translational velocity of the 
helmet in the helmet coordinate system (OH-xHyHzH) is denoted 
by (
,
,
x
y
z
V V V  ), and the angular velocity (roll, pitch, and yaw 
angular velocities) by (
,
,
H
H
H ).  
The following motion model of the helmet can be obtained 
assuming that the helmet moves at nearly constant translational 
and rotational velocities: 
( )
( )
( )
( )
1
( )
( )
( )
( )
( )
( )
2
( )
3
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
cos
cos
sin
sin
cos
cos
sin
cos
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
x
t
y
t
z
t
H
t
H
t
H
x
a
a
a
x
y
z
V
V
V
　　
　　
　
( )
( )
( )
( )
( )
( )
( )
( )
( )
1
( )
( )
( )
( )
( )
( )
2
( )
( )
( )
( )
( )
( )
3
( )
( )
( )
( )
( )
( )
1
2
( )
( )
3
sin
cos
sin
sin
cos
sin
sin
sin
sin
cos
cos
cos
sin
sin
sin
cos
sin
sin
cos
cos
cos
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
y
a
a
a
z
a
a
a
　　
　　
　　
( )
( )
( )
( )
( )
( )
( )
( )
4
5
6
( )
( )
( )
( )
( )
5
6
( )
( )
( )
( )
( )
5
6
( )
( )
( )
( )
( )
( )
( )
sin
cos
tan
cos
sin
1
sin
cos
cos
H
H
H
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
x t
Vx
y t
Vy
z t
Vz
t
H
t
H
t
H
a
a
a
a
a
a
a
V
w
V
w
V
w
w
w
w
  
(A) 
where ( , , )
x y z   and ( , , )   denote the position and attitude 
angle (roll, pitch, and yaw angles), respectively, of the helmet 
in 
the 
world 
coordinate 
system 
(Ow-xwywzw). 
(
,
,
,
,
,
)
H
H
H
Vx
Vy
Vz
w w
w w
w
w
  denotes the acceleration disturbance. 
 denotes the sampling period of LiDAR scan data and IMU 
data. 
2
1
/2
x
x
V
a V
w
 , 
2
2
/2
y
Vy
a
V
w
 , 
2
3
/2
z
z
V
a V
w
 , 
4
H
a
2
/ 2
w H
, 
2
5
/2
H
H
a
w
, and 
2
6
/2
H
H
a
w
.  
 
 
 
 
Figure A.  Notation for helmet motion. 
 
ACKNOWLEDGMENT  
This study was partially supported by the KAKENHI Grant 
#23K03781, the Japan Society for the Promotion of Science 
(JSPS).  
REFERENCES 
[1] I. Yoshida, A. Yoshida, M. Hashimoto, and K. Takahashi, 
“Simultaneous Localization, Mapping and Moving-Object 
Tracking Using Helmet-Mounted LiDAR for Micromobility,” 
Proc. the 13th Int. Conf. on Sensor Device Technologies and 
Applications, pp. 25–31, 2022. 
[2] E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, “A Survey 
of Autonomous Driving: Common Practices and Emerging 
Technologies,” IEEE Access, vol. 8, pp. 58443–58469, 2020. 
[3] V. Balaska et al., “A Viewpoint on the Challenges and Solutions 
for Driverless Last-Mile Delivery,” Machines 2022, 10, 1059, 
2022. 
[4] B. Huang, J. Zhao, and J. Liu, “A Survey of Simultaneous 
Localization and Mapping,” eprint arXiv:1909.05214, 2019. 
[5] 
S. Kuutti et al., “A Survey of the State-of-the-Art Localization 
Techniques and Their Potentials for Autonomous Vehicle 
Applications,” IEEE Internet of Things Journal, vol.5, pp. 829–
846, 2018. 
[6] 
A. Mukhtar, L. Xia, and TB. Tang, “Vehicle Detection 
Techniques for Collision Avoidance Systems: A Review,” IEEE 
Trans. Intelligent Transportation Systems, vol. 16, pp. 2318–
2338, 2015. 
[7] 
Á. Llamazares, E. J. Molinos, and M. Ocaña, “Detection and 
Tracking of Moving Obstacles (DATMO): A Review,” Robotica, 
vol. 38, pp. 761–774, 2020.  
[8] 
E. Marti, J. Perez, MA. Miguel, and F. Garcia, “A Review of 
Sensor Technologies for Perception in Automated Driving,” 
IEEE Intelligent Transportation Systems Magazine, pp. 94–108, 
2019.  
[9] 
Q. Chena, Y. Xiea, S. Guob, J. Baic, and Q. ShudaKey, “Sensing 
System of Environmental Perception Technologies for 
Driverless Vehicle: A Review of State of the Art and Challenges,” 
Sensors and Actuators A: Physical, vol. 319, 112566, 2021.  
[10] B. Sengul and H. Mostofi, “Impact of E-Micromobility on the 
Sustainability of Urban Transportation-A Systematic Review,” 
Applied Science. 2021, 11(13), 5851, 2021. 
[11] S. Tanaka, C. Koshiro, M. Yamaji, M. Hashimoto, and K. 
Takahashi, “Point Cloud Mapping and Merging in GNSS-
Denied and Dynamic Environments Using Only Onboard 
Scanning LiDAR,” Int. J. Advances in Systems and 
Measurements, vol. 13, pp. 275–288, 2020. 
[12] K. Matsuo, A. Yoshida, M. Hashimoto, and K. Takahashi, “NDT 
Based Mapping Using Scanning Lidar Mounted on Motorcycle,” 
51
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Proc. the Fifth Int. Conf. on Advances in Sensors, Actuators, 
Metering and Sensing, pp. 69–75, 2020. 
[13] S. Sato, M. Hashimoto, M. Takita, K. Takagi, and T. Ogawa, 
“Multilayer Lidar-Based Pedestrian Tracking in Urban 
Environments,” Proc. IEEE Intelligent Vehicles Symp., pp. 849–
854, 2010. 
[14] S. Muro, I. Yoshida, M. Hashimoto, and K. Takahashi, “Moving-
Object Tracking by Scanning LiDAR Mounted on Motorcycle 
Based on Dynamic Background Subtraction,” Artificial Life and 
Robotics, vol. 26, issue 4, pp. 412–422, 2021. 
[15] I. Yoshida, A. Yoshida, M. Hashimoto, and K. Takahashi, “Map 
Building Using Helmet-Mounted LiDAR for Micro-Mobility,” 
Artificial Life and Robotics, vol. 28, issue 2, pp. 471–482, 2023. 
[16] Y. Cai, S. Hackett, G., Ben, F. Alber, and S. Mel, “Heads-Up 
Lidar Imaging with Sensor Fusion,” Electronic Imaging, The 
Engineering Reality of Virtual Reality 2020, pp. 338-1–338-7, 
2020. 
[17] B. Cinaz and H. Kenn, “Head SLAM- Simultaneous Localization 
and Mapping with Head-Mounted Inertial and Laser Range 
Sensors,” Proc. 12th IEEE Int. Symp. on Wearable Computers, 
2008. 
[18] H. Sadruddin, A. Mahmoud, and M. M. Atia, “Enhancing Body-
Mounted LiDAR SLAM using an IMU-based Pedestrian Dead 
Reckoning (PDR) Model,” Proc. 2020 IEEE 63rd Int. Midwest 
Symp. on Circuits and Systems, 2020. 
[19] E. Niforatos, I. Elhart, A. Fedosov, and M. Langheinrich, “ s-
Helmet: A Ski Helmet for Augmenting Peripheral Perception,” 
Proc. the 7th Augmented Human Int. Conf.,2016. 
[20] K. Zang, J. Shen, H. Huang, M. Wan, and J. Shi, “Assessing and 
Mapping of Road Surface Roughness based on GPS and 
Accelerometer Sensors on Bicycle-mounted Smartphones,” 
Sensors, 2018, 18, 914, 2018. 
[21]  S. Cafiso, A. D. Graziano, V. Marchetta, and G. Pappalardo, 
“Urban Road Pavements Monitoring and Assessment Using 
Bike and E-scooter as Probe Vehicles,” Case Studies in 
Construction Materials 16, 2022. 
[22]  K. R. Opara, K. Brzezinski, M. Bukowicki, and K. Kaczmarek-
Majer, “Road Roughness Estimation Through Smartphone-
Measured Acceleration,” IEEE Intelligent Transportation 
Systems Magazine, pp. 209–220, 2022.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
[23]  W. Titov and T. Schlegel, “Monitoring Road Surface Conditions 
for Bicycles – Using Mobile Device Sensor Data from Crowd 
Sourcing,” HCI in Mobility, Transport, and Automotive Systems, 
pp. 340–356, 2019. 
[24]  V. Douangphachanh and H. Oneyama, “Formulation of a Simple 
Model to Estimate Road Surface Roughness Condition from 
Android Smartphone Sensors,” Proc. 2014 IEEE Ninth Int. Conf. 
on Intelligent Sensors, Sensor Networks and Information, 2014. 
[25] A. Pangestu, M. N. Mohammed, S. Al-Zubaidi, S. H. K. Bahrain, 
and A. Jaenul, “An Internet of Things Toward a Novel Smart 
Helmet for Motorcycle: Review,” AIP Conf. Proceedings 2320, 
050026, 2021. 
[26] T. Raj, F. H. Hashim, A. B. Huddin, M. F. Ibrahim, and A. 
Hussain, “A Survey on LiDAR Scanning Mechanisms,” 
Electronics, vol. 9, 2020. 
[27] P. Biber and W. Strasser, “The Normal Distributions Transform: 
A New Approach to Laser Scan Matching,” Proc. IEEE/RSJ Int. 
Conf. on Intelligent Robots and Systems, pp. 2743–2748, 2003. 
[28] M. Hashimoto, S. Ogata, F. Oba, and T. Murayama, “A Laser 
Based Multi-Target Tracking for Mobile Robot,” Intelligent 
Autonomous Systems 9, pp. 135–144, 2006. 
[29] Y. Tamura, R. Murabayashi, M. Hashimoto, and K. Takahashi, 
“Hierarchical Cooperative Tracking of Vehicles and People 
Using Laser Scanners Mounted on Multiple Mobile Robots,” Int. 
J. Advances in Intelligent Systems, vol. 10, no. 1 & 2, pp. 90–
101, 2017. 
[30] E. Mazor, A. Averbuch, Y. Bar-Shalom, and J. Dayan, 
“Interacting Multiple Model Methods in Target Tracking: A 
Survey,” IEEE Trans. Aerospace and Electronic Systems, vol.34, 
pp.103–123, 1998. 
[31]  R. Wicklin, “The Hampel Identifier: Robust Outlier Detection in 
a Time Series,” https://blogs.sas.com/content/iml/2021/06/01/ 
hampel-filter-robust-outliers.html, 10 September, 2022.  
[32]  O. G. Dela Cruz, C. A. Mendoza, and K. D. Lopez, 
“International Roughness Index as Road Performance Indicator: 
A Literature Review,” IOP Conf. Series: Earth and 
Environmental Science, vol. 822, 2021. 
[33] R. B. Rusu and S. Cousins, “3D is here: Point Cloud Library 
(PCL),” Proc. 2011 IEEE Int. Conf. on Robotics and Automation, 
2011. 
 
52
International Journal on Advances in Systems and Measurements, vol 16 no 1 & 2, year 2023, http://www.iariajournals.org/systems_and_measurements/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

