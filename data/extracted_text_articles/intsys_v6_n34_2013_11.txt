289
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Interactive Rigid-Body Dynamics and Deformable Surface Simulations
with Co-Located Maglev Haptic and 3D Graphic Display
Peter Berkelman
Department of Mechanical Engineering
University of Hawaii at Manoa
Honolulu Hawaii, USA
Email: peterb@hawaii.edu
Sebastian Bozlee
Mathematics Department
University of Portland
Portland Oregon, USA
Email: sebbyj@gmail.com
Muneaki Miyasaka
Department of Mechanical Engineering
University of Washington
Seattle Washington, USA
Email: muneaki@uw.edu
Abstract—We have developed a system which can combine
realtime dynamic simulations, 3D display, and magnetic levita-
tion to provide high-ﬁdelity co-located haptic and graphic inter-
action. Haptic interaction is generated by a planar horizontal
array of cylindrical coils which act in combination to produce
arbitrary forces and torques in any direction on magnets ﬁxed
to an instrument handle held by the user, according to the
position and orientation sensed by a motion tracking sensor
and the dynamics of a realtime physical simulation. Co-located
graphics are provided by a thin ﬂat screen placed directly above
the coil array so that the 3D display of virtual objects shares the
same volume as the motion range of the handheld instrument.
Shuttered glasses and a head tracking system are used to
preserve the alignment of the displayed environment and the
interaction handle according to the user’s head position. Basic
interactive environments have been developed to demonstrate
the system feasibility and operation, including rigid bodies with
solid contacts, suspended mass-spring-damper assemblies, and
deformable surfaces. Interactive physical simulation of these
environments requires real-time collision detection between
geometric models; numerical, discrete-time numerical integra-
tion to calculate the physics of networks of mass, spring, and
damper elements; and calculation and actuation of interactive
forces to the user in haptic rendering. Incorporating these
functions into a single executable requires multiple program
threads with various update rates, ideally performed using
a multicore processor PC. Details and discussion of various
simulations are given with experimental results.
Keywords- haptics, interaction, simulation
I. INTRODUCTION
The ideal of virtual reality and haptic interfaces is to phys-
ically interact with simulated objects with the highest possi-
ble ﬁdelity in both the graphical display and the kinesthetic
forces and torques sensed by the user during interaction.
Computer-generated graphics can produce highly realistic,
dynamic 3D imagery in real time, but haptic interfaces are
generally based on single point contact feedback, tactile
cues, and linkage devices which have various limitations
in their force and motion ranges, frequency response band-
width, and resolution.
Our system combines a graphical display with a large
range of motion magnetic levitation device, as shown in
coils
flat display
magnets
instrument handle
position
markers
Magnetic Levitation Haptic Interface:
3D Display of Virtual Environment to User:
3D display of 
physical simulation
virtual extension of handle
Figure 1: Co-located maglev haptic and 3D graphic display
Fig. 1. The graphical display is placed directly above a
horizontal array of cylindrical coils and underneath the
instrument handle held by the user, so that electromagnetic
forces and torques can be generated on magnets embedded
in the handle as the instrument is moved by the user into
contact with the displayed simulated environment.
The magnets embedded in the instrument handle, the coil
array with its current ampliﬁers, and the motion tracking
sensor with its infrared LED markers, function together as
a magnetic levitation system. Several variations of magnet
conﬁgurations have been developed for stable levitation with
the planar coil array, each providing a different combination

290
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
of parameters such as mass and size; force, torque and
impedance ranges; and vertical translation and tilt (roll
and pitch) rotation ranges. For the interactive simulations
described here, two-magnet and four-magnet conﬁgurations
were used, where the two-magnet handle provides greater
feedback force and torque capabilities and the four-magnet
handle is less massive, smaller, and provides somewhat
greater vertical and rotational motion ranges. Both magnet
conﬁgurations provide motion ranges of at least 100x100
mm horizontally, approximately 50 mm vertically, with
unlimited yaw and tilt up to at least 35 degrees.
A secondary, slower and less precise motion tracking
system tracks the position of the user’s head so that the
3D views are generated correctly according to the position
of each of the user’s eyes. A pair of shuttered glasses,
synchronized to the update rate of the graphics on the
monitor, is worn by the user so that each eye sees a
different image as the shutters alternate. In practice, this
head tracking system allows the user to observe the handheld
instrument and the 3D displayed environment together from
the side and from above, in a natural ergonomic position
for hand-eye coordination during dextrous manipulation of
a handheld instrument or tool. Examples of relevant dextrous
tool manipulation tasks include any writing, carving, or
cutting tasks, operation of wrenches or screwdrivers, and
medical needle manipulation for suturing, injections, and
biopsy.
The real-time haptic interaction and graphical display are
generated from a dynamic simulation which must perform
collision detection, ﬁnite element deformation, and haptic
rendering sufﬁciently quickly to support graphical updates
at 30-60 Hz and haptic interaction and magnetic levitation
at 800-1000 Hz. These tasks are sufﬁciently computationally
intensive to be the limiting factor regarding the resolution
and realism of the simulated environment.
This paper is an extended version of the previously pub-
lished conference paper [1]. A survey of similar research
in co-located haptics and graphics, magnetic levitation, and
interactive physical simulation areas is given in Section II.
The implementation details are given in Section III for the
magnetic levitation subsystem and Section IV for the co-
located 3D display subsystem. The physical simulation soft-
ware and haptic rendering details are given in Section V.
Force and motion experimental results for selected interac-
tive simulations are given in Section VI. Continuing work is
described in Section VII followed by a conclusion section.
II. RELATED WORK
The realization of our interactive system depends on the
performance and integration of technology in the areas of
maglev haptics, graphics, and physical simulation. Relevant
prior work in each of these areas is surveyed below.
A. Co-Located Haptics and Graphics
3D graphics and haptic force and/or torque feedback can
be generated at the same location by simply placing the 3D
display behind the haptic interaction device, however, this
method has two drawbacks. First, the body of the haptic
interface device partially occludes the display, and second,
there may be a signiﬁcant difference produced between the
perceived location of the displayed imagery and the surface
of the screen, so that the convergence and focal distance of
the user’s eyes do not match, which is unnatural and may
cause discomfort to the user.
ReachIn, ImmersiveTouch, and SenseGraphics systems [2]
use a partially silvered mirror between the head and hand of
the user, so that the display can be moved out of the way and
the focal and convergence distances of the user’s eyes can
be matched. The haptic device and the user’s hand do not
occlude the 3D graphics behind them, but rather the real and
virtual environments are superimposed and semitransparent
due to the half-silvered mirror, which may be a distraction
to the user.
The ”what you see is what you feel” system [3] uses a
thin ﬂat display with a camera behind it. The video image of
the user’s hand is then extracted from the camera view using
a green screen chroma-key technique, and rendered in the
virtual environment. Holographic display [4] using a diffrac-
tion grid reﬂector and multiple projectors is another method
which has been used for haptic and graphic co-location.
Other systems which have combined co-located haptics
and graphics for user interaction have included mechanisms
built into the display monitor [5], a cable-driven pen above
the monitor [6], or linear induction motors with graphics
projected from overhead [7]. The haptic feedback provided
by these systems is limited, however, to only planar forces
and torques, or predetermined locations, whereas the haptic
interaction in our system provides full six degree-of-freedom
rigid-body force and torque feedback over large ranges of
translation and rotation.
Comparative studies have shown [8] [9] evidence of im-
proved perception and performance from co-located haptic
interaction.
B. Haptic Magnetic Levitation
Hollis and Salcudean ﬁrst developed Lorentz force mag-
netic levitation devices [10] and applied them to haptic
interaction and force-feedback teleoperation. Lorentz force
magnetic levitation haptic interaction development continued
with other more specialized device designs [11] [12] and
larger range devices developed by Berkelman [13] [14].
Lorentz magnetic levitation is based on the Lorentz force
F, which is directly proportional to both electric current
I and magnetic ﬁeld ﬂux density B, integrated along the
current path l, expressed as F =
R
B × I dl. Fixed magnet
assemblies and a set of six coil windings on the levitated
platform must be arranged so that forces and torques can

291
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
be produced in any direction as required for stable 6 DOF
position feedback motion control and stable levitation. The
advantage of the Lorentz actuation method compared to elec-
tromagnetic attraction and repulsion forces is that the force
to current and ﬂux density relationships are linear, and there
is no direct dependence on position, so that the coil current
to force and torque vector transformation is nearly constant
over the motion range of the levitated body.
The range of motion in translation for Lorentz levitation
is limited by the size of the gaps between the magnet faces
in which the magnetic ﬁelds are concentrated, and the range
in rotation is limited by the active areas of each coil in
which the coil windings pass through the magnetic ﬁelds.
To maximize the range of motion in both translation and
rotation, it is best to arrange large-area ﬂat-wound coils onto
a thin hemispherical shell, with a user interaction handle
mounted at its center.
Compared to linkage-based haptic devices such as the
Sensable Phantom [15], the Novint Falcon [16], and the
Force Dimension Delta [17], Lorentz levitation haptic inter-
face devices can provide much greater forces and torques
greater than 10 N and 1.0 N-m, and control stiffnesses
of 10 N/mm are achievable without difﬁculty. Closed-loop
position control bandwidths are greater than 100 Hz in
all directions in both translation and orientation. Lorentz
levitation motion ranges are much more limited, however,
as the Carnegie Mellon and Butterﬂy Haptics devices have
ranges of approximately 25 mm and 30 degrees of rotation,
and a University of Hawaii prototype with a modiﬁed magnet
and coil conﬁguration has a range of 50 mm and 60 degrees
of rotation [18]. Overheating of the actuation coils is not a
problem, as the levitated hemispherical shell is quite thin
with a large surface area and acts as an effective heat
dissipator.
The general design and function of the planar coil array
magnetic levitation system used here is described in [19].
This system uses a ﬁxed planar array of cylindrical coils to
levitate a platform of one or more cylindrical magnets. The
yaw of the levitated platform is unlimited and its horizontal
motion range is determined by the size of the planar array.
Vertical levitation distances of up to 75 mm and tilt angles
of 45 degrees are achievable, depending on the mass of the
levitated platform and the dimensions of the magnets used.
Similar tabletop-scale large range magnetic levitation sys-
tems have been developed for suspension of models in wind
tunnels [20] and for micromanipulation using pole pieces to
shape magnetic ﬁelds [21].
C. Realtime Physical Simulation Libraries and Haptic Ren-
dering Programming Interfaces
Realistic software simulations of dynamic physical envi-
ronments have been developed by Baraff both for rigid [22]
and deformable [23] objects, including efﬁcient collision and
reaction force detection and surface friction. Freely available
Figure 2: Implemented system
physical simulation software packages include the SOFA
framework [24] [25], Bullet Physics, and the PhysX library
from NVIDIA. Higher resolution and performance can be
obtained by using precomputed deformation modes [26] and
6-DOF haptic rendering including torque feedback as well
as force on an interactive instrument can be integrated with
simulations [27].
Realistic haptic interaction with dynamic simulated envi-
ronments typically requires realtime computation at update
rates in the range of 1000 Hz. Collision detection, calcula-
tion of rigid body contacts [28], and simulation of physical
dynamics, must be performed concurrently with the control
of the haptic interaction device. Virtual coupling, using a
virtual spring and damper to connect an interaction object in
the simulated environment with the physical object grasped
by the user [29], is a straightforward method to integrate a
simulated environment with a haptic interaction device.
Several software packages are freely available for hap-
tic rendering and realtime physical simulation. H3D [30]
and Chai3D [31] include driver interfaces for common
commercial haptic interface devices such as the Sensable
Technologies Phantom [15]. A programming interface is also
available with the magnetic levitation haptic interface from
Butterﬂy Haptics LLC [32].
III. IMPLEMENTED MAGNETIC LEVITATION SYSTEM
The motion tracking, magnetic levitation control, haptic
rendering, physical simulation, and graphical display in our
current system are all executed in real time in separate
threads on a single quad-core PC in Linux 2.6. GNU C/C++
was used for all programming. An initial demonstration
concept of the system with a simulation of a single paddle
instrument and a ball rolling on a plane, an earlier magnet
and coil conﬁguration, and a conventional 2D display was
demonstrated previously [33]. The current system is shown

292
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 3: Levitated 4-magnet instrument
in Fig. 2, including the planar 3D display, haptic instrument
handle, current ampliﬁers, and head tracker.
A. Magnetic Levitation Hardware Setup
The motion tracking of the handheld instrument in our
system is done using a Northern Digital Optotrak Certus
position sensor and three infrared Smart Markers. Motion
tracking updates are provided at 860 Hz with a position
resolution of approximately 0.01 mm for each marker. Ac-
tuation forces and torques are generated by a closely packed
array of 27 cylindrical coils, each with 1000 windings, 25
mm diameter, and 30 mm height. Either a two-magnet or
four-magnet instrument handle can be used with the system;
the two-magnet 125 g instrument can provide greater haptic
forces and torques but is more massive and bulky, and the
smaller 75 g four-magnet instrument occludes the user’s
view of the display less due to its compact size. Forces are
limited to approximately 4 N due to heating of the actuation
coils, although higher momentary peak forces are possible.
The four-magnet instrument is shown in Fig. 3, levitated
above the 27-coil array at a height of 30 mm and a tilt
angle of 20 degrees. This coil array is underneath the 3D
display monitor shown in Fig. 2. The motion tracker for
the haptic instrument is mounted on a rigid frame at ceiling
level, looking downwards.
B. Design and Control Software
The general design and evaluation methods used in the
development of the magnetic levitation system are described
in detail in [34]. Electromagnetic modeling of the forces and
torques between each magnet and coil was performed using
Mathematica [35] from Wolfram Research and Radia [36], a
freely available software package developed by the European
Synchrotron Radiation Facility.
At each sensor update of the levitation control system,
the coil current to levitation force and torque transformation
matrix is calculated according to the levitated body posi-
tion and orientation and the precomputed electromagnetic
models, control forces and torques are generated according
to proportional and derivative (PD) error gain control laws
for each of the total 6 degrees of freedom in translation
and rotation, and updated coil currents are calculated using
the pseudoinverse of the coil current to force and torque
transformation matrix.
IV. CO-LOCATED 3D GRAPHICAL DISPLAY
The NVIDIA 3D Vision package was used with Linux
drivers to provide 3D display of the simulated environment.
This package uses shutter glasses which are synchronized
with the graphics card by an infrared emitter box. A Quadro
4000 graphics card was used with a ViewSonic vm2268
monitor with a 120 Hz update rate. OpenGL and GLUT
graphics libraries are used for the 3D graphics rendering.
The case of the monitor was removed and backplane
circuit boards and wiring were moved so that the monitor
backlight and display could be placed directly on the coil
array. The combined thickness is under 10 mm, so that haptic
forces and torques can be applied to the handheld instrument
up to a vertical height of at least 60 mm. Magnetic ﬁelds
from the instrument magnets and coil array were not found
to interfere with the display, and there are no ferromagnetic
components in the display to interfere with the magnetic
levitation system. A thin sheet of polycarbonate plastic was
ﬁxed on top of the monitor screen for protection from
impacts from the magnets and instrument, and an aluminum
frame was built to protect the edges of the display.
Head tracking was implemented using a Northern Digital
Polaris Vicra and passive reﬂective markers to produce
correct 3D display according to the position of each eye. The
spatial position and orientation of the shutter glasses from
the positions of four reﬂective markers ﬁxed to the glasses.
Position and orientation data were updated at a 10 Hz rate
with a resolution of approximately 0.1 mm for each marker.
It would be possible to track both the magnet instrument and
and the user’s head using a single motion tracking system,
but this would require using wired infrared markers on the
3D shutter glasses, slowing down the update rate of the
magnetic levitation localization due to the additional LED
markers on the glasses, and mounting the localizer at least

293
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 4: Shutter glasses with synchronization signal trans-
mitter, reﬂective markers, and localizer for 3D graphic
display with head tracking
3.5 m high so that its sensing volume includes the location
of the glasses.
As both the Optotrak and Polaris motion trackers use
infrared position sensing, and 3D Vision systems uses in-
frared communication to synchronize display frames with
the shutter glasses, it is necessary to ensure that each
infrared system does not interfere with the others. In our
system, each set of emitters and receivers are oriented in
orthogonal directions and positioned so that each emitter
is only visible to its corresponding receiver. The Optotrak
sensor is mounted above the table looking down at the LEDs
on the instrument, the Polaris is mounted on the side of
the table to track the reﬂective markers on the side of the
glasses, and the synchronization emitter is mounted at the
front of the tabletop. The synchronization emitter, shutter
glasses with reﬂective position markers, and head tracking
localizer are shown in Fig. 4.
The 3D vision system as described produced reasonably
convincing 3D graphics but had a number of minor short-
comings. The horizontal position of the monitor resulted in
a reduction in brightness observed by the user due to the
change in viewing angle. Light reﬂections from the glossy
screen could be distracting, but the room can be darkened
to eliminate this problem. The 10-15 Hz update rate of the
head tracking system and its communication latency produce
a noticeable lag if the user’s head moves quickly. The motion
tracking reﬂectors on the side of the shutter glasses are also
somewhat cumbersome. Many of the shortcomings of the
present head tracking system could be overcome by using a
radio-frequency emitter, currently available from NVIDIA,
rather than an infrared signal for the shutter glasses synchro-
nization, and using an optical tracker with a quicker update
rate.
Figure 5: Peg in hole simulation with grapsed tool aligned
with graphical peg
V. HAPTIC SIMULATIONS
Basic interactive simulations which have been imple-
mented on our system at present include point, edge, and
face contacts between simple solid shapes such as square
peg-in-hole insertion as shown in Fig. 5, simple dynamic
environments including suspended masses and springs, and
rolling objects. These simple initial simulations allow the
dynamics and contact models of the environments to be
modiﬁed and adjusted to provide the most realistic haptic
interaction while preventing unstable dynamics.
A more sophisticated simulation which involves an in-
strument contacting a deformable surface is shown in Fig.
6. In this simulation, a virtual extension is added to the
actual haptic instrument handle, and the deformation of the
surface and reaction forces and torques on the instrument
are calculated at the haptic update rate. Damping is added
to the internal dynamics of the deformable body and the
surface dynamics during contact with the haptic instrument.
The MLHI library and programming interface, originally
from Butterﬂy Haptics LLC, has been adapted for use
with our system and can be used for haptic rendering and

294
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 6: Deformable tissue simulation with grasped tool
aligned with graphical scalpel
communication between simulation and magnetic levitation
threads with a haptic update rate of 1000 Hz. Alternatively,
haptic rendering and dynamic simulation calculations can
be performed synchronously with the motion tracking at
860 Hz.
A. Software Implementation
Four basic simulations were written to demonstrate the
capabilities of the colocated haptic system. All simulations
made use of the 3D-parallax, head tracking, and haptic capa-
bilities of the system. All but one made use of the system’s
6-DOF position sensing and haptic output capabilities. In
all cases, head tracking, 3D display, and haptics were run at
full speed, and physics calculations were performed at the
haptic update rate.
The simplest simulation consists of a ball hanging on a
virtual spring whose other end is moved freely by the user.
By moving the maglev handle, the user may swing the ball,
experiencing inertial forces as well as seeing 3D parallax
effects. No torques are experienced by the user.
A second simulation consists of a paddle manipulated by
the user in both translation and orientation, and a ball that the
user may bounce on the paddle. This demonstration presents
the user with stronger and more variable haptic feedback,
this time including gentle torques based upon the location
of the paddle-ball contact.
The third simulation consists of a rectangular user-con-
trolled peg and a rectangular hole into which the user may
insert the peg. Edge-edge, edge-face, and vertex-face con-
tacts are all posible and result in both forces and torques
applied to the haptic instrument handle. While inserted into
the hole, haptic feedback is sufﬁciently stable that the user
may safely let go of the instrument handle, leaving the vir-
tual hole walls to support the handle. This demonstration
involves stiff haptic feedback in both forces and torques.
The ﬁnal and most sophisticated simulation consists of a
virtual tool controlled by the user and a deformable surface
with which the user may interact. The deformable surface
is modeled by a hexahedral mass-spring-damper lattice. De-
formation and jello-like vibration may be observed by the
user on contact with the surface.
While the code for display, physics, and haptic feedback
differs from program to program, each demo utilizes a com-
mon core of simulation software, which provides for head
tracking, 3D-rendering, and timing of physics, graphics, and
haptics calculations.
The code for the magnetic levitation system controller
was collected into a separate software library, modelled on
the MLHI library provided by Butterﬂy Haptics LLC for
their haptic device. This library provides for initiating and
shutting down the haptic device, conveying feedback forces
to it, and for performing PD control of the haptic device
position.
B. Multithreading and Timing
Each of the simulations makes use of multithreading to
manage the different timing requirements of graphics, head
tracking, and physics/haptic feedback, while still provid-
ing low-latency feedback. Each of these tasks is allocated
a thread. Communication between threads is performed
through data structures stored in global memory.
In order to more easily ensure low-latency feedback, this
communication is not synchronized. As no more than one
thread writes to a given set of data, the data sets are small,
and changes to data between frames tend to be small, error
due to threading conﬂicts is imperceptible to the user.
The program begins by initializing data structures, then
launching threads for head tracking and physics. The main
thread then assumes the role of running graphics. Haptics is
initiated later by the user.
The graphics thread is managed by the freeGLUT library,
an open source alternative to the OpenGL ulility toolkit. It is
responsible for displaying the current state of the simulation
data structures, which it does at the display rate of the 3D
monitor (120 frames per second). The freeGLUT library uses
the same thread to manage keyboard input to the simulation.
Meanwhile, the head tracking thread initializes and repeat-
edly queries the NDI Polaris for the location of the user’s
head. If this position can be determined, the translation and
orientation of the user’s head is calculated and stored in

295
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
global memory. This happens at the update rate of the Polaris
sensor, which is about 10 Hz.
Finally, a third thread runs the simulation. At the begin-
ning of the program, haptics is not initiated, and so this
thread just performs physics calculations, running in a loop
at approximately the rate of the maglev controller (860 Hz).
When haptics is initiated, a transition is made from running
in a loop to running in a callback from the device controller
code. Once this has happened, the code runs at the rate of
the device controller and haptic rendering is performed in
addition to the simulation. When haptics is deactivated a
transition is made back to running in a loop.
Upon receiving a command to exit the program, each of
the threads shuts down in turn. Next, logging data, if any,
is stored, then the program exits.
C. Coordinate System Correspondance
In the colocated haptic system, the size of the display
and its location relative to the user’s eyes is known. As a
result, the apparent locations of virtual objects correspond in
a one-to-one fashion to real locations: virtual objects may
be considered as embedded in real space. For example, a
virtual ball may be thought of as being 2 cm in diameter
and located 4 cm below the display. Using information about
the location of the user and size of the display, that virtual
ball may be rendered on the display so that to the user’s eye
it appears to be 2 cm in diameter, 4 cm below the center of
the display, no matter where the user moves.
Accordingly, virtual units and coordinate systems take on
more meaning when used with the colocated system. For
simplicity, it was chosen to locate the origin of the virtual
coordinates at the center of the 3D display, with coordinate
axes aligned to the display’s edges, with units of mm.
It is interesting to note that due to the correspondence of
virtual and real locations, the simulations’ virtual coordinate
system is also a coordinate system for the real space sur-
rounding the display. Calculating the location of the user’s
head in real space also calculates the location of the user’s
head in virtual space.
D. Deformable Surface Modeling and Simulation
The deformable surface simulation was designed to
demonstrate the possibility of sophisticated haptic environ-
ments involving non-rigid contacts and complicated geom-
etry. The deformable “landscape” consists of an approxi-
mately regular mass-spring-damper lattice whose top side
varies in height according to a heightmap. The construction
of the landscape proceeds in 3 phases: calculation of the
height map, distribution of point masses, and ﬁnally, linking
neighboring point masses by springs.
The height map consists of a rectangular array of heights,
in millimeters, indexed by x and y positions. The heights are
either calculated in a pseudo-random fashion or according
X
Z
(a)
X
Z
(b)
X
Z
(c)
Figure 7: Building the lattice from a heightmap
to a simple function, such as a sine wave. This is enough to
determine the surface that the user sees.
The next step is to generate the lattice points beneath
the heightmap. To do this, a three dimensional grid is
placed over the heightmap. Each point of the grid below
the corresponding surface point is made into a point mass
of the lattice. A slice of the grid and an example heightmap
is pictured in Figs. 7a –7c. In order to avoid unusually short

296
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 8: Additional links added when surface vertices are
far apart
links (as in Fig. 7b), the masses in each column are respaced
uniformly rather than on the points of the grid (Fig. 7c).
This means horizontally adjacent vertices in the grid will
not be at exactly the same heights in space. All vertices
are assigned the same mass, except for the vertices on the
bottom and edges, which are assigned inﬁnite mass, to keep
them stationary.
The ﬁnal step is to link the lattice points together. In order
to prevent undesirable inversions of the grid, for each grid
point, each of the up to 26 horizontal, vertical, and diagonal
neighbors of the grid point are linked to it. Adjacent surface
vertices that are not already linked together are then linked
together. Finally, each of the vertices below a surface vertex
but above an adjacent surface vertex are linked to the lower
surface vertex. This can be seen in Fig. 8. All of the links
are assigned the same stiffness and damping constants.
During physics calculations, a collision detection routine
determines the force the user is applying to the point masses.
The opposite forces and corresponding torques are added
and sent to the haptics device for force feedback. Next, the
forces on the point masses due to spring compression and
damping are calculated and added to the user’s applied force.
An Eulerian integration scheme is then used to update the
velocity and position of each point mass.
VI. RESULTS
Force and position experimental data in x, y, and z direc-
tions obtained during interactive simulations are presented
in Figs. 9 and 10. The position data was measured by the
position tracking system, and the force data are calculated
by the simulations and generated by the coil array of the
magnetic levitation system in real time. The commanded
forces were shown to be within 0.1 Newtons of force
sensor measurements throughout the range of the magnetic
levitation system in [19].
The Fig. 9 plots are from a haptic peg-in-hole simulation
in which a 25 mm square peg is controlled by the haptic
instrument handle and inserted into a 27x54 mm, 10 mm
deep square hole. The Fig. 10 plots are from a deformable
simulation in which a pointed virtual instrument contacts a
deformable object, as shown in Fig. 4. For both cases, haptic
0
5
10
15
20
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Haptic Force Feedback (N) 
0
5
10
15
20
0
20
40
60
80
100
120
Time (seconds)
Peg Position (mm)
x 
x 
y
z 
y
z 
Figure 9: Interactive peg-in-hole simulation data
forces and torques are zero while the instrument is moving
freely, contact forces are approximately proportional to the
depth of contact, and haptic torques depend on each contact
force and the displacement between the contact point and
the center of the haptic instrument and simulated tool.
In the peg-in-hole simulation of Fig. 9, the peg is not in
contact with the hole or top surfaces at the 8-9 and 14-15
second intervals, the z coordinate is greater than 30, and
there is no haptic force feedback. As the peg is moved in
and out of the hole, the z position moves between 20 and
30 mm. The x position can vary between approximately 40
and 70 mm while the peg is in the hole, as the hole is more
than twice as wide as the peg in the x direction. Non-zero
x and y forces are present when the virtual peg is pushed
against any of the four sides of the virtual hole. Contact
stiffnesses are approximately 0.4 N/mm and the kinetic and
static friction coefﬁcents are 0.15 in the simulations.
For the deformable surface of Fig. 10, the probe is moved
across the surface during the 12-20 second interval, and
the surface is struck with the probe several times in the
interval from 8 to 12 seconds. The object was modeled
with millimeter-scale surface variations rather than a smooth
ﬂat surface. Therefore, this surface texture produces variable
vertical (z) forces in response to horizontal (x and y) mo-
tions of the intstrument tip. Oscillations in both the position

297
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
0
5
10
15
20
−1
−0.5
0
0.5
1
1.5
2
2.5
3
Haptic Force Feedback (N)
0
5
10
15
20
−20
0
20
40
60
80
100
120
Time (seconds)
Instrument Position (mm)
z 
x 
y
x 
y
z 
Figure 10: Interactive palpation of deformable surface data
and force data can be seen in the 12-20 second period
due to sticking and slipping of the sliding surface contact.
Overall the force plots are smoother in the deformable
surface simulation than the peg-in-hole simulation due to the
compliance and lower friction of the deformable surface.
VII. FUTURE WORK
At present, the magnetic levitation and motion tracking
aspects of our system are fully developed, but the inter-
active environments are at a preliminary stage. We plan
to reﬁne the detail and physical realism of the simulated
environments to a degree where they are useable and can
provide measureable beneﬁts in medical training tasks such
as surgery, intubation, and needle driving. User studies will
be conducted to evaluate the beneﬁts of colocated haptic and
graphical training of simulated medical procedures.
The complexity of the modelled environment and the
sophistication of the simulated dynamics can be improved
by using the graphics processor for additional numerical
computations, as a general purpose graphics processing unit
or GPGPU. NVIDIA provides the CUDA [37] programming
interface to utilize the parallel processing capabilities of the
GPGPU on the graphics cards used, however, the physical
simulation programming must be completely reformulated
to realize these beneﬁts.
One more planned improvement to be made on the system
is to reconﬁgure the system to be simpler and more compact.
The optical localizer presently in use is over 1.1 m in length
and 18 kg and must be ﬁxed at least 1.5 m from the sensed
position markers, which necessitates the use of a large rigid
frame assembled from aluminum extrusions. Compact lo-
calizer systems such as the AccuTrack from Atracsys have
speciﬁcations with comparable accuracy, update rates, and
latency as needed for stable levitation and haptic feedback,
yet are much smaller and can be mounted as close as 0.15 m
to the position markers on the handheld instrument. It may
also be possible to use electromagnetic sensing systems to
track the magnet locations [38], however, interference from
the actuator coil currents may need to be overcome to realize
sufﬁcient positioning accuracy.
VIII. CONCLUSION
Our system is the ﬁrst to combine high-ﬁdelity haptic
interaction through a magnetic levitation coil array with in-
teractive virtual environments displayed in 3D in a co-lo-
cated manner, where the handheld tool grasped by the user is
manipulated in the same tabletop space as the perceived 3D
graphics display of the simulated environment. The motion
range of the magnetic levitation haptic interface device in
both translation and rotation is well suited to human hand
motions and tabletop displays.
The operation of the system was demonstrated with 6-
DOF haptic interactive simulations with solid objects incor-
porating rigid-body dynamics and deformable surfaces. Con-
tinual increases in the the computational speed and capacities
available from standard PC hardware, combined with the
increasing availability of sophisticated graphics, modeling,
and physical simulation programming interfaces, lead to the
feasibility of sophisticated interactive medical simulations
which could be used with theis co-located haptic and graphic
interface system.
Our co-located haptic and graphic interface system is
novel in that there is no hardware between the user and
the display other than the handheld interaction instrument.
The 3D environment is displayed close to the surface of the
monitor, so there is no conﬂict between visual convergence
and focal ranges. Electromagnetic force and torque actua-
tion is used for haptic interaction rather than a motorized
linkage, providing advantages in backdriveability, precision,
and response frequency bandwidths.
We have demonstrated the feasibility and function of our
system with the basic simulation environments described.
ACKNOWLEDGMENT
This work was supported in part by National Science
Foundation grants IIS-0846172 and CNS-0551515, and by
the University of Hawaii College of Engineering.

298
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
REFERENCES
[1] P. Berkelman, S. Bozlee, and M. Miyasaka, “Interactive
dynamic simulations with co-located maglev haptic and 3D
graphic display,” in International Conference on Advances in
Computer-Human Interactions, February 2013, pp. 324–329.
[2] C. Luciano, P. Bannerjee, L. Florea, and G. Dawe, “Design
of the ImmersiveTouch: a high-performance haptic augmented
virtual reality system,” in 11th International Conference on
Human-Computer Interaction, Las Vegas, August 2005.
[3] Y. Yokokoji, R. Hollis, and T. Kanade, “WYSIWYF display:
A visual/haptic interface to virtual environment,” Presence,
vol. 8, no. 4, pp. 412–434, August 1999.
[4] P. Olsson, F. Nysjo, S. Siepel, and I. Carlbom, “Physically co-
located haptic interaction with 3d displays,” in IEEE Haptics
Symposium, Vancouver, March 2012, pp. 267–272.
[5] C. Swindells, M. Enriquez, K. MacLeand, and K. Booth, “Co-
locating haptic and graphic feedback in manual controls,”
Computer Science, University of British Columbia, Tech.
Rep. TR-2005-28, 2005.
[6] L. Lin, Y. Wang, Y. Liu, and M. Sato, “Application of
pen-based planar haptic interface in physics education,” in
International Conference on Computer-Aided Design and
Computer Graphics, September 2011, pp. 375–378.
[7] H. Noma, S. Yoshida, Y. Yanagida, and N. Tetsutani, “The
proactive desk: A new haptic display system for a digital desk
using a 2-DOF linear induction motor,” Presence: Teleoper-
ators and Virtual Environments, vol. 13, no. 2, pp. 146–163,
April 2004.
[8] D. Swapp, V. Pawar, and C. Loscos, “Interaction with haptic
feedback and co-location in virtual reality,” Presence, vol. 10,
no. 1, pp. 24–30, April 2006.
[9] G. Jansson and M. Ostrom, “The effects of co-location of
visual and haptic space on judgements of form,” in Euro-
Haptics, Munich, June 2004, pp. 516–519.
[10] R. L. Hollis and S. E. Salcudean, “Lorentz levitation technol-
ogy: a new approach to ﬁne motion robotics, teleoperation,
haptic interfaces, and vibration isolation,” in Proc. 6th Int’l
Symposium on Robotics Research, Hidden Valley, PA, Octo-
ber 1993.
[11] S. Salcudean and T. Vlaar, “On the emulation of stiff walls
and static friction with a magnetically levitated input-output
device,” in ASME IMECE, Chicago, November 1994, pp.
303–309.
[12] P. J. Berkelman, R. L. Hollis, and S. E. Salcudean, “Inter-
acting with virtual environments using a magnetic levitation
haptic interface,” in Int’l Conf. on Intelligent Robots and
Systems, Pittsburgh, August 1995.
[13] P. J. Berkelman and R. L. Hollis, “Lorentz magnetic levitation
for haptic interaction: Device design, function, and integra-
tion with simulated environments,” International Journal of
Robotics Research, vol. 9, no. 7, pp. 644–667, 2000.
[14] P. Berkelman and M. Dzadovsky, “Extending the motion
ranges of magnetic levitation for haptic interaction,” in Eu-
rohaptics Conference and Symposium on Haptic Interfaces
for Virtual Environment and Teleoperator Systems, Salt Lake
City, March 2009, pp. 517–522.
[15] T. Massie and K. Salisbury, “The PHANToM haptic interface:
A device for probing virtual objects,” in Proceedings of
the ASME Winter Annual Meeting, Symposium on Haptic
Interfaces for Virtual Environment and Teleoperator Systems,
Chicago, Illinois, November 1994.
[16] Novint Falcon User Manual, Novint Technologies Inc., 2007.
[17] S. Grange, F. Conti, P. Rouiller, P. Helmer, and C. Baur,
“Overview of the delta haptic device,” in Eurohaptics, Birm-
ingham, July 2001, pp. 164–166.
[18] P. Berkelman, “A novel coil conﬁguration to extend the
motion range of lorentz force magnetic levitation devices for
haptic interaction,” in IEEE/RSJ International Conference on
Intelligent Robots and Systems, San Diego, October 2007.
[19] P. Berkelman and M. Dzadovsky, “Magnetic levitation over
large translation and rotation ranges in all directions,”
IEEE/ASME Transactions on Mechatronics, vol. 18, no. 1,
pp. 44–52, 2013.
[20] N. J. Groom and C. P. Britcher, “A description of a laboratory
model magnetic suspension testﬁxture with large angular
capability,” in IEEE Conference on Control Applications,
Dayton, September 1992, pp. 454–459.
[21] M. B. Khamesee and E. Shameli, “Regulation technique for
a large gap magnetic ﬁeld for 3d non-contact manipulation,”
Mechatronics, vol. 15, pp. 1073–1087, 2005.
[22] D. Baraff, “Interactive simulation of solid rigid bodies,” IEEE
Computer Graphics and Applications, vol. 15, pp. 63–75,
1995.
[23] D. Baraff and A. Witkin, “Dynamic simulation of non-
penetrating ﬂexible bodies,” in Computer Graphics (Proc.
SIGGRAPH), vol. 26.
ACM, July 1992, pp. 303–308.
[24] J. Allard, S. Cotin, F. Faure, P.-J. Bensoussan, F. Poyer,
C. Duriez, H. Delingette, and L. Grisoni, “Sofa
an open
source framework for medical simulation,” in Medicine Meets
Virtual Reality (MMVR’15), Long Beach, USA, February
2007.
[25] M. Marchal, J. Allard, C. Duriez, and S. Cotin, “Towards
a framework for assessing deformable models in medical
simulation,” in International Symposium on Computational
Models for Biomedical Simulation, London, July 2008.
[26] J. Barbic and D. James, “Six-dof haptic rendering of contact
between geometrically complex reduced deformable models,”
IEEE Transactions on Haptics, preprint published online,
June 2008.
[27] D. James and D. Pai, “A uniﬁed treatment of elastostatic
and rigid contact simulation for real time haptics,” Haptics-e,
vol. 2, no. 1, 2001.

299
International Journal on Advances in Intelligent Systems, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/intelligent_systems/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[28] C. Zilles and J. Salisbury, “A constraint-based god-object
method for haptic display,” in Int’l Conf. on Intelligent Robots
and Systems, Pittsburgh, August 1995.
[29] J. Colgate, M. Stanley, and J. Brown, “Issues in the haptic
display of tool use,” in Int’l Conf. on Intelligent Robots and
Systems, Pittsburgh, August 1995.
[30] HAPI Manual, 1st ed., SenseGraphics Inc., May 2009.
[31] F. Conti, D. Morris, F. Barbagli, and C. Sewell. CHAI 3D.
Online: http://www.chai3d.org, 2006.
[32] Magnetic Levitation Haptic Interface API Reference Manual,
1st ed., Microdynamic Systems Lab, Carnegie Mellon Uni-
versity, September 2008.
[33] P. Berkelman, M. Miyasaka, and J. Anderson, “Co-located 3d
graphic and haptic display using electromagnetic levitation,”
in IEEE Haptics Symposium, Vancouver, March 2012, pp.
77–81.
[34] P. Berkelman and M. Dzadovsky, “Novel design, characteri-
zation, and control method for large motion range magnetic
levitation,” IEEE Magnetics Letters, vol. 1, January 2010.
[35] S. Wolfram, The Mathematica Book, 5th ed. Wolfram Media,
2003.
[36] O. Chubar, P. Elleaume, and J. Chavanne, “A three-
dimensional magnetostatics computer code for insertion de-
vices,” Journal of Synchrotron Radiation, vol. 5, pp. 481–484,
1998.
[37] Nvidia CUDA Compute Uniﬁed Device Architecture Program-
ming Guide 1.0, Nvidia, 2007.
[38] C. Hu, M. Li, W. Yang, R. Zhang, and M.-H. Meng, “A cubic
3-axis magnetic sensor array for wirelessly tracking magnet
position and orientation,” IEEE Sensors Journal, vol. 10,
no. 5, pp. 903–913, 2010.

