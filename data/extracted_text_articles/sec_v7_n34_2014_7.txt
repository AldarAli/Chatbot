Improving Automated Cybersecurity by Generalizing Faults
and Quantifying Patch Performance
Scott E. Friedman, David J. Musliner, and Jeffrey M. Rye
Smart Information Flow Technologies (SIFT)
Minneapolis, USA
email: {sfriedman,dmusliner,jrye}@sift.net
Abstract‚ÄîWe are developing the FUZZBUSTER system to auto-
matically identify software vulnerabilities and create adaptations
that shield or repair those vulnerabilities before attackers can
exploit them. FUZZBUSTER‚Äôs goal is to minimize the time that
vulnerabilities exist, while also preserving software functionality
as much as possible. This paper presents new adaptive cyberse-
curity tools that we have integrated into FUZZBUSTER, as well as
new metrics that FUZZBUSTER uses to assess their performance.
FUZZBUSTER‚Äôs new tools increase the efÔ¨Åciency of its diagnosis
and adaptation operations, they produce more general, accurate
adaptations, and they effectively preserve software functionality.
We present FUZZBUSTER‚Äôs analysis of 16 fault-injected command-
line binaries and six previously known bugs in the Apache
web server. We compare FUZZBUSTER‚Äôs results for different
adaptation strategies and tool settings, to characterize their
beneÔ¨Åts.
Keywords-cyber defense, adaptive security, security metrics, Ô¨Ålter
generation.
I. INTRODUCTION
Cyber-attackers constantly threaten today‚Äôs computer sys-
tems, increasing the number of intrusions every year.Firewalls,
anti-virus systems, and patch distribution systems react too
slowly to newfound ‚Äúzero-day‚Äù vulnerabilities, allowing in-
truders to wreak havoc. We are investigating ways to solve
this problem by allowing computer systems to automatically
identify their own vulnerabilities and adapt their software
to shield or repair those vulnerabilities, before attackers can
exploit them [1]. Such adaptations must balance the safety
of the system against its functionality: the safest behavior
might be to simply turn the power off or entirely disable
vulnerable applications, but that would render the systems
useless. To make a Ô¨Åner-grained balance between security and
functionality, adaptations must be:
‚Ä¢ General enough to shield the entire vulnerability (i.e., not
just blocking an overspeciÔ¨Åc set of faulting inputs).
‚Ä¢ SpeciÔ¨Åc enough to minimize the negative impact on
program functionality (e.g., by causing incorrect results
on valid inputs).
‚Ä¢ EfÔ¨Åciently-generated, to minimize the time during which
a vulnerability is present or exposed.
These considerations for adaptive cybersecurity pose several
challenges, including: how faults are discovered and diag-
nosed, with and without direct access to source code or
binaries; how adaptations are generated from the diagnoses;
how the many possible adaptations are assessed and chosen;
and how all of these operations are orchestrated for efÔ¨Åciency.
This paper describes strategies for automatically discovering
vulnerabilities, diagnosing them, and adapting programs to
shield or repair those vulnerabilities. We have implemented
these strategies within the FUZZBUSTER integrated system
for adaptive cybersecurity [2], which includes metrics [3]
and metacontrol [4] for self-adaptive software defense. FUZZ-
BUSTER uses a diverse set of custom-built and off-the-shelf
fuzz-testing tools and code analysis tools to develop protective
self-adaptations. Fuzz-testing tools Ô¨Ånd software vulnerabili-
ties by exploring millions of semi-random inputs to a program.
FUZZBUSTER also uses fuzz-testing tools to reÔ¨Åne its models
of known vulnerabilities, clarifying which types of inputs can
trigger a vulnerability. FUZZBUSTER‚Äôs behavior falls into two
general classes, as illustrated in Figure 1:
1) Proactive: FUZZBUSTER discovers novel vulnerabilities
in applications using fuzz-testing tools. FUZZBUSTER
reÔ¨Ånes its models of the vulnerabilities and then repairs
them or shields them before attackers Ô¨Ånd and exploit
them.
2) Reactive: FUZZBUSTER is notiÔ¨Åed of a fault in an ap-
plication (potentially triggered by an adversary). FUZZ-
BUSTER subsequently tries to reÔ¨Åne the vulnerability and
repair or shield it against attackers. Reactive vulnerabil-
ities pose a greater threat to the host, since these may
indicate an imminent exploit by an attacker.
	 
 
 

!! 

 
 


 

 
 
  
 
!! 
 ! 
 
 
 
	

Fig. 1.
FUZZBUSTER automatically Ô¨Ånds vulnerabilities, reÔ¨Ånes its under-
standing of their extent, and creates adaptations to shield or repair them.
FUZZBUSTER‚Äôs primary objective is to protect its host by
adapting its applications, but this may come at some cost.
For example, applying an input Ô¨Ålter or a binary patch may
create a new vulnerability, re-enable a previously-addressed
vulnerability, or otherwise negatively impact an application‚Äôs
usability by changing its expected behavior. This illustrates
121
International Journal on Advances in Security, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/security/
2014, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

a tradeoff between functionality and security, and measuring
both of these factors is important for making decisions about
adaptive cybersecurity.
We begin by outlining related work in Section II and
describing FUZZBUSTER‚Äôs process of discovering, reÔ¨Åning,
and repairing vulnerabilities in Section III. This supports
the adaptation assessment metrics described in Section IV.
We then describe FUZZBUSTER‚Äôs novel diagnosis tools for
adaptive cybersecurity in Section V and a novel fault-injection
method for generating binaries for testing in Section VI. We
evaluate FUZZBUSTER‚Äôs performance on several experiments
in Section VII on real and automatically-injected vulnerabil-
ities in production-grade software. Section VIII summarizes
our contributions and future research directions.
II. RELATED WORK
The FUZZBUSTER approach has roots in fuzz-testing, a term
Ô¨Årst coined in 1988 applied to software security analysis [5]. It
refers to invalid, random or unexpected data that is deliberately
provided as program input in order to identify defects. Fuzz-
testers‚Äî and the closely related ‚Äúfault injectors‚Äù‚Äî are good at
Ô¨Ånding buffer overÔ¨Çow, cross-site scripting, denial of service
(DoS), SQL injection, and format string bugs. They are gener-
ally not highly effective in Ô¨Ånding vulnerabilities that do not
cause program crashes, e.g., encryption Ô¨Çaws and information
disclosure vulnerabilities [6]. Moreover, existing fuzz-testing
tools tend to rely signiÔ¨Åcantly on expert user oversight, testing
reÔ¨Ånement and decision-making in responding to identiÔ¨Åed
vulnerabilities.
FUZZBUSTER is designed both to augment the power of
fuzz-testing and to address some of its key limitations. FUZZ-
BUSTER fully automates the process of identifying seeds for
fuzz-testing, guides the use of fuzz-testing to develop general
vulnerability proÔ¨Åles, and automates the synthesis of defenses
for identiÔ¨Åed vulnerabilities.
To date, several research groups have created specialized
self-adaptive systems for protecting software applications. For
example, both AWDRAT [7] and PMOP [8] used dynamically-
programmed wrappers to compare program activities against
hand-generated models, detecting attacks and blocking them
or adaptively selecting application methods to avoid damage
or compromises.
The CORTEX system [9] used a different approach, placing
a dynamically-programmed proxy in front of a replicated
database server and using active experimentation based on
learned (not hand-coded) models to diagnose new system
vulnerabilities and protect against novel attacks.
While these systems demonstrated the feasibility of the self-
adaptive, self-regenerative software concept, they are closely
tailored to speciÔ¨Åc applications and speciÔ¨Åc representations of
program behavior. FUZZBUSTER provides a general approach
to adaptive immunity that is not limited to a single class of
application. FUZZBUSTER does not require detailed system
models, but will work from high-level descriptions of com-
ponent interactions such as APIs or contracts. Furthermore,
FUZZBUSTER‚Äôs proactive use of intelligent, automatic fuzz-
testing identiÔ¨Åes possible vulnerabilities before they can be
exploited.
Other adaptive cybersecurity work focuses on white-box
program analysis instead of black-box fuzz-testing. Some of
these defenses instrument binaries to change their execution
semantics [10] or protect exploitable data [11]. Other ap-
proaches adapt binaries to add diversity at compile-time [12]
or ofÔ¨Çine [13], or at load-time [14]. These diversity-based
defenses incur comparatively lower overhead and offer statis-
tical guarantees against code-reyse exploits, but unlike FUZZ-
BUSTER, they do not protect the program from the fault itself.
III. BACKGROUND: FUZZBUSTER ADAPTIVE
CYBERSECURITY
FUZZBUSTER automatically tests and adapts multiple pro-
grams on a host machine by monitoring and adapting the
programs‚Äô input and output signals. Consequently, programs
defended by FUZZBUSTER may be compiled from any high-
level programming language or interpreted by a virtual ma-
chine, provided the program or virtual machine emits fault
signals (e.g., segmentation faults). FUZZBUSTER is designed
to accomplish three general goals:
1) Discovery: proactively Ô¨Ånd vulnerabilities within the
host‚Äôs applications.
2) ReÔ¨Ånement: produce a general, accurate, proÔ¨Åle of every
vulnerability that FUZZBUSTER encounters.
3) Adaptation: provided a reÔ¨Åned vulnerability proÔ¨Åle, cre-
ate and assess an adaptation (i.e., patch or Ô¨Ålter), and
apply it if it improves the state of the application.
FUZZBUSTER is a fully automated system, and it uses a
threat-based control strategy [4] to orchestrate its tools (see
Table I) in pursuit of these goals.
When FUZZBUSTER discovers a fault in an application‚Äî
or when it is notiÔ¨Åed of a reactive fault triggered by some
other input source‚Äî it represents the fault as an exemplar
that contains information about the system‚Äôs state when it
faulted, as shown in Figure 1. Note that FUZZBUSTER is
not responsible for fault detection; we assume that other
security and correctness mechanisms detect the fault and notify
FUZZBUSTER.
An exemplar includes information for replicating the fault,
such as environment variables and data passed as input to
the faulting application (e.g., via sockets or stdin). Some
of this data may be unrelated to the underlying vulnerability.
For instance, when FUZZBUSTER encounters a fault during
the Apache web server experiment discussed in Section VII,
it captures all environment variables (none of which are
necessary to replicate the fault), and the entire string of
network input that was sent to the application (most of which is
not necessary to replicate the fault). FUZZBUSTER uses fuzz-
testing tools to incrementally reÔ¨Åne the exemplar, trying to
characterize the minimal inputs needed to trigger the fault.
Since time and processing power is limited, FUZZBUSTER uses
a greedy meta-control strategy to orchestrate these tools [4].
122
International Journal on Advances in Security, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/security/
2014, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE I
FUZZBUSTER‚ÄôS TASKS. (* = NEW CONTRIBUTION)
Discovery actions replicate and discover vulnerabilities:
‚Ä¢ replicate-fault: Given an exemplar from the host, repli-
cate the fault under FUZZBUSTER‚Äôs control.
‚Ä¢ gen-exemplar: Generate an exemplar that might produce a
fault.
‚Ä¢ fuzz-2001: Generate random binary data and use it as input
for stdin, Ô¨Åle i/o, or command arguments [15], [16].
‚Ä¢ cross-fuzz: Use Javascript and the DOM to fuzz-test web
browsers.
‚Ä¢ wfuzz: Fuzz-test web servers with templated attacks.
‚Ä¢ retrospective-fault-analysis:
Run
faulting
test
cases through input Ô¨Ålters to generate new faulting test cases.*
ReÔ¨Ånement actions improve vulnerability proÔ¨Åles:
‚Ä¢ env-var: Identify environment variables that are necessary for
a fault.
‚Ä¢ smallify: Semi-randomly remove data from the faulting input
to Ô¨Ånd faulting substring(s).
‚Ä¢ div-con: Binary search for a smaller faulting input.
‚Ä¢ line-relev: Remove unnecessary lines from multi-line fault-
ing input.
‚Ä¢ find-regex: Compute a regular expression to capture the
faulting input.
‚Ä¢ crest: Given source code, use concolic search to Ô¨Ånd con-
straints on the faulting input [17].
‚Ä¢ replace-all-chars:
Replace
characters
to
generalize
buffer overÔ¨Çows.*
‚Ä¢ replace-delimited-chars: Replace delimited characters
to generalize embedded buffer overÔ¨Çows.*
‚Ä¢ replace-individual-chars: Replace single characters
to generalize a faulting input pattern.*
‚Ä¢ insert-chars: Insert characters to generalize regular expres-
sions.*
‚Ä¢ shorten-regex: Shorten wildcard patterns to Ô¨Ånd more
accurate buffer overÔ¨Çow thresholds.*
Adaptation actions deploy a shield or repair a vulnerability:
‚Ä¢ create-patch: Given a vulnerability proÔ¨Åle, create a patch
to Ô¨Ålter input channels and environment variables.
‚Ä¢ verify-patch: Assess a patch created by create-patch
to ensure that it outperforms a security baseline.
‚Ä¢ apply-patch: Apply a veriÔ¨Åed patch.
‚Ä¢ evolve-patch: Given source code, use GenProg [18] to
evolve a new non-faulting program source and binary.
ReÔ¨Ånement is an iterative process, where each task improves
the vulnerability proÔ¨Åle that FUZZBUSTER uses to characterize
the vulnerability. The reÔ¨Ånement process turns the initial
(often over-speciÔ¨Åc) vulnerability proÔ¨Åle into a more accurate
and general proÔ¨Åle. While reÔ¨Åning the Apache web server
vulnerabilities, FUZZBUSTER uses an environment variable
fuzzer to test and remove unnecessary environment variables
for replicating the fault, uses input fuzzers to delimit, test,
and remove/replace unnecessary network input, and thereby
develops a more accurate vulnerability proÔ¨Åle.
FUZZBUSTER has several general adaptation methods, in-
cluding input Ô¨Ålters, environment variable Ô¨Ålters, and source-
code repair and recompilation. These protect against entire
classes of exploits that may be encountered in the future.
FUZZBUSTER uses each of these by (1) constructing the adap-
tation, (2) assessing the adaptation by temporarily applying it
for test runs, and (3) applying the adaptation to the production
application if it is deemed beneÔ¨Åcial. FUZZBUSTER may apply
multiple adaptations to an application to repair a single under-
lying vulnerability. In the case of adapting the Apache web
server in Section VII, FUZZBUSTER creates input Ô¨Ålters based
on its vulnerability proÔ¨Åles: it extracts regular expressions that
characterize the pattern of faulting inputs, including necessary
character sequences (e.g., ‚ÄúCookie:‚Äù), length-dependent wild-
cards (e.g., ‚Äú.{256,}?‚Äù), and more. FUZZBUSTER then uses
these input Ô¨Ålters to identify potentially-faulting inputs and
then discard them or rectify them, based on the application
under test.
To date, our previous work on FUZZBUSTER has described
the overall defense framework and integrated tools [19], [2],
extended these tools with concolic testing [20], developed
a meta-control strategy for mission- and time-sensitive or-
chestration of proactive and reactive tools [4], and improved
the adaptation metrics [3]. This paper extends our previous
publications with (1) new strategies for representing and
generalizing vulnerabilities within program inputs, (2) new
methods for assessing the safety and functionality of program
adaptations, (3) new methods for injecting faults in production-
level binaries, and (4) empirical results of FUZZBUSTER
adaptating real programs with real vulnerabilities, illustrating
the practical beneÔ¨Åts of FUZZBUSTER‚Äôs extensions.
IV. ASSESSING ADAPTATIONS
FUZZBUSTER cannot blindly apply adaptations, since they
might have a negative impact on functionality or, even worse,
they could create new faults altogether. Thus, FUZZBUSTER
uses concrete metrics to assess the impact of candidate adapta-
tions on security and functionality. In this section, we discuss
FUZZBUSTER‚Äôs adaptation metrics, and then we describe
and evaluate two of FUZZBUSTER‚Äôs strategies for assessing
adaptations.
A. Metrics for Adaptive Cybersecurity
FUZZBUSTER‚Äôs adaptation metrics are based on test cases:
mappings from application inputs (e.g., sockets, stdin,
command-line arguments, and environment variables) to appli-
cation outputs (e.g., stdout and return code). FUZZBUSTER
automatically runs test cases to measure the safety and func-
tionality of the programs it defends. A faulting test case
terminates with an error code or its execution time exceeds a
set timeout parameter, while a non-faulting test case terminates
gracefully. FUZZBUSTER stores the following sets of test cases
for each application under its control:
1) Non-faulting test cases are test cases that were supplied
with an application for regression testing. FUZZBUSTER
tracks which of these have correct behavior (i.e., output
and return code), and which have different/incorrect
behavior, given some adaptations.
2) Faulting test cases include exemplars that caused faults
on their Ô¨Årst encounter, and other faulting test cases
encountered while reÔ¨Åning the exemplar. FUZZBUSTER
tracks which of these have been Ô¨Åxed by the adaptations
123
International Journal on Advances in Security, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/security/
2014, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

created so far, and which are still faulting. There are two
speciÔ¨Åc types of faulting test cases:
a) Reactive faulting test cases: encountered by host
notiÔ¨Åcation and subsequent reÔ¨Ånement (see Fig-
ure 1). These pose more of a threat, since the
underlying vulnerability may have been triggered
deliberately by an adversary.
b) Proactive faulting test cases: encountered by dis-
covery and reÔ¨Ånement (see Figure 1). These pose
less threat, since they were discovered internally
and FUZZBUSTER has no evidence that an adver-
sary is aware of them.
FUZZBUSTER calculates two important metrics from these
sets of test cases over time:
1) Exposure is computed as the number of unÔ¨Åxed fault-
ing test cases over time. This represents an estimated
window of exploitability.
2) Functionality loss is computed as the number of incor-
rect non-faulting test cases over time. This represents the
usability that FUZZBUSTER has sacriÔ¨Åced for the sake
of security.
Since FUZZBUSTER relies on test cases for measuring
exposure and functionality, these measurements are only as
complete as the set of faulting and non-faulting test cases,
respectively. Before FUZZBUSTER has discovered faults or
been notiÔ¨Åed of faults, there are no faulting test cases for
any application. As FUZZBUSTER encounters proactive and
reactive faults and reÔ¨Ånes those faults (e.g., by experimenting
with different inputs), it will create additional faulting and
non-faulting test cases. FUZZBUSTER accumulates these test
cases‚Äî as well as any regression tests supplied with the
application‚Äî and automatically runs them as described below
to assess potential adaptations. FUZZBUSTER then applies and
removes adaptations to Ô¨Åx the faulting test cases and restore
the behavior of non-faulting test cases. These adaptations
ultimately protect the host against adversaries.
We note that concolic testing tools (e.g., [21], [22]), includ-
ing those already integrated with FUZZBUSTER [17], [20], can
generate sets of test cases from the program‚Äôs source code;
however, since this work focuses on black-box fuzz-testing,
FUZZBUSTER uses black-box tools to generate its test cases.
B. Two Adaptation Policies
Not all of FUZZBUSTER‚Äôs adaptations improve the status
of the analyzed program: some adaptations may sacriÔ¨Åce
functionality (i.e., change the behavior of non-faulting test
cases) without improving exposure (i.e., Ô¨Åxing faulting test
cases), others may cause new faults, and still others may
have no measurable effect. Using the metrics described above,
FUZZBUSTER can apply different adaptation policies to assess
whether an adaptation should be applied to a program.
In our Ô¨Årst experiment, we compare two different adaptation
policies. We describe these policies next, and then provide
empirical results to characterize their effect on exposure and
functionality.
1) Strict policy: In strict mode, FUZZBUSTER can never
change the behavior of a non-faulting test case when adapting
a program. What‚Äôs more, all faulting test cases that correspond
to the present vulnerability must be repaired (i.e., test cases
corresponding to another vulnerability may still fault). The
strict policy thereby only allows adaptations that are com-
plete repairs and that preserve all known functionality, as
determined by the available test cases. Once an adaptation
is applied, it is never removed.
2) Relaxed policy: In relaxed mode, FUZZBUSTER may
sacriÔ¨Åce functionality to Ô¨Åx faulting test cases. The exact
balance can be tuned for different applications, but FUZZ-
BUSTER‚Äôs default priorities are:
1) Fixing reactive faulting test cases.
2) Fixing proactive faulting test cases.
3) Maintaining the behavior of non-faulting test cases.
This means that FUZZBUSTER will tolerate functionality loss
(i.e., by changing the behavior of non-faulting test cases) in
order to decrease exposure.
C. Experiment: Comparing Adaptation Policies
We conducted an experiment to compare the strict and
relaxed adaptation policies. We provided FUZZBUSTER with a
faulty version of dc, a Unix calculator program. This version
of dc causes a segmentation fault when either (1) the modulo
(%) operator is executed with at least two numbers on the
stack or (2) base conversion is attempted with at least two
numbers on the stack. Since many different input sequences
will produce the fault, we do not expect a single adaptation
to address the entire space of faults.
We also provided FUZZBUSTER 25 non-faulting test cases
for dc to seed the non-faulting test cases set. These were
gathered from examples in the dc manual and the Wikipedia
dc entry, with the modulo and base conversion test cases
removed.
Results are shown as functionality/exposure plots in Fig-
ure 2 and Figure 3. These plots display the following adaptive
cybersecurity metrics, as described above:
‚Ä¢ The number of faulting test cases FUZZBUSTER has
identiÔ¨Åed through discovery and reÔ¨Ånement (solid light
red line).
‚Ä¢ The number of those faulting test cases that FUZZBUSTER
has Ô¨Åxed (dashed light red line).
‚Ä¢ Exposure to vulnerabilities (area between light red lines),
which FUZZBUSTER should ideally minimize.
‚Ä¢ The number of non-faulting test cases FUZZBUSTER has
for the application (solid dark blue line).
‚Ä¢ The number of those non-faulting test cases whose return
code and output behavior is preserved in the patched
version (dashed dark blue line).
‚Ä¢ Loss of functionality (area between dark blue lines),
which FUZZBUSTER should ideally minimize.
‚Ä¢ The patches that have been applied.
Figure 2 shows the results of FUZZBUSTER‚Äôs strict policy.
By deÔ¨Ånition, the strict policy preserves all functionality of
124
International Journal on Advances in Security, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/security/
2014, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

the application, so we do not plot non-faulting test cases in
Figure 2.
Adaptations are numbered sequentially, starting with ‚ÄúPatch
1‚Äù and increasing with each adaptation created. Following the
patch are three numbers: (1) number of faulting test cases
created by the patch; (2) number of faulting test cases Ô¨Åxed by
the patch; and (3) number of non-faulting test cases preserved
by the patch. As shown in Figure 2, under the strict policy,
FUZZBUSTER created over 109 adaptations in 45 minutes,
but only 11 passed the strict assessment criteria and were
subsequently applied.
Figure 3 shows the results of the relaxed policy. This
is plotted in the same way as the strict results, except we
also include the dark blue non-faulting dataset. FUZZBUSTER
accumulates non-faulting test cases over time by using fuzz-
tools to generate and run test cases that do not produce faults.
 0
 50
 100
 150
 200
 250
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 1, 27)
PATCH 3 (0, 4, 29)
PATCH 5 (0, 7, 29)
PATCH 7 (0, 11, 32)
PATCH 9 (0, 14, 60)
PATCH 10 (0, 47, 60)
PATCH 11 (0, 48, 77)
PATCH 55 (0, 50, 149)
PATCH 58 (0, 56, 149)
PATCH 60 (0, 57, 198)
PATCH 109 (0, 61, 223)
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Fig. 2.
FUZZBUSTER uses the strict policy to preserve its applications‚Äô
functionality throughout the course of protective adaptation.
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
Fig. 3.
FUZZBUSTER uses the relaxed policy to sacriÔ¨Åce functionality for
the sake of improving security.
As shown in Figure 3, FUZZBUSTER incurs a loss of
functionality (gap in between dark blue lines) to reduce its
exposure to vulnerabilities. More speciÔ¨Åcally, it sacriÔ¨Åces up
to 10% of its non-faulting test cases to Ô¨Åx faulting test cases,
and it restores the behavior of erroneous non-faulting test-cases
at multiple points, e.g., around 600s and 950s.
There are several key differences between the strict and
relaxed results:
‚Ä¢ The exposure gap is much smaller in relaxed mode than
in strict mode, indicating more protection over time in
relaxed mode.
‚Ä¢ In relaxed mode, FUZZBUSTER applied 18 of 35 (51%)
created adaptations, compared to 11 of 109 (10%) in strict
mode. This means that relaxed mode wasted less time
constructing and assessing unused adaptations.
‚Ä¢ At the end of each run, relaxed mode yields
110 total
faulting test cases, and strict mode yields
245 total
faulting test cases. This is because FUZZBUSTER was
unable to apply as many adaptations in strict mode, so it
discovered more variations of similar faulting test cases
(which we veriÔ¨Åed were due to the same underlying
vulnerability).
In relaxed mode, FUZZBUSTER does not fully restore dc‚Äôs
functionality by the end of the 45 minute trial, nor does it re-
store the functionality after six hours ‚Äì it retains a 10% loss of
functionality. We next describe new fuzz-tools that help close
this margin and improve the effectiveness of FUZZBUSTER, as
measured by the above adaptive cybersecurity metrics. All of
the experiments in the remainder of this paper use the relaxed
policy for assessing and applying adaptations.
V. TOOLS FOR DISCOVERY & REFINEMENT
In this section, we describe the new tools we developed to
improve FUZZBUSTER‚Äôs discovery and reÔ¨Ånement capabilities.
For the sake of comparison, we Ô¨Årst review the set of fuzz tools
we used in previous work [2], [3], [4].
A. Previous Fuzz-Tools
FUZZBUSTER‚Äôs fuzz-tools included a random string gen-
erator for discovering faults (called Fuzz-2001) [15], [16]
and various minimization (i.e., unnecessary character removal)
tools for reÔ¨Åning faults.
Fuzz-2001 quickly constructs a sequence of printable
and non-printable characters and feeds it as input to the
program under test. This is effective for discovering some
buffer overÔ¨Çows, problems with escape characters, and other
such problems.
The minimization tools FUZZBUSTER uses to reÔ¨Åne vulner-
abilities include:
‚Ä¢ smallify: semi-randomly removes single characters from
the input string.
‚Ä¢ line-relev: semi-randomly removes entire lines from the
input string.
‚Ä¢ divide-and-conquer: Use a binary search to attempt to
remove entire portions of the input string.
Each of these tools is designed take a faulting test case as
input, and produce smaller faulting test case(s).
Minimization tools can operate in a black-box fashion,
where FUZZBUSTER does not have the source code or even
access to the binary, since they only require an output signal
to determine whether the program faulted.
125
International Journal on Advances in Security, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/security/
2014, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

B. New Fuzz-Tools
We now discuss several new tools that we have incorporated
into FUZZBUSTER for discovering and reÔ¨Åning faults. We
then present empirical results comparing the new and existing
tools to characterize the effects on the host‚Äôs exposure to
vulnerabilities.
These tools work with input Ô¨Ålter adaptations; that is,
program adaptations that remove content from input data
before passing the data to the original program.
1) Retrospective Fault Analysis:
We implemented and
tested Retrospective Fault Analysis (RFA), a new tool for
vulnerability discovery. RFA works by Ô¨Ånding the most recent
faulting test case such that:
‚Ä¢ The test case‚Äôs input is Ô¨Åltered by the most recent
adaptation applied, so some input data has been removed.
‚Ä¢ The test case still faults, despite its input being Ô¨Åltered.
RFA then uses the test case‚Äî with Ô¨Åltered input‚Äî as an
exemplar. This effectively allows FUZZBUSTER to Ô¨Åx test
cases that still fault, despite incremental adaptations.
To illustrate why this is important, consider the following
simpliÔ¨Åed example, where a program faults if it receives either
CRASH or fault in an incoming message. Some messages
may have more than one fault within them, e.g.:
‚Ä¢ Cookie: foo=...CRASH...fault...
‚Ä¢ Cookie: foo=...faCRASHult...
This means that FUZZBUSTER can automatically build a
Ô¨Ålter adaptation to address CRASH, but in both of the above
cases, there will still be a fault. Using RFA, FUZZBUSTER
will follow its CRASH adaptation with a retrospective investi-
gation of the remaining fault test case(s). This produces a
more complete analysis of problematic inputs, and it reduces
the host‚Äôs exposure to vulnerabilities, as we demonstrate in the
experiments in Section VII.
2) Input Generalization Tools: As described above, min-
imization tools remove unnecessary characters for a fault.
Unfortunately, reÔ¨Åning vulnerabilities based on removal alone
will tend to produce overspeciÔ¨Åc adaptations.
Consider the example of IP addresses within a packet
header: minimization tools might trim 192.168.0.1 to 2.8.0.1,
which might still produce the fault; however, an adaptation
based on this model will only be effective when 2, 8, 0, and
1 are all present in the address.
FUZZBUSTER‚Äôs new generalization tools go the extra step
of replacing characters and inserting characters to generalize
FUZZBUSTER‚Äôs regular expression model of the faulting input
pattern. This means that FUZZBUSTER will be able to substi-
tute the IP address‚Äô digits with other digits to develop a more
general, accurate adaptation.
We have implemented the following generalization tools:
‚Ä¢ replace-all-chars: replaces all characters with dif-
ferent characters, reruns the test case, and then general-
izes. This helps determine whether the test case is an
instance of a buffer overÔ¨Çow. For example:
ABCDEFGH ==> .{8,}
‚Ä¢ replace-delimited-chars: splits the input into
chunks, using common delimiters, removes and replaces
delimited chunks, and then generalizes. For example:
host: 1.1.1.1\nCookie ==> .{0,}?Cookie
‚Ä¢ replace-individual-chars:
removes
and
re-
places individual characters, sensitive to character classes
(e.g., letters, digits, whitespace, etc.), and generalizes. For
example:
GCOJR34A59S94H ==> .*C.*R.*A.*S.*H
‚Ä¢ insert-chars: inserts characters in-between consec-
utive concrete characters, to test and relax adjacency
constraints. For example:
CRASH ==> .*C.*R.*A.*S.*H
‚Ä¢ shorten-regex:
reduces
character
counts
within
wildcard blocks to provide more accurate buffer overÔ¨Çow
thresholds. For example:
host: .{951,} ==> host: .{256,}
We conducted experiments on multiple programs to charac-
terize the effect of generalization tools and RFA. We discuss
these experiments and results next.
VI. EVOLVING FAULTY BINARIES FOR TESTING
An adaptive cybersecurity evaluation requires a set of
programs to adapt and protect. Adapting production-grade
software against known Common Vulnerabilities and Expo-
sures (CVEs) is important for demonstrating realism (see
Section VII-C); however, these CVEs do not always cover
interesting spaces of program inputs for sensitivity analyses.
Conversely, hand-injecting faults into the source code of
production-grade software allows us to cover interesting input
spaces; however, we aim to avoid hand-tailoring our dataset
wherever possible.
Given these considerations, we designed and implemented a
system to automatically inject faults into existing production-
quality binaries using evolutionary programming. Our fault-
injection approach takes the following inputs:
‚Ä¢ The C source code of the application.
‚Ä¢ A set of non-faulting test cases, where each test case is
labeled positive or negative, and there is at least one test
case with each label.
Given these inputs, our fault-injector wraps the positive test
cases with shell code to return normally (i.e., 0) if there is
no error (i.e., [$? -lt 128 ]), and then modiÔ¨Åes negative
test cases to return abnormally (i.e., 1) if there is no error.
This transformation modiÔ¨Åes the test cases to expect an error
for any test cases labeled negative.
Our fault-injector then uses the application source code
and transformed test cases as input to GenProg [23], an
evolutionary program repair tool. In its normal mode of
operation, GenProg generates variants of the program and uses
the supplied test cases as a Ô¨Åtness function to select variants for
the next round of mutation. By transforming the supplied test
cases to expect failure from a subset of working test cases,
we effectively make GenProg inject faults someplace in the
126
International Journal on Advances in Security, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/security/
2014, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

 0
 20
 40
 60
 80
 100
 120
 140
 160
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 1, 27)
PATCH 2 (0, 7, 27)
PATCH 3 (0, 14, 27)
PATCH 4 (0, 21, 27)
PATCH 5 (0, 21, 27)
PATCH 6 (0, 22, 27)
PATCH 7 (0, 29, 27)
PATCH 8 (0, 29, 27)
PATCH 9 (0, 39, 27)
PATCH 10 (0, 39, 27)
PATCH 11 (0, 39, 27)
PATCH 12 (0, 46, 27)
PATCH 13 (0, 46, 27)
PATCH 14 (0, 52, 27)
PATCH 15 (0, 58, 27)
PATCH 16 (0, 67, 27)
PATCH 17 (0, 74, 27)
PATCH 18 (0, 76, 27)
PATCH 19 (0, 76, 27)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
 0
 50
 100
 150
 200
 250
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 13, 27)
PATCH 2 (0, 20, 27)
PATCH 3 (0, 29, 27)
PATCH 4 (0, 36, 27)
PATCH 5 (0, 48, 27)
PATCH 6 (0, 56, 27)
PATCH 7 (0, 68, 27)
PATCH 8 (0, 77, 27)
PATCH 9 (0, 87, 27)
PATCH 10 (0, 91, 27)
PATCH 11 (0, 97, 27)
PATCH 12 (0, 111, 27)
PATCH 13 (0, 116, 27)
PATCH 14 (0, 124, 27)
PATCH 15 (0, 135, 27)
PATCH 16 (0, 146, 27)
PATCH 17 (0, 151, 27)
PATCH 18 (0, 158, 27)
PATCH 19 (0, 172, 27)
PATCH 20 (0, 184, 27)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 4, 27)
PATCH 2 (0, 11, 27)
PATCH 3 (0, 17, 27)
PATCH 4 (0, 25, 27)
PATCH 5 (0, 32, 27)
PATCH 6 (0, 39, 27)
PATCH 7 (0, 39, 27)
PATCH 8 (0, 54, 27)
PATCH 9 (0, 62, 27)
PATCH 10 (0, 62, 27)
PATCH 11 (0, 72, 27)
PATCH 12 (0, 85, 27)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
 0
 50
 100
 150
 200
 250
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 10, 27)
PATCH 2 (0, 26, 27)
PATCH 3 (0, 37, 27)
PATCH 4 (0, 45, 27)
PATCH 5 (0, 57, 27)
PATCH 6 (0, 65, 27)
PATCH 7 (0, 78, 27)
PATCH 8 (0, 87, 27)
PATCH 9 (0, 94, 27)
PATCH 10 (0, 102, 27)
PATCH 11 (0, 108, 27)
PATCH 12 (0, 121, 27)
PATCH 13 (0, 127, 27)
PATCH 14 (0, 139, 27)
PATCH 15 (0, 148, 27)
PATCH 16 (0, 158, 27)
PATCH 17 (0, 169, 27)
PATCH 18 (0, 178, 27)
PATCH 19 (0, 191, 27)
PATCH 20 (0, 195, 27)
PATCH 21 (0, 201, 27)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
Minimization Fuzz-Tools
Generalization Fuzz-Tools
Without RFA
With RFA
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
Fig. 4.
Results comparing the exposure window of Retrospective Fault Analysis and minimization vs. generalization tools.
code to produce faults for the negatively-labeled cases, and
still retain a working program for the positively-labeled cases.
Importantly, evolutionary fault-injection does not produce
faults that are limited to the negatively-labeled test cases.
Consider the case of the faulty dc used in Section IV-C:
we supplied a negative test case that used the modulo (%)
operation, and a dozen positive test cases that use other
arithmetic operations. Evolutionary programming injected a
null pointer dereference within an arithmetic helper function
used for computing remainders, converting bases, and printing
output with a non-decimal radix. Further, for all of these
operations to fault, the internal calculator stack had to be non-
empty. This demonstrates that evolutionary fault-injection will
(1) produce non-trivial faults from simple sets of working test
cases and (2) inject faults at arbitrary locations of the program,
provided they produce a fault on the given input(s).
We used this automatic fault-injection method to create
faulty binaries from the source code of unix command-line
applications including dc, fold, uniq, and wc. Some of the
fault-injected application variants faulted on the vast majority
of possible inputs, and gracefully-terminating inputs were very
rare. We discarded these variants since they lacked a stable,
non-faulting operating mode.
We describe FUZZBUSTER‚Äôs results adapting 16 fault-
injected applications in Section VII-D.
VII. FUZZ-TOOL EXPERIMENTS
We conducted an empirical evaluation on different programs
to measure the effect of RFA and the new generalization fuzz-
tools. We divide this into four discussions: (1) a compara-
tive analysis of minimization, generalization, and RFA on a
single program; (2) an example of FUZZBUSTER sacriÔ¨Åcing
functionality in order to increase security; (3) a quantitative
comparison of minimization and generalization using FUZZ-
BUSTER to shield a web server against known vulnerabilities;
and (4) adaptation statistics across multiple programs using
FUZZBUSTER with generalization and RFA.
A. Comparative Analysis: Generalization, Minimization, RFA
For this experiment, we used the same fault-injected version
of dc as in Section IV-C, with a faulty modulo operation. We
ran FUZZBUSTER in Ô¨Åve settings: with and without RFA, with
either minimization or generalization tools (Figure 4); and then
with RFA using both minimization and generalization tools
(Figure 5).
The comparison plots in Figure 4 illustrate the tradeoffs
of generalization and RFA. Minimization tools (Figure 4, left)
quickly produce overspeciÔ¨Åc patches. For instance, PATCH 16
in Figure 4 upper-left plot Ô¨Ålters the pattern .*9.*5.*%.*.
While this is a legitimate example of the fault, it does not
characterize the fault in its entirety. By comparison, the
generalization patches are slightly more general: PATCH 6
127
International Journal on Advances in Security, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/security/
2014, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

in the Figure 4 upper-right plot Ô¨Ålters the pattern .*d.*%.*
(where d duplicates the value on the stack).
Figure 4 also illustrates the effect of retrospective fault
analysis. In the RFA trials, the exposure (the area between
the red lines) is signiÔ¨Åcantly reduced. This is because FUZZ-
BUSTER often deploys a Ô¨Ålter that addresses some ‚Äì but not
all ‚Äì problems in a faulting input, and then RFA allows
FUZZBUSTER to focus on the remainder of the problematic
input. For instance, if a single test case has both a modulo
operation and a base conversion, Ô¨Åltering out only one of these
operations will not repair the test case.
In the setting with both generalization and RFA, FUZZ-
BUSTER Ô¨Ålters against the entire vulnerability within 15 min-
utes; in the other cases, FUZZBUSTER does not level off for
over three hours.
Note that in all settings in Figure 4, FUZZBUSTER did not
lose functionality of the underlying application, as measured
by the correctness of the non-faulting test cases.
Figure 5 shows the results of FUZZBUSTER with both
minimization and generalization enabled. It Ô¨Åxes the entire
vulnerability and levels off in 18 minutes. Compared to the
same condition with minimization disabled (Figure 4, lower
right), enabling minimization made FUZZBUSTER spend some
unnecessary time attempting to shorten faulting inputs when
its generalization tools can Ô¨Ånd (or have already found) a more
general representation of a faulting input pattern.
Also note that the minimization and generalization condition
(Figure 5) destroys the functionality of one of the non-faulting
test cases with its PATCH 5. The overgeneral PATCH 5
Ô¨Ålters out all occurrences of z (push the stack depth onto the
calculator stack), which was indeed a character of a faulting
input, but when FUZZBUSTER attempted to build a regular
expression over its minimization and generalization results,
the expression was overgeneral.
These results suggest that FUZZBUSTER‚Äôs new input-
generalizing tools are a suitable replacement for its input-
minimizing tools.
 0
 20
 40
 60
 80
 100
 120
 140
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 2000
# Faulting Test Cases
Time (s)
PATCH 1 (0, 17, 27)
PATCH 2 (0, 34, 27)
PATCH 3 (0, 54, 27)
PATCH 4 (0, 80, 27)
PATCH 5 (0, 92, 27)
PATCH 6 (0, 105, 27)
PATCH 7 (0, 120, 27)
PATCH 8 (0, 139, 27)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
Fig. 5.
Results using RFA, minimization, and generalization.
B. SacriÔ¨Åcing Functionality to Increase Security
We ran another FUZZBUSTER trial on a different fault-
injected version of the dc binary. This version faulted when-
ever an arithmetic operation is invoked on an empty stack,
so for instance, the sequence ‚Äò‚Äò9 5 +‚Äô‚Äô would not fault,
but the inputs ‚Äò‚Äò+‚Äô‚Äô or ‚Äò‚Äò4 n +‚Äô‚Äô would fault due to an
empty stack (‚Äò‚Äòn‚Äô‚Äô pops the stack).
The results are shown in Figure 6. Using generalization
tools and RFA, FUZZBUSTER isolates individual arithmetic
operations and generates Ô¨Ålters for each, ultimately disabling
its arithmetic operations to prevent any faults. Note that
almost every adaptation has an adverse impact on program
functionality, but by design, these are acceptable losses to
increase safety of the host.
TABLE II
FUZZBUSTER‚ÄôS REACTION TIME ON CVES OF THE APACHE WEB SERVER.
CVE
RT (Min.)
RT (Gen.)
Speedup
2011-3192
96
4
24x
2011-3368-1
53
10
5x
2011-3368-2
32
10
3x
2011-3368-3
77
11
7x
2012-0021
36
3
12x
2012-0053
30
7
4x
Reaction times reported in minutes; speedup reported as quotient.
C. Adapting a Web Server
We conducted FUZZBUSTER experiments on known Com-
mon Vulnerabilities and Exposures (CVEs) on the Apache web
server. This demonstrates FUZZBUSTER working on larger
production-quality applications with real vulnerabilities, and
it shows the generality of FUZZBUSTER and its fuzz-tools.
For each trial, we initialized FUZZBUSTER with the Apache
web server as the only application under test. We then sent
a faulting message to the server‚Äî as dictated by the corre-
sponding CVE‚Äî and FUZZBUSTER detected the reactive fault
and began its fuzzing. Table II reports how many minutes
FUZZBUSTER took to produce an input Ô¨Ålter adaptation (from
experiment start to patch time) for the corresponding CVE us-
ing only minimization tools (i.e., ‚ÄúMin.‚Äù), only generalization
tools (i.e., ‚ÄúGen.‚Äù), and the speedup provided by generalization
tools.
In addition to producing more general patches, the gener-
alization tools also yield a signiÔ¨Åcant speedup factor between
3x and 24x, and on average, produce useful adaptations in an
order of magnitude less time.
For these CVE trials, RFA was not necessary since FUZZ-
BUSTER Ô¨Åxes all faulting test cases with the Ô¨Årst patch it
produces.
D. Statistics Across Programs
We now present additional results from using FUZZBUSTER
with the generalization tools and retrospective fault analysis on
16 fault-injected binaries created using the process described
in Section VI.
128
International Journal on Advances in Security, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/security/
2014, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 1000
 1100
# Faulting Test Cases
Time (s)
PATCH 1 (6, 0, 30)
PATCH 2 (6, 0, 30)
PATCH 3 (6, 0, 30)
PATCH 4 (6, 1, 30)
PATCH 5 (6, 10, 30)
PATCH 6 (6, 10, 30)
PATCH 7 (6, 40, 30)
PATCH 8 (6, 40, 30)
PATCH 9 (6, 40, 30)
PATCH 10 (6, 62, 30)
PATCH 11 (6, 62, 30)
PATCH 12 (6, 78, 30)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
Disable
addition
Disable
division
Disable
subtraction
Disable
modulo
Disable
exponentiation
Disable
multiplication
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
 0
 20
 40
 60
 80
 100
 120
 0
 500
 1000
 1500
 2000
 2500
 3000
# Faulting Test Cases
Time (s)
PATCH 3 (0, 1, 27)
PATCH 4 (0, 12, 27)
PATCH 7 (0, 15, 29)
PATCH 8 (0, 24, 29)
PATCH 13 (0, 26, 29)
PATCH 15 (0, 36, 29)
PATCH 18 (0, 39, 29)
PATCH 19 (0, 47, 29)
PATCH 22 (0, 50, 29)
PATCH 23 (0, 57, 29)
PATCH 26 (0, 58, 41)
PATCH 27 (0, 67, 41)
PATCH 30 (0, 68, 54)
PATCH 31 (0, 76, 54)
PATCH 32 (0, 77, 57)
PATCH 33 (0, 84, 57)
PATCH 34 (0, 86, 66)
PATCH 35 (0, 99, 66)
Reference Test Cases
Reference Test Cases Preserved
Faulting Test Cases Fixed
Faulting Test Cases Found
# Test Cases 
Non$faul)ng+Test+Cases+Preserved+
Non$faul)ng+Test+Cases+
Fig. 6.
FUZZBUSTER sacriÔ¨Åces functionality to protect the program against vulnerabilities.
FUZZBUSTER automatically analyzed each faulty binary
for two hours, using a mix of proactive fuzz-tools (e.g.,
Fuzz-2001 and RFA), reÔ¨Ånement fuzz tools (e.g., the gen-
eralization fuzz-tools), and adaptation strategies (e.g., input
Ô¨Ålters).
Fuzzing leveled off (i.e., FUZZBUSTER patched the entire
injected fault, based on our manual analysis of patches) on
10/16 binaries. Of these leveled-off binaries, FUZZBUSTER
took an average of 5.87 minutes to level off, and it sacriÔ¨Åced
an average of 6% functionality (i.e., by changing the output
of non-faulting test cases). FUZZBUSTER retained full func-
tionality on 7 of the 10 leveled-off binaries.
Over all 16 fault-injected binaries, FUZZBUSTER created
an average of 8.2 adaptations and applied an average of 7.8,
which amounts to a 95% usage of the adaptations it created.
Over all binaries, FUZZBUSTER Ô¨Åxed an average of 82%
of the faulting test cases and sacriÔ¨Åced an average of 10%
functionality during each 2-hour trial. This suggests that when
FUZZBUSTER cannot generate a perfect adaptation, it still
manages to close the exposure window over time.
VIII. CONCLUSIONS AND FUTURE WORK
FUZZBUSTER is designed to discover vulnerabilities and
then quickly reÔ¨Åne and adapt its applications to prevent them
from being exploited by attackers. This paper uses FUZZ-
BUSTER as a research tool to further the state-of-the-art in
measuring and improving adaptive cybersecurity. We presented
useful exposure and functionality metrics, we used these met-
rics to compare and evaluate FUZZBUSTER‚Äôs self-adaptation
policies, and we used the metrics to characterize the beneÔ¨Åts
FUZZBUSTER‚Äôs new fuzz-testing tools‚Äî retrospective fault
analysis and input generalization‚Äî aimed at improving the
quality and efÔ¨Åciency of adaptive cybersecurity. Further, we
described how to automatically inject faults into production-
grade software to build datasets for adaptive cybersecurity
experimentation.
We presented empirical results of FUZZBUSTER‚Äôs auto-
mated analysis of both fault-injected programs and real CVEs,
comparing the vulnerability exposure, functional loss, and
reaction time. When analyzing fault-injected programs, the
generalization fuzz-tools and RFA reduced vulnerability ex-
posure by a factor of Ô¨Åve on fault injected programs, and
allowed FUZZBUSTER to shield more of the vulnerability
in less time. When analyzing the Apache HTTP server, the
new fault generalization tools yielded an order of magnitude
speedup in reaction time over the previous fault minimization
tools.
Currently, FUZZBUSTER uses a wrapper around the pro-
129
International Journal on Advances in Security, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/security/
2014, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

grams it controls, and its wrapper Ô¨Ålters all incoming data
according to the current adaptations (e.g., input Ô¨Ålters) before
sending the data to the binary. One next step is to revise
the program‚Äôs binary directly, and embed the input Ô¨Ålters as
preprocessors.
The generalization fuzz-tools and RFA are all domain-
independent strategies, and we demonstrated this by using
them to improve program analysis on command-line Ô¨Ålter
programs (e.g., wc), state-dependent standard input programs
(e.g., dc), and grammar-speciÔ¨Åc web programs (e.g., Apache
HTTP server). The most domain-speciÔ¨Åc enhancement is the
replace-delimited-chars tool that uses common de-
limiters to analyze portions of data. This tool contributed sig-
niÔ¨Åcantly to the speedup of FUZZBUSTER‚Äôs analysis of HTTP
headers in the Apache HTTP server experiment. We believe
that we will see additional performance beneÔ¨Åts by adding
more domain-speciÔ¨Åc knowledge to FUZZBUSTER, including
input grammars (e.g., packet header structure) and deeper
application models (e.g., recording application command-line
options and values).
Our fault-injector currently requires source code in order to
use GenProg [23]. We are investigating fault-injection using
evolutionary programming methods that operate directly on
assembly code or compiled binaries (e.g., [24]), and do not
require source code. This has the advantages of (1) being high-
level-language-independent and (2) producing more diverse
vulnerabilities at the binary or library level.
We anticipate using the adaptive cybersecurity metrics from
this paper to evaluate future design decisions for FUZZBUSTER
and other adaptive cybersecurity projects.
ACKNOWLEDGMENTS
This work was supported by The Defense Advanced Re-
search Projects Agency (DARPA) and Air Force Research
Laboratory under contract FA8650-10-C-7087.
The views
expressed are those of the author and do not reÔ¨Çect the
ofÔ¨Åcial policy or position of the Department of Defense or the
U.S. Government.
Approved for public release, distribution
unlimited.
REFERENCES
[1] D. J. Musliner, S. E. Friedman, and J. M. Rye, ‚ÄúAutomated fault
analysis and Ô¨Ålter generation for adaptive cybersecurity,‚Äù in Proceedings
of ADAPTIVE 2014: The Sixth International Conference on Adaptive
and Self-Adaptive Systems and Applications, June 2014, pp. 56‚Äì62.
[2] D. J. Musliner, J. M. Rye, D. Thomsen, D. D. McDonald, and M. H.
Burstein, ‚ÄúFUZZBUSTER: A system for self-adaptive immunity from
cyber threats,‚Äù in Eighth International Conference on Autonomic and
Autonomous Systems (ICAS-12), March 2012, pp. 118‚Äì123.
[3] D. J. Musliner et al., ‚ÄúSelf-adaptation metrics for active cybersecurity,‚Äù
in SASO-13: Adaptive Host and Network Security Workshop at the
Seventh IEEE International Conference on Self-Adaptive and Self-
Organizing Systems, September 2013, pp. 53‚Äì58.
[4] D. J. Musliner, S. E. Friedman, J. M. Rye, and T. Marble, ‚ÄúMeta-control
for adaptive cybersecurity in FUZZBUSTER,‚Äù in Proc. IEEE Int‚Äôl Conf.
on Self-Adaptive and Self-Organizing Systems, September 2013, pp.
219‚Äì226.
[5] B. Miller, L. Fredriksen, and B. So, ‚ÄúAn empirical study of the reliability
of unix utilities,‚Äù Communications of the ACM, vol. 33, no. 12,
December 1990.
[6] C. Anley, J. Heasman, F. Linder, and G. Richarte, The Shellcoder‚Äôs
Handbook: Discovering and Exploiting Security Holes, 2nd Ed.
John
Wiley & Sons, 2007, ch. The art of fuzzing.
[7] H. Shrobe et al., ‚ÄúAWDRAT: a cognitive middleware system for infor-
mation survivability,‚Äù AI Magazine, vol. 28, no. 3, 2007, p. 73.
[8] H. Shrobe, R. Laddaga, B. Balzer et al., ‚ÄúSelf-Adaptive systems for
information survivability: PMOP and AWDRAT,‚Äù in Proc. First Int‚Äôl
Conf. on Self-Adaptive and Self-Organizing Systems, 2007, pp. 332‚Äì
335.
[9] ‚ÄúCortex: Mission-aware cognitive self-regeneration technology,‚Äù Final
Report, US Air Force Research Laboratories Contract Number FA8750-
04-C-0253, March 2006.
[10] J. Hiser, A. Nguyen-Tuong, M. Co, M. Hall, and J. W. Davidson, ‚ÄúIlr:
Where‚Äôd my gadgets go?‚Äù in Security and Privacy (SP), 2012 IEEE
Symposium on.
IEEE, 2012, pp. 571‚Äì585.
[11] L. Davi, A.-R. Sadeghi, and M. Winandy, ‚ÄúRopdefender: A detection
tool to defend against return-oriented programming attacks,‚Äù in Pro-
ceedings of the 6th ACM Symposium on Information, Computer and
Communications Security.
ACM, 2011, pp. 40‚Äì51.
[12] M. Franz, ‚ÄúE unibus pluram: massive-scale software diversity as a
defense mechanism,‚Äù in Proceedings of the 2010 workshop on New
security paradigms.
ACM, 2010, pp. 7‚Äì16.
[13] V. Pappas, M. Polychronakis, and A. D. Keromytis, ‚ÄúSmashing the
gadgets: Hindering return-oriented programming using in-place code
randomization,‚Äù in Security and Privacy (SP), 2012 IEEE Symposium
on.
IEEE, 2012, pp. 601‚Äì615.
[14] R. Wartell, V. Mohan, K. W. Hamlen, and Z. Lin, ‚ÄúBinary stirring:
Self-randomizing instruction addresses of legacy x86 binary code,‚Äù in
Proceedings of the 2012 ACM conference on Computer and communi-
cations security.
ACM, 2012, pp. 157‚Äì168.
[15] B. Miller, L. Fredriksen, and B. So, ‚ÄúAn Empirical Study of the
Reliability of UNIX Utilities,‚Äù Communications of the ACM, vol. 33,
no. 12, 1990, pp. 32‚Äì44.
[16] B. Miller, G. Cooksey, and F. Moore, ‚ÄúAn Empirical Study of the Ro-
bustness of MacOS Applications Using Random Testing,‚Äù in Proceedings
of the 1st international workshop on Random testing.
ACM, 2006, pp.
46‚Äì54.
[17] J. Burnim and K. Sen, ‚ÄúHeuristics for scalable dynamic test generation,‚Äù
in Proceedings of the 2008 23rd IEEE/ACM International Conference
on Automated Software Engineering, ser. ASE ‚Äô08.
Washington, DC,
USA: IEEE Computer Society, 2008, pp. 443‚Äì446. [Online]. Available:
http://dx.doi.org/10.1109/ASE.2008.69
[18] W. Weimer, S. Forrest, C. Le Goues, and T. Nguyen, ‚ÄúAutomatic
program repair with evolutionary computation,‚Äù Communications of the
ACM, vol. 53, no. 5, May 2010, pp. 109‚Äì116.
[19] D. J. Musliner, J. M. Rye, D. Thomsen, D. D. McDonald, and M. H.
Burstein, ‚ÄúFUZZBUSTER: Towards adaptive immunity from cyber
threats,‚Äù in 1st Awareness Workshop at the Fifth IEEE International
Conference on Self-Adaptive and Self-Organizing Systems, October
2011, pp. 137‚Äì140.
[20] D. J. Musliner, J. M. Rye, and T. Marble, ‚ÄúUsing concolic testing to
reÔ¨Åne vulnerability proÔ¨Åles in FUZZBUSTER,‚Äù in SASO-12: Adaptive
Host and Network Security Workshop at the Sixth IEEE International
Conference on Self-Adaptive and Self-Organizing Systems, September
2012, pp. 9‚Äì14.
[21] P. Godefroid, M. Levin, and D. Molnar, ‚ÄúAutomated Whitebox Fuzz
Testing,‚Äù in Proceedings of the Network and Distributed System Security
Symposium, 2008, pp. 151‚Äì166.
[22] C. Cadar, D. Dunbar, and D. Engler, ‚ÄúKLEE: Unassisted and Automatic
Generation of High-coverage Tests for Complex Systems Programs,‚Äù
in Proceedings of the 8th USENIX conference on Operating systems
design and implementation.
USENIX Association, 2008, pp. 209‚Äì224.
[23] W. Weimer, T. Nguyen, C. L. Goues, and S. Forrest, ‚ÄúAutomatically
Ô¨Ånding patches using genetic programming,‚Äù Software Engineering,
International Conference on, 2009, pp. 364‚Äì374.
[24] E. Schulte, S. Forrest, and W. Weimer, ‚ÄúAutomated program re-
pair through the evolution of assembly code,‚Äù in Proceedings of the
IEEE/ACM international conference on Automated software engineer-
ing.
ACM, 2010, pp. 313‚Äì316.
130
International Journal on Advances in Security, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/security/
2014, ¬© Copyright by authors, Published under agreement with IARIA - www.iaria.org

