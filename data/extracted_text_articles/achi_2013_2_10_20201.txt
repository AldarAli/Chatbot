A Deported View Concept for Touch Interaction 
Alexandre Alapetite, 
Henning Boje Andersen 
Department of Management 
Engineering, 
Technical University of Denmark 
Produktionstorvet 424, 
DK-2800 Kongens Lyngby, Denmark 
{alal, hebq}@dtu.dk 
Rune Fogh 
 
Centre for Playware, 
Department of Electrical Engineering, 
Technical University of Denmark 
Elektrovej 325, 
DK-2800 Kongens Lyngby, Denmark 
{rufo}@dtu.dk 
Ali Gürcan Özkil 
 
Department of Mechanical 
Engineering, 
Technical University of Denmark 
Produktionstorvet 426, 
DK-2800 Kongens Lyngby, Denmark 
{alio}@dtu.dk 
 
 
Abstract—Following the paradigm shift where physical 
controls are replaced by touch-enabled surfaces, we report on 
an experimental evaluation of a user interface concept that 
allows touchscreen-based panels to be manipulated partially 
blindly (aircrafts, cars). The proposed multi-touch interaction 
strategy – involving visual front-view feedback to the user 
from a copy of the peripheral panel being manipulated – 
compares 
favourably 
against 
trackballs 
or 
head-down 
interactions. 
Keywords-HCI; Tactile interaction; Touch; Blind; Visual 
attention; Cockpit; In-vehicle systems 
I. 
INTRODUCTION 
There is a trend to replace physical controls (buttons, 
dials, switches, etc.) by touch-enabled surfaces with adaptive 
layout. Doing so provides some advantages but also some 
drawbacks such as the loss of convenient blind activation, 
where users “feel” controls with their fingers. To explore this 
problem, we have made a study of a “deported view” mode 
of interaction with touchscreens and we report the results of 
an experimental evaluation of the concept. This mode of 
touchscreen interaction can be particularly useful in aviation 
and automotive applications in which pilots and drivers may 
be required to maintain a head-up position for safety reasons. 
 
Figure 1. The “deported view” concept in a cockpit. 
Figure 1 illustrates the proposed concept: When a user 
must operate (blindly) a control panel that lies outside his or 
her natural line of sight, the display area (being touched) will 
be temporally duplicated on a control screen area in front of 
the user. Simultaneously, a pointer is shown on the front 
display indicating the position of the user’s finger. By doing 
so, the lower touch-display panel becomes an indirect 
pointing device somewhat similar to that of a track-pad. 
II. 
STATE OF THE ART 
The idea of implementing touch-based interaction in 
cockpits dates back to the “Super Cockpit” (US Air Force 
1986) which contained a touch-enabled display system to 
provide spatial awareness to the pilot in all directions and in 
three dimensions. Furness investigated the system and the 
challenges it poses to the human factors community [1], 
focusing on how should information be rendered in the 
“focal” versus “ambient” visual areas of the display? 
NASA also investigated touchscreen input concepts for 
interaction with a large screen cockpit display as early as 
1990 [2], comparing three input methods: thumball, thumb 
switch and touchscreen, to interact with a large, multi-
window, “whole-flight-deck” display. While the thumball 
concept outperformed the others in simulator tests, it is 
acknowledged that touch entry would be useful in transport 
environment, dependent on error-free operation. Pilots 
emphasized the importance of positioning the touchscreen, 
suggesting that it should be placed so close to the throttle 
controls that the operator need not reach forward. 
Touchscreens were then put into operations, first in 
military aircrafts such as the Dassault Rafale in 1991 [3]. 
Since 2009, Garmin [4] and others offer similar touch-based 
panels for commercial aviation. Kaminani [5] identified two 
unique interaction issues with touchscreen in cockpits, 
namely touch activation and accidental touches, and fatigue 
due to extending arms to touch and reach. 
Similar work has also been done for automotive 
applications. Young et al. [6] research experiments on using 
touch interfaces for music selection in cars show how the 
participants’ driving performance decreases and the amount 
of time drivers have their eyes off the road increases. A 
multi-touch interface implemented in a car’s steering wheel 
was presented by Döring et al. [7], showing that gestural 
interactions can significantly reduce visual demand for non-
critical interactions. Bach et al. [8] compared three different 
means of interaction (tactile, touch, and gestures) for in-
vehicle systems in terms of efficiency and visual demand, 
finding that gestures reduce visual demands while touch 
interaction is most efficient. Although gesture interactions 
demand less visual attention [8], they are less useful for more 
complex environments with dozens of precise controls such 
as found in a cockpit. 
 
22
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

The differences in performance between touchscreens 
and physical interfaces, such as a joystick or a stencil, have 
also been studied by Kadytė and Tétard [9], showing how 
applications such as navigating through menus were more 
efficient with touch devices than the joystick. 
Using a touch display for remote control has been 
investigated for the television market. Han et al. [10] 
developed a remote control using absolute position on a 
touch display: the user’s finger is detected by infrared when 
hovering over the remote, which is shown on the TV with a 
shadow of the finger (pointer). But the hovering concept was 
found more difficult and stressful than current remotes. 
Turning to research into cognitive demands and input 
modalities, other studies have demonstrated the importance 
of spatial memory [11] and kinaesthetic cues for user 
interfaces. For instance, Tan et al. [12] have shown a 19% 
increase in spatial memory for information controlled with a 
touch-screen compared to a mouse interface. Similarly, Jetter 
et al. [13] found that multi-touch instead of mouse input 
improves users’ spatial memory and navigation performance 
for user interfaces involving panning. 
We would like to explore a variation of the concepts 
above in order to preserve some of the assets traditionally 
offered by physical interfaces, such as spatial memory, while 
migrating to touch-screen interfaces. 
III. 
FUTURE AIRCRAFT COCKPITS 
The main use-case for the concept presented in this paper 
is for future aircraft cockpits, as envisioned by the EU 
project ODICIS [14] on “One Display for a Cockpit 
Interactive Solution” (2009-2012, see Figure 2). 
 
 
Figure 2. The ODICIS single-screen tactile cockpit. 
It is the view of the ODICIS consortium [14], [15] that 
the next evolutionary step in cockpit design is to provide the 
crew with a large continuous, adaptive, multi-touch display 
that no longer is limited by the physical boundaries of 
adjacent displays. A single screen offers a more flexible 
design of the human-machine interface and therefore 
improved opportunities for providing the right information at 
the right place at the right phase of flight. Moreover, it offers 
optimised usage of the main instrument panel in terms of 
display space. Finally, prompted by novel design concepts in 
the consumer industry, new tactile technologies are 
introduced into the cockpit, offering more natural ways of 
interacting with controls, e.g., by allowing the locus of 
interaction to coincide with the locus of feedback [16]. 
The ODICIS concept aims to integrate the emerging 
tactile technologies into its single display concept. While 
offering new possibilities, it also raises many questions and 
potential pitfalls and, therefore, challenges in creating a 
design to make the most out of this new tool. One of the 
most salient drawbacks of the approach is the loss of tactile 
feedback of physical buttons and knobs [16]; hence, pilots 
will have to rely on other means of ensuring that they operate 
the intended control and in the intended way. This is 
especially true for those control-panels, which are located at 
the periphery of the pilot’s centre of vision. 
IV. 
THE “DEPORTED VIEW” CONCEPT 
To compensate for the loss of tactile feedback, which can 
increase the amount of time spent on looking at the interface 
[6], the “deported view” concept makes use of the flexibility 
of having a large display (as ODICIS) and instantly 
duplicates the lower-positioned (or overhead) panel being 
operated, onto the front display and thus directly in front of 
the pilot. The feature is optional since pilots can choose to 
use it or not at any time. 
In an aircraft cockpit, the surfaces available in the 
periphery of a pilot’s vision (sides, above) provide important 
locations for controls and lamps, because many functions 
require direct access, and not everything can fit in front of 
the pilot. 
The user’s initial touch will activate the duplicate screen 
(the “deported view” panel) and show the area now 
initialized by the user’s finger – like a mouse cursor. The 
“deported view” control panel will thus function both as a 
display and pointing device. 
The proposed pointing device uses absolute positioning, 
differing from the relative positioning of a track-pad or 
mouse. This will allow a user to operate the panel looking 
either directly on the lower display or using the duplicate on 
the front display. 
Using absolute positioning also means that users can use 
their spatial skills in order to increase efficiency. While the 
first finger controls the location of the cursor, clicking is 
performed with a tap of a secondary finger (Figure 3). In 
order to avoid inadvertent actions, the lower positioned 
control panels should not trigger any action when the first 
finger is just landing. This multi-touch approach creates a 
mouse-like interaction that should be efficient and robust to 
operate. 
V. 
EXPERIMENTAL SETTINGS 
We report an experimental assessment of a prototype that 
implements the “deported view” concept, compared with 
three other modes of interaction: a traditional touch 
interaction (in two different positions) and a trackball, all 
programmed with the Microsoft technology WPF (Windows 
Presentation Foundation). Ten participants were recruited (2 
females, 8 males, all but one right-handed; mean age 24 
years, range 20-27). The participants executed two tasks 
simultaneously (Figure 3). 
Copyright: Thales Avionics / Philippe Coni 
23
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
Figure 3. The “deported view” experimental setup, with the operator 
using a second finger to perform a click. 
The first task required participants to move sliders on a 
screen. The second task was a control task, which required 
them to respond as fast as possible and within three seconds 
to a randomly appearing visual cue, and which was 
introduced to prompt participants to maintain a head-up 
position as much as possible. Participants would receive 
points depending on the speed with which they completed 
each of the tasks, thereby using a game concept as 
motivation for being as fast as possible. A penalty was 
implemented for the control task to prevent users from 
clicking if no cue was presented, discouraging them from 
gambling when they were looking head-down at the control 
panel at their side. 
Prior to the experiments, participants were trained for 
each setup until they reached a certain skill level (criterion 
level). The order of tasks and task training was randomly 
distributed among participants in order to balance out any 
effect of order of training, task familiarity, or fatigue. Each 
setup trial was performed for two minutes. 
A. Detailed Description of the Tasks 
(Cf. appendix for an explanatory video.) 
1) Control Task (left hand) 
The control task required the participants to respond to a 
visual alert, in the form of a red box on the left side of the 
upper monitor (see Figure 3). The alert appeared randomly at 
intervals ranging between 0.1 to 3 seconds, and was turned 
off by pressing any of the joystick’s buttons. 
2) The Slider Task (right hand) 
The program would randomly display one of two sliders 
on either the desktop screen, lower screen or both, depending 
on the setup. Participants had to pull the slider down as fast 
as possible to reach a highlighted area and then release it. 
When completed, the slider disappeared and a new one 
would be randomly generated. When the participants had 
completed three sliders, the program would wait three 
seconds before generating three more sliders. 
B. Description of the Four Setups 
1) The “Deported View” Setup 
As described in the concept description, the lower multi-
touch screen was used as remote input for operating the 
sliders. Users could look down but were instructed that it 
was more efficient to stay heads-up. 
2) The “Direct Touch Desktop” Setup 
In this setup the sliders were operated by the touchscreen 
on the desktop in a traditional touch manner. However, 
before the sliders were available on the desktop screen, the 
correct area must be selected on the lower screen by a single 
tap. This was done to simulate that a secondary control-panel 
was selected in a lower position, but operated in front of the 
participant. 
3) The “Direct Touch Low” Setup 
Participants operate the two sliders directly on the lower 
touchscreen and must thus look down to be sure to touch the 
correct areas. 
4) The “Trackball” Setup 
Similar to the “direct touch desktop” setup, the proper 
panel must first be clicked on the lower screen, (again to 
simulate that a secondary control-panel is selected) after 
which the slider must be operated by trackball. The trackball 
is located near the lower touchscreen (Figure 4), in a setup 
similar to what is found in some plane cockpits. 
 
 
Figure 4. Illustration of experimental setup. 
C. Data Collection 
The participants’ performances were logged in a text file 
by the program during the experiments. The main 
performance indicator was the amount of time before the 
participants responded to the alert (“reaction time”), and how 
fast they completed the slider tasks (“completion time”). 
VI. 
QUANTITATIVE RESULTS 
The software R version 2.15 was used for the statistical 
analyses. First, an analysis of variance (ANOVA, within-
subjects) showed that there was a strongly significant effect 
of the type of setup with regard to the “completion time” 
(p<0.0001), and “reaction time” (p<0.0001). 
Single-touch screen 
 
Joystick 
 
PC 
 
Trackball 
 
Multi-touch screen 
 
24
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

TABLE I. POST-HOC BONFERRONI-ADJUSTED PAIRWISE COMPARISON 
WITH P-VALUES FOR SIGNIFICANT DIFFERENCES. 
α<0.05 
Deported view 
Direct touch 
desktop 
Direct touch 
low 
Direct 
touch 
desktop 
Completion Time 
(p<0.004) 
- 
- 
Direct 
touch low 
Completion Time 
(p<0.0004) 
Reaction 
Time 
(p<0.0005) 
Reaction 
Time (p<0.004) 
- 
Track ball 
Completion Time 
(p<0.002) 
Completion 
Time 
(p<0.0001) 
Completion 
Time 
(p<0.0001) 
 
A post-hoc Bonferroni-adjusted pairwise comparison was 
then conducted on the two dependent variables with a 
significant effect (α ≤ 0.05), as reported in TABLE I. 
 
 
Figure 5. Average time (ms) for participants to complete the task of 
pulling the 3 sliders for the four different setups. Error bars indicate the 
standard deviation. 
Figure 5 summarizes the results of the participants’ 
average time of completion for pulling down three coherent 
sliders across the four setups. The average amounts of 
completed tasks (i.e., three sliders in a row) were: 14.5 for 
“deported view”, 17 for “direct touch desktop”, 17.7 for 
“direct touch low” and 10.9 for the trackball. 
The “trackball” is by far the least efficient mode of 
operation (p<0.004, cf. TABLE I). The “deported view” 
concept ranks third, being considerably faster than the 
“trackball” (p<0.0004), while not quite as fast as the two 
direct touch setups (p<0.004). 
Figure 6 summarizes the results of the participants’ 
average reaction time to turning off the alert (the measure of 
head-up attention). The “deported view” and the “direct 
touch desktop” setups perform the best, and there is a 
significant difference (p<0.0005) between “deported view” 
and “direct touch low”. However, there is no significant 
difference between “deported view” and the two other setups 
(“direct touch desktop” and “trackball”). 
Regarding the number of errors during the control task 
(pressing buttons at a wrong time, or no reaction within 3 
seconds), no significant difference between the four setups 
was found. 
 
 
Figure 6. Average time (ms) for participants to react to the control task 
for the four different setups. Error bars indicate the standard deviation. 
VII. RESULTS FROM QUESTIONNAIRES 
The participants ranked the four setups according to their 
general preference and also according to the perceived stress 
experienced. The results are summarised in Figure 7, where 
the different setups received 0 points for being ranked last, 1 
point for second last and so on up to 3 points. 
 
 
Figure 7. Ranking results from the questionnaire. 
The “direct touch low” has the highest reported stress 
level, while the “deported view” and the “direct touch 
desktop” were globally preferred by the participants. 
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
Deported View
Direct Touch
Desktop
Direct Touch
Low
Track Ball
0
100
200
300
400
500
600
700
800
900
Deported View
Direct Touch
Desktop
Direct Touch
Low
Track Ball
0
5
10
15
20
25
30
35
40
Deported View
Direct Touch
Desktop
Direct Touch
Low
Track Ball
Favorite
Stress level
25
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

VIII. INTERPRETATION 
The lower performances of the trackball (Figure 5, 
TABLE I, p<0.002) may be explained by the fact that a three-
step manoeuvre is needed to start the task: first grabbing the 
trackball (which is smaller than the tactile areas), then move 
the ball to locate the cursor (not obligatory since the cursor is 
always visible and placed in the middle of the screen, but a 
natural behaviour), and finally operate the ball to move the 
cursor to the appropriate area. This manoeuvre is relatively 
more complex than with direct touch interaction, which 
allows these three steps to be performed at once. Although 
the participants were not familiar with trackballs, they had a 
chance to train before the experiments just like for the 
“deported view” and the other setups. 
The lower operating speed of the “deported view” 
compared to the two “direct touch” conditions (p<0.004) can 
be explained by the fact that the “deported view” uses an 
artificial feedback (cursor), which both introduces some 
technical latency, and is more demanding for the user. The 
physical movements involved in the “deported view”, while 
somewhat similar to a mouse, are not as natural as the “direct 
touch” ones. Furthermore it was clear that the participants 
were very accustomed to a relative positioned pointing 
device (such as a mouse or track-pad) compared to the 
“deported 
view” 
absolute 
positioning. 
Using 
this 
inappropriate mental model, the participants sometimes tried 
to move the cursor by moving the finger a bit forward, lifting 
the finger back to the original position and moving a bit 
forward again (see Figure 8), like with mouse when reaching 
the end of the mouse pad. With absolute positioning, this 
approach will not move the cursor onward but instead keep 
bringing it back to the start position. Further studies on the 
effect of using absolute position for an indirect pointing 
device should be investigated. 
 
 
Figure 8. Typical beginner mistake of trying to move the cursor by 
repeatedly pushing, and lifting the finger back to the original location. 
 
Regarding the reaction time (Figure 6), the fact that the 
“deported view” performed better than the “direct-touch 
low” (p<0.0005) setup is because participants could stay 
heads-up and therefore immediately notice the visual alert 
appearing on the upper screen. 
IX. 
CONCLUSION AND FUTURE WORK 
The results suggest that the “deported view” concept 
could provide an efficient and secure means for interacting 
with out-of-sight displays in e.g., automotive and aviation 
applications. 
While the “deported view” design is slower to operate 
than a touchscreen directly on a front panel, it offers better 
ergonomics than the latter, as it does not require the user to 
have an arm in the air with ensuing fatigue [5]. In any case, it 
is not always possible to place all instruments directly in 
front of the user at all times due to space and ergonomic 
considerations. In this regard the proposed concept – 
temporally showing a secondary touch display in front of the 
user – may be helpful and safer. Finally, the proposed 
method proved to be intuitive, attracted greater preference 
rating and was more efficient than the traditional trackball 
(which is used in current aircraft cockpits), and resulted in 
better reaction times than head-down interactions. Anyhow, 
the concept is mainly meant to supplement and enrich future 
touchscreen-based solutions, not to replace or compete with 
other technologies such as haptic feedback, and in 
combination these solutions may very well benefit from each 
other to produce increased usability. 
This interaction concept  might also, we may speculate, 
be convenient in the tablet and gaming industry. With an 
increase in integration between TV screens and tablets, the 
“deported view” concept could provide a means for 
interacting with the TV screen on the tablet without looking 
down. In gaming, this could provide some of the many 
controls needed to play real-time strategy games – which 
normal gamepads cannot handle – or accessible menus when 
playing fast paced shooter games. These hypotheses are left 
for future studies. 
While the results are promising, further investigation is 
required to assess human factors aspects of operational use in 
a cockpit environment, as well as the effect of different types 
of feedback such as haptic and auditory. 
X. 
ACKNOWLEDGMENTS 
The work presented in this paper has been carried out as 
part of the EU project ODICIS [14] on “One Display for a 
Cockpit Interactive Solution” (2009-2012), led by Thales 
Avionics (France) and partly funded by the European 
Commission’s Framework Programme 7 under grant number 
ACP8-GA-2009-233605. 
XI. 
APPENDIX 
A video of the different setups presented in this article is 
available at:  
http://alexandre.alapetite.fr/research/odicis/#DeportedView 
XII. REFERENCES 
[1] T. A. Furness, “The Super Cockpit and its Human Factors 
Challenges,” Human Factors and Ergonomics Society Annual 
Meeting, pp. 48 -52, 1986. doi:10.1177/154193128603000112 
[2] D. R. Jones, and R. V. Parrish, “Simulator comparison of thumball, 
thumb switch and touch screen input concepts for interaction with 
large screen cockpit display format,” NASA Technical Memorandum; 
no 1025, 1990. 
26
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

[3] P. Coni, L. Laluque, J. N. Perbet and J. Blanc, “Capacitive 
Touchscreen 
Under 
Electromagnetic 
Environment,” 
EuroDisplay’2011 conference, 2011. 
[4] S. Pope, “Garmin G3000 brings touch screen tech to flight deck,” 
Avionics Magazine, 2009. 
[5] S. Kaminani, “Human computer interaction issues with touch screen 
interfaces in the flight deck,” In Digital Avionics Systems Conference 
(DASC), 
2011 
IEEE/AIAA, 
6B4–1, 
2011. 
doi: 
10.1109/DASC.2011.6096098 
[6] K. L. Young, E. Mitsopoulos-Rubens, C. M. Rudin-Brown and M. G. 
Lenné, “The effects of using a portable music player on simulated 
driving 
performance 
and 
task-sharing 
strategies,” 
Applied 
Ergonomics, 43(4) 738-746, 2012. doi:10.1016/j.apergo.2011.11.007 
[7] T. Döring, V. Gruhn, D. Kern, P. Marshall, M. Pfeiffer, A. Schmidt 
and J. Schöning, “Gestural Interaction on the Steering Wheel – 
Reducing the Visual Demand,” Proc. of the 2011 annual conference 
on Human factors in computing systems (CHI'2011), ACM, pp. 3033-
3042, 2011. doi:10.1145/1978942.1979010 
[8] K. M. Bach, M. G. Jæger, M. B. Skov and N. G. Thomassen, “You 
Can Touch, but You Can’t Look: Interacting with In-Vehicle 
Systems,” 
CHI’2008, 
pp. 
1039-1048, 
2008. 
doi: 
10.1145/1357054.1357233 
[9] V. Kadytė and F. Tétard, “The role of usability evaluation and 
usability testing techniques in the development of a mobile system,” 
NordiCHI’2004, 2004. 
[10] J. Han, G. Lee N. Lee, S. Choi and W. Lee, “Remote Touch - Touch-
Screen-like Interaction in the TV viewing Environment,” CHI’2011, 
pp. 292-402, 2011. doi:10.1145/1978942.1978999 
[11] A. Cockburn and B. McKenzie, “Evaluating the effectiveness of 
spatial memory in 2D and 3D physical and virtual environments,” 
Proceedings of CHI'2002, the SIGCHI conference on Human factors 
in computing systems, 2002 doi:10.1145/503376.503413. 
[12] D. S. Tan, J. K. Stefanucci, D. R. Proffitt and R. Pausch, “Kinesthetic 
cues aid spatial memory,” Extended abstracts of CHI'2002, 
conference on Human factors in computing systems. ACM, New 
York, NY, USA, 806-807, 2002, doi:10.1145/506443.506607. 
[13] H. Jetter, S. Leifert, J. Gerken, S. Schubert, and H. Reiterer, “Does 
(multi-)touch aid users' spatial memory and navigation in 'panning' 
and in 'zooming & panning' UIs?,” Proceedings of AVI’2012, the 
International Working Conference on Advanced Visual Interfaces, 
Genny Tortora, Stefano Levialdi, and Maurizio Tucci (Eds.). ACM, 
New York, NY, USA, 83-90, 2012, doi:10.1145/2254556.2254575. 
[14] ODICIS Web site https://odicis.org 
[15] D. Z. Mangion, L. Bécouarn, M. Fabbri and J. Bader, “A Single 
Interactive Display Concept for Commercial and Business Jet 
Cockpits,” ATIO’2011, Conf. of the AIAA (American Institute of 
Aeronautics and Astronautics) on Aviation Technology, Integration, 
and Operations, 2011. 
[16] A. Alapetite, R. Fogh, D. Zammit-Mangion, C. Zammit, I. Agius, M. 
Fabbri, M. Pregnolato and L. Becouarn, “Direct tactile manipulation 
of the flight plan in a modern aircraft cockpit,” Proceedings of HCI 
Aero’2012, 
International 
Conference 
on 
Human-Computer 
Interaction in Aerospace, Brussels, Belgium, 13 September 2012. 
 
 
27
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

