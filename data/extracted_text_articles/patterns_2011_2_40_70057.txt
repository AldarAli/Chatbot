Application of Motion Vector in Live 3D Object 
Reconstruction 
Renshu Gu, Jie Yuan, Sidan Du 
School of Electronic Science and Engineering, Nanjing University 
Nanjing, China 
Email: gu_renshu@yahoo.cn, yuanjie@nju.edu.cn, coff128@nju.edu.cn 
 
 
Abstract—This paper applied a novel method to live 3D object 
reconstruction. Provided that static 3D reconstruction has been 
completed with basic frames, the proposed linear method 
calculates free 3D motion of a rigid-body in a video sequence, so 
as to display the moving object. The succinct method does not 
involve the fundamental matrix, and the point correspondence 
procedure in 3D reconstruction is reused. Least Mean Square 
decomposition is adopted to solve linear equation set and thus 
obtain motion vector. Furthermore, an iterative process enables 
the method to eliminate outliers. Sufficient experiments proved 
the validity and efficiency of the method. 
Keywords-3D motion; volumetric display; point correspondence; 
linear equation set; QR decomposition. 
I.  INTRODUCTION 
Estimating rigid-body 3-D motion parameters from two 
or more images of an image sequence has been extensively 
studied for a long time in the field of computer vision. It can 
be tracked down to the 1970s, when early researches proved 
that three views are necessary to recover motion from an 
orthographic camera projection model, and that  two views are 
enough to estimate motion from full perspective projection [1].  
In 1989, Weng, Huang and Ahuja [2] gave a detailed 
discussion on motion estimation from two views of full 
perspective projection, and proposed the classic 8-point 
algorithm, namely to utilize the epipolar geometric constraint, 
estimate the basic matrix from feature points (matrix E in [2]), 
and then obtain 3D motion parameters from the basic matrix. 
From then on, to overcome its sensitivity to noise, a large 
number of improved methods were proposed, including 
American researcher Hartley’s Improved 8-point Algorithm 
which standardizes 2D data. Except from methods based on 
point feature, researchers also proposed various methods based 
on line feature [4] [5], point feature and line feature combined, 
optic flow [6] [7] and methods using multiple frames and 
iterative algorithm [8] [9] [10]. 
Nevertheless, the methods mentioned above are integrated 
methods of 3-D motion estimation from two (or more) images 
without any prior knowledge of the object. In general, how to 
apply the epipolar geometric constraint is a key problem. As 
for methods based on point feature, recent works focus on 
error analysis and control (e.g., [1] [11] [12]), or on better 
estimation of the fundamental matrix [13] [14] [15], all of 
which inevitably engage the fundamental matrix. The most 
recent and relevant work in [16] described 2-frame recovery of 
structure and motion using uncalibrated cameras. This is 
somewhat similar to Han-Kanade’s method in [17].  
To perform live 3D object reconstruction, we intend to 
display moving 3D object in accordance with real pictures 
from 
live 
videos. 
However, 
adopting 
Han-Kanade 
reconstruction to video sequences over time would be too 
time-consuming. In our scheme, static 3D reconstruction is 
performed only at intervals, while motion vector is calculated 
and added to the static model at any time during the intervals. 
This paper proposed a linear method to calculate motion alone 
instead of both motion and structure, added the motion to the 
original 3D object, and then displayed it from whatever view 
the spectator prefer.  
Once static 3D reconstruction has been completed, we 
know the 3D coordinates corresponding to 2D feature points 
in image t0, and we also know new 2D coordinates of those 
points in image t1 through image matching. It is convenient 
for us to match images, since it is a step in 3D reconstruction. 
With these data we know, 3D motion parameters can be 
calculated already, and there is no need to use the integrated 
methods in the above articles. In other words, we have 
obtained enough information of 2D mapping 3D object so 
there is no need for the epipolar constraint. In theory, with 3D 
reconstruction completed, 3 point pairs from 2 views can 
recover 3D motion when motion is small. This paper gives 
Approximate Algorithm applicable to small motion, and also 
Universal Algorithm applicable to any motion. Frame rate of a 
video is greater than 25 fps, and motion between two close 
frames are quite small; therefore, if the two images under 
estimation t0, t1 are two successive (or close) frames in the 
video sequence, the Approximate Algorithm is adequate 
enough for motion estimation. Part Ⅲ  of this paper 
demonstrates experiments on both algorithms, and compares 
them with work in [17] (similar to [16]). 
 
II. THEORY AND ALGORITHM 
A. Camera model: affine camera  
2D coordinates and 3D coordinates obey: 
1
1
1
1
1
1
1
X
x
Y
y
P Z






















                                 
(1) 
41
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

wa
of 
of 
dep
B. 
can
rig
and
Le
( X
( X
by


N
cer
mo
equ
ma
var
fol
(i) 
where
0
p
P
p


 

ay of linear ma
It should be 
real cameras.
the intereste
pths. 
Motion mode
In 3D space
n be decomp
gid-body in 3D
d translation c
et 3D coordin
1
1
1
,
,
)
X
Y Z
, and
1
1
1
',
',
')
X
Y
Z
. The
y: 
where the mot
Note that an 
rtain coordina
otion, i.e., the 
uation (2). 
Although rota
atrix and subje
j
Therefore, the
rious ways of 
llowing 2 way
Axis-an
11
12
13
21
22
23
0
0
0
p
p
p
p
p
p
apping from 3
noted that aff
. It is applicab
ed object can
el of rigid-bod
, motion of ri
osed to rotati
D space can b
can be describ
1
2
3
t
T
t
t
 
 
  
 
 ,
R 
nates of any 
d coordinates 
e motion betw
1
1
1
'
'
'
1
X
Y
Z












tion matrix is 
1
R
T
M
O








important pro
ate system, any
motion matrix
ation matrix ha
ects to 6 indep
3
2
1
1(
1,
ij
j
r
i




ere are only 3 
expressing R
ys are common
ngle represent
14
24
1
p
p




is a matri
D space to 2D
fine camera is
ble only when
n be neglecte
dy  
igid-body betw
ion and trans
e described by
ed by a 3 x 1 m
11
12
13
21
22
23
31
32
33
r
r
r
r
r
r
r
r
r




 





feature poin
of this poin
ween t0 and t1 
1
1
1
1
X
Y
M Z














11
12
13
21
22
23
31
32
33
0
0
0
r
r
r
r
r
r
r
r
r






operty of rigi
y point of the
x. That is, eve
as 9 elements
pendent constr
2,3)  
3
1
ik
jk
k
r r



independent p
by 3 independ
nly used: 
ation 
ix that determ
D plane. 
s the approxim
n the depth ch
d compared 
ween two pos
slation. Rotati
y a 3 x 3 mat
matrix T. 

  
nt in image 
nt in image 
can be repres
1
2
3
1
t
t
t







id-body is, un
 body has the
ery point pair 
, it is an ortho
raints: 
0( ,
1,2,3
i j


parameters. A
dent parameter
mines a 
mation 
hanges 
to its 
sitions 
ion of 
trix R, 
t0 be
t1 be
sented 


nder a 
e same 
obeys 
ogonal 
3)  (4) 
Among 
rs, the 
T
rep
rota
(ii)
R
C.
Mo
Firs
equ
Fro
we 
Eve
 













The 3D vec
resents the ax
ates is . 
 Roll, Pi
cos
c
sin
sin
cos
cos
sin
cos
y
x
y
x
y












(a) left
(b) right: 
Figure 1.
Solve the mot
otion estimatio
st step: conver
uation set. 
om
1
1
1
1
1
1
1
X
x
Y
y
P Z























have: 
ery point pair 
1
14
1
1
24
1
'
(
'
(
x
p
p
Y p
y
p
p
Y p










11
12
13
21
22
23
31
32
33
cos
(1
(1
cos
(1
cos
(1
cos
cos
(
(1
cos
(1
cos
(1
cos
cos
(
r
r
r
r
r
r
r
r
r


























 1













ctor from th
xis the rigid-bo
itch and Yaw 
cos
cos
sin
s
sin
sin
z
z
x
z
z
x
z










ft: Angle-axis way
Roll-Pitch-Yaw w
. Two common w
tion problem  
on can be comp
rt the problem


, 
1
1
1
1
1
'
'
'
'
'
1
1
X
x
Y
y
P Z























 
1
1
'
'
1
x
y




gives 2 equati
11 1
12 2
13
11 12
12 22
13
21 1
22 2
23
21 12
22 22
23
p t
p t
p t
p r
p r
p
p t
p t
p
p r
p r
p








2
1
1
2
1
3
1
2
2
2
2
3
3
1
3
2
1
cos
)
)
(sin
)
)
(sin
)
)
(sin
)
1
cos
)
)
(sin
)
)
(sin
)
)
(sin
)
n
n n
n n
n
n n
n
n n
n n
n n






















32
1
cos
 )n

he origin (0,
ody rotates ar
representation
cos
si
sin
sin
sin
cos
sin
sin
y
x
y
z
x
y
z










 
y of expressing 3D
way of expressing
way of expressing 
pleted in 2 ste
m of estimating


and 
1
1
1
'
'
'
1
X
Y
M
Z





 






1
1
1
'
1
X
Y
PM Z








 








 
ions: 
3
1
11 11
12
3 32
1
11 13
3 3
1
21 11
3 32
1
21 13
(
)
(
(
)
(
t
X p r
p
r
Z p r
p
t
X p r
p
r
Z p r








3
2
3
1
2
1
)
)
n
n
n
n
n
n

0,0) to 
( 1
,
n n
round. The an
n 
in
cos
cos
s
sin
cos
co
z
x
z
x
z








D motion 
g 3D motion 
3D motion 
eps. 
g motion to sol
1
1
1
1
X
Y
M Z












 
 
      
2 21
13 31
12 23
13 33
22 21
23 31
22 23
23 33
)
)
)
)
r
p r
p r
p r
p r
p r
p r
p r





2
, 3
n n )
ngle it 
sin
sin
cos
os
cos
y
x
y
x
y









(6) 
lving 
   (7) 
(8) 
42
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

If m
if t
ma

wh
Re
1
1
x
y


 M
1
1
'
'
'
'
n
n
x
y
x
y
 
 







Th
wr
is 
can
is t
thr
row
to A
firs
n）
Y
Y
Y
Y










sub
no
ran
Ele
ran
nee
sec
motion is sma
the Roll-Pitch
atrix is simplif
R


 


here
2
( 1
)
(
n
n


ewrite the equa
14
1 11
1 12
24
1 21
1 22
'
'
p
X p
Yp
p
X p
Yp








Multiple point p
14
1 11
1 12
1
24
1 21
1 22
1
14
11
12
24
21
22
'
'
n
n
n
n
p
Xp
Yp
Zp
p
Xp
Yp
Z
p
X p
Yp
Z
p
X p
Yp
Z


















he first matrix
ritten down as 
due to the lim
nnot give info
the proof: 
Observe mat
ree columns ar
ws. Elementar
A. Eliminate t
st row, and th
）: 
1
13
1
12
1
23
1
22
13
12
23
22
n
n
n
n
Y p
Z p
Z
Y p
Z p
Z
Y p
Z p
Y p
Z p




Of the obtain
b-matrix has a
nzero rows; th
nk of 3 at mos
ementary row 
nk of matrix, s
From the abo
ed at least 2 ca
cond camera b
all enough for 
-Yaw represen
fied to: 
3
3
2
1
1
1
n
n
n
n
n







2
2
2
3
)
(
)
n
n



ation set (8): 
1 13
1 13
1
1 23
1 23
1
Z p
Yp
Z p
Z p
Yp
Z p


 

 


 
pairs can give 
13
1 13
1 12
1 23
1 23
1 22
13
23
n
n
p
Yp
Zp
Zp
Yp
Zp
Z p
Z p











13
12
23
22
n
n
n
n
Yp
Z p
Yp
Z p











x on the righ
A, which has
mitation of sin
ormation of de
trix A. We can
re identical to
ry row transfo
the right three
ose of Row 2i 
1
11
1
13
1
21
1
23
11
13
21
23
n
n
n
n
Z p
X p
Z p
X p
Z p
X p
Z p
X p







ned matrix (12
a rank of 2 at m
he right three 
st. So maxim ra
transformatio
so matrix A ha
ove we know m
ameras. Let th
be: 
small-angle ap
ntation is adop
2
1
3
2
1
1
n
n











 






  2
.i.e. 
2
1 
12
1 11
1 13
22
1 21
1 23
p
Z p
X p
X
p
Z p
X p
X


equation set: 
1 11
1 13
1 12
1 21
1 23
1 22
Zp
Xp
Xp
Zp
Xp
Xp




11
13
12
21
23
22
n
n
n
n
n
n
Z p
X p
X p
Z p
X p
X p






ht side of the 
s a rank of no 
ngle camera, b
epth along its 
 see the odd ro
o each other, a
ormation opera
e columns of R
with the secon
1
12
1
11
1
22
1
21
12
11
22
21
n
n
n
n
X p
Y p
X p
Y p
X p
Y p
X p
Y p




2), the right thr
most, because
columns as a 
ank of matrix 
on operations 
as a rank of 5 a
motion estima
he projection m
pproximation,
pted, the rotati
3
2
1
1
1
1










2
2
2
3






1 12
1 11
11
1 22
1 21
21
2
X p
Yp
p
p
X p
Yp
p
p


 
1 11
11
12
13
1 21
21
Yp
p
p
p
Yp
p
p


22
23
11
11
12
13
21
21
22
23
n
n
p
Yp
p
p
p
Yp
p
p
p











above equat
greater than 5
because the ca
optical axis. B
ows of the righ
and so are the 
ations are app
Row 2i-1 with t
nd row（i=2,
11
12
13
21
22
23
1
0
0
0
0
0
0
0
0
0
0
0
0
p
p
p
p
p
p

ree columns a
e there are only
sub-matrix ha
(12) is 5. 
do not change
at most. 
ation of rigid-b
matrix of the 
, and 
ion 

2
 (10) 
1
2
3
12
13
22
23
1
2
3
p
p
p
p
t
t
t



 
 
 
 
 
 
 
 
 
 
1
2
3
1
2
3
t
t
t



 








  (11) 
ion is 
5. This 
amera 
Below 
ht 
even 
plied 
the 
3…
3
3









 (12) 
as a 
ly 2 
as a 
e the 
body 
1
1
'
'
'
'
n
n
x
y
x
y











 
 
righ
6 at
equ
We have: 
14
1 11
1 12
1
24
1 21
1 22
1
14
11
12
24
21
22
'
'
'
'
'
'
n
n
n
n
p
X p
Yp
Z p
p
X p
Yp
Z
p
X p
Y p
Z
p
X p
Y p
Z


















The matrix on
ht is denoted a
t most now. T
uation set 
(a) Matched poin
(b) Matched poin
Figure 2. Matche
200
50
100
150
200
250
300
350
400
450
500
550
200
50
100
150
200
250
300
350
400
450
500
550
11
21
'
'
0
p
P
p


 

13
1 13
1 12
1 23
1 23
1 22
13
23
'
'
n
n
p
Yp
Z p
Z p
Yp
Z p
Z p
Z p











13
12
23
22
'
'
n
n
n
n
Y p
Z p
Y p
Z p











n the left is de
as A , x  succe
Thus, estimatin
b
A

nt pairs in image t
 
nt pairs in image t
ed point pairs in i
0
400
6
0
400
6
 
 
12
13
14
22
23
24
'
'
'
'
'
'
0
0
1
p
p
p
p
p
p
1 11
1 13
1
1 21
1 23
1
Z p
X p
X p
Z p
X p
X p


2
11
13
2
21
23
'
'
'
'
'
'
n
n
n
n
n
n
Z p
X p
X
Z p
X p
X




enoted as b . T
essively. The r
ng motion is co
Ax                  
 
t0 (left) and t1 (ri
 
t0 (left) and t1 (ri
mage t0 and t1 fr
00
800
600
800




 
12
1 11
11
12
22
p
Yp
p
p
p
Y

 1 21
21
22
12
11
11
12
22
21
21
22
'
'
'
'
'
'
n
n
n
n
p
p
p
X p
Y p
p
p
X p
Y p
p
p


(13)
The matrix on 
rank of A  can
onverted to so
                      
ght) from camera
ight) from camera
rom camera1 and
1000
1200
1000
1200
13
p
1
2
23
3
1
2
13
2
2
23
3
'
'
'
'
p
t
p
t
p
t



 
 
 
 
 
 
 
 
 
 
) 
the 
n be 
olving 
 (14) 
a 1 
a 2 
2 
1400
1400
43
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

Use m point pairs from camera 1 (or view 1) and k point 
pairs from camera 2 (or view 2), on the condition that 
,( ,
1)
n
m
k
m n



 (
,
m n  1
). In theory, 3 point pairs 
from two views are enough for solving the equation set. When 
n 3, we need to solve an equation set to derive the results. 
After 3D reconstruction with image t0 and image matching 
between image t0&t1, we can obtain a large number of feature 
point pairs. To exploit the information we have, we first use 
all point pairs to estimate motion. To enhance the algorithm’s 
robustness, use the initial motion matrix for reprojection, 
compare the results with real 2D coordinates in image t1, and 
eliminate outliers whose errors are larger than threshold 10 
(experiment shows error is normally less than 5, if error >10 
appears, it is probably an outlier). Appearance of outliers may 
be the result of wrong matching. 
Second step : solve the equation set by least mean square 
error principle.   
To solve the equation set by least mean square error 
principle, QR decomposition method is adopted in this paper.  
Because of the existence of error, Ax
b



. The problem 
equates solving x that minimizes norm
2
 2
. We can find Q
that obeys
R
QA
O
 





, where Q  is an orthogonal matrix and R
is a nonsingular upper triangular matrix. 
2
2
2
2
2
2
2
2
(
)
R
Ax
b
Q Ax
b
QAx
Qb
Qb
O













 
Write
1
2
b
Qb
b


 



, then we have
2
2
1
2
2
2
Rx
b
Ax
b
b




 



. It is 
a column vector, and thus
2
2
1
2
2
2
Rx
b
b



. Notice that 
2
2
b 2
is constant; therefore, the original problem of minimizing 
2
2
Ax
 b
converts to minimizing
2
1
2
Rx
 b
, namely
1
Rx
 b
=0. 
Thus, we have obtained x vector. For equation set (13), x=
1
2
3
1
2
3
( ,
,
, , , )T
   t t t
 
D. Universal Algorithm 
If there is no small-angle approximation, we can write 
equation set as follows: 
1
14
1 11
1 12
1 13
1 11
1 12
1 13
1 11
1 12
1 13
11
12
13
1 21
1 22
1 23
1 21
1 22
1 23
1 21
1 22
1 23
21
22
23
1
24
14
11
12
24
'
'
'
'
'
'
'
'
n
n
n
n
x
p
X p
X p
X p
Yp
Yp
Yp
Z p
Z p
Z p
p
p
p
X p
X p
X p
Yp
Yp
Yp
Z p
Z p
Z p
p
p
p
y
p
x
p
X p
X p
X
y
p
 



















































11
21
31
12
22
32
13
23
33
1
13
11
12
13
11
12
13
11
12
13
2
21
22
23
21
22
23
21
22
23
21
22
23
3
'
'
'
'
'
'
'
'
'
'
'
'
'
'
'
'
'
'
'
'
'
'
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
r
r
r
r
r
r
r
r
r
t
p
Y p
Y p
Y p
Z p
Z p
Z p
p
p
p
t
X p
X p
X p
Y p
Y p
Y p
Z p
Z p
Z p
p
p
p
t

















































 
 
 
 
 
 
(15) 
Use m point pairs from camera 1 (or view 1) and k point 
pairs from camera 2 (or view 2), on the condition that 
,( ,
1)
n
m
k m n



. The method of solving equation set is 
the same as Approximate Algorithm. For equation set (15), 
11
21
31
12
22
32
13
23
33
1
2
3
(
,
,
,
,
,
,
,
,
, , , )T
x
r r
r
r
r
r
r
r
r
t t t

. 
For Universal Algorithm, to ensure A has full rank, at least 
4+4=8 pairs of feature points are needed. 
III. EXPERIMENTAL VALIDATION 
Validity of the proposed algorithm was verified by 
MATLAB experiments. We used images caught by 2 cameras. 
Below are the steps of the algorithm. 
-------------------------------------------------------------------- 
1. {3D Reconstruction} 
2. {Match image t0 and t1. Obtain 2D coordinates of feature points in t1.} 
3. {Binocular Vision: 3D reconstruction and image matching for 2 cameras} 
4. {Initialization: input m pairs of camera1 and k pairs of camera2)} 
5. {Turn motion estimation into solving equation set. Every point pair 
gives 2 equations. } 
6. {Solve equation set b
 Ax
 under LSQ criterion (by QR decomposition).} 
7. {Reproject with derived M, compare 2D results to real 2D coordinates.}  
If (Errors of all points are under threshold) then 
{Eliminate point pairs whose errors exceeds threshold. 
 Go to 5, repeat 5, 6, and 7} 
Else {Give final results} 
8. {Live Display of 3D Object} 
-------------------------------------------------------------------- 
A. 5°around axis x (both algorithms) 
A box was known to have rotated an angle of 5°. For 
simplicity, the motion in our experiment only had rotation 
component. That is, t1, t2 and t3 in the translation matrix 
should be all zeros in theory, and the emphasis of result should 
be the rotation matrix. First, the Approximate Algorithm was 
tested. Second, the Universal Algorithm, which is rigorous, 
was tested. According to (6), 
23
33
arctan(
/
)
x
r
r
 

. 
We adopted two images shown in Figure 2. Reconstruct 
image t0 in 3D space, and match image t0 and t1, we obtained 
318 point pairs from camera 1in all. Similarly, we obtained 
246 point pairs from camera 2 in all.  
Experiments with different input numbers of point pair n 
were conducted. Input point pairs were results of equal-
interval sampling from all point pairs. The calculated t1, t2, t3 
were all approximately zeros; calculated
x
 is demonstrated in 
Figure 3. 
 
0
100
200
300
400
500
600
4.6
4.8
5
5.2
5.4
5.6
5.8
6
n
angle(degree)
approximate algorithm and universal algorithm
 
 
universal
approximate
44
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

cal
dat
dat
Al
nu
Ap
0.0
are
les
po
alo
wh
gu
B. 
x 
rot
Al
16
10
C. 
Figure 3. Exp
As shown, 
lculation, resu
ta, and are unr
ta, both the 
gorithm can g
mber of point 
pproximate Al
05 / 5.4=0.9%
e within 0.06 /
It was also 
ss than 2 pixel
int pairs. Figu
ong x axis. In 
hose reproject
aranteeing stab
 
F
10°around 
The box was
in Figure 1. 
tation compo
gorithm can b
8 pairs from c
.6700. 
Live display 
-150
-100
0
50
100
150
200
250
300
350
400
450
500
periments with dif
when input p
ults are greatly 
reliable; howe
Approximate
give stable re
pairs is greate
lgorithm, fluct
%, and for the U
/ 5.2559= 1.2%
verified that r
s along any ax
ure 4 shows a
addition, the 
tion errors are
ble and robust
Figure 4. reproject
axis x (Univer
s known to hav
Similar to e
onent of the
be applied. 21
camera 2 were
of 3D object 
(a) c
Above: origin
-50
0
50
fferent number of
point pairs are
influenced by
ever, when the
e Algorithm 
esults (5.4 and
er than 200 (tw
tuations of res
Universal Alg
%. 
reprojection er
xis in the imag
an example of
algorithm can
e greater than
t results.  
 
tion errors distrib
rsal Algorithm
ve rotated abo
experiment 1, 
 motion. On
0 point pairs 
e used. The ca
camera 1 
al. Below: rotated
100
150
 
f input point pairs
e barely enoug
y the choice of
ere are enough
and the Uni
d 5.3). When 
wo cameras), f
sults do not e
orithm, fluctu
rrors were alm
ge for 318+246
f reprojection 
n eliminate ou
n threshold 10
bution  
m only) 
out 10°aroun
there was m
nly the Uni
from camera 
alculated angl
d. 
s  
gh for 
f input 
h input 
iversal 
input 
for the 
exceed 
uations 
most all 
6=564 
errors 
utliers, 
0, thus 
nd axis 
merely 
versal 
1 and 
le was 
 
 
 
Add
bee
obj
(iii)
surf
obj
from
left
cam
atta
wit
wit
orig
the 
fram
dec
D.
exp
We
obj
pro
As 
Kan
F
unc
com
for 
from
red
Figure 5. c
Our process o
d the calculate
en reconstructe
ect. (ii) Conn
) Attach the t
face. 
Figure 5 show
ect from 2 vie
m camera 1 an
t column sho
meras. The m
aching texture
th texture attac
As shown, fr
th its prototype
3D object ca
ginal object i
object using 3
me within cert
cided by the m
Comparison w
To further stu
periments wer
e adopted Han
ect as well a
oposed method
shown in Fi
nade method i
 
Figure 6. compari
Thorough re
calibrated cam
mputer of 2.4G
one time of 
m 5 views wil
duce the numb
1
2
3
4
5
6
7
8
angle(degree)
 
(b) ca
Above: origina
comparison of re
of live display
ed motion to t
ed, i.e. apply 
ect all points 
texture of the
ws 10°rotati
ews in experim
nd, and (b) pr
ows real pic
middle column
e. The right c
ched. 
rom both view
e in real world
an be displaye
s entirely rec
3 frames of a 
tain range of t
maximum angle
with recent wo
udy accuracy 
re conducted f
n-Kanade meth
as recovering 
d to calculate 
igure 6, our m
in terms of pre
ison with Han-Ka
ecovery of 3
meras is quite 
GHz master f
3-frame recon
ll take much m
er of cameras
2
3
4
5
cont
num
amera 2 
al. Below: rotated
eal object and 
y of 3D objec
the original 3D
equation (2) t
in meshes to 
e original obj
ion of real ob
ment B. (a) pr
esents those fr
ctures shoot 
n shows 3D o
column show
ws, the display
d.  
ed in free-ang
constructed. W
video sequenc
the video sequ
e of image ma
ork and merits
of the propose
from many dif
hod [17] to re
motion, and 
motion using 
method is co
ecision. 
anade method (sam
D structure 
time-consumi
frequency, it 
nstruction. Ste
more time. Mo
s, since fewer 
6
7
8
9
trol experiment
mber of times
proposed
Han-Kanad
theoretical
. 
d displayed object
ct is as follow
D object whic
to all points o
obtain the sur
ect to the me
bject and displ
resents the pic
from camera 2
by correspon
object grids b
s displayed o
yed object acc
gle as long a
We can recons
ce, and display
uence. The ran
atching. 
s discussion
ed method, co
fferent viewpo
econstruct a st
then adopted
same input po
omparable to 
me input point pa
and motion 
ing. On a per
cost 5~10 mi
ereo reconstru
oreover, we ca
cameras will 
10
11
12
 
de
l precise value
 
t 
ws: (i) 
h has 
on the 
rface. 
eshed 
layed 
ctures 
2. The 
nding 
before 
object 
cords 
as the 
struct 
y any 
nge is 
ontrol 
oints. 
tereo-
d the 
oints. 
Han-
 
airs)  
from 
rsonal 
nutes 
uction 
annot 
bring 
45
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

more difficulties on image matching because of wider baseline 
between every two camera. This directly affects 3D 
reconstruction. The proposed method, on the other hand, costs 
less than one second with same number of input points. (Same 
to Han-Kanade method, this does not include the time for 
display, making it a fair comparison) Evidently, it would save 
a great deal of time in case of long video sequence if we 
recover motion using proposed method, rather than recover 
structure and motion simultaneously over time. 
E. Simple error analysis 
Errors include: (1) Quantization error. Quantization error 
exists throughout the whole process of computer vision 
including motion calculation problem in this paper. (2) 3D 
reconstruction errors, including multi-view correspondence 
error, etc. As input of motion vector calculation, 3D 
reconstruction with errors will lead to errors in the motion 
vector. (3) Error in matching image t0 and t1. Finite pixel 
resolution may introduce error, and matching algorithm could 
also influence accuracy in 2D coordinate of image t1. 
Additionally, camera calibration and computational 
accuracy have little effects on results of our method. In our 
method, 3D reconstruction is based on uncalibrated cameras, 
and motion calculation does not involve calibration either. 
Computational accuracy of MATLAB is more than 40 decimal 
places; therefore, truncation error has no influence on our 
results. 
 
IV. CONCLUSION AND FUTURE WORK 
Several conclusions can be drawn from the experiment 
results.  
Firstly, the correctness and feasibility of the proposed 
algorithm are verified. For the Approximate Algorithm, 
exactly 3 point pairs are needed for motion estimation. Of 
course, the result depends largely on the selected 3 point pairs, 
and is too sensitive and unstable.  
Secondly, the algorithm is stable and robust when there is 
enough input data, as results are obtained on a unanimous 
ground. The algorithm can eliminate outliers to a certain 
extent.  
Thirdly, since the algorithm merely calculates motion 
vector but not simultaneously recover motion and structure, it 
is quite succinct. Time cost depends on the number of point 
pairs. For a typical experiment with about 200 point pairs each 
camera, both the Universal Algorithm and the Approximate 
Algorithm cost less than a few second. Thus, static 3D 
reconstruction can be performed only at intervals, while 
motion vector is calculated and added to the static model at 
any time during the intervals. Compared to conducting 3D 
reconstruction over time, this scheme will save much time. 
Future work will concentrate on the following several 
aspects: 
Selecting a proper interval between every two times of 
motion vector calculation: strike the balance between time 
consumption and necessity; 
Improving the continuity of real-time display, perhaps by 
interpolating;   
Testing cases when motion calculation is limited by  
wide-baseline image matching ， in order to find out the 
limitation of application. 
 
V. 
ACKNOWLEDGEMENT 
This paper is supported by the Fundamental Research 
Funds for the Central Universities, Number: 1107021051, and 
National Natural Science Funds of Jiangsu Province of China, 
Number:  BK2010386. 
REFERENCES 
[1] 
T. Papadimitriou, M. G. Strintzis, and M. Roumeliotis, “Robust 
Estimation of Rigid Body 3-D Motion Parameters Based on Point 
Correspondences”, IEEE Trans. on Circuits And Systems for Video 
Technology, 2000, pp. 541-549.  
[2] 
J. Weng, T. S. Huang, N. Ahuja, “Motion and Structure From Two 
Perspective Views: Algorithms, Error Analysis, and Error Estimation”, 
IEEE Trans. on Pattern Analysis and Machine Intelligence, 1989, pp. 
451-476.  
[3] 
R. Hartley, “In Defense of the 8-point Algorithm”, Proc. 5th ICCV, 
Cambridge, USA, 1995, pp. 1064-1070.  
[4] 
H. Ebrahimnezhad, H. G. Robust, “Motion From Space Curves and 3D 
Reconstruction From Multiviews Using Perpendicular Double Stereo 
Rigs”, Image and Vision Computing, 2008, pp. 1397–1420.  
[5] 
J. M. M. Montiel, J.D. TardoHs, and L. Montano, “Structure and Motion 
From Straight Line Segments”, Patten Recognition, 2000, pp. 1295-1307.  
[6] 
E. G. Steinbach, P.Eisert, and B. Girod, “Model-based 3D Shape and 
Motion Estimation Using Sliding Textures”, Proc. Vision Modeling and 
Visualization Conference, 2001, pp. 375-382. 
[7] 
J. Neumann, C. Fermuller, and Y. Aloimonos, “Polydioptric camera 
design and 3D motion estimation”, Proc. IEEE Conf. Computer Vision 
and Pattern Recognition, volume 2, 2003, pp. 294-301. 
[8] 
J. W. Roach and J. K. Aggarwal, “Determining the Movement of 
Objects From A Sequence of Images”, IEEE Trans. on Pattern Anal. 
Machine Intell, 1980, pp. 554–562.  
[9] 
J. Oliensisa, “Multi Frame Structure from Motion Algorithm under 
Perspective Projection”, Int J. Comput Vis, 1999, pp. 163–192.  
[10] P. Sand and S. Teller, “Particle Video: Long-Range Motion Estimation 
Using Point Trajectories”, Int J. Comput. Vis, 2008, pp. 72–91.  
[11] B. Matei and P. Meer, “A General Method for Errors-in-Variables 
Problems in Computer Vision,” Proc. IEEE Conf. Computer Vision and 
Pattern Recognition, 2000, pp. 18–25. 
[12] P. Firoozfam and S. Negahdaripour, “Theoretical Accuracy Analysis of 
N-Ocular Vision Systems for Scene reconstruction, Motion Estimation, 
and Positioning Problems in Computer Vision”, Proc. 2nd Int. 
Symposium on 3D Data Processing, Visualization, and Transmission, 
2004, pp. 888-895. 
[13] Y. Sheikh, A. Hakeem, and M. Shah, “On the Direct Estimation of The 
Fundamental Matrix”. Proc. IEEE Conf. Comput Vis. Patt. Rec., 2007, 
pp. 1–7.  
[14] P. Chen, “Why Not Use the Levenberg–Marquardt Method For 
Fundamental Matrix Estimation”, IET Comp. Vis., Vol. 4, Iss. 4, 2010, 
pp. 286–294. 
[15] H. P. Wu and S. H. Chang, “Fundamental Matrix of Planar Catadioptric 
Stereo Systems”, IET Comput. Vis., Vol. 4, Iss. 2, 2010, pp. 85–104. 
[16] R. Szeliski, “Computer Vision: Algorithms and Applications”, Chapter 7, 
2010, pp. 343-374.  
[17] M. Han and T. Kanade, “Creating 3D Models with Uncalibrated 
Cameras”, Proc. IEEE Computer Society Workshop on the Application 
of Computer Vision, 2000, pp. 178-185. 
 
46
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

