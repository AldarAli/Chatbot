Challenges in Evaluating Methods for Detecting Spatio-Temporal Data Quality Is-
sues in Weather Sensor Data  
 
Douglas E. Galarus 
Computer Science Department 
Utah State University 
Logan, UT 84322-4205, United States 
douglas.galarus@usu.edu 
 
Rafal A. Angryk 
Department of Computer Science 
Georgia State University 
Atlanta, GA 30302, United States 
angryk@cs.gsu.edu 
 
Abstract—There is a need for robust solutions to the challeng-
es of near real-time spatio-temporal outlier and anomaly de-
tection. Yet, there are many challenges in developing and 
evaluating methods including: real-world cost and infeasibil-
ity of verifying ground truth, non-isotropic covariance, near-
real-time operation, challenges with time, bad data, bad 
metadata, and other quality factors. In this paper, we demon-
strate the challenges of evaluating spatio-temporal data quali-
ty methods for weather sensor data via a method we devel-
oped and other popular, interpolation-based methods to con-
duct model-based outlier detection. We demonstrate that a 
multi-faceted approach is necessary to counteract the impact 
of outliers. We demonstrate the challenges of evaluation in the 
presence of incorrect labels of good and bad data. 
Keywords-Data Quality; Spatial-Temporal Data; Quality 
Control; Outlier; Inlier; Bad Data; Ground Truth 
I. INTRODUCTION 
In our research, we address near-real-time determination 
of outliers and anomalies in spatiotemporal weather sensor 
data, and the implications of quality assessment on compu-
tation from the perspective of the data aggregator. Data 
might not reflect the conditions they measure for a variety 
of reasons. The challenges go beyond identifying individual 
outlying observations. A sensor might become “stuck” and 
produce the same output over an extended period. A sen-
sor’s output may conform to other nearby observations and 
fall within an acceptable range of values, but not reflect ac-
tual conditions. A sensor may drift, reporting values further 
from ground truth over time. A sensor may report correct 
values, but the associated clock may be incorrect, resulting 
in bad timestamps. An incorrect location may be associated 
with a site. These and related problems cause challenges 
that are far more complex than simple outlier detection. 
Sensor-level quality control processes often utilize do-
main-specific, rule-based systems or general outlier detec-
tion techniques to flag “bad” values. NOAA’s Meteorolog-
ical Assimilation Data Ingest System (MADIS) [1] applies 
the range [-60° F, 130° F] to check for air temperature ob-
servations [2] while the University of Utah’s MesoWest [3] 
uses the range [-75° F, 135° F] [4] for validity checks. 
These ranges are intended to represent the possible air tem-
perature values in real world conditions, at least within the 
coverage area of the provider. If an observation falls out-
side the range, then the provider flags that observation as 
having failed the range test and the observation will, for all 
practical purposes, be considered “bad”. Range tests are not 
perfect. The record high United States temperature would 
fail MADIS’s range test, although it would pass MesoW-
est’s test. Both MADIS and MesoWest further employ a 
suite of tests that go beyond their simple range tests. “Bud-
dy” tests compare an observation to neighboring observa-
tions. MADIS uses Optimal Interpolation in conjunction 
with cross-validation to measure the conformity of an ob-
servation to its neighbors [2]. MesoWest estimates observa-
tions using multivariate linear regression [5]. A real obser-
vation is compared to the estimate, and if the deviation is 
high, then the real observation is flagged as questionable. 
These approaches are flawed in that they do not account 
for bad metadata, such as incorrect timestamps or incorrect 
locations. They do not account for chronically bad sites 
which produce bad data including data that may sometimes 
appear correct. Of even greater concern, they may not do a 
good job in assessing accuracy and may be incorrectly la-
beling bad data as good and good data as bad. 
The consequences of ignoring data quality are great. 
How can we trust our applications and models if the inputs 
are bad? In turn, how can we better assess data for quality 
so that we can be confident in its use? 
In this paper, we present new evaluation results for our 
previously-published method including evaluation with 
several new data sets. These results are significant in that 
they demonstrate the challenges of evaluation of methods 
for data quality assessment of spatio-temporal weather sen-
sor data. The rest of this paper is organized as follows: Sec-
tion II presents relevant literature, Section III identifies 
general challenges, Section IV defines our approach, Sec-
tion V documents evaluation results, and Sections VI gives 
our conclusion. 
II. LITERATURE REVIEW 
The data mining process includes data preprocessing 
and cleaning as critical components. Outlier analysis, is ad-
dressed within these headings by Han, et al. [6], and the 
impact of outliers is covered by Nisbet, et al. [7]. Robust 
regression techniques are employed in data mining to over-
come outliers and low quality data in the process of data 
cleaning by Witten, et al. [8]. The handling of errors and 
missing values is presented by Steinbach and Kumar [9], 
1
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-687-3
GEOProcessing 2019 : The Eleventh International Conference on Advanced Geographic Information Systems, Applications, and Services

 
along with quality attributes, such as accuracy and preci-
sion, as well as the adverse impact that outliers, can have 
on clustering algorithms. Such examples demonstrate the 
chicken-egg nature of the problem in which a method used 
to identify outliers is adversely impacted by outliers.  
Aggarwal [10] presents a number of useful, general ob-
servations: Correlation across time series can help to identi-
fy outliers, using one or multiple series to predict another. 
Deviations between predicted and actual values can then be 
used to identify outliers. When used on temporal snapshots 
of data, spatial methods can fall short because they do not 
address the time component. Decoupling the spatial and 
temporal aspects can be suboptimal. Neighborhoods can be 
used to make predictions, yet it is a challenge to combine 
spatial and temporal dimensions in a meaningful way. Do-
main-specific methods can be used to filter noise, but such 
filtering can mask anomalies in the data. 
Shekhar, et al. [11] present a unified approach for de-
tecting spatial outliers and a general definition for spatial 
outliers, but they do not address the spatio-temporal situa-
tion. Klein, et al. [12]–[16] present work on transfer and 
management challenges related to the inclusion of quality 
control information in data streams and develop optimal, 
quality-based load-shedding for data streams in. A missing 
component is the spatial aspect.  
The weather and road-weather communities employ de-
tailed accuracy checks for individual observations. The Ok-
lahoma Mesonet uses the Barnes Spatial Test [17], a varia-
tion of Inverse Distance Weighting (IDW) (see Shepard 
[18]). MesoWest [3] uses multivariate linear regression to 
assess data quality for air temperature, as described by 
Splitt and Horel in [19] and [20]. MADIS [1] implements 
multi-level, rule-based quality control checks including a 
level-3 neighbor check using Optimal Interpolation / 
kriging [2][21][22]. These approaches (IDW, Linear Re-
gression, kriging) can be used to check individual observa-
tions for deviation from predicted and flag individual ob-
servations as erroneous or questionable if the deviation is 
large. But if interpolated values are erroneous, then the 
quality assessment will be bad too. If metadata, such as lo-
cation or timestamps associated with a site, is erroneous, 
then the quality control assessment may be bad because of 
comparison with the wrong data from the wrong sites. 
None of these approaches identify incorrect location 
metadata and one provider, Mesowest, attempts to identify 
bad timestamps, yet their approach only identifies one of 
the most obvious timestamp-related problem – timestamps 
that cannot possibly be correct because they occur in the 
future relative to collection time. 
Many spatial approaches use interpolation for quality 
assessment, so it is useful to examine work that compares 
and enhances traditional interpolation methods. Zimmer-
man, et al. [23] use artificial surfaces and sampling tech-
niques, as well as noise level and strength of correlation, to 
compare Ordinary kriging (OK) and Universal kriging 
(kriging with a trend) (UK) and IDW. They found that the 
kriging methods outperformed IDW across all variations 
they examined. Lu and Wong [24] found instances in which 
kriging performed worse than their modified version of 
IDW, where they vary the exponent depending on the 
neighborhood. They indicate that kriging would be favored 
in situations for which a variogram accurately reflects the 
spatial structure. Mueller, et al. [25] show similar results, 
saying that IDW is a better choice than OK in the absence 
of semi-variograms to indicate spatial structure.  
In prior work, we proposed a modification of IDW that 
used a data-based distance rather than geographic distance 
to assess observation quality [26][27]. That work focused 
on the use of robust methods to associate sites for assess-
ment of individual observations. In [28][29][30], we ex-
tended the mappings to better account for spatio-temporal 
variation and observation time differences when assessing 
observations. In [31] and [32], we developed quality 
measures that extended beyond sites, to help evaluate over-
all spatial and temporal coverage of a region. 
IDW is widely applied, including applications which in-
volve outlier detection and mitigation. Xie, et al. [33] ap-
plied it to surface reconstruction, in which they detect out-
liers using distance from fitted surfaces. Others extend the 
method in different ways including added dimensions, par-
ticularly time. Li, et al. extend IDW in [34] to include the 
time dimension in their application involving estimated ex-
posure to fine particulate matter. Grieser warns of problems 
with arbitrarily large weights when sites are near in analyz-
ing monthly rain gauge observations [35], and mitigates the 
problem in a manner that Shepard originally used by defin-
ing a neighborhood for which included points are averaged 
with identical weights in place of the large, inverse distance 
weights.  
Kriging and Optimal Interpolation were developed sepa-
rately and simultaneously as spatial best linear unbiased 
predictors (blups) that are for practical purposes equivalent. 
L. S. Gandin, a meteorologist, developed and published op-
timal interpolation in the Soviet Union in 1963. Georges 
Matheron, a French geologist and mathematician, devel-
oped and published kriging in 1962, named for a South Af-
rican mining engineer, Danie Krige, who partially devel-
oped the technique in 1951 and later in 1962. For further 
information, refer to Cressie [36]. 
Kriging is easily impacted by multiple data quality di-
mensions and its applicability is hindered unless data quali-
ty issues in the inputs are addressed. Kriging will down-
weight observations that are clustered in direction, as indi-
cated by Wackernagel, et al. [37]. This may be beneficial. 
However, a near observation can also shadow far observa-
tions in the same direction, causing them to have small or 
even negative weights. This is problematic in the case that 
the near observation is bad. 
Kriging is typically used to interpolate values at loca-
tions for which measurements are unknown using observa-
tions from known locations. As such, covariance is typical-
ly estimated. This estimate usually takes the form of a func-
tion of distance alone and is determined by the data set. A 
principal critique of kriging is that while it does produce 
optimal results when the covariance structure is known, the 
motivation for using kriging is questionable when the co-
2
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-687-3
GEOProcessing 2019 : The Eleventh International Conference on Advanced Geographic Information Systems, Applications, and Services

 
variance structure must be estimated. Handcock and Stein 
[38] make such an argument. Another critique is that 
kriging will yield a model that matches data input to the 
model, giving the (false) impression that the model is per-
fect, as stated by Hunter, et al. [39]. 
Unfortunately, none of these approaches alone directly 
addresses outlier and anomaly detection for spatio-temporal 
data in a robust and comprehensive manner that meets our 
needs. None identify bad sites and metadata in a compre-
hensive manner. However, the data quality attributes pre-
sented are of some benefit and the methods used by the 
weather data providers appear to be state of the art for as-
sessment of accuracy.  
III. CHALLENGES 
Our research involves (fixed) site-based, spatio-temporal 
sensor big data, acquired and evaluated for data quality 
with real-time potential. There are many computational 
challenges associated with our problem. We focus subse-
quent evaluation on scalability and accuracy. 
Scalability. Our data sets include thousands of sites, 
with potential to expand to tens of thousands of sites. Sites 
have varying reporting frequencies ranging from every mi-
nute to hourly or longer. These sites collectively generate 
millions of observations daily. We desire to run our algo-
rithms in near real-time, and scalability is key to achieving 
this goal. 
Accuracy. The underlying data has many data quality 
challenges. Accurately modeling the data is challenging, 
because the modeled data will inherently include errors. 
We desire robust, accurate models that can be used to as-
sess the quality of individual observations. 
There are many indirect issues causing challenges that 
must be overcome. These all influence or are influenced by 
computation in one way or another. 
Real-World Cost and Infeasibility of Verifying Ground 
Truth. Agencies cannot verify ground truth on a regular ba-
sis across hundreds or thousands of sites. Human-required 
resolution processes can be focused if problems are identi-
fied automatically. Third-party data aggregators have no 
control over original data quality. Assessment of quality is 
essential for use. 
Non-Isotropic Covariance. Distance cannot be treated 
equally in all dimensions nor in all directions. There are 
differences between the time dimension and spatial dimen-
sions. Elevation, proximity to the ocean, terrain, microcli-
mates, prevailing weather patterns, the diurnal effect, sea-
sonal change, etc. also cause differences in covariance. 
Near-Real-Time Operation. We intend for our processes 
to run in near-real-time when observations are acquired. 
We store and use only the most recent observations for near 
real-time presentation and comparison. We do not intend to 
store third-party historical data on our production systems. 
This does not preclude the potential for offline prepro-
cessing and analysis that makes use of historical data. Even 
if providers apply their own quality control measures, near-
real-time operation may require us to use observations that 
have not been fully quality-checked. 
Further Challenges with Time. Sites report observations 
at discrete times resulting in granularity and non-
uniformity. Observation frequencies and reporting times 
vary across sites. Network latency and batch processing 
further disrupt timeliness. 
“Bad” Data. Bad data includes but is not limited to erro-
neous observation data – individual observations that differ 
from ground truth; “bad” sites – sites that chronically pro-
duce erroneous data; and “bad” metadata including incor-
rect locations and/or incorrect timestamps. Bad data may 
include items that are not individually considered outliers. 
Other Quality Factors. There are many other quality fac-
tors including reliability (site, sensor, communication net-
work), timeliness of data, imprecision of data, and impreci-
sion of metadata. 
IV. DEFINITIONS AND APPROACH 
A. General Definitions 
An individual site refers to a fixed-location facility that 
houses one or multiple sensors that measure conditions. A 
measurement and associated metadata are referred to as an 
observation. The set of all sites, represented by S, is the set 
of sites for which observations are available for a time pe-
riod and geographic area of interest. 
An observation, 𝑜𝑜𝑜𝑜𝑜𝑜, is represented as a 4-tuple, 𝑜𝑜𝑜𝑜𝑜𝑜 =
〈𝑜𝑜, 𝑡𝑡, 𝑙𝑙, 𝑣𝑣〉 = 〈𝑜𝑜𝑜𝑜𝑜𝑜𝑠𝑠, 𝑜𝑜𝑜𝑜𝑜𝑜𝑡𝑡, 𝑜𝑜𝑜𝑜𝑜𝑜𝑙𝑙, 𝑜𝑜𝑜𝑜𝑜𝑜𝑣𝑣〉
 consisting 
of 
the 
site/sensor s, timestamp t, location l (spatial coordinates), 
and an observed value v. We investigate observations from 
a single sensor type, so we assume that s identifies both the 
site and sensor. The set of all observations, represented by 
O, consists of observations from sites in S over a time-
period of interest. 
Ground-truth is the exact value of the condition that a 
given sensor is intended to measure at a given location and 
time. Ground-truth will rarely be known because of sensor 
error, estimation error, and high human costs, among other 
reasons. Human cost is a huge challenge, with agencies 
struggling to accurately inventory assets and technicians 
unable to service and maintain all equipment, including sit-
uations where they may not even be able to find the equip-
ment. 
We wish to evaluate observations to determine if they 
are erroneous. To do so, we compare observations to esti-
mates of ground-truth. For our purposes, these estimates 
will be determined via interpolation, which is commonly 
used in the GIS community, as well as in the weather and 
road-weather communities. 
B. Approach 
Identification of Outlyingness and Outliers. We measure 
outlyingness as the absolute deviation between an observed 
value and ground truth. Ground truth may not be known, so 
we estimate outlyingness as the absolute deviation between 
an observation and modeled ground truth corresponding to 
the observed value in time and location. Given the degree 
of outlyingess (exact or estimated), we identify outliers us-
ing a threshold. If the degree of outlyingness for an obser-
3
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-687-3
GEOProcessing 2019 : The Eleventh International Conference on Advanced Geographic Information Systems, Applications, and Services

 
vation meets or exceeds the threshold, then we flag the ob-
servation as an outlier. Otherwise, we flag it as an inlier. 
The degree of outlyingness is more informative than an 
outlier/inlier label. 
Our approach is consistent with general model-based 
approaches for outlier detection found in Han, et al. [6], 
Tan, Steinbach and Kumar [9] and Aggarwal [10], and fol-
lows the general data-mining framework of Train, Test and 
Evaluate. 
C. Interpolation to Model Ground Truth 
IDW estimates ground truth as the weighted average of 
observation values using (geographic) distance from the 
site for which an observation is to be estimated as the 
weight, raised to some exponent h. If ground truth is 
known, a suitable exponent h can be determined to mini-
mize error. Isaaks and Srivastava [40] indicate that if h=0, 
then the estimate becomes a simple average of all observa-
tions, and for large values of h, the estimate tends to the 
nearest neighboring observation(s). This simple version of 
IDW does not account for time, so it is assumed that obser-
vations fall in temporal proximity.  
Least Squares Regression (LSR) estimates observed 
values using the coordinates of the sites. We only use x-y 
coordinates in our experiments for LSR. There could be 
benefit in using elevation and other variables including 
time. However, doing so compounds problems related to 
bad metadata, such as incorrect locations, bad timestamps 
and inaccurate elevations.  
UK estimates observed values using the covariance be-
tween sites, the coordinates of the sites, and the observed 
values. In our experiments, we used a Gaussian covariance 
function of distance and estimated the related parameters to 
minimize error relative to ground-truth for our training data 
using data from the present time window. Refer to 
Huijbregts and Matheron [41] for further information on 
UK. We implemented a fitter/solver for the estimation of 
the covariance function parameters using the Gnu Scientific 
Library (GSL) non-linear optimization code [42]. Refer to 
Bohling [43] for additional covariance functions. 
These methods can be applied using a restricted radius 
or a bounding box to alleviate computational challenges 
and to focus on local trends. Other interpolators could be 
applied in a similar manner. There are obvious risks in us-
ing interpolators. Outliers and erroneous values will have 
an adverse impact on interpolation, causing poor estimates. 
Lack of data in proximity to a point to be estimated can al-
so result in a poor estimate. For these reasons, we devel-
oped our own robust interpolator in prior work. 
D. Our SMART Approach 
In prior work, we developed a representative approach 
for data quality assessment of site-based, spatio-temporal 
data using what we call Simple Mappings for Approxima-
tion and Regression of Time series (SMART) [26-32]. We 
used the SMART mappings to identify bad (inaccurate) ob-
servations and “bad” sites/sensors, so that they can be ex-
cluded from display and computation, and to subsequently 
estimate (interpolate) ground truth. 
Site-to-Site Mappings. Let an observation be represent-
ed as 𝑜𝑜𝑜𝑜𝑜𝑜 = {(𝑡𝑡, 𝑣𝑣): 𝑡𝑡 = 𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡, 𝑣𝑣 = 𝑣𝑣𝑣𝑣𝑙𝑙𝑣𝑣𝑡𝑡}, pairing the val-
ue with the reported time. Let 𝑜𝑜𝑜𝑜𝑜𝑜𝑖𝑖 be the set of observa-
tions from site i and 𝑜𝑜𝑜𝑜𝑜𝑜𝑗𝑗 be the set of observations from 
site j. For a given time radius r we pair the observations 
from sites i and j as 𝑜𝑜𝑜𝑜𝑜𝑜𝑝𝑝𝑝𝑝𝑖𝑖𝑝𝑝𝑠𝑠𝑖𝑖,𝑗𝑗 = ൛(𝑥𝑥, 𝑦𝑦): (𝑡𝑡1, 𝑥𝑥) ∈
𝑜𝑜𝑜𝑜𝑜𝑜𝑖𝑖, (𝑡𝑡2, 𝑦𝑦) ∈ 𝑜𝑜𝑜𝑜𝑜𝑜𝑗𝑗, |𝑡𝑡2 − 𝑡𝑡1| ≤ 𝑟𝑟ൟ. We then define a site-
to-site mapping l as a linear function of the x-coordinate 
(the observed value from site i) of the paired observations 
𝑜𝑜𝑜𝑜𝑜𝑜_𝑝𝑝𝑣𝑣𝑡𝑡𝑟𝑟𝑜𝑜𝑖𝑖,𝑗𝑗: 𝑙𝑙𝑖𝑖,𝑗𝑗(𝑥𝑥) = 𝑣𝑣 + 𝑜𝑜𝑥𝑥. We determine this function 
to minimize the squared error between the values of the 
function and the y-coordinates (the observed values from 
site j) for the paired observations.  
We next determine a quadratic estimate q of the squared 
error of the linear mapping relative to the time offset be-
tween the paired observations. We expect an increased 
squared error for increased time differences. This model 
estimates the squared error and accounts for time offsets 
between observations. Our method does not require a com-
plex, data-specific covariance model. 
These simple mappings are the core elements of our ap-
proach, and we must overcome the potential impact of the 
erroneous data in determining them. LSR suffers from sen-
sitivity to outliers. We use the method from Rousseeuw and 
Van Driessen to perform Least Trimmed Squares Regres-
sion [44]. Least Trimmed Squares determines the least 
squares fit to a subset of the original data by iteratively re-
moving data furthest from the fit. Before applying least 
trimmed squares to determine the linear mapping, we select 
the percentage of data that will be trimmed. We can inter-
pret the trim percentage either as our willingness to accept 
bad data in our models or our estimate of how much data is 
bad. We used a trim percentage of 0.1 throughout. 
For the quadratic error mappings, we experienced prob-
lems with local minima when attempting quadratic least 
trimmed squares. Instead we group data into intervals, de-
termine the trimmed mean for each group, and then com-
pute the least squares quadratic fit for the (time difference, 
trimmed mean) pairs. 
We then check the coefficients and derived measures of 
the linear and quadratic mappings for outlying values rela-
tive to all other mappings. If we find outlying values, we 
flag the mapping as unusable. For instance, if the axis of 
symmetry of the quadratic error mapping is an outlier rela-
tive to that for another pairing, then there may be a problem 
with the timestamps of at least one of the two sites. 
SMART Interpolator. Our SMART interpolator uses 
these mappings. Formally: Let 𝑆𝑆 be the set of all sites. Let 
𝑜𝑜 ∈ 𝑆𝑆 be a site for which we are evaluating observations. 
Let ⟨𝑜𝑜1, … , 𝑜𝑜𝑛𝑛|𝑜𝑜𝑖𝑖 ∈ 𝑆𝑆, 𝑜𝑜𝑖𝑖 ≠ 𝑜𝑜⟩ be the set of sites other than 
site 𝑜𝑜. We want to estimate 𝑜𝑜𝑜𝑜𝑜𝑜𝑠𝑠(𝑡𝑡𝑠𝑠), the value of the ob-
servation at site 𝑜𝑜 at time 𝑡𝑡𝑠𝑠 using the most recent observa-
tions from the other sites relative to time 𝑡𝑡: (𝑡𝑡𝑖𝑖, 𝑣𝑣𝑖𝑖).  
Our SMART interpolator is like IDW, using our quad-
ratic error estimates instead of distance given the time lag 
4
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-687-3
GEOProcessing 2019 : The Eleventh International Conference on Advanced Geographic Information Systems, Applications, and Services

 
between observations and using our SMART linear map-
pings to yield estimated ground truth producing an esti-
mate. Neither distance nor direction are directly used. The 
linear mappings and quadratic error estimates account for 
similarity between sites. No attempt is made to down-
weight clustered sites, although there may be benefit in do-
ing so. 
We determine the exponent g by minimizing error rela-
tive to ground truth, if available, or estimated ground truth. 
Prior to computing the weighted estimate, we examine the 
weights and, if necessary, “re-balance” to reduce the poten-
tial influence of single sites on the outcome. We found it 
useful to restrict the maximum relative weight a site can be 
given to 0.25 to reduce the risk that a bad value from one 
site will overly influence the resulting average. Rather than 
take a simple weighted average, we use a trimmed mean to 
further reduce the influence of outliers. 
E. Artificial Data Set 
We developed a weather-like phenomenon representing 
temperature as approximate fractal surfaces produced using 
the method of Successive Random Addition. For further 
information on Successive Random Addition, refer to Voss 
[45], Feder [46], and Barnsley, et al. [47]. Fractional 
Brownian processes were used by Goodchild and Gopal to 
generate random fields representing mean annual tempera-
ture and annual precipitation for the purpose of investigat-
ing error in [48]. We used a similar approach to model time 
series in [49]. A 513x513 approximate fractal surface, 
𝑜𝑜𝑣𝑣𝑟𝑟𝑠𝑠𝑣𝑣𝑠𝑠𝑡𝑡(𝑥𝑥, 𝑦𝑦), was generated with Hurst Exponent H=0.7 
and 𝜎𝜎2 = 1.0 , representing elevation. A 1025x513x513 
fractal-like weather pattern, 𝑤𝑤𝑡𝑡𝑣𝑣𝑡𝑡ℎ𝑡𝑡𝑟𝑟(𝑥𝑥, 𝑦𝑦, 𝑡𝑡) , was also 
generated with Hurst Exponent H=0.7 and 𝜎𝜎2 = 1.0. The 
larger x-coordinate allowed us to simulate motion/flow. We 
generated one surface and eight weather patterns, allowing 
us to train on one weather pattern and test on those remain-
ing.  
We generated time series of “ground truth” data by 
combining the surface data with the weather data, a period-
ic effect and a north-south effect to simulate a weather-like 
phenomenon like the diurnal effect and general north-south 
variation in the Northern Hemisphere respectively. We 
added the weather data as is, with varying offsets in the x-
coordinate to represent a west to east flow in the weather 
pattern. The surface value is subtracted so that low points 
are “warmer” than high points. The periodic effect repre-
sents warming during the day and cooling at night. The 
north-south effect yields warmer points to the south and 
cooler points to the “north”. Our approach yields a time se-
ries of length n=513 for each (𝑥𝑥, 𝑦𝑦) on the 513x513 sur-
face. 
We selected 250 “sites” using random uniform x-y (spa-
tial) coordinates. For each site we assigned a reporting pat-
tern with a random frequency and offset. We added errors 
to the observations from 25 sites via: random noise added 
to ground truth (NOISE), rounding of ground truth 
(ROUNDING), replacement of ground truth with a constant 
value (CONSTANT), replacement with random bad values 
with varying probabilities (RANDOMBAD), or negation of 
ground truth. The remaining 225 sites were left error-free.  
V. EVALUATION 
We evaluated the performance of the various interpola-
tors including our SMART Method in-depth, in terms of 
computation and ability to identify bad data. We compared 
our SMART method, IDW, LSR, UK and OK. We meas-
ured performance and scalability using run-time in milli-
seconds. We measured accuracy using mean-squared-error 
(MSE) between estimated and known ground-truth. We 
compared means using t-tests when multiple runs were 
available. We used Area Under the ROC Curve (AUROC) 
analysis to evaluate accuracy of outlier classification given 
varying “threshold” values for outlier/inlier determination. 
We analyzed our artificial data set, MADIS air tempera-
ture for Northern California from December 2015, MADIS 
air temperature for Montana from January 2017, and Aver-
age Daily USGS Streamflow for Montana from 2015, 2016, 
2017. 
A. Evaluation Using our Artificial Data Set 
We performed an in-depth comparison of the various al-
gorithms using our artificial data set. We enhanced the 
standard algorithms by randomly choosing neighboring 
sites using set inclusion percentages (0.1, 0.2, 0.3, …, 0.9, 
1.0). For instance, a 0.9 inclusion percentage corresponds 
to selecting neighboring sites individually with 0.9 inclu-
sion / 0.1 exclusion probability. We varied the radius (50, 
75, 100, …, 175, 200) over which sites were included rela-
tive to the location of the site whose observation we were 
testing. We repeated this procedure 10 times for each pa-
rameter combination (inclusion percent and radius) and 
used the median of the resulting estimates as the estimate 
for that parameter combination. By randomly holding out 
sites, bad data will be held out in some of the resulting 
combinations. By taking the median of the results, we elim-
inate the extreme estimates, particularly those impacted by 
bad data, and ideally determine a robust estimate. 
We ran the methods in aggregate over the eight time pe-
riods spanning 512 time units. For each time period, there 
are 37,293 observations total from the 250 sites. We iterat-
ed through the observations in order by time and estimated 
ground truth for each observation as if computing in real 
time as the observations become known. Only observations 
that occurred at the same time as or prior to each observa-
tion were used for prediction, simulating real-time opera-
tion of the system. We averaged the MSE and run time for 
each configuration (inclusion radius and inclusion percent). 
We compared the results of the various runs of the meth-
ods. The run time for the SMART method was 6336.6 ms, 
and the MSE was 0.1026. The SMART method was com-
parable in run time to IDW, but the accuracy achieved was 
far better than for any of the other methods.  
We measured the ability of each method to distinguish 
increasing percentages of the bad data from good data us-
ing an AUROC analysis. True outliers were defined as data 
that differs from ground-truth – i.e., data that was modified 
5
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-687-3
GEOProcessing 2019 : The Eleventh International Conference on Advanced Geographic Information Systems, Applications, and Services

 
to be erroneous. Predicted outliers were data that differed 
from estimated ground truth by a given threshold. We var-
ied thresholds for outlier/inlier cutoffs and compared re-
sults with the actual labels identifying whether the data was 
truly an outlier or inlier. The AUROC (area under the ROC 
curve) values are shown in Table I. The AUROC values 
show better discriminative power for the SMART method 
versus the other methods. No method will be perfect in 
identifying all errors. Some errors are small and impossible 
to distinguish from interpolation error. Known ground truth 
and known error from ground truth yields perfect labels.  
TABLE I. AUROC VALUES FOR ARTIFICIAL DATASET 
Method 
SMART 
UK 
LSR 
IDW 
AUROC 
0.827 
0.740 
0.739 
0.708 
 
Our SMART method’s computation time is comparable 
to IDW and is far better than LSR and UK, but we still 
should account for the preprocessing computation time re-
quired for determining the linear mappings and quadratic 
error functions. The overall amount of preprocessing time 
required to determine the linear mappings and quadratic 
error functions was comparable to run time required for 
UK. This was encouraging. Generation of the mappings 
will be done as an offline, batch process, so the observed 
time required is still within reason to help facilitate the 
faster and more accurate, online process. Additional bene-
fits, such as identification of bad sites and bad metadata, 
come from these mappings, further justifying the effort re-
quired. Optimization can reduce the overall time needed to 
compute the mappings. The benefits and potential to im-
prove the run time outweigh the amount of required pre-
processing time. 
B.  December 2015 MADIS California Data 
We analyzed Northern California December 2015 ambi-
ent air temperature data from the MADIS Mesonet subset. 
We used a bounding box defined by 38.5° ≤ 𝑙𝑙𝑣𝑣𝑡𝑡𝑡𝑡𝑡𝑡𝑣𝑣𝑙𝑙𝑡𝑡 ≤
42.5° and −124.5° ≤ 𝑙𝑙𝑜𝑜𝑙𝑙𝑙𝑙𝑡𝑡𝑡𝑡𝑣𝑣𝑙𝑙𝑡𝑡 ≤ −119.5°, yielding 888 
sites. We excluded observations that failed the MADIS 
Level 1 Quality Control Check. This range check restricts 
observations in degrees Fahrenheit to the interval [-
60°F,130°F]. Many values failing this check fall far outside 
the range and can have a dramatic impact on the interpola-
tion methods. Our SMART method performs very well in 
the presence of extreme bad data, and it would have easily 
out-performed the other methods in the presence of the 
range-check failed data. 
There were over 2 million observations. MADIS flagged 
73.5% of these observations as “verified” / V, slightly less 
than 4% as “questioned” / Q, and 22.5% as “screened” / S, 
indicating that it had passed the MADIS Level 1 and Level 
2 quality checks, but that the Level 3 quality checks had not 
been applied.  
Training. Verified (V) observations from the first week 
in December 2015 were used to train all methods, including 
our SMART method. In the absence of range-failed data, 
the “enhanced” (iterated subset) versions of the other algo-
rithms showed little improvement in accuracy while con-
suming excessive computation time, particularly “en-
hanced” UK. In some cases, it would have taken days to 
compute results. Because of this, we used the methods di-
rectly, without enhancement. We also tested OK (refer to 
Bailey and Gatrell [50] for further information). Since we 
do not know “ground truth” for this data, the verified data 
is the closest to ground truth. We trained all methods on 
this data to MSE of predicted versus actual. We used a 50-
mile inclusion radius due to the density of sites to avoid ex-
cessive computation time for the kriging approaches. 
The SMART mapping coefficients and derived values 
were examined for outliers, and ranges were determined for 
valid mappings. If any coefficient or derived value for a 
given SMART mapping fell outside these ranges, then the 
SMART mapping was considered bad, and that mapping 
was not used for predictions. 
Our SMART method produced significantly better re-
sults than all other methods for the training data in terms of 
estimation of ground truth measured by MSE, as shown in 
Table II. A paired, one-sided t-test was used for signifi-
cance testing using paired squared errors from predicted 
values. Only the verified (V) data was used in this compari-
son since it best approximates ground truth. The SMART 
method was compared pairwise with the other methods and 
results were aggregated over instances where both methods 
produced predictions. 
 
TABLE II. MSE FOR MADIS CALIFORNIA TRAINING DATA 
Method 
MSE 
Method 
MSE 
SMART 
2.8322 
IDW 
7.6212 
SMART 
2.8322 
LSR 
17.1446 
SMART 
2.8046 
OK 
18.4989 
SMART 
2.8046 
UK 
16.5289 
 
Testing. Testing was conducted using all data from the entire 
month of December 2015, minus the range-check-failed data. We 
computed the MSE for the verified (V) data since it best repre-
sents ground truth, but all observations were used in making esti-
mates. The testing results indicate the robustness of methods in 
the presence of bad data. In comparisons across all other methods, 
the SMART method significantly out-performed all other methods 
in terms of MSE, as shown in Table III.  
TABLE III. MSE FOR MADIS CALIFORNIA TESTING DATA 
Method 
MSE 
Method 
MSE 
SMART 
4.4611 
IDW 
9.1306 
SMART 
4.4611 
LSR 
16.5223 
SMART 
4.3360 
OK 
16.0868 
SMART 
4.3360 
UK 
14.2086 
 
We conducted an AUROC analysis to compare classifi-
cation ability of the methods based on the MADIS quality 
control flags. We considered the following flags from 
MADIS to be good/inlier data: V/verified, S/screened, 
good. The Q/questioned, was treated as bad/outlier data. 
Recall that we excluded the observations having a QC flag 
of X, those that failed the range test, from our evaluation. 
Even if we accept the MADIS quality control flags as being 
6
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-687-3
GEOProcessing 2019 : The Eleventh International Conference on Advanced Geographic Information Systems, Applications, and Services

 
correct, and we do not, this approach is problematic. The 
MADIS QC flag S corresponds to data for which not all the 
QC checks have been run. While this data had not failed 
any quality control checks that have been applied, it possi-
bly would have failed the higher-level checks.  
TABLE IV. AUROC FOR MADIS CALIFORNIA TESTING DATA 
 
AUROC 
IDW 
0.7906 
LSR 
0.7578 
SMART 
0.7317 
OK 
0.6458 
UK 
0.6062 
 
In terms of AUROC, IDW, LSR and SMART were 
comparable, with IDW finishing slightly ahead, as shown 
in Table IV. While these AUROC values seem reasonable, 
they are affected by incorrect outlier/inlier labels, and our 
SMART method suffers the greatest impact because the 
distance-based methods approximate the MADIS Level 3 
quality control check. OK and UK fall short because they 
fail to make predictions for many observations. 
C.  December 2017 MADIS Montana Data 
We investigated ambient air temperature for Western 
Montana / Northern Idaho from the MADIS Mesonet and 
the MADIS HFMetar subset in January 2017. We added the 
HFMetar data set to account for aviation AWOS/ASOS 
sites that had previously been included in the Mesonet data 
set. We used a bounding box defined by 44° ≤ 𝑙𝑙𝑣𝑣𝑡𝑡𝑡𝑡𝑡𝑡𝑣𝑣𝑙𝑙𝑡𝑡 ≤
49° and −116° ≤ 𝑙𝑙𝑜𝑜𝑙𝑙𝑙𝑙𝑡𝑡𝑡𝑡𝑣𝑣𝑙𝑙𝑡𝑡 ≤ −110°, resulting in obser-
vations from 497 sites. This bounding box is comparable in 
size to the one used for Northern California, although the 
density of sites is less. We excluded observations that failed 
the MADIS Level 1 Quality Control Check. 
All total there were over 1 million observations. MADIS 
flagged 71.2% of these observations as “verified” / V; 
10.3% of as “screened” / S, indicating that they had passed 
the MADIS Level 1 and Level 2 quality checks, but that the 
Level 3 quality checks had not been applied; and a relative-
ly large 18.5% of the data as “questioned” / Q. This is over 
four times the percentage of questioned data as there was 
for the California data set.  
Training. Verified (V) observations from the first week 
in January 2017 were used to train all methods, including 
our SMART method. We used a 100-mile inclusion radius 
due to a low density of the Montana/Idaho sites. The 
SMART mapping coefficients and derived values were ex-
amined for outliers, and bad mappings were identified as 
any mapping associated with such values. The quality of 
the mappings as measured by MSE was noticeably less 
than that for the Northern California data set. We found 
problems with many of the timestamps in this data set. 
Recognizing that much of the Idaho data comes from the 
Pacific Time Zone while the Montana data comes from the 
Mountain Time Zone, there appeared to be many sites for 
which the conversion to UTC time was not consistent. The 
Northern California data all falls within Pacific Time, and 
we did not see this problem in that data set. In terms of 
MSE, the SMART method produced significantly better 
results than each of the other methods for the training data, 
as shown in Table V.  
TABLE V. MSE FOR MADIS MONTANA TRAINING DATA 
Method 
MSE 
Method 
MSE 
SMART 
8.1513 
IDW 
16.7217 
SMART 
8.1513 
LSR 
29.8039 
SMART 
10.6726 
OK 
47.0028 
SMART 
10.6726 
UK 
33.9863 
 
Testing. Testing was conducted using data from the re-
mainder of January 2017. All data was used for this test ex-
cept for the observations that failed the MADIS Level 1 
range test. The SMART method significantly out-
performed all other methods in terms of MSE, as shown in 
Table VI.  
TABLE VI. MSE FOR MADIS MONTANA TESTING DATA 
Method 
MSE 
Method 
MSE 
SMART 
21.4714 
IDW 
38.3208 
SMART 
21.4714 
LSR 
38.5063 
SMART 
23.1496 
OK 
50.5078 
SMART 
23.1496 
UK 
36.6762 
 
We conducted an AUROC analysis to test classification 
ability based on the MADIS quality control flags in the 
same way as described for the Northern California data set 
in the previous section. As noted in that section, many of 
the MADIS QC flags are incorrect. In terms of Area Under 
the ROC curve, LSR, IDW and SMART were comparable, 
with LSR finishing ahead, as shown in Table VII. These 
AUROC values are less than those for the Northern Cali-
fornia data set at least in part because all methods adversely 
affected by incorrect outlier/inlier labels.  
TABLE VII. AUROC FOR MADIS MONTANA TESTING DATA 
Method 
LSR 
IDW 
SMART 
OK 
UK 
AUROC 
0.6900 
0.6697 
0.6393 
0.5432 
0.5476 
 
This data set includes a large percentage of observations 
(18.5%) that are flagged as “questionable” by MADIS. 
These were considered “bad” / outliers for the purposes of 
our analysis. It also includes a large percentage (10.3%) 
that are flagged as “screened” by MADIS, indicating that 
not all QC checks have been conducted. These are consid-
ered “good” / inliers for our analysis.  
There were many observations flagged as “questiona-
ble” / outliers in the HFMetar subset that should have been 
flagged as “good” / inliers. This data alone accounts for 
most of the questionable data in the data set. Aviation 
weather sites are well-maintained and regularly calibrated, 
so it is hard to believe that these sites would produce data 
that is entirely bad. We checked this data against predicted 
values, as well as neighboring sites, and it was very close, 
so it is unclear why the data was labeled as questionable. 
7
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-687-3
GEOProcessing 2019 : The Eleventh International Conference on Advanced Geographic Information Systems, Applications, and Services

 
Numerous sites were flagged by our SMART method as 
“bad” and all observations from those sites were labeled as 
bad. MADIS flagged some observations from these sites as 
good when they were close to predicted values. In some 
cases, this may have been reasonable, but in others it was a 
random occurrence. There were some sites that produced 
bad data for the training period but then produced good da-
ta for at least a portion of the test period. One could argue 
that for such sites all associated observations should be 
questioned. If a site was identified as bad by the SMART 
method, then the V and S observations would adversely 
impact the SMART method in the AUROC analysis. The 
chance situations in which the other methods came close to 
the “good” values and far from the “bad” values improved 
their performance. 
D. December 2015-2017 USGS Streamflow Data 
Mean daily streamflow (ft3/sec) was downloaded for all 
sites in Montana from the USGS [51] for every day from 
January 1st, 2015 through April 24th, 2017. There were 
145 sites having data than spanned this period, and these 
sites were analyzed. This data set is far different from the 
air temperature data used for prior analysis. Since daily av-
erages were used, there is no visible diurnal effect. There is 
a seasonal effect which varies with elevation and location 
relative to watersheds. Due to the dramatic fluctuations that 
occur in this data during times of peak runoff, the base-10 
logarithm of the data was used for analysis. 
This data set includes quality flags. Daily values are 
flagged as “A”, approved for publication, and “P”, provi-
sional and subject to revision. Values may further be 
flagged as “e” for estimated. Values transition from provi-
sional to approved after more extensive testing is conduct-
ed, so provisional values aren’t necessarily bad. These flags 
were of limited use to us and we did not use them for anal-
ysis. We treated the data as being all good and subsequent-
ly introduced errors into some of the observations, making 
them known bad. There were 122,380 total observations. 
Training. All data from 2015 was used to train all meth-
ods, including our SMART method. We assume this data, 
which was mostly “approved”, to be ground truth. We 
trained over this data to minimize MSE of predicted versus 
actual. We used a 200-mile inclusion radius.  The SMART 
mapping coefficients and derived values were examined for 
outliers. If any coefficient or derived value for a given 
SMART mapping was an outlier, then the SMART map-
ping was considered bad, and it wasn’t used for predictions. 
In terms of MSE, the SMART method produced signifi-
cantly better results than the other methods for the training 
data, as shown in Table VIII.  
TABLE VIII. MSE FOR USGS TRAINING DATA 
Method 
MSE 
Method 
MSE  
SMART 
0.0174 
IDW 
0.8751 
SMART 
0.0174 
LSR 
0.9611 
SMART 
0.0174 
OK 
0.9431 
SMART 
0.0174 
UK 
0.9617 
 
Testing. Testing was conducted using the 2016-2017 da-
ta. The SMART method significantly out-performed all 
other methods in terms of MSE, as shown in Table IX.  
TABLE IX. MSE FOR USGS TESTING DATA (NO ERRORS) 
Method 
MSE 
Method 
MSE 
SMART 
0.0429 
IDW 
0.9031 
SMART 
0.0429 
LSR 
0.9869 
SMART 
0.0429 
OK 
0.9755 
SMART 
0.0429 
UK 
0.9874 
 
Testing was then conducted using the 2016-2017 data, 
with errors introduced into 10% of the observations. A ran-
dom normal value with mean zero and standard deviation 
one was added to each of the observations in the 10% 
group. The MSE was computed relative to the known, orig-
inal observations which represent ground truth, and all ob-
servations (including bad observations) were used in mak-
ing estimates. The testing results help to indicate the ro-
bustness of methods in the presence of bad data. The 
SMART method significantly out-performed all other 
methods in terms of MSE, as shown in Table X. 
TABLE X. MSE FOR USGS TESTING DATA (WITH ERRORS) 
Method 
MSE 
Method 
MSE 
SMART 
0.0453 
IDW 
0.9103 
SMART 
0.0453 
LSR 
0.9907 
SMART 
0.0453 
OK 
0.9776 
SMART 
0.0453 
UK 
0.9914 
 
We conducted an AUROC analysis to test the methods 
on classification ability based on whether observations had 
been altered to be erroneous by our process of randomly 
selecting 10% of the observations and adding a normal ran-
dom variable with mean 0 and standard deviation 1 to those 
observations. The altered observations were labeled 
“bad”/outlier and the unaltered observations were labeled 
as “good”/inlier. Our SMART method performed far better 
than all the other methods, achieving an AUROC value of 
0.8722, as shown in Table XI. The other methods had val-
ues between 0.6 and 0.63. 
TABLE XI. AUROC VALUES FOR USGS TESTING DATA 
Method 
SMART 
IDW 
OK 
UK 
LSR 
AUROC 
0.8722 
0.6241 
0.6136 
0.6046 
0.6031 
E. Evaluation Summary 
For all four data sets and for every training and testing 
instance compared, our SMART method performed signifi-
cantly better in terms of accuracy (MSE) than all other 
methods. Its computational performance was competitive 
even though no effort was made to optimize it. For the two 
MADIS data sets, its performance for AUROC analysis of 
classification and discrimination capability showed it to be 
competitive with the best of the other methods. This com-
parison and evaluation made use of MADIS data quality 
labels for which we have found numerous problems. As 
such, all methods underperformed, and the SMART meth-
od was penalized most by mislabeling. For the other two 
8
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-687-3
GEOProcessing 2019 : The Eleventh International Conference on Advanced Geographic Information Systems, Applications, and Services

 
data sets (artificial and USGS) in which ground truth is 
known or assumed and errors were introduced relative to 
ground truth, the SMART method outperformed the other 
methods by a wide margin. This further supports our asser-
tions regarding the impact of bad labels on the MADIS da-
ta, and the need for better methods and benchmark data sets 
for data quality assessment.  
OK and UK both failed to produce estimates for many 
observations, likely due to singular matrices. They were not 
competitive in terms of run time and their accuracy was no 
better than the other methods. UK and LSR are prone to 
occasional very large errors if the predicted surface slopes 
in an extreme manner. 
Our SMART method identifies “bad sites” that chroni-
cally produce bad data, and does not use data from these 
sites in estimating ground truth for other sites. Similarly, 
data from these “bad sites” is labeled as all bad. The 
SMART method falls short in cases where a site exhibits 
chronic behavior during training but recovers to produce 
good data during a testing period. 
The USGS streamflow data exhibits correlation between 
sites, but the correlation corresponds to sites close to each 
other and in the same river/stream. Correlation will not 
necessarily be high for sites that are close but in different 
rivers. For rivers that have dams and other features that 
may influence streamflow in unusual ways, sensors will be 
correlated on each side of such features, but not as much on 
opposite sites, and certainly not as much with sites on riv-
ers that do not have similar features.  
The SMART method identifies like sites, yielding better 
correlations. IDW and LSR will not perform well in this 
circumstance. And, the kriging methods will not perform 
well either if a stationary, isotropic covariance function is 
used. Such an assumption is typical, and we used this as-
sumption in determining the covariance matrices for the 
kriging tests.  
VI. CONCLUSION 
While our SMART method out-performed the other 
methods in nearly all instances, it was not our intent present 
it as the “best” method. Instead, we present it as representa-
tive of the type of approach needed to overcome challenges 
of spatio-temporal data quality assessment.  
It makes no assumption of isotropic covariance and does 
not require the determination of a specific covariance func-
tion. While it requires preprocessing time, it is suitable for 
near-real-time, online use. It accounts for disparate report-
ing times and frequency of reporting across sites. It not on-
ly helps to identify “bad data”, but it also works well in the 
presence of bad data. It helps to identify and mitigate erro-
neous observations, “bad sites”, and bad metadata. It uses 
multiple, robust methods to mitigate the impact of bad data 
on its estimates. Other methods, such as LSR and the vari-
ous kriging approaches, could (and should) be modified in s 
similar manner to produce better, more robust results. Fur-
ther, it is important to recognize the impact of bad data 
quality labels on evaluation. It is necessary to develop and 
use benchmark datasets with known, correct data quality 
labels.  
In this research, we investigated relatively simple situa-
tions and data sets involving ambient air temperature. We 
intend to expand our work to further examine other 
measures including wind and precipitation, as well as 
CCTV camera images. Departments of Transportation use 
CCTV camera images to verify road weather conditions 
reported by sensors. Yet, these images also suffer from 
poor data quality. Further research is needed to develop 
methods for detecting bad CCTV image data and for using 
CCTV image data to confirm sensor conditions and vice-
versa. We intend to further develop benchmark datasets 
with known, good data quality labels. 
REFERENCES 
[1] NOAA, “Meteorological Assimilation Data Ingest System 
(MADIS).” 
[Online]. 
http://madis.ncep.noaa.gov/. 
[Accessed: 15-Dec-2018]. 
[2] NOAA, “MADIS Meteorological Surface Quality Control.” 
[Online]. 
https://madis.ncep.noaa.gov/madis_sfc_qc.shtml. 
[Accessed: 15-Dec-2018]. 
[3] U. 
of 
Utah, 
“MesoWest 
Data.” 
[Online]. 
http://mesowest.utah.edu/. [Accessed: 15-Dec-2018]. 
[4] U. of Utah, “MesoWest Data Variables.” [Online]. 
http://mesowest.utah.edu/cgi-bin/droman/variable_select.cgi. 
[Accessed: 26-Dec-2015]. 
[5] M. E. Splitt and J. D. Horel, “Use of multivariate linear 
regression for meteorological data analysis and quality 
assessment in complex terrain,” in Preprints, 10th Symp. on 
Meteorological Observations and Instrumentation, Phoenix, 
AZ, Amer. Meteor. Soc, 1998, pp. 359–362. 
[6] J. Han, J. Pei, and M. Kamber, Data mining: concepts and 
techniques. Elsevier, 2011. 
[7] R. Nisbet, G. Miner, and J. Elder IV, Handbook of statistical 
analysis and data mining applications. Academic Press, 
2009. 
[8] I. H. Witten, E. Frank, M. A. Hall, and C. J. Pal, Data 
Mining: Practical machine learning tools and techniques. 
Morgan Kaufmann, 2016. 
[9] P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to data 
mining. Pearson Education, Inc., 2006. 
[10] C. C. Aggarwal, Outlier Analysis. Springer Publishing 
Company, Incorporated, 2013. 
[11] S. Shekhar, C. T. Lu, and P. Zhang, “A unified approach to 
detecting spatial outliers,” Geoinformatica, vol. 7, no. 2, pp. 
139–166, 2003. 
[12] A. Klein and W. Lehner, “Representing Data Quality in 
Sensor Data Streaming Environments,” J. Data Inf. Qual., 
vol. 1, no. 2, pp. 1–28, 2009. 
[13] A. Klein and W. Lehner, “How to Optimize the Quality of 
Sensor Data Streams,” Proc. 2009 Fourth Int. Multi-
Conference Comput. Glob. Inf. Technol. 00, pp. 13–19, 2009. 
[14] A. Klein, “Incorporating quality aspects in sensor data 
streams,” Proc. {ACM} first {Ph.D.} Work. {CIKM}, pp. 77–
84, 2007. 
[15] A. Klein, H. H. Do, G. Hackenbroich, M. Karnstedt, and W. 
Lehner, “Representing data quality for streaming and static 
data,” Proc. - Int. Conf. Data Eng., pp. 3–10, 2007. 
[16] A. Klein and G. Hackenbroich, “How to Screen a Data 
9
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-687-3
GEOProcessing 2019 : The Eleventh International Conference on Advanced Geographic Information Systems, Applications, and Services

 
Stream.” 
[Online]. 
http://mitiq.mit.edu/ICIQ/Documents/IQ%20Conference%20
2009/Papers/3-A.pdf. [Accessed: 15-Dec-2018]. 
[17] S. L. Barnes, “A technique for maximizing details in 
numerical weather map analysis,” J. Appl. Meteorol., vol. 3, 
no. 4, pp. 396–409, 1964. 
[18] D. Shepard, “A two-dimensional interpolation function for 
irregularly-spaced data,” 23rd ACM Natl. Conf., pp. 517–
524, 1968. 
[19] M.E. Splitt and J. Horel, “Use of Multivariate Linear 
Regression for Meteorological Data Analysis and Quality 
Assessment 
in 
Complex 
Terrain.” 
[Online]. 
http://mesowest.utah.edu/html/help/regress.html. [Accessed: 
15-Dec-2018]. 
[20] U. of Utah, “MesoWest Quality Control Flags Help Page.” 
[Online]. 
http://mesowest.utah.edu/html/help/key.html. 
[Accessed: 15-Dec-2015]. 
[21] NOAA, 
“MADIS 
Quality 
Control.” 
[Online]. 
http://madis.ncep.noaa.gov/madis_qc.html. [Accessed: 15-
Dec-2018]. 
[22] S. L. Belousov, L. S. Gandin, and S. A. Mashkovich, 
“Computer Processing of Current Meteorological Data, 
Translated from Russian to English by Atmospheric 
Environment Service,” Nurklik, Meteorol. Transl., no. 18, p. 
227, 1972. 
[23] D. Zimmerman, C. Pavlik, A. Ruggles, and M. P. Armstrong, 
“An experimental comparison of ordinary and universal 
kriging and inverse distance weighting,” Math. Geol., vol. 
31, no. 4, pp. 375–390, 1999. 
[24] G. Y. Lu and D. W. Wong, “An adaptive inverse-distance 
weighting spatial interpolation technique,” Comput. Geosci., 
vol. 34, no. 9, pp. 1044–1055, 2008. 
[25] T. G. Mueller, et al., “Map Quality for Ordinary Kriging and 
Inverse Distance Weighted Interpolation,” Soil Sci. Soc. Am. 
J., vol. 68, no. 6, p. 2042, 2004. 
[26] D. E. Galarus, R. A. Angryk, and J. W. Sheppard, 
“Automated Weather Sensor Quality Control.,” FLAIRS 
Conf., pp. 388–393, 2012. 
[27] D. E. Galarus and R. A. Angryk, “Mining robust 
neighborhoods for quality control of sensor data,” Proc. 4th 
ACM SIGSPATIAL Int. Work. GeoStreaming (IWGS ’13), pp. 
86–95, Nov. 2013. 
[28] D. E. Galarus and R. A. Angryk, “A SMART Approach to 
Quality Assessment of Site-Based Spatio-Temporal Data,” in 
Proceedings of the 24th ACM SIGSPATIAL International 
Conference on Advances in Geographic Information Systems 
(GIS ’16), Nov. 2016, pp 1-4. 
[29] D. E. Galarus and R. A. Angryk, “The SMART Approach to 
Comprehensive Quality Assessment of Site-Based Spatial-
Temporal Data,” in 2016 IEEE International Conference on 
Big Data (Big Data), 2016, pp. 2636–2645. 
[30] D. E. Galarus and R. A. Angryk, “Beyond Accuracy - A 
SMART Approach to Site-Based Spatio-Temporal Data 
Quality Assessment,”. Intell. Data Anal., vol. 22, no. 1, 
2018, pp 21-43. 
[31] D. E. Galarus and R. A. Angryk, “Quality Control from the 
Perspective of the Real-Time Spatial-Temporal Data 
Aggregator and (re)Distributor,” in Proceedings of the 22nd 
ACM SIGSPATIAL International Conference on Advances in 
Geographic Information Systems (SIGSPATIAL ’14), 2014, 
pp. 389–392. 
[32] D. E. Galarus and R. A. Angryk, “Spatio-temporal quality 
control: implications and applications for data consumers and 
aggregators,” Open Geospatial Data, Softw. Stand., vol. 1, 
no. 1, p. 1, 2016. 
[33] H. Xie, K. T. McDonnell, and H. Qin, “Surface 
reconstruction of noisy and defective data sets,” in 
Proceedings of the conference on Visualization’04, 2004, pp. 
259–266. 
[34] L. Li, X. Zhou, M. Kalo, and R. Piltner, “Spatiotemporal 
interpolation methods for the application of estimating 
population exposure to fine particulate matter in the 
contiguous US and a Real-Time web application,” Int. J. 
Environ. Res. Public Health, vol. 13, no. 8, p. 749, 2016. 
[35] J. Grieser, “Interpolation of Global Monthly Rain Gauge 
Observations for Climate Change Analysis,” J. Appl. 
Meteorol. Climatol., vol. 54, no. 7, pp. 1449–1464, 2015. 
[36] N. Cressie, “The origins of kriging,” Math. Geol., vol. 22, 
no. 3, pp. 239–252, 1990. 
[37] H. Wackernagel, Multivariate geostatistics: an introduction 
with applications. Springer Science & Business Media, 2013. 
[38] M. S. Handcock and M. L. Stein, “A Bayesian analysis of 
kriging,” Technometrics, vol. 35, no. 4, pp. 403–410, 1993. 
[39] G. J. Hunter, A. K. Bregt, G. B. M. Heuvelink, S. De Bruin, 
and K. Virrantaus, “Spatial data quality: problems and 
prospects,” in Research trends in geographic information 
science, Springer, 2009, pp. 101–121. 
[40] E. H. Isaaks and R. M. Srivastava, An introduction to applied 
geostatistics. Oxford University Press, 1989. 
[41] C. Huijbregts and G. Matheron, “Universal kriging (an 
optimal method for estimating and contouring in trend 
surface analysis),” in Proceedings of Ninth International 
Symposium on Techniques for Decision-making in the 
Mineral Industry, 1971. 
[42] M. Galassi and Et-al, GNU Scientific Library Reference 
Manual (3rd Ed.). Free Software Foundation. 
[43] G. Bohling, “Introduction to Geostatistics and Variogram 
Analysis.” 
[Online]. 
http://people.ku.edu/~gbohling/cpe940/Variograms.pdf. 
[44] P. J. Rousseeuw and K. Van Driessen, “Computing LTS 
regression for large data sets,” Data Min. Knowl. Discov., 
vol. 12, no. 1, pp. 29–45, 2006. 
[45] R. F. Voss, “Random fractal forgeries,” in Fundamental 
algorithms for computer graphics, Springer, 1985, pp. 805–
835. 
[46] J. Feder, Fractals. Springer Science & Business Media, 
2013. 
[47] M. F. Barnsley et al.., The science of fractal images. Springer 
Publishing Company, Incorporated, 2011. 
[48] M. F. Goodchild and S. Gopal, The accuracy of spatial 
databases. CRC Press, 1989. 
[49] D. E. Galarus, “Modeling stock market returns with local 
iterated function systems,” 1995. 
[50] T. C. Bailey and A. C. Gatrell, Interactive spatial data 
analysis, vol. 413. Longman Scientific & Technical Essex, 
1995. 
[51] USGS, “USGS Water Data for the Nation.” [Online]. 
https://waterdata.usgs.gov/nwis/. [Accessed: 15-Dec-2018]. 
 
10
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-687-3
GEOProcessing 2019 : The Eleventh International Conference on Advanced Geographic Information Systems, Applications, and Services

