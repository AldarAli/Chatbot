Recursive Least-Squares Algorithms for Echo Cancellation
An Overview and Open Issues
Camelia Elisei-Iliescu and Constantin Paleologu
University Politehnica of Bucharest
Bucharest, Romania
Email: pale@comm.pub.ro
Abstract—The recursive least-squares (RLS) algorithm is very
popular in many applications of adaptive ﬁltering, especially
due to its fast convergence rate. However, the computational
complexity of this algorithm represents a major limitation in some
applications that involve high length adaptive ﬁlters, like echo
cancellation. Moreover, the speciﬁc features of this application
require good tracking capabilities and double-talk robustness for
the adaptive algorithm, which further implies an optimization
process on its parameters. In case of most RLS-based algorithms,
the performance can be controlled in terms of two main param-
eters, i.e., the forgetting factor and the regularization term. The
goal of this paper is to outline the inﬂuence of these parameters
on the overall performance of the RLS algorithms and to present
several solutions to control their behavior, taking into account the
speciﬁc requirements of echo cancellation application.
Keywords–Adaptive ﬁlters; Echo cancellation; Recursive least-
squares (RLS) algorithm.
I.
INTRODUCTION
The recursive least-squares (RLS) algorithm [1][2] is one
of the most popular adaptive ﬁlters. As compared to the
normalized least-mean-square (NLMS) algorithm [1][2], the
RLS offers a superior convergence rate especially for highly
correlated input signals. Of course, there is a price to pay
for this advantage, which is an increase in the computational
complexity. For this reason, it is not very often involved in
echo cancellation [3][4], where high length adaptive ﬁlters
(e.g., hundreds of coefﬁcients) are required.
The performance of the RLS algorithm is mainly controlled
by two important parameters, i.e., the forgetting factor and
the regularization term. Similar to the attributes of the step-
size from the NLMS-based algorithms, the performance of
RLS-type algorithms in terms of convergence rate, tracking,
misadjustment, and stability depends on the forgetting factor
[1][2]. The classical RLS algorithm uses a constant forgetting
factor (between 0 and 1) and needs to compromise between
the previous performance criteria. When the forgetting factor
is very close to one, the algorithm achieves low misadjustment
and good stability, but its tracking capabilities are reduced [5].
A small value of the forgetting factor improves the tracking
but increases the misadjustment, and could affect the stability
of the algorithm [6]. Motivated by these aspects, a number
of variable forgetting factor RLS (VFF-RLS) algorithms have
been developed, e.g., [7]–[10] (and references therein).
It should be mentioned that in the context of system
identiﬁcation (like in echo cancellation), where the output of
the unknown system is corrupted by another signal (which is
usually an additive noise), the goal of the adaptive ﬁlter is
not to make the error signal goes to zero, because this will
introduce noise in the adaptive ﬁlter. The objective instead is
to recover the “corrupting signal” from the error signal of the
adaptive ﬁlter after this one converges to the true solution. This
was the approach behind the VFF-RLS algorithm proposed in
[9], which is analyzed in Section II.
As compared to the forgetting factor, the regularization
parameter has been less addressed in the literature. Apparently,
it is required in matrix inversion when this matrix is ill con-
ditioned, especially in the initialization stage of the algorithm.
However, its role is of great importance in practice, since reg-
ularization is a must in all ill-posed problems (like in adaptive
ﬁltering), especially in the presence of additive noise [11]–[14].
Consequently, in Section III, we focus on the regularized RLS
algorithm [2]. Following the development from [12], a method
to select an optimal regularization parameter is presented, so
that the algorithm could behave well in all noisy conditions.
Since the value of this parameter is related to the echo-to-noise
ratio (ENR), a simple and practical way to estimate the ENR in
practice is also presented, which leads to a variable-regularized
RLS (VR-RLS) algorithm.
The simulation results (presented in Section IV) are per-
formed in the context of echo cancellation and support the
theoretical ﬁndings. Finally, the conclusions are outlined in
Section V, together with some open issues related to future
works.
II.
VARIABLE FORGETTING FACTOR RLS ALGORITHM
Let us consider a system identiﬁcation problem (like in
echo cancellation), where the desired signal at the discrete-
time index n is obtained as
d(n) = hT x(n) + v(n)
= y(n) + v(n),
(1)
where h = [ h0
h1
· · ·
hL−1 ]T is the impulse response
(of length L) of the system that we need to identify (i.e., the
echo path), superscript T denotes transpose of a vector or a
matrix,
x(n) = [ x(n)
x(n − 1)
· · ·
x(n − L + 1) ]T
(2)
is a vector containing the most recent L samples of the zero-
mean input signal x(n) (i.e., the far-end signal), v(n) is
a zero-mean additive noise signal [which is independent of
x(n)], and y(n) represents the output of the unknown system
(i.e., the echo signal). In the context of echo cancellation,
the output of the echo path could be also corrupted by the
87
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-546-3
ICN 2017 : The Sixteenth International Conference on Networks (includes SOFTNETWORKING 2017)

near-end speech (besides the background noise), which is
usually known as the double-talk scenario [3][4]. The main
objective is to estimate or identify h with an adaptive ﬁlter
bh(n) =
[ bh0(n)
bh1(n)
· · ·
bhL−1(n)
]T .
Using the previous notation we may deﬁne the a priori
error signal as
e(n)
=
d(n) − xT (n)bh(n − 1)
=
xT (n)
[
h − bh(n − 1)
]
+ v(n).
(3)
In this context, the relations that deﬁne the classical RLS
algorithm are:
k(n)
=
P(n − 1)x(n)
λ + xT (n)P(n − 1)x(n),
(4)
bh(n)
=
bh(n − 1) + k(n)e(n),
(5)
P(n)
=
1
λ
[
P(n − 1) − k(n)xT (n)P(n − 1)
]
,
(6)
where λ (0 < λ ≤ 1) is the exponential forgetting factor,
k(n) is the Kalman gain vector, P(n) is the estimate of the
inverse of the input correlation matrix, and e(n) is the a priori
error signal deﬁned in (3). The a posteriori error signal can be
deﬁned using the adaptive ﬁlter coefﬁcients at time n, i.e.,
ε(n)
=
d(n) − xT (n)bh(n)
(7)
=
xT (n)
[
h − bh(n)
]
+ v(n),
Using (3) and (5) in (7), it results in
ε(n) = e(n)
[
1 − xT (n)k(n)
]
.
(8)
In the framework of system identiﬁcation, it is desirable to re-
cover the system noise from the error signal [5]. Consequently,
we can impose the condition:
E
[
ε2(n)
]
= σ2
v,
(9)
where E[·] denotes mathematical expectation and σ2
v
=
E
[
v2(n)
]
is the power of the system noise. Furthermore, using
(9) in (8) and taking (4) into account, it ﬁnally results in
E
{[
1 −
θ(n)
λ(n) + θ(n)
]2}
=
σ2
v
σ2e(n),
(10)
where θ(n) = xT (n)P(n − 1)x(n). In (10), we assumed that
the input and error signals are uncorrelated, which is true when
the adaptive ﬁlter has started to converge to the true solution.
We also assumed that the forgetting factor is deterministic
and time dependent. By solving the quadratic equation (10), it
results a variable forgetting factor
λ(n) =
σθ(n)σv
σe(n) − σv
,
(11)
where E
[
θ2(n)
]
= σ2
θ(n). In practice, the variance of the
error signal can be recursively estimated based on
bσ2
e(n) = αbσ2
e(n − 1) + (1 − α)e2(n),
(12)
where α = 1 − 1/(KL), with K ≥ 1. The variance of θ(n) is
evaluated in a similar manner, i.e.,
bσ2
θ(n) = αbσ2
θ(n − 1) + (1 − α)θ2(n).
(13)
The estimate of the noise power, bσ2
v(n) [which should be used
in (11) from practical reasons], can be estimated in different
ways, e.g., [9][15][16].
Theoretically, σe(n) ≥ σv in (11). Compared to the NLMS
algorithm (where there is the gradient noise, so that σe(n) >
σv), the RLS algorithm with λ(n) ≈ 1 leads to σe(n) ≈ σv.
In practice (since power estimates are used), several situations
have to be prevented in (11). Apparently, when bσe(n) ≤ bσv, it
could be set λ(n) = λmax, where λmax is very close or equal
to 1. But this could be a limitation, because in the steady-state
of the algorithm bσe(n) varies around bσv. A more reasonable
solution is to impose that λ(n) = λmax when
bσe(n) ≤ ρbσv,
(14)
with 1 < ρ ≤ 2. Otherwise, the forgetting factor of the VFF-
RLS algorithm [9] is evaluated as
λ(n) = min
[
bσθ(n)bσv(n)
ζ + |bσe(n) − bσv(n)|, λmax
]
,
(15)
where the small positive constant ζ prevents a division by zero.
Before the algorithm converges or when there is an abrupt
change of the system, bσe(n) is large as compared to bσv(n);
thus, the parameter λ(n) from (15) takes low values, providing
fast convergence and good tracking. When the algorithm
converges to the steady-state solution, bσe(n) ≈ bσv(n) [so that
condition (14) is fulﬁlled] and λ(n) is equal to λmax, providing
low misadjustment. It can be noticed that the mechanism that
controls the forgetting factor is very simple and not expensive
in terms of multiplications and additions.
III.
VARIABLE REGULARIZED RLS ALGORITHM
In this section, a different version of the RLS algorithm
is presented, which allow us to outline the importance of the
regularization parameter. Let us consider the regularized least-
squares criterion:
J(n) =
n
∑
i=0
λn−i [
d(i) − bhT (n)x(i)
]2
+ δ
bh(n)

2 ,
(16)
where λ is the same exponential forgetting factor, δ is the
regularization parameter, and ∥·∥2 is the ℓ2 norm. From (16),
the update of the regularized RLS algorithm [2] results in
bh(n) = bh(n − 1) +
[
bRx(n) + δIL
]−1
x(n)e(n),
(17)
where
bRx(n) =
n
∑
i=0
λn−ix(i)xT (i)
= λ bRx(n − 1) + x(n)xT (n)
(18)
is an estimate of the correlation matrix of x(n) at time n, IL
is the identity matrix of size L × L, and
e(n)
=
d(n) − bhT (n − 1)x(n)
=
d(n) − by(n)
(19)
is the a priori error signal as deﬁned in (3); the signal by(n)
represents the output of the adaptive ﬁlter, which should be an
estimate of the echo signal. We will assume that the matrix
bRx(n) has full rank, although it can be very ill conditioned.
As a result, if there is no noise, regularization is not really
88
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-546-3
ICN 2017 : The Sixteenth International Conference on Networks (includes SOFTNETWORKING 2017)

required; however, the more the noise, the larger should be
the value of δ.
Summarizing, the regularized RLS algorithm is deﬁned
by the relations (17)–(19). In the following, we present one
reasonable way to ﬁnd the regularization parameter δ. It can
be noticed that the update equation of the regularized RLS can
be rewritten as [12]
bh(n) = Q(n)bh(n − 1) + eh(n),
(20)
where
Q(n) = IL −
[
bRx(n) + δIL
]−1
x(n)xT (n)
(21)
and
eh(n) =
[
bRx(n) + δIL
]−1
x(n)d(n)
(22)
is the correctiveness component of the algorithm, which de-
pends on the new observation d(n). In this context, we can
notice that Q(n) does not depend on the noise signal and
Q(n)bh(n − 1) in (20) can be seen as a good initialization of
the adaptive ﬁlter. In fact, (22) is the solution of the noisy
linear system of L equations:
[
bRx(n) + δIL
]
eh(n) = x(n)d(n).
(23)
Let us deﬁne
ee(n) = d(n) − ehT (n)x(n),
(24)
the error signal between the desired signal and the estimated
signal obtained from the ﬁlter optimized in (22). Consequently,
we could ﬁnd δ in such a way that the expected value of ee2(n)
is equal to the variance of the noise, i.e.,
E
[
ee2(n)
]
= σ2
v.
(25)
This is reasonable if we want to attenuate the effects of the
noise in the estimator eh(n).
For the sake of simplicity, let us assume that x(n) is
stationary and white. Apparently, this assumption is quite
restrictive, even if it was widely used in many developments in
the context of adaptive ﬁltering [1][2]. However, the resulting
VR-RLS algorithm will still use the full matrix bRx(n) and,
consequently, it will inherit the good performance feature of
the RLS family in case of correlated inputs. In this case and
for n large enough (also considering that the forgetting factor
λ is on the order of 1 − 1/L), we have
[
bRx(n) + δIL
]
≈
[ σ2
x
1 − λ + δ
]
IL
≈
[
Lσ2
x + δ
]
IL
(26)
and xT (n)x(n) ≈ Lσ2
x, where σ2
x = E
[
x2(n)
]
is the variance
of the input signal. Next, from (1), we can deﬁne the echo-to-
noise ratio (ENR) as
ENR = σ2
y
σ2v
,
(27)
where σ2
y = E
[
y2(n)
]
is the variance of y(n). Developing
(25) and based on the previous approximations, we obtain the
quadratic equation:
δ2 − 2 Lσ2
x
ENRδ −
(
Lσ2
x
)2
ENR
= 0,
(28)
with the obvious solution:
δ = L
(
1 + √1 + ENR
)
ENR
σ2
x
= βσ2
x,
(29)
where
β = L
(
1 + √1 + ENR
)
ENR
(30)
is the normalized regularization parameter of the RLS algo-
rithm.
As we can notice from (29), the regularization parameter
δ depends on three elements, i.e., the length of the adaptive
ﬁlter, the variance of the input signal, and the ENR. In most
applications, the ﬁrst two elements (L and σ2
x) are known,
while the ENR can be estimated. Using a proper evaluation of
the ENR, the algorithm should own good robustness features
against the additive noise.
Let us assume that the adaptive ﬁlter has converged to a
certain degree, so that we can use the approximation
y(n) ≈ by(n).
(31)
Hence,
σ2
y ≈ σ2
by,
(32)
where σ2
by = E
[
by2(n)
]
. Since the output of the unknown
system and the noise can be considered uncorrelated, (1) can
be expressed in terms of power estimates as
σ2
d = σ2
y + σ2
v,
(33)
where σ2
d = E
[
d2(n)
]
. Using (32) in (33), we obtain
σ2
v ≈ σ2
d − σ2
by.
(34)
The power estimates can be evaluated in a recursive manner
as
bσ2
d(n) = αbσ2
d(n − 1) + (1 − α)d2(n),
(35)
bσ2
by(n) = αbσ2
by(n − 1) + (1 − α)by2(n),
(36)
where α = 1 − 1/(KL), with K ≥ 1 [similar to (12) and
(13)]. Therefore, based on (32), (34), and (35), an estimation
of the ENR is obtained as
d
ENR(n) =
bσ2
by(n)
|bσ2
d(n) − bσ2
by(n)|,
(37)
so that the variable regularization parameter results in
δ(n)
=
L
[
1 +
√
1 + d
ENR(n)
]
d
ENR(n)
σ2
x
=
β(n)σ2
x,
(38)
where
β(n) =
L
[
1 +
√
1 + d
ENR(n)
]
d
ENR(n)
(39)
89
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-546-3
ICN 2017 : The Sixteenth International Conference on Networks (includes SOFTNETWORKING 2017)

0
20
40
60
80
100
120
−0.05
−0.04
−0.03
−0.02
−0.01
0
0.01
0.02
0.03
0.04
0.05
Samples
Amplitude
Figure 1. Impulse response used in simulations (the fourth echo path from
G168 Recommendation [17]).
is the variable normalized regularization parameter. Conse-
quently, based on (38), we obtain a variable-regularized RLS
(VR-RLS) algorithm, with the update:
bh(n) = bh(n − 1) +
[
bRx(n) + δ(n)IL
]−1
x(n)e(n),
(40)
where bRx(n) is recursively evaluated according to (18) and
δ(n) is computed based on (35)–(38).
Finally, some practical issues should be outlined. The
absolute values in (37) prevent any minor deviations (due to the
use of power estimates) from the true values, which can make
the denominator negative. It is a non-parametric algorithm,
since all the parameters in (37) are available. Also, good
robustness against the additive noise variations is expected.
The main drawback is due to the approximation in (32). This
assumption will be biased in the initial convergence phase or
when there is a change of the unknown system. Concerning
the initial convergence, we can use a constant regularization
parameter δ in the ﬁrst steps of the algorithm (e.g., in the ﬁrst
L iterations).
IV.
SIMULATION RESULTS
Let us consider a network echo cancellation scenario, in
the framework of G168 Recommendation [17]. The echo path
is depicted in Figure 1; it is the fourth impulse response
(of length L = 128) from the above recommendation. The
sampling rate is 8 kHz. All adaptive ﬁlters used in the
experiments have the same length as the echo path. The far-end
signal (i.e., the input signal) is a speech signal. The output of
the echo path is corrupted by an independent white Gaussian
noise with 20 dB ENR. An echo path change scenario is some
experiments (in order to evaluate the tracking capabilities of
the algorithms), by shifting the impulse response to the right
by 8 samples in the middle of simulation. The performance
measure is the normalized misalignment (in dB) evaluated as
Mis(n) = 20log10
h(n) − bh(n)

2
∥h(n)∥2
.
(41)
0
2
4
6
8
10
−35
−30
−25
−20
−15
−10
−5
0
5
Time (seconds)
Misalignment (dB)
 
 
RLS, λ = 1 − 1/L
RLS, λ = 1
VFF−RLS
Figure 2. Misalignment of the RLS algorithm (using different constant
values of the forgetting factor) and VFF-RLS algorithm in a single-talk
scenario, including echo path change.
0
2
4
6
8
10
−30
−25
−20
−15
−10
−5
0
5
Time (seconds)
Misalignment (dB)
 
 
RLS, β20
RLS, β0
VR−RLS
Figure 3. Misalignment of the regularized RLS algorithm (using different
constant values of the regularization parameter) and VR-RLS algorithm in a
single-talk scenario, including echo path change.
In the ﬁrst the experiment, the performance of the VFF-
RLS algorithm (presented in Section II) is evaluated, as
compared to the classical RLS algorithm deﬁned in (4)–(6),
which uses different constant values of the forgetting factor.
A single-talk case is considered and the echo path changes in
the middle of simulation. It can be noticed in Figure 2 that the
VFF-RLS algorithm achieves the same initial misalignment as
the RLS with its maximum forgetting factor, but it tracks as
fast as the RLS with the smaller forgetting factor. As expected,
the classical RLS algorithm using constant forgetting factors
has to compromise between these performance criteria, i.e.,
the larger the value of λ, the better the misalignment level but
worse the tracking capability.
Next, the performance of the VR-RLS algorithm (from
Section III) is investigated, as compared to the regularized
RLS algorithm deﬁned in (17)–(19), using different constant
values of the regularization parameter. In real-world applica-
tions, the value of ENR is not available. However, based on
90
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-546-3
ICN 2017 : The Sixteenth International Conference on Networks (includes SOFTNETWORKING 2017)

0
2
4
6
8
10
−30
−20
−10
0
10
20
30
Time (seconds)
Misalignment (dB)
 
 
RLS, β20
RLS, β0
VR−RLS
Figure 4. Misalignment of the regularized RLS algorithm (using different
constant values of the regularization parameter) and VR-RLS algorithm in a
double-talk scenario.
(30), we can determine the values of the optimal normalized
regularization parameter of the RLS algorithm for different
cases; for example, let us consider two values of the ENR, i.e.,
20 dB (the true one) and 0 dB. Using appropriate notation,
we obtain β20 = 14.14 and β0 = 309.01, respectively. In
the next set of experiments, we compare the regularized RLS
algorithm using these constant regularization parameters with
the VR-RLS algorithm. The constant forgetting factor is set to
λ = 1 − 1/(3L) for all the algorithms.
In Figure 3, a single-talk scenario is considered and an echo
path change is introduced in the middle of the simulation. It
can be noticed that the VR-RLS algorithm behaves similarly to
the RLS algorithm using the constant parameter β20, which is
associated to the value of the true ENR. Also, it can be noticed
that a larger value of the normalized regularization parameter
(β0) improves the misalignment but affects the convergence
rate and tracking.
In Figure 4, a double-talk scenario [3][4] is considered.
The near-end speech appears between time 2.5 and 5 seconds,
so that the signal v(n) is now non-stationary, since it contains
both noise and speech. It is clear that the VR-RLS algorithm
is more robust in this case as compared to the regularized RLS
using constant values of β. It should be outlined that we do
not use any double-talk detector (DTD) [3][4] with the VR-
RLS algorithm, which is the regular approach in a double-
talk situation. Therefore, the VR-RLS algorithm owns good
robustness features against double-talk, which is an important
gain in practice.
V.
CONCLUSIONS AND PERSPECTIVES
The RLS algorithms are very appealing due to their fast
convergence rate. In this paper, we have focused on the main
parameters that control the performance of these algorithms,
i.e., the forgetting factor and the regularization term. In order
to achieve a better compromise between the performance
criteria (i.e., convergence and tracking versus misadjustment
and robustness), these parameters could be controlled. In this
context, the solutions presented in Sections II and III led to
the VFF-RLS and VR-RLS algorithms, respectively.
The experiments were performed in the context of echo
cancellation, which is a very challenging system identiﬁcation
problem. According to the simulation results, the VFF-RLS
and VR-RLS algorithms perform very well as compared to
their classical counterparts (which use constant values of the
key parameters). On the other hand, the complexity of the
RLS-based algorithms is O(L2), which represents a problem-
atic issue for high values of L (like in echo cancellation). The
alternative is to combine these VFF and VR methods with low-
complexity versions of the RLS algorithm, e.g., [18]. Also,
another interesting issue to address in future works could be
a combination between the VFF and VR approaches, in order
to inherent the advantages of both methods.
ACKNOWLEDGMENT
This work was supported by UEFISCDI Romania under
Grant PN-II-RU-TE-2014-4-1880.
REFERENCES
[1]
S. Haykin, Adaptive Filter Theory. Fourth Edition, Upper Saddle River,
NJ: Prentice-Hall, 2002.
[2]
A. H. Sayed, Adaptive Filters. New York, NY: Wiley, 2008.
[3]
J. Benesty, T. Gaensler, D. R. Morgan, M. M. Sondhi, and S. L.
Gay, Advances in Network and Acoustic Echo Cancellation. Berlin,
Germany: Springer-Verlag, 2001.
[4]
C. Paleologu, J. Benesty, and S. Ciochin˘a, Sparse Adaptive Filters for
Echo Cancellation. Morgan & Claypool Publishers, 2010.
[5]
S. Ciochin˘a, C. Paleologu, J. Benesty, and A. A. Enescu, “On the
inﬂuence of the forgetting factor of the RLS adaptive ﬁlter in system
identiﬁcation,” in Proc. IEEE ISSCS, 2009, pp. 205–208.
[6]
S. Ciochin˘a, C. Paleologu, and A. A. Enescu, “On the behaviour of
RLS adaptive algorithm in ﬁxed-point implementation,” in Proc. IEEE
ISSCS, 2003, pp. 57–60.
[7]
S. Song, J. S. Lim, S. J. Baek, and K. M. Sung, “Gauss Newton variable
forgetting factor recursive least squares for time varying parameter
tracking,” Electronics Lett., vol. 36, pp. 988–990, May 2000.
[8]
S.-H. Leung and C. F. So, “Gradient-based variable forgetting factor
RLS algorithm in time-varying environments,” IEEE Trans. Signal
Processing, vol. 53, pp. 3141–3150, Aug. 2005.
[9]
C. Paleologu, J. Benesty, and S. Ciochin˘a, “A robust variable forgetting
factor recursive least-squares algorithm for system identiﬁcation,” IEEE
Signal Processing Lett., vol. 15, pp. 597–600, 2008.
[10]
Y. J. Chu and S. C. Chan, “A new local polynomial modeling-based
variable forgetting factor RLS algorithm and its acoustic applications,”
IEEE/ACM Trans. Audio, Speech, Language Processing, vol. 23, pp.
2059–2069, Nov. 2015.
[11]
P. C. Hansen, Rank-Deﬁcient and Discrete Ill-Posed Problems: Numer-
ical Aspects of Linear Inversion. Philadelphia, PA: SIAM, 1998.
[12]
J. Benesty, C. Paleologu, and S. Ciochin˘a, “Regularization of the RLS
algorithm,” IEICE Trans. Fundamentals, vol. E94-A, pp. 1628–1629,
Aug. 2011.
[13]
Y. V. Zakharov and V. H. Nascimento, “Sparse sliding-window RLS
adaptive ﬁlter with dynamic regularization,” in Proc. EUSIPCO, 2016,
pp. 145–149.
[14]
C. Elisei-Iliescu, C. Stanciu, C. Paleologu, J. Benesty, C. Anghel, and
S. Ciochin˘a, “Robust variable regularized RLS algorithms,” in Proc.
IEEE HSCMA, 2017, pp. 171–175.
[15]
C. Paleologu, S. Ciochin˘a, and J. Benesty, “Variable step-size NLMS
algorithm for under-modeling acoustic echo cancellation,” IEEE Signal
Processing Lett., vol. 15, pp. 5–8, 2008.
[16]
M. A. Iqbal and S. L. Grant, “Novel variable step size NLMS algorithms
for echo cancellation,” in Proc. IEEE ICASSP, 2008, pp. 241–244.
[17]
Digital Network Echo Cancellers, ITU-T Rec. G.168, 2002.
[18]
Y. V. Zakharov, G. P. White, and J. Liu, “Low-complexity RLS algo-
rithms using dichotomous coordinate descent iterations,” IEEE Trans.
Signal Processing, vol. 56, pp. 3150–3161, July 2008.
91
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-546-3
ICN 2017 : The Sixteenth International Conference on Networks (includes SOFTNETWORKING 2017)

