418
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Protecting Against Reﬂected Cross-Site Scripting
Attacks
P˚al Ellingsen and Andreas Svardal Vikne
Department of Computing, Mathematics and Physics
Western Norway University of Applied Sciences
Bergen, Norway
Email: pal.ellingsen@hvl.no, andreas.vikne@student.hvl.no
Abstract—One of the most dominant threats against web applica-
tions is the class of script injection attacks, also called cross-site
scripting. This class of attacks affects the client-side of a web
application, and is a critical vulnerability that is difﬁcult to both
detect and remediate for websites, often leading to insufﬁcient
server-side protection, which is why the end-users need an extra
layer of protection at the client-side, utilizing the defense in
depth strategy. This paper discusses a client-side ﬁlter for Mozilla
Firefox that protects against Reﬂected cross-site scripting attacks,
while maintaining high performance. By conducting tests on the
implemented solution, the conclusion is that the ﬁlter does provide
more protection than the original Firefox version, at the same
time achieving high performance, which with only some further
improvements would become an effective option for end-users of
web applications to protect themselves against Reﬂected cross-site
scripting attacks.
Keywords–Cross-site scripting protection; input ﬁltering; soft-
ware security; injection attacks.
I.
INTRODUCTION
A. Background
Cross-Site Scripting (XSS) has for long been among the top
threats to Internet security as deﬁned in numerous reports con-
taining detailed information about the prevalence and danger
regarding this class of vulnerability. Based on these existing
results, a ﬁltering solution for Firefox was ﬁrst proposed by the
authors of this paper in ”Client-Side XSS Filtering in Firefox”
[1]. This paper builds on the same work and expands on the
results given there.
One of the reports that underpins the need for better
XSS attack protection is the ”Open Web Application Security
Project (OWASP) Top 10 - 2017” report, which contains a
list of the 10 most critical web application security risks [2].
Even though XSS has fallen to a 7th place in the ”OWASP
Top 10 - 2017” report [2], XSS still remains one of the most
serious attack forms. Another report, being published annually
for the past 12 years, by WhiteHat Security, called “2017 -
WhiteHat Security Application Security Statistics Report” [3],
also identiﬁes that XSS is among the top two most critical
web vulnerabilities. An interesting and troubling observation
made in this report is that even though XSS is considered one
of the most critical vulnerabilities, it is not being prioritized
for remediation by most websites. The statistics referred above
suggest that the vulnerabilities receiving the most attention are
vulnerabilities that are easy to ﬁx, which is not the case for
XSS. As a result of this, we would suggest that organizations
must adopt a risk-based remediation process, which means that
the most critical vulnerabilities should be prioritized ﬁrst, like
XSS. A report [4] published by Bugcrowd Inc., a web-based
platform that uses crowd-sourced security for companies to
identify vulnerabilities in their web applications, has analyzed
the data from their platform, including information about the
most common vulnerabilities found. The data in their report
is based on all BugCrowd’s collected data from January 2013
through March 2017, which contains over 96 000 submissions,
where the by far most reported vulnerability is XSS with a
submission rate of 25%. They also have data on the most
critical vulnerabilities sorted by type, where XSS is considered
the second most critical, which correspond to the same result
found in WhiteHat Security’s report. These are some of
the most recent numbers regarding XSS statistics, but there
have been published numerous studies on XSS vulnerabilities,
attacks and it’s prevalence. One study by Hydara et al. [5]
from 2014 conducted a systematic literature review were they
reviewed a total of 115 studies related to XSS. They concluded
that XSS still remains a big problem for web applications,
despite all the proposed research and solutions being provided
so far. As seen from the more recent numbers from OWASP,
WhiteHat, and BugCrowd, this conclusion still holds true, that
XSS vulnerabilities remain to be at large.
B. Problem Description
XSS vulnerabilities are caused by insufﬁcient valida-
tion/sanitation of user-submitted data that is used and returned
by the website in the response, which could compromise
the user of the site. An attacker could potentially use this
vulnerability to steal users’ sensitive information, hijack user
sessions or rewrite whole website contents displaying fake
login forms. With the observation about how prevalent this type
of attack is, and according to the mentioned WhiteSecurity

419
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
report that it is being not prioritized nor easy for websites to
ﬁx and remediate, it becomes clear that the user needs some
means of protecting themselves at the client-side, since it is
mainly the end-users of vulnerable web applications that are
affected by potential attacks. Amongst the top 5 most used
web browsers [6], Mozilla Firefox is the only browser, which
does not include any kind of built-in ﬁltering against XSS,
which may compromise users in the case of a vulnerable web
application.
In this paper we address this problem by creating a
built-in ﬁlter protecting against Reﬂected Cross-Site Scripting
(XSS) vulnerabilities inside the Mozilla Firefox browser. The
choice of protecting against XSS for Mozilla Firefox is made
for several reasons, one being that XSS vulnerabilities are
among the most critical and prevalent web vulnerabilities in
existence today with lacking protection mechanisms on both
the server- and client-side of web applications [5] [3] [2]. This,
in combination with the fact that Mozilla Firefox, which is the
second most used web browser [7], does not provide a built-
in ﬁlter for XSS protection, in contrast with the other major
web browsers, Chrome, Edge, Safari and Internet Explorer,
which do have such a ﬁlter built-in. The work of this paper
will, therefore, be to create this ﬁlter built into and integrated
with the existing source code of Mozilla Firefox, which is
possible due to the fact that Mozilla Firefox is fully open
source, allowing full access to the source code of the browser.
This would be a case-study/pilot-case for the effect of building,
integrating and running a ﬁlter protecting against XSS inside
of Mozilla Firefox. As this is the second most used browser,
with a market share of approximately 11.7%, as of the statistics
from StatCounter’s desktop browser market share worldwide
for April 2018 [7], and the fact that XSS vulnerabilities are
as prevalent as they are, it would be beneﬁcial to look at a
possible solution for adding this extra layer, the added ﬁlter, to
the defense in depth strategy combining several XSS protection
mechanisms for optimal overall protection.
For the work to be considered a possible usable solution,
it needs to be evaluated thoroughly. There exists several
different web browsers, all competing to being the best one,
in terms of different factors such as performance, security,
usability, customization and general look and feel. In such a
competitive industry, web browser need to make sure that every
included functionality is integrated and running as smoothly
and efﬁcient as possible, meaning an additional feature need
to be well deﬁned and robustly integrated. In the case of
creating a ﬁlter for XSS, it needs to be secure, providing
the necessary protection, and at the same time be efﬁciently
integrated so the overall performance of the browser is not
affected in any huge negatively direction. This means that the
work needs to be evaluated in terms of at least two different
categories, how well it protects against XSS attacks and how
much it affects the performance compared to Firefox without
the ﬁlter implemented. The overall validation of the ﬁlter
would be a qualitative research, as of how well the ﬁlter is
implemented into the existing solution, but at the same time
contain a quantitative method for measuring the performance
of the ﬁlter, which could be accurately measured and compared
to the original browser. By analyzing the performance number,
however, it is not possible to correctly classify it as either
right or wrong, but rather an estimation and analysis about
if the added feature is in fact within reasonable limits to be
considered as a well-performing solution.
C. Paper Outline
Section II will go into detail about web security and more
speciﬁcally about XSS, explaining everything from what it
is to different ways of protecting against it, focusing mainly
at the client-side of web applications. This section will also
include information about the current state regarding XSS
prevalence and existing work, before ending with a detailed
description of the methodology used in this paper. Following,
in Section III, will be describing the web browser, Mozilla
Firefox, which is the application that this paper is building on.
Section IV-D7 will then describe all the design choices and
the actual implementation of the work done, before Section V
will contain an analysis of how well the work is done, in terms
of protection effectiveness, performance and integration into
Firefox. The Sections VI and VII, contains a conclusion based
on all the work done, before ending with some suggestions for
further improvement.
II.
THEORETICAL BACKGROUND
A. Web Security
Web applications need to be protected against malicious
users who want to steal and tamper their data. Web security is
a broad concept, including many different aspects, protection
mechanisms and potential outcomes. To be able to protect a
web application, a basic understanding of information security
is therefore needed, as it regards some basic principles and
objectives for why security is important and how to utilize
it correctly. Information security deﬁnes three important ob-
jectives of security [8], which are maintaining conﬁdentiality,
integrity, and availability. Conﬁdentiality is about protection
of information and data from being accessed by unauthorized
parties. When someone gets access to data that they should
not have access to, like sensitive information about users, it
is considered a breach of conﬁdentiality. Integrity is about the
authenticity of information, ensuring that it is not altered and
to make sure the source of the information is genuine. In web
applications, this could be if an attacker is redirecting you to
a different site than you originally intended to visit, as the site
you get redirected to is not genuine. And lastly, availability
regards that information should be accessible for the authorized
users, which of course should be done in such a way that there
is no breach of conﬁdentiality or that someone might alter

420
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
the available data when accessing it. All these objectives of
security are important when creating secure web applications.
To be able to fulﬁll them all, web applications need to protect
against several different attacks from malicious parties trying
to steal their and their user’s data. This is not an easy task,
as there exist so many different types of attacks for targeting
all kinds of vulnerabilities that are often contained in web
applications.
Figure 1: Web application vulnerability disclosures in 2016.
Figure is taken from ”IBM X-Force Threat Intelligence Index
2017” [9]
Several companies and organizations are doing annual
research and assessment work containing a lot of collected data
from a huge number of web applications and reports regarding
security breaches and vulnerabilities. One of these reports
[3], already mentioned in the introduction, from WhiteHat
Security, goes into depth describing the current web security
state. This report does not only contain information about
XSS attacks, but a whole range of other web vulnerabilities
with information of how prevalent they are, as well as which
industries and areas that are the most vulnerable to different
attacks. WhiteHat’s report also contains a list of a vast number
of web application vulnerability classes, describing 64 different
web vulnerabilities that need to be protected against for web
applications. This is a huge number of vulnerabilities, and
while not all are relevant for every web application, many
of them are critical, which needs to be addressed accordingly,
where the injection attacks XSS and Structured Query Lan-
guage injection (SQLi) is considered the most critical. Another
report, by IBM, ”IBM X-Force Threat Intelligence Index 2017”
[9], is another comprehensive report containing statistics from
different security events including web security, identifying
what vulnerabilities are used and targeted industries. IBM also
concludes that XSS and SQLi vulnerabilities are the most
critical and prevalent, as seen in Figure 1, which need more
attention from the different industries. As a whole, containing
all web vulnerabilities, both reports have identiﬁed a small
decrease in vulnerabilities in web applications, but also that
attackers are targeting the most critical vulnerabilities more, in
which one of the most critical, XSS vulnerability is the least
prioritized by applications to ﬁx. Another concerning factor
identiﬁed by both reports is that it takes too long to ﬁx web
vulnerabilities, which means both the application itself, as well
as the end-users, are at a higher risk of being affected by a
security breach.
The reports from WhiteHat Security and IBM, as discussed
above, make it clear that the most prevalent attack on web ap-
plications is injection attacks, which includes attackers trying
to break the conﬁdentiality by stealing data from the web appli-
cation itself or from the users of the web application. Injection
attacks are performed with attackers inputting untrusted input
to web applications that is executed as a command or query
in such a way that it alters the course of execution, which
could result in stealing of sensitive information or altering of
data. There exist several types of injection attacks, but the most
prevalent is by far SQLi and XSS. SQLi involves unauthorized
users to inject Structured Query Language (SQL) commands
that can read or modify data from a database connected to the
web application. This is achieved through the usage of user-
supplied input that gets used as part of a SQL query without
the web application validating or encoding the input correctly.
As attackers can read and modify data upon a successfully
executed SQLi attack, it is possible to steal sensitive user
data such as usernames and passwords, alter the contents of
the stored information or simply delete everything contained
in a database, which would incur huge complications for
the affected web applications. The other critical vulnerability,
XSS, will be covered in more depth in the following section.
B. Cross-Site Scripting (XSS)
Cross-site scripting vulnerabilities are caused by insufﬁ-
cient validation/sanitation of user-submitted data in the form of
JavaScript code, that is used and returned by the website in the
response without making sure the content is safe to use, which
could compromise the users of the site. An attacker could
potentially use this vulnerability to rewrite the contents on
the website creating fake login forms to steal users’ sensitive
information, hijack user sessions or redirect them to other
malicious websites.
There are three main types of cross-site scripting attacks,
but there also exists some other deﬁned types:
•
Stored XSS, also called Persistent XSS
•
Reﬂected XSS, also called Non-Persistent XSS
•
Document Object Model (DOM) Based XSS
•
Others - Plug-in XSS, Universal XSS, Self XSS
1) Stored/Persistent XSS: Stored XSS occurs when the
injected script is stored on a publicly accessible area of a
website, which means on the actual website itself. Typical
places susceptible to Stored XSS attacks are in comment

421
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Attacker
Vulnerable 
Web Server
4. Response containing script,
executed in victim's browser
3. Visits vulnerable website 
1. Exploit website, inject:
<script>...</script> 
Victim
2. Stores exploit
on server 
5. Steal data,
sent to attacker 
Stored XSS
Figure 2: Stored/Persistent XSS
sections, message board posts or in chat rooms. Since the
input data is stored in these places if the input data is an
injected script, the injected script might get executed upon
loading of the page, if the page is vulnerable. When a user
visits one of these places, the browser will retrieve the data
and render it, which in turn will execute the Stored XSS
attack in the browser’s context. Figure 2 illustrates the ﬂow
of a typical Stored XSS attack. Other places susceptible to
Stored XSS attacks might include areas of a website only
accessible to administrators, like a visitor log or other logs
containing information about the usage of the website from
users, as it is possible to inject JavaScript code into Hypertext
Transfer Protocol (HTTP) headers [10] like the Referer
[11] or User-Agent [12] headers. As the data from these
headers are not unlikely to show up in some kinds of logs, a
successful XSS attack here would be performed in the context
of an administrator’s browser, where it might be possible to not
only get access to sensitive information from a single victim,
but rather data from the whole web application. This type of
XSS is very difﬁcult to protect against on the client-side, as
the client has no means to identify whether the JavaScript
code coming from a website is legitimate, or if it is malicious
JavaScript code injected by an attacker. A user does not need to
visit any speciﬁc Uniform Resource Locator (URL) or include
anything in the request to a website for a Stored XSS attack to
be executed. From the client’s perspective, all JavaScript code
coming from a website is legitimate and should be rendered
accordingly.
2) Reﬂected/Non-Persistent XSS:
Reﬂected XSS occurs
when the user input data is sent in a request to a website,
which immediately returns data in the response to the browser,
without the website ﬁrst making sure the data is safe. Reﬂected
XSS attacks are performed by entering data into search ﬁelds,
creating an error message or by other means where the
response use data from the request. In a Reﬂected XSS attack,
the JavaScript attack code is not stored on the website itself,
like it is in a Stored XSS attack. For a Reﬂected XSS attack
Figure 3: Reﬂected/Non-Persistent XSS
to work, the attacker needs to somehow make the victim
request a special query, containing the malicious script. As
mentioned, the search ﬁeld is a typical input ﬁeld that can be
attacked. When searching for a query, the website often returns
a page containing some results, which also will generate a
unique URL containing the submitted query. This is how an
attacker would create a specially crafted URL containing the
exploit code, which then needs to be shared with a victim. If
a user visits this particular URL, the attack code will run and
execute in the user’s browser. Figure 3 illustrates the ﬂow of
a typical Reﬂected XSS attack. As seen from this ﬁgure, a
Reﬂected XSS attack contains a request to and response from
a website, where the code inserted in the request is being used
in the response. It is this particular data ﬂow that protection
mechanisms can take advantage of, where it is possible to
compare the contents of the request with the contents of the
response, to identify a potential attack. In this paper, this
technique is utilized, which means it focuses on primarily
stopping Reﬂected XSS attacks.
3) DOM Based XSS: DOM Based XSS is a type of XSS
attack that in contrast to the other two types of XSS attacks
only rely on JavaScript vulnerabilities on the client-side of
the website and not the server-side. DOM Based XSS attacks
exploits how a website uses JavaScript to dynamically change
the DOM of a web page. The DOM of a web page is the
structure of the page, containing information for the browser
on how to render the page, with the usage of different HTML
tags and attributes. The DOM of a page makes it possible for
JavaScript code to interact with the page, making the page
more dynamic. This also makes it possible for malicious code
to change the page if JavaScript input is not handled correctly.
If a website includes some JavaScript code in the response that
directly uses input from an input source, like the URL, a DOM
Based XSS might be executed. Figure 4 illustrates the data
ﬂow of a typical DOM Based XSS attack. These attacks can
actually be performed without even sending the attack script
to the web server at all, by using a special HyperText Markup

422
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 4: DOM Based XSS
Language (HTML) character, the fragment identiﬁer #, in the
URL. When using the fragment identiﬁer, everything behind
it will not be part of the request. This means that from the
user inputs some data, to the malicious code is executed in
the browser, the malicious code is neither part of the request
nor the response of the website, but rather part of the DOM
of the web page, if the content after the fragment identiﬁer
is used by client-side code in the response. DOM Based XSS
is the least common type of XSS attacks, but it is also the
most difﬁcult to ﬁnd and protect against. Since the attack only
relies on ﬂaws on the client side, by using JavaScript code,
server-side ﬁltering cannot detect this attack at all, which is a
good reason why it is necessary to have protection also on the
client-side of a web application.
4) Other XSS Types: Although there exist three main types
of cross-site scripting, as these attacks have evolved and been
used in different ways, XSS types could now be categorized
into some additional sub-categories, Universal XSS, Plug-in
XSS and Self XSS.
a) Universal XSS: Universal XSS [13] is a form of XSS
attack that exploits the browser itself, browser extensions or
website extensions in order to exploit a website. Universal XSS
is a very dangerous type of XSS as it does not exploit the
website directly, meaning that a website does not need to con-
tain any vulnerabilities to be exploited. Modern web browsers
support extending their functionality by utilizing plug-ins,
small programs that add more features to the browsers. There
also exists plug-ins that are not loaded through the browser
but by the website itself. These plug-ins often have access
to the contents of the websites, and often require input from
the user for its functionality to work. By having user input in
combination with features for displaying or editing contents on
a web-page, the plug-in might create an opening for allowing
a cross-site scripting attack against the web-page it is being
used on. An example could be a plug-in that allows websites
to display pdf-ﬁles. If an attacker injects some JavaScript code
in the ﬁlename of the displayed pdf-ﬁle, this JavaScript code
could be rendered in the browser, if the plug-in does not
have proper validation and encoding for the input ﬁeld used
for the ﬁlename. XSS vulnerabilities introduced by insecure
plug-ins are often categorized as Plug-in XSS, which could be
considered as a sub-type of Universal XSS.
b) Self XSS: Self XSS is when users themselves create
and execute the attack in their own browsers, which can not
exploit other users, as in the case of the three main types of
XSS. Self XSS is mostly a social-engineering attack used to
trick users into executing XSS attacks on themselves, often by
making them copy and paste JavaScript code into their own
browsers. Awareness around this particular attack was gained
through the popular social media website Facebook.com,
as this attack became quite widespread against the users of
their site, which led to Facebook publishing a warning [14]
against Self XSS scams. Facebook even created a warning
displaying when a user opens the developer console window
in their browser, while visiting their site facebook.com, to
mitigate the attack.
As described above, XSS attacks occur because web ap-
plications are using unsanitized input data when displaying
and rendering content. For a successful XSS injection, from
the attacker’s perspective, the input containing the malicious
JavaScript content needs to be entered into the web application
in a way that it is somehow gets executed in the browser. The
next sections will explain how this is done, and give some
examples of how typical XSS attacks are performed.
When performing an XSS attack, it is possible to inject
the malicious script into the web application by using several
different input sources. An input source is considered an entry
point for user input to enter into the application. The most
common input sources for XSS attacks are from the GET- and
POST- parameters, which most often comes from HTML input
elements. A typical example is the search ﬁeld found on many
websites, which most often is an HTML input tag. After
using the search ﬁeld, the search query is likely to be included
in the URL of the returned web page, which would consist of a
GET parameter containing the query. HTTP headers is another
input source for script injections, as discussed in Section II-B1.
Injecting script content through HTTP cookies, which is a
small piece of data sent to the user’s web browser from a
server, is also an option, although this is much less common,
as a potential attacker would most likely need to get access
to other users’ cookies for injecting their script. Since the end
goal of an XSS attack often includes getting access to such
cookies, using them as an input source for an attack seems
less likely, although in theory, it is still a possibility.
For a successful XSS attack, the injected script content
needs to be entered into the web application in a way that
would actually render the script in the browser. This could
be done by using a wide variety of attack vectors, depending
on how the web application uses the input when generating
the response. Attack vectors are typically a combination of

423
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
HTML tags that include the script to be injected and executed.
These tags could either embed the script content directly or
reference an external resource containing the JavaScript code.
The most common attack vector is the usage of the script
tag. Another very common attack vector is the usage of the
img tag in combination with on-event handlers [15]. The
on-event handlers are properties that let HTML elements
react to events, where events are different actions like when
an element is being clicked, getting focus, or when it is loaded.
The reaction to an event can be speciﬁed to load script content,
which is why they are often used in XSS attacks. OWASP’s
”XSS Filter Evasion Cheat Sheet” [16] is a comprehensive
list of attack vectors utilizing a lot of different techniques,
including many uses of on-event handlers. Other than the
most common script and img tag, the iframe-, body-
, svg-, object- and style- tag are also HTML tags not
uncommonly used in XSS attacks. OWASP’s list [16] contains
descriptions of these and many more, including techniques to
hide the injected script from being detected by potential XSS
ﬁlters.
c) Example attack: A typical scenario for an XSS
attack starts with an attacker looking for input ﬁelds on a
web page where the submitted data is output without being
encoded. As mentioned above, the search ﬁeld is a common
input source. An attacker could, therefore, exploit a vulnerable
search ﬁeld, with the intention of trying to hijack another
user’s session. The search ﬁeld is often exposed for an attack,
as when you input a query, the same query is most likely
being returned and rendered by the website. If this input is
not properly being encoded, it could allow the attacker to
input JavaScript code that is being executed in the browser’s
context when the website returns the query, which could be
achieved using the script tag as the attack vector. For
hijacking a user’s session, the attacker would need some
JavaScript code that extracts the user’s session data, typically
found in a cookie from the logged in targeted user. The
exploit code, <script>document.location=’http:
//attacker/cookieStealer.js?c=document.
cookie</script>, could then be inserted into the search
ﬁeld. After creating this exploit, the attacker would need to
copy the URL from the result page after doing the search.
Since this is a Reﬂected XSS attack, the attacker would then
need to share this URL to potential users of this exploited site.
If a targeted logged in user now visits this particular URL,
the user’s session cookie is being sent to the attacker. The
attacker could then use this cookie to log in onto the exploited
website, which means the attacker would be impersonating
the user.
Another popular XSS attack is to rewrite the contents of a
website, creating fake forms for tricking users to enter sensitive
data like credit card information or login details. The attacker
would then make these forms submit the sensitive data to
themselves, rather than to the exploited website.
A typical thing that XSS attacks have in common is that
they are often not easy to detect by the end-users themselves.
In case of both the cookie stealing and fake forms exploits, the
attacker could simulate the actual behavior of the exploited
website, making it almost impossible for users to detect
that they have been compromised. By having a client-side
ﬁlter in the browser, a user could not only be notiﬁed of a
potential attack, but the ﬁlter could also completely stop it
from occurring in the ﬁrst place, which is the intent of the
ﬁlter.
C. Counter-Measures
There exist many counter-measures for XSS attacks, con-
sisting of several techniques as well as more speciﬁc policies
to follow, for securing web applications. It is highly recom-
mended to utilize a variety of many different counter-measures,
as it might be challenging to implement them being completely
robust and secure from unknown attacks and not all policies
are fully supported by all web browsers.
a) Validation/sanitization: The ﬁrst step towards pro-
tecting against XSS attacks is to make sure that valid mali-
cious code does not enter the web application at all. Valida-
tion/sanitization of all untrusted data input to a web application
makes sure that malicious input is either being rejected or
manipulated into being safe for usage in the response from
the website, used in the output of users’ browsers. It might be
difﬁcult to implement this properly as it can be challenging
to know what a malicious input looks like, considering all
the possible attack vectors that use advanced obscuration
techniques. A common mistake is to rely only on blacklist
validation, which is often trivial for attackers to circumvent, by
utilizing alternative input variations. White-listing is in general
considered much safer, only allowing the characters that the
web application should accept, for example, an integer or a
date. In case of free-form text input, white-listing becomes
difﬁcult, as the users should be allowed to enter almost
any character, hence the free-form. Any validation technique
becomes ineffective and difﬁcult to implement in the case of
free-from text, which is why input validation should not be
used as the primary defense against cross-site scripting attacks,
and why output encoding is needed.
b) Output encoding: Output encoding is the most ef-
fective remediation for cross-site scripting attacks when done
properly. Output encoding should be implemented every place
untrusted input is being outputted and rendered in the browser,
making sure the input is displayed as data and not executed
as code in the browser. It is important to implement the
output encoding according to the context it is being used
in, because different encodings are needed depending on the
context used. JavaScript, HTML, and URL’s all use various
encodings, which is why there is no single solution to how
output encoding should be implemented. Typical strategies are

424
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
to escape unicode, a typical character encoding, converting
unwanted characters to benign equivalents, percent encoding
and escaping hex values, as described in more detail in
OWASP’s XSS (Cross Site Scripting) Prevention Cheat Sheet
[17].
c) Content Security Policy (CSP): Another powerful
counter-measure is Content Security Policy (CSP), which is
a declarative policy that let web application owners create
rules for what sources the client is expecting the application to
load resources from. To enable CSP, the web server needs to
utilize the Content-Security-Policy HTTP response
header [18], where the policy for the application is speciﬁed,
including desired directives. Each directive describes a policy
for a certain resource type or policy area, for example to
prevent inline scripts from running, only allowing content to
be loaded for some trusted domains or restricting all content to
only load from the site’s own origin. CSP also have a reporting
feature, which means when a policy is being violated, it is
possible to get a report sent to the desired location, containing
information about the violation. This could be helpful for web
application owners to know if their policies are too strict or
needs modiﬁcations, as a policy can consist of many different
directives. Even though CSP can stop most cross-site scripting
attacks by utilizing a set of well-deﬁned directives, it is stated
in the World Wide Web Consortium (W3C) Recommendation
[19] that CSP is not meant as a ﬁrst line of defense mechanism,
but rather an element in a defense in-depth strategy, as an
added layer of security. A study by Weichselbaum et al. [20]
was done in 2016, including 1,680,867 hosts with 26,011
unique CSP policies, observing that 94.68% of all policies that
attempts to limit script execution are ineffective, as well as
99.34% of the hosts have policies that offer no beneﬁt against
XSS at all. This is a very clear indication that CSP in practice
is difﬁcult to utilize correctly and this is why it should not be
used as the primary defense against cross-site scripting attacks.
d) Same-origin policy: Same-origin policy [21] is a pol-
icy implemented inside web browsers that isolates potentially
malicious documents by restricting how a document or script
loaded from a speciﬁc origin can interact with resources from
other origins. For two web pages to have the same origin, they
need to have the same protocol, port and host, which means
they are allowed to load resources from each other. Cross-site
scripting attacks often involve the usage of different external
JavaScript ﬁles for collecting data from compromised users,
which could be blocked by utilizing the same-origin policy.
e) HTTPOnly cookie ﬂag: As mentioned in Section
II-B4c, cookies could contain valuable information for at-
tackers, which means they should be protected from unau-
thorized access. The HTTPOnly cookie ﬂag is an additional
ﬂag included in the Set-Cookie HTTP response header [22],
preventing JavaScript code from accessing the contents of
cookies. This is not considered a counter-measure for XSS,
but rather for mitigating the risk of an attacker accessing other
users cookies in the case of an attack.
f) Disabling JavaScript: A more drastic approach that
would effectively stop XSS is to disable JavaScript, since these
attacks rely on a JavaScript environment for execution. This
solution can be effective for simple static websites, but most
dynamic websites require some sort of JavaScript support for
basic functionality, which means this remediation would not
be suited as a general solution.
D. Cross-Site Scripting Filters
Filters try to stop cross-site scripting attacks by utilizing
a set of rules to detect potentially malicious input data,
before either blocking it or sanitizing it for safe usage. There
exists many XSS ﬁlter implementations, with varying focus on
the different areas such as security, performance, low false-
positives and usability. All of these areas are in focus in
most ﬁlters, but it is not common for a ﬁlter to be best in
all categories, as they do not necessarily compensate each
other. There is, however, one clear way to differentiate between
ﬁlters, which is to divide them into two groups, server-side and
client-side ﬁlters:
a) Server-side ﬁlters:
Server-side ﬁlters are imple-
mented on the server side of a website, which means it can
only detect input data that are sent via the server. The DOM
Based XSS attack is possible to perform without sending the
attack code to the server at all, as discussed in Section II-B3.
This means a server-side ﬁlter would not be able to detect
the attack at all, which implies it would not be able to stop
the attack. There are several existing server-side ﬁlters, which
typically needs to be integrated into the source code of the web
application. A study made by S. Gupta and B.B. Gupta [23] has
a quantitative discussion for server-side ﬁlters, discussing some
of the state-of-the-art techniques they are using. The study
concludes that there are generally several ﬂaws with server-
side ﬁlters that need to be addressed, like too much altering of
existing code-base, long learning phase, as well as too many
false-positives and false-negatives. The study also emphasizes
that server-side ﬁlters do not detect DOM-based XSS attacks.
With all the combined ﬂaws and design limitations of server-
side ﬁlters, it becomes evident that only relying on server-side
protection is not enough, and why it is necessary with client-
side ﬁlters as an extra layer of security.
b) Client-side ﬁlters: Client-side ﬁlters are located in
the client, which typically would be the web browser used to
access web applications. Client-side ﬁltering could be able to
detect DOM Based XSS attacks, providing the extra protection
server-side ﬁlters are missing. However, even though client-
side ﬁlters could possibly detect all types of XSS attacks,
they should not be used without server-side ﬁlters. By placing
the ﬁlter on the client-side, it means that the user might be
able to modify it to circumvent the ﬁltering. It is, therefore,

425
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
strongly recommended to utilize both server- and client-side
ﬁltering, to be able to protect against all attack types of XSS
and achieving good protection following the defense in depth
strategy. This paper focuses on client-side ﬁltering, which
includes a discussion of various existing solutions, presented
in the next sections.
Regular Expression Based Filters Using regular expres-
sions is a popular technique for client-side ﬁlters, where the
ﬁlter is typically located between the network layer and HTML
parser in the browser. Regular expressions are then used to
identify potentially malicious code in the HTTP requests and
to approximate the rules of the HTML parser to know which
content in the HTTP response that would be treated as script
content [24]. By doing these approximations, the ﬁlter do
not have to recreate the browser’s own HTML parser, which
would lead to the HTTP response being parsed twice, ﬁrst
for the ﬁlter to identify and remove potential malicious code
and then for the browser to parse the page as normal. These
approximations do, however, have their drawbacks, as they
incur a higher number of false positives, due to several ﬂaws in
their design [24]. These ﬂaws are a consequence of attackers
trying to make the content from the request, the actual attack
code, differ from the response so that the approximation rules
would not detect it as an attack. Some common ﬂaws are that
the ﬁlters do not correctly approximate the decoding process
of different encodings or do not take into consideration that
different characters can be used to delimit HTML attributes.
A popular client-side XSS ﬁlter using regular expressions
is an extension called NoScript [25], for the Mozilla Firefox
browser, ﬁrst released in 2005 and actively updated by the
maker Giorgio Maone. The ﬁlter is matching HTML code
for injected JavaScript in the request by utilizing regular
expression rules for simulating the HTML parser, which would
potentially lead to false-positives, as it is better to over-
approximate these rules than to let an attack bypass the ﬁlter
[24]. Due to a lot of false-positives, NoScript try to solve this
by prompting the user to repeat the request with the ﬁlter dis-
abled, allowing the user to decide for themselves if they think
it was a false positive. This is a decent approach for security-
aware users, but in general, users do not have the knowledge
or desire to take action in the case of security-related issues
[26]. String-matching Based Filters String matching is another
method for client-side XSS ﬁltering, used by the ﬁlter in the
Google Chrome browser, called XSS Auditor. XSS Auditor
works by matching the HTML code for injected JavaScript
code from the request with the response from the website, after
it is been parsed by the browser’s own HTML parser [24]. This
means that XSS Auditor does not need to approximate any of
the HTML parser rules, since the parsing is already done when
the matching algorithm starts. This is achieved by the location
of XSS Auditor, which is between the HTML parser and the
JavaScript engine, as shown in Figure 5. This placement makes
it possible to block scripts after parsing, by blocking them from
being sent to the JavaScript engine for execution. The location
of XSS Auditor have beneﬁts like performance, by not having
to simulate the HTML parser, and the fact that the JavaScript
engine has a narrow interface it is reasonable to assure that all
scripts are being processed by the ﬁlter before being executed.
HTML
Parser
XSS?
JavaScript 
Engine
Block
Script
HTTP Response
Yes
No
document.write
Figure 5: XSS Auditor design
XSS Auditor also has some limitations, some of which are
discussed in the paper from Stock et al. [26], which lists several
ﬂaws with the design and string-matching algorithm used in
XSS Auditor. As mentioned in the paper, these are mainly
ﬂaws regarding protection of DOM-based XSS, which is not
the main type of attack that XSS Auditor is protecting against.
It is, however, relevant to take notice of these limitations, as
it might be desirable to not make the same limitations when
designing and implementing a new ﬁlter.
Scope issues are related to the fact that XSS Auditor does
not support every type of XSS or are neglecting functionality
that enables XSS attacks. One example being that XSS Auditor
relies on encountering dangerous elements during the HTML
parsing of the response, which is not always the case, for
example, when a web page is using the JavaScript function
eval() [27]. eval() is a function that evaluates the string
representation of JavaScript code inserted inside its parenthe-
ses, which means if eval() uses data from the URL of
the loaded web page, this evaluation could be done without
entering the HTML parser, which means that XSS Auditor
would not detect it.
Another ﬂaw in XSS Auditor is that some special char-
acters needs to be present in the request for the ﬁlter to
be activated. If any of these characters are not present, the
ﬁlter deactivates. As the paper describes, it is possible to
successfully execute an XSS attack without any of these special
characters being used at all.
Double injections is yet another limitation that XSS Audi-
tor does not protect against, which is the inability to detect
attacks containing concatenated values coming from more
than one source of user input. An attacker could use two
different input sources due to application-speciﬁc code that
concatenates two or more user inputs. When creating an attack
using double injections, the exploit code consist of two or more
parts, but gets executed as one concatenated attack code. Since
XSS Auditor’s string-matching algorithm checks for the whole

426
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
script code, the algorithm would not detect the attack, as the
whole script code does not exist from any single user input
source.
1) State of Current Browsers: Regular expressions and
string matching are among the techniques being implemented
in the top ﬁve most used web browsers for desktop, which
according to the desktop browser market share worldwide from
StatCounter [7] are Chrome, Firefox, Internet Explorer/Edge
and Safari. Table I contains information on the state of their
XSS protection status. Both Chrome and Safari use the men-
tioned string matching based XSS Auditor ﬁlter. XSS Auditor
was ﬁrst built into the browser engine WebKit, which Safari
uses, before also being integrated into a fork of WebKit called
Blink, which Chrome uses. Internet Explorer and Edge both
have a ﬁlter implemented based on the regular expression
technique, ﬁrst introduced in Internet Explorer 8 [28]. Firefox,
however, being the second most used web browser, does not
have a built-in ﬁlter, but rather relies solely on CSP support,
which again relies on websites to properly deﬁne the CSP
rules. By not having a client-side ﬁlter, the defense in depth
strategy is also weakened, where a potential ﬁlter would
provide an extra layer of security for the end-users of the
application.
Table I: Top 5 Web Browsers XSS Protection Status.
Data retrieved from Mozilla [18]
Built-in ﬁlter
CSP
XSS
Protection
Limited support
III.
MOZILLA FIREFOX
Mozilla Firefox is a free and open-source web browser
developed by Mozilla, with its ﬁrst major release in 2002 [29].
Firefox’s source code has a layered architecture where the code
is organized as separate modular components. Firefox is multi-
threaded and follows the rules of object-oriented programming,
where access to internal data is achieved through public
interfaces of the classes [30]. One of the primary requirements
of Firefox is that it must be completely cross-platform, which
is why the browser consists of several components focusing
on this area, like making sure the operating system dependent
logic is hidden from the application logic.
This section will explain some of the most relevant parts
of Firefox, with regards to the ﬁlter described in this paper.
The parts explained have been slightly simpliﬁed, making it
easier to understand the relation of how everything is working
together, again with regards to the added XSS ﬁlter.
A. Firefox Overview
The main components of Firefox can be divided into the
user interface XML User Interface Language (XUL) and the
browser and the rendering engine Gecko. XUL is Mozilla’s
own language for building portable user interfaces, which is
an Extensible Markup Language (XML) language [31]. Gecko
is Mozilla’s browser engine built to support many different
Internet standards, including HTML 5, Cascading Style Sheets
(CSS) 3, DOM, XML, JavaScript, and others. Gecko contains
many different components for document parsing (HTML and
XML), layout engine, style system (CSS), JavaScript engine
called SpiderMonkey, image library, networking, security, as
well as other components [32].
Mozilla also has a build system [33] using the make tool
[34], consuming Makefiles. The command-line interface
Mach [35] is used to help developers perform common tasks
for working with the Mozilla codebase, making it easy to start
building, debugging and testing Mozilla projects.
Firefox consists of over 36 million lines of code [36],
written in several languages, which are mostly C++ and
JavaScript, but also HTML, C, Rust, XML, Python and Java,
as well as other less used. The source code directory of Firefox
[37] contains many folders where the code is grouped based on
their functionality. Some of these groups consist of function-
ality related to document parsing, JavaScript execution, image
loading, extensions, and networking, just to mention a few.
Mozilla also has strict rules about how the code should be
implemented, not just how it is structured into directories.
As mentioned above, Firefox is object-oriented, using a lot
of public interfaces. They have also implemented several
utility- and helper-classes for writing speciﬁc functionality
inside their code-base. Although the source code is mostly
written in the C++ language, which provides this functionality
built-in, Mozilla uses many of their own methods for these
functions. This means that it is necessary to acquire speciﬁc
knowledge regarding these coding rules before attempting to
make changes to the Mozilla codebase, as it is a complex piece
of software.
1) Loading of a Web Page: As mentioned above, Firefox
consists of several components, including its rendering engine
Gecko, which is the most relevant part for the implementation
of this ﬁlter, as it contains everything related to document
parsing and handling of JavaScript execution. Figure 6 is
a simpliﬁed description of the loading of a document in
Firefox, containing only the relevant parts which are im-
portant regarding the XSS ﬁlter. When a typical HTML
web page is loaded through Firefox, two internal document
classes, nsDocument, and nsHTMLDocument, are created,
controlling the creation and representation of the web page

427
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Network
nsDocument 
nsHTMLDocument
nsHTML5Parser
ScriptLoader
EventListener 
Manager
JS Engine
Security checks: 
scripts
on-events
DOM 
changes
CSP
Figure 6: Simpliﬁed data ﬂow for rendering a web page
to be loaded. These documents are responsible for creating
and calling all the relevant parsers, like the HTML parser
[38], nsHtml5Parser, as well as initializing the script
executioner class, ScriptLoader, which is responsible for
handling script content coming from script tags. The
HTML parser receives data from the network that needs
to be parsed. Every time the parser encounters some script
content, the relevant parts of Firefox that handle this con-
tent is invoked. In the case of on-event handlers, the
EventListenerManager class is invoked. A common
source for script content is the script tag, where the script
loader class, ScriptLoader, would be invoked with the
discovered script. The script loader class will then try to
extract the script and either execute it as an inline or external
script. Before the script is passed to the JavaScript engine
for execution, a security check is performed for ﬁnding out
if the script is allowed to run. This security check involves
checking with the CSP rules if it is allowed to load if these
rules are speciﬁed by the loaded website. If the script passes
this check, it will be handed over to the JavaScript engine
which will execute the script in the browser. The HTML parser
will continue parsing the data entering through the network,
repeating the steps when new script content is discovered.
B. Security Mechanisms
Firefox includes many internal security mechanisms for
making sure that the browser itself is not being compro-
mised by attackers, as Gecko loads JavaScript content from
untrusted and potentially malicious web pages, which then
again run on the user’s computer. These security mechanisms
include several complicated concepts regarding same-origin
policy, compartments, and principals, all explained in detail
at Mozilla’s own website [39]. This section will try to give a
simpliﬁed explanation of why all these concepts are important
and how they are used. The reason why this is interesting
to look at is because a countermeasure for XSS, CSP, is
implemented inside Firefox using the principal concept. Since
CSP provides similar functionality as the work described
in this paper provides, the ﬁlter created should also ideally
be implemented in a way that follows the same principles,
fulﬁlling the necessary security requirements.
1) Same-Origin Policy: The same-origin policy is restrict-
ing how a document or script loaded from a speciﬁc origin
can interact with resources from other origins, as described in
Section II-C0d. The security model for web content is based on
this policy, which is also used inside Firefox as a script security
mechanism [39]. As Firefox’s rendering engine Gecko consist
of different languages, its core in C++ and its front-end in
JavaScript, these two parts need to interact with each other in
a secure manner. The JavaScript front-end is actually running
with system privileges, meaning that if it is compromised,
attackers might get control of the user’s computer. As this
JavaScript code is interacting with web content from web
applications, which again is susceptible to XSS attacks, it
is important to make sure that JavaScript code from Gecko
itself is not affected by any such attack, which is achieved by
utilizing the principle of the same-origin policy.
Figure 7: An overview of the relationships between the differ-
ent security principals.
Figure
Is
taken
from
Mozilla’s
website,
about
”Script
Security”[39]
2) Compartments and Principals: A security measure in
Gecko is that it is divided into different compartments.
Compartments could either be internal parts in Gecko or a
content window, a typical website, where different parts can
only access other parts if they are in the same compart-
ment. The concept of compartments is, therefore, using the
same-origin policy principle. Every part inside a compartment
is, therefore, same-origin with the others and no additional
security checks are performed when parts inside the same
compartment talk to each other. If Firefox loaded the website
at http://example.com/subfolder/, all the HTML

428
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
elements and script content residing on this exact address
would be inside the same compartment. There are, however,
different ways for compartments to access parts of other
compartments, where the main rules are that higher privileged
compartments have access to less privileged compartments,
but not the opposite, unless the higher privileged compartment
explicitly chooses to share its access.
To
be
able
to
determine
the
security
relation
be-
tween different compartments, a concept called security
principals is used, which is something every compartment
have. Figure 7 illustrates the relationship between different
principals, as there are several different principals, each with
its own rules. System principals pass all security checks, which
is what the JavaScript code from Gecko is running with.
Content principals are associated with web content, meaning
that content from a speciﬁc origin could access parts from
content inside the same origin. An expanded principal is
speciﬁed as an array of origins, meaning that it contains several
content principals. The expanded principal itself gets access
to its contents, but the content principals within does not get
access to the expanded principal. Finally, there is the null
principal, which fails almost all security checks, meaning it
has no privileges and can only be accessed by itself and the
JavaScript code from within Gecko.
3) Content Security Policy (CSP): Content Security Policy
(CSP), as described in Section II-C0c, is a security feature
that is also implemented in Firefox. Since CSP is part of the
script security model, it also has a principal. This means that
CSP is created through a principal and access to it needs to be
done through a principal. The main class, the nsDocument
class, is the place where the CSP is initialized, by using a
principal. As the nsDocument creates and holds a reference
to the CSP Principal, other classes can get access to the
CSP through the nsDocument class. Some noteworthy places
that CSP is used inside Firefox are the script loader class,
ScriptLoader, and the EventListenerManager class.
These are locations which handle content related to script
execution, and therefore also the place where the proposed
ﬁlter should be placed.
IV.
DESIGN AND IMPLEMENTATION
This section will go through everything from the de-
velopment process of the implemented ﬁlter, including the
requirements, design, tools used and the actual implementation
of the solution.
A. Design Choices
Software development includes a lot of choices that need to
be made during the development lifecycle, regarding analyzing
the problem, coming up with a solution, making the design and
ﬁguring out how it should be implemented. When creating a
ﬁlter for Firefox defending against cross-site scripting attacks,
it is possible to choose many different approaches towards
the same main goal, but yet achieving differently in different
categories such as performance, availability, usability, mainte-
nance and of course security. In this section, some of the design
choices made for our solution will be explained in detail.
a) Usability: The ﬁlter should be easy to use, by not
requiring any user-interaction at all. The NoScript plug-in
for Firefox, mentioned in Section II-D0b, is an example of
something that is not wanted, as NoScript do require a fair
amount of user interaction, as the plug-in have a lot of false-
positives. In a worst-case scenario, a user might accidentally
allow an attack to get executed, even though the ﬁlter did stop
the attack and warned about it, as users might not understand
what it means and the risk of ignoring the warnings.
b) Low false-positives: It is important that the ﬁlter do
not interfere with a user’s normal browsing sessions, unless it
is to protect the user from an actual attack. To achieve this, the
ﬁlter should have a low number of false-positives, which means
that the ﬁlter should minimize the number of times where it
think there is an attack when in reality it is not. The opposite
of a false-positive is a false-negative, which is when the ﬁlter
thinks a script is safe to load when in reality it is an attack
and should be blocked. In practice it is difﬁcult to guarantee
both non-existent false-positives and false-negatives in a ﬁlter
meant for defending against cross-site scripting attacks, as
there are so many different ways of using JavaScript in web
applications, which again is one of the reasons why cross-site
scripting attacks are so prevalent. There is, however, a balance
to be made, to make sure that the ﬁlter do protect against most
attacks, which means it might introduce some false-positives,
but at the same time it cannot be too strict either. An example
of a too strict ﬁlter is again the NoScript plug-in for Firefox,
which is really aggressive, introducing a lot of false-positives
which would interfere a lot during normal browsing sessions,
again requiring user interactions as a workaround.
c) High performance: The ﬁlter should not incur a lot
of performance overhead, which would make the loading of
web pages slower, which again would interfere with the usage
of normal web browsing. When using the ﬁlter, there should
be no noticeable delay when loading web pages in comparison
with the version of Firefox without the ﬁlter. This is an
important requirement, because of the competition between
web browsers, as discussed in Section II.
d) Provide protection against Reﬂected XSS: The whole
point of a ﬁlter protecting against cross-site scripting attacks
is to provide this protection properly. As there exist several
different types of XSS, as discussed in Section II-B, it is
important to clarify that the main focus of the ﬁlter is to
protect against the Reﬂected XSS type. This is the type of
XSS that ﬁlters for the other major web browsers also primarily
focuses on, as it is very prevalent and the easiest to discover,
as described in Section II-B2. It is, however, desirable to also

429
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
protect against DOM Based XSS, which there will be some
basic protection against, as a byproduct of the Reﬂected XSS
protection. Complete DOM Based XSS support will, however,
be lacking, as in the case of XSS Auditor, as explained by
Stock et al. [26].
1) Browser Extension vs Internal Implementation: The
main goal of this work is to add some functionality to the
Firefox browser, which there are several ways of accomplish-
ing. Firefox do provide support for browser extensions [40],
which can extend and modify the capabilities of the browser.
These extensions are built using JavaScript, HTML, and CSS
by using the WebExtensions API, a cross-platform system for
developing extensions. They can provide a lot of functionality
for altering the contents of or extracting information from a
web page, either with or without required user interaction.
There are, however, some reasons why browser extensions are
not suitable for this, explained in the following paragraphs.
a) Availability: The main reason why browser exten-
sions are less suitable is because they are something that users
themselves need to ﬁnd, install and use. It should not be
necessary for users to know about what cross-site scripting is
and why it is important to protect against it, for them to take
advantage of this ﬁlter. By making this protection a choice
for the user, the ﬁlter would most likely not be used by the
majority of users. This is why an integration with Firefox
itself would be a better solution, as then all users would take
advantage of the ﬁlter without the need of any knowledge about
it or action required.
b) Performance: Even if there are users choosing to
install and use such a security ﬁlter, there is another drawback
by making it as a browser extension, which is a performance
issue. When creating a browser extension for Firefox you
can only use the API’s supported by Firefox [41], utilizing
JavaScript code that talks to the internals of the browser itself.
This means there are more layers that the data needs to go
through, from getting from the ﬁlter to the internals of Firefox,
which is needed for the functionality of the extension to work.
If the ﬁlter, however, is placed inside the internals of Firefox,
some redundancy will be removed, which again will lead to
a better performance, which is what is chosen for this ﬁlter
design.
c) Security: The purpose of the proposed ﬁlter is to
protect against Reﬂected XSS attacks, which means the in-
jected script is contained in both the request and response. By
implementing the ﬁlter as a part of the internal implementation
of Firefox, it is easier to have a more robust integration being
more secure, as Firefox has a lot of coding principals including
many security features, as described in Section II.
2) Blocking Technique: When detecting an XSS attack, the
ﬁlter needs to take action to block the injected script. There
are mainly two ways of doing this, either blocking only the
injected script or blocking the whole web page from loading.
By only blocking the injected script you interfere less with
the browsing experience of the user, as they can still use the
website as normal, without the parts potentially affected by
the injected script, which is what has been chosen for this
proposed ﬁlter.
3) Filtering Technique: As discussed in Section II-D, there
exists XSS ﬁlters based mainly on the two ﬁltering techniques
regular expressions and string matching. For this paper, the
string matching technique and design from XSS Auditor was
chosen as the main basis. XSS Auditor used in the Google
Chrome browser does achieve high performance, few false-
positives and low interference with normal web browsing,
providing protection against mainly Reﬂected XSS attacks, as
desired from the requirements in this paper.
B. Design Overview
The main design of the ﬁlter is to compare every script
returned in the response with every potential dangerous script
from the request. If there is an occurrence of a script appearing
in both the request and response, the cross-site scripting ﬁlter
will block this particular script from being executed. The ﬁlter
itself is structured as its own class inside Firefox’s source code,
which makes it easy for other components in Firefox to use the
ﬁlter when needed. The ﬁlter is placed after the HTML parser,
but before script execution, providing beneﬁts regarding both
security and performance. The following sections will describe
the design of the ﬁlter in more detail.
1) Placement: By basing the solution on the ﬁltering
principals of XSS Auditor, the placement in Firefox will
also be similar to how Auditor is placed inside of Google’s
Chrome browser. Auditor is placed between the HTML parser
and JavaScript execution environment, which provides several
beneﬁts, regarding high security and performance, as explained
in Section II-D0b.
The ﬁlter needs to know what Firefox would intercept as
script content, to be able to ﬁlter on the correct data. If the
ﬁlter was placed before the HTML parser, the ﬁlter would need
to simulate the rules of the parser to try to approximate and
identify what Firefox would intercept as script content. This
means that each loaded document would be parsed twice, once
from the ﬁlter and once from Firefox’s own parser, which
would incur a lot of performance overhead. Since Firefox
need to parse the HTML documents regardless of the ﬁlter’s
presence, by placing the ﬁlter after the HTML parser, it can
use the results from Firefox’s own parsing when determining
which content to ﬁlter on, which again would not add any extra
performance overhead regarding the actual parsing process.
Since the ﬁlter does not need to approximate the parser rules
when placed behind the HTML parser, the ﬁlter can also be
sure that it will discover, identify and act upon all the scripts
entered through Firefox, as the parser in Firefox will properly
identify all script content before they are processed further. As

430
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
explained in Section III, script content from script tags and
on-event handlers get sent to the classes ScriptLoader
and EventListenerManager, which will further examine
the data and conduct the necessary security checks before they
are sent to the JavaScript engine for being executed, as shown
in Figure 6. By extending on this ﬁgure, extracting the relevant
parts, Figure 8 shows the placement of the XSS ﬁlter, residing
in the same location as the CSP security feature.
ScriptLoader
ScriptLoader
ScriptLoader
EventListener 
Manager
JS Engine
Security checks 
CSP
ScriptLoader
EventListener 
Manager
JS Engine
Security checks: 
CSP
XSS Filter
Figure 8: XSS Filter placement inside Firefox
2) Filter Class Structure: The ﬁlter class contains many
methods for handling the different stages needed in the ﬁl-
tering process. Since the ﬁlter can be invoked from different
locations, the ﬁlter class contains several input points that all
starts the ﬁltering process. This process contains a series of
different tasks that are performed in a particular order, before
concluding whether there exists a cross-site scripting injection
or not. This includes methods for fetching the input from the
request to different methods for comparing this data with either
inline scripts, external scripts or on-event handlers, all of which
need to be processed differently.
C. Environment
This section will describe the system and tools used when
developing the Firefox ﬁlter. The operating system used is
Arch Linux [42], a lightweight and ﬂexible Linux distribution.
For developing and writing the source code, the free and open-
source text editor Visual Studio Code (VS Code) [43] was
used.
1) Tools: Several different tools were utilized during the
development of the proposed ﬁlter.
a) Development Software: When developing computer
software, there exist several Integrated Development Environ-
ment (IDE) and code editors with a lot of added functionality
for helping with software development. For the development
and writing of the source code for this project, a lightweight,
free and open source text editor, VS Code [43], was used. VS
Code provides the necessary syntax highlighting and autocom-
plete, while also making it easy to navigate around in the huge
Firefox source code. Without adding extra additions to VS
Code, it does not handle building and debugging of the Firefox
code, which is one of the reasons it is a lightweight editor. For
these operations, however, there are more specialized tools that
are better suited for the development of Firefox, as Mozilla
have their own recommendations and tools available.
b) Mach: As mentioned in Theoretical Background II,
the tool mach [35] is a command-line interface used to start
the building, debugging and testing of Mozilla projects, which
also was used in the development of this modiﬁed version of
Firefox. mach makes it possible to conﬁgure Firefox builds
through the usage of a mozconfig conﬁguration ﬁle [44].
c) GNU Project Debugger (GDB):
For debugging,
GNU Project Debugger (GDB) [45] was used, a tool that can
start programs, make it stop on speciﬁed conditions, examine
what is happening at runtime and change things in the program
as it runs. GDB is a tool that can be invoked using the
gdb command, but when debugging Firefox it is possible to
start GDB through the usage of the mach command. After
starting the debugging mode, GDB makes it possible to create
breakpoints in the code, which lets the debugger inspect the
state of the application as it is running.
D. Implementation
This section will describe the implementation of the ﬁl-
ter, how it is implemented and integrated into Firefox, also
containing details about every part of the ﬁltering process.
1) Data Flow: The data ﬂow in Firefox is illustrated in
Figure 6, found in Section III-A. This ﬁgure is then being
expanded in Section IV-B1, Figure 8, where it is shown that the
classes ScriptLoader and EventListenerManager
perform several security checks, including using the XSS
ﬁlter. When the XSSFilter class is being invoked from
these classes, it ﬁrst needs to get all the input data from
the request. This data is retrieved through the nsDocument
class. The relevant input data fetched are all the GET- and
POST-parameters contained in the request. These parameters
are saved in a list, which is then examined further. Every
parameter is checked if it contains any potentially malicious
content, which in the case of a cross-site scripting attack would
be any input that contains some form of script content. This
examination is explained further below, in the next section.
If the ﬁlter identiﬁes any parameters as potentially unsafe, it
will compare them to every script entered into the ﬁlter class,
from the ScriptLoader and EventListenerManager
classes. If any of these scripts are also found in the request,
the ﬁlter will mark the script as unsafe, which will again notify
these classes to not send the detected script to the JavaScript

431
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
engine for execution. All the other scripts will be executed as
normal.
2) Examining Input Data: After fetching all the GET- and
POST-parameters from the request, these need to be analyzed
for potentially malicious content, which as mentioned above,
would consist of any type of script content. It is not a simple
task to identify whether or not these parameters contain any
actual script content, as there exists many different ways
of creating and trying to hide the malicious content of a
parameter. A good source of many such attack payloads is
OWASP’s guidelines “XSS Filter Evasion Cheat Sheet” [16],
which contains many examples of injections trying to cir-
cumvent typical XSS ﬁltering techniques, including variations
of using the script tag, on-event handlers, as well as
other, less used attack vectors. This is why the ﬁlter does
not actually identify any script content in the parameters
before marking them as potentially unsafe, but rather make
an assumption based on their contents. If a parameter only
contains alphanumeric characters, [a-z] [A-Z] [0-9], or
the underline character, _, the parameter is considered safe,
and should not be processed further by the ﬁlter. These are very
common characters that can not be used to execute any scripts,
making them safe to include in the response. The reason why
the underline character is included is that it is often used in
the case of a space in a parameter, which should be considered
safe. If there any other characters than the one speciﬁed, the
ﬁlter would include the parameter in further processing, which
will be described in more depth in the next section.
3) Looking for Injections - Matching Algorithm: If there
are any potentially harmful content in the request parameters,
for every script received in the response, the ﬁlter is running a
matching algorithm which tries to identify whether any of these
scripts are also contained in any of the parameters. Depending
on the type of script received from the response, the ﬁlter
handles the matching a bit differently. With inline scripts, a
comparison of the string representation of the actual script
content is done with each and all of the script content from
the inline scripts entered through the ScriptLoader class.
ScriptLoader also handles external scripts, in which case
it ﬁrst gets the information about the external URL where the
actual script is located before it executes the content inside the
script. For the ﬁlter, in the case of an external script, it does
not do a comparison between the contents of the external script
with the parameters, but rather a comparison between the string
representation of the external URL and the parameters. As for
other attack vectors, like the on-event handlers, the same
approach as the inline script matching is done. A similarity
between the inline and external script matching, however,
is that before the actual matching takes place, the content
from the scripts and the parameters need to be normalized.
This means that these contents might differ slightly, as the
parameters content might have changed after going through
the HTML parser in Firefox, which again means that some of
the same changes need to be done by the ﬁlter for it to detect
all injections properly. Several possible factors that need to
be addressed when normalizing the contents are listed below,
with a basis in the rules from OWASP’s ﬁlter evasion cheat
sheet [16].
a) Basic evasion techniques: A basic normalization
technique
is
to
not
differentiate
between
upper-
and
lower-case
characters.
The
script
injection
<script
src="http://xss.rocks/xss.js"></script>,
which
try
to
load
an
external
script
through
a
different
domain,
and
the
slightly
different
<script
src="http://xss.ROCKS/xss.js"></script>
would thus both be treated as the same injection, as the
uppercase
characters
in
the
second
example
would
be
converted to lowercase. Another basic technique is to use
added whitespace or other characters that do not change the
behavior of the injected script, but that tries to hide the script
from being recognized by ﬁlters. An example attack could
be the injection <script>alert (1)</script>, where
additional spaces are included, but where the injection could
successfully execute the script content, alert(1). This is
related to using different encodings in the injections, which
could include more advanced attack payloads.
b) Different encodings: It is common for attackers to
use different encodings in their attack payloads, by for example
using URL encoding [46] for the injected script, which again
is a means of hiding the injected string. URL encoding is
something that needs to be used in URL’s when the URL
contains characters outside the American Standard Code for
Information Interchange (ASCII) character encoding set, which
is why the URL has to be converted into supported ASCII
format. This is done by replacing unsafe ASCII characters with
a percent sign, %, followed by two hexadecimal characters.
It is also possible to use this encoding for any input for a
website, which means the ﬁlter needs to properly decode and
identify the encoded data. In this ﬁlter’s implementation, it
is supported by using Mozilla Firefox’s own internal class for
handling URL’s, which also handles decoding of URL encoded
data.
c) Different attack vectors: The attack vector for inject-
ing XSS attacks used in most examples in this paper, utilize
the script tag, <script>. It is, however, possible to perform
XSS injections by using many other different attack vectors,
as explained in Section II-B4. The ﬁlter does currently support
the script tag and every usage of the on-event handler,
which may be used in combination with many different attack
vectors.
4) Handling of Discovered Script: If the ﬁlter does ﬁnd a
match between a script from the response with a script from
the request, it marks that particular script as unsafe and notiﬁes
the class that invoked the ﬁlter, telling the class that it should
not execute this particular script. Even if a script is detected

432
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
and blocked, the ﬁlter do continue to check all other scripts
from the response with the request parameters, as there might
be more than one injected script. This is an important aspect
of the ﬁlter, as it only blocks the actual injected script and not
the whole page from loading. By choosing a different solution
where the ﬁlter is blocking the whole page when an attack is
detected, the ﬁlter does not need to do any further checking,
as you can not execute any more scripts as the page is not
being loaded.
5) Firefox Integration: This section will brieﬂy describe
how the ﬁlter class is integrated and how it connects to other
parts of Firefox. The ﬁlter is implemented as its own class
inside Firefox’s source code, called XSSFilter, making it
easy for other components to use the ﬁlter when needed. The
class is located in the mozilla/dom/security
folder,
which is the same location as where all the Content Security
Policy (CSP) related classes reside. The ﬁlter is currently
being created in places where the ﬁltering functionality is
needed, by supplying it with the owning document class,
nsDocument, in its constructor. As discussed in Section
IV-B1, ScriptLoader is one of the primary classes that
use the ﬁlter. Upon creation of the ScriptLoader class,
it also creates a ﬁlter instance with the main document in its
constructor. Every time the main document is loading new data,
like updated GET- and POST-parameters, the XSSFilter
instance located in ScriptLoader also gets updated, fetch-
ing the new request data, before using it in the ﬁltering
process every time ScriptLoader encounters a script,
either inline or external. Another internal class in Firefox,
EventListenerManager, do also use the XSSFilter in
a similar manner, but rather than inline and external scripts it
takes care of scripts from on-event handlers.
The XSSFilter class itself is also accessing other com-
ponents inside Firefox. To retrieve the GET parameters it
has to access the URL from the main document class before
using the URLParams class for parsing it correctly, making
sure the content is properly URL-decoded. As for the POST
parameters, the ﬁlter gets access to the nsIHttpChannel
class through the main document, which contains the necessary
data for retrieving the parameters, by utilizing different helper
classes in Firefox. It also uses several helper classes for a lot of
string manipulation, operations like searching for whole strings
or single characters, or converting between different types of
strings and encodings.
6) Challenges: There have been some challenges with
the implementation of the ﬁlter. Since the ﬁlter is being
implemented inside an already built software, the Mozilla
Firefox web browser, the ﬁlter needs to be integrated in a
way so that it can cooperate with existing code, data ﬂow and
different ways of doing things. Mozilla Firefox is a very huge
piece of software, containing many different classes spread
across separated modules that talk to each other by using
different means. To properly understand this whole structure
and following the data ﬂow proved to be a challenging task, as
there were used a lot of different coding principles and internal
code for different tasks. String-handling is a good example of
how complex the code is, as there exists many different types
of strings and as many ways of converting between them and
utilizing them correctly.
7) Unit Testing: Unit testing is a good way of assuring
that separate parts of the code is working as desired. In the
case of the ﬁlter implementation, the parts containing the
examination of input data and the matching algorithms are
the most important to test, as these are the parts dealing
with the actual ﬁltering process. Several unit tests have been
implemented to verify this process, by supplying some sample
injected data. As the ﬁlter require some special characters
to be included in the parameter for it to be checked for in
the matching process, several tests have been implemented
conﬁrming these character checks. The matching algorithm
also have several tests with different injection inputs, verifying
that the string matching works correctly. As for testing other
parts of the ﬁlter, which relies on many different parts of other
functions in Firefox, a more complete testing is done in Section
V.
V.
ANALYSIS AND ASSESSMENT
The ﬁlter needs to be evaluated, as explained in Section
II, in terms of several different categories. The ﬁlter should
be tested for how well it protects against XSS attacks and
how much it affects the performance of Firefox . An analysis
of the ﬁlter’s implementation, some of the design choices
and different limitations are also an important part of the
evaluation, as it will highlight what is good and what needs to
be improved.
A. Protection Effectiveness
Protection effectiveness is about how well the ﬁlter is able
to protect against XSS attacks, in particular, Reﬂected XSS
attacks.
1) Methodology of Testing: To be able to measure the
effectiveness of the ﬁlter, it is necessary to conduct testing
by doing an examination of a known vulnerable website, as
it is not the website’s own security features that need to be
tested, but the ﬁlter’s capabilities. One way of making sure
this is the case is to implement a sample website, used for the
sole purpose of testing the ﬁlter. The created website should
try to mimic some of the functionality found on other typical
websites, as this would provide a better generalization of the
ﬁlter’s overall effectiveness. A common functionality found
on a majority of websites is the search ﬁeld, which is also
susceptible to Reﬂected XSS attacks. The website should,
therefore, consist of a search ﬁeld, which would send the
query to a web server, where the response should be a page

433
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
containing the input query from the search ﬁeld. Since the
website has no built-in security features, inputting a script into
this search ﬁeld would effectively execute it upon receiving
the response. By visiting this vulnerable website through the
modiﬁed version of Firefox, containing the XSS ﬁlter, the
ﬁlter should be able to both detect and stop the injected script
from being executed. This is being tested by conducting an
automated test consisting of several different script injections,
to see if the ﬁlter detects all of the attacks or just a subset
of them. The automated test is made possible by the usage
of Selenium WebDriver [47], which makes it possible to do
direct calls to a speciﬁc web browser instance, by using its
native support for automation. A simple script will be created
that uses Selenium, which takes a list of injections as input,
which will than test each of them against the sample vulnerable
website. The outcome of this script will be a list of both
the successfully injected scripts and the ones that did not get
injected.
The script injections that are to be tested, are collected
from a variety of sources. An extensive list found on the web-
site gbhackers.com [48], and three different collections
gathered from github.com [49] [50] [51]. In total, a list
containing 920 unique script injections where created from
these sources. This list consist of many different attack vectors
targeted at very speciﬁc functionality of common websites.
Since the sample vulnerable website created is a very simple
website, not containing a complex usage of different HTML
tags, it is assumed that most of the injections would not be
successfully injected. This is why several hundred injections
were collected, to make sure that a big enough subset would
actually be successfully injected, which could be used in
the analysis. For achieving accurate results, the automation
testing script would actually need to be executed twice. This
process is shown in Figure 9. First, all the injections had to be
tested against the vulnerable website without the ﬁlter enabled.
This way, all the injections that are actually working on the
vulnerable site, would be recorded in a list created by the
testing script. Next, the list of injected scripts would be used
to run the testing script another time, this time using a version
of Firefox that has the ﬁlter enabled. The script would once
again create a list containing both the successful injections and
the injections stopped by the ﬁlter, which then would be used
for further analysis. This is done to make sure that the analyzed
results are containing actually injectable script content so that
it is known that it is the ﬁlter that stops the injections, and not
something wrong with the injections themselves.
2) Results: When running the automated test as described
above, the website without the ﬁlter was successfully injected
with a total of 138 different script injections. Although many
of the injections used similar attack vectors, there were still a
good mixture of different attack vectors and encodings used,
typically trying to circumvent ﬁltering mechanisms. When
using these injections in the version of Firefox containing
Vulnerable
https://www.example.com
<script>alert(1)</script>
Go!
Only keep successful injections
Use as new input
Injected
Not injected
Injections list
Vulnerable
https://www.example.com
<script>alert(1)</script>
Go!
Injected
Not injected
Injections list
Original Firefox - Without ﬁlter
Modiﬁed Firefox - With ﬁlter
XSS
Alert
OK
920 entries
138 entries
29 entries
138 entries
109 entries
782 entries
Figure 9: Testing of the implemented ﬁlter
the XSS ﬁlter, only 29 were successfully injected. The ﬁlter
did, therefore, block 109 of 138 injections. By examining the
results further, it is possible to pinpoint the weaknesses of the
ﬁlter, which again could be used to improve it.
a) Blocked scripts: Most of the script injections were
both detected and blocked by the ﬁlter. This included the
usage of many different variations of the script tag, where
the injections were adding other unnecessary characters or
using URL encoding trying to circumvent the ﬁlter. Because
of the ﬁlter’s location, behind the HTML parser, and the fact
that all parameters gets URL-decoded, all of these injections
were blocked. There were also a lot of usage of on-event
handlers utilizing similar circumvention techniques. Most of
the on-event handlers were also blocked, used in combina-
tion with different attack vectors like the img tag, svg tag
and body tag, since all of these on-event handlers had to
go through the EventListenerManager class, where the
ﬁlter was invoked from.
b) Injected scripts: As there were a total of 29 suc-
cessful injections, it is interesting to analyze why the ﬁlter
did not detect them. Table II contains an overview of the
injections, which will be further analyzed here. 16 of 29 of
the successful injections used the HTML tag iframe, in
different forms, utilizing upper/lower capitals, URL encoding
and otherwise including different characters to confuse the
ﬁlter. The iframe tag allows web pages to load other web
pages inside itself, where the tag also support the usage of
on-event handlers. Even though 16 of 138 of the iframe-
injections got successfully executed, the ﬁlter did actually
block the instances utilizing on-event handlers, as this is
well supported by the ﬁlter. The ﬁlter does not, however, detect
iframe’s using the src attribute, as it is not being invoked
in the parts of Firefox that handles the script content inside
these tags. 7 other injections were also cases where the attack
vectors are not supported, which used the embed tag, svg tag

434
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
and the object tag. The last 6 cases, however, used either the
script tag or on-event handlers, but did not get detected.
This is because they used varies encodings, like HTML entity
encoding and base64 encoding, which are not supported by
the ﬁlter. This is a good example showing that only dealing
with the most common URL encoding is not enough, as there
exist several other encodings that might be interpreted as script
content by the website, that also needs to be considered.
Table II: Testing of the implemented ﬁlter
Attack vector Number Not supported Difﬁculty to ﬁx 
iframe 
16 
attack vector 
easy 
embed 
3 
attack vector 
easy 
object 
3 
attack vector 
easy 
svg 
1 
attack vector 
easy 
script 
2 
encoding 
moderate 
on-event 
4 
encoding 
moderate 
3) Limitations: There are several limitations regarding the
capabilities of the ﬁlter, which could be categorized into
several categories. Some limitations were related to the actual
ﬁltering rules, which means the capability of the ﬁlter to detect
different types of script injections, using different methods for
trying to circumvent the ﬁlter. The other types of limitations
is related to the different input and output sources supported
by the ﬁlter, as there are more ways than using script tags and
on-event handlers to inject script content into websites.
a) Limitations regarding ﬁltering rules: As described
in Section IV-D3b, the ﬁlter did support URL encoded data,
which turned out to work really well, stopping several injec-
tions. It did not, however, support HTML entity and base64
encoding, which led to script injections being executed in
the browser. Support for more different encodings should,
therefore, be implemented.
As seen from the results, every injection utilizing the
iframe tag was successfully injected and executed in the
browser, as this was one amongst several injections in which
the injected attack vector was not accounted for by the ﬁlter.
This is a general limitation that the ﬁlter simply supports too
few attack vectors utilizing different HTML tags. Although
the ﬁlter supports on-event handlers, which is used by a
vast amount of HTML tags, these on-event handlers are
not always necessary to trigger a script for execution, which
is why this support needs to be improved.
In Section II-D0b, some limitations of the XSS Auditor
ﬁlter were discussed, which are tightly related to the limitations
of this ﬁlter implementation, as they are based on the same
string matching design. Not all of the limitations from Auditor
applies though, as this ﬁlter does not require the same strict
subset of special characters to be present, as Auditor requires.
However, the limitations regarding partial string injections are
something that has not been addressed in this ﬁlter either.
If a website have several input ﬁelds were its content gets
concatenated without proper validation, an attacker might take
advantage of this to create a complete injection by splitting
the injection into two or more ﬁelds. It is worth mentioning
that this is a rather special case, as the website needs to have
some very speciﬁc functionality for this attack to work, but it
is still a possibility that should be considered to be addressed.
b) Limitations regarding request input sources: An-
other type of limitation is regarding every input source from
the request, which means every source of user modiﬁed ﬁelds
that might enter into a web application. The absolutely most
used input sources are the GET- and POST parameters, which
are currently the only sources supported by the ﬁlter. There
are, however, other possible input sources where users could
inject malicious content, like for example HTTP headers and
cookies. Although these are more special cases, where the web
applications need some more speciﬁc use-cases, they might
still occur, which is why they should be considered to be
supported.
B. Performance
The performance of the implemented ﬁlter is an important
factor for its usefulness. For measuring the performance,
Mozilla’s own methodology for comparing page load times
across browsers [52] was used. This methodology consists
of choosing a set of websites that are loaded in Firefox,
repeated several times, while measuring the loading time for
each page load. This is a process that is automated with the
help of Selenium WebDriver [47], which makes it possible
to make direct calls to speciﬁed browsers using their native
support for automation. For this implemented ﬁlter, it would be
interesting to compare the performance of the modiﬁed Firefox
instance with the original Firefox instance, which does not
include a built-in XSS ﬁlter. By using the Selenium WebDriver
it is possible to supply both of these instances as options,
which means that the testing would be fully automated. As
mentioned, it is necessary to have a set of websites to be used
for testing. In the case for Firefox’s own testing, they chose to
pick the 200 most popular websites from the Alexa page rank
site [6], because news sites typically contain a lot of trackers.
1) Methodology of Testing: For the testing of this ﬁlter,
news sites are also well suited, as they contain a lot of script
content and most often also contains a search ﬁeld for looking
up articles, which is something that is useful for invoking the
ﬁlter mechanism. For the testing, only a subset of the most
popular news sites from the Alexa page rank site were chosen,
as not every news site had a working search ﬁeld. A total of

435
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
20 news sites were selected for the testing. It is assumed that
most of the top news sites can be considered to be relatively
safe, not containing any easy to exploit cross-site scripting
vulnerabilities. This does not, however, hinder the ﬁltering
mechanism to activate, since the ﬁlter would still search the
request parameters for potential dangerous contents, and do
the comparison between them and the scripts contained in the
response. This is done regardless of the existence of any actual
vulnerabilities or not, since that is the whole point of the ﬁlter,
to act as an added layer in the defense in depth strategy trying
to stop attacks from potential vulnerabilities.
To make sure the modiﬁed browser actually runs the code
for the implemented ﬁlter, each website was given some input
data by using their search ﬁelds. The testing was done with
two different input data, with the ﬁrst one simulating a totally
legit request that does not contain any script content at all,
inputting the query article, and the second one containing a
simple script, <script>alert(1)</script>, simulating
a very simple XSS attack. In the ﬁrst case, by inputting a
safe query, the ﬁlter would inspect this query and not ﬁnd
any potentially dangerous characters, which means the ﬁlter
would not need to do any additional processing. In the second
case, the same inspection of the query would be done, which
would mark the injection as unsafe. After marking it as unsafe,
every time a ﬁler would get a script from the response, this
script would be matched against the unsafe parameter, trying to
identify if the parameter is contained in any of the scripts. The
performance difference between the original and the modiﬁed
browser should be expected to be lower from the ﬁrst case than
the second case, as the ﬁlter is doing more work the second
query. One thing to notice here is that the ﬁlter would most
likely not detect an actual attack, as previously assumed that
popular news websites are probably protected against simple
injection attacks.
2) Results: After running the automated test, the result
does not suggest any added performance overhead by in-
cluding the ﬁlter. The measured load times were actually
so similar that an accurate estimate of how much the ﬁlter
affected the performance is not possible to measure. Table
III illustrates the results, where the unit of the load times
are milliseconds. The columns marked ”Invalid” means that
a web page did not load correctly, which means it got re-
moved when calculating the average load time. In the case
of loading web pages with the query article, the version
containing the ﬁlter did actually perform approximately 3.2%
faster on average, than Firefox without the ﬁlter. In the case
of using the query <script>alert(1)</script>, the
original Firefox version performed approximately 1.7% faster
on average. It is worth noting that the results did not contain
any huge ﬂuctuations when performing the test, and the biggest
difference after calculating the average for each test run was
about 362 ms, which was the difference between Run 1a
of the original version and Run 2s of the original version.
The difference between the different runs of the modiﬁed
ﬁlter was really small, as seen in the ﬁgure. As the total
difference between the original and modiﬁed versions are also
relatively small, the conclusion is that the ﬁlter did not add
any measurable performance overhead, meaning it achieves
very high performance. There are, however, several factors that
might have affected the testing, as described in the next section,
V-B3. Although, since there were so few ﬂuctuations between
the calculated averages, it is assumed that the results reﬂect
the reality fairly well.
Table III: Loading times results, measured in milliseconds
3) Limitations: There are several factors that might have
affected the performance testing, which could mean the results
are misleading. When Mozilla did their own performance
testing, they used a total of 200 different websites, a number
much higher that what was used when testing this ﬁlter.
Choosing a larger subset of websites for the testing could
have given some results reﬂecting a more average loading
time, but the 20 selected websites did achieve a very small
variance in the calculated average, so it should not be of much
difference if choosing to include any more than this. Some
other factors that might have had more impact on the results are
ﬂuctuations in the local Internet speed of the testing machine
and the ﬂuctuations in the web trafﬁc received by the tested
websites at the time of the testing. It is typical that these factors
varies throughout the day, depending on the time. The test of
the original and modiﬁed browser were done consecutively,
where each test, where one test contains loading of 1000
websites, took approximately 80 minutes to perform. This is
not a very huge time span, meaning these ﬂuctuations should
not be considered to be of any huge signiﬁcance. Another
factor that is less likely to have affected the performance is
the processing power of the testing machine itself, meaning
the CPU of the machine might have been running different
tasks when conducting the testing of the different browsers.
The testing machine was, however, left alone during the actual
testing period, which should result in minimal affection from
other tasks running.
These are all limitations that somehow might have affected the
testing results, some easier than others to control and minify,
which was done to the best of ability. Each of them should
not be of any signiﬁcance, and the results are considered to be

436
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
very accurate, but it is still worth mentioning these limitations,
as is is often small variances in the results which should be
tried to be explained.
C. Implementation
It is also interesting to analyze how well the ﬁlter itself is
implemented, in terms of how well it is integrated into Firefox,
and how it affects the usage of Firefox other than the already
measured performance.
1) Conform to Mozilla Firefox’s Internal Coding Stan-
dards: Mozilla Firefox has strict guidelines for how things
should be integrated into the browser, a coding standard for
everything from simple formatting to the usage of different
parts from the code. The implemented ﬁlter has tried to comply
to these rules, by following the general coding standards,
particularly regarding the handling of strings [53], as string
matching has been a major part of the ﬁlter mechanism.
Getting access to other parts of the code, parsing data correctly,
exception handling, and testing are other examples of good
implementation regarding Mozilla’s coding principles. There
is, however, one aspect of the implementation that is not being
integrated well enough for being part of a release version
of Firefox. This is the fact that the ﬁlter is not utilizing
the concept of script security and the usage of principals, as
explained in Section III-B.
2) Blocking Technique: When detecting a potential XSS
attack, the ﬁlter should be able to act upon it and block the
script injection. There are several ways of doing this blocking,
as mentioned in Section IV-A2, it is possible to only block
the injected script or the whole web page. Both of these
techniques have their advantages and disadvantages, which are
being discussed here.
a) Partial blocking: One of the reasons for blocking
only the injected script is that it would interfere less with a
user’s normal browsing of web pages, as the user could still
use the other parts of the web page, which are not affected by
the injected script. This is also a huge advantage in the case
of a false-positive, again as the user gets less interrupted, as
only a subpart of the page gets blocked.
b) Blocking the whole page: There are, however, some
disadvantages when choosing to only block parts of the page.
When the ﬁlter detects an attack, it is not unexpected that
an attacker might have combined several techniques and parts
when injecting the script into the website, hoping that one of
the included parts of the script would be able to circumvent the
ﬁlter. Hopefully, the ﬁlter would be advanced enough to prop-
erly detect and block all the parts of the injection, but it might
be some special conditions that the ﬁlter does not account for,
leading to a successful attack. This is one of the reasons why it
might be a better approach, when only concerned with security,
to block the whole web page from loading when an attack is
detected, as the detected attack might just be part of a bigger
attack. Another possibility for an attacker is to trick the ﬁlter
to not block an injected script, but to block some important
security feature that is actually needed by the attacked website
itself. An example is a website that requires the JavaScript
ﬁle security.js for its security features to work, which
will be included in the response when requesting the website.
Since the ﬁlter compares script content from the request with
script content from the response, an attacker might inject a
script containing the same ﬁlename, security.js, which
would then be detected by the ﬁlter as an attack, as the ﬁle
is both in the request and the response of the website. This
would then disable the websites security features, which means
the attacker could create an injection that combines the ﬁle
security.js with some other malicious script executing
an attack. Since the ﬁlter actually detected an attack, it would
be better to block the entire web page from loading, as this
would prevent this issue altogether.
3) Usability: The implemented ﬁlter does not currently
support any interaction from or with the user of Firefox, which
is something that should be considered, as more control of and
information about the ﬁlter’s behavior could be beneﬁcial to
websites and Firefox’s users.
a) Choosing blocking technique: As there are clearly
advantages and disadvantages with both the blocking tech-
niques, it is possible to make this a decision for websites to
take, by utilizing the X-XSS-Protection HTTP response
header [54]. This is a header currently supported by most of
the major web browsers, Chrome, Safari, Internet Explorer and
Edge, and makes it possible to decide how the browser should
act when they detect XSS attacks. There are four possible
values for the X-XSS-Protection header. Setting it to 0 will
disable the ﬁlter and 1 will enable it and only remove the
dangerous parts. By using 1; mode=block, the ﬁlter will be
enabled and the whole web page will be blocked. A last option
is using 1; report=<reporting-uri>, which will only
remove the dangerous parts and use a feature from CSP where
the violation is reported and sent to the speciﬁed URL.
b) Violation feedback: Another functionality missing
from the implementation, something the implemented CSP
feature already has, is the ability to properly notify users of
a violation. In the case of an XSS violation, this would be
when the ﬁlter detects and/or blocks the attack, depending on
what the previously mentioned X-XSS-Protection header is set
to. This header, did as mentioned above, support a reporting
feature, where details of the violation would get sent to a
speciﬁed URL. However, a violation notice should also be
indicated to the users of Firefox, regardless of the reporting
feature of the X-XSS-Protection header. In the ﬁlter’s current
state, these violation details are only shown in a special console
meant for the developers of Firefox itself, and not in the
developer console accessible to normal users of Firefox. The
details shown to the users does not have to contain every detail

437
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
about the violation, but an indication of what has happened
should be displayed.
VI.
CONCLUSION
Cross-Site Scripting (XSS) vulnerabilities continue to be
one of the most critical web security threats among today’s web
applications, despite the large quantity of research, proposals
and solutions being published and implemented [5]. This is
a type of vulnerability that mainly and directly compromise
the end-users of web applications, which means they need
additional protection. All of the major web browsers have
taken action by implementing several protection mechanisms
defending against these XSS attacks. Since XSS is such a
complex vulnerability, there is not a single protection mech-
anism that will stop all of the attacks, but rather a strategy
of having several mechanisms that together provide the best
protection, utilizing the defense-in-depth strategy. All of the
major web browsers have included a built-in ﬁlter for XSS
protection as one of these counter measures, except the second
most used, Mozilla Firefox, which have neglected to include
such a feature. As seen from the lacking effectiveness of the
most comprehensive protection mechanism in Firefox, CSP, as
discussed in Section II-C, the need for a built-in XSS ﬁlter in
Firefox is evident, considering the prevalence and consequence
of these attacks.
This paper has made a proposal and implementation for
such a ﬁlter, which is built-in and integrated into Firefox.
The ﬁltering principles for the ﬁlter was based on the ﬁl-
ter used in Google’s Chrome browser, XSS Auditor, which
utilizes an advantageous placement inside the web browser,
achieving both good protection and high performance. After
doing several tests of the implemented ﬁlter, ﬁndings suggest
that the ﬁlter did perform very well in protecting against a
wide variety of script injections, which contained different
attack vectors utilizing several methods trying to circumvent
the ﬁltering mechanism. Adding and removing characters,
using URL encodings and different on-event handlers were
efﬁciently blocked by the ﬁlter. There were, however, some
limitations regarding different types of encodings and a lack
of support for some attack vectors, which are something
that needs to be added before the ﬁlter could be considered
sufﬁcient for every-day usage. Performance-wise, the ﬁlter
did not show any measurable difference compared to the
version of Firefox without the ﬁlter. By not having any huge
performance overhead means that adding small additions for
ﬁxing the limitations mentioned should not incur signiﬁcantly
more overhead, as the most demanding ﬁltering mechanisms
are already implemented.
The modiﬁed version of Firefox containing the ﬁlter do,
therefore, already provide much better protection than the
original version of Firefox. Even though there are limitations
that needs to be addressed for it to be a considered a fully
ﬂedged solution, it already serves as an important layer in the
defense-in-depth strategy, providing a little extra to the much
desired protection that is needed for XSS vulnerabilities.
VII.
FURTHER WORK
As discussed in Section V, the implemented ﬁlter still has
room for improvements considering its protection effective-
ness. The areas for improvements are regarding input sources,
attack vectors, support for more encodings and integration with
existing Firefox code. Most of these improvements should be
rather trivial to implement. Firefox’s internal code has easy
access to other input sources data, like the most relevant, which
are HTTP headers. In the case of attack vector support, the
already supported attack vectors only needed about two lines
of code for them to be covered, so it should be as trivial
to add support to other vectors, like the iframe, embed,
svg and object tags, as mentioned in Section V-A2b. The
only challenge with these is to identify the location inside the
Firefox code where they are being processed, as they might be
handled in vastly different areas in the code. Support for more
encodings should also not be too difﬁcult to achieve, as there
exist good documentation covering how different encodings
work, and the fact that the ﬁlter class is structured in such
a way that it is easy to add more advanced ﬁltering rules.
The most challenging task would be to better integrate the
ﬁlter into the existing Firefox code, to comply with all the
security principals and coding standards that are required by
Mozilla. Another improvement could be to implement support
for the X-XSS-Protection header, which would let websites
themselves decide if they want to use it or not.
REFERENCES
[1]
A. Vikne and P. Ellingsen, “Client-Side XSS Filtering in Firefox,” in
SOFTENG 2018, The Fourth International Conference on Advances
and Trends in Software Engineering, April 2018, pp. 24–29.
[2]
OWASP Foundation, “Owasp top 10 - 2017 the ten most critical
web
application
security
risks,”
accessed:
2017-12-27.
[Online].
Available: https://www.owasp.org/images/7/72/OWASP Top 10-2017
%28en%29.pdf.pdf
[3]
WhiteHat
Security,
Inc.,
“2017
whitehat
security
application
security statistics report,” 2017, accessed: 2017-12-21. [Online].
Available: https://info.whitehatsec.com/rs/675-YBI-674/images/WHS%
202017%20Application%20Security%20Report%20FINAL.pdf
[4]
Bugcrowd Inc., “2017 state of bug bounty report,” 2017, accessed:
2018-01-09. [Online]. Available: https://pages.bugcrowd.com/hubfs/
Bugcrowd-2017-State-of-Bug-Bounty-Report.pdf
[5]
I. Hydara, A. B. M. Sultan, H. Zulzalil, and N. Admodisastro, “Current
state of research on cross-site scripting (XSS)–A systematic literature
review,” Information and Software Technology, vol. 58, 2015, pp. 170–
186.
[6]
Alexa Internet, Inc., “The top 500 sites on the web,” January
2018, accessed: 2018-01-15. [Online]. Available: https://www.alexa.
com/topsites
[7]
StatCounter, “Desktop browser market share worldwide,” May 2018,
accessed: 2018-05-09. [Online]. Available: http://gs.statcounter.com/
browser-market-share/desktop/worldwide

438
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[8]
Mozilla
Developer
Network,
“Conﬁdentiality,
Integrity,
and
Availability,” April 2018, accessed: 2018-04-19. [Online]. Available:
https://developer.mozilla.org/en-US/docs/Web/Security/Information
Security Basics/Conﬁdentiality, Integrity, and Availability
[9]
M. Alvarez, N. Bradley, P. Cobb, S. Craig, R. Iffert, L. Kessem,
J. Kravitz, D. McMillen, and S. Moore, “IBM X-Force Threat In-
telligence Index 2017 The Year of the Mega Breach,” IBM Secu-
rity,(March), 2017, pp. 1–30.
[10]
Mozilla Developer Network, “HTTP headers,” April 2018, accessed
2018-05-13. [Online]. Available: https://developer.mozilla.org/en-US/
docs/Web/HTTP/Headers
[11]
——, “Referer,” June 2017, accessed 2018-05-13. [Online]. Available:
https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Referer
[12]
——,
“User-Agent,”
June
2017,
accessed
2018-05-13.
[On-
line]. Available: https://developer.mozilla.org/en-US/docs/Web/HTTP/
Headers/User-Agent
[13]
S. Di Paola and G. Fedon, “Subverting ajax,” 2006.
[14]
Facebook
Inc.,
“Httponly,”
August
2017,
accessed:
2018-03-23.
[Online]. Available: https://www.facebook.com/help/246962205475854
[15]
Mozilla Developer Network, “DOM on-event handlers,” Jan 2018,
accessed 2018-05-24. [Online]. Available: https://developer.mozilla.
org/en-US/docs/Web/Guide/Events/Event handlers
[16]
OWASP Foundation, “Xss ﬁlter evasion cheat sheet,” October 2017,
accessed: 2017-12-27. [Online]. Available: https://www.owasp.org/
index.php/XSS Filter Evasion Cheat Sheet
[17]
——, “Xss (cross site scripting) prevention cheat sheet,” October
2017, accessed: 2018-01-24. [Online]. Available: https://www.owasp.
org/index.php/XSS (Cross Site Scripting) Prevention Cheat Sheet
[18]
Mozilla Developer Network, “Content security policy (csp),” January
2018, accessed: 2018-01-24. [Online]. Available: https://developer.
mozilla.org/en-US/docs/Web/HTTP/CSP
[19]
The World Wide Web Consortium, W3C, “Content security policy
level 2,” December 2016, accessed: 2018-01-11. [Online]. Available:
https://www.w3.org/TR/2016/REC-CSP2-20161215/
[20]
L. Weichselbaum, M. Spagnuolo, S. Lekies, and A. Janc, “Csp is dead,
long live csp! on the insecurity of whitelists and the future of content
security policy,” in Proceedings of the 2016 ACM SIGSAC Conference
on Computer and Communications Security.
ACM, 2016, pp. 1376–
1387.
[21]
Mozilla Developer Network, “Same-origin policy,” March 2018,
accessed: 2018-03-21. [Online]. Available: https://developer.mozilla.
org/en-US/docs/Web/Security/Same-origin policy
[22]
OWASP Foundation, “Httponly,” August 2017, accessed: 2018-03-21.
[Online]. Available: https://www.owasp.org/index.php/HttpOnly
[23]
S. Gupta and B. B. Gupta, “Cross-Site Scripting (XSS) attacks and
defense mechanisms: classiﬁcation and state-of-the-art,” International
Journal of System Assurance Engineering and Management, vol. 8,
no. 1, 2017, pp. 512–530.
[24]
D. Bates, A. Barth, and C. Jackson, “Regular expressions considered
harmful in client-side xss ﬁlters,” in Proceedings of the 19th interna-
tional conference on World wide web.
ACM, 2010, pp. 91–100.
[25]
G. Maone, “NoScript - JavaScript/Java/Flash blocker for a safer
Firefox experience! - features - InformAction,” accessed: 2017-12-28.
[Online]. Available: https://noscript.net/features
[26]
B. Stock, S. Lekies, T. Mueller, P. Spiegel, and M. Johns, “Pre-
cise client-side protection against dom-based cross-site scripting.” in
USENIX Security Symposium, 2014, pp. 655–670.
[27]
Mozilla Developer Network, “eval,” April 2018, accessed 2018-05-
29. [Online]. Available: https://developer.mozilla.org/en-US/docs/Web/
JavaScript/Reference/Global Objects/eval
[28]
D. Ross, “Ie8 security part iv: The xss ﬁlter,” July 2008, accessed:
2018-01-11. [Online]. Available: https://blogs.msdn.microsoft.com/ie/
2008/07/02/ie8-security-part-iv-the-xss-ﬁlter/
[29]
Mozilla Developer Network, “History of the Mozilla Project,” April
2018, accessed: 2018-04-04. [Online]. Available: https://www.mozilla.
org/en-US/about/history/details
[30]
——, “An introduction to hacking mozilla,” March 2017, accessed:
2017-12-28. [Online]. Available: https://developer.mozilla.org/en-US/
docs/Mozilla/An introduction to hacking Mozilla
[31]
——, “Introduction,” September 2014, accessed: 2017-12-28. [Online].
Available: https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XUL/
Tutorial/Introduction
[32]
——, “Gecko faq,” September 2015, accessed: 2017-12-28. [Online].
Available: https://developer.mozilla.org/en-US/docs/Gecko/FAQ
[33]
——,
“How
mozilla’s
build
system
works,”
De-
cember
2017,
accessed:
2018-02-14.
[Online].
Avail-
able:
https://developer.mozilla.org/en-US/docs/Mozilla/Developer
guide/Build Instructions/How Mozilla s build system works
[34]
GNU/Free Software Foundation, “Gnu make,” May 2016, accessed:
2018-02-14. [Online]. Available: https://www.gnu.org/software/make/
[35]
Mozilla Developer Network, “mach,” December 2017, accessed:
2018-02-14. [Online]. Available: https://developer.mozilla.org/en-US/
docs/Mozilla/Developer guide/mach
[36]
I. Black Duck Software, “The Mozilla Firefox Open Source Project
on Open Hub: Languages Page,” April 2018, accessed 2018-05-22.
[Online]. Available: https://www.openhub.net/p/ﬁrefox/analyses/latest/
languages summary
[37]
Mozilla
Developer
Network,
“Mozilla
Source
Code
Di-
rectory
Structure,”
Jan
2018,
accessed
2018-05-22.
[On-
line].
Available:
https://developer.mozilla.org/en-US/docs/Mozilla/
Developer guide/Source Code/Directory structure
[38]
——, “HTML parser threading,” March 2013, accessed 2018-05-30.
[Online]. Available: https://developer.mozilla.org/en-US/docs/Mozilla/
Gecko/HTML parser threading
[39]
——,
“Script
Security,”
Aug
2016,
accessed
2018-05-24.
[Online]. Available: https://developer.mozilla.org/en-US/docs/Mozilla/
Gecko/Script security
[40]
——,
“Browser
Extensions,”
March
2018,
accessed
2018-05-
27. [Online]. Available: https://developer.mozilla.org/en-US/Add-ons/
WebExtensions
[41]
——, “Browser support for JavaScript APIs,” May 2018, accessed
2018-05-27. [Online]. Available: https://developer.mozilla.org/en-US/
Add-ons/WebExtensions/Browser support for JavaScript APIs
[42]
“Arch Linux,” Jan 2018, accessed 2018-05-30. [Online]. Available:
https://wiki.archlinux.org/index.php/Arch Linux
[43]
Microsoft, “Visual Studio Code - Code Editing. Redeﬁned,” accessed
2018-05-30. [Online]. Available: https://code.visualstudio.com/
[44]
Mozilla
Developer
Network,
“Conﬁguring
Build
Op-
tions,”
March
2018,
accessed
2018-05-30.
[On-
line].
Available:
https://developer.mozilla.org/en-US/docs/Mozilla/
Developer guide/Build Instructions/Conﬁguring Build Options
[45]
GNU/Free Software Foundation, “Gdb: The gnu project debugger,”
February 2018, accessed: 2018-03-02. [Online]. Available: https:
//www.gnu.org/software/gdb/
[46]
R. D. W3Schools, “HTML URL Encoding Reference,” May 2018,
accessed: 2018-05-02]. [Online]. Available: https://www.w3schools.
com/tags/ref urlencode.asp
[47]
S.
Project,
“Selenium
WebDriver
-
Selenium
Documentation,”
April 2018, accessed 2018-05-02. [Online]. Available: https://www.
seleniumhq.org/docs/03 webdriver.jsp

439
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[48]
Balaji N., “Top 500 most important xss script cheat sheet for web appli-
cation penetration testing,” May 2018, accessed 2018-05-30. [Online].
Available: https://gbhackers.com/top-500-important-xss-cheat-sheet/
[49]
Thapa, Prabesh, “Xss-payloads,” Aug 2016, accessed 2018-05-30.
[Online]. Available: https://github.com/Pgaijin66/XSS-Payloads/blob/
master/payload.txt
[50]
FuzzDB
Project,
“xss-other.txt,”
Oct
2016,
accessed
2018-05-
30. [Online]. Available: https://github.com/fuzzdb-project/fuzzdb/blob/
master/attack/xss/xss-other.txt
[51]
——, “xss-rsnake.txt,” May 2016, accessed 2018-05-30. [Online].
Available: https://github.com/fuzzdb-project/fuzzdb/blob/master/attack/
xss/xss-rsnake.txt
[52]
D. Strohmeier, P. Dolanjski, “Comparing browser page load time:
An
introduction
to
methodology,”
November
2017,
accessed:
2018-01-15.
[Online].
Available:
https://hacks.mozilla.org/2017/11/
comparing-browser-page-load-time-an-introduction-to-methodology/
[53]
Mozilla Developer Network, “Mozilla internal string guide,” April
2018, accessed 2018-05-25. [Online]. Available: https://developer.
mozilla.org/en-US/docs/Mozilla/Tech/XPCOM/Guide/Internal strings
[54]
——,
“X-XSS-Protection,”
Feb
2018,
accessed
2018-05-28.
[Online].
Available:
https://developer.mozilla.org/en-US/docs/Web/
HTTP/Headers/X-XSS-Protection

