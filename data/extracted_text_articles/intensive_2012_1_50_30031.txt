Efﬁcient Color-Based Image Segmentation and Feature Classiﬁcation
for Image Processing in Embedded Systems
Alexander Jungmann, Bernd Kleinjohann, Lisa Kleinjohann
C-LAB
University of Paderborn, Germany
{global, bernd, lisa}@c-lab.de
Maarten Bieshaar
Faculty of Computer Science,
Electrical Engineering and Mathematics
University of Paderborn, Germany
maarten@mail.uni-paderborn.de
Abstract—In this paper, a color based feature extraction
and classiﬁcation approach for image processing in embedded
systems in presented. The algorithms and data structures
developed for this approach pay particular attention to reduce
memory consumption and computation power of the entire
image processing, since embedded systems usually impose
strong restrictions regarding those resources. The feature
extraction is realized in terms of an image segmentation
algorithm. The criteria of homogeneity for merging pixels and
regions is provided by the color classiﬁcation mechanism, which
incorporates appropriate methods for deﬁning, representing
and accessing subspaces in the working color space. By doing
so, pixels and regions with color values that belong to the
same color class can be merged. Furthermore, pixels with
redundant color values that do not belong to any pre-deﬁned
color class can be completely discarded in order to minimize
computational effort. Subsequently, the extracted regions are
converted to a more convenient feature representation in terms
of statistical moments up to and including second order. For
evaluation, the whole image processing approach is applied to
a mobile representative of embedded systems within the scope
of a simple real-world scenario.
Keywords-Image Segmentation; Feature Classiﬁcation; Run-
Length Encoding; Moments; Embedded Systems
I. INTRODUCTION
Image processing is a crucial service needed by many
intelligent applications like driver assistance, surveillance or
production in order to obtain and analyze visual information
about the current situation. Usually this data and processing
intensive task has strong timing requirements and is often
realized on embedded systems. Although the performance
capabilities of such embedded systems rise steadily, im-
age processing approaches that stick appropriately to the
prevalent restrictions regarding computational power and
memory are required. Therefore many image processing
solutions are realized on special hardware components such
as Field Programmable Gate Arrays (FPGAs) or Digital
Signal Processors (DSPs) in order to increase the processing
speed. But this reduces the portability of the approach.
In contrast, the approach presented in this paper can be
applied to any embedded system that provides a Linux
based operating system., independent of the underlying CPU
instruction set.
The focus of this paper lies on the low level image
processing phase, namely the combination of an efﬁcient
image segmentation algorithm with a simple but ﬂexible
feature classiﬁcation mechanism. Algorithms and data struc-
tures were optimized with regard to computational effort and
memory consumption. By image segmentation the original
pixel-based data is drastically reduced to a feature-level
representation, which requires signiﬁcantly less memory on
the one hand and is more convenient as well as less com-
putational expensive for subsequent information processing
steps on the other hand. As a convenient feature descriptor,
which further reduces the visual data produced by the image
segmentation, an optimized representation of features based
on statistical moments is used. For feature classiﬁcation a
memory efﬁcient color class representation together with
computationally efﬁcient access methods was developed,
that nevertheless allows a representation of sophisticated
color spaces made up of several subspaces. The feature
classiﬁcation is realized by two ﬂexible heuristics. They are
integrated into the segmentation process, in order to avoid
further processing of pixels in the original image that can
be classiﬁed as not relevant due to their color, thus reducing
the computational effort even further.
A simple evaluation scenario with an embedded system
- the miniature robot BeBot [1] - was developed to prove
the performance of the presented approach. Within this
scenario, the BeBot only relies on visual data in terms of
images captured by its camera and processed by the image
processing approach presented in this paper. By doing so,
subsequent object detection and information processing steps
for autonomous behavior are facilitated.
This paper is organized as follows. At ﬁrst, the basic
segmentation algorithm as well as a convenient feature
representation are presented in Section II. Section III subse-
quently describes a ﬂexible color-based feature classiﬁcation
mechanism. Afterwards, the real-world scenario, in which
the whole image processing approach was successfully ap-
plied, is introduced in Section IV, followed by the associated
results in Section V. Some existing approaches that are sim-
ilar with respect to the basic concepts are brieﬂy discussed
in Section VI. Finally, the paper concludes with Section VII.
22
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-188-5
INTENSIVE 2012 : The Fourth International Conference on Resource Intensive Applications and Services

II. FEATURE EXTRACTION BY IMAGE SEGMENTATION
The fundamental idea of image segmentation is the iden-
tiﬁcation of contiguous blocks of pixels (so-called regions)
that are homogeneous with respect to a pre-deﬁned criterion.
Afterwards, those regions are abstracted into features that are
compactly described in terms of position, size and shape. By
doing so, the original pixel-based data is drastically reduced
to a feature-level representation, which requires signiﬁcantly
less memory. Furthermore, features are more convenient
as well as less computationally expensive for subsequent
information processing steps. In fact, the overall challenge
of image segmentation on embedded systems is to extract ad-
equate features in order to reduce the memory consumption
and computational effort for subsequent processing steps
while simultaneously minimizing the computational effort
of the actual segmentation process.
For that reason, our approach was developed to process an
entire image only once, while simultaneously identifying and
extracting regions, which are in turn compactly represented
in order to reduce the memory consumption.
A. Region Representation
A convenient representation of a region during the seg-
mentation process is crucial since it depends on the available
memory and computational power. For that reason, our seg-
mentation approach incorporates the Run-Length Encoding
concept [2]. Sequences of adjacent pixels are encoded as
runs, where a single run is compactly represented by means
of only three integer values:
runi = (xi,yi,li),
(1)
with xi,yi being the coordinates of the starting pixel and li
denoting the number of adjacent pixels within the same row.
Furthermore, a region R may consist of a set of n runs:
R = {(x1,y1,l1),(x2,y2,l2),...,(xn,yn,ln)}
(2)
While adding a pixel to a region is nothing but incrementing
the length li of the related run runi, merging two regions is
realized by simply appending the sequence of runs of the
ﬁrst region to the sequence of runs of the second region.
B. Segmentation Algorithm
The entire image segmentation process is depicted in
Algorithm 1. The main loop (lines 8-43) iterates row by
row over the whole image, starting at the topmost row. The
inner iteration loop (lines 9-42) processes each pixel within
a row once, starting with the leftmost pixel.
After identifying a possibly existing left adjacent run
runleft as well as its associated region Rle ft (lines 10-14),
a heuristic Hi (cf. Section III-B) is applied in order to
check if the current pixel is interesting at all (line 15). If
so, a heuristic Hm (cf. Section III-B) is applied in order
to check if the current pixel and the left adjacent region
Rleft can be merged (line 17). If heuristic Hm succeeds,
Algorithm 1 Segmentation Algorithm
1: img = latest camera image
2: regions = {∅}
3: lengthmin = minimal length of a single run
4: /* heuristic for deciding if a pixel is interesting at all */
5: Hi = configure(interesting heuristic)
6: /* heuristic for deciding if pixels and/or regions can be merged */
7: Hm = configure(merge heuristic)
8: for all row ∈ img do
9:
for all pixel ∈ row do
10:
runleft = left ad jacent run(pixel)
11:
if runleft not exists then
12:
goto line 35 /* start new run */
13:
end if
14:
Rleft = region(runleft) /* runleft’s associated region */
15:
if Hi(pixel) is TRUE then
16:
/* current pixel is interesting */
17:
if Hm(pixel,Rleft) is TRUE then
18:
add(runleft, pixel) /* region growing */
19:
goto line 9 /* continue with next pixel */
20:
end if
21:
end if
22:
/* ﬁnalize a previously built left adjacent run */
23:
if length(runleft) < lengthmin then
24:
remove(regions,Rleft)
25:
else
26:
regionstop = top ad jacent regions(runleft)
27:
for all Rtop ∈ regionstop do
28:
if Hm(Rleft,Rtop) is TRUE then
29:
/* region merging */
30:
merge(Rleft,Rtop)
31:
remove(regions,Rtop)
32:
end if
33:
end for
34:
end if
35:
if Hi(pixel) is TRUE then
36:
/* start new run with current pixel as ﬁrst pixel */
37:
runnew = new run(pixel)
38:
/* start new region with current run as ﬁrst run */
39:
Rnew = new region(runnew)
40:
add(regions,Rnew)
41:
end if
42:
end for
43: end for
44: return regions
the region growing step takes place by adding the current
pixel to the left adjacent run runleft (length of runleft is
incremented by value 1) and updating the associated region
Rleft with respect to its average color. Afterwards, the
algorithm directly continues with the next pixel (line 19).
If either heuristic Hi or Hm fails, the left adjacent run
23
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-188-5
INTENSIVE 2012 : The Fourth International Conference on Resource Intensive Applications and Services

runleft is considered to be complete. If its length is smaller
than the pre-deﬁned minimal length value lengthmin (line
23), runleft as well as its associated region Rle ft are dis-
carded by removing Rle ft from the set of heretofore iden-
tiﬁed regions (line 24). Otherwise, the algorithm proceeds
with its region merging mechanism. For this purpose, all
regions bordering run runle ft at the top are identiﬁed. Sub-
sequently, another iteration loop tries to identify every region
Rtop within regionstop that can be merged with the runleft
associated region Rle ft (lines 27-33) by again applying
heuristic Hm. If Hm succeeds for two regions Rtop and Rleft,
the regions are merged by appending all runs of region Rtop
to Rleft (by simply changing pointers). Furthermore, region
Rleft is updated with respect to its average color, whereas
region Rtop is completely discarded by removing it from the
set of heretofore identiﬁed regions.
Independent of the result of the previous region merging
mechanism, a new run runnew with the current pixel as its
starting position as well as a new region Rnew with runnew
as its ﬁrst run are allocated. But only, if the current pixel is
interesting. Last but not least, the region Rnew is added to
the set of heretofore identiﬁed regions.
In fact, the arrangement of the several conditional clauses
are optimized in the ﬁnal implementation in order to min-
imize their redundant invocations. However, the depicted
algorithmic structure was chosen to ease understanding of
the algorithm’s overall functionality.
C. Feature Descriptor
For high level object detection processes, a convenient
feature descriptor, which further reduces the visual data
in comparison to the run-based representation, is required.
Furthermore, it should incorporate a model of uncertainty
that takes stochastic errors such as sensor noise into account.
For that reason, we decided for an implicit representation in
terms of statistical quantities.
By interpreting a region and its associated pixels as a
two-dimensional Gaussian distribution in the image plane,
a region can be implicitly described by means of statistical
parameters, namely the two mean values mx and my, the two
variances σ2
x and σ2
y, and the covariance σxy. A generaliza-
tion of these speciﬁc parameters are the statistical moments.
In this context, the two mean values correspond to the two
moments of ﬁrst order (m10 and m01), whereas the two
variances and the covariance correspond to the centralized
(or central) moments of second order (µ20, µ02 and µ11):
⃗m =

mx
my

=

m10
m01

(3)
Σ =
 σ2
x
σxy
σxy
σ2
y

=

µ20
µ11
µ11
µ02

(4)
In the ﬁeld of digital image processing, this statistical
approach can be directly applied in terms of discretized
moments [3]. In general, a discretized two-dimensional
moment mpq of order p+q belonging to a region R is deﬁned
as
mpq = ∑
(x,y)∈R
xpyq,
(5)
meaning nothing but only the coordinates (x,y) that belong
to R have to be considered for computing the sum. Fur-
thermore, in our optimized approach, the required central
moments of second order are efﬁciently computed by means
of the moments up to and including second order:
µ20 = m20 − m2
10
m00
,
µ02 = m02 − m2
01
m00
,
µ11 = m11 − m10 ·m01
m00
.
(6)
Consequently, for representing a single region as a two-
dimensional Gaussian distribution, our basic feature descrip-
tor only comprises the following set M of moments:
M = {mpq|p+q ≤ 2}
= {m00,m10,m01,m11,m20,m02}
(7)
For efﬁciently converting the region representation in
terms of runs into the feature descriptor, the Delta ”δ”
Method [4] is applied by directly incorporating the numerical
quantities of a run, namely the coordinates xi and yi of its
starting pixel as well as its length li. For this purpose, let
S1i and S2i be deﬁned as follows:
S1i =
li−1
∑
k=0
k = (l2
i −li)
2
S2i =
li−1
∑
k=0
k2 = l3
i
3 − l2
i
2 + li
6
(8)
The required geometric moments m00i, m10i, m01i, m11i, m20i
and m02i of a single run runi are then computed in the
following optimized way:
m00i = li
m01i = li ·yi
m10i = li ·xi +S1i
m02i = li ·y2
i
m20i = li ·x2
i +2·S1i ·xi +S2i
m11i = li ·[li ·xi +S1i] = yi ·m10i
(9)
Finally, the moments of a region R correspond to the sums
of the particular moments of all related n runs:
M = {mpq|mpq =
n
∑
i=1
mpqi ∧ p+q ≤ 2}
(10)
As shown by Prokop and Reeves [5], an adequate set of
additional attributes can be derived from these moments
24
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-188-5
INTENSIVE 2012 : The Fourth International Conference on Resource Intensive Applications and Services

a
b

y
x
x'
y'
Figure 1.
Representation of an ellipse by its major axis x′, minor axis y′
and angle of inclination φ.
(and central moments), in order to additionally describe a
feature in a more explicit manner. In this context, the mass
of a feature corresponds to the number of related pixels.
It is, therefore, equivalent to the zeroth order moment m00.
Furthermore, the coordinates (x,y) of the center of mass of
a feature in the image plane are deﬁned by means of the
moments of zeroth and ﬁrst order:
x = m10
m00
and y = m01
m00
(11)
In addition, according to [6], by statistically describing a
feature in terms of moments up to and including second or-
der, the feature is equivalent to a elliptical disk (see Figure 1)
with constant intensity, having deﬁnite size, orientation and
eccentricity and being centered at the origin of the image
plane. Its major radius a and minor radius b as well as its
orientation φ (see Table I) can be in turn derived from the
central moments in the following way:
a =
v
u
u
u
t2

µ20 +µ02 +
q
(µ20 −µ02)2 +4µ2
11

µ00
(12)
b =
v
u
u
u
t(
2

µ20 +µ02 −
q
(µ20 −µ02)2 +4µ2
11

µ00
(13)
III. FEATURE CLASSIFICATION
The classiﬁcation mechanism enables the user to deﬁne
so called color classes for classifying pixels, thus features
depend on their color. These color classes provide the basics
for the heuristics Hi and Hm needed by the segmentation
algorithm (cf. Section II-B).
A. Representation and Deﬁnition of the Color Classes
Color Classes in our context are nothing but an id rep-
resenting a subspace within the working color space (i.e.,
YUV or RGB), which is in turn represented in terms of
a cube (see Figure 2) with the particular color channels
corresponding to the three dimensions. Each channel has a
maximal resolution of 8bit, leading to 256×256×256 possi-
ble combinations for addressing a position in the respective
Table I
DETERMINING THE ANGLE OF INCLINATION φ AS A FUNCTION OF THE
CENTRAL MOMENTS OF SECOND ORDER.
µ20 −µ02
µ11
φ
0
0
0
0
+
+π/4
ξ =
2µ11
µ20−µ02
0
−
−π/4
+
0
0
−
0
−π/2
+
+
(1/2)·arctanξ
(0 < φ < π/4)
+
−
(1/2)·arctanξ
(−π/4 < φ < 0)
−
+
(1/2)·arctanξ+π/2
(π/4 < φ < π/2)
−
−
(1/2)·arctanξ−π/2
(−π/2 < φ < −π/4)
space. By realizing the color space as a look up table in
terms of a one-dimensional array of integer values instead
of a three-dimensional one, the number of memory accesses
are minimized. For indicating, whether a coordinate within
the color space is assigned to a color class with id i, the ith
bit of the respective integer value is set to 1. Consequently,
the number of possible color classes is bound to the chosen
integer size (e.g., an 8bit integer allows the application of
8 different color classes). Additionally, by following this bit
based approach, a coordinate can be simultaneously assigned
to more than one color class at the same time. Furthermore,
necessary mechanisms such as inserting and removing a
color class as well as testing if a color tuple is assigned
to a color class can be consequently implemented in terms
of simple but very efﬁcient bit operations.
The resolution of the channels can be restricted by deﬁn-
ing a quantization factor λ ∈ {2i|i = 1,2,...,8} in order to
drastically reduce the over all memory consumption. Graph-
ically, this means that the whole color space is no longer
represented by 256×256×256 different color combinations,
but 256/λ × 256/λ × 256/λ statically allocated sub-cubes,
where color combinations that are located within the same
sub-cube are not further differentiated but considered as
identical. A color class, thus in general is deﬁned by a set
of sub-cubes of size λ×λ×λ in the color space, where sub-
cubes may belong to several color classes as already men-
Y
U
V
B
G
R
Figure 2.
YUV and RGB color spaces, represented as three-dimensional
cubes.
25
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-188-5
INTENSIVE 2012 : The Fourth International Conference on Resource Intensive Applications and Services

tioned above. As a consequence, the resolution and hence
the granularity of the color space is signiﬁcantly decreased,
while the robustness of the overall feature classiﬁcation is
increased with respect to image noise. Dependent on the
chosen quantization factor λ, the reduced granularity still
leaves enough space for adequately distinguishing different
colors.
Accessing the integer value of a color tuple (c1,c2,c3) in
the look up table lut is done by
lut[f(c1,c2,c3)]
(14)
with
f(c1,c2,c3) =(c1/λ)·(256/λ)2
+(c2/λ)·256/λ
+(c3/λ)
(15)
By only allowing powers of two as values for λ, the devision
by λ can be replaced by a bit shift mechanism:
f(c1,c2,c3) =(c1 ≫ λs)·(256 ≫ λs)2
+(c2 ≫ λs)·(256 ≫ λs)
+(c3 ≫ λs),
(16)
with
λs = log2λ.
(17)
The two multiplications can be also replaced as follows:
f(c1,c2,c3) =((c1 ≫ λs) ≪ c1s)
+((c2 ≫ λs) ≪ c2s)
+(c3 ≫ λs),
(18)
with
c1s = log2((256/λ)2) = log2((256 ≫ λs)2)
(19)
and
c2s = log2(256/λ) = log2(256 ≫ λs).
(20)
Since λs, c1s and c2s have to be calculated only once
whenever the quantization factor λ is changed (and hence the
overall resolution of the color space), calculating the position
as a function of a color tuple (c1,c2,c3) for accessing
the appropriate integer value in the look up table is very
efﬁcient.
In order to deﬁne a color class i as a subspace within
the color space, the well known Flood ﬁll [7] approach is
applied. In this context, a user has to select a particular pixel
in order to indicate the starting position in a camera image.
Dependent on the conﬁguration parameters, the algorithm
recursively detects pixels with similar color in the adjacent
neighborhood, taking each detected pixel as a new starting
position. After termination, the color tuples (c1,c2,c3) of
all identiﬁed pixels are marked within the color space to be
associated to color class i by setting bit i of the related sub-
cube (i.e. of the integer value at the particular position in
the one-dimensional array) to 1.
B. Heuristics for Segmentation
The necessary heuristics Hm for deciding whether a region
growing or region merging step should take place and Hi for
determining if a given pixel is interesting at all can now be
easily constructed.
Let (c1,c2,c3) be the color values of the current pixel.
By applying (18), the associated position in the previously
mentioned look up table can be computed and accessed.
Then, by simply checking whether the value read from the
memory is different to zero or not (i.e., if at least one bit
is set), the heuristic Hi is able to determine, if the pixel is
interesting or not:
Hi(pixel) = Hi(c1,c2,c3)
=

1
if
lut[f(c1,c2,c3)] ̸= 0
0
else.
(21)
For allowing that, e.g., two regions can be merged, their
estimated average color values (c1,c2,c3) and (c4,c5,c6)
have to be assigned to the same color class. For that purpose,
the respective entries in the look up table have to be read and
compared. Since the assignment of a color class in the look
up table is done by activating bits, the comparison of the
two integer values can be simply implemented by a bitwise
AND operator (&) and a subsequent test for 0, meaning that
there are no bits, which are set to 1 in both values. Hence,
heuristic Hm can be realized as follows:
Hm = Hm((c1,c2,c3),(c4,c5,c6))
=











1
if
(lut[f(c1,c2,c3)]
&
lut[f(c4,c5,c6)]) ̸= 0,
0
else.
(22)
IV. EVALUATION SCENARIO
For evaluating the functional principle of the presented
approach, a simple real-world scenario that incorporates the
miniature robot BeBot [1] was implemented.
A. BeBot Speciﬁcations
The BeBot is a miniature chain driven robot (shown in
Figure 3(a)), which has been developed at the Heinz Nixdorf
Institute of the University of Paderborn. Despite its small
size (approximately 9 cm x 9 cm x 8 cm), it is equipped with
modern technology distributed on two modular boards. The
lower one of these boards implements basic functionalities
such as controlling the motors by means of an ARM 7 based
microcontroller. In contrast, the upper board constitutes a
more powerful computing platform. Whereas a camera that
is mounted at the front allows the BeBot to perceive its
environment within a limited ﬁeld of view, algorithms for
convenient information processing steps can be executed
in an embedded Linux environment running on an ARM
Cortex-A8 CPU with a maximum frequency of 600 MHz
26
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-188-5
INTENSIVE 2012 : The Fourth International Conference on Resource Intensive Applications and Services

camera
passive gripper
differential 
chain drive
led dome
(a)
vision
10 Hz
worldmodel
10 Hz
move
10 Hz
behavior
5 Hz
leddome
3 Hz
(b)
Figure 3.
(a) The miniature robot BeBot, pushing an orange ping-pong
ball. (b) The node structure of the implemented behavior system.
accessing up to 256MB RAM of main memory. The BeBot is
additionally equipped with a passive gripper, which enables
the robot to push small objects such as ping-pong balls.
Furthermore, by illuminating its so called led dome in
arbitrary colors during runtime, the miniature robot can
express its current state to human observers and other robots,
respectively.
The network based TCP/IP communication is exclusively
based on Bluetooth. For that reason, an approach that relies
on sending raw visual data to external systems in order to
apply computational expensive image processing algorithms
is not feasible.
B. Scenario Description
The overall task of a BeBot in our evaluation scenario
is to push a single-colored ping-pong ball through a slalom
course, which consists of small pylons that are arranged in a
straight line (see Figure 4 for a schematic illustration). For
facilitating the identiﬁcation of the start and the ﬁnish of
the course, simple markers with a single color are attached
to the respective points. Furthermore, the ping-pong ball is
initially positioned somewhere around the course.
First of all, the robot has to search and catch the ball and
return with it to the start point. Afterwards, it has to push
the ball through the slalom course without loosing it. If the
BeBot looses the ball, it has to start all over again. When
successfully ﬁnished its drive through the course, the robot
has to stop while blinking with its led dome.
Finish
Pylone
Pylone
Ball
BeBot
Figure 4.
Schematic bird’s eye view of the evaluation scenario.
C. Realization of the BeBot Behavior System
The node-based implementation of the BeBot’s behavior
system is depicted in Figure 3(b). The vision node incor-
porates all image processing steps including the presented
segmentation and classiﬁcation approach as well as a simple
object detection mechanism in order to recognize the sce-
nario’s particular objects. The reduced visual information is
subsequently transmitted to the worldmodel node, in which a
very abstract representation of the BeBot’s state within the
environment is accumulated. The overall strategy is com-
puted in the behavior node on a very abstract level. Those
abstract behaviors (e.g., DriveSlalom, CatchBall) do not
change quickly and do not contain any detailed information
about how to achieve the respective goal. Furthermore, the
connected leddome node changes the color of the led dome
according to the current overall behavior. Finally, the low
level behaviors (e.g., MoveCurve, SearchBall) that output the
commands for the differential chain drive are implemented
in the move node.
V. RESULTS
The presented approach was completely implemented in
C++ and was successfully applied to the BeBot within
the scope of the evaluation scenario. The resolution of
the images grabbed by the BeBot camera in the YUV
color space was set to 320 × 240 pixels. Furthermore, the
quantization factor λ was set to 16, resulting in a resolution
of each channel in the YUV color space of 256/λ = 16.
Due to the setup of the scenario, the number of color classes
were restricted to less then 8. For that reason, the look up
table was initialized with 8bit integer values. Consequently,
the overall memory consumption of the basic classiﬁcation
array was
(256/16·256/16·256/16)·1Byte = 4096Byte = 4kB (23)
The subspaces corresponding to the different color classes
within the working color space are depicted in Figure 5.
In order to demonstrate the efﬁciency of the introduced
approach, a typical subjective view of the scenario setup
(see Figure 6(a)) was selected for further experimental
investigations. Within the scope of these experiments, the
frequency of the upper board’s ARM CPU was ﬁxed to
520Mhz. Furthermore, the behavior system was disabled in
order to run the image processing as standalone process.
Four different test cases were considered. The particular
results are shown in Table II. In this context, case A solely
contains the image acquisition step as evaluation basis. In
addition, case A was used for identifying the maximum
possible frame rate (16 fps) currently supported by the
BeBot.
Case B incorporates a modiﬁed version of the presented
segmentation approach: the classiﬁcation mechanisms were
completely discarded. Consequently, all pixels were sup-
posed to be of interest and therefore considered during the
27
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-188-5
INTENSIVE 2012 : The Fourth International Conference on Resource Intensive Applications and Services

(a) YUV
(b) RGB
Figure 5.
(a) Representation of the different color classes (pylones, ball,
marker) in the YUV color space, in which the whole image processing
takes place. The inﬂuence of the quantization factor λ can be seen in terms
of the topology of the particular subspaces, which consist of small cubes.
Additionally, (b) depicts the deﬁned color classes transformed into the RGB
color space.
segmentation step. Furthermore, the original heuristic Hm
(22) for deciding whether a region growing or region merg-
ing step should take place was replaced by a heuristic, which
determines, whether two color tuples reside in a predeﬁned
neighborhood within the YUV color space. Figure 6(b)
shows the processed image as well as the extracted features
in terms of their respective centers of mass and ellipses.
Based on the classiﬁcation independent segmentation
in case B, case C additionally incorporates the presented
classiﬁcation approach. However, the classiﬁcation was not
applied during the segmentation process on pixel level,
but as a subsequent processing step on feature level. The
classiﬁed features can be seen in Figure 6(c) in terms of their
colored ellipses, where the color of an ellipse represents the
related color class.
The advantages of the original approach (case D), where
the classiﬁcation heuristics Hi and Hm are applied during
segmentation as described in Algorithm II-B, become ob-
vious by comparing the performance values as well as the
results that are depicted in Figure 6(d) and Figure 6(e) with
the previously mentioned results. Since all redundant pixels
that are not of interest are already discarded during the
segmentation step, the amount of visual data is drastically
reduced and only scenario relevant features are extracted.
As a consequence, the workload of the CPU is signiﬁcantly
decreased. Furthermore, since only pixels that belong to the
Table II
PERFORMANCE VALUES OF THE FOUR DIFFERENT TEST CASES.
Case
Related ﬁgures
CPU
Mem.
P. time
#F
fps
A
Fig. 6(a)
4%
8.6%
-
-
16
B
Fig. 6(b)
79%
10.3%
46.7 ms
29
16
C
Fig. 6(c)
80%
10.5%
47.1 ms
29
16
D
Fig. 6(d), Fig. 6(e)
29%
10.5%
15.3 ms
10
16
P. time: Average processing time (ms).
#F: Average number of extracted features.
fps: Frames per second.
(a) case A
(b) case B
(c) case C
(d) case D
(e) case D
(f)
Figure 6.
A BeBot’s typical subjective view of the scenario setup: (a)
original image, (b) result of image segmentation without incorporating
classiﬁcation, (c) subsequent feature classiﬁcation, (d) original and (e) result
image of the combined approach, (f) detected scenario relevant objects.
same color class are merged, regions being more proper
are constructed during the segmentation step (compare, e.g.,
the three extracted and classiﬁed features of the ball in
Figure 6(c) with the one feature of the ball in Figure 6(d)).
Finally, Figure 6(f) shows a representative result of the
entire image processing (presented approach + object detec-
tion) within the scope of the evaluation scenario. While the
classiﬁed features are again represented by means of their
colored ellipses, the identiﬁed scenario relevant objects are
represented by means of their bounding boxes.
VI. RELATED WORK AND DISCUSSION
A similar color-based segmentation and classiﬁcation ap-
proach was presented in [8]. However, in their context, an
image has to be processed in several steps. After an optional
step of color space projection, each pixel is classiﬁed by one
of up to 32 color classes. In comparison to our classiﬁcation
mechanism, the associated subspaces within the working
color space can only take shape of convex cuboids, whereas
our subspaces may also take shape of concave structures or
either may consist of two and more unconnected subspaces.
Afterwards, each row is processed again in order to
encode adjacent pixels of the same color class into runs.
28
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-188-5
INTENSIVE 2012 : The Fourth International Conference on Resource Intensive Applications and Services

Finally, connected runs belonging to the same color class
are efﬁciently grouped into regions by means of a tree-
based union ﬁnd algorithm, whereas the constructed regions
are represented in terms of their bounding boxes, centroids
and sizes. Although the basic concepts are similar to our
approach, there are signiﬁcant differences with respect to
the number of processing steps, the ﬂexibility in representing
color classes, the method and point of time of grouping runs
as well as the robustness of the actual feature descriptor. In
the latter case, especially the bounding boxes are error prone
and cannot appropriately compensate the negative effects of
stochastic errors such as image noise.
Another quite similar approach was presented in [9].
Whereas the classiﬁcation mechanism is identical to the
previously discussed one, the number of image processing
steps are reduced to two. At ﬁrst, similar to our approach,
the image is compressed by comparing adjacent pixels with
respect to their associated color classes and by encoding
each contiguous block of pixels into a run. After completing
a run, neighboring runs belonging to the same color class are
identiﬁed and marked as connected. However, in comparison
to our approach, they are not yet merged. For increasing the
performance of the algorithm, pixels that are not of interest
are discarded, just as our segmentation algorithm does. In the
second step, a recursive depth-ﬁrst traversal is performed in
order to identify runs that were marked as connected and to
assign them to the same feature, which is in turn represented
by its center of gravity efﬁciently derived from the runs.
A fast component labeling and description algorithm that
also bases on run length was introduced in [10]. Similar to
our approach, the main idea is to scan an image from left to
right row by row starting at the topmost row in order to con-
struct runs of connected pixels. However, only binary thus
already segmented images are considered. Consequently, in
comparison to our approach, the entire image has to be
processed at least twice in order to identify and extract
regions. Furthermore, again, no robust feature representation
in terms of statistical moments is derived from the grouped
runs, but only the area, the center and the bounding box.
VII. CONCLUSION
In this paper, an efﬁcient color-based image segmentation
and feature classiﬁcation approach was presented. The entire
image has to be processed only once in order to identify
regions by means of an efﬁcient classiﬁcation mechanism.
During the segmentation process, regions are compactly
represented in terms of runs in order to drastically reduce
the overall memory consumption. Furthermore, the com-
putational effort for constructing and merging regions is
signiﬁcantly reduced. The classiﬁcation approach addition-
ally provides an efﬁcient mechanism for deciding whether
a pixel is relevant for the segmentation process or not. As
a consequence, redundant visual data is already discarded
on pixel level. A second and ﬁnal step efﬁciently converts
the extracted regions into a convenient and robust feature
representation in terms of statistical moments by directly
incorporating the previously assembled runs. The presented
results show the functional correctness of our combined
approach as well as its efﬁciency in comparison to a se-
quential one. The presented approach constitutes a portable
and ﬂexible solution for any Linux based embedded system
in order to overcome the data intensive and computationally
extremely expensive task of image processing.
ACKNOWLEDGMENT
This work was partially supported by the German Re-
search Foundation (DFG) within the Collaborative Research
Center ”On-The-Fly Computing” (SFB 901).
REFERENCES
[1] S. Herbrechtsmeier, U. Witkowski, and U. R¨uckert, “Bebot:
A modular mobile miniature robot platform supporting hard-
ware reconﬁguration and multi-standard communication,” in
Progress in Robotics.
Springer Berlin Heidelberg, 2009,
vol. 44, pp. 346–356.
[2] R.
Jain,
R.
Kasturi,
and
B.
G.
Schunck,
Machine
vision. McGraw-Hill, 1995, (veriﬁed: 10.01.2012). [Online].
Available: http://www.cse.usf.edu/∼r1k/MachineVisionBook/
MachineVision.htm
[3] M.-K. Hu, “Visual pattern recognition by moment invariants,”
IEEE Transactions on Information Theory, vol. 8, no. 2, pp.
179–187, 1962.
[4] M. F. Zakaria, L. J. Vroomen, P. J. A. Zsombor-Murray, and
J. M. H. M. van Kessel, “Fast algorithm for the computation
of moment invariants,” Pattern Recogn., vol. 20, no. 6, pp.
639–643, 1987.
[5] R. J. Prokop and A. P. Reeves, “A survey of moment-
based techniques for unoccluded object representation and
recognition,” CVGIP: Graph. Models Image Process., vol. 54,
no. 5, pp. 438–460, 1992.
[6] M. R. Teague, “Image analysis via the general theory of
moments,” Journal of the Optical Society of America (1917-
1983), vol. 70, pp. 920–930, 1980.
[7] L.
Vandevenne,
“Lode’s
computer
graphics
tutorial
-
ﬂood
ﬁll,”
(veriﬁed:
10.01.2012).
[Online].
Available:
http://lodev.org/cgtutor/ﬂoodﬁll.html
[8] J. Bruce, T. Balch, and M. Veloso, “Fast and inexpensive color
image segmentation for interactive robots,” in Proceedings of
the 2000 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2000, pp. 2061–2066.
[9] C. Messom, S. Demidenko, K. Subramaniam, and G. Gupta,
“Size/position identiﬁcation in real-time image processing
using run length encoding,” in Proceedings of the 19th IEEE
Instrumentation and Measurement Technology Conference
(IMTC), 2002, pp. 1055–1059.
[10] L. Qiu and Z. Li, “A fast component labeling and description
algorithm for robocup middle-size league,” in 7th World
Congress on Intelligent Control and Automation (WCICA),
2008, pp. 6575–6579.
29
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-188-5
INTENSIVE 2012 : The Fourth International Conference on Resource Intensive Applications and Services

