54
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
FutureBody-Finger 
A Novel Alternative Aid for Visually Impaired Persons 
 
Kiyohide Ito, Yoshiharu Fujimoto, Makoto Okamoto 
Department of Media Architecture 
Future University Hakodate 
Hakodate, Japan 
e-mail: itokiyo@fun.ac.jp, fujimonia@gmail.com, 
maq@fun.ac.jp 
Junichi Akita, Akihiro Masatani 
School of Electrical and Computer Engineering 
Kanazawa University 
Kanazawa, Japan 
e-mail: akita@is.t.kanazawa-u.ac.jp, masatani@ifdl.jp
Tetsuo Ono 
Graduate School of Information Science and Technology 
Hokkaido University 
Sapporo, Japan 
e-mail: tono@complex.ist.hokudai.ac.jp 
 
 
Abstract—We have developed a sensory substitution device 
(SSD), called FutureBody-Finger (FB-Finger) based on a 
“smart” mechanism with an ecological interface. The primary 
aim of FB-Finger is to enable visually impaired persons to 
“recognize” their surrounding environment, specifically in 
terms of distance. FB-Finger comprises a position-sensitive 
device (PSD) sensor unit and a small actuator unit and is used 
to sense the distance as follows: (1) The distance between a 
(visually impaired) user and an object is measured via 
ultrasonic waves or infrared rays radiated from the PSD 
sensor unit; (2) Information on the measured distance is 
transformed in the actuator unit into haptic stimulation 
(“somatosensory stimulation”) and then sent to a servo motor 
incorporated in the actuator unit; and (3) A lever connected to 
the servo motor catches the stimulation and creates angular 
motions to convey the information to the user’s finger. In order 
to afford the device simple use and portability, FB-Finger was 
designed with a shape such that the forefinger skin/joints 
receive somatosensory stimulation. In this paper, we outline 
the concept underlying FB-Finger, describe its underlying 
mechanism, and report on two psychological experiments 
conducted. The results of the experiments show that FB-Finger 
estimates the distance between two objects (i.e., the user and an 
object) more accurately, and the somatosensory interface 
enclosed in the device performs better, than commercially 
available SSDs. On the basis of these findings, we also discuss 
the 
effectiveness, 
possible 
future 
improvements, 
and 
applicability of FB-Finger to electric travel aids and other 
assistive aids. 
Keywords-haptic interface; somatic sensation; ecological 
interface; assistive technology; electric travel aid. 
I. 
 INTRODUCTION 
A. Purpose 
In general, human beings are thought to obtain 
information via visual modality, but it is natural that 
nonvisual modalities also provide people with a significant 
amount of information. Thus, it is important to shed some 
light on the role of nonvisual modalities in exploring the 
surroundings. On the basis of the philosophy and psychology 
associated with human perception and behaviors, we 
hypothesize that people are able to subjectively have an 
“extended body” experience (hereafter referred to as 
“FutureBody”) that endows them with a sense of effectivity 
in their surroundings if a device functions as a part of their 
bodies to enable them to recognize an unfamiliar 
environment by using that device. We have been working on 
the development of a device that proves our hypothesis. In 
particular, our research has been focused on developing a 
new type of sensory substitution device (SSD) for visually 
impaired persons to enable such persons to enhance the 
quality of their lives in terms of utilizing their nonvisual 
modalities. This paper is an extended version of a paper we 
presented at AMBIENT2012 [1]. In this extended version, 
we explain the key concept underlying the FutureBody 
device, give an outline of our developed device, FutureBody-
Finger (hereafter referred to as FB-Finger), describe its 
hardware configuration, and discuss its efficiency. Finally, 
we present the latest improvements to FB-Finger (FB-
Finger2). However, before delving into those areas, we give 
a general overview of previous and current aid devices for 
the visually impaired. 
B. Assistive devices for the visually impaired 
Devices called sensory substitution devices (SSDs) and 
electric travel aids (ETAs) have been developed in both 
academic and industry fields [2-9]. SSDs and ETAs [10, 11] 
are intended to assist the visually impaired with their 
activities, such as exploring their surroundings and 
locomotion. To ensure the safety of these activities, these 
devices have sensor(s) installed that detect a user’s location, 
the direction in which the user is moving, and the distance 
between himself/herself and nearby objects. 

55
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
SSDs 
and 
ETAs 
obtain 
information 
about 
the 
surroundings via two major methods. In the first method, a 
small camera is used to capture images that are then analyzed, 
and the results of the analysis output to an electro-tactile 
display or vibration display. OPTACON [12, 13] adopted 
this method to help the visually impaired read printed letters, 
and the Forehead Retina System [14, 15] utilizes it to assist 
users with search of their surroundings. The second method 
utilizes supersonic wave sensors, which makes it suitable for 
measuring the distance between a visually impaired person 
and an object. Products equipped with supersonic wave 
sensors include Sonicguide [16-18], Miniguide [4], and 
Palmsonar [6]. 
SSDs and ETAs are categorized in terms of their output 
feedback interface as either “auditory” or “haptic.” Auditory 
type devices transform spatial information into audible sound. 
Sonicguide, for example, measures the distance between an 
object and a user with ultrasonic waves, converts the data 
into sound, and conveys the sound to the user. This type of 
device typically emits a low-pitched sound when an object is 
distant from the user and an increasingly higher-pitched 
sound as the object approaches. 
Haptic type devices convert distance information into 
mechanical vibration or electrical-tactile stimulation and 
convey it to the skin (haptic sense). The intensity/frequency 
of the mechanical vibration varies according to distance: it 
increases when a user approaches an object. 
However, in order for users to handle such devices, a 
number of problems need to be solved. Users are required to 
be trained to effectively use their cognitive inference and 
memory to comprehend what a stimulus means; that is, how 
much distance a certain stimulus equates to. To make full use 
of a device equipped with an interface that outputs the data 
in the form of sound pitch or vibration, visually impaired 
adults and children must use the device repeatedly to become 
expert users. Thus, visually impaired persons have to learn to 
associate a specific pitch/vibration with a corresponding 
distance. Such a practice has to be carried out because an 
arbitrary frequency or an arbitrary intensity from a stimulus 
is by itself a meaningless signal. Success with associative 
learning depends on cognitive abilities including inference 
and memory capabilities. Cognitive abilities take on the 
leading role in processes where users interpret a 
pitch/vibration signal correctly, understand the meaning of 
such a stimulation, and associate it with the distance to an 
object. This problem applies to the visually impaired, 
whether congenital or adventitious. If they hope to master 
one of these devices completely, they have to improve their 
cognitive abilities; otherwise, training will take a long time, 
or they will have to give up on actually mastering the device. 
Furthermore, visually impaired children are unlikely to 
develop enough high-level cognitive processing abilities, and 
adventitious visually impaired persons may have more 
difficulty 
discriminating 
pitches of 
sound 
and 
the 
intensity/frequency of vibration than congenitally visually 
impaired persons. To enable visually impaired users to 
receive spatial information more “intuitively” (directly), we 
developed our device, FB-Finger, with a novel haptic 
interface. 
II. 
OUTLINE OF FUTUREBODY DEVICE: “SMART” 
MECHANISM WITH ECOLOGICAL INTERFACE 
A. Key concept underlying FutureBody 
The key concept underlying FB-Finger is adoption of a 
“smart” mechanism. As suggested by Runeson [19], we 
define a “smart” mechanism as a mechanism that directly 
registers complex variables. The operation of a polar 
planimeter can be used to give an indication of how this 
“smart” mechanism operates. A polar planimeter is a tool 
that is used to measure the area of irregular shapes, which 
necessitates 
calculation 
of 
complex 
variables. 
A 
representative polar planimeter is shown in Figure 1. When a 
user moves the tracer arm, the attached measuring wheel 
carefully traces the outline with the index to calculate the 
area (a complex variable) automatically. The length and 
angle measured by the polar planimeter are directly 
proportional to the area. The device is sufficiently simple to 
use such that those who have no knowledge of the 
calculations, e.g., summing up small pieces of a figure, can 
easily determine the area. This “smart” mechanism does not 
require any computational skills, higher cognitive inference 
ability, or excellent memory capability. 
Extending the discussion of smart mechanism to human 
perception and performance, we assume that the human body 
operates in a manner similar to the polar planimeter. The 
polar planimeter uses a tracer arm with an index to register 
the summation of the area; analogously, the human body 
registers information on the surroundings by moving legs, 
arms, fingers, and joints and by stimulating their bones and 
muscles (called somatosensory stimulation). Whether they 
are conscious or unconscious, people are always exposed to 
such somatosensory stimulations from birth. To directly and 
intuitively register spatial information for surroundings, 
humans need to develop somatic sensations that underlie a 
person’s higher cognition and behavioral regulatory systems. 
Figure 1.  Polar planimeter as a tool for measuring the dimensions of 
irregular shapes. 

56
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Following this assumption, somatosensory stimulation plays 
an important role in devices that convey information to users 
directly or intuitively. Thus, we hypothesize that using a 
somatosensory/haptic interface would enable us to solve the 
problems outlined in the previous section. 
Further, a smart mechanism is required to be equipped 
with an ecological interface. Here, the term “ecological” 
originates from “Ecological Psychology” [20] — a subfield 
of psychology aimed at revealing the human-environment 
interrelationship from the viewpoint of human’s perception 
and behavior in the environment. An ecological interface is 
designed to “reflect” the surroundings, in which information 
is directly and perceptually available to the persons who use 
it [21]. Design of ecological interfaces is mainly focused on 
interfaces for large and complex systems, such as power 
plants and medical equipment in order to avoid human errors. 
In this paper, we extend the discussion of ecological 
interface design to SSDs (or ETAs). The ecological interface 
of the device functions as a part of the user’s body so that 
they feel as if their bodies are extended by using it. 
Furthermore, they are directly exploring and connected to the 
surroundings so they are able to take effective actions with 
the interface. An example of a tool with an ecological 
interface is a pen. When we write letters with a pen, we feel 
the texture of the paper surface in which the pen is in contact. 
There are no touch receptors on top of a pen, but the skin of 
the hand holding the pen has a sense of touch. This sense of 
touch extends from the skin, through the hand, to the top of 
the pen. The shape of the pen is such that it is easy to hold 
with the fingers to help its users easily feel the smooth 
texture of the paper surface. This can be termed an 
“incarnation,” as argued by Merleau-Ponty [22]. On the basis 
of the above arguments, FutureBody should satisfy two 
requirements. First, it must operate as a smart mechanism 
with which users can directly or intuitively register spatial 
information without higher cognitive abilities. Second, it 
should be equipped with an ecological interface by which the 
device can function as a part of the bodies of users, and 
consequently allow users to have a sense of extending their 
bodies. 
B. Preliminary version of FutureBody device (CyARM) and 
its mechanism 
Our first step in the development of FutureBody was 
CyARM [23, 24]. CyARM is characterized by its “smart” 
user interface. Users do not need calculations, inference, or 
higher-level cognitive processing to determine the distance 
between them and an object, whether or not it is moving. 
Figure 2 depicts the structural diagram of CyARM. The 
strength of the device’s wire tension enables users to specify 
distance as well as direction. Connected to the user by a wire, 
CyARM measures the distance between the user and an 
object. CyARM emits ultrasonic waves, spotting an object 
and measuring the distance between the user and the object. 
At the same time, it also controls the tension of the wire 
connecting the device to the user. The wire’s tensile strength 
is directly proportional to this distance. When an object is a 
short distance away, CyARM pulls the wire tightly, and the 
user understands that the object can be reached by bending 
the arm. When the object is far from the user, CyARM 
slackens the wire, indicating to the user that the object is not 
within reach. The user can thus explore his/her surroundings 
with the device. 
Ultrasonic sensors measure the distance between the user 
and an object, and CyARM’s motor slackens or tightens the 
wire in accordance with the measured distance. The wire is 
rewound to the initial default position, and the rewinding 
tension is regulated in accordance with the measured 
distance. High tension signifies a short distance, while low 
tension signifies a longer distance. CyARM uses a 
somatosensory stimulation user interface such that users can 
obtain distance from themselves to an object via bending or 
extending their arms. 
The basic mechanism underlying CyARM is as follows: 
The motor is Maxon GP16 (4.5W) with a 29:1 gear head and 
magnetic rotary encoder; the motor driver is iXs iMDs03-
CL; the MPU is Renesas H8/3664, and the ultrasonic 
frequency used is 38 kHz. 
The results of our previous studies indicated that 
CyARM is feasible for visually impaired persons. 
Psychological experiments conducted in which CyARM was 
used to estimate the distance to an object showed high 
accuracy and correlation to the actual distance. In addition, 
another psychological experiment also found that CyARM is 
effective in perceiving the shapes of objects [25, 26]. 
However, CyARM is too large to carry for daily use and its 
mechanism inhibits the user’s arm and trunk movement. In 
addition, its ultrasonic sensor has only low resolution for 
measurement of distance. Thus, CyARM is impractical for 
daily use. In order to overcome those usability and 
portability issues, we developed a novel “FutureBody” 
device called FutureBody-Finger (FB-Finger). 
III. 
FUTUREBODY-FINGER: BASIC MECHANISM AND 
HARDWARE CONFIGURATION 
FB-Finger was developed to enable users to recognize 
the direction of, and distance from, an object. Using it, 
people do not require higher cognitive abilities such as 
mathematical calculations, inference, and excellent memory 
to recognize the distance to an object. The device has been 
verified to solve some of the usability problems discussed in 
Figure 2.  Structural diagram of the prototype  CyARM, developed by 
us. 

57
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
the previous sections. 
The hardware architecture of the prototype FB-Finger is 
shown in Figure 3. The developed FB-Finger consists of 
three functional blocks: a controller, a sensor, and actuator 
units, all of which are connected to a common 
communication channel. Each unit has a microcontroller 
(MCU, Cypress CY8C21123).  
The sensor unit acquires information about the 
environment via an adequate sensor device, and converts it to 
a digital value. We developed four types of sensor units with 
different sensor devices. The first and the second equip a 
position-sensitive device (PSD)-type distance sensor that 
radiates infrared rays toward an object; it detects the 
reflected position of the received rays using a PSD that 
implements a trigonometric distance measurement technique. 
We employed two different PSD devices: GP2Y0A21YK 
and GP2Y0A02YK by Sharp Inc. They have different 
distance measurement ranges: 100 mm – 800 mm for 
GP2Y0A21YK, and 200 mm – 1500 mm for GP2Y0A02YK. 
They output voltage signals corresponding to the measured 
distances. The supply voltage used is 5 V, and their physical 
dimensions are 30 mm (W), 13 mm (H), 14 mm (D) and 30 
mm (W), 13 mm (H), 22 mm (D), respectively. The 
microcontroller is installed on the sensor unit, and calculates 
the distance from FB-Finger to an object; with an adequate 
conversion equation for each PSD sensor. 
The third sensor unit equips the ultrasonic distance 
sensor of PING by Parallax Inc. It measures the distance to 
the target object in the range 20 mm – 30 mm, and has 
physical dimensions 22 mm (W), 46 mm (H), and 16 mm 
(D). It outputs a pulse with a modulated signal according to 
the measured distance. The microcontroller installed on the 
sensor unit converts the distance from FB-Finger to the 
object. 
The fourth sensor unit equips the light sensor to measure 
the intensity of the incoming light, with lens for focusing. It 
measures luminance intensity at the narrow point on the 
surface of the target object. 
The microcontroller installed in each sensor unit converts 
the sensor signal to the “distance” information in the same 
signal format. This enables the system to easily exchange the 
sensor unit with the same controller unit and actuator unit. In 
other words, the user can exchange the sensor unit for 
suitable applications with the one for FB-Finger body with 
the controller unit and the actuator unit. We here emphasize 
that the user can select a type of sensor unit according to the 
purpose of use. This is an important advantage of FB-Finger. 
We employ a 2.5 mm stereo plug and jack for physical 
connection between the sensor unit and the FB-Finger body, 
including the controller unit and the actuator unit. The stereo 
plug and jack provide the power, the ground, and the signal 
terminals. 
The user can also apply the adequate distance sensor unit 
for the purpose based on its characteristics. For example, the 
ultrasonic distance sensor can steadily measure the distance 
to the object regardless of the material, while the infrared 
reflection used in the PSD sensor tends to be weak for black 
objects because of light absorption. On the other hand, the 
PSD sensor can measure the distance beyond a transparent 
wall, which is impossible for an ultrasonic sensor. In terms 
of the distance range available, the ultrasonic distance sensor 
can be applied to long distances, such as 3 m compared with 
the PSD sensor. In terms of physical size of the distance 
sensor devices, the ultrasonic distance sensor tends to be 
larger than the PSD device for the physical wavelength of the 
ultrasonic waves. 
The actuator unit has a servo motor equipped with a 55-
mm-long lever to form a one degree-of-freedom (1-DOF) 
link. The microcontroller on the actuator unit controls the 
servo motor according to the angular information received. 
The controller unit periodically requests distance 
information from the sensor unit, converts the measured 
distance to angular information, and transmits it to the 
actuator unit; this chain of operations forms the sensor-
actuator system. The angle of the link increases when the 
distance between FB-Finger and the object decreases (i.e., 
when the object is approaching), whereas it decreases when 
the distance increases.  
Figure 4 illustrates the method by which FB-Finger is 
operated. A user holds FB-Finger and places his/her 
forefinger on the link. The finger bends or extends depending 
on the link’s angular motion. The angle changes from zero to 
70 degrees in correspondence with the metric distances 
between the user and an object. The extent that the user 
bends his/her forefinger is directly associated with the link 
movements, such that a finger motion allows a user to 
“directly,” and “intuitively” perceive the distance to an 
object. In this sense, FB-Finger has a somatosensory (haptic) 
interface that is ecologically designed. 
The hardware specifications of the prototype FB-Finger 
are as follows: weight = 60 g; height = 75 mm; width = 45 
mm; and depth = 35 mm. The body and the lever are 
composed of aluminum. The measurable distance ranges 
from 300 mm to 1400 mm for the PSD-short range sensor, 
and 1000 mm to 2800 mm for the PSD-long range sensor as 
the link angle changes from 70 to zero degrees in both 
sensors. The distance-angle coefficients are 7 deg/110 mm 
for the PSD-short range sensor and 7 deg/180 mm for the 
PSD-long range sensor. The output of the PSD is converted 
by an analog-to-digital converter, and then transformed to 
Figure 3.  Block diagram of  FB-Finger, developed by us, with infrared 
ray sensor. 

58
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 4.  Illustrations showing how to operate FB-Finger with PSD-
short range sensor (upper panel) and with PSD-long range sensor (lower 
panel). 
the angle of the lever, which is controlled by the width of the 
control pulse. The theoretical minimum resolution for the 
distance measurement is approximately 1 mm. 
IV. 
PSYCHOLOGICAL EXPERIMENT 1: ACCURACY OF 
ESTIMATED DISTANCE 
A.  Purpose 
To demonstrate that FB-Finger enables users to perceive 
the distance between them and an object more accurately 
than commercially available products, we performed a 
psychological experiment as follows. 
B. Method 
1) Participants: 16 persons, visually impaired and 
sighted, participated in the experiment. Eight visually 
impaired adults, four congenitally and four adventitiously, 
participated in the visually impaired group. Their ages were 
between 28 and 57 years (mean = 43.0 years). Eight sighted 
adults with ages in the range 20 to 22 years (mean = 20.8 
years), participated in the sighted group. 
2) Distance Range: Two separate FB-Finger devices 
were used in order to test for a short range stimuli set 
(“Short Range”) and a long range stimuli set (“Long 
Range”). The device designated to test for Short Range was 
equipped with a short distance sensor whereas the other was 
equipped with a long distance sensor. 
3) Object for Stimuli: A piece of cardboard adhered to a 
whiteboard (1.6 m × 1.0 m × 0.02 m) was used as the 
standard stimulus and the test stimuli. We used a standard 
stimulus and four test stimuli in the Short Range and five 
test stimuli in the Long Range scenarios. 
4) Stimuli presentation: In the Short Range stimuli set, 
the standard stimulus was presented at a distance of 0.4 m 
from a device affixed to a table. One test stimulus was 
presented at each of four positions, specifically, 0.4, 0.6, 0.8, 
and 1.0 m. In the Long Range stimuli set, the standard 
stimulus was presented at a distance of 1.0 m in the same 
manner as the Short Range standard stimuli set. One test 
stimulus was presented at each of five positions, specifically, 
1.0, 1.4, 1.8, 2.2, and 2.6 m. 
5) Device Conditions: Three types of SSDs (FB-Finger, 
Vibratory device, and Sonar device) were used as a within-
subject factor. Participants were asked to estimate the 
distance to the stimuli using each SSD in turn. The 
Vibratory and Sonar devices used are commercially 
available ETAs. The Vibratory device (70 mm × 40 mm × 
25 mm) was equipped with a haptic interface that 
transformed measured distances into vibration signals. The 
Sonar device (60 mm × 35 mm × 15 mm) transformed 
measured distances into audible sounds (i.e., sounds with a 
specific pitch). Both devices use ultrasonic waves to 
determine the distance to an object. 
6) Procedure: Figure 5 shows the experimental setup. In 
each trial, participants were asked to use an SSD to detect 
the distance to a stimulus that was presented for 3 s. Initially, 
the standard stimulus was presented, after which one of the 
test stimuli was randomly presented at a certain distance in 
each distance range stimuli set. 
The magnitude estimation method was used to estimate 
the distance to the presented stimulus. Using this method, 
each participant was asked to report the magnitude of a 
stimulus that corresponded to some proportion of the 
standard. The participant then assigned numbers reflecting 
the adjudged magnitude of his/her subjective experiences to 
each stimulus. In the magnitude estimation practice, each 
stimulus was assigned a number that reflected its distance as 
a proportion of the standard. The standard stimulus was set 
as “100.” Thus, if a test stimulus was subjectively twice as 
far as the standard, a participant was expected to assign it a 
magnitude of “200.” Under the three device conditions, each 
participant performed five trials for each of the four test 
stimuli in Short Range, and five trials for each of the five test 
stimuli in Long Range. 
Figure 5.  Setup used in Experiment 1: participants sat in front of a table 
on which one of three devices remained fixed. Experimenters randomly 
moved the object to change the distances between the object and the 
device. 

59
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
C. Result and discussion 
The product-moment correlation coefficient (r) between 
the presented distance (the presented stimulus) and the 
estimated distance (the distance participants estimated) by 
each device was computed. It was computed for each group, 
each device, and for both the Short and Long Range 
scenarios. We categorized both congenitally impaired and 
adventitiously impaired adults as “visually impaired group,” 
because there was no significant difference between them, 
and compared the group with “sighted group.” In the Short 
Range scenario, the product-moment correlation coefficients 
when the visually impaired group used FB-Finger, Vibratory 
device, and Sonar device were 0.918, 0.742, and 0.763, 
respectively. When the sighted group used those devices, the 
correlation coefficients were 0.882, 0.730, and 0.740, 
respectively. In the Long Range scenario, the correlation 
coefficients when the visually impaired group used FB-
Finger, Vibratory device, and Sonar device were 0.908, 
0.663, and 0.461, respectively. When the sighted group used 
those devices, the correlation coefficients were 0.928, 0.777, 
and 0.422, respectively. Without regard to each device, each 
group, or each range, the estimated distances were correlated 
with the presented distances. Remarkably, FB-Finger 
exhibited the highest correlation between the presented and 
estimated distances. 
Figures 6 and 7 show the regression lines for the overall 
data of each device, calculated using the least squares 
method. These figures indicate that the farther away a 
stimulus was presented, the farther the distance was 
estimated. From the abovementioned correlation coefficients 
and the regression lines, it was found that FB-Finger 
provided participants with the most accurate estimation of 
the distance to the presented stimuli. 
Figure 6.  Regression lines for the overall data in the Short Range setup, calculated using the least squares method. 
Figure 7.  Regression lines for the overall data in the Long Range setup, calculated using the least squares method. 

60
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Determination coefficients (square of r) were also 
computed for each participant. Figures 8 and 9 show the 
mean determination coefficients of each group in the three 
device conditions. In the Short Range scenario, the mean 
determination coefficients of FB-Finger, Vibratory device, 
and Sonar device were 0.872, 0.622, and 0.660, respectively, 
for the visually impaired group. They were 0.813, 0.647, and 
0.744 for the sighted group, respectively. In the Long Range 
scenario, the mean determination coefficients of each device 
were 0.878, 0.608, and 0.294 for the visually impaired group, 
and 0.884, 0.746, and 0.255 for the sighted group, 
respectively. 
A two-way analysis of variance was performed on both 
the Short and Long Range scenarios by setting both visually 
impaired and sighted groups as a between-subject factor, and 
three device conditions (FB-Finger, Vibratory device, Sonar 
device) as a within-subject factor. The results indicated the 
same significant main effects of device condition (ps < 0.01) 
for both the Short and Long Range scenarios. Similarly, 
multiple comparison tests between the three devices found 
that the distance estimated using FB-Finger was positive 
proportional to the presented distance with the highest 
linearity compared to the other two devices. By contrast, 
there was no significant difference between the visually 
impaired and sighted groups. These results demonstrate that 
FB-Finger endows users with two advantages that cause it to 
excel above other devices: (1) better estimation of distance, 
and (2) capability of assisting users with accurate detection 
of distance. 
Evaluation of the output interfaces of Sonar device (pitch 
of sound), Vibratory device (mechanical vibration), and FB-
Finger (lever motion) showed that they all required users to 
use their bodies/senses to hear sound, feel vibration with skin, 
and feel finger movement. The findings from Experiment 1 
suggest that finger movements, or finger joint motions, most 
effectively transfer distance information to users. 
V. 
PSYCHOLOGICAL EXPERIMENT 2: FINGER FIXATION 
ON ACCURACY OF DISTANCE ESTIMATION 
A. Purpose 
In Experiment 1, the participants placed a finger on the 
lever of FB-Finger but the finger was not fixed to the lever. 
This may lead them to have a wrong perception of the 
angular motions of the lever. Hypothesizing that the 
participants may be able to estimate distances entirely based 
on the information conveyed from FB-Finger, we conducted 
another experiment (Experiment 2) to demonstrate the effect 
of fixing finger joints on the lever of FB-Finger to estimate 
the distance. 
B. Method 
1) Participants: 16 sighted adults participated in the 
experiment. Their ages ranged from 21 to 23 years. Of the 
16, eight participants were asked to wear blindfolds and 
were randomly assigned to each experimental condition 
(given below). 
2) Object for Stimuli: We used the same object for 
stimuli as Experiment 1. 
3) Experimental 
condition: 
We 
adopted 
two 
experimental conditions: Fixing condition and Non-fixing 
condition. In the Fixing condition, shown in Figure 10, each 
participant’s forefinger was affixed to the lever of FB-
Finger using a Velcro touch fastener. In the Non-fixing 
condition set, the forefinger was not affixed to the lever (the 
same condition as in Experiment 1). 
4) Procedure: FB-Finger was used to estimate distances. 
The standard stimulus was set at a distance of 0.4 m, and 
one test stimulus was respectively positioned at 0.4, 0.6, 0.8, 
1.0 and 1.2 m. Each participant performed six trials for each 
of the five stimuli. Other procedures (experimental setup 
and magnitude estimation method) were the same as in 
Experiment 1. 
C. Result and discussion 
Product-moment 
correlation 
coefficients 
(r) 
were 
computed for each condition. The r values of the Fixing 
condition and the Non-fixing condition were 0.965 and 0.938, 
Figure 9.  Mean determination coefficients in Long Range setup in the 
three device conditions for each of the visually impaired and the sighted 
groups. Standard deviations are shown as error bars. 
Figure 8.  Mean determination coefficients in Short Range setup in the 
three device conditions for each of the visually impaired and the sighted 
groups. Standard deviations are shown as error bars. 

61
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
respectively. This suggests that the distances estimated were 
more correlated with the presented distances in the Fixing 
condition than in the Non-fixing condition. Statistical 
analysis found that there was a significant difference 
between the Fixing condition and the Non-fixing condition (z 
= 2.515, p < 0.01). From this result, it is clear that an FB-
Finger user is able to estimate the distance between 
himself/herself and an object more accurately when his/her 
finger is properly affixed to the lever. 
In the Fixing condition, the forefinger was affixed 
sufficiently tightly that the finger was likely to follow a link-
angular motion completely and its joint(s) and skin become 
more sensitive to somatosensory stimulation with their haptic 
sense. A finger’s haptic sense to somatic stimulation is 
considered to be effective to obtain distance information. It is 
remarkable that FB-Finger enabled blindfolded, sighted 
participants to correctly determine the distance to an object, 
even though the individuals had little experience with haptic 
exploration of the surroundings using FB-Finger. Taking into 
account the fact that the visually impaired have higher 
somatic sensitivity than the sighted, it is conceivable that the 
results from Experiment 2 will lead to the development of 
some promising applications of FB-Finger for the visually 
impaired. 
VI. 
“TWO DEGREES-OF-FREEDOM” PROTOTYPE OF 
FB-FINGER2 
Toward upgrading of the functionality of FB-Finger to 
perceive shapes, we developed an FB-Finger designed with 
2-DOF (Figure 11, hereafter “FB-Finger2”). A user holds 
this device with forefinger on the lever, as can be seen in 
Figure 12(b). The lever consists of two components with two 
servo motors attached for each. One servo motor controls the 
distal interpharangeal (DIP) joint of the forefinger, and the 
other the proximal interpharangeal (PIP) joint of the finger. 
The distances from FB-Finger2 to three points (p1, p2, and 
p3) on the surface of the object, as can be seen in Figure 
12(a), were measured using a depth camera (ASUS Xtion 
Pro Live, hereafter referred to as “Xtion”). This was 
connected to a Microsoft Kinect to show the depth image of 
the measured object. On the basis of the measured distances 
between Xtion and p1, p2, and p3 (referred to as d1, d2, and d3), 
the angles of the finger’s DIP joint (θ1) and PIP joint (θ2) 
were calculated using the following equations: 
 
θ1 = arctan 𝑑1 − 𝑑2
𝑣1
 
 
θ2 = arctan 𝑑3 − 𝑑2
𝑣2
 
 
Here,  
v1: vertical distance between p1 and p2,  
v2: vertical distance between p2 and p3. 
 
The distance was set as 20 mm with adequate selection of 
the acquired depth image by Xtion. To cover the user’s 
Figure 11.  Prototype of FB-Finger2, designed with 2-degrees-of 
freedom. 
Figure 10.  A finger affixed to the lever of FB-Finger. 
Figure 12.  Measuring the distance to an object using the depth camera of 
FB-Finger2: (a) three points on the surface of the object were measured, 
(b) a user holds FB-Finger2 with forefinger on the lever. 

62
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
finger motion, θ1 ranged between -10 deg and 90 deg, and θ2 
ranged between -40 deg and 90 deg. 
Figure 13 shows the system configuration of the FB-
Finger2 prototype developed. The distance to the targeted 
object is measured using a PC-controlled Xtion. The PC also 
controls two servo motors that are attached to levers 1 and 2. 
The levers correspond with the movements of the DIP and 
PIP joints of the user’s forefinger. The servo motors are 
controlled by microcontroller (Cypress’s CY8C24123) from 
the PC control command. To accommodate various finger 
sizes, the lengths of levers 1 and 2 are set at 25 mm and 30 
mm, respectively. The weight of the developed FB-Finger2 
is 100 g (without Xtion). The distance range measurable by 
Xtion is between 0.8 m and 3.5 m, and its depth resolution is 
10 mm. 
To verify the performability of FB-Finger2, we 
conducted a preliminary experiment with 12 sighted adult 
participants. The participants were asked to wear blindfolds 
and use either FB-Finger or FB-Finger2 to identify the shape 
of objects (triangle, rectangular, trapezoid, semicircle). All 
participants identified triangular and rectangular objects 
more accurately when they used FB-Finger2 than when they 
used FB-Finger, but no such difference was found when they 
tried to identify trapezoidal and semicircular objects. The 
results of this experiment partially verify the performability 
of the 2-DOF in FB-Finger2. However, further studies on the 
method for measuring the depths of an object and for 
outputting information are necessary. 
VII. CONCLUSION 
The development of FB-Finger was inspired by the 
“smart” mechanism and the ecological interface design. As a 
novel SSD, FB-Finger is primarily aimed at helping the 
visually impaired to “feel” and “recognize” their surrounding 
environment. The current device comprises a sensor unit 
(one of two different types of sensor units) and a small 
actuator. One type of sensor unit radiates ultrasonic waves or 
infrared rays to measure the distance between the user and an 
object. The other type of light sensor unit measures luminous 
intensity. An actuator transmits the distance or luminance 
level into the haptic sense of a finger as somatosensory 
stimulation via a link-angular motion. This represents a form 
of ecological interface, in that information on distance and 
brightness is directly converted to another sensory modality. 
This paper focused on forefinger skin and joints to determine 
how somatosensory stimulation helps to improve the 
performance of FB-Finger. 
Two experiments were conducted to verify the feasibility 
of FB-Finger. The results of Experiment 1 demonstrate that 
FB-Finger is more accurate than commercial products, and 
has the highest linearity in estimating distances. The results 
of Experiment 2 clarified that users are able to estimate 
distance more accurately when the joints of their forefinger 
are fixed at a 1-DOF link with FB-Finger. 
The findings obtained from these two experiments 
suggest 
the 
following: 
First, 
finger 
joints 
motion 
(“somatosensory feedback”) provides users with richer 
spatial information than tactile or auditory feedback. Second, 
FB-Finger can serve as a useful travel aid. It can help the 
visually impaired avoid obstacles and find landmarks such as 
poles, bus stops, and trash cans, while walking, particularly 
when used along with a cane or a guide dog. In order to 
verify its usefulness, we will ask visually impaired 
participants to use FB-Finger along with a cane and walk on 
roads, avoiding obstacles or detecting landmarks in a more 
real environment like city streets, where the obstacles and/or 
the users are moving. Third, FB-Finger can help to enhance 
quality of life for visually impaired persons. 
We conducted two further case studies in accordance 
with the progress of our research. From the first case, we 
found that FB-Finger, when equipped with an infrared ray 
sensor, enables users to “feel” the outline of objects in 
display windows. In a test at a museum, FB-Finger users 
managed to feel the contour of exhibits contained in glass 
cases without touching and seeing them, as illustrated in 
Figure 14. 
In the second case study, in order to obtain suggestions 
regarding the potential for a variety of applications, we 
conducted a small workshop in which visually impaired 
participants were asked to use FB-Finger with a light sensor 
Figure 14.  “Feeling” an exhibit in a display window using FB-Finger: A 
scene from a demonstration in a museum. 
Figure 13.  Block diagram of FB-Finger2, which we developed, with 
depth camera. 

63
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
unit (see Section III) to “feel” several events, such as a 
flame of fire, pendulum motion, and a rolling ball, without 
touching. The results were better than what we expected; 
visually impaired children as well as visually impaired adults 
were able to “feel” and recognize a flame, as illustrated in 
Figure 15, the direction of pendular motion, and an 
approaching ball. Thereby, we confirmed that FB-Finger can 
be applied to devices that allow visually impaired users to 
obtain knowledge of events that are impossible to touch, 
even though we do not yet have quantitative evidence. In 
other words, these findings indicate that FB-Finger can 
facilitate recognition of the surroundings without visual 
modality. 
FB-Finger currently still needs improvements. First, its 
distance measurement capabilities need to be consistent 
when nearby objects emit infrared rays.  
Second, though the sensors have a theoretically high 
resolution in daily use, the actual resolution of our device is 
lowered for a few reasons. These reasons include a servo 
motor’s control noise (small vibrations even in stable 
condition), 
analog-to-digital 
converter’s 
noise, 
and 
quantization error. In real situations, we expect that a user 
will move around his/her hand holding the FB-Finger to 
perceive the distance of surrounding objects and their 
direction, so that such activities enhance the perceptual 
resolution of our device such as “active touch” [27]. We will 
continue to improve FB-Finger to eliminate factors that 
disrupt the device’s resolution. We will finally improve FB-
Finger so that it can convey information for various textures 
of objects.  
Third, the sensor needs to be replaceable between 
infrared rays and ultrasonic waves, so that users can use an 
appropriate sensor depending on the scenario. In order to 
allow the use of more than two sensors, there are some 
alternative methods. One is to let the user manually select an 
individual sensor, and the other is to select or integrate a 
sensor automatically, which is called “sensor fusion.” We 
hypothesize that the manual switching has an advantage in 
normal use, because humans are smarter and more flexible as 
compared to artificial intelligence, and can adjust to complex 
and various environments. Consequently, users will be able 
to handle FB-Finger effectively. In one of our future works, 
we will develop two versions of FB-finger: one with manual 
sensor switching, and the other with “sensor fusion,” in order 
to compare their usability. 
Fourth, the distance sensors need to be applicable to both 
Short and Long Range scenarios to allow for measurement of 
more expansive locations. 
Fifth, we will take the output interface into account. It 
should be possible to have another method of conveying 
distance information via somatosensation, besides finger 
joints motion. A laparoscopic surgery simulator with the 
haptic device, employing force sense feedback, has been 
commercially available recently [28]. This product ensures 
that force sense can be used for the feedback interface. In the 
further improvement of our device, we will consider multiple 
somatosensory feedback interfaces by adding force sense to 
lever angular motion. 
FB-Finger2 was developed to capitalize on the 
movements of finger joints for more accurate distance 
estimation, but it also needs to be improved. The prototype 
system is too large to be portable. Consequently, it is 
necessary to downsize it to a size similar to that of FB-Finger. 
Moreover, in order to obtain information on surrounding 
objects or their textures, Microsoft Kinect should be used 
effectively. Zöllner et al. recently developed Mobile 
Navigational Aid for visually impaired persons based on 
Microsoft Kinect [29]. This Aid system uses two cameras of 
a Kinect separately. An RGB camera is used in “micro 
navigation,” and a depth camera is used in “macro 
navigation.” In a similar manner, we will improve FB-
Finger2 so that it can be equipped with sensors and 
information processing system available in Kinect.  
To ensure availability, applicability, and ease of use, 
SSDs and ETAs should be capable of being held in one hand. 
They should be capable of assisting users with exploration of 
their surroundings, i.e., detection of distance and direction to 
nearby objects, accurate perception of the shape of objects, 
and recognition of events or objects that cannot be physically 
touched. Such spatial information will help users to avoid 
collisions with obstacles and to approach objects. FB-Finger 
encourages visually impaired persons to acquire knowledge 
about events that they have not experienced before. 
Additionally, FB-Finger devices should be manageable by 
anyone, regardless of age or cognitive ability, and they 
should require little knowledge or skills in understanding the 
signals emitted by the devices. If a user can easily replace the 
sensor with a different one, FB-Finger can respond to 
demands under various situations. On realizing this 
improvement, we guarantee that users will receive full 
benefit from our developed device. 
Figure 15.  “Feeling” a flame using FB-Finger: A scene from a 
demonstration in the exhibition in 2013. 

64
International Journal on Advances in Life Sciences, vol 7 no 1 & 2, year 2015, http://www.iariajournals.org/life_sciences/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
In this study, we verified that FB-Finger can fulfill the 
requirements of visually impaired people. To enhance the 
usability of FB-Finger or FB-Finger2, we will continue 
experimental studies, analyze the results, make necessary 
improvements, and enhance the performance in traveling and 
exploring environments. The device presented in this paper 
is functionally promising and we expect to make the device 
function as a part of the body for both visually impaired and 
sighted people to develop their potential capabilities. If this 
idea of “Extended Body” is realized, our device can assist 
users in improving the quality of life. 
ACKNOWLEDGMENT 
This research is supported by a Grant-in-Aid for 
Scientific Research (KAKENHI Grant Number 25282004) 
from the Ministry of Education, Culture, Sports and 
Technology, Japan. The authors wish to extend our special 
thanks to those who volunteered to participate in our 
psychological experiments. We are also grateful to Miho 
Takahashi and Yasuko Nagasaki for correcting words and 
arranging a manuscript, Yuuka Niiyama for helping data 
analyses. 
REFERENCES 
[1] K. Ito, Y. Fujimoto, J. Akita, R. Otsuki, A. Masatani, T. 
Komatsu, M. Okamoto, and T. Ono, “Development of the 
Future Body-Finger — A novel travel aid for the blind,” The 
Second International Conference on Ambient Computing, 
Applications, Services and Technologies (AMBIENT2012) 
IARIA, pp. 60-63, ISSN: 2326-9324, ISBN: 978-1-61208-
235-6. Sep. 2012. 
[2] EyePlusPlus 
US, 
Inc. 
AuxDeco. 
Available 
from: 
http://www.eyeplus2.com/?lang=en (Accessed 2015.05.29). 
[3] Bay Advanced Technologies Limited of New Zealand. The 
BAT K-Sonar. Available from: http://www.ksonar.com/ 
(Accessed 2015.05.29). 
[4] GDP Research. The Miniguide mobility aid. Available from: 
http://www.gdp-research.com.au/minig_1.htm 
(Accessed 
2015.05.29). 
[5] Affection. Miruburu. Available from: http://www.affection-
j.com/miruburu.html (Accessed 2015.05.29). 
[6] Takes. 
Palmsonar.
 
Available 
from: 
http://www.palmsonar.com/ (Accessed 2015.05.29). 
[7] Akita Precisions Industry Co., Ltd. Smart cane. Available 
from: 
http://www.api-kk.com/denshi-hakujo/ 
(Accessed 
2015.05.29). 
[8] M. Okayasu, “The development of a visual system for the 
detection of obstructions for visually impaired people,” 
Journal of Mechanical Science and Technology, vol.23, pp. 
2776-2779, 2009. 
[9] Sound Foresight Technology Ltd. UltraCane. Available from: 
http://www.ultracane.com/ (Accessed 2015.05.29). 
[10] L. A. Johnson and C. M. Higgins, “A navigation aid for the 
blind using tactile-visual sensory substitution,” Engineering in 
Medicine and Biology Society, 2006 (EMBS’06), 28th 
Annual International Conference of the IEEE, vol. 6, pp. 
6289-6292, ISSN:1557-170X, ISBN:1-4244-0032-5. 
[11] B. B. Blasch, W. R. Wiener, and R. L. Welsh, “Foundations 
of Orientation and Mobility,” 2nd ed. New York: AFB Press, 
1997. 
[12] J. G. Linvill and J. C. Bliss, “A Direct Translation Reading 
Aid for the Blind,” Proceedings of the IEEE, vol. 54, pp. 40-
51, 1966. 
[13] J. C. Bliss, “A Relatively High-Resolution Reading Aid for 
the Blind,” IEEE Transactions on Man-Machine Systems, vol. 
10, pp. 1-9, 1969. 
[14] H. Kajimoto, M. Suzuki, Y. Kanno, “HamsaTouch: Tactile 
Vision Substitution with Smartphone and Electro-Tactile 
Display,” CHI ’14 Extended Abstracts on Human Factors in 
Computing 
(CHI 
EA’14), 
pp. 
1273-1278. 
doi:10.1145/2559206.2581164, ISBN: 978-1-4503-2474-8. 
[15] H. Kajimoto, Y. Kanno, and S. Tachi, “Forehead electro-
tactile display for vision substitution,” Proc. EuroHaptics 
2006. 
[16] C. Carter and K. A. Ferrell, “The Implementation of 
Sonicguide with Visually Impaired Infants and School 
Children,” Sensory Aids Corporation, D. W. Campbell, Ed. 
Ill: Bensenville, 1980. 
[17] L. Kay, “A sonar aid to enhance spatial perception of the 
blind: engineering design and evaluation,” in Radio and 
Electronic Engineer, vol. 44, pp. 605-627, 1974. 
[18] L. Kay, “Acoustic coupling to the ears in binaural sensory 
aids,” Journal of Visual Impairment & Blindness, pp. 12-16, 
1984. 
[19] S. Runeson, “On the possibility of ‘smart’ perceptual 
mechanisms,” in Scandinavian Journal of Psychology, vol. 18, 
pp. 172-179, 1977. 
[20] J. J. Gibson, “The Ecological Approach to Visual Perception,” 
Lawrence Erlbaum Associates: Hillsdale, 1986. 
[21] C. M. Burns and J. R. Hajdukiewicz, “Ecological interface 
design,” CRC Press: Boca Raton, 2004. 
[22] M. 
Merleau-Ponty, 
“Phenomenology 
of 
Perception,” 
Routledge: London, 2002. 
[23] K. Ito, M. Okamoto, J. Akita, T. Ono, I. Gyobu, T. Takagi, T. 
Hoshi, and Y. Mishima, “CyARM: an Alternative Aid Device 
for Blind persons.” Extended Abstracts Proceedings of the 
2005 Conference on Human Factors in Computing Systems 
(CHI 
2005), 
pp. 
1483-1486. 
2005. 
doi: 
10.1145/1056808.1056947. 
[24] J. Akita, T. Komatsu, K. Ito, T. Ono, and M. Okamoto, 
“CyARM: Haptic Sensing Device for Spatial Localization on 
Basis of Exploration by Arms,” Advances in Human-
Computer Interaction, vol. 2009, Article ID 901707, 2009. 
doi:10.1155/2009/901707. 
[25] R. Mizuno, K. Ito, J. Akita, T. Ono, T. Komatsu, and M. 
Okamoto, “Shape Perception using CyARM - Active Sensing 
Device,” Proceeding of the 6th International Conference of 
Cognitive Science (ICCS2008), pp. 182-185, 2008. 
[26] R. Mizuno, K. Ito, T. Ono, J. Akita, T. Komatsu, and M. 
Okamoto, “User’s Motion for Shape Perception Using 
CyARM,” In D. D. Schmorrow et al. (eds.) Augmented 
Cognition, HCII 2009. LNAI, vol. 5638, pp. 185-191, 
Springer-Verlag, Berlin Heidelberg, 2009. 
[27] J. J. Gibson, “Observations on active touch,” Psychological 
Review, Vol. 69, pp. 477-491, 1962. 
[28] K. Makiyama, M. Nagasaka, T. Inuiya, K. Takanami, M. 
Ogata, and Y. Kubota, “Development of a patient-specific 
simulator for laparoscopic renal surgery,” Internationl Journal 
of Urology, vol. 19, pp. 829-835, 2012. 
[29] M. Zöllner, S. Huber, H.C.Jetter, and H. Reiterer, “NAVI – A 
Proof-of-Concept of a Mobile Navigational Aid for Visually 
Impaired Based on the Microsoft Kinect,” in Proceeding of 
13th IFIP Tc13, Conference on Human – Computer Interaction, 
pp. 584-587, 2011. 

