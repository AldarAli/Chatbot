Consensus Making Algorithms based on Invariants
Perception for Cognitive Sharing in Multi-Robot
Shodai Tomita and Kosuke Sekiyama
Department of Micro-Nano
Systems Engineering, Nagoya University
Furo-cho, Chikusa-ku, Nagoya, Japan
Email: {tomita, sekiyama}@robo.mein.nagoya-u.ac.jp
Toshio Fukuda
Faculty of Science and Engeneering, Meijo University
1-501 Shiogamaguchi, Tenpaku-ku, Nagoya, Japan
Email: tofukuda@meijo-u.ac.jp
Abstract—Visual recognition in multi-robot systems is afﬂicted
with a peculiar problem that observations from different view-
points present different perspectives. Moreover, a representation
of the same target object is highly affected by the viewpoint or
environmental condition. Hence, realizing cognitive sharing of
the object among robots in an unconstructed environment has
become challenging. To cope with this issue, we have proposed
a Hierarchical Invariants Perception Model (HIPM) in which
multiple representations; color, shape, geometric relation, are
dynamically evaluated and selected by the robot. In this paper,
we propose consensus-making algorithms to acquire viewpoint-
invariant representations. Experimental results show the ability of
cognitive sharing signiﬁcantly improved by the proposed method.
Keywords–cognitive sharing; multi-robot; consensus making
I.
INTRODUCTION
Cognitive sharing of an object is a primary issue in multi-
robot task execution, where robots with different perspective
are expected to cooperate in our daily unstructured environ-
ment. As robots engage in more varied and difﬁcult tasks, they
will become a ubiquitous part of our daily life in the future
[1]. Researchers generally agree that multi-robot systems of
inherently distributed character may behave more robustly
and effectively and accomplish cooperative tasks that are not
possible for single robot systems [2].
However, cognitive sharing methods in conventional works
are generally difﬁcult to be applied to unstructured environ-
ment. The conventional cognitive sharing has been based on
the premise that the target is attached by an artiﬁcial marker
such as RFID [3] and QR code [4], and that the target is a
single-colored sphere [5].
We deem that visual information is suitable for the cog-
nitive sharing in unstructured environment. One of the main
issues in the cognitive sharing is to avoid misrecognition of
the target object. Since visual information gives many kinds
of representations of an object, we deem that, by using a
combination of the representations, robots can verify whether
or not they observe the same object. Although the cognitive
sharing may be realized by sharing a global position of
the target, localization errors by robots will make difﬁcult
cognitive sharing based on positional information. The position
of the target in a global coordinate with errors only enables
robots to share a region of interest (ROI).
2013/8/20
1
Describing Representation of Target
Single Object
Semantic
Label
Color
Shape
Left of landmark
Meal(Dish)
Chopstick
Red
Rectangle
4
2
5
6
1
Ambiguity
Stationarity
Relation
Primitive
Entity
Evaluation
Input
Sensing 
Data
Hierarchical Invariants Perception Model (HIPM)
Multiple 
Objects
3
Calculating Repres-
entational Priority
Sharing a Viewpoint-
Invariant Decision Tree
Compare 
and 
Adjust 
Representa-
tions
Comm-
unicate
Other Robot’s HIPM
✔
✔
Figure 1.
Hierachical Invariants Perception Model
However, visual recognition in multi-robot systems has a
peculiar problem that representations are highly affected by
the robot’s viewpoint and environmental condition. Since ob-
servations from different viewpoints present different represen-
tations, all representations cannot be shared. Also, unstructured
environments abound with unavoidable disturbances, such as
illumination changes, object occlusion and sensor faults, that
would disturb cognitive sharing and even object recognition.
To cope with these issues, we have proposed a Hierarchical
Invariants Perception Model (HIPM) [6] which deals with the
cognitive sharing and object tracking simultaneously (Figure
1). A robot describes different classes of representation from
sensing data (Describing Representations of Target), and evalu-
ates the ambiguity, which estimates the risk of recognition fail-
ures for the target in the ROI, and stationarity which indicates
the steadiness of the ambiguity over time. The robot selects
a unique and stable representation based on ambiguity and
stationarity (Calculating Representational Priority). Then the
robot combines the representations and constructs a decision
tree. By comparing and adjusting the representations in the
decision tree through communication, robots reach a consensus
(i.e., what representations can be shared). Finally, when the
robots share the same decision tree, the target is identiﬁed
(Sharing Viewpoint-Invariant Decision Tree).
In this paper, we extend the HIPM in that a deﬁnition
of a relation representation and sharing a viewpoint-invariant
decision tree are formulated, and experimentally illustrated.
Although we have proposed the technique of autonomous
landmark generation [7], in which some peripheral objects near
the target are selected as landmarks in real time to relate the
target to the surroundings, problems resulting from differet
viewpoints are not considered and a geometric relation among
the target and multiple-landmark has not been used.
32
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

This paper is organized as follows. The System overview
is described in Section II. Visual representations; primitive
representation and geometric relation-based representation are
presented in Section III. The method of identify the target
and consensus-making algorithms are elaborated in Section IV.
Experimental results are described in Section V. Finally, the
conclusions are discussed in Section VI.
II.
SYSTEM OVERVIEW
The purpose of the proposed approach is to share one target
object in unstructured environment between two robots: robot
A, which recognizes the target, and robot B, which does not
know any information on the target. Several assumptions are
made as follows.
(a) Environment: Some objects in the environment are
similar to the target. The objects do not move. An appearance
of the objects may change as a result of a change in viewpoint.
An occlusion of the objects may occur.
(b) Robots: The robots can localize themselves though the
localization has errors and are equipped with a RGBD sensor.
The proposed approach based on the HIPM has the two
features:
•
Describing representations of the target (in Section
III) Robot A describes representations of the target
from a RGBD image. In this paper, color, shape
and geometric relation-based representation (GRR) are
employed.
•
Sharing viewpoint-invariant decision tree (in Sec-
tion IV) Robot A constructs a decision tree, which
is a suitable combination of the representations for
identifying the target from the viewpoint of robot
A. Robot B receives the decision tree and decides
which representations are viewpoint-invariant. If robot
B concludes the received decision tree includes non
viewpoint-invariant representations, robot A sends a
new decision tree. Through this process, robots share
a viewpoint-invariant decision tree gradually.
Calculating representational priority is addressed in [7]. Since
we do not assume drastic illumination changes, this component
is not discussed in this paper.
III.
DESCRIBING REPRESENTATIONS OF TARGET
A. Primitive Representation
We employ color and shape features as the basic represen-
tations to recognize an object, which are referred to as primi-
tive representation in this paper. In visual recognition, image
features (e.g., color [8], shape [9], and feature point [10]) have
been used to recognize an object. Although feature points may
be salient and therefore suitable for object recognition, they are
susceptible to viewpoint changes. However, color and shape
features tend to be robust against viewpoint changes.
Because the robots have different viewpoint, the primitive
representation should be invariant with respect to scale and il-
lumination changes in the visual recognition. A hue histogram
is known to be an invariant representation with respect to scale,
illumination direction, and angle changes [11]. In this paper,
the histogram similarity function is expressed by histogram
intersection [12]. Histogram intersection is computationally
efﬁcient and robust against partial occlusion and resolution
changes. The histogram axis is divided into 32 sections for
computational efﬁciency and recognition accuracy. The simi-
larity of color representation Sc is calculated from
Sc(Ha,Hb) =
32
∑
i=1
min(Ha(i),Hb(i)),
(1)
where Ha and Hb represent the hue histogram of object Oa
and Ob respectively, and H(i) represents the value of i-th
histogram’s bin.
Also, hu moments of a contour is used as a shape repre-
sentation in this paper. Hu moments are invariant with respect
to scale changes and rotation. By following the deﬁnition in
[14], the similarity between two moments can be calculated
Ss(ha,hb) =
7
∑
i=1

ma(i)−mb(i)
ma(i)
,
(2)
where,
m(i) = sign(h(i))·log|h(i)|,
ha and hb represent hu moments of object Oa and Ob respec-
tively, and h(i) represents the value of i-th hu momemts.
B. Object Segmentation
To describe the primitive representation of each object,
the input image has to be divided into objects and the other
areas, and the boundaries have to be contours of objects. In
general, a computationally efﬁcient segmentation method is
required because the robots are supposed to move around.
In this paper, we employ a segmentation method based on
the depth information obtained using an RGBD sensor [15].
RGBD sensors (Kinect or Xtion) can output sensing data at
a frame rate of 30 Hz and this method can extract accurate
contours by using the depth information. Fortunately, because
the depth information can be captured under any ambient
light conditions, the shape representation is invariant against
arbitrary illumination changes.
C. Similar Primitive Representation from Different Viewpoints
In general, a primitive representation will be highly affected
by changes in viewpoint. However, empirically, a similarity
of primitive representation between the same object from
different viewpoints lies within a certain range.
Assume a color representation Ha and shape representation
ha of an object Oa are sent from robot A. When a color
representation Hb of object Ob, which is perceived by robot
B, satisﬁes
Sc(Ha,Hb) ≥ 0.7,
(3)
we deﬁne Ob has a similar color representation to Ha. Also,
when a shape representation hb of Ob satisﬁes
Ss(ha,hb) ≤ 0.3.
(4)
we deﬁne Ob has a similar shape representation to ha.
33
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

If robot B perceives only one similar color (shape) repre-
sentation to Ha (ha) after sharing a ROI, deﬁne Ha (ha) as
viewpoint-invariant representation. If Ha or ha is viewpoint-
invariant, Oa and Ob may be the same object.
D. Geometric Relation-Based Representation
In this paper, as the relation representation, we use geomet-
ric relation between the target and sharable surroundings. Use
of geometric relation offers two advantages: 1. the error for
the relative positions of objects is comparably smaller than the
error of global position and 2. the information is independent
of robot localization and odometry. Therefore, this relation is
viewpoint-invariant.
Two processes are needed to form a GRR as follows.
1)
Select candidate objects of GRR components that
have a salient primitive representation.
2)
Describe the geometric relation among the target
and the components of GRR based on a distance
information.
An object which has no similar representation from robot
A’s viewpoint is likely to be identiﬁed uniquely. Therefore,
such objects are suitable for candidates of GRR components.
Three objects of the same kind of primitive representation
(e.g., three objects which have unique color representations)
are selected from the candidates, and a triangle is formed. The
reason why the same kind of primitive representation should
be selected is that color and shape representations are invariant
with respect to different disturbances (e.g., color representation
is invariant to partial occlusion and shape representation is
invariant to changes in lightning conditions). The reason why
a triangle is chosen is that it is the minimum unit needed
to divide a ﬂat space into a closed area and other geometric
shapes can be represented by a combination of triangles.
A GRR divides the recognition area into 7 areas Aa(a ∈
{1,2,...7}). The decomposed area Aa is represented by the
triple set of + and - sign as shown in Figure2. The decomposed
area where the target belongs is denoted by At. Assuming
l candidates for the GRR components, the number of con-
structed GRR m is lC3. A GRR is regarded as viewpoint-
invariant when all GRR components are viewpoint-invariant.
2013/8/21
+
-
+
+
+
-
-
-
n1
n2
n3
e12
e23
e31
A1
A3
A2
A6
A4
A5
t A7
{ }
8
p = +
{ }
9
p = −
P8
P9
}
,
,
{
31
23
12
e
e
e
Aa =
{ , , }
1
= − − +
A
{ , , }
3
A = − + +
{ , , }
2
= − + −
A
{ , , }
4
A = + − −
{ , , }
5
= + − +
A
{ , , }
7
A = + + +
{ , , }
6
= + + −
A
t
t
t
t
t
t
Figure 2.
Representation of Target Position. The components of a GRR are
denoted by n1,n2,n3 and connected in the counterclockwise direction. A link
vector (e12,e23,e31) which connects to the components will decompose the
recognition area into two domains. The left-side of the link vector where each
vector is linked counterclockwise is denoted as the positive sign (+) and the
right-side of each vector is denoted as the negative sign (-).
IV.
SHARING VIEWPOINT-INVARIANT DECISION
TREE
A. Construction of the Decision Tree
We use a binary decision tree that consists of combined
representations to identify the target because it is very rare
when the target object can be uniquely identiﬁed by means of
a single representation. Such a limited case is the following:
(i)
When a primitive representation is employed, no
similar representation is found closely to the target
object.
(ii)
When a GRR is employed, only the target object is
included in the decomposed area At of the GRR.
In order to construct the decision tree, we employ the
branch and bound algorithm. In the cognitive sharing, since
the data of the target class is only one, conventional methods
(e.g., C4.5 and CART) cannot be used because these methods
require adequate data to select an effective node.
The branch and bound algorithm has the advantage of
constructing a decision tree that can minimize the required
number of nodes. Redundant information may increase search-
ing time because not all representations can be shared owing to
appearance changes and occlusion. The solution to the problem
resulting from viewpoint changes is discussed in Sec. IV-B.
The branch and bound algorithm is given as follows:
Assume a set of n objects O = {O1,O2,··· ,On}, which is
perceived by robot A. An object Oi(∈ O) is described by color
representation i.e., hue histogram Hi, shape representation i.e.,
hu moments hi and m GRRs gi
j(j ∈ M = {1,2,...,m}). From
robot A’s viewpoint, candidates of the target object Ot(∈ O)
can be reduced by using the target’s representations Ht,ht,gt
j
according to
Rj = {Oi ∈ O|Oi ∈ Aj
t } (j ∈ M),
(5)
Rm+1 = {Oi ∈ O|Sc(Ht,Hi) ≥ 0.7},
(6)
Rm+2 = {Oi ∈ O|Ss(ht,hi) ≤ 0.3}.
(7)
Here R j,Rm+1, and Rm+2 represent a set of candidates reduced
by using similarity criteria gt
j,ht, and Ht respectively.
(i)
Objective Function and Constraint Condition
Let us denote a collection of the target’s candidate
sets by R = {R1,R2,...,Rm,Rm+1,Rm+2}. The goal
is to ﬁnd a combination of the target’s representa-
tions that can narrow candidate objects of the target
down to one such that the number of tree nodes
is minimized. The objective function and constraint
condition are deﬁned as follows:
minimize |X| = |∩Rk∈R′ Rk|, (R′ ⊂ R),
(8)
subject to |X| ≥ 1, |R′| ≤ 3,
(9)
where |·| represents the cardinality of a set. If R′ =
{R1,Rm+1}, 1st GRR gt
1 and color representation Ht
are employed as nodes of a decision tree. In order to
reduce redundant information, the number of nodes
|R′| is limited to 3.
(ii)
Branching
Subproblem Pi ( breadth ﬁrst search)
Minimize |X| subject to |R′| = i (i ∈ {1,2,3})
34
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

(iii)
Bounding
Prune if |X| ≤ z. z is the minimum upper bound seen
among subproblems examined so far.
Finish if |X| = 1.
B. Comparing and Adjusting Representations
As mentioned in Sec. IV-A, a decision tree sent by robot A
can include representations which cannot be shared between
the robots resulting from different viewpoints. In this sec-
tion, we discuss how two robots perceive viewpoint-invariant
representations and share a viewpoint-invariant decision tree
through communication. Figure 3 shows a state transition
diagram of robot A and robot B. The consensus-making
algorithms are composed of four functional parts; ROI sharing,
searching, invariants perception, and decision tree adjustment.
We explain how robot B changes its state mainly because robot
A changes its state according to robot B’s request.
2013/9/13
1
wait
Wait for 
receiving 
decision tree
Adjust 
decision 
tree
Finish 
cognitive 
sharing
searching
Perceive 
invariant s
Change 
viewpoint
ROI
sharing
Receive global 
position of 
target
Finish 
sharing 
ROI
Receive 
n=1
n=0
n>1
Request new 
decision tree
Add 
representation
GRR or Primitive 
representation is non-invariant
Occlusion may occur
Finish 
moving
Robot B
Robot A
wait
ROI
sharing
Finish 
sharing 
ROI
Send 
decision 
tree
Send additional 
representation
Finish 
sending
Receive 
request
Receive 
request
Finish 
sending
①
③
②
①
③
④
④
No valid 
representations
n: number of    
target’s 
candidates
Figure 3.
State Transition Diagram of Robot A and B
1) Sharing ROI (Figure 3- 1⃝): To verify whether or not a
representation is viewpoint-invariant, a ROI has to be shared.
Without sharing the ROI, robots cannot determine the cause
of the misrecognition by searching another region or a non-
viewpoint-invariant representation.
In this paper, since we assume robots can localize them-
selves and get depth information from RGBD sensor, robots
can calculate a global position of objects. We deﬁne a region
around a global position of the target as a ROI. The robots
share the ROI by sharing the global position of the target. How-
ever, since the robot localization has errors, cognitive sharing
cannot be achieved by only using the positional information.
Robot A sends the global position of the target to robot
B. On receiving, robot B moves to a position only a certain
distance away from the global position of the target and ﬁnish
sharing the ROI. In this paper, we deﬁne the distance as 1.5m.
2) Searching (Figure 3- 2⃝): After sharing ROI, robot A
sends a decision tree. On receiving the decision tree, robot
B starts searching. Since robot B can calculate a position
of objects in robo-centric coordinate, robot B searches while
making a map of objects. Considering an area that has been
searched, robot B decides where to search. After rotation, robot
B perceives objects and updates the map. Robot B compares
objects in the map with the received decision tree and moves to
a position where objects which has identiﬁed representations
in the decision tree can be observed.
When all representations in the decision tree are identiﬁed
and candidates of the target object are found, robot B change
the state: change the state to ﬁnish cognitive sharing if the
number of the candidates is one, change the state to adjust
a decision tree if the number of the candidates is more
than one. When the number of the candidates is zero or all
representations in the decision tree is not identiﬁed, robot
B lasts searching. Robot B change the state to invariants
perception when ﬁnishes searching around the ROI.
3) Invariants Perception (Figure 3- 3⃝): Representations
which are not identiﬁed though robot B ﬁnishes searching
around the ROI are regarded as non viewpoint-invariant repre-
sentations. Robot B takes different actions according to a kind
of the non-viewpoint-invariant representation.
(i)
Primitive representation of the target
Two situations are conceivable: the appearance
of the target may change or the occlusion of the
target occurs. In order to verify whether the occlu-
sion occurs or not, robot B changes its viewpoint
and searches around the ROI again. In this paper,
changing viewpoint is achieved by moving a certain
distance to a tangential direction of a circle whose
center is the received global position of the target
and the distance is deﬁned as 0.8[m] empirically. If
the primitive representation cannot be identiﬁed even
after changing viewpoint, robot B requires robot A
to send a new decision tree which does not include
the primitive representation because its description is
likely to be varied.
(ii)
GRR
When not all components of a GRR in the received
decision tree are identiﬁed, robot B requires robot A
to send a new decision tree which does not include
the unidentiﬁable components of the GRR. When the
GRR is shared but the number of candidates narrowed
down by using the GRR is zero, robot B changes
its viewpoint and begins searching again because an
occlusion of the target objects may occurs. If the
candidates are not found after changing viewpoint,
the GRR is regarded as non-viewpoint invariant.
Then, robot B requires robot A to send a new decision
tree.
4) Decision Tree Adjustment(Figure 3- 4⃝): Even though
the robots successfully share viewpoint-invariant representa-
tions, robot B sometimes may fail to identify the target when
the primitive representation is not viewpoint-invariant. For
example, this would occur when an object out of robot A’s
view exists in At of the GRR or when the objects near the
target in At change their appearance.
In these situations, robot B has to determine what in-
formation is needed autonomously. Robot B calculates the
similarity of primitive representations among the candidates
as reduced by using the received decision tree. If the primitive
representations of the candidates are not similar, robot B
requests the primitive representation. Then, robot B adds the
primitive representation to the decision tree and tries to identify
the target. Robot B also requests another decision tree if the
number of candidates cannot be reduced by using only the
primitive representation. Robot B will ﬁnish searching when
35
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

the number of candidates is reduced to one or when the number
of candidates obtained by using the decision tree is the same
between the robots.
V.
EXPERIMENT
A. Experimental Conditions
This experiment demonstrates that the proposed approach
has robustness against following problems:
•
Robot B mistakes the target for a similar object.
•
Not all representations can be shared because an
occlusion and appearance change may occur resulting
from different viewpoints.
The experiment environment is shown in Figure 4. Each robot
(robot A: amigobot; robot B: Pioneer 3-AT) is equipped with
a Xtion Pro Live and a communication module (OKI UDv4).
The initial position and pose of a robot (x,y,θ) are deﬁned by
adding random noise w1,w2,w3 with a Gaussian distribution
having standard deviation 0.3 to true value (xtrue,ytrue,θtrue).
(x,y,θ) = (xtrue +w1,ytrue +w2,θtrue +w3).
(10)
The robots localize themselves using odemetry. Robot A
recognizes the target in the middle left of Figure 4 and robot
B does not know a propri target information. There exists a
similar object to the target in the environment in the middle
of Figure 4 labeled as ”dummy”. We assume that dramatic
illumination changes and the movement of objects will not
occur during cooperation.
Figure 4.
Experiment Environment
B. Experimental Result
Snapshots of the experiment are shown in Figure 5-9. The
constructed decision tree is shown in the upper left of each
image. Robot’s action and communication condition are shown
in the above each image. The red bounding box in the image
represents the target and the blue bounding boxes represent
identiﬁed components of the GRR. The objects with white
crosses represent non viewpoint-invariant representations.
1) ROI Sharing: By t = 3[s], robot B received the global
position of the target and start sharing the ROI. By t = 8[s],
Robot B moved 1.5m away from the received global coordinate
and ﬁnished ROI sharing (Figure 5).
Since the initial position of robots are added random noise,
a target position estimated by robot B has an error as shown
in Figure 10. Only using positional information of the target,
robot B may fail to identify the target because the estimated
1
t=3[s]
t=8[s]
Figure 5.
Snapshots of ROI sharing
1
t=16[s]
2013/9/13
1
t=19[s]
2013/9/13
1
t=36[s]
Figure 6.
Snapshots of Searching (Top row: robot A’s view (left), robot B’s
view (right). Middle and bottom row: robot B’s view (left), a map made by
robot B (right). )
2013/9/13
1
t=47[s]
2013/9/13
1
t=63[s]
Figure 7.
Snapshots of Invariants Perception (Left column: robot A’s view.
Right column: robot B’s view. )
2013/9/16
t=73[s]
t=84[s]
Figure 8.
Snapshots of Change Viewpoint
2013/9/13
t=91[s]
Figure 9.
Snapshots of Finish Cognitive Sharing. (Left: Robot B’s view,
Right: a map made by robot B)
36
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

Figure 10.
true and estimated position of robots and the target. Red robot:
true position of robot A. Pink robot: robot A’s position estimated by robot
A. Blue robot: true position of robot B. Light Blue robot: robot B’s position
estimated by robot B. Green circle: true position of the target. Red circle: target
position estimated by robot B. Black circles: surroundings near the target (not
all). Lines are drawn at intervals of 0.5m.
position is closer to position of surroundings than the true
position.
2) Searching: By t = 16[s], robot B received the decision
tree from robot A and started searching. By t = 19[s], robot B
found two components of the GRR and then continue searching
in the ROI to ﬁnd the rest of components. By t = 36[s],
although robot B ﬁnished searching in the ROI, robot B did not
ﬁnd the rest of the components. Therefore, robot B changed
its state to Invariants Perception. (Figure 6)
3) Invariants Perception: By t = 40[s], robot B regarded
the unidentiﬁable component of the GRR as non-viewpoint-
invariant, and required robot A to send a new decision tree
which did not include the unidentiﬁed component of the GRR.
In fact, an occlusion of the component occurred.
By t = 47[s] (Figure 7), robot B received a new decision
tree. However, one component of the GRR did not found.
Therefore, robot B regarded it as non viewpoint invariant and
required the new decision tree. In fact, the appearance of the
component changed resulting from different viewpoints.
By t = 43[s] (Figure 7), robot B received the new decision
tree and succeeded in identifying the GRR in the decision
tree. However, candidates of the target were not found in the
decomposed area of the GRR. Robot B considered that the
target was occluded, and changed its viewpoint.
By t = 84[s], robot B ﬁnished changing its viewpoint by
moving 0.8[m] to a tangential direction of a circle whose center
is the received global position of the target as shown in Figure
8.
4) Finish Cognitive Sharing: By t = 91[s] (Figure 9), robot
B found a candidate of the target in the decomposed area in
the new viewpoint. Because the color representation of the
candidate corresponded with the color representation of the
second tree node, robot B succeeded in sharing the target.
5) Decision Tree adjustment: Another experiment was con-
ducted to demonstrate the necessity of decision tree adjustment
as shown in Figure 11. By t = 95[s], the robots shared the deci-
sion tree composed of the viewpoint-invariant representations.
However, similar color object appeared in the decomposed area
owing to an appearance change, so robot B did not make
a difference between these candidates. Robot B determined
the necessary representation autonomously and requested the
1
t=95[s] (Robot B)
t=100[s] (Robot B)
Figure 11.
Snapshots of Other Experiment
shape representation in this situation. By t = 100[s], robot B
succeeded in cognitive sharing by adding the shape represen-
tation to the decision tree.
VI.
CONCLUSION AND FUTURE WORK
We proposed cognitive sharing algorithm based on visual
information. A decision tree including geometric relation-
based representation allows robots to share precise ROI and
avoid mistaking the target for a similar object. The Consensus-
making algorithms serve to acquire viewpoint-invariant repre-
sentations.
For future work, an important issue will be adaptation to
disturbances. Although we assumed that dramatic illumination
changes and the movement of objects would not occur in
the experiment, such disturbances occur in unstructured en-
vironment. Evaluating ambiguity and stationarity in a HIPM,
robots should select robust representations against a certain
disturbance.
REFERENCES
[1]
B. Gates, ”A Robot in Every Home”, Scientiﬁc American vol.296, pp.58-65, 2007.
[2]
T. Arai, E. Pagello and L. E. Parker, ”Editorial: Advances in multi-robot systems”,
IEEE Transaction on Robotics and Automation, vol. 18, no. 5, pp. 655-661, 2002.
[3]
K. G. Tan, A. R. Wasif and C.P. Tan, ”Objects Tracking Utilizing Square Grid Rﬁd
Reader Antenna Network”，Journal of Electromagnetic Waves and Applications,
vol. 22 , pp.27-38, 2008.
[4]
Y. Xue, G. Tian, R. Li and H. Jiang, ”A New Object Search and Recognition
Method based on Artiﬁcial Object Mark in Complex Indoor Environment”, World
Congress on Intelligent Control and Automation, pp.6648 - 6653, 2010.
[5]
D. Gohring and J. Homann, ”Multi Robot Object Tracking and Self Localization
Using Visual Percept Relations”，Proceedings of IEEE/RSJ International Confer-
ence of Intelligent Robots and Systems, pp.31-36. 2006.
[6]
T. Umeda, K. Sekiyama and T, Fukuda, ”Cooperative Distributed Object Tracking
by Multiple Robots Based on Feature Selection”, DARS, Springer Berlin Heidel-
berg, pp.103-114, 2013.
[7]
T. Umeda, K. Sekiyama and T. Fukuda, ”Vision-Based Object Tracking by Multi-
Robots”, Journal of Robotics and Mechatronics, vol. 24, No.3, pp.531-539, 2012.
[8]
D. Comaniciu, V. Ramesh and P.Meer,”Kernel-Based Object Tracking”, IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol.25, no.5, pp.564-
577, 2003.
[9]
M. Isard and A. Blake, ”Contour Tracking by Stochastic Propagation of Con-
ditional Density”, Proc. Fourth European Conf. Computer Vision, pp.343-356,
1996.
[10]
D. G. Lowe, ”Object Recognition from Local Scale-Invariant Features,” Proc.
Seventh IEEE Int’l Conf. Computer Vision, vol. 2, pp.1150-1157, 1999.
[11]
T. Gevers and A.W.M. Smeulders, ”Color based object recognition,”　 Pattern
Recognit., vol. 32, pp.453-465, Mar. 1999.
[12]
M.J. Swain and D.H. Ballard, ”Color Indexing”, International Journal of Computer
Vison, vol. 7, No. 1, pp. 11-32, 1991.
[13]
B. Gary and A. Kaehler. ”Learning OpenCV: Computer vision with the OpenCV
library”, O’Reilly Media, Incorporated, pp.251-256, 2008.
[14]
D. Filliat, E. Battesti, S. Bazeille and G. Duceux, ”RGBD object recognition and
visual texture classiﬁcation for indoor semantic mapping.”, IEEE International
Conference on Technologies for Practical Robot Applications (TePRA), pp.127-
132, 2012.
37
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

