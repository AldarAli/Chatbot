Simulating Psychological Experiments:
An Agent-Based Modeling Approach
Lukas Reuter, Jan Ole Berndt, Ingo J. Timm
Business Informatics 1
Trier University
54296 Trier, Germany
Email: [reuter,berndt,itimm]@uni-trier.de
Abstract—Analyzing human behavior in organizational structures
becomes more difﬁcult due to a rising complexity of teamwork
processes and interdependencies between team members. The pa-
per proposes an interdisciplinary approach of agent-based model-
ing and laboratory experiments from organizational psychology
to overcome the shortcomings of each discipline and to allow
a more detailed and realistic view on teamwork in theory and
practice. To demonstrate the beneﬁts of simulating psychological
experiments, the replication of a small group experiment for
analyzing the distribution of meta-knowledge is conducted. The
results from simulation and experiment are plausible.
Keywords–Agent-Based Modeling; Social Simulation; Team
Mental Models; Meta-Knowledge.
I.
INTRODUCTION
Due to rising complexity of teamwork processes, knowl-
edge distribution and interrelationships between team mem-
bers, understanding human behavior in organizational struc-
tures becomes more difﬁcult. To overcome the shortcomings
in analyzing teamwork, an interdisciplinary approach of Agent-
Based Modeling (ABM) and theories from organizational
psychology is promising to gain new insights. From a psy-
chological perspective, theories and concepts of teamwork can
be analyzed in laboratory experiments. The major drawbacks
of these experiments are restrictions due to their limitations
such as ﬁnancial ones. ABM can be used to overcome the
shortcomings and complement laboratory experiments, e.g.,
their design and scalability [1]. From an ABM perspective,
modeling teamwork experiments can enable a more detailed
comprehension of human behavior and should allow for more
realistic agent architectures. One major characteristic of ABM
is modeling behavior with speciﬁc actions which enables
causal explanations of an observed pattern. However, in psy-
chological research, experiments are commonly used to reveal
correlations between conditions and measurements. Therefore,
replicating a teamwork experiments with ABM supports psy-
chologists to observe how behavioral team patterns emerge
from individual actions [2].
The contribution of this paper is twofold: (1) how can
ABM researcher and psychologists beneﬁt from each other
as well as what prerequisites are necessary to model and
simulate psychological experiments and (2) presenting a study
of replicating a teamwork experiment, which should clarify
challenges and opportunities. The replicated study in this
paper focuses on the different measurements (correlation vs.
causality) in psychological research and ABM.
The remainder of this paper is structured as follows. In
Section II, psychological research methods in a simulation
context are shown. Section III presents an experimental study
which utilizes a measurement: the Team Mental Model index
(TMM Index) to analyze the distribution of meta-knowledge in
teams and its consensus among team member. Consequently,
an agent-based model of the TMM Index experiment is shown
in Section IV. In Section V, a comparison of the results from
experiment and simulation is given.
II.
SIMULATING PSYCHOLOGICAL EXPERIMENTS:
FOUNDATIONS AND CHALLENGES
Psychological research focuses on human cognition and
behavior. In that context, experiments reveal correlations be-
tween controlled conditions and observable behavior (indepen-
dent and dependent variables). Those correlations represent
generalizable behavioral patterns, which can be utilized to
indirectly draw conclusions about psychological constructs of
human cognition for individuals and small groups.
Various scholars have proposed computer simulation in
general and ABM in particular as a method in the ﬁeld
of psychology. However, these simulation studies can only
provide insights into artiﬁcial systems as modeled and rep-
resented in a computer. Consequently, two questions arise.
Firstly, why should psychological experiments be simulated
since a computer does not exhibit human behavior? Secondly,
how should these experiments be simulated, i.e., what are
challenges and existing practices of modeling and simulat-
ing psychological experiments? The following two sections
explore these questions and provide an overview of existing
research in that area.
A. Why Simulate Psychological Experiments?
There are three main beneﬁts of simulating psychological
experiments. These roughly correspond to the following three
phases of a computer simulation study.
1)
The model development phase
2)
The experiment and simulation design phase
3)
The experimentation and results analysis phase
Activities in each of these simulation phases can both con-
tribute to psychological research. Vice versa, simulating psy-
chological experiments throughout these phases can also pro-
vide novel impulses for social simulation as a research method.
The ﬁrst beneﬁt of simulating experiments is their con-
tribution to formal procedures and theory building. Due to
5
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-648-4
HUSO 2018 : The Fourth International Conference on Human and Social Analytics

the necessity to formalize the experiment in a computational
model, such a simulation enforces a rigorous formulation of
independent and dependent variables as well as the underlying
theoretical constructs and their interrelations. The variables
denote which conditions are manipulated during an experiment
and which measurements are taken. These measurements are
then used to conclude on underlying theoretical constructs
which cannot be observed directly. However, in a simulation,
these must be explicitly modeled in order to create observable
effects. Consequently, developing simulation models requires
explicating these constructs in a computable form. This sup-
ports precise descriptions of both psychological experiments
and theory. In turn, precisely formalized psychological theory
and cognitive or behavioral models can help building better
social simulations. To be realistic, such simulations must be
grounded in actual human cognition and behavior. Psycholog-
ical experiments provide this grounding.
The second beneﬁt covers the design of both experiments
and simulations. In organizational as well as social psychology,
experiments frequently involve groups of people and their
interactions. To enable statistically relevant conclusions, each
combination of independent variables must be sampled often
enough. Every test must be repeated with at least 30 groups.
Given a minimal group size of three persons, this kind of
research requires at least 90 test persons per experiment con-
dition; e.g., 360 persons for a simple 2x2 experiment design.
Thus, it rapidly becomes unfeasible to scale up either the
number of independent variables or the group size. Simulating
experiments can help solve this problem since the number
of agents in a model can be scaled up easily. If such a
simulation is grounded in actual experiments, scaling it up
may reveal interesting effects in the observed artiﬁcial system.
These can then be used to design further experiments to
verify or reject those ﬁndings. The experiments can then be
reduced to the speciﬁc conditions that produce the respective
effects in the simulation. Hence, the problem of scalability is
alleviated. Moreover, the results provide additional validation
of the simulation model or reveal the requirement for its
reﬁnement. This leads to a iterative experimentation process in
which psychological experiments provide new hypotheses for
the design of simulation studies and simulation results inspire
additional laboratory experiments.
The third beneﬁt of psychological experiment simulation
is its contribution to the analysis of either method’s results
as well as to deriving and testing theoretical concepts. In
psychology, experiments are used to reveal correlations be-
tween manipulated conditions and measurements to conclude
on underlying theoretical concepts. However, these correlations
cannot provide causal explanations of the observed effects.
By contrast, computer simulations require the modeling of
causality. Such a model provides a possible candidate for
an explanation. While its correctness cannot be proven, it
can be assumed to be feasible as long as it can replicate
experiment results. Therefore, simulating experiments helps
both developing and testing psychological theory as well as
verifying measurements and results of social simulations.
Nonetheless, there are several challenges to be overcome
for achieving the aforementioned beneﬁts. The following sec-
tion discusses existing research and identiﬁes the challenges
which will be addressed in the remainder of this paper.
B. Related Work
In order to simulate psychological experiments, it is neces-
sary to transform these experiments into computational models
and social simulation studies. Despite the recognition of ABM
as a research method in (organizational and social) psychology,
few researchers have attempted this so far. Instead, the majority
of existing work focuses solely on simulation models of
teamwork processes or on cognitive agent architectures. Thus,
there is little work available on the challenge of transforming
experiments into simulations and vice versa. Nevertheless, the
following works in this context are noteworthy.
The majority of related work focuses on the development
and exploration of conceptual models for explaining behavioral
processes. This approach is primarily used for theory building
by studying how the modeled interactions change within a
simulation. It usually starts with a theoretical concept of
cognition and behavior for the addressed application area
which is then transferred into a computational model. For
instance, Ren et al. model the meta-knowledge in teams to
systematically explore when it is beneﬁcial to know what
other team members know in an agent-based simulation [3].
Similarly, Smith & Collins use such a simulation to analyze
the impact of social contexts on distributed cognition processes
[4]. Thus, both of these studies focus on model development
to gain a more thorough understanding of theoretical concepts
and their interplay.
Another line of research covers on the simulation design
phase. Instead of being strictly theory-driven, those approaches
focus on parameterizing individual agents from survey data.
They use this data for creating unique proﬁles, which control
the respective agents’ decision-making in a social simulation
[5]. While such an approach is more common for analyzing
large populations [6], Kangur et al. make use of that technique
to model inﬂuence factors and decision-behaviors for the
acceptance of electric cars [7]. This contribution provides
interesting insights into the transfer of variables in the form
of survey items into parameters for artiﬁcial agents in a
simulation setting.
The aforementioned approaches follow the recommended
pattern of developing a model from theoretical concepts,
implementing it, and parameterizing it using empirical data
[2] [8]. However, none of them combines simulations with
laboratory experiments. In that regard, Grand et al. comple-
ment those works by starting from a theory-driven model and
then conducting an experiment to verify that model [9]. They
ﬁrst analyze processes of knowledge emergence in teams in a
computer simulation and then compare the results with those
gained from experiments with human subjects. Consequently,
they contribute to the results analysis phase of experimentation
and simulation.
While the discussed related work covers all of the beneﬁts
of simulating experiments, a direct replication of an experiment
in a simulation has not yet been attempted. Especially the
advantage of scaling up experiments and exploring different
settings in a simulation for further experiment design remains
unused. To achieve that, it is necessary to transform the settings
of an existing experiment into simulation inputs and outputs
similar to using survey data for agent parameterization. In
addition, the available information and the decision-making
of these agents must be speciﬁed. To that end, an appropriate
abstraction of knowledge and interaction between these agents
6
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-648-4
HUSO 2018 : The Fourth International Conference on Human and Social Analytics

is required. Only then can the resulting model be used to
replicate an experiment as well as varied to explore alternative
settings. In the following, such a study is presented which
shows how these challenges can be met.
III.
APPLICATION: TEAM MENTAL MODELS FOR
EXPERTISE LOCATION
In order to show the beneﬁts and challenges of simulating
psychological studies, an experiment is needed with: (1) a
remote set of variables and (2) with relevance for ABM.
Therefore, we choose an experiment which analyzes the distri-
bution of meta-knowledge between team members. For team
processes the knowledge of each team member is important
to team performance [10]. Besides task related knowledge,
the perspective of ”who knows what in the team” (meta-
knowledge) is also essential. In the past, studies showed posi-
tive effects of high meta-knowledge and team performance [11]
[12]. Therefore Ellwart et. al. developed a valid and economic
measurement for expertise location in organizational context,
the TMM Index [13]. They conducted a study which validates
the TMM Index in an experimental as well as a longitudinal
ﬁeld study. The TMM Index is a measure of team mental
models based on the location of team member expertise. The
measure integrates the quality of meta-knowledge and team
consensus of within-team expertise. TMM is a subjective mea-
sure of individuals’ perceived knowledge of team members’
expertise (e.g., ”I have a good ’map’ of other team members’
talents and skills”). Replicating the TMM Index experiment in
an ABM has two beneﬁts. On the one hand, with the use of
ABM it is possible to analyze different experiment setups and
examine if the same effects hold for e.g., larger group sizes.
Therefore a plausible agent-based model is needed. On the
other hand, teamwork plays an important role in ABM. In an
ABM a team of agents can work together solve a particular task
cooperatively. To that end, agents need a mutual beliefs of the
skills and knowledge of other agents to coordinate successfully
and efﬁciently [14]. Especially in recognizing the need to solve
a task cooperatively, meta-knowledge can enhance the process.
A measurement which indicates the quality of meta-knowledge
is desirable for agent-based systems.
The original experimental study (N = 120, 40 teams)
was conducted with university students. Participants worked
in three-person teams to solve a decision-making task. The
experimental task described the setting of working at a com-
pany that analyses weather data to evaluate travel routes and
give recommendations to customers. Each participant received
speciﬁc customer requests regarding three possible travel
routes. The routes each consisted of three different stations.
Each station was described by three weather properties (e.g.
wind, temperature, and rain). Each participant was assigned
expertise for one speciﬁc weather property, e.g. expertise for
temperature, which included all information concerning the
temperature at a given location. Expertise information was
only visible to the assigned expert. Information concerning the
other participants’ expertise was presented as missing values.
In order to create interdependence between team members and
to process a customer request (e.g. customer 1: warm, dry, calm
weather for swimming), all team members had to exchange
information. Team members had to solve the tasks individually
while cooperating concerning information exchange, which
was crucial for processing a customer request. The experiment
consisted of three phases. In the ﬁrst phase each partici-
pant received individualized information about their expertise,
weather properties, as well as the customer requests. In the
second phase participants communicated via email to get the
necessary information from other experts. In the third phase,
each participant chose the best travel route for their customer
request. In Figure 1, the travel routes which where used in the
experiment are shown.
Figure 1. Travel Routes in Group Experiment
In order to validate the TMM Index, meta-knowledge and
consensus about meta-knowledge was manipulated in a 2x2
design (”high” vs. ”low” meta-knowledge). This resulted in
(i) teams with high quality of meta-knowledge and high team
consensus (all participants knew which team member received
information and knew that all other team member received
this information); (ii) teams with low meta-knowledge and
high consensus (no participants received information about
the expertise location but all participants knew that no one
else received this information); (iii) teams with high meta-
knowledge and low consensus (two participants received in-
formation about the expertise of other team members but no
one was given information about what other team members
were/were not told); and (iv) teams with low meta knowledge
and high consensus (only one of the three team members
received expertise information but all were informed that only
one member received said information and were told which
member).
The TMM index used in the laboratory experiment is based
on a 7-point Likert-scale and is calculated using four items
(”I have a good ’map’ of other team members’ talents and
skills,” ”I know which team members have expertise in speciﬁc
areas,” ”I know what task- related skills and knowledge each
team member possesses,” and ”I know who on the team has
specialized skills and knowledge that are relevant to me”). The
calculation of the TMM Index is deﬁned as follows:
TMM Index =
Pn
i=1 xi
n
−
rPn
i=1 (xi − µ)2
n
(1)
The index results in the subtraction of the mean value of the
survey and the standard deviation and therefore also deﬁned
on a 7-point scale. The main hypothesis for the studies is to
show if the TMM Index differentiates between teams with high
knowledge and high consensus versus high knowledge and
low consensus as well as between teams with low knowledge
and low consensus and low knowledge and high consensus.
The results of the experiment show that the hypothesis can
7
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-648-4
HUSO 2018 : The Fourth International Conference on Human and Social Analytics

be veriﬁed and the index is sensitive to meta-knowledge and
consensus (for more detailed results see Section V).
IV.
AGENT-BASED MODEL FOR SIMULATING
META-KNOWLEDGE DISTRIBUTION
In theory, subjective measurements from participants in
a laboratory experiments are based on theirs observations
from underlying processes or their knowledge combined with
emotions. The original study implemented the TMM as a
subjective measure, however in ABM, the TMM Index has
to be based on objective knowledge or behavior (e.g., how
agents communicate between each other). For reasons of
simpliﬁcation our ﬁrst simulation only focused on the effects
of high vs. low meta-knowledge on TMM Index. Thus, the
goal of the ABM was to investigate whether the (objective)
TMM Index in the simulation shows a similar sensitivity to
manipulated changes of meta-knowledge as the perception
based measure in the original experiment. The agent-based
model is structured according to the setup of the laboratory
experiment. In Figure 2, the components of the ABM are
shown. On the left hand side, the input parameters for model
conﬁguration are shown. These parameters are based on the
laboratory experiment and its scalability potential. On the right
hand side, the output parameters, which are used to measure
the TMM Index, are shown. In the center, the agent behavior
is depicted which is modeled like the participants’ behavior
from the experiment. The agent’s decision-making process is
described in the next paragraphs detailedly.
Figure 2. Agent-Based Model for Expertise Location
One major disadvantage of laboratory experiments is their
limited scalability due to e.g. personnel or ﬁnancial limitations.
Therefore, the input parameters of the model are the number of
team members, number of routes, number of locations, as well
as actual meta-knowledge to analyze the results for larger team
conﬁgurations. The output of the simulation is the TMM Index.
The agent-based model itself consists of reactive agents with
a representation of meta-knowledge. The knowledge about the
expertise location for an agent is deﬁned as follows for ∀w ∈
Weatherproperty :
Expertise(w) →
Agent a;
a is an Expert for w
∅;
guessing an Expert
(2)
For every team member each agent knows if they are
experts for a particular kind of weather data. This representa-
tion determines if an agent has high or low meta-knowledge.
In a case of high meta-knowledge, individual agents know
exactly which team member holds what kind of weather
information. In case of low meta-knowledge, an agent has no
information concerning the expertise of other team members.
Besides meta-knowledge, each agent has knowledge about
weather conditions for different locations on travel routes.
The following example in 3 shows the knowledge for a travel
route consisting of three different location and each location
is describes by three different weather properties.
Location × Weatherdata =
"−1
5
−1
−1
2
−1
−1
6
−1
#
(3)
The example shows the knowledge of an expert for the
second weather property, e.g., rain. The evaluation of a weather
property for locations is represented by a value from 1 to 9
which describes the suitability of this location for a speciﬁc
customer request. A higher value shows a higher similarity. A
value of -1 denotes missing information. The overall evaluation
value for a route is the sum of the single ratings of the weather
properties. L denotes the set of Locations and W denotes the
set of weather properties:
RouteV alue(R) =
L
X
i=1
W
X
j=1
rij
(4)
The route with the highest overall rating is proposed to the
customer. In order to calculate the route ratings for a customer
request each agent can perform different actions which are
modeled in the process from Figure 3:
Figure 3. Process of Requesting Expert Information
The process is iterative and ends if an agent has no missing
weather data left or has requested this information. In the
second step the meta-knowledge determines which expert is
requested. If an agent has no information about the expertise
location, i.e., has low meta-knowledge it messages every team
member. In Figure 4 the processing of messaging is shown.
As communication protocol between agents FIPA ACL is
used. In order to manage the information exchange agents
use FIPA performatives. In this scenario ”request”, ”inform”
and ”refuse” are applied. Each agent is able to get requests
on missing information as well as informs on requested
information. In case of a refuse, which means the requested
information cannot be provided no action is performed. If there
is no missing information left in the knowledge base, then each
agent calculates the best route for the customer request.
The main issue of measuring the Index is to transform
the subjective measure (4 item survey) into an objective
8
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-648-4
HUSO 2018 : The Fourth International Conference on Human and Social Analytics

Figure 4. Processing Messages in Agents
measurement. To that end, the individual expertise of an agent
is compared to the actual knowledge distribution to cover the 4
item measurement. Besides that, in the laboratory experiment,
the highest correlation of TMM Index was measured with the
coordination performance. Therefore, the TMM Index calcu-
lation in the simulation is based on the average of individual
versus actual expertise location distribution (equation 5).
Expertisea(w) →



1;
if indivual and actual
Expertise are equal
0
no match
(5)
The actual TMM Index in the simulation is calculated as
the difference of the mean average of the expertise matches
and their standard deviation multiplied by the coordination
performance. The coordination performance is represented as
the ratio of positive requests and total amount of messages. It
is used as an indicator for the performance of the underlying
teamwork process. In order to test the model’s plausibility to
represent the TMM Index, the next Section show results from
simulating the original experiment.
V.
RESULTS AND DISCUSSION
The simulation model is implemented in Java using the
Repast Simphony Framework. To test for comparability and
plausibility of the two methods, we compared simulation based
TMM Index scores in the two conditions (high vs. low meta-
knowledge) with TMM Index scores from the original study.
The results are described in Section V-A. Additionally, the
examination of the models behavior regarding the group size
is shown in Section V-B.
A. Psychological Experiment versus ABM Simulation
In order to compare the results from the original experiment
and the ABM, the simulation uses the same conﬁguration (3
routes, 3 agents, 3 locations per route as well as high / low
meta-knowledge). In case of a high meta-knowledge, each
agent knows exactly which other agents are experts for. In
case of low meta-knowledge there is no information about
the expertise location provided. To measure the equivalence of
simulation and experiment, the simulation output was trans-
formed to a 7-point rating. The simulation was executed 1000
times. Simulation results are displayed in Table I.
TABLE I. TMM INDEX RESULTS ORIGINAL EXPERIMENT AND
ABM SIMULATION
Experimental Manipulation
Low Meta-Knowledge
High Meta-Knowledge
Subjective TMM Index
in original experiment
3,05
4,36
TMM Index in
ABM Simulation
2,64
7,00
Similar to the laboratory experiment, the TMM Index
calculated in the simulation can distinguish between high
and low meta-knowledge, with a lower TMM Index in the
low meta-knowledge condition and a high Index in the high
meta-knowledge condition (low meta knowledge: experiment
3.05, simulation 2.64; high meta-knowledge: experiment 4.36,
simulation 7.00). In the high meta-knowledge condition, the
simulation results overestimated the TMM Index. This is due
to the direct measure of TMM used in the simulation. In
contrast, the experimental study used a subjective measure of
participants’ perceived TMM. Unsurprisingly, with complete
agreement of knowledge of expertise location in the ABM
simulation, high meta-knowledge teams reached the maximum
score of 7.00 on the TMM Index with zero standard deviation.
An overview of the results is shown in Figure 5. In the low
Figure 5. TMM Index Results of Simulation with Low Meta-Knowledge
meta-knowledge condition, the simulation based TMM Index
was lower compared to the experimental results. Thus, we
presume the different methods of measurement and abstraction
caused the described differences in TMM Index. Moreover, the
experiment measured a larger set of variables as the simulation
model. Concerning the deviation of the meta-knowledge distri-
bution, the computed results show similar characteristics (mean
deviation simulation 1,23 and Experiment 1,36). Consequently,
choosing an expert at random ﬁts the observations of the
empirical study for low meta-knowledge. Nevertheless, the
Figure 6. TMM Index results with increased Number of Group Members
9
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-648-4
HUSO 2018 : The Fourth International Conference on Human and Social Analytics

results show that it is possible to transfer the concept of meta-
knowledge measurement to agent-based systems. The simula-
tion results for low meta-knowledge showed high consistency
with the experiment and are plausible according to the concept
of TMM Index.
B. Increasing sample size with ABM
In further simulations, we investigated how the TMM Index
would change for low meta-knowledge teams with a varying
number of team members (team size of 3 to 7 individuals;
results are shown in Figure 6 ). Results show that with an in-
creasing number of team members, the TMM Index decreases.
In smaller teams, agents are more likely to select the correct
expert at random as the number of possible experts is also
smaller. Additionally, the ratio of coordination performance is
decreasing due to an increased number of team members.
In total, the result for scaled experiments are plausible
in the way that missing meta-knowledge has more negative
effects on a team’s performance than in smaller groups. The
scaled experiment results show that the effect of an objective
measurement of the TMM Index can distinct meta-knowledge
distribution among team members more clearly. Consequently,
measuring an objective TMM Index in the next laboratory
experiment could produce more accurate results.
VI.
CONCLUSION AND FUTURE WORK
Using agent-based modeling to complement psychological
research is a promising approach with up- and downsides.
This paper presents an ABM which models a psychological
experiment on meta-knowledge distribution in small groups.
The experiment is used to validate the TMM Index, which
is a subjective measurement to distinguish a team’s meta-
knowledge as well as its consensus about this meta-knowledge.
In order to calculate the TMM Index for an agent-based
simulation model it is objectiﬁed so that causal relationships
between a team’s performance and meta-knowledge could
be shown. The TMM Index calculated from the simulation
output is able to differentiate high and low meta-knowledge.
Additionally, the simulation experiments revealed that the
TMM Index can distinguish between high and low meta-
knowledge in teams with larger group sizes, too. Due to
the model’s assumptions and restrictions, the TMM Index
simulation results are plausible but overestimate the index.
The major challenge in replicating this experiment is the
transformation of the subjective TMM Index measurement to
an objective one which represents a shift in not only measuring
correlations but causality of teamwork processes and meta-
knowledge. The presented study in this paper is a ﬁrst step
towards the vision of simulating psychological experiments
realistically.
Future studies should aim to test if a more complex agent
architecture is able to reproduce the results more accurately
by integrating a consensus component in agents with different
manifestations of meta-knowledge in form of different or
mutual beliefs is promising. Moreover, ABM can not only
be used to support design, formalization and analysis of
experiments but complement conducting experiments. In a
hybrid experiments with a valid agent-based model humans can
interact with agents [15]. Such an approach could overcome
limitations of laboratory experiments especially in formalizing
measurements as well as modeling causality.
ACKNOWLEDGMENTS
The project AdaptPRO: Adaptive Process and Role design
in Organisations (TI 548/-1) is funded by the German Re-
search Foundation (DFG) within the Priority Program “Inten-
tional Forgetting in Organisations” (SPP 1921).
REFERENCES
[1]
I. J. Timm et al., “Towards multiagent-based simulation of knowl-
edge management in teams.” in Flexible knowledge practices and the
Digital Workplace (FKPDW). Workshop within the 9th Conference
on Professional Knowledge Management, M. Leyer, A. Richter, and
S. Vodanovich, Eds.
KIT: Karlsruhe, 2017, pp. 25–40.
[2]
E. R. Smith and F. R. Conrey, “Agent-based modeling: A new approach
for theory building in social psychology,” Personality and social psy-
chology review, vol. 11, no. 1, 2007, pp. 87–104.
[3]
Y. Ren, K. M. Carley, and L. Argote, “The contingent effects of
transactive memory: When is it more beneﬁcial to know what others
know?” Management Science, vol. 52, no. 5, 2006, pp. 671–682.
[4]
E. R. Smith and E. C. Collins, “Contextualizing person perception:
Distributed social cognition.” Psychological Review, vol. 116, no. 2,
2009, p. 343.
[5]
T. Balke and N. Gilbert, “How do agents make decisions? a survey,”
Journal of Artiﬁcial Societies and Social Simulation, vol. 17, no. 4,
2014, p. 13.
[6]
N. E. Williams, M. L. OBrien, and X. Yao, “Using survey data for agent-
based modeling: design and challenges in a model of armed conﬂict and
population change,” in Agent-Based Modelling in Population Studies.
Springer, 2017, pp. 159–184.
[7]
A. Kangur, W. Jager, R. Verbrugge, and M. Bockarjova, “An agent-
based model for diffusion of electric vehicles,” Journal of Environmental
Psychology, vol. 52, 2017, pp. 166–182.
[8]
J. C. Jackson, D. Rand, K. Lewis, M. I. Norton, and K. Gray, “Agent-
based modeling: A guide for social psychologists,” Social Psychological
and Personality Science, vol. 8, no. 4, 2017, pp. 387–395.
[9]
J. A. Grand, M. T. Braun, G. Kuljanin, S. W. Kozlowski, and G. T.
Chao, “The dynamics of team cognition: A process-oriented theory of
knowledge emergence in teams.” Journal of Applied Psychology, vol.
101, no. 10, 2016, p. 1353.
[10]
S. W. Kozlowski and G. T. Chao, “The dynamics of emergence:
Cognition and cohesion in work teams,” Managerial and Decision
Economics, vol. 33, no. 5-6, 2012, pp. 335–354.
[11]
J. R. Austin, “Transactive memory in organizational groups: the effects
of content, consensus, specialization, and accuracy on group perfor-
mance.” Journal of applied psychology, vol. 88, no. 5, 2003, p. 866.
[12]
A. B. Hollingshead, “Distributed knowledge and transactive processes in
decision-making groups.” in Composition., ser. Research on managing
groups and teams, Vol. 1.
US: Elsevier Science/JAI Press, 1998, pp.
103–123.
[13]
T. Ellwart, U. Konradt, and O. Rack, “Team mental models of expertise
location: Validation of a ﬁeld survey measure,” Small Group Research,
vol. 45, no. 2, 2014, pp. 119–153.
[14]
M. J. Wooldridge, Reasoning about rational agents.
MIT press, 2000.
[15]
L. Reuter, J. O. Berndt, and I. J. Timm, “Challenges of simulating
teamwork in organizational scenarios,” in 2017 Winter Simulation
Conference (WSC), Dec 2017, pp. 4542–4543.
10
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-648-4
HUSO 2018 : The Fourth International Conference on Human and Social Analytics

