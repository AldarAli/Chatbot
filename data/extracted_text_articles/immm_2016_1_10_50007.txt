Semi-Supervised Learning in the Framework of Data Multiple 1-D Representation
Jianzhong Wang
College of Sciences
Sam Houston State University
Huntsville, Texas 77341–2206, USA
e-mail: jzwang@shsu.edu
Abstract—The paper develops 1D-based ensemble method for
semi-supervised learning (SSL). The method integrates the clas-
siﬁer based on data 1-D representations and label boosting in a
serial ensemble. In each stage, the data set is ﬁrst represented by
several 1-D stacks, which preserve the local similarity between
data samples. Then, a 1-D ensemble labeler (1DEL) is constructed
and used to create a newborn labeled subset from the unlabeled
set. United with the subset, the original labeled is boosted for
the next learning stage. The boosting process is repeated till the
updated labeled set reaches a certain size. Finally, a 1DEL is
applied again to build the classiﬁer. The validity and effectiveness
of the method are conﬁrmed by experiments. Comparing to
several other popular SSL methods, the results of the proposed
method are very promising.
Keywords–Data 1-D representation; regularization; label boost-
ing; ensemble; semi-supervised learning.
I.
INTRODUCTION
In this paper, we introduce a novel ensemble method for
SSL based on data 1-D representation. In SSL, the essential
problem is data binary classiﬁcation, which can be brieﬂy
described as follows: Assume that the samples (or members,
points) of a given data set X = {⃗xi}n
i=1 ⊂ Rm belong
to two classes A and B, labeled by 1 and −1, respectively.
Denote by yj the label of the sample ⃗xj, where yj
∈
{1, −1}, 1 ≤ j ≤ n. In a SSL problem, X is divided into
two disjoint subsets: X = Xℓ ∪ Xu, Xℓ ∩ Xu = ∅, where
the members in Xℓ = {⃗x1, ⃗x2, · · · , ⃗xn0} have known labels
Yℓ = {y1, y2, · · · , yn0}, while the labels for the members in
Xu = {⃗xn0+1, ⃗xn0+2, · · · , ⃗xn} are unknown. We often call a
function f : X → {1, −1} a classiﬁer (or labeler) on X. The
classiﬁcation error is measured by the misclassiﬁed number:
E(f) = |{⃗x∈ X| f(⃗xi) ̸= yi 1 ≤ i ≤ n}| ,
where |S| denotes the cardinality of a set S. Then, the quality
of a classiﬁer is measured by the error rate E(f)/|X|. The
task of SSL is to ﬁnd a classiﬁer f with the error rate as small
as possible.
The monograph [1] and the survey paper [2] gave a
comprehensive review of various SSL methods, among which
the popular ones are based on kernel technique such as
transductive support vector machines, manifold regularization,
and other graph-based methods [3] [4]. In these methods, using
kernel trick, people construct a kernel function to map original
samples onto a reproducing kernel Hilbert space (RKHS) [5],
where the non-linear decision boundary in the raw data space
becomes nearly linear. Thus, people can construct classiﬁers
in the RKHS using regularization methods. The success of
a kernel-based method strongly depends on the exploration
of data structure by kernels. However, it is often difﬁcult to
design suitable kernels, which precisely explore the feature
spaces. Recently, researchers have developed new SSL models,
which construct classiﬁers without adopting kernel technique,
for instance, the data-tree based method [6] [7] constructs the
classiﬁer based on the data multi-layer structure.
In all of the models above, a single classiﬁer is employed
to label unlabeled points. However, when a data set has a
complicate intrinsic structure and high-dimensionality, a single
classiﬁer usually cannot complete the task satisfactorily. The
proposed method takes the idea of ensemble methodology in
the multiple classiﬁer systems (MCSs) [8]: It build a ﬁnal
classiﬁer by integrating multiple pre-classiﬁers. Since MCSs
perform information fusion at different levels, they overcome
the limitations of the traditional approaches [9]–[11].
The novelty of the introduced ensemble SSL method is the
following: It adopts the framework of data 1-D representation,
in which the data set is represented by several different 1-D
sequences, then a classiﬁer is constructed as an ensemble of
pre-classiﬁers built on these sequences. Here, we choose 1-D
models because 1-D decision boundary is a set of points on a
line, which has the simplest topological structure. As a result,
the pre-classiﬁers can be easily constructed by classical 1-D
regularization methods without using kernel trick or data trees.
Furthermore, the simplicity of 1-D models makes the algorithm
for building the ﬁnal classiﬁer relatively reliable and stable. We
new describe the architecture and technological process of our
method in the following.
1)
The data set X is ﬁrst mapped to several 1-D sets
{T i}k
i=1, which preserve the local similarity of mem-
bers in X. Correspondingly, the couple {Xℓ, Xu} is
mapped to {T i
ℓ, T i
u} for each 1-D set T i.
2)
A pre-classiﬁer gi on X is constructed based on T i
by a 1-D regularization method. Then an ensemble
labeler g on X is assembled from {gi}k
i=1 to label
all members of X.
3)
A feasibly conﬁdent subset L ⊂ Xu is produced by
g. According to the class weights of the members of
L, a half of members in L is chosen into the newborn
labeled subset S. Then, the initial labeled set Xℓ is
boosted to Xnew
ℓ
= Xℓ ∪ S.
4)
The procedure above is repeated till the updated
labeled set Xnew
ℓ
reaches a certain size. Finally, the
classiﬁer f is obtained by applying the ensemble
labeler g on the newest couple {Xnew
ℓ
, Xnew
u
}.
Our strategy adopts Model-guided Instance Selection (MIS)
approach [9], but is slightly different from AdaBoost algorithm
[12] in the sense that AdaBoost updates the misclassiﬁed
weights on Xu, while our method updates the set Xu itself.
The paper is organized as follows: In Section II, we develop
the 1-D based ensemble SSL method. In Section III, we
1
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-477-0
IMMM 2016 : The Sixth International Conference on Advances in Information Mining and Management (includes DATASETS 2016)

demonstrate the validity of our method in two examples and
give the comparison of our results with other methods. The
conclusion is given in the last section.
II.
THE 1-D BASED ENSEMBLE SSL METHOD
In this section, we introduce the novel SSL method based
on data 1-D representation.
A. Data 1-D Representations
Assume that the data set X is initially arranged in a stack
x = [⃗x1, · · · , ⃗xn], where the ﬁrst n0 members are in Class A
and others are in Class B. Let d(⃗x, ⃗y) be a metric on X that
measures the dissimilarity between the points of X. Let π be an
index permutation of the index sequence [1, 2, · · · , n], which
induces a permutation Pπ on the initial stack x, yielding a
stack of X headed by ⃗xπ(1): xπ = Pπx = [⃗xπ(1), · · · , ⃗xπ(n)].
We deﬁne the set of all permutations of X headed by ⃗xℓ by
Pℓ = {Pπ;
π(1) = ℓ}.
According to [13], the shortest-path sorting of X headed by
⃗xℓ is the stack xπ that minimizes the path starting from ⃗xℓ
and though all points in X, i.e., xπ = Pπx, where Pπ is given
by
Pπ = arg min
P ∈Pℓ
n−1
X
j=1
d((Px)j, (Px)j+1).
(1)
Let the stack xπ be the shortest-path sorting of X headed by
⃗xℓ. Set
t1 = 0, tj+1 − tj =
d(⃗xπ(j), ⃗xπ(j+1))
Pn−1
k=1 d(⃗xπ(k), ⃗xπ(k+1))
.
(2)
Then, the stack t = [t1, · · · , tn] is called the 1-D (shortest-
path) representation of X headed by ⃗xℓ.
The problem (1) has NP computational complexity. A
greedy algorithm to ﬁnd an approximation of Pπ in (1) is
referred to [13]. Once, Pπ is found, the corresponding 1-D
representation is obtained by (2).
Denote by T the set of the components of t. The bijective
mapping h : T
= h(Xℓ) is called a 1-D (shortest-path)
embedding of X headed by ⃗xℓ, which also map the unlabeled
set Xu onto Tu = h(Xu) ⊂ T . Then, a classiﬁer on T induces
a classiﬁer on X. Since T is a 1-D set, its class decision
boundary is reduced to a discrete set in [0, 1].
B. The 1-D based ensemble labeler
Although the simplest topological structure of data 1-D
representation reduces the decision boundary to a discrete
set in [0, 1] points, a single 1-D representation cannot truly
preserve the data similarity because the sorting is a serial
process that makes earlier selected adjacent pairs are more
similar than the later selected ones. To overcome the drawback
of a single 1-D embedding, we employ the spinning technique
to build several 1-D representations. Based on each of them,
we ﬁrst construct a pre-classiﬁer, then assemble an ensemble
labeler from them. The following is the details.
Let ⃗h = [h1, · · · , hk] be a k-ple 1D-embedding and Pi be
the permutation operator on X corresponding to hi such that
the stack xπi = Pix is headed by a randomly selected point
⃗xπi(1). The embedding hi produces a 1-D representation of
X: ti = hi(xπi). For a function f on X, si = f ◦ h−1
i
is a
function on ti. We now represent a function f on X by its
vector form f = [f1, · · · , fn], fj = f(⃗xj), and a function s on
ti by the vector s = [si
1, · · · , si
n], si
j = s(ti
j).
Let T i
ℓ = hi(Xℓ) and T i
u = hi(Xu). Using a classical
regularization method, we construct a pre-classiﬁer gi for
X based on the couple {T i
ℓ, T i
u}. For instance, denote by
C1[0, 1] the space of smooth functions on [0, 1] and by
Dsj = (s(ti
j+1) − s(ti
j))/(ti
j+1 − ti
j) the difference quotient
of s ∈ C1[0, 1] on the stack ti at ti
j. Let qi be the solution of
the following constrained minimization problem:
qi = arg min
s∈C1[0,1]
1
n0
n0
X
j=1

and d(⃗x, X−
ℓ ) = min⃗y∈X−
ℓ d(⃗x, ⃗y). We now associate ⃗x with
the class weight
w(⃗x) =
d(⃗x, X−
ℓ )
d(⃗x, X−
ℓ ) + d(⃗x, X+
ℓ ).
Finally, let the set S+ contain the half of members of L+ with
the greatest class weights and S− contain the half of members
in L− with the smallest class weights. We call the operator
S : S(L) = S a newborn labeled subset selector and call
the composition M= S ◦ G a newborn labeled subset creator
because the newborn labeled subset S = M(Xu).
D. Construction of the ﬁnal classiﬁer
We now build the (ﬁnal) classiﬁer by a serial ensemble, in
which the labeled set is cumulatively boosted. Let the initial
labeled set be equipped with the index 0: X0
ℓ = Xℓ. Starting
from X0
ℓ , we apply the newborn labeled subset creator M1 to
create a newborn labeled set S1, which is united with X0
ℓ to
produce X1
ℓ = X0
ℓ ∪ S1. Repeating the procedure n times, the
labeled set will be cumulatively boosted to a labeled set Xn
ℓ :
X0
ℓ =⇒ X1
ℓ =⇒ · · · =⇒ Xn
ℓ .
We set a boosting-stop parameter p, 0 < p < 1. The process
will not be terminated until the labeled set Xn
ℓ reaches the
size |Xn
ℓ | ≥ p|X|. Finally, we apply 1DEL on the couple
{Xn
ℓ , Xn
u} to construct the ﬁnal classiﬁer f on X, which labels
each ⃗x ∈ X by sign f(⃗x).
III.
EXPERIMENTS
We use two benchmark databases of handwritten digits,
MNIST [21] and USPS [22] in the experiments to present
the validity and effectiveness of the proposed method. In the
literature of machine learning, MNIST is often used to test the
error rate of classiﬁers obtained by supervised learning. The
best result for the error rate up to 2012 was 0.23%, reported in
[14] by using the convolutional neural network technique. In
2013, the authors of [15] claimed to achieve 0.21% error rate
using DropConnect, which is based on regularization of neural
networks. Because in SSL no large training set is available for
producing classiﬁers, the error rates obtained by SSL methods
usually are much higher than the claimed error rates obtained
by supervised learning. Besides, the error rates of SSL are
strongly dependent the size of the initial label set Xℓ. In
general, the smaller the size of Xℓ, the higher the error rate.
Hence, it is unfair to compare the error rates obtained by SSL
methods to the above recorded ones.
In all of our experiments, the spin number 3 is used for
constructing 1DEL while 20 for building the ﬁnal classiﬁer,
and the boosting-stop parameter p is set to 0.7.
For comparison, we choose the same data setting as in [7]:
In MINST, for each of the digits {3, 4, 5, 7, 8}, 200 samples
were selected at random so that the cardinality of the data set
is |X| = 1000, where the digit 8 is assigned to Class B, and
others belong to Class A. In USPS, for each of the digits 0−9,
150 samples are selected at random so that |X| = 1500, where
the digits 2 and 5 are assigned to Class B, and others belong
to Class A. In all experiments, the initial labeled set X0 is
preset to 10 various sizes of 10, 20, · · · , 100, respectively, and
the labeled digits are distributed evenly on each chosen digit.
TABLE I. ERROR RATE OF THE PROPOSED 1-D BASED ENSEMBLE
SSL METHOD FOR 50 RANDOMLY SELECTED SUBSETS FROM
MNIST WITH |X| = 1000.
|X0|
10
20
30
40
50
60
70
80
90
100
Mean%
7.8
7.9
4.6
2.5
2.1
1.9
1.9
1.9
1.2
1.2
Min%
7.6
7.9
4.6
1.9
2.1
1.9
1.9
1.9
1.2
1.2
Max%
19.4
7.9
4.6
3.5
2.1
1.9
1.9
1.9
1.2
1.2
STD%
1.7
0
0
0.7
0
0
0
0
0
0
TABLE II. ERROR RATE OF THE PROPOSED 1-D BASED ENSEMBLE
SSL METHOD FOR 50 RANDOMLY SELECTED SUBSETS FROM
USPS WITH |X| = 1500.
|X0|
10
20
30
40
50
60
70
80
90
100
Mean%
3.3
2.1
1.5
1.5
1.3
1.4
1.4
1.4
1.4
1.2
Min%
2.0
1.3
1.5
1.5
1.3
1.4
1.4
1.4
1.4
1.2
Max%
16.8
2.9
1.7
1.5
1.3
1.4
1.4
1.4
1.4
1.2
STD%
1.99
0.8
0.02
0
0
0
0
0
0
0
Note that a vector ⃗x ∈ X is originally represented by a
c × c matrix [xi,j]c
i,j=1, where c = 20 for MNIST and c = 16
for USPS. To reduce the shift-variance, we deﬁne the 1-shift
distance between two digit images [16]:
d(⃗x, ⃗y) =
min
|i′−i|≤1
|j′−j|≤1
v
u
u
t
c−1
X
i=2
c−1
X
j=2
(xi,j − yi′,j′)2.
We ﬁrst run our algorithm on 50 subsets (with 1000 members)
randomly chosen from the MNIST database and show the
test results in Table I, where the ﬁrst row is the number of
samples in Xℓ, and the 2nd−5th rows are the mean, minimum,
maximum, and standard deviation of the error rates of the
50 tests, respectively. In the second experiment, we run our
algorithm for USPS is a similar way: 50 subsets with 1500
members are randomly chosen from USPS database. The test
results are shown in Table II, where the setting for the rows
is the same as in Table I. The Tables I and II show that the
standard deviations of the error rates are quite small, particular
when the known labeled members are more than 1%. This
indicates the high stability of the proposed SSL algorithm.
In Figure 1, we give the comparison of the average error
rates (of 50 tests) of our 1-D based ensemble method to
Laplacian Eigenmaps (Belkin & Niyogi, 2003 [3]), Laplacian
Regularization (Zhu et al., 2003 [17]), Laplacian Regulariza-
tion with Adaptive Threshold (Zhou and Belkin, 2011 [18]),
and Haar-Like Multiscale Wavelets on Data Trees (Gavish
et al., 2011 [7]) on the subsets randomly chosen from both
MNIST and USPS databases. The results show that our method
achieves competitive results comparing to other SSL methods.
We have also applied the proposed method on the real-
world applications, such as the classiﬁcation of hyperspectral
images [19] and the face recognition [20]. In these experi-
ments, we have even adopted a much simpler label boosting
method: Choosing the newborn labeled subset at random. The
obtained results are still very promising and superior over
many other popular methods. It is also worth to point out
that the method is not very sensitive to the parameters. For
instance, in our experiments, if spinning numbers are set to
3–5, and the boosting-stop parameter is set in the range of
3
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-477-0
IMMM 2016 : The Sixth International Conference on Advances in Information Mining and Management (includes DATASETS 2016)

10
20
30
40
50
60
70
80
90
100
0
5
10
15
20
25
# of labeled points (out of 1000)
Test Error (%)
Method Comparison: Classification errors for MNIST benchmark
 
 
Laplacian Eigenmaps
Laplacian Reg.
Adaptive Threshold
Wavelets On Data Tree
1DEI
10
20
30
40
50
60
70
80
90
100
0
5
10
15
20
25
30
# of labeled points (out of 1500)
Test Error (%) 
Method Comparison: Classification errors for USPS benchmark
 
 
Laplacian Eigenmaps
Laplacian Reg.
Adaptive Threshold
Wavelets On Data Tree
1DEI
Figure 1. RESULT COMPARISON WITH DIFFERENT SSL MODELS.
0.6–0.8, the results are similar. The detailed discussion on the
parameter tuning can be found [19] [20].
IV.
CONCLUSION
We proposed a new ensemble method for SSL based
on data 1-D representations, which enable us to construct
ensemble classiﬁers assembled from several pre-classiﬁers for
the same data set using classical 1-D regularization tech-
nique. Furthermore, a label boosting technique is applied for
robustly enlarging the labeled set to a certain size so that
the ﬁnal classiﬁer is built based on the boosted labeled set.
The experiments show that the performance of the proposed
method is superior to many popular methods in SSL. The
new method also exhibits a clear advantage for learning the
classiﬁer when only a small labeled set is given. Because the
method is independent of the data dimensionality, it can also
be applied to various types of data. Since the algorithms to
construct the classiﬁers in the proposed method only employ
1-D regularization technique, avoiding the complicate kernel
trick, they are simple and stable. It can be expected that the
created 1-D framework in this paper will be applied to the
development of more machine learning methods for different
purposes. In the future work, we will study how to accelerate
the sorting algorithm in 1-D embedding and consider to
integrate the data-driven wavelets with the proposed method.
ACKNOWLEDGMENT
This work is supported by SHSU-2015 ERG Grant no.
250711. The authors would like to thank the anonymous
reviewers for their highly insights and helpful suggestions.
REFERENCES
[1]
O. Chapelle, A. Zien, and B. Sch¨olkopf, Semi-supervised Learning.
MIT Press, 2006.
[2]
X. Zhu, “Semi-supervised learning literature survey,” University of
Wisconsin-Madison, Computer Sciences TR-1530, July 2008.
[3]
M. Belkin and P. Niyogi, “Using manifold structure for partially labeled
classiﬁcation,” Advances in Neural Information Processing Systems,
vol. 15, 2003, pp. 929–936.
[4]
T. Joachims, “Transductive learning via spectral graph partitioning,” in
Proceedings of the 20th International Conference on Machine Learning,
2003, pp. 290–297.
[5]
V. Vapnik, Statistical Learning Theory.
Wiley-Interscience, New York,
1998.
[6]
R. Coifman and M. Gavish, “Harmonic analysis of digital data bases,”
Applied and Numerical Harmonic Analysis. Special issue: Wavelets and
Multiscale Analysis, 2011, pp. 161–197.
[7]
M. Gavish, B. Nadler, and R. Coifman, “Multiscale wavelets on
trees, graphs and high dimensional data: Theory and applications to
semi supervised learning,” in Proceedings of the 27th International
Conference on Machine Learning, 2010, pp. 367–374.
[8]
T. K. Ho, J. J. Hull, and S. N. Srihari, “Decision Combination in
Multiple Classiﬁer Systems,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 16, no. 1, Jan. 1994, pp. 66–75.
[9]
L. Rokach, “Ensemble-based classiﬁers,” Artif. Intell. Rev., vol. 33,
2010, pp. 1–39.
[10]
M. Wozniak, M. Grana, and E. Corchado, “A survey of multiple
classiﬁer systems as hybrid systems,” Information Fussion, vol. 16,
2014, pp. 3–17.
[11]
Z-H. Zhou, J. Wu, and W. Tang, “Ensembling neural networks: Many
could be better than all,” Artif. Intell. vol. 137, 2002, pp. 239–263.
[12]
D. Z. Li, W. Wang, and F. Ismail, “A selective boosting technique for
pattern classiﬁcation,” Neurocomputing, vol. 156, 2015, pp. 186–192.
[13]
I. Ram, M. Elad, and I. Cohen, “Image processing using smooth
ordering of its patches,” IEEE Trans. on Image Processing, vol. 22,
no. 7, July 2013, pp. 2764–2774.
[14]
C. Dan, U. Meier, and J. Schmidhuber, “Multi-column deep neural
network for image classiﬁcation,” IEEE Conference on Computer Vision
and Pattern Recognition, 2012, pp. 3642–3649.
[15]
W. Li, M. Zeiler, S. Zhang, Y. LeCun, and R. Fergus, “Regularization
of neural network using dropconnect,” Journal of Machine Learning
Research, vol. 28, no. 3, 2013, pp. 1058–1066.
[16]
J. Z. Wang, “Semi-supervised learning using multiple one-dimensional
embedding based adaptive interpolation,” International Journal of
Wavelets, Multiresolution and Information Processing, vol. 14, no. 2,
2016, pp. 1640002: 1–11.
[17]
X. Zhu, Z. Ghahramani, and J. Lafferty, “Semi-supervised learning
using gaussian ﬁelds and harmonic functions,” in Proceedings of the
20th International Conference on Machine Learning, vol. 3, 2003, pp.
912–919.
[18]
X. Zhou and M. Belkin, “Semi-supervised learning by higher order
regularization,” in Proceedings of the 14th International Cofference on
Artiﬁcial Intelligence and Statistics, 2011, pp. 892–900.
[19]
H. Luo et al., “Hyperspectral image classiﬁcation based on spectral-
spatial 1-dimensional manifold,” IEEE Trans. Geosci. d Remote Sens.,
in press.
[20]
Y. Wang, Y. Y. Tang, L. Li, and J. Z. Wang, “Face recognition via col-
laborative representation based multiple one-dimensional embedding,”
International Journal of Wavelets, Multiresolution and Information
Processing, vol. 14, no. 2, 2016, pp. 1640003:1–15.
[21]
Y. LeCun, C. Cortes, and C. J. C. Burges, “THE MNIST DATABASE of
handwritten digits,” http://yann.lecun.com/exdb/mnist/, accepted March
17, 2016.
[22]
“USPS handwritten digit data,” http://www.gaussianprocess.org/gpml/data/,
accepted March 17, 2016.
4
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-477-0
IMMM 2016 : The Sixth International Conference on Advances in Information Mining and Management (includes DATASETS 2016)

