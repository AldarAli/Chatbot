STP-Net: Semi-Tensor Product Neural Network for Image Compressive Sensing 
 
Youhao Yu1,2,   Richard M. Dansereau1 
 
Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada1 
School of Information Engineering, Putian University, Fujian, China2 
e-mail: youhaoyu@sce.carleton.ca              e-mail: rdanse@sce.carleton.ca 
 
Abstractâ€”Semi-tensor product (STP) is developed into a neural 
network in this paper and applied to image compressive sensing 
(CS). Large matrix computation for fully connected layers 
results in a large number of weight coefficients that need long 
training times. Instead of using an MÃ—N measurement matrix, 
according to the theory of STP a smaller measurement matrix of 
size M/tÃ—N/t can be applied, where t is a shrinkage factor. STP 
only needs N/t elements of the original signal for one 
measurement and the measurement matrix is shrunk to 1/t2 that 
of traditional CS. The shrinkage factor t is adjustable. To 
demonstrate the effectiveness of the STP-based neural network, 
we apply it to image reconstruction. The goal is to sample and 
recover larger images, without partitioning into smaller blocks 
that introduces block artifacts, and provide good initial 
reconstruction for subsequent networks. 
Keywords-compressive sensing; convolutional neural network; 
semi-tensor product; image reconstruction. 
I. 
INTRODUCTION 
Compressive sensing (CS) has become a significant 
research field in analog information processing, image 
compression, machine learning and so on [1]â€“[3]. The 
fundamental issue in CS is to reconstruct a sparse signal from 
few measurements. Since natural images are intrinsically 
sparse in some domain, they can be restored efficiently from 
CS measurements. Sparse signal reconstruction is an inverse 
problem that can be solved by techniques like basis pursuit 
(BP) [4], matching pursuit (MP) [5], orthogonal matching 
pursuit (OMP) [6], approximate message passing (AMP) [7], 
and least absolute shrinkage and selection operator (LASSO) 
[8], to name a few, but these tend to be time-consuming. 
CS acquires signals that are sparse in a certain basis in a 
compressed form. The sparsifying basis and the measurement 
matrix should be incoherent [3]. According to CS theory, a 
high dimension sparse signal x is sampled by a measurement 
matrix Î¦ resulting in a low dimension measurement y as 
 
y = Î¦x 
(1) 
where x is an NÃ—1 vector, y is an MÃ—1 vector and Î¦ is an MÃ—N 
matrix (M<<N). The measurement matrix should satisfy the 
restricted isometry property (RIP) [9]. 
Each measurement yi is the linear combination of the 
elements in x through a row of Î¦ as 
 
ð‘¦ð‘– = ðœ™ð‘–,1ð‘¥1 + â‹¯ + ðœ™ð‘–,ð‘—ð‘¥ð‘— + â‹¯ + ðœ™ð‘–,ð‘ð‘¥ð‘ 
(2) 
where xj, yi, and ðœ™ð‘–,ð‘— are elements of x, y, and Î¦. In (2), all N 
elements in x are used to obtain one measurement yi, which 
causes large computational cost when x is long since the 
measurement matrix Î¦ will be large. 
There are numerous data reconstruction approaches for 
compressive sensing. In [10][11], STP is adopted and an 
iterative optimization approach used for image CS 
reconstruction. While the approach in [10][11] produces good 
results at a high measurement rate, it is time consuming, needs 
many iterations, and a wavelet transform is used before CS and 
an inverse wavelet transform after CS reconstruction. 
In this paper, we propose developing the Semi-Tensor 
product into a neural network (NN). Such a network uses 
fewer parameters to train and provides theoretical foundation 
for a layer to be designed. The proposed NN needs fewer 
layers and less training time for efficient measurement and a 
good initial reconstruction compared to others, performing 
better than other full convolutional NN systems. Given the 
efficiency, the developed NN is used for whole image CS 
reconstruction using no block partitioning. 
The rest of this paper is organized as follows. Section II is 
an overview of previously proposed CS reconstruction 
methods. Section III describes the measurement and the initial 
reconstruction of CS based on STP. Section IV gives a detailed 
description of STP-Net. Our experiments are set up to 
demonstrate the outstanding performance of STP-Net in 
Section V. Finally, we conclude with a discussion of our 
findings in Section VI. 
II. 
RELATED WORK 
Many NN algorithms have been studied for CS 
measurement reconstruction, such as [12]â€“[18]. Among them, 
[12] develops a good framework for sensing and recovering 
structured signals, but a few full connection layers make it less 
efficient. [13] and [14] use convolutional layers or residual 
blocks to refine the initial reconstruction of every image block. 
After that, they use block-matching and 3-D filtering (BM3D) 
[15] to remove block artifacts, which is an image denoising 
strategy based on an enhanced sparse representation in the 
transform domain. [16][17] give novel methods that measure 
an image using convolutional layers. However, our 
experiments show that when the sampling rate is 1% the 
reconstructed image is affected by block artifacts, especially 
at the edges of the image, even with a residual network 
(ResNet) after initial reconstruction as they did to improve 
performance. [18] introduces an interpretable optimization-
7
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-970-6
SIGNAL 2022 : The Seventh International Conference on Advances in Signal, Image and Video Processing

inspired deep network for image compressive sensing. They 
cast the iterative shrinkage-threshold algorithm (ISTA) into a 
deep network that produces good performance. 
To overcome the limitation of GPU memory, existing 
methods usually divide an image into blocks and vectorize the 
blocks before measurement. Then, the image is reconstructed 
block-by-block, making block artifacts inevitable [16] and 
requiring denoising to remove block artifacts. 
Our method is different from others since we process an 
image as a whole rather than block-by-block, which avoids the 
block artifacts because the structure information of an image 
is preserved. Normally, operating on the full image would be 
computationally costly but the proposed STP-Net helps reduce 
that computational cost. 
III. STP APPROACH FOR CS 
STP approach can be applied for CS measurement and its 
initial reconstruction of 1D and 2D signal. 
A. 
Measurement 
According to the theory of Semi-Tensor product [19], a 
smaller measurement matrix Î¦(ð‘¡)  can be obtained with 
dimensions ð‘€/ð‘¡ Ã— ð‘/ð‘¡. Here, t is a shrinkage factor which is 
a common divisor of M and N. (ð‘€, ð‘, ð‘¡, ð‘€/ð‘¡ and ð‘/ð‘¡ are all 
positive integers) [11]. Only ð‘/ð‘¡ elements of the signal are 
needed for one measurement. The dimension of the 
measurement matrix is shrunk by 1/t2 of that for traditional CS. 
Using the left product operator â‹‰ for STP, (1) is rewritten as 
ð‘¦ = Î¦(ð‘¡) â‹‰ ð‘¥.        
 
(3) 
For clarity, the signal ð‘¥ and its measurements ð‘¦ can be 
segmented into groups and every group only has ð‘¡ points. So, 
ð‘¥ is divided into ð‘/ð‘¡ fragments and ð‘¦ is divided into ð‘€/ð‘¡ 
fragments. Reshaping the vectors ð‘¥ âˆˆ â„ð‘Ã—1 and ð‘¦ âˆˆ â„ð‘€Ã—1 
into matrix form ð‘‹ âˆˆ â„ð‘¡Ã—ð‘
ð‘¡ and ð‘Œ âˆˆ â„ð‘¡Ã—ð‘€
ð‘¡ , we rewrite (3) to 
maintain column-wise order as 
ð‘Œ = ð‘‹ âˆ™ Î¦(ð‘¡)ð‘‡.      
 
  (4) 
Considering ð‘‹ as an image, this means an image can be 
sampled directly by matrix multiplication.  
For our work, ð‘¡ is set to the number of rows the image. 
Our method takes an image as a whole without dividing it into 
blocks or vectorizing the image. 
B. 
Initial Reconstruction 
Typically, the rows of the measurement matrix Î¦ are 
chosen to be orthonormal and the least-squares solution 
(minimum energy reconstruction) as the initial estimate for x 
[20]. Let Î¦ be an MÃ—N matrix and let y be a vector in RM. 
The least-squares solution of Î¦x = y is the solution of Î¦TÎ¦x 
= Î¦Ty [21]. If Î¦ has orthonormal rows, Î¦Î¦T is an MÃ—M 
identity 
matrix. 
Using 
Î¦ð‘‡ð‘¦  is 
a common initial 
reconstruction. 
STP maintains the above-mentioned property [11]. If 
Î¦(ð‘¡) is a matrix with orthonormal rows and ð‘¦ = Î¦(ð‘¡) â‹‰ ð‘¥, 
the least-squares solution for x is the solution of [Î¦(ð‘¡)]ð‘‡ â‹‰
ð‘¦ = [Î¦(ð‘¡)]ð‘‡ â‹‰ Î¦(ð‘¡) â‹‰ ð‘¥
. 
Since 
Î¦(ð‘¡) â‹‰ [Î¦(ð‘¡)]ð‘‡ =
Î¦(ð‘¡)[Î¦(ð‘¡)]ð‘‡ = ð¼ , ð‘¥Ìƒ = [Î¦(ð‘¡)]ð‘‡ â‹‰ ð‘¦  is an initial estimate. 
The result can be written in matrix form as ð‘‹Ìƒ = ð‘Œ â‹… Î¦(ð‘¡). 
C. 
2D Compressive Sensing 
According to compressive sensing theory [1]â€“[3], a signal 
that is sparse in a certain domain can be sampled at a rate less 
than the Nyquist sampling rate. In Fig. 1, (a) is an image with 
size 256 Ã— 256. Usually, a natural image is sparse in the 
frequency domain, as in Fig. 1(b) where a 2D discrete cosine 
transform (DCT) has been applied using matrix D. Most 
energy is concentrated on top-left corner. If measurement 
matrix Î¦(ð‘¡) = ð‘ƒ, where P is a random matrix, then the image 
measured by (4) produces Fig. 1(c) with smaller size. 
Since t is set to the number of rows, the measurement 
matrix has effectively measured each column separately. As 
shown in Fig. 1(d), the measurement results are still sparse 
with the right choice of sparsifying basis D; most energy 
concentrates in the top area. Following CS theory, the 
measurements can be sampled and compressed again using 
ð‘Œ = Î¦1(ð‘¡) âˆ™ ð‘‹ âˆ™ Î¦2(ð‘¡)ð‘‡.              (5) 
In Fig. 1, since the image is square, we suppose Î¦1(ð‘¡) =
Î¦2(ð‘¡) = ð‘ƒ, where P is a random matrix. Let us now map 
image ð‘‹ and its measurements ð‘Œ into vectors ð‘¥ and ð‘¦ by 
column ordering, it is equivalent to (1) when Î¦ = Î¦1(ð‘¡) âŠ—
Î¦2(ð‘¡) where âŠ— is the Kronecker product. In this case, the 
1D operation of (1) is expressed as the separable 2D operation 
that reduces the computational complexity [22]. 
It has been shown that a sparse matrix ð‘‹Ì‚ (i.e, ð‘‹Ì‚ = ð·ð‘‹ð·ð‘‡) 
can be recovered from its matrix sketching ð‘Œ = Î¦1 âˆ™ ð‘‹Ì‚ âˆ™ Î¦2
ð‘‡ 
[23][24][25]. Here, we assume ð‘‹  has size ð‘¡ Ã— ð‘¡  where 
ð‘¡2 = ð‘ . The dimension of ð‘Œ  is ð‘š Ã— ð‘š  with ð‘š = ð‘€/ð‘¡ 
and ð‘š â‰ª ð‘¡. Î¦1 and Î¦2 are two ð‘š Ã— ð‘¡ matrices that can 
be seen as measurement matrices. It is equivalent to ð‘¦ =
(Î¦1 âŠ— Î¦2) â‹… ð‘¥. The initial estimate of the image can then be 
ð‘‹Ìƒ = Î¦1
ð‘‡ âˆ™ ð‘Œ âˆ™ Î¦2. 
As shown in the dashed box of Fig. 2, an image is 
effectively measured and compressed twice and its initial 
reconstruction uses two corresponding steps. 
IV. STP-NET: NEURAL NETWORK LAYER 
Inspired by the flexibility of STP, we build a neural 
network layer to implement it. The sampling and initial 
recovery process of the proposed STP neural network (STP-
Net) for compressive sensing can be implemented by building 
layers of an NN in two ways. One is defining a custom deep 
learning layer with learnable parameters in forward and 
backward propagation [26], and the other is by means of 
ready-made 
convolutional 
layers, 
like 
in 
MATLAB, 
 
Figure 1.  Image measurements and sparsity: (a) original image, (b) 
1D sparsifying basis D applied along rows and columns, (c) 
measurement of (a) using random matrix P, (d) result showing 
measurement is still sparse, and (e) image sampled along rows and 
columns. Sampling rate = (81/256)2 ï‚» 10%. 
 
 
8
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-970-6
SIGNAL 2022 : The Seventh International Conference on Advances in Signal, Image and Video Processing

TensorFlow, or PyTorch, followed by a custom reshape layer. 
A. 
Defining a Custom Deep Learning Layer 
A custom STP layer has a forward pass, a backward pass, 
and learnable parameters including weights and biases. It can 
be used as a measurement matrix by setting the biases to zero 
and the learning rates to zero so that the set weights remain 
static. It also can be used in initial reconstruction through a 
least-squares like solution. 
Reconstruction of a signal is a regression problem, so the 
output layer is a regression layer. The loss function of the 
regression layer is the half-mean-square-error (HMSE) [27] 
 
ð¿ =
1
2ð‘† âˆ‘
âˆ‘
(ð‘¡ð‘–ð‘—âˆ’ð‘¦ð‘–ð‘—)2
ð‘…
ð‘…
ð‘—=1
ð‘†
ð‘–=1
 
(6) 
where ð‘† is the number of observations in a mini-batch, ð‘… is 
the number of responses, ð‘¡ð‘–ð‘— is the target output, and ð‘¦ð‘–ð‘— is 
the networkâ€™s prediction for the response variable 
corresponding to observation ð‘–. 
For a semi-tensor product layer, the derivative of the loss 
function with respect to the input data of the custom layer are 
 
ðœ•ð¿
ðœ•ð‘¥ð‘–,ð‘— = âˆ‘
ðœ•ð¿
ðœ•ð‘¦ð‘–,ð‘Ž ðœ™ð‘Ž,ð‘—
ð‘€/ð‘¡
ð‘Ž=1
 
(7) 
where ð‘– = 1, ï‚¼, ð‘¡, ð‘Ž = 1, ï‚¼, ð‘€/ð‘¡, and ð‘— = 1, ï‚¼, ð‘/ð‘¡. 
The derivatives of the loss with respect to the weights are  
 
ðœ•ð¿
ðœ•ðœ™ð‘Ž,ð‘— = âˆ‘
ðœ•ð¿
ðœ•ð‘¦ð‘–,ð‘Ž ð‘¥ð‘–,ð‘—
ð‘¡
ð‘–=1
 
(8) 
where ð‘– = 1, ï‚¼, ð‘¡, ð‘Ž = 1, ï‚¼, ð‘€/ð‘¡, and ð‘— = 1, ï‚¼, ð‘/ð‘¡. 
For better performance, it could have biases ðµ added to 
the STP layer. Since ð‘Œ = ð‘‹ âˆ™ Î¦(ð‘¡)ð‘‡ + ðµ, the derivatives of 
the loss with respect to the biases ðµ have the same size with 
output ð‘Œ and the values are 
 
ðœ•ð¿
ðœ•ðµð‘–ð‘— =
ðœ•ð¿
ðœ•ð‘¦ð‘–ð‘— 
(9) 
where ðµð‘–ð‘—  and ð‘¦ð‘–ð‘— are the elements of ðµ  and ð‘Œ  ( ð‘– =
1, ï‚¼, ð‘¡ and ð‘— = 1, ï‚¼, ð‘€/ð‘¡). 
Obtaining the derivative of the loss with respect to the 
measurement matrix and the biases, the learnable parameters 
are updated with 
 
 ðœ™ð‘Ž,ð‘
ð‘˜
= ðœ™ð‘Ž,ð‘
ð‘˜âˆ’1 âˆ’ ðœ‚
ðœ•ð¿
ðœ•ðœ™ð‘Ž,ð‘ 
(10) 
 
ðµð‘–ð‘—
ð‘˜ = ðµð‘–ð‘—
ð‘˜âˆ’1 âˆ’ ðœ‚
ðœ•ð¿
ðœ•ðµð‘–ð‘— 
(11) 
where ð‘˜ is the iteration number, ðœ‚ is the learning rate, ð‘Ž =
1, ï‚¼, ð‘€/ð‘¡, ð‘ = 1, ï‚¼, ð‘/ð‘¡, ð‘– = 1, ï‚¼, ð‘¡ and ð‘— = 1, ï‚¼, ð‘€/ð‘¡. 
The initial reconstruction would be ð‘‹Ìƒ = ð‘Œ âˆ™ Î¦(ð‘¡) + ðµ . 
The traditional compressive sensing measurement paradigm 
applies fixed linear measurement [12], which is easy to 
implement in practical applications. Biases are not added with 
the measurement results, but they are added during initial 
reconstruction to have better results.  
B. 
Ready-made Convolutional Layer 
The CS measurement process based on STP is similar to 
dilated convolution (also known as â€œÃ  trousâ€ convolution). 
STP can be implemented by means of a dilated convolutional 
layer with factor t used to increase the receptive field (the area 
of the input signal which the layer can detect) of the layer 
without increasing the number of parameters and computation 
[28]. Since the number of rows of the measurement matrix 
correspond to the number of neurons in the convolutional layer 
and every filter produces one channel output, this layer should 
be followed by a reshaping, which makes the measurements in 
the same channel. 
V. 
EXPERIMENTS 
In this section, we conduct a series of experiments to test 
the measurement and reconstruction performance of STP-Net. 
A. 
Implementation Details 
The experiments are conducted on MATLAB R2019a. The 
computer is equipped with Intel i7-8700K, GeForce GTX 
1080 CPU with frequency of 3.7 GHz and 16 GB RAM. 
Natural images from the ILSVRC2014 ImageNet dataset are 
adopted. Here, 20k images are chosen: 14k (70%) for training, 
3k (15%) for validation, and 3k (15%) for testing. We 
extracted the central 256Ã—256 part of each image and 
converted them to 8-bit grayscale. Training used stochastic 
gradient descent with momentum, minibatch size of 64, 
maximum epoch of 40, learning rate of 2e-03, drop factor of 
0.10, and drop period of 15. For comparison, we also utilize 
the widely used benchmark dataset Set11 [13] during testing. 
Data In 
STP-Mea1 
STP-Mea2 
STP-Rec1 
STP-Rec2 
U-Net 
Data Out 
256Ã—256       256Ã—26        26Ã—26        256Ã—26       256Ã—256      256Ã—256 
STP-Net 
Figure 2.  STP-Net connected with U-Net for deblurring (Sampling rate:1%). 
9
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-970-6
SIGNAL 2022 : The Seventh International Conference on Advances in Signal, Image and Video Processing

B. 
Measurement Matrix Based on STP 
Every image has 65536 pixels. A sampling rate of 1% will 
produce 655 measurements. The size of such a measurement 
matrix is 655Ã—65536, which needs more than 300 MB of 
memory to store the measurement matrix (with double 
precision matrix elements). According to the STP method, if 
the parameter t is 256, then a measurement matrix with size 
equal to ceiling(256Ã—1%)Ã—256 can be used that satisfies RIP 
[9]. It needs 6 kB of memory and is much smaller than the full 
measurement matrix. Too small of a measurement matrix 
leads to excessive information loss, so we implement CS 
measurement in two steps. As mentioned in Section III, we can 
use two 1D projections for the 2D image, which can be done 
by applying STP twice. For a target of 1%, one option is that 
the first pass compresses the signal by 10% and the second 
pass compresses by another 10%. The size of measurement 
matrix would be ceiling(256Ã—10%)Ã—256 with t equal to 256. 
C. 
Proposed Structure 
Fig. 2 shows the structure of the neural network. STP-Net 
samples the image and provides initial reconstruction that can 
be passed to subsequent networks. U-Net [29] has proved to 
work well with semantic segmentation, which motivates us to 
apply it for reconstruction as a deblurring step. To set up an 
image regression network, we remove the last two layers (soft 
max and segmentation layers) of a U-Net with an encoder-
decoder depth of 3 and add a regression layer. In Fig. 4, STP-
Net is connected with ISTA-Net and STP layers are used 
repeatedly as measurement and reconstruction for each 
iteration. Our experiments adopt five ISTA iterations (phases) 
and every convolutional layer uses 16 kernels.  
D. 
Experimental Results 
For the sampling process, we applied different sampling 
rate combinations (20%+5%, 10%+10% and 5%+20%) in the 
two measurement layers to reach total measurement rate of 1%. 
The mean peak signal-to-noise ratio (PSNR) of the 3000 test 
images with these sampling rate combinations are 20.55, 21.50 
and 20.64 dB, respectively, without U-Net. It seems that the 
square root (10%+10%) of the total measurement rate could 
be a better choice. Tables I to III show that the proposed 
method has better performance than other methods and has 
higher PSNR and structural similarity measure (SSIM). 
From Fig. 3, we see that other methods have block artifacts. 
For DR2-Net [14], the image is measured block-by-block and 
reconstructed with 4 residual blocks. Then, they use BM3D 
[15] to remove the artifacts caused by block-wise processing. 
We modified the process by composing an intermediate 
reconstructed image with their initial reconstruction and then 
using the 4 residual blocks to remove block artifacts. It has 
better performance than their original method even without 
 
Figure 3.  Reconstruction results for parrot and pepper from noiseless CS measurements at measurement rate of 1%. It is evident that STP based 
method restores more visually appealing images than the competitors. 
Parrot
Original
Pepper
Original
Modi-DR2
SSIM:0.5197, PSNR:19.18dB
FCMN
SSIM:0.6494, PSNR:20.12dB
FC-Res
SSIM:0.6537, PSNR:21.42dB
STP
SSIM:0.7145, PSNR:22.17dB
STP-Unet
SSIM:0.7512, PSNR:22.96dB
SSIM:0.4059, PSNR:17.24dB
SSIM:0.5206, PSNR:19.07dB
SSIM:0.5344, PSNR:19.45dB
SSIM:0.5540, PSNR:20.48dB
SSIM:0.6242, PSNR:21.28dB
Output0                   0   Output1                                  0  Outputk 
  
ð¹1 
ð¹Ìƒ1 
ð‘†ð‘œð‘“ð‘¡(âˆ™, ðœƒ1) 
âˆ™âˆ™âˆ™ 
  
ð¹k 
ð¹Ìƒk 
ð‘†ð‘œð‘“ð‘¡(âˆ™, ðœƒð‘˜) 
  
âˆ™âˆ™âˆ™ 
ð‘‹    ð‘Œ    ð‘‹0   ð‘Œ0   ð‘…1                ð‘‹1            ð‘‹ð‘˜âˆ’1  ð‘Œð‘˜âˆ’1  ð‘…ð‘˜                ð‘‹ð‘˜ 
1
st Phase                                           k-th Phase 
âˆ’ 
âˆ’ 
âˆ’ 
âˆ’ 
STP   STP      STP   STP                                                         STP   STP 
STP-Net 
STP-Net 
STP-Net 
ISTA-Net 
Figure 4.  Connect STP-Net with ISTA-Net and use STP layers for measurement and reconstruction in every phase. 
10
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-970-6
SIGNAL 2022 : The Seventh International Conference on Advances in Signal, Image and Video Processing

BM3D. Full convolutional measurement network (FCMN) has 
block artifacts too, especially at the four edges of a 
reconstructed image. We apply one residual block after it as 
the authors did in [16]. The block artifacts are alleviated, but 
the four corners of restored image are darker. From Tables I to 
IV, it shows that STP-Net provides an attractive initial 
reconstruction quality for another network. Table III shows 
that STP-Net works well in less time. In Table IV, ISTA-Net 
and ISTA-Net+ adopt nine ISTA iterations with every 
convolutional layer having 32 kernels. STP-Net provides 
ISTA-Net 
with 
information-rich 
measurements 
and 
reasonable initial reconstruction, that enable it to simplify its 
structure with fewer ISTA iterations and kernels to have better 
performance. 
VI. CONCLUSION 
We have presented an STP-based neural network to the 
problem of CS image reconstruction. The measurement and 
initial reconstruction process are efficiently implemented 
through STP without dividing the image into blocks and 
vectorizing. At different measurement rates, our algorithm 
yields superior quality reconstructions than other methods. 
The method does not have block artifacts that many people try 
to solve. It makes the sampling process convenient and 
provides good initial reconstruction for subsequent network, 
such as U-Net or ISTA-Net. In future work, we are going to 
apply it in deep equilibrium architecture to develop an efficient, 
high performance fixed-point iteration layer [30]. 
TABLE I.  PSNR VALUES IN DB ON SET11 WITH DIFFERENT ALGORITHMS AT 1% MEASUREMENT RATE 
Image Name 
ReconNet 
+BM3D [13] 
DR2-Net 
[14] 
DR2+BM3D 
[14] 
Modified 
DR2-Net [14] 
FCMN 
[16] 
FC-Res 
[16] 
STP-Net 
STP-UNet 
Barbara 
19.08 
18.65 
19.10 
19.02 
20.38 
20.97 
21.83 
22.10 
Boat 
18.83 
18.67 
18.95 
18.82 
19.96 
20.57 
21.46 
22.23 
Cameraman 
17.49 
17.08 
17.34 
17.72 
19.16 
19.68 
20.14 
21.25 
Fingerprint 
14.88 
14.73 
14.95 
14.92 
15.56 
15.83 
16.16 
16.16 
Flintstones 
14.08 
14.01 
14.18 
13.29 
14.46 
14.77 
15.28 
15.37 
Foreman 
20.33 
20.59 
21.08 
22.54 
21.08 
23.72 
27.15 
27.00 
House 
19.52 
19.61 
19.99 
20.61 
20.93 
22.38 
23.16 
24.47 
Lena 
18.05 
17.97 
18.40 
18.51 
20.49 
21.15 
21.95 
22.72 
Monarch 
15.49 
15.33 
15.50 
15.52 
17.20 
17.58 
18.28 
18.79 
Parrot 
18.30 
18.01 
18.41 
19.18 
20.12 
21.42 
22.17 
22.96 
Pepper 
16.96 
16.90 
17.11 
17.24 
19.07 
19.45 
20.48 
21.28 
(For ReconNet, we use the results reported in [13]. For DR2-Net and DR2+BM3D, we use the results reported in [14]. For the other algorithms, the 
experiments use MATLAB with networks trained from the same dataset with the same images.) 
 
TABLE II.  SSIM VALUE FOR 11 EXTRA IMAGES 
Image Name 
ReconNet [13] 
Modified DR2-
Net [14] 
FCMN [16] 
FC-Res [16] 
STP-Net 
STP-UNet 
Barbara 
0.3730 
0.3578 
0.4555 
0.4575 
0.5024 
0.5271 
Boat 
0.4140 
0.3838 
0.4729 
0.4771 
0.4950 
0.5587 
Cameraman 
0.4517 
0.4391 
0.4998 
0.5389 
0.5503 
0.6565 
Fingerprint 
0.1641 
0.0708 
0.0853 
0.0858 
0.0884 
0.0886 
Flintstones 
0.2733 
0.1789 
0.2386 
0.2429 
0.2580 
0.2871 
Foreman 
0.5647 
0.6078 
0.6680 
0.6849 
0.7536 
0.7869 
House 
0.5278 
0.5282 
0.5809 
0.5948 
0.6291 
0.7056 
Lena 
0.4418 
0.4344 
0.5364 
0.5489 
0.5765 
0.6324 
Monarch 
0.3802 
0.3427 
0.4683 
0.4816 
0.5003 
0.5578 
Parrot 
0.5329 
0.5197 
0.6494 
0.6537 
0.7145 
0.7512 
Pepper 
0.4002 
0.4059 
0.5206 
0.5344 
0.5540 
0.6242 
Mean SSIM 
0.4112 
0.3881 
0.4705 
0.4819 
0.5111 
0.5615 
(For ReconNet, we calculate the values of SSIM from the images the authors provide.) 
 
TABLE III.  RESULTS OF 3000 TEST IMAGES 
Evaluation index  Modified DR2-Net [14] 
FCMN [16] 
FC-Res [16] 
STP-Net 
STP-UNet 
Mean SSIM 
0.3696 
0.4347 
0.4563 
0.4911 
0.5301 
Mean PSNR 
18.72 
19.26 
20.45 
21.50 
22.06 
Elapsed Time(s) 
56.62 
10.70 
22.02 
9.36 
41.33 
(The number in the table are the mean of 10 times experiments.) 
11
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-970-6
SIGNAL 2022 : The Seventh International Conference on Advances in Signal, Image and Video Processing

TABLE IV.  AVERAGE PSNR (DB) PERFORMANCE COMPARISONS ON SET11 WITH DIFFERENT CS RATIOS 
Sampling 
rate 
ReconNet 
+BM3D 
[13] 
DR2-Net 
[14] 
DR2+BM3D 
[14] 
FCMN 
[16] 
FC-Res 
[16] 
ISTA-Net 
[18] 
ISTA-
Net+ 
[18] 
STP-Net 
STP-
ISTA-Net 
1% 
17.55 
17.44 
17.73 
18.95 
19.77 
17.30 
17.34 
20.65 
21.30 
4% 
20.44 
20.80 
21.29 
23.14 
24.22 
21.23 
21.31 
23.39 
24.92 
10% 
23.23 
24.32 
24.71 
25.36 
27.30 
25.80 
26.64 
26.02 
28.65 
25% 
25.92 
28.66 
29.06 
28.69 
31.15 
31.53 
32.57 
30.06 
33.54 
 (The best performance is labeled in bold.) 
 
ACKNOWLEDGEMENT 
This research was partially funded through a grant from 
the Natural Sciences and Engineering Research Council 
(NSERC) of Canada. 
REFERENCES 
[1] E. J. Candes and J. K. Romberg, â€œSignal recovery from random 
projections,â€ Comput. Imaging III, vol. 5674, pp. 76â€“86, 2005. 
[2] R. G. Baraniuk, V. Cevher, and M. F. Duarte, â€œModel-based 
compressive sensing,â€ IEEE Trans. Inf. Theory, vol. 56, no. 4, 
pp. 1982â€“2001, 2010. 
[3] D. L. Donoho, â€œCompressed sensing,â€ IEEE Trans. Inf. Theory, 
vol. 52, no. 4, pp. 1289â€“1306, 2006. 
[4] S. S. Chen, D. L. Donoho, and M. A. Saunders, â€œAtomic 
decomposition by basis pursuit,â€ SIAM, vol. 43, pp. 129â€“159, 
2001. 
[5] S. G. Mallat and Z. Zhang, â€œMatching pursuits with time-
frequency dictionaries,â€ IEEE Trans. Signal Process., vol. 41, 
no. 12, pp. 3397â€“3415, 1993. 
[6] Y. C. Pati, R. Rezaiifar, and P. S. Krishnaprasad, â€œOrthogonal 
matching pursuit: recursive function approximation with 
applications to wavelet decomposition,â€ Conf. Rec. Asilomar 
Conf. Signals, Syst. Comput., vol. 1, pp. 40â€“44, 1993. 
[7] J. Zammit and I. J. Wassell, â€œAdaptive block compressive 
sensing: 
Toward 
a 
real-time 
and 
low-complexity 
implementation,â€ IEEE Access, vol. 8, pp. 120999â€“121013, 
2020. 
[8] R. Tibshirani, â€œRegression shrinkage and selection via the 
Lasso,â€ J. R. Stat. Soc. Ser. B, vol. 58, no. 1, pp. 267â€“288, 1996. 
[9] E. J. CandÃ¨s, J. K. Romberg, and T. Tao, â€œStable signal 
recovery from incomplete and inaccurate measurements,â€ 
Commun. Pure Appl. Math., vol. 59, no. 8, pp. 1207â€“1223, 
2006. 
[10] J. Wang, S. Ye, Y. Ruan, and C. Chen, â€œLow storage space for 
compressive sensing: Semi-tensor product approach,â€ Eurasip 
J. Image Video Process., vol. 2017, no. 1, pp. 1-13, 2017. 
[11] J. Wang, Z. Xu, Z. Wang, S. Xu, and J. Jiang, â€œRapid 
compressed sensing reconstruction: A semi-tensor product 
approach,â€ Inf. Sci. (Ny)., vol. 512, pp. 693â€“707, 2020. 
[12] A. Mousavi, A. Patel, and R. Baraniuk, â€œA deep learning 
approach to structured signal recovery,â€ 53rd Annu. Allert. 
Conf. Commun. Control. Comput. Allert. 2015, pp. 1336â€“1343, 
2016. 
[13] K. Kulkarni, S. Lohit, P. Turaga, R. Kerviche, and A. Ashok, 
â€œReconNet: Non-iterative reconstruction of images from 
compressively sensed measurements,â€ Proc. IEEE Comput. 
Soc. Conf. Comput. Vis. Pattern Recognit., vol. 2016, pp. 449â€“
458, 2016. 
[14] H. Yao, F. Dai, S. Zhang, Y. Zhang, Q. Tian, and C. Xu, â€œDR2-
Net: Deep residual reconstruction network for image 
compressive sensing,â€ Neurocomputing, vol. 359, pp. 483â€“493, 
2019. 
[15] K. Dabov, A. Foi, and K. Egiazarian, â€œImage denoising by 
sparse 3-D transform-domain collaborative filtering,â€ IEEE 
Trans. IMAGE Process., vol. 16, no. 8, pp. 2080â€“2095, 2007. 
[16] J. Du, X. Xie, C. Wang, G. Shi, X. Xu, and Y. Wang, â€œFully 
convolutional measurement network for compressive sensing 
image reconstruction,â€ Neurocomputing, vol. 328, pp. 105â€“112, 
2019. 
[17] J. Du, X. Xie, C. Wang, and G. Shi, â€œPerceptual compressive 
sensing,â€ Lect. Notes Comput. Sci, vol. 11258 LNCS, pp. 268â€“
279, 2018. 
[18] J. Zhang and B. Ghanem, â€œISTA-Net: Interpretable 
optimization-inspired deep network for image compressive 
sensing,â€ IEEE/CVF Conf. Computer Vision and Pattern 
Recognition, pp. 1828â€“1837, 2018. 
[19] Qi, Hongsheng and Daizhan Cheng. "Analysis and control of 
boolean networks: A semi-tensor product approach." 2009 7th 
Asian Control Conference. IEEE, pp. 1352â€“1356, 2009. 
[20] Candes E, Romberg J. â€œL1-magicâ€¯: Recovery of sparse signals 
via convex programming,â€ April 2022. [Online]. Available:  
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.212
.9120&rep=rep1&type=pdf. 
[21] D. Margalit and J. Rabinoff, Interactive Linear Algebra. 
Georgia Institute of Technology, 2019. 
[22] A. Akansu, R. Haddad, and P. Haddad, Multiresolution signal 
decomposition: transforms, subbands, and wavelets. Academic 
Press, 2001. 
[23] T. Wimalajeewa, Y. C. Eldar, and P. K. Varshney, â€œRecovery 
of sparse matrices via matrix sketching,â€ CoRR, vol. 2, no. 5, 
pp. 1â€“5, 2013. 
[24] G. Dasarathy, P. Shah, B. Bhaskar, and R. Nowak, â€œSketching 
sparse matrices,â€ arXiv Prepr. arXiv1303.6544, pp. 1â€“33, 2013. 
[25] G. Dasarathy, P. Shah, B. Bhaskar, and R. Nowak, â€œSketching 
sparse matrices, covariances, and graphs via tensor products,â€ 
IEEE Trans. Inf. Theory, vol. 61, no. 3, pp. 1373â€“1388, 2015. 
[26] MathWorks, â€œDefine Custom Deep Learning Layer with 
Learnable Parameters,â€ April 2022. [Online]. Available: 
https://www.mathworks.com/help/deeplearning/ug/define-
custom-deep-learning-layer.html. 
[27] MathWorks, â€œSpecify Layers of Convolutional Neural 
Network,â€ 
April 
2022. 
[Online]. 
Available: 
https://www.mathworks.com/help/deeplearning/ug/layers-of-
a-convolutional-neural-network.html. 
[28] MathWorks, â€œ2-D Convolutional Layer,â€ April 2022. [Online]. 
Available: https://www.mathworks.com/help/deeplearning/ref
/nnet.cnn.layer.convolution2dlayer.html. 
[29] O. Ronneberger, P. Fischer, and T. Brox, â€œU-Net: 
Convolutional networks for biomedical image segmentation,â€ 
Int. Conf. Med. Image Comput. Comput. Interv., pp. 234â€“241, 
2015. 
[30] D. Gilton, G. Ongie, and R. Willett, â€œDeep equilibrium 
architectures for inverse problems in imaging,â€ arXiv Prepr. 
arXiv2102.07944, pp. 1â€“21, 2021. 
 
12
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-970-6
SIGNAL 2022 : The Seventh International Conference on Advances in Signal, Image and Video Processing

