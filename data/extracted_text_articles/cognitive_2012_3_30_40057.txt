Speed Up Learning in a Test Feature Classiﬁer Using Overlap Index Lists
Yoshikazu Matsuo
Hokkaido University
Information Science and Technology
Sapporo, Japan
matsuo@ssc.ssi.ist.hokudai.ac.jp
Hidenori Takauji
Muroran Institute of Technology
Robot Arena
Muroran, Japan
uji@mmm.muroran-it.ac.jp
Takamichi Kobayashi
Nippon Steel Corporation
Futtsu, Japan
kobayashi.takamichi@nsc.co.jp
Shun’ichi Kaneko
Hokkaido University
Information Science and Technology
Sapporo, Japan
kaneko@ssi.ist.hokudai.ac.jp
Abstract—This paper presents a novel low cost learning
algorithm for a Test Feature Classiﬁer using Overlap Index
Lists (OILs). In general, pattern classiﬁers require a large
amount of training data to attain high performance, which
is expensive in terms of computation time. Our proposed
algorithm uses OILs to efﬁciently ﬁnd and check combinations
of features starting with lower dimensions and working up-to
higher ones. Our algorithm can solve classiﬁcation problems
in real industrial inspection lines with large reductions in
computation time.
Keywords-Test Feature Classiﬁer; Overlap Index List; Speed
Up Learning; Curse of Dimensionality.
I. INTRODUCTION
Recently, automatic tools for inspecting products have
become increasingly important for develop ﬂexible manu-
facturing lines. Examples can be found in visual inspection
in production and precision work that is necessary for quality
management. Classiﬁcation is one of the most important
techniques employed in automatic inspection systems. In
general, classiﬁers require a large set of training samples
for learning in order to attain a high level of performance.
However, the labelling of samples, which makes supervised
learning possible for a classiﬁer, is typically, a very time-
consuming task.
A current research subject in pattern recognition is re-
ducing the cost of learning. for which several approaches
have been proposed. One approach aims at reducing the size
of the training data set [1] [2]. Another approach involves
streamlining the learning algorithm [3] [4]. In this study,
we discuss the latter approach and propose a high speed
learning algorithm for a Test Feature Classiﬁer (TFC), which
is a pattern classiﬁer. Real inspections of manufacturing-line
quality have several problems including the following: 1)
some results of labelling for single data are not matched, 2)
the reliability of the labels must be validated, and 3) data
with low reliability are not useful for learning. Research into
efﬁcient learning methods would contributes to solve these
problems.
TFC’s learning process involves ﬁnding and recording
PTFs which are basic sub-features, beginning with a search
in lower dimensions. In this paper, we propose the method
for efﬁcient learning that exploits the fact that class overlaps
in a high dimensional feature space require overlaps in lower
dimensions. In addition, we propose an efﬁcient learning
technique for adding new features or dimensions. The detail
of these methods shows in section 3 and 4. The veriﬁcation
experiments shows in section 5.
II. TEST FEATURE CLASSIFIER
Because the mathematical formalization of TFC has al-
ready been provided in [5], we brieﬂy introduce classiﬁers
through qualitative and semantic explanations. TFC consists
of learning and discrimination procedures. In the learn-
ing procedure, a nonparametric and speciﬁc investigation
divides the overall feature space into local subspaces of
combinatorial features in which class overlap. Combination
of features are called “test features”, (TFs) or “prime test
features”, (PTFs), which are irreducible test features. In
the discrimination procedure, an unclassiﬁed candidate input
pattern is checked in each subspace using the corresponding
PTF, and then, the pattern is classiﬁed according to the
average voting scores in sub-spaces. Thus, the classiﬁer aims
to achieve high performance with a small training dataset by
the partial discriminations contained in sub-spaces. Given
a training dataset in which classes do not overlap, a TF is
deﬁned as any combination of features that does not classify
every pattern with the selected features alone, that is, it
satisﬁes the condition of having non-overlapping classes. In
general, smaller combinations of features may allow class
overlaps, but they are advantageous form the perspective
of low-cost computing. Because it is provable that any
combination of features that includes a TF is itself a TF,
63
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

Table I
EXSAMPLE OF TWO CLASSES PROBLEM.
f1
f2
f3
f4
C1
1x1
2
5
9
7
1x2
5
6
10
5
1x3
4
7
7
6
1x4
5
8
8
9
1x5
3
9
7
6
C2
2x1
5
8
6
1
2x2
7
9
2
2
2x3
3
10
4
5
2x4
6
7
2
6
2x5
5
4
1
5
we should choose irreducible TFs, or PTFs, for discrimina-
tion. In [6], an efﬁcient and effective learning method for
successively adding training data by using weights for PTFs
was proposed.
III. HIGH-SPEED LEARNING ALGORITHM USING
OVERLAP INDEX LISTS
TFC’s learning procedure involves identifying enough TFs
so that the non-overlap condition is satisﬁed. In our proposed
algorithm, the learning procedure ﬁnds NTFs, which are
sub-spaces that have data with overlapping classes. The
learning procedure begins by searching for TFs in low
dimensional sub-spaces and adding feature dimensions. Data
with overlapping classes in higher-dimensional sub-spaces
will be overlapped in each lower dimensional subspace that
is contained in a higher dimensional subspace. Therefore,
we use Overlap Index Lists (OILs) to record classiﬁca-
tion overlaps in order to circumvent checking data that
are already known to satisfy the non-overlap condition. In
this algorithm, additional checking is necessary in lower
dimensional sub-spaces where few data overlaps occur, and
this checking provides additional overhead for the algorithm.
We will brieﬂy discuss the computational complexity of our
high-speed learning algorithm using OILs.
A. Creating OILs for Single Feature Dimensions
TableI shows an example of a two class problem. It has
a dataset with ﬁve data elements 1x1, 1x2, · · · , 2x5 in each
class and four independent features f1, f2, f3 and f4 . The
left subscripted letters indicate the class the elements belong
to, whereas the right subscripted letters are indices. We will
describe our high-speed learning algorithm by tracing the
procedures that search PTFs. The ﬁrst step of the algorithm
is create OILs for one-dimensional features. In the case of
f1, 1x2, 1x4, 2x1 and 2x5 as well as 1x5 and 2x3 constitute
an interclass overlap. Thus, feature f1 is an NTF, and the
index information below is added to the OIL L(f1).
L(f1)
=
{i1, i2}
i1
=
{{2, 4} , {1, 5}}
i2
=
{{5} , {3}}
The set i1 shows the interclass overlap consisting of
1x2, 1x4, 2x1, and 2x5. The set {2, 4} represents the indices
of 1x2 and 1x4 in class 1, and the set {1, 5} represents
the indices of 2x1 and 2x5 in class 2. The set i2 shows the
overlap involving 1x5 and 2x3, 5 and 3 represent the indices
of 1x5 in class 1 and 2x3 in class 2, respectively. We create
OILs for f2, f3 and f4 in a similar manner, obtaining.
L(f2)
=
{i1, i2, i3}
i1
=
{{3} , {4}}
i2
=
{{4} , {1}}
i3
=
{{5} , {2}}
L(f4)
=
{i1, i2}
i1
=
{{2} , {3, 5}}
i2
=
{{3, 5} , {4}}
There is no interclass overlap with respect to f3. Thus, f3
is a TF, for which an OIL is not needed.
B. Updating OILs for Higher Feature Dimensions
In the next step, we search TFs and update the OILs
for two dimensions using the OILs for single dimensions.
Searching all data for interclass overlaps is necessary while
checking for TFs in higher dimensions. Thus, the result of
checking for TFs does not depend on the order in which
features are added. In the sub-feature f1f2, we must decide
whether to add information about features f1 to the OIL
for f2 or vice versa. To perform a more efﬁcient check,
we use the OIL with the smallest number of overlapping
combination. The number of data elements that must be
searched while checking for a TF is n1 × n2, where n1 and
n2 are the number of data elements in class 1 and class 2,
respectively. Assume s(f1) is the number of data elements
that must be searched. It is calculated by multiplying the
number of L(f1) ’ s i1 and i2 elements of class 1 by the
number of elements of class 2:
s(f1)
=
(2 × 2) + (1 × 1)
=
5
Similarly, s(f2) = (1 × 1) + (1 × 1) + (1 × 1) = 3.
Thus, we choose to update L2 because s(f2) < s(f1).
In the case of the feature f1f2, the three pair of data
(1x3, 2x4) , (1x4, 2x1) , and (1x5, 2x2). TableI shows that
there is a class overlap with respect to these features in the
case of 1x4 and 2x1. Thus, f1f2 is not a TF and the OIL is
updated:
L(f1f2)
=
{i1}
i1
=
{{4} , {1}}
We then check for TFs among the ﬁve sets of features
f1f3, f1f4, f2f3, f2f4, and f3f4. Features f1f3, f2f3, and
64
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

Number of dimensions
1                   5                      10                      15                      20
Time(s)
900
600
300
0
Total time(s)
300
200
100
0
creation time(checklist)
creation time(normal)
total time(checklist)
total time(normal)
Figure 1.
Comparison of time costs for searching PTFs (Inspection
Dataset).
f3f4 are TFs because f3 is a TF. Thus, we only need to
check the two sets of features f1f4 and f2f4. For f1f4, we
use f4’s OIL because s(f1) = 5 > s(f4) = 4. Checking
the data (1x2, 2x3) , (1x2, 2x5) , (1x3, 2x4) and (1x5, 2x4)
from the list L(f4) revealed that there is a class overlap
in the case of 1x2 and 2x3. Thus, f1f4 is an NTF and
the OIL is updated with L (f1f4) = {{2} , {5}}. For f2f4,
we use f2’s list because s (f2) < s (f4). Similarly, f2f4
is an NTF because there is an overlap involving 1x3 and
2x4. This completes the process of checking for TFs in two
dimensions.
Our next step is to check for TFs in three dimensions. The
process is almost the same as that for two dimensions. The
only feature combination that we need to check is f1f2f4.
We can use any of the lists L (f1f2), L (f1f4), or L (f2f4)
because s (f1f2) = s (f1f4) = s (f2f4). In this case, we
will use L (f1f2). Based on this list, we only need to check
the set (1x4, 2x1), where there is no overlap. Thus, the
combination f1f2f4 is a TF. The process of creating TFC is
complete because there are no NTFs in the three dimensional
case.
IV. EFFICIENT LEARNING FOR ADDING DIMENSIONS
After deﬁning a set of PTFs, i.e., training the initial
TFC with a speciﬁed data for which feature dimensions
are provided, TFC must be able to up-date or modify itself
for augmented feature spaces. We propose an algorithm for
modifying a set of PTFs on the basis of new features. When
a new feature is presented and new subspaces are added,
the original sub-spaces are included in the new set of sub-
spaces, but bit vice versa. Thus, the modiﬁcation procedure
will only check for TFs in the new sub-spaces that are not
known to be TFs on the basis of previously identiﬁed PTFs.
V. EXPERIMENTAL RESULTS
A. High Speed Learning Algorithm using OILs
We used three data sets for our experiments. The ﬁrst data
set is used in a real industrial inspection line (Inspection
Dataset). It has two classes and 20 dimensions. Class 1 and
Total time(s)
40
30
20
10
0
creation time(checklist)
creation time(normal)
total time(checklist)
total time(normal)
Number of dimensions
16
12
8
4
0
Time(s)
1    2    3    4    5    6    7    8    9    10  11  12  13  14  15  16
Figure 2. Comparison of time costs for searching for PTFs (Letter Dataset).
Total time(s)
0.8
0.6
0.4
0.2
0
Number of dimensions
0.5
0.4
0.3
0.2
0.1
0
Time(s)
1            2          3          4          5          6          7          8
1  
 2 
creation time(checklist)
creation time(normal)
total time(checklist)
total time(normal)
Figure 3.
Comparison of time costs for searching for PTFs (Abalone
Dataset).
2 contain 2033 and 3799 data elements, respectively. The
second dataset is the Letter Data-set from UCI database [7];
it has two classes, class D and class P, and 16 features. Class
D and P contain 805 and 803 data elements, respectively.
The third dataset is the Abalone Data-set from UCI database
[7] that is used for distinguishing between male and female
abalones. The male and female classes contain 1528 and
1307, respectively, and there are 8 features. We used these
three data sets as training data. The respective times for
checking for TFs in each dimension for the three datasets
are shown in Fig. 1, 2, and 3. The bar graphs show the time
needed for checking for TFs in each dimension, and the
line graphs show the total time for each case. These results
demonstrate the efﬁciency of the algorithm for the Inspection
and Letter Datasets. However, the method dose not show a
clear advantage in the case of the Abalone Dataset because
of the overhead for lower dimensions. We believe that one
factor may be that the total amount of time needed to create
the initial TFC was too small to allow a signiﬁcant change.
A PTF is deﬁned as a prime TF that dose not include
other TFs. In TFC and sTFC, the learning procedure involves
searching and recording PTFs. As an example, Fig. 4 shows
the number of PTFs and TFs for the Inspection Data-set.
65
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

1              5                  10                15                20                25
12
10
8
6
4
2
0
4
3
2
1
0
Number of dimensions
Number of PTFs
Number of TFs
23dims PTF
26dims PTF
23dims TF
26dims TF
×10
×10
3
6
Figure 4.
Number of PTFs with increasing number of feature dimensions.
Number of dimensions
Time(s)
Total time(s)
10    11      12     13     14     15     16     17     18    19
120
100
80
60
40
20
0
300
250
200
150
100
50
0
creation time(eﬃcient)
creation time(normal)
total time (eﬃcient)
total time(normal)
Figure 5.
Comparison of time costs for adding feature dimensions
(Inspection Dataset).
The ﬁgure 4 shows two cases: 23 and 26 features. The
highest numbers of dimensions of PTFs in two cases are 8
and 9. We observed similar characteristics for the other two
datasets, and we assume that this is a n instance of the curse
of dimensionality [8] [9]. In general, higher-dimensional
features need sufﬁcient amount of data. Features with very
high dimensions can negatively impact training performance.
Thus, it is necessary to maintain a certain density when
adding features. The highest dimensions that PTFs can have
depend on the data set. If we can determine the highest
dimensions in advance, TFC can learn more quickly. This
will be one of our future research projects.
B. Efﬁcient Learning for Adding Feature Dimensions
In this section, we present the result of an experiment
that examined the efﬁciency of our learning procedure for
adding feature dimensions. In the experiment, we used the
Inspection Data set with 10 features as training data for
the initial learning and up to 19 features were successively
added. Fig. 5 shows the times needed for creating TFCs
for each dimension. The bar graph shows the time needed
for checking for TFs in each dimension, and the line graph
shows the total time for creating TFCs in each dimension.
The average time reduction for each dimension was approx-
imately 50%.
VI. CONCLUSION AND FUTURE WORK
We have proposed a new method for efﬁcient learning us-
ing OILs that can eliminate unnecessary checks and efﬁcient
learning method for adding feature dimensions using pre-
liminary information. We compared the performance of our
proposed methods with that of conventional TFC methods.
Our proposed methods can greatly reduce the time needed
for creating TFCs.
We found that the highest number of dimensions in
the created PTFs was stable, although the highest number
of dimensions depends on the dataset. One topic for our
future research will be determining the highest number of
dimensions.
ACKNOWLEDGMENTS
We are deeply indebted to associate Professor Tanaka,
whose comments and suggestions were invaluable for this
research.
REFERENCES
[1] U. Bhattacharya, S. Vajda, A. Mallick, B.B. Chaudhuri, and A.
Belaid.: “On the choice of training set, architecture andcom-
bination rule of multiple mlp classiﬁers for multireso-lution
recognition of handwritten characters” In IWFHR, pp. 419-
424, 2004
[2] J. Wang, P. Neskovic and L.N. Cooper： “Training Data
Selection for Support Vector Machines” LNCS, vol.3610, pp.
554-564, 2005
[3] D. Thanh-Nghi, N. Van-Hoa and P. Francois: “Speed Up SVM
Algorithm for Massive Classiﬁcation Tasks” Advanced Data
Mining and Applications, vol.5139, pp. 147-157, 2008
[4] J.X. Dong, A. Krzyzak, and C.Y. Suen: “Fast SVM training
algorithm with decomposition on very large datasets” IEEE
Transaction Pattern Analysis and Machine Intelligence, vol.27,
pp. 603-618, 2005.
[5] V. Lashkia, S. Kaneko and S. Aleshin： “Distance-based Test
Feature Classiﬁers and its Applications” IEICE Trans. Inf. and
Syst., vol.E83-D, no.4, pp. 904-913, 2000.
[6] Y. Sakata, S. Kaneko, Y. Takagi, H. Okuda： “Successive
pattern classiﬁcation based on test feature classiﬁer and its
application to defect image classiﬁcation” Pattern Recognition,
vol.38, no.11, pp. 1847-1856, 2005
[7] P.M. Murphy and D.W. Aha: “UCI Repository of Machine
Learning
Databases”
University
of
California-Irvine,
1994(anoymous
http://archive.ics.uci.edu/ml/machine-
learning-databases/letter-recognition/)
[retrieved:
May,
2012].
[8] Bellman R.E.: “Adaptive Control Processes” Princeton Univer-
sity Press, Princeton, NJ. 1961
[9] Brown. M, Bossley.K.M, Mills.D.J, Harris.C.J: “High dimen-
sional neurofuzzy systems: overcoming the curse of dimen-
sionality” International Joint Conference of the Fourth IEEE
International Conference on Fuzzy Systems, vol.4, pp. 2139 -
2146, 1995
66
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

