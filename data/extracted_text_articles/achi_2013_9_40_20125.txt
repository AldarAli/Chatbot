TV Applications for the Elderly: Assessing the Acceptance of Adaptation and 
Multimodality 
José Coelho, Tiago Guerreiro and Carlos Duarte 
LaSIGE, Department of Informatics, University of Lisbon 
C6 Piso3, 1749-016, Lisboa 
Lisboa, Portugal 
{jcoelho, tvjg, cad} @di.fc.ul.pt 
Pradipta Biswas, Gokçen Aslan and Pat Langdon 
Computer Laboratory, Department of Informatics, 
University of Cambridge 
15 JJ Thomson Avenue, Cambridge CB3 0FD 
Cambridge, United Kingdom 
{pb400, ga283, pml24} @cam.ac.uk
 
Abstract— Current TV applications present uncountable 
challenges to an elderly user and these are prone to increase, 
working as a vehicle of exclusion. The GUIDE project aims at 
improving the elderly experience with present TV applications 
by deploying interfaces adapted to their abilities and 
preferences. We do so by building the interface based on a user 
model and providing new ways to interact with the TV 
(multimodality), and tailoring the UI to the users’ abilities, 
needs and preferences. In this paper, we assess concepts of the 
GUIDE framework with particular focus to the User 
Initialization Application (UIA), an interactive application able 
to build the aforementioned user model. We report an 
evaluation with 40 older users from two countries (UK and 
Spain). Results show that the UIA is able to create adequate 
profiles and that the users are able to positively observe the 
adaptations. Further, novel ways of interacting with the TV 
were also successfully evaluated as the users tended to 
experiment and rate positively most alternatives, particularly 
Speech and Tablet interaction.  
Keywords-accessible 
applications, 
elderly, 
multimodal, 
simulation, GUIDE 
I. 
 INTRODUCTION 
The rapid increase of new TV-based systems and 
applications excludes users with certain impairments or 
different levels of technology awareness and expertise from 
accessing the same information as others do. By 2050 elderly 
people will represent 26% of the developed countries’ 
population. Living in this constantly evolving world is 
getting harder, leaving them no time to adapt to modern 
technology from which, sooner or later, their well-being and 
social inclusion will dramatically depend. Having this type 
of technology adapt to each user without requiring too much 
learning and giving them similar opportunities as everybody 
else is the only way to prevent this. However, solutions being 
studied focus more on the use of assistive devices to help or 
guide the interaction than on the design of inclusive systems. 
We can offer valuable contributions for making this type of 
systems and applications more accessible by focusing on 
modern research in multimodal interactive systems, leading 
to less effort from elderly users, and offering them more 
natural ways of interaction.  
This paper focuses on how the GUIDE project 
concentrates on the integration of several natural modalities 
and devices towards the inclusive use of TV-based 
applications in the user’s home. Special attention will be 
given to the understanding of how elderly people can interact 
naturally with this type of technology. With this in mind, and 
after presenting related work, we will show how the GUIDE 
framework is built around keeping relevant user information 
stored and accessible in a User Model component. This 
allows 
adapting 
UIs, 
based 
on 
the 
specific 
user 
characteristics and preferences as well as on a context model. 
We will then focus on how a novel approach called User 
Initialization Application (UIA) was implemented. This 
process 
allows 
discovering 
each 
user’s 
relevant 
characteristics for the system to adapt in the most appropriate 
and understandable way. At the same time, the UIA instructs 
the user on how to make use of the most suitable interaction 
possibilities available. We complement and attest these 
approaches with the description and results of a user study 
consisting of a technical trial with both the UIA and a TV 
based realistic Electronic Program Guide (EPG) application. 
We present results of the evaluation of the UIA capability to 
discover user characteristics and assign user profiles. With 
the EPG application we assessed users’ modality preferences 
and evaluated the adaptation capabilities of the framework. 
The paper main contributions are the results of user studies 
conducted in two different countries (Spain and UK) with 40 
participants, and the evaluation of novel approaches on 
adaptability and multimodal interaction of TV-based systems 
supported by the UIA application and GUIDE’s multimodal 
and adaptive capabilities. 
II. 
RELATED WORK 
Elderly users are getting detached from technology and 
consequently from the modern world characterized by 
innovation in interaction and communication standards. In a 
society where people live longer, it’s common to see elderly 
people living alone and struggling to adapt to all the 
technological advances [13]. Although older people are not 
generally considered to have disabilities, the natural ageing 
process carries some degenerative ability changes, which can 
include diminished vision, varying degrees of hearing loss, 
psychomotor impairments, as well as reduced attention, 
memory and learning abilities [20]. However, problems 
learning to use and engage with interactive technology are 
not conﬁned to physical and cognitive factors [11]. In recent 
investigation, it is clear that even if computer based systems 
are a positive influence on the lives of older people [12], they 
tend to reject standard technologies [8] or technologies too 
234
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

difficult to use, that look bad, make them appear older, or 
that do not appear to have net gain effect for them [7].  
For these reasons, Information and Communication 
Technology (ICT) must be adjusted to ensure that elderly 
users are not disadvantaged when using it [7, 8]. This 
accommodation can only be done by realizing the 
consequences associated with ageing and offering natural 
and efficient ways of interacting with both new and well-
known technical systems. 
Multimodal interfaces aim to provide a more natural and 
transparent interaction to users. They have been able to 
enhance human-computer interaction (HCI) in many ways, 
including: User satisfaction: studies revealed that people 
favor 
multiple-action 
modalities 
for 
virtual 
object 
manipulation tasks [18], and that about 95% of users prefer 
multimodal interaction over unimodal interaction [18]; 
Robustness and Accuracy: “using a number of modes can 
increase the vocabulary of symbols available to the user” 
leading to an increased accessibility [16]. Oviatt stated also 
that multiple inputs have a great potential to improve 
information 
and 
systems 
accessibility, 
because 
by 
complementing each other, they can yield a “highly 
synergistic blend in which the strengths of each mode are 
capitalized upon and used to overcome weaknesses in the 
other” [19]; 
Efficiency and 
Reliability: Multimodal 
interfaces are more efficient than unimodal interfaces, 
because they can in fact speed up tasks completion by 10% 
and improve error handling and reliability [17]; and 
Adaptivity: Multimodal interfaces oﬀer an increase in 
flexibility and adaptivity in interaction because of the ability 
to switch among diﬀerent modes of input, to whichever is 
more convenient or accessible to a user [16]. For all these 
reasons, multimodal interfaces have the potential to increase 
ICT applications’ accessibility for elderly users. Introducing 
elderly users to multimodal solutions makes possible a 
familiar and natural step through the accommodation 
necessary to ensure that older users are not disadvantaged 
when using new technologies like the ones based on TV and 
STBs [13, 21]. 
Interface personalization is mainly explored in the 
domain of content personalization and developing intelligent 
information filtering or recommendation systems based on 
user profiles. In most of those systems, content is represented 
in a graph like structure and filtering or recommendation is 
generated by storing and analyzing users’ interaction 
patterns. Little research has been done beyond content 
personalization. A few significant projects on interface 
personalization are the SUPPLE project[10], the Lumiere 
Project [14] and the AVANTI project [22] for people with 
disabilities. The SUPPLE project personalizes interfaces by 
changing layout and font size for people with visual and 
motor impairment and ubiquitous devices. However, the user 
models do not consider visual and motor impairment in 
detail and thus work for only loss of visual acuity and a few 
types of motor impairment. The Lumiere project uses an 
influence diagram in modeling users. This records the 
relationships among users’ needs, goals, background etc. The 
AVANTI project provides a multimedia Web browser for 
people with light or severe motor disabilities, and for blind 
people. It distinguishes personalization into two classes: 
static adaptation which is personalization based on user 
profile and dynamic adaptation that is personalization 
following the interaction pattern with the system. However, 
the Lumiere project does not generalize their personalization 
mechanisms for other applications and the AVANTI project 
only addresses a small segment of disabilities for a particular 
application. The lack of a generalized framework for 
personalization of users with a wide range of abilities affects 
the scalability of products. 
III. 
THE GUIDE PROJECT 
As a multimodal adaptive system, GUIDE offers several 
modalities of interaction coordinated by the system core, 
through fusion and fission modules that function based on a 
User Model [4], which represents the user, and a Context 
Model, which represents the context (environment, devices, 
etc.) the user is in. Input modalities are based in natural ways 
of communication for humans: speech and pointing (and 
gestures). Complementary to these modalities and being 
based in a TV environment, the system also supports the 
usage of remote controls (both traditional and endowed with 
gyroscopic capabilities) and other devices capable of 
providing haptic input or feedback. GUIDE incorporates four 
main types of UI components: visual sensing and gesture 
interpretation; audio; remote control; haptic interfaces and a 
multi-touch tablet. In what concerns the output modalities, 
the framework integrates the following: video rendering 
equipment (TV); audio rendering equipment (speakers); 
tablet supporting a subset of video and audio rendering; and 
remote control supporting a subset of audio rendering and 
vibration feedback. The tablet, used as a secondary display, 
may be used to clone the TV screen or complement 
information displayed on the TV. In addition to the 
application’s interface, the framework is capable of 
rendering a 3D avatar. This is expected to play a major role 
for elderly acceptance and adoption of the GUIDE system, 
being able to perform non-verbal expressions like facial 
expressions and gestures and giving the system a more 
human like communication ability. Both input and output 
modalities can be used in a combined manner to enrich 
interaction and reach all user types.  In order for the UI to be 
adapted to the user’s preferences and abilities, the interface 
elements are highly configurable and scalable (vector-based). 
Graphical properties like size, font, location and color are 
some attributes needed to ensure adaptability. Other 
modalities’ properties, like sound volume, are also 
configurable. The GUIDE User Model, responsible for the 
dynamic adaptation of the system, will be described in a later 
section. 
A previous study was conducted last year [5] with the 
goal of identifying the most relevant UI component 
configurations and user impairments for elderly users. This 
was particularly important for deriving a list of relevant 
topics and metrics to be considered in the development of 
accessible applications for these users. Closely connected to 
the new study are the conclusions related with the way 
elderly prefer and perform multimodal interaction in TV-
based systems. However, in the previous study the 
235
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

application used was not a realistic TV-application. Thus, a 
more contextualized evaluation can be done. Additionally, 
the process of user characterization and clustering is now 
focused. 
IV. 
THE USER MODEL 
The GUIDE user model maps users’ functional 
parameters to interface parameters. It was developed using a 
simulator [4] and calibrated through the user study [5] 
discussed in the previous section. The simulator consists of 
detailed models of visual and auditory perception, cognition 
and motor action. The simulator can show the effects of a 
particular disease on visual functions and hand strength 
metrics and in turn their effect on interaction. Using 
information from more than 100 users from three different 
countries (Spain, UK and Germany), collected with an 
extensive survey focusing a wide array of characteristics, we 
have selected a set of variables that are relevant to the user 
model and statistically significantly different among clusters 
(p<0.01). We separately clustered these data for visual, 
cognitive and motor abilities of users using k-means 
clustering. Table 1 shows the cluster centers. 
Table 1: Cluster Centers 
Visual1 
A 
B 
C 
CS1 (Contrast Sensitivity on Pelli Robson Chart n. 1)  0.15 
1.75 
1.46 
CS2 (Contrast Sensitivity on Pelli Robson Chart n. 2)  0.15 
1.84 
1.56 
CS3 (Contrast Sensitivity on Pelli Robson Chart n. 3)  0 
1.79
1.37
Cognitive 
A 
B 
 
TMTSEC (Time to complete Trial Making Test in sec)  45.97  115.3  
DIGIT_SY (Result on Digit Symbol Test)
45.93  23.71
Motor 
A 
B 
C 
GS (Grip Strength in Kg) 
16.22  25.02 58.68
ROMW (Active Range of Motion of Wrist) 
71.28  51.58 65.67
Following this, we ran the GUIDE simulator [4], taking 
the parameters of each cluster centre for configuration, and 
generated recommendations for each cluster. Individual users 
were assigned to the recommendations based on their cluster 
memberships. For the present set of users we identified the 
following three profiles: Profile A: No adaptation required; 
Profile B: Increase button spacing (Mobility Impaired); 
Profile C: Increase button spacing + change Color Contrast 
(Mobility Impaired + Color Blind). 
The GUIDE User Model predicts three sets of 
parameters: UI parameters for the Multimodal Fission 
Module, Adaptation Code for the Input Adaptation Module 
and Modality Preference for the Multimodal Fusion Module. 
The rules relating the users’ range of abilities with interface 
parameters were developed by running the simulator [3, 4] in 
Monte Carlo simulation. 
We began by selecting a set of variables to define a Web 
based interface. These parameters include: Button spacing: 
Minimum distance between two buttons to avoid missed 
                                                           
1 Each of these clusters is further divided based on presence of color 
blindness 
selection; Button Color: foreground and background color of 
a button; Button Size: The size of a button; Text Size: Font 
size for any text rendered in the interface; Cursor Type: The 
shape and color of the cursor. 
The user model predicts the minimum button spacing 
required, from the users’ motor capabilities and screen size. 
The simulation predicts that users having less than 10 kg of 
grip strength or 80º of Active Range of motion of wrist or 
significant tremor in hand produce a lot of random 
movement while they try to stop pointer movement to select. 
The area of this random movement is also calculated from 
the simulator. Based on this result, we calculated the radius 
of the region of the random movement and the minimum 
button spacing is predicted in such a way so that this random 
movement does not produce a wrong target selection. 
Regarding the other parameters, the UIA takes user 
preferences for color, text size and cursor type. The user 
model stores these preferences. However if a user has color 
blindness it recommends an appropriated foreground and 
background color. The adaptation code aims to help users 
while they use a pointer to interact with the screen through 
the visual human sensing capabilities or the gyroscopic 
remote. So if the user has any motor impairment, the 
adaptation will remove jitters in movement through 
exponential average and then attract the pointer towards a 
target when it is near by using the gravity well mechanism 
[3]. Otherwise, the adaptation will only work to remove 
minor jitters in movement. The modality prediction system 
predicts the best modality of interaction for users. Though 
users are free to use any modality irrespective of the 
prediction, the fusion module uses this prediction to 
disambiguate input streams when there is more than one.  
V. 
USER INITIALIZATION APPLICATION 
The UIA is an introductory application that runs the first 
time a user initializes the system. When a new user is 
recognized, the UIA presents a step-by-step introduction of 
the system, acting as a tutorial on how to use the system and 
how to interact using the different modalities available. 
Another purpose of the UIA, as important as the one 
described before, is to expedite the user profiling procedure. 
It would not be feasible to ask each user of a mass market 
product to complete an extensive survey before using it. The 
UIA presents a much reduced set of questions and tasks to 
the user in order to allow the User Model to assign the user 
to one of the previously created profiles. Even though this 
does not allow for a profile perfectly fitted to the user, it is a 
good starting point for adaptation purposes, because the 
profiles described in the previous section were created from 
a large pool of representative users. Additional information 
collected during system usage is used to refine the user 
profile (run-time adaptation). 
The tasks and metrics chosen for the UIA are the ones for 
which the resulting data is the most capable to assign the 
more appropriate profile to the user profile. They were 
selected from an analysis of the extensive survey data, taking 
into account the feasibility of gathering the data. For those 
instances where it was not feasible to gather the data in a 
living room environment, alternative sources were selected 
236
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

and combined to estimate the required data. A description of 
these variables is listed below: Color Blindness: Plates 16 
and 17 of Ishihara Test [6] as it may classify among 
Protanopia, Deuteranopia and any other type of color 
blindness; Dexterity: We estimated Grip Strength and Active 
Range of Motion of wrist from age, sex and height of users 
following earlier Ergonomics research [2]; Tremor: We 
conducted earlier a test involving a Tablet device in 
horizontal position, and estimated tremor from the average 
number of times users need to touch the screen to select 
small buttons. Details of the study can be found in a separate 
paper [4]. Additionally, other tasks were chosen with the 
purpose of allowing users to personalize the system, while 
being a hands-on tutorial regarding new modality interaction 
and feedback configuration. The most relevant ones are the 
following: Modality Introduction: Self-explanatory videos of 
how to interact with each modality, followed by “do-it-
yourself” tasks; Button and Menu Configuration: Button 
size, and font and background color configuration; Cursor 
Configuration: Cursor size, shape and color configuration; 
Audio Perception: Hearing capabilities and preferences. 
The UIA has a simple user interface, with a different 
screen for every task and metric identified above. Few 
buttons are presented per screen (preventing user confusion). 
Every screen preserves the same navigation model - an area 
with “next”, “previous” and “repeat” buttons, and another 
visually distinct area for presenting information and requests. 
For every metric to be measured, tests are presented as 
simple questions about preferences. Also, for every modality 
available in the system, a video introducing its use is 
presented, followed by the possibility for the user to try it 
out. A virtual character accompanies the user through this 
process, offering explanations and assisting the user in the 
personalization. As the user goes through each task and 
preference setting, the UIA adapts itself to the preferences 
already manifested. For example, if user manifests 
preference for big, blue buttons with yellow text, all buttons 
will be presented with those settings from that moment 
onwards. It is worth pointing out that the results of our 
previous study are reflected in the UIA’s design: high 
contrast colors, big, centered and well-spaced buttons, etc. 
VI. 
STUDY DESCRIPTION 
The study’s main goal is to assess the acceptance of 
adaptation and multimodality by elderly users when 
interacting with TV based applications. We do this by 
addressing several questions related with the UIA, 
multimodal interaction and adaptation, and GUIDE in 
general. Regarding the UIA: first we want to measure the 
efficacy of this application in discovering the relevant 
characteristics of users and assigning user profiles; secondly, 
we want to evaluate how understandable the UIA is in terms 
of its goals and the instructions it provides; and finally, how 
easy it is for elderly to interact with this application, or if 
they would do it if it was part of their daily lives. Regarding 
multimodal interaction, we want to assess which modalities 
are the most used by elderly in a realistic TV interaction 
scenario. In what relates to adaptation, the goal of this study 
is to understand if users perceive it and if they are more 
satisfied using UIs adapted to their characteristics than non-
adapted versions of the same applications. Lastly, regarding 
GUIDE in general, we want to measure its acceptability and 
if elderly users perceive improvements in their quality of life 
just by using this system. 
A. Development of a Realistic EPG 
Similarly to the UIA, we used the knowledge gained in 
the first study to develop an Electronic Program Guide 
(EPG) application and focused on the engagement between 
users and a realistic TV interaction scenario. By developing 
this application we wanted to validate previously developed 
notions about elderly users interaction. For instance, do users 
favor alternative and multimodal ways of interaction with the 
TV or when they are in the presence of a realistic application 
they prefer the traditional interaction devices, like a remote 
control? 
The development of this application started with mockup 
designs, which were then reviewed and approved by experts 
in the development of TV and STB applications. Later all 
elements were implemented using HTML, JQuery, CSS and 
JS languages. The EPG had real information about channels 
and respective schedules and shows, to confer more realism 
to the application. Adapted versions were developed to fit 
each user profile. Output modalities, including the Virtual 
character, were made available and selected according to the 
user profile. 
B. Participants (Pre-Survey) 
We recruited 40 elderly people (24 female and 16 male) 
with different age-related disabilities. Users were recruited in 
two countries, with 21 participants (14 female and 7 male) 
being recruited in Spain and 19 participants (10 female and 9 
male) in the UK. The average age was 70.9 years old and the 
different user profiles were assigned to the participants in the 
following manner: 14 users with profile A, 22 users with 
profile B, and 4 users with profile C. As the non-adapted 
version of the EPG was developed already addressing 
accessibility issues, each of the profiles reflected few 
adaptations. All users participated voluntarily and all 
activities involved in this study were safeguarded from the 
ethical point of view.  
C. Apparatus 
The study was conducted in two locations (Spain and 
UK). Efforts were directed to create similar environment and 
technical conditions in both labs. Trials were conducted by 
usability experts. Users were given freedom to interact (the 
trial conductor would only intervene when really needed, or 
user asked for help). In what concerns the technical setup 
and specification, different modalities of interaction were 
configured: pointing resorted to the use of a Microsoft 
Kinect; for speech recognition we used the Loquendo SR 
engine; a simplified remote control, with less buttons than 
traditional ones and capable of controlling pointer 
coordinates using a gyroscopic sensor was made available; 
an iPad was used for tablet interaction; and a full 1080p 
HDMI TV with integrated speakers and a 32’’ screen was 
237
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

used for visual and audio output. User interactions and 
answers were video recorded. 
D. Design and Analysis 
We used a within-subjects design where all users ran the 
UIA and were evaluated in both adapted and non-adapted 
settings. The order in which they performed both versions of 
the EPG was randomized to counteract learning effects. 
Qualitative analysis was retrieved from the pre, intermediate 
and post-questionnaires. Quantitative data was retrieved 
from the UIA (user profile and interface preferences). 
Herein, we discarded quantitative measures like trial errors 
and time as the trials followed a semi-supervised 
methodology: the participants were motivated to perform the 
tasks on their own but they were free to ask questions when 
they felt lost. Wilcoxon Signed rank tests were used in 
comparisons to subjective measures between both versions 
of the EPG. Whereas to binomial measures, Mcnemar’s was 
performed. Cohen’s Kappa was used to assess the inter-
reliability of the profile ratings. 
VII. RESULTS 
The goal of this study was to make a preliminary 
assessment of our adaptation approach for interacting with 
the TV. We focused our attention on how well we are able to 
interactively retrieve information about the user and generate 
adaptations accordingly. We start by analyzing how well our 
UIA is able to profile the users along with an assessment of 
the initialization interface itself. Then, we analyze how the 
users interacted with the different modalities and their 
acceptance of the multimodal concept. To end, we evaluate 
subjective acceptance towards adaptation and GUIDE in the 
overall. 
A. Discovering Elderly Profiles with UIA 
Our take for adaptation relies on a User Model fed by the 
UIA. All participants in our study performed both the pre-
survey and the UIA. Twenty-nine out of forty profile 
assessments were performed similarly by the two methods 
(74%). The interrater reliability between the profiles 
assigned with the pre-survey and the UIA was found to be 
Kappa = 0.58 (p <.0.001), revealing a moderate agreement 
[15]. It is relevant to notice that the UIA enables the user to 
input preference values, something that goes beyond ability 
profile. This is likely to explain part of the mismatch (e.g., a 
user with no visual impairments is likely to prefer a higher 
contrast button when he is confronted with such an 
hypothesis). Another source of uncertainty may be the under-
statements by part of the users in the pre-survey. Indeed, in a 
questionnaire it is likely that part of the users fail to 
acknowledge some limitations while they clearly state them 
when confronted with an interface with options to surpass it. 
A deeper understanding of the mismatches that are not 
created by these observed flaws can only be retrieved in a 
more extensive evaluation by analyzing how both 
methodologies enable the users to improve performance. 
B. UIA evaluation by the Elderly 
As mentioned before, we have reproduced a realistic 
EPG and have focused our attention on improving its overall 
accessibility. Conversely, we have included the UIA, a 
component the users are not used to. It is important to assess 
how the users see this component and if they are willing to 
use such a thing to improve their performance.  
The participants took between 12 and 37 minutes to 
complete the UIA (M=22.8, SD=5.9). Once again, although 
they were discouraged to engage in long dialogues the 
participants were free to express their opinions and doubts 
during the UIA which increased the time to finalize the 
process. The UIA classified 16 people as profile A, 20 as 
profile B, and 4 as profile C. Table 2 presents the subjective 
ratings given by all the participants to the questions posed. 
Regarding the understanding of the purpose of the UIA 
(Question 1), 9 out of 40 (22%) did not understand the 
purpose of the UIA. This indicates that such a process should 
be better motivated or else it will be likely ignored by the 
users. In line with this, 11 out of 40 (28%) stated they would 
skip the process if they had the system at home (Q2). Five 
participants stated to find the process too long while four 
other were neutral about it (Q3) All the remaining thought it 
was neither too long nor tiring. Most users (35) thought the 
UIA was easy to follow and understand (Q4). Regarding the 
adaptations felt during the UIA (Q5), 26 participants stated 
to have noticed them. This is easily explained as 16 
participants were classified as profile A which means they 
had little or no adaptations done during the UIA. In sum, the 
users seem positive towards the UIA (Table 2) although it is 
clear that it should be well motivated and accompanied. 
Table 2: Subjective ratings of the UIA 
Question about the UIA 
Median IQR
Have you understood why we do the UIA? [1 ‐ Yes ; 2 ‐ No] 
1 
0 
If you have had the system at home, would you go through 
it or skip it?  [1 ‐ Would do it ; 2 ‐ Would skip it] 
1 
1 
Do you think the UIA is too long? [1‐ Yes;2 ‐ Neutral;3 ‐ No] 
3 
0 
Were the instructions easy enough to understand? [1 ‐ Yes; 
2 ‐ No] 
1 
0 
Did you notice any changes in the application while you 
were using it? [1 ‐ Yes ; 2 ‐ No] 
1 
1 
C. Evaluating Multimodal Interaction 
One of GUIDE’s main concepts relies on offering a set of 
modalities to the elderly population to fit their different 
profiles and abilities. In this study, in the Adapted setting we 
allowed the participants to perform the tasks with any of the 
previously described modalities. After each task, the users 
were asked about their preferred modality. Figure 1 presents 
their preference count after each task. Overall, it shows that 
both Speech as Tablet interaction were seen as positive 
improvements for interacting with the EPG. Conversely, the 
standard remote is seen by a part of the users as a safe option 
which they are not willing to trade easily. The gyroscopic 
remote was mildly used while pointing at the screen was the 
least preferred unimodal option. The users were also able to 
use Pointing and Speech together but this option was not 
revealed as interesting. This may be due to the complexity of 
238
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

using more than one modality and the reduced timespan of 
training. Further, the simplicity of the tasks is likely to be 
fitted with simpler selection approaches while a multimodal 
option is directed at more complex tasks. Nonetheless, given 
the population and the overall goal of simplifying processes 
and the aforementioned trends, it seems reasonable to 
suggest that a variety of unimodal interfaces seems to be 
adequate to fit the users’ needs and preferences. Interestingly 
enough, when explained the modalities available in the 
adapted version, most users stated that they would stick with 
standard remote. Conversely, during the trials they showed 
interest in using other modalities.  
Looking at the order of the trials, those that ran the 
adapted version first seem to be eager to try out different 
modalities, particularly the gyroscopic remote, tablet and 
speech, while those that performed the tasks previously with 
the Non-Adapted version, use a more conservative approach 
in the Adapted setting, giving preference to Speech followed 
by the standard remote and the tablet and ignoring the 
remaining. 
In both scenarios, about half the users shifted their 
preference during the Adapted trial while the other half 
sticked with their initial preference (first task). Preferences 
also diverge between users from different profiles. 
Participants from Profile A (no adaptation) showed a 
predilection for both the standard remote and speech and a 
mild preference for using the tablet. All the other modalities 
were ignored. As to Profile B (medium adaptation level), 
participants showed a less consistent selection. The tablet 
was the most preferred modality but all the other unimodal 
approaches were seen as useful by part of the users. As to 
Profile C (high adaptation level), consistency was once again 
revealed with preferences going for both remotes, pointing 
and 
speech
 
Figure 1: Modality preferences by task 
This first analysis on how older people resort to non-
conventional modalities reveals acceptance of the underlying 
multimodal concepts: results suggested that the population 
does not reject the usage of other control interfaces if these 
are adapted to their profiles and the task in hand. Further, 
Age did not show a significant correlation with acceptance 
nor any observable tendency on the set of preferred 
modalities, suggesting that this overall acceptance is 
pervasive to the age groups in our sample. 
D. Evaluating Adaptation 
Subjective acceptance to EPG adapted and non-adapted 
versions was evaluated through a statement to classify using 
a five-point Likert scale (1-the system has not supported me 
at all; 2-the system has supported me only in some parts of 
the task; 3-undecided; 4- the system has supported me in 
almost all the tasks; 5-the system has supported me at every 
moment). A Wilcoxon Signed Rank Test revealed a 
statistically significant minor difference between the EPG 
adapted (Mode=4, IQR = 1) and non-adapted versions 
(Mode=3, IQR=2), z=-1.665, p<.1, with a medium effect size 
(r=0.37). Although not significant statistically, subjective 
acceptance of adaptation showed to be slightly higher by 
participants in profile C, suggesting that the adaptations 
(more noticeable) improved the relation between the 
participants 
and 
the 
interface. 
In-detail 
acceptance 
comparisons of icon and text properties (evaluated as a 
dichotomous response to the suitability of the property) in 
both adapted and non-adapted settings revealed no 
significant differences. Apart from the large number of users 
with no adaptation, the baseline EPG was already designed 
as an accessible version. This may be the reason for the 
participants to rate the non-adapted EPG as adapted to their 
needs as well.  
E. Evaluating GUIDE concepts 
The adaptation and multimodality concepts pervade the 
GUIDE project. These preliminary trials seeked to assess 
how older people would react to such an adaptable and 
flexible system. While the aforementioned behaviors 
suggested that both adaptation and a wide coverage of 
control interfaces were positively seen and felt by the 
participants, we questioned them directly about their 
opinions on GUIDE. To this end, we performed a self-rating 
post-questionnaire to assess their overall opinion about the 
system (5-point Likert scale) and their Behavioral Intention 
to Use the System (7-point Likert scale).  
Table 3: Subjective ratings to GUIDE, Median[IQR} 
Overall opinion about the system 
Rating [1‐5]
Overall, I am satisfied with how easy it is to use 
4 [2] 
I am able to efficiently complete the tasks using the system
4 [2]
I feel comfortable using the system 
5 [1] 
It was easy to learn to use it 
4 [1] 
Whenever I make a mistake, I recover easily and quickly 
4 [2]
it is easy to find the information I needed 
4 [1] 
The interface of the system is pleasant 
5 [1] 
Overall, I am satisfied with the system 
5 [1]
Behavioural Intention to Use the System (BI)  
Rating [1‐7]
Intend to use GUIDE in the next semesters if I have access to it.
6 [2.5] 
Overall, all items were highly positively rated by most 
users (table 3). Regarding the opinion about the system, the 
overall satisfaction with it, the comfort and pleasure it 
guarantees were consistently highly rated as shown by the 
median and low dispersion. The ratings with slightly larger 
dispersions were about the easiness, efficiency and 
239
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

recovering of errors of the system, although still on the 
positive end of the rating scale (Median 5, IQR = 2). This 
can be explained by technical errors with a small part of the 
users regarding both the speech recognizer and the tablet 
device (temporarily unavailable wireless access). These users 
were unable to complete the tasks as they desired which 
translated in the mentioned lower ratings. This, although it 
can be explained by a technical glitch, is a relevant lesson 
learned as an unexpected, even localized, flaw, particularly 
in an adoption phase, is prone to damage the user’s 
relationship with the system as a whole.   
When asked about their Behavioral intention, users were 
also very positive which can be observed by an overall 
median value of 6 in 7 regarding intention of use. Here, the 
dispersion is a little larger. Five participants, although 
enjoying and understanding the benefits of GUIDE, argued 
to be happy with their systems at home, and therefore were 
neutral about a change. Only two participants showed to be 
unwilling to adopt GUIDE: one of the users showed 
preference to use the tablet for all tasks and experienced the 
aforementioned technical glitches which can explain his 
position; the other user did not have a TV at home, did not 
show an interest in having one, and presented an overall 
attitude towards technology (“I do not like technology”). 
Interestingly enough, he did not resort to conventional 
interaction modalities. 
VIII. DISCUSSION 
Upon analyzing the UIA process and its impact on 
adaptation along with the usage of the GUIDE system and its 
underlying concepts, we answer our research topics as 
follows: 
A. Deriving a suitable user adaptation profile through the 
UIA.  
The UIA aims at creating a user profile by performing a 
simple set of questions and interactive tests. Results showed 
that the UIA is able to match profiles obtained with an 
extensive survey in 74% of the cases. Further, the UIA 
showed to be more realistic than its paper-based counterpart 
as data is likely to be more accurate when the users are faced 
with their limitations rather than just being questioned about 
them. Moreover, the UIA gives space for preference and 
subjectiveness. In sum, we consider that adapted TV 
applications based on simple initialization profiling are 
feasible 
and 
likely 
to 
improve 
over 
traditional 
methodologies.   
B. Acceptance of the UIA.  
The UIA took over 12 minutes, averaging around 23 
minutes. This amount of time can be discouraging for an 
elderly user if the benefits are not clear. Taking in 
consideration that it is supposed to be ran only once, the 
participants showed to be very positive about it. This is 
supported by the almost general understanding of the 
purpose of the UIA: they understood the benefits of such an 
application and perceived the adaptations during the process. 
Most participants (35) considered the application easy to 
follow which indicates that although the concepts underlying 
the creation of the user model are complex, the interface to 
generate it is not. 
C. On perceived adaptation.  
Our analysis on adaptation is restricted to the subjective 
understanding and acceptance of the created profiles and 
consequent adaptation. Results showed that participants 
perceived the adaptation both during the UIA and the 
adapted EPG tasks. Also, those that were subject to 
adaptations rated the adaptive version as an improvement 
over the non-adapted one. The baseline EPG was already an 
improved accessibility-wise version over traditional EPGs, a 
fact that may have reduced the impact of the adaptations in 
such a short term evaluation. The impact of these adaptations 
needs 
further longitudinal evaluations supported by 
quantitative 
measurements 
to 
be 
further 
assessed. 
Nonetheless, the participants showed to be positive about the 
adaptations, which is relevant as a requirement for adoption, 
particularly in an elderly population. 
D. On Multimodality.  
It is commonplace to underlook the elderly population as 
one that is attached to traditional methods and unwilling to 
adopt new technologies. We acknowledge that the adoption 
of new technologies by the elderly is not straightforward and 
needs to be supported and accompanied. In this study, the 
participants showed to be eager to try new methodologies 
and experience their benefits over traditional counterparts. 
Speech recognition and interacting with the Tablet were tried 
during the tasks by most users and together achieve over 
50% of the participants’ preference. On the other hand, more 
conservative users, although resorting to other modalities 
when they saw fit, selected the standard remote as their main 
control interface. Providing an enriched set of modalities 
showed to fit the different user profiles. In that sense, 
multimodality seems to provide the flexibility argued in our 
motivation. Conversely, the unique multimodal option 
provided (Pointing + Speech) failed to prove the 
combination of modalities as useful for the target population. 
This may be due to the difficulty (easy) of the tasks in hand.  
E. Overall acceptance of GUIDE. 
Participants showed an overall positive acceptance 
towards GUIDE. Some participants stated that their EPGs 
were difficult and would desire a simpler system. They saw 
it in GUIDE. The avatar in the UIA was seen as a friendly 
helper and the overall usage of the EPG was considered 
simple and comfortable. Moderate opinions were given only 
in extreme cases of technology rejection (1 user) and 
technical glitches that disabled the users from completing the 
tasks as desired (inability to use their preferred interfaces). 
Participants stated to be interested in using GUIDE if it 
would be made available 
IX. 
CONCLUSIONS 
New interaction paradigms, supported by new modalities 
and applications, are transforming a classical appliance that 
is the TV. If not handled properly, this transformation can 
increase the access barriers to TV content for elderly users. 
240
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

In this paper, we assessed several of the proposals that the 
GUIDE project puts forward in order to increase the 
accessibility of TV applications. GUIDE aims to provide 
application 
developers 
with 
a 
multimodal 
adaptive 
framework and a set of functionalities that will increase their 
products’ accessibility, without demanding major changes in 
their development process. The assessment was based on a 
user trial, with 40 participants from two different countries. 
Being an adaptive framework, it relies on knowledge and 
information about users. We described how the GUIDE User 
Model was built and how it integrates with the framework. 
Essential for the integration, is the UIA, a process that 
streamlines user profile identification, based on short number 
of tasks and questions. We present an assessment of the 
efficacy of this process, concluding that it is possible to 
reliably identify user profiles, while also recognizing ways in 
which to further improve the process. From the user’s point 
of view, the process motivation was understood, and it was 
considered easy enough, although also here we were able to 
find ways to improve it. 
Being a multimodal framework, it makes use of several 
input and output modalities. The paper assessed the usage of 
these modalities in the context of a realistic EPG application. 
Conversely to what could be expected, the traditional remote 
control was not the most favored modality, being surpassed 
by speech and tablet as preferred modalities. Effects of 
adaptation on this application, in spite of an already 
accessible non-adapted version and the limited amount of 
interaction time with it, achieved a statistically significant 
minor difference between subjective acceptance of adaptive 
and non-adaptive versions. 
These results, together with the positive acceptance of the 
GUIDE concepts and their expected impact in the quality of 
life of its users, validate the approach followed so far and 
pave the road for the project’s future developments, which 
will be verified in a longitudinal trial for better assessing the 
effects of adaptation 
A. Relation with the Previous Study.  
Regarding the use of modalities, speech interaction was 
singled out as the most attractive modality. In the first study, 
a Wizard-of-Oz approach was used to replace the speech 
recognition engine, and we questioned how that might have 
contributed to the results. In this follow-up study, where a 
speech recognition engine was used, we can see that speech 
remains the preferred modality, overcoming any technical 
issues that might be raised. It seems safe to say that speech 
plays an important role in promoting the adoption of these 
systems, and efforts to ensure its adequate operation are 
justified by the satisfaction it provides users with. Tablets, 
although not fully integrated with the system in the initial 
study, collected a positive response from participants, with 
92% of them considering interacting with a TV using the 
Tablet. This tendency was confirmed in this study, with 
Tablet interaction being the second most used modality to 
interact with the EPG. Finally, regarding the clustering 
process, by increasing the number of users available we have 
updated the profiling process, which resulted in a more 
accurate representation of the users’ characteristics and a 
more precise identification of the relative importance of each 
variable. For example, contrast sensitivity is now more 
relevant than the capability of seeing at distance or at night, 
and grip strength is now more relevant than any other motor 
related variable. 
REFERENCES 
[1] Anderson J. R. and Lebiere C. "The Atomic Components of 
Thought”, Lawrence Erlbaum Associates, 1998. 
[2] Angst F. et. al.., Prediction of grip and key pinch strength in 
978 healthy subjects, BMC Musculoskeletal Disorders 2010. 
[3] Biswas P. and Langdon P.  (2012) Developing multimodal 
adaptation algorithm for mobility impaired users by 
evaluating their hand strength, IJHCI, Taylor & Francis. 
[4] Biswas P., Langdon P. & Robinson P. (2012) Designing 
inclusive interfaces through user modelling and simulation, 
IJHCI, Taylor & Francis, Vol; 28, Issue 1, 2012 
[5] Coelho J., Duarte C., Biswas P. and Langdon P., Developing 
Accessible TV Applications, Proceedings of ASSETS2011 
[6] Colour 
Blindness 
Tests 
2008. 
 
Available 
at: 
http://www.kcl.ac.uk/teares/gktvc/vc/lt/colourblindness/cblind
.htm, Accessed in 12th August February, 2008 
[7] Czaja, S. and Lee, C. (2007). The impact of aging on access 
to technology. Univ.Access Inf. Soc. 5 (4), 341-349. 
[8] Dickinson, A. and Gregor, P., 2006. Computer use has no 
demonstrated impact on the well-being of older adults. Intl. 
Journal of Human Computer Studies. 68, 744-753. 
[9] Duffy V. G. "Handbook of Digital Human Modeling: 
Research for Applied Ergonomics and Human Factors 
Engineering." Boca Raton, FL, USA: CRC Press, 2008 
[10] Gajos K., et al.Automatically generating user interfaces 
adapted to users' motor and vision capabilities. UIST 2007. 
[11] Hanson, V. (2009) Age and web access: the next generation. 
Proceedings of W4A 2009 
[12] Harley D. & al. (2009). Age Matters: Bridging the Generation 
Gap through Technology-Mediated Interaction. In Proc. of 
CHI. 
[13] Holzinger, A., Ziefle, M. and Rocker, C. Human-computer 
interaction 
and 
usability 
engineering 
for 
elderly 
(HCI4AGING). In Proceedings of ICCHP'10. 
[14] Horovitz, E. and colleagues (2010), Microsoft Research, The 
Lumiere Project: Bayesian User Modeling for Inferring the 
Goals and Needs of Software Users. 
[15] Landis, J. R., Koch, G. G. (1977). The measurement of 
observer agreement for categorical data. Biometrics 33:159-
174. 
[16] Oakley, I., Brewster, S. A., and Gray, P. D.: Solving multi-
target haptic problems in menu interaction. In Proc. CHI’01. 
[17] Oviatt, S. Multimodal interactive maps: Designing for human 
performance. Human-Computer Interaction, 1997. 93-129 
[18] Oviatt, S. L., DeAngeli, A., and Kuhn, K. Integration and 
synchronization of input modes during multimodal human-
computer interaction. CHI '97. 
[19] Oviatt, S.L.: Mutual Disambiguation of Recognition Errors in 
a Multimodal Architecture. CHI’99.  
[20] S. Kieffer, A. et al., “Towards StandardizedPen-Based 
Annotation of Breast Cancer Findings”, Proceedings HCII09. 
[21] Sharon Oviatt, Trevor Darrell, and Myron Flickner. 2004. 
Multimodal interfaces that flex, adapt, and persist. Commun. 
ACM 47, 30-33. 
[22] Stephanidis, C. et al: Adaptable and Adaptive User Interfaces 
for Disabled Users in the AVANTI Project. S.Triglia et al. 
(Eds.): IS&N’98, 153-166, 1998 
241
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
242
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

