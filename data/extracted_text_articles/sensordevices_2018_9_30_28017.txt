An Approach to Behavioural Distraction Patterns Detection and Classification in a 
Human-Robot Interaction
Bruno Amaro1, Vinicius Silva2 
1Dept. of Industrial Electronics  
2Algoritmi Research Centre  
University of Minho 
Guimarães, Portugal 
e-mail: [a70785, a65312]@alunos.uminho.pt 
Filomena Soares1,2, João Sena Esteves1,2  
1Dept. of Industrial Electronics  
2Algoritmi Research Centre  
University of Minho 
Guimarães, Portugal 
e-mail: [fsoares, sena]@dei.uminho.pt
 
 
Abstract—The capacity of remaining focused on a task can be 
crucial in some circumstances. In general, this ability is 
intrinsic in a human social interaction and it is naturally used 
in any social context. Nevertheless, some individuals have 
difficulties in remaining concentrated in an activity, resulting 
in a short attention span. In order to recognize human 
distraction behaviours and capture the user attention, several 
patterns of distraction, as well as systems to automatically 
detect them, have been developed. One of the most used 
distraction patterns detection methods is based on the 
measurement of the head pose and eye gaze. The present work 
proposes a system based on a RGB camera, capable of 
detecting the distraction patterns, head pose, eye gaze, blinks 
frequency, and the distance of the user to the camera, during 
an activity, and then classify the user's state using a machine 
learning algorithm. The goal is to interface this system with a 
humanoid robot to consequently adapt its behaviour taking 
into account the individual affective state during an emotion 
imitation activity. 
Keywords-Human-Robot 
Interaction; 
ZECA 
Robot; 
Distraction Patterns; Emotional States; Machine Learning. 
I. 
 INTRODUCTION 
In general, the ability of concentrating on a task for an 
extended period of time is a paramount skill to develop. As 
observed by various authors [1][2], when a person tries to 
reach a particular object, the observer’s gaze arrives at the 
target before the action is completed. Additionally, the 
predictive gaze provides the time for the observer to plan and 
execute an action towards a goal. 
Following this idea, the attention span and the eye gaze 
can be very important elements in social interaction, as they 
can help an individual to perceive the goals of others. 
However, some individual’s present a low attention span, 
especially children with Autism Spectrum Disorder (ASD) 
[3] when, for example, focusing on things that do not interest 
them, 
i.e., 
activities 
that 
involve 
shared 
attention. 
Additionally, in accordance to the literature [4][5], authors 
suggest that individuals with ASD attend less to faces than 
typically developing individuals. 
In an attempt to increase and captivate these individuals’ 
interest, authors have been proposing new technological 
tools in the field of assistive robotics to help users with 
special needs in their daily activities. Assistive robots are 
designed to identify, measure, and react to social behaviours, 
being repeatable and objective offering an exceptional 
occasion for quantifying social behaviour [6]. 
They can be a social support to motivate children, 
socially educate them and beyond that to help transferring 
knowledge. Furthermore, research with assistive robots have 
showed that, in general, individuals with ASD express 
elevated interest while interacting with robots: increase 
attention [7], recognition and imitation abilities [8], verbal 
utterances [7], among others. According to studies, it was 
observed that children with ASD can exhibit certain positive 
social behaviours when interacting with robots in contrast to 
what is perceived when interacting with their peers, 
caregivers, and therapists [9]. Furthermore, few projects 
worldwide pursue to include robots as part of the 
intervention program for individuals with autism [10][11]. 
These robots are presented with different embodiments, 
varying their physical appearance from simple designs, e.g., 
four-wheeled 
mobile 
robots, 
to 
many 
levels 
of 
anthropomorphic forms, including humanoid [12], animal-
like [13], and machine-like systems [14]. 
Recently, the research in the area of assistive robotics 
have moved to using robots with a humanoid design, since it 
can promise a great potential for generalisation, especially in 
tasks of imitation and emotion recognition which can be 
harder if the robot does not present a human form 
[11][12][15]. 
The majority of the systems proposed in the literature are 
controlled using the Wizard-of-Oz (WOZ) setup, meaning 
that in fact the robot does not adapt its behaviour to the 
children's actions as it does not perceive them [16]. 
Additionally, there have been studies with assistive robots 
with the goal of measuring the children eye gaze duration 
[17][18] and direction. However, this analysis is not 
performed in real-time by the robot, meaning that usually the 
sessions are recorded, and the metrics are manually 
quantified during a post-analysis of the videos. 
More recently, there has been a concern in developing 
more adaptive approaches to interact with children with 
ASD. These recent approaches usually use wearable and 
non-wearable technologies in order to measure the children 
affective states.  
The work developed by [19] consisted in controlling the 
robot reactions and responses, by using a combination of 
hardware, wearable devices, and software algorithms to 
152
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-660-6
SENSORDEVICES 2018 : The Ninth International Conference on Sensor Device Technologies and Applications

measure the affective states (e.g., eye gaze attention, facial 
expressions, vital signs, skin temperature, and skin 
conductance signals) of children with ASD. The wearable 
devices that the authors used were a sensorized t-shirt to 
acquire the subject physiological signals (ECG and 
respiration rate), wireless electrode bands to collect the 
user’s skin temperature and Electro Dermal Activity (EDA) 
and the HATCAM, a system composed of a hat with markers 
and a grid of cameras to estimate the user’s gaze. The 
developed system, FACET, includes a multisensory room in 
which a psychologist drives a stepwise protocol involving 
the android FACE and the autistic subject. This interaction 
between the robot and the subject is tailored by the therapist.  
A preliminary test was conducted with six male subjects: 
four individuals with ASD aged between 15 and 22 years old 
and two typically developing individuals aged between 15 
and 17 years old. By analysing the results, the authors 
conclude that the subjects were calm during the activity and 
responded well to the robot. Additionally, the results 
confirmed that the system can be used as an innovative tool 
during the intervention sessions with subjects with ASD. 
Bekele and colleagues [20][21] developed and later 
evaluated a humanoid robotic system capable of intelligently 
managing joint attention prompts and adaptively respond 
based on gaze and attention measurements. The system is 
composed of a humanoid robot with augmented vision by 
using a network of cameras for real-time head tracking using 
a distributed architecture. In order to track the child’s head 
motion, a hat with markers was used. Thus, based on the 
cues from the child’s head motion, the robot adapts its 
behaviour to generate prompts and reinforcements. A pilot 
usability study was conducted with six children with ASD.  
The results allowed to conclude that the children directed 
their gaze towards the robot when it prompted them with a 
question. The authors suggested that robotic systems, 
endowed with enhancements for successfully captivating the 
child’s attention, might be capable to meaningfully enhance 
skills related to coordinated attention. 
Following this trend, the present work proposes the 
development of a framework that uses a RGB camera to 
interface with the humanoid robot ZECA (Zeno Engaging 
Children with Autism). Generally, in order to track the user’s 
attention patterns, the literature approaches combine the use 
of several wearable sensors, which can be invasive, with 
non-wearable sensors. Thus, in order to become less 
invasive, the present approach uses only one camera to 
estimate eye gaze, head motion, the blinks frequency and the 
distance of the user to the camera. The proposed system 
allows to infer children distraction patterns (if any) when 
performing a task and to adapt the robot behaviour 
accordingly. 
The final goal of the present work is to collect the 
selected patterns (head pose, eye gaze, blink frequency, and 
distance of the user to the camera), and based on these 
patterns classify the user state, attentive or distracted, during 
a laboratorial activity.  
The paper is organized as follows: in Section 2 the 
experimental set-up is presented. Section 3 presents the 
experimental methodology describing the system modules as 
well as the robot behaviour. The results and their discussion 
are presented in Section 4. The conclusion and future work 
are addressed in Section 5.  
II. 
EXPERIMENTAL SETUP 
The proposed structure (Figure 1) was designed to use 
only an RGB camera to detect patterns of user distraction in 
order to adapt the behaviour of the robot during the activity. 
In addition to an RGB camera, the experimental setup 
uses a computer and the ZECA humanoid robot. 
 
 
Figure 1.  Experimental setup: starting from the left, the RGB camera; in 
the centre, the computer; and on the right, the humanoid robot ZECA. 
The camera used in the present work is a RGB camera, 
more specifically the Microsoft VX-1000 camera. This 
device has a minimum resolution of 320 by 240 pixels and a 
maximum resolution of 640 by 480 pixels. The camera 
dimensions are 5.5 cm of width, 6.8 cm of height, and 5.3 cm 
of depth. For a better detection and feature extraction, the 
camera is placed on the robot chest, being at the same level 
as the user’s face.  
The Zeno R50 RoboKind humanoid child-like robot 
ZECA is a robotic platform that has 34 degrees of freedom: 4 
are located in each arm, 6 in each leg, 11 in the head, and 1 
in the waist. The robot is capable of expressing facial cues 
thanks to the servo motors mounted on its face and a special 
material, Frubber, which looks and feels like human skin, 
being the major feature that distinguishes Zeno R50 from 
other humanoid robots. 
III. 
EXPERIMENTAL METHODOLOGY 
Since the purpose of this work is to know whether the 
user is distracted, a study was done to determine which 
patterns best fit in this evaluation. The following patterns 
were identified: the eye gaze, the head orientation, the eyes 
blinking frequency, and the distance of the user to the 
camera. These patterns will be detected during an emotion 
imitation activity.  
After the extraction of these patterns, the classification 
was done using an algorithm based on machine learning, 
thus classifying the user as attentive or distracted. 
The design of this activity consists in ZECA displaying a 
facial expression and asking the child to imitate it. Then, the 
robot automatically verifies if the answer is correct and 
responds accordingly. Meanwhile, if the system detects any 
153
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-660-6
SENSORDEVICES 2018 : The Ninth International Conference on Sensor Device Technologies and Applications

distraction pattern in the child, the robot adapts its behaviour, 
encouraging the child to return and participate in the activity. 
A. Affective state detection and classification 
Figure 2 presents the block diagram of the overall system 
considering the detection of distraction patterns, as well as 
the classification of the emotional state of the chid during an 
activity.   
 
Figure 2.  System flowchart highlighting the defined modules for detecting 
distraction patterns and emotional states: Models training, Feature 
extraction, Models predictions, and Robot prompt. 
The first module, Models Training, consists in training 
each machine learning model with a previously defined 
database.  
Three other modules are defined: Feature Extraction, 
Models predictions, and Robot prompt.   
For the feature extraction an algorithm based on the 
OpenFace [22]-[25] library is used to track the face and eyes 
of the user, as well as the user’s Action Units (AUs).  
The OpenFace is an open source library that makes 
available a collection of facial landmarks (a total of 68 facial 
points) and AUs based on the Facial Action Coding System 
(FACS). The FACS system, developed by Ekman and 
Friesen [26], allows researchers to analyse, and classify 
facial expressions in a standardized framework. This system 
associates the action of the facial muscles to the changes in 
facial appearance. The basic metric of the FACS system are 
the AUs which are actions performed by a muscle or a group 
of muscles. Ekman also proposed the six basic emotions 
[26], also considered the six universal emotions – happiness, 
sadness, anger, surprise, fear, and disgust.  
Additionally, OpenCV is also used due to its suitability 
and applicability in computer vision solutions.  
Generally, some researches use machine learning 
techniques in order to infer the user emotional states [27]. 
Then, by using machine learning methods, for example 
Support 
Vector 
Machines 
(SVM) 
or 
k-Nearest 
Neighbours (k-NN), the attention patterns and the user facial 
expression will be recognized.  
Once the patterns corresponding to distraction and 
emotional states are detected, the robot should trigger the 
corresponding action (robot prompt module) to acknowledge 
the emotion, to give reinforcement and, if necessary, to 
capture the user's attention again. 
B. Robot Behaviour  
The general procedure during an activity is: 1) ZECA 
greets the researcher and the user; 2) ZECA asks which 
activity shall be played; 3) The selected activity starts and 
continues until the experimenter decides to end it.  
In the activity, the robot prompts a different behaviour 
accordingly to the results, the child attentiveness, and 
response. Thus, according to the classifier output, four 
conditions may occur:  
• 
the user is attentive and answer to the robot prompt; 
• 
the user is attentive but does not answer to the robot 
prompt;  
• 
the user is distracted and does not answer to the robot 
prompt;  
• 
the user is distracted but answer to the robot prompt. 
 
In order to adapt the robot behaviour accordingly to the 
four different conditions mentioned, a state machine model is 
proposed [28].  
After classification it is necessary to take an action, that 
is, if the robot classified the user as attentive, then it will 
continue the activity. Then, ZECA revises the patterns, so as 
to always know if the user is attentive. If it has previously 
considered the user inattentive, then it will trigger an action 
in order to capture the user's attention again. 
At the end of the cycle, it is always checked if the 
activity is complete; if it is accomplished, the robot does not 
need to revise the defaults; if it is not, then the robot will 
have to continue to analyse the user defaults. Figure 3 
depicts the general procedure that happens during an activity. 
154
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-660-6
SENSORDEVICES 2018 : The Ninth International Conference on Sensor Device Technologies and Applications

 
Figure 3.  Robot behaviour flowchart.  
IV. 
RESULTS AND DISCUSSION  
In order to display the user’s information and to monitor 
the activity, a Graphical User Interface (GUI) was 
developed. Figure 4 shows the GUI where the head pose 
angles (pitch, roll, and yaw), the number of blinks, the gaze 
estimation, the distance of the user to the camera, and the 
prediction of the classification model are displayed.  
 
 
Figure 4.  The GUI displaying the webcam feed, the user’s eye gaze, the 
head pose, the number of blinks, the distance of the user to the camera, and 
the prediction of the classification model. 
Several distraction and attention behaviours were 
simulated in a laboratorial environment, where the user sat in 
front of the camera. The corresponding patterns were stored 
in a training database (head pose, eye gaze, blink frequency, 
distance of the user to the camera). This database was built 
with six different subjects with 500 samples each. Then, with 
Accord [29], two classification methods, Gaussian SVM and 
k-NN [27], were used.  
To test the robustness of the system a test database was 
created and tested with these two methods, in order to find 
out the best method for this type of classification. This new 
database was composed by three different subjects and with 
100 samples each. The same patterns were stored in the test 
database (head pose, eye gaze, blink frequency, distance of 
the user to the camera). 
Tables 1 and 2 represent the confusion matrix for the 
Gaussian SVM method with the two databases created. 
TABLE I.  
CONFUSION MATRIX FOR GAUSSIAN SVM METHOD WITH 
TRAINING DATABASE. 
Predicted Class 
Actual Class 
Attentive 
Distracted 
Attentive 
81.0% 
5.1% 
Distracted 
19.0% 
94.9% 
TABLE II.  
CONFUSION MATRIX FOR GAUSSIAN SVM METHOD WITH 
TEST DATABASE. 
Predicted Class 
Actual Class 
Attentive 
Distracted 
Attentive 
63.5% 
0% 
Distracted 
36.5% 
100% 
 
By analysing Table 1, it is possible to see that the results 
are satisfactory, since the accuracy of the attentive and 
distracted classes are above 80%. Regarding the confusion 
matrix with the test database (Table 2), it is possible to see 
that the accuracy of the distracted class is 100%. Conversely, 
the accuracy of the attentive class has decreased. Although, 
in the test database (Table 2), the accuracy of the attentive 
class decreased, the distracted class accuracy increased, 
meaning that the system can accurately detect when the user 
is distracted which is the main goal of the proposed system. 
In Tables 3 and 4, it is presented the confusion matrix for 
the k-NN method with the two databases created. 
TABLE III.  
CONFUSION MATRIX FOR K-NN METHOD WITH TRAINING 
DATABASE. 
Predicted Class 
Actual Class 
Attentive 
Distracted 
Attentive 
100% 
0% 
Distracted 
0% 
100% 
TABLE IV.  
CONFUSION MATRIX FOR K-NN METHOD WITH TEST 
DATABASE. 
Predicted Class 
Actual Class 
Attentive 
Distracted 
Attentive 
65.3% 
2.8% 
Distracted 
34.7% 
97.2% 
 
Analysing Table 3, the k-NN performed better in the 
training step in comparison to the Gaussian SVM method. 
However, by analysing the performance of the k-NN method 
in with the test database (Table 4), the performance, in 
general, decreased.  
155
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-660-6
SENSORDEVICES 2018 : The Ninth International Conference on Sensor Device Technologies and Applications

Tables 5 and 6 present the values of accuracy, the 
Matthews Correlation Coefficient (MCC), the sensitivity, the 
specificity, the precision, and the Area Under the Curve 
(AUC) obtained with the Gaussian SVM and the k-NN 
methods for the training and test databases, respectively. 
TABLE V.  
METRICS OBTAINED WITH THE GAUSSIAN SVM METHOD 
FOR THE TRAINING AND TEST DATABASES. 
Metrics 
Database 
Training  
Test  
Accuracy 
88.1% 
80.3% 
MCC 
76.9% 
66.6% 
Sensitivity 
81.0% 
63.5% 
Specificity 
94.9% 
100% 
Precision 
93.8% 
100% 
AUC 
88.0% 
81.8% 
TABLE VI.  
METRICS OBTAINED WITH THE K-NN METHOD FOR THE 
TRAINING AND TEST DATABASES. 
Metrics 
Database 
Training  
Test  
Accuracy 
100% 
79.9% 
MCC 
100% 
64.7% 
Sensitivity 
100% 
65.3% 
Specificity 
100% 
97.2% 
Precision 
100% 
96.5% 
AUC 
100% 
81.3% 
 
Analysing both Tables, it is possible to conclude that, in 
general, the SVM with the Gaussian kernel achieved better 
results with an accuracy of 80.3% when compared with the 
k-NN method (accuracy: 79.9%) with the test database. 
Although the k-NN method obtains excellent results with the 
training database, the same does not happen with the test 
database, having lower results in some of the metrics. The 
Gaussian SVM method, despite obtaining slightly lower 
results with the training database when compared to the k-
NN, the overall performance is slightly better and more 
consistent than the k-NN method with the test database. 
Thus, in general, it can be concluded that the results obtained 
with the test database, using Gaussian SVM method, are 
overall better than when using the k-NN method.  
As the Gaussian SVM trained model showed better 
results, it was used in a real-time laboratorial environment 
evaluation. The user sat in front of the camera and performed 
simulated behaviours, attentive and distracted. The system 
automatically classified the user state. Some of the results for 
different positions obtained using this method are shown in 
Figure 5. 
V. 
CONCLUSION AND FUTURE WORK 
The ability of concentrating on a task for an extended 
period of time is a paramount skill to develop. In general, 
when a person tries to reach a particular object, the 
observer’s gaze arrives at the target before the action is 
completed. Thus, the predictive gaze provides the time for 
the observer to plan and execute an action towards a goal. 
During a social interaction, the predictive gaze and the 
attention span can be crucial elements, as they can help an 
individual to perceive the goals of the others. 
 
 
Figure 5.  The GUI displaying the classification considering different 
poses (results using Gausian SVM method). The classifier output is 
presented in the interface (red rectangles).  
However, some individuals present a low attention span, 
especially children with Autism Spectrum Disorder (ASD), 
and in general they attend less to faces than typically 
developing individuals. 
Researches have been using robotic platforms for 
promoting social interaction with individuals with ASD. 
Furthermore, it has already been proven that the use of 
robots encourages the promotion of social interaction and 
skills lacking in children with ASD. However, most of these 
systems are controlled 
remotely and 
cannot adapt 
automatically to the situation. Even those who are more 
autonomous still cannot perceive whether or not the user is 
paying attention to the instructions and the actions of the 
robot. Additionally, some of these systems use an array of 
cameras and a hat with markers in order to infer the user 
gaze. 
The present paper concerns the development of a 
framework to estimate the user/child affective states. The 
system is based on a camera to detect and follow the face and 
contours, and extract the user head orientation angles (yaw, 
pitch, and roll), eye gaze, action units, blinking frequency, 
and the distance between the user and the camera. It applies 
an algorithm based on Opencv functions and OpenFace 
library. Using the features extracted from the user, and a 
machine learning model (Gaussian SVM or k-NN) it is 
possible to recognize these patterns and classify the user as 
distracted or attentive. 
In general, the method that registered a better accuracy 
was the SVM with the Gaussian kernel (accuracy: 80.3%); 
the k-NN method had slightly lower results (accuracy: 
79.9%) both with the test database.  
As future work, it is necessary to recognize these patterns 
(distraction and emotional states) during a triadic emotion 
imitation activity with children with ASD (child, researcher 
and robot ZECA), where the child facial expression is 
recognized through facial features in real-time. Robot 
behaviour will be constantly adapted taking into account 
child affective state. Through the use of a friendly interface, 
the teacher/therapist will be able to access the child’s 
performance as well as to monitor the running intervention 
activity.  
To accomplish this, the developed system is going to be 
evaluated in different scenarios in order to assess its 
156
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-660-6
SENSORDEVICES 2018 : The Ninth International Conference on Sensor Device Technologies and Applications

performance. A first evaluation is going to be conducted in a 
school environment with typically developing children with 
the purpose of detecting the system constraints and to tune 
the conditions of the experimental scheme. After this first 
validation in a controlled environment, a second test should 
be developed in a real-world context with individuals with 
ASD. 
ACKNOWLEDGEMENTS 
The authors would like to express their acknowledgments 
to COMPETE: POCI-01-0145-FEDER-007043 and FCT – 
Fundação para a Ciência e Tecnologia within the Project 
Scope: UID/CEC/00319/2013. Vinicius Silva also thanks 
FCT 
for 
the 
PhD 
scholarship 
SFRH/BD/ 
SFRH/BD/133314/2017. 
REFERENCES 
[1] G. Gredebäck and T. Falck-Ytter, “Eye Movements During 
Action Observation.,” Perspect. Psychol. Sci., vol. 10, no. 5, 
pp. 591–8, Sep. 2015. 
[2] J. R. Flanagan and R. S. Johansson, “Action plans used in 
action observation,” Nature, vol. 424, no. 6950, pp. 769–771, 
2003. 
[3] C. Chevallier et al., “Measuring social attention and 
motivation in autism spectrum disorder using eye-tracking: 
Stimulus type matters,” Autism Res., vol. 8, no. 5, pp. 620–
628, 2015.  
[4] A. Klin, W. Jones, R. Schultz, F. Volkmar, and D. Cohen, 
“Visual fixation patterns during viewing of naturalistic social 
situations as predictors of social competence in individuals 
with autism,” Arch. Gen. Psychiatry, vol. 59, no. 9, pp. 809–
816, 2002.  
[5] G. Dawson, S. J. Webb, and J. Mcpartland, “Understanding 
the Nature of Face Processing Impairment in Autism: Insights 
From Behavioral and Electrophysiological Studies,” Dev. 
Neuropsychol., vol. 27, no. 3, pp. 403–424, 2005. 
[6] A. Tapus, M. J. Mataric´, and B. Scassellati, “The grand 
challenges in socially assistive robotics,” IEEE Robot. 
Autom. Mag., vol. 14, no. 1, pp. 35–42, 2007.  
[7] E. S. Kim, R. Paul, F. Shic, and B. Scassellati, “Bridging the 
Research Gap: Making HRI Useful to Individuals with 
Autism,” J. Human-Robot Interact., vol. 1, no. 1, pp. 26–54, 
2012. 
[8] I. Fujimoto, T. Matsumoto, P. R. S. de Silva, M. Kobayashi, 
and M. Higashi, “Mimicking and evaluating human motion to 
improve the imitation skill of children with autism through a 
robot,” Int. J. Soc. Robot., vol. 3, no. 4, pp. 349–357, 2011.  
[9] D. J. Ricks and M. B. Colton, “Trends and considerations in 
robot-assisted autism therapy,” in 2010 IEEE International 
Conference on Robotics and Automation, 2010, pp. 4354–
4359. 
[10] K. Dautenhahn, “Design issues on interactive environments 
for children with autism.,” 3Th Int. Conf. Disabil. ,Virtual 
Real. Assoc. Technol., pp. 153–162, 2000.  
[11] P. Chevalier et al., “Dialogue Design for a Robot-Based Face-
Mirroring Game to Engage Autistic Children with Emotional 
Expressions,” in Lecture Notes in Computer Science 
(including subseries Lecture Notes in Artificial Intelligence 
and Lecture Notes in Bioinformatics), 2017, vol. 10652 
LNAI, pp. 546–555.  
[12] S. C. C. Costa, “Affective robotics for socio-emotional 
development in children with autism spectrum disorders,” 
Oct. 2014. 
[13] E. S. Kim et al., “Social Robots as Embedded Reinforcers of 
Social Behavior in Children with Autism,” J. Autism Dev. 
Disord., vol. 43, no. 5, pp. 1038–1049, May 2013. 
[14] F. Michaud et al., “Autonomous Spherical Mobile Robot for 
Child-Development Studies,” IEEE Trans. Syst. Man, 
Cybern. - Part A Syst. Humans, vol. 35, no. 4, pp. 471–480, 
Jul. 2005. 
[15] M. Begum et al., “Measuring the Efficacy of Robots in 
Autism Therapy,” in Proceedings of the Tenth Annual 
ACM/IEEE International Conference on Human-Robot 
Interaction - HRI ’15, 2015, pp. 335–342. 
[16] P. Pennisi et al., “Autism and social robotics: A systematic 
review,” Autism Research, vol. 9, no. 2. pp. 165–183, 2016.  
[17] B. Robins, K. Dautenhahn, and P. Dickerson, “From Isolation 
to Communication: A Case Study Evaluation of Robot 
Assisted Play for Children with Autism with a Minimally 
Expressive Humanoid Robot,” in 2009 Second International 
Conferences on Advances in Computer-Human Interactions, 
2009, pp. 205–211. 
[18] S. Costa, H. Lehmann, K. Dautenhahn, B. Robins, and F. 
Soares, “Using a Humanoid Robot to Elicit Body Awareness 
and Appropriate Physical Interaction in Children with 
Autism,” Int. J. Soc. Robot., vol. 7, no. 2, pp. 265–278, Apr. 
2015. 
[19] D. Mazzei et al., “Development and evaluation of a social 
robot platform for therapy in autism,” in 2011 Annual 
International Conference of the IEEE Engineering in 
Medicine and Biology Society, 2011, pp. 4515–4518. 
[20] E. T. Bekele et al., “A step towards developing adaptive 
robot-mediated intervention architecture (ARIA) for children 
with autism,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 
21, no. 2, pp. 289–299, 2013.  
[21] E. Bekele, J. A. Crittendon, A. Swanson, N. Sarkar, and Z. E. 
Warren, “Pilot clinical application of an adaptive robotic 
system for young children with autism.,” Autism, vol. 18, no. 
5, pp. 598–608, Jul. 2014. 
[22] T. Baltrušaitis, P. Robinson, and L. P. Morency, “Constrained 
local neural fields for robust facial landmark detection in the 
wild,” in Proceedings of the IEEE International Conference 
on Computer Vision, 2013, pp. 354–361.  
[23] T. Baltrusaitis, P. Robinson, and L. P. Morency, “OpenFace: 
An open source facial behavior analysis toolkit,” in 2016 
IEEE Winter Conference on Applications of Computer 
Vision, WACV 2016, 2016.  
[24] T. Baltrusaitis, M. Mahmoud, and P. Robinson, “Cross-
dataset learning and person-specific normalisation for 
automatic Action Unit detection,” in 2015 11th IEEE 
International Conference and Workshops on Automatic Face 
and Gesture Recognition (FG), 2015, pp. 1–6.  
[25] E. Wood et al., “Rendering of Eyes for Eye-Shape 
Registration 
and 
Gaze 
Estimation,” 
in 
2015 
IEEE 
International Conference on Computer Vision (ICCV), 2015, 
pp. 3756–3764. 
[26] P. Ekman and W. V. Friesen, “Facial Action Coding System: 
A Technique for the Measurement of Facial Movement,” in 
Consulting Psychologists Press, 1978. 
[27]  V. Silva et al., “Real-time emotions recognition system,” in 
2016 
8th 
International 
Congress 
on 
Ultra 
Modern 
Telecommunications and Control Systems and Workshops 
(ICUMT), 2016, pp. 201–206. 
[28] B. Amaro, V. Silva, F. Soares, and J. S. Esteves, “Building a 
Behaviour Architecture: An Approach for Promoting Human-
Robot 
Interaction,” 
in 
Lecture 
Notes 
in 
Electrical 
Engineering, Guimarães: Springer, Cham, 2019, pp. 39–45. 
[29] “Accord.NET Machine Learning Framework.” [Online]. 
Available: http://accord-framework.net/. [Accessed: 14-Jul-
2018].
157
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-660-6
SENSORDEVICES 2018 : The Ninth International Conference on Sensor Device Technologies and Applications

