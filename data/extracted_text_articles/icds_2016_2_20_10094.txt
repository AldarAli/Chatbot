Privacy Preserving Reconstruction-based Techniques and Randomisation-based
Methods for Calculating Surveys’ Statistics and Participants Sampling in Deliberative
Consultations
Piotr Andruszkiewicz
Institute of Computer Science
Warsaw University of Technology
Warsaw, Poland
Email: P.Andruszkiewicz@ii.pw.edu.pl
Abstract—In deliberative consultations, the most important are
the opinions of residents that want to discuss an important issue.
In order to encourage them to participate in such consulta-
tions, besides the Internet platform that facilitates the whole
process of consultations, privacy preserving techniques should
be employed. In this paper, we propose a framework for pri-
vacy incorporation in deliberative consultation that will improve
eGovernment services provided for digital society. We present the
solution for reconstruction-based privacy preserving technique
and randomisation-based methods. The proposed framework
enables a scientist to prepare a list of candidates and calculate
statistics over privacy preserved survey data in deliberative
consultations.
Keywords–Privacy
Preserving;
Reconstruction-based
tech-
niques; Randomisation-based methods; deliberative consultations.
I.
INTRODUCTION
In deliberative consultations [1], residents discuss issues
important for them. An example could be a deliberative
consultation run by a local government in order to ﬁnd and
understand opinions of residents about the desired place of
building a new elementary school.
In the era of digital society, organizers provide Internet
portals that facilitate the process of gathering opinions of resi-
dents. As a starting point, residents provide their characteristics
in order to be invited to a deliberative consultation that is of
their interest. Moreover, one of the most important methods
of gathering data during consultations is providing electronic
surveys, especially Internet surveys. Residents may give their
opinions through the surveys.
In order to encourage candidates to provide their char-
acteristics, participate in a survey and provide true opinions,
privacy should be preserved. To this end, several techniques
for incorporating privacy in data mining can be employed.
These methods are also helpful in statistical tasks, e.g., mean
calculation, that are often used in analysis of data collected
in surveys. In [2] we performed the analysis of applica-
bility in deliberative consultations of the following privacy
preserving techniques: heuristic-based, reconstruction-based,
and cryptography-based [3]. We showed that reconstruction-
based privacy preserving technique is useful for deliberative
consultations and can provide adequate level of privacy in
order to encourage residents to participate in surveys which
makes consultations valuable.
In this paper, we propose how to use reconstruction-
based techniques and randomisation-based methods for de-
liberative consultations; namely, for calculating statistics over
data colected by surveys and calculating a list of candidates
for a deliberative consultation. In our solution we assume that
data that was only distorted by means of randomistaion-based
methods is collected and stored as a centralised database. The
database describes candidates’ characteristics and results of
surveys.
The remainder of this paper is organized as follows: in
Section II, we discuss works related to our task. Section III
presents the privacy preserving data mining solutions important
in the context of deliberative consultations. In Section IV
we propose the solution for consultations with the usage of
reconstruction-based technique. Finally, Section V summarises
the conclusions of the study and outlines future avenues to
explore.
II.
RELATED WORK
In this section, we present literature review of privacy
preserving classiﬁcation as it is the ﬁeld closest to our task
and we adopt some of the algorithms presented in literature in
order to create a solution for the task in question.
Privacy preserving classiﬁcation has been extensively dis-
cussed in literature [4]–[9].
The pioneer work in privacy preserving classiﬁcation for
centralised data was [10], where R. Agrawal and R. Srikant
proposed how to build a decision tree over centralised data
distorted with the randomisation-based method (except the
target/class attribute) and then classify not distorted data
with this decision tree. In this solution, they also presented
the algorithm called AS (Agrawal-Srikant) for a probability
distribution reconstruction for continuous attributes, which
estimates an original probability distribution based on distorted
samples (details about the algorithm AS can be found in
Section III-B2).
Paper [11] extends the AS (Agrawal Srikant) algorithm and
presents the EM (Expectation Maximisation) reconstruction
algorithm, which does not take into account nominal attributes
either (for details refer to Section III-B3).
Randomised
Response
technique
for
related-question
model was presented in [12]. It allows creating a decision
22
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-521-0
ICDS 2016 : The Tenth International Conference on Digital Society and eGovernments

tree but only for nominal attributes. Randomised Response
technique for unrelated-question model was discussed in [13],
[14] and applied in building na¨ıve Bayes classiﬁer.
The solution we proposed in [15] differs from those above,
because it enables a miner to classify centralised perturbed data
containing simultaneously continuous and nominal attributes
by means of randomisation-based methods to preserve pri-
vacy on an individual level. This approach uses the EM/AS
(Expectation Maximisation/Agrawal Srikant) algorithm (de-
scribed in details in Section III-B5) to reconstruct a proba-
bility distribution for nominal attributes and the ARVeSNA
(Algorithm for Assigning Reconstructed Values to Samples
for Nominal Attributes) algorithm (please refer to Section
III-B7) for assigning reconstructed values to samples for this
type of attributes to build a decision tree simultaneously with
continuous attributes.
In [16], we proposed the EQ (the abbreviation comes
from system of EQuations) algorithm (details can be found
in Section III-B6) for reconstructing a probability distribution
of nominal attributes. The algorithm achieves better results,
especially for high level of privacy, i.e., low probability of
retaining an original value of a nominal attribute.
Our work is different from the above mentioned proposals
as it focuses on calculation of statistics based on privacy
preserved centralised database and sampling participants for
deliberative consultations. To this end we adopt algorithms
developed for privacy preserving classiﬁcation. We differ
from Randomised Response technique for related-question and
unrelated-question models because we assume that all surveys’
participants answer the same questions.
III.
PRIVACY PRESERVING DATA MINING PRELIMINARIES
A. Randomisation-based Methods
For detailed description of randomisation-based methods
used in Privacy Preserving Data Mining please refer to [17].
B. Algorithms for Distribution Reconstruction and for Assign-
ing Reconstructed Values to Samples
The algorithms for distribution reconstruction of both nom-
inal and continuous attributes are described in this section.
Moreover, the algorithms for assigning reconstructed values to
samples for nominal and continuous attributes are presented.
The deﬁnition of information loss in reconstruction is intro-
duced, as well.
1) Information Loss: The lack of precision in the recon-
struction of a probability distribution is called information loss.
It is deﬁned as follows [11]:
Deﬁnition Information loss I(fX, ˆfX) equals half of the
expected value of L1 norm between the original probability
distribution fX and its estimate ˆfX.
I(fX, ˆfX) = 1
2E[
R
ΩX | fX − ˆfX |]
Information
loss
I(fX, ˆfX)
lies
between
0
and
1.
I(fX, ˆfX)
=
0 means the perfect reconstruction, and
I(fX, ˆfX) = 1 implies that there is no overlap between the
original distribution and its estimate.
2) AS Algorithm for Probability Distribution Reconstruc-
tion of Continuous Attributes: The algorithm AS for a proba-
bility density function reconstruction for continuous attributes
distorted with the randomisation-based method was proposed
in [10].
The algorithm solves the following problem:
Original values x1, x2, ..., xn of a one-dimensional distribu-
tion are the realisation of n independent random variables
X1, X2, ..., Xn with the same distribution as the variable
X. To hide information, n independent random variables
Y1, Y2, ..., Yn with the same distribution as the random variable
Y have been used. Given x1 + y1, x2 + y2, ..., xn + yn (yi
is the realisation of the random variable Yi) and cumulative
distribution function FY for the variable Y , a cumulative
distribution function FX for the random variable X is to be
estimated.
The solution to the given problem is as follows:
Let wi be the value of Xi + Yi, thus wi = xi + yi. The
individual values xi and yi are not known, only their sums
are revealed. Assuming that the probability density function
fX for variable X and fY for Y are known, Bayes rule [18]
can be used to estimate the posterior (cumulative) distribution
function F ′
X1 for the variable X1. The posterior distribution
function F ′
X1 can be written as follows:
F ′
X1(a) =
Z a
−∞
fX1(z|X1 + Y1 = w1)dz,
(1)
where F ′
X1(a) is the estimator of the posterior (cumulative)
distribution function FX1(a).
Using Bayes rule:
F ′
X1(a) =
Z a
−∞
fX1+Y1(w1|X1 = z)fX1(z)
fX1+Y1(w1)
dz.
(2)
After additional calculations [10] the posterior density function
f ′
X is obtained by differentiating F ′
X:
f ′
X(a) = 1
n
n
X
i=1
fY (wi − a)fX(a)
R ∞
−∞ fY (wi − z)fX(z)dz .
(3)
Having a large number of samples, f ′
X should correspond to
the original probability density function fX.
To estimate f ′
X, the knowledge of fY and fX is needed.
fY is known, because the distorting distribution function is
known for a miner. As the original probability density function
fX is unknown, a uniform distribution is assumed as an initial
estimate of density function and then reﬁned in an iterative
way by applying (3). See Figure 1 for details.
Details about the calculation complexity reduction can be
found in [10].
To stop an iterate reconstruction, three possible stopping
criteria were proposed in [10].
The ﬁrst criterion is met when the reconstructed distribu-
tion is statistically the same as the original distribution. To
check the similarity of distributions, for instance, χ2 measure
(details about χ2 can be found in [19]) can be used. This
23
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-521-0
ICDS 2016 : The Tenth International Conference on Digital Society and eGovernments

f 0
X := uniform distribution
j := 0 // iteration number
repeat
f j+1
X
(a) = 1
n
Pn
i=1
fY (wi−a)f j
X(a)
R ∞
−∞ fY (wi−z)f j
X(z)dz
j := j + 1
until(stopping criterion met)
Figure 1. The AS algorithm.
criterion could be used only for testing, because the original
distribution is not known in practice.
The second solution is to compare the randomised current
estimate of the original distribution with the distorted distri-
bution used for the reconstruction and stop when these two
distributions are statistically the same. This criterion assumes
that the current estimate which is close enough to the original
distribution should be the same after the distortion as the
distorted distribution used for the reconstruction. As stated in
[10], the difference between two distorted distributions is not
a reliable indicator.
The last approach is to compare two consecutive estimates
of the original distribution. When the difference is small
enough, the process is completed. 1% of the threshold of χ2
test was used in [10].
As stated in [11], the AS algorithm may not always
converge and even it converges, there is no guarantee that it
gives a reasonable estimate of the original distribution. There
was no proof given for that statement and this issue was not
mentioned in [10].
3) Algorithm EM for Probability Distribution Reconstruc-
tion of Continuous Attributes: The algorithm for a probability
density function reconstruction for continuous attributes dis-
torted by means of the the randomisation-based method was
proposed in [11], as well. The algorithm was called EM by
the authors. The problem to be solved is the same as for the
AS algorithm.
The details about the EM algorithm and the proof that
it converges can be found in [11]. The authors of the EM
algorithm stated that it is theoretically the best algorithm and
having a large set of distorted samples, the EM algorithm
can reconstruct the original distribution with little or without
information loss [11]. The deﬁnition of information loss can
be found in Section III-B1.
4) Assigning Reconstructed Values to Samples for Con-
tinuous Attributes: The algorithm for assigning reconstructed
values to samples for continuous attributes was presented in
[10]. We describe this algorithm in this section.
Let I1, ..., Im denote m intervals and N(Ik) be the num-
ber of samples in Ik interval. Samples should be sorted in
an ascending order and assigned to consecutive intervals as
follows: N(I1) ﬁrst samples are assigned to the ﬁrst interval
I1, the next N(I2) samples to the second interval I2, etc.
5) EM/AS Algorithm for Probability Distribution Recon-
struction of Nominal Attributes: In [15], we proposed the
EM/AS algorithm for reconstructing a probability distribution
of a nominal attribute.
The EM/AS algorithm is based on two algorithms: AS
proposed in [10] and its extension EM presented in [11]. Both
Pr(X = vp)0 := 1
k, p = 1, ..., k
j := 0 //iteration number
repeat
Pr(X = vp)j+1 = 1
n
Pn
s=1
P r(vp→X(s))P rj(X=vp)
Pk
t=1 P r(vt→X(s))P rj(X=vt)
j := j + 1
until(stopping criterion met)
Figure 2. The EM/AS nominal attribute probability distribution
reconstruction algorithm.
algorithms reconstruct a probability distribution of continuous
attributes.
To reconstruct probability distribution of a nominal at-
tribute, both EM and AS algorithms were modiﬁed to obtain
the EM/AS (Figure 2). The modiﬁcations of both algorithms
(AS and EM) give the same result.
The algorithm solves the following problem: a nominal
attribute X has the possible values v1, v2, v3, ..., vk and n
samples. Value for each sample is modiﬁed according to a
probability Pr(vp → vr) (a probability that a value vp will
be changed to a value vr). X(s) means a value of an attribute
X for a sample s. An original probability distribution of an
attribute X should be reconstructed.
The algorithm starts with the uniform distribution and
calculates the estimate of the probability distribution in every
iteration.
Stopping criterion is the same as for the AS and EM algo-
rithms (the algorithm is stopped when the difference between
successive estimates of the original probability distribution
becomes small, as little as 1% of the threshold of the χ2 test).
6) EQ Algorithm for Probability Distribution Reconstruc-
tion of Nominal Attributes: In [16], we proposed the EQ
algorithm, the name of the algorithm comes from the phrase
system of EQuations, that reconstructs the probability distri-
bution of nominal attributes and can be used instead of the
EM/AS algorithm. The EQ algorithm outperforms the EM/AS,
especially for high levels of privacy [16].
The problem to be solved is the same as for the EM/AS
algorithm: there are a nominal attribute X with the possible
values v1, v2, v3, ..., vk and n samples. A value for each sample
is modiﬁed according to a probability Pr(vp
→ vr) (a
probability that a value vp will be changed to a value vr)
and we want to reconstruct an original probability distribution
of an attribute X.
Let us assume that there is an attribute Colour with 3
values: v1 = green , v2 = blue, and v3 = black.
For the original value of the attribute, e.g., green, the
probability Pr(v1 → v1) that the value will be the same
after the modiﬁcation is known, as well as the probability
of changing the value from green to blue and from green
to black. Moreover, when the value of the attribute after the
distortion is, e.g., green, the original value was one of the
three possible values: green, blue, and black and all the
probabilities Pr(v1 → v1), Pr(v2 → v1), Pr(v3 → v1) how
the value has become green are known.
Let Z be the attribute after the modiﬁcation with the
possible values v1, v2, v3, ..., vk. In the example, the attribute
24
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-521-0
ICDS 2016 : The Tenth International Conference on Digital Society and eGovernments

Z has 3 values: green, blue, and black and the following
equation can be written:
P(Z = green) = a1,1P(X = green) + a1,2P(X =
blue) + a1,3P(X = black),
where as,p = Pr(vp → vs). For colours blue and black the
similar equations can be written:
P(Z = blue) = a2,1P(X = green) + a2,2P(X =
blue) + a2,3P(X = black)
P(Z = black) = a3,1P(X = green) + a3,2P(X =
blue) + a3,3P(X = black).
Now there are 3 equations and 3 unknown variables
(P(X = green), P(X = blue), P(X = black)), thus the
system of linear equations can be solved.
In general there is the following system of k equations:
P(Z = v1) = a1,1P(X = v1) + a1,2P(X =
v2) + · · · + a1,kP(X = vk)
P(Z = v2) = a2,1P(X = v1) + a2,2P(X =
v2) + · · · + a2,kP(X = vk)
...
P(Z = vk) = ak,1P(X = v1) + ak,2P(X =
v2) + · · · + ak,kP(X = vk)
with k unknown variables.
Let X be the column vector with elements x1, ..., xk, where
xi = P(X = vi) and Z be the column vector with elements
z1, ..., zk, where zi = P(Z = vi). Let P be the matrix
of retaining/changing values of a nominal attribute. We can
rewrite the system of equations in the matrix form as:
Z = PX
(4)
To ﬁnd values of P(X = vi), i = 1, . . . , k, we need to
solve (4). We can solve it by left multiplying both sides by
inverted P, i.e., P−1 (only if inverted P exists).
Nonexistence of the inverted matrix is not troublesome
because the number of values of a nominal attribute is known
before collecting data starts and a non-singular matrix P can
be chosen to guarantee the existence of inverted P matrix.
7) ARVeSNA Algorithm for Assigning Reconstructed Values
to Samples for Nominal Attributes: We proposed the algorithm
for assigning reconstructed values to samples for nominal
attributes in [20] and describe this algorithm in this section.
Having reconstructed a probability distribution of a nomi-
nal attribute, reconstructed values can be assigned to samples.
The algorithm solves the following problem:
Since modiﬁed values of a nominal attribute are given, the
probability distribution of a modiﬁed attribute (i.e., P(Z =
vi), i = 1, . . . , k) and the number of all samples n are known.
The reconstructed probability distribution (P(X = vi), i =
1, . . . , k) is estimated. The aim is to assign reconstructed
values to samples taking into account the reconstructed prob-
ability distribution.
In order to solve this problem, the number of distorted
samples (nZ(vi)) is counted separately for each value of
an attribute and the number of original samples (nX(vi) =
P(X = vi)n) is estimated.
TABLE I. THE EXAMPLE OF THE ORIGINAL DATABASE.
Id
Salary
Age
Sex
Credits status
Children
1
1000
35
M
none
N
2
1500
37
F
overdue
Y
3
5000
41
M
present
N
4
3000
44
M
repaid
N
5
4200
50
F
repaid
N
6
2000
28
F
none
N
7
1000
30
M
none
Y
TABLE II. THE EXAMPLE OF THE DISTORTED DATABASE WITH UNIFORM
DISTORTION DISTRIBUTION ⟨−500, 500⟩ FOR SALARY, ⟨−10, 10⟩ FOR
AGE AND p = 0.6 FOR SEX AND CREDITS STATUS ATTRIBUTES.
Id
Salary
Age
Sex
Credits status
Children
1
1353.32
33.42
M
repaid
N
2
1611.83
40.64
M
overdue
Y
3
5428.27
51.27
M
present
N
4
2573.22
39.51
F
none
N
5
4145.89
42.67
M
repaid
N
6
2258.34
38.72
F
none
N
7
1054.03
36.65
M
overdue
Y
Then the difference, called δ(vi), between nZ(vi) and
nX(vi) is calculated. δ(vi) > 0 means that there are too many
samples because there are more samples with distorted value
of vi than the reconstructed number of samples for the value
vi. A sample corresponding to a positive value of δ(vi) is
found and assigned with a reconstructed value vj for which a
value of δ(vj) is negative and the reconstructed value vj has
the highest probability to be distorted to the value vi. Values
of corresponding δ(vi) and δ(vj) are updated and the process
is continued until all values of δ(vi), i = 1, . . . , k are zero.
Having completed the process, samples with the recon-
structed values are assigned according to an original (recon-
structed) probability distribution.
IV.
CALCULATING SURVEYS’ STATISTICS AND
PARTICIPANTS SAMPLING
We deﬁne two tasks in deliberative consultations that
involve calculations on data with preserved privacy by means
of reconstruction-based techniques and randomisation-based
methods; namely, calculating surveys’ statistics and partici-
pants sampling.
A. Calculating Surveys’ Statistics
In the task of calculating statistics from survey’s data, we
assume that there is a centralised database that is collected by
means of electronic surveys. A participant provides an answer
to each question in the survey. The answer is an attribute value.
Thus, we can state that the database consists of: a deﬁnition
of attributes and its values.
Each attribute describes possible answer’s values for a
survey’s question. For example, an attribute that describes a
question ’Do you have a car?’ is a binary attribute with possible
values: yes or no. The possible types of attributes are: binary,
nominal, ordinal, and continuous.
Values of attributes are distorted answers provided by
participants. Thus, if we have n participants and k questions,
25
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-521-0
ICDS 2016 : The Tenth International Conference on Digital Society and eGovernments

we have n values for each of k attributes. According to
randomisation-based methods values of attributes are distorted
at a client side; that is, participant’s side, with one of the
possible methods. For binary attributes we may use basic
randomisation factor method or distortion with a matrix of
retaining/changing values of a nominal attribute (for more
details please refer to [2]). For ordinal attributes we may use
a modiﬁed matrix of retaining/changing values of an attribute
described in [21]. For continuous attributes the additive pertur-
bation method [10], multiplicative perturbation [22], and the
retention replacement perturbation [23] may be employed.
Tables I and II show the example databases that could be
results of electronic survey after a deliberative consultation.
Table I presents the original, not distorted, database that
could be a real result of a survey if there is no privacy
preserving methods applied. The example database that could
be a result of a survey with privacy preserved by means of
the randomisation-based method is shown in Table II. If we
use the randomisation-based method in real applications, the
original database (Table I) does not exist, only distorted values
(Table II) are stored. Id attribute is not necessary and is shown
to ease the process of comparing both databases.
In the aforementioned task, there is a table with distorted
values, like in Table II and a scientist wants to calculate some
statistics about participants of a survey. The statistics could be
the number of participants that meet a speciﬁc condition which
is based on gathered data, e.g., the number of participants that
have children and are at least 30 years old. Another statistics
to calculate is mean of some attribute for all participants or
participants that meet a speciﬁc condition. For instance, mean
salary for participants that have children and are at least 30
years old. Last but not least a scientist may want to see a
distribution of an attribute for all participants or a group of
participants.
1) Calculating Number of Participants that Meet Speciﬁc
Condition: Fist we deﬁne a condition that a scientist may
create in order to choose a group of participants. Let C
be a condition that participants should meet. Let us assume
that c1, c2, ..., cm are subconditions and form condition C,
i.e., C = c1 ∧ c2 ∧ ... ∧ cm. ci condition is a condition
that is based on one attribute. The possible types of this
conditions are: vaj
> t, vaj
≤ t, t1 ≤ vaj
< t2 for
continuous attributes, vaj ∈ {v1, v2, ..., vl} for binary, ordinal,
and nominal attributes, where vaj is a value of attribute aj,
t is a known threshold, v1, v2, ..., vl are possible values of an
attribute.
Let us consider that condition C = c1 and c1 is of a form
t1 ≤ vaj < t2 and is based on a continuous attribute. In this
case we can choose intervals in a way that their end/begin
in points t1 and t2 and apply AS or EM algorithm. Let us
assume that we choose the following intervals: i1 that starts
in −∞ and ends in t1, i.e., i1 = (−∞; t1), i2 =< t1; t2),
i3 =< t2; ∞). The number of intervals need not to be limited
to 3, however, it is important that they should be intervals that
end/begin in t1 and t2.
The AS or EM algorithms estimate a distribution of values
of attribute over intervals. The number of participants can be
obtained by multiplying a probability for an interval by the
number of participants. Thus the number of values that lie in
each interval N1, N2, N3 is known. If we assume that t1 < t2,
then N2 is the number of participants that meet condition C.
Otherwise, N1 + N3 is the right number. For conditions of
form vaj > t, vaj ≤ t the solution is analogous.
If c1 is based on a nominal attribute with k possible values
and is of form vaj ∈ {v1, v2, ..., vl} EM/AS or EQ algorithm
can be employed. The output of the algorithm is the number of
participants for each possible value of an attribute. As shown
in the following equation, for condition vaj ∈ {v1, v2, ..., vl}
we need to sum all numbers for values that are present in the
condition; that is vaj ∈ {v1, v2, ..., vl}.
N{v1,v2,...,vl} =
X
vi∈{v1,v2,...,vl}
Nvi
(5)
where:
•
N{v1,v2,...,vl} is the estimated number of samples for
which the original attribute has one of values in this
set {v1, v2, ..., vl},
•
Nv1 is the estimated number of samples for which the
original attribute has a value v1.
In order to calculate the number of participants that meet
condition C with more than one subcondition, C = c1 ∧ c2 ∧
... ∧ cm, we need to calculate the number of participants in an
iterative manner (see Figure 3). For the ﬁrst subcondition we
calculate the number of participants that meet subcondition
c1 as shown in this section. Then, we choose participants
that meet this subcondition. For binary, nominal and ordinal
attributes we can employ ARVeSNA algorithm (Section III-B7,
[20]). For continuous attributes we can apply the algorithm
described in Section III-B4 that sorts participants in the as-
cending order over a condition attribute. Then the algorithm
assigns the estimated number of sorted participants to each
interval. As a result, we obtain a set Pc1 of participants that
meet a subcondition c1. In the next iteration we start with
Pc1 set of participants and apply a subcondition c2. The result
of this iteration is a set Pc1∧c2 of participants that meet
condition c1 ∧ c2. Then we proceed to the next iteration until
Pc1∧c2∧...∧cm is obtained and hence the number of participants
that meet condition C. In the last iteration ARVeSNA or the
algorithm for continuous attributes that chooses a subset of
participants according to calculated distribution need not to be
aplied because we need only the number of participants that
meet condition C and the list of participants is not necessary.
As an example, we will analyse the calculation of the
number of participants who meet the following conditions:
females at most 30 years old.
Let us assume that the Table II contains the distorted results
of a survey. We will use the algorithm presented in Figure 3.
The ﬁrst condition is Sex = F. In order to ﬁnd the estimated
number of objects that meet this condition we use EM/AS
or EQ algorithm. Let us assume that the result of EM/AS
or EQ algorithm is 3. Based on Table II without taking into
account the distortion we would obtain the number 2. Then,
by the means of ARVeSNA algorithm we assign participants
to values of the attribute Sex in this case. Let us assume that
the participants 2, 5 and 6 are assigned. The second condition
is Age ≤ 30. The considered attribute is continuous thus we
use EM or AS algorithm. It gives the number of participants
at most 30 years old within participants 2, 5 and 6. The result
is 1. If we used data from Table II directly without taking into
26
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-521-0
ICDS 2016 : The Tenth International Conference on Digital Society and eGovernments

INPUT: m // number of subconditions
INPUT: C = c1 ∧ c2 ∧ ... ∧ cm // condition to be met
INPUT: P // set of participants
OUTPUT: Pc1∧c2∧...∧cm // set of participants that meet
// c1 ∧ c2 ∧ ... ∧ cm condition
OUTPUT: Nc1∧c2∧...∧cm // number of participants that meet
// c1 ∧ c2 ∧ ... ∧ cm condition
for i = 1 to m do
if ci is based on continuous attribute then
prepare intervals:
i1 = (−∞; t1), i2 =< t1; t2), i3 =< t2; ∞)
calculate distribution with AS or EM algorithm
assign participants to intervals
choose Pc1∧...∧ci
calculate Nc1∧...∧ci
elseif // binary, nominal, ordinal attributes
calculate distribution with EM/AS or EQ algorithm
assign participants to attribute values (ARVeSNA)
choose Pc1∧...∧ci
calculate Nc1∧...∧ci
end
end
Figure 3. The list and the number of participants that meet a speciﬁc
condition calculation algorithm.
account that values are distorted, we would conclude that there
were no females at most 30 years old.
2) Mean Calculation: Considering the mean of continuous
attributes and the additive perturbation the calculations are the
same as for not distorted data if the distorting distribution
with mean equal to 0 is used. The type of a distribution,
e.g., uniform, normal, makes no difference. A distribution with
mean equal to 0 does not statistically change the mean of
attribute’s values. The mean can be calculated for an arbitrary
set of values of an attribute. Therefore, a scientist is able to
calculate a mean if a group of participants is chosen. To this
end, a scientist may use the algorithm presented in Figure 3 in
order to ﬁnd a set of participants that meet a speciﬁc condition
and then calculate the mean.
B. Participants Sampling
In participants sampling, a scientist chooses a set of par-
ticipants that take part in a deliberative consultation regarding
participants’ characteristics. Let us assume that all candidates
provide information about them and the randomised-based
method is applied, hence, only distorted data is stored. Table
II may represent such characteristics provided by candidates.
A scientist wants to ﬁnd a condition that chooses a right group
of people to be involved in a deliberative consultation. Hence,
algorithm presented in Figure 3 can be used to perform this
task, since it provides a number and a list of candidates that
meet a speciﬁc condition.
If we assume that Table II represents characteristics of
participants and we want to ﬁnd the list of candidates for
consultations that are females at most 30 years old, we may
use the example from Section IV-A1 to illustrate participants
samplig. However, in this case we need to perform the last
step of ARVeSNA algorithm; that is, assign values of attribute
Age to participants.
V.
CONCLUSION AND FUTURE WORK
In this work, we proposed a framework for reconstruction-
based techniques and randomisation-based methods applica-
tion in deliberative consultations. The solution for calculating
statistics over privacy preserved survey data and candidates’
characteristics has been presented. The proposed framework
lets a scientist apply privacy preserving in real deliberative
consultations.
In future work, we plan to investigate the possibility of
k-anonymity application in a hybrid solution that combines
aggregation, reconstruction-based technique and k-anonymity
approach.
The incorporation of the presented framework in the system
for deliberative consultations that is being under development
is planned also.
ACKNOWLEDGMENT
This research has been supported by the National Centre for
Research and Development under grant No SP/I/1/77065/10
and the Institute of Computer Science, Warsaw University of
Technology under Grant No. II/2015/DS/1.
REFERENCES
[1]
J. Abelson et al., “Deliberations about deliberative methods: issues in
the design and evaluation of public participation processes,” Social
science & medicine, vol. 57, no. 2, 2003, pp. 239–251.
[2]
P. Andruszkiewicz, “Privacy preserving data mining for deliberative
consultations,” in (accepted for) Hybrid Artiﬁcial Intelligent Systems
- 11th International Conference, HAIS 2016, Seville, Spain, April 18-
20, 2016.
[3]
V. S. Verykios et al., “State-of-the-art in privacy preserving data
mining.” SIGMOD Record, vol. 33, no. 1, 2004, pp. 50–57.
[4]
L. Xiong, S. Chitti, and L. Liu, “Mining multiple private databases
using a knn classiﬁer,” in SAC ’07: Proceedings of the 2007 ACM
symposium on Applied computing, 2007, pp. 435–440.
[5]
M. R. Keyvanpour and S. S. Moradi, “A perturbation method based
on singular value decomposition and feature selection for privacy
preserving data mining,” IJDWM, vol. 10, no. 1, 2014, pp. 55–
76. [Online]. Available: http://dx.doi.org/10.4018/ijdwm.2014010104
[accessed: 2016-03-18]
[6]
P. Andruszkiewicz, “Hierarchical combining of classiﬁers in privacy
preserving data mining,” in Hybrid Artiﬁcial Intelligence Systems -
9th International Conference, HAIS 2014, Salamanca, Spain, June
11-13, 2014. Proceedings, ser. Lecture Notes in Computer Science,
M. M. Polycarpou, A. C. P. L. F. de Carvalho, J. Pan, M. Wozniak,
H. Quinti´an, and E. Corchado, Eds., vol. 8480.
Springer, 2014,
pp. 573–584. [Online]. Available: http://dx.doi.org/10.1007/978-3-319-
07617-1 [accessed: 2016-03-18]
[7]
X. Li, Z. Yan, and P. Zhang, “A review on privacy-preserving
data mining,” in 14th IEEE International Conference on Computer
and Information Technology, CIT 2014, Xi’an, China, September
11-13,
2014.
IEEE,
2014,
pp.
769–774.
[Online].
Available:
http://dx.doi.org/10.1109/CIT.2014.135 [accessed: 2016-03-18]
[8]
P. Andruszkiewicz, “Frequent sets discovery in privacy preserving
quantitative association rules mining,” in Hybrid Artiﬁcial Intelligent
Systems - 10th International Conference, HAIS 2015, Bilbao, Spain,
June 22-24, 2015, Proceedings, ser. Lecture Notes in Computer
Science, E. Onieva, I. Santos, E. Osaba, H. Quinti´an, and E. Corchado,
Eds., vol. 9121.
Springer, 2015, pp. 3–15. [Online]. Available:
http://dx.doi.org/10.1007/978-3-319-19644-2 [accessed: 2016-03-18]
[9]
J. Hamm, A. C. Champion, G. Chen, M. Belkin, and D. Xuan,
“Crowd-ML: A privacy-preserving learning framework for a crowd of
smart devices,” CoRR, vol. abs/1501.02484, 2015. [Online]. Available:
http://arxiv.org/abs/1501.02484 [accessed: 2016-03-18]
[10]
R. Agrawal and R. Srikant, “Privacy-preserving data mining,” in SIG-
MOD Conference, W. Chen, J. F. Naughton, and P. A. Bernstein, Eds.
ACM, 2000, pp. 439–450.
27
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-521-0
ICDS 2016 : The Tenth International Conference on Digital Society and eGovernments

[11]
D. Agrawal and C. C. Aggarwal, “On the design and quantiﬁcation of
privacy preserving data mining algorithms,” in PODS ’01: Proceedings
of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on
Principles of database systems, 2001, pp. 247–255.
[12]
W. Du and Z. Zhan, “Using randomized response techniques for
privacy-preserving data mining.” in KDD, 2003, pp. 505–510.
[13]
J. Z. Zhan and S. Matwin, “Privacy-preserving data mining in electronic
surveys,” in ICEB, J. Chen, Ed. Academic Publishers/World Publishing
Corporation, 2004, pp. 1179–1185.
[14]
——, “Privacy-preserving data mining in electronic surveys,” I. J.
Network Security, vol. 4, no. 3, 2007, pp. 318–327.
[15]
P. Andruszkiewicz, “Privacy preserving classiﬁcation for continuous and
nominal attributes,” in Proceedings of the 16th International Conference
on Intelligent Information Systems, 2008.
[16]
——, “Probability distribution reconstruction for nominal attributes in
privacy preserving classiﬁcation,” in ICHIT ’08: Proceedings of the
2008 International Conference on Convergence and Hybrid Information
Technology.
Washington, DC, USA: IEEE Computer Society, 2008,
pp. 494–500.
[17]
C. C. Aggarwal, “Privacy-preserving data mining,” in Data Mining.
Springer International Publishing, 2015, pp. 663–693.
[18]
M. Fisz, Probability Theory and Mathematical Statistics.
New York:
John Wiley and Sons, 1963.
[19]
L. M. Surhone, M. T. Timpledon, and S. F. Marseken, Pearson’s Chi-
Square Test.
Beau Bassin: Betascript Publishers, 2010.
[20]
P. Andruszkiewicz, “Privacy preserving data mining on the example
of classiﬁcation (in Polish),” Master’s thesis, Warsaw University of
Technology, 2005.
[21]
——, “Privacy preserving classiﬁcation for ordered attributes,” in Man-
Machine Interactions, ser. Advances in Soft Computing, J. F. P. U. S.
Krzysztof A. Cyran, Stanisaw Kozielski and A. Wakulicz-Deja, Eds.,
vol. 59/2009.
Springer, 2009, pp. 353–360.
[22]
J. J. Kim and W. E. Winkler, “Multiplicative noise for masking con-
tinuous data,” Statistical Research Division, US Bureau of the Census,
Washington D.C., Tech. Rep., 2003.
[23]
R. Agrawal, R. Srikant, and D. Thomas, “Privacy preserving olap,” in
SIGMOD ’05: Proceedings of the 2005 ACM SIGMOD international
conference on Management of data. New York, NY, USA: ACM, 2005,
pp. 251–262.
28
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-521-0
ICDS 2016 : The Tenth International Conference on Digital Society and eGovernments

