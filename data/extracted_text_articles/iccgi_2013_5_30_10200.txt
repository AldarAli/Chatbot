Filtering of Large Signal Sets: An Almost Blind
Case
Anatoli Torokhti,
Phil Howlett
CIAM, School of Inf. Techn. & Math. Sci.
University of South Australia,
Adelaide, SA 5095, Australia
Email: anatoli.torokhti@unisa.edu.au, phil.howlett@unisa.edu.au
Hamid Laga
PBRC, University of South Australia
ACPFG,
Adelaide, SA 5095, Australia
Email: hamid.laga@unisa.edu.au
Abstract—We propose a new technique which allows to es-
timate any random signal from a large set of noisy observed
data on the basis of information on only a few reference signals.
The conceptual device behind the proposed estimator is a linear
interpolation of an optimal incremental estimation applied to
random signal pairs interpreted an extension of the Least Squares
Linear (LSL) estimator. We consider the case of observations
corrupted by an arbitrary noise (not by an additive noise only)
and design the estimator in terms of the Moore-Penrose pseudo-
inverse matrix. Therefore, it is always well deﬁned. The proposed
estimator is justiﬁed by establishing an upper bound for the
associated error and by showing that this upper bound is directly
related to the expected error for an incremental application of
the optimal LSL estimator. It is shown that such an estimator is
possible under quite unrestrictive assumptions.
Keywords–large signal sets; ﬁltering; least squares linear esti-
mator.
I. INTRODUCTION
A. Motivation
We write xω = xω(t) for a stochastic vector xω(t) associ-
ated with a random outcome ω and time t ∈ T = [a, b] ⊂ R.
A rigorous notation is given in the section that follows.
In many applications associated with a difﬁcult environ-
ment, a priori information on a large set of signals of interest,
Kx = {xω(t)}, can only be obtained for a few signals
{xω(tj)}p
1 ⊂ Kx where p is a small number. Typical exam-
ples are devices and equipment exploited in the stratosphere,
underground and underwater such as those in defence and the
mining industry. Signals xω(t1), . . . , xω(tp), are associated
with given times, t1, t2, . . . , tp, respectively, such that
a = t1 < t2 < · · · < tp−1 < tp = b.
(1)
A choice of signals xω(t1), . . . , xω(tp) might be beyond our
control (in geophysics and defence tasks, for instance). At the
same time, it is required to estimate each reference signal in
the set Kx from the corresponding set of noisy observations.
Thus, all we can exploit to develop an associated ﬁlter is
observed noisy data and a sparse information on reference
signals.
Example 1: Suppose we need to process a set Ky of N =
121 random signals over set T = [τ1, τ2, . . . , τN] so that each
input signal from this set, y(t, ·), enters a ﬁlter at time t = τk
0
100
200
300
400
500
1
1.5
2
2.5
3
3.5
4
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
(a) Observed signals from the set Ky.
0
100
200
300
400
500
1
1.5
2
2.5
3
3.5
4
−1.5
−1
−0.5
0
0.5
1
1.5
(b) Samples of reference signals ˜
X(j) at times tj for j = 1, . . . , 11.
Fig. 1.
Signals and samples considered in Example 1.
where τ1 = 0 and τk+1 = τk + 0.05, for k = 1, . . . , 120.
At time τk, for k = 1, . . . , N, the observed signal y(τk, ·), is
represented by its realizations as a 4 × 4 matrix
Y (k) = {y(k)
ℓ,r }4
ℓ,r=1 = [y(τk, ω1), . . . , y(τk, ω4)].
(2)
A column of matrix Y (k), y(tk, ωi) ∈ R4, represents the real-
ization of the signal y(t, ωi) at time t = τk generated by the
random event ωi, for each i = 1, 2, . . . , 4. Thus, all observed
signals are given by the 4×484 matrix Y = [Y (1), . . . , Y (121)]
represented in Fig. 1 (a).
Suppose that, for j = 1, . . . , p, information on the refer-
ences signals can only be obtained at some times t1 = τ1,
101
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-283-7
ICCGI 2013 : The Eighth International Multi-Conference on Computing in the Global Information Technology

tj+1 = τ12j+1 where j = 1, . . . , 10 (see (1)) in the form of
samples given by 4 × 4 matrices
˜X(j) = [˜x(tj, ω1), . . . , ˜x(tj, ω4)] = {˜x(k)
ℓ,r }4
ℓ,r=1.
(3)
Fig. 1 demonstrates a typical situation with noisy observed
signals and sparse information on the reference signals. In
Example 2 below we show that, under certain conditions, the
proposed technique allows us to estimate the signals of interest
with an acceptable accuracy.
B. Formalization of the problem
To formalize the problem, we write {Ω, Σ, µ} for a proba-
bility space where Ω is the set of all experimental outcomes,
Σ ⊂ Ω is a sigma-algebra of measurable sets known as the
event space and µ is a non-negative probability measure with
µ(Ω) = 1. We denote by Kx = {xω | ω ∈ Ω} a set of
reference stochastic signals and by Ky = {yω | ω ∈ Ω} a set
of observed signals.
In an intuitive way, y can be regarded as a noise-corrupted
version of x. For example, y can be interpreted as y = x + n
where n is white noise. We do not restrict ourselves to this
simplest version of y and assume that the dependence of y
on x and n is arbitrary. Note that, theoretically, Kx and Ky
are inﬁnite signal sets. In practice, however, sets Kx and Ky
are ﬁnite and large, each with, say, N signals.
To each random outcome ω ∈ Ω we associate a unique
signal pair (xω, yω) where xω : T → C0,1(T, Rm) and yω :
T → C0,1(T, Rn). The space C0,1(T, Rp) is the set of vector-
valued H¨older continuous functions f of order 1 with f(t) ∈
Rp and ∥f(s) − f(t)∥ ≤ K|s − t| (see [1], p. 96.) Write
P = Kx × Ky = {(xω, yω) | ω ∈ Ω}
(4)
for the set of all
such signal pairs. For each ω ∈ Ω,
the components xω = xω(t), yω = yω(t) are Lipschitz
continuous vector-valued functions on T [1].
We wish to construct an estimator F (p−1) that estimates
each reference signal xω(t) in P from related observed input
yω(t) under the restriction that a priori information on only a
few reference signals, xω(t1), . . ., xω(tp), is available where
p ≪ N.
In more detail, this restriction implies the following. Let us
denote by K(p)
x
a set of p signals xω(t1), . . ., xω(tp) for which
a priori information is available. A set of associated observed
signals yω(t1), . . ., yω(tp) is denoted by K(p)
y . Then for all
yω(t) that do not belong to K(p)
y , yω(t) /∈ K(p)
y , estimator
F (p−1) is said to be the blind estimator [2], [3], [4], [5] since
no information on xω(t) /∈ K(p)
x
is available. If yω(t) ∈ K(p)
y
then F (p−1) becomes a nonblind estimator since information
on xω(t) ∈ K(p)
x
is available. Thus, depending on yω(t),
estimator F (p−1) is classiﬁed differently. Therefore, such a
procedure of estimating reference signals in Kx is here called
the almost blind estimation.
C. Differences from known techniques
We would like to note that the almost blind estimation
is different from known methods such as nonblind [6]–[18],
semiblind and blind techniques [2]–[5], [19]–[22].Indeed, at
each particular time t ∈ T, the input of the almost blind
estimator F (p−1) developed below in this paper, is a random
vector yω(t). Thus, for different t ∈ T, the input is a different
random vector yω(t) but we wish to keep the same estimator
F (p−1) for any t ∈ T, i.e., for any observed signal yω(t) in
the set Ky. The literature on these subjects is very abundant.
Here, we listed only some related references.
By known techniques in [2]–[16] and [19]–[22], an esti-
mator (here, we choose the united term ‘estimator’ to denote
an equalizer or a system) is speciﬁcally designed for each
particular input–output pair represented by random vectors.
That is, for different inputs (observed signals) yω(t), known
techniques require different speciﬁed estimators and the num-
ber of estimators should be equal to a number of processed
signals. In the case of large signal sets, such approaches
become inconvenient because the number of signals N can
be very large as it is supposed in this paper. For example, in
problems related to DNA analysis, N = O(104). Therefore,
the inconvenient restriction of using a priori information on
only p reference signals, with p ≪ N, is quite signiﬁcant. At
the same time, beside difﬁculties that this restriction imposes
on the estimation procedure, we use it in a way that allows
us to avoid the hard work associated with known techniques
applied to large signal sets. To the best of our knowledge, the
exception is the methodology in [17], [18], where, for estima-
tion of a set of signals, the single estimator is constructed. The
estimation techniques in [17], [18] exploit information in the
form of a vector obtained, in particular, from averaging over
signals in K(p)
x .
Further, the semiblind techniques are not applicable to the
considered problem because they require a knowledge of some
‘parts’ of each reference signal in Kx (e.g., see [3], [5], [19])
but it is not the case here. Although the blind techniques
allow us to avoid this restriction, it is known that they have
difﬁculties related to accuracy and computational load. In the
problem under consideration, the advantage is a knowledge of
some (small) part of the set of reference signals. It is natural
to use this advantage in the estimator structure and we will do
it in Section II.
Nonblind estimators [6]–[16] are not applicable here be-
cause they require a priori information on each reference
signal in Kx (e.g., a knowledge of covariance matrix E[xωyT
ω]
where E is the expectation operator). In particular, it is known
that there are signiﬁcant advantages in adaptive or recursive
estimators (e.g., associated with Kalman ﬁltering approach
[23]) and it may well be possible to embed our estimator into
such an environment—but that is not our particular concern
here. Further, we note that much of the literature on piecewise
linear estimators [24]–[28] seems to be not directly relevant
to the estimator proposed here. In the ﬁrst instance papers
such as [24]–[28] are mostly concerned with the theoretical
problems of approximation by piecewise linear functions on
multi-dimensional domains which is not the case here.
Also, unlike many known techniques, we consider the case
of observations corrupted by an arbitrary noise (not by an
102
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-283-7
ICCGI 2013 : The Eighth International Multi-Conference on Computing in the Global Information Technology

additive noise only) and design the estimator in terms of
the Moore-Penrose pseudo-inverse matrix [29]. Therefore it
is always well deﬁned.
II. THE MAIN RESULTS
In this section, we outline the rationale for the proposed
estimator and state the main results.
A. Some preliminaries
The proposed estimator F (p−1) is adaptive to a sparse set
K(p)
x .
The conceptual device behind the proposed estimator is
a linear interpolation of an optimal incremental estima-
tion applied to random signal pairs (xω(tj), yω(tj)) and
(xω(tj+1), yω(tj+1)), for j = 1, . . . , p − 1, interpreted an
extension of the Least Squares Linear (LSL) estimator (see,
for example, [6], [11], [16]). Although this idea may seem
reasonable, the detailed justiﬁcation of the new estimator is
not straightforward and requires careful analysis. We shall do
this by establishing an upper bound for the associated error
and by showing that this upper bound is directly related to the
expected error for an incremental application of the optimal
LSL estimator. In Section II-B below, we will show that such
an estimator is possible under quite unrestrictive assumptions.
Since the estimator proposed below is based on an extension
of the LSL estimator it is convenient to
sketch known
related results
here.
Consider a single random signal pair
(x(ω), y(ω)) where x ∈ L2(Ω, Rm) and y ∈ L2(Ω, Rn)
with zero mean (E[x], E[y]) = (0, 0), where 0 is the zero
vector. Note that here, x and y do not depend on t as above.
The estimate bx of the reference vector x by the optimal least
squares linear estimator is given by
bx(ω) = ExyE†
yy y(ω)
(5)
where Exy = E[xyT ] and Eyy = E[yyT ] are known
covariance matrices and E†
yy is the Moore-Penrose pseudo-
inverse of Eyy. By the LSL estimator, matrices Exy and E†
yy
should be speciﬁed for each signal pair (x(ω), y(ω)).
Further, for a justiﬁcation of our estimator, we need some
more notation as follows. It is convenient to denote x(t, ω) =
xω(t) and y(t, ω) = yω(t) so that x(t, ω) ∈ Rm and
y(t, ω) ∈ Rn.
B. The piecewise LSL interpolation estimator
For each signal pair (or vector function pair) in the set P,
(x(t, ω), y(t, ω)), we assume that (E[x(t, ·)], E[y(t, ·)]) =
(0, 0). To begin the estimation process we need to ﬁnd an
initial estimate bx(t1, ω). It is assumed this can be found by
some known method. Further, let us consider a signal estima-
tion procedure at t2, · · · , tp. We use an inductive argument to
deﬁne an incremental estimation procedure. Consider a typical
interval [tj, tj+1] and deﬁne incremental random vectors
vj(ω) = x(tj+1, ω) − x(tj, ω) ∈ Rm,
(6)
wj(ω) = y(tj+1, ω) − y(tj, ω) ∈ Rn
(7)
and construct the optimal linear estimate
bvj(ω) = EvjwjE†
wjwj wj(ω)
(8)
of the increment vj(ω) for each j = 1, . . . , p − 1. We will
write
Bj = EvjwjE†
wjwj ∈ Rm×n.
(9)
Deﬁne the estimate at point tj+1 by setting bx(tj+1, ω) =
bx(tj, ω) + bvj(ω). Thus we have
bx(tj+1, ω)
=
bx(tj, ω) + Bj[y(tj+1, ω) − y(tj, ω)]
=
ϵj(ω) + Bjy(tj+1, ω)
(10)
where we write
ϵj(ω) = bx(tj, ω) − Bjy(tj, ω).
(11)
Note that this deﬁnition can be rewritten more suggestively as
bx(tj, ω) = ϵj(ω) + Bjy(tj, ω)
(12)
for each j = 1, . . . , p − 1.
The formula (10) shows that on each subinterval [tj, tj+1]
the estimate of the reference signal at tj+1 is obtained from
the initial estimate at tj by adding the optimal LSL estimate
of the increment.
Now, consider a signal estimation at any t ∈ [a, b]. The
ﬁrst step is simply to extend the formulæ (10) and (12) to all
t ∈ [tj, tj+1] by deﬁning
bx(t, ω) = ϵj(ω) + Bjy(t, ω).
(13)
Thus, the incremental estimation across each subinterval is
extended to every point within the subinterval. Because of
determining estimate bx(tj+1, ω) in the form (8)–(10) we
interpret this procedure as the LSL piecewise interpolation.
The incremental estimations are collected together in the
following way. For each j = 1, 2, . . . , p − 1, write
Fj[y(t, ω)] = ϵj(ω) + Bjy(t, ω)
(14)
for all t ∈ [tj, tj+1] and hence deﬁne the piecewise LSL
interpolation estimator by setting
F (p−1)[y(t, ω)] =
p−1
X
j=1
Fj[y(t, ω)][u(t − tj) − u(t − tj+1)]
(15)
for all t ∈ [a, b] where u(t) =
½
1
for t > 0
0
otherwise.
is the unit
step function. Thus we can now use the estimate
bx(t, ω) = F (p−1)[y(t, ω)]
(16)
for all (t, ω) ∈ T × Ω. The idea of a piecewise LSL
interpolation estimator on T seems intuitively reasonable for
signals with a well deﬁned gradient over T.
We note that by (9)-(16), the estimator F (p−1) is adaptive
to a variation of signals in K(p)
x . A change of signals in K(p)
x
implies a change of determinations of sub-estimators Bj in
(9) and keep the same structure of the F (p−1).
103
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-283-7
ICCGI 2013 : The Eighth International Multi-Conference on Computing in the Global Information Technology

C. Justiﬁcation of the LSL interpolation estimator
We wish to justify the proposed estimator by establishing
an upper bound for the associated error.
To explain the technical details we introduce some further
terminology.
Let us denote ∥x(t, ·)∥2
Ω =
R
Ω ∥x(t, ω)∥2dµ(ω). Assume
that for all t ∈ T, we have
∥x(t, ·)∥2
Ω < ∞
and
∥y(t, ·)∥2
Ω < ∞,
(17)
where ∥x(t, ω)∥ and ∥y(t, ω)∥ are the Euclidean norms for
x(t, ω) and y(t, ω) for each (t, ω) ∈ T × Ω, respectively.
Thus we will say that the signals are square integrable in ω
and write x(t, ·) ∈ L2(Ω) and y(t, ·) ∈ L2(Ω) for each ﬁxed
t ∈ T.
For each t ∈ T, let F = {f : T × Ω → Rm | f(t, ·) ∈
L2(Ω, Rm)} and deﬁne
∥f∥T,Ω =
1
b − a
Z
T ×Ω
∥f(t, ω)∥ dt dµ(ω)
=
1
b − a
Z
T
E[∥f(t, ·)∥] dt
for each f ∈ F where ∥f(t, ω)∥ is the Euclidean norm of
f(t, ω) on Rm for all (t, ω) ∈ Rm. Suppose that for all
(x, y) ∈ P there exist constants γj, δj > 0 such that
∥x(s, ω) − x(t, ω)∥ ≤ γj|s − t|,
(18)
∥y(s, ω) − y(t, ω)∥ ≤ δj|s − t|
(19)
for all (s, ω), (t, ω) ∈ [tj, tj+1] × Ω, i.e. we suppose that the
Lipschitz constants in (18) are independent of ω.
The error bound for the piecewise LSL interpolation esti-
mator is established in Theorem 1 below.
Theorem 1: If condition (18) is satisﬁed then the error
ϵp = ∥x − F (p−1)[y]∥T,Ω associated with the piecewise LSL
interpolation estimator satisﬁes the inequality
ϵp ≤
max
j=1,...,p−1
{(γj + ∥Bj∥2δj)|tj+1 − tj|
(20)
+
h
∥E1/2
vj,vj∥2
F − ∥Evjwj(E1/2
wjwj)†∥2
F
i1/2
}
(21)
where ∥Bj∥2 denotes the 2-norm given by the square root of
the largest eigenvalue of BT
j Bj and ∥·∥ denotes the Frobenius
norm.
Example 2: The time interval T is the same as in Example
1. At each time τk, for k = 1, . . . , N, the training reference
signal x(τk, ·) is represented by its realizations as a 4 × 4
matrix
X(k) = [x(τk, ω1), . . . , x(τk, ω4)] = {˜x(k)
ℓ,r }4
ℓ,r=1.
(22)
where
x(k)
1,1
=
0.918η(k)
1 , x(k)
1,2
=
1.02η(k)
2 , x(k)
1,3
=
1.122η(k)
3 , x(k)
1,4
=
0.918η(k)
4 , η(k)
1
=
− cos(2k), η(k)
2
=
sin(cos(k)), η(k)
3
= − cos(k), η(k)
4
= cos(sin(k)). All training
reference signals are simulated as a 4 × 484 matrix X =
[X(1), . . . , X(121)] shown in Fig. 2 (a). Note that in (3), due
to measurement errors the values of ˜x(k)
ℓ,r are different from
0
100
200
300
400
500
1
1.5
2
2.5
3
3.5
4
−1.5
−1
−0.5
0
0.5
1
1.5
(a) Training reference signals.
0
100
200
300
400
500
1
1.5
2
2.5
3
3.5
4
−1.5
−1
−0.5
0
0.5
1
1.5
(b) Their estimates by ﬁlter F (10).
Fig. 2.
Training signals and their estimates considered in Example 2.
the values of x(k)
ℓ,r . The observed signals in Example 1 were
simulated from X by adding random noise.
The estimates of the reference signals by ﬁlter F (p−1), for
p = 11, obtained on the basis of the information represented
in Fig. 1 (b) are given in Fig. 2 (b). The covariance matrices
are estimated from samples Y (j) and ˜X(j) taken at times tj,
for j = 1, . . . , 11 (see Example 1). The averaging polynomial
ﬁlter [16] gives much worse accuracy.
III. CONCLUSION
The piecewise least squares linear (LSL) interpolation esti-
mator was developed to estimate a large set of random signals
of interest from the set of observed data. The distinctive feature
is that a priori information can be obtained on only a few
reference signals in the form of samples. Since no information
of the major part of the set of reference signals is known, such
a procedure is called almost blind estimation.
The proposed estimator mitigates to some extent the difﬁ-
culties associated with existing estimation approaches such as
the necessity to know information (in the form of a sample,
for instance) on each random reference signal; invertibility
of the matrices used to deﬁne the estimators; and demanding
computational work.
REFERENCES
[1] E. Zeidler, Applied Functional Analysis, Applications to Mathematical
Physics, Applied Mathematical Sciences 108, Springer, 1997.
104
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-283-7
ICCGI 2013 : The Eighth International Multi-Conference on Computing in the Global Information Technology

[2] Y. Hua, Fast maximum likelihood for blind identiﬁcation of multiple FIR
channels, IEEE Trans. on Signal Processing, 44, no. 3, pp. 661-672, 1996.
[3] K. Georgoulakis and S. Theodoridis, Blind and semi-blind equalization
using hidden Markov models and clustering techniques, Sifnal Processing,
80, Issue 9, pp. 1795-1805, 2000.
[4] V. Zarzoso and P. Comon, Blind and Semi-Blind Equalization Based on
the Constant Power Criterion, IEEE Trans. on Signal Processing, 53, 11,
pp. 4363–4375, 2005.
[5] C.–Y. Chi, C.–H. Chen, C.–C. Feng, C.–Y. Chen, Blind Equalization and
System Identiﬁcation, Springer, 2006.
[6] Y. Hua, M. Nikpour, and P. Stoica, Optimal Reduced-Rank estimation
and ﬁltering, IEEE Trans. on Signal Processing, vol. 49, pp. 457-469,
2001.
[7] Y. Hua and W. Q. Liu, Generalized Karhunen-Lo`eve transform, IEEE
Signal Processing Letters, vol. 5, pp. 141-143, 1998.
[8] S. Haykin, Adaptive Filter Theory, Prentice Hall, ISBN 0-13-048434-2,
2002.
[9] J. Chen, J. Benesty, Y. Huang, and S. Doclo, New Insights Into the Noise
Reduction Wiener Filter, IEEE Trans. on Audio, Speech, and Language
Processing, 14, no. 4, 1218 - 1234, 2006.
[10] M. Spurbeck and P. Schreier, Causal Wiener ﬁlter banks for periodically
correlated time series, Signal Processing, 87, 6, pp. 1179-1187, 2007.
[11] J. S. Goldstein, I. Reed, and L. L. Scharf, A Multistage Representation
of the Wiener Filter Based on Orthogonal Projections, IEEE Trans. on
Information Theory, vol. 44, pp. 2943-2959, 1998.
[12] V. J. Mathews and G. L. Sicuranza, Polynomial Signal Processing, J.
Wiley & Sons, 2001.
[13] A. P. Torokhti and P. G. Howlett, An Optimal Filter of the Second Order,
IEEE Trans. on Signal Processing, 49, No 5, 1044-1048, 2001.
[14] A. Torokhti and P. Howlett, Method of recurrent best estimators of
second degree for optimal ﬁltering of random signals, Signal Processing,
83, 5, 1013-1024, 2003.
[15] A. Torokhti and P. Howlett, Optimal Transform Formed by a Combina-
tion of Nonlinear Operators: The Case of Data Dimensionality Reduction,
IEEE Trans. on Signal Processing, 54, no. 4, 1431-1444, 2006.
[16] A. Torokhti and P. Howlett, Computational Methods for Modelling of
Nonlinear Systems, Mathematics in Science and Engineering, 212, Series
editor C. K. Chui, Elsevier, 2007.
[17] A. Torokhti and P. Howlett, Filtering and Compression for Inﬁnite Sets
of Stochastic Signals, Signal Processing, 89, pp. 291-304, 2009.
[18] A. Torokhti and J. Manton, Generic Weighted Filtering of Stochastic
Signals, IEEE Trans. on Signal Processing, 57, issue 12, pp. 4675-4685,
2009.
[19] H. A. Cirpan and M. K. Tsatsanis, Stochastic Maximum Likelihood
Methods for Semi-Blind Channel Estimation, IEEE Signal Processing
Letters, 5, no. 1, pp. 21–24,1998.
[20] G. Kutz and D. Raphaeli, Maximum-Likelihood Semiblind Equalization
of Doubly Selective Channels Using the EM Algorithm, EURASIP
Journal on Advances in Signal Processing, Springer Open J., 2010.
[21] D. He and H. Leung, Semi-Blind Identiﬁcation of ARMA Systems Using
a Dynamic-Based Approach, IEEE Trans. on Circuits and Systems-I, 52,
1, pp. 179–190, 2005.
[22] J. Even and K. Sugimoto, An ICA approach to semi-blind identiﬁcation
of strictly proper systems based on interactor polynomial matrix, Int. J.
Robust Nonlinear Control, 17, pp. 752768, 2007.
[23] R.E. Kalman, A New Approach to Linear Filtering and Prediction
Problems, Transactions of the ASMEJournal of Basic Engineering, 82
(Series D), 35-45 1960.
[24] S. Kang and L. Chua, A global representation of multidimensional
piecewise-linear functions with linear partitions, IEEE Trans. on Circuits
and Systems, 25 Issue:11, pp. 938 - 940, 1978.
[25] L.O. Chua and A.-C. Deng, Canonical piecewise-linear representation,
IEEE Trans. on Circuits and Systems, 35 Issue:1, pp. 101 - 111, 1988.
[26] J.-N. Lin and R. Unbehauen, Canonical piecewise-linear approximations,
IEEE Trans. on Circuits and Systems I: Fundamental Theory and Appli-
cations, 39 Issue:8, pp. 697 - 699, 1992.
[27] P. Julian, A. Desages, B. D’Amico, Orthonormal high-level canonical
PWL functions with applications to model reduction, IEEE Trans. on
Circuits and Systems I: Fundamental Theory and Applications, 47 Issue:5,
pp. 702 - 712, 2000.
[28] J.E. Cousseau, J.L. Figueroa, S. Werner, T.I. Laakso, Efﬁcient Nonlinear
Wiener Model Identiﬁcation Using a Complex-Valued Simplicial Canon-
ical Piecewise Linear Filter, IEEE Trans. on Signal Processing, 55 5, pp.
1780 - 1792, 2007.
[29] A. Ben-Israel and T. N. E. Greville, Generalized Inverses: Theory and
Applications, John Wiley & Sons, New York, 1974.
[30] P. G. Howlett, P. Pudney, and X. Vu, Local energy minimization
in optimal train control, Automatica, 45(11), 2692–2698, 2009. DOI:
10.1016/j.automatica.2009.07.028.
105
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-283-7
ICCGI 2013 : The Eighth International Multi-Conference on Computing in the Global Information Technology

