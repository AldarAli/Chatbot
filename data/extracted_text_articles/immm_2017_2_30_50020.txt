Decision Making by a Fuzzy Regression Model with Modiﬁed Kernel
Kiyoshi Nagata
Faculty of Business Administration
Daito Bunka University
Tokyo 175-8571, Japan
Email: nagata@ic.daito.ac.jp
Michihiro Amagasa
Faculty of Business Administration
Hokkai Gakuen University
Sapporo 062-8605, Japan
Email: amagasa@ba.hokkai-s-u.ac.jp
Abstract—Regression model is a popular and powerful model
for ﬁnding a rule from large amount of collected data. It is
widely used in various areas for predicting the value derived
from observable values. Especially in multivariate numerical
analysis, several types of regression models, not only linear but
also polynomial or exponential, are established. In case of non-
numerical data, although fuzzy regression models are proposed
and investigated by some researchers, most of them are linear
models. In order to construct a non-linear regression model
with fuzzy type data set, new type of devices are needed since
fuzzy numbers have a complicated behavior in multiplication and
division. In this paper, we try to extend a linear fuzzy regression
model to non-linear model by adapting a modiﬁed kernel method.
Keywords–Fuzzy regression model; Kernel method; Decision
making.
I.
INTRODUCTION
As an analysis method of numerical big-data mining, the
regression model is still playing an important role. However,
the huge amount of data processing requires strong computing
power and resources. In particular, when handling data with
non-linear features, ﬁnding a proper regression model is not
easy, sometimes even infeasible. The kernel method, so-called
a kernel trick, is one of smart devices solving this kind of
problem. A kernel deﬁned on the product of a data set induces
an element of Hilbert space, a space of functions with an inner
product, and considering a linear model in the space gives
us a non-linear model in the original space. Thus, only the
calculation of kernels for the given data set is non-linear, and
the calculation for solving the problem to give a model is
performed in the linear operation method. The kernel method
is applied to many analytical systems, such as the Principal
Component Analysis (PCA), [16], the Canonical Correction
Analysis (CCA), [6], [12], Fisher’s Linear Discriminant Anal-
ysis (LDA), [13], the Support Vector Machine(SVM), [1], [7],
the regression model, [14], [17], etc.
In the real world, the collected data are sometimes ex-
pressed in linguistic values, and in order to apply well-known
and authorized stochastic methods such as regression analysis,
these values are transformed into numerical data. For instance,
the price of a production or a service are determined from
several factors, such as price of raw materials, selling expenses,
consumer demand, etc. Also the price has high correlations
with the customer value of product or service. Bradley T. Gale
proposed a scenario where price satisfaction carries 40% of
the weight and non-price attributes 60% in the customer-value
equation, and showed a ﬁgure representing the relationship
between relative performance overall score and relative price
for luxury cars based on data [9, pp. 218-219]. In that ﬁgure,
the relative price is generically expressed in linguistic values
such as “Higher”, “Lower”, etc., then these values are trans-
formed into numerical values in order to plot corresponding
points on the performance-price plane. For the price prediction
model, Inoue et al. proposed a sale price prediction model by
fuzzy regression, [11], and Michihiro Amagasa, also proposed
a method to handle data with uncertainty in the model of
regression analysis as an extension of their model, [3]. We
also give a precise formulation of a multi-variable regression
model where both explanatory variables and objective one are
L-R type fuzzy numbers, [4].
Construction approaches for regression models handling
fuzzy set are roughly divided into two types, one is Fuzzy Least
Square Regression (FLSR) and the other is dual model for
possibilistic regression. The concept of FLSR model is similar
to that of ordinary regression model where each value of three
vertexes is processed to minimize the sum of distances between
the given data and the estimated values. D’Urso adopts this
approach handling linear regression model with several types
of input-output data, such as crisp-fuzzy, fuzzy-crisp, and
fuzzy-fuzzy, with not only type1 fuzzy data set but also type2
fuzzy data set, [8]. The dual model of possibilistic regression
approach, originally proposed by Tanaka et al., [18], [19], gives
upper and lower regression model by using linear programing
analysis approach. Although their model is extended to non-
linear models, [10], explanatory variables are still crisp values.
In this paper, we propose a non-linear regression model of
fuzzy input-fuzzy output type as an extension of our previously
proposed model in [4] by applying the kernel method.
The rest of the paper is organized as follows: In Section II,
we will review general theory of the kernel method and give a
concrete construction of quadratic kernel for a small number
of variables. Section III is dedicated to a brief explanation of
Guo and Tanaka’s non-linear fuzzy regression model and the
details of our linear model. Then, in Section IV, we describe
the extension version of our model into non-linear type with
modiﬁed kernels. Examples to see how the proposed model
works are coming up with some discussions. The last section,
Section V, is the conclusion and the future works.
II.
KERNEL THEORY
First, we give a brief description of kernel theory, then
give an expression of the functions in the reproducing kernel
Hilbert space for a quadratic kernel.
18
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-566-1
IMMM 2017 : The Seventh International Conference on Advances in Information Mining and Management

A. Overview of Kernel Theory
For any set X and the Hilbert space H of functions on X
over R, a positive deﬁnite kernel is a map
k : X × X → H
satisfying
• k(x, y) = k(y, x) for any x, y ∈ X,
• For any {ci} ⊂ R and any {xi} ⊂ X,
X
cicjk(xi, xj) ≥ 0.
Here, we give some examples of kernel over Rk.
For ⃗x = (x1, . . . , xk), ⃗y = (y1, . . . , yk),
• k(⃗x, ⃗y) = ⃗xt⃗y = Pk
i=1 xiyi (linear kernel)
• kP (⃗x, ⃗y) = (⃗xt⃗y + c)d,
with c ≥ 0, 0 < d ∈ Z (polynomial kernel)
• kE(⃗x, ⃗y) = exp(β⃗xt⃗y), with β > 0 (exponential kernel)
• kG(⃗x, ⃗y) = exp(−
1
2σ2 ∥⃗x − ⃗y∥2)
(Gaussian radial basis function kernel)
• kL(⃗x, ⃗y) = exp(−α Pk
i=1 |xi − yi|) (Laplacian kernel)
If, for any x ∈ X, there exists a function kx ∈ H such that
f(x) =< f, kx >H, (∀f ∈ H)
(1)
where < ·, · >H is the inner product of the Hilbert space, the
Hilbert space H is called a Reproducing Kernel Hilbert Space
(RKHS). It is shown that kx ∈ H is unique, and k(·, x) = kx
is a positive deﬁnite kernel on X called the reproducing kernel.
Conversely, the following theorem is known, [5].
Theorem 1. (Moore-Aronszajn) For any positive deﬁnite ker-
nel on X, there exist unique Hilbert space H satisfying
1) k(·, x) ∈ H (for any x ∈ X),
2) The subspace spanned by {k(·, x); x ∈ X} is dense in H,
3) k is the reproducing kernel of H.
Although Hilbert space has inﬁnity dimension, solution of
some optimization problem with data, if there is any, can be
expressed as a linear combination of at most the number of data
elements in H. This is guaranteed by the following theorem,
[15].
Theorem 2. (The Representer Theorem) Let k be a kernel on
X and let H be its associated RKHS. Fix x1, · · · , xn ∈ X,
and consider the optimization problem
min
f∈H D(f(x1), . . . , f(xn)) + P(∥f∥2
H)
(2)
where
P
is
nondecreasing
and
D
depends
only
on
f(x1), · · · , f(xn). If there is a minimizer, then it has the form
of
f =
n
X
i=1
aik(·, xi)
(3)
with some a1, · · · , an ∈ R. Furthermore, if P is strictly
increasing, then every solution has this form.
B. Example Expression of RKHS Basis
From the representer theorem, we can express an optimal
function as in the form of (3). However, if the given data set
is big, we will have many unknown variables {ai}i=1,...,n to
be determined. For the convenience of calculation, we try to
reduce the number of components for the polynomial kernel
and give an example for the quadratic polynomial kernel of
the case that d = 2 and k = 3 variables.
From the representer theorem and the equation below,
kP (⃗x, ⃗y) = (Pk
j=1 xjyj + c)d
=
X
0 ≤ e1 + · · · + ek ≤ d
0 ≤ ej
cd−(e1+·+ek)xe1
1 · · · xek
k ye1
1 · · · yek
k
we have that for any (e1, . . . , ek) such that 0 ≤ e1+· · ·+ek ≤
d, 0 ≤ ei, there exist N =k+d Cd vectors, ⃗x1, . . . ⃗xN, and
a1, . . . , aN satisfying
PN
i=1 aixf1
i1 · · · xfk
ik
=
c−(d−(e1+·+ek))
if(f1 . . . , fk) = (e1, . . . , ek),
0
otherwise.
(4)
In a simple case of d = 2 and k = 3 then N =5 C2 = 10,
and the left side of equation (4) is expressed as














x2
11
x2
21
· · ·
x2
10 1
x2
12
x2
22
· · ·
x2
10 2
x2
13
x2
23
· · ·
x2
10 3
x11
x21
· · ·
x10 1
x12
x22
· · ·
x10 2
x13
x23
· · ·
x10 3
x11x12
x21x22
· · ·
x10 1x10 2
x11x13
x21x23
· · ·
x10 1x10 3
x12x13
x22x23
· · ·
x10 2x10 3
1
1
· · ·
1





















a1
a2
...
...
a10







.
However, we only have to determine ⃗x1, ⃗x2, ⃗x3 and solve
the 10 equations of (4) shown as follows.


x2
1j
x2
2j
x2
3j
x1j
x2j
x3j
1
1
1


 a1
a2
a3
!
=
 1
0
0
!
,
 0
c
0
!
,


0
0
c2

 ,
or


x2
1j
x2
2j
x2
3j
x1j
x2j
x3j
x1jx1l
x2jx2l
x3jx3l


 a1
a2
a3
!
=
 0
0
1
!
,
where j, l = 1, 2, 3 and j ̸= l. Just analyzing the invertibility
of these matrices, we have 10 functions spanning the dense
subspace H′
k of Hk.
H′
k =< k(·, ⃗xi); i = 1, . . . , 10 >R,
where
⃗x1 = (1, 0, 0), ⃗x2 = (0, 1, 0), ⃗x3 = (0, 0, 1),
⃗x4 = (−1, 0, 0), ⃗x5 = (0, −1, 0), ⃗x6 = (0, 0, −1),
⃗x7 = (1, 1, 0), ⃗x8 = (0, 1, 1), ⃗x9 = (1, 0, 1), and
⃗x10 = (0, 0, 0).
III.
SOME EXISTING FUZZY REGRESSION MODEL
In this section, we will give a brief explanation of two fuzzy
regression models, one is crisp-input and fuzzy-output type by
Guo and Tanaka, and the other is fuzzy-input and fuzzy-output
type.
19
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-566-1
IMMM 2017 : The Seventh International Conference on Advances in Information Mining and Management

A. Guo and Tanaka’s Non-Linear Model
Guo and Tanaka have investigated the dual possibilistic
regression models of both linear and non-linear types with
crisp-input and symmetric triangular fuzzy-output in [10]. At
ﬁrst, the linear model whose output Y = (y; p)F = (y; p, p)F
from crisp input values for variables xj (j = 1, . . . , k) is
deﬁned as follows,
Y = A1x1 + A2x2 + · · · + Akxk,
(5)
with symmetric fuzzy coefﬁcients Aj
= (aj; rj)F (j
=
1, . . . , k). In this formula, the value of Y is obtained by
calculating (Pk
j=1 ajcj, Pk
j=1 rj|cj|), once explicit values
c1, . . . , ck for each given variable. When we have a data set of
n number of data, {(Yi; xi1, · · · , xik)}i=1,...,n with crisp xij
and symmetric fuzzy numbers Yi = (yi; pi)F , we consider the
upper regression model and the lower regression model.
For the upper regression model, try to ﬁnd fuzzy coefﬁ-
cients A∗
j = (a∗
j; r∗
j )F such that
Minimizing: J(⃗r∗) = Pk
j=1 r∗
j
Pn
i=1 |xij|

,
under the condition
Yi ⊆ Y ∗
i = A∗
1xi1 + · · · + A∗
kxik
(i = 1, . . . , n).
(6)
The inclusion condition above can be expressed by the follow-
ing equations, because the shapes of fuzzy set are supposed to
be similar




yi − pi ≥ Pk
j=1 a∗
jxij − Pk
j=1 r∗
j |xij|
yi + pi ≤ Pk
j=1 a∗
jxij + Pk
j=1 r∗
j |xij|
r∗
j ≥ 0
.
(7)
For the lower regression model, try to ﬁnd fuzzy coefﬁ-
cients Aj∗ = (aj∗; rj∗)F such that
Maximizing: J(⃗r∗) = Pk
j=1 rj∗
Pn
i=1 |xij|

,
under the condition
Yi ⊇ Yi∗ = A1∗xi1 + · · · + Ak∗xik
(i = 1, . . . , n).
(8)
The inclusion condition above also can be expressed by the
following equations.



yi − pi ≤ Pk
j=1 aj∗xij − Pk
j=1 rj∗|xij|
yi + pi ≥ Pk
j=1 aj∗xij + Pk
j=1 rj∗|xij|
rj∗ ≥ 0
.
(9)
For the existence of upper and lower regression model, Guo
and Tanaka showed the following theorem.
Theorem 3. (by Guo and Tanaka, [10])
1) There always exists an optimal solution in the upper
regression model (6) under (7) .
2) There exists an optimal solution in the lower regres-
sion model (8) under (9) if and only if there exist
a(0)
1∗ , a(0)
2∗ , . . . , a(0)
k∗ satisfying
yi −pi ≤
k
X
j=1
a(0)
j∗ xij ≤ yi +pi
(i = 1, . . . , n). (10)
From the theorem, there might not be any optimal solution
for the lower regression model. This problem is caused by the
relationship between the number of variables and the number
of data. They tried to solve the problems by extending the
model into non-linear model which has more formal variables
xixj (i, j = 1, . . . , k) in the following formula.
Y = A0 +
k
X
j=1
Ajxj +
k
X
j,l=1
Ajlxjxl,
(11)
with symmetric fuzzy coefﬁcients Aj, Ajl (j, l = 1, . . . , k).
The right hand side has a quadratic part when considering xi
variables, however we need to ﬁnd Aj and Ajl for a given
data set which minimize or maximize the value, so this might
be solved by LP method.
B. Our Linear Model
As a general type of fuzzy number, we consider L-
R fuzzy set with monotone decreasing functions satisfying
L(0) = R(0) = 1 and L(1) = R(1) = 0, and denote a L-
R fuzzy set by Y = (y; p, q)F , where y is the value giving
the maximum uncertainty, e.g., 1, and p, q are left and right
range from y, i.e., y − p and y + q give the uncertainty value
0, [2]. We proposed the following type of possibilistic fuzzy
regression model
Y = A1X1 + A2X2 + · · · + AkXk,
(12)
with L-R fuzzy variables Y
=
(y; p, q)F
and Xj
=
(xj; wj, zj)F and L-R fuzzy coefﬁcients Aj = (aj; rj, sj)F
(j = 1, . . . , k).
Let [Y ]h be the support of fuzzy number Y above h-cut
line, we have
[Y ]h = [y − pL−1(h), y + qR−1(h)],
[Xj]h = [xj − wjL−1(h), xj + zjR−1(h)],
[Aj]h = [aj − rjL−1(h), aj + sjR−1(h)].
Applying commonly known multiplication and summation of
L-R fuzzy numbers, we have
[Pk
j=1 AjXj]h =
[Pk
j=1(aj − rjL−1(h))(xj − wjL−1(h)),
Pk
j=1(aj + sjR−1(h))(xj + zjR−1(h))]h,
and the range of the interval, denoted by J, is calculated by
subtracting the left end value from the right end value. Then
J
=
Pk
j=1{(zjR−1(h) + wjL−1(h))aj
+(xj + zjR−1(h))R−1(h)sj
+(xj − wjL−1(h))L−1(h)rj}.
Following Guo and Tanaka, we consider upper and lower
models, and describe the inclusion relation of the support of
Yi and that of the obtained fuzzy number in the regression
model for a given data set.
Now we let ZWj, XZj, XWj be as follows,



ZWj = (Pn
i=1 zij)R−1(h) + (Pn
i=1 wij)L−1(h)
XZj = ((Pn
i=1 xij) + (Pn
i=1 zij)R−1(h))R−1(h)
XWj = ((Pn
i=1 xij) − (Pn
i=1 wij)L−1(h))L−1(h)
.
(13)
Then our upper model Y ∗ is constructed with A∗
j
=
(a∗
j; r∗
j , s∗
j)F , such that
Minimizing: J(A∗) = Pk
j=1(ZWja∗
j + XZjs∗
j + XWjr∗
j ),
where A∗ = (A∗
1, . . . , A∗
k),
(14)
20
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-566-1
IMMM 2017 : The Seventh International Conference on Advances in Information Mining and Management

under the condition that for all i











yi − piL−1(h) ≥ Pk
j=1(a∗
j − r∗
j L−1(h))×
(xij − wijL−1(h))
yi + qiR−1(h) ≤ Pk
j=1(a∗
j + s∗
jR−1(h))×
(xij + zijR−1(h))
r∗
j , s∗
j ≥ 0
.
(15)
The lower model Y∗ is similarly constructed with Aj∗ =
(aj∗; rj∗, sj∗)F , such that
Maximizing: J(A∗) = Pk
j=1(ZWjaj∗ + XZjsj∗ + XWjrj∗),
where A∗ = (A1∗, . . . , Ak∗),
(16)
under the condition that for all i











yi − piL−1(h) ≤ Pk
j=1(aj∗ − rj∗L−1(h))×
(xij − wijL−1(h))
yi + qiR−1(h) ≥ Pk
j=1(aj∗ + sj∗R−1(h))×
(xij + zijR−1(h))
rj∗, sj∗ ≥ 0
.
(17)
We could also show the following theorem similar to the
Theorem 3 on the existence of models.
Theorem 4. When xij − wijL−1(h) > 0 (i = 1, . . . , n, j =
1, . . . , k), then
1) There always exists an optimal solution in the upper
regression model (14) under (15) .
2) There exists an optimal solution in the lower regres-
sion model (16) under (17) if and only if there exist
a(0)
1∗ , a(0)
2∗ , . . . , a(0)
k∗ satisfying
(
yi − piL−1(h) ≤ Pk
j=1(xij − wijL−1(h))a(0)
j∗
yi + qiR−1(h) ≥ Pk
j=1(xij + zijR−1(h))a(0)
j∗
.
(18)
Proof.
1) If xij − wijL−1(h) ≥ 0 in (15), then xij > 0 from
wij ≥ 0 and 0 ≤ L−1(h) ≤ 1. Therefore xij +zijR−1(h)
are also non-negative, and sufﬁciently large r∗
j and s∗
j
satisfy the condition.
2) If there exist Aj∗ = (aj∗; rj∗, sj∗)F (j = 1, . . . , k) satis-
fying (17), then we have the condition (18). Conversely,
for a(0)
j∗ satisfying (18), put A(0)
j∗ = (a(0)
j∗ ; 0, 0)F and they
satisfy the condition (17).
□
Remark1:
When the data for independent variables are
given in linguistic values, they are usually transformed into
fuzzy numbers satisfying the condition xij − wijL−1(h) >
0 (i = 1, . . . , n, j = 1, . . . , k). So, the assumptions in the
Theorem 4 are not special condition.
Remark2: The condition (18) means the inclusion relation
between Yi and the resulted fuzzy number Yi∗ of areas between
h-cut horizontal line and the base-line (h = 0) of them.
Remark2.1: In case of h = 1, L−1(1) = R−1(1) = 0 and
(18) is reduced to
yi =
k
X
j=1
xija(0)
j∗ ,
which means that the line segment of Yi∗ is in the area of Yi.
Remark2.2: In case of h = 0, L−1(0) = R−1(0) = 1 and
(18) is reduced to
(
yi − pi ≤ Pk
j=1(xij − wij)a(0)
j∗ ≤ Pk
j=1 xija(0)
j∗
yi + qi ≥ Pk
j=1(xij + zij)a(0)
j∗ ≥ Pk
j=1 xija(0)
j∗
.
which means that Yi∗ ∩ Yi ̸= φ.
IV.
REGRESSION METHOD WITH KERNEL
We extend our linear model to a regression model with a
kernel-like function, we call modiﬁed kernel, on a set of L-
R fuzzy number. First we describe a general formula, then
give more precise formula as an extension of the polynomial
kernel, kP (x, y), for the case of degree d = 2 and the number
of explanatory variables k = 3 as described in B. of section
II.
A. General Formula
We suppose that there exists a function K(X, Y ) satisfying
only K(Y, X) = K(X, Y ) on the product of a set of fuzzy
numbers, X k
F ×X k
F to XF . Actually, we use a function induced
from one of kernels explained in A. of section II if it can be
well-deﬁned on fuzzy numbers.
For a given data set of L-R fuzzy numbers, {(Yi, Xi); i =
1, . . . , M}, where Yi = (yi; pi, qi)F , Xi = (Xi1, . . . Xik)F
with Xij = (xij; wij, zij)F (i = 1, . . . , M, j = 1, . . . , k). We
just modify the formula (12) by replacing Xj with K(X, Xi),
and consider the model
Y = A1K(X, X1)+A2K(X, X2)+· · ·+AMK(X, XM), (19)
where X = (X1, . . . , Xk) is vector expression of the explana-
tion fuzzy variable and Y is the objective fuzzy variable. For
this formula, we can apply our proposed method for the dual
model with h-cut. Since the number of data, M, is usually
much greater than the number of explanatory variables, k, the
possibility of existence for the lower model increases from the
Theorem 4.
On the other hand, when M is very big, there will be too
many possible fuzzy number coefﬁcients {Ai} for both upper
and lower model. Thus, try to ﬁnd smaller set of representer
if possible, and denote their number by N. Then fuzzy
coefﬁcients A∗ = (A∗
1, . . . , A∗
N) and A∗ = (A1∗, . . . , AN∗)
are calculated for upper and lower models from the following
formulas of fuzzy numbers respectively,
A1K(Xi, ˜X1) + A2K(Xi, ˜X2) + · · · + ANK(Xi, ˜XN), (20)
where i = 1, . . . , M, and {˜Xl; l = 1, . . . , N} is a representer.
B. Case of Modiﬁed Polynomial Kernel
Here we consider a modiﬁed kernel induced from polyno-
mial kernel, kP (x, y), denoted by KF (X, ˜X) = (Xt ˜X + C)d.
When we could ﬁnd N(=k+d Cd) number of proper value vec-
tors ⃗˜xl = (˜xl1, . . . , ˜xlk) (l = 1, . . . , N) for the dense subspace
of HkP , put ˜Xl = ( ˜Xl1, . . . , ˜Xlk) with ˜Xli = (˜xli; 0, 0)F
(l = 1, . . . , N).
Now calculate the h-cut of the equation (20) for C =
(c; 0, 0)F in the way of B. of section III. When putting ⃗xi =
21
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-566-1
IMMM 2017 : The Seventh International Conference on Advances in Information Mining and Management

(xi1, . . . , xik), ⃗wi = (wi1, . . . , wik), ⃗zi = (zi1, . . . , zik),
i = 1, . . . , M,we have
[Xi]h = ([Xi1]h, . . . , [Xik]h) = [⃗xi−L−1(h)⃗wi, ⃗xi+R−1(h)⃗zi],
and the h-cut of the modiﬁed kernel is as follows,
[K(Xi, ˜Xl)]h =
Pk
j [Xij]h[ ˜Xlj]h + [C]h
d
=
Pk
j=1(xij − wijL−1(h))˜xlj + c
d
,
Pk
j=1(xij + zijR−1(h))˜xlj + c
d
=
h
kP (⃗xi − L−1(h)⃗wi, ⃗˜xl), kP (⃗xi + R−1(h)⃗zi, ⃗˜xl)
i
.
Thus we have
 N
P
l=1
AlK(Xi, ˜Xl)

h
=
 N
P
l=1
(al − rlL−1(h))kP (⃗xi − L−1(h)⃗wi, ⃗˜xl),
N
P
l=1
(al + slR−1(h))kP (⃗xi + R−1(h)⃗zi, ⃗˜xl)

,
(21)
and minimizing or maximizing objective value is
J(A) =
N
P
l=1
al

1
M
PM
i=1

kP (⃗xi + R−1(h)⃗zi, ⃗˜xl)
−kP (⃗xi − L−1(h)⃗wi, ⃗˜xl)

+R−1(h)
N
P
l=1
sl 1
M
PM
i=1 kP (⃗xi + R−1(h)⃗zi, ⃗˜xl)
+L−1(h)
N
P
l=1
rl 1
M
PM
i=1 kP (⃗xi − L−1(h)⃗wi, ⃗˜xl),
(22)
where ⃗˜xl = (˜xl1, . . . , ˜xlk) for l = 1, . . . , N.
Then our upper model Y ∗ is constructed with A∗
j
=
(a∗
j; r∗
j , s∗
j)F minimizing J(A∗) under the condition that for
all i = 1, . . . , M,



















yi − piL−1(h) ≥
N
P
l=1
(a∗
l − r∗
l L−1(h))×
kP (⃗xi − L−1(h)⃗wi, ⃗˜xl)
yi + qiR−1(h) ≤
N
P
l=1
(a∗
l + s∗
l R−1(h))×
kP (⃗xi + R−1(h)⃗zi, ⃗˜xl)
r∗
j , s∗
j ≥ 0
.
(23)
The lower model Y∗ is similarly constructed with Aj∗ =
(aj∗; rj∗, sj∗)F maximizing J(A∗) under the condition that
for all i = 1, . . . , M,



















yi − piL−1(h) ≤
N
P
l=1
(al∗ − rl∗L−1(h))×
kP (⃗xi − L−1(h)⃗wi, ⃗˜xl)
yi + qiR−1(h) ≥
N
P
l=1
(al∗ + s∗lR−1(h))×
kP (⃗xi + R−1(h)⃗zi, ⃗˜xl)
rj∗, sj∗ ≥ 0
.
(24)
We also have the same kind of theorem as Theorem 4.
Theorem 5. When kP (⃗xi − L−1(h)⃗wi, ⃗˜xl) > 0 and kP (⃗xi +
R−1(h)⃗zi, ⃗˜xl) > 0 (i = 1, . . . , M, l = 1, . . . , N), then
1) There always exists an optimal solution in the upper
regression model under (23) .
2) There exists an optimal solution in the lower regression
model under (24) if and only if there exist a(0)
1∗ , . . . , a(0)
N∗
satisfying
(
yi − piL−1(h) ≤ PN
l=1 kP (⃗xi − L−1(h)⃗wi, ⃗˜xl)a(0)
l∗
yi + qiR−1(h) ≥ PN
l=1 kP (⃗xi + R−1(h)⃗zi, ⃗˜xl)a(0)
l∗
.
(25)
C. Illustrative Example
As an illustrative example, we consider a polynomial kernel
kP (x, y) of degree d = 2 and the number of explanatory
variables k = 3 cases, so the number of basis for the dense
subspace H′
k of Hk is N = 10. Only considering triangular
type fuzzy numbers, i. e., L = R is the linear function from
(0, 1) to (1, 0) and L−1(h) = R−1(h) = 1 − h, and using the
base vectors given in B. of section II, we have
˜Xl = ( ˜Xl1, ˜Xl2, ˜Xl3)
(l = 1, . . . , 10) with
˜X11 = (1; 0, 0)F , ˜X22 = (1; 0, 0)F , ˜X33 = (1; 0, 0)F ,
˜X41 = (−1; 0, 0)F , ˜X52 = (−1; 0, 0)F , ˜X63 = (−1; 0, 0)F ,
˜X71 = (1; 0, 0)F , ˜X72 = (1; 0, 0)F ,
˜X82 = (1; 0, 0)F , ˜X83 = (1; 0, 0)F ,
˜X91 = (1; 0, 0)F , ˜X93 = (1; 0, 0)F ,
˜Xlj = (0; , 0, 0)F
otherwise.
Here, we have M = 8 pairs of fuzzy numbers as an
example data set shown in Table I. From these fuzzy numbers,
calculate kP (⃗xi − L−1(h)⃗wi, ⃗˜xl) and kP (⃗xi + R−1(h)⃗zi, ⃗˜xl)
for each pair of (i, l) (i = 1, . . . , 8, l = 1, . . . , 10), then take
averages through i for each l. Notice that the calculation is
done using ⃗˜xl not ˜Xl,i.
Next, after setting the constant value for c and the value for
h-cut, solve two LP problems, one is for upper model with A∗
and the other is lower model with A∗, satisfying the conditions
(23) and (24) respectively.
TABLE I. DATA SET FOR THE ILLUSTRATIVE EXAMPLE
(y; p, q)F
(x1; w1, z1)F
(x2; w2, z2)F
(x3; w3, z3)F
(3.5; 1.5, 1.5)
(1.0; 0.5, 0.1)
(2.0; 0.5, 0.5)
(3.0; 0.5, 1.0)
(4.5; 2.0, 2.0)
(2.0; 0.5, 0.1)
(2.0; 0.5, 1.0)
(3.5; 0.75, 1.0)
(7.0; 2.5, 2.5)
(3.0; 0.1, 0.0)
(6.5; 0.5, 1.5)
(5.5; 1.0, 1.25)
(9.5; 2.0, 2.0)
(2.0; 0.5, 0.1)
(9.5; 1.0, 0.5)
(10.0; 2.0, 2.5)
(11.0; 3.0, 3.0)
(4.0; 0.5, 1.0)
(9.0; 1.0, 1.0)
(10.5; 3.0, 2.5)
(6.0; 2.0, 2.0)
(2.0; 0.0, 0.0)
(3.0; 1.0, 2.0)
(2.0; 0.5, 1.0)
(8.0; 2.5, 2.5)
(3.0; 0.1, 0.0)
(5.0; 1.5, 1.5)
(5.0; 1.5, 2.0)
(9.0; 3.0, 3.0)
(3.5; 0.5, 0.0)
(4.0; 0.5, 0.5)
(6.0; 2.0, 1.25)
By applying the solver function in MS-EXCEL, when
setting c = 1 and h = 0.3, for the upper model we have
A∗
1 = (0.218; 0, 0.038)F , A∗
6 = (0.030; 0, 0)F ,
A∗
10 = (1.455; 0, 5.230)F , A∗
l = (0; 0, 0)F
(for other l),
and
Y = A∗
1K(X, ˜X1) + A∗
6K(X, ˜X6) + A∗
10K(X, ˜X10).
(26)
22
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-566-1
IMMM 2017 : The Seventh International Conference on Advances in Information Mining and Management

For the lower model, we have
A1∗ = (0.160; 0, 0.038)F , A2∗ = (0.037; 0, 0)F ,
A3∗ = (0.002; 0, 0)F , A10∗ = (3.301; 0, 0.167)F ,
Al∗ = (0; 0, 0)F
(for other l),
and
Y
=
A1∗K(X, ˜X1) + A2∗K(X, ˜X2)
+A3∗K(X, ˜X3) + A10∗K(X, ˜X10).
(27)
Table II describes the correspondence of original values and
the resulted values by lower model (27) and by upper model
(26). The expression of fuzzy numbers here is not the same as
used so far in this paper. These values express the left edge, the
center point, and the right edge of each triangular shape. We
can see three corresponding fuzzy numbers have no inclusion
relation, because they are full numbers before operating h-cut
procedure. When looking at the support interval of h-cut of
each fuzzy set, we have the set relationship [Y∗]h ⊂ [Y ]h ⊂
[Y ∗]h. Figure 1 illustrates the relationship among three fuzzy
numbers from the second row in Table II.
TABLE II. COMPARISON: Y , Y ∗, AND Y∗
(y − p, y, y + q)
(y∗ − p∗, y∗, y∗ + q∗)
(y∗ − p∗, y∗, y∗ + q2)
(2.0, 3.5, 5.0)
(2.0, 2.4, 8.1)
(3.9, 4.3, 4.8)
(2.5, 4.5, 6.5)
(2.9, 3.6, 9.5)
(4.6, 5.1, 6.0)
(4.5, 7.0, 9.5)
(5.1, 5.6, 11.8)
(7.6, 8.0, 9.8)
(7.5, 9.5, 11.5)
(4.3, 5.8, 13.1)
(7.8, 9.1, 10.2)
(8.0, 11.0, 14.0)
(7.1, 9.6, 20.2)
(9.7, 11.3, 15.5)
(4.0, 6.0, 8.0)
(3.4, 3.4, 9.1)
(5.1, 5.4, 6.6)
(5.5, 8.0, 10.5)
(5.0, 5.4, 11.9)
(6.5, 7.3, 8.8)
(6.0, 9.0, 12.0)
(5.2, 6.6, 13.0)
(6.7, 7.6, 8.7)
Figure 1. Relationship of Three Fuzzy Numbers
We also tried other type of kernels for these test data, and
may have some discussion on the ﬁtness.
V.
CONCLUSION
As an extension of our fuzzy dual linear regression model,
we proposed to apply kernel method and give a general formula
with a modiﬁed kernel of polynomial type. Then, we showed
how it works using artiﬁcial sample data set for illustration of
performance in a simple case.
Although we could see that the kernel method can be
incorporated with fuzzy regression model, the effectiveness of
our method, depending on data set type, is not yet clear. In
the example handling small data, when changing the values
slightly, we could not have any solution for the lower model.
This infeasibility also occurs by increasing the value of h,
which may reduce the degree of freedom of resulted fuzzy
number of lower model. Though the number of data is less
than the number of base set, the merit of choosing base set is
that the number N depends only on the degree of kernel and
the number of explanatory variables, and does not depend on
the size of data set, M.
In order to apply our model to real data, we need to prepare
several types of modiﬁed kernel model and need to investigate
feasibility conditions for the induced LP problem.
REFERENCES
[1]
M. A. Aizerman, E. M. Braverman, and L. I. Rozonoer, “Theoretical
foundations of the potential function method in pattern recognition
learning,” Automation and Remote Control, Vol. 25, pp. 821837, 1964.
[2]
A. Alim, F. T. Johora, S. Babu, and A. Sultana, “Elementary Operations
on L-R Fuzzy Number,” Advanced in Pure Mathematics, 5, pp. 131-136,
2015.
[3]
M. Amagasa, “Formulation of A Sale Price Prediction Model Based on
Fuzzy Regression Analysis,” Operations Research Proceedings 2011,
Springer Berlin Heidelberg, pp 567-572, 2012.
[4]
M. Amagasa and K. Nagata, “Prediction Model with Interval Data -
Toward Practical Applications-,” Proceedings of Information Processing
and Management of Uncertainty in Knowledge-Based Systems(IPMU
2016), Part II, CCIS611, pp.213-224, 2016.
[5]
N. Aronszajn, “Theory of reproducing kernels,” Transaction of the
American Mathematical Society, 68(3), pp.337-404, 1950.
[6]
F. R. Bach and M. I. Jordan, “Kernel independent component analysis,”
Journal of Machine Learning Research, Vol. 3, pp.1-48, 2002.
[7]
B. E. Boser, I. M. Guyon, V. N. Vapnik, “A training algorithm
for optimal margin classiﬁers,” Proceedings of the ﬁfth annual ACM
workshop on Computational learning theory(COLT‘92), pp. 144-152,
ACM Press, 1992.
[8]
P. D’Urso, “Linear regression analysis for fuzzy/crisp input and
fuzzy/crisp output data,” Computational Statistics & Data Analysis, 42,
pp. 47-72, 2003.
[9]
B. T. Gale, Managing Customer Value, FREE PRESS, 1994.
[10]
P. Guo, and H. Tanaka, “Dual Models for Possibilistic Regression
Analysis,” Computational Statistics & Data Analysis, Vol.51(1), pp.
252-266, 2006.
[11]
Y. Inoue, Y. Uematsu, M. Amagasa, and G. Tomizawa, “The method
of setting forecast selling price model by fuzzy regression analysis,”
Proceedings of Japan industrial Management Association, Autumn
Congress, pp.194-195, 1991.
[12]
T. Melzer, M. Reiter, and H. Bischof, “ Nonlinear feature extraction us-
ing generalized canonical correction analysis, ” Proceedings of Internal
Conference Artiﬁcial Neural Networks(ICANN2001), pp.353-360, 2001.
[13]
S. Mika, G. R¨atsh, J. Weston, B. Sch¨olkopf, and K.-R. M¨uller, “Fisher’s
discriminant analysis with kernels,” Neural Networks for Signal Pro-
cessing,, Vol. 11, pp. 41-48, IEEE, 1999.
[14]
R. Rosipal and L. J. Trejo, “Kernel Partial Least Squares Regression
in Reproducing Kernel Hilbert Space,” Journal of Machine Learning
Research, 2, pp. 97-123, 2001.
[15]
B. Sch¨olkopf, R. Herbrich, and A. J. Smola, “ A Generalized Repre-
senter Theorem,” Proceedings of 14th Annual Conference on Computa-
tional Learning Theory and 5th European Conference on Computational
Learning Theory, pp. 416-426, Springer-Verlag, 2001.
[16]
B. Sch¨olkopf, A. J. Smola, and K.-R. M¨uller, “ Nonlinear Component
Analysis as a Kernel Eigenvalue Problem,” Neural Computation, Vol.
10, No. 5, pp.1299-1319, MIT Press, 2006.
[17]
J. Shawe-Taylor and N. Cristinanini, “Margin distribution and soft
margin, ” Advanced in Large Margin Classiﬁers, pp. 349-358, MIT
Press, 2000.
[18]
H. Tanaka, S. Uejima, and K. Asai, “Linear regression analysis with
fuzzy model,” IEEE Transactions on Systems, Man and Cybernetics,
SMC-12, No.6, pp.903-907, 1982.
[19]
H. Tanaka, “Fuzzy analysis by possibility linear models, ” Fuzzy Sets
and Systems, Vol.24, pp.363-375, 1987.
23
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-566-1
IMMM 2017 : The Seventh International Conference on Advances in Information Mining and Management

