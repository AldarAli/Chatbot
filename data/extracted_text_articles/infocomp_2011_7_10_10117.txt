Min-Sum-Min Message-Passing for Quadratic Optimization
Guoqiang Zhang and Richard Heusdens
Department of Mediamatics
Delft University of Technology
Delft, the Netherlands
Email: {g.zhang-1,r.heusdens}@tudelft.nl
Abstract—We study the minimization of a quadratic objec-
tive function in a distributed fashion. It is known that the
min-sum algorithm can be applied to solve the minimization
problem if the algorithm converges. We propose a min-sum-
min message-passing algorithm which includes the min-sum
algorithm as a special case. As the name suggests, the new
algorithm involves two minimizations in each iteration as com-
pared to the min-sum algorithm which has one minimization.
The algorithm is derived based on a new closed-loop quadratic
optimization problem which has the same optimal solution as
the original one. Experiments demonstrate that our algorithm
improves the convergence speed of the min-sum algorithm by
properly selecting a parameter in the algorithm. Furthermore,
we ﬁnd empirically that in some situations where the min-
sum algorithm fails, our algorithm still converges to the right
solution. Experiments show that if our algorithm converges, our
algorithm outperform a reference method with fast convergence
speed.
Keywords-Distributed optimization, Gaussian belief propa-
gation, message-passing algorithms
I. INTRODUCTION
In this paper we consider minimizing a quadratic opti-
mization problem, namely
min
x∈Rn f(x) = 1
2xT Jx − hT x,
(1)
where J ∈ Rn×n is a positive deﬁnite matrix and h ∈ Rn.
It is known that the optimal solution x∗ satisﬁes a linear
equation
Jx∗ = h.
We suppose that the matrix J is sparse and the dimension-
ality n is large. In this situation, the direct computation
(without using the sparse structure of J) of the optimal solu-
tion may be expensive and unscalable. One natural question
is how to exploit the sparse geometry to efﬁciently obtain
the optimal solution. To achieve this goal, the quadratic
function f(x) can be associated with an undirected graph
G = (V, E). That is, the graph has a node for each
variable xi and an edge between i and j for each nonzero
Jij term. The algorithms that exploit the sparse geometry
exchange information between nodes in the graph until
reaching consensus.
Existing algorithms are either applicable to a speciﬁc
class of J or are computationally expensive (which we will
explain in detail in next section). Our work will focus on
designing an efﬁcient distributed message-passing algorithm
for a general positive deﬁnite matrix J.
The reminder of the paper is organized as follows. In
Section II, we provide a literature review. Section III brieﬂy
describes the GaBP algorithm, or equivalently, the min-
sum algorithm for quadratic optimization. In Section IV, we
present our new min-sum-min message-passing algorithm.
Section V provides the experimental results. Finally, we
draw conclusions in Section VI.
II. RELATED WORK
The quadratic optimization problem is closely related to
the Gaussian belief propagation (GaBP) for inference in
graphic models. This is due to the fact that f(x) can be
associated with a Gaussian distribution p(x) via
p(x) ∝ exp(−(1/2)xT Jx + hT x).
The mean value of p(x) is the same as the optimal solution
of the quadratic optimization problem. The GaBP algorithm
is a min-sum message-passing algorithm for estimating the
mean of the Gaussian random vector. Due to its simplicity,
the GaBP algorithm has found many applications in practice,
such as signal processing [1][2], consensus propagation
in sensor networks [3], multiuser detection [4] and Turbo
decoding with Gaussian densities [5]. It is known that if the
GaBP algorithm converges, it converges to the mean value
of p(x) (see [6],[7]). Unfortunately, the GaBP algorithm
does not always converge, which limits its application. Two
general sufﬁcient conditions for the convergence of the
GaBP algorithm are established: diagonal dominance of J
[8] and walk-summability of J [9][6]. For completeness, we
give their deﬁnitions in the following.
Deﬁnition [8],[10] A matrix J ∈ Rn×n, with all ones on
its diagonal, is walk-summable if the spectral radius of the
matrix ¯J − I, where ¯J = [|Jij|]n
i,j=1 , is less than one.
Deﬁnition [10] A matrix J ∈ Rn×n is diagonally dominant
if |Jii| > P
j̸=i |Jij| for all i.
Recently research attention has moved to overcome the
convergence-failure of the GaBP algorithm for a general
matrix J. In [10], Ruozzi and Tatikonda proposed a variant
137
INFOCOMP 2011 : The First International Conference on Advanced Communications and Computation
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-161-8

of the GaBP algorithm by changing the edge structure of
the graph. In their algorithm, two parameters have been
introduced to ensuring the correct convergence. However,
it is not clear how to choose the two parameters. In [11],
Johnson et al. proposed a double-loop algorithm with the
GaBP algorithm as a subroutine (corresponds to the inner
loop). Each time the GaBP algorithm is called a better
estimate of the mean vector is obtained. The double-loop
algorithm guarantees the convergence at the cost of high
computational complexity. The basic idea of the double-loop
algorithm is to precondition the matrix J such that the new
matrix is diagonal dominant, allowing the use of the GaBP
algorithm.
In this paper we generalize the min-sum algorithm by
proposing a new min-sum-min algorithm. We ﬁrst construct
a new closed-loop quadratic optimization problem which has
the same optimal solution as that of the original problem.
Instead of solving the original problem, we solve the new
problem by developing the min-sum-min algorithm. The ba-
sic idea behind the algorithm is to transform the closed-loop
optimization problem into n scalar closed-loop optimization
problems, one for each node. Note that our algorithm has
two minimizations for each iteration as compared to the min-
sum algorithm which has one minimization. The additional
minimization in our algorithm serves to break the loop at
each node.
We test our algorithm for two scenarios. When the min-
sum algorithm converges, we ﬁnd that our algorithm can be
more efﬁcient than the min-algorithm by properly choosing
a parameter in the algorithm. When the min-sum algorithm
fails, we ﬁnd that our algorithm still converges in some
situations. Experiments show that our algorithm signiﬁcantly
improves the convergence speed of the double-loop algo-
rithm [11].
III. MIN-SUM MESSAGE-PASSING
In this section we brieﬂy review the min-sum message-
passing algorithm for quadratic optimization (which is actu-
ally the GaBP algorithm). This algorithm is the basis for de-
veloping our new min-sum-min message-passing algorithm.
Before considering the quadratic optimization problem
(1), we ﬁrst study a more general objective function f(x),
which takes the form
f(x) =
X
j∈V
fj(xj) +
X
(i,j)∈E
fij(xi, xj).
(2)
In the literature, fj and fij are often called self-potentials
and edge potentials, respectively. fij captures the correlation
between nodes i and j. Due to the pairwise correlations,
the jth component x∗
j of x∗ that minimizes f(x) requires a
global knowledge of f(x). The min-sum algorithm describes
the form of the messages exchanged between the nodes.
Speciﬁcally, the message sent from node i to node j at
iteration t + 1 takes the form
m(t+1)
i→j (xj) = κ + min
xi
 
fi(xi)
+fij(xi, xj) +
X
u∈N(i)\j
m(t)
u→i(xi)
!
, (3)
where N(i) denotes the set of neighboring nodes of i, i.e.,
N(i) = {j|(i, j) ∈ E}. The parameter κ in (3) represents an
arbitrary offset term that may be different from message to
message. (3) implies that there are two messages associated
with each edge (i, j) ∈ E, one for each direction on the
edge. To facilitate the performance analysis, we introduce a
directed graph ⃗G = (V, ⃗E) for G. For every edge (i, j) ∈ E,
there are two elements (i → j), (j → i) in ⃗E.
At each time t, each vertex j forms a local belief function
f (t)
j (xj) by combining messages received from all neighbors
f (t)
j (xj) = fj(xj) +
X
u∈N(j)
m(t)
u→j(xj).
(4)
An estimate of the jth component x∗
j is then given by
ˆx(t)
j
= arg min
xj f (t)
j (xj).
(5)
The min-sum algorithm is successful if ˆx(∞)
j
is equal to x∗
j
for all j ∈ V .
When the function f(x) in (2) is speciﬁed to the quadratic
function as in (1), the min-sum algorithm becomes the GaBP
algorithm. In this situation, the self-potentials and edge
potentials are given by
fj(xj) = (1/2)Jjjx2
j − hjxj
fij(xi, xj) = Jijxixj.
Without loss of generality, we may assume that J is normal-
ized to have unit diagonal, i.e., Jjj = 1. Since the functions
fj and fij are in quadratic form, the belief function f (t)
j
also takes a quadratic form [7]:
f (t)
j (xj)
=
1
2

1 −
X
i∈N(j)
J2
ijγ(t)
ij

 x2
j
−

hj −
X
i∈N(j)
z(t)
ij

 xj,
(6)
where γ(t)
ij and z(t)
ij are updated as
γ(t+1)
ij
=
1
1 − P
u∈N(i)\j J2
uiγ(t)
ui
,
(7)
z(t+1)
ij
=
Jij
1 − P
u∈N(i)\j J2
uiγ(t)
ui

hi −
X
u∈N(i)\j
z(t)
ui

 . (8)
138
INFOCOMP 2011 : The First International Conference on Advanced Communications and Computation
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-161-8

The
update
of
γ(t+1)
ij
and
z(t+1)
ij
are
valid
if
P
u∈N(i)\j J2
uiγ(t)
ui < 1 for all i, j and t. These inequalities
are always satisﬁed under the walk-summability condition
or the diagonal dominant condition [8],[10]. Given the form
of the belief function f (t)
j (xj) in (6), the estimate of x∗
j is
obtained by applying (5)
ˆx(t)
j
=
1
1 − P
i∈N J2
ijγ(t)
ij

hj −
X
i∈N(j)
z(t)
ij

 .
(9)
The message-updating equations (6)-(9) are described
above for comparison with our algorithm in Section IV.
We will explain how our min-sum-min algorithm is derived
based on the min-sum messages (3)-(5).
IV. MIN-SUM-MIN MESSAGE-PASSING
In this section we ﬁrst construct a new closed-loop
quadratic minimization problem. The new problem has the
same optimal solution as that of the original problem.
We then propose a so-called min-sum-min message-passing
algorithm for the new problem. Finally, we provide explicit
message-updating expressions for solving the new problem.
A. A New Cost Function
Based on (1), we deﬁne a new quadratic minimization
problem:
x∗ = arg min
x
˜f(x, x∗),
(10)
where
˜f(x, x∗) = 1
2xT (sI+(1−s)J)x−[(1−s)h+sx∗]T x, (11)
where s is a scalar parameter and I is the identity matrix.
Different from (1), the optimal solution x∗ appears on both
sides of (10). Thus, (10) is in fact a closed-loop optimization
problem. It is obvious that the min-sum algorithm cannot be
directly applied here since x∗ is not known. We explain in
the following how ˜f(x, x∗) is constructed as in (11).
Before providing the motivation for ˜f(x, x∗), we ﬁrst
show that the optimal solution of (10) is the same as
that of the original minimization problem. We let ˜Js =
sI + (1 − s)J. Same as J, the new matrix ˜Js also has unit
diagonal. In order that the new optimization problem is well
deﬁned, we choose s such that ˜Js is positive deﬁnite. It
should be noted that s can be negative depending on J. To
solve (10), we ﬁrst ﬁx x∗ in ˜f(x, x∗). We then set the ﬁrst
derivative of ˜f(x, x∗) w.r.t. x to be 0. By doing so, we have
[sI + (1 − s)J] x∗
=
(1 − s)h + sx∗,
Jx∗
=
h.
Thus instead of solving (1), we can solve the new optimiza-
tion problem.
Note that the introduction of sx∗ in constructing ˜f(x, x∗)
is the key point in designing a new message-passing algo-
rithm. Due to the simple form of sx∗, the self-potentials and
edge potentials of ˜f(x, x∗) also take a simple form:
˜fj(xj, x∗
j)
=
(1/2)x2
j − [(1 − s)hj + sx∗
j]xj, (12)
˜fij(xi, xj)
=
(1 − s)Jijxixj.
(13)
We point out that the self-potential ˜fj(xj, x∗
j) has only
x∗
j involved instead of the whole vector x∗. This property
of ˜fj(xj, x∗
j) makes it possible for node j to deal with
x∗
j locally. In fact, one can introduce Γx∗, where Γ is a
diagonal matrix, in constructing a new closed-loop function
(the matrix ˜Js should be changed accordingly). For the same
reason, one can also design a massage-passing algorithm. In
this paper we focus on sx∗ for simplicity.
We point out that the diagonal-loading on J to obtain ˜Js
is inspired by the work in [11]. The main difference between
our work and [11] is that we propose a new message-passing
algorithm based on (10). On the other hand, the authors in
[11] took the min-sum algorithm as a subroutine to solve
(1) directly.
B. Algorithm design
In this subsection we present the min-sum-min algorithm
for solving the closed-loop optimization problem (10). We
show that one of the two minimizations of the algorithm
unlocks the loopy effect of x∗ in (10).
In order to tackle the unknown parameters x∗
j in self-
potentials ˜fj(xj, x∗
j), we ﬁrst revisit the min-sum algorithm
as described by (3)-(5). Note that after each iteration of
message-passing, an estimate ˆx(t)
j
of x∗
j can be obtained
from the local belief function f (t)
j (xj). In other words, a
new estimate of x∗
j is always accessible to node j after each
iteration. Inspired by this property of the min-sum algorithm,
we propose to compute an estimate of x∗
j in (12) at each
iteration in designing our new algorithm. We then take the
estimate of x∗
j for message-updating in the next iteration.
In principle, if the estimate of x∗
j becomes more and
more accurate as the information diffuses through message-
passing, the algorithm converges to the right solution.
Based on the above analysis, we propose new message-
updating expressions as
ˆx(t)
i
= arg min
xi

 ˜fi(xi, ˆx(t)
i ) +
X
u∈N(i)
˜m(t)
u→i(xi)

 ,(14)
ˇx(t)
i
= gi

ˆx(t)
i , ˆx(t)
u , u ∈ N(i)

,
(15)
˜m(t+1)
i→j (xj) = κ + min
xi
 
˜fi(xi, ˇx(t)
i )
+ ˜fij(xi, xj) +
X
u∈N(i)\j
˜m(t)
u→i(xi)
!
. (16)
Note that there are two minimization operations in (14)-(16)
for each iteration, as compared to (3) which has only one
minimization. The name min-sum-min for our new algorithm
139
INFOCOMP 2011 : The First International Conference on Advanced Communications and Computation
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-161-8

then arises naturally. The ﬁrst minimization in (14) comes
from (5) and (12). This minimization plays an important
role in breaking the loop in (10). The second minimization
in (16) comes from the min-sum algorithm. The function gi
in (15) is utilized to reﬁne the estimate using the outputs of
the ﬁrst minimization. (14)-(15) together provide an estimate
of x∗
i for node i at each iteration.
Note that (14) is again a closed-loop minimization with
respect to ˆx(t)
i . Thus we successfully transform the global
closed-loop optimization problem into n local closed-loop
optimization problems, one for each node. As all the mes-
sages are in quadratic form, it is not difﬁcult to compute
ˆx(t)
i
after each iteration. Once {gi|i ∈ V } is speciﬁed, we
effectively provide a min-sum-min algorithm to solve (10).
We can also interpret (14)-(16) from another viewpoint.
Note that (14)-(15) combines information from neighboring
nodes in computing an estimate of the optimal solution.
Thus (14)–(15) can be viewed as an information-fusion
step. On the other hand, the second minimization (16)
carries information from node i to a neighboring node j.
Correspondingly, (16) can be viewed as an information-
diffusion step. The two steps are implemented in order until
reaching consensus at each individual node. That is, the
estimate ˇx(t)
i
of the optimal component x∗
i is stable over
time for all i.
Remark 1: The min-sum-min algorithm is a natural exten-
sion of the min-sum algorithm. To see this, we let s approach
to 0, it is immediate that ˜mij(xj) → mij(xj). Since our
algorithm has a free parameter s to choose, we can improve
the performance of the algorithm by properly adjusting the
parameter.
Remark 2: Note that the min-sum-min algorithm is not lim-
ited to the quadratic minimization problem. In fact, as long
as an original optimization problem can be reformulated into
a proper closed-loop optimization problem, the min-sum-
min algorithm can be applied in correspondence.
C. Explicit message-updating expressions
In this subsection we provide explicit message-updating
expressions for solving the closed-loop optimization prob-
lem. We study the three updating expressions (14)-(16) one
by one for the quadratic form of the potentials (12)-(13).
We ﬁrst consider the minimization (14). We suppose that
the message ˜m(t)
u→i(xi) at iteration t takes the form
˜m(t)
u→i(xi) = −1
2(1 − s)2J2
ui˜γ(t)
ui x2
i + ˜z(t)
ui xi,
(17)
where ˜γ(t)
ui and ˜z(t)
ui are the associated parameters charac-
terizing the quadratic form. By plugging (17) into (14), we
obtain
ˆx(t)
i
=
arg min
xi
 
1
2
h
1 −
X
u∈N(i)
(1 − s)2J2
ui˜γ(t)
ui
i
x2
i
−
h
(1 − s)hi + sˆx(t)
i
−
X
u∈N(i)
˜z(t)
ui
i
xi
!
.
(18)
The optimal solution ˆx(t)
i
can be easily computed from (18),
expressed as
ˆx(t)
i
=
(1 − s)hi − P
u∈N(i) ˜z(t)
ui
1 − s − P
u∈N(i)(1 − s)2J2
ui˜γ(t)
ui
.
(19)
Given the expression for ˆx(t)
i , we then specify the function
set {gi|i ∈ V } in (15). To achieve this goal, we construct
an equality
x∗ = 1
2

h + x∗ − (J − I)x∗
,
(20)
where x∗ is the optimal solution to the minimization prob-
lem. Based on (20), we then let gi be
ˇx(t)
i
= 1
2

hi + ˆx(t)
i
−
X
u∈N(i)
Jiuˆx(t)
u

∀i ∈ V.
(21)
The new estimate ˇx(t)
i
is obtained by combining information
from neighboring nodes and the node itself. The update
expression (21) is just one instance of gi. In principle, there
are many ways to construct the function gi by building new
equalities in terms of x∗.
Upon obtaining the expression for ˇx(t)
i , we study the
second minimization (16). Again by plugging (17) into (16),
we obtain
˜m(t)
i→j(xj)
= κ + min
xi
 
1
2
h
1 −
X
u∈N(i)\j
(1 − s)2J2
ui˜γ(t+1)
ui
i
x2
i −
h
sˇx(t)
i
+(1 − s)hi − (1 − s)Jijxj −
X
u∈N(i)\j
˜z(t)
ui
i
xi
!
,
where ˇx(t)
i
is given by (21). We then simplify ˜m(t)
i→j(xj) by
solving the minimization. The resulting expression takes the
from:
˜m(t+1)
i→j (xj) = −1
2(1 − s)2J2
ij˜γ(t+1)
ij
x2
j + ˜z(t+1)
ij
xj + κ′,
where
˜γ(t+1)
ij
=
1
1 − P
u∈N(i)\j(1 − s)2J2
ui˜γ(t)
ui
,
(22)
140
INFOCOMP 2011 : The First International Conference on Advanced Communications and Computation
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-161-8

and
˜z(t+1)
ij
=
(1 − s)Jij

(1 − s)hi + sˇx(t)
i
−P
u∈N(i)\j ˜z(t)
ui

1 − P
u∈N(i)\j(1 − s)2J2
ui˜γ(t)
ui
, (23)
and κ′ is a new constant. ˜m(t+1)
i→j (xj) again takes a quadratic
form, which is consistent with (17).
One observes that ˜γ(t+1)
ij
and γ(t+1)
ij
essentially take the
same message-passing form. The only difference between
them is that ˜γ(t+1)
ij
is derived from ˜Js while γ(t+1)
ij
is derived
from J. Thus as long as s is chosen such that ˜Js is diagonal
dominant or walk-summable, ˜γ(t+1)
ij
always converges.
The parameter ˜z(t+1)
ij
has an additional term sˇx(t)
i
com-
pared to z(t+1)
ij
. This additional term sˆx(t)
i
is an estimate
of sx∗
i in the self-potential (12). If z(t)
ij converges, the min-
sum-min algorithm then converges to the right solution.
Stage
Operation
1
Initialize
Choose a value s;
Set ˜γij = 0 and ˜zij = 0, ∀(i → j) ∈ ⃗E
2
Iterate
For all (i → j) ∈ ⃗E
Update ˆxi using (19)
Update ˇxi using (21)
Update ˜γij using (22)
Update ˜zij using (23)
End
3
Check
If {ˇxi}, { ˜
γij} and {˜zij} become
stable, go to 4; else, return to 2.
4
Output
Return ˇxi, ∀i.
Table I
MIN-SUM-MIN MESSAGE-PASSING FOR COMPUTING
x∗ = arg minx 1
2xT Jx − hT x.
Based on the above analysis, we brieﬂy summarize the
min-sum-min algorithm for the quadratic minimization (1)
in Table I. In the algorithm, we choose the parameter s
such that ˜Js is diagonal dominant. This guarantees that ˜γij
converge for all (i → j) ∈ ⃗E.
V. EXPERIMENTAL RESULTS
In the experiment, we test the convergence speed of the
min-sum-min algorithm. We study two scenarios: the one
where the min-sum algorithm converges and the other one
where the min-sum algorithm fails.
We considered two graphs for constructing J as shown
in Fig. 1. Graph (a) is a 4-cycle with a chord. Graph (b) is
a 5-cycle. For each graph, the matrix J is constructed with
its diagonal elements being 1 and its off-diagonal elements
being the edge weights as described in the graph. The h
vector in (1) for the two graphs are h = [ 1
2
1
2 ]T
and h = [ 1
2
1
2
1 ]T , respectively.
1
2
4
3
r
r
r
r
-r
1
2
4
3
-r
-r
5
-r
-r
-r
Graph (a): 4-cycle with a chord
Graph (b): 5-cycle
Figure 1.
The two graphs for constructing J. The edge weights are as
denoted by −r or r in the two graphs.
−0.2
0
0.2
0.4
16
18
20
22
24
26
28
30
32
0
0.2
0.4
0.6
0.8
10
12
14
16
18
20
22
24
26
For Graph (a)
For Graph (b)
iterations
iterations
parameter s
parameter s
Figure 2.
Effect of diagonal loading on the convergence speed. We use the
symbol ”◦” to denote the number of iterations required for different values
of s in the min-sum-min algorithm. For comparison, we use the dash-dot
line to denote the number of iterations required for the min-sum algorithm.
A. Comparison between the min-sum-min and the min-sum
algorithms
In the ﬁrst experiment, we investigate the scenario where
the min-sum algorithm converges. We take the min-sum
algorithm as a reference for performance comparison.
We set r = 0.34 and r = 0.4 in Graph (a) and (b),
respectively. Correspondingly, we obtain two realizations for
J. The spectral radius of ¯J − I, where ¯J = [|Jij|]n
i,j=1, are
0.8709 (for (a)) and 0.8 (for (b)). Thus the J matrices satisfy
the walk-summable condition.
In the implementation, we chose the criterion for ter-
minating the algorithm to be
1
n
Pn
i=1 |ˇx(t)
i
− x∗
i | ≤ 10−5.
We selected the parameter s between -0.2 and 1 for our
algorithm.
The experiment results are as shown in Fig. 2. Surpris-
ingly, we observe that for a range of s values, the min-
sum-min algorithm outperforms the min-sum algorithm in
both cases. The results suggest that there exist more efﬁcient
algorithms than the min-sum algorithm. The min-sum-min
algorithm is one example in improving the convergence
speed. We also tested other values r in Fig 1. The results
are similar to those in Fig. 2.
141
INFOCOMP 2011 : The First International Conference on Advanced Communications and Computation
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-161-8

B. Comparison between the min-sum-min and the double-
loop algorithms
In the second experiment, we investigate the scenario
where the min-sum algorithm fails. We take the double-loop
algorithm [11] as a reference for performance comparison.
In this situation, we set r = 0.45 and r = −0.52 in
Graph (a) and (b), respectively. Correspondingly, the spectral
radius of ¯J − I, are 1.1527 (for (a)) and 1.04 (for (b)). The
J matrices are not walk-summable anymore.
Again we chose the criterion for terminating the algorithm
to be
1
n
Pn
i=1 |ˇx(t)
i
− x∗
i | ≤ 10−5. In implementing the
double-loop algorithm, we had to setup one more criterion
for the inner-loop iteration. We terminated the inner-loop
each time when 1
n
Pn
i=1 |ˆx(t)
i
− ˆx(t−1)
i
| ≤ 10−5.
Fig. 3 and Fig. 4 display the experiment results for
Graph (a) and (b), respectively. It is seen from the ﬁgures
that if our algorithm converges, it converges much faster than
the double-loop algorithm. The performance gain in terms of
the number of iterations range from hundreds to thousands
in the experiment.
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10
1
10
2
10
3
10
4
 
 
Min-sum-min Alg.
Double-loop Alg.
parameter s
iterations
Figure 3. Comparison between the min-sum-min algorithm and the double-
loop algorithm for Graph (a).
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10
1
10
2
10
3
10
4
 
 
Min-sum-min Alg.
parameter s
iterations
Double-loop Alg.
Figure 4. Comparison between the min-sum-min algorithm and the double-
loop algorithm for Graph (b).
VI. CONCLUSION
We have proposed a new min-sum-min message-passing
algorithm which includes the min-sum algorithm as a special
case. The new algorithm has been derived based on a closed-
loop optimization problem which has the same optimal
solution as the original problem. Compared to the min-sum
algorithm, the min-sum-min algorithm has a free parameter s
to choose. This property renders two advantages of the min-
sum-min algorithm over the min-sum algorithm. First, the
min-sum-min algorithm provides faster convergence speed
when the parameter s is chosen properly. Second, in some
situations where the min-sum algorithm fails, the min-sum-
min algorithm still converges.
One open issue is how to choose the parameter s to make
our algorithm most efﬁcient. This issue is quite relevant to
engineering in practice.
REFERENCES
[1] D. Bickson, O. Shental, and D. Dolev, “Distributed Kalman
Filter via Gaussian Belief Propagation,” in the 46th Allerton
Conf. on Communications, Control and Computing, 2008.
[2] H. A. Loeliger, J. Dauwels, J. Hu, S. Korl, L. Ping, and F. R.
Kschischang, “The Factor Graph Approach to Model-Based
Signal Processing,” in Proceedings of the IEEE, vol. 95, 2007,
pp. 1295–1322.
[3] C. C. Moallemi and B. V. Roy, “Consensus Propagation,”
IEEE Trans. Inf. Theory, vol. 52, no. 11, pp. 4753–4766,
2006.
[4] A. Montanari, B. Prabhakar, and D. Tse, “Belief Propagation
Based Multi-User Detection,” in Proc. 43rd Allerton Conf. on
Communications, Control and Computing, 2005.
[5] P. Rusmevichientong and B. B. Roy, “An analysis of Belief
Propagation on the Turbo Decoding Graph with Gaussian
Densities,” IEEE Trans. Inf. Theory, vol. 47, no. 2, pp. 745–
765, 2001.
[6] D. M. Malioutov, J. K. Johnson, and A. S. Willsky, “Walk-
Sums and Belief Propagation in Gaussian Graphical Models,”
J. Mach. Learn. Res., vol. 7, pp. 2031–2064, 2006.
[7] C. C. Moallemi and B. V. Roy, “Convergence of Min-Sum
Message Passing for Quadratic Optimization,” IEEE Trans.
Inf. Theory, vol. 55, no. 5, pp. 2413–2423, 2009.
[8] Y. Weiss and W. T. Freeman, “Correctness of Belief Propa-
gation in Gaussian Graphical Models of Arbitrary Topology,”
Neural Computation, vol. 13, pp. 2173–2200, 2001.
[9] J. K. Johnson, D. M. Malioutov, and A. S. Willsky, “Walk-
sum Interpretation and Analysis of Gaussian Belief Propaga-
tion,” in Advances in Neural Information Processing Systems,
vol. 18, Cambridge, MA: MIT Press, 2006.
[10] N. Ruozzi and S. Tatikonda, “Unconstrained Minimization
of Quadratic Functions via Min-Sum,” in Proceedings of
thee Conference on Information Sciences and systems (CISS),
March 2010.
[11] J. K. Johnson, D. Bickson, and D. Dolev, “Fixing Conver-
gence of Gaussian Belief Propagation,” in the International
Symposium on Information Theory, 2009.
142
INFOCOMP 2011 : The First International Conference on Advanced Communications and Computation
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-161-8

