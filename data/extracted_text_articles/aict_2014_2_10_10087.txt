Compression of Polysomnographic Signals Using the Discrete Cosine Transform and
Deadzone Optimal Quantization
Hugo N. de Oliveira, Arnaldo Gualberto de A. e Silva, Igor C. Diniz, Gustavo B. Sampaio, Leonardo V. Batista
Informatics Department
Federal University of Paraiba (UFPB)
Joao Pessoa, Brazil
Email: {hugoneves, arnaldo.gualberto, ygorcrispim, gustavobrito, leonardo}@ci.ufpb.br
Abstract—Data compression techniques for electrocardiographic
and electroencephalographic exams have been widely reported in
the literature over the last decades; but, there are no papers
offering a unique solution for all biological signals typically
present in polysomnographic records. Aiming to ﬁll this gap,
the present work proposes a method of lossy compression for
polysomnographic signals based on optimal quantization of the
coefﬁcients obtained from the discrete cosine transform. The
potentially grave distortions generated by the information loss
are controlled by a compression parameter that may be conﬁg-
ured to reach the desired Normalized Percent Root-mean-square
Difference generating the optimum quantization vector with a
minimization of the Lagrange parameter. The quantized signal
is sent to a prediction by partial matching compressor, which
works as the entropy coder of this compression strategy. The
method was tested using the signals in the Polysomnographic
database created by the Massachusetts Institute of Technology
and Boston’s Beth Israel Hospital, achieving compression ratios
between 2.16:1 and 67.48:1 with distortion values between 1.0%
and 4.0%.
Keywords–data compression; telemedicine; polysomnographic
signals; lossy compression; discrete cosine transform.
I.
INTRODUCTION
The technological advances in data transmission have
turned the ability to communicate into one of the foundations
of the contemporary society. Access to broadband Internet,
despite recent technological advances, still remains as a service
which is accessible to few people, especially in third world
countries. Likewise, a large amount of hard disk space may
represent a great cost for applications with either personal
or commercial purposes. One way to ease these problems is
to reduce the need for storage and/or transmission of data,
while preserving all or most of the information on the original
message.
The methods used in messages to minimize the disk space
needed for its storage is the process called data compression,
and this special type of data processing is classiﬁed into
lossy and lossless compression techniques. A lossless com-
pression and decompression process of a signal results in a
reconstructed signal with exactly the same information as the
original one. A lossy compression technique may produce a
fairly accurate approximation of the original signal, depending
on the compression techniques and the parameters used in
these techniques.
The biological signals are among the various types of
signals on which lossy compression techniques may be ap-
plied. These signals are often used for either biometrical or
diagnostic purposes, requiring a very low amount of errors
– or even none – in a reconstructed signal decoded after the
application of a lossy compression method. Polysomnographic
(PSG) monitoring has been useful to clarify the physiological
mechanism to produce sleep related signs or attacks, such as
apnea, arrhythmia, hemodynamics changes, and/or myocardial
ischemia [1]. Therefore, this kind of exam cant be performed
during day time and is usually done in specialized clinics.
The disk space needed for the storage of one hour of a PSG
channel may be as large as 2.57 MB, if a proper digitalization
is used in the process. Some PSGs may contain nine data
channels, resulting in a disk space of approximately 23 MB/h.
In eight hours, the average sleep duration of a human being,
this signal requires a space equivalent to 185 MB. This may
not seem much for the storage capacity of modern computers,
but it represents a huge scalability problem when the exams
need to be stored for the rest of the patients life for health
progress evaluation. This large amount of space represents a
problem for the design of embedded homecare polygraphs,
which can perform most of these exams in the patients house.
This problem can be eased with the use of smart lossy data
compression techniques over the PSGs, achieving a good
Compression Ratio (CR).
There are no works describing a solution for a uniﬁed
method for compression of PSGs; so, some modern techniques
for electrocardiographic (ECG) and electroencephalographic
(EEG) compression will be described as follows:
The method proposed by Mukhopadhyay et al. [2] uses
a differentiation technique to detect all R-peaks in the ECG
signal, allowing the algorithm to apply a differential encoding
process to R-peak regions (also called QRS regions). The
QRS slices are passed to an algorithm based on a Lossless
Compression using Character Encoding (LLCCE), since these
parts of the ECG are more important to the signal reconstruc-
tion. The rest of the ECG is passed to a sub sampler, which
reduces the sampling frequency of the signal by one half.
Then non-QRS data are processed by a Lossy Compression
using Character Coding Encoding (LCCE) scheme. The non-
parameterized compressor was tested using the Physikalisch-
Technische Bundesanstalt (PTB) Diagnostic ECG Database
and reported a CR value of 23.10 : 1 with a Percent Root-
mean-square Difference (PRD) value of 7.55%.
Ranjeet et al. [3] uses a Cut And Align (CAB) strategy
26
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-360-5
AICT2014 : The Tenth Advanced International Conference on Telecommunications

to slice the ECG signal and reorganize the blocks in a 2D
array, which is passed to a 2D Discrete Wavelet Transform
(DWT). The remaining coefﬁcients are encoded using Huffman
entropy coding, achieving an average of 65% in compression
efﬁciency with 0.999 correlation score. The tests performed
using this near lossless strategy resulted in an optimum 2D
array size of 180x20 samples in the Massachusetts Institute
of Technology and Boston’s Beth Israel Hospital (MIT-BIH)
Arrhythmia Database [4].
The work described by Lai et al. [5] explores the use of the
Discrete Cosine Transform (DCT) IV – in contrast to the DCT-
II, often used for compression purposes. Initially, the ECG
signal is divided into DCT blocks with 64 samples, then a
differential coding procedure is applied, feeding a Huffman
entropy coder. This non-parameterized strategy achieved an
average CR of 5.25 : 1 with a PRD of 0.19% and a Normalized
Percent Root-mean-square Difference (NPRD) of 2.88% using
the MIT-BIH Arrhythmia Database [4].
The method described by Anas et al. [6], similarly to
the present work, is a DCT-based compressor. It uses the
correlation between the ECG cycles (identiﬁed by QRS com-
plexes) to eliminate the redundancy in the data of the records.
This ECG compressor has a preprocessing step counting with
baseline elimination, an average ﬁlter, a high pass ﬁlter and
a butterworth ﬁlter, preparing the record to the compression
routine. Then, the ECG is passed to a R-peak detector, the ECG
cycles are interpolated to a ﬁxed value M = 512, normalized
to an interval [0, 1], transformed using the DCT, quantized
and encoded. This parameterized method achieved good CRs
for low PRD values, but only the results obtained by the
compression of three records from the MIT-BIH Arrhythmia
Database [4] were published.
Srinivasan et al. [7] propose a multichannel near-lossless
EEG compression algorithm based on image and volumetric
coding. The algorithm arrange multichannel EEG signal in
the form 2D image or 3D volume, then apply either a 2D or
3D DWT to exploit simultaneously both spatial and temporal
correlation in a single procedure. The proposed algorithm
achieved a compression ratio of 6.63 with PRD of 9.21% for
a quantizer step-size equals to ten in one of the datasets used.
The fact that no work in the literature describes a com-
pression technique for all the PSGs is responsible for the
non-existence of a standard distortion measure for the lossy
compression of these signals. However, there is a large amount
of papers describing lossy and lossless compression methods
for electrocardiographic signals. The lossy compressors often
use the PRD as an objective evaluation of the distortion present
in the decoded signal. The PRD is deﬁned as:
PRD =
v
u
u
t
PN−1
n=0 (x [n] − ˜x [n])2
PN−1
n=0 (x [n])2
× 100%
(1)
This measure is very sensitive to the baseline of the original
signal. A second deﬁnition for the PRD, the NPRD, which
overcomes this problem, is described by Batista et al. [8] as:
NPRD =
v
u
u
t
PN−1
n=0 (x [n] − ˜x [n])2
PN−1
n=0 (x [n] − x)2
× 100%
(2)
where x is given for:
x = 1
N
N−1
X
n=0
x [n]
(3)
The NPRD, however, is still not the ideal distortion mea-
sure for biological signals, as it does not consider the different
characteristics present in each record. This criterion only
provides an objective approximation for the amount of errors
in the reconstructed signal.
This paper is organized as follows: Section I presents an
overview about the PSG data volume problem, the method we
propose to solve it and the quality metrics we used. Section
II gives an overview about PSG signals. Section III explains
the basis of DCT-based lossy compressors. Section IV explains
the proposed compression method. Section V shows the results
obtained by the test application. Section VI describes the
conclusions obtained after the analysis of the results and some
possible applications for the proposed method.
II.
POLYSOMNOGRAPHIC SIGNALS
The PSGs may include several types of signals, including
both well-known signals as electrocardiograms and electroen-
cephalograms; and signals with more speciﬁc purposes, as
electrooculograms (EOG), stroke volume (SV) and oxygen
saturation records (SO2). The signals included and discarded
from the exams are determined by the health condition the
physician wants to analyze and the patients health state.
The large diversity of behavior among the PSGs is re-
sponsible for the lack of uniﬁed compression solutions for all
signals described in the biological data compression literature.
While some signals are periodical, as ECG, blood pressure
(BP) and respiration (Resp) signals, other signals are almost
completely chaotic, as electromyographic (EMG), EOG and
EEG records. Some works take advantage of the periods in
ECG records to achieve greater CR values and other works
cover only EEG signals, but none of them was tested in all
PSGs.
The work described by Ichimaru and Moody [1] presents
a standardized physiological PSG database format, including
an amount of 18 signals, with duration ranging from two to
seven hours. The PSGs were digitalized with 250 Hz sampling
frequency and a 12 bits/sample quantization. The so called
MIT-BIH Polysomnographic Database became the standard
test corpus for the PSG processing applications. An image of
the samples of a signal in the MIT-BIH Polysomnographic
Database is shown in Fig. 1.
III.
DCT-BASED COMPRESSORS
Among the several domain transformations applicable to
digital signals, the DCT [9] and the DWT [10] have been
27
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-360-5
AICT2014 : The Tenth Advanced International Conference on Telecommunications

Figure 1.
Full disclosure of the polysomography data. Including ECG, BP, EEG, Resp, SV and SO2 records. Adapted from [1].
widely used in lossy compressors due to their energy com-
paction properties. Fast implementations of both transforms in
both 1 and 2 dimensions have been described in the data com-
pression literature over the last decades. The DCT – the most
popular one – is used in many encoding formats, including
ECG encoders [11][12][13][14]; video encoders [15][16]; still
image encoders [17]; and audio encoders [9]. As described by
Batista et al. [8], there are four steps often used for the creation
of DCT-based encoders to compress a data sequence x:
1) Partition of x in Nb consecutive blocks bi, i
=
0, 1, ..., Nb − 1, each one with Lb samples;
2) DCT computation for each block;
3) Quantization of the DCT coefﬁcients;
4) Lossless encoding of the quantized DCT coefﬁcients.
Increasing the block size increases the CR and the DCT
computing time. Various results show, however, that increasing
the block size above a certain point results in a very modest
CR gain, while the processing time signiﬁcantly increases
[12][18]. The DCT-II is widely used in lossy data compres-
sors and it is the closest unitary transform approximation
for the optimal Karhunen-Love Transform (KLT) [9]. Let
bi[n], n = 0, 1, ..., Lb − 1, represent the Lb values in block bi;
the one-dimensional DCT-II of this block generates a trans-
formed block Bi constituted by a sequence of Lb coefﬁcients
Bi[m], m = 0, 1, ..., Lb − 1, given by:
Bi [m] =

2
Lb
 1
2 cm
PLb−1
n=0

bi [n] cos
h
(2n+1)mπ
2Lb
i
,
m = 0, 1, ..., Lb − 1
(4)
where cm = 1 for 1 ≤ m ≤ Lb − 1 and c0 =
q
The lossless encoding of the quantized DCT coefﬁcients
generally involves run-length encoding, because the quanti-
zation normally generates many null values, followed by an
entropy encoder [12].
The present work describes a method to deﬁne q and t in
a way that minimizes the estimated entropy of the quantized
coefﬁcients for a given distortion, and uses these optimized
vectors as the basis for a PSG compressor. The main goal is
to demonstrate the possibility of attaining good compression
ratios by using a carefully deﬁned quantization strategy.
IV.
DESCRIPTION OF THE PROPOSED METHOD
The measure used for the calculation of the distortion after
the compression was the NPRD due to the baseline variation
among the PSG signals. Some PSGs, such as the respiratory
(Resp) signals, may have a very high baseline value, allowing
the error amount of the common PRD to grow a lot for a
low PRD value. The NPRD uniﬁes the computation of the
distortion to a single measure, without the need of a baseline
elimination preprocess.
To reduce the long-term storage problem created by exams
involving PSGs in small and medium sized clinics, the pur-
posed technique was created. This uniﬁed solution was tested
in the four main PSG signals: ECG, EEG, BP and Resp. The
method works as a parameterized compressor, deﬁning the
optimum q and t vectors for the codiﬁcation of each channel
in the signal. An optimization for the choice of the q and t
parameters was made using the minimization of the Lagrange
multiplier for each DCT coefﬁcient, similarly to the work
presented in Ratnakar [19], which proposed a solution to the
optimum quantization of images. The Lagrangian minimization
allows the compressor to perform an independent optimization
for each coefﬁcient independently. The number of decoding
iterations using exhaustive search methods Nexa – as shown
in (8) – is then optimized to a much lower value Nopt.
Nexa = Qmax
B + Tmax
B
(8)
Lee and Buckley [12] tested the use of a 2D DCT with
block sizes from 4x4 to 64x64, narrowing the tests only to
powers of 2. These tests resulted in a saturation of the coding
gain with block sizes around 32x32 and 64x64 samples. Based
on the experiments presented in [12] and the ones performed
by Batista et al. [8], we used block sizes containing 16, 32
and 64 samples in the tests.
For a given signal, let H(q, t) be the zero-order entropy of
all DCT coefﬁcients quantized by using t and t, and D(q, t) a
measure of the distortion introduced in the PSG signal by the
quantization. The proposed optimization problem can then be
given by the statement: for a given D(q, t), determine q and
t in a way that minimizes H(q, t).
Optimization can be achieved by minimizing the La-
grangian J = H(q, t) + λD(q, t) for a given value of the
Lagrange multiplier λ [19]. The value of λ that leads to the
desired H(q, t) or D(q, t), within a given tolerance, can be
efﬁciently found by using the bisection method [20]. The
Lagrangian minimization allows the compressor to set a maxi-
mum number of decoding operations empirically predeﬁned by
tests of the method on the signals. The number Nopt was set to
the value 17 in the test application, but this number may vary
according to the type of signal, its digitalization parameters
and its statistic distribution.
Empirical tests showed that NPRD values lower than or
equal to 3.0% required Qmax and Tmax values lower than
128, thus, this was the maximum value the elements of the
QT vectors could reach for these distortions. For NPRD values
higher than 3.0%, the maximum values for the QT elements
were set to 256. Therefore, for NPRD values higher than
3.0% using 64 samples DCT blocks, Nexa would assume
the value 2.68x10154. The process described in the next
paragraphs allows reducing the complexity of the problem to
practical levels. It should be noted that the entire records to be
compressed are used to calculate the optimal q and t vectors.
For the optimization procedure, we use the mean square
error as the distortion measure D(q, t). Since the DCT is
an orthonormal transform, D(q, t) can be calculated from
the distortions introduced in the DCT coefﬁcients [19]. This
eliminates the need to apply the inverse DCT to the dequan-
tized coefﬁcients in order to measure the distortion in the
time-domain. Thus, the mean squared error introduced by the
quantization can be calculated as:
D (q, t) =
PNb−1
i=0
PLb−1
n=0

Bi [n] − q [n] ˆBi [n]
2
LbNb
(9)
The mean square error due to the quantization of coefﬁcient
number k of all blocks, which will be called Dk(q[k], t[k]), is
given by:
Dk (q [k] , t [k]) = 1
Nb
Nb−1
X
i=0

Bi [k] − q [k] ˆBi [k]
2
(10)
Thus, we can write (9) as:
D (q, t) =
PLb−1
n=0 Dn (q [n] , t [n])2
LbNb
(11)
Consider now that the coefﬁcient number k of the quantized
blocks assumes value v in nk(v) of the Nb blocks. Then the
entropy Hk(q[k], t[k]) of the coefﬁcient number k measured
over all quantized DCT blocks is given by:
Dk (q [k] , t [k]) = −
X
v
[pk (v) log2 (pk (v))]
(12)
where pk(v) = nk(v)/Nb.
To estimate the entropy of all quantized coefﬁcients we use
the following simpliﬁed model [19]:
H (q, t) = 1
Lb
L−1
X
n=0
[Hn (q [n] , t [n])]
(13)
29
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-360-5
AICT2014 : The Tenth Advanced International Conference on Telecommunications

In the experimental results presented by Ratnakar [19], the
error between the estimated and the real entropy was normally
below 0.02 bits/symbol, which indicates the precision of the
model.
With the possibility to calculate D(q, t) and H(q, t) as the
mean of the distortion and of the entropy of each coefﬁcient,
the minimization of J reduces to the minimization of:
Jn = Hn (q [n] , t [n]) + λDn (q [n] , t [n]) ,
n = 0, 1, ..., Lb − 1
(14)
In other words, the minimization can be done indepen-
dently for each coefﬁcient. With this simpliﬁcation, if Lb = 64
samples and the elements of q are integer values in the range
1 to 256, only 64 ∗ 256 = 214 of the 64256 possible values
of q need to be analyzed in the minimization procedure. This
complexity reduction combined with the use of histograms,
incremental calculations and other techniques [19], allow per-
forming an efﬁcient search for the optimum q and t vectors.
After deﬁning the optimum q and t for a given signal,
the compressor closely follows the steps of general DCT-
based compressors already described. A scheme representing
graphically the compressors steps is shown in Fig. 2.
The dead-zone quantization step of the encoding process
normally generates a large amount of subsequent null fre-
quency samples, mostly in the high frequency AC coefﬁcients.
This characteristic of the quantized signal allows the compres-
sor use an efﬁcient lossless entropy coding technique, such as a
Golomb coding [21] or a Huffman coding [22]. The proposed
method does not aim to achieve a hardware implementation
compression model; so, for validation purposes, we used a
more efﬁcient lossless coding algorithm, the Prediction by
Partial Matching (PPM) [23]. The optimal entropy coding of
the DC coefﬁcients – which tend to assume higher values than
the AC coefﬁcients – was also decisive to the choosing of the
PPM. The PPM creates a generic statistic distribution model in
the coding process, so the high values in the DC coefﬁcients
are not a problem for the optimal coding of the signal.
Figure 2.
Scheme showing the processing steps used by the proposed
compressor.
The decompressor, as in many other lossy codecs, is a
simple subset of the compressor. It is composed by three
simple steps, recreating an approximation to the original signal
by applying the inverse encoding operations in an inverse
order. The ﬁrst decompression stage is to run a decoding
operation, retrieving the domain frequency quantized signal
ˆxf from the channel it was stored by the compressor. Then a
dequantization operation is applied, creating the approximation
to the signal still composed by frequency domain coefﬁcients
˜xf. At last, an inverse DCT transform is run over the DCT
blocks in the ˜xf signal, generating the approximated time
domain original signal ˜x. Fig. 3 presents an overview of the
decompression scheme.
V.
RESULTS AND DISCUSSION
Table I and Table II show, respectively, the CR results from
compression– using NPRD values among 1.0% and 4.0%, and
with DCT block sizes of 16, 32 and 64 samples – of ECG
and EEG signals, which showed the best visual reconstruction
qualities. Table III and Table IV show the ﬁndings for BP
and Resp signals, respectively, which obtained good results
with certain combinations of parameters, although they have
tolerated NPRD values lower than the other signals to approx-
imately the same level of visual distortion. The thresholding
effect of these signals proved to be stronger than the other
PSGs, explaining the high CR values obtained, since most
of the information to be encoded by the PPM in the entropy
coding stage is concentrated on the DC levels of the blocks
generated by the DCT signals.
In more chaotic signals, which also have a larger amount of
noise, such as ECGs, the best results of optimum compression
were obtained with DCT blocks of smaller size, as seen in Fig.
4(a). This result is reversed in the case of more linear and less
noisy PSGs, such as BP and Resp signals, which is shown in
Fig. 4(b).
Figure 3.
Scheme showing the processing steps used by the proposed
decompressor.
30
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-360-5
AICT2014 : The Tenth Advanced International Conference on Telecommunications

TABLE I.
CR RESULTS FOR NPRD VALUES BETWEEN 1.0% AND 4.0% AND DCT BLOCK SIZES OF 16, 32 AND 64 SAMPLES FOR ECG SIGNALS.
DCT block size
NPRD
1.0%
1.5%
2.0%
2.5%
3.0%
3.5%
4.0%
16
2.52401
3.05967
3.49161
4.0208
4.8165
5.22089
5.98384
32
2.43439
2.8334
3.23771
3.84271
4.38118
4.83237
5.22027
64
2.26796
2.64573
3.02288
3.43338
3.84632
4.09859
4.72272
TABLE II.
CR RESULTS FOR NPRD VALUES BETWEEN 1.0% AND 4.0% AND DCT BLOCK SIZES OF 16, 32 AND 64 SAMPLES FOR EEG SIGNALS.
DCT block size
NPRD
1.0%
1.5%
2.0%
2.5%
3.0%
3.5%
4.0%
16
2.16316
2.49929
2.78552
3.05686
3.34842
3.6387
3.9742
32
2.18337
2.53933
2.82184
3.13717
3.43413
3.79547
4.18758
64
2.21229
2.55054
2.8828
3.09125
3.53912
3.75921
4.07101
TABLE III.
CR RESULTS FOR NPRD VALUES BETWEEN 1.0% AND 4.0% AND DCT BLOCK SIZES OF 16, 32 AND 64 SAMPLES FOR BP SIGNALS.
DCT block size
NPRD
1.0%
1.5%
2.0%
2.5%
3.0%
3.5%
4.0%
16
7.05813
9.83612
12.5254
15.0561
16.9096
18.5575
20.813
32
7.6318
11.0152
13.7244
16.2092
18.397
20.6467
22.6501
64
8.31705
11.9357
14.9918
17.6552
20.0764
22.2745
24.4644
TABLE IV.
CR RESULTS FOR NPRD VALUES BETWEEN 1.0% AND 4.0% AND DCT BLOCK SIZES OF 16, 32 AND 64 SAMPLES FOR RESP SIGNALS.
DCT block size
NPRD
1.0%
1.5%
2.0%
2.5%
3.0%
3.5%
4.0%
16
14.9775
22.3109
28.533
36.1156
42.9171
51.4388
57.9435
32
16.4825
24.6164
32.0925
41.7645
47.9586
57.3724
65.4241
64
18.2037
26.8057
35.7891
42.893
52.0199
61.0977
67.4799
(a)
(b)
Figure 4.
NPRD x CR graphic showing the evolution of the CRs for different block sizes in (a) ECG signals. (b) Resp signals.
As seen in Fig. 5, the reconstruction of the ECG signals
achieved very good results, even for higher NPRD values, as
4.0%. This means that this value can be increased even more
without grave reconstruction errors. EEG signals, although
much more chaotic, had a reconstruction quality close to the
ECGs for the same values of NPRD, as seen in Fig. 6. As
expected, these signals obtained the lowest CRs among the
PSGs due to the large amount of information present in their
samples.
The reconstruction of BP signals, exempliﬁed by Fig. 7,
showed a strong thresholding effect for signal reconstruction
with NPRD values higher than 3.0%. The compression of these
signals resulted in high CRs, even for tests with lower NPRD
values, allowing compressions with milder distortions to be
applied and still result in acceptable CR values. Some kinds
of medical exams require only the basic shape of the signal
to be stored, so, depending on the purpose of the exam, the
thresholding effect can be accepted for BP signals, achieving
higher CRs.
In Resp signals (see Fig. 8) like the BPs, the effect of
thresholding was strong enough in tests with higher NPRD
values, but the CR obtained with the compression of these
PSGs is the best among all PSGs thanks to its continuity and
to a small amount of noise present in the samples. This allows
the application of lighter quantization, still achieving good CR
values.
Compression using the Lagrangian minimization to deter-
mine the optimal parameters did not behave well with signals
31
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-360-5
AICT2014 : The Tenth Advanced International Conference on Telecommunications

(a)
(b)
(c)
(d)
(e)
Figure 5.
Slice of the ECG channel of the ﬁle slp37.dat of the MIT-BIH
Polysomnographic Database (a) original. (b) reconstructed with a NPRD of
1.0%. (c) reconstructed with a NPRD of 2.0%. (d) reconstructed with a NPRD
of 3.0%. (e) reconstructed with a NPRD of 4.0%.
(a)
(b)
(c)
(d)
(e)
Figure 6.
Slice of the EEG channel of the ﬁle slp01a.dat of the MIT-BIH
Polysomnographic Database (a) original. (b) reconstructed with a NPRD of
1.0%. (c) reconstructed with a NPRD of 2.0%. (d) reconstructed with a NPRD
of 3.0%. (e) reconstructed with a NPRD of 4.0%.
(a)
(b)
(c)
(d)
(e)
Figure 7.
Slice of the BP channel of the ﬁle slp16.dat of the MIT-BIH
Polysomnographic Database (a) original. (b) reconstructed with a NPRD of
1.0%. (c) reconstructed with a NPRD of 2.0%. (d) reconstructed with a NPRD
of 3.0%. (e) reconstructed with a NPRD of 4.0%.
(a)
(b)
(c)
(d)
(e)
Figure 8.
Slice of the Resp channel of the ﬁle slp16.dat of the MIT-BIH
Polysomnographic Database (a) original. (b) reconstructed with a NPRD of
1.0%. (c) reconstructed with a NPRD of 2.0%. (d) reconstructed with a NPRD
of 3.0%. (e) reconstructed with a NPRD of 4.0%.
32
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-360-5
AICT2014 : The Tenth Advanced International Conference on Telecommunications

with a highly uneven statistical distribution. The large number
of samples with values outside the normal range interferes with
the calculation of the NPRD, since this measure is inversely
proportional to the standard deviation, which is highly affected
by values outside a certain range near the baseline of the
signal. A high standard deviation allows a higher value for
the numerator of the equation – the Root Mean Square Error
(RMSE) – to achieve the same NPRD, which implies a very
large amount of visual errors.
Although in most signals the DCT blocks with 64 samples
obtained the best CR, these blocks also feature a worse visual
reconstruction quality, if compared to PSGs with the same
NPRD values compressed using blocks with 16 or 32 samples.
The CR evolution graphics for different NPRD values in ECG,
EEG, BP and Resp records are seen in Fig. 9.
(a)
(b)
(c)
Figure 9.
CR evolution for ECG, EEG, BP and Resp signals according to
different NPRD values in DCT blocks of (a) 16 samples. (b) 32 samples. (c)
64 samples.
All PSGs showed basically the same CR evolution in
different blocks sizes. In some signals it is noticeable a greater
change in the CR values for higher NPRDs, what may be
attributed to the amount of redundant information, the amount
of noise present in the original signal and the amplitude of the
samples.
The optimal quantization tends to eliminate signal noise,
leaving its baseline more visible and removing the temporal
redundancy present in their samples. Some signals – as Resp
and BPs – suffer from a faster saturation in the visual quality
because of their higher standard deviation.
Depending on the amount of noise that can be accepted
in the ECG and EEG signals, an increase of desired NPRD
passed to the compressor presents a good CR performance
at the cost of low noise, reaching 5.98 : 1 and 4.18 : 1,
respectively, for NPRD values of 4.0%. The CR values of
ECGs came close to the compression obtained by [8], if a
visual analysis of the reconstructed signals is performed. A
more detailed comparison with studies involving the compres-
sion of ECGs and EEGs is hampered by the use of databases
where the digitalization process was done differently than [1],
hindering the comparison using objective distortion measures.
The frequent use of the PRD – and not the NPRD – for the
calculation of the errors of the reconstructed signal is also a
factor that complicates a comparison with other papers.
Fig. 10 shows the effect of sensor defects in a SO2 signal.
These PSG channels – along with the EMGs, EOGs and SV –
were not considered by the tests because of the large amount
of errors present in their capture processes and the low number
of signals containing channels of these records in [1].
(a)
(b)
(c)
(d)
Figure 10.
Slice of the SO2 channel of the ﬁle slp67x.dat of the MIT-BIH
Polysomnographic Database affected by errors (a) original. (b) reconstructed
with a NPRD of 1.0%. (c) reconstructed with a NPRD of 2.0%. (d) recon-
structed with a NPRD of 4.0%.
33
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-360-5
AICT2014 : The Tenth Advanced International Conference on Telecommunications

The runtime of the compressor varied according to the
length of the signal, since the database has no ﬁxed length
for the signals. The larger signals considered for this papers
test routines – with six hours and four data channels – were
compressed in less than 30 minutes. This result enables the
adoption of the proposed method for practical cases, since the
compression method took less than a sixth of the length of the
signals to compress the data. Since most polysomnographic
exams are performed during night time, clinics can use a
fraction of the daytime to compress the PSG records captured
during the previous night.
Embedded mobile systems for PSG exams with less pro-
cessing power than conventional computers may divide the
captured recordings into slices, allowing the execution of the
record to run at the same time as the compress algorithm for
the previous data section. This may result in cheaper homecare
PSG capture devices, bringing more comfort to the patients.
VI.
CONCLUSION
Technological advances in the processing and storage ca-
pacities of personal computers and the price reduction of the
biological signal sensors allowed the popularization of medical
exams using polysomnographic signals. There was an increase
in the data volume generated by these types of exams, although
no compression techniques covering the codiﬁcation of all
PSGs have been reported in the data compression literature.
We presented a lossy parameterized compression method
for PSGs, prioritizing the reconstruction quality of the com-
pressed signals. A Lagrangian minimization was used to drasti-
cally reduce the computational complexity for the choice of the
optimal dead-zone quantization vectors. The amount of errors
allowed in the reconstruction of the PSGs may vary according
to their diagnostic purpose, presetting the compressor in order
to tolerate lower or higher objective distortions.
For low NPRD values the compressor achieved different
results, depending on the entropy of the PSG. The best CRs
were reached in EEG signals, which varied between 2.16 : 1
and 4.17 : 1. In ECG signals, on the other hand, CR ranged
between 2.26
:
1 and 5.98
:
1. The BP signals were
compressed with CRs in interval of 7.05 : 1 and 24.06 : 1.
The highest compression ratios were obtained by the method
in Resp signals, with values in the range of 14.97 : 1 and
67.47 : 1. However, low objectives distortion metrics do not
imply in a good visual quality of signals reconstruction.
Empirical tests validated the presented codiﬁcation tech-
nique, which achieved a small runtime in comparison to the
length of the original signals. The simplicity of the method
may be a motivation for the development of both hardware
implemented solutions and desktop applications.
REFERENCES
[1]
Y.
Ichimaru
and
G.
B.
Moody,
“Development
of
the
polysomnographic
database
on
cd-rom,”
Psychiatry
and
Clinical
Neurosciences,
vol.
53,
no.
2,
1999,
pp.
175–7.
[Online].
Available:
http://www.biomedsearch.com/nih/
Development-polysomnographic-database-CD-ROM/10459681.html
[2]
S. Mukhopadhyay, M. Mitra, and S. Mitra, “An ecg data compression
method via rpeak detection and ascii character encoding,” in Computer,
Communication and Electrical Technology (ICCCET), 2011 Interna-
tional Conference on, March 2011, pp. 136–141.
[3]
K. Ranjeet, A. Kumar, and R. Pandey, “An efﬁcient compression system
for ecg signal using qrs periods and cab technique based on 2d dwt
and huffman coding,” in Control, Automation, Robotics and Embedded
Systems (CARE), 2013 International Conference on, Dec 2013, pp. 1–6.
[4]
G. Moody and R. Mark, “The impact of the mit-bih arrhythmia
database,” Engineering in Medicine and Biology Magazine, IEEE,
vol. 20, no. 3, May 2001, pp. 45–50.
[5]
S.-C. Lai, C.-S. Lan, and S.-F. Lei, “An efﬁcient method of ecg signal
compression by using a dct-iv spectrum,” in Communications, Circuits
and Systems (ICCCAS), 2013 International Conference on, vol. 1, Nov
2013, pp. 46–49.
[6]
E. Anas, M. Hossain, M. Afran, and S. Sayed, “Compression of ecg
signals exploiting correlation between ecg cycles,” in Electrical and
Computer Engineering (ICECE), 2010 International Conference on, Dec
2010, pp. 622–625.
[7]
K. Srinivasan, J. Dauwels, and M. R. Reddy, “Multichannel eeg
compression: Wavelet-based image and volumetric coding approach,”
IEEE Journal of Biomedical and Health Informatics, 2013, pp. 113–
120.
[8]
L. V. Batista, E. U. K. Melcher, and L. C. Carvalho, “Compression
of ecg signals by optimized quantization of discrete cosine transform
coefﬁcients,” Medical Engineering & Physics, vol. 23, no. 2, 2001, pp.
127 – 134. [Online]. Available: http://www.sciencedirect.com/science/
article/pii/S1350453301000303
[9]
K. R. Rao and P. Yip, Discrete Cosine Transform: Algorithms, Ad-
vantages, Applications.
San Diego, CA, USA: Academic Press
Professional, Inc., 1990.
[10]
G. Strang and T. Nguyen, Wavelets and Filter Banks.
Wellesley-
Cambridge Press, 1996. [Online]. Available: http://books.google.com.
br/books?id=Z76N\ Ab5pp8C
[11]
J. v.d. Poel, “Eletrocardiogram signals compression,” Master’s thesis,
NETEB/UFPB, Jo˜ao Pessoa, Brazil, May 1999.
[12]
H.
Lee
and
K.
M.
Buckley,
“Ecg
data
compression
using
cut
and
align
beats
approach
and
2-d
transforms,”
IEEE
Transactions on Biomedical Engineering, vol. 46, no. 5, 1999,
pp. 556–64. [Online]. Available: http://www.biomedsearch.com/nih/
ECG-data-compression-using-cut/10230134.html
[13]
N. Ahmed, P. J. Milne, and S. G. Harris, “Electrocardiographic data
compression via orthogonal transforms,” Biomedical Engineering, IEEE
Transactions on, vol. BME-22, no. 6, Nov 1975, pp. 484–487.
[14]
F. Zou and R. R. Gallagher, “Ecg data compression with wavelet
and discrete cosine transforms,” Biomedical Sciences Instrumentation,
vol. 30, 1994, pp. 57–62. [Online]. Available: http://www.biomedsearch.
com/nih/ECG-data-compression-with-wavelet/7948650.html
[15]
Recommendation H.264, ISO/IEC and ITU-T, 2003.
[16]
Recommendation H.262, ISO/IEC and ITU-T, 1995.
[17]
G. K. Wallace, “The jpeg still picture compression standard,” Commu-
nications of the ACM, 1991, pp. 30–44.
[18]
M. Nelson and J.-L. Gailly, The Data Compression Book (2nd Ed.).
New York, NY, USA: MIS:Press, 1996.
[19]
V. Ratnakar, “Quality-controlled lossy image compression,” Tech. Rep.,
1997.
[20]
A.
Ortega,
Optimization
Techniques
for
Adaptive
Quantization
of
Image
and
Video
Under
Delay
Constraints.
Columbia
University,
1994.
[Online].
Available:
http://books.google.com.br/
books?id=PQR8HwAACAAJ
[21]
S. Golomb, “Run-length encodings,” Information Theory, IEEE Trans-
actions on, vol. 12, no. 3, 1966, pp. 399–401.
[22]
D. A. Huffman, “A method for the construction of minimum-
redundancy
codes,”
Proceedings
of
the
Institute
of
Radio
Engineers,
vol.
40,
no.
9,
September
1952,
pp.
1098–1101.
[Online]. Available: http://compression.graphicon.ru/download/articles/
huff/huffman\ 1952\ minimum-redundancy-codes.pdf
[23]
J. G. Cleary and I. H. Witten, “Data compression using adaptive coding
and partial string matching,” IEEE Transactions on Communications,
vol. 32, no. 4, April 1984, pp. 396–402.
34
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-360-5
AICT2014 : The Tenth Advanced International Conference on Telecommunications

