81
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
A Reliability and Survivability Analysis of US Local Telecommunication Switches 
 
Andrew Snow, Aimee Shyirambere, and Julio Arauz 
School of Information and Telecommunication Systems 
Ohio University 
Athens, Ohio USA 
e-mail: snow@ohio.edu 
Gary Weckman 
Department of Industrial and Systems Engineering 
Ohio University 
Athens, Ohio USA 
e-mail: weckmang@ohio.edu
 
 
Abstract— This paper presents a comprehensive analysis of the 
reliability and survivability of US local telecommunication 
switches over a 14-year study period (from 1996 to 2009). 
Using local switch outage empirical data, the causes, failure 
trends and impacts have been identified, analyzed and 
assessed. A total of 12,860 switch outages were investigated for 
which very significant reliability growth was identified over the 
study period. Outages were also studied temporally, from time 
of day, day of week, and month of year perspectives. 
Additionally, 2,623 of the outages were found to come from 
only 156 unique switches, each of which experienced eight or 
more outages over the study period. The data were separated 
into two categories, for comparison: more frequently failing 
switches and less frequently failing switches. Major findings 
are that scheduled maintenance activities and hardware 
failures 
are 
the 
major 
causes 
of 
outages 
in 
local 
telecommunication switches; there are significant causality 
differences between more frequently and less frequently failing 
switches; and there are considerable differences in switch 
characteristics between the more and less frequently failing 
switches. Additionally, the manufacturers of the more 
frequently out switches are identified. 
Keywords-network reliability; reliability trends; public 
switched telephone network; local telecommunication switches 
I. 
 INTRODUCTION  
Early in 2013, research on local communication switches 
suffering frequent outages was reported in [1]. This work is 
expanded here, not just to switches with frequent outages, 
but to all local switches experiencing outages over a 14 year 
period. Additionally, in this paper, information on frequently 
out switches is expanded to include manufacturer. In this 
section, the research purpose, importance, and scope of this 
work are discussed. 
A. Research Purpose 
As an extension and expansion of previous research, this 
study analyzes the reliability and survivability of US local 
switches using data on Public Switched Telephone Network 
(PSTN) local switch outages. In addition to that, this study 
determines if a minority of switches account for significant 
reliability or survivability deficits in this important 
component of the PSTN. This study also examines local 
telecommunication switch outage trends and causes. Finally, 
the national policy regarding local switch outage collection 
regulations is briefly assessed. 
In this study, the PSTN is considered as a single 
repairable system. A repairable system is defined as a system 
that passes from an operating mode to a failed mode, and 
then returns to operating mode after a certain period of time 
[2]. In fact, the system is returned to the operating mode by 
means other than replacing the entire system. As local 
switches serve as access nodes for users, it is important to 
understand switch outages in their natural operating setting.  
B. Research Importance and Scope 
Analyzing the reliability and survivability of local 
switches is of great importance because it can help in 
monitoring and improving the efficiency of the entire PSTN 
switching system since local switches form a large 
percentage of all the PSTN switches. Additionally, mobile 
and fixed wireless systems will always benefit from the 
reliability and survivability of the PSTN switching system 
because they greatly depend on it. This is especially true 
when wireless subscribers want to communicate with 
landline subscribers or call wireless subscribers in a different 
geographical area. The reliability assessment of local 
switches, which also includes determining the nature and 
trend of failure events, can help designers (switch 
manufacturers) and operators (service providers) in taking 
corrective or preventive actions where needed. Also, 
investigating the causes of outages in PSTN local switches 
can help in improving wireless switches, as they are very 
similar to wireline switches (i.e., same vendors and similar 
models). Definitely, the reliability of the PSTN is crucial, as 
it is the heart of landline and mobile voice communications. 
The PSTN is a complex system composed of three main 
systems, namely the switching system, the signaling system, 
and the transmission system. The switching system controls 
and routes voice or data signals throughout the network. The 
signaling system enables switches to cooperate in call 
initiation, maintenance, and termination. Finally, the 
transmission system ensures physical connections between 
switches. These three systems enable end-to-end connections 
among PSTN subscribers. The signaling and transmission 
systems are not included in this study. 
The switching system can also be subdivided into 
subsystems that include, the local exchange switching 
subsystem (local switches), the tandem switching subsystem 
(tandem switches), and the international gateway exchange 
subsystem (access switches) (Fig. 1). Only the local 
exchange switching subsystem is investigated in this study. 
Local switches include standalone, host, and remote local 

82
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
switches. As an exception, some tandem switches also have 
access lines, and although they are included in this study, 
they represent a very small percentage of all reported 
switches or more outages. This study investigates reported 
local telecommunication switch outages in the U.S. of at 
least 2 minutes in duration for a 14-year period (1996-2009) 
and considers only totally failed switches rather than 
partially failed switches. Data below 2 minutes and partially 
failed switches are of interest, but not reported by the 
carriers. 
 
Figure 1.  The PSTN switching system. 
Again, the local exchange switching subsystem is 
analyzed as a whole, where all outages of individual local 
switches are pooled to analyze the overall reliability and 
survivability of the PSTN local exchange switching 
subsystem. Availability, safety, security, maintainability are 
other system dependability attributes not assessed in this 
study. Also, the calculated impact of outages includes only 
access lines but not blocked calls, as blocked call data were 
not reported. 
In industry, local switches are called by different names, 
including “class 5 local exchange switch”, “local switch”, 
“central office switch. 
II. 
LITERATURE REVIEW 
In this section, a review of previous relevant research is 
presented in order to assess areas of focus of past researches, 
and establish a relationship between those studies and this 
research. The information presented was gathered from 
different sources, both electronic and non-electronic; 
including, but not limited to, research papers, books, 
conference papers and journals. 
A. Past Studies on Local Telecom Switch Outages 
There have been very few published research papers 
focusing on the reliability or survivability of US local 
telecommunication switches. In a previous research study, 
Snow analyzed local switch outages from 1991 to 1995, 
finding significant reliability growth [3]. Also, a few results 
regarding frequently failing switches were reported, but not 
studied in [4]. Kuhn studied local telecommunication switch 
outage records reported by telephone companies to the 
Federal Communication Systems (FCC), consisting of  
outages affecting at least 30,000 customers for at least 30 
minutes. From those records, covering the period April 1992 
through March 1994, Kuhn reported that the principal causes 
of PSTN large-scale outages were human error, acts of 
nature and traffic overloads [5]. 
Later, Snow investigated the effectiveness of the FCC 
outage reporting threshold, consisting of all reported outages 
affecting at least 30,000 users for at least 30 minutes. He 
used over 18,000 local telecommunication switch outages 
above and below the FCC reporting threshold and reported 
that the FCC outage reporting threshold was not optimal [6]. 
Again, Snow and Agarwal investigated over 19,000 local 
switch outages that occurred over the period 1993 through 
2002 in order to explore an optimal outage reporting 
threshold that could reduce the number of outage reports and 
at the same time allow enough insight into network 
survivability. They found that “PRODUCT” thresholds such 
as lost line-hour (lines out times outage duration) are more 
optimal than AND (lines and duration) thresholds for 
assessing the survivability of telecommunication networks 
[7]. 
B.  PSTN Overview 
The PSTN, which is a collection of interconnected voice-
oriented public telephone networks, was originally designed 
to support circuit-switched landline (or fixed line) voice 
communication. However, it is also used as the backbone 
network of mobile (or wireless) voice communication. Some 
of its elements are also used for Internet based network 
technologies such as Voice over Internet Protocol (VoIP). 
PSTN subscribers are connected to the PSTN network 
through local loops, which physically connect users’ homes 
to central or local offices switches (also known as class 5 
switches or end offices switches) [8]. In fact, local class 5 
switches are the access and delivery points of voice 
communication to and from landline subscribers, and they 
receive numerous software upgrades during their lifetime in 
order to meet user and network ever-changing requirements. 
Currently, most of the PSTN core system uses digital 
switching and transmission whereas many local loops still 
use analog mechanisms. It is clear that today’s PSTN is 
transitioning to a packet switching, IP-based network, but 
this transition will not happen overnight. It will take many 
years to transition the entire PSTN into an all-IP-based 
network, especially the local loops [9]. In fact, although most 
of the interoffice transport network has been replaced by IP 
technology, the majority of the PSTN customers are still 
connected to the PSTN through local circuit-switched 
networks [9]. A recent report found that, “[in] December 
2010, there were 117 million end-user switched access lines 
in service and 32 million interconnected VoIP subscriptions 
in the United States…” [10]. 
In fact, many network operators (services providers) 
want to provide a smooth migration from the legacy PSTN to 
a Next Generation Network (NGN), so they decided to 
consider changes to local loop networks only when 

83
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
expanding them or when replacing a failed system. “VoIP 
began another evolution of the PSTN architecture. The 
PSTN is a large infrastructure that will likely take some time 
to completely migrate to the next generation of technologies; 
but this migration process is underway” [11]. 
In the cellular system, Mobile Telephone Switching 
Offices (MTSOs), also known as Mobile Switching Centers 
(MSCs), are very similar to PSTN Central Offices (COs), 
and they function like class 5 switches [12]. MSCs are 
connected to the PSTN switching centers through Gateway 
Mobile Switching Centers (GMSCs), which are a type of 
MSC, as shown in Fig. 2. The main function of an MSC is to 
connect mobile telephones to landline telephones or two 
mobile telephones in the same area, and an “MTSO is 
supposed to appear as a seamless extension of the public 
switched 
telephone 
network 
from 
the 
customer’s 
perspective” [12]. In fact, both the PSTN and the cellular 
system use circuit switches made by the same equipment 
manufacturers, which are very similar. 
 
 
Figure 2.  PSTN and mobile network. 
As noted from the FCC website, the number of local 
switches has been slightly decreasing with time. This is 
partly due to the current migration from the legacy PSTN 
infrastructure to the more cost effective NGN infrastructure. 
In fact, at the end of their lifecycle, traditional PSTN Class 4 
and Class 5 switches are transformed to media gateways or 
replaced by VoIP soft switches [13, 14, 15]. Media gateways 
(also called access servers) interconnect the traditional PSTN 
to VoIP networks, and they can originate or terminate 
landline phone calls. Soft switches function as other 
telephone switches except that they are software installed on 
servers, and they deal with IP to IP phone calls only. Fig. 3 
depicts an interconnection between the traditional PSTN and 
a VoIP network. 
C. Reliability and Survivability Theories 
The reliability of a system is the probability of that 
system performing and maintaining its designated functions 
at an adequate level of performance, under specified 
circumstances and for a specified period of time [2]. 
Thresholds are used in specifying adequate levels of 
performance for repairable systems in order to differentiate 
operating and failed states [2]. 
 
 
Figure 3.  PSTN and VoIP network. 
As mentioned before, a repairable system is defined as a 
system that passes from an operating mode to a failed mode, 
and back again after a certain period of time by means other 
than replacing the entire system [2]. A repairable system is 
also a counting failure process where successive inter-arrival 
failure times (time-between-failures) will tend to become 
larger for an improving system or become smaller for a 
deteriorating system [16]. The reliability of local switches 
can be assessed by analyzing the nature of failures 
experienced by those switches. 
Point processes have been chosen for modeling the 
failure times since the time to repair or replace the system 
(local switches in our case) was negligible compared to its 
operating time. When the failure rate is constant over time, it 
can be modeled as a homogeneous Poisson process (HPP), 
which means that there is no improvement. On the other 
hand, when the failure rate varies over time, a non-
homogeneous Poisson process (NHPP) is a better fit to 
model either an improving or deteriorating system [2]. 
The Laplace trend test, a reliability trend test, determines 
if there is a significant change in the pattern of successive 
failures of a repairable system over time. In fact, “[the] 
Laplace test, also known as the centroid test, is a measure 
that compares the centroid of observed arrival times with the 
midpoint of the period of observation. This measure 
approximates the standardized normal random variable (e.g., 
z-score)” [17]. The reliability trend test for repairable 
systems assumes the null hypothesis (Ho) to be HPP (no 
trend) and the alternative hypothesis (Ha) to be NHPP (there 
is a trend). If the null hypothesis can be rejected at a 
specified significance level, then it can be concluded that the 
system is either improving or deteriorating over the 
timeframe of interest [16]. The Laplace score U is given by 
the formula [17]:  

84
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
(1) 
where: 
ti is the time (e.g., number of days) from a fixed start 
point to the time of each event (outage). 
n is the number of outage events (if tn = T, then n-1 is 
used instead of n in the formula). 
T is the time from the start point to the end of the study 
period. 
A positive score implies an upward or increasing trend 
(i.e., the system is deteriorating), a negative score implies a 
downward or decreasing trend (i.e., the system is improving), 
and a null score implies a constant trend (i.e., no change). 
Furthermore, “[when] the score is greater than (less than) 
+1.96 (-1.96), we are at least 95% confident that there is a 
significant trend upward (downward)” [17]. 
As an example of reliability assessment using the 
Laplace trend test, consider the failure arrival times given in 
Table I, where U-scores were calculated using the 
aforementioned formula. We can see from the table that the 
first set of sample failure arrival times represents an 
increasing trend (i.e., reliability deterioration) where the 
Laplace score U equals +3.46. The second set of sample 
failure arrival times represents a constant trend (i.e., no 
change in reliability) because the Laplace score U, which 
equals +1.79, is neither greater than +1.96 nor less than -
1.96. The third set of sample failure arrival times represents a 
decreasing trend (i.e., reliability growth) where the Laplace 
score U equals -2.07. Visual representations of the increasing 
trend, the constant trend and the decreasing trend are given 
in Figs. 4, 5 and 6 respectively. 
TABLE I. 
LAPLACE SCORE & FAILURE TREND EXAMPLES 
Failure Trend 
Examples 
Increasing 
Constant 
Decreasing 
Failure Arrival 
Times 
1 
1 
1 
8 
3.5 
1.1 
10 
6 
1.3 
11.5 
8.5 
1.8 
12.5 
10.5 
2.6 
13 
13 
3.5 
Laplace Score (U) 
3.46 
1.79 
-2.07 
 
 
Survivability is defined as “the capability of a system to 
fulfill its mission, in a timely manner, in the presence of 
attacks, failures, or accidents.” Typically, for a system to 
survive, it must automatically react to (and recover from) a 
harmful incident well before the root cause has been 
identified [18]. The survivability of local switches can be 
determined by analyzing the frequency and impact of 
failures experienced by those switches. 
A system is delivering correct service when it is 
adequately fulfilling its functions. A service failure (or 
simply a failure) occurs when the system is not adequately 
implementing its functions, and the period during which the 
system is delivering incorrect service is called a service 
outage (or simply an outage) [19]. Failures are incidents that 
are likely to disturb the system and cause it to deliver 
incorrect service. They may be caused by deficiencies in the 
system or by external components to which the system is 
attached. Failures may be due to such things as 
software/hardware design errors, human errors, traffic 
overload, and natural disasters [18]. 
 
 
 
Figure 4.  Example of reliability deterioration . 
 
 
 
Figure 5.  An example of a constant failure trend. 
 
 
 
Figure 6.  Example of reliability growth (decreasing failure trend). 

85
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Lost line hours, LLH, is an outage impact metric (or 
communication loss metric) that can be used to assess system 
survivability. LLH is the product of the number of access 
lines served by the and the duration of the outage [6]. For 
example, if a 3,000 access lines switch went down for 20 
minutes, the LLH would be 1,000, meaning equivalency with 
1,000 lines out for an hour. In this research, the LLH metric 
has been chosen to assess the survivability of local switches 
because it is simple and intuitive.  
The metric also takes into account the size of the failed 
switch and the duration of the outage. It does not however 
account for blocked calls, but that data is not available from 
the switch outage reports. 
 
D. The FCC Reporting Systems 
The FCC, established by the Communications Act of 
1934, is an independent agency of the US government. It 
regulates interstate and international communications by 
radio, television, wire, satellite and cable, and its jurisdiction 
covers the 50 states, the District of Columbia, and other US 
territories [20]. 
Until 2009, all US Local Exchange Carriers (LECs) 
reported each telecommunication switch outage of two 
minutes or more to the ARMIS (Automated Reporting 
Management Information System) section of the FCC 
website. Those public reports are part of FCC Report 43-05, 
the ARMIS Service Quality Report [21]. Note that the two-
minute ARMIS reporting limit is used as the reliability 
threshold in this study. Again note that only totally failed 
switches were reported. The switch population required to 
report by LECs account for over 90% of the landline 
telephone access lines in the US [3]. 
ARMIS was initiated in 1987 to collect financial and 
operational data from the largest carriers in the US. Later in 
1991, additional ARMIS reports were added to collect 
service quality (i.e., the FCC Report 43-05) and network 
infrastructure information from all US LECs subject to price 
cap regulations. However, after 2009, the FCC stopped 
collecting Report 43-05 as stated in the ARMIS Forbearance 
Order, where 
The Commission granted conditional forbearance 
from carrier obligations to file ARMIS Reports 43-
05 and 43-06 provided that the carriers committed 
to file the data voluntarily for 24 months after 
September 6, 2008. The 24 months ended on 
September 6, 2010; and carriers do not file Reports 
43-05 and 43-06 for reporting year 2010 and 
subsequent years [21]. 
The reports submitted to ARMIS contain information on 
each failed switch, including the date, time of outage 
occurrence, number of lines supported, outage duration, and 
outage cause. Additionally reported items include whether 
the switch is located in a Metropolitan Statistical Area (MSA 
= urban area) or not (Non-MSA = rural area), COSA 
(Company Operating company Study Area), and the switch 
CLLI code (Common Language Location Identifier). CLLI 
codes are unique identifiers for individual local switches. 
Carriers reported outage cause using one of fifteen different 
cause codes created by the FCC. In this study, cause code 1 
is a scheduled maintenance outage; while cause codes 2 
through 15 are considered failures, resulting in an outage. All 
15 cause codes are defined in Section V. 
III. 
RESEARCH GOALS AND OBJECTIVES 
For a system to be properly improved it is necessary to 
know its past state and performance. Hence, from this 
perspective, monitoring the performance and assessing the 
reliability of local switches during past years will help 
understand what caused the outages to occur, and thus take 
corrective/preventive 
actions 
to 
alter 
future 
trends. 
Preliminary data exploration reveals a small number of 
switches or more outages an appreciable percent of all switch 
outages. The principal goal of this research is to compare the 
outage causality and switch characteristics between more 
frequently failing local switches and less frequently failing 
local switches. As its objective, this research focuses on 
addressing the following questions concerning local switches 
performance over a study period of 14 years: 
A. Research Questions 
1. 
What 
are 
the 
major 
causes 
of 
local 
telecommunication switch outages? 
2. 
Is the reliability of local switches improving, 
constant, or deteriorating? 
3. 
Are there individual switches that experience 
outages/failures more so than others? 
4. 
Are there similarities/dissimilarities between switches 
failing more often and those that do not? In terms of: 
a. 
LLH 
b. 
Rural versus Urban 
c. 
Outage causes 
d. 
Outage duration 
e. 
Time of Day (TOD) 
f. 
Day of Week (DOW) 
g. 
Month of Year (MOY) 
h. 
Outage/failure trends 
5. 
Are there switch manufacturers that account for 
outages/failures more so than others? 
IV. 
METHODOLOGY 
A. Research Data 
The data used in this study were drawn from the FCC’s 
ARMIS website (http://transition.fcc.gov/wcb/armis/) where 
US LECs reported switches that experienced a downtime of 
two minutes or more, and the data cover a 14-year period 
(from 1996 to 2009). The data include different information 
on the failed switches such as date, week day and time when 
the outages occurred, number of lines supported by the 
switches, duration and cause of the outages, MSA, COSA, 
and CLLI codes. Fifty-nine records were removed due to 
errors in recording the data, which left a total of 12,860 
records.  

86
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
In order to conduct research on frequently failing local 
switches, ARMIS data has been augmented by data from 
TelcoData.us, 
where 
additional 
information 
on 
US 
telecommunication switches is provided, such as switch 
models/manufacturers, and switch locations [22]. The Local 
Calling Guide website also provides information on US local 
switches, and was used to verify data from TelcoData.us and 
match these data to data from ARMIS [23]. 
B. Data Analysis Methods 
The frequency of outage causes has been analyzed to 
assess the reliability of local switches, and the impact of 
those outage events has been analyzed to assess switch 
survivability. In order to assess differences that might exist 
in local telecommunication switch outages, the data have 
been separated into two categories: more frequently failing 
switches (8 or more outages over the study period) and less 
frequently failing switches (7 or less outages over the study 
period). The reliability difference between categories has 
been investigated by comparing failure trends and Laplace 
scores. Likewise, the survivability difference has been 
investigated by comparing impact trends and their respective 
LLH values. 
In this study two measures of central tendency have been 
used to compare the two categories of data: the mean and the 
median. The t-Test (Two-Sample Assuming Unequal 
Variances) has been used to determine whether any 
difference between the means in the two local switch 
categories is statistically significant. We assumed that the 
two data sets came from distributions with unequal variances 
because one data set is considerably larger than the other 
one. Acts of god include such circumstances as wind, 
flooding, and earthquake. 
For the t-Test, the null hypothesis (Ho) is that the means 
are the same, while the alternative hypothesis (Ha) is that the 
means are different. 
V. 
RESEARCH RESULTS ANALYSIS 
A. Causes of Local Telecommunication Switch Outages 
As mentioned earlier, the cause of each reported outage 
was classified by carriers using 15 different cause codes. An 
abbreviated definition of each cause code together with the 
total number of reported outages for each are shown in Table 
II. The outage distribution is also shown in Fig. 7. From this 
table, two causes account for over 50% of the local switch 
outages:  
 
Scheduled outages (cause code 1) 
 
Random hardware failure (cause code 8) 
Scheduled outages are planned for short duration, and 
therefore have little impact on the PSTN users because they 
are scheduled during hours and days of low traffic on the 
PSTN network. On the contrary, failures resulting in outages 
are unpredictable, occurring at any time and any day, 
impacting PSTN users in many cases. For that reason, it is 
also important to investigate the major causes of failures in 
PSTN local switches. 
By discounting scheduled outages (cause code 1) and 
looking at failed switches (cause codes 2 to 15), we note 
from Table III that the main sources of failures in local 
switches account for almost 2/3 of all failures: 
 
Hardware failures (cause code 8) 
 
Software design errors (cause code 6) 
 
Acts of God (cause code (9) 
 
External power failures (cause code 12) 
Tables III and IV give detailed information on the causes of 
local telecommunication switch outages; where Table IV 
consolidates cause codes to provide insights into major 
causal categories. 
In this paper, major cause codes have been categorized as 
follows: 
 
Scheduled Outages (Cause Code 1): scheduled or 
planned maintenance activities. 
 
 Human Procedural Errors (Cause Codes 2 to 5): 
installation/non-installation and maintenance/non-
maintenance related errors made by the operating 
company technicians, or other errors made by system 
vendors or other vendors. 
 
Design Errors (Cause Codes 6 and 7): errors made 
by system vendors in designing the software or  
 
Hardware Failures (Cause Code 8): other hardware 
failures except design errors. 
 
External Circumstances (Cause Codes 9 to 14): other 
events, but external to the switch, which cause the 
switch to fail. 
 
Others/Unknown (Cause Code 15): all other events, 
different from the above, that cause the switch to fail. 
TABLE II. OUTAGE DISTRIBUTION BY CAUSE  
Cause 
Code 
Description 
No. 
Outages 
% 
1 
Scheduled 
3,885 
30.2% 
2 
Procedural error (Telco 
install./maint.) 
446 
3.5% 
3 
Procedural error (Telco 
non-install./non-maint.) 
376 
2.9% 
4 
Procedural error (System 
vendor procedural error) 
315 
2.4% 
5 
Procedural error (Other 
vendor procedural error) 
257 
2.0% 
6 
Software design 
1,078 
8.4% 
7 
Hardware design 
136 
1.1% 
8 
Hardware failure 
2,951 
22.9% 
9 
Acts of God 
935 
7.3% 
10 
Traffic overload 
17 
0.1% 
11 
Environmental 
83 
0.6% 
12 
External power failure 
896 
7.0% 
13 
Massive line outage, 
cable cut, other 
660 
5.1% 
14 
Remote (Loss of facilities 
between host and remote) 
309 
2.4% 
15 
Other/unknown 
516 
4.0% 
 
Total 
12,860 
100% 

87
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
Figure 7.  Local switch outage distribution. 
TABLE III. 
FAILURE DISTRIBUTION BY CAUSE 
Cause 
Code 
Description 
No. 
Outages 
% 
2 
Procedural error (Telco 
install./maintenance) 
446 
5.0% 
3 
Procedural error (Telco 
non-install./non-
maintenance) 
376 
4.2% 
4 
Procedural error (System 
vendor procedural error) 
315 
3.5% 
5 
Procedural error (Other 
vendor procedural error) 
257 
2.9% 
6 
Software design 
1,078 
12.0% 
7 
Hardware design 
136 
1.5% 
8 
Hardware failure 
2,951 
32.9% 
9 
Acts of God 
935 
10.4% 
10 
Traffic overload 
17 
0.2% 
11 
Environmental 
83 
0.9% 
12 
External power failure 
896 
10.0% 
13 
Massive line outage, 
cable cut, other 
660 
7.4% 
14 
Remote (Loss of facilities 
between host and remote) 
309 
3.4% 
15 
Other/unknown 
516 
5.7% 
 
Total 
8,975 
100% 
 
TABLE IV. LOCAL SWITCH FAILURE DISTRIBUTION BY CATEGORY  
Cause Code Category 
No.  
Outages 
% 
Human Procedural Errors (2-5) 
1,394 
15.5% 
Design Errors (6-7) 
1,214 
13.5% 
Hardware Failures (8) 
2,951 
32.9% 
External Circumstances (9-14) 
2,900 
32.3% 
Others/Unknown (15) 
516 
5.7% 
Total 
8,975 
100% 
 
The percentage distribution of outages among the 
different cause code categories is shown in Table IV. It can 
be seen that scheduled outages (about 30% of all outages), 
hardware failures (about 23% of all outages) and external 
circumstances (also about 23% of all outages) are the main 
cause categories of local telecommunication switch outages. 
Table IV indicates that after hardware and external 
circumstances, procedural and design errors account for 29% 
of failures.  
B. Reliability Trends of Local Switches 
The outage rate of local switches can give us insight into 
reliability trends. As mentioned before, the variation of the 
outage rate over time implies a NHPP, which allows us to 
determine whether the system has been improving or 
deteriorating. Again, Laplace scores greater than 1.96 and 
less than -1.96 indicate strong statistical evidence of an 
increasing or decreasing trend, respectively. 
A cumulative time series graph of all outages occurring 
over the 14-year study period is shown in Fig. 8. 
 
 
 
Figure 8.  Switch reliability trend (all cause codes). 
  
The figure, as well as the accompanying Laplace score 
(U = -29.75), show that, overall, there has been exceptionally 
strong reliability growth over the study period. However, 
there also seems to have been periods of reliability growth 
and periods of reliability deterioration along the study 
period. For example, the reliability was relatively constant 
from the start of the study period until the end of year 4; then 
it improved from year 5 until the end of year 11; 
subsequently, the trend started bending upwards towards the 
end of the study period (from year 12), which means that the 
reliability was starting to deteriorate. 
By separating failures from scheduled outages we can 
gain a clearer picture of local switch reliability improvement. 
The reliability trend, considering only the scheduled outages, 
is shown in Fig. 9. From the figure, as well as from the 
accompanying Laplace score (U = -58.41), we conclude 

88
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
remarkably strong reliability growth, especially from year 5 
(i.e., 2000). These switches have “A” and “B” processors, 
one of which is primary and the other a live backup. Early in 
the study period vendors/carriers took the entire switch down 
for software/feature changes, resulting in short outages. 
Towards the end of the study period, one processor was 
taken down at a time for upgrade, and switch continued 
operating with many fewer outages..  
On the other hand, from Fig. 10, which shows failures 
(all outages other than those that were scheduled), we notice 
a statistically significant reliability decrease (U = 2.81).  
 
 
 
Figure 9.  Switches reliability trend (Scheduled: cause code 1). 
 
 
 
Figure 10.  Switch reliability trend (Failures: cause codes 2-15). 
 
 
This means that the switches have been failing more and 
more often due to increasing failures for the last three years 
of the study period. Therefore, by combining the two trends, 
one from scheduled outages and the other one from failures 
that resulted in outages, we get a trend that shows an overall 
reliability improvement in local switches from 1996 to 
2009, but interesting insights are gained by separating 
scheduled outages and failure-induced outages. 
C. More Frequently Failing Switches Analysis 
There are many switches that failed more than once 
during the 14-year study period. A logarithmic chart of the 
number of outages encountered by failed switches during the 
study period is shown in Fig. 11. A total of 6,132 local 
switches have been responsible for 12,860 outages. The 
failed switches can be divided into two categories according 
to the number of outages that each unique switch 
encountered during the study period. In fact, 5,976 unique 
switches (about 97% of all switches) experienced 7 outages 
or less (which made about 80% of all outages); while 156 
unique switches (about 3% of all switches) experienced 8 or 
more outages (which made about 20% of all outages) during 
the study period. The choice to focus on 8 or more outages 
was influenced by the fact that the curve from 1 outage to 8 
outages in Fig. 11 was smooth and started to become 
irregular from 9 outages and up. 
 
 
Figure 11.  Unique local switch outage frequency (logarithmic scale). 
 
Indeed, the more frequently failing switches account for 
only 3% of all individual switches failing over the study 
period. However, those 3% of the individual switches caused 
20% of all outages that occurred.. From a survivability 
perspective, even if the more frequently failing switches 
caused 20% of all outages, they are responsible for only 7% 
of all LLH. Additionally, these more frequently failing 
switches are responsible for 9% of all lines down during the 
study period, and those lines out account for 22% of the total 
outage duration (more details are shown later in Table V). 
As Fig. 11 uses a logarithmic scale in presenting the 
number of switches, instances of 1 switch having numerous 
outages is not indicated. Those instances are in Table V, 
where it is seen that remarkably, five different switches 
experienced 92, 75, 71, 60, and 48 outages. Additionally, 
two different switches both experienced 47 outages. 
D. Frequently and Less-Frequently Failing 
SwitchComparisons 
As mentioned earlier, the local switches have been 
divided into two categories, one of less frequently failing 
switches and another one of more frequently failing 
switches. Comparisons are made between impact (LLH), 

89
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
switch location (rural or urban), outage cause codes, outages 
duration, time of day, day of week and month of year, and 
outage trends. 
Each switch that fails has an impact on customers 
connected to that switch. A large switch (i.e., that has many 
lines connected to it) can go down for few minutes and still 
have the same impact as a small switch that goes down for 
many minutes. That is why the LLH metric is the best means 
to assess the impact of many different outages pooled 
together because it gives only one value, which is the 
product of the number of lines supported by the failed switch 
and the duration of the outage. 
TABLE V. 
UNIQUE SWITCH OUTAGE FREQUENCY OVER 14 YEARS 
No.  
Outages 
No. Unique 
Switches 
 
No.  
Outages 
No. Unique 
 Switches 
1 
3,701 
 
22 
2 
2 
1,259 
 
23 
2 
3 
489 
 
25 
4 
4 
247 
 
26 
3 
5 
158 
 
27 
1 
6 
81 
 
28 
5 
7 
41 
 
29 
1 
8 
21 
 
30 
1 
9 
27 
 
31 
1 
10 
15 
 
32 
2 
11 
9 
 
36 
1 
12 
4 
 
40 
1 
13 
11 
 
43 
1 
14 
6 
 
47 
2 
15 
9 
 
48 
1 
16 
5 
 
60 
1 
17 
6 
 
71 
1 
18 
1 
 
75 
1 
20 
5 
 
92 
1 
21 
5 
 
 
 
 
 
Before comparing the two categories of switches that 
experienced outages during the study period, let us have a 
look at the total LLH per year that resulted from all outages 
that occurred during the study period. We can see from Fig. 
12 that there have been higher survivability deficits in 2001, 
2005, and 2007. The figure also shows that the majority of 
outages were due to cause codes other than cause code 1, 
which is scheduled outage. 
The high survivability decrease in 2001 is related to the 
9/11 attacks in New York City since 77% (about 33,400,000 
LLH) of the 2001 total LLH resulted from five switches 
located in New York, on September 11, 2001. Furthermore, 
the high survivability decrease in 2005 appears to be due to 
the 2005 Atlantic hurricane season, “the most devastating 
hurricane season the country has experienced in modern 
times,” [25] as 97% of the 2005 LLH occurred during the 
hurricane season, which begins June 1st and ends November 
30th. The 2005 Atlantic hurricane season’s strongest 
hurricanes include hurricanes Wilma and Katrina. Similarly, 
the survivability decrease in 2007 appears to be due to the 
2007 Atlantic hurricane season as 69% (about 47,000,000 
LLH) of the 2007 LLH resulted from outages occurring 
during hurricane season. The 2007 Atlantic hurricane 
season’s strongest hurricane was hurricane Dean. 
A LLH comparison for each local switch category during 
the study period is given in Fig. 13. It is seen that the more 
frequently failing switches account for a very small portion 
of the impact on the PSTN.  
 
 
 
Figure 12.  LLH per year for all versus scheduled outages. 
 
Figure 13.  LLH per year for frequent/infrequectly out switches. 
As for the 8 or more outages category (shown more 
clearly by Fig. 14), there have been two major survivability 
deficits along the 14-year study period. The first one 
occurred in 1999 where the LLH count reached almost 
2,000,000 lines hours. The second and longest survivability 
decrease started in 2005 and continued until 2008 where it 
started to increase. There has been a slight survivability 
increase in 2007 but it was nothing compared to the 
survivability decrease that preceded in 2006 and the one that 

90
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
followed in 2008. During this deterioration period, the 
highest LLH count reached almost 5,000,000 lines hours. 
The reason for the peaks is not discernible from the data.  
A comparison of the number of outages/year experienced 
by switches in both categories, taking into account whether 
they are located in urban or rural areas, is shown in Figs. 15 
and 16. In both categories rural switches suffered outages 
more often than urban switches. 
 
 
Figure 14.  LLH per year for switches out ≥8 times. 
 
From Fig. 15 we can see switches or more outages 7 outages 
or less had most of its outages at the start of the study period, 
from both rural and urban switches. On the other hand, 
switches or more outages 8 or more outages had most of its 
outages towards the end of the study period, mostly from 
rural switches (Fig. 16). 
 
 
 
Figure 15.  Urban vs rural switch outages per year (≤ 7 outages per switch) 
 
Figure 16.  Urban vs. rural switch outages per year (≥8 outages per switch) 
A comparison of the cause code frequency in both 
categories is shown in Fig. 17. Note that the switches that 
experienced 7 outages or less suffered considerably from 
scheduled outages (cause code 1) more often than the 
switches that experienced 8 or more outages. On the other 
hand, the switches that experienced 8 or more outages 
considerably suffered from hardware failures (cause code 8) 
and acts of God (cause code 9) more often than the switches 
in the other category. 
 
 
Figure 17.  Outage cause code frequency. 
By categorizing cause codes, we see from Fig. 18 that 
switches experiencing 8 or more outages suffered from 
design 
errors, 
random 
hardware 
failures, 
external 
circumstances and other/unknown causes more often than 
the switches experiencing 7 outages or less. Those failures 
might be the consequence of insufficient maintenance 
activities for the more frequently failing switches since most 
are located in rural areas. It is most interesting that the less 
frequently failing switches suffered a higher percentage of 
outages due to human procedural errors (Fig. 18).  
 

91
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
Figure 18.  Causal Category frequency distribution. 
Outages can be examined for all causes, scheduled 
causes, and failures. For each of these cases, the two outage 
frequency categories can also be compared.  
 
1) Outages from all Causes 
A numerical comparison between the two outage 
frequency categories, for all outage causes, is given in Table 
VI. Again, note that even if the switches experiencing 8 or 
more outages represent only 3% of all unique switches that 
failed during the 14-year study period, they are responsible 
for 20% of all outages that occurred. Additionally, those 
more frequently failing switches are responsible for 9% of 
all lines that went down during the study period, which 
corresponds to 22% of the total down time, in addition to 
7% of the total LLH. Although the average duration values 
are roughly equal in both categories (3.6 and 3.2 hours per 
outage), the average LLH values are considerably different 
(8,129 and 28,890 LLH per outage) because the more 
frequently failing switches are in most cases smaller 
switches. 
 
2) Scheduled Outages 
A numerical comparison between the two outage 
frequency categories for scheduled outages is given in Table 
VII. In this case, the more frequently failing switches 
represent 3% of all unique switches that went down due to 
scheduled maintenance activities, which made 11% of all 
scheduled outages that occurred during the 14-year study 
period. Those more frequently failing switches are also 
responsible for 7% of all lines that went down due to 
scheduled maintenance activities during the study period, 
which corresponds to 21% of the total down time and hence 
9% of the LLH due to scheduled maintenance activities. 
Also notice that average duration and average LLH for both 
categories are approximately equal. 
 
TABLE VI. OUTAGE CHARACTERISTICS COMPARISON 
All Cause 
Codes 
All 
Outages 
≥ 8 
Outages 
≤ 7 
Outages 
% ≥ 8 
%≤ 7 
No. of 
Outages 
12,860 
2,623 
10,237 
20% 
80% 
No. of 
Switches 
6,132 
156 
5,976 
3% 
97% 
Tot. Switch 
Lines Out 
123.2 M 
10.5 M 
112.7 M 
9% 
91% 
Tot. Dur. 
(Hours) 
42,391 
9,443 
32,947 
22% 
78% 
Total LLH 
317.1 M 
21.3 M 
295.8 M 
7% 
93% 
Avg. Switch 
Size (Lines) 
9,584 
4,040 
11,005 
 
 
Avg. Dur. 
(Hours) 
3.3 
3.6 
3.2 
 
 
Avg. LLH 
24,655 
8,129 
28,890 
 
 
Median TOD 
10:14am 
11:38am 
9:51am 
 
 
Mean TOD 
10:55am 
11:41am 
10:43am 
 
 
Median 
DOW 
4.13 
4.36 
4.11 
 
 
Mean DOW 
4.27 
4.37 
4.25 
 
 
Median 
MOY 
6.89 
6.98 
6.89 
 
 
Mean MOY 
6.92 
6.90 
6.93 
 
 
Med. MSA 
No=Rural 
No 
No 
No 
 
 
TABLE VII. 
SCHEDULED OUTAGE COMPARISON 
Cause Code 1 
All 
Outages 
≥ 8 
Outages 
≤ 7 
Outages 
% ≥ 8 
% ≤ 7 
No. of  
Outages 
3,885 
413 
3,472 
11% 
89% 
No. of 
 Switches 
2,470 
86 
2,384 
3% 
97% 
Tot. Switch 
Lines Out 
57.6 
4.0 M 
53.6 M 
7% 
93% 
Tot. Dur. 
(Hours) 
998 
211 
787 
21% 
79% 
Total LLH 
9.2 M 
0.9 M 
8.3 M 
9% 
91% 
Avg. Switch 
Size(Lines)  
14,830 
9,605 
15,451 
 
 
Avg. Dur. 
(Hours) 
0.3 
0.5 
0.2 
 
 
Avg. LLH 
2,385 
2,093 
2,420 
 
 
Median TOD 
5:47am, 
4:37am 
6:00am 
 
 
Mean TOD 
10:38am 
10:32am 
10:39am 
 
 
Median 
DOW 
4.18 
5.03 
4.16 
 
 
Mean DOW 
4.40 
4.68 
4.36 
 
 
Median 
MOY 
6.92 
6.56 
6.97 
 
 
Mean MOY 
7.04 
6.89 
7.06 
 
 
Med. MSA 
(No=Rural) 
No 
No 
Yes 
 
 
 
 

92
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
3) Outages Due to Failures 
A numerical comparison between the two outage 
frequency categories, due to failures (cause codes 2-15) is in 
Table VIII. In this case, the more frequently failing switches 
still represent only 3% of all unique failed switches resulting 
in outages, but they caused 25% of all failures occurring. 
Those more frequently failing switches are also responsible 
for 10% of all lines that went down, which corresponds to 
22% of the total down time, and hence 7% of the total LLH. 
Also notice that even if the average duration in both 
categories are roughly equal, the average LLH per switch 
are very different (9,257 vs. 42,475 LLH). 
The t-Test results are given in Table IX. Again, a 
significance level of 0.05 has been used. Significant 
temporal differences between frequently out and less 
frequently out switches are apparent.  
 
E. Local Switch Outage and Failure Trends 
The outage trends of local switches in both categories 
during the 14-year study period, taking into consideration all 
cause codes, are shown in Figs. 19 and 20.  
 
 
TABLE VIII. 
FAILURE CHARACTERISTICS COMPARISON 
Cause: 
Codes 2-15 
All 
Outages 
≥ 8 
Outages 
≤ 7 
Outages 
% ≥ 8 
% ≤ 7 
Number 
Outages 
8,975 
2,210 
6,765 
25% 
75% 
Number 
Switches 
4,517 
154 
4,363 
3% 
97% 
Tot. Switch 
Lines 
65.6 M 
6.6 M 
59.0 M 
10% 
90% 
Tot. Dur.  
(Hours) 
41,393 
9,232 
32,160 
22% 
78% 
Total LLH 
307.8 M 
20.5 M 
287.3 M 
7% 
93% 
Avg. Switch 
Size 
7,313 
3,000 
8,723 
 
 
Avg. Dur.  
(Hours) 
4.6 
4.2 
4.8 
 
 
Avg LLH 
34,295 
9,257 
42,475 
 
 
Median 
TOD 
10:57am 
12:00pm 
10:34am 
 
 
Mean TOD 
11:02am 
11:54am 
10:45am 
 
 
Median 
DOW 
4.09 
4.29 
4.06 
 
 
Mean DOW 
4.22 
4.32 
4.19 
 
 
Median 
MOY 
6.89 
7.08 
6.82 
 
 
Mean MOY 
6.87 
6.90 
6.86 
 
 
Med. MSA 
(N=Rural) 
N 
N 
N 
 
 
 
 
TABLE IX. TEMPORAL T-TEST RESULTS 
t-Test 
(MEANS) 
≥ 8 
Outages 
≤ 7 
Outages 
Difference 
P-value  
(one-tail) 
TOD All 
Codes 
11:41 am  
10:44 am 
YES 
0.000 
TOD 
Scheduled 
10:32 am 
10:40 am 
NO 
0.400 
TOD  
Failures 
11:54 am 
10:46 am 
YES 
0.000 
DOW  
All 
4.37 
4.25 
YES 
0.001 
DOW 
Scheduled 
4.68 
4.36 
YES 
0.000 
DOW 
Failures 
4.32 
4.19 
YES 
0.003 
MOY 
All 
6.90 
6.93 
NO 
0.335 
MOY 
Scheduled 
6.89 
7.06 
NO 
0.146 
MOY 
Failures 
6.90 
6.86 
NO 
0.322 
 
For the switches that experienced 7 outages or less, we can 
see from Fig. 19, as well as from the accompanying Laplace 
score (U = -48.66), that there has been steady reliability 
growth from the start of the study period until the end of the 
study period. 
 
Figure 19.  Outage trend for switches with ≤ 7 outages. 
 
Figure 20.  Outage trend for local switches or more outages ≥8 outages. 
 

93
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
However, for the switches that experienced 8 or more 
outages, we can see from Fig. 20, as well as from the 
accompanying Laplace score (U = 30.26), that the reliability 
has exhibited dramatic deterioration. However, there seem 
to have been periods of strong reliability growth and 
deterioration during the study period. In order to gain more 
insight into the causes of the outage trend for switches that 
experienced 8 or more outages along the study period, the 
outage trend has been divided into three regions as follows: 
 
0 ≤ Region I < 2 years 
 
2 years ≤ Region II < 11 years 
 
11 years ≤ Region III < 14 years 
The outage trend of more frequently failing switches 
divided into regions and considering all the cause codes is 
shown in Fig. 21. From the figure we can see that the 
reliability was constant in Region I (U = -0.29) from 1996 to 
1997; it then slightly decreased in Region II (U = 3.22) from 
1998 to 2006; and finally things got worse in Region III 
where the reliability sharply decreased (U = 11.84) from 
2007 to 2009.  
The number of outages per cause code that occurred in 
each region is shown in Table X. We can see from the table 
that most of the outages occurred in Region III (62% of all 
outages in all regions). We can also note from the table that 
most of the outages in Region I resulted from cause code 1 
(scheduled outages) and cause code 6 (software design); 
most of the outages in Region II resulted from cause codes 1 
and 8 (hardware failures), and most of the outages in Region 
III resulted from cause codes 8 and 9 (acts of God). 
 
 
Figure 21.  Trend analysis for local switches or more outages  ≥8 outages. 
By combining the cause codes in categories we can see 
from Table XI that the major causes of outages in Region I 
are scheduled outages and design errors; the major causes of 
outages in Region II are external circumstances, hardware 
failures and scheduled outages; and the major causes of 
outages in Region III are external circumstances and 
hardware failures. 
TABLE X. OUTAGE FREQUENCY BY CAUSE CODE FOR MORE FREQUENTLY 
FAILING SWITCHES IN REGIONS  
Cause 
Code 
All 
No.         % 
Reg. I 
No.        % 
Reg. II 
No…….% 
Reg. III 
No.         % 
1 
413 
15.7 
253 
49.8 
95 
19.5 
65 
4.0 
2 
49 
1.9 
5 
1.0 
24 
4.9 
20 
1.2 
3 
12 
0.5 
1 
0.2 
5 
1.0 
6 
0.4 
4 
14 
0.5 
1 
0.2 
7 
1.4 
6 
0.4 
5 
30 
1.1 
1 
0.2 
8 
1.6 
21 
1.3 
6 
284 
10.8 
213 
41.9 
52 
10.7 
19 
1.2 
7 
19 
0.7 
3 
0.6 
3 
0.6 
13 
0.8 
8 
718 
27.4 
26 
5.1 
112 
23.0 
580 
35.6 
9 
368 
14.0 
0 
0.0 
21 
4.3 
347 
21.3 
10 
4 
0.2 
0 
0.0 
1 
0.2 
3 
0.2 
11 
14 
0.5 
0 
0.0 
3 
0.6 
11 
0.7% 
12 
224 
8.5 
4 
0.8 
41 
8.4 
179 
11.0% 
13 
215 
8.2 
1 
0.2 
61 
12.6 
153 
9.4% 
14 
104 
4.0 
0 
0.0 
41 
8.4 
63 
3.9% 
15 
155 
5.9 
0 
0.0 
12 
2.5 
143 
8.8% 
Total 
2,623 
100 
508 
100 
486 
100 
1,629 
100% 
Pcnt.. 
100% 
19.4% 
18.5% 
62.1% 
 
Note that the number of scheduled outages considerably 
decreased from Region I to Region III.  Perhaps that is the 
reason why the number of outages resulting from hardware 
failures and external circumstances considerably increased 
from Region I to Region III, as the switches received 
maintenance. 
TABLE XI. OUTAGE FREQUENCY BY CAUSE CATEGORY FOR MORE 
FREQUENTLY FAILING SWITCHES IN REGIONS   
Cause 
Category 
All 
No.         % 
Region I 
No.         % 
Region II 
No.         % 
Region III 
No.         % 
Sched. 
(1) 
413 
15.7 
253 
49.8 
95 
19.5 
65 
4.0 
Proc. Err  
(2-5)  
105 
4.0 
8 
1.6 
44 
9.1 
53 
3.3 
Design 
Err.(6-7) 
303 
11.6 
216 
42.5 
55 
11.3 
32 
2.0 
Hdw 
 (8) 
718 
27.4 
26 
5.1 
112 
23.0 
580 
35.0 
Ext. 
Circum. 
(9-14) 
929 
35.4 
5 
1.0 
168 
34.6 
756 
46.4 
Other 
(15) 
155 
5.9 
0 
0.0 
12 
2.% 
143 
8.8 
Total 
2,623 
100 
508 
100 
486 
100 
1,629 
100 
 
 
The outage trends of local switches in both categories 
during the 14-year study period, for scheduled outages , are 
shown in Figs. 22 and 23. For the switches that experienced 
7 outages or less, we can see from Fig. 22, as well as from 
the accompanying Laplace score (U = -56.03), that the 
reliability strongly improved during the study period. This 
means that there have been less and less scheduled 
maintenance activities in those switches along the study 
period. 
For the switches that experienced 8 or more outages, we 
can see from Fig. 23, as well as from the accompanying 
Laplace score (U = -16.67), that the reliability in those 

94
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
switches also improved during the study period. In fact, the 
reliability has been constant from the start of the study 
period until year 11, but with a high outage rate from the 
start to year 2 (around 125 outages per year) and then a low 
outage rate from year 3 to year 11 (around 11 outages per 
year). The overall reliability growth was moderate because 
the switches seemed to have had a short period of reliability 
deterioration at the end of the study period (from year 12 to 
the end). This means that there have been more scheduled 
maintenance activities during the last few years of the study 
period. 
 
 
Figure 22.  Scheduled outage trend for switches with ≤ 7 outages. 
 
 
Figure 23.  Scheduled outage trend for switches with ≥8 outages. 
The failure trends of local switches in both categories 
during the 14-year study period, for failures (cause codes 2-
15) are shown in Figs. 24 and 25. For the switches that 
experienced 7 outages or less, we can see from Fig. 24, as 
well as from the accompanying Laplace score (U = -19.72), 
that there has been steady reliability growth from the start of 
the study period until the end of the study period, with 
exception of last year. 
In general, the more frequently failing switches showed 
a serious reliability deterioration, for cause codes 2-15 over 
the study period as shown by the Laplace score (U = 40.17). 
For the switches that experienced 8 or more outages, we can 
see from Fig. 25 that the reliability seemed to improve from 
the start of the study period to year 11, and then it extremely 
deteriorated from year 12 until the end of the study period. 
 
 
Figure 24.  Failure trend for switches with ≤7 outages. 
 
 
Figure 25.  Failure trend for switches with≥ 8 outages. 
F. Manufacturers of More Frequently Failing Local 
Telecommunication Switches  
This section discusses the manufacturers of local 
switches or more outages 8 or more outages during the 14-
year study period. As the percentages of switches by 
manufacturer in the total switch population are unknown, 
this presentation in this ection is of interest, but not a 
conclusive assessment. 
The 156 unique switches that compose the 8 or more 
outages were classified among 25 different switch models, 
then aggregated into outages per manufacturer. There are 
indeed some switch models that account for the majority of 
outages. Here however, the 156 unique switches with 8 or 
more outages are classified among seven different switch 
manufacturers. The Northern Telecom switches are the ones 
that failed the most (about 38% of all outages) as shown in 
Table XII. The Automatic Electric and Ericsson switches 
come at the second place with roughly equal percentages of 
about 19% each. Lucent technology switches represent a 
very low percentage of the frequently out switches. 
Northern Telcom and Lucent switches are the most prolific 
in the switch population. The age of switches was not 
reported, however, we would expect older switches to have 
more random hardware outages. 

95
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
TABLE XII. 
SWITCH MANUFACTURER OUTAGE FREQUENCY FOR   
MORE FREQUENTLY FAILING SWITCHES   
Switch 
Manufacturer 
Outage Frequency 
(No. of Outages) 
No. Unique 
Switches 
Northern Telecom 
999 
38.1% 
57 
36.5% 
Automatic Electric 
505 
19.3% 
25 
16.0% 
Ericsson 
493 
18.8% 
35 
22.4% 
Stromberg Carlson 
264 
10.1% 
16 
10.3% 
Siemens 
193 
7.4% 
9 
5.8% 
Unknown 
130 
5.0% 
11 
7.1% 
Lucent 
Technologies 
39 
1.5% 
3 
1.9% 
Total 
2623 
100 % 
156 
100% 
 
 
VI. 
RESEARCH FINDINGS AND CONCLUSION  
A. Research Findings 
The research findings are presented for each research 
question, in the order presented earlier: 
1) What 
are 
the 
major 
causes 
of 
local 
telecommunication switch outages? 
In general, the major causes of local telecommunication 
switch outages between 1996 and 2009 were scheduled 
maintenance activities (i.e., scheduled outages) and 
hardware failures. When considering only failure induced 
outages, most local switch failures resulted from hardware 
failures, software design errors, acts of God and external 
power failures. 
2) Is the reliability of local switches improving, 
constant, or deteriorating? 
In general, the reliability of local switches has been 
improving over the study period. However, significant 
deterioration towards the end of the study period is very 
apparent. Also, when scheduled outages are discounted, the 
outage trend due to switch failures dramatically increased.  
3) Are there individual switches that experience 
outages/failures more so than others? 
Only 3% of all individual switches experiencing outages 
were responsible for 20% of all outages and 22% of the total 
downtime. 
These 
more 
frequently 
failing 
switches 
encountered eight or more outages each during the study 
period. 
4) Are 
there 
similarities/dissimilarities 
between 
switches failing more often and those that do not? 
a) Impact of Outages 
Over the 14 years study period, scheduled outages had 
little apparent impact. Three years in particular (2001, 2005, 
and 2007) account for the lion’s share of impact due to 
switch failures. The year 2001 corresponds to the 9-11 
disaster where five switches were out for over a month, 
while the years 2005 and 2007 correspond to heavy 
hurricane induced outages. Switches having 7 or less 
outages are several times larger than switches suffering 8 or 
more outages. The less frequently failing switches account 
for 3% of the switches, 25% of the outages, but only 7% of 
the lost line hours.  
b) Rural versus Urban 
In both categories, rural switches failed more often than 
urban switches. However, most of the outages in the 
category of less frequently failing switches occurred at the 
start of the study period; whereas, most of the outages in the 
category of more frequently failing switches occurred 
towards the end of the study period. Additionally, most of 
outages in both categories, no matter where the switches are 
located, resulted from causes other than scheduled 
maintenance activities. Interestingly, switches failing more 
often were about three times smaller than switches failing 
less often. About 52% of less frequently out switches were 
rural, while about 66% of more frequently failing switches 
were rural. 
c) Outage causes 
For all failing switches, random hardware accounts for 
33%, Design error 14% and Acts of God/Power outages 
20%. The less frequently failing switches suffered 
considerably more often from scheduled outages; whereas, 
the more frequently failing switches suffered considerably 
more often from hardware failures and acts of God (i.e., 
natural disasters). The more frequently failing switches also 
often suffered from software design errors, external power 
failures, massive line outages, loss of facilities between host 
and remote, and other/unknown causes. There is an 
impression that the more frequently failing switches 
suffered from random hardware failures and external 
circumstances because they were not frequently maintained 
(i.e., no many scheduled outages); on the other hand, the 
less frequently failing switches suffered from human 
procedural errors maybe because they encountered a lot of 
scheduled outages. 
d) Outage duration 
The average switch outage duration was 3.3 hours. Even 
though the average duration per outage in both categories is 
approximately the same (considering all cause codes), the 
average impact (LLH) per outage is considerably different. 
This implies that the more frequently failing switches are 
smaller than the less frequently failing switches. Perhaps 
larger switches receive better maintenance responses, but 
this is not discernible from the data. 
e) Time of Day 
The differences between the mean times of day in both 
categories 
are 
statistically 
significant 
except 
when 

96
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
considering the scheduled outages only.. No additional 
insights as to why were discernible from the data. 
f) Day of Week 
The differences between the mean days of week in both 
categories are also statistically significant. No additional 
insights as to why were discernible from the data. 
g) Month of Year 
The differences between the mean months of year in both 
categories are not statistically significant. 
h) Outage/failure trends 
For all switches, failure trends exhibited a dramatic 
increase, when scheduled outages were discounted. The 
reliability of the less frequently failing switches has been 
steadily improving during the study period; whereas, the 
reliability of the more frequently failing switches has been 
deteriorating during the study period, especially towards the 
end. No additional insights as to why were discernible from 
the data. 
5) Are there switch models or manufacturers that 
account for outages/failures more so than others? 
In the category of more frequently failing switches, the 
top three manufacturers of switches that experienced more 
outages than others are the Northern Telecom, Automatic 
Electric and Ericsson, although the percentage of those 
switches in the total switch population is not known. 
However, Northern Telecom switches accounted for about 
38% of the more frequently failing switch outages while 
Lucent Technologies accounted for only 1.5%. 
B. Conclusions and Limitations 
This research reveals that there are significant 
reliability deficits in more frequently failing local 
telecommunication switches, especially towards the end of 
the study period (i.e., 2009). In fact, the last three years of 
the study period show significant reliability deterioration. 
This research also shows that there are significant 
differences in the causes of outages, impacts of outages, and 
switch characteristics between the two categories (the more 
frequently failing switches and the less frequently failing 
switches). In fact, only 3% of the failing switches accounts 
for 20% of all switch outages and for 7% of the lost line 
hours. Limitations include that the data is non-experimental, 
reported by a number of different carries who applied many 
cause codes. The consistency between reporting companies 
is unknown, as is the accuracy of the reports. Although 
scheduled outages reduced dramatically, it is not known if 
those outages induce subsequent failures, such as 
introducing software/hardware bugs. 
The limitations of this work include the fact that outage 
reports are made by different carriers, and by different 
people at individual carriers. The danger is lack of 
consistency in reporting. A counterpoint is that the reporting 
rules were constant over the study period. The fact the FCC 
has stopped collecting reports on switch outages since 2009 
is unfortunate, as future trends of local telecommunication 
switch reliability and survivability cannot be assessed from 
publically available data. However, the trends reveal 
opportunities for corrective/preventive actions on the part of 
switch manufacturers and service providers. 
REFERENCES 
[1] Snow, A. P., Shyirambere, A., Weckman G., Arauz, J., “A 
reliability 
and 
survivability 
analysis 
of 
US 
local 
telecommunication 
switches 
that 
sxperience 
frequent 
outages,” The Twelfth International Conference on Networks 
(ICN 2013), IARIA. 
[2] Leemis, L. M., Reliability: Probabilistic Models and 
Statistical Methods. Englewood Cliffs: Prentice-Hall, 1995. 
[3] Snow, A P.  “The Reliability of Telecommunication 
Switches,” 
Six 
International 
Conference 
on 
Telecommunications Systems: Modeling and Analysis, pp. 
288-295, March 1997.  
[4] Snow, A. P.,  “Internet implications of telephone access,”  
IEEE Computer, Vol. 32, No. 9, pp. 108-110, September 
1999. 
[5] Kuhn, R., “Sources of failure in the public switched telephone 
network,” IEEE Computer, Vol. 30, No. 4, April 1997. 
[6] Snow, A. P., “Assessing pain below a regulatory outage 
reporting threshold,” Telecommunications Policy, Vol.  28, 
pp. 523–536, Elsevier, 2004. 
[7] Snow, A. P., and Agarwal, S., “Towards an optimal network 
survivability threshold,” Ninth IFIP/IEEE International 
Symposium on Integrated Network Management, pp. 761 – 
774, 2005. 
[8] Davidson, J., Peters, J., Bhatia, M., Kalidindi, S., and 
Mukherjee, S., Voice over IP Fundamentals, Second Edition. 
Cisco Press, 2006. 
[9] Gillan, J., and Malfara, D. The Transition to an All-IP 
Network: A Primer on the Architectural Components of IP 
Interconnection. National Regulatory Research Institute, May 
2012. 
[10] FCC. Local Telephone Competition: Status as of December 
31, 2010. Industry Analysis and Technology Division, 
Wireline Competition Burea, October 2011. 
[11] Dryburgh, L., and Hewett, J., Signaling System No. 7 
(SS7/C7): Protocol, Architecture, and Services. Cisco,  2004. 
[12] Bedell, P., Cellular/PCS Management, New York: McGraw-
Hill, 2000. 
[13] Dialexia., White Paper: Softswitch Technology and the 
Migration to Full Convergence, 2005, November, Retrieved 
October 
2012, 
from 
Dialexia: 
http://www.dialexia.com/index.php/en/technology/white-
papers/86-white-paper-softswitch-technology-and-the-
migration-to-full-convergence 
[14] Alcatel-Lucent, STRATEGIC WHITE PAPER: How to 
Effectively Transition to VoIP and IMS - Big Bang or Phased 
Approach?, 2008, Retrieved October 2012, from Alcatel-
Lucent: 
http://www.webtorials.com/main/resource/papers/lucent/paper
92/VoIP-and-IMS.pdf 
[15] ITALTEL, and CISCO, NGN for PSTN Transformation, 
Tech. Overview, May 2009, Retrieved Oct.2012,from  
http://www.cisco.com/web/partners/pr67/pr36/docs/NPT_Cis
co_Italtel_technical_flyer_May09.pdf 
[16] Wang, P., and Coit, D. W. , “Repairable systems reliability 
tend tests and evaluation,” Reliability and Maintainability 
Symposium Proceedings Annual, pp. 416-421, January 2005. 
[17] Adams, T. C., THE LAPLACE TEST, Retrieved September 
2012, from National Aeronautics and Space Administration, 
KSC: 

97
International Journal on Advances in Telecommunications, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/telecommunications/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
http://kscsma.ksc.nasa.gov/Reliability/Documents/Laplace_T
est.pdf 
[18] Ellison, R., Fisher, D., Linger, R., Lipson, H., Longstaff, T., 
and Mead, N., Survivable Network Systems: An Emerging 
Discipline, Pittsburgh: Carnegie Mellon University, 1999. 
[19] Avizienis, A., Laprie, J.-C., Randell, B., and Landwehr, C., 
“Basic concepts and taxonomy of dependable and secure 
computing,” IEEE Transactions on Dependable and Secure 
Computing, Volume 1, Issue 1, pp. 11-33, Jan. 2004. 
[20] Federal Communications Commission, “About the FCC”, 
Retrieved February 2013, from Federal Communications 
Commission: http://transition.fcc.gov/aboutus.html 
[21] Federal Communications Commission, Automated Reporting 
Management Information System (ARMIS),  Retrieved 
October 2012, from Federal Communications Commission: 
http://transition.fcc.gov/wcb/armis/ 
[22] Timmins, P., Telecommunications Database. Retrieved from 
TelcoData.us: http://www.telcodata.us/data-downloads 2012. 
[23] Switch Search. (n.d.). Retrieved October 2012, from Local 
Calling Guide: http://www.localcallingguide.com/ 
[24] Johnson, B., and Christensen, L. Educational Research: 
Quantitative, Qualitative, and Mixed Approaches,  SAGE 
Publications, 2012. 
[25] National Oceanagraphic and Atmospheric Administration,  
NOAA Reviews Record-Setting 2005 Atlantic Hurricane 
Season: Active Hurricane Era Likely To Continue.2006, April 
13. Retrieved March 24, 2013, from NOAA News: 
http://www.noaanews.noaa.gov/stories2005/s2540.htm. 
 
 

