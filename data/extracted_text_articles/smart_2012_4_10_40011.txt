Ambient Assistance Using Mobile Agents 
 
 
Amine Arezki 
Université de Versailles Saint-Quentin-en-Yvelines 
Laboratoire d'Ingénierie des Systèmes de Versailles 
Versailles, France 
amine.arezki@lisv.uvsq.fr 
Eric Monacelli and Yasser Alayli 
Université de Versailles Saint-Quentin-en-Yvelines 
Laboratoire d'Ingénierie des Systèmes de Versailles 
Versailles, France 
eric.monacelli@lisv.uvsq.fr, yasser.alayli@lisv.uvsq.fr
 
 
Abstract—In this paper, a method is presented to assure 
ambient assistance in an urban environment, using a mobile 
agent. The goal is to anticipate the assistance if needed. 
Therefore, the robot has to understand the human behaviour 
and a person’s needs. We will determine how to focus on a 
moving subject and using interaction for confirmation, in 
order to provide an assistance if needed.  Therefore, the 
trajectory type concept is used to define the first step of 
analyses, which is called the approach step. Combining this 
step with the field information provided by the mobile agent 
will insure a certain type of assistance. In terms of observation, 
two different views are employed to detect assistance 
requirements, i.e., the Fix Intelligent Device and the Ambient 
Intelligent Devices, both communicating by Wi-Fi. The Fix 
Intelligent Device is a fix camera standing on a very top view 
allowing the detection of possible motions and classification of 
trajectories, using neural network. In our research, a Touch 
Ambient Intelligent Device is utilized, a mobile robot with 
three degrees of freedom, including a 3D camera (KinectTM) 
and a touch screen to interact with the subject. In this paper, 
the behavior of the mobile agent, receiving detected 
trajectories emitted by the Fix Intelligent Device, is presented 
and it is shown that the human intervention is only needed in 
critical cases. 
Keywords-Human Activity Recognition; Ambient Assistance; 
Human Tracking; Robot Agent. 
I. 
 INTRODUCTION  
The actual increased longevity in Europe and other 
developed countries [1] results in a fast growing population 
of senior citizens. Based on estimations [2], one-ﬁfth of the 
elderly citizens will be 80 years and older by the middle of 
the century. Many elderly need nursing support to perform 
their normal daily activities, mostly due to the lack of 
strength or balance. Assistance robots can play an important 
role to reduce the workload of nurseries and increase the 
independence of elderly to perform their daily activities with 
little help from the people around them. At the same time, 
assistance robots developed for frail and disabled adults can 
be used for people with minor disabilities, as well.  
That is one of the reasons that assistance robots have 
become one of the most rapidly developing ﬁelds in robotics. 
Among the assistance devices, there are some providing 
cognitive assistance and some increasing the mobility of the 
user. 
A simple type of ambient assistance fully relying on 
human already exists. However, the disadvantages of this 
system are high costs, discontinuity and interpretation 
mistakes, such as animal detection, shadow, light, etc. A 
more complex type is using an automatic system (Vision, 
Sensors, etc.), employing the same disadvantages of 
interpretation mistakes eventually causing dangerous 
situations. In this work, the person’s needs are interpreted, 
in order to provide assistance. The assistance robots 
described by Annicchiarico et al. [3] and Nagai et al. [4] are 
examples that can be used in displacement and transfer 
between two seated positions, respectively. Both examples 
are of relatively simple structure providing only a single 
type of assistance. While Chugo et al. reported a robot 
providing assistance for standing, walking and seating [5], 
Méderic et al. described ambient assistance for walking and 
the sit-to-stand transfer [6]. Assistance devices offering 
multi-support are normally large and of relatively 
complicated structure. Multi-robot assistance architecture 
has been proposed for frail and disabled adults in our 
laboratory [7] [8]. 
Our proposed system is designed for public places like 
waiting halls of train stations or airports or private places 
like apartments or offices. 
The most challenging task is to understand the human 
behaviour, in order to understand a person’s needs. In our 
research work, this detection is done by fusion of two 
different views to provide better results by having a 
continuous and safe automatic system. A robotic agent is 
used to confirm unclear information, e.g., the doubt if the 
subject is a human, or only a shadow artefact movement. 
Therefore, our system proposes a solution to use humans 
only in critical cases. 
In this paper, details of our system architecture are given 
first by explaining the role of the Touch Ambient Intelligent 
Device (TAID), followed by trajectory detections done by 
the Fix Intelligent Device (FID). The results of the 
combination of these two views (TAID and FID) are 
described subsequently. The experimental part with the 
results completes this paper, followed by the conclusion of 
our research. 
89
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

II. 
AMBIENT ASSISTANCE USING MOBILE AGENT 
The architecture shown in Figure 1 allows to visualize 
simultaneously several movements of subjects, as well as to 
analyse their trajectories. 
The trajectory detection is done by image capture at first. 
The assisting robot TAID has been developed for this 
project to act on the field (hospital, station, airport, etc.), 
when needed, or to confirm information. 
 
Figure 1. General view of the concept. 
TAID is a robot with three degrees of freedom, allowing 
movement on X and Y axis, as well as rotation on Z axis 
(Figure 2). TAID is capable of lateral crablike movements, 
playing with direction and velocity of the wheel rotation. 
 
 
Figure 2. Degrees of freedom of TAID (on the left); Rotation axes of 
TAID’s actions (on the right). 
 
A robotic 3D sensor KinectTM of MicrosoftTM [9] has 
been embedded for simplification, allowing the detection 
and consequently the avoidance of obstacles, on top of the 
detection of subjects to determine their pose and to 
differentiate between human and animals. 
A. Robotic Assistance Description 
The communication between TAID and the subject is 
done by visual (Vi), tactile (Ta), vocal (Vo) or text 
interaction (Te). 
 
Examples of interaction: 
1. Subject: Making a hand signal → Vi. 
2. TAID: Approaching and asking “DO YOU NEED 
HELP? RAISE YOUR RIGHT HAND FOR YES 
AND YOUR LEFT HAND FOR NO!” → Te and 
Vo. 
3. Subject: Answering by giving the corresponding 
hand signal → Vi or by pressing the button YES → 
Ta. 
4. TAID: Asking “WHAT INFORMATION DO 
YOU NEED?” → Te and Vo. 
On the part of the robot, the question is always asked 
vocally, as well as by text using the robots graphic interface. 
The subject’s response is then analysed by reading its 
gesture: Right hand raised for YES and left hand raised for 
NO. In order to avoid any incomprehension, vocal 
recognition was evaded, as the system is generally used in a 
noisy environment. 
A quadrilateral transformation needs to be done 
primarily before a follow-up of any movement may start. 
The location of the camera has to permit the visualization of 
the platform to analyse. 
B.  Movement Detection 
There are several approaches in a flux video for 
movement detection, based either on comparison of the 
current image video with one of the previous images (1), 
most commonly used, or with the background (2). 
If a colour image is considered as example, a copy of the 
current image is done in grey scale, as well as of the 
previous image video. First of all, the region is determined 
by subtraction of the two different images. An image of 
white pixels is obtained at the place where the image I(tx) 
differs from the image I(tx-1). If the surface is bigger than 
the defined lower limit, a movement alert is obtained. The 
inconvenient of this approach lies in the velocity of the 
object: If the object moves at a slow rate, the comparison of 
the current image with the previous image may provide 
insufficient results for object detection. 
 
The comparison of the image I(t0) with the initial image 
I(tin) allows the contour detection of a subject in movement, 
independently of the movement velocity. The inconvenient 
of this approach lies in the disappearance of the object: If an 
object is present during the first image capture of the 
background, but absent in the second image capture, the 
object is still detected at this place. 
 
In our research work, a combination of these two 
approaches has been chosen, i.e., the comparison with the 
90
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

image I(tx-100). The advantage of this approach lies in the 
movement detection, even when the subject is moving at a 
slow rate and even when the reference image is still being 
up-dated. Thus, the problem of disappearing objects from 
the background is solved, as well as their permanent 
detection. 
C.  Robot Interaction 
In the following section, the different interactions of the 
robot are presented. 
1. Hand Signal 
The agent may capture a hand signal and hence, may 
directly orient itself towards the subject for interaction. The 
interaction 
is 
done 
by 
vocal 
and 
written 
questioning/answering. 
 
 
Figure 3. TAID‘s tactile interaction tool.  
 
The mobile agent is using a tactile interaction tool, as 
shown in Figure 3, simplifying the interaction even in 
crowded and noisy environments. 
2. Body Pose 
The assisting agent TAID may detect a sitting or an on 
the ground lying subject, employing the 3D camera 
KinectTM. 
The following two situations may occur, causing 
different reactions on the part of TAID : 
 
• The agent detects the subject’s falling down (subject 
standing and subsequently lying). In this case, the 
robot TAID will send an emergency alert. 
• The agent detects a lying subject without knowing if 
the subject fell down or lied down on purpose. In this 
case, the robot TAID will interact with the subject 
first, asking him to confirm the need of assistance. 
For safety reasons, the robot will send an emergency 
alert after a certain time if the subject is not 
responding.     
III. 
TRAJECTORY PATTERN 
Five types of trajectories have been selected: Zigzag 
trajectory (A), circular trajectory (B), direct trajectory (C), 
back-and-forth trajectory (D) and random trajectory. 
A.  Zigzag Trajectory  
A zigzag trajectory (TZ) defines a hesitating or lost 
subject that does not know where to go. Hence, the goal is to 
assist the subject to regain orientation. 
 
 
Figure 4. Examples of zigzag trajectories (TZ). 
 
Different Tz are possible, as it can be seen in Figure 4. 
B. Circular Trajectory 
A circular trajectory (Tc) can be a trajectory of a lost or 
waiting subject in movement, requiring assistance. 
 
 
Figure 5. Examples of circular trajectories (Tc). 
 
Figure 5 is representing some examples of such possible Tc. 
C.  Direct Trajectory 
The direct trajectory (Td) describes a decided subject, 
knowing where to go. The different possible paths are 
shown in Figure 6. 
 
Figure 6. Examples of direct trajectories (Td). 
91
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

D. Back-and-Forth Trajectory 
A back-and-forth trajectory (Tb) helps to anticipate the 
agent’s position, in order to have a fast intervention if a 
zigzag or a circle trajectory is detected (Figure 7). 
 
Figure 7. Examples of back-and-forth trajectories (Tb). 
 
1) Trajectory detection 
The detection of the trajectory type has been done by the 
use of a neural network [10] with 1000 inputs for each type 
of trajectory (Figure 8), allowing their classification. 
 
 
Figure 8. Illustration of the neural network model. 
The fixed camera is connected to a computer via USB 
port, containing the algorithm of analysis and decision. The 
algorithm will transmit the command to the mobile robot 
TAID by Wi-Fi, using the User Datagram Protocol. 
 
Figure 9. Illustration of the agent’s communications. 
Each mobile agent possesses its own IP address, as it is 
shown in Figure 9.  
IV. 
SEMANTIC MODEL 
During the agent movement, the FID is orienting the 
agent towards the circulating subject to have the possibility 
of information capture, emitted from the subject. The agents 
are able to detect following situations: 
 
• A hand signal is coming from the subject [11]; this 
detection provokes TAID to approach the subject 
and to interact with the same. 
• The subject is sitting or lying on the ground [11]; 
this detection provokes an emergency alert 
provided by TAID. 
Figure 10 shows TAID‘s movement and its anticipation 
during analysis of the captured trajectory. 
 
Figure 10. Examples of TAID’s movement, based on the captured 
trajectory: (a-d) Back-and-forth trajectories; (e) Zigzag trajectory; (f) 
Circular trajectory. 
V. EXPERIMENTAL SECTION 
A volunteer was required to walk around in the test area, 
respecting 
certain 
trajectories 
and 
repeating 
these 
trajectories 9 times. Figure 11 illustrates the resulted 
trajectories. 
92
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

 
Figure 11. Illustration of the tested trajectories: (a) Direct trajectory ; (b) 
Zigzag trajectory; (c) Circular trajectory; (d) Back-and-forth trajectory. 
 
The algorithm applied in our system is depicted in 
Figure 12.  
Figure 12. Illustration of the system’s algorithm. 
 
A. Detection of a Direct Trajectory  
If the FID detects a direct trajectory Td, TAID will orient 
itself towards the subject in movement without approaching 
the same. 
The rotation angle α(TAID, moving subject) is 
calculated subtracting the β(TAID, X axis) angle and the 
γ(moving subject, X axis) angle. 
Figure 13. Illustration of the orientation of the robot TAID. 
 
In Figure 13, the orientation of the mobile agent is 
illustrated. The red arrow represents the motion direction of 
the subject in movement, which allows knowing if the 
subject is directing itself towards an area of danger. The 
blue arrow represents the rotation that the robot has to 
execute, in order to observe the subject in movement. 
 
Figure 14. Illustration of TAID verifying the hand sign and the body pose. 
 
TAID is always checking for hand signals asking for 
help, as it is shown in Figure 14. 
However, the robot may detect a falling subject pointed by 
the FID, employing the 3D camera KinectTM. 
B. Detection of a Nondirect Trajectory 
If the fixed camera detects a zigzag trajectory Tz, the 
robot will receive the information of intervention with the 
detected individual. The mobile agent TAID will interfere 
and progress towards the subject, in order to verify its 
condition and to interact with the same. After having 
moved, TAID will collect the following information: size, 
corpulence and pose of the subject (standing, sitting on the 
ground or lying down). 
In the case of a standing subject, the robot TAID will 
only ask if the subject needs help and if yes, what kind of 
help. The interaction is done on the level of the tactile 
screen of the robot. If a subject is sitting on the ground, 
TAID will transfer the information to the FID. The agent 
TAID will send an emergency alert, in order to get a human 
intervention, when a subject is lying in the ground. 
V. 
RESULTS 
In our experiments, each trajectory was tested 9 times, 
which allowed obtaining 91.67 % successful results. 
Examples of trajectory detection are given in Figure 15. 
93
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

Figure 15. Examples of trajectory detection. 
 
No Trajectory detected
Direct Trajectory
Back and forth Trajectory
Circular Trajectory
b
c
d
a
 
Figure 16. Illustration of trajectory detection and interaction of TAID with 
a person. 
 
In Figure 16-a, the FID is not detecting any trajectories 
because of insufficient points. 
In Figure 16-b, the FID is detecting a direct trajectory. 
Hence, the robot is waiting, in case the subject fells or 
makes a hand sign. 
In Figure 16-c, the FID has detected a back-and-forth 
trajectory. Subsequently, the robot TAID approaches the 
subject in movement. 
In Figure 16-d, the FID has detected a circular trajectory 
(same result if zigzag trajectory is detected). Consequently, 
the robot TAID is sent to interact with the subject, asking if 
help is needed. 
If the object in movement is an animal, the intervention 
will be cancelled, as TAID is able to distinguish between 
humans and animals.    
VI. CONCLUSION AND FUTURE WORK 
The results show that assistance on a subject by human 
intervention is only needed in the most critical cases. The 
analysis of trajectory behaviour helps to make a distinction 
between decided and hesitating subjects. This pre-analysis 
allows the mobile agent to approach the hesitating subject, 
in order to interact as fast as possible. Therefore, the 
definition of a trajectory can be changed, according to the 
environment. Having two different views helps the system 
to confirm unclear cases, such as detected animals or 
vehicles. 
The future work will consist in testing our system with 
more than one subject, as well as with more agents in the 
field. This will allow the development of the communication 
between the agents and the determination of how interaction 
priorities can be managed between the present agents. 
REFERENCES 
[1] 
R. Rosales, and S. Sclaroff. Trajectory Guided Tracking and 
Recognition of Actions. BU-CS-TR-99-002: Publisher 
Boston University Boston, MA, USA ©1999, 1999. 
http://dcommon.bu.edu/xmlui/bitstream/handle/2144/1779/1
999-002-trajector-guided-tracking-and-action-
recognition.pdf [retrieved: April, 2012]. 
[2] 
R. Bodor, B. Jackson, and N. Papanikolopoulos.Vision-
Based Human Tracking and Activity, in Proc. of the 11th 
Mediterranean Conf. on Control and Automation, Vol. 1 
(2003). 
http://mha.cs.umn.edu/Papers/Vision_Tracking_Recognition.
pdf [retrieved: April, 2012]. 
[3] 
R. Annicchiarico, C. Barrué, T. Benedico, F. Campana, U. 
Cortés, and A. Martínez-Velasco, The i-walker: an intelligent 
pedestrian mobility aid, in ECAI 2008 - 18th European 
Conference on Artificial Intelligence, Patras, Greece, July 
21-25, 2008, Proceedings, pp. 708–712, 2008. ISBN: 978-1-
58603-891-5. 
http://www.diagnostic-walker.es/pubs/iWalkerPatras.pdf 
[retrieved: April, 2012]. 
[4] 
K. Nagai, I. Nakanishi, and H. Hanafusa, Assistance of self-
transfer of patients using a power-assisting device, in IEEE 
Int. Conference on Robotics and Automation, p. 4008 
U4015, 2003. ISBN: 0-7803-7736-2. 
[5] 
D. Chugo, T. Asawa, T. Kitamura, S. Jia, and K. Takase, A 
motion control of a robotic walker for continuous assistance 
during standing, walking and seating operation, in IROS, pp. 
4487–4492, 
2009. 
ISBN: 
978-1-4244-3803-7. 
http://cdn.intechopen.com/pdfs/8624/InTech-
A_motion_control_of_a_robotic_walker_for_continuous_ass
94
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

istance_during_standing_walking_and_seating_operation.pdf 
[retrieved: April, 2012].  
[6] 
P. Méderic, V. Pasqui, F. Plumet, P. Bidaud, and J. Guinot, 
Design of a walking-aid and sit-to-stand transfer assisting 
device for elderly people, in 7th Int. Conference on Climbing 
on 
Walking 
Robots 
(CLAWARŠ04), 
2004. 
http://www.isir.upmc.fr/files/2004ACTI97.pdf 
[retrieved: 
April, 2012]. 
[7] 
C. Riman, E. Monacelli, I. Mougharbel, A. El Aij, Y. Alayli, 
"A Multi-Interface Platform System for Assistance and 
Evaluation of Disabled People", First Number Special Issue 
on Assistive Robotics, Applied Bionics and Biomechanics 
Journal, vol. 8, pp. 55-66, 2010. 
[8] 
E. Monacelli, F. Dupin, C. Dumas, P. Wagstaff, “A review 
of the current situation and some future developments to aid 
disabled and senior drivers in France“, Elsevier Masson 
BioMedical Engineering and Research, IRBM,  30, pp 234–
239, 2009. 
[9] 
Microsoft, Xbox 360 Kinect Sensor Manual, October 2010. 
http://www.xbox.com [retrieved: April, 2012]. 
[10] C. Schüldt, I. Laptev, and B. Caputo, Recognizing Human 
Actions: A Local SVM Approach, 17th International 
Conference on Pattern Recognition (ICPR'04) - Volume 3, 
2004. 
ftp://ftp.nada.kth.se/CVAP/users/laptev/icpr04actions.pdf 
[retrieved: April, 2012]. 
[11] M. Khalili, E. Monacelli, and Y. Hirata, Identiﬁcation of 
pedestrian characteristics for assistive systems, in IEEE/SICE 
International Symposium on System Integration (SII 2011), 
2011. 
95
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-225-7
SMART 2012 : The First International Conference on Smart Systems, Devices and Technologies

