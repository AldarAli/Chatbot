Using Environmental Contexts to Model Restrictions on Sensor Capabilities
Martin Richter, Christine Jakobs, Theresa Werner, Matthias Werner
Operating Systems Group
Chemnitz University of Technology
09111 Chemnitz, Germany
email: {martin.richter, christine.jakobs, theresa.werner, matthias.werner}@informatik.tu-chemnitz.de
Abstractâ€”Cyber-Physical Systems (CPS) incorporate the phys-
ical and digital worlds via sensors and actuators. The system
devices may be heterogeneous, distributed in space, and mobile.
This leads to new challenges in the design of applications that
should utilize the full potential of the system. As devices are
unreliable and may be arbitrarily moving away from locations
of interest, there is a continuous necessity to replace them
with alternatives during runtime. The handling of this task
by the application programmer is error-prone and complex
because the system might be heterogeneous and different sensors
and actuators possibly possess varying means to observe and
influence the environment. Therefore, a capability model has to be
employed on the operating system level which provides a holistic
view of the different devices. In this regard, an environmental
context model has to be supplied as the devicesâ€™ capabilities
may be restricted depending on the contexts they are located
in. This paper presents an environmental context model based
on an abstract sensor capability model. In conjunction, the
models provide a description of sensors concerning their ability
to observe properties of physical objects of interest within their
particular environmental contexts. The effect of contexts on the
spatial interpretability of sensor measurements is therefore made
explicit. Corresponding inferences are made with respect to the
localization of physical objects or phenomena of interest.
Keywordsâ€”cyber-physical systems; context awareness; hetero-
geneity; sensor virtualization.
I. INTRODUCTION
Through emerging trends like the Internet of Things, In-
dustry 4.0 or Smart Home, devices like sensors and actuators
are of increasing importance in our daily lives. This leads to
the emergence of Cyber-Physical Systems (CPS) that create
a link between the physical and digital worlds. Such systems
consist of devices that may be large in number, distributed in
space, heterogeneous, unreliable, and mobile. As the target of
CPS is the observation and influence of the physical world,
the respective applications are often bound to certain devices
situated in locations of interest. This severely impairs the
portability of applications between systems. Additionally, the
unpredictable motion and failure of sensors as well as actuators
become a challenge for properly executing the applications.
Furthermore, the systemâ€™s full potential may go unrecognized
as certain tasks may only be executable when taking the
aggregated capabilities of multiple devices into account instead
of considering them individually. The coordination of such
groups of devices heavily depends on their environmental
contexts as they may limit the devicesâ€™ abilities to cooperate
(e.g., robots being close to each other but being located on
opposite sides of a wall). The described challenges lead to
a demand for new device capability models that abstract
the heterogeneous hardware such that applications are made
portable and the full potential of CPS is utilized.
This paper provides an abstract sensor capability model. It
incorporates the influence of environmental contexts on the
ability of sensors to observe their surroundings. Additionally,
it allows to describe which combinations of the available
devices are suitable for performing a task with regard to
their respective locations and environmental contexts. This
enables the virtualization of hardware resources, i.e., the
transparent utilization of a common set of devices by multiple
applications.
Existing approaches to sensor virtualization (such as [1]
and [2]) focus on concurrently executing multiple applications
on a fixed set of sensors. In our view, the dynamics of the
different devices have to be recognized. Due to the motion
and failure of sensors, it is necessary to alternately execute
applications on a changing set of devices such that they are
able to continuously observe and influence the environment
as desired. Our model enables the Operating System (OS)
to decide which sensor measurements are required for a
given task based on the programmerâ€™s task description, the
sensorsâ€™ capabilities, as well as the devicesâ€™ environmental
contexts. As the task description is detached from controlling
the sensors, the execution of an application on transparently
alternating sets of devices is accomplished. This leads to new
possibilities for the task planning of sensors. Their mobility
and heterogeneity can be exploited such that a leaving or
failing device may be replaced by one or possibly multiple
other devices that provide similar types of measurements.
In that way, the applicationâ€™s possibilities for sensing and
influencing the environment remain the same or may even be
enhanced. Therefore, it is possible to unlock the full potential
of CPS and to port applications to other systems that employ
unrelated devices.
As a running example, we use a street surveillance system
near a highway. Its goal is the detection of environmental
hazards close to the road, such as wildfires. It consists of
immobile cameras on the roadside as well as cars driving down
the street which incorporate a dashboard camera as well as
two temperature sensors for measuring interior and exterior
temperatures respectively. The back seat windows of the cars
shall be tinted such that cameras from the outside are not able
to observe the back seat area of the car. The front seats of the
car are observable through untinted windows. We assume that
all sensors provide information on their respective locations
and are able to communicate wirelessly.
7
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-106-0
UBICOMM 2023 : The Seventeenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

The article is further structured as follows. Section II
depicts the concept for an abstract sensor and environmental
context model. Section III discusses existing approaches for
virtualizing sensors and compares them to our model with
respect to modeling the influence of environmental contexts on
the capabilities of devices. Section IV presents the conclusion
and possible directions for future work.
II. CONCEPT
The following sections present the programming model on
which our sensor capability model is based, followed by the
capability model, as well as the environmental context model.
A. Programming Model
The presented capability model is created in the context of
the programming model introduced in [3]. The programming
model provides sensor and actuator virtualization, as well as
transparency with respect to distribution, location, and motion
at the application level while allowing location- and motion-
aware control of devices at the OS level. This is achieved
by reversing the view of the CPS developer such that he
or she takes a physical perspective. In his application, the
programmer describes the properties of a physical object
and its behavior depending on internal dynamics as well as
potential actuator actions. The set of properties and their
values form the state of the object, which may change over
time. Additionally, the developer specifies a target state for
the object via constraints. These constraints are then solved
based on the state and behavioral descriptions of the object.
The capability and context model we propose in this work
is used to determine whether the available sensor capabilities
with respect to the sensor contexts are sufficient for observing
the object of interest and if so, in which locations it is present.
As mentioned before, the programmer provides a property
description for the physical object of interest. For the object
ğ‘œ it takes the form of a state description vector Â®ğ‘§ğ‘œ.
Â®ğ‘§ğ‘œ =
(ğœ1, ğ‘Ÿ1), . . . , (ğœğ‘›, ğ‘Ÿğ‘›)ğ‘‡
(1)
Each element of the vector consists of a tuple (ğœğ‘–, ğ‘Ÿğ‘–). The first
element denotes an object property type ğœğ‘– (e.g., color, shape,
or temperature), which represents the domain of possible
values for the corresponding property. The second element is a
rule for specifying a range of values for the property (e.g., for
a fire the color has to be red or yellow, and the temperature has
to exceed 300 degrees Celsius). Each rule represents a logic
formula that evaluates either to true or false for a given
location based on the available sensor measurements. If all the
rules for an object evaluate to true at a given location, the
object is present there, otherwise it is assumed not to be.
B. Sensor Model
The provided programming model allows the developer
to exclusively focus on the creation of the state description
vector Â®ğ‘§ğ‘œ. The OS is responsible for utilizing the required
sensors transparently. To achieve this, it requires knowledge
of the sensorsâ€™ capabilities as each of them may have different
measurands and may observe varying physical phenomena.
The following paragraphs provide an abstract model for the
capabilities of a sensor.
A possibly mobile sensor at location Â®ğ‘¥ = Â®ğ‘¥(ğ‘¡) observes a
physical quantity ğ‘ (e.g., electromagnetic radiation or tem-
perature). The measurement of the physical quantity is then
transformed by the sensor into a digital signal ğ‘£ = ğ‘£(ğ‘¡) (e.g., an
array of pixels) through a measuring process ğœ‡. The resulting
signal is interpretable for different locations in space ğ‘‹ âŠ† ğ‘‹Î£,
where ğ‘‹Î£ denotes all locations relevant to the system. The set
of interpretable locations depends on the sensorâ€™s location Â®ğ‘¥
and possibly other sensor-specific parameters Â®ğ‘ = Â®ğ‘(ğ‘¡), which
may also change over time (i.e., ğ‘‹ = ğ‘‹(Â®ğ‘¥, Â®ğ‘)). In conclusion,
a sensor ğ‘  is characterized by the following six-tuple.
ğ‘  = (ğ‘, Â®ğ‘¥, ğ‘£, ğœ‡, Â®ğ‘, ğ‘‹(Â®ğ‘¥, Â®ğ‘))
(2)
To select sensors for given tasks efficiently it is necessary
to group them into classes. The knowledge about the class
of a sensor allows the OS to decide how to interpret the
outputs of sensors (i.e., which actions to perform on them).
It can then transform the results into instantiated physical
object properties (e.g., shape or form). Sensors within a class
measure the same physical quantity and their results can be
utilized similarly. Therefore, the class of a sensor depends on
its observed quantity ğ‘ and its measurement process ğœ‡. We
denote a sensor class ğ›¾ as a boolean function that returns true
for a sensor ğ‘  ğ‘— belonging to the class and false otherwise.
ğ›¾ğ‘–(ğ‘  ğ‘—) =
(
ğ‘¡ğ‘Ÿğ‘¢ğ‘’, sensor ğ‘  ğ‘— belongs to class ğ›¾ğ‘–
ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’, otherwise
(3)
For example, digital dashboard cameras measure electro-
magnetic radiation within a certain wavelength and transform
it into an array of pixels through their measurement process.
On these arrays, methods like image recognition can be
performed to extract desired object properties such as shape
or color. The set of interpretable locations for a camera is
described by the following formula for a cone, where Â®ğ‘¥ = Â®ğ‘¥(ğ‘¡)
denotes the location of the sensor at time point ğ‘¡, Â®ğ‘¢ is the
location vector for which the equation has to be solved, and
the parameter vector Â®ğ‘ consists of the orientation of the camera
Â®ğ‘œ = Â®ğ‘œ(ğ‘¡) as well as its viewing angle ğœ™.
ğ‘‹

Â®ğ‘¥,
Â®ğ‘œ
ğœ™âŠ¤
=

Â®ğ‘¢ âˆˆ ğ‘‹Î£ : (Â®ğ‘¢ âˆ’ Â®ğ‘¥) Â· Â®ğ‘œ
|Â®ğ‘¢ âˆ’ Â®ğ‘¥|| Â®ğ‘œ| â‰¤ cos ğœ™

(4)
This equation could also be further constrained by considering
the maximum range of the camera. A brightness sensor, in
contrast, also measures electromagnetic radiation but through
its measurement process its output can not be utilized for
similar methods as the camera. Additionally, its measurement
is solely interpretable for a small radius around its location.
C. Sensor Identification
Depending on the object description presented in Sec-
tion II-A, multiple different sensors may have to be utilized to
gather data on an object of interest. The information on which
methods may be used to extract object properties from sensor
8
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-106-0
UBICOMM 2023 : The Seventeenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

outputs and which classes of sensors are required to utilize
them is managed in a dictionary D. The dictionary takes as
inputs:
1) the set of all sensors ğ‘†(ğ‘¡), which may change over time
due to motion or failure,
2) the set of all available methods ğ‘€, which may be applied
to different classes of sensors, and
3) the type ğœ of the physical property ğ‘§ğ‘–.
As a result of these inputs, the dictionary returns a vector of
different methods Â®ğ‘š. They can be applied to the outputs of
sensors of the corresponding classes to calculate a value for
the physical property.
D(ğ‘†(ğ‘¡), ğ‘€, ğœ) = Â®ğ‘š
(5)
For example, for determining the shape of an object image
recognition methods may be used on the outputs of multiple
cameras or sophisticated laser scanning systems may be uti-
lized.
Each method ğ‘š ğ‘— in
Â®ğ‘š is a tuple consisting of a set
of sensor classes Î“ğœ
ğ‘— and a function ğœ“ ğ‘— for calculating the
physical propertyâ€™s value from the outputs of sensors of the
corresponding classes.
ğ‘š ğ‘— = (Î“ğœ
ğ‘— , ğœ“ ğ‘—)
(6)
For each of the sensor classes in Î“ğœ
ğ‘— an individual sensor has
to be chosen for providing the inputs to the corresponding
calculation function ğœ“ ğ‘—. All possible sets of input sensors are
determined by a function ğ‘”. It maps the currently available
sensors ğ‘†(ğ‘¡) and the required classes of sensors Î“ğœ
ğ‘— to the
corresponding sets of sensors Î˜ ğ‘—, which are a subset of the
power set of ğ‘†(ğ‘¡).
ğ‘”(ğ‘†(ğ‘¡), Î“ğœ
ğ‘— ) = Î˜ğ‘—, Î˜ ğ‘— âŠ‚ P(ğ‘†(ğ‘¡))
(7)
The dimensions of each set of sensors ğœƒğ‘– âˆˆ Î˜ ğ‘— depend on how
many sensors are required by the method ğ‘š ğ‘—. For example,
for the calculation of the position of an object either one
distance sensor with known location and orientation is required
or several cameras may be utilized through the method of
triangulation.
When the function ğœ“ ğ‘— is applied to the set of chosen sensors
ğœƒğ‘– a value ğ‘£ of type ğœ is created.
ğœ“ ğ‘— (ğœƒ ğ‘—) = ğ‘£
(8)
This value ğ‘£ may correspond to the value of a physical object
property depending on whether the corresponding rule ğ‘Ÿ in the
state description vector is satisfied or not.
D. Environmental Context Model
Not all sensors that belong to the required classes for
performing a method can be utilized for the method. Different
sensors may be located in different environmental contexts
such that their measurements do not stand in any relationship
with each other. An example of this is an interpolation method
that can not be used simultaneously for temperature sensors
in a car and temperature sensors on the roadside.
An environmental context constrains the area for which the
sensorsâ€™ output can be interpreted. Each context ğ‘ is defined
with respect to its influence on a physical quantity ğ‘ (e.g., a
closed window influences the interpretability of temperature
sensors but not of cameras). A context possesses an anchor
location Â®ğ‘¥(ğ‘¡) which may change over time. This location is
continuously updated, e.g., by the use of a positioning sensor.
Depending on its anchor location the context additionally con-
sists of a set of surrounding locations ğ‘‹(Â®ğ‘¥(ğ‘¡)) which describe
the spatial extent of the context. A context might inhibit the
interpretation of sensor measurements of the corresponding
physical quantity ğ‘ in two ways:
1) It may impede the interpretation of a measurement from
outside the context for a location inside the context.
2) It may impede the interpretation of a sensor measurement
within the context for a location outside the context.
For example, a camera located inside a vehicle may be able
to observe locations outside through a tinted window but a
camera located outside the vehicle is not able to observe its
inside through the same window. This is accounted for by
introducing the boolean attributes ğ‘–ğ‘› and ğ‘œğ‘¢ğ‘¡ for each context.
If ğ‘–ğ‘› is false, the context constrains the interpretation of
sensor measurements from outside the context for locations
inside the context. If it is true an interpretation is possible.
The attribute ğ‘œğ‘¢ğ‘¡ describes whether the interpretation of a
measurement from inside a context is feasible for a location
outside in a similar fashion. Figure 1 depicts this situation.
In conclusion, a context ğ‘ related to a physical quantity ğ‘ is
defined by the following quintuplet.
ğ‘ = ğ‘(ğ‘¡) = {ğ‘, Â®ğ‘¥(ğ‘¡), ğ‘‹(Â®ğ‘¥(ğ‘¡)), ğ‘–ğ‘›, ğ‘œğ‘¢ğ‘¡} , ğ‘‹(Â®ğ‘¥(ğ‘¡)) âŠ† ğ‘‹Î£
(9)
The locations for different contexts may overlap arbitrarily.
Therefore, a sensor may be influenced by multiple contexts at
once. As the sensorâ€™s location and the contextsâ€™ locations may
change over time, the set of contexts that influence a sensor
may also change over time.
E. Impact of Contexts on Sensor Measurements
For a given sensor ğ‘ , the set of interpretable locations
ğ‘ .ğ‘‹ is created without taking its environment into account
(see Section II-B). The set of locations the sensor is actually
able to observe may be smaller because it is constrained
by the set of environmental contexts ğ¶. The measurement
of a sensor can be restricted by a context in two ways, as
described in Section II-D. Therefore, the locations ğ‘‹ğ‘ 
ğ‘œğ‘ğ‘  a
ğ‘1
ğ‘ 1
ğ‘œğ‘¢ğ‘¡?
Â®ğ‘¥1
(a) Observability of Â®ğ‘¥1 outside ğ‘1 with
ğ‘ 1 being located within ğ‘1 (ğ‘œğ‘¢ğ‘¡).
ğ‘2
ğ‘ 2
ğ‘–ğ‘›?
Â®ğ‘¥2
(b) Observability of Â®ğ‘¥2 within ğ‘2 with
ğ‘ 2 being located outside ğ‘2 (ğ‘–ğ‘›).
Figure 1. Depiction of the ğ‘–ğ‘› and ğ‘œğ‘¢ğ‘¡ attributes of the contexts ğ‘1 and ğ‘2
regarding the sensors ğ‘ 1 and ğ‘ 2 that may or may not measure the locations
Â®ğ‘¥1 and Â®ğ‘¥2 respectively.
9
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-106-0
UBICOMM 2023 : The Seventeenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

sensor observes from within its current contexts depend on two
sets of locations ğ‘‹ğ‘ 
Â¬ğ‘œğ‘¢ğ‘¡ and ğ‘‹ğ‘ 
Â¬ğ‘–ğ‘›. The set ğ‘‹ğ‘ 
Â¬ğ‘œğ‘¢ğ‘¡ denotes the
locations of contexts a sensor is located in which do not allow
the measurement of the sensor to be interpreted for locations
outside (i.e., Â¬ğ‘.ğ‘œğ‘¢ğ‘¡).
ğ‘‹ğ‘ 
Â¬ğ‘œğ‘¢ğ‘¡ =
Ã™
ğ‘âˆˆğ¶,ğ‘.ğ‘=ğ‘ .ğ‘,
Â¬ğ‘.ğ‘œğ‘¢ğ‘¡,ğ‘ . Â®ğ‘¥âˆˆğ‘.ğ‘‹
ğ‘.ğ‘‹
(10)
Thus, the locations ğ‘ .ğ‘‹ a sensor may be able to measure have
to be intersected with ğ‘‹ğ‘ 
Â¬ğ‘œğ‘¢ğ‘¡. This removes all locations from
ğ‘ .ğ‘‹ which lie outside of the corresponding contexts. From
the resulting set the locations ğ‘‹ğ‘ 
Â¬ğ‘–ğ‘› have to be removed. They
refer to contexts in which the sensor is not located and which
prohibit the interpretation of the sensorâ€™s output from outside
the context for a location within the context (i.e., Â¬ğ‘.ğ‘–ğ‘›).
ğ‘‹ğ‘ 
Â¬ğ‘–ğ‘› =
Ã˜
ğ‘âˆˆğ¶,ğ‘.ğ‘=ğ‘ .ğ‘,
Â¬ğ‘.ğ‘–ğ‘›,ğ‘ . Â®ğ‘¥âˆ‰ğ‘.ğ‘‹
ğ‘.ğ‘‹
(11)
In conclusion, the resulting set of locations ğ‘‹ğ‘ 
ğ‘œğ‘ğ‘  a sensor is
able to observe is defined by:
ğ‘‹ğ‘ 
ğ‘œğ‘ğ‘  = (ğ‘ .ğ‘‹ âˆ© ğ‘‹ğ‘ 
Â¬ğ‘œğ‘¢ğ‘¡) \ ğ‘‹ğ‘ 
Â¬ğ‘–ğ‘›.
(12)
Figure 2 shows an example for this. The camera sensor ğ‘  is
able to measure a cone-shaped area in front of it and is located
within the contexts ğ‘1 and ğ‘2. Therefore, the ğ‘œğ‘¢ğ‘¡ attributes
of these contexts are relevant for the spatial interpretation of
the output of ğ‘ . Only ğ‘2 constrains the sensorâ€™s measurement
locations in this regard. The sensor is not located within ğ‘3
and ğ‘4. Thus, their respective ğ‘–ğ‘› attributes are of relevance
as both contexts consist of locations that may intersect with
the sensorâ€™s observable locations ğ‘ .ğ‘‹. The locations ğ‘‹ğ‘ 
ğ‘œğ‘ğ‘  for
which its measurements are interpretable are represented by
diagonal stripes.
ğ‘2, ğ‘–ğ‘› = ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’, ğ‘œğ‘¢ğ‘¡ = ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’
ğ‘ 
ğ‘1
ğ‘œğ‘¢ğ‘¡ = ğ‘¡ğ‘Ÿğ‘¢ğ‘’
ğ‘–ğ‘› = ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’
ğ‘3
ğ‘–ğ‘› = ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’
ğ‘œğ‘¢ğ‘¡ = ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’
ğ‘4
ğ‘–ğ‘› = ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’
ğ‘œğ‘¢ğ‘¡ = ğ‘¡ğ‘Ÿğ‘¢ğ‘’
Figure 2. Depiction of observable locations ğ‘‹ğ‘ 
ğ‘œğ‘ğ‘  for a sensor ğ‘  with
respect to the contexts ğ‘1, ğ‘2, ğ‘3, and ğ‘4 that relate to the same physical
quantity ğ‘  is measuring.
The set of context regions ğ‘‹ğ‘ 
ğ‘ğ‘œğ‘› for a sensor ğ‘  is defined
similarly to ğ‘‹ğ‘ 
ğ‘œğ‘ğ‘ .
ğ‘‹ğ‘ 
ğ‘ğ‘œğ‘› = (ğ‘‹Î£ âˆ© ğ‘‹ğ‘ 
Â¬ğ‘œğ‘¢ğ‘¡) \ ğ‘‹ğ‘ 
Â¬ğ‘–ğ‘›
(13)
It denotes the entirety of the sensorâ€™s context without taking
its observable locations ğ‘ .ğ‘‹ into account. This set allows to
analyze whether the measurements of different sensors stand
in relation to each other, i.e., whether their context regions
overlap.
The set ğ‘‹ ğœƒğ‘šğ‘–
ğ‘œğ‘ğ‘  denotes the sets of observable locations for a
chosen set of sensors ğœƒğ‘šğ‘– âˆˆ Î˜ğ‘– utilized by method ğ‘šğ‘–.
ğ‘‹ ğœƒğ‘šğ‘–
ğ‘œğ‘ğ‘  = {ğ‘  âˆˆ ğœƒğ‘šğ‘– : ğ‘‹ğ‘ 
ğ‘œğ‘ğ‘ }
(14)
The set ğ‘‹ ğœƒğ‘šğ‘–
ğ‘ğ‘œğ‘› depicts the sets of context regions for a chosen
set of sensors similarly to ğ‘‹ ğœƒğ‘šğ‘–
ğ‘œğ‘ğ‘  .
ğ‘‹ ğœƒğ‘šğ‘–
ğ‘ğ‘œğ‘› = {ğ‘  âˆˆ ğœƒğ‘šğ‘– : ğ‘‹ğ‘ 
ğ‘ğ‘œğ‘›}
(15)
The scope Sğ‘šğ‘– for the result of a method ğ‘šğ‘– is determined by
a function ğ›¼.
Sğ‘šğ‘– = ğ›¼(ğ‘‹
ğœƒğ‘šğ‘–
ğ‘œğ‘ğ‘  , ğ‘‹ ğœƒ ğœƒğ‘šğ‘–
ğ‘ğ‘œğ‘› , ğœ“)
(16)
The scope describes which locations in the system can be
covered by the corresponding method by utilizing varying sets
of sensors. If the scope is an empty set, the method is not
applicable to the available sensors in the given contexts. It
depends on the following three parameters:
1) the different sets of observable locations ğ‘‹ ğœƒğ‘šğ‘–
ğ‘œğ‘ğ‘  of the
utilized sensors as they determine for which locations
valid sensor outputs are available,
2) the different sets of context regions ğ‘‹ ğœƒğ‘šğ‘–
ğ‘ğ‘œğ‘› of the utilized
sensors because they describe which sensor measure-
ments stand in relation to each other and may be used
for methods like interpolation for example, and
3) the function ğœ“ for calculating the result of the method as
varying calculation functions may lead to different results,
e.g., interpolation between temperature sensors increases
the scope of a method and triangulation between cameras
reduces it.
F. Object Identification
As described in Section II-A, a physical object property
ğ‘§ is computable by a set of different methods. Each of
these methods provides a result for a given scope based on
which sensors are chosen for its execution. Depending on the
programmerâ€™s target, it may be feasible to utilize multiple
different methods to increase the area in which an objectâ€™s
property can be observed. For a chosen set of methods Â®ğ‘šğœ
for determining the physical object property ğ‘§ of type ğœ, ğ‘§ is
observable in the set of locations ğ‘‹ğ‘§ which is a union of the
utilized methodsâ€™ scopes.
ğ‘‹ğ‘§ =
Ã˜
ğ‘šğ‘– âˆˆ Â®ğ‘šğœ
Sğ‘šğ‘–
(17)
The physical object ğ‘œ (i.e., all its properties) is observable at
the intersection of the observed locations of all its properties.
ğ‘‹ğ‘œ
ğ‘œğ‘ğ‘  =
Ã™
ğ‘§ğ‘– âˆˆÂ®ğ‘§
ğ‘‹ğ‘§ğ‘–
(18)
The actual spatial scope ğ‘‹ğ‘œ of the object is determined by the
rules ğ‘Ÿğ‘– provided by the state description Â®ğ‘§ğ‘œ (see Section II-A).
ğ‘‹ğ‘œ =
Ã™
(ğœğ‘–,ğ‘Ÿğ‘–)âˆˆÂ®ğ‘§ğ‘œ, Â®ğ‘š=ğ·(ğ‘†,ğ‘€,ğœğ‘–),
ğ‘šğ‘— âˆˆ Â®ğ‘š,ğ‘£=ğœ“ğ‘šğ‘— (ğœƒğ‘šğ‘— ),
ğ‘Ÿğ‘– (ğ‘£)=ğ‘¡ğ‘Ÿğ‘¢ğ‘’
ğ‘‹ğ‘§ğ‘–
(19)
10
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-106-0
UBICOMM 2023 : The Seventeenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

The scope of an object may encompass multiple regions of
space which are detached from each other (e.g., multiple fires
being localized by the street surveillance system). They are
interpreted as distinct instances ğ‘œğ‘– of the same object type
with a set of multiple unconnected regions ğ‘‹ğ‘œğ‘– âŠ† ğ‘‹ğ‘œ as a
result.
For each of the objects ğ‘œğ‘– resulting from this interpretation,
an instance vector Â®ğ‘£ is calculated for the corresponding scope
ğ‘‹ğ‘œğ‘– by applying the according methods, i.e., applying their
calculation functions ğ‘š ğ‘—.ğœ“ to a chosen set of sensors ğœƒğ‘šğ‘—.
Â®ğ‘£ğ‘‹ğ‘œğ‘– (ğ‘¡, Â®ğ‘¥) = Â®ğœ“( Â®ğœƒ) =
ï£®ï£¯ï£¯ï£¯ï£¯ï£°
ğ‘š1.ğœ“(ğœƒğ‘š1)
. . .
ğ‘šğ‘.ğœ“(ğœƒğ‘šğ‘)
ï£¹ï£ºï£ºï£ºï£ºï£»
, Â®ğ‘¥ âˆˆ ğ‘‹ğ‘œğ‘–
(20)
Each element ğ‘£ğ‘– of this vector provides a value of type ğœğ‘– for
the corresponding element ğ‘§ğ‘– of the state vector in the object
description of an object instance ğ‘œğ‘– in the scope ğ‘‹ğ‘œğ‘–. The
property value may also change within the scope over time
and space. Therefore, it depends on time ğ‘¡ as well as space Â®ğ‘¥.
This vector allows monitoring the state of the physical object
of interest and changes in its location within its observable
scope.
III. RELATED WORK AND DISCUSSION
This section discusses related work on incorporating envi-
ronmental contexts into sensor capability models. Thereafter,
the presented approaches are compared to our model.
A. Related Work
As mentioned in [4], the current state of the art is to
utilize ontologies for describing the abilities of a sensor and
the environmental context it observes (i.e., what it is mea-
suring). In [5] and [6], different ontologies for the semantic
specification of sensors are surveyed. They allow interpreting
the sensor measurements with respect to their measurands
and in which contexts their observations are made. These
approaches are static in the sense that the measurands and
environmental contexts are directly bound to sensors which
does not take the motion of sensors into account. Additionally,
the influence of contexts on the sensor capabilities is not
made clear as the corresponding measurements are tagged but
no adjustments to the specifications of a sensorâ€™s capabilities
are made. Therefore, the developer has to manually infer the
capabilities of the sensors depending on which contexts they
are located in. Potential operating systems employing these
techniques would therefore be obstructed in their ability to
dynamically schedule devices suitably for the tasks at hand.
In [1], an approach to sensor virtualization is described
which allows a set of applications to concurrently utilize
a common set of sensors. The environmental contexts and
capabilities of sensors are not taken into account in this model.
Therefore, the choice of which devices to utilize during the
execution of the application still lies with the programmer.
This introduces room for errors as some sensors may be
located in contexts that do not allow to measure the desired
environmental entities.
In [7], a sensor virtualization method is presented. It allows
the developer to declaratively specify a virtual sensor with
respect to which behavior it has to implement. Based on
their capabilities, changing physical devices are chosen during
runtime for the execution of the applicationâ€™s tasks. This
approach allows utilizing possibly changing sets of sensors
and also accounts for the motion of devices. The contexts of
sensors are not taken into consideration. Thus, an improper
selection of devices may lead to an application utilizing data
that does not suit the context for which it was developed.
In [2], virtual sensors are created such that each one wraps
a physical device. A virtual sensor provides a service that can
then be used by multiple different applications. Therefore, the
sensors are bound to their respective applications such that
taking a changing set of devices transparently into account is
not feasible. Additionally, contexts are not considered which
leads to similar challenges as described above.
In [8], an ontology is provided which allows the developer
to create individual capability models for each sensor. These
models are utilized to generate code for the distinct devices
which makes their functionalities available to the system. This
approach is most suited for systems with a static set of sensors
as individual capability models have to be created and the
corresponding code needs to be generated for each sensor.
Transparently scheduling a changing set of devices is therefore
only feasible in systems with an a priori known set of sensors.
The importance of taking the sensor contexts into account is
mentioned but not further elaborated on.
In [9], an abstraction for sensors and actuators is presented,
called resource, which allows the programmer to declarative
describe which devices are required by his application. If a
device is not available, a new resource may be created by
coupling a set of different devices which in combination are
able to perform similar actions as the unavailable resource.
Therefore, a transparent concurrent utilization of different
sensors based on the task at hand is possible. The contexts
of the devices are taken into account for the querying process
but they are statically bound to sensors and actuators. Thus, the
mobility of devices (i.e., changing contexts) is not considered
which may lead to an erroneous allocation of mobile sensors
or actuators depending on their change of position.
In [10] and [11], two approaches are presented which inte-
grate the notion of contexts into the programming model. The
developer binds parts of his application to different contexts
based on which devices are available within them. Their
models allow to transparently utilize different sets of sensors
or actuators based on where they are located. Neither sensor
capabilities nor how their environmental contexts influence
them are taken into account, which leads to similar drawbacks
as described above.
In [12]â€“[14], varying approaches for modeling contexts
of sensor systems are surveyed. The examined propositionsâ€™
primary focus lies on annotating sensor data such that they
can be interpreted with respect to their environment. None
of these approaches describe the influence of contexts on the
capabilities of sensors. Rather, they enrich the sensor data with
11
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-106-0
UBICOMM 2023 : The Seventeenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

context information such that the programmer has to make the
correct choice of which contexts are of relevance and which
sensor measurements have to be gathered within them. This
introduces room for errors as the effect of contexts on the
capabilities of sensors is not handled explicitly. Additionally,
the developer may not be able to utilize all devices as a sensor
located within a given context may still be able to provide
measurements for locations outside of a context.
In [15], an approach to defining contexts onthologically
is presented. The focus lies on documenting the impact of
different contexts (i.e., developmental, behavioral, structural,
and functional contexts) on the development of the CPS. This
allows the programmers to consider challenges emerging from
changes of the systemâ€™s contexts during runtime. While the
presented model is aiding developers in designing the system,
it does not provide runtime support for transparently managing
devices based on the context descriptions.
B. Discussion
None of the presented approaches discuss the influence of
environmental contexts on the capabilities of sensors. There-
fore, performing tasks like sensor fusion on measurements that
may not relate to each other due to environmental constraints
may create erroneous results. The virtualization of devices is
affected similarly. The replacement of a failed sensor by one
or more other devices is only feasible if their measurements
are related.
Our model describes a solution to this challenge by taking
the influence of environmental contexts into account. They
are defined as regions in space that restrict the interpretable
locations of sensor measurements. This allows to decide which
measurements are related to each other. A context-aware
virtualization of the devices is therefore enabled with respect
to which devices possess the capabilities to perform a task.
For performance reasons, it may be required to restrain the
definition of contexts in our model to discrete sets of locations
in the form of predefined geometric shapes instead of arbitrary
continuous regions in space (as in [11] for example). This
allows the efficient computation of the required operations on
the different sets of locations as described in Section II-D. The
approaches discussed in [7], [10], and [11] provide performant
implementations which enable sensor virtualization without
taking contexts into account. Therefore, they are suitable as
a starting point for implementing our model.
IV. CONCLUSION AND FUTURE WORK
This paper presents a model for describing the influence of
environmental contexts on the capabilities of sensors. This is
necessary since a sensorâ€™s ability to observe its environment
is strongly impacted by its surroundings. Contexts are defined
as sets of locations such that they describe regions in space.
They are viewed as constraints on the ability to interpret
the measurements of a sensor for a given location. These
constraints are effective at the edges of contexts, such that
they influence the observable locations of sensors within or
outside of the context. Our sensor capability and context model
makes the influence of environmental contexts on the available
sensors explicit.
The model is presented in the context of identifying physical
objects based on an object description provided by the pro-
grammer. It allows to reason about the influence of choosing
different sets of sensors on the coverage of the system space
with respect to the object properties to be measured. This
allows the OS to choose devices according to their ability
to observe locations of interest. Therefore, a virtualization of
sensors is achieved. This allows to transparently execute an
application on alternating sets of heterogeneous devices while
ensuring that locations of interest are observed.
For future work, we intend to enrich the model with further
metrics for allowing an optimal choice of sensors based on
their capabilities. The adaptation of contexts via exploration
during runtime based on available sensor measurements is also
a future goal. Additionally, an implementation and integration
of the model into the presented programming model is in-
tended.
REFERENCES
[1] C. Mouradian et al., â€œNetwork functions virtualization architecture for
gateways for virtualized wireless sensor and actuator networks,â€ IEEE
Network, vol. 30, no. 3, pp. 72â€“80, 2016.
[2] P. Evensen and H. Meling, â€œSensewrap: A service oriented middle-
ware with sensor virtualization and self-configuration,â€ in International
Conference on Intelligent Sensors, Sensor Networks and Information
Processing (ISSNIP), 2009, pp. 261â€“266.
[3] M. Richter, T. Werner, and M. Werner, â€œA Programming Model for
Heterogeneous CPS from the Physical Point of View,â€ in The Sixteenth
International Conference on Mobile Ubiquitous Computing, Systems,
Services and Technologies, 2022, pp. 1â€“6.
[4] N. Sahlab, N. Jazdi, and M. Weyrich, â€œDynamic context modeling for
cyber-physical systems applied to a pill dispenser,â€ in 25th IEEE Inter-
national Conference on Emerging Technologies and Factory Automation
(ETFA), 2020, pp. 1435â€“1438.
[5] G. M. Honti and J. Abonyi, â€œA review of semantic sensor technologies
in internet of things architectures,â€ in Complexity, 2019, pp. 1â€“21.
[6] M. Compton, C. A. Henson, L. Lefort, H. Neuhaus, and A. P. Sheth,
â€œA survey of the semantic specification of sensors,â€ in CEUR Workshop
Proceedings, 2009, pp. 17â€“32.
[7] S. Kabadayi, A. Pridgen, and C. Julien, â€œVirtual sensors: abstracting
data from physical sensors,â€ in International Symposium on a World of
Wireless, Mobile and Multimedia Networks, 2006, pp. 586â€“592.
[8] O. Lemaire, K. Ohba, and S. Hirai, â€œDynamic integration of ubiquitous
robotic systems through capability model processing,â€ in SICE-ICASE
International Joint Conference, 2006, pp. 1207 â€“ 1211.
[9] V. Tsiatsis et al., â€œThe sensei real world internet architecture,â€ in
Towards the Future Internet: Emerging Trends from European Research,
2010, pp. 247â€“256.
[10] Y. Ni, U. Kremer, and L. Iftode, â€œSpatial views: Space-aware program-
ming for networks of embedded systems,â€ in Lang.s and Compilers for
Parallel Comput., 2004, pp. 258â€“272.
[11] C. Borcea, C. Intanagonwiwat, P. Kang, U. Kremer, and L. Iftode, â€œSpa-
tial programming using smart messages: design and implementation,â€ in
24th Int. Conf. on Distrib. Comput. Syst., 2004, pp. 690â€“699.
[12] C. Bettini et al., â€œA survey of context modelling and reasoning tech-
niques,â€ in Pervasive and Mobile Computing, 2010, pp. 161â€“180.
[13] C. Perera, A. Zaslavsky, P. Christen, and D. Georgakopoulos, â€œContext
aware computing for the internet of things: A survey,â€ in IEEE Com-
munications Surveys & Tutorials, 2014, pp. 414â€“454.
[14] T. Strang and C. Linnhoff-Popien, â€œA context modeling survey,â€ in First
International Workshop on Advanced Context Modelling, Reasoning And
Management at UbiComp, 2004, pp. 34â€“41.
[15] M. Daun and B. Tenbergen, â€œContext modeling for cyber-physical
systems,â€ Journal of Software: Evolution and Process, vol. 35, no. 7, p.
e2451, 2022.
12
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-106-0
UBICOMM 2023 : The Seventeenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

