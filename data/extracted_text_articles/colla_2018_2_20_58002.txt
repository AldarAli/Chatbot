A Scalable Vector Symbolic Architecture Approach for Decentralized Workﬂows
Chris Simpkin∗, Ian Taylor∗, Graham A. Bent†, Geeth de Mel† and Raghu K. Ganti‡
∗Cardiff University, UK
Email: {SimpkinC,Taylorij1}@cardiff.ac.uk
†IBM Research, UK
Email: {Gbent,geeth.demel}@uk.ibm.com
‡IBM TJ Watson Research Center, USA
Email: rganti@us.ibm.com
Abstract—Vectors Symbolic Architectures (VSAs) are distributed
representations that combine random patterns, representing
atomic symbols across a hyper-dimensional vector space, into
new symbolic vector representations that semantically represent
the component vectors and their relationships. In this paper, we
extend the VSA approach and apply it to decentralized workﬂows,
capable of executing distributed compute nodes and their inter-
dependencies. To achieve this goal, services must be discovered
and orchestrated in a decentralized way with the minimum
communication overhead whilst providing detailed information
about the workﬂow - tasks, dependencies, location, metadata,
and so on. To this end, we extended VSAs using a hierarchical
vector chunking scheme that enables semantic matching at each
level and provides scaling up to tens of thousands of services. We
then show how VSAs can be used to encode complex workﬂows
by building primitives that represent sequences (pipelines) and
then extend this to support full Directed Acyclic Graphs (DAGs)
and apply this to ﬁve well-known Pegasus scientiﬁc workﬂows to
demonstrate the approach.
Keywords–vector symbolic architectures; decentralized work-
ﬂows; semantics; associative memory models.
I.
INTRODUCTION
Workﬂows provide a robust means of describing applica-
tions consisting of control and data dependencies along with
the logical reasoning necessary for distributed execution. For
wired networks, there have been a wide variety of workﬂow
systems developed [1]–[10]. A scientiﬁc workﬂow is a set of
interrelated computational and data-handling tasks designed
to achieve a speciﬁc goal. It is often used to automate pro-
cesses, which are frequently executed, or to formalize and
standardize processes. The majority of existing workﬂows
rely on centralized management and therefore require a stable
endpoint in order to deploy such a manager. In more dynamic
settings, such as Mobile Ad Hoc Networks (MANETs) [11]
where more collaborative applications (e.g., multi-user chats,
or distributed analytics for coalitions [12], [13]) are needed, on
demand workﬂows that are capable of spontaneously discov-
ering multiple distributed services without central control are
essential. The resulting distributed pathways are complex, and
in some cases impossible to manage centrally because they are
based on localized decisions, and operate in extremely transient
environments.
In the current state-of-the-art, discovery of workﬂow steps
is not dynamic; the exact services to be used for each workﬂow
step must be speciﬁed in advance and, further, workﬂow or-
chestration is typically managed via a central point of control.
Macker and Taylor [14] provides a mechanism that is decen-
tralised, however, the speciﬁcation of each workﬂow step, ip-
address, connections and data, must be known in advance and
is passed around between services via a JSON data block.
Wittern et al. [15] present a graph based data model that
aims to capture relationships between services and their usage
detail for API ecosystems; the approach provides an interface
that enables consumers to search for required services and
service providers to obtain usage and compatibility statistics
between the services and those of other providers as well as
discovering new requirements. In contrast, we describe the use
of VSAs to enable individual services to be self-describing and
to learn contexts for which they are compatible. We describe a
decentralised, multicast, environment where individual services
can interrogate VSA encoded workﬂow messages, compute
their compatibility to participate in a particular workﬂow step
and be chosen if they are the best match. This is analogous
to a group of humans listening to various work requests and
deciding for themselves that they are capable and available to
do a piece of work, i.e., all humans can understand the work
request message and each human knows for themselves if they
are available and capable of doing the the work. Naturally,
more than one person may offer to do a particular job and we
describe mechanisms for negotiating who gets to do the work.
However, applying VSAs to workﬂows requires several ex-
tensions and our contributions to this area can be summarized
as follows:
• Scaling Through Chunking: To address scalability, we
extend VSAs using a hierarchical vector chunking scheme
that is capable of binding multiple levels of abstraction
(workﬂow and sub-workﬂows/branches) into single vec-
tor. This approach scales to tens of thousands of vectors
while maintaining semantic matching.
• Encoding Workﬂows: We employ our chunk encoding
scheme to encode and decode very large sequences of
services.
• Representing Workﬂows Primitives: We extend the
encoding scheme to support Directed Acyclic Graph
(DAG) workﬂows having one-to-many, many-to-many,
and many-to-one connections.
• Distributed Discovery and Orchestration: We show
how our VSA encoding scheme can be used for dis-
tributed discovery and orchestration of complex work-
ﬂows. Workﬂow vectors are multicast to the network and
participating services compute their own compatibility
and offer themselves up for participation in the workﬂow.
We show that the result provides several desirable features
and byproducts: it can encode workﬂows /sub-workﬂows that
can be unbound on-the-ﬂy and executed in a completely
decentralized way; associated metadata can also be embedded
21
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-645-3
COLLA 2018 : The Eighth International Conference on Advanced Collaborative Networks, Systems and Applications

into the vector, e.g., security, conﬁguration, etc.; the vector
representation is extremely compact and self-contained and
can be passed around using standard group transport protocols;
and semantic comparisons or searches are scoped within a
sub-group of services in a workﬂow, allowing scoped service
matchmaking. We, then, apply the implementation to several
Pegasus [16] workﬂows (Montage, CyberShake, Epigenomics,
Inspiral Analysis, and SIPHT) and analyze the output.
The rest of the paper is structured as follows. In the next
section, we provide some background into VSAs. In Section
III, we outline the contributions we have made to scale VSAs
and the extensions we applied for Workﬂows. In Section IV, we
show how the VSA approach is applied to Linear Workﬂows
and then to more complex DAG workﬂows. We discuss the
resulting architecture and implementation in Section V, apply it
to several Pegasus workﬂows in Section VI, and in Section VII,
we conclude.
II.
VECTOR SYMBOLIC ARCHITECTURE OVERVIEW
VSAs [17] are distributed representations that can be
considered to sit somewhere between pure connectionist ap-
proaches and classic symbolic approaches, in that they employ
’atomic’ symbols to build complex representations of objects,
like the classical symbolic approach to cognitive modeling and
artiﬁcial intelligence research. However, the atomic symbols
employed are random patterns of values spread over a hyper-
dimensional vector space of dimension N. VSA symbols
represent data and object features using random symbolic
component vectors and combine these into vectors that seman-
tically represent the component vectors and their relationships.
VSA vectors are, therefore, said to be semantically self-
describing [18]. Atomic vectors can be real valued like Plate’s
Holographic Reduced Representation (HRR) [19], typically
having dimension 512 ≤ N < 2048, or binary vectors, such as
Pentti Kanerva’s Binary Spatter Codes (BSC) [20], typically
having N ≥ 10, 000. For the work described here, we chose
to build off Kanerva’s BSCs, but most of the equations and
operations listed and discussed should also be compatible with
HRRs [21], too.
In BSCs, atomic symbols are assigned a random vector to
represent an entity and due to the very high dimensionality
employed, such atomic vector symbols are uncorrelated to
each other with a high probability [20]. Higher level/com-
plex concepts are built by combining atomic vectors using
a bundling or superposition operation. A key advantage of
this approach is that superposition involves no computation-
ally expensive iterative learning of weights like traditional
connectionist approaches [22]; rather, learning is achieved by
direct combination of sub-features. An additional advantage
is that VSA methods are completely deterministic and hence
analysable and explainable, unlike traditional connectionist
methods.
Superposition is the combination of sub-feature vectors
into a same sized compound vector such that each vector
element participates in the representation of many entities,
and each entity is represented collectively by many elements
[21]. Normalised Hamming Distance (HD) can be used to
probe such a vector for its sub-features without unpacking or
decoding the sub-features. If two high level concept vectors
contain a number of similar sub-features, such vectors are said
to be semantically similar, e.g., if we have three services:
Service1 = AudioINv + DeNoisev + Convolutionv + Classifyv
Service2 = AudioINv + DeNoisev + DFTv + Classifyv
Service3 = AudioINv + LowPassv + PowerSpecv + Classifyv
where ’+’ is the superposition or bundling operator; then,
comparing Service1 with Service2 will give a match since they
have 3 common sub-features. Also, Service1 and Service2 will
be more similar to each other than they are to Service3.
An issue arises, however, when using superposition to com-
pare compound vectors in this way because such compound
vectors behave as an unordered bag of features. Thus, if:
Service4 = AudioINv + DeNoisev + Classifyv + ShutDownLinev
then Service4 would be equally similar to Service1 as Service2,
despite having a different output step. In order to resolve such
issues VSAs employ a binding operator that allows feature
values such as DeNoise and Classify to be associated with a
particular ﬁeld name, or role. This is analogous to how variable
names are used in programming languages to associate values
with a particular property, e.g., speed=3.
Services must agree upon a common method to assign
atomic vectors for roles whereas feature vectors are usually
compound vectors built up from lower level compound vectors
and/or atomic vectors. When a role is bound to a value this
results in a role-ﬁller pair. Feature values such as DeNoise can
be detected or extracted from the role-ﬁller using an inverse
binding operator. Bitwise XOR is used as both binding and
unbinding with BSCs because it is its own inverse. In addition
it is commutative and distributive over superposition ( [20],
page 147). It is also lossless, which means that both roles and
ﬁllers can be retrieved from a role-ﬁller pair without any loss,
e.g., using ′.′ as the bitwise XOR operator; if Z = X.A then
X.Z = X.(X.A) = X.X.A = A, since X.X = 0, the zero
vector. Similarly A.Z = X. Due to the distributive property,
the same method can be used to test for sub-feature vectors
embedded in a compound vector as follows:
Z = X.A + Y .B
(1)
X.Z = X.(X.A + Y.B) = X.X.A + X.Y.B
(2)
X.Z = A + X.Y.B
(3)
Examination of (3) reveals that vector ‘A’ has been ex-
posed, thus, if we perform HD(X.Z, A) we will get a match.
The second term ′X.Y.B′ is considered noise because ′X.Y.B′
is not in our known ’vocabulary’ of features/symbols. When
a role and value (ﬁller) are bound together this is equivalent
to preforming a mapping or permutation of a vector value’s
elements within the hyper-dimensional space, so that the new
vector produced is uncorrelated to both the role and ﬁller
vectors. For example, if V = R.A and W = R.B then R, A
and B will have no similarity to V or W. However, comparing
V with W will produce the same match value to comparing
A with B. In other words, if A is closely similar to B then V
will be closely similar to W because binding preserves distance
within the hyper-space ( [20, page 147]).
Thus, binding with atomic role vectors can be used as a
method of hiding and separating values within a compound
vector while maintaining the comparability between vectors.
22
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-645-3
COLLA 2018 : The Eighth International Conference on Advanced Collaborative Networks, Systems and Applications

This important property can be used to encode position
and temporal information about sub-feature vectors within
a compound vector. It also explains why we can state that
′X.Y.B′ from (3) above, will not match to any known symbol,
however, note that we can get back to B from ′X.Y.B′; simply
perform the appropriate xor’s, B = ((X.Y.B).X).Y . We can
now rephrase our Service description to differentiate its sub-
features, i.e., we can reformulate Service1 to:
Service1 = Inputrv. AudioINv +Cleanuprv.DeNoisev
+ P rocessrv.Convolutionv + Outputrv.Classifyv
This clearly resolves the incorrect matching between Service1
and
Service2 with
Service4. To test if
Service1 uses
DeNoisev as its cleanup step we perform:
HD(xor(Cleanuprv, Service1), DeNoisev)
(4)
When using 10kbit vectors, if the result of (4) is less than
0.47 then the probability of DeNoisev being detected in error
is less than 1 in 109 ( [20, page 143]). If we have an audio
signal we want to classify, we might multicast a request vector
Z = Inputrv.AudioINv +Outputrv.Classifyv which would
cause listening services such as services 1, 2 and 3 to respond
or become activated. We could further query the responding
services to determine what type of cleanup and processing they
do as per (4).
III.
EXTENSIONS TO VSAS FOR WORKFLOWS
The number of detectable sub-features that can be superim-
posed into a single vector is of limited capacity, 89 vectors for
BSCs of dimension 10k [23], and this issue must be addressed
in order to encode large workﬂows. Chunking is a bundling
method that combines groups of vectors into a single com-
pound vector which is then used as base for further bundling
operations, recursively producing a hierarchical tree structure
where each node in the tree is a compound vector, as shown
in Figure 1. Various methods of recursive chunking have been
described [19]–[21], [23]. However, such methods suffer from
limitations when employed for multilevel recursion - some
lose their semantic matching ability if any single term differs,
others can not maintain separation of sub-features for higher
level compound vectors when lower level chunks contain the
same vectors. We addressed these issues and describe a novel
recursive encoding scheme that provides semantic matching at
each level by combining two different methods of permuting
vectors.
C
B1
B2
B3
B4
A1
A2
A3
A4
+
+
+
+
+
+
Figure 1. Workﬂow Chunk Tree, chunking proceeds from the bottom up.
In our scheme, the terminal nodes are worker services,
the higher level nodes are concepts used to apply grouping to
parts of workﬂow. The higher level nodes (known as ’clean-up
memory’ [19], [20], [23]) are still services but they simply pro-
vide a proxy to the services to be unbound and executed, and
thus are typically co-located with the ﬁrst service of the sub-
sequence, i.e., there is no network overhead. In a centralised
system, Clean-up memory is typically implemented as an
autoassociative memory. For our distributed workﬂow system,
clean-up memory is implemented by the services themselves
matching and resolving to their own vector representation.
Recchia et al., [24] point out that, for large random vectors,
any mapping that permutes the elements can be used as a
binding operator including cyclic-shift. The encoding scheme
shown in (5) employs both XOR and cyclic-shift binding to
enable recursive bindings capable of encoding many thousands
of sub-features even when there are repetitions and similarities
between sub-features:
Zx =
x
X
i=1
Zi
i.
i−1
Y
j=0
p0
j + StopV ec.
i
Y
j=0
p0
j
(5)
Omitting StopV ec for readability, this expands to,
Zx = p0
0.Z1
1 + p0
0.p0
1.Z2
2 + p0
0.p0
1.p0
2.Z3
3 + ...
(6)
where
• ‘.′ = XOR and ‘+′ = BitwiseMajority V ote.
• The exponentiation operator is redeﬁned to mean cyclic-
shift, +ve exponents mean Cshift right, −ve expo-
nents mean Cshift left.
• Zx is the next highest semantic chunk item containing
a superposition of x sub-feature vectors. Zx chunks can
be combined using (5) into higher level chunks, e.g., Zx
might be B1, the superposition of A1, A2, A3, ...
• Z1, Z2, Z3, ... are sub-features being combined for the in-
dividual nodes in Figure 1. Each ‘Z’ is itself a compound
vector representing a sub-workﬂow or a compound vector
description for an individual service step.
• p0, p1, p2, ... are a set of known atomic role vectors used
to deﬁne the current position/step in the workﬂow. The
reason multiple ‘p′ vectors are XOR’ed together to deﬁne
a single position within the workﬂow is to provide an
iterative method for ordered activation of workﬂow steps
during workﬂow execution, see (9).
• x is the, deﬁnable, ’chunk size’.
• StopV ec is a role vector that indicates to Zx that all sub-
feature/workﬂow steps have been executed.
A. Workﬂow execution
During workﬂow execution of a chunk tree similar to
Figure 1 and encoded using (5), control ﬁrst passes down
the chunk tree, i.e., from C → B1 → A1, before traversing
horizontally, A1 → A2 → A3 → A4 → B1 StopV ec. At this
point B1 ’sees’ its own StopV ec and employs (9) to traverse
horizontally at the next higher semantic level; see also ﬂow
arrows in Figure 2.
Referring to (6), an initiator or requester prepares the
workﬂow, Zx, for instantiation onto the network by performing
an unbind operation, using (9), thus exposing the ﬁrst workﬂow
step, Z1, as shown:
Z′
1 = (T + p0
0.Zx)-1
(7)
Z′
1 = p-1
0 .T -1 + Z0
1 + p-1
1 .Z1
2 + p-1
1 .p-1
2 .Z2
3+...
(8)
23
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-645-3
COLLA 2018 : The Eighth International Conference on Advanced Collaborative Networks, Systems and Applications

The T vector is a known atomic role vector used by Zx’s
children to calculate their position within the sub-workﬂow.
It is only bundled onto the workﬂow vector when an initiator
or higher level node is requesting execution of its own sub-
workﬂow, i.e., when traveling down the workﬂow.
In (8), note that all other Z vectors remain hidden because
they are still permuted. Thus, listening services can only match
to Z1. As control passes, horizontally, from Z1 → Z2 → Z3 ...
each active service uses the current permutation of the T
vector to calculate its zero based position ‘n’ within the
currently active parent chunk vector. It can then activate the
next workﬂow step in the chunk by repeating the unbind
operation, generalized as:
Z′
n+1 = (p−n
n
.Z′
n)
−1
(9)
Hence, Z′
2 = (p-1
1 .Z′
1)-1 = p-1
1 .p-2
0 .T -2 + p-1
1 .Z-1
1
+ Z0
2
+
p-2
2 .Z1
3 + . . . Thus, execution proceeds in a completely de-
centralized manner whereby each node is activated when its
preceding node, or parent, unbinds the currently active chunk
vector and multicasts it to the network.
B. Local Arbitration
A major advantage of the VSA approach is the ability
to ﬁnd semantic matches because each service can extend
beyond simple matches to include measures of real-time
compute utility as well as policy. For certain scenarios, such
as military coalition environments, there is a need to ensure
multiple copies of services are distributed throughout the
communications network. In order to decide which service is
invoked, we employ a process of local arbitration, which is
achieved as follows. Using terminology from (7) and (9), if
the currently active service is Z′
n, then before transmitting the
next service request, it enters match collecting mode in order
to arbitrate matches from all nodes that reply within a tunable
window of time. After the interval expires, the highest ranking
responder is selected and a continue message is broadcast
by Z′
n identifying the winner. Since all communications are
multicast, all services see all messages, and consequently the
winning service continues and losing services discontinue.
To reduce communication overhead further, matching services
delay their response by an interval inversely proportional to
their match value. Thus, better matches respond quicker. If
a service sees an equal or higher match value before it has
responded then it terminates without sending a reply.
C. Pre-provisoning and Learning to get ready
From (9) we see that each workﬂow step is exposed by
iterative application of ‘p’ vector permutations. Non-matching
services can use this method to peek a vector enabling an-
ticipatory behavior such as the pre-provisioning of a large
data-set or changing a device’s physical position, e.g., drones.
Obviously, services can peek multiple steps into the future and
could learn how early to start pre-provisioning. This ability
to anticipate could be used to perform more complex, on-
line, utility optimisation learning. For example, a drone, by
monitoring multiple workﬂows may be able to understand that
it will be needed in 10 minutes to perform a low priority
task and in 15 minutes for a high priority task. Under these
circumstances it may choose not to accept the low priority
task.
IV.
VSA REPRESENTATION OF COMPLEX WORKFLOWS
As a test case and to compare to alternative approaches
(e.g., [14]), we modeled each word of Shakespeare’s play
Hamlet as a service and applied hierarchical chunking to
abstract into stanzas, scenes, and acts (see Figure 2). This
approach tests the capability of the chunking scheme to encode
serial and chunked workﬂows where the services at the lowest
level are the 4620 unique words of the play. The semantic level
above are the individual stanzas spoken by each character; the
level above this are individual scenes of the play (e.g., A1S1,
A1S2); next are the ﬁve acts, A1 to A5, and ﬁnally a single
10kbit vector semantically represents the whole play. The
individual word services are distributed in a communications
network and by multicasting the top level vector the whole
play is performed in a distributed manner with 29770 word
services being invoked in the correct order.
H1 = (p00.H)-1
A1
A2
A5
A1S1
A1S2
A1S3
w
A1S52
H = T + p00.A11 + p00.p10.A22 + …
A1 = T + p00.A1S11 + 
p0.p10.A1S22 + …
(p1-1. A1S1’)-1
(p1-1. H1)-1
A1’=(p00.A1)-1
(p00.A1S1)-1
Hamlet
A1
A2
A5
A1S1
A1S2
A1S3
A1S52
A1S1
who’s
A1S1
where
A1S1
Ney
A1S1
A1S1
me
A1S1
stand
the
there
w
w
(p1-1.A1’)-1
StopVec
StopVec
StopVec
Chunking
answer
Figure 2. Hamlet as a serial workﬂow.
We employ a vector alphabet, a unique vector per character,
and (5) to build a semantic vector description of each word-
service in the test case. The idea is that each letter making up
a word represents some feature of a service description, i.e.,
analogous to the different input/output/name/descriptions parts
of a real world service. Thus, variable lengths of words and
similarity of spellings represent a mix of different services of
different complexity and functional compatibility. We can use
this feature to ﬁnd semantically similar service compositions
when the best match composition is not available, i.e., we can
ﬁnd alternative words or stanzas, as shown in Figure 2, where
the word where is selected as an alternate to there.
We note that this workﬂow is a linear sequence of services,
next we show how such an approach can be extended to DAG
workﬂows by employing three phases:
1) A recruitment phase where services are discovered, se-
lected and uniquely named.
2) A connection phase where the selected services connect
themselves together using the newly generated names.
3) An atomic start command indicates to the connected
services that the workﬂow is fully composed and can be
started.
Thus, in mathematical terms, using (6):
W P
= p00. (RecruitNodes)1 +
p00.p10. (ConnectNodes)2+ p00.p10.p20.Start3
RecruitNodes = p00.Z1
1 + p00.p10.Z2
1 + ...p00.p10.p20.p30.Z4
1
+p00.p10.p20.. .p40.Z5
2 ...+ p00.p10.p20.. .p90.Z10
2
+p00.p10.p20.. .p100.Z11
3
+ p00.p10.p20.. .p110.Z12
4
+p00.p10.p20.. .p120.Z13
5
+ p00.p10.p20.. .p150.Z16
5
+p00.p10.p20.. .p160.Z17
6
+ p00.p10.p20.. .p170.Z18
7
+p00.p10.p20.. .p180.Z19
8
+ p00.p10.p20.. .p190.Z20
9
24
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-645-3
COLLA 2018 : The Eighth International Conference on Advanced Collaborative Networks, Systems and Applications

ConnectNodes =

The Visualisation Component takes the log output and
generates a DAG layout graph using Graphviz [25].
VI.
COMPARATIVE EVALUATION
For the evaluation, we imported ﬁve different DAX work-
ﬂows generated using the Pegasus workﬂow generator [16]:
1) Montage (NASA/IPAC) stitches multiple input images
together to create custom mosaics of the sky.
2) CyberShake (Southern Calfornia Earthquake Center)
characterizes earthquake hazards in a region.
3) Epigenomics (USC Epigenome Center and Pegasus) auto-
mates various operations in genome sequence processing.
4) Inspiral Analysis (LIGO) generates and analyzes gravita-
tional waveforms from data collected during the coalesc-
ing binary systems.
5) SIPHT (Harvard) automates the search for untranslated
RNAs (sRNAs) for bacterial replicons in the NCBI
database.
We ran the experiment on a MacBookPro11-4; Intel Core
i7, 2.8 GHz; 4 cores; 16 GB memory using the CORE/E-
MANE network simulator using the Python toolkit discussed
in the previous section in order to verify workﬂows could be
loaded, encoded and then executed using the VSA format.
We then recreate the workﬂows the VSAs have encoded to
verify against the original. This proceeds as follows. Once
imported, the DAX workﬂows are processed using the VSA
creator to build the semantic vector workﬂow encoding, and
apply the recruitment and connectivity phases to create service
instances of the workﬂow jobs and interconnections. During
the execution of the workﬂow using our simulator, we extract
the metrics described in the previous section, which essentially
contain a log of the run in the order of execution. This results
in a set of nodes and edges being generated which we graph
using Graphviz.
Figure 3 shows the resulting comparisons of the ﬁve
workﬂows. The coloured images represent the Pegasus gener-
ated workﬂows and blue workﬂows show the VSA generated
reconstruction of the workﬂows. Aside from the cosmetic
difference, this demonstrates that all workﬂows were composed
and correctly connected accurately in all cases.
VII.
CONCLUSIONS
In this paper, we applied and demonstrated the viability of
using VSA approach to encode workﬂows containing multiple
coordinated sub-workﬂows in a way that allows the workﬂow
logic to be unbound on-the-ﬂy and executed in a completely
decentralized manner. The Hamlet test-case demonstrated that
we can use VSA service discovery to select alternate services
on-the-ﬂy. We anticipate that such an approach will lend itself
well to edge networks where the transient nature of the mobile
nodes will require such dynamic and decentralized control.
In addition this test-case demonstrates that our encoding
scheme is scalable, i.e., 30k individual services steps where
successfully encoded and decoded in the correct order. The
Pegasus test-case demonstrates the potential for encoding more
complex multi-pathway workﬂows by encoding and decoding
a number of Pegasus DAGs.
The local arbitration mechanism employed to choose the
best matched services not only demonstrates a method that
enables workﬂows to be orchestrated without a central point
of control but can also be used to perform utility optimsation.
Using VSAs to enable services to become self-describing
has a distinct advantage because of superposition. Conven-
tional approaches could use multicast to transmit a bag of
features across the network but each individual component
feature within the bag would have to be examined and com-
pared separately for a service to assess its compatibility to the
request. In addition VSAs are robust to noise, they support
mathematical inference and analogical mapping operations
[19], [20] which could be used to learn similarities between
vector symbols across coalitions and infer new workﬂows
from previously seen workﬂows. For these reasons, our future
work will therefore focus on such self-describing service
compositions in order to realize the vision set out in [13].
ACKNOWLEDGEMENTS
This research was sponsored by the U.S. Army Research
Laboratory and the U.K. Ministry of Defence under Agreement
Number W911NF-16-3-0001. The views and conclusions con-
tained in this document are those of the authors and should
not be interpreted as representing the ofﬁcial policies, either
expressed or implied, of the U.S. Army Research Laboratory,
the U.S. Government, the U.K. Ministry of Defence or the U.K.
Government. The U.S. and U.K. Governments are authorized
to reproduce and distribute reprints for Government purposes
notwithstanding any copyright notation hereon.
REFERENCES
[1]
M. Wieczorek, R. Prodan, and T. Fahringer, “Scheduling of scientiﬁc
workﬂows in the askalon grid environment.” SIGMOD Record, vol. 34,
no. 3, 2005, pp. 56–62.
[2]
T. Fahringer et al., Workﬂows for e-Science. Springer, New York, 2007,
ch. ASKALON: A Development and Grid Computing Environment for
Scientiﬁc Workﬂows, pp. 143–166.
[3]
Altintas et al., “Kepler: An Extensible System for Design and Execution
of Scientiﬁc Workﬂows,” in 16th International Conference on Scientiﬁc
and Statistical Database Management (SSDBM).
IEEE Computer
Society, New York, 2004, pp. 423–424.
[4]
P. Kacsuk, “P-grade portal family for grid infrastructures,” Concurr.
Comput. : Pract. Exper., vol. 23, March 2011, pp. 235–245.
[5]
Deelman et al., “Pegasus: a Framework for Mapping Complex Scientiﬁc
Workﬂows onto Distributed Systems,” Scientiﬁc Programming Journal,
vol. 13, no. 3, 2005, pp. 219–237.
[6]
Oinn at al., “Taverna: A Tool for the Composition and Enactment of
Bioinformatics Workﬂows,” Bioinformatics, vol. 20, no. 17, November
2004, pp. 3045–3054.
[7]
A. Harrison, I. Taylor, I. Wang, and M. Shields, “WS-RF Workﬂow
in Triana,” International Journal of High Performance Computing
Applications, vol. 22, no. 3, Aug. 2008, pp. 268–283.
[8]
Barga et al., “The trident scientiﬁc workﬂow workbench,” in Proceed-
ings of the 2008 Fourth IEEE International Conference on eScience.
Washington, DC, USA: IEEE Computer Society, 2008, pp. 317–318.
[9]
T. Glatard, J. Montagnat, D. Lingrand, and X. Pennec, “Flexible and
efﬁcient workﬂow deployment of data-intensive applications on grids
with MOTEUR,” Int. J. High Perform. Comput. Appl., vol. 22, August
2008, pp. 347–360.
[10]
B. Balis, “Increasing scientiﬁc workﬂow programming productivity
with hyperﬂow,” in Proceedings of the 9th Workshop on Workﬂows
in Support of Large-Scale Science, ser. WORKS ’14.
Piscataway, NJ,
USA: IEEE Press, 2014, pp. 59–69.
[11]
S. Basagni, M. Conti, S. Giordano, and I. Stojmenovi´c, Mobile Ad Hoc
Networking: Edited by Stefano Basagni et al.
IEEE, 2004.
[12]
T. Pham, G. Cirincione, A. Swami, G. Pearson, and C. Williams,
“Distributed analytics and information science,” in In IEEE International
Conference on Information Fusion (Fusion), 2015.
26
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-645-3
COLLA 2018 : The Eighth International Conference on Advanced Collaborative Networks, Systems and Applications

Figure 3. A comparison of ﬁve different DAX workﬂows as input and the VSA reconstructed workﬂows from post processing the semantic vector.
[13]
D. Verma, G. Bent, and I. Taylor, “Towards a distributed federated brain
architecture using cognitive iot devices,” in 9th International Conference
on Advanced Cognitive Technologies and Applications (COGNITIVE
17), 2017.
[14]
J. P. Macker and I. Taylor, “Orchestration and analysis of decentralized
workﬂows within heterogeneous networking infrastructures,” Future
Generation Computer Systems, 2017.
[15]
E. Wittern, J. Laredo, M. Vukovic, V. Muthusamy, and A. Slominski, “A
graph-based data model for api ecosystem insights,” in Web Services
(ICWS), 2014 IEEE International Conference on.
IEEE, 2014, pp.
41–48.
[16]
“Workﬂow
Generator
Pegasus,”
https://conﬂuence.pegasus.isi.edu/
display/pegasus/WorkﬂowGenerator, [accessed on 30/05/2018].
[17]
R. W. Gayler, “Vector symbolic architectures answer jackendoff’s chal-
lenges for cognitive neuroscience,” arXiv preprint cs/0412059, 2004.
[18]
Eliasmith et al., “A large-scale model of the functioning brain,” Science,
vol. 338, no. 6111, 2012, pp. 1202–1205.
[19]
T. A. Plate, Distributed representations and nested compositional struc-
ture.
University of Toronto, Department of Computer Science, 1994.
[20]
P. Kanerva, “Hyperdimensional computing: An introduction to comput-
ing in distributed representation with high-dimensional random vectors.”
Cognitive Computation, vol. 1, no. 2, 2009, pp. 139–159.
[21]
T. A. Plate, Holographic Reduced Representation: Distributed Represen-
tation for Cognitive Structures. Stanford, CA, USA: CSLI Publications,
2003.
[22]
J. L. McClelland, “Connectionist models james l. mcclelland & axel
cleeremans in: T. byrne, a. cleeremans, & p. wilken (eds.), oxford
companion to consciousness. New York: Oxford university press, 2009.”
[23]
D. Kleyko, “Pattern recognition with vector symbolic architectures,”
Ph.D. dissertation, Lule˚a tekniska universitet, 2016.
[24]
G. Recchia, M. Sahlgren, P. Kanerva, and M. N. Jones, “Encoding se-
quential information in semantic space models: comparing holographic
reduced representation and random permutation,” Computational intel-
ligence and neuroscience, vol. 2015, 2015, p. 58.
[25]
“Graphviz - Graph Visualization Software,” http://www.graphviz.org,
[accessed on 30/05/2018].
27
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-645-3
COLLA 2018 : The Eighth International Conference on Advanced Collaborative Networks, Systems and Applications

