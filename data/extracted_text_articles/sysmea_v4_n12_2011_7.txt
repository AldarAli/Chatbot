Asymptotically Valid Conﬁdence Intervals for Quantiles and Values-at-Risk
When Applying Latin Hypercube Sampling
Marvin K. Nakayama
Computer Science Department
New Jersey Institute of Technology
Newark, New Jersey, 07102, USA
marvin@njit.edu
Abstract—Quantiles, which are also known as values-at-risk
in ﬁnance, are often used as risk measures. Latin hypercube
sampling (LHS) is a variance-reduction technique (VRT) that
induces correlation among the generated samples in such
a way as to increase efﬁciency under certain conditions; it
can be thought of as an extension of stratiﬁed sampling
in multiple dimensions. This paper develops asymptotically
valid conﬁdence intervals for quantiles that are estimated via
simulation using LHS.
Keywords-quantile; value-at-risk; Latin hypercube sampling;
variance reduction; conﬁdence interval.
I. INTRODUCTION
Complex stochastic systems arise in many application
areas, such as supply-chain management, transportation,
networking, and ﬁnance. The size and complexity of such
systems often preclude the availability of analytical methods
for studying the resulting stochastic models, so simulation
is frequently used.
Suppose that we are interested in analyzing the behavior
of a system over a (possibly random) ﬁnite time horizon, and
let X be a random variable denoting the system’s (random)
performance over the time interval of interest. For example,
X may represent the time to complete a project, or X
may be the loss of a portfolio of ﬁnancial investments over
the next two weeks. Most simulation textbooks focus on
estimating the mean µ of X. This typically involves running
independent and identically distributed (i.i.d.) replications of
the system over the time horizon, and estimating µ via the
sample average of the outputted performance from the repli-
cations. To provide a measure of the error in the estimate of
µ, the analyst will often construct a conﬁdence interval for
µ using the simulated output; e.g., see Section 9.4.1 of [2].
In many contexts, however, performance measures other
than a mean provide more useful information. One such
measure is a quantile. For 0 < p < 1, the p-quantile ξp
of a random variable X is the smallest constant x such that
P(X ≤ x) ≥ p. A well-known example is the median,
which the 0.5-quantile. In terms of the cumulative distribu-
tion function (CDF) F of X, we can write ξp = F −1(p).
Quantiles arise in many practical situations, often to measure
risk. For example, in bidding on a project, a contractor may
want to determine a date such that his ﬁrm has a 95% chance
of ﬁnishing the project by that date, which is the 0.95-
quantile. In ﬁnance, quantiles, which are known as values-
at-risk, are frequently used as measures of risk of portfolios
of assets [3]. For example, a portfolio manager may want
to know the 0.99-quantile ξ0.99 of the loss of his portfolio
over the next two weeks, so there is a 1% chance that the
loss over the next two weeks will exceed ξ0.99.
Estimation of a quantile ξp is complicated by the fact that
ξp cannot be expressed as the mean of a random variable,
so one cannot estimate ξp via a sample average. However,
the fact that ξp = F −1(p) suggests an alternative approach:
develop an estimator of the CDF F, which may be a sample
average, and then invert the estimated CDF.
In addition to a point estimator of ξp, we also would like
a conﬁdence interval (CI) of ξp to provide a measure of the
accuracy of the point estimator. One approach to developing
a CI is to ﬁrst prove that the estimator of ξp satisﬁes a
central limit theorem (CLT), and then construct a consistent
estimator of the variance constant appearing in the CLT to
obtain a CI.
Sometimes, the CI for quantile ξp is large, especially
when p ≈ 0 or p ≈ 1, motivating the use of a variance-
reduction technique (VRT) to obtain a quantile estimator
with smaller error. VRTs that have been applied to quantile
estimation include control variates (CV) [4], [5]; induced
correlation, including antithetic variates (AV) and Latin
hypercube sampling (LHS) [6], [7]; importance sampling
(IS) [8]; and combined importance sampling and stratiﬁed
sampling (IS+SS) [9]. Typically, variance reduction for
quantile estimation entails applying a VRT to estimate the
CDF, and then inverting the resulting CDF estimator to
obtain a quantile estimator.
While most of the papers in the previous paragraph
establish CLTs for the corresponding quantile estimators
when applying VRTs, none of them provides a way to
consistently estimate the CLTs’ variance constants. Indeed,
[9] states that this is “difﬁcult and beyond the scope of
this paper.” To address this issue, [10] develops a general
framework for analyzing some asymptotic properties (as the
sample size gets large) of quantile estimators when applying
86
International Journal on Advances in Systems and Measurements, vol 4 no 1 & 2, year 2011, http://www.iariajournals.org/systems_and_measurements/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

VRTs, and [10] shows how this can be exploited to construct
consistent estimators of the variance constants in the CLTs.
Also, [10] shows the framework encompasses CV, IS+SS
and AV. In the current paper, we now do the same for LHS.
The rest of the paper is organized as follows. Section II
develops the mathematical framework. We describe LHS
in Section III, giving a CI for a quantile estimated via
LHS. We present experimental results on a small example in
Section IV. Section V provides some concluding remarks,
and Section VI contains the proofs of our theorems. The
current paper is based on and expands a previous conference
paper [1], which includes neither the experimental results
nor the proofs.
II. BACKGROUND
Consider a random variable X having CDF F. For ﬁxed
0 < p < 1, the goal is to estimate the p-quantile ξp =
F −1(p) of X, where F −1(q) = inf{x : F(x) ≥ q} for any
0 < q < 1. We assume that X can be expressed as
X = g(U1, U2, . . . , Ud)
(1)
for a known and given function g : ℜd → ℜ, where
U1, U2, . . . , Ud are i.i.d. unif[0, 1) random numbers. Thus,
generating an output X can be accomplished by transform-
ing d i.i.d. uniforms through g. We now provide examples
ﬁtting in this framework.
Example 1: Suppose X is the time to complete a project,
and we are interested in computing the 0.95-quantile ξ0.95 of
X. Assume the time to complete the project is modeled as
a stochastic activity network (SAN) [11] having s activities,
labeled 1, . . . , s. Suppose that there are r paths through
the SAN, and let Bj be the set of activities on path
j, j = 1, 2, . . . , r. For each activity i = 1, . . . , s, let
Ai be its (random) duration. We allow for A1, . . . , As to
be dependent, and let H denote the joint distribution of
A ≡ (A1, . . . , As). Suppose that we can generate a sample
of A from H using a ﬁxed number d of i.i.d. unif[0, 1)
random variables U1, . . . , Ud. In the case when A1, . . . , As
are independent with each Ai having marginal distribution
Hi, we can generate Ai as Ai = H−1
i
(Ui), for i = 1, . . . , s,
assuming that H−1
i
can be computed efﬁciently. The length
of the jth path in the SAN is Tj = P
i∈Bj Ai, and we can
express X = max(T1, . . . , Tr) as the time to complete the
project. Thus, the function g in (1) in this case takes the
i.i.d. uniforms U1, . . . , Ud as arguments, transforms them
into A1, . . . , As, computes the length of each path Tj, and
returns the maximum path length as X.
Example 2: Consider a ﬁnancial portfolio consisting of a
mix of investments, e.g., stocks, bonds and derivatives. Let
V (t) be the value of the portfolio at time t, and suppose the
current time is t = 0. Let T denote two weeks, and we are
interested in the 0.99-quantile of the loss X in the portfolio
at the end of this period. Assume we have a stochastic
model for V (T), and suppose that simulating V (T) given
the current portfolio value V (0) requires generating a ﬁxed
number d of i.i.d. unif[0, 1) random variables U1, . . . , Ud;
see Chapter 3 of [12] for algorithms to simulate V (T) under
various stochastic models describing the change in values of
the investments. Thus, the function g in (1) takes the i.i.d.
uniforms U1, . . . , Ud as input, transforms them into V (T),
and then outputs X = V (0) − V (T) as the portfolio loss.
(A negative loss is a gain.) The 0.99-level value-at-risk is
then the 0.99-quantile of X.
We now review how quantiles can be estimated when
applying crude Monte Carlo (CMC) (i.e., no variance re-
duction). We ﬁrst generate n × d i.i.d. unif[0, 1) random
numbers Ui,j, i = 1, 2, . . . , n, j = 1, 2, . . . , d, which we
arrange in an n × d grid:
U1,1
U1,2
· · ·
U1,d
U2,1
U2,2
· · ·
U2,d
...
...
...
...
Un,1
Un,2
· · ·
Un,d
.
(2)
Then
we
use
the
uniforms
to
generate
n
outputs
X1, X2, . . . , Xn as follows:
X1
=
g(U1,1, U1,2, . . . , U1,d)
X2
=
g(U2,1, U2,2, . . . , U2,d)
...
Xn
=
g(Un,1, Un,2, . . . , Un,d)
.
Thus, the ith row of uniforms Ui,1, Ui,2, . . . , Ui,d from (2) is
used to generate the ith output Xi, and the independence of
the columns of uniforms in (2) ensures that each Xi has the
correct distribution F. Also, because of the independence of
the rows of uniforms in (2), we have that X1, X2, . . . , Xn
are i.i.d. We then estimate F via the empirical CDF ˆFn,
which is constructed as
ˆFn(x) = 1
n
n
X
i=1
I(Xi ≤ x),
where I(A) is the indicator function of the event A, which
takes the value 1 (resp., 0) if the event A occurs (resp., does
not occur). Then
ˆξp,n = ˆF −1
n (p)
(3)
is the CMC estimator of the p-quantile ξp.
Let f denote the derivative of the CDF F, when it exists,
and assume F is differentiable at ξp with f(ξp) > 0. The
estimator ˆξp,n then is strongly consistent; i.e., ˆξp,n → ξp as
n → ∞ with probability 1 (e.g., see p. 75 of [13]). Also,
ˆξp,n satisﬁes the following CLT (Section 2.3.3 of [13]):
√n
κp

ˆξp,n − ξp

⇒ N(0, 1)
(4)
as n → ∞, where ⇒ denotes convergence in distribution
(p. 8 of [13]) and N(a, b2) is a normal random variable
87
International Journal on Advances in Systems and Measurements, vol 4 no 1 & 2, year 2011, http://www.iariajournals.org/systems_and_measurements/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

with mean a and variance b2. The constant
κp =
p
p(1 − p)φp,
(5)
where
φp =
1
f(ξp).
(6)
Using (4), we can construct an approximate 100(1 − α)%
CI for ξp as
"
ˆξp,n ± zα/2
p
p(1 − p) φp
√n
#
,
where zα/2 = Φ−1(1−α/2) and Φ is the CDF of a N(0, 1).
Unfortunately, the above CI is not implementable in practice
since φp is unknown. Thus, we require a (weakly) consistent
estimator of φp, and for the CMC case, such estimators have
been proposed in the statistics literature [14]–[17]. As φp =
d
dpF −1(p), these papers develop ﬁnite-difference estimators
of the derivative (Section 7.1 of [12]):
ˆφp,n(h′) =
ˆF −1
n (p + h′) − ˆF −1
n (p − h′)
2h′
,
(7)
where h′ ≡ h′
n > 0 is known as the smoothing parameter.
Consistency holds (i.e., ˆφp,n(h′
n) ⇒ φp as n → ∞) when
h′
n → 0 and nh′
n → ∞ as n → ∞. In this case, we obtain
the following approximate 100(1 − α)% CI for ξp:
Jn(h′
n) ≡
"
ˆξp,n ± zα/2
p
p(1 − p) ˆφp,n(h′
n)
√n
#
,
(8)
which is asymptotically valid in the sense that
P{ξp ∈ Jn(h′
n)} → 1 − α
as n → ∞. The consistency proofs of ˆφp,n(h′
n) in [15] and
[16] utilize the fact that the i.i.d. outputs Xi, i = 1, 2, . . . , n,
can be represented as Xi = F −1(Ui) for Ui ∼ unif[0, 1)
i.i.d. and then exploits properties of uniform order statistics.
But this approach does not work when applying VRTs,
including LHS, so we require another proof technique when
applying LHS.
In the case of CMC, there has been some asymptotic
analysis suggesting how one should choose the smoothing
parameter h′ = h′
n in (7). To asymptotically minimize the
mean square error (MSE) of ˆφp,n(h′
n) as an estimator of φp,
one should choose h′
n = O(n−1/5); see [16]. Alternatively,
if one wants to minimize the coverage error of the conﬁdence
interval in (8), then [17] gives the asymptotically optimal
rate for h′
n to be O(n−1/3).
III. LATIN HYPERCUBE SAMPLING
LHS, which was introduced by [18] and further analyzed
by [19], is a VRT that induces correlation among the
simulated outputs in such a way as to increase the statistical
efﬁciency under certain conditions. It can be viewed as
an extension of stratiﬁed sampling (Section 4.3 of [12]).
We next describe an approach in [6] for applying LHS to
estimate the quantile ξp of the random variable X.
A. Single-Sample LHS
We will ﬁrst generate a Latin hypercube (LH) sample of
size t in dimension d as follows. Let Ui,j for i = 1, . . . , t
and j = 1, . . . , d be t × d i.i.d. unif[0, 1) random numbers.
Let π1, . . . , πd be d independent random permutations of
{1, . . . , t}; i.e., for each πj, each of the t! permutations
is equally likely. The π1, . . . , πd are generated indepen-
dently of the Ui,j. For each j = 1, 2, . . . , d, we have
πj = (πj(1), πj(2), . . . , πj(t)), and πj(i) is the number to
which i ∈ {1, 2, . . . , t} is mapped in the jth permutation.
Then deﬁne
Vi,j = πj(i) − 1 + Ui,j
t
; i = 1, . . . , t; j = 1, . . . , d;
and arrange them into a t × d grid:
V1,1
V1,2
· · ·
V1,d
V2,1
V2,2
· · ·
V2,d
...
...
...
...
Vt,1
Vt,2
· · ·
Vt,d
.
(9)
For each i = 1, . . . , t, it is easy to show that the ith row
Vi ≡ (Vi,1, Vi,2, . . . , Vi,d) from (9) is a vector of d i.i.d.
unif[0, 1) numbers. But within each column j, the t uniforms
V1,j, V2,j, . . . , Vt,j are dependent since they all use the same
permutation πj. Thus, the rows V1, V2, . . . , Vt are dependent,
and we call the t × d uniforms in (9) an LH sample of size
t in d dimensions.
The LH sample in (9) has an interesting feature, as we
now explain. Partition the unit interval [0, 1) into t equal-
length subintervals [0, 1/t), [1/t, 2/t), . . . , [(t − 1)/t, 1).
Then one can show that the t uniforms in any column of
(9) have the property that exactly one of them lies in each
subinterval. Each of the d columns thus forms a stratiﬁed
sample of the unit interval in one dimension, so the LH
sample can be seen as an extension of stratiﬁed sampling in
d dimensions.
We use the ith row Vi from (9) to generate an output X′
i:
X′
1
=
g(V1,1, V1,2, . . . , V1,d)
X′
2
=
g(V2,1, V2,2, . . . , V2,d)
...
X′
t
=
g(Vt,1, Vt,2, . . . , Vt,d)
.
(10)
Each X′
i has distribution F since Vi,1, Vi,2, . . . , Vi,d from
the ith row of (9) are i.i.d. unif[0, 1). An estimator of F is
then
¯Ft(x) = 1
t
t
X
i=1
I(X′
i ≤ x).
We can then invert this to obtain
¯ξp,t = ¯F −1
p,t (p),
88
International Journal on Advances in Systems and Measurements, vol 4 no 1 & 2, year 2011, http://www.iariajournals.org/systems_and_measurements/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

which we call the single-sample LHS (SS-LHS) quantile
estimator. As shown in [6], the estimator ¯ξp,t satisﬁes the
CLT
√
t
ηp
(¯ξp,t − ξ) ⇒ N(0, 1)
(11)
as t → ∞ under certain regularity conditions, where
ηp = ζpφp,
(12)
ζp is given in [6], and φp is deﬁned in (6). Recalling (4)
and (5), which are for the case of CMC, we see that SS-
LHS yields an asymptotic variance reduction when ζp <
p
p(1 − p), and [6] provides sufﬁcient conditions to ensure
this holds.
Constructing a conﬁdence interval for ξp based on
the CLT (11) requires consistently estimating ηp. But
the dependence of the rows V1, V2, . . . , Vt implies that
X′
1, X′
2, . . . , X′
t are dependent, and this complicates con-
structing an estimator for ηp. Indeed, [6] does not develop
estimators for ζp and φp.
B. Combined Multiple-LHS
To avoid the above complication, rather than taking a
single LH sample of size t to obtain a set of t (dependent)
outputs as in [6], we instead generate a total of n = mt out-
puts in groups of t, where each group is constructed from an
LH sample of size t and the m different groups are sampled
independently. We then use all n samples to compute a CDF
estimator, which we invert to obtain a quantile estimator. We
incur some loss in statistical efﬁciency by using m different
independent LH samples, each of size t, instead of taking
one big LH sample of size n, but [19] notes the degradation
is small when t/d is large.
We now provide details of our approach. Deﬁne t×d×m
i.i.d. unif[0, 1) random variables U (k)
i,j , where i = 1, . . . , t;
j = 1, . . . , d; and k = 1, . . . , m. Also, let π(k)
j
for j =
1, . . . , d and k = 1, . . . , m be d × m independent permuta-
tions of {1, . . . , t}, and the U (k)
i,j and the π(k)
j
are all mutually
independent. Each π(k)
j
= (π(k)
j
(1), π(k)
j
(2), . . . , π(k)
j
(t)),
and π(k)
j
(i) is the value to which i is mapped in permutation
π(k)
j
. For each k = 1, . . . , m, let
V (k)
i,j =
π(k)
j
(i) − 1 + U (k)
i,j
t
; i = 1, . . . , t; j = 1, . . . , d;
and we arrange them in a t × d grid:
V (k)
1,1
V (k)
1,2
· · ·
V (k)
1,d
V (k)
2,1
V (k)
2,2
· · ·
V (k)
2,d
...
...
...
...
V (k)
t,1
V (k)
t,2
· · ·
V (k)
t,d
,
(13)
which is an LH sample of size t in d dimensions. We
have m such independent grids. Thus, for each grid k =
1, . . . , m, and for each i = 1, . . . , t, we have that V (k)
i
≡
(V (k)
i,1 , V (k)
i,2 , . . . , V (k)
i,d ) is a vector of d i.i.d. unif[0, 1)
numbers. Also, for each k
= 1, . . . , m, the t vectors
V (k)
1
, V (k)
2
, . . . , V (k)
t
are dependent. We use the ith row V (k)
i
from (13) to generate an output X(k)
i
:
X(k)
1
=
g(V (k)
1,1 , V (k)
1,2 , . . . , V (k)
1,d )
X(k)
2
=
g(V (k)
2,1 , V (k)
2,2 , . . . , V (k)
2,d )
...
X(k)
t
=
g(V (k)
t,1 , V (k)
t,2 , . . . , V (k)
t,d )
.
(14)
Each X(k)
i
has distribution F since V (k)
i,1 , V (k)
i,2 , . . . , V (k)
i,d
from the ith row of (13) are i.i.d. unif[0, 1).
Since we independently repeat (14) for k = 1, 2, . . . , m,
we get t × m outputs, which we arrange in a grid:
X(1)
1
X(1)
2...
X(1)
t
X(2)
1
X(2)
2...
X(2)
t
. . .
. . .
...
. . .
X(m)
1
X(m)
2...
X(m)
t
.
(15)
Each boxed column k corresponds to one set of t (depen-
dent) outputs from an LH sample of size t as in (14), so the
rows of (15) are dependent. But since we generate the m LH
samples independently, the columns of (15) are independent.
We subsequently form an estimator of the CDF F as
˜Fm,t(x) = 1
m
m
X
k=1
1
t
t
X
i=1
I(X(k)
i
≤ x).
(16)
For any 0 < p < 1, we then obtain
˜ξp,m,t = ˜F −1
m,t(p),
(17)
which we call the combined multiple-LHS (CM-LHS) esti-
mator of ξp.
As we will later see, the CM-LHS quantile estimator
˜ξp,m,t obeys a CLT, and we will use an approach developed
in [10] to estimate the asymptotic variance in the CLT. To
do this, ﬁrst let pm be any perturbed value of p satisfying
pm → p as m → ∞, and let ˜ξpm,m,t = ˜F −1
m,t(pm). The fol-
lowing theorem, whose proof is in Section VI-A, establishes
that ˜ξpm,m,t satisﬁes a so-called Bahadur representation [20].
Theorem 1: If f(ξp) > 0, then for any pm = p +
O(m−1/2),
˜ξpm,m,t = ξ′
pm −
˜Fm,t(ξp) − p
f(ξp)
+ Rm
(18)
with ξ′
pm = ξp + (pm − p)/f(ξp) and
√mRm ⇒ 0
(19)
as m → ∞. If in addition f is continuous in a neighborhood
of ξp, then (18)–(19) hold with ξ′
pm = F −1(pm) for all
pm → p.
89
International Journal on Advances in Systems and Measurements, vol 4 no 1 & 2, year 2011, http://www.iariajournals.org/systems_and_measurements/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

A consequence of the Bahadur representation in (18)–(19)
is that the CM-LHS quantile estimator ˜ξp,m,t then satisﬁes
a CLT, which we can see as follows. Take pm = p in (18),
and rearranging terms and multiplying by √m lead to
√m(˜ξp,m,t −ξp) = √m
 
p − ˜Fm,t(ξp)
f(ξp)
!
+√mRm. (20)
Let
W (k)(x) = 1
t
t
X
i=1
I(X(k)
i
≤ x),
so
˜Fm,t(x) =
1
m
Pm
k=1 W (k)(x). For any ﬁxed x, the
W (k)(x), k = 1, . . . , m, are i.i.d., and since each X(k)
i
has
distribution F, we see that E[W (k)(x)] = F(x). Deﬁne
ψ2
p = Var[W (k)(ξp)],
which is ﬁnite since 0 ≤ W (k)(x) ≤ 1 for all x. Hence, the
ordinary CLT (e.g., p. 28 of [13]) implies that for ﬁxed t,
√m
ψp

F(ξp) − ˜Fm,t(ξp)

⇒ N(0, 1)
(21)
as m → ∞. Since F(ξp) = p, we then get that if f(ξp) > 0,
√m
ψp/f(ξp)
 
p − ˜Fm,t(ξp)
f(ξp)
!
⇒ N(0, 1)
as m → ∞ with t ﬁxed. Thus, it follows from (19), (20)
and Slutsky’s theorem (e.g., p. 19 of [13]) that for ﬁxed t,
√m
τp

˜ξp,m,t − ξp

⇒ N(0, 1)
(22)
as m → ∞, where
τp = ψpφp
and φp = 1/f(ξp), as in (6).
To construct a CI for ξp based on the CLT in (22) for
the CM-LHS quantile estimator, we now want consistent
estimators of ψp and φp. We can estimate ψ2
p via
˜ψ2
p,m,t =
1
m − 1
m
X
k=1

W (k)(˜ξp,m,t) − ¯Wm
2
,
where
¯Wm = 1
m
m
X
k=1
W (k)(˜ξp,m,t).
Even though W (k)(x), k = 1, . . . , m, are i.i.d. for any
ﬁxed x, each W (k)(˜ξp,m,t) depends on ˜ξp,m,t, which is a
function of all of the samples by (16) and (17), and this
induces dependence among W (k)(˜ξp,m,t), k = 1, 2, . . . , m.
This complicates the analysis of ˜ψp,m,t, but we can apply
the techniques in [10] to prove that
˜ψp,m,t ⇒ ψp
(23)
as m → ∞ for ﬁxed t ≥ 1. An estimator for φp =
d
dpF −1(p) is the ﬁnite difference
˜φp,m,t(hm) =
˜F −1
m,t(p + hm) − ˜F −1
m,t(p − hm)
2hm
(24)
for smoothing parameter hm > 0. In the proof of Theorem 2
below, we prove that
˜φp,m,t(hm) ⇒ φp
(25)
as m → ∞ for ﬁxed t ≥ 1 under certain conditions (given
in Theorem 2) on hm and the CDF F.
When (23) and (25) hold, Slutsky’s theorem guarantees
the CLT in (22) remains valid when we replace τp = ψpφp
with its consistent estimator ˜ψp,m,t ˜φp,m,t(hm). We then
obtain the following approximate 100(1 − α)% CI for ξp
when applying CM-LHS:
˜Jm,t(hm) ≡
"
˜ξp,m,t ± zα/2
˜ψp,m,t ˜φp,m,t(hm)
√m
#
.
(26)
The following theorem, whose proof is in Section VI-B,
establishes the asymptotic validity of the above CI.
Theorem 2: If f(ξp) > 0, then for any ﬁxed t ≥ 1,
lim
m→∞ P{ξp ∈ ˜Jm,t(hm)} = 1 − α
(27)
for hm = cm−1/2 and any constant c > 0. If in addition f
is continuous in a neighborhood of ξp, then (27) holds for
any hm > 0 satisfying hm → 0 and 1/hm = O(m1/2).
The range of valid values for the smoothing parameter
hm in the second case of Theorem 2 is of particular interest
since it covers the asymptotically optimal rates for CMC, as
discussed at the end of Section II.
We close this section describing how to invert ˜Fm,t, which
is needed to compute the CM-LHS quantile estimator ˜ξp,m,t
in (17) and the ﬁnite difference ˜φp,m,t(hm) in (24). First take
the n = mt values X(k)
i
for i = 1, . . . , t and k = 1, . . . , m,
and sort them in ascending order as X(1) ≤ X(2) ≤ · · · ≤
X(n). Then for 0 < q < 1, we can compute ˜F −1
m,t(q) =
X(⌈nq⌉), where ⌈·⌉ is the round-up function.
IV. EXPERIMENTAL RESULTS
We now present some results from running experiments
on a small SAN, and below we follow the notation developed
in Example 1 from Section II. Previously studied in [4] and
[10], the SAN has s = 5 activities, where the activity dura-
tions are i.i.d. exponential with mean 1. Since the activities
are independent, we generate the s activity durations using
d = s i.i.d. unif[0, 1) random variables via inversion; e.g.,
see Section 2.2.1 of [12]. There are r = 3 paths in the
SAN, with B1 = {1, 2}, B2 = {1, 3, 5}, and B3 = {4, 5}
as the collections of activities on the 3 paths. The goal is
to estimate the p-quantile ξp of the length X of the longest
path for different values of p.
90
International Journal on Advances in Systems and Measurements, vol 4 no 1 & 2, year 2011, http://www.iariajournals.org/systems_and_measurements/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

CM-LHS
normal critical point
Student-T critical point
n
CMC
t = 10
t = 20
t = 50
t = 10
t = 20
t = 50
100
0.899
0.877
0.838
0.618
0.906
0.904
0.739
(0.359) (0.229) (0.212) (0.171) (0.255) (0.275) (0.658)
400
0.881
0.879
0.867
0.838
0.887
0.883
0.883
(0.162) (0.106) (0.101) (0.098) (0.108) (0.106) (0.112)
1600
0.885
0.887
0.879
0.879
0.889
0.884
0.889
(0.081) (0.053) (0.051) (0.050) (0.053) (0.052) (0.052)
6400
0.898
0.895
0.891
0.897
0.895
0.893
0.899
(0.041) (0.027) (0.026) (0.025) (0.027) (0.026) (0.025)
Table I
COVERAGES (AND AVERAGE HALF WIDTHS) FOR CONFIDENCE
INTERVALS USING CMC AND CM-LHS FOR p = 0.5
Tables I and II present results for p = 0.5 and p = 0.9,
respectively. In our experiments we constructed nominal
100(1−α)% = 90% conﬁdence intervals (CIs) for ξp based
on the CMC and the CM-LHS quantile estimators from (3)
and (17), respectively. We ran 104 independent replications
to estimate coverage levels and average half widths (in
parentheses) of the conﬁdence intervals. We varied the total
sample size n = 4a × 100 for a = 0, 1, 2, 3. When applying
CM-LHS, we varied the LH sample size as t = 10, 20
and 50. We set the smoothing parameter for CMC to be
h′
n = 0.5n−1/2; for CM-LHS, we chose hm = 0.5(tm)−1/2,
so h′
n = hm in all our experiments when n = mt. Column 2
in the tables gives the results for the CMC CIs from (8).
Columns 3–5 correspond to the CM-LHS CIs in (26), where
we recall zα/2 is the 1 − α/2 critical point of a standard
normal. For columns 6–8, we replace zα/2 in (26) with the
1 − α/2 critical point Tm−1,α/2 of a Student-T distribution
with m − 1 degrees of freedom (df); i.e., if Gm−1 is the
CDF of a Student-T random variable with m − 1 df, then
Tm−1,α/2 = G−1
m−1(1 − α/2). Since Tm−1,α/2 → zα/2 as
m → ∞, the resulting CI with critical point Tm−1,α/2 is
also asymptotically valid as m → ∞ with ﬁxed t. But
Tm−1,α/2 > zα/2 for all m and α, so CIs with Student-T
critical points are wider than those with the normal critical
point, which can lead to higher coverage.
We ﬁrst discuss the results for p = 0.5 (Table I). CMC
gives close to nominal coverage (0.9) for all sample sizes n.
The same is true for CM-LHS with t = 10 and the normal
critical point zα/2, but now the average half width decreased
by about 35%. For CM-LHS with t = 20 and t = 50 and
the normal critical point, coverage is less than 0.9 for small
n, but when n is large, coverage is close to nominal and the
average half width is about the same as for t = 10. The poor
coverage for small n and large t arises because the value of
m = n/t is then small; e.g., when t = 50 and n = 100,
then m is only 2. But the validity of our CM-LHS conﬁdence
interval in (26) requires m → ∞ (see Theorem 2), so we
see poor coverage when t is large and n is small. Coverage
is improved for small n when we instead use the Student-
CM-LHS
normal critical point
Student-T critical point
n
CMC
t = 10
t = 20
t = 50
t = 10
t = 20
t = 50
100
0.868
0.861
0.810
0.549
0.891
0.883
0.616
(0.706) (0.578) (0.512) (0.362) (0.644) (0.663) (0.391)
400
0.885
0.877
0.869
0.846
0.886
0.885
0.891
(0.348) (0.285) (0.260) (0.230) (0.292) (0.274) (0.265)
1600
0.899
0.891
0.888
0.878
0.893
0.892
0.889
(0.173) (0.142) (0.130) (0.117) (0.143) (0.131) (0.121)
6400
0.898
0.902
0.895
0.890
0.903
0.896
0.893
(0.086) (0.071) (0.065) (0.059) (0.071) (0.065) (0.059)
Table II
COVERAGES (AND AVERAGE HALF WIDTHS) FOR CONFIDENCE
INTERVALS USING CMC AND CM-LHS FOR p = 0.9
T critical point Tm−1,α/2 to construct the CI, but it is still
signiﬁcantly below the nominal level for t = 50. However,
this problem goes away when n is large for both the normal
and Student-T critical points since then m is also large, so
the asymptotic validity takes effect.
The results for p = 0.9 (Table II) exhibit similar qualities
to those for p = 0.5, but there are some important differ-
ences. For p = 0.9, we see that the amount that CM-LHS
decreases the average half width depends on the choice of
t. For t = 10, the average half width shrunk by about 17%
compared to CMC. Average half width is even smaller when
using CM-LHS with larger t, with more than 30% reduction
for t = 50. Thus, while the choice of LH sample size t does
not appear to have much affect on the amount of variance
reduction when estimating quantiles in the middle of the
distribution, it can have a large impact when estimating more
extreme quantiles.
Recall that the CM-LHS quantile estimator ˜ξp,m,t in (17)
is computed by inverting ˜Fm,t in (16), which depends on
all n = mt samples generated. An alternative approach is
to use batching, where we compute m different independent
quantile estimates, one from each of the m columns of
size t in (15). Since the m columns are i.i.d., we can then
compute the average and sample variance of the m i.i.d.
quantile estimates to construct a conﬁdence interval for ξp.
This approach is discussed on p. 242 of [12] for the case of
estimating a mean using LHS, and now we provide details
on how one could apply batching for estimating a quantile;
see also p. 491 of [12]. For each k = 1, 2, . . . , m, let
˙ξp,k,t = ˙F −1
k,t (p), where
˙Fk,t(x) = 1
t
t
X
i=1
I(X(k)
i
≤ x),
which is the estimated CDF from using only the kth boxed
column of LH samples in (15). Then the batched LHS
quantile estimator is
¯ξp,m,t = 1
m
m
X
k=1
˙ξp,k,t,
(28)
91
International Journal on Advances in Systems and Measurements, vol 4 no 1 & 2, year 2011, http://www.iariajournals.org/systems_and_measurements/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

p = 0.5
p = 0.9
n
t = 10
t = 20
t = 50
t = 10
t = 20
t = 50
100
0.587
0.817
0.891
0.437
0.733
0.876
(0.218) (0.242) (0.607) (0.470) (0.546) (1.327)
400
0.093
0.531
0.836
0.042
0.411
0.768
(0.103) (0.103) (0.111) (0.222) (0.234) (0.245)
1600
0.000
0.066
0.652
0.000
0.021
0.487
(0.051) (0.050) (0.051) (0.110) (0.114) (0.113)
6400
0.000
0.000
0.178
0.000
0.000
0.046
(0.025) (0.025) (0.025) (0.055) (0.057) (0.056)
Table III
COVERAGES (AND AVERAGE HALF WIDTHS) FOR LHS CONFIDENCE
INTERVALS USING BATCHING WITH m = n/t BATCHES AND LH
SAMPLE SIZE t
and an approximate 100(1 − α)% conﬁdence interval for ξp
is

¯ξp,m,t ± Tm−1,α/2
Sm
√m

,
(29)
where
S2
m =
1
m − 1
m
X
k=1
( ˙ξp,k,t − ¯ξp,m,t)2.
As in our experiments with the CM-LHS quantile esti-
mator in Tables I and II, we also ran experiments with
the batched CI in (29) for p = 0.5 and 0.9 to study its
behavior as the total sample size n increases. Since n = mt,
increasing n requires correspondingly increasing the number
m of batches and/or the LH sample size t.
Table III presents results using batching in which we
keep t ﬁxed at 10, 20 or 50 so m = n/t increases as
n grows. For n = 100, coverage is close to nominal for
t = 50, but coverage is signiﬁcantly low for t = 10 and
t = 20. As n increases, coverage actually worsens for each
t. This occurs because quantile estimators are biased, with
bias decreasing as the sample size increases; see [6]. In each
column of Table III, t is ﬁxed and does not increase, so
¯ξp,m,t is the average of m biased quantile estimators ˙ξp,k,t,
k = 1, 2, . . . , m, each computed from a single LH sample
of size t, with more bias for small t. If E[| ˙ξp,1,t|] < ∞, then
¯ξp,m,t = 1
m
m
X
k=1
˙ξp,k,t ⇒ E[ ˙ξp,1,t]
as m → ∞ by the law of large numbers since ˙ξp,k,t,
k = 1, 2, . . . , m, are i.i.d. But E[ ˙ξp,1,t] ̸= ξp because of
the bias for ﬁxed t. Thus, as n and m increase, the CI
in (29) is shrinking at rate m−1/2, but it is centered at an
estimate whose bias is not decreasing since t is ﬁxed. This
results in the poor coverage. For a batching approach to
work, we instead need the LH sample size t to increase as
the total sample size n increases to ensure the bias of ¯ξp,m,t
decreases.
Table IV presents results with batching in which the
number of batches is ﬁxed at m = 10, so the LH sample
n
p = 0.5
p = 0.9
100
0.587
0.437
(0.218)
(0.470)
400
0.807
0.720
(0.109)
(0.241)
1600
0.879
0.850
(0.055)
(0.118)
6400
0.889
0.888
(0.027)
(0.060)
Table IV
COVERAGES (AND AVERAGE HALF WIDTHS) FOR LHS CONFIDENCE
INTERVALS USING BATCHING WITH m = 10 BATCHES AND LH SAMPLE
SIZE t = n/m
size t = n/m grows as the total sample size n increases.
In contrast to the case when t was ﬁxed as n increases
(Table III), we now see in Table IV that the coverage levels
of the CIs in (29) converge to the nominal level 0.9 as n
increases. However, compared to the results in Tables I and
II for the CM-LHS quantile estimator, we see that batching
with t increasing in n gives lower coverage when n is small.
This occurs because of the bias of quantile estimators, as
we discussed in the previous paragraph. The batched LHS
estimator in (28) averages m i.i.d. quantile estimators, each
based on an LH sample of size t, so the bias of the batched
quantile estimator is determined by t. On the other hand,
CM-LHS computes a single quantile estimator (17) based
on inverting the CDF estimator (16) from all of the n = mt
samples. Because the bias of quantile estimators decreases as
the sample size grows, the batched LHS quantile estimator
has larger bias than the CM-LHS quantile estimator. This
leads to the coverage of batched CI in (29) converging more
slowly to the nominal level as the total sample size n grows
than the CI in (26) for CM-LHS. This property is more
pronounced for p = 0.9 than for p = 0.5, so batching
seems to do worse for extreme quantiles than for those in
the middle of the distribution.
V. CONCLUSION
We presented an asymptotically valid CI for a quantile
estimated using LHS. We developed a combined multiple-
LHS approach in which one generates a total of n samples
in m independent groups, where each group is generated
from an LH sample of size t. Using a general framework
developed in [10] for quantile estimators from applying
VRTs, we proved that the resulting CM-LHS quantile es-
timator satisﬁes a Bahadur representation, which provides
an asymptotic approximation for the estimator. The Bahadur
representation implies a CLT for the CM-LHS quantile esti-
mator and also allows us to construct a consistent estimator
for the asymptotic variance in the CLT.
We ran experiments on a small SAN, and our results
demonstrate the asymptotic validity of our CM-LHS CIs.
Also, the results show that CM-LHS can decrease the aver-
92
International Journal on Advances in Systems and Measurements, vol 4 no 1 & 2, year 2011, http://www.iariajournals.org/systems_and_measurements/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

age half width of conﬁdence intervals relative to CMC, but
the amount of decrease can depend of the LH sample size t
when estimating an extreme quantile. We also experimented
with an alternative approach based on batching with m in-
dependent batches, each consisting of t samples constructed
from an LH sample of size t. To lead to asymptotically
valid CIs, batching requires t to increase as the total sample
size n = mt grows. Compared to CM-LHS, batching needs
larger samples sizes n for the CIs to have close to nominal
coverage. Further work is needed to study the empirical
performance of the proposed method when simulating other
larger stochastic models.
VI. APPENDIX
A. Proof of Theorem 1
Chu and Nakayama [10] develop a general framework
giving sufﬁcient conditions for quantile estimators obtained
when applying VRTs to satisfy a Bahadur representation,
and we now establish (18)–(19) by showing that our CM-
LHS approach ﬁts into the framework. Speciﬁcally, for the
ﬁrst result in Theorem 1 (i.e., for pm = p + O(m−1/2)),
we need to show that ˜Fm,t in (16) satisﬁes the following
assumptions from [10]:
Assumption A1: P(Mm,t) → 1 as m → ∞, where Mm,t
is the event that ˜Fm,t(x) is monotonically increasing in x
for ﬁxed m and t.
Assumption A2: E[ ˜Fm,t(x)] = F(x) for all x, and for
every am = O(m−1/2),
E
h
˜Fm,t(ξp + am) − ˜Fm,t(ξp)
i2
= [F(ξp + am) − F(ξp)]2 + sm(am)/m
with sm(am) → 0 as m → ∞.
Assumption A3: √m
h
˜Fm,t(ξp) − F(ξp)
i
⇒ N(0, ψ2
p)
as n → ∞ for some 0 < ψp < ∞.
As shown in [10], if f(ξp) > 0, then Assumptions A1–A3
imply that (18) and (19) hold for any pm = p + O(m−1/2).
Also, [10] proves that if we additionally strengthen A2 to be
true for all am → 0 and f is continuous in a neighborhood
of ξp, then (18) and (19) hold for any pm → p, which is
what we need to show for the second part of Theorem 1.
Thus, we now prove that ˜Fm,t in (16) satisﬁes A1–A3, with
A2 holding for all am → 0.
Examining (16), we see that Assumption A1 holds since
˜Fm,t(x) is always monotonically increasing in x because
each I(X(k)
i
≤ x) has this property. Also, we previously
showed Assumption A3 holds in (21), so it remains to prove
Assumption A2 holds, which we show is true for any am →
0.
Since each X(k)
i
has distribution F, we have that
E[ ˜Fm,t(x)] = 1
m
m
X
k=1
1
t
t
X
i=1
E[I(X(k)
i
≤ x)] = F(x)
for all x, so the ﬁrst part of A2 holds. To establish the
second part of A2, let ρm = min(ξp, ξp + am) and ρ′
m =
max(ξp, ξp + am) for any am → 0. Then
bm ≡ E
h
˜Fm,t(ξp + am) − ˜Fm,t(ξp)
i2
= E
 
1
m
m
X
k=1
1
t
t
X
i=1
C(k)
i
!2
,
(30)
where C(k)
i
= I(ρm < X(k)
i
≤ ρ′
m). Note that C(k)
i
and
C(k′)
i′
are independent for k ̸= k′ and any i and i′ since C(k)
i
and C(k′)
i′
correspond to outputs from different LH samples.
Thus, expanding the square in (30) gives
bm =
1
(mt)2
m
X
k=1
t
X
i=1
E[(C(k)
i
)2]
+
1
(mt)2
m
X
k=1
m
X
k′=1
k′̸=k
t
X
i=1
t
X
i′=1
E[C(k)
i
]E[C(k′)
i′
] + cm,
where
cm =
1
(mt)2
m
X
k=1
t
X
i=1
t
X
i′=1
i′̸=i
E[C(k)
i
C(k)
i′ ].
For all i and k, we have E[C(k)
i
] = F(ρ′
m) − F(ρm) ≡ dm
and (C(k)
i
)2 = C(k)
i
. Thus,
bm = 1
mtdm + m − 1
m
d2
m + cm
= d2
m + em + cm,
where
em = 1
mtdm − 1
md2
m.
Since [F(ξp + am) − F(ξp)]2 = d2
m, A2 holds if we show
that mem → 0 and mcm → 0 as m → ∞.
Now mem → 0 holds since
dm → 0
(31)
because am → 0 and F is continuous at ξp. To show
mcm → 0, note that
E[C(k)
i
C(k)
i′ ] = P(ρm < X(k)
i
≤ ρ′
m, ρm < X(k)
i′
≤ ρ′
m)
≤ P(ρm < X(k)
i
≤ ρ′
m) = dm,
so it follows that
|mcm| ≤
m
(mt)2
m
X
k=1
t
X
j=1
t
X
j′=1
j′̸=j
dm
= t − 1
t
dm → 0
as m → ∞ by (31). Thus, the proof is complete.
93
International Journal on Advances in Systems and Measurements, vol 4 no 1 & 2, year 2011, http://www.iariajournals.org/systems_and_measurements/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

B. Proof of Theorem 2
All that remains is to prove (25) for the two different cases
given in the theorem. For the ﬁrst case, it is shown in [10]
that if f(ξp) > 0, then it follows from the ﬁrst Bahadur
representation (i.e., with ξ′
pm = ξp + (pm − p)/f(ξp)) in
Theorem 1 that (25) holds when hm = cm−1/2 for any
constant c > 0. Moreover, [10] shows that when f is
continuous in a neighborhood of ξp, the second Bahadur
representation in Theorem 1 in which ξ′
pm = F −1(pm)
implies (25) holds for any hm satisfying hm → 0 and
1/hm = O(m1/2), which is what we need to show for the
second case of Theorem 2.
ACKNOWLEDGMENT
This work has been supported in part by the National
Science Foundation under Grant No. CMMI-0926949. Any
opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors and do
not necessarily reﬂect the views of the National Science
Foundation.
REFERENCES
[1] M. K. Nakayama, “Conﬁdence Intervals for Quantiles When
Applying Latin Hypercube Sampling,” in Proceedings of
the Second International Conference on Advances in System
Simulation (SIMUL 2010), pp. 78–81, 2010.
[2] A. M. Law, Simulation Modeling and Analysis, 4th ed.
New
York: McGraw-Hill, 2006.
[3] D. Dufﬁe and J. Pan, “An overview of value at risk,” Journal
of Derivatives, vol. 4, pp. 7–49, 1997.
[4] J. C. Hsu and B. L. Nelson, “Control variates for quantile
estimation,” Management Science, vol. 36, pp. 835–851,
1990.
[5] T. C. Hesterberg and B. L. Nelson, “Control variates for
probability and quantile estimation,” Management Science,
vol. 44, pp. 1295–1312, 1998.
[6] A. N. Avramidis and J. R. Wilson, “Correlation-induction
techniques for estimating quantiles in simulation,” Operations
Research, vol. 46, pp. 574–591, 1998.
[7] X. Jin, M. C. Fu, and X. Xiong, “Probabilistic error bounds
for simulation quantile estimation,” Management Science,
vol. 49, pp. 230–246, 2003.
[8] P. W. Glynn, “Importance sampling for Monte Carlo esti-
mation of quantiles,” in Mathematical Methods in Stochastic
Simulation and Experimental Design: Proceedings of the 2nd
St. Petersburg Workshop on Simulation.
Publishing House
of St. Petersburg University, St. Petersburg, Russia, 1996, pp.
180–185.
[9] P. Glasserman, P. Heidelberger, and P. Shahabuddin, “Vari-
ance reduction techniques for estimating value-at-risk,” Man-
agement Science, vol. 46, pp. 1349–1364, 2000.
[10] F. Chu and M. K. Nakayama, “Conﬁdence intervals for
quantiles when applying variance-reduction techniques,” sub-
mitted, 2010.
[11] V. G. Adlakha and V. G. Kulkarni, “A classiﬁed bibliography
of research on stochastic PERT networks,” INFOR, vol. 27,
pp. 272–296, 1989.
[12] P. Glasserman, Monte Carlo Methods in Financial Engineer-
ing.
New York: Springer, 2004.
[13] R. J. Serﬂing, Approximation Theorems of Mathematical
Statistics.
New York: John Wiley & Sons, 1980.
[14] M. M. Siddiqui, “Distribution of quantiles in samples from
a bivariate population,” Journal of Research of the National
Bureau of Standards B, vol. 64, pp. 145–150, 1960.
[15] D. A. Bloch and J. L. Gastwirth, “On a simple estimate of the
reciprocal of the density function,” Annals of Mathematical
Statistics, vol. 39, pp. 1083–1085, 1968.
[16] E. Boﬁnger, “Estimation of a density function using order
statistics,” Australian Journal of Statistics, vol. 17, pp. 1–7,
1975.
[17] P. Hall and S. J. Sheather, “On the distribution of a Studen-
tized quantile,” Journal of the Royal Statistical Society B,
vol. 50, pp. 381–391, 1988.
[18] M. D. McKay, W. J. Conover, and R. J. Beckman, “A
comparison of three methods for selecting input variables in
the analysis of output from a computer code,” Technometrics,
vol. 21, pp. 239–245, 1979.
[19] M. Stein, “Large sample properties of simulations using Latin
hypercube sampling,” Technometrics, vol. 29, pp. 143–151,
1987, correction 32:367.
[20] R. R. Bahadur, “A note on quantiles in large samples,” Annals
of Mathematical Statistics, vol. 37, pp. 577–580, 1966.
94
International Journal on Advances in Systems and Measurements, vol 4 no 1 & 2, year 2011, http://www.iariajournals.org/systems_and_measurements/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

