493
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Explorative Design as an Approach to Understanding Social Online Learning Tools
Naemi Luckner and Peter Purgathofer
Vienna University of Technology
Institute of Design and Assessment of Technology
Argentinierstr.8/178
1040 Vienna
Email: {naemi, purg}@igw.tuwien.ac.at
Abstract—The everyday availability and use of technology
has changed education as much as it has changed everything
else. For 8 years now, we have used technological interventions
to change a setting where we teach up to 800 participants
per semester in a class, in order to make it more interactive,
engaging, and interesting for the students. We document a
snapshot from this ongoing process. Aurora is an online system
that has been developed from simple experiments with existing
tools and software to bring interaction to the crowd of learners.
Over the years, it has turned into a solid and extensive collection
of tools for online teaching, learning, and communication. This
article traces the development of Aurora over two consecutive
years. We document the structure of the system we developed,
the insights from an academic year of using it, changes designed
and implemented, and ﬁrst evaluations of the use of the revised
version.
Index
Terms—Asynchronous
Interaction;
E-Learning;
E-
Portfolio; Electronic Note Taking; Explorative Design
I. INTRODUCTION
Introductory remark: This article is an extended and sub-
stantially revised version of [1].
The Web 2.0 [2] changed online culture and transformed
it from a passive consumer culture to a participatory culture.
This shift also inﬂuenced the process of teaching and learning,
which is since referred to as E-Learning 2.0. The notion
of E-Learning 2.0 is that Web 2.0 technologies are adapted
and integrated in E-Learning systems [3]. Knowledge can be
created, shared, remixed and re-purposed by communities of
practice [4]. Students are part of this process, collect sources
and participate in the communities, by sharing their own ideas
and ﬁndings. Brown and Adler [5] describe a new age of
education, in which lifelong learning is not only needed but
also supported by the participatory architecture of the Web 2.0.
They speak of a new learning approach, ’...characterized by a
demand-pull rather than the traditional supply-push mode...’ of
obtaining knowledge. They emphasize the importance of social
learning in the new online learning environment, pointing out,
that the traditional teacher-student relationship is exchanged
by a peer-based learning relationship.
Siemens [6] built on this change of learning culture and
devised his theory of connectivism. He states that learning in
the digital age is the self-driven process of building up net-
works of knowledge. Nodes in a network can be data sources,
communities or people and are connected to the network with
strong and weak links. Weak links are more interesting since
they can open doors to new areas of knowledge, diversity and
innovation. Siemens points out that the life-cycle of ’correct’
facts is getting shorter, and new knowledge is created faster,
so memorizing facts is not yielding desired results anymore
[6]. More important is the ’Know-where’, which describes
where knowledge can be found quickly rather than learning
the knowledge itself by heart.
In this paper, we present an E-Learning System with the
aim of letting students take responsibility of their own learning
process. The system is an attempt to create a holistic learning
platform, valuing not only assigned course work, but also
social interactions and additional content students create or
discover over the course of a semester. We wanted to avoid
to develop another system increasing the distance between
teacher and students. Instead, our goal was to start from the
rather difﬁcult situation of very large classes, where contact
between teacher and student is short and rare, and transform it
so that students have a feeling of more immediate involvement,
more contact, and more personal mentoring. To achieve this,
we put concepts such as social interaction, participation, and
exchange at the center of our design efforts.
Aurora is a learning platform that consists of three modules
that can interconnect with each other. Firstly, the Dashboard is
an administrative tool, containing an administrative Newsfeed
as well as widgets to enhance communication between all
participants of the course and maintain an overview of the
course progress as well as interesting developments around
the course topics. Secondly, the Slides module is used during
and after lectures as backchannel and basis for upcoming
discussions around course topics. Thirdly, students are pro-
vided with a pool of activities they can choose from in the
Portfolio. We chose the word ’activity’ rather than ’exercise’
for work assignments, since we want to motivate students
to actively pursue their work for this course and we want
to avoid the vocabulary usually associated with course work
to try to increase motivation. The Discuss module is used
for discussions surrounding the topics that are covered in
the courses. The name Aurora is not an acronym, nor has
it any deeper meaning. We used the name because it refers to
something beautiful, and because it sounds appealing.

494
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
The remainder of this paper is structured as follows: The
next section ’Overall Goal’ will give an idea of our motivation
and process to create a new e-learning system as well as
explain why we chose to develop the system ourselves instead
of taking existing tools. Subsequently, in the section ’Compo-
nents’, each module of Aurora is described and compared with
existing solutions from literature. The description is followed
by a preliminary ’Evaluation’ and ’Conclusion so far’, in
which we describe how the evaluated data inﬂuenced future
designs. The section ’Iteration and Redesign: Another Version
of Aurora’ presents a new design of the e-learning platform
Aurora that was used in the summer semester 2014. The
biggest change of this redesign is discussed in the section
’Challenges’, which is a new module replacing the Portfolio
module. Finally, a section ’Future Plans’ outlines upcoming
iterations of the platform.
II. OVERALL GOAL AND APPROACH
At the Vienna University of Technology, lecture participa-
tion is sometimes in the high three-digit numbers. Tradition-
ally, this would mean that lectures have to be endured with
a passive and consuming stance. Around 2005, we set out
to explore new ways to make lectures more interactive. We
started by appropriating existing systems like IRC and Twitter
to facilitate backchannel communication and interaction for
students visiting large lectures. Early on, we were fascinated
by the idea that we could time-sync this information to the
slides. This would enable students to understand the backchan-
nel as a means of taking (collaborative) course notes that
became attached to the individual slides of the lecture.We also
started to replace the then prevailing passive HTML web pages
for course information with blogs, which seemed an ideal ﬁt
for some years.
As we better understood the necessities of the context, we
faced two possible directions for the further development: use
existing systems and services to piece together a larger system,
or implement a whole new system according to our needs
and ideas. Comparing these two approaches, we found several
advantages of the latter over the former. One advantage is that
in a custom system, we could make sure that students get by
with a single login, as compared to multiple logins in a setting
where several existing services are stitched together. Also, if
we build the system ourselves, we can experiment much more
freely with the organisation, structuring and interaction of the
system, compared to pre-existing solution. Finally, as we are
part of an informatics faculty, this approach also gave us an
opportunity to offer meaningful master theses projects to our
students.
So, as we better understood the necessities of the situation,
we supplanted the use of an existing blogging solution with
a custom-made Newsfeed implementation that was heavily
inspired by the structures and aesthetics of social media
systems like Facebook or Twitter, still offering us more control
over composition and access for us.
By and by, we replaced all passive elements of the informa-
tion infrastructure for our large scale courses with interactive
components, we also set out to change the way we evaluate
student performance in order to come to a ﬁnal grade. This led
to a somewhat idiosyncratic redeﬁnition of a portfolio system
that we implemented.
All these systems are currently being actively developed
and reﬁned in an effort to explore new ways of teaching
and learning for a generation that grew up with ever-present
Internet access and for the most part played a lot of games [7].
We redesign our systems year after year after understanding
what works and what does not. We pursue this research in the
spirit of design as research, or explorative design. One core
idea is that with each version, new concepts become evident
that were not yet visible last year, be it from use, from formal
or informal evaluation, or because we reﬂect on our progress
from the feedback we get from students.
As approaches such as participatory design, contextual
inquiry and user involvement are deeply rooted within the
institute this project is created in, we used multiple approaches
to make sure the interests and perspectives of students are
considered in the design process. Those approaches include:
• offering the users a continuous feedback channel that was
constantly monitored by project members, making sure
that all issues are responded to accordingly;
• offering opportunities for students to do bachelor or mas-
ter projects within the project, exploring new directions
and implementing novel ideas;
• organising user testing sessions during active develop-
ment with students from previous semesters as testers;
• starting the semester with a week labelled as trial run,
where feedback was especially appreciated;
• offering exercise activities framed within the content of
the lectures using Aurora where students could reﬂect
on the concept, features and design of the system, and
propose redesign ideas;
• obtaining
structured
feedback
using
questionnaires
widely distributed among the students;
• organising semi-structured feedback rounds at the end
of each semester in order to talk to the participants
about what worked well, what did not work so well,
and what was missing. These sessions routinely turned
into co-design sessions where new ideas were proposed,
discussed and evaluated.
Following this path for some years now, we have come to a
place where individual components have been published about,
but we never set out to describe the system as a whole. This
is what this paper sets out to do.
III. COMPONENTS
Aurora is a collections of different components, each of
which takes on a vital role for the lecture to run smoothly.
There needs to be a place to publish information about how
the lecture is run and how it is graded, a place for the lecture
content and a place for work to be done by the students that
is evaluated by staff. In the following sections, we describe
each of the solutions we implemented for these requirements
in detail. Each section is preceded by a literature review of

495
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
relevant other work in the same design space, in order to
provide an overview of how others previously approached
similar problems.
A. Dashboard
Dashboards are often used in complex system to provide
participants with an overview of activity on these platforms.
The role of a dashboard is variable, depending on the context
of its application. Dashboards have been used to track activity
from different applications in a complex system [8]; to create
peripheral awareness, provide navigation, and a system-wide
inbox [9]; to create awareness of group members’ actions and
to convey the status of shared artifacts [10]; and to provide
multiple views of a large dataset in a system [11]. More
speciﬁcally, in an e-learning context, dashboards have been
used for self-monitoring for students and to improve teachers’
awareness [12]; and to help students to relate their learning
experience to that of their peers or other actors in the system
[13].
In Aurora, the Dashboard is the ﬁrst page every student is
presented with when logging into the system. It is a collection
of widgets, containing the Newsfeed, an individual course
status overview, showing colleagues, groups, current links
and additional contact information. The page draws together
course-relevant information related to the content from other
websites, as well as information from other components of
Aurora.
In former versions of Aurora, we included a statistics page
to enhance students’ peripheral awareness. The page provided
a statistical overview of the data that is distributed over the
whole system. Students could, for example, look up who of
their peers was involved in a lot of discussions, or who got
a lot of stars, which could be awarded for good comments
by other students and members of the staff. At the start,
seeing an overview of the work done in the system had
a motivating impact on students and staff alike. Especially
dedicated students could easily be singled out and earned a
good reputation and trust among their peers. However, it also
created a ranking among the students, which changed a lot
at the start, but after a while it was very hard to move up
ranks, which had a negative effect on some students. Since
we did not want to strengthen competitiveness in the course,
we ﬁrst decided to hide the statistics view from students, and,
in later versions, the view has not even made it into the system
because of a lack of time and resources for the development.
1) Newsfeed: The Newsfeed is a largely organizational
message board, but can also be used for content related post-
ings. The lecture staff can use the Newsfeed to publish course
updates and other relevant news for the students. Questions,
annotations, complaints and praise can also be posted here,
and can be answered by other actors in the system. Students
post content related comments as well, but are asked to ﬁrst
look for a suitable slide in the Slides section to provide context
for the content, before blindly posting it in the Newsfeed.
Information from other components is collected and posted
via sticky notes at the top of the Newsfeed. Students are
informed if someone answered to one of their postings in the
Slides section and can jump directly to the posting via link. If
students get points for a good comment in the Slides section or
for a newly marked activity in the Portfolio, they are notiﬁed
here. Direct messages show up on top of the Newsfeed section,
and can be sent by either colleagues or team members.
The Newsfeed enhances direct communication between
students and staff and also provides a forum for discussions
about the course design. It can be searched or ﬁltered to see
either only staff postings, only organizational postings, or only
content related postings. Students can subscribe to Newsfeed
postings via RSS to integrate them into their everyday online
environment.
2) Additional widgets: The Progress Bar widget is a tool
students can use to get an overview of their progress in each
of their classes. Each lecture has an overview of the student’s
activity status. It shows the amount of points received in the
lecture through activities and comments, as well as the total
amount of points. Additionally, it shows how much work the
student has handed in but that has not yet been graded, and
the how much the student can still hand in until the end of
the semester.
In the Colleagues widget, users can add other students to
their course network and, on acceptance, see their avatars and
further information. They can write direct messages to their
colleagues as well as see all their colleagues’ comments in the
Newsfeed and the Slides highlighted. This intends to create a
feeling of connectedness within the course and motivate to
interact with others regularly.
Some activities in the Portfolio can be worked on in teams.
The Teams widget shows a list of all existing teams the student
is a part of. Each entry contains the name of the project the
team is working on, the possibility to send a message to all
team members, and a list of the other team members.
The Current Links widget displays a list of recent articles
and interesting websites - supplementary reading material
of topics covered in the course. The collection of links is
compiled in a blog using soup.io and integrated into the
Dashboard via RSS.
Furthermore, the Dashboard lists contact information to
correspond with the staff directly. Students are invited to ask
all course relevant questions directly in the Newsfeed so that
other students can proﬁt from the answers as well, but some
issues (e.g., personal problems) need to be taken up with the
staff directly.
B. Slides
There is some research on how to offer interactivity in large
lectures. One approach are ’Audience Response Systems’, also
called ’Clickers’. Kumar and Rogers pioneered such systems
in their 1976 ’Olin Experimental Classroom’ [14] that featured
a feedback channel for students in the form of 12 buttons.
Today, clickers are commercially developed products, offering
a number of potential beneﬁts to large lectures. Caldwell
[15] summarized the literature on using clickers in lectures.
Recently, software clickers have begun to appear, based on

496
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
the fact that most students bring a network-connected device,
most prominently mobile phones, to lectures, but this approach
is still mostly experimental [16] [17].
Of course, more elaborate backchannel communication sys-
tems have been tried as well, such as ActiveClass [18],
Fragmented Social Mirror [19] or ClassCommons [20]. The
development and evaluation of these systems overlaps with the
development of the approaches presented here, ﬁrst published
in 2008 [21].
It can be argued that backchannel communication during
lectures is potentially distracting, diverting the attention from
the speaker to unrelated things. On the other hand, students
regularly bring their laptops to class in the hope of ﬁnding
productive use, but often end up being distracted by other
things that are available on their computer. We have observed
that supplying students with a backchannel that is centered
around the lecture itself brings some of that attention back,
and while it creates ’bubbles of diversion’ from the lecture
itself, at least these bubbles are focused on the content of the
lecture.
Slides consist of two major components, Livecasting and
Studio. Livecasting lets participants add notes to individual
slides of a lecture, either in the style of a backchannel
conversation, or privately. Once the lecture is ﬁnished, slides
and comments are available in a combined view in the Studio.
Participants can keep adding comments, links etc. in the
Studio, so that the lecture slides become the focal point of
discussion and exchange for participants and lecturers alike.
1) The Livecasting component: During a lecture, the lec-
turer runs a script on her computer. By pressing the next
slide button on an accordingly conFig.d remote, she triggers a
script that sends the number and title of the newly displayed
slide to the Livecasting server (Fig. 1). Additionally, the script
retrieves the lecture notes of this slide in the presentation
document and scans them for a custom-made meta-syntax
signifying information that is meant to be posted with the same
slide. These text-lines include explanations, enhanced quotes,
references and other links, activities and discussion starters.
Students load a web page that changes with each slide the
lecturer shows, offering them ﬁelds to enter public comments
and private notes. Information entered into either of these
ﬁelds ends up being attached to the slide that was visible
when the participant started typing. Like in a chat system all
connected participants can see the public comments entered
by other participants, and they can reply to these comments,
creating ad-hoc discussions of the lecture content. To ease
the cognitive load, a participant’s own comments are colored
yellow. Additionally, students have the opportunity to mark
slides as ’liked’, ’important’ or ’unclear’ with a single click.
2) The Slides Studio: Once the lecture is over, the lecturer
makes slide-by-slide images of his presentation available to
the Slides module (Fig. 2). While this can also be done before
the lecture, we decided not to show the slides because of the
obvious spikes in network trafﬁc this would generate whenever
a new slide is shown. The slides, all the participants’ comments
as well as the lecturers automatically posted comments are
Fig. 1. Structure of the Livecasting setup. While the lecturer talks about a
slide, connected students are offered text entry ﬁelds where they can attach
public comments or leave private notes with the slide. Because public
comments are immediately visible to all other students, this creates a setup
similar to an instant messenger that is contextualized by the slide currently
projected for everybody to see.
then made available in the Studio.
Here, participants and lecturers can post comments even
after the lecture is ﬁnished. In the Studio, the slides are
arranged horizontally, sorted by their time of appearance in
the lecture. The comments attached to each slide are laid
out vertically, with the earliest comments up on top (usually,
these are the comments posted by the script on the lecturers
computer immediately when the slide is shown), with reply
threads sorted in the same way.
Participants can give praise to good comments by clicking
the star next to the avatar of the author, in which case
the star turns yellow and shows the number of clicks it
has accumulated. Lecturer can use this same mechanism to
award points to outstanding comments. In this case, the star
is distinguished with a green glowing outline, making its
commendation visible to everybody.
While lecturer’s comments are generally displayed in the
same way as student comments but distinguishable by a light-
blue color, there are two lecturer-posted types that stand out
from the rest: discussion starters and activities. Comments of
both these types are arranged between the slide and the ’private
notes’ border, thus standing out even when scrolling through
the slides quickly.
Discussion starter comments typically contain a questions
and an invitation to discuss this question in the comments
of the slide. We use this mechanism to initiate discourse
on the content of slides worth of discussion, and to initiate
discourse between lectures, asking participants to discuss
upcoming content. Activities contain a brief explanation of an
activity, linking into the Portfolio system where an elaborate
description of this activity can be found. This gives the lecturer

497
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Fig. 2. Slides Studio, where all slides and comments become accessible to participants and lecturers alike. Note that your own notes are colored yellow.
an opportunity to announce new activities that derive from the
content of a slide. Activities comments link students to the
Portfolio of Aurora, where they hand in their work for review
and evaluation.
C. Portfolio
In areas like HCI or Informatics and society, it is hard to
conduct written exams, and once you have more than a couple
of hundred students, it becomes impractical to the point of
impossible to conduct oral exams. We started to abandon tests
and exams at some point when we made the observation that
the prospect of a written exam changed what we taught. This
compromises the whole idea of teaching and learning a subject
matter, especially at the university level.
For a couple of years now, research papers have been
explaining the theoretical sense the adoption of ePortfolios
would make. Advantages implied are, among others, ’im-
proved reﬂection, increased student engagement, improved
learning outcomes, and increased integration of knowledge’
[22]. The paper quoted gives a comprehensive overview over
ePortfolio research, and points out the lack of empirical
support for many of the asserted advantages.
The module we call Portfolio is not really an ePortfolio
in the strict sense of the word. While we explicitly ask
the students to upload artifacts that show what they have
learned, we offer a large catalog of predeﬁned activities that
can be handed in here (Fig. 3). These activities include a
broad range of tasks, from simple applications of theoretical
content, to actions reﬂecting their own prior projects, to com-
plex design exercises. Many of those activities would make
viable exercises in a traditional deadline-based context, while
others would be quite unsuitable for such an environment.
The catalog also contains meta-activities such as ﬁnding new
sources, suggesting new activities, and organizing round table
discussions with experts in the ﬁeld. No single activity yields
substantially more than 10% of the ﬁnal grade, so that students
will be exposed to a broad range of topics.
Additionally, we attached a commenting section to each of
the activities, designated as ’Q&A’-area, where students can
ask questions regarding the activity that will be answered by
the course organisers.
Participants hand in their work using the portfolio system of
Aurora. We do not set any speciﬁc deadlines other than the end
of the semester, and we do not expect them to follow a speciﬁc
order. The only requirement they have to meet is to make sure
that their work is distributed throughout the semester, instead
of congested at the end. To this end, we devised a system to
keep students on track by pushing them to regularly hand in
work over the semester.
Each students has to reach a certain amount of credits in
order to successfully ﬁnish the course. Each activity is worth
a given amount of credit points that, when handed in, count
towards the ﬁnal grade. During the semester, students manage
a certain contingent of ’possible points’ they can hand in at any
given moment. The number of credit points of each activity
handed in are subtracted from this contingent, leaving the
student with only a small amount of points left to be handed in
at that time. Each day though, the contingent is replenished a
little, to the point where it is full. If the student does not hand
in activities before the contingent is totally reﬁlled, they start
losing those points. On the other hand, as long as the student
does not have the needed amount of points in their contingent,
they cannot hand in new activities. That way, students have
to continuously hand in activities over the semester, but can
chose when to do so. They get enough points to easily ﬁnish
the course, even if they loose some of those points along the
way. This system basically makes sure that if students waited
too long into the semester, they would be unable to accumulate
enough credit to complete the course.
The Portfolio includes an easy-to-use review component for
the course admins to review and evaluate the participants’

498
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Fig. 3. Portfolio view of a participant, with the catalog of available activities, shown as cards, in the left column, a working area in the middle titled ’Deine
Activities’ (Your activities), and the area for hand-in in the right column. A student would drag an activity card from the catalog into the working area to
elaborate, and later drop it into the hand-in column for evaluation.
work, with the notable addition of enabling students to hand in
repeated submission of work that failed to meet the standards.
It also includes a double blind peer review component that
makes part of the assessment process into an activity by itself
on the premise that if you do an honest review of somebody
else’s work, you will learn a lot. This functionality was used
for an activity where students wrote in-depth articles of some
more or less freely chosen course content, targeted at an
imaginary journal. The articles were then double blind peer
reviewed by other students.
The organisational approach described here tries to abandon
the usual scattering of deadlines through the semester, giving
the students a lot of autonomy in their work, which self-
determination theory deems essential for intrinsic motivation
[23].
D. Discourse
Discussion systems are widely used online, also in an
educational context. Research shows that discussions foster
active student participation and knowledge transfer [24], train
critical thinking skills [25], and are used as a communication
channel between students and teachers [26]. In the context
of this project, a discussion system has been created, with
an emphasis on redesigning threaded discussion systems to
effortlessly join long discussions and easily follow single
discussion threads. The effect of layout generally ( [27], [28],
[29]) and of layout in discussion systems ( [30], [31]) has been
discussed before.
In our opinion, traditional online discussion forum systems
share a couple of common problems. For example, as the
number of postings grows, readers lose track of all the places
where they posted something. This often leads to users reduc-
ing their involvement in order to retain the feeling of control.
Another problem is that intense discussions between individual
participants can quickly derail a discussion, making useful and
on-topic contributions hard to ﬁnd. Problems like these seem
to come from the way information is presented to the user,
suggesting that a better visual structure and more adequate
interactive organisation could improve on these problems.
Thus, we set out to design and implement a completely new
system for online debates.
Discourse is an asynchronous, multi-threaded discussion
system, that can be used for on-topic discussions among
students. Lecture staff can post discussion starters or additional
material to a slide. Students can navigate to the discussion
via a link, which opens an inﬁnite discussion canvas, inspired
by Scott McCloud’s Inﬁnite Canvas idea [32]. There, each
discussion is displayed in two dimensions: vertically and
horizontally. The vertical axis is used for new ideas, thought
and inputs into the discussion. Each comment can be replied
to, which in turn creates the second, horizontal dimension:
replies are displayed in a new column to the right of the
original reply. Only one thread of the discussion can be opened
horizontally at any given time. Fig. 4 shows an example of an
open discussion thread.
A more detailed description of this format as well as an
elaborate evaluation of the effect of this speciﬁc layout and
interaction on the content of the discussion can be found in
[33].
IV. EVALUATION
Our focus in evaluating these components is a better under-
standing how we can advance the system. We do not have
an ultimate goal, but we use both the design process, in
the sense of ’doing for the sake of knowing’ [34], and the
evaluation to understand how the system should be enhanced,

499
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Fig. 4. A horizontal discussion thread. Selected items are displayed grey and with a dashed outline. Information boxes on the top right of each comment
provide a quick overview of the rest of the discussion thread.
reﬁned and changed in order to satisfy our needs as teachers
as well as the needs of the students as learners. We understand
that each step in this process changes the situation, leads all
participants to new and often unforeseeable behaviour, which
in turn inﬂuences and challenges all the assumptions we made
to come to this point. This is why we refer to this process as
explorative (or exploratory) design.
TABLE I. The table shows how many people were involved and how many
certiﬁcates were handed out in the lecture. Note: columns do not add up
because teachers and students could be associated with both courses.
Profs
Predoc
Tutors
Students
Certiﬁcates
BHCI
3
1
6
733
442
IST
1
1
4
521
337
Total
3
1
10
842
779
This partial evaluation is based on data from two courses,
Basics of Human Computer Interaction (BHCI) and Interac-
tions of Society and Technology (IST), which took place in
the summer semester of 2013. A total of 11.793 activities was
handed in over the course of the semester, 7126 in BHCI and
4667 in IST. The staff of both courses combined consisted of 3
professors, 1 predoctoral fellow and 10 tutors, exact numbers
can be found in Table I. Students only got a certiﬁcate if they
handed in at least one activity. Every student who ultimately
received a certiﬁcate handed in 15 activities on average. In the
Slides section, 1283 slides were posted distributed over two
courses with 23 lectures in total, and 3975 comments were
written during and after these lectures.
A. Portfolio evaluation
Fig. 5 shows a pie chart of the time it took to grade
activities. One third of the activities were graded after a week,
which would be an acceptable amount of time for students to
wait for feedback. Given the student-staff ratio, we tried to
achieve a maximum waiting time of three weeks until every
activity is graded. As can be seen in Fig. 5, we were not able
to reach that goal, as only two thirds of handed in work was
evaluated within the given time frame. The ﬁnal third of the
pie chart consists of activities that took 4 and more weeks to
be graded. Considering the importance of feedback in order
to keep students motivated and continuously working [35], 4+
weeks seems too long a time to hear back on one’s work.
Fig. 5. Time it took to grade an exercise, calculated in weeks
We suspect that this ﬂuctuation in delay can actually be
explained by queue modeling in game theory. Activities tend to
be handed in unequally distributed time, leading to an overload
that causes congestion and that is then almost impossible to
resolve within the given resources until the end of the semester.
B. Slides evaluation
Fig. 6. How long after a slide was posted (Day 0) are students interacting
with it via the comment stream. Note that the ﬁrst column had to be
shortened as indicated for reasons of scale.

500
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
A: opinion
B: opinion
↓
C: statement
D: opinion
↑
E: opinion
↑
F: opinion
↑
C: opinion
↑
G: opinion
H: example
↑
I: opinion
↑
J: opinion
K: example
↑
L: opinion
M: opinion
↑
N: example
O: clariﬁcation
P: opinion
↓
P: clariﬁcation
Q: example
R: opinion
↑
+ 57 isolated postings
initial statement + questions
Fig. 7. The structure of a discussion in the traditional discussion, using the Slides Section
In Fig. 6, the comment data was analyzed to ﬁnd out if and
how long after the lecture students engage with the content by
writing comments and discussing it in the Studio. The graph
shows that most comments are written during the lecture, but
there is a long tail (going up to 75 days) after the original slide
was presented. Approximately 120 comments were posted
even after the semester was over.
Especially interesting for us is the peak a couple of days
later, as well as the ’long tail’ of posted content after the
lecture that can be seen in Fig. 6. An evaluation of these ’late’
postings show that students came back to post information
they ﬁnd relevant, like news coverage, examples, references,
etc. or to partake in discussions they have started with another
postings. We see this as a successful feature of the system, as
it induces reﬂection on and occupation with the content of a
lecture for quite some time after the lecture is over.
C. Discourse evaluation
Two types of discussion systems have been used in two
consecutive years; the ﬁrst system used was the Slides Studio
of Aurora, which features a traditional, one-level deep vertical
representation of the comments. The seconds system used was
the Discourse module. In each year, students were invited
to join in voluntarily, and were rewarded credits towards
their ﬁnal grade for well written comments. For example
comments that were argued conclusively and that featured
links to additional material and sources.
We evaluated the outcome of 5 corresponding pairs of
discussions on the same topic, one held in the vertical dis-
cussion section, the other in the 2-dimensional counterpart.
For each of these discussions, we mapped the course of the
debate as a tree structure, the results of which can be exam-
ined online at http://igw.tuwien.ac.at/designlehren/discourse\
evaluation.html. One of these evaluations is presented here in
this section, for more details, please refer to [33].
The discussion presented here was held on the topic of
phishing. We analysed the structure using a qualitative content
analysis based on Mayring [36]. Each posting was coded
as either Statement (facts proved with sources), Opinion,
Question, Answer, Material, Example, Clariﬁcation, or Insight
(for details to these categories, please refer to [33]). Over
all, most postings were identiﬁed as Opinion, which are
position postings without sources. In the traditional threaded
’vertical’ discussion format we also found postings of the
type Statement, Example and Clariﬁcation. In Discourse on
the other hand, discussions seemed to be more diverse in the
types of postings that students wrote, much more engaging
with more students contributing more than one posting, and
much more in-depth as indicated by the fact that we found
even comments coded as Insight, which are realizations of
something that was so far unknown to them.
Fig. 7 shows the structure of the 1-dimensional threaded
discussion. It was attended by 63 students and contained 77
postings, 57 of which were isolated postings. Only about 17%
of the comments were written as a reply to someone else.
Of the 63 participants, only 10 wrote more than 1 comment,
and only 2 students ended up writing 3 comments each. The
average amount of comments per students was 1.2, so most
just wrote one isolated posting and left the discussion straight
away.
The structure of same discussion in Discourse can be seen
in Fig. 8. It was joined by 29 students and resulted in 50
comments, 19 of which were isolated postings. 50% of all
comments created in the discussion were in reply to other
comments. 8 students wrote more that 1 comment, 5 of them
more than 3. Overall, each student wrote 1.7 comments.
The overall impression is, that Discourse leads to students
being more involved in the discussion and even coming back
to read up on new postings. The content seems more diverse

501
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
M: clariﬁcation
↑
A: example
↑
A: answer
L: question
E: opinion
↓
J: insight
↑
D: opinion
↑
K: example
↓
B: opinion
↓
J: opinion
↓
I: opinion
↓
D: opinion
↑
D: opinion
↑
G: opinion
D: materials
↑
F: materials
↑
D: example
↑
A: clariﬁcation
↓
D: insight
↓
A: clariﬁcation
A: materials
↑
K: question
↑
A: opinion
F: opinion
H: statement
M: statement
+ 19 isolated postings
B: opinion
↑
C: example
↑
E: opinion
↓
M: materials
↑
M: example
↑
initial statement + questions
Fig. 8. The structure of a discussion using Discourse
and, in some cases, even signiﬁes that learning occurred due
to the discussion. The difference in the amount of isolated
postings (19 vs. 57) shows that in Discourse, more students
were motivated to ﬁnd the correct discussion thread to post
their own comment to, rather than just write isolated postings
with no connection to what happened in the discussion until
then.
V. LESSONS LEARNED
The main goal of our work is to explore the design space of
online teaching and learning support systems. Our approach
is best described as explorative design, with the main goal to
better understand the context, the players, and their needs. At
the same time, we acknowledge that technological interven-
tions also transform the situation, and also, to a lesser extent,
the needs of the players. In building and using systems that
implement novel approaches to the context of teaching and
learning, we in turn have a chance to understand the change
such systems bring into the situation, and react accordingly.
This approach shifts the focus of evaluation from understand-
ing how and why the approach worked, or failed to work,
to understanding and assessing the impact of an approach
on a situation, and ultimately to ﬁnding new approaches to
try. In the end, we are not so much focused on proving that
our approach is right, e.g., by showing effectiveness by some
abstract learning measurements. Instead, we want to ﬁnd new
and better ways to teach and learn that use the potentials of
new technologies, engage and motivate students and tap their
self-motivational capabilities.
The following conclusions were drawn from the use of the
version of Aurora described in this paper so far:
A. Dashboard
The way the Newsfeed works to unify all organisational
communication as well as general questions and discussions
into one stream is promising. However, more effort has to be
put into promoting important messages, which sometimes tend
to get lost in the constant stream of incoming comments.
B. Slides
Slides demonstrates the potential to make content more
interactive using novel forms of presentation. Discussions
form around individual slides, students contribute additional
resources and material, use the Slides module to pose ques-
tions and ask for clariﬁcation, and even share entertaining
associations. The high granularity in the presentation makes
it possible to post such contributions quite targeted, albeit for
the price of overview. However, when a couple of hundred
participants post comments, overview is hardly something one
would expect to preserve.
A recurring critique of some students is the way information
is organised in the Slides module. Speciﬁcally, those students
who do not feel comfortable with horizontal scrolling, often
due to constraints posed by their computer hardware, ex-
pressed a dissatisfaction with the principal structure of Slides
Studio.
Also, the question prevails whether the slides used in the
lecture sorted by date are the ideal organisational scaffold for
such conversations. Slides often over-emphasise examples and
illustrations, as those parts of the lecture often use more slides
than abstraction, concepts and ideas, which are thus pushed
into the background.
C. Portfolio
The use of a portfolio-based approach in the Portfolio mod-
ule provided for ﬂexibility and versatility unparalleled in prior
version of Aurora, or compared to a traditional deadline-based
course organisation. Many students appreciated the freedom
and choice that come with such a system, while some students
are clearly overwhelmed with the necessity to show such a

502
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
degree of self-organisation and self-discipline. Students did
complain that the ’many small exercises’ principle causes them
to only superﬁcially get in contact with a lot of interesting
themes and questions, leaving them without a possibility to
delve deeper and engage with some of the themes more in-
depth.
At the same time, the tutors were ﬂooded with a very large
number (11.000+) of small exercises to evaluate, making it
impossible to write explicit feedback and grade the hand-ins
in a timely manner. One deﬁnite goal for the next version of
Portfolio was to ﬁnd an organisational form that enables us to
keep the average waiting time for feedback within a week of
hand-in.
On the other hand, it turned out that the ’Q&A’ commenting
section with each activity description was a advantage for the
students. The amount of work generated is minimal compared
to the value it has. For example, ambiguities in the description
of an activity could be cleared up here quite efﬁciently and
without generating version conﬂicts in the description.
D. Discourse
Discourse worked exceptionally well, resulting in more fo-
cused discussions, heightened participation and more interest-
ing conclusions. Unfortunately, the student who implemented
Discourse as part of his master thesis left at the end of the
semester, leaving a system that worked reasonably well within
the technical context of the old version of Aurora, but was very
hard to incorporate into the substantially rewritten version that
we used in the following year.
VI. ITERATION AND REDESIGN: ANOTHER VERSION OF
AURORA
After the evaluation of our experiences with the version of
Aurora described in the paper so far, we set out to reorganise,
redesign and implement a new version of Aurora. The resulting
version has been developed during winter semester 2013 and
used during summer semester 2014. In the following, we will
brieﬂy describe the changes planned, the redesigns taken and
ﬁrst results from the use of this new version.
A. Dashboard and Newsfeed
Based on last year’s version, Newsfeed was improved in
several places (Fig. 9). We added a dynamic ﬁltering mech-
anism that lets user selectively shrink and expand individual
Newsfeed postings based on criteria, resulting in views that
e.g., show only comments posted by course admins, only new
postings, or only top level postings. Especially the ’new com-
ments’ ﬁlter was highly effective, showing all contributions
posted since this ﬁlter was last activated.
We added an opportunity to up- and down-vote individ-
ual postings, inspired by social media sites like reddit.com.
Additionally, we added an element that allowed comments
to be ’bookmarked’, with an aggregation of all bookmarked
comments in a separate view. These changes were effective
for all instances of commenting in Aurora, not only in the
Newsfeed.
Fig. 9. The Dashboard in the new version of Aurora. The left column is
allocated to the Newsfeed, the right column offers the other dashboard
widgets such as the points overview and general information widgets
(collapsed) and the FAQ widget. The notiﬁcations widgets was granted a
privileged spot in the menu bar of Aurora, here indicating 6 unseen
notiﬁcations.
One of the minor changes that penetrated the whole system
was the optimization of the overall page dimensions to a width
of 960px to make Aurora better suited for mobile screens.
B. Slides
The Slides module was effectively unchanged (Fig. 10). Due
to technical problems we were unable to reactivate some of
the functions of the Slides module in time for the course.
Speciﬁcally, the Livecasting component did not work, so that
comments could only be added after a lecture was ﬁnished.
As a consequence, the amount of comments posted in the
Slides Studio was much lower than last year.
Fig. 10. The Slides Studio in the new version of Aurora.

503
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
C. Portfolio
Following the problems with the portfolio approach de-
scribed above, the principal organisation of the Portfolio
module was dropped, and replaced by the ’Challenges’ module
described below.
D. Discourse
Due to the problems described above, the Discourse module
could not be used this year.
VII. CHALLENGES
After abandoning the quite ambitious e-portfolio approach,
we introduced a slightly gamiﬁed terminology in the following
year, with challenge as the main metaphor for a complex set
of exercises called tasks.
In the context of Aurora, a challenge is a chain of exercises
called tasks where each task (except the ﬁrst task) requires the
completion of the preceding task (Fig. 11). These exercises are
usually increasing in their difﬁculty, and each task is based on
the outcome, the skills, or the knowledge gained from the
preceding task. Challenges are composed from three to ﬁve
tasks, with the ﬁnal task being signiﬁcantly more work than
the preparatory tasks. Typically, a single challenge represents
between 10% and 25% of your semester goal.
Students can choose from a catalog of challenges that grows
over the semester (Fig. 11 left), following the lecture content.
In the end, the catalog comprised of 15 challenges in each
course, offering more than 250% of the points necessary to
reach the semester goal. The content in each course was
divided into four chapters, and students had to choose at least
one challenge per chapter to ensure exposure to a balanced set
of topics.
As in the previous Portfolio module, we did not set dead-
lines other than the ﬁnal end-of-term deadline. Students had
to have enough challenges worked through and handed in at
the end of the semester in order to fulﬁll the semester goal.
We want to deter students from postponing their work until
the end of the semester, so we introduced an organisational
constraint, where after handing in a completed challenge you
have to wait a number of days until you can hand in another
challenge.
The ﬁnal task represents approximately 50% of the total
amount of credits that can be reached in a challenge, and
it is evaluated and graded by a staff member or tutor. All
preparatory tasks leading up to the ﬁnal task are submitted
into a double blind peer review process, with other course par-
ticipants as reviewers. Consequently, for every task a student
hands in, they have to review three elaborations for the same
task handed in by anonymous colleagues. This has the strong
pedagogic appeal that you expose students to the work of their
peers immediately after they did the same work, leading to
not only an exposure to different perspectives on the same
material, but also to a guided reﬂection on their own work.
The double blind peer review process was modelled after the
way it is typically organised at conferences; work was assigned
randomly to reviewers who were required to answer a couple
of questions covering areas such as completeness, correctness,
objectivity or originality of the reviewed work. Finally, review-
ers were asked to assess the work on a quadrinomial scale
ranging from ’Great work’ to ’Unacceptable Work’, the latter
reserved for plagiarism and empty hand-ins.
Participants can work through all preparatory tasks of a
challenge without regard for the reviews received. In order
to access the ﬁnal task of a challenge, it is necessary to have
at least two positive reviews for each preparatory task.
To maintain the level of quality, tasks as well as reviews
were randomly checked by members of the staff. We injected
bad and plagiarized work into the review process, and also
informed students about it, in order to be able to detect
students who systematically refused to invest adequate time
into writing their reviews. Also, we implemented an easy way
for students to report meaningless reviews they received.
A. Evaluation of Challenges
1) Grading Time: One of the main goals of Challenges
was to relieve us of the evaluation overload. Looking back at
the semester, we can say that the use of double blind peer
reviewing clearly reduced our work load, resulting in a much
shorter time-to-evaluation for the students.
If you compare the main pie chart in Fig. 12 with Fig.
5, you will see that the average time for feedback was
reduced signiﬁcantly. We are optimistic that we can reduce
it even more, as an organisational mishap in the middle of the
semester generated a week of hand-in frenzy, which led to a
substantial increase in feedback time; before that week, almost
90% of all hand-ins were evaluated within a week.
Part of that beneﬁt results from a lower number of hand-ins.
Instead of up to twenty submissions from each student that had
to be graded in the portfolio, we now received six or seven
challenges typically handed in by students that subsequently
passed. While each challenge consists of three to ﬁve separate
task elaborations that have to be checked, the reviews attached
to all but the ﬁnal task hand-in helped speed up evaluation and
grading signiﬁcantly.
Additionally, we were now able to provide more substantial
feedback to the ﬁnal task elaboration, as the reduced total num-
ber of hand-ins to be evaluated leave more time to compose
written feedback. Finally, students no longer complained that
it was impossible to engage with some subject matter more
in-depth, as challenges provided ample opportunity to delve
deeper into any of the offered topics.
2) Peer Reviewing: One of the risks of the introduction of
peer reviewing is associated with the fact that students could
start any challenge at any time, making it unclear whether they
would also receive enough reviews for them to be able to start
the ﬁnal task of the challenge in time. To compensate for this,
we had a separate list of all hand-ins that did not receive the
necessary two reviews 72 hours after hand-in. Tutors regularly
checked that list, providing substitute reviews so that students
could advance to the ﬁnal task of the challenge.
As it turned out, this list was empty most of the time. Only
towards the end of the semester did the work of the students

504
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Fig. 11. The three views needed to navigate challenges: [left] Catalog of all challenges (three shown), sorted with latest additions on top; [middle] All tasks
in a challenge; [right] A single task view, with the associated Q&A area
Fig. 12. Time it took to grade an exercise in Challenges, calculated in weeks
diversify so much that they did not receive these minimal
two reviews. Of course, the last students to start each task
at the end of the semester also had to rely on tutor substitute
reviews. In other words, peer reviewing worked very well, and
thus most feedback from the students about the peer reviewing
mechanism was very positive.
One problem that stood out was the quality of some of
the reviews. Some students saw reviewing as an annoying
appendage to the core task of working through the challenge,
so they wrote predominantly short reviews free of any sub-
stantial feedback. We plan to tackle this problem by placing
more importance on good reviewing, e.g., by introducing a
review reputation value and incorporating this value into the
ﬁnal grade, or by designing an introductory ’meta’-challenge
that explains and focuses on the reviewing process.
On the other hand, we received feedback from students
who pointed out reviewing as an essential component of the
overall experience. The combination of ﬁrst working through
a task, and then reviewing the work of others for that same
task was described as very interesting experience, central to
what they learned in this course. They also rather liked the
reviews they received for their own work, as long as they
were substantial enough. Overall, we have the impression that
the system leads to a higher involvement in the course, at least
for those students who want to get involved.
B. Scope of Challenges
The speciﬁc structure of a challenge, being comprised of
multiple preparatory tasks escalating into the ﬁnal task, made
it very hard and sometimes impossible to translate a number
of the more exotic activities from the Portfolio into the new
structure. Especially activities best described as meta-activities
like ﬁnding new sources, suggesting new content, detecting
and correcting mistakes on the slides, or suggesting new
activities were hard to incorporate into a challenge. That
way, the organisational structure was a step back into more
conventional exercise territory. For us, this drawback is more
than compensated by the fact that the tasks in the challenge
build on each other offer a way to lead students deep into a
subject matter, offering guidance and focus.
C. Conclusions
With the structure and organisation of the Challenges mod-
ule, we believe to have solved a number of our core problems.
The delay between hand-in and feedback was down across
the board, from the very quick peer reviewing process to the
overall evaluation of the whole challenge. As a result, we were
able to send students their certiﬁcates signiﬁcantly faster than
in any prior year. While quality problems with the double blind
peer reviewing were observed as anticipated, we are conﬁdent
that we can develop concepts to counter those problems.
VIII. FUTURE PLANS
As described in Section VII, we consider the Challenges
module a huge step in the right direction, and plan to enhance
and strengthen it in the suggested ways. We think that we
’cracked’ double blind peer reviewing in the context of large
university classes.
Slides will probably undergo a major revision for next year.
While we consider the general concept to offer a conversa-
tional structure following the course content as viable, doubts
are mounting whether the slides are in fact the best scaffold
for such a structure.
We are already thinking about a better Newsfeed structure,
to alleviate the problems observed this year. Due to a much
higher conversational ’background noise’ in the Newsfeed this

505
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
year, many students complained that vital information went
under their radar.
The value of having a single channel of communication for
a course, without the need to hunt around and look through
several modules to ﬁnd all relevant information and answers
cannot be overstated. So far, we have failed in ﬁnding a
suitable structure, not only but also due to the fact that students
never use the offered structures in the way we intend them to.
We understand this as a design challenge for the coming years.
Another step in Aurora’s development will be to include
our discussion component Discourse into the system. The
gains from using Discourse to facilitate discussions around
the course content were substantial.
Handling more than 500 students in university courses
is a rare situation. Often, such a challenge is tackled by
introducing distance between teachers and learners, and by
relying on examination and tests. This removes autonomy from
the learning process, which we see as a central property.
Thus, we tried to go the opposite way, and designed Aurora
with the explicit goal to give students as much autonomy
as possible in such a setting. In our experience, such a
challenge requires explorative approaches, learning not only
from evaluation, but also from the design process itself.
ACKNOWLEDGMENT
The authors would like to thank all contributors who have
been involved in the development of Aurora over the years:
Stephan Bauer, Elisabeth Bauernhofer, Christoph B¨orner,
Daniel Domberger, Michael Emhofer, Andreas Fermitsch,
Martin Flucka, Thomas Gradisnik, Peter Holzkorn, Daniel Ke-
ceci, Lucia Leitner, Peter Minarik, Wilfried Reinthaler, Gerald
Reitschmied, Diane Salter, Reinhard Seiler, Martin Sereinig,
Raif Tabucic, Bruno Tunjic, Wolfgang Zalesak. Furthermore,
we would like to thank all tutors and students who used Aurora
over the years for their invaluable feedback and bug tracking
work.
REFERENCES
[1] P. Purgathofer and N. Luckner, “Aurora - Exploring Social Online Learn-
ing Tools Through Design,” in Proceedings of The Seventh International
Conference on Advances in Computer-Human Interactions (ACHI 2014),
Barcelona, Spain, 2014, pp. 319–324.
[2] T. O’Reilly, “What Is Web 2.0: Design Patterns and Business Models
for the Next Generation of Software,” Design, vol. 65, pp. 17–37, 2007.
[3] S. Downes, “Feature: E-learning 2.0,” Elearn magazine, vol. 2005,
no. 10, p. 1, Oct. 2005, [Accessed Jan. 24, 2014]. [Online]. Available:
http://doi.acm.org/10.1145/1104966.1104968
[4] E.
Wenger,
Communities
of
Practice,
Learning,
Meaning,
and
Identity, 1998. [Online]. Available: http://www.stanford.edu/∼eckert/
PDF/eckert2006.pdf
[5] J. S. Brown and R. P. Adler, “Minds on Fire: Open Education,
the Long Tail, and Learning 2.0,” Educause Review, vol. 43 (1),
pp. 16–32, 2008, [Accessed Jan. 24, 2014]. [Online]. Available:
http://www.educause.edu/ir/library/pdf/ERM0811.pdf
[6] G. Siemens, “Connectivism: A Learning Theory for the Digital
Age,” 2004, [Accessed Jan. 24, 2014]. [Online]. Available: http:
//www.elearnspace.org/Articles/connectivism.htm
[7] M.
Irvine,
“Survey:
97
Percent
Of
Children
Play
Video
Games,”
2008,
[Accessed
Jan.
24,
2014].
[Online].
Available:
http://www.hufﬁngtonpost.com/2008/09/16/
survey-97-percent-of-chil\ n\ 126948.html
[8] J. L. Santos, S. Govaerts, K. Verbert, and E. Duval, “Goal-oriented
Visualizations of Activity Tracking: A Case Study with Engineering
Students,” in Proceedings of the 2Nd International Conference on
Learning Analytics and Knowledge, ser. LAK ’12.
New York, NY,
USA: ACM, 2012, pp. 143–152, [Accessed Jan. 24, 2014]. [Online].
Available: http://doi.acm.org/10.1145/2330601.2330639
[9] C. Treude and M.-A. Storey, “Awareness 2.0: Staying Aware of
Projects, Developers and Tasks Using Dashboards and Feeds,” in
Proceedings of the 32Nd ACM/IEEE International Conference on
Software Engineering - Volume 1, ser. ICSE ’10.
New York, NY,
USA: ACM, 2010, pp. 365–374, [Accessed Jan. 24, 2014]. [Online].
Available: http://doi.acm.org/10.1145/1806799.1806854
[10] J.
T.
Biehl,
M.
Czerwinski,
G.
Smith,
and
G.
G.
Robertson,
“FASTDash: A Visual Dashboard for Fostering Awareness in Software
Teams,” in Proceedings of the SIGCHI Conference on Human Factors
in Computing Systems, ser. CHI ’07.
New York, NY, USA: ACM,
2007, pp. 1313–1322, [Accessed Jan. 24, 2014]. [Online]. Available:
http://doi.acm.org/10.1145/1240624.1240823
[11] M. McKeon, “Harnessing the web information ecosystem with wiki-
based visualization dashboards.” IEEE transactions on visualization and
computer graphics, vol. 15, no. 6, pp. 1081–1088, 2009, [Accessed Jan.
24, 2014]. [Online]. Available: http://www.ncbi.nlm.nih.gov/pubmed/
19834175
[12] S. Govaerts, K. Verbert, J. Klerkx, and E. Duval, “Visualizing
activities for self-reﬂection and awareness,” Advances in Web-Based
Learning ˆa ICWL 2010, vol. 6483, pp. 1–10, 2010, [Accessed Jan.
24, 2014]. [Online]. Available: http://link.springer.com/chapter/10.1007/
978-3-642-17407-0\ 10
[13] E. Duval, “Attention Please!: Learning Analytics for Visualization and
Recommendation,” in Proceedings of the 1st International Conference
on Learning Analytics and Knowledge, ser. LAK ’11.
New York,
NY, USA: ACM, 2011, pp. 9–17, [Accessed Jan. 24, 2014]. [Online].
Available: http://doi.acm.org/10.1145/2090116.2090118
[14] V. K. Kumar and J. L. Rogers, “Student Response Behaviors in an
Instrumented Feedback Environment,” SIGCUE Outlook, vol. Special,
pp. 34–54, Dec. 1978, [Accessed Jan. 24, 2014]. [Online]. Available:
http://doi.acm.org/10.1145/1318457.1318461
[15] J. E. Caldwell, “Clickers in the Large Classroom: Current Research
and Best-Practice Tips,” CBE-Life Sciences Education, vol. 6, no. 1,
pp. 9–20, 2007, [Accessed Jan. 24, 2014]. [Online]. Available:
http://www.lifescied.org/content/6/1/9.abstract
[16] D. Lindquist, T. Denning, M. Kelly, R. Malani, W. G. Griswold,
and B. Simon, “Exploring the Potential of Mobile Phones for Active
Learning in the Classroom,” SIGCSE Bull., vol. 39, no. 1, pp.
384–388, Mar. 2007, [Accessed Jan. 24, 2014]. [Online]. Available:
http://doi.acm.org/10.1145/1227504.1227445
[17] S. Teel, D. Schweitzer, and S. Fulton, “Braingame: A Web-based
Student Response System,” J. Comput. Sci. Coll., vol. 28, no. 2,
pp. 40–47, Dec. 2012, [Accessed Jan. 24, 2014]. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2382887.2382895
[18] M. Ratto, R. Shapiro, T. M. Truong, and G. W. Griswold, “The Ac-
tiveclass Project: Experiments in Encouraging Classroom Participation,”
in Designing for Change in Networked Learning Environments, 2003,
vol. 2, pp. 477–486.
[19] T. Bergstrom and K. Karahalios, “Social Mirrors as Social Signals:
Transforming Audio into Graphics,” Computer Graphics and Applica-
tions, IEEE, vol. 29, no. 5, pp. 22–32, 2009, [Accessed Jan. 24, 2014].
[20] H. Du, M. B. Rosson, and J. M. Carroll, “Augmenting Classroom
Participation Through Public Digital Backchannels,” in Proceedings
of the 17th ACM International Conference on Supporting Group
Work,
ser.
GROUP
’12.
New
York,
NY,
USA:
ACM,
2012,
pp. 155–164, [Accessed Jan. 24, 2014]. [Online]. Available: http:
//doi.acm.org/10.1145/2389176.2389201
[21] W.
Purgathofer,
Peter
Reinthaler,
“Exploring
the
Massive
Multiplayer
E-Learning
Concept,”
Ed-Media
Invited
Talk,
pp.
1–9,
2008,
[Accessed
Jan.
24,
2014].
[Online].
Available:
https://igw.tuwien.ac.at/designlehren/exploring\ for\ edmedia.pdf
[22] L. H. Bryant and J. R. Chittum, “ePortfolio Effectiveness: A(n Ill-Fated)
Search for Empirical Support,” International Journal of ePortfolio,
vol. 3, no. 2, pp. 189–198, 2013, [Accessed Jan. 24, 2014]. [Online].
Available: http://www.theijep.com
[23] E. L. Deci and R. M. Ryan, “Motivation, personality, and development
within embedded social contexts: an overview of self-determination

506
International Journal on Advances in Intelligent Systems, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/intelligent_systems/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
theory,” in The oxford handbook of human motivation, R. M. Ryan,
Ed.
Oxford, 2012, pp. 85–107.
[24] D. Nandi and M. Hamilton, “How active are students in online
discussion
forums?”
Proceedings
of
the
Thirteenth
Australasian
Computing Education Conference, no. Ace, pp. 125–134, 2011.
[Online]. Available: http://dl.acm.org/citation.cfm?id=2459952
[25] M.
Wilson
and
C.
Fairchild,
“Collaborative
Learning
and
the
Importance of the Discussion Board,” Journal of Diagnostic Medical
Sonography, vol. 27, no. 1, pp. 45–51, Dec. 2010. [Online]. Available:
http://jdm.sagepub.com/cgi/doi/10.1177/8756479310389609
[26] D.
R.
Comer
and
J.
a.
Lenaghan,
“Enhancing
Discussions
in
the Asynchronous Online Classroom: The Lack of Face-to-Face
Interaction Does Not Lessen the Lesson,” Journal of Management
Education, vol. 37, no. 2, pp. 261–294, Apr. 2012. [Online]. Available:
http://jme.sagepub.com/cgi/doi/10.1177/1052562912442384
[27] P.
Wright,
“The
psychology
of
layout:
Consequences
of
the
visual structure of documents,” American Association for Artiﬁcial
Intelligence Technical Report FS-99-04, 1999. [Online]. Available: http:
//www.aaai.org/Papers/Symposia/Fall/1999/FS-99-04/FS99-04-001.pdf
[28] M. C. Dyson, “How physical text layout affects reading from screen,”
Behaviour & Information Technology, vol. 23, no. 6, pp. 377–393,
Nov. 2004. [Online]. Available: http://www.tandfonline.com/doi/abs/10.
1080/01449290410001715714
[29] S. E. Middlestadt and K. G. Barnhurst, “The inﬂuence of layout on the
perceived tone of news articles,” Journalism & Mass Communication
Quaterly, vol. 76, no. 2, pp. 264–276, 1999. [Online]. Available:
http://jmq.sagepub.com/content/76/2/264.short
[30] D. Popolov, M. Callaghan, and P. Luker, “Conversation Space:
Visualising
Multi-threaded
Conversation,”
in
Proceedings
of
the
Working Conference on Advanced Visual Interfaces, ser. AVI ’00.
New York, NY, USA: ACM, 2000, pp. 246–249. [Online]. Available:
http://doi.acm.org/10.1145/345513.345330
[31] D. D. Suthers, “Effects of Alternate Representations of Evidential
Relations on Collaborative Learning Discourse,” in Proceedings of the
1999 Conference on Computer Support for Collaborative Learning,
ser. CSCL ’99.
International Society of the Learning Sciences, 1999.
[Online]. Available: http://dl.acm.org/citation.cfm?id=1150240.1150314
[32] S. McCloud, Reinventing Comics: How Imagination and Technology Are
Revolutionizing an Art Form.
William Morrow Paperbacks, 2000.
[33] P. Purgathofer and N. Luckner, “Layout Considered Harmful : On
the Inﬂuence of Information Architecture on Dialogue,” in Learning
and Collaboration Technologies. Designing and Developing Novel
Learning Experiences, HCII 2013, P. Zaphiris and A. Ioannou,
Eds.
Heraklion, Crete: Springer International Publishing, 2014, pp.
216–225. [Online]. Available: http://link.springer.com/chapter/10.1007/
978-3-319-07482-5\ 21
[34] D. J, Logic: the theory of inquiry.
H. Holt and Company, New York,
1938.
[35] A. P. Rovai, “A constructivist approach to online college learning,”
The
Internet
and
Higher
Education,
vol.
7,
no.
2,
pp.
79
–
93,
2004,
[Accessed
Jan.
24,
2014].
[Online].
Available:
http:
//www.sciencedirect.com/science/article/pii/S1096751604000144
[36] P. Mayring, Qualitative Inhaltsanalyse, Grundlagen und Techniken,
8th ed.
Weinheim: Beltz, 2003.

