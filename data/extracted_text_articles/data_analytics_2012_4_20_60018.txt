Evaluating Data Minability Through Compression – An Experimental Study
Dan Simovici
Univ. of Massachusetts Boston,
Boston, USA,
dsim at cs.umb.edu
Dan Pletea
Univ. of Massachusetts Boston,
Boston, USA,
dpletea at cs.umb.edu
Saaid Baraty
Univ. of Massachusetts Boston,
Boston, USA,
sbaraty at cs.umb.edu
Abstract—The effectiveness of compression algorithms is
increasing as the data subjected to compression contains
repetitive patterns. This basic idea is used to detect the
existence of regularities in various types of data ranging from
market basket data to undirected graphs. The results are quite
independent of the particular algorithms used for compression
and offer an indication of the potential of discovering patterns
in data before the actual mining process takes place.
Keywords-data mining; lossless compression; LZW; market
basket data; patterns; Kronecker product.
I. INTRODUCTION
Our goal is to show that compression can be used as
a tool to evaluate the potential of a data set of producing
interesting results in a data mining process. The basic idea
that data that displays repetitive patterns or patterns that
occur with a certain regularity will be compressed more
efﬁciently compared to data that has no such characteristics.
Thus, a pre-processing phase of the mining process should
allow to decide whether a data set is worth mining, or
compare the interestingness of applying mining algorithms
to several data sets.
Since compression is generally inexpensive and compres-
sion methods are well-studied and understood, pre-mining
using compression will help data mining analysts to focus
their efforts on mining resources that can provide a highest
payout without an exorbitant cost.
Compression has received lots of attention in the data min-
ing literature. As observed by Mannila [7], data compression
can be regarded as one of the fundamental approaches to data
mining [7], since the goal of the data mining is to “compress
data by ﬁnding some structure in it”.
The role of compression developing parameter-free data
mining algorithms in anomaly detection, classiﬁcation and
clustering was examined in [4]. The size C(x) of a com-
pressed ﬁle x is as an approximation of Kolmogorov com-
plexity [2] and allows the deﬁnition of a pseudo-distance
between two ﬁles x and y as
d(x, y) =
C(xy)
C(x) + C(y).
Further
advances
in
this
direction
were
developed
in [8][5][6]. A Kolmogorov complexity-based dissimilarity
was successfully used to texture matching problems in [1]
which have a broad spectrum of applications in areas like
bioinformatics, natural languages. and music.
We illustrate the use of lossless compression in pre-mining
data by focusing on several distinct data mining processes:
ﬁles with frequent patterns, frequent itemsets in market
basket data, and exploring similarity of graphs.
The LZW (Lempel-Ziv-Welch) algorithm was introduced
in 1984 by T. Welch in [9] and is among the most popular
compression techniques. The algorithm does not need to
check all the data before starting the compression and the
performance is based on the number of the repetitions and
the lengths of the strings and the ratio of 0s/1s or true/false
at the bit level. There are several versions of the LZW
algorithm. Popular programs (such as Winzip) use variations
of the LZW compression. The Winzip/Zip type of algorithms
also work at the bit level and not at a charater/byte level.
We explore three experimental settings that provide strong
empirical evidence of the correlation between compression
ratio and the existence of hidden patterns in data. In Sec-
tion II, we compress binary strings that contain patterns;
in Section III, we study the compressibility of adjacency
matrix for graphs relative to the entropy of distribution of
subgraphs. Finally, in Section IV, we examine the compress-
ibility of ﬁles that contain market basket data sets.
II. PATTERNS IN STRINGS AND COMPRESSION
Let A∗ be the set of strings on the alphabet A. The length
of a string w is denoted by |w|. The null string on A is
denoted by λ and we deﬁne A+ as A+ = A∗ − {λ}.
If w ∈ A∗ can be written as w = utv, where u, v ∈ A∗
and t ∈ A+, we say that the pair (t, m) is an occurrence of
t in w, where m is the length of u.
The occurrences (x, m) and (y, p) are overlapping if p <
m + |x|. If this is the case, there is a proper sufﬁx of x
that equals a proper preﬁx of y. If t is a word such that the
sets of its proper preﬁxes and its proper sufﬁxes are disjoint,
there are no overlapping occurrences of x in any word. The
number of occurrences of a string t in a string w is denoted
by nt(w). Clearly, we have P{na(w) | a ∈ A} = |w|. The
prevalence of t in w is the number ft(w) = nt(w)·|t|
|w|
which
gives the ratio of the characters contained in the occurrences
of t relative to the total number of characters in the string.
97
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-242-4
DATA ANALYTICS 2012 : The First International Conference on Data Analytics

The result of applying a compression algorithm C to a
string w ∈ A∗ is denoted by C(w) and the compression
ratio is the number
CRC(w) = |C(w)|
|w|
.
In this section, we shall use the binary alphabet B = {0, 1}
and the LZW algorithm or the compression algorithm of the
package java.util.zip.
We generated random strings of bits (0s and 1s) and
computed the compression ratio strings with a variety of
symbol distributions. A string w that contains only 0s
(or only 1s) achieves a very good compression ratio of
CRjZIP (w) = 0.012 for 100K bits and CRjZIP = 0.003
for 500K bits, where jZIP denotes the compression algo-
rithm from the package java.util.zip. Figure 1 shows,
as expected, that the worst compression ratio is achieved
when 0s and 1s occur with equal frequencies.
Figure 1.
Baseline CRjZIP Behavior
For strings of small length (less than 104 bits) the
compression ratio may exceed 1 because of the overhead
introduced by the algorithm. However, when the size of the
random string exceeds 106 bits this phenomenon disappears
and the compression ratio depends only on the prevalence of
the bits and is relatively independent on the size of the ﬁle.
Thus, in Figure 1, the curves that correspond to ﬁles of size
106 and 5 · 106 overlap. We refer to the compression ratio
of a random string w with an (n0(w), n1(w)) distribution
as the baseline compression ratio.
We created a series of binary strings ϕt,m which have
a minimum guaranteed number m of occurrences of pat-
terns t ∈ {0, 1}k, where 0 ≤ m ≤ 100. Speciﬁcally,
we created 101 ﬁles ϕ001,m for the pattern 001, each
containing 100K bits and we generated similar series for
t ∈ {01, 0010, 00010}. The compression ratio is shown in
Figure 2. The compression ratio starts at a value of 0.94 and
after the prevalence of the pattern becomes more frequent
than 20% the compression ratio drops dramatically. Results
of the experiment are shown in Table I and in Figure 3.
Table I
PATTERN ’001’ PREVALENCE VERSUS THE CRjZIP
Prevalence of
CRjZIP
Baseline
’001’ pattern
0%
0.93
0.93
10%
0.97
0.93
20%
0.96
0.93
30%
0.92
0.93
40%
0.86
0.93
50%
0.80
0.93
60%
0.72
0.93
70%
0.62
0.93
80%
0.48
0.93
90%
0.31
0.93
95%
0.19
0.93
100%
0.01
0.93
Figure 2.
Variation of compression rate depends on the prevalence
of the pattern ’001’
Figure 3. Dependency of Compression Ratio on Pattern Prevalence
We conclude that the presence of repeated patterns in
strings leads to a high degree of compression (that is, to low
compression ratios). Thus, a low compression ratio for a ﬁle
indicates that the mining process may produce interesting
results.
III. RANDOM INSERTION AND COMPRESSION
For a matrix M ∈ {0, 1}u×v denote by ni(M) the number
of entries of M that equal i, where i ∈ {0, 1}. Clearly, we
have n0(B) + n1(B) = uv. For a random variable V which
98
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-242-4
DATA ANALYTICS 2012 : The First International Conference on Data Analytics

ranges over the set of matrices {0, 1}u×v let νi(V ) be the
random variable whose values equal the number of entries
of V that equal i, where i ∈ {0, 1}.
Let A ∈ {0, 1}p×q be a 0/1 matrix and let
B :

B1
B2
· · ·
Bk
p1
p2
· · ·
pk

,
be a matrix-valued random variable where Bj ∈ Rr×s, pj ≥
0 for 1 ≤ j ≤ k, and Pk
j=1 pj = 1.
Deﬁnition 3.1: The random variable A ← B obtained by
the insertion of B into A is given by
A ⊗ B =



a11B
. . .
a1nB
...
...
...
am1B
. . .
amnB


 ∈ Rmr×ns
In other words, the entries of A ← B are obtained by
substituting the block aijBℓ with the probability pℓ for aij
in A.
Note that this operation is a probabilistic generalization
of Kronecker’s product for if
B :

B1
1

,
then A ← B has as its unique value the Kronecker product
A ⊗ B.
The expected number of 1s in the insertion A ← B is
E[ν1(A ← B)] = n1(A)
k
X
j=1
n1(Bj)pj
When n1(B1) = · · · = n1(Bk) = n, we have E[ν1(A ←
B)] = n1(A)n.
In the experiment that involves insertion, we used a
matrix-valued random variable such that n1(B1) = · · · =
n1(Bk) = n. Thus, the variability of the values of A ← B
is caused by the variability of the contents of the matrices
B1, . . . , Bk which can be evaluated using the entropy of the
distribution of B,
H(B) = −
k
X
j=1
pj log2 pj.
We expect to obtain a strong positive correlation between the
entropy of B and the degree of compression achieved on the
ﬁle that represents the matrix A ← B, and the experiments
support this expectation.
In a ﬁrst series of compressions, we worked with a matrix
A ∈ {0, 1}106×106 and with a matrix-valued random variable
B :
B1
B2
B3
p1
p2
p3

,
where Bj ∈ {0, 1}3×3, and n1(B1) = n1(B2) = n1(B3) =
4. Several probability distributions were considered, as
shown in Table II. Values of A ← B had 1062∗32 = 101124
entries.
In Table II, we had 39% 1s and the baseline compression
rate for a binary ﬁle with this ratio of 1s is 0.9775. We
also computed the correlation between the CRjZIP and the
Shannon entropy of the probability distribution and obtained
the value 0.9825 for 3 matrices. In Table III, we did the same
experiment but with 4 different matrices of 4 × 4. A strong
correlation (0.992.) was observed between CRjZIP and the
Shannon entropy of the probability distribution.
Table II
MATRIX INSERTIONS, ENTROPY AND COMPRESSION RATIOS
Probability distribution
CRjZIP
Shannon Entropy
(0, 1, 0)
0.33
0
(1, 0, 0)
0.33
0
(0, 0, 1)
0.33
0
(0.2, 0.2, 0.6)
0.77
1.37
(0.6, 0.2, 0.2)
0.74
1.37
(0.33, 0.33, 0.34)
0.79
1.58
(0, 0.3, 0.7)
0.7
0.88
(0.9, 0.1, 0)
0.51
0.46
(0.8, 0, 0.2)
0.61
0.72
(0.49, 0.25, 0.26)
0.77
1.5
(0.15, 0.35, 0.5)
0.78
1.44
Table III
KRONECKER PRODUCT AND PROBABILITY DISTRIBUTION FOR 4
MATRICES
Probability distribution
CRjZIP
Shannon Entropy
(0, 1, 0, 0)
0.23
0
(0, 1, 0, 0)
0.23
0
(0.2, 0.2, 0.2, 0.4)
0.69
01.92
(0.25, 0.25, 0.25, 0.25)
0.69
2
(0.4, 0, 0.2, 0.4)
0.53
1.52
(0.3, 0.1, 0.2, 0.4)
0.65
1.84
(0.45, 0.12, 0.22, 0.21)
0.61
1.83
Figure 4.
Evolution CRjZIP and Shannon Entropy of Probability
Distribution.
In Figure 4, we have the evolution of CRjZIP on the y
axis and on the x axis the Shannon Entropy of the probability
distribution for both experiments. We can see clearly the
linear correlation between the two.
99
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-242-4
DATA ANALYTICS 2012 : The First International Conference on Data Analytics

This experiment proves us again that in case of repe-
titions/patterns the CRjZIP is better than in the case of
randomly generated ﬁles.
Next, we examine the compressibility of binary square
matrices and its relationship with the distribution of principal
submatrices. A binary square matrix is compressed by ﬁrst
vectorizing the matrix and then compressing the binary
sequence. The issue is relevant in graph theory, where the
principal submatrices of the adjacency matrix of a graph
correspond to the adjacency matrices of the subgraphs of
that graph. The patterns in a graph are captured in the form
of frequent isomorphic subgraphs.
There is a strong correlation between the compression
ratio of the adjacency matrix of a graph and the frequencies
of the occurrences of isomorphic subgraphs of it. Speciﬁ-
cally, the lower the compression ratio is, the higher are the
frequencies of isomorphic subgraphs and hence the worthier
is the graph for being mined.
Let Gn be an undirected graph having {v1, . . . , vn} as its
set of nodes. The adjacency matrix of Gn, AGn ∈ {0, 1}n×n
is deﬁned as
(AGn)ij =
(
1
if there is an edge between vi and vj in Gn
0
otherwise.
We denote with CRC(AGn) the compression ratio of the
adjacency matrix of graph Gn obtained by applying the
compression algorithm C. Deﬁne the principal subcom-
ponent of matrix AGn with respect to the set of indices
S = {s1, . . . , sk} ⊆ {1, 2, . . ., n} to be the k × k matrix
AGn(S) such that
AGn(S)ij =





1
if there is an edge between vsi and vsj
in Gn
0
otherwise.
The matrix AGn(S) is the adjacency matrix of the subgraph
of Gn which consists of the nodes with indices in S along
with those edges that connect these nodes. We denote by
Pn(k) the collection of all subsets of {1, 2, . . ., n} of size
k where 2 ≤ k ≤ n. We have |Pn(k)| =

Figure 5.
Standard deviation vs. average of the CRjZIP (AGn) for
a number of different permutations of nodes for the same graph. The
horizontal axis is labelled with the number of edges of the graph.
Figure 6.
Standard deviation vs. average of the HP (Gn, k) of a number
of different permutations of nodes for the same graph. The horizontal axis
is labelled with the number of edges of the graph. Each curve corresponds
to one value of k.
IV. FREQUENT ITEMS SETS AND COMPRESSION RATIO
A market basket data set consists of a multiset T of
transactions. Each transaction t is a subset of a set of
items I = {i1, . . . , iN}. A transaction is described by its
characteristic N-tuple t = (t1, . . . , tN), where
tk =
(
1
if ik ∈ t.
0
otherwise,
for 1
≤
k
≤
N. The length of a transaction t is
|t| = PN
k=0 tk, while the average size of transactions is
P{|t| | t in T }
|T |
.
The support of a set of items K of the data set T is
the number supp(K) = |{t∈T | K⊆t|
|T |
. The set of items K is
s-frequent if supp(K) > s.
The study of market basket data sets is concerned with
the identiﬁcation of association rules. A pair of item sets
(X, Y ) is an association rule. Its support, supp(X → Y )
equals supp(X) and its conﬁdence conf(X → Y ) is deﬁned
as
conf(X → Y ) = supp(XY )
supp(X) .
n = 60 and k = 3
n = 60 and k = 4
n = 60 and k = 5
Figure 7.
Plots of average CRjZIP (AGn) (CMP RTIO) and average
HP (Gn, k) (DIST ENT) for randomly generated graphs Gn of equal
number of edges with respect to the number of edges.
101
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-242-4
DATA ANALYTICS 2012 : The First International Conference on Data Analytics

Table IV
CORRELATIONS BETWEEN CRjZIP (AGn) AND HP (Gn, k)
k
Correlation
3
0.92073175
4
0.920952812
5
0.919256573
Using the artiﬁcial transaction ARMiner generator de-
scribed in [3], we created a basket data set. Transactions
are represented by sequences of bits (t1, · · · , tN). The
multiset of M transactions was represented as a binary string
of length MN obtained by concatenating the strings that
represent transactions.
We generated ﬁles with 1000 transactions, with 100 items
available in the basket, adding up to 100K bits.
For data sets having the same number of items and trans-
actions, the efﬁciency of the compression increases when
the number of patterns is lower (causing more repetitions).
In an experiment with an average size of a frequent item
set equal to 10, the average size of a transaction equal to
15, and the number of frequent item sets varying in the set
{5, 10, 20, 30, 50, 75, 100, 200, 500, 1000}, the compression
ratio had a signiﬁcant variation ranging between 0.20 and
0.75, as shown in Table V. The correlation between the num-
ber of patterns and CR was 0.544. Although the frequency
of 1s and baseline compression ratio were roughly constant
(at 0.75), the number of patterns and compression ratio were
correlated.
Table V
NUMBER OF ASSOCIATION RULES AT 0.05 SUPPORT LEVEL AND 0.9
CONFIDENCE
Number of Patterns
Frequency of 1s
Baseline
Compression
Number of assoc.
compression
ratio
rules
5
16%
0.75
0.20
9,128,841
10
17%
0.73
0.34
4,539,650
20
17%
0.73
0.52
2,233,049
30
17%
0.76
0.58
106,378
50
19%
0.75
0.65
2,910,071
75
18%
0.75
0.67
289,987
100
18%
0.75
0.67
378,455
200
18%
0.75
0.70
163
500
18%
0.75
0.735
51
1000
18%
0.75
0.75
3
Further, there was a strong negative correlation (-0.92)
between the compression ratio and the number of association
rules indicating that market basket data sets that satisfy many
association rules are very compressible
V. CONCLUDING REMARKS
Compression ratio of a ﬁle can be computed fast and easy,
and in many cases offers a cheap way of predicting the
existence of embedded patterns in data. Thus, it becomes
possible to obtain an approximative estimation of the use-
fulness of an in-depth exploration of a data set using more
sophisticated and expensive algorithms. The use of compres-
sion as a measure of minability is illustrated on a variety
of paradigms: graph data, market basket data, etc. Recent
investigations show that identifying compressible areas of
human DNA is a useful tool for detecting areas where the
gene replication mechanisms are disturbed (a phenomenon
that occurs in certain genetically based diseases.
REFERENCES
[1] B. J. L. Campana and E. J. Keogh. A compression based
distance measure for texture. In SDM, pages 850–861,
2010.
[2] R. Cilibrasi and P. M. B. Vitnyi. Clustering by com-
pression.
IEEE Transactions on Information Theory,
51:1523–1545, 2005.
[3] L. Cristofor. ARMiner project, 2000.
[4] E. Keogh, S. Lonardi, and C. A. Ratanamahatana. To-
wards parameter-free data mining. In Proc. 10th ACM
SIGKDD Intnl Conf. Knowledge Discovery and Data
Mining, pages 206–215. ACM Press, 2004.
[5] E. Keogh, S. Lonardi, C. A. Ratanamahatana, L. Wei,
S. Lee, and J. Handley. Compression-based data min-
ing of sequential data.
Data Mining and Knowledge
Discovery, 14:99–129, 2007.
[6] E. J. Keogh, L. Keogh, and J. Handley. Compression-
based data mining. In Encyclopedia of Data Warehous-
ing and Mining, pages 278–285. 2009.
[7] H. Mannila. Theoretical frameworks for data mining.
SIGKDD Exploration, 1:30–32, 2000.
[8] L. Wei, J. Handley, N. Martin, T. Sun, and E. J. Keogh.
Clustering workﬂow requirements using compression
dissimilarity measure. In ICDM Workshops, pages 50–
54, 2006.
[9] T. Welch.
A technique for high performance data
compression. IEEE Computer, 17:8–19, 1984.
102
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-242-4
DATA ANALYTICS 2012 : The First International Conference on Data Analytics

