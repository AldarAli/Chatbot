Data Sanitisation Protocols for the Privacy Funnel with Differential Privacy
Guarantees
Milan Lopuha¨a-Zwakenberg, Haochen Tong, and Boris ˇSkori´c
Department of Mathematics and Computer Science
Eindhoven University of Technology
Eindhoven, the Netherlands
email: {m.a.lopuhaa,b.skoric}@tue.nl, h.tong@student.tue.nl
Abstract—In the Open Data approach, governments and other
public organisations want to share their datasets with the public,
for accountability and to support participation. Data must be
opened in such a way that individual privacy is safeguarded.
The Privacy Funnel is a mathematical approach that produces
a sanitised database that does not leak private data beyond a
chosen threshold. The downsides to this approach are that it does
not give worst-case privacy guarantees, and that ﬁnding optimal
sanitisation protocols can be computationally prohibitive. These
problems are tackled by using differential privacy metrics, and by
considering local protocols that operate on one entry at a time. It
is shown that under both the Local Differential Privacy and Local
Information Privacy leakage metrics, one can efﬁciently obtain
optimal protocols. Furthermore, Local Information Privacy is
more closely aligned to the privacy requirements of the Privacy
Funnel scenario, and optimal protocols satisfying Local Infor-
mation Privacy are more efﬁciently computable. This paper also
considers the scenario where each user has multiple attributes,
for which a side-channel resistant privacy criterion is deﬁned,
and efﬁcient methods to ﬁnd protocols satisfying this criterion,
while still offering good utility, are given. Finally, Conditional
Reporting is introduced, an explicit LIP protocol that can be used
when the optimal protocol is infeasible to compute. Experiments
on real-world and synthetic data conﬁrm the validity of these
methods. The main output of this paper consists of methods to
compute optimal privacy protocols, and explicit privacy protocols
when the former are unfeasible computationally.
Keywords—Privacy funnel; local differential privacy; in-
formation privacy; database sanitisation; complexity.
I. INTRODUCTION
This paper is an extended version of [1]. Under the Open
Data paradigm, governments and other public organisations
want to share their collected data with the general public.
This increases a government’s transparency, and it also gives
citizens and businesses the means to participate in decision-
making, as well as using the data for their own purposes.
However, while the released data should be as faithful to the
raw data as possible, individual citizens’ private data should
not be compromised by such data publication.
Let X
be a ﬁnite set. Consider a database
⃗X
=
(X1, . . . , Xn) ∈ X n owned by a data aggregator, containing a
data item Xi ∈ X for each user i (For typical database settings,
each user’s data is a vector of attributes Xi = (X1
i , . . . , Xm
i );
this will be considered in more detail in Section VI). This
data may not be considered sensitive by itself, but it might
be correlated to a secret Si. For instance, Xi might contain
the age, sex, weight, skin colour, and average blood pressure
Sensitive
Data
S1
S2
...
Sn
Database
X1
X2
...
Xn
Sanitised
Database
Y1
Y2
...
Yn
Q
Q
Q
Hidden from public
Figure 1. Model of PF with local protocols.
of person i, while Si is the presence of some medical condi-
tion. To publish the data in a privacy-preserving manner, the
aggregator releases a sanitised database ⃗Y = (Y1, . . . , Yn),
obtained from applying a sanitisation mechanism R to ⃗X. In
this setting, privacy is considered to be the extent to which one
is unable to infer information about the Si from the sanitised
database ⃗Y . One way to formulate this is by measuring the
privacy leakage as the mutual information I(⃗S; ⃗Y ), and utility
as the mutual information I( ⃗X; ⃗Y ). This leads to the Privacy
Funnel (PF) problem:
Problem 1. (Privacy Funnel, [2]) Suppose the joint probability
distribution of ⃗S and ⃗X is known to the aggregator, and let
M ∈ R≥0. Then, ﬁnd the sanitisation mechanism R such that
I( ⃗X; ⃗Y ) is maximised while I(⃗S; ⃗Y ) ≤ M.
There are two difﬁculties with this approach:
1) Finding and implementing good sanitisation mechanisms
that operate on all of ⃗X can be computationally pro-
hibitive for large n, as the complexity is exponential in
n [3][4].
2) Taking mutual information as a leakage measure has as
a disadvantage that it gives guarantees about the leakage
in the average case. If n is large, this still leaves room
for the sanitisation protocol to leak undesirably much
information about a few unlucky users.
To deal with these two difﬁculties, two changes are made
to the general approach. First, the focus is on local data
sanitisation, i.e., optimisation protocols Q: X
→ Y are
162
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

considered, for some ﬁnite set Y, and Q is applied to each
Xi individually; this situation is depicted in Figure 1. Local
sanitisation can be implemented efﬁciently. In fact, this ap-
proach is often taken in the PF setting [5][3]. Second, to ensure
strong privacy guarantees even in worst-case scenarios,stricter
notions of privacy are considered, based on Local Differential
Privacy (LDP) [6]. For these metrics, methods are developed
to ﬁnd optimal protocols. Furthermore, for situations where
the optimal protocol is computationally unfeasible to ﬁnd,
a new protocol is introduced, Conditional Reporting (CR),
that takes advantage of the fact that only Si needs to be
protected. Determining CR only requires ﬁnding the root of
a onedimensional increasing function, which can be done fast
numerically.
A. New contributions
In this paper, two Differential Privacy-like privacy metrics
are adapted to the PF situation, namely ε-LDP [6] and Local
Information Privacy (ε-LIP) [7][8]. These metrics are modiﬁed
so that they measure leakage about the underlying S rather
than X itself (for notational convenience, S, X, Y rather than
Si, Xi, Yi is used throughout the rest of this paper). For a
given level of leakage, the aim is to ﬁnd the privacy protocol
that maximises the mutual information between input Xi and
output Yi. Adapting methods from [9] on LDP and [10] on
perfect privacy, the following Theorem is proven:
Theorem 1 (Theorems 2 and 3 paraphrased). Suppose X
and S are discrete random variables on sets of size a and c,
respectively. Suppose that their joint distribution and a privacy
level ε ≥ 0 are given.
1) The optimal ε-LDP protocol can be found by enumerating
the vertices of a polytope in a2 − a dimensions deﬁned
by a(c2 − c) inequalities.
2) The optimal ε-LIP protocol can be found by enumerating
the vertices of a polytope in a − 1 dimensions deﬁned by
2ac inequalities.
This theorem gives us methods to get data sanitisation
protocols that give strong privacy guarantees, and optimal
utility under these guarantees. This is important in settings
where worst-case guarantees for privacy leakage are needed,
rather than a bound on the average user’s privacy.
Since the complexity of the polytope vertex enumeration
depends signiﬁcantly on both its dimension and the number of
deﬁning inequalities [11], ﬁnding optimal LIP protocols can be
done signiﬁcantly faster than ﬁnding optimal LDP protocols.
Furthermore, it will be argued that LIP is a privacy metric that
more accurately captures information leakage than LDP in the
PF scenario. For these two reasons only LIP is considered in
the remainder of the paper, although many results can also be
formulated for LDP.
A common scenario is that a user’s data X consists of
multiple attributes, i.e., X = (X1, . . . , Xm). Here one can
consider an attacker model where the attacker has access to
some of the Xj. In this situation ε-LIP does not accurately
reﬂect a user’s privacy. Because of this, a new privacy condi-
tion called Side-channel Resistant LIP is introduced that takes
such sidechannels into account, and methods to ﬁnd optimal
protocols that satisfy this privacy condition are described.
Finding the optimal protocols can become computationally
unfeasible for large a and c. In such a situation, one needs
to resort to explicitely given protocols. In the literature there
is a wealth of protocols that satisfy ε-LDP w.r.t. X. These
certainly work in the PF situation, but they might not be ideal,
because these are designed to obfuscate all information about
X, rather than just the part that relates to S. For this reason,
Conditional Reporting (CR) is introduced, a privacy protocol
that focuses on hiding S rather than X. Finding the appropriate
CR protocol for a given probability distribution and privacy
level can be done fast numerically.
The structure of this paper is as follows. In Section II, an
overview is given of related work on PF, LDP, and ﬁnding
optimal protocols. The mathematical setting of this paper is
formalised in Section III. In Sections IV and V, Theorem 1 is
proven for LDP and LIP, respectively. In Section VI privacy
in the multiple attribute scenario is discussed. Section VII is
dedicated to Conditional Reporting and its privacy properties.
In Section VIII, he methods and protocols discussed above
are tested on both synthetic and real data. Compared to [1],
new contents in this extended paper are Section VII, the
experiments on real data, and the extended literature review.
II. RELATED WORK
The PF setting was introduced in [5], to provide a frame-
work for obfuscating data in such a way that the obfuscated
data remains as faithful as possible to the original, while
ensuring that the information leakage about a latent variable
is limited. PF is related to the Information Bottleneck (IB)
[12], a problem from machine learning that seeks to compress
data as much as possible, while retaining a minimal threshold
of information about a latent variable. In PF as well as IB,
both utility and leakage are measured via mutual information.
Many approaches to ﬁnding the optimal protocols in PF also
work for IB and vice versa [13][3]. A wider range of privacy
metrics for PF, and their relation to Differential Privacy, is
discussed in [8].
LDP was introduced in [6]. It is an adaptation of Differential
Privacy (DP) [14] to a setting where there is no trusted central
party to obfuscate the data. As a privacy metric, it has the
advantage that it offers a privacy guarantee in any case, not
just the average case, and that it does not depend on the
data distribution. On the downside, it can be difﬁcult to fulﬁll
such a stringent deﬁnition of privacy, and many relaxations
of (L)DP have been proposed [15][16][17][18]. Of particular
interest to this paper is LIP [7][8], also called Removal Local
Differential Privacy [19]. LIP retains the worst-case guarantees
of LDP, but is less restrictive, and can take advantage of
a known distribution. In the context where only part of the
data is considered secret, many privacy metrics fall under the
umbrella of Pufferﬁsh Privacy [20].
163
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

In [9], a method was introduced for ﬁnding optimal LDP-
protocols for a wide variety of utility metrics, including mutual
information. The method relies on ﬁnding the vertices of
a polytope, but since this is the well-studied Differential
Privacy polytope, its vertices can be described explicitly [21].
Similarly, [10] uses a vertex enumeration method to ﬁnd the
optimal protocol in the perfect privacy situation, i.e., when the
released data is independent of the secret data. The complexity
of vertex enumeration is discussed in [22][11].
One can conclude that PF and LDP are both well-studied,
and so are methods to ﬁnd optimal LDP protocols. However,
LDP-like metrics so far have not been applied to the PF
scenario. The aim of this paper is to do so, and to ﬁnd optimal
PF protocols that satisfy LDP-like privacy requirements.
III. MATHEMATICAL SETTING
The database ⃗X = (X1, . . . , Xn) consists of a data item
Xi for each user i, each an element of a given ﬁnite set
X. Furthermore, each user has sensitive data Si ∈ S, which
is correlated with Xi; again S is assumed to be ﬁnite (see
Figure 1). Each (Si, Xi) is assumed to be drawn independently
from the same distribution pS,X on S × X that is known to
the aggregator through observing (⃗S, ⃗X) (if one allows for
non-independent Xi, then differential privacy is no longer an
adequate privacy metric [15][8]). The aggregator, who has
access to ⃗X, sanitises the database by applying a sanitisation
protocol (i.e., a random function) Q: X → Y to each Xi,
outputting ⃗Y = (Y1, . . . , Yn) = (Q(X1), . . . , Q(Xn)). The
aggregator’s goal is to ﬁnd a Q that maximises the information
about Xi preserved in Yi (measured as I(Xi; Yi)) while leaking
only minimal information about Si.
Without loss of generality X, Y, S are identiﬁed with the
sets {1, . . . , a}, {1, . . . , b}, {1, . . . , c}, respectively, for inte-
gers a, b, c. The subscript i from Xi, Yi, Si is omitted as no
probabilities depend on it, and probabilities are written as px,
ps, px|s, etc., which form vectors pX, pS|x, etc., and matrices
pX|S, etc.
As noted before, instead of looking at the mutual infor-
mation I(S; Y ), two different, related measures of sensitive
information leakage known from the literature are considered.
The ﬁrst one is an adaptation of LDP, the de facto standard in
information privacy [6]:
Deﬁnition 1. (ε-LDP) Let ε ∈ R≥0. say that Q satisﬁes ε-
LDP w.r.t. S if
∀y ∈ Y, ∀s, s′ ∈ S :
P(Y = y|S = s)
P(Y = y|S = s′) ≤ eε.
(1)
Most literature on LDP considers LDP w.r.t. X, i.e., for all
y, x, x′ it holds that
P(Y = y|X = x)
P(Y = y|X = x′) ≤ eε.
(2)
This is a stricter requirement, because under this deﬁnition
all data needs to be protected, rather than just the underlying
sensitive data. This typically comes at a cost in utility [10].
ε-LDP
2ε-LDP
ε-LIP
I(S; Y ) ≤ ε
ε-SRLIP
Multiple attributes,
see Section VI
Figure 2. Relations between privacy notions. The multiple attributes setting
is discussed in Section VI.
Throughout the present paper, ε-LDP always means ε-LDP
w.r.t. S, unless otherwise speciﬁed.
The LDP metric reﬂects the fact that in the PF scenario one
is only interested in hiding sensitive data, rather than all data;
it is a speciﬁc case of what has been named Pufferﬁsh Privacy
[20]. The advantage of LDP compared to mutual information
is that it gives privacy guarantees for the worst case, not just
the average case. This is desirable in the database setting, as
a worst-case metric guarantees the security of the private data
of all users, while average-case metrics are only concerned
with the average user. Another useful privacy metric is Local
Information Privacy (LIP) [7][8], also called Removal Local
Differential Privacy [19]:
Deﬁnition 2. (ε-LIP) Let ε ∈ R≥0. The protocol Q satisﬁes
ε-LIP w.r.t. S if
∀y ∈ Y, s ∈ S :
e−ε ≤ P(Y = y|S = s)
P(Y = y)
≤ eε.
(3)
Compared to LDP, the disadvantage of LIP is that it depends
on the distribution of S; this is not a problem in the PF
scenario, as the aggregator, who chooses Q, has access to the
distribution of S. The advantage of LIP is that is more closely
related to an attacker’s capabilities: since
P(Y = y|S = s)
P(Y = y)
= P(S = s|Y = y)
P(S = s)
,
(4)
satisfying ε-LIP means that an attacker’s posterior distribution
of S given Y = y does not deviate from their prior distribution
by more than a factor eε. The following lemma outlines
the relations between LDP, LIP and mutual information (see
Figure 2).
Lemma 1. (See [8]) Let Q be a sanitisation protocol, and let
ε ∈ R≥0.
1) If Q satisﬁes ε-LDP, then it satisﬁes ε-LIP.
2) If Q satisﬁes ε-LIP, then it satisﬁes 2ε-LDP, and
I(S; Y ) ≤ ε.
Remark 1. One gets robust equivalents of LDP and LIP
by demanding that Q satisfy ε-LIP (ε-LDP) for a set of
distributions pS,X, instead of only a single distribution [20].
Letting pS,X range over all possible distributions on S × X
yields LIP (LDP) w.r.t. X.
164
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

In this notation, instead of Problem 1 the following problem
is considered:
Problem 2. Suppose pS,X is known to the aggregator, and
let ε ∈ R≥0. Then, ﬁnd the sanitisation protocol Q such
that I(X; Y ) is maximised while Q satisﬁes ε-LDP (ε-LIP,
respectively) with respect to S.
Note that this problem does not depend on the number of
users n, and as such this approach will ﬁnd solutions that are
scalable w.r.t. n.
IV. OPTIMIZING Q FOR ε-LDP
The goal is now to ﬁnd the optimal Q, i.e., the protocol
that maximises I(X; Y ) while satisfying ε-LDP, for a given
ε. Any sanitisation protocol can be represented as a matrix
Q ∈ Rb×a, where Qy|x = P(Y = y|X = x). Then, ε-LDP is
satisﬁed if and only if
∀x:
X
y
Qy|x = 1,
(5)
∀x, y: 0 ≤ Qy|x,
(6)
∀s, s′, y: (Q pX|s)y ≤ eε(Q pX|s′)y.
(7)
As such, for a given Y, the set of ε-LDP-satisfying sanitisation
protocols can be considered a closed, bounded, convex poly-
tope Γ in Rb×a. This fact allows us to efﬁciently ﬁnd optimal
protocols.
Theorem 2. Let ε ∈ R≥0. Let Q: X → Y be a ε-LDP
protocol that maximises I(X; Y ), i.e., the protocol that solves
Problem 2 w.r.t. LDP.
1) One can take b = a.
2) Let Γ be the polytope described above, for b = a. Then
the optimal Q corresponds to one of the vertices of Γ.
Proof. The ﬁrst result is obtained by generalising the results
of [9]: there this is proven for regular ε-LDP (i.e., w.r.t. X),
but the arguments given in that proof hold just as well in this
situation; the only difference is that their polytope is deﬁned
by the ε-LDP conditions w.r.t. X, but this has no impact on
the proof. The second statement follows from the fact that
I(X; Y ) is a convex function in Q; therefore, its maximum on
a bounded polytope is attained in one of the vertices.
This theorem reduces the search for the optimal LDP
protocol to enumerating the set of vertices of Γ, a a(a − 1)-
dimensional convex polytope. Note that the only property of
I(X; Y ) used in the proof is the fact that it is convex in Q.
Therefore, the theorem holds for any convex utility metric.
One might argue that, since the optimal Q depends on
pS,X, the publication of Q might provide an aggregator with
information about the distribution of S. However, information
on the distribution (as opposed to information of individual
users’ data) is not considered sensitive [23]. In fact, the reason
why the aggregator sanitises the data is because an attacker
is assumed to have knowledge about this correlation, and
revealing too much information about X would cause the
aggregator to use this information to infer information about S.
V. OPTIMIZING Q FOR ε-LIP
If one uses ε-LIP as a privacy metric, one can ﬁnd the
optimal sanitisation protocol in a similar fashion. To do this,
a sanitisation protocol Q is again described as a matrix, but
this time a different one. Let q ∈ Rb be the probability mass
function of Y , and let R ∈ Ra×b be given by
Rx|y = P(X = x|Y = y);
(8)
its y-th row is denoted by RX|y ∈ Ra. Then, a pair (R, q)
deﬁnes a sanitisation protocol Q satisfying ε-LIP if and only
if
∀y: 0 ≤ qy,
(9)
Rq = pX,
(10)
∀y:
X
x
Rx|y = 1,
(11)
∀x, y: 0 ≤ Rx|y,
(12)
∀y, s: e−ε ps ≤ ps|X RX|y ≤ eε ps .
(13)
Note that (13) deﬁnes the ε-LIP condition, since for a given
s, y one has
ps|X RX|y
pS
= P(S = s|Y = y)
P(S = s)
= P(Y = y|S = s)
P(Y = y)
.
(14)
(In)equalities (11–13) can be expressed as saying that for every
y ∈ Y one has that RX|y ∈ ∆, where ∆ is the convex closed
bounded polytope in RX given by
∆ =


v ∈ RX :
P
x vx = 1,
∀x : 0 ≤ vx,
∀s : e−ε ps ≤ ps|X v ≤ eε ps


 .
(15)
As in Theorem 2, this polytope can be used to ﬁnd optimal
protocols:
Theorem 3. Let ε ∈ R≥0, and let ∆ be the polytope above.
Let V = {v1, . . . , vM} be its set of vertices. For vi ∈ V, let
H(vi) be its entropy, i.e.
H(vi) = −
X
x∈X
vi,x ln(vi,x).
(16)
Let ˆα be the solution to the optimisation problem
minimiseα∈RM
M
X
i=1
H(vi)αi
(17)
subject to ∀i : αi ≥ 0,
M
X
i=1
αivi = pX .
Then the ε-LIP protocol Q: X → Y that maximises I(X; Y )
is given by
Y = {i ≤ M : ˆαi > 0},
(18)
qi = ˆαi,
(19)
Rx|i = vi,x,
(20)
for all i ∈ Y ⊆ {1, . . . , M} and all x ∈ X. One has b ≤ a.
165
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Proof. This was proven for ε = 0 (i.e., when S and Y are
independent) in [10], but the proof works similarly for ε > 0;
the main difference is that the equality constraints of their (10)
will be replaced by the inequality constraints of this paper’s
(13), but this has no impact on the proof presented there.
Since linear optimisation problems can be solved fast, again
the optimisation problem reduces to ﬁnding the vertices of
a polytope. The advantage of using LIP instead of LDP is
that ∆ is a (a − 1)-dimensional polytope, while Γ of Section
IV is a(a − 1)-dimensional. The time complexity of vertex
enumeration is linear in the number of vertices [22], while the
number of vertices can grow exponentially in the dimension of
the polyhedron [11]. Together, this means that the dimension
plays a huge role in the time complexity, hence the optimum
under LIP is expected to be found signiﬁcantly faster than
under LDP.
VI. MULTIPLE ATTRIBUTES
An often-occuring scenario is that a user’s data consists of
multiple attributes, i.e.,
X = (X1, . . . , Xm) ∈ X = X 1 × · · · × X m.
(21)
This can be problematic for this paper’s approach for two
reasons:
1) Such a large X can be problematic, since the computing
time for optimisation both under LDP and LIP will
depend heavily on a.
2) In practice, an attacker might sometimes utilise side
channels to access some subsets of attributes Xj
i for some
users. For these users, a sanitisation protocol can leak
more information (w.r.t. to the attacker’s updated prior
information) than its LDP/LIP parameter would suggest.
To see how the second problem might arise in practice,
suppose that X1
i is the height of individual i, X2
i is their
weight, and Si is whether i is obese or not. Since height is
only lightly correlated with obesity, taking Yi = X1
i would
satisfy ε-LIP for some reasonably small ε. However, suppose
that an attacker has access to X2
i via a side channel. While
knowing i’s weight gives the attacker some, but not perfect
knowledge about i’s obesity, the combination of the weight
from the side channel, and the height from the Yi, allows the
attacker to calculate i’s BMI, giving much more information
about i’s obesity. Therefore, the given protocol gives much
less privacy in the presence of this side channel.
To solve the second problem, a more stringent privacy no-
tion called Side-channel Resistant LIP (SRLIP) is introduced,
which ensures that no matter which attributes an attacker
has access to, the protocol still satisﬁes ε-LIP with respect
to the attacker’s new prior distribution. One could similarly
introduce SRLDP, and many results will still hold for this
privacy measure; nevertheless, since it has been concluded that
LIP is preferable to LDP, the focus is on SRLIP. For any subset
J ⊆ {1, . . . , m}, the notation X J is used for the set Q
j∈J X j,
and its elements are written as xJ.
Deﬁnition 3. (ε-SRLIP). Let ε > 0, and let X = Qm
j=1 X j.
The protocol Q satisﬁes ε-SRLIP if for every y ∈ Y, for every
s ∈ S, for every J ⊆ {1, . . . , m}, and for every xJ ∈ X J one
has
e−ε ≤ P(Y = y|S = s, XJ = xJ)
P(Y = y|XJ = xJ)
≤ eε.
(22)
In terms of Remark 1, Q satisﬁes ε-SRLIP if and only if it
satisﬁes ε-LIP w.r.t. pS,X|xJ for all J and xJ. Taking J = ∅
gives us the regular deﬁnition of ε-LIP, proving the following
Lemma:
Lemma 2. Let ε > 0. If Q satisﬁes ε-SRLIP, then Q satisﬁes
ε-LIP.
While SRLIP is stricter than LIP itself, it has the advantage
that even when an attacker has access to some data of a
user, the sanitisation protocol still does not leak an unwanted
amount of information beyond the knowledge the attacker
has gained via the side channel. Another advantage is that,
contrary to LIP itself, SRLIP satisﬁes an analogon of the
concept of privacy budget [14]:
Theorem 4. Let X
=
Qm
j=1 X j, and for every j, let
Qj : X j → Yj be a sanitisation protocol. Let εj ∈ R≥0
for every j. Suppose that for every j
≤ m, for every
J ⊆ {1, . . . , j − 1, j + 1, . . . , m}, and every xJ ∈ X J, Qj
satisﬁes εj-LIP w.r.t. pS,X|xJ. Then Q
j Qj : X → Q
j Yj
satisﬁes P
j εj-SRLIP.
The proof is presented in Appendix A. This theorem tells
us that to ﬁnd a ε-SRLIP protocol for X, it sufﬁces to ﬁnd a
sanitisation protocol for each X j that is ε
m-LIP w.r.t. a number
of prior distributions. Unfortunately, the method of ﬁnding an
optimal ε-LIP protocol w.r.t. one prior pS,X of Theorem 3 does
not transfer to the multiple prior setting. This is because this
method only ﬁnds one (R, q), while by (10) a different (R, q)
is needed for each prior distribution. Therefore, an approach
similar to the one in Theorem 2 is adopted. The matrix
Qj (given by Qj
yj|xj = P(Qj(xj) = yj)) corresponding to
Qj : X j → Yj satisﬁes the criteria of Theorem 4 if and only
if the following criteria are satisﬁed:
∀xj :
X
yj
Qj
yj|xj = 1,
(23)
∀xj, yj : 0 ≤ Qj
yj|xj,
(24)
∀J, xJ, s, yj : e−ε/m(Qj pXj|xJ)yj ≤ (Qj pXj|s,xJ)yj, (25)
∀J, xJ, s, yj : (Qj pXj|s,xJ)yj ≤ eε/m(Qj pXj|xJ)yj.
(26)
Similar to Theorem 2, the optimal Qj satisfying these
conditions can be found by ﬁnding the vertices of the polytope
deﬁned by (23–26). In terms of time complexity, the com-
parison to ﬁnding the optimal ε-LIP protocol via Theorem
3 versus ﬁnding a ε-SRLIP protocol via Theorem 4 is not
straightforward. The complexity of enumerating the vertices of
a polytope is O(ndv), where n is the number of inequalities, d
is the dimension, and v is the number of vertices [22]. For the
166
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

∆ of Theorem 3 one has d = a−1 and n = a+2c. By contrast,
the polytope deﬁned by (23–26) satisﬁes d = aj(aj − 1) and
n = (aj)2 + 2c Q
j′̸=j(aj′ + 1). Finding v for both these
polytopes is difﬁcult, but in general v ≤

-4
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
log(LIP time / LDP time)
0.8
0.82
0.84
0.86
0.88
0.9
0.92
0.94
0.96
0.98
1
LDP info / LIP info
eps = 0.5
eps = 1
eps = 1.5
eps = 2
(a). ε-LDP vs ε-LIP
-4
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
log(LIP time / LDP time)
0.8
0.82
0.84
0.86
0.88
0.9
0.92
0.94
0.96
0.98
1
LIP info / LDP info
eps = 0.5
eps = 1
eps = 1.5
eps = 2
(b). ε/2-LIP vs ε-LDP
Figure 3. Comparison of computation time and I(X; Y ) for LDP protocols found via Theorem 2 and LIP protocols found via Theorem 3, for random pS,X
with c = 2, a = 5, and ε ∈ {0.5, 1, 1.5, 2}.
Proof. For all y ∈ X and s ∈ S one has, following equation
(48) in Appendix A, that
P(CRα(X, S) = y|S = s) =
1
eα+c−1

eα py|s +
X
s′̸=s
py|s′

.
(34)
It follows that
P(CRα(X, S) = y|S = s)
P(CRα(X, S) = y|S = s′)
=
eα py|s +py|s′ + P
s′′̸=s,s′ py|s′′
py|s +eαpy|s′ + P
s′′̸=s,s′ py|s′′
(35)
≤ max
(
1,
eα py|s +py|s′
py|s +eαpy|s′
)
(36)
≤ eα.
VIII. EXPERIMENTS
The feasibility of the different methods is tested by per-
forming small-scale experiments on synthetic data and real-
world data. All experiments are implemented in Matlab and
conducted on a PC with Intel Core i7-7700HQ 2.8GHz and
32GB memory.
A. Synthetic data: LDP vs LIP
The computing time for ﬁnding optimal ε-LDP and ε-LIP
protocols was compared for c = 2 and a = 5 for 10 random
distributions pS,X, obtained by generating each ps,x uniformly
from [0, 1] and then normalising. The LDP/LIP privacy pa-
rameter ε is taken to be in {0.5, 1, 1.5, 2}; the results are in
Figure 3(a). As one can see, Theorem 3 gives signiﬁcantly
faster results than Theorem 2; the average computing time
for Theorem 2 for ε = 0.5 is 133s, while for Theorem 3
this is 0.0206s. With regards to the utility I(X; Y ), since ε-
LDP implies ε-LIP, the optimal ε-LIP protocol will have better
utility than the optimal ε-LDP protocol. However, as can be
seen from the ﬁgure, the difference in utility is relatively low.
-4
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
log(LIP time / SRLIP time)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SRLIP info / LIP info
eps = 0.5
eps = 1
eps = 1.5
eps = 2
Figure 4. Comparison of computation time and I(X; Y ) for
ε-(SR)LIP-protocols found via Theorems 3 and 4, for random pS,X with
c = 2, a1 = a2 = 3, a3 = 4, and ε ∈ {0.5, 1, 1.5, 2}.
Note that for bigger ε, both the difference in computing time
and the difference in I(X; Y ) between LDP and LIP become
less. This is because of the probabilistic relation between S
and X, for ε large enough, any sanitisation protocol satisﬁes
ε-LIP and ε-LDP. This means that as ε grows, the resulting
polytopes will have fewer deﬁning inequalities, hence they will
have fewer vertices. This results in lower computation times,
which affects LDP more than LIP. At the same time, the fact
that every protocol is both ε-LIP and ε-LDP will result in the
same optimal utility.
In Figure 3(b), optimal ε
2-LDP protocols are compared to
to optimal ε-LIP protocols. Again, LIP is signiﬁcantly faster
than LDP. Since ε-LIP implies ε
2-LDP, the optimal ε
2-LDP has
higher utility; again the difference is low.
B. Synthetic data: LIP vs SRLIP
Similar comparisons are perfomed for multiple attributes,
for c = 2, a1 = a2 = 3 and a3 = 4, comparing the methods
168
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(a). S = marital status, X = education
(b). S = occupation, X = education
(c). S = marital status, X = relationship
(d). S = occupation, X = relationship
(e). S = marital status, X = sex
(f). S = occupation, X = sex
Figure 5. Experiments on the adult-dataset.
of Theorems 3 and 4. The results are presented in Figure 4. As
one can see, Theorem 4 is signiﬁcantly slower, with Theorem
3 being on average 476 times as fast. There is a sizable
difference in utility, caused on one hand by the fact that ε-
SRLIP is a stricter privacy requirement than ε-LIP, and on the
other hand by the fact that Theorem 4 does not give us the
optimal ε-SRLIP protocol.
C. Adult-dataset
The utility of Conditional Reporting (CR) is tested both
on real world data and synthetic data. The real world data
is from the well-known adult-dataset [27], which contains
demographic data from the 1994 US census. For these exper-
iments S is taken to be in {marital status, occupation} (with
c = 7 and c = 15, respectively) and X is taken to be in
{education, relationship, sex} (with a = 16, 6, 2). Based on
the ﬁndings in the previous sections, LIP is taken as a privacy
measure, and I(X; Y ) as a utility measure. CR is compared
on the one hand with the optimal method (Opt-LIP) found in
Section V, and on the other hand with the established LDP
protocols GRR and OUE. The results are shown in Figure
5. For X = education, the mutual information for OUE
was infeasible to compute. Similarly, for S = occupation,
some cases of Opt-LIP failed to compute within a reasonable
169
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(a). a = 5, c = 2
(b). a = 2, c = 5
(c). a = 5, c = 5
(d). a = 3, c = 5
(e). a = 5, c = 7
(f). a = 7, c = 5
Figure 6. Experiments on synthetic data. For each value of a and c, the average utility is taken over 100 randomly generated probability distributions. Bar
size denotes standard deviation.
timeframe. Nevertheless, it can concluded that GRR and CR
both perform somewhere between Opt-LIP and OUE. As the
LIP value ε grows larger, GRR and CR grow close to Opt-LIP.
At the same time, OUE falls off for large ε, having 1
2 H(X) as
its limit. This is because OUE by design only has probability
1
2 transmitting the true X (as element of the set Y ). The
difference between GRR and CR is less clear, and it appears
to depend on the joint distribution pX,S which protocol gives
the best utility.
D. Synthetic data: GRR vs CR
To investigate the difference between GRR and CR, both
methods are applied to synthetic data. OUE is disregarded as
it performs worse than the other two protocols, especially in
the low privacy regime. For a ﬁxed choice of a and c, 100
probability distributions are drawn from the Jeffreys prior on
S×X, i.e., the symmetric Dirichlet distribution with parameter
1
2. A set of LIP values ε is ﬁxed, and for each of these and each
probability distribution, equations (29) and (33) are solved,
setting the left hand side equal to ε and solving for αGRR
and αCR. The mutual information I(X; Y ) is then calculated,
170
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

which is normalised by dividing by H(X). The resulting
averages and standard deviations are displayed in Figure 6.
On the whole, it can be seen that the larger a is compared to
c, the more utility CR provides compared to GRR. However,
this does not tell the whole story, as the difference between
datasets has more impact on the utility than the difference
between methods.
E. GRR and CR parameter α
To investigate what property of the probability distribution
pXS causes CR to outperform GRR, the parameters αCR and
αGRR that govern the privacy protocols CR and GRR are
considered. Both of these have the property that the higher
their value, the less ‘random’ the protocols are, resulting in a
better utility. Since these α are found from ε through different
equations, the difference in utility of GRR and CR for different
probability distributions may be explained by a difference
in α. This assertion is tested for 100 randomly generated
distributions in Figure 7. As can be seen, the difference in
mutual information can for a large part be explained by a
difference in α (ρ = 0.9815, ρ = 0.9889, and ρ = 0.9731,
respectively). In Figure 8, the relation between α and the LIP
value ε for the experiments in 5(b) and 5(d) is shown. The
fact that αGRR > αCR in 8(a) corresponds to the fact that
GRR outperforms CR in 5(b), and the opposite relation holds
between 8(b) and 5(d).
Unfortunately, we were not able to relate the differ-
ence in parameter α to other properties of the distribution.
Without presenting details we mention that the properties
I(X; S), maxx,s px,s, maxx px and maxs ps do not appear to
have an impact on the difference in utility between GRR and
CR.
IX. CONCLUSIONS AND FUTURE WORK
Local data sanitisation protocols have the advantage of
being scalable for large numbers of users. Furthermore, the
advantage of using differential privacy-like privacy metrics
is that they provide worst-case guarantees, ensuring that the
privacy of every user is sufﬁciently protected. For both ε-LDP
and ε-LIP methods are derived to ﬁnd sanitisation protocols
that maximise mutual information between input and output,
solving the PF problem for these metrics.
Within this setting, it can be observed that ε-LIP has two
main advantages over ε-LDP. First, it ﬁts better within the PF
setting, where the distribution pS,X is (at least approximately)
known to the estimator. Second, ﬁnding the optimal protocol
is signiﬁcantly faster than under LDP, especially for small ε.
If one nevertheless prefers ε-LDP as a privacy metric, then it
is still worthwile to ﬁnd the optimal ε
2-LIP protocol, as this
can be found signiﬁcantly faster, at a low utility penalty.
In the multiple attributes setting, it is shown that ε-SRLIP
provides additional privacy guarantees compared to ε-LIP,
since without this requirement a protocol can lose all its
privacy protection in the presence of side channels. Unfor-
tunately, however, experiments show that this is paid for both
in computation time and in utility.
(a). ε = 1
(b). ε = 1.5
(c). ε = 2
Figure 7. Difference in α versus difference in utility for 100 randomly
generated probability distributions, for a = c = 5.
With regard to the speciﬁc protocols, it is found that the
newly introduced protocol, CR, generally outperforms OUE,
especially for high values of ε-LIP. This can be explained
from the fact that by design the utility of OUE is capped at
1
2 H(X). CR behaves more or less similar to GRR, and which
of these two protocols performs best depends on properties
of the joint distribution pX,S. In particular, it largely depends
on which of the two protocols has the highest value of their
governing parameter α. Also, it can be seen that CR performs
better on average if a is large compared to c.
For further research, a number of important avenues remain
171
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(a) S = occupation, X = education
(b) S = occupation, X = relationship
Figure 8. Value of GRR and CR parameter α for different values of ε for the adult-dataset.
to be explored. First, the aggregator’s knowledge about pS,X
may not be perfect, because they may learn about pS,X
through observing (⃗S, ⃗X). Incorporating this uncertainty leads
to robust optimisation [28], which would give stronger privacy
guarantees.
Second, it might be possible to improve the method of
obtaining ε-SRLIP protocols via Theorem 4. Examining its
proof shows that lower values of εj may sufﬁce to still ensure
ε-SRLIP. Furthermore, the optimal choice of (εj)j≤m such
that P
j εj = ε might not be εj =
ε
m. However, it is
computationally prohibitive to perform the vertex enumera-
tion for many different choices of (εj)j≤m, and as such a
new theoretical approach is needed to determine the optimal
(εj)j≤m from ε and pS,X.
Third, it would be interesting to see if there are other ways
to close the gap between the theoretically optimal protocol,
which may be hard to compute in practice, and general LDP
protocols, which do not see the difference between sensitive
and non-sensitive information. This is relevant because CR
needs both S and X as input, and there may be situations
where access to S is not available.
Finally, although CR outperforms GRR and OUE for some
datasets, it does not do so consistently. More research into the
properties of distributions where CR fails to provide a signif-
icant advantage might lead to improved privacy protocols.
ACKNOWLEDGEMENTS
This work was supported by NWO grant 628.001.026
(Dutch Research Council, the Hague, the Netherlands).
REFERENCES
[1]
Milan Lopuha¨a-Zwakenberg. “The Privacy Funnel from
the Viewpoint of Local Differential Privacy”. In: Four-
teenth International Conference on the Digital Society
(ICDS) (2020), pp. 19–24.
[2]
Flavio du Pin Calmon, Ali Makhdoumi, Muriel M´edard,
Mayank Varia, Mark Christiansen, and Ken R Duffy.
“Principal inertia components and applications”. In:
IEEE Transactions on Information Theory 63.8 (2017),
pp. 5011–5038.
[3]
Ni Ding and Parastoo Sadeghi. “A Submodularity-based
Agglomerative Clustering Algorithm for the Privacy
Funnel”. In: arXiv:1901.06629 (2019). Preprint, ac-
cessed 2020.11.8.
[4]
Fabian Prasser, Florian Kohlmayer, Ronald Lauten-
schlaeger, and Klaus A. Kuhn. “Arx-a comprehen-
sive tool for anonymizing biomedical data”. In: AMIA
Annual Symposium Proceedings. Vol. 2014. American
Medical Informatics Association. 2014, p. 984.
[5]
Ali Makhdoumi, Salman Salamatian, Nadia Fawaz, and
Muriel M´edard. “From the information bottleneck to
the privacy funnel”. In: 2014 IEEE Information Theory
Workshop (ITW 2014). IEEE. 2014, pp. 501–505.
[6]
Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi
Nissim, Sofya Raskhodnikova, and Adam Smith. “What
can we learn privately?” In: SIAM Journal on Comput-
ing 40.3 (2011), pp. 793–826.
[7]
Bo Jiang, Ming Li, and Ravi Tandon. “Local Infor-
mation Privacy with Bounded Prior”. In: ICC 2019-
2019 IEEE International Conference on Communica-
tions (ICC). IEEE. 2019, pp. 1–7.
[8]
Salman Salamatian, Flavio du Pin Calmon, Nadia
Fawaz, Ali Makhdoumi, and Muriel M´edard. “Privacy-
Utility Tradeoff and Privacy Funnel”. In: http:// www.
mit . edu / ∼salmansa / ﬁles / privacy TIFS . pdf (2020).
Preprint, accessed 2020.11.8.
[9]
Peter Kairouz, Sewoong Oh, and Pramod Viswanath.
“Extremal mechanisms for local differential privacy”.
In: Advances in neural information processing systems.
2014, pp. 2879–2887.
[10]
Borzoo Rassouli and Deniz Gunduz. “On perfect pri-
vacy”. In: 2018 IEEE International Symposium on In-
formation Theory (ISIT). IEEE. 2018, pp. 2551–2555.
172
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[11]
Imre B´ar´any and Attila P´or. “On 0-1 polytopes with
many facets”. In: Advances in Mathematics 161.2
(2001), pp. 209–228.
[12]
Naftali Tishby, Fernando C Pereira, and William
Bialek. “The information bottleneck method”. In:
arXiv:physics/0004057 (2000). Preprint.
[13]
SY Kung. “A compressive privacy approach to gener-
alized information bottleneck and privacy funnel prob-
lems”. In: Journal of the Franklin Institute 355.4 (2018),
pp. 1846–1872.
[14]
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and
Adam Smith. “Calibrating noise to sensitivity in private
data analysis”. In: Theory of cryptography conference.
Springer. 2006, pp. 265–284.
[15]
Paul Cuff and Lanqing Yu. “Differential privacy as
a mutual information constraint”. In: Proceedings of
the 2016 ACM SIGSAC Conference on Computer and
Communications Security. 2016, pp. 43–54.
[16]
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSh-
erry, Ilya Mironov, and Moni Naor. “Our data, our-
selves: Privacy via distributed noise generation”. In:
Annual International Conference on the Theory and
Applications of Cryptographic Techniques. Springer.
2006, pp. 486–503.
[17]
Cynthia Dwork and Guy N Rothblum. “Concentrated
differential privacy”. In: arXiv:1603.01887
(2016).
Preprint, accessed 2020.11.8.
[18]
Ilya Mironov. “R´enyi differential privacy”. In: 2017
IEEE 30th Computer Security Foundations Symposium
(CSF). IEEE. 2017, pp. 263–275.
[19]
´Ulfar
Erlingsson,
Vitaly
Feldman,
Ilya
Mironov,
Ananth Raghunathan, Shuang Song, Kunal Talwar, and
Abhradeep Thakurta. “Encode, shufﬂe, analyze privacy
revisited: formalizations and empirical evaluation”. In:
arXiv:2001.03618 (2020). Preprint, accessed 2020.11.8.
[20]
Daniel Kifer and Ashwin Machanavajjhala. “Pufferﬁsh:
A framework for mathematical privacy deﬁnitions”. In:
ACM Transactions on Database Systems (TODS) 39.1
(2014), pp. 1–36.
[21]
Naoise Holohan, Douglas J Leith, and Oliver Mason.
“Extreme points of the local differential privacy poly-
tope”. In: Linear Algebra and its Applications 534
(2017), pp. 78–96.
[22]
David Avis and Komei Fukuda. “A pivoting algorithm
for convex hulls and vertex enumeration of arrange-
ments and polyhedra”. In: Discrete & Computational
Geometry 8.3 (1992), pp. 295–313.
[23]
Milan Lopuha¨a-Zwakenberg, Boris ˇSkori´c, and Ninghui
Li. “Information-theoretic metrics for Local Differen-
tial Privacy protocols”. In: arXiv:1910.07826 (2019).
Preprint, accessed 2020.11.8.
[24]
Mengmeng Yang, Lingjuan Lyu, Jun Zhao, Tianqing
Zhu, and Kwok-Yan Lam. “Local Differential Privacy
and Its Applications: A Comprehensive Survey”. In:
arXiv:2008.03686 (2020). Preprint, 2020.11.8.
[25]
Stanley L Warner. “Randomized response: A survey
technique for eliminating evasive answer bias”. In:
Journal of the American Statistical Association 60.309
(1965), pp. 63–69.
[26]
Tianhao Wang, Jeremiah Blocki, Ninghui Li, and
Somesh Jha. “Locally differentially private protocols
for frequency estimation”. In: 26th {USENIX} Security
Symposium ({USENIX} Security 17). 2017, pp. 729–
745.
[27]
Dheeru Dua and Casey Graff. UCI Machine Learning
Repository. 2017. URL: http://archive.ics.uci.edu/ml/
datasets/Adult.
[28]
Dimitris Bertsimas, Vishal Gupta, and Nathan Kallus.
“Data-driven robust optimization”. In: Mathematical
Programming 167.2 (2018), pp. 235–292.
APPENDIX A
PROOFS
Proof of Theorem 4. For
J
⊆
{1, . . . , m}
and
j
∈
{1, . . . , m}, deﬁne J[j] := J ∩ {1, . . . , j − 1}. Furthermore,
write X \J = Q
j /∈J X j, and its elements as x\J. Deﬁne
ε := P
j εj. Then
py|s,xJ =
X
x\J
py|x px\J|s,xJ
(37)
= pyJ|xJ
X
x\j

Y
j /∈J
pyj|xj

 px\J|s,xJ
(38)
= pyJ|xJ
X
x\j
Y
j /∈J
pyj|xj pxj|s,xJ[j]
(39)
= pyJ|xJ
Y
j /∈J
X
xj
pyj|xj pxj|s,xJ[j]
(40)
= pyJ|xJ
Y
j /∈J
pyj|s,xJ[j]
(41)
≤ pyJ|xJ
Y
j /∈J
eεj pyj|xJ[j]
(42)
≤ eε pyJ|xJ
Y
j /∈J
pyj|xJ[j]
(43)
= eε py|xJ .
(44)
The fact that e−ε py|xJ ≤ py|s,xJ is proven analogously.
Proof of Proposition 1. Write Qy|x,s = P(CRα(x, s) = y).
Then
Qy|x,s =
X
s′
P(CRα(x, s) = y|˜s = s′)P(˜s = s′|S = s)
(45)
=
eα
eα + c − 1 +
1
eα + c − 1
X
s′̸=s
py|s′,
(46)
where δx=y is the Kronecker delta. It follows that
P(CRα(X, S) = y|S = s)
=
X
x
Qy|x,s px|s
(47)
173
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

=
eα
eα + c − 1 py|s +
1
eα + c − 1
X
s′̸=s
py|s′
(48)
=
eα − 1
eα + c − 1 py|s +
1
eα + c − 1
X
s′
py|s′,
(49)
P(CRα(X, S) = y)
=
X
s
P(CRα(X, S) = y|S = s) ps
(50)
=
eα
eα + c − 1 py +
1
eα + c − 1
X
s
X
s′̸=s
py|s′ ps
(51)
=
eα
eα + c − 1 py +
1
eα + c − 1
X
s′
py|s′
X
s̸=s′
ps
(52)
=
eα
eα + c − 1 py +
1
eα + c − 1
X
s′
(py|s′ − py,s′)
(53)
=
eα − 1
eα + c − 1 py +
1
eα + c − 1
X
s′
py|s′ .
(54)
It follows that
L(α) = max
y,s
ln P(CRα(X, S) = y|S = s)
P(CRα(X, S) = y)
 ,
(55)
hence CRα satisﬁes ε-LIP if and only if ε ≥ L(α).
174
International Journal on Advances in Security, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/security/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

