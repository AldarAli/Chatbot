Advanced Computation of a Sparse Precision Matrix
HADAP: A Hadamard-Dantzig Estimation of a Sparse Precision Matrix
Mohammed Elanbariâˆ—, Reda Rawiâ€ , Michele Ceccarelliâ€ , Othmane Bouhaliâ€¡, Halima Bensmailâ€ 
âˆ—Sidra Medical and Research Center, Qatar Foundation
â€ Qatar Computing Research Institute, Qatar Foundation
â€¡Texas A&M University-Qatar,
Doha, Qatar
Email: âˆ—melanbari@sidra.org; â€ {rrawi, mceccarelli, hbensmail}@qf.org.qa; â€¡othmane.bouhali@qatar.tamu.edu
Abstractâ€”Estimating large sparse precision matrices is an in-
teresting and challenging problem in many ï¬elds of sciences,
engineering, and humanities, thanks to advances in computing
technologies. Recent applications often encounter high dimension-
ality with a limited number of data points leading to a number
of covariance parameters that greatly exceeds the number of
observations. Several methods have been proposed to deal with
this problem, but there is no guarantee that the obtained
estimator is positive deï¬nite. Furthermore, in many cases, one
needs to capture some additional information on the setting of
the problem. In this work, we propose an innovative approach
named HADAP for estimating the precision matrix by minimizing
a criterion combining a relaxation of the gradient-log likelihood
and a penalization of lasso type. We derive an efï¬cient Alternating
Direction Method of multipliers algorithm to obtain the optimal
solution.
Keywordsâ€“Covariance
matrix;
Frobenius
norm;
Gaussian
graphical model; Precision matrix; Alternating method of multi-
pliers; Positive-deï¬nite estimation; Sparsity..
I.
INTRODUCTION
Recent applications often encounter high dimensionality
with a limited number of data points leading to a number
of covariance parameters that greatly exceeds the number of
observations. Examples include marketing, e-commerce, and
warehouse data in business; microarray, and proteomics data
in genomics and heath sciences; and biomedical imaging,
functional magnetic resonance imaging, tomography, signal
processing, high-resolution imaging, and functional and longi-
tudinal data. In biological sciences, one may want to classify
diseases and predict clinical outcomes using microarray gene
expression or proteomics data, in which hundreds of thousands
of expression levels are potential covariates, but there are typi-
cally only tens or hundreds of subjects. Hundreds of thousands
of single-nucleotide polymorphisms are potential predictors
in genome-wide association studies. The dimensionality of
the variables spaces grows rapidly when interactions of such
predictors are considered. Large-scale data analysis is also
a common feature of many problems in machine learning,
such as text and document classiï¬cation and computer vision.
For a p Ã— p covariance matrix Î£, there are p(p + 1)/2
parameters to estimate, yet the sample size n is often small.
In addition, the positive-deï¬niteness of Î£ makes the problem
even more complicated. When n > p, the sample covariance
matrix is positive-deï¬nite and unbiased, but as the dimension
p increases, the sample covariance matrix tends to become
unstable and can fail to be consistent.
II.
BACKGROUND
In this paper, we use the notation in Table I.
TABLE I. NOTATION USED IN THIS PAPER
Notation
Description
A âª° 0
A âˆˆ RnÃ—p is symmetric and positive
semideï¬nite
A â‰» 0
A âˆˆ RnÃ—p is symmetric and positive
deï¬nite
âˆ¥Aâˆ¥1
â„“1 norm of A âˆˆ RnÃ—p, i.e P
ij aij
âˆ¥Aâˆ¥âˆ
â„“âˆ norm of A âˆˆ RnÃ—p, i.e maxijaij
âˆ¥Aâˆ¥2
spectral norm of A âˆˆ RiÃ—j i.e. the max-
mum eigenvalues of A â‰» 0
âˆ¥Aâˆ¥F
Frobenius norm of A âˆˆ RnÃ—p i.e
qP
ij a2
ij
Tr(A)
trace of A âˆˆ RpÃ—p, i.e Tr(A) = P
i aii
vec(A)
stacked form of A âˆˆ RnÃ—p, i.e
vec(A) = (a1,1, . . . , an,1, a1,2, . . . , a1,p, . . . , an,p)t,
vec(ABC)
(Ct âŠ— A) vec(B)
vec(A â—¦ B)
diag(vec(A))vec(B)
diag(A)
diag(u) =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
u1
0
Â· Â· Â·
0
0
u2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
uk
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
âˆˆ RNÃ—N,
A â—¦ B
Hadamard product of A and B, âˆˆ RNÃ—M, i.e
element-wise multiplication (A â—¦ B)ij = (A)ij Ã— (B)ij
A âŠ— B
Kronecker product of A and B
A âŠ— B =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a1,1B
a1,2B
Â· Â· Â·
a1,mB
a2,1B
a2,2B
Â· Â· Â·
a2,mB
...
...
...
...
an,1B
an,2B
Â· Â· Â·
an,mB
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
A âŠ— B âˆˆ RnpÃ—mq
a) Existing Methods: Due in part to its importance, there
has been an active line of work on efï¬cient optimization
methods for solving the â„“1 regularized Gaussian MLE problem:
PSM that uses projected subgradients [1], ALM using alter-
nating linearization [2], IPM an inexact interior point method
[11], SINCO a greedy coordinate descent method [3] and
Glass a block coordinate descent method [4][5] etc. For typical
high-dimensional statistical problems, optimization methods
typically suffer sub-linear rates of convergence [6]. This would
be too expensive for the Gaussian MLE problem, since the
number of matrix entries scales quadratically with the number
of nodes.
b) Sparse Modeling: Sparse modeling has been widely
used to deal with high dimensionality. The main assumption
is that the p-dimensional parameter vector is sparse, with
1
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

many components being exactly zero or negligibly small.
Such an assumption is crucial in identiï¬ability, especially
for the relatively small sample size. Although the notion of
sparsity gives rise to biased estimation in general, it has proved
to be effective in many applications. In particular, variable
selection can increase the estimation accuracy by effectively
identifying important predictors and can improve the model in-
terpretability. To solve this, constraints are frequently imposed
on the covariance to reduce the number of parameters in the
model including the spectral decomposition, Bayesian meth-
ods, modeling the matrix-logarithm, nonparametric smoothing,
banding/thresholding techniques (see [5], [7][8]).
Speciï¬cally,
thresholding
is
proposed
for
estimating
permutation-invariant consistent covariance matrices when the
true covariance matrix is bandable [4]. In this sense, threshold-
ing is more robust than banding/tapering for real applications.
In this paper we focus on the soft-thresholding technique as
in [4] and [9] because it can be formulated as the solution
of a convex optimization problem. In fact, Graphical Lasso
approach (Glasso) is introduced as the following. Let âˆ¥.âˆ¥F be
the Frobenius norm and |.|1 be the element-wise â„“1-norm of all
non-diagonal elements. Then the soft-thresholding covariance
estimator is equal to
Ë†â„¦+ = arg min
â„¦
1
2âˆ¥â„¦ âˆ’ Ë†â„¦nâˆ¥2
F + Î»|â„¦|1,
(1)
where â„¦ = Î£âˆ’1, where â„¦ = Ï‰ij1â‰¤i,jâ‰¤p is the precision
matrix, Ë†â„¦ is the solution of (1) and Î» is a tuning parameter.
This equation emphasizes the fact that the solution â„¦ may not
be unique [such nonuniqueness can occur if rank(â„¦) < p].
It has been shown that the existence of a robust optimization
formulation is related to kernel density estimation [10] where
property of the solution and a proof that Lasso is consistent
was given using robustness directly. Moreover, [11] proved
that there exists a linear subspace that is almost surely unique,
meaning that it will be the same under different boundary sets
corresponding to different solutions of equations of type (1).
However, there is no guarantee that the thresholding estimator
is always positive deï¬nite (see [9], [12] and [13]). Although
the positive deï¬nite property is guaranteed in the asymptotic
setting with high probability, the actual estimator can be an
indeï¬nite matrix, especially in real data analysis.
Structure of the inverse covariance matrix has attracted
special interest. Example, when dealing with asset allocation
in ï¬nance. The ï¬nancial problems are often written in terms
of the inverse covariance matrix, in such case the covariance
structure of return series is often estimated using only the
most recent data, resulting in a small sample size compared
to the number of parameters to be estimated. In biological
applications (graphical models), zero correlations represent
conditional independence between the variables. For exam-
ple to account for network information in the analysis of
metabolites data, the reconstruction of metabolic networks
from a collection of metabolite patterns is a key question in
the computational research ï¬eld. Previous attempts focused on
linear metabolite associations measured by Pearson correlation
coefï¬cients [14]. A major drawback of correlation networks,
however, is their inability to distinguish between direct and
indirect associations. Correlation coefï¬cients are generally
high in large-scale omics data sets, suggesting a plethora of
indirect and systemic associations. Gaussian Graphical models
(GGMs) circumvent indirect association effects by evaluating
conditional dependencies in multivariate Gaussian distributions
or equivalently the inverse covariance matrix (see [15]).
On the other hand, additional structure on the precision ma-
trix coefï¬cients is also often required (Comparative genomic
hybridizaton) where the difference between two successive
coefï¬cients is required to be small or to vary slowly.
Our Contributions: In this paper, we emphasize on intro-
ducing a new criteria that insures the positive-deï¬niteness of
the covariance matrix adding a tuning parameter Ïµ > 0 in
the constraints. This additional constraint will guard against
positive semi-deï¬nite. We add structure on the coefï¬cient
of the precision matrix and we derive an efï¬cient ADMM
algorithm form to obtain an optimal solution. We perform
Alternating Direction Method of Multipliers steps, a variant of
the standard Augmented Lagrangian method, that uses partial
updates, but with three innovations that enable ï¬nessing the
caveats detailed above.
In Section III, we link the gaussian graphical model to the
precision matrix estimation, in which we show that recovering
the structure of a graph G is equivalent to the estimation
of the support of the precision matrix and describe different
penalized optimization algorithm that have been used to solve
this problem and their limitations.
We describe, in Section IV, the ADMM and its application
to solve the estimation of the precision matrix under the
Dantzig-selector setting and we show its ability to perform
distributed optimization where we break up the big optimiza-
tion problem into smaller problems that are more manageable.
In fact, as in the recent methods [9][12], we build on the
observation that the Newton direction computation is a Lasso
problem, and perform iterative coordinate descent to solve this
Lasso problem. Then, we use a Dantzig-selector rule to obtain
a step-size that ensures positive-deï¬niteness of the next iterate.
In Section V, we validate our algorithm on artiï¬cial and real
data.
III.
LINK WITH GAUSSIAN GRAPHICAL MODEL (GGM)
Given a data set consisting of samples from a zero mean
Gaussian distribution in Rp,
X(i) âˆ¼ N(0, Î£), i = 1, ..., n,
(2)
with positive deï¬nite pÃ—p covariance matrix Î£. Note that the
zero mean assumption in equation 2 is mainly for simplifying
notation. The task here is how to estimate the precision matrix
â„¦ = Î£âˆ’1 when it is sparse. We are particularly interested by
the case of sparse â„¦ because it is closely linked to the selection
of graphical models.
To be more speciï¬c, let G = (V, E) be a graph representing
conditional independence relations between components of
X = (X1, ..., Xp).
â€¢
The vertex set V has p components X1, ..., Xp;
â€¢
The edge set E consists of ordered pairs (i, j) of V Ã—
V , where (i, j) âˆˆ E if there is an edge between Xi
and Xj.
We exclude the edge between two vertexes Xi and Xj if and
only if Xi and Xj are independent given {Xk, k Ì¸= i, j}.
If in addition X âˆ¼ N(0, Î£), the conditional independence
between Xi and Xj given the remaining variables is equivalent
2
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

to Ï‰ij = 0, where â„¦ = Î£âˆ’1 = {Ï‰ij}1â‰¤i,jâ‰¤p. Hence, in the
case of Gaussian Graphical Model (GGM), the edges are given
by the inverse of covariance matrix. More precisely, recovering
the structure of a graph G is equivalent to the estimation of
the support of the precision matrix â„¦. When n > p, one
can estimate Î£âˆ’1 by maximum likelihood, but when p > n
this is not possible because the empirical covariance matrix is
singular and often performs poorly [16]. Since the â„“1-norm
is the tighest convex upper bound of the cardinality of a
matrix, several â„“1-regularization methods have been proposed.
Consequently a number of authors have considered minimizing
an â„“1 penalized log-likelihood (see [4] and [17]). It is also
sometimes called Glasso [4] after the algorithm that efï¬ciently
computes the solution. It consists of solving the penalized
problem
Ë†â„¦Glasso = arg min
â„¦âª°0{âŸ¨Ë†Î£n, â„¦âŸ© âˆ’ log det â„¦ + Î»âˆ¥â„¦âˆ¥1}.
(3)
where Ë†Î£n is the sample covariance matrix and Î» is a tuning
parameter. The term âˆ¥â„¦âˆ¥1 encourages sparseness of the pre-
cision matrix. The asymptotic properties of the estimator has
been studied in [17]. Lauritzen [13] proposed a constrained l1
minimization proposed a constrained l1 minimization proce-
dure for estimating sparse precision matrices by solving the
optimization problem
minimize
âˆ¥â„¦âˆ¥1
subject to
âˆ¥Ë†Î£nâ„¦ âˆ’ Iâˆ¥âˆ â‰¤ Î»,
(4)
where Î» is a tuning parameter. The authors established the
rates of convergence under both the entry-wise lâˆ and the
Frobenius norm. In computationally viewpoint, equation 4 can
be solved by remarking that it can be decomposed into a series
of Dantzig selector problems [18]. This observation is useful
in both implementation and technical analysis. Theoretically,
the authors prove that the estimator is positive deï¬nite with
high probability. However, in practice there is no guarantee
that the estimator is always positive deï¬nite, especially in real
data analysis.
IV.
ALTERNATING DIRECTION METHOD OF MULTIPLIERS
FOR HADAP
We deï¬ne our algorithm HADAP as a solution to the
following problem
Ë†â„¦+ = arg min
â„¦âª°ÏµI
1
2âˆ¥Ë†Î£nâ„¦ âˆ’ Iâˆ¥2
F + Î»|D â—¦ â„¦|1,
(5)
where
Ë†Î£n
is
the
empirical
covariance
matrix,
D
=
(dij)1â‰¤i,jâ‰¤p is an arbitrary matrix with non-negative elements
where D can take different forms: it can be the matrix of all
ones or it can be a matrix with zeros on the diagonal to avoid
shrinking diagonal elements of â„¦. Furthermore, we can take
D with elements dij = 1iÌ¸=j|Ë†â„¦init
ij |, where Ë†â„¦init is an initial
estimator of â„¦. The later choice of D corresponds to precision
matrix analogue of the Adaptive Lasso penalty.
We propose an ADMM algorithm to solve the problem (5).
To derive ADMM, we will ï¬rst introduce a new variable Î˜
and an equality constraint as follows:
(Ë†Î˜+, Ë†â„¦+)
=
arg min
Î˜,â„¦
1
2âˆ¥Ë†Î£nâ„¦ âˆ’ Iâˆ¥2
F + Î»|D â—¦ â„¦|1
:
â„¦ = Î˜, Î˜ âª° ÏµI} .
(6)
The solution to (6) gives the solution to (5). To deal with the
problem (6), we have to minimize its augmented Lagrangian
function for some given penalty parameter Ï, i.e.
LÏ(Î˜, â„¦, Î›)
=
1
2âˆ¥Ë†Î£nâ„¦ âˆ’ Iâˆ¥2
F + Î»|D â—¦ â„¦|1 âˆ’ âŸ¨Î›, Î˜ âˆ’ â„¦âŸ©
+
1
2Ïâˆ¥Î˜ âˆ’ â„¦âˆ¥2
F ,
(7)
where Î› is the Lagrange multiplier.
At iteration k, the ADMM algorithm consists of the three
steps, namely Î˜-Step, â„¦-Step and the dual-update step :
Î˜k+1 := arg min
Î˜âª°ÏµI LÏ(Î˜, â„¦k, Î›k);
Î˜-minimization
(8)
â„¦k+1 := arg min
â„¦ LÏ(Î˜k+1, â„¦, Î›k);
â„¦-minimization
(9)
Î›k+1 := Î›k âˆ’ 1
Ï(Î˜k+1 âˆ’ â„¦k+1).
dual-update
(10)
â€¢
In the ï¬rst step of the ADMM algorithm, we ï¬x â„¦
and Î› and minimize the augmented Lagrangian over
Î˜.
â€¢
In the second step, we ï¬x Î˜ and Î› and minimize the
augmented Lagrangian over â„¦.
â€¢
Finally, we update the dual variable Î›.
To further simplify the ADMM algorithm, we will derive the
closed-form solutions for (8)-(10).
A. The Î˜-Step.
Let A+ be the projection of a matrix A onto the
convex cone {Î˜ âª° ÏµI}. Assume that A has the eigen-
decomposition Pp
j=1 Î»jvjvt
j. Then, it is well known that
A+ = Pp
j=1 max(Ïµ, Î»j)vjvt
j. Using this property, the Î˜-Step
can be analytically solved as follows
Î˜k+1
=
arg min
Î˜âª°ÏµI LÏ(Î˜, â„¦k, Î›k)
=
arg min
Î˜âª°ÏµI
1
2âˆ¥Ë†Î£nâ„¦k âˆ’ Iâˆ¥2
F + Î»|D â—¦ â„¦k|1
âˆ’
âŸ¨Î›k, Î˜ âˆ’ â„¦kâŸ© + 1
2Ïâˆ¥Î˜ âˆ’ â„¦kâˆ¥2
F
=
arg min
Î˜âª°ÏµI âˆ’âŸ¨Î›k, Î˜âŸ© + 1
2Ïâˆ¥Î˜ âˆ’ â„¦kâˆ¥2
F
=
arg min
Î˜âª°ÏµI âˆ¥Î˜ âˆ’ (â„¦k + ÏÎ›k)âˆ¥2
F
=
(â„¦k + ÏÎ›k)+
=
p
X
j=1
max(Ïµ, Î»j)vjvt
j,
where Pp
j=1 Î»jvjvt
j is eigen-decomposition of â„¦k + ÏÎ›k.
3
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

B. The â„¦-Step.
â„¦k+1
=
arg min
â„¦ LÏ(Î˜k+1, â„¦, Î›k)
=
arg min
â„¦
1
2âˆ¥Ë†Î£nâ„¦ âˆ’ Iâˆ¥2
F + Î»|D â—¦ â„¦|1
âˆ’
âŸ¨Î›k, Î˜k+1 âˆ’ â„¦âŸ© + 1
2Ïâˆ¥Î˜k+1 âˆ’ â„¦âˆ¥2
F
=
arg min
â„¦
1
2âˆ¥Ë†Î£nâ„¦ âˆ’ Iâˆ¥2
F + Î»|D â—¦ â„¦|1
+
âŸ¨Î›k, â„¦âŸ© + 1
2Ïâˆ¥Î˜k+1 âˆ’ â„¦âˆ¥2
F
=
arg min
â„¦
1
2âˆ¥Ë†Î£nâ„¦ âˆ’ Iâˆ¥2
F + 1
2Ïâˆ¥â„¦ âˆ’ Î˜k+1âˆ¥2
F
+
âŸ¨Î›k, â„¦âŸ© + Î»|D â—¦ â„¦|1.
We have
1
2âˆ¥Ë†Î£nâ„¦ âˆ’ Iâˆ¥2
F
=
1
2âŸ¨Ë†Î£nâ„¦ âˆ’ I, Ë†Î£nâ„¦ âˆ’ IâŸ©
=
1
2
n
âˆ¥Ë†Î£nâ„¦âˆ¥2
F âˆ’ 2âŸ¨Ë†Î£nâ„¦, IâŸ© + âˆ¥Iâˆ¥2
F
o
=
1
2
n
âˆ¥Ë†Î£nâ„¦âˆ¥2
F âˆ’ 2 Tr(â„¦t Ë†Î£t
nI) + Tr(ItI)
o
=
1
2
n
âˆ¥Ë†Î£nâ„¦âˆ¥2
F âˆ’ 2 Tr(â„¦t Ë†Î£n) + p
o
=
1
2
n
âˆ¥Ë†Î£nâ„¦âˆ¥2
F âˆ’ 2âŸ¨â„¦, Ë†Î£nâŸ© + p
o
.
and
1
2Ïâˆ¥â„¦ âˆ’ Î˜k+1âˆ¥2
F
=
1
2ÏâŸ¨â„¦ âˆ’ Î˜k+1, â„¦ âˆ’ Î˜k+1âŸ©
=
1
2Ï

âˆ¥â„¦âˆ¥2
F âˆ’ 2âŸ¨â„¦, Î˜k+1âŸ© + âˆ¥Î˜k+1âˆ¥2
F
	
.
Then, the â„¦-Step is equivalent to
â„¦k+1
=
arg min
â„¦
1
2âˆ¥Ë†Î£nâ„¦ âˆ’ Iâˆ¥2
F + 1
2Ïâˆ¥â„¦ âˆ’ Î˜k+1âˆ¥2
F
+
âŸ¨Î›k, â„¦âŸ© + Î»|D â—¦ â„¦|1
=
arg min
â„¦
1
2
n
âˆ¥Ë†Î£nâ„¦âˆ¥2
F âˆ’ 2âŸ¨â„¦, Ë†Î£nâŸ© + p
o
+
1
2Ï

âˆ¥â„¦âˆ¥2
F âˆ’ 2âŸ¨â„¦, Î˜k+1âŸ© + âˆ¥Î˜k+1âˆ¥2
F
	
+ âŸ¨Î›k, â„¦âŸ©
+
Î»|D â—¦ â„¦|1
=
arg min
â„¦
1
2
n
âˆ¥Ë†Î£nâ„¦âˆ¥2
F âˆ’ 2âŸ¨â„¦, Ë†Î£nâŸ©
o
+
1
2Ï

âˆ¥â„¦âˆ¥2
F âˆ’ 2âŸ¨â„¦, Î˜k+1âŸ©
	
+
âŸ¨Î›k, â„¦âŸ© + Î»|D â—¦ â„¦|1
=
arg min
â„¦
1
2

âˆ¥Ë†Î£nâ„¦âˆ¥2
F + 1
Ïâˆ¥â„¦âˆ¥2
F
âˆ’
2âŸ¨â„¦, Ë†Î£n + 1
ÏÎ˜k+1 âˆ’ Î›kâŸ©

+ Î»|D â—¦ â„¦|1
=
arg min
â„¦
1
2

âˆ¥Ë†Î£nâ„¦âˆ¥2
F + 1
Ïâˆ¥â„¦âˆ¥2
F
âˆ’
2
ÏâŸ¨â„¦, Ï(Ë†Î£n âˆ’ Î›k) + Î˜k+1âŸ©

+ Î»|D â—¦ â„¦|1.
At this level, we are not able to derive a closed form for â„¦. To
overcome this problem, we propose to derive a new ADMM to
update â„¦. To do this, we reparametrize the D â—¦ â„¦ with Î“ and
we add an equality constraint D â—¦ â„¦ = Î“, then we minimize
1
2

âˆ¥Ë†Î£nâ„¦âˆ¥2
F + 1
Ïâˆ¥â„¦âˆ¥2
F âˆ’ 2
ÏâŸ¨â„¦, Ï(Ë†Î£n âˆ’ Î›k) + Î˜k+1âŸ©

+ Î»|Î“|1
subject to
D â—¦ â„¦ = Î“.
(11)
The
augmented
Lagrangian
associated
to
this
problem
Lk
Ï(â„¦, Î“, âˆ†) is
1
2

âˆ¥Ë†Î£nâ„¦âˆ¥2
F + 1
Ïâˆ¥â„¦âˆ¥2
F âˆ’ 2
ÏâŸ¨â„¦, Ï(Ë†Î£n âˆ’ Î›k) + Î˜k+1âŸ©

+
Î»|Î“|1 âˆ’ âŸ¨âˆ†, Î“ âˆ’ D â—¦ â„¦âŸ© + 1
2Ïâˆ¥Î“ âˆ’ D â—¦ â„¦âˆ¥2
F ,
(12)
where âˆ† is the Lagrange multiplier and Ï is the same param-
eter as in (7).
As before, the ADMM for this problem consists of the follow-
ing three intermediate steps:
â„¦j+1
k
:= arg min
â„¦ Lk
Ï(â„¦, Î“j, âˆ†j);
â„¦-minimization
(13)
Î“j+1 := arg min
Î“ Lk
Ï(â„¦j+1
k
, Î“, âˆ†j);
Î“-minimization
(14)
âˆ†j+1 := âˆ†j âˆ’ 1
Ï(â„¦j+1
k
âˆ’ Î“j+1).
dual-update
(15)
C. The intermediate â„¦-Step.
â„¦j+1
k
=
arg min
â„¦ Lk
Ï(â„¦, Î“j, âˆ†j)
=
arg min
â„¦
1
2

âˆ¥Ë†Î£nâ„¦âˆ¥2
F + 1
Ïâˆ¥â„¦âˆ¥2
F
âˆ’
2
ÏâŸ¨â„¦, Ï(Ë†Î£n âˆ’ Î›k) + Î˜k+1âŸ©

+ Î»|Î“j|1
âˆ’
âŸ¨âˆ†j, Î“j âˆ’ D â—¦ â„¦âŸ© + 1
2Ïâˆ¥Î“j âˆ’ D â—¦ â„¦âˆ¥2
F
=
1
2

âˆ¥Ë†Î£nâ„¦âˆ¥2
F + 1
Ïâˆ¥â„¦âˆ¥2
F âˆ’ 2
ÏâŸ¨â„¦, Ï(Ë†Î£n âˆ’ Î›k)
+
Î˜k+1âŸ©
	
+ âŸ¨âˆ†j, D â—¦ â„¦âŸ© + 1
2Ïâˆ¥Î“j âˆ’ D â—¦ â„¦âˆ¥2
F
= 1
2 Tr

â„¦t Ë†Î£t
n Ë†Î£nâ„¦ + 1
Ïâ„¦tâ„¦ âˆ’ 2
Ïâ„¦t(Ï(Ë†Î£n âˆ’ Î›k) + Î˜k+1)

+ Tr((âˆ†j)tD â—¦ â„¦) + 1
2Ï Tr((Î“j âˆ’ D â—¦ â„¦)t(Î“j âˆ’ D â—¦ â„¦)).
Then, the partial differential of Lk
Ï with respect to â„¦,
âˆ‚Lk
Ï(â„¦,Î“j,âˆ†j)
âˆ‚â„¦
is
= 1
2

2Ë†Î£t
n Ë†Î£nâ„¦ + 2
Ïâ„¦ âˆ’ 2
Ï(Ï(Ë†Î£n âˆ’ Î›k) + Î˜k+1)

+âˆ†j â—¦ D + 1
2Ï

âˆ’2Î“j â—¦ D + 2D â—¦ D â—¦ â„¦
	
=

Ë†Î£t
n Ë†Î£n + 1
ÏI

â„¦ + 1
ÏD â—¦ D â—¦ â„¦
4
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

+

âˆ†j âˆ’ 1
ÏÎ“j

â—¦ D âˆ’ 1
Ï

Ï(Ë†Î£n âˆ’ Î›k) + Î˜k+1
.
Since â„¦j+1
k
is the minimizer of Lk
Ï(., Î“j, âˆ†j), we must have
âˆ‚Lk
Ï(â„¦j+1
k
, Î“j, âˆ†j)
âˆ‚â„¦
=
0,
which is equivalent to

Ë†Î£t
n Ë†Î£n + 1
ÏI

â„¦j+1
k
+ 1
ÏD â—¦ D â—¦ â„¦j+1
k
+

âˆ†j âˆ’ 1
ÏÎ“j

â—¦ D âˆ’ 1
Ï

Ï(Ë†Î£n âˆ’ Î›k) + Î˜k+1
= 0.
Finally, â„¦j+1
k
has a closed form given by the previous expres-
sion despite the additional but straightforward computational
effort at this level. This additional step, can be considered as a
warming start for the original ADMM algorithm. This is very
important when dealing with complex problem and large data.
D. The intermediate Î“-Step.
To
deal
with
this
Step,
deï¬ne
an
entry-wise
soft-
thresholding rule for all the off-diagonal elements of a matrix
A as S(A, Îº) = {s(ajl, Îº)}1â‰¤j,lâ‰¤p with
s(ajl, Îº) = sign(ajl) max(|ajl| âˆ’ Îº, 0)I{jÌ¸=l}.
Then the Î“-Step has a closed form given by
Î“j+1
=
arg min
Î“ Lk
Ï(â„¦j+1
k
, Î“, âˆ†j)
=
arg min
Î“
1
2

âˆ¥Ë†Î£nâ„¦j+1
k
âˆ¥2
F + 1
Ïâˆ¥â„¦j+1
k
âˆ¥2
F
âˆ’
2
ÏâŸ¨â„¦j+1
k
, Ï(Ë†Î£n âˆ’ Î›k) + Î˜k+1âŸ©

+
Î»|Î“|1 âˆ’ âŸ¨âˆ†j, Î“ âˆ’ D â—¦ â„¦j+1
k
âŸ© + 1
2Ïâˆ¥Î“ âˆ’ D â—¦ â„¦j+1
k
âˆ¥2
F
=
arg min
Î“
1
2âˆ¥Î“ âˆ’ D â—¦ â„¦j+1
k
âˆ¥2
F âˆ’ ÏâŸ¨âˆ†j, Î“âŸ© + ÏÎ»|Î“|1
=
arg min
Î“
1
2
n
âˆ¥Î“âˆ¥2
F âˆ’ 2âŸ¨Ïâˆ†j + D â—¦ â„¦j+1
k
, Î“âŸ©
o
+
ÏÎ»|Î“|1
=
arg min
Î“
1
2âˆ¥Î“ âˆ’ (Ïâˆ†j + D â—¦ â„¦j+1
k
)âˆ¥2
F + ÏÎ»|Î“|1
=
S(Ïâˆ†j + D â—¦ â„¦j+1
k
, ÏÎ»).
Algorithm 1 shows complete details of HADAP using ADMM
(see Figure 1).
V.
EXPERIMENTS
Among existing methods, the lasso penalized Gaussian
likelihood estimator is the only popular matrix precision es-
timator that can simultaneously retain sparsity and positive
deï¬niteness. To show the goodness of our approach, we use
simulations and real example to compare the performance of
our estimator with Glasso.
Algorithm 1: HADAP algorithm
initialize the variables: Î˜0 = 0, â„¦0 = 0, Î›0 = 0,
Î“0 = 0, âˆ†0 = 0 ;
Select a scalar Ï > 0;
for k = 0, 1, 2, ... until convergence do
Î˜k+1 := (â„¦k + ÏÎ›k)+;
for j = 0, 1, 2, ... until convergence do
Aj+1
Î£
â†

ÏË†Î£t
n Ë†Î£n + DtD + I
âˆ’1
;
Bj+1
Î£
â†

(1 âˆ’ Ï)Dtâˆ†j + Ï(Ë†Î£n âˆ’ Î›k) + Î˜k+1
;
â„¦j+1
k
:= AÎ£ Ã— BÎ£ ;
Î“j+1 := S(Ïâˆ†j + Dâ„¦j+1
k
, ÏÎ»);
âˆ†j+1 := âˆ†j âˆ’ 1
Ï(â„¦j+1
k
âˆ’ Î“j+1);
end
â„¦k+1 := limjâ†’+âˆ â„¦j
k;
Î›k+1 := Î›k âˆ’ 1
Ï(Î˜k+1 âˆ’ â„¦k+1);
end
Figure 1. Complete details of HADAP using the Alternating Direction
Method of Multipliers.
A. Validation on synthetic data
In order to validate our approach, we used the same
simulation structure as in [13]. We generated n = 1000
samples from a p = 600-dimensional normal distribution with
correlation structure of the form Ïƒ(xi, xj) = 0.6|i âˆ’ j|. This
model has a banded structure, and the values of the entries
decay exponentially as they move away from the diagonal.
We generated an independent sample of size 1000 from the
same distribution for validating the tuning parameter Î». Using
the training data, we compute a series of estimators with
50 different values of Î» and use the one with the smallest
likelihood loss on the validation sample, where the likelihood
loss [19], is deï¬ned by
L(Î£, â„¦) = âŸ¨Î£, â„¦âŸ© âˆ’ log det(â„¦)
(16)
We mention that all the experiments are conducted on a PC
with 4 Gb RAM, 3Ghz CPU using Matlab 2009a.
B. Measurable quantities and results.
Output displays the primal residual |rk|, the primal feasi-
bility tolerance epsilon Ïµpri, the dual residual sk, and the dual
feasibility tolerance Ïµdual quantities. Also included is a plot of
the objective values by iterations. Note that the objective value
at any particular iteration can go below the true solution value
pâˆ— because the iterates does not need to be feasible (e.g., if
the constraint is xâˆ’z = 0, we can have xk âˆ’zk Ì¸= 0 for some
k).
Results for Î» = 0.01 are summarized in Figure 2 and Table
II. The convergence is achieved in 25 steps and need just 0.54
seconds. After a few steps of ï¬‚uctuations (â‰ˆ 12 iterations),
the objective function stabilizes and converges to its optimal
value where the eigenvalues of the precision matrix estimated
by the HADAP are real and positive, which prove the positive
deï¬niteness of the obtained precision matrix as shown in
Figure 3.
5
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

Figure 2. Plot of the objective function for Î» = 0.01
TABLE II. VALUES OF THE PRIMAL RESIDUAL |rk|, THE PRIMAL
FEASIBILITY TOLERANCE Ïµpri, THE DUAL RESIDUAL sk, AND
THE DUAL FEASIBILITY TOLERANCE Ïµdual for Î» = 0.01. ALSO
INCLUDED THE OBJECTIVE VALUES BY ITERATION.
iter
r.norm
eps.pri
s.norm
eps.dual
objective
1
2.76
0.04
2.86
0.04
8.49
2
1.08
0.07
1.86
0.03
8.11
3
0.88
0.08
1.99
0.02
8.89
4
1.34
0.09
3.37
0.03
7.95
5
1.88
0.10
3.77
0.03
8.64
6
1.42
0.11
2.52
0.02
8.94
7
0.94
0.11
1.28
0.02
7.79
8
0.62
0.11
0.81
0.01
9.70
9
0.41
0.11
0.58
0.01
7.70
10
0.28
0.11
0.42
0.01
6.70
11
0.16
0.11
0.25
0.01
6.71
12
0.09
0.11
0.16
0.01
6.73
13
0.05
0.11
0.11
0.01
6.75
14
0.03
0.11
0.08
0.01
6.77
15
0.02
0.11
0.07
0.01
6.79
16
0.01
0.12
0.06
0.01
6.80
17
0.01
0.12
0.06
0.01
6.82
18
0.01
0.12
0.04
0.01
6.83
19
0.01
0.12
0.04
0.01
6.84
20
0.01
0.12
0.03
0.01
6.85
21
0.01
0.12
0.03
0.01
6.86
22
0.01
0.12
0.02
0.01
6.86
23
0.01
0.12
0.02
0.01
6.87
24
0.01
0.12
0.02
0.01
6.88
25
0.01
0.12
0.02
0.01
6.88
C. Validation on real data
For experimental validation, we used 4 cancer datasets
publicly available at the Gene Expression Omnibus [20]. For
a fair comparison with the other method of estimating the
inverse covariance matrix, we follow the same analysis scheme
used by [19]. Datasets are: Liver cancer (GSE1898), Colon
cancer (GSE29638), Breast cancer (GSE20194) and Prostate
cancer (GSE17951) with sample size n = 182; 50; 278 and 154
respectively and number of genes p = 21794; 22011; 22283
and 54675. We preprocessed the data so that each variable is
zero mean and unit variance across the dataset. We performed
100 repetitions on a 50%âˆ’50% validation and testing samples.
Since regular sparseness promoting methods do not scale to
large number of variables, we used the same regime proposed
by [19] and validated our method in two regimes. In the ï¬rst
Figure 3. Plot of the objective function
regime, for each of the 50 repetitions, we selected n = 200
variables uniformly at random and use the glasso. In the
second regime, we use all the variables in the dataset, and
use the method dpglasso from [21]. Since the whole sample
covariance matrix could not ï¬t in memory, we computed it
in batches of rows [21]. In order to make a fair comparison,
the runtime includes the time needed to produce the optimal
precision matrix from a given input dataset. Average runtimes
was summarized in table III. This includes not only the time to
solve each optimization problem but also the time to compute
the covariance matrix (if needed). Our HADAP method is
considerably faster than the Glasso method as shown in table
III .
TABLE III. RUNTIMES FOR GENE EXPRESSION DATASETS. OUR
HADAP METHOD IS CONSIDERABLY FASTER THAN SPARSE
METHOD.
Dataset
Graphical lasso
Our estimator
GSE1898
3.8 min
1.0 min
GSE29638
3.8 min
2.6 min
GSE20194
3.8 min
2.5 min
GSE17951
14.9min
4.8 min
VI.
CONCLUSION AND FUTURE WORK
The sparse precision matrix estimator has been shown
to be useful in many applications. Penalizing the matrix is
a tool with good asymptotic properties for estimating large
sparse covariance and precision matrices. However, its positive
deï¬niteness property and unconstrained structure can be easily
violated in practice, which prevents its use in many important
applications such as graphical models, ï¬nancial assets and
comparative genomic hybridization. In this paper, we have
expressed the precision matrix estimation equation in a convex
optimization framework and considered a natural modiï¬cation
by imposing the positive deï¬niteness and problem-solving
constraints. We have developed a fast alternating direction
method to solve the constrained optimization problem and the
resulting estimator retains the sparsity and positive deï¬niteness
properties simultaneously. We are at the phase of demonstrat-
ing the general validity of the method and its advantages over
6
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

correlation networks based on competitive precision matrix
estimators with computer-simulated reaction systems, to be
able to demonstrate strong signatures of intracellular pathways
and provide a valuable tool for the unbiased reconstruction of
metabolic reactions from large-scale metabolomics data sets.
REFERENCES
[1]
J. Duchi, S. Gould, and D. Koller, â€œProjected Subgradient Methods
for Learning Sparse Gaussians,â€ in Proceedings of the Twenty-fourth
Conference on Uncertainty in AI (UAI), 2008.
[2]
K. Scheinberg, S. Ma, and D. Goldfarb, â€œSparse inverse covariance
selection via alternating linearization methods,â€ in Advances in Neural
Information Processing, NIPS10, 2010.
[3]
L. Li and K. Toh, â€œAn inexact interior point method for l1-reguarlized
sparse covariance selection,â€ Mathematical Programming Computation,
vol. 2, 2010, pp. 291â€“315.
[4]
J. Friedman, T. Hastie, and R. Tibshirani, â€œSparse Inverse Covariance
Estimation With the Graphical Lasso,â€ Biostatistics, vol. 9, 2008, pp.
432â€“441.
[5]
O. Banerjee, L. El Ghaoui, and A. dâ€™Aspremont, â€œModel selection
through sparse maximum likelihood estimation for multivariate Gaus-
sian or binary data,â€ Journal of Machine Learning Research, vol. 9,
2008, pp. 485â€“516.
[6]
A. Agarwal, S. Negahban, and M. Wainwright, â€œConvergence rates of
gradient methods for high-dimensional statistical recovery,â€ in Advances
in Neural Information Processing, NIPS10, 2010.
[7]
P. Bickel and E. Levina, â€œRegularized estimation of large covariance
matrices,â€ Ann. Statist., vol. 36, 2008a, pp. 199â€“227.
[8]
R. Yang and J. Berger, â€œEstimation of a covariance matrix using the
reference prior,â€ Ann. Statist., vol. 3, 1994, pp. 1195â€“1211.
[9]
X. Lingzhou, S. Ma, and H. Zhou, â€œPositive Deï¬nite L1 Penalized
Estimation of Large Covariance Matrices,â€ Journal of the American
Statistical Association, vol. 500, 2012, pp. 1480â€“1491.
[10]
H. Xu, C. Caramanis, and S. Mannor, â€œRobust Regression and Lasso,â€
IEEE Transaction on Information Theory, vol. 56, 2010, pp. 3561â€“357.
[11]
R. Tibshirani and J. Taylor, â€œThe solution path of the generalized lasso,â€
Ann. Statist, vol. 39 (3), 2011, pp. 1335â€“1371.
[12]
N. El Karoui, â€œOperator norm consistent estimation of large dimensional
sparse covariance matrices,â€ Annals of Statistics, vol. 36, 2008, pp.
2717â€“2756.
[13]
T. Cai and H. Zhou, â€œA constrained l1 minimization approach to sparse
precision matrix estimation,â€ J. American Statistical Association, vol.
106, 2011, pp. 594â€“607.
[14]
J. Krumsiek, K. Suhre, T. Illig, J. Adamski, and F. Theis, â€œGaus-
sian graphical modeling reconstructs pathway reactions from high-
throughput metabolomics data,â€ BMC Systems Biology, vol. 5 (21),
2011, pp. 2â€“16.
[15]
S. Lauritzen, Graphical Models.
Clarendon Press, Oxford, 1996.
[16]
I. Johnstone, â€œOn the distribution of the largest eigenvalue in principal
components analysis,â€ Ann. Statist., vol. 29 (2), 2001, pp. 295â€“327.
[17]
P. Ravikumar, W. M.J., G. Raskutti, and B. Yu, â€œHigh-dimensional
covariance estimation by minimizing l1-penalized log-determinant di-
vergence,â€ Electron. J. Statist., vol. 5, 2011, pp. 935â€“980.
[18]
E. Candes and T. Tao, â€œThe Dantzig selector: statistical estimation when
p is much larger than n,â€ Annals of Statistics, vol. 35, 2007, pp. 2313â€“
2351.
[19]
J. Honorio and T. Jaakkola, â€œInverse covariance estimation for high-
dimensional data in linear time and space: Spectral methods for riccati
and sparse models,â€ in Proceedings of the 29th Conference on Uncer-
tainty in Artiï¬cial Intelligence, 2013.
[20]
R. Edgar, M. Domrachev, and A. Lash, â€œGene Expression Omnibus:
NCBI gene expression and hybridization array data repository,â€ Nucleic
Acids Res, vol. 30 (1), 2002, pp. 207â€“210.
[21]
R. Mazumder and T. Hastie, â€œExact covariance thresholding into
connected components for largescale graphical lasso,â€ The Journal of
Machine Learning Research, vol. 13, 2012, pp. 781â€“794.
7
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

