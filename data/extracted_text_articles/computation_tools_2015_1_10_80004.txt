Advanced Computation of a Sparse Precision Matrix
HADAP: A Hadamard-Dantzig Estimation of a Sparse Precision Matrix
Mohammed Elanbari∗, Reda Rawi†, Michele Ceccarelli†, Othmane Bouhali‡, Halima Bensmail†
∗Sidra Medical and Research Center, Qatar Foundation
†Qatar Computing Research Institute, Qatar Foundation
‡Texas A&M University-Qatar,
Doha, Qatar
Email: ∗melanbari@sidra.org; †{rrawi, mceccarelli, hbensmail}@qf.org.qa; ‡othmane.bouhali@qatar.tamu.edu
Abstract—Estimating large sparse precision matrices is an in-
teresting and challenging problem in many ﬁelds of sciences,
engineering, and humanities, thanks to advances in computing
technologies. Recent applications often encounter high dimension-
ality with a limited number of data points leading to a number
of covariance parameters that greatly exceeds the number of
observations. Several methods have been proposed to deal with
this problem, but there is no guarantee that the obtained
estimator is positive deﬁnite. Furthermore, in many cases, one
needs to capture some additional information on the setting of
the problem. In this work, we propose an innovative approach
named HADAP for estimating the precision matrix by minimizing
a criterion combining a relaxation of the gradient-log likelihood
and a penalization of lasso type. We derive an efﬁcient Alternating
Direction Method of multipliers algorithm to obtain the optimal
solution.
Keywords–Covariance
matrix;
Frobenius
norm;
Gaussian
graphical model; Precision matrix; Alternating method of multi-
pliers; Positive-deﬁnite estimation; Sparsity..
I.
INTRODUCTION
Recent applications often encounter high dimensionality
with a limited number of data points leading to a number
of covariance parameters that greatly exceeds the number of
observations. Examples include marketing, e-commerce, and
warehouse data in business; microarray, and proteomics data
in genomics and heath sciences; and biomedical imaging,
functional magnetic resonance imaging, tomography, signal
processing, high-resolution imaging, and functional and longi-
tudinal data. In biological sciences, one may want to classify
diseases and predict clinical outcomes using microarray gene
expression or proteomics data, in which hundreds of thousands
of expression levels are potential covariates, but there are typi-
cally only tens or hundreds of subjects. Hundreds of thousands
of single-nucleotide polymorphisms are potential predictors
in genome-wide association studies. The dimensionality of
the variables spaces grows rapidly when interactions of such
predictors are considered. Large-scale data analysis is also
a common feature of many problems in machine learning,
such as text and document classiﬁcation and computer vision.
For a p × p covariance matrix Σ, there are p(p + 1)/2
parameters to estimate, yet the sample size n is often small.
In addition, the positive-deﬁniteness of Σ makes the problem
even more complicated. When n > p, the sample covariance
matrix is positive-deﬁnite and unbiased, but as the dimension
p increases, the sample covariance matrix tends to become
unstable and can fail to be consistent.
II.
BACKGROUND
In this paper, we use the notation in Table I.
TABLE I. NOTATION USED IN THIS PAPER
Notation
Description
A ⪰ 0
A ∈ Rn×p is symmetric and positive
semideﬁnite
A ≻ 0
A ∈ Rn×p is symmetric and positive
deﬁnite
∥A∥1
ℓ1 norm of A ∈ Rn×p, i.e P
ij aij
∥A∥∞
ℓ∞ norm of A ∈ Rn×p, i.e maxijaij
∥A∥2
spectral norm of A ∈ Ri×j i.e. the max-
mum eigenvalues of A ≻ 0
∥A∥F
Frobenius norm of A ∈ Rn×p i.e
qP
ij a2
ij
Tr(A)
trace of A ∈ Rp×p, i.e Tr(A) = P
i aii
vec(A)
stacked form of A ∈ Rn×p, i.e
vec(A) = (a1,1, . . . , an,1, a1,2, . . . , a1,p, . . . , an,p)t,
vec(ABC)
(Ct ⊗ A) vec(B)
vec(A ◦ B)
diag(vec(A))vec(B)
diag(A)
diag(u) =







u1
0
· · ·
0
0
u2
· · ·
0
...
...
...
...
0
0
· · ·
uk







∈ RN×N,
A ◦ B
Hadamard product of A and B, ∈ RN×M, i.e
element-wise multiplication (A ◦ B)ij = (A)ij × (B)ij
A ⊗ B
Kronecker product of A and B
A ⊗ B =







a1,1B
a1,2B
· · ·
a1,mB
a2,1B
a2,2B
· · ·
a2,mB
...
...
...
...
an,1B
an,2B
· · ·
an,mB







A ⊗ B ∈ Rnp×mq
a) Existing Methods: Due in part to its importance, there
has been an active line of work on efﬁcient optimization
methods for solving the ℓ1 regularized Gaussian MLE problem:
PSM that uses projected subgradients [1], ALM using alter-
nating linearization [2], IPM an inexact interior point method
[11], SINCO a greedy coordinate descent method [3] and
Glass a block coordinate descent method [4][5] etc. For typical
high-dimensional statistical problems, optimization methods
typically suffer sub-linear rates of convergence [6]. This would
be too expensive for the Gaussian MLE problem, since the
number of matrix entries scales quadratically with the number
of nodes.
b) Sparse Modeling: Sparse modeling has been widely
used to deal with high dimensionality. The main assumption
is that the p-dimensional parameter vector is sparse, with
1
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

many components being exactly zero or negligibly small.
Such an assumption is crucial in identiﬁability, especially
for the relatively small sample size. Although the notion of
sparsity gives rise to biased estimation in general, it has proved
to be effective in many applications. In particular, variable
selection can increase the estimation accuracy by effectively
identifying important predictors and can improve the model in-
terpretability. To solve this, constraints are frequently imposed
on the covariance to reduce the number of parameters in the
model including the spectral decomposition, Bayesian meth-
ods, modeling the matrix-logarithm, nonparametric smoothing,
banding/thresholding techniques (see [5], [7][8]).
Speciﬁcally,
thresholding
is
proposed
for
estimating
permutation-invariant consistent covariance matrices when the
true covariance matrix is bandable [4]. In this sense, threshold-
ing is more robust than banding/tapering for real applications.
In this paper we focus on the soft-thresholding technique as
in [4] and [9] because it can be formulated as the solution
of a convex optimization problem. In fact, Graphical Lasso
approach (Glasso) is introduced as the following. Let ∥.∥F be
the Frobenius norm and |.|1 be the element-wise ℓ1-norm of all
non-diagonal elements. Then the soft-thresholding covariance
estimator is equal to
ˆΩ+ = arg min
Ω
1
2∥Ω − ˆΩn∥2
F + λ|Ω|1,
(1)
where Ω = Σ−1, where Ω = ωij1≤i,j≤p is the precision
matrix, ˆΩ is the solution of (1) and λ is a tuning parameter.
This equation emphasizes the fact that the solution Ω may not
be unique [such nonuniqueness can occur if rank(Ω) < p].
It has been shown that the existence of a robust optimization
formulation is related to kernel density estimation [10] where
property of the solution and a proof that Lasso is consistent
was given using robustness directly. Moreover, [11] proved
that there exists a linear subspace that is almost surely unique,
meaning that it will be the same under different boundary sets
corresponding to different solutions of equations of type (1).
However, there is no guarantee that the thresholding estimator
is always positive deﬁnite (see [9], [12] and [13]). Although
the positive deﬁnite property is guaranteed in the asymptotic
setting with high probability, the actual estimator can be an
indeﬁnite matrix, especially in real data analysis.
Structure of the inverse covariance matrix has attracted
special interest. Example, when dealing with asset allocation
in ﬁnance. The ﬁnancial problems are often written in terms
of the inverse covariance matrix, in such case the covariance
structure of return series is often estimated using only the
most recent data, resulting in a small sample size compared
to the number of parameters to be estimated. In biological
applications (graphical models), zero correlations represent
conditional independence between the variables. For exam-
ple to account for network information in the analysis of
metabolites data, the reconstruction of metabolic networks
from a collection of metabolite patterns is a key question in
the computational research ﬁeld. Previous attempts focused on
linear metabolite associations measured by Pearson correlation
coefﬁcients [14]. A major drawback of correlation networks,
however, is their inability to distinguish between direct and
indirect associations. Correlation coefﬁcients are generally
high in large-scale omics data sets, suggesting a plethora of
indirect and systemic associations. Gaussian Graphical models
(GGMs) circumvent indirect association effects by evaluating
conditional dependencies in multivariate Gaussian distributions
or equivalently the inverse covariance matrix (see [15]).
On the other hand, additional structure on the precision ma-
trix coefﬁcients is also often required (Comparative genomic
hybridizaton) where the difference between two successive
coefﬁcients is required to be small or to vary slowly.
Our Contributions: In this paper, we emphasize on intro-
ducing a new criteria that insures the positive-deﬁniteness of
the covariance matrix adding a tuning parameter ϵ > 0 in
the constraints. This additional constraint will guard against
positive semi-deﬁnite. We add structure on the coefﬁcient
of the precision matrix and we derive an efﬁcient ADMM
algorithm form to obtain an optimal solution. We perform
Alternating Direction Method of Multipliers steps, a variant of
the standard Augmented Lagrangian method, that uses partial
updates, but with three innovations that enable ﬁnessing the
caveats detailed above.
In Section III, we link the gaussian graphical model to the
precision matrix estimation, in which we show that recovering
the structure of a graph G is equivalent to the estimation
of the support of the precision matrix and describe different
penalized optimization algorithm that have been used to solve
this problem and their limitations.
We describe, in Section IV, the ADMM and its application
to solve the estimation of the precision matrix under the
Dantzig-selector setting and we show its ability to perform
distributed optimization where we break up the big optimiza-
tion problem into smaller problems that are more manageable.
In fact, as in the recent methods [9][12], we build on the
observation that the Newton direction computation is a Lasso
problem, and perform iterative coordinate descent to solve this
Lasso problem. Then, we use a Dantzig-selector rule to obtain
a step-size that ensures positive-deﬁniteness of the next iterate.
In Section V, we validate our algorithm on artiﬁcial and real
data.
III.
LINK WITH GAUSSIAN GRAPHICAL MODEL (GGM)
Given a data set consisting of samples from a zero mean
Gaussian distribution in Rp,
X(i) ∼ N(0, Σ), i = 1, ..., n,
(2)
with positive deﬁnite p×p covariance matrix Σ. Note that the
zero mean assumption in equation 2 is mainly for simplifying
notation. The task here is how to estimate the precision matrix
Ω = Σ−1 when it is sparse. We are particularly interested by
the case of sparse Ω because it is closely linked to the selection
of graphical models.
To be more speciﬁc, let G = (V, E) be a graph representing
conditional independence relations between components of
X = (X1, ..., Xp).
•
The vertex set V has p components X1, ..., Xp;
•
The edge set E consists of ordered pairs (i, j) of V ×
V , where (i, j) ∈ E if there is an edge between Xi
and Xj.
We exclude the edge between two vertexes Xi and Xj if and
only if Xi and Xj are independent given {Xk, k ̸= i, j}.
If in addition X ∼ N(0, Σ), the conditional independence
between Xi and Xj given the remaining variables is equivalent
2
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

to ωij = 0, where Ω = Σ−1 = {ωij}1≤i,j≤p. Hence, in the
case of Gaussian Graphical Model (GGM), the edges are given
by the inverse of covariance matrix. More precisely, recovering
the structure of a graph G is equivalent to the estimation of
the support of the precision matrix Ω. When n > p, one
can estimate Σ−1 by maximum likelihood, but when p > n
this is not possible because the empirical covariance matrix is
singular and often performs poorly [16]. Since the ℓ1-norm
is the tighest convex upper bound of the cardinality of a
matrix, several ℓ1-regularization methods have been proposed.
Consequently a number of authors have considered minimizing
an ℓ1 penalized log-likelihood (see [4] and [17]). It is also
sometimes called Glasso [4] after the algorithm that efﬁciently
computes the solution. It consists of solving the penalized
problem
ˆΩGlasso = arg min
Ω⪰0{⟨ˆΣn, Ω⟩ − log det Ω + λ∥Ω∥1}.
(3)
where ˆΣn is the sample covariance matrix and λ is a tuning
parameter. The term ∥Ω∥1 encourages sparseness of the pre-
cision matrix. The asymptotic properties of the estimator has
been studied in [17]. Lauritzen [13] proposed a constrained l1
minimization proposed a constrained l1 minimization proce-
dure for estimating sparse precision matrices by solving the
optimization problem
minimize
∥Ω∥1
subject to
∥ˆΣnΩ − I∥∞ ≤ λ,
(4)
where λ is a tuning parameter. The authors established the
rates of convergence under both the entry-wise l∞ and the
Frobenius norm. In computationally viewpoint, equation 4 can
be solved by remarking that it can be decomposed into a series
of Dantzig selector problems [18]. This observation is useful
in both implementation and technical analysis. Theoretically,
the authors prove that the estimator is positive deﬁnite with
high probability. However, in practice there is no guarantee
that the estimator is always positive deﬁnite, especially in real
data analysis.
IV.
ALTERNATING DIRECTION METHOD OF MULTIPLIERS
FOR HADAP
We deﬁne our algorithm HADAP as a solution to the
following problem
ˆΩ+ = arg min
Ω⪰ϵI
1
2∥ˆΣnΩ − I∥2
F + λ|D ◦ Ω|1,
(5)
where
ˆΣn
is
the
empirical
covariance
matrix,
D
=
(dij)1≤i,j≤p is an arbitrary matrix with non-negative elements
where D can take different forms: it can be the matrix of all
ones or it can be a matrix with zeros on the diagonal to avoid
shrinking diagonal elements of Ω. Furthermore, we can take
D with elements dij = 1i̸=j|ˆΩinit
ij |, where ˆΩinit is an initial
estimator of Ω. The later choice of D corresponds to precision
matrix analogue of the Adaptive Lasso penalty.
We propose an ADMM algorithm to solve the problem (5).
To derive ADMM, we will ﬁrst introduce a new variable Θ
and an equality constraint as follows:
(ˆΘ+, ˆΩ+)
=
arg min
Θ,Ω
1
2∥ˆΣnΩ − I∥2
F + λ|D ◦ Ω|1
:
Ω = Θ, Θ ⪰ ϵI} .
(6)
The solution to (6) gives the solution to (5). To deal with the
problem (6), we have to minimize its augmented Lagrangian
function for some given penalty parameter ρ, i.e.
Lρ(Θ, Ω, Λ)
=
1
2∥ˆΣnΩ − I∥2
F + λ|D ◦ Ω|1 − ⟨Λ, Θ − Ω⟩
+
1
2ρ∥Θ − Ω∥2
F ,
(7)
where Λ is the Lagrange multiplier.
At iteration k, the ADMM algorithm consists of the three
steps, namely Θ-Step, Ω-Step and the dual-update step :
Θk+1 := arg min
Θ⪰ϵI Lρ(Θ, Ωk, Λk);
Θ-minimization
(8)
Ωk+1 := arg min
Ω Lρ(Θk+1, Ω, Λk);
Ω-minimization
(9)
Λk+1 := Λk − 1
ρ(Θk+1 − Ωk+1).
dual-update
(10)
•
In the ﬁrst step of the ADMM algorithm, we ﬁx Ω
and Λ and minimize the augmented Lagrangian over
Θ.
•
In the second step, we ﬁx Θ and Λ and minimize the
augmented Lagrangian over Ω.
•
Finally, we update the dual variable Λ.
To further simplify the ADMM algorithm, we will derive the
closed-form solutions for (8)-(10).
A. The Θ-Step.
Let A+ be the projection of a matrix A onto the
convex cone {Θ ⪰ ϵI}. Assume that A has the eigen-
decomposition Pp
j=1 λjvjvt
j. Then, it is well known that
A+ = Pp
j=1 max(ϵ, λj)vjvt
j. Using this property, the Θ-Step
can be analytically solved as follows
Θk+1
=
arg min
Θ⪰ϵI Lρ(Θ, Ωk, Λk)
=
arg min
Θ⪰ϵI
1
2∥ˆΣnΩk − I∥2
F + λ|D ◦ Ωk|1
−
⟨Λk, Θ − Ωk⟩ + 1
2ρ∥Θ − Ωk∥2
F
=
arg min
Θ⪰ϵI −⟨Λk, Θ⟩ + 1
2ρ∥Θ − Ωk∥2
F
=
arg min
Θ⪰ϵI ∥Θ − (Ωk + ρΛk)∥2
F
=
(Ωk + ρΛk)+
=
p
X
j=1
max(ϵ, λj)vjvt
j,
where Pp
j=1 λjvjvt
j is eigen-decomposition of Ωk + ρΛk.
3
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

B. The Ω-Step.
Ωk+1
=
arg min
Ω Lρ(Θk+1, Ω, Λk)
=
arg min
Ω
1
2∥ˆΣnΩ − I∥2
F + λ|D ◦ Ω|1
−
⟨Λk, Θk+1 − Ω⟩ + 1
2ρ∥Θk+1 − Ω∥2
F
=
arg min
Ω
1
2∥ˆΣnΩ − I∥2
F + λ|D ◦ Ω|1
+
⟨Λk, Ω⟩ + 1
2ρ∥Θk+1 − Ω∥2
F
=
arg min
Ω
1
2∥ˆΣnΩ − I∥2
F + 1
2ρ∥Ω − Θk+1∥2
F
+
⟨Λk, Ω⟩ + λ|D ◦ Ω|1.
We have
1
2∥ˆΣnΩ − I∥2
F
=
1
2⟨ˆΣnΩ − I, ˆΣnΩ − I⟩
=
1
2
n
∥ˆΣnΩ∥2
F − 2⟨ˆΣnΩ, I⟩ + ∥I∥2
F
o
=
1
2
n
∥ˆΣnΩ∥2
F − 2 Tr(Ωt ˆΣt
nI) + Tr(ItI)
o
=
1
2
n
∥ˆΣnΩ∥2
F − 2 Tr(Ωt ˆΣn) + p
o
=
1
2
n
∥ˆΣnΩ∥2
F − 2⟨Ω, ˆΣn⟩ + p
o
.
and
1
2ρ∥Ω − Θk+1∥2
F
=
1
2ρ⟨Ω − Θk+1, Ω − Θk+1⟩
=
1
2ρ

∥Ω∥2
F − 2⟨Ω, Θk+1⟩ + ∥Θk+1∥2
F
	
.
Then, the Ω-Step is equivalent to
Ωk+1
=
arg min
Ω
1
2∥ˆΣnΩ − I∥2
F + 1
2ρ∥Ω − Θk+1∥2
F
+
⟨Λk, Ω⟩ + λ|D ◦ Ω|1
=
arg min
Ω
1
2
n
∥ˆΣnΩ∥2
F − 2⟨Ω, ˆΣn⟩ + p
o
+
1
2ρ

∥Ω∥2
F − 2⟨Ω, Θk+1⟩ + ∥Θk+1∥2
F
	
+ ⟨Λk, Ω⟩
+
λ|D ◦ Ω|1
=
arg min
Ω
1
2
n
∥ˆΣnΩ∥2
F − 2⟨Ω, ˆΣn⟩
o
+
1
2ρ

∥Ω∥2
F − 2⟨Ω, Θk+1⟩
	
+
⟨Λk, Ω⟩ + λ|D ◦ Ω|1
=
arg min
Ω
1
2

∥ˆΣnΩ∥2
F + 1
ρ∥Ω∥2
F
−
2⟨Ω, ˆΣn + 1
ρΘk+1 − Λk⟩

+ λ|D ◦ Ω|1
=
arg min
Ω
1
2

∥ˆΣnΩ∥2
F + 1
ρ∥Ω∥2
F
−
2
ρ⟨Ω, ρ(ˆΣn − Λk) + Θk+1⟩

+ λ|D ◦ Ω|1.
At this level, we are not able to derive a closed form for Ω. To
overcome this problem, we propose to derive a new ADMM to
update Ω. To do this, we reparametrize the D ◦ Ω with Γ and
we add an equality constraint D ◦ Ω = Γ, then we minimize
1
2

∥ˆΣnΩ∥2
F + 1
ρ∥Ω∥2
F − 2
ρ⟨Ω, ρ(ˆΣn − Λk) + Θk+1⟩

+ λ|Γ|1
subject to
D ◦ Ω = Γ.
(11)
The
augmented
Lagrangian
associated
to
this
problem
Lk
ρ(Ω, Γ, ∆) is
1
2

∥ˆΣnΩ∥2
F + 1
ρ∥Ω∥2
F − 2
ρ⟨Ω, ρ(ˆΣn − Λk) + Θk+1⟩

+
λ|Γ|1 − ⟨∆, Γ − D ◦ Ω⟩ + 1
2ρ∥Γ − D ◦ Ω∥2
F ,
(12)
where ∆ is the Lagrange multiplier and ρ is the same param-
eter as in (7).
As before, the ADMM for this problem consists of the follow-
ing three intermediate steps:
Ωj+1
k
:= arg min
Ω Lk
ρ(Ω, Γj, ∆j);
Ω-minimization
(13)
Γj+1 := arg min
Γ Lk
ρ(Ωj+1
k
, Γ, ∆j);
Γ-minimization
(14)
∆j+1 := ∆j − 1
ρ(Ωj+1
k
− Γj+1).
dual-update
(15)
C. The intermediate Ω-Step.
Ωj+1
k
=
arg min
Ω Lk
ρ(Ω, Γj, ∆j)
=
arg min
Ω
1
2

∥ˆΣnΩ∥2
F + 1
ρ∥Ω∥2
F
−
2
ρ⟨Ω, ρ(ˆΣn − Λk) + Θk+1⟩

+ λ|Γj|1
−
⟨∆j, Γj − D ◦ Ω⟩ + 1
2ρ∥Γj − D ◦ Ω∥2
F
=
1
2

∥ˆΣnΩ∥2
F + 1
ρ∥Ω∥2
F − 2
ρ⟨Ω, ρ(ˆΣn − Λk)
+
Θk+1⟩
	
+ ⟨∆j, D ◦ Ω⟩ + 1
2ρ∥Γj − D ◦ Ω∥2
F
= 1
2 Tr

Ωt ˆΣt
n ˆΣnΩ + 1
ρΩtΩ − 2
ρΩt(ρ(ˆΣn − Λk) + Θk+1)

+ Tr((∆j)tD ◦ Ω) + 1
2ρ Tr((Γj − D ◦ Ω)t(Γj − D ◦ Ω)).
Then, the partial differential of Lk
ρ with respect to Ω,
∂Lk
ρ(Ω,Γj,∆j)
∂Ω
is
= 1
2

2ˆΣt
n ˆΣnΩ + 2
ρΩ − 2
ρ(ρ(ˆΣn − Λk) + Θk+1)

+∆j ◦ D + 1
2ρ

−2Γj ◦ D + 2D ◦ D ◦ Ω
	
=

ˆΣt
n ˆΣn + 1
ρI

Ω + 1
ρD ◦ D ◦ Ω
4
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

+

∆j − 1
ρΓj

◦ D − 1
ρ

ρ(ˆΣn − Λk) + Θk+1
.
Since Ωj+1
k
is the minimizer of Lk
ρ(., Γj, ∆j), we must have
∂Lk
ρ(Ωj+1
k
, Γj, ∆j)
∂Ω
=
0,
which is equivalent to

ˆΣt
n ˆΣn + 1
ρI

Ωj+1
k
+ 1
ρD ◦ D ◦ Ωj+1
k
+

∆j − 1
ρΓj

◦ D − 1
ρ

ρ(ˆΣn − Λk) + Θk+1
= 0.
Finally, Ωj+1
k
has a closed form given by the previous expres-
sion despite the additional but straightforward computational
effort at this level. This additional step, can be considered as a
warming start for the original ADMM algorithm. This is very
important when dealing with complex problem and large data.
D. The intermediate Γ-Step.
To
deal
with
this
Step,
deﬁne
an
entry-wise
soft-
thresholding rule for all the off-diagonal elements of a matrix
A as S(A, κ) = {s(ajl, κ)}1≤j,l≤p with
s(ajl, κ) = sign(ajl) max(|ajl| − κ, 0)I{j̸=l}.
Then the Γ-Step has a closed form given by
Γj+1
=
arg min
Γ Lk
ρ(Ωj+1
k
, Γ, ∆j)
=
arg min
Γ
1
2

∥ˆΣnΩj+1
k
∥2
F + 1
ρ∥Ωj+1
k
∥2
F
−
2
ρ⟨Ωj+1
k
, ρ(ˆΣn − Λk) + Θk+1⟩

+
λ|Γ|1 − ⟨∆j, Γ − D ◦ Ωj+1
k
⟩ + 1
2ρ∥Γ − D ◦ Ωj+1
k
∥2
F
=
arg min
Γ
1
2∥Γ − D ◦ Ωj+1
k
∥2
F − ρ⟨∆j, Γ⟩ + ρλ|Γ|1
=
arg min
Γ
1
2
n
∥Γ∥2
F − 2⟨ρ∆j + D ◦ Ωj+1
k
, Γ⟩
o
+
ρλ|Γ|1
=
arg min
Γ
1
2∥Γ − (ρ∆j + D ◦ Ωj+1
k
)∥2
F + ρλ|Γ|1
=
S(ρ∆j + D ◦ Ωj+1
k
, ρλ).
Algorithm 1 shows complete details of HADAP using ADMM
(see Figure 1).
V.
EXPERIMENTS
Among existing methods, the lasso penalized Gaussian
likelihood estimator is the only popular matrix precision es-
timator that can simultaneously retain sparsity and positive
deﬁniteness. To show the goodness of our approach, we use
simulations and real example to compare the performance of
our estimator with Glasso.
Algorithm 1: HADAP algorithm
initialize the variables: Θ0 = 0, Ω0 = 0, Λ0 = 0,
Γ0 = 0, ∆0 = 0 ;
Select a scalar ρ > 0;
for k = 0, 1, 2, ... until convergence do
Θk+1 := (Ωk + ρΛk)+;
for j = 0, 1, 2, ... until convergence do
Aj+1
Σ
←

ρˆΣt
n ˆΣn + DtD + I
−1
;
Bj+1
Σ
←

(1 − ρ)Dt∆j + ρ(ˆΣn − Λk) + Θk+1
;
Ωj+1
k
:= AΣ × BΣ ;
Γj+1 := S(ρ∆j + DΩj+1
k
, ρλ);
∆j+1 := ∆j − 1
ρ(Ωj+1
k
− Γj+1);
end
Ωk+1 := limj→+∞ Ωj
k;
Λk+1 := Λk − 1
ρ(Θk+1 − Ωk+1);
end
Figure 1. Complete details of HADAP using the Alternating Direction
Method of Multipliers.
A. Validation on synthetic data
In order to validate our approach, we used the same
simulation structure as in [13]. We generated n = 1000
samples from a p = 600-dimensional normal distribution with
correlation structure of the form σ(xi, xj) = 0.6|i − j|. This
model has a banded structure, and the values of the entries
decay exponentially as they move away from the diagonal.
We generated an independent sample of size 1000 from the
same distribution for validating the tuning parameter λ. Using
the training data, we compute a series of estimators with
50 different values of λ and use the one with the smallest
likelihood loss on the validation sample, where the likelihood
loss [19], is deﬁned by
L(Σ, Ω) = ⟨Σ, Ω⟩ − log det(Ω)
(16)
We mention that all the experiments are conducted on a PC
with 4 Gb RAM, 3Ghz CPU using Matlab 2009a.
B. Measurable quantities and results.
Output displays the primal residual |rk|, the primal feasi-
bility tolerance epsilon ϵpri, the dual residual sk, and the dual
feasibility tolerance ϵdual quantities. Also included is a plot of
the objective values by iterations. Note that the objective value
at any particular iteration can go below the true solution value
p∗ because the iterates does not need to be feasible (e.g., if
the constraint is x−z = 0, we can have xk −zk ̸= 0 for some
k).
Results for λ = 0.01 are summarized in Figure 2 and Table
II. The convergence is achieved in 25 steps and need just 0.54
seconds. After a few steps of ﬂuctuations (≈ 12 iterations),
the objective function stabilizes and converges to its optimal
value where the eigenvalues of the precision matrix estimated
by the HADAP are real and positive, which prove the positive
deﬁniteness of the obtained precision matrix as shown in
Figure 3.
5
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

Figure 2. Plot of the objective function for λ = 0.01
TABLE II. VALUES OF THE PRIMAL RESIDUAL |rk|, THE PRIMAL
FEASIBILITY TOLERANCE ϵpri, THE DUAL RESIDUAL sk, AND
THE DUAL FEASIBILITY TOLERANCE ϵdual for λ = 0.01. ALSO
INCLUDED THE OBJECTIVE VALUES BY ITERATION.
iter
r.norm
eps.pri
s.norm
eps.dual
objective
1
2.76
0.04
2.86
0.04
8.49
2
1.08
0.07
1.86
0.03
8.11
3
0.88
0.08
1.99
0.02
8.89
4
1.34
0.09
3.37
0.03
7.95
5
1.88
0.10
3.77
0.03
8.64
6
1.42
0.11
2.52
0.02
8.94
7
0.94
0.11
1.28
0.02
7.79
8
0.62
0.11
0.81
0.01
9.70
9
0.41
0.11
0.58
0.01
7.70
10
0.28
0.11
0.42
0.01
6.70
11
0.16
0.11
0.25
0.01
6.71
12
0.09
0.11
0.16
0.01
6.73
13
0.05
0.11
0.11
0.01
6.75
14
0.03
0.11
0.08
0.01
6.77
15
0.02
0.11
0.07
0.01
6.79
16
0.01
0.12
0.06
0.01
6.80
17
0.01
0.12
0.06
0.01
6.82
18
0.01
0.12
0.04
0.01
6.83
19
0.01
0.12
0.04
0.01
6.84
20
0.01
0.12
0.03
0.01
6.85
21
0.01
0.12
0.03
0.01
6.86
22
0.01
0.12
0.02
0.01
6.86
23
0.01
0.12
0.02
0.01
6.87
24
0.01
0.12
0.02
0.01
6.88
25
0.01
0.12
0.02
0.01
6.88
C. Validation on real data
For experimental validation, we used 4 cancer datasets
publicly available at the Gene Expression Omnibus [20]. For
a fair comparison with the other method of estimating the
inverse covariance matrix, we follow the same analysis scheme
used by [19]. Datasets are: Liver cancer (GSE1898), Colon
cancer (GSE29638), Breast cancer (GSE20194) and Prostate
cancer (GSE17951) with sample size n = 182; 50; 278 and 154
respectively and number of genes p = 21794; 22011; 22283
and 54675. We preprocessed the data so that each variable is
zero mean and unit variance across the dataset. We performed
100 repetitions on a 50%−50% validation and testing samples.
Since regular sparseness promoting methods do not scale to
large number of variables, we used the same regime proposed
by [19] and validated our method in two regimes. In the ﬁrst
Figure 3. Plot of the objective function
regime, for each of the 50 repetitions, we selected n = 200
variables uniformly at random and use the glasso. In the
second regime, we use all the variables in the dataset, and
use the method dpglasso from [21]. Since the whole sample
covariance matrix could not ﬁt in memory, we computed it
in batches of rows [21]. In order to make a fair comparison,
the runtime includes the time needed to produce the optimal
precision matrix from a given input dataset. Average runtimes
was summarized in table III. This includes not only the time to
solve each optimization problem but also the time to compute
the covariance matrix (if needed). Our HADAP method is
considerably faster than the Glasso method as shown in table
III .
TABLE III. RUNTIMES FOR GENE EXPRESSION DATASETS. OUR
HADAP METHOD IS CONSIDERABLY FASTER THAN SPARSE
METHOD.
Dataset
Graphical lasso
Our estimator
GSE1898
3.8 min
1.0 min
GSE29638
3.8 min
2.6 min
GSE20194
3.8 min
2.5 min
GSE17951
14.9min
4.8 min
VI.
CONCLUSION AND FUTURE WORK
The sparse precision matrix estimator has been shown
to be useful in many applications. Penalizing the matrix is
a tool with good asymptotic properties for estimating large
sparse covariance and precision matrices. However, its positive
deﬁniteness property and unconstrained structure can be easily
violated in practice, which prevents its use in many important
applications such as graphical models, ﬁnancial assets and
comparative genomic hybridization. In this paper, we have
expressed the precision matrix estimation equation in a convex
optimization framework and considered a natural modiﬁcation
by imposing the positive deﬁniteness and problem-solving
constraints. We have developed a fast alternating direction
method to solve the constrained optimization problem and the
resulting estimator retains the sparsity and positive deﬁniteness
properties simultaneously. We are at the phase of demonstrat-
ing the general validity of the method and its advantages over
6
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

correlation networks based on competitive precision matrix
estimators with computer-simulated reaction systems, to be
able to demonstrate strong signatures of intracellular pathways
and provide a valuable tool for the unbiased reconstruction of
metabolic reactions from large-scale metabolomics data sets.
REFERENCES
[1]
J. Duchi, S. Gould, and D. Koller, “Projected Subgradient Methods
for Learning Sparse Gaussians,” in Proceedings of the Twenty-fourth
Conference on Uncertainty in AI (UAI), 2008.
[2]
K. Scheinberg, S. Ma, and D. Goldfarb, “Sparse inverse covariance
selection via alternating linearization methods,” in Advances in Neural
Information Processing, NIPS10, 2010.
[3]
L. Li and K. Toh, “An inexact interior point method for l1-reguarlized
sparse covariance selection,” Mathematical Programming Computation,
vol. 2, 2010, pp. 291–315.
[4]
J. Friedman, T. Hastie, and R. Tibshirani, “Sparse Inverse Covariance
Estimation With the Graphical Lasso,” Biostatistics, vol. 9, 2008, pp.
432–441.
[5]
O. Banerjee, L. El Ghaoui, and A. d’Aspremont, “Model selection
through sparse maximum likelihood estimation for multivariate Gaus-
sian or binary data,” Journal of Machine Learning Research, vol. 9,
2008, pp. 485–516.
[6]
A. Agarwal, S. Negahban, and M. Wainwright, “Convergence rates of
gradient methods for high-dimensional statistical recovery,” in Advances
in Neural Information Processing, NIPS10, 2010.
[7]
P. Bickel and E. Levina, “Regularized estimation of large covariance
matrices,” Ann. Statist., vol. 36, 2008a, pp. 199–227.
[8]
R. Yang and J. Berger, “Estimation of a covariance matrix using the
reference prior,” Ann. Statist., vol. 3, 1994, pp. 1195–1211.
[9]
X. Lingzhou, S. Ma, and H. Zhou, “Positive Deﬁnite L1 Penalized
Estimation of Large Covariance Matrices,” Journal of the American
Statistical Association, vol. 500, 2012, pp. 1480–1491.
[10]
H. Xu, C. Caramanis, and S. Mannor, “Robust Regression and Lasso,”
IEEE Transaction on Information Theory, vol. 56, 2010, pp. 3561–357.
[11]
R. Tibshirani and J. Taylor, “The solution path of the generalized lasso,”
Ann. Statist, vol. 39 (3), 2011, pp. 1335–1371.
[12]
N. El Karoui, “Operator norm consistent estimation of large dimensional
sparse covariance matrices,” Annals of Statistics, vol. 36, 2008, pp.
2717–2756.
[13]
T. Cai and H. Zhou, “A constrained l1 minimization approach to sparse
precision matrix estimation,” J. American Statistical Association, vol.
106, 2011, pp. 594–607.
[14]
J. Krumsiek, K. Suhre, T. Illig, J. Adamski, and F. Theis, “Gaus-
sian graphical modeling reconstructs pathway reactions from high-
throughput metabolomics data,” BMC Systems Biology, vol. 5 (21),
2011, pp. 2–16.
[15]
S. Lauritzen, Graphical Models.
Clarendon Press, Oxford, 1996.
[16]
I. Johnstone, “On the distribution of the largest eigenvalue in principal
components analysis,” Ann. Statist., vol. 29 (2), 2001, pp. 295–327.
[17]
P. Ravikumar, W. M.J., G. Raskutti, and B. Yu, “High-dimensional
covariance estimation by minimizing l1-penalized log-determinant di-
vergence,” Electron. J. Statist., vol. 5, 2011, pp. 935–980.
[18]
E. Candes and T. Tao, “The Dantzig selector: statistical estimation when
p is much larger than n,” Annals of Statistics, vol. 35, 2007, pp. 2313–
2351.
[19]
J. Honorio and T. Jaakkola, “Inverse covariance estimation for high-
dimensional data in linear time and space: Spectral methods for riccati
and sparse models,” in Proceedings of the 29th Conference on Uncer-
tainty in Artiﬁcial Intelligence, 2013.
[20]
R. Edgar, M. Domrachev, and A. Lash, “Gene Expression Omnibus:
NCBI gene expression and hybridization array data repository,” Nucleic
Acids Res, vol. 30 (1), 2002, pp. 207–210.
[21]
R. Mazumder and T. Hastie, “Exact covariance thresholding into
connected components for largescale graphical lasso,” The Journal of
Machine Learning Research, vol. 13, 2012, pp. 781–794.
7
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

