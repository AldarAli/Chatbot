Contribution of Statistics and Value of Data for the Creation of Result Matrices
from Objects of Knowledge Resources
Claus-Peter R¨uckemann
Westf¨alische Wilhelms-Universit¨at M¨unster (WWU),
Leibniz Universit¨at Hannover,
North-German Supercomputing Alliance (HLRN), Germany
Email: ruckema@uni-muenster.de
Abstract—This article presents and summarises the main research
results on computing optimised result matrices from the practical
creation of knowledge resources. With this paper we introduce the
main implemented long-term multi-disciplinary and multi-lingual
knowledge resources’ means, fundamentals and application of
documentation, structure, universal classiﬁcation, and statistics
and components for computational workﬂows and result matrix
generation. The resources and workﬂows can beneﬁt from
High End Computing (HEC) resources. The paper presents
a knowledge processing procedure using long-term knowledge
resources and introduces the n-Probe Parallelised Workﬂow for
an exemplary case study and discussion on a practical application.
The goal of this research is to extend the applied features used
with long-term knowledge resources’ objects and context. The
extensions are concentrating on structure and content as well as
on processing. The focus is the contribution of statistics and the
value of data for the creation of complex result matrices. The
major outcome within the last years is the impact on long-term
resources based on the scientiﬁc results regarding the systematics
and methodologies for caring for knowledge.
Keywords–Knowledge Resources; Processing and Discovery;
n-Probe Parallelised Workﬂow; Universal Decimal Classiﬁcation;
High End Computing.
I.
INTRODUCTION
Within the last decades the value of data has steadily
increased and with this the demand for ﬂexible and efﬁcient
discovery processes for creating results from requests on
data sources. The fundamental research on optimising result
matrices and statistics has been published and presented at the
INFOCOMP conference in June 2014 in Paris [1]. This article
presents the extended research, especially focussing on data
aspects and practical workﬂows.
Comparable to statistical models used for on-line text clas-
siﬁcation [2] even more sophisticated models can be used
with advanced, structured, and classiﬁed knowledge. These
models can be assisted using statistical approaches for data
analysis [3] in complex information systems as well as for
measuring the reliability of classiﬁcations models [4] from
the content side. The demand for long-term sustainability of
the resources increases with the complexity of content and
context. The organisation and structure of the resources are
getting essentially important, the more important the more the
data sizes and complexity as well as their intelligent use are
required [5].
The article therefore introduces and discusses the back-
ground, including the systematics and methodologies required
for an advanced long-term documentation, which can be de-
ployed in most ﬂexible ways – supported by a comprehen-
sive knowledge deﬁnition. The general requirements have to
consider the condition that it is not sufﬁcient to support only
an isolated or special methodology. The knowledge requires
special qualities in order to be usable as well as the quantities
of knowledge counts. A suitable general conceptual handling
and a universal knowledge deﬁnition is required in this en-
vironment for supporting advanced workﬂows in beneﬁt for
higher qualities of resulting context and matrices. One the side
of methodologies and statistics, some major instruments have
been developed and successfully integrated. The combination
of instruments and resources allows to ﬂexibly compute opti-
mised result matrices for discovery processes in information
systems, expert and decision making system components,
search engine algorithms, and last but not least supports the
further development of the long-term knowledge resources.
The presented results are the outcome of the developments
and case studies conducted over the last years.
This paper is organised as follows. Section II discusses
the motivation, Section III introduces the available knowledge
resources regarding processing, workﬂows, value of data,
and their needs for classiﬁcation and computing. Section IV
presents the details of methodologies and components used
as it illustrates the details of the implemented resources’
features and procedures, structure and classiﬁcation, statistics.
Section V illustrates the resulting, implemented workﬂow
algorithms, and an example for a parallelised workﬂow, data-
centric parallelisation results, and weighted results from statis-
tics and value of data contributions. Section VI discusses the
prominent statistics available and tested with the resources
and Section VII shows the implementation results for the
matrices on a sample case. Sections VIII and IX evaluate the
main results and conclude the presented implementation, also
discussing the future work.
II.
MOTIVATION
Knowledge resources are the basic components in complex
integrated systems. Their target is mostly to create a long-
term multi-disciplinary knowledge base for various purposes.
Request and selection processes result in requirements for
computing result matrices from the available information and
data. Optimisation in the context of result matrices means
30
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

“improved for a certain purpose”. Here, the certain purpose is
given by the target and intention of the application scenario,
e.g., requests on search results or associations. Therefore,
improving the result matrices is a very multi-fold process and
“optimising result matrices” primarily refers to the content
and context but in second order also to the workﬂows and
algorithms. The major means presented here contributing to
the optimisation are classiﬁcation and statistics, based on the
knowledge resources. The employed knowledge resources can
provide any knowledge documentation and additional informa-
tion on objects and knowledge references, e.g., from natural
sciences and decision making. Any data used in case studies is
embedded into millions of multi-disciplinary objects, including
dynamical and spatial information and data ﬁles.
It is necessary to develop logical structures in order to
govern the existing unstructured and structured big data today
and in future, especially in volume, variability, and velocity
and to keep the information addressable on long-term. Prepar-
ing and structuring big data is the essential process, which
has to preceed creating and implementing algorithms. The
systematic, methodological, and “clean” big data knowledge
preparation and structuring must generally be named as largest
achievement in this context and can be considered by far
the most signiﬁcant overall contribution [5]. The creation
and optimisation of respective algorithms is of secondary
importance, the more the data must be considered for long-
term knowledge creation as, e.g., the beneﬁts of most of those
implementations depend on a certain generation of computing
and storage architectures, which change all few 4–6 years.
III.
KNOWLEDGE AND RESOURCES
With the creation of result matrices we have to introduce a
common understanding of knowledge and its processing and
the value associated with its application.
A. Knowledge deﬁnition and understanding
The World Social Science Report 2013 [6] deﬁnes knowl-
edge as “The way society and individuals apply meaning to
experience . . .”. Accordingly, the report proposes that “New
media and new forms of public participation and greater access
to information, are crucial” for open knowledge systems.
In general, we can have an understanding, where knowledge
is: Knowledge is created from a subjective combination of
different attainments as there are intuition, experience, infor-
mation, education, decision, power of persuasion and so on,
which are selected, compared and balanced against each other,
which are transformed and interpreted.
The consequences are: Authentic knowledge therefore does
not exist, it always has to be enlived again. Knowledge must
not be confused with information or data, which can be stored.
Knowledge cannot be stored nor can it simply exist, neither in
the Internet, nor in computers, databases, programs or books.
Therefore, the demands for knowledge resources in support of
the knowledge creation process are complex and multi-fold.
There is no universal “deﬁnition” of the term “knowledge”,
but UDC provides a good overview of the possible width,
depth, and facets. For this research the classiﬁcation references
of UDC:0 (Science and knowledge) deﬁne the view on univer-
sal knowledge [7], which reﬂects the conceptual dimension and
is intended to be used with the full bandwidth of knowledge
and knowledge resources.
B. Processing and workﬂows
Workﬂows based on the knowledge resources’ objects and
facilities have been created for different applications. The
knowledge resources can make sustainable and vital use of
Object Carousels [8] in order to create knowledge object
references and modularise the required algorithms [9]. This
provides a universal means for improving coverage, e.g., dark
data, and quality within the workﬂow. Secondary resources be-
ing available for data, information, and knowledge integration,
besides Integrated Information and Computing System (IICS)
applications, allow for workﬂows and intelligent components
on High End Computing (HEC) and High Performance Com-
puting (HPC) resources [10], [11]. This paper presents the up-
to-date experiences with selected components for structures
and workﬂows.
C. Value of data
The value of data is a central driving force for creating
sustainable knowledge resources, the more as data is increas-
ingly important for long period of times. Long-term in cases of
sustainable high-value data means many decades of availability
and usability. Therefore, usability, security, and archiving are
most important aspects of the value of data sets. Value is not
the price a data set can be sold as there are many individual
factors.
The long-term studies, as the “Cost of Data Breach” study at
the Ponemon Institute [12] summarise that the costs related to
data loss are high and as predicted [13] do increase [14] every
year [15], [16] (sponsored by Symantec), [17] (sponsored by
IBM). Straight approaches for calculating individual risks and
data loss, as with the Symantec Data Breach Calculator [18]
illustrate the effects. Besides science and industry, assessing
knowledge loss risks resulting from departing personnel and
other factors of loss [19], [20] can be summarised by the risk
of knowledge loss, the probability for loss of employees, the
consequences of human knowledge loss, and the quality of
knowledge resources.
The high quality and value of the knowledge resources used
for supporting discovery processes are results of the multi-
and trans-disciplinary long-term creation and documentation
processes, the structuring of the data, the context of knowledge
objects, and the availableness of an universal classiﬁcation.
D. Knowledge resources
The knowledge resources implement structure and features
and can be integrated most ﬂexibly into information and
computing system components. Main elements are so called
knowledge objects. The objects can consist of any content
and context documentation and can employ a multitude of
means for description and referencing of objects, data sets,
collections, used with computational workﬂows. Essential core
31
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

attributes are a facetted universal classiﬁcation and various
content views and attributes, created manually and automated
in interactive and batch operation. Developing workﬂow im-
plementations for various purposes requires to compute result
matrices from the knowledge objects and referred knowledge.
The purposes can require individual processing means, com-
plex algorithms, and a base of big data collections. Advanced
discovery workﬂows can easily demand large computational
requirements for High End Computing (HEC) resources sup-
porting an efﬁcient implementation.
IV.
METHODOLOGIES AND COMPONENTS EMPLOYED
The following passages refer to the main components and
methodologies and introduce the main aspects for the creation
of result matrices.
A. Content, context, and procedures
The data used here is based on the content and context
from the knowledge resources, provided by the LX Foundation
Scientiﬁc Resources [21], [22]. The LX structure and the
classiﬁcation references based on UDC [23], [24] are essential
means for the processing workﬂows and evaluation of the
knowledge objects and containers. Both provide strong multi-
disciplinary and multi-lingual support. The analysis of different
classiﬁcations and development of concepts for intermediate
classiﬁcations from the Knowledge in Motion (KiM) long-term
project [25] has contributed to the application of UDC in the
context of knowledge resources.
An instructive example for an archaeological and geoscien-
tiﬁc use case, deploying knowledge resources, classiﬁcation,
references, and Object Carousels has been recently published
[8]. With this research the presentation complements the use
case by an important methodology, statistics for intermediate
result matrices, usable in any associated workﬂow. In order to
get an overview, the following practical example for a speciﬁc
workﬂow as part of an application component shows how
result matrices for requests can be computed iteratively.
1)
Application component request,
2)
Object search (i.e., knowledge objects, classiﬁcation,
references, associations),
3)
Creation of intermediate result matrices,
4)
Iterative and alternating matrix element creation (i.e.,
based on intermediate result matrices, object search,
referenced content, classiﬁcation, and statistics),
5)
Creation of result matrix,
6)
Application component response.
The workﬂow will mostly be linear if the used algorithms are
linear and the data involved is ﬁxed in number and content.
The knowledge objects are under continuous development
for more than twenty-ﬁve years. The classiﬁcation information
has been added in order to describe the objects with the
ongoing research and in order to enable more detailed doc-
umentation in a multi-disciplinary and multi-lingual context.
Classiﬁcation is state-of-the-art with the development of
the knowledge resources, which implicitly means that the
classiﬁcation is not created statically or even ﬁxed. It can
be used and dynamically modiﬁed on the ﬂy, e.g., when
required by a discovery workﬂow description. Representations
and references can be handled dynamically with the context of
a discovery process. So, the classiﬁcation can be dynamically
modelled with the workﬂow context. The applied workﬂows
and processing are based on the data and extended features
developed for the Gottfried Wilhelm Leibniz resources [26].
Mathematical statistics is a central means for data analysis
[27], [28]. It can be of huge beneﬁts when analysing reg-
ularities and patterns when used for machine learning with
information system components [29]. It is a valuable means
deployed in natural sciences and has been integrated in multi-
disciplinary humanities-based disciplines, e.g., in archaeology
[30]. The span of ﬁelds for statistics is not only very broad
but statistics itself goes far beyond a simple “tool” status [31].
Methodological means, which have been created in order to
be deployed for regular use are workﬂows improving result
quantity and result quality, various ﬁlters, universal classiﬁca-
tions, statistics applications, manually documented resources’
components, integration interfaces for knowledge resources,
comparative methods, combination of several means.
The methodologies with the knowledge resources are based
on computational methods, processing, classiﬁcation and struc-
turing of multi-disciplinary knowledge, systematic documenta-
tion, long-term knowledge creation, vitality of data concepts,
sustainable resources architecture, and collaboration frame-
works.
In the past, many algorithms have been developed and
implemented [21], [22] for supporting different targets, e.g.,
silken criteria, statistics, classiﬁcation, references and citation
evaluation, translation, transliteration, and correction support,
regular expression based applications, phonetic analysis sup-
port, acronym expansions, data and application assignments,
request iteration, centralised and distributed discovery, and
automated and manual contributions to the workﬂow.
B. Structure and classiﬁcation
The key issues for computing result matrices from knowl-
edge resources are that they require long-term tasks on efﬁ-
ciently structuring and classifying content and context. The
classiﬁcation, which has shown up being most important with
complex multi-disciplinary long-term classiﬁcation with prac-
tical simple and advanced applications of knowledge resources
is the Universal Decimal Classiﬁcation (UDC) [32].
According to Wikipedia currently about 150,000 institutions,
mostly libraries and institutions handling large amounts of
data and information, e.g., the ETH Library (Eidgen¨ossische
Technische Hochschule), are using basic UDC classiﬁcation
worldwide [33], e.g., with documentation of their resources,
library content, bibliographic purposes on publications and
references, for digital and realia objects. Just regarding the
library applications UDC is present in more than 144,000
institutions and 130 countries [34]. Further operational areas
are author-side content classiﬁcations and museum collections.
32
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

UDC allows an efﬁcient and effective processing of knowl-
edge data. UDC provides facilities to obtain a universal and
systematical view on the classiﬁed objects. UDC in com-
bination with statistical methods can be used for analysing
knowledge data for many purposes and in a multitude of ways.
With the knowledge resources in this research handling
70,000 classes, for 100,000 objects and several millions of
referenced data then simple workﬂows can be linear but the
more complex the algorithms get the workﬂows will mostly
become non-linear. They allow interactive use, dynamical
communication, computing, decision support, and pre- and
postprocessing, e.g., visualisation.
The classiﬁcation deployed for documentation [35] is able
to document any object with any relation, structure, and level
of detail as well as intelligently selected nearby hits and
references. Objects include any media, textual documents,
illustrations, photos, maps, videos, sound recordings, as well
as realia, physical objects, such as museum objects. UDC is a
suitable background classiﬁcation, for example:
The objects use preliminary classiﬁcations for multi-
disciplinary content. Standardised operations used with UDC
are coordination and addition (“+”), consecutive extension
(“/”), relation (“:”), order-ﬁxing (“::”), subgrouping (“[]”), non-
UDC notation (“*”), alphabetic extension (“A-Z”), besides
place, time, nationality, language, form, and characteristics.
C. Statistics implementation for the knowledge resources
A vast range of statistics, e.g., mathematical statistics, can be
deployed based on the knowledge resources. The application
of mathematical statistics beneﬁts from an increased number
of probes or elements. Probes can result from measurements,
e.g., from applied natural sciences and from available material.
In many cases, without further analysis a distribution or result
may seem random. If the accumulation of an occurrence may
indicate a regularity or a rule then this may correlate with a
statistical method. Many cases require that statistical results
have to be veriﬁed for realness. This can be done checking
against experience and understanding and using mathematical
means, e.g., computing probabilities based on probes.
Statistics have been used for steering the development of
the resources. Classiﬁcation and keyword statistics support
the optimisation of the quality of data within the knowledge
resources. Counts of terms, references, homophones, synonyms
and many more support the improvement of the discovery
workﬂows. Comparisons of content with different language
representations increase the intermediate associated result ma-
trices for a discovery process.
The created knowledge resources’ architecture is very ﬂex-
ible and efﬁcient because the components allow a natural
integration of multi-disciplinary knowledge. The processes of
optimising a result matrix differ from a statistical optimisation
by the fact that statistics is only one of the factors within the
workﬂows.
V.
IMPLEMENTED KNOWLEDGE RESOURCES’ MEANS
The goals for the combination of statistics and classiﬁcation
are, for example:
•
Creating and improving result matrices.
•
Decision making within workﬂows.
•
Further development of knowledge resources.
•
Extrapolation and prediction.
The implementation for the required ﬂexible workﬂow cre-
ation and levels is shown in the following sketch (Figure 1).
Workﬂow
Sub-workﬂow . . . Sub-workﬂow
Sub-sub-workﬂow . . . Sub-sub-workﬂow
[. . . any level . . .]
Algorithm . . . Algorithm
Resources interface . . .
Figure 1. Workﬂow-algorithm sketch of the implementation (non-hierarchical):
Workﬂow chains, algorithm calls, and resources’ interfaces.
The architecture is non-hierarchical. Any workﬂows can be
applied in chains. Each workﬂow can use sub-workﬂows, these
can use sub-sub-workﬂows and so on. Each workﬂow can
call or implement algorithms, e.g., for discovery processes,
evaluation, and statistics. The workﬂows and algorithms can
use or implement interfaces to the resources. The ellipses
indicate that any step can be called or executed in parallel on
HEC resources, e.g., in data-parallel or task-parallel processes,
in any number of required instances.
An example for this is a “multi-probe parallelised opti-
misation” workﬂow, which generates an intermediate result
matrix and uses the elements in order to create additional
results, all of which are combined for an overall optimised
result matrix. The intermediate result matrices are deploying
statistical, numerical methods, and various algorithms on base
of additional knowledge and information resources.
The
knowledge
resources
allow
to
implement
non-
hierarchical and hierarchical architectures. Depending on the
workﬂows these architectures may be created dynamically.
Figure 2 shows a workﬂow-algorithm sketch of a hierarchical
implementation based on the resources and emphasizing the
methodological aspects.
Workﬂow
Algorithms
Interface
Methods
Sub-workﬂow
Algorithms
Interface
Methods
Sub-sub-workﬂow
Algorithms
Interface
Resources
Figure 2. Workﬂow-algorithm sketch of a hierarchical implementation: Hier-
archies of workﬂows, resources provided by methods.
33
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

In this scenario workﬂows are implemented in a hierarchy
of sets workﬂows, sub-workﬂows, sub-sub-workﬂows and so
on. Algorithms can be employed by each of these workﬂows
on any level of this hierarchy. The algorithms in turn are
connected to the resources through interfaces. The resources
can be provided by creating different methods (e.g., static
access, dynamic access, batch operation).
A. n-Probe Parallelised Workﬂow
Computing result matrices can be handled in a multitude
of ways. An illustrating example is the n-Probe Parallelised
Workﬂow (nPPW). The workﬂow is deﬁned by the following
steps:
1)
A request is started searching for a term called start-
element.
2)
The search delivers a number of resulting elements,
called primary result-elements, being in context with
the start-element.
3)
The primary result-elements are sorted by a deﬁned
attribute (e.g., number of appearance or quality marker).
4)
The n most prominent primary result-elements as from
the previous step are retained.
5)
Secondary requests are started with each of the promi-
nent primary result-elements from the last step.
6)
The n most prominent secondary result-elements are
gathered for each request, according to the procedure
for the primary result-elements.
The workﬂow is not limited to a single type of elements.
Elements can be terms, numbers or other items depending on
the use case. For the same reason there is neither a limitation
on how to select or weight the elements or which algorithms
to use.
The following sketch (Figure 3) demonstrates this at the
example of a 5-probe parallelised workﬂow used for optimisa-
tion. In principle, the probes can consist of any type of object,
in this example, terms (“T”) are used, which are represented
by text strings for illustration. c indicates the count for an
element in the respective instance while in this example, the
absolute count is not in the focus. n indicates the position of
the elements.
The “ﬂat search” results in a primary result matrix (Fig-
ure 3a) containing terms corresponding with the request for
Term 1. In this 5-probe case the resulting primary matrix M0
consists of ﬁve elements, Term 1 to Term 5 (dark blue colour).
The “iterative parallelised search instances” in turn get the
elements of the result matrix from the ﬂat search as starting
seed. In this case, 5-probe means that besides the ﬂat search
another four secondary search instances have to be created.
The results of the four secondary requests are secondary
result matrices, here, Result Matrix 1 (M1) to Result Matrix 4
(M4) (Figures 3b to 3e). The terms are indexed “Term (m, n)”
in short “T (m, n)” with result matrix index m and matrix
element index n, starting on the primary matrix at zero.
T (0,1)
T (0,2)
T (0,3)
T (0,4)
T (0,5)
Primary Result Matrix M0 on Request T (0,1)
-
6c
n
a⃝



























Flat
Search
Instance
T (1,1)
T (1,2)
T (1,3)
T (1,4)
T (1,5)
Secondary Result Matrix 1 (M1) on T (0,2)
-
6c
n
b⃝



























Iterative
Parallelised
Search
Instance
T (2,1)
T (2,2)
T (2,3)
T (2,4)
T (2,5)
Secondary Result Matrix 2 (M2) on T (0,3)
-
6c
n
c⃝



























Iterative
Parallelised
Search
Instance
T (2,1)
T (2,2)
T (2,3)
T (2,4)
T (2,5)
Secondary Result Matrix 3 (M3) on T (0,4)
-
6c
n
d⃝



























Iterative
Parallelised
Search
Instance
T (2,1)
T (2,2)
T (2,3)
T (2,4)
T (2,5)
Secondary Result Matrix 4 (M4) on T (0,5)
-
6c
n
e⃝



























Iterative
Parallelised
Search
Instance
Figure 3. Result matrix creation from a single sub-sub-workﬂow via interme-
diate matrices (5-probe parallelised optimisation).
Only those secondary result elements ﬁtting with the original
primary elements are considered. The results of the secondary
instances are shown in light blue colour. The counts on the
34
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

various terms differ signiﬁcantly. Also some secondary search
instance can deliver higher counts on a term than the primary
search. The larger the primary result matrix is, the higher the
number of required consecutive secondary iterative search in-
stances is. In most cases it is a good approach to parallelise the
secondary instances, e.g., depending on the available compute
resources. The sum of the secondary instances contribute to the
overall workﬂow with an approachingly linear parallelisation
curve for an increasing number of instances. As shown, this
approach allows statistical support for the iterations, sub-
workﬂows, and discovery algorithms.
The knowledge resources can contribute to the processes
and optimisation with increased numbers of objects and also
more structured higher quality data included in the processes.
In many cases, e.g., for factual knowledge, manually created
components provide the highest values with the optimisation.
Hybrid “semi-automatically” and automatically created com-
ponents especially contribute due to their number, dynamical
content, and properties.
B. Data-centric parallelisation results
Common workﬂows can contain an arbitrary number of
result matrix operations. In this simple case the matrix contains
5×5 count elements, which may consist of 5 to 21 different
terms. As we want to discuss an elementary set of matrix
operations every other operations as considered to be pre- and
postprocessing in this case:
•
Preprocessing workﬂow,
•
Set of result matrix operations,
•
Postprocessing workﬂow.
The calculation depends on the assumption that the resources
can provide a sufﬁcient number elements on a speciﬁc request
via the workﬂow algorithms.
The following summary (Table I) shows the consequences
with n-probe result matrix operations for different numbers n
of elements, with nmax = (n − 1)2 + n:
TABLE I. n-PROBE: CONSEQUENCES WITH RESULT MATRIX OPERATIONS
FOR DIFFERENT NUMBERS n OF ELEMENTS (5, 10, AND 100,000).
Matrix
Different Elements Parallelisation ⇒ opt. time fact.
5×5
5–21
5e; 4c ⇒ n:2
10×10
10–91
10e; 9c ⇒ n:2
100,000×100,000 100,000–9,999,900,001
100,000e: 99,999c ⇒ n:2
That means, the algorithm provides a core set of elements
and a larger outer race set of elements, which absolutely and
relatively increases with increasing matrix sizes.
For a certain implementation allowing soft criteria for the
result matrices the relative and absolute numbers and content
of the core and outer race set of elements can be adapted
in order to create an implementation scalable in terms of
data, architecture, operation. In the example presented here
(Figure 3) 5 and 16 are particular numbers.
The “core” cores are a reasonable set of cores, which will
contribute to the efﬁciency of the respective result matrix
operation. The outer race cores can be handled very ﬂexible.
While different distributions of core and outer race sets can
still deliver the same results, e.g., for a given set of knowledge
resources, they can especially contribute to the workﬂow
scalability and optimisation process. The parallelisation of n
elements (“e”) with n − 1 outer race cores (“c”) can improve
the speedup from an optimisation time factor n to 2, compared
with the non-parallel implementation. For a per-instance-cycle
of 1 minute a full multi-parallel cycle takes about 2 minutes.
Any lower casts of multitude lead to the respective increase
of wall times. Under the assumption that the algorithm is
not modiﬁed for a set of different constellations of compute
resources then the process scales about linear. In general,
options for providing computing resources are a ﬁxed number
of many cores or a situative number of cores. The workﬂows
in this case can be adapted and react to certain compute and
storage architectures, considering the situative “Core number
of Cores” and the “Cloud Cores” (2C:2C) for the core and
outer race sets. This is even more signiﬁcant as most workﬂows
can integrate dynamical and intelligent components.
C. Weighted results: Statistics and value
Figure 4 illustrates the weighting of the result elements with
the above 5-probe parallelised workﬂow (Figure 3).
T (0,1)
T (0,2)
T (0,3)
T (0,4)
T (0,5)
Weighted Result Matrix (/w normalisation)
Primary Result Matrix M0 on Request T (0,1)
-
6c
n





































Weighted
Result
Matrix
/
Flat
Search
Instance
Figure 4. Weighted result matrix (green, without normalisation) creation via
intermediate matrices compared to ﬂat search instance (blue), via 5-probe
parallelised optimisation.
The weighted result matrix without (/w) normalisation is
resulting from the application of the 5-probe parallelised
optimisation on the result matrices of the search instances. In
this constellation the process is a single sub-sub-workﬂow, for
which we consider the result matrices as intermediate matrices.
In contrast to the ﬂat search instance (dark blue colour) the
weighted result matrix (green colour) shows different counts.
These may consequently result in different priorities and sort
orders, as in case of the weighted result for T (0,4) in relation
to T (0,3). The weighted priorities and sort orders represent
the content and context of the deployed resources, e.g., the
asymmetries and references.
The attributes of the content and context can require appro-
priate algorithms depending of the purposes and workﬂows
for the optimisation. Examples are mean values on counts and
ﬁtting to distribution curves with data sets.
35
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

D. Statistics and value based on resources
As implementations of statistics are based on counting and
numbers the statistics sub-workﬂows can deploy everything,
e.g., any feature or attributes, which can be counted. Sources
and means of statistics and computation are:
•
Dynamical statistics on the internal and external con-
tent and context (e.g., overall statistics, keyword-,
categories-, classiﬁcation-, and media-statistics).
•
Mathematics and formula on statistics from the content.
•
Elements’ statistics (structuring, content, references).
•
Statistics based on UDC classiﬁcation.
•
UDC-based statistics computed from comparisons and
associations of UDC groups and descriptions.
•
Statistics based on any combination of classiﬁcation,
keywords, content, references, context, and computation.
Workﬂows based on the statistics can be type “semi-manually”
or “automated”. Besides the major processing and optimisation
goals descriptive statistics can be done with each workﬂow
or sub-workﬂow. Any change of the means supported within
a workﬂow can contribute to the optimisation of the result
matrix. Suitable and appropriate means have to be determined
for best supporting the goals of the respective step in the
workﬂow. The implementation considers measuring the op-
timisation by quantity and quality of attributes and features,
on intelligence-based and learning processes. With either use
there is no general quality measure. Possible quality measures
depend on purpose, view, and deployed means. In addition, the
decision on these measures can be well supported by statistics,
e.g., comparing result matrices from different workﬂows on the
same request. Learning systems components can be used for
capturing the success of different measures. The knowledge
resources can contain equations and formulary of any grade
of complexity. Due to the very high complexity level of the
multi-disciplinary components it is necessary to use the basic
instances for a comparison in this context of matrix statistics.
The following passages show basic excerpts of statistics
objects (LATEX representation) being part of the implemented
knowledge resources. These statistics methods/equations are
selected and shown mainly for two reasons: The selected
methods are taken from the knowledge objects contained
in the resources. These methods are used for result matrix
calculations and compared with the evaluation in this research.
VI.
STATISTICS: FUNDAMENTALS AND APPLICATION
Statistics on itself can rarely give an overall decisive answer
on a question. Statistic means merely can be used as tools
for supporting valuations and decisions. Statistics, probability,
and distributions are valuable auxiliaries within workﬂows and
integrated application components, e.g., on numbers of objects,
spatial or georeferences, phonetic variations, and series of
measurement values. Probability and statistics measures are
used with integrated applications, e.g., with search requests,
with seismic components (e.g., Median and Mean Stacks),
which can also be implemented on base of the resources.
A. Basic algorithms applied with knowledge resources
The mean value, arithmetic mean or average M for n values
is given by
M = 1
n
n
X
ν=1
xν
(1)
Calculating the mean value is described by a linear operation.
The median value or central value is the middle value in a
size-depending sort order of a number of values. For making
a statement on the extent of a group of values, the variance
(“scattering”) can be calculated, with the mean deviation m
and the squared mean deviation m2.
m2 = 1
n
n
X
ν=1
(xν − M)2 = (x − M)2
(2)
For any value this holds m2(A) = m2 + (M − A)2. When
applying statistics, especially when calculating the propagated
error, the following deﬁnition of the variance is used:
m2 =
1
n − 1
n
X
ν=1
(xν − M)2
(3)
The mean deviation ζ(A) is deﬁned as:
ζ(A) = |x − A| for which holds ζ(A) = min. for A = Z (4)
The probable deviation or probable error ρ with the probable
limits Q1 and Q3 is deﬁned as:
ρ = Q3 − Q1
2
(5)
The relative frequency hi is deﬁned as:
hi = ni
n , then it holds
k
X
i=1
hi = 1
(6)
where ni is the class frequency, which means the number of
elements in a class of which the middle element is xi.
B. Distributions deployed with knowledge resources
A continuous summation results in the cumulative frequency
distribution
Hi =
i
X
j=1
hj
(7)
which gives the relative number, for which holds x ≤ xi · Hi
is a function discretly increasing from 0 to 1. The presentation
results in a summation line. With steady variables, for which at
an interval width of ∆x the quotient hi/∆x nears a limit, one
can calculate a frequency density h(xi) and for the summation
frequency H(x):
h(xi) = lim
∆x→0
hi
∆x
and
dH(x)
dx
= h(x)
(8)
36
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

With statistical distributions the Gaussian normal distribution
is of basic importance.
h(x) =
1
√
2π e
−1
2x2
(9)
H(x) can not be given “closed”. It can be shown that
K =
+∞
Z
−∞
h(x)dx =
1
√
2π
+∞
Z
−∞
e
−1
2x2
dx = 1
(10)
The Binominal distribution wk(s) is deﬁned by
wk(s) =
k
s

psqk−s
(11)
The sum of the two binominal coefﬁcients is equal to

k+1
s

.
This is described by Pascals’ Triangle. It holds:
M =
k
X
s=0
wk(s) · s = kp
and
m =
p
kqp
(12)
Accordingly, the mean error of the mean value decreases
proportional to 1/
√
k. This describes the error propagation law.
h(X) =
1
√
2πme
−1
2

C. Classiﬁcation and statistics in this sample case
Table II shows a small excerpt of resulting main UDC
classiﬁcation references practically used for the statistics with
the knowledge resources in the example case presented here.
TABLE II. UNIVERSAL DECIMAL CLASSIFICATION OF STATISTICS
FEATURES WITH THE KNOWLEDGE RESOURCES (EXCERPT).
UDC Code
Description
UDC:3
Social Sciences
UDC:310
Demography. Sociology. Statistics
UDC:311
Statistics as a science. Statistical theory
UDC:311.1
Fundamentals, bases of statistics
UDC:311.21
Statistical research
UDC:311.3
General organization of statistics. Ofﬁcial statistics
UDC:5
Mathematics. Natural sciences
UDC:519.2
Probability. Mathematical Statistics
UDC:531.19
Statistical mechanics
UDC:570.087.1 Biometry. Statistical study and treatment of biological data
UDC:615.036
Clinical results. Statistics etc.
The small unsorted excerpts of the knowledge resources
objects only refer to main UDC-based classes, which for this
part of the publication are taken from the Multilingual Univer-
sal Decimal Classiﬁcation Summary (UDCC Publication No.
088) [23] released by the UDC Consortium under the Creative
Commons Attribution Share Alike 3.0 license [36] (ﬁrst release
2009, subsequent update 2012).
As with any object the statistics features can be combined
for facets and views for any classiﬁcation subject. On the other
hand statistics objects from the resources can be selected and
applied. The listing (Figure 5) shows an excerpt intermediate
object result matrix on statistics content.
1
ANOVA
[Statistics, ...]:
2
Analysis of Variance.
3
BIWS
[Whaling]:
4
Bureau of International Whaling Statistic.
5
GSP
[Geophysics]:
6
Geophysical Statistics Project.
7
Median
[Statistics]:
8
In the middle line.
9
s. also Median-Stack
10
Median-Stack [Seismics]:
11
Stacking based on the median value of
adjacent traces.
12
MSWD
[Mathematics]:
13
Mean Square Weighted Deviation.
14
MSA
[Abbbreviation, GIS]:
15
Metropolitan Statistical Area.
16
MOS
[Abbreviation]:
17
Model Output Statistics.
18
MCDM
[GIS, GDI, Statistics, ...]:
19
Multi-Criteria Decision Making.
20
SHIPS
[Meteorology]:
21
Statistical Hurricane Intensity Prediction
Scheme.
22
SAND
[Abbreviation]:
23
Statistical Analysis of Natural resource
Data, Norway.
Figure 5. Intermediate object result matrix on “statistics” content.
Learning from this: The classiﬁcations used for this inter-
mediate matrix are based on contributions from more than
one discipline. The elements themselves do not necessarily
have to contain a requested term because the classiﬁcation
contributes. Several steps may be necessary in order to improve
the matrix, e.g., selecting disciplines, time intervals on the
entries, references, and associations. Because different content
carries different attributes and features the evaluation can be
used in comparative as well as in complementary context.
The implemented knowledge resources means of statis-
tics and computation described above are integrated in the
workﬂows, including classiﬁcation, dating, and localisation of
objects. In addition, probability distributions, linear and non-
linear modelling, and other supportive tools are used within
the workﬂow components.
D. Resulting numbers on processing and computing
The processing and computational demands per workﬂow
instance result from the implementation scenarios. The follow-
ing comparison (Table III) results from a minimal workﬂow
request for a result matrix compared to a workﬂow request
for a result matrix supporting classiﬁcation views referring
to UDC, supporting references and statistics on intermediate
results. Both scenarios are based on the same number of
elements and entries and can be considered atomic instances
in a larger workﬂow. Views and result matrices can be created
manually and automated in interactive and batch operation.
TABLE III. PROCESSING AND COMPUTATIONAL DEMANDS: 2 SCENARIOS,
BASED ON 50000 OBJECT ELEMENTS AND 10 RESULT MATRIX ENTRIES.
Scenario Workﬂow Request for Result Matrix
Value
“geosciences archaeology” (minimal)
Number of elements
50,000
Number of result matrix entries (deﬁned)
10
Number of workﬂow operations
15
Wall time on one core
14 s
“geosciences archaeology” (UDC, references, statistics)
Number of elements
50,000
Number of result matrix entries (deﬁned)
10
Number of workﬂow operations
6,500
Wall time on one core
6,700 s
As the discussed scenarios are instances this means work-
ﬂows based on n of these instances will at least require n-
times the time for an execution on the same system. It must
be remembered that the parallelisation will have a signiﬁcant
effect when workﬂows are created based on many of these
instances when required in parallel. Without modifying the
algorithms of the instances, which mostly means simplifying,
the positive parallelisation effect for the workﬂows can be
nearly linear. Besides the large requirements per instance
with most workﬂows there are signiﬁcant beneﬁcial effects
from parallelising even within single instances as soon as the
number of comparable tasks based on the instances increases.
A typical case where parallelisation within a workﬂow is
favourable is the implementation of an application creating
result matrices and being used with many parallel instances,
e.g., with providing services. The number of 70,000 elementary
UDC classes currently results in 3 million basic elements
when only considering multi-lingual entries – without any
38
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

combinations. With most isolated resources only several thou-
sand combinations are used in practice each. The variety
and statistics are mostly deployed for decision processing,
increasing quantity, and increasing quality. Many of the above
cases require to compute more than one data-workﬂow set
to create a decision. A review and an auditing process are
mandatory for mission critical applications. The computational
requirements can increase drastically with the computation of
multiple workﬂows. Each workﬂow will consist of one or
more processes, which can contain different conﬁgurations and
parameters. Therefore, creating a base for an improved result
matrix starts with creating several intermediate result matrices.
With a ten process workﬂow, e.g., the possible conﬁgurations
and parameters can easily lead to computing a reasonable set
of thousands to millions of intermediate result matrices.
The objects and methods used can be long-term documented
as knowledge objects. Nevertheless, there is explicitely no
demand for a certain programming language. Even multiple
implementations can be done with any object. The workﬂows
and algorithms with the cases discussed here have been im-
plemented as objects in Fortran, Perl, and Shell. Anyhow, the
implementation of algorithms is explicitely not part of any core
resources. It is the task of anyone having an application to do
this and to decide on the appropriate means and methods.
E. Complementary Components
As an example we choose to mention three state-of-the-art
components for implementing the “data-base”, operating sys-
tem, and distributed platform. With this it should be possible
to build and use containers. For implementation of very simple
non-hierarchical but data-set centred scenarios the MongoDB
[37] concept may be used. This database model greps the con-
cept of a data-set centred approach and extends the traditional
database models. CoreOS [38] can be used for data-warehouse
style computing, providing and operating system for massive
server deployment. In addition, Docker [39] can be used, an
open platform for distributed applications, which shall enable
to build, ship and run applications anywhere.
Anyhow, these components are not data-centric themselves.
It is also more than questionable if data can be sustainably
preserved in close integration with these components, even for
mid-term purposes of a few decades only.
The knowledge resources, including their creation and fur-
ther development, should be kept in a long-term and portable
concept, as an implementation based on such above compo-
nents has shown to be still much too application centric.
VIII.
CASE RESULTS AND EVALUATION
Computing result matrices is an arbitrary complex task,
which can depend on various factors. Applying statistics and
classiﬁcation to knowledge resources has successfully provided
excellent solutions, which can be used for optimising result
matrices in context of natural sciences, e.g., geosciences,
archaeology, volcanology or with spatial disciplines, as well
as for universal knowledge. The method and application types
used for optimisation imply some general characteristics when
putting discovery workﬂows into practice regarding compo-
nents like terms, media, and other context (Table IV).
TABLE IV. RESULTING PER-INSTANCE-CALLS FOR METHOD AND
APPLICATION TYPES ON OPTIMISATION WITH KNOWLEDGE DISCOVERY.
Type
Terms
Media
Workﬂow
Algorithm
Combination
Mean
500
20
20
50,000
3,000
Median
10
5
2
5,000
50
Deviation
30
5
5
200
20
Distribution
90
40
15
20
120
Correlation
15
10
5
20
90
Probability
140
15
20
50
150
Phonetics
50
5
10
20
50
Regular expr.
920
100
50
40
1,500
References
720
120
30
5
900
Association
610
60
10
5
420
UDC
530
120
20
5
660
Keywords
820
100
10
5
600
Translations
245
20
5
5
650
Corrections
60
10
5
5
150
External res.
40
30
5
5
40
Statistics methods have shown to be an important means
for successfully optimising result matrices. The most widely
implemented methods for the creation of result matrices are
intermediate result matrices based on regular expressions and
intermediate result matrices based on combined regular ex-
pressions, classiﬁcation, and statistics, giving their numbers
special weight. Based on these per-instance numbers this
results in demanding requirements for complex applications
– On numerical data: Millions of calls are done per algorithm
and dataset, hundreds in parallel/compact numeric routines.
On “terms”: Hundred thousands of calls are done per sub-
workﬂow, thousands in parallel/complex routines, are done.
Most resources are used for one application scenario only.
Only 5–10 percent overlap between disciplines – due to mostly
isolated use. Large beneﬁts result from multi-disciplinary
multi-lingual integration. The multi-lingual application adds
an additional dimension to the knowledge matrix, which can
be used by most discovery processes. As this implemented
dimension is of very high quality the matrix space can beneﬁt
vastly from content and references.
IX.
CONCLUSION AND FUTURE WORK
This paper presented the extended research, focussing on
data aspects and practical workﬂows, based on the funda-
mental research on optimising result matrices from knowledge
discovery workﬂows. This research has extended the applied
features used with long-term knowledge resources’ objects
and context. Starting with the multi-disciplinary and multi-
lingual knowledge resources examples for non-hierarchical and
hierarchical workﬂows have been presented.
First, knowledge resources’ objects with their structured
content, references, and conceptual knowledge are providing
an excellent means for long-term multi-disciplinary and multi-
lingual documentation and reuse. This especially includes the
ﬂexible universal classiﬁcation of any objects. The quality of
39
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

data can be used to contribute to the discovery and optimisation
processes, which increases the emphasis on the values of data
the more the long-term signiﬁcance gets into the focus.
Second, the use of statistics and algorithms based on statis-
tics has shown to provide solid tools for creating and improving
result matrices. Both, the documentation and resources and the
statistics applicable in workﬂows result in beneﬁts for complex
result matrix generation.
The case study introduced the application of n-Probe Paral-
lelised Workﬂows, which can be used for result matrix genera-
tion. The matrix generation and processes have been discussed
in detail. Workﬂows like these have been successfully used
for the optimisation of result matrices. They allow to use
statistics methods and data value weighting and can contribute
to the creation and development of resources. A number of
structuring elements and workﬂow procedures have been suc-
cessfully implemented for processing objects from knowledge
resources, which allow optimising result matrices in very
ﬂexible ways. Long-term multi-disciplinary and multi-lingual
knowledge resources can provide a solid source of structured
content and references for a wealth of result matrices. The
long-term results conﬁrm that for the usability the organisation
of the content and the data structures are most important
and should have the overall focus compared to algorithm
adaptation and optimisation. Nevertheless, the computational
requirements may be very high but compared against the long-
term data creation issues, they should be regarded secondary
from the scientiﬁc point of view. Employing a classiﬁcation
like UDC has shown to be a universal and most ﬂexible solu-
tion with statistics for supporting long-term multi-disciplinary
knowledge resources. Computing optimised result matrices
from objects of universally classiﬁed knowledge resources can
be efﬁciently supported by various statistics and probability
measures. With the quality and quantity of matrix elements
this can also improve the decision making processes within
the workﬂows.
The research conducted provided that advanced discovery
will have to go into depth as well as into broad surface of the
context of the multi-disciplinary and multi-lingual information
in order to effectively improve the quality for most workﬂows.
Many of these workﬂow processes can be very well paral-
lelised on HEC resources. A typical case where parallelisation
is required is the implementation of an application creating
result matrices and used with many parallel instances. This
introduces beneﬁts for the applicability of the discovery facing
big data resources to be included. The integration of the
above strategies and means has proven an excellent method
for computing optimised result matrices.
On the computational side, the workﬂows contribute to the
parallelisation of processes and result in higher scalability re-
garding data resources, architectures, and operation. Therefore,
the resources and processing workﬂows can beneﬁt from a
ﬂexible deployment of High End Computing resources. The
major outcome on the content side is the impact on long-
term resources based on the scientiﬁc results regarding the
systematics and methodologies for caring for knowledge.
Besides all future application scenarios, the further creation
of development of content and context and its documentation
is a main goal. Future work will be focussed on the workﬂow
processes and standardisation and best practice for container
and resources’ objects but also concentrate on the develop-
ment of ﬂexible structures for objects and the automation of
processes.
ACKNOWLEDGEMENTS
We are grateful to all national and international partners
in the GEXI cooperations for their support and contributions.
We thank the Science and High Performance Supercomput-
ing Centre (SHPSC) for long-term support of collaborative
research since 1997, including the GEXI developments and
case studies on archaeological and geoscientiﬁc information
systems. Special thanks go to the scientiﬁc colleagues at the
Gottfried Wilhelm Leibniz Bibliothek (GWLB) Hannover, es-
pecially Dr. Friedrich H¨ulsmann, for collaboration and proliﬁc
discussion within the “Knowledge in Motion” project, for
inspiration, and practical case studies. Many thanks go to
the scientiﬁc colleagues at the Leibniz Universit¨at Hannover,
especially to Dipl.-Biol. Birgit Gersbeck-Schierholz and to
Dr. Bernhard Bandow, Max Planck Institute for Solar System
Research (MPS), G¨ottingen, for their collaboration and discus-
sion on non-hierarchical and hierarchical workﬂows, as well as
to the scientiﬁc colleagues at the Institute for Legal Informatics
(IRI), Leibniz Universit¨at Hannover, and to the Westf¨alische
Wilhelms-Universit¨at (WWU), for discussion, support, and
sharing experiences on collaborative computing and knowledge
resources and for participating in fruitful case studies as well as
to my students and participants of the postgraduate European
Legal Informatics Study Programme (EULISP) for proliﬁc
discussion of scientiﬁc, legal, and technical aspects over the
last years.
REFERENCES
[1]
C.-P. R¨uckemann, “Computing Optimised Result Matrices for the
Processing of Objects from Knowledge Resources,” in Proceedings
of The Fourth International Conference on Advanced Communica-
tions and Computation (INFOCOMP 2014), July 20–24, 2014, Paris,
France. XPS Press, 2014, pages 156–162, ISSN: 2308-3484, ISBN-13:
978-1-61208-365-0, URL: http://www.thinkmind.org/index.php?view=
article&articleid=infocomp 2014 7 20 60039 [accessed: 2015-02-01].
[2]
P. Cerchiello and P. Giudici, “Non parametric statistical models for on-
line text classiﬁcation,” Advances in Data Analysis and Classiﬁcation
– Theory, Methods, and Applications in Data Science, vol. 6, no. 4,
2012, pp. 277–288, special issue on “Data analysis and classiﬁcation
in marketing” Baier, D. and Decker, R. (guest eds.) ISSN: 1862-5347
(print), ISSN: 1862-5355 (electronic).
[3]
W. Gaul and M. Schader, Eds., Classiﬁcation As a Tool of Research.
North-Holland, Amsterdam, 1986, Proceedings, Annual Meeting of the
Classiﬁcation Society, (Proceedings der Fachtagung der Gesellschaft f¨ur
Klassiﬁkation), ISBN-13: 978-0444879806, ISBN-10: 0-444-87980-3,
Hardcover, XIII, 502 p., May 1, 1986.
[4]
J. Templin and L. Bradshaw, “Measuring the Reliability of Diagnostic
Classiﬁcation Model Examinee Estimates,” Journal of Classiﬁcation,
vol. 30, no. 2, 2013, pp. 251–275, Heiser, W. J. (ed.), ISSN: 0176-4268
(print), ISSN: 1432-1343 (electronic), URL: http://dx.doi.org/10.1007/
s00357-013-9129-4 [accessed: 2015-02-01].
[5]
A.
Woodie,
“Forget
the
Algorithms
and
Start
Cleaning
Your
Data,”
Datanami,
2014,
March
26,
2014,
URL:
http://www.datanami.com/datanami/2014-03-26/forget the
algorithms and start cleaning your data.html [accessed: 2015-02-01].
40
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[6]
World Social Science Report 2013, Changing Global Environments,
1st ed.
Published jointly by the United Nations Educational, Scientiﬁc
and Cultural Organization (UNESCO), the International Social Science
Council (ISSC), the Organisation for Economic Co-operation and
Development (OECD), 2013, DOI: 10.1787/9789264203419-en, OECD
ISBN 978-92-64-20340-2 (print), OECD ISBN 978-92-64-20341-9
(PDF), UNESCO ISBN 978-92-3-104254-6 (PDF and print).
[7]
C.-P. R¨uckemann, “From Multi-disciplinary Knowledge Objects to
Universal Knowledge Dimensions: Creating Computational Views,” In-
ternational Journal On Advances in Intelligent Systems, vol. 7, no. 3&4,
2014, pp. 385–401, ISSN: 1942-2679, LCCN: 2008212456 (Library of
Congress).
[8]
C.-P. R¨uckemann, “Sustainable Knowledge Resources Supporting Sci-
entiﬁc Supercomputing for Archaeological and Geoscientiﬁc Infor-
mation Systems,” in Proceedings of The Third International Con-
ference on Advanced Communications and Computation (INFO-
COMP 2013), November 17–22, 2013, Lisbon, Portugal.
XPS
Press, 2013, pp. 55–60, ISSN: 2308-3484, ISBN: 978-1-61208-310-
0, URL: http://www.thinkmind.org/download.php?articleid=infocomp
2012 3 10 10012 [accessed: 2015-02-01].
[9]
C.-P. R¨uckemann, “High End Computing for Diffraction Amplitudes,”
in The Third Symposium on Advanced Computation and Information
in Natural and Applied Sciences, Proceedings of The 11th Interna-
tional Conference of Numerical Analysis and Applied Mathematics
(ICNAAM), September 21–27, 2013, Rhodes, Greece, Proceedings of
the American Institute of Physics (AIP), AIP Conference Proceedings,
vol. 1558.
AIP Press, 2013, pp. 305–308, ISBN: 978-0-7354-1184-5,
ISSN: 0094-243X, DOI: 10.1063/1.4825483.
[10]
U. Inden, D. T. Meridou, M.-E. C. Papadopoulou, A.-C. G. Anadiotis,
and C.-P. R¨uckemann, “Complex Landscapes of Risk in Operations
Systems Aspects of Processing and Modelling,” in Proceedings of
The Third International Conference on Advanced Communications and
Computation (INFOCOMP 2013), November 17–22, 2013, Lisbon, Por-
tugal.
XPS Press, 2013, pp. 99–104, ISSN: 2308-3484, ISBN: 978-1-
61208-310-0, URL: http://www.thinkmind.org/download.php?articleid=
infocomp 2013 5 10 10114 [accessed: 2015-02-01].
[11]
P. Leit˜ao, U. Inden, and C.-P. R¨uckemann, “Parallelising Multi-agent
Systems for High Performance Computing,” in Proceedings of The
Third International Conference on Advanced Communications and
Computation (INFOCOMP 2013), November 17–22, 2013, Lisbon,
Portugal.
XPS Press, 2013, pp. 1–6, ISSN: 2308-3484, ISBN: 978-1-
61208-310-0, URL: http://www.thinkmind.org/download.php?articleid=
infocomp 2013 1 10 10055 [accessed: 2015-02-01].
[12]
Ponemon Institute, “Data-Security,” 2014, Ponemon Institute, URL:
http://www.ponemon.org/data-security [accessed: 2015-02-01].
[13]
C.-P. R¨uckemann, “High End Computing Using Advanced Archaeol-
ogy and Geoscience Objects,” International Journal On Advances in
Intelligent Systems, vol. 6, no. 3&4, 2013, pp. 235–255, iSSN: 1942-
2679, URL: http://www.iariajournals.org/intelligent systems/intsys v6
n34 2013 paged.pdf [accessed: 2015-02-01].
[14]
Ponemon Institute, “Ponemon Study Shows the Cost of a Data Breach
Continues to Increase,” 2014, Ponemon Institute, URL: http://www.
ponemon.org/news-2/23 [accessed: 2015-02-01].
[15]
Ponemon Institute, “Cost of Data Breach 2011,” 2011, Ponemon Insti-
tute / Symantec, URL: http://www.ponemon.org/library/archives/2012/
03 [accessed: 2015-02-01].
[16]
Ponemon
Institute,
“2013
Cost
of
Data
Breach:
Global
Analysis,”
2013,
Ponemon
Institute
/
Symantec,
URL:
http://www.ponemon.org/local/upload/ﬁle/2013%20Report%
20GLOBAL%20CODB%20FINAL%205-2.pdf
[accessed:
2015-
02-01].
[17]
Ponemon Institute, “2014 Cost of Data Breach: Global Analysis,” May
2014, Ponemon Institute / IBM, Ponemon Institute (c) Research Report,
Benchmark research sponsored by IBM, Independently conducted by
Ponemon Institute LLC, IBM Document Number: SEL03027USEN,
URL: http://www.ibm.com/services/costofbreach [accessed: 2015-02-
01], URL: http://public.dhe.ibm.com/common/ssi/ecm/en/sel03027usen/
SEL03027USEN.PDF [accessed: 2015-02-01].
[18]
Symantec, “Symantec Data Breach Calculator,” 2014, Symantec, URL:
https://databreachcalculator.com/ [accessed: 2015-02-01].
[19]
“Wissensverlust vermeiden beim Abgang von Wissensarbeitern,” li-
brary essentials, LE Informationsdienst, Juni/Juli 2014, 2014, pp. 9–11,
ISSN: 2194-0126, URL: http://www.libess.de [accessed: 2015-02-01].
[20]
M. E. Jennex, “A Proposed Method for Assessing Knowledge Loss Risk
with Departing Personnel,” vol. 44, no. 2, 2014.
[21]
“LX-Project,”
2014,
URL:
http://www.user.uni-hannover.de/cpr/x/
rprojs/en/#LX (Information) [accessed: 2015-02-01].
[22]
C.-P. R¨uckemann, “Enabling Dynamical Use of Integrated Systems and
Scientiﬁc Supercomputing Resources for Archaeological Information
Systems,” in Proc. INFOCOMP 2012, Oct. 21–26, 2012, Venice, Italy,
2012, pp. 36–41, ISBN: 978-1-61208-226-4.
[23]
“Multilingual Universal Decimal Classiﬁcation Summary,” 2012, UDC
Consortium, 2012, Web resource, v. 1.1. The Hague: UDC Consortium
(UDCC Publication No. 088), URL: http://www.udcc.org/udcsummary/
php/index.php [accessed: 2015-02-01].
[24]
“UDC Online,” 2014, URL: http://www.udc-hub.com/ [accessed: 2015-
02-01].
[25]
F. H¨ulsmann and C.-P. R¨uckemann, “Value of Data and Long-term
Knowledge,” KiMrise, Knowledge in Motion, August 12, 2014, 10 Year
Anniversary Workgroup Meeting, “Unabh¨angiges Deutsches Institut f¨ur
Multi-disziplin¨are Forschung (DIMF)”, Hannover, Germany, 2014.
[26]
C.-P. R¨uckemann, “Archaeological and Geoscientiﬁc Objects used with
Integrated Systems and Scientiﬁc Supercomputing Resources,” Inter-
national Journal on Advances in Systems and Measurements, vol. 6,
no. 1&2, 2013, pp. 200–213, ISSN: 1942-261x, LCCN: 2008212470
(Library of Congress), URL: http://www.thinkmind.org/download.php?
articleid=sysmea v6 n12 2013 15 [accessed: 2015-02-01], URL: http:
//lccn.loc.gov/2008212470 [accessed: 2015-02-01].
[27]
Y. Dodge, The Oxford Dictionary of Statistical Terms.
Oxford
University Press, 2006, ISBN: 0-19-920613-9.
[28]
B. S. Everitt, The Cambridge Dictionary of Statistics, 3rd ed.
Cam-
bridge University Press, Cambridge, 2006, ISBN: 0-521-69027-7.
[29]
C. M. Bishop, Pattern Recognition and Machine Learning.
Springer,
2006, ISBN: 0-387-31073-8.
[30]
R. D. Drennan, Statistics in Archaeology.
Elsevier Inc., 2008, in:
Pearsall, Deborah M. (ed.), Encyclopedia of Archaeology, pp. 2093-
2100, Elsevier Inc., ISBN: 978-0-12-373962-9.
[31]
D. Lindley, “The Philosophy of Statistics,” Journal of the Royal Sta-
tistical Society, 2000, JSTOR 2681060, Series D 49 (3), pp. 293–337,
DOI: 10.1111/1467-9884.00238.
[32]
“Universal Decimal Classiﬁcation Consortium (UDCC),” 2014, URL:
http://www.udcc.org [accessed: 2015-02-01].
[33]
“Universal
Decimal
Classiﬁcation
(UDC),”
2015,
Wikipedia,
URL:
http://en.wikipedia.org/wiki/Universal Decimal Classiﬁcation
[accessed: 2015-02-01].
[34]
A.
Slavic,
“UDC
libraries
in
the
world
-
2012
study,”
uni-
versaldecimalclassiﬁcation.blogspot.de,
2012,
Monday,
20
August
2012, URL: http://universaldecimalclassiﬁcation.blogspot.de/2012/08/
udc-libraries-in-world-2012-study.html [accessed: 2015-02-01].
[35]
C.-P. R¨uckemann, “Integrating Information Systems and Scientiﬁc
Computing,” International Journal on Advances in Systems and
Measurements,
vol.
5,
no.
3&4,
2012,
pp.
113–127,
ISSN:
1942-261x,
LCCN:
2008212470
(Library
of
Congress),
URL:
http://www.thinkmind.org/index.php?view=article&articleid=sysmea
v5 n34 2012 3/ [accessed: 2015-02-01].
41
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[36]
“Creative Commons Attribution Share Alike 3.0 license,” 2012, URL:
http://creativecommons.org/licenses/by-sa/3.0/ [accessed: 2015-02-01].
[37]
“MongoDB,” 2015, URL: http://www.mongodb.org/ [accessed: 2015-
02-01].
[38]
“CoreOS,” 2015, URL: https://coreos.com/ [accessed: 2015-02-01].
[39]
“Docker,” 2015, URL: https://www.docker.com/ [accessed: 2015-02-
01].
42
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

