Prioritized Adaptive Max-Min Fair Residual Bandwidth Allocation for
Software-Deﬁned Data Center Networks
Andrew Lester
School of Information Technology
Illinois State University
aeleste@ilstu.edu
Yongning Tang
School of Information Technology
Illinois State University
ytang@ilstu.edu
Tibor Gyires
School of Information Technology
Illinois State University
tbgyires@ilstu.edu
Abstract—Modern data center networks commonly adopt
multi-rooted tree topologies. Equal-Cost Multi-Path (ECMP)
forwarding is often used to achieve high link utilization and
improve network throughput. Meanwhile, max-min fairness
is widely used to allocate network bandwidth fairly among
multiple applications. However, today’s data centers usually
host diverse applications, which have various priorities (e.g.,
mission critical applications) and service level agreements
(e.g., high throughput). It is unclear how to adopt ECMP
forwarding and max-min fairness in the presence of such
requirements. We propose Prioritized Max-Min Fair Multiple
Path forwarding (PMP) to tackle this challenge. PMP can opti-
mally allocate current available bandwidth to maximally satisfy
user demands. When predeﬁned application requirements are
available, PMP can prioritize current demands and allocate
available bandwidth accordingly. Our performance evaluation
results show that PMP can improve application throughput 10-
12% on average and increase overall link utilization especially
when the total demanded bandwidth is close or even exceeds
the bisectional bandwidth of a data center network.
Keywords- SDN, max-min fair, scheduling.
I. INTRODUCTION
In recent years, data centers have become critical compo-
nents for many large organizations [1] [2] [19]. A data center
(DC) refers to any large, dedicated cluster of computers
that is owned and operated by a single authority, built and
employed for a diverse set of purposes. Large universities
and private enterprises are increasingly consolidating their
IT services within on-site data centers containing a few
hundred to a few thousand servers. On the other hand, large
online service providers, such as Google, Microsoft, and
Amazon, are rapidly building geographically diverse cloud
data centers, often containing more than 10,000 servers, to
offer a variety of cloud-based services such as web servers,
storage, search, on-line gaming. These service providers also
employ some of their data centers to run large-scale data-
intensive tasks, such as indexing Web pages or analyzing
large data-sets, often using variations of the MapReduce
paradigm.
Many data center applications (e.g., scientiﬁc computing,
web search, MapReduce) require substantial bandwidth.
With the growth of bandwidth demands for running various
user applications, data centers also continuously scale the
capacity of the network fabric for new all-to-all commu-
nication patterns, which presents a particular challenge for
traditional data forwarding (switching and routing) mech-
anisms. For example, MapReduce based applications, as a
currently adopted default computing paradigm for big data,
need to perform signiﬁcant data shufﬂing to transport the
output of its map phase before proceeding with its reduce
phase. Recent study shows the principle bottleneck in large-
scale clusters is often inter-node communication bandwidth.
Trafﬁc pattern study [17] showed that only a subset (25%
or less) of the core links often experience high utilization.
Modern data center networks commonly adopt multi-
rooted tree topologies [1] [2] [19]. ECMP is often used to
achieve high link utilization and improve network through-
put. Meanwhile, max-min fairness is widely used to allo-
cate network bandwidth fairly among multiple applications.
Many current data center schedulers, including Hadoops Fair
Scheduler [26] and Capacity Scheduler [25], Seawall [24],
and DRF [27], provide max-min fairness. The attractiveness
of max-min fairness stems from its generality. However,
today’s data centers usually host diverse applications, which
have various priorities (e.g., mission critical applications)
and service level agreements (e.g., high throughput). It
is unclear how to adopt ECMP forwarding and max-min
fairness in the presence of such requirements. We propose
Prioritized Max-Min Fair Multiple Path forwarding (PMP)
to tackle this challenge. PMP can optimally allocate current
available bandwidth to maximally satisfy user demands.
When predeﬁned application requirements are available,
PMP can prioritize current demands and allocate available
bandwidth accordingly.
The disruptive Software-Deﬁned Networking (SDN) tech-
nology shifts today’s networks that controlled by a set
of vendor speciﬁc network primitives to a new network
paradigm empowered by new programmatic abstraction.
OpenFlow provides a protocol such that the logical cen-
tralized controller can exploit forwarding tables on SDN
switches for programmatic multi-layer forwarding ﬂexibility.
One of the fundamental transformations that ﬂow based
forwarding presents is the inclusion of multi-layer header
information to make forwarding match and action logic pro-
198
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

grammatically. Programmatic policy is vital to manage the
enormous combinations of user requirements. For example,
an SDN controller can ﬂexibly deﬁne a network ﬂow using
a tuple as (incoming port, MAC Src, MAC Dst, Eth Type,
VLAN ID, IP Src, IP Dst, Port Src, Port Dst, Action).
The rest of the paper is organized as the following.
Section II discusses the related research work. Section III
formalizes the problem and describes our approach. Sec-
tion IV presents our simulation design and results, respec-
tively. Finally, Section V concludes the paper with future
directions.
II. RELATED WORK
Current large data center networks connect multiple Eth-
ernet LANs using IP routers and run scalable routing algo-
rithms over a number of IP routers. These layer 3 routing
algorithms allow for shortest path and ECMP routing, which
provide much more usable bandwidth than Ethernets span-
ning tree. However, the mixed layer 2 and layer 3 solutions
require signiﬁcant manual conﬁguration.
The trend in recent works to address these problems is
to introduce special hardware and topologies. For example,
PortLand [2] is implementable on Fat Tree topologies and
requires ECMP hardware that is not available on every
Ethernet switch. TRILL [3] introduces a new packet header
format and thus requires new hardware and/or ﬁrmware
features.
There have been many recent proposals for scale-out
multi-path data center topologies, such as Clos networks [4],
[5], direct networks like HyperX [6], Flattened Butterﬂy [8],
DragonFly [8]. etc.,, and even randomly connected topolo-
gies have been proposed in Jellyﬁsh [9].
Many current proposals use ECMP-based techniques,
which are inadequate to utilize all paths, or to dynamically
load balance trafﬁc. Routing proposals for these networks are
limited to shortest path routing (or K-shortest path routing
with Jellyﬁsh) and end up underutilizing the network, more
so in the presence of failures. While DAL routing [6] allows
deroutes, it is limited to HyperX topologies. In contrast,
Dahu [19] proposes a topology-independent, deployable so-
lution for non-minimal routing that eliminates routing loops,
routes around failures, and achieves high network utilization.
Hedera [10] and MicroTE [13] propose a centralized con-
troller to schedule long lived ﬂows on globally optimal paths.
However they operate on longer time scales and scaling them
to large networks with many ﬂows is challenging. Tech-
niques like Hedera, which select a path for a ﬂow based on
current network conditions, suffer from a common problem:
when network conditions change over time the selected path
may no longer be the optimal one. While DevoFlow [14]
improves the scalability through switch hardware changes,
it does not support non-minimal routing or dynamic hashing.
Dahu can co-exist with such techniques to better handle
congestion at ﬁner time scales.
MPTCP [11] proposes a host based approach for multi-
path load balancing by splitting a ﬂow into multiple sub
ﬂows and modulating how much data is sent over different
subﬂows based on congestion. However, as a transport
protocol, it does not have control over the network paths
taken by subﬂows. Dahu [19] exposes the path diversity
to MPTCP and enables MPTCP to efﬁciently utilize the
non-shortest paths in a direct connect network. There have
also been proposals that employ variants of switch-local per-
packet trafﬁc splitting [20].
Trafﬁc engineering has been well studied in the context
of wide area networks. TeXCP [21], and REPLEX [22] split
ﬂows on different paths based on load. However, their long
control loops make them inapplicable in the data center
context that requires faster response times to deal with
short ﬂows and dynamic trafﬁc changes. FLARE [7] exploits
the inherent burstiness in TCP ﬂows to schedule “ﬂowlets”
(bursts of packets) on different paths to reduce extensive
packet reordering.
III. PRIORITIZED ADAPTIVE MAX-MIN FAIR
BANDWIDTH ALLOCATION
While ECMP is often used to achieve high link utilization,
max-min fairness is widely used to allocate network band-
width fairly among multiple applications. However, today’s
data centers usually host diverse applications, which have
various priorities (e.g., mission critical applications) and
service level agreements (e.g., high throughput). It is unclear
how to adopt ECMP forwarding and max-min fairness in
the presence of such requirements. We propose Prioritized
Max-Min Fair Multiple Path forwarding (PMP) to tackle this
challenge. In the following, we ﬁrst formalize the problem,
and then present how PMP works.
A. Problem Formalization
Consider a data center network with K-ary fat-tree topol-
ogy as shown in Fig.1, composed of a set of core switches
Sc, a set of aggregation switches Sa, a set of edge switches
Se, and a set of hosts H. Each switch has k-port. There are k
pods. Each pod contains k/2 aggregation switches and k/2
edge switches. In each pod, each k-port edge switch is di-
rectly connected to k/2 hosts and k/2 aggregation switches.
The ith port of each core switch si ∈ Sc(i ∈ [1, (k/2)2])
is connected to pod i [2]. We assume all links (e.g., L1 in
Fig.1) have the same bandwidth for both uplink (e.g., Lu
1)
and downlink (e.g., Ld
1) connections.
Recent study [17] showed that less than 25% of the core
links have been highly utilized while packet losses and
congestions may still often occur. In this paper, we only
focus on inter-pod network trafﬁc that requires bandwidth
from core links. We denote all links between aggregation
and core layers as a set Lac, all links between edge and
aggregation layers as a set Lea, and all links between
application server and edge layers as a set Lse. Generally,
199
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

Core
Aggregation
Edge
Pod1
Pod2
Pod3
Pod4
S11
S22
P1
P2
P3
P4
L1
L2
L3
L4
L5
L6
App 
Servers
Figure 1: Fat tree topology.
in a network with K-ary fat-tree topology , there are k paths
between any two hosts from different pods.
A network task Ti is speciﬁed by a source and destination
hosts (e.g., S11 and S22) and the expected trafﬁc volume. We
also consider each task with different priority level wi. Here,
wi ∈ [1, m] with the lowest and highest priority levels as 1
and m, respectively. A network scheduler modular (also sim-
ply referred to as scheduler) on an SDN controller needs to
decide how to allocate available bandwidth to maximally sat-
isfy the application requirements. We deﬁne a valid Network
Path Assignment PAi for a given task Ti is a set of paths
and their corresponding allocated bandwidths connecting the
source to the destination (e.g., a subset of {P1, P2, P3, P4}),
in which each path consists of a list of directional links (e.g.,
P1 = {Lu
1, Lu
2, Lu
3, Ld
4, Ld
5, Ld
6}) connecting the source to
the destination hosts. Here, Lu
1, Ld
6 ∈ Lse; Lu
2, Ld
5 ∈ Lea;
Lu
3, Ld
4 ∈ Lac.
There is a variety of applications on a data center net-
work, which have different service requirements regarding
throughput, packet loss, and delay. For our analysis, we char-
acterize the applications’ requirements through their priority
levels, which can be the output of some utility function.
Priorities can offer a basis for providing application and
business oriented service to users with diverse requirements.
We consider a model where the weight associated with the
different priority classes is user-deﬁnable and static. Users
can freely deﬁne the priority of their trafﬁc, but are charged
accordingly by the network. We aim to study the bandwidth-
sharing properties of this priority scheme. Given a set of
network tasks T = {Ti} (i ≥ 1) and their corresponding
priority levels K = {Ki}, we consider a Network Path
Assignment problem is to ﬁnd a set of path assignment
PA = {PAi} to satisfy the condition of Prioritized Max-
Min Fairness.
Deﬁnition 1. Prioritized Max-Min Fairness A feasible
path assignment PAx is “prioritized max-min fair” if and
only if an increase of any path bandwidth within the domain
of feasible bandwidth allocations must be at the cost of
a decrease of some already less allocated bandwidth from
the tasks with the same or higher priority level. Formally,
for any other feasible bandwidth allocation scheme PAy, if
BW(PAy
Ti) > BW(PAx
Ti), then it decreases the allocated
bandwidth of some other path with the same or higher prior-
ity level. Here, BW(PAy
Ti) is the total allocated bandwidth
for the task Ti in the bandwidth allocation scheme PAy.
Deﬁnition 2. Saturated Path A path Pi is saturated if at
least one bottleneck link Lj exists on the path Pi. A link
is bottlenecked if the total assigned bandwidth on this link
from the given tasks is more than or equal to the maximum
bandwidth of the link. Formally, a bottleneck link is the one
that ∑
i BWTi(Lj) ≥ BWmax(Lj).
B. The Algorithm of Multi-Level Progressive Filling
The network tasks can be dynamically and continuously
generated, and submitted to the scheduler. In PMP, the
scheduler can periodically query all network switches to
collect current link utilizations. Once a new task list re-
ceived, the scheduler will use a practical approach called
“progressive ﬁlling” [23] provisioning available bandwidth
that results in a prioritized max-min fair allocation following
the priority order from the highest to the lowest priority
level. The idea is shown in Fig. 2: The scheduler starts
with all provisioned bandwidth equal to 0 and increases
all bandwidths together at the same pace for the tasks
with the same priority level until one or several saturated
paths are found. The bandwidth for the corresponding tasks
that use these paths are not increased any more and the
scheduler continue increasing the bandwidth for other tasks
on the same priority level. All the tasks that are stopped
have a saturated path. The algorithm continues until it is
not possible to increase the bandwidth for the tasks at
certain priority level. Then, the algorithm moves to the next
200
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

Input: A list of tasks {Ti}; current link utilization U(Lj)
Output: Path assignment PA with PAi for each task Ti
1: Sort {Ti} based on their priority levels Ki
2: Start from the highest priority W = m /*m is the
highest priority level*/
3: for all Ti! = ∅ (PL(Ti) = W) do
4:
/*The function PL() returns the priority level of a
given task*/
5:
Find all paths for each task Ti
6:
Assign a unit bandwidth (UB) to the least utilized
path for each task /*we choose UB = 100Kbps*/
7:
PAi ← {Ti, {Pi}}
8:
PA ← PA ∪ {PAi}
9:
if A path P is saturated and P ∈ APL(Ti) then
10:
APL(Ti) ← APL(Ti) − P
11:
end if
12:
if APL(Ti) == ∅ then
13:
Remove Ti
14:
end if
15:
if ({Ti} == ∅) and (W > 1) then
16:
W = m − 1
17:
end if
18: end for
19: return PA
Figure 2: Multi-Level Progressive Filling Algorithm
priority level and repeats the same bandwidth provisioning
operations until all tasks are assigned to some paths. The
algorithm terminates because the total paths and tasks are
ﬁnite. When the algorithm terminates all tasks have been
served at some time and thus have a saturated path. By
Deﬁnition 1 the allocation is max-min fair for the tasks at
the same priority level.
IV. EVALUATION
A. Data Center Network Trafﬁc Pattern
Several recent studies [16]–[18] have been conducted in
various data center networks to understand network trafﬁc
patterns. The studied data center networks include university
campus, private enterprise data centers, and cloud data
centers running Web services, customer-facing applications,
and intensive Map-Reduce jobs. The studies have shown
some interesting facts: (1) The majority of the trafﬁc in
data center networks is TCP ﬂows. (2) Most of the server
generated trafﬁc in the cloud data centers stays within a rack,
while the opposite is true for campus data centers. (3) At
the edge and aggregation layers, link utilizations are fairly
low and show little variation. In contrast, link utilizations
at the core network are high with signiﬁcant variations over
the course of a day. (4) In some data centers, a small but
signiﬁcant fraction of core links appear to be persistently
congested, but there is enough spare capacity in the core to
alleviate congestion. (5) Losses on the links that are lightly
utilized on the average can be attributed to the bursty nature
of the underlying applications run within the data centers.
B. Methodology and Metrics
In our experiments, we simulate a data center with a fat-
tree topology. We implemented PMP based on RipL [28],
a Python library that simpliﬁes the creation of data center
code, such as OpenFlow network controllers, simulations,
or Mininet topologies. We compared PMP scheduler with a
commonly used randomization based scheduling method.
In our evaluation, we use three different priority policies
for a mixture of trafﬁc patterns: (1) high priority for long
TCP ﬂows with the total data size between 1MB and
100MB; (2) high priority for short TCP ﬂows with the total
data size between 10KB and 1MB; (3) high priority for
random selected ﬂows including both short and long ones
referred to as mixed TCP ﬂows.
We focus on two performance metrics: (1) Link Utiliza-
tion that demonstrates how effectively the scheduler utilizes
the network bandwidth. Intuitively, when there are high
bandwidth demands from user applications, the overall link
and path utilizations should be kept in high. (2) Network
throughput that shows how efﬁciently the network serves
different applications.
C. Link Utilization
We created 16 test scenarios to evaluate PMP with dif-
ferent inter-pod trafﬁc patterns. We ran 5 tests for each
scenarios. In all test scenarios, the test trafﬁc traversed all
edge, aggregation, and core links. The results of multiple
test runs from the same test scenario present similar results.
In the following, we only report the result of one test run
for each test scenario that created trafﬁc between two pods
in both directions. Under the same three different priority
policies, Fig.3(a)∼(c) shows the overall path utilization;
Fig.4(a)∼(c) shows the aggregation link utilizations; and
Fig.5(a)∼(c) shows the core link utilizations. Comparing to
the randomization based scheduler, our algorithm 2 achieves
high utilization on path level, aggregation and core link lev-
els by: (1) dynamically observing all link utilization status,
and (2) progressively ﬁlling the jobs of the same priority
with the available bandwidth with the max-min fairness. The
average gain on utilization is approximately improved from
59% to 66%. Note that with the increase of link utilization,
idle bandwidth can be effectively utilized by demanding
network applications, which can correspondingly improve
their performance by reducing their network latencies.
D. Network Throughput
Once the overall utilization can be increased, we expect
that the overall application throughput should also be im-
proved. The experiment results presented some interesting
201
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41
Link Utilization
All Link Sequence Number
Max-Min
Random
High Priority for Long Flows
(a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41
Link Utilization
All Link Sequence Number
Max-Min
Random
High Priority for Small Flows
(b)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41
Link Utilization
All Link Sequence Number
Max-Min
Random
High Priority for Mixed Flows
(c)
Figure 3: Path utilization with high priority for (a) long ﬂows (b)short ﬂows (c)mixed ﬂows
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11131517192123252729313335373941
Utilization
Aggregation Link Sequence Number
Max-Min
Random
High Priority for Long Flows
(a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11131517192123252729313335373941
Utilization
Aggregation Link Sequence Number
Max-Min
Random
High Priority for Short Flows
(b)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11131517192123252729313335373941
Utilization
Aggregation Link Sequence Number
Max-Min
Random
High Priority for Mixed Flows
(c)
Figure 4: Aggregation link utilization with high priority for (a) long ﬂows (b)short ﬂows (c)mixed ﬂows
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11131517192123252729313335373941
Utilization
Core Link Sequence Number
Max-Min
Random
High Priority for Long Flows
(a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11131517192123252729313335373941
Utilization
Core Link Sequence Number
Max-Min
Random
High Priority for Short Flows
(b)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11131517192123252729313335373941
Utilization
Core Link Sequence Number
Max-Min
Random
High Priority for Mixed Flows
(c)
Figure 5: Core link utilization with high priority for (a) long ﬂows (b)short ﬂows (c)mixed ﬂows
results as shown in Fig.6(a)∼(c). When we emulate more re-
alistic application scenarios, where short and long TCP ﬂows
are randomly mixed together, our PMP scheduler obviously
outperforms the performance of the random scheduler with
about 10-12% improvement. In the scenario of the different
policies favoring either short or long ﬂows, our scheduler
adopts max-min fairness, and thus, the average throughput
has been improved from 2.52Mbps in the random scheduler
and to 3.46Mbps in the max-min scheduler.
V. CONCLUSION
The role of the data center network is becoming ever more
crucial today, which is evolving into the integrated platform
for next-generation data centers. Because it is pervasive
and scalable, the data center network is developing into a
foundation across which information, application services
and all data center resources, including servers, storage
are shared, provisioned, and accessed. Modern data cen-
ter networks commonly adopt multi-rooted tree topologies.
ECMP is often used to achieve high link utilization and
improve network throughput. Meanwhile, max-min fairness
is widely used to allocate network bandwidth fairly among
multiple applications. However, today’s data centers usually
host diverse applications, which have various priorities (e.g.,
mission critical applications) and service level agreements
(e.g., high throughput). It is unclear how to adopt ECMP
forwarding and max-min fairness in the presence of such
requirements. We propose Prioritized Max-Min Fair Multi-
202
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

0
1
2
3
4
5
6
7
8
9
10
1
3
5
7
9
11 13 15 17 19 21 23 25 27 29
Throughput (Mbps)
Application Flow ID
Max-Min
Random
High Priority for Short Flows
(a)
0
1
2
3
4
5
6
7
8
9
10
1
3
5
7
9
11 13 15 17 19 21 23 25 27 29
Throughput (Mbps)
Application Flow ID
Max-Min
Random
High Priority for Long Flows
(b)
0
1
2
3
4
5
6
7
8
9
10
1
3
5
7
9
11 13 15 17 19 21 23 25 27 29
Throughput (Mbps)
Application Flow ID
Max-Min
Random
High Priority for Mixed Flows
(c)
Figure 6: Throughput with high priority for (a) long ﬂows (b)short ﬂows (c)mixed ﬂows
ple Path forwarding (PMP) to tackle this challenge. PMP can
prioritize current demands and allocate available bandwidth
accordingly. Our performance evaluation results show that
PMP can improve application throughput 10-12% on average
and increase overall link utilization especially when the total
demanded bandwidth close or even exceed the bisectional
bandwidth of a data center network.
REFERENCES
[1] M. Al-Fares, A. Loukissas, and A. Vahdat, A Scalable, Com-
modity Data Center Network Architecture. In SIGCOMM,
2008
[2] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri,
S. Radhakrishnan, V. Subramanya, and A. Vahdat, PortLand:
A scalable fault-tolerant layer 2 data center network fabric. In
SIGCOMM, 2009
[3] R. Perlman, Rbridges: Transparent routing. In INFOCOMM,
2004.
[4] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim,
P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta, VL2: A
Scalable And Flexible Data Center Network. In Proc. of ACM
SIGCOMM, 2009.
[5] V. Liu, D. Halperin, A. Krishnamurthy, and T. Anderson, F10:
A Fault-Tolerant Engineered Network. In NSDI, 2013
[6] J. H. Ahn, N. Binkert, A. Davis, M. McLaren, and R. S.
Schreiber, HyperX: Topology, Routing, and Packaging of Ef-
ﬁcient Large-Scale Networks. In Proc. of SC, 2009.
[7] J. Kim, W. J. Dally, and D. Abts, Flattened butterﬂy: A Cost-
efﬁcient Topology for High-radix networks. ISCA, 2007.
[8] J. Kim, W. J. Dally, S. Scott, and D. Abts, Technology-Driven,
Highly-Scalable Dragonﬂy Topology. In Proc. of ISCA, 2008
[9] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey, Jellyﬁsh:
Networking Data Centers Randomly. In NSDI, 2012.
[10] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat, Hedera: Dynamic Flow Scheduling for Data Center
Networks. In NSDI, 2010.
[11] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, and M. Sridharan, Data center TCP
(DCTCP). In Proc. of ACM SIGCOMM, 2010.
[12] T. Benson, A. Akella, and D. A. Maltz, Network Trafﬁc
Characteristics of Data Centers in the Wild. IMC, 2010.
[13] T. Benson, A. Anand, A. Akella, and M. Zhang, MicroTE:
Fine Grained Trafﬁc Engineering for Data Centers. In Proc. of
ACM CoNEXT, 2011.
[14] A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula,
P. Sharma, and S. Banerjee, DevoFlow: Scaling Flow Man-
agement for High-Performance Networks. In Proc. of ACM
SIGCOMM, 2011.
[15] OpenFlow
Switch
Speciﬁcation
(Version
1.1),
http://www.openﬂow.org/documents/openﬂow-spec-v1.1.0.pdf.
(retrieved: Sept. 2013)
[16] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and R.
Chaiken, The Nature of Data Center Trafﬁc: Measurements
and Analysis. In IMC, 2009.
[17] T. Benson, A. Akella, and D. A. Maltz, Network Trafﬁc
Characteristics of Data Centers in the Wild. In IMC, 2010.
[18] G. Wang, D. G. Andersen, M. Kaminsky, M. Kozuch, T. S. E.
Ng, K. Papagiannaki, M. Glick, and L. Mummert, Your data
center is a router: The case for reconﬁgurable optical circuit
switched paths. In Hotnets, 2009.
[19] S. Radhakrishnan, M. Tewari, R. Kapoor, G. Porter, and A.
Vahdat, Dahu: Commodity Switches for Direct Connect Data
Center Networks. In Proceedings of the 9th ACM/IEEE Sym-
posium on Architectures for Networking and Communications
Systems (ANCS13), October 2013
[20] D. Zats, T. Das, P. Mohan, D. Borthakur, and R. Katz,
DeTail: Reducing the Flow Completion Time Tail in Datacenter
Networks. In SIGCOMM, 2012.
[21] S. Kandula, D. Katabi, B. Davie, and A. Charny, Walking
the Tightrope: Responsive Yet Stable Trafﬁc Engineering. In
SIGCOMM, 2005.
[22] S. Fischer, N. Kammenhuber, and A. Feldmann, REPLEX:
Dynamic Trafﬁc Engineering Based on Wardrop Routing Poli-
cies. In CoNEXT, 2006.
[23] A. Ghodsi, M. Zaharia, S. Shenker and I. Stoica, Choosy:
Max-Min Fair Sharing for Datacenter Jobs with Constraints,
EuroSys 2013.
[24] A. Ghodsi, M. Zaharia, B. Hindman, A. Konwinski, I. Stoica,
and S. Shenker, Dominant resource fairness: Fair allocation of
multiple resource types. In NSDI, 2011
[25] Hadoop Capacity Scheduler. hadoop.apache.org/common/
docs/r0.20.2/capacity scheduler.html. (retrieved: Sept. 2013)
[26] M. Zaharia, D. Borthakur, J. Sen Sarma, K. Elmeleegy, S.
Shenker, and I. Stoica, Delay Scheduling: A Simple Technique
for Achieving Locality and Fairness in Cluster Scheduling. In
EuroSys 10, 2010.
[27] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha,
Sharing the data center network. In NSDI, pages 2323, 2011.
[28] M. Casado, D. Erickson, I. A. Ganichev, R. Grifﬁth, B. Heller,
N. Mckeown, D. Moon, T. Koponen, S. Shenker, and K. Zariﬁs,
Ripcord: A modular platform for data center networking. UC,
Berkeley, Tech. Rep. UCB/EECS-2010-93
203
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

