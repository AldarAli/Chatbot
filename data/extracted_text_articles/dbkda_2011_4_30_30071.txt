New fuzzy multi-class method to train SVM classiﬁer
Taouﬁk Guernine
D´epartement d’Informatique, Facult´e de Sciences
Universit´e de Sherbrooke
Sherbrooke, Canada
Taouﬁk.guernine@usherbrooke.ca
Kacem Zeroual
D´epartement d’Informatique, Facult´e de Sciences
Universit´e de Sherbrooke
Sherbrooke, Canada
Kacem.zeroual@usherbrooke.ca
Abstract—In this paper we present a new classiﬁcation
method based on Support Vector Machine (SVM) to treat
multi-class problems. In the context of multi-class problems,
we have to separate large number of classes. SVM becomes
an important machine learning tool to handle multi-class
problems. Usually, SVM classiﬁers are implemented to deal
with binary classiﬁcation problems. In order to handle multi-
class problems, we present a new method that builds dynam-
ically a hierarchical structure from training data. Our multi-
class method is based on three main concepts : Hierarchical
classiﬁcation, Fuzzy logic and SVM. We combine multiple
binary SVMs to solve multi-class problems. The proposed
method divides the original problem into sub-problems in order
to reduce its complexity.
Keywords-Classiﬁcation; SVM; Fuzzy logic;
I. INTRODUCTION
Solving multi-class problems with high performance is a
challenging problem because there is an important increasing
processing of data in databases. Until now, multi-class
problems remain among the primary worry in the ﬁeld of
classiﬁcation. Furthermore, the manual classiﬁcation is not
able to keep up with the growth of data. An automatic
classiﬁcation becomes necessary. Many machine learning
methods and statistical techniques has been proposed :
Decision trees [1], Nearest neighbor classiﬁers [2], Bayesian
models [3] and Support Vector Machine [4].
Unlike the other classiﬁers, SVM classiﬁers ﬁnd an opti-
mal hyperplane maximizing the marge between two classes.
Generally, SVM is used for binary classiﬁcation but its
extension to multi-class problems remains an open research
topic [5]. There are two techniques for extending SVM to
multi-class problems. The ﬁrst technique consists in resolv-
ing optimization problems where the whole training data set
is used [6]. This technique requires huge time to train all
the data set. The second technique consists in constructing
binary classiﬁers from the root until leaves [7]. The original
problem is subdivided into simple binary sub-problems.
Each sub-problem contains a small portion of data and is
less complex than the original problem. In this paper, we
are interested in subdividing the original problem into binary
sub-problems. We propose a new classiﬁcation method based
on SVM to treat multi-class problems. The proposed method
uses a fuzzy hierarchical structure to extract relationships
between objects. It introduces the transitive closure measure
to discover fuzzy similarity between objects. Training data
set of SVM obtained a priori by the transitive closure Min-
Max assures discriminating between positive and negative
classes. Introducing membership values extracted from tran-
sitive closure matrix to SVM optimization problem allows
high performance.
The remainder of this paper is organized as follows. In
section II, we provide an overview of related works. In
section III, we give a brief review of SVM. In section IV,
we describe the fuzzy hierarchical classiﬁcation method. In
section V, we present our experimental results. Our future
research works are presented in section VI.
II. RELATED WORKS
The most important issue in multi-class problems is the
existence of confusion classes [8]. The hierarchical structure
is among techniques used to solve the confusion classes. The
multi-class problems based on SVM is mainly related to
hierarchical multi-class pattern recognition problems. Most
of recent works used hierarchical structure to address the
classiﬁcation task. In [9], they proposed a new classiﬁcation
algorithm based on a hierarchical structure. The algorithm
consists of the following stages : (i) generating category
information tree (ii) hierarchical feature propagation (iii)
feature selection of category information and (iv) single path
traversal. The proposed hierarchical classiﬁcation system
allows adding new categories as required, organizing the
web pages into a tree structure and classifying web pages
by searching through only one path of the tree structure.
In [10], authors explore a hierarchical classiﬁcation to
classify heterogenous collections of the web content. They
used hierarchical structure in order to distinguish a second
level category from other categories within the same top
level. They introduced SVM at each level to obtain a
hierarchy. In [11], authors added fuzzy membership values
to each input data and reformulate the SVM optimization
problem. The membership values make more contribution
in the classiﬁcation process. The proposed fuzzy SVM can
solve different kinds of multi-class problems. In [12], the
fuzzy set theory is introduced in the classifying module.
The authors proposed a One-against-all fuzzy SVM (OAA-
77
DBKDA 2011 : The Third International Conference on Advances in Databases, Knowledge, and Data Applications
Copyright (c) IARIA, 2011              ISBN:978-1-61208-115-1

FSVM) classiﬁer to implement a multi-class classiﬁcation
system. The empirical results obtained by the proposed
system show that OAA-FSVM method performs better than
OAA-SVM method.
III. SUPPORT VECTOR MACHINE
In this section we give a brief review of Support Vector
Machine. We present respectively binary and multi-class
classiﬁcation.
A. Binary classiﬁcation
Generally, SVM classiﬁers are designed to solve binary
classiﬁcation problem [13]. It consists in minimizing the
empirical classiﬁcation error and ﬁnding optimal hyperplane
with large margin [14]. Suppose a data set (xi, yi) : (i =
1, ..., n), where xi corresponds to the attribute set for the
ith element. Let yi ∈ {−1, +1} be a labelled class. The
optimal hyperplane can be found by minimizing the margin
w in equation III-A :
(P) =

Min 1
2∥w∥2
yi(wxi + b) ≥ 1 : i ∈ 1, n : ∀x ∈ Rn
Where w and b are parameters of the model. The solution
of optimization problem is given by Lagrangian :
Lp = 1
2∥w∥2 −
n
X
i=1
αi[yi(wxi + b) − 1]
(1)
Where αi are called the Lagrange multiplier. We can
simplify the problem given by equation 1 as follows :
LD =
n
X
i=1
αi − 1
2
n
X
ij
αiαjyiyjxixj
(2)
In several cases, linear solutions could not solve the
optimization problem. In this situation, a non linear separator
is required. The formulation of the problem is given bellow :
f(x) =



f(x) = wxi + b ≥ (1 − ξi)
if
yi = 1
wxi + b ≤ (1 − ξi)
if
yi = −1
ξi > 0, ∀i
The objective function will change as follows :
f(w) = 1
2∥w∥2 + C
n
X
i=1
(ξi)κ
(3)
Where C and ξi are speciﬁed by the user and represent
the penalty of mis-classiﬁcation. The Lagrangian is written
as follows :
Lp = 1
2∥w∥2+C
n
X
i=1
(ξi)−
n
X
i=1
αi[yi(wxi+b)−1+ξi]−
n
X
i=1
µiξi
(4)
We can however, simplify the problem given by equation 4
as follows :
LD =
n
X
i=1
αi − 1
2
n
X
ij
αiαjyiyjxixj
(5)
The problem given by equation 5 becomes identical to the
linear discrimination problem given by equation 2.
B. Multi-class classiﬁcation
In order to treat multi-class problems by constructing bi-
nary problems, several methods have been proposed. The are
three methods developed to deal with multi-class problems
using SVM classiﬁer at each node :
1) One-against-one method: To resolve multi-class prob-
lem, one-against-one method requires one classiﬁer SV Mij
for each pair of classes (i, j). It builds [n(n−1)/2] classiﬁers
for n-class classiﬁcation problem. During the test phase, the
test set is evaluated by all SV Mij.
Let E = (xi, yi)i=1,n, be a training data set, where
xi ∈
Rn and yi ∈ {1, 2, ..., k}. For k class problem, the
optimization problem to construct SV Mij that separate two
classes Ci and Cj is given as follows :
(P) =













min
wi,bi,ξi
1
2(wij)T wij + C
X
t
ξij
t
(wij)T φ(xt) + bij ≥ 1 − ξij
t : yj = 1
(wij)T φ(xt) + bij ≤ −1 + ξij
t : yj ̸= 1
ξij
t ≥ 0 : j = 1, ..., k.
(6)
To determine the decision function (fij(x) = Sgn(wijx+
bij)) which separates classes Ci and Cj, we use Max-Win
strategy :
Sgn(x) =

+1 : x > 0
−1 : x ≤ 0
x ∈

Ci : fij(x) = 1
Cj : fij(x) = −1
The process of Max-Win strategy is given as follows :
• For each xi :
fij(x) =
k
X
j̸=i,j=1
Sgn(fij(x))
(7)
• The class of xi is obtained by :
arg max
i:1,...,k fi(x)
(8)
78
DBKDA 2011 : The Third International Conference on Advances in Databases, Knowledge, and Data Applications
Copyright (c) IARIA, 2011              ISBN:978-1-61208-115-1

2) One-against-all method: The one-against-all method
is simple and efﬁcient. It requires n classiﬁers SV Mi : (i =
1, n), for n-class classiﬁcation problem. During the test
phase, the test set is evaluated by the SV Mi. SV Mi which
shows highest decision value is chosen.
Let E = {(x1, y1j), (x2, y2j)..., (xl, ylj)} be a training
data set, where xi(i=1,l) represents the ith observation and
yij(j=1,k) represents the jth class of the ith observation. For
k class problem, the formulation of the jth SVM is given
as follows :
(P) =















min
wi,bi,ξi
1
2(wj)T wj + C
l
X
j=1
ξj
i
(wj)T φ(xi) + bj ≥ 1 − ξj
i : yj = j
(wj)T φ(xi) + bj ≤ −1 + ξj
i : yj ̸= j
ξj
i ≥ 0 : i = 1, l; j = 1, k
(9)
We solve the problem in (9) and obtain k decision
functions :
(P) =







(w1)T φ(xi) + b1,
·
·
(wk)T φ(xi) + bk
(10)
The class of xi is obtained as follows :
Class(x) = arg
max
(i=1,...,l)((wj)T φ(xi) + bj).
(11)
3) Directed
Acyclic
Graph
SVM
(DAGSVM):
The
DAGSVM method constructs also [n(n − 1)/2] classiﬁers
SV Mij. During the test phase, it creates a list of all
candidates classes. At each test, the class that obtained
negative score is eliminated from the list.
IV. SVM FUZZY HIERARCHICAL CLASSIFICATION
METHOD
The new method we propose in this paper supplies an
alternative to the three methods : One-against-one, One-
against-all and DAGSVM. Our method is based on a fuzzy
hierarchical classiﬁcation technique we developed for the
speciﬁcation software reuse [15]. It provides also advantages
to treat hierarchical multi-class problems. The method we
propose consists of three steps : (A) Training data set
compression by K-Mean (B) Fuzzy hierarchical classiﬁca-
tion building and (C) Introducing membership function for
training SVM.
A. Training data set compression
Several works focused on reducing the number of training
data set of SVM [16]. The ﬁrst step in our method is
compressing training data set of SVM. We apply basic K-
Mean algorithm in order to regroup similar data in the
same cluster and reduce time spent in training data set of
SVM. The goal of this step is expressed by an objective
function that depends on the proximities of the points to
their centroids. To assign each object to the closest centroid,
we apply equation 12 :
gi = 1
mi
X
x∈Ci
x
(12)
Where gi represents the centroid of cluster Ci, mi repre-
sents the number of objects in the ith cluster and x is an
observation.
In order to measure the quality of clustering, we use the
sum of the squared error (SSE), given by :
SSE =
k
X
i=1
X
x∈Ci
dist(gi, x)2
(13)
Where k represents the number of clusters.
B. Fuzzy hierarchical classiﬁcation building
1) Similarity measure: The notion of a distance between
x and y has long been used in many contexts as a measure of
similarity or dissimilarity between a set’s elements. In this
work, we deﬁne a relative generalized Hamming distance δ
to compute similarity between clusters which is deﬁned by :
δ(ςi, ςj) = 1
n × d(ςi, ςj) = 1
n
n
X
i=1
|µςi(xi) − µςj(xi)| (14)
Where n represents the number of clusters and d(ςi, ςj) is
the Hamming distance between clusters ςi and ςj.
Since µςi(xi) and µςj(xi) ∈ [0,1], ∀ i = 1, n ⇒
0 ≤ δ(ςi, ςj) ≤ 1.
(15)
2) Fuzzy subsets: Let K be a universe of discourse, A ⊂
K, and K = {xi}. An element x of K belonging to A is
deﬁned as : x ∈ A. Let µA(x) be a characteristic function
whose value indicates whether x belongs to A according to :
µA(x) =
 1
if
x ∈ A
0
if
x /∈ A
(16)
The characteristic function µA(x) takes its values in the
interval [0,1]. It is deﬁned as a mapping :
µA(x) : A → {0, 1}
(17)
The fuzzy logic is based on partial membership function.
An object is belonging to one or more than a class in the
same case. Let A be a sub set, deﬁned by its membership
function µA. The membership function µA(x) of an object
x used in fuzzy set theory is deﬁned as follows : An object
x does not belong to class C if the membership function
µC(x) = 0, belongs a little to class C if µC(x) border to
0, belongs enough to class C if µC(x) does not border to 0
nor to 1, belongs strongly to class C if µC(x) border to 1
and belongs completely to class C if µC(x) = 1.
79
DBKDA 2011 : The Third International Conference on Advances in Databases, Knowledge, and Data Applications
Copyright (c) IARIA, 2011              ISBN:978-1-61208-115-1

3) Fuzzy operators: Let A and B be fuzzy subsets of
universe K. The fuzzy operators on the fuzzy subset A and
B of K are given as follows :
• Intersection operator (AND)
The membership function used by [17] to deﬁne the
set (A ∩ B), is given by the minimum of membership
functions µA and µB as follows :
∀x ∈ X : µA∩B(x) = min{µA(x), µB(x)}.
(18)
• Union operator (OR)
The membership function deﬁnes the set (A ∪ B) is
given by the maximum of membership functions µA
and µB as follows :
∀x ∈ X : µA∪B(x) = max{µA(x), µB(x)}.
(19)
4) Transitive closure of a fuzzy relation: To extract am-
biguous relationships between objects, we used the theory of
fuzzy sets [17]. It is deﬁned by their memberships function.
In our work, we used Min-Max transitivity relation to ﬁnd
fuzzy relationships between objects :
∀ x, y, z ∈ K× K × K :
µR(x, z) ≤ Miny[Max(µR(x, y), µR(y, z))]
(20)
We compute the transitive closure Min-Max given by equa-
tion 20 until we obtain transitive closure Γ equals to
Γ=Rκ−1=Rκ at κ levels. This equality assures the existence
of a hierarchy. This relation gives the transitive distance Min-
Max which locates the level of each objects and ﬁnd the
short link between these objects. Let Ci = {xi1, xi2..., xin}
and Cj = {x1j, x2j, ..., xnj} be two clusters obtained by
the similarity matrix. The fuzzy shortest link between two
clusters is given as follows : Γij = ∨[(xi1 ∧ x1j), (xi2 ∧
x2j), ..., (xin ∧ xnj)].
C. Introducing membership function for training SVM
In this step, we train fuzzy SVM at each node of the
hierarchy to subdivide the original problem into binary sub-
problems.
Let M be a set of classes C = {c1, c2, ..., ck}, where k is
the number of clusters obtained by K-Mean in the ﬁrst step
(k ≤ n).
First, we compute the average transitive closure of all
classes from the transitive closure matrix by the equation :
¯X = 1
n
n
X
i=1
n
X
j=1
Γij
(21)
Where n represents the number of values of Γij in
transitive closure matrix and Γij represents fuzzy similarity
value between Ci and Cj that are obtained by transitive
closure.
Second, we compute the average of transitive closures of
each class according to the following equation :
υi = 1
k
n
X
j=1
Γij, j : 1, n
(22)
The fuzzy membership υi, which is the average similarity
between Ci and the rest (k−1) of classes, is extracted from
the transitive closure matrix.
Suppose that E = {(C1, y1, υ1), ..., (Ck, yk, υk)} a set of
training data with associated membership, where Ci ∈ Rk,
yi = {−1, +1} and 0 ≤ υi ≤ 1.
In our work and in order to handle multi-class with high
precision, we introduced fuzzy membership function in the
training SVM step. Each row i of the transitive closure
matrix deﬁnes the membership between class i and the
others classes. To construct positive and negative classes,
we compute for each class Ci the membership value υi. At
each node of the hierarchy, the problem can be deﬁned as
follows :
SV M =
 {Ci} ∪ SV M +
ij : υi > ¯X
{Ci} ∪ SV M −
ij : υi ≤ ¯X
(23)
The optimization problem given by our fuzzy SVM in (23)
is given as follows :









1
2wT · w + C
k
X
i=1
υiξi
yi(w · xi + b) > 1 − υiξi
υiξi ≥ 0 : i = 1, ..., k
(24)
Where C, ξi represent the penalties of mis-classiﬁcation
and υiξi represents error of classiﬁcation with different
weights.
Using the Lagrangian multiplier, the problem is given as
follows :











Max : w(α) =
k
X
i=1
αi − 1
2
k
X
i=1
k
X
j=1
αiαjyiyjK(xi, xj)
Subject :
k
X
i=1
yixi = 0, 0 ≤ αi ≤ υiC : i = 1, k
(25)
We repeat the process at each node of the hierarchy until
reaching leaves containing only one class. Consequently, we
obtain a descendant hierarchical classiﬁcation represented by
a succession of classes. Each class contains similar objects.
The advantage of our method is that training data set of
SVM obtained a priori by the transitive closure assures
discriminating between positive and negative classes.
80
DBKDA 2011 : The Third International Conference on Advances in Databases, Knowledge, and Data Applications
Copyright (c) IARIA, 2011              ISBN:978-1-61208-115-1

V. EXPERIMENTAL RESULTS
A. Data
In this paper, we compareded the performance of our
method with those of the methods : One-against-one, One-
against-all and DAGSVM. We used three different prob-
lems available in [18]. The ﬁrst problem is Iris database
which contains 150 records grouped equally in three classes.
The second problem is Glass database which contains 214
records distributed in six classes. The third problem is Letter
database which contains 16000 records distributed in twenty
six classes. We give detail of the three problems in Table I.
Table I
PROBLEM DETAIL
Problem
Data
Class
Attributes
Iris
150
3
4
Glass
214
6
9
Letter
16000
26
16
B. Experimental
• Compression step
To show how the compression step is usefull, we conducted
two experiments. The ﬁrst experiment consists in applying
K-Mean to training data set step with the original data set
replaced by clusters centroid. In the second experiment, we
apply our method,without calling K-Mean. Table II shows
results given by the two experiments.
Table II
COMPRESSION STEP INFLUENCE ON SVM-CHF PERFORMANCE
With K-Mean
Without K-Mean
Data
#
Training
Accuracy
#
Training
Accuracy
SVM
time
SVM
time
Iris
2
0.021
98.00
3
0.05
98.23
Glass
4
0.05
77.63
6
11
78.10
Lettre
21
110
98.35
24
255
98.45
In the ﬁrst step, our method performs better in number of
SVMs and training time criteria. Using K-Mean algorithm
reduced automatically the number of SVMs and cost training
time. In the second step we used the original data set
wich allows slightly better accuracy compared with accuracy
result obtained in the ﬁrst step. Since the two ﬁrst criteria in
the classiﬁcation domain are very important, we introduced
the K-Mean algorithm in the process of our method.
• Kernel function
In order to choose the best kernel function of each problem,
we tested different kernel functions : Polynomial (d=2,3,...,8)
and RBF (γ = 0.1, 0.2, ..., 1). We choose only results where
SVM performs well. The results are given in Table III.
For Iris (k=3) and Glass (k=6) problems, polynomial
function gives best results. For Letter (k=26) problem, RBF
function performs best. In our case, polynomial function
Table III
ACCURACY OBTAINED BY POLYNOMIAL AND RBF KERNEL
FUNCTIONS
Data
SV MP oly:d
SV MRB:γ
2
4
6
8
0.1
0.2
0.4
1.0
Iris
0.98
0.95
0.94
0.94
0.97
0.96
0.90
0.96
Glass
0.66
0.77
0.76
0.69
0.66
0.69
0.72
0.65
Letter
0.54
0.67
0.88
0.87
0.78
0.93
0.98
0.92
performs better when the number of classes is small. High
accuracy is obtained when C = 210, C = 211 and C = 211
for Iris, Glass and Letter problems respectively. The pro-
posed method proved high performance for the three prob-
lems (Iris : 98.00%, Glass : 77.63% and Letter : 98.35%).
• Accuracy comparison
We use accuracy criterion to evaluate our results with results
obtained by methods : One-against-one, One-against-all and
DAGSVM. To obtain high accuracy, we tested our method
with different values of C : (22,..., 212). Accuracy is obtained
from confusion matrix. Our accuracy comparison results
are compared with : One-against-one, One-against-all and
DAGSVM (see Table IV). The proposed method proved high
performance for the three problems.
Table IV
ACCURACY COMPARISON
Problem
One-
One-
DAGSVM
Our
gainst-
gainst-
Proposed
one
rest
Method
Iris
97.33
96.67
97.36
98.00
Glass
71.49
71.96
72.22
77.63
Letter
97.98
97.88
96.73
98.35
• The fuzzy membership function inﬂuences on the
classiﬁer performance
In this section, we tested the inﬂuence of the fuzzy mem-
bership function on the classiﬁer performance. We varied υi
in the range from 0.1 to 0.8. Figure 1 shows that the high
performance is obtained when υi is equal to 0.31, 0.22 and
0.32 for problems Iris, Glass and Letter respectively. These
fuzzy values are extracted from the transitive closure matrix.
The values υi are introduced to train SVM. Choosing υi
from transitive closure matrix allows our method to perform
better.
VI. CONCLUSION
In this paper, we proposed a new fuzzy SVM hierar-
chical method to handle multi-class problems. The fuzzy
hierarchical structure consists in subdividing the original
problem into simple binary problems. Our method takes
its advantage from using fuzzy hierarchical classiﬁcation
and fuzzy Support Vector Machine. Furthermore, it has the
advantage of using only values from the similarity matrix
81
DBKDA 2011 : The Third International Conference on Advances in Databases, Knowledge, and Data Applications
Copyright (c) IARIA, 2011              ISBN:978-1-61208-115-1

Figure 1.
Fuzzy membership inﬂuence.
for the SVM training rather than using values randomly.
Similarity matrix assures a priori separate classes in the
hierarchy.
Unlike other classiﬁcation methods, our method requires a
number less than or equal to (k−1) SVM classiﬁers from the
root until leaves (see Table II). The number of SVM required
reduce automatically the cost of training SVM time.
In this work, we ﬁnd that introducing the membership
values extracted from transitive closure matrix to SVM
optimization problem gives a high accuracy.
Our future works consists in adapting our method to
video sequencing problem in order to extract fuzzy relations
between objects. Moreover, we will create a new dynamic
kernel function to handle automatically classiﬁcation pro-
cess.
REFERENCES
[1] S. Weiss and C. Apte and F. Damerau and D. Johnson and
F. Oles and H. Goetz, Maximizing text mining performance.
IEEE Intelligent Systems : 2–8, 1999.
[2] X. Xie, A validity measure for fuzzy clustering.
IEEE
Transactions Pattern analysis and Machine Intelligence : 841-
847, 1991.
[3] D. Koller and M. Sahami, Hierarchically classifying documents
using very few words.
Proceedings of the 14th International
Conference on Machine Learning, Nashville : 171–178, 1997.
[4] T. Joachims, Text Categorization with Support Vector Machines
: Learning with many relevant features.
Proceedings of the
10th European conference on machine learning : 1998.
[5] Y. Guermeur and A. Elisseeff and H. Paugam, A new multiclass
SVM based on a uniform convergence result.
IJCNN : 183-
188, 2000.
[6] C. Hsu and C. Lin, A comparison of methods for multi-class
support vector machines. IEEE Trans Neural Networks : 415–
425, 2002.
[7] G. Zhang, Support Vector Machine with Huffman tree architec-
ture for multi-class classiﬁcation.
Lecture Notes in computer
Science : 24–33, 2005.
[8] F. Schwenker, Hierarchical Support Vector Machines for Multi-
Class Pattern Recognition.
The 4th International Conference
on knowledge-Based Intelligent Engineering Systems, Allied
Technologies, Brighton, UK : 2000.
[9] X. PENG and B. CHOI, Automatic Web Page Classiﬁcation
in a Dynamic and Hierarchical Way.
IEEE International
Conference on Data Mining : 386–393, 2002.
[10] S. Dumais and H. Chen, Hierarchical Classiﬁcation of Web
Content.
Proceedings of the 23rd annual international ACM
SIGIR conference on Research and development in information
retrieval, Athens, Greece : 256–263, 2000.
[11] L. Chun-Fu and W. Sheng-De, Fuzzy Support Vector Ma-
chines.
IEEE Transactions On Neural Networks : 464–471,
2000.
[12] T. Wang and H. Chiang. Fuzzy support vector machine for
multi-class text categorization.
Information Processing and
Management : 914–929, 2007.
[13] V. Vapnik, Statistical Learning Theory.
John Wiley, Sons :
1998.
[14] S. Tuffery, Data Mining et
statistique decisionnelle
:
L’intelligence des donnes.
Technip. Paris : 2007.
[15] J. Nkoghe and K. Zeroual and W. Shengrui, Speciﬁcation
Reuse : A Fuzzy Approach.
International Joint Conference on
Artiﬁcial Intelligence : 1995.
[16] B. Xiaojuan and S. Qilong and C. Hao and T. Xuyan,
Compression method based on training data set.
Journal of
Systems Engineering and Electronics : 198–201, 2008.
[17] L. Zadeh, Fuzzy Sets.
Journal of Information and Control :
338-353, 1965.
[18] C. L. blake and C. J. Merz, UCI repository of machine learn-
ing databases.
Available at ftp//ftp.ics.uci.edu/pub/machine-
learning-databases, 1998.
82
DBKDA 2011 : The Third International Conference on Advances in Databases, Knowledge, and Data Applications
Copyright (c) IARIA, 2011              ISBN:978-1-61208-115-1

