Smart Shopping Cart Learning Agents Modeling And Evaluation 
 
Dilyana Budakova 
Department Of Computer Systems and 
Technologies 
Technical University of Sofia, Branch 
Plovdiv, Plovdiv, Bulgaria 
email: dilyana_budakova@tu-
plovdiv.bg  
Lyudmil Dakovski   
Department of Communication & 
Computer Technologies 
European Polytechnic University 
Pernik, Bulgaria 
email: lyudmil.dakovski@epu.bg  
Veselka Petrova-Dimitrova  
Department of Computer Systems and 
Technologies  
Technical University of Sofia, Branch 
Plovdiv, Plovdiv, Bulgaria  
email: vesi_s_petrova@yahoo.com  
 
Abstract—The paper describes the design, implementation and 
user evaluation of utility and goal-based intelligent learning 
agents for smart shopping cart. In keeping user’s shopping list, 
they guide visitors through the shops and the goods in the 
shopping center or according to new promotions in the shops, 
respectively. It is envisaged that concrete implementation of 
the shopping agents will be running on each shopping cart in 
the shopping centers. The k-d decision tree and reinforcement-
learning algorithm are used for agents learning. The task 
environment is partially observable, cooperative, deterministic, 
and a multi - agent environment, with some stochastic and 
uncertainty elements. It incorporates text-to-speech and speech 
recognizing technology, Bluetooth low energy technology, 
holographic technology, picture exchange communication 
system. Machine learning techniques are used for agents 
modeling. This kind of intelligent system enables people with 
different communication capabilities to navigate in large 
buildings and in particular to shop in the large shopping 
centers and maximize user comfort. Some initial user opinions 
of the shopping cart agents are presented.  
Keywords – Smart shopping cart virtual learning agent; 
machine learning; reinforcement learning; decision tree; 
Ambient intelligence; holographic technology; beacon-based 
technology; assistive technologies. 
I. 
 INTRODUCTION 
In big and unfamiliar indoor spaces, such as shopping 
centers, airports, stadiums, hotels, office buildings, people 
may have difficulties with finding the desired destination. 
Many categories of people – the elderly, the children, the 
people with visual or hearing impairment, with difficulties in 
communication 
etc. 
– 
need 
specialized 
ways 
of 
communication [2][20][21]. This paper presents modeling, 
implementation and user evaluation of two intelligent 
learning agents for smart shopping cart. They guide visitors 
through the shops and the goods in the shopping center 
according to user’s shopping list or according to new 
promotions in the shops, respectively. It is envisaged that 
concrete implementation of the shopping agents will be 
running on each shopping cart in the shopping centers. 
The task environment incorporates text-to-speech and 
speech recognizing technology, Bluetooth low energy 
technology, holographic technology, information kiosks, 
picture exchange communication system.  
The rest of the paper is structured as it follows: in Section 
II the technologies that the task environment incorporates are 
briefly discussed; in Section III the task environment 
specification, including performance measure, properties, 
environment actuators and sensors description is presented; 
the agent programs realization of the goal-based learning 
agent and utility goal-based learning agent by means of a 
decision k-d tree and reinforcement-learning are explained in 
Section IV; the degree of development of the proposed 
cognitive architecture components is explained in Section V; 
an empirical survey about the interest of end customers to the 
used technologies and  a survey about the way the customers 
perceive the two developed agents are considered in Section 
VI; in the VIIth Section a number of conclusions are drawn. 
II. 
BACKGROUND TECHNOLOGY USED FOR TASK 
ENVIRONMENT 
Beacons are used to mark the location of objects and 
navigate people in indoor spaces [10][25][26][33][34]. They 
work on the principle of lighthouses by emitting signals at 
short intervals based on Bluetooth Low Energy (BLE) 
technology. The distance to the Beacon can be defined 
depending on the signal strength [3]. In addition to emitting 
advertising or other types of announcements, it is also 
possible to locate beacons [10][26][34].  
Holograms are made of light and sound, appear in the 
around space and reply to gestures, voice and gaze 
commands [9]. A hologram can be placed and integrated in 
the real world or can tag along with user as an active part of 
user’s world helping for navigation in indoor spaces. 
Another possible solution to the problem of orientating 
people in indoor spaces is the use of embodied 
conversational information kiosks [27][31]. These systems 
use the information they have both about their own location 
and about the layout of the building and give instructions to 
the users how to find the desired place in the building.   
The information kiosks are a collection of different 
technologies such as video processing from face detection, 
speaker-independent speech recognition, array microphone 
for noise cancellation, a database system, and a dynamic 
question answering system [16][31]. 
The Picture Exchange Communication System (PECS) 
[1][30] allows people with little or no communication 
abilities to communicate using pictures. People using PECS 
12
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-705-4
COGNITIVE 2019 : The Eleventh International Conference on Advanced Cognitive Technologies and Applications

are taught to approach another person and give them a 
picture of a desired item in exchange for that item [4][5].  
Screen readers [7][11][17] and text-to-speech (TTS) 
systems [6][14] enable blind and vision impaired people to 
use computers and provide the key to education and 
employment.   
According to [13], the first step in designing an agent 
must always be to specify the task environment as fully as 
possible. That includes performance measure, environment 
actuators and sensors description. That’s why we will 
consider smart shopping problems, task environment 
specifying and shopping learning agents modeling in the 
next section.  
III. 
SPECIFYING THE TASK ENVIRONMENT 
It is envisaged that shopping agents will be implemented 
on the shopping cart. The consumers will run their cart 
following the directions given by the agents. In the future, 
the shopping agents can be implemented on a robotic 
shopping cart like an autonomous Kuka robot that can be 
controlled by gestures [8][22][23][28][29] to follow the user. 
Then, the environment will become very complex and 
similar to the environment of the automated driver. 
The modulus of the system prototype is given in Figure 
6. The task environment consists of four main blocks: input, 
output, shopping, and navigation.  
The technologies, used in the input block, are face 
detection and speech recognition. The equipment comprises 
a camera, a microphone, a keyboard, a mouse, and a touch 
screen. The general object detection algorithm consisting of 
a cascade of classifiers proposed by Viola and Jones [35] is 
used to detect faces. For video processing, C# and Intel 
OpenCV library [15] is used. 
The output block uses speech synthesis and virtual 
character visualization for giving information to the user.  
 The shopping block includes: drag and drop pictures for 
creating the shopping list (Figure 4); pictures-to-speech 
convertor;  
The navigation block includes: Beacons/iBeacons or/and 
Holograms for smart buildings, smart shopping mall 
navigation. Using of Google Beacon Platform or/and 
Microsoft HoloLens respectively is needed. 
Agent programs include goal-based learning agent and 
utility goal-based learning agent realization by means of a 
decision k-d tree and reinforcement-learning. 
A. Performance Measure  
The performance measure to which the shopping agents 
are aspired include getting to the correct shop in the 
shopping mall; getting to the new promotion in the shopping 
mall; minimizing the path when going through the shops 
from the shopping list; maximizing passenger comfort; 
maximizing purchases; and enabling people with different 
communication possibilities to navigate in big buildings and 
in particular to shop in the big shopping centers. 
B. Environment 
Any shopping agent deals with a variety of shops in the 
shopping malls; the newest promotion could be in each and 
any of the shops in a mall; the agents can recommend 
visiting the shops in a mall in various sequences. An option 
is to visit all desired shops following the shortest possible 
way. Another option is to go around the shops in accordance 
with the arrangement of the items on the shopping list. A 
third option is to go to the shops in accordance with the 
availability of sales or new promotions. The location of the 
shops in an exemplary Mall is given in Figure 1. The model 
of the environment in Figure 3 is presented in the form of a 
graph. The nodes are the shops and the edges are the 
connecting corridors.   
Figure 1.  Exemplar location of eight shops in a shopping center  
C. Actuators 
The shopping agents are visualized on the display screen. 
Only the head of an agent is modeled by means of the 
program 
Crazy 
Talk. 
Face 
animation 
includes 
synchronization of the lip movement with the pronounced 
text and expressed emotions. The agents’ faces normally 
express friendliness and calmness and when a new 
promotion or sale is announced they express excitement and 
joy. The emotions of elevation are realized through changing 
the strength and the height of the speech and by visualizing a 
model of the emotion “joy” on the face.  
D. Interaction and Sensors 
For interaction both with the intelligent agents and the 
consumers are used: Keyboard entry; Microphone; Touch 
Screen; Camera – OpenCV, Face Detecting; Natural 
Language Understanding; Speech recognizing; drag-and-
drop 
pictures, 
pictures 
to 
speach 
convertor; 
Beacons/iBeacons or/and Holograms for smart shopping 
mall navigation. 
E. Properties of a Task Environment 
The behavior of the two intelligent agents is mutually 
complementary. They aim at facilitating the user access to 
the desired commodities and increasing the number of 
purchases, made by him/her, as well as at offering 
information about promotions and sales, in which he/she is 
interested.   
The agent does not know when a new promotion or a 
new customer will appear. Therefore he/she periodically 
 
 
 
 
 
 
 
 
 
 
 
 
13
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-705-4
COGNITIVE 2019 : The Eleventh International Conference on Advanced Cognitive Technologies and Applications

checks on the site of the mall if there are files, containing 
information about new promotions or sales and reads them if 
available. Then, he/she transmits this information to the 
customers, planning to visit the corresponding shops. 
Whenever a new customer appears, the agent receives his/her 
shopping list and defines the sequence for visiting the shops 
in the mall. That’s why task environment is partially 
observable, cooperative and a multi-agent environment. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  Shop k-d decision tree 
 
 
 
 
 
 
 
Figure 3.  Presentation of the location of the shops in an exemplary mall 
by an orientated graph  
The shopping world is also deterministic with some 
stochastic elements and contains elements of uncertainty of 
the environment. The task environment is episodic and it can 
be realized either as static or as semi-dynamic environment. 
The environment can be regarded as static as the location of 
the shops in the mall is known. The agent receives the whole 
shopping list and suggests a certain path around the shops. 
Whenever there is information about a new promotion or 
sale appearing during the shopping, the agent can 
dynamically recommend a change in that shopping sequence.  
The environment can be regarded as both known and 
sequential because every next shop to visit is determined by 
the current location of the user and by the items he/she has 
pointed at as important to buy. 
IV. 
SMART SHOPPING LEARNING AGENTS MODELING 
Two software agents have been realized. The first one is 
a Utility Goal–based learning agent, while the second is a 
Goal-based learning agent.  
A. Utility Goal-Based Learning Agent 
One of the agents can be regarded as a Utility goal-based 
agent. That is because it feels happy when discovering that 
there is a promotion or a sale in a shop, in which the 
customer is interested to go.  
The utility goal-based agent uses a decision k-d tree to 
quickly find where (in which shop) the customer is located 
according to his/her coordinates. The theory of building and 
implementing a decision k-d tree is given in [18]. The 
customer is depicted in Figure 1 by means of an emoticon, 
which can be moved using the mouse and placed everywhere 
on the shown map of the shops in the shopping center. 
Another way of finding the location of the customer is by 
using estimate beacons sensors or holograms.  
The Utility goal-based agent checks if there are new files 
about promotions or sales published on the site of the 
shopping center. In case there are such files, it withdraws 
them and informs the customer about those of them, which 
are related to the shops the customer intends to visit.  
The information about promotions and sales is given to 
the customer also in the case when it can be seen from the 
shopping list that the customer has planned to visit a 
particular shop where there is a promotion or a sale.  
The 
customer 
receives 
notifications 
about 
promotions/sales when he/she goes past a beacon as well.  
 
    
  
Figure 4.  Making a shopping list by dragging and dropping pictures 
1) Decision k-d Tree Realization 
In order to build the decision tree, the location of the 
eight shops in the exemplary shopping mall, given in Figure 
1, is considered. As it is described in [18][24] all shops are 
divided first by width alone into two sets, each with an equal 
number of shops. Next each of the two sets is divided by 
heights alone. Finally, each of those four sets is divided by 
width alone, producing eight sets of just one block each. The 
shop sets are divided horizontally and vertically until only 
one block remains in each set as it is shown in Figure 2. The 
overall result is called a k-d tree, where the term k-d is used 
to emphasize that the distances are measured in k-
dimensions.  
Finding the nearest block is really just a matter of 
following a path through a decision tree that reflects the way 
the objects are divided up into sets. As the decision tree in 
Figure 2 shows, only three one–axis comparisons are 
required to guess the shop in which the user is positioned. 
In general [18], the decision tree with branching factor 
k=2 and depth d=3 will give 23= 8 leaves (shops in our task). 
Accordingly, if there are n shops (or goods, or users) to be 
identified, d will have to be large enough to ensure that 2d≥n. 
Then, the number of comparisons required, which 
corresponds to the depth of the tree, will be of the order of 
log2n.     
3 
1 
4 
5 
6 
7 
0 
2 
Y 
 
0.
 2.
 
4.
 
6.
 
 
 
 
 
1.
 
 3.
 
5.
 
7.
 
 
 
 
 
 
 
 
           X 
 
 
 
 
 
 
 
 
 
 
 
14
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-705-4
COGNITIVE 2019 : The Eleventh International Conference on Advanced Cognitive Technologies and Applications

B. Goal-Based Learning Agent 
According to [12][19][32], Reinforcement learning is a 
method of learning, by which what to do is taught, i.e., how 
to match a situation to an action, so that a numerical reward 
received as a signal, is maximized. The teacher does not 
point at the actions to be undertaken. Instead, the trainee has 
to find out those, leading to the greatest reward and try to 
realize them.  In the most interesting and challenging cases, 
not only the immediate reward could be taken into account 
when choosing an action, but also the further situations and 
the future rewards.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 5.  Message about a promotion of a new Compact Disk 
All reinforcement learning agents have explicit goals, can 
sense aspects in their environment and choose actions, which 
influence it. The agent is realized by a program, matching the 
way the agent perceives reality and the actions it undertakes.  
A reinforcement learning algorithms is used for the 
second Goal-based learning agent. The agent receives the 
shopping list from the customer (this is what the agent 
perceives) and informs the customer about the sequence of 
shops he/she can visit in order to buy all the goods needed 
(these are the actions the agent undertakes). The shortest 
possible route is suggested, in accordance with the particular 
shopping list.  
Since the goal is to visit all the shops from the shopping 
list, the particular shopping list can be regarded as a plan or a 
sequence of goals to achieve in order to fulfill the task 
completely.  
It is also possible for the agent to get the exact location of 
a customer and a particular shop to get to. The shortest 
possible path to the desired shop is suggested in this case as 
well.  
In order to realize the agent’s learning process the 
following is to be developed: Environment model; Rewards 
model; Agent’s memory model; Agent’s behavior function; 
Value of the training parameter.  
The environment model is a graph (Figure 3) of the 
different environment conditions. The nodes in the graph 
(Figure 1) are the shops in the exemplary shopping mall. The 
edges point at the shops, between which there is a transition. 
Then, this graph is presented by an adjacency matrix. The 
number of rows and columns in this matrix is equal to the 
number of shops in the mall. Zero is put in the matrix in a 
place where there is a connection between the number of a 
shop, set by a number of a row, and the number of a shop, 
given by a number of a column. Values of -1 are placed in 
the other positions of the adjacency matrix.  
The rewards model is needed to set a goal for the agent.  
Reaching every shop from the customer’s shopping list is 
such a goal. Since the agent is a goal-based one, it behavior 
can be changed by just setting a new goal, changing the 
rewards model [12].  A reward is only given when the agent 
gets to a particular shop.  
The agent’s memory is modeled by presenting it with the 
help of an M-matrix (Memory of the agent). The rows in the 
M-matrix represent the current location of the customer, 
while the columns are the shops, where he/she can go. It is 
assumed at the beginning that the agent does not have any 
knowledge and therefore all elements in the M-matrix are 
zeros.  
The rule for calculating the current location of the 
customer at the moment of choosing the next shop to visit is 
as it follows:  
M (cuurent location of the customer, chosen shop to visit 
next) = R(current location of the customer, next shop) + ϒ. 
Max[М(next shop, all possible shops where the customer 
could go from the next shop)]. 
The following is taken into account in the above formula:  
The immediate reward, obtained when the customer 
decides from the current location to go to a next shop: 
R(current position, chosen shop to go next);   
The biggest possible future reward. This is the biggest 
reward, chosen from among the rewards, which would have 
been obtained when the customer goes out of the next shop 
and enters any possible shop: Max[М(next shop, all possible 
shops where it is possible to go from the next shop)]. 
The value of the learning parameter ϒ defines the extent 
to which the agent will take into account the value of the 
future reward. The value of the learning parameter ϒ is 
within 0 to 1 (0 ≤ϒ< 1). If ϒ is closer to zero, then the agent 
will prefer to consider only the immediate reward. 
Experiments have shown that in this case it is impossible to 
teach the agent to achieve the goal. If ϒ is closer to one, then 
the agent will consider the future reward to a greater extent. 
This is the better option for successful training of the agent. 
The value of the learning parameter was experimentally 
chosen to be ϒ=0.8. At this value, the obtained weights for 
all possible actions are clearly identifiable and the process of 
training is reliable. A random initial position is chosen for 
the customer in the algorithm for training the agent. The 
following steps are realized until the target shop is reached:  
One of all possible shops is chosen, where it is possible 
to go from the current position. The shop to which the 
customer would go next is considered. For this next position 
now all the shops, to which it is possible to go further are 
considered. The value of the highest reward is taken. The 
next position is then set as a current one.  
15
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-705-4
COGNITIVE 2019 : The Eleventh International Conference on Advanced Cognitive Technologies and Applications

V. 
DEGREE OF DEVELOPMENT OF THE PROPOSED 
COGNITIVE ARCHITECTURE COMPONENTS 
The goal-based learning agent and the utility goal-based 
learning agent are fully developed. The head of each agent is 
modeled and visualized. We have used Crazy Talk 6 for 
emotion modeling. The decision k-d tree and reinforcement-
learning algorithm are completed and used for agents 
function realization. The program for creating a shopping list 
by using key combinations and drag and drop pictures is 
ready. The picture to speech converter program can 
pronounce all the existing pictures and the created shopping 
list. The agents can recognize and react to a few speech 
commands. They start communication with the users when 
detecting a face in front of themselves. A number of 
experiments are conducted with some Estimote beacons and 
a notification program [14]. The complete beacon based 
navigation system and the corresponding software are not 
ready yet, however. The holograms and holographic 
computer have not been used for now. We hope to obtain 
and use the holographic computer soon. Experiments in a 
real shopping center are planned as well.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6.  Specifying the task environment and smart shopping learning agents modeling. 
 
 
 
 
 
 
Environment 
NAVIGATION 
Beacons/iBeacons 
or/and Holograms 
for smart 
buildings, smart 
shopping mall 
navigation (using 
Google Beacon 
Platform or/and 
Microsoft 
HoloLens 
respectively) 
and Headsets)  
Android Studio/ 
Visual Studio.NET 
 
 
 
 
Camera 
OpenCV 
Face Detecting 
 
Microphone 
 
INPUT 
 
LCD display or 
Touch Screen 
 
Natural Language 
Understanding C# 
VisualStudio.NET 
SHOPPING 
Creating a shopping 
list by using key 
combinations, drag-
and-drop pictures, 
pictures to speech 
convertor. 
C# 
VisualStudio.NET 
Virtual agent 
visualization 
 
TTS Engine 
Speech synthesis 
 
OUTPUT 
 
Visualization of 
messages C# 
 
Utility goal-based learning agent 
Agent purpose: search for promotions and sales; 
defining customer’s location; informing the 
customer about promotions and sales.  
K-D decision tree for finding of the user’s 
location.  
Checking on the site of the mall for new attached 
files, 
containing 
information 
about 
new 
promotions or sale in a shop, downloading and 
reading such files if available. Informing the 
customer about a promotion or a sale in case 
he/she is in the corresponding shop or intends to 
visit it.  
Visual Studio.NET 
Goal-based learning agent 
Agent purpose: directing the customer on his/her 
way to reaching all desired shops from the 
shopping list. 
Reinforcement-learning algorithm for finding out 
the optimal path to all items on the shopping list.  
 
Model of the environment  
 
Model of the rewards  
*Algorithm for changing the reward model 
according  to the new goal set in front of the 
agent. 
*Defining the sequence of visiting the desired by 
the customer shop in the shopping center .  
Visual Studio.NET 
Agent programs 
16
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-705-4
COGNITIVE 2019 : The Eleventh International Conference on Advanced Cognitive Technologies and Applications

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7.  А survey about the interest of end customers in beacon-based services, holographic technology, PECS, TTS and Speech Synthesis technologies. 
 
Figure 8.  А survey about rediness of the user to use beacon-based services and holographic technology. 
 
 
VI. 
EMPIRICAL SURVEY 
The survey was conducted at the university. The total 
number of 115 students were offered the questionnaire. All 
of the participants were between the ages of 18 to 23 years 
old. 
A. А survey about the interest of end customers in beacon-
based services, holographic technology, PECS, TTS and 
Speech Synthesis technologies. 
To investigate people’s mindset towards the use of 
beacons, the use of holograms, drag-drop pictures, pictures 
to speech, TTS and Speech Synthesis an empirical study was 
conducted. The survey’s purpose was to explore the interest 
of end customers in beacon-based services, holographic 
technology, PECS, TTS and Speech Synthesis technologies 
and the willingness to use them. As a base we use [34] but 
append some questions about new technologies. 
With this end in view, we designed a questionnaire with 
the following tree sections. The participants were asked 
(Figure 7), whether they (1) own a smart-phone, (2) know 
Bluetooth, (3) know Bluetooth Low Energy, (4) know 
Holographic computer, (5) know Holographic technologies, 
(6) know beacons, (7) have used beacons before, (8) have 
used holograms before, (9) are  familiar with PECS, (10) are 
familiar with TTS and speech recognizing technologies. 
This helps to understand, whether consumers are aware 
of beacons. Then, they were given a short introduction of the 
beacon technology, holographic technology, PECS, TTS and 
17
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-705-4
COGNITIVE 2019 : The Eleventh International Conference on Advanced Cognitive Technologies and Applications

speech recognizing technologies, as a preparation for the 
remaining questions. Participants were asked to assess the 
usefulness of typical applications, which were based on 
already existing scenarios by using beacon-based or 
holographic realizations (Figure 8): General situations, such 
as navigation on airports, coupons in stores, information on 
exhibits in museums, etc.; Specific retail store types, e.g., 
supermarkets, outdoor stores, furniture shops.; Applications 
in a super-market, e.g., personal welcome message, 
navigation to products on the shopping list, information on 
products, special offers, and electronic payment at the 
checkout.; Applications in a stadium, e.g., offers for seat 
category upgrade, navigation to wardrobe/restrooms, special 
offers for drinks and snacks.  
Beacon-based technology and holographic technology 
are little known and the services based on them are not used 
widely yet, but the respondents declared willingness and 
readiness to use them. 
Figure 9.  A survey about the way the customers perceive the two 
developed agents and whether they consider their purpose useful  
(values 1-10) 
Five blind people aged 45-65 also took part in the survey. 
These respondents were not familiar with the described 
technologies and had not used them before. However, they 
do know and use in their daily routine Internet, Skype, 
smartphones, e-mail, all TTS programs and desktop reading 
programs. They showed great enthusiasm and willingness to 
get acquainted with beacon-based services and holographic 
services for navigation in buildings. 
B. А survey about the way the customers perceive the two 
developed agents and whether they consider their 
purpose useful.  
The capabilities of the two agents were demonstrated in 
front of the students. The idea of Smart Shopping and Smart 
Shopping Cart Agents was presented. Then, the students 
were asked to  evaluate usefulness of the two agents, to 
compare  their  functionality and  to consider the services of 
which  agent prefer; to say their opinion about shopping with 
Shopping Cart Smart Agents. Some of the questions were: 
Would you use a shopping cart with Intelligent Virtual 
Agents installed on it; is it useful for you to be informed by 
Smart Shopping Cart Agents (SSCA) about the latest 
promotion in the shops you visit; is it useful for you if SSCA 
explain to you how to get to a given shop in the mall; do you 
think 
that 
shopping 
is 
more 
comfortable 
when 
communicating with SSCA; do you think the sales will go up 
as a result of the communication between the customers and 
SSCA during shopping.  
It can be seen from Figure 9 that, the customers would 
use SSCA and they think the agents will be useful and their 
presence would make the shopping practice more 
comfortable. The utility goal-based learning agent that search 
for promotions and sales is the preferred one. 
VII. CONCLUSION 
The paper describes the design and implementation of an 
intelligent Smart Shopping Cart Learning Agents prototype 
and their environment. The system differs from other 
intelligent systems by the combination of machine learning 
techniques, beacon-based navigation and/or hologram-based 
navigation in the mall, the integration of Picture Exchange 
Communication System in it and by its language 
understanding and speech synthesis capabilities, drag-and-
drop techniques and keyboard button combinations enabled 
access.  
The task environment is partially observable, cooperative 
and a multi-agent environment. The shopping world is 
deterministic with some stochastic and uncertainty elements. 
The task environment is episodic and can be realized either 
as static or as semi-dynamic. 
The utility goal-based agent uses a decision k-d tree to 
quickly find where (in which shop) the customer is located 
according to his/her coordinates. It getting to the new 
promotion in the shopping mall according to user‘s shopping 
list and inform them.  
Reinforcement learning algorithm is used for the other 
Goal-based learning agent. The agent gets the shopping list 
from the customer and informs the customer about the 
sequence in which he/she can visit the shops to buy all 
needed goods. 
The performance measure to which the shopping agents 
are aspired includes getting to the correct shop in the 
shopping mall; getting to the new promotion in the shopping 
mall according to user‘s shopping list; minimizing the path 
when going through the shops from the shopping list; 
maximizing customer comfort; maximizing purchases; and 
enabling people with different communication capabilities to 
18
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-705-4
COGNITIVE 2019 : The Eleventh International Conference on Advanced Cognitive Technologies and Applications

navigate in big buildings and in particular to shop in big 
shopping centers. 
The empirical survey conducted with a limited number of 
users showed their positive mindset for using such Smart 
Shopping Cart Learning Agents in indoor spaces. The utility 
goal-based learning agent that search and informs for 
promotions and sales is the preferred one. In the future work, 
it is intended to develop other commercial agents, such as 
those who will be familiar with users shopping habits. 
ACKNOWLEDGMENT 
The authors gratefully acknowledge the financial support 
provided within the Ministry of Education and Science of 
Bulgaria Research Fund Project FNI DN17/11. 
REFERENCES 
[1] A. Bondy and L. Frost, PECS - Picture Exchange 
Communication System. [Online] https://pecsusa.com/apps/ 
[retrieved: 03, 2019] 
[2] Y. J. Chang, S. M. Peng, T. Y. Wang, S. F. Chen, Y. R. Chen, 
and H. C. Chen, “Autonomous indoor wayfinding for 
individuals 
with 
cognitive 
impairments,” 
Journal 
of 
NeuroEngineering and Rehabilitation, 7(1), pp 1– 13, 2010 
[3] Google Beacon Platform. [Online]. [retrieved: 03, 2019] 
https://www.youtube.com/watch?v=0QeY9FueMow  
[4] National Autism Resources Inc.[Online]. [retrieved: 03, 2019] 
https://www.nationalautismresources.com/the-picture-
exchange-communication-system-pecs/  
[5] What is PECS. [Online]. [retrieved: 03, 2019]. https://pecs-
unitedkingdom.com/pecs/  
[6] Innoetics TTS Reader Female Bulgarian Voice IRINA. 
[Online]. https://www.innoetics.com/ [retrieved: 03, 2019] 
[7] JAWS. 
[Online]. 
[retrieved: 
03, 
2019]. 
http://www.freedomscientific.com/Products/Blindness/JAWS  
[8] Kuka 
robot. 
[Online]. 
[retrieved: 
03, 
2019]. 
https://cyberbotics.com/doc/guide/robots  
[9] Microsoft 
HoloLens 
holographic 
computer. 
[Online]. 
hhttps://developer.microsoft.com/en-us/windows/mixed-
reality/ [retrieved: 03, 2019] 
[10] My Dream Companion project – YGA. [Online]. http://www. 
yga.org.tr/en/my-dream-companion [retrieved: 03, 2019] 
[11] NVDA. [Online]. https://www.nvaccess.org/ [retr.: 03, 2019], 
[12] R. S. Sutton and A. G. Barto, Reinforcement Learning: An 
Introduction, The MIT Press, Cambridge, Massachusetts, 
London, England, 2014. [Online]. [retrieved: 03, 2019]. 
http://incompleteideas.net/book/bookdraft2017nov5.pdf  
[13] S. Russell and P. Norvig, “Artificial Intelligence A Modern 
Approach”, Prentice Hall, Third Edition, 2010, ISBN-13 978-
0-13-604259-4, ISBN-10 0-13-604259-7 
[14] Estimote Beacons. [Online]. https://estimote.com/ [ 03, 2019], 
[15] The Intel Open Source Computer Vision Library, vol. 2006: 
[Online]. https://software.intel.com, [retrieved: 03, 2019], 
[16] Tomorrow‘s digital signage, today with 3d holographic 
kiosks, Intel IoT Digital kiosks, Solution brief, [Online], 
https://d1io3yog0oux5.cloudfront.net/_362a9c6b4a3713a1070
0da32acd8a3ad/provision/db/255/661/pdf/Intel+Provision+3
D+Brief+2016.pdf, [retrieved 03, 2019] 
[17] Vocalizer 
Android/Dariya/Bulgarian 
voice. 
[Online]. 
https://play.google.com/store/apps/details?id=es.codefactory.
vocalizertts&hl=bg [retrieved:03,2019] 
[18] P. H. Winston, “Artificial Intelligence,” Addison-Wesley 
Publishing Company ISBN-13: 978-0201533774 ISBN-10 
[19] A. Gosavi, “Reinforcement Learning: A Tutorial Survey and 
Recent Advances,” INFORMS Journal on Computing 21(2), 
pp. 178-192, 2009 
[20] V. Kulyukin, C. Gharpure, J. Nicholson, and G. Osborne, 
“Robot-assisted wayfinding for the visually impaired in 
structured indoor environments,” Autonomous Robots, 21(1), 
pp. 29–41, 2006. 
[21] L. Niua and Y. Song, “A schema for extraction of indoor 
pedestrian navigation grid network from floor plans,” In The 
International Archives of the Photogrammetry, Remote 
Sensing and Spatial Information Sciences, volume XLI-B4, 
Prague, Czech Republic, pp. 325-330, 2016. 
[22] N. Ç. Kılıboz and U. Güdükbay, “A hand gesture recognition 
technique for human–computer interaction,” Journal of Visual 
Communication and Image Representation, Elsevier, pp 97-
104,Volume 28, pp. 97-104, April 2015 
[23] N. G. Shakev, S. A. Ahmed, A. V. Topalov, V. L. Popov, and 
K. B. Shiev, “Autonomous Flight Control and Precise 
Gestural Positioning of a Small Quadrotor, Learning Systems: 
From Theory to Practice,” Springer, pp. 179-197, 2018. | 
[24] Y. Y. Song and Y. Lu, “Decision tree methods: applications 
for classification and prediction,” Shanghai Arch Psychiatry. 
2015 
Apr 
25;27(2):130-5. 
doi: 
10.11919/j.issn.1002-
0829.215044, PMID: 26120265 
[25] Y. Zhuang, J. Yang, Y. Li, L. Qi, and N. El-Sheimy, 
“Smartphone-based indoor localization with Bluetooth low 
energy beacons,” Sensors, April 2016. 16(5), pp. 596, doi: 
10.3390/s16050596 
[26] D. Ahmetovic, et al. “Navcog: A navigational cognitive 
assistant for the blind,” In International Conference on 
Human Computer Interaction with Mobile Devices and 
Services. ACM, pp. 90-99, 2016 
[27] J. Cassell, et al. “MACK: Media lab Autonomous 
Conversational Kiosk,” Imagina’02, Monte Carlo, 2002 vol. 
2, 
12–15. 
[Online] 
Available 
at: 
http://alumni.media.mit.edu/∼tstocky/pubs/Cassell.Stocky 
Imagina02.pdf [retrieved: 03, 2019] 
[28] E. Coupeté, et al. “New Challenges for Human-Robot 
Collaboration in an Industrial Context: Acceptability and 
Natural 
Collaboration,” 
Fifth 
workshop 
"Towards 
a 
Framework for Joint Action", IEEE RO-MAN, pp. 1-4, 2016.  
[29] S. R. Fletcher and P. Webb, “Industrial robot ethics: facing 
the challenges of human-robot collaboration in future 
manufacturing systems,” A world with robots: international 
conference on robot ethics: ICRE 2015”, 2017, pр. 159-169. 
[30] C. Lord, M. Rutter, P. C. Dilavore, and S. Risi, “ADOS - 
Autism 
Diagnostic 
Observation 
Schedule,” 
Western 
Psychological Services, 2002. 
[31] L. McCauley and S. D’Mello, “MIKI: A speech enabled 
intelligent kiosk,” IVA 2006, LNAI 4133, pp.132-144, 2006 
[32] R. R. Torrado, P. Bontrager, J. Togelius, J. Liu, D. Perez-
Liebana, “Deep Reinforcement Learning for General Video 
Game AI,” IEEE Conference on Computatonal Intelligence 
and Games, CIG. 2018-August, 10.1109/CIG.2018.8490422 
[33]  A. Ch. Seyed, N. Vinod, and S. Kaushik, “IBeaconMap: 
Automated Indoor Space Representation for Beacon-Based 
Wayfinding,” 
Human-Computer 
Interaction 
arXiv: 
1802.05735v1 [cs.HC], USA, 2018 
[34] A. Thamm, J. Anke, S. Haugk, and D. Radic, “Towards the 
Omni-Channel: Beacon-based Services in Retail”, BIS 2016, 
Leipzig, Germany, July 6-8, pp. 181-192, 2016 
[35] P. Viola and M. Jones, “Rapid object detection using a 
boosted cascade of simple features,” CVPR,pp. I-511- I-518,2001
 
19
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-705-4
COGNITIVE 2019 : The Eleventh International Conference on Advanced Cognitive Technologies and Applications

