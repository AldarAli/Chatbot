Parallelization on Heterogeneous Multicore and Multi-GPU Systems of the Fast
Multipole Method for the Helmholtz Equation using a Runtime System
Cyril Bordage
CEA/CESTA
Le Barp, France
INRIA
Bordeaux, France
cyril.bordage@inria.fr
Abstract—The Fast Multipole Method (FMM) is considered
as one of the top ten algorithms of the 20th century. The FMM
can speed up solving of electromagnetic scattering problems.
With N being the number of unknowns, the complexity usually
O(N 2) becomes O(N log N) allowing a problem with hundreds
of millions of complex unknowns to be solved. The FMM
applied in our context has a serious drawback: the parallel
version is not very scalable. In this paper, we present a new
approach in order to overcome this limit. We use StarPU,
a runtime system for heterogeneous multicore architectures.
Thus, our aim is to have good efﬁciency on a cluster with
hundreds of CPUs, and GPUs. Much work have been done on
parallelization with advanced distribution techniques but never
with such a runtime system. StarPU is very useful, especially for
the multi-level algorithm on a hybrid machine. At present, we
have developed a multi-core and a GPU version. The techniques
for distributing and grouping the data are detailed in this paper.
The ﬁrst results of the strategy used are promising.
Keywords-Fast multipole method (FMM); Helmholtz equation;
heterogeneous architecture; parallel algorithm.
I. INTRODUCTION
The main aim is the simulation of the electromagnetic
behavior of 3D complex objects in the frequency domain.
For that, we use standard numerical methods such as Bound-
ary Integral Equations [1], [2] based on a classical Finite
Element approximation of surface Integral Equations such
as EFIE and CFIE formulations [3].
These formulations lead to a linear system with a full
matrix, which is complex non Hermitian but symmetric. It
is solved by an iterative method, which has a complexity
of O(N 2), with N being the number of unknowns The
complexity comes from the matrix-vector products com-
puted at each iteration. The Fast Multipole Method (FMM)
[4] is able to reduce the complexity of these matrix-vector
products, and so of the global problem, to O(N log(N)) [5].
In this paper, we will study the FMM only in the context of
electromagnetic scattering problems, with the kernel from
the Helmholtz equation.
With modern parallel architectures, the parallelization of
the FMM is essential, if we want to solve very large prob-
lems, which need a lot of memory. The different paralleliza-
tions are not efﬁcient on distributed memory architectures
and unfortunately, architectures nowadays are becoming
more complex, by integrating accelerators like GPUs. With
these new architectures, load balancing is more complicated
and calculations have to be ﬁtted.
This paper is organized as follows. In Section II, we
outline the FMM. In Section III, we brieﬂy describe the
different strategies in the parallelization for distributing
the computations. Our approach and its justiﬁcation are
explained in Section IV. Finally, in Section V, we present
some results.
II. THE FMM
The FMM was introduced by Greengard and Rokhlin in
1987 [4]. In the 90s, the method was applied to electromag-
netism by Rokhlin [6] and Chew [5] in its diagonal version.
We will present brieﬂy the FMM algebraically from [7],[8].
A good analytic presentation can be found in [9] or [10].
A. Principle
The FMM computes the matrix-vector product:
⃗v = G.⃗u
(1)
with: Gi,j = G(|xi − xj|).
In our context, the Helmholtz equation, the Green function
G, is deﬁned by:
G(|xi − xj|) =
eik|xi−xj|
4π|xi − xj|
The FMM is based on a space partitioning. First, a par-
titioning P of the points is created, based on a geometrical
criterion. The partitioning is made up of boxes. If B is a
box of the partition, we deﬁne:
 ⃗uB = (ui)xi∈B
GBt,Bs = (Gxi,xj)(xi,xj)∈(Bt×Bs)
(2)
With the Gegenbauer theorem [11], we have an approxi-
mate factorization of GBt,Bs if Bt and Bs are not neighbours,
which means that they do not share any vertex.
90
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-237-0
ADVCOMP 2012 : The Sixth International Conference on Advanced Engineering Computing and Applications in Sciences

GBt,Bs ≃

The father vector has to be interpolated, it is done by the
multiplication with the interpolation matrix I. These two
operations are merged in the downward pass.
We also have to fetch the values from the higher levels for
the disaggregation step. This operation is the upward pass.
In conclusion, the algorithm differs from the SL-FMM
in the translation step, which is replaced by a loop on the
levels of 3 steps: the downward pass, the translation and the
upward pass. We deﬁne Bl, a box on the level l, and Pl, the
partition of the level l. The highest level is level 1 and the
lowest, level L. The algorithm has ﬁve types of operations:
1) Aggregation:
⃗F BL
s = ABL
s ⃗uBL
s ,
∀BL
s ∈ PL
(10)
2) Upward pass: ∀Bl
s ∈ Pl, L − 1 ≤ l ≤ 3,
⃗F Bl
s = Il,l+1
X
Bl+1
s
⊂Bls
EBl
s,Bl+1
s
⃗F Bl+1
s
(11)
where:
• EBl,Bl+1 is a diagonal P l+1 × P l+1 matrix.
• Il,l+1 is a P l × P l+1 matrix.
3) Translation: ∀Bl
s ∈ Pl, L ≤ l ≤ 3,
⃗N Bl
T t
=
X
Bls∈Vfar(Bl
t)
TBl
t,Bl
s ⃗F Bl
s
(12)
4) Downward pass: L − 1 ≤ l ≤ 3,
⃗N Bl+1
t
=







⃗N
Bl+1
T t
if l = 3
⃗N
Bl+1
T t
+

EBl
t,Bl+1
t
⋆ 
C. With GPUs
Work have been done in the FMM, but only for Laplace,
or other non oscillatory kernels. Good efﬁciencies have been
achieved on GPUs thanks to the BLAS [19].
The important points for using a GPU in scientiﬁc appli-
cations are the consistency of the computations and enough
computations compared to the data transfers. The data have
to be well-sorted to beneﬁt from the coalescing accesses.
IV. OUR STRATEGY
Many efforts have been done on the distribution of the
computations in the last ﬁfteen years. There has also been
research on computation scheduling with tasks queues [20].
We have chosen the hierarchical distribution, and for the top
level, a simple SLFFM, which can be upgraded in a FMM-
FFT. We have decided to focus on computation scheduling
because a good scheduling is the key in modern machines,
with heterogeneous processing units. A good scheduling
depends on the machine: speed of the processing units,
bus speed, network speed, etc. The schedule has to ﬁt the
algorithm but also the machine, as the computations have
to be adapted to the processing units. Another important
point for a good scalability is to hide the communications
by computations.
A. The dynamic scheduling
Our aim is to compute the FMM on a supercomputer
with shared and distributed memory, thousands of CPUs
and GPUs. For that we use the same distribution between
the nodes as in the combination of the FMM-FFT and the
ML-FMM. In a node, we use the dynamic scheduler StarPU
[21]. It can handle the scheduling on CPUs and GPUs with
different strategies: greedy, work stealing, minimal termi-
nation time, priority, etc. It automates transfers throughout
heterogeneous machines and favours data locality. Tasks and
data dependencies must be declared and StarPU does the
rest.
The strategy, which has been considered, is the minimal
termination time. It takes a task execution model and data
transfer model into account to know where a task will end
the soonest. The models can be provided for StarPU or it
can build them.
B. Efﬁcient operations
To be efﬁcient, the tasks handled by the task scheduler
should imply enough computations to hide the costs of the
scheduling and of the data transfers. This is especially the
case for the GPUs where the data transfers are more costly
and because they can do simultaneous computations.
1) The data: Many computations have to be grouped in
a same task. For that, the same strategy as in parallelization
III-B is used: groups are made with many directions for
many boxes.
To avoid calculation starvation and deadlocks, the granu-
larity must be low. But high enough to permit the GPUs to
be efﬁcient. The number and the size of the blocks should
be tuned depending on the machine and the input data. For
that, we can set the number of tasks by level, depending on
the numbers of computing workers.
2) Dependencies: All the operations except the transla-
tion and the upward pass can be done just by using the
data contained in one direction block. Thus, there is no
communication.
For the upward pass, the computation of a parent box
needs all the directions of all its children. Transposed to our
blocks, that becomes: a direction block needs all its child
box blocks with all their direction blocks. This can be done
by using one task for each direction block in the child level
but by reducing the out data. StarPU can deal with reduce
operations itself.
The translations to a box use all its far neighbours.
Consequently, for translating all the boxes in a block, the far
neighbours of all the boxes are needed. Although most of
the far neighbours are in the same block, some are external
to the block. Therefore, the translations in a block need data
from other blocks. To limit the memory accesses between
blocks, the data of the external far neighbours are copied
to the block; see Figure 3. Thus, all the translations will be
internal to the block. That adds synchronization because a
translation can occur in a block only if all its neighbours
have been computed. In fact, this is not a problem because
the translations are useful only for the downward pass.
Figure 3.
The distribution of the data
3) With GPUs: All the operations in the FMM are simple
to implement on GPUs. There are mainly matrix vector
multiplications. This is relatively efﬁcient on the GPUs when
the sizes of the matrix are not to small. We just have make
use of coalescent accesses and avoid bank conﬂicts.
93
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-237-0
ADVCOMP 2012 : The Sixth International Conference on Advanced Engineering Computing and Applications in Sciences

V. RESULTS
For the time being, the tests have only been done for the
shared memory and for the GPUs but not for the distributed
memory yet.
A. On shared memory
The test was done on a 2 Hexa-core Westmere Intel Xeon
X5650 2.67 GHz (10.664 GFlops by core) with a sphere of
2 million points at 500 MHz. The results are presented in
the Table I. The scalability is strong with 12 processors but
# cores
# blocks by level
Time (s)
Efﬁciency
1
1
80.1
100%
2
8
41.1
97%
4
8
21.4
94%
6
10
14.5
92%
8
10
10.9
92%
10
10
8.7
92%
12
10
7.3
91%
Table I
ON SHARED MEMORY WITH A 2 MILLION SPHERE
we have to do some tests on a machine with more CPUs.
When we look at the scheduling, Figure 4, we ﬁnd that the
copies of the far neighbours (called block sharing) do not
represent much time. Therefore, the cost of the parallelism
is insigniﬁcant. The other important point is the waste time
of the processors (called blocked): instead of executing a
task, a processor is waiting for a new task.
Near interactions
Aggregation
Disaggregation
Translation
Up/Downward
Blocked
Fetching input
Fetching output
CPU 01
CPU 02
CPU 03
CPU 04
CPU 05
CPU 06
CPU 07
CPU 08
CPU 09
CPU 10
CPU 11
CPU 12
Figure 4.
Gantt diagram for the execution on 12 cores
This is the result of an insufﬁcient number of tasks, but
here, if we increase the number of tasks, the cost of the par-
allelism becomes signiﬁcant and the global time increases.
Nevertheless, to avoid this kind of situation, we can favour
the tasks that create other tasks and so parallelism. These
tasks are the aggregations and the upward passes. In our
scheduler, favouring a task can be done easily by adding a
priority to this task.
B. With GPUs
The aim of our approach is to use GPUs. We have only
done preliminary tests on the same machine with 3 NVIDIA
Tesla M2070 (1 TFlops). 3 processors are dedicated to the
handling of the 3 GPUs by StarPU. The test case is a 10
meter sphere with 2 million points at 1 GHz. At present we
have only implemented the aggregation, the translations and
the near interactions.
The aggregation is computed at the same speed on the
GPUs, Figure 5. The near interactions are 7 times faster.
The translation is 50 times faster. This last result is very
good if we look at the ﬂops of the processing units.
Translation
0.1
1
10
100
1000
106
107
108
Near interactions
1
10
100
1000
105
106
107
0.1
1
106
107
108
10
Aggregation
GPU 3
CPU
Figure 5.
Execution time (µs) depending on the input size (B)
For having this efﬁciency, we must have enough directions
(at least 100). But we have planned to develop kernels for
small numbers of directions. StarPU will choose the better
kernel depending on the size of the inputs.
The scheduling, Figure 6, is good on the GPUs but poor
on CPUs. This is due to the fact that the blocks are too big
for the CPUs. But if we decrease the size, we will loose our
efﬁciency on the GPUs. That is why we want to create tasks
with different sizes.
CPU 1
CPU 2
CPU 3
CPU 4
CPU 5
CPU 6
CPU 7
CPU 8
CPU 9
GPU 1
GPU 2
GPU 3
Near interactions
Aggregation
Disaggregation
Translation
Up/Downward
Blocked
Fetching input
Fetching output
Figure 6.
Gantt diagram for the execution on 9 cpu cores and 3 gpus
Without the GPUs, the global time of the computations is
4 times greater. It is not much as compared to the accelera-
tion of the tasks separately. We hope for better results with
94
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-237-0
ADVCOMP 2012 : The Sixth International Conference on Advanced Engineering Computing and Applications in Sciences

a better block creation and with the implementation of all
the tasks on GPUs.
VI. CONCLUSION AND FUTURE WORK
In this paper, we have studied the parallelization of the
FMM, applied to scattering problems, with a dynamic sched-
uler. We have also seen how to arrange the computations
in order to permit an efﬁcient scheduling. Thus, on shared
memory, the strong scalability is good. On GPUs,our ﬁrst
tests encourage us to continue. Our work is promising; but,
to make conclusions we still have a lot of work to do: upward
pass on GPUs, improvement of the kernels, strategies for the
tasks, adaptive method, and MPI.
REFERENCES
[1] D. Jones, “Acoustic and electromagnetic waves,” Oxford/New
York, Clarendon Press/Oxford University Press, 1986, 764 p.,
vol. 1, 1986.
[2] J. Stratton, Electromagnetic theory. Wiley-IEEE Press, 2007,
vol. 33.
[3] D. L. Colton and R. Kress, Integral equation methods in
scattering theory, ser. Pure and Applied Mathematics (New
York).
New York: John Wiley & Sons Inc., 1983, a Wiley-
Interscience Publication.
[4] L. Greengard and V. Rokhlin, “A fast algorithm for particle
simulations* 1,” Journal of Computational Physics, vol. 73,
no. 2, pp. 325–348, 1987.
[5] J. Song and W. Chew, “Multilevel fast-multipole algorithm
for solving combined ﬁeld integral equations of electromag-
netic scattering,” Microwave and Optical Technology Letters,
vol. 10, no. 1, pp. 14–19, 1995.
[6] R. Coifman, V. Rokhlin, and S. Wandzura, “The fast multipole
method for the wave equation: A pedestrian prescription,”
Antennas and Propagation Magazine, IEEE, vol. 35, no. 3,
pp. 7–12, 2002.
[7] E. Darve, “The fast multipole method. I. Error analysis and
asymptotic complexity,” SIAM J. Numer. Anal., vol. 38, no. 1,
pp. 98–128 (electronic), 2000.
[8] X. Sun and N. Pitsianis, “A matrix version of the fast
multipole method,” Siam Review, vol. 43, no. 2, pp. 289–300,
2001.
[9] G. Sylvand, “La méthode multipôle rapide en électromag-
nétisme. Performances, parallélisation, applications,” 2002.
[10] W. Chew, E. Michielssen, J. Song, and J. Jin, Fast and efﬁ-
cient algorithms in computational electromagnetics.
Artech
House, Inc. Norwood, MA, USA, 2001.
[11] M. Abramowitz and I. Stegun, Handbook of Mathematical
Functions with Formulas, Graphs, and Mathematical Tables.
National Bureau of Standards Applied Mathematics Series 55.
Tenth Printing, 1972.
[12] A. Rahimian, I. Lashuk, S. Veerapaneni, A. Chandramowlish-
waran, D. Malhotra, L. Moon, R. Sampath, A. Shringarpure,
J. Vetter, R. Vuduc, D. Zorin, and G. Biros, “Petascale
direct numerical simulation of blood ﬂow on 200k cores and
heterogeneous architectures,” SC Conference, pp. 1–11, 2010.
[13] S. Velamparambil and W. Chew, “Analysis and performance
of a distributed memory multilevel fast multipole algorithm,”
Antennas and Propagation, IEEE Transactions on, vol. 53,
no. 8, pp. 2719–2727, 2005.
[14] Ö. Ergül and L. Gürel, “Hierarchical parallelization strategy
for multilevel fast multipole algorithm in computational elec-
tromagnetics,” Electron. Lett., vol. 44, pp. 3–5, Jan. 2008.
[15] C. Waltz, K. Sertel, M. Carr, B. Usner, and J. Volakis,
“Massively parallel fast multipole method solutions of large
electromagnetic scattering problems,” Antennas and Propa-
gation, IEEE Transactions on, vol. 55, no. 6, pp. 1810–1816,
2007.
[16] R. Wagner, J. Song, and W. Chew, “Monte Carlo simulation
of electromagnetic scattering from two-dimensional random
rough surfaces,” Antennas and Propagation, IEEE Transac-
tions on, vol. 45, no. 2, pp. 235–245, 2002.
[17] J. Mouriño, A. Gómez, J. Taboada, L. Landesa, J. Bér-
tolo, F. Obelleiro, and J. Rodríguez, “High scalability mul-
tipole method. Solving half billion of unknowns,” Computer
Science-Research and Development, vol. 23, no. 3, pp. 169–
175, 2009.
[18] J. Taboada, M. Araujo, J. Bertolo, L. Landesa, F. Obelleiro,
and J. Rodriguez, “Mlfma-fft parallel algorithm for the so-
lution of large-scale problems in electromagnetics,” Progress
In Electromagnetics Research, vol. 105, pp. 15–30, 2010.
[19] N. Gumerov and R. Duraiswami, “Fast multipole methods on
graphics processors,” Journal of Computational Physics, vol.
227, no. 18, pp. 8290–8313, 2008.
[20] G. Sylvand, “Performance of a parallel implementation of
the FMM for electromagnetics applications,” International
Journal for Numerical Methods in Fluids, vol. 43, no. 8, pp.
865–879, 2003.
[21] C. Augonnet, “Scheduling Tasks over Multicore machines en-
hanced with Accelerators: a Runtime System’s Perspective,”
Ph.D. dissertation, Université Bordeaux 1, 351 cours de la
Libération — 33405 TALENCE cedex, Dec. 2011.
95
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-237-0
ADVCOMP 2012 : The Sixth International Conference on Advanced Engineering Computing and Applications in Sciences

