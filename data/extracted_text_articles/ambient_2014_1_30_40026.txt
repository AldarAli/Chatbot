Increasing Stability of Mark Projections on Real World 
with Angular Velocity Sensor 
 
Kyota Aoki and Naoki Aoyagi 
Graduate school of Engineering 
Utsunomiya University 
Utsunomiya, JAPAN 
kyota@is.utsunomiya-u.ac.jp, porutakaya2@gmail.com 
 
 
Abstract— There are varieties of methods for realizing the 
augmented reality. For example, there is a method to display 
augmented information and video information on a single 
display using augmented reality markers. However, this method 
needs the users to look at the display. In order to share same 
information among multiple persons, everyone should look at 
the displays that show the same image. Projection type 
augmented reality solves this problem. Projection type 
augmented reality is the method projecting augmented 
information on the surfaces in the real world using projectors. 
A projection type augmented reality proposes augmented 
information to the real world. Every person can share the 
experience at the environment. However, the method needs 
many preparation works and enormous energy for freely 
projecting images. It can work on only limited conditions, such 
as indoors or at night. Our research solves these problems. We 
propose a method that works in a head-worn type equipment 
using the method of projection type augmented reality. It 
recognizes an object from the camera images. It projects a mark 
that carries a little information onto a recognized object. There 
is an error of the position where the mark projected when the 
equipment moves. We decrease this error using an angular 
velocity sensor. We propose the method, the implementation 
and the experiments for evaluating the performance. 
Keywords—augmented reality; projection; angular velocity 
sensor; 
I. 
 INTRODUCTION 
Augmented reality (AR) is the technology to create a “next 
generation, reality-based interface” and is moving from 
laboratories around the world into various industries and 
consumer markets [1]. In cooperative works, all members 
must carry devices that show the same information. The 
device may be a head-worn type or a hand-held type. However, 
in many cases, it is difficult to carry the devices that 
communicate among them. In the cases, the projective AR 
works well. All members may share a single device. This 
enables to make a very simple system comparing the system 
based on the personal device. 
Many projective ARs work in many environments [2]-[5]. 
In the environments, all related devices share the proper places 
and have proper calibrations. This enables to make beautiful 
projection mappings. In cooperative working environments, 
we cannot control objects’ surfaces. We cannot place the 
devices at the proper places. The relation between the 
projector and the surface projected changes time by time. For 
instance, in connecting works, the connecting terminals must 
have their surfaces’ materials. On pure shining surfaces, we 
have no way to project proper images on the surfaces. 
However, almost all surfaces are the mixture of pure shining 
and pure matt. If there is a matt feature, we can project some 
kinds of images that our eyes can catch. 
Our proposed system will not project beautiful images on 
a surface. The system will project a mark that catches a human 
attention. With the system, users may say ‘We move that one.’ 
This is the expansion of our pointing ability. This helps many 
kinds of cooperative work. 
In many kinds of cooperative works, there is no proper 
place to set up the related devices about implementing a 
projective AR system. They are cameras and projectors. In 
many cooperative works, workers move in their working 
environments. In the case, the spatial projective type of AR 
system has many problems about the occlusions with the 
workers and related materials. A head-worn type of AR 
system eases this problem. Our proposed system selects a 
head-worn type. 
Head-worn types of AR systems decrease the problems 
about the occlusions. However, there are problems about the 
delay between the image acquisition and the projection. Most 
of the projective AR systems presume no movement about 
related devices and objects. At non-projective types of AR 
systems, there is a little problem about the delay between the 
image acquisition and the image display. The created images 
are good with the delay. At projected types of AR systems, the 
delay between image acquisition and the projection influences 
the complex both of a projected image and a surface. The 
complex may be dirty to look. In some cases, the complex 
carries false information. For instance, in connecting works, 
the delay may cause the false terminal connection. In the non-
projective types of AR systems, there is no chance of these 
errors. The displayed image may have some direction errors. 
However, the displayed image shows the proper terminal for 
connecting. 
This paper proposes the stabilization of the marks 
projected on real objects with a head-worn type projective AR 
system. First, we discuss about the effects on the projected 
mark quality with translation and rotation of the proposed 
head-worn projective AR system. Then, we propose the 
compensation method about the rotation using the angler 
velocity sensor. Next, we show our implementation of the 
14
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

proposed projective AR system. Then, we show our 
experiments. And last, we conclude our work. 
II. 
TRANSLATIONS AND ROTATIONS 
The motion of a solid object is a combination of 
translations and rotations. Fig. 1 shows the three elements of 
rotations. Our head-worn projective AR system must include 
a projector and a camera. The system must include the 
projector that projects the proper image on an objective 
surface. It must include the camera that understands the 
environments around the projective AR system. The camera 
may be a normal color camera, a depth camera or some other 
types of sensors that understand the environments. The 
projector may be a normal image projector or some types of 
pointers that can show the understandable images on the 
environments. 
If there is no change about the environment and there is a 
well description of the environment, the projective AR system 
excludes the camera for understanding the environment. 
However, our proposed head-worn projective AR system 
itself the part of the environment. If there is no object that 
moves or changes, the proposed head-worn projective AR 
system must move. 
The motion of a solid object is a construction of the 3-
dimensional translation and 3-dimensional rotation. When the 
size of an interesting object is much smaller than the size of 
our human, it makes the projected mark to move onto another 
object that is a little translation in the plane that is vertical with 
the line directed to the object. 
The proposed head-worn projective AR system works 
well in the environments where the sizes of interesting objects 
are similar to the size of a human. In the environments, 
translation motions of a proposed projective AR system is not 
larger than the scale of the interesting objects. As a result, the 
effect about the translation motion is not important about the 
combination both of the projected image and the surface 
projected. 
Rotation affects the combination of a projected image and 
a surface projected. The scale of the distance between the 
surface and the projector is similar to the scale both of an 
interesting object and a human. The direction of the projector 
affects the position of the projected image. For easing this 
problem, we propose the rotation compensation method using 
an angler velocity sensor. 
III. 
PROPOSED ROTATION COMPENSATION METHOD 
A. Understanding an environment and Making a 
projection. 
Projective AR systems need to know and understand the 
environment. For cooperative works, the interesting objects 
must be known and found. Our application offers no clear 
image projected. However, the places of the interesting 
objects are important. With the images captured by a camera, 
our proposed system knows the relative positions of the 
interesting objects to the projector. 
There is some delay from taking an image to understand 
the position of the interesting objects. We need some 
processing time for finding the interesting objects in the 
captured image. There is a delay from deciding the position of 
the interesting objects to making the projection of the proper 
image. We can decrease the delays with the help of expensive 
very high speed camera, expensive processors and special 
designed projectors. The total cost of a high speed system 
easily exceeds the benefit from the usage of the proposed 
projective AR system. 
B. Processing loop of our proposed projective AR system 
Without the rotation compensation, a processing loop of a 
normal AR system has three stages. They are capturing an 
image, processing the image and displaying a projection 
image. Using normal cameras, we can have an image at every 
33 mS. We can create and display a simple mark at every 33 
mS interval. However, for finding an interesting object, we 
need 100 mS at least in our PC. The PC has Intel Core i3 
processor and 8 GB of RAM. Our object finder is based on the 
object finder included in the OpevCV distribution [2]. 
The object finder has some stages. They are a pre-
processing, a feature description and a feature matching. At 
each processing stage, we need some processing time. We 
insert a processing for compensating the rotations. The size of 
the processing code is small. It is constructed from three 
 
 
 
Figure 1. Rotations. 
 
 
 
 
Figure 2. Delay and Processing 
C
P
A
yaw
roll
pitch
Get a new 
frame
Processing 
loop
Project the 
image
Delay for 
processing
15
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

function calls. They are a call for getting an elapsed time, one 
getting the measurement of an angler velocity sensor and one 
calculating the compensation amount of the rotations. Fig. 2 
shows the overall relation between the processing and the 
delay about projections. 
C. Compensation of rotations 
We have the angular velocity using the angular velocity 
sensor. Fig. 1 shows the experimental device setup. In Fig. 1, 
‘A’ stands for an angular velocity sensor. ‘C’ does for a 
camera. ‘P’ does for a projector. The light axes both of the 
camera and the projector are parallel. We call the rotation 
around the light axis as ‘roll’. Fig. 1 shows the definitions of 
‘pitch’, ‘roll’ and ‘yaw’ in our experimental system. 
Our proposed device is a head-worn type. So, the motion 
of the device depends on the motion of a head. We can shake 
a head 10 times in 1S at most. In a normal movement, we 
shake a head one time in 1S. The processing loop needs about 
0. 3S. If we have an angular velocity at each processing loop, 
we have three measurements of angular velocity in a second. 
In normal environments, these measurements may be enough 
for observing the head motions. In the processing loop, the 
projection position error depends on the movements from the 
image acquisition to the projection. In a single measurement 
of the pair of angular velocity and time, we cannot estimate 
the movement between the image capturing and the projection. 
At least, we need to have two pairs of measurements. With 
more measurements, the estimation of movements increases 
its precision. We try to measure the pair of time and angular 
velocity six times in processing loop. However, the 
measurements of angular velocity are discrete observation. 
We must interpolate the observations. There is no information 
between two successive measurements. We estimate the 
amount of rotations using the trapezoid interpolation that has 
no assumption about the measurements.  
A = 1
2 (𝑡1 − 𝑡0)(𝑎0+𝑎1) 
(1) 
         
In (1), A is the difference of the direction between the time 
𝑡0 and the time 𝑡1. 𝑎0 is the angular velocity obtained at the 
time 𝑡0. 𝑎1 is the angular velocity obtained at the time 𝑡1. 
With the movements of the projector and the surface 
projected, the complex of the projected image and the surface 
projected loses consistency. For keeping the consistency of 
the complex, we must compensate the movements of the 
projector and the surface projected. However, without the 
recognition of the surface projected from the captured image, 
we have no information about the movement of the surface 
projected. At least, we compensate the movement of the 
projector. In processing the captured image, we update the 
image projected with the movement of the projector that is 
measured with the angular velocity sensor. 
With a captured image, we have the position of the object 
that we interest. We need to compensate the movement of the 
projector from the time when the image is captured to the time 
when the image is projected.  
When we do not have the position of the object that we 
interest from the captured image, we must project the image 
with the recorded position of the object. In the case, the 
compensation of the movements of the projector is much 
important. With the measured angular velocity, we update the 
position of the image projected continuously.  
IV. 
IMPLEMENTATIONS OF THE PROPOSED PROJECTIVE 
AR SYSTEM 
A. Component hardwares and softwares. 
The main components are a camera, a projector, an angular 
velocity sensor and a controlling computer. Fig. 3 shows the 
devices that construct the head-worn part. In Fig. 3, they are a 
projector, a camera and a Wii controller with Wii Remote Plus 
[7] as an angular velocity sensor, from left to right respectively. 
We use the camera that has a global-shutter. With motions, we 
cannot avoid motion blur. However, with a global shutter, we 
avoid the rotate distortions. The camera takes 1024x768 pixels 
images. In our experimental system, we use the small DLP 
projector for projecting a mark on the interesting object. The 
projector’s angle of projection is 24 degrees in vertical and 38 
degree in horizontal. The projected image is 1280x800 pixels. 
The weight is 0.8 Kg. For measuring the angular velocity, we 
use the Wii’s controller. The controller has an acceleration 
sensor, an angular velocity sensor and a Bluetooth transceiver. 
The controlling computer is Windows PC with a Bluetooth 
transceiver. The processor is Intel core-i3. The PC has 8 GB 
memory. 
We fix the camera, the projector and the sensor. The 
distance between the camera and the projector is smaller, the 
control of the projection is easier. We place the projector’s 
lens and the camera side by side. There is no distortion on the 
angular velocity sensor with the distance from the projector. 
Fig. 1 proposes the proposed experimental system. The 
experimental system needs the wired connections about the 
camera. It needs a power cable also. As a result, the 
experimental hardware is difficult to use in head-worn. The 
direction of the camera and the direction of the projector are 
same. 
The main software component is an object finder included 
in OpenCV distribution. The object finder is ‘find_obj_fern.’ 
We add the generation of the mark projected, the angular 
velocity readout and the position calculation.  
 
 
Figure 3. Devices constructing Head-worn part. 
16
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

B. Projection Position. 
When we have the position of the interesting object on the 
image captured from the camera, we can calculate the position 
of the mark projected on a projector’s frame. We need to use 
projective transformation for calculating the precise position 
on the image projected from the position on the image 
captured. However, there is a little distortion between two 
images. We use a simple liner transformation. The (2) is the 
expression that calculates the horizontal position on the 
projected frame. The (3) is the expression that calculates the 
vertical position on the projected frame. In (2) and (3), (x, y) 
is the position on the image projected. (X, Y) is the position 
on the image captured. c𝑥, c𝑦, d𝑥and d𝑦 are constants.  
x = c𝑥 X + d𝑥 
(2) 
y = c𝑦 Y + d𝑦 
(3) 
In (2), X is the position of the interesting object in the 
frame captured by the camera. c𝑥 and d𝑥 are constants. In our 
experimental system, c𝑥 is 1.5 and d𝑥   is -700. In vertical 
position, c𝑦 is 1.3 and d𝑦 is -64. 
C. Position compensation with angular velocity. 
We have the position that the mark is projected. With the 
measured pair of an angular velocity and a time, we 
compensate the position for projecting the mark. We have the 
amount of the change of a position with (1). In the 
find_obj_fern, there are 5 operations in the main loop. At each 
operation, we measure the pair of the time and the angular 
velocity. After each operation, we compensate the rotations of 
the projector and update the projected image. Fig. 4 shows the 
motion compensation points in a processing loop.  
The projector in our experimental system has the 38x24 
degree in the projection angle. We compensate the pitch and 
yaw. We ignore the rotate. In a head-worn system, the rotation 
around the optical axis does not change the view. We do not 
slant our head between left and right. As a result, the rotation 
is small around the light axis both of the camera and the 
projector. 
We compensate the horizontal position with dx in (4) and 
the vertical position dy in (5). 
 In (3), 𝑓𝑥 is the horizontal frame size. 𝐴𝑝 is the amount of 
pitch in degree. In (4), 𝑓𝑦 is the vertical frame size. 𝐴𝑦 is the 
amount of yaw in degree. In our experiment, 𝑓𝑥 is 1280 and 𝑓𝑦 
is 800. 
When we have the amount of an angular motion and the 
position of the interesting object on the image captured by a 
camera, we calculate the position in the image projected with 
(6) and (7).   
𝑥𝑐𝑝 = c𝑥 X + d𝑥 + 𝑓𝑥 × 𝐴𝑝/38 
(6) 
𝑦𝑐𝑝 = c𝑦 Y + d𝑦 + 𝑓𝑦 × 𝐴𝑦/24 
(7) 
In (6) and (7), (𝑥𝑐𝑝, 𝑦𝑐𝑝) is the position on the image 
projected. (X, Y) is the position of the interesting object on the 
image captured by the camera.  
 
V. 
EXPERIMENTS AND DISCUSSIONS 
A. Experiment method. 
We use a picture of an interesting object for teaching to 
our proposed projective AR system in experiments. We make 
three types of experiments based on the projector’s motions. 
They are vertical motion, horizontal motion and circular 
motion. Vertical motion is derived from the pitch. Horizontal 
motion is derived from the yaw. And, circular motion is 
derived from the combination of vertical motion and 
horizontal motion in the image. The speed of the motion spans 
from 10 degrees/S to 120 degree/S. We analyze the recorded 
dx = 𝑓𝑥 × 𝐴𝑝/38 
(4) 
 dy = 𝑓𝑦 × 𝐴𝑦/24 
(5) 
 
 
Figure 4. Motion compensation timings. 
System loop
Get a new frame
Object finding (1)
Update the 
recorded position
Get an image
Project the image
Pre-processing
Object finding (2)
Object finding (3)
17
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

images that representing the result of object recognitions. The 
recorded image shows the result of object recognition. In the 
recorded images, we can see the mark projected. In 100 
recorded images, we check the position of the projected mark 
is good or not. In the case that the mark is on the object, we 
decide that the system keeps placing the mark. In other cases, 
we decide that the system cannot keep the mark on the object. 
B. Experimental results. 
Fig. 6 shows the four examples of the captured images in 
the circular motion experiments. In the images, at the left-
upper corner, there is the interesting object. When the object 
is found, the rectangular is shown around the object found. 
The small circle is the projected mark that specifies the object. 
In the images (a) and (b) in Fig.6, the projected marks drop on 
the interesting object. In the images (c) and (d), the projected 
marks do not. In the image (d) in Fig. 6, the interesting object 
is found. In other images, the interesting object is not found. 
In those images, there is a motion blur.  
We show the experimental results in Table I. Table I 
shows the number of frames that show good results in 100 
recorded frames. Without the motion compensation, we have 
only less than 50% frames that show good results. With the 
motion compensation, every result shows more performance 
than the result without the motion compensation. 
In the experiment of the vertical motion, there is a little 
difference between two measurements of angular velocity in 
a processing loop and six measurements. The horizontal 
experiments show more performance than the vertical 
experiment does. The circular motion experiments show the 
best performance in three types of experiments. At circular 
motion experiments, the amount of motions is less than the 
horizontal motion experiments. In 2 measurements, there is no 
difference between horizontal and circular experiments. In 6 
measurements, horizontal and circular experiments show a 
difference. In circular motion experiments, the angular 
velocity has four peaks in one cycle motion. In horizontal and 
vertical motions, there are two peaks in one cycle motion. This 
difference affects the difference of the performances. 
Fig. 5 shows the success rates expressed in percentage. In 
Fig.5, we easily confirm the differences between the 
performances of all experiments. With 6 times of angular 
velocity measurements, in every type of movement, the 
performance is improved. 
VI. 
CONCLUSIONS 
We show the motion compensation method in a projective 
AR system with an angular velocity sensor. We also confirm 
the performance of the proposed experimental system. With 
an angular velocity sensor, the projected mark stays much 
more periods at the proper place. In all cases of the 
experiments, there are over 50% of all frames that keep the 
proper marks’ positions. In the experiments, the system moves 
in all frames. In real usages, the head-worn system has much 
stable time. If there are 90% stable frames, the total correct 
mark projections share 97% in all frames in circular 
movements. This enables to use the proposed stabilized 
projective AR system in real environments. 
The experimental system needs some cables for 
connecting devices. In the next step, we will make a wireless 
experimental system. 
 
ACKNOWLEDGMENT 
This work is partially supported with the cooperative 
research project with Soft CDC Corporation. 
 
REFERENCES 
[1] D. W. F. van Krevelen and R. Poelman, “A Survey of 
Augmented 
Reality 
Technologies, 
Applications 
and 
Limitations,” The International Journal of Virtual Reality, vol. 
9, 
No. 
2, 
pp.1-20, 
Sep. 
2010, 
DOI: 
10.1016/j.suronc.2011.07.002. 
[2] M. Mine, D. Rose, B. Yang, J. v. Baar and A Grundhöfer, 
"Projection-Based Augmented Reality in Disney Theme 
Parks." IEEE Computer, vol. 45, no. 7, pp. 32-40, July. 2012, 
doi: 10.1109/MC.2012.154. 
[3] Y. Tang, B. Lam, I. Stavness and S. Fels, "Kinect-based 
augmented reality projection with perspective correction." 
ACM 
SIGGRAPH 
2011, 
p.79, 
Aug. 
2011, 
doi:10.1145/2037715.2037804. 
[4] S. Nicolau, L. Soler, D. Mutter and J. Marescaux, "Augmented 
reality in laparoscopic surgical oncology." Surgical oncology, 
vol. 
20, 
no. 
3, 
pp. 
189-201, 
Sep. 
2011, 
10.1016/j.suronc.2011.07.002. 
[5] H. Hua, C. Gao, L. D. Brown, N. Ahuja, and J. P. Rolland, 
"Using a head-mounted projective display in interactive 
augmented 
environments," 
Augmented 
Reality, 
2001. 
Proceedings. IEEE and ACM International Symposium on, Oct, 
2001, pp.217-223, doi:10.1109/ISAR.2001.970540. 
[6] OpenCV, “OpenCV”, http://opencv.org/, retrieved: June, 2014. 
TABLE I. EXPERIMENTAL RESULT 
Compensation 
method 
(the number of 
measurements) 
Type of Motions 
Vertical 
motion 
Horizontal 
motion 
Circular 
motion 
No compensation 
41 
35 
36 
2 times 
57 
58 
58 
6 times 
58 
63 
66 
 
 
 
 
Figure 5. Performance improvements with the number of compensations 
in a loop.  
0
20
40
60
80
Vertical motion
Horizontal
motion
Circular motion
No compensation
2 times
6 times
18
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

[7] Nintendo, http://wii.nintendo.com/, retrieved: June, 2014. 
 
 
 
  
 
 
(a)                                                                                    (b) 
 
 
  
 
 
(c)                                                                                 (d) 
 
Figure 6. Examples of experiments. 
19
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-356-8
AMBIENT 2014 : The Fourth International Conference on Ambient Computing, Applications, Services and Technologies

