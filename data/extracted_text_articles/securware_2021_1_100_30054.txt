Cost-benefit Analysis Toward Designing Efficient Education Programs for
Household Security
N’guessan Yves-Roland Douha1, Bernard Ousmane Sane2, Masahiro Sasabe1,
Doudou Fall1, Yuzo Taenaka1, and Youki Kadobayashi1
1Division of Information Science, Nara Institute of Science and Technology, Ikoma, Japan
email: {douha.nguessan yves-roland.dn6, sasabe, doudou-f, yuzo, youki-k}@is.naist.jp
2Faculty of Science and Technology, University Cheikh Anta Diop, Dakar, Senegal
email:bernardousmane.sane@ucad.edu.sn
Abstract—The human factor is still a crucial issue in the
security chain. People who will live in a smart home might be
exposed to many cyber threats due to the remaining lack of
Internet of Things (IoT) device security. Cybersecurity awareness
training could help households to become more resilient to face
cyberattacks. However, the financial costs of training programs
and the significant amount of time needed to notice security
countermeasures could refrain many smart-home users from
engaging in cybersecurity education. In this paper, we propose
a game-theoretic approach to analyze the security investment
cost-benefit of households. Our numerical results show that the
increase of quality of services accessible in a smart home and the
security rewards for noticing security countermeasures compared
to the potential impacts of cyberattacks will increase the payoffs
of households and reinforce the security behaviors. Our results
also emphasize the urgent need to address human security toward
a more resilient smart home.
Keywords-Cost-benefit analysis; game theory; household security
awareness; smart-home security.
I. INTRODUCTION
Human factor is a recognized issue in information security
and many researchers have proposed security awareness and
training as a solution [1]–[3]. With the recent advancement of
technologies such as ubiquitous systems and human-computer
interaction, user security awareness issues are back on the
table. Households, especially those who are interested in smart
homes, a branch of ubiquitous computing that incorporates
smartness into dwellings for a better quality of life [4], might
face additional security challenges such as lack of device
management, insecure software/firmware, and poor physical
security [5]. A recent survey on cybersecurity education shows
that adults are worried about cyber threats and the safety
and security of children [6]. Given that user awareness of
security countermeasures directly influence information sys-
tems misuses [7], cybersecurity awareness education could
be an effective solution to empower households, including
children [8] and senior citizens [9], with knowledge and skills
to reduce the success rate of cyberattacks exploiting human
vulnerabilities in homes.
However, a critical obstacle to adopting those cybersecurity
education programs is the financial costs and resources [10].
For example, regarding employees’ training, companies seek
to minimize their budget regarding costs that are not tight
to their operations. Furthermore, individuals are willing to
take cybersecurity awareness training only if their employers
sponsor them [6]. Similarly, we assume that the financial costs
of cybersecurity training could be challenging for households.
Cost-benefit studies are important to understand the poten-
tial value of investing in cybersecurity education programs.
Existing security cost-benefit analysis include the work of
Zeng [11] who focuses on digital right management products.
The author uses the stochastic Petri nets to simulate and
predict the impact of the deployment of these digital systems
on normal business processes. Furthermore, Zhang et al. [12]
propose a new theoretical framework for conducting a cost-
benefit analysis of cybersecurity awareness training programs
to evaluate different costs and benefits on a company’s optimal
degree of security. Regarding household security awareness
training, we need to identify the minimum investment of
time and money that will encourage households to engage in
cybersecurity education programs.
To the best of our knowledge, prior research have not
addressed households’ needs for cybersecurity training. The
purpose of the present work is to address this research gap
using a game-theoretic approach. We choose this approach
to analyze the impacts of households’ decision-making of
investing in security training and identify the payoffs of each
decision.
We summarize the research contributions below.
• We provide a game-theoretical approach to analyze the
cost-effectiveness of households’ investments in cyberse-
curity awareness education.
• We investigate the pure and mixed Nash equilibria of the
proposed game.
• We propose graphical representations to analyze invest-
ment costs and households’ payoffs.
We structure the remainder of this paper as follows. Sec-
tion II presents the related work. Section III introduces the
proposed game model, presents the normal-form game, and
analyzes the pure and mixed equilibria. Section IV presents
the numerical results. Section V discusses the findings of the
paper. Section VI concludes the work.
II. RELATED WORK
This section describes the related work that uses a game-
theoretic approach to analyze security investment cost-benefits.
Generally speaking, IT security investment reflects decision-
making resulting from an analysis of potential costs and
59
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-919-5
SECURWARE 2021 : The Fifteenth International Conference on Emerging Security Information, Systems and Technologies

benefits. Thus one might consider decision theory as essential
support for this purpose. However, Cavusoglu et al. [13]
show that game-theoretic approaches are more suitable than
traditional decision-theoretic risk management techniques re-
garding IT security investment, especially when considering
that attackers are strategic. Furthermore, they find that in a
game including two players: a firm and an attacker, the firm
maximizes its payoff in a sequential game when the firm is
the leader and the attacker is the follower. This result shows
that it is possible to use a game-theoretic approach to address
the research problem of our work. Sun et al. [14] propose a
game model to address information security problems in the
mobile electronic commerce industry chain. They introduce a
penalty parameter that affects organizations that do not invest
in IT security. The results indicate that reducing investment
costs is essential to promote information security investments.
Otherwise, the regulation of a penalty parameter might help to
encourage those investments. In the present paper, we propose
a reward parameter for users who take cybersecurity training
and notice security countermeasures. Qian et al. [15] propose
a game model based on information sharing and security
investment between strategies for the two firms. The Nash
equilibrium analysis shows that firms share no information
when they make decisions individually. Furthermore, Zuo et
al. [16] use a game-theoretic approach and Nash equilibrium
to analyze information security cost investment to improve
network security. The existing research and game models do
not address security cost-benefits issues regarding households
awareness education, which are the main focus of this paper.
In
the
literature,
the
studies
on
user
cybersecurity
awareness-based cost-benefit analysis are limited. Further-
more, the related work on security investment cost-benefit
analysis only consider corporate areas, which are different
from households’ reality to some extent. In this new era of
the Internet of Things (IoT), households’ devices and data
are valuable to attackers. We need to address issues, such as
cost-benefit, related to households’ cybersecurity education to
avoid large-scale cyberattacks and ensure people’s safety and
security.
III. PROPOSED GAME MODEL
This section introduces and analyzes our game model
through four subsections. Subsection A describes the system.
Subsection B defines the parameters of the game. Subsection
C presents the normal-form game. Subsection D investigates
the pure and mixed Nash equilibria of the proposed game.
A. System
We consider a smart home comprising three types of house-
holds: adults (User1), children (User2), and senior citizens
(User3). This house is composed of many IoT devices that are
convenient for every household. For example, User1 could use
IP cameras and smart door locks to ensure the house’s physical
security. User2 could use a smart TV and smart speakers
for advertisement. User3 could use a smart pill dispenser or
smartwatch for healthcare.
User 1
User 2
User 3
Fig. 1. Illustration of the proposed model.
As illustrated in Figure 1, an attacker could gain interests
in compromising that house for various motives, such as
accessing private information, using IoT-based home devices
to execute Distributed Denial-of-Service (DDoS) attacks, the
absence of resistance such as a dedicated cybersecurity team.
Furthermore, attackers could discern that households might no-
tice part of security countermeasures, such as changing default
passwords, using multi-factor authentication, or recognizing
and avoiding phishing links, which could give them various
entry points. These attacks could be effective by targeting
User1, User2, or User3.
B. Game Modeling
Let Ti and ¯Ti, respectively, be the events Useri has got
cybersecurity awareness training, and Useri has not got cy-
bersecurity awareness training with 1 ≤ i ≤ 3.
Let A be the event that an attacker compromises a user.
We consider P(A/Ti) the probability of an attacker to com-
promise Useri given that Useri has got cybersecurity aware-
ness training, and P(A/ ¯Ti) the probability of an attacker to
compromise Useri given that Useri has not got cybersecurity
awareness training.
We assume that
P(A/T1) = P(A/T2) = P(A/T3).
(1)
P(A/ ¯T1) = P(A/ ¯T2) = P(A/ ¯T3).
(2)
We have (1) and (2) because how users could react to an ongo-
ing cyberattack depends more on their level of cybersecurity
awareness than on their age.
Let S and ¯S, respectively, be the events that a user notices
security countermeasures, and a user notices part of security
countermeasures. We consider P(A/Ti ∩ S) the probability
of an attacker compromising Useri given that Useri has got
cybersecurity awareness training and notices security coun-
termeasures, and P(A/Ti ∩ ¯S) the probability of an attacker
compromising Useri given that Useri has got cybersecurity
60
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-919-5
SECURWARE 2021 : The Fifteenth International Conference on Emerging Security Information, Systems and Technologies

awareness training and notices part of security countermea-
sures. Like (1) and (2), we assume that
P(A/T1 ∩ S) = P(A/T2 ∩ S) = P(A/T3 ∩ S).
(3)
P(A/T1 ∩ ¯S) = P(A/T2 ∩ ¯S) = P(A/T3 ∩ ¯S).
(4)
We assume that, for a given Useri with 1 ≤ i ≤ 3,
P(A/Ti ∩ S) < P(A/Ti ∩ ¯S) < P(A/ ¯Ti).
(5)
We have (5) because Useri is more secure in the event Ti ∩S
than in Ti ∩ ¯S and more secure in the event Ti ∩ ¯S than in ¯Ti.
We also assume that
0 < P(T3 ∩ S) ≤ P(T2 ∩ S) ≤ P(T1 ∩ S) ≤ 1.
(6)
0 < P(T1 ∩ ¯S) ≤ P(T2 ∩ ¯S) ≤ P(T3 ∩ ¯S) ≤ 1.
(7)
Furthermore, we have (6) and (7) because many challenges,
such as those with cognitive or physical aspects, could regu-
larly hinder senior citizens from noticing security countermea-
sures. Furthermore, we consider that the basis of knowledge
of adults is greater than those of children. Considering every
user faces the same potential threats, children might not
notice various countermeasures out of ignorance because their
cybersecurity-training content might be less intensive than
those of adults.
Moreover, we consider the following Useri’s costs: cmi the
monetary costs related to the event T, cti the time costs related
to the event S, and ct′i the time costs related to the event ¯S.
We have
0 ≤ cm1 ≤ cm2 ≤ cm3.
(8)
0 ≤ ct3 ≤ ct2 ≤ ct1.
(9)
0 ≤ ct′i < cti.
(10)
We have (8) because User2 and User3 might require
specific cybersecurity awareness training, which could be
more expensive than the training of User1. Furthermore,
we consider that it is harder to provide training materials
and resources to get User3 than User2 involved. Therefore,
the training cost of User3 is more than the training cost
of User2, which is more than that of User1. We have (9)
because we assume that User1 might invest much more time
than User2 and User3 to notice security countermeasures.
Furthermore, the effect of age User3’s on memory makes us
consider that this user might spend less time noticing security
countermeasures than User2. We have (10) because Useri
spends much more time in the event S than in the event ¯S.
We also consider δ (δ > 0) the costs of a cyberattack
on a smart home which could involve interruption costs of
smart-home services (e.g., home automation, electric power,
healthcare, entertainment, the Internet). Note that δ applies
to every user. Furthermore, we consider θ (θ > 0) the costs
associated with security breaches following an exploit through
a user’s device. This cost is assigned to the compromised
user only. We assume that δ > θ. Note that θ = 0 for a
user who is not attacked and for a user who notices security
countermeasures. We assume that
θP(A/Ti ∩ ¯S) + δ ≥ cmi + cti > cmi + ct′i.
(11)
θ is different from λ (λ ≥ 0), which is the cost associated
with privacy incidents related to households. λ depends on
households’ income and social status. We decide to assign
this cost to User1 only because being in charge of home
safety and security. While θ could relate to the quality of
life (e.g., unavailability of services, a decrease in the sense of
privacy and self-esteem), λ could relate to money (e.g., ransom
requests). Finally, we consider φ (φ > 0), the parameter that
quantifies all the comforts and benefits a user could enjoy
when living in a smart home. φ has the same value for every
user. We also consider R the reward for noticing security
countermeasures. Note that R = 0 for users who notice part
of security countermeasures.
C. Normal-Form Game
We describe strategy sets of each player as matrices. Table
I, Table II, and Table III, respectively, present the normal-form
games of an attacker targeting User1, User2, and User3. In
these tables, each cell from Line 7 - Column 4 represents
the payoffs of each player. In each cell, the first line shows
User1’s payoffs, the second line shows User2’s payoffs, the
third line shows User3’s payoffs, and the fourth line shows the
attacker’s payoffs. As an illustration, we explain the payoffs
of User1 and the attacker described in Table I.
When User1 chooses the events T and S, User1’s payoff
is φ−cm1−ct1+R and the attacker’s payoff is 0. Note that in
our model the attack fails (attacker’s payoff = 0) if the target is
a user who takes cybersecurity awareness training and notices
security countermeasures. When User1 chooses the events T
and ¯S, User1’s payoff is φ−cm1−ct′1−θP(A/T1∩ ¯S)−δ−λ
and the attacker’s payoff is θP(A/T1∩ ¯S)+δ+λ. When User1
chooses the event ¯T, User1’s payoff is φ−θP(A/ ¯T1)−δ −λ
and the attacker’s payoff is θP(A/ ¯T1) + δ + λ. Note that
when the targeted user chooses the events ¯S or ¯T, the attack
affects the other users through the parameter δ. For example
in Table I, the payoffs of User2 and User3 are respectively
φ − cm2 − ct2 + R − δ and φ − cm3 − ct3 + R − δ when both
users choose the event S and User1, the target of the attacker,
chooses the event ¯S.
D. Game Analysis
We aim to understand the rational decision-making of every
player: users and the attacker from the perspective of Nash
equilibrium. We analyze the best actions of players based
on their payoffs. According to the Nash equilibrium, every
rational player chooses an action that maximizes his or her
payoff.
1) Pure Strategy Nash Equilibrium: It refers to a game in
which every player’s mixed strategy in a mixed strategy Nash
equilibrium assigns probability 1 to a single action [17]. In
pure strategy Nash equilibrium, a player plays his or her best
61
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-919-5
SECURWARE 2021 : The Fifteenth International Conference on Emerging Security Information, Systems and Technologies

TABLE I
NORMAL FORM: AN ATTACKER TARGETS USER 1.
Attacker
targets
User 1
User 3
User 3
T
¯T
S
¯S
User 2
User 2
User 2
T
¯T
T
¯T
T
¯T
S
¯S
S
¯S
S
¯S
User 1
T
S
φ − cm1 − ct1 + R;
φ − cm2 − ct2 + R;
φ − cm3 − ct3 + R;
0
φ − cm1 − ct1 + R;
φ − cm2 − ct′2;
φ − cm3 − ct3 + R;
0
φ − cm1 − ct1 + R;
φ;
φ − cm3 − ct3 + R;
0
φ − cm1 − ct1 + R;
φ − cm2 − ct2 + R;
φ − cm3 − ct′3;
0
φ − cm1 − ct1 + R;
φ − cm2 − ct′2;
φ − cm3 − ct′3;
0
φ − cm1 − ct1 + R;
φ;
φ − cm3 − ct′3;
0
φ − cm1 − ct1 + R;
φ − cm2 − ct2 + R;
φ;
0
φ − cm1 − ct1 + R;
φ − cm2 − ct′2;
φ;
0
φ − cm1 − ct1 + R;
φ;
φ;
0
¯S
φ − cm1 − ct′1 − θP(A/T1 ∩ ¯S) − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − cm3 − ct3 + R − δ;
θP(A/T1 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − θP(A/T1 ∩ ¯S) − δ − λ ;
φ − cm2 − ct′2 − δ;
φ − cm3 − ct3 + R − δ;
θP(A/T1 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − θP(A/T1 ∩ ¯S) − δ − λ ;
φ − δ;
φ − cm3 − ct3 + R − δ;
θP(A/T1 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − θP(A/T1 ∩ ¯S) − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − cm3 − ct′3 − δ;
θP(A/T1 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − θP(A/T1 ∩ ¯S) − δ − λ;
φ − cm2 − ct′2 − δ;
φ − cm3 − ct′3 − δ;
θP(A/T1 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − θP(A/T1 ∩ ¯S) − δ − λ;
φ − δ;
φ − cm3 − ct′3 − δ;
θP(A/T1 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − θP(A/T1 ∩ ¯S) − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − δ;
θP(A/T1 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − θP(A/T1 ∩ ¯S) − δ − λ;
φ − cm2 − ct′2 − δ;
φ − δ;
θP(A/T1 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − θP(A/T1 ∩ ¯S) − δ − λ;
φ − δ;
φ − δ;
θP(A/T1 ∩ ¯S) + δ + λ;
¯T
φ − θP(A/ ¯T1) − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − cm3 − ct3 + R − δ;
θP(A/ ¯T1) + δ + λ;
φ − θP(A/ ¯T1) − δ − λ;
φ − cm2 − ct′2 − δ;
φ − cm3 − ct3 + R − δ;
θP(A/ ¯T1) + δ + λ;
φ − θP(A/ ¯T1) − δ − λ;
φ − δ;
φ − cm3 − ct3 + R − δ;
θP(A/ ¯T1) + δ + λ;
φ − θP(A/ ¯T1) − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − cm3 − ct′3 − δ;
θP(A/ ¯T1) + δ + λ;
φ − θP(A/ ¯T1) − δ − λ;
φ − cm2 − ct′2 − δ;
φ − cm3 − ct′3 − δ;
θP(A/ ¯T1) + δ + λ;
φ − θP(A/ ¯T1) − δ − λ;
φ − δ;
φ − cm3 − ct′3 − δ;
θP(A/ ¯T1) + δ + λ;
φ − θP(A/ ¯T1) − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − δ;
θP(A/ ¯T1) + δ + λ;
φ − θP(A/ ¯T1) − δ − λ;
φ − cm2 − ct′2 − δ;
φ − δ;
θP(A/ ¯T1) + δ + λ;
φ − θP(A/ ¯T1) − δ − λ;
φ − δ;
φ − δ;
θP(A/ ¯T1) + δ + λ;
TABLE II
NORMAL FORM: AN ATTACKER TARGETS USER 2.
Attacker
targets
User 2
User 3
User 3
T
¯T
S
¯S
User 2
User 2
User 2
T
¯T
T
¯T
T
¯T
S
¯S
S
¯S
S
¯S
User 1
T
S
φ − cm1 − ct1 + R;
φ − cm2 − ct2 + R;
φ − cm3 − ct3 + R;
0
φ − cm1 − ct1 + R − δ − λ;
φ − cm2 − ct′2 − θP(A/T2 ∩ ¯S) − δ;
φ − cm3 − ct3 + R − δ;
θP(A/T2 ∩ ¯S) + δ + λ
φ − cm1 − ct1 + R − δ − λ;
φ − θP(A/ ¯T2) − δ;
φ − cm3 − ct3 + R − δ;
θP(A/ ¯T2) + δ + λ
φ − cm1 − ct1 + R;
φ − cm2 − ct2 + R;
φ − cm3 − ct′3;
0
φ − cm1 − ct1 + R − δ − λ;
φ − cm2 − ct′2 − θP(A/T2 ∩ ¯S) − δ;
φ − cm3 − ct′3 − δ;
θP(A/T2 ∩ ¯S) + δ + λ
φ − cm1 − ct1 + R − δ − λ;
φ − θP(A/ ¯T2) − δ;
φ − cm3 − ct′3 − δ;
θP(A/ ¯T2) + δ + λ
φ − cm1 − ct1 + R;
φ − cm2 − ct2 + R;
φ;
0
φ − cm1 − ct1 + R − δ − λ;
φ − cm2 − ct′2 − θP(A/T2 ∩ ¯S) − δ;
φ − δ;
θP(A/T2 ∩ ¯S) + δ + λ
φ − cm1 − ct1 + R − δ − λ;
φ − θP(A/ ¯T2) − δ;
φ − δ;
θP(A/ ¯T2) + δ + λ
¯S
φ − cm1 − ct′1;
φ − cm2 − ct2 + R;
φ − cm3 − ct3 + R;
0;
φ − cm1 − ct′1 − δ − λ ;
φ − cm2 − ct′2 − θP(A/T2 ∩ ¯S) − δ;
φ − cm3 − ct3 + R − δ;
θP(A/T2 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − δ − λ ;
φ − θP(A/ ¯T2) − δ;
φ − cm3 − ct3 + R − δ;
θP(A/ ¯T2) + δ + λ;
φ − cm1 − ct′1;
φ − cm2 − ct2 + R;
φ − cm3 − ct′3;
0;
φ − cm1 − ct′1 − δ − λ;
φ − cm2 − ct′2 − θP(A/T2 ∩ ¯S) − δ;
φ − cm3 − ct′3 − δ;
θP(A/T2 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − δ − λ;
φ − θP(A/ ¯T2) − δ;
φ − cm3 − ct′3 − δ;
θP(A/ ¯T2) + δ + λ;
φ − cm1 − ct′1;
φ − cm2 − ct2 + R;
φ;
0;
φ − cm1 − ct′1 − δ − λ;
φ − cm2 − ct′2 − θP(A/T2 ∩ ¯S) − δ;
φ − δ;
θP(A/T2 ∩ ¯S) + δ+ λ;
φ − cm1 − ct′1 − δ − λ;
φ − θP(A/ ¯T2) − δ;
φ − δ;
θP(A/ ¯T2) + δ + λ;
¯T
φ;
φ − cm2 − ct2 + R;
φ − cm3 − ct3 + R;
0;
φ − δ − λ;
φ − cm2 − ct′2 − θP(A/T2 ∩ ¯S) − δ;
φ − cm3 − ct3 + R − δ;
θP(A/T2 ∩ ¯S) + δ + λ;
φ − δ − λ;
φ − θP(A/ ¯T2) − δ ;
φ − cm3 − ct3 + R − δ;
θP(A/ ¯T2) + δ + λ;
φ;
φ − cm2 − ct2 + R;
φ − cm3 − ct′3;
0;
φ − δ − λ;
φ − cm2 − ct′2 − θP(A/T2 ∩ ¯S) − δ;
φ − cm3 − ct′3 − δ;
θP(A/T2 ∩ ¯S) + δ + λ;
φ − δ − λ;
φ − θP(A/ ¯T2) − δ;
φ − cm3 − ct′3 − δ;
θP(A/ ¯T2) + δ + λ;
φ;
φ − cm2 − ct2 + R;
φ;
0;
φ − δ − λ;
φ − cm2 − ct′2 − θP(A/T2 ∩ ¯S) − δ;
φ − δ;
θP(A/T2 ∩ ¯S) + δ + λ;
φ − θ − λ;
φ − θP(A/ ¯T2) − δ;
φ − δ;
θP(A/ ¯T2) + δ + λ;
62
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-919-5
SECURWARE 2021 : The Fifteenth International Conference on Emerging Security Information, Systems and Technologies

TABLE III
NORMAL FORM: AN ATTACKER TARGETS USER 3.
Attacker
targets
User 3
User 3
User 3
T
¯T
S
¯S
User 2
User 2
User 2
T
¯T
T
¯T
T
¯T
S
¯S
S
¯S
S
¯S
User 1
T
S
φ − cm1 − ct1 + R;
φ − cm2 − ct2 + R;
φ − cm3 − ct3 + R;
0
φ − cm1 − ct1 + R;
φ − cm2 − ct′2;
φ − cm3 − ct3 + R;
0
φ − cm1 − ct1 + R;
φ;
φ − cm3 − ct3 + R;
0
φ − cm1 − ct1 + R − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − cm3 − ct′3 − θP(A/T3 ∩ ¯S) − δ;
θP(A/T3 ∩ ¯S) + δ + λ
φ − cm1 − ct1 + R − δ − λ;
φ − cm2 − ct′2 − δ;
φ − cm3 − ct′3 − θP(A/T3 ∩ ¯S) − δ;
θP(A/T3 ∩ ¯S) + δ + λ
φ − cm1 − ct1 + R − δ − λ;
φ − δ;
φ − cm3 − ct′3 − θP(A/T3 ∩ ¯S) − δ;
θP(A/T3 ∩ ¯S) + δ + λ
φ − cm1 − ct1 + R − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − θP(A/ ¯T3) − δ;
θP(A/ ¯T3) + δ + λ
φ − cm1 − ct1 + R − δ − λ;
φ − cm2 − ct′2 − δ;
φ − θP(A/ ¯T3) − δ;
θP(A/ ¯T3) + δ + λ
φ − cm1 − ct1 + R − δ − λ;
φ − δ;
φ − θP(A/ ¯T3) − δ;
θP(A/ ¯T3) + δ + λ
¯S
φ − cm1 − ct′1;
φ − cm2 − ct2 + R;
φ − cm3 − ct3 + R;
0;
φ − cm1 − ct′1;
φ − cm2 − ct′2;
φ − cm3 − ct3 + R;
0;
φ − cm1 − ct′1 ;
φ;
φ − cm3 − ct3 + R;
0;
φ − cm1 − ct′1 − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − cm3 − ct′3 − θP(A/T3 ∩ ¯S) − δ;
θP(A/T3 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − δ − λ;
φ − cm2 − ct′2 − δ;
φ − cm3 − ct′3 − θP(A/T3 ∩ ¯S) − δ;
θP(A/T3 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − δ − λ;
φ − δ;
φ − cm3 − ct′3 − θP(A/T3 ∩ ¯S) − δ;
θP(A/T3 ∩ ¯S) + δ + λ;
φ − cm1 − ct′1 − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − θP(A/ ¯T3) − δ;
θP(A/ ¯T3) + δ + λ;
φ − cm1 − ct′1 − δ − λ;
φ − cm2 − ct′2 − δ;
φ − θP(A/ ¯T3) − δ ;
θP(A/ ¯T3) + δ + λ;
φ − cm1 − ct′1 − δ − λ;
φ − δ;
φ − θP(A/ ¯T3) − δ;
θP(A/ ¯T3) + δ + λ;
¯T
φ;
φ − cm2 − ct2 + R;
φ − cm3 − ct3 + R;
0;
φ;
φ − cm2 − ct′2;
φ − cm3 − ct3 + R;
0;
φ;
φ;
φ − cm3 − ct3 + R;
0;
φ − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − cm3 − ct′3 − θP(A/T3 ∩ ¯S) − δ;
θP(A/T3 ∩ ¯S) + δ + λ;
φ − δ − λ;
φ − cm2 − ct′2 − δ;
φ − cm3 − ct′3 − θP(A/T3 ∩ ¯S) − δ;
θP(A/T3 ∩ ¯S) + δ + λ;
φ − δ − λ;
φ − δ;
φ − cm3 − ct′3 − θP(A/T3 ∩ ¯S) − δ;
θP(A/T3 ∩ ¯S) + δ + λ;
φ − δ − λ;
φ − cm2 − ct2 + R − δ;
φ − θP(A/ ¯T3) − δ;
θP(A/ ¯T3) + δ + λ;
φ − δ − λ;
φ − cm2 − ct′2 − δ;
φ − θP(A/ ¯T3) − δ;
θP(A/ ¯T3) + δ + λ;
φ − δ − λ;
φ − δ;
φ − θP(A/ ¯T3) − δ;
θP(A/ ¯T3) + δ + λ;
strategy; the rational player would never change his or her
strategy to get a lower payoff than that of the best strategy.
Theorem 1. When every user notices security countermea-
sures, the proposed game admits a pure strategy Nash equi-
librium related to the strategic profile (S, S, S, A).
Proof. The proposed game generates nine strategic profiles
when users choose the same actions and 72 otherwise. We
study each of these two types of strategic profiles. Let
Uatt(Useri) be the utility of the attacker when targeting Useri.
• Strategic profiles (Type 1): Users play the same actions.
Case 1.1: Every user has not got cybersecurity awareness
training.
Uatt(Useri)( ¯T, ¯T, ¯T, A) = θP(A/ ¯Ti) + δ + λ
From (2), there is equality between the attacker’s payoffs.
The attacker cannot increase his or her payoff. However, Useri
can increase his or her payoff from “φ − θP(A/ ¯Ti) − δ − λ”
to “φ − cmi − cti + R” by choosing to play S instead of ¯T
because (5) and (11) show that −(θP(A/ ¯Ti) + δ) < −(cmi +
cti). Therefore, the strategic profile ( ¯T, ¯T, ¯T, A) is not a pure
strategy Nash equilibrium.
Case 1.2: Every user notices part of security countermeasures.
Uatt(Useri)( ¯S, ¯S, ¯S, A) = θP(A/Ti ∩ ¯S) + δ + λ
From (4), there is equality between the attacker’s payoffs
whoever his or her target is. The attacker cannot increase his
or her payoff. However, Useri can increase his or her payoff
from “φ − cmi − ct′i − θP(A/ ¯Ti ∩ ¯S) − δ − λ” to “φ − cmi −
cti + R” by choosing to play S instead of ¯S because (11)
shows that −(θP(A/ ¯Ti ∩ ¯S) + δ) < −(cmi + ct′i). Therefore,
the strategic profile ( ¯S, ¯S, ¯S, A) is not a pure strategy Nash
equilibrium.
Case 1.3: Every user notices security countermeasures.
Uatt(Useri)(S, S, S, A) = 0
The attacker gets the same payoff whoever his or her target
is. Furthermore, users get the maximum payoff (i.e., “φ −
cmi − cti + R”) when they play “S”. Therefore, the strategic
profile (S, S, S, A) is a pure strategy Nash equilibrium.
• Strategic profiles (Type 2): Every user does not play the
same action.
Case 2.1: One or two users notices security countermeasures.
The attacker’s payoff is zero when targeting a user who
notices security countermeasures. The attacker can increase his
or her payoff by targeting a user who notices part of security
countermeasures. Therefore, the related strategic profiles, such
as (S, ¯S, ¯T, A), (S, S, ¯T, A), and (S, S, ¯S, A), are not pure
strategy Nash equilibria.
Case 2.2: One or two users notices part of security counter-
measures and the other user(s) has (have) not got cybersecu-
rity awareness training.
The attacker’s payoff is θP(A/Ti∩ ¯S)+δ+λ or θP(A/ ¯Ti)+
δ + λ. From (5), P(A/Ti ∩ ¯S) < P(A/ ¯Ti); then the attacker
can increase his or her payoff by targeting a user who has not
63
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-919-5
SECURWARE 2021 : The Fifteenth International Conference on Emerging Security Information, Systems and Technologies

got cybersecurity awareness training. Therefore, the related
strategic profiles, such as ( ¯S, ¯T, ¯T, A), ( ¯T, ¯T, ¯S, A), and
( ¯S, ¯S, ¯T, A), are not pure strategy Nash equilibria.
2) Mixed Strategy Nash Equilibrium: It refers to a game in
which every player plays a mixed strategy (i.e., a probability
distribution over the pure strategies) and cannot improve his
or her payoff under the mixed-strategy profile.
We consider the following parameters.
• ui: The probability of Useri taking cybersecurity aware-
ness training, and 1 − ui the probability of Useri not
taking the training.
• usi: The probability of Useri noticing security counter-
measures, and 1 − usi the probability of noticing part of
security countermeasures.
0 ≤ ui, usi ≤ 1.
(12)
Note that ui, 1 − ui, usi, and 1 − usi, respectively, refer to
as P(Ti), P( ¯Ti), P(Ti ∩ S), and P(Ti ∩ ¯S) with 1 ≤ i ≤ 3.
We consider a1, a2, and a3, respectively, the probabilities
associated with the attacker targeting User1, User2, and
User3.
0 ≤ a1, a2, a3 ≤ 1.
(13)
a1 + a2 + a3 = 1.
(14)
We assume that every player (i.e., attacker and users)
randomizes his or her strategy.
2.1) User 1 plays a mixed strategy
The utility (U1) of User1 is the same when noticing security
countermeasures (S), noticing part of security countermea-
sures ( ¯S), or not taking cybersecurity awareness training ( ¯T).
We have
U1(S) = U1( ¯S) = U1( ¯T)
(15)
where
U1(S) =(δ + λ)(a2u2us2 + a3u3us3 − a2 − a3)+
R + φ − cm1 − ct1
U1( ¯S) = − a1θP(A/T1 ∩ ¯S) + (δ + λ)(a2u2us2 + a3u3us3)
+ φ − δ − λ − cm1 − ct′1
U1( ¯T) = − a1θP(A/ ¯T1) + (δ + λ)(a2u2us2 + a3u3us3)
+ φ − δ − λ
From (14), we have a2 + a3 = 1 − a1 then
If U1(S) = U1( ¯S) then
a1 =
−R + ct1 − ct′1
θP(A/T1 ∩ ¯S) + δ + λ
(16)
If U1(S) = U1( ¯T) then
a1 = −R + cm1 + ct1
θP(A/ ¯T1) + δ + λ
(17)
If U1( ¯S) = U1( ¯T) then
a1 =
−(cm1 + ct′1)
θ(P(A/T1 ∩ ¯S) − P(A/ ¯T1))
(18)
2.2) User j plays a mixed strategy
Similarly, regarding User j, with 2 ≤ j ≤ 3, we obtain
If Uj(S) = Uj( ¯S) then
aj = −R + ctj − ct′j
θP(A/Tj ∩ ¯S) + δ
(19)
If Uj(S) = Uj( ¯T) then
aj =−R + cmj + ctj
θP(A/ ¯Tj) + δ
(20)
If Uj( ¯S) = Uj( ¯T) then
aj =
−(cmj + ct′j)
θ(P(A/Tj ∩ ¯S) − P(A/ ¯Tj))
(21)
2.3) The attacker plays a mixed strategy
The utility (Uatt) of the attacker is the same when targeting
User1, User2, or User3.
Uatt(User1) = Uatt(User2) = Uatt(User3)
(22)
Using Equations (2) and (4), for 1 ≤ i ≤ 3, we obtain
Uatt(Useri) =uiθP(A/Ti ∩ ¯S)(1 − usi) + θP(A/ ¯Ti)(1 − ui)
− uiusi(δ + λ) + δ + λ
The strategy profile at mixed strategy Nash equilibrium is
{u1us1S+u1(1−us1) ¯S+(1−u1) ¯T; u2us2S+u2(1−us2) ¯S+
(1−u2) ¯T; u3us3S+u3(1−us3) ¯S+(1−u3) ¯T; a1A1+a2A2+
a3A3}.
Theorem 2. The proposed game admits many mixed strategy
Nash equilibria, especially when λ = 0, Useri chooses to
randomize to play S and ¯S with cti − ct′i > R, or chooses to
play S and ¯T with cmi + cti > R, or chooses to randomize
to play ¯S and ¯T.
Proof. Equations (16) and (19) show that, for 1 ≤ i ≤ 3,
ai > 0 only if cti − ct′i > R. Similarly, Equations (17) and
(20) show that ai > 0 only if cmi +cti > R. Therefore, under
these conditions, the proposed game may reach mixed strategy
Nash equilibria when Useri chooses randomly the events S
and ¯S or the events S and ¯T. Equations (18) and (21) show
that ai > 0 because (5) states that P(A/Ti ∩ ¯S) < P(A/ ¯Ti).
Therefore, the proposed game may reach a mixed strategy
Nash equilibrium when Useri plays randomly the events ¯S
and ¯T.
IV. NUMERICAL RESULTS
This section presents the numerical results of the proposed
game using the equations obtained in Section III-D. We
analyze the payoffs of households and attackers from the
perspective of security costs and rewards for noticing security
countermeasures. We further consider a more realistic cost
covering scenario where User1 pays the monetary cost of
cybersecurity training of User2 and User3. In this scenario,
we refer to Useri as Actual User i (1 ≤ i ≤ 3).
64
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-919-5
SECURWARE 2021 : The Fifteenth International Conference on Emerging Security Information, Systems and Technologies

0
2
4
6
8
10
Rewards
15
10
5
0
5
10
Payoffs
User 1 (S)
User 2 (S)
User 3 (S)
Actual User 1 (S)
Actual User 2 (S)
Actual User 3 (S)
Attacker
Fig. 2. Illustration of players’ payoffs based on users’ rewards for noticing
security countermeasures with φ < min(cm1 + ct1, cm2 + ct2, cm3 + ct3).
0
2
4
6
8
10
Rewards
10
5
0
5
10
15
Payoffs
User 1 (S)
User 2 (S)
User 3 (S)
Actual User 1 (S)
Actual User 2 (S)
Actual User 3 (S)
Attacker
Fig. 3. Illustration of players’ payoffs based on users’ rewards for noticing
security countermeasures with φ > max(cm1 +ct1, cm2 +ct2, cm3 +ct3).
Our results are essentially based on the following parame-
ters: φ, θ, R, cmi, cti, ct′i, P(Ti), P(Ti ∩ S), P(A/ ¯Ti), and
P(A/Ti ∩ ¯S) (1 ≤ i ≤ 3). We proposed, respectively, two
scenarios related to the pure strategy Nash equilibrium and
nine scenarios related to the mixed strategy Nash equilibria to
examine the potential impacts of rewards for noticing security
countermeasures, security costs, and the likelihood of the event
T1 ∩ S on the players’ payoffs.
In the first two scenarios, the graph results are based on each
player’s payoff regarding the strategic profile (S, S, S, A). We
set cm1 = 3; cm2 = 4; cm3 = 6; ct1 = 6; ct2 = 3; ct3 = 2. We
choose φ = 1 in the first scenario and φ = 10 in the second.
Figure 2 presents the results of the first scenario. We can see
that when the comfort and benefit of living in a smart home
are less considerable than security costs (money and time) to
be invested, User 1, User 2, and User 3 will be satisfied with
taking security training and noticing security countermeasures
only if the security rewards are extremely significant and
greater than the security costs invested (R > 8). Furthermore
0
5
10
15
20
25
30
35
40
Monetary costs of cybersecurity awareness training
80
60
40
20
0
20
Payoffs
User 1 (S)
User 1 (S)
User 2 (S)
User 2 (S)
User 3 (S)
User 3 (S)
Fig. 4. Illustration of users’ payoffs based on security investment costs when
φ > (θ + δ) > R.
0
5
10
15
20
25
30
35
40
Monetary costs of cybersecurity awareness training
60
40
20
0
20
Payoffs
User 1 (S)
User 1 (S)
User 2 (S)
User 2 (S)
User 3 (S)
User 3 (S)
Fig. 5. Illustration of users’ payoffs based on security investment costs when
φ > R > (θ + δ).
“Actual User 2” and “Actual User 3” could be satisfied with
a very few security reward (R > 2) while “Actual User 1”,
will never be satisfied whatever the security rewards because
his or her payoff remains negative. Figure 3 presents the
results of the second scenario. It shows that when User 1,
User 2, and User 3 estimate that the comfort and benefit of
living in a smart home are more significant than the security
costs to be spent, they are more likely to invest and notice
security countermeasures whatever the reward. Same goes for
“Actual User 2” and “Actual User 3” who are keen to notice
security countermeasures. However, “Actual User 1” will be
satisfied only if the security rewards are extremely significant
(R > 9). As it might be seen, in both scenarios, the results
show a linear relationship between households’ payoffs and
the security rewards. Furthermore, the attacker’s payoff is null,
which reveals that the attacks would fail in such situations.
The graph results of the other scenarios are based on the
players’ payoffs in the mixed strategy Nash equilibria. We set
0 ≤ cm1 < 40; cm2 = 1.25 ∗ cm1; cm3 = 1.75 ∗ cm1; ct1 = 6;
65
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-919-5
SECURWARE 2021 : The Fifteenth International Conference on Emerging Security Information, Systems and Technologies

0
5
10
15
20
25
30
35
40
Monetary costs of cybersecurity awareness training
60
40
20
0
20
Payoffs
User 1 (S)
User 1 (S)
User 2 (S)
User 2 (S)
User 3 (S)
User 3 (S)
Fig. 6. Illustration of users’ payoffs based on security investment costs when
R > φ > (θ + δ).
0
5
10
15
20
25
30
35
40
Monetary costs of cybersecurity awareness training
60
40
20
0
20
Payoffs
User 1 (S)
User 1 (S)
User 2 (S)
User 2 (S)
User 3 (S)
User 3 (S)
Fig. 7. Illustration of users’ payoffs based on security investment costs when
R > (θ + δ) > φ.
ct2 = 3; ct3 = 2; ct′1 = 4; ct′2 = 2; ct′3 = 1; P(T3 ∩ S) =
0.5; P(T2 ∩ S) = 0.6; P(T1 ∩ S) = 0.7; P(A/Ti ∩ ¯S) = 0.4;
P(A/ ¯Ti) = 0.9 (1 ≤ i ≤ 3).
Scenario 3 [φ > (θ + δ) > R]: We choose φ = 18; θ =
3; δ = 7; R = 5. Figure 4 presents the expected payoffs
of households depending on the security costs in money of
cybersecurity awareness training. We can see that the maximin
strategy (the best of a set of worst possible security investment
strategies) of households is reached when User 1 plays ¯S and
User 3 plays S with cm1 = 6.56 and payoff = 4.39 > 0.
Scenario 4 [φ > R > (θ + δ)]: We choose φ = 18; θ =
2; δ = 3; R = 10. Figure 5 shows that the maximin strategy of
households is reached when User 1 plays ¯S and User 3 plays
S with cm1 = 10.24 and payoff = 4.16 > 0.
Scenario 5 [R > φ > (θ + δ)]: We choose φ = 10; θ =
2; δ = 3; R = 18. As presented in Figure 6, the maximin
strategy of households is reached when User 1 plays ¯S and
User 3 plays S with cm1 = 17.82 and payoff = −9.47.
Scenario 6 [R > (θ + δ) > φ]: We choose φ = 5; θ =
0
5
10
15
20
25
30
35
40
Monetary costs of cybersecurity awareness training
80
60
40
20
0
20
Payoffs
User 1 (S)
User 1 (S)
User 2 (S)
User 2 (S)
User 3 (S)
User 3 (S)
Fig. 8. Illustration of users’ payoffs based on security investment costs when
(θ + δ) > R > φ.
0
5
10
15
20
25
30
35
40
Monetary costs of cybersecurity awareness training
80
60
40
20
0
Payoffs
User 1 (S)
User 1 (S)
User 2 (S)
User 2 (S)
User 3 (S)
User 3 (S)
Fig. 9. Illustration of users’ payoffs based on security investment costs when
(θ + δ) > φ > R.
3; δ = 7; R = 18. Figure 7 shows that the maximin strategy
of households is reached when User 1 plays ¯S and User 3
plays S with cm1 = 18.63 and payoff = −17.19 < 0.
Scenario 7 [(θ + δ) > R > φ]: We choose φ = 5; θ =
6; δ = 12; R = 10. Figure 8 shows that the maximin strategy
of households is reached when User 1 and User 3 play ¯S with
cm1 = 13.58 and payoff = −17.77 < 0.
Scenario 8 [(θ + δ) > φ > R]: We choose φ = 10; θ =
6; δ = 12; R = 5. As presented in Figure 9, the maximin
strategy of households is reached when User 1 plays ¯S and
User 3 plays S with cm1 = 8.91 and payoff = −9.41 < 0.
Scenario 9 [φ > (θ + δ) > R]: We choose φ = 18; θ =
3; δ = 7; R = 5. The previous results demonstrate that
Scenario 3 is the best option for households to minimize the
monetary costs and get better payoffs. However, Figure 10
shows that Scenario 3 may not suit actual users when only
“Actual User 1” is accountable for the monetary costs. We
can see that the maximin strategy of actual users is reached
when “Actual User 1” plays ¯S or S with cm1 = 6.71 and
66
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-919-5
SECURWARE 2021 : The Fifteenth International Conference on Emerging Security Information, Systems and Technologies

0
5
10
15
20
25
30
35
40
Monetary costs of cybersecurity awareness training
150
100
50
0
50
Payoffs
Actual User 1 (S)
Actual User 1 (S)
Actual User 2 (S)
Actual User 2 (S)
Actual User 3 (S)
Actual User 3 (S)
Fig. 10.
Illustration of actual users’ payoffs based on security investment
costs when φ > (θ + δ) > R.
0.0
0.2
0.4
0.6
0.8
1.0
Probability P(T1
S)
20
15
10
5
0
5
10
15
Payoffs
User 1 (S)
User 1 (S)
User 2 (S)
User 2 (S)
User 3 (S)
User 3 (S)
Actual User 1 (S)
Actual User 1 (S)
Actual User 2 (S)
Actual User 2 (S)
Actual User 3 (S)
Actual User 3 (S)
Attacker
Fig. 11.
Illustration of players’ payoffs based on P(T1 ∩ S) when φ >
(θ + δ) > R and cm1 = 6.56.
payoff = −15.80 < 0.
Scenario 10 [φ > (θ + δ) > R]: We choose φ = 18; θ =
3; δ = 7; R = 5. We analyze the payoffs of users and the
attacker based on the probability P(T1 ∩ S) regarding to the
best maximin strategy (cm1 = 6.56). Figure 11 shows that
the attacker payoff decreases linearly from 8.65 to 2.91. The
payoffs of User 1, User 2, and User 3 increase linearly in the
range of −2.09 to 7.75. Furthermore, the payoff of “Actual
User 2” and “Actual User 3” increase linearly in the range of
7.49 to 17.29. We note that the payoffs of “Actual User 1”
increases linearly from −19.16 to −13.28. Even with P(T1 ∩
S) = 1, the payoffs of “Actual User 1” remain negative.
Scenario 11 [φ > (θ + δ) > R]: We choose φ = 18; θ =
3; δ = 7; R = 5; cm1 = 0. Figure 12 shows that the attacker
payoff decreases linearly from 8.65 to 2.91. Users’ payoffs are
all positive even though they decrease linearly. Furthermore,
User 1 payoff ≥ attacker payoff when P(T1 ∩ S) ≥ 0.55. We
can also notice that the payoffs of User i and “Actual User i”
0.0
0.2
0.4
0.6
0.8
1.0
Probability P(T1
S)
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
22.5
Payoffs
User 1 (S)
User 1 (S)
User 2 (S)
User 2 (S)
User 3 (S)
User 3 (S)
Actual User 1 (S)
Actual User 1 (S)
Actual User 2 (S)
Actual User 2 (S)
Actual User 3 (S)
Actual User 3 (S)
Attacker
Fig. 12.
Illustration of players’ payoffs based on P(T1 ∩ S) when φ >
(θ + δ) > R and cm1 = 0.
are similar (with 1 ≤ i ≤ 3).
V. DISCUSSION
The analysis of the numerical results indicates that security
investments and the reward for noticing security countermea-
sures may influence households to engage in cybersecurity
awareness education. The numerical results related to the
pure strategy Nash equilibrium show that households would
take the cybersecurity awareness training and notice security
countermeasures under two conditions. First, the smart home
should provide original values and vital comfort, and the
other is that the security rewards should be very significant.
Thus, investigating and providing new frameworks for security
rewards in smart homes is a research area that needs to be
explored and addressed.
Regarding the results of mixed strategies, we can see that
Scenario 3 is the best option for households because they
can minimize the security investment costs and get a positive
payoff. However, as shown in Figure 10, if a rational adult
(e.g., “Actual User 1”) has to pay the monetary costs for every
user, then minimizing the security investment costs would
provide a negative payoff. Furthermore, Figure 11 shows that
even though a rational adult notices security countermeasures
(with P(T1 ∩ S) = 1), his payoff will remain negative.
Therefore “Actual User 1” will not be satisfied with the
security investment done, which may impact his decision to
keep noticing security countermeasures and affect the secu-
rity behaviors of the other users. To address this issue, we
encourage government to support households by subsidizing
the cybersecurity training costs. As presented in Figure 12,
when the training costs are zero, the payoff of every user
is positive. Thus, households will be more likely to notice
security countermeasures. Note that the decrease of users’
payoff could shed light on the need to encourage households
constantly on the importance of noticing security behaviors.
67
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-919-5
SECURWARE 2021 : The Fifteenth International Conference on Emerging Security Information, Systems and Technologies

It is worth noting that the results of this paper rely on the
effectiveness of cybersecurity awareness programs. We have
assumed that those programs provide the required information
to households to be aware of and deal with most known
cyberattacks. Therefore, one limitation of this study is due to
the existence of unknown cyberattacks that will not be teaching
in the cybersecurity awareness programs. Furthermore, we
have made some assumptions such as those of Equations (1),
(2), (3), and (4) which may not be realistic. Additional research
on this matter is recommended. Moreover, this study provides
many insights regarding the future of cybersecurity education
programs. The numerical results show the importance of the
parameters φ and R. It would be highly appropriate for
households to access tailored-service in smart homes. Thus,
the comfort and benefit of living in such houses will encourage
users to invest in cybersecurity to preserve their quality of
life at home. Furthermore, providing households with tangible
security rewards could also engage them in cybersecurity
education programs. Our work also highlights the importance
of developing specific and efficient programs for each category
of households: children, adults, and senior citizens. Finally,
we encourage public cybersecurity policy towards households
security to provide free cybersecurity awareness training. Once
the monetary costs are addressed, another challenge will be to
reduce the time costs and make cybersecurity easier to learn
and more intuitive for households.
VI. CONCLUSION AND FUTURE WORK
In this paper, we proposed a game-theoretic approach to
analyze cybersecurity awareness cost-benefit toward designing
efficient education programs for households security. The
goal is to encourage home users to engage in cybersecurity
awareness education by identifying the minimum security
investment cost that satisfies households and compare house-
holds’ payoffs and the attacker’s payoffs given a cyberattack.
We provide a normal-form game with four players: three
home users, including a senior citizen, an adult, and a child,
and one attacker. We determine the conditions to reach the
pure and mixed Nash equilibria of the proposed game. The
numerical results show that the quality of services provided
in a smart home, the security rewards of taking cybersecurity
awareness training and noticing security countermeasures, and
the potential impacts of cyberattacks may affect the payoffs
of households and the attacker. Our research finds that the
increase of quality of services accessible in a smart home may
motivate households to engage in cybersecurity awareness edu-
cation. Furthermore, providing security rewards to households
may help them raise and maintain a high level of security
awareness.
Future work may extend the present study to more than
three users and many attackers. More importantly, we will
propose an evolutionary game-theoretic approach to study the
evolution of real users’ behaviors in the proposed game and
provide more realistic results. We will also seek to provide a
survey research to confirm the findings of this paper. This work
may also encourage a deeper investigation into cybersecurity
education programs to provide more efficient frameworks for
households, including children, adults, and senior citizens.
Furthermore, our work may inspire smart-home providers to
develop high-quality, tailored services for households. Finally,
future work may also investigate the reduction of time costs
and the design of security rewards in smart homes.
ACKNOWLEDGMENT
Part of this study was funded by the ICS-CoE Core Human
Resources Development Program.
REFERENCES
[1] S. Hansche, “Designing a security awareness program: Part 1,” Infor-
mation systems security, vol. 9, no. 6, pp. 1–9, 2001.
[2] S. Furnell, M. Gennatou, and P. Haskell-Dowland, “A prototype tool
for information security awareness and training,” Logistics Information
Management, vol. 15, pp. 352–357, 12 2002.
[3] D. Ki-Aries and S. Faily, “Persona-centred information security aware-
ness,” Computers & Security, vol. 70, pp. 663–674, 2017.
[4] M. R. Alam, M. B. I. Reaz, and M. A. M. Ali, “A review of smart
homes—past, present, and future,” IEEE Transactions on Systems, Man,
and Cybernetics, Part C (Applications and Reviews), vol. 42, no. 6, pp.
1190–1203, 2012.
[5] OWASP, “Internet of Things Top Ten,” 2014.
[6] J. Ricci, F. Breitinger, and I. Baggili, “Survey results on adults and cy-
bersecurity education,” Education and Information Technologies, vol. 24,
no. 1, pp. 231–249, 2019.
[7] J. D’Arcy, A. Hovav, and D. Galletta, “User awareness of security
countermeasures and its impact on information systems misuse: A
deterrence approach,” Information systems research, vol. 20, no. 1, pp.
79–98, 2009.
[8] A. A. Al Shamsi, “Effectiveness of Cyber Security Awareness Program
for young children: A Case Study in UAE,” International Journal of
Information Technology and Language Studies, vol. 3, no. 2, 2019.
[9] C. G. Blackwood-Brown, “An Empirical Assessment of Senior Citi-
zens’ Cybersecurity Awareness, Computer Self-Efficacy, Perceived Risk
of Identity Theft, Attitude, and Motivation to Acquire Cybersecurity
Skills,” Ph.D. dissertation, Nova Southeastern University, 2018.
[10] H. Aldawood and G. Skinner, “Challenges of implementing training and
awareness programs targeting cyber security social engineering,” in 2019
Cybersecurity and Cyberforensics Conference (CCC).
IEEE, 2019, pp.
111–117.
[11] W. Zeng, “A methodology for cost-benefit analysis of information
security technologies,” Concurrency and Computation: Practice and
Experience, vol. 31, no. 7, p. e5004, 2019.
[12] Z. J. Zhang, W. He, W. Li, and M. Abdous, “Cybersecurity awareness
training programs: a cost–benefit analysis framework,” Industrial Man-
agement & Data Systems, vol. 121, pp. 613–636, 2021.
[13] H. Cavusoglu, S. Raghunathan, and W. T. Yue, “Decision-theoretic
and game-theoretic approaches to it security investment,” Journal of
Management Information Systems, vol. 25, no. 2, pp. 281–304, 2008.
[14] W. Sun, X. Kong, D. He, and X. You, “Information security problem
research based on game theory,” in 2008 International Symposium on
Electronic Commerce and Security, 2008, pp. 554–557.
[15] X. Qian, X. Liu, J. Pei, and P. M. Pardalos, “A new game of information
sharing and security investment between two allied firms,” International
Journal of Production Research, vol. 56, no. 12, pp. 4069–4086, 2018.
[16] Z. Zuo, Y. Fang, L. Liu, F. Fang, and X. Hu, “Research on information
security cost based on game-theory,” in 2013 IEEE 8th Conference on
Industrial Electronics and Applications (ICIEA). IEEE, 2013, pp. 1435–
1436.
[17] M. J. Osborne, An introduction to game theory. Oxford university press
New York, 2004, vol. 3, no. 3.
68
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-919-5
SECURWARE 2021 : The Fifteenth International Conference on Emerging Security Information, Systems and Technologies

