A Perspective on Machine Consciousness 
Dilip K. Prasad 
School of Computer Engineering 
Nanyang Technological University 
Singapore - 639798 
e-mail: dilipprasad@pmail.ntu.edu.sg 
Janusz A. Starzyk 
School of Electrical Engineering and Computer Science, 
Ohio University, 
Athens, OH 45701, USA 
e-mail: starzykj@ohio.edu 
 
 
Abstract—Understanding consciousness and implementing it in 
manmade machines has interested researchers for a long time. 
Despite the amount of research efforts, the measurable 
development in this field is very small. Among various 
technical, philosophical and ethical reasons, the primary 
reason is the difficulty in understanding consciousness. Most 
researchers assume that consciousness is a meta-physical 
phenomenon. In this paper, we consider various perspectives 
related to cognitive and information science to conclude that 
consciousness is a physical phenomenon. We also discuss the 
role of information in making machines conscious and present 
our own views on consciousness.  
Keywords — Machine consciousness; models of machine 
consciousness 
I. 
 INTRODUCTION  
Many researchers (philosophers, psychologists, cognitive 
neuroscientists, artificial intelligence (AI) community, etc.) 
have tried to define consciousness [7], [11], [15], [19], [26], 
[32], [35]-[38], [49], [51]-[53]. A dogmatic theory of 
consciousness still evades the researchers and consciousness 
remains an abstract term (similar to intelligence). Many 
considered consciousness as causal (or non-causal) [32], 
[34], [35], accessible (or inaccessible) [33], [34], [38], [53], 
stateless (or having physical state) [1], [3], [38], [47], [52], 
[53], representational (or non-representational) [3], [11], 
[13], [18], [20], [23], [24], [26], [39] and so on. Yet, none of 
the approaches provides a complete and thorough theory of 
consciousness. Based on the current research work [1]-[50], 
one can conclude that consciousness is not merely a physical 
process but a meta-physical phenomenon which builds upon 
and is manifest in some physical sense.  
In this work, we discuss several approaches and present 
our view on this topic that “consciousness need not be 
defined in metaphysical terms”. We also address the role of 
information and information processing mechanisms in 
realizing machine consciousness. We compare the existing 
practical models of machines consciousness with our 
perspective. We conclude that consciousness is a physical 
phenomenon, and can be realized in machines. 
II. 
MACHINE CONSCIOUSNESS 
In order to facilitate further discussion, we introduce the 
concept of virtual machines and attempts of researchers to 
link them to consciousness [1], [2], [23], [24], [37], [41]. 
Virtual machines are biological/non-biological, living/non-
living, or physical/non-physical systems (parts of system) 
that exhibit conscious phenomenon. Such machines are 
typically expected to: 
1. Operate 
on 
complex 
information 
by 
acquiring, 
processing, and using information selectively,  
2. Make decisions/selections from available options, and 
3. Initiate appropriate actions. 
One of the primary perspectives in the definition of 
machine consciousness is that consciousness is the catalyst 
of action (consciousness initiates action, or appropriate 
action as suggested by Sloman [22], [24]).  We question 
whether satisfying the properties of virtual machines listed in 
(1-3) above are sufficient for consciousness. 
In the framework of virtual machines, many day-to-day 
activities of plants (evident and undeniable examples are 
pitcher plants and touch-sensitive mimosa pudica) indicate 
that they are virtual machines too.  Yet we do not refer to 
plants as conscious. Since if plants are conscious, then it can 
be argued that contemporary AI machines that exhibit some 
kind of information processing and control should also be 
referred to as being conscious which, obviously, is an 
overstatement.  
The actions performed by low level animals (like insects) 
are basically sensory motor skills or results of circadian 
(sleep/wake) rhythm, and do not require consciousness. 
These animals are not conscious, though they may partially 
satisfy (1-3) above.  
Another perspective is that consciousness needs to 
involve a centralized system (unlike plants) as reasoned by 
Muller [58]. He says that it is someone (I, he, it), a center of 
being, that is aware or conscious and that experiences. We do 
partially agree that centralized system is necessary for 
consciousness. More evolved and conscious animals 
invariably have a developed central nervous system. It is also 
notable that the level of consciousness increases with the 
development of pre-frontal cortex. In our opinion, 
centralized system is necessary but not sufficient for 
consciousness and other conditions must be present as well 
[57]. 
Different perspective is presented by Blackmore [12] and 
Velmans [36]. They propose that human-like consciousness 
is an illusion (it exists but is not what it appears to be). When 
we try to replicate human/animal behavior, we create this 
illusion that we are a conscious self having a stream of 
experiences [11]. We do not agree with this statement. In our 
109
COGNITIVE 2010 : The Second International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-108-3

opinion, consciousness is not an illusion; it does exist and is 
experienced physically. 
A.  Levels of machine consciousness 
From the perspective of philosophers, „machine 
consciousness‟ is a very complex issue. To make things 
more tractable, Sloman and Chrisley [13], [15], [22]-[24] 
suggest that instead of defining and characterizing 
consciousness, it is better to put down the expected traits of 
something being conscious (a fly being conscious, a new-
born calf being conscious, a file protection system being 
conscious, etc.). Besides avoiding unnecessary abstraction 
and unproductive philosophical rigor, it serves another 
important purpose. It provides a guideline for artificially 
implementable aspect of consciousness and a manner of 
defining the requirements for a machine to be conscious.  
For instance, for a fly (robot-fly) to be conscious, it needs 
to be able to identify an impending swat as a threat 
(something to avoid), the direction of threat, and a possible 
escape. Its consciousness need not require it to be aware of 
this action, or to feel happy about escaping the threat. 
Similarly, a new born calf needs to be aware of mother‟s 
presence, and that she can feed him, that he should suckle the 
mother, and that he needs to reach the mother in order to 
feed himself. He need not know the exact form of hunger, or 
what mother exactly is, or that there is a need to traverse the 
space between him and the mother, and so on. Another 
example is the requirement of consciousness in a file 
protection system. Such system needs to be conscious about 
the types of threats and the actions that it should take in the 
event of unauthorized access. On the other hand, it does not 
need to be aware of its capability of doing so. It also need not 
feel helpful or obliging or enslaved to do a boring job. 
The point that is being made is that multiple levels of 
abstractions may be defined regarding the expected level and 
form of consciousness. We do not agree with these 
arguments any more than with the concept of a virtual 
machine satisfying (1-3) as conscious. 
According to us, consciousness is an emerging 
phenomenon. It involves perception, learning, memory, 
decision making, and self-awareness.  Like intelligence, 
consciousness may be more or less developed.  For instance, 
social consciousness is less developed in an average 10 years 
old child than in an adult.  The child‟s frontal area 
responsible for empathy, responsibility, etc. is still 
developing. So, we would like to define a scalable concept of 
consciousness by describing a minimum set of conditions for 
a conscious mind. We would rather focus on realizing some 
form of machine consciousness, before delving into the 
levels and degrees of consciousness. 
B. Problems in the implementation of machine 
consciousness 
Many problems are encountered when implementing 
machine consciousness; some of them are presented next. 
First, the knowledge of possibilities is usually limited, and 
may not suffice for implementing the desired effect [1], [18], 
[23]. Further, virtual machines themselves are not well 
understood [24]. Another issue is the availability, adequacy 
and viability of the technology for implementation of 
consciousness [22], [42], [57], [59].  
A more important problem is the problem of formalism 
and abstraction. As discussed before, consciousness is built 
upon, concerns, and affects, the physical phenomena 
occurring in and around the virtual machines. Thus, various 
important questions arise. Should, to what extent, and which 
physical phenomena affect the consciousness? How these 
physical phenomena should be represented? How should 
they affect the consciousness?  In what form should the 
consciousness be implemented and how can it manifest itself 
(actions, alarms, emotions, adaptations, etc.)?  
This brings us to the importance of qualia, information, 
architecture, ontology, and information processing. These are 
discussed individually in the subsequent sections.  
III. 
QUALIA, ONTOLOGY AND THEIR IMPORTANCE FOR 
MACHINE CONSCIOUSNESS 
A. Qualia 
Following the difficulties in dealing with consciousness, 
researchers have come up with „qualia‟, which helps us to 
speak of consciousness in a simplified (though again 
incomplete) manner. If we think about an experience, we do 
it (or feel it) in terms of certain phenomenal qualities [15], 
[24], [52] typically referred to as „qualia‟.  Examples of 
qualia include perceptual experiences, bodily sensations, 
feelings of reactions/passions/emotions/moods, etc. 
Qualia are considered central to the mind-body problem 
and to the development of a proper understanding of 
consciousness. They are usually referred to as introspection 
[60] or awareness, which are different from consciousness 
[49]. It is notable that Sloman has specifically warned 
against using just introspection towards the means of 
knowing (becoming aware of) consciousness [23]. 
However, if qualia have a functional nature in the form of 
an intermediate causal occurrence between physical inputs 
(like body-damage) and outputs (like withdrawal behavior), 
then qualia should be multiply realizable [61], [62]. In other 
words, physically very different states may generate the 
same feeling. From the point of view of functionalism, the 
internal modeling of physical states becomes an important 
part of implementing consciousness.  
Such functional approach shall involve the capability to 
derive a connection between the possible sensory inputs, the 
intentions, and the possible outcomes. Therefore, it must deal 
with physical information and internal states (which may be 
in the form of direct or indirect manifestations of previous 
knowledge, experience, intentions, etc.) to give rise to a 
physical phenomenon of consciousness.  
B. Ontology  
Ontology 
is 
defined 
as 
the 
characterization 
of 
conceptualization. Philosophers and artificial intelligence 
scientists differ in the definition of ontology. Since our paper 
pertains to machine consciousness, we use the definition 
used in the AI community [23], [25], [38]. According to 
them, ontology is the study of various categories of abstract 
110
COGNITIVE 2010 : The Second International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-108-3

entities, events, processes, matters, etc., and their inter-
relationships. 
In this sense, ontology is an integral part of machine 
consciousness, as it formalizes the relationship between 
physically existing, measurable, decipherable, or deducible 
information to the abstract mental phenomena that the AI 
scientists wish to implement. Also, in this sense, it is directly 
related to the functional nature of qualia.  
It is evident that if the chosen ontology is unable to 
characterize the desired phenomenon, implementation of 
desired phenomenon may result in failure. Technically, the 
effect of choosing an unsuitable ontology is called 
ontological blindness [15], [23], [24]. It refers to the failure 
of the ontology in visualizing certain abstract characteristics 
that are pertinent to the understanding and realization of the 
desired phenomenon.  
Information, as used in the machine consciousness, is 
typically different from Shannon‟s information and simply 
means „content of relevance to something‟. The properties of 
information that are relevant and important for machine 
consciousness are listed below [15], [23]:   
 Information can be false 
 Items of information can stand in relations like 
consequence, contradiction and relevance 
 Items of information can be understood or misunderstood. 
 Information content is sometimes completely predictable 
 Information is non-physical (albeit physically realized), 
thus, it requires specialized methods for identifying, 
explaining, and processing. 
As agreed by most researchers, the ontology (used to 
study and link information to consciousness) and the 
information processing architecture (with consideration of 
the depth and breadth of information analysis, processing, 
interpretation, and consequent awareness, learning, planning, 
action initiation, etc.) are very important for realizing 
machine consciousness. As mentioned above, if information 
is picked wrongly, misrepresented, misinterpreted, or 
wrongly processed, it is expected to at least warp or distort 
consciousness, and in harsher conditions may lead to failure 
in the realization of desired phenomenon. 
Thus, it is very important to suitably understand, classify, 
distinguish, and group information. It is also important that 
information is cast into suitable architectures and processing 
methodologies so that the pertinent aspects are not neglected.  
Sloman [22]-[24] says that for implementation of 
machine consciousness, it is useful to deal with information 
in terms of its useful characteristics. Examples are various 
types of information, the forms it can take, the means of 
acquiring it, manipulating it, storing it, and communicating 
it, the purposes for which it can be used, the various ways of 
using it, etc. It is also important to choose suitable forms of 
representations, suitable algorithms, and identification of 
subsystems that process independently and concurrently. 
Their variations lead to the variation in the resulting 
consciousness achieved by these choices. For instance, such 
choices may determine the role of consciousness, in 
identifying possible events, actions, internal states, internal 
processes, and causal chain of happenings.  
It should be noted that a machine gathers the information 
from the external signals generated in the sensory units and 
the internal signals like pains/rewards and machine‟s 
motivations. Thus, these physical units of information 
processing are necessary for consciousness.  
IV. 
REACTIVE, DELIBERATIVE AND REFLECTIVE 
MECHANISMS OF INFORMATION PROCESSING 
Another impact of the choice of information and 
information processing is the possibility of reactive [23], 
[33], [37], [47], [50], deliberative [7], [23], [50], or reflective 
mechanisms [2], [4], [23], [28], [50]. Of course, the converse 
also holds, which means, that the type of mechanism desired 
has an impact on the choice of structural and processing 
aspects of information. 
Reactive mechanism means that the virtual machine 
always lives in present (never in past or future) and simply 
reacts or responds to the present stimuli. It is able to make 
sense of the presently acquired information and choose an 
action from presently possible options. It never learns from 
past, nor can it predict future possibilities, and thus it cannot 
plan, predict, foresee and adapt. It should be noted that if 
such machines are equipped with internal states they may be 
capable of influencing the future and may be adaptive. 
Deliberative mechanism, on the other hand, is equipped 
with an advanced ontology that can represent, store, process 
and relate to the possibilities (of input information, 
intermediary and internal states, as well as the possible 
actions). Thus, in some sense (depending upon the richness, 
depth and breadth, and design of ontology), it has the ability 
to learn, plan and govern its actions while being aware of the 
process and having active involvement in it. Though the 
importance of reactive mechanisms should not be 
understated, the deliberative mechanism forms the first link 
between contemporary AI and the desired machine 
consciousness. 
Reflective mechanism enables the virtual machine to 
reflect on its own actions, to self-monitor, self-examine, and 
self control. Self-control in reflective mechanism is a result 
of being aware of one-self and one‟s own actions. Thus, it is 
reflective mechanism that shall enable the machine to 
possess qualia, introspect itself, possess emotions, and relate 
to itself and its own existence. It also enables a machine to 
interpolate its self-knowledge in order to understand other 
machines. Thus, reflective mechanism is very helpful in 
realizing machine consciousness. 
V. 
MODELS OF MACHINE CONSCIOUSNESS 
We summarize the state of art research in this area by 
presenting some of the practical achievements in machine 
consciousness. We mention useful frameworks/architectures 
and some actually implemented systems for machine 
consciousness and their models.  
We begin with the physical definition of consciousness, 
its working mechanism and a computational model proposed 
by us [57]. We define machine consciousness as follows: 
“A machine is conscious if besides the required 
components for perception, action, and associative memory, 
111
COGNITIVE 2010 : The Second International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-108-3

it has a central executive that controls all the processes 
(conscious or subconscious) of the machine; the central 
executive is driven by the machine‟s motivation and goal 
selection, attention switching, learning mechanism, etc. and 
uses cognitive perception and cognitive understanding of 
motivations, thoughts, or plans. Thus, central executive, by 
relating cognitive experience to internal motivations and 
plans, creates self-awareness and conscious state of mind.” 
A simplified version of our proposed model of the 
conscious machine similar to the one presented in [57] is 
shown in Fig. 1. 
 
Fig. 1 Proposed computational model of consciousness. 
 
Our current efforts are to build such defined conscious 
machines, and improve their mental ability and level of 
intelligence. 
Baars and Franklin proposed a model of consciousness 
called the global workspace theory and developed an agent 
called Intelligent Distribution Agent (IDA) based on this 
model [6]-[8]. IDA is an agent that was designed for the U.S. 
navy for the purpose of collecting information from 
personnel, assessing the personnel on the basis of their 
performance and the issues they have had as human beings, 
and help in new task-allocation and problem identification. 
The global workspace theory [54] says “Consciousness is 
accomplished by a distributed society of specialists that is 
equipped with working memory, called a global workspace, 
whose contents can be broadcast to the system as a whole” 
(an argument refuted by Susan Blackmore [11]). Further 
Baars says [55], “Global workspace theory suggests a 
fleeting memory capacity in which only one consistent 
content can be dominant at any given moment”. The content 
of the memory is decided by the consciousness. The exact 
role of consciousness is to decide the dominant content of the 
memory. In our model, conscious mechanism that uses 
planning, thinking, goal creation and goal selection based on 
machine‟s motivation, plays a critical role in selecting the 
dominant content of memory [57].  
Rosenthal‟s Higher Order Thought (HOT) theory [56] 
says “We don't sense our conscious thoughts and sensations, 
since there's no distinctive sensory modality or sense organ 
for doing so. The only alternative is that we are conscious of 
our conscious thoughts, feelings, and sensations by having 
thoughts about them. These higher-order thoughts are 
themselves seldom conscious; so we are typically unaware of 
them”. According to us, there can be only one thought at any 
moment in the conscious state of a conscious machine. 
According to us [57], “Conscious machine central executive 
directs cognitive aspects of machine experiences but its 
operation is influenced by competing signals representing 
motivations, desires, and attention switching that are not 
necessarily cognitive or consciously realized. Central 
executive does not have any clearly identified decision 
making center. Instead, its decisions are the result of 
competition between signals that represent motivations, 
pains and desires. At any moment, competition between 
these signals can be interrupted by attention switching signal. 
Such signals constantly vary in intensity as a result of 
internal stimuli (e.g., hunger) or externally presented and 
observed opportunities. Thus, the fundamental mechanism 
that directs machine in its action is physically distributed as 
competing signals are generated in various parts of 
machine‟s mind. Further, it is not fully cognitive, since, 
before a winner is selected, machine does not interpret the 
meaning of competing signals” [57]. Hence, in our opinion, 
Rosenthal‟s HOT theory is based on fallacy that multiple 
thoughts exist in conscious machine simultaneously. 
Haikonen has proposed a cognitive architecture based on 
his theory that if the neural network is large enough and 
complicated enough, the traits of consciousness will 
eventually emerge on their own [16], [17], [19], [50]. Thus, 
he suggests that no algorithm specifically designed for 
implementing consciousness is necessary. According to us, 
his claim is vague and too generic. It does not give any 
concrete direction towards the realization of machine 
consciousness. According to us, mere presence of large and 
complicated neural network is not enough to realize machine 
consciousness. Apart from sufficiently large neural network 
and suitable cognitive architecture we do require other units 
such as motivation unit, attention switching unit, action 
monitoring unit, goal creation system unit, planning and 
thinking unit [57]. Although these units are fully distributed 
and are part of large interconnected network, they are 
functionally different in terms of their role in selecting a 
cognitive input, governing the planning process, and 
monitoring the control sequence for motor action.  
Sun proposed another architecture called CLARION [27, 
29]. CLARION is a two-level architecture in which the 
lower level concerns things, events, perceptions, reactive 
information, which cannot be associated with consciousness, 
and the higher level solely implements consciousness. These 
two levels are inter-connected with each other in both 
directions to form complete virtual machine architecture. A 
similar theory is the supramodular interaction theory 
proposed by Morsella [48], which models consciousness as 
integration of high level, specialized, multi-modal systems. 
Under this model, a distinction between the consciously 
penetrable 
and 
impenetrable 
modules 
(systems) 
is 
highlighted, and consciousness appears as a cross-talk 
among these modules.  No mechanism is implied how these 
112
COGNITIVE 2010 : The Second International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-108-3

cross-talks between different modules or levels are organized 
and how a machine reflects its own actions. 
Taylor proposed corollary discharge of attention 
movement (CODAM) model, that emphasizes the role of 
attention and change in attention for implementing 
consciousness [31]. We do agree that attention and attention 
switching mechanism is important to realize machine 
consciousness. However, other factors also need to be 
considered [57]. 
Sloman, Chrisley, and their team have proposed the 
cognitive and affective (CogAff) schema [22]-[24], which is 
a generalized schema applicable to a wide range of AI and 
consciousness architectures. It is able to accommodate 
multiple hierarchical levels as well as independent lateral 
modules in its architecture. It is capable of representing 
reactive, 
deliberative, 
as 
well 
as 
reflective 
(meta-
management) systems, thus providing a broad framework for 
comparison, judgment, and possibilities exploration of AI 
and consciousness architectures.  
Some AI robots or systems have also been made to 
explore, understand and implement machine consciousness, 
which include CRONOS (a human like musculoskeletal 
robot) with the aim of phenomenal consciousness [47], 
Cyber Child [46] (a test bed for machine consciousness), 
Khepera models/robots [40], etc. A good overview is 
provided in [50]. 
VI. 
CONCLUSION 
Based on our review of the research done on machine 
consciousness, we can make the following observations. 
Like intelligence, consciousness is a property of a 
physical mind not a meta-physical phenomenon.  It can be 
alive or not but it requires awareness and intelligence as 
illustrated in Fig. 2. 
  
 
 
Fig. 2 Consciousness centered view of intelligent systems 
Given its complicated nature, a unified explanation of 
consciousness is difficult. Yet, it is much more difficult to 
realize it in machines. Thus, in our opinion, though we have 
to use discretely defined, crisp, and formal theories and 
architectures, we also have to incorporate the gray area, the 
possibility for the system to evolve by itself and find its own 
consciousness. Our work in [57] is a step towards this aim. 
Large part of the work in machine consciousness is 
inspired by biological systems. Our idea is also based on it.  
The idea is actually similar to the appearance of 
consciousness in human beings that we can understand the 
best.  From the conception of a fetus, through its 
development as a human being, till its death, a human being 
is taught a lot of things, including moral behavior, 
importance of social existence, relationship with nature, 
science, mathematics, sports, art, and so on. But no human 
being is explicitly taught to be conscious. There might be 
some intelligence/thinking enhancing exercises. However, 
these are enhancements only, while the basic mechanism is 
provided and consciousness develops itself. Furthermore, 
there is no basic consciousness training, and in its 
fundamental meaning consciousness is not a thought 
phenomenon.  Higher levels of consciousness (like social 
awareness) may be trained, but a mechanism for 
consciousness is a property of the mind and cannot be taught. 
In short, a large part of what we think as consciousness 
emerges through social interactions if a proper architecture, 
mechanism and functional blocks required for consciousness 
are available [57]. We conclude that consciousness is a 
physical phenomenon which can be realized in machines. 
REFERENCES 
[1] M. L. Anderson, "Embodied Cognition: A field guide," Artificial 
Intelligence, vol. 149, pp. 91-130, 2003. 
[2] M. L. Anderson and T. Oates, "A review of recent research in 
metareasoning and metalearning," AI Magazine, vol. 28, pp. 7-16, 
2007. 
[3] M. L. Anderson, "Circuit sharing and the implementation of 
intelligent systems," Connection Science, vol. 20, pp. 239-251, 2008. 
[4] M. L. Anderson and D. Perlis, "What puts the "meta" in 
metacognition?," Behavioral and Brain Sciences, vol. 32, pp. 138-
139, 2009. 
[5] J. Newman, B. J. Baars, and S. B. Cho, "A neural global workspace 
model for conscious attention," Neural Networks, vol. 10, pp. 1195-
1206, 1997. 
[6] S. Franklin, A. Kelemen, and L. McCauley, "IDA: A cognitive agent 
architecture," in Proceedings of the IEEE International Conference 
on Systems, Man and Cybernetics, San Diego, CA, USA, 1998, pp. 
2646-2651. 
[7] B. J. Baars and S. Franklin, "How conscious experience and working 
memory interact," Trends in Cognitive Sciences, vol. 7, pp. 166-172, 
2003. 
[8] B. J. Baars and S. Franklin, "An architectural model of conscious and 
unconscious brain functions: Global Workspace Theory and IDA," 
Neural Networks, vol. 20, pp. 955-961, 2007. 
[9] S. J. Blackmore, "Meme machines and consciousness," Journal of 
Intelligent Systems, vol. 9, pp. 355-376, 1999. 
[10] S. Blackmore, "Evolution and memes: The human brain as a selective 
imitation device," Cybernetics and Systems, vol. 32, pp. 225-255, 
2001. 
[11] S. Blackmore, "There is no stream of consciousness," Journal of 
Consciousness Studies, vol. 9, pp. 17-28, 2002. 
[12] S. Blackmore, "Consciousness in meme machines," Journal of 
Consciousness Studies, vol. 10, 2003. 
[13] R. Chrisley, "Embodied artificial intelligence," Artificial Intelligence, 
vol. 149, pp. 131-150, 2003. 
[14] R. Clowes, S. Torrance, and R. Chrisley, "Machine consciousness - 
Embodiment and imagination," Journal of Consciousness Studies, 
vol. 14, pp. 7-14, 2007. 
[15] R. Chrisley, "Philosophical foundations of artificial consciousness," 
Artificial Intelligence in Medicine, vol. 44, pp. 119-137, 2008. 
113
COGNITIVE 2010 : The Second International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-108-3

[16] P. O. A. Haikonen, "Towards associative non-algorithmic neural 
networks," in Proceeding of the IEEE International Conference on 
Neural Networks, Orlando, FL, USA, 1994, pp. 746-750. 
[17] P. O. A. Haikonen, "Modular neural system for machine cognition," 
in Proceedings of the International Joint Conference on Neural 
Networks, Como, Italy, 2000, pp. 47-50. 
[18] P. O. A. Haikonen, "Essential issues of conscious machines," Journal 
of Consciousness Studies, vol. 14, pp. 72-84, 2007. 
[19] P. O. A. Haikonen, "Reflections of consciousness: The mirror test," in 
AAAI Fall Symposium - Technical Report, Arlington, VA, 2007, pp. 
67-71. 
[20] E. T. Rolls, "Consciousness in neural networks?," Neural Networks, 
vol. 10, pp. 1227-1240, 1997. 
[21] E. 
T. 
Rolls, 
"A 
computational 
neuroscience 
approach 
to 
consciousness," Neural Networks, vol. 20, pp. 962-982, 2007. 
[22] A. Sloman, "What kind of cognitive architecture does an emotional 
agent need?," International Journal of Psychology, vol. 31, pp. 4001-
4001, 1996. 
[23] A. Sloman and L. Chrisley, "More things than are dreamt of in your 
biology: Information-processing in biologically inspired robots," in 
International Workshop on Biologically Inspired Robotics, Bristol, 
England, 2002, pp. 145-174. 
[24] A. Sloman and R. Chrisley, "Virtual machines and consciousness," 
Journal of Consciousness Studies, vol. 10, pp. 133-172, 2003. 
[25] J. Chappell and A. Sloman, "Natural and artificial meta-configured 
altricial information-processing systems," International Journal of 
Unconventional Computing, vol. 3, pp. 211-239, 2007. 
[26] R. Sun, "Learning, action and consciousness: A hybrid approach 
toward modelling consciousness," Neural Networks, vol. 10, pp. 
1317-1331, Oct 1997. 
[27] R. Sun, "Cognitive Architectures and Multi-agent Social Simulation," 
in 8th Pacific Rim International Workshop on Mult-Agents (PRIMA 
2005), Kuala Lumpur, MALAYSIA, 2005, pp. 7-21. 
[28] R. Sun, X. Zhang, and R. Mathews, "Modeling meta-cognition in a 
cognitive architecture," Cognitive Systems Research, vol. 7, pp. 327-
338, 2006. 
[29] R. Sun, "The importance of cognitive architectures: an analysis based 
on CLARION," Journal of Experimental & Theoretical Artificial 
Intelligence, vol. 19, pp. 159-193, 2007. 
[30] J. G. Taylor, "Neural networks for consciousness," Neural Networks, 
vol. 10, pp. 1207-1225, 1997. 
[31] J. G. Taylor, "CODAM: A neural network model of consciousness," 
Neural Networks, vol. 20, pp. 983-992, 2007. 
[32] M. 
Velmans, 
"Is 
human 
information-processing 
conscious," 
Behavioral and Brain Sciences, vol. 14, pp. 651-668, 1991. 
[33] M. Velmans, "A reflexive science of consciousness," Ciba 
Foundation Symposium, vol. 174, pp. 81-91, 1993. 
[34] M. Velmans, "When perception becomes conscious," British Journal 
of Psychology, vol. 90, pp. 543-566, 1999. 
[35] M. Velmans, "Making sense of causal interactions between 
consciousness and brain," Journal of Consciousness Studies, vol. 9, 
pp. 69-95, 2002. 
[36] M. Velmans, "Why conscious free will both is and isn't an illusion," 
Behavioral and Brain Sciences, vol. 27, p. 677, 2004. 
[37] M. Velmans, "Where experiences are: Dualist, physicalist, enactive 
and 
reflexive 
accounts 
of 
phenomenal 
consciousness," 
Phenomenology and the Cognitive Sciences, vol. 6, pp. 547-563, 
2007. 
[38] M. Velmans, "How to define consciousness: And how not to define 
consciousness," Journal of Consciousness Studies, vol. 16, pp. 139-
156, 2009. 
[39] C. Browne, R. Evans, N. Sales, and I. Aleksander, "Consciousness 
and neural cognizers: A review of some recent approaches," Neural 
Networks, vol. 10, pp. 1303-1316, 1997. 
[40] T. Kitamura and K. Ono, "An architecture of emotion-based behavior 
selection for mobile robots," in 5th International Conference on 
Simulation of Adaptive Behavior, Zurich, Switzerland, 1998, pp. 671-
690. 
[41] S. Densmore and D. Dennett, "The virtues of virtual machines (Paul 
Churchland)," Philosophy and Phenomenological Research, vol. 59, 
pp. 747-761, 1999. 
[42] R. H. Schlagel, "Why not artificial consciousness or thought?," Minds 
and Machines, vol. 9, pp. 3-28, 1999. 
[43] J. McCarthy, "Free will - Even for robots," Journal of Experimental 
and Theoretical Artificial Intelligence, vol. 12, pp. 341-352, 2000. 
[44] G. Buttazzo, "Artificial consciousness: Utopia or real possibility?," 
Computer, vol. 34, pp. 24-30, 2001. 
[45] F. Crick and C. Koch, "A framework for consciousness," Nature 
Neuroscience, vol. 6, pp. 119-126, 2003. 
[46] R. M. J. Cotterill, "CyberChild - A simulation test-bed for 
consciousness studies," Journal of Consciousness Studies, vol. 10, pp. 
31-45, 2003. 
[47] O. Holland, "The future of embodied artificial intelligence: Machine 
consciousness?," in Lecture Notes in Artificial Intelligence (Subseries 
of Lecture Notes in Computer Science), Dagstuhl Castle, 2004, pp. 
37-53. 
[48] E. Morsella, "The function of phenomenal states: Supramodular 
interaction theory," Psychological Review, vol. 112, pp. 1000-1021, 
2005. 
[49] Z. L. Torey, "The immaculate misconception," Journal of 
Consciousness Studies, vol. 13, pp. 105-110, 2006. 
[50] D. Gamez, "Progress in machine consciousness," Consciousness and 
Cognition, vol. 17, pp. 887-910, 2008. 
[51] D. Dennett, "The milk of human intentionality," Behavioral and 
Brain Sciences, vol. 3, pp. 428-430, 1980. 
[52] D. Dennett, "Recent work in philosophy of interest to AI," Artificial 
Intelligence, vol. 19, pp. 3-5, 1982. 
[53] D. Dennett, "Are we explaining consciousness yet?," Cognition, vol. 
79, pp. 221-237, 2001. 
[54] B. J. Baars, A cognitive theory of consciousness, Cambridge 
University Press, 1998. 
[55] B. J. Baars, “The conscious access hypothesis: Origins and recent 
evidence,” in Trends Cogn. Science, Vol 6, pp. 47–52, 2002. 
[56] D. M. Rosenthal, The nature of Mind, Oxford University Press, 1991. 
[57] J. A. Starzyk and D. K. Prasad, " Machine Consciousness: A 
Computational Model," Brain-inspired Cognitive Systems (BICS 
2010), Madrid, Spain, 14-16 Jul 2010. 
[58] V. C. Muller, "Is there a future for AI without representation?," Minds 
and Machines, vol. 17, pp. 101-115, 2007. 
[59] R. Sun and C. X. Ling, "Computational cognitive modeling, the 
source of power, and other related issues," in Workshop on 
Computational Modeling - The Source of Power, at the 13th National 
Conference on Artificial Intelligence (AAAI-96), Portland, Oregon, 
1996, pp. 113-120. 
[60] P. M. Churchland, "Reduction, Qualia, and the Direct Introspection of 
Brain States," The Journal of Philosophy, vol. 82, pp. 8-28, 1985. 
[61] V. S. Ramachandran and W. Hirstein, "Three laws of qualia: What 
neurology tells us about the biological functions of consciousness," 
Journal of Consciousness Studies, vol. 4, pp. 429-457, 1997. 
[62] W. G. Lycan, "Consciousness explained - Dennett, D. C.," 
Philosophical Review, vol. 102, pp. 424-429, 1993.  
 
114
COGNITIVE 2010 : The Second International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2010               ISBN: 978-1-61208-108-3

