A Pointing Device for 3D Interactive Spherical Displays 
Oky Dicky Ardiansyah Prima, Katsuyoshi Hotta, Rintaro Takahashi, Hisayoshi Ito  
Graduate School of Software and Information Science, Iwate Prefectural University 
152-52 Sugo Takizawa, Japan 
email: prima@iwate-pu.ac.jp { g236q004, g231r019 }@s.iwate-pu.ac.jp, hito@iwate-pu.ac.jp 
 
 
Abstract—Three-Dimensional 
(3D) 
displays 
have 
been 
developed to allow users to view 3D objects from any angle. 
There are several input interfaces for interacting with this 
display, such as joysticks and gesture interfaces. However, in 
order to properly interact with the 3D objects in the display, the 
input interface needs to obtain its own 3D location from the 3D 
tracking device. In this study, we propose a low-cost pointing 
device for a 3D interactive display using an infrared camera and 
a smartphone instead of a 3D tracking device.  The camera is set 
up at the bottom of the display to detect the contact point 
between the pointing device and the display surface. The 
pointing device gets its 3D orientation from the smartphone's 
built-in Inertial Measurement Unit (IMU). The proposed 
pointing device enables the user to see the auxiliary line from 
the device tip from different angles. We demonstrated some 3D 
pointing applications, such as selecting objects inside the display 
to show the usability of the proposed pointing device. 
Keywords-VR; 3D stylus; spherical display; virtual reality; 
perception. 
 
I.  INTRODUCTION 
The increasing use of Three-Dimensional (3D) contents in 
the media and entertainment industry is providing a catalyst 
for the development of devices to effectively represent and 
interact with this content. This study extends our previous 
research on a 3D pointing device [1]. Over the past decade, 
this new development has boosted and refreshed interest in 
Virtual Reality (VR). VR devices such as the Oculus Rift and 
HTC Vive [2], which are capable of full-body tracking, 
provide users with a natural interaction with 3D contents in a 
virtual space. 
Apart from VR devices, the development on 3D displays 
has taken a big step forward with the introduction of light field 
displays. These displays are generally intended to 
create motion parallax and stereoscopic disparity, allowing 
the observer to perceive the scene in 3D without the need to 
wear obtrusive glasses [3]. Looking Glass [4] is a currently 
available holographic display system that generates 45 distinct 
views of a 3D scene using the light field technique. In the 
future, rapid advances in electronics, optics and photonics are 
bringing more true 3D display technologies to the market [5]. 
Another attempt to achieve true 3D representation is the 
use of non-planar displays instead of flat displays. 3D 
Spherical Display (3DSD) has an advantage on representing 
3D objects to be seen from any angle. SnowGlobe [6] and 
Spheree [7] were developed as perspective-corrected 3DSDs 
based on a non-planar Two- to Three-Dimensional (2D-to-
3D) mapping technique. SnowGlobe uses a single projector 
and a hemispherical mirror to reflect the image onto the 
display surface. In contrast, Spheree uses multiple calibrated 
projectors to project the image directly onto the display 
surface. Both displays use a 3D tracking device to track the 
location of the user's head relative to the display, providing a 
motion-parallax-corrected perspective of the objects on the 
display. CoGlobe [8][9][10], an upgrade of the Spheree, 
supports two users to view 3D contents from each own 
viewpoint using modified active shutter glasses. 3DSDs may 
have overcome the limited situation awareness of headset VR. 
3DSDs can be equipped with a touch interface by optically 
tracking the user’s rear illuminated fingers that appear as blob 
images when touching the display surface [11]. Each center of 
the blob has a 3D coordinate (x, y, z) originating from the 
center of the 3DSD. CoGlobe demonstrated several 3D games 
using 3D pointing devices such as pointing devices and 
rackets. The OptiTrack system [12] tracked the 3D location 
and orientation (x, y, z, pitch, yaw, roll) of multiple passive 
reflective markers attached to those devices. Hereinafter, the 
information about the 3D location and orientation is simply 
referred to as Six-Degree of Freedom (6DoF). This system 
also uniquely identified and labeled each group of markers 
attached to each device.  
Attaching multiple reflective markers to a pointing device 
can degrade its functionality. In addition, in order to estimate 
the 6DoF, those markers needed to be connected to each other 
like branches, which increased the shape of the pointing 
device and made it difficult to use. Instead of reflective 
markers, Augmented Reality (AR) markers can be used to 
estimate the 6DoF of a pointing device, such as the 
DodecaPen [13]. However, due to the shape of the 3DSD, the 
pointing device is often hidden from the camera used to track 
the AR markers, making it impossible to estimate the 6DoF. 
In this study, we propose a pointing device suitable for a 
spherical display by using a touch interface to acquire the 3D 
location and an Inertial Measurement Unit (IMU) to acquire 
the orientation. The orientation data is calibrated using the 
Motion Platform and the corrected data is sent to the computer 
controlling the 3DSD and combined with the 3D location 
obtained from the touch interface.  
The rest of this paper is organized as follows.  In section 
II, we describe the 3DSD hardware and software developed 
for this study. Section III introduces our approach to 
implement the pointing device. Section IV describes our 
experiment results. Section V discusses about the further 
enhancements to the proposed pointing device. Finally, 
Section VI presents our conclusions and future works. 
284
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

II. THE 3D SPHERICAL DISPLAY FOR THIS STUDY 
A 3DSD was created for this study using a single projector. 
Figure 1(a) shows the hardware design of the 3DSD. We used 
a spherical acrylic lighting case (acrylic globe) as the display 
material. The interior of the display was painted with rear-
projection acrylic paint, so that images projected from inside 
are visible from the outside of the display. A 3D projector with 
4K resolution to was used to generate a high-resolution image 
onto the display. A fisheye lens was installed between the 
projector and the acrylic globe to spread the image from the 
projector to the entire surface of the interior of the spherical 
acrylic. In order to make the display touchable, we placed a 
wide-lens infrared camera at the bottom. Since the axes of the 
camera and of the fisheye lens are different, we aligned these 
axes using an affine transformation. For the infrared 
illumination, we installed a high-power infrared LED at the 
opposite side of the camera. Figure 1(b) shows the globe on 
our 3DSD.  
To make it easier for users to visualize 3D objects, we 
have created a software program based on the Unity 3D 
Platform that allows any 3D application running on the 
platform to be displayed on the 3DSD. Figure 2 shows how 
two users view a 3D scene from different perspectives. As 
shown in the figure, the software program generated images 
for each viewpoint and projected those images onto the 
surface of the 3DSD. The two images appear to be 
superimposed, but each image is displayed at a different 
timing. Therefore, if the user uses shutter glasses, the images 
at each timing can be viewed separately. Currently, the HTC 
 
 
(a) The hardware design of our 3DSD 
(b) The globe 
 
Figure 1. Our 3D spherical display (a) and the globe (b) projected onto the display. 
 
Projector
Acrylic globe
Fish lens
Illumination 
 
Figure 2. Our 3DSD software program based on the Unity 3D platform. 
User #1
User #2
User #1
User #2
285
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Vive Tracker [14] needs to be attached to the user's head to 
determine the user's perspective. In the future, we plan to 
simplify the tracking system by replacing it with a 3D face 
tracking based on a vision camera. 
 
III. OUR PROPOSED POINTING DEVICE 
Figure 3 shows a scene in which the proposed pointing 
device is used on a 3DSD. The camera for the 3DSD touch 
interface captures any blob images on the 3DSD surface. The 
proposed pointing device does not use a 3D tracking device 
such as motion capture, but acquires the location based on the 
blob image detected by the infrared camera and the orientation 
based on the IMU. The details of the calculation to determine 
the location and orientation of the pointing device are as 
follows. 
A. Detecting the location of the tip of the pointing device 
When the tip of the pointing device touches the surface of 
the 3DSD, the blob image corresponding to the touched area 
is extracted to localize the tip's location. This process involves 
binarizing the blob image to form a contour at the boundary 
and fitting an ellipse to the contour. OpenCV (opencv.org), an 
open-source computer vision software library, is used for this 
calculation. Here, the coordinate of the tip of the pointing 
device is calculated as the center coordinate of the ellipse.  
We perform the following coordinate transformations to 
convert the coordinates of the tip of the pointing device 
mentioned above into the 3DSD's 3D coordinate system. At 
first, since the image in the 3DSD is projected according to 
Lambert Azimuthal Equal-Area Projection, the coordinates of 
the tip of the pointing device are converted to this projection. 
Next, these coordinates are re-projected into the Universal 
Transverse Mercator (UTM) coordinate system and mapped 
onto spheroid, which represents the 3D coordinate system of 
the 3DSD. Figure 4 shows a series of the above 
transformations. The world map was used as the background 
image projected onto the 3DSD to make a better 
understanding of these transformations.  
In our system, the blob image is extracted for everything 
that touches the surface of the 3DSD, but currently, we do not 
distinguish between the user's fingertip or the pointing device. 
Therefore, when using the prototype of this pointing device, 
the 3DSD should not be touched except by this device. 
B. Getting the rotation of the pointing device 
In this study, we prototyped the pointing device using a 
mobile device, the iPhone 7. As shown in Figure 5, we set up 
a pointing stick on the top of the iPhone 7 and attached a 
rubber to the tip to touch the 3DSD. The pointing device gets 
 
Figure 3. The mechanism of the proposed pointing device. 
 
Infrared camera
Orientation
Location
Infrared LED
Pointing
direction
Projector with a fisheye lens
Blob images
IMU
 
Figure 4. 3D coordinates of the tip of the pointing device. 
 
Lambert Azimuthal Equal-Area
UTM coordinate system
Speroid
3DSD’s 3D coordinate system
(x, y, z)
Tip’s location
r
286
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

its own orientation (pitch, yaw, roll) from the iPhone 7's built-
in IMU. This information is transmitted to the 3DSD via 
Bluetooth of the iPhone 7. In this way, this integration of 
location and orientation information allows the pointing 
device to have 6DoF information. Using this information, the 
3DSD software system will be able to calculate the posture of 
the pointing device and project an auxiliary line from its tip 
according to the user's perspective. Here, the length of the 
auxiliary line can be adjusted with the audio buttons on the 
iPhone 7 or the Graphical User Interface (GUI) on its screen. 
The iPhone 7's built-in IMU includes a compass and a 
gyroscope sensor. However, to make the pointing device 
works properly, these sensor values must be calibrated in 
advance. In this study, we constructed a simplified Stewart 
Platform (Motion Platform) to calibrate the rotation 
information obtained from the IMU of the iPhone 7. Although 
this simplified platform has only three actuators instead of the 
six used in the original form, it is enough as a validation device 
for this study. 
Figure 6 shows the calibration of the iPhone 7 on the 
Motion Platform. The calibration analyzes the difference 
between the rotation angle from the IMU and the rotation 
angle of the Platform. For calibration, the Motion platform 
was used to perform 27 arbitrary motions only on pitch and 
roll angles. The reason for not changing direction to the yaw 
angle is to measure the stability of the IMU when measuring 
the yaw angle. Since the changes in the yaw angle are the 
result of the changes in pitch and roll, there will be less 
changes in the yaw angle. Figure 7 shows orientation angles 
derived from the IMU after being calibrated using the Motion 
Platform. Figure 8 shows the relationship between the rotation 
angle of the Motion Platform and the rotation angles of the 
iPhone 7. Despite the variability of the pitch and roll angles of 
about ±10 degrees, the variability of the yaw angle is limited 
 
 
Figure 5. The proposed pointing device for the 3DSD.  
Figure 6. The Motion Platform built for this study. 
 
 
+
ー
+
ー
Changing the length of the 
auxiliary line (volume buttons)
Changing the length of the 
auxiliary line (GUI slider)
The tip of the pointing device
Pitch
Roll
Yaw
iPhone 7
 
Figure 7. Values of orientation angles derived from the IMU 
after being calibrated using the Motion Platform. 
Motion Platform
10.0
7.5
5.0
2.5
0.0
-2.5
-5.0
-7.5
-10.0
10.0
7.5
5.0
2.5
0.0
-2.5
-5.0
-7.5
-10.0
10.0
7.5
5.0
2.5
0.0
-2.5
-5.0
-7.5
-10.0
1          5         10        15         20        25
Degree
Degree
Degree
Pitch
Roll
Yaw
Data
iPhone 7
287
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

to about ±2 degrees. There was a high correlation between the 
pitch angles, 𝑟(25) = .99 , 𝑝 < .001 , with a 𝑅! = .979 . 
Similarly, there was also a high correlation between the roll 
angles, 𝑟(25) = .99 , 𝑝 < .001 , with a 𝑅! = .998 , and 
between the yaw angles, 𝑟(25) = .78, 𝑝 < .001, with a 𝑅! =
.602. These results indicate that the IMU built into the iPhone 
7 is accurate enough to perform interactions that require 
absolute orientation. 
Figure 9 shows the proposed pointing device that works 
with the 3DSD. The HTC Vive Tracker was attached to the 
camera to capture this image from the correct perspective. To 
give users a strong impression of motion parallax, we attached 
a wallpaper pattern on the back of the virtual fish-tank. From 
the camera's perspective, the auxiliary line of the pointing 
device extends in the correct direction according to the 
orientation of the device. The volume button or slider as 
shown in Figure 5 allows the user to adjust the length of this 
line. 
 
IV. EXPERIMENTAL RESULTS 
We evaluated the proposed pointing device in terms of 
accuracy, pointing stability, and user experience. A 51 cm 
diameter 3DSD was used in the experiment. The display, 
pointing device and user perspective coordinate systems were 
calibrated using the HTC Vive tracker. The 3DSD runs on a 
desktop computer with a 3.6 GHz CPU, 32 GB of RAM and 
a GTX980Ti graphics card.  
A. Pointing Accuracy 
In the 3DSD, the accuracy of the pointing device is 
required for operations such as selecting a 3D object. In the 
case of the 3DSD with a diameter of 51 cm, one degree of 
 
 
 
Figure 9. Our working pointing device. 
 
Figure 8. The relationships between orientation angles of the Motion Platform and the iPhone 7. 
 
R² = 0.9787
-10
-8
-6
-4
-2
0
2
4
6
8
10
-10
-8
-6
-4
-2
0
2
4
6
8
10
iPhone 7 (Degree)
Motion Platform (Degree)
Pitch
R² = 0.9975
-10
-8
-6
-4
-2
0
2
4
6
8
10
-10
-8
-6
-4
-2
0
2
4
6
8
10
iPhone 7 (Degree)
Motion Platform (Degree)
Roll
R² = 0.6019
-10
-8
-6
-4
-2
0
2
4
6
8
10
-10
-8
-6
-4
-2
0
2
4
6
8
10
iPhone ７ (Degree)
Motion Platform (Degree)
Yaw
R 2 = 0.979
R 2 = 0.998
R 2 = 0.602
288
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

pointing error corresponds to a deviation of about 1 cm at the 
center of the 3DSD. In order to evaluate the accuracy of 
pointing, 12 arbitrary locations on the 3DSD surface were 
selected, and the pointing device was set from these locations 
towards the center of the display. At each point, vectors 
representing the pointing direction were recorded for two 
seconds, and the difference with the vector connecting the 
pointing point and the center of the 3DSD, hereafter referred 
to as the ground truth vector, was analyzed. Each 
measurement was sampled at 60 Hz, resulting in 120 vectors. 
Figure 10 shows the 12 locations used for the evaluation 
and vectors of the pointing device pointing to the center of the 
display from those locations (blue lines). The red lines show 
the ground truth vectors. A small discrepancy can be observed 
between the red and blue lines. Here, we took the inner 
product of both vectors and quantified the difference in angles. 
Figure 11 shows the distribution of the angle differences at 
 
Figure 10. The resulting vectors (blue) from the pointing device tip and their 
corresponding ground truth (red). 
 
Figure 12. Schematic diagram of the pointing experiment in yaw 
angle direction. 
 
 
x [m]
z [m]
y [m]
#1
#12
#11
#10
#9
#8
#7
#6
#5
#4
#3
#2
Pointing device
Fixed point
 
 
Figure 11. The distribution of the angle differences at each of the 12 
locations. 
 
Figure 13. Intersection points of the vectors from the pointing 
device tip and the display surface. 
 
 
Difference in angle (degree)
Location
x [m]
z [m]
y [m]
289
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

each of the 12 locations. The circle inside the interquartile 
range represents the mean value of the data. The median 
values and interquartile ranges varied by locations. However, 
the median and mean values in total measured 1.08 and 1.05 
degrees, respectively. These values correspond to about 1 cm 
at the center of the 3DSD. The range of quartiles is at most 1 
degree or less, except at the sixth location. At the sixth 
location, pointing diagonally upward might be the cause of the 
increase in the differences. In practical use, most users are not 
aware of any differences within this range. The one-way 
Analysis of Variance (ANOVA) shows that there is a 
significant effect of the location on the pointing accuracy 
[F(11, 1392) = 2.26, p < .01]. 
B. Pointing Stability 
In the previous calibration using the Motion Platform, the 
correlation between the IMU’s yaw angle and the measured 
yaw angle was the lowest compared to the pitch and roll 
angles. In order to investigate how this result affects the 
stability of the pointing operation, we performed an 
experiment of pointing solely in the yaw direction. 
Figure 12 shows a schematic diagram of the experiment. 
The pointing device is placed on a board placed on the side of 
the 3DSD to allow pointing only in the yaw angle direction. 
Here, the pointing device was rotated ±135 degrees with 
respect to the center of the 3DSD and the vectors representing 
the pointing direction were recorded. In total, 1,147 vectors 
were collected during the operation.  
In order to verify the pointing stability, we calculated the 
intersection points (x, y, z) of these vectors with the 3DSD and 
fitted a plane 𝑓(𝑥, 𝑦) = 𝑧 = 𝑎𝑥 + 𝑏𝑦 + 𝑐 to these points using 
the least squares method. The maximum absolute value of the 
residual was measured 0.20 cm. This small error means that 
the pointing vector changes smoothly in the yaw direction and 
the proposed pointing device is sufficiently stable. Figure 13 
shows the distribution of the intersection points on the 3DSD.  
C. Object Selection 
We conducted an experiment to select objects in 3DSD 
using the proposed pointing device. For the experiment, we 
constructed a virtual fish-tank in 3DSD and placed four fishes 
at a distance from each other. We measured the time required 
to select every fish.  
Nine students of the Faculty of Software and Information 
Science of Iwate Prefectural University participated in the 
experiment. Each subject randomly selected each fish using 
the proposed pointing device. However, in order to avoid gaps 
in results due to differences in pointing distance between 
TABLE I. 
TIME BEING TAKEN BY THE SUBJECT TO POINT EACH FISH. 
Subject 
ID 
Fish 
A 
B 
C 
D 
1 
8.4 
3.4 
5.5 
4.9 
2 
7.5 
2.6 
3.6 
3.3 
3 
4.3 
2.5 
5.1 
4.6 
4 
3.5 
10.6       
4.4 
4.8 
5 
8.4 
4.3 
5.3 
4.9 
6 
5.9 
4.6 
4.4 
5.2 
7 
7.8 
3.5 
4.1 
3.8 
8 
6.9 
3.2 
2.9 
3.8 
9 
10.6       
3.7 
3.7 
4.8 
Mean 
7.03      
4.26      
4.34      
4.45     
StDev. 
2.204    
2.458    
0.844    
0.663   
(seconds) 
 
Figure 14. The virtual fish-tank for the experiment. 
 
A
C
B
D
Touch location
290
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

subjects, we had all subjects start pointing at the same point. 
When the selection was successful, the color of the fish 
changed automatically and the time since the start of pointing 
was recorded. The experiment ended when all four fishes have 
been pointed out successfully.  
Figure 14 shows the virtual fish-tank for the experiment. 
The “touch location” is a location to start to point to a fish, 
regardless of the location of the fish. The four fishes (A, B, C, 
D) are 34.9 cm, 22.6 cm, 23.7 cm, and 21.8 cm away from this 
point, respectively. 
Table I shows the time being taken by each subject to 
select each fish. While the average time required to select fish 
in B, C, and D takes about 4 seconds, most subjects spent the 
most time to successfully select the fish A. We considered that 
it is a reasonable result since the fish A is the farthest away 
from the touch location. When selecting an object that is far 
away, a small amount of slippage in the pointing angle can 
 
(a) Volumetric data 
 
 
(b) Motion capture data 
 
 
(c) Virtual fish-tank 
 
 
(d) 3D digital globe 
 
Figure 15. Various 3D contents that can be displayed onto the 3DSD. 
 
291
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

reduce the pointing accuracy to the object. The one-way 
Analysis of Variance (ANOVA) shows that there is a 
significant effect between the pointing locations on the time 
taken by subjects to successfully point to a fish [F(3, 32) = 
4.46, p < .01]. 
V. DISCUSSION 
Various spherical displays have been proposed in the past 
decade. The main purpose of developing these displays was to 
allow users to view content from various angles. To our 
knowledge, Sphere [15] is the first 2D spherical display that 
introduced multi-touch interactions, such as dragging, scaling, 
flicking, and rotating. This multi-touch function provides the 
same level of interaction as that of an ordinary planar display. 
However, as the use of 3D content increases, the direction of 
development of spherical displays has changed from 2D to 3D. 
Currently, there are some companies producing spherical 
displays, such as Global Imagination [16], PufferSphere [17], 
and ArcScience [18].   
In 2011, SnowGlobe [6] was demonstrated to be the first 
3DSD with a fish-tank VR. It has the same multi-touch 
functionality as the Sphere, allowing for interaction with 3D 
objects. However, interaction with 3D objects is likely to take 
advantage of the depth information, which requires a complex 
system to realize. Spheree [7] demonstrated 3D gestures using 
an optical motion capture device. Currently, CoGlobe [8] 
might be the most advanced 3DSD that allows two users to 
interact with the display at the same time. However, as long 
as the 3D interactions rely on expensive motion capture 
devices, the 3DSD may not become widely available. A 3D 
pen stylus that does not require a motion capture device such 
as the TouchTM Haptic Device [19] may be used to perform 
3D interaction with the display within a certain range. 
However, its arm limits the work area. 
This study extends the touch interface to enable 3D 
interactions without the use of a motion capture device. For 
this purpose, we have built a new 3DSD that has the same 
functionality as CoGlobe. Our proposed pointing device 
combines the 3D position on the display surface detected by 
the touch interface with the orientation measured by the IMU 
to obtain the 6DoF. Due to the high precision of recent IMUs, 
such as the one built into the iPhone 7, we believe that it is 
reasonable to obtain the orientation information of the 
pointing device from the IMU. Our calibration results using 
the Motion Platform indicate that the IMU built into the 
iPhone 7 is accurate enough to perform interactions that 
require absolute orientation. 
The 3DSD has the potential to represent a variety of 3D 
contents, such as volumetric data [20], motion capture data, 
virtual fish-tank, and 3D digital globe, as shown in Figure 15. 
Since the proposed pointing device uses a mobile device, its 
functionality can be further enhanced by software application 
programs on the device. For example, after selecting a 3D 
object, gestures such as rotating and scaling can be performed 
on the mobile device screen to manipulate the 3D object.  The 
capability to control the length of the auxiliary line from the 
tip of the pointing device may allow the user to sculpt or slice 
the 3D objects. Moreover, by using the haptic sensor inside 
the mobile device, the user can receive a haptic feedback when 
touching a 3D object.  
 
VI. CONCLUSION 
In this study, we have proposed a new pointing device that 
can be used to interact with a 3D spherical display without 
using a motion capture device. Our proposed device is 
characterized by the two different sources to calculate its 
6DoF. They are a touch interface to acquire the location of the 
device tip and an IMU to measure the orientation of the device. 
We built a Motion Platform with three actuators to calibrate 
the IMU. Our experiments have confirmed the pointing 
accuracy and stability for the device.  
The proposed pen has not yet been studied in detail in 
terms of resolution and sensitivity, but we will further 
experiment to clarify it in the future. We will also extend the 
functionality of the proposed pointing device as an interface 
in virtual surgical training based on a spherical display.  
 
ACKNOWLEDGEMENT 
This work was supported by JSPS KAKENHI Grant 
Numbers 20K12123. We also thank to the Iwate Prefectural 
University of Japan, for funding this project. We also thank 
the editor and three anonymous reviewers for their 
constructive comments, which helped us to improve the 
manuscript. 
 
REFERENCES 
[1] R. Takahashi, K. Hotta, O. D. A. Prima, and H. Ito, “A 
Perspective-Corrected Stylus Pen for 3D Interaction,” The 
Thirteenth 
International 
Conference 
on 
Advances 
in 
Computer-Human Interactions (ACHI 2020), pp. 11-14, 2020. 
[2] A. Borrego, J. Latorre, M. Alcañiz, and R. Llorens, 
“Comparison of Oculus Rift and HTC Vive: Feasibility for 
Virtual Reality-Based Exploration, Navigation, Exergaming, 
and Rehabilitation,” Games for Health Journal, 7(3), pp. 151-
156, 2018. 
[3] B. Masia, G. Wetzstein, P. Didyk, and D. Gutierrez, “A survey 
on computational displays: Pushing the Boundaries of Optics, 
Computation, and Perception,” Computers and Graphics 
(Pergamon), 37(8), pp. 1012-1038, 2013. 
[4] Looking Glass, https://lookingglassfactory.com. 
[retrieved: September, 2020] 
[5] J. 
Geng, 
“Three-Dimensional 
Display 
Technologies,” 
Advances in Optics and Photonics, 5(4), pp. 456–535, 2013. 
[6] J. Bolton, K. Kim, and R. Vertegaal, “SnowGlobe: A Spherical 
Fish-Tank VR Display,” In Conference on Human Factors in 
Computing Systems - Proceedings, pp. 1159-1164, 2011. 
[7] F. Ferreira et al., “Spheree: A 3D Perspective-Corrected 
Interactive Spherical Scalable Display,” ACM SIGGRAPH 
2014 Emerging Technologies, pp. 1, 2014. 
https://doi.org/10.1145/2614217.2630585 
[8] Q. Zhou et al., “CoGlobe - A Co-located Multi-Person FTVR 
Experience,” ACM SIGGRAPH 2018 Emerging Technologies.  
https://doi.org/10.1145/3214907.3214914. 
[9] G. Hagemann, Q. Zhou, I. Stavness., O. D. A. Prima, and S. 
Fels, “Here’s looking at you: A Spherical FTVR Display for 
Realistic Eye-Contact,” ISS 2018 - Proceedings of the 2018 
292
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

ACM International Conference on Interactive Surfaces and 
Spaces, pp. 357–362, 2018.  
[10] D. Fafard et al., “FTVR in VR: Evaluating 3D performance 
with a simulated volumetric fish-tank virtual reality display,” 
Conference on Human Factors in Computing Systems, pp. 1-
12, 2019. 
[11] T. Crespel, P. Reuter, and X. Granier, “A Low-Cost Multitouch 
Spherical Display: Hardware and Software Design,” Display 
Week 2017, May 2017, Los Angeles, California, United States. 
pp. 619- 622, 10.1002/sdtp.11716 . hal-01455523. 
[12] OptiTrack, https://optitrack.com. [retrieved: September, 2020] 
[13] P. C. Wu et al., ”DodecaPen: Accurate 6DoF Tracking of a 
Passive Stylus,” UIST 2017 - Proceedings of the 30th Annual 
ACM Symposium on User Interface Software and Technology, 
pp. 365–374, 2017. 
[14] Vive Tracker, https://www.vive.com/eu/vive-tracker/ 
[retrieved: September, 2020] 
[15] H. Benko, A. D. Wilson, and R. Balakrishnan, “Sphere: Multi-
Touch Interactions on a Spherical Display,” UIST 2008 - 
Proceedings of the 21st Annual ACM Symposium on User 
Interface Software and Technology, pp. 77-86, 2008. 
[16] S. W. Utt, P. C. Rubesin, and M. A. Foody, “Display System 
Having a Three-Dimensional Convex Display Surface,” US 
Patent 7,352,340. 2005. 
[17] Pufferfish Ltd. http://pufferfishdisplays.co.uk/, 2002. 
[retrieved: September, 2020] 
[18] L. Thomas, F. Christopher, and L. Jonathan, “A Self-Contained 
Spherical Display System,” In ACM Siggraph 2003 Emerging 
Technologies. 
[19] T h e  T o u c h T M  H a p t i c  D e v i c e ,  3 D  S y s t e m s , 
 https://www.3dsystems.com/haptics-devices/touch. 
[retrieved: September, 2020] 
[20] T. Grossman, D. Wigdor, and R. Balakrishnan, “Multi-Finger 
Interaction with 3D Volumetric Displays,” In UIST ’04: 
Proceedings of the 17th annual ACM symposium on User 
interface software and technology, pp. 61-70, 2004. 
  
 
 
293
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

