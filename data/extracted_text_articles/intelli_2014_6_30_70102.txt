Velocity Estimation from Visual Information using Environmental Property 
 
Hitoki Takase, Kazuyuki Ito 
Dept. of Electrical and Electronics Engineering 
Hosei University 
Tokyo, Japan 
e-mail:tmrmeme7@gmail.com, ito@hosei.ac.jp 
 
 
Abstract—In ecological psychology, it is considered that 
animals and insects use visual instead of distance 
information. In this paper, we take the mechanism of 
animals into consideration and address the method used to 
estimate the velocity of a robot by employing only one 
camera. Simulations are conducted and their accuracy is 
discussed. 
Keywords-velocity 
estimation; 
monocular 
camera; 
ecological psychology. 
I. INTRODUCTION 
Intelligent safety systems for automobiles, such as 
autonomous collision avoidance and navigation systems, 
have recently attracted considerable attention, leading to 
extensive research into developing effective autonomous 
vehicles. The DARPA ground challenge and the Google 
car are two examples [1]-[3]. The most common approach 
in these conventional studies is to create three-dimensional 
models of the environment. In this approach, the vehicles 
have sensors, such as laser range-finders, to measure 
distances to obstacles and create a precise three-
dimensional model of the environment [4]-[8]. However, 
the creation of this environment model and the extraction 
of useful information involve huge computational costs. In 
contrast, although lower animals and insects do not have 
such distance sensors and their brains are very small, they 
behave adaptively even in unknown environments [9]-[11]. 
In ecological psychology, it is presumed that animals and 
insects use visual information instead of distance 
information. 
This study considers the navigation mechanism of 
animals and addresses the method used to estimate the 
velocity of a robot using visual information. We focus on 
the ecological niche framework and assume that the 
average distance from obstacles is constant. Under this 
assumption, we derive an equation to estimate velocity 
from visual information. Simulations are conducted, and 
the error rate arising from the error of average distance is 
discussed. 
This paper consists of the following parts. Section II 
defines the global coordinate system and explains the 
problem domain and the assumption in this paper. Section 
III describes a preliminary experiment that was conducted 
to discuss the validity of the assumption. Section IV 
proposes a method to estimate the velocity, and in Section 
V, simulations are conducted and the error rate of the 
proposed method is discussed. Section VI concludes this 
paper. 
 
II. PROBLEM DOMAIN 
In this study, we address the method of estimating the 
velocity of a robot by using only one camera; we employ 
no other sensors. In general, it is impossible to estimate 
velocity using a single camera. Hence, we make an 
assumption based on ecology. In ecology, it is reported 
that each animal occupies and exploits a unique niche. The 
size and variation of obstacles in its environment are 
aspects of this niche. Accordingly, we focus on the 
average distance from obstacles. The average distance 
depends on the niche and does not change rapidly. 
Therefore, in this study, we assume that the average 
distance from visible obstacles is known (i.e., it can be 
learned) and constant. Under this assumption, we derive an 
equation to estimate velocity from visual information. In 
practice, the average distance shows small variations; thus, 
we discuss the error rate of the estimated velocity arising 
from the error of average distance. The details are as 
follows.  
We define the global and view coordinate systems as 
shown in Figure. 1, and we define the estimated average of 
distance as shown in Figure 2. 
 
Figure 1.  Definition of coordinate systems. 
(
(
(
(
(0,
,0)
f
O
Y
X
118
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

 
Figure 2.  Definition of average distance. 
The camera is located on the y-axis of the global 
coordinate system and moved forward at a constant 
velocity v. The lens is positioned at (0, yf, 0). There are 
several static object, and the coordinates of their 
characteristic points pi are denoted by (xi, yi, zi), where n is 
the number of characteristic points. Di is the distance 
between the lens and the i-th point, and it is expressed as 
Di = yi – yf. Ri is the distance from the origin in the view 
coordinate system, ri is the distance from the y-axis in the 
global coordinate system, and  f is the focal length of the 
camera.As described above, we assume that each Di is 
unknown. However, only the average distance from the 
visible characteristic points is estimated, as shown in (1): 
  ̂   ̅   
 
 ∑
  
   
     
                          (1) 
where only pk+1 to pk+m are visible among all characteristic 
points (p1 to pn), as shown in Figure. 2.  ̅ is the actual 
average distance from the visible characteristic points, and 
 ̂ is the predefined constant, estimated value of  ̅. 
III. PRELIMINARY EXPERIMENT 
To discuss the validity of the assumption written in 
Section II, we conducted preliminary experiment.  
Figure. 3 shows setting of the experiment. We measured 
distance    and angle    ever    from    to     and from 
     to     ,then we move forward 2 m and measured 
again. From    and     we calculated     and  ̅. Figure. 4 
and Figure. 5 shows environments, and Table 1 and 2 
shows experiment results 
 
Figure 3.  Range of measurement 
 
Figure 4.  Environment 1 
 
Figure 5.  Environment 2 
 
 
lens
Top  view
Measuring rang
Measuring rang
Laser sensor
Measuring range
Measuring range
・
・
・
119
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

TABLE 1. RESULT OF ENVIRONMENT 1 
 
TABLE 2. RESULT OF ENVIRONMENT 2 
 
Environment 1 is a passage way in a building and there 
is walls on its both sides. Though there are some objects, 
influence of these objects is filtered by the calculation of 
average. Thus the average distance does not change 
rapidly as shown in Table. 1. Environment 2 is a road 
between a building and a green zone, and the similar 
tendency is observed as shown in Table. 2. From these 
results, we can confirm that  ̅  is dependent on the 
environment, however,  ̅ does not change rapidly. 
IV. VELOCITY ESTIMATION 
 
Now, we estimate the velocity of the camera using the 
visual information Ri. From the geometric relationship, Ri 
is given by (2): 
   =  
  
                                         (2) 
Differentiating both sides of (2), we obtain (3): 
 ̇     
  
     ̇                                   (3) 
Dividing (2) by (3), we obtain (4): 
  
 ̇    
   
  
   
     ̇                                      (4) 
From (4), we obtain (5): 
    
  
 ̇   ̇                                      (5) 
Alternatively, as the objects are static, the velocity of the 
camera can be expressed by (6) using the temporal change 
of Di. 
    ̇     ̇       ̇       ̇           (6) 
From (5) and (6), we obtain (7): 
   
  
 ̇                                           (7) 
From (7), we can obtain the sum of the distances from the 
visible characteristic points (pk+1 to pm), and thus, we 
obtain (8): 
∑
  
   
     
  ∑
    
     
  
 ̇                               (8) 
From (1) and (8), we obtain (9): 
  
 ̅
 
 ∑
  
 ̇  
   
     
                                       (9) 
In this paper, we utilize a predefined estimated value  ̂ 
instead of the actual average  ̅. Therefore, the estimated 
velocity of the camera  ̂ is given by (10): 
 ̂ 
 ̂
 
 ∑
  
 ̇  
   
     
                                     (10) 
We conduct simulations using this method in the next 
section. 
V. SIMULATION 
 We conduct simulations to discuss the accuracy of the 
proposed estimation method. In the simulations, we set all 
the characteristic points on horizontal planes (z = 0) to 
simplify the environments. Figures. 6–9 show the 
simulation environments. The small circles denote the 
characteristic points and the triangle denotes the visible 
area.  
 
Figure 6.  Environment 1 (regular pattern, 900 points). 
Measurement position
(0,0)
2.73
(0,2)
2.99
(0,4)
2.87
(0,6)
2.96
(0,8)
2.88
 ̅
Measurement position
(0,0)
12.15
(0,2)
12.03
(0,4)
12.83
(0,6)
12.32
(0,8)
12.13
 ̅
-15
-10
-5
0
5
10
15
0
5
10
15
20
25
30
120
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

 
Figure 7.  Environment 2 (regular pattern, 3600 points). 
 
Figure 8.  Environment 3 (random pattern, 900 points). 
 
 
Figure 9.  Environment 4 (random pattern, 3600 points). 
The characteristic points are set at a regular interval in 
Environment 1 (Figure. 6) and 2 (Figure. 7), and are set 
randomly in Environment 3 (Figure. 8) and 4 (Figure. 9). 
The density of the characteristic points is the same in 
Environment 1 and 3, and in Environment 2 and 4. 
  
Figure 10.  Results of Environment 1 
 
Figure 11.  Results of Environment 2 
 
 
Figure 12.  Results of Environment 3 
-15
-10
-5
0
5
10
15
0
5
10
15
20
25
30
-15
-10
-5
0
5
10
15
0
5
10
15
20
25
30
-15
-10
-5
0
5
10
15
0
5
10
15
20
25
30
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
5
10
15
20
25
Time (s)
 Velocity (m/s)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
5
10
15
20
25
Time (s)
 Velocity (m/s)
= 5
= 20
= 10
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
5
10
15
20
25
Time (s)
 Velocity (m/s)
= 20
= 10
= 5
121
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

 
Figure 13.  .Results of Environment 4 
We conducted simulations at different velocity (v = 5, 10, 
20) in every environment. Figure. 10–13 show the results, 
and based on them, we can infer that the velocity can be 
estimated. However, the estimated velocity has a small 
oscillating error. In a precise sense, the average distance  ̅ 
is not constant and it oscillates because, while the camera 
is moving, some characteristic points go out of view and 
others come into view. At the same time, the sum of     ̇  
also oscillates, and it counterbalances the oscillation of  ̅. 
Thus, the actual v is determined as a constant value, as 
shown in (9).However, as shown in (10), we employ a 
constant  ̂  instead of  ̅  in the simulations because the 
actual  ̅ is unknown. As  ̂ is constant and the sum of 
    ̇  oscillates, the estimated velocity  ̂ also oscillates, 
which causes the error. Therefore, the amount of error 
depends on the variation of  ̅. And the error rate of  ̂ is 
the same as the error rate of  ̂. In the environments, there 
are a large number of characteristic points and they are 
uniformly distributed. Hence, the error becomes small, as 
shown in Figure. 12 and 13. 
In summary, we can conclude that the velocity can be 
estimated using only one camera, and the error rate of the 
estimated velocity is dependent on the error rate of   ̂ and 
is independent of the actual velocity. As the variance of 
the error of distance is dependent on the density of 
characteristic points, the variance of the error of velocity is 
also dependent on the density of characteristic points.  
 
VI. CONCLUSION  
  In this paper, we addressed a method for the velocity 
estimation of a robot from visual information. We focused 
on the property of the environment and assumed that the 
average distance from the visible characteristic points was 
constant and predefined. Under this assumption, we 
proposed a method for velocity estimation. Simulations 
were conducted and the results showed that the velocity 
could be estimated with a small error, which depended on 
the environment. 
 Our future work is to develop a prototype system and 
conduct experiments in real environments, and discuss 
effectiveness of the proposed framework in practical use.  
 
Acknowledgment 
This research was partially supported by the Japan 
Society for the Promotion of Science through the Grant-in-
Aid for Scientific Research (C) 24500181. 
 
REFERENCES 
[1] “How Google’s Self-Driving Car Works – IEEE 
Spectrum”.Spectrum.ieeee.org, February 26, 2013. 
[2] M. Montemerlo, S. Thrun, H. Dahlkamp, D. Stavens, and S. 
Strohband, "Winning the DARPA grand challenge with an 
ai robot," in AAAI Conference on Artificial Intelligence,  
vol. 1, pp. 982-987, 2006.  
[3] U. Ozguner ,S. Christoph and R. Keith, "Systems for safety 
and autonomous behavior in cars, ” The DARPA Grand 
Challenge experience," Proceedings of the IEEE 95.2: 
pp.397-412, 2007. 
[4] S. Thrun, “Robotic Mapping: A Survey”, CMU Technical 
Report, CMU-CS-02-111, School of Computer Science, 
Carnegie Mellon University, Pittsburgh, PA, 2002. 
[5] M. Montemerlo, S. Thrun, D. Koller, and B. Wegbreit, 
“FastSLAM, “A Factored Solution to the Simultaneous 
Localization and Mapping Problem, ”  Proc. AAAI 
National Conference on Artificial Intelligence (Edmonton 
2002), pp. 593–598, 2002. 
[6] A. Diosi, and L. Kleeman, “Laser Scan Matching in Polar 
Coordinates with Application to SLAM,” IEEE/RSJ 
International Conference on Intelligent Robots and 
Systems(IROS 2005), pp.3317-3322, 2005. 
[7] M. Bosse, P. Newman, J. Leonard, and S. Teller, 
“Simultaneous Localization and Map Building in Large-
Scale Cyclic Environments Using the Atlas Framework,” 
The International Journal of Robotics Research, Dec. 2004, 
vol. 23, 12, pp. 1113–1139. 
[8] A. Nguyen, N. Martinelli, Tomatis, and R. Siegwart, “A 
Comparison of Line Extraction Algorithms using 2D Laser 
Rangefinder for Indoor Mobile Robotics,” Int. Conf. 
Intelligent Robots and Systems, pp. 1929–1934, 2005. 
[9] J. Gibson, ”The ecological approach to visual perception,” 
Houghton Mifflin, 1979. 
[10] J. Gibson, ”The perception of the visual world,”  Houghton 
Mifflin, 1950. 
[11] W. Epstein, and R. Sheena, eds. “Perception of space and 
motion.” Academic press.pp.263-325, 1995. 
 
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
5
10
15
20
25
Time (s)
 Velocity (m/s)
= 5
= 20
= 10
122
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

