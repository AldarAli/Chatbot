Implementation and Evaluation of Recurrence Equation Solvers on GPGPU systems
using Rearrangement of Array Conﬁgurations
Akiyoshi Wakatani∗
∗ Faculty of Intelligence and Informatics
Konan University
Higashinada, Kobe, 658-8501, Japan
wakatani@konan-u.ac.jp
Abstract—The recurrence equation solver is used in many
numerical applications and other general-purpose applications,
but it is inherently a sequential algorithm, so it is difﬁcult
to implement the parallel program for it. Recently, GPGPU
(General Purpose computing on Graphic Processing Unit)
attracts a great deal of attention, which is used for general-
purpose computations like numerical calculations as well as
graphic processing. In this paper, we implement a parallel
and scalable algorithm for solving recurrence equations on
GPUs by using CUDA (Compute Uniﬁed Device Architecture)
and evaluate its effectiveness. The algorithm was originally
implemented for MIMD parallel computers like a PC cluster
and an SMP system by the authors and we modify the
algorithm suitable for the GPGPU system by rearranging
arrays conﬁgurations.
Keywords-multithreading; tridiagonal solver; GPU; multi-
core; CUDA
I. INTRODUCTION
Recently, the peak performance of GPU (Graphic Process-
ing Unit) has increased very much and outperforms that of
general-purpose processors. Since past GPUs consisted of
special-purpose hardware, they were used only for graphic
processing and image processing. However, recent GPUs
like GeForce 8 type of NVIDIA are composed of general-
purpose uniﬁed shaders, so by using CUDA (Compute
Uniﬁed Device Architecture) [1], they are used for general-
purpose processing like numerical calculations as well as
graphic processing.
Parallel applications having less data dependencies can
be easily implemented on GPGPU systems, but complicated
data dependencies prevent an optimal implementation of
applications on GPGPU systems because we must carefully
select which data should be kept in a small but fast mem-
ory. Linear ﬁrst-order recurrence equations are expressed
as wi = si × wi−1 + ti, but these cannot be parallelized
straightforwardly by dividing domains because the value of
wi is determined by using wi−1. The recurrence equations are
used frequently on many applications like Gauss elimination,
the tridiagonal matrix solver and DPCM (Differential Pulse-
Code Modulation) codec, so it is very important to imple-
ment the recurrence equation solver on GPGPU systems in
order to achieve a high performance [11], [12].
In this paper, we modify the parallel algorithm of recur-
rence equations “P-scheme” suitable for GPGPU system and
we evaluate the performance comparison of our methods on
GPU and CPU. Note that P-scheme has been developed for
distributed memory computers by the authors [2].
The rest of this paper is organized as follows: Section 2
presents the P-scheme algorithm and Section 3 summarizes
the prior arts related to our method. Section 4 presents the
experimental method and discusses the results and Section
5 concludes this paper with a summary.
II. RECURRENCE EQUATIONS
A. Tridiagonal system of equations
P-scheme is an algorithm that solves a recurrence equation
in parallel. Our purpose is to parallelize a solver for the
following tridiagonal system of equations of A×x = c where
A is a tridiagonal matrix with N ×N elements and x and c
are vectors with N elements. The system is given by
x0
=
c0
(1)
−bi ·xi−1 +ai ·xi −bi ·xi+1
=
ci (1 ≤ i ≤ N −2) (2)
xN−1
=
cN−1
(3)
where arrays a and b are elements of matrix A and array c
is given in advance, array x is an unknown variable and N
is the number of elements of the arrays.
B. P-scheme
It is known that the system given by Equations (1), (2) and
(3) can be deterministically solved by Gaussian elimination,
which utilizes two auxiliary arrays p and q.
p0
=
0, q0 = c0
(4)
pi
=
bi
ai −bi · pi−1
, qi = ci +bi ·qi−1
ai −bi · pi−1
(5)
xi
=
xi+1 · pi +qi
(6)
The above procedure consists of a forward substitution
(Equation (5)) and a backward substitution (Equation (6)).
However, on parallel computers, the procedure cannot be
straightforwardly parallelized due to the data dependency
32
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

that resides on both the forward and backward substitutions.
Suppose that the number of processor is P, N = P ∗ M + 2
and arrays are block-distributed. Processor k (0 ≤ k ≤ P−1)
is in charge of M elements of arrays from k ∗ M + 1 to
k ∗ M + M, thus pk∗M+1 can be calculated on processor
k only after p(k−1)∗M+M is calculated on processor k − 1.
Meanwhile, xk∗M+M can be calculated on processor k only
after x(k+1)∗M+1 is calculated on processor k+1. These data-
dependencies completely diminish the possibility of parallel
computing.
We have proposed a parallel and scalable algorithm, called
“P-scheme” [2], [3], [4]．We focus on array p on Equation
(5) and explain how P-scheme works for it. Note that the
equation for array p is a non-linear recurrence equation
and the equations for arrays q and x are linear recurrence
equations. So, our method can be easily extended to the
equations q and x. Then we assume that pi−j and pi can be
expressed by the following equation:
βj +γj · pi
δ j + pi
=
(−1)j · pi−j (j > 0),
(7)
where β j, γj and δj are auxiliary arrays that are deﬁned
below. By substituting Equation (5) to Equation (7), the
following relation can be found.
δ j+(−1)j+1·
ai−j
bi−j ·βj
γ j
+
1+(−1)j+1·
ai−j
bi−j ·γj
γ j
· pi
β j
γ j + pi
=
(−1)j+1 · pi−(j+1)
(8)
Thus, β j, γj and δ j can be determined by the following
system of equations:
β1
=
1, γ1 = −ai
bi
, δ1 = 0
(9)
βj+1
=
1
γj
(δj +(−1)j+1 · ai−j
bi−j
·βj)
(10)
γj+1
=
1
γj
(1+(−1)j+1 · ai−j
bi−j
·γj)
(11)
δj+1
=
β j
γj
(12)
It should be noted that β j, γj and δ j are independent of
pj. Hence pi can be determined by using only p0 as follows:
pi
=
−βi +(−1)i · p0 ·δi
γi −(−1)i · p0
(13)
Therefore, if βi, γi and δi are calculated in advance, pi
can be directly determined just after p0 is determined.
By using the above relation, we proposed a scheme called
P-scheme(Pre-Propagation scheme), which consists of three
phases. First of all, every processor simultaneously starts its
calculation of βi, γi and δi. This is called pre-computation
phase. After that, processor 0 can directly determine pM
from p0, βM, γM and δM and sends pM to processor 1. After
receiving it, processor 1 can directly determine p2∗M from
pM, βM, γM and δM and sends p2∗M to processor 2 and then
processor 2 can directly determine p3∗M from received p2∗M
and its auxiliary arrays and sends p3∗M to processor 3 and
so on. This is called propagation phase. It should be noted
that processor k can determine p(k+1)∗M without calculating
pk∗M+i (1 ≤ i ≤ M−1). Finally, all processors can determine
pk∗M+i (1 ≤ i ≤ M −1) by using received data pk∗M. This is
called determination phase.
The pre-computation and determination phases can be
completely parallelized. Meanwhile the propagation phase is
still sequential but the data to be exchanged is very slight,
like just one data, so the communication cost is also expected
to be very slight. The cost of the propagation phase is
in proportion to the number of processors. Thus the total
execution time is estimated by O( N
P ) + O(P) + O( N
P ). It is
general that O(P) is absolutely less than O(N), because P
is supposed to be much less than N.
Although the pre-computation and determination phases
can be parallelized, the computational complexity is larger
than the original substitution given by Equation (5). Since
the original contains 1 multiplication, 1 division, 1 addition,
3 loads and 1 store and P-scheme contains 4 multiplications,
3 divisions, 3 additions, 6 loads and 4 stores, P-scheme must
carry out over twice more computation than the original
substitution. Execution times of the original substitution
and P-scheme on PC (Pentium III (1 GHz), 1GB memory,
GCC4.1.1 with O3) are shown in Figure 1. The graph shows
that the execution time of P-scheme is about twice slower
than that of the original substitution.
0.01
0.1
1
10
100
1000
Elapsed time (ms)
original
P-scheme
Elapsed time (ms)
Array size
P-scheme
Figure 1.
Comparison of execution times
III. RELATED WORKS
It is known that the ﬁrst-order recurrence equation cannot
be parallelized straightforwardly since the i-th element can
be determined by using the (i − 1)-th element. CR (Cyclic
Reduction) and RD (Recursive Doubling) are recurrence
equation solvers which can be directly applied on parallel
computers [5], [6], [7], [8], [9].
33
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

A tridiagonal matrix can be solved by using recurrence
equations and several tridiagonal matrix solvers have been
implemented on GPUs. Kass et al. used ADI method for
an approximate depth-of-view computation and solved the
tridiagonal matrix by using CR method on a GPU [10].
Zhang et al. applied four methods (CR, parallel CR, RD
and hybrid) to the tridiagonal matrix solver on the GPU
and evaluated the performances to ﬁnd that the hybrid
method achieved the best performance [11]. Goddeke and
Strzodka proposed mixed precision iterative solvers using
CR method and implemented it on the GPU. They found
that the resulting mixed precision schemes are always faster
than double precision alone schemes, and outperform tuned
CPU solvers [12].
When the size of the matrix is N ×N, the computational
complexity of CR is O(N) but it requires 2log2 N syn-
chronizations between processors. Meanwhile, parallel CR
requires only log2 N synchronizations but its total computa-
tional complexity is O(N ×log2 N). On the other hand, since
the sequential algorithm (Gaussian elimination) consists of
two recurrence equations (a forward substitution and a back-
ward substitution) and both of the recurrence equations can
be parallelized by using our method, those computational
complexities are O(N/P), O(P) and O(N/P), respectively.
Our method also requires only two synchronizations for each
recurrence equation. Then, we implement our method for
recurrence equations on GPUs and evaluate the parallelism
and the effectiveness of the rearrangement of array conﬁg-
urations in order to utilize the coalesced communication.
IV. REARRANGEMENT OF ARRAY CONFIGURATIONS
0
１
2
3
4
5
6
7
8
linear address
multi-threading
0
１
2
3
4
5
6
7
8
0
3
6
1
4
7
2
5
8
thread-0
thread-1
thread-2
thread-0
thread-1
thread-2
configuration-1
configuration-2
Figure 2.
Rearrangement of array conﬁgurations
As mentioned before, the pre-computation and determina-
tion phases can be completely parallelized between threads,
but the global memory accesses are done in either the coa-
lesced communication or the non-coalesced communication,
which depends on the array assignment. On the P-scheme
algorithm for distributed memory computers, the i-th thread
is in charge of the computations between wi×(N/P) and
w(i+1)×(N/P)−1 when wi is distributed into P threads. On
GPGPU systems, w0+k, w(N/P)+k, w2∗(N/P)+k, w3∗(N/P)+k ···
are concurrently accessed at the k-th step since calculations
on GPUs are in principle SIMD calculations. However, these
are accessed using the non-coalesced communication, so the
access cost is very large.
In order to cope with this difﬁculty, array elements that
are accessed simultaneously should be rearranged so that
they are adjacent to each other.
w
′
i∗P+j
=
wj∗s+i (0 ≤ i ≤ s−1,0 ≤ j ≤ P−1)
where s = N
P . Namely, this rearrangement is equal to the
transposition of a P×s two-dimensional array into a s×P
two-dimensional array.
Figure 2 shows an example of the rearrangement of
array conﬁgurations. Suppose that the size of an array is
9 and the array should be divided into three parts. In an
ordinary parallel computer like a PC cluster or an SMP
system, the array should be just divided simply, so thread
0 is in charge of array elements 0, 1 and 2, thread 1
is in charge of array elements 3, 4 and 5, and thread 2
is in charge of array elements 6, 7 and 8, because this
conﬁguration (conﬁguration 1) can enhance the locality of
memory accesses and the efﬁciency of the cache memory.
On the other hand, in a GPGPU system having NVIDIA’s
GPUs, the array should be rearranged (conﬁguration 2) in
order to utilize the coalesced communication, that is, thread
0 is in charge of array elements 0, 3 and 6, thread 1 is in
charge of array elements 1, 4 and 7, and thread 2 is in charge
of array elements 2, 5 and 8. So, at the ﬁrst step, the threads
access array elements 0, 1 and 2, and at the second step, the
threads access array elements 3, 4 and 5, and so on. Thus
threads can always coalesce their memory accesses into one
memory request.
In the following subsections, we will evaluate the effec-
tiveness of the rearrangement of arrays empirically by using
the experiments.
V. EXPERIMENT AND DISCUSSION
A. Experimental environment
Our experiments are carried out on the GPGPU system
that consists of AMD Phenom II X4 945 (3.0 GHz), 4.0
GB memory and Tesla C1060 GPU (30 MPs and compute
capability 1.3) under Windows 7 Ultimate and CUDA 3.0.
In order to evaluate our approach on the GPGPU system,
we focus on the following linear recurrence equation:
w0
=
C
wi
=
scale×wi−1 +of fset (1 ≤ i ≤ N)
where C, scale and of fset are constant. The value of N
is set to 218 (small arrays) and 220 (large arrays), and we
construct G thread blocks having T threads and execute them
in parallel on GPUs, that is, the total number of threads is
P = T ×G.
The elements of arrays are fetched from the global mem-
ory to registers of SPs and the results are directly stored
to the global memory, so the shared memory is not used
because no data is repeatedly used in our method. Therefore
we do not care the bank conﬂict of the shared memory. The
34
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

1
2
3
4
5
6
Elapsed time (ms)
precomp
propagate
determin
0
1
2
3
4
5
6
16
32
64
128
256
512
Elapsed time (ms)
No. of threads per block
precomp
propagate
determin
total
(a) G=4 (no rearrangement)
1
2
3
4
5
Elapsed time (ms)
precomp
propagate
determin
0
1
2
3
4
5
16
32
64
128
256
512
Elapsed time (ms)
No. of threads per block
precomp
propagate
determin
total
(b) G=4 (w/ rearrangement)
0
2
4
6
8
10
Elapsed time (ms)
precomp
propagate
determin
total
0
2
4
6
8
10
16
32
64
128
256
512
Elapsed time (ms)
No. of threads per block
precomp
propagate
determin
total
(c) G=32 (no rearrangement)
0
2
4
6
8
10
Elapsed time (ms)
precomp
propagate
determin
total
0
2
4
6
8
10
16
32
64
128
256
512
Elapsed time (ms)
No. of threads per block
precomp
propagate
determin
total
(d) G=32 (w/ rearrangement)
Figure 3.
Results of N = 256K(= 218)
global synchronization is implemented by invoking different
kernels. Since our method consists of three phases, only two
global synchronizations are required between the phases. So,
since the overhead of the synchronization is quite small, it
does not affect the total performance.
B. Occupancy and threads
Occupancy is one of performance metrics that predict
the effective performance on GPU execution. As mentioned
earlier, one MP has 8 SPs and each SP executes one thread,
so the efﬁciency of the MP does not reach 100% unless
there are at least 8 threads. Due to the difference between
the clock frequencies of the SP (shader clock) and that of the
MP (core clock), at least 4 threads should be concurrently
executed on one SP in order to keep the instruction pipeline
of the MP full. Therefore, it is recommended that at least
32 threads should be placed on each MP and this size (32)
is called “warp.” Moreover, since the access latency to the
global memory is large, the large size of the thread group
is preferable for hiding the latency.
In the CUDA execution environment, G thread blocks are
assigned to MPs and T threads are assigned to SPs within
each MP. As mentioned before, the number of SPs within
a MP is 8, but at least 32 threads, namely the size of the
warp, must be assigned to one MP in order to maintain
the efﬁcient execution. Moreover, more threads should be
assigned to each MP in order to improve the occupancy.
On the other hand, the GPGPU system that is used for our
experiment has 30 MPs, so G should be around 30 but it may
be less than 30 for the optimal P. Since P is T ×G and the
execution times of the pre-computation and determination
phases are in inverse proportion to P, they decrease when
T and G increase. However, the execution time of the
propagation phase increases when T and G increase, so
the total execution time may be worsen. Therefore, one of
purposes of our experiments is to conﬁrm the contribution
of T and G to the execution time.
C. Experiment 1 (small arrays)
0.5
1
1.5
2
2.5
Speedup
G=4 
Rearrangement
G=4 No. 
rearrange.
G=32 
0
0.5
16
32
64
128 256 512
No. of threads per block
G=32 
Rearrangement
G=32 No. 
Rearrange.
Figure 4.
Speedups of N = 256K(= 218)
The experimental results with the array size of 256K(=
218) are shown in Figure 3. In the ﬁgure, the execution
times of the pre-computation, propagation and determination
phases are illustrated when G is 4 and 32 and T is varied
from 16 to 512. It should be noted that the elapsed times of
the pre-computation and the determination are same on the
whole, and thus these lines are almost overlapped.
For both cases with no rearrangement and with the re-
arrangement of array conﬁgurations, the execution times of
these phases decrease as P increases. For example, when G
is 4 and the rearrangement is used, the execution times of the
pre-computation phase with T of 16, 32, 64 and 128 are 2.07
35
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

0
5
10
15
20
Elapsed tme (ms)
precomp
propagate
determin
total
0
5
10
15
20
16
32
64
128
256
512
Elapsed tme (ms)
No. of threads per block
precomp
propagate
determin
total
(a) Elapsed time (G=4)
0
1
2
3
4
5
6
Elapsed time (ms)
precomp
propagate
determin
total
0
1
2
3
4
5
6
16
32
64
128
256
512
Elapsed time (ms)
No. of threads per block
precomp
propagate
determin
total
(b) Elapsed time (G=16)
Figure 5.
Results of N = 1M(= 220) (w/ rearrangement)
msec, 1.07 msec, 0.59 msec and 0.34 msec, respectively.
When the rearrangement is not used, the execution times
of the pre-computation phase with T of 16, 32, 64 and
128 are 2.69 msec, 1.92 msec, 1.18 msec and 0.9 msec,
respectively. When T is 64 and the rearrangement is used,
the execution times of the pre-computation phase with G
of 4 and 32 are 0.59 msec and 0.13 msec, respectively.
When the rearrangement is not used, the execution times
of the pre-computation phase with G of 4 and 32 are 1.18
msec and 0.59 msec, respectively. As P increases, the area
where each thread is in charge is getting smaller, so the
overhead like a thread creation increases relatively. Note
that the speedup seems to be ﬂat when T is over 8, because
the number of SPs per MP is 8, but, as T increases, the
occupancy increases until the number of threads reaches
the warp size (The occupancy is 1.0 when a MP of Tesla
C1060 has 128 threads). Therefore, by increasing T (over 8),
the performance can be improved. It should be also noted
that, by rearranging the array conﬁguration and using the
coalesced communication, the performance can be enhanced
from 2 to 10 times, which depends on the value of G.
On the other hand, as P increases, the execution time
of the propagation phase increases. For example, when G
is 4 and the rearrangement is used, the execution times
of the pre-computation phase with T of 16, 32, 64 and
128 are 0.25 msec, 0.38 msec, 0.69 msec and 1.21 msec,
respectively. Since the propagation phase is carried out on
one SP, there is no difference between the execution time
using the rearrangement and that without the rearrangement.
The comparisons of the speedups with a variety of pa-
rameter settings based on the execution time of the CPU are
shown in Figure 4.
On the whole, the speedups using the rearrangement
outperform those without the rearrangement. When the re-
arrangement is used, the difference of the speedups is small
since the change of the value of T does not result in the
difference of the execution time so much. However, when the
rearrangement is used, the value of T and P decide whose
phase should be dominant among the execution times of
three phases, so an optimal value of T and P results in the
largest speedup. For example, when G is 4, the maximum
speedup is 1.9 with the value of T of 256. As mentioned
below, G×T is constant when the combination of G and T
results in the maximum speedup.
Moreover, when G and T are large, the value of the
speedup using the rearrangement is almost identical to that
without the rearrangement. For example, this is true when
G = 32,T = 128,256,512. The reason is that the propagation
phase is dominant among three phases when G and T are
large. But there is no difference between the execution time
using the coalesced communication and that using the non-
coalesced communication, since the propagation phase is
carried out on one SP.
D. Experiment 2 (large arrays)
A part of the experimental results with the array size of
1 M (= 220) are shown in Figure 5. The results of this case
are almost equal to those of the case with the size of 256
K. Namely, as P increases, the execution times of the pre-
computation and the determination phases decrease. It is also
found that the performance can be enhanced from 2 to 10
times by using the rearrangement of array conﬁgurations
and reducing the access cost to the global memory. As
P increases, the execution time of the propagation phase
increases. The difference from the case with the size of 256
K is that the absolute time of the pre-computation and the
determination phases increases due to the increase of the
array size and then the execution time of the propagation
phase is relatively smaller than the 256 K case. Therefore,
the optimal value of P is larger than the 256 K case.
1
2
3
4
5
Speedup
G=4 
Rearrangement
G=4 No. 
rearrange.
G=16 
0
1
16
32
64
128 256 512
No. of threads per block
G=16 
Rearrangement
G=16 No. 
rearrange.
Figure 6.
Speedups of N = 1M(= 220)
36
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

0.24 
0.45 
0.79 
1.20 
1.31 
1.32 
1.935
1.69
1.245
0.74
0.382
0.5
1
1.5
2
2.5
Speedup
PCR(G=32)
P-scheme(G=32)
0.24 
0.382
0.197
0
0.5
16
32
64
128
256
512
No. of threads per block
(a) N=256K
0.67 
1.10 
1.19 
3.13
4.48
4.176
2.658
1.568
1
2
3
4
5
Speedup
PCR(G=32)
P-scheme(G=32)
0.20 
0.37 
0.67 
1.25 
0.79
0
1
16
32
64
128
256
512
No. of threads per block
(b) N=1M
Figure 7.
Comparison with PCR (Parallel Cyclic Reduction)
The comparisons of the speedups with a variety of pa-
rameter settings based on the execution time of the CPU are
also shown in Figures 5 and 6 .
The trend of the results of the speedups are also similar
to that of the 256 K case except that the value of T for the
maximum speedup is smaller than the 256 K case.
E. Comparison with PCR
We compare the performance of our methods with PCR
(parallel cyclic reduction) method [11] and show the ex-
perimental results in Figure 7. The maximum speedup of
the P-scheme is larger than that of the PCR method for
both cases because the computational complexities of the
PCR method and the P-scheme are O(N ·logN) and O(N),
respectively. However, the speedup depends on the size of
the thread block very much, so the tuning parameter for
GPGPU programs must be carefully selected in order to
achieve the maximum speedup.
F. Discussion
We discuss the optimal combination of G and T when the
coalesced communication is used. The speedups using the
rearrangement of array conﬁgurations when N is 256 K and
1 M are shown in Figure 8,
As shown in previous sections, the execution times of all
the phases are estimated as follows:
pre−comp
=
α · N
P
(14)
propagation
=
β ·P
(15)
determination
=
γ · N
P
(16)
where α, β and γ are the execution costs per data for
the pre-computation phase, the propagation phase and the
determination phase, respectively.
The computational complexities of these three phases are
almost identical. However, while both the pre-computation
phase and the determination phase are executed on T threads
(over 1 warp) within thread blocks, the propagation phase is
carried out only on one thread. Thus, since the core clock
is 4 times slower than the shader clock, we assume the
computational complexities of all the phases as follows:
α : β : γ
≃
1 : 4 : 1.
(17)
The parallelism is in proportion to G when G increases
until the number of MPs, but it is almost ﬂat when G is
over the number of MPs. Since Tesla C1060 has 30 MPs,
we evaluate the cases with the value of G of up to 32 in our
experiments. Each MP has 8 SPs, so the parallelism is in
proportion to T until T reaches 8 and it is in quasi-proportion
to T until the warp size (32). When T is more than 32,
the occupancy is getting close to 1.0 but the increase of
the parallelism is almost ﬂat. Therefore, in order to achieve
the maximum speedup by increasing P, the following policy
should be applied: 1) G should be maximized ﬁrst and 2)
the optimal T should be selected.
By using Equations (14), (15) and (16), the execution time
t and the optimal parallelism Popt are determined as follows:
t
=
α · N
P +β ·P+γ · N
P
Popt
=
√
(α +γ)N
β
=
√
N
2 .
(18)
When N is 1 M, Popt is nearly equal to 720, so T should be
180, 90, 45 and 22.5 for the cases with the value of G of 4,
8, 16 and 32, respectively. According to Figure 8-(a), T for
the maximum speedup is 256, 128, 64 and 32 when G is 4, 8,
16 and 32. Thus, the estimation and the experimental results
are identical. Moreover, When N is 256 K, Popt is nearly
equal to 360, so T should be 90, 45, 22.5 and 11.25 for the
cases with the value of G of 4, 8, 16 and 32, respectively.
According to Figure 8-(a), T for the maximum speedup is
256, 64, 32 and 16 when G is 4, 8, 16 and 32. Thus, the
estimation and the experimental results are almost identical
for this case as well.
Therefore, the maximum speedup is achieved when G is
32. So it is indicated that the optimization policy described
above is rational. Namely, G should be maximized ﬁrst and
37
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

then the optimal T should be selected in order to achieve
the maximum speedup.
It should be noted that Popt is
√
N
2 and thus
N
Popt is
√
2N.
Therefore the computational complexity of the propagation
phase is O(Popt) = O(
√
N) and it is not larger than the com-
putational complexities of other phases (O( N
Popt ) = O(
√
N)).
0.5
1
1.5
2
2.5
Speedup
G=4
G=8
G=16
0
0.5
1
1.5
2
2.5
16
32
64
128
256
512
Speedup
No. of threads per block
G=4
G=8
G=16
G=32
(a) N=256K
1
2
3
4
5
Speedup
G=4
G=8
G=16
0
1
2
3
4
5
16
32
64
128
256
512
Speedup
No. of threads per block
G=4
G=8
G=16
G=32
(b) N=1M
Figure 8.
Comparison of speedups
VI. CONCLUSION
We implemented a parallel recurrence equation solver
on GPGPU systems by using CUDA and evaluated the
effectiveness of the rearrangement of array conﬁguration
in order to utilize the coalesced communication. We also
proposed a policy to decide the optimal number of threads
per thread block and the optimal number of thread blocks
in order to maximize the efﬁciency of parallelism.
In the near future, we will apply the recurrence equation
solver to real applications. We will also try to implement
our approach using OpenCL that recently attracts a lot of
attention.
ACKNOWLEDGMENT
This work was supported in part by MEXT, Japan.
REFERENCES
[1] D. Kirk, and W. Hwu, Programming Massively Parallel
Processors: A Hands-on Approach. Morgan Kaufmann, Mas-
sachusetts, 2010.
[2] A. Wakatani, “A Parallel and Scalable Algorithm for ADI
Method with Pre-propagation and Message Vectorization,”
Parallel Computing, vol. 30, pp. 1345-1359, 2004.
[3] A. Wakatani, “A Parallel Scheme for Solving a Tridiagonal
Matrix with Pre-propagation,” Proc. 10th Euro PVM/MPI
Conference, Venice, 2003, pp. 222-226.
[4] A. Wakatani, “A Parallel and Scalable Algorithm for Calculat-
ing Linear and Non-linear Recurrence Equations,” Proc. Int’l
Conf. Parallel and Distributed Computing and Networks, Las
Vegas, 2004, pp. 446-451.
[5] R. Hockney and C. Jesshope, Parallel Computer 2. Taylor &
Francis, London, 1988.
[6] J. Lopez and E. Zapata, “Uniﬁed Architecture for Divide and
Conquer Based Tridiagonal System Solver,” IEEE Transac-
tions on Computer, vol. 43, pp. 1413-1425, 1994.
[7] O. Eˇgecioˇglu, et al., “A Recursive Doubling Algorithm for
Solution of Tridiagonal Systems on Hypercube Multiproces-
sors, ” J. of Comput. and Applied Mathematics, vol. 27, pp.
95-108, 1989.
[8] E. Dekker and L. Dekker, “Parallel Minimal Norm Method
for Tridiagonal Linear Systems,” IEEE Transactions on Com-
puter, vol. 44, pp. 942-946, 1995.
[9] D. Lee and W. Sung, “Multi-core and SIMD architecture
based implementation of recursive digital ﬁltering algorithms,
” Proc. IEEE International Conference on Acoustics Speech
and Signal Processing (ICASSP), 2010, pp. 1550-1553.
[10] M. Kass, A. Lefohn and J.D. Owens, “Interactive Depth of
Field Using Diffusion,” Technical report 0601,Pixar Anima-
tion Studios, 2006, pp. 1-8.
[11] Y. Zhang, L. Cohen and J.D. Owens, “Fast Tridiagonal
Solvers on the GPU,” Proc. PPoPP 2010, Bangalore, 2010,
10 pages.
[12] D. Goddeke and R. Strzodka, “Cyclic Reduction Tridiagonal
Solvers on GPUs Applied to [ Mixed Precision Multigrid,”
IEEE Transactions on Parallel and Distributed Systems, vol.
22, pp. 22-32, 2011.
38
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-217-2
FUTURE COMPUTING 2012 : The Fourth International Conference on Future Computational Technologies and Applications

