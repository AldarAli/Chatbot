Aurora - Exploring Social Online Learning Tools Through Design
Peter Purgathofer and Naemi Luckner
Vienna University of Technology
Institute of Design and Assessment of Technology
{purg, naemi}@igw.tuwien.ac.at
Abstract—Teaching is an integral part of our work at university
and we strive to achieve a high quality in teaching our students.
Yet, we face classes with up to 800 students per semester and do
not have the resources to ensure a close rapport with each and
every one of them. For the last 6 years we have been working
on an online solution for this problem, with the aim of letting
students take responsibility of a large part of their own learning
process. The e-learning system Aurora envelops a number of
components ranging from organizational and informational tools
to discussion systems and student portfolios. Students are invited
to participate actively in this online learning environment and
are presented with a variety of options they can choose from in
order to accumulate enough credits to pass their courses. Over
the years of developing Aurora, issues and inadequacies of the
system became apparent and lead us to change our system design
iteratively, learning from its shortcomings.
Keywords-Asynchronous Interaction, E-Learning, E-Portfolio,
Electronic Note Taking, Backchannel, Teaching, Design, Newsfeed
I.
INTRODUCTION
The Web 2.0 as described in [1] changed online culture
and shifted it from a passive consumer culture to a partici-
patory culture. This development also inﬂuenced the process
of teaching and learning, which is since referred to as E-
Learning 2.0. The notion of E-Learning 2.0 is that Web
2.0 technologies are adapted and integrated in E-Learning
systems [2]. Knowledge can be created, shared, remixed and
repurposed by communities of practice. Students are part of
this process, collect sources and participate in the communities,
by sharing their own ideas and ﬁndings. Brown and Adler [3]
describe a new age of education, in which lifelong learning
is not only needed but also supported by the participatory
architecture of the Web 2.0. They speak of a new learning
approach, ”...characterized by a demand-pull rather than the
traditional supply-push mode...” of obtaining knowledge. They
emphasize the importance of social learning in the new online
learning environment, pointing out, that the traditional teacher-
student relationship is exchanged by a peer-based learning
relationship.
Siemens [4] took this change of learning culture and
devised his theory of connectivism. He states that learning in
the digital age is the self-driven process of building up net-
works of knowledge. Nodes in a network can be data sources,
communities or people and are connected to the network with
strong and weak links. Weak links are more interesting since
they can open doors to new areas of knowledge, diversity and
innovation. Siemens points out that the lifecycle of ’correct’
facts is getting shorter, and new knowledge is created faster,
so memorizing facts is not yielding desired results anymore.
More important is the ’Know-where’, which describes where
knowledge can be found quickly rather than learning the
knowledge itself.
In this paper, we present an E-Learning System with the
aim of letting students take responsibility of their own learning
process. The system is an attempt to create a holistic learning
platform, valuing not only assigned course work, but also
social and other additional content students create or discover
over the course of a semester. What we wanted to avoid was
to develop another system increasing the distance between
teacher and students. Instead, our goal was to start from the
rather difﬁcult situation of very large classes, where contact
between teacher and student is short and far between, and
transform it so that students have a feeling of more immediate
involvement, more contact and more personal mentoring. To
achieve this, we put concepts like social interaction, participa-
tion, and exchange at the center of our design efforts.
Aurora is a learning platform consisting of three modules
that can communicate with each other. The Dashboard is
an administrative tool, containing an administrative News-
feed as well as widgets to enhance communication between
all participants of the course and maintain an overview of
the course progress and interesting developments around the
course topics. The Slides module is used during and after
lectures as backchannel and basis for upcoming discussions
around course topics. Students are provided with a pool of
activities they can choose from in the Portfolio. We chose the
word activity rather than exercise for work assignments, since
we want to motivate students to actively pursue their work
for this course and we wanted to avoid the vocabulary usually
associated with course work to try to increase motivation. The
name Aurora is not an acronym, nor has it any deeper meaning.
We used the name because it refers to something beautiful, and
because it sounds good. Also, pictures of auroras give awesome
visuals to be used in the layout.
The remainder of this paper is structured as follows: The
next section ’Overall Goal’ will give an idea of our motivation
and process to create a new e-learning system and explain why
we chose to develop the system ourselves instead of taking
existing tools. Subsequently, in the section ’Components’, each
module of Aurora is described and compared with existing
solutions from literature. The description is followed by a
preliminary evaluation, ’Evaluation’, and ﬁnally a conclusive
section, ’Conclusion and Future plans’, in which we describe
how the evaluated data inﬂuences future designs.
319
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

II.
OVERALL GOAL
At the Vienna University of Technology, lecture partici-
pants are sometimes in the high three-digit numbers. Tradi-
tionally, this would mean that lectures have to be endured
with a passive and consuming stance. Around 2005, we set
out to explore new ways to make lectures more interactive. We
started by appropriating existing systems like IRC and Twitter
to facilitate backchannel communication and interaction for
students visiting large lectures. Early on, we were fascinated by
the idea that we could time-sync this information to the slides.
This would enable students to understand the backchannel as
a means of taking (collaborative) course notes that became
attached to the individual slides of the lecture.
We also started to replace the then prevailing passive html
web pages for course information with blogs, which seemed an
ideal ﬁt for some years. Later, we started to supplant blogs with
a custom-made newsfeed solution that was heavily inspired
by the structures and aesthetics of social media systems like
Facebook or Twitter.
By replacing all the passive elements of large scale courses
with interactive components, we also set out to change the way
we evaluate student performance in order to come to a ﬁnal
grade. This led to a somewhat idiosyncratic redeﬁnition of a
portfolio system that we implemented.
All these systems are currently being actively developed
and reﬁned in an effort to explore new ways of teaching
and learning for a generation that grew up with ever-present
internet access and for the most part played a lot of games [5].
We redesign our systems year after year after understanding
what works and what doesn’t. We pursuit this research in the
spirit of design as research, or explorative design. One core
idea is that with each version, new concepts become evident
that were impossible to see last year, be it from use, from
formal or informal evaluation, or because we reﬂect on our
progress from the feedback we get from students.
Following this path for some years now, we have come
to a place where individual components have been published
about, but we never set out to describe the system as a whole.
This is what this paper sets out to do.
III.
COMPONENTS
A. Dashboard
Dashboards are often used in complex system to provide
participants with an overview of activity on these platforms.
The role of a dashboard is variable, depending on the context
of its application. Dashboards have been used to track activity
from different applications in a complex system [6]; to create
peripheral awareness, provide navigation and a system-wide
inbox [7]; to create awareness of group members’ actions and
to convey the status of shared artifacts [8]; and to provide
multiple views of a large dataset in a system [9]. More specif-
ically, in an e-learning context, they have been used for self-
monitoring for students and to improve teachers’ awareness
[10]; and to help students to relate their learning experience
to that of their peers or other actors in the system [11].
In Aurora, the Dashboard is the ﬁrst page every student is
presented with when logging into the system. It is a collection
of widgets, containing the Newsfeed, an individual course
status overview, showing colleagues, groups, current links
and additional contact information. The page draws together
course-relevant information related to the content from other
websites, as well as information from other components of
Aurora.
In former versions of Aurora, we included a statistics page
to enhance students’ peripheral awareness. The page provided
a statistical overview of the large amount of data that is
distributed over the whole system. Students could for example
look up who of their peers was involved in a lot of discussions,
or who got a lot of stars, which could be awarded for good
comments by other students and members of the staff. This
view has not made it into current versions of the system, mostly
because of a lack of time and resources for the development.
1) Newsfeed: The Newsfeed is a largely organizational
message board, but can also be used for content related post-
ings. The lecture staff can use the Newsfeed to publish course
updates and other relevant news for the students. Questions,
annotations, complaints and praise can also be posted here, and
can be answered by other actors in the system. Students post
content related comments as well, but are asked to ﬁrst look
for a suitable slide in the Slides section to provide context for
the content, before blindly posting it in the Newsfeed.
Information from other components is collected and posted
via sticky notes at the top of the Newsfeed. Students are
informed if someone answered to one of their postings in the
Slides section and can jump directly to the posting via link. If
students get points for a good comment in the Slides section or
for a newly marked activity in the Portfolio, they are notiﬁed
here. Direct messages show up on top of the Newsfeed section,
and can be sent by either colleagues or team members.
The Newsfeed enhances direct communication between
students and staff and also provides a forum for discussions
about the course design. It can be searched or ﬁltered to see
either only staff postings, only organizational postings, or only
content related postings. Students can subscribe to Newsfeed
postings via RSS to integrate them into their everyday online
environment.
2) Additional widgets: The Progress Bar widget is a tool
students can use to get an overview of their progress in each
of their classes. Each lecture has an overview of the student’s
activity status. It shows the amount of points received in the
lecture through activities and through comments, as well as
the total amount of points. Additionally, it shows the amount
of points that the student has handed in but which have not
yet been graded and the amount of points the student can still
hand in until the end of the semester.
In the Colleagues widget, users can add other students to
their course network and, on accept, see their avatars and
further information. They can write direct messages to their
colleagues as well as see all their colleagues’ comments in the
Newsfeed and the Slides highlighted. This can create a feeling
of connectedness within the course and motivate to interact
with others regularly.
Some activities in the Portfolio can be worked on in teams.
The Teams widget shows a list of all existing teams the student
is a part off. Each entry contains the name of the project the
team is working on, the possibility to send a message to all
team members, and a list of the other team members.
The Current Links widget displays a list of recent articles
and interesting websites - supplementary reading material
of topics covered in the course. The collection of links is
compiled in a blog using soup.io and integrated into the
Dashboard via RSS.
Lastly, the Dashboard lists contact information to corre-
spond with the staff directly. Students are invited to ask all
320
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

course relevant questions directly in the Newsfeed so that other
students can proﬁt from the answers as well, but some issues
need to be taken up with the staff directly.
B. Slides
There is some research on how to offer interactivity in
large lectures. One approach are the Audience Response
Systems, also called Clickers. Kumar and Rogers pioneered
such systems in their 1976 Olin Experimental Classroom [12]
that featured a feedback channel for students in the form
of 12 buttons. Today, clickers are commercially developed
products, offering a number of potential beneﬁts to large
lectures. Caldwell [13] summarized the literature on using
clickers in lectures. Recently, software clickers based on the
fact that most students bring a network-connected device, most
prominently mobile phones, to lectures have begun to appear,
but this approach is still mostly experimental [14] [15].
Of course, more elaborate backchannel communication
systems have been tried as well, such as ActiveClass [16],
Fragmented Social Mirror [17] or ClassCommons [18]. The
development and evaluation of these systems overlaps with the
development of the approaches presented here, ﬁrst published
in 2008 [19].
It can be argued that backchannel communication during
lectures is potentially distracting, diverting the attention from
the speaker to unrelated things. On the other hand, students
regularily bring their laptops to class in the hope of ﬁnding
productive use, but often end up doing other stuff that is
available on the computer. We have observed that supplying
students with a backchannel that is centered around the lecture
itself brings some of that attention back, and while it creates
bubbles of divertion from the lecture itself, at least these
bubbles are focussed on the content of the lecture.
Slides consist of two major components, Livecasting and
Studio. Livecasting lets participants add notes to individual
slides of a lecture, either in the style of a backchannel
conversation, or privately. Once the lecture is ﬁnished, slides
and comments are available in a combined view in the Studio.
Participants can keep adding comments, links etc. in the
Studio, so that the lecture slides become the focal point of
discussion and exchange for participants and lecturers alike.
1) The Livecasting component (Figure 1): During a lecture,
the lecturer runs a script on her computer. By pressing the
next slide button on the remote, she triggers a script that
sends the number and title of the newly displayed slide to the
slidecasting server. Additionally, the script retrieves the lecture
notes of this slide in the presentation document and scans
them for a custom-made meta-syntax signifying information
that is meant to be posted with the same slide. These text-
lines include explanations, enhanced quotes, references and
other links, activities and discussion starters.
Participants load a web page that changes with each slide
the lecturer shows, so that information entered into either the
public comment or private note ﬁelds on this page ends up
being attached to the slide that was visible when the participant
started typing. All participants can see the public comments
entered by other participants, and they can reply to these
comments, creating ad-hoc discussions of the lecture content.
To ease the cognitive load, a participant’s own comments are
colored yellow. Additionally, students have the opportunity to
mark slides as liked, important or unclear with a single click.
These talking machines are going to ruin the artistic 
development of music in this country. When I was a 
boy...in front of every house in the summer evenings, 
you would ﬁnd young people together singing the 
songs of the day or old songs. Today you hear these 
infernal machines going night and day. We will not have 
a vocal chord left. The vocal chord will be eliminated by 
a process of evolution, as was 
the tail of man when he came 
from the ape. 
John Philip Sousa
student laptop
presentation
Figure 1: Livecasting setup of the Slidecasting system
2) The Slidecasting Studio (Figure 2): Once the lecture is
over, the lecturer makes slide-by-slide pictures of his presen-
tation available to the slidecasting system. While this can also
be done before the lecture, we decided not to show the slides
because of the obvious spikes in network trafﬁc this would
generate whenever a new slide is shown. The slides, all the
participants’ comments as well as the lecturers automatically
posted comments are then made available in the Studio.
Here, participants and lecturers can post comments even
after the lecture is ﬁnished. In the Studio, the slides are
arranged horizontally, sorted by their time of appearance in
the lecture. The comments attached to each slide are laid out
vertically, with the earliest comments up on top (usually the
comments posted by the script on the lecturers computer), with
reply threads sorted in the same way.
Participants can give praise to good comments by clicking
the star next to the avatar of the author, in which case the star
turns yellow and shows the number of clicks it has accumu-
lated. Lecturer can use this same mechanism to award points
to outstanding comments. In this case, the star is distinguished
with a green glowing outline, making its commendation visible
to everybody.
While lecturer’s comments are generally displayed in the
same way as student comments, there are two lecturer-posted
types that stand out from the rest: discussion starters and
activities. Comments of both these types are arranged between
the slide and the private notes delineator, thus standing out
even when scrolling through the slides quickly.
Discussion starter comments typically contain a questions
and an invitation to discuss this question in the comments of
the slide. We use this mechanism to post slides in between
lectures, asking participants to discuss upcoming content.
Activities contain a brief explanation of an activity, linking
into the Portfolio system where an elaborate description of this
activity can be found. This gives the lecturer an opportunity
to announce new activities that derive from the content on a
slide.
Activities take student to the Portfolio of Aurora, where
they hand in their work for review and evaluation.
C. Portfolio
In areas like HCI or Informatics and society, it is hard to
make written exams, and once you have more than a couple
of hundred students, it becomes impractical to the point of
impossible to conduct oral exams. We started to abandon tests
321
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

Figure 2: Slidecasting Studio, where all slides and comments become accessible to participants and lecturers alike
and exams at some point when we had the distinct impression
that the fact that we had to make a written exam changed
what we taught. This compromises the whole idea of teaching,
especially at the university level.
For a couple of years now, research papers have been
explaining the theoretical sense the adoption of ePortfolios
would make. Advantages implied are, among others, improved
reﬂection, increased student engagement, improved learning
outcomes, and increased integration of knowledge [20]. The
paper quoted gives a comprehensive overview over ePortfolio
research, and points out the lack of empirical support for many
of the asserted advantages.
The module we call Portfolio is not really an ePortfolio in
the strict sense of the word. While we explicitly ask the stu-
dents put upload artefacts that shows what they have learned,
we offer a large catalog of predeﬁned activities that can be
handed in here. These activities include a broad range of tasks,
from simple applications of theoretical content, to actions
reﬂecting their own prior projects, to complex design exercises.
Many of those activities would make viable exercises in a
traditional deadline-based context, while others would be quite
unsuitable for such an environment. The catalog also contains
meta-activities such as ﬁnding new sources, suggesting new
activities, and organizing round table discussions with experts
in the ﬁeld. No activity should yield substantially more than
10% of the ﬁnal grade, so that students will be exposed to a
broad range of topics.
Participants hand in their work using the portfolio system
of Aurora. We do not set any deadlines other than the end of
the semester, and we do not expect them to follow a speciﬁc
order. The only requirement they have to meet is to make sure
that their work is distributed throughout the semester, instead
of congested at the end.
The Portfolio includes an easy review component for the
course admins to review and evaluate the participant’s work,
with the notable addition of enabling for the students repeated
submission of work that failed to meet the standards. It also
includes a double blind peer review component that makes
part of the assessment process into an activity by itself on the
premise that if you do an honest review of somebody else’s
work, you will learn a lot.
This approach tries to abandon the usual scattering of
deadlines through the semester, giving the students a lot
of autonomy in their work, which self-determination theory
deems essential for intrinsic motivation [21].
IV.
EVALUATION
Our focus in evaluating these components is in better
understanding how we can advance the system. We do not
have an ultimate goal, but we use both the design process
and the evaluation to understand how the system should be
enhanced, reﬁned and changed in order to satisfy our needs as
teachers as well as the needs of the students as learners.
TABLE I: The table shows how many people were involved
in the evaluated courses, staff as well as students.
Additionally, it shows how many certiﬁcates were handed out
for each course at the end of the semester.
Profs
Predoc
Tutors
Students
Certiﬁcates
BHCI
3
1
6
733
442
IST
1
1
4
521
337
Total
3
1
10
1254
779
This preliminary evaluation is based on data from two
courses, Basics of Human Computer Interaction (BHCI) and
Interactions of Society and Technology (IST), which took
place in the summer semester of 2013. A total of 11.793
activities was handed in over the course of the semester, 7126
in BHCI and 4667 in IST. The staff of both courses combined
consisted of 3 professors, 1 predoctoral fellow and 10 tutors,
exact numbers can be found in Table I. Students only got a
certiﬁcate if they handed in at least one activity. Every student
who ultimately received a certiﬁcate handed in 15 activities
on average. In the Slides section, 1283 slides were posted
distributed over two courses with 23 lectures in total, and 3975
comments were written during and after these lectures.
322
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

Figure 3: Time it took to grade an exercise, calculated in
weeks
A. Portfolio evaluation
Figure 3 shows a pie chart of the time it took to grade
activities. One third of the activities were graded after a week,
which would be an acceptable amount of time for students to
wait for feedback. Given the student-staff ratio, we tried to
achieve a maximum waiting time of three weeks until every
activity is graded. As can be seen in Figure 3, we were not
able to reach that goal, as only two thirds of handed in work
was evaluated within the given time frame. The ﬁnal third of
the pie chart consists of activities that took 4 and more weeks
to be graded. Considering the importance of feedback in order
to keep students motivated and continuously working [22], 4+
weeks seems too long a time to hear back on one’s work.
We suspect that this ﬂuctuation in delay can actually be
explained by queue modeling in game theory. Activities tend
to be handed in unequally distributed in time, leading to
an overload what causes congestions that are then almost
impossible to resolve within the given resources until the end
of the semester.
B. Slides evaluation
Figure 4: How long after a slide was posted (Day 0) are
students interacting with it via the comment stream.
In Figure 4, the comment data was analyzed to ﬁnd out if
and how long after the lecture students engage with the content
by writing comments and discussing it in the Studio. The graph
shows that most comments are written during the lecture, but
there is a long tail (going up to 75 days) after the original slide
was presented. Approximately 120 comments were posted even
after the semester was over.
Also interesting for us is the second peak a couple of days
later, as well as the ’long tail’ of posted content after the lecture
that can be seen in Figure 4. A cursory evaluation of these
’late’ postings show that students come back to post things
they ﬁnd relevant, like news coverage, examples, references,
etc. or to partake in discussions they have started with another
postings. We see this as a successful feature of the system, as
it induces reﬂection of and occupation with the content of a
lecture quite some time after the lecture is over.
V.
CONCLUSION AND FUTURE PLANS
The main goal of our work is to explore the design space
of online teaching and learning support systems. Our approach
is best described as explorative design, with the main goal to
better understand the situation, the players, and their needs. In
building and using systems that implement novel approaches
to the context of teaching and learning, we in turn have a
chance to understand the change such systems bring into the
situation, and react accordingly. This approach shifts the focus
of evaluation from understanding how and why the approach
worked (or not), to ﬁnding new approaches to try. In the
end, we are not interested in proving that our approach is
right, e.g., by showing effectiveness by some abstract learning
measurements. Instead, we want to ﬁnd new and better ways
to teach and learn that use the potentials of new technologies,
and tap student’s self-motivational capabilities.
The next version of Aurora is already in development. We
redesigned Portfolio in order to address previously identiﬁed
problems, and to explore new ways of organizing evaluation
and grading. Our main goal was to reduce the students’
waiting time for graded activities by introducing double blind
peer reviews into the grading process. Also, activities will
be organized by topic and will be structured in ’levels’ that
build on each other. Only top level activities are considered
grade relevant and will thus be graded by a staff member. All
other activities will just be reviewed by two or more peers
with an emphasis on constructive feedback, not marks, given
by the reviewers. For every activity a student hands in, they
have to review three activities of the same type handed in by
colleagues. To maintain a certain level of quality, activities
as well as reviews will randomly be checked by members of
the staff. We will also implement an easy way for students
to report extremely poor elaborations, plagiarized work, or, on
the other side, if they did not get meaningful feedback from a
reviewer.
The second component that is undergoing major changes is
the Newsfeed. We are putting more emphasis on ways to ﬁlter
and reorder the comment stream in order to make information
better accessible. This redesign is based on the way reddit
[23] works, e.g., using up- and downvoting to incentivize
collaborative content ﬁltering. We also designed a feature to
extract good (student) questions and corresponding (student
or staff) answers from the Newsfeed for display in a separate
FAQ section.
Another step in Aurora’s development will be to include
our discussion component Discourse into the system. Dis-
course is a threaded asynchronous discussion forum that differs
from traditional systems in the way discussions are repre-
sented. It vertically displays new thoughts, arguments and ideas
in the discussion, and horizontally the answers and exchanges
to said arguments. Students can gain points towards their ﬁnal
grade when participating in discussions and composing clear
arguments supported by references.
Handling more than 500 students in university courses
is a rare situation. Often, such a challenge is tackled by
introducing distance between teachers and learners, and by
relying on examination and tests. This removes autonomy from
the learning process, which we see as a central property. Thus,
we tried to go the opposite way, and designed Aurora with the
323
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

explicit goal to give students as much autonomy as possible
in such a setting. In our experience, such a challenge requires
explorative approaches, learning not only from evaluation, but
also from the design process itself.
ACKNOWLEDGMENT
The authors would like to thank all contributors who have
been involved in the development of Aurora over the years:
Stephan Bauer, Elisabeth Bauernhofer, Christoph Brner, Daniel
Domberger, Michael Emhofer, Andreas Fermitsch, Martin
Flucka, Thomas Gradisnik, Peter Holzkorn, Daniel Kececi,
Lucia Leitner, Peter Minarik, Wilfried Reinthaler, Gerald Re-
itschmied, Diane Salter, Reinhard Seiler, Martin Sereinig, Raif
Tabucic, Bruno Tunjic, Wolfgang Zalesak
REFERENCES
[1]
T. O’Reilly, “What Is Web 2.0: Design Patterns and Business Models
for the Next Generation of Software,” Design, vol. 65, 2007, pp. 17–37.
[2]
S. Downes, “Feature: E-learning 2.0,” Elearn magazine, vol. 2005,
no. 10, Oct. 2005, pp. 1—-, [Accessed Jan. 24, 2014]. [Online].
Available: http://doi.acm.org/10.1145/1104966.1104968
[3]
J. S. Brown and R. P. Adler, “Minds on Fire: Open Education,
the Long Tail, and Learning 2.0,” Educause Review, vol. 43 (1),
2008, pp. 16–32, [Accessed Jan. 24, 2014]. [Online]. Available:
http://www.educause.edu/ir/library/pdf/ERM0811.pdf
[4]
G. Siemens, “Connectivism: A Learning Theory for the Digital
Age,” 2004, [Accessed Jan. 24, 2014]. [Online]. Available: http:
//www.elearnspace.org/Articles/connectivism.htm
[5]
M.
Irvine,
“Survey:
97
Percent
Of
Children
Play
Video
Games,”
2008,
[Accessed
Jan.
24,
2014].
[Online].
Available:
http://www.hufﬁngtonpost.com/2008/09/16/
survey-97-percent-of-chil\ n\ 126948.html
[6]
J. L. Santos, S. Govaerts, K. Verbert, and E. Duval, “Goal-oriented
Visualizations of Activity Tracking: A Case Study with Engineering
Students,” in Proceedings of the 2Nd International Conference on
Learning Analytics and Knowledge, ser. LAK ’12.
New York, NY,
USA: ACM, 2012, pp. 143–152, [Accessed Jan. 24, 2014]. [Online].
Available: http://doi.acm.org/10.1145/2330601.2330639
[7]
C. Treude and M.-A. Storey, “Awareness 2.0: Staying Aware of
Projects, Developers and Tasks Using Dashboards and Feeds,” in
Proceedings of the 32Nd ACM/IEEE International Conference on
Software Engineering - Volume 1, ser. ICSE ’10.
New York, NY,
USA: ACM, 2010, pp. 365–374, [Accessed Jan. 24, 2014]. [Online].
Available: http://doi.acm.org/10.1145/1806799.1806854
[8]
J. T. Biehl, M. Czerwinski, G. Smith, and G. G. Robertson,
“FASTDash: A Visual Dashboard for Fostering Awareness in Software
Teams,” in Proceedings of the SIGCHI Conference on Human Factors
in Computing Systems, ser. CHI ’07.
New York, NY, USA: ACM,
2007, pp. 1313–1322, [Accessed Jan. 24, 2014]. [Online]. Available:
http://doi.acm.org/10.1145/1240624.1240823
[9]
M. McKeon, “Harnessing the web information ecosystem with wiki-
based visualization dashboards.” IEEE transactions on visualization and
computer graphics, vol. 15, no. 6, 2009, pp. 1081–8, [Accessed Jan.
24, 2014]. [Online]. Available: http://www.ncbi.nlm.nih.gov/pubmed/
19834175
[10]
S. Govaerts, K. Verbert, J. Klerkx, and E. Duval, “Visualizing
activities for self-reﬂection and awareness,” Advances in Web-Based
Learning ˆa ICWL 2010, vol. 6483, 2010, pp. 1–10, [Accessed
Jan. 24, 2014]. [Online]. Available: http://link.springer.com/chapter/10.
1007/978-3-642-17407-0\ 10
[11]
E. Duval, “Attention Please!: Learning Analytics for Visualization and
Recommendation,” in Proceedings of the 1st International Conference
on Learning Analytics and Knowledge, ser. LAK ’11.
New York,
NY, USA: ACM, 2011, pp. 9–17, [Accessed Jan. 24, 2014]. [Online].
Available: http://doi.acm.org/10.1145/2090116.2090118
[12]
V. K. Kumar and J. L. Rogers, “Student Response Behaviors in an
Instrumented Feedback Environment,” SIGCUE Outlook, vol. Special,
Dec. 1978, pp. 34–54, [Accessed Jan. 24, 2014]. [Online]. Available:
http://doi.acm.org/10.1145/1318457.1318461
[13]
J. E. Caldwell, “Clickers in the Large Classroom: Current Research
and Best-Practice Tips,” CBE-Life Sciences Education, vol. 6, no. 1,
2007, pp. 9–20, [Accessed Jan. 24, 2014]. [Online]. Available:
http://www.lifescied.org/content/6/1/9.abstract
[14]
D. Lindquist, T. Denning, M. Kelly, R. Malani, W. G. Griswold,
and B. Simon, “Exploring the Potential of Mobile Phones for Active
Learning in the Classroom,” SIGCSE Bull., vol. 39, no. 1, Mar.
2007, pp. 384–388, [Accessed Jan. 24, 2014]. [Online]. Available:
http://doi.acm.org/10.1145/1227504.1227445
[15]
S. Teel, D. Schweitzer, and S. Fulton, “Braingame: A Web-based
Student Response System,” J. Comput. Sci. Coll., vol. 28, no. 2,
Dec. 2012, pp. 40–47, [Accessed Jan. 24, 2014]. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2382887.2382895
[16]
M. Ratto, R. Shapiro, T. M. Truong, and G. W. Griswold, “The Ac-
tiveclass Project: Experiments in Encouraging Classroom Participation,”
in Designing for Change in Networked Learning Environments, 2003,
vol. 2, pp. 477–486.
[17]
T. Bergstrom and K. Karahalios, “Social Mirrors as Social Signals:
Transforming Audio into Graphics,” Computer Graphics and Applica-
tions, IEEE, vol. 29, no. 5, 2009, pp. 22–32, [Accessed Jan. 24, 2014].
[18]
H. Du, M. B. Rosson, and J. M. Carroll, “Augmenting Classroom
Participation Through Public Digital Backchannels,” in Proceedings
of the 17th ACM International Conference on Supporting Group
Work, ser. GROUP ’12.
New York, NY, USA: ACM, 2012,
pp. 155–164, [Accessed Jan. 24, 2014]. [Online]. Available: http:
//doi.acm.org/10.1145/2389176.2389201
[19]
W. Purgathofer, Peter Reinthaler, “Exploring the Massive Multiplayer
E-Learning
Concept,”
Ed-Media
Invited
Talk,
2008,
pp.
1–9,
[Accessed Jan. 24, 2014]. [Online]. Available: https://igw.tuwien.ac.at/
designlehren/exploring\ for\ edmedia.pdf
[20]
L. H. Bryant and J. R. Chittum, “ePortfolio Effectiveness: A(n
Ill-Fated) Search for Empirical Support,” International Journal of
ePortfolio, vol. 3, no. 2, 2013, pp. 189–198, [Accessed Jan. 24, 2014].
[Online]. Available: http://www.theijep.com
[21]
E. L. Deci and R. M. Ryan, “Motivation, personality, and development
within embedded social contexts: an overview of self-determination
theory,” in The oxford handbook of human motivation, R. M. Ryan,
Ed.
Oxford, 2012, pp. 85–107.
[22]
A. P. Rovai, “A constructivist approach to online college learning,”
The
Internet
and
Higher
Education,
vol.
7,
no.
2,
2004,
pp.
79 – 93, [Accessed Jan. 24, 2014]. [Online]. Available: http:
//www.sciencedirect.com/science/article/pii/S1096751604000144
[23]
“reddit.com,” [Accessed Jan. 24, 2014]. [Online]. Available: http:
//www.reddit.com/
324
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

