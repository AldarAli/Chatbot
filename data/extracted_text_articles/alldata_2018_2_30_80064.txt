A New Representation of Air Trafﬁc Data Adapted to Complexity Assessment
Georges Mykoniatis†, Florence Nicol‡, Stephane Puechmorel (*)
Ecole Nationale de l’Aviation Civile
Toulouse France
Email: †georges.mykoniatis@enac.fr, ‡florence.nicol@enac.fr, (*)stephane.puechmore@enac.fr
Abstract—Air trafﬁc is generally characterized by simple
indicators like the number of aircraft ﬂying over a given area or
the total distance ﬂown during a time window. As an example,
these values may be used for estimating a rough number of
air trafﬁc controllers needed in a given control center or for
performing economic studies. However, this approach is not
adapted to more complex situations such as those encountered
in airspace comparison or air trafﬁc controllers training. An
innovative representation of the trafﬁc data, relying on a sound
theoretical framework, is introduced in this work. It will pave
the way to a number of tools dedicated to trafﬁc analysis. Based
on an extraction of local covariance, a grid with values in the
space of symmetric positive deﬁnite matrices is obtained. It can
serve as a basis of comparison or be subject to ﬁltering and
selection to obtain a digest of a trafﬁc situation suitable for
efﬁcient complexity assessment.
Keywords—Air trafﬁc complexity; spatial data; manifold val-
ued images; covariance function estimation; non-parametric
estimation.
I. INTRODUCTION
Key performance indicators (KPI) are of common use in air
transportation. However, they are designed mainly to address
global aspects of the systems and cannot address problems,
where it is mandatory to be able to distinguish between
trafﬁc situations based on the structure of the trajectories.
As an example, the training of air trafﬁc controllers relies
on carefully selected trafﬁc patterns that are presented to
the trainees in an order of increasing perceived complexity.
Creating such scenarios is quite a lengthy process, involving
hundreds of hours of works by experienced controllers. On
the other hand, it is easy to start from real situations, with
known ﬂight plans, and to use a trafﬁc simulator to put the
trainees in a nearly operational setting. The drawback of this
approach is the need to evaluate the trafﬁc patterns in order to
assess a complexity value for each of them. It has to be done
automatically, to avoid having to resort to human experts.
In a more operational context, nearly the same question
arises when trying to ﬁnd the right number of controllers
needed at a give time to take care of the incoming ﬂights
in their assigned airspace. Too many controllers induces an
extra cost and too few put a high pressure on the operators,
with possible detrimental effects on ﬂight safety. Assessing the
right level of complexity of the expected trafﬁc may greatly
improve over the current state of the art that simply estimates
the number of aircraft that will be present. Once again, it
is mainly a matter of ﬁnding an adequate trafﬁc complexity
indicator [1] [2].
A lot of work was dedicated to the issue of air trafﬁc
complexity measurement. Unfortunately, no really satisfactory
solution exists, as the problem itself is ill posed: depending on
the point of view, the complexity may be a concept roughly
equivalent to the cognitive workload or, on the contrary, be
based on purely structural features, without any reference
to the way it will be used. One of the most widely used
complexity measures is the dynamic density [3], that com-
bines several potential indicators, like number of maneuvering
aircraft, number of level changes, convergence and so on. All
these values are used as inputs of a multivariate linear model,
or in recent implementations, of a neural network. The tuning
of the free parameters of the predictors is made using examples
coming from an expertized database of trafﬁc situations. While
being quite efﬁcient for assessing complexity values in an
operational context, the method has two important drawbacks:
• The tuning procedure requires a sufﬁcient number of
expertized samples. A costly experiment involving several
air trafﬁc controllers must be set up.
• The indicator is valid only in a speciﬁc area of the
airspace. Adaptation to other countries or even control
centers requires a re-tuning that is almost as complicated
as the ﬁrst one.
The last point is a severe ﬂaw if one wants to use dynamic
complexity in the context of air trafﬁc databases, as a world
covering has to be obtained ﬁrst. Even for country sized
databases, some geographical tuning has to be added.
Another way to deal with complexity is through purely
geometrical indicators [4] [5]. Within this frame, there is no
reference to a perceived complexity but only to structural
features. An obvious beneﬁt is that the same metric may
be used everywhere, without needing a speciﬁc tuning. It is
also the weak point of the method as the relation with the
controllers workload is not direct.
The present article introduces the theoretical material un-
derlying a new approach to complexity assessment and more
generally to trafﬁc characterization, based on a representation
of trafﬁc situations as images whose pixels are covariance
matrices. The idea underlying it is that local disorder is an
indicator of complexity that captures most of the elementary
metrics entering the dynamic density. This is a work in
28
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-631-6
ALLDATA 2018 : The Fourth International Conference on Big Data, Small Data, Linked Data and Open Data

progress that will ultimately allow the use of deep learning
on such pseudo-images in conjunction with an expertized
database to produce a complexity metric with low tuning
requirements. A by product is the ability to compute distances
between trafﬁc situations, allowing for efﬁcient indexing in
dedicated databases. The rest of the paper is structured as
follows. In Section II, the trafﬁc is modeled after a Gaussian
random ﬁeld, whose covariance function is estimated on two
dimensional grid. In Section III, tools dedicated to the pro-
cessing of such grids of symmetric positive deﬁnite matrices
are introduced. Finally, in Section IV, a conclusion is drawn,
introducing the next generation of algorithms able to exploit
this novel representation.
II. TRAFFIC REPRESENTATION
The ﬁrst stage in the computation of the complexity index
is the processing of trafﬁc samples in order to summarize
their local features. It is done by estimating a local covariance
matrix at each point of an evenly spaced grid. While aircraft
positions are actually points in R3, the altitude plays a special
role and is not presented on controllers displays. The choice
made in the present work is to use a planar representation,
disregarding the altitude, which is in compliance with the
operational setting. From here on, all the derivations will be
made using aircraft positions in R2.
All samples are assumed to be dated positions (t, x) where
t is the sampling time and x ∈ R2 is the sample position.
Finally, a dataset is simply a sequence (ti, xi)i=1...N of
samples collected in a given time interval and spatial area.
Please note that samples will not be distinguished by the
trajectory they belong to, so that different ﬂight patterns may
generate exactly the same dataset. This is a limitation of the
present work that will be addressed in a future extension of
the method.
A. A Gaussian ﬁeld model
Collected samples without taking time into consideration
may be viewed as realizations of an underlying spatial stochas-
tic process X with values in R2. Such a process is called
a Gaussian vector ﬁeld when for any collection of points
(x1, . . . , xp), the joint distribution of the random variables
(X(x1), . . . , X(xp)) is Gaussian. Such a process is charac-
terized by its mean and covariance functions:
µ(x) = E[X(x)]
(1)
C(x, y) = E

(X(x) − µ(x))(X(y) − µ(y))t
(2)
In practice, µ and C must be estimated from a dataset of
couples (xi, vi)i=1...N where vi is the observed vector value
at position xi. Available methods fall into two categories:
parametric and non parametric. In the parametric approach, µ
and C are approximated by members of a family of functions
depending on a ﬁnite number of free parameters that are tuned
to best match the observations. A usual choice is to use ﬁnite
cubic splines expansions whose coefﬁcients are identiﬁed by a
least square ﬁtting. The efﬁciency of parametric estimation is
heavily dependent on the actual mean and covariance functions
and may be computationally expensive for large datasets. In
the non parametric approach, a different methodology is used:
the samples themselves act as coefﬁcients of an expansion
involving so-called kernel functions. Apart from the obvious
beneﬁt of avoiding a costly least square procedure, most
of the kernels used in practice are compactly supported, so
that evaluating an approximate mean and covariance at a
given location requires far less terms that the number of
samples. Due to its simplicity and the previous property, a
non parametric estimation was selected to process the trafﬁc.
B. Mean and covariance matrix estimation
The key ingredient in non parametric estimation is the kernel
function, that is a smooth enough mapping K : R2 → R+ that
integrates to 1.
It is usually more tractable to restrict kernels to a smaller
class:
Deﬁnition 1. A radial kernel is a mapping K : R2 → R+ that
is of the form K(x) = K(∥x∥) and such that :
Z
R2 K(x)dx = 1
From here on, only radial kernels will be used.
Kernels are most of the time at least continuous. Further-
more, as mentioned above, compactly supported function will
save a lot of computation in the evaluations and it is thus
advisable to use kernels pertaining to this class. Classical
examples are the multivariate Epanechnikov kernel [6] and
its higher regularity versions which are widely used in the
non-parametric estimation community:
Ke(x) = 2
π

generalized to yield an estimate for the covariance function,
under normality assumption for the random ﬁeld X. Given
a dataset (xi, vi)i=1...N, where the sampling positions xi are
assumed to be distinct, the weighted joint log likelihood of
the couples (xi, vi), (xj, vj), j ̸= i at locations x, y is given,
for a ﬁxed kernel bandwidth h, by:
L(x, y) =
− 1
2
N
X
i=1
N
X
j=1
V t
ijΣ−1(x, y)VijKh (∥xi − x∥) Kh (∥xj − y∥)
+ 1
2 log

Algorithm 1 Mean kernel estimate
1: for i ← 0, 2L;j ← 0, 2 ∗ M do
2:
m(i, j) ← 0
3: end for
4: for k ← 0, N − 1 do
5:
(k, l) ← ClosestGridPoint(xi)
6:
for i ← −P, P;j ← −Q, Q do
7:
if k + i ≥ 0 ∧ k + i ≤ 2L then
8:
if l + j ≥ 0 ∧ l + j ≤ 2M then
9:
m(k + i, l + j) ← m(k + i, l + j) +
Kh(i, j)vi/N
10:
end if
11:
end if
12:
end for
13: end for
Algorithm 2 Covariance kernel estimate
1: for i ← 0, 2L;j ← 0, 2 ∗ M do
2:
C(i, j) ← 0
3: end for
4: for k ← 0, N − 1 do
5:
(k, l) ← ClosestGridPoint(xi)
6:
for i ← −P, P;j ← −Q, Q do
7:
if k + i ≥ 0 ∧ k + i ≤ 2L then
8:
if l + j ≥ 0 ∧ l + j ≤ 2M then
9:
A ← (vi − m(k, l))(vi − m(k, l))t
10:
C(k + i, l + j) ← C(k + i, l + j) +
Kh(i, j).A/N
11:
end if
12:
end if
13:
end for
14: end for
p(k1l1)
p(k2l1)
p(k2l2)
p(k1l2)
xi
dy
dx
Fig. 1. Bilinear interpolation
with:
a = Kh(k2, l1) − Kh(k1, l1)
b = Kh(k1, l2) − Kh(k1, l1)
c = Kh(k2, l2) + Kh(k1, l1) − Kh(k2, l1) − Kh(k1, l2)
Gathering terms by tabulated values yields a kernel value:
Kh(k1, l1) (1 − sx − sy + sxsy)
(18)
+ Kh(k2, l1) (sx − sxsy)
(19)
+ Kh(k1, l2) (sy − sxsy)
(20)
+ Kh(k2, l2)sxsy
(21)
where:
sx = dx
δx
, sy = dy
δy
It
is
thus
possible
to
compute
the
mean
and
co-
variance functions on a coarser grid using Algorithms
1
and
2
by
applying
them
on
the
four
locations
(k1, l1), (k2, l1), (k1, l2), (k2, l2), with an observed value mul-
tiplied by their respective coefﬁcients (1 − sx − sy + sxsy),
(sx − sxsy),(sy − sxsy),Kh(k2, l2)sxsy.
The overall complexity of the algorithm is linear in the
number of grid points and in the number of samples. It is
similar to ﬁltering an image and can be implemented the
same way on modern Graphics Processing Units (GPU). Please
note also that for kernels with large supports, a fast Fourier
transform may be used at the expense of a slight increase in
the complexity order that will be balance by the constant term
due to the support size.
III. PROCESSING TOOLS
The preceding phase allows the computation of a trafﬁc
pattern digest as a two dimensional grid of Symmetric Positive
Deﬁnite (SPD) matrices. It may be used as is for building
an index in a database, using the same procedure as for
images. However, the geometry underlying the space of 2 × 2
positive deﬁnite matrices is not euclidean, but hyperbolic. The
proposed index is an adaptation of images distances, using
hyperbolic geometry.
A. The Riemannian structure of symmetric positive deﬁnite
matrices
The purpose of this part is to introduce at a basic level the
tools used to build the index. Results are given without proofs,
the interested reader may refer to [11] for a more in-depth
exposition.
Proposition 1. The space of n × n SPD matrices, denoted
by SPD(n), may be endowed with a Riemannian manifold
structure with metric at point A given by the differential:
ds2 = tr

Proposition 2. Let A, B be SPD matrices. It exists a unique
minimizing geodesic joining them in SPD(n). It is given in
parametrized form by:
γ : t ∈ [0, 1] 7→ A1/2 
exp t log

A−1/2BA−1/2
A1/2
(23)
Proposition 2 yields the geodesic distance between any two
matrices A, B from SPD(n) as d(A, B) =
q
tr log2(A−1B.
It can be expressed as d(A, B) =
qPn
i=1 log2 λi with λi, i =
1 . . . n the eigenvalues of A−1B.
The geodesic distance between matrices from SPD(2) may
be used to compute a distance between grids produced by the
trafﬁc processing phase in a very simple way, as indicated in
Algorithm 3.
Algorithm 3 Distance between grids
1: A, B are P × Q grids of SPD(2) matrices.
2: dsq = 0
3: for i ← 0, P − 1;j ← 0, Q − 1 do
4:
dsq ← dsq + tr log2(A(i, j)−1B(i, j)
5: end for
6: d(A, B) = √dsq
Please note that this distance is based on a point-wise
comparison and is very similar to the L2 distance used for
images. It has a higher cost of evaluation due to the distance
computation in SPD(2) that involves an matrix inverse, prod-
uct and logarithm. However, it is not as critical in SPD(2)
than it may be in a general SPD(n), since eigenvalues and
eigenvectors can be computed in closed form (this is true also
in SPD(3) and SPD(4), with more complex expressions).
Furthermore, grid distance computation is easy to parallelize
on modern graphics hardware since it involves independent
operations on small matrices. As an example, computing the
distance between two grids of size 100 × 100 on a TitanX
pascal card from Nvidia takes around 100µs.
B. Grid ﬁltering
In the trafﬁc processing phase, grids have sizes ranging
from 100 × 100 to 300 × 300. Due to the processing cost
incurred by the SPD(2) setting, it is advisable in many cases,
and especially if one wants to use the grids as index in a
trafﬁc database, to reduce the size of grids to more tractable
dimensions, say 10×10 to 50×50. This has to be done without
wiping out the salients features of the trafﬁc captured by the
original grid. In the spirit of what is done in the ﬁrst layers
of an image processing deep network, it is proposed to apply
in sequence a ﬁltering and a selection process on the original
grid.
Deﬁnition 2. Let Ai, i = 1 . . . n be a sequence of elements
of SPD(n), w1, . . . wn be a sequence of real numbers and
B be an element of SPD(n). The log-euclidean weighted
combination (LWC) at B of the (Ai)i=1...n with weights
(wi)i=1...n is the matrix:
B1/2 exp
 n
X
i=1
wi log

B−1/2AiB−1/2!
B1/2
(24)
The LWC may be used to compute a ﬁltered version of a
grid using the same procedure as for an image. The process
is given in Algorithm 4 that yields the ﬁltered grid as B.
Algorithm 4 Grid ﬁltering
1: A is a P × Q grid of SPD(2) matrices.
2: wi, i = 1 . . . 9 is a sequence of real numbers
3: for i ← 0, P − 1;j ← 0, Q − 1 do
4:
(C1, . . . C9) are the adjacent cells to A(i, j) and itself.
5:
B(i, j) ← LWC(C1, . . . , C9) with weight wi, i =
1 . . . 9 at A(i, j).
6: end for
The ﬁltering process on SPD(2) grids behaves roughly like
in image processing: when the weights are real numbers in the
interval [0, 1] that sum to 1, then a weighted mean is produced.
It tends to smooth out the grid, making spatially close matrices
more similar. On the opposite, when weights sum to 0, the
equivalent of a high pass ﬁlter is produced, that emphases
sharp variations. Please note that the size of the grids after
ﬁltering is unaltered.
The second processing phase is simpliﬁcation to reduce grid
size. The main idea is to replace a block of grid cells by a
single one using a digest. An obvious approach is to replace
a block by its mean, that can be obtained from LWC by using
equal positive weights 1/n if n is the number of cells in the
block. A major drawback is that the important information
tends to be lost, with matrices going close to multiples of
the identity in many cases. Another way of dealing with the
problem is to introduce an order on SPD(2) and to select the
largest (resp. lowest) element in the block. This procedure has
two beneﬁts:
• The selected matrix is an element of the original grid.
• As in deep learning networks, it will select the most
representative elements.
After some experiments on simulated matrix images, the order
chosen is a lexicographic one, the ﬁrst comparison being made
on the determinant of the matrices and the second on the trace.
After the selection phase, the size of the grid is reduced by the
ratio of the number of elements considered in a block. In the
current implementation, it is 3×3, thus shrinking the grid by a
factor 3 in each dimension. The ﬁltering/selection phases may
be chained in order to get smaller grids. As for the distance
computation, it is quite easy to implement the process on a
GPU, all operations being independent.
IV. CONCLUSION AND FUTURE WORK
The work presented here is still in early stage of develop-
ment. Only theoretical concepts and computer implementation
32
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-631-6
ALLDATA 2018 : The Fourth International Conference on Big Data, Small Data, Linked Data and Open Data

of the trafﬁc processing, ﬁltering and selection phases are
completed or nearly completed. Testing on real data is yet
to be done, and a complete complexity computation metric
has to be built. The next steps are:
• Constitution of a trafﬁc database from surveillance
(Radar) and ADS-B data. This work has been launched,
and data collection is in progress.
• Complexity index coding. Although most of the software
bricks needed are available or close to release, it remains
to implement the overall process. A weight adjustment
procedure is lacking for the ﬁltering phase: it is the topic
of a more theoretical work and involves some Lie group
techniques.
• Evaluation against existing indicators and expert advices.
Based on the experience gathered on the topic, it is expected
that the new approach presented here will outperform the
current state-of-the-art metrics. Furthermore, thanks to the
ability to compute the distance between two grids of SPD(2)
elements, it offers the unique opportunity to derive an index
for a trafﬁc situation database that will be an invaluable tool
for practitioners in the ﬁeld of air trafﬁc management.
REFERENCES
[1] M. Prandini, L. Piroddi, S. Puechmorel, and S. L. Brazdilova, “To-
ward air trafﬁc complexity assessment in new generation air trafﬁc
management systems,” IEEE Transactions on Intelligent Transportation
Systems, vol. 12, no. 3, pp. 809–818, Sept 2011.
[2] A. Cook, H. A. Blom, F. Lillo, R. N. Mantegna, S. Miccich, D. Rivas,
R. Vzquez, and M. Zanin, “Applying complexity science to air trafﬁc
management,” Journal of Air Transport Management, vol. 42, pp. 149
– 158, 2015. [Online]. Available: http://www.sciencedirect.com/science/
article/pii/S0969699714001331
[3] L. I. S. Shelden, R. Branstrom, and C. Brasil, “Dynamic density: An
air trafﬁc management metric,” NASA, Tech. Rep. NASA/TM-1998-
112226, 1998.
[4] K. Lee, Feron, and A. E.and Prichett, “Air trafﬁc complexity : An
input-output approach,” in Proceedings of the US Europe ATM Seminar.
Eurocontrol-FAA, 2007, pp. 2–9.
[5] D. Delahaye and S. Puechmorel., “Air trafﬁc complexity based on
dynamical systems.” in Proceedings of the 49th CDC conference. IEEE,
2010.
[6] V. A. Epanechnikov, “Non-parametric estimation of a multivariate
probability
density,”
Theory
of
Probability
&
Its
Applications,
vol.
14,
no.
1,
pp.
153–158,
1969.
[Online].
Available:
https:
//doi.org/10.1137/1114019
[7] P. Hall, N. I. Fisher, and B. Hoffmann, “On the nonparametric estimation
of covariance functions,” Ann. Statist., vol. 22, no. 4, pp. 2115–2134,
12 1994. [Online]. Available: https://doi.org/10.1214/aos/1176325774
[8] Y. Li, N. Wang, M. Hong, N. D. Turner, J. R. Lupton, and R. J. Carroll,
“Nonparametric estimation of correlation functions in longitudinal and
spatial data, with application to colon carcinogenesis experiments,”
Ann. Statist., vol. 35, no. 4, pp. 1608–1643, 08 2007. [Online].
Available: https://doi.org/10.1214/009053607000000082
[9] J. Yin, Z. Geng, R. Li, and H. Wang, “Nonparametric covariance
model,” Statistica Sinica, vol. 20, no. 1, pp. 469–479, 2010. [Online].
Available: http://www.jstor.org/stable/24309002
[10] E. A. Nadaraya, “On estimating regression,” Theory of Probability &
Its Applications, vol. 9, no. 1, pp. 141–142, 1964. [Online]. Available:
https://doi.org/10.1137/1109020
[11] F. Nielsen and R. Bhatia, Matrix Information Geometry.
Springer
Berlin Heidelberg, 2012. [Online]. Available: https://books.google.fr/
books?id=MAhygTspBU8C
33
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-631-6
ALLDATA 2018 : The Fourth International Conference on Big Data, Small Data, Linked Data and Open Data

