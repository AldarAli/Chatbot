Method to Use Low-priced Data-glove Effectively
Based on Medical Knowledge for Hand Motion Pattern
Kenji Funahashi
Department of Computer Science
Nagoya Institute of Technology
Nagoya 466–8555 Japan
Email: kenji@nitech.ac.jp
Yutaro Mori1
Hiromasa Takahashi2
Nagoya Institute of Technology
(1present: NEC Solution Innovators, Ltd.)
(2present: Chubu Electric Power Co.,Inc.)
1Email: moriyu@center.nitech.ac.jp
2Email: hiromasa@center.nitech.ac.jp
Yuji Iwahori
Department of Computer Science
Chubu University
Kasugai, Aichi 487-8501 Japan
Email: iwahori@cs.chubu.ac.jp
Abstract—A data glove is one of the major interfaces used in the
ﬁeld of virtual reality. In order to get detailed data about the
ﬁnger joint angles, we must use a data glove with many sensors.
However, a data glove with many sensors is expensive and a low-
priced data glove does not have enough sensors to capture all the
hand data correctly. We propose a method to obtain all ﬁnger
joint angles by estimating the pattern of hand motion from the
low-priced data glove sensor values. In our experiment system, we
assumed some representative hand motion patterns as grasping
behavior based on medical knowledge. We also assumed that
other hand motions can be represented by synthetic motion of
the representative patterns. In this paper, we used the data glove
with sensors covering two joints of each ﬁnger. And we also
estimate the ﬁnger joint angles when using the data glove that
sensors cover only the middle angle of each ﬁnger.
Keywords–Data-glove; Hand motion estimation; Finger joint
angles estimation; Medical knowledge.
I.
INTRODUCTION
Virtual Reality (VR) is a rapidly growing research ﬁeld
in recent years. VR technologies give us various advantages.
There are simulators to practice an operation and to ﬂy a plane
as examples of VR technologies. These simulators enable us to
avoid the risk and to save on cost. VR researches that targets
to households also have been attracted. A data glove is one
of the major interfaces, which are used in the ﬁeld of VR.
It measures curvatures of ﬁngers using bend sensors. In our
laboratory, we propose a method to get plausible user hand
motion pattern from the low-cost glove [1]. In our ﬁrst work,
we use 5DT Data Glove (see Figure 1) whose sensors cover
two joints of each ﬁnger. Then, we estimate ﬁnger joint angles
when using the data glove DG5 VHand (see Figure 2) whose
sensors cover only the middle angle of each ﬁnger.
The rest of the paper is structured as follows. In Section
II, we present a state of the art of data gloves. In Section
III, we describe a method how to estimate ﬁnger joint angels.
In Section IV, we describe about representative hand motion
patterns based on medical knowledge. In Section V, we apply
this method to the data-glove whose sensor positons are
limited. In Section VI, the experimental results are shown. In
Section VII, we consider the difference between users’ hand
shapes. In Section VIII, the experimental results for hand shape
are shown. Finally, we conclude in Section IX.
Figure 1. 5DT Data Glove 5 Ultra
Figure 2. DG5 VHand
II.
STATE OF THE ART
In order to obtain accurate hand motions, it is necessary to
use a data glove, i.e. Immersion CyberGlove, which has many
sensors, but it is expensive. It is preferable that an interface is
small scale and low cost. Various types of researches about data
glove have been conducted [2][3][4][5]. On the other hand,
there is a low cost data glove, which measures an angle for
each ﬁnger through one sensor. But it cannot get detailed data
directly. For example, the 5DT Data Glove 5 Ultra and DG5
VHand have a single sensor on each ﬁnger, so they have ﬁve
sensors in the whole hand (see Figures 1 and 2). However,
there are three ﬁnger joints for each ﬁnger, a single sensor can
not measure all of these three angles directly.
Our proposed novel method estimates the kind of hand
motion patterns using each relation among angles of ﬁngers
241
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

!"#$%"#&"#$"#'#()%*+#&,%(-&./*/(&$%0#"('&
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&!"#$%"%*/(&
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&1'#2*/(&33456&&
7#28-#&$%"%.#)#"'&)/&2%92:9%)#&;(<#"&=/8()&%(<9#'&&
&&&>/"&#%2,&"#$"#'#()%*+#&,%(-&./*/(&$%0#"(!
&
&
3($:)&7%)%&?9/+#&@#('/"&+%9:#'!
&
&
A'*.%)#&),#&,%(-&./*/(B&
&&&"%*/&>/"&"#$"#'#()%*+#&,%(-&./*/(&$%0#"('&
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&!"/2#''&8(&"#%9&*.#&
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&1'#2*/(&334C6&
D%92:9%)#&&&%99&%(<9#'&&
&&&:'8(<&),#&$%"%.#)#"'&/>&$%0#"('&%(-&),#&"%*/&
!
Figure 3. Overview of method
during operation. Then, it estimates all ﬁnger joint angles by
estimating the types of hand motion patterns from the corre-
lation between each ﬁnger angle in the hand motion pattern.
We assume some representative hand motion patterns based on
medical knowledge [6], and consider that other hand motions
can be represented as a synthetic motion of the representative
hand motion patterns. In addition, we calculate the ratio of each
representative motion pattern. Moreover, estimating each ﬁnger
angle using the result, we express any hand motion patterns
other than the representative hand motions.
III.
ESTIMATION OF FINGER JOINT ANGLES
In Section III, we describe an estimation method of ﬁnger
joint angles using 5DT data glove, which is developed in our
laboratory (see Figure 3).
A. Representative Hand Motion Patterns
To estimate ﬁnger joint angles, this method limits user’s
hand motion to grasping motion. First of all, we had set the
three representative hand motions as grip, pinch and nip.
Furthermore, we assume that a human’s grasping motion
can be represented as a synthetic motion of representative
hand motion patterns. To derive three ﬁnger joint angles from
a single sensor value, we use the following method (see
Figure 4). We sample many sets of the sensor values with the
low-priced data glove when some subjects open their hand ﬁrst
and then close it to each representative hand motion patterns.
Also, we sample the sets of the true angles of ﬁnger joints
for the same representative patterns, provided that we use true
angles obtained from a data glove, which has a lot of sensors.
We use Immersion CyberGlove as data glove with a lot of
sensors. Then, the sensor values and the true angles of ﬁnger
joints at the same time are associated. We show an example
of correspondence in Figure 5.
We derive the following numerical formulas using this
!"#$%&'())*+,!
!"""!"""!"""###"""!"""###$
$
!"""!"""!"""""""""!""""$
$
!"""!"""!"""""""""!""""$
$
%"""""""""""""""""%$
$
!"""!"""!"""""""""!""$
$
!"""!"""!"""###"""!"""###$
$
-.#/-0#(&1(2/3#(-'04#!
-(/#5/#"#'2-%.#(5-6#/'(p 
"#'"&/(.-43#(((78(
(S(!S1, S2, ... S5) 
µpn, Σpn(((((977 
97(-'04#"((
(((((((&1((:&;'2"(θpij(
Epij, Fpij, 
 Gpij, Hpij 
$3//#'2("#'"&/(
(((((.-43#(S 
$3//#'2(-'04#"<(
(((=(
(((>(
$4&"# 
(((>(
(((>(
(((>(
&5#'(
(((>(
(((.(
?-'@(A&%&'(!/-%&,(
#-$?("#2(&1("-A54#!
Figure 4. Detail of method
correspondence.
θpi1
=
2
3θpi2
(1)
θpi2
=
Epi2S3
i + Fpi2S2
i + Gpi2Si + Hpi2
(2)
θpi3
=
Epi3S3
i + Fpi3S2
i + Gpi3Si + Hpi3
(3)
where pattern p is one of representative hand motion patterns.
Angles θpi1, θpi2 and θpi3 express the DIP, PIP, and MP joint
angle of the ﬁnger i for the pattern p. The DIP, PIP, and
MP joint mean the ﬁrst, second and third joint of a ﬁnger
respectively. The Si is sensor value of ﬁnger i. And Epij, Fpij,
Gpij and Hpij are constant parameters for the pattern p, ﬁnger
i and joint j. These parameters, Epij to Hpij, are calculated
by pre-experiment. Besides, DIP joint angle is obtained by
proportional connection with PIP joint angle (equation (1)) [7].
Joint angles of ﬁnger i of pattern p are obtained by these
numerical formulas.
B. Hand Motion Estimation and Angles Estimation
To represent user’s hand motion as synthetic motion of
representative hand motion patterns, we need to know how
similar the user’s hand motion is and to which representative
hand motion patterns. Then, we set the following formula
based on the probability density function of the multivariate
normal distribution for n points in the ﬁve dimensional feature
amount space.
Lpn = exp{−1
2(S − µpn)T Σ−1
pn (S − µpn)}
(4)
where S is the sensor value vector. And µpn and Σpn
represent mean vector of sensor sample values, and variance-
covariance matrix of sample point n (an integer satisfying
1≤n≤a number of samples) in representative hand motion pat-
tern p. Besides, µpn and Σpn are obtained by pre-experiment
for an average user. If the sensor values are obtained actually
242
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Figure 5. Example of correspondence
!"#$%#&%
'&()*%
+#&#,,-,./0"
1(&23,#&.4,-0
'().1*$"#2"
5**67,(6-
8$%-0./0"
+9#&#$:-#, /0"
;#"-&#,.1*$"#2"
+#&#,,-,.4,-0
Figure 6. Candidates of representative motions
from the glove, we select the maximum value according to the
following formula.
Lp = max
n {Lpn(S : µpn, Σpn)}
(5)
Thus, we get the likelihood on representative hand motion
pattern p in current sensor values. After that, we decide the
ratio rp of hand motion pattern p according to the following
formula.
rp =
Lp
ΣP
p=1Lp
(6)
where P is the total number of representative hand motions,
which takes the value of four. As stated above, we can obtain
θpij and rp. At last, each angle θij of current hand posture is
derived by the following formula.
θij =
P
∑
p=1
rp·θpij
(7)
Figure 7. Example of MP and PIP Joint Angle of Index Finger
Figure 8. Dendrogram of the Candidata 1
Figure 9. Average Hand Motions (left: MC2, right: MC3)
243
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Figure 10. Representative hand motion patterns
IV.
RECONSIDER REPRESENTATIVE HAND MOTIONS
In Section IV, we reconsider the representative hand motion
patterns based on medical knowledge.
A. Candidate Selection
We reconsider them through the research on the grasping
behavior of human hand [8]. They had observed daily grasping
forms in experimental condition and classiﬁed them into 14
types to help reference in clinical. We select 10 candidates
as representative hand motion from these 14 types, because
they change enough sensor values of a data-glove respectively
(Figure 6). And we obtained the transitions of each ﬁnger
joint angle of the 10 motions from open hand to each form
using data-glove, which has many sensor (CyberGlove). It was
also conﬁrmed that Parallel Flex and Circular Flex can be
represented in a part of Standard (Figure 7), and Phalangeal
Ext can be represented in a part of Lateral Contact. Therefore,
we selected 7 motions as representative ones of candidate No.1.
B. Candidate Reduction
If one of the representative hand motions is similar to
another one, it may not occur good estimation. If the number of
the motions of candidate 1 can be reduced with enough result,
TABLE I. JOINT ANGLE ERROR (INPUT: REPRESENTATIVE ONE)
[DEGREE]
thumb
index
middle
ring
little
average
Candidate 1
8.3
5.7
3.9
3.4
6.5
5.6
Candidate 2
5.9
2.5
3.2
4.9
3.0
3.9
Candidate 3
7.2
4.2
3.2
4.5
4.2
4.7
TABLE II. JOINT ANGLE ERROR (INPUT: EXCEPT REPRESENTATIVE
ONE) [DEGREE]
thumb
index
middle
ring
little
average
Candidate 1
8.5
9.4
9.1
6.5
18.5
10.4
Candidate 2
9.4
9.6
9.6
6.5
17.55
10.5
Candidate 3
8.2
8.7
8.7
7.4
12.22
9.0
we can remove the redundant computation. So, we sampled
the sensor values for the candidate 1 and standardize them
(mean 0 and variance 1). Then we performed hierarchical
cluster analysis for them using the ward method to create
a dendrogram about the candidate 1 (Figure 8). The hand
motions were classiﬁed into 4 classes based on the cutting
point 2.5 as a middle distance. The classes are deﬁned as
following; C1: Standard, C2: Hook-like, Lateral Contact, Index
Ext, C3: Tripod, Tip Contact, and C4: Parallel Ext. Thereby
Standard, Lateral Contact, Tripod and Parallel Ext were se-
lected as candidate No.2 according to the score. Furthermore
we constructed average hand motions MC2 and MC3 for
the classes C2 and C3 respectively (Figure 9), and obtained
candidate No.3.
C. Conﬁrmation Experiment
We had experiment for the three candidates using the
5DT Data Glove 5 Ultra, which has a bend sensor for each
ﬁnger. When the input data are the representative motions
in this experiment, the averages of estimated ratio rp are;
candidate 1: 0.83, candidate 2: 0.86, and candidate 3: 0.95. We
can also conﬁrm the average of candidate 3 is higher than the
average 0.92 of conventional system with ﬁrst representative
hand motions as grip, pinch and nip. Table I shows the average
of the errors of DIP, PIP and MP between estimated ﬁnger joint
angles and obtained angles by CyberGlove. The input data is
Tripod motion for candidate 1 and 2, and MC2 for candidate 3.
Table II shows the error when the input motion data is not
representative one for each candidate, that is, the input data is
MC2 motion for candidate 1 and 2, and Tripod for candidate 3.
We conﬁrmed that the error of candidate 3 is smallest for the
average of both results, and it can deal with any hand motions
other than the representative ones. Therefore, we found that
candidate No.3 is the most suitable for representative hand
motions, and candidate No.2 is the second suitable one.
V.
DATA-GLOVE WHOSE SENSOR POSITIONS ARE
LIMITED
In Section V, we describe an estimation method of ﬁnger
joint angles using DG5 data glove whose sensor positions are
limited only to PIP joints.
A. MP Angle for Representative Hand Motion Pattern
Although we mentioned above a set of representative hand
motion patterns is selected as candidate No.3, the pattern Par-
244
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

allel Ext is almost the motion related only to MP joints. When
doing the Parallel Ext pattern, the sensor values hardly change.
We tentatively use three other patterns as representative hand
motion patterns except Parallel Ext from the candidate No.2
(Figure 10) for now.
For the 5DT data glove whose sensors cover PIP and MP
joints, the DIP angle is related to PIP directly, as mentioned in
the Section III. It means the sensor values contain all of their
information. However, using DG5 whose sensors are only on
PIP, the motion of MP does not change the sensor value. Of
course, we assume that the hand motion is a grasping one, so
the MP angle of a ﬁnger is related to the PIP angle of the
same ﬁnger. Then we can assume that the MP of a ﬁnger is
related to the PIPs of all ﬁngers.
We consider a new estimation model to obtain angles for
representative hand motion patterns using multiple regression
analysis. First, we make a estimation equation with explanatory
variable is a set of sensor values, and response variable is each
MP joint angle, as follows.
θpi3 =
5
∑
f=1
Cpif3Sf + Ipi3
(8)
where θpi3 is MP joint angle of ﬁnger i of representative
pattern p, Sf is sensor value of ﬁnger f, and Cpif3 and Ipi3
are constant.
Now, a subject opens his hand ﬁrst and then closes it
to each representative pattern with DG5 data glove, the set
of sensor value Sf(time) of ﬁnger f at time is sampled.
Then, the subject moves his hand as each same pattern with
CyberGlove, which has many sensors, the set of angle value
θpi3(time) is sampled as true one.
Here, we should get the constant Cpif3 and Ipi3. The
residual sum of squares Q is represented as in (9).
Q =
∑
time


θpi3(time) −


5
∑
f=1
Cpif3Sf(time) + Ipi3





2
(9)
Focusing on coefﬁcient Cpi13 where f = 1;
Q =
∑
time



(
S1(time)Cpi13
)2
+ 2S1(time)Cpi13


5
∑
f=2
Cpif3Sf(time) + Ipi3


− 2θpi3(time)S1(time)Cpi13
+


5
∑
f=2
Cpif3Sf(time) + Ipi3


2
− 2θpi3(time)


5
∑
f=2
Cpif3Sf(time) + Ipi3


+
(
θpi3(time)
)2



(10)
Using the partial differentiations with Cpi13;
∂Q
∂Cpi13
= 2
∑
time
S1(time)



5
∑
f=1
Cpif3Sf(time)
+Ipi3 − θpi3(time)



(11)
Using the partial differentiations also with Cpif3 and Ipi3;
∂Q
∂Cpif3
= 2
∑
time
Sf(time)



5
∑
f ′=1
Cpif ′3Sf ′(time)
+Ipi3 − θpi3(time)



(12)
∂Q
∂Ipi3
= 2
∑
time


Ipi3 +
5
∑
f=1
Cpif3Sf(time)
−θpi3(time)



(13)
The constant Cpif3 and Ipi3 to be obtained make Q represented
as the minimum of the equation from (9). And the Cpif3 and
Ipi3 that make Q minimum satisfy following equation.
∂Q
∂Cpif3
= ∂Q
∂Ipi3
= 0
(14)
Solving this, coefﬁcient Cpif3 and constant Ipi3 are obtained
to estimate MP joint angle for representative pattern with (8).
The angles of PIP are obtained directly from the sensor value
with (2), and the angles of DIP are also obtained only from
PIP with (1).
B. Hand Motion Estimation with Pseudo-Inverse Matrix
When the variance of sensor values is zero at the sample
point n of representative hand motion pattern, the variance-
covariance matrix will be abnormal at the sample point n.
It means the inverse matrix of variance-covariance matrix of
sensor values Σ−1
pn can not be obtained, and the likelihood for
the sample data of representative pattern p can not be obtained
with (4).
So, we use Moore-Penrose pseudo-inverse matrix to solve
it. The variance-covariance 5×5 matrix of sensor values Σ−1
pn ,
which is abnormal at the sample point n is represented as next
equation with 5 × r matrix Apn and r × 5 matrix Bpn where
rank (Σpn) = r;
Σpn = ApnBpn
(15)
Here the Moore-Penrose pseudo-inverse matrix Σ+
pn for Σpn
is described as:
Σ+
pn = BT
pn
(
AT
pnΣpnBT
pn
)−1
AT
pn
= BT
pn
(
BpnBT
pn
)−1 (
AT
pnApn
)−1
AT
pn
(16)
Using this Moore-Penrose pseudo-inverse matrix Σ+
pn for (4)
instead of the inverse matrix of variance-covariance matrix of
245
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(1) Power grasp
(2) Precision grasp
(3) Lateral grasp
(4) Extension grasp
(5) Tripod grasp
(6) Index pointing
(7) Basic gestures
Figure 11. Hand motions needed for ADL
sensor values Σ−1
pn at the sample point n where inverse matrix
can not be deﬁned, the likelihood is obtained and the ratio
of each hand motion pattern is determined with (5) and (6),
respectively. Now, we can use a low-priced data glove whose
sensors cover only the middle angle of each ﬁnger to estimate
all ﬁnger joint angles of current hand posture with (7).
VI.
EXPERIMENT AND RESULT
We performed an experiment to conﬁrm the effectiveness
of the method described above. The experiment system was
constructed using the DG5 Data Glove whose sensor positions
are limited only on middle joints. Other hand motions that
were different from representative patterns were tested. The
minimum of Activities of Daily Living (ADL) needs the
following hand motions (see Figure 11) [9].
1)
Power grasps (used in 35% ADLs)
2)
Precision grasps (30% ADLs)
3)
Lateral grasps (20% ADLs)
4)
Extension grasps (10% ADLs),
5)
Tripod grasps,
6)
Index pointing, and
7)
Basic gestures.
We tested ﬁve motions; 1)–5).
Figure 12. Result CG for Power grasp
Figure 13. Result CG for Precision grasp
The subjects opened their hands and then closed them to
each test pattern 1)–5) with DG5 data glove. The average of
estimated joint angles were compared with the true angles
obtained from CyberGlove, which had many bend sensors.
Table III shows the average error of ﬁnger joint angles.
Each error is around 10 degrees. The result using the 5DT data
glove whose sensors cover two joints of each ﬁnger also had
about 10 degrees error [6]. This means that the lower-priced
data glove can obtain joint angles accurately enough.
Actual hand posture images and the CG images generated
from estimated joint angles are shown in Figures 12 and 13.
The MP joints that were not covered with bend sensors are
estimated from the sensors on PIP joints.
VII.
DIFFERENCE OF HAND SHAPE
In the method stated in previous sections, parameters are
needed to be precomposed for each user. However, using an
TABLE III. ERROR OF FINGER JOINT ANGLES [DEGREE]
thumb
index
middle
ring
little
average
Power G.
7.3
12.0
10.5
12.5
10.0
10.5
Precision G.
8.1
9.2
7.2
7.0
6.8
7.7
Lateral G.
9.4
6.0
8.8
7.5
10.5
8.4
Extension G.
9.8
8.1
11.0
11.3
9.0
9.9
Tripod G.
8.5
8.5
7.2
11.6
10.9
9.3
average
8.6
8.7
8.9
10.0
9.4
9.2
246
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

expensive glove to obtain the true angles of ﬁnger joints is
not suitable from perspective of utilization in ordinary home.
Furthermore, parameters to calculate angles are obtained by
a lot of trials of hand motions. They are troublesome for
general user. In this section, we try to determine the parameters
automatically for motion and angles estimation.
A. Estimation Accuracy between Different Users
We investigated estimation accuracy between different
users. With the cooperation of three research participants, we
had an experiment. In this experiment we asked each partici-
pant to grasp a plastic bottle (500ml) with equipped data glove.
The reason to choose this grasping motion was user’s hand
motion is little by little different every time even if user thinks
that one performs the same hand motion. And we deﬁned the
hand size of user as Hsize, which is decided by the distance
from the wrist to the top of the middle ﬁnger (Figure 14). The
Hsize of each participant is shown in Table IV. The sample
person is who provided each parameter for estimation in pre-
experiment. And the parameters for estimation were obtained
by the sample person’s hand. When each participant grasped
the plastic bottle, their ﬁnger joint angles were estimated by
these parameters of sample person. We measured ﬁnger joint
angles when their hand was touching completely with the
plastic bottle. And we investigated the average of the errors
between estimated ﬁnger joint angles and obtained angles by
CyberClove. Table V shows the results. These results indicate
that estimation accuracy using parameters of the person whose
hand size is different becomes worse. We expected that joint
angle errors of the sample person was minimum because
sample person’s parameters were used for estimation. However
the average error of participant 2 was minimum in Table V.
We concluded that the reason was the sensor values were
not uniform but also scattering when user moved one’s hand.
However, only because of these numerical values, the results
can not be judged whether they are signiﬁcant or not. Then
we had statistical hypothesis testing to conﬁrm these results
are signiﬁcant. At this time, we adopted Student’s t-test. We
had Student’s t-test to the average of joint angle errors of
the sample person and other participants. Test statistic t0 is
obtained from the following formula.
t0 =
| ¯X − ¯Y |
√
Ue( 1
m + 1
n)
(17)
where ¯X and ¯Y are the average of joint angle errors, m and n
represent the sample size of two groups. And Ue is obtained
from the following formula.
Ue = (m − 1)Ux + (n − 1)Uy
m + n − 2
(18)
where Ux and Uy are unbiased variance. As stated above, test
statistic t0 can be obtained and t0 follows t-distribution. P-
values obtained from Student’s t-test are shown in Table VI.
The P(T≤t) represents signiﬁcance probability. In this paper,
we decide that signiﬁcance level α is 0.05. So, it is statistical
signiﬁcance if P(T≤t) is smaller than 0.05. Looking at
Table VI, the P(T≤t) in all categories are smaller than 0.05.
They indicate that there are statistical signiﬁcance in estimation
accuracy between the sample person and other participants. We
conﬁrmed necessity of determining parameters for each user.
Figure 14. Deﬁnition of hand size
TABLE IV. Hsize OF EACH PARTICIPANT [cm]
Hsize
standard deviation
participant 1
17.0
-
participant 2
18.1
-
participant 3
20.5
-
sample person
17.7
-
average of Japanese male [10]
18.3
0.8
average of Japanese female [10]
16.9
0.7
B. Hand Size Estimation
We assume that parameters to calculate ﬁnger joint angles
are determined by knowledge of user’s Hsize. To evaluate
user’s Hsize, we try to use the sensor values when user
performs one hand motion. When deciding hand motion for
estimation of hand size, it is important that a hand motion
is simple. If it is obscurity motion, there is difﬁculty in
performing hand motion. Then, we consider the total value
of ﬁve sensors when user closes hand (=Stotal). We obtained
Stotal and each Hsize from each research participant. The cor-
respondence between Hsize and Stotal is shown in Figure 15.
Then we conclude the following formula.
Hsize = aStotal + b
(19)
where a, b are constant parameters. Using this formula,
user’s Hsize can be obtained by performing the simple hand
motion.
TABLE V. JOINT ANGLE ERROR OF GRASPING PLASTIC BOTTLE
[DEGREE]
Thumb
Index
Middle
Ring
Little
avg.
sample
7.6
17.9
11.2
16.2
14.8
13.6
participant 1
31.1
17.7
26.4
5.0
5.1
17.1
participant 2
15.3
17.7
11.6
11.7
5.0
12.3
participant 3
30.2
10.5
27.2
17.7
17.7
20.7
TABLE VI. P-VALUES OF STUDENT’S T-TEST
P (T ≤t)
participant 1
6.8 × 10−6
participant 2
1.0 × 10−2
participant 3
9.6 × 10−4
247
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Figure 15. Relation between Hsize and Stotal
C. Determination of Estimation Parameters
We would like to determine the estimation parameters of
new user whose Hsize is hu. The size hu is obtained by (19).
If two of the three participants are A and B, each hand size
is hA and hB respectively (hA > hB), the parameters for hu
user are deﬁned as below.
Eupij
=
(hu − hB)EApij + (hA − hu)EBpij
hA − hB
(20)
Fupij
=
(hu − hB)FApij + (hA − hu)FBpij
hA − hB
(21)
Gupij
=
(hu − hB)GApij + (hA − hu)GBpij
hA − hB
(22)
Hupij
=
(hu − hB)HApij + (hA − hu)HBpij
hA − hB
(23)
Of course the parameters EApij to HApij and EBpij to HBpij
for hA and hB participants are calculated previously using
expensive data glove (parameters for another participant are
also calculated). Then numerical formula for estimation of
ﬁnger joint angles of the user is decided as follows.
θupij = EupijS3
i + FupijS2
i + GupijSi + Hupij
(24)
Also, we decide the µupn according to the following formula.
µupn = (hu − hB)µApn + (hA − hu)µBpn
hA − hB
(25)
where µupn, µApn, and µBpn represent vector of sensor sam-
ple values of user, A, and B. The µupn is used when using (4).
Using a weighted average of hand size, each parameter for
estimation can be determined.
D. Equivalency of Variance-Covariance Matrix
When using (4) to estimate hand motion, it is difﬁcult
to calculate the all parameters Σ−1
pn directly for each user.
So, we investigated equivalency of variance-covariance matrix
between different users by using Box’s M Test. First of all,
we got the sensor values when each participant performed
representative hand motions. Each representative hand motion
is performed n times. Table VII shows an example of the
sensor values at the time t in motion p. Next, the average of
TABLE VII. EXAMPLE OF SENSOR VALUE FOR MOTION p AT THE
TIME t
trial
Thumb
Index
Middle
Ring
Little
1
s11
s21
s31
s41
s51
2
s12
s22
s32
s42
s52
3
s13
s23
s33
s43
s53
...
...
...
...
...
...
n
s1n
s2n
s3n
s4n
s5n
the sensor values si for ﬁnger i is obtained by the following
formula.
si = 1
n
n
∑
j=1
sij
(26)
At this time, covariance of ﬁnger x and y, represented as
Vxy, is obtained by (27). And variance-covariance matrix V
is deﬁned as (28).
Vxy
=
1
n
n
∑
k=1
(sxk − sx)(syk − sy)
(27)
V
=


V11
V12
. . .
V15
V21
V22
. . .
V25
...
...
...
...
V51
V52
. . .
V55


(28)
Then we decide V ′ according to the following natural loga-
rithm (29).
V ′
=
ln |V AB|ν1+ν2
|V A|ν1|V B|ν2
(29)
ν1
=
nA − 1
(30)
ν2
=
nB − 1
(31)
where V A and V B represent mean variance-covariance matrix
of participant A and B. The number of times n of participant
A is different from B generally, each number is deﬁned as nA
and nB respectively. In this paper, each participant performs
hand motion 10 times. And V AB is the matrix obtained from
the following formula.
V AB = ν1V A + ν2V B
ν1 + ν2
(32)
Also, we decide k according to the following formula.
k
=
1 − ( 1
ν1
+ 1
ν2
−
1
ν1 + ν2
)·2q2 + 3q − 1
6(q + 1)
(33)
where q represents the number of explanatory variables. Now q
is number of Thumb, Index, Middle, Ring, and Little ﬁnger, 5.
Finally, we obtain test statistic χ2
0 from the following formula.
χ2
0 = kV ′
(34)
The χ2
0 follows chi-squared distribution. We describe the χ2
0
of each motion/user in Table VIII. We decide signiﬁcance
level α as 0.001. The χ2(α = 0.001) is 37.70. So, if the
χ2
0 is smaller than 37.70, there is not signiﬁcantly different in
variance-covariance matripants as Σpn. Finally we can obtain
the ﬁnger joint angles of new user.
248
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE VIII. χ2
0 OF EACH RESEARCH PARTICIPANT
participant 1, 2
participant 2, 3
pariticpant 1, 3
Standard
22.44
28.64
28.84
Lateral Contact
29.34
36.22
20.87
Tripod
31.60
35.51
29.89
Parallel Ext
33.69
35.54
32.41
TABLE IX. ESTIMATED Hsize OF PARTICIPANTS [cm]
True Hsize
Estimated Hsize
Error
Participant 4
17.6
18.0
0.4
Participant 5
19.1
19.9
0.8
VIII.
EXPERIMENT AND RESULT FOR HAND SHAPE
We had an experiment to conﬁrm the effectiveness of the
method for hand shape.
A. Experiment Environment
We used 5DT data glove with representative hand motion
set No.2 for two experiment systems, system A and system B.
And two research participants 4 and 5 performed several hand
motions.
1) System A: The system A estimates user’s ﬁnger joint angles
using same parameters for all users. These parameters were
obtained by the hand of participant 2 because his hand size is
mostly the same as average Japanese male’s one.
2) System B: The system B estimates user’s ﬁnger joint angles
using parameters obtained by the user’s own hand. User closes
hand ﬁrst, then each parameter is determined using Hsize.
B. Estimation Results
Table IX shows estimated Hsize of two participants. An
estimation error of participant 4 is 0.37, and the error of
participant 5 is 0.79. The method can estimate user’s Hsize
almost correctly. We conﬁrmed the effectiveness of the Hsize
estimation method described in Section VII-B.
Tables X to XIII show the average of the errors of the ﬁnger
joint angles between estimated ﬁnger joint angles and obtained
angles by CyberGlove. Besides, “Plastic bottle” represents the
hand motion as same as in Section VII-A.
These results indicate that the estimation accuracy of the
system B is better than the system A. We had Student’s t-
test to these averages of the errors of each ﬁnger joint angles.
Tables XIV and XV show the differences of the estimation
accuracy between the system A and B, and p-values obtained
from Student’s t-test. It is statistical signiﬁcance if the P(T≤t)
is smaller than 0.05. They show that there is statistical signif-
icance about the hand motion of which estimation accuracy
were improved. There is not statistical signiﬁcance about
Parallel Ext, but the estimation accuracy was not improved.
Totally the system B is better than the system A, it means
calibrated parameters for each user is effectiveness, and hand
size estimation is needed.
IX.
CONCLUSION
In this paper, we described a useful method using a low-
priced data-glove based on hand motion patterns. It estimates
all ﬁnger joint angles using the data glove that sensors cover
two joints of each ﬁnger, and also only the middle angle
TABLE X. JOINT ANGLE ERROR OF PARTICIPANT 4 IN SYSTEM A
[DEGREE]
Thumb
Index
Middle
Ring
Little
avg.
Standard
8.8
11.2
11.1
11.5
10.1
10.5
Lateral Contact
10.8
13.1
13.6
15.1
7.3
12.0
Tripod
18.6
10.6
13.5
11.5
17.5
14.4
Parallel Ext
10.4
13.4
8.8
9.0
10.3
10.4
Plastic bottle
15.2
10.7
15.3
20.2
9.9
14.3
TABLE XI. JOINT ANGLE ERROR OF PARTICIPANT 4 IN SYSTEM B
[DEGREE]
Thumb
Index
Middle
Ring
Little
avg.
Standard
6.5
6.3
3.6
15.0
18.4
10.0
Lateral Contact
12.0
11.7
11.4
11.4
8.7
11.0
Tripod
18.3
7.5
12.7
8.4
15.8
12.5
Parallel Ext
13.8
11.4
9.6
9.2
9.8
10.8
Plastic bottle
15.8
3.6
14.0
14.2
8.9
11.3
of each ﬁnger. A data glove is one of the major interfaces,
which are used in the ﬁeld of VR. It measures curvatures
of ﬁngers using bend sensor. However, in order to obtain
accurate hand motions, it is necessary to use an expensive
data glove, which has many sensors. On the other hand,
there is a low cost data glove, which measures an angle for
each ﬁnger through one sensor. It cannot get detailed data
directly. Our method estimates plausible user hand motion
patterns using each relation among angles of ﬁngers during
the operation of the low-cost glove ﬁrst. Then, it estimates
all ﬁnger joint angles by estimating the types of hand motion
patterns from the correlation between each ﬁnger angle in the
hand motion pattern. We assumed some representative hand
motion patterns, and considered that other hand motions could
be represented as synthetic motion of these. In the method
for the glove whose sensors cover only the middle angle, the
ratio of each representative motion pattern is calculated using
Moore-Penrose pseudo-inverse matrix, and all ﬁnger angles are
estimated using multiple regression analysis. The difference
between users’ hand shape was considered and conﬁrmed
using the glove whose sensors cover two joint angles. With
the low priced data-glove being useful, it is expected that
VR systems that target households will become more popular.
In the future, we should reconsider the representative hand
motion patterns because we removed Parallel Ext from our
TABLE XII. JOINT ANGLE ERROR OF PARTICIPANT 5 IN SYSTEM A
[DEGREE]
Thumb
Index
Middle
Ring
Little
avg.
Standard
12.3
12.8
12.7
14.2
13.8
13.2
Lateral Contact
11.5
21.5
16.4
21.2
19.2
18.0
Tripod
10.2
8.2
12.8
22.9
17.6
14.3
Parallel Ext
24.0
9.2
4.3
7.6
13.6
11.7
Plastic bottle
23.2
13.6
16.2
31.0
20.8
21.0
TABLE XIII. JOINT ANGLE ERROR OF PARTICIPANT 5 IN SYSTEM B
[DEGREE]
Thumb
Index
Middle
Ring
Little
avg.
Standard
13.1
8.5
6.9
8.8
12.5
10.0
Lateral Contact
9.3
14.1
13.1
11.4
16.8
12.9
Tripod
9.4
7.7
18.8
19.0
17.5
14.5
Parallel Ext
13.0
8.8
16.0
10.8
13.5
12.4
Plastic bottle
26.1
10.3
20.8
13.4
11.0
16.3
249
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE XIV. DIFFERENCE BETWEEN SYSTEM A AND B OF
PARTICIPANT 4
participant 4
difference
P (T ≤t)
Standard
-0.6
3.3 × 10−2
Lateral Contact
-1.0
2.7 × 10−2
Tripod
-1.8
3.9 × 10−2
Parallel Ext
+0.4
1.6 × 10−1
Plastic bottle
-3.0
6.7 × 10−3
average
-1.2
-
TABLE XV. DIFFERENCE BETWEEN SYSTEM A AND B OF
PARTICIPANT 5
participant 5
difference
P (T ≤t)
Standard
-3.2
3.4 × 10−2
Lateral Contact
-5.0
1.7 × 10−2
Tripod
+0.2
4.0 × 10−2
Parallel Ext
+0.7
8.1 × 10−1
Plastic bottle
-4.6
1.0 × 10−2
average
-2.4
-
ﬁrst research based on medical knowledge. We should also
expand the target hand motion patterns to various ones that
are not only grasping patterns.
ACKNOWLEDGMENT
The authors would like to thank our colleagues in our
laboratory for useful discussions.
REFERENCES
[1]
K. Funahashi, Y. Mori, H. Takahashi, and Y. Iwahori, “A Data Adjust-
ment Method of Low-priced Data-glove based on Hand Motion Pattern,”
in Proceedings of the PATTERNS 2017 (ComputationWorld), 2017, pp.
97–102.
[2]
P. Temoche, E. Ramirez, and O. Rodrigues., “A Low-cost Data Glove
for Virtual Reality,” in Proceedings of the XI International Congress of
Numerical Methods in Engineering and Applied Sciences (CIMENICS),
2012, pp. TCG31–36.
[3]
F. Camastra and D. Felice, “LVQ-based Hand Gesture Recognition using
a Data Glove,” in Proceedings of the Neural Nets and Surroundings Smart
Innovation, Systems and Technologies, vol. 19, 2013, pp. 159–168.
[4]
N. Tongrod, T. Kerdcharoen, N. Watthanawisuth, and A. Tuantranont,
“A Low-Cost Data-Glove for Human Computer Interaction Based on
Ink-Jet Printed Sensors and ZigBee Networks,” in Proceedings of the
International Symposium on Wearable Computers (ISWC), 2010, pp. 1–
2.
[5]
Y. Mori, K. Kawashima, Y. Yoshida, and K. Funahashi, “A Study for
Vision Based Data Glove with Back Image of Hand,” in Proceedings of
the ICAT2015, 2015, (USB Flash Drive, no page number).
[6]
H. Takahashi and K. Funahashi, “A Data Adjustment Method of Low-
priced Data-glove based on Representative Hand Motion Using Medical
Knowledge,” in Proceedings of the ICAT2013, 2013, (USB Flash Drive,
no page number).
[7]
G. Elkoura, “Handrix: Animating the Human hand,” in Proceedings of
the ACM SIGGRAPH/Eurographics Symposium on Computer Animation,
2003, pp. 110–119.
[8]
N. Kamakura, H. Matsuo, M. Ishii, and Y. Mitsuboshi, F. Miura, “Patterns
of Static Prehension in Normal Hands,” Am J Occup Ther 34, pp. 437–
445, 1980.
[9]
C. Capriani, “Objectives, criteria and methods for the design of the
SmartHand transradial prosthesis,” in Proceedings of the Robotica 2010,
vol. 28, 2010, pp. 919–927.
[10]
M. Kawauchi, “AIST Measurements Hand Data of Japanese,” in
https://www.dh.aist.go.jp/database/hand/ 2012. (in Japanese, last access:
5/Nov/2017)
250
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

