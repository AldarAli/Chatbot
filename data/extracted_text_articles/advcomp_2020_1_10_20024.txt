The F-Measure Paradox
Tim vor der Br¨uck
Fernfachhochschule Schweiz (FFHS)
Brig, Switzerland
Email: tim.vorderbrueck@ffhs.ch
Abstract—Paradoxes have raised a lot of interest in mathematics
and computer science. What fascinates people about them is
that such a paradox contains a self-contradictory statement
that dissents with usual believes and expectations. The range of
discovered paradoxes is long. One of the most famous is probably
the proposition of Russell that states that no set can exist that
contains all sets that do not contain itself as a subset. The paradox
arises in the proof, where it is shown that such a set must contain
itself if and only if it does not contain itself. In this paper, we
derive a paradox about the F-measure, one of the most important
metrics in machine learning. The contribution of this paper is two-
fold. On the one hand, we investigate typical properties of the F-
Measure, on the other hand, we show that they are contradictory
and therefore constitute a paradox, to several properties of the
harmonic mean, where the F-Measure is a special case of.
Keywords–F-Measure; paradox; precision; recall; NaN.
I.
INTRODUCTION
The word paradox originates from Greece and is composed
of the word para (beyond) and doxa (opinion). A paradox
contains a self-contradictory statement and dissents with
people’s believes and expectations [1]. It often does not have
a direct practical use case but it gives theoretical insights and
helps to understand certain problems better. Especially in the
area of mathematics, there is a large amount of identiﬁed
paradoxes. A quite well-known paradox is the proposition of
Russel.
Russel’s Paradox: This proposition [2] claims that no set can
exist that contains all sets that do not contain themselves
and nothing more. The proof is done by contradiction. Let
us assume such a set would exist. Then exactly one of the
following propositions must be true about this set:
•
This set contains itself. This is not possible since this
set only contains sets that do not contain themselves.
•
This set does not contain itself. Then per deﬁnition,
this set must contain itself, which is a contradiction.
Since both cases lead to a contradiction, such a set cannot exist.
Banach-Tarski
Paradox:
Another
well-known
paradox
from mathematics is the so-called Banach-Tarski-Paradox [3]
that claims that a sphere can be decomposed and put together
afterward in such a way that one has obtained two spheres
of the same volume as the original sphere. Thus, one of the
spheres was seemingly created out of nothing. This paradox
is based on the principle that some concepts of mathematics
cannot be transferred into reality.
Actual
value
Prediction outcome
p
n
total
p′
8
2
P′
n′
12
9978
N′
total
P
N
Figure 1. Example of confusion matrix for an imbalanced class distribution.
Stein’s
Paradox:
Normally,
the
expected
value
is
best
approximated by the average value, since the average value
is actually its best unbiased estimator. Stein’s paradox [4]
states that, if several expected values of the same type are
to be determined (like batting statistics for a collection of
baseball players), the isolated averages are no longer the best
choice. Instead, all the estimates should be determined jointly
by shifting the individual estimates in direction of the overall
cross-estimate average.
Accuracy Paradox: Related to Data Science is the so-called
accuracy paradox [5][6]. It states that when comparing two
classiﬁcation methods, the one with the lower accuracy can
have in fact higher predictory capability. This phenomenon
usually occurs in the case of highly imbalanced class
distributions. Consider for example a very infrequent event
like a rare disease that only shows up for around 0.1% of
the cases. Let us assume, we have a method that can detect
40% of the events correctly and its precision is 80%. So, its
confusion matrix could look like the one in Figure 1, where
the columns denote the predicted and the rows the actual
values. The obtained accuracy of this method would then
amount to 9986/10000=0.9986 while predicting always the
majority class (event not occurring), which has in fact no
predictive power, would achieve an accuracy of 0.999.
In this paper, we will ﬁrst derive several general statements
about the harmonic mean of two variables. Afterward, we will
proof that these statements are indeed incorrect for the F1-
score, which is a special case of the harmonic mean, in partic-
1
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-812-9
ADVCOMP 2020 : The Fourteenth International Conference on Advanced Engineering Computing and Applications in Sciences

ular, it is the harmonic mean of precision and recall. Finally,
we will analyze the reasons for this paradox and investigate
the consequences for mathematical proofs in general.
The remainder of this paper is organized as follows. In
Section II, we deﬁne the general harmonic mean and show
several of its principal properties. Section III gives an overview
of the F1-score, which is the harmonic mean of precision
and recall. In the next section (Section IV), the paradox of
the F1-score is described. The ﬁndings and the cause of this
paradox are discussed in the next Section V. Finally, the paper
concludes with Section VI, which summarizes the obtained
results.
II.
HARMONIC MEAN
The harmonic mean H(a, b) of two values a and b is the
Hoelder-mean with coefﬁcient -1 and is formally given by [7]:
H(a, b) =
a−1 + b−1
2
−1
=
2
1
a + 1
b
(1)
with a, b ∈ R (or C). An alternative and simpler formulation
is:
H(a, b) = 2ab
a + b
(2)
In contrast to the arithmetic mean, the harmonic mean is
a rather pessimistic mean that is drawn in direction to the
minimum of both arguments. Note that it can only be applied
to argument values of identical signs [7]. To see this, consider
the following example:
H(−2, 4) = 2(−2) · 4
−2 + 4 = −16
2
= −8 ̸∈ [−2, 4]
Since -8 is not located between -2 and 4, it cannot possibly
constitute any mean of those values.
Consider now the following two propositions (1+2):
1. a = 0 ⇒ H(a, b) = 0
2. H(a, b) = 0 ⇒ a = 0
(3)
Note that without limitation of generality a=0 can be replaced
by b=0 due to the symmetry of the harmonic mean.
It is not difﬁcult to show that the ﬁrst statement is true and
the second false.
Proof: Let us ﬁrst have a look at proposition 1. The
following two cases can be discerned: b ̸= 0 and b = 0. First,
we consider the case b ̸= 0. Plugging in a = 0 in formula 2
results in:
H(a, b) = 2 · 0 · b
0 + b = 0
b = 0
(4)
Now consider a = 0, b = 0. Plugging both values into
H(a, b) results in an expression 0
0, which is not deﬁned. Let
us, however, look at the behavior of H(a, b) for a and b
approaching zero using formula 1. Since the sign of a and
b must coincide, we get:
lim
a,b→0 H(a, b) =
2
1
a + 1
b
= 2
∞ = 0
(5)
Therefore, it is a reasonable approach to deﬁne H(0, 0) as 0,
to which we henceforth abide.
Proposition 2 is straight-forward to show by the following
counterexample: H(2, 0) = 0 but 2 ̸= 0.
One can also draw some conclusions, under which con-
ditions the harmonic mean H and one of its input arguments
have to coincide. In particular, assuming a is not diminishing,
then from the fact that a and H coincide one can infer that b
must also assume their common value. The opposite, however,
is false.
Formally, the ﬁrst proposition (proposition 3) is true and
the second (proposition 4) is false:
3. a ̸= 0 ∧ a = H(a, b) ⇒ b = H(a, b)
4. a ̸= 0 ∧ b = H(a, b) ⇒ a = H(a, b)
(6)
Proof of Proposition 3:
From the deﬁnition of the
harmonic mean, it follows that: H(a, b) = 2ab
a+b
Since H(a, b) equals a, we can plugin a on the left-hand side:
a = 2ab
a+b
Since a ̸= 0, both sides can be divided by a
1 =
2b
a+b
Afterward, we multiply both sides by a + b:
a + b = 2b
By subtracting b from both sides one ﬁnally obtains:
a = b
The opposite direction (proposition 4) can be shown by
contraction, let a=1,b=0=H(a,b), then herewith it follows that
a ̸= H(a, b).
III.
F1-SCORE
The F1-Score is the harmonic mean of precision and recall,
where precision is the percentage of predicted positive events
that are indeed positive, while recall is the percentage of
positive events that are actually correctly detected by the
algorithm [8]. All three measures originated from the area of
information retrieval but quickly spread into other areas of
machine learning too. Let TP be the true positives, i.e., the
number of positive events that were correctly classiﬁed by the
algorithm, FP the number of negative events that were actually
classiﬁed as positive, and FN the number of positive events that
were misclassiﬁed as negative. Then precision (prec), recall
(rec), and F-measure are formally deﬁned as follows:
prec =
TP
TP + FP
rec =
TP
TP + FN
F1(prec, rec) =H(prec, rec)
=2prec · rec
prec + rec
(7)
Note that recall or precision can potentially be undeﬁned.
Consider, for example, that the positive class never shows up in
the evaluation data. In this case, TP and FN assume both zero,
which results in an undeﬁned recall value. Similarly, if the
positive class is never predicted, the precision is left undeﬁned.
Analogously to the deﬁnition of ﬂoating point numbers, we
use the expression NaN to denote an undeﬁned value, which
2
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-812-9
ADVCOMP 2020 : The Fourteenth International Conference on Advanced Engineering Computing and Applications in Sciences

stands for Not a Number. We also deﬁne arithmetic on NaN
in the following way by following the Bochvar extension [9].
Let a ∈ R ∪ {NaN } be arbitrarily chosen, then:
a · NaN =NaN
a + NaN =NaN
a − NaN =NaN
a
NaN =NaN
(8)
As one can easily perceive, if at least one of the oper-
ator arguments assumes NaN, then also the result is NaN.
Therefore, NaN is also called an absorbing element. Regarding
the algebraic structure, R ∪ {NaN } is a semi-group for both
summation and multiplication with 0 (1 respectively) as its
neutral element. (R∪{NaN }, +) and (R∪{NaN }\{0}, ·) are
no groups, since there is no inverse element of NaN .
Consider the example precision=NaN, and recall=0, then
the F1-score becomes
F1(NaN , 0)
=2NaN · 0
NaN + 0 = NaN
NaN = NaN
(9)
Note that sometimes, the F1-score is also deﬁned directly
based on TP, FP, and FN as follows [10]:
F1(TP, FP, FN ) =
2TP
2TP + FP + FN
(10)
which leads to other behaviors regarding deﬁnedness. How-
ever, in this paper, we stick to the usual deﬁnition based on
precision and recall.
IV.
THE F-MEASURE PARADOX
Recall the four propositions from Section II.
1. a = 0 ⇒ H(a, b) = 0
2. H(a, b) = 0 ⇒ a = 0
3. a ̸= 0, a = H(a, b) ⇒ b = H(a, b)
4. a ̸= 0, b = H(a, b) ⇒ a = H(a, b)
(11)
If we set a=prec(ision) and b=rec(all), those four propositions
become:
1. prec = 0 ⇒ F1(prec, rec) = 0
2. F1(prec, rec) = 0 ⇒ prec = 0
3. prec ̸= 0, prec = F1(prec, rec) ⇒ rec = F1(prec, rec)
4. prec ̸= 0, rec = F1(prec, rec) ⇒ prec = F1(prec, rec)
(12)
From Section II, one would expect that Proposition 1 and
3 are true and Proposition 2 and 4 are false. But, surprisingly,
it is just the opposite. In fact, propositions 1 and 3 are false
and propositions 2 and 4 are true.
Proof: For proposition 1, we give a counterexample.
Consider the confusion matrix in Figure 2. For this matrix,
the precision assumes 0 and the recall NaN. Therefore, the
F1-Score is given as 2·0·NaN
2+NaN = NaN ̸= 0, which concludes
the proof by counterexample.
Actual
value
Prediction outcome
p
n
total
p′
0
8
P′
n′
0
10000
N′
total
P
N
Figure 2. Example confusion matrix as counterexample for proposition 1
Proposition 2: Consider the second proposition and let us
assume that the F1-Score is zero. Hence, either precision or
recall is zero. In case, the precision is zero, our proof is
ﬁnished. So let us, therefore, assume instead that the recall
is zero. Since the F1-score is deﬁned (not NaN), both recall
and precision must be deﬁned too. Furthermore, we have:
0 = rec =
TP
TP + FN
⇒ TP = 0
⇒
TP
TP + FP = 0
(Precision is not NaN , therefore TP + FP ̸= 0)
⇒ prec = 0
(13)
Proposition 3: Again, we give a counterexample, we can
use the same confusion matrix as for proposition 1. With this
we get prec = NaN = F1(prec, rec) and rec = 0.
Proposition 4:
Proof: We discern the following three cases:
Case 1: rec = F1(prec, rec) = NaN
rec =F1(prec, rec) = NaN
⇒ TP =0
⇒ FP + TP =0
(since prec ̸= 0)
⇒ prec =NaN = rec = F1(prec, rec)
(14)
Case 2: rec = F1(prec, rec) = 0
Due to proposition 2, it follows that prec = 0 = F1(prec, rec).
Since the precision cannot diminish, this case actually turns out
to be impossible.
Case 3: rec = F1(prec, rec) ̸= 0 and rec = F1(prec, rec) ̸=
NaN .
The precision cannot assume NaN , since otherwise the F1
score would be NaN , too.
Furthermore, from the deﬁnition of the harmonic mean, it
is known that:
3
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-812-9
ADVCOMP 2020 : The Fourteenth International Conference on Advanced Engineering Computing and Applications in Sciences

actual
value
Prediction outcome
p
n
total
p′
0
0
P′
n′
8
10000
N′
total
P
N
Figure 3. Example confusion matrix as an counterexample for proposition 1
(case recall)
rec = F1(prec, rec) = 2prec·rec
prec+rec
Since rec ̸= 0, we can divide both sides of the equation
by the recall and obtain:
1 =
2prec
prec+rec
We then multiply both sides of the equation by prec + rec:
prec + rec = 2prec
Finally, we subtract from both sides of the equation the
precision and get:
rec = prec which constitutes just the conducted claim.
Finally, let us investigate if the same paradox holds also
for F-measure and recall instead of precision. Analogously
to the precision case, we ﬁrst present a counterexample (see
Figure 3).
This time, the recall is 0 and the precision NaN, which
leads to a NaN F1-Score.
Proof: Let us now prove the second proposition for F-
measure and recall. Again, since the F1-score is zero, neither
of precision and recall can be NaN. If the recall is zero, our
proof is ﬁnished. Therefore, let us instead assume the precision
is zero.
0 = Precision =
TP
TP + FP
⇒ TP = 0
⇒
TP
TP + FN = 0
⇒ Recall = 0
(15)
The two remaining properties 3 and 4 can be proven
analogously.
V.
DISCUSSION
Purely formally seen, the computation rules for NaN values
are mathematically consistent and correct and also reﬂect the
standard procedure for computer-based F1-Score implementa-
tions if they make use of ordinary ﬂoating-point computation
logic. It remains, however, to investigate, if these rules are also
reasonable in the given context. The answer is partly yes and
partly no. Consider ﬁrst the case the recall is undeﬁned (NaN),
which means that the positive class never shows up in the
evaluation data set. If the algorithm predicts only for a single
data item the positive class, then the precision immediately
turns to zero. In this case, an NaN F1-Score seems to be the
best choice.
However, if the precision is undeﬁned (NaN), the matters
look a bit different. In this case, the tested machine learning
method would never predict the positive class. If the positive
class shows up quite a few times in the evaluation data, such
a method would clearly perform very poorly and an F1-score
of zero would seem adequate.
So far, we investigated only the F1-score, although in the
title we mentioned the F-measure in general. This generalized
F-measure is given by:
Fβ = (1 + β2) ·
precision · recall
(β2 · precision) + recall
(16)
For β = 2 we get for instance the F2-score, which is deﬁned
as:
F2 = 5 ·
precision · recall
(4 · precision) + recall
(17)
The F-measure for β ≥ 2 penalizes a poor recall stronger
than a bad precision, since in some situations, like cancer
detection, missing any items of the positive class can be fatal in
practice. However, the use of different weighting factors does
not inﬂuence in any way the properties derived here. Thus, our
ﬁndings also hold for the F-measure in general.
Finally, this paradox also reveals a shortcoming of most
mathematical proofs. Undeﬁned values are not rare in practice.
They can be caused by missing values or incomputability as
investigated here. Albeit, in proofs, they are usually completely
ignored. The paradox investigated here shows that such unde-
ﬁned values can easily ﬂip statements completely around.
While the ﬁndings as stated here are mainly theoretical,
they can have some practical implications as well. If the dif-
ferent behaviors of harmonic mean and F-measure as described
here were ignored, then in certain anomalous situations, incor-
rect conclusions might be drawn from the data.
VI.
CONCLUSION
We presented two basic statements about the harmonic
mean, where the ﬁrst is true and the second false. However, for
the F1-score as the harmonic mean of precision and recall, the
truth value of both statements is completely turned around.
This paradox is caused by the fact that the possibility that
input values can be undeﬁned is not taken into account in
the original propositions for the harmonic mean. Hence, with
this paradox, we also revealed an important shortcoming of
mathematical proofs in general.
REFERENCES
[1]
S. J. Farlow, Paradoxes in Mathematics.
Chicago, Illinois: Dover
Books, 2014.
[2]
A. Whitehead and B. Russell, The Principles of Mathematics, 2nd ed.
New York, New York: W. W. Norton & Company, 1996.
4
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-812-9
ADVCOMP 2020 : The Fourteenth International Conference on Advanced Engineering Computing and Applications in Sciences

[3]
S. Banach and A. Tarski, “Sur la d´ecomposition des ensembles de points
en parties respectivement congruentes (on the decomposition of sets of
points into respectively congruent parts),” Fundamenta Mathematicae,
vol. 6, 1924, pp. 244–277.
[4]
B. Efron and C. Morris, “Stein’s paradox in statistics,” Scientiﬁc
American, vol. 236, no. 5, 1977, pp. 119–127.
[5]
B. Abma, “Evaluation of requirements management tools with support
for traceability-based change impact analysis,” Master’s Thesis, Univer-
sity of Twente, 2009.
[6]
J. S. Akosa, “Predictive accuracy : A misleading performance measure
for highly imbalanced data,” in Proceedings of the SAS Global Forum,
2017, p. 2–5.
[7]
D. B. MacNeil, Fundamentals of Modern Mathematics: A Practical
Review.
Dover Publications, 2013.
[8]
C. D. Manning and H. Sch¨utze, An Introduction to Information Re-
trieval.
Cambridge University Press, 2009.
[9]
L. B˘ehounek and M. Da˘nkov´a, “Extending aggregation functions for
undeﬁned inputs,” in Proceedings of the International Symposium on
Aggregation and Structures, 2018, pp. 15–16.
[10]
D. Chicco and G. Jurman, “The advantages of the matthews correlation
coefﬁcient (MCC) over F1 score and accuracy in binary classiﬁcation
evaluation,” BMC Genomics, vol. 21, no. 6, 2020, pp. 1–13.
5
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-812-9
ADVCOMP 2020 : The Fourteenth International Conference on Advanced Engineering Computing and Applications in Sciences

