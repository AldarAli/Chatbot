Predicting Software Quality from Development
and Release Factors
Rishita Mullapudi
Computer and Information Science
Gannon University
Erie, PA
Email: mullapud002@gannon.edu
Tajmilur Rahman
Computer and Information Science
Gannon University
Erie, PA
Email: rahman007@gannon.edu
Joshua Nwokeji
Computer and Information Science
Gannon University
Erie, PA
Email: nwokeji001@gannon.edu
Abstract—Long lasting sustainable systems require quality
software releases. If a new version of the software encounters rel-
atively fewer post-release defects, i.e., bugs, then we can consider
that version as a better quality release. In the competitive world
of faster release and shorter release cycle based development, it
is challenging to deliver a quality release of a software product.
Predicting the release quality certainly helps developers to take
precautions and measures to prevent post-release bugs. Although
many researchers studied software quality prediction, a lack of
robust empirical study on software development historical data
to predict their impact on software release quality has been
observed. In this study, we predict the release quality of Eclipse
Equinox project by constructing a decision tree model from six
factors, such as code changes (churns), commits, churns in test-
ﬁles, churns in conﬁg-ﬁles, last-minute-change, etc., observed
from the historical data extracted from the version control
system. Such development and release factors will give us a
better understanding on how the developers’ activities affect the
quality of a software release. Five quality levels, i.e., classes
are used in our classiﬁcation model from the Eclipse bugs
depending on the presence of different levels of severity of bugs.
Furthermore, we will construct three more models, Na¨ıve Bayes,
K-means Clustering, and Linear Regression, and will compare
the accuracy of prediction. The outcome of this study will be a
set of classiﬁcation models built on the six development factors
and an insightful comparison among them.
Keywords—Software Quality, Release Quality, Software Qual-
ity Model, Open Source Software, Decision Trees
I. INTRODUCTION
One of the objectives of software development is to achieve
a high level of customer satisfaction [20]. In general, quality
is deﬁned as the ability of a product to satisfy the needs and
expectations of customers. Software quality focuses on making
the customer happy by providing a satisfactory outcome of
the software application with an uninterrupted user-experience.
Moreover, explicit attention to the quality factors may save the
software life-cycle cost signiﬁcantly [6]. Various approaches
and frameworks [21] [22] [23] for measuring software quality
have been proposed in literature. However, in this paper, we
use post-release bugs to measure software quality.
Software quality has been measured in various techniques.
Wehaibi et. al. examined the impact of self-admitted technical
debt as a measure of software quality [10]. On the other
hand, Araujo et. al. [1] used code-quality as a measure of
software quality.. However, the majority of the studies have
emphasized on predicting software quality issues to improve
software quality [9].
The increasing popularity of rapid releases bringing the
software products and new features into the market more
frequently than before [3]. Maintaining the quality of the
software product can be challenging in such a limited time-
frame of release-cycles since testing in rapid release becomes
challenging while manual system-integration test needs ef-
fective and efﬁcient prioritization [11]. The effect of rapid
releases on software quality also has been studied by Khomh
et al. [24] for Mozilla Firefox. Since Eclipse is following a
rapid-release model for their development, this increases our
interest to choose the Eclipse Equinox project for this study.
A large amount of effort is involved in stabilization activities
such as correcting coding standards, ﬁxing bugs, adjusting
conﬁgurations, twiking test ﬁles etc., during the testing or
Quality Assurance (QA) period in a release cycle [7]. Al-
though, in a rapid release, the effort during stabilization is
not as large as the development effort, developers tend to rush
towards the end of the development period right before the
releasing phase starts [7]. Therefore, we are more interested
to see whether the last minute changes have any impact on
the post-release bugs, i.e., the overall software release quality.
To drive this research, we are interested in ﬁnding answers
to the following research questions:
RQ1 How much code-change efforts are involved for a new
release version?
Here, we quantify the number of commits and churns (code-
changes) as a measure of effort to release a new version. We
use a number of commits and churns as release factors to
construct our prediction models.
RQ2 Do we see more post-release bugs when a new release
version involves more test-ﬁles or conﬁguration related ﬁles?
We quantify the code-changes in test ﬁles and conﬁguration
ﬁles in the commits to a release version. We use them as
release factors to construct our prediction models.
RQ3 How signiﬁcant are the last-minute changes to produce
post-release bugs?
We consider the last one month window as the last-minute
changes before publishing a release version. We want to see
if we see more bugs where developers were more in a rush
during the last one month of development.
1
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-864-8
IMMM 2021 : The Eleventh International Conference on Advances in Information Mining and Management

TABLE I. COMMITS LABELED WITH BUILD AND RELEASE VERSION.
Commit
Build Version
Author Date
Release Version
bdfb311c27b7af506d9df031c0fa86c01bd2d88f
v200712031723
2007-11-29 14:21:48-05
3.1.2
850f068ac1f4264641adacc707c87f0f07a721de
v20090127-1212
2009-01-27 11:15:27-05
3.5.0
7c88166c83944184600862ecbe77935cbb4360ef
v200712031723
2007-11-29 14:55:38-05
3.1.2
2a34a6d8f11644b9ee80ac0cedbda0364f3f4116
v200712031723
2007-11-29 15:00:49-05
3.1.2
e539b0d4eeeb2686743eed41d2260a4bf6d92ef3
v200712031723
2007-11-29 14:27:35-05
3.1.2
adec7392cfe5ee6292a28d7520b567e897c8975b
v20071015
2007-10-15 18:20:27-04
3.4.0
86ad8d63826d6186aa433020f7d4b06330feec04
v20071015
2007-10-15 16:33:20-04
3.4.0
TABLE II. COMMITS DETAILS.
Commit
Churns
Old File
New File
bdfb3...2d88f
3
bundles/.../.../EclipseGeneratorApplication.java
bundles/.../.../EclipseGeneratorApplication.java
850f0...721de
2
bundles.../.../BuildPublisherAntTasks.launch
bundles.../.../PublisherAntTasks.launch
7c881...360ef
8
bundles/.../.../EclipseInstallGeneratorInfoProvider.java
bundles/.../.../EclipseInstallGeneratorInfoProvider.java5
2a34a...f4116
56
bundles/.../.../generator/Generator.java
bundles/.../.../generator/Generator.java
In this study, while ﬁnding answers to the RQ1 we obtain
the numbers for various release factors from our data that will
help us build our prediction models. Based on the results from
our prediction models, we will be able answer RQ2 and RQ3.
We deﬁne the quality levels of release versions depending
on the presence of different severity levels of bugs. We use
these quality levels as the classes of our prediction model for
training and testing. Our main focus is to understand if there
is any strong relationship between the quality levels and one
or more of the release factors.
The following sections are organized as: Section II talks
about the related studies in the literature and compares with
our contribution in this paper. Section III explains the data
source, data collection, and data pre-processing. Section IV
explains our methodology, Section V explains our preliminary
results obtained, statistics on the development/release factors
that build our prediction models. Finally, in Section VI we
summarize our research so far, explain how much progress
we have made, and how much work still remaining.
II. LITERATURE REVIEW
Many researchers have predicted software quality using
various prediction models. We are performing the study on
Eclipse post-release bugs and six development and release
factors. Similar to this, a study has been conducted by Misirli
et. al. where they performed an explanatory analysis on eclipse
beta-release bugs [18]. They considered six development re-
lated in-process metrics that have explanatory impact on beta-
release bugs. The factors that they used are, age, number
of edits, number of committers, average changed lines of
code, last edit date and average time between edits. In our
understanding a different set of factors may have a larger
impact on the post-release bugs. Compared to their approach,
we are considering a different set of development related
factors (metrics) in each release version such as, number of
commits, bug-ﬁx commits, churn per ﬁle, churn per test-ﬁle,
churn per conﬁg-ﬁle, and last-minute churns. Furthermore, we
will investe each release version and predict the post-release
bugs using other prediction models.
Zimmerman et. al. [9] predicted software quality from the
historical data. Unlikely our approach, they considered bug-
ﬁx changes as a measure of software quality. They focused on
analyzing the testing process to assess the impact on software
quality in a rapid release model.
Seliya et. al. [17] used classiﬁcation algorithms to predict
software quality. They used C4.5 [17] and Random Forest
decision-tree to build defect predictors. However, their focus
was to investigate the cost-sensitivity of the learning mecha-
nism on multiple data-sets collected from different software
projects.
Ara´ujo et. al. used four code quality features related to
poor programming practice and evaluated the effectiveness of
these features on post-release bugs in the procedural software
applications [1].
Wehaibi et. al. [10] considered self-admitted technical debts
as a measure of software quality. Phadke et. al. [5] considered
fault-prone modules to measure and predict software quality.
However, none of them used any classiﬁcation tree to construct
a prediction model.
Other prediction models have also been applied to predict
software quality, such as the Bayesian network. A Bayesian
network based approach has been taken to assess and predict
software quality by Wagner et. al. [19]. They introduce the
use of general quality models and show how the modelling of
activities and facts in an organization helps deﬁne quality more
precisely. They used the Bayesian network since it shows bet-
ter performance for assessment and prediction incorporating
variables with uncertainty.
We have not found any study which has followed the exact
similar approach that we are following. Our approach is to
construct a classiﬁcation based prediction model based on the
factors related to development and release from the historical
development repository data. We would like to ﬁnd if there
is any strong relationship between post-release bugs and one
or more of these factors. Furthermore, we will construct three
more prediction models to ﬁnd the best result and explain
why such factors are signiﬁcant to pay attention during the
development activities in a release.
III. DATA
We used Eclipse Equinox [13] development historical data
that we collected from their public Github repository which is
2
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-864-8
IMMM 2021 : The Eleventh International Conference on Advances in Information Mining and Management

a mirror of the ofﬁcial Eclipse repository [14]. We collected
the the post-release bug reports from their Bugzilla portal [12].
A. Repository Data
First, we cloned the Eclipse-Equinox repository, which
contains commits earliest from 2006 with a total of more
than 6K commits. More than 63 developers contributed to this
repository as of today. We then run a python script to extract
the commit history and store them into a postgres database.
The Python script extracts the commit data into 5 different
tables, where all the commits are stored in the main commit
table with author date (the date-time when the commit was
made), and the commit message. Another table related to this
table contains all the details about each commit such as lines
modiﬁed, ﬁles modiﬁed/renamed, etc.
Another useful data we collect from the repository is the
release-tags. Each time a build is created Github creates a tag
with that build-commit and by extracting those tags from the
repository we can track the release commits. Once we track
the release commits, we then apply another python script to
extract the git Directed Acyclic Graph (DAG) [15]. This script
walks backward through the DAG traversing each and every
commit all the way to the ﬁrst one starting from a release
(build) commit and labeling the commits on it’s way with that
release tag. This is how we know which commit belongs to
which build version.
Once we have all the commits labeled with the build
versions, we then look for what release versions the builds
belong to. For this, we needed to put some manual effort
to search for the build archives for various Eclipse-Equinox
documentations [16]. We found lists of builds associated with
release versions from various Eclipse documentations and
online resources. Table I shows a segment of our commit data
labeled with build versions and the release versions. We get
the number of commits made to a release version from this
table. Number of churns, test-ﬁles, conﬁg-ﬁles, churns in test
ﬁles, churns in conﬁg ﬁles, these release factors we get from
commit details. Table II shows a portion of our commit details
data for the ﬁrst four commits in table I. In this table, column
”Churn” contains the total number of lines of code changed
(addition + remove), ”File” and ”New File” columns indicate
if there is any ﬁle renamed, deleted, or added in that commit.
Table II shows a portion of our commit details data for the
ﬁrst four commits in table I. In this table, column ”Churn”
contains the total number of lines of code changed (addition
+ remove). The “Old File” and “New File” columns indicate if
there is any ﬁle rename, or deletion or addition in that commit.
We get the release factors “total Churns”, “total ﬁles”, “churns
in test ﬁle”, “churns in conﬁg ﬁles” from the table II.
We get the release factors “total Churns”, “total ﬁles”,
“churns in test ﬁle”, “churns in conﬁg ﬁles” from the table
II. To calculate the churn data we sum up the addition and
deletion of lines of code in that commit. To calculate total
churns in test ﬁles and conﬁg ﬁles, we ﬁrst identify the test
ﬁles and by search for the existence of the words “test” or
“Test” in the ﬁle path. To identify the conﬁguration ﬁles, we
TABLE III. ECLIPSE EQUINOX BUG DATA.
Bug ID
Release Version
Bug Severity
564065
4.17.0
Critical
566014
4.17.0
Normal
61632
3.0.0
Blocker
191487
3.0.0
Critical
67588
3.0.0
Major
285341
3.5.0
Normal
search for the existence of the words “conf” or “setting” but
no “test” or “Test”. This is because there are test ﬁles to test
conﬁguration settings too and we want to consider those ﬁles
as test ﬁles not conﬁguration ﬁles.
Last Minute Changes: Finally, another development factor
we would like to investigate is how much changes developers
are doing during the last one month of the release time-line.
We are calling the last one month of changes as the “Last
Minute Churn”.
B. Bug data
Eclipse-Equinox bug reports are stored in their Bugzilla
portal [12]. We downloaded all the bug ‘id’s as a “csv” ﬁle
from that website. We wrote another python script to fetch
the details of each of the bugs using the bug id. This python
script pulls out the detailed information about each bug that
contains date, bug-status, bug description, release version the
bug was created, release version the bug was ﬁxed, severity
of the bug, and many other useful information. We stored this
information into the same Postgres database. Table III shows
a segment of the bug data we collected from Eclipse Bugzilla
archive.
Eclipse Equinox uses bugzilla to store their bugs. Bugzilla
allows us to categorize bugs in different types: “Enhance-
ment”, “Trivial”, “Minor”, “Normal”, “Major”, “Critical”,
“Blocker” etc., depending on the severity of the bugs.
“Enhancement” types of bugs are not actually defects, they
are the limitations of a feature which probably because of
missing that part during the initial planning for the feature, or
during the development. “Trivial” bugs are the ones that do
not have much impact on the performance or user experience.
“Minor” bugs are the ones that have some impact but we
can live with it for some time. No one will complain, or no
signiﬁcant performance issues at this point. However, it is a
defect and we need to address this. “Normal” bugs are the ones
that we need to address and schedule an appropriate scope
of ﬁx. Users have complaints but they can at least manage
their work. “Major” bugs are the high-priority defects that are
causing problems to the users and we need to ﬁx this as soon as
possible. “Critical” bugs are the bugs that are causing serious
problems to the system. System is mal-functioning, users are
having bad experience and having difﬁculties to do their work
using the relevant feature. “Blocker” has the highest degree
of impact which is blocking the affected feature, users are
completely unable to use the feature.
Post-release bugs are an obvious fenomena in a software
life-cycle. However, the presence of different types of severity
bugs in a release indicates the level of quality of the release
3
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-864-8
IMMM 2021 : The Eleventh International Conference on Advances in Information Mining and Management

Git 
Repo
Development and 
Release Commit Data
Eclipse.Bugzilla
# Commits
# Ch/F
# Ch/TF
# Ch/CF
# Bugfix 
Commits
# LM Ch
Development and 
Release Factors
Decision 
Tree
Compare
Outperforming Model & Discuss 
Significance of Factors
Bayesian 
Network
K-Means 
Cluster
Linear 
Regression
Post-release Bugs 
Archive
QL1
QL2
QL3
QL4
QL5
FIG. 1. THE METHOD OF OUR STUDY AT A GLANCE.
version. Our study focuses on predicting not just the number
of post-release bugs, rather understanding the level of quality
of the release based on the presence of different severity-levels
of bugs.
IV. METHODOLOGY
Our primary prediction model is the decision tree using the
six factors: “Churn per File” (Ch/F), “Total Number of Com-
mits” (# C), “Bugﬁx Commits” (# BfC), “Churn per Test File”
(# Ch/TF), “Churn per Conﬁg File’ (# Ch/CF)’, and “Last-
minute Churns” (#LCh). To obtain the best performing model
we will construct three more prediction models (Na¨ıve Bayes,
K-means Clustering, and Linear Regression) and compare the
performance. Finally, we will discuss the impact of each of the
factors on the results. Figure 1 presents our research-method
at a glance.
We deﬁne the quality levels based on the following formula
that considers high-impact bugs (hb), minor bugs (mb), major
bugs (Mb), and total bugs (Tb). According to this formula, the
magnitude (M) of a release is the product of the high-impact
bugs (hb) and the ratio of minor and major bugs associated
with a release version.
magnitudeM = hbϵhb > 0 : hb ∗ (mb + Mb) ∗ 100/Tb (1)
Magnitude of a release indicates how large is the impact
of the bugs in that release. To measure that, we consider
the percentage of major and minor bugs. The magnitude of
the release is dominated by the presence of the high-impact
TABLE IV. QUALITY LEVELS (CLASSIFICATIONS).
Class
Quality Magnitude
QL1
0 - 50
QL2
51 - 100
QL3
101 - 150
QL4
151 - 200
QL5
201+
bugs. Here, the number of high-impact bugs is the summation
of critical and blocking bugs. We multiply the percentage of
major and minor bugs by high-impact bugs to calculate the
magnitude of a release. For example, the quality magnitude of
release version 3.4.0 has been calculated like below:
m3.4 = 19 ∗ (26 ∗ 100/644) = 76.7
(2)
Here, the number of high-impact bugs is 19, and we multiply
by this only when this is > 0. Table VI shows the magnitudes
of release version 3.4.0, 3.5.0, and 3.6.0.
Release Magnitude: We deﬁne a threshold for the ﬁve
quality levels based on the severity of bugs. The Severity levels
include minor, major, critical, blocker. The ﬁve quality levels
are represented as “QL1”, “QL2”, “QL3”, “QL4” and “QL5”.
The thresholds for the quality levels are presented in Table IV.
If a release magnitude falls within this range, we will label
that release with the corresponding quality level. For example,
the magnitude of release 3.4 is 76.7. Therefore, we can label
this release with “QL2”.
Decision Tree: Our primary classiﬁcation model is the
supervised learning technique “Decision Tree (DT)”. Decision
trees are easier to understand and categorize samples, and
4
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-864-8
IMMM 2021 : The Eleventh International Conference on Advances in Information Mining and Management

TABLE V. PRELIMINARY STATS ON DEVELOPMENT/RELEASE FACTORS.
Version #
Ch/F
# C
# Ch/TF
# Ch/CF
# LCh
# BfC
3.4.0
33.54
433
109.16
10.90
40.02
-
4.2.0
14.53
59
13.38
12.25
22.66
-
4.5.0
10.67
72
10.04
3.40
42.5
-
Sept 2008
Oct 2009
jan 2010
March 2010
May 2010
Commits
Churns
Nov 2012
Nov 2013
Nov 2017
FIG. 2. LAST-MINUTE CHANGES IN VERSION 3.6.0.
interpret the results. First, we prepare our model-data table
with labeled data labeled using the quality levels. Decision
trees do classiﬁcation based on conditions at each level. In our
case that condition will check the quantity of the six factors
for each release, and calculate the magnitude using equation
(1) to classify it to a quality level. We will train our DT with
70% of our labeled data and will test with 30% of the data.
V. PRELIMINARY RESULTS
Our data has been pre-processed and by this time we have
obtained some preliminary statistics of the ﬁve of our six
factors. Table V shows the preliminary counts for the ﬁve
factors.
The column “# LCh” indicates the last minute changes in a
release version which we collect from the last one month of
churns in the release time-line as shown in ﬁgure 2. Eclipse
Equinox stops making any commits in the repository three
weeks before they announce their release version.
Figure 2 shows that as the developers in Eclipse approach
towards the release, the ratio of churns per commit keeps
increasing which has also been observed by Rahman et. al [7]
in Linux and Google Chrome. This indicates that similar to
Google Chrome and Linux, there is a little rush towards the
release period observed in Eclipse Equinox as well.
We have deﬁned the quality levels based on the release
magnitudes from the bug data in Table IV. This magnitude
will be used to determine different quality levels based on
thresholds as explained in the methodology section.
TABLE VI. QUALITY MAGNITUDES OF RELEASES.
Version #
Min
Maj
Crit
Block
Total Bugs
M
3.4.0
6
20
12
7
644
76.7
3.5.0
14
17
5
8
349
115.0
3.6.0
3
11
5
1
180
47.0
VI. CONCLUSION AND FUTURE WORK
Software quality assurance is an important aspect in the soft-
ware development lifecycle. It helps the software developers to
measure the extents to which the product/software meets user’s
needs. The prediction of software quality has been studied in
literature and various models have been proposed. However,
achieving software quality still remains a major challenge to
the software developers especially when the “Rapid Release”
is in practice. The aim of this paper is to provide an approach
and better understanding to support software quality improve-
ment through prediction. We combine machine learning (ML)
and artiﬁcial intelligence (AI) models to examine how code-
changes and other relevant activities during development and
release impact software quality measured through post-release
bugs. Our study will provide an insight about developers
activities and code changes to the developers which will
improve existing methods where quality is usually assessed
post-development. We believe that if we are able to identify
a software is consuming high amount of effort in terms of
commits or other quality factors at an early stage of the
software development, then the software application is most
likely to meet user expectations, satisfaction and thus have a
higher quality.
We have not measured our sixth factor “bug-ﬁx commits”
yet. We need to apply Natural Language Processing (NLP)
to understand the commit messages whether a commit is due
to bug-ﬁx or not. At this point, we will prepare our training
and testing data. The classiﬁcation model in this case would
be the decision-tree which will ﬁt the release to a quality-
level based on the different threshold values of the quality-
magnitudes as described in the methodology. Furthermore, we
will construct three other prediction models: Na¨ıve Bayes,
K-means Clustering, and Linear Regression using the same
development/release factors. We will compare the results, de-
termine the best or outperforming model. We will also discuss
the signiﬁcance of each of the factors on the post-release
bugs and will discuss the rationale behind. Furthermore, we
plan to continue in this line of study. We plan to increase
our sample size and expand the study to other open source
software projects.
REFERENCES
[1] C. W. Ara´ujo, Z. Vanius, and N. Ingrid, ”Using code quality features
to predict bugs in procedural software systems.” In Proceedings of the
XXXII Brazilian Symposium on Software Engineering, pp. 122-131.
2018.
[2] R. Chopra, ”Software quality assurance: a self-teaching introduction”.
Stylus Publishing, LLC, 2018.
[3] K. Beck and A. C. Andres. ”Extreme programming explained: Embrace
change. 2-nd edition.” (2004).
[4] AT Misirli, B. Murphy, T. Zimmermann, and A. B. Bener, ”An explana-
tory analysis on eclipse beta-release bugs through in-process metrics.”
In Proceedings of the 8th international workshop on Software quality,
pp. 26-33. 2011.
[5] A. A. Phadke and E. B. Allen, ”Predicting risky modules in open-source
software for high-performance computing.” In Proceedings of the second
international workshop on Software engineering for high performance
computing system applications, pp. 60-64. 2005.
5
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-864-8
IMMM 2021 : The Eleventh International Conference on Advances in Information Mining and Management

[6] W. B. Boehm, J. R. Brown, and M. Lipow, ”Quantitative evaluation of
software quality.” In Proceedings of the 2nd international conference on
Software engineering, pp. 592-605. 1976.
[7] M. T. Rahman and P. C. Rigby, ”Release stabilization on linux and
chrome.” IEEE Software 32, no. 2 (2015): pp. 81-88.
[8] M. V. M¨antyl¨a, B. Adams, F. Khomh, E. Engstr¨om, and K. Petersen, ”On
rapid releases and software testing: a case study and a semi-systematic
literature review.” Empirical Software Engineering 20, no. 5 (2015): pp.
1384-1425.
[9] T. Zimmermann, N. Nagappan, and A. Zeller, ”Predicting bugs from
history.” In Software evolution, pp. 69-88. Springer, Berlin, Heidelberg,
2008.
[10] S. Wehaibi, E. Shihab, and L. Guerrouj, ”Examining the impact of self-
admitted technical debt on software quality.” In 2016 IEEE 23Rd inter-
national conference on software analysis, evolution, and reengineering
(SANER), vol. 1, pp. 179-188. IEEE, 2016.
[11] H. Hemmati, Z. Fang, M. V. M¨antyl¨a, and B. Adams, ”Prioritizing
manual test cases in rapid release environments.” Software Testing,
Veriﬁcation and Reliability 27, no. 6 (2017): e1609.
[12] Eclipse bugs. url: https://bugs.eclipse.org/bugs/xmlrpc.cgi, Accessed:
March 2021.
[13] Eclipse-Equinox
Github
repository.
url:
https://github.com/eclipse/rt.equinox.p2. Accessed: December 2020.
[14] Eclipse-Equinox
ofﬁcial
repository.
url:
git://git.eclipse.org/gitroot/equinox/rt.equinox.p2.git.
Accessed:
December 2020.
[15] Gitlab
documentation
on
Git
DAG.
url:
https://docs.gitlab.com/ee/ci/directed acyclic graph. Accessed: March
2021.
[16] Eclipse-Equinox build archive. url: https://archive.eclipse.org/equinox.
Accessed: March 2021.
[17] N. Seliya and T. M. Khoshgoftaar, ”The use of decision trees for cost-
sensitive classiﬁcation: an empirical study in software quality predic-
tion.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery 1, no. 5 (2011): pp. 448-459.
[18] A. T. Misirli, B. Murphy, T. Zimmermann, and A. B. Bener, ”An
explanatory analysis on eclipse beta-release bugs through in-process
metrics.” In Proceedings of the 8th international workshop on Software
quality, pp. 26-33. 2011.
[19] S. Wagner, ”A Bayesian network approach to assess and predict software
quality using activity-based quality models.” Information and Software
Technology 52, no. 11 (2010): pp. 1230-1241.
[20] T. Dey and A. Mockus, ”Deriving a usage-independent software quality
metric.” Empirical Software Engineering 25, no. 2 (2020): pp. 1596-
1641.
[21] I. Atoum, ”A novel framework for measuring software quality-in-
use based on semantic similarity and sentiment analysis of software
reviews.” Journal of King Saud University-Computer and Information
Sciences 32, no. 1 (2020): pp. 113-125.
[22] E. Stephen and E. Mit, ”Framework for measuring the quality of
software speciﬁcation.” Journal of Telecommunication, Electronic and
Computer Engineering (JTEC) 9, no. 2-10 (2017): pp. 79-84.
[23] H. Schnoor and W. Hasselbring, ”Poster: Toward Measuring Software
Coupling via Weighted Dynamic Metrics.” In 2018 IEEE/ACM 40th
International Conference on Software Engineering: Companion (ICSE-
Companion), pp. 342-343. IEEE, 2018.
[24] F. Khomh, T. Dhaliwal, Y. Zou, and B. Adams. ”Do faster releases
improve software quality? an empirical case study of mozilla ﬁrefox.”
In 2012 9th IEEE Working Conference on Mining Software Repositories
(MSR), pp. 179-188. IEEE, 2012.
[25] Eclipse
transition
to
smaller
release
cycles.
“https://www.eclipse.org/lists/eclipse.org-planning-council/
msg02927.html”. Accessed: March 2021.
6
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-864-8
IMMM 2021 : The Eleventh International Conference on Advances in Information Mining and Management

