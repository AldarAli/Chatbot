Performance Improvement in Applying Network Coding to On-demand Scheduling
Algorithms for Broadcasts in Wireless Networks
G. G. Md. Nawaz Ali ∗, Yuxuan Meng ∗, Victor C. S. Lee ∗, Kai Liu † and Edward Chan ∗
∗ Department of Computer Science
City University of Hong Kong, HKSAR, China
Email: gnawazali2-c@my.cityu.edu.hk, ymeng7@student.cityu.edu.hk, csvlee@cityu.edu.hk, csedchan@cityu.edu.hk
† College of Computer Science, Chongqing University, Chongqing, China
Email: liukai0807@cqu.edu.cn
Abstract—Due to its ability to satisfy multiple data items
through a single broadcast, on-demand broadcasting has enjoyed
wide usage in wireless data dissemination. Recently, applying
network coding in broadcast has received much interest, because
using this technique a number of items can be served through a
single broadcast, which further maximizes the channel bandwidth
utilization and improves the overall system performance. A
generalized network coding based encoding model has been
proposed which helps to migrate a number of existing scheduling
algorithms to the network coding assisted version while pre-
serving their original scheduling criteria. However, the proposed
system only studies the homogeneous system environment. In
this work we have done an extensive simulation based on the
proposed generalized encoding model, both in the homogeneous
and heterogeneous environment, to analyze the efﬁciency and
adaptability of network coding assisted scheduling algorithms
against a number of performance metrics in the real-time
environment. Simulation studies reveal some interesting results.
Keywords-Network
coding;
on-demand
scheduling;
wireless
broadcast
I. INTRODUCTION
In wireless and mobile networks, data broadcasting is an
efﬁcient means for data dissemination. The server disseminates
data items on the shared broadcast channel, and the clients
can be served simultaneously by listening to this channel.
In general, there are three broadcasting categories: push-
based, pull-based, and hybrid broadcast [1][2]. Pull-based
broadcast is also referred to as on-demand broadcast. In on-
demand broadcast, which is the focus of this research, the
server broadcasts according to the requests sent by clients, by
compiling requests in queue and broadcasting requested data
items based on the various attributes of pending data items at
the server. Clients listen to the broadcast channel, and receive
the data items that they need.
Some on-demand data scheduling algorithms have been
proposed by researchers [1][3]-[6], which have different
application-speciﬁc performance objectives. Ahlswede et al.
[7] proposed network coding that further improves perfor-
mance. It utilizes previously received data items cached at
clients. Clients requesting different data items can be satisﬁed
simultaneously. The impact of network coding is determined
by the encoding decision of the server at each broadcast tick.
This paper is based on a generalized encoding framework
proposed and described in [8] which incorporates a ﬂexible
and adaptive network coding into data scheduling algorithms
for on-demand broadcast. Our contribution is in providing
extensive simulation experiments to study the performance of
scheduling algorithms with and without network coding.
This paper is structured as follows. Section II covers related
work in this area. Section III covers the system model used
in the paper, followed by coverage of extensive simulation
experiments in Section IV. The paper concludes in Section V.
II. RELATED WORKS
A number of data scheduling algorithms have been proposed
for on-demand broadcast, such as FCFS by Wong [1], Most
Requests First (MRF) and Longest Wait First (LWF) in [2],
and R ×W by Aksoy et al. [3]. They focused on different
metrics, such as reducing access time, deadline miss ratio, or
stretch. The well- known EDF algorithm [9] is a scheduling
algorithm in real time systems. Xuan et al. [5] proved its good
performance in on-demand broadcast. Xu et al. [6] proposed
“Slack time Inverse Number of pending requests” (SIN), which
can achieve good performance in the real-time environment.
Network coding encodes different data items, broadcasts the
encoded data in a broadcast tick, and improves performance.
The simplest coding schemes, linear coding is studied by Li
et al. [10] which examined the network capacity of multi-cast
networks. Park et al. [11] showed that network coding can
achieve even 65% higher throughput than conventional multi-
cast in a typical ad hoc network scenario. Fragouli et al. [12]
formulated an analytical model for energy-efﬁcient broadcast
in wireless ad hoc networks with coding .
Most works [1][3][4][6] assumed that through one broad-
cast, only clients requesting the same data item can be satis-
ﬁed, where the broadcast bandwidth cannot be fully utilized.
Chu et al. [13] proposed a multi-data delivery algorithm using
the XOR operator to encode the data. However, in their
encoded data item at most two items can be encoded. Recently,
Zhan et al. [8] proposed a generalized coding framework
which can be used to determine the set of request items which
can be broadcast to the maximum number of clients, where
an encoded items can use maximum coding opportunity.
In this paper, based on the generalized framework in [8],
we conduct extensive simulation experiments of a number of
134
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-346-9
ICCGI 2014 : The Ninth International Multi-Conference on Computing in the Global Information Technology

existing scheduling algorithms with coding and without coding
to explore the performance in both same and heterogeneous
item size to analyze the coding adaptivity efﬁciency.
III. SYSTEM MODEL
In this section we describe the system model as well as the
encoding framework used in this research.
A. System Architecture
Scheduler
Q 1,Q 3
Q 4,Q 2
Q 6,Q 8,Q 5
Q 10,Q 7,Q 9
«
Database
Uplink
channel
Downlink
channel
Service queue
Encoded packets
t1
t2
t3
tn
C1
C2
C3
C4
C5
Server
d1,d3,d4
d1ْ2
d1ْ3ْ4
d7
d5ْn-1ْn
Cached Data item s in C3
Clients
...
Fig. 1.
System Architecture
Our system architecture is the typical on-demand system
model for data broadcasting in wireless environment [3] as
shown in Figure 1. The system consists of one server and
many clients. When a client requires a item and cannot locate
it in its local cache, it will send a request through the uplink
channel to the server. After sending the request, a client listens
to the broadcast channel A client can decode the requested data
item from an encoded packet when it has all the other encoded
data items in the packet in its cache.
The server has a received queue where it stores the gener-
ated requests received from clients. A request is feasible if it
has enough slack time to be served. An infeasible request is
one whose deadline has missed and will be removed from the
received queue. After invoking a certain scheduling algorithm
the server retrieves the data items from the local database and
according to the clients’ cache, it forms the encoded packet
for broadcasting through the broadcast channel in the next
serving cycle. We use simple XOR operations for encoding
and decoding due to its lower overhead functionality [8][14].
The primary goal of real-time scheduling is to serve as many
requests as possible before their deadlines and to maximize
the broadcast channel bandwidth utilization.
B. Graph Model
A graph model by Zhan et al. [8] can be constructed as
follows. The system has a data server s and n clients, R =
{c1, c2, ··· , cn}. Let W(ci) be the set of data items requested
by client ci, and H(ci) be the set of data items cached at client
ci. A database containing data items is in the server, where d j
TABLE I
THE UNIFIED ENCODING MODEL
Scheduler
Input
FCFS
φ(ti,li,Ti) = 2N−j+1ti, ti is the j-th largest in
{t1, ··· , tN},
χ(t,l,T,x) = 1
MRF
φ(ti,li,Ti) = 1, χ(t,l,T,x) = 1
LWF
φ(ti,li,Ti) = ti, χ(t,l,T,x) = 1
R×W
φ(ti,li,Ti) = 1, χ(t,l,T,x) = max1≤i≤N{xi ×ti}
EDF
φ(ti,li,Ti) = 2N−j+1 1
Ti ,
1
Ti is the j-th largest in
{ 1
T1 , ··· ,
1
TN }, χ(t,l,T,x) = 1
SIN
φ(ti,li,Ti) = 1, χ(t,l,T,x) = max1≤i≤N{xi × 1
Ti }
is the j-th data item, and m is the total number of data items
in the database.
Deﬁnition 3.1: Given R
=
{c1, c2, ··· , cn}, D
=
{d1, d2, ··· , dm}, W(ci) ⊆ D, H(ci) ⊆ D, W(ci)∩H(ci) = /0,
we construct a graph G(V,E) as:
V = {vij| client ci requests for item d j, 1 ≤ i ≤ n, 1 ≤ j ≤ m}
E = {(vi1 j1,vi2 j2)| j1 = j2; or j1 ̸= j2, d j2 ∈ H(ci1), d j1 ∈
H(ci2)}
Accordingly, there are two rules to construct the graph
G(V,E), by connecting an edge between two vertices in the
graph:
the ﬁrst rule is, (vi1 j1,vi2 j2) with j1 = j2, which means that if
client ci1 and client ci2 require the same data item, there is a
link between two vertices vi1 j1 and vi2 j2.
The second rule is, (vi1 j1,vi2 j2) with j1 ̸= j2, d j2 ∈ H(ci1), and
d j1 ∈ H(ci2), which means that if client ci1’s cache contains
the data item being requested by client ci2 and vice versa, there
is a link between vertices vi1 j1 and vi2 j2.
A clique is a subset of the vertices in an undirected graph,
such that every two vertices in the subset are connected by
an edge in graph theory. To apply the network coding to
broadcast, needs to ﬁnd the maximum clique δmax of the graph
G(V,E). By broadcasting the set of requested data items in
δmax with coding, the maximum number of clients is served
in each broadcast tick.
C. Encoding Framework
Besides satisfying a number of requests by each broadcast
unit, scheduling also should pay attention to some other
applications speciﬁc requirements such as the longest waiting
time or current stretch, minimal slack time, etc. A generalized
encoding framework is proposed in [8] as follows:
The vertices in the graph are noted as v1, v2, ··· , vN, where
N is number of vertices and vi corresponds to a request qi.
Three weights, ti,li and Ti, are associated with vertex vi, where
they are respectively the waiting time, the size of data item
and the slack time of the request qi corresponding to vi.
xi ∈ {0,1} is to denote whether vertex vi is selected or
not in the optimal clique C, whereas xi = 1 means vi is
selected. Here T = {T1, T2, ··· , TN}, x = {x1, x2, ··· , xN},
t = {t1, t2, ··· , tN}, l = {l1, l2, ··· , lN}. φ(ti,li,Ti) is
the weight function of attribute(s) associated with vi and
χ(t,l,T,x) is the optimized function in different applications.
135
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-346-9
ICCGI 2014 : The Ninth International Multi-Conference on Computing in the Global Information Technology

TABLE II
SIMULATION PARAMETERS
Parameter
Default
Range
Description
ClientNum
300
—
Number of clients
λ
20
—
Mean request generation rate
DBSIZE
1000
—
Size of the database
SIZEMIN, SIZEMAX
1, 10
—
Min. and Max. data item size
CacheSize
60
30-180
Client cache size
θ
0.6
0.0-1.0
Zipf distribution parameter
μ−,μ+
120, 200
—
Min. and Max. laxity
maximize
ψ(C) = χ(t,l,T,x)
N
∑
i=1
φ(ti,li,Ti)xi
(1)
subject to
(xi + xj) ≤ 1,(vi,vj) /∈ E(G),1 ≤ i ̸= j ≤ N,xi = {0,1}
The weight function φ(ti,li,Ti) and the optimized function
χ(t,l,T,x) are used with different settings to meet various
application requirements.
The strategies for various scheduling algorithms with net-
work coding in a uniﬁed model described in [8] are summa-
rized in Table I, where each of the scheduling disciplines tries
to maximize the functions ψ(C) in (1).
IV. PERFORMANCE EVALUATION
A. Simulation Setup
Our simulation model is implemented in CSIM19 using
the default parameters shown in Table II which are based on
[8]. A closed system model is used. The item access pattern
follows the zipf distribution, where skewness is controlled
by the parameter θ. For a real-time environment, we set the
relative deadline (RD) of a request Ri as follows:
RDi = (1 + uniform(μ−,μ+))∗ T serv
i
(2)
where μ− and μ+ are the minimum and maximum laxity for
calculating the relative deadline, respectively, and T serv
i
is the
service time of Ri. The deadline of a request Ri is Dli, which
is computed by:
Dli = ATi + RDi
(3)
where ATi is the arrival time of Ri.
The channel bandwidth is 1.0. For the homogeneous envi-
ronment each item size is 1.0, but for the heterogeneous envi-
ronment different item sizes are generated using the random
item size distribution (RAND) [15].
B. Performance Metrics
1) Deadline Miss Ratio (DMR): The ratio of the number of
requests which missed their deadline over all submitted
requests..
2) Average Encode Length (AEL): The average number of
data items encoded in each encoded packet. A high AEL
means more clients can decode their expected data items
from an encoded broadcast packet.
3) Saved Bandwidth Ratio (SBR):
SBR = #Satisfied Item− #Broadcast Item
#Satisfied Item
(4)
where Satisﬁed Item Number means the number of data
items satisﬁed and Broadcast Item Number the number
of encoded items being broadcast.
C. Performance Analysis
In this section, we discuss the performance analysis of
coding and non-coding versions of different existing schedul-
ing algorithms under both the same and different item size
distribution. Simulation experiments continue until 98% con-
ﬁdence interval has been achieved. The different scheduling
algorithms can be divided into two types: algorithms such as
SIN and EDF which target time constraints or urgency (TC)
and those that do not focus on time constraints (NTC).
1) Impact of Skewness Parameter θ: Figure 2 shows the
impact of data item access pattern θ. When θ = 0.0, item
access pattern is uniformly distributed. But with increasing
θ, the probability for accessing popular data items increases.
In serving such a popular item, a number of requests can
be served concurrently, which explains why the performance
of all algorithms improves with increasing θ. From Figure
2(a)(I), for the same item size distribution, network cod-
ing assisted TC algorithm SIN N and EDF N show better
performance than NTC network coding assisted algorithms
MRF N, FCFS N, R×W N and LWF N. To see the impact
of considering urgency as a request selection, consider Figure
3 (I). This ﬁgure shows the percentage of served requests
over total submitted requests against the slack time (ST) of
requests in the system under default settings. The percentage
value in the y-axis against the value ‘0-1/4 of RD’ in the x-
axis means percentage number of requests served when these
requests have remaining deadlines i.e., slack times (ST) less
than 1/4 of the original assigned relative deadlines (RD) of the
respective requests. Similarly, there are total four categories.
To understand this metric, let us have an example. Suppose
a request Ri’s relative deadline RDi is 12. So, initially Ri’s
slack time STi is also 12. As time passes STi also decreases.
Now, if Ri is served when STi = 10, it will fall in the 4th
category (that is 3/4 - RD = 9 - 12). Similarly, if Ri is served
when ST = 7 or ST = 1 it will fall in the 3rd and 1st category
respectively. Accordingly, if more number requests served in
the 1st and subsequent categories, the algorithm is more aware
of the scheduling of deadline urgent requests. From Figure
3 (I) we can see that, in SIN N more requests are served in
the ﬁrst category when compared to MRF N, in which more
requests are served in the last category. It proves that MRF N
does consider the slack time of the requests, which is why
fewer requests are served in the ﬁrst category, hence urgent
requests may not meet their deadlines resulting in higher DMR
in MRF N.
On the contrary, from Figure 2(a) (II), surprisingly TC
algorithms SIN N and EDF N can not retain their supremacy
in terms of DMR for RAND item size distribution. Recalling
136
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-346-9
ICCGI 2014 : The Ninth International Multi-Conference on Computing in the Global Information Technology

0.0
0.2
0.4
0.6
0.8
1.0
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
 
 
Deadline Miss Rate (DMR)
THETA
 FCFS_N
 FCFS
 MRF_N
 MRF
 LWF_N
 LWF
 R×W_N
 R×W
 EDF_N
 EDF
 SIN_N
 SIN
0.0
0.2
0.4
0.6
0.8
1.0
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
Deadline Miss Ratio (DMR)
THETA
 FCFS_N
 FCFS
 MRF_N
 MRF
 LWF_N
 LWF
 RW_N
 RW
 EDF_N
 EDF
 SIN_N
 SIN
(a) DMR for (I) Same, and (II) RAND item size distribution
0.0
0.2
0.4
0.6
0.8
1.0
1.8
2.0
2.2
2.4
2.6
2.8
3.0
 
Average Encode Length (AEL)
THETA
 FCFS_N
 MRF_N
 LWF_N
 R×W_N
 EDF_N
 SIN_N
0.0
0.2
0.4
0.6
0.8
1.0
1.8
2.0
2.2
2.4
2.6
2.8
3.0
3.2
3.4
Average Encode Length (AEL)
THETA
 FCFS_N
 MRF_N
 LWF_N
 RW_N
 EDF_N
 SIN_N
(b) AEL for (I) Same, and (II) RAND item size distribution
0.0
0.2
0.4
0.6
0.8
1.0
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
Saved Bandwidth Ratio (SBR)
THETA
 FCFS_N
 FCFS
 MRF_N
 MRF
 LWF_N
 LWF
 R×W_N
 R×W
 EDF_N
 EDF
 SIN_N
 SIN
0.0
0.2
0.4
0.6
0.8
1.0
-0.3
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Saved Bandwidth Ratio (SBR)
THETA
 FCFS_N
 FCFS
 MRF_N
 MRF
 LWF_N
 LWF
 RW_N
 RW
 EDF_N
 EDF
 SIN_N
 SIN
(c) SBR for (I) Same, and (II) RAND item size distribution
Fig. 2.
Impact of skewness parameter (θ).
0 - 1/4 of RD
1/4 - 1/2 of RD 1/2 - 3/4 of RD
3/4 - RD
Overall
0
10
20
30
40
50
60
70
80
90
Percentage of Served Requests over 
total submitted requests
Request Slack Time (ST)
 MRF_N
 SIN_N
0 - 1/4 of RD
1/4 - 1/2 of RD 1/2 - 3/4 of RD
3/4 - RD
Overall
0
10
20
30
40
50
60
Percentage of Served Requests over 
total submitted requests
Request Slack Time (ST)
 MRF_N
 SIN_N
Fig. 3.
Distribution of serving requests over total submitted requests for(I) Same, and (II) RAND item size distribution.
137
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-346-9
ICCGI 2014 : The Ninth International Multi-Conference on Computing in the Global Information Technology

30
60
90
120
150
180
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
 
 
Deadline Miss Ratio (DMR)
Cache Size
 FCFS-N
 FCFS
 MRF-N
 MRF
 LWF-N
 LWF
 R×W-N
 R×W
 EDF-N
 EDF
 SIN-N
 SIN
30
60
90
120
150
180
0.32
0.34
0.36
0.38
0.40
0.42
0.44
0.46
0.48
0.50
0.52
0.54
0.56
0.58
Deadline Miss Ratio (DMR)
Cache Size
 FCFS_N
 FCFS
 MRF_N
 MRF
 LWF_N
 LWF
 RW_N
 RW
 EDF_N
 EDF
 SIN_N
 SIN
(a) DMR for (I) Same, and (II) RAND item size distribution
30
60
90
120
150
180
1.6
1.8
2.0
2.2
2.4
2.6
2.8
3.0
3.2
3.4
3.6
 
Average Encode Length (AEL)
Cache Size
 FCFS-N
 MRF-N
 LWF-N
 RW-N
 EDF-N
 SIN-N
30
60
90
120
150
180
1.8
2.0
2.2
2.4
2.6
2.8
3.0
3.2
3.4
3.6
3.8
Average Encode Length (AEL)
Cache Size
 FCFS_N
 MRF_N
 LWF_N
 RW_N
 EDF_N
 SIN_N
(b) AEL for (I) Same, and (II) RAND item size distribution
30
60
90
120
150
180
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
Saved Bandwidth Ratio (SBR)
Cache Size
 FCFS-N
 FCFS
 MRF-N
 MRF
 LWF-N
 LWF
 RW-N
 RW
 EDF-N
 EDF
 SIN-N
 SIN
30
60
90
120
150
180
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
Saved Bandwidth Ratio (SBR)
Cache Size
 FCFS_N
 FCFS
 MRF_N
 MRF
 LWF_N
 LWF
 RW_N
 RW
 EDF_N
 EDF
 SIN_N
 SIN
(c) SBR for (I) Same, and (II) RAND item size distribution
Fig. 4.
Impact of Cache Size.
from the network coding view, for different item sizes, the
size of the encoded packet will be the maximum item size
in the selected clique. Now, a TC algorithm uses slack time
for selecting the optimal clique. Unlike productivity based
algorithm, it may select a clique having smaller clique size
with urgent request as a clique vertex. Regardless of the
selected clique size, the service time of the encoded packet
will be as large as the service time of the largest data item in
the selected clique, hence the service time is increased, which
makes it more difﬁcult to meet the deadline. This statement
is supported by Figure 3 (II), which shows that unlike same
item size distribution, SIN N does not outperform MRF N
in the ﬁrst category (0 - 1/4 of RD), and overall requests
serving percentage also lower than MRF N. Nevertheless, the
non network coding version of the algorithms do not have this
clique size problem, and hence show superior performance
for skewed access pattern (Figure 2(a)(II)). On the other
hand, the coding version of a NTC based algorithm typically
tries to serve a clique having maximum clique size, hence
although the service time is increased, many requests can be
served concurrently. Therefore, although such an algorithm in
heterogeneous item size environment has worse performance
than the same item size environment, it still has a better
performance than its corresponding non-coding version as well
as the network coding version.
Except for FCFS N, all the NTC algorithms use item
productivity as their request selection criterion, i.e. use size of
a clique ((in other words, number of vertices in the clique)),
but ignore the urgency of a request. However, FCFS N selects
the optimal clique, including the weight of a corresponding
vertex, waiting time of all the vertices in a clique is counted,
which indirectly supports the clique size. For example, it is
138
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-346-9
ICCGI 2014 : The Ninth International Multi-Conference on Computing in the Global Information Technology

more likely that a clique having larger size, might have more
total weight than that of a smaller size clique. So, the network
coding version of FCFS, i.e. FCSF N, has similar performance
to other WTC algorithms. It is evident from Figure 2(a)(I),
network coding version of a algorithm has better performance
than its non-network coding version. An exception is EDF,
when θ = 1.0; EDF N shows better DMR than EDF. This is
because in the skewed item access pattern the non-network
coding version of an algorithm can gain more than network
coding environment.
From Figures 2(b)(I) and (II), with increasing θ, average
encode length (AEL) of all the algorithms increases, because
with increased access pattern more requests ask for the popular
data items, hence being a requested item of a client could
be a cached item of another client with a higher probability,
which helps to form a bigger clique and increase the coding
opportunity as well. NTC algorithms have better AEL and
saved bandwidth ratio (SBR) than TC algorithms irrespective
of item size distribution as shown in Figures 2(b)(I), and
2(b)(II), 2(c)(I) and 2(c) (II). Regarding SBR, regardless of
item size distribution, coding version of an algorithm has better
SBR than its non-coding version. This is because, as the client
gradually accumulates more items in the cache, the client can
exploit its cache in the coding version to satisfy more requests.
This is clearly not possible in the non-coding version of the
algorithms.
2) Impact of Cache Size: Figure 4 shows the impact of
the cache size of a client for both same and different item
size distribution. Since the non-coding version of an algorithm
does not consider client’s cache, changes in cache size has no
impact on performance. Increasing cache of a client provides
more room to store more items, which increases the probability
of one client’s cached item to be another client’s requested
item. This is the key to increase the coding ﬂexibility and
AEL which provides more opportunity for performance gain.
For this reason DMR declines with increasing cache size for
both same and rand item size distributions (Figures 4(a)(I),
4(a)(II)), AEL increases (Figures 4(b)(I), 4(b)(II)), and SBR
increases (4(c)(I), 4(c) (II)).
V. CONCLUSION AND DISCUSSION
In this paper, based on the generalized encoding model
proposed in [8], we conducted extensive simulations for same
and different item size distributions in the real-time environ-
ment. We analyze the simulation performance of a number
of existing scheduling algorithms both in their coding and
non-coding versions against a number of performance metrics
to examine the efﬁciency and adaptability of coding assisted
scheduling algorithms. Simulation results show that network
coding assisted algorithms improve their performance with
increased cache size and skewed access pattern. Generally
speaking, NTC algorithms have superior performance in AEL
and SBR than TC algorithms irrespective of data item size
distribution. In addition, regardless of item size distribution, all
the coding assisted algorithms outperform their corresponding
non-coding version in SBR. On the other hand TC based
algorithms perform better in DMR in the real-time setting
for the same item size distribution. However, surprisingly, in
the different item size distribution their performance decline
dramatically. A plausible reason is that in the coding version,
the scheduling algorithm needs to serve the maximum sized
item of the selected clique irrespective of the clique size in
each broadcast, hence a clique may have urgent vertex but also
may have smaller clique size. On the contrary, the non-coding
version simply selects the best item based on the underlying
scheduling principle.
We are exploring the use of other metrics in deepening our
understanding of the behavior of different algorithms when
network coding is used, and will hopefully present the results
in a future paper.
REFERENCES
[1] J. W. Wong and M. H. Ammar, “Analysis of broadcast delivery in
videotex system,” Journal of IEEE Transactions on Computers, vol. 34,
no. 9, September 1985, pp. 863–866.
[2] J. W. Wong, “Broadcast delivery,” Proceedings of the IEEE, vol. 76,
no. 12, 1988, pp. 1566–1577.
[3] D. Aksoy and M. Franklin, “R×W: A scheduling approach for large-
scale on-demand data broadcast,” Journal of IEEE/ACM Transactions
on Networking (TON), vol. 7, no. 6, December 1999, pp. 846–860.
[4] S. Acharya and S. Muthukrishnan, “Scheduling on-demand broad-
casts: new metrics and algorithms,” in Proceedings of the 4th annual
ACM/IEEE international conference on Mobile computing and network-
ing (MobiCom’98), 1998, pp. 43–54.
[5] P. Xuan, S. Sen, O. Gonzalez, J. Fernandez, and K. Ramamritham,
“Efﬁcient and timely dissemination of data in mobile environments,” in
Proceedings of the 3rd IEEE Real Time Technology and Applications
Symposium(RTAS’97), Montreal, Canada, 1997.
[6] J. Xu, X. Tang, and W. Lee, “Time-critical on-demand data broadcast
algorithms, analysis and performance evaluation,” IEEE Transactions on
Parallel and Distributed Systems, vol. 17, no. 1, 2006, pp. 3–14.
[7] R. Ahlswede, N. Cai, S.-Y. Li, and R. Yeung, “Network information
ﬂow,” IEEE Transactions on Information Theory, vol. 46, no. 4, July
2000, pp. 1204–1216.
[8] C. Zhan, V. Lee, J. Wang, and Y. Xu, “Coding-based data broadcast
scheduling in on-demand broadcast,” IEEE Transactions on Wireless
Communications, vol. 10, no. 11, November 2011, pp. 3774–3783.
[9] C. Liu and J. Layland, “Scheduling algorithms for multiprogramming
in hard real-time trafﬁc environments,” Journal of the Association for
Computing Machinery (ACM), vol. 20, no. 1, 1973, pp. 46–61.
[10] S.-Y. Li, R. Yeung, and C. Ning, “Linear network coding,” IEEE
Transactions on Information Theory, vol. 49, no. 2, February 2003, pp.
371–381.
[11] J.-S. Park, D. Lun, F. Soldo, M. Gerla, and M. M´edard, “Performance
of network coding in ad hoc networks,” in Proceedings of the IEEE
Military Communications Conference, Washington, DC, October 2006,
pp. 1–6.
[12] C. Fragouli, J. Widmer, and J.-Y. Le Boudec, “A network coding
approach to energy efﬁcient broadcasting: From theory to practice,” in
Proceedings of the 25th IEEE International Conference on Computer
Communications (INFOCOM’06), Barcelona, Spain, April 2006, pp. 1–
11.
[13] C.-H. Chu, D.-N. Yang, and M.-S. Chen, “Multi-data delivery based
on network coding in on-demand broadcast,” in Processings of the
9th International Conference on Mobile Data Management (MDM’08),
Beijing, April 2008, pp. 181–188.
[14] J. Chen, V. C. S. Lee, and C. Zhan, “Efﬁcient processing of real-
time multi-item requests with network coding in on-demand broadcast
environments,” in Proceedings of the 15th IEEE International Confer-
ence on Embedded and Real-Time Computing Systems and Applications
(RTCSA ’09), Beijing, August 2009, pp. 119 – 128.
[15] J. Xu, Q. Hu, W. Lee, and D. L. Lee, “Performance evaluation of
an optimal cache replacement policy for wireless data dissemination,”
IEEE Transactions on Knowledge and Data Engineering, vol. 16, no. 1,
January 2004, pp. 125–139.
139
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-346-9
ICCGI 2014 : The Ninth International Multi-Conference on Computing in the Global Information Technology

