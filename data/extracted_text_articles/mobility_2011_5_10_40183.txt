Performance Evaluation of
Distributed Application Virtualization Services
Using the UMTS Mobility Model
Chung-Ping Hung∗ and Paul S. Min†
Department of Electrical and Systems Engineering, Washington University in St. Louis
One Brookings Drive, St. Louis, MO 63130, USA
Email: ∗chung23@wustl.edu †psm@wustl.edu
Abstract—In this paper, we ﬁrst introduce how virtualization
technologies can mitigate mobile application software publishing
problems due to platform diversity and fragmentation. In our
previous work, we proposed a distributed server arrangement
and the corresponding hand-off protocol to provide better user
experience for application virtualization on mobile devices and
evaluated the performance using the modiﬁed UMTS outdoor to
indoor pedestrian mobility model. We continue our previous work
on evaluating performance of the proposed service architecture
using the UMTS rural vehicular mobility model with similar
modiﬁcations. In this paper, combined with our previous work,
we complete the establishment of quantitative relations between
the performance improvement or impact and the infrastructure
related parameters in the typical mobility model.
Index Terms—telecommunication and wireless networks, com-
puter networks, information technology, UMTS mobility model.
I. INTRODUCTION AND RELATED WORK
Advanced wireless communication and low-power semicon-
ductor technologies have enabled mobile computing widely
available to consumers and dramatically expanded our imag-
ination on personal computing. Mobile computing devices,
however, are hardly considered as technical extension of
general purpose computers. Mobile computing devices are
limited in computational resources, communication-oriented,
and highly compact. Therefore, mobile computing devices are
“advanced interactive embedded real-time systems” rather than
“reduced PCs”.
The nature of mobile computing devices inevitably makes
the software engineering on them tightly managed by plat-
form vendors. Although centrally managed software publish-
ing helps software developers securing their revenue, this
paradigm also enables platform vendors to take much more
control on SDKs, design guidelines, and whether a 3rd party
software product can be shipped or not. Furthermore, various
mobile operating systems are still competing with each other
and none is expected to dominate the market in 2 or 3
years. Software developers have to deal with multiple dictating
bureaucracies, instead of one, to make their products available
to most customers and maximize their revenue. Therefore,
developing cross-platform software for mobile computing de-
vices is a costly work using conventional frameworks.
Fortunately, virtualization technologies can work around
the difﬁculties of deploying cross-platform mobile application
software. Roughly speaking, application virtualization tech-
nologies can be categorized into two major paradigms: one is
creating a compatible runtime platform, i.e., virtual machine,
on each client’s device and publishing well managed code
packages running on top of it [1][2], and the other is executing
application software on a well managed server while each
client’s device only handles user inputs and server outputs
[3][4][5]. We generally refer to the later paradigm as the
browser-based approach since web browsers provide a very
ideal framework for it.
Despite being technically feasible, deploying a virtual ma-
chine running on top of a mobile operating system to execute
downloaded common codes circumvents the ofﬁcial software
publication platform and generally is considered a violation of
the “Non-Compete” policy [7][8] by marketplace operators.1
Therefore, the browser-based approach becomes the remaining
legitimate way to provide application virtualization services
on mobile computing devices for general 3rd party developers
without special privilege, unless marketplace operators enforce
the “Non-Compete” policy against interactive web contents.
The conventional web-based application virtualization re-
lies on a centralized server to provide the service through
established Internet infrastructure. Although this conﬁguration
can be built with low cost, the long response latency could
signiﬁcantly reduce the user experience since every input
must travel through a series of routers and bridges to the
colocation center and the corresponding update has to traverse
backward through the nodes. Each node along the route
induces processing delay, queuing delay, and transmission
delay, and each link comprises the route induces propagation
delay. Generally speaking, network delay is highly related to
the geographical distance between two end points given similar
network infrastructure technologies.
1VMware’s Mobile Virtualization Platform (MVP) [6], which implements
this paradigm, is not available in Android Marketplace. To install MVP on an
Android phone requires sideloading, and only Android platform leaves this
loophole to install apps outside the marketplace, which is at the mercy of
Google and wireless service providers. In fact, some wireless providers do
block sideloading on some Android phones. Furthermore, among the major
mobile device players, only Android is supported by MVP. Therefore, even
VMware starts their own app store for MVP, it doesn’t help cross-platform
software deployment anyway.
93
MOBILITY 2011 : The First International Conference on Mobile Services, Resources, and Users
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-164-9

In our previous work [11], we have proposed an alterna-
tive conﬁguration to address this issue that geographically
partitions the service area into multiple smaller service areas
and deploys a smaller server for each one to provide the
service locally. The proposed conﬁgurations should signiﬁ-
cantly reduce propagation delay in most case due to the shorter
average communication distance. The proposed conﬁguration,
however, has to handle hand-off cases, i.e., mobile stations
in use moving from one service area to another. Therefore,
we also proposed a hand-off protocol offering seamless user
experience.
Handling hand-offs induces longer response latency and
thus the overall performance depends on the hand-off behav-
ior model. Therefore, we used an empirical and simpliﬁed
approach to evaluate the performance as a result of infrastruc-
ture arrangement and application software’s properties in our
previous work [11]. We also used one of the UMTS mobility
models to empirically establish the correlations between the
performance and the size of each local service area and the
capabilities of the network infrastructure in [12]. In this paper,
we complete the performance simulation of the proposed
architecture with the UMTS rural vehicular mobility model.
In the UMTS rural vehicular mobility model, base stations
(BSs) are sparsely but optimally placed, mobile stations (MSs)
move faster and more freely, and the hand-off behavior among
base stations is different as well. Consequently, the simulation
program in the UMTS rural vehicular model is signiﬁcantly
different from the one presented in our previous work though
sharing the same concept. The simulation algorithm and re-
sults based on the UMTS rural vehicular mobility model are
represented in this paper.
There are several papers proposed to optimize service
migration though for different applications. Bienkowski et
al. proposed competitive analysis for service migration in
optimizing the server allocation in VNets in [9]. Arora et
al. proposed some strategies for ﬂexible server allocation in
[10] following the previous work [9]. Although these works
were not speciﬁcally for mobile application virtualization, they
provide a precious insight on the performance evaluation for
dynamic service allocation considering both user experience
and operational cost. However, the analytical approach used
in these works is topological and does not focus on the
user mobility and interaction models. In our approach, we
simulate the user mobility geographically based on the UMTS
mobility models which provides an alternative performance
preview in resource migration. Furthermore, the authors of [9]
and [10] allow services being temporarily interrupted during
migrations, which is not feasible for application virtualization
services. In the proposed conﬁguration, application services
are available to users with reduced performance during hand-
offs.
The rest of this paper is organized as follows. In Section
II, we describe the proposed conﬁguration aim to improve
the user experience of application virtualization services. In
Section III, we propose a VM-level hand-off protocol to
handle the additional information exchange brought by the
proposed server conﬁguration. We specify our experiment
design, settings, and cost metrics in Section IV. Then the
simulation results given different parameter adjustments are
presented in Section V. Finally, we conclude our work and
outline some future work we expect to do in Section VI.
II. PROPOSED CONFIGURATION
Running application software on a remote server while cre-
ating an illusion that the client has full control of the software
in hand is conceptually similar to the usage model of time-
sharing mainframe computers in the 1960s [13]. Although the
communication bandwidth between terminals and mainframe
servers at that time was very low by modern standards, it did
not affect the user experience thanks to the text-only display
and short traverse distance. However, in recent application
virtualization technologies which follow the same concept,
such as Virtual Desktop Infrastructure (VDI) proposed by
VMware [14], much more versatile and bloated content must
be exchanged over much longer distances between clients and
servers than their predecessors.
An infrastructure ready to offer mobile users application
virtualization services includes base stations covering the
whole service area, a core network connecting base stations
and servers together, and a server hosting the services. A
command sent by a mobile station has to travel over the
wireless channel to the BS, go through the backhaul network to
the server, and then make some changes on the server. Should
any update corresponding to the command be sent to the MS,
the information has to travel all the way backward. In order
to reduce the network delay generated by long transmission
distances among the backhaul network, we geographically
deploy multiple servers among a wide area to serve their
nearby MSs in the proposed conﬁguration, instead of setting
up one centralized server serving all MSs.
In the proposed conﬁguration, each server connects to
several nearby BSs to form a local service group (LSG). The
area covered by the BSs of the same LSG is deﬁned as the
local service area (LSA). Every BS should belong to one
LSG in order to provide the service all over the wireless
network’s coverage area. When a user demands a virtual
application program, the server of the LSG, based on VDI
[14] paradigm, starts a virtual machine (VM) dedicated to the
user and launches the application software on top of it. The
MS only handles inputs and outputs that interact with the VM
at the server.
As long as the MS stays in the same LSA, the user can
enjoy using application software with low response latency. If
the MS moves from the original LSA to a nearby one, a hand-
off at the VM level, which transfers the runtime environment to
the server of the next LSG, is triggered. Therefore, a protocol
to deal with the hand-off condition is required.
III. HAND-OFF PROTOCOL
The purpose of the proposed hand-off protocol is to trans-
fer minimum information required to recreate the runtime
environment on a remote server, i.e., the snapshot, without
94
MOBILITY 2011 : The First International Conference on Mobile Services, Resources, and Users
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-164-9

Fig. 1.
Protocol timeline for an MS moving from Server A to Server B.
interrupting the service. To provide a seamless user experience
during transmitting the snapshot, the next server has to record
all inputs from the MS, relay all inputs to the previous server,
and relay all output from the previous server to the MS, until
the runtime environment resumes locally. The proposed hand-
off protocol is described as below:
1) When an MS moves from Server A’s to Server B’s
LSA and sends an input command, Server B notices
a newcomer within its LSA.
2) Server B broadcasts the newcomer’s identiﬁcation to all
geographically nearby servers.
3) Server A, which hosts the MS’s runtime environment,
i.e., its VM server, responds Server B’s inquiry. Now
Server B knows the newcomer’s VM server is Server A.
4) Server B records and relays the user’s input commands
to Server A, signals Server A to transfer the runtime
environment, and relays display updates from Server A
to the newcomer.
5) Once Server A is signaled to transfer the runtime envi-
ronment, it takes a snapshot.
6) Besides continually responding to the input commands
relayed from Server B as the MS is still in its LSA,
Server A also sends the snapshot to Server B in the
background.
7) Once Server B receives the complete snapshot and
recreates the runtime environment from the snapshot
and base data, it internally feeds the input queue, which
was recorded during the transition period, to the runtime
environment. Therefore, the runtime environment state
on Server B is synchronous with that on Server A after
the snapshot was transferred.
8) Server A completely stops serving the MS, the MS’s
VM server is now Server B instead.
The timeline of the proposed hand-off protocol is illustrated
in Fig. 1.
If the MS turned around and reentered Server A’s LSA
before the hand-off was completed, Server A can preempt
the snapshot transmission and resume serving the MS as if
the hand-off never happened. Since Server B relays all inputs
to Server A while the MS is absent from Server A’s LSA,
aborting the hand-off procedure would not generate any no-
ticeable glitch. This hand-off abortion mechanism can prevent
unnecessary data transmission from moving VM servers back
and forth if an MS were moving around the edge of an LSA.
On the other hand, if the MS moved to Server C’s LSA
before the hand-off was completed, Server C initializes another
hand-off procedure with Server B. In addition to the snapshot,
Server B has to transfer the input record before Server C joins
the hand-off chain. We allow pipelining transmission to reduce
hand-off periods and shorten subsequent hand-off chains in
this scenario.
IV. PERFORMANCE EVALUATION
The proposed service architecture is designed to reduce in-
teraction latency and thus provide more responsive user experi-
ence on remote controlled application virtualization services.
However, due to the involvement of the hand-off protocol,
the performance of the proposed service architecture depends
highly on the probability of hand-offs, the geographical de-
ployment of the BSs, and the conﬁguration and capability of
the backhaul network. The former two factors can be modeled
by the test environments of existing communication systems,
such as the well-published UMTS benchmarks [15]. On the
other hand, the conﬁguration and capability of the backhaul
network can only be assumed based on reasonable technical
and cost considerations.
A. UMTS Vehicular Mobility Model
The UMTS document [15] provides three different test
environments, which are the Indoor Ofﬁce, the Outdoor to
Indoor and Pedestrian, and the Vehicular ones, for technology
selection and evaluation. We simulated the hand-off behavior
and evaluated performance of different infrastructure settings
using the Outdoor to Indoor and Pedestrian mobility model in
our previous work [12]. In this paper, we complete the perfor-
mance evaluation by simulating the application virtualization
services on the Vehicular test environment speciﬁed in the
UMTS document.
As shown in Fig. 2, the UMTS rural vehicular test en-
vironment is a plain with no physical obstacle. Each MS’s
speed is ﬁxed at 120 km/h. Each MS’s moving direction is
allowed to change up to 45◦ left or right every 20 meters with
20% chance. All MSs are initially uniformly distributed on the
plain.
The BSs in the UMTS rural vehicular test environment are
located at the dark grey dots in Fig. 2. Each BS has three
directional antennae to serve tri-sectored cells. Each cell is
assumed to be a hexagon and seamlessly tiles with each other.
Each cell’s radius R is either 2000 meters (for services up
to 144kbit/s) or 500 meters (for services above 144kbit/s).
Therefore, the minimum distance between two BSs can be 6
km or 1.5 km, respectively.
The original UMTS mobility model generates discontinu-
ities on the boundaries of the test area. We consequently
add some special trafﬁc rules, known as portals, to eliminate
the boundary discontinuities and allow the interaction among
95
MOBILITY 2011 : The First International Conference on Mobile Services, Resources, and Users
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-164-9

Fig. 2.
The UMTS rural vehicular test environment with LSA arrangement.
LSAs to be simulated and observed for an indeﬁnite period of
time. The characteristics of the portals will be detailed in the
next section.
B. M¨obius County
What interests us is the geographical relation between the
service facilities and the MSs’ moving space. As the method
we conducted in our previous work [12], the ﬁrst step is to
deﬁne a sample area which can represent all the geographical
characteristics of service infrastructure we need. We ﬁrst group
the BSs in Fig. 2 to form approximately hexagon-shaped
LSAs which are optimized in both coverage and average
transmission distance by deploying servers at the centers. As
the urban counterpart, i.e., M¨obius City, in our previous work
[12], the sample area should include one complete LSA in
the center and six neighboring halves. Given R and N, the
number of the BS intervals per LSA’s edge, if we align the
origin to the server of an LSG, we deﬁne the Parallelogram
ABCD surrounded by four straight lines, which are:
1)
√
3x − 3(2N + 1)y = −6
√
3R(3N 2 + 3N + 1) on the
north,
2)
√
3x − 3(2N + 1)y = 6
√
3R(3N 2 + 3N + 1) on the
south,
3)
√
3(2N + 1)x + y = −3
√
3R(3N 2 + 3N + 1) on the
west,
4) and
√
3(2N + 1)x + y = 3
√
3R(3N 2 + 3N + 1) on the
east.
as the sample area of our best interest. We can, therefore, crop
out Parallelogram ABCD in Fig. 2 as our test area, where we
call M¨obius County as shown in Fig. 3, to represent every
identical piece comprises the indeﬁnite large test area.
Fig. 3.
M¨obius County map with teleporting directions.
Like M¨obius City [12], assigning four logical LSGs in
M¨obius County is sufﬁcient to ﬁgure out when, where, and
how frequently an MS moves from one LSA to another.
However, to apply the hand-off aborting mechanism, which
was disabled in [12], we need to distinguish whether an MS
is coming back to the LSA it just left or entering the LSA on
the opposite side of the one it just crossed. Therefore, we have
to assign an additional unique identiﬁcation for each LSG.
The portals around M¨obius County are also similar to those
around M¨obius City. Whenever an MS is about escaping from
M¨obius County, the portal teleports it to a proper location at
the opposite side so that it reenters M¨obius County. Therefore
M¨obius County can emulate a limitless test area. Since there
is no street structure to align in M¨obius County, the rules of
the portals are much more simple and straightforward than of
M¨obius City:
1) For MSs about crossing the north boundary, teleport
them to
(
3R, −3
√
3R(2N + 1)
)
from their current lo-
cations.
2) For MSs about crossing the south boundary, teleport
them to
(
−3R, 3
√
3R(2N + 1)
)
from their current lo-
cations.
3) For MSs about crossing the west boundary, teleport them
to
(
− 9R(2N+1)
2
, − 3
√
3R
2
)
from their current locations.
4) For MSs about crossing the east boundary, teleport them
to
(
9R(2N+1)
2
, 3
√
3R
2
)
from their current locations.
The teleport directions are shown in Fig. 3 as well.
The purpose of the portals is to eliminate all discontinuities
except the MS’s coordinates when it is moving out of the
boundary: it keeps the same direction and speed, it associates
with the same logical LSG, and preserves the geographical
parameters relative to the service group’s facilities. Thus,
96
MOBILITY 2011 : The First International Conference on Mobile Services, Resources, and Users
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-164-9

everything interests us is equivalent as the MS moving into
an adjacent parallelogram area in a limitless test area.
C. Conﬁguration of Backhaul Network
We assume a mesh-styled backhaul network as we did
in [12]. Therefore, each BS only has direct links to its
six neighboring BSs. In the mesh-styled backhaul network,
network latency between a BS and the server depends on the
number of nodes along the shortest path, the total length of the
path, and the relay latency per node. The former two factors
are related to the coordinates of the BS and the server, while
the last one is varied to simulate different nodal transmission
capabilities.
D. Performance Metric
We deﬁne the response time as the average time interval
between when a user sends an input and gets an expected
output update. The proposed server conﬁguration is meant to
improve the response time by reducing traverse delay along
the communication route from each MS to the server which
is hosting the service. Factors other than the traverse delay,
such as computational capabilities provided by servers, would
affect the user experience and the quality of our service. Most
of them, however, either affect different conﬁgurations equally,
or can be overcome with reasonable cost. The traverse delay
is deﬁned as:
Ttv = 2 ·
{Lr
Vr
+ Ll
Vl
+ Nrt · Trt + Nrl · Trl
}
(1)
where Lr is the distance of radio transmission, which is the
distance between the MS and the BS covering it, Vr is the
propagation speed of radio, which is the speed of light, Ll is
the total length of wireline transmission in the mesh network,
Vl is the propagation speed in wireline, which is approximately
two thirds of Vr, Nrt is the number of nodes along the
transmission path in the mesh network, Trt is the average
waiting time per node in the mesh network, which includes
nodal processing delay, queuing delay, and transmission delay,
Nrl is the number of servers which are receiving the snapshot
and relaying data to/from the VM server, and Trl is the
processing and relay time per server in the hand-off chain.
Obviously, all parameters, except Vr and Vl, depend on an
MS’s geographical location and hand-off state.
E. Hand-off Duration
Whenever a VM-level hand-off occurs, we set up an an-
ticipated hand-off end time by adding hand-off duration to
the current time. The hand-off duration is calculated by the
following equation:
Tho = Tx + Ls
Vl
+ Ns · Trt
(2)
where Tx is the total time to deliver every bit of a snapshot to
media, which is the summation of queuing delay, processing
delay, and transmission delay of the snapshot, which is propor-
tional to the size of the snapshot, Ls is the total transmission
distance between the current and the next VM servers, and
Ns is the number of nodes between two neighboring servers,
which always equals to 2N + 1 in this case.
F. Update Time Points and Cost Charging
Although we only calculate costs at position update points,
updates actually take place when a hand-off is completed in
addition to when an MS reaches an update position. At each
update time point, Ttv and transaction counts are updated
concurrently.
Whenever a position update comes at Tnow, all hand-off
end times registered in queue prior to Tnow are update time
points as well. The corresponding costs have to be calculated
in retrospect according to the algorithm described below:
1) Deﬁne Tn as the nth earliest hand-off end time in queue,
Lsn as the total transmission distance between servers
corresponding to the nth earliest hand-off in queue,
Lr, Ll, Nrt, and Nrl are the current cost parameters
calculated by the MS’s current position and hand-off
status, and Tlast as the previous update time.
2) If Tnow
> T0, insert an update time point at T0,
calculate the transaction counts by the Poisson process
given user input rate λ and time duration (T0 − Tlast),
set Tlast = T0, subtract Nrl by one, subtract Nrt by
{2N + 1}, subtract Ll by Ls0, update Ttv according to
the new parameters, and remove T0 and corresponding
Ls0 from the queues.
3) Redo step 2 until Tnow < T0 or the queue is emptied.
4) Calculate the transaction counts by the Poisson process
given λ and time duration (Tnow − Tlast), update Ttv
according to the new parameters, and set new Tlast =
Tnow.
As speciﬁed in the UMTS rural vehicular mobility model,
we update the MSs’ positions every 20 meters. Since a hand-
off may occur at the same time, we have to handle the extra
cost brought by it as well. When a new hand-off occurs with a
position update at current time Tnow while the previous update
time is Tlast, and every hand-off end time earlier than Tnow is
already treated with the above algorithm, we use the following
algorithm to update the cost parameters:
1) Register the new hand-off end time and the correspond-
ing Ls in the queue.
2) Increment Nrl by one.
3) Nrt is recalculated by the MS’s current position and
added by {Nrl · (2N + 1)}.
4) Let Ll equals to the summation of all Ls’s in queue.
5) Ttv is then updated accordingly.
6) The transaction counts are calculated by the Poisson
process given λ and time duration (Tnow − Tlast), and
then set new Tlast = Tnow for the next update.
Since the variation of the geographical parameters is negli-
gible along the 20 meters (or less) long path, every transaction
in an update interval is charged with identical Ttv to reduce
the computational complexity. Note that Ttv updated at time
T is applied to the transactions which occur after T, while
the transaction counts calculated at T are placed in the time
interval ended at T.
97
MOBILITY 2011 : The First International Conference on Mobile Services, Resources, and Users
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-164-9

Fig. 4.
Simulation results of different N of both cell conﬁgurations given
Trl = 0.5s Trt = 20ms, and Tx = 600s, λ = 1.0.
G. Traverse Time Accounting
The average Ttv per transaction is calculated at the end of
100,000 independent simulations, each lasting 86400 seconds.
The simulation results of variable N, Trt, Trl, Tx, and λ for
both R = 2000m or 500m, are presented in the following
section.
V. SIMULATION RESULTS
We ﬁrst simulate how the size of LSAs affects Ttv given
nominal parameters, which are Trt = 20ms, Trl = 500ms,
Tx = 600s, and λ = 1.0. The simulation results of both R
settings are shown in Fig. 4.
As we can see in Fig. 4, both Ttv’s bear a strong re-
semblance in shape to the counterpart in [12] despite the
signiﬁcantly different mobility models. Ttv’s are high in small
LSA conﬁgurations due to the higher hand-off occurrence rate.
As N increases, Ttv’s ﬁrst descend, level for several N’s, and
then linearly ascend. The descending for low N’s is due to
the reduction of hand-off occurrences. The smooth ascending
for higher N’s is caused by the higher average number of
the nodes along the backhaul route and the longer average
transmission distance while the hand-off occurrence rate is
too low to matter. The ﬂat bottom in between is the result of
the two effects competing with each other.
Note although we compare two cell conﬁgurations, R =
2000m and R = 500m, in the same ﬁgure, each LSA of the
former one is in fact 4 times larger than of the latter one.
Therefore, each MS encounters much fewer hand-offs in the
large cell conﬁguration than in the small cell one. We can also
observe slightly steeper ascending for higher N’s in the large
cell conﬁguration than in the small cell one due to the higher
propagation delay brought by the longer wireline and wireless
transmission distances.
We can conclude that in this case, setting N = 4 for the
large cell conﬁguration, and N = 8 for the small cell one, are
optimal in reducing average Ttv and keeping the total number
of the servers low, which also means lower deployment and
maintenance cost.
Since the above quantitative conclusion is only applicable
in this set of parameters, we adjust each parameter in the
Fig. 5.
Simulated Ttv’s of both cell conﬁgurations given Trl = 0.2s, 0.5s,
0.8s, 1.1s and Trt = 20ms, Tx = 600s, λ = 1.0.
Fig. 6.
Simulated Ttv’s of both cell conﬁgurations given Trt = 20ms,
40ms, 60ms and Trl = 500ms, Tx = 600s, λ = 1.0.
nominal set and compare the results to see how it affects Ttv’s
as functions of N in the following subsections.
A. Effect of Trl
Trl only participates in hand-off conditions. In this simula-
tion, we set Trl to 200ms, 800ms, and 1100ms, and see how it
affects both Ttv’s. Both simulated Ttv’s in large and small cell
conﬁgurations as functions of N and Trl given Trt = 20ms,
Tx = 600s, λ = 1.0 are shown in Fig. 5.
As we can see in Fig. 5, higher Trl signiﬁcantly increases
Ttv’s in small LSA conﬁgurations due to the higher occurrence
rate of hand-offs. As N increases, Ttv’s in each cell conﬁgura-
tion given different Trl’s have a tendency to converge together
since the hand-off occurrence rate is dramatically reduced and
thus renders the effect of Trl insigniﬁcant. In the large cell
conﬁguration, Ttv’s converge more signiﬁcantly and earlier
due to the extremely low hand-off occurrence rate.
B. Effect of Trt
Higher Trt ampliﬁes the inﬂuence of transmission distance.
The simulated Ttv’s in both cell conﬁgurations as functions of
N and Trt given Trl = 0.5s, Tx = 600s, λ = 1.0 are shown
in Fig. 6.
Fig. 6 shows the comparison of Ttv’s of both cell conﬁgura-
tions as functions of N given Trt = 20ms, 40ms, and 60ms.
Besides the resemblance in shape to the counterpart in [12],
we can also notice that Trt is a more decisive factor for the
large cell conﬁguration’s performance due to the low hand-off
98
MOBILITY 2011 : The First International Conference on Mobile Services, Resources, and Users
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-164-9

Fig. 7.
Simulated Ttv of both cell conﬁgurations given Tx = 300s, 600s,
900s, 1200s and Trt = 20ms, Trl = 0.5s, λ = 1.0.
occurrence rate and the long average communication distance
in each LSA. Even N = 1 can be preferable if Trt is greater
than 60ms in the large cell conﬁguration.
C. Effect of Tx
Tx only affects the cost brought by hand-offs. A higher Tx
may mean a larger snapshot ﬁle, a longer hand-off initializa-
tion time, or a longer queuing delay. How Tx affects Ttv is
represented in Fig. 7.
Similar to the counterpart in [12], Ttv’s of each cell
conﬁguration as functions of N given different Tx’s are
virtually parallel for high N to each other and show very little
tendency to converge as N increases. However, slightly higher
optimal N brought by higher Tx in both conﬁgurations is still
observable.
D. Effect of λ
Although we have shown that user input rate λ was not a
relevant parameter in [12], we still simulate Ttv’s as functions
of N given different user input rates λ in M¨obius County. We
again conﬁrm that the property doesn’t change in the UMTS
rural vehicular mobility model.
However, we should keep in mind that the user experience
depends more on the interactivity of the application software
than on the absolute response latency.
VI. CONCLUSION AND FUTURE WORK
In this paper, we complete the performance evaluation of
the proposed application virtualization conﬁguration and the
corresponding hand-off protocol by applying the UMTS rural
vehicular mobility model. In addition to the model M¨obius
City which we proposed in our previous work [12], we propose
M¨obius County using a similar concept to enable MSs to
move in the test environment based on the UMTS rural
vehicular mobility model indeﬁnitely without dealing with any
boundary condition. We simulate the network delay as a result
of MSs’ movements and the occurrences of VM-level hand-
offs in M¨obius County given variable sizes of LSAs, server
relay latencies, nodal relay costs, and snapshot transmission
durations.
M¨obius County, combined with M¨obius City, can provide
a performance preview of network infrastructures aimed at
improving mobile application virtualization services in large
scale unknown environments. We can also design a benchmark
framework speciﬁc for distributed application virtualization
services based on the proposed simulation environments.
In this paper, we employ deterministic infrastructure delay
parameters and a simple usage model to evaluate the per-
formance. We will introduce more sophisticated usage and
infrastructure delay model to facilitate more precise mobile
application virtualization service simulations. Furthermore,
besides the beneﬁt of lowering the average response latency,
the distributed application virtualization service conﬁguration
and the hand-off protocol can also be applied to load balancing
and fault tolerance for better resource management and service
robustness. We will investigate these potential applications in
the future as well.
REFERENCES
[1] Sunwook Kim et al., “On-demand software streaming system for
embedded system”, WiCOM 2006 International Conference on
Wireless Communications, Networking and Mobile Computing,
22-24 Sept. 2006, pp. 1-4.
[2] EMA Report: “AppStream: Transforming On-premise Software
for SaaS Delivery - without reengineering”
[3] Joeng Kim; Baratto, R.A.; Nieh, J., “An application streaming
service for mobile handheld devices”, SCC’06 IEEE International
Conference on Services Computing, Sept. 2006, pp. 323-326.
[4] VMware Inc., “VMware ThinApp: Agentless Application Virtu-
alization Overview”.
[5] Ana Fernandez Vilas et al., “Providing web services over DVB-H:
mobile web services”, IEEE Transactions on Consumer Electron-
ics, Vol. 53, No, 2, May 2007, pp. 644-652.
[6] VMware Inc., “VMware MVP (Mobile Virtualization Plat-
form)”, http://www.vmware.com/products/mobile/overview.html,
Retreived 7 Aug. 2011.
[7] Apple Inc., “App Store Review Guidelines for iOS Apps”, 2.7
and 2.8, http://developer.apple.com/appstore/guidelines.html, Re-
trieved 9 Sep. 2010.
[8] Google Inc., “Android Market Developer Distribution Agree-
ment”,
4.5,
http://www.android.com/us/developer-distribution-
agreement.html, Retrieved 22 Feb. 2011.
[9] Marcin Bienkowski et al., “Competitive Analysis for Service
Migration in VNets”, in Proc. 2nd ACM SIGCOMM Workshop
on Virtualized Infrastructure Systems and Architectures, 2010, pp.
17-24.
[10] Dushyant Arora et al., “On the beneﬁt of virtualization: strate-
gies for ﬂexible server allocation”, Hot-ICE’11 Proceedings of
the 11th USENIX conference on Hot topics in management of
internet, cloud, and enterprise networks and services, 2011.
[11] Chung-Ping Hung and Paul S. Min, “Infrastructure arrange-
ment for application virtualization services”, I2TS 2010 The 9th
International Information and Telecommunication Technologies
Symposium, 13-15 Dec. 2010, Vol. 1, pp. 78-85.
[12] Chung-Ping Hung and Paul S. Min, “Service area optimization for
application virtualization using UMTS mobility model”, ICOMP
2011 International Conference on Internet Computing, 18-21 July
2011, pp. 128-134.
[13] L. P. Deutch and B. W. Lampson, “SDS 930 Time-sharing System
Preliminary Reference Manual”, Doc. 30.10.10, Project Genie,
Univ. Cal. at Berkeley, April 1965.
[14] VMware Inc., “Virtual Desktop Infrastructure”.
[15] ETSI. “Universal Mobile Telecommunications System (UMTS);
selection procedures for the choice of radio transmission tech-
nologies of the UMTS (UMTS 30.03, version 3.2.0)”. Techni-
cal report, European Telecommincation Standards Institute, Apr.
1998.
99
MOBILITY 2011 : The First International Conference on Mobile Services, Resources, and Users
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-164-9

