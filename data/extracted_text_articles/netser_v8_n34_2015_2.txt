Analytic Method for Evaluation of the Weights of a Robust
Large-Scale Multilayer Neural Network
Mikael Fridenfalk
Department of Game Design
Uppsala University
Visby, Sweden
mikael.fridenfalk@speldesign.uu.se
Abstract—The multilayer feedforward neural network is presently
one of the most popular computational methods in computer
science. However, the current method for the evaluation of its
weights is performed by a relatively slow iterative method known
as backpropagation. According to previous research on a large-
scale neural network with many hidden nodes, attempts to use
an analytic method for the evaluation of the weights by the
linear least square method showed to accelerate the evaluation
process signiﬁcantly. Nevertheless, the evaluated network showed
in preliminary tests to fail in robustness compared to well-trained
networks by backpropagation, thus resembling overtrained net-
works. This paper presents the design and veriﬁcation of a
new method that solves the robustness issues for such a neural
network, along with MATLAB code for the veriﬁcation of key
experiments.
Keywords–analytic; big data; FNN; large-scale; least square
method; multilayer; neural network; robust; sigmoid.
I.
INTRODUCTION
As an extension of an earlier work presented in ADV-
COMP 2014 [1], this paper reconﬁrms the initial inference
by the presentation of a new layer of experiments. As a brief
introduction, the artiﬁcial neural network constitutes one of the
most useful and popular computational methods in computer
science. The most well-known category is the multilayer Feed-
forward Neural Network, in this paper abbreviated as FNN,
where the weights are estimated by an iterative training method
called backpropagation [2], [3]. Although backpropagation is
relatively fast for small networks, it is rather slow for large
ones, given the computational power of modern computers [4],
[5]. To accelerate the training speed of FNNs, many approaches
have been suggested based on the least square method [6].
Although the presentation on the implementation, as well as of
the data on the robustness of these methods may be improved,
the application of the least square method seems to be a
promising path to investigate [7], [8].
What we presume to be required for a new method to
replace backpropagation in such networks, is not only that it
is efﬁcient, but also that it is superior compared to existing
methods and is easy to understand and implement. Therefore,
the goal of this work has been to investigate the possibility to
ﬁnd a robust analytic solution (i.e., with good generalization
abilities compared with a well-trained network using backprop-
agation, but without any iterations involved), for the weights of
an FNN, which is easily understood and may be implemented
relatively effortlessly, using a mathematical application such
as MATLAB [9].
x1
x2
hk
x3
1
vk1
vk2
vk3
vk4
Figure 1. An example with three input nodes (M = 3), hk = S(vku) =
S(vk1x1 + vk2x2 + vk3x3 + vk4), using a sigmoid activation function S.
x1
h1
y1
x2
h2
y2
x3
y3
y4
1
1
V
W
Figure 2. A vectorized model of a standard FNN with a single hidden layer,
in this example with M = 3 input nodes, H = 2 hidden nodes, K = 4
output nodes and the weight matrices V and W, using a sigmoid activation
function for the output of each hidden node. In this model, the biases for the
hidden layer and the output layer correspond to column M + 1 in V versus
column H + 1 in W.
II.
OVERVIEW
As an overview of the main structure of this paper, in
Section III, the history of artiﬁcial neural networks is brieﬂy
reiterated. In Sections IV-V a recap is made of the theory
behind the fundamentals of the analytic method presented
139
International Journal on Advances in Networks and Services, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/networks_and_services/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

in this paper. In Section VI, the derivation of an upgrade
is reiterated for the improvement of the robustness of the
proposed analytic method. In Section VII, the experimental
setup is brieﬂy described for the experiments presented in
this paper, using a mathematical engine based on C++. In
the results sections, Section VIII presents experiments for
the evaluation of the original analytic solution we proposed
in [1], in this paper labeled as the initial experiments. Sec-
tion IX presents an evaluation of an upgrade of the original
proposal, denoted as diagonal reinforcement [10], and labeled
as the primary experiments. Section X presents a MATLAB-
based version of the experiments presented in Figures 4-9,
thereby additionally validating the experiments presented in
Section IX, thus fulﬁlling the initial goal of this project, as
formulated in [1], and the introduction section.
III.
BACKGROUND
The history of artiﬁcial neural networks is considered to
have started in 1943 [11]. The 1950s and 1960s is often
regarded as a golden age for neural networks, marked among
other things by the development of the ﬁrst successful neu-
rocomputer, Mark I Perceptron [12]. However, towards the
end of the 1960s, this age was turned into an ice age after
the criticism of the perceptron model [13]. In the following
decades the neural networks research slowly recovered by the
introduction of multilayer networks and the introduction of
backpropagation [12], [14]. Backpropagation is a slow iterative
method for the evaluation of the weights of multilayer neural
networks. During the last decades a large number of neural
networks have been suggested, but presently the evaluation of
the weights of the most well-known and widely used category,
namely the FNN, is still based on backpropagation, which so
far has made many mathematicians to avoid this ﬁeld, since
an analytic method for the evaluation of the weights of robust
FNNs is considered today to be missing.
IV.
RELATED WORK
In a previous work [1], an analytic solution was proposed
for the evaluation of the weights of a textbook FNN. This
solution was found to be signiﬁcantly much faster, and for
H = N − 1 (where H denotes the number of hidden nodes
and N, the number of training points), more accurate than
solutions provided by backpropagation, but at the same time
signiﬁcantly less robust (e.g., more noise sensitive) compared
to a well-trained network using backpropagation, why the ana-
lytic solution was in this context considered to lack robustness
for direct use.
However, further experiments showed that even small mea-
sures, such as an increase in the input range of the network
by the duplication of the training set, with the addition of
perturbation, led to signiﬁcant improvement of the robustness
of the evaluated network. As a systematic attempt to address
the issue of robustness, this paper presents the derivation,
implementation and further veriﬁcation of the new method
proposed in [10], based on the expansion of the training set of
an FNN, with addition of perturbation, but in practice without
any impact on the execution speed of the original method
introduced in [1].
V.
AN ANALYTIC SOLUTION
In [1], a textbook FNN is vectorized based on a sigmoid
activation function S(t) = 1/(1 + e−t). The weights V and
W of such system (often denoted as WIH versus WHO), may
be represented by Figures 1-2. In this representation, deﬁned
here as the normal form, the output of the network may be
expressed as:
y = Wh = W

S(Vu)
1

, u =

x
1

(1)
where x = [x1 x2 . . . xM]T denotes the input signals,
y = [y1 y2 . . . yK]T the output signals, and S, an element-
wise sigmoid function. In this paper, a winner-take-all classiﬁ-
cation model is used, where the ﬁnal output of the network
is the selection of the output node that has the highest
value. Since the sigmoid function is constantly increasing and
identical for each output node, it can be omitted from the
output layer, as max(y) results in the same node selection
as max(S(y)). Further on, presuming that the training set is
highly fragmented (the input-output relations in the training
sets were in our experiments established by a random number
generator), the number of hidden nodes is in many experiments
set to H = N − 1. Deﬁning a batch of input signals, e.g., a
training set, the input matrix U may be expressed as:
U =


x11
x12
· · ·
x1N
x21
x22
· · ·
x2N
...
...
...
...
xM1
xM2
· · ·
xMN
1
1
· · ·
1


(2)
where column vector i in U, corresponds to training point i,
column vector i in Y0 (target output value) and in Y (actual
output value). Further, deﬁning H of size N ×N, as the batch
values for the hidden layer, given a training set of input and
output values and M + = M + 1, the following relations hold:
U =

X
1T

: [M + × N]
(3)
H =

S(VU)
1T

: [N × N]
(4)
Y = WH : [K × N]
(5)
To evaluate the weights of this network analytically, we
need to evaluate the target values (points) of H0 for the hidden
layer. In this context, the initial assumption is that any point is
feasible, as long as it is unique for each training set. Therefore,
in this model, H0 is composed of random numbers. Thus,
the following evaluation scheme is suggested for the analytic
solution of the weights of such network:
VT = (UUT )−1UHT
0 : [M + × H]
(6)
WT = (HHT )−1HYT
0 : [N × K]
(7)
where a linear least square solution is used for the evaluation
of each network weight matrix. Such equation is nominally
expressed as:
Ax = b
(8)
with the least square solution [6]:
x = (AT A)−1AT b
(9)
Since the mathematical expressions for the analytic solu-
tion of the weights of a neural network may be difﬁcult to
140
International Journal on Advances in Networks and Services, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/networks_and_services/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

follow, an attempt has been made in Figure 3 to visualize
the matrix operations involved. While a nonlinear activation
function (such as the sigmoid function) is vital for the success
of such network, the inclusion of a bias is not essential. It is
for instance possible to omit the biases and to replace H0 with
an identity matrix I. Such a conﬁguration would instead yield
the following formula for the evaluation of V and H (where
UI can further be simpliﬁed as U):
VT = (UUT )−1UI : [M + × N]
(10)
H = S(VU) : [H × N]
(11)
VI.
DIAGONAL REINFORCEMENT
To recap the theory on diagonal reinforcement, as proposed
in [10], we expand the input training set U in (2), by the
addition of perturbation to the input signal, given the deﬁnition
Θ = UUT , with θM +M + = N in:
Θ =


θ11
θ12
. . .
θ1M
θ1M +
θ21
θ22
. . .
θ2M
θ2M +
...
...
...
...
...
θM1
θM2
. . .
θMM
θMM +
θM +1
θM +2
. . .
θM +M
N


(12)
Further, an extended matrix ˜U is introduced, where:
˜U =
 ˜U1
˜U2
. . .
˜UN

(13)
with:
Uj =


u1j + ∆
u1j − ∆
u1j
u1j
u2j
u2j
u2j + ∆
u2j − ∆
...
...
...
...
uMj
uMj
uMj
uMj
1
1
1
1
· · ·
u1j
u1j
· · ·
u2j
u2j
...
...
...
· · ·
uMj + ∆
uMj − ∆
· · ·
1
1


(14)
and where ∆ is deﬁned as the amplitude of the perturbation.
Thus, for the right hand side of (8), ˜Θ = ˜U ˜UT , or more
explicitly:
˜Θ = 2M


d1
θ12
. . .
θ1M
θ1M +
θ21
d2
. . .
θ2M
θ2M +
...
...
...
...
...
θM1
θM2
. . .
dM
θMM +
θM +1
θM +2
. . .
θM +M
N


(15)
with di = θii + α, where α = N∆2/M, or:
˜Θ = 2M [Θ + diag(α, α, . . . , α, 0)]
(16)
where diag(d1, d2, . . . , dM +) denotes a diagonal matrix of size
M + ×M + (where M + = M +1), with the diagonal elements
d1, d2, . . . , dM +. Similarly, for the left hand side of (8), Ψ and
Λ are deﬁned as:
HT
0 = Ψ =


ψ11
ψ12
. . .
ψ1H
ψ21
ψ22
. . .
ψ2H
...
...
...
...
ψN1
ψN2
. . .
ψNH


(17)
Λ = UHT
0 =


λ11
λ12
. . .
λ1H
λ21
λ22
. . .
λ2H
...
...
...
...
λM +1
λM +2
. . .
λM +H


(18)
and thereby, Ψ and Ψj as:
Ψ =


Ψ1
Ψ2
...
ΨN


(19)
Ψj =


ψj1
ψj2
. . .
ψjH
ψj1
ψj2
. . .
ψjH
...
...
...
...
ψj1
ψj2
. . .
ψjH


(20)
with ˜Λ = ˜UΨ:
˜Λ = 2MΛ
(21)
This transforms (8) into:
˜U ˜UT X = ˜UΨ
(22)
or:
˜ΘX = ˜Λ
(23)
Thus:
2M [Θ + diag(α, α, . . . , α, 0)] X = 2MΛ
(24)
Given the matrix equation:
AX = B
(25)
since, given a scalar c ∈ R:
c · (AX) = (c · A)X = c · B
(26)
thereby:
[Θ + diag(α, α, . . . , α, 0)] X = Λ
(27)
This yields thus, the ﬁnal expression:

UUT + diag(α, α, . . . , α, 0)

X = UHT
0
(28)
Hence, the expansion of U into a perturbation matrix ˜U of size
M + × 2MN, and similarly of HT
0 into a matrix Ψ of size
2MN×H, is according to (28) equivalent to the reinforcement
of the diagonal elements of the square matrix Θ = UUT ,
by the addition of a scalar α = N∆2/M to each diagonal
element, except for the last one, which as a consequence of
the use of bias in the network, is left intact.
VII.
EXPERIMENTAL SETUP
The experiments presented in Sections VIII-IX, are based
on a minimal mathematical engine that was developed in C++,
with the capability to solve X in a linear matrix equation
system of the form AX = B, where A, B, and X denote
matrices of appropriate sizes, since it is computationally more
efﬁcient to solve a linear equation system directly, than by
matrix inversion. In this system, the column vectors of X are
evaluated using a single Gauss-Jordan elimination cycle [6],
where each column vector xi in X corresponds to the column
vector bi in B. Backpropagation was in these experiments,
for high execution speed (and a fair comparison with the
new methods), also implemented in C++, using the code
141
International Journal on Advances in Networks and Services, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/networks_and_services/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

h ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗
i
VT
=


h ∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
1 1 1 1 1 1
i


∗ ∗ 1
∗ ∗ 1
∗ ∗ 1
∗ ∗ 1
∗ ∗ 1
∗ ∗ 1




−1
h ∗ ∗ ∗
∗ ∗ ∗
∗ ∗ ∗
i
(UUT )−1


h ∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
1 1 1 1 1 1
i


∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗




h ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗
i
UHT
0


∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
1 1 1 1 1 1


H
=


S





" ∗ ∗ ∗
∗ ∗ ∗
∗ ∗ ∗
∗ ∗ ∗
∗ ∗ ∗
#
V
h ∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
1 1 1 1 1 1
i
U





[ 1 1 1 1 1 1 ]
1T




∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗


WT
=




∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
1 1 1 1 1 1




∗ ∗ ∗ ∗ ∗ 1
∗ ∗ ∗ ∗ ∗ 1
∗ ∗ ∗ ∗ ∗ 1
∗ ∗ ∗ ∗ ∗ 1
∗ ∗ ∗ ∗ ∗ 1
∗ ∗ ∗ ∗ ∗ 1




−1
(HHT )−1




∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
1 1 1 1 1 1


H


∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗


YT
0




∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗
∗ ∗ ∗ ∗


HYT
0
 ∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗

Y
=
 ∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗

W


∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
1 1 1 1 1 1


H
Figure 3. A visual representation of the evaluation of weights V and W by the analytic method presented in the initial work [1], and the actual output Y, in
this example as a function of six training points, N = 6, the training input and output sets U (for a simpliﬁed notation, whenever the relation U = U0 is
implicit) and Y0, with two inputs, M = 2, four outputs, K = 4, and ﬁve hidden nodes, H = N − 1 = 5. In this ﬁgure, an asterisk denotes a ﬂoating-point
number. To facilitate bias values, certain matrix elements are set to one.
presented in [15] as a reference. To measure the efﬁciency of
the new method in [1], compared with the standard method
(backpropagation), the standard textbook deﬁnition for the
mean-squared error was used:
ϵ =
1
2N
K
X
i
N
X
j
(aij − yij)2
(29)
where aij denotes an element in Y0 (target value), and yij
the corresponding element in Y (actual value). Although the
new method, as in [1], does not intrinsically beneﬁt from such
deﬁnition (since there is no need here for the differentiation
of the mean-squared error, which however, is essential for
backpropagation), to simplify comparison in Tables I-V, the
same deﬁnition of the mean-squared error was also applied to
the new method.
VIII.
INITIAL EXPERIMENTS
The experimental results presented in this section, as shown
in Tables I-V, are based on ten individual experiments for
each parameter setting using different random seeds, where
¯t denotes average execution time, using a single CPU core on
a modern laptop computer (the same computer was used for all
experiments presented in this paper), ¯ϵ, the average value of the
mean-squared errors, and ˜ϵ, the median value. The success rate,
¯s, is similarly based on an average value. Since the variation
of the results is large between the experiments, the average
values are in general larger than the median values. If the
number of experiments per parameter setting is increased, the
average value tends to increase as well.
On a note of preliminary experiments with respect to
robustness, regarding the generalization abilities of the net-
work, the original method (i.e., without the application of
diagonal reinforcement) showed to steeply lose accuracy with
the addition of noise to the input values, compared with a
network trained by backpropagation. This shows that although
the results seem to be in order according to Tables I-V,
the original method lacks robustness for direct use. However,
further experiments showed that even small measures, such as
an increase in the input range of the network by the duplication
142
International Journal on Advances in Networks and Services, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/networks_and_services/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Table I. Initial experiments (Tables I-V) – Backpropagation with H = N − 1 and average success rate ¯s.
M
N
H
K
Iterations
¯t
¯ϵ
˜ϵ
¯s (%)
5
20
19
5
104
46.6 ms
0.0671
0.0620
93.0
5
20
19
5
106
4.39 s
0.0175
4.64 · 10−5
96.5
10
50
49
10
104
182 ms
0.116
0.114
83.2
10
50
49
10
106
18.1 s
0.0680
0.0650
86.4
20
50
49
20
104
333 ms
0.0394
0.0392
94.6
20
50
49
20
106
33.3 s
0.0170
0.0200
96.6
40
100
99
40
104
1.27 s
0.0671
0.0670
88.9
40
100
99
40
106
127 s
0.0180
0.0200
96.4
Table II. New method with H = N − 1.
M
N
H
K
¯t
¯ϵ
˜ϵ
¯s (%)
5
20
19
5
332 µs
3.34 · 10−8
2.00 · 10−13
100.0
10
50
49
10
3.74 ms
6.99 · 10−9
2.26 · 10−12
100.0
20
50
49
20
4.79 ms
2.68 · 10−13
5.93 · 10−16
100.0
40
100
99
40
36.7 ms
3.93 · 10−12
2.33 · 10−13
100.0
Table III. Backpropagation with H < N − 1 and 104 iterations.
M
N
H
K
¯t
¯ϵ
˜ϵ
¯s (%)
5
20
5
5
14.7 ms
0.130
0.125
86.5
10
50
10
10
43.1 ms
0.227
0.237
71.6
20
50
20
20
144 ms
0.0624
0.0599
94.2
40
100
40
40
531 ms
0.115
0.115
84.8
Table IV. New method with H < N − 1.
M
N
H
K
¯t
¯ϵ
˜ϵ
¯s (%)
5
20
5
5
59 µs
0.281
0.289
58.5
10
50
10
10
370 µs
0.350
0.350
50.0
20
50
20
20
1.30 ms
0.279
0.277
91.8
40
100
40
40
9.24 ms
0.290
0.290
98.5
Table V. Selective use of bias for the new method, with M = 40, N = 100, H = 99, and K = 40.
Hb
Yb
¯t
¯ϵ
˜ϵ
¯s (%)
No
No
35.7 ms
0.00514
0.00513
100.0
No
Yes
36.4 ms
1.35 · 10−12
1.91 · 10−14
100.0
Yes
No
36.2 ms
0.00544
0.00534
100.0
Table VI. Average execution time, ¯tbp (backprop.), ¯tnew (initial), ¯tnew+ (primary), ¯tnew* (veriﬁcation).
Figure
M
N
H
K
¯tbp
¯tnew
¯tnew+
¯tnew*
4
10
25
24
10
92.5 ms
688 µs
668 µs
326 µs
5
20
50
49
20
332 ms
4.84 ms
4.86 ms
559 µs
6
40
100
99
40
1.27 s
37.0 ms
37.1 ms
1.47 ms
7
10
25
10
10
43.3 ms
212 µs
202 µs
175 µs
8
20
50
20
20
144 ms
1.33 ms
1.31 ms
306 µs
9
40
100
40
40
535 ms
9.35 ms
9.38 ms
631 µs
143
International Journal on Advances in Networks and Services, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/networks_and_services/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

∆
Average Success Rate ¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100% •
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
×
×
×
×
×
×
Figure 4. Backpropagation (×), initial analytic method (◦), versus new method
using diagonal reinforcement (•), with M = 10 (input nodes), N = 25
(training points), H = 24 (hidden nodes), and K = 10 (output nodes).
∆
¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100% •
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
×
×
×
×
×
×
Figure 5. M = 20, N = 50, H = 49, and K = 20.
∆
¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100% •
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
×
×
×
×
×
×
Figure 6. M = 40, N = 100, H = 99, and K = 40.
∆
¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100%
•
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
×
×
×
×
×
×
Figure 7. M = 10, N = 25, H = 10, and K = 10.
∆
¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100%
•
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
×
×
×
×
×
×
Figure 8. M = 20, N = 50, H = 20, and K = 20.
∆
¯s
0
0.1
0.2
0.3
0.4
0.5
0%
20%
40%
60%
80%
100% •
•
•
•
•
•
◦
◦
◦
◦
◦
◦
•
•
•
•
•
•
×
×
×
×
×
×
Figure 9. M = 40, N = 100, H = 40, and K = 40.
144
International Journal on Advances in Networks and Services, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/networks_and_services/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

of the training set with the addition of perturbation and a
more conscious design of H0, by for instance the clustering
of the random values as a function of the output values, or for
layers with few hidden nodes, the binary encoding [16] of H0,
led to signiﬁcant improvements of the robustness of the new
method. Therefore, this was an indication at an early stage of
the development of the method presented in this paper, that
these robustness issues could be solved, and in this context,
without any signiﬁcant impact to the computational speed of
the initial version of the new method (i.e., without diagonal
reinforcement).
IX.
PRIMARY EXPERIMENTS
The experimental results presented in this section, as shown
in Figures 4-9, are in similarity with previous section based
on a mathematical engine developed in C++, here in addition
examining the effects of diagonal reinforcement, as described
in Section VI, measuring average success rate, and Table VI,
measuring execution speed. Each experiment is based on ten
individual experiments (with different random seeds), using a
single CPU core. In Table VI, ¯tbp denotes the execution time for
backpropagation based on 10000 iterations, which applies to all
backpropagation experiments presented in this paper. Similarly,
¯tnew denotes the execution time for the original analytic method
in [1], and ¯tnew+, the execution time for the new method, using
diagonal reinforcement.
In all the experiments presented in this paper, the input
values to the FNN is based on the integers {0, 1, 2}, and the
output values of a binary number, {0, 1}. To avoid inconsisten-
cies (or repetition) in any training set, no identical input vectors
are permitted. For the addition of noise, a random value (with
uniform distribution) in the range of ±∆, was added to each
input value. However, although according to our derivation
of (28), α = N∆2/M, α had in practice to be retuned to
105 · N∆2 for good results in the experiments in Figures 4-6
(H = N − 1), and to 104 · N∆2 in Figures 7-9 (few hidden
nodes).
X.
MATLAB VERIFICATION EXPERIMENTS
This paper presents a veriﬁcation of previously presented
results in [10], but here based on MATLAB. As initially
inferred in [1], the goal in this work is to develop a method
that is efﬁcient, superior compared to existing methods, and
in addition easy to understand and implement using a mathe-
matical application such as MATLAB.
Thus, to verify the results presented in Figures 4-9, which
were based on our own computational engine developed in
C++, we hereby present a MATLAB version of the same so-
lution, as presented in Figure 10 (and veriﬁed by the auxiliary
MATLAB code in Figures 11-13), corresponding to the same
experiments as in Figures 4-9.
The MATLAB code in Figures 10-13, produces numerical
results that coincide relatively accurately (considering the use
of random numbers) with previous evaluations using the engine
based on C++. The execution speed proved to be faster, as
shown in Table VI, compared to the C++ engine. According
to this table, while for smaller matrix sizes, the execution
speed is only marginally faster for MATLAB (deﬁned by
execution time, ¯tnew*), in comparison with the C++ engine
(¯tnew+), however, for larger matrices, MATLAB is signiﬁcantly
faster. One reason is that while MATLAB is multithreaded,
the C++ engine uses only a single thread. In addition, while
the core computational engine of MATLAB is expected to be
implemented by assembly code, we used regular C++ code,
prioritizing code readability. MATLAB is in addition expected
to use smart tricks to accelerate matrix operations, including
using CPU cores very efﬁciently.
However, as a note, although MATLAB in average showed
to be faster than our C++ engine, the application proved to
be initially slower at a startup phase, running the MATLAB
master script in Figure 13. This script is thus, for a more
representative time measurement, recommended to be executed
twice.
Since the MATLAB code presented in this paper constitutes
the actual results of this work, we have below included a
brief explanation of this code. To facilitate direct copy and
paste of this code from a PDF version of this document
into the suggested ﬁlenames (e.g., EvalVW.m for the code
in Figure 10), the code have been placed in a single column
environment and without line numbers. In addition, on two
instances, a comma is placed after an end-statement, since copy
and paste may place two end-statements after one another,
which presently generates an error in MATLAB. The commas
are not visible in the code (due to white font color), but appear
after a paste operation into an m-ﬁle.
A. EvalVW.m
The function EvalVW constitutes the core results of this
paper. The MATLAB function rand(h,n) creates a matrix
of size H × N, consisting of random numbers within the
interval of 0 to 1. Regarding the evaluation of Q, note that only
the ﬁrst M diagonal elements are affected by the application
of diagonal reinforcement. In the evaluation of W, since
HHT showed in our experiments to often be a near singular
matrix, MATLAB will by default issue a warning statement at
execution, unless the warning messages are temporarily turned
off and on again by the commands warning('off','all')
and warning('on','all').
B. GenTrainingSet.m
This function calculates a training set, i.e., a complete
set of input and output signals for the training of an FNN.
The generation of the input matrix U0 is straightforward and
concise, yielding unique columns of values in {0, 1, 2}, based
on random numbers. As a note on the generation of Y0, which
is also based on random numbers, this loop-based solution
showed to be both straightforward and fast.
C. Exp.m
This inner loop experimental function is used for the
complete evaluation of a single point in Figures 4-9. As shown
here, N1 denotes the number of FNNs that are generated and
tested, and N2, the number of input/output tests performed on
each evaluated FNN. On time measurements, since we are only
interested in the time it takes to evaluate the weights V and
W, the tic (reset and start) and toc (stop) MATLAB timer
commands, are only placed around the function EvalVW.
To obtain the exact same results each time the function
Exp is called, using the same input parameters, the function
rng is used to set the seed of the random number generator in
MATLAB. This is actually not necessary at this stage of the
145
International Journal on Advances in Networks and Services, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/networks_and_services/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

code development, but is often useful at a development stage
of a system based on random numbers.
As a potential pitfall, note that the disturbance added to
U, is only allowed to affect the ﬁrst M rows of this matrix
(i.e., the input signals), and thus not the bias row, which by
deﬁnition is always set to a column vector of ones.
The logical expression I0 == I, yields ﬁnally a value of
one, for each element of I0 that is equal to the same element
in I, else zero. This expression is used for the evaluation of
the success rate of the network, by comparing the expected
output Y0, with the actual output Y.
D. NNX.m
The master script NNX.m (corresponding to the veriﬁcation
experiments presented in this paper) generates results that
correspond to Figures 4-9, except for backpropagation, which
was not implemented in MATLAB, but only in C++, since
while matrix operations are efﬁcient in MATLAB, the handling
of iterations and scalars are, as a general rule, slower than C++.
XI.
DISCUSSION
The strength of an analytic method is clear in the sense
that it could potentially expand the use of FNNs to a wider
range of applications. However, at this stage, there are also
some question marks that could require further study.
The ﬁrst of these is the question of the use of diagonal
reinforcement, compared with the initial method in [1], since
as shown in Figures 4-9, almost the same effect can be
produced by the reduction of the number of hidden nodes.
From this perspective, diagonal reinforcement serves mainly
to generalize the initial method as proposed in [1].
In addition, two observations that caught our attention
during the experiments were that the matrix HHT in many
cases was nearly singular in the evaluation of W, and that α
had to be set to a very high value in comparison to N∆2/M, to
have the intended effect. Thus, although the analytic method
presented in this paper seems to operate correctly for large-
scale FNNs, further analysis is recommended to shed light on
its inner workings.
XII.
CONCLUSION
The method presented in this paper provides for a robust
analytic solution of the weights of a large-scale FNN (i.e.,
as previously deﬁned, with good generalization abilities com-
pared with a well-trained network using backpropagation, but
without any iterations involved), which is signiﬁcantly faster
compared with backpropagation, yet straightforward to imple-
ment, using a mathematical application such as MATLAB.
Examples of a few application areas that could beneﬁt
from this method are artiﬁcial intelligence, economics, control
theory, and in general any ﬁeld dealing with big data for
decision making, such as cancer treatment and research.
REFERENCES
[1]
M. Fridenfalk, “The Development and Analysis of Analytic Method
as Alternative for Backpropagation in Large-Scale Multilayer Neural
Networks,” in ADVCOMP 2014: The Eighth International Conference
on Advanced Engineering Computing and Applications in Sciences,
Rome, Italy, August 2014, pp. 46–49.
[2]
S. Russell and P. Norvig, Artiﬁcial Intelligence: A Modern Approach.
3rd ed., Prentice Hall, 2009, pp. 727–736.
[3]
B. J. Wythoff, “Backpropagation Neural Networks: A Tutorial,” in
Chemometrics and Intelligent Laboratory Systems, vol. 18, no. 2, 1993,
pp. 115–155.
[4]
P. D. Wilde, Neural Networks Models: An Analysis. Springer, 1996,
pp. 35–51.
[5]
R. P. W. Duin, “Learned from Neural Networks,” in ASCI 2000,
Lommel, Belgium, 2000, pp. 9–13.
[6]
C. H. Edwards and D. E. Penney, Elementary Linear Algebra. Prentice
Hall, 1988, pp. 220–227.
[7]
B. Widrow and M. A. Lehr, “30 Years of Adaptive Neural Networks:
Perceptron, Madaline, and Backpropagation,” in Proceedings of the
IEEE, vol. 78, no. 9, 1990, pp. 1415–1442.
[8]
Y. Yam, “Accelerated Training Algorithm for Feedforward Neural
Networks Based on Least Squares Method,” in Neural Processing
Letters, vol. 2, no. 4, 1995, pp. 20–25.
[9]
“MATLAB, The MathWorks, Inc.” 2015, URL: http://www.mathworks.
com/ [accessed: 2015-11-24].
[10]
M. Fridenfalk, “Method for Analytic Evaluation of the Weights of
a Robust Large-Scale Multilayer Neural Network with Many Hidden
Nodes,” in ICSEA 2014: The Proceedings of the Ninth International
Conference on Software Engineering Advances, Nice, France, October
2014, pp. 374–378.
[11]
W. S. McCulloch and W. Pitts, “A Logical Calculus of the Ideas
Immanent in Nervous Activity,” in Bulletin of Mathematical Biophysics,
vol. 5, 1943, pp. 115–133.
[12]
N. Yadav, A. Yadav, and M. Kumar, “History of Neural Networks,” in
An Introduction to Neural Network Methods for Differential Equations.
Springer, 2015, pp. 13–15.
[13]
M. Minsky and S. Papert, Perceptrons. MIT Press, Cambridge, 1969.
[14]
P. J. Werbos, “Beyond Regression: New Tools for Prediction and Anal-
ysis in the Behavioral Sciences,” Ph.D. dissertation, Harvard University,
1974.
[15]
M. T. Jones, AI Application Programming.
2nd ed., Charles River,
2005, pp. 165–204.
[16]
F. Gray, “Pulse Code Communication,” U.S. Patent no. 2 632 058, 1947.
146
International Journal on Advances in Networks and Services, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/networks_and_services/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

function [V,W] = EvalVW(U0,Y0,h,d)
[mp,n] = size(U0);
H0 = rand(h,n);
Q = U0 * U0' + diag([d * ones(1,mp-1) 0]);
V = (Q\(U0 * H0'))';
H = [1./(1 + exp(-V*U0)); ones(1,n)];
warning('off','all');
W = ((H * H')\(H * Y0'))';
warning('on','all');
Figure 10. The results of this paper, condensed as the MATLAB function EvalVW.m.
function [U0,Y0] = GenTrainingSet(m,n,k)
ok = 0;
for i = 1:1000,
UX = randi([0 2],floor(1.5*n),m);
U0 = unique(UX,'rows','stable')';
if size(U0,2) >= n, ok = 1; break; end
end
if ok, U0 = U0(:,1:n); U0 = [U0; ones(1,n)];
else U0 = -1; end
Y0 = zeros(k,n);
v = randi([1 k],1,n);
for i = 1:n, Y0(v(i),i) = 1; end
Figure 11. Generation of a training set for testing, GenTrainingSet.m.
function [sr,time] = Exp(m,n,h,k,d,delta)
N1 = 10; N2 = 100; srsum = 0; t = 0;
for j = 1:N1
rng(1000*j);
[U0,Y0] = GenTrainingSet(m,n,k);
tic; [V,W] = EvalVW(U0,Y0,h,d); t = t + toc;
[~,I0] = max(Y0);
for i = 1:N2
rng(1000*j+i);
U = U0;
U = U + [2 * delta * (rand(m,n) - .5); zeros(1,n)];
H = [1./(1 + exp(-V*U)); ones(1,n)];
Y = W * H; [~,I] = max(Y);
srsum = srsum + 100 * sum(I == I0)/n;
end,
end
time = t/N1;
sr = srsum/(N1*N2);
Figure 12. Inner loop experiments Exp.m, called by NNX.m, based on 10 training sets, each performing 100 input/output tests.
147
International Journal on Advances in Networks and Services, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/networks_and_services/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

clear, clc
m = [10 20 40]';
p = [m 2.5*m 2.5*m-1 m; m 2.5*m m m]
diagReinfRel = 10^4*[10 10 10 1 1 1];
for r = 1:6
time = 0;
m = p(r,1); n = p(r,2); h = p(r,3); k = p(r,4);
dr = diagReinfRel(r) * n;
for j = 1:2
for i = 1:6
delta = .1*(i-1);
if j == 1, d = 0; else d = dr * delta * delta; end
[sr,t] = Exp(m,n,h,k,d,delta);
successRate(i,j) = sr;
time = time + t;
end,
end
successRate
aveTime(r) = time/12;
end
1000 * aveTime %milliseconds
Figure 13. The MATLAB master script NNX.m, the outer loop for the validation of the function EvalVW(U0, Y0, h, d) in Figure 10, and the experimental
results in Figures 4-9 (backpropagation excluded).
p =
10
25
24
10
20
50
49
20
40
100
99
40
10
25
10
10
20
50
20
20
40
100
40
40
ans =
100.0000
78.7560
58.7600
46.2760
38.2480
32.4600
100.0000
83.9840
82.4880
79.7600
76.4720
72.9680
ans =
100.0000
68.1140
43.2180
30.1940
22.9300
18.5740
100.0000
90.1140
88.9560
87.2680
85.0620
82.2980
ans =
100.0000
75.1150
51.6290
35.2760
24.9340
18.6990
100.0000
98.2980
97.9940
97.4470
96.5440
95.1400
ans =
83.2000
82.2400
80.3280
77.6960
74.3560
70.7360
83.2000
84.4800
82.6160
79.7920
76.4280
72.5720
ans =
91.4000
90.4880
89.0460
86.8360
83.8920
80.4020
91.4000
90.0060
88.9680
87.2620
85.0540
81.0420
ans =
98.5000
98.2320
97.8330
97.1770
96.0400
94.3460
98.5000
98.3130
98.0030
97.4440
96.5250
95.0490
ans =
0.3257
0.5587
1.4664
0.1749
0.3056
0.6307
Figure 14. Results from the execution of NNX.m by MATLAB (with some line breaks removed), corresponding to the code in Figure 13, Figures 4-9 (C++),
and the veriﬁcation column in Table VI.
148
International Journal on Advances in Networks and Services, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/networks_and_services/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

