 
 
Implementation of Interactive E-learning System Based on Virtual Reality  
SeungJoon Kwon, HyungKeun Jee 
 Electronics and Telecommunications Research Institute (ETRI)  
 
Daejeon, Republic of Korea 
Email: {kwonsj, hkjee}@etri.re.kr 
 
 
Abstract—VR (Virtual Reality)-based e-learning applications 
have become an important part of the educational program in 
kindergarten as well as in the National Children's Library 
(NLCY), in Republic of Korea. As kindergarten pupils and 
their teachers use the VR-based e-learning system, they can be 
more visually aware of ongoing course materials and more 
intuitively aware of getting quick response from event handling 
on screens. However, the existing VR-based e-learning system 
consists of complex equipment such as a large screen and beam 
projector to show the actual scene in 3D, more than two PCs, a 
forward camera to detect the pupils' movement, and a rear 
camera to capture the pupils' image. In this paper, we 
introduce a VR-based interactive e-learning system, which is 
implemented to enable kindergarten pupils to quickly 
experience a better sense of reality and immersion in a virtual 
reality environment without regard to the floor space. 
Applying technologies such as RTSP (Real Time Streaming 
Protocol) to the VR-based interactive e-learning system allows 
users to display the same view on different remote display 
devices, allowing separate users to share interactive events for 
collaboration.  
Keywords-e-learning; virtual reality; user interaction. 
I. 
 INTRODUCTION 
The popularity of VR (Virtual Reality) application 
systems as e-learning resources has increased significantly. 
Many e-learning applications have focused on developing 
online 
course 
materials, 
but 
VR-based 
e-learning 
applications [1][2] mainly have focused on making course 
materials interactively. VR-based e-learning applications are 
becoming a key part of the educational program in NLCY 
(National Library for Children and Young Adults) [3], 
Republic of Korea. The people who support the library are 
trying to expand its use of VR-based e-learning system 
nationwide. In particular, the e-learning system using VR 
technologies can enhance attention and engagement of 
kindergarten pupils who have short attention spans [4]. As 
kindergarten pupils and their teachers use the VR-based e-
learning system, they can be more visually aware of ongoing 
course materials and more intuitively aware of getting quick 
response from event handling on screens. However, the 
existing VR-based e-learning system [1][3] consists of 
complex equipment such as a large screen and beam 
projector to show the actual scene in 3D, more than two PCs, 
a forward camera to detect the pupils' movement, and a rear 
camera to capture the pupils' image. As a result, the process 
of installing the system becomes complicated, and once the 
system is installed in a specific place, it becomes impossible 
to move it to another place. It also requires an isolated and 
large room to install and operate the system. It is necessary 
to develop a system to increase the learning efficiency by 
giving the kindergarten pupils a feeling of immersion in a 
certain virtual space or situation and providing a vivid virtual 
experience. Also, it is necessary to develop technologies that 
enable virtual experiential learning classes held in one 
kindergarten to be shared with other kindergartens in remote 
locations in real time, to enable collaborative learning. In 
terms of the kindergarten administration, the introduction of 
an interactive virtual experiential learning system, which 
features a low cost and simple system installation process, is 
preferable to the existing system.  
The rest of this paper is organized as follows. Section II 
describes the functional components of the proposed system 
and Section III explains the process of evaluation in terms of 
the system performance. Finally, in Section IV, we present 
the conclusions and our future work. 
II. 
SYSTEM DESCRIPTION 
The environment of the VR-based e-learning system 
installed in the NLCY [3] for the purpose of running 
interactive storytelling programs for kindergarten pupils is 
shown in Figure 1. This system projects pupils into the 
background of various fairy tales in VR through large 
screens and it promotes reading by stimulating the interest in 
books. 
The 
VR-based 
interactive 
e-learning 
system 
developed in this study is shown in Figure 2. 
 
 
Figure 1.  Interactive storytelling program in NLCY 
As an output of the previous project [5], the VR-based e-
learning system installed at the NLCY had a very positive 
effect on kindergarten pupils in that they improved attention, 
comprehension and retention. By deploying the one-wall 
full-scale VR-based e-learning system, we can take this to 
the next level by offering immersive and realistic hands-on 
165
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-598-2
UBICOMM 2017 : The Eleventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

 
 
virtual learning experiences in which several pupils can 
participate at the same time.  
 
 
Figure 2.  Proposed VR-based e-learning system: (a) environment, (b) 
virtual experience to Santa village (deployment screen shot) 
The disadvantages of the existing system were that the 
system installation was complicated, a lot of equipment was 
needed, and space was limited. Also, there was a color image 
resolution (640x480) issue. In order to overcome these 
problems, we have developed the VR-based interactive e-
learning system that enables kindergarten pupils to 
experience a better sense of reality and immersiveness in a 
virtual reality environment, and can be quickly and easily 
installed by kindergarten teachers or administrators. The 
system consists of a keyboard, an RGB-D camera (Kinect 
V2, Full HD), a 64-bit windows desktop PC, and single TV 
monitor. Through a wireless mobile network, we have 
expanded to enabling parents or pupils in remote locations to 
use their mobile screen devices to access the same VR 
hands-on contents outside of kindergarten. This will enable 
pupils to conduct virtual learning experience outside of 
kindergarten before coming to class where teachers can tutor 
the pupils in VR. The proposed VR-based interactive e-
learning system consists of three functional components in 
terms of design, as shown in Figure 3. 
 
 
Figure 3.  Functional configuration of proposed system 
One is a VR-based interactive e-learning application 
including RGB-D camera, another is a control server, and 
the other is a mobile screen device.  The control server 
processes messages/events such as a session, an access URL, 
and a contents streaming URL for connection management 
between the screen device and the e-learning application. 
The mobile screen device is responsible for functions such as 
audio/video and user screen transmission of experiential 
contents, and handles media player and event handler 
functions for RTSP (Real Time Streaming Protocol)-based 
content streaming. The VR-based interactive e-learning 
application includes input data processing function that 
receives RGB data and depth data from a RGB-D camera 
and a user’s region segmentation function that extracts only 
a user region through image processing techniques. There is 
a user interaction control function for controlling interaction 
and event processing between the extracted actual user 
region data and virtual objects, and a contents authoring 
function for authoring and modifying the experiential 
contents. 
A. User’s region segmentation function 
The user’s region segmentation function extracts user 
pixel candidates based on color and depth frame images 
obtained from the Kinect V2 device. It extracts image objects 
corresponding to the user's foreground region from the depth 
frame image, and applies temporal filtering and vectorization 
processes to minimize outline noise and to correct blinking 
outline of user's foreground regions. After eliminating the 
background image objects that are not the user's foreground 
regions, the resulting images are synthesized into three-
dimensional virtual contents. The workflow of the user’s 
region segmentation function applied in the proposed system 
is illustrated in Figure 4.  
 
 
Figure 4.  Workflow of user’s region segmentation 
B. User interaction control function 
This function consists of two kinds of interaction control 
processing. One is the interaction control through user 
speech recognition, and the other is through user gesture 
recognition. The recognition rate of user speech is different 
according to the surroundings environment where Kinect is 
installed and the state of speech signal. In order to process 
interaction events based on user speech recognition between 
virtual image objects and real image objects, the Kinect 
Sensor performs an audio search and locates the sound. After 
finding the direction of the sound, it recognizes the speech 
the user has spoken.  
According to the recognition result, the interaction event 
between the user and the virtual object is performed and the 
result screen is rendered. In general, there are two 
approaches to user gesture recognition. One is a heuristic 
approach and the other is a machine learning approach. For 
this function, we use machine learning based user gesture 
recognition. The more iterations of the action recording and 
tagging process for many people, the higher the accuracy of 
gesture recognition can be. Finally, the gesture recognized 
and the corresponding event are mapped and the set 
interaction is performed. 
C. Input data processing function 
The function receives the color image, depth image, 
audio stream, and skeleton information from the Kinect 
sensor, and converts it into the required data format. Then, 
the converted data is input to the user’s region segmentation 
function and user interaction control function. 
166
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-598-2
UBICOMM 2017 : The Eleventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

 
 
III. 
SYSTEM PERFORMANCE 
In order to build the proposed system, the user's whole 
body image is extracted in real time and synthesized into 
three-dimensional virtual contents, and the gesture according 
to the user's motion and the speech of the user are recognized 
to process the interaction events between the virtual object 
and the user. To evaluate the performance of the proposed 
system, we test the accuracy of user’s region segmentation 
function and user interaction control function applied inside 
the system. For the user’s region segmentation function, we 
extract the user's body region at 16 FPS (frames per second) 
in the image frames obtained from Kinect and test the 
performance of the user’s region segmentation function as 
follows. First, we digitize the Full HD color image obtained 
from Kinect and directly extract the user's whole body region. 
Through this, a ground truth image (1920x1080 resolution) is 
prepared, which is divided into a user's body region and a 
background region, and the accuracy is compared with the 
result image (1920x1080 resolution) generated by the user’s 
region segmentation function of the VR-based interactive e-
learning system. In the same dataset range, the ground truth 
image is paired with the result image of user’s region 
segmentation function. Then, TP (True Positive), TN (True 
Negative), FP (False Positive), and FN (False Negative) 
values are calculated for each image frame, and Recall, 
Precision, and F-measure values are obtained using TP, TN, 
FP, and FN values. Finally, the user’s region segmentation 
function applied to the VR-based interactive e-learning 
system shows an average of 91.1% F-measure for a total of 
20,000 frames of input image. In order to evaluate the 
performance of the user speech and gesture recognition, we 
examine the result of recognition event processing by 
inputting English and Korean data strings. Looking at the 
front of the Kinect attached to the large screen, a single user 
shouts tomatoes. The user confirms the virtual tomato object 
displayed on the screen. The user touches a virtual object 
(tomato) with one hand, and then performs an action of 
throwing it in the forward direction, as shown in Figure 5. At 
this time, when the virtual object is thrown forward, it is 
judged that speech and gesture recognition is successful. 
Throughout a total of 50 field tests, the recognition rate of 
English and Korean data strings for the same object was 
about 90%. Researchers, not pupils, directly participated in 
testing the accuracy of the user’s region segmentation 
function and user interaction control function. 
 
 
Figure 5.  Test for user interaction control (throwing virtual objects) in the 
proposed system  
The proposed system in this study has been developed 
based on Unity 5.6 64bit. For performance tests, we have 
used Nuri curriculum contents. The Nuri curriculum is an 
educational 
welfare 
project 
targeting 
the 
holistic 
development of children aged 3 to 5 in Republic of Korea. 
The user interface of the display configuration to experience 
the content is simple, and the entire system for running the 
contents can be installed within a few minutes, without the 
need for large facilities or costly physical equipment. As 
shown in Figure 6, applying technologies such as RTSP to 
the VR-based interactive e-learning system allows users to 
display the same view on different remote display devices, 
allowing separate users to share interactive events for 
collaboration.  
 
 
Figure 6.  Real-time synchronization of experiencing Nuri curriculum 
content (life safety) across multiple screens 
The users can share speech and gesture interaction results 
for the co-registered virtual objects with other users on a 
single shared display. 
IV. 
CONCLUSION AND FUTURE WORK 
In this paper, we propose a VR-based interactive e-
learning 
system, 
which 
is 
implemented 
to 
enable 
kindergarten pupils to quickly experience a better sense of 
reality and immersion in a virtual reality environment 
without regard to the floor space. The proposed system is 
easy to install, easy to use, and easy to configure. A 
performance evaluation of the proposed system shows that it 
is effective for speech and gesture interaction to the co-
registered virtual objects between users on a single shared 
display.  
In the future, we will install the proposed system in a 
kindergarten and perform the tests in which the kindergarten 
pupils participate. Also, we plan to find a method to enhance 
the high-speed synchronization on display views across 
multiple smart devices so that virtual learning held in one 
kindergarten can be shared with other kindergartens in 
remote locations in real-time. 
ACKNOWLEDGMENT 
This work was supported by the ICT R&D program of 
MSIP/IITP [14-811-12-002, Development of personalized 
and 
creative 
learning 
tutoring 
system 
based 
on 
participational interactive contents and collaborative learning 
technology]. 
 
 
167
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-598-2
UBICOMM 2017 : The Eleventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

 
 
REFERENCES 
[1] S. Lee, J. Ko, S. Kang, J. Lee, “An immersive e-learning 
system 
providing 
virtual 
experience”. 
Proc. 
IEEE  
International Symposium on  Mixed and Augmented 
Reality 
(ISMAR 
2010), 
pp.249-250, 
doi:  
10.1109/ISMAR.2010.5643591. 
[2] 
Z. Li, J. Yue, D. Jáuregui, “A New Virtual Reality 
Environment 
Used 
for 
e-Learning”, 
Proc. 
IEEE 
International Symposium on IT in Medicine and 
Education(ITIME 
2009), 
pp.445-449, 
doi:10.1109/ITIME.2009.5236382. 
[3] S. Kang, Y. Lee, S. Lee, “Kids in Fairytales: Experiential 
and Interactive Storytelling in Children's Libraries”, CHI 
EA '15 Proceedings of the 33rd Annual ACM Conference 
Extended Abstracts on Human Factors in Computing 
Systems, 
pp.1007-1012, 
ISBN: 
978-1-4503-3146-3, 
doi:10.1145/2702613.2732826. 
[4] J. Yoon, H. Jee, B. Kim, S. Myung, K. Noh, “Trend of u-
learning technology”, Korean Journal of Information 
Science,  vol.27(7), pp.41-50, July. 2009, ISSN : 1229-6821. 
[5] MSIP/KEIT, Project report: Development of learner-
participational and interactive 3D Virtual learning contents 
technology, 2015. 
 
168
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-598-2
UBICOMM 2017 : The Eleventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

