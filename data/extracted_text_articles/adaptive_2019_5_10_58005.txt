 
 
Flood Prediction through Artificial Neural Networks 
A case study in Goslar, Lower Saxony 
 
Pascal Goymann, Dirk Herrling, and Andreas Rausch 
Institute for Software and Systems Engineering 
Clausthal University of Technology 
Clausthal-Zellerfeld, Germany 
e-mail: {firstname.lastname}@tu-clausthal.de 
 
 
Abstract—In this project, a system was developed, which allows 
a flood prediction based on given data sets for a level measuring 
station in the Goslar area for a period of four hours. First, 
existing neural networks, which were developed during a 
seminar at the TU Clausthal, were extended with the help of the 
framework Tensorflow and investigated whether larger water 
level values and further flood scenarios allow good qualitative 
prognoses. Furthermore, the influencing factors of possible 
floods were identified based on past scenarios. In addition to 
gauge and precipitation measuring stations in the immediate 
vicinity of Goslar, weather data from the Institute of Electrical 
Information Technology (IEI), which have been available every 
15 minutes since 2003, were also taken into consideration. These 
data sets were processed and evaluated accordingly, so that a 
qualitative prediction can be made for exact water gauge 
heights. In addition, in order to reduce the training time, a 
dimension extraction was performed using a Principal 
Component Analysis (PCA), in which main components were 
identified and the data set examined for patterns in order to 
determine the possibility of a dimension reduction. In order to 
transfer the neural network to further scenarios, a prediction 
was made for the area of Bad Harzburg, where two measuring 
stations with additional weather data were used as inputs. 
Keywords-Machine 
learning; 
Neural 
networks; 
PCA; 
Feedforward neural networks; Flood prediction. 
I. 
INTRODUCTION 
Floods are events, which, depending on the region and 
their characteristics, can have devastating consequences for 
people and their homes. At first, it is not always quite clear 
which exact interrelationships have been involved in the 
development of these events and have led to the subsequent 
catastrophe. Even if several flood events have occurred in the 
same region over the years, different reasons may have led to 
the individual incidents. This also applies to the area around 
Goslar, a town in the northwestern part of the Harz in the 
federal state of Lower Saxony. In July 2017, probably the 
most devastating natural disaster of the last century occurred 
here. Within only two days, 306 l/m2 (according to the records 
of Harzwasserwerke GmbH at the Eckertalsperre) of rainfall 
fell in the immediate vicinity. This rainfall could be recorded 
at the gauging stations installed there in the period from 24.07. 
9:00 am to 26.07. 12:00 pm [1]. The reason for this high 
precipitation could be found quickly and can be traced back to 
the low-pressure area “Alfred”, which led to many floods in 
other areas in the northern Harz. 
  
    Another example concerned 10 May 2018 (Ascension 
Day). Here, precipitation of up to 100 l/m2 fell within a few 
hours, with the Abzucht, a tributary river of the Oker, which 
was still largely responsible for the flood in the previous year, 
remaining far below the critical limit. Both situations show 
that although precipitation can play an important role, it is 
limited to a certain catchment area and can lead to flooding 
due to the direction of water flow in the local environment. 
Experience has also shown that other variables, such as soil 
moisture or snow melting must also be considered in order to 
be able to make accurate statements about whether there is a 
risk of flooding.  
    An evaluation of all theoretically possible parameters and 
measuring stations would take a lot of time and is currently 
already being implemented by an external warning system 
from the “Harzwasserwerke”. Different predictions e.g., for 
precipitation and temperatures are compiled and evaluated 
manually. The problem that arises at this point is the resulting 
warning time of approx. 20 minutes. This period is not 
sufficient for a complete preparation of the local fire brigade.  
In order to increase this early warning time and to be able to 
make a qualitative statement about a future flood, a self-
learning neural network will be created which automatically 
predicts a future (possible) exceeding of a threshold value at a 
water gauge measuring station. For this purpose, historical 
weather data as well as water level and rainfall data from the 
immediate vicinity of Goslar are used, which are fed into the 
neuronal network and used for training. The network 
optimizes the data based on the previously processed data and 
then provides information on whether there is a risk of 
flooding for another independent set of test data. 
    In the beginning existing work was taken up to check 
whether a flood can be predicted. These were evaluated and 
improved in order to be able to predict exact water levels with 
a maximum deviation of 5cm. Finally, a transfer to another 
scenario in Bad Harzburg takes place, where a flood 
prediction for another environment is made using another 
measuring station. 
    Section 2 starts with two related works, which have already 
dealt with similar topics. In section 3 a short explanation of 
the preliminary work is given, which has already investigated 
this topic in the context of a seminar at the TU Clausthal. 
Based of this works, a forecast of floods in Goslar is finally 
56
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-706-1
ADAPTIVE 2019 : The Eleventh International Conference on Adaptive and Self-Adaptive Systems and Applications

 
 
made. This serves to investigate whether an improvement of 
their algorithms can be made. Then a prediction of exact water 
levels is made. This forecast is then examined for dimensional 
reduction using a Principal Component Analysis (PCA) and 
evaluated for another scenario in Bad Harzburg. Section 4 
briefly lists and compares two further alternative learning 
methods. In the end section 5 concludes with an explanation 
of the further outlook. 
II. 
RELATED WORK 
In this section, two related works are taken up, which deal 
with similar topics. The first work deals with a service 
provided by Google, which is part of the Google Public Alerts 
Program and can use Artificial Intelligence (AI) to predict 
floods in the Indian region and then send warnings to the 
inhabitants [2]. The second work deals with an AI technology 
for reliably predicting earthquakes in different parts of the 
world [3]. 
A. Google Public Alerts 
In 2017, Google provided special warning services within 
Google Maps, Google Now and in the normal Google search 
to warn affected people of imminent disasters. These include 
storm warnings, hurricane evacuation alerts, forest fires and 
earthquakes. The data is made available by the cooperation 
partners from the USA, Australia, Canada, Colombia, Japan, 
Taiwan, Indonesia, Mexico, the Philippines, India, New 
Zealand and Brazil, collected by Google and displayed for all 
users worldwide. Only early flood warnings were not made 
available to users on this platform. Since these were not 
offered by the cooperating partners, Google has developed its 
own service, which uses Artificial Intelligence to predict flood 
catastrophes. Using historical weather and flood events, as 
well as river level measurement stations and terrain 
conditions, these data are processed and fed into a neural 
network and then simulated on maps [4]. The subsequent 
prediction results are stored in Google Public Alerts with the 
severity of the event. 
    For a first test with real data, Google Public Alerts was 
released in September 2018 in the Patna region in India and 
first floods were successfully predicted. India's central water 
authority is working closely with Google to achieve better 
results in the future, which can be better achieved by Google 
than by the authorities themselves due to its technical 
expertise and the computing power it provides. In the future, 
cooperation’s with Europe will also be realized in order to 
make similar predictions and make them available to users. In 
this case, interfaces would have to be created to enable Google 
to have permanent access to data. 
B. Artificial Intelligence based techniques for earthquake 
prediction 
A second elaboration [3] also deals with an early warning 
system, which was built based on Artificial Intelligence. Here, 
the prediction was not of floods but of earthquakes, whereby 
different approaches for the realization of such systems were 
compared with each other. Basically, earthquakes can be 
characterized by two properties. This is on the one hand the 
magnitude and on the other hand the depth. Those that are 
classified as fundamentally dangerous are those that are at a 
shallow depth and have a high magnitude. These are weighted 
correspondingly higher in the neuronal network. Based on 
input data from southern California, archived in the Southern 
California Earthquake Data Center (SCEC), earthquakes were 
tested in a Probabilistic Neural Network (PNN) of various 
strengths, of which 102 out of 127 earthquakes were 
successfully detected in the test data sets of classes 1 to 3. 
Stronger earthquakes were also used in the test data sets. 
These were not successfully detected according to the paper, 
so instead of PNNs RNNs (Recurrent Neural Networks) for 
magnitudes from 6.0 were investigated [5]. In data 
preprocessing, the area is divided into smaller regions and the 
time in which the earthquake occurred into several time slots. 
Subsequently, these sections were processed including their 
relation to larger earthquakes in the investigated region. 
Another scenario presented here concerns Chile. For this 
purpose, a Novel Neural Network, which predicts whether an 
earthquake will occur or not over the next five days [6]. The 
earthquakes used for this purpose were taken from two 
earthquake catalogues [7] and [8], which record all 
seismological activities in South America. Earthquakes from 
a magnitude of 4.5 in the period from 1957 to 2007 were used 
for this purpose. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Pie chart for the earthquake forecast of the Chile Peninsula [6] 
    The training and test procedure took place in various 
regions in Chile, whereby different warning times and 
earthquake magnitudes were used. On average, results were 
close to 71%. From the 122 earthquakes defined in the test 
data, a pie chart was created, which is shown in Figure 1. Here, 
not only the occurrence of an earthquake was predicted, but 
also its strength, whereby a deviation of 1% flowed into the 
result. 
III. 
IMPLEMENTATION AND RESULTS 
A. Preliminary work  
In the context of a seminar at the Clausthal University of 
Technology, three seminar papers on this topic were written, 
which dealt with the help of different AI frameworks (CNTK 
from Microsoft, Tensorflow from Google and Caffe from the 
University of California, Berkeley) and the previously defined  
57
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-706-1
ADAPTIVE 2019 : The Eleventh International Conference on Adaptive and Self-Adaptive Systems and Applications

 
 
Figure 2. Used measuring stations: source (modified): 
openstreetmap.org 
 
 
 
problem [9] [10] [11]. It was their task to create a neural 
network based on given data and to determine the data and 
result quality on this basis. It should be identified if that 
framework is suitable for this problem and how the result can 
still be improved. Two data sets were provided: 
 
A training data set from 01.11.2003 to 
03.12.2012 consisting of approx. 80000 data 
points in 1-hour intervals 
 
A test data set from 14.06.2015 to 01.01.2018 
consisting of approx. 22000 data points in 1-hour 
intervals 
 
    Both data sets contained the water levels (cm) and flow 
rates (m3/s) at the water level measuring stations Sennhütte 
and the Margarethenklippe, rainfall data (mm) in Hahnenklee 
and the Granetalsperre. In addition, weather data were 
provided by the Institute for Electrical Information 
Technology at Clausthal University of Technology. These 
contained the temperature, the humidity, the air pressure, the 
solar radiation, the wind speed and the wind direction, 
whereby an average over the period of one hour took place. In 
addition, each data point was assigned a label ϵ [0,1] 
indicating if flooding had occurred. Decisive for this was the 
water level at the water level measuring point Sennhütte, 
whereby it was defined that the water level of 40cm marked a 
flood. The measuring stations are shown in Figure 2 on a map 
(A: Hahnenklee, B: Granetalsperre, C: Margarethenklippe, D: 
Sennhütte). 
    The task was to make a flood prediction for the location 
Sennhütte with an early warning period of one or 48 hours. In 
order to achieve this, the provided data was processed. The 
average values of the last 2, 4, 8, 16 and 32 hours of rainfall 
were used for the input vector by a supposed connection 
between the water quantities in the rivers and the past 
precipitation values. The same procedure was used for the 
temperature values. Similarly, the seminar participants 
formed the averages of the individual water levels and air 
pressures over 1, 2, 3 and 4 hours. After processing, the data 
was fed into the neuronal network. 
    For all three seminar papers, the results for the prediction 
of one hour are presented in the Figures 3-5. For this purpose, 
a confusion matrix was created, which compares the results of 
the prediction and the results in the test data set. The correctly 
predicted results were highlighted in green, while the false 
alarms (false positives) were highlighted in yellow and the 
false negatives in red. This has the background that the false-
negative results should be avoided altogether, as they would 
have devastating consequences by an incoming flood. The 
false positives should also converge towards 0, as this would 
result in unnecessary deployment of the emergency services. 
The consequences, however, would be manageable and have 
a lower damage potential. 
 
 
 
        
 
                         Figure 3. Result with Framework Caffe [9] 
 
 
 
 
Figure 4. Result with Framework CNTK [10] 
 
 
 
 
 
Figure 5. Result with Framework Tensorflow [11] 
    In all three elaborations, a two-hour and four-hour forecast 
was discussed in more detail following the 1-hour forecast.  
Similar results could be achieved, which only worsened 
significantly with an increase to eight hours. For this reason, 
the further investigation of this work will concentrate on a 4-
hour forecast, which would represent a considerable 
improvement in view of the warning time currently in use. 
B. Prediction for higher water levels  
Based on the preliminary work of the seminar participants, 
their algorithms should first be examined for major flood 
hazards. For this purpose, the water levels, which characterize 
floods, were increased to 50, 60, 70 and 80 cm. For better 
results, the data from the measuring stations were processed 
at 
15-minute 
intervals 
instead 
of 
one 
hour. 
Like the preliminary work, the precipitation data at the 
measuring stations of the Granetalsperre and Hahnenklee 
were averaged over 1, 2, 4, 8, 16 and 32 hours and fed into the 
net as additional parameters. The values of the last 1, 2, 3, 4 
and 5 hours were calculated for the outflow and water level 
measurements at the Margarethenklippe and the Sennhütte 
and fed into the neuronal network as additional dimensions. 
58
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-706-1
ADAPTIVE 2019 : The Eleventh International Conference on Adaptive and Self-Adaptive Systems and Applications

 
 
In order to guarantee an even better quality, the same weather 
data set was used that was already used in the seminar papers. 
From these the average temperatures of the last 1, 2, 4, 8, 16, 
32 hours were calculated and transferred together with the air 
humidity, the air pressure at the considered time, as well as 
before 1, 2, 3 and 4 hours into the input vector. The considered 
solar irradiation value and the wind speed were taken over in 
addition. 
    For the prediction of higher water levels, moving averages 
were formed which, in contrast to the arithmetic mean, are not 
formed over all data records, but only over a selected period. 
In this case, for example, this applies to temperatures of up to 
32 hours and 128 data records. For the classification of the 
network, a label was created, which can be used for different 
Scenarios indicated if there was a flood (0=no flood, 1=flood). 
In order to consider data sets that indicate a flood hazard 
during the training process of the neural network more, an 
additional field is added to the input vector called “weight”. 
This tells the network how often this data set should be trained 
in comparison to the data that do not represent a flood hazard. 
It is calculated from the ratio between the data sets with the 
label 1 and the label 0 used in the training data set. In addition 
to the data preparation, the scenarios to be tested were also 
determined. These were a total of 12, whereof the water levels 
of 50, 60, 70 and 80cm with a warning time of 1, 2 and 4 hours 
were considered. Good forecasts were achieved for all 
forecasts. This also affected the events with a four-hour 
warning time. 
 
    Two hidden layers with a neuron count of 128 and 64 
neurons were used for the network architecture. The sigmoid 
function was used as activation function, which carried out the 
training with a learning rate of 0.001 and 10000 training steps. 
The optimization function for this case was the Proximal 
Adagrad-Optimizer. Figure 6 shows the confusion matrix at a 
water level of 50cm and an hour warning time. 
 
 
 
Figure 6. Confusion matrix at 50cm water level and one hour warning time 
    The number of false negatives could be completely reduced 
to zero. This applies to all scenarios listed above in the same 
way. In addition, 413 results were issued as false alarms, 
which, like the preliminary work, was largely due to a better 
early warning period, in which the occurrence of a flood event 
was predicted too early. Since the confusion matrices for the 
other eleven scenarios contained similar values, these are not 
listed here. 
C. Prediction for concrete water levels 
After the prediction of flood events for different scenarios, 
the prediction of exact water levels will be dealt in the 
following. The same data basis was used as for the previous 
prediction. The forecast was made for the gauging station in 
Sennhütte. Since only a few floods were available in the 
database from 2003 to 2018 and their level levels varied, two 
different training and test data sets were distinguished for this 
prediction: 
 
A test data set from 2014 to 2018 and a training data 
set from 2003 to 2013 
 
A test data set from 2003 to 2008 and a training data 
set from 2009 to 2018 
 
 
    Since a neural network can only learn the water levels that 
were made available to it in the training set, it was not possible 
in the first case to correctly predict the water levels of the 2017 
flood because these were outside the value range. For this 
reason, the order was reversed and this flood was integrated 
into the training data set.  
    In order to enable the neural network to correctly learn less 
frequently occurring water levels in the training set, all data 
points have been assigned a weight indicating how often the 
corresponding water level should be trained in the data set. 
The rarer the water level appears in the entire training data set, 
the higher the number of training runs in the neural network 
for this one data set.  
    In contrast to the prediction for higher water levels more 
neurons were used in this scenario. For the first hidden layer 
these were 512 and for the second hidden layer 256 neurons. 
Furthermore, the sigmoid function was used again as 
activation function and the Proximal Adagrad-Optimizer as 
optimization function, again with a learning rate of 0.001. 
Only the number of training steps was adjusted in the 
parameters. So, this has increased from 10000 training steps 
to 100000 steps, because the best prediction results could be 
achieved. 
    For the test cases, 170 (largest measured water level) 
different classes were created, which were used as 
classification basis for the data sets. In the prediction, the 
neural network finally assigned each data point to a class, 
whereby the actual and target states could be compared with 
each other. Tables 1 and 2 show the results of both predictions. 
TABLE 1 PREDICTION FOR 2014-2018 (157611 RECORDS) 
Difference greater 
than [cm] 
Number of data 
sets 
Accuracy [%] 
0 
7429 
95.29 
1 
356 
99.77 
2 
184 
99.88 
3 
147 
99.91 
4 
127 
99.92 
5 
113 
99.93 
 
TABLE 2  PREDICTION FOR 2003-2008 (181026 RECORDS) 
Difference greater 
than [cm] 
Number of data 
sets 
Accuracy [%] 
0 
12571 
93.056 
1 
1741 
99.04 
2 
740 
99.59 
3 
367 
99.8 
4 
207 
99.89 
5 
131 
99.93 
     
59
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-706-1
ADAPTIVE 2019 : The Eleventh International Conference on Adaptive and Self-Adaptive Systems and Applications

 
 
    On the one hand, the tables contain the number of data sets 
whose values in the prediction differ from those in reality by 
a certain amount and on the other hand the ratio of these to the 
total number of data sets. Both tables show good results, 
which show an accuracy of 99% at a water level difference of 
1cm 
D. Principal Component Analysis (PCA) 
From the amount of data used so far, it is not possible to 
deduce which attributes have the greatest impact on the 
training and results of the neural network. An identification of 
the most important influencing factors allows a reduction of 
the input data and computing time, as well as a de-noising of 
the data set. An overfitting of the network is also limited by 
the dimension reduction, since a larger number of dimensions 
leads to a larger adjustment of the neural network. to get better 
results. 
    In the following, a Principal Component Analysis (PCA) is 
presented, which calculates possible main components based 
on linear combinations and enables a dimension reduction on 
of these. For this purpose, the original dimensions p are 
reduced to a smaller number of dimensions q, which 
summarize the essential information of the p dimensions. 
    First, the data set is standardized in order to ensure a correct 
distribution of the individual characteristics and to realize 
independence from the value ranges. From this standardized 
data set, a covariance matrix is created, which contains the 
covariance of each attribute with every other attribute. The 
eigenvalues and eigenvectors are calculated from this 
covariance matrix. The eigenvectors form the main 
components, while the corresponding eigenvalues signal how 
much information is contained in them [12]. The p 
eigenvectors with the largest eigenvalues are then filtered out. 
This serves to form a transformation matrix T consisting of m 
rows (number of dimensions) and p columns (number of 
eigenvectors). The following 12 attributes were selected for 
the realization of the PCA in this paper: rainfall  from 
Hahnenklee and the Granetalsperre, outflow and water levels 
of the Margarethenklippe and the Sennhütte, as well as 
weather data consisting of temperature, air humidity, air 
pressure, radiation, wind speed and wind direction, in each 
case in the interval of one hour, resulting in 102040 data 
records. The first main component is obtained by minimizing 
the sum of the squared deviations of all variables. In other 
words, to extract the first component, the portion of variance 
that the component can explain across all variables is 
maximized. The remaining variance is then explained step by 
step. This means that the second component should clarify as 
much residual variance as possible. This procedure continues 
until the total variance of all data is theoretically explained by 
the main components.  
    The first seven main components of the Principal 
Component Analysis would be sufficient to maintain 90 
percent variance. These account for 93.27 percent of the total 
variance, so a reduction from twelve to seven dimensions 
would only result in a 6.73 percent loss of information. Using 
these results, it can be concluded that all data from the 
measuring stations would be sufficient to have a large part of 
the information. The weather data only have an influence of 
6.73 percent on the total information content and could 
therefore be discarded. 
 
 
                             Figure 7. Reduction to 2 dimensions  
 
    The Reduction means that the computation time and 
memory problems can be decreased, as fewer dimensions are 
included in the calculation without a significant loss of the 
prediction quality. Figure 7 shows a reduction to two 
dimensions, with clustering of the data points around the 
coordinate origin. 
E. Bad Harzburg area 
The following is a review of the neural network for a 
further Scenario outside the previously considered catchment 
area. For this purpose, the region around Bad Harzburg with 
one precipitation and one outflow/level measuring station 
each installed at the Baste was examined more closely. 
Since in contrast to Goslar only two measuring points were 
available, the values for soil moisture (only derived), air 
temperature and precipitation, provided by the “Deutschen 
Wetterdienst” at 10-minute intervals, were also used. In total, 
this resulted in 520128 data records. 
    The task was to predict water levels of 50, 60, 70 and 80cm 
for one, two and four hours and to evaluate them by the flood 
from 24.07.2017 to 27.07.2017. After the data preparation, the 
data set was labelled. All data points which exceeded the flood 
threshold value were assigned the code number 1. All other 
data points were given the value 0. Another field contained a 
weight that trained data sets that signaled flood hazard more 
frequently in the network than data that did not. 
 
 
 
 
 
 
 
Figure 8. Prediction for Bad Harzburg at a water level of 80cm and two 
hours 
60
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-706-1
ADAPTIVE 2019 : The Eleventh International Conference on Adaptive and Self-Adaptive Systems and Applications

 
 
 
 
 
 
 
 
 
Figure 9. Prediction for Bad Harzburg at a water level of 80cm and four 
hours 
    The results of the prediction presented at Figure 8 and 9 are 
exemplary for a two- and four-hour prediction for a threshold 
value of 80cm. At first glance, it is clear that good results were 
obtained for an advance warning time of two hours, but that 
these results clearly deteriorated with an increase to four 
hours.   
    In order to enable a comparability with the results of the 
level measuring station Sennhütte, the same parameters were 
used for the creation, the training and the testing of the 
neuronal network. These parameters included the learning rate 
(0.001), the number of training sessions (10000) and the 
number of hidden layers (2 with a neuron count of 128 and 64 
neurons). The activation function was the sigmoid function 
and the optimization function the Proximal Adagrad-
Optimizer. For this reason, it can be concluded that the 
number and location of the gauge, outflow and precipitation 
measuring stations have a major impact on the quality of the 
results and that it might be advisable to add more for better 
results.  
IV. 
ALTERNATIVE METHODS 
    This paper deals with Artificial Neural Networks (ANN), 
which in this case were excellently suited for supervised 
learning. There are also alternative methods such as 
Regression Trees or Ensemble Learning that can also be used. 
Regression Trees belong to the group of decision trees and 
deal with the mapping of decision rules to a branched tree, 
which is then traversed from the root to the individual leaves 
based on various decisions. 
    With regression trees a continuous value is mapped on the 
individual leaves (e.g., a time series analysis), so the aim of 
this method is the prediction of continuous values. This has 
the advantage that for small amounts of data it is easy to 
understand which decision is made at which moment. 
However, this clarity decreases considerably with an 
increasing number of decisions. A further problem here is the 
overfitting, in which the error increases continuously with 
new test data records at a certain point [13]. 
    In contrast to other methods, Ensemble Learning uses 
several different learning algorithms at the same time. A set 
of predictors (ensemble) is formed, which together form an 
average (ensemble average). In this way, certain outliers can 
be corrected by other learning methods. If there is a method 
that is best suited to the problem, it will outperform most other 
learning methods in ensemble learning. Furthermore, the more 
methods used, the more difficult it will be to interpret the 
results. The biggest restriction, however is the learning time. 
The more methods are used, the longer the algorithm needs to 
calculate the prediction result, which is the reason why this 
work is limited to only one learning procedure [14]. 
V. 
CONCLUSIONS 
    The aim of this work was to establish, train and evaluate a 
neural network for the detection of flood hazards and concrete 
water levels in the area around Goslar. In addition, existing 
works were examined, trained and tested for the recognition 
of flood inlets starting from a higher water level. Increasing 
the warning time to four hours produced very good results. All 
floods in the period from 2003 to 2018 were successfully 
detected and predicted accordingly. A reduction of false 
alarms was successfully achieved, but some were left over 
time, which in some cases resulted in false positive 
predictions. This, however, was usually related to a danger 
that had already occurred shortly before. Further water level, 
precipitation and outflow measuring stations could be used to 
identify further floods outside the area (here: Sennhütte). For 
example, the flood of 10 May 2018 did not appear in the data, 
but was defined as a flood hazard.  
    In addition to the identification of floods, the prediction of 
exact water levels was also a task of this work. The same data 
sets as for the flood prediction were used to divide the data 
into two scenarios. For each scenario, it was necessary to 
predict the water levels at the Sennhütte with a maximum 
deviation of 5cm. The results showed an accuracy of close to 
99% from a difference of 1cm, only higher water levels, which 
were either outside the value range of or only with a very small 
number in the training datasets were not detected and were 
above the 5 cm deviation limit. In order to obtain better results 
for the higher water levels, further floods in the training data 
would be necessary. 
    The consideration of a dimension reduction for the selected 
data set with 12 dimensions has shown that seven dimensions 
would be enough for over 90 percent variance conservation. 
As an alternative classification method an LDA [15] can be 
used. When using a discriminant function two or more groups 
can be examined simultaneously for a plurality of 
characteristic variables. Furthermore, new objects whose class 
affiliation is not known can be rearranged. Later collected data 
can be easily assigned. 
    The evaluation for the area of Bad Harzburg was also able 
to precisely detect and predict floods up to a warning time of 
two hours. When this time was increased to four hours, the 
accuracy was no longer sufficient. An addition of further 
measuring stations to determine the water levels, outflow 
quantities and precipitation could produce a similar result 
quality as for the area around Goslar. 
 
    Until now, the measurement of the prediction quality has 
only been calculated using confusion matrices, which did not 
provide a meaningful evaluation of false alarms. For a better 
determination of the quality of the neuronal network, another 
method can be used, called cross-validation method [16]. The 
most frequently used method is k-fold cross-validation. At the 
beginning there is a division of the data into k equal parts, also 
61
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-706-1
ADAPTIVE 2019 : The Eleventh International Conference on Adaptive and Self-Adaptive Systems and Applications

 
 
called folds. Subsequently, the network is run through k times, 
where k-1 folds are used as training data set and the remaining 
fold as test data set. With each further run a different fold is 
used as test data set, so at the end of the validation procedure 
k accuracies are available from which the arithmetic mean will 
be formed. The multiple runs of the net with different folds 
provide information about the sensitivity of the net, since each 
data point in the data set was available exactly once in the test 
data set. 
    The preprocessing of the time series of the different 
measuring stations and weather data can also be replaced by a 
recurrent neural network. Here, the data of the past time points 
are stored in the input layer. A recurrent neural network can 
use this information, which are stored internally. 
ACKNOWLEDGEMENT 
At this point I would like to thank all the people who 
supported me in the preparation of this paper. My special 
thanks go to the Harzwasserwerke (HWW) for providing the 
data from the measuring stations in Goslar and Bad Harzburg, 
as well as to the Institute of Electrical Information Technology 
for providing the data at the weather station located there. 
REFERENCES 
 
[1] NLWKN, The July flood 2017 in southern Lower Saxony, 
2017 
[2] J. Vincent, Google is using AI to predict floods in India and 
warn users. [Online]. Available from:  
 
https://www.theverge.com/2018/9/25/17900018/google-ai-
predictions-flooding-india-public-alerts. [retrieved:  04, 2019]. 
[3] F. Azam, M. Sharif, M. Yasmin, and S. Mohsin,  “Artificial 
Intelligence Based Techniques For Earthquake Prediction: A 
Review”, Science International, vol. 26, pp. 1495 – 1502, 2014. 
[4] Y. Matias, Keeping people safe with AI-enabled flood 
forecasting. [Online]. Available from: 
 
https://www.blog.google/products/search/helping-keep-
people-safe-ai-enabled-flood-forecasting/.  
[retrieved:  04, 2019]. 
[5] H. Adeli, and A. Panakkat, “A probabilistic neural network for 
earthquake magnitude prediction”, Neural networks : the 
official journal of the International Neural Network Society, 
vol. 22, pp. 1018-24, doi: 10.1016/j.neunet.2009.05.003.  
[6] J. Reyes, and V. H. Cárdenas, “A Chilean seismic 
regionalization through a Kohonen neural network”, Neural 
Computing and Applications, vol. 19, pp. 1081-1087, doi: 
10.1007/s00521-010-0373-9. 
[7] Ceresis catalog. Earthquake catalogue for South America. 
[Online]. Available from: http//www.ceresis.org./. [retrieved:  
04, 2019]. 
[8] Usgsr, Earthquake Hazards Program. [Online]. Available 
from: https://earthquake.usgs.gov/. [retrieved:  04, 2019]. 
[9] S. Rupsch, Prediction of floods through neuronal networks 
using the framework Caffe, unpublished. 
[10] R. Kern, Flood prediction with the Machine Learning 
Framework Microsoft Cognitive Toolkit, unpublished. 
[11] H.  Rosenberg, Flood prediction with Tensorflow, unpublished. 
[12] H. Lohninger, Principal Component Analysis (PCA). 
[Online]. Available from: 
http://www.statistics4u.info/fundstat_germ/cc_pca.html. 
[retrieved:  04, 2019]. 
[13] Popular Decision Tree: Classification and Regression Trees 
(C&RT). [Online]. Available from: 
http://www.statsoft.com/Textbook/Classification-and-
Regression-Trees. [retrieved:  04, 2019]. 
[14] R. Polikar, Rowan University, Ensemble learning. [Online]. 
Available from:  
http://www.scholarpedia.org/article/Ensemble_learning. 
[retrieved:  04, 2019]. 
[15] V. Zeissler, Robust recognition of prosodic phenomena and 
emotional user states in a multimodal dialogue system, Studien 
zur Mustererkennung, Logos Verlag Berlin, pp. 165-166, 2012. 
[16] A. C. Müller, and S. Guido, Introduction to Machine Learning 
with Python: Practical Data Science Knowledge, O’Reilly, pp. 
236ff, , 2017. 
 
 
62
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-706-1
ADAPTIVE 2019 : The Eleventh International Conference on Adaptive and Self-Adaptive Systems and Applications

