Moral Behavior and Empathy Modeling through the Premise of Reciprocity
Fernanda Monteiro Eliott∗, Carlos Henrique Costa Ribeiro†
Computer Science Division. Aeronautics Institute of Technology
S˜ao Jos´e dos Campos, Brazil
Email: fernandaeliott@gmail.com∗; carlos@ita.br†
Abstract—We may get the opportunity of conceiving modeling
artiﬁcial moral behavior and empathy if we renounce the perspective
of an immaterial soul playing a role in the process of moral behavior.
Philosophers such as Michel de Montaigne wrote that the laws of
consciousness, supposed to emerge from nature, are essentially born
from custom. Hence, we may provide a basis to that modeling if
we pore over moral behavior as a form of cooperation built upon
customs among emotions and feelings (as part of cognition). With this
perspective in mind, we describe herein a bio-inspired computational
multiagent architecture composed of artiﬁcial emotions, feelings and
by an Empathy Module responsible for providing an action selection
that rudimentary mimics moral behavior. The Empathy Module
follows a reciprocity assumption as its main design concept. As
relations between different subjects can be represented by networks,
we explore different network topologies that can characterize the
agent-agent interactions, by deﬁning the moral agents neighborhood.
For assessment of the proposed architecture, we use a version of an
evolutionary game that applies the prisoner dilemma paradigm to
establish changes over the network topology. Our results indicate the
feasibility of artiﬁcial moral behavior leading to cooperative selection
of action when applied in environments (networks) whose reciprocity
assumption works in accordance with the environmental topology:
networks with neutral assortativity w.r.t. node degree (i.e., agent
neighborhood size) ﬁt more closely with the leading premise of our
Empathy Module than those with a disassortative degree correlation.
Keywords–Artiﬁcial moral machine; Empathy; Biologically inspired
architecture; Evolutionary game; Assortativity in networks.
I.
INTRODUCTION
The complex behavior of living things enlivens research and
incongruous reasoning. Despite that, we may get the opportunity
of conceiving modeling artiﬁcial moral behavior and empathy if
we renounce the perspective of an immaterial soul playing a role
in the process of moral behavior. The artiﬁcial modeling of bio-
inspired mechanisms embodies a positioning on the premises and
assumptions inherited from the selected and pursued theoretical
biological references. The development of a bio-inspired compu-
tational multiagent architecture supposed to mimic moral behavior
has to be grounded on biological and philosophical investigation
to provide a coherent construction and an intuitive working system
dynamics. Thinking through the constitution of a group and
its members attendance, Tomasello [1] regards cooperation as a
sewing up action that connects the members of the group. By
using an evolutionary perspective, morality could be conceivable
as a form of cooperation: through matching skills and aims for
cooperation, morality may emerge [2]. Moral behavior consists
of following the set of rules from the group, keeping it cohesive,
and a gradual incorporation of new customs can change that
set. According to Montaigne [3], when we reiterate a custom
and naturally incorporate it among our thoughts and ideas we
submit to it and establish it; therefore, the laws of consciousness,
supposed to emerge from nature, are essentially born from custom:
the common judgments and ideas tacitly respected among our
group show themselves as general and natural. To approach
the human judgments fallibility and weakness, in Montaigne [4]
humans are compared with the other animals and the pyrrhonic
suggestions from Empiricus [5] are delineated: our ways of in-
teracting with the environment are fragile. Our bodies, reasoning,
interpretation and capabilities are subjected to uncertainty and
debate. Supposing we had other sense organs, our apprehension
of the world and interaction with it could be different.
Montaigne [3] also addresses the judgments and customs
relativity and fallibility. Different groups usually have different
customs and follow different laws and rules. Since we are fal-
lible, the groups common behavior provides us with a guidance
and, given its continuous application, an indication of the most
provable consequences given to its application. Therefore, the
modeling of a bio-inspired computational architecture supposed
to mimic moral behavior may beneﬁt itself from reﬂections over
moral behavior as a form of cooperation built upon customs
among emotions and feelings (as part of cognition) - and it
is pertinent to seek the human universal perspective. There are
some dilemmas regarding the feelings and emotions participation
on judging our actions while interacting with others. Would the
human being be naturally sociable or would the sociability have
emerged to ensure survival? Would we be the Zoon Politikon from
Aristotle [6], or the bon sauvage from Rousseau [7], or still would
our nature be better translated by the fear of all against all [8]?
Thus which should be our positioning while designing an artiﬁcial
empathy module? Should we design an artiﬁcial empathy on the
“Machiavellian” [9] guidance? Nonetheless premises do have to
be assumed.
The division of our paper is built as follows: in Section I,
as a preliminary background to think through our computational
architecture, we introduce some moral-related philosophical per-
spectives, as well as our bio-inspired motivation. In Section II,
our artiﬁcial moral architecture and its Empathy Module are both
described. Section III details the experimental setting built to test
the feasibility of our artiﬁcial moral architecture. In Section IV we
analyze the obtained results with the purpose of elucidating the
implication of the reciprocity design concept from the Empathy
Module. Finally, in Section V, we provide our ﬁnal remarks.
From a biological standpoint, Dam´asio [10] highlights the
relevance of social emotions and feelings as empathy to the equi-
librium of humans homeostatic goals. Moreover, from a cognitive
aspect the dynamics involved on the existence of empathy can be
approached while holding an emotional background [11] [12].
Truly, emotions and feelings contribution on aiding humans on
making faster and more intelligent decisions was already detailed
in Dam´asio [13], inspiring the single agent driven bio-inspired
Asynchronous Learning by Emotion and Cognition (ALEC) com-
putational architecture from [14] [15]. Before choosing an action,
ALEC is inﬂuenced by artiﬁcial homeostasis and by a cognitive
system motivated by the Clarion Model [16]. With the aim
of establishing its internal equilibrium (i.e., holding its internal
variables within a threshold), ALEC has to achieve artiﬁcial
homeostatic goals.
We used the ALEC computational architecture as the outset of
37
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-447-3
HUSO 2015 : The First International Conference on Human and Social Analytics

our bio-inspired computational multiagent architecture MultiA.
The design of MultiA was guided by thoughts on the pertinence
of moral behavior to attain a rational and cooperative bio-inspired
artiﬁcial agent. Our leading hypothesis relies on the idea that
cooperation can emerge from the assistance of emotions and moral
behavior during the process of decision making — even when
selﬁsh behavior is rewarded by high reinforcements. The analogy
with moral behavior is promoted through simulating the feeling
of empathy. The importance of such feeling is its function on
regulating MultiA agents priorities making possible the selection
of actions that may not be the best selﬁsh selection. Non selﬁsh
decision making may be crucial to equalize the interactions among
agents and bring up cooperation.
We provided an outline for computational moral modeling
in [17], heightened by biological references on basic and social
emotions, and mirror neurons (from [18] [19]). Examples of com-
putational moral models were also mentioned. In [20] we detailed
our computational architecture based on [17] and discussed some
preliminary results. Herein we show the feasibility of our artiﬁcial
moral architecture and present new results with the purpose of
elucidating the implication of the reciprocity design concept from
the Empathy Module (EM) of MultiA. The EM is a constituent
part of the Cognitive System (CS) and determines the intensity
of the emotion responsible for feeding the feeling of empathy.
II.
THE MULTIA ARCHITECTURE AND THE EMPATHY
MODULE
Having detailed MultiA in [20], herein we only summarize
a few of its key points. As long as our research is grounded
on moral behavior, we intend to test and study MultiA agents
interacting among themselves. Thus, each MultiA agent i will
keep a list of every agent it has interacted with (then neighbors
of i). The MultiA architecture consists of three main systems
(Figure 1): the Perceptive (PS), the Cognitive (CS) and the
Decision Systems (DS). The collaboration between the three
systems will result in the selection of actions derived from
sensations triggered by the environment while provoking envi-
ronmental changes that will, in turn, trigger new sensations, and
so on. MultiA artiﬁcial sensations (all in the range [0, 1]) are
triggered by reinforcements, and by an identifying index for
the neighbor it is interacting with: every MultiA agent has an
identifying index i = {1, ..., N}. Likewise, the neighbors relating
to each agent i also have an identifying index p = {1, ..., K}.
A given p value thus refers to a particular neighbor that is
interacting with i. There are basic emotions ={Eb
1,i, Eb
2,i, ..., Eb
d,i}
and social emotions ={Es
1,i, Es
2,i, ..., Es
y,i}, all normalized to the
range [−1, 1]. The basic emotions are associated with the general
condition of the MultiA agent itself. On the other hand, social
emotions are stimulated by neighbors and by the impact of the
own agents actions on those neighbors. The artiﬁcial feelings
= {S1,i, S2,i, ..., Sz,i} also fall in the range [−1, 1] and are fed by
emotions. For a complete description of feelings and emotions,
see [20].
The artiﬁcial sensations feed emotions, feelings and, after-
wards, through a weighted sum on feelings, the general per-
spective of MultiA (named Well-Being, Wi) about its own
performance. MultiA follows its artiﬁcial homeostatic goals,
which consist of keeping its feelings within a threshold with the
aim of achieving high levels of Wi. The feelings maintenance on a
threshold relies upon the selection of adequate actions in response
to the environment. Wi uses feelings to internally represent the
general condition of agent i, and is calculated with normalizing
weights, such that the ﬁnal value will fall in the range [−1, 1].
Wi enlightens how suitable has been the action selection (from
DS) concerning the reinforcements received by the MultiA agent
itself, but also to the remaining feelings, as empathy. The last is
represented by S4,ip: feeling number 4 of MultiA agent i for
neighbor p; in Figure 1, see feeling number 4. We designed the
empathy to reﬂect the impact of the action selection of MultiA
on its neighbors. Therefore, the higher the empathy for a speciﬁc
neighbor p, the lower is Wi, all the remaining variables that
feed Wi kept constant. This means that the MultiA agent may
not have been selecting its actions appropriately, since it may
be affecting negatively on this particular neighbor p, thus high
empathy levels are an indication of inadequate action selection.
Selected actions are considered adequate when they do generate
positive reinforcements while not provoking high empathy levels.
If p ﬁres high empathy on i, p may be getting low reinforcements
and therefore its neighbors, such as i, should check their actions.
The CS delivers ﬁve sets of data to the PS: 1. the current
number of neighbors of agent i; 2. the reinforcements history
of agent i; 3. the number of times agent i has interacted with
each neighbor p; 4. the number of times interacting with p ended
up in positive reinforcements; 5. the CS accesses to the current
emotions from PS. Then, the EM (from the CS) produces Wpi:
an assumption on i related to the current condition of neighbor p.
If p is supposed to be facing low reinforcements, MultiA may
have its empathy raised to select less selﬁsh actions and try to
cooperate with the raise of the reinforcements of p. Regarding
Wpi, the CS delivers it to the PS, where it will stimulate the
social emotion Es
4,ip (social emotion number 4 of agent i for
neighbor p; in Figure 1, see social emotion number 4), then
reaching the empathy feeling S4,ip. The PS will then calculate its
artiﬁcial emotions, feelings and Wi. In the PS the emotion Es
4,ip
is fed both by Wpi, and by the empathy feeling by p right after the
last interaction with p, a residual value from the past inﬂuencing
the current emotion. Then, right before a new interaction with p,
the empathy feeling is fed both by the emotions Es
4,ip and Es
3,ip
(social emotion number 3 of agent i for neighbor p; in Figure 1,
see social emotion number 3). The last summarizes the utility of
neighbor p: the average number of times interacting with neighbor
p has resulted in positive reinforcements.
Regarding the EM, a reciprocity assumption works as the
main design concept on generating Wpi - and subsequently
the empathy feeling. Ergo the EM reproduces a reciprocity
assumption: a) due to neighbors mirroring, following a premise
of similarity between agents and neighbors current situation.
Even though we are aware of the controversy relating to mirror
neurons (as in [21]), we used it as motivation on a mechanism
for projecting the agents own emotions to mirror other agents’
condition - thus we avoid explicit data sharing among local agents.
We call that as emotionally reciprocal guidance. Thus, no agent
can observe the neighbors actions or reinforcements, but only
mirror its own emotions on neighbors to make assumptions about
their condition. Therefore, the EM mechanism of generating
Wpi was motivated by the notion of mirror-neurons internally
mirroring the current condition of another agent, then, a set
of the agents own emotions are used to emulate another agent
p situation (before interacting with it) and to provide Wpi; b)
reciprocity on the way those mirrored emotions are going to be
interpreted. We settled the utilitarian calculus from [22] as our
guideline on determining how those mirrored emotions would be
interpreted on the EM. Thence the MultiA agents have a more
sensitive empathy for those agents whose interactions have been
resulting in positive reinforcements (it is neighbor reciprocal).
Furthermore, the MultiA agent is more likely to cooperate if
it has been receiving in general (from its neighborhood) a high
number of positive reinforcements. We also apply a reciprocity
38
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-447-3
HUSO 2015 : The First International Conference on Human and Social Analytics

assumption through the utilitarian design: the ﬁnal value of Wpi
is motivated by reciprocity. Hence neighbors whose interactions
result in positive reinforcements (it only has to be positive; there
is never a comparison between positive reinforcements) tend to
lead to higher empathy levels.
Figure 1. The general scheme of the MultiA Architecture.
In general, those actions related to high empathy are designed
to be avoided, since it is considered that when a neighbor rouses
high empathy it is because the agent itself may be disturbing the
performance of the others. The consequence is that MultiA is
designed to seek those actions that will not increase its levels
of empathy. The CS applies three-layer feed-forward artiﬁcial
neural networks (ANNs), one for each action, and the Q-Learning
reinforcement learning algorithm [23] to estimate the resulting
Well-Being (provided from the single output unit) if, concerning
the current emotions (input space from PS) and bias, the equiv-
alent action is to be selected. Each ANN is trained in accordance
with the outcome driven by the execution of its corresponding ac-
tion [24] through the Backpropagation algorithm [25] employing
Wi as the target value. The CS will then deliver the outputs from
all ANNs to the DS to choose an action with the highest output
(in case of existing outputs with the same value, selection will
be random), except during the beginning of a simulation, when it
will be use a high exploration rate for the state(emotion)-action
space.
III.
EXPERIMENTAL SETTING
A. The Evolutionary Game: Task and Changes on Topology
Relations in natural societies can be analysed through public
goods games analogies, with public goods characterized by two
main features: they are public and they are not wasted through
consumption. It can be shown that those games generalize, to an
arbitrary number of individuals, the Prisoner’s Dilemma Game
(PDG) [26]. In natural societies and games that use them as a
metaphor, situations described by unfair relations are common: an
agent taking advantage of another agent social commitment. The
last may be required to accomplish the best social outcome: for
a pack of non-solitary animals, it may be crucial to go hunting
together, each one selecting those actions that only as a group
will result on the best social outcome. Since cooperating with
the group usually inquires a cost to the cooperator and defectors
beneﬁt from common resources [27], a dilemma emerges between
each one’s self-interest and the group’s maintenance.
The performance of MutiA agents and the changes on neigh-
borhoods promoted by the agents interactions that we intend to
analyze will follow from the generalized PDG model of [28]: it
starts with a network where agents are represented by the nodes
while the neighborhood by the links between the nodes. Evolu-
tionary games are described in [28] and related to the emergence
of cascading failures: agents (nodes) and links being eliminated
from a network as the matching result of agents actions. The
outcome from a few agents (and its links) elimination may cause
another agent elimination. The process can continue until the
complete elimination of all links and agents. Wang et al. [28] also
present a generalized PDG model where connected agents through
a link (considered neighbors) choose to defect or to cooperate and
the matching strategies will deﬁne the nodes reinforcement. Once
all agents have interacted with each and every neighbor, a match
ends and the individual sum of reinforcements of each agent is
calculated, therefore a match is deﬁned by all agents interacting
only once with every neighbor. Matches are repeated in sequence
until the network topology stops changing as the consequence of
agents interactions.
Agents strategies are established before the beginning of the
ﬁrst match, but at the end of each match there is a probability
of agents changing their strategy by imitating a neighbor with
high ﬁnal reinforcement. Just before that, the agents that did
not get enough cooperative actions from neighbors (then low
reinforcements) are eliminated, causing changes on the network
topology. If unilateral defection (one agent cooperates and the
other defects) renders a higher reinforcement value than the other
matching strategies, higher will also be the probability of a coop-
erating node imitating a defective neighbor. Straightforwardly the
defective strategy can spread to the network in such a way that it
causes a cascading failure effect: cooperative agents simply being
eliminated and their elimination causing neighbors elimination (of
both defectors and cooperators).
B. Environment: Networks Initial Topology
As long as networks can be used as metaphors to represent
diverse systems [29], the environment where our agents will
try to accomplish the task will be delineated by them. In the
literature there are different models to construct networks, each
one of them ensuring different features emerging from the model
application. Indeed the model selection has to ﬁt in the network
usage. Insofar as we want to mimic moral behavior (many agents
from diverse neighborhoods interacting among themselves), it is
relevant to apply a model that provides high clustering and long-
range connections.
Our undirected networks were constructed through the grow-
ing networks model proposed in [30], supposed to unify certain
features of real networks, as a power-law degree distribution
( [31] [32]) and the small world effect - high neighborhood
clustering and short average distance between the nodes. A power-
law degree distribution typically results from a network growing
process called preferential attachment, often displayed by real
networks [33]: once a growing network is about to receive a
new node, the ones that already have more links are more likely
to be connected to the new node. Wherefore, the node age in
the network is relevant on deﬁning the number of links it will
have [34]. According to Klemm and Egu´ıluz [35] ( [30] derived
from it), its model of generating scale-free networks presents real-
world properties, as a negative correlation between the age of a
node and its link attachment rate. On the other hand, the basic
reference on growing scale-free networks [36], would present
a mean attachment rate positively correlated with age (as the
attachment rate is proportional to the degree and the oldest nodes
start accumulating links since the beginning of the construction
of the network). Additionally, in opposition to Barab´asi and
39
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-447-3
HUSO 2015 : The First International Conference on Human and Social Analytics

Albert [36], the growing networks from Klemm and Egu´ıluz [35]
preserve the degree distribution (still power-law), even if all but
the most recently grown part are disregarded. The model [35]
produces high clustering scale-free networks and, even though its
clustering is higher than in regular lattices, its topology is similar
to one-dimensional regular lattices.
The model presented in [35] may include long-range con-
nections (originating from [30]), aiming to obtain small path
length [29] while holding the original properties of high clustering
and scale-free degree distribution. The guidelines to construct our
tested networks are: consider a network that shall end the growing
process with N nodes (since we are going to test N agents)
and each of them will be taken as active or deactivated. The
growing process starts with m active nodes completely connected.
Then, until the size of the network grows to N, a new node: 1)is
considered in active state and will be connected to m different
nodes. For each of the m connections, a decision shall be made:
a)the connection will be made to a random active node or b)to a
general random node. The probability of connecting to a general
random node is µ, that case the random node is chosen following
Linear Preferential Attachment; 2)The new node is activated;
3)One of the m nodes is deactivated. The deactivation process
was inspired by a memory idea: in general, the newer nodes in
the network are more likely to receive links than the older ones -
as an example, consider technical papers referencing more recent
works rather than older ones.
To examine the degree correlation of our created networks,
we used the assortativity coefﬁcient ϱ from Newman [37] with
the purpose of studying MultiA agents performance over the
inﬂuence of a) disassortative degree correlation (i.e., negative
values of ϱ, when highly connected nodes have the propensity of
being connected to the nodes that are little connected [38]; and b)
neutral degree correlation (neutral values of ϱ), when there is not
such a propensity, be it to little connected or highly connected
nodes. A common condition given by the disassortative degree
correlation is the existence of polarized nodes: the ones that have
just a few links but, most importantly, are connected to highly
connected nodes.
IV.
RESULTS
The relevant deﬁnitions and constraints used to describe our
results are:
1. We call game a set of interactions among agents as deﬁned
in the PDG by Wang et al. [28]).
2. An elimination process will always occur at the end of each
match t. Each match will be given by all non-eliminated agents
(represented by the corresponding nodes) interacting with those
agents they are linked with (neighbors). No agent will interact
(choose to defect or cooperate) twice with the same neighbor in
the same match.
3. A simulation is a deﬁned number of matches played in
sequence.
4. The initial number V 1
i of neighbors for each agent i will
be given by the network original topology.
5. Reinforcements are normalized to [−1, 1]. Each agent i has
to end up a match t with an individual sum of reinforcements
rt
i at least equals to Ti. The parameter Ti represents a minimal
individual survival need (Ti falls in the range [0; 1]). The values
of reinforcements resulting from the agents interactions will
follow as from the following example: suppose that the agent
0 initially has 4 links in the network. Then, it has 4 neighbors
(V 1
0 = 4). Agent 0 will be represented by the node zero and
will receive reinforcement 1/V 1
i = 0.25 for mutual cooperation;
2 ∗ 1/V 1
i
= 0.5 for defecting when a neighbor p cooperates
(unilateral defection) and, ﬁnally, zero for mutual defection or
for cooperation vs. defection. We made Ti = 0.5, thus the
cooperative agent will need half of its neighbors cooperating
to avoid elimination and the defective will only need 25% of
it. By receiving a double reinforcement (comparing to mutual
cooperation), the defective agent will get the chance of being
more resilient in the network (when it has cooperative neighbors)
than cooperators. If the agent ends up a match with rt
i < Ti, the
network topology changes: the agent itself and all its connections
are eliminated. Observe that after elimination, concerning those
agents that have fewer neighbors at match t than in the ﬁrst match:
if they follow a cooperative strategy they will never have the
chance of getting the full reinforcement of 1 (as V 1
i will be higher
than the current number of neighbors at match t).
6. The results were collected only when the size of the network
stopped changing (tF matches). Aiming to prevent a massive
elimination of agents during exploration time (from ﬁrst match
until tx) and a small number of upcoming matches (from tx+1
until tB), the real elimination process only starts from match
tB+1. Despite that, the neighbors of those agents that should have
been eliminated during the matches t < tB+1 actually do receive
information about neighbors elimination. That intervention puts
forward some issues, as the mismatch between loss of neighbors
and lower reinforcements (as all neighbors of a given agent i
will be kept on network, allowing the possibility of the full
reinforcement of 1 for cooperation on the next match).
7. In order to present the ﬁnal results relating to the network
topology, we called ρd the percentage of defectors in the ﬁnal
network, and ρc the percentage of cooperators. The percentage of
remaining nodes from the original network (agents that have not
been eliminated) is ρf.
8. The experimental parameters applied on all simulations are:
DS uses a 10% exploration rate, the hidden and output units
from DC use the logarithmic activation function and we applied
a learning rate 0.07 and a momentum term 0.9.
A. Moral Agents and Degree Correlation
We developed two agent versions. Notice that the well-being
Wi (from PS) is calculated with normalizing weights on feelings
so that the ﬁnal value falls in the range [−1, 1]. Thus the weights
have to be set respecting the relevance of each feeling to the
domain. In general, the feeling S1,i is sensitive to the neighbors
elimination and S2,i to the agents own reinforcements. The feeling
S3,i represents the average number of times agent i has been
receiving positive reinforcements and S4,i is the empathy feeling.
The agents are:
•
The MultiA agent designed to rudimentarily mimic
moral agents, with a weight of the empathy feeling over
the Wi value supposed to be considerable. The feelings
weights used on our experiments are: S1,i = 0.4; S2,i =
0.05; S3,i = 0.05; S4,i = −0.5. Thus the empathy feeling
(S4,i) is responsible for half of the value of Wi. The
well-being Wi measures the performance of the MultiA
agent in the environment and, if the empathy reaches high
levels, Wi will be low. That is an indication that probably
the last selected actions may be causing bad outcomes to
neighbor p; therefore, the well-being Wi of agent i should
be low, even though its reinforcements may be high.
•
The MultiAA agent that rudimentarily mimic amoral
agents. We provided theoretical ideas about immoral and
amoral agents in [20] [17]. The amoral agent lacks social
40
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-447-3
HUSO 2015 : The First International Conference on Human and Social Analytics

emotions and feelings fed by them (it also lacks an EM).
Therefore, it has the 6 basic emotions and 2 feelings.
The ANNs from CS were adapted accordingly. For this
version we tested two different weights set on feelings:
S1,i = 0.3; S2,i = 0.7; and S1,i = 0.5; S2,i = 0.5. As
we had better results on the former setting, we used that.
The agents task is to learn to avoid elimination, and that
involves a compromise: learn to accumulate high reinforcements
at the end of each match t (rt
i >= Ti) while avoiding neighbors
elimination. Notice that the outcome (rt
i and neighbors elimina-
tion or not) of each agent i at match t is due to its own actions,
to all its neighbors p actions and also to the neighbors actions of
its neighbors p. That means the strategies cause-consequence can
easily be shadowed: both, defective and cooperative agents may
loose a neighbor, inﬂuencing both agents PS. If the agents have
difﬁculties learning the task on a given network topology, it may
be helpful to increase the exploration rate and make tx and tB
encompass more matches. However, depending on the network
topology, most of the agents will not learn the task at all. Thus,
an insufﬁcient number of agents will learn to cooperate and, given
to the number and/or position of defectors within the network, a
cascading failure effect may occur: all agents will be eliminated.
That is an indication that the matching agent-network properties
should be reconsidered.
Although we are not going to present it here, we already
have preliminary results indicating that MultiA also beneﬁts
from highly connected networks as in [30] (increment on the m
parameter). By increasing m, we are also increasing the number
of links between nodes and likewise the number of completely
connected nodes (from the beginning of the network growing
process). Those preliminary results also make sense with the
EM: a generalized increment on the links number (even if
allowing differences on the nodes degree) may produce so highly
connected nodes that the actual number of each node’s links may
loose its importance (allowing similar environmental condition to
the MultiA agents through emotions mirroring). Herein through
different networks, we explore the relations between small values
of m and the effect of degree correlation on MultiAs failures.
We produced networks with different ϱ by changing the µ value.
Thus by varying the value of µ and keeping m = 1.8% of N,
N = 2000, we built different networks for the Experiment 1
(Exp.1). For each network, we ran 10 simulations for each of
our MultiA and MultiAA agents. Observe that each network
topology is used to deﬁne the agents interactions. Therefore,
through different network topologies (diverse ϱ given the µ value),
in Figure 2 we show both agents simulations that did not lead to
a failure (when it occurs a cascading failure effect). Those had
ρf > 90%N, ρd < 50% and ρc >= 50%.
With the purpose of enlightening our results of Figure 2,
some observations shall be made: 1. For µ > 0: if we increase
the value of µ, nodes with more links will be more likely to
be linked to the new nodes in the growing network (making
it possible that deactivated nodes with more links, but older
in the network, receive the new links). If µ = 1, the model
becomes [36], then ϱ tends to a zero value. Regarding the
preferential attachment brought through µ > 0: by increasing µ,
older deactivated nodes have the chance of receiving new links,
then forming new connections between different neighborhoods:
older nodes with high number of connections keep receiving
new links. The process of avoiding establishing a number of
links once the node is deactivated allows a tendency to a neutral
assortativeness - as µ = 1 should return to the model from
Barab´asi and Albert [36], [39]; 2. For µ < 1: if we diminish µ,
the newest nodes from the list of active ones are given the chance
to connect to new nodes (since the selection from the active list
is random). When µ = 0, it returns to the original model [35].
The memory process of “forgetting” nodes (deactivating them)
promotes networks tending to a disassortative degree correlation
(negative values of ϱ). The cross-over (0 < µ << 1) between the
two models ( [36] and [35]) would reproduce the real networks
features.
Considering the reciprocity assumption from the EM, the
MultiA agents will achieve better results in networks that pro-
vide similar environmental conditions to the neighborhood (as
neighbors with similar number of links). By the same reason, its
performance is affected by neighborhoods with highly different
degree distribution. As shown in Figure 2a), MultiA agents
were able to solve the task on networks with a tendency to a
neutral assortativeness. Polarized agents (from networks with a
negative ϱ) are less impacted by a shadow effect (cooperators
loosing neighbors the same way as defectors) than those agents
with higher number of links. But given the agents neighborhoods
differences (number of neighbors) in those networks described
by negative ϱ, polarized and non-polarized MultiA agents EM
ends up mirroring neighbors inadequately, leading to a cascading
failure effect. On the contrary, a most similar neighborhood
beneﬁts the emotional mirroring as well as the EM, preventing
from high oscillations in the learning process (see Figure 3a)
allowing the agents to solve the task on networks with larger
ϱ.
Considering Figure 2b), it is important to note that no agent
can observe the neighbors actions or reinforcements. MultiAA
agents are able to solve the task in environments where the match-
ing of strategies in the way of conquering high reinforcements and
neighborhood upkeep is easier. Then, as mutual defection renders
reinforcement zero, agents learn to match their action selections
in a way of avoiding mutual defection (then, cooperating). When
different neighborhoods are connected (by long-range connections
through high values of µ), more opportunities of matching agents
strategies are created. Thus, on the networks with an almost non-
negative ϱ, the neighborhoods tending to a cooperative strategy
will be affected by connections with unstable or more defective
neighborhoods. On those environments described by topologies
that repeatedly allow different scenarios (reinforcements and
agents elimination) for the same strategy, the agents will be
inﬂuenced by the strategies combinations possibilities, then the
shadow effect will strongly impact on agents DS. That makes it
harder for the MultiAA agents to solve the task. As Figure 2b)
shows, MultiAA agents were able to solve the task on networks
with high disassortative degree correlation (negative ϱ). The polar-
ized agents (fewer neighbors equals to fewer matching strategies
possibilities) can ﬁnd more easily the matching strategy that keeps
their internal variables balance (feelings). From the polarized
nodes strategy establishment, it becomes easier for its neighbors
to deﬁne their own strategies. Then, a positive cascading effect
happens: once the polarized agents have deﬁned their strategies,
it is easier for the highly connected agents to ﬁnd their own
strategies.
B. Agents Learning Dynamics
We run two more experiments (using two networks from the
ﬁrst experiment) to present our agents learning dynamics. As
MultiA and MultiAA had different performances for the tested
values of µ (given m and N) from Exp.1, we used a network with
an almost neutral ϱ on simulating the former (Exp.2, Figure 3)
and a network with a negative ϱ on the later (Exp.3, Figure 4).
Regarding Exp.2, Figure 3, the parameters used to create the
41
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-447-3
HUSO 2015 : The First International Conference on Human and Social Analytics

Figure 2. Degree correlation and agents performances: agents simulations that
did not lead to a failure. a)MultiA agents; b)MultiAA agents. The parameters
we used: Ti = 0.5, N = 2000, m = 1.8%N.
network are: Ti = 0.5, N = 2000, m = 1.8%N, µ = 0.95.
Given the µ value, the network had ϱ = −0.05. The parameters
used to create the network of Exp.3, Figure 4, are: Ti = 0.5,
N = 2000, m = 1.8%N, µ = 0.1. Given the µ value, the
network had ϱ = −0.76. Observe that in Figures 3 and 4 we
do not consider the agents that should have been eliminated from
the ﬁrst match until tb (real elimination starts at tb+1). Both the
mean reinforcement ¯R and the mean cooperative neighborhood
size ¯C were averaged over 20 simulations, and the values of tx
and tB where set according to the experimental minimum possible
values to allow learning while preventing from a cascading failure
effect.
The defective strategy showed itself to be a bad decision when
there is mutual defection and when it causes neighbor elimination
(or only an indication of it, during the ﬁrst tB matches). The real
elimination (from match tB+1) contributes to the network deterio-
ration, as that may cause a cascading failure effect: the elimination
of cooperative nodes causing its neighborhood elimination, and so
on. As the outcome of each agent results from its own actions
face to the action selection of its neighborhood, the strategy
cause-consequence link can easily be shadowed: both defective
and cooperative agents may lose neighbors. Another issue is that
agents have access to the number of eliminated neighbors only
when the match ends. However, once the agents strategies stabilize
in such a way to prevent agents elimination (i.e., cooperators and
defectors do have a sufﬁcient number of cooperating neighbors),
the elimination process ends. The consequence of that is that a
bad effect of the defective strategy (neighbors elimination) will
stop inﬂuencing the agents DS. It is worth considering that there
is no local agent (both MultiA and MultiAA) access to neighbor
reinforcements — in the case of MultiA, the EM tries to mirror
the neighbors current state before the DS selects an action. The
successive interactions among agents will impact on the PS, ergo
on the whole architecture, and the action selection (both in the
same match and from a match to another) will be inﬂuenced by
previous interactions.
To better interpret the results in Figures 3 and 4, a general
analysis is required: a similar drop on ¯C and ¯R at the same match
indicates mutual defection or cooperators elimination. If both ¯C
and ¯R increase, mutual defection is being replaced by unilateral
defection or mutual cooperation. If ¯C increases and ¯R drops,
unilateral defection is being replaced by mutual cooperation.
When ¯C drops and ¯R increases, mutual cooperation is being
replaced by unilateral defection. Notice that if an agent with
fewer neighbors defects and some of its neighbors cooperates,
this defector will accumulate high reinforcements more easily
than a highly connected defector: e.g., if the defector agent 0
has 2 neighbors (node 0 has two links to other nodes) and one of
them cooperates, agent 0 will easily get the full reinforcement of 1
(V 1
0 = 2 and unilateral defection for agent 0 will be 2∗1/V 1
0 = 1).
On the other hand, if the defector agent 5 has 100 neighbors and
just one of them cooperates, agent 5 will get the reinforcement
of 0.02 (V 1
5 = 100 and 2 ∗ 1/V 1
5 = 0.02).
The MultiA agent performance in Exp.2 is illustrated in
Figure 3. Between the ﬁrst and third matches, once the agents get
higher reinforcements for unilateral defection, they start to defect.
Then mutual defection results in lower reinforcements and elim-
ination. At the same time, unilateral defection ( ¯C dropping more
heavily than ¯R) results in information of neighbors elimination
also, just on the side of cooperators, lower reinforcements. During
exploration (until match tx), even though the agents are still
changing strategies, the EM prevents from high oscillations in the
learning process. Right after exploration ends, some agents em-
phasize the defection strategy, leading to the unilateral defection
(increasing ¯R and dropping ¯C), causing neighbors elimination
(still not actually applied) and — as a reciprocity utilitarian
effect — mutual defection. That causes even more cooperating
agents elimination (since ¯C becomes smaller than 50% and ¯R
also drops). Overcoming a shadow effect, from match 44 the
defective agents start changing to the cooperative strategy, even
before real elimination begins. The elimination induces part of the
defective agents to try the cooperative strategy ( ¯C increases). But
at the end (as the elimination process is done since every agent
already has ri >= Ti), they return to the defective strategy and
all agents end up stabilizing their strategies, with the following
ﬁnal percentages of remaining agents, defectors and cooperators,
respectively: ρf = 99%, ρd = 36% and ρc = %63.
Note that the empathy feeling impacts less on the action
selection of those MultiA agents that have been surrounded
by agents with which the interactions did not render positive
reinforcements. The consequence is that the EM will repeatedly
send low levels of empathy and make the agent prioritize its other
feelings, thus learning to select actions in accordance to such
other feelings (specially the basic ones). Therefore, the utilitarian
reciprocity design from the EM will allow a more selﬁsh action
selection. On the other hand, MultiA agents that did have enough
positive interactions will have a neighborhood-driven empathy
feeling, following an utilitarian reciprocity policy on selecting
less selﬁsh actions.
To see MultiAA agents performance in Exp.3, note Figure 4.
As the MultiAA agents are not inﬂuenced by the empathy feeling
and by the EM, the changing strategies oscillations are very
clear by comparing ¯C and ¯R. Driven by reinforcements ﬁrst
and then by keeping neighbors, those agents go by matching
strategies. When exploration ends, agents turn to the defective
42
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-447-3
HUSO 2015 : The First International Conference on Human and Social Analytics

Figure 3. Results for almost neutral degree correlation networks composed of
MultiA agents. a)Mean cooperative neighborhood size ¯C. b)Mean
reinforcement ¯R. MultiA parameters: tx = 39; tB = 49.
strategy leading to mutual defection and information on neighbors
elimination. That makes some agents try the cooperative strategy.
Then, the actual elimination causes a high and fast change on
defecting agents strategy — before that, observe that ¯C was
kept below the minimum required value to prevent cooperating
agents from elimination (50% of cooperating neighbors). The
low value of
¯C kept until the real elimination begins does
not cause a cascading failure effect, due to the disassortative
network topology: polarized agents learn fast and stabilize their
strategies and the positive cascading effect, with the following
ﬁnal percentages of remaining agents, defectors and cooperators,
respectively: ρf = 98%, ρd = 36% and ρc = %62.
V.
CONCLUSION
We described a bio-inspired computational multiagent archi-
tecture that considers an artiﬁcial morality component and pre-
sented both its moral (MultiA) and amoral (MultiAA) versions.
Regarding the reciprocity paradigm over the MultiA design, it
prevented a cascading failure effect on networks described by an
almost neutral degree correlation, aiding the agents on being more
successful on mirroring neighbors condition. The amoral version
prevented a cascading failure effect on networks described by a
negative degree correlation, but that was due to its reinforcement
seeking priorities, game dynamics and network topology. The
comparison between both agents versions empirically conﬁrms
the inﬂuence of the empathy model on MultiA Decision System.
As future work, we intend to study the performance of MultiA
over the inﬂuence of different tasks and positive assortativeness
(the tendency of highly connected nodes being also connected
among themselves [37]).
Consideration should be given to the fact that technologies are
increasingly present in our daily life, progressing to a reality in
which our connection with the artiﬁcial will be so deep that it will
no longer make sense to distinguish between natural and artiﬁcial
experiences. Thus, we also have the purpose of exploring our
Figure 4. Results for disassortative networks composed of MultiAA agents.
a)Mean cooperative neighborhood size ¯C. b)Mean reinforcement ¯R. MultiAA
parameters: tx = 39; tB = 42.
hypothesis from [17] regarding a hybrid agent that can trigger
both moral and immoral behavior, e.g., autonomously activate
moral action policies with biological creatures, and immoral
actions otherwise. Hence, it may be relevant an artiﬁcial agent
able to simulate a moral behavior in general social or domestic
assignments, e.g., monitoring highly dangerous criminals, people
in quarantine or in other context, where there are social dilemmas
to deal with. Furthermore, the artiﬁcial morality component could
be implemented as a resource in argumentation-based negotiation
in multiagent systems.
ACKNOWLEDGMENTS
The authors would like to thank CNPQ and FAPESP for the
ﬁnancial support.
REFERENCES
[1]
M. Tomasello, “Human culture in evolutionary perspective,” Advances in
culture and psychology, vol. 1, 2011, pp. 5–51.
[2]
M. Tomasello and A. Vaish, “Origins of human cooperation and morality,”
Annual Rev. of psychology, vol. 64, 2013, pp. 231–255.
[3]
M. Montaigne, “Essays: of custom; we should not easily change a law
received. (De la coustume et de ne changer ais´ement une loy receue),” vol.
I, XXIII, 1950 (1580).
[4]
——, “Essays: Apology for Raimond Sebond. (Apologie de Raymond
Sebond),” vol. II, XII, 1950 (1580).
[5]
S. Empiricus, Outlines of scepticism (Pyrrhoniae Hypotyposes, PH). Cam-
bridge U. Press, 2000 (160-210AD).
[6]
Aristotle, Aristotle’s Politics.
Chicago U., 2013 (350BC).
[7]
J. Rousseau, The Social Contract.
Penguin, 1971 (1762).
[8]
T. Hobbes and E. Curley, Leviathan: with selected variants from the Latin
edition of 1668.
Hackett Publ., 1994, vol. 2.
[9]
N. Machiavelli, The Prince.
U. of Chicago, 1985 (1532).
[10]
A. Dam´asio, Looking for Spinoza: Joy, sorrow, and the feeling brain.
Random House, 2004.
[11]
F. De Waal, The age of empathy: Nature’s lessons for a kinder society.
New York: Harmony, 2009.
43
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-447-3
HUSO 2015 : The First International Conference on Human and Social Analytics

[12]
D. Proctor, S. Brosnan, and F. De Waal, “How fairly do chimpanzees play
the ultimatum game?” Communicative & integrative biology, vol. 6, no. 3,
2013, p. e23819.
[13]
A. Dam´asio, “Descartes’ error (new york: Putnam),” 1994.
[14]
S. Gadanho and L. Cust´odio, “Asynchronous learning by emotions and
cognition,” in Procs. of the seventh Int. Conf. on simulation of adaptive
behavior on From animals to animats.
MIT Press, 2002, pp. 224–225.
[15]
S. Gadanho, “Learning behavior-selection by emotions and cognition in a
multi-goal robot task,” The J. of Machine Learning Research, vol. 4, 2003,
pp. 385–412.
[16]
R. Sun and T. Peterson, “Autonomous learning of sequential tasks: experi-
ments and analysis.” IEEE Transactions on Neural Networks, vol. 9, no. 6,
1998, pp. 1217–1234.
[17]
F. Eliott and C. Ribeiro, “A computational model for simulation of moral
behavior,” in Procs. Of the I. Conf. on Neural Computation Theory and
Applications (NCTA-2014).
SCITEPRESS (Science and Technology
Publications), 2014, pp. 282–287.
[18]
G. Di Pellegrino, L. Fadiga, L. Fogassi, V. Gallese, and G. Rizzolatti,
“Understanding motor events: a neurophysiol. study,” Experimental brain
research, vol. 91, no. 1, 1992, pp. 176–180.
[19]
G. Rizzolatti, L. Fadiga, V. Gallese, and L. Fogassi, “Premotor cortex and
the recognition of motor actions,” Cogn. brain research, vol. 3, no. 2, 1996,
pp. 131–141.
[20]
F. Eliott and C. Ribeiro, “Emergence of cooperation through simulation of
moral behavior,” in Hybrid Artiﬁcial Intelligent Systems. HAIS 2015: 10th
I. Conf. on Hybrid Artiﬁcial Intelligence Systems, Bilbao, Spain. Lecture
Notes in Artiﬁcial Intelligence, vol. 9121.
Springer International Pub.,
2015, pp. 200–212.
[21]
G. Hickok, The myth of mirror neurons: the real neuroscience of commu-
nication and cognition.
WW Norton & Company, 2014.
[22]
J. Bentham, An introduction to the principles of morals and legislation.
Courier Dover Publications, 2007 (1789).
[23]
C. Watkins, “Learning from delayed rewards,” Ph.D. dissertation, Kings
College, UK, 1989.
[24]
L. Lin, “Reinforcement learning for robots using neural networks,” DTIC
Document, Tech. Rep., 1993.
[25]
P. Werbos, “Beyond regression: New tools for prediction and analysis in
the behavioral sciences.” Ph.D. dissertation, Harvard, 1974.
[26]
J. Wakano and C. Hauert, “Pattern formation and chaos in spatial ecological
public goods games,” J. of theoretical biology, vol. 268, no. 1, 2011, pp.
30–38.
[27]
L. Wardil and C. Hauert, “Origin and structure of dynamic cooperative
networks,” Scientiﬁc reports, vol. 4, 2014, pp. 5725: 1–6.
[28]
W. Wang, Y. Lai, and D. Armbruster, “Cascading failures and the emergence
of cooperation in evolutionary-game based models of social and economical
networks,” Chaos: An Interdisciplinary J. of Nonlinear Science, vol. 21,
no. 3, 2011, pp. 033 112: 1–12.
[29]
D. Watts and S. Strogatz, “Collective dynamics of ‘small-world’networks,”
nature, vol. 393, no. 6684, 1998, pp. 440–442.
[30]
K. Klemm and V. Egu´ıluz, “Growing scale-free networks with small-world
behavior,” Physical Rev. E, vol. 65, no. 5, 2002, p. 057102.
[31]
D. Price, “Networks of scientiﬁc papers.” Science (New York, NY), vol.
149, no. 3683, 1965, pp. 510–515.
[32]
H. Simon, “On a class of skew distribution functions,” Biometrika, 1955,
pp. 425–440.
[33]
S. Dorogovtsev, J. Mendes, and A. Samukhin, “Structure of growing
networks with preferential linking,” Phys. Rev. letters, vol. 85, no. 21, 2000,
p. 4633.
[34]
S. Dorogovtsev and J. Mendes, “Evolution of networks with aging of sites,”
Phys. Rev. E, vol. 62, no. 2, 2000, p. 1842.
[35]
K. Klemm and V. Egu´ıluz, “Highly clustered scale-free networks,” Phys.
Rev. E, vol. 65, Feb 2002, p. 036123.
[36]
A. Barab´asi and R. Albert, “Emergence of scaling in random networks,”
science, vol. 286, no. 5439, 1999, pp. 509–512.
[37]
M. Newman, “Mixing patterns in networks,” Physical Rev. E, vol. 67, no. 2,
2003, p. 026126.
[38]
S. Maslov and K. Sneppen, “Speciﬁcity and stability in topology of protein
networks,” Science, vol. 296, no. 5569, 2002, pp. 910–913.
[39]
M. Newman, “Assortative mixing in networks,” Phys. Rev. letters, vol. 89,
no. 20, 2002, p. 208701.
44
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-447-3
HUSO 2015 : The First International Conference on Human and Social Analytics

