Fairness Improvement of Multiple-Bottleneck Flow in Data Center Networks
Kenta Matsushima, Yuki Tanisawa and Miki Yamamoto
Faculty of Engineering Science
Kansai University
3-3-35 Yamate-cho, Suita-shi, Osaka, Japan
Email: k506903, k284455, yama-m@kansai-u.ac.jp
Abstract—Quantized Congestion Notiﬁcation (QCN), discussed in
IEEE 802.1Qau, is one of the promising layer 2 congestion control
methods for data center networks. Data center network funda-
mentally has symmetric structure and links are designed to have
high link utilizations. So, data center ﬂows probably pass through
multiple bottleneck links. QCN reduces its transmission rate
with each congestion notiﬁcation feedback reception, which might
cause excessive regulation of transmission rate. We have already
proposed QCN with Bottleneck Selection (QCN/BS) for multicast
communications in data center. QCN/BS is originally proposed
for multicast communications, but it can also be applied to unicast
communication with multiple bottleneck points. QCN/BS selects
the worst congestion level and the transmission rate of the sending
device is calculated exclusively according to feedback from the
selected switch. In this paper, we preliminary evaluate QCN/BS
in unicast communications with multiple bottleneck points. Our
preliminary evaluation reveals that QCN/BS can resolve this
excessive rate regulation problem but has new fairness problem
for long-hop ﬂow. To resolve this fairness problem, we integrates
QCN/BS and our proposed Adaptive BC_LIMIT. In Adaptive
BC_LIMIT, parameter BC_LIMIT is adaptively decided so that
the time interval between QCN rate increase is independent
of transmission rate. With rate increase interval independent
of transmission rate deﬁned in the original QCN as well as
rate decrease dependent on it deﬁned in our proposed Adaptive
BC_LIMIT, convergence of fair rate allocation among ﬂows
sharing a bottleneck link is accelerated. Our simulation results
show that our proposed integration of QCN/BS and Adaptive
BC_LIMIT signiﬁcantly improves fairness problem for unicast
communications with multiple bottleneck points in data center
networks.
Keywords–data center; QCN; congestion control; fairness.
I.
Introduction
Eﬃcient networking is extremely important when a large
number of servers are connected in a high-speed, high-capacity
network, which is typical situation of data centers supporting
cloud computing. Many large-scale data centers implement
both a storage area network (SAN) and a local area network
(LAN). A SAN generally uses a Fiber Channel for high
reliability. Fiber Channel technology is more expensive than
Ethernet technology, and the management cost of maintaining
the two types of network is high. Thus, the integration of
LANs and SANs with Ethernet technology has been under
standardized by the IEEE 802.1 Data Center Bridging Task
Group [1]. The convergence of data center networking is
expected to yield low power consumption and simplify hard
wiring [2].
With LAN/SAN integration, conventional SAN perfor-
mance, i.e., high reliability with low frame loss, will be
provided on Ethernet technology. A multi-hop Ethernet con-
ﬁguration is generally used to accommodate the large number
of end systems (servers) in current data center networks.
Consequently, heterogeneous traﬃc causes congestion in traﬃc
hot spots. Quantized Congestion Notiﬁcation (QCN) [3] [4]
is one of the promising congestion control methods for data
center networks. QCN can keep queue length at a low value
because the sending device determines a transmission rate
according to feedback from a switch.
Data center network generally has symmetry structure,
e.g., Fat Tree [5] and VL2 [6]. Currently, traﬃc engineering
techniques for data center networks, such as MicroTE [7] and
Penalized Exponential Flow-spliTing algorithm (PEFT) in data
center [8], have been proposed to improve oversubscription
environment in data center networks. Purpose of these traﬃc
engineering techniques is generally minimizing maximum link
utilization, which might increase the number of highly utilized
links in data center networks. Symmetric structure with highly
utilized links introduces quite diﬀerent traﬃc feature from
current wide area Internet, multiple bottlenecks on a path. QCN
is feedback-based congestion control and reduces transmission
rate with each feedback reception. With multiple bottleneck
links, QCN might cause excessive rate regulation due to
feedback frames sent by multiple bottleneck points.
This excessive feedback problem typically occurs in mul-
ticast communications. We have already proposed Quantized
Congestion Control with Bottleneck Selection (QCN/BS) [9]
which resolves Loss Path Multiplicity (LPM) problem [10].
In QCN/BS, the sending device identiﬁes the transmission
rate of each bottleneck point and selects the lowest one. Our
preliminary evaluation in this paper shows that QCN/BS can
improve somewhat fairness problem caused by excessive rate
regulation but we discovers curious fairness problem for long-
hop ﬂow. In multiple bottlenecks situation, frame departure
process from a highly utilized bottleneck link (in QCN, link
utilization generally grows up to 0.99 [3]), is groomed. This
means inter-arrival time to a next bottleneck link of this ﬂow is
almost discrete (exactly it is not discrete due to 0.99 utilization
and frame length ﬂuctuation: with 1.0 utilization and ﬁxed
frame size, it is completely discrete). When discrete arrival
is mixed with a ﬂuctuated ﬂow, i.e., ﬂow encountering the
ﬁrst bottleneck point, discrete arrival sees slightly large queue
length. This slight diﬀerence in measured queue length causes
slightly more feedback reception in long-hop ﬂow.
The original QCN rate calculation algorithm has a ten-
dency of keeping unfair situation [11]. We proposed adaptive
BC_LIMIT [11] where parameter BC_LIMIT is adaptively
decided according to current transmission rate of ﬂows (details
of adaptive BC_LIMIT is explained in section III). This
parameter setting accelerates convergence of fair rate allocation
among ﬂows sharing a bottleneck link. In this paper, we
integrate our proposed two algorithms, QCN/BS and Adaptive
BC_LIMIT, in order to resolve unfair problem suﬀered in
long-hop ﬂow. Our simulation results show that our proposed
103
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

integrated algorithm signiﬁcantly improves fairness among
ﬂows in multiple-bottleneck situation.
The paper is structured as follows. In Section II, we
review the QCN and QCN/BS algorithm and show preliminary
performance evaluation of QCN and QCN/BS for multiple-
bottleneck ﬂow. Section III gives detailed description of our
proposed integrated QCN/BS. In Section IV, we present the
simulation results. Section V concludes our paper.
II.
QCN/BS for Multiple Bottleneck Points
A. QCN
QCN is a Layer 2 congestion control method for multi-
hop Ethernet, and its standardization is discussed mainly in
IEEE 802.1 Qau. The QCN algorithm is based on a congestion
notiﬁcation from a switch. A QCN switch observes current
queue length and calculates a feedback value when a data
frame arrives. When congestion occurs, as identiﬁed by the
calculated feedback value, the switch sends a feedback frame
with a certain probability to the source. When a feedback
frame is received, the source decreases the transmission rate
according to the feedback value. If no feedback frame is
received, the transmission rate is increased according to the
three-phase rate increase algorithm.
The QCN algorithm works at Reaction Points (RPs) and
Congestion Points (CPs). RPs and CPs represent QCN source
and switch dynamics, respectively. These behaviors have been
described in greater detail in [12].
The CP Algorithm
The CP calculates its feedback value as follows:
Fb = (Qeq − Q) − w(Q − Qold),
(1)
where Fb is the feedback value, Q is the current queue length
of the corresponding switch, Qeq is the target queue length,
Qold is the queue length at the time of the previous feedback
transmission, and w is a constant weight value.
The ﬁrst component on the right side of 1 shows how much
smaller the current queue length is than the target queue length,
the second component shows how much the queue length has
decreased.
When a CP receives a data frame with a negative feedback
value, it returns the feedback value to the corresponding RP.
To avoid incurring overhead, the CP returns the feedback value
with a certain probability.
The RP Algorithm
The behavior of the RP is shown in Figure 1. The rate increase
phase is divided into three sub-phases: Fast Recovery, Active
Increase, and Hyper-Active Increase. Each RP maintains four
Figure 1. behavior of the RP.
variables for its rate control: Current Rate (CR), Target Rate
(TR), Byte Counter, and Time Counter. CR is the current
transmission rate, and TR is the transmission rate that is the
goal for CR; it is always larger than or equal to CR. The
Byte Counter is incremented by 1 whenever the RP sends a
mixed byte value (denoted BC_LIMIT = 150 KB in pseudo
QCN code [12]). The Time Counter is also incremented by 1
whenever a mixed amount of time has passed (15 ms in [12]).
The behaviors of RPs are described below.
Rate Decrease. Whenever a feedback frame is received, the
RP ﬁrst activates the rate decrease phase, and both the Byte
Counter and Time Counter are initialized, i.e., reset to 0. Just
after receiving feedback, TR and CR are changed as follows:

TR ← CR
CR ← CR(1 − Gd|Fb|),
(2)
where Gd is a constant that satisﬁes Gd|Fbmax| = 0.5.
Fast Recovery. In the Fast Recovery phase, CR is increased
rapidly just after rate reduction if no feedback is received.
Whenever the Byte Counter or Time Counter is incremented
by 1 (after the RP sends data frames of BC_LIMIT KB or the
timer spends 15 ms) CR is recovered as follows:
CR ← 1
2(CR + TR).
(3)
Active Increase. When the Byte Counter or the Time Counter
reaches 5, the Fast Recovery phase transits to the Active
Increase phase. The rate increase in this phase is slower than
that in the Fast Recovery phase because the transmission rate
might be very close to the rate at which congestion occurred
(and feedback was received). In this phase, whenever the Byte
Counter or Time Counter is incremented by 1, TR and CR are
changed as follows:
⎧⎪⎪⎨⎪⎪⎩
TR ← TR + RAI
CR ← 1
2(CR + TR),
(4)
where RAI is a ﬁxed value (5 Mbps). During this phase,
whenever the RP sends half of BC_LIMIT (75 KB) or the
timer records 7.5 ms, the Byte Counter and Time Counter are
incremented by 1.
Hyper-Active Increase. If no feedback is received after Fast
Recovery and Active Increase phases, considerable bandwidth
might be left unused. For example, this occurs, when one
or more sessions that share a bottleneck switch are closed,
and their unused bandwidth is available for other sessions. In
this case, unused bandwidth should be rapidly consumed by
existing session(s). In the Hyper-Active Increase phase, RAI in
4 is replaced by a higher value (50 Mbps) to achieve the rapid
growth of consumed bandwidth. Hyper-Active Increase starts
when both Byte Counter and Time Counter reach 5.
B. QCN/BS
In multicast communications, multiple bottleneck points
(switches) might be observed on tree-shaped multicast trans-
mission path. Taking the ideal protocol for tracking the most
congested path under changing network conditions discussed
in the LPM problem paper [10] into consideration, we pro-
posed modiﬁed QCN for a ﬂow with multiple bottleneck
points, QCN/BS [9].
In QCN/BS, the most congested switch is selected. The
transmission rate of the sending device is calculated from
104
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

Figure 2. Overview of QCN/BS.
Figure 3. Multiplebottleneck Topology.
the feedback frames sent by the selected switch. The sending
device stores the transmission rates for all switches on its
multicast tree. To limit the number of transmission rates
that the sending device has to manage, only the rates for
congested switches should be stored. New transmission rates
are calculated for all feedback frames received. The sending
device selects the lowest transmission rate, which means it
selects the most congested switch. Switches can be identiﬁed
by their MAC address, which is stored in a feedback frame
ﬁeld.
Figure 2 is a schematic diagram of QCN/BS. In con-
ventional QCN, the source (RP) maintains table based Rate
Limiter (RL) for congestion control. The RL table maintains
CR, TR, Byte Counter, and Timer Counter data. In QCN/BS,
to determine the location of the worst switch congestion, the
RL table also needs to maintain RL entries for all congested
switches. Each switch is identiﬁed by a MAC address. The
source (RP) can identify each congested switch and calculate
its transmission rate independently.
For QCN/BS, the source (RP) has been modiﬁed as follows.
•
When a feedback frame arrives
A new transmission rate is calculated for the congested
switch, identiﬁed by its MAC address. The initial value of the
QCN/BS RL table is set as initial value of the convention QCN
RL table [12].
•
When a data frame is transmitted
In QCN/BS, the source updates transmitted bytes for all en-
tries (RL[i].Byte_Counter for all congested switches i); when
the Byte Counter reaches a certain value, the transmission rate
is increased.
In QCN/BS, the sending device does not need to identify
whether a ﬂow is multicast or unicast because QCN/BS only
selects a single bottleneck. This means that QCN/BS can be
applied to multiple bottlenecks on a unicast path.
TABLE I. Simulation Parameters.
Simulator
ns-2.33
Bandwidth
10[Gbps]
the number of Seeds
20
RTT
100[μs]
Packets size
1500[byte]
Queue length
100[pkts]
Qeq
22[pkts]
C. Preliminary Evaluation
In this section, we preliminary evaluate QCN and QCN/BS
in unicast communications with multiple bottleneck points.
We use ns2 [13] as the simulation tool. Figure 3 shows the
simulation model. Flow 1 passes through 3 bottleneck links
each of which is respectively shared with ﬂow2, 3 and 4.
All links have identical bandwidth and operate 10Gbps. sw0,
sw1 and sw2 are independent bottleneck switches and the CP
implemented in these three switches return feedback to the
RP of the corresponding sending devices (server0, 1, 2 and 3).
RTT of all ﬂows is 100μs and the propagation delay of each
link is depicted in this ﬁgure. Other simulation parameters are
listed in Table I. We use greedy model where transmission rate
of the sending device is exactly deﬁned by the QCN algorithm.
For the detailed parameters of QCN, we used the parameters
recommended in [12].
Figure 4 (a) and (b) show the transmission rate character-
istics of the original QCN and QCN/BS, respectively. These
ﬁgures also show the queue length characteristics of three
bottleneck switches. In the original QCN, transmission rate
of Flow 1 (red line) is suppressed just after ﬂow is initiated
(1.0[sec]). Other ﬂows (Flow 2, 3 and 4) can obtain remained
bandwidth, i.e., their transmission rates are signiﬁcantly higher
than Flow1 (Flow 2, 3 and 4 has almost the same transmission
rate and these curves in this ﬁgure are eventually overlapped).
In QCN/BS, transmission rate of Flow 1 is generally close
to other ﬂows, which means link bandwidth is fairly shared
between one-hop ﬂow and three-hop ﬂow having multiple
bottlenecks on its path. Queue length of QCN/BS is slightly
ﬂuctuated but no signiﬁcant overshoot is observed. From
congestion control viewpoint, it is good behavior because
queue length is kept low. As shown in Table II, link utilization
of QCN/BS is generally high, which means QCN/BS does not
over-regulate the transmission rate.
We evaluate the original QCN and QCN/BS for 20 diﬀerent
seeds. For the original QCN, transmission rate of Flow 1 is
signiﬁcantly regulated in all 20 seeds, i.e., all other 19 seeds
have almost the same results as Figure 5(a). QCN/BS has
good shape (close to other ﬂows) in 7 seeds. However, in
our simulation results for QCN/BS, other 13 seeds show too
much regulated transmission rate of Flow 1. Figure 5 shows
transmission rate characteristics for one of these 13 seeds.
So, QCN/BS occasionally improves multi-bottleneck fairness
issues in QCN but is a little far from the satisﬁed solution.
Table III shows the number of received feedback frames for
each ﬂow(these results are obtained for the same simulation
seed as Figure 5). Flow 1 receives more feedbacks than other
ﬂows even though its transmission rate is the lowest (as shown
in Figure 5, Flow 1 has the lowest rate). Figure 6 shows CDF
of queue length of QCN/BS at each congested switch observed
by each ﬂow. At switch 0, ﬂow 1 and 2 observes almost the
same queue length. However, at switch 1 and 2, ﬂow 1 observes
105
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

(a) originalQCN
(b) QCN/BS
Figure 4. Characteristic originalQCN and QCN/BS queue lengths and transmission rate.
(a) originalQCN
(b) QCN/BS
Figure 5. Characteristic originalQCN and QCN/BS.
TABLE II. Bottleneck link utilization.
QCN
QCN/BS
Link Utilization
0.999726
0.999338
slightly larger queue length than one-hop ﬂow (ﬂow 3 and 4 at
sw1 and 2, respectively). This curious situation of observing
larger queue length is the reason for unfair bandwidth sharing.
We try to explain why this situation happens only for Flow 1
by using Figure 7. QCN stabilizes queue length of bottleneck
link around pre-deﬁned target queue and enables high link
utilization, such as over 0.99. Output process of this highly
utilized link (over 0.99) is spaced out with ﬁxed frame length.
For a multi-hop ﬂow, an output process of a link is an input
process of the subsequent switch. So, input process of ﬂow
1 in sw2 and sw3 is normalized with ﬁxed frame length.
Transmission rate of ﬂow 3 and 4 in Figure 5 is deﬁned
by QCN rate control and is slightly ﬂuctuated. When their
transmission rate is slightly smaller than ﬂow 1, ﬂow 1 might
see slightly larger queue size as shown in Figure 7. Flow 1
might see slightly shorter queue size when their (ﬂow 3 or
4’s) transmission rate is slightly larger. So, all ﬂows have a
possibility to see slightly larger queue length. QCN/BS selects
the worst congested switch and adjusts its transmission rate
as a calculated rate for feedbacks sent from this selected
switch. This behavior causes unfair condition for long-hop
ﬂows when compared with short-hop ﬂows even though their
observed queue length has similar tendency. For a long-hop
ﬂow, eventual decrease of queue length at one switch cannot
enforce increase of its transmission rate when another switch is
selected and dominates QCN transmission. However, a short-
hop ﬂow, such as ﬂow 3 and 4 can increase its transmission
rate when it observes decreased queue length. This diﬀerence
for rate increase behavior brings unfair condition for long-hop
ﬂow.
III.
Integrated QCN/BS
In this section, we propose QCN/BS integrated with Adap-
tive BC_LIMIT for resolving fairness issues for long hop ﬂow
in multiple bottleneck situation. First, we explain Adaptive
BC_LIMIT.
A. Adaptive BC_LIMIT
When QCN occasionally falls into ﬂow imbalance (unfair)
situation, original QCN algorithm tends to maintain a state
of this ﬂow imbalance [11]. This is because the increase
and decrease in its transmission rate depends on the ﬂow’s
transmission rate. In all three increase phases, the transmission
rates in 3 and 4 are increased when the counters incremented.
Byte Counter is incremented whenever a ﬁxed byte amount is
transmitted. Therefore, an RP whose current rate is high tend
to have a small interval for rate increases, and an RP with a
low transmission rate has a long interval. Thus, RPs having
a high transmission rate increase their transmission rate more
rapidly than those with a low rate, and an unfair ﬂow state
might be maintained.
To improve this undesirable rate increase behavior, we
proposed Adaptive BC_LIMIT [11]. In Adaptive BC_LIMIT,
106
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

(a) sw0
(b) sw1
(c) sw2
Figure 6. CDF of queue length observed by each ﬂow.
Figure 7. Discrete arrival sees slightly larger queue length.
TABLE III. The number of received feedback frames.
ﬂow1
ﬂow2
ﬂow3
ﬂow4
sw0
6
13
0
0
sw1
27
0
14
0
sw2
26
0
0
14
the value of BC_LIMIT is deﬁned as follows,
BC_LIMIT = K ∗ Current_Rate,
(5)
where parameter K is set to 2.4∗10−4 as deﬁned in our former
paper [11].
In Adaptive BC_LIMIT, the time interval between rate
increases is independent of the current rate of a ﬂow. This
means the opportunities for rate increases are almost the same
for all ﬂows, even if their transmission rates (throughput) diﬀer.
Flows with a high transmission rate have more opportunities
to receive feedback frames leading to rate decreases. This
is because a feedback frame is transmitted (with a certain
probability) back to the source when a data frame arrives at the
CP. Two features of Adaptive BC_LIMIT, i.e., rate increases
independent of transmission rate and rate decreases dependent
on it, accelerate convergence to fair rate allocation among
ﬂows sharing a bottleneck link.
B. QCN/BS Integrated with Adaptive BC_LIMIT
When multiple bottleneck points are located on unicast
path, long-hop ﬂow might see slightly larger queue length
and suﬀers throughput degradation. And QCN algorithm itself
tends to keep unfair situation as explained in the previous
subsection. When QCN algorithm is replaced to Adaptive
BC_LIMIT, unfair condition might be instantly improved to
fair condition.
Algorithm 1 QCN/BS integrated with Adaptive BC_LIMIT
algorithm
Rate Increase Phase
When data transmitted
BC_LIMIT[S Wi] = BC_LIMIT[S Wi] − transmitted_data
if BC_LIMIT <= 0 then
Byte Counter is incremented by 1
BC_LIMIT[S Wi] = K ∗ RL[S Wi].CR (Adaptive BC_LIMIT)
end if
When timer is expired
Time Counter is incremented by 1
When Byte Counter or Time Counter is incremented
if Byte Counter < 5 && Time Counter < 5 then
In Fast Recovery
else
if (Byte Counter => 5 && Time Counter < 5) || (Byte Counter
< 5 && Time Counter => 5) then
In Active Increase
else
if Byte Counter => 5 && Time Counter => 5 then
In Hyper Active Increase
end if
end if
end if
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Rate Decrease Phase
When RP receives feedback from SWi
if SWi already has table RL[SWi] then
RL[S Wi].Byte_Counter and RL[S Wi].Time_Counter are initial-
ized
BC_LIMIT[S Wi] = K ∗ RL[S Wi].CR (Adaptive BC_LIMIT)
RL[S Wi].CR decreases as in the conventional QCN
else
RP creates a new table RL[SWi]
end if
for all SWi do
if Min_rate > RL[S Wi].CR then
Min_rate = RL[S Wi].CR
end if
end for
Transmission_rate = Min_rate
Inspired by this idea, we propose, in this paper, integration
of QCN/BS and Adaptive BC_LIMIT to resolve fairness
problem of long-hop ﬂow in multiple bottleneck condition.
Algorithm 1 shows our proposed QCN/BS integrated with
Adaptive BC_LIMIT. As shown in this algorithm, transmission
rate for each switch sending feedback frame(s) is managed at
107
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

Figure 8. Characteristic QCN/BS integrated with Adaptive
BC_LIMIT queue lengths and transmission rate.
Figure 9. Characteristic QCN/BS integrated with Adaptive
BC_LIMIT transmission rate.
the source (RP). In transmission increase phase, transmission
rate is increased every time transmitted bytes reach BC_LIMIT
(this BC_LIMIT is adaptively computed as shown in (5) ). The
transmission rate of the sender (RP) is deﬁned as the lowest
transmission rate among its managed rates (for all congested
switches).
IV.
Performance Evaluation
In this section, we evaluate our proposed QCN/BS inte-
grated with Adaptive BC_LIMIT. We use the same simulation
model and parameters as section II. Figure 8 shows transmis-
sion rate and queue length characteristics of QCN/BS inte-
grated with Adaptive BC_LIMIT (simulation seed is the same
one as Figure 5). With integration with Adaptive BC_LIMIT,
transmission rate of Flow1 is improved towards fair bandwidth
allocation. Adaptive BC_LIMIT stabilizes all ﬂows transmis-
sion rate towards fair situation, which improves fair bandwidth
allocation to Flow 1. Among 20 simulation seeds, we only
ﬁnd 3 seeds which has slight worse situation. Figure 9 shows
the worst case among these 3 seeds. As shown in this ﬁgure,
even though imbalance between Flow1 and other ﬂows can
be observed in short time period, it can be regained towards
fair condition. Other 17 seeds give quite good fair condition
as shown in Figure 8.
V.
Conclusion
QCN/BS is layer 2 congestion control applicable to mul-
tiple bottleneck points. Originally it has been proposed for
multicast communications where multiple bottlenecks are gen-
erally observed on multicast tree, but QCN/BS can be applied
to unicast communications with multiple bottleneck points.
In the paper, we preliminary evaluate QCN and QCN/BS for
unicast communications in multiple-bottleneck situation. Our
evaluation results show that QCN/BS occasionally improves
fairness issues but cannot resolve unfair condition to long-hop
ﬂow completely. We reveal that the reason for this fairness
issue is that long-hop ﬂow observes slightly longer queue and
receives more congestion feedbacks. To resolve this unfair
situation, we integrate QCN/BS with Adaptive BC_LIMIT,
which accelerates stabilization towards fair condition. Our
simulation results show that our proposed integrated QCN/BS
can signiﬁcantly improve fairness issues for long-hop ﬂow in
multiple bottleneck situation.
References
[1]
“IEEE 802.1 Data Center Bridging Task Group.” http://www.ieee802.org/
1/pages/dcbridges.html. [retrieved : may 2014]
[2]
R. Hays and H. Frasier, “40G Ethernet Market Potential,” IEEE 802.3
HSSG Interim Meeting, Apr. 2007.
[3]
M. Alizadeh, et al., “Data Center Transport Mechanisms: Congestion
Control Theory and IEEE Standardization,” in Proc. Annual Allerton
Conference, Sep. 2008, pp. 1270-1277.
[4]
“IEEE802.1 Qau.” http://www.ieee802.org/1/pages/802.1au.html. [re-
trieved : may 2014]
[5]
M. Al-Fares, A. Loukissas, and A. Vahdat, ʠ A Scalable, Commodity
Data Center Network Architecture,ʡin Proc. ACM SIGCOMM, Seattle,
WA, USA, Aug. 2008, pp. 63-74.
[6]
A. Greenberg, et al.,ʠVL2: a Scalable and Flexible Data Center Network,
ʡin Proc. ACM SIGCOMM, Barcelona, Spain, Aug. 2009, pp. 51-62.
[7]
T. Benson, A. Anand, A. Akella, and M. Zhang,ʠMicroTE: Fine Grained
Traﬃc Engineering for Data Centers, ʡ in Proc. ACM CoNEXT 2011,
Tokyo, Japan, Dec. 2011, pp. 6-9.
[8]
F. Tso and D. Pezaros, ʠ Improving Data Center Network Utilization
Using Near-Optimal Traﬃc Engineering, ʡIEEE Trans. on Parallel and
Distributed Systems, June 2013, Vol.24, No.6, pp. 1139-1148.
[9]
Y. Tanisawa, Y. Hayashi, and M. Yamamoto, ʠ Quantized Congestion
Notiﬁcation for Multicast in Data Center Networks, ʡin Proc. Interna-
tional Conference on Cloud Networking(IEEE CLOUDNET 2012), Paris,
France, Nov. 2012, pp. 51-56.
[10]
S. Bhattacharyya, D. Towsley, and J. Kurose, “The Loss Path Multiplic-
ity Problem in Multicast Congestion Control,” in Proc. IEEE INFOCOM,
Mar. 1999, pp. 856-863.
[11]
Y. Hayashi, H. Itsumi, and M. Yamamoto, ʠ Improving Fairness of
Quantized Congestion Notiﬁcation for Data Center Ethernet Networks,ʡ
DCPerf 2011(The 1st international Workshop on Data Center Perfor-
mance), Minneapolis, USA, June. 2011, pp. 20-25.
[12]
R. Pan, “QCN Pseude Code Version 2.1.” http://www.ieee802.org/
1/ﬁles/public/docs2008/au-pan-qcn-serial-hai-2-1-0408.zip. [retrieved :
may 2014]
[13]
NS2, Networks Simulator, 1991. Available at http://www.isi.edu/
ns-nam/ns/ [retrieved : may 2014]
108
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-365-0
INFOCOMP 2014 : The Fourth International Conference on Advanced Communications and Computation

