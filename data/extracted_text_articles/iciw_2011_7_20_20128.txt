Dynamic Music Lessons on a Collaborative Score Annotation Platform 
Véronique Sébastien, Didier Sébastien, Noël Conruyt 
IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 
University of Reunion Island 
Saint-Denis, Reunion (FRANCE) 
veronique.sebastien/didier.sebastien/noel.conruyt@univ-reunion.fr
 
Abstract 
- 
The 
recent 
progress 
in 
Information 
and 
Communication 
Technologies 
gave 
birth 
to 
advanced 
applications in the field of instrumental e-learning. However, 
most of these applications only propose a limited number of 
lessons on predetermined pieces, according to the vision of a 
single music expert. Thus, this article introduces a web 
platform 
to 
create 
music 
lessons 
dynamically 
and 
collaboratively, with the assistance of a semi-automatic score 
annotation module: @-MUSE.  To do so, we first describe a 
new 
methodology 
to 
design 
such 
a 
platform: 
Sign 
Management. Then, we detail its general architecture as an 
Iterative Sign Base System based on a common practice in 
music learning: score annotation. Lastly, we give various 
algorithms to generate relevant annotations (explanations) on a 
score, based on the analysis of musical patterns difficulty. 
Keywords - e-learning; music; knowledge management; sign 
management; multimedia; annotation; semantic web; ontology; 
digital score;  piano; human-computer interaction; logic; 
inference 
I. 
 INTRODUCTION 
Information and Communication Technology for Education 
(ICTE) expanded rapidly these last years. Indeed more and 
more teachers resort to platforms such as Moodle or 
Blackboard to design their own online courses. While this 
trend is being confirmed in academic subjects such as 
mathematics and languages [7], it remains rare for know-
how transmission and sharing, for instance in the field of 
music learning. Indeed, know-how transmission requires 
heavy multimedia usage and interaction to show the “correct 
gesture” and is thus complex to implement. 
Some instrumental e-learning solutions exist in the form of 
offline tools, such as instructional DVDs (see the technical 
report of E-guitare [16]), or business software (Guitar Pro 
[17], Garage Band [18]). Nevertheless, getting a feedback is 
capital in know-how acquisition (is my gesture or fingering 
correct ?). But few applications try to implement a learner to 
teacher communication axis through video upload and 
commentaries on the web (see the FIGS [19] glosses 
system). 
Still, the lessons provided by these platforms remain limited 
to a fixed list of pieces. Although a student can suggest a 
new title, the realization of a whole lesson on these 
platforms requires heavy installations and treatments (multi-
angle video recording, 3D motion capture), as well as the 
intervention of multiple actors other than the teacher 
himself. While these methods produce high quality teaching 
material, the realization of a new course remains a complex 
and expensive process. In parallel, several teachers, for 
instance retired experts, wish to transmit their know-how in 
a simple way, without any constraint on the recording 
location and time and with minimal tool appropriation. 
We thus introduce in this paper a complementary 
framework to rapidly create dynamic music lessons on new 
pieces with the assistance of a score annotation module. 
This framework is implemented on a collaborative score 
annotation platform for music learning called @-MUSE 
(Annotation platform for MUSical Education). As described 
in [11], an online annotation system is chosen because it 
allows musicians to work with digital scores in a way 
similar to traditional lessons, where scores are a support for 
memory and information sharing. In addition, the digital 
transposition of this common practice enables to enrich it 
with multimedia incrustation, collaborative working and 
mobility. As such, its aim is also to constitute a scalable 
music playing knowledge base to collect and share tips and 
performances on all possible artistic works referenced on 
music data warehouse such as MusicBrainz.org [20], and 
which can evolve according to the learners’ needs. This base 
is called ISBS (Iterative Sign Base System). 
In this paper, we first introduce the methodology and 
principles of Sign Management that supports this platform. 
Then, we describe the general architecture of @-MUSE, 
based on Semantic Web concepts, in order to constitute a 
musical sign base (ISBS). To assist users into feeding and 
exploiting this base, we describe various methods to 
generate relevant annotations (i.e., explanations) on a score. 
Lastly, we conclude this work by detailing its principal 
perspectives: an adapted tactile interface and some serious 
gaming aspects. 
II. 
METHODOLOGY : SIGN MANAGEMENT 
Sign Management deals with the management of know-
how rather than knowledge. It manages live knowledge, i.e., 
subjective objects found in interpretations of real subjects 
on the scene (live performances) rather than objective 
entities found in publications (bookish knowledge). A Sign 
is a semiotic and dynamic object issued from a Subject and 
composed of three parts, Data, Information and Knowledge. 
All these subjective components communicate together to 
build a chain of sign-ifications that we want to capture. 
178
ICIW 2011 : The Sixth International Conference on Internet and Web Applications and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-124-3

Sign management is thus more central than Knowledge 
management for our purpose in instrumental music learning. 
Indeed, the musical signs to treat are made of emotional 
content (performances), technical symbols (scores) and tacit 
knowledge (rational and cultural know-how). Thus, a Sign 
is the interpretation of an object by a subject at a given time 
and place, composed of a form (Information), a content 
(Data) and a sense (Knowledge). The sign management 
process that we have created is made on a Creativity 
Platform for delivering an instrumental e-learning service 
[10][4][5]. It is founded on an imitation and explanation 
process for understanding gestures that produce a right and 
beautiful sound. The advantage for learners is that we are 
able to decompose the teacher’s movement and understand 
the instructions that are behind the process of playing a 
piece of music. In fact, a lovely interpretation is made of a 
lot of technical and motivated details that the learner has to 
master, and the way we want to deliver this information is to 
show 
examples 
from 
experts 
through 
multimedia 
annotations indexed on the score. To do so, we introduce a 
new platform to design dynamic music lessons through 
multimedia annotations: @-MUSE. 
III. 
@-MUSE GLOBAL ARCHITECTURE 
As the aim of @-MUSE is to enable dynamic teaching and 
learning, it is capital that its architecture remains flexible. 
The usage of Semantic Web tools is thus an appropriate lead 
to allow the platform to benefit from a “networking effect”. 
Indeed, a significant amount of scattered musical resources 
already exist on the web and can be relevant in the context 
of music lessons. These resources can be music metadata 
(MusicBrainz.org), 
digital 
scores 
(images, 
PDFs, 
MusicXML free or proprietary files available on Werner 
Icking Archive [21]), multimedia documents (recordings of 
video performances and lessons on YouTube [22] or eHow 
[23]) or simple textual comments. They constitute the 
different sign components listed in part II: data, information 
and knowledge. As many of these resources benefit from a 
Creative Commons License [24], they can be used in the 
context of a music lesson, complementary to high quality 
resources from a professional multimedia capture set [5]. 
Figure 1 exposes a comparison between traditional 
instrumental e-learning applications architecture and @-
MUSE architecture. In the first case, lessons are defined in a 
static way. Each lesson correspond to a musical piece, with 
its associated resources : video, audio and image files 
synchronized together to form the lesson. While this system 
produces complete lessons, it cannot establish relations 
between two distinct resources or pieces, which is an 
essential point when learning music as a whole. In the 
second case, @-MUSE dynamically creates lessons by 
linking related resources and presenting them to the user in 
an adapted interface [11]. If a resource is not available (for 
instance, a logic representation of a score), the system still 
works with a temporary replacement (for instance a simple 
image representing the score) in the frame of a degraded 
mode. It can then point to any user the need to provide such 
resource to enable new functionalities on the platform. As 
more links are created between resources, different 
representations of the same piece can be proposed to learn 
 
Figure 1. Architecture comparison between traditional instrumental e-learning application and @-MUSE 
 
 
 
179
ICIW 2011 : The Sixth International Conference on Internet and Web Applications and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-124-3

how to play it. Some links such as a time synchronization 
between two representations (i.e., a video performance and a 
logical description of the score) can be realized by specific 
independent modules (see Figure 1). 
We have done previous work in [12] to propose an adapted 
ontology to link musical resources in an educational context 
using the Resource Description Framework (RDF [8]). 
In the end, the association of these elements will allow the 
creation of an Iterative Sign Base System in the same vein 
as IKBS (Iterative Knowledge Base System [3]). The 
difference here lies in the manipulation of semiotic objects 
(signs), instead of conceptual ones (knowledge), as 
described in part II. The following chapter explains how 
new signs can be generated on this platform through semi-
automatic score annotation, and thus participate in the 
enrichment of the sign base (ISBS) by demanding minimal 
efforts from the platform users. 
IV. 
INFERENCE ON DIGITAL SCORES 
ISBS is a sign base model designed to collect musical signs 
such as scores (model) and performances (cases), in order to 
explain and compare them. To realize such analysis in a 
semi-automatic way, we need to detect specific patterns 
within a score. This detection could be made directly on 
performances [14] but audio signal analysis algorithms are 
difficult to implement in a web application and may be 
unreliable in an educational context. That is why we rely on 
XML representations of a score. MusicXML [1] is an XML 
open source format to describe digital scores staff by staff, 
measure by measure, and lastly note by note (Figure 2). 
In what follows, we review and propose different methods 
to extract various playing information from a piece metadata 
and structure.  
We base these methods on how a pianist would address an 
unknown piece. As detailed in the descriptive model 
presented in [12], the musical work is first replaced in its 
context (composer, period, form). Then, its difficulty is 
evaluated, firstly globally, and then part by part, in order to 
determine what type of work can be made on this piece and 
where. 
Thus, the first playing related information we display on a 
new piece is an approximation of its difficulty. In TABLE 1, 
we propose seven criteria affecting the level of a piece for 
piano and detail how they can be estimated from a 
MusicXML file. Globally, a piece difficulty depends on its 
tempo, its fingering, its required hand displacements, as well 
as its harmonic, rhythmic and polyphonic specificities. Of 
course, these various criteria affect each other in a complex 
manner. For example, hand displacement is strongly 
affected by fingering, as noted in TABLE 1. 
Indeed, among these seven criteria, fingering plays an 
important role. Several works present methods to 
automatically deduce fingering on a given musical extract 
for piano ([2][9][6]). Most of them are based on dynamic 
programming. All possible fingers combinations are 
generated and evaluated, thanks to cost functions. The latter 
are 
determined 
by 
kinematic 
considerations. 
Some 
functions, like in [6], even consider the player’s hand size to 
adjust its results. Then, expensive (in term of effort) 
combinations are suppressed until only one remains, which 
will be displayed as the resulting fingering. While the result 
often differs from a fingering determined by a human 
professional, it remains largely playable and exploitable in 
the frame of an educational usage. However, few algorithms 
can process polyphonic extracts [6], and many other cases 
are ignored (i.e., left hand, finger substitutions, black and 
white keys alternation).  
Even if more work is needed on this issue, the use of cost 
functions remain relevant as it is close from the process 
humans implicitly apply while working on a musical piece. 
That is why we extend this idea and create complementary 
criteria to design a piece difficulty analyzer for piano 
learning. For each criterion described in TABLE 1, a score is 
calculated in percentage. The piece difficulty rate is thus the 
average rate of each criterion. Furthermore, some weighting 
coefficients can be affected to each criterion to reflect the 
particularities of the player. For instance, pianists who are 
really at ease with polyrhythm would not consider it a 
relevant factor, thus affecting it a 10% weight.  
However, we insist that the resulting difficulty rate should 
be 
interpreted 
with 
care 
and 
remains 
a 
simple 
approximation. As stated in [15], a pleasant performance is 
not a mere addition of criteria since it contains an important 
subjective part. Moreover, for the time being, the algorithms 
we propose remain bold and need some specific refinements 
which will be the object of a next paper. Indeed, some cost 
functions are applied measure by measure, while they 
should be applied phrase by phrase to remain coherent with 
the piece logic. Also, some of the parameters were 
determined after the practices of a small group of advanced 
pianists and need to be extended by working with a larger 
sample of musicians, including other instruments. 
 
Figure 2. Score logical structure 
 
180
ICIW 2011 : The Sixth International Conference on Internet and Web Applications and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-124-3

 
TABLE 1. PLAYING DIFFICULTY CRITERIA IN PIANO PRACTICE 
Performance 
difficulty 
criterion 
Musicological definitions 
Cost function definition 
Examples 
MusicXML 
implementation 
Playing speed Tempo: speed or pace of a musical 
piece. May be indicated by a word 
(ex: allegro) or by a value in BPM 
(Beats Per Minute) 
 
Pulsation: reference value indicated 
in the tempo :  = 1, = 2, = 4, = 
8,  = 16, etc. 
Playing speed = tempo / (shortest 
note value in the piece) 
 
Unit: beats (time value) 
P1: tempo = 120 
Shortest value = 
P1 playing speed = 120*8/16 = 60 
P2: tempo = 120 
Shortest value = 
P2 playing speed = 120*4/16 = 30 
 
Conclusion: Some parts in P2 are played 
faster than in P1. To be more accurate, it is 
possible to multiply the result by the 
proportion of notes of shortest value. Thus, if 
P1 contains 40% , and P2 only 5%, then P1 
is globally faster. 
 
<note><type> 
elements 
Tempo attribute in 
<sound> element 
Fingering 
Fingering: choice of finger and 
hand position on various 
instruments. Different notations 
exist according to the instrument. 
(Ex: in piano: 1 = thumb, 2 = index 
finger, 3 = middle finger, etc.) 
If m1, m2, ..., mn represent the 
measures of a given piece P, 
Fingering_difficulty(P) = ∑ 
(Fingering_cost(mi)>50) 
See [2][6][9][13] for more detail. 
P = 
 
Fingering_cost(m1) = 10 
Fingering_cost(m2) = 0 
Fingering_cost(m3) = 70 
 
Fingering_difficulty(P) = 70 
 
<measure> and 
<note> elements 
Hand 
Displacement 
Interval: pitch distance between 
two notes, in semitones. 
A hand displacement is considered 
difficult when two successive notes 
(or two chords) are spaced by at 
least 7 semitones, played by close 
fingers (on the same hand, distance 
< 4 fingers) at a high tempo. The 
displacement cost of an interval 
increases with its gap length. It also 
increases with polyphony. 
If d1, d2, ..., dn represent n 
intervals verifying the conditions 
given in the previous description, 
in a piece P 
Displacement_difficulty(P) = ∑ 
Displacement_cost(di) 
 
 
P = 
 
 
Displacement_difficulty(P) = 340 
Combined <note> 
elements where 
<pitch> gap ≥ 7. 
Associated 
fingering file. 
Polyphony 
Chord: aggregate of musical 
pitches sounded simultaneously. 
Proportion of chords and chords 
sequences in the piece 
P = 
 
Chords_proportion(P) = 6/16 = 38% 
<chord> element 
Harmony 
Tonality: system of music in which 
specific hierarchical pitch 
relationships are based on a key 
"center", or tonic. Various 
tonalities impose various sharps 
and flats as a key signature. The 
most basic ones (no alteration) are 
A minor and C major. 
Proportion of altered notes 
P = 
 
Altered_notes_proportion(P) = 3/25 = 12% 
<alter> and 
<accidental> 
elements 
Irregular 
Rhythm 
Polyrhythm: simultaneous 
sounding of two or more 
independent rhythms. Example : 
synchronizing a triplets over 
duplets 
Proportion of remarkable 
polyrhythm patterns (Time 
reference = pulsation) 
P = 
 
Polyrhythm_proportion(P) = 4/4 = 100% 
<time-
modification> 
element 
Length 
The length of the piece in beats. 
NB: the number of pages cannot 
really reflect the length of a piece 
because of page setting parameters 
Number of measures * number of 
beats per measure. 
P = 
 
Length(P) = 3*3 = 9 
<beats> element 
of <time> element 
and <measure> 
elements 
 
181
ICIW 2011 : The Sixth International Conference on Internet and Web Applications and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-124-3

This algorithm also serves as a base for the next chain of 
information inference on the given piece. Indeed, it can be 
applied to identify difficult parts within the piece. By 
calculating the difficulty rate of each measure, we can 
display the remarkable parts, which rate exceeds a given 
threshold (determined by the player’s level). The cause of 
its difficulty can then be deduced from the rates of each 
criterion (Figure 3). The application can then annotate the 
part accordingly, for instance by redirecting the learner to an 
adapted exercise. 
In parallel to difficult parts, other remarkable structures can 
be identified within a piece. Indeed music learning relies a 
lot on the repetition of specific short patterns, with slight 
differences (for example the tone of a piece), which can be 
reused in various context, especially within the same genre 
(baroque, classical, jazz, etc). TABLE 2 gives some patterns 
examples. 
 
If long enough (and thus actually remarkable), each of these 
patterns can be detected as a note sequence within a 
MusicXML file. Then, corresponding exercises can be 
pointed to guide the learner. These exercises can be directly 
adapted from the considered pattern. For instance, in the 
case of an arpeggio, the latter will be extended to the whole 
keyboard and repeated part by part, by adding a new note 
every ten repetition. This process can easily be computed as 
suggested by Figure 4. Anytime, the annotation's owner and 
teachers can modify it in order to improve the given 
explanation with textual and video commentaries, symbols 
and tags. Users can also invalidate the generated annotation 
if considered as inappropriate. In this case, the motive for 
the suppression should be specified. This data will be later 
used to determine the reasoning error in order to improve 
the next generated annotations. This process will be detailed 
in an upcoming paper. 
V. 
CONCLUSION AND PERSPECTIVES 
In this paper, we proposed a methodology (Sign 
Management), a model (Iterative Sign Base System) and 
some inference methods to build an instrumental e-learning 
platform called @-MUSE. This platform allows teachers 
and learners to create music lessons dynamically with the 
assistance of a semi-automatic pieces annotator. These 
lessons can evolve according to the users’ needs by 
submitting contextual exercises to them, in the form of 
multimedia annotations. These exercises are generated from 
the original score based on the identification of remarkable 
patterns and their playability. Users can then give their point 
of view on the generated annotations but also add new ones, 
thanks to a dedicated symbols library as well as a 
multimedia capture module. The more knowledge is created 
on the platform, the more detailed will be the lessons, 
thanks to the emerging network effect resulting from the 
semantic linking of the various resources. 
 
Different perspectives are also considered for this work, 
including the addition of tactile functionalities, as well as 
some serious gaming aspects. For instance, an interface 
adapted to tablet PC would allow to use our platform 
directly in front of the instrument, guaranteeing an 
experience close to a traditional music lesson. The 
collaborative aspects of such a platform also need to be 
studied to approach music learning under an entertaining 
angle, 
for 
instance 
by 
proposing 
specific 
group 
performances (Global Sessions [25]) and game features. 
Indeed, as implied by our platform's name, learning music 
should first and foremost be a pleasure. 
TABLE 2. MUSICAL PATTERNS EXAMPLES 
Pattern 
name 
Example 
Scale 
 
Arpeggio 
 
Trill 
 
Real 
sequence 
 
 
 
Figure 3. Difficulty analysis and recommendations on a digital score 
 
182
ICIW 2011 : The Sixth International Conference on Internet and Web Applications and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-124-3

 
REFERENCES 
[1] G. Castan, M. Good, and P. Roland, “Extensible Markup Language 
(XML) for Music Applications: An Introduction”, The Virtual Score: 
Representation, Retrieval, Restoration, MIT Press, Cambridge, MA, 
pp. 95-102, 2001. 
[2] C.-C. Lin, “An Intelligent Virtual Piano Tutor”, National Chung 
Cheng University 2006. 
[3] N. Conruyt and D. Grosser, “Knowledge management in 
environmental sciences with IKBS: application to Systematics of 
Corals of the Mascarene Archipelago”, Selected Contributions in 
Data Analysis and Classification, Series: Studies in Classification, 
Data Analysis, and Knowledge Organization, pp. 333-344, Springer, 
ISBN: 978-3-540-73558-8, 2007. 
[4] N. Conruyt, O. Sébastien, V. Sébastien, D. Sébastien, D. Grosser, S. 
Caldéroni, D. Hoarau, and P. Sida, “From Knowledge to Sign 
Management on a Creativity Platform, Application to Instrumental E-
learning”, 4th IEEE International Conference on Digital Ecosystems 
and Technologies (DEST 2010), IEEE Press, 2010, pp. 367-374. 
[5] N. Conruyt, O. Sébastien, and V. Sébastien, “Living Lab in practice: 
the case of Reunion Creativity Platform for Instrumental e-Learning”, 
13th International Conference on Interactive Computer Aided 
Learning (ICL 2010), September 15-17, Hasselt, Belgium, 2010. 
[6] A. Al Kasimi, E. Nichols, and C. Raphael, “A simple algorithm for 
automatic 
generation 
of 
polyphonic piano 
fingerings”, 
8th 
International Conference on Music Information Retrieval, September 
23rd-27th, Vienna, Austria, 2007. 
[7] K.J. Kim and C.J. Bonk, “The Future of Online Teaching and 
Learning in Higher Education”, Educause Quarterly, vol. 29, 2006, 
pp. 22-30. 
[8] O. Lassila and R. R. Swick, “Resource Description Framework 
(RDF) Model and Syntax”, W3C specification, 1998.  
[9] R. Parncutt, J. A. Sloboda, M. Raekallio, E. F. Clarke, and P. Desain. 
“An Ergonomic Model of Keyboard Fingering for Melodic 
Fragments”, 
Music 
Perception: 
An 
Interdisciplinary 
Journal 
Vol. 14, No. 4, 1997, pp. 341-382. 
[10] O. Sébastien, N. Conruyt, and D. Grosser, “Defining e-services using 
a co-design platform: Example in the domain of instrumental e-
learning", Journal of Interactive Technology and Smart Education, 
Vol. 5, issue 3, pp. 144-156, ISSN 1741-5659, Emerald Group 
Publishing Limited, 2008. 
[11] V. Sébastien, D. Sébastien, and N. Conruyt, “A collaborative 
platform model for digital scores annotation”, 3rd Annual Forum on 
e-Learning Excellence in the Middle East, Dubaï, 2010. 
[12] V. Sébastien, D. Sébastien, and N. Conruyt, “An Ontology for 
Musical Performances Analysis. Application to a Collaborative 
Platform dedicated to Instrumental Practice”, The Fifth International 
Conference on Internet and Web Applications and Services, 
Barcelona, 2010, pp. 538-543. 
[13] J. A. Sloboda, E. F. Clarkeb, R. Parncutt, and M. Raekallio, 
“Determinants of Finger Choice in Piano Sight-Reading”, Journal of 
Experimental Psychology: Human Perception and Performance, 
Volume 24, Issue 1, 1998, pp. 185-203. 
[14] D.R. Stammen and B. Pennycook, “Real-time Recognition of 
Melodic Fragments using the Dynamic Timewarp Algorithm”. ICMC 
Proceedings, 1993, pp. 232-235. 
[15] M. Stanley, R. Brooker, and R. Gilbert, “Examiner Perceptions of 
Using Criteria in Music Performance Assessment”. Research Studies 
in Music Education, June 2002, vol. 18, issue 1, pp. 46-56. 
[16] http://e-guitare.univ-reunion.fr, visited on the 15/11/2010. 
[17] http://www.guitar-pro.com, visited on the 15/11/2010. 
[18] http://www.apple.com/ilife/garageband/#basic-lessons, visited on the 
15/11/2010 
[19] Flash Interactive Guitar Saloon: http://e-guitare.univ-reunion.fr/figs, 
visited on the 15/11/2010 
[20] http://musicbrainz.org, visited on the 15/11/2010 
[21] http://icking-music-archive.org, visited on the 15/11/2010 
[22] http://youtube.com, visited on the 15/11/2010 
[23] http://www.ehow.com/, visited on the 15/11/2010 
[24] http://creativecommons.org/, visited on the 15/11/2010 
[25] http://www.youtube.com/watch?v=ZTOmYLTitGg, visited on the 
15/11/2010 
 
Figure 4. Pseudocode algorithm generating progressive arpeggio exercises 
 
183
ICIW 2011 : The Sixth International Conference on Internet and Web Applications and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-124-3

