Haptic Guided Driving with Seated Self-balancing Personal Transporters  
for People with Deafblindness 
 
Andreas Hub 
BNI Blindnavigation International gGmbH 
Ayestr. 2 
88441 Mittelbiberach, Germany  
e-mail: andreas.hub@blindnavigation.org 
 
Jutta Cook 
CO4 Communications 
Talfeldstr.10 
88400 Biberach, Germany 
e-mail: jutta.cook@co4-communications.com
 
Abstract—In this paper, we will present the experiences 
from our work in progress with a first test drive with a new 
seated self-balancing personal transporter by an entirely 
deafblind driver, supported by a guide. Our aim is to help 
reduce the feeling of isolation and dependency and the 
restrictions in the range of mobility deafblind people report. 
Using this personal transporter greatly enlarges the distance a 
deafblind person can cover in a self-determined way. Being 
active and moving together constitutes significant, essential 
social interaction and social participation. It offers a very 
relaxed atmosphere of independence while fostering a 
relationship of mutual trust. Within this paper, we describe 
how it works and also some challenges concerning the 
deafblind driver’s feelings about missing visual information 
such as the position, the driving direction and the current 
velocity. These challenges might be solved in the near future by 
using additional existing or new haptic devices. We are 
confident that, this way, we could make significantly enlarged 
environments accessible in a smart way for people with 
deafblindness as well as for people with additional physical 
restrictions. 
Keywords-accessibility; deafblindess; mobility; haptics; self-
balancing personal transporter. 
I. 
INTRODUCTION 
For years, there have been numerous research projects, 
e.g., the Rolland project [1], as well as a number of quite 
sophisticated products on the market, enabling drivers to use 
electronic wheelchairs autonomously or with only a 
minimum of motoric input, such as eye trackers, minimal 
joysticks or other control mechanisms that can be operated, 
e.g., with only one finger or a small head motion. 
In contrast to these wheelchairs, the self-balancing 
personal transporter we used demands real physical activity 
from its driver. The so called AddSeat® by Swedish 
manufacturer AddMovement [2] is a self-balancing seated 
single person vehicle based on a Segway® Personal 
Transporter (PT). It was initially developed to enable people 
with physical mobility restrictions such as amputations, 
paraplegia, autoimmune disorders, etc. to move around more 
self-determined than in a regular electric wheelchair. 
In field reports and video documentations, we were able 
to show that driving a seated self-balancing Personal 
Transporter for several weeks can significantly increase the 
independence and quality of life. This includes feelings of  
 
 
Figure 1. Successful tests with sighted people with other mobility 
restrictions (in this case a high leg amputation) were the motivation to 
expand these tests to guided driving with a deafblind person. 
joy and fun brought on by experiencing the physical 
sensations of movement. This is also applied in challenging 
environments such as trails, grass, snow, or sand where it 
would be difficult or impossible to maneuver with other 
types of wheelchairs (Figure 1). 
People with complete deafblindness rarely have an 
opportunity to feel the physical forces that come with driving 
a motor-powered vehicle on their own, such as acceleration 
or centripetal forces, including the fun and the feeling of 
freedom that can be related with driving faster. However, 
from our previous work concerning navigation support for 
people with deafblindness [3]-[7], we know some of their 
dreams of freedom and independence concern mobility, 
including the wish to drive extremely fast vehicles, such as 
motorbikes even Jet Skis®. Especially in cases where 
deafblindness occurs only later in life, people might already 
have gained the experience of driving, and really want to do 
it again. With this background in mind, we established the 
idea of letting a first, suitable deafblind test person drive an 
AddSeat® (Figure 2). 
 
5
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-622-4
SMART ACCESSIBILITY 2018 : The Third International Conference on Universal Accessibility in the Internet of Things and Smart Environments

 
 
Figure 2. The photo shows the test driver on the vehicle, receiving a short 
briefing on how to steer the AddSeat® sighted guide using Lorm 
fingerspelling.  
Good posture and control of the upper body is the main 
pre-condition to being able to steer an AddSeat® in a 
controlled manner. This applies to sighted drivers, too. 
According to our experience, some people with complete 
deafblindness have very good posture and body control even 
though they are lacking visual or acoustical input, which 
makes visual or acoustical corrections of malposition 
impossible. 
In prior tests with people with mobility impairments, the 
AddSeat® technology has already proven to be a life-
changing addition [8]. Participants reported a tremendous 
increase in quality of life as it enhanced their mobility and 
independence, it opened up their social range of motion and 
allowed them to take on completely new roles within their 
family lives. Additionally, it turned out that people without 
mobility restrictions can benefit from this vehicle, too. The 
AddSeat® can cover distances beyond 30 km. Its big wheels 
and good traction even make decent slopes, farming roads or 
broader trails in the mountains accessible. We see a special 
benefit in this way of locomotion for people with visual 
impairments, as a prolonged use of a cane can be exhausting 
for the muscles in the hand, arm and shoulder. Walking 
alongside a sighted person for longer periods of time can be 
stressful, too. Both can lead to unhealthy and non-ergonomic 
motions, causing muscle sores, cramps and other discomforts 
in the musculoskeletal system.  
During prior experiments with blind AddSeat® drivers, 
we saw that their abilities to control both their bodies and the 
vehicle often surmounted those of sighted subjects. In 
comparison to persons with deafblindness, blind people have 
the advantage of being able to estimate their current position, 
direction, and velocity by using acoustical input; they either 
use noises in the environment or produce sounds of their 
own, as a click of the tongue, singing, talking or knocking 
the cane on the floor, etc. The acoustic response of those 
sounds helps them to detect environmental structures, such 
as open doors, and to estimate their own velocity or the 
velocity of others, such as the Doppler frequency shift of 
cars near crosswalks. People with complete deafblindness 
are void of this kind of sensory input. With both visual and 
hearing senses missing, their sense of gravity and 
appertaining their sense of body control plays a more 
important role than in people without sensory restrictions. 
Structure of this paper: Section II describes the technical 
features of the AddSeat® and the methods we used for the 
briefing the deafblind person as well as for the driving 
support provided by the sighted guide. Section III describes 
the results concerning the astonishing feasibility of the 
principle, but it also addresses the remaining challenges 
concerning mainly technical issues such as hardware 
integration. In Section IV, we conclude this early stage of 
our investigations and suggest a compass for the future work 
in this new field of research for people with deafblindness. 
II. 
METHODS 
Driving an AddSeat® is quite simple and intuitive. The 
AddSeat® is the only self-balancing vehicle available on the 
market that is equipped with a panic-braking mechanism, so 
far. For our tests, we used an AddSeat® model 5.1 equipped 
with pneumatic suspension and height adjustment. We 
deemed those features essential in testing seated Segways® 
with a deafblind person supported by a sighted guide. Those 
features bring a range of advantages for sighted people, too. 
For one, it is more comfortable than a model with a fixed 
seat. Secondly, the user can raise the seat to allow 
communication at almost eye level with others standing, 
depending on the person’s height. This feature is not only 
comfortable for both sides, but being at eye level also greatly 
changes people’s perception.  
The first thing the driver has to do is to place himself on 
the AddSeat®. Next, the parking handles on both sides have 
to be released. Now, the vehicle is in balance mode and the 
user can drive forward by moving the upper part of the body 
forward or correspondingly backward by shifting the center 
of gravity to the rear. The seat of the AddSeat® is mounted 
on a sliding rail. Thus, the driver can very rapidly shift his 
center of mass. A quick backwards or forwards shift allows 
the driver to perform precise driving maneuvers such as 
avoiding moving obstacles. It can also be used to quickly 
stop the vehicle in an effective full stop emergency brake 
maneuver. Implementation of this sliding mechanism 
reduced the braking distance by several meters, in particular 
when driving at speeds exceeding 10 km/h. In order to avoid 
collisions with moving obstacles or other people, it was 
necessary to have a sighted person walk along and advise 
appropriately.  
Our deafblind male driver received a short briefing from 
a sighted guide by using Lorm fingerspelling [9]. He quickly 
grasped how to move forward and backward, how to 
navigate turns and how to stop by using the gliding seat. He 
intuitively was able to turn in place. This brief driving lesson 
only took a few minutes and in principle enabled the 
deafblind test driver to handle the vehicle by himself. During 
this short driving lesson, haptic support was used to provide 
6
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-622-4
SMART ACCESSIBILITY 2018 : The Third International Conference on Universal Accessibility in the Internet of Things and Smart Environments

 
 
Figure 3. The photo depicts the sighted guide standing behind the driver, 
giving him the haptic sign for driving straight ahead with the same velocity 
by moving the back of the hand up and down the driver’s spine.  
principle movement and velocity information as well as 
direction suggestions. For this, we used a haptic training 
method comparable to Riitta Lathinen’s haptices and 
haptemes advices [10]. To enhance the forward movement, 
the sighted guide gave a gentle tap on the driver’s upper back 
until the velocity was appropriate with respect to the 
environment and the driver’s capacities (Figure 3). To trigger 
a left or a right turn, a finger or hand tap was given on the 
corresponding shoulder (Figure 4). Braking or velocity 
reduction were induced by a softer and longer tip with the 
finger or hand on the front of the driver’s shoulder. For 
safety reasons, we had a third person chaperone every test in 
the background as to minimize risks in the initial phase of 
the test. 
III. 
RESULTS 
The deafblind test driver learned to understand, to steer 
and to control the AddSeat® within a few minutes. The 
challenge for the sighted guide was to provide adequate and 
precise navigation advices using haptic commands. 
As part of the results we would like to cite our deafblind 
test driver (P.H.) (information in [..] was shown in signs): 
“It’s a pity that I can’t see a little. Otherwise I would start 
immediately to drive around. My biggest problem is to be 
completely blind. I can well imagine that people with limited 
visual impairments can use it [quite easy]. It’s also possible 
[for me] to feel safe [because it’s stable to the side]. To help 
with orientation for [completely] blind people, we need 
something additional.” In addition, he reported that it was 
not always clear for him whether he was driving forward or 
backward, in particular when driving slowly, as he was 
missing all visual or acoustical input. For the same reason, he 
reported difficulties estimating his current velocity. The used 
haptic and direct type of driving advice offers the advantage 
 
 
Figure 4. This picture shows how finger tapping on the by the 
corresponding shoulder was used as haptic sign for left and right turns.  
that it can be translated into action a lot more intuitively and 
faster than conventional sign language or fingerspelling. 
IV. 
CONCLUSION  
Our main focus was on feasibility – can a deafblind 
person steer a single seat personal transporter – and what are 
the emotional and social effects associated with moving 
self-determined at greater speeds. We have reached a couple 
of conclusions. 
The physical task of steering the AddSeat® is mainly a 
matter of upper body control which we found not to be 
negatively influenced by the co-occurrence of visual and 
acoustic impairments. Our deafblind test person performed 
better than some sighted testers, a fact we attribute to his 
heightened sense for posture and position. However, we 
have to say that we picked our deafblind test person based 
on the precondition of a good general mobility as to make 
successful testing more probable. 
After a quick instruction session with haptic translation, 
our test person was able to steer the vehicle by himself with 
the help of a very limited set of haptic guidance signs such 
as left, right, forward, back or keeping a steady pace. We 
conclude that these basic instructions could also be given by 
a sighted guide lacking knowledge of Lorm language, 
thereby potentially widening the circle of interactive 
contacts a deafblind person can have. Guided driving on an 
AddSeat® might therefore be the basis for an increase in 
important social interaction.  
Our previous tests with sighted people with mobility 
restrictions showed the AddSeat®’s significant positive 
influence on more than mobility. Three dimensions of life 
which we labeled “fun”, “freedom” and “family&friends” 
yielded especially positive results: self-determined driving 
enabled those test persons to be more independent and lead 
a more equitable life with less discrimination. It can be 
7
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-622-4
SMART ACCESSIBILITY 2018 : The Third International Conference on Universal Accessibility in the Internet of Things and Smart Environments

reasonably assumed that driving an AddSeat® can also 
enhance the quality of life for people with deafblindness, as 
well as for their partners, family members, friends and 
caregivers.  
Driving an AddSeat® is not without risks, in particular 
when driving deafblind. We strongly recommend that 
deafblind people drive only with an experienced guide, 
especially in challenging environments such country lanes, 
steep roads etc. and most certainly at higher speeds. 
Deafblind drivers will need a way of receiving constant 
advice while moving. The relationship between the driver 
and the sighted guide is one of extreme mutual trust, as the 
driver lays his well-being and ultimately his life into the 
hands of his guide. The sighted person in return takes over 
the responsibility of safeguarding his guidee, relying on him 
to perform the required actions. This can be a great step 
towards breaking the isolation.  
With its range of up to 30 km, the AddSeat® lets drivers 
cover long distances. Its off-road capabilities make a wide 
array of different environments accessible to deafblind 
people. As the AddSeat®’s movement is controlled by 
shifting the driver’s center of gravity, every change of speed 
or direction also serves as a work-out to improve the 
abdominal and back muscles.  
 
FUTURE WORK:  
This work is at an early stage. The next step will be to 
increase the number of usability tests while establishing a 
questionnaire that allows for an evaluation of a wider range 
of variables. We also suggest a comparison between sighted, 
blind, and deafblind drivers as well as design options for the 
haptic feedback device that could then be utilized for all 
groups. 
After our research experience concerning object 
recognition and navigation support for people with 
deafblindness, there is no doubt that quite a number of them 
will be able to profit from driving an AddSeat®. People 
with complete deafblindness will have the best experiences 
with additional support from haptic interfaces implemented 
to interact with the sighted guide or by driving completely 
autonomous – conceivably through the integration of 
autonomous driving systems used in the car industry or in 
electronic wheelchairs with four wheels. Those systems, 
however, might imply that in the future the driver has 
nothing to do at all while driving. All our tests, also with 
sighted people, have shown that the drivers actually enjoy 
the very act of driving; driving alone or together with a 
guide is fun, because they get to perform the task 
themselves. It is this activity that sets driving an AddSeat® 
apart from their usual, passive experience of being taken 
somewhere. We gained the impression that this is 
particularly the case with blind users as well as with our 
deafblind tester, too. Therefore, we decided to focus our 
future work on ideas for systems that allow the driver to 
keep in control of the steering rod and the motor power, 
with navigation support from a sighted guide through a 
haptic interface. Here, future research can look into two 
different options. One is to have a haptic assistance system 
controlled by the guide attached directly onto the driver’s 
body. The other possibility is to integrate the haptic 
interface onto or into the AddSeat®. An example of a body-
fixed system that might be used with slight modifications in 
combination with the AddSeat® is the NavBelt [11]. It 
provides acoustic information and additional vibrating 
haptic information within a belt around the body. This 
system could be particularly helpful concerning direction 
advices from the guide. The NavBelt has the advantage that 
the driver keeps both hands free to steer the AddSeat® more 
safely than with just one hand. 
Our previous research proposed alternative systems 
which are more or less handheld, such as the following: A 
portable Braille display connected to a smartphone with a 
special navigation software provides technical orientation 
support for people with blindness and deafblindness as 
developed in previous work [12]. This system uses short 
Braille patterns of two or three Braille characters, some of 
them animated, to indicate for example a right or a left turn. 
By using such short navigation advices, the blind user can 
be informed about the appropriate direction and speed faster 
than with complete written text information in Braille. The 
other proposed functional system [13] uses a haptic phone 
keyboard as basic user interface and applies additional 
adaptive control elements including a haptic compass. This 
allows the user to feel the direction of motion and to 
determine the current mode of the system without asking for 
it or without changing to another level in the program 
hierarchy. The advantage of this system is that it can be 
used as a compass-based navigation system. 
When looking at future systems that might be fixed 
somehow to or on the AddSeat, we first think about slightly 
modified systems such as “The ViibraCane” based on the 
remote control of a Wii® game console proposed by 
Schmitz [14] that could be integrated into the handles of the 
steering rod. Another possibility would be to integrate 
vibrating or tactile elements into the seat to enhance the 
navigation support and to shorten reaction time similar to 
systems used in fighter jets [15][16]. Those kind of systems 
will be able to solve the challenges of missing information 
on current position, movement, direction and orientation for 
deafblind drivers. Future work should furthermore include 
the question if the AddSeat® can be used to support 
deafblind – with varying degrees of restrictions – in other 
fields, such as the workplace, or if it might be possible to 
create new kinds of workplaces for this group of people still 
experiencing discrimination in many aspects of life.  
Finally, in the near future the question needs to be 
addressed how the requirements of the United Nations’ 
Convention on the Rights of Persons with Disabilities [17] 
can be implemented by all parties to the convention on a 
national, regional and local level in order to afford deafblind 
people the right to participate in traffic with support from a 
sighted guide in order to benefit from this innovative and 
interactive technology. The sighted guide would need a 
secure command over only a very limited number of tactile 
gestures. This could be the basis for a sustainable way of 
“breaking the isolation”, as it would multiply the number of 
potential communication partners usually limited by 
knowledge of complex languages with tactile signs. Even if 
8
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-622-4
SMART ACCESSIBILITY 2018 : The Third International Conference on Universal Accessibility in the Internet of Things and Smart Environments

this interaction would first pertain to navigation support 
only, it can still make all the difference for everybody 
involved. 
ACKNOWLEDGMENT 
We would like thank our test driver Peter Hepp and 
Margherita Hepp for the sign language interpretation, the 
fingerspelling and for being a perfect guide from the very 
beginning. 
REFERENCES 
[1] C. Mandel, Navigation of the Smart Wheelchair Rolland. 
Kumulative Dissertation, 2007. 
[2] http://www.addmovement.com/ 2018.01.30 
[3] A. Hub, J. Diepstraten, and T. Ertl, “Design of an Object 
Identification and Orientation Assistant for the Deafblind” In: 
Proceedings of the 6th DbI European Conference on 
Deafblindness, August 2-7, Presov, Slowakia, p. 97, 2005. 
[4] A. Hub, S. Kombrink, K. Bosse, and T. Ertl, “Conference 
Navigation and Communication Assistant for the Deafblind 
based on Tactile and Acoustically Amplified Augmented Map 
Information for the 14th Deafblind International World 
Conference” In: Proceedings of the 14th Deafblind 
International World Conference (DbI 2007), September 25-
30, Perth, Australia, 2007. [Online]. Available from: 
ftp://ftp.informatik.uni-
stuttgart.de/pub/library/ncstrl.ustuttgart_fi/INPROC-2007-
124/INPROC-2007-124.pdf 2018.02.01 
[5] A. Hub, “Integration of Active Tactile Control Braille 
Technology into Portable Navigation and Object Recognition 
Systems for the Blind and Deafblind” In: Proceedings of the 
9th International Conference on Low Vision (Vision 2008), 
July 7-11, Montreal, Canada, 2008. [Online]. Available from: 
ftp://ftp.informatik.uni-
stuttgart.de/pub/library/ncstrl.ustuttgart_fi/INPROC-2008-
128/INPROC-2008-128.pdf 2018.01.01 
[6] A. Hub, “Making Complex Environments Accessible on the 
Basis of TANIA's Augmented Navigation Support” In: 
Proceedings of the California State University Northridge 
Center on Disabilities 25th Annual International Technology 
and Persons with Disabilities Conference (CSUN 2010), 
March 22-27, San Diego, CA, USA, 2010. [Online]. 
Available from: 
https://www.researchgate.net/publication/266282306_Making
_Complex_Environments_Accessible_on_the_Basis_of_TAN
IA%27s_Augmented_Navigation_Support 2018.02.01 
[7] A. Hub, Beyond the senses: model-based assistance systems 
for people with audio-visual impairments. Weiter als die 
 
 
Sinne 
reichen: 
Modellbasierte 
Assistenzsysteme 
für 
Menschen mit Hörsehschädigungen. In: Ursula Horsch, 
Andrea Wanka: Das Usher-Syndrom: Ein Fachbuch für 
Mediziner, 
Pädagogen, 
Psychologen 
und 
Betroffene. 
Reinhardt-Verlag, pp. 165-174, 2012. 
[8] http://addmovement.com/video's.html 2018.03.08 
[9] http://www.deafblind.com/lorm.html 2018.03.08 
[10] R. Lahtinen, Haptices and haptemes. A case study of 
developmental process in touch-based communication of 
acquired deafblind people. PhD thesis, 2008. 
[11] J. Borenstein, “The NavBelt - A Computerized Multi-Sensor 
Travel Aid for Active Guidance of the Blind” In: Proceedings 
of the CSUN's Fifth Annual Conference on Technology and 
Persons with Disabilities, Los Angeles, California, March 21-
24, pp. 107-116, 1990. 
[12] A. Hub, S. Krysmanski, “Navigation with Haptic or Braille 
Support” In: Proceedings of the California State University 
Northridge Center on Disabilities 28th Annual International 
Technology and Persons with Disabilities Conference (CSUN 
2013), February 25 - March 2, San Diego, CA, USA, 2013. 
[Online]. Available from:  
 
http://www.csun.edu/cod/conference/2013/sessions/index.php
/public/presentations/view/214 2018.02.01 
[13] J. Winterholler and B. Janny, “Innovative Haptic Interface for 
Navigation” In: Proceedings of the California State University 
Northridge Center on Disabilities 28th Annual International 
Technology and Persons with Disabilities Conference (CSUN 
2013), February 25 - March 2, San Diego, CA, USA, 2013. 
[Online]. Available from:  
 
http://www.csun.edu/cod/conference/2013/sessions/index.php
/public/presentations/view/130 2018.02.01 
[14] B. Schmitz, “The ViibraCane – A White Cane for Tactile 
Navigation Guidance” In: Proceedings of the California State 
University Northridge Center on Disabilities 25th Annual 
International Technology and Persons with Disabilities 
Conference (CSUN 2010), March 22-27, San Diego, CA, 
USA, 2010. [Online]. Available from:  
 
https://pdfs.semanticscholar.org/058a/fc0c1473984ddf75de21
00ab108d94ace167.pdf 2018.01.01 
[15] W.B. Albery, “Multisensory cueing for enhancing orientation 
information during flight. Aviation Space and Environmental 
Medicine, May 2007, 78(5s), B186-190. 
[16] S.M. Ko, K. Lee, D. Kim, and Y.G. Ji, “Vibrotactile 
perception assessment for a haptic interface on an antigravity 
suit” Applied Ergonomics, Volume 58, pp. 198-207, January 
2017. 
[17] United Nations, Convention on the Rights of Persons with 
Disabilities. [Online]. Available from: 
 
http://www.un.org/disabilities/documents/convention/convopt
prot-e.pdf 2018.01.30 
 
 
9
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-622-4
SMART ACCESSIBILITY 2018 : The Third International Conference on Universal Accessibility in the Internet of Things and Smart Environments

