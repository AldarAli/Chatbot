Designing  Multi-Modal Map-Based Interfaces for Disaster Management 
 
 
 
Volker Paelke 
3D Geo-Visualization and Modeling Group 
Institut de Geomàtica 
Barcelona, Spain 
volker.paelke@ideg.es 
Karsten Nebe 
Computer Sciences, Internet-Technologies and 
Usability Engineering 
Rhine-Waal University of Applied Sciences 
Kamp-Lintfort, Germany 
Karsten.Nebe@hochschule-rhein-waal.de 
Christian Geiger 
Department of Media Mixed Reality and Visualization 
Düsseldorf University of Applied Sciences 
Düsseldorf, Germany 
geiger@fh-duesseldorf.de 
 
 
Florian Klompmaker, Holger Fischer 
C-LAB 
University of Paderborn 
Paderborn, Germany 
{florian.klompmaker; holger.fischer}@c-lab.de 
 
 
Abstract— The access to current and reliable maps and data  is 
a critical factor in the management of disaster situations. 
Standard user interfaces are not well suited to provide this 
information to crisis managers. Especially in dynamic 
situations conventional cartographic displays and mouse based 
interaction techniques fail to address the need to review a 
situation rapidly and act on it as a team. The development of 
novel interaction techniques like multi-touch and tangible 
interaction in combination with large displays provides a 
promising base technology to provide crisis managers with an 
adequate overview of the situation and to share relevant 
information with other stakeholders in a collaborative setting. 
However, design expertise on the use of such techniques in 
interfaces for real-world applications is still very sparse. We 
are, therefore, conducting interdisciplinary research with a 
user and application centric focus to establish real-world 
requirements, to design new multi-modal mapping interfaces, 
and to validate them in disaster management applications. 
Initial results show that tangible and pen-based interaction are 
well suited to provide an intuitive and visible way to control 
who is changing data in a multi-user command and control 
interface. 
Keywords-post-WIMP 
user 
interfaces; 
natural 
user 
interfaces; mapping; geo-visualization; multi-touch interaction; 
pen-based interaction; tangible interaction 
I. 
 INTRODUCTION  
Our goal is to improve the management of large-scale 
disaster situations and complex emergencies by providing 
crisis managers with an interactive mapping system that aims 
to improve their effectiveness. Crisis managers often lack 
access to vital information. With digital map repositories and 
the proliferation of new sensors the problem increasingly 
changes from one where information is missing to one where 
the required information is too difficult to access and 
analyze. For example, even if the required information can 
be collected, crisis managers are still faced with the need to 
cope with large amounts of data that needs to be processed in 
order to guarantee successful operations.  
Existing work on post-WIMP (Windows, Icons, Menu 
Pointer) user interfaces has largely focused on the 
development 
of 
base-technologies 
and 
individual 
visualization and interaction techniques. We complement 
this work using an approach that starts with the application 
and focuses on user centered design (UCD). In this process, 
we use close collaboration with end users to establish 
requirements, to examine new concepts in data analysis and 
presentation and to validate the newly developed user 
interfaces. With regards to user interface techniques, we are 
working  both on the technology level and on the interface 
level. At the technology level we  develop new multimodal 
interfaces, incorporating advanced techniques for interaction 
and information. At the interface level we provide interface 
solutions to real-world problems, specifically command and 
control interfaces with advanced multi-user capabilities. 
In this paper, we initially review related work (Section 
II.) and then introduce our design approach (Section III.). 
The design of our multi-modal system for disaster 
management is detailed in the following sections, starting 
with the requirements (Section IV.). According to our 
hierarchical design approach these requirements are 
addressed by base technology components (Section V.) 
which are then combined into the complete system (section 
VI.). Our experiences and initial feedback from the end-users 
are reviewed in section VII.  Finally, we draw conclusions 
from the design and evaluation process and provide an 
outlook on future work.  
95
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

II. 
RELATED WORK 
Most relevant data in disaster management are of spatial 
nature (environment, location of resources) and one key 
challenge is to effectively integrate and unify spatial data 
from different sources. Geographic Information Systems 
(GIS) are well established to deal with the acquisition, 
storage, analysis and presentation of spatial data and are 
established tools in the management of crisis situations in 
many agencies. Challenges arise in the user interface, the 
integration of data and the integration with other systems. 
While the integration of static geo-data from different 
providers is addressed by initiatives like INSPIRE [4], a 
special challenge remains due to the fact that high-bandwidth 
sensors acquire much data in a disaster use-case at the time 
of use that is difficult to handle, analyze and present. 
In addition, a crisis management system must also be 
able to handle different types of imprecise and non-digital 
data sources that arise in typical emergency situations.  
A critical factor in the management of disaster situations 
is the access to current and reliable data. Novel sensors like 
infrared cameras, LIDAR [11] and SAR [3] allow to capture 
geo-spatial data when and where required, e.g., in the case of 
SAR irrespective of weather-conditions and visibility. Key 
challenges are the control of such sensors and the integration 
of such geo-spatial sensor information into a crisis 
management system. Especially in dynamic crisis situations, 
conventional displays interaction techniques fail to address 
the needs of crisis managers. Similarly, current GIS 
interfaces are not well adapted to use “in the field”. The 
extension of GIS with novel techniques for real-time data 
handling and advanced interaction techniques is therefore 
required. In the recent years, multi-modal user interfaces and 
post-WIMP interfaces have seen a rapid development, 
especially the emergence of so-called natural user interfaces 
(NUI), sometimes referred to as reality-based interaction 
[12]. While many NUI techniques have been demonstrated in 
research, the design expertise on the use of such techniques 
in interfaces for large real-world applications like disaster 
management is still very sparse [7]. For the successful 
application of NUIs, it is therefore essential to integrate 
contributions from a number of different research areas 
within a user centered design process to adequately support 
users of disaster management systems.  
A key aspect is an up-to-date and reliable presentation of 
the geo-spatial environment. In the past this has been mostly 
presented by (digital) maps, but advancing technologies in 
sensors, displays and interaction devices enable the 
integration of real-time data and the use of new displays and 
output devices to provide both mobile users (e.g., rescue 
forces) and decision makers with more adequate user 
interfaces. A central challenge is to adequately adapt these 
emerging technologies to geo-spatial information. Again it is 
possible to draw on previous research, e.g., in cartography, 
navigation and geo-visualization, but this has to be adapted, 
integrated and validated in the application context. 
Some earlier research has been conducted in the field of 
disaster management that addresses technological support for 
collaboration and coordination. However, few systems 
address 
the 
existing 
real-life-workflows 
of 
disaster 
management organizations like fire departments, police, 
medical services, etc. While interactive crisis management 
based on new interaction technology is regarded as 
promising [6], its adoption is limited because classical tools 
like pens and paper, paper-maps and plastic labels are proven 
and failsafe. A study of the potential use of multi-touch 
tabletops by the Dutch research institute TNO [13] 
investigated some possible uses of large scale interactive 
displays to provide effective assistance for decision-making 
during a disaster. It investigated in which departments such 
devices could be used, which workflows can be mapped and 
which not. The study shows that even casual users can use a 
tabletop without much learning effort and that it was 
effective in supporting collaborative work. The study also 
identified some problems in the specific system design. By 
using a user centered design approach from the beginning, 
we aim to avoid similar problems in our system.  
III. 
DESIGN APPROACH 
A user centered approach is essential to develop new user 
interfaces that realize the benefits of technologies like post-
WIMP user interfaces. In our project, we collaborate directly 
with the German Federal Agency for Technical Relief 
(Technisches Hilfswerk/THW) and draw on previous 
collaborations with fire fighters to adjust to real-world 
requirements [10]. To design and develop the system, we 
required an approach that takes both the requirements of 
large scale software engineering and usability engineering 
into account, while being adaptable to new and rapidly 
changing base-technologies on which little design expertise 
is available. Several approaches have aimed to integrate 
software engineering and usability engineering activities, 
either at the abstract overarching level of standards (serving 
as a framework to ensure consistency, compatibility, 
exchangeability, and quality), the level of process models 
(providing an organizational framework) or the operational 
process level (direct prescription of activities). For a detailed 
discussion see [9]. For our purposes the level of standards 
was to abstract to provide useful guidance and processes 
described at the operational level did not take the specific 
requirements (especially handling new and immature base-
technologies) into account. Therefore, we started at the 
process model level, using the ISO standard 13407 (now 
replaced by 9241-210:2010) as our base, and derived an 
iterative operational process from this (Figure 1). 
As our project involves stakeholders from many different 
disciplines, it was essential to provide an adequate mental 
model not only within the applications user interface (for the 
users), but also for the stakeholders in the development 
process (end-users, domain experts, experts for different 
base-technologies, 
designers, 
developers). 
We 
have, 
therefore, structured an iterative design process in a three 
level hierarchy (Fig. 1).  
The base-technology level covers the hardware and 
software to enable an interaction or visualization, e.g., for a 
multi-touch device this would cover the sensors and 
associated processing to detect and process the touches of a 
user. In a project dealing with immature base-technologies it 
96
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

is useful to decouple this development into a separate level, 
as many iterations or even changes in the base-technology 
employed (e.g., capacitive, resistive or optical multi-touch 
technologies) may be required.  
 
 
 
 
Figure 1.  Hierarchical iterative design process based on ISO-13407 
The second level covers individual interaction and 
visualization 
techniques. 
These 
employ 
the 
base-
technologies to effect a useful task within the user interface, 
e.g., for a multi-touch device this could be the recognition of 
a specific touch-gesture. Again, it is useful to separate this 
design and development to achieve re-usable components. In 
mature environments this is typically not required because 
mature techniques are provided.  
The 
third 
(application) 
level integrates 
different 
techniques from level 2, to construct a functional user 
interface for the application. A similar layering is applied to 
the data handling parts of the system. 
Within the design process, the different activities 
correspond to common practice, specifically: 
• 
Context of use  
• 
User requirement (definition of scenarios, identification 
of requirements, definition of appropriate measures) 
• 
Production of design solution (from scenarios over 
prototypes to implementation), 
• 
Evaluation (analysis of requirements, review of designs, 
tests of prototypes, and evaluation of the complete 
system). 
IV. 
REQUIREMENTS 
The requirements elicitation and analysis is conducted as 
an iterative, on-going process. We were able to build on 
experience from previous projects with firefighters and the 
German Federal Agency for Technical Relief (THW). 
Existing studies on disaster management were also consulted 
to provide additional background information. 
 
 
Figure 2.  Observed THW exercises 
However, the most important insights were derived from 
interviews, workshops and on-site training exercises 
conducted with disaster managers and technicians from 
THW. During training exercises we were able to observe the 
experts in their real work environment (Figure 2).  
In order not to interfere with the exercises we installed 
video cameras and microphones in the operations center and 
recorded the course of events during the exercise. Based on 
the recordings we were able to analyze the details of each 
workflow. These insights were captured in scenarios that 
cover the different types of crisis situations and their 
information 
and 
command 
requirements, 
user 
roles 
(including professionals, volunteers, local collaborators, 
politicians, press, public), and operating environments (fixed 
and mobile command centers, operations in disaster zones).  
A typical scenario is the management of a flooding 
incident by a THW team. Tools used include paper-based 
maps and special sheets, so called ʻdamage accountsʼ, that 
contain summaries of local incidents and assigned units. 
Sheets are referenced to map locations by using little 
magnetic 
tiles. 
Typical 
activities 
are 
the 
creation, 
manipulation and spatial update of damage accounts.  
These scenarios were then analyzed, to derive key 
requirements at the application, task and interaction level. 
 
General application level requirements include: 
• 
Provide access and control to information the way 
users are used to 
• 
Manage and visualize the current situation in the 
field 
• 
Maintain the benefits of the established robust 
workflow, that is clearly visible to all stakeholders 
• 
Easily integrate non-expert personal (e.g. local 
support staff) 
• 
Clear allocation of control for critical tasks 
• 
Support for information sharing 
• 
Separation of situation display and planning 
• 
Data interface with OGC standard and commonly 
used non-standard data formats 
• 
Enable the integration of imprecise and non-digital 
data sources 
• 
Provide understandable presentations for different 
user groups (expert, local collaborator) 
• 
Enable fast and easy communication and sending of 
commands to mobile units in the field 
• 
Enable integration of software tools that allow a 
more efficient processing of recurring tasks 
Process Planning 
Context Spec. 
Req. Spec. 
Design 
Evaluation 
application / pragmatic level 
techniques / semantic level 
base - technologies / syntactic level 
Design 
Evaluation 
Req. Analysis 
1 : n 
Req. Analysis 
Design 
Evaluation 
n : m 
Process Planning 
Process Planning 
Context Spec. 
Req. Spec. 
Design 
Evaluation 
Context Spec. 
Req. Spec. 
Design 
Evaluation 
Context Spec. 
Req. Spec. 
Design 
Evaluation 
application / pragmatic 
(Level 3) 
techniques / semantic 
(Level 2) 
 
- 
Design 
Evaluation 
Req. Analysis 
1 : n 
Design 
Evaluation 
Req. Analysis 
1 : n 
Req. Analysis 
Design 
Evaluation 
n : m 
Req. Analysis 
Design 
Evaluation 
n : m 
basetechnologies / syntactic 
(Level 1) 
97
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

 
Expected additional benefits for a new system include: 
• 
Seamless and scalable map display 
• 
Support for rich media presentations of information 
• 
Selective use of information layers 
• 
Support for geo-referencing of units and incidents 
and automated transmission of coordinates 
• 
Integration with existing GIS systems 
• 
Access to real-time sensor and location data 
• 
Information filtering and spatial analysis functions 
• 
Support for private workspaces 
• 
Combination of co-located and off-site interaction 
• 
Ability to distinguish between different users; 
traceable interaction 
    
The requirements analysis revealed a number of areas for 
potential improvements and indicated the need for a 
collaborative visualization and interaction environment.  
Natural user interface technologies like multi-touch, tangible, 
gestural and vocal interaction seems promising, but their 
suitability and usability must be confirmed. To ensure that 
the interaction remains coherent and “natural” the design of 
the system must ensure that users develop and maintain a 
suitable mental model of the system in operation.  
V. 
BASE-TECHNOLOGIES AND TECHNIQUES 
 
 
Figure 3.  The useTable and it’s interaction possibilities (pen, multi-touch, 
tangible puck) 
As described previously, we address the design and 
development at three hierarchical levels. The separate 
consideration of the base-technology and interaction / 
visualization technique level allows to change base-
technologies during development (if required) and to develop 
interaction and visualization techniques that can be reused.  
As the central hardware component for interaction and 
visualization we build on the ‘useTable’ [14], a flexible 
visualization table constructed at C-LAB, that supports 
multi-touch, tangible and pen-based interaction. Compared 
to off-the-shelve solutions this approach enables us to adapt 
the technologies and techniques to the application and does 
not limit the design to the constraints of a given hardware 
environment. Using the feedback collected throughout the 
design process, the useTable has evolved into an interaction 
environment adapted to disaster management requirements.  
The ‘useTable’ consists of a 55” display that offers full 
HD image projection. The projector is mounted beneath the 
surface and the image is projected on the top surface by two 
mirrors. For finger-tracking FTIR (Frustrated Total Internal 
Reflection) is applied and objects on the surface are tracked 
using combined DI (Diffused Illumination). The camera on 
the bottom of the table is equipped with a corresponding IR 
filter and is connected to a tracking PC that applies the filter 
and tracking algorithms. The projection surface is equipped 
with an antireflex diffusor sheet that enables pen-based 
interaction by using Anoto digital pens [1, 2]. 
On the software side, a new detection and tracking 
framework for advanced interaction using a depth-sensing 
camera [6], called dSensingNI, was developed. The 
dSensingNI framework is capable of tracking user fingers 
and palm of hands, which enables precise and advanced 
multi-touch interactions as well as complex tangible 
interactions. For tangible interaction arbitrary physical 
objects can be used to control interaction. Using the depth-
sensing camera, physical objects can be used in common 
(2D) actions, such as placing and moving, and also in 3D 
actions, such as grouping or stacking. The depth-sensing also 
allows extending the multi-touch interaction to object 
surfaces without the need for integrated logic and sensors.  
Combing RFID chips and depth-sensing cameras we are 
able to identify and track the persons that are interacting with 
the useTable. This allows applying different functionality to 
different users based on their roles during the interaction, a 
central requirement not addressed by off-the-shelve multi-
touch tables. 
Using the useTable and dSensingNI as base technologies, 
a number of different interaction and visualization 
techniques have been implemented. These techniques enable 
experiments with users, e.g., to study the usability 
differences between touch input, pen-input and the use of 
interaction-objects. A key advantage of the interactive 
display in the disaster management application is the ability 
to rapidly switch between different maps and map 
representations. Using a layer concept different maps and 
additional information (e.g., airborne imagery) can be mixed 
while maintaining the established workflow. The extension 
of 
the 
visualization 
beyond 
map-display 
allows 
experimenting with integration of derived information (e.g., 
danger zones, uncertainty) as well as task dependent map 
generalization and highlighting strategies.  
Insights from these studies are used to guide the 
development at the base technology level. For example, 
experience showed that in some scenarios a strict separation 
between visualization of the current situation and the 
planning of future actions is essential. Our design approach 
allows to adapt to these requirements by modifying and 
extending the set of available base technologies. To provide 
an intuitive separation we extended the useTable into an L-
Shape display. The L-Shape employs the useTable for 
planning as described. An additional wall display was added 
to visualize the current situation.  
98
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

VI. 
SYSTEM 
Building on the base technologies and interaction 
techniques we have implemented various iterations of a 
functional prototype that covers the functionality required by 
the THW to handle a regional flooding incident.  
 The system integrates, analyzes and presents the spatial 
data pertaining to the situation. To achieve this, it integrates 
digital maps, air- and space-borne imagery and a 3D terrain 
model. The use of a 3D terrain model enables important 
interactive analysis functions not available in the traditional 
setup using paper-maps, e.g., the calculation of number of 
pumps required to transport water along a specific trajectory.  
According to the layering concept the system is 
structured into three different levels. 
Essential tasks at the application level to be supported in 
this use-case are the communication and update of the 
current situation in the field (requirements: display of maps 
and additional information layers; editing of damage 
accounts), the planning of future actions and assignation of 
units (information input; communication). These in turn 
require appropriate interaction and visualization techniques. 
For the visualization we started with a digital equivalent 
of the forms, signs and labels that are standardized and 
familiar to the THW staff. Additional features not present in 
the conventional environment include the possibility to 
overlay additional (geo-referenced) information layers, 
dynamic changes of symbolization and the level of detail 
presented, as well as the possibility to zoom, pan and rotate 
the map (Figure 3).  
 
Figure 4.  Situation overview with damage acount 
On the interaction side techniques were required to 
control the system and visualization (system control), as well 
to conduct primary tasks, e.g., the creation and update of 
damage account documents that capture the current situation, 
and the assignment of vehicles and entities to different 
damages in the field.  
Early feedback indicated, that for editing damage 
accounts the use of a digital pen (mimicking the traditional 
paper forms) was regarded as preferable to traditional 
hardware keyboard or on-screen software keyboard input 
(Figure 4). 
 
Figure 5.  Editing damage acounts 
In practice sessions, the digital pen also proved to be 
more suitable for marking and planning tasks than finger-
touch input, as it allows more precise input.  
While multi-touch gestures for scrolling, zooming and 
rotating have become popular with smartphones and tablets, 
early tests revealed that these are not suitable in our 
application scenario: Multiple users touching simultaneously 
(e.g., to comment) can easily lead to undesired changes. In 
the disaster management application usually one person 
should be in charge of the map representation, e.g., when 
changing the data layers displayed or the map scale.  
Tangible interaction ([12]) using a physical puck (see 
Figure 2, bottom right) provides a suitable interaction 
technique: The puck is placed on the useTable surface - by 
moving and rotating the puck the map can be translated and 
zoomed. Since there is only one puck, it is always clear to all 
collaborators who is currently controlling the map. This 
considerably enforces group and interaction awareness.  
Since all information is available in digital form the 
system enables simulation and planning capabilities, that are 
not available in the conventional workflow. E.g., in planning 
the transport of water between two locations the systems 
provides support to calculate the number of pipe sections and 
the number of pumps required (using a digital terrain model 
for the calculation). The system improves on the state of the 
art at all three levels - at the syntactic level by extending the 
scope of geo-spatial data-sources from static maps towards a 
wide range of (dynamic) data sources, at the semantic level 
by providing analysis function and at the pragmatic level 
through a geo-visualization component that exploits the 
benefits of post-WIMP interaction techniques.  
 
VII. EXPERIENCES AND FEEDBACK 
As explained in section 3 the ongoing development takes 
place in close collaboration with the intended end-users from 
THW. In addition to formative evaluation that guides the 
development we have also conducted initial tests with 
experts from the THW and also discussed the system with 
members of the THW authority. The feedback has been very 
positive. Even small improvements enabled by the digital 
map (e.g., switching maps while keeping the data and 
99
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

annotations geo-referenced) caused enthusiastic responses. 
Process improvements enabled by having all data in digital 
form (e.g., the calculation of the number of required pumps) 
lead to significant improvements in efficiency (in the 
example of  calculating water pipelines a simply drawing of 
the intended connection with immediate feedback replaces a 
manual process that required 30min with paper maps and 
required significant experience to avoid calculation errors). 
Initial feedback also led to a number of interesting 
insights into post-WIMP interaction techniques. E.g., while 
multi-touch gestures for rotation and translation are well 
established and one of the typical showcases for multi-touch 
interaction, we found out that they are not applicable in a 
mission critical multi-user map application. This is due to the 
potential 
for 
un-intended 
and 
non-comprehensible 
transformations and the need for traceable commands. For 
other interactions (especially in temporary local workspaces) 
multi-touch gestures were found suitable. The experience 
with digital pens and tangible indicates that they enable very 
natural interactions in our application context with 
correspondingly high acceptance by users. 
In future work, we aim to complement the formative 
evaluations with more comprehensive user studies and tests, 
to study the usability of different interaction and 
visualization techniques. In addition, we also aim to study 
the impact on the cognitive workload of users and examine 
the potential physiological dangers that may be incurred by 
prolonged use of a large-scale post-WIMP display. While the 
ergonomic requirements of desktop workplaces are well 
understood, the same is not true for new interaction 
environments like the useTable. Established ergonomic 
standards were often ignored in early demonstrators of post-
WIMP techniques because of technology constraints (e.g., by 
limiting lighting levels to reduce IR contamination). And 
while interaction techniques like free-hand gestures are 
intuitive they can also cause a high-level of fatigue. 
VIII. CONCLUSION 
Research is required to exploit the potential of advanced 
visualization techniques and user interaction techniques in 
disaster management and similar applications. The step from 
technology demonstrators to usable real-world systems 
requires adequate tools as well as stable base-technologies 
and the evaluation and validation of different design options. 
In this paper we present an initial step in this direction with a 
focus on user centered design and development in a disaster 
management application. 
A promising extension of the system would be to extend 
the support not only to the planning staff in the command 
center, but also to individual rescue workers in the field. In a 
separate project (FireNet, [16]), we have conducted early 
experiments with a mobile personal sensor network, in which 
each rescue worker was equipped with a sensor node (using 
Sun’s SunSpot as the basis and extending it with GPS 
receiver 
and 
additional 
sensors). 
Integrating 
such 
functionality within the disaster management application 
could further improve situation awareness in the command 
center by allowing real-time tracking of rescue personal as 
well as equipment. Another area for future work concerns 
the extension from the current focus on observation 
(situation awareness) to analysis and prediction. The digital 
data enable the use of analysis functions (as exemplified by 
the pump planning) and a future extension towards 
simulation/prediction could be useful, especially in dynamic 
natural disaster situations like flooding or fires.  
ACKNOWLEDGMENTS 
The authors wish to thank the German Federal Agency for 
Technical Relief (Technisches Hilfswerk/THW) in Detmold 
for the productive collaboration, specifically Torsten Meier 
and Oliver Charles as well as the students in our students 
project groups at the University of Paderborn. We would 
also like to thank Petre Dini for his helpful comments during 
the revision of the paper. 
REFERENCES 
 
[1] ANOTO: http://www.anoto.com; (last accessed 11.01.2012). 
[2] Haller, M., Brandl, P., Leitner, J., and Seifried, T.: Large interactive 
surfaces based on digital pens. In: 10th Interantional Conference on 
Humans and Computers, pp. 172-177 (2007). 
[3] Hanssen, R.: Radar Interferometry: Data Interpretation and Error 
Analysis, Kluwer Academic, 2001. 
[4] Inspire Directive (Directive 2007/2/EC of the European Parliament 
and of the Council of 14 March 2007): http://inspire.jrc.ec.europa.eu/ 
(last accessed 11.01.2012). 
[5] Jokela, T.: An Assessment Approach for User-Centred Design 
Processes. In: Proceedings of EuroSPI 2001. Limerick: Limerick 
Institute of Technology Press (2001). 
[6] Jung, H, Nebe, K., Klompmaker, F., and Fischer, H.: Authentifizierte 
Eingaben auf Multitouchtischen, Proc. Mensch und Computer, 
Chemnitz, Germany, 2011, pp. 305-309. 
[7] Kobayashi, K., Kakizaki, T., Narita, A., Hirano, M., and Kase, I.: 
Tangible user interface for supporting disaster education. In: 
Proceedings of SIGGRAPH '07, ACM, New York (2007). 
[8] Leitner, J., Powell, J., Brandl, P., Seifried, Th., Haller, M., Dorray, 
B., and To, P.: Flux: a tilting multi-touch and pen based surface. In: 
CHI '09 Proceedings of the 27th international conference extended 
abstracts on Human factors in computing systems, ACM, New York 
(2009), pp. 3211--3216. 
[9] Nebe, K.: Integration von Usability Engineering und Software 
Engineering, Dissertation, University of Paderborn, Shaker, 2009. 
[10] Nebe, K., Müller, T., and Klompmaker, F.: An Investigation on 
Requirements for Co-located Group-Work using Multitouch-, Pen-
based- and Tangible-Interaction. In: Proceedings of the HCII2011, 
Springer-Verlag, New York, 2011, pp. 90-99. 
[11] Shan, J. and Toth, C. (Eds.): LIDAR: Topographic Laser Ranging and 
Scanning: Principles and Processing, CRC Press, 2008. 
[12] Terrenghi, L., Kirk, D., Richter, H., Krämer, S., Hilliges, O., and 
Butz, A.: Physical handles at the interactive surface: Exploring 
tangibility and its benefits. In:   Proceedings of the working 
conference on Advanced visual interfaces, ACM, New York (2008), 
pp. 138—145. 
[13] TNO: http://www.tno.nl/ (last accessed 11.1.2012). 
[14] UseTable: http://www.usetable.de; (last accessed 11.1.2012). 
[15] van Oosterom, P.; Zlatanova, S.; and Fendel, E. (Eds.): Geo-
information for Disaster Management, Springer, 2005. 
[16] http://www.ikg.uni-hannover.de/index.php?id=602&type=98 
100
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

