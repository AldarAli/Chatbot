Shift-Invariant Motif Discovery in Image Processing
Sahar Torkamani and Volker Lohweg, Senior Member, IEEE
inIT – Institute Industrial IT
Ostwestfalen-Lippe University of Applied Sciences
Lemgo, Germany
Email: {sahar.torkamani, volker.lohweg}@hs-owl.de
Abstract—Nowadays, the boost of optical imaging technologies
results in more data with a faster rate are being collected.
Consequently, data and knowledge discovery science has become
an attractive and a fast growing topic in several industry and
research area. Motif discovery in image processing aims to tackle
the problem of deriving structures or detecting regularities in
image databases. Most of the motif discovery methods ﬁrst
convert images into time series and then attempt to ﬁnd motifs
in such data. This might lead to information loss and also the
problem of inability to detect shifted and multi-scale image motifs
of different size. Here, a method is proposed to ﬁnd image
motifs of different size in image datasets by applying images
in original dimension without converting them to time series.
Images are inspected by the Complex Quad Tree Wavelet Packet
transform which provides broad frequency analysis of an image
in various scales. Next, features are extracted from the wavelet
coefﬁcients. Finally, image motifs are detected by measuring
the similarity of the features. The performance of the proposed
method is demonstrated on a dataset with images from diverse
applications, such as hand gesture, text recognition, leaf and plant
identiﬁcation, etc.
Keywords–Motif discovery; Image processing; Wavelet transfor-
mation.
I.
INTRODUCTION
In this new millennium, the growth of digital computation
and telecommunication has resulted in a ﬂood of information.
Most of this information is in the form of text, graphics, pic-
tures, videos or integrated multimedia. In order to analyse and
acquire efﬁcient information from such datasets, data mining
and machine learning tasks are essential. These tasks can be
categorized into clustering, classiﬁcation, anomaly detection
and motif discovery.
In order to perform the aforementioned tasks, one needs
to have information such as number of clusters or classes,
prototype patterns/images for each class and a given image
query to ﬁnd [1]. The problems of clustering or classifying im-
ages as well as ﬁnding the query images in an image database
have been investigated during last decades [2]–[4]. However,
the problem of deriving structures or detecting regularities
in image databases is rather new topic and investigated by
researchers [5]. Detecting frequently repeated unknown images
in a data base without any prior information is called motif
discovery. The term motif ﬁnds its origin in genetics and
Deoxyribonucleic Acid (DNA) sequence. A sequence motif
in a DNA is a widespread amino-acid sequence pattern which
shows a biological signiﬁcance [6]. However, this term was
ﬁrst triggered by Patel et al. [7] in time series data mining.
Motifs provide valuable insights about the investigated
problem to the user. In the past decade, huge research effort
has been performed on this topic [5] [8]. However, most of
the image motif discovery methods, ﬁrst convert images into
one-dimensional time series and then attempt to ﬁnd motifs in
such data. This might leads to information loss and also the
problem of inability to detect shifted and multi-scale motifs of
different size [9]. Correspondingly, a method is proposed to
ﬁnd shifted and multi-scale motifs of different size in image
datasets by applying the images in original dimension without
converting them to one dimensional time series.
The paper is structured as follows: the related work in motif
discovery for image data type is described in Section II. The
proposed approach is explained in Section III. Next, evaluation
of this method and the obtained results are illustrated in Section
IV. Finally, the directions of the future work and a conclusion
are indicated in Section V.
II.
RELATED WORK
Image and shape analysis have been a matter for discussion
over the past decades. Huge amount of research has been
performed in several image processing tasks such as clustering,
classiﬁcation, query by content, segmentation, etc. [3], [10]–
[13]. Recently, motif discovery in image and shape analysis
has gained great interest. Researchers aimed to link time series
data mining tasks and issues to the image and shape analysis
domain [5] [9] [14].
First, Xi et al. [9] tried to detect image motifs in image
data sets. Nevertheless, their approach is based on representing
an image or a shape in a one dimensional time series. The
main problem of such an approach is that transforming a
two dimensional data to a one dimensional might lead to
information loss.
Chi et al. [15] applied the same procedure as in [9] in
order to detect image motifs in face image data sets. Ye and
Keogh [14], as well as Grabocka et al. [16] extended the
proposed approach in [9] by introducing the term shaplet. After
transforming an image to a one dimensional representation,
instead of analysing the whole time series only a discriminative
subsequence of the time series will be considered. Although
the performance of these methods is promising, but these
approaches transform the data to a one dimensional time series.
Recently, Rakthanmanon and his colleagues [17] aimed to
tackle this problem by detecting motifs in image data without
representing them into a one dimensional signal. In [17], ﬁrst
images are segmented using a sliding window of a ﬁxed
size, then the similarity between these segments are measured
by the generalized Hough transform. One of the drawback
of this method is the size of the window. This results in
inability of detecting motifs with various proportions. En et al.
27
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-549-4
PESARO 2017 : The Seventh International Conference on Performance, Safety and Robustness in Complex Systems and Applications

[18] followed a similar approach, nevertheless they employed
sliding windows with varying sizes of 20, 40, 80, 160 pixels.
In our previous approach [19] motifs in an image data base
are discovered in their original dimension without converting
them to time series. Images are decomposed into several
frequency scales by the dual tree complex wavelet transform
(DTCWT) [20], next features are extracted from the wavelet
coefﬁcients and ﬁnally motif images are found by measuring
the similarity of their features. However, further experiments
showed that the DTCWT is shift tolerance and not shift invari-
ant [21]. For this reason, in this work, an approach is proposed
which is based on a shift-invariant feature extraction method
for motif discovery (SIMD), given in [21]. This method is
applied as core in our approach and explained in the following
section.
III.
PROPOSED APPROACH
The sketch of our approach is depicted in Figure 1.
CQTWP 
SF 
SM 
Detected 
Motifs 
Motif Discovery 
Input 
Data 
Figure 1. The proposed approach; CQTWP is the Complex Quad Tree
Wavelet Packet; SF are the statistical features and SM are similarity
measures.
In the ﬁrst step, images are transformed by the Complex
Quad Tree Wavelet Packet (CQTWP) into a broad frequency
scales. After that features are extracted from the normalized
wavelet coefﬁcients. Finally, motifs are discovered by mea-
suring the similarity between features using various distance
measures. These steps are explained in details in the following.
A. Complex Quad Tree Wavelet Packet Transform
1) 1D-CQTWP: The CQTWP is an extended version of
the DTCWT [20] and it consists of two wavelet packet trees
working parallel to each other; namely “WPT A” and “WPT
B” where “WPT A” represents the real part and “WPT B”
provides the imaginary part of the signal. Figure 2 is a
graphical representation of the “1D-WPT A”, where ↓ 2e and
↓ 2o depict the even and odd down-sampling.
The wavelet and scaling functions of the CQTWP are
deﬁned as:
Deﬁnition
1.
Let
ψa,2J+1(t),ψa,2J+3(t),
ψb,2J+1(t),
ψb,2J+3(t)
and
φa,2J(t),φa,2J+2(t),
φb,2J(t),
φb,2J+2(t)
be the wavelet and scaling functions of the CQTWP. For
convenience both wavelet transforms are considered as
orthonormal. The wavelet and scaling functions in “WPT A”,
1ga
1ha
x[n]
1ga
1ha
↓2e
↓2o
↓2e
↓2o
2C0
2C2
2C1
2C3
2C4
2C6
2C5
2C7
2C8
2C10
2C9
2C11
2C12
2C14
2C13
2C15
2ha
2ga
2ha
2ga
2ha
2ga
2ha
2ga
↓2e
↓2o
↓2e
↓2o
↓2e
↓2o
↓2e
↓2o
↓2e
↓2o
↓2e
↓2o
↓2e
↓2o
↓2o
↓2e
Figure 2. First wavelet packet ﬁlter bank of a two scale CQTWP. The ﬁlters
sga and sha are low and high pass ﬁlters and s is the number of scales.
∀n ∈ N are given by
s+1ψa,2J+1(t) =
√
2
M
X
n=0
sha[n] sφa,2J(2t − n),
s+1ψa,2J+3(t) =
√
2
M
X
n=0
sha[n] sφa,2J+2(2t − n + 1),
s+1φa,2J(t) =
√
2
M
X
n=0
sga[n] sφa,2J(2t − n),
s+1φa,2J+2(t) =
√
2
M
X
n=0
sga[n] sφa,2J+2(2t − n + 1).
Parameter J = 2j where 0 ≤ j < 2s · (s − 1), and s ∈ N is
number of scales.
For “WPT B” the wavelet and scaling functions are deﬁned
in the same manner, but the high-pass ﬁlter sha and the low-
pass ﬁlter sga are replaced by
shb and
sgb respectively. All
ﬁlters are causal so sha,b[n] = 0 and sgb,b[n] = 0 for n < 0.
The CQTWP applies the same ﬁlters as the DTCWT
whereby the ﬁlters are real and orthonormal. In the ﬁrst scale,
the ﬁlters have the even-length of 10 [22] and in the scale
greater than one, ﬁlters have the even-length of 14. Both ﬁlters
form a Hilbert pair due to their design [20].
Deﬁnition 2. Wavelets ψa and ψb with the following property
Ψa(jω) =

−jΨb(jω),
ω > 0,
jΨb(jω),
ω < 0,
are called the Hilbert pair, where Ψ(jω) is the Fourier
transform of ψ(t).
This means, the response of each branch of the “WPT A”
and the corresponding branch of the “WPT B” forms a Hilbert
pair. Consequently, the CQTWP is approximately analytic in
each sub band. The analytic representation has advantages such
as reduction of aliasing.
The CQTWP has another advantage of being shift-invariant
[21]. This property is achieved by decomposing a non shifted
and a shifted version of the input signal in each scale. Shift-
invariance property results in identical wavelet coefﬁcients for
28
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-549-4
PESARO 2017 : The Seventh International Conference on Performance, Safety and Robustness in Complex Systems and Applications

both the original signal and its shifted versions. In other words,
the wavelet and scaling functions of the CQTWP select both
even and odd samples of the signal in order to detect the
occurred shift. For simplicity, the wavelet and scaling functions
of “WPT A” are denoted by ψa,e(t) = ψa,2J+1(t) and
ψa,o = ψa,2J+3(t); and φa,e = φa,2J(t), φa,o = φa,2J+2(t).
The functions of “WPT B” are represented in the same manner.
The proof of shift invariance property of the CQTWP is given
in [21].
2) 2D-CQTWP: The ﬁrst scale of the 2D-CQTWP is
similar to the 2D-discrete wavelet transform [23], where an
image is decomposed into four sub bands namely LL1, LH1,
HL1 and HH1, cf. Figure 3(a). However, in the ﬁrst scale the
2D-CQTWP has two LL, two LH, two HL and two HH sub
bands obtained from both “2D-WPT A” and “2D-WPT B”.
The structure of two scales decomposition of the “2D-WPT
A” is depicted in Figure 3(b) where both low and high-pass
ﬁltered sub bands decomposed further. This property results
in a more ﬂexible and broad frequency decomposition of the
images.
LL1LL2
LL1HL2
HL1LL2
HL1HL2
LL1LH2
LH1LL2 LH1LH2
LL1LH2
HL1LH2
HL1HH2
LH1HL2 LH1HH2
HH1LL2
HH1HL2
HH1LH2
HH1HH2
LL1
LH1
HL1
HH1
(a)
(b)
Figure 3. Structure of two scales decomposition of the “2D-WPT A”: (a) the
ﬁrst scale decomposition, (b) the second scale decomposition.
LL1 is the product of the low-pass function φa() along the
ﬁrst dimension (row) and the low-pass function φa() along the
second dimension (column). LH1 is the product of the low-pass
function φa() along the ﬁrst dimension (row) and the high-pass
function ψa() along the second dimension (column). Similarly
the HL1 and HH1 are labelled, and the index 1 determines the
decomposed scale. The same procedure is performed on each
subband in order to obtain the second scale coefﬁcients.
The wavelet and scaling functions of the 2D-CQTWP are
deﬁned as:
Deﬁnition 3. The “2D-WPT A”of the 2D-CQTWP is char-
acterized by twelve wavelets and four scaling functions. The
2D-wavelet ψ(x, y) = ψ(x)ψ(y) is associated with the row-
column implementation of the wavelet transform. The wavelet
functions for the wavelet packet tree A are given by
ψa,1(x, y) = φa,e(x)ψa,e(y),
ψa,4(x, y) = φa,e(x)ψa,o(y),
ψa,2(x, y) = ψa,e(x)φa,e(y),
ψa,5(x, y) = ψa,e(x)φa,o(y),
ψa,3(x, y) = ψa,e(x)ψa,e(y),
ψa,6(x, y) = ψa,e(x)ψa,o(y).
The rest of the wavelet functions are obtained similarly. The
scaling functions are deﬁned as
φa,1(x, y) = φa,e(x)φa,e(y),
φa,2(x, y) = φa,e(x)φa,o(y),
φa,3(x, y) = φa,o(x)φa,e(y),
φa,4(x, y) = φa,o(x)φa,o(y).
The wavelet and scaling functions of the “2D-WPT B” are
given accordingly.
B. Feature Extraction
Features present the special characters of the data, there-
fore it is important that they are detectable under changes
in proportion, location or even under noise circumstances.
Before extracting features, coefﬁcients of each scale must
be normalized by a normalized histogram. The normalized
histogram H(p) is given by
H(p) =
1
v · u · h(p),
where u, v ∈ N determine the size of the matrix coefﬁcients
and parameter p is number of the histogram bins. The rate in
each bin is presented by h(p).
The extracted features are the ﬁrst four statistical moments
[24], namely, mean value, variance, skewness and kurtosis
which are derived from the wavelet coefﬁcients in both wavelet
packet trees. As CQTWP is shift-invariant, then these features
have identical values even in the case of shift occurrence in
the data.
The scaling and wavelet functions of the CQTWP are
orthonormal, and according to the Parseval’s theorem the
energy of the signal (image) is preserved in the coefﬁcients.
Therefore, the energy of the wavelet coefﬁcients and their
shifted ones are similar. As a result, in addition to statistical
features, energy of the wavelet coefﬁcients is considered as
another feature.
C. Similarity Measures
In order to detect image motifs, the similarity between their
features must be measured. Most cited and applied distance
similarity measures in motif discovery are: two members of the
Minkowski distance family [25], and Dynamic Time Warping
(DTW) [25], which are applied as well in this work. The two
members of the Minkowski distance or Lp-distance, Euclidean
distance (ED) and Canberra distance (CD), both have linear
computational time complexity O(n), and are metric.
The Euclidean distance is obtained by setting p = 2
in Lp-distance. This measure is also known as L2-distance.
Besides the advantages of the Euclidean distance, results of
this similarity measure are not promising in the case of
outliers. The Canberra distance is actually a weighted version
of Manhattan distance or L1-distance, and is useful in the case
of ranking lists or results.
DTW matches various sections of a time series by warp-
ing of the time axis, or ﬁnding the proper alignment. This
similarity measure is more ﬂexible than Euclidean or Can-
berra distance although its time-complexity is O(n2). Besides
its quadratic computational time complexity, still DTW is
one of the most popular approaches for measuring similar-
ity/dissimilarity.
29
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-549-4
PESARO 2017 : The Seventh International Conference on Performance, Safety and Robustness in Complex Systems and Applications

IV.
EXPERIMENTS AND RESULTS
To evaluate the performance of the proposed approach, the
results are analysed by different quality measures, explained
in the following. Next, the captured results of image motif
discovery are presented.
A. Quality Measures
A perfect image motif is the one which matches all the
images in the target class and no other images out of that class.
To evaluate the result following quality measures are employed
[1]: Correct motif discovery rate CR, Sensitivity Sn, Precision
Pr and F-Measure F − M. These quality measures are based
on four possibilities to qualify a motif matching an image;
namely, true positive rate (TP), false negative rate (FN), true
negative rate (TN), and false positive rate (FP).
Correct motif discovery rate is given by CR = n+
N where
N is number of all motifs and n+ is number of correctly
detected motifs. Sensitivity (Sn =
T P
T P +F N ) measures the
capacity of images of the target class correctly matched by
the motif, Precision (Pr =
T P
T P +F P ) provides the fraction of
images of the target class that are matched by the motif and the
images that are not correctly matched by the motif and ﬁnally
both precision and sensitivity are considered by F-Measure
(F − M = 2 · ( P r·Sn
P r+Sn)) [1].
B. Results and Evaluation
The test image data base consists of images from diverse
applications and domains like hand gesture and leaf iden-
tiﬁcation [26], [27]. All the tests are executed with a 3.40
GHz Intel(R) core processor with 8GB RAM. The codes are
performed by MATLAB R2015b [28]. In Figure 4, images of
four groups are depicted. These images have various size and
scale, since the proposed method is able to handle both images
of ﬁxed and variable size.
(a)
(b)
(c)
(d)
Figure 4. Data set of different images.
Among these images, the pictures of hands and leaves are
the most occurred images (top inserted image motifs). Figure
5 shows the inserted image motifs (hand and leaf). In order
to demonstrate the shift-invariant property of the 2D-CQTWP
in feature space, images (b-d) are considered. These images
are the shifted version of the image (a), and image (e) is the
rotated version of image (a). Images (f-j) are different leaf
types.
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
Figure 5. Images of hands and leaves. Images (b-d) are the shifted version
of image (a); Image (e) is the rotated version of image (a). Images (f-j) are
various sorts of leaves.
In the preprocessing step, all the images are presented in
grey-scale, since the colour information is not required. Next,
all the images are sent to 2D-CQTWP and decomposed into
various scales. In this work, the wavelet coefﬁcients of the
second scale are selected, since the amount of noise is usually
reduced in the second scale for the noisy data. The normalized
histogram of the wavelet coefﬁcients are calculated. Figure 6
is the graphical representation of the normalized histogram of
the HL sub band coefﬁcients of “2D-WPT A”.
0
100
0
100
(c)
0
100
0
100
0
100
(a)
0
100
(b)
0
100
(d)
0
100
Figure 6. Normalized histogram of the HL sub band coefﬁcients, obtained
from the corresponding images from Figure 4 (a-d).
As depicted, the histograms of wavelet coefﬁcients from
the two depicted images in each group (a), (b), (c) and (d) are
similar to each other but different to the histograms of other
groups. This helps to determine the variations between various
motif classes (inter-class).
The shift-invariant property of the 2D-CQTWP can be
observed in Figure 7. A hand pattern and its shifted version
is depicted in Figure 7 (a)-(b), where the position of the hand
is changed. The normalized histogram of the HL sub band
coefﬁcients are given in Figure 7 (c)-(d) respectively. Since
the 2D-CQTWP is shift-invariant, the histograms are identical
to each other.
Next, the ﬁve stated features are extracted from the his-
tograms of the wavelet coefﬁcients in each scale. The efﬁ-
ciency of these features are tested by the linear discriminant
30
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-549-4
PESARO 2017 : The Seventh International Conference on Performance, Safety and Robustness in Complex Systems and Applications

(a)
(b)
0
100
p
(c)
0
0.8
H(p)
0
100
p
(d)
0
0.8
Figure 7. (a) a hand pattern; (b) shifted version of image in (a); (c) and (d)
represent the normalized histograms of the HL sub band coefﬁcients from
images (a) and (b).
analysis (LDA) algorithm [1] [29].
LDA is a supervised method which projects the features
from the samples of the two or more classes onto a lower
dimensional space with good class separability in order to
avoid over-ﬁtting and computational costs reduction. This
method provides a combination of features which separates
the classes ideally with the minimum amount of error. The
less the minimum error, the better is merit of the features.
If the data can be separated linearly and correctly the error
will be 0, and if the whole data cannot be classiﬁed linearly
and correctly then the error has its maximum amount of 1.
LDA ﬁnds the most discriminant projection by maximizing
between-class distance and minimizing within-class distance.
The experiments show that for most of the tested features
the minimum error is between 0 and 0.01. Furthermore, the
distance between feature clusters should be as great as possible
to facilitate grouping.
Figure 8 demonstrates the result of the LDA projection of
the two extracted features from the image motifs in Figure
5. It is obvious that the distance between the two groups is
large enough in order to separate them correctly. Moreover,
the distance between features belonging to the same image
motif group (represented on the projection line) is minimized.
Since the 2D-CQTWP is shift-invariant, the ﬁrst four
hand images are as close as possible to each other and their
projection on the projection line is at the same position. These
images are depicted by the circle red marker. Nevertheless,
the projection of the features extracted from the rotated image
(illustrated by the square red marker) is not at the same position
of other hand images. This illustrates that the 2D-CQTWP is
not rotation invariant, however we are able to detect this image
motif and separate it from other image motifs.
In the last step, the similarity between feature values is
measured by Euclidean, Canberra and Dynamic Time Warping
0
1.2
Variance
0
1.2
Skewness
Shifted hand motifs
Rotated hand motif
Leaf motifs
Figure 8. LDA projection of the two features from the hand and leaf image
motifs; the distance between features within an image motif group is as
minimum as possible and the distance between features of different image
motif groups is large enough.
distance measures. Both Euclidean distance and Dynamic Time
Warping performed similar in the case of small datasets (less
than 50 images). These measures were able to detect 22 image
motifs out of 25 inserted image motifs. The Canberra distance
was able to discover 21 image motifs in the same data base.
Results of the stated similarity measures in Section III-C
are depicted in Table. I. The presented results are obtained
TABLE I. Evaluating results of detected motifs, CR: Correct motif discovery
rate, F-M: F-Measure, Sn: Sensitivity, Pr: Precision; ED: Euclidean distance,
DTW: Dynamic Time Warping and CD: Canberra distance.
Similarity Measure
CR(%)
F-M(%)
Sn(%)
Pr(%)
ED
84.62
84.94
85.27
84.62
DTW
84.62
84.94
85.27
84.62
CD
83.85
84.82
85.83
83.85
from the tested data set with 50 different inserted image motifs.
Both Euclidean and DTW distance outperformed the Canberra
distance. Euclidean and DTW distances detected the same
amount of image motifs but, the image motifs were different.
Since, the aforementioned image motif discovery approaches
in Section II convert an image in a one dimensional signal, it
is not possible to benchmark the performance of the proposed
method against them.
V.
CONCLUSION AND OUTLOOK
In this contribution, a method is presented to detect image
motifs of various size. Detection of image motifs is performed
within three steps: First, the Complex Quad Tree Wavelet
Packet transform is applied to provide a comprehensive anal-
ysis of images in various frequency scales. This wavelet
transform has efﬁcient properties such as being shift-invariant.
Moreover, its ability for analytic representation, is helpful in
order to reduce aliasing.
31
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-549-4
PESARO 2017 : The Seventh International Conference on Performance, Safety and Robustness in Complex Systems and Applications

The ﬁrst four statistical moments and energy of the wavelet
coefﬁcients are extracted as feature values. Since motif discov-
ery is an unsupervised task, there is no information about the
tested images. Consequently, the statistical features are applied
in this work, but depending on the task it is possible to employ
other types of features.
In the next step, motifs are detected by measuring the
similarity between their feature values. The performance of the
proposed method is evaluated by different quality measures.
One advantage of the proposed method is its ability to consider
both images of ﬁxed and variable lengths. In addition, as
images are not transformed to a one dimensional representation
form, no information is lost.
In the next approach, our aim will be to ﬁnd the proper
scale and nodes of the 2D-CQTWP with the best information
content. Moreover, it is desirable to detect motifs within
various images without segmenting them via a sliding window.
ACKNOWLEDGMENT
This research is partly supported by the International Grad-
uate School of Intelligent Systems in Automation Technology
(ISA), which is run by scientists of the Faculty of Computer
Science, Electrical Engineering and Mathematics and the Fac-
ulty of Mechanical Engineering of the University of Paderborn
and the Institute of Industrial Information Technologies (inIT)
of the Ostwestfalen-Lippe University of Applied Sciences.
REFERENCES
[1]
E. Alpaydın, Introduction to Machine Learning, 2nd ed.
Cambridge:
The MIT Press, 2010.
[2]
S. A. Khan, M. Nazir, S. Akram, and N. Riaz, “Gender classiﬁcation
using image processing techniques: A survey,” in 2011 IEEE 14th
International Multitopic Conference.
IEEE, 2011, pp. 25–30.
[3]
S. S. Nath, G. Mishra, J. Kar, S. Chakraborty, and N. Dey, “A survey of
image classiﬁcation methods and techniques,” in International Confer-
ence on Control, Instrumentation, Communication and Computational
Technologies (ICCICCT).
IEEE, 2014, pp. 554–557.
[4]
S. Gangwar and R. P. Chauhan, “Survey of clustering techniques
enhancing image segmentation process,” in 2015 Second International
Conference on Advances in Computing and Communication Engineer-
ing.
IEEE, 2015, pp. 34–39.
[5]
P. Esling and C. Agon, “Time-series data mining,” vol. 45.
ACM,
2012, pp. 1–34.
[6]
M. K. Das and H.-K. Dai, “A survey of dna motif ﬁnding algorithms,”
BMC bioinformatics, vol. 8 Suppl 7, 2007, p. S21.
[7]
P. Patel, E. Keogh, J. Lin, and S. Lonardi, “Mining motifs in massive
time series databases,” in Proceedings IEEE International Conference
on Data Mining.
IEEE, 2002, pp. 370–377.
[8]
S. Torkamani and V. Lohweg, “Survey on time series motif discovery,”
in Journal: Wiley Interdisciplinary Reviews: Data Mining and Knowl-
edge Discovery.
Article ID: WIDM1199, 2017.
[9]
X. Xi, E. Keogh, L. Wei, and A. Mafra-Neto, “Finding motifs in a
database of shapes,” in Proceedings of the 2007 SIAM international
conference on data mining.
SIAM, 2007, pp. 249–260.
[10]
W. Hu, R. Hu, N. Xie, H. Ling, and S. Maybank, “Image classiﬁcation
using multiscale information fusion based on saliency driven nonlinear
diffusion ﬁltering,” IEEE Transactions on Image Processing, vol. 23,
no. 4, 2014, pp. 1513–1526.
[11]
N. M. Zaitoun and M. J. Aqel, “Survey on image segmentation
techniques,” Procedia Computer Science, vol. 65, 2015, pp. 797–806,
elsevier.
[12]
K. Zhang, W. K. Leow, and Y. Cheng, Performance Analysis of Active
Shape Reconstruction of Fractured, Incomplete Skulls.
Springer
International Publishing, 2015, pp. 312–324.
[13]
B. K. Gayathri and P. Raajan, “A survey of breast cancer detection
based on image segmentation techniques,” in 2016 International Con-
ference on Computing Technologies and Intelligent Data Engineering
(ICCTIDE’16).
IEEE, 2016, pp. 1–5.
[14]
L. Ye and E. Keogh, “Time series shapelets: A new primitive for
data mining,” in Proceedings of the 15th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, ser. KDD ’09.
ACM, 2009, pp. 947–956.
[15]
L. Chi, Y. Feng, H. Chi, and Y. Huang, “Face image recognition based
on time series motif discovery,” in IEEE International Conference on
Granular Computing.
IEEE, 2012, pp. 72–77.
[16]
J. Grabocka, N. Schilling, M. Wistuba, and L. Schmidt-Thieme, “Learn-
ing time-series shapelets,” in Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining,
ser. KDD ’14.
ACM, 2014, pp. 392–401.
[17]
T. Rakthanmanon, Q. Zhu, and E. Keogh, “Mining historical documents
for near-duplicate ﬁgures,” in IEEE 11th International Conference on
Data Mining.
IEEE, 2011, pp. 557–566.
[18]
S. En, C. Petitjean, S. Nicolas, and L. Heutte, “Segmentation-free
pattern spotting in historical document images,” in 13th International
Conference on Document Analysis and Recognition (ICDAR).
IEEE,
2015, pp. 606–610.
[19]
S. Torkamani, H. D¨orksen, and V. Lohweg, “Multi-scale motif discovery
in image processing,” in Workshop on Probabilistic Graphical Models,
Heidelberg, Germany, Oct 2015.
[20]
I. W. Selesnick, R. G. Baraniuk, and N. G. Kingsbury, “The dual-
tree complex wavelet transform,” Signal Processing Magazine, IEEE,
vol. 22, no. 6, 2005, pp. 123–151.
[21]
S. Torkamani and V. Lohweg, “Shift-invariant feature extraction for
time-series motif discovery,” in 25. Workshop Computational Intelli-
gence, ser. Schriftenreihe des Instituts f¨ur Angewandte Informatik - Au-
tomatisierungstechnik am Karlsruher Institut f¨ur Technologie, vol. 54.
KIT Scientiﬁc Publishing, 2015, pp. 23–45.
[22]
A. F. Abdelnour and I. W. Selesnick, “Symmetric nearly shift-invariant
tight frame wavelets,” IEEE Transactions on Signal Processing, vol. 53,
no. 1, 2005, pp. 231–239, IEEE.
[23]
C. S. Burrus, R. A. Gopinath, and H. Guo, Introduction to wavelets and
wavelet transforms: A primer.
Upper Saddle River and NJ: Prentice
Hall, 1998.
[24]
A. D. Polyanin and A. V. Manzhirov, Handbook of mathematics for
engineers and scienctists.
Boca Raton: Chapman & Hall/CRC, 2007.
[25]
M. M. Deza and E. Deza, Encyclopedia of distances.
Springer, 2009.
[26]
M. B. Stegmann and D. D. Gomez, “A brief introduction to statistical
shape analysis,” 2002, informatics and Mathematical Modelling, Tech-
nical University of Denmark, DTU.
[27]
P. F. B. Silva, A. R. S. Marcal, and da Silva R. A., “UCI machine
learning repository: Leaf data set,” 2014, last access: 06.04.17.
[Online]. Available: https://archive.ics.uci.edu/ml/datasets/Leaf
[28]
MathWorks,
“MATLAB,”
2017,
last
access:
06.04.17.
[Online].
Available: https://de.mathworks.com/products/matlab.html
[29]
C. Bayer, M. Bator, U. M¨onks, A. Dicks, O. Enge-Rosenblatt, and
V. Lohweg, “Sensorless drive diagnosis using automated feature extrac-
tion, signiﬁcance ranking and reduction,” in 18th IEEE Int. Conf. on
Emerging Technologies and Factory Automation (ETFA 2013).
IEEE,
2013, pp. 1–4.
32
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-549-4
PESARO 2017 : The Seventh International Conference on Performance, Safety and Robustness in Complex Systems and Applications

