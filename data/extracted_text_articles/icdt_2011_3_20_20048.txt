Language Recognition With Locality Preserving Projection
Jinchao Yang, Xiang Zhang, Li Lu, Jianping Zhang, Yonghong Yan
ThinkIT Speech Lab, Institute of Acoustics
Chinese Academy of Sciences
Beijing, P.R.China
{yangjinchao, xzhang, llu, jzhang, yonghong.yan}@hccl.ioa.ac.cn
Abstract - In this paper, we introduce locality pre-
serving projection (LPP) to language recognition under
the support vector machine (SVM) framework. The suc-
cess of the use of total variability in language recognition
shows that the global structure and linear manifold pre-
serve discriminative language dependent information.
The proposed LPP language recognition system believes
the local structure and nonlinear manifold also contain
discriminative language dependent information. Exper-
iment results on 2007 National Institute of Standards
and Technology (NIST) language Recognition Evalua-
tion (LRE) databases show LPP language recognition
system combining total variability language recognition
system gains relative improvement in EER of 11.7% and
in minDCF of 9.6% comparing to total variability lan-
guage recognition system in 30-second tasks, and further
improvement is obtained combining with state-of-the-art
systems. It leads to gains of 13.8% in EER and 20.2% in
minDCF compare with the performance of the combina-
tion of the MMI and the GMM-SVM systems.
Index Terms— language recognition, language total vari-
ability, PCA, LDA, LPP, SVM,
I. INTRODUCTION
The aim of language recognition is to determine the language
spoken in a given segment of speech. Phoneme recognizer
followed by language models (PRLM) and parallel PRLM
(PPRLM) approaches that use phonotactic information have
shown very successful performance [1][2]. In PPRLM, sev-
eral tokenizers are used to transcribe the input speech into
phoneme strings or lattices [3][4], which are scored by n-gram
language models. It is generally believed that phonotactic fea-
ture and spectral feature provide complementary cues to each
other [1]. The spectral features of speech are collected as in-
dependent vectors. The collection of vectors can be extracted
as shifted-delta-cepstral acoustic features, and then modeled
by Gaussian Mixture Model (GMM). The result was reported
in [5].
The approach was further improved by using dis-
criminative train that named Maximum Mutual Information
(MMI).
Several studies using SVM in language recognition to
form GMM-SVM system [6][7]. SVM as a classiﬁer maps
the input feature vector into high dimensional space then
separate classes with maximum margin hyperplane.
It is
important to choose an appropriate SVM feature expansion.
Recently total variability approach has been proposed in
speaker recognition [8][9], which uses the factor analysis to
deﬁne a new low-dimensional space that named total variabil-
ity space. In this new space, the speaker and the channel vari-
ability are contained simultaneously. In ours previous work,
we introduce the idea of total variability to language recog-
nition and propose total variability language recognition sys-
tem. The success of the use of total variability in language
recognition show that most of the discriminative language
dependent information is captured by low-dimensional sub-
space.
Actually, total variability method is a classical applica-
tion of the probabilistic principal component analysis (PPCA)
[10]. In our previous work about language recognition with
language total variability, we can say that PCA+LDA is used
to reduce the dimension of GMM supervector before SVM
model.
Locality preserving projection (LPP) [11][12] that
gains an embedding that preserves local and linear informa-
tion is different from PCA and LDA which effectively pre-
serve global structure and linear manifold.
In this paper, LPP algorithm is carried out after PCA to
a conversation to get the supervector that contain discrimi-
native language dependent information by the local structure
and nonlinear manifold. We can call laplacian supervector
extraction method.
SVM classiﬁers are employed to model the laplacian su-
pervector and LDA and diagonal covariance gaussians are
used as backend in Language Score Calibration.
This paper is organized as follows: In Section 2, we give a
simple review of Support Vector Machines and total variabil-
ity language recognition system. Section 3 shows the lapla-
cian algorithmic procedure. In Section 4, the proposed lan-
guage recognition system is presented in detail. corpora and
evaluation are given in Section 5. Section 6 gives out experi-
51
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

mental result. Finally, we conclude in Section 7.
II. BACKGROUND
A. Support Vector Machines
An SVM [13] is a two-class classiﬁer constructed from sums
of a kernel function K(·, ·):
f(x) =
N
X
i=1
αitiK(x, xi) + d
(1)
where N is the number of support vectors, ti is the ideal out-
put, αi is the weight for the support vector xi, αi > 0 and
PN
i=1 αiti = 0. The ideal outputs are either 1 or -1, depend-
ing upon whether the corresponding support vector belongs to
class 0 or class 1. For classiﬁcation, a class decision is based
upon whether the value, f(x), is above or below a threshold.
The kernel K(·, ·) is constrained to have certain properties
(the Mercer condition), so that K(·, ·) can be expressed as
K(x, y) = φ(x)
′φ(y)
(2)
where φ(x) is a mapping from the input space (where x lives)
to a possibly inﬁnite dimensional SVM expansion space. We
refer to the φ(x) as the SVM features.
B. Language Total Variability
In total variability speaker recognition, the factor analysis is
used to deﬁne a new low-dimensional space that named to-
tal variability space and contains the speaker and the channel
variability simultaneously. Then, the intersession compensa-
tion can be carried out in low-dimensional space. We deﬁne
language total variability space.
a. Language Total Variability Space Estimation
There is only one difference between total variability space T
estimation and eigenvoice space estimation in speaker recog-
nition [9]. All the recordings of speaker are considered to
belong to the same person in eigenvoice estimation, however,
in total variability space estimation, a given speaker’s entire
set of utterances are regarded as having been produce by dif-
ferent speakers. We suppose that different conversation from
one language is produced by different languages.
For a given conversation, the language and variability de-
pendent supervector is denoted in equation (4).
M = mubm + Tw
(3)
where mubm is the UBM supervector, T is language total
variability space, and the member of the vector w are total
factor. We could call w language total factor vector. We can
think the language total factor vector model a new feature ex-
tractor that project a conversation to a low rank space T to
get a language and variability dependent language total factor
vector w.
b. Intersession Compensation
After the feature extractor, the intersession compensation can
be carried out in low-dimensional space. We use the Linear
Discriminant Analysis approach (LDA) to intersession com-
pensation. All the language total factor vector of the same
language are think as the same class.
w∗ = Aw
(4)
By LDA transformation in equation (4), the language total
factor vector w is projected to new axes that maximize the
variance between language and minimizing the intra-class
variance. The matrix A is trained using the dataset show in
session 5 and is contained of the more larger eigenvectors of
equation (5).
Sbν = λSwν
(5)
where λ is the diagonal matrix of eigenvalues. The matrix Sb
is the between class covariance matrix and Sw is the within
class covariance matrix.
III. LAPLACIAN ALGORITHM PROCEDURE WITH
LOCALITY PRESERVING PROJECTION
Since total variability method is a classical application of
the probabilistic principal component analysis (PPCA). In
our previous work about language recognition with language
total variability, we can say that PCA+LDA is used to reduce
the dimension of GMM supervector before SVM model. As
a type of PCA, the total variability method does not need
language information.
And PCA seeks directions that are
efﬁcient for representation.
LDA seeks directions that are
efﬁcient for discrimination. PCA+LDA that aims to preserve
the global structure and linear manifold is successful for
language recognition, then the local structure and nonlinear
manifolds may be useful to language recognition.
Though LPP is still a linear technique, it seems to reserve
important aspects of the intrinsic nonlinear manifold structure
by preserving local structure. The algorithmic procedure in
this paper is formally stated below.
A. PCA projection
In session 2.2, a conversation is projected to language total
variability space to get a language and variability dependent
language total factor vector w. Actually, it is a PCA projec-
tion. In this paper, LPP is used after PCA projection.
B. Constructing the nearest-neighbor gragh
Let G denote a gragh with m nodes. We put an edge between
nodes i and j while i is among k nearest neighbors of j or j is
among k nearest neighbors of i.
52
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

C. choosing the weights
If nodes i and j are connected, let
Sij = e−
(wi−wj )2
t
(6)
The justiﬁcation for this choice of weights can be traced back
to [14].
D. eigenmap
Compute the eigenvectors and eigenvalues for generalized
eigenvector problem:
WLW T a = λWDW T a
(7)
where D is a diagonal matrix whose entries are column sums
of S, Dij = P
j Sji. L = D − S is the Laplacian matrix.
The ith row of matrix W is wi. Let a0, a1, ..., al−1 be the
solution of (7), ordered according to their eigenvalues, 0 ≤
λ0 ≤ λ0 ≤ ... ≤ λl−1. Thus, the embedding is as follows:
wi −→ yi = AT wi, A = (a0, a1, ..., al−1)
(8)
where y is a l dimensional vector, and A is a transformation
matrix.
IV. THE PROPOSED LANGUAGE RECOGNITION
SYSTEM
Fig. 1. The Proposed Language recognition System
Figure 1 show the frame of the proposed Language recog-
nition system.
A. MSDC Feature Extraction
The MSDC feature in the system is 7 MFCC coefﬁcient con-
catenated with SDC 7-1-3-7 feature, which are in total 56 di-
mension coefﬁcients each frame. MSDC feature refers to this
56 demension feature in my system. Nonspeech frames are
eliminated after speech activity detection, then 56 dimension
MSDC feature are Extraction. Then feature warping and cep-
stral variance normalization are applied on the previously ex-
tracted MSDC feature which results that each feature is nor-
malized to mean 0 and variance 1.
B. Laplacian supervector Extraction
In our system, we use MSDC feature after compensation
of channel factors. Firstly, total variability spaces are esti-
mated as session 2.2. MSDC feature, UBM and Language-
independent Total variability space T are need as equation
(3) in language total factor vector extraction (actually it is a
PCA Projection). Then LPP transformation matrix is learned
as session 3. The embedding is as follows to each GMM
supervector x:
x −→ y = AT x
(9)
A = AP CAALP P
(10)
where AP CA denote the transformation matrix of PCA as
session 2.2. And ALP P denote the transformation matrix of
LPP, while the algorithmic procedure is in session 3. We call
A Laplacian transformation matrix.
C. SVM Model and Language Score Calibration
Our experiments are implemented using the SVMTorch [15]
with a linear inner-product kernel function.
Calibrating conﬁdence scores in mutiple-hypothesis lan-
guage recognition has been studied in [16]. We should esti-
mate the posterior probability of each hypotheses and make
a maximum a posterior decision. In standard SVM-SDC sys-
tem [7], log-likelihood ratios (LLR) normalization is applied
as a simple backend process and is useful. Suppose S =
[S1...SL]t is the vector of L relative log-likelihoods from the
L target languages for a particular message. Considering a ﬂat
prior, a new log-likelihood normalized score S′
i is denoted as:
S′
i = Si − log(
1
L − 1
X
j̸=i
eSj)
(11)
A more complex full backend process is given [7] [17],
LDA and diagonal covariance gaussians are used to calculate
the log-likelihoods for each target language and achieve im-
provement in detection performance.
In this paper, the two backend processes are used in lan-
guage recognition system. Experiments also show the similar
conclusion that the LDA and diagonal covariance gaussians
backend process is superior over log-likelihood ratios normal-
ization.
V. CORPORA AND EVALUATION
The experiments are done using the NIST LRE 2007 evalua-
tion database. There are 14 target languages in corpora used
in this paper: Arabic, Bengali, Chinese, English, Farsi, Ger-
man, Hindustani, Japanese, Korean, Russian, Spanish, Tamil,
That and Vietnamese. The task of this evaluation was to de-
tect the presence of a hypothesized target language for each
test utterance. The Training data was primarily from Call-
friend corpora, Callhome corpora, mixer corpora, OHSU cor-
pora, OGI corpora and LRE07Train. The development data
53
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

consist of LRE03, LRE05, LRE07Train. We use equal error
rate (EER) and the minimum decision cost value (minDCF)
as metrics for evaluation.
VI. EXPERIMENTS
Firstly, total variability Language recognition System (PCA+LDA)
is experimented, then export to Laplacian Language recogni-
tion System (PCA+LPP).
Table 1. Results of the MMI system, GMM-SVM system, the
total variability and proposed Laplacian language recognition
systems on the NIST LRE07 30s corpus.
System
EER
MinDCF
MMI (a)
3.62
3.78
GMM-SVM (b)
2.65
2.61
total variability (c)
3.15
2.61
Laplacian (d)
3.29
2.83
In Table 1, we give the performance of the MMI, the
GMM-SVM, the total variability and proposed Laplacian lan-
guage recognition systems on NIST 2007 language recogni-
tion Evaluation 30s corpus after score backend.
EER and
minDCF are observed. With the performance comparison, we
can see that total variability and proposed Laplacian language
recognition systems achieve performance comparable to that
obtained with state-of-the-art approaches, which shows my
proposed systems are effective and Laplacian language recog-
nition system indeed contains language information.
Table 2. Score Fusion and Super Join Results of the total vari-
ability and proposed Laplacian language recognition systems
on the NIST LRE07 30s corpus.
System
EER
MinDCF
total variability (c)
3.15
2.61
Laplacian (d)
3.29
2.83
c+d Score Fusion
2.78
2.36
c+d Super Join
2.87
2.51
Table 2 shows the score fusion and super join results of
the total variability and proposed Laplacian language recog-
nition systems. The score fusion leads to gains of 11.7% on
EER and 9.6% minDCF compare with the performance of
the total variability language recognition systems, And su-
per join gains 8.9% on EER and 3.8% minDCF. The result
show Laplacian language recognition system that preserves
local and nonlinear information includes different language
information comparing to total variability language recogni-
tion system that preserves global and linear information.
Table 3.
Results of the combination of MMI system and
GMM-SVM system, and the combination of the MMI sys-
tem, GMM-SVM system, total variability system, and Lapla-
cian system on the NIST LRE07 30s corpus.
System
EER
MinDCF
Score Fusion (a+b)
2.47
2.42
Score Fusion(a+b+c)
2.18
2.06
Score Fusion(a+b+c+d)
2.13
1.93
Table 3 shows the results of the combination of the MMI
system, the GMM-SVM system, the total variability system
and the Laplacian system. EER and minDCF are observed.
In language recognition evaluation, MMI and GMM-SVM
are primary acoustic system. Usually the combination of the
MMI system and the GMM-SVM system is the given perfor-
mance of acoustic system. It leads to gains of 13.8% on EER
and 20.2% minDCF compare with the performance of the
combination of the MMI and GMM-SVM systems. Lastly,
ﬁgure 1 gives DET curves for each system and fusion of each
system.
  0.2 
 0.5  
  1   
  2   
  5   
  10  
  20  
  0.2 
 0.5  
  1   
  2   
  5   
  10  
  20  
  40  
  60  
False Alarm Rate(in %)
Miss Rate (in %)
 
 
MMI(a)
GSV−SVM(b)
(a+b)
Total Variability(c)
Laplacian(d)
(c+d)
(a+b+c)
(a+b+c+d)
Fig. 2. DET curves for each system and fusion of each system
VII. CONCLUSIONS
In this paper, we propose a new language recognition sys-
tem by introducing LPP to language recognition. while our
previous propose total variability language recognition sys-
tem show discriminative language dependent information is
contained by global structure and linear manifold, the new
54
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

language features of Laplacian supervector that preserve lo-
cal structure and nonlinear manifolds also contain discrimi-
native language dependent information. SVM classiﬁers are
employed to model the new language features and LDA and
diagonal covariance gaussians are used as backend in Lan-
guage Score Calibration. Experiments show that combining
two systems LPP and total variability can achieve relative im-
provement in EER of 11.7% and in minDCF of 9.6% com-
pare to only total variability in 2007 NIST language Recogni-
tion Evaluation databases 30-second tasks. Further improve-
ment of relative improvement of 13.8% in EER and 20.2% in
minDCF is obtained combining with state-of-the-art systems,
comparable with the performance of the combination of the
MMI and GMM-SVM systems.
Acknowledgments
This work is partially supported by The National Science and
Technology Pillar Program (2008BAI50B00), National Natu-
ral Science Foundation of China (No. 10925419, 90920302,
10874203, 60875014).
References
[1] M.A. Zissman,
“Language identiﬁcation using phoneme
recognition and phonotactic language modeling,” in IEEE In-
ternational Conference On Acoustics Speech And Signal Pro-
cessing. Institute Of Electrical engineers INC (IEE), 1995,
vol. 5, pp. 3503–3503.
[2] Y. Yan and E. Barnard, “An approach to automatic language
identiﬁcation based on language-dependent phone recogni-
tion,” in icassp. IEEE, 1995, pp. 3511–3514.
[3] J.L. Gauvain, A. Messaoudi, and H. Schwenk,
“Language
recognition using phone latices,” in Eighth International Con-
ference on Spoken Language Processing. ISCA, 2004.
[4] W. Shen, W. Campbell, T. Gleason, D. Reynolds, and E. Singer,
“Experiments with lattice-based PPRLM language identiﬁca-
tion,”
in IEEE Odyssey 2006: The Speaker and Language
Recognition Workshop, 2006, 2006, pp. 1–6.
[5] P.A. Torres-Carrasquillo, E. Singer, M.A. Kohler, R.J. Greene,
D.A. Reynolds, and J.R. Deller Jr, “Approaches to language
identiﬁcation using Gaussian mixture models and shifted delta
cepstral features,” in Seventh International Conference on Spo-
ken Language Processing. Citeseer, 2002.
[6] H. Li, B. Ma, and C.H. Lee, “A vector space modeling ap-
proach to spoken language identiﬁcation,” IEEE Transactions
on Audio, Speech, and Language Processing, vol. 15, no. 1, pp.
271–284, 2007.
[7] W.M. Campbell, J.P. Campbell, D.A. Reynolds, E. Singer,
and P.A. Torres-Carrasquillo,
“Support vector machines for
speaker and language recognition,” Computer Speech & Lan-
guage, vol. 20, no. 2-3, pp. 210–229, 2006.
[8] N. Dehak, R. Dehak, P. Kenny, N. Brummer, P. Ouellet, and
P. Dumouchel, “Support vector machines versus fast scoring in
the low-dimensional total variability space for speaker veriﬁca-
tion,” in Proc. International Conferences on Spoken Language
Processing (ICSLP), 2009, pp. 1559–1562.
[9] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet,
“Front-end factor analysis for speaker veriﬁcation,” submitted
to IEEE Transaction on Audio, Speech and Language Process-
ing.
[10] M.E. Tipping and C.M. Bishop, “Probabilistic principal com-
ponent analysis,” Journal of the Royal Statistical Society: Se-
ries B (Statistical Methodology), vol. 61, no. 3, pp. 611–622,
1999.
[11] X. He and P. Niyogi, Locality preserving projections, Citeseer,
2005.
[12] X. He, S. Yan, Y. Hu, P. Niyogi, and H.J. Zhang, “Face recogni-
tion using laplacianfaces,” IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, pp. 328–340, 2005.
[13] N. Cristianini and J. Shawe-Taylor,
“Support Vector Ma-
chines,” Cambridge University Press, Cambridge, UK, 2000.
[14] M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral
techniques for embedding and clustering,” Advances in neural
information processing systems, vol. 1, pp. 585–592, 2002.
[15] R. Collobert and S. Bengio, “SVMTorch: support vector ma-
chines for large-scale regression problems,”
The Journal of
Machine Learning Research, vol. 1, pp. 143–160, 2001.
[16] N. Brummer and D.A. van Leeuwen,
“On calibration of
language recognition scores,”
in IEEE Odyssey 2006: The
Speaker and Language Recognition Workshop, 2006, 2006, pp.
1–8.
[17] E. Singer, P.A. Torres-Carrasquillo, T.P. Gleason, W.M. Camp-
bell, and D.A. Reynolds,
“Acoustic, phonetic, and discrim-
inative approaches to automatic language identiﬁcation,”
in
Eighth European Conference on Speech Communication and
Technology, 2003.
55
ICDT 2011 : The Sixth International Conference on Digital Telecommunications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-127-4

