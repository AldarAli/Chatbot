On Benefits of Interactive Online Learning in Higher Distance Education 
Repeating a Learning Analytics Project in the Context of Programming Education 
 
Klaus Gotthardt, Bernd J. Krämer 
Faculty of Mathematics and Computer Science 
FernUniversität in Hagen 
Hagen, Germany 
klaus.gotthardt@fernuni-hagen.de 
bernd.kraemer@fernuni-hagen.de 
Johannes Magenheim, Jonas Neugebauer 
Department of Didactics of Informatics 
University of Paderborn 
Paderborn, Germany 
jsm@uni-paderborn.de 
jonas.neugebauer@uni-paderborn.de
 
 
Abstract - All generations of the Web have given rise to new 
technologies and services for teaching and learning. Ample 
research projects have investigated their impact on educational 
processes and students’ learning outcomes. This is especially 
true for the comparison of online and face-to-face learning or 
distance and classroom instruction. There is, however, very 
little original research and evaluation evidence about the im-
pact of incorporating web-based learning technology and mul-
timedia content in higher distance education. This article pre-
sents the results of two studies with distance students of 
FernUniversität. These results address four strands: the stu-
dents’ competence gain on key course topics; an analysis of the 
learning behavior of a subset of online students; a performance 
comparison between online students and a control group using 
classical correspondence learning materials; and online student 
satisfaction with the online version of the course and the e-
learning environment. Authentic comments from two students 
who were involved in all evaluation strands supplement the 
formal findings. 
Keywords - distance learning, Web-based learning, learning 
analytics, competence analysis, learning performance, technology 
assessment. 
I. 
 INTRODUCTION 
Distance education describes an organization of educa-
tional processes in which teachers and students are separated 
in space and time and communicate with each other mainly 
asynchronously. Traditional teaching media were and still are 
prepackaged self-instructional correspondence courses that 
allow distance students to study at the time and location of 
their choice.  
Starting out in the early ‘90s, this teaching model was 
challenged by the advent of the Worldwide Web. The initial 
Web of Information opened up the possibility to exploit inter-
active multimedia content and deliver learning materials 
electronically. It also allowed their adaptation to different 
learning styles by drawing appropriate learning objects from 
the Web or content repositories. Hypermedia structures in 
online courses could be used to lead students on different 
paths at the students’ own paces through the course content, 
depending on their individual performances in intermediate 
online tests and quizzes.  
The Web of Services then brought about the possibility to 
include all kinds of collaboration and communication services 
in online learning activities. More recently, the Social Web 
offered a range of social software tools that allowed students 
to participate actively and in real-time in educational process-
es, independent of any physical proximity.  
Numerous studies have compared online and face-to-face 
learning to understand the impact of web-based learning 
technology on higher and extended education. There is, how-
ever, very little original research and evaluation evidence 
about the possible advantages and disadvantages of incorpo-
rating web-based learning technology and multimedia content 
in higher distance education. To fill this gap, an interdiscipli-
nary group of researchers from FernUniversität in Hagen and 
the University of Paderborn performed a quantitative study 
and experimental research on online learning. Subjects were 
volunteer students from FernUniversität in Hagen, the only 
German language distance teaching university. The project 
set out to find answers to the following research questions: 
1. 
To what extent can the learning objectives of a dis-
tance-learning course be achieved with traditional 
custom textbooks and asynchronous tutoring activi-
ties using email, text forums, and phone? 
2. 
Is there a significant difference in competence gains 
and learning outcomes between students relying on 
traditional distance learning settings and online dis-
tance students? 
3. 
Do students accept or even prefer online learning 
technologies to traditional correspondence course-
ware? 
During the summer semester 2012, we invited all 693 stu-
dents enrolled in the distance-learning course on “Object-
oriented Programming” (OOP) to participate in a pre- and 
post-test evaluating the students’ modeling and comprehen-
sion competencies in the topic area. Further, we asked for 
volunteers who would agree to study a new interactive ver-
sion of a selected course module online and allow us to log 
and evaluate their online behavior anonymously. The group 
of online students was also asked to perform a technology 
assessment at the end of the course.  
Study settings, evaluation methods and tools, and prelimi-
nary evaluation results of this project were published in the 
proceedings of the Sixth International Conference on Mobile, 
350
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Hybrid, and On-line Learning [1]. Due to the small number of 
subjects participating in all phases of the study, these results 
were, however, statistically not reliable. Therefore we decid-
ed to repeat the study in the winter semester 2013/14 with the 
same course but a different group of students.  
In the following section, we summarize the evolution of 
higher distance education over the last two decades. We also 
provide a brief review of significant research close to the 
work reported here. Section III sketches the learning setting 
in which the study was performed, including course content, 
learning objectives, and the newly designed digital learning 
environment. Section IV presents the quantitative study de-
sign and sketches the research methods and tools used in the 
analysis of the data gathered in the form of log data and sur-
veys. This section also describes the modifications applied to 
the competence test used in the second survey. Section V 
summarizes the results of the competence analysis. The inter-
pretation of the log data collected from online students is 
described in Section VI.  Key results of the technology ac-
ceptance test are reported in Section VII. Then, Section VIII 
relates the test scores of those students who attended the final 
examination to their online retention times and interaction 
with learning objects. Here, we also compare the examination 
results of the online students with those of a control group of 
offline students. The latter used traditional correspondence 
materials for learning. Section IX summarizes the feedback 
from two online students who were interviewed 6 months 
after the end of the course. Section X concludes the paper and 
presents the lessons learned from the two projects. 
 
II. 
BACKGROUND AND RELATED RESEARCH 
The new technological and communicative options of the 
Web allowed educators at distance-teaching universities to 
tailor course design and instruction to interactive online learn-
ing [2]. As an intermediate step, existing textbooks were 
supplemented with interactive multimedia systems. They 
were delivered electronically through the Web and allowed 
students to experiment with simulated or animated virtual 
worlds, access remote labs, and thereby get meaningful feed-
back on student-system interactions in real-time [3]. Asyn-
chronous e-mail, chat tools, or text forums and synchronous 
webinars allowed educators to narrow the distance between 
fellow students and teachers concerning social interaction 
possibilities.  
Ever since distance education emerged, researchers and 
educators tried to answer the question whether distance learn-
ing can be as effective as learning in face-to-face settings.  T. 
Russel has summarized a rich body of literature addressing 
this question in an annotated bibliography entitled “The No 
Significant Difference Phenomenon” [4]. The essence of this 
work is not the claim that distance education and classroom 
education are equally effective for all students, courses, and 
instructors but that individual variations and extraneous vari-
ables may render many of the findings in related literature 
inconclusive. A huge meta-analysis of empirical literature on 
the comparison between distance education and classroom 
instruction basically supports Russel’s conclusion [5]. 
More than thousand empirical studies can be found in re-
search literature comparing online to face-to-face learning, 
measuring student learning outcomes, or proposing innova-
tive learning designs. The meta-analysis in [6] found that, on 
average, online students performed slightly better than stu-
dents who attended face-to-face instruction. In detail, opinion 
is divided, however. Ferguson and Tryjanowski can show that 
students in their face-to-face class scored significantly higher 
than online students [7]. In contrast, Cooper’s study on a 
“Fundamentals of Computer Applications” course with 94 in-
class and 37 online students, could not observe any signifi-
cant difference in scoring between the two groups [8].  
Today, many people equate distance learning and online 
or e-learning, but, in fact, these concepts do not refer to the 
same thing [9]. Physical distance is not a defining feature for 
e-learning, but it is for distance learning. The target popula-
tion of distance universities is students who are unable to 
regularly attend on-campus classes due to family or work 
commitments or because of personal limitations. These stu-
dents depend on the institution “distance-teaching university” 
because it offers: 
• Complete distance learning programs,  
• Learning materials especially designed for self-study, 
• Effective asynchronous communication media,  
• Flexible examination conditions,  
• Well-trained tutors, and  
• A network of study centers. 
Against this background, the key question is not: “online or 
face-to-face learning”.  Rather, the issue is how online learn-
ing solutions can be effectively used in higher distance educa-
tion. 
III. 
SETTING THE SCENE 
In this section we introduce the learning setting both for 
A) traditional higher distance learning with correspondence 
material and established distance learning support tools and 
B) a newly designed online version of a selected course mod-
ule. 
A. Course Objectives, Structure and Learning Support 
Since 2004, the course OOP is taught on the basis of cor-
respondence materials consisting of 41 chapters summing up 
to a 500 pages textbook. The course objectives claim that by 
the end of the course, successful students will be able to: 
• Identify, explain, and properly apply at least 10 core 
concepts of object-oriented programming. They in-
clude: object, class, class relationships, single and 
multiple inheritance, interface, constructor, method 
and method invocation, variable, array, data and object 
type, program flow statements, and package. 
• Design appropriate data structures and algorithms 
solving a given problem. 
• Govern the essential sequential programming concepts 
of Java. 
• Develop the data structures and algorithms of a design 
solution into a correct Java program. 
• Understand and modify a given Java program. 
351
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

• Distinguish between and implement iterative and re-
cursive algorithms. 
• Read and interpret simple UML diagrams. 
The course content is divided into 7 course modules that 
are processed every other week. Besides course-related learn-
ing objectives, more detailed learning objectives are set for 
each module. Questions and self-assessment exercises with 
sample solutions and software-based self-tests providing real-
time feedback aim to provide stimuli and learning guidance.  
In distance education, regular task blocks, course-related 
newsgroups and forums replace exercise hours of on-campus 
instruction. Students edit the optional task blocks in a 14-day 
cycle. To this end, they use FernUniversität’s online exercise 
system to submit their solutions and get automated responses. 
Student solutions can be IMS QTI-type interactions, but in 
this course they mostly consist of program snippets or small 
programs the students have to develop themselves. Once a 
program is uploaded, it is automatically compiled on the 
remote university server. If the compilation succeeds, the 
resulting code is run through a style checker and a sequence 
of pre-defined test cases. The style checker and test results 
provide a quick feedback and aim to motivate the student to 
improve his or her solution if some test cases failed or the 
programming style is weak. This edit-compile-test cycle can 
be repeated as often as needed until the final delivery sched-
ule is reached. A tutor then corrects the submitted final ver-
sion of each student program manually to provide individual 
feedback.  
B. Digital Learning Environment  
Distance students possess above average experience with 
self-directed learning. They are used to organize their learn-
ing freely but have limited time to participate in synchronous 
learning events. This requires the barriers for group learning 
actions to be kept low.  
To challenge students in the online group to perform more 
demanding learning activities, we redesigned both the instruc-
tional design and the content of one module of the course 
substantially. The selected course module deals with program 
exceptions, testing, program documentation, and error han-
dling. The learning objectives of this module require both 
conceptual knowledge and practical skills. According to 
Bloom's taxonomy of learning objectives [10], these are, in 
particular, application, analysis, and synthesis skills. Concept 
knowledge is needed when students must:  
• Recognize the benefits of meaningful program docu-
mentations. 
• Understand the difficulties and limitations of program 
testing.  
• Identify and properly apply error-handling techniques.  
• Avoid typical programming errors. 
Practical skills are needed when students must: 
• Document their own and foreign Java programs in a 
meaningful way using Javadoc. 
• Write Java code that raises meaningful exceptions and 
properly intercepts them or passes them on to a higher 
level of exception handling. 
• Plan test cases, perform program tests, and interpret 
test results. 
The online course module, which can be access at [11], 
addresses exactly the same course topics and learning objec-
tives as the textbook version. Besides short reading passages 
in the form of HTML pages capturing basic factual 
knowledge students need to learn, the online module includes 
ten interactive learning objects. They are implemented in 
Adobe Flash and allow the students to experiment with alter-
native solutions of program designs, explore, modify and 
explain the behavior of given program solutions, evaluate 
their own solutions, or grasp the semantics of certain pro-
gramming concepts in a trial-and-error fashion. To encourage 
teamwork between physically remote students, we also de-
signed a few new learning tasks that involve 2-4 students 
playing different roles in the programming task, such as a 
programmer or tester. Furthermore, we proposed homework 
assignments whose solution was composed of several pro-
gram modules to be contributed collaboratively by several 
students. 
 
Figure 1.  Crash lab providing explorative programming experiences.  
Fig. 1 depicts one of the interactive learning objects, 
called crash lab. It allows students to explore the behavior of 
programming exceptions. They drag one of the program 
statements in the pile underneath the editor window on the 
left into a code frame provided in the editor window. Then, 
they push the button “Übersetzen” to compile the resulting 
code and run it if the compiler succeeds (in which case the 
button on the right is highlighted). Students are supposed to 
predict whether the different programs they build this way 
will fail or terminate successfully. The results of the test runs 
are collected in the two boxes labeled “vorzeitig abge-
brochen” (aborted) and “regulär beendet” (regularly complet-
ed) at the bottom right.  
An example of a team problem is a simple game with a 
treasure being hidden in an area of 24 cells. A player has to 
find a treasure by moving strategically in this area. One stu-
dent has to develop the program component controlling the 
game; the partner has to implement the behavior of the player. 
Some constraints are imposed on the behavior of both com-
ponents, which may lead to program exceptions.  
The online course was delivered through the learning 
management system Moodle [12]. It was supervised and 
352
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

tutored by the same faculty who also taught the textbook 
version. The installation of Moodle we used in the study was 
seamlessly connected to the learning object repository edu-
sharing [13][14]. This repository enabled the online students 
to share and collaboratively work on their contributions in a 
protected space. The repository functionality allowed them to 
control which access rights to their personal workspace they 
wanted to grant to selected peers. 
IV. QUANTITATIVE STUDY DESIGN 
To acquire data about the students’ perception of the new 
online learning environment and their competence gain on the 
subject of this course, two online questionnaires were devel-
oped.  
Our first experimental study was conducted during the 
summer semester 2012 [1]. It consisted of a: 
• Pre- and post-test to determine the students’ compe-
tence gain in core topics of the course. 
• Learning behavior analysis of a subgroup of online 
students based on various log data.  
• Technology assessment of the online student group.  
• Analysis of homework submissions. 
• Written examination at the end of the semester cover-
ing all learning objectives for the course. 
Unfortunately, a disappointingly low number of students 
participated in the post-test performed in July 2012. As a 
consequence, we were unable to determine the competence 
gain of the anonymized subjects. In addition, we observed 
hardly any overlap between students who participated in the 
competence tests and in the final examination. This fact pre-
vented us from correlating competence gains and examination 
scores. For similar reasons, a statistically valid comparison of 
the final grades of online and offline students was impossible.  
These weak results encouraged us to repeat the study in 
the winter semester 2013/14. To reduce the problems of the 
first study, we adapted some study parameters that had caused 
problems before. These modifications are discussed in Sec-
tion C.  
A. Learner Satisfaction Analysis  
Besides acquiring a detailed insight into the students’ 
online behavior, we also wanted to collect their satisfaction 
with the online course material and the learning environment 
provided.  
Based on Davis’ technology acceptance model (TAM) 
[15], Venkatesh’s extension TAM2 [16], and Brooke’s Sys-
tem Usability Scale (SUS) [17], we developed a technology 
acceptance questionnaire tailored to our specific learning 
environment. Both TAM and SUS have been successfully 
used in the study of e-learning environments [18][19]. We 
also considered the most recent model TAM3 [20] but did not 
find extensions like “Perceptions of External Control”, 
“Computer Anxiety”, “Perceived Enjoyment” and others 
relevant for our learning analysis purpose. The online ques-
tionnaire we gave our students includes 28 Likert-scaled 
items and covers the following analytical dimensions: 
• Preferred computer equipment. 
• Individual experiences with e-learning. 
• Usability of the online course and tool environment.  
• Subjective evaluation of prior knowledge on the 
course topics.  
• Communication and cooperation competencies gained 
in the online course. 
• Comparison of the effectiveness of the study process 
with traditional distance learning textbooks and the 
online study material, respectively. 
At the end of the semester, the questionnaire was deliv-
ered online via LimeSurvey [21]. Those students who attend-
ed the online-course and also agreed to participate in the 
behavior analysis were asked to work on it. 
B. Competence Measurement  
For the competence analysis, a new competence meas-
urement instrument was applied. The instrument was devel-
oped in several steps in the project “Measurement Procedure 
for Informatics in Secondary Education (MoKoM)” by re-
searchers in the fields of didactics of informatics and psy-
chology. The German Research Foundation (DFG) funded the 
MoKoM project. The goal of the project was the development 
of an empirically sound competence model and correspond-
ing measurement instrument for the domains of informatics 
modeling and system comprehension. 
In the first step, several relevant national and international 
curricula and syllabi like the ACM/IEEE Computing Curricu-
lum 2011 [22] and the ACM Model Curriculum for K-12 
Computer Science [23] were analyzed and used to derive a 
theory based competence framework. It consisted of three 
cognitive and one non-cognitive competence dimension. 
To refine this framework, the Critical Incident Technique 
was used to conduct 30 expert interviews with experienced 
persons in the field of computer science education. Each 
interviewee was presented four problem scenarios and was 
asked how (s)he would solve the given task [24]. The scenar-
ios were derived from the theoretical competence framework. 
The transcribed interviews were analyzed by means of the 
qualitative content analysis [25]. The results were used to 
refine and restructure the competence framework. This pro-
cess led to an empirically refined competence model with six 
dimensions: K1 System Application, K2 System Comprehen-
sion, K3 System Development, K4 dealing with system com-
plexity and K5 Non-Cognitive Skills [26]. 
In the next step, for each competence a test item was de-
veloped to compile a measurement instrument for the consid-
ered domains. The items were developed following the prin-
cipals of Situational Judgment Tests and the experiences 
made in other competence measurement studies like TIMMS 
and PISA [27]. Based on detailed competence descriptions, 
tasks for every single competence item were created. To en-
sure an objective and coherent evaluation, a comprehensive 
grading manual was created alongside the test items. Due to 
the resulting large amount of test items, the instrument was 
split into six blocks and then compiled into six booklets with 
three blocks each. Additionally, each booklet contained a 
block for the non-cognitive dimensions of the competence 
model. These booklets could be completed in 90 minutes and 
were used in a test with 800 students in upper secondary 
education computer science classes.  
353
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

To use the instrument in the study at hand, all items were 
transformed into respective online versions. The resulting 
questionnaire was delivered through the online survey system 
LimeSurvey. Due to the nature of the questions, some items 
could not be transformed properly, e.g., the drawing of dia-
gram. For each of these items, a decision had to be made 
whether the associated competence was appropriate for the 
field of OOP. If the answer was positive, a new item had to be 
developed. As we wanted to exploit the breadth of the compe-
tence model and expected a high number of participants in the 
test, the partition into six booklets was kept for the first online 
survey conducted in 2012. 
To allow for an anonymous survey, an additional item 
was added to ask for a unique code. This code was generated 
individually for each student based on personal information. 
This allowed for the association of pre- and post-tests without 
revealing the students’ identities. 
To assess the competence gain of the students, they were 
asked to complete the online survey created from the 
MoKoM measurement instrument twice: Once at the start of 
the term and once at the end. The students were randomly 
partitioned into six groups. Each group had access to one of 
the six test booklets provided in LimeSurvey. 
C. Revision of the Competence Analysis Questionnaire 
The underwhelming results of the first survey (see Table 
V and Section V.A) led us to redesign the test instrument for 
a second survey that was carried out during the winter semes-
ter 2013/14. To get more students to finish the test, the target 
was to enable its completion within 60 minutes. For this rea-
son, instead of testing the complete range of competences of 
the MoKoM model, a subset of items especially tailored to 
the requirements of the course at hand was selected. To 
choose the appropriate test items, we matched the learning 
objectives of the course with the competence descriptions of 
the MoKoM model. This way, we could accumulate a proper 
selection of relevant test items and combine them into a cus-
tom-designed competence test. This tailoring also eliminated 
the need for several test booklets and let us assume that more 
students than before would now work on the same set of 
items. 
D. Study Groups in the First Study 
Before the course started in summer semester 2012, we 
invited all 693 students enrolled to evaluate the online compe-
tence test. To raise their interest in the study, we announced 
to give away three books in a raffle based on voluntarily 
provided email addresses. 146 students followed this request 
and worked on the test with different degrees of completion. 
They formed the competence study group. 
Then we asked for students who would be willing to study 
the online course unit and allow us to track their online be-
havior.  We found 12 volunteers who formed the online stu-
dent group. This group was also invited to participate in a 
technology assessment survey conducted after the online 
course module was passed.  
At the end of the course, we asked the whole student pop-
ulation again to evaluate the competence (post-) test and mark 
it with the same code the students had used in the pre-test.  
Finally, we wanted to differentiate the test scores of final 
examination participants among online and offline (textbook) 
students to search for significant differences between both 
groups. The written examination test typically consists of 6 or 
7 problems addressing a) major levels of Bloom’s cognitive 
taxonomy and b) core course topics. A typical test includes:  
1. A block of 5-7 questions testing the understanding of 
core OOP concepts and their relationships. 
2. One or two problems for which the meaning of a given 
program must be interpreted. 
3. Four constructive tasks including the design of a class 
structure and the implementation of selected methods, 
the design of an iterative and a recursive algorithm for 
a given computational problem, and the design of sim-
ple data structures and algorithms operating on them. 
The test lasts two hours and is conducted simultaneously 
in more than 10 different locations throughout Germany, 
Switzerland, and Austria (usually in lecture halls of universi-
ties) under the supervision of FernUniversität academic per-
sonnel.  
E. Study Groups in the Second Study 
980 students were enrolled in the course OOP in the win-
ter semester 2013/14. Following the communication process 
of the first study, all students were informed about the study 
before the course started. Simultaneously, they were asked to 
work on the competency pre-test and volunteer as online 
students and thereby allow us to track their online behavior.  
To inspire the participation in these studies and balance 
out the extra workload, we offered all students 5 bonus point 
if they completely answered both competence tests and up to 
5 bonus points for online students, depending on their degree 
of online learning activity. 
165 students volunteered for the online study. As we con-
sidered these students as particularly motivated, we decided 
to accept only 98 subjects as online students and form a con-
trol group of motivated students, called offline students, from 
the other 67 volunteers.  
At the end of the course, all students enrolled were asked 
to work on the post-test using the same personal code they 
had used in the pre-test. The online students were asked to 
evaluate their subjective impression of the learning environ-
ment by answering the TAM questionnaire. The students who 
had registered for the final examination were requested to 
provide their personal code on the examination paper and thus 
allow us to correlate competence test results with the exami-
nation test scores. 
V. 
RESULTS OF THE COMPETENCE ANALYSES 
We separate our discussion of the analysis results into two 
subsections, one for each study. 
A. First Study performed 2012 
Unfortunately, the participation in the survey performed 
during the summer semester 2012 was disappointing. Of the 
693 students registered for the course, only 57 started their 
pre-test booklet and just 19 students in total finished more 
than 75% of all items. These numbers got even worse for the 
post-test with 30 subjects who started to work on the booklets 
354
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

and only 5 finished it. IN both tests intervals around 150 users 
visited the survey page but only one third and one fifth, re-
spectively, started to answer the first item at all. Since the 
participants who finished their booklet were spread across the 
six booklet variants, the useable data for each test item was 
too small to get any meaningful results regarding the compe-
tence gains of the students. Only one student completed both 
tests. Even tendencies supporting or falsifying any of the 
hypotheses we started from are hard to state.  
To at least get a notion what to change for subsequent tests, 
the answering habits of the participants were examined. By 
counting the number of students who tried to solve an item 
over all 87 datasets, it is easy to see that the completion rate 
drops to 60% after only four items (see Fig. 2). The eighth 
item was completed by only 50%. 
 
Figure 2.  Relative completion rate of survey items. 
A reason for these results might have been the length of 
the competence test. Tailored to be conducted in a German 
classroom setting, where two successive lessons equals 90 
minutes, it seems the time a distance-learning student is will-
ing to spend on his own in answering the survey is considera-
bly shorter. Examination of the response rates for online sur-
veys even showed that the ideal length for an online survey is 
thirteen minutes or less [28]. This ideal timeframe is, howev-
er, too short for a competence test. As discussed in Section 
IV.C, the average workload for the revised competence test 
was reduced to 60 minutes. 
B. Second Study Conducted 2013/14 
This time, the pre-test was called 162 times. After cleanup 
of duplicates and removal of data sets not completely an-
swered, 126 evaluable records remained. The post-test was 
edited only by half as many students and just produced 49 
complete data sets. Based on the personal code used on both 
tests, 40 data sets could be paired with each other to compare 
the results on an individual though anonymous basis.  
In each test, a maximum of 77 points could be obtained 
from 21 tasks. The blue bars in Fig. 3 depict the points stu-
dents achieved in the pre-test, while the red bars on top indi-
cate the gain in points in the post-test. Four students even 
showed a negative gain ranging between -3 and -28 points 
less in the second test. 
In the pre-test, an average of 43.54 (57%) points was 
achieved, in the post-test 55.15 (72%). Based on the personal 
code, 40 data sets of both tests could be paired to see the 
competence gain of individual students (see Fig. 3). 
 
 
Figure 3.  Competence gain of anonymized students. 
Based on this reduced sample, the mean value increases in 
the pre-test to 46.38 points (60%), while it nearly stagnates in 
the post-test with 55.25 points (72%). This result represents a 
significant improvement in skills (t (39) = 5.68, p <0.001). 
The boxplot in Fig. 4 visualizes the distribution of points. 
 
 
Figure 4.  Boxplot of the competence points achieved. 
Out of the 40 pairs of competence analysis sets, 16 were 
delivered from online students and 24 from students in the 
control group (offline students). With regard to the total num-
ber of points, both groups indicate significant increases. 
However, the online group improved by an average of 10.7 
points compared with 7.6 in the control group (see Fig. 5). 
 
Figure 5.  Boxplot of the competence points achieved. 
0%#
10%#
20%#
30%#
40%#
50%#
60%#
70%#
80%#
90%#
100%#
1# 2# 3# 4# 5# 6# 7# 8# 9# 10#11#12#13#14#15#16#17#18#19#20#21#22#23#24#25#26#27#28#29#30#
Rela1ve#comple1on#rate#
Itemnumber#
355
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

But this difference turns out to be insignificant. At the 
item level, the post-test reveals significant differences in two 
tasks: The online group explains the concept “inheritance” 
more reliably and can better explain the function of a stack. 
Fig. 6 depicts the increase of competences on the topic “or-
dering of test sequences” as a bar chart. 
 
 
Figure 6.  Pre- and post-test results of competence test for topic “ordering 
of test sequences. 
The shift from lesser to more points from the first to the 
second test is obvious. 
VI. 
BEHAVIOR ANALYSIS OF ONLINE STUDENTS 
The number of online students involved in the first study 
(12) is too small to provide statistically valid results. Moreo-
ver, as only 6 of the online students took the examination test 
at the end of the semester, these figures are not sufficient to 
compare the outcomes of online and offline students. There-
fore, we focus our discussion about the online behavior of 
distance students enrolled in the course OOP on the results of 
the second study. Here, 57 of the 98 volunteer online students 
we had accepted were finally active in the learning environ-
ment. But first, we sketch the data sources and analysis tech-
niques employed. 
A. Data Sources and Analysis Techniques 
The database for the behavior analysis of the online stu-
dents was compiled from the log data provided by Moodle 
and edu-sharing and the log data captured by the Flash-based 
interactive learning objects. The latter allowed us to see how 
successful a student was in the interaction with a learning 
object, which errors he or she made, and how often a student 
tried to solve a given task implemented by an object.  
All log data were time-stamped. These time-stamps 
helped to integrate the data coming from different sources. 
The raw data were cleaned and integrated to a single database 
that was then analyzed with the help of the business analytics 
software SAS [29]. SAS was particularly used for structure 
and usage mining. Structure mining relies on the links be-
tween information pages and links from within course pages 
to self-assessment examples, homework assignments, forum 
entries, and objects maintained in the repository and work-
spaces.  
The objective of structure mining is to identify recurring 
patterns of behavior, e.g., in the form of paths through the 
learning materials or repeated experiments with exercises and 
programming problems. These paths form a network that 
visualizes how students navigate through the course material 
and the learning environment. Particular indexes of the net-
work analysis are the weighted in- and out-degrees of course 
elements, which indicate the frequencies of visits.  
Usage mining provides useful descriptive statistics. This 
includes:  
• Information about the number of page visits. 
• Retention time, i.e., the cumulative elapsed time be-
tween page views; an average value was assumed for 
exit pages.  
• Typical entry and exit pages.  
B. Results of the Second Behavior Study 
Among the 98 students who had been admitted to the 
online study group, a total of 55 subjects logged-in at least 
once in Moodle. 52 students accessed at least once the online 
content. Of these 52, twelve students spent less than 15 
minutes online, 19 were less than an hour online. As these 
overly short online times seem unreasonable, we chose a 
retention time of one hour as the lower threshold for the data 
analysis. This decision left 33 online students to evaluate.  
The study conditions with remote subjects who learn au-
tonomously render it impossible to know whether a student’s 
online time was fully dedicated study time. However, the 
retention times neither vary conspicuously by clock time, nor 
did we detect meaningless values. A realistic workload for the 
online learning module should, however, be much higher than 
the measured average retention time. It is therefore likely that 
some online students had also used other study materials for 
learning, e.g., the course textbook.  
The number of pages viewed varies from 25 to 696 with 
an average of 169 and a median of 142 page views. The aver-
age time spent on a text page varies from1:00 to 4:51 minutes 
(median: 02:21 minutes). The maximum time spent on a page 
varies between 8:32 and 107:38 minutes (median: 58:41 
minutes). Although we cannot be sure whether students spent 
this time on effective learning, the high values appear plausi-
ble, since they were observed for self-assessment pages. The 
highest values occurred for pages with solutions to self-
assessment tests on the topics “exceptions” and “testing”.  A 
quiz on topic “program documentation” had also very high 
page view figures.  
TABLE I.  
RETENTION TIMES SHARED OF PAGE TYPES 
Chapter 
Documentation Exceptions Testing 
Error han-
dling 
Reading & 
Understanding 
51.68% 
54,64% 
55,52% 
75,34% 
Self-assessment 
30.11% 
42,99% 
32,46% 
18,98% 
Forum 
18.21% 
2,37% 
12,02% 
5,68% 
 
356
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Table 1 shows the retention times students spent on the 
average per chapter on the different page types. We distin-
guish between knowledge acquisition, practice, and forum 
activities. The former occur when students need to read into 
and understand new course topics. In the online course, in-
formation pages are intertwined with self-assessment pages. 
On the latter, students find interactive learning objects and 
programming tasks to test their understanding and give them 
hands-on experience with smaller programs. Each chapter has 
its own forum, which takes a prominent place at the begin-
ning of each chapter.  
Students spent between 52% and 75% of the time on read-
ing and understanding new topics, 20-43% for self-
assessment tests, and 6-18% in the forum. The differences 
may be explained by the fact that Chapter Error Handling 
includes only one self-assessment exercise, while Chapter 
Documentation provides 6, Chapter Exceptions 9, and Chap-
ter Testing 13. Although students visited the forums quite 
frequently, only a few students contributed by posing and 
answering questions or exhibiting an own problem solution. 
The largest proportion of practice time was spent on the topic 
Exceptions. The corresponding chapter offers a few challeng-
ing programming tasks and interactive learning objects for 
self-assessment.  
The usage data collected through the learning objects rec-
ord very detailed observations. They show that about half the 
online students interacted with them quite intensively, where-
as the other online students largely ignored them. In the dis-
cussion of the examination scores in Section VIII we will fall 
back on these observations. 
Interestingly, 30% of the volunteers for online study were 
female students although their share among the students en-
rolled in the course is only 25%. 
 
C. Reading into the Log Data of Interactive Learning 
Objects and Programming Tasks 
The log data captured from the students’ interactions with 
Flash objects and a built-in compiler back-end to compile and 
run own solutions to programming tasks provide some inter-
esting insights. In the following discussion we focus on learn-
ing activities related to Chapter Exceptions because this 
theme was particularly tested in the written examination. 
For instance, 17 online students worked with the learning 
object Crash Lab depicted in Fig. 1. Together they spent 2:40 
hours working with this object. 16 students placed all 6 
statements properly, 9 students created 1-3 own statements. 
Only five students evaluated the predicted behavior of the test 
programs, which was correct in four cases. Students #7, #27, 
#40 and #44 were successful in the first attempt. We explicit-
ly indicate anonymous student identities here because we will 
refer to them later in the discussion of grades in Section VIII. 
Another learning object tests the students’ ability to un-
derstand the passing of exceptions. It is composed of 4 sub-
tests in which the last sub-test can be varied and explored 
several times. 13 students interacted with this learning object, 
summing up 3:11 hours total interaction time. 11 students 
successfully solved the first sub-test and 9 students also com-
pleted the second sub-test. Finally, 5 students worked suc-
cessfully on the other two sub-tests as well, including stu-
dents #14, #27, #40, and #44. We observe a decreased activi-
ty rate with each sub-test, which supports the author’s inten-
tion to increase the difficulty of the sub-tests step by step.  
An investigation of the log data of a learning object in 
which students had to write code reveals that one student 
performed significantly worse than his or her peers who 
worked on the same object. This behavior seems to exhibit a 
trial-and-error strategy as opposed to the structured progress 
of the other students. 
VII. TECHNOLOGY ACCEPTANCE  
In the second study, 34 online students completed the 
TAM questionnaire. The results show that the majority of 
students has at least two devices in the selection “notebook, 
tablet, smartphone, or desktop” to access the online study 
material. Only 16% of all respondents used the services of 
the learning environment less than once per week, 61% used 
it two or three times, or more. In particular, the self-
assessment tasks and reading materials were retrieved and 
used frequently.  
The online students exhibit a high degree of technical af-
finity: 85% work frequently with web applications and 67% 
are online most of the day. Most students found the ergo-
nomics of the e-learning environment convincing (see Fig. 
7). It consists of the online course and the learning tools 
Moodle, edu-sharing, news, forum, and chat.  
 
 
 
Figure 7.  Student perception of the e-learning environment 
Fig. 8 illustrates the students’ evaluation of the value of 
the online course module to mediate methodological 
knowledge about program documentation. The high values of 
positive answers (84% and 78%, respectively) to the items:  
Q1: The online module helped me to understand the im-
portance of program documentation and quality as-
surance.  
Q5: I’m able to produce a program documentation using 
Javadoc. 
confirm that only a minority of online students was unable to 
assess the learning support function of the online version of 
the course positively.  
 
357
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
Figure 8.  Mediation of methodlogical knowledge  on program 
documentation. Q1: The online module helped me to understand the 
importance of program documentation and quality assurance. Q5: I’m able 
to produce a program documentation using Javadoc. 
Similar questions were raised for the other key topics of 
this course module, including program testing, exception 
handling, and error recovery. Table II summarizes the stu-
dents’ perception of the e-learning environment’s ability to 
convey methodological knowledge on program testing. 
Again, the positive answers predominate with the exception 
of constructive knowledge and practical skills needed to work 
with JUnit. Note that such skills were also lower rated in the 
domain program testing.  
TABLE II.  
IMPARTING OF METHODOLOGICAL KNOWLEDGE ABOUT  
QUESTION 
RATHER 
APPLIES 
APPLIES 
Q6: I understand the possibilities and limits 
of testing 
53% 
25% 
Q7: I know about test levels and test 
methods 
67% 
15% 
Q8: I know how to identify and apply test 
cases 
48% 
12% 
Q9: I’m able to implement test cases in 
JUnit 
24% 
18% 
 
Fig. 9 shows again a high confidence in factual and con-
ceptual but a weaker result in procedural knowledge about 
exception handling. 
Most respondents also certify that the online learning en-
vironment exhibits a slightly higher motivating factor. This 
increased motivation does, however, not carry over to coop-
erative tasks. The vast majority (82%) did not feel encour-
aged to interact with other students and to solve team tasks 
cooperatively. An aversion to synchronous person-to-person 
interaction over a distance is also reflected in the preference 
of asynchronous communication methods such as email and 
forums over synchronous forms such as instant messaging 
and Skype. This trend towards independent study habits is 
confirmed by the low interest in cooperative problem solving 
reported in Section VI.B and previous experience of the 
course tutors. The main reason quoted for the low interest in 
cooperation with peer students is a job-related lack of time. 
A few respondents also saw no reason to contact other stu-
dents to help them get over hurdle because they had solved 
their study problems themselves, e.g., by researching the 
Internet. 
 
 
 
Figure 9.  Mediation of methodological knowledge on exception handling. 
Q2: This course module animated me to reflect on exception handling 
during program development. Q3: Now, I understand the concept of 
exceptions. Q4: I’m able to throw and catch exceptions in Java 
At the beginning of the semester, the students had not 
much prior experience with program testing and exception 
handling. Only about a quarter was knowledgeable in these 
areas. This changed after completion of the course unit ad-
dressing these topics. About 60% of respondents now be-
lieved to be able to identify proper test cases and test levels. 
More than 70% were convinced to know the possibilities and 
limits of testing. This self-evaluation is also confirmed by the 
competence test (see Section V.B). Here, the tasks related to 
testing showed the statistically most significant improve-
ments in competences. 
Several items in the TAM questionnaire addressed a 
comparison of the classical correspondence material and the 
online version of the course module. Table III summarizes 
the students’ judgment. 
TABLE III.  
STUDENT PREFERENCES: ONLINE VS. DISTANCE STUDY 
TEXTBOOK  
QUESTION 
NA 
RNA 
NN 
RA 
A 
The 
online 
module 
illustrates 
complicated 
facts 
better 
than 
the 
traditional textbook  
0% 
16% 
25% 
47% 
13% 
Due to the online module I 
feel easier to work on a 
typical problem from the 
topic area than with the 
traditional textbook. 
7% 
13% 
40% 
33% 
7% 
I prefer the online course 
to the classical textbook  
0% 
27% 
33% 
20% 
20% 
I prefer to study with the 
textbook 
9% 
25% 
19% 
34% 
13% 
I would like to see more 
course content be offered 
in a similar online version 
0% 
7% 
20% 
47% 
27% 
I totally dislike e-learning 
64% 
6% 
18% 
9% 
3% 
358
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

NA: does Not Apply; RNA: does Rather Not Apply; NN: Neither; RA: Rather Applies; A: Applies 
 
The responses to these questions suggest that the students 
are rather indifferent towards the use of the online course 
material compared to the traditional textbook. Though most 
students are open-minded regarding the new learning oppor-
tunities (74% would like more courses to offer online mate-
rial) and a majority of 60% acknowledge, that the online 
version can help to illustrate complicated matter in better 
ways, only 40% of the students actually prefer the online 
course to the classical textbook and 47% prefer to study with 
the classical version exclusively. These answers suggest, that 
the students think of the online course as a supplement to the 
traditional textbooks. An explanation might be the new expe-
rience the online material offers and the lack of experience 
with it. Thus, the students don’t want to rely on an e-learning 
approach alone. When online courses become more com-
mon, the willingness to utilize them might increase. 
VIII. EXAMINATION RESULTS PUT IN CONTEXT  
The maximum test score students can reach in the final 
examination of this course is 100 points. At least 50 points 
are needed to pass the test. The grade acquired for the course 
is derived from a student’s score in the final examination. 
Table IV illustrates the mapping of test scores to grades. 
Grade 5.0 means failed. 
TABLE IV.  
SCORES MAPPED TO GRADES 
Score Points 
Grade 
100 - 95 
1.0 
94 - 90 
1.3 
89 - 84 
1.7 
83 - 80 
2.0 
75 - 79 
2.3 
74 - 69 
2.7 
68 - 65 
3.0 
64 - 60  
3.3 
55 - 59 
3.7 
50 - 54 
4.0 
49 - 0 
5.0 
A. Examination in August 2012 
Out of the 693 students who were enrolled in the course 
during the first study 199 took the final examination in Sep-
tember 2012.  This group included 6 of the 12 online students. 
142 students passed, including 4 online students, and 57 
failed, including 2 online students.  
B. Examination in February 2014 
141 students participated in the final examination test 
conducted in February 2014. (In our experience, the relative 
number of examination participants is lower in the February 
than in the August examination test.  For instance, in the 
summer semester 2014 only 636 students were enrolled in the 
course OOP but 250 took the examination test.) 
89 students passed, producing a typical success rate in the 
range 60-67%. The examination group included 22 students 
from the online and 28 from the control group. Fig. 10 shows 
the distribution of scores for the group of online students (N = 
22) and the control group (N = 31) for the examination ele-
ment program exceptions. The distribution curves are very 
similar. Slight differences can be seen only in the failure rates 
(grade 5.0 or score points < 50). They were: 45.5% in the 
online and 51.6% in the control group. In both groups, seven 
students achieved a grade than better 2.3. Given this small 
number of subjects, no effect of the study material, online 
course unit versus textbook, can be reliably derived from 
these variances.  
The failure rate of the whole examination test, which in-
cluded four additional problems, was only 39.6% (N=141).  9 
students in the online group and 7 students in the control 
group, who would have failed on test element “exceptions”, 
finally passed the examination test because they scored better 
on other test problems. This can be explained by the fact that 
other test problems have a higher success rate over the whole 
examination group. Test element “exceptions”, which ranges 
at level “Analysis” in Bloom’s cognitive taxonomy [10], 
achieved a rate of 49,5%, while two other test problems end-
ed up with 67.9 and 72.1. In Bloom’s taxonomy they range on 
levels “Comprehension” and “Application”, respectively. 
This observation about the difficulty level of test problems 
coincides with remarks that students communicated to the 
tutors independently of the study: They found the examina-
tion topic “exceptions” very difficult because it has not yet 
been tested this intensively in previous examinations. (Note 
here that examination problems and master solutions are 
made available after a test has been concluded to help future 
students to prepare for their own examination.) 
 
Figure 10.  Distribution of examination grades among students in online and 
offline group. 
When overlaying the examination scores of the online 
students for the test element “exceptions” (line diagram) and 
their retention times spent on information and exercise pages 
of this chapter, the result in Fig. 11 emerges. If we would 
draw a trend line, it would slightly fall from left to right. But 
let us discuss a few more striking issues of the bar charts and 
score line in Fig. 11. 
359
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
Figure 11.  Retention times and examination results of individual students 
for test item “exceptions”.  
In the discussion of Fig. 11, we also consider both the log 
data collected from within the online learning environment 
(see Section VI.C) and the online exercise system for home-
work assignments, which was mentioned in Section III.A, and 
the competence tests.  
Let us pick a few extreme examples, starting with the 
positive ones. Students #40 and #44 achieved 100 and 96% of 
the maximum sore. Both accessed the information pages 
between 7 and 5 hours and worked on the exercise pages 
roughly 5 hours. We also know from Section VI.C that both 
were higly active and successful with the self-assessment 
elements of this course unit. In their homework, they acquired 
100 and 95%, respectively. Student #40 also achieved the 
highest score in the competence post-test (69 score point out 
of 77) with a gain of 16 points. Student #44 scored a bit 
worse with a final score of 62 points and a gain of 15 points.  
With 82%, student #32 scored relatively close to these 
two students but spent only 2.5 hours in total on exception 
topics, and a bit more on assessment than reading activities. 
(S)he ranges second in the competence post-test with 68 
points and showed the higehst gain with 29 points. Student 
#25 exhibits the lowest time period spent on Chapter 
Exceptions but still ended up with a score of 76% in 
exception problems. All these students worked quickly on the 
excercises and largely error-free. Most likely, some of them 
acquired the necessary topic knowledge from other sources 
than the online course. 
On the other hand, the student with the longest retention 
time both on reading and exercise pages (#51) missed the 
50% mark necessary to pass the examination. This student 
achieved 95% of possible scores in the homework but did not 
access any of the self-assessment elements on topic 
exception. The latter is also true for Student #23 who also 
failed in the examination. Student #51 had a competence 
score of 59 and a gain of 11 points, while #23 collected 60 in 
the pre-test and gained 9 points compared to the pre-test. 
The result of the correlation of examination scores and 
retention times indicates that we have no evidence to draw 
any conclusion on examination success from retention times 
in the online learning system. However, the targeted use of 
interactive learning objects and self-assessment programming 
tasks seems to have a positive impact on the examination test 
scores.  
IX. VERBAL STUDENT FEEDBACK 
After all survey and log data had been evaluated, we felt 
the need to add a qualitative data analysis to clarify some 
results of the quantitative research.  Questions of interest 
include:  
• How do online students learn?  
• Had the online students used other sources than just 
the online materials provided?  
• What motivated them to volunteer for online study?  
• Were their expectations satisfied?  
• Did they find the online materials sufficient for reach-
ing the learning objectives? 
• How do they compare learning with traditional corre-
spondence materials and online media? 
The plan was to perform semi-structured telephone inter-
views with a representative sample of online students and 
analyze the interview content to find possible answers to 
these questions and detect unexpected relationships between 
them.  
An interview guideline with 32 questions was designed, 
and in July 2014 a request to volunteer for a telephone inter-
view was sent to all online students of the winter semester 
2013/14 course. Unfortunately, only two students were able 
to take an interview. Many respondents regretted not to be 
able to follow the request because they were preparing for the 
September 2014 examination in other courses. Some students 
did not react at all. For reliable qualitative research, this num-
ber of subjects is by far too small. Nevertheless, we decided 
to organize at least the two interviews offered in search of 
authentic responses that would support or question our as-
sumptions about online learner behavior. 
The typical method of learning of both students is to read 
the course material cursorily first to judge the learning effort. 
Then, in a second run, they study the content in detail and 
thereby take notes, mark and annotate the text, perform self-
assessment tests, and solve homework assignments. Some 
students seem to produce their own summary of course top-
ics; our two interview partners did not. Shortly before the 
examination test takes place, they go through previous exam-
ination tests to get a better feel for what a typical examination 
test looks like and how they operate under time constraints. In 
addition, they re-evaluate homework assignments. To over-
come understanding problems or acquire more detailed in-
formation on a topic, both students preferably use a search 
engine to find resources in the web that help clarify their 
questions. Depending on the given subject, they also buy 
additional textbooks recommended in the reading list. Both 
students felt no need to contact peer students or tutors to 
discuss unclear course topics. They also did not work on the 
collaborative tasks because they consider it difficult to syn-
chronize with peers. This confirms our earlier experiences 
that distance students largely learn autonomously, mostly due 
to time problems.  
Both interview partners state that their performances in 
this course match those of other courses or are slightly better. 
They believe that this course has a difficulty level comparable 
to other courses they have been enrolled in. Both students felt 
that they learned a lot. One student was inspired to apply and 
360
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

extend his/her programming skills by adding dynamic web 
pages to his/her web presence using PHP [30].  
The motivation to volunteer for online studies were: a) a 
general interest in e-education paired with e-learning experi-
ences in the work context; b) a personal interest in the ques-
tion whether online learning is advantageous over the study 
with classical distance learning textbooks. One student said 
that his expectations were partly satisfied. On one side, the 
interactive learning objects and programming tasks were 
considered helpful in assessing the learning progress. The 
lack of possibilities to highlight and annotate passages in the 
online materials was seen negative. In summary (s)he felt that 
“Basic knowledge about object-oriented programming con-
cepts and basic programming skills definitely came across.”  
The other student said: “I liked the general layout, the con-
tent, and the mix of reading and exercise pages of the online 
course. I would like to have more of this.” Both students 
solved all online tasks but the collaborate task completely.  
Asked whether the students perceived a difference in 
learning effort for both media types, one student thought that 
the learning objectives could be achieved faster with the 
online version. The other student was undecided.   
Both students stated that they also studied the textbook 
version after having studied the online version to be on the 
safe side because they had noticed that both versions differ in 
structure and presentation. This may explain why the online 
times of most students were relatively short and largely dedi-
cated to the examination of online tests.  When comparing 
both versions, one student replied: “Studying the online mate-
rial was full of variety, while the textbook is drier to go 
through”. The other said that (s)he is better motivated by 
working with the online material, while studying the course 
text on paper leads earlier to a loss of concentration. The 
students did not feel exhausted when learning for a longer 
time in front of a computer or tablet screen. Both students 
said that the communication means offered were useful. In 
particular the topic-specific online forum is considered more 
effective than the course newsgroup, which requires special 
newsreader and addresses all course topics. 
In an independent student evaluation, which is carried out 
every semester for every course of the curriculum, this course 
typically receives a rating above average. This is also true for 
the evaluation performed at the end of the winter semester 
2013/14 (1.6 on a scale 1-3). The students also find the work-
load in the usual range.  Some were asking for additional 
learning video and interactive online media. Many students 
consider the supervised forum essential for an effective learn-
ing process.   
X. 
CONCLUSION AND LESSONS LEARNED 
With this study we wanted to learn more about the influ-
ence of e-learning elements in higher distance education. To 
this end, we used an introductory programming module in the 
undergraduate degree program on Business Informatics as a 
case to compare learning attitudes and outcomes of students 
using the classical correspondence study material with stu-
dents learning along an interactive online version of a select-
ed learning module. The study was performed twice in two 
different years on the same course but with different student 
populations because the first run produced statistically unreli-
able results. Table V depicts relevant figures of the two inves-
tigations undertaken. The questionnaires used in the second 
study and the raw data collected thereby are accessible online 
[31]. 
In summary, the research questions we started from can 
be answered as follows:  
1. The competence analysis shows that the students 
achieved significant growth in competence, particular-
ly on the course topics “testing” and “program excep-
tions”. However, no significant difference can be ob-
served between online students and the control group 
(see Fig. 5).  
2. Even if we chose the students’ final grades acquired in 
a written examination that took place at the end of the 
course, no significant difference in learning outcomes 
can be seen (see Fig. 10).  
3. The retention times in the online environment are no 
useful indicator for test success or failure, a result that 
has been confirmed before in a workload study [32].  
4. The behavior analysis depicts a correlation between 
high and targeted use of interactive learning objects.  
5. The results of the technology acceptance analysis and 
the online behavior analysis suggest that the students 
appreciate technology-supported learning. At the same 
time, both studies clearly show that students, on the 
one hand, have little tendency to adopt cooperative 
learning and, on the other hand, do not feel capable 
because time constraints.  
TABLE V.  
TEST PARAMETERS FOR BOTH STUDIES 
Activity 
SS2012 
WS 2013/14 
Students enrolled in course 
693 
980 
Competence pre-test 
57 
162 
  Pre-test: complete sets 
19 
126 
Competence post-test 
12 
78 
  Post-test: clean sets 
5 
49 
  Post-test paired with pre-test 
1 
40 
Online study volunteers*  
12 
186 
  Volunteers selected  
12 
98 
  Volunteers actively studying    
10 
45 
Technology acceptance test 
 
34 
Final examination 
141 
200 
  Participating online students 
6 
21 
*Online students have experience with traditional correspondence mate-
rials from other course units and courses 
 
As a result of this investigation, we decided to enrich oth-
er topics of the course OOP with further interactive learning 
objects for self-assessment and practice. For the near future, 
we will keep the textbook version as the main learning medi-
um for our students. 
361
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

To support the autonomous preparation for an examina-
tion test, we plan to produce short video clips explaining how 
to understand a given problem and constructively approach its 
solution. 
In general, we find that technology-assisted learning ele-
ments for distance students increasingly arouse interest, to the 
extent that they are efficient and supportive for the learning 
process. We believe that e-learning still offers many degrees 
of freedom for pedagogic innovations of distance learning, 
while the pedagogy of the traditional correspondence-based 
distance education model is largely exhausted. Research in 
the field of online distance education is, however, still largely 
disconnected and disparate. In their new book, for the first 
time, Olaf Zawacki-Richter and Terry Anderson have tried to 
integrate major research trends in online distance education 
into a systematic research agenda [33].  
 
ACKNOWLEDGMENT 
We are particularly grateful to all students who volun-
teered for the online study and the three surveys. Winfried 
Hering owes our thanks for performing the log data and be-
havior analysis of the first study as part of his Bachelor thesis 
research. We thank Silvia Schreier for being overly support-
ive in the organization and execution of the first study. Helga 
Huppertz owes our gratitude for co-designing and implement-
ing the online sections of the course. In the second study, she 
also helped to analyze the log data of the online student 
group. Christoph Wöldecke helped us setting up the compe-
tence measurement instrument and evaluating the results of 
the competence measurements. 
 
 
 
REFERENCES 
[1] W. Hering, H. Huppertz, B. J. Krämer, J. Magenheim, J. 
Neugebauer, and S. Schreier, “On Benefits of Interactive 
Online Learning in Higher Distance Education – Case Study in 
the Context of Programming education,” eLmL 2014: The 
Sixth International Conference on Mobile, Hybrid & On-line 
Learning, 
2014. 
Also 
available 
from 
http://www.thinkmind.org/index.php?view= 
article&articleid=elml_2014_3_20_50048, last access: Dec. 
10, 2014. 
[2] B. J. Krämer, “Web-based Learning Environments: Tools and 
Engineering Issues,” in Handbook of Software Engineering 
and Knowledge Engineering, vol 1: Fundamentals, pp. 697-
718, World Scientific, 2001. 
[3] B. J. Krämer and L. Wegner, “From Custom Text Books to 
Interactive Distance Teaching,” ED-MEDIA/ED-TELECOM 
98, Prentice Hall, 1998. 
[4] T. Russel, The No Significant Difference Phenomenon, 5th 
edition Raleigh, NC, North Carolina State University, 1999. 
[5] R. M. Bernard, P. C. Abrami, Y. Lou, E.Borokhovski, A. 
Wade, L. Wozney, P. A. Wallet, M. Fiset, and B. Huan, “How 
does distance education compare with classroom instruction? 
A meta-analysis of the emirical literature,” Review of 
Educational Research, pp. 379-439, vol. 74, 2004. 
[6] B. Means, Y. Toyama, R. Murphy, M. Bakia, and K. Jones, 
Evaluation of evidence-based practices in online learning: a 
meta-analysis and review of online learning studies, U.S. 
Department of Education, 2010, https://www2.ed.gov/rschstat/ 
eval/tech/evidence-based-practices/finalreport.pdf, last access: 
Dec. 10, 2014. 
[7] J. Ferguson and A. M. Tryjankowski, “Online versus face‐to‐
face learning: looking at modes of instruction in Master’s‐level 
courses,” Journal of Further and Higher Education, vol. 33, no. 
3, pp. 219-228, 2009. 
[8] L. M. Cooper, “A comparison of online and traditional 
computer applications classes,” T.H.E. Journal, vol. 28 no. 8, 
March 2001.  
[9] S. Guri-Rosenblit, “‘Distance education’ and ‘e-learning’: Not 
the same thing”, Higher Education, vol. 49, no.4, pp. 467-493, 
Springer, June 2005. 
[10] B. S. Bloom, M. D. Engelhart, E. J. Furst, W. H. Hill, and D. 
R. Krathwohl, Taxonomy of educational objectives: The 
classification of educational goals - Handbook I: Cognitive 
domain, New York: David McKay Company, 1956. 
[11] B. J. Krämer, Object-oriented Programming – Module 5: 
Exceptions and Testing. [Online,in German]. Available from 
http://fernuni-hagen.edu-sharing.net/OOP_M5 
(user 
name: 
elearning, password: TryIt), last access Nov. 22, 2014. 
[12] Moodle, Open Source Learning Platform. [Online]. Available 
from http://moodle.com/, last access: Nov 15, 2014.  
[13] edu-sharing, Open source repository network for learning 
content and educational knowledge. [Online]. Available from 
http://edu-sharing.net/, 2009, last access: Dec. 10, 2014. 
[14] M. Klebl, B. J. Krämer, and A. Zobel, “From content to 
practice: sharing educational practice in edu-sharing,” British 
Journal of Educational Technology, vol. 41, no. 6, pp. 936-
951, Nov. 2010.  
[15] F. Davis, “A technology acceptance model for empirically 
testing new end-user information systems - theory and results,” 
PhD thesis, Massachusetts Institute of Technology, 1985.  
[16] V. Venkatesh and F. D. Davis, “A Theoretical Extension of the 
Technology Acceptance Model: Four Longitudinal Field 
Studies,” Management Science, 46, pp. 186-204, 2000. 
[17] J. Brooke, “SUS - A quick and dirty usability scale.” [Online]. 
http://www.usabilitynet.org/trump/documents/Suschapt.doc, 
last access: Dec. 10, 2014. 
[18] C. Ong, J. Lai, and Y.  Wang. “Factors affecting engineers‘ 
acceptance of asynchronous e-learning systems in high-tech 
companies,” Information & Management, vol.  41. No. 6, pp. 
795-804, 2004. 
[19] M. Zviran, C.  Glezer, and I. Avni, “User satisfaction from 
commercial web sites: the effect of design and use,” 
Information and Management,  vol. 43, no. 2, pp. 157-178, 
2006. 
[20] V. Venkatesh and H. Bala, “Technology Acceptance Model 3 
and a Research Agenda on Interventions,” Decision Sciences, 
vol. 39, no. 2, pp. 273–315, 2008. 
[21] LimeSurvey, Open source online survey system, 2003, 
[Online]. Available from http://www.limesurvey.org, last 
access: Dec. 10, 2014 
[22] The Joint Task Force on Computing Curricula. (2001). 
Computing Curricula 2001 Computer Science. IEEE-CS, 
ACM. Final Report. 
[23] Tucker, Allen (ed.), “A Model Curriculum for K-12 Computer 
Science: Final Report of the ACM K-12 Task Force 
Curriculum Committee,” 2nd Edition, New York: Association 
for Computing Machinery (ACM), 2006. [Online]. Available 
from.http://www.acm.org/education/education/curric_vols/k12
final1022.pdf, last access: Dec. 10, 2014. 
[24] B. Linck et al., “Empirical refinement of a theoretically 
derived competence model for informatics modelling and 
system comprehension,” Proc. IFIP Conference Addressing 
educational challenges: the role of ICT (AECRICT), 2-5 July 
2012 
362
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[25] Lehner, L. et al., “Informatics Systems and Modelling–Case 
Studies of Expert Interviews”, Reynolds, N. and Turcsányi-
Szabó, M. (eds.) Key Competencies in the Knowledge Society 
IFIP AICT. pp. 222–233, Heidelberg: Springer, 2010.. 
[26] B. Linck et al., “Competence model for informatics modelling 
and system comprehension,” IEEE Global Engineering 
Education Conference (EDUCON), pp. 85–93, Berlin 2013. 
[27] T. Rhode, “Development and testing of an instrument for 
measuring informatics modelling competences in a didactic 
context,” dissertation, Universität Paderborn, 2012 (in 
German). 
[28] W. Fan and Z. Yan, “Factors affecting response rates of the 
web survey: a systematic review,” Computers in Human 
Behavior, vol. 26, no. 2, pp. 132–139, Feb. 2010. 
[29] SAS, Business Analytics Software. [Online]. Available from 
http://www.sas.com, last access: Dec. 10, 2014. 
[30] PHP, Hypertext Preprocessor. [Online]. Available from 
http://php.net/, last access: Dec. 10, 2014 
[31] Bernd J. Krämer and Johannes Magenheim: Datasets Acquired 
in a Comparative Study on the Impact of Higher Distance 
Education with Traditional Correspondence and Interactive 
Online Media, FernUniversität 2014. [Online]. Available from 
http://deposit.fernuni-hagen.de/2990/ last access: Dec. 10, 
2014 
[32] R. Schulmeister and C. Metzger (Eds.), “The Workload in 
Bachelor Programs: Time Budget and Study Behavior,” 
Waxmann, 2011 (in German). 
[33] O. Zawacki-Richter and T. Anderson (Eds), “Online Distance 
Education: A Research Agenda,” Athabasca University Press, 
2014. 
 
 
 
363
International Journal on Advances in Life Sciences, vol 6 no 3 & 4, year 2014, http://www.iariajournals.org/life_sciences/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

