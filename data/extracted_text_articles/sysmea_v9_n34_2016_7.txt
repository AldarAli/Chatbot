199
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Combining spectral and spatial information for heavy equipment 
detection in airborne images 
 
Katia Stankov 
R&D department 
Synodon Inc. 
Edmonton AB Canada 
e-mail: katia.stankov@usherbrooke.ca 
Boyd Tolton 
R&D department 
Synodon Inc. 
Edmonton AB Canada 
e-mail: boyd.tolton@ synodon.com
 
 
 
Abstract - Unsupervised construction on the pipeline right-of-
way may provoke pipe rupture and consequently gas leaks. 
Heavy equipment is seen as a clue for construction activity. 
Monitoring the pipeline right-of-way for heavy equipment is 
therefore important for environmental and human safety. 
Remotely sensed images are an alternative to expensive and 
time consuming foot patrol. Existing image processing methods 
make use of previous images and/or external data. Both are not 
always available. We propose a new method for image 
processing to detect heavy equipment without the need of 
auxiliary data. We first detect potential heavy equipment 
locations and then use spatial descriptors and spectral 
information to eliminate false alarms. The method was 
validated in different environments – urban, vegetation, open 
excavation – and in different seasons. The experiments 
demonstrated the capacity of the method to detect heavy 
equipment without the use of previous images and/or external 
data.  
Keywords-remote sensing image processing; right-of-way 
threats detection; differential morphological profile; spectral 
information; Hausdorff distance. 
I. 
INTRODUCTION 
Unsupervised construction activity on oil and gas 
pipeline’s Right-of Way (ROW) may lead to pipeline 
rupture and leaks. The periodic surveillance of the ROW for 
the presence of heavy equipment or construction machinery, 
referred also as ROW threats, is therefore vital to protect the 
human safety and to prevent ecological damage. Pipeline 
networks span thousands of kilometers and may be located 
in remote and difficult to access areas. Airborne and satellite 
images are considered to complete the surveys. Computer 
based methods to detect construction machinery in these 
images represent an alternative of the slow and tedious task 
of visual image analysis.  
In our previous paper [1] we presented a method for 
heavy equipment detection in airborne images, based on the 
differential morphological profile and spatial characteristics 
of the objects. In this paper, we present a notable 
improvement of the method by analyzing spectral 
information as well. We have also shown expanded 
experiments to support the method’s achievements. 
The paper is organized as follows. In Section II we give 
the state-of-the-art, Section III presents the methodology, in 
Section IV we provide results, validation and discussion, 
and in Section V a conclusion. 
II. 
STATE-OF-THE-ART 
The automation of the process of heavy equipment 
detection in airborne images faces difficulties from different 
origin:  great variety of vehicles; uneven flight altitude; 
different view and orientation of the images; variable 
illumination conditions; occlusion by neighboring objects, 
and others [2]. In addition, construction vehicles are 
sometimes very similar to transportation vehicles. All these 
make the development of pattern recognition algorithms for 
ROW threat detection a challenging remote sensing image 
processing task.  
Existing methods extract characteristic features to 
decrease the differences between construction vehicles 
(decrease the intra-class heterogeneity), while increasing the 
inter-class heterogeneity, i.e., make heavy equipment more 
distinguishable from other objects. In [3], scale-invariant 
feature transform was applied on previously defined scale 
invariant regions to receive object descriptors and detect 
vehicles. Presuming that local distribution of oriented 
gradients (edge orientations) is a good indicator for the 
presence of an object, Dalal [4] proposed the accumulative 
Histogram of the Oriented Gradients (HOG). In [5], the 
authors mapped HOG to Fourier domain to achieve rotation 
invariance and used kernel Support Vector Machine (SVM) 
to classify the data and identify construction vehicles. Using 
local textural descriptors and adaptive perception based 
segmentation, the authors in [2] sequentially eliminate 
background objects from the image, such as buildings, 
vegetation, roads, etc. The remaining potential threat 
locations are divided into several parts to extract and 
evaluate descriptive features and match them against 
template data. Extraction of local phase information allowed 
the separation between structure details and local energy 
(contrast) [6]. Afterwards, based on previously defined 
image template, the authors in [6] created a voting matrix to 
detect construction vehicles. An interesting approach to 
derive the template images from the processed image itself 
was proposed in [7]. The authors created an immune 
network and first trained image areas against vehicles 
samples; next, they processed the whole images in a similar 
way to detect vehicles. However, the vehicles samples were 
defined by human operator. Potential vehicle locations were 
derived through rule based classifier applied on numerous 
spatial and gray-level features computed on a segmented 
image in [8]. Statistical classifier was then used to assign 

200
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
objects to the vehicle class. Though this method avoids the 
definition of template images, it involved manual image 
analysis to design training samples.  
Exploiting the fact that heavy equipment has a larger 
number of right corners than natural objects, the authors in 
[9] defined target and background templates from the 
images. They used Harris corner detector to perform a first 
fast processing of UAV images and to reject background. In 
the second stage of the method the authors compared the 
performance of four classifiers (k Nearest Neighbors, 
Support Vector Machine, Decision Trees and Random 
Tress) and two feature extraction algorithms (HOG and 
Gabor coefficients). Best results were achieved with Gabor 
coefficients and Random Trees classifier. However, the 
results were more consistent while using a set of indoor 
images of model vehicles, taken in a sand box, than with the 
set defined from the images.  
To decrease the large sets of templates needed to train 
and test the classifier, the authors in [10] developed a novel 
system based on the AdaBoost classifier. They applied it on 
SAR images to detect three types of vehicles and achieved 
recognition rates above 95% with a very limited training set. 
Gabor wavelet of SAR images was used to extract 
descriptive feature parameters in [11] to identify several 
types of vehicles.  
Synthetic aperture radar images provide all weather 
coverage and are not restricted to the presence of daylight 
illumination. This property proved very efficient for change 
detection and potential threat localization [12]. Additional 
high spatial resolution optical images are to be analyzed to 
identify positive alarms.  
To fully avoid the need of image template, potential 
threats locations are assessed with the aid of change 
detection in [13], next auxiliary data is used to decide upon 
the presence of a threat. 
A common trait of existing methods is that the 
successful recognition of heavy equipment is impossible 
without complete set of image templates, previous images, 
and/or auxiliary data. These are not always available in 
practice. Airborne images of the pipeline ROW are taken 
only when a customer orders a survey. Previous images are 
not available when it comes to a new customer. Acquiring 
auxiliary data or building complete set of templates for the 
large variety of heavy equipment vehicles will significantly 
increase the cost of the survey. All of the above made 
existing methods not applicable in our case, thus we opted 
for a method that involves the interpretation of individual 
images. 
The new methodology for heavy equipment detection we 
present in this paper avoids both the need of template 
images, and the need of auxiliary data or previously 
acquired images. In addition to increased flexibility, it also 
makes the performance of the method independent of the 
quality of the external data. As in our previous method [1], 
we first localize potential threats by detecting areas of high 
frequency of the image that correspond to the size of 
construction machinery and compute spatial descriptors. 
Unlike the previous method, where we used all the 
descriptors at once to discriminate between threats and other 
objects, here we consecutively eliminate non threats 
locations. The significant improvement of the recognition 
came from the inclusion of spectral information. We analyze 
spectral information on the inner parts of the potential 
locations retained in the previous steps to refine the results. 
In the following section we give a step by step description 
of the method. 
    
III. 
DESCRIPTION OF THE METHOD 
The method may be roughly divided in three parts. First, 
we find potential threat locations. In the next step these 
locations are treated as objects, and spatial indices are 
derived to eliminate the ones that are certainly no threats. 
Finally, we introduce spectral information to further tune the 
results. A detailed description is given in Fig. 4. 
A. Finding potential threat locations 
To build our method we take advantage of the fact that 
construction vehicles have non-flatten surfaces, which 
creates inequality in the intensity of surface pixels and 
together with their outer edges make that they appear as 
areas of high frequency in the image. Therefore, potential 
threat locations may be found by identifying areas of high 
frequencies that are in the range of heavy equipment size. 
We apply the differential morphological profile on the 
gradient of the image to find areas of high frequency. 
1) 
Differential Morphological Profile (DMP): DMP is 
an iterative algorithm that performs opening/closing by 
reconstruction with a structuring element (SE) to find 
structures that are brighter/darker than their surroundings. 
The size of the SE is increased in the consecutive iteration 
and the result is extracted from the result of the previous 
iteration. As we are searching for structures that are brighter 
than their surroundings, we used only the opening by 
reconstruction to compute the DMP, as follows [14]. Let the 
vector 
Πγ (x)
 be the opening profile at the point x of image 
I defined by: 
 
            
[
]
{
n }
x
x
0,...,
( ),
:
)
(
*
∀ ∈
=
Π
= Π
Π
λ
γ
γ
γ
γ
λ
λ
λ
 
(1) 
 
where 
)
(
* x
γ λ
 is 
the 
morphological 
opening 
by 
reconstruction operator and the size of the SE = λ. 
The DMP 
∆γ (x)
is a vector that stores a measure of the 
slope between consecutive iterations of the opening profile 
corresponding to the increased size of the SE: 
 
  
[
]
{
n }
x
1,...,
,
:
)
(
1
∀ ∈
− Π
= Π
∆
= ∆
∆
−
λ
γ
γ
γ
γ
γ
λ
λ
λ
λ
 (2) 
 
When the SE size exceeds the object size, the background 
intensity values are assigned to the object. Thus, by 
extracting two consecutive results, bright objects that 
correspond or are bigger than the size of the corresponding 
SE are retained. The object is eliminated in the consecutive 
iteration if it is smaller than the SE. Thus by knowing when, 

201
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
in what level of the DMP, an object disappeared one may 
conclude about its size. 
The DMP has to be applied on a grayscale image, 
usually the brightness (the maximum between the red, green 
and blue channel - RGB) is used. Here we introduce a new 
technique based on the Color Invariant Model developed by 
Gevers and Smeulders [15].  
2) 
Invariant Color Model: The invariant color model 
computes the angles of the reflection vector and is invariant 
to illumination intensity and viewing direction [15].  
 
                      C1 = arctan (R/max{G,B}) 
(3) 
 
                      C2 = arctan (G/max{R,B}) 
(4) 
 
                      C3 = arctan (B/max{R,G}) 
(5) 
 
The model was designed to compensate for matte and 
dull surfaces and increases the differences in the inner parts 
of the construction machinery, which are sometimes 
attenuated in RGB. To enhance these inequalities, we 
generate a new image using the maximum between C1, C2, 
and C3, and compute the gradient of this image.  
The gradient of an image measures the directional 
changes of the intensity levels of an image. Because these 
changes are greater towards the edges of an object, the 
gradient highlights the transitions between objects. The 
uneven surface of heavy equipment may be related to as 
composed from few small objects, which in the gradient 
image generated from the invariant color model will appear 
as a high concentration of edges.  
3) 
Computing the gradient: To obtain the gradient of 
the image we use the measure of the discontinuity Dxy at 
each pixel with image coordinates x and y [16]: 
 
                               
2
2
y
x
xy
G
G
D
+
=
 
 (6) 
 
where Gx and Gy are the gradients at an image pixel in the x 
(horizontal) and y (vertical) directions, respectively. To 
compute the gradient we approximated the partial derivative 
in the horizontal direction with the central difference 
between columns; and in the vertical direction – with the 
central difference between rows, based on Sobel kernel.  
As shown in Fig. 1, the gradient of the image derived 
from the invariant color model produces an aggregation of 
edges in the inner parts of the threats, which allow for better 
differentiating construction machinery from other vehicles, 
compared to the gradient obtained from the brightness 
image. The higher the gradient value of a pixel, the higher 
the possibility that it belongs to an edge. To retain edges we 
threshold the gradient image derived from the invariant 
color model using the Otsu’s method.  
4) 
Localizing areas of high frequency: To find areas 
of high frequency we applied the DMP on the gradient 
image. The separation between objects with DMP depends 
on the size of the SE used for the opening by reconstruction 
[17]. To fit inner parts of heavy equipment machinery we 
derived the set of SEs from the size of these parts in 
accordance with the spatial resolution of the image. In our 
case, the size of the image pixel is 9 cm, which allowed 
using a set of SE ranging from 4x4 to 12x12 pixels with an 
increment of 4. When using DMP, objects situated closer 
than the size of the SE may be merged together in the 
corresponding DMP level. The set of SE we used was kind 
of compromise between retaining the whole construction 
vehicle and avoiding the merge with nearby objects. To 
eliminate irrelevant locations we first used spatial 
information.  
Unlike our previous method where we used all relevant 
spatial properties simultaneously to apply principal 
component analysis (PCA) and reduce false positives, here 
we first eliminated irrelevant locations based on the 
thresholding of few spatial properties. Next, we introduced 
spectral information and together with additional spatial 
property, we used the PCA to further refine the detection. 
As in the feature space the discrimination between classes is 
not linear [18] this two steps filtering proved to be more 
efficient.  
B. Spatial information 
For each DMP level, we find out connected components 
 
Figure 1. Gradient image. (a) Original RGB (b) Gradient of the brigthness image (from the original RGB); (c) Gradient of the maximum between C1, C2 
and C3 (the invariant color model). Heavy equipment is given in red rectangles. The inavriant color model enhances the edges in the inner parts of the 
heavy euipment, and in result the threats are more distinguishible from other vehicles in (c), compared to (b). 
 

202
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
and obtained image objects for which spatial properties may 
be computed. To each one of the spatial properties we 
assigned thresholds with a large margin of error in order to 
ensure only objects that for certain are not threats will be 
removed. As we further refine the results we were not 
concerned in this step of the method about the number of 
false positives. Following is a description of the spatial 
properties that better discriminate heavy equipment from 
other objects together with the way we set an appropriate 
threshold for them: 
1) 
Area: As stated earlier threats may merge with 
background depending on the interaction between their size 
and the size of the SE. For example, construction machinery 
is often situated in areas with digging activity. Digging is 
characterized by soil piles, which cast shadow and produce 
edge like effect in images and may merge with nearby 
objects. Sometimes, the distance between threats may be 
smaller than the size of the SE and consequently they will 
merge in the corresponding level of the DMP. As in these 
cases larger objects will correspond to heavy equipment, we 
defined a high threshold value for area (10000 pixels) and 
removed objects above it. 
2) 
Elongation: Heavy equipment has rectangular 
shape. The ratio between the major and the minor axis 
length of the object is a good indicator for the rectangularity 
of a shape. Higher values for this ratio correspond to objects 
with linear extension such as roads. Square shapes obtain 
values closer to 1. We gave a large margin between 1.1 and 
5. 
3) 
Curvature: We approximate the curvature with the 
radius of the circle that fits the best the contours of the 
object. First we fill the objects, next we find the coordinates 
of the contour of the filled objects to solve the least mean 
square problem and calculate the radius of the circle. The 
curvature is given by 1/radius [19]. The curvature of a 
straight line will be zero, so the greater the ratio 1/radius, 
the greater the curvature of the curve. We are searching 
therefore for objects whose curvature is greater than 0 and 
lower than some value corresponding to curved shapes. As 
we eliminate lines based on the elongation property we are 
not concerned to impose a lower threshold to the curvature. 
To define an upper threshold we use the Hough transform. 
4) 
Hough transform: Hough transform is useful 
technique to fit straight lines to object boundaries. For small 
round objects it finds zero lines. We use the Matlab 
implemented algorithm with the only constraint of 100 
peaks in the parametric space, and find the minimum 
curvature of the objects that received no lines after the 
Hough transform. We use this minimum curvature value of 
the most curved objects in the image to define an upper 
threshold on the curvature. 
To further refine the results and decide whether an 
object belongs to the class of heavy equipment we analyzed 
also spectral information. 
C. Assigning objects to the class of heavy equipment 
To assign objects to the class of heavy equipment we use 
spectral information, vegetation mask, and a property called 
edgeness. We designed decision rules to determine whether 
an object may represent a threat. 
1) 
Spectral 
properties: 
We 
analyze 
spectral 
information derived from the inner parts of the objects 
because the spectra of their contours may be affected by the 
transition between objects and therefore not a good indicator 
of the spectral properties of the object. As heavy equipment 
is painted in saturated colors it appears as bright spot in at 
least one of the invariant color bands C1, C2, and C3, and as 
dark spots in the remaining bands, on the contrary of other 
manmade objects or the background, which have average 
values in all of the channels. We generate two images; one 
 
Figure 2. Images used to compute the Hausdorff distance. 
(a) Minimum pixel value between C1, C2 and C3 (the invariant 
color model); (b) Maximum pixel value. In the red rectangles are 
given examples of heavy equipement. Lower values in (a) 
correspond to higher values in (b).  

203
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
takes the minimum between the three new channels for each 
pixel (Fig. 2 (a)) and the other one – the minimum (Fig. 2 
(b)).  
To assess the differences between the maximum and 
minimum of the set of pixels occupying the inner parts of 
objects, we use the Hausdorff distance. It is an efficient 
measure to estimate the mismatch between two sets of 
points without being influenced by variance or noise [20].  
Given two sets of points A = {a1, …., an} and B = {b1, 
…., bn}, the Hausdorff distance is defined as [20]: 
 
                     H(A,B) = max(h(A,B), h(B,A))  
(7) 
 
where 
                          
||
maxmin ||
( , )
b
a
h A B
b B
a A
−
=
∈
∈
 
(8) 
 
and ||.|| is some norm. We used the Euclidean distance. 
The Hausdorff distance first identifies the point of given 
set A that is farthest from any point in set B and then 
computes the distance from this point to its nearest neighbor 
in B and vice versa, from the point of B that is farthest from 
any other point of A, it finds the distance to the nearest 
neighbor in A. Then it takes the maximum between the two 
distances. Thus, every point in one set is within the 
Hausdorff distance from some other point in the other set 
and vice versa [20].  
We calculate the Hausdorff distance between the two 
sets of pixels values obtained from the maximum and 
minimum images received from the C1, C2, and C3 bands. 
As heavy equipment vehicles have saturated colors they will 
receive higher values for the Hausdorff distance. Moreover, 
a larger amount of their pixels will reside within this 
margin, according to the per pixel difference between the 
maximum and the minimum of the C1, C2, and C3 
channels. We compute the percent of pixels in the whole 
object whose differences between the maximum and the 
minimum of the C1, C2, and C3 channels are within the 
Hausdorff distance for the corresponding object. In the rest 
of the paper we refer to this property as spectral mismatch 
occupancy (SMO). Construction machinery receives higher 
SMO values, as most of the pixels are in the margins 
defined by the Hausdorff distance. Would we define a 
threshold on the saturation image to compute the SMO, we 
would receive higher values for other objects too. We 
illustrate this in Fig. 3. As shown, the Hausdorff distance 
allowed to better differentiate the levels of saturation.   
To define whether an object may belong to the class of 
 
Figure 3. Saturation versus Hausdorff distance:  (a) Heavy euqipment, original RGB (above), corresponding object (below); (b) Saturation profile of the 
object in (a); (c) Maximum and minimum profile of the object in (a); (d)  Background object, original RGB (above), corresponding object (below); (e) 
Saturation profile of the object in (b); (f) Maximum and minimum profile of the object in (b). For the heavy equipment in (a) the Hausdorff distance is 
equal to 0.59, the SMO is 0.24. For the object in (d) Hausdorff distance equals 0.52, SMO – 0.2. 

204
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
heavy equipment we compare the SMO to the vegetation 
occupancy.  
2) 
Compare SMO to vegetation occupancy and apply 
decision rules: First, we compute the vegetation index from 
the RGB bands, because the airborne images we work with 
have only these three bands: 
 
  Vegetation Index = (Green – Red)/(Green + Red) 
(9) 
 
Using the Otsu’s threshold method, we generate a 
vegetation mask. The vegetation occupancy of an object is 
obtained as the percent of masked pixels in the object area.   
The difference between SMO and vegetation occupancy 
varies according to the scene. To adapt our method we apply 
the following rules to retain ROW threats. 
In scenes occupied mostly by green vegetation, a single 
threat will have definitely higher SMO than vegetation 
occupancy, compared to other objects. Thus, if less than 10 
percent of the objects have higher SMO than vegetation 
occupancy, we assume that this is the case and retain these 
objects (Fig. 6 (a)). However, when the scene is occupied by 
buildings, excavations, or the image is taken in winter 
season when no green vegetation is present, but only 
leafless trees and shrubs, the vegetation mask may also 
cover part of the heavy equipment and the predominance of 
SMO over vegetation occupancy is not so evident.  
In cases where there is a lack of strict distinction 
between vegetation and other objects (the SMO is less than 
the vegetation occupancy for all of the objects or more than 
10 percent of the objects have higher SMO than vegetation 
occupancy), we use the property called edgeness. It is 
obtained by dividing the area of the object by the number of 
edge pixels of the object. To receive the area of the object 
we fill the boundary of the object. As stated earlier, because 
of the surface inequalities of construction vehicles, they will 
receive higher edgeness than vegetation areas, or other 
transport vehicles as shown in Fig. 1 (c). 
We concatenate the two properties: 1) the difference 
between SMO and vegetation occupancy; 2) the edgeness 
value. Then we computed the principal component analysis 
and divided the objects according to their first principal 
score, setting the median of the scores as a threshold. To 
decide which group to retain we used the ratio between the 
eigenvalues of the covariance matrix of each group. Threats 
have higher intra-class heterogeneity compared to other 
objects. The ratio between the eigenvalues is a good 
indicator for heterogeneity [21], thus we used the ratio of 
the eigenvalues of the covariance matrix. Greater values 
correspond to greater heterogeneity, thus we retained the 
group that produced higher value for the eigenvalue’s ratio.  
Step by step results together with a flow chart of the 
method are given in Fig. 4.  
D. Post-processing 
As the images may not contain threats at all, we included 
a step of automate post-processing to refine the results. We 
used a vector composed from the Hausdorff distance and the 
SMO. We then sorted the magnitude of the vector and 
computed the slope of the tangent at each point. A point 
 
 
 
Figure 4. Flow chart of the proposed method. 
From (a) to (h) - the consecutive steps. 

205
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
represents the sorted magnitude value for each object. An 
empirically derived threshold was set to retain objects 
whose slope was less than the threshold. The rationale here 
is that if the objects belong to the class of threat their 
magnitude will be similar and therefore the slope of the 
tangent will be less than this ratio. If no threat were retained 
the values for the magnitude will be quite different and the 
slope will be higher than the threshold. As shown in Fig. 5 
this step was efficient to further refine the results. 
 
IV. 
RESULTS AND VALIDATION 
We present some results in Figs. 6, 7 and 8. We 
indicated true detections – heavy equipment that are present 
in the image and were identified by the method – with 
rectangles with yellow contours; false alarms, or false 
positives – objects that were identified by the method as 
threats, but are actually not threats – with rectangles with 
red contours, and missed detection – heavy equipment that 
is present in the image but was not identified as such by the 
method – with rectangles with green contours. 
 As may be seen from Fig. 6 the method performs well 
in different scenes – forest (a), urban scene (c), abandoned 
site (e), and ongoing excavations (g). These different 
backgrounds provide low (abandoned site – (e)) to high 
contrast (ongoing excavations – (g)). Despite that, the 
method successfully detected ROW threats. We explain this 
with the improved spectral contrast obtained with the 
invariant color model. In Fig. 6 (c) many transportation 
vehicles were present, which are usually a source of 
confusion with heavy equipment. The edgeness property 
allowed for better separation between them. Similar results 
are shown in Fig. 7 ((a) and (e)).  
Fig. 7 (c) shows a typical case of missing a threat. 
Although both vehicles are very similar, only one was 
detected. The other one was merged with the nearby fence 
and treated as a linear object. Fig. 7 (g) demonstrates the 
capacity of the method to detect threats when there is an 
accumulation of objects with size similar to this of heavy 
equipment, that appear as areas of high frequency. However, 
in this case, many false alarms were also detected as threats. 
In Fig. 8 we demonstrate the limitation of the method. 
When a single threat is present in the image, other objects 
may be misinterpreted as threats and the real threat omitted. 
In our opinion, the main reason for this is that while 
performing the PCA we assume the presence of only two 
classes. This may be improved by using cluster analysis, 
applied on the objects principal component scores. In Fig. 8 
(c, e and g) we demonstrate typical cases of false alarms. 
We believe that more rigorous post- processing step would 
decrease their number. 
To validate the accuracy of the method we compared the 
results to manually detected threats. We refer to the latter as 
ground truth data. A set of 300 images taken from different 
surveys was processed. The image size is 1200x800 pixels 
with average pixel resolution of 9 cm. The detection rate 
was 83.9% - heavy equipment machines that are present in 
the ground truth data and were detected by the algorithm. 
This is a slight improvement compared to our previous 
method, where the detection rate was 82.6%. However, in 
the current experiment the images are more heterogeneous, 
taken from different seasons, as opposite to the previous test 
where we used images of the same flight day. Also, visual 
comparison reveals that the number of false positive was 
significantly reduced. 
To place our method among other algorithms for threat 
detection we compare its achievement to the results reported 
in [9]. The authors compared several classifiers. Our 
method, with the detection rate of 83.9% performs slightly 
better than the kNN classifier (83.3%) and less than the 
regression trees and SVM classifiers – 85.7% and 93.3%, 
respectively. However, these classifiers were applied on 
rural scene only and used template models. As we 
demonstrated above, our method performs well in different 
scenes, without using templates or auxiliary data. We may 
say therefore that it has a potential and we focus our further 
developments to improve the detection rate and reduce the 
false detections. 
At this stage of the development of the algorithm we are 
less concerned with the rate of false recognition, as the 
results are reviewed by an operator. We consider including 
additional descriptors to reduce the number of false 
positives events while increasing the detection rate.  
The limitation of the method is related to the spatial 
resolution of the image. In our opinion, the method 
performance may decrease when applied on images with 
much lower spatial resolution, more than 1 meter for 
example, as it relies explicitly on information taken from an 
increasing neighborhood. 
Figure 5. Post-processsing. (a) Original RGB; (b) Processed image; (c) Results after post-processing.  

206
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
Figure 6. Results. Left column original RGB image. Righ column – detection. Rectangles with yellow contours indicate true detections; 
rectangles with red contours - false alarms; rectangles with green contours - missed detections. 

207
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
Figure 7. Results. Left column original RGB image. Righ column – detection. Rectangles with yellow contours indicate true detections; rectangles 
with red contours - false alarms; rectangles with green contours - missed detections. 

208
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
Figure 8. Results. Left column original RGB image. Righ column – detection. Rectangles with yellow contours indicate true detections; rectangles 
with red contours - false alarms; rectangles with green contours - missed detections. 

209
International Journal on Advances in Systems and Measurements, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/systems_and_measurements/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
V. 
CONCLUSION 
In this paper, we presented a novel methodology for 
heavy equipment detection. The method first detects high 
frequency areas in the image that may represent potential 
heavy equipment locations, and then compute spatial 
descriptors and explore spectral information to eliminate 
false detections. It does not involve the use of external data, 
or previously acquired images, which makes it more 
flexible, compared to already existing algorithms. An 
improvement compared to our previous method [1] is due to 
the exploration of spectral information all along the spatial 
descriptors. Our method compares favorably to other 
methods for threat detection. On the contrary of other 
studies, we tested it in different scenes – urban, forest, 
excavation areas. The experiments proved its efficiency for 
surveillance of the pipeline ROW, which is important for 
human safety and ecological damage prevention. The results 
are promising and we believe that the method has the 
potential to replace the manual processing of the images. 
ACKNOWLEDGMENT 
This research was funded by Alberta Innovates 
Technology Future. The authors would like to express their 
gratitude for the support provided by the Industry Associate 
Program of the institution.   
REFERENCES 
[1] K. Stankov and B. Tolton, “Differential morphological profile 
for threat detection on pipeline right-of-way. Heavy 
equipment detection,” The Eighth International Conference 
on Advanced Geographic Information Systems, Applications, 
and Services (Geoprocessing 2016), IARIA, Apr 2016,  pp. 
76-77, ISSN: 2308-393X , ISBN: 978-1-61208-469-5. 
[2] V. Asari, Vijayan, P. Sidike, C. Cui, and V. Santhaseelan. 
"New wide-area surveillance techniques for protection of 
pipeline infrastructure,” SPIE Newsroom, 30 January 2015, 
DOI: 10.1117/2.1201501.005760 
[3] G. Dorko and C. Schmid, “Selection of scale-invariant parts 
for object class recognition,” IProceedings of the 9th 
International Conference on Computer Vision, Nice, France, 
pp. 634–640, 2003. 
[4] N. Dalal and B. Triggs, ”Histograms of oriented gradients for 
human detection,” IEEE Conference on Computer Vision and 
Pattern Recognition, pp. 886-893, 2005 
[5] A. Mathew and V. K. Asari, ”Rotation-invariant Histogram 
Features for Threat Object Detection on Pipeline Right-of-
Way,” in Video Surveillance and Transportation Imaging 
Applications 2014, edited by Robert P. Loce, Eli Saber, Proc. 
of SPIE-IS&T Electronic Imaging, SPIE vol. 9026, pp. 
902604-1-902604-1, 
2014 
SPIE-IS&T 
doi: 
10.1117/12.2039663 
[6] B. Nair, V. Santhaseelan, C. Cui, and V. K. Asari, “Intrusion 
detection on oil pipeline right of way using monogenic signal 
representation,” Proc. SPIE 8745, 2013, p. 87451U,  
doi:10.1117/12.2015640 
[7] H. Zheng and L. Li, “An artificial immune approach for 
vehicle detection from high resolution space imagery,” 
IJCSNS International Journal of Computer Science and 
Network Security, vol.7 no.2, pp. 67-72, February 2007. 
[8] L. Eikvil, L. Aurdal, and H. Koren, “Classification-based 
vehicle detection in high resolution satellite images,” ISPRS 
Journal of Photogrammetry and Remote Sensing, vol. 64, 
issue 
1, 
pp. 
65-72, 
January 
2009. 
doi.org/10.1016/j.isprsjprs.2008.09.005 
[9] J. Gleason, A. Nefian, X. Bouyssounousse, T. Fong, and G. 
Bebis, “Vehicle detection from aerial imagery,” 2011 IEEE 
International Conference on Robotics and Automation, pp. 
2065-2070, May 2011.  
[10] Y. Sun, Z. Liu, S. Todorovic, and J. Li, “Synthetic aperture 
radar automatic target recognition using adaptive boosting,” 
Proceedings of the SPIE, Algorithms for Synthetic Aperture 
Radar Imagery XII, 5808, pp. 282-293, May 2005. 
[11] P. Vasuki and S. M. M. Roomi, “Man-made object 
classification in SAR images using Gabor wavelet and neural 
network classifier,” Proc. IEEE. Devices, Circuits and 
Systems Int. Conf., India, pp. 537–539, IEEE 2012. 
[12] Roper, W. E. and Dutta, S. "Oil Spill and Pipeline Condition 
Assessment Using Remote Sensing and Data Visualization 
Management Systems,” George Mason University, 4400 
University Drive, 2006. 
[13] M. Zarea, G. Pognonec, C. Schmidt, T. Schnur, J. Lana, C. 
Boehm, M. Buschmann, C. Mazri, and E. Rigaud, “First steps 
in developing an automated aerial surveillance approach,” 
Journal of Risk Research, vol.13(3–4): pp. 407–420, 2013 
doi:10.1080/13669877.2012.729520. 
[14] J. A. Benediktsson, M. Pesaresi, and K. Arnason, 
“Classification and feature extraction from remote sensing 
images 
from 
urban 
areas 
based 
on 
morphological 
transformations,”  IEEE Transactions on Geoscience and 
Remote Sensing, vol. 41(9), pp. 1940–1949, 2003. 
[15] T. Gevers and A. W. M. Smeulders, “Color based object 
recognition,” Pattern Recognit., vol. 32, pp. 453–465, Mar. 
1999. 
[16] H. Wang and D. Suter, “Color image segmentation using 
global information and local homogeneity,” in: Proc. VIIth 
Digital Image Computing: Techniques and Applications, Sun 
C., Talbot H., Ourselin S. and Adriaansen T., Eds., Sydney, 
pp. 89-98, Dec. 2003. 
[17]  G.K. Ouzounis, M. Pesaresi, and P. Soille, "Differential area 
profiles: 
decomposition 
properties 
and 
efficient 
computation,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 34, no. 8, pp. 1533-1548, Aug. 
2012. 
[18] X. Chen, T. Fang, H. Huo, and D. Li, “Graph-based feature 
selection for object-oriented classification in VHR airborne 
imagery,” IEEE Transactions on Geoscience and Remote 
Sensing, vol.  49 (1), pp. 353–365, 2011. 
[19] https://www.mathworks.com/matlabcentral/newsreader/view_
thread/152405 
[20] D. P. Huttenlocher, G. Klanderman, and W. J. Rucklidge,. 
“Comparing images using the Hausdorff distance,” IEEE 
Trans. Pattern Anal. Mach. Intell. 15, 9, pp. 850–863, 1993. 
[21] F. O’Sullivan, S. Roy, and J. Eary, “A statistical measure of 
tissue heterogeneity with application to 3D PET sarcoma 
data,” Biostatistics vol. 4, pp. 433–448, 2003. 
 
 
 

