Low-Variance Software Reliability Estimation Using Statistical Testing
Fouad ben Nasr Omri, Safa Omri and Ralf Reussner
Chair for Software Design and Quality
Karlsruhe Institute of Technology
Karlsruhe, Germany
Email: {fouad.omri, safa.omri, ralf.reussner}@kit.edu
Abstract—Software statistical testing establishes a basis for sta-
tistical inference about the expected ﬁeld quality of software
based on an expected operational proﬁle. The standard statistical
testing approach draws randomly test cases from the expected
operational proﬁle according to the statistical distribution on the
expected inputs. Standard Statistical testing is in the most of the
cases impractical. The number of required test cases to reach a
target reliability with a given conﬁdence is too large. In this paper,
we present a test selection approach to minimize the variance
of reliability estimator and reduce the overhead of estimating
reliability. The presented approach combines the idea of statistical
testing with the concepts of stratiﬁed sampling. Experiments are
conducted to validate the efﬁciency of our approach.
Keywords–Software reliability testing, reliability estimation, sta-
tistical testing, stratiﬁed sampling
I.
INTRODUCTION
Statistical testing draws test cases from the expected Op-
erational Proﬁle (OP) according to the statistical distribution
on the expected inputs.
Reliability assessment using statistical testing can be
grouped in three different categories: (i) reliability growth
models, (ii) fault seeding models and (iii) sampling models.
Reliability growth models are making assumptions about the
number of faults removed at each testing step by trying to
extrapolate the future behavior of the software based on its past
behavior. The assumptions made by reliability growth models
are difﬁcult to justify [1][2]. Fault seeding models are also
making assumptions about the distribution of faults remaining
in the program after testing. Such assumptions cannot be
rigorously justiﬁed [3].
One class of reliability assessment approaches using sta-
tistical testing are sampling models. These models are the-
oretically sound [4], but they suffer from several practical
problems. Sampling models require a large number of test
cases [5]. In addition, a major concern is how to choose a
proper estimator for the reliability that provides accurate and
stable reliability estimate. The goodness of an estimator is
judged based on the following four properties: (i) unbiased, (ii)
minimum variance, (iii) consistent and (iv) sufﬁcient. The main
focus when selecting an estimator is the minimum variance
of the estimator. The other three properties are in most of
the cases satisﬁed by most of the estimators. The variance of
an estimator describes the closeness of the future estimate to
the previous estimate when rerunning the estimation with the
same setting. An estimator with low variance increases the
conﬁdence on the predicted estimate. In fact, a low variance
usually implies tighter conﬁdence interval for the estimate.
Consequently, we can improve the accuracy of the reliability
estimation by providing or choosing an unbiased estimator
that has a minimum variance. It is also important to note
that the more tests are executed the more will the variance of
the estimator decrease. Consequently, an estimator with low
variance can ﬁnd an accurate estimation with fewer test cases.
This paper presents a test selection strategy based on
adaptive constrained sampling of the OP to deliver an unbiased
reliability estimator which is both efﬁcient and accurate (i.e.,
needs less test cases than standard approaches to ﬁnd an
accurate estimate). We call our approach Adaptive Constrained
Test Selection (ACTS).
The rest of the paper is organized as follows. Section II
formulates the problem of reliability estimation variance re-
duction and identiﬁes the adaptive optimal test cases allocation
over the operational proﬁle sub-domains as a solution. The
main steps of our approach are described in detail, in Section
III. Experiments are set up to validate the performance of the
proposed approach in Section IV. We give an overview on
related work in Section V. Section VI concludes this paper
and proposes future research direction.
II.
PROBLEM FORMULATION
The target of this paper is to present a reliability estima-
tion approach that minimizes the variance of the reliability
estimator for discrete-time domain software. For discrete-time
domain systems, one is interested in the probability of success
of a single usage of the software.
A. Software Statistical Testing
Software statistical testing is testing based on an opera-
tional proﬁle.
Operational Proﬁle Deﬁnition: As deﬁned by Musa [6] ”an
Operational Proﬁle (OP) is a quantitative characterization of
how a (software) system will be used”. It consists of a set
of partitions of the software input domain (sub-domains) and
their probabilities of occurrence.
In most of the cases, the OP describes also the distribution
of the input variables of a program. We use in this work the
abstract OP representation deﬁned by Musa [6], which we
introduce in Section II-B.
286
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

B. Standard Tests Selection Approach
Statistical Testing as proposed by Musa [6] generates by
random sampling test cases according to the OP.
The OP is used to divide the input domain D of the
software to test in L disjoint sub-domains: D1, D2, . . . , DL.
Each sub-domain represents a possible operational use and has
a probability of occurrence according the OP. Let pi be the
probability of occurrence of sub-domain Di. The OP can be
represented as OP = {(Di, pi)|i = 1, 2, . . . , L}.
Let
A
a
sequence
deﬁned
as
follows:
A
=
{A0, A1, . . . , AL}, |A| = L + 1, where Ai = Pi
k=1 pi
for i = 1, . . . , L, and A0 = 0.
The generation of the test cases is then as follows:
1)
Generate an uniformly distributed random number
⇣ 2 (0, 1), if ⇣ 2 [Ai, Ai+1], then the sub-domain
Di+1 will be randomly sampled since Ai+1 − Ai =
pi+1, where pi+1 the probability of occurrence of
sub-domain Di+1.
2)
Generate input variables from the sub-domain Di+1
based on the provided input distributions, and execute
the test case.
3)
Repeat the above steps until a stopping criteria is
reached (e.g, target reliability value reached, target
conﬁdence on the estimated reliability reached, re-
quired test time reached, etc.)
C. Discussion
The test selection approach proposed by Musa [6] is a
random selection process without replacement. The selection
is controlled by the uniformly distributed random variable
⇣ 2 (0, 1). The main idea behind this approach is to ensure that
when the testing process is terminated because of (for example)
imperative software project constraints, then the most used
operations will have received the most testing effort. Musa
also claims that ”the reliability level will be the maximum
that is practically achievable for the given test time” [6]. One
key assumption here is that the sample of selected test cases
represents the expected software execution according to the
OP and delivers the maximum achievable reliability level.
However, this assumption is not always valid. It would be
ideal if we could separate successful program execution from
the failing ones. However, this is not likely, because failures
are often caused by small faults in a large program.
A software fault is a hidden programming error in one
or more program statements. A program consists of a set of
statements. A program execution is a program path executed
with an input value from the program’s input domain. A
program path is a sequence of statements. Each program path
has an input and an executed output which usually depends on
the input. Consequently, a program execution is considered as
a failure if the corresponding executed program path deviates
from the expected output.
Two program execution are similar if they execute the same
program path with different input value. If the same input value
is used then the two executions are equal.
Two similar program executions may differ only in regard
to executing a particular fault, with the result that one execu-
tion fails while the other does not. Conversely, two dissimilar
program execution may both fail because they execute the
same faulty program statement. Consequently, we may not
group the failing program executions together even if they have
the same causing fault.
Hence, it is realistic to assume that the reliability estimate
across the test sub-domains have different statistical properties
(i.e., mean and variance). In this case, we refer to the sub-
domains as heterogeneous sub-domains. Using conventional
proportional random sampling to select test cases from het-
erogeneous sub-domains does not guarantee that a statistically
sufﬁcient number of test cases will be selected from every
sub-domain. Hence, the statistical quality of the samples may
be compromised for some sub-domains. This may lead to
inaccurate statistical estimate.
Stratiﬁed sampling is designed to cluster a population made
of heterogeneous groups into disjoint strata and then randomly
sampling each strata. This paper addresses the problem of
heterogeneity of the OP sub-domains by using optimal strat-
iﬁed sampling. The goal is to formulate the statistical testing
approach as an optimal stratiﬁed random sampling process and
provide a reliability estimator which should reduce the number
of required test cases for the estimation while delivering
accurate reliability estimates.
D. Stratiﬁed Sampling Variance Reduction
Stratiﬁed sampling is based on the idea of iterated ex-
pectations [7]. Let Y be a discrete random variable taking
values y1, y2, ..., yL with probabilities p1, p2, ..., pL. Let X be
a discrete random variable. Then, E[X] = E[E[X|Y ]] =
PL
l=1 E[X|Y = yl]pl. Suppose that the population can be
divided into L > 1 groups, known as strata. Suppose that a
stratum l contains Nl units from the population (PL
l=1 NL =
N), and the value for the units in stratum l are x1l, x2l, ..., xNll.
Let Wl =
Nl
N
and µl =
1
Nl
PNl
i=1 xil, then it follows
that the population mean is µ
=
1
N
PL
l=1
PNl
i=1 xil
=
1
N
PL
l=1 Nlµl = PL
l=1 Wlµl.
Then, instead of taking a simple random sample (SRS) of
n units from the total population, we can take a SRS of size nl
from each stratum (PL
l=1 nl = n). Here µl = E[X|stratum
l] and Wl = P[Stratum l], so the overall mean satisﬁes the
setup of an iterated expectation.
Let X1l, X2l, ..., Xnll be a sequence of independent and
identically distributed random variables samples from stratum
l,
¯
Xl
=
1
nl
Pnl
i=1 Xil be the sample mean, and Sl
2
=
1
nl−1
Pnl
i=1(Xil − ¯
Xl)2 be the sample variance. Then, an
estimate of the population mean µ is:
¯
XS = PL
l=1
Nl
N ¯
Xl =
PL
l=1 Wl ¯
Xl = PL
l=1 Wl 1
nl
Pnl
i=1 Xil. Since the random vari-
ables Xl are independent, then:
var( ¯
XS) = PL
l=1 Wl
2V ar( ¯
Xl) = PL
l=1 Wl
2 1
nl (1− nl−1
Nl−1)σl2,
where σ2
l =
1
Nl
PNl
i=1(xil − µl)2 is the variance of stratum l.
If we assume that nl ⌧ Nl for each stratum l so that the
ﬁnite population factor FPC = 1− nl−1
Nl−1 ⇡ 1 can be ignored,
then:
var( ¯
XS) =
L
X
l=1
Wl
2 1
nl
σl
2 = 1
N
L
X
l=1
Wl
2 σl2
al
(1)
where al = nl/N indicates the fraction of samples drawn from
the stratum l.
287
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

This variance is controllable through the allocation ra-
tio al. For example, the proportional allocation, where
al
= Wl.N/N
= Wl, yields the variance var( ¯
XS) =
1
N
PL
l=1 Wlσl2.
By Lagrange multiplier method, the optimal allocation
a⇤ := (a⇤
1, . . . , a⇤
L) is derived in closed form
a⇤
k =
Wk.σk
PL
l=1 Wl.σl
(2)
achieving the minimal variance var( ¯
XS) = 1
N
PL
l=1 Wl
2 σl
2
a⇤
l =
1
N (PL
l=1 Wlσl)2, [7].
Moreover, due to the mutual independence of samples
across the strata, the empirical mean
¯
XS is asymptotically
normal [7].
E. Assumptions
In order to formulate the concerned research goal, some
assumptions on the software are presented.
1)
The software is frozen when estimating the reliability,
since reliability estimation aims at testing the current
status of the software. The software will not be
modiﬁed during the estimation process. The software
can be modiﬁed after the estimation process.
2)
The output of each test is independent of the testing
history. In some cases, it is possible that a test case is
judged to be failure free although it actually leads to
some faults which cannot be observed due to limited
test oracles. We consider such test cases to be failure
free. However, such unobserved faulty program states
can cause the failure of some following test cases.
Consequently, the latter test cases can be mistakenly
considered as faulty test cases. This leads to an error
in the reliability estimation. However, this is not a
reliability estimation approach concern rather is a test
oracle problem.
3)
Each test case either fails or succeeds. A test oracle
is used to verify the behavior of the software under
test.
4)
We assume that a proper test oracle is available, since
this work focuses on the effectiveness and efﬁciency
of reliability estimation.
5)
In each operational use represented by a sub-domain
Di, all possible software operations and possible
inputs are equally likely to arise.
6)
We assume that an OP is provided for the tested
software.
III.
ADAPTIVE CONSTRAINED TEST SELECTION
The OP = {(Di, pi)|i 2 {1, ..., L}, PL
i=1 pi = 1} deﬁnes
the expected input domain of the program’s input variables.
Each partition (Dl, pl) is a subset of the OP, and pl ≥ 0 is
the probability that a program input belongs to sub-domain Dl.
The OP is a natural deﬁnition of the strata for stratiﬁed random
sampling. Each stratum l corresponds to the sub-domain Dl
and has a weight Wi = pl.
A test case either fails or not. Consequently, each test case
execution is a Bernoulli trial. Let Xil be the outcome of test
case i from stratum l, i.e., from sub-domain Dl, then:
Xil =
⇢1,
if the test case fails
0,
if the test case does not fail.
Let µi deﬁned as P(test cases from sub-domain Di fail) =
µi, where i = {1, 2, . . . , L} and µi 2 [0, 1].
Based on assumption 2, {Xil} are independent random
variables, and since PL
i=1 pi = 1, then it can inferred that
P(Xil = 1) = µi (i.e., the probability that test case i from
sub-domain Dl fails). Each test case will lead the software
under test to failure or success. And in each sub-domain the
probability of failure of each test case is equal for all test cases
in the sub-domain. Hence, the distribution of Xil is binomial
distribution with µi.
Consequently, the sample mean of stratum l,
¯Xl
=
1
nl
Pnl
i=1 Xil is an unbiased point estimator of µi.
The reliability of the tested software can be deﬁned
as the weighted sum of the reliability of the sampled OP
sub-domains Di,i={1,...,L} : R
=
PL
i=1 pi(1 − µi). An
unbiased estimator of the reliability is then deﬁned as:
bR = 1 −
L
X
i=1
pi ¯Xi = 1 −
L
X
l=1
1
nl
.pl
nl
X
i=1
Xil
(3)
with
E[ bR]
=
1
−
PL
i=1 piµi
and
var[ bR]
=
PL
i=1 pi2 µi.(1−µi)
ni
= PL
i=1 pi2 σi
2
ni , since the distribution of
Xil is a binomial distribution with µi.
A. Optimal Test Cases Selection
The Problem of selecting the test cases optimally from the
OP sub-domains is an adaptive optimization problem formu-
lated as follows. Given the OP, we want to select a total number
n of test cases, where (i) ni test cases are selected from each
sub-domains Di,i2{1,...,L} and (ii) PL
i=1 ni = n, with the goal
to minimize var[ ˆR]. For mathematical tractability, we assume
in this section that the total number of required test case n as
well as the sub-domains failure rates and consequently their
variances are known. These assumptions will be relaxed in the
next sections. According to Section II-D:
ni = n
piσi
PL
k=1 pkσk
(4)
Note that the larger the variance σi2 of the failure rate of
the software when executed with inputs from the sub-domain
Di, the more test cases should be selected from that sub-
domain. This makes sense, since the sub-domain with higher
estimated/observed failure rate variability should require more
testing to attain the same degree of precision as those with
lower variability. If the variances of all sub-domains are all
equal, the optimal allocation is proportional allocation.
B. Constrained optimal allocation
The intuition behind statistical testing is that the highest
the probability of occurrence of a sub-domain, the larger the
number of test cases executed from that sub-domain.
To account for this, the optimal allocation introduced in the
previous section is formulated as a constrained optimization to
a utility cost function c⇤. Let ci = 1 − pi the cost of selecting
288
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

1: if SC(TOP ) 6= 1 ^ SCmin = SC(T(Dk,pk)) < 0 then
// T(Dk,pk) is over-proportional sampled
2:
n = d nk
pk e
3:
for T(Di,pi) 2 T ^ T(Di,pi) 6= T(Dk,pk) do
4:
ni = dn.pie
5: //select extra (dn.pie − ni) test cases
6:
end for
7: else
8:
for T(Di,pi) 2 T do
9:
ni = dn.pie
10:
end for
11: end if
Figure 1. Adjust to Proportional Sampling
a test case from a sub-domain Di that has a probability of
occurrence pi, Then
ni = c⇤.
piσi/pci
PL
k=1 pkσk/pck
(5)
The cost function c⇤ is deﬁned in Section III-D.
Note that the higher the cost ci of selecting a test case from
sub-domain Di, the smaller the sub-domain sample size ni.
Since the cost function ci is deﬁned as ci = 1 − pi, then
(5) means: the smaller the probability of occurrence of a sub-
domain Di, the smaller the sample size ni.
C. Similarity Conﬁdence
When testing a software according to an OP, the goal is to
simulate the expected software execution as described by the
OP. Consequently, it is interesting to quantify the similarity
of the total set of selected test cases to the expected OP. It is
also interesting to control the testing process toward a 100%
similarity to the OP.
Let T(Di,pi) be the set of test cases selected from the sub-
domain (Di, pi){i21,...,L}. Let |T(Di,pi)| = ni, i.e., the set
T(Di,pi) contains ni different test cases selected from the sub-
domain (Di, pi){i21,...,L}. Let TOP = {T(Di,pi)|(Di, pi) 2
OP = {(Di, pi)|i 2 {1, ..., L}, PL
i=1 pi = 1}} the set of
selected test cases from the OP. The similarity of T(Di,pi) to the
OP when a total number n = | S
(Di,pi)2OP T(Di,pi)| = |TOP |
of test cases is selected from the OP sub-domains, is deﬁned
as follows:
SC(T(Di,pi)) =
(
ni
dpi.ne,
if ni  dpi.ne
−
ni
dpi.ne,
if ni > dpi.ne
(6)
The similarity conﬁdence of the total selected test cases is
consequently deﬁned as follows: SC(S
(Di,pi)2OP T(Di,pi)) =
PL
i=1 SC(T(Di,pi))
L
Let SCmin
=
min{SC(T(Di,pi))|i
2
{1, ..., L}}
=
SC(T(Dk,pk))k2{1,...,L}, the minimum computed similarity to
the OP.
Algorithm 1, adjusts the allocation of the test cases from
each sub-domain (Di, pi){i21,...,L} to reach a similarity conﬁ-
dence of 100%. The steps of the algorithm are as follows. If
the selected tested cases TOP is not similar to the OP and if
SCmin = SC((Dk, pk)) is negative (line 1), then it means that
the sub-domain (Dk, pk) is over proportionally sampled. In this
case, the total number of test case n is updated proportionally
to nk (line 3), and for each sub-domain except the sub-domain
(Dk, pk), extra (dn.pie − ni) test case are selected (lines 4-6).
Otherwise, the sub-domains are under proportionally sam-
pled, and for each sub-domain (Di, pi), extra (dn.pie − ni)
test case are selected (lines 8-9).
D. Stopping Criteria
We deﬁne a test stopping criteria based on the tester
required (i) maximal error of the reliability estimate d, and
(ii) conﬁdence level (1 − ↵). The goal of reliability testing is
then to estimate the reliability ˆR to within d with 100(1−↵)%
probability.
The total required number of test cases depends on the
allocation of the selected test cases to the sub-domains. Let
al (as deﬁned in Section II-D) be the allocation ratio for the
sub-domain Dl, with nl = n.al. Also, let z be the upper ↵/2
critical point of the standard normal distribution. Then, we
want to ﬁnd n such that z[var[ bR]]1/2 = d (margin of error
equation), where var[ bR] = PL
i=1 pi2 σi
2
ni = 1
n. PL
i=1 pi2 σi
2
al .
Solving the margin of error equation for n leads to:
n = z2
d2
PL
i=1 pi2 σi
2
al .
From (5), we can compute the total cost c⇤ required to
achieve the desired level of accuracy as follows [7]:
al =
pl.σl/pcl
PL
i=1 pi.σi.pci
and c⇤ = z2
d2
" L
X
i=1
pi.σi.pci
#2
.
(7)
From here, we can compute nl = c⇤.al, and then, ulti-
mately n.
E. Adaptive Constrained Test Selection Algorithm
Based on the discussions above, the adaptive constrained
test selection algorithm works as described in Algorithm 2. In
the intialization phase of the algorithm (lines 6-7), |T(Di,pi)|
test case are selected from each sub-domain (Di, pi) based on
a given initial number of test case nstart. T(Di,pi) represents
the set of test cases selected from sub-domain (Di, pi). In the
sampling phase (lines 10-27), the algorithm computes for each
sub-domain the optimal required number of test cases to be
select based on the stopping criteria formula in equation 7
(line 12-13). Extra test cases are then selected if required (lines
15-16). Otherwise, test cases have been optimally selected
from that sub-domain (line 18). The algorithm computes the
variance of the observed failure rate for each sub-domain after
each sampling phase (line 24), and adjust the test allocation
toward 100% similarity to the OP. The algorithm stops and
returns the estimated reliability if (i) a maximal allowed test
time interval ∆ has passed or (ii) for all sub-domains the
optimal required number of test cases has been selected and
the total selected test cases are 100% similar to the OP (line
21).
289
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

Require: OP = {(Di, pi)|i 2 {1, ..., L}, PL
i=1 pi = 1}
1:
TOP = {T(Di,pi)|(Di, pi) 2 OP}
2:
∆ : maximal allowed test time
3:
nstart : initial number of test cases to start
4:
1 − ↵ : conﬁdence level
5:
d : margin of error
6: for (Di, pi) 2 OP do // 1. Initialization
7:
|T(Di,pi)|  dnstart.pie
8:
9: end for
//2. Adaptive optimal constrained stratiﬁcation
10: while true do
11:
for (Di, pi) 2 OP do
12:
c⇤ = z2
d2
hPL
i=1 pi.σi.
p
(1 − pi)
i2
13:
ai =
pi.σi/p
(1−pi)
PL
k=1 pk.σk.p
(1−pk)
14:
no
i = dc⇤aie
15:
if |T(Di,pi)| < no
i then
//select extra (no
i −|T(Di,pi)|) test cases from (Di, pi)
16:
|T(Di,pi)|  no
i
17:
else
18:
opt = opt + 1
19:
end if
20:
end for
21:
if ∆ passed or (opt = L ^ SC(TOP ) = 100%) then
22:
break;
23:
end if
24:
update statistics for all (Di, pi)
25:
Adjust to proportional sampling: call Algorithm 1
26:
opt = 1
27: end while
28: return bR = PL
i=1 pi. bRi
Figure 2. Adaptive constrained test cases selection
IV.
EXPERIMENTAL EVALUATION
We conduct a set of experiments on a real subject program
to evaluate the performance of the Adaptive Constrained Test
Selection (ACTS) algorithm against the standard proportional
test selection approach as proposed by Musa [6] (PS), and the
theoretical optimal test selection approach (OS) with respect
to the estimated reliability accuracy and precision. For (OS),
we assume that we know the failure rates in advance, and we
sample accordingly.
A. Experiment Design and Setup
1) Subject Program and Operational Proﬁles: Space: a
language-oriented user interface developed by the European
Space Agency. It allows the user to describe the conﬁguration
of an array of antennas with a high level language. The correct
version as well as the 38 faulty versions and a test suite of
13, 585 test cases are downloaded from the software-artifact
infrastructure repository [8]. In these experiments, three faulty
versions are not used because we did not ﬁnd test cases that
failed on these faulty versions. Space is 9126 LOCs big.
A failure of an execution is determined by comparing the
outputs of the faulty version and the correct version of the
program. A failure is a deviation from the expected output.
The failure rates for both studied programs are empirically
computed by executing all the available test cases against each
faulty version of a program and recording the number of failed
test cases.
Operational proﬁles for Space are not available. We create
operational proﬁles for Space as follows. We assume that in
each sub-domain Di, all possible inputs are equally likely
to arise. Hence, it follows that the number of sub-domains
(greater or equal to two sub-domains) as well as the number
of inputs in each sub-domain may not bias the statistical
properties (i.e., variance and mean) of the estimated reliability.
The estimated reliability is inﬂuenced by the probability of
occurrence of the sub-domains, as well as the true failure rate
of the tested software when executed with inputs from each
sub-domain. In that sense, we partition the test cases of Space
in six disjoint sub-domains. All six sub-domains contain the
same number of test cases except for rounding issues. For
each sub-domain, test cases are randomly selected without
replacement from the pool of test cases. The 13, 585 test cases
of Space are partitioned into six disjoint classes: 2264, 2264,
2264, 2264, 2264 and 2265. In order to minimize possible bias
due to the choice of the test cases in each sub-domain, we
repeat the allocation of the test cases of each subject program
into the six sub-domains twice. This results into 2 possible
allocations of the test cases to sub-domains Di for each subject
program.
Due to time and space limitation, not all possible opera-
tional proﬁles can be adopted in the experiments. We deﬁne
two different proﬁles for the probability of occurrence of the
sub-domains: (i) uniform proﬁle: the probability of occurrence
of each sub-domain is the same except for rounding error and
(ii) optimal proﬁle: the probability of occurrence of each sub-
domain is proportional to the number of test cases allocated
to each sub-domain using optimal allocation
These two proﬁles are some typical or extreme proﬁles and
cannot represent all usage scenarios in ﬁeld use. Consequently,
for each subject program, 4 different operational proﬁles are
created.
2) Performance Metrics: ACTS, PS and OS are random-
ized test selection strategies. For statistical signiﬁcance, we
conduct 200 independent repetitions of each experiment for
each test selection strategy.
We compare the performances of ACTS, PS and OS
by comparing the accuracy and precision of the estimated
reliability by each approach. The accuracy of an estimate is
a measure of how close the estimated value is to its true
value. The precision of an estimate is a measure of how close
the estimates measured from different samples are to another,
when the samples are taken from the same data set. We use
the sample variance as metric for the reliability estimation
accuracy. The sample variance is an unbiased estimator of
the variance. We use the root mean squared error (RMSE)
to quantify the estimate precision.
Based on assumption 5 in Section II-E, the reliability
estimates delivered by ACTS, PS, and OS are unbiased.
Consequently, we can compare the relative efﬁciency of the
estimates using the sample variance. For each experiment E we
deﬁne the mean value of the reliability estimate (R), its sample
variance ( S2
199( bR)), its root mean squared error (RMSE( bR)),
and the relative efﬁciency of the reliability estimator using
ACTS to PS and OS as follows:
290
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

R =
1
200
200
X
i=1
d
Ri,
S2
199( b
R) =
1
199
200
X
i=1
(d
Ri−R)2,
RMSE( b
R) =
v
u
u
u
t
1
200
200
X
i=1
(d
Ri − R)2
eff( b
RACT S , b
RP S ) =
RMSE( b
RP S )
RMSE( b
RACT S )
,
eff( b
RACT S , b
ROS ) =
RMSE( b
ROS )
RMSE( b
RACT S )
where R is the true reliability calculated based on the true
failure rates, c
Ri the reliability estimate in repetition i of
the experiment, bRACT S the reliability estimate using ACTS,
bRP S the reliability estimate using PS and bROS the reliability
estimate using OS.
The differences in reliability mean values between the
different test selection strategies is conﬁrmed using the non-
parametric Mann-Whitney U test [9]. The differences between
the sample variances are tested using the Brown-Forsythe test
[9].
For each experiment and for each test selection strategy,
we compute the reliability estimate at seven checkpoints:
200, 250, 350, . . . , 500. After 200 repetitions of the experi-
ment, we compute the mean value, sample variance and the
root mean square error of the reliability estimates for each
test selection strategy. Note that the more test cases are
executed the more will the variance of the estimator decrease.
In addition, the experimental dataset is selected randomly
from the population and the selection is repeated 200 times.
Consequently, the selected dataset do not affect the efﬁciency
and the generalizability of ACTS.
B. Experimental Results
The goal of this set of experiments is to assess the
efﬁciency and precision of our reliability estimation approach.
Figure 3 presents the sample means and sample variances
for Space. The dashed lines are the true reliability values for
the subject programs.
According to the experimental results, the means as well
as the sample variances of the reliability estimates of ACTS
are closer to the true values than those of PS and OS. This
is conﬁrmed by the statistical tests Mann-Whitney U test and
Brown-Forsythe test in table I. The table conﬁrms that ACTS
signiﬁcantly deliver more accurate reliability estimate that PS
and OS.
The computed mean of the relative efﬁciency of the reli-
ability estimator using ACTS compared to the one using PS
for the Space experiments was 1, 57. This means that PS will
yield a reliability estimate as accurate as ACTS only if 57%
more test cases are selected.
The computed mean of the relative efﬁciency of the reli-
ability estimator using ACTS compared to the one using OS
for the Space experiments was 1, 32. This means that OS will
yield a reliability estimate as accurate as ACTS only if 32%
more test cases are selected.
V.
RELATED WORK
Stratiﬁed sampling is linked to the idea of partition testing
or sub-domain testing of a software. Techniques to estimate
software reliability using partition testing are proposed by
Brown and Lipow [10] and Duramn and Wiorkowski [11], for
example. They introduced the idea of sampling to reliability
estimation but did not specify a sampling design. Podgurski
TABLE I. Mann-Whitney U and Brown-Forsythe test results
for the sample means and variances for Space
Variance
Mean
Scenarios
ACTS
OS
ACTS
OS
Space pro-
ﬁle1
PS
0/7
4/7
6/7
0/7
OS
0/7
-
7/7
-
Space pro-
ﬁle2
PS
0/7
1/7
7/7
7/7
OS
1/7
-
7/7
-
Space pro-
ﬁle3
PS
1/7
1/7
7/7
5/7
OS
1/7
-
5/7
-
Space pro-
ﬁle4
PS
0/7
1/7
5/7
1/7
OS
2/7
-
6/7
-
et al.’s [12] work of is the most related work to our research.
However, they only used the idea of equal stratiﬁcation using
clustering to estimate the software reliability from software
execution proﬁles collected by capture/replay tools. Failure
rates have been extensively used in the area of adaptive
random testing (for example Cangussu et al.’s [13] and Chen
et al.’s [14]). Adaptive random testing aims to distribute the
selected test cases as spaced out as possible to increase the
chance of hitting the failure patterns. The intuition behind
adaptive random sampling can be added in a future work to
our approach to probably further enhance the efﬁciency of
the reliability estimator. Cangussu et al.’s [13] and Chen et
al.’s [14] do not address the problem of reliability estimator
efﬁciency.
Th´evenod-Fosse and Waeselynck [15] present the usage
of probabilistic test generation for fault detection. They gen-
erate automatically tests to address different behavioral and
structural test criteria. Apparently, Th´evenod-Fosse and Wae-
selynck [15] view the evaluation of tests as inexpensive. They
call their approach ”statistical testing” although it dos not
involve reliability estimation. In contrast to Th´evenod-Fosse
and Waeselynck [15], we think that evaluating test is an
expensive process. Our approach aims to reduce the variance
of a reliability estimator and consequently reduce the required
number of executed and evaluated test cases to reach a target
reliability conﬁdence. A recent work on adaptive testing by
Junpeng and Cai [16], allocates test cases using a gradient
search method based on the variance variation of the failure
rate. However, their approach introduces bias resulting from
the use of the gradient method: it is possible that all test cases
are selected from the sub-domain that ﬁrst reveals a failure.
They avoid such situations by introducing a biased estimator
using Bayesian estimation.
VI.
CONCLUSIONS AND FUTURE WORK
Statistical testing is in the most of the cases impractical
due to the large number of test cases required to reach a
target reliability. In this paper, we presented an approach to
automatically select test cases from an operational proﬁle sub-
domains with the goal to reduce the variance of the reliability
estimator. Our initial experimental results are promising and
shows that our approach ACTS outperforms PS and OS.
We plan to conduct further experiments to validate the
291
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

Figure 3. Sample means and sample variances of the reliability estimates for Space
effectiveness of ACTS on software with real speciﬁed opera-
tional proﬁles. We also plan to further investigate the efﬁciency
of our approach for ultra-high software reliability scenarios.
ACKNOWLEDGMENT
This work was partially supported by the German Fed-
eral Ministry of Economics and Energy (BMWI), grant No.
01MD11005 (PeerEnergyCloud).
REFERENCES
[1]
M.-H. Chen, M. Lyu, and W. Wong, “Effect of code coverage on
software reliability measurement,” Reliability, IEEE Transactions on,
vol. 50, no. 2, 2001, pp. 165–170.
[2]
P. K. Kapur, D. N. Goswami, and A. Bardhan, “A general software
reliability growth model with testing effort dependent learning process,”
Int. J. Model. Simul., vol. 27, no. 4, Sep. 2007, pp. 340–346.
[3]
C. V. Ramamoorthy and F. B. Bastani, “Software reliability status and
perspectives,” IEEE Trans. Softw. Eng., vol. 8, no. 4, Jul. 1982, pp.
354–371.
[4]
T. A. Thayer, M. Lipow, and E. C. Nelson, Software reliability : a
study of large project reality, ser. TRW Series of software technology;
2.
Amsterdam: North-Holland, 1978.
[5]
R. W. Butler and G. B. Finelli, “The infeasibility of quantifying the
reliability of life-critical real-time software,” IEEE Trans. Softw. Eng.,
vol. 19, no. 1, Jan. 1993, pp. 3–12.
[6]
J. D. Musa, “Operational proﬁles in software-reliability engineering,”
IEEE Softw., vol. 10, no. 2, Mar. 1993, pp. 14–32.
[7]
W. Cochran, Sampling techniques, ser. Wiley series in probability and
mathematical statistics: Applied probability and statistics. Wiley, 1977.
[8]
“The software-artifact infrastructure repository,” http://sir.unl.edu, ac-
cessed: 2014-08-30.
[9]
S. Wilks, Mathematical Statistics.
Read Books, 2008.
[10]
J. R. Brown and M. Lipow, “Testing for software reliability,” SIGPLAN
Not., vol. 10, no. 6, Apr. 1975, pp. 518–527.
[11]
J. W. Duran and J. J. Wiorkowski, “Quantifying software validity by
sampling,” Reliability, IEEE Transactions on, vol. R-29, no. 2, June
1980, pp. 141–144.
[12]
A. Podgurski, W. Masri, Y. McCleese, F. G. Wolff, and C. Yang,
“Estimation of software reliability by stratiﬁed sampling,” 1999.
[13]
J. W. Cangussu, K. Cooper, and W. E. Wong, “A segment based
approach for the reduction of the number of test cases for performance
evaluation of components,” International Journal of Software Engineer-
ing and Knowledge Engineering, vol. 19, no. 04, 2009, pp. 481–505.
[14]
T. Y. Chen, F.-C. Kuo, R. G. Merkel, and T. H. Tse, “Adaptive random
testing: The art of test case diversity,” J. Syst. Softw., vol. 83, no. 1,
Jan. 2010, pp. 60–66.
[15]
P. Th´evenod-Fosse and H. Waeselynck, “Statemate applied to statistical
software testing,” in Proceedings of the 1993 ACM SIGSOFT Interna-
tional Symposium on Software Testing and Analysis, ser. ISSTA ’93.
New York, NY, USA: ACM, 1993, pp. 99–109.
[16]
B.-B. y. Junpeng Lv and K. yuan Cai, “On the asymptotic behavior of
adaptive testing strategy for software reliability assessment,” Transac-
tion on software Engineering, vol. 40, no. 4, April 2014, pp. 396–412.
292
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-367-4
ICSEA 2014 : The Ninth International Conference on Software Engineering Advances

