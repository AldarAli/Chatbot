Refining the Scatteredness of Classes using Pheromone-Based Kohonen Self-
Organizing Map (PKSOM) 
Azlin Ahmad 
Faculty of Computer and Mathematical Sciences 
Universiti Teknologi MARA (UiTM)   
Shah Alam, Malaysia 
Center of Artificial Intelligence and Robotics (CAIRO), 
Malaysia Japan International Institute of Technology 
(MJIIT) 
Universiti Teknologi Malaysia (UTM) 
Kuala Lumpur, Malaysia 
azlin@tmsk.uitm.edu.my  
Rubiyah Yusof 
Center of Artificial Intelligence and Robotics (CAIRO), 
Malaysia Japan International Institute of Technology 
(MJIIT) 
Universiti Teknologi Malaysia (UTM) 
Kuala Lumpur, Malaysia 
rubiyah@ic.utm.my 
 
 
 
Abstractâ€” The Kohonen Self-Organizing Map (KSOM) is one of 
the well-known unsupervised learning algorithms, which has 
been applied in various areas. This algorithm can cluster and 
classify an enormous amount of data into several clusters 
according to the similarity of the data features. However, it has 
many drawbacks, such as difficulty of clustering the data, which 
have similar features. These may lead to the inefficient result; 
the data is scatteredly mapped even though it is accurately 
clustered into several clusters according to the features. 
Therefore, this paper proposed a Pheromone-based Kohonen 
Self-Organizing Map (PKSOM) algorithm to refine the 
scatteredness of the data in the clusters, thus to improve the 
cluster density. Some modifications have been made to the 
original Kohonen Self-Organizing Map (KSOM), adapted from 
the Ant Clustering Algorithm procedures. This PKSOM has 
been tested on three different datasets; Iris flowers, Glass and 
Wood datasets. Based on the result, the proposed method has 
improved the classification impressively by increasing the 
density of the data in clusters. Hence, it has also refined the 
scatteredness of classes, where each dataset is well clustered 
where data that have similar features are located closely to each 
other in the same cluster. However, there are a few overlapped 
clusters that still occur.   
Keywordsâ€“Kohonen Self-Organizing Map (KSOM); Pheromone; 
Ant Clustering Algorithm (ACA); Clustering; Cluster density.   
I. 
INTRODUCTION 
The Kohonen Self-Organization Map (KSOM) is an 
unsupervised learning technique that is expended by 
topological self-organizing maps stemming from techniques 
that were first proposed for competitive learning [1]. This 
algorithm is used to process the high dimensional data, and it 
is also designed to cluster the data into clusters of data that 
exhibit some similarities. Each cluster with similar features is 
projected onto the same node on the map. Otherwise, the 
dissimilarity increases with the distance that separates two 
projections on the map. Thus, the cluster space is identified to 
the map, so that the projection enables simultaneous 
visualization of the cluster and the observation space [2]. 
Being one of the most popular unsupervised learning 
technique, KSOM has been used in different areas, in 
clustering, that might help to solve a complex problem. 
Giraudel et al. [3] discussed a comparison between the 
application of KSOM and other conventional ordination 
methods for ecological community. As mentioned earlier, 
KSOM has been used in different areas for many purposes. 
Anthony has used the KSOM to develop a new color 
quantization algorithm for mapping the 24-bit color images to 
eight-bit color [4]. They proved that their proposed algorithm 
could produce better output compared to the Oct-Tree and 
Median-Cut algorithms with very limited samples. While 
Emamian et al. [5] have proposed the application of KSOM to 
recognize the transient crack-related signals in the presence of 
strong time-varying noise and other interferences. The 
application can cluster the acoustic emission signals for fault 
monitoring and as a result, it is showed that the KSOM did 
perform well with a small probability of error. 
The KSOM has also been widely used as a visualization 
tool for dimensionality reduction. Its unique topology 
preserving property can be used to visualize the relative 
mutual relationships among the data. It has been applied to 
organize and visualize vast amount of textual information, for 
example, the SOM that organizes massive document 
collection; WEBSOM [6]. The main benefit of KSOM is the 
topology preservation of an input space, which makes similar 
object appear closely on the map. Most of these applications, 
however, are based on 2D grids and map. Weijian et al. [7] 
investigated a hybrid neural network framework by 
combining 
the 
supervised 
learning 
algorithm 
with 
unsupervised algorithm on integrated representation platform 
of multiple two dimensional KSOM with the assistance of 
associative memory for clustering and classification of 
Remotely Sensed (RS) imagery. The formation of the clusters 
and the transformation from clusters to decision regions are 
implemented by unsupervised and supervised self-organizing 
learning on several Kohonen 2D Self-Organizing Maps 
(M2dSOM), individually. Xu et al. [8] used the two important 
operations in KSOM: vector quantization and topological 
preserving mapping, while introducing an online Self-
Organizing Topological Tree (SOTT), with faster learning, is 
proposed. Their proposed learning rule is novel and delivers 
the efficiency and the topological preservation compared to 
other structures of KSOM. The computational complexity of 
107
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

SOTT is better and its computation time is much shorter than 
the entire search process done by KSOM. Forkan et al. [9] 
proposed a new method for surface based on hybrid 
techniques using KSOM and Particle Swarm Optimization 
(PSO). The KSOM learns the sample data through mapping 
grid. Consequently, the learned and well-represented data has 
become the input for surface fitting procedure. The authors 
have proposed PSO to probe the optimum fitting points on 
surfaces and this algorithm has been applied on different 
types of curve to observe its ability in reconstructing the 
object while preserving the original shapes.  
However, the KSOM has several weaknesses, such as the 
difficulty of clustering and classifying the data, which have 
similar features, and this may lead to an inefficient result. 
Therefore, to improve the clustering result, a few researchers 
have proposed an optimized Kohonen by hybridizing KSOM 
with other techniques such as K-mean [19], simulated 
annealing [20], rough set theory and genetic algorithm [21], 
and Ant Colony Optimization (ACO) [22][23].  
There are numerous researches have been conducted using 
the combination of KSOM with ACO for clustering. For 
example, Mora et al. [22] proposed an ant-based method that 
takes benefit of the cooperative self-organizing ACO named 
as KohonAnts. It is designed as a clustering algorithm that is 
capable of grouping a set of the input samples into clusters 
with similar features same as KSOM behavior. The data is 
grouped without considering the class of the input pattern 
during the process. While Yang et al. [23] also applied ACO 
in their Ant-based of Self-Organizing feature Maps algorithm 
(ABSOM). This algorithm utilized the pheromone mechanism 
of ant colony system to memorize the history of the best 
matching unit and also adopted the state of transition rules of 
exploitation and exploration in ACO to determine the best 
matching unit. Chen et al. [27] proposed a new clustering 
method named AMC algorithm that can be accelerated by the 
use of a global memory bank, increase the radius of 
perception and also a density-based method that permits each 
ant to look for objects. This algorithm has reduced the times 
of region inquiry, hence, saved the clustering time.  
Even though most of these hybrid algorithms are capable 
of grouping the data samples accurately into required classes, 
unfortunately the data samples are scatteredly mapped on the 
topology map. Moreover, it is hard to identify the separation 
boundary among the classes. Therefore, this paper has 
proposed 
a 
Pheromone-Based 
Self-Organizing 
Map 
(PKSOM) algorithm to refine and improve the scatteredness 
of the data in the clusters by modifying the KSOM algorithm 
using the pheromone concept from the Ant Clustering 
Algorithm (ACA). The ACA is chosen because of its strength, 
where it is robust, flexible, self-organize, good convergence 
and parallel [24]. It is a probabilistic technique for solving 
computation problem, which can be reduced to finding a good 
path through graph [25]. Besides, it can cluster the data 
samples into numbers of clusters and the total number of 
cluster is generated automatically [26].  
 
This paper has been organized as follows. After presenting 
all concepts used in our method, in section Data Clustering, 
then we will discuss the proposed methods thoroughly, 
followed by a discussion on experimental works and analysis 
of results. Finally, we will conclude out description in the last 
section with a discussion of the obtained results and future 
works. 
II. 
DATA CLUSTERING 
A. Kohonen Self-Organizing Map (KSOM) 
Developed by Teuvo Kohonen in year 1982, Kohonen 
Self-Organizing Map (KSOM) is an example of unsupervised 
training method for neural networks that implements the 
vector quantization [6]. Differing from the traditional vector 
quantization where KSOM task is to define how the mapping, 
m is ordered and how the input x is distributed on the map. In 
KSOM, there are a few factors that might influence the 
clustering results such as learning parameters, topology map 
and map sizes. The major steps in KSOM are in the distance 
calculation and the weights update. A KSOM unit computes 
the Euclidean Distance between an input x and its weight 
vector w. In the Kohonen one-dimensional network, the 
neighborhood of radius 1 of a unit at the kth position consists of 
the units at the positions k âˆ’ 1and k + 1. The KSOM network 
and its algorithm are shown in Fig. 1 and Fig. 2.  
 
 
 
Figure 1. The KSOM network. 
 
The performance of KSOM algorithm is measured using 
two measurements that commonly used in evaluating and 
measuring the self-organization algorithm: topological error 
and quantization error [10]. The topological error is used to 
measure the proportion of all data vectors for which first and 
second-best matching unit or winning unit are not adjacent 
vectors. If the value of topological error is lower then the 
KSOM preserves the topology is better. The quantization error 
is used to measure the average distance between each data 
vector and its best matching unit or winning unit. If the value 
of quantization error is small then it shows that the input 
vector is closer to its prototype.   
 
â€¦â€¦â€¦â€¦.â€¦.
 X1
 X2
 X3
 X3
 X157
Size-Y
Size-X
Input Vector
108
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

 
 
Figure 2. The KSOM Algorithm 
B. Ant Clustering Algorithm (ACA) 
The Ant Clustering Algorithm (ACA) [16] is an ant 
colony optimization algorithm that is designed for clustering 
purposes. It is a self-organizing algorithm where, the positive 
and negative feedback can display sudden modification that 
affect the pattern at the global level and the interaction is 
based on the cues and signals. Artificial ants are needed and 
the movements of these ants are based on the probabilities to 
pick up and drop down the object that are inversely 
proportional to the number of objects that are has experiences 
within a short period. Therefore, the ant will be more likely to 
deposit the object near larger clusters of objects. This 
algorithm applied the similarity idea where the degree of pair 
of data objects can reveal their probability of grouping into 
the same cluster. Fig. 3 shows the algorithm for ACA by 
Lumer et al. [11]. 
The ant can measure the similarity of the objects 
perceived within a local region and also identify the objects at 
the central site that is equally likely to group with the other 
objects. This algorithm used two types of pheromone for its 
searching strategy: cluster pheromone and object pheromone. 
Normally the cluster pheromone is used to lead or guide the 
ant to search for compact clusters while the object pheromone 
is to guide the ant to search for an object to be picked up and 
dropped. Both picking and dropping decisions require the 
evaluation of f(i), which provides information on the 
similarity and density of the data items in the antâ€™s local 
neighborhood.  
 
 
 
1: procedure LUMER & FAEITA 
2: Randomly scatter data items on toroidal grid 
3: Randomly place agents on the toroidal grid  
4: for t=1 to max iterations do 
5:   j=random agent 
6:   move agent j randomly by step size grid cells  
7:   l= is agent jâ€™s grid position occupied by data 
item? 
8:   e= is agent jâ€™s grid position occupied by a data  
item? 
9:   if(l=TRUE) AND (e=FALSE) then  
10:  i=data item carried by agent j 
11:  drop=(random()<=pdrop(i) 
12:  if drop = TRUE then 
13: let agent j drop data item i at its current post  
14:  end if 
15: end if 
16: if (l=FALSE) and (e=TRUE) then  
17:  i=data item at agent jâ€™s grid position 
18:  pick=random()<=Ppick(i)  
19:  if pick=TRUE then 
20:  let agent j pick up data item i 
21:  end if  
22: end if 
23: end for 
24: end procedure 
 
Figure 3. The Ant Clustering Algorithm (ACA) algorithm 
III. 
PHEROMONE-BASED KOHONEN SELF-ORGANIZING 
MAP (PKSOM) 
The proposed algorithm; Pheromone-Based Kohonen Self-
Organizing Map (PKSOM) is a modified KSOM using a 
pheromone concept adapted from the ACA algorithm. As 
mentioned before, there are two main steps in KSOM that 
determine the self-organizing process: (1) distance calculation 
and (2) weight update. Some modification has been made to 
these two steps using the pheromone concept. The full 
PKSOM algorithm is shown in Fig. 4.  
 
 
 
Figure 4. The PKSOM Algorithm 
Initialize weights Wij, set topological 
map, radius and learning rate
Calculate distance between input and 
output,
 
Stopping condition > Tmax
! ! =!
(!"# âˆ’ !")!
!
!
 
Find index J such that D(j) is minimum 
to be the winning unit
For all units j, update weights w
 
!"# !"# = !"# !"# + ![!" âˆ’ !"# !"# ] 
Update learning rate at speciï¬ed time
Print location items
Initialize weights Wij, set topological 
map, radius and learning rate
Calculate distance between input and 
output,
 
Stopping condition > Tmax
Find index J such that D(j) is minimum 
to be the winning unit
Update learning rate at speciï¬ed time
Print location items
! ! =! 1
!!
1 âˆ’ !(!, !)
!
!
!!!
 
f(i) >=1.0
Calculate probability to pick
 
!"#$% =!
!1
!1 + !(!)
!
! 
Calculate probability to drop
 
!!"#$ = !2(! ! ) 
Yes
No
No
Yes
109
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

A. Distance Calculation 
In KSOM, the distance calculation uses Euclidean 
Distance (ED); a distance between two points on a plane. The 
calculation is shown in (1) [1].  
ğ‘‘ ğ‘— =  Â 
(ğ‘¤ğ‘–ğ‘— âˆ’ ğ‘¥ğ‘–)!
!
!!!
 
(1) 
The wij is representing the weight connection between input 
and output node, while xi is the value of input X. This 
equation compares two objects across a range of variables and 
determines how dissimilar the objects are. In KSOM, the 
minimum value of ED is selected to be a winning unit. The 
minimum value also shows the two objects are very similar to 
each other. 	 Â 
For modification, the ED is replaced using Pheromone 
Density Measure (PDM) in ACA, as shown in (2). The 
pheromone density function is a way to measure the average of 
the similarity of objects oi and other objects present in the 
neighbor Î´, instead of looking at the distance, individually 
[11].  
ğ‘“ ğ‘– =  Â  1
ğ›¿!
1 âˆ’ ğ‘‘(ğ‘–, ğ‘—)
ğ›¼ Â  Â 
!
!!!
 
(2) 
Here, the d(i,j) is to define the distance or dissimilarity 
between objects in the space of objectsâ€™ attributes. While the ğ›¼ 
is a discriminant factor that defines the scale of dissimilarity 
and it is important for it to determine when two items should 
be or should not be located next to each other. The value 
selection for ğ›¼ is also crucial where this might affect the 
formation of clusters. If the ğ›¼ value is too large, there might 
not be enough discrimination between different items that may 
lead to the formation of clusters composed of item, which 
should not belong to the same cluster. On the other hand, if the 
value is too small, the distances between items in the space are 
amplified to the point where the items, which are relatively 
close in attribute space cannot be clustered together because 
the discrimination is too high.  
B. Weights Update 
      Despite the pheromone density calculation, the other 
modified step is the weightsâ€™ update. In original KSOM 
algorithm, the weight changes are calculated using Gaussian 
function [2], as shown in (3). The Gaussian function is used as 
a decreasing function of the grid distance between objects. 
Due to the collective learning scheme in KSOM, the input 
signals, which are near to each other, will be mapped on 
neighboring neurons. Thus, the topology inherently present in 
the input signals will be preserved in the mapping. The rk and 
rc represent the two objects to be calculated, while the Î´ is 
representing the neighborhood radius.   
 
âˆ†ğ‘¤ = ğ‘’ğ‘¥ğ‘ Â  âˆ’ (ğ‘Ÿğ‘˜ âˆ’ ğ‘Ÿğ‘)!
2ğ›¿!
 Â  
(3) 
 
In order to ensure this algorithm works well with 
pheromone density calculation, the Gaussian function is 
replaced with the probability to pick up (4) and probability to 
drop (5) the object in ACA. Deneubourg et al. [17] have 
proposed these probabilities based on the corpse clustering and 
the larvae sorting in ants; where the isolated item should be 
picked up and dropped at some other location where more 
items of that type are present. The decisions to drop and pick 
the object are random and influenced by the data items in the 
neighborhood. The probability of dropping an item might 
increase if the surrounded neighborhood data is similar. In 
contrast, the probability of picking an item might increase if 
the surrounded neighborhood is dissimilar.   
ğ‘ƒğ‘ğ‘–ğ‘ğ‘˜ =  Â 
ğ‘˜1
ğ‘˜1 + ğ‘“(ğ‘–)
!
 Â  
(4) 
ğ‘ƒğ‘‘ğ‘Ÿğ‘œğ‘ = 2(ğ‘“ ğ‘–  Â ğ‘¤â„ğ‘’ğ‘› Â ğ‘“ ğ‘– < ğ‘˜2
1 Â ğ‘¤â„ğ‘’ğ‘› Â ğ‘“ ğ‘– â‰¥ ğ‘˜2 Â  Â 
 
(5) 
 
For both equations, the f(i) is the PDM, while the k1 and k2 
are threshold constants. The selected value for both k is 1 and 
this is according to Lumer et al. [11], where they have defined 
a distance or dissimilarity between objects in the space of 
object attributes: 
â€¢ 
If two objects are similar or identical, then the d(oi,oj) 
= 0 
â€¢ 
If two objects are not similar or identical, then the 
d(oi,oj) = 1 
IV. 
EXPERIMENTAL WORKS AND ANALYSIS OF RESULTS 
A. Experimentals Works 
The proposed algorithm has been tested using three 
different datasets; (1) Iris, (2) Glass and (3) Wood datasets (as 
shown in Table 1). The Iris dataset consists of 150 samples, 
which represent 3 species; sentosa, virginica and versicolor. 
While for Glassâ€™s data, the dataset has 216 samples and can 
be categorized into 2 categories; window and non-window 
glass. Moreover, the dataset for Wood that owned 5040 
samples of data comprises 52 tropical Wood species and this 
dataset can also be categorized according to the most 
dominant features; the pore size. There are three sizes of 
pores; small, medium-sized and large and the list of tropical 
wood species based on pores sizes is shown in Table 2. 
 
TABLE 1: THE DATASET USED TO EVALUATE THE ALGORITHM 
 
Datasets 
No of Samples 
Attributes 
Category 
Iris 
150 
4 
3 
Glass 
215 
9 
2 
Wood 
5040 
157 
3 
 
TABLE 2. TROPICAL WOOD SPECIES BASED ON PORES SIZES 
 
Pore Sizes 
Wood Species 
Small 
mataulat 
Medium-
Sized 
balau, bintangor, bitis, chengal, gerutu, giam 
jelutong, kapur, kasai, kekatong, keledang, keranji, kulim, 
machang, medang, melunak, perupok, redbalau 
Large 
bintangor, durian, gerutu2, kapur, kasai, keledang, keruing, 
machang, merantibakau, redbalau, rubberwood, sesendok 
 
110
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

The training processes are done repetitively and each 
dataset used different parameters; such as learning rate and 
number of epochs. These values have been personalized to 
each dataset and parameters values are obtained from the 
KSOM training process for performance comparison purposes. 
Table 3 shows the full parameters used for these three 
datasets. The selection of topological map for all datasets is 
based on Alhoniemi et al. [18], calculated using: 
ğ‘šğ‘ğ‘ğ‘ ğ‘–ğ‘§ğ‘’ = 5( Â ğ‘›ğ‘œ Â ğ‘œğ‘“ Â ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ Â !.!"#$%) 
(6) 
TABLE 3. FULL PARAMETERS USED FOR ALL DATASETS 
 
Parameters 
Datasets 
Iris 
Glass 
Wood 
Topological Map 
9x9 
(81 nodes) 
9x9 
(81 nodes)  
23x23 
(529 nodes) 
Sample number 
150 
216 
5040 
Categories/ 
Number of 
Clusters to be 
clustered  
 3  
(Setosa, 
Versicolor & 
Virginica) 
 2  
(Window-
Glass &  
Non-window 
Glass )  
 3  
(Small, 
Medium-sized 
& Large 
pores) 
B. Results 
As a result, it is obviously seen that the PKSOM has 
improved and refined the scatteredness of clustering data by 
increasing the density of the data in clusters. Most of the data 
are well clustered and closed to each in the same cluster even 
though there is a few dislocated data occurred and performed 
the overlapped clusters. Fig. 5 shows the results for Iris 
dataset using both algorithms. KSOM has clustered the Iris 
dataset into three big clusters (consists of a few clusters for 
every species) according to the species. However, the 
PKSOM has also clustered the dataset into three different 
clusters with high density of data in each cluster.  
 
 
 
(a) 
(b)  
Figure 5. Clustering Results using (a) KSOM  and (b) PKSOM for Iris  
Dataset 
Fig. 6 shows the clustering results performed by KSOM 
and PKSOM for Glass dataset, where the KSOM has 
separated the dataset into two categories: the window glass 
(on the upper part of the map) and non-window glass (on the 
lower part of the map). Conversely, the PKSOM has clearly 
separated the dataset into two different classes with a 
minimum number of cluster nodes and high density of data in 
each node. While for Wood dataset, KSOM has clustered the 
whole dataset into five big clusters: one cluster for small 
pores, two clusters for medium-sized pores and two clusters 
for large pores. However, the PKSOM has clustered the 
Wood dataset into three desired clusters, accurately. This is 
shown in Fig. 7.  
 
 
 
(a) 
(b) 
Figure 6. Clustering Results using (a) KSOM and (b) PKSOM for Glass Dataset 
 
 
(a)  
 
(b)  
Figure 7. Clustering Results using (a) KSOM and (b) PKSOM for Wood Dataset  
 
C
C
C
C
B
C
B
B
B
C
C
C
C
C
C
B
C
C
C
C
C
C
B
B
C
C
C
B
C
B
B
B
C
C
B
B
B
B
B
B
B
B
B
B
B
B
B
A
A
A
B
A
A
A
A
A
A
B
B
A
A
A
A
A
A
B
C
B
C
B
A
Virgnica
Setosa
Versicolor
A
A
A
A
A
A
A
A
A
A
B
B
B
A
A
A
A
A
B
B
B
A
A
A
A
A
B
B
B
A
A
A
A
B
B
A
A
A
A
A
B
B
A
A
A
A
A
B
B
A
A
A
A
A
A
B
A
A
A
A
A
A
A
B
B
B
A
A
A
A
A
A
B
A
B
A
B
A
WINDOW- 
GLASS
NON-WINDOW 
GLASS
b,c,e,
g,h,j,
k,l,m,
p,r,s
b,j,k,
m,p,s
a,b,k,
m,p,sb,k,m
,p
a,b,k,
m,p,r,
u
a,b,f,
k,m,p
,q,r,u
a,f,g,
k,q,r,t
,u,w q,u,w
,x
b,f,k,
m,p
m,p
b,m,r
a,b,k,
p,q,r,
u,w
a,g,q,
r,t,u,
w,x
u
a,b,f,
k,m,p
,r,u,wn,p,q,
r,t,w
q,u,w
,x
d
g,q,u,
w,x
g
g
g
g,s
e,g,h
g,s
d,e,g
b,p
g,h,s
b,s
d
p
h,s
g,o,p
b,d,h,
o,p,s,
v b,h,o,
s,v
d,o
n,v
d
e
h
o,s
d,n,o,
s
n,p,s
d,h,n,
s
p,s
n,p
n,s,x
g
b,d,e,
n,p,s,
v
n,o,x
s,x
x
p
n,o,s
b,d,o,
s
h
b,e,h,
s
d,n,p,
s,x
h
h,l
d,p,s,
v
n
h
l
c,l,p,
s
h
l,s
l
a,d,f,
g,h,i,,
l,n,q,
s,t,u
a,b,c,
e,f,g,i
,n,s,t,
v,x
a,b,i,j
,k,m,
n,u,v,
w,x
LARGE
MEDIUM-SIZED
SMALL
111
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

TABLE 4: FULL RESULT FOR ALL DATASET 
 
Dataset 
Map Size 
Cluster Usage 
(Cluster Node 
With data) 
Average Cluster 
Density of single 
node 
KSOM 
PKSOM 
KSOM 
PKSOM 
Iris 
9 x 9  
(81 cluster nodes) 
64 
4 
+2 
+38 
Glass 
9 x 9  
(81 cluster nodes) 
69 
4 
+3 
54 
Wood 
23 x 23  
(529 cluster nodes) 
399 
71 
+13 
+71 
 
Conclusively, as shown in Table 4, the KSOM has 
clustered Iris dataset into 64 clusters, Glass dataset into 69 
clusters and wood dataset into 399 clusters. The percentage of 
cluster usage (the cluster node that consists of data) for Iris 
dataset is 79.01%, Glass dataset is 85.19% and for wood 
dataset is 75.43%. While for proposed algorithm, PKSOM, 
the percentage of cluster usage for every dataset are 4.94% 
(Iris dataset), 4.94% (Glass dataset) and 23.92% (wood 
dataset), as shown in Fig. 8.  
Even though the KSOMâ€™s average of cluster usage is 
higher than the PKSOM, but the average cluster density of 
each cluster is lower compared to PKSOM (shown in Fig. 9). 
The PKSOM has the produced high average cluster density 
for every dataset; for Iris, the average cluster density is 38, 
Glass dataset average cluster density is 54 and for wood 
dataset is 70.99. However, the average cluster densities for 
every dataset produced by KSOM are very low; 2.34 (Iris 
dataset), 3.13 (Glass dataset) and 12.63 (Wood dataset). 
 
Figure 8. The Difference of Cluster Usage between KSOM and PKSOM 
 
Figure 9. The Average Cluster Density of single node 
 
 
For performance evaluation, we have compared the 
PKSOM result with previous methods; KohonAnts [22] and 
ABSOM [23] based on Iris results. Fig. 10 shows the results 
for KohonAnts, ABSOM and PKSOM. Both methods: 
KohonAnts and ABSOM have produced good classification 
result but the clustered data is scatteredly mapped into a two 
dimensional map. However, it is clearly seen that PKSOM 
has clustered the Iris dataset into three clusters, same as 
KohonAnts and ABSOM but with a minimum number of 
cluster usage.  This is proved that PKSOM has refined the 
scatteredness of the data, thus improved the separable 
boundary between clusters. 
 
 
(a)  
 
(b)  
 
(c)  
Figure 10. Clustering Results using (a) KohonAnts, (b) ABSOM and  
(c) PKSOM for Iris  Dataset 
V. 
CONCLUSION AND FUTURE WORK 
In conclusion, it is obviously seen and proven that the 
PKSOM has improved and refined the scatteredness of 
clustering data by increasing the density of the data in clusters. 
Most of the data for all datasets are well clustered and closed 
to each other in the same cluster. However, there are a few 
112
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

dislocated data that performed the overlapped clusters. These 
overlapped clusters consist of at least two different species or 
category in every overlapped cluster. According to the result, 
also, we can conclude that PKSOM also can deal with high 
dimensional dataset, such as wood dataset. Furthermore, we 
will perform further test using different high dimensional and 
sparse dataset to investigate the effectiveness of the proposed 
method. In addition, we will make some refinement to the 
algorithm in order to solve overlapped clusters problem and 
increase the classification accuracy. 
ACKNOWLEDGEMENT 
The authors would like to thank Ministry of Education 
Malaysia for funding this research project through a Research 
University Grant of Universiti Teknologi Malaysia (UTM), 
project titled "Dimension Reduction & Data Clustering for 
High 
Dimensional 
& 
Large 
Dataset" 
(04H40). 
Also, thanks to the Research Management Center (RMC) of 
UTM for providing an excellent research environment in 
which to complete this work 
REFERENCES 
[1] 
R. Rojas, â€œUnsupervised Learning and Clustering Algorithms,â€ in 
Neural Networks, Berlin: Springer Berlin Heidelberg, 1996, pp. 99â€“
121. 
[2] 
T.Kohonen, Self-Organizing Map. Springer  Series in Information 
Sciences 3rd Edition, Springer, 2000.  
[3] 
J. L. Giraudel and S. Lek, â€œA comparison of self-organizing map 
algorithm and some conventional statistical methods for ecological 
community ordination,â€ Ecol. Modell., vol. 146, no. 1â€“3, Dec. 2001, pp. 
329â€“339. 
[4] 
A. Dekker, â€œKohonen neural networks for optimal colour quantization,â€ 
Network-computation Neural Syst., vol. 5, no. 3, 1994, pp. 351â€“367.  
[5] 
V. Emamian, A. H. Tewfik, E. Shizcorningcom, and L. J. Jacobs, 
â€œRobust Clustering of Acoustic Emission Signals Using Neural 
Networks and Signal Subspace Projectionsâ€ , 2003, pp. 276â€“286. 
[6] 
T. Honkela, S. Kaski, K. Lagus, T. Kohonen, and N. Networks, 
â€œExploration of Full-Text Databases with Self-organizing Maps,â€ in 
IEEE International Conference on Neural Networks, 1996, pp. 56 â€“ 61. 
[7] 
W. Wan and D. Fraser, â€œM2dSOMAP: Clustering and Classification of 
Remotely Sensed Imagery by Combining Multiple Kohonen Self-
organizing Maps and Associative Memory,â€ no. 41, 1993, pp. 2464â€“
2467.  
[8] 
P. X. P. Xu, C.-H. C. C.-H. Chang, and A. Paplinski, â€œSelf-organizing 
topological tree for online vector quantization and data clustering.,â€ 
IEEE Trans. Syst. man Cybern. Part B Cybern. a Publ. IEEE Syst. Man 
Cybern. Soc., vol. 35, no. 3, 2005, pp. 515â€“526.  
[9] 
F. Forkan and S. M. Shamsuddin, â€œKohonen-Swarm Algorithm for 
Unstructured Data in Surface Reconstruction,â€ 2008 Fifth Int. Conf. 
Comput. Graph. Imaging Vis. , Aug. 2008, pp. 5â€“11.  
[10] Z. M. Zin, M. Khalid, E. Mesbahi, and R. Yusof, â€œData Clustering and 
Topology Preservation Using 3D Visualization of Self Organizing 
Maps,â€ vol. II, 2012, pp. 1â€“6.  
[11] E. D. Lumer and B. Faieta, â€œDiversity and Adaptation in Populations of 
Clustering Ants,â€ in The Third International Conference on Simulation 
of Adaptive Behavior: From a Animals to Animats 3, 1994, pp. 501â€“
508. 
[12] P. Vaijayanthi, A. M. Natarajan, and R. Murugadoss, â€œAnts for 
Document Clustering,â€ vol. 9, no. 2, pp. 493â€“499, 2012. 
[13] C. Tsang and S. Kwong, â€œAnt Colony Clustering and Feature 
Extraction for Anomaly Intrusion Detection,â€ in Swarm Intelligence in 
Data Mining, vol. 34, Springer Berlin Heidelberg,  2006, pp. 101â€“123. 
[14] J. Handl, J. Knowles, and M. Dorigo, â€œAnt-based clustering and 
topographic mapping.,â€ Artificial Life, vol. 12, no. 1, Jan. 2006, pp. 35â€“
62. 
[15] S. Bala, S. I. Ahson, and R.P.Agarwal, â€œA Pheromone Based Model for 
Ant Based Clustering,â€ Int. J. Adv. Comput. Sci. Appl., vol. 2, no. 11, 
2012, pp. 180â€“183. 
[16] O. A. M. Jafar and R. Sivakumar, â€œAnt-based Clustering Algorithms: A 
Brief Survey,â€ Int. J. Comput. Theory Eng., vol. 2, no. 5, 2010, pp. 
787â€“796. 
[17] J. L. Deneubourg, S.Goss, N.Franks, A.Sendova-Franks, C.Detrain, and 
L. Chretien, â€œThe Dynamics of Collective Sorting Robot-Like Ants and 
Ant-Like Robots,â€ in Proceeding of the First International Conference 
on Simulation of Adaptive Behavior on From Animals to Animats, 1990, 
pp. 356â€“363. 
[18] E. Alhoniemi et.al. (April,2014), Laboratory of Computer and 
Information 
Science, 
2005. 
[Online]. 
Available: 
http://www.cis.hut.fi/projects/somtoolbox/documentation/. 
[19] M. Mishra, â€œKohonen Self Organizing Map with Modified K-means 
clustering For High Dimensional Data Set,â€ vol. 2, no. 3, 2012, pp. 34â€“
39.  
[20] E. Mohebi and M. N. M. Sap, â€œAn Optimized Hybrid Kohonen Neural 
Network for Ambiguity Detection in Cluster Analysis Using Simulated 
Annealing,â€ in Enterprise Information Systems, Springer Berlin 
Heidelberg, 2009, pp. 389â€“401. 
[21] A. H. Abdullah, â€œAn Optimized Clustering Algorithm Using Genetic 
Algorithm and Rough set Theory based on Kohonen self organizing 
map,â€ Int. J. Comput. Sci. Inf. Secur., vol. 8, no. 4, 2010, pp. 39â€“44. 
[22] A. M. Mora, C. M. Fernandes, J. J. Merelo, V. Ramos, J. L. J. Laredo, 
and A. C. Rosa, â€œKohonAnts: A Self-Organizing Ant Algorithm for 
Clustering and Pattern Classification,â€ Artificial Life XI, 2008, pp. 428â€“
435. 
[23] C. C. Yang and S. Chi, â€œAn Ant-Based Self-Orgazining Feature Maps 
Algorithm,â€ in 5th Workshop On Self-Organizing Maps, 1997, pp. 65â€“
74. 
[24] W. Dai, S. Liu, and S. Liang, â€œAn Improved Ant Colony Optimization 
Cluster Algorithm Based on Swarm Intelligence,â€ J. Softw., vol. 4, no. 
4, 2009, pp. 299â€“306. 
[25] I. Michelakos, N. Mallios, E. Papageorgiou, and M. Vassilakopoulos, 
â€œAnt Colony Optimization and Data Mining,â€ in Next Generation Data 
Technologies for Collective Computational Intelligence, Springer 
Berlin Heidelberg, 2011, pp. 31â€“60. 
[26] Q. Chen and J. Mo, â€œOptimizing the Ant Clustering Model Based on K-
Means Algorithm,â€ 2009 WRI World Congr. Comput. Sci. Inf. Eng., no. 
4, 2009, pp. 699â€“702. 
[27] J. Chen, J. Sun, and Y. Chen, â€œA New Ant-based Clustering Algorithm 
on High Dimensional Data Space,â€ in Complex Systems Concurrent 
Engineering, Springer London, 2007, pp. 605â€“611. 
113
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

