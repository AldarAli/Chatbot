 
 
 
Designing a Low-Cost Early Diagnosis System Based on Deep Learning 
Monitoring the Development of Chronic Venous Disorder with Indirect Augmented Reality 
 
Huseyin A. Erdem 
Department of Computer Engineering, 
The Graduate School of Natural and 
Applied Sciences 
Dokuz Eylül University 
İzmir, Turkey 
e-mail: huseyinaerdem@gmail.com 
Işıl Erdem 
Department of Civil Engineering,  
The Graduate School  
İzmir Institute of Technology 
İzmir, Turkey 
e-mail: isilerdem@iyte.edu.tr 
Semih Utku 
Department of Computer Engineering, 
Faculty of Engineering 
Dokuz Eylül University 
İzmir, Turkey 
e-mail: semih@cs.deu.edu.tr
 
Abstract - Today's healthcare industry agrees that early 
diagnosis is just as important as the treatment of diseases. 
Academic and commercial studies carried out in this direction 
within the scope of early diagnosis contribute to a better 
quality of human life. Thanks to devices that offer early 
diagnosis, treatments can be started when the diseases are at 
their initial levels, and thus treatment costs can be reduced. 
However, most of these devices work with harmful rays and 
are generally used in hospital environments only by specialized 
staff. In this study, a low-cost early diagnosis system for home 
use, developed as part of a doctoral thesis, is introduced. The 
most important aspect of the proposed system that supports 
the convenience of home use is that it provides a harmless 
imaging with near-infrared light for the body. The system can 
detect both Class-1 (spider/telangiectasias vein) and Class-2 
(varicose vein) types of the clinical classification of Chronic 
Venous Disorder, with 4 different classes (in the form of two 
separate levels as beginner and advanced). In the system, 
which will monitor the development of the disorders in the 
superficial veins, the confidence values and positions of the 
detections in the images were determined by the You Only 
Look Once version-3 object detection algorithm used in deep 
learning applications. Confidence values of 0.90 and above 
were achieved in the object detection experiments performed 
with Class-1 and Class-2 type artificial patterns. According to 
the test results obtained, the system was able to detect Chronic 
Venous Disorder patterns with the values of Accuracy Rate (1), 
Misclassification Rate (0), Precision (1), Prevalence (0.5) and 
F-Score (1). The confidence values and positions of the patterns 
detected in the study are presented to the user/physician with 
the help of indirect augmented reality visuals as an e-health 
application that will support a long-term monitoring system. In 
this way, the beginner and the advanced levels of venous 
disorder can be monitored by before and after video visuals. 
Keywords - near-infrared light; chronic venous disorder; 
deep learning; YOLOv3; indirect augmented reality. 
I. 
INTRODUCTION 
Especially in the 21st century, in the academic and 
commercial sector, advances in health technologies have 
increased at an unstoppable pace. While some of these 
advances target treatments after the disease occurs, others 
aim at early diagnosis by making use of various imaging 
techniques that are harmful/harmless to the body. This study 
is an extended version of the study [1] investigating the 
detection of varicose vein development. In this version, the 
current 
system 
designed 
for 
vascular 
degeneration 
monitoring within the scope of the doctoral thesis [2] has 
been restructured in such a way that it can detect 2 different 
types (spider_vein and varicose_vein) of vein enlargement at 
2 levels (beginner and advanced) in more detail. The current 
system [2] consists of 6-phases. In the Imaging Technique 
Phase, video recordings of superficial veins are obtained 
using a low-cost (65 dollars) near-infrared camera. With the 
Digital Image Pre-Processing Phase, these recordings are 
converted into images and enhancements are made on the 
raw images. These first two phases are detailed in the study 
[3]. In the enhanced images, the discontinuous vascular 
structures caused by illumination are removed to a certain 
level in the Digital Image Post-Processing Phase. In the 
Classification Phase, the classes of vascular degeneration are 
determined by using Convolutional Neural Networks and 
Hybrid Decision-Making Algorithm (first introduced in the 
study [4]), and the positions of these degenerations are 
determined in the Object Detection Phase. In the last phase, 
the Augmented Reality Phase, the object detection results 
obtained are superimposed on the raw images, and the video 
visuals are created and presented to the user and his/her 
physician. All phases of this system are described in the 
study [4] specifically for vascular narrowing (stenosis_vein 
class). 
Medical imaging devices are one of the primary auxiliary 
methods used in hospitals to diagnose different diseases. The 
devices used in this context work on the basis of visualizing 
the area to be viewed with light or sound waves. Imaging 
with light is carried out by utilizing different wavelengths in 
the electromagnetic spectrum. Medical imaging devices 
currently in use are classified according to “the body tissue 
they can monitor” and “the effects of the light used to 
illuminate the area of interest on the body”. While the X-Ray 
device, which emits harmful rays (ionizing radiation) to the 
body, is predominantly used in the imaging of bone tissue 
and abdominal diseases, Computed Tomography is used for 
imaging both bone tissue and internal organs [5]-[7]. In 
addition, Magnetic Resonance Imaging provides imaging of 
tissues with magnetic waves, whereas Ultrasound uses high-
20
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
frequency sound waves for imaging [5][6]. Computed 
Tomography or Magnetic Resonance Imaging techniques 
can be used for vascular imaging [6]. However, both the 
negative effects of these devices on human health and their 
high costs limit their use to hospital environments only. 
Furthermore, even if the ultrasound device, which provides 
visualization of blood flow [5][6], is harmless to the body, it 
has a high cost and is generally used and interpreted by 
radiologists in hospitals. 
Although technology advances at a dizzying pace, many 
lives are still lost due to late detection of diseases that can be 
easily cured if detected earlier. Despite efforts to raise 
awareness of early diagnosis of all kinds of diseases, today’s 
people do not give up the habit of going to the doctor after 
the onset of the disease and often neglect routine checkups. 
In these omissions, the concern of triggering other diseases 
by imaging devices working with harmful rays to the body 
during controls has a large share. However, technological 
techniques currently under development offer the possibility 
of producing safe alternatives for the early detection of some 
diseases. Among them, we can include, the detection of 
varicose veins, which is one of the most common venous 
disorders and caused by the enlargement of veins close to the 
surface of the skin (i.e., superficial veins), can be counted. 
Near-infrared light, which is a type of light that is harmless 
to the body, is used in hospitals within the scope of 
superficial vein imaging, especially during vascular access 
procedures.  
The main advantage of near-infrared light in the scope of 
vein imaging is that photons of this type of light can be 
absorbed by Hemoglobin molecules in the veins [8]-[10]. 
The Hemoglobin (Hb) molecule is a blood protein [11] that 
is found in the vascular system and is in charge of carrying 
Oxygen to the tissues. While the molecule carries Oxygen to 
the tissues and organs by passing through the arteries in the 
form of oxygenated Hb (HbO2), it returns to the heart 
through the veins as de-oxygenated Hb (Hb) after leaving its 
Oxygen. These molecules transported in a recirculation 
system are more sensitive to near-infrared light in the range 
of 800-900 nanometers (nm) as HbO2 and in the range of 
700-800 nm as Hb [12]. One of the tricks in near-infrared 
imaging is to choose the wavelength of the near-infrared 
light that illuminates the tissue and the wavelength of the 
camera that will take the image in harmony. In this way, the 
veins in the tissue area illuminated with near-infrared light of 
a certain wavelength (in the studies carried out in [9]-[13], a 
wavelength of 850 nm was used, which usually gives 
optimum results) can be viewed in a better quality with a 
camera having the same wavelength filter. Superficial veins 
(thanks to the Hb molecules inside) absorb the near-infrared 
light (700-900 nm range of the electromagnetic spectrum) 
that penetrates 3-5 millimeters, passing through the skin and 
fat layer in the illuminated tissue area, and creates dark areas 
in the images. These dark areas in the images represent the 
veins, while the bright areas represent the surrounding 
tissues. In this way, the visualization of the superficial veins 
can be easily performed with only the light source and the 
camera (even an ordinary camera can be turned into a simple 
near-infrared camera by changing the filter on it). This 
imaging method is evaluated within the scope of 
Spectroscopy (acquiring knowledge of the structure of matter 
interacting with a particular type of light [14]). Although the 
dark areas in near-infrared spectroscopy images more or less 
allow the visualization of veins, they are not of sufficient 
quality for further analysis. For this, by applying digital 
image processing filters on the obtained near-infrared image, 
some improvements can be made on the image. The 6-phase 
system used in the study performs image enhancement 
processes in two separate phases. In the Digital Image Pre-
Processing Phase, two-coloured binary images consisting of 
black (vein) and white (surrounding tissues) are obtained 
from raw videos (video recordings are taken during tissue 
imaging) with image processing filters and methods. In this 
way, it is ensured that the edges of the veins are sharpened, 
only the relevant vein patterns are revealed by eliminating 
the surrounding tissues and the noise in the images is 
removed. In the veins of these images, the discontinuous 
structures caused by the illumination have been eliminated 
up to a certain level with the Digital Image Post-Processing 
Phase (Speeded Up Robust Features (SURF) Local Feature 
Detector Algorithm [15] is used). Processed near-infrared 
vascular images can be used for many different purposes 
from disease pre-diagnosis to biometric recognition 
[3][4][16][17]. For these purposes, deep learning techniques 
(such as classification or object detection/recognition) are 
applied on images. The Classification Phase of the 6-phase 
system makes it possible to determine to which class the 
observed tissue belongs. At this phase, video recordings of 
the first viewing period are used to introduce the classes to 
be used in the system. Within the scope of monitoring, the 
system checks the belongingness of the images obtained in 
the following periods (can be set as day, week, month) to one 
of the defined classes (as many as the number of different 
tissue regions the user views in the first period). If the classes 
of new images can be determined, the Object Detection 
Phase is used to detect vascular degenerations from these 
images. In this study, the vein enlargement patterns in the 
images are detected by the object detection algorithm, too. 
The system, which was prepared within the scope of the 
doctoral study (near-infrared images of the right and left 
forearms were used) and which enables the superficial veins 
(in the near-infrared images) to be visualized as an e-health 
application in the home environment, was re-trained in the 
study [1] to monitor the vein enlargement. In order to 
increase the low confidence values encountered in object 
detection in the study [1] and to perform a more sensitive 
detection, the system was re-trained with 4 different classes 
in this study. In addition, in the study [1], two artificial 
datasets (representing vein enlargements) created to be used 
in the training and testing processes of the You Only Look 
Once version-3 (YOLOv3) object detection algorithm were 
re-arranged. In this study, in addition to the processes 
described in the study [1], the Augmented Reality Phase, 
which is the 6th phase of the system, was also included in the 
study. In this way, it has been ensured that the detections and 
developments of vein enlargement can be visually presented 
to the user and his/her physician within the scope of Indirect 
Augmented Reality. 
21
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
In Section II, both object detection studies within the 
scope of the YOLOv3 object detection algorithm and the 
virtual environment literature within the scope of Indirect 
Augmented Reality were examined. In Section III, Chronic 
Venous Disorder (CVD) types were introduced and the re-
trained YOLOv3 object detection algorithm was explained 
so that the system can detect vein enlargement in the Object 
Detection Phase. How the datasets were created, which 
classes were defined and the results obtained as a result of 
the experiments were also stated in this section. In Section 
IV, how video-based images are arranged for the Augmented 
Reality Phase is detailed. In the last section, the study is 
discussed in general terms. 
II. 
RELATED WORK  
Classification processes are used to determine the class to 
which an image belongs. Traditional Convolutional Neural 
Networks can detect the belongingness of the inputs in the 
form of images, audio or video to defined classes. However, 
these networks do not give the position information of the 
objects they detect in the image. For this, object detection 
algorithms (Single Shot Detector SSD [18], Fast/Faster 
Region-based Convolutional Neural Networks [19]-[21], 
You Only Look Once YOLO [22]) are used. The YOLO 
algorithm is frequently preferred in studies especially 
because it can perform real-time object detection faster. In 
this context, YOLO version-3 [23] is included in this study 
because it can detect small objects as well [24]. 
In the study [25], which provides recognition of human 
movements and classification with location detection, the 
YOLO algorithm was trained using a total of 10 classes, 
including actions such as conversation between two people, 
exiting/entering the room, and handshaking. Images were 
processed as video streams and action recognition was 
performed with a small number of images (even a single 
image was sufficient in some cases). 
In the study [26], which aims to provide real-time social 
distance detection in public areas within the scope of the 
Covid-19 pandemic, close proximity of more than 2 meters 
was detected by using the YOLOv3 object detection 
algorithm and marked with a red bounding box. 
In the study [27], which performed fire detection as a 
real-time video-based tracking application, the tiny-YOLOv3 
algorithm [28], a lighter version of the YOLOv3 object 
detection algorithm, was used. Experimental results obtained 
using 5000 training images and 5000 test images in the study 
confirmed the effectiveness of the proposed system. 
In the study [29] investigating water consumption 
monitoring, the consumption indicator on the water meter 
counter images were determined by using the YOLOv4 
algorithm [30], and the image was processed through the 
image processing stage and digit recognition was performed 
by converting it to black and white format. In addition, 
within the scope of the system developed in the study, a 
mobile application was made available. The study had an 
object detection accuracy of 98%. 
In the study [31], which aimed to detect suspicious breast 
lesions on digital mammography images, early diagnosis and 
classification was performed using the YOLO algorithm. In 
the study, 4 different classes were used as mass, 
calcification, architectural distortion and normal. 
In the study [32], in which the YOLO algorithm was used 
for the detection of Diapetic retinopathy, which manifests 
with fundus legions at its early level, fundus legions were 
defined 
in 4 different classes as 
microaneurysms, 
hemorrhages, hard exudates and soft exudates. 
In the study [33], which aims to detect Pediatric 
Pneumonia, Convolutional Neural Networks, which use X-
Ray images as input, were used and it was aimed to 
accelerate the diagnosis decision process in this way. It was 
stated that the classification accuracy of the study for 3 
different classes as normal, viral pneumonia, and bacterial 
pneumonia was 90.71%. 
In the study [34], which aims to detect chest abnormality 
with deep learning, it had been underlined that doctors could 
be provided with a faster decision-making opportunity in 
diagnosing. In the study, Computed Tomography scan image 
inputs, a dataset of 18000 images (15000 train / 3000 test) 
labelled by radiologists, 14 different classes (Atelectasis, 
Calcification, 
Cardiomegaly, 
Consolidation, 
ILD, 
Infiltration, Lung Opacity, Nodule/Mass, Other lesions, 
Pleural 
effusion, 
Pleural 
thickening, 
Pneumothorax, 
Pulmonary fibrosis, No finding observation) and the 
YOLOv5 algorithm [35] were used. 
In the system used in this study, Indirect Augmented 
Reality was used to present the CVD detection results 
obtained with the YOLOv3 object detection algorithm and 
their position in the image to the user/physician. Indirect 
Augmented Reality is a video-based virtual technology that 
has two separate modes, offline (creation of video visuals 
where virtual material is superimposed on real material) and 
online (displaying the superimposed video to the user in its 
real-world location). Studies in the literature on the concept 
are very limited. 
In the study [36], which introduced the concept of 
Indirect Augmented Reality to the literature, previously 
recorded panoramic video images were used in two separate 
case studies within the scope of location finding and it was 
emphasized that the concept was useful for outdoor use. 
In the study [37], in which it was stated that the 
alignment problems between virtual and real material could 
be solved with Indirect Augmented Reality, the participants 
gave positive feedback according to the results of the trials of 
virtual applications introducing Rome. 
In the study [38], which combines both traditional 
Augmented Reality and Indirect Augmented Reality within 
the scope of the Casa Batllo museum promotion application 
in Spain, it was underlined that Indirect Augmented Reality 
could also be used as an indoor application. The application 
was also covered under the mobile-based Augmented Reality 
title. The application automatically switches between 
traditional and Indirect Augmented Reality according to the 
user's location. According to the results obtained in the 
study, the application can easily reflect the life of the 
beginning of the 20th century to the user. 
In the study [39], that aims to eliminate temporal 
discrepancies (e.g., if online registration is performed at 
night while offline images are created with daytime images), 
22
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
which are the negative aspects of Indirect Augmented 
Reality applications especially in outdoor use, samples taken 
from different environmental conditions at different times 
were added to the dataset. For online registration, a selection 
was made among the images in the dataset according to the 
histogram similarity. 
In the study [40], which presents the Omaha Beach 
landing in an Indirect Augmented Reality environment, an 
environment was prepared in such a way that panoramic 
video images containing war scenes and narrations would be 
activated when the user was within 200 meters of the 
relevant region. 
III. 
VEIN ENLARGEMENT DETECTION: DEEP LEARNING 
AND THE YOLOV3 OBJECT DETECTION ALGORITHM  
Near-infrared imaging system is basically examined in 
two parts as hardware and software. While the hardware part 
is about the wavelength of the light source, Light Emitting 
Diode (LED) placements and camera features, the software 
part covers the extraction of vein patterns by making the 
veins in the obtained near-infrared images more prominent 
via digital image processing techniques. In this way, 
superficial veins can be visualized. The hardware part and 
digital image processing steps of the doctoral study were 
introduced in [3]. Also, the presentation of narrowing 
detections in processed images (using the YOLOv3 object 
detection algorithm with a single-class as stenosis_vein) to 
patient and his/her physician as a video-based Indirect 
Augmented Reality environment was explained in [4]. The 
2-class 
vein 
enlargement 
patterns 
(spider_vein 
and 
varicose_vein) in the superficial vein images discussed in the 
study [1] were detailed as a total of 4 classes, with each 2 
class represented by 2 levels (beginner and advanced) in this 
study. 
When the superficial vein valves do not work properly, 
blood accumulations occur, and as a result, the veins expand, 
elongate, and form twisted folds, resulting in varicose veins 
[41][42]. Although varicose veins are most commonly 
observed in leg veins (which are under more pressure than 
other veins [41]), varicose veins can be encountered in any 
part of the body [43]. In general, however, the development 
of venous disorder in hand veins does not give results as 
dramatic as in leg veins. Chronic Venous Disorders (CVDs) 
affecting millions of people worldwide are caused by 
morphological and functional abnormalities of the venous 
system [44][45]. Risk factors such as heredity (family 
history, height), lifestyle (long term standing/sitting, 
occupation, smoking), gain (age, pregnancy, obesity, deep 
vein thrombosis) or hormones (female gender, progesterone) 
can lead to venous disorders, such as vein enlargement 
[46][47]. CVD clinical types are defined by the CEAP 
(Clinical, Etiological, Anatomical and Pathophysiological 
[44]) classification system (letter C represents “Class”): C0 
(no visible signs of venous disease), C1 (visible veins, 
spider/telangiectasias veins), C2 (varicose veins), C3 
(swelling/edema), C4 (changes to skin quality), C5 (healed 
ulceration), and C6 (active ulceration) [45][48]. CVD is 
often overlooked at its early levels [44]. In case of early 
diagnosis, advanced symptoms such as edema, skin changes 
or leg ulcers can be alleviated with the support of lifestyle 
changes [45][47]. It was determined that the incidence of 
venous disorders in adults in urban and rural areas of Bonn 
in Germany was 59% for telangiectasias vein and 14% for 
varicose vein, respectively [45]. 
As most superficial veins, varicose veins are also not 
easily visible to the naked eye, so near-infrared light is used 
to visualize these veins [49]. In this study, the hand vein 
dataset (an example image from the dataset and its processed 
version are given in Fig. 1) obtained in the study [1] by using 
the superficial vein monitoring system was used for the 
experiments of the YOLOv3 object detection algorithm, 
which was re-trained to detect the beginner and advanced 
levels of CVD in the C1 (spider/telangiectasias veins) and 
C2 (varicose veins) types. 
 
The Near-Infrared Raw Image  
 
 
(a) 
Region of Interest  
 
 
(b) 
Processed Image 
 
 
(c) 
Figure 1.  The near-infrared vein image. (a) The near-infrared raw image 
of hand dorsum. (b) Region of interest, containing only the veins to be 
examined. (c) Vein patterns obtained by digital image processing. [1] 
The ability of the computer to make a determination (i.e., 
classification) on similar new data by using previously 
learned data is a decision-making process. Decision making 
is unique to biological creatures. However, nowadays 
computers can also "imitate" this process in various ways. 
This imitation is provided by Artificial Intelligence (the 
ability 
to 
imitate 
human-specific 
abilities 
such 
as 
recognition, classification, problem solving and learning by 
machines using various inputs such as image, sound and 
23
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
signal [50]). In Machine Learning, which is a sub-branch of 
Artificial Intelligence, rule sets defined with expert 
knowledge (human factor) are needed to provide learning or 
to determine the distinctive features of the patterns [51]. In 
this way, various classification, regression or clustering 
operations (disease classification [52], stock forecasting [53] 
or airport passenger forecasting [54]) can be performed [51]. 
In Deep Learning, a sub-branch of Machine Learning, with 
the help of Artificial Neural Networks, features that cannot 
be detected by humans can be detected without any human 
intervention. In this way, learning is carried out via Deep 
Learning Algorithms by itself when only training and test 
data are given to the system as input. With Deep Learning, 
operations such as object recognition, speech recognition or 
object detection can be provided [55]. 
The YOLOv3 is a Deep Learning Algorithm that 
performs 
object 
detection. 
With 
Object 
Detection 
Algorithms, training can be performed for multiple objects 
(up to 80 classes [23] placed at certain positions on the 
image) that can represent different classes. The YOLOv3 
object detection algorithm marks the positions of the objects 
detected onto the image with bounding boxes. In addition, 
the name and detection rate (confidence value is shown 
between 0.00 and 1.00 in the study) of the class with the 
highest probability to which the object may belong are 
printed on the box. 
In this study, the YOLOv3 object detection algorithm 
was used for CVD detection in vein patterns obtained by 
image processing steps. The developed near-infrared 
imaging system was re-trained in this study to detect 2 
different CVD types (C1 and C2) as 4 classes 
(spider_beginner, spider_advanced, varicose_beginner and 
varicose_advanced). 
There is currently no venous disorder dataset consisting 
of near-infrared images, available to the public. For this, in 
this study, a new 4-class (to detect vascular degenerations in 
more detail) training dataset was prepared by adding 
artificial patterns on to near-infrared images (the dataset, 
which is also used in the study [1], was created by the 
method of obtaining images from video recordings described 
in the study [4]). The new dataset used in this study was 
prepared with images that each contain 10 spider_beginner 
and 10 spider_advanced artificial patterns, and each 5 
varicose_beginner 
and 
5 
varicose_advanced 
artificial 
patterns (the dataset in the study [1] contains 150 near-
infrared images, each containing 5 spider_vein and 5 
varicose_vein patterns). Furthermore, 50 additional images 
were created (as in the study [1]) from the existing images by 
data augmentation methods (10-degree rotation, 30-degree 
rotation, mirroring, noise addition and downscaling). In this 
way, a (4-class) training dataset with 6000 artificial patterns 
was obtained (in the study [1], a (2-class) dataset with 2000 
artifacts including 1000 spider_vein and 1000 varicose_vein 
patterns was used). The patterns in the images were labelled 
with the free (under General Public License version 3) 
makesense [56] web-based application. 
A second dataset consisting of 600 images containing 
artificial vein enlargement patterns was prepared for the test 
process to be carried out after the trainings (in the study [1] 
the test dataset consists of 300 images). The test dataset was 
created by adding only a single pattern (artificial patterns not 
used in the training) to each image in random rotations and 
positions (preserving a certain figural format, 150 patterns 
for each class) (the dataset used in [1] was created by adding 
only one spider_vein or varicose_vein pattern). In this way, a 
test dataset of 600 images with 4 classes containing 150 
patterns was obtained (in the study [1], a test dataset of 300 
images in total was obtained, with 150 test images 
containing spider_vein class and 150 test images containing 
varicose_vein class). The confusion matrix of the object 
detection results of the YOLOv3 object detection algorithm, 
obtained by using the test dataset is given in Table I for the 
study [1] (for comparison purposes) and in Table II for this 
study. As can be seen from the matrix, all of the searched 
objects 
(spider_beginner, 
spider_advanced, 
varicose_beginner and varicose_advanced patterns) in the 
test images were detected correctly (for the study [1], all of 
the spider_vein and varicose_vein patterns were detected 
correctly). The developed system can detect CVD patterns in 
C1 and C2 types with Accuracy Rate (1), Misclassification 
Rate (0), Precision (1), Prevalence (0.5) and F-Score (1) 
values (similar to the study [1]). 10 sample patterns for each 
class with the YOLOv3 object detection algorithm 
confidence values marked are given in Table III. 
 
TABLE I.  
THE CONFUSION MATRIX OF THE YOLOV3 OBJECT 
DETECTION ALGORITHM RESULTS OBTAINED WITH SPIDER_VEIN AND 
VARICOSE_VEIN CLASSES [1] 
n=300 
Predicted Class 
spider_vein  
(Positive) 
varicose_vein  
(Negative) 
Actual 
Class 
spider_vein  
(Positive) 
True Positive=150 
False Negative=0 
varicose_vein  
(Negative) 
False Positive=0 
True Negative=150 
 
TABLE II.  
THE CONFUSION MATRIX OF THE YOLOV3 OBJECT 
DETECTION ALGORITHM RESULTS OBTAINED WITH SPIDER_BEGINNER, 
SPIDER_ADVANCED, VARICOSE_BEGINNER AND VARICOSE_ADVANCED 
CLASSES 
n=600 
Predicted Class 
spider 
beginner 
spider  
advanced 
varicose 
beginner 
varicose 
advanced 
unclassified 
Actual 
Class 
spider 
beginner 
150 
0 
0 
0 
0 
spider 
advanced 
0 
150 
0 
0 
0 
varicose 
beginner 
0 
0 
150 
0 
0 
varicose 
advanced 
0 
0 
0 
150 
0 
 
 
 
 
 
 
 
24
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
TABLE III.  
CONFIDENCE VALUE RESULTS OF THE YOLOV3 OBJECT DETECTION ALGORITHM FOR SPIDER_BEGINNER, SPIDER_ADVANCED, 
VARICOSE_BEGINNER AND VARICOSE_ADVANCED CLASS PATTERNS 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
# 
spider_beginner 
spider_advanced 
varicose_beginner 
varicose_advanced 
1 
 
 
 
 
2 
 
 
 
 
3 
 
 
 
 
4 
 
 
 
 
5 
 
 
 
 
6 
 
 
 
 
7 
 
 
 
 
8 
 
 
 
 
9 
 
 
 
 
10 
 
 
 
 
 
25
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
The venous disorder detection confidence values of the 
YOLOv3 object detection algorithm performed with sample 
images of the C1 type of CVD are shown in Fig. 2 (a, b, c) 
for spider_vein (0.99) [1], spider_beginner (0.99) and 
spider_advanced (1.00). Also, confidence values for C2 type 
are shown in Fig. 3 (a, b, c) for varicose_vein (0.90) [1], 
varicose_beginner (0.98) and varicose_advanced (0.98). 
Although all classes in the test images were predicted 
correctly in the study [1], the YOLOv3 object detection 
algorithm had a lower confidence value for some patterns. 
As stated in Table IV, in the study [1], among 150 images 
containing spider_vein patterns, 130 had a confidence value 
in the range of 0.95-1.00, 13 in the range of 0.90-0.94, 6 in 
the range of 0.80-0.89, and 1 of them was determined as 
0.32. When the pattern with the confidence value of 0.32 is 
examined, it is determined that it is not much different from 
the patterns in the training dataset (mostly large-sized 
patterns were used) or other test patterns, but it is smaller in 
size, as can be seen in Fig. 4 (a). It was evaluated that this 
situation may result in a low confidence value.  
 
 
Figure 2.  The YOLOv3 object detection algorithm test process result 
image samples for C1 type CVD patterns. (a) 0.99 confidence valued result 
for spider_vein class [1]. (b) 0.99 confidence valued result for 
spider_beginner class. (c) 1.00 confidence valued result for 
spider_advanced class. 
 
The YOLOv3 Object Detection Algorithm Detection Rate of C2 
Type CVD Pattern (Training with 2-Classes) 
 
(a) 
 
The YOLOv3 Object Detection Algorithm Detection Rate of C2 
Type CVD Patterns (Training with 4-Classes) 
 
 
 (b) 
 
 
(c) 
 
Figure 3.  The YOLOv3 object detection algorithm test process result 
image samples for C2 type CVD patterns. (a) 0.90 confidence valued result 
for varicose_vein class [1]. (b) 0.98 confidence valued result for 
varicose_beginner class. (c) 0.98 confidence valued result for 
varicose_advanced class. 
 
The YOLOv3 Object Detection Algorithm Detection Rate of C1 
Type CVD Pattern (Training with 2-Classes) 
 
(a) 
 
The YOLOv3 Object Detection Algorithm Detection Rate of C1 
Type CVD Patterns (Training with 4-Classes) 
 
 
(b) 
 
 
(c) 
 
 
 
 
 
 
 
26
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
The YOLOv3 Object Detection Algorithm Detection Rates of 
Re-evaluated C1 Type CVD Patterns 
 
 
 
 
                          (a)                                                  
 
 
 
 
(b) 
Figure 4.  C1 type CVD patterns shown in accordance with their actual 
dimensions. (a) Patterns with confidence values of 0.32 and 1.00 belonging 
to the spider_vein class [1]. (b) Patterns with confidence values of 0.96 and 
0.98 belonging to the spider_beginner and spider_advanced classes, 
respectively. 
In this study, for control purposes, the CVD artificial 
patterns that were detected with low confidence values in the 
study [1] were re-examined. For this reason, spider_beginner 
patterns (smaller sized and less branched spider_vein 
patterns to represent the beginner level of C1) were added to 
the dataset in this study. Spider_vein detection with a 
confidence value of 0.32 obtained in [1] was determined as a 
spider_beginner with a confidence value of 0.96 according to 
the results of the YOLOv3 object detection algorithm trained 
with 4 classes, as in Fig. 4 (b). This shows that the system 
can also detect vascular degenerations in smaller sizes (at the 
beginner level). 
As shown in Table IV, again in the study [1], among the 
150 images containing varicose_vein pattern, 126 had a 
confidence value in the range of 0.95-1.00, 6 had a range of 
0.90-0.94, 11 had a range of 0.80-0.89, and 7 had a range of 
0.79-0.30. When the 7 patterns with the lowest confidence 
values are examined, it is determined that these patterns are 
slightly different (U-shaped, twisted) from the patterns in the 
training dataset or other test patterns (mostly linear line 
patterns were used) which can be seen in Fig. 5 (a). It was 
evaluated that this condition may lead to a low confidence 
value. Therefore, in this study, varicose_advanced patterns 
(U-shaped and twisted patterns of varicose_veins to 
represent the advanced level of C2) were added to the 
dataset. According to the results of the YOLOv3 object 
detection algorithm trained with 4 classes, the varicose_vein 
pattern with a confidence value of 0.30 obtained in the study 
[1] was determined as the varicose_advanced pattern with a 
confidence value of 0.94 as in Fig. 5 (b). This indicates that 
the system can also detect vascular degenerations when the 
patterns become more twisted (for monitoring progress at 
advanced levels). 
Since small-sized spider_vein patterns represent the early 
levels of CVD (spider_beginner) and varicose_vein patterns 
can also twist and fold (may not follow a linear line) over 
time (varicose_advanced), such patterns are important in the 
scope of detection system. Therefore, in order to overcome 
the low confidence values of spider_vein and varicose_vein 
classes described in the study [1], new smaller-sized patterns 
and also new U-shaped patterns were added to the training 
dataset with different rotations.  
The YOLOv3 Object Detection Algorithm Detection Rates of 
Re-evaluated C2 Type CVD Patterns 
 
 
 
 
(a)                                                  
 
  
 
 
(b) 
Figure 5.  C2 type CVD patterns shown in accordance with their actual 
dimensions. (a) Patterns with confidence values of 1.00 and 0.30 belonging 
to the varicose_vein class [1]. (b) Patterns with confidence values of 1.00 
and 0.94 belonging to the varicose_beginner and varicose_advanced 
classes, respectively. 
Examining Table V showing the results obtained, it can 
be seen that out of 150 images containing spider_beginner 
pattern, 68 of them had confidence values in the range of 
0.95-1.00, 28 of them in the range of 0.90-0.94, 43 of them 
in the range of 0.80-0.89, and 11 of them in the range of 
0.79-0.55. For the spider_advanced pattern, the confidence 
values for 113 images were found to be in the range of 0.95-
1.00, 16 of them between 0.90-0.94, 12 of them between 
0.80-0.89, and 9 of them between 0.79-0.63. Looking at 
Table V for the varicose_beginner pattern, 135 confidence 
values were found between 0.95-1.00, 10 between 0.90-0.94, 
3 between 0.80-0.89, and 2 confidence values between 0.79-
0.63. For the varicose_advanced pattern, 145 confidence 
values were found between 0.95-1.00, 3 between 0.90-0.94, 
1 between 0.80-0.89, and 1 between 0.79-0.63. The number 
of CVD types’ confidence results obtained in this study, 
especially in the range of 0.95-1.00, is greater for the C2 
(varicose_vein) type (280 confidence values) than for the C1 
(spider_vein) type (181 confidence values). This shows that 
the system can detect varicose levels with higher confidence. 
In addition, as it can be seen from Table II, the fact that no 
misclassification has been made in the system proves that the 
system works extremely effectively in the detection of both 
CVD types and progress. 
The confidence value arithmetic mean (0.973) of the 
single-class C1 type (spider_vein) in Table IV is slightly 
higher than the arithmetic mean (0.912 and 0.953) of the 
two-class C1 type (spider_beginner and spider_advanced) in 
Table V. Since, the patterns of spider_vein and varicose_vein 
are very different from each other in shape, it provides an 
easier distinction (130 spider_vein patterns were correctly 
classified with a confidence value in the range of 0.95-1.00, 
which increased the arithmetic mean). This less significant 
decrease in the test results of the system in this study is a 
result of the distribution of the confidence values to different 
detection intervals (0.95-1.00, 0.90-0.94, 0.80-0.89 and 0.79-
0.00) when the C1 type is divided into two separate classes 
with basically similar pattern formats. On the other hand, the 
fact that the minimum confidence values (0.55 and 0.63) in 
27
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
Table V for C1 type are higher than those in Table IV (0.32), 
it draws attention to the fact that a more precise classification 
decision can be made. 
 
TABLE IV.  
RESULTS OF CONFIDENCE VALUES FOR THE YOLOV3 
OBJECT DETECTION ALGORITHM TRAINED WITH TWO CLASSES [1] 
n=300 (150 for each class) 
C1: 
spider 
vein 
C2: 
varicose 
vein 
Arithmetic Mean 
0.973 
0.955 
Minimum 
0.32 
0.30 
Maximum 
1.00 
1.00 
[0.95-1.00] Range  
130 
126 
[0.90-0.94] Range 
13 
6 
[0.80-0.89] Range 
6 
11 
[0.79-0.00] Range 
1 
7 
 
TABLE V.  
RESULTS OF CONFIDENCE VALUES FOR THE YOLOV3 
OBJECT DETECTION ALGORITHM TRAINED WITH FOUR CLASSES 
n=600 (150 for each class) 
C1: 
spider 
beginner 
C1: 
spider 
advanced 
C2: 
varicose 
beginner 
C2: 
varicose 
advanced 
Arithmetic Mean 
0.912 
0.953 
0.982 
0.993 
Minimum 
0.55 
0.63 
0.67 
0.79 
Maximum 
1.00 
1.00 
1.00 
1.00 
[0.95-1.00] Range 
68 
113 
135 
145 
[0.90-0.94] Range 
28 
16 
10 
3 
[0.80-0.89] Range 
43 
12 
3 
1 
[0.79-0.00] Range 
11 
9 
2 
1 
 
Looking at Table IV and Table V for C2 type CVD 
(varicose_vein), it can be seen that the two-class arithmetic 
mean results (0.982 and 0.993) are higher than the one-class 
result (0.955). It was evaluated that the introduction of U-
shaped patterns to the system (due to the use of flat shaped 
patterns for training, twists are detected with lower 
confidence values in Table IV) contributed to this situation. 
In addition, as can be seen from Table V, the confidence 
value determinations for both levels of the C2 type are 
especially concentrated in the range of 0.95-1.00. In addition, 
the minimum confidence values (0.67 and 0.79) in Table V 
for C2 type were higher than those in Table IV (0.30). These 
cases also reinforce that the system can make a classification 
decision with higher precision. 
IV. 
INDIRECT AUGMENTED REALITY 
Virtual technologies are used in every imaginable field, 
especially health, education, construction, agriculture, 
tourism and entertainment. Although these technologies are 
known as virtual reality with the most popular definition 
among the public, they are called virtual environments in 
academic context [57]. The goal of virtual environments is to 
create a perception of reality in the user. The degree of 
perception of reality can be determined by criteria such as 
sense of presence, immersion, real-time and interaction [58]-
[61]. While the user's mental feeling in the virtual 
environment expresses the "sense of presence" criterion, the 
coverage of the user's field of view with the physical 
hardware expresses the "immersion" criterion, and the 
manipulation of the environment in "real-time" expresses the 
"interaction" criterion. The more these criteria can be 
supported, the more the environment is perceived as real by 
the user. The perception of reality is application-specific. For 
example, while the main goal in computer games is to create 
a feeling of reality, in some educational applications, only 
the visualization of 3-D concepts can be aimed. The type of 
virtual environment to be designed according to the target 
and the devices to be used are determined. Milgram's 
Reality-Virtuality Continuum [62] is one of the most basic 
classifications used for virtual environments. In this 
Continuum, there are Virtual Environment (containing 
completely 
virtual 
material) 
and 
Real 
Environment 
(containing completely real material) under the main heading 
of Mixed (Hybrid) Reality. In the case of a mixture of these 
two environments, the concepts of Augmented Virtuality 
(contains virtual material more than real ones) and 
Augmented Reality (contains real material more than virtual 
ones) are defined. 
The aim of this study is to present the confidence values 
and positions (virtual material) of CVD development to the 
user and his/her physician as a low-cost early diagnosis 
system within the scope of e-health service. For this reason, 
the method of providing access to these contents from 
mobile device screens has been preferred instead of high-cost 
devices. Although, monitoring the virtual content from a 
small screen minimizes the perception of reality (due to lack 
of immersion), it is considered useful within the scope of the 
study. However, the computing capacity and hardware 
features of a mobile device are insufficient for the operations 
to be performed in the 6-phase system used. For this, all 
operations in the system (except the Imaging Technique 
Phase) are carried out on the server and processed video 
visuals (containing only virtual-real material with calculation 
results) are returned to the user/physician. 
Although one of the important criteria in Augmented 
Reality applications is real-time, there is a visualization 
spread over a long time interval (to support monitoring at 
certain periods) in this study. For this reason, real-time of 
visuals is supported only when the region where the relevant 
tissue is displayed and the class of the images stored on the 
server match (requires Classification Phase to be used for a 
second test). Displaying videos of the matched class on the 
screen in this way is examined under Indirect Augmented 
Reality [36]. Indirect augmented reality is especially suited 
for outdoor use [36]. Its basis is the superimposition (overlay 
of virtual and real material) of virtual models of 
structures/places in their real locations while displaying them 
in their real environment. Two separate registration (spatial 
alignment of virtual and real material) methods (offline and 
online registration) are used in Indirect Augmented Reality 
[39]. Offline registration is combining the pre-recorded video 
of the relevant tissue with the virtual model. In the offline 
registration in this study, the YOLOv3 object detection 
algorithm confidence values and positions of the CVD 
detected in the digitally processed image are superimposed 
on the near-infrared raw video recordings. Online 
registration, 
on 
the 
other 
hand, 
is 
the 
real-time 
superimposition of the superimposed video prepared in 
offline registration on the relevant tissue viewed in the real 
28
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
world. In the online registration of this study, offline video 
visuals are presented to the user in real-time when the tissue 
region to which they belong is displayed. 
Among the advantages of using Indirect Augmented 
Reality within the scope of this study, having low-cost and 
low computational/processing load requirements (effective 
for use on mobile devices as no additional hardware is 
required), tight matching of virtual and real material with 
offline registration (effective for displaying the vein position 
on the relevant tissue), and offering a time-independent 
visualization as before/after (effective for tracking CVD 
development process from images obtained at certain 
periods) can be counted [36][40][63]. 
In the offline registration of this study, on near-infrared 
raw image(s) (it is the real material captured in the Imaging 
Technique Phase and shown in Fig. 6 (a)), processed 
image(s) (it is virtual material processed by the Digital 
Image Post-Processing Phase and shown in Fig. 6 (b)) and 
their YOLOv3 object detection algorithm results (it is the 
virtual material obtained from the Object Detection Phase 
and shown in Fig. 6 (c)) were superimposed, as depicted in 
Fig. 6 (d) and video visuals of the relevant period were 
created. As online registration, these superimposed video 
visuals are presented to the user/physician via the mobile 
device (smartphone) screen when the relevant tissue is re-
displayed, as shown in Fig. 6 (e). 
V. 
CONCLUSION AND FUTURE WORK 
Visualization of superficial veins by using near-infrared 
light is among the applications currently used in the 
healthcare industry. This imaging technique, which is 
especially useful for vascular access and is harmless to the 
body, was used in this study within the scope of early 
diagnosis. The superficial vein monitoring system, which 
was prepared within the scope of the doctoral study, was re-
trained in this study to detect Chronic Venous Disorder with 
4 different classes (spider_beginner, spider_advanced, 
varicose_beginner and varicose_advanced). In this study, all 
of the artificial vein enlargement patterns in the test images 
could be accurately detected by using the You Only Look 
Once version-3 object detection algorithm and no 
misclassification 
was 
encountered. 
The 
absence 
of 
misclassification can show that the proposed system is 
particularly useful for the health sector. The classes and their 
detection rates of the tested patterns are marked on the 
resulting image as the class name and the confidence value. 
The developed system was able to detect the classes of 
objects 
with 
the 
values 
of 
Accuracy 
Rate 
(1), 
Misclassification Rate (0), Precision (1), Prevalence (0.5) 
and F-Score (1). In addition to the class detection, the pattern 
positions are also determined with the help of this algorithm 
and marked on the images. Also, a video-based indirect 
augmented reality environment was integrated into the study 
for the monitoring of 4 classes of superficial vein 
enlargement, thus informing the patient and the physician.  
 
 
 
 
The Near-Infrared Raw Image  
 
(a) 
C2 Type (varicose_beginner) CVD Patterned Digitally Processed 
Image 
 
(b) 
The YOLOv3 Object Detection Algorithm Detection Rate of C2 Type 
(varicose_beginner) CVD Pattern (Training with 4-Classes) 
 
(c) 
Offline Registration 
 
(d) 
Online Registration 
 
(e) 
Figure 6.  Indirect augmented reality offline registration and online 
registration for C2 type CVD pattern (varicose_beginner). (a) The near-
infrared raw image. (b) Digitally processed image with artificial 
varicose_beginner pattern added. (c) The YOLOv3 object detection 
algorithm confidence value obtained with the 4-class training. (d) Offline 
registration. (e) Online registration. 
29
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
On the other hand, black pixels that may occur on the 
image due to the illumination or imaging can have a negative 
effect on the detection process. In the experiments, this 
situation was encountered in a few images. It was observed 
that the dense accumulation of these pixels in a certain 
region makes detections of the spider_beginner type (due to 
its small size), which does not actually exist, albeit with low 
confidence values (0.25, 0.30). However, when considered 
within the scope of video visuals, these determinations in one 
or more images were not taken into account, since there are 
100 or more image combinations.  
Detection of Class-1 type (spider/telangiectasias vein) of 
Chronic Venous Disorder is of very critical importance 
especially in determining the transition process to Class-2 
type (varicose vein) and the treatment process. In this 
respect, the most important benefit of the indirect augmented 
reality environment within the scope of this system and early 
diagnosis is that it allows visualization in the form of before 
and after presentation. In this way, treatment can be started 
and directed without delay. Although the system has been 
tested with near-infrared data and artificial patterns, it is 
planned to test the system on real patients within the scope of 
future studies. In addition, it is considered that the proposed 
system can shed light on researchers who want to make 
similar determinations on images obtained with different 
imaging techniques. 
REFERENCES 
[1] H. A. Erdem and S. Utku, “Detecting Venous Disorders via 
Near-infrared Imaging: Observation of Varicose Vein 
Development”, The IARIA Annual Congress on Frontiers in 
Science, Technology, Services, and Applications (IARIA 
Congrees 2022), IARIA, Jul. 2022, pp. 65-68, Nice, France. 
https://www.thinkmind.org/index.php?view=article 
&articleid=iaria_congress_2022_1_130_50088 
[2] H. 
A. 
Erdem, 
“Integrating 
Detection 
of 
Vascular 
Degeneration into Augmented Reality Environment: An E-
Health Application Based on Near-Infrared Spectroscopy and 
Deep Learning”, Doctoral Dissertation, Dokuz Eylül 
University, 
İzmir, 
Turkey, 
Aug. 
2022. 
https://tez.yok.gov.tr/UlusalTezMerkezi/TezGoster?key=kIrId
tdJ31bRgjb6fHvMUazxnXZ9naUDrsdv_v_8KDNe7rlBiHk9
Ti8ryVaaMRGz 
[3] H. A. Erdem, I. Erdem, and S. Utku, “Near-Infrared Mobile 
Imaging Systems for E-Health: Lighting the Veins”. The 
Twelfth International Conference on eHealth, Telemedicine, 
and Social Medicine (eTELEMED), Nov. 2020, IARIA, pp. 
80-84, 
Valencia, 
Spain. 
https://www.thinkmind.org/index.php?view=article&articleid
=etelemed_2020_3_90_40039 
[4] H. A. Erdem and S. Utku, “Augmented Reality Aided Pre-
Diagnosis Environment for Telemedicine: Superficial Vein 
Surveillance System”. European Journal of Science and 
Technology, 
vol. 
38, 
pp. 
376-385, 
Aug. 
2022. 
https://doi.org/10.31590/ejosat.1107531 
[5] M. Y. M. Chen, T. L. Pope, and D. J. Ott, Basic Radiology, 
2nd ed., McGraw Hill: Lange Clinical Medicine, 2011. 
[6] Inside View: A Blog For Our Patients, From UVA Radiology 
and Medical Imaging. Different Imaging Tests, Explained. 
17.Sep.2017. 
[Online]. 
Available 
from: 
https://blog.radiology.virginia.edu/different-imaging-tests-
explained/ [retrieved: May, 2023] 
[7] Bravo Imaging. Medical Imaging Modality Options and Their 
Uses. 
20.Jul.2008. 
[Online]. 
Available 
from: 
https://www.bravoimaging.com/medical-imaging-equipment-
miami/medical-imaging-modality-options-and-their-uses/ 
[retrieved: May, 2023] 
[8] V. P. Zharov et al., “Infrared Imaging of Subcutaneous 
Veins”, Lasers in Surgery and Medicine: The Official Journal 
of the American Society for Laser Medicine and Surgery, 
34(1), pp. 56-61, 2004. https://doi.org/10.1002/lsm.10248 
[9] R. Fuksis, M. Greitans, O. Nikisins, and M. Pudzs, “Infrared 
Imaging System for Analysis of Blood Vessel Structure”, 
Electronics and Electrical Engineering, System Engineering, 
Computer 
Technology, 
97(1), 
pp. 
45-48, 
2010. 
https://eejournal.ktu.lt/index.php/elt/article/view/9943 
[10] N. Bouzida, A. H. Bendada, and X. P. Maldague, “Near-
Infrared Image Formation and Processing for the Extraction 
of Hand Veins”, Journal of Modern Optics, 57(18), pp. 1731-
1737, 2010. https://doi.org/10.1080/09500341003725763 
[11] Enclopedia Britannica, Hemoglobin. [Online]. Available 
from: 
https://www.britannica.com/science/hemoglobin 
[retrieved: May, 2023] 
[12] S. Crisan, “A novel perspective on hand vein patterns for 
biometric 
recognition: 
Problems, 
challenges, 
and 
implementations”, in Biometric security and privacy. Signal 
Processing for Security Technologies, R. Jiang, S. Al-
Maadeed, A. Bouridane, P. Crookes, and A. Beghdadi, Eds. 
Springer International Publishing, Cham., pp. 21-49, 2017. 
https://doi.org/10.1007/978-3-319-47301-7_2 
[13] Y. Ayoub et al., “Diagnostic Superficial Vein Scanner”, 
International Conference on Computer and Applications 
(ICCA 
2018), 
Aug. 
2018, 
IEEE, 
pp. 
321-325. 
https://doi.org/10.1109/COMAPP.2018.8460229 
[14] Royal Society of Chemistry, Introduction to Spectroscopy. 
[Online]. 
Available 
from: 
https://edu.rsc.org/download?ac=11384 
[retrieved: 
May, 
2023] 
[15] H. Bay, T. Tuytelaars, and L.Van Gool, “SURF: Speeded up 
robust features”, in Computer vision-European Conference on 
Computer Vision Lecture Notes in Computer Science, vol 
3951. A. Leonardis, H. Bischof, and A. Pinz, Eds. Springer, 
Berlin, 
Heidelberg, 
pp. 
404-417, 
2006. 
https://doi.org/10.1007/11744023_32 
[16] C. L. Lin and K. C. Fan, “Biometric Verification Using 
Thermal Images of Palm-Dorsa Vein Patterns”, IEEE 
Transactions on Circuits and Systems for Video Technology, 
14(2), 
pp. 
199-213, 
2004. 
https://doi.org/10.1109/TCSVT.2003.821975 
[17] R. Garcia-Martin and R. Sanchez-Reillo, “Vein Biometric 
Recognition on a Smartphone”, IEEE Access, 8, pp. 104801-
104813, 
2020. 
https://doi.org:/10.1109/ACCESS.2020.3000044 
[18] W. Liu et al., “SSD: Single Shot Multibox Detector”, 
Computer 
Vision–ECCV 
14th 
European 
Conference, 
Amsterdam, The Netherlands, Part I 14 Springer International 
Publishing, 
October 
2016, 
pp. 
21-37. 
https://arxiv.org/abs/1512.02325 
[19] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich 
Feature Hierarchies for Accurate Object Detection and 
Semantic Segmentation”, In: Proceedings of the IEEE 
conference on computer vision and pattern recognition, 2014, 
IEEE, pp. 580-587. https://arxiv.org/abs/1311.2524 
[20] R. Girshick, “Fast R-CNN”, In: Proceedings of the IEEE 
International Conference on Computer Vision, 2015, pp. 
1440-1448. https://arxiv.org/abs/1504.08083 
[21] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: 
Towards Realtime Object Detection with Region Proposal 
Networks”, Advances in Neural Information Processing 
Systems 
(NIPS), 
2015, 
pp. 
91–99. 
https://arxiv.org/abs/1506.01497 
30
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
[22] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You 
Only Look Once: Unified, Real-Time Object Detection”, In: 
Proceedings of the IEEE Conference on Computer Vision and 
Pattern Recognition (CVPR), 2016, pp. 779-788. 
[23] J. Redmon and A. Farhadi, “YOLOv3: An Incremental 
Improvement”, 
Arxiv 
preprint, 
2018. 
https://doi.org/10.48550/arXiv.1804.02767 
[24] H. Bandyopadhyay, YOLO: Real-Time Object Detection 
Explained. 
[Online]. 
Available 
from: 
https://www.v7labs.com/blog/yolo-object-detection 
[retrieved: May, 2023] 
[25] S. Shinde, A. Kothari, and V. Gupta, “YOLO Based Human 
Action Recognition and Localization”, Procedia Computer 
Science, 
2018, 
133, 
pp. 
831-838. 
https://doi.org/10.1016/j.procs.2018.07.112 
[26] K. S. Babulal et al., “Real-Time Surveillance System for 
Detection of Social Distancing”, International Journal of E-
Health and Medical Communications (IJEHMC), vol. 13(4), 
pp. 1-13, 2022. https://doi.org/10.4018/IJEHMC.309930 
[27] L. W. Kang, , I. S. Wang, K. L. Chou, S. Y. Chen, and C. Y. 
Chang, “Image-Based Real-Time Fire Detection Using Deep 
Learning 
with 
Data 
Augmentation 
for 
Vision-Based 
Surveillance Applications”, The 16th IEEE International 
Conference 
on 
Advanced 
Video 
and 
Signal 
Based 
Surveillance (AVSS 2019), IEEE, Sept. 2019, pp. 1-4. 
https://doi.org/10.1109/AVSS.2019.8909899 
[28] R. Huang, J. Pedoeem, and C. Chen, “YOLO-LITE: A Real-
Time Object Detection Algorithm Optimized for Non-GPU 
Computers”, In: 2018 IEEE International Conference on Big 
Data 
(Big 
Data). 
IEEE, 
2018. 
pp. 
2503-2510. 
https://doi.org/10.48550/arXiv.1811.05588 
[29] J. Ktari, T. Frikha, M. Hamdi, H. Elmannai, and H. Hmam, 
“Lightweight AI Framework for Industry 4.0 Case Study: 
Water Meter Recognition”, Big Data and Cognitive 
Computing, 
2022, 
vol. 
6(3), 
72, 
July 
2022. 
https://doi.org/10.3390/bdcc6030072 
[30] A. Bochkovskiy, C. Y. Wang, and H. Y. M. Liao, “YOLOv4: 
Optimal Speed and Accuracy of Object Detection”, ArXiv 
preprint, 2020. https://doi.org/10.48550/arXiv.2004.10934 
[31] A. Baccouche, B. Garcia-Zapirain, Y. Zheng, and A. S. 
Elmaghraby, 
“Early 
Detection 
and 
Classification 
of 
Abnormality in Prior Mammograms Using Image-to-Image 
Translation and YOLO Techniques”, Computer Methods and 
Programs 
in 
Biomedicine, 
vol. 
221, 
Jun. 
2022. 
https://doi.org/10.1016/j.cmpb.2022.106884 
[32] C. Santos, M. Aguiar, D. Welfer, and B. Belloni, “A New 
Approach for Detecting Fundus Lesions Using Image 
Processing and Deep Neural Network Architecture Based on 
YOLO Model”, Sensors, vol. 22 (17), Aug. 2022. 
https://doi.org/10.3390/s22176441 
[33] E. Ayan, B. Karabulut, and H. M. Ünver, “Diagnosis of 
Pediatric Pneumonia with Ensemble of Deep Convolutional 
Neural Networks in Chest X-Ray Images”, Arabian Journal 
for Science and Engineering, vol. 47, pp. 2123–2139, Sept. 
2022. https://doi.org/10.1007/s13369-021-06127-z 
[34] Y. Luo, Y. Zhang, X. Sun, H. Dai, and X. Chen, "Intelligent 
Solutions in Chest Abnormality Detection Based on YOLOv5 
and ResNet50", Journal of Healthcare Engineering, vol. 2021, 
Oct. 2021. https://doi.org/10.1155/2021/2267635 
[35] R. Couturier, H. N. Noura, O. Salman, and A. Sider, “A Deep 
Learning Object Detection Method for an Efficient Clusters 
Initialization”, 
Arxiv 
preprint, 
2021. 
https://doi.org/10.48550/arXiv.2104.13634  
[36] J. Wither, Y. T. Tsai, and R. Azuma, “Indirect Augmented 
Reality”, Computers and Graphics, vol. 35(4), pp. 810-822, 
Aug. 2011. https://doi.org/10.1016/j.cag.2011.04.010 
[37] G. Liestol and A. Morrison, “Views, Alignment and 
Incongruity In Indirect Augmented Reality”, 2013 IEEE 
International Symposium on Mixed and Augmented Reality - 
Arts, Media, and Humanities (ISMAR-AMH), Adelaide, SA, 
Australia, pp. 23-28, 2013. https://doi.org/10.1109/ISMAR-
AMH.2013.6671263 
[38] J. Gimeno, C. Portales, I. Coma, M. Fernandez, and B. 
Martinez, “Combining Traditional and Indirect Augmented 
Reality for Indoor Crowded Environments: A Case Study on 
the Casa Batllo Museum”, Computers and Graphics, vol. 69, 
pp. 
92-103, 
Dec. 
2017. 
https://doi.org/10.1016/j.cag.2017.09.001 
[39] F. Okura, T. Akaguma, T. Sato, and N. Yokoya, “Indirect 
Augmented Reality Considering Real-World Illumination 
Change”, In Proceedings of the IEEE International 
Symposium on Mixed and Augmented Reality (ISMAR), 
IEEE, Sept. 2014, pp. 287-288, Munich, Germany. 
https://doi.org/10.1109/ISMAR.2014.6948453 
[40] G. Liestoel, “Augmented Reality Storytelling: Narrative 
Design And Reconstruction Of A Historical Event In Situ”, 
International Journal of Interactive Mobile Technologies, vol. 
13(12), 
pp. 
96-209, 
2019. 
https://doi.org/10.3991/ijim.v13i12.11560 
[41] N. Özbayrak, “Varis Çoraplarının Performans Özelliklerinin 
İncelenmesi (An Investigation About Performance Properties 
of Compression Stockings)”, Master’s Thesis, Uludağ 
University, Bursa, Turkey, Jul. 2009. Thesis in Turkish with 
an abstract in English. http://hdl.handle.net/11452/3321 
[42] A. E. Gabbey and J. Marcin (reviewed by), Healthline. 
Varicose Veins. 8.Mar.2019. [Online]. Available from: 
https://www.healthline.com/health/varicose-veins [retrieved: 
May, 2023] 
[43] Health. Johns Hopkins Medicine. Varicose Veins. [Online]. 
Available 
from: 
https://www.hopkinsmedicine.org/health/conditions-and-
diseases/varicose-veins [retrieved: May, 2023] 
[44] T. Feodor, S. Baila, I. A. Mitea, D. E. Branisteanu, and O. 
Vittos, “Epidemiology and Clinical Characteristics of Chronic 
Venous Disease in Romania”, Experimental and Therapeutic 
Medicine, 
vol. 
17(2), 
pp. 
1097-1105, 
Dec. 
2019. 
https://doi.org/10.3892/etm.2018.7059 
[45] H. 
Partsch, 
“Varicose 
Veins 
and 
Chronic 
Venous 
Insufficiency”, Vasa: European Journal of Vascular Medicine, 
38(4), pp. 293-301, Jan. 2009. https://doi.org/10.1024/0301-
1526.38.4.293 
[46] G. Piazza, “Varicose Veins”, Circulation, vol. 130(7), pp. 
582-587, 
Aug. 
2014. 
https://doi.org/10.1161/CIRCULATIONAHA.113.008331 
[47] N. Labropoulos, “How Does Chronic Venous Disease 
Progress from the First Symptoms to the Advanced Stages? A 
Review”. Advances in Therapy, vol. 36(1), pp. 13-19, Feb. 
2019. https://doi.org/10.1007/s12325-019-0885-3 
[48] S. Behring and A. Gonzalez (reviewed by), Healthline. What 
Are the Stages of Chronic Venous Insufficiency? 10.Jun.2021. 
[Online]. 
Available 
from: 
https://www.healthline.com/health/chronic-venous-
insufficiency-stages [retrieved: May, 2023] 
[49] N. Kahraman et al., “Detection of Residual Varicose Veins 
with Near Infrared Light in the Early Period After Varicose 
Surgery and Near Infrared Light Assisted Sclerotherapy”, 
Vascular, 
vol. 
30(6), 
Oct. 
2021. 
https://doi.org/10.1177/17085381211051489 
[50] Cambridge Dictionary. Artificial Intelligence. [Online]. 
Available 
from: 
https://dictionary.cambridge.org/dictionary/english/artificial-
intelligence [retrieved: May, 2023] 
[51] O. Campesato, Artificial Intelligence, Machine Learning, and 
Deep Learning. Mercury Learning and Information, 2020. 
31
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
[52] A. Govindu and S. Palwe, “Early Detection of Parkinson's 
Disease Using Machine Learning”, Procedia Computer 
Science, 
vol. 
218, 
pp. 
249-261, 
Jan. 
2023. 
https://doi.org/10.1016/j.procs.2023.01.007 
[53] N. N. Sakhare, I. S. Shaik, and S. Saha, “Prediction of Stock 
Market Movement via Technical Analysis of Stock Data 
Stored on Blockchain Using Novel History Bits Based 
Machine Learning Algorithm”, The Institution of Engineering 
and 
Technology 
IET 
Software, 
Jan. 
2023. 
https://doi.org/10.1049/sfw2.12092 
[54] I. Erdem, “Ülkemizdeki Havalimanlarının Yolcu ve Uçak 
Taleplerinin Çok Yönlü Değerlendirilmesi (Multivariate 
Analysis of Passenger and Aircraft Demands of Airports in 
Turkey)”, Master’s Degree Thesis, Dokuz Eylül University, 
İzmir, Turkey, Aug. 2012. Thesis in Turkish with an abstract 
in 
English. 
https://tez.yok.gov.tr/UlusalTezMerkezi/tezDetay.jsp?id=beo
mkTrj0BSyzbqaJYaYpQ&no=WHgKOWHTcFfbOi8a3PerJ
A 
[55] Y. LeCun, Y. Bengio, and G. Hinton, “Deep Learning”, 
Nature, 
vol. 
521, 
pp. 
436–444, 
May 
2015. 
https://doi.org/10.1038/nature14539 
[56] P. Skalski, Makesense, Alpha. Free To Use Online Tool For 
Labelling 
Photos. 
[Online] 
Available 
from: 
https://www.makesense.ai/ [retrieved: May, 2023] 
[57] T. Mazuryk and M. Gervautz, “History, Applications, 
Technology And Future”, Virtual Reality Vienna: Vienna 
University 
of 
Technology, 
vol.72, 
1996. 
https://www.researchgate.net/publication/2617390_Virtual_R
eality_-_History_Applications_Technology_and_Future 
[58] M. A. Gutiérrez, F. Vexo, and D. Thalmann, Stepping Into 
Virtual Reality, England: Springer-Verlag London, 2008. 
https://doi.org/10.1007/978-1-84800-117-6 
[59] W. R. Sherman and A. B. Craig, Understanding Virtual 
Reality-Interface, Application and Design, The United States 
of 
America: 
Morgan 
Kaufmann 
Publishers, 
2003. 
https://doi.org/10.1016/C2013-0-18583-2 
[60] C. C. Ko and C. D. Cheng, Interactive web-based virtual 
reality with Java 3D. The United States of America: 
Information Science Reference, 2009. 
[61] H. A. Erdem, “Utilization of Virtual Reality Environment as 
an Interactive Visual Learning Tool in Primary School 
Education System”, Master’s Degree Thesis, Dokuz Eylül 
University, 
İzmir, 
Turkey, 
Aug. 
2013. 
https://tez.yok.gov.tr/UlusalTezMerkezi/tezDetay.jsp?id=iofm
_MFGdK7U1KPrXLYjzQ&no=L77dJQmcOK5zBMOxhpM
cUw 
[62] P. Milgram, H. Takemura, A. Utsumi, and F. Kishino, 
“Augmented Reality: A Class of Displays on the Reality-
Virtuality Continuum”, Telemanipulator and Telepresence 
Technologies. Society of Photo-Optical Instrumentation 
Engineers (SPIE) Digital Library, vol. 2351, pp. 282-292, 
Dec. 1995. https://doi.org/10.1117/12.197321 
[63] J. B. Alves, B. Marques, C. Ferreira, P. Dias, and B. S. 
Santos, 
“Comparing 
Augmented 
Reality 
Visualization 
Methods for Assembly Procedures”, Virtual Reality, vol. 26, 
pp. 235-248, Jul. 2022. https://doi.org/10.1007/s10055-021-
00557-8
 
32
International Journal on Advances in Life Sciences, vol 15 no 1 & 2, year 2023, http://www.iariajournals.org/life_sciences/
2023, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

