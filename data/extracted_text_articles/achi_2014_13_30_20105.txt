Experimental Study into the Time Taken to Understand Words 
when Reading Japanese Sign Language 
 
 
Mina Terauchi 
Polytechnic University 
(Visiting Fellow of Kogakuin University) 
Kodaira-shi, Tokyo, Japan 
e-mail: terauchi@uitec.ac.jp 
 
Keiko Watanabe   Yuji Nagashima 
Kogakuin University 
Hachioji-shi, Tokyo, Japan 
e-mail: ed13001@ns.kogakuin.ac.jp   e-mail: nagasima@cc.kogakuin.ac.jp 
 
 
Abstract— We are conducting research in areas such as 
linguistic and cognitive analysis of sign language, and 
animation, with the aim of assisting the hearing impaired with 
communication.  It was already assumed that hearing-
impaired people recognize the meaning of sign language words 
while the sign language movement is being performed.  Based 
on the results of dialog-based analysis of sign language 
however, we predicted that hearing-impaired people would 
understand the meaning of sign language words during the 
“transition” stages.  For the purposes of this paper, we 
compiled a series of experiment sentences and conducted an 
experiment to determine the timing with which hearing-
impaired people recognized target words in each sentence.  The 
results of the experiment indicated that hearing-impaired 
people recognized a high percentage of words during the “in-
transition” stage.  Based on these results, we can assume that 
hearing-impaired people understand sign language sentences 
by 
effectively 
utilizing 
hand 
shapes, 
movements 
and 
information such as expressions and intonation.  The challenge 
for the future is how to harness this information to improve the 
comprehension abilities of students undergoing sign language 
education. 
Keywords- Sign Language,  predict recognition,  transition. 
I. 
INTRODUCTION 
We are in the process of conducting linguistic and 
cognitive research into sign language in order to assist the 
hearing impaired with communication. 
For the purposes of this paper, we conducted an 
experimental study into the time taken for deaf people to 
predict and understand words at the ends of sentences when 
reading Japanese sign language.  We divided the sign 
language sentences used in the reading experiment into three 
different types depending on the nature of the word at the 
end of each sentence (the “target word”).  Type A sentences 
contained target words that enabled the reader to correctly 
interpret the meaning of the sentence.  Type B sentences 
contained target words that were of the correct type 
syntactically, but that did not enable the reader to correctly 
interpret the meaning of the sentence.  Type C sentences 
contained target words consisting of fabricated movements 
based on phonemes that are absent from Japanese sign 
language (“pseudowords”), making it impossible for the 
reader to interpret their meaning.  We compiled these three 
types of sentences with assistance from a native signer.  We 
filmed the sentences being signed and then edited the sign 
language clips for use in the experiment. 
For the experiment itself, we asked 13 native signers to 
read sign language sentences following each of the three 
patterns. The results were as follows. 
 
II. 
THE TEMPORAL STRUCTURE OF SIGN LANGUAGE 
Sign language consists of two different types of signal; 
manual signals and non-manual signals.  It is a visual 
language that uses both types of signal simultaneously and 
continuously.  Manual signals (MS) are determined by 
handshapes, the direction of the palms, the position of the 
hands and other broad movements, and are used to form 
words. Non-manual signals (NMS) meanwhile consist of 
elements such as the shape of the mouth, the signer’s eye line, 
expressions and nodding, and are used for semantic and 
syntactic purposes. 
The temporal structure of sign language can be broadly 
divided into two categories; “signs” and “transitions”. A 
“sign” starts when the signer has decided and formed the 
shape for a single sign language word.  When the signer has 
finished moving, the sign then ends the moment before they 
release their handshape.  A “transition” starts when the 
signer has finished the movements required for a single sign 
language word, and lasts until the moment before the signer 
starts to sign the next sign language word, as they move into 
position.  Transitions can be further divided into two sub-
categories; “in-transitions” and “out-transitions”.  An “in-
transition” starts when the signer begins to create the manual 
signal for the target word, and ends when they have finished 
forming the relevant manual signal.  An “out-transition” 
starts when the signer releases the manual signal for the 
target word, and ends the moment before they form the 
manual signal for the next word.  Figure 1 shows the defined 
categories that make up sign language, with “signs” and 
“transitions” plotted along a timeline. 
 
313
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

 
Figure 1.  The temporal structure of sign language 
 
III. 
COGNITIVE EXPERIMENT ON SIGN LANGUAGE WORDS 
A. Compiling experiment data 
We compiled the experiment data (sentences) with the 
aim of ascertaining the point at which the subjects 
recognized the target sign language word.  Expressions in 
sign language have various different characteristics.  The 
chosen method can therefore have a significant effect on the 
results of cognitive experiments.  We carried out a 
preliminary experiment using selected sentences from 
KOSIGN Ver.2, a dialog corpus that includes 10 sign 
language words.  As the sentences were parts of dialog 
however, their contents depended on questions asked by 
another person, which influenced reading of the sign 
language.  Being dialog also meant that the sign language 
included 
elements 
such 
as 
agreement, 
denial 
and 
acknowledgement.  This created further issues because it 
changed the position of the target word within the sentences.   
With that in mind, we began to look into conditions for the 
compilation of sentences that we could use in the main 
experiment. 
 
1) Exploring target words 
We started by determining the position of the target word, 
as the third word in each sentence.  We were unable to 
position the target word at the start of the sentence because it 
would have included transitional movements from the initial 
position, making it impossible to ascertain the point during 
the out-transition or in-transition at which the subject 
recognized the word.  In natural sign language, the choice of 
first words is also limited, because the first word in a 
sentence is often a referent word such as {I} or {you}, or a 
word indicating the tense of the sentence such as {today} or 
{yesterday}.  The second word in a sentence is easily 
influenced by the first word, so that would also have limited 
the range of words available.  Taking all of these factors into 
account, we decided to make the third word the target word, 
and to use sentences consisting of three or four words. 
Next, we selected the target words.  Many words in sign 
language are made up of two or more morphemes.  The 
Japanese word {kazoku} (family) for example consists of the 
two morphemes {ie} (house) and {hitobito} (people).  The 
in-transition for a word like this is likely to suggest a number 
of possible words, including {ie} (house) {yane} (roof) or 
{kazoku} (family).  We therefore decided to use words 
consisting of a single morpheme as candidates for the 
experiment.  We also decided to include sign language for 
the selected words, as detailed below. 
a) One-handed sign language: {omou} (think) 
b) Two-handed sign language: {hana} (flower) 
c) Different sign language with each hand:  
{odororku (surprise) 
d) Sign language with changing handshape:  
 
 {nenrei} (age) 
e) Sign language with classifier handshape:  
{asa} (morning), {neko} (cat) 
 
To enable the experiment to be carried out on both native 
signers and on non-native signers, we selected words that 
even a beginner just starting to learn sign language would 
know. 
 
2) Compiling experiment sentences 
The spatial structure of sign language consists of 
elements such as handshapes and articulatory gestures.  To 
understand the way in which subjects recognize sign 
language words in practice, we looked at three different 
types of example sentences; Type A, Type B and Type C. 
We selected the target words for Type A sentences so 
that the sentences would be grammatically and semantically 
correct in the context of sign language.  We called these 
“correct sentences”.  The following is an example of the 
correct sentences used in the experiment.  The words 
between curved brackets { } are sign language words.  The 
words between speech marks “ “ give the meaning of the 
sentence in English. 
 
Example correct sentence: 
{fujisan}      {miru}  {asa}         {kirei} 
(Mount Fuji) (look) (morning) (beautiful) 
 
English translation: 
 
“Mount Fuji looks beautiful in the morning” 
 
Type B sentences had the correct grammatical structure, 
but we selected target words that would affect how the 
sentence was interpreted.  We called these “difficult to 
interpret sentences”.  For Type B sentences, we tried to 
select target words whose sign language movements were 
dissimilar to the target words used in the Type A sentences. 
 
Example difficult to interpret sentence: 
{ashita}         {tenki}  {ame} 
 
(tomorrow)  (weather)  (candy) 
 
English translation: 
“Tomorrow the weather will be candy” 
 
Type C sentences contained target words consisting of 
fabricated movements based on phonemes that are absent 
314
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

from Japanese sign language (“pseudowords”), making it 
impossible for the reader to interpret their meaning.  We 
called these “pseudoword sentences”. We complied the sign 
language movements for Type C sentences so that the 
transition from the second word to the target word resembled 
the equivalent transition in the Type A sentences. 
 
Example pseudoword sentence:  
{watashi}      {ototo}            {pseudoword 1} 
(I)     (younger brother)   (pseudoword 1) 
 
In this sentence, the sign language movement for 
“pseudoword 1” starts by moving from the cheek, as in the 
sign language word {shumi} (hobby), before opening and 
lowering the hand, as in the sign language word {hikari} 
(light).  The aim of using a pseudoword is to check that the 
subjects are taking the reading experiment seriously. 
 
3) Filming clips of example sentences 
We compiled 226 correct sentences, 48 difficult to 
interpret sentences and 12 pseudo word sentences for use in 
the reading experiment.  We then filmed a native signer 
signing the sentences.  For the purpose of the reading 
experiment itself, we selected 20 correct sentences, two 
difficult to interpret sentences and one pseudo word sentence 
from the sign language sentences we had recorded.  Table 1 
shows the selected sentences. 
We used the following procedure to edit the sign 
language sentences for the reading experiment. 
 
a) We took the section from the start of the sentence to 
the first frame of the out-transition from the second word, 
immediately before the target word, and called it “Pattern 1” 
(Figure 2(a)).  All Pattern 1 clips cut off at the out-transition 
from the second word. 
b) Next, we added the next frame following on from 
Pattern 1 and called the resulting clip “Pattern 2” (Figure 
2(b)).  So Pattern 2 clips kept running for one frame longer 
than Pattern 1 clips. 
c) We continued to produce similar patterns, by adding 
another frame at the out-transition from the second word, 
one frame at a time. 
We continued this process of adding one frame at a time, 
all the way through to the out-transition from the target word.  
The final experiment clip was Pattern n, which showed the 
target word in its entirety. 
Figure 2 shows the progression through the different 
patterns used for the experiment clips.  If subjects had been 
given a long time to think, they might have worked out the 
end of the sentence.  We therefore edited the clips so that the 
patterns played through continuously, one after another.  As 
subjects required some thinking time however, we edited the 
clips with a three-second countdown between each pattern. 
B. Experiment procedure 
We explained to the subjects in advance that we wanted 
them to recognize the third word in each sentence.  To 
prevent subjects from working out the answers from the 
context, and from being influenced by the Japanese language, 
we asked them to watch each clip and then give their 
answers in sign language during the countdown before the 
next clip.  We then recorded the experiment to determine at 
what point (pattern) the subjects were able to read the target 
word.  If a subject was unsure about the target word or gave 
an incorrect answer, we continued to play the clips until they 
gave the correct answer.  We recorded subjects’ answers 
even if they were incorrect.  Correct, difficult to interpret and 
pseudoword sentences were shown in a random order, rather 
than the order listed in TABLE I (next page). 
 
 
(a) Patter 1 
 
(b) Pattern 2 
 
(c) Pattern 3 
 
    (n) Final pattern 
Figure 2.  Example patterns for the experiment sentences 
C. Subjects 
A total of 13 native-signers took part in the experiment as 
subjects, as outlined in TABLE II.  It is difficult for to the 
cooperation of the experiment for a hearing impaired people.   
In addition, this experiment takes long time.  Therefore  I 
have become 13 people in this article. 
TABLE II.  
LIST OF SUBJECTS 
Subject 
Age group 
Gender 
Age when Subject  
Lost Hearing 
A 
60s 
Female 
0 
B 
30s 
Female 
4 
C 
20s 
Male 
3 
D 
50s 
Female 
5 
E 
60s 
Female 
0 
F 
40s 
Male 
3 
G 
40s 
Female 
0 
H 
60s 
Male 
1 
I 
60s 
Female 
0 
J 
20s 
Male 
0 
K 
40s 
Female 
0 
L 
50s 
Female 
0 
M 
50s 
Female 
0 
 
315
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

TABLE I.  
LIST OF SENTENCES USED IN THE EXPERIMENT 
Sign language words 
1st 
2nd 
3rd 
4th 
Correct sentences 
1 
fujisan 
(Mount Fuji) 
miru 
(look) 
asa 
(morning) 
kirei 
(beautiful) 
2 
kore 
(this) 
yoi 
(good) 
omou 
(think) 
  
3 
isshokenmei 
(to the best of one's ability) 
sagasu 
(search) 
shikashi 
(however) 
nakatta 
(not there) 
4 
taberu 
(eat) 
ato 
(after) 
tsugi 
(next) 
nani 
(what) 
5 
ima 
(now) 
kyuryo 
(salary) 
sukunai 
(low) 
  
6 
kino 
(yesterday) 
neko 
(cat) 
shinu 
(die) 
kanashii 
(sad) 
7 
ima 
(now) 
nani 
(what) 
tsukuru 
(make) 
anata? 
(you) 
8 
moshikomi 
(application) 
ashita 
(tomorrow) 
made 
(by) 
daijyobu? 
(OK) 
9 
watashi 
(I) 
okane 
(money) 
nai 
(not have) 
watashi 
(I) 
10 
kyo 
(today) 
miru 
(watch) 
dake 
(only) 
owaru 
(end) 
11 
anata 
(you) 
kao 
(face) 
odoroku 
(surprise) 
nani? 
(what) 
12 
massugu 
(straight ahead) 
iku 
(go) 
mura 
(village) 
aru 
(there is) 
13 
hon 
(book) 
kasu 
(lend) 
kamawanai 
(OK) 
  
14 
watashi 
(I) 
suiei 
(swimming) 
tokui 
(good at) 
watashi 
(I) 
15 
anata 
(you) 
chichi 
(father) 
toshi 
(age) 
ikutsu? 
(how old) 
16 
senshu 
(last week) 
doyobi 
(Saturday) 
shibai 
(play) 
mita 
(see) 
17 
anata 
(you) 
suki 
(like) 
hana 
(flower) 
nani? 
(what) 
18 
sono 
(that) 
heya 
(room) 
tabako 
(cigarette) 
dame 
(forbidden) 
19 
kino 
(yesterday) 
yoru 
(night) 
msuume 
(daughter) 
kaetta 
(returned) 
20 
anata 
(you) 
shuwa 
(sign language) 
dekiru 
(can do) 
  
Semantically unconventional 
sentences 
1 
ashita 
(tomorrow) 
tenki 
(weather) 
ame 
(candy) 
  
2 
watashi 
(I) 
musuko 
(son) 
neko 
(cat) 
kaicho 
(chairman) 
Phonologically unconventional 
sentences 
1 
watashi 
(I) 
ototo 
(younger brothre) 
shumi(opening hand 
from fist as in 
{hikari}(light) 
(hobby) 
gemu 
(video games) 
  
IV. 
COGNITIVE EXPERIMENT ON SIGN LANGUAGE 
WORDS 
Table III shows the point at which the 13 subjects 
recognized the target word in each of the sign language 
sentences.  The position of the out-transition and in-
transition stages was determined by the native signer who 
compiled the example sentences. 
Table III shows that, when faced with correct sentences, 
subjects answered most target words correctly between the 
out-transition from the second word and the in-transition 
to the third word. 
 
Figure 3 shows the cumulative rate of recognition of 
target words at three points in time; during the in-transition, 
out-transition and word movement. 
As Figure 3 clearly shows, most of the subjects 
recognized the target word before the target word 
movement. The rate of recognition also increased 
dramatically during the first 80% (approx.) of the time 
following the start of the out-transition from the second 
word.  During the first 30% (approx.) of the time 
following the start of the in-transition to the target word 
meanwhile, the rate of recognition was over 50%. 
316
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

TABLE III.  
RECOGNITION TIMES FOR TARGET WORDS 
 
Out-transition
from second word
In-transition
to third word
Movement
for third word
asa (morning)
0
1
12
omou (think)
13
0
0
shikashi (however)
1
7
4
tsugi (next)
1
9
2
sukunai (low)
1
12
1
shinu (die)
7
6
0
tsukuru (make)
3
9
1
made (by)
13
0
0
nai (not have)
13
0
0
dake (only)
13
0
0
bikkuri (surprise)
0
9
4
mura (village)
0
13
0
kamawanai (OK)
13
0
0
tokui (good at)
8
5
0
toshi (age)
7
6
0
shibai (play)
8
5
0
hana (flower)
0
5
8
tabako (cigarette)
7
6
0
musume (daughter)
0
9
4
dekiru (can do)
13
0
0
ame (candy)
0
10
3
neko (cat)
1
10
2
Phonologically
unconventional sentences
Non-sign language
(shumi (hobby))
0
4
9
Example sentences
Target word
Result (recognition)
Correct sentences
Semantically
unconventional sentences
 
 
 
 
Figure 3.     Cumulative rate of recognition in correct sentences 
We also ran the experiment using difficult to interpret and 
pseudoword sentences, to examine how subjects were able to 
recognize words during the transition stages.  In the case of 
difficult to interpret sentences, 3% of subjects recognized the 
target word during the out-transition, 67% during the in-
transition, and 30% during the word movement, indicating 
that 70% of subjects still recognized the target word during 
the transition stages. The target words in the difficult to 
interpret sentences were sign language words whose 
meaning was unpredictable. It is therefore to be expected that 
recognition amongst the subjects would be slightly slower 
than normal.  Although there were few correct answers 
during the out-transition from the second word in the case of 
difficult to interpret sentences, the rate of recognition shot up 
to 67% during the in-transition to the target word.  This 
suggests that native signers are able to recognize words 
based on the transition, even when dealing with difficult to 
interpret sentences. 
If we look at incorrect answers given in response to 
correct sentences, we see that the subjects incorrectly 
recognized very similar words as the target.  The following 
section examines two typical examples in sign language. 
 
Example incorrect answer 1: 
 
{asa}        → {mitomeru}  {mata} 
(morning) → (recognize)    (again) 
 
In the first example, the movement for the in-transition is 
similar.  This is probably why many of the subjects 
recognized the target word during the word movement for 
the third word. 
317
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

 
Example incorrect answer 2: 
 
{sukunai} → {yasui} {mata}  {binbo} 
(few/low)  → (cheap)  (again)  (poor) 
 
The second example contains a different movement for 
the in-transition.  The sign language for the target word was 
supposed to be the following. 
 
Example incorrect answer 3: 
 
{ima} {kyuryo} {sukunai} 
(now)    (salary)    (low) 
English translation: 
 
“My current salary is low” 
 
It is likely that the subjects inferred from the second word 
{kyuryo} (salary) that the next word would be {yasui} 
(cheap) or {binbo} (poor). 
At the same time, there were some words that more or less 
all of the subjects recognized during the out-transition from 
the second word.  The words in question – {made} (by), 
{dake} (only), {nai} (not have), {kamawanai} (OK) and 
{dekiru} (can do) – all have characteristic word movements 
that are easy to recognize. 
Based on these results, we can assume that native signers 
predict the next word in a sentence partway through the 
transition stages, based on information such as expressions 
and intonation. 
V. 
CONCLUSION AND CHALLENGES FOR THE 
FUTURE 
This paper is an experimental study into the time taken 
for deaf people to predict and understand words at the end of 
sentences when reading Japanese sign language. 
We divided the sign language sentences used in the 
reading experiment into three different types depending on 
the nature of the word at the end of each sentence (the “target 
word”).  Type A sentences (“correct sentences”) contained 
target words that enabled the reader to correctly interpret the 
meaning of the sentence.  Type B sentences (“difficult to 
interpret sentences”) contained target words that were of the 
correct type syntactically, but that did not enable the reader 
to correctly interpret the meaning of the sentence.  Type C 
sentences (“pseudoword sentences”) contained target words 
consisting of fabricated movements based on phonemes that 
are absent from Japanese sign language (“pseudowords”), 
making it impossible for the reader to interpret their meaning. 
For the experiment itself, we asked native signers to read 
sign language sentences following three different patterns.  
The results showed that, in the case of correct sentences, 
over 80% of the subjects predicted and correctly understood 
the target word before the word itself was signed.  In the case 
of difficult to interpret sentences meanwhile, 70% of subjects 
predicted the target word before the word itself was signed 
and were able to interpret the sentences in spite of their 
unconventional meanings.  When dealing with pseudoword 
sentences however, the subjects were unable to predict the 
signer’s movements until the fabricated movement for the 
target word was performed. 
Based on these results, we can assume that deaf people 
predict words and understand sentences by effectively 
utilizing the information required to form correct sentences, 
including expressions and intonation. 
The challenge for the future is whether this method of 
reading information can be learnt through sign language 
education. With the clarification of ascertaining the point at 
which the subjects recognized the target sign language word, 
that can be helpful in generating sign language animation.   
 
ACKNOWLEDGMENT 
We would like to thank everyone who assisted with the 
recording of the sign language clips and who took part in the 
experiment.  
 
REFERENCES 
[1] 
Ministry of Health, Labour and Welfare of Japan:”Survey on 
persons with pyhsical disability”,2006(in Japanese). 
[2] 
C. Baker:” Regulators and turn-taking in American Sign 
Language discourse”, In:L.A.Fridman(Ed.), On the other hand, 
New York: Academic Press, pp.215-236,1977. 
[3] 
K.Nakazono, et.al :” Quantification Method of Delay Effect 
on Sign Dialogue”, The IEICE Transactions on Information 
and Systems, Vol.J90-D, No.3, pp. 1-11, 2007(in Japanese). 
[4] 
M.Terauchi, K.Nakazono and Y.Nagashima:” Fundamental 
Analysis of Sign Recognition during JSL Sentence Reading”, 
IEICE technical report, Vol.107, No.369(HIP2007-145)，pp. 
91-95, 2007(in Japanese). 
318
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

