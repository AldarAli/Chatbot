A Platform for the Integrated Management of IT Infrastructure Metrics
Christian Straube, Wolfgang Hommel, Dieter Kranzlm¨uller
Leibniz Supercomputing Centre, MNM-Team, Boltzmannstrasse 1, 85748 Garching
[straube,hommel,kranzlmueller]@lrz.de
Abstract—In this work-in-progress paper, we argue that
most measurement and metrics as they are used in today’s IT
management do not provide a sufﬁcient foundation for qualiﬁed
upper level management decisions. By exemplary applying
the state-of-the-art energy efﬁcient metrics to SuperMUC, an
energy-efﬁcient three PetaFlop/s high performance computing
system that has been put into service in June 2012 at the
Leibniz Supercomputing Centre, we show that there are four
major gaps between the information that can be measured
on a technical level and the information that is needed for
management decision making. We then present our vision of a
management cockpit that centralizes measurement and metrics
management in an organization-wide manner. It aggregates,
processes and transforms metrics data into indicators for
management decisions. We present the research questions, our
solution approaches, and preliminary results regarding the
design and implementation of this management cockpit.
Keywords-IT
management;
measurement;
metrics;
gover-
nance; decision support.
I. INTRODUCTION
For many cloud computing, high performance computing
(HPC), and other large data centers, raising energy con-
sumption costs are one of the prime motives for an in-
depth examination of energy-efﬁcient technology. Obviously,
energy efﬁciency must be considered before investing into
new hardware because, for example, CPU frequency scaling
is an important functionality that helps to adjust energy
consumption proportional to the current workload. However,
energy efﬁciency is not a static property that is only relevant
during purchasing decisions. On the one hand, energy-saving
capabilities must constantly be monitored and controlled to
ensure that they are working as expected and to keep their
parameters optimized for the current workload. On the other
hand, the energy efﬁciency of, for example, air-conditioning
or hot liquid cooling systems depends on ever-changing
environmental characteristics such as the current outdoor
temperature.
We argue that in order to better support management
decisions – such as how much money to invest into speciﬁc
Information Technology (IT) infrastructure improvements –
a more holistic approach to measurements and metrics is
urgently required. As we show in Section II, many energy
efﬁciency metrics have been created in the past few years.
However, using SuperMUC – our three PetaFlop/s HPC
system [1] that entered the top 10 of the Top 500 Super-
computer Sites list in June 2012 – as an example, without
loss of generality, we demonstrate in Section III that there
are four major gaps that need to be closed before metrics can
directly contribute to a holistic view of IT infrastructures: At
the moment, 1) the information provided by metrics is not
sufﬁcient for decisions on higher abstraction levels, 2) de-
pendencies between metrics are not sufﬁciently considered,
3) organization-speciﬁc requirements cannot be incorporated
adequately, and 4) there is a lack of improvement recommen-
dations that can be deduced from the metrics values.
Our work-in-progress envisions a management cockpit,
which centralizes measurement and metrics management
organization-wide. We present its design in Section IV, using
energy efﬁciency metrics for SuperMUC as an example
of how low-level metrics can be aggregated to lay the
foundation for upper-level management decisions. While
works for simple metrics compositions with an immediate
practical beneﬁt already, we discuss several research ques-
tions that need to be addressed in Section V. We then outline
our approach, present our preliminary results, and give an
outlook to our next steps.
II. METRICS MANAGEMENT IN RELATED WORK
The related work to our management cockpit vision can
be grouped into the three following categories.
Metric deﬁnition – There are a lot of metrics dealing with
different aspects of energy-efﬁciency, for instance the mea-
surement of the energy consumption of computing servers
and clusters [2], [3], or the energy consumption in optical
IP networks [4]. Further examples are TEEER [5], EPI [6],
ECR, and ECRW [7], [8], [9]. All of them are deﬁned by
providing calculation and interpretation rules, partially in
a very comprehensive way, but nevertheless they all focus
on technical aspects of a single entity on a very low level.
Hence, they do not facilitate a holistic view on the energy
efﬁciency situation of SuperMUC, which, being a large HPC
system, aggregates many different hardware components in
a complex architecture.
Structuring and comparison – There is literature and
ongoing research in several topics about metrics taxon-
omy [10], [11], classiﬁcation [12], and comparison [13].
These approaches structure and compare the aforementioned
metrics, but they neither aggregate different metrics to derive
new statements, nor do they consider dependencies and
correlations between metrics. Instead, they focus on a single
speciﬁc class of metrics, like equipment-level metrics, and
125
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-226-4
INFOCOMP 2012 : The Second International Conference on Advanced Communications and Computation

Level of abstraction
Top-Level-Management (CIO, COO)
Strategic, general questions and decisions
concerning the energy efciency
TEEER
ECR
Interconnect
Storage
CPU
...
ECR
EPI
GB/Watt
MFlop/Watt
Ops/kWh
Gap 3
Environment
?
Gap 4
Gap 2
?
Gap 1 ?
?
Figure 1.
The four gaps in today’s situation that hamper a holistic view.
conﬁne themselves to comparison. Therefore, they do not
allow a holistic view either.
Analyzing combined metrics – Besides the sole deﬁnition
of metrics (group one) and the classiﬁcation of those metrics
(group two) there is a third group of related work that deals
with the task of combining and aggregating [14] several
metrics to deduce new statements about the energy efﬁciency
of an IT infrastructure as a whole. There is no work yet that
focuses on supporting upper-level management decisions.
In the following, we propose our vision of a management
cockpit to address this issue.
III. PROBLEM STATEMENT
The problem description is given with the help of the
following exemplary management question: “Which com-
ponents should be invested into during the next SuperMUC
upgrade in order to save as much money by energy cost
reduction as possible during the next n years?”. By trying to
answer this question, and given the previous outlined contri-
butions of related work, we have identiﬁed four gaps, which
are depicted in Figure 1. In the following explanations of the
gaps, we ﬁrst give a short example concerning SuperMUC
and then a generalization of the problem, respectively.
Gap 1 – The Information Gap In order to answer
the aforementioned management question, we ﬁrst have to
decide which components have the poorest energy efﬁciency
in SuperMUC, as their potential for further improvement
during the next system extension is the highest. Beside
a few generally applicable metrics, most metrics can be
applied only in one area, for instance an HPC/CPU metric
like MFlops/Watt cannot be applied to storage, interconnect,
or software components. Hence, there are several different
metrics that have to be considered for SuperMUC as a
complete system.
Generalization: For a holistic view, the (purely) technical
information has only limited expressiveness and must be
enriched by context and comparison information; addition-
ally, all those information have to be aggregated to provide
comprehensive information to support decision making at
high level. Therefore, conversions, e. g., into currencies or
hours of work, may be required.
Gap 2 – The Dependency Gap In the SuperMUC sce-
nario, changing the CPU type to achieve a higher energy efﬁ-
ciency would have (strong) side effects on other components
of SuperMUC. For instance, using CPUs with a smaller L2
cache size might improve the CPU energy efﬁciency, but at
the same time, SuperMUC’s system interconnect between
the CPUs and the non node-local memory will have higher
workloads and therefore, its energy efﬁciency is decreased.
This may lead to a decreased overall energy efﬁciency.
Generalization: There are a lot of dependencies between
metrics that have to be considered in order to include all
relevant information. In most cases it is not adequate to
improve one or only a few metrics, i. e., partial optimizations
do not yield optimum results. Instead, all (involved) metrics
should be improved [16], correlations have to be shown, and
conclusions have to be drawn from these correlations [17].
Gap 3 – The Environment Gap Changes regarding the
CPUs obviously do not only affect the storage, but also the
cooling facilities. In order to assess the changed amount of
energy, which the cooling facilities need for the additional
CPUs, we have to compare numbers to other supercomputing
centers. But we can get only useful statements if we consider
the speciﬁc environment, including the location SuperMUC
is deployed in: Bavaria in Germany is a relatively warm
region, whereby Iceland is quite cold, so the demand for
cooling would be lower there.
Generalization: The same measurements and metrics may
not have the same expressiveness and purpose in different
scenarios. Each organization will have to make speciﬁc
adoptions to existing metrics, create complementary metrics
for its speciﬁc environment, and specify, for example, how
results must be interpreted properly.
Gap 4 – The Activity Gap In the end it may turn out that
despite all reciprocity, using different CPUs than previously
is the best way to optimize energy costs. Now we need
activity recommendations that describe what to do while
considering implications to other metrics and the caused
costs. For instance, changing cache size has a medium
energy saving effect and is quite cheap, but has a high effect
on the interconnect components, whereby changing the clock
speed has a high energy saving effect, is quite cheap as
well, and has a low effect on the interconnect and storage
components.
Generalization: In order to perform the best adoptions
on the analyzed IT infrastructure, activity recommendations
have to be generated out of the holistic view in a (semi-)
automated way. There are a lot of challenges, such as the
calculation of costs associated with each activity recommen-
dation, and selecting the best one for a given scenario based
on a consideration of the mutual reactions of metrics when
changing the infrastructure, e. g., based on the application
126
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-226-4
INFOCOMP 2012 : The Second International Conference on Advanced Communications and Computation

Energy Efciency
Interconnect
Storage
CPU
Ops/kWh
MFlops/Watt
Oblique Value
Current Value
1
2
%
Management Cockpit - SuperMUC
Figure 2.
Our vision of a management cockpit that provides high-level,
management-relevant information.
of mathematical optimization algorithms.
Closing these gaps is already a non-trivial task for the
area of energy efﬁciency, which we use as an example here.
The key challenge of our research is, however, to ﬁnd a
management solution that works with an arbitrary number
of metrics categories and their interdependence in parallel,
as we outline in the next section.
IV. OUR VISION OF A MANAGEMENT COCKPIT
To close the described four gaps, we envision a manage-
ment cockpit that is built on top of an underlying compre-
hensive and integrated processing layer, which manages all
applied metrics, and a presentation layer; a mock-up of its
GUI is depicted in Figure 2. It provides an holistic view
concerning the energy efﬁciency of SuperMUC to the top-
level-management and makes all aspects feasible that were
described in Section III.
Besides its main objective to “monitor, visualize and
explore measurement data from different perspectives and
at various levels of detail” [17] in order to characterize,
evaluate, predict, and improve IT infrastructures, there are
some underlying sub-objectives that shape the management
cockpit, which we discuss next.
A. Considering metric dependencies
As described in Section III, metrics are currently used
isolated of each other, and hence reciprocity cannot be
respected. Our management cockpit shall manage all met-
rics that are used by an organization in a holistic way
and thus make integrated statements about the SuperMUC
example feasible. Besides the big advantage of respecting
the reciprocity of metrics, considering metric dependencies
facilitates the uncovering of strategic goal conﬂicts [18] and
the consideration of trade-offs, for instance between energy
efﬁciency and performance. Those trade-offs are already
analyzed in some lower-level-approaches (e. g., [12]), but
not yet at upper-level management.
B. Integrating measurement and metrics data
To support the holistic view and root cause analysis
facilities, our solution shall integrate (“selection, embedding,
and handling of the underlying data sources” [19]) and
use as many data sources as required and reasonable. This
leads to the problem of using and consolidating several data
structures and to identify valid data contexts [17]. To be
able to use the management cockpit from the ﬁrst day and
to avoid the “cold-start–problem” [20], existing and actual
data, metrics, and measurements have to be embedded [17].
C. Using data trees
To achieve provenance for the statements provided to
the management decisions, every statement must provide its
data source tree to facilitate root cause analysis: starting at
the top level, any aggregated metrics value can be broken
down into smaller pieces and it can be explained how
this high-level current value materializes. Figure 2 depicts
an exemplary data tree for SuperMUC: the overall energy
efﬁciency value is aggregated by interconnect-, storage-, and
CPU-speciﬁc values, whereby the CPU value is composed
of Operations/kWh and MFlops/Watt measurements.
D. Warnings and activity recommendations
As described in the aforementioned objectives, our so-
lution shall support decision making and action planning.
Therefore, there are activity recommendations that are pro-
posed by the management cockpit, marked by (2) in Fig-
ure 2. Those recommendations depend on the delta of
oblique and current values. One of the research questions
we have to investigate is the modeling and creation of those
recommendations; we describe this in the next section.
V. RESEARCH QUESTIONS AND PRELIMINARY RESULTS
To achieve our vision of the management cockpit, we have
identiﬁed the following research questions, which we will
analyze and answer in our future work.
A. Adequate nomenclature and classiﬁcation
There are a lot of different terms that are used in the
context of assessing, characterizing, and valuating the energy
efﬁciency as well as other characteristics of an IT infrastruc-
ture, for instance metric, measurement, quality, benchmark,
or key performance indicator (KPI). Even if there is some
literature about deﬁning those terms (e. g., [21], [13], [22],
[23], [24]) we have to deﬁne them by ourselves. Otherwise
there is the risk to compare and aggregate values with
different meanings and intentions, for instance a metric
value and a benchmark value. To achieve this goal, we
look at a metric as mathematical function and hence, it has
four main components: function domain, function image,
dependencies, and meta information. With this perspective,
we concentrate on the characteristics of the image or range
of those functions, for instance the scale, while sorting
existing terms and deﬁning our own terminology.
B. Considering metric dependencies
The area of metric dependencies is twofold: detection of
dependencies, and modeling of dependencies. Both areas
have individual characteristics and challenges, which have
to be analyzed. The detection of dependencies can be done
127
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-226-4
INFOCOMP 2012 : The Second International Conference on Advanced Communications and Computation

analytically or empirically. An analytical detection would
look at existing information about dependencies, for instance
a Conﬁguration Management Database (CMDB) [25], and
derive dependencies from those sources. An empirical de-
tection would collect all available data at different points
in time and compare them, for instance before and after
a reconﬁguration. After we have detected the dependencies,
we have to store them in an appropriate way, so a data model
is necessary, which must be capable of all the objectives
that were introduced in the last section. Beside the afore-
mentioned mathematical perspective, our data model divides
dependencies into reciprocity–dependencies and aggrega-
tion–dependencies: the ﬁrst one models correlations between
metrics – for instance, improving CPU energy efﬁciency
potentially decreases interconnect energy efﬁciency. The
second one models the aggregation of metrics to form new
statements (cf. Section IV). Interesting questions in the
context of the data model are, whether there are ﬁelds that
all metrics have in common, and if those ﬁelds could be
placed into an abstract metrics class. This would lead to a
very efﬁcient and handy data model.
C. Derivation and generation of new statements
In the next step, we have to shape the holistic view
out of the low-level approaches and thereby close gap 1.
Possible solution paths are bottom-up, hypothesis generation
on middle, and top-down. Bottom-up means that we use
existing data from low abstraction levels and try to aggregate
them iteratively until we reach the values that are displayed
in the management cockpit. The most difﬁcult task while
doing a bottom-up generation is the “correct” selection of
attributes/values at the lowest level. Hypothesis generation
means that we formulate hypotheses on an intermediate level
and try to prove or disprove those hypotheses by applying
data from low abstraction levels. Those (dis)proved hypothe-
ses are afterwards used to generate statements for the high-
level management-cockpit. Top-down means that we start at
certain points in the management cockpit and try to create
the data tree beginning at the root by recursively ﬁnding
suitable metrics on the next lower level. Our assumption is
that we have to analyze each of those three possible ways
and use a hybrid solution.
D. Target values and comparison
In order to provide “Warnings and activity recommen-
dations” (cf. Section IV-D), target values and interpretation
rules for a delta between those target values and current
values are mandatory. We have to investigate how to deﬁne
or rather ﬁnd those target values. This step is very critical,
because having wrong target values would lead to optimizing
the infrastructure towards wrong values. Additionally, we
have to analyze how to interpret a delta between the current
value and the target value for any given metric. This interpre-
tation has three dimensions: overall meaning, timing aspects
(e. g., “delta implies the necessity to act immediately”, “delta
is just for the annual, paper-based report”), and impact (e. g.,
“the severity of the delta is very high”, “solving the delta is
very costly”).
E. Modelling environment characteristics and their connec-
tions to metrics
The environment of an IT infrastructure can be very
challenging and inﬂuences the operation and assessment of
an IT infrastructure heavily. The strong impact is shown
by various empirical studies, for instance [26]. To respect
this fact, we want to model the environment characteristics
and their connections to metrics, respectively, in order to
consider this impact in the statements the management
cockpits produces. The questions that arise in this context
are the selection of environment characteristics that shall
be modeled, the design of the data scheme to store those
characteristics, and on which points those characteristics
shall be connected to metrics.
VI. CONCLUSION AND FUTURE WORK
We have shown that the energy efﬁciency metrics that are
in use today serve their purpose of benchmarking technical
components quite well, but their individual expressiveness
is insufﬁcient for IT management decisions on higher ab-
straction levels. Using the SuperMUC HPC system as an
example, we demonstrated that the information gap, the
dependency gap, the environment gap, and the activity gap
need to be overcome in order to gain a holistic view on the
energy efﬁciency of a complex IT infrastructure.
We then presented the core component of our approach,
the management cockpit, which integrates all technical as-
pects of organization-wide measurement and metrics man-
agement. By aggregating, processing, and transforming ex-
isting metrics, which then are visualized for different target
audiences, it is intended to be a central management tool for
monitoring and decision making support. Fully overcoming
the four identiﬁed gaps requires addressing several research
questions ﬁrst. We outlined them along with our solution
approach and our preliminary results.
Our next steps include the speciﬁcation of a generic met-
rics data model that can be applied to existing metrics and
also captures their interdependence. It will serve as a basis
for a prototype implementation to demonstrate the beneﬁts
of our approach in the SuperMUC real-world example. We
are also working on a generalization of our approach so
that besides energy efﬁciency, also performance, quality-of-
service, and security metrics can be managed and combined
in an integrated manner.
ACKNOWLEDGMENT
The authors wish to thank the members of the Munich Network
Management (MNM) Team for helpful discussions and valuable
comments on previous versions of this paper. [27].
128
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-226-4
INFOCOMP 2012 : The Second International Conference on Advanced Communications and Computation

REFERENCES
[1] Ludger Palm, “LRZ awarded German Data Centre Prize,”
Inside – Innovatives Supercomuting in Deutschland, vol. 10,
no. 1, 2012.
[2] Christos Kozyrakis and Parthasarathy Ranganathan and
Suzanne Rivoire and Mehul A. Shah, “JouleSort: a Balanced
Energy-Efﬁciency Benchmark,” in Proceedings of the Inter-
national ACM Conference on Management of Data (SIGMOD
’07), 2007.
[3] Christos Kozyrakis and Justin Meza and Parthasarathy Ran-
ganathan and Suzanne Rivoire and Mehul A. Shah, “Mod-
els and Metrics to Enable Energy-Efﬁciency Optimizations,”
Computer, vol. 40, no. 12, pp. 39–48, 2007.
[4] Robert Ayre and Jayant Baliga and Kerry Hinton and Wayne
V. Sorin and Rodney S. Tucker, “Energy Consumption in
Optical IP Networks,” Lightwave Technology, vol. 27, no. 13,
pp. 2391–2403, 2009.
[5] L. C. Graff and T. Talbot, “Verizon NEBS TM Compliance:
TEEER Metric Quantiﬁcation,” Tech. Rep., 2009.
[6] Sujata Banerjee and Priya Mahadevan and Parthasarathy
Ranganathan and Puneet Sharma, “A Power Benchmarking
Framework for Network Devices,” in Proceedings of the
8th International IFIP-TC 6 Networking Conference (NET-
WORKING ’09), 2009.
[7] Luc Ceuppens and Daniel Kharitonov and Alan Sardella,
“Power Saving Strategies and Technologies in Network
Equipment Opportunities and Challenges, Risk and Rewards,”
in Proceedings of the International IEEE Symposium on
Applications and the Internet (SAINT 2008), 2008.
[8] A. Alimian and B. Nordman, “Network and Telecom Equip-
ment - Energy and Performance Assessment – Test Procedure
and Measurement Methodology,” Tech. Rep., 2008.
[9] “Energy Efﬁciency for Network Equipment: Two Steps be-
yond Greenwashing,” Juniper Networks, Inc., Tech. Rep.,
2010.
[10] Ronda Henning and Ambareen Siraj and Rayford B. Vaughn,
Jr., “Information Assurance Measures and Metrics - State of
Practice and Proposed Taxonomy,” in Proceedings of the 36th
International IEEE Hawaii Conference on System Sciences
(HICSS03), 2003.
[11] Samee U. Khan and Lizhe Wang, “Review of Performance
Metrics for Green Data Centers: a Taxonomy Study,” The
Journal of Supercomputing, 2011.
[12] Aruna Prem Bianzino and Anand Kishore Raju and Dario
Rossi, “Apples-to-Apples: a Framework Analysis for Energy-
Efﬁciency in Networks,” ACM SIGMETRICS Performance
Evaluation Review, vol. 38, no. 3, pp. 81–85, 2010.
[13] S. C. Payne, “A Guide to Security Metrics,” SANS Institute,
Tech. Rep., 2006.
[14] Viktoria Firus and Ralf Reussner, “Basic and Dependent Met-
rics,” in Proceedings of the Dependability Metrics: Advanced
Lectures [result from a Dagstuhl seminar], 2008.
[15] Victor R. Basili and David M. Weiss, “A Methodology for
Collecting Valid Software Engineering Data,” in Proceedings
of the 6th IEEE Transactions on Software Engineering, 1984.
[16] Eduardo Fernndez-Medina and Mario Piattini and Carlos
Villarrubia, “Metrics of Password Management Policy,” in
Proceedings of the International Conference on Computa-
tional Science and Its Applications (ICCSA 2006), Part III,
2006.
[17] Rudolf Ramler and Klaus Wolfmaier, “Issues and Effort in In-
tegrating Data from Heterogeneous Software Repositories and
Corporate Databases,” in Proceedings of the 2nd International
ACM/IEEE Symposium on Empirical Software Engineering
and Measurement (ESEM ’08), 2008.
[18] Daniel Mares and Alessia C. Neuroni and Reinhard Riedl
and Christoph Schaller and Sauter Urs, “Cockpits for Swiss
Municipalities: a Web Based Instrument for Leadership,” in
Proceedings of the 11th International ACM Digital Govern-
ment Research Conference on Public Administration Online:
Challenges and Opportunities (dg.o ’10), 2010.
[19] Klaus R. Dittrich and Patrick Ziegler, “Three Decades of Data
Integration - All Problems Solved?” in Proceedings of the
18th World Computer Congress on Building the Information
Society (IFIP), 2004.
[20] David M Pennock and Alexandrin Popescul and Andrew
I. Schein and Lyle H. Ungar, “Methods and Metrics for
Cold-Start Recommendations,” in Proceedings of the 25th
International ACM Conference on Research and Development
in Information Retrieval (SIGIR ’02), 2002.
[21] Horst Lichter and Holger Schackmann, “Process Assessment
by Evaluating Conﬁguration and Change Request Manage-
ment Systems,” in Proceedings of the Warm Up Workshop
for ACM/IEEE ICSE 2010 (WUP ’09), 2009.
[22] Reijo Savola, “Towards a Security Metrics Taxonomy for
the Information and Communication Technology Industry,”
in Proceedings of the International IEEE Conference on
Software Engineering Advances (ICSEA 2007), 2007.
[23] John I. Alger, “On Assurance, Measures, and Metrics: Deﬁ-
nitions and Approaches,” in Proceedings of the Workshop on
Information-Security-System Rating and Ranking (WISSSR),
2002.
[24] “System Security Engineering - Capability Maturity Model
– Model Description Document (Version 3.0),” Tech. Rep.,
2003.
[25] S. Knittl, “Werkzeugunterst¨utzung f¨ur interorganisationales
IT-Service-Management - ein Referenzmodell f¨ur die Erstel-
lung einer ioCMDB,” Ph.D. dissertation, Technische Univer-
sit¨at M¨unchen (TUM).
[26] Bin Gu and Gautam Ray and Ling Xue, “Environmental
Uncertainty and IT Infrastructure Governance: A Curvilinear
Relationship,” Information Systems Research (INFORMS),
vol. 22, no. 2, pp. 389–399, 2011.
[27] “Munich
Network
Management
Team
(MNM),”
www.
mnm-team.org, 2012.
129
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-226-4
INFOCOMP 2012 : The Second International Conference on Advanced Communications and Computation

