Signal Processing-based Anomaly Detection
Techniques: A Comparative Analysis
Joseph Ndong
Universit´e Pierre et Marie Curie
LIP6-CNRS, France
Email: joseph.ndong@lip6.fr
Kav´e Salamatian
Universit´e de Savoie Chambery Annecy
LISTIC PolyTech, France
Email: kave.salamatian@univ–savoie.fr
Abstract—In this paper, we present an analysis for anomaly
detection by comparing two well known approaches, namely the
Principal Component Analysis (PCA) based and the Kalman
ﬁltering based signal processing techniques. The PCA-based
approach is coupled with a Karuhen-Loeve expansion (KL) to
achieve higher improvement in the detection performance; on
the other hand, based on a Kalman ﬁlter, we built a new
method by combining statistical methods such as: gaussian
mixture and a hidden markov modellers, which allows us to
obtain performances better than those obtained with the PCA-KL
expansion method. For this newer method, our approach consists
of not assuming anymore that the Kalman innovation process
is gaussian and white. In place, we are assuming that the real
distribution of the process is a mixture of normal distributions
and that, there is time dependency in the innovation that we will
capture by using a Hidden Markov Model. We therefore derive
a new decision process and we show that this approach results
in an considerable decrease of false alarm rates. We validate
the two comparative approaches over several different realistic
traces.
Index
Terms—Anomaly
Detection,
Monitoring
System,
Kalman ﬁlter, GMM, HMM
I. INTRODUCTION
The literature of the recent years has used two fundamental
classes of monitoring techniques, to implement anomaly detec-
tion techniques for networking applications: PCA-based and
Kalman-ﬁlter based methods. An anomaly detector consists
essentially of two components: (i) an entropy reduction com-
ponent and (ii) a decision component applying statistical tests
to a decision variable issued from the ﬁrst step. The entropy
reduction step is here, to simplify the second step. When
using statistical signal processing based techniques, entropy
reduction is obtained by a predictive model that uses a model
of normal behavior to forecast the values of the parameters to
monitor. The prediction error obtained after ﬁltering out the
normal behavior model prediction has a smaller entropy than
the initial signal, resulting in a considerable entropy reduction.
A decision variable is thereafter derived as a function of the
prediction error and fed to the decision stage. In the decision
stage a statistical test is applied to the decision variable and
an anomaly is detected if this variable exceed a threshold.
In PCA-based method, the predictive model used in the
entropy reduction step is built by using a projection in a
low dimensional orthogonal sub-space, that minimizes the
approximation error. This subspace is derived using Principal
Component Analysis (PCA) or more precisely a Karhunen-
Loeve Transform (KLT). The decision variable in PCA-based
techniques is obtained as, a square sum of the prediction errors
made by projecting the observed signal in the PCA deﬁned
subspace (see [1], [2] for a detailed description).
Kalman-ﬁlter based techniques ﬁrst, calibrate a Maximum-
Likelihood based model for normal behavior modeling for
the entropy reduction step. Thereafter the decision variable is
obtained as the innovation process at the output of a Kalman-
ﬁlter, that ﬁlters the normal behavior component from network
observations [3].
In the two above cited methods, under the conditions that
the observed signals follow a jointly gaussian distribution,
the decision variable is known to converge to a zero mean,
gaussian and white (uncorrelated) signal . However, realistic
network trafﬁc is well known to not follow a gaussian dis-
tribution. Moreover, anomalies can break the stationarity of
the decision signal when they happen generally. These last
two problems result in divergence from the basic assumptions
(gaussianity) under which classical anomaly detectors are
built. Therefore, the error probabilities bounds predicted by
assuming a gaussian distribution at the decision variable are
not tight enough to be used for a robust anomaly detection
test. Prior work
[1], [2], [4] have shown that, even after
careful calibration of the normal behavior model, the decision
variables still exhibit non-gaussian and correlated behaviors.
This last problem explains the high false alarm rate observed
when using classical anomaly detection approaches.
The fundamental issue in the area of anomaly detection in
networks is the false alarm rate vs. detection rate trade-off.
This trade-off is represented by the ROC (Receiver Operating
Characteristics) curve [4]. In operational settings, one would
like to attain a high detection rate (larger than 90%) with the
lowest false alarm probability. The main aim of this paper is
twofold: to see between the two methods, PCA or Kalman
ﬁltering, which one is the most robust towards deviation
from the gaussianity assumptions, and to improve the false
alarm versus detection rate trade-off by making a more robust
anomaly detection decision. This will allow us to compare the
two above methods and to show that the method based on
Kalman ﬁltering performs best.
The contribution of this paper is a careful comparative
analysis of the decision step in PCA and Kalman-ﬁlter based
32
INTERNET 2011 : The Third International Conference on Evolving Internet
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-141-0

anomaly detectors. Based on this analysis, we are proposing
one approach to improve the robustness of the decision step.
This approach is based on accepting that the distribution of
the decision variable at the output of the anomaly detector is
not an uncorrelated gaussian process. We will rather assume
that, the decision variable follows a distribution that can be
modelled by a Gaussian Mixture Model (GMM) with a small
number of components.
The residual temporal correlation in the decision variable
is modelled by a Hidden Markov Model (HMM) deﬁned on
the sequence of component index of the GMM model. Since
anomalies might be rare, we assume that they might happen in
some (but not all) components of the GMM. This means that,
by inferring the states of the HMM calibrated on the residual
variable, one is able to decide if an anomaly has happened.
The above model is not the only one that is able to capture
the deviation from gaussian hypothesis as well as the residual
correlation in the decision variable. However, we will show in
this paper that the crude and relative complexity of the model
is powerful enough to result in a considerable decrease in false
alarm rate for a given detection ratio.
In summary our proposed robust decision scheme differs
from previous work in the ﬁeld of network anomaly detection
in at least two ways: i) we do not assume that the innovation
process does strictly remain a zero mean gaussian process.
Instead, we assume that the real distribution of the process is
a mixture of gaussians, and residuals can be split in different
mixture components (where anomalies might or might not
happen), ii) we use hidden markov models over the different
mixture components to capture residual time dependencies
that can be relevant to anomaly detection. Therefore, anomaly
decision is not done as usual, using a simple threshold based
decision but rather in a two step approach: we ﬁrst use a
Viterbi algorithm to estimate the Maximum a posteriori (MAP)
estimates of the state sequence of the HMM; clearly the Viterbi
path will select some of the HMM states (i.e some of the
GMM components, because each HMM state contains part of
the GMM components) able to capture all the low and high
variations in the decision variable; thereafter we only apply
thresholding in these selected states.
The organization of this paper is as follows. Section 2
describes the monitoring system we used. Section 3 deals with
the methodology we adopt in our anomaly detection scheme.
In section 4, we detail our calibration method and we validate
our approach by showing efﬁcient results, which we compare
with the results obtained by the KL-PCA method. Section 5
concludes our work and ﬁx some ideas for future works.
II. ARCHITECTURE OF THE MONITORING SYSTEM
We are assuming a novel network monitoring system shown
in Figure 1. The monitoring system contains three functional
blocks: data collection (by a NOC-Network Operations Cen-
ter), a data analysis block and a decision process phase. The
data collection block is responsible for receiving the ﬂow of
microscopic measurement coming from network equipment
in form of SNMP or Netﬂow/IPFIX ﬂows. This ﬂow is
Fig. 1.
A new monitoring system combining a Kalman ﬁlter for entropy
reduction, a GMM for clustering, a HMM for time dependencies learning
and ﬁnally the use of the Viterbi algorithm for decision manage.
aggregated and transformed to a vector of time sequences that
are fed to other blocks. The Data analysis block calibrates
the normal behavior model, by applying machine learning
techniques like PCA or Maximum Likelihood analysis and
generates the decision variable by ﬁltering the expected normal
behavior from the observations. The last block implements
the decision process. Classically, the decision stage consists
simply of a comparison with a threshold, i.e. if the decision
variable exceeds the threshold, an anomaly is detected.
We propose in this paper a more elaborate decision scheme
containing three new components shown in Fig.
1. These
three new stages need to calibrate two models: a GMM to
account for deviation from gaussian distribution of the decision
variable and a HMM to integrate the residual temporal correla-
tion. The GMM is calibrated over a learning set of continuous
valued decision variable time series coming directly from
observations. Clearly, using the multi-dimensional Kalman
innovation process, we form a one-dimensional residual vector,
which we put as input of the GMM to build a family of gaus-
sians. These normal distributions will be later transformed into
discrete sequences using a Maximum A Posteriori criterion.
Thereafter the HMM is calibrated over the discrete sequence
of GMM Components membership, for time dependence learn-
ing. The three new components consists of a MAP (Maximum
A Posteriori) phase that maps each observation to an index of
the discrete mixture using a Maximum a Posteriori probability;
thereafter a Viterbi algorithm is used to detect and select the
state of the decision variable. Finally in the third and last step,
threshold based anomaly detection is only applied if we are
in these states detected by the Viterbi path. We will describe
in detail these three steps in the forthcoming.
III. METHODOLOGY
A. Normal behavior modeling
The ﬁrst step is to seek for a normal behavior model
that captures trafﬁc dynamics. For this sake, two classes of
models have been proposed in the literature: PCA-based model
33
INTERNET 2011 : The Third International Conference on Evolving Internet
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-141-0

and state-space model. In PCA-based method, we build the
predictive model assuming the projection in an orthogonal
subspace obtained through application of PCA in a good
predictor of the signal. We refer the reader to a precise
description of this class of model in [1]. State-space model is
a classical approach to model dynamical signal and is shown
in Eq. 1.
½
Xt+1=CtXt + Wt
Yt = AtXt + Vt
(1)
The model contains two equations: the ﬁrst describes the dy-
namic of state variations and the second one the measurement
dynamics that may result, in not directly observing the states
but rather a linear combination of them. The noise Wt accounts
for intrinsic noise in the state variations as well as modeling
errors; the noise Vt accounts for measurement errors. Both are
assumed to be uncorrelated zero-mean gaussian white-noise
processes with covariance matrices Qt and Rt, respectively.
This class of models can be used to model a very large class
of signals and in particular, they can be calibrated to model
any signal if the number of states is large enough [4].
Normal behavior model calibration can be done for PCA
models using the diagonalisation of the observation covari-
ance matrix [1]. The calibration of the state-space model is
presented in [4].
B. Decision variable generation
After calibrating the normal behavior model over a learning
set, we can use this model to generate the decision variable.
The decision variable in PCA is derived as the square sum of
the prediction error, where the prediction error is derived as
the difference between the observation and the projection in
the PCA-based subspace; the reader can refer to [1] for an in
detail analysis of the decision variable generation for PCA.
For state-space based approach, the decision variable is ob-
tained through the application of a Kalman ﬁlter. The decision
variable is derived as the sum of squares of the innovation
process weighted by its variance. When the state space is a
n-dimensional, the resulting decision variable becoming a χ2
random variable with n degree of freedom. When n is large
enough, this converges to a gaussian random variable with
mean n and variance 2n.
C. Kalman ﬁlter equations
The ﬁrst problem to solve after building a simple model to
monitor features (link counts and TCP/UDP metrics), is to ﬁnd
an optimal estimate ( ˆXt) of our unobservable network states
Xt, given a set of past and current observations {Y1, ....., Yt}.
To estimate the state of the system using only all information
until time t, a robust method is the Kalman ﬁxed-interval
ﬁltering algorithm. From the dynamical linear system, we refer
to Yt as the observation vector at a speciﬁc time t. And the state
of the system at time t is given by Xt; let also ˆXt|k denotes the
estimate of Xt using all the information available up to time k,
i.e, ∀τ < k. ˆXt+1 denotes the estimate of Xt+1 using all the
information up to time t, (this constitutes the phase predictor).
The quantity ˆXt+1|t+1 denotes the estimate of Xt+1 using
all past information and the recently arrived data point at
time t+1. On the other hand, Pt|t denotes the variance of the
state estimate and Pt+1|t indicates the variance of the state
prediction. As it is shown in its earlier elaboration, the Kalman
ﬁlter addresses the problem of estimating a discrete state
vector when the observations are only a linear combination
of the underlying state vector. As an iterative algorithm, it
estimates the system state using two steps: the ﬁlter runs as
a predictor-corrector algorithm. Prediction comes in the time
update phase, and correction in the measurement update phase.
• Prediction step (time update equations):
In this step, the estimated state of the system at time
t,
ˆXt|t, is used to predict the state at next time t+1,
ˆXt+1|t. And, as we know that the noise Wt inﬂuences
the evolution of the system at each time t, we compute
only the variance of the prediction, Pt+1|t based on the
updated variance at the previous time t, Pt|t, and the noise
covariance at the same time, Qt. The error covariance
Pt+1|t provides an indication of the uncertainty associ-
ated with the state estimate.
½ ˆXt+1|t=
Ct ˆXt|t
Pt+1|t=CtPt|tCT
t + Qt
(2)
• Correction step (measurement update equations):
This step updates (corrects) the state and the variance of
the estimate in the previous step, using a combination
of their predicted values and the new observations Yt+1.
The accuracy of this update depends on the Kalman
innovation Yt+1 − At+1 ˆXt+1|t.





ˆXt+1|t+1 = ˆXt+1|t + Kt+1(Yt+1 − At+1 ˆXt+1|t)
Pt+1 =(I − Kt+1At+1)Pt+1|t(I − Kt+1At+1)T
+ Kt+1Rt+1KT
t+1
(3)
In the measurement equations, Kt+1 denotes the Kalman gain.
For more details in linear dynamical system, estimation and
Kalman ﬁltering techniques, we refer the reader to : [5],[6],[7]
and [8]. The above equations with initial conditions of the
state of the system ˆX0|0 = E[X0] and the associated error
covariance matrix P0|0 = E[( ˆX0−X0)( ˆX0−X0)T ] deﬁne the
discrete-time sequential recursive algorithm for determining
the linear minimum variance estimate known as the Kalman
ﬁlter.
D. Gaussian Mixture Model
In Kalman ﬁltering philosophy, it is more generally assumed
that the residual remains a zero mean gaussian process,
however this assertion is not always true in practice. Thus our
motivation in using gaussian mixture model is based on our
belief that the real distribution of the process is an ensemble
(mixture) of gaussians and there is some time dependency in
the innovation (which we will later study by using hidden
markov model theory). This assertion allows us to build a
method, which has the ability to ﬁnd anomalies in different
families built on a one-dimensional residual process. The
GMM takes as input the 1-dimensional residual vector we
34
INTERNET 2011 : The Third International Conference on Evolving Internet
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-141-0

formed with the multi-dimensional Kalman innovation pro-
cess, and ﬁnds a few number (K) of families (clusters). Each
gaussian component is ﬁxed by its ﬁrst order statistic. The
variance obtained for each component will help to determine
the suitable number of K. Each cluster contains data coming
from one gaussian component. Thereafter, we aim to convert
each cluster into discrete sequence of symbols (1,2,3,...) by
means of a MAP criterion. In the next step, we propose the use
of an hidden markov model (HMM) to classify these discrete
groups into P states. Each hidden state could probably contain
mixing symbols yielding in different clusters. This operation
has the main advantage to discover the potential temporal
dependencies in the innovation process.
Technically, to ﬁnd the values of the model parameters µm
and Σm , as well as the prior probability vector π, we are
interested in maximizing the likelihood £(θ|X) = p(X; θ)
of generating the known observed data (X) given the model
parameters θ = {µm, Σm, πm}, 1≤m≤M. X denotes all the
observation while θ contains all the parameters of the mixture.
In other words, we hope to ﬁnd ˆθML=argmax p(x|θ). This
approach is called the Maximum Likelihood (ML) framework
since it ﬁnds the parameter settings that maximize the likeli-
hood of observing the data sets. To ﬁnd the best parameters
of the features of θ, the Expectation Maximization (EM)
iterative algorithm can be used to simplify the mathematical
routines considerably and numerically compute the unknown
parameters. In the E-step (Expectation phase), the parameters
are estimated given the observed data and current estimates of
the model parameters (i.e EM comes with an initial guess of
the model parameters, µm, Σm and πm; we have used the K-
means algorithm). In the M-step (Maximization phase), EM
takes the expected complete log-likelihood and maximizes
it w.r.t. the parameters to estimate (πm, µm, Σm). For more
details about mathematical routines for EM, see [9],[10],[11].
E. Hidden Markov Model
From the above learning phase, each GMM component
is transformed into a sequence of a ﬁnite set of alphabet
(symbols as 1,2,3,...), using a maximum a posteriori crite-
rion. This discrimination phase will help for plugging the
above clusters into different a priori unknown states, using
hidden markov model. We represent these families of states
by the following collection of unknown random variables
{Q1, Q2, .....QT } (where Qt is a constant value with values
in {1, 2, ..., K}). We also represent our alphabet by the known
vector {O1, O2, ....., OT }. Now, the problem is resumed to ﬁnd
a model to produce the states and to determine the probability
of each symbol being in a state.
A well known model-based approach to tackle this problem,
is the discrete hidden markov model (HMM) approach. Our
choice of using HMM is based on the fact that: i) potential
time dependencies in the innovation process can be modelled
and captured using a ﬁnite set of a priori hidden states,
each of them containing a subset of gaussian components
ii) relatively efﬁcient algorithm can be derived to solve the
problems related to them, [9], [10], [12]. The full HMM model
we used is deﬁned by the quantity λ = (A, B, π) (where A
is the transition matrix, B is the emission probabilities matrix
and pi the prior probabilities).
To ﬁnd and estimate the best parameters of our model, we
use the well-known forward-backward algorithm parameter
estimation (or Baum-Welch algorithm). For more details for
EM techniques related to hidden markov model, see, [9], [12].
Thereafter, we reuse the model to ﬁnd the optimal state
sequence associated with the given observation sequence. We
believe that this ﬁnal step of our approach will allow us to
capture all the variations in the innovation process. An optimal
criterion we have chosen here is to ﬁnd the single best state
sequence (path), Q = {q1, q2, ..., qT } for the given observation
sequence O = {O1, O2, ..., OT },i.e., we aim to maximize
P(Q|O, λ). A formal technique for ﬁnding this unique best
state sequence is the Viterbi algorithm. The Viterbi algorithm
will ﬁnd a unique path (containing some of the symbols)
witch will be able to capture all the variations in the decision
variable.
IV. MODEL EVALUATION
A. Experimental Data
In this work we used two kinds of data coming from two
different networks: the Abilene and the SWITCH networks.
The Abilene backbone has 11 Points of Presence(PoP) and
spans the continental US. The data from this network was
collected from every PoP at the granularity of IP level ﬂows.
The Abilene backbone is composed of Juniper routers whose
trafﬁc sampling feature was enabled. Of all the packets enter-
ing a router, 1% are sampled at random. Sampled packets are
aggregated at the 5-tuple IP-ﬂow level and aggregated into
intervals of 10 minute bins. The raw IP ﬂow level data is
converted into a PoP-to-PoP level matrix using the procedure
described in [2]. Since the Abilene backbone has 11 PoPs, this
yields a trafﬁc matrix with 121 OD ﬂows. Note that each trafﬁc
matrix element corresponds to a single OD ﬂow, however, for
each OD ﬂow we have a seven week long time series depicting
the evolution (in 10 minute bin increments) of that ﬂow over
the measurement period. All the OD ﬂows have traversed 41
links. Synthetic anomalies are injected into the OD ﬂows by
the methods described in [2], and this resulted in 97 anomalies
in the OD ﬂows.
The second collection of data we used for our experiments
is a set of three weeks of Netﬂow data coming from one of the
peering links of a medium-sized ISP (SWITCH, AS559),[13],
[1]. This data was recorded in August 2007 and comprise a
variety of trafﬁc anomalies happening in daily operation such
as network scans, denial of service attacks, alpha ﬂows, etc.
For computing the detection metrics, we distinguish between
incoming and outgoing trafﬁc, as well as TCP and UDP
ﬂows. For each of these four categories, we computed seven
used trafﬁc features: byte, packet, ﬂow counts, source and
destination IP address entropy, unique source and destination
IP address counts. All metrics were obtained by aggregating
the trafﬁc at 15 minute intervals resulting in 28x96 data matrix
per measurement day. Anomalies in the data were identiﬁed
35
INTERNET 2011 : The Third International Conference on Evolving Internet
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-141-0

using available manual labelling methods, as visual inspection
and time series and top-n queries on the ﬂow data. This
resulted in 28 detected anomalous events in UDP and 73
detected in TCP trafﬁc. The SWITCH data was collected in
a single link and the observed metrics are correlated in time
and in space.
B. Validation
1) Tuning of the System parameters.: Our method begins
with a learning phase where we calibrate a Kalman ﬁlter for
denoising, using our linear dynamical system ( 1), and the
collection of observation data . In order to run the Kalman
ﬁlter, we need the state and measurement matrices A, Q,
C and R. For the Abilene data, the matrix A is available
given the routing scheme of a network. We thus only need
to obtain Q, C and R. It is sufﬁcient to learn the model’s
parameters using a sample of one week measurements. We
ﬁnd the tuning parameters , using two days of consecutive
samples of all link counts, and we do our calibration using
the EM algorithm developed in [14],[15] and implemented (in
R language) in [16]. In Practice to run and ﬁnd the GMM and
HMM parameters, we had used the HMM toolbox [17].
The way to ﬁnd the tuning parameters for the Kalman ﬁlter
for the SWITCH data, is slightly different from the case of the
Abilene case, because for these data, we don’t know a priori
the matrix A and it should be estimated. For this case, we
used the EM algorithm developed in [18], and implemented
(in Matlab) in [19].
To build the model for the PCA and the KL expansion,
we use the vector of metrics X[1 : 192] and the vector of
link counts X[1 : 288] containing the ﬁrst two days of data,
respectively for the SWITCH network and for the Abilene
backbone. And we follow exactly the method described in
[1] (based on pole-zero diagram) to choose the components
number to be included in the model.
To apply the KL expansion, the SVD (Singular Vector
Decomposition) technique is applied to the data (resulting
to a basis change matrix) and the squared prediction error
Q[k] = e[k]T e[k] (e[k] is the residual process)is computed
from, which the decision variable D[k] is calculated. Thus,
using the multi-dimensional data (all vectors of metrics and
all vectors of link counts), we obtained the one-dimensional
array D[k] with, which we perform anomaly detection.
As for the KL expansion, the same samples of metrics and
link counts are used for the calibration of the Kalman ﬁlter.
Thereafter we use the multi-dimensional innovation process
to form a one-dimensional innovation process used to perform
anomaly detection. To obtain this one-dimensional process, we
take into account the variance of the residual obtained after
running the Kalman ﬁlter and we built a new process using
the formulas: enew(t) = e(t)T V e(t), where V is the inverse
of the variance of the innovation process, e(t) is the multi-
dimensional innovation process, and T denotes the transpose.
This space reduction for the residual process allows to perform
a good comparison between the PCA-KL expansion method
and our method based on Kalman ﬁltering. On the other hand,
performing anomaly detection in a single one-dimentional
residual vector is more simple and less complex than analysing
a multi-dimensional array.
2) Summary of the results.: The ﬁrst result to show is the
ability of our method to track the behavior of link counts for
the Abilene data (total byte per unit time) and the behavior
of the different TCP and UDP metrics for the SWITCH data,
over time. In Figure
2, we show the real and inferred link
counts (Abilene) for our model. The evolution of the trafﬁc
and estimates are shown for a seven weeks duration for each
observation vector. The calibration is applied only once. In
Figure 3, we show the results obtained using SWITCH data
for the TCP metrics; here too, the calibration is done once a
time, for a three weeks of measurements. The results for the
UDP metrics are quite similar.
Now we are looking at the compared performances of
our two methods, for the Abilene network and also for the
SWITCH network. First, to validate our model, one has to ﬁnd
the suitable number of components in the GMM. To do this,
we calibrate a GMM with a set of r components (r=2,3,4,5...)
using the EM algorithm as described above, and the decision
to select the best model (i.e the suitable r) is done by analysing
the variance performed for each component in the mixture. We
compare the results obtained for the different values of r and
the model with the lowest variance is chosen.
We have found that the data residual can be organized into
three (r=3) distinct clusters for the Abilene data, and into ﬁve
(r=5) clusters for the SWITCH data. Thereafter a maximum a
posteriori criterion is used to build, taking as input the clusters,
a ﬁnite alphabet of symbols, where we perform the hidden
markov modeller. To train the hidden markov model, one must
ensure that the different hidden states are clearly distinct. This
means that one should have a transition matrix with higher
probabilities in its main diagonal. For the datasets we used,
we have discovered that an hidden markov model with two (2)
states were able to capture the temporal dependencies in the
datasets. We show below only the transition and observation
matrices for the Abilene case:
transmat =
· 0.9662
0.0338
0.0162
0.9838
¸
obsmat =
·
0.0113
0.0020
0.9867
0.4092
0.5904
0.0004
¸
.
These results show clearly that, the state 1 is composed
with almost entirely the symbol#3 and the state 2 is a
mixture of the two remaining symbols (1 and 2) with 41% of
probability of presence of symbol#1 and 59% for symbol#2.
In the philosophy of anomaly detection theory, generally
it is assumed, that anomalies might be rare; base on this
assumption we can ask this question: if anomalous events
occur, do they might come from state #1 or state #2 or both?
It is not obvious to answer this question, but we believe that
all the changes in the mean of the residual (abrupt changes,
lower or higher variations) can be tracked by a combination
of symbols yielding in the different states. In other words,
one can think that some (but not all) a priori unknown
states could be classiﬁed as normal states and the others as
36
INTERNET 2011 : The Third International Conference on Evolving Internet
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-141-0

−3
−2
−1
0
1
2
3
Time
Payload (Byte)
link#3
−3
−2
−1
0
1
2
3
4
link#15
Time
−3
−2
−1
0
1
2
3
link#16
Time
Payload (Bytes)
−3
−2
−1
0
1
2
3
link#17
Time
Real
Estimated
Fig. 2.
Real(red) and estimated (blue) link counts obtained using Kalman ﬁlter.
−10
−5
0
5
10
Time
Payload (Byte)
−3
−2
−1
0
1
2
3
4
Time
Entropy  address src
−20
−15
−10
−5
0
5
Time
Entropy address dst
−30
−20
−10
0
10
20
30
Time
Number of packets
Real
Estimated
Fig. 3.
Real(red) and estimated (blue) TCP metrics obtained using Kalman ﬁlter.
abnormal states. If one can ﬁnd a combination of states to
track all the lower, higher or/and abrupt variations in the
decision variable, these states will participate in the detection
of anomalous events when they occur. And then these states
will be etiquetted as abnormal, and one should take attention
to them. The remaining states that don’t participate in the
tracking operation will then be labelled as normal. Recall that
PCA and Kalman-based anomaly detection techniques analyze
residuals to perform the detection issue. So, if anomalies occur,
they will appear in the residual process either in the form of
low or high variations or in the form of abrupt changes in
the mean of the residual process. We believe that it will be
interesting to divide the residual into two parts corresponding
to the low and high variations, and separately apply thresholds
for each part. To conﬁrm our intuition, we run the Viterbi
algorithm for all the sequences of discrete alphabet and we
obtain one unique sequence (path) composed by only the
symbols in the state #2.
At this time, we can argue that, if anomalies exist they
might be caught either by the two symbols simultaneously,
or by one symbol only. One can observe that in Fig.
4, all
the variations in the mean of the decision variable can be
caught by these two symbols. The top graph corresponds to
TCP, the middle to UDP and the bottom graph to Abilene.
To track the anomalies, one has just to extract the residual
corresponding to the symbol #1 and extract also the part of
residual corresponding to symbol #2 and apply to each part
thresholds. We reused the methods described in [1] to obtain
these thresholds. In addition, in our study we discover that
the injected anomalous events never evolve in the cluster with
mean closely equal to zero(namely the cluster corresponding
to symbol #3).
Another result is about the performance obtained in ana-
lyzing the trade-off between false positive and false negative
rates. We examine the entire trafﬁc for each method, namely
the PCA-KL and the Kalman-based approaches. We then
can compute one false positive percentage and one false
negative percentage for each threshold conﬁguration scheme.
The performances of the two methods on the Abilene and the
SWITCH data are depicted in the ROC (Receiver Operating
Characteristic) curve of Figures 5, 6 and 7. For all of the
results shown on these ﬁgures, one can see clearly that the
method based on the Kalman ﬁtering techniques, the gaussian
mixture model and the hidden markov model performs better
than the KL-PCA method, when the temporal correlation range
is set to N=1,2,3. For the TCP trafﬁc (SWITCH network),
we obtain in Figure
5, 90% of detection rate with 8% of
probability of false alarm, for KL expansion with N=2, while
we have only 3.5% of false alarm rate with the same detection
probability for our new method with N=2, and for N=3, we
obtain 2.5% of false positive rate. We observe that, with the
newer approach, the false alarm ratio decreases signiﬁcantly
37
INTERNET 2011 : The Third International Conference on Evolving Internet
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-141-0

−5
0
5 x 107
Time
residual
−5
0
5 x 107
residual
−2
−1
0
1
2 x 108
Time
residual
symbol  2
symbol 1
Fig. 4.
Tracking the low and high variations in the decision variable by combination of mixing symbols extracted by the Viterbi algorithm.
for all values of N. In the same ﬁgure we note that the
newer method can achieve 100% (0% of miss detection) of
probability of detection with 3.2% of false positive rate while
the KL expansion exhibits the same detection rate with 14% of
false alarm rate. Also, for the UDP trafﬁc, we obtain in Figure
6, for the KL expansion method for N=2, 96% of detection
rate with 7% of false alarm rate versus 2.4% (for N=3) of
false alarm rate with the same probability of detection. As for
the TCP case, here too, our new method can achieve 100% of
detection rate with 2.5% of false positive (N=3) when the KL
expansion shows 12% of false positive with the same detection
rate (for N=2).
For the Abilene network, we conﬁrm the improvement in the
performance of our method above the PCA-KL expansion.
One must clearly observe, in Figure 7, that the KL expansion
(N=2) shows 100% of probability of detection with 13% of
false positive rate while the new method exhibits a false alarm
rate of 5% (N=3) with the same detection rate. In summary, the
ROCs curve exhibit different points, which can be served as
good references to show the high performance we gain above
the KL method.
10
−2
10
−1
10
0
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive
Detection Rate
ROC curve for TCP
Kalman N=3
Kalman N=1
KL N=2
KL N=3
KL N=1
Kalman N=2
Fig. 5.
ROC curve using SWITCH data (TCP)
V. CONCLUSION
In this work we proposed a profound analysis allowing
us to show that an anomaly detection technique combining
a panoply of different methodologies and based on Kalman
ﬁltering perform better than the PCA technique, which the
performance is highly improve by the use of the Karuhen-
Loeve expansion. Using a multi-dimensional residual process
for each kind of network data, we built a one-dimensional
innovation process used as a decision variable, and the com-
parison of the two schemes is done by analyzing the trade-off
between false positive rate and the probability of detection. We
found that the use of the Viterbi algorithm as a ﬁnal tool to
make possible to split the one-dimensional decision variable
into several subset where we applied different thresholds is
an important discovery. It has make possible to track the
variability in the residual process using only a combination
of symbols yielding in one state. The main drawback of
38
INTERNET 2011 : The Third International Conference on Evolving Internet
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-141-0

10
−3
10
−2
10
−1
10
0
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive
Detecttion Rate
ROC curve for UDP
Kalman N=3
Kalman N=1
KL N=2
KL N=3
KL N=1
Kalman N=2
Fig. 6.
ROC curve using SWITCH data (UDP).
10
−3
10
−2
10
−1
10
0
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate
Detection Rate
ROC Curve
Kalman. N=3
Kalman N=1
KL N=3
KL N=2
KL N=1
Kalman N=2
Fig. 7.
ROC curve using Abilene data.
this method is the relative complexity introduced by the use
of a gaussian mixture model followed by using an HMM,
for time dependencies tracking. However, we obtained good
performance by reducing considerably the false alarm rate.
We had shown in this study that for the two kind of data, the
temporal dependencies can be tracked with a hidden markov
model with a few number of states (each state being composed
by parts of the GMM component). At the other hand, when
applying the Viterbi algorithm, we discover that all anomalies
are detected in one state with only components 1 and 2, no
anomaly were found in the remaining components (number 3
for Abilene and number 3 ,4 and 5 for SWITCH network). The
state where no anomaly has been found is the one containing
the clusters with mean closely equal to zero. At this moment,
one could ask a question about the potential uncertainties
about the possibility of these remaining components to capture
anomalies in some situations.
REFERENCES
[1] Brauckhoff, D., Salamatian, K. and May, M.: Applying PCA for Trafﬁc
Anomaly Detection: Problems and Solutions. Proceedings IEEE INFO-
COM 2009, pp. 2866-2870.
[2] Lakhina, A., Crovella, M. and Diot, C.: Characterization of network-
wide trafﬁc anomalies. In Proceedings of the ACM/SIGCOMM Internet
Measurement Conference (2004). pp. 201-206.
[3] Soule, A., Salamatian, K. and Taft, N.: Trafﬁc Matrix Tracking using
Kalman Filters. ACM LSNI Workshop (2005).
[4] Soule, A., Salamatian, K. and Taft, N.: Combining Filtering and Statis-
tical Methods for Anomaly Detection. USENIX , Association, Internet
Measurement Conference (2005). pp. 331344.
[5] Kalman, R. E. and Bucy R. S.: New results in linear ﬁltering and
predictions. Trans. ASM E., Series D, Journal of Basic Engineering,
Vol. 83 (1961), pp. 95-107.
[6] Kailath, T., Sayed, A. H. and Hassibi B.: Linear Estimation. Prentice
Hall, 2000.
[7] Wolverton, C.: On the Linear Smoothing Problem. IEEE Transactions
on Automatic Control, vol. 14 Issue:1 pp. 116-117. February 1969.
[8] Raugh, H. E.: Solutions to the linear smoothing problem. IEEE Trans.
Automatic Control AC-8 (October 1963) pp. 371372.
[9] Bilmes, J. A.:A Gentle Tutorial of the EM algorithm and its Application
to Parameter Estimation for Gaussian Mixture and Hidden Markov Mod-
els, International Computer Science Institute, Berkeley CA. Technical
Report TR-97-021, ICSI, 1997.
[10] McLachlan, G. and Krishnan, T.: The EM Algorithm and Extensions.
John Wiley and Sons, New York, 1996.
[11] Dempster, A. P., Laird N. M., and Rubin D. B.: Maximum likelihood
from in-complete data via the em algorithm. Journal of the Royal
Statistical Society: Series B, 39(1): pp. 138, November 1977.
[12] Rabiner, L., R.,: A Tutorial on Hidden Markov Models and Selected
Applications in Speech Recognition. Proc. IEEE, Vol. 77, No. 2, pp.
257-286, February 1989
[13] Brauckhoff, D., Dimitropoulos, X., Wagner, A. and Salamatian, K. :
Anomaly Extraction in Backbone Networks using Association Rules.
IMC09, November 46, 2009, pp. 28-34 Chicago, Illinois, USA.
[14] Shumway, R. H. and Stoffer, D. S.: An Approach to Time Series
Smoothing And Forecasting Using the EM Algorithm. Journal of Time
Series Analysis, vol.3, No 4,pp. 253-264.
[15] Shumway, R. H. and Stoffer, D. S.: Dynamic Linear Models With
Switching. Journal of the American Statistical Association, 86, pp. 763-
769, 1992.
[16] http://www.stat.pitt.edu/stoffer/tsa2/chap6.htm. 2011
[17] http://www.cs.ubc.ca/ murphyk/Software/HMM/hmm.html. 2011
[18] Ghahramani, Z. and Hinton, G. E.:Parameter Estimation for Linear
Dynamical Systems. Technical Report CRG-TR-96-2. February 22,
1996.
[19] http://learning.eng.cam.ac.uk/zoubin/software.html. 2011
39
INTERNET 2011 : The Third International Conference on Evolving Internet
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-141-0

