Toward the Development of a VR Simulator for Speed Sprayers 
 
Yu Tanaka, Oky Dicky Ardiansyah Prima, Kanayo Ogura, and Koichi Matsuda 
Graduate School of Soft. and Info. Science, Iwate Prefectural University 
152-52 Sugo, Takizawa, Iwate, Japan 
email: g231t024@s.iwate-pu.ac.jp, {prima, ogura_k, matsuda}@iwate-pu.ac.jp 
Shoichi Yuki 
YAMABIKO Corporation 
10-2 Sugo, Takizawa, Iwate, Japan 
email: s_yukixx@yamabiko-corp.co.jp 
 
Abstract—The speed sprayer, an agricultural chemical spraying 
vehicle, has been used to efficiently control pests and diseases in 
orchards such as grapes and apples. The use of speed sprayers 
is also problematic from the perspective of environmental 
protection, such as the measures to prevent pesticides from 
drifting outside the field. It also requires a skilled operator to 
operate the vehicle, as adjustments to travel speed, pressure, 
and nozzle selection must be considered. To make the speed 
sprayer easier and safer to operate, improvements to the control 
panel are required. In this study, we report our effort on the 
development of a simulator based on Virtual Reality (VR) that 
can analyze the operator's body movements while driving and 
operating a speed sprayer. As reference data, we videotaped the 
operator at the actual site and extracted the characteristics of 
the body movements during operation using computer vision 
techniques. The characteristics of the body movements observed 
while operating the various panels provide useful information 
for improving the operation panels.  
Keywords-vr-simulator; speed sprayer; virtual reality; head pose; 
hand gesture. 
I.  INTRODUCTION 
Most of the chemicals sprayed in orchards are liquids. 
There are several types of sprayers, including pipe sprayers, 
portable power sprayers, and speed sprayers. The introduction 
of speed sprayers has a great effect on the efficiency and labor 
saving of pesticide spraying in orchards. Many studies have 
been conducted on the prevention of pesticide drift and 
exposure to pesticides, and the application of small amounts 
of concentrated pesticides [1].  
In Japan, speed sprayers were introduced in 1957. The 
Japanese Ministry of Health, Labor and Welfare (MHLW) 
introduced the positive list system for agricultural chemical 
residues, such as pesticides, feed additives, and veterinary 
drugs in foods [2]. The use of speed sprayers poses several 
challenges in terms of occupational safety and environmental 
protection. In the case of speed sprayers used in orchards, the 
amount of pesticides sprayed increases due to the large spray 
area. Therefore, there is an urgent need for measures to 
prevent drift of pesticides outside the field. A variety of 
unmanned pest control machines have been developed for 
more accurate spraying [3], but they have not been widely 
used. 
Speed sprayers require a skilled operator to operate due to 
the need to consider the relationship between travel speed, 
pressure, and nozzle selection. Furthermore, the complexity of 
the operation panel, in addition to the difficulty in seeing the 
surrounding environment from inside the vehicle, makes the 
work on this vehicle hard. Driving skills are required, 
especially in vineyards, where cars need to pass through the 
path between the branches. 
As the number of agricultural workers in Japan has 
decreased and the population has aged in recent years, there 
has been a growing demand for the development of more 
efficient speed sprayers. Especially, simplifying the 
operations that require technical skills becomes critical. In the 
automobile industry, car simulators have been developed as a 
measure of vehicle performance. However, there are few 
examples of studies that simulate the operation and driving of 
tractors in agriculture [4][5]. 
In this study, we build a simulator using Virtual Reality 
(VR) to analyze the driving and operation of a speed sprayer. 
The simulator collects information on the driver's Six-Degree-
of-Freedom (6-DoF) head pose, three-dimensional (3D) hand 
gestures, gas pedal and brake operations, and visualizes the 
characteristics of driving behavior based on the features of 
head and hand movements. In the physical environment, we 
set up two cameras to capture the cockpit and the driver at the 
same time. Computer vision techniques were used to track the 
body movements and the rotation of the steering wheel. 
This paper is organized as follows. In Section II, we 
describe the VR simulator of a speed sprayer built for this 
study. Section III describes experiments using our VR 
simulator and their results. In Section IV, we present our 
computer vision techniques used to observe the operator 
behavior in the physical environment. Finally, Section V 
summarizes the results of this study and discusses future 
perspectives. 
 
Figure 1. The electric speed sprayer (SSV1091FSC) manufactured 
by the YAMABIKO Corporation used in this study.  
Ventilation fan
transmission
Sprayer tank
Cockpit
1
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-870-9
ACHI 2021 : The Fourteenth International Conference on Advances in Computer-Human Interactions

II. VR SIMULATOR OF A SPEED SPRAYER 
Figure 1 shows the speed sprayer (SSV1091FSC) that can 
carry 1000L of pesticide which was used in this study. The 
simulator of the speed sprayer developed in this study has the 
following conditions. First, the simulator must be able to track 
the driver's head pose and finger movements in 6-DoF. Next, 
the driver must be able to operate the vehicle using physical 
car controls. Finally, the driver should be able to perceive the 
car control in the virtual space and that in the physical space 
equally. Considering these constraints, we adopted a set of 
Logitech G27 Force Feedback Wheel and Pedal as the car 
control, and the Oculus Quest 2 as the VR device [6].  
A. Logitech G27 Force Feedback Wheel and Pedal 
The Logitech G27 is a gaming racing wheel compatible 
with PlayStation 2 and 3. It runs on Windows and Mac using 
the Logitech G27 driver and gaming SDK. The SDK for 
Windows is also available on the Unity Asset Store. Figure 2 
shows the steering wheel and pedals of Logitech's G27 
attached to the seat of a racing car fixed with a metal frame.  
B. Oculus Quest 2 
The Oculus Quest 2 is consumer-oriented Head-Mounted 
Display (HMD) based VR device developed by Facebook, Inc. 
featuring a 6-DoF angular and linear tracking system that can 
measure head pose and hand gestures. This system uses 
Inertial Measurement Units (IMUs) that assess linear 
acceleration and rotational velocity with low latency and 
cameras in the HMD that creates a 3D map of the room space 
and hand landmarks of the user. Figure 2 shows a user wearing 
the Oculus Quest operating a panel in a virtual space. 
     The simulator outputs information on car controls such as 
steering, gas pedal, and brake, as well as position and rotation 
information on the user's head pose and finger joints of both 
hands observed by the Oculus Quest 2. The initial position of 
the Oculus Quest 2 was calibrated against the position of the 
car control beforehand, since no auxiliary devices, such as 
base stations were used to obtain the absolute position of the 
user. For data recording, the Oculus Quest 2 was registered to 
the SteamVR Desktop application. By default, the Oculus 
Quest 2 runs at a frame rate of 72 Hz, however, to 
accommodate the frame rate drop caused by the program for 
the experiment, all data was recorded at a sampling rate of 60 
 
Figure 2. The VR simulator of a speed sprayer developed in this study. 
 
(a) The virtual farmland for this study. 
 
(b) A view from the cockpit in a virtual environment 
Figure 3. Virtual farmland used in the experiments of this study. 
2
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-870-9
ACHI 2021 : The Fourteenth International Conference on Advances in Computer-Human Interactions

Hz. Sound feedback was implemented to make it easier to 
operate buttons and other interfaces in the virtual environment. 
III. EXPERIMENTS AND RESULTS 
Using the developed simulator, the participants are asked 
to perform several tasks in a virtual farmland in different 
locations. The behavior during the tasks and the time required 
are measured. Figure 3 shows the virtual farmland we set up 
for the experiment. The field was set up with four points to 
perform the tasks, and audio instructions were played as the 
subject passed each point. The tasks are as follows. Following 
the voice instructions, the subject pressed a virtual button to 
spray to the left at point A, then another virtual button to spray 
to the right at point B, then turned off the ventilation fan at 
point C, and finally parked the car at point D.  
In this experiment, we obtain the subject's driving path, 
head pose, and the time required for each task. The car's 
position (x, y, z) and the subject's head pose (pitch, yaw, roll) 
were all measured in the Unity coordinate system. For the 
head pose, clockwise is positive and counterclockwise is 
negative. The signal when the brake is pressed is recorded as 
a binary signal, a digital signal with two distinguishable levels. 
For this experiment, five adult males between the ages of 
20 and 23 were recruited as subjects. All of the subjects had 
obtained a car driving license, but had little experience using 
a driving simulator. Figure 4(a) show the routes traveled by 
the subjects when performing each task of the experiment. 
The subjects' routes are almost identical. This means that the 
subjects can navigate the vehicle along a predetermined route 
regardless of the virtual environment. Figure 4(b) shows 
subjects’ head poses  when performing each task of the 
experiment. At points A and B, we observed that, in order to 
press the virtual button, the subjects' heads turned down once 
to confirm the button's position (negative pitch angle), and 
then they raised their heads to confirm the direction of travel 
(positive pitch angle). At point C, the subjects were expected 
to check the condition of the fan in order to stop the rear 
ventilation fan, but as they used the rearview mirror to check, 
the yaw angle did not change significantly. On the contrary, at 
point D, when the subjects parked the car, they checked the 
left and right sides of the car, resulting in a significant 
variation of the yaw angle. 
This outcome can be attributed to the fact that the virtual 
environment can reproduce the relief of the terrain and 
scenery, so it is likely that subjects who see these scenes will 
behave in a similar manner. The time taken to accomplish the 
task for each subject is shown in Table I. The reason that task 
4 took longer than the others is that it requires forward, 
backward, and stop actions to park the vehicle at point D. 
These experimental results show that the developed 
simulator can reproduce the actual task work of pesticides 
spraying and quantify the operator's behavior in each task, 
which can be used to improve the interface design in the 
cockpit for efficient work. 
IV. OBSERVATION OF OPERATOR BEHAVIOR IN 
THE PHYSICAL ENVIRONMENT 
We installed two 4K 60fps cameras (GoPro Hero8) in the 
cockpit of the speed sprayer (SSV1091FSC), one on the 
dashboard near the door and the other next to the driver's seat.  
Each camera was used to observe the operator's head and hand 
movements, steering wheel and button operations.   
 
(a) Superimposed travel routes of all subjects 
 
(b) Subjects’ head poses as they perform each task in the experiment. 
Figure 4. Routes traveled by the subjects and their head poses when performing each task of the experiment. 
X
Z
Y
Start
End
A
B
C
D
TABLE I.  TIME TAKEN TO ACCOMPLISH EACH TASK FOR EACH SUBJECT． 
 
 
Subject
1
1.9
2.9
2.1
57.1
2
3.1
2.4
8.4
106.3
3
9.0
4.2
18.0
111.1
4
3.3
2.9
3.4
79.9
5
2.8
2.8
2.5
59.0
 Mean
 Std. Dev.
2.835
0.680
6.710
25.438
Task 1 (s)
Task 2 (s)
Task 3 (s)
Task 4 (s)
4.02
3.04
6.88
82.68
3
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-870-9
ACHI 2021 : The Fourteenth International Conference on Advances in Computer-Human Interactions

A. Measuring head and hand movements 
     MediaPipe [7][8], a framework for building perception 
pipelines, was used to detect landmarks of faces and hands in 
real time from images captured by the camera on the 
dashboard. The head and hand movements can be quantified 
by obtaining the coordinate values for each landmark. We 
applied the Perspective-n-Point (PnP) solution to estimate the  
head pose in 6-DoF from the landmarks that consist of the face 
[9].  
B. Detecting Pressed-Button  
In this study, we detected that a button was pressed based 
on the inclusion of the fingertip coordinates with the range of 
pre-determined coordinates on the image of the button. This 
process is done on a frame-by-frame basis, but if a sequence 
of detected results occurs, these results are counted as a single 
pressing. 
C. Measuring Steering Wheel Rotation 
A relatively simple deep feed-forward neural network was 
constructed to estimate the steering wheel rotation angle. The 
network takes as input a 64x64 image of the steering wheel 
and performs convolution, batch normalization, and Rectified 
Linear Unit (RELU) four times to optimize the results. The 
network successfully estimated the steering rotation angle 
from low resolution steering images, where the SIFT features 
[10] are difficult to extract. 
     All the data, including video, head and hand movement 
measurements, steering wheel rotation, and pushed-button 
detection, is integrated into the ELAN (EUDICO Linguistic 
Annotator) [11]. ELAN provides a powerful tool for 
annotating and labeling data with time series characteristics. 
By statistically analyzing the statistical characteristics of these 
labels, further analysis can be performed to find out the 
operations that cause fatigue and how to overcome them. 
Figure 5 shows the data integrated into ELAN. In this way, 
ELAN enables us to analyze the data of head pose (pitch, yaw, 
roll), hand landmarks, and steering wheel rotation angles in a 
time series. We intend to verify the differences between the 
two environments by implementing similar tasks in the 
simulator using labeled tasks that correspond to the behavior 
of the operator obtained in the physical environment. 
V. CONCLUSION 
In this study, we have developed a simulator that allows 
the user to experience driving and operating a speed sprayer 
in a virtual environment using an HMD-type VR device and a 
car controller used in games. The simulator enables us to 
collect a time series of information on the driver's head pose, 
hand gestures, and gas pedal and brake controls measured by 
the simulator, and to visualize the characteristics of the 
driver's behavior during driving. 
To achieve a more realistic simulator, data on the behavior 
of operators in the field was collected so that similar 
behavioral data could be reproduced in a virtual environment. 
The MediaPipe framework made it possible to extract data of 
operator behavior even in a light-sensitive environment such 
as that inside a cockpit. Next, we will conduct experiments 
with many tasks using the developed simulator and find 
valuable data for improving the operation interface of the 
speed sprayer. 
 
Figure 5. Data of operator behavior integrated into the ELAN. 
4
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-870-9
ACHI 2021 : The Fourteenth International Conference on Advances in Computer-Human Interactions

ACKNOWLEDGEMENTS 
This work was done in collaboration with YAMABIKO 
Corporation and the Faculty of Software and Information 
Science, Iwate Prefectural University. 
REFERENCES 
[1] H. L. Wong, D. G. Garthwaite, C. T. Ramwell, and C. D. Brown, “How 
Does Exposure to Pesticides Vary in Space and Time for Residents 
Living Near to Treated Orchards?,” Environmental Science and 
Pollution Research, 24 (34), pp. 26444–26461, 2017.  
[2] The Japanese Ministry of Health, Labor and Welfare (MHLW), 
“Introduction of the Positive List System for Agricultural Chemical 
Residues in Foods,” 
https://www.mhlw.go.jp/english/topics/foodsafety/positivelist060228/
introduction.html [retrieved: June, 2021]  
[3] 
J. H. Han, C. H. Park, Y. J. Park, and J. H. Kwon, “Preliminary Results 
of the Development of a Single-Frequency GNSS RTK-Based 
Autonomous Driving System for a Speed Sprayer,” Journal of Sensors, 
pp. 1-9, 2019.  
[4] 
M. Watanabe and K. Sakai, “Development of a Nonlinear Tractor 
Model Using in Constructing a Tractor Driving Simulator,” 2017 
ASABE Annual International Meeting, pp. 1–6, 2017.  
[5] 
D. O. Gonzalez et al., “Development and Assessment of a Tractor 
Driving Simulator with Immersive Virtual Reality for Training to 
Avoid Occupational Hazards,” Computer and Electronics in 
Agriculture, 143, pp. 111–118, 2017.  
[6] 
R. Venkatakrishnan et al., “Towards an Immersive Driving Simulator 
to Study Factors Related to Cybersickness,” 26th IEEE Conference 
Virtual Reality and 3D User Interfaces, VR 2019, pp. 1201–1202, 2019.  
[7] 
C. Lugaresi et al., “Mediapipe: A Framework for Building Perception 
Pipelines,” arXiv preprint arXiv:1906.08172, pp. 1-9, 2019.  
[8] 
F. Zhang et al., “MediaPipe Hands: On-device Real-time Hand 
Tracking,” http://arxiv.org/abs/2006.10214, 2020.  
[9] 
F.Rocca, M. Mancas, and B. Gosselin, “Head Pose Estimation by 
Perspective-n-Point Solution Based on 2D Markerless Face Tracking,” 
Lecture Notes of the Institute for Computer Sciences, Social-
Informatics and Telecommunications Engineering, LNICST, 136 
LNICST, pp. 67–76. 2014. 
[10] D. G. Lowe, “Object Recognition from Local Scale-Invariant Features,” 
Proceeding of the International Conference on Computer Vision, pp. 
1150–1157, 2004.  
[11] ELAN (Version 6.0), “The Language Archive,”  
https://archive.mpi.nl/tla/elan  [retrieved: June, 2021] 
 
 
 
 
 
5
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-870-9
ACHI 2021 : The Fourteenth International Conference on Advances in Computer-Human Interactions

