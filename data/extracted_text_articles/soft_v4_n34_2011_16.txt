434
International Journal on Advances in Software, vol 4 no 3 & 4, year 2011, http://www.iariajournals.org/software/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Efﬁcient Non-Sequential Access and More Ordering
Choices in a Search Tree
Lubomir Stanchev
Computer Science Department
Indiana University - Purdue University Fort Wayne
Fort Wayne, IN, USA
stanchel@ipfw.edu
Abstract—A traditional search tree allows for efﬁcient sequen-
tial access to the elements of the tree. In addition, a search
tree allows for efﬁcient insertion of new elements and efﬁcient
deletion of existing elements. In this article we show how to extend
the capabilities of a search tree by presenting an algorithm for
efﬁcient access to predeﬁned subsets of the indexed elements.
This is achieved by marking some of the elements of the search
tree with marker bits. In addition, our algorithm allows us to
efﬁciently retrieve the indexed elements in either ascending or
descending direction relative to each of the ordering attributes.
Index Terms—marker bits; search trees; ordering directions;
data structures
I. INTRODUCTION
The paper extends a conference paper on the topic of
efﬁcient access to non-sequential elements of a search tree
([5]). The major contribution of this article is the introduction
of an algorithm that extends the ordering choices for the
elements that are returned. As a minor contribution, we show
that retrieving multiple elements from a search tree with
marker bits takes time that this proportional to the size of
the tree.
A balanced search trees, such as an AVL tree ([1]), an
AA tree ([2]), or a B+ tree ([3]), allows efﬁcient retrieval of
elements that are consecutive relative to an in-order traversal
of the tree. However, there is no obvious way to efﬁciently
retrieve the elements that belong to a predeﬁned subset of the
stored elements if they are not sequential in the search tree.
Similarly, there is no obvious way of retrieving the elements
in an order that is different from the tree order (or the reverse
of the tree order). For example, consider a database that stores
information about company employees. A search tree may
store information about the employees ordered ﬁrst by age
ascending and then by name ascending. This search tree can
be used to retrieve all the employees sorted by age, but the
search tree does not efﬁciently support the request of retrieving
all rich employees (e.g., making more than $100,000 per year)
sorted by age. In this paper, we will show how the example
search tree can be extended with marker bits so that both
requests can be efﬁciently supported. In addition, we will
show how the search tree can be used to efﬁciently retrieve
the elements in a different order, for example, ordered by
age descending (rather than ascending) and then by name
ascending.
The techniques that are proposed in this paper will increase
the set of requests that can be efﬁciently supported by a search
tree. This means that fewer search trees will need to be built.
This approach will not only save space, but will also improve
update performance. For example, [6] shows how our approach
can be applied to perform index merging. Speciﬁcally, indices
on the same elements that have different orderings can be
merged when the orderings are on the same attributes. Simi-
larly, indices with orderings on the same attributes that contain
common elements can be merged together.
Na¨ıve solutions to the problem of efﬁciently accessing a
non-sequential subset of the elements that are indexed fails.
For example, it is not enough to mark all the nodes of the
search tree that contain data elements that belong to the subset.
This approach will not allow us to prune out subtrees because
it can be the case that the parent node does not belong to
an interesting subset, but some of the descendent nodes do
belong. Similarly, efﬁciently accessing the elements of a search
tree where the ascending and descending direction of the
attributes is changed is not trivial because this can require
both forward and backward scanning.
To the best of our knowledge, detailed explanation of how
marker bits work have not been previously published. Our
previous work [6] brieﬂy introduces the concept of marker
bits, but it does not explain how marker bits can be maintained
after insertion, deletion, or update. Other existing approaches
handle requests on different subsets of the search tree elements
by exhaustive search or by creating additional search trees.
However, the second approach leads to not only unnecessary
duplication of data, but also slower updates to multiple copies
of the same data. Similarly, to the best of our knowledge,
no previous research addresses the problem of efﬁciently
retrieving the elements of a search tree in order that is different
from the search order or its reverse.
Given a subset of the search elements S, our approach to
efﬁciently retrieve these elements marks every node in the tree
that contains an element of S or that has a descendant that
contains an element of S. These additional marker bits will
only slightly increase the size of the search tree (with one bit
per tree node), but will allow efﬁcient logarithmic execution
of requests that ask for the elements of S in the tree order. It
will take time proportional to the size of the tree to retrieve
all the elements of S.

435
International Journal on Advances in Software, vol 4 no 3 & 4, year 2011, http://www.iariajournals.org/software/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
The algorithm that returns the elements of a search tree in an
order that changes the ascending and descending direction of
the attributes repeatedly calls the next method. The method
tries to ﬁnd the “next” element that has the same value for all
but the last attribute as the current node, where next is deﬁned
relative to the direction of the last attribute. If such an element
does not exist, then the method tries to ﬁnd the next element
that has the same value for all but the last two attributes and
so on, where the next method is recursive.
In what follows, Section II presents core deﬁnitions, Section
III describes how to efﬁciently perform different operations
on a search tree with marker bits, and Section IV contains the
conclusion and directions for future research.
II. DEFINITIONS
Deﬁnition 1 (MB-tree): An MB-tree has the following syn-
tax: ⟨⟨S1, . . . , Ss⟩, S, O⟩, where S and {Si}s
i=1 are sets over
the same domain ∆, Si ⊆ S for i ∈ [1..s], and O is an
ordering over ∆. This represents a balanced search tree of the
elements of S (every node of the tree stores a single element
of S), where the in-order traversal of the tree produces the
elements according to the order O. In addition, every node
of the tree contains s marker bits and the ith marker bit is
set exactly when the node or one of its descendants stores an
element that belongs to Si - we will refer to this property as
the marker bit property.
The above deﬁnition can be extended to allow an MB-tree
to have multiple data values in a node, as is the case for a B
Tree. However, this is area for future research.
Going back to our motivating example, consider the MB-
tree ⟨⟨RICH EMPS⟩, EMPS, ⟨age asc, name asc⟩⟩. This
represents a search tree of the employees, where the ordering
is ﬁrst relative to the attribute age in ascending order. If two
employees have the same age, then they are ordered relative to
their name in ascending order. The RICH EMPS set consists
of the employees that make more than $100,000 per year.
Figure 1 shows an example instance of this MB-tree. Each
node of the tree contains the name of the employee followed
by their age and salary.
 John, 23, $35,000 
Peter, 22, $20,000
      Dave, 30, $20,000
   1
Kate, 27, $35,000
Mike, 20, $105,000       Ann, 23, $40,000
                       1
          1
        0
       0
          0
Fig. 1.
Example of an MB-tree
Each node in the MB-tree contains the name of the em-
ployee, their age, and their salary. Note that Ann and John are
the same age. However, Ann comes before John in the search
order because “Ann” is lexicographically before “John”.
TABLE I
INTERFACE OF A NODE
(operation)
(return value)
left()
left child
right()
right child
parent()
parent node
data()
stored data
m[i]
the i marker bit (1 ≤ i ≤ s)
Above each node, the value of the marker bit is denoted,
where the bit is set exactly when the node or one of its
descendants contains a rich employee. As the ﬁgure suggests,
the subtree with root node that contains the name Dave can
be pruned out when searching for rich employees because the
marker bit of the root node is not set. We will show that this
MB-tree can be used to efﬁciently ﬁnd not only all employees
sorted by age, but also all rich employees sorted by age.
The tree can also be used to efﬁciently ﬁnd employee
(or rich employees) ordered by, for example, age descending
and then name ascending. The query that is asking for all
employees will return the employees in order: Dave, Kate,
Ann, John, Peter, and Mike, while the second query will return
only Mike (the only rich employee). Note that throughout this
paper efﬁcient refers to logarithmic time relative to the size of
the search tree.
III. OPERATIONS ON AN MB-TREE
Although an MB-tree does not need to be binary, in the
following section we will consider only binary trees. The
presented algorithms can be extended to non-binary trees and
this is area for future research. We will assume that every node
of the search tree supports the methods of the interface that
is shown in Table I in constant time, where {Si}s
i=1 are the
marker bit sets.
Next, we describe how the algorithms for tree update and
search can be extended in the presence of marker bits. Note
that supporting more ordering choices only affects the search
algorithm.
A. Element Insertion
After an algorithm has inserted a leaf node n, it should call
the insert_fix method from Algorithm 1 to update the
marker bits in the tree.
Algorithm 1 insert_fix(Node n)
1: for i ← 1 to s do
2:
if n.data() ∈ Si then
3:
n.m[i] ← 1
4:
else
5:
n.m[i] ← 0
6:
end if
7: end for
8: insert_parent_fix(n.parent(), n.m)
Lines 1-7 of the code set the marker bits for the new node.
The call to the recursive procedure insert_parent_fix

436
International Journal on Advances in Software, vol 4 no 3 & 4, year 2011, http://www.iariajournals.org/software/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
ﬁxes the marker bits of the ancestors of the inserted node,
where the later procedure is presented in Algorithm 2.
Algorithm
2 insert_parent_fix(Node n, Bit[]
m)
1: if n = null then
2:
return
3: end if
4: changed ← false
5: for i ← 1 to s do
6:
if m[i] = 1 and n.m[i] = 0 then
7:
n.m[i] ← 1
8:
changed ← true
9:
end if
10: end for
11: if changed then
12:
insert_parent_fix(n.parent(), n.m)
13: end if
We claim that the resulting tree satisﬁes the marker bit
property. In particular, note that only the marker bits of the
inserted node and its ancestors can be potentially affected by
the insertion. Lines 1-7 of the insert_fix method update
the marker bits of the node that is inserted. If the ith marker
bit of the node is set, then we check the ith marker bit of its
parent node (Lines 6 of the insert_parent_fix method).
If the ith marker bit of the parent is set, then the ith marker bit
of all ancestors will be set because of the marker bit property
and nothing more needs to be done for the ith marker bit.
Conversely, if the ith marker bit of the parent is not set, then
we need to set it and then check the ith marker bit of the parent
of the parent node and so on. This is done by Line 7 and the
recursive call at Line 12, respectively. The variable changed
is used to record whether any of the marker bits of the current
node have been modiﬁed. If the variable is equal to true,
then the marker bits of the ancestor nodes will not need to be
updated. Therefore, the marker bits of the inserted node and
its ancestors are updated correctly and the marker bit property
is preserved for the updated search tree.
B. Deleting a Node with Less than Two Children
Deleting a node with two children from a binary tree cannot
be performed by just connecting the parent of the deleted node
to the children of the deleted node because the parent node
may end up with three children. Therefore, we will consider
two cases: when the deleted node has less than two non-null
children and when the deleted node has two non-null children.
The ﬁrst case is explained next, while the second case is
explained in Section III-D.
An implementation of Algorithm 3 should be called before
a node n with less than two non-null children is deleted. In
the algorithm, n.child() is used to denote the non-null
child of n and m[i] is set when the ith marker bits of the
ancestor nodes need to be checked. The algorithm for the
method delete_parent_fix that updates the marker bits
of n’s ancestors in the search tree is shown in Algorithm 4.
Algorithm 3 delete_fix_simple(Node n)
1: for i ← 1 to s do
2:
if
n.data()
∈
Si
and
(n
is
leaf
node
or
n.child().m[i] = 0) then
3:
m[i] ← 1
4:
else
5:
m[i] ← 0
6:
end if
7: end for
8: delete_parent_fix(n.parent(), m)
Algorithm
4
delete_parent_fix(Node n, Bit[]
m)
1: if n = null then
2:
return
3: end if
4: changed←false
5: for i ← 1 to s do
6:
if m[i] = 1 and n.data()̸∈ Si and (n has no other
child or n.other_child().m[i] = 0) then
7:
n.m[i] ← 0
8:
changed ← true;
9:
end if
10: end for
11: if changed then
12:
delete_parent_fix(n.parent(), m)
13: end if
Note that we have used n.other_child to denote the
child node of n that is not on the path to the deleted node.
We claim that the deletion algorithm preserves the marker bit
property. In particular, note that only the ancestors of the
deleted node can be affected. If m[i] = 1 (Line 6 of the
delete_parent_fix method), then we check whether the
data in the node belongs to Si and whether the ith marker bit
of the other child node is set. If both conditions are false, then
the only reason the ith marker bit of n is set is because the
data in the deleted node belonged to Si and now this marker
bit needs to be reset (Line 7) and the ancestors of n need to
be recursively checked (Line 12). Conversely, if one of the
conditions is true or m[i] = 0, then the ith marker bit of
n and its ancestors will not be affected by the node deletion.
Therefore, the marker bits of the ancestors of the deleted node
are updated correctly and the marker bit property holds for the
updated search tree.
C. Element Update
Algorithm 5 should be executed after the data in a node n
is modiﬁed, where v is the old data value of n. The pseudo-
code updates the marker bits of the node n and then calls
the update_parent_fix method, which is presented in
Algorithm 6.
Note that we have used n.other_child() to denote
the child node of n that is not on the path to the updated
node. The method update_fix preserves the marker bit

437
International Journal on Advances in Software, vol 4 no 3 & 4, year 2011, http://www.iariajournals.org/software/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Algorithm 5 update_fix(Node n, Value v)
1: old ← n
2: for i = 1 to s do
3:
if n.data() ∈
Si or (n.left()̸= null and
n.left().m[i] = 1) or (n.right()̸= null and
n.right().m[i] = 1) then
4:
n.m[i] ← 1
5:
else
6:
n.m[i] = 0
7:
end if
8:
if n.m[i] = 1 and old.m[i] = 0 then
9:
m[i] ← “insert”
10:
else if n.m[i] = 0 and old.m[i] = 1 then
11:
m[i] ← “delete”
12:
else
13:
m[i] ← “no change”
14:
end if
15: end for
16: update_parent_fix(n.parent(), m)
Algorithm
6
update_parent_fix(Node n,
Value[] m)
1: if n = null then
2:
return
3: end if
4: changed ← false
5: for i = 1 to s do
6:
if m[i] = “insert” and n.m[i] = 0 then
7:
n.m[i] ← 1
8:
changed ← true
9:
end if
10:
if
m[i]
=
“delete”
and
n.data()
̸∈
Si
and
(n.other_child()
=
null
or
n.other_child().m[i] = 0)) then
11:
n.m[i] ← 0
12:
changed ← true
13:
end if
14: end for
15: if changed then
16:
update_parent_fix(n.parent(), m)
17: end if
property because it is a combination of the insert_fix
and delete_fix_simple methods. In particular, m[i] in
the method update_fix is set to insert when the ith
marker bit of the updated node was changed from 0 to 1 and
to delete when this marker bit was updated from 1 to 0.
The ﬁrst case is equivalent to a node with the ith marker bit
set being inserted, while the second case is equivalent to a
node with the ith marker bit set being deleted.
D. Deleting a Node with Two Children
As it is usually the case ([4]), we assume that the deletion
of a node n1 with two non-null children is handled by ﬁrst
deleting the node after n1 relative to the tree order, which
we will denote as n2, followed by changing the data value
of n1 to that of n2. The pseudo-code in Algorithm 7, which
implementation should be called after a node is deleted from
the tree, shows how the marker bits can be updated, where
initially n = n1, p is the parent of n2, v is the value of
the data that was stored in n2, and m[i] = 1 exactly when
n2.m[i] = 1 and n′.m[i] = 0 for all descendants n′ of n2.
Algorithm 7 delete_fix_two_children(n,p,v,m)
1: if p = n then
2:
update_fix(n, v)
3: end if
4: changed ← false
5: for i=1 to s do
6:
if m[i] = 1 and p.data() ̸∈ Si and (p has no other
child or p.other_child().m[i] = 0) then
7:
p.m[i] ← 0
8:
changed ← true
9:
end if
10: end for
11: if changed then
12:
delete_fix_two_children(n, p.parent(),
v, m)
13: else
14:
update_fix(n, v)
15: end if
In the above code “p has no other child” refers to the condi-
tion that p has no other child than the child that it is on the path
to the deleted node n2. Similarly, p.other_child() is used
to denote the child of p that is not on the path to the deleted
node n2. Note that the above algorithm changes the nodes on
the path from n2 to n1 using the deletion algorithm from the
method delete_parent_fix and the nodes on the path
from n1 to the root of the tree using the update algorithm
from the method update_fix and is therefore correct.
E. Tree Rotation
Most balancing algorithms (e.g., the ones for AVL, red-
black, or AA trees) perform a sequence of left and/or right
rotations whenever the tree is not balanced as the result of
some operation. Here, we will describe how a right rotation
can be performed, where the code for a left rotation is
symmetric. The pseudo-code in Algorithm 8 should be called
with a parent node n2 and a right child node n1 after the
rotation around the two nodes was performed. The pseudo-
code only ﬁxes the marker bits of n1 and n2. The descendants
of all other nodes will not change and therefore their marker
bits do not need to be updated.
F. Time Analysis for the Modiﬁcation Methods
Obviously, the pseudo-code for the rotation takes constant
time. The other methods for updating marker bits visit the
updated node and possibly some of its ancestors and perform
constant number of work on each node and therefore take order
logarithmic time relative to the number of nodes in the tree.

438
International Journal on Advances in Software, vol 4 no 3 & 4, year 2011, http://www.iariajournals.org/software/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Algorithm 8 rotate_right_fix(n1, n2)
1: for i ← 1 to s do
2:
if n1.data() ∈
Si or (n1 has left child and
n1.left().m[i] = 1) then
3:
n1.m[i] ← 1
4:
end if
5:
if n2.data() ∈
Si or (n2 has left child and
n2.left().m[i] = 1) or (n2 has right child and
n2.right().m[i] = 1) then
6:
n2.m[i] ← 1
7:
end if
8: end for
Therefore, the extra overhead of maintaining the marker bits
will not change the asymptotic complexity of the modiﬁcation
operations.
G. Search
Let us go back to our motivating example from Figure 1.
Our desire is to efﬁciently retrieve all rich employees in
the tree order. This can be done by repeatedly calling the
implementation of the next method from Algorithm 9. The
terminating condition is when the method returns null. The
algorithm ﬁnds the ﬁrst node after n, relative to the tree order,
that has data that belongs to the set Si, where d is initially set
to false.
Algorithm 9 next(n,i,d)
1: if n.data() ∈ Si and d then
2:
return n
3: end if
4: if n.left() is not the last node visited and n.left()̸=
null and n.left().m[i] = 1 and d then
5:
return next(n.left(), i, true)
6: end if
7: if
n.right()
is
not
the
last
node
visited
and
n.right()̸= null and n.right().m[i] = 1 then
8:
return next(n.right(), i, true)
9: end if
10: if n.parent() = null then
11:
return null
12: end if
13: return next(n.parent(), i, d)
The algorithm ﬁrst checks if the data in the current node
is in Si and d is true. Note that d becomes true when n is a
node that is after the initial node in the search tree. When both
conditions are true, we have found the resulting node and we
just need to return it. Next, we check the left child node. If we
did not just visit it and its ith bit is marked and it is after the
start node relative to the in-order tree traversal order, then the
subtree with root this node will contain a node with data in
Si that will be the resulting node. Next, we check if the right
child has its ith bit marked. This condition and the condition
that we have not visited it before guarantees that this subtree
will contain the resulting node. Finally, if nighter of the child
subtrees contain the node we are looking for, we start checking
the ancestor nodes in order until we ﬁnd an ancestor that has a
right child node that we have not visited and its ith marker bit
for this child is set. We then visit this subtree because we are
guaranteed that it will contain the resulting node. Therefore,
the algorithm ﬁnds the ﬁrst node after n that has data that is
in Si. Since, in the worst case, we go up a path in the search
tree and then down a path in the search tree, our worst-case
asymptotic complexity for ﬁnding the next node with data in
Si is logarithmic relative to the size of the tree, which is the
same as the asymptotic complexity of the traditional method
for ﬁnding a next element in a balanced search tree.
Next, we will consider a method that ﬁnds all the elements
in the search tree without using the next method and we will
show that this method runs in time that is proportional to the
size of the tree. Note that, in the worst case all nodes belong
to the query result and therefore we cannot do any better. The
algorithm is presented in Algorithm 10. In the method, n is
initially the root node of the search tree. Since the method
visits every node once, it runs in time that is proportional to
the size of the tree. Note that the nodes that are visited by
calling the next method multiple times are the same as the
nodes that are visited by calling the find_all method. In
both cases, the nodes in the tree are visited relative a in-order
traversal of the tree, where subtrees that have root nodes that
are unmarked are pruned out.
Algorithm 10 find_all(n,i)
1: result ← ∅
2: if n.left() ̸= null and n.left().m[i] = 1 then
3:
result ← result ∪ find_all(n.left(),i)
4: end if
5: if n.m[i] = 1 then
6:
result ← result ∪ {n}
7: end if
8: if n.right() ̸= null and n.right().m[i] = 1 then
9:
result ← result ∪ find_all(n.right(),i)
10: end if
11: return result
Next, we will describe a search algorithm that can be used
to efﬁciently retrieve the elements of the search tree in an
order that this different from the search order. For starters,
we present the method previous that returns the previous
element that belongs to the set Si. The method is presented
in Algorithm 11, where d is initially set to false. Note that
the method previous is analogous to the method next. To
only difference is that it searches for an element to the left
(rather than to the right) of the current element.
Next, we present a method search that is also need in
order to retrieve the elements of the search tree in an order
that is different from the search tree order. The method has
the following properties, where we assume that the search tree
contains elements with attributes {Ai}a
i=1 and that the ordering
of the tree is ⟨A1 asc, . . ., Aa asc⟩.

439
International Journal on Advances in Software, vol 4 no 3 & 4, year 2011, http://www.iariajournals.org/software/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Algorithm 11 previous(n,i,d)
1: if n.data() ∈ Si and d then
2:
return n
3: end if
4: if
n.right()
is
not
the
last
node
visited
and
n.right()̸= null and n.right().m[i] = 1 and d
then
5:
return previous(n.right(), i, true)
6: end if
7: if n.left() is not the last node visited and n.left()̸=
null and n.left().m[i] = 1 then
8:
return previous(n.left(), i, true)
9: end if
10: if n.parent() = null then
11:
return null
12: end if
13: return previous(n.parent(), i, d)
search(A1, P1, . . ., Al, Pl, dir, r,i):
• pre-conditions: l ≤ a and r ∈ [l − 1, l].
• return value: If dir = asc (dir = desc), then this
method returns the the ﬁrst (last) node n in Si, relative
to the tree order, for which:
1)
r∧
i=1
(n.data.Ai = Pi) and
2) if r < l, then n.data.Al > Pl when dir = asc and
n.data.Al < Pl when dir = desc.
The method returns null when such a node does not
exist.
The method search is used to search for the node that has
the same value for some of the attributes as the current node,
which allows us to go forward and backwards in the tree and
return the elements in the desired order. The pseudo-code for
the method when r = l is presented in Algorithm 12. Note
that we have added a node n as a parameter, which is initially
the root of the search tree. The code when dir = asc and
dir = desc are symmetric. The expression ⟨P1, . . . , Pl⟩ ≺
n returns true when Pj ≤ n.data().Aj for j ∈ [1 . . . l],
but not all inequalities are equalities. Similarly, the expression
n = ⟨P1, . . . , Pl⟩ returns true when Pj = n.data().Aj for
j ∈ [1 . . . l].
The code ﬁrst checks if the element that we are searching for
is strictly in the left subtree (Lines 2-4) or in the right subtree
(Lines 5-7). Of course, the subtrees are only considered if the
appropriate marker bit is set. If both if statements fail (on
Lines 2 and 5), then the current node has values P1, . . . , Pl
for the attributes A1, . . . , Al, respectively. If there is a node in
the left subtrees with these values, then we recursively call the
method on the left subtree. Otherwise, we simply return the
current root node n. The algorithm ﬁnds the ﬁrst node with
the desired property and therefore is correct. At each step,
the algorithm considers only the left or the right subtree and
therefore it runs in logarithmic time relative to the size of the
tree.
Algorithm
12
search1(n,A1, P1, . . ., Al, Pl,
dir, l,i)
1: if dir = asc then
2:
if n.left() ̸= null and n.left().m[i] = 1 and
⟨P1, . . . , Pl⟩ ≺ n then
3:
return
search1(n.left(),A1, P1, . . .,
Al, Pl, dir, l,i)
4:
end if
5:
if n.right() ̸= null and n.right().m[i] = 1 and
n ≺ ⟨P1, . . . , Pl⟩ then
6:
return
search1(n.right(),A1, P1, . . .,
Al, Pl, dir, l,i)
7:
end if
8:
if n ̸= ⟨P1, . . . , Pl⟩ then
9:
return null
10:
end if
11:
if
search1(n.left(),A1, P1, . . ., Al, Pl,
dir, l,i)=null then
12:
return
n
13:
end if
14:
return
search1(n.left(),A1, P1, . . .,
Al, Pl, dir, l,i)
15: end if
16: if n.right() ̸= null and n.right().m[i] = 1 and
n ≺ ⟨P1, . . . , Pl⟩ then
17:
return search1(n.right(),A1, P1, . . ., Al,
Pl, dir, l,i)
18: end if
19: if n.left() ̸= null and n.left().m[i] = 1 and
⟨P1, . . . , Pl⟩ ≺ n then
20:
return
search1(n.left(),A1, P1, . . ., Al,
Pl, dir, l,i)
21: end if
22: if n ̸= ⟨P1, . . . , Pl⟩ then
23:
return null
24: end if
25: if
search1(n.right(),A1, P1, . . ., Al, Pl,
dir, l,i)=null then
26:
return
n
27: end if
28: return
search1(n.right(),A1, P1, . . .,
Al, Pl, dir, l,i)
We will next consider the search method when r = l−1.
We will only show the code for when dir l = asc, where
the other case is symmetric. The pseudo-code is presented
in Algorithm 13. Again n is initially the root node of the
tree. The expression ⟨P1, . . . , Pl⟩ ≼ n returns true when
Pj ≤ n.data().Aj for j ∈ [1 . . . l]. The algorithm ﬁrst checks
to see if the resulting node is the right subtree (Lines 2-4). If
not, then it checks the left subtree (Lines 5-7). If both options
fail, then the algorithm checks if the current root node passes
the condition. If it does, then it returns it (Line 11). If it does
not, then it returns null (Line 9). Therefore, the algorithm

440
International Journal on Advances in Software, vol 4 no 3 & 4, year 2011, http://www.iariajournals.org/software/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
ﬁnds the correct node. The algorithm runs in logarithmic time
on a balanced tree because it calls itself recursively on either
the left or right subtree, but not both.
Algorithm 13 search2(n,A1, P1, . . ., Al, Pl, dir,
r, i)
1: if dir = asc then
2:
if n.right() ̸= null and n.right().m[i] = 1 and
n ≼ ⟨P1, . . . , Pl⟩ then
3:
return
search2(n.right(),A1, P1, . . .,
Al, Pl, dir, r,i)
4:
end if
5:
if n.left() ̸= null and n.left().m[i] = 1 and
search2(n.left(),A1, P1, . . ., Al, Pl, dir, r,i)̸=
null then
6:
return
search2(n.left(),A1, P1, . . .,
Al, Pl, dir, r,i)
7:
end if
8:
if n ̸= ⟨P1, . . . , Pl−1⟩ or n.data().Al ̸≥ Pl then
9:
return null
10:
end if
11:
return n
12: end if
13: . . .
We will next show how the elements of the search tree can
be efﬁciently retrieved in the order ⟨A1 dir[1], . . ., Aa dir[a]⟩
where dir[j] ∈ {asc, desc} for j ∈ [1 . . . a]. The algorithm is
presented in Algorithm 14, where the method will return only
elements that belong to the set Si. Note that the variables
{dir[i]}a
i=1 are parameters to the method. However, they have
been omitted from the algorithm in order to keep the code
simpler and more understandable.
Algorithm 14 ordered_next(n,i)
1: if dir[a]=asc then
2:
n′ ← next(n, i, false)
3: end if
4: if dir[a]=desc then
5:
n′ ← previous(n, i, false)
6: end if
7: if n′ ̸= null and
a−1
∧
i=1
(n.Ai = n′.Ai) then
8:
return n′
9: end if
10: if a = 1 then
11:
return null
12: end if
13: n′ ← search(A1, n.A1, . . . , Aa−1, n.Aa−1,
dir[a − 1], a − 2, i)
14: return next_up(n, n′, i, a − 1)
Note that we have used n.Aj to denote the value of
the jth attribute of the element that is stored in node n.
The method calls the next_up method, which in tern can
call the next_down method. The methods are presented in
Algorithms 15 and 16, respectively.
Algorithm 15 next_up(n,n′,i,l)
1: if n′ = null then
2:
if l = 1 then
3:
return null
4:
end if
5:
n′ ← search(A1, n.A1, . . . , Al−1, n.Al−1,
dir[l − 1], l − 2, i)
6:
return next_up(n, n′, i, l − 1)
7: end if
8: return next_down(n′, i, l)
Algorithm 16 next_down(n,i,l)
1: if n = null then
2:
return null
3: end if
4: if l = a then
5:
return n
6: end if
7: n′ ← search(A1, n.A1, . . . , Al, n.Al, dir[l + 1], l, i)
8: return next_down(n′, i, l + 1)
Consider ﬁrst Lines 1-6 of the method ordered_next.
The code ﬁrst checks whether the next node n′ relative to the
order ⟨A1 dir[1], . . ., Aa−1 dir[a − 1]⟩ has the same values
for the attributed A1, . . . , Aa−1 as n. If this is the case, then
only this node needs to be returned. If this is not the case and
a = 1 (Lines 10-12), then a “next” node does not exist and
the method returns null. If this is not the case and a > 1,
then Line 13 of the code looks for the ﬁrst node that has the
same value as n for the attributes {Ai}a−2
i=1 and a value for
the attribute Aa−1 that is right after the value of the attribute
Aa−1 for n. In Line 14 of the code the next_up method
is called with the element that is found in the previous line,
where the value for n′ is null when such an element does
not exist.
The method next_up looks for a node n′ that has the
property that
l−1
∧
i=1
(n.Ai = n′.Ai) and the value for Al of n′
is right after the value for Al of n. When this is the case,
then the next_down method is executed. It ﬁnds the ﬁrst
element in the search tree for which
l−1
∧
i=1
(n.Ai = n′.Ai) holds.
This is indeed the node that needs to be returned. When n′
does not have the desired property, l is decremented by 1 and
next_up is called recursively. If l becomes equal to 0, then a
“next” tuple does not exist and the method next_up returns
null. Since the next method goes up and down a path in the
three, its running time is logarithmic.
IV. CONCLUSION AND FUTURE RESEARCH
We introduced MB-trees and showed how they are beneﬁcial
for accessing predeﬁned subsets of the tree elements. MB-trees

441
International Journal on Advances in Software, vol 4 no 3 & 4, year 2011, http://www.iariajournals.org/software/
2011, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
use marker bits, which add only light overhead to the different
operations and do not change the asymptotic complexity of the
operations. An obvious application of MB-trees is merging
search trees by removing redundant data, which can result
in faster updates because fewer copies of the redundant data
need to be updated. In addition, we showed how elements in
different orders can be retrieved from a search tree. Again,
the application is index merging because fewer indices can
efﬁciently answer the same set of queries.
One obvious area for future research is showing that the
algorithm for retrieving all the elements of a search tree
that belong to a set Si in a non-trivial order takes time that
is proportional to the size of the tree. Another contribution
would be to present algorithms that extend our algorithms to
secondary storage structures, such as B and B+ trees.
REFERENCES
[1] G. M. Adelson-Velskii and E. M. Landis, “An Algorithm for the Orga-
nization of Information,” Soviet Math. Doklady, vol. 3, pp. 1259–1263,
1962.
[2] A. Andersson, “Balanced search trees made simple,” Workshop on Algo-
rithms and Data Structures, pp. 60–71, 1993.
[3] R. Bayer and E. McCreight, “Organization and Maintenance of Large
Ordered Indexes,” Acta Informatica, vol. 1, no. 3, 1972.
[4] T. Cormen, C. Leiserson, R. Rivest, and C. Stein, Introduction to
Algorithms.
McGraw Hill, 2002.
[5] L. Stanchev, “Efﬁcient Access to Non-Sequential Elements of a Search
Tree,” The Third International Conference on Advances in Databases,
Knowledge, and Data Applications, DBKDA 2011, pp. 181–185, January
2011.
[6] L. Stanchev and G. Weddell, “Saving Space and Time Using Index
Merging,” Elsevier Data & Knowledge Engineering, vol. 69, no. 10, pp.
1062–1080, 2010.

