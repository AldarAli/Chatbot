Weight Difference Propagation for
Stochastic Gradient Descent Learning
Shahrzad Mahboubi,
Hiroshi Ninomiya
Graduate School of Electrical and Information Engineering,
Shonan Institute of Technology
Fujisawa, Kanagawa, Japan
Email : mahboubi.shaa@gmail.com,
ninomiya@info.shonan-it.ac.jp
Abstract—This paper proposes a new stochastic (mini-batch)
training algorithm to reduce the computational and hardware
implementation costs of the error Back Propagation (BP) method.
In recent years, with the rapid development of IoT, there has been
an increasing necessity to use microcomputers, especially edge
computers that implement neural networks capable of training
large-scale data. Since the neural network training is based
on the BP method, the error propagation architecture from
the output to the input layers is required for each training
sample to calculate the gradient. As a result, the hardware
and computational costs are increased. This paper attempts to
improve the BP method by using the inner product of the weights
and their updated amounts (differences) for training reducing
the hardware and computational costs. This method eliminates
the requirement of the BP architecture for each training sample
in mini-batch and calculates the amounts of the weight update
with only one weight difference propagation. This means that
the proposed method can reduce the computational complexity
in the backward calculations of the training to 1/𝑏 (𝑏 is the mini-
batch size) compared to BP. Computer simulations demonstrate
the effectiveness of the proposed method.
Index Terms—neural network, gradient-based training algo-
rithm, stochastic gradient descent method, error back propaga-
tion, weight difference propagation
I. INTRODUCTION
With the development of information and communication
technology, various devices have been converted to IoT, mak-
ing it possible to acquire and store diverse and vast amounts
of data. The complexity (nonlinearity) of data and its volume
increase day by day. Developing technologies can process
large-scale nonlinear data with high accuracy and speed. In
recent years, Artiﬁcial Intelligence (AI) has attracted attention
as one of the technologies that make this possible and has been
applied in various ﬁelds, including mechanical engineering,
statistics, physics, economics, cognitive science, and brain
science. The core technology in the rapid development of AI is
neural networks (NNs) [1] [2]. NNs are generally trained using
gradient algorithms based on the error Back Propagation (BP)
method. BP training is an efﬁcient and practical algorithm,
and various improvement methods have been proposed. One
of the improved methods is the Stochastic Gradient Descent
(SGD) method for big data learning [1].
Recently, several training algorithms have been proposed
considering biological plausibility [3]–[6]. These researches
began with questions such as, “Does the biological brain per-
form complex and accurate backward calculations like BP?”
[3] [6] or “Is it necessary to calculate the exact gradient used
in training?” [4] [5]. Furthermore, a gradient-based training al-
gorithm that does not require the backpropagation of errors has
been proposed [7]–[9]. This algorithm can potentially reduce
computational and hardware burdens even when processed
by microcontrollers installed in IoT devices. However, these
algorithms are unsuitable for asynchronous parallel learning or
training large datasets and still need improvement. In addition,
there are preliminary simulation experiments of the proposed
algorithms, and the learning accuracy for recent nonlinear data
is not guaranteed.
In this paper, focusing on the algorithm of [7]–[9], a novel
algorithm is proposed to simplify the calculation of the gradi-
ent. The proposed method is referred to as Stochastic Weight
Difference Propagation (SWDP) to improve the disadvantages
of SGD. SWDP differs from the conventional SGD in that the
weights can be updated by only the inner product of the post-
synaptic weights and their differences. This enables parallel
training of each neuron in each layer. Therefore, the backward
process for each training sample in mini-batch (stochastic)
training is unnecessary, and the hardware and computational
cost can be reduced. This means that the proposed method
can reduce the computational complexity in the backward
calculations of the training to 1/𝑏 (𝑏 is the mini-batch size)
compared to BP. The proposed method, compared with the
conventional SGD method on two benchmark problems and
simulations, demonstrates its effectiveness.
The paper is organized as follows: Section 2 describes
the training of SGD. In Section 3, the proposed SWDP is
derived and explained. In Section 4, the computational cost
of SGD and SWDP is discussed. Sections 5 and 6 present
computational simulations and conclusions, respectively.
II. STOCHASTIC GRADIENT DESCENT METHOD
This paper considers the multi-layer feed-forward NN train-
ing, which is an unconstrained optimization problem to min-
imize the error function 𝐸(w) concerning the weight vector
w ∈ R𝑛.
min
w∈R𝑛
𝐸(w).
(1)
There are two training methods for error Back Propagation
(BP): batch and stochastic (mini-batch) strategies. All training
samples in a dataset 𝑇𝑟 are used in an epoch to calculate
12
Copyright (c) IARIA, 2023.     ISBN:  ISBNFILL
ICCGI 2023 : The Eighteenth International Multi-Conference on Computing in the Global Information Technology

Fig. 1. Structural diagram of SGD.
gradients in the batch strategy. The error function 𝐸(w) is
deﬁned as
𝐸(w) =
1
|𝑇𝑟 |
∑
𝑝∈𝑇𝑟
𝐸 𝑝(w),
(2)
where, |𝑇𝑟 | denotes the number of samples, and 𝐸 𝑝(w) is the
error of the 𝑝th sample. On the other hand, the stochastic
training strategy in BP, so-call Stochastic Gradient Descent
(SGD), uses 𝑋 ⊆ 𝑇𝑟 dataset randomly selected from 𝑇𝑟 in an
iteration to calculate the gradient. The error function 𝐸𝑏(w)
of SGD is deﬁned as
𝐸𝑏(w) = 1
𝑏
∑
𝑝∈𝑋
𝐸 𝑝(w),
(3)
where 𝑏 = |𝑋| is the mini-batch size and if 𝑋 = 𝑇𝑟 and 𝑏 = |𝑇𝑟 |
then mini-batch training shifts to the batch mode.
Let x𝑝 and o𝑝 be the 𝑝th input and output vectors, respec-
tively. The relation between the inputs and outputs of the NN
is deﬁned as
o𝑝 = xout
𝑝 = f𝑁 𝑁 (w, x𝑝).
(4)
Moreover, let 𝑥𝑠
𝑖,𝑝(1 ≤ 𝑠 ≤ 𝑜𝑢𝑡) be the output of the 𝑖th neuron
in the 𝑠 layer for the 𝑝th sample, and 𝑤𝑠
𝑖 𝑗 be the weight from
the 𝑗th neuron of the 𝑠 − 1 layer to the 𝑖th neuron of the 𝑠
layer, the input-output relationship of the neuron is given by
(5) and (6). Note that 𝑠 = out denotes the output layer.
𝑥𝑠
𝑖,𝑝 = 𝑓 (𝑧𝑠
𝑖,𝑝),
(5)
𝑧𝑠
𝑖,𝑝 =
∑
𝑗
𝑤𝑠
𝑖 𝑗 · 𝑥𝑠−1
𝑗,𝑝 ,
(6)
where 𝑓 (𝑧𝑠
𝑖,𝑝) and 𝑤𝑠
𝑖 𝑗 denote the activation function and a
component of the weight vector w, respectively. The update
formula of SGD is deﬁned as (7) using learning rate 𝜂.
𝑤𝑠
𝑖 𝑗 (𝑡 + 1) = 𝑤𝑠
𝑖 𝑗 (𝑡) − 𝜂 𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
.
(7)
where 𝑡 denotes the iteration number and
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
= 1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑤𝑠
𝑖 𝑗
,
(8)
is the stochastic gradient, which is expansion by chain rule as
(9).
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
= 1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑥𝑠
𝑖,𝑝
·
𝜕𝑥𝑠
𝑖,𝑝
𝜕𝑧𝑠
𝑖,𝑝
·
𝜕𝑧𝑠
𝑖,𝑝
𝜕𝑤𝑠
𝑖 𝑗
= 1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑥𝑠
𝑖,𝑝
· 𝑓 ′(𝑧𝑠
𝑖,𝑝) · 𝑥𝑠−1
𝑗,𝑝 ,
(9)
where 𝑓 ′(𝑧𝑠
𝑖,𝑝) is the derivative of the activation function and
the partial differentiation of 𝜕𝐸𝑏(w)/𝜕𝑤𝑠
𝑖 𝑗 varies depending
on whether the 𝑠 is the output (𝑠 = 𝑜𝑢𝑡) or the hidden layers.
• 𝑠 is the output layer (𝑠 = 𝑜𝑢𝑡):
If 𝑠 is the output layer, the partial differentiation of
𝜕𝐸𝑏(w)/𝜕𝑤𝑠
𝑖 𝑗 is directly derived from the error function. Here,
the two types of error functions, that is, the mean squared error
(MSE) and the cross entropy (CE) are considered.
<MSE >
The error function is deﬁned as
𝐸𝑏(w) = 1
𝑏
∑
𝑝∈𝑋
𝐿
∑
𝑖=1
1
2
(
𝑑𝑖,𝑝 − 𝑥𝑜𝑢𝑡
𝑖,𝑝
)2
,
(10)
where 𝐿 is the number of the output units and 𝑑𝑖,𝑝 denote 𝑖th
unit of the output layer for the 𝑝th desired vector. Therefore,
the partial differentiation of 𝜕𝐸𝑏(w)/𝜕𝑤𝑠
𝑖 𝑗 is given by
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
= 1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑥𝑠
𝑖,𝑝
· 𝑓 ′(𝑧𝑠
𝑖,𝑝) · 𝑥𝑠−1
𝑗,𝑝
= − 1
𝑏
∑
𝑝∈𝑋
(𝑑𝑖,𝑝 − 𝑥𝑜𝑢𝑡
𝑖,𝑝 ) · 𝑓 ′(𝑧𝑠
𝑖,𝑝) · 𝑥𝑠−1
𝑗,𝑝 .
(11)
13
Copyright (c) IARIA, 2023.     ISBN:  ISBNFILL
ICCGI 2023 : The Eighteenth International Multi-Conference on Computing in the Global Information Technology

<CE >
The error function and the activation function are deﬁned
as (12) and the softmax function of (13), respectively.
𝐸𝑏(w) = − 1
𝑏
∑
𝑝∈𝑋
𝐿
∑
𝑖=1
𝑑𝑖,𝑝 log(𝑥𝑜𝑢𝑡
𝑖,𝑝 ),
(12)
𝑥𝑜𝑢𝑡
𝑖,𝑝 = 𝑓 (𝑧s
𝑖,𝑝) =
𝑒𝑥𝑝(𝑧𝑠
𝑖,𝑝)
∑𝐿
𝑖=1 𝑒𝑥𝑝(𝑧𝑠
𝑖,𝑝)
.
(13)
where 𝐿 is the classiﬁcation class. The gradient is given by
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
= 1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑥𝑠
𝑖,𝑝
· 𝑓 ′(𝑧𝑠
𝑖,𝑝) · 𝑥𝑠−1
𝑗,𝑝
= − 1
𝑏
∑
𝑝∈𝑋
(𝑑𝑖,𝑝 − 𝑥𝑜𝑢𝑡
𝑖,𝑝 ) · 𝑥𝑠−1
𝑗,𝑝 .
(14)
• 𝑠 is a hidden layer:
If 𝑠 is the hidden layer, the partial differentiation of
𝜕𝐸 𝑝(w)/𝜕𝑥𝑠
𝑖,𝑝 in (9) is given by
𝜕𝐸 𝑝(w)
𝜕𝑥s
𝑖,𝑝
=
∑
𝑘
𝜕𝐸 𝑝(w)
𝜕𝑥s+1
𝑘,𝑝
·
𝜕𝑥s+1
𝑘,𝑝
𝜕𝑧s+1
𝑘,𝑝
·
𝜕𝑧s+1
𝑘,𝑝
𝜕𝑥s
𝑖,𝑝
(15)
=
∑
𝑘
𝜕𝐸 𝑝(w)
𝜕𝑥𝑠+1
𝑘,𝑝
· 𝑓 ′(𝑧𝑠+1
𝑘,𝑝) · 𝑤𝑠+1
𝑘𝑖 .
(16)
Substituting (16) into (9), the gradient of 𝜕𝐸𝑏(w)/𝜕𝑤𝑠
𝑖 𝑗 can
be obtained as (17).
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
= 1
𝑏
∑
𝑝∈𝑋
∑
𝑘
𝜕𝐸 𝑝(w)
𝜕𝑥𝑠+1
𝑘,𝑝
· 𝑓 ′(𝑧𝑠+1
𝑘,𝑝) ·𝑤𝑠+1
𝑘𝑖 · 𝑓 ′(𝑧𝑠
𝑖,𝑝) ·𝑥𝑠−1
𝑗,𝑝 .
(17)
(17) shows that the inner product of the back propagation
components 𝜕𝐸 𝑝(w)/𝜕𝑥s+1
𝑘,𝑝 · 𝑓 ′(𝑧𝑠+1
𝑘,𝑝) and the weights 𝑤𝑠+1
𝑘𝑖
is necessary for the all samples in a mini-batch 𝑏. This means
that the derivation of the gradient requires 𝑏 times backward
processes. The structural diagram of SGD is shown in Fig. 1.
This ﬁgure also shows that SGD requires an error propagation
architecture from the output layer to the input layer for each
training sample in order to calculate the gradient.
III. STOCHASTIC WEIGHT DIFFERENCE PROPAGATION
LEARNING
In this paper, to reduce the computational cost of training
in SGD, Stochastic Weight Difference Propagation (SWDP)
is proposed. The proposed SWDP focuses on the backprop-
agation stream of each training sample in SGD training and
eliminates the BP architecture requirement for each training
sample in a mini-batch. As a result, SWDP can calculate the
amounts of weight updates with only one weight difference
propagation. This means that the proposed method can reduce
the computational complexity in the backward calculations of
the training to 1/𝑏 compared to SGD.
Since the training of SWDP is based on SGD, and the
computation process differs depending on whether 𝑠 is the
output or hidden layers.
• 𝑠 is the output layer (𝑠 = 𝑜𝑢𝑡):
If 𝑠 is the output layer, 𝜕𝐸𝑏(w)/𝜕𝑤𝑠
𝑖 𝑗 is same as SGD given
by (11) or (14).
• 𝑠 is the hidden layer:
If 𝑠 is the hidden layer, a variant of (17) is considered. In
the proposed SWDP, the gradient of 𝜕𝐸𝑏(w)/𝜕𝑤𝑠
𝑖 𝑗 given by
(17) is transformed as follows:
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
= 1
𝑏
∑
𝑝∈𝑋
∑
𝑘
𝜕𝐸 𝑝(w)
𝜕𝑥𝑠+1
𝑘,𝑝
· 𝑓 ′(𝑧𝑠+1
𝑘,𝑝) · 𝑤𝑠+1
𝑘𝑖 · 𝑓 ′(𝑧𝑠
𝑖,𝑝) · 𝑥𝑠−1
𝑗,𝑝
(17)
=
∑
𝑘
𝑤𝑠+1
𝑘𝑖 · 1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑥𝑠+1
𝑘,𝑝
· 𝑓 ′(𝑧𝑠+1
𝑘,𝑝) · 𝑓 ′(𝑧𝑠
𝑖,𝑝) · 𝑥𝑠−1
𝑗,𝑝 . (18)
Here, (18) is transformed to (19) by assuming that the output
of 𝑖th neuron in 𝑠 layer 𝑥𝑠
𝑖,𝑝 ≠ 0.
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
=
∑
𝑘
𝑤𝑠+1
𝑘𝑖 · 1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑥𝑠+1
𝑘,𝑝
· 𝑓 ′(𝑧𝑠+1
𝑘,𝑝) · 𝑥𝑠
𝑖,𝑝
·
𝑓 ′(𝑧𝑠
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥𝑠−1
𝑗,𝑝 .
(19)
From (9),
𝜕𝐸 𝑝(w)
𝜕𝑤𝑠+1
𝑘𝑖
= 𝜕𝐸 𝑝(w)
𝜕𝑥𝑠+1
𝑘,𝑝
· 𝑓 ′(𝑧𝑠+1
𝑘,𝑝) · 𝑥𝑠
𝑖,𝑝.
(20)
Therefore, (21) can obtained by substituting (20) into (19).
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
=
∑
𝑘
𝑤𝑠+1
𝑘𝑖 · 1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑤𝑠+1
𝑘𝑖
·
𝑓 ′(𝑧s
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥𝑠−1
𝑗,𝑝 . (21)
Here, (21) is transformed as follows: The second sum in
(21) for the sample 𝑝 is divided into two parts, that is,
𝜕𝐸 𝑝(w)/𝜕𝑤𝑠
𝑖 𝑗 and 𝑓 ′(𝑧s
𝑖,𝑝)/𝑥𝑠
𝑖,𝑝 · 𝑥𝑠−1
𝑗,𝑝 . Then (21) is rewritten
as (22),
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
=
∑
𝑘
𝑤𝑠+1
𝑘𝑖
{
1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑤𝑠+1
𝑘𝑖
· 1
𝑏
∑
𝑝∈𝑋
𝑓 ′(𝑧𝑠
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥𝑠−1
𝑗,𝑝
+ 1
𝑏
∑
𝑝∈𝑋
((
𝜕𝐸 𝑝(w)
𝜕𝑤𝑠+1
𝑘𝑖
− 1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑤𝑠+1
𝑘𝑖
)
·
( 𝑓 ′(𝑧𝑠
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥𝑠−1
𝑗,𝑝 − 1
𝑏
∑
𝑝∈𝑋
𝑓 ′(𝑧𝑠
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥𝑠−1
𝑗,𝑝
))}
.
(22)
That is, the ﬁrst term denotes the product of the averages of
𝜕𝐸 𝑝(w)/𝜕𝑤𝑠+1
𝑘𝑖
and 𝑓 ′(𝑧𝑠
𝑖,𝑝)/𝑥𝑠
𝑖,𝑝 · 𝑥𝑠−1
𝑗,𝑝 . The second term is
the covariance of these parts. In (22), if 𝜕𝐸 𝑝(w)/𝜕𝑤𝑠+1
𝑘𝑖
and
𝑓 ′(𝑧𝑠
𝑖,𝑝)/𝑥𝑠
𝑖,𝑝 · 𝑥𝑠−1
𝑗,𝑝 are independent variables, (23) holds.
(
𝜕𝐸 𝑝(w)
𝜕𝑤𝑠+1
𝑘𝑖
− 1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑤𝑠+1
𝑘𝑖
)
·
( 𝑓 ′(𝑧𝑠
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥𝑠−1
𝑗,𝑝 − 1
𝑏
∑
𝑝∈𝑋
𝑓 ′(𝑧𝑠
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥𝑠−1
𝑗,𝑝
)
= 0
(23)
14
Copyright (c) IARIA, 2023.     ISBN:  ISBNFILL
ICCGI 2023 : The Eighteenth International Multi-Conference on Computing in the Global Information Technology

Fig. 2. Structural diagram of SWDP.
However, these terms are clearly not independent because
these parts are derived from the sample 𝑝. It is also estimated
that as the number of samples in the mini-batch increases, the
value of (23) becomes larger. On the other hand, it is clear that
learning of NN can be performed even if there is some error
in gradients [3]–[6]. Therefore, the proposed SWDP assumes
that (23) holds and then obtains (24).
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
≃
∑
𝑘
𝑤s+1
𝑘𝑖
(
1
𝑏
∑
𝑝∈𝑋
𝜕𝐸 𝑝(w)
𝜕𝑤𝑠+1
𝑘𝑖
· 1
𝑏
∑
𝑝∈𝑋
𝑓 ′(𝑧𝑠
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥𝑠−1
𝑗,𝑝
)
=
∑
𝑘
(
𝜕𝐸𝑏(w)
𝜕𝑤𝑠+1
𝑘𝑖
· 𝑤𝑠+1
𝑘𝑖
)
·
(
1
𝑏
∑
𝑝∈𝑋
𝑓 ′(𝑧𝑠
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥s−1
𝑗,𝑝
)
.
(24)
In this study, the gradient 𝜕𝐸𝑏(w)/𝜕𝑤𝑠
𝑖 𝑗 of (24) is used for
learning. Consider the update formula for 𝑤𝑠+1
𝑘𝑖 in SGD of (25).
𝑤𝑠
𝑘𝑖(𝑡 + 1) = 𝑤𝑠
𝑘𝑖(𝑡) − 𝜂 𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑘𝑖
.
(25)
Here, the update formula of (25) can be transformed into an
expression for the amount of update (difference) Δ𝑤𝑠+1
𝑘𝑖
as
Δ𝑤𝑠+1
𝑘𝑖
= 𝑤𝑠+1
𝑘𝑖 (𝑡 + 1) − 𝑤𝑠+1
𝑘𝑖 (𝑡) = −𝜂 𝜕𝐸𝑏(w)
𝜕𝑤𝑠+1
𝑘𝑖
,
(26)
and then
𝜕𝐸𝑏(w)
𝜕𝑤𝑠+1
𝑘𝑖
= −1
𝜂Δ𝑤𝑠+1
𝑘𝑖 .
(27)
By substituting (27) into (24),
𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
=
∑
𝑘
(
−1
𝜂Δ𝑤𝑠+1
𝑘𝑖 · 𝑤𝑠+1
𝑘𝑖
)
·
(
1
𝑏
∑
𝑝∈𝑋
𝑓 ′(𝑧𝑠
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥𝑠−1
𝑗,𝑝
)
.
(28)
Furthermore, by substituting (28) into the SGD update formula
(7), the proposed SWDP update formula can be obtained as
(29).
𝑤𝑠
𝑖 𝑗 (𝑡 + 1) = 𝑤𝑠
𝑖 𝑗 (𝑡) − 𝜂 𝜕𝐸𝑏(w)
𝜕𝑤𝑠
𝑖 𝑗
= 𝑤𝑠
𝑖 𝑗 (𝑡) +
∑
𝑘
(
Δ𝑤𝑠+1
𝑘𝑖 · 𝑤𝑠+1
𝑘𝑖
)
·
(
1
𝑏
∑
𝑝∈𝑋
𝑓 ′(𝑧𝑠
𝑖,𝑝)
𝑥𝑠
𝑖,𝑝
· 𝑥𝑠−1
𝑗,𝑝
)
.
(29)
In (29),
(
1/𝑏 ∑
𝑝 𝑓 ′(𝑧𝑠
𝑖,𝑝)/𝑥𝑠
𝑖,𝑝 · 𝑥𝑠−1
𝑗,𝑝
)
can be calculated in
the forward operation in the mini-batch. Therefore, the back-
ward operation for the learning of NN is one calculation
of ∑
𝑘
(
Δ𝑤𝑠+1
𝑘𝑖 · 𝑤𝑠+1
𝑘𝑖
)
in the stochastic learning. This means
that the learning architecture is simple, and the computational
cost decreases compared to SGD. However, it is estimated
that the error of the gradients affects the learning from the
assumption of (23) holds. In Section V, the effect on learning
is investigated through computer experiments. The structural
diagram of SWDP is shown in Fig. 2. This ﬁgure shows
that SWDP eliminates the requirement of the BP architecture
for each training sample in mini-batch and calculates the
amounts of the weight update with only one weight difference
propagation.
IV. COMPUTATIONAL COST
This section discusses the computational costs of the pro-
posed SWDP and SGD for updating a weight of 𝑤𝑠
𝑖 𝑗. Since
the forward calculations of SWDP and SGD are the same,
the calculation costs of the backward processes for both
SWDP and SGD are compared. The summary of the backward
computational cost is illustrated in TABLE I. To estimate the
costs of backward processes, (17) and (28) are considered
for SGD and SWDP, respectively. In SGD, the computational
cost is 𝑏(2𝑘 + 2), which denotes the inner product of the
propagated error components and weights in the 𝑠 + 1 layer
for all samples in the mini-batch 𝑏. In SWDP, the cost 𝑘
indicates the inner product of weights and their differences
in the 𝑠 + 1 layer. Therefore, it can be seen from TABLE
15
Copyright (c) IARIA, 2023.     ISBN:  ISBNFILL
ICCGI 2023 : The Eighteenth International Multi-Conference on Computing in the Global Information Technology

I that the proposed SWDP can reduce the computational
complexity in the backward calculations of the training to
1/𝑏 compared to SGD. Here, the computational cost of
(
1/𝑏 ∑
𝑝∈𝑋 𝑓 ′(𝑧𝑠
𝑖,𝑝)/𝑥𝑠
𝑖,𝑝 · 𝑥𝑠−1
𝑗,𝑝
)
in (28) is 2𝑏+1, but this term
can be calculated in forward process. Therefore, this cost is
ignored in the backward process of SWDP.
TABLE I
SUMMARY OF THE COMPUTATIONAL COST.
Algorithm
Computational Cost
of Backward
SGD
b(2k + 2) + 1
SWDP
k + 1
V. SIMULATION RESULTS
To investigate the performance of the proposed method,
SWDP is compared with SGD on two classiﬁcation bench-
mark problems. For all problems, the learning rate is set to
𝜂 = 0.1 for both algorithms. The mini-batch sizes are set to
𝑏 = 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, and 32 for the problems.
The termination condition were set to 𝐸(w) < 10−3 and the
maximum iteration counts 𝑡𝑚𝑎𝑥 = 500, 000.
A. 8 × 8 MNIST
The ﬁrst problem is the classiﬁcation problem of MNIST
handwritten digits dataset [10] with 8 × 8 pixels as shown in
Fig. 3. The 8×8 MNIST problem classiﬁes a handwritten digit
image of |𝑇| = 1, 797 training data samples into ten classes
from 0 to 9. In this experiment, the dataset was randomly
divided to 75% (1,347) as the training data 𝑇𝑟 and 25% (450)
as the test data 𝑇𝑠. The network structure is 64-10-10, which
denotes inputs, the number of neurons in a hidden layer, and
the outputs. In this problem, the activation functions for the
hidden and the output layers were set to the sigmoid and the
softmax of (13) functions, respectively. The error function is
cross-entropy of (12). The experimental results are shown in
Fig. 4 and TABLE II. In the ﬁrst, Fig. 4 shows the training
and test accuracies for each mini-batch size, where the 𝑥-axis
is the mini-batch size, and the 𝑦-axis is the accuracy. This
ﬁgure shows that the training accuracies of SGD and SWDP
are almost the same. In terms of test accuracy, both algorithms
maintain almost the same accuracy too. That is, SWDP was
Fig. 3. Examples of 8 × 8 MNIST handwritten digits dataset.
Fig. 4. Accuracy .vs. mini-batches for 8 × 8 MNIST.
up to 3% less accurate than SGD. Therefore, it concluded
that training with almost the same accuracy is possible even
if simpliﬁed backward calculation. The table summarizes the
epoch, iteration, time, and time per iteration required for the
convergence of SGD and SWDP. The table shows that SWDP
requires more epoch, iteration, and time as the mini-batch size
increases. It can be attributed to (23) becoming stronger as the
mini-batch size increases. However, the computation time per
iteration of the proposed SWDP is shorter than SGD because
the backward computation is simpliﬁed. Therefore, it can be
concluded that the proposed SWDP method is practical for
this problem.
B. 3-Spiral
The next problem is another nonlinear problem called the
3-Spiral problem [11]. 3-Spiral is a problem of classifying
training data samples 𝑇𝑟 = 1, 050 into three classes, as shown
in Fig. 5, where each class has 350 data samples. The input
data is the coordinates of each point, and the NN structure is 2-
10-3, which denotes inputs, hidden layer neurons, and outputs
numbers, respectively. The activation and error functions were
the same as the previous problem. The experimental results
are shown in Fig. 6. Fig. 6 shows the training accuracy for
each mini-batch size, where the x-axis is the mini-batch size,
and the y-axis is the training accuracy. This ﬁgure shows that
the proposed method and SGD have similar accuracy when
the mini-batch size 𝑏 = 1 and 2. However, as the mini-
batch size increases, the accuracy of the proposed SWDP
method decreases from 3% to a maximum of 22% (𝑏 = 10).
Therefore, in training this problem, the training of SWDP
strongly depends on the mini-batch size and the initial values.
VI. CONCLUSION
In this research, a novel training algorithm was proposed.
The proposed method was referred to as Stochastic Weight
Difference Propagation (SWDP) to simplify the gradient calcu-
lation (backward processes) in SGD. SGD, based on the error
propagation architecture from the output to the input layers, is
required for each training sample to calculate the gradient and
16
Copyright (c) IARIA, 2023.     ISBN:  ISBNFILL
ICCGI 2023 : The Eighteenth International Multi-Conference on Computing in the Global Information Technology

TABLE II
THE RESULTS SUMMARY OF 8 × 8 MNIST.
Algorithm
Mini-batch
Epoch
Iteration
Time
per Iteration
Size
(sec)
Time(msec)
SGD
1
28
39,062
0.364
0.9318 ×10−2
2
26
18,170
0.32
0.1761 ×10−1
4
26
9,071
0.284
0.3130 ×10−1
6
26
6,047
0.284
0.4696 ×10−1
8
26
4,535
0.268
0.5909 ×10−1
10
27
3,751
0.276
0.7358 ×10−1
12
26
3,023
0.262
0.8666 ×10−1
14
26
2,591
0.277
0.1069
16
26
2,267
0.268
0.1182
18
26
1,997
0.27
0.1352
20
26
1,808
0.263
0.1454
32
27
1,175
0.274
0.2331
SWDP
1
28
39,062
0.366
0.9369 ×10−2
2
92
62,588
0.947
0.1513 ×10−1
4
192
64,847
1.826
0.2815 ×10−1
6
625
140,223
5.614
0.4003 ×10−1
8
860
144,647
7.691
0.5317 ×10−1
10
1,233
165,355
10.943
0.6617 ×10−1
12
2,254
252,559
19.474
0.7710 ×10−1
14
3,312
318,047
28.558
0.8979 ×10−1
16
4,922
413,499
41.972
0.1015
18
5,006
370,517
42.398
0.1144
20
5,898
395,232
50.48
0.1277
32
11,905
500,000
99.671
0.1993
Fig. 5. Layout of the 3-Spirals dataset.
causes an increase in hardware and computational costs. The
proposed SWDP was focused on the disadvantage of SGD
and reduced the computational complexity in the backward
calculations of the training compared to SGD. The proposed
SWDP simpliﬁes the backward process of SGD using only the
inner product of the weights and their differences in training.
The computational costs of the backward processes during
SWDP training are reduced to 1/𝑏 (𝑏 is the mini-batch size)
compared to SGD. Experimental results showed that SWDP
could learn accurately close to SGD even when the backward
processes were simpliﬁed. However, the number of epochs
Fig. 6. Training accuracy for mini-batches for 3-Spirals.
required for learning SWDP increased as the mini-batch size
increased. This is caused by the fact that SWDP ignored the
covariance term required for the SGD backward processes.
In future works, the proposed method SWDP will be
improved to achieve more robust learning similar to SGD re-
gardless of the mini-batch size, initial values, and nonlinearity
of the problem. In addition to investigating the effectiveness
of the proposed method on edge computing, we plan to
implement SWDP on hardware such as FPGA to verify its
effectiveness.
ACKNOWLEDGMENT
The authors gratefully acknowledge the Honjo International
Scholarship Foundation for supporting this work. The Japan
Society supported this work for the Promotion of Science
(JSPS), KAKENHI (20K11979).
REFERENCES
[1] I. Goodfellow, Y. Bengio and A. Courville, “Deep Learning”, MIT Press,
2016.
[2] S. Haykin: “Neural Networks and Learning Machines”, Pearson, 2009.
[3] D. H. Lee, S. Zhang, A. Fischer and Y. Bengio, ”Difference target
propagation,” Proc. ECML/PKDD, Springer, Cham, pp. 498–515, 2015.
[4] T. P. Lillicrap, D. Cownden, D. B. Tweed and C. J. Akerman, ”Ran-
dom synaptic feedback weights support error backpropagation for deep
learning,” Nature Communications, 2016.
[5] A. Nøkland, ”Direct feedback alignment provides learning in deep neural
networks,” Proc. NeurIPS, vol. 29, 2016.
[6] T. Miyato, D. Okanohara, S. I. Maeda and M. Koyama, ”Synthetic gradi-
ent methods with virtual forward-backward networks,” Proc. Workshop
trac - ICLR, 2017.
[7] R. D. Brandt and F. Lin, ”Supervised Learning in neural networks with-
out explicit error backpropagation,” Proc. Annual Allerton Conference
on Communication Control and Computing, pp. 294–303, 1994.
[8] R. D. Brandt and F. Lin, ”Can supervised learning be achieved without
explicit error backpropagation?”, Proc. IEEE ICNN, pp. 300–305, 1996.
[9] H. Ninomiya and N. Kinoshita, ”A new learning algorithm without
explicit error backpropagation,” Proc. IJCNN’99, vol.2, No.99CH36339,
pp. 1389–1392, 1999.
[10] E. Alpaydin and C. Kaynak, “Optical recognition of handwritten digits
data set”, UCI Machine Learning Repository, 1998.
[11] J. Bassey, X. Li and L. Qian, “An Experimental Study of Multi-Layer
Multi-Valued Neural Network”, Proc. IEEE ICDIS, pp. 233–236, 2019.
17
Copyright (c) IARIA, 2023.     ISBN:  ISBNFILL
ICCGI 2023 : The Eighteenth International Multi-Conference on Computing in the Global Information Technology

