Facial Mimicry Analysis Based on 3D Morphable Face Models
Oky Dicky Ardiansyah Prima, Yuta Ono, 
Hisayoshi Ito
Grad. School of Soft. and Inf. Sci., Iwate Pref. Univ.
Takizawa, Japan
email: {prima, hito}@iwate-pu.ac.jp, 
g236s001@s.iwate-pu.ac.jp
Takahiro Tomizawa
Hitachi Ind. & Ctrl. Solutions
Yokohama, Japan
email: Takahiro.tomizawa.ax 
@hitachi.com
Takashi Imabuchi
Office of Regional Collaboration, 
Iwate Pref. Univ.
Takizawa, Japan
email: t_ima@ipu-office.iwate-pu.ac.jp
Abstract—Facial mimicry is an important non-verbal 
communication that can promote favorable social behavior 
and positive relationships. As recent computer vision 
technologies have reached the level of human perception for 
processing facial information, the study of automated analysis 
of facial mimicry has attracted the attention of the Human 
Computer Interaction (HCI) society. In this study, we propose 
a system to evaluate the similarity of facial images based on the 
shape of the face derived from 3-Dimensional (3D) face data. 
Two different 3D face data were used in this study: a 3D Digital 
Character (3DDC) and the Surrey 3D Morphable Face Model 
(3DMFM). Our approach consists of the following steps: (1) 
landmark extraction from the facial image; (2) 3D shape 
fitting; (3) similarity analysis for the face point cloud. Our 
results show that the similarity between faces can be assessed 
by analyzing the non-rigid portions of the faces. The proposed 
system can be extended as a facial mimicry training tool to 
improve social communication. 
Keywords-mimicry; 
expression 
training;
emotion; 
image 
processing.
I.
INTRODUCTION
People often consciously or unconsciously mimic the 
facial expressions of their conversation partners. This type of 
non-verbal communication is important in providing 
additional information beyond verbal communication. This 
study extends our previous research on a facial mimicry 
training based on 3-Dimensional (3D) morphable face 
models 
[1]. 
Non-verbal 
communication 
refers 
to 
communication methods without using languages, such as 
gestures, facial expressions, tone of voice, eye contact, and 
posture. Unconscious mimicry increases affiliation, which 
helps to foster relationships with others [2].
Facial expressions involve
signals of a larger 
communicative process, conveying the focus of attention, 
intention, motivation, and emotion. For instance, a smile with 
the corners of the mouth turned up to expose the front teeth 
may express joy. A frown, typically with the corners of the 
mouth turned down, forms an expression of disapproval.
Some researchers believe that emotions are a universal 
construct. However, differences in culture can lead to 
differences in the absolute level of intensity of emotions [3].
The FACS (Facial Action Coding System), which is a set 
of facial muscle movements corresponding to displayed 
emotions, is a traditional measure for analyzing facial 
expressions [3]. The movements of individual facial muscles 
are encoded by FACS from slightly different instantaneous 
changes in facial appearance. Each action unit (AU) is 
described in the FACS manual.
Manual coding of video recordings of participants 
according to the units of action described by FACS takes a 
significant amount of time and effort. Automatic analysis of 
facial expressions has received a great attention from the 
computer vision community. Bartlett et al. (1999) made an 
early attempt to automate facial expressions using FACS by 
applying computer image analysis to classify the basic 
elements that comprise complex facial movements [4]. Using 
template matching to detect facial features, such as lips, eyes, 
brows and cheeks, Tian et al. (2000) developed an Automatic 
Face Analysis (AFA) system [5], which recognizes changes 
in facial expression into AUs.
Recent computer vision technology has made it possible 
to perform conventional face recognition processes such as 
face detection, face matching, facial feature extraction, and 
facial expression classification in real time. Baltrušaitis et al. 
(2018) developed an open source facial behavior toolkit, 
OpenFace 2.0. This toolkit uses a linear kernel support vector 
machine to detect facial expressions by detecting the unit of 
motion (AU) of the face [6]. iMotions [7] is a commercial 
tool for automatic analysis of facial expressions. The tool 
allows the user to select the FACET [8] or AFFDEX [9]
algorithm for facial expression recognition.
In contrast to the growing interest in the application of 
automated facial expression analysis, there have been 
surprisingly few attempts to measure the similarity of facial 
expressions. In our previous work, we used a deformable 3D
Digital Character (3DDC) to find the most similar shape to a 
given facial image [1]. The similarity of the faces was 
measured by calculating the relationship between the 3D 
point clouds obtained from each face.
In this study, we extend our previous work to evaluate the 
similarity of facial images based on the shape of the face 
derived from two different face data: a 3DDC [10] and the 
Surrey 3D Morphable Face Model (3DMFM) [11]. We fit 
these models to the corresponding facial images and derive 
the facial features that are important for similarity analysis. 
The rest of this paper is organized as follows. Section II 
discusses known approaches to facial expression imitation. 
Section III introduces our proposed facial mimicry analysis 
system. Section IV shows our experiment results. Finally, 
Section V gives a short conclusion and highlights the most 
important outcomes of this paper.
274
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

II. RELATED WORK
Measuring the degree of similarity between facial images 
is an important task in the field of face recognition. Many 
studies have used dimensional reduction techniques between 
feature representations to infer similarity. These studies 
include Eigenfaces [12] and Fisherfaces [13], which have 
been used in traditional face recognition.
An attempt to construct a facial similarity map was made 
by Holub et al. (2007). This map was calculated based on 
Triplets [14]. The resulting map demonstrated that the 
resulting map can effectively create a metric space that 
preserves notions of facial similarity.
The drawback of existing automatic facial expression 
recognition methods is that most of them focus on AU 
detection. Sadovnik et al. (2018) suggested that face 
recognition and face similarity are correlated, but the two are 
inherently different [15]. Since similarity in facial images can 
be recognized even when they are not the same person, there 
is a need to build a new dataset corresponding to the 
similarity of facial expressions. Vemulapalli and Agarwala 
(2019) built a large faces-in-the-wild dataset that labels facial 
expressions based on human visual preferences [16] and 
visualized the similarity of faces using t-SNE [17].
As described above, many studies have analyzed face 
similarity using 2-Dimensional (2D) facial images. However, 
Figure 2. Generation of 3D face data for this study. 
Figure 3. The experiment setup in this study.
Facial
Image
Landmark
Extraction
Shape-to-landmark 
Fitting (3DDC)
Shape-to-landmark 
Fitting (3DMFM)
3D Face
3D Face
50cm
Display
Expression
Intensity
0.25
0.5
0.75
1.0
Joy
Surprise
Figure 1. Some facial expressions generated using 3DDC.
275
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

only few studies have used 3D facial images. Moorthy et al. 
(2010) extracted Gabor features from automatically detected 
fiducial points of texture images from the 3D face and 
demonstrate that these features correlate well with human 
judgements of similarity [18].
After Blanz and Vetter (1999) introduced a 3DMFM as a 
general face representation and a principled approach to 
image analysis [19], 3DMFM have incorporated into many 
solutions for facial analysis. 3DMFM generates a 3D face 
data from a given facial image and modifying the shape and 
texture in a natural way using its Principal Component 
Analysis (PCA) model of face shape and color information.
Huber et al (2016) have made a publicly available 3DMFM, 
accompanied by their open-source software framework [11].
Another technique to generate an associate 3D face data
from a given facial image is the deformation transfer. It is a 
well-recognized technique in computer graphics that creates 
expressive and plausible animations. Sumner and Popović 
(2004) proposed deformation transfer for triangle meshes, 
where the cloning process does not require the source and the 
target model to share several vertices or triangles [10]. Prima 
et al. (2020) demonstrated deformations of faces based on 
their expressions [1] as an initial attempt to measure 
similarity of pairs of 3D faces (Figure 1). 
Facial
Actions
Target Faces
Mimicry
Subject I
Subject II
Subject III
Subject IV
A
B
C
D
2
E
F
Figure 4. The target face and facial mimicry performed by each subject.
276
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

To fit a 3D face data into a facial image, some points of 
the model need to be associated to the corresponding 2D 
landmark points in the facial image. Extracting landmark 
points from facial images can be done using automated facial 
landmarks tools, such as Dlib library [20]. Those 2D 
landmark points are mapped into the 3D face data using a 
shape-to-landmarks fitting method [21].
III. FACIAL MIMICRY ANALYSIS
Our attempt to analyze the facial mimicry of a pair of 
facial images takes three steps. The first step is to generate 
3D face data from the facial images. The resulting 3D face 
data is then registered in a common 3D coordinate space. The 
second step is to sampling point clouds from the 3D face data.
In the last step, we verify the similarity of the faces by 
analyzing the point clouds that correspond to each face. Each 
of these steps is described below.
1) STEP 1: Generation of 3D Face Data 
Here, 3DDC and 3DMFM were generated from the input 
face images, respectively. Figure 2 shows the flow for 
generating the 3D face data in this study. At first, 68 facial 
landmark points were extracted from the face image using 
the Dlib library. The 3D face data was then fitted to these 
landmark points to generate 3D face data that represents the 
original face image. For the 3DDC, the shape-to-landmark 
Figure 5. Differences in distance between the point clouds of 3DDC for the target face and the mimetic face of subject I.
A                     B                     C                    D                     E                    F
A
B
C
D
E
F
Subject
Target
low
high
277
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

calculation was performed by solving the Perspective-n-
Point (PnP) problem and deformation transfer [10]. The 
resulting rotation vector from the PnP were applied to the 
3DDC so that the 3DDC was oriented the same as the input 
facial image. For the 3DMFM, pose estimation and shape 
fitting was performed using a lightweight 3DMFM fitting 
library, “eos” [22]. The generated 3D face data is saved in 
OBJ file format.
2) STEP 2: Point Cloud Extraction
We extracted point clouds from the resulting 3D face 
data obtained from the previous step. Open3D [23], an open-
source library for analyzing 3D data, was used for this 
purpose. Sampling was done uniformly from the 3D surface 
based on the triangle area. Here, due to the different scales 
of 3DDC and 3DMFM, the sampling interval was adjusted 
to have the same number of sampling data from both face 
data.
3) STEP 3: Similarity Analysis
In order to analyze the similarity between facial images, 
the distance of each point on the original face was calculated 
to determine the distance to the target face. The internal 
functions of Open3D were used to calculate the distance.
Here, the shorter the distance, the more similar the faces are.
Figure 6. Differences in distance between the point clouds of 3DMFM for the target face and the mimetic face of subject I.
A                     B                     C                    D                     E                    F
A
B
C
D
E
F
Subject
Target
low
high
278
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Considering that the non-rigid motion of the face is 
dominant when mimicking the opponent's face, we cropped 
the mouth and analyzed the correlation between the point 
clouds of this area.
IV. RESULT
Four male subjects (mean age 20.6 years) were recruited 
for the experiments. All subjects agreed to participate and 
signed the consent forms, to allow their data to be used in 
publications of this study. The room was set up with a table 
and stool for the subjects. On the table was a laptop 
computer that was used to display the target face to be 
mimicked by the subject. The monitor resolution was set at 
1,440 × 900 and the refresh rate was 60 Hz. The subject was 
seated at approximately 50 cm from the laptop. Figure 3
shows the experimental setup in this study.
Subjects completed one practice block followed by six 
experimental blocks. For every block, a target face to mimic 
was displayed on the laptop. Subjects were asked to press 
the space button on the keyboard when they best imitate the 
target's face image. By pressing this button, the computer's 
built-in camera will take a picture of the subject's face.
Finally, subjects were debriefed by the experimenter about 
the purpose of the study. Figure 4 shows the target face and 
facial mimicry performed by each subject. Subjects are
imitating not only the facial expression of the target, but also 
the head posture of the target face.
1) Differences in distance between the 3D face data
3DDC and 3DMFM were used to generate 3D face data 
for target and mimetic faces. The difference in distance 
between the point clouds of 3D face data for the target face 
and the mimetic face of each subject was calculated. In order 
to be able to compare the two data correctly, we performed 
frontalization to each 3D face data in advance.
Figures 5 and 6 show the calculation results for 3DDC 
and 3DMFM of subject I, respectively. The colors indicate 
the degree of difference between the target face and the 
imitated face. Ideally, there will be the least difference 
between the two 3D face data in the right downward 
diagonal direction. In Figure 5, no facial actions yield the 
most similar (least different) results in that direction.
However, in Figure 6, facial action: D, E, and F (dotted 
rectangles) were found to be the most similar between pairs, 
indicating subject I properly mimic the given three target 
faces.
In the case of the 3DDC face data, there are large 
differences in non-rigid areas such as the forehead, cheeks, 
eyebrows, and mouth. Similar results were seen in the 
3DMFM face data, but the color distribution appeared to be 
less. Interestingly, there were significant differences at 
around the area of the nose, which is the rigid area. To figure 
out what caused these differences, we observed the 3D face 
data generated from the same person but in different head
poses. 
Figure 7 shows the 3D face data generated from the two 
target faces: B and E. As described in the previous section, 
we applied the initial 3DMFM to these faces using the 
shape-to-landmark method. However, after the 3D face data 
was frontalized, the difference in the shape of the nose are 
observable even in the same person's face. This suggests that 
the resulting frontalized face data contain some degree of 
distortion. Therefore, 3DMFM may not be supposed to 
generate 3D facial data with extreme head postures. This 
drawback also applies to 3DDC.
Figure 7. Distortions around the nose after the face frontalization.
(from left: 3DMFM aligned to the target face, face cropping, face frontalization (isomap), and distortion at the nose)
Facial actions: E
Facial actions: B
279
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Figure 8 shows the difference in distance between the 
3DDC of the subject faces and the 3DDC of the mimetic 
faces of all subjects. Similarly, Figure 9 shows the difference 
in distance between the 3DMFM of the subject faces and the 
3DMFM of the mimetic faces of all subjects. Values were 
standardized to enable comparison among different data 
sources: 3DDC and 3DMFM. These values were colored in 
three levels to make them visually distinguishable. The 
values surrounded by dotted rectangles indicate the smallest 
difference between the presented face image and the 
imitated face image. To summarize, the 3DMFM sees that 
subjects I and III imitated the target faces of C, D and E, 
whereas, subject II imitated the target faces of B, C and E.
Unfortunately, we were not able to see the suitability of 
facial mimicry in 3DDC just by calculating the difference in 
distance of the 3D face data. Overall, these results indicated 
that measuring the similarity between two 3D face data 
based solely on the differences between them is not 
sufficient.
2) Correlation Analysis between the 3D face data
For a more detailed analysis, we analyzed the similarity 
between two 3D facial data for point clouds in and around 
the mouth.  Here, since the 3DDC is a complete head model, 
a 3D bounding box was needed to crop the mouth region. 
Open3D was used to perform this task.
Table I shows the correlations between the target face 
and the mimetic face in the mouth region of 3DDC for all 
subjects, whereas Table II shows the correlations in the 
mouth region of 3DMFM. In the 3DDC, the correlation was 
highest when the subject's facial expression was the same as 
that of the presenting face (values surrounded by a square on 
the diagonal). However, it might be difficult to assess facial 
mimicry solely based on this value because of the overall 
high values of the correlation coefficients in all data, as 
shown in Table I. On the other hand, the values of 
correlation coefficients obtained from the 3DMFM varied, 
but the correlation coefficients were found to be high when 
the target face's facial action were consistent with those of 
the subject. This finding indicates that 3DMFM is more 
suitable than 3DDC for evaluating facial mimicry.
3) Perceptual Judgment
In order validate the resulted facial mimicry performed 
by each subject, a 5-point Likert scale was used to gauge 
similarity. A value of 1 indicated that the two faces were 
completely dissimilar, while a value of 5 indicated that the 
two faces being compared were identical. Ten independent 
Figure 8. Differences in distance among 3DDC of the target faces and the mimetic faces of all subjects.
A
1.54
1.82
0.31
0.24
0.84
0.93
A
0.63
0.82
2.66
2.31
0.99
0.31
B
0.96
2.92
0.59
0.48
1.37
1.72
B
1.29
1.73
2.01
1.48
0.76
0.62
C
1.11
2.10
0.97
0.35
0.64
1.32
C
1.04
1.05
1.99
1.98
0.50
0.98
D
1.79
1.49
0.75
0.41
0.01
0.52
D
0.35
0.29
2.89
2.66
1.20
0.76
E
1.53
1.84
0.59
0.33
0.53
0.87
E
0.49
0.70
2.56
2.31
0.95
0.56
F
0.46
5.23
2.60
2.45
3.40
3.75
F
3.41
3.98
1.02
0.06
1.59
2.63
A
B
C
D
E
F
A
B
C
D
E
F
A
0.25
0.73
1.60
0.30
0.20
1.14
A
2.67
0.68
1.03
1.35
0.57
1.37
B
0.00
1.65
2.49
0.58
0.60
0.71
B
1.85
1.52
0.48
0.96
0.80
0.96
C
0.21
0.96
1.67
0.98
0.53
0.49
C
2.25
1.16
1.31
0.43
0.44
0.44
D
0.41
0.46
0.96
0.75
0.18
1.07
D
3.09
0.53
1.47
1.10
0.33
1.11
E
0.11
0.69
1.46
0.57
0.06
0.95
E
2.71
0.79
1.14
1.06
0.39
1.07
F
1.97
3.88
4.65
2.63
2.76
1.31
F
0.15
3.50
1.52
1.46
2.68
1.47
A
B
C
D
E
F
A
B
C
D
E
F
Target
Target
Subject I
Subject II
Target
Target
Subject III
Subject IV
280
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

raters were asked to rate the similarity between their faces 
and the presented faces shown in Figure 4. The raters were 
instructed to focus on the three areas of the eyebrows, eye 
shape, and mouth, which varied greatly.
To evaluate whether there was a difference in the mean
score on the perceived similarity of pairs of faces, we 
conducted a one-way ANOVA. The means and standard 
deviations are shown in Table III. These results showed that 
there was a significant effect of the type of the facial action 
on the perceptual judgment level for the six conditions [F(5, 
234) = 2.25, p < 0.01]. The target face and the face imitated 
by the subject were perceived to be similar, except for the 
facial action C.
V. CONCLUDING REMARKS
In this study, we extended our previous work to assess the 
similarity of pairs of 3D face data by using differences in the 
distance and correlation of point clouds in the 3D face data. 
The face data use in our study was generated using 
deformable 3-Dimensional Digital Character (3DDC) and 
the Surrey 3-Dimensional Morphable Face Model (3DMFM). 
We utilized the deformation transfer technique to clone the 
subject’s facial movements into the 3DDC when mimicking 
a given facial image. The 3DMFM allows for a closer 
correspondence between the points, rather than only a set of 
facial feature points as in 3DDC.
To analyze the similarity of pairs of 3D face data, all 3D 
face data generated in this study were frontalized. We 
believed that comparison of faces can be made more effective 
by using a face-frontal view. However, while analyzing the 
experimental data, we noticed that some facial parts were 
distorted by this process. Therefore, we suggest that we need 
to be aware of this distortion when generating 3D facial data 
with extreme head posture.
Our experimental results show that measuring similarity 
using only the differences between point clouds of 3D face 
data is not sufficient. However, for the non-rigid part of the 
face, the similarity between the two 3D facial data can be 
measured by performing correlation analysis.
The proposed system can be extended as a face imitation 
training tool to improve social communication. Using this 
tool, users can practice imitating faces from photographs by 
referring to the differences in the reconstructed 3D facial data.
ACKNOWLEDGMENT
This work was supported by the grant of the Iwate 
Prefectural University's strategic project. We also thank the 
Figure 9. Differences in distance among 3DMFM of the target faces and the mimetic faces of all subjects.
A
0.31
0.35
1.38
2.08
0.49
4.55
A
1.02
0.62
2.40
1.64
0.67
3.19
B
0.16
0.37
1.22
1.75
0.66
3.92
B
0.77
0.42
1.99
1.32
0.96
2.72
C
1.69
2.23
1.05
1.44
2.71
1.55
C
1.00
1.33
0.39
0.40
3.10
0.28
D
2.32
2.78
1.52
0.81
3.20
3.37
D
1.70
1.95
2.11
1.47
3.59
2.19
E
1.21
0.66
2.67
3.23
0.51
5.03
E
1.80
1.39
3.12
2.76
0.00
4.02
F
2.06
2.63
1.29
1.56
2.99
1.65
F
1.41
1.75
0.81
0.67
3.36
0.39
A
B
C
D
E
F
A
B
C
D
E
F
A
1.07
0.19
2.10
1.63
0.89
2.37
A
1.41
0.59
0.93
0.99
0.89
1.97
B
0.78
0.31
1.67
1.34
1.23
1.94
B
1.14
0.74
0.87
0.76
0.91
1.63
C
0.77
2.22
0.33
1.52
3.42
0.71
C
1.24
2.53
2.12
1.11
2.00
1.08
D
1.68
2.65
2.01
1.25
3.78
2.26
D
2.17
2.88
2.52
1.78
2.39
2.21
E
2.18
0.80
2.87
2.60
0.18
3.07
E
2.13
0.76
1.36
1.97
1.41
2.74
F
1.09
2.56
0.71
1.67
3.68
1.13
F
1.58
2.91
2.35
1.42
2.26
1.49
A
B
C
D
E
F
A
B
C
D
E
F
Target
Subject I
Target
Subject II
Subject III
Subject IV
Target
Target
281
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

editor and three anonymous reviewers for their constructive 
comments, which helped us to improve the manuscript.
REFERENCES
[1]
O. D. A. Prima, H. Ito, T. Tomizawa, and T. Imabuchi, “Facial 
Mimicry Training Based on 3D Morphable Face Models,” 
The Thirteenth International Conference on Advances in 
Computer-Human Interactions, ACHI 2020, pp. 57-60, 2020.
[2]
J. L. Lakin, V. E. Jefferis, C. M. Cheng, and T. L. Chartrand,
“The Chameleon Effect as Social Glue: Evidence fot the 
Evolutionary Significance of Nonconscious Mimicry,”
Journal of Nonverbal Behavior, 27(3), pp. 145–162, 2003.
[3]
P. Ekman et al., “Universals and Cultural Differences in the 
Judgments of Facial Expressions of Emotion,” Journal of 
Personality and Social Psychology, 53(4), pp. 712-717, 1987.
[4]
M. S. Bartlett, J. C. Hager, P. Ekman, and T. J. Sejnowski, 
“Measuring Facial Expressions by Computer Image Analysis,” 
Psychophysiology, Cambridge University Press, 36(2), pp.
253–263, 1999.
[5]
Y. L. Tian, T. Kanade, and J. F. Cohn, “Recognizing Lower 
Face Action Units for Facial Expression Analysis.” 
Proceedings -
4th IEEE International Conference on 
Automatic Face and Gesture Recognition, FG 2000, 23(2), pp.
484–490, 2000.
[6]
T. Baltrusaitis, A. Zadeh, Y. C. Lim, and L-P. Morency, 
“OpenFace 2.0: Facial Behavior Analysis Toolkit,” IEEE 
International Conference on Automatic Face and Gesture 
Recognition, 2018.
[7]
iMotions, https://imotions.com/ [retrieved: August 31, 2020]
[8]
G. Littlewort et al., "The computer expression recognition 
toolbox (CERT)," Face and Gesture 2011, Santa Barbara, CA, 
2011, pp. 298-305, doi: 10.1109/FG.2011.5771414.
[9]
D. McDuff et al., “AFFDEX SDK: A Cross-Platform Real-
Time 
Multi-Face 
Expression 
Recognition 
Toolkit,” 
TABLE I. CORRELATIONS BETWEEN THE TARGET FACE AND THE MIMETIC 
FACE IN THE MOUTH REGION OF 3DDC.
TABLE II.
CORRELATIONS BETWEEN THE TARGET FACE AND THE 
MIMETIC FACE IN THE MOUTH REGION OF 3DMFM.
A
B
C
D
E
F
A
B
C
D
E
F
A
0.99
0.91
0.88
0.9
0.87
0.92
A
0.99
0.67
0.71
0.68
0.6
0.66
B
0.91
0.99
0.89
0.92
0.89
0.93
B
0.67
0.99
0.68
0.76
0.74
0.59
C
0.89
0.91
0.99
0.93
0.92
0.93
C
0.74
0.66
0.99
0.69
0.6
0.77
D
0.91
0.93
0.93
0.99
0.92
0.93
D
0.68
0.73
0.71
0.99
0.69
0.62
E
0.87
0.9
0.92
0.93
0.99
0.92
E
0.61
0.76
0.61
0.73
0.99
0.44
F
0.89
0.93
0.91
0.91
0.9
0.99
F
0.76
0.68
0.79
0.68
0.56
0.97
A
B
C
D
E
F
A
B
C
D
E
F
A
0.99
0.91
0.88
0.89
0.85
0.91
A
0.99
0.68
0.73
0.69
0.64
0.72
B
0.91
0.99
0.89
0.91
0.87
0.93
B
0.67
0.99
0.70
0.77
0.76
0.66
C
0.89
0.91
0.98
0.92
0.91
0.93
C
0.75
0.68
0.99
0.69
0.64
0.78
D
0.91
0.92
0.93
0.99
0.91
0.92
D
0.68
0.74
0.73
0.99
0.72
0.68
E
0.87
0.9
0.93
0.93
0.99
0.91
E
0.60
0.75
0.65
0.75
0.99
0.53
F
0.89
0.93
0.91
0.9
0.88
0.99
F
0.77
0.70
0.78
0.68
0.6
0.99
A
B
C
D
E
F
A
B
C
D
E
F
A
0.99
0.92
0.89
0.91
0.87
0.91
A
0.98
0.69
0.71
0.7
0.65
0.64
B
0.91
0.99
0.90
0.92
0.89
0.93
B
0.66
0.99
0.68
0.74
0.77
0.56
C
0.90
0.91
0.99
0.93
0.92
0.93
C
0.75
0.68
0.99
0.73
0.64
0.75
D
0.92
0.93
0.93
0.99
0.92
0.92
D
0.68
0.74
0.71
0.99
0.72
0.6
E
0.88
0.9
0.93
0.93
0.99
0.91
E
0.58
0.75
0.62
0.70
0.99
0.41
F
0.9
0.93
0.92
0.91
0.9
0.99
F
0.77
0.71
0.78
0.72
0.6
0.96
A
B
C
D
E
F
A
B
C
D
E
F
A
0.99
0.91
0.9
0.9
0.85
0.91
A
0.93
0.72
0.73
0.72
0.67
0.72
B
0.91
0.99
0.90
0.91
0.88
0.93
B
0.61
0.98
0.69
0.76
0.76
0.66
C
0.89
0.90
0.99
0.93
0.91
0.93
C
0.76
0.73
0.99
0.75
0.72
0.79
D
0.91
0.92
0.94
0.99
0.91
0.92
D
0.64
0.75
0.72
0.99
0.74
0.68
E
0.87
0.90
0.93
0.93
0.99
0.91
E
0.51
0.72
0.63
0.72
0.94
0.53
F
0.89
0.92
0.92
0.90
0.89
0.99
F
0.79
0.76
0.79
0.74
0.67
0.99
Subject III
Target
Subject IV
Target
Subject I
Target
Subject II
Target
Subject I
Subject II
Subject III
Subject IV
Target
Target
Target
Target
TABLE III.
THE MEANS AND STANDARD DEVIATIONS
OF THE 
PERCEPTUAL JUDGEMENT.
FACIAL ACTION
A
B
C
D
E
F
MEAN
(STDEV)
2.9
(1.16)
3.3
(1.10)
2.2
(1.19)
4.2
(0.94)
4.0
(0.88)
3.8
(1.07)
282
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Conference on Human Factors in Computing Systems, pp. 
3723–3726, 2016. https://doi.org/10.1145/2851581.2890247
[10] R. W. Sumner and J. Popović, “Deformation Transfer for 
Triangle Meshes,” ACM Transactions on Graphics, 23(3), pp. 
399-405, 2004.
[11] P. Huber et al., “A Multiresolution 3D Morphable Face Model 
and Fitting Framework,” 11th International Joint Conference 
on Computer Vision, Imaging and Computer Graphics Theory 
and Applications, pp. 79–86, 2016.
[12] M. A. Turk and A. P. Pentland. Face Recognition Using 
Eigenfaces. In Computer Vision and Pattern Recognition, 
1991. Proceedings CVPR’91., IEEE Computer Society 
Conference on, pp. 586–591. IEEE, 1991.
[13] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, “Eigen 
faces vs. Fisherfaces: Recognition using class specific linear 
projection,” IEEE Transactions on pattern analysis and ma-
chine intelligence, 19(7), pp. 711–720, 1997.
[14] A. Holub, Y. Liu, and P. Perona, “On Constructing Facial 
Similarity Maps,” IEEE Computer Society Conference on 
Computer Vision and Pattern Recognition, pp. 17–22, 2007.
[15] A. Sadovnik, W. Gharbi, T. Vu, and A. Gallagher, “Finding 
Your Lookalike: Measuring face similarity rather than face 
identity,” IEEE Computer Society Conference on Computer 
Vision and Pattern Recognition Workshops, pp. 2235–2353, 
2018.
[16] R. Vemulapalli and A. Agarwala, “A Compact Embedding for 
Facial Expression Similarity,” Proceedings of the IEEE 
Computer Society Conference on Computer Vision and 
Pattern Recognition, pp. 5683–5692, 2019.
[17] V. D. M. Laurens and G. Hinton, “Visualizing Data Using t-
SNE” Journal of Machine Learning Research, 9, pp. 2579–
2605, 2008.
[18] A. K. Moorthy, A. Mittal, S. Jahanbin, K. Grauman and A. C. 
Bovik, “3D Facial Similarity: Automatic Assessment Versus 
Perceptual Judgments,” 2010 Fourth IEEE International 
Conference on Biometrics: Theory, Applications and Systems 
(BTAS), Washington, DC, pp. 1-7, 2010.
[19] V. Blanz and T. Vetter, “A Morphable Model for the 
Synthesis of 3D Faces,” In ACM Transactions on Graphics 
(Proceedings of SIGGRAPH), pp. 187–194, 1999.
[20] V. Kazemi and J. Sullivan, “One Millisecond Face Alignment 
with an Ensemble of Regression Trees,” Proceedings of the 
IEEE Conference on Computer Vision and Pattern 
Recognition, pp. 1867–1874, 2014.
[21] O. Aldrian and W. A. P. Smith, “Inverse Rendering of Faces 
with a 3D Morphable Model,” IEEE Transactions on Pattern 
Analysis and Machine Intelligence, 35(5), pp. 1080–1093, 
2013.
[22] eos: A Lightweight Header-Only 3D Morphable Face Model 
fitting library in modern C++11/14.
https://github.com/patrikhuber/eos
[retrieved: August 31, 2020]
[23] Open3D, http://www.open3d.org/ 
[retrieved: August 31, 2020]
283
International Journal on Advances in Software, vol 13 no 3 & 4, year 2020, http://www.iariajournals.org/software/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

