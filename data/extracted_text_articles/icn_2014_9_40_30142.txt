OpenFlow Networks with Limited L2 Functionality
Hiroaki Yamanaka, Eiji Kawai, Shuji Ishii, and Shinji Shimojo
Network Testbed Research & Development Promotion Center
National Institute of Information and Communications Technology
KDDI Otemachi Bldg. 21F, 1-8-1 Otemachi, Chiyoda-ku, Tokyo 100-0004, Japan
Email: {hyamanaka, eiji-ka, shuji, sshinji}@nict.go.jp
Abstract—OpenFlow enables ﬂexible control of network trafﬁc
with arbitrary ﬂow deﬁnitions. On carrier access networks,
OpenFlow can be used to provide customized network settings
for each client for virtual private network (VPN), Internet
Protocol television (IPTV), and content delivery network (CDN)
services, etc. Since there is a massive amount of trafﬁc in
carrier access networks, high performance switches are necessary.
However, costs tend to increase due to the number of switches.
An OpenFlow switch processes wide-range of header ﬁelds and
supports wildcard matching. Large spaces for ternary content
addressable memory (TCAM) and application-speciﬁc integrated
circuits (ASICs) are used for high performance lookups. Cur-
rently proposed techniques reduce the amount of energy that is
consumed by reducing the frequency of TCAM usage in switches.
However, these techniques require complex functionality in order
to manage matching in the switches. As a result, switches are still
expensive. In this paper, we propose a technique that enables the
construction of OpenFlow networks using switches that require
little more than L2 switch functionality. The functionalities that
are required for the switches include an OpenFlow interface
for handling the ﬂow table externally and a simple matching
function for the MAC header. Arbitrary ﬂow deﬁnitions from an
OpenFlow controller are translated to the ﬂow deﬁnitions by the
MAC address at the external proxy.
Keywords-OpenFlow; TCAM; L2 switch; carrier access
network
I.
INTRODUCTION
Software Deﬁned Networking (SDN) technologies (e.g.,
OpenFlow [1]) ease network management, service manage-
ment, and quality of service (QoS) provisioning. SDN tech-
nologies are being considered for use in carrier-grade net-
works. One possible model for applying SDN technologies
in carrier networks involves the control of ﬁned-grained ﬂows
using OpenFlow in carrier access networks and multi-protocol
label switching (MPLS) tunnels (i.e., relatively simple logic
for packet forwarding) in core networks [2].
When OpenFlow is deployed in carrier access networks, the
costs for infrastructure are relatively high. There are two rea-
sons for high costs. The ﬁrst is that there are many OpenFlow
switches in carrier access networks. The second reason is that
hardware OpenFlow switches are costly. The ternary content
addressable memory (TCAM) signiﬁcantly increases the costs
that are associated with a hardware-based OpenFlow switches.
TCAM is a special type of memory that enables matching
on the headers of received data packets during one clock
cycle, regardless of the number of entries in memory. This
capability of TCAM is preferable in carrier access networks
because it enables high performance for the forwarding of high
volumes of data packets. However, TCAM is power hungry
and expensive [3]. It has been noted that TCAM is up to
80 times more expensive than static random access memory
(SRAM) [4]. In an OpenFlow switch, the required TCAM
space is large due to the wide range of header ﬁelds that are
supported in OpenFlow [5].
Techniques have been proposed in the research community
for reducing the amount of power that is consumed by TCAM
in an OpenFlow switch. The basic idea is to decrease the
frequency of the usage of TCAM. Only the ﬁrst data packet
in a ﬂow is matched using TCAM and subsequent data
packets that have the same header ﬁelds are matched using
SRAM or binary content addressable memory (BCAM). In
DevoFlow [6], SRAM is used in conjunction with the hash
method in order to match the subsequent data packets. The
hash method improves the performance as far as possible when
SRAM is used. Generally, when the TCAM is avoided in a
switch, it is necessary to include chipsets in the switch for
complex packet processing (e.g., applying the hash function)
in order to obtain high performance packet forwarding.
In this paper, we propose a technique that retains the
high performance of during packet forwarding and lowers the
cost of switches for OpenFlow infrastructure through the use
of relatively simple and inexpensive devices. The idea is to
restrict the matching ﬁelds in the memory of a switch to the
source MAC header and allow the switch to perform matches
in a simple manner using the single source MAC header.
Meanwhile, the proposed technique enables an OpenFlow con-
troller and the end-hosts to use all of the header ﬁelds that are
supported in OpenFlow. The external proxy that is between an
OpenFlow controller and the switches translates the matching
ﬁelds that were originally deﬁned by the controller to the
matching ﬁelds for the source MAC addresses. Furthermore,
the proxy manages the edge switches in order to modify the
source MAC addresses for data packets and forward them to
the network. In the network, the source MAC address of a
data packet represents all of the original header ﬁelds. As a
result, the switches in the network only need to match on the
single source MAC header. The proposed technique enables
the construction of an OpenFlow network with switches that
are implemented using similar chipsets of L2 switches.
The remainder of this paper is organized as follows.
Section II describes the mechanism and the limitations of
221
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

the current technique for reducing TCAM usage. Section III
describes the concept and the architecture of our proposal. Sec-
tion IV describes the detailed implementation of the proposal.
Section V evaluates the overheads that are associated with the
proposal. Section VI contains remarks about related work and
Section VII presents the conclusion.
II.
REDUCING TCAM COST IN OPENFLOW
A typical hardware-based OpenFlow switch contains
TCAM for high performance data packet processing in a net-
work. An OpenFlow [1] network is composed of a controller
and a group of OpenFlow switches. The controller and the
OpenFlow switches communicate through a control plane on
the network in order to maintain ﬂow entries in the switches.
The OpenFlow switches transfer data packets on the data
plane of the network based on their ﬂow entries. A ﬂow
entry includes deﬁnitions of the ﬂows that are referred to as
the matching ﬁelds. The matching ﬁelds include the ingress
port number and the header ﬁelds from layers 2–4 that are
speciﬁed in the OpenFlow switch speciﬁcation [5]. Wildcards
are allowed for any of the matching ﬁelds. An OpenFlow
switch searches the ﬂow entries that need to be matched in
the header ﬁelds of each data packet that is received. TCAM
enables searching during one clock cycle, regardless of the
number of the entries in the TCAM and regardless of whether
wildcards are included or not included in the matching ﬁelds.
Since TCAM is power hungry and expensive, it increases
the infrastructure costs for OpenFlow networks tremendously.
State-of-the-art techniques have been proposed academic pa-
pers in order to reduce the frequency of the usage of TCAM
(i.e., energy consumption) in switches. These techniques allow
TCAM to only be used for matching for the ﬁrst data packets
that arrive, while subsequent data packets are matched using
SRAM or BCAM. A switch sees all of the header ﬁelds for
the ﬁrst data packet that are matched using TCAM. Then, it
sets up the matching ﬁelds for the subsequent data packets
using SRAM or BCAM. When SRAM is used, matching can
be implemented using the hash method for subsequent data
packets in a constant time, which is not one clock cycle of
central processing unit (CPU). When BCAM is used, matching
can be implemented for subsequent data packets in one clock
cycle of CPU.
In the section below, “wildcard matching ﬁelds” refers to
matching ﬁelds in which at least one header ﬁeld is a wildcard.
For an IP header, it may be the IP preﬁx. “Exact matching
ﬁelds”, on the other hand, refers to matching ﬁelds in which
there is no header ﬁeld with wildcards or IP preﬁxes.
A. Setting Exact Matching Fields
This section summarized the method for reducing the
frequency of TCAM usage. This method is found in De-
voFlow [6]. This method determines the exact matching ﬁelds
inheriting from the wildcards using the header ﬁelds of the
data packets that are arriving at the switch. The procedure for
setting the exact matching ﬁelds and the data packet processing
is as follows (Figure 1).
1)
The wildcard matching ﬁelds that are originally set
by the OpenFlow controller are memorized in TCAM
in the switch.
OpenFlow controller
Switch
BCAM or SRAM
TCAM
1)Se!ng the wildcard 
matching ﬁelds.
Ports
Data packet
2)Matching. 
If matched, 
go to 5).
5)Ac#on.
3)Matching in TCAM.
4)If matched, 
se!ng exact 
matching 
ﬁelds.
4’)If not matched, sending a 
packet-in message. Go to 1).
Figure 1.
The basic procedure of setting exact matching ﬁelds.
2)
When a data packet arrives, its header ﬁelds are
matched to the exact matching ﬁelds in BCAM or
SRAM. If the header ﬁelds are matched, the data
packet is processed based on the ﬂow entry (go to
5)).
3)
If they are not matched to any of the exact matching
ﬁelds in memory, then the header ﬁelds are matched
to the wildcard matching ﬁelds in TCAM.
4)
If the header ﬁelds are matched using TCAM, then
the corresponding exact matching ﬁelds (Figure 2)
are stored in BCAM.
4′) If they are not matched, then the data packet is
forwarded as a packet-in message to the OpenFlow
controller in order to query about the proper method
for processing the data packet.
5)
The data packet is processed using the action that is
speciﬁed in the matching ﬂow entry.
B. Limitations
Generally, there is a trade-off between the level of packet
forwarding performance and the complexity of the chipsets for
switches when TCAM is not used. Even if it is possible for
a switch to process only the exact matching ﬁelds, BCAM
is still necessary in order to obtain line-rate performance for
packet forwarding. Because there are fewer circuits in BCAM
than in TCAM [3], the prices for BCAM devices are lower
and the devices also consume less energy. However, BCAM
is still more costly than SRAM. Current techniques propose
methods for obtaining high performance levels with limited
BCAM space.
In DevoFlow [6], the exact matching ﬁelds are stored in
SRAM and the hash method is used to search the ﬂow entries
that need to be matched for a data packet. The chipset for
DevoFlow is relatively simple. However, the performance is
limited because the matching process largely depends on the
CPU.
Congdon et al. [7] utilized BCAM to match against the
exact matching ﬁelds. BCAM stores the small size data of
a partial header ﬁeld or a hash value of the exact matching
222
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

src MAC
00:11:22:33:44:55
dst MAC
*
src IP
192.168.12.1
dst IP
*
src TCP/UDP
*
dst TCP/UDP
80
The wildcard matching ﬁelds
The header ﬁelds of the arriving data packet
src MAC
00:11:22:33:44:55
dst MAC
00:66:77:88:99:AA
src IP
192.168.12.1
dst IP
192.168.12.100
src TCP/UDP
2020
dst TCP/UDP
80
The corresponding exact matching ﬁelds
matched
matched
matched
compliment
compliment
compliment
src MAC
00:11:22:33:44:55
dst MAC
00:66:77:88:99:AA
src IP
192.168.12.1
dst IP
192.168.12.100
src TCP/UDP
2020
dst TCP/UDP
80
Figure 2.
An example of the wildcard matching ﬁelds and the exact matching ﬁelds.
OpenFlow controller
Network infrastructure
Matching ﬁelds 
in OpenFlow
Matching ﬁeld 
translator
Matching ﬁelds 
in OpenFlow
End-host
Ma"ng ﬁeld used in the 
network infrastructure
Figure 3.
Limiting the matching ﬁelds in the network infrastructure.
ﬁelds. Along with the small data, BCAM stores the pointer
to the original matching ﬁelds stored in SRAM. When the
partial value or the hash value of the data packet header is
matched to the data in BCAM, the correctness of the matching
is conﬁrmed by referencing the original exact matching ﬁelds
in SRAM. This method requires a chipset that is capable of
performing a certain amount of complex logic.
III.
PROPOSAL OF OPENFLOW DEPLOYMENT USING THE
L2-BASED SWITCHES
We propose a technique that enables deployments of Open-
Flow networks using simple, low-cost switches. The main idea
of this technique is to limit the use of matching ﬁelds to
the single source MAC header inside the network (Figure 3).
Switches inside the network simply match on the source MAC
header ﬁelds of the data packets. Meanwhile, the technique
enables an OpenFlow controller and the end-hosts to use all
of the matching ﬁelds that are normally supported in OpenFlow
as the matching ﬁelds. As a result, this technique retains the
programmability of OpenFlow for an OpenFlow controller.
Because the single MAC header is the only matching ﬁeld
for single ﬂow entries in a switch, more ﬂow entries can be
stored in BCAM. Furthermore, the switch simply matches on
the MAC header. As a result, the chipset for the switch requires
only minor extension beyond what is required in an L2 switch.
A. Summary of the proposed technique
The technique maps the matching ﬁelds that the OpenFlow
controller and end-host manage to the corresponding MAC
addresses that are manged by switches inside the network.
Correspondence 
manager
OpenFlow controller
End-host
Matching ﬁeld translator
Flow entry 
translator
Matching ﬁelds
Flow entry
Packet header 
translator
Switches
Handling ﬂow entries 
originally set by the 
OpenFlow controller.
Se#ng edge 
switches to translate 
data packet headers.
Figure 4.
The architecture of the matching ﬁeld translator.
The matching ﬁelds for the OpenFlow protocol messages are
mapped for the OpenFlow controller. The header ﬁelds of data
packets are mapped for end-hosts. In the section below, we
refer to the translated MAC addresses that are managed by
switches inside the network handle as MAC ID address.
There is a manager for the correspondence between the
exact matching ﬁelds and the MAC ID addresses. The cor-
respondence is global throughout the network. Based on this
correspondence, the exact matching ﬁelds that are speciﬁed by
the OpenFlow controller are translated into the source MAC
ID addresses and the source MAC ID addresses are stored in
the BCAM of the switch. For the wildcard matching ﬁelds, the
exact matching ﬁelds are determined using the header ﬁelds
in actual data packets that arrive, and then, the exact matching
ﬁelds are translated into the MAC ID addresses.
At a network ingress switch, the source MAC header in
the data packet is replaced with the MAC ID address, which
corresponds to all of original header ﬁelds in the data packet.
At a network egress switch, the original header ﬁelds from the
data packet are recovered based on the correspondence with
the source MAC ID address in the data packets and the data
packet is transferred to the end-host.
B. Architecture
There is a proxy between the switches and the OpenFlow
controller that is referred to as a “matching ﬁeld translator”.
For the OpenFlow controller, the matching ﬁeld translator
behaves like an OpenFlow switch. For the switches, the
223
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

TABLE I.
AN SIMPLE EXAMPLE OF THE CORRESPONDENCE OF THE
ORIGINAL MATCHING FIELDS AND THE MAC ID ADDRESSES.
Original L2–L4 headers
MAC ID address
L2
L3
L4
00:11:BB:CC:DD:EE
192.168.1.1
80
00:00:00:00:00:01
00:11:BB:CC:DD:EE
192.168.1.1
22
00:00:00:00:00:02
00:01:BB:CC:DD:FF
192.168.1.100
80
00:00:00:00:00:03
matching ﬁeld translator behaves like an OpenFlow controller.
As shown in Figure 4, there are the correspondence manager,
the ﬂow entry translator, and the packet header translator in
the matching ﬁeld translator.
1) The correspondence manager:
The correspondence
manager handles the correspondences between the exact
matching ﬁelds and the MAC ID address. For the exact
matching ﬁelds, the correspondence manager allocates a 48-
bit ID in the form of a MAC address. Table I shows a simple
example of the correspondence between the matching ﬁelds
and the MAC ID addresses. The matching ﬁelds and the IDs
are in a one-to-one correspondence. There are two modules that
search the correspondence: the ﬂow entry translator and the
packet header translator. If the correspondence manager has not
allocated IDs to the matching ﬁelds yet, it allocates a new ID
and returns it as the MAC ID address. This ID allocation can
be implemented by allocating sequential numbers to the new
exact matching ﬁelds. Searching can be implemented using the
hash method in a constant time.
Note that the MAC ID address always serves as the basis
for the matching ﬁelds that are speciﬁed by the OpenFlow
controller or for the header ﬁelds in the data packets that are
sent and received by end-hosts. As a result, when the original
matching ﬁelds are searched using the MAC ID address, the
corresponding matching ﬁelds always exist.
2) The ﬂow entry translator: The ﬂow entry translator
modiﬁes and relays the OpenFlow protocol messages between
the OpenFlow controller and the switches. For a ﬂow entry in-
stallation for the exact matching ﬁelds, the ﬂow entry translator
simply replaces them with the corresponding MAC address and
forwards the message to the switch.
For a ﬂow entry installation for the wildcard matching
ﬁelds, the ﬂow entry translator determines the corresponding
exact matching ﬁelds when the new data packet arrives at the
switch. Then, the ﬂow entry translator obtains the correspond-
ing MAC ID address, and sends the ﬂow entry installation
message for the MAC ID address.
For a ﬂow statistics request, from the OpenFlow controller,
for the ﬂow entry of the wildcard matching ﬁelds, the ﬂow
entry translator collects the ﬂow statistics from the switch,
at ﬁrst. The objectives of the collection are the ﬂow entries
of the exact matching ﬁelds that correspond to the wildcard
matching ﬁelds. Then, the ﬂow entry translator responds with
the aggregated value as the statistics for the requested ﬂow
entry.
3) The packet header translator: The packet header trans-
lator modiﬁes the header ﬁelds of data packets that are
transferred through the edge ports (i.e., the ports that connect
directly to the end-hosts) of the edge switches. For a data
packet that is sent from an end-host, the packet header trans-
lator simply replaces the source MAC header with the MAC ID
Backbone 
network
ISP’s OpenFlow controller
Career access 
network
Matching ﬁeld translator
Edge switch
Switch inside 
the network
Figure 5.
Deployment in a carrier access network.
address corresponding to the original header ﬁelds in the data
packet. For a data packet that is sent from the switch to the end-
host, all of the header ﬁelds are set to the values in the exact
matching ﬁelds that correspond to the source MAC ID address
of the data packet. Note that header ﬁelds other than the source
MAC header are modiﬁed at an egress switch because the
OpenFlow controller may have modiﬁed other header ﬁelds.
C. Deployment Scenario
The proposed architecture can reduce the capital expendi-
ture (CAPEX) and operational expenditure (OPEX) for infras-
tructure in a carrier access network deployment scenario [2].
On the carrier access network, an ISP’s controller manages
ﬂows for clients in order to provide customized networks
settings for virtual private network (VPN), Internet Protocol
television (IPTV), content delivery network (CDN) services,
etc. As shown in Figure 5, an ISP’s controller manages
switches in central ofﬁces and data centers in the carrier access
network via the matching ﬁeld translator. Edge switches for
packet header translation exist in each homes. Although the
edge switches need to match all of the header ﬁelds, the cost
of edge switches is not high. Since the amount of trafﬁc is
relatively low in individual home, software-based switches are
sufﬁcient for meeting the demands that are replaced on edge
switches.
IV.
DETAILED IMPLEMENTATION
In this section, we describe the detailed implementation
of packet header translation at ingress and egress switches,
and the handling of ﬂows (i.e., translations for ﬂow entry
installations, obtaining ﬂow statistics, and managing packet-
out messages). These scenarios involve translations of the
matching ﬁelds into the MAC ID addresses.
A. Translation of data packet headers
Edge switches rewrite the headers of data packets that
are sent directly from and to end-hosts. The edge switches
include OpenFlow functionality, i.e., they can manage all of the
supported header ﬁelds. An edge switch has at two ports. One
is connected to an end-host in a home network. The other is
224
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

connected to the central ofﬁce, i.e., the carrier access network.
The packet header translator is conﬁgured in advance with
information that speciﬁes the port that connects to the end-
host or the central ofﬁce.
When an unknown data packet (i.e., no matching ﬂow entry
is set) arrives at the port in the edge switch that connects to an
end-host, the switch sends a packet-in message to the packet
header translator. Then, the packet header translator obtains
the corresponding MAC ID address for the header ﬁelds in
the data packet. Finally, the packet header translator sets the
ﬂow entry in the edge switch. The matching ﬁelds for the ﬂow
entry include the header ﬁelds of the data packet. The actions
that are performed include modifying the source MAC header
to the corresponding MAC ID address and sending out the
data packet from the other port. Subsequent data packets for
the same header ﬁelds are simply matched. Then, ﬂow entry
actions are applied and the data packets are transferred.
When an unknown data packet arrives at the port of an
edge switch that connects to the central ofﬁce, the switch
obtains the ﬂow entry from the packet header translator. The
ﬂow entry is used for recovering the original data packet
header and transferring the data packet from the other port.
The matching ﬁelds in the ﬂow entry include the source MAC
ID address, which corresponds to the original header ﬁelds.
The actions that are performed include the modiﬁcation of all
of the header ﬁelds to the original header ﬁelds. Note that
the OpenFlow controller may modify arbitrary header ﬁelds
in data packets in the network. Since only the source MAC
header is managed in the network, other header ﬁelds are
left unchanged. However, the source MAC ID address of the
arriving data packet corresponds to the header ﬁelds when the
data packet arrives at the switch. This is guaranteed by the
translation of the ﬂow entry installation that is described in
Section IV-B2.
B. Flow Entry Installation
In order to manage ﬂows, the ﬂow entry translator manages
the correspondence of the original ﬂow entries and the exact
matching ﬁelds for the ﬂow entries that have actually been
installed (Table II). The original ﬂow entries are for the wild-
card or exact matching ﬁelds that were set by the OpenFlow
controller. The installed ﬂow entries are for the exact matching
ﬁelds, which are the same as the original exact matching
ﬁelds or the exact matching ﬁelds that were speciﬁed by the
data packet that actually arrived. The ﬂow entry installation
process is described in the section below. In this process, the
correspondence database (i.e., Table II) is updated when a ﬂow
entry is installed into, or removed from the switch.
For the installation of a ﬂow entry whose matching ﬁelds
are the exact ones, the ﬂow entry translator simply translates
the ﬂow entry whose matching ﬁelds are the source MAC ID
address, and installs (“FI-iii” in Figure 6). For the installation
of a ﬂow entry whose matching ﬁelds are the wildcard values,
the ﬂow entry translator installs possible ﬂow entries in the
switch ﬁrst in order to preserve the consistency of the priorities
in the switch’s ﬂow table (“FI-i” in Figure 6). Then, the ﬂow
entry translator determines the corresponding exact matching
ﬁelds (“FI-ii” in Figure 6) and installs the ﬂow entry (“FI-iii”
in Figure 6) .
Are the 
matching ﬁelds 
wildcard?
Receiving a ﬂow_mod
message.
(FI-ii) Determining the exact 
matching ﬁelds by the every 
arriving unknown data packets.
(FI-iii) Replacing the matching 
ﬁelds with the source MAC 
address corresponding to the 
exact matching ﬁelds.
Sending the ﬂow_mod
message to the switch.
(FI-i) Se#ng the possible ﬂow 
entries to avoid the conﬂict of 
the priority in the ﬂow table.
Yes
No
Figure 6.
Summarized process of ﬂow entry installation.
1) Maintaining the priorities in the ﬂow table (FI-i): As
described in Section IV-B2, for a ﬂow entry for the wildcard
matching ﬁelds, the ﬂow entry translator does not immediately
install the ﬂow entry when it receives the ﬂow entry installation
message, i.e., a ﬂow mod message. Due to the time lag that is
associated with the installation of a ﬂow entry, an arriving data
packet can be matched inappropriately to another ﬂow entry
whose priority is lower than that of the original one.
For instance, in Table II, let us assume that a ﬂow en-
try for switch 0x1 (the priority is 65536 and the matching
ﬁelds are “00:11:22:33:44:55, 192.168.10.2, 80”) is not in-
stalled. Meanwhile, let us assume that another ﬂow entry for
switch 0x1 (the priority is 65534 and the matching ﬁelds
are “00:11:22:33:44:55, 192.168.10.2, 80”) is installed. Subse-
quently, a data packet that arrives with header ﬁeld values of
“00:11:22:33:44:55, 192.168.10.2, 80” should be matched to
the ﬂow entry with priority 65536. However, the data packet
matches to the one with priority 65534, and the switch fails to
send a packet-in message. This conﬂicts with the OpenFlow
controller because the OpenFlow controller assumes that the
data packet matches to the ﬂow entry with priority 65536.
In order to avoid this conﬂict, the ﬂow entry transla-
tor installs the possible ﬂow entries for the exact matching
ﬁelds corresponding to the ﬂow entry (denoted by “E”) for
the wildcard matching ﬁelds. First, the ﬂow entry manager
compares the wildcard matching ﬁelds for E and the exact
matching ﬁelds for the ﬂow entries that have already been
installed for the lower priorities in the ﬂow table. Then, for
the installed ﬂow entries that are found, whose exact matching
ﬁelds overlap with the matching ﬁelds of E, the ﬂow entry
translator immediately installs the ﬂow entries that correspond
to E. The exact matching ﬁelds of the installed ﬂow entries
are the same as the matching ﬁelds for the ﬂow entries that
were found. The priority of the ﬂow entries are the same as
the priorities for E. The actions for the installed ﬂow entries
are the same as the actions for E. The speciﬁed ﬂow entries
are translated and installed by the same process that is found
in “install-iii” in Figure 6.
2) Determining the matching ﬁelds and installing the ﬂow
entry (FI-ii): The ﬂow entry translator determines the corre-
sponding exact matching ﬁelds based on the data packets that
225
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

TABLE II.
AN EXAMPLE OF THE FLOW ENTRY CORRESPONDENCE DATABASE.
Switch ID
Original matching ﬁelds
Priority
Installed exact matching ﬁelds
0x1
00:11:22:33:44:55, ∗, 80
65536
00:11:22:33:44:55, 192.168.10.1, 80
65536
00:11:22:33:44:55, 192.168.10.2, 80
0x1
00:11:22:33:44:55, 192.168.10.2, 23
65535
00:11:22:33:44:55, 192.168.10.2, 23
0x1
00:11:22:33:44:55, 192.168.10.2, 80
65534
00:11:22:33:44:55, 192.168.10.2, 80
0x2
00:11:22:33:44:55, 192.168.10.1, ∗
65536
00:11:22:33:44:55, 192.168.10.1, 23
65536
00:11:22:33:44:55, 192.168.10.1, 80
0x2
00:11:22:33:44:55, 192.168.10.2, 23
65535
00:11:22:33:44:55, 192.168.10.2, 23
OpenFlow controller
Matching ﬁeld translator
1)Se"ng ﬂow 
entry
Inside switch
End-host
Edge switch
Used header 
ﬁelds
4)Matching of 
original ﬁelds
*
*
ac$on
3)recovery
5)transla$on
Correspondence 
manager
Flow entry translator
3)
5)
2)packet-in
(6)Installing the ﬂow entry
ac$on
Figure 7.
Installing a physical ﬂow entry based on an arriving data packet.
Acon 
Modify: 
L4=F
The matching 
ﬁelds
L2
A
L3
B
L4
C
Original ﬂow entry
Correspondence of the 
matching ﬁelds
L2
A
L3
B
L4
F
L2
Y
L2
A
L3
B
L4
C
L2
X
MAC ID
address
Original 
Modify: 
L2=Y
L2
X
Flow entry in the switch
Figure 8.
Translation of header modiﬁcation.
are arriving. When the ﬂow entry translator receives a packet-in
message from the switch (#2) in Figure 7), it ﬁrst recovers the
original L2–L4 headers from the source MAC ID address of
the data packet (#3) in Figure 7). Then, the ﬂow entry translator
compares the L2–L4 headers with the wildcard matching ﬁelds
from the entries in the ﬂow entry correspondence database (#4)
in Figure 7). If there is the ﬂow entry for the wildcard matching
ﬁelds to be matched to, a ﬂow entry translator generates a ﬂow
entry using the exact matching ﬁelds that correspond to the
source MAC ID address in the packet-in message. Then, the
ﬂow entry is translated and installed into the switch by the
same process that is described in “install-iii” (#5) and #6) in
Figure 7). If there is no ﬂow entry to be matched to, the header
ﬁelds in the data packet are recovered using the original ones
from the source MAC ID address and the packet-in message
is transferred to the OpenFlow controller.
3) Translation of a ﬂow entry (FI-iii): For an original ﬂow
entry whose matching ﬁelds are exact, the ﬂow entry translator
simply replaces the original matching ﬁelds with the source
MAC ID address corresponding to the original matching ﬁelds,
and installs the ﬂow entry.
When the ﬂow entry translator installs a ﬂow entry into
the switch, the action for modifying the header ﬁeld is also
replaced. When an OpenFlow controller modiﬁes the header
ﬁeld of a data packet, only the partial header ﬁeld that is
to be modiﬁed is speciﬁed in the message. The ﬂow entry
translator translates the action ﬁeld so that only the source
MAC ID address is replaced (Figure 8). This translation
enables switches to modify only the source MAC header while
the source MAC ID address of the data packet in the network
always represents the original header ﬁelds that should be at
the given position in the network at that time. The translated
source MAC ID address is the one that corresponds to the
combination of the values of all header ﬁelds resulting from the
modiﬁcation. The combination of the resulting header ﬁelds
can be determined by replacing the value of the partial header
in the matching ﬁelds with the value that is speciﬁed in the
action of the ﬂow entry.
C. Other processes of ﬂow handling
1) Obtaining ﬂow statistics: If the OpenFlow controller
sends a message to obtain the ﬂow statistics from the ﬂow
entry for the exact matching ﬁelds, the ﬂow entry translator
simply replaces the original matching ﬁelds to the source MAC
ID address. The MAC ID address corresponds to the original
exact matching ﬁelds.
If the request is for the ﬂow statistics for the wildcard
matching ﬁelds, the ﬂow entry translator collects the ﬂow
statistics from the switch, sums them up, and responds with the
sum for the ﬂow statistics from the original ﬂow entry. Since
the ﬂow statistics represent the number of data packets that
were matched to the ﬂow entry, the sum of the ﬂow statistics
for the corresponding ﬂow entries that were installed for the
exact matching ﬁelds is returned.
2) Packet-out: A packet-out message is for sending out a
data packet from the OpenFlow switch. In the message, the
data packet that is being sent is speciﬁed by the buffer ID
or by the data packet included in the message. If the data
packet is included in the message, the ﬂow entry translator
replaces the source MAC header with the MAC ID address
that corresponds to the original header ﬁelds in the data packet.
Then, the message is forwarded to the switch and the switch
sends out a data packet with MAC address that corresponds to
the original header ﬁelds.
V.
EVALUATION
The proposed technique enables simple, low-cost imple-
mentations of hardware-based switches for OpenFlow infras-
tructures. However, the matching ﬁeld translator adds over-
heads. The overhead is especially critical for the data trans-
mission performance. The overheads that are caused by the
matching ﬁeld translator are caused by the latencies for ﬂow
entry settings. In our proposal, the corresponding matching
ﬁelds are determined for each new data packet.
226
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

In order to evaluate the latency from the ﬂow entry installa-
tions, we measured the lost data size due to installations by the
ﬂow entry translator. For comparison purposes, we measured
the lost data size on a network that was controlled directly by
an OpenFlow controller. The parameter that affects the delay
of the ﬂow entry installation process is the number of original
ﬂow entries for the wildcard matching ﬁelds that are set by the
OpenFlow controller. The number of original ﬂow entries for
the wildcard matching ﬁelds directly affects the delay because
the matching to the wildcard matching ﬁelds is implemented
by software, i.e., linear searching.
A. Settings
We assumed a simple network whose data plane was
composed of two end-hosts, two edge switches, and a switch.
On the control plane, there was an OpenFlow controller and
a matching ﬁeld translator. There was no matching ﬁeld
translator and the software edge switches functioned as the
switching hubs when measurement were being performed in
the network without the proposed technique. The OpenFlow
controller and the matching ﬁeld translator ran on systems
with an Intel Xeon E5520 processor and 6GB RAM. The edge
switches used systems with Intel Celeron Dual-Core T3330
processors with 2GB RAMs that ran Open vSwitch [8]. We
used the USB Network Interface Cards (NICs) as the ports of
the edge switches. The end-hosts ran on systems with Intel
Celeron Dual-Core T3330 processors and 2GB RAMs. In the
experiments, the operating system on the all hosts was Ubuntu
12.04 LTS.
The switch inside the network was NEC PF 5240 switch.
Based on the switch speciﬁcation that we conﬁrmed, the
switch supported a maximum buffer size of 544 packets, i.e.,
the maximum number of new data packets with the same
header ﬁelds that can be buffered in the switch is 544. Note
that our proposed technique does not require TCAM in a
switch, but does require the BCAM space for the source
MAC headers. Unfortunately, we did not have a hardware-
based switch that incorporates the required and necessary
functionality for the proposed technique. As a result, we used
the hardware-based OpenFlow switch that includes TCAM
when we ran the matching ﬁeld translator. The exact matching
ﬁelds of the source MAC ID addresses for the ﬂow entries
installed by the matching ﬁeld translator were processed using
TCAM. However, since the matching performance of TCAM
and BCAM is same for the exact matching ﬁelds, the switch
used in this experiment did not affect the results in terms of
the packet forwarding performance of a switch in the network.
The OpenFlow controller set the number ﬂow entries for
the wildcard matching ﬁelds when it connected to the switch.
When we used the matching ﬁeld translator, the ﬂow entries
were stored in the matching ﬁeld translator at ﬁrst. However,
if we did not use the matching ﬁeld translator, the ﬂow entries
were installed immediately into the switch.
We used iperf to send data packets between the end-hosts.
The UDP packets were sent at a rate of 75.40 Mbps because
that is the maximum throughput for environment that was used
in the experiment. The maximum throughput was low due to
the USB NICs. However, it was sufﬁcient because we assumed
that the edge switches were home router-class switches. In the
experiments, the size of a UDP datagram was 1.47 KB.
0.00 
5.77 
6.11 
5.95 
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
Without the
matching
ﬁeld
translator
100 wildcard
entries
1000
wildcard
entries
5000
wildcard
entries
Size of Lost Data (MB)
Figure 9.
Lost data size vs. the wildcard entries in the ﬂow entry translator.
B. Results
We measured the total lost data size for the ﬁrst 1 seconds
of sending UDP packets. Note that, regardless of the number
of the wildcard ﬂow entries, packet loss ended within 1 second
in our experiment. Figure 9 shows the sizes of the lost data
based on the numbers of wildcard entries in the ﬂow entry
translator and also for the cases where the OpenFlow controller
was connected directly to the switch. The average for 10
experiments is shown.
As expected, a certain amount of data was lost when the
ﬂow entry translator was used. In this experiment, the size
of the lost data was in the range of 5.77–6.11 MB. At most,
the data for 1.47KB ∗ 544packets = 799.68KB was buffered
in the switch. Note that it was not possible to measure the
precise size of the buffered data because the buffer was in the
shared memory in the switch. The sum of the lost data and
the rough estimate for the size of the buffered data is less
than 7 MB. This is comparable with the buffer sizes in current
top-of-rack (ToR) switches, e.g., the HP 5900 has 9MB of
buffer space [9]. In general, the necessary buffer size depends
on the throughput and the rate at which data packets with new
header ﬁelds are arriving. Larger buffer sizes may be necessary.
However, since buffers can be implemented using SRAM, we
believe that the technique that we have proposed is capable of
enabling low cost switches for the OpenFlow infrastructure.
The cost is lower than the cost of using TCAM and complex
chipsets.
VI.
RELATED WORK
Other than the approach that is described in Section II for
reducing TCAM power, there are also approaches that reduce
the cost of OpenFlow switches.
A. Software switch implementation
Software-based OpenFlow switch implementations uses
general purpose processors (GPP) and DRAM or SRAM
devices. The software implementation reduces the CAPEX and
the OPEX of the infrastructure signiﬁcantly. Open vSwitch [8]
is the most widely used software for OpenFlow switch func-
tionality on Linux systems. Dedicated hardware is not required
when Open vSwitch is being used. It is designed for edge
switches for virtual machines in data centers. Matsumoto et
227
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

al. [10] proposed a hash searching method for high speed
matching to ﬂow entries in RAM. For the exact matching ﬁelds
for a ﬂow entry, the hash value is calculated immediately when
it is installed. For the wildcard matching ﬁelds for a ﬂow entry,
matching for the packet that arrives ﬁrst is performed using
one-by-one comparisons to ﬂow entries in RAM. Then, the
hash value of the header in the ﬁrst data packet is calculated
and the value is used for matching the subsequent data packets.
A software switch is not suitable for forwarding the massive
volumes of data packets in carrier access networks.
B. Product OpenFlow switches
Hybrid OpenFlow switches from NEC, HP, IBM, and
Juniper use combination of general purpose and dedicated
hardware devices. There are various patterns of combinations.
Examples include combinations of application-speciﬁc inte-
grated circuits (ASICs) and SRAM, combinations of TCAM
for a limited number ﬂow entries and SRAM for other ﬂow
entries, etc. The hybrid implementations can obtain high
performance levels and reduce costs slightly. We propose a
technique that would dramatically reduce the complexity of
an OpenFlow switch implementation and also achieve line-rate
packet forwarding performance levels.
C. IP source routing-like approach
Source ﬂow [11] proposes an IP source routing-like ap-
proach. This technique embeds the action lists for data packets
from intermediates switches in the network into the header
ﬁelds at the ingress switches. Switches inside the network base
their actions on the pointers in the header ﬁelds. As a result,
the number of entries in the switch is the same as the number
of actions in the switch. Because the number of ﬂow entries
is typically lower than of the number of actions that have
been performed in the switch, the technique can reduce the
number of entries in switches. The action list for the switch
is retrieved from a centralized controller like an OpenFlow
controller. The constraint on this approach is that all of the
actions have to be supplied for the data packet at the ingress
switch at once. Our proposal, on the other hand, virtualizes an
OpenFlow network completely, i.e., an OpenFlow controller
can see ﬂows by obtaining the ﬂow statistics at every hop in
the network. As a result, the proposed functionality offers a
higher level of control of the network than the source ﬂow
technique.
D. Label switching
The label switching techniques enable all switches in the
network to match using only a single header that is referred
to as the label. At the ingress switch of the network, the
label for the data packet is added to the header ﬁelds of the
data packet or the header ﬁelds are replaced using the value
from the label. Inside the network, switches do not need to
have TCAM. Furthermore, the length of the matching ﬁeld is
short. As a result, less BCAM space is sufﬁcient. MPLS [12]
is widely used in carrier core networks. The MPLS label
is added at the ingress switch based on the destination IP
address. In the network, switches forward data packets based
simply on the label. However, MPLS does not have the same
level of programmability as OpenFlow because it has two
constraints: inﬂexible ﬂow deﬁnitions (i.e., the label is based
on the destination IP address) and a lack of a global view of
the network.
PortLand [13] is designed to control routing for data center
communications using the MAC header as the label in order to
support the communications for many hosts at different sites
using a large space for a MAC addresses. For every data packet
that is destined for another site in the data center, the MAC
header is replaced at the gateway. The new MAC address
corresponds to the original MAC address and the ID of the
destination site. This correspondence is managed globally. The
MAC address is recovered at the gateway of the destination
site. The difference between PortLand and our proposal is that
PortLand is not designed to virtualize an OpenFlow network,
but is designed for construction of a large-scale L2 network
without the ARP broadcast overhead. On the other hand, our
proposal supports arbitrary ﬂow deﬁnitions in the network.
VII.
CONCLUSIONS AND FUTURE WORK
This paper proposed a technique that enables construction
of OpenFlow networks using switches that have little more
than L2 switch functionality. Arbitrary ﬂow deﬁnitions from
an OpenFlow controller are translated into ﬂow deﬁnitions that
are based on the MAC ID address at the external matching
ﬁeld translator and the ﬂow entries are installed into the
switch. For the wildcard matching ﬁelds in the ﬂow entry,
the corresponding exact matching ﬁelds are determined and
the ﬂow entry is installed into the switch after the ﬁrst
arrival of the matching data packet in order to avoid using
TCAM. For translations in the data plane, the matching ﬁeld
translator manages the edge switches in order to modify the
MAC headers of data packets. In our proposal, the OpenFlow
interface is required in the switches inside the network in order
to manage the ﬂow table externally and enable simply matches
on the MAC header. We believe that this type of switch can
be assembled as an extension of an L2 switch.
A future work will focus on a distributed implementation
for the matching ﬁeld translator for scalability in carrier access
networks. In the architecture of the proposed technique, the
single matching ﬁeld translator manages all of the switches
in the network. The functions of the packet header and the
matching ﬁeld translators involve independent processes for
individual switches in the network. As a result, they can be
implemented easily in a distributed manner. Furthermore, the
global correspondence of the original header ﬁelds and the
MAC ID address can be managed easily by a distributed
lookup system, e.g., a distributed hash table. However, we need
to explore fast sharing schemes for the correspondence data
because the distributed version of the matching ﬁeld translator
will be deployed at geographically remote sites. The time that
is required in order to reference the correspondence is critical
for the performance of the packet header and the matching
ﬁeld translators.
REFERENCES
[1]
N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peter-
son, J. Rexford, et al., “OpenFlow: Enabling innovation in campus
networks,” in Proceedings of the ACM SIGCOMM 2008 Conference,
vol. 38, no. 2, 2008, pp. 69–74.
[2]
“SPARC deliverable d2.1: Initial deﬁnition of use cases and carrier
requirements,” 2010, [retrieved: Dec. 2013]. [Online]. Available:
http://www.fp7-sparc.eu/home/deliverables/
228
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

[3]
K. Pagiamtzis and A. Sheikholeslami, “Content-addressable memory
(CAM) circuits and architectures: A tutorial and survey,” IEEE Journal
of Solid-State Circuits, vol. 41, no. 3, 2006, pp. 712–727.
[4]
J. Liao, “SDN system performance,” 2012, [retrieved: Dec. 2013].
[Online]. Available: http://pica8.org/blogs/?p=201
[5]
“OpenFlow
switch
speciﬁcation
version
1.3.0
(wired
pro-
tocol
0x04),”
2012,
[retrieved:
Dec.
2013].
[Online].
Available:
https://www.opennetworking.org/images/stories/downloads/
sdn-resources/onf-speciﬁcations/openﬂow/openﬂow-spec-v1.3.0.pdf
[6]
A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma,
and S. Banerjee, “DevoFlow: Scaling ﬂow management for high-
performance networks,” in Proceedings of the ACM SIGCOMM 2011
Conference, 2011, pp. 254–265.
[7]
P. Congdon, P. Mohapatra, M. Farrens, and V. Akella, “Simultane-
ously reducing latency and power consumption in openﬂow switches,”
IEEE/ACM Transactions on Networking, 2013, to appear.
[8]
“Open vSwitch,” [retrieved: Dec. 2013]. [Online]. Available: http:
//openvswitch.org/
[9]
“QuickSpecs
HP
5900
Switch
Series,”
[retrieved:
Dec.
2013].
[Online]. Available: http://h18004.www1.hp.com/products/quickspecs/
14252 div/14252 div.pdf
[10]
N. Matsumoto and M. Hayashi, “Performance improvements of ﬂow
switching with automatic maintainance of hash table assisted by wild-
card ﬂow entries,” in Proceedings of the 10th International Conference
on Optical Internet (COIN 2012), 2012.
[11]
Y. Chiba, Y. Shinohara, and H. Shimonishi, “Source ﬂow: handling
millions of ﬂows on ﬂow-based nodes,” in Proceedings of the ACM
SIGCOMM 2010 conference, 2010, pp. 465–466.
[12]
“RFC
2031:
Multiprotocol
label
switching
architecture,”
2001,
[retrieved: Dec. 2013]. [Online]. Available: http://tools.ietf.org/html/
rfc3031
[13]
R. Niranjan Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri,
S. Radhakrishnan, et al., “PortLand: A scalable fault-tolerant layer 2
data center network fabric,” in Proceedings of ACM SIGCOMM 2009
Conference, 2009, pp. 39–50.
229
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

