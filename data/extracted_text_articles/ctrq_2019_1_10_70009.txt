Data Loss in RAID-5 Storage Systems
with Latent Errors
Ilias Iliadis
IBM Research – Zurich
8803 R¨uschlikon, Switzerland
Email: ili@zurich.ibm.com
Abstract—Storage systems employ redundancy and recovering
schemes to protect against device failures and latent sector errors,
and to enhance reliability. The effectiveness of these schemes has
been evaluated based on the Mean Time to Data Loss (MTTDL)
and the Expected Annual Fraction of Data Loss (EAFDL) metrics.
The reliability degradation due to device failures has been
assessed in terms of both these metrics, but the adverse effect
of latent errors has been assessed only in terms of the MTTDL
metric. This article addresses the issue of evaluating the amount
of data losses caused by latent errors. It presents a methodology
for obtaining MTTDL and EAFDL of RAID-5 systems analytically
in the presence of unrecoverable or latent errors. A theoretical
model capturing the effect of independent latent errors and device
failures is developed, and closed-form expressions are derived for
the metrics of interest.
Keywords–Storage; Unrecoverable or latent sector errors; Reli-
ability analysis; MTTDL; EAFDL; RAID; MDS codes; stochastic
modeling.
I.
INTRODUCTION
Today’s large-scale data storage systems use data re-
dundancy schemes to recover data lost due to device and
component failures, and to enhance reliability [1]. Erasure
coding schemes are deployed that provide high data reliability
as well as high storage efﬁciency. Special cases of erasure
codes are the replication schemes and the Redundant Arrays
of Inexpensive Disks (RAID) schemes, such as RAID-5 and
RAID-6, that have been deployed extensively in the past thirty
years [2-5]. The effectiveness of these schemes has been
evaluated based on the Mean Time to Data Loss (MTTDL) [2-
11] and, more recently, the Expected Annual Fraction of Data
Loss (EAFDL) reliability metrics [1][12][13]. The introduction
of the latter metric was motivated by the fact that Amazon
S3 considers the durability of data over a given year [14],
and, similarly, Facebook [15], LinkedIn [16] and Yahoo! [17]
consider the amount of data lost in given periods.
The reliability of storage systems is also degraded by the
occurrence of unrecoverable or latent sector errors, that is,
of errors that cannot be corrected by the standard sector-
associated error-correcting code (ECC) nor by the re-read
mechanism of hard-disk drives (HDDs). The effect of latent er-
rors is quite pronounced in higher-capacity HDDs and storage
nodes because of the high frequency of these errors [18-22].
The risk of irrecoverable loss of data rises in the presence of
latent errors.
Analytical reliability expressions for MTTDL that take
into account the effect of latent errors have been obtained
predominately using Markovian models, which assume that
component failure and rebuild times are independent and
exponentially distributed [8][20][21][23]. The effect of latent
errors on MTTDL of erasure coded storage systems for the
practical case of non-exponential failure and rebuild time
distributions was assessed in [22].
In this article, we consider the effect of latent errors not
only MTTDL, but also on the amount of data lost for the
case of non-exponential failure and rebuild time distributions.
Clearly, when a data loss occurs, the amount of data lost due to
a device failure is much larger than the amount of sectors lost
due to latent errors. We present a non-Markovian methodology
for deriving the MTTDL and EAFDL metrics analytically for
the case of RAID-5 systems. We extend the methodology
developed in prior work [12][13] to assess MTTDL and
EAFDL of storage systems in the absence of latent errors.
The validity of this methodology for accurately assessing the
reliability of storage systems was conﬁrmed by simulations
in several contexts [4][9][12][24]. It was demonstrated that
theoretical predictions for the reliability of systems comprised
of highly reliable storage devices are in good agreement with
simulation results. Consequently, the emphasis of the present
work is on theoretically assessing the effect of latent errors on
system reliability. This is the ﬁrst work to study the effect of
latent errors on EAFDL.
The remainder of the article is organized as follows.
Section II describes the storage system model and the cor-
responding parameters considered. Section III considers the
unrecoverable or latent errors and the frequency of their
occurrence. Section IV presents the general framework and
methodology for deriving the MTTDL and EAFDL metrics
analytically for the case of RAID-5 systems and in the presence
of latent errors. Closed-form expressions for relevant reliability
metrics, such as the probability of data loss and the amount of
data loss, are derived. Section V presents numerical results
demonstrating the effectiveness of the RAID-5 scheme for
improving system reliability and the adverse effect of unre-
coverable or latent errors on the probability of data loss and
on the MTTDL and EAFDL reliability metrics. Section VI
provides a discussion concerning the results obtained. Finally,
we conclude in Section VII.
II.
STORAGE SYSTEM MODEL
The storage system considered here comprises n storage
devices (nodes or disks), with each device storing an amount
c of data, such that the total storage capacity of the system is
n c. User data is divided into blocks (or symbols) of a ﬁxed
size s (e.g., sector size of 512 bytes) and complemented with
parity symbols to form codewords.
1
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-699-6
CTRQ 2019 : The Twelfth International Conference on Communication Theory, Reliability, and Quality of Service

TABLE I.
NOTATION OF SYSTEM PARAMETERS
Parameter
Deﬁnition
n
number of storage devices
c
amount of data stored on each device
l
number of user-data symbols per codeword (l ≥ 1)
m
total number of symbols per codeword (m > l)
(m, l)
MDS-code structure
s
symbol size
N
number of devices in a RAID-5 array (N = m)
b
average reserved rebuild bandwidth per device
R
time required to read (or write) an amount c of data at an average
rate b from (or to) a device
FR(.)
cumulative distribution function of R
Fλ(.)
cumulative distribution function of device lifetimes
se(RAID-5)
storage efﬁciency of redundancy scheme (se(RAID-5) = l/m)
U
amount of user data stored in the system (U = se(RAID-5) n c)
C
number of codewords stored in a RAID-5 array (C = c/s)
µ−1
mean time to read (or write) an amount c of data at an average rate
b from (or to) a device (µ−1 = E(R) = c/b)
λ−1
mean time to failure of a storage device
(λ−1 =
R ∞
0 [1 − Fλ(t)]dt)
A. Redundancy
We consider an (m, l) = (N, N − 1) maximum distance
separable (MDS) erasure code, which is a mapping from N −1
user-data symbols to a set of N symbols, called a codeword,
having the property that any subset containing N −1 of the N
symbols of the codeword can be used to decode (reconstruct,
recover) the codeword. A single parity symbol is computed by
using the XOR operation on l = N − 1 user-data symbols to
form a codeword with m = N symbols in total. Such a scheme
can tolerate a single erasure anywhere in the codeword. The
N symbols of each codeword are stored on N distinct devices.
More speciﬁcally, this scheme is used by the popular RAID-
5 system, in which the n devices are arranged in groups (or
arrays), each with N devices, one of which is redundant [2][3].
The storage system therefore comprises n/N RAID-5 arrays
with each array having the ability to tolerate one device failure.
The storage efﬁciency se(RAID-5) of the system is given by
se(RAID-5) =
l
m = N − 1
N
.
(1)
Consequently, the amount of user data U stored in the system
is given by
U = se(RAID-5) n c = l n c
m
.
(2)
Also, the number C of codewords in a device is given by
C = c
s .
(3)
Our notation is summarized in Table I. The parameters are
divided according to whether they are independent or derived,
and are listed in the upper and lower part of the table,
respectively.
B. Codeword Reconstruction
When a storage device of an array fails, the C codewords
stored in the array lose one of their symbols. Subsequently, the
system starts to reconstruct the lost codeword symbols using
the surviving symbols of the affected codewords. We assume
that device failures are detected instantaneously, which imme-
diately triggers the rebuild process. A certain proportion of
the device bandwidth is reserved for data recovery during the
rebuild process, where b denotes the actual average reserved
rebuild bandwidth per device. This bandwidth is usually only
Figure 1.
Rebuild for a RAID-5 array with N = m = 8 and l = 7.
a fraction of the total bandwidth available at each device, the
remaining bandwidth being used to serve user requests.
The rebuild process attempts to restore the codewords of
the affected array sequentially. The lost symbols are recon-
structed directly in a spare device as shown in Figure 1.
Decoding and re-encoding of data are assumed to be done
on the ﬂy, so the reconstruction time is equal to the time
taken to read and write the required data to the spare device.
Consequently, the time required to recover the amount c of
data lost is equal to the time R required to read (or write)
an amount c of data from (or to) a device. In particular, 1/µ
denotes the average time required to read (or write) an amount
c of data from (or to) a device, which is given by
1
µ ≜ E(R) = c
b .
(4)
C. Failure and Rebuild Time Distributions
We adopt the model and notation considered in [13]. The
lifetimes of the n devices are assumed to be independent and
identically distributed, with a cumulative distribution function
Fλ(.) and a mean of 1/λ. We consider real-world distributions,
such as Weibull and gamma, as well as exponential distribu-
tions that belong to the large class deﬁned in [24]. The storage
devices are characterized to be highly reliable in that the ratio
of the mean time 1/µ to read all contents of a device (which
typically is on the order of tens of hours), to the mean time
to failure of a device 1/λ (which is typically on the order of
thousands of hours) is very small, that is,
λ
µ = λ c
b
≪ 1 .
(5)
We consider storage devices whose cumulative distribution
function Fλ satisﬁes the condition
µ
Z ∞
0
Fλ(t)[1 − FR(t)]dt ≪ 1,
with λ
µ ≪ 1 ,
(6)
where FR(.) is the cumulative distribution function of the
rebuild time R. Then the MTTDL and EAFDL reliability
metrics tend to be insensitive to the device failure distribution,
that is, they depend only on its mean 1/λ, but not on its density
Fλ(.) [13].
III.
DATA LOSS FROM UNRECOVERABLE ERRORS
The reliability of RAID-5 systems is affected by the
occurrence of unrecoverable or latent errors. Let Pbit de-
note the unrecoverable bit-error probability. According to the
speciﬁcations, Pbit is equal to 1×10−15 for SCSI drives and
2
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-699-6
CTRQ 2019 : The Twelfth International Conference on Communication Theory, Reliability, and Quality of Service

1×10−14 for SATA drives [8]. Assuming that bit errors occur
independently over successive bits, the unrecoverable sector
(symbol) error probability Ps is given by
Ps = 1 − (1 − Pbit)s ,
(7)
with s expressed in bits. Assuming a sector size of 512 bytes,
the equivalent unrecoverable sector error probability is Ps ≈
Pbit × 4096, which is 4.096×10−12 in the case of SCSI and
4.096×10−11 in the case of SATA drives. However, empirical
ﬁeld results suggest that the actual values can be orders of
magnitude higher reaching Ps ≈ 5×10−9 [25].
IV.
DERIVATION OF MTTDL AND EAFDL
The MTTDL metric assesses the expected amount of time
until some data can no longer be recovered and therefore is
irrecoverably lost whereas the EAFDL assesses the fraction
of stored data that is expected to be irrecoverably lost by the
system annually. We brieﬂy review the general methodology
for deriving the MTTDL and EAFDL metrics presented in
[12]. This methodology does not involve Markovian analysis
and holds for general failure time distributions, which can
be exponential or non-exponential, such as the Weibull and
gamma distributions that satisfy condition (6).
At any point in time, the system can be thought to be in one
of two modes: normal mode or rebuild mode. During normal
mode, all devices are operational and all data in the system has
the original amount of redundancy. Any symbols encountered
with unrecoverable or latent errors are corrected through the
RAID-5 capability. However, multiple unrecoverable errors
encountered in a codeword can no longer be recovered and
therefore lead to data loss. A transition from normal mode
to rebuild mode occurs when a device fails; we refer to
the device failure that causes this transition as a ﬁrst-device
failure. During rebuild mode, an active rebuild process attempts
to restore the lost data in a spare device, which eventually
leads the system either to an irrecoverable data loss (DL)
with probability PDL or back to the original normal mode
by restoring initial redundancy, which occurs with probability
1 − PDL.
Let T be a typical interval of a fully operational period, that
is, the time interval from the time t that the system is brought to
its original state until a subsequent ﬁrst-device failure occurs.
For a system comprising n devices with a mean time to failure
of a device equal to 1/λ, the expected duration of T is given
by [12]
E(T) = 1/(n λ) ,
(8)
and MTTDL by
MTTDL ≈ E(T)
PDL
=
1
n λ PDL
.
(9)
The EAFDL is obtained as the ratio of the expected amount
of user data lost, normalized to the amount of user data, to the
expected duration of T [12, Equation (9)]:
EAFDL ≈
E(Q)
E(T) · U
(8)
= n λ E(Q)
U
(2)
= m λ E(Q)
l c
,
(10)
with E(T) and 1/λ expressed in years.
The expected amount E(H) of data lost, given that a data
loss has occurred, is given by [12, Equation (8)]:
E(H) = E(Q)
PDL
.
(11)
From (9) and (10), it follows that the derivation of the
MTTDL and EAFDL metrics requires the evaluation of PDL
and E(Q), respectively. These quantities are derived using the
direct path approximation [4][24][26], which, under conditions
(5) and (6), accurately assesses the reliability metrics of
interest [11][12][24][27].
A. Rebuild Process
When a storage device of an array fails, the C codewords
stored in the array lose one of their symbols. Using the direct-
path-approximation methodology, we proceed by considering
only the subsequent potential data losses and device failures
related to the affected array.
1) Unrecoverable Failure: The rebuild process attempts to
restore the C codewords of the affected array sequentially.
Let us consider such a codeword and let L be the number of
symbols irrecoverably lost and I be the number of symbols
encountered with unrecoverable errors in the codeword. As Ps
is the probability that a symbol has a latent (unrecoverable)
error, 1 − Ps is the probability that a symbol can be read
successfully and, owing to the independence of symbol errors,
it therefore holds that
P(I = i) =
m − 1
i

P i
s(1−Ps)m−1−i, for i = 0, . . . , m−1 ,
(12)
such that
E(I) =
m−1
X
i=1
i P(I = i) = (m − 1) Ps .
(13)
Clearly, the symbol lost due to the device failure can be
corrected through the RAID-5 capability only if the remaining
m−1 symbols can be read. Thus, L = 0 if and only if I = 0.
Using (12), the probability q that a codeword can be restored
is given by
q = P(L = 0) = P(I = 0) = (1 − Ps)m−1 .
(14)
Note that if a codeword cannot be restored, then at least one
of its l user-data symbols is lost. We now deduce that the
probability PUF of encountering an unrecoverable failure (UF)
during the rebuild process of the C codewords is given by
PUF = 1 − qC
(14)
=
1 − (1 − Ps)(m−1) C .
(15)
Furthermore, such an unrecoverable failure entails the loss of
user data. Let us denote by NUF the number of codewords that
cannot be recovered owing to unrecoverable failures. Then it
holds that
E(NUF) = C (1 − q) .
(16)
Remark 1: For very small values of Ps, it holds that (1 −
Ps)(m−1) C ≈ 1−(m−1) C Ps. Consequently, it follows from
(15) that
PUF ≈
(
(m − 1) C Ps ,
for Ps ≪ P (2)
s
1 ,
for Ps ≫ P (2)
s
.
(17)
3
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-699-6
CTRQ 2019 : The Twelfth International Conference on Communication Theory, Reliability, and Quality of Service

where P (2)
s
is obtained from the approximation (17) as follows:
PUF ≈ (m − 1) CP (2)
s
= 1
⇒
P (2)
s
≜ 1
C ·
1
m − 1 .
(18)
Note also that from (14) and (16), it follows that
E(NUF) ≈ C (m − 1) Ps ,
for Ps ≪
1
m − 1 .
(19)
In particular, for Ps = P (2)
s
, it holds that E(NUF) ≈ 1 and
this, combined with the fact that PUF ≈ 1, implies that almost
surely one of the C codewords cannot be recovered owing to
an unrecoverable failure.
If I > 0, the number L of symbols lost is equal to I + 1.
Consequently, the expected number E(L) of symbols lost is
given by
E(L) =
m−1
X
i=1
(i + 1) P(I = i) = E(I) + 1 − P(I = 0) , (20)
and using (12), (13), and (14) yields
E(L) = 1 − q + (m − 1)Ps = 1 − (1 − Ps)m−1 + (m − 1)Ps .
(21)
Remark 2: For small values of Ps, it holds that q = (1 −
Ps)m−1 ≈ 1 − (m − 1) Ps. Consequently, it follows from (21)
that
E(L) ≈ 2 (m − 1) Ps ,
for Ps ≪
1
m − 1 .
(22)
In particular, the expected number E(L|L > 0) of symbols
lost, given that the codeword cannot be restored, is given by
E(L|L > 0) =
E(L)
P(L > 0) =
E(L)
1 − P(L = 0)
(14)
=
E(L)
1 − q
(22)
≈
2 ,
for Ps ≪
1
m − 1 .
(23)
2) Device Failure: A subsequent device failure (DF) may
occur during the rebuild process triggered by the initial device
failure. The probability PDF|R that one of the m−1 remaining
devices in the array fails during the rebuild process depends
on the duration of the corresponding rebuild time R and the
aggregate failure rate of these m − 1 highly reliable devices,
and is given by [24]
PDF|R ≈ (m − 1) λ R .
(24)
In particular, it was shown in [28, Lemma 2] that, for highly
reliable devices satisfying conditions (5) and (6), the fraction
of the rebuild time R still remaining when another device fails
is approximately uniformly distributed between 0 and 1. This
implies that the probability PDF(j|R) that a device failure
occurs while reconstructing the jth (1 ≤ j ≤ C) codeword
during the rebuild process, and given a rebuild time of R, is
equal to PDF|R/C, which, using (24), yields
PDF(j|R) ≈ (m − 1) λ R
C
,
for j = 1, 2, . . . , C .
(25)
The probability PDF of a device failure during the rebuild
process is obtained by unconditioning (24) on R, that is,
PDF = E(PDF|R) ≈ (m − 1) λ E(R)
(4)
= (m − 1) λ
µ .
(26)
Similarly, the probability PDF(j) of a subsequent device
failure during the reconstruction of the jth (1 ≤ j ≤ C)
codeword is obtained by unconditioning (25) on R, that is,
PDF(j) = E(PDF(j|R)) ≈ (m − 1) λ E(R)
C
(4)
= (m − 1)
C
λ
µ .
(27)
B. Data Loss
Data loss may occur because of another device failure
or an unrecoverable failure of one or more codewords, or a
combination thereof. Note that in all cases, data loss cannot
involve only parity data, but also loss of user data. Let PDL
denote the probability of data loss. Then, the probability
1 − PDL of the rebuild being completed successfully is equal
to the product of 1−PDF, the probability of not encountering a
device failure during a rebuild, and 1−PUF, the probability of
not encountering an unrecoverable failure during the rebuild,
namely, 1 − PDL = (1 − PDF)(1 − PUF). Consequently,
PDL = PDF + (1 − PDF) PUF .
(28)
Substituting (15) and (26) into (28) yields
PDL ≈ (m−1) λ
µ +

1 − (m − 1) λ
µ
 h
1 − (1 − Ps)(m−1) Ci
.
(29)
Remark 3: It follows from (17) and (26) that the region
[0, P (1)
s
] of Ps in which the probability PUF is much smaller
than the probability PDF of encountering a device failure during
the rebuild process is obtained by
PUF ≪ PDF
⇔
(m − 1) C Ps ≪ (m − 1) λ
µ
⇔
Ps ≪ P (1)
s
≜ 1
C · λ
µ .
(30)
C. Amount of Data Loss
Depending on whether a subsequent device failure occurs
during the rebuild process, two cases are considered:
1) No Device Failure during Rebuild: The probability of
this event is equal to 1−PDF. The expected number of symbols
lost due to unrecoverable errors, E(S⊙
U | no DF), is given by
E(S⊙
U | no DF) = C E(L) = C [1 − q + (m − 1)Ps ] . (31)
Unconditioning on the event of not having a device failure
during the rebuild process, and using (14) and (26), we get
E(S⊙
U )=E(S⊙
U | no DF)P(no DF)=E(S⊙
U | no DF)(1 − PDF)
≈ C [ 1 − (1 − Ps)m−1 + (m − 1)Ps ]

1 − (m − 1) λ
µ

.
(32)
2) Device Failure during Rebuild: Suppose a subsequent
device failure occurs while reconstructing the jth (1 ≤ j ≤ C)
codeword. The probability of this event, denoted by PDF(j), is
given by (27). In this case, the two symbols of this codeword
that are stored on the two failed devices can no longer be
recovered and are lost. Furthermore, each of the remaining
m − 2 symbols may be lost owing to unrecoverable errors
with probability Ps. The same applies for the remaining C −j
codewords. Thus, the total number SD(j) of symbols that are
stored on the two failed devices and are lost is given by
4
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-699-6
CTRQ 2019 : The Twelfth International Conference on Communication Theory, Reliability, and Quality of Service

SD(j) = 2 (C + 1 − j) .
(33)
Also, the expected total number E(S+
U | DF at j) of symbols
stored in these C − j + 1 codewords that are lost owing to
unrecoverable errors is given by
E(S+
U | DF at j) = (C + 1 − j) (m − 2) Ps .
(34)
Furthermore, each of the j − 1 codewords considered for
reconstruction prior to the subsequent device failure loses
an expected number of E(L) symbols. Consequently, the
expected total number E(S−
U | DF at j) of symbols stored in
these j − 1 codewords that are lost owing to unrecoverable
errors is given by
E(S−
U | DF at j) = (j − 1) E(L) .
(35)
Unconditioning (33), (34), and (35) on the event of a device
failure during the reconstruction of the jth codeword, and using
(27), yields
E(SD) ≈
C
X
j=1
2 (C + 1 − j) (m − 1)
C
λ
µ
(36)
= (C + 1) (m − 1) λ
µ ,
(37)
E(S+
U ) ≈
C
X
j=1
(C + 1 − j) (m − 2) Ps
(m − 1)
C
λ
µ
(38)
= C + 1
2
(m − 1) (m − 2) Ps
λ
µ ,
(39)
and using (21)
E(S−
U ) ≈
C
X
j=1
(j − 1) E(L) (m − 1)
C
λ
µ
(40)
= C − 1
2
[1 − (1 − Ps)m−1 + (m − 1)Ps](m − 1) λ
µ .
(41)
Combining the two cases, and using (32), (39), and (41),
the expected number E(SU) of symbols lost due to unrecov-
erable errors is obtained as follows:
E(SU) = E(S⊙
U ) + E(S+
U ) + E(S−
U )
≈ C [ 1 − (1 − Ps)m−1 + (m − 1)Ps ]
− C + 1
2
[ 1 − (1 − Ps)m−1 + Ps ](m − 1) λ
µ .
(42)
Remark 4: From (32), (39), and (41), it follows that
E(S⊙
U ) ≫ E(S−
U ) > E(S+
U ) because E(S−
U ) and E(S+
U ) are
of the order O(λ/µ), which is very small, whereas E(S⊙
U )
is not. Moreover, for large C, we have E(S−
U )/E(S+
U ) ≈
[ 1−(1−Ps)m−1+(m−1)Ps ]/[(m−2)Ps] > 1. In particular,
for small Ps, we have (1 − Ps)m−1 ≈ 1 − (m − 1)Ps,
which implies that E(S−
U )/E(S+
U ) ≈ 2(m − 1)/(m − 2) > 1.
Consequently, the symbols lost due to unrecoverable errors are
predominately encountered during a rebuild that is completed
without experiencing an additional device failure.
From (37) and (42), it follows that the expected total
number of symbols lost E(S) is given by
E(S) = E(SD) + E(SU)
(43)
≈ C [ 1 − (1 − Ps)m−1 + (m − 1)Ps ]
+ C + 1
2
[ 1 + (1 − Ps)m−1 − Ps ](m − 1) λ
µ .
(44)
Remark 5: For small values of Ps, it follows from (44)
that
E(S) ≈ 2 C (m−1)Ps + C + 1
2
(2−mPs)(m−1) λ
µ , (45)
which implies that for Ps = 0, E(S) = E(SD) = (C+1) (m−
1) λ/µ.
Remark 6: When Ps increases and approaches 1, it follows
from (44) that E(S) approaches C m. This is intuitively
obvious because when Ps = 1, all the C m symbols stored
in the system are lost because of unrecoverable errors.
We now proceed to derive E(Q), the expected amount of
user data lost. First, we note that the expected number of user
symbols lost is equal to the product of the storage efﬁciency to
the expected number of symbols lost. Consequently, it follows
from (44) that
E(Q) = l
m E(S) s
(3)
=
l
m
E(S)
C
c ,
(46)
where s denotes the symbol size. Similar expressions for the
expected amounts E(QD) and E(QU) of user data lost due
to device and unrecoverable failures are obtained from E(SD)
and E(SU), respectively. Thus, from (37), (42), and (44), it
follows that
E(QD) ≈ l
m
C + 1
C
(m − 1) λ
µ c ,
(47)
E(QU) ≈ l
m

1 − (1 − Ps)m−1 + (m − 1)Ps
− C + 1
2 C
[ 1 − (1 − Ps)m−1 + Ps ](m − 1) λ
µ

c ,
(48)
and
E(Q) = E(QD) + E(QU)
(49)
≈ l
m

1 − (1 − Ps)m−1 + (m − 1)Ps
+C + 1
2 C
[ 1 + (1 − Ps)m−1 − Ps ](m − 1) λ
µ

c .
(50)
Remark 7: For small values of Ps, and using (5), it follows
from (48) that
E(QU) ≈ 2 l
m (m − 1) c Ps .
(51)
Remark 8: From (47) and (51), it follows that the region
[0, P (3)
s
] of Ps in which E(QU) is much smaller than E(QD)
5
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-699-6
CTRQ 2019 : The Twelfth International Conference on Communication Theory, Reliability, and Quality of Service

is obtained by
E(QU) ≪ E(QD)
⇔
2 l
m (m − 1) c Ps ≪
l
m
C + 1
C
(m − 1) λ
µ c
⇔
Ps ≪ P (3)
s
≜ 1
2 · C + 1
C
· λ
µ .
(52)
Remark 9: When Ps increases and approaches 1, it follows
from (50) that E(Q) approaches C l. This is intuitively obvious
because when Ps = 1, upon the ﬁrst-device failure, all the C l
user-data symbols stored in the RAID-5 array are lost owing
to unrecoverable errors.
D. Reliability Metrics
The MTTDL normalized to 1/λ is obtained by substituting
(29) into (9) as follows:
λ MTTDL ≈
1
n
n
(m − 1) λ
µ +
h
1 − (m − 1) λ
µ
i 
1 − (1 − Ps)(m−1) Co ,
(53)
where C and λ/µ are given by (3) and (5), respectively.
The EAFDL normalized to λ is obtained by substituting
(50) into (10) as follows:
EAFDL/λ ≈ 1 − (1 − Ps)m−1 + (m − 1)Ps
+ C + 1
2 C
[ 1 + (1 − Ps)m−1 − Ps ](m − 1) λ
µ ,
(54)
where C and λ/µ are given by (3) and (5), respectively.
The E(H) normalized to c is obtained by substituting (50)
and (29) into (11) as follows:
E(H)/c ≈
l
m

1 − (1 − Ps)m−1 + (m − 1)Ps
+ C + 1
2 C
[ 1 + (1 − Ps)m−1 − Ps ](m − 1) λ
µ


(m − 1) λ
µ +

1 − (m − 1) λ
µ
 h
1 − (1 − Ps)(m−1) Ci
,
(55)
where C and λ/µ are given by (3) and (5), respectively.
Similarly to (11), expressions for E(HD) and E(HU),
the expected amounts of user data lost due to device and
unrecoverable failures, given that such failures have occurred,
are obtained as follows:
E(HD) = E(QD)
PDF
,
and
E(HU) = E(QU)
PUF
,
(56)
respectively.
From (11), (49), and (56), we deduce that the following
relation holds
E(H) = PDF
PDL
E(HD) + PUF
PDL
E(HU) .
(57)
Note that this is not a weighted average of E(HD) and
E(HU) because the events of a subsequent device failure
and of unrecoverable failures are not mutually exclusive, and
therefore, and according to (28), the sum of weights is not
equal to 1.
Remark 10: The
normalized
E(H)/c
exhibits
two
plateaus. According to (26), (30), (47), and (52), the ﬁrst
plateau is in the region [0, P (1)
s
] of Ps, that is,
E(H)
c
≈
l
m
C + 1
C
,
for Ps ≪ P (1)
s
.
(58)
For the second plateau, depending on the value of λ/µ, the
following two cases are considered:
Case 1: λ/µ ≫ 2/[(m − 1)(C + 1)]. From (18) and (52),
it holds that P (2)
s
≪ P (3)
s
. According to (17), (18), (47), and
(52), the second plateau is in the region [P (2)
s
, P (3)
s
] of Ps,
that is,
E(H)
c
≈
l
m
C + 1
C
(m − 1) λ
µ ,
for P (2)
s
≪ Ps ≪ P (3)
s
.
(59)
Case 2: λ/µ ≪ 2/[(m − 1)(C + 1)]. From (18) and (52),
it holds that P (3)
s
≪ P (2)
s
. According to (17), (18), (51), and
(52), the second plateau is in the region [P (3)
s
, P (2)
s
] of Ps,
that is,
E(H)
c
≈
l
m
2
C ,
for P (3)
s
≪ Ps ≪ P (2)
s
.
(60)
Also, it follows from (51) that
E(H)
c
≈ 2 l
m (m − 1)Ps ,
for Ps ≫ max(P (2)
s
, P (3)
s
) .
(61)
Substituting (15), (26), (47), and (48) into (56) yields
E(HD)/c ≈
l
m
C + 1
C
,
(62)
and
E(HU)/c ≈ l
m

1 − (1 − Ps)m−1 + (m − 1)Ps
− C + 1
2 C
[ 1 − (1 − Ps)m−1 + Ps ](m − 1) λ
µ

. h
1 − (1 − Ps)(m−1) Ci
,
(63)
where C and λ/µ are given by (3) and (5), respectively.
Remark 11: For small values of Ps, substituting (17) and
(51) into (56) yields
E(HU)/c ≈
(
2 l
m
1
C ,
for Ps ≪ P (2)
s
2 l
m (m − 1)Ps
for Ps ≫ P (2)
s
.
(64)
Remark 12: When Ps increases and approaches 1, it fol-
lows from (55) that E(H) approaches C l. This is intuitively
obvious because when Ps = 1, all the C l user-data symbols
stored in the system are lost because of unrecoverable errors.
6
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-699-6
CTRQ 2019 : The Twelfth International Conference on Communication Theory, Reliability, and Quality of Service

V.
NUMERICAL RESULTS
We consider a RAID-5 system comprised of n = 8 devices
with N = m = 8, l = 7, λ/µ = 0.001, capacity c = 1 TB,
and symbol size s equal to a sector size of 512 bytes, such
that the number of codewords stored in a device is given by
C = c/s = 1.9×109.
The probability of data loss PDL is obtained by (15), (26),
and (29) as a function of the unrecoverable error probability
Ps of a symbol (sector), and shown in Figure 2. It follows
from (17) that, for small values of Ps, the probability PUF
of encountering an unrecoverable failure during the rebuild
process increases linearly with Ps, as indicated by the dotted
green line in Figure 2. According to (26), the probability PDF
of encountering a device failure during the rebuild process
is independent of the unrecoverable symbol error probability,
as indicated by the horizontal dotted blue line in Figure 2.
It follows from (30) that, when Ps is in the region [0, P (1)
s
],
the probability PUF of encountering an unrecoverable failure
is much smaller than the probability PDF of encountering a
device failure during the rebuild process. From (30), and for
the parameters considered, it follows that P (1)
s
= 5×10−13,
as shown in Figure 2. Subsequently, for Ps > P (1)
s
, the
probability PUF of encountering an unrecoverable failure is
much greater than that of encountering a device failure. In
particular, it follows from (17) that, when Ps ≫ P (2)
s
, PUF
and, in turn, PDL approach 1 and are essentially independent
of Ps. From (18), and for the parameters considered, it follows
that P (2)
s
= 7×10−11, as shown in Figure 2. As expected, the
total probability of data loss PDL is monotonically increasing
in Ps.
The normalized λ MTTDL measure is obtained by (53)
and is shown in Figure 3 as a function of the unrecoverable
symbol error probability. The various regions and plateaus are
also depicted and correspond to the regions discussed above
regarding the probability of data loss.
The normalized expected amount of user data lost to the
amount of data stored in a device, E(Q)/c, is obtained by (47),
10
−18 10
−16 10
−14 10
−12 10
−10 10
−8 10
−6 10
−4 10
−2
10
0
10
−8
10
−6
10
−4
10
−2
10
0
 5e−13  7e−11
Unrecoverable Symbol Error Probability ( Ps )
Probability of Data Loss
 
 
PDL
PDF
PUF
Figure 2.
Probability of data loss PDL for a RAID-5 array under latent errors
(λ/µ = 0.001, m = N = 8, l = 7, c = 1 TB, and s = 512 B).
10
−18 10
−16 10
−14 10
−12 10
−10 10
−8 10
−6 10
−4 10
−2
10
0
10
−1
10
0
10
1
10
2
Unrecoverable Symbol Error Probability ( Ps )
λ MTTDL
Figure 3. Normalized MTTDL for a RAID-5 array under latent errors (λ/µ =
0.001, m = N = 8, l = 7, c = 1 TB, and s = 512 B).
10
−18 10
−16 10
−14 10
−12 10
−10 10
−8 10
−6 10
−4 10
−2
10
0
10
−10
10
−5
10
0
 5e−04
Unrecoverable Symbol Error Probability ( Ps )
E(Q) / c
 
 
Q
QD
QU
Figure 4.
Normalized amount of data loss E(Q) for a RAID-5 array under
latent errors (λ/µ = 0.001, m = N = 8, l = 7, c = 1 TB, and s = 512
B).
(48), and (50) as a function of the unrecoverable symbol error
probability Ps, and shown in Figure 4. It follows from (51)
that, for small values of Ps, the normalized expected amount
E(QU)/c of user data lost due to unrecoverable failures in-
creases linearly with Ps, as indicated by the dotted green line in
Figure 4. According to (47), the normalized expected amount
E(QD)/c of user data lost due to a subsequent device failure
during the rebuild process is independent of the unrecoverable
symbol error probability, as indicated by the horizontal dotted
blue line in Figure 4. As anticipated, the total expected amount
E(Q) of user data lost increases monotonically with Ps. In
particular, when Ps approaches 1 and according to Remark
9, the normalized expected amount E(Q)/c of user data lost
approaches l = 7, as all user data is lost.
The normalized EAFDL/λ measure is obtained by (54) and
is shown in Figure 5 as a function of the unrecoverable symbol
7
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-699-6
CTRQ 2019 : The Twelfth International Conference on Communication Theory, Reliability, and Quality of Service

10
−18 10
−16 10
−14 10
−12 10
−10 10
−8 10
−6 10
−4 10
−2
10
0
10
−4
10
−2
10
0
10
2
Unrecoverable Symbol Error Probability ( Ps )
EAFDL / λ
Figure 5. Normalized EAFDL for a RAID-5 array under latent errors (λ/µ =
0.001, m = N = 8, l = 7, c = 1 TB, and s = 512 B).
error probability. Equation (10) suggests that this measure is
proportional to E(Q), which implies that the preceding dis-
cussion regarding the behavior of E(Q) also holds here. Note
also that, although the fraction of data loss never exceeds 1,
EAFDL can exceed 1 because it expresses the annual fraction
of data loss, which also takes into account the frequency of
data losses.
The normalized expected amount E(H)/c of user data lost,
given that a data loss has occurred, to the amount of data stored
in a device is obtained by (55), (62), and (63) as a function of
the unrecoverable symbol error probability Ps, and shown in
Figure 6. In contrast to the PDL, EAFDL, and E(Q) measures
that increase monotonically with Ps, we observe that E(H)
does not.
Data losses occur because of a subsequent device failure,
unrecoverable failures of codewords, or a combination thereof.
According to (62), the expected amount E(HD) of user data
lost associated with a subsequent device failure, given that
such a device failure has occurred during the rebuild process,
is independent of the unrecoverable symbol error probability,
as indicated by the horizontal dotted blue line in Figure 6.
Such a device failure causes the loss of many symbols as
opposed to a small number of additional symbols that may be
lost owing to unrecoverable failures. In particular, according
to Remark 2 and (23), each of the codewords that cannot
be restored loses approximately two symbols. When Ps is
extremely small, an unrecoverable failure is very unlikely,
but when this occurs, it is caused by encountering a single
codeword that cannot be recovered, which in turn results in
the loss of two symbols. Consequently, and according to (64),
for Ps such that Ps ≪ P (2)
s
= 7×10−11, the expected amount
E(HU) of user data lost due to unrecoverable failures, given
that such unrecoverable failures have occurred, is independent
of Ps, as indicated by the horizontal part of the dotted
green line shown in Figure 6. Also, the amount of data lost
corresponding to the two symbols lost is negligible compared
with the amount of data lost due to a subsequent device failure,
that is, E(HU) ≪ E(HD).
10
−18 10
−16 10
−14 10
−12 10
−10 10
−8 10
−6 10
−4 10
−2
10
0
10
−10
10
−5
10
0
 5e−13  7e−11
 5e−04
Unrecoverable Symbol Error Probability ( Ps )
E(H) / c
 
 
H
HD
HU
Figure 6.
Normalized E(H) for a RAID-5 array under latent errors (λ/µ =
0.001, m = N = 8, l = 7, c = 1 TB, and s = 512 B).
The combined expected amount E(H) of user data lost,
given that a data loss has occurred, is an average of E(HD)
and E(HU) with weights expressed by (57). For Ps ≪ P (1)
s
=
5×10−13, a data loss is most likely attributed to a device
failure, which results in the ﬁrst plateau expressed by (58).
However, for values of Ps in the region [5×10−13, 7×10−11],
this is reversed, in that it becomes more likely to encounter
an unrecoverable failure than a device failure, and this causes
PDL to increase as shown in Figure 2. Consequently, as the
weight of the E(HD) component decreases, so does E(H).
Subsequently, as Ps increases further, this weight can no longer
decrease because PDL has reached its maximum value of 1.
Also, according to (19) and (64), the number of codewords
with unrecoverable failures and the corresponding amount of
data lost E(HU) increase linearly in Ps, but, although E(HU)
increases, as indicated by the dotted green line in Figure 6, it
still remains negligible compared with E(HD). Consequently,
E(H) no longer decreases and stabilizes at the second plateau
level given by (59). As Ps increases further and exceeds
P (3)
s
= 5×10−4, the increasing amount of data lost due to
unrecoverable failures E(HU) exceeds E(HD), which in turn
leads to an increase of the E(H) metric. In particular, when
Ps approaches 1, and according to Remark 9, the amount
l c of user data stored in the RAID-5 array is lost owing to
unrecoverable errors, which in turn implies that the normalized
expected amount E(H)/c of user data lost approaches l = 7.
VI.
DISCUSSION
As discussed in Section III, ﬁeld results suggest that the
probability of unrecoverable sector errors lies in the range
[4.096×10−11, 5×10−9]. Figure 3 shows that MTTDL is
signiﬁcantly degraded by the presence of latent errors, whereas
Figure 5 reveals that EAFDL is practically unaffected in this
range. When the probability of unrecoverable sector errors
lies in the region of practical interest, the probability of
encountering an unrecoverable failure is much larger than that
of encountering a device failure, which degrades MTTDL.
However, the amount of sectors lost due to latent errors is
negligible compared with the amount of data lost due to a
8
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-699-6
CTRQ 2019 : The Twelfth International Conference on Communication Theory, Reliability, and Quality of Service

device failure, which in turn implies that EAFDL remains
unaffected. In contrast, Figure 6 reveals that the expected
amount E(H) of data lost, given that a data loss has occurred,
decreases in the region of practical interest. This is due to
the fact that when a data loss occurs, it is more likely caused
by a unrecoverable failures that involve the loss of a small
number of sectors rather than by a device failure that results
in a signiﬁcantly larger amount of data lost.
It follows from (30) and (52) that
P (1)
s
= 1
C · λ
µ
≪
1
2 · C + 1
C
· λ
µ = P (3)
s
.
(65)
Consequently, increasing Ps ﬁrst affects PDL, MTTDL, and
E(H) and then E(Q) and EAFDL.
VII.
CONCLUSIONS
The effect of latent sector errors on the reliability of
RAID-5 data storage systems was investigated. A method-
ology was developed for deriving the Mean Time to Data
Loss (MTTDL) and the Expected Annual Fraction of Data
Loss (EAFDL) reliability metrics analytically. Closed-form
expressions capturing the effect of unrecoverable latent errors
were obtained. We established that the reliability of storage
systems is adversely affected by the presence of latent errors.
The results demonstrated that the effect of latent errors depends
on the relative magnitudes of the probability of encountering
a latent error versus the probability of encountering a device
failure. It was found that, for actual values of the unrecoverable
sector error probability, MTTDL is adversely affected by the
presence of latent errors, whereas EAFDL is not.
Extending the methodology developed to derive the
MTTDL and EAFDL reliability metrics of erasure coded
systems in the presence of unrecoverable latent errors is a
subject of further investigation.
REFERENCES
[1]
I. Iliadis, “Reliability of erasure coded systems under rebuild bandwidth
constraints,” in Proceedings of the 11th International Conference on
Communication Theory, Reliability, and Quality of Service (CTRQ),
Apr. 2018, pp. 1–10.
[2]
D. A. Patterson, G. Gibson, and R. H. Katz, “A case for redundant arrays
of inexpensive disks (RAID),” in Proceedings of the ACM SIGMOD
International Conference on Management of Data, Jun. 1988, pp. 109–
116.
[3]
P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson,
“RAID: High-performance, reliable secondary storage,” ACM Comput.
Surv., vol. 26, no. 2, Jun. 1994, pp. 145–185.
[4]
V. Venkatesan, I. Iliadis, C. Fragouli, and R. Urbanke, “Reliability of
clustered vs. declustered replica placement in data storage systems,” in
Proceedings of the 19th Annual IEEE/ACM International Symposium
on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), Jul. 2011, pp. 307–317.
[5]
I. Iliadis, D. Sotnikov, P. Ta-Shma, and V. Venkatesan, “Reliability of
geo-replicated cloud storage systems,” in Proceedings of the 2014 IEEE
20th Paciﬁc Rim International Symposium on Dependable Computing
(PRDC), Nov. 2014, pp. 169–179.
[6]
M. Malhotra and K. S. Trivedi, “Reliability analysis of redundant arrays
of inexpensive disks,” J. Parallel Distrib. Comput., vol. 17, Jan. 1993,
pp. 146–151.
[7]
A. Thomasian and M. Blaum, “Higher reliability redundant disk arrays:
Organization, operation, and coding,” ACM Trans. Storage, vol. 5, no. 3,
Nov. 2009, pp. 1–59.
[8]
I. Iliadis, R. Haas, X.-Y. Hu, and E. Eleftheriou, “Disk scrubbing versus
intradisk redundancy for RAID storage systems,” ACM Trans. Storage,
vol. 7, no. 2, Jul. 2011, pp. 1–42.
[9]
V. Venkatesan, I. Iliadis, and R. Haas, “Reliability of data storage
systems under network rebuild bandwidth constraints,” in Proceedings
of the 20th Annual IEEE International Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Aug. 2012, pp. 189–197.
[10]
J.-F. Pˆaris, T. J. E. Schwarz, A. Amer, and D. D. E. Long, “Highly
reliable two-dimensional RAID arrays for archival storage,” in Pro-
ceedings of the 31st IEEE International Performance Computing and
Communications Conference (IPCCC), Dec. 2012, pp. 324–331.
[11]
I. Iliadis and V. Venkatesan, “Most probable paths to data loss: An
efﬁcient method for reliability evaluation of data storage systems,” Int’l
J. Adv. Syst. Measur., vol. 8, no. 3&4, Dec. 2015, pp. 178–200.
[12]
——, “Expected annual fraction of data loss as a metric for data storage
reliability,” in Proceedings of the 22nd Annual IEEE International
Symposium on Modeling, Analysis, and Simulation of Computer and
Telecommunication Systems (MASCOTS), Sep. 2014, pp. 375–384.
[13]
——, “Reliability evaluation of erasure coded systems,” Int’l J. Adv.
Telecommun., vol. 10, no. 3&4, Dec. 2017, pp. 118–144.
[14]
Amazon
Simple
Storage
Service.
[Online].
Available:
http://aws.amazon.com/s3/ [retrieved: January 2019]
[15]
D. Borthakur et al., “Apache Hadoop goes realtime at Facebook,”
in Proceedings of the ACM SIGMOD International Conference on
Management of Data, Jun. 2011, pp. 1071–1080.
[16]
R. J. Chansler, “Data availability and durability with the Hadoop
Distributed File System,” ;login: The USENIX Association Newsletter,
vol. 37, no. 1, 2013, pp. 16–22.
[17]
K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop
Distributed File System,” in Proceedings of the 26th IEEE Symposium
on Mass Storage Systems and Technologies (MSST), May 2010, pp.
1–10.
[18]
Hitachi Global Storage Technologies, Hitachi Disk Drive Product
Datasheets. [Online]. Available: http://www.hitachigst.com/ [retrieved:
January 2019]
[19]
E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large
disk drive population,” in Proceedings of the 5th USENIX Conference
on File and Storage Technologies (FAST), Feb. 2007, pp. 17–28.
[20]
A. Dholakia, E. Eleftheriou, X.-Y. Hu, I. Iliadis, J. Menon, and K. Rao,
“A new intra-disk redundancy scheme for high-reliability RAID storage
systems in the presence of unrecoverable errors,” ACM Trans. Storage,
vol. 4, no. 1, May 2008, pp. 1–42.
[21]
I. Iliadis, “Reliability modeling of RAID storage systems with latent
errors,” in Proceedings of the 17th Annual IEEE/ACM International
Symposium on Modeling, Analysis, and Simulation of Computer and
Telecommunication Systems (MASCOTS), Sep. 2009, pp. 111–122.
[22]
V. Venkatesan and I. Iliadis, “Effect of latent errors on the reliability
of data storage systems,” in Proceedings of the 21th Annual IEEE
International Symposium on Modeling, Analysis, and Simulation of
Computer and Telecommunication Systems (MASCOTS), Aug. 2013,
pp. 293–297.
[23]
I. Iliadis and V. Venkatesan, “Rebuttal to ‘Beyond MTTDL: A closed-
form RAID-6 reliability equation’,” ACM Trans. Storage, vol. 11, no. 2,
Mar. 2015, pp. 1–10.
[24]
V. Venkatesan and I. Iliadis, “A general reliability model for data
storage systems,” in Proceedings of the 9th International Conference
on Quantitative Evaluation of Systems (QEST), Sep. 2012, pp. 209–
219.
[25]
I. Iliadis and X.-Y. Hu, “Reliability assurance of RAID storage sys-
tems for a wide range of latent sector errors,” in Proceedings of the
2008 IEEE International Conference on Networking, Architecture, and
Storage (NAS), Jun. 2008, pp. 10–19.
[26]
V. Venkatesan and I. Iliadis, “Effect of codeword placement on the
reliability of erasure coded data storage systems,” in Proceedings of the
10th International Conference on Quantitative Evaluation of Systems
(QEST), Sep. 2013, pp. 241–257.
[27]
I. Iliadis and V. Venkatesan, “An efﬁcient method for reliability evalu-
ation of data storage systems,” in Proceedings of the 8th International
Conference on Communication Theory, Reliability, and Quality of
Service (CTRQ), Apr. 2015, pp. 6–12.
[28]
V. Venkatesan and I. Iliadis, “Effect of codeword placement on the
reliability of erasure coded data storage systems,” IBM Research Report,
RZ 3827, Aug. 2012.
9
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-699-6
CTRQ 2019 : The Twelfth International Conference on Communication Theory, Reliability, and Quality of Service

