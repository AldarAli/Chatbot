Communicating Semantic Content to Persons with Deafblindness by Haptograms 
and Smart Textiles: Theoretical Approach and Methodology  
Sándor Darányi, Nasrine Olson1, Eva Lindell, Nils-
Krister Persson 
University of Borås 
Borås, Sweden 
E-mail: {sandor.daranyi, nasrine.olson, eva.lindell, 
nils-krister.persson}@hb.se 
Marina Riga, Efstratios Kontopoulos, Ioannis Kompat-
siaris 
Information Technologies Institute (ITI/CERTH) 
Thessaloniki, Greece 
E-mail: {mriga, skontopo, ikom}@iti.gr 
 
 
Abstract— By means of a proof-of-concept prototype, which is 
work in progress, we adopted a multidisciplinary approach to 
develop a smart-textile-based communication system for use by 
people with deafblindness. In this system, sensor technologies 
and computer vision are used to detect environmental cues 
such as presence of obstacles, faces, objects, etc. Focusing on 
the communication module here, a new ontology connects vis-
ual analytics with the user to label detected semantic content 
about objects, persons and situations for navigation and situa-
tional awareness. Such labelled content is then translated to a 
haptogram vocabulary with static vs. dynamic patterns, which 
are mapped to the body.  A haptogram denotes a tactile symbol 
composed over a touchscreen, its dynamic nature referring to 
the act of writing or drawing. A vest made of smart textile, in 
the current variant equipped with a 4 x 4 grid of vibrotactile 
actuators, is used to transmit haptograms on the user’s back. 
Thereby system messages of different complexity -- both alerts 
and short sentences -- can be received by the user, who then 
has the option to respond by pre-coded questions and messag-
es. By means of grids with more actuators, displays with higher 
resolution can be implemented and tested, paving the way for 
an extended haptogram vocabulary, covering more detailed 
ontology content. 
Keywords - deafblind communication; haptograms; word and 
sentence semantics; ontology; smart textiles. 
I. 
 INTRODUCTION 
Communication with and between people with deaf-
blindness is constrained by the nature of the condition and 
the lack of supportive tools and societal structures. This 
study explores novel communication modes for this group. 
Deafblindness refers to a unique combination of vision 
and hearing loss, where the level of loss in either senses is 
such that it does not allow compensation of one impaired 
sense by the other. This condition in its various forms, rang-
ing from congenital (i.e., present at birth or acquired prior to 
language development) to acquired (i.e., due to illness, acci-
dent or ageing), involves a broad diversity in abilities. Based 
on [1], we report work in progress that aims to address 
communication challenges that accompany deafblindness. 
Our approach combines a simple conceptual language, with 
navigation, situational awareness and communication com-
ponents. The communication is conducted by means of hap-
tic stimuli, projected on appropriate parts of the human body 
via a smart textile screen, and is intended to be useful across 
the complete spectrum of this condition. 
Our research is carried out in the SUITCEYES 
(https://suitceyes.eu/)1EU Research and Innovation Action 
project, with the mission to deploy a prototype which is 
wearable, combines situational awareness, visual analytics 
and communication by a joint ontology, and works in dis-
tance mode by wireless connection as a default. Instead of a 
combination of elements of touch, e.g., consecutive dots, 
dashes and strokes by hand to encode characters and their 
sequences, we propose to use haptograms, where the limited 
size and resolution of a body part as screen is counterbal-
anced by evolving patterns, i.e., the dynamics of signs. Our 
effort is in line with the approach by [2], building on their 
Tactile Brush findings. However, the focus here is on lan-
guage design by means of an ontology-compliant vocabulary 
vs. grammar, where the latter implements relational contex-
tualization and sign sequencing. Thus, our work belongs to 
the category of a priori defined spatial-temporal patterns in 
the semiotic vein. Here we will discuss the integration of 
ontology, haptograms and textile aspects only from a theoret-
ical and methodological perspective. 
The rest of the paper is structured as follows: Section II 
reviews research in haptics relevant to our design considera-
tions, contrasted with its respective linguistic underpinnings. 
Section III introduces the SUITCEYES ontology that plays 
the role of the unified model for semantic integration of in-
formation from the environment, while Section IV presents 
our approach for designing the haptogram vocabulary. Sec-
tion V discusses implementation in smart textile, whereas 
Section VI concludes the paper and refers to future work 
directions. 
II. 
RELATED RESEARCH 
A. 
Research in haptics relevant to our design 
For haptogram development we were inspired by [3], and 
[4], where the authors’ work expands over multiple decades. 
Their approach, Social Haptic Communication (SHC), basi-
cally reproduces ideograms on different regions of the body 
by a combination of hand strokes, gestures, pressure, etc. For 
this, they developed a rich set of tactile signs, compiled into 
a simple language with its own syntax built from haptemes, 
i.e., building blocks of touch, and vocabulary of so-called 
haptices, tailored to a range of situations and topics of great 
practical importance for the receiver. At the same time, due 
 
1 Corresponding author 
129
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

to its consensual nature, this approach is idiosyncratic, and in 
its social mode unable to be applied at a distance.  
The above phonemic approach in [4] finds support from 
[5], where the authors found that decomposing language into 
phonemes that are transcribed into unique vibrotactile pat-
terns enables people to receive lexical messages on the arm. 
A potential barrier to adopting this new communication sys-
tem is the time and effort required to learn the association 
between phonemes and vibrotactile patterns. However, their 
study was limited to the learning of 100 patterns by different 
methodologies, displayed on the arm, and the concepts were 
not connected to an ontology.  
On the other hand, in [6], the authors experimented with 
a new tactile speech device based on the presentation of pho-
nemic-based tactile codes. The device consisted of 24 tac-
tors, under independent control for stimulation at the elbow 
to wrist area. Using properties that included frequency and 
waveform of stimulation, amplitude, spatial location, and 
movement characteristics, unique tactile codes were de-
signed for 39 consonant and vowel phonemes of the English 
language. The participants, 10 young adults, were then 
trained to identify sets of consonants and vowels, before be-
ing tested on the full set of 39 tactile codes. 
In [7], the authors investigated several haptic interfaces 
designed to reduce mistakes in Morse code reception of 12 
characters. Results concluded that a bimanual setup, discrim-
inating dots/dashes by left/right location, reduced the amount 
of errors to only 56.6% of the errors compared to a unimanu-
al setup that used temporal discrimination to distinguish dots 
and dashes. 
Very much in line with what we would like to achieve, 
the authors in [2] proposed Tactile Brush, an algorithm that 
produces smooth, two-dimensional tactile moving strokes 
with varying frequency, intensity, velocity and direction of 
motion. The design of the algorithm was derived from the 
results of psychophysical investigations of two tactile illu-
sions, apparent tactile motion and phantom sensations. 
Combined together they allowed for the design of high-
density two-dimensional tactile displays using sparse vi-
brotactile arrays. In a series of experiments and evaluations, 
they demonstrated that Tactile Brush is robust and can relia-
bly generate a wide variety of moving tactile sensations for a 
broad range of applications. 
Another related track of research [8] conducted four ex-
periments to evaluate tactile localization and tactile pattern 
recognition on the torso by means of a one-dimensional 
eight-tactor display vs. a two-dimensional 16-tactor display 
to present tactile cues to the waist and back, respectively. 
They found that a display with eight tactors mounted circum-
ferentially around the waist can provide tactile cues that are 
perceived very accurately in terms of the location of stimula-
tion, whereas the 16-tactor array on the back was found to be 
inadequate to support precise spatial mapping, but an array 
with fewer elements could provide such spatial cues, so that 
simple navigational and instructional commands can be pre-
sented tactually on the torso. These findings were augmented 
in [9], where the authors evaluated the effectiveness of tactile 
displays either on the forearm or the back to communicate 
simple instructions and commands in a military context. 
Their results suggested that with the judicious selection of 
tactile patterns both the arm and back provide a functional 
substrate for tactile communication. 
A next source of inspiration was [10], which described a 
different domain of study. As in a situation of sensory over-
load, touch is a promising candidate for messaging given that 
it is our largest sensory organ with impressive spatial and 
temporal acuity, there is need for a theory that addresses the 
design of touch-based building blocks for expandable, effi-
cient, rich and robust touch languages that are easy to learn 
and use; moreover, beyond design, there is a lack of imple-
mentation and evaluation theories for such languages. To 
overcome these limitations, he proposed a unified, theoreti-
cal framework, inspired by natural, spoken language, called 
Somatic ABC’s for Articulating (designing), Building (de-
veloping) and Confirming (evaluating) touch-based lan-
guages. To evaluate the usefulness of Somatic ABC’s, its 
design, implementation and evaluation theories were applied 
to create communication languages for two very unique ap-
plication areas: audio-described movies, and motor learning. 
It was found that Somatic ABC’s aided the design, develop-
ment and evaluation of rich somatic languages with distinct 
and natural communication units. 
Implementation-wise, the approach to conveying any 
tactile code to the body is often rather simple. Glued-on 
solutions or non-compliant devices are common in many of 
the more research-oriented studies. For more technical de-
scriptions with users in focus, textiles and garment have 
been employed and are described by many [11], [12]. The 
interest in the literature is typically in the hardware and its 
control system, which soon becomes problematic when the 
number of actuators and connections increases. Gaming has 
attracted a lot of interest. By far the most common haptic 
output device used is the vibrator, denoted vibrotactile ele-
ment (VTE), with textile aspects being secondary. In [13], 
the authors provide a solution with pieces made out of neo-
prene, which is stretchy. Hook-and-loop fasteners were used 
to keep five large pieces (for left shoulder, right shoulder, 
left elbow, right elbow, wrist belt) together. Comfort was 
not discussed, neither was wiring.  What was denoted heavy 
weight Lycra was used in a harness design in [14], in order 
to achieve tightness and close fit to the body. Assembly and 
design were not in focus. The authors in [15] used a tight-
fitting T-shirt in a study of haptic elements for promoting 
motion training. Once again, the technical focus was on the 
hardware. In general, there are few studies where the role of 
textile and the construction thereof is central. 
B. 
Semantic theories underlying haptograms 
Semantic content in language is manifest by means of 
word vs. sentence meaning. To pin down word semantic vs. 
sentence semantic theories which can help one identify kinds 
of meaning implementable for deafblind communication, we 
took inspiration from Chinese logograms representing con-
cepts, instead of characters that stand for speech sounds. 
They come as a blend from different directions and schools 
of semantic research [16]. More importantly, one can apply 
130
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

touch primitives to create haptic drawings for concepts, with 
connections to several semantic theories briefly listed below. 
To conceive a set of actuator patterns, which correspond 
to units in a mental vocabulary, partly overlapping with on-
tology labels, semantic primitives [17] and semantic univer-
sals [18] were relevant points of departure. These include 
elementary, archetypical concepts such as substantives (you, 
I; someone, people; something), mental predicates (think, 
know, want, feel, see, hear), descriptors (big, small), tempo-
rality (when, after, before, a long time, a short time, now), 
etc., while their concatenations can be related to the Lan-
guage of Thought hypothesis [19], a theory which describes 
the nature of thought as possessing "language-like" or com-
positional structure, with simple concepts combined in sys-
tematic ways (akin to the rules of grammar in language) so 
that in its most basic form, thought, like language, has syn-
tax. Another aspect to connect semantic primitives as com-
ponents of concepts with haptemes as components of touch 
[3] originates in structural linguistics [20]; here we chose the 
interpretation that phonemes constitute an abstract underly-
ing representation for segments of words. Distributional se-
mantics as in [21], [22], and [23] -- responsible for meaning-
fulness in dimensional reduction methods and feeding for-
ward to the ontology -- adds another relevant semantic theo-
ry to the aforementioned, with statements derived from on-
tology labels by semantic reasoning going back to logical 
semantics [24]. Another umbrella term for the above in a 
deafblind context is the research field of sign language se-
mantics [25]. A number of relevant PhD theses augmented 
our resources to relate semantics mapped to the body in a 
sensory deprivation context, e.g., [26], [27], and [28].   
III. 
THE SUITCEYES ONTOLOGY 
The key aim of the SUITCEYES ontology is to semanti-
cally integrate information coming from the environment 
(via sensors), and from the system’s analytic components 
(e.g., visual analysis of camera feed). In this sense, the on-
tology is primarily focused on semantically representing 
aspects relevant to the users’ context, in order to provide 
them with enhanced situational awareness, and augment their 
navigation and communication capabilities. More important-
ly, the proposed ontology also serves as the bridge between 
environmental cues and content communicated to the user 
via the haptograms described in the next section.  
In ontology engineering, it is common practice to reuse 
existing third-party models and vocabularies during the de-
velopment of a custom ontology, in order to rely on previ-
ously used and validated ontologies. We thus adopted the 
semantic representation of objects and activities from the 
Dem@Care ontology [29], [30], which contains a set of de-
scriptions of everyday activities and common objects used in 
an everyday context that are highly relevant to our goals 
(e.g., mug, plate, toothbrush, furniture, window, door, etc.), 
all inspired by real case scenarios. Moreover, we relied on 
SOSA/SSN [31] for representing sensors and the respective 
observations, and on the Friend-Of-A-Friend (FOAF) speci-
fication [32] for representing persons and social associations. 
Finally, we integrated the SEAS (Smart Energy Aware Sys-
tems) Building Ontology [33], which is a schema for de-
scribing the core topological concepts of a building, such as 
buildings, building spaces and rooms. A summary of all im-
ported ontologies is presented in Table 1. The hierarchy of 
concepts related to rooms and space, adopted from both 
Dem@Care and the SEAS Building Ontology, is visualized 
in Fig. 1. The latter set of concepts is grouped under the no-
tion of sot:SemanticSpace, where sot is the namespace prefix 
for the SUITCEYES ontology. Moreover, we extended our 
ontology with additional concepts that emerged during the 
requirements analysis (see deliverables D2.1 and D2.2 post-
ed on the project website) of our Haptic, Intelligent, Person-
alised, Interface (HIPI), which is a key SUITCEYES out-
come. 
The ontology, populated with the incoming information 
(i.e., detected persons, objects etc.), constitutes the system’s 
Knowledge Base (KB). An additional component, the 
Knowledge Base Service (KBS) that sits “on top” of the KB, 
ensures that the incoming data are semantically annotated 
and fused properly into the KB, providing the groundwork 
for producing a higher-level interpretation of the combined 
information. The end goal is to enable the HIPI to deliver 
semantic content to the user with respect to his/her physical 
surroundings.
TABLE 1. 
A LIST OF THIRD-PARTY ONTOLOGIES UTILIZED AND EXTENDED WITHIN THE SUITCEYES ONTOLOGY 
prefix 
Ontology 
URL 
Concepts 
Imported in 
SUITCEYES 
version 
foaf 
Friend-Of-A-Friend 
http://xmlns.com/foaf/spec/   Person  
and its asserted properties 
v1 
sosa 
Semantic Sensor 
Network 
https://www.w3.org/TR/vo
cab-ssn/ 
Sensor  
and its asserted properties 
v1 
dem 
Dem@Care 
http://www.demcare.eu/res
ults/ontologies 
Activity 
Object 
Room 
and their asserted properties 
v1 
v1 
v2 
seas 
Smart Energy Aware 
Systems Building 
Ontology 
https://ci.mines-
stetienne.fr/seas/BuildingO
ntology-1.0 
BuildingSpace 
Room 
and their asserted properties 
v2 
v2 
 
 
131
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Finally, a rule-based semantic reasoning mechanism 
serves specific queries for inferring the context-related in-
formation, either in the form of structured content in JSON 
format, or as natural language phrases. 
 
Figure 1. Concept hierarchy related to semantic space, which were adopted 
in the SUITCEYES ontology from Dem@Care and SEAS Building.  
A. 
Ontology conceptualization 
Fig. 2 displays an overview of the core ontology classes 
based on the Graffoo ontology visualization notation [34]: 
the yellow rectangles represent classes, while the green ones 
represent data properties (i.e., properties that take a raw data 
value, like, e.g., integers and strings). The prefixes in front of 
some of the class names indicate the namespace of the re-
spective third-party ontologies, as mentioned above. Classes 
and properties that have no prefix belong to the core 
SUITCEYES ontology.  
As indicated in Fig. 2, class Detection is fundamental 
within the context of the SUITCEYES ontology and refers to 
environmental cues detected by the sensors that have been 
instantiated in the KB. An instance of type Detection may be 
associated with the relevant sensor(s) that provide data to the 
ontology, via property providedBy. Currently, there are two 
specific categorizations of class Sensor (i.e., Camera and 
iBeacon), which are related to the relevant operational sen-
sors attached to the HIPI and provide data to the KB. On the 
basis of the sensors’ data, an instance of class Detection can 
be associated with one or more instances of type Person 
(known or unknown persons), Object, Activity, and/or Se-
manticSpace (i.e., rooms, building spaces, as previously de-
scribed); the relevant assertion is achieved via property de-
tects. On the basis of the semantic reasoning mechanism 
mentioned previously, higher-level results that combine in-
coming data from all sensors, are produced and represented 
in the ontology via class Output and its specializations: Alert, 
Message, and Warning. 
With regard to spatial relations, we focus on orientation 
(left/right), 
existence 
(in 
a 
room) 
and 
distance 
(far/close/immediate). Thus, an entity that occupies space 
(e.g., persons, objects) is considered a SpatialEntity, and the 
occupied space (e.g., a room or a location) belongs to the 
SemanticSpace. 
These 
two 
aspects 
formulate 
the
 
 
Figure 2. Overview of the core classes of the SUITCEYES ontology. 
132
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
Figure 3. Semantic spaces and spatial contexts in the SUITCEYES ontology. 
respective entity’s SpatialContext, which provides infor-
mation regarding the entity’s relationship to the semantic 
space it is located in. Examples include in, on, left, right, far, 
close, etc. The aforementioned concepts are depicted in Fig. 
3. These definitions play a key role in the semantic reasoning 
mechanism, as they form the basis for inferring spatially 
related information to the user, e.g., which objects are close 
to or far from the user, what sort of entities are located in the 
room where the user is, etc.  
 
 
 
 
B. 
Sample instantiation 
Based on the ontological concepts presented above, Fig. 
4 illustrates a sample instantiation, resembling an activity 
detected by the system’s camera. The activity involves two 
people speaking to each other: one of them is known to the 
user (i.e., John) whereas the other is unknown. Moreover, 
these two people are currently located in the kitchen (i.e., 
in_room_spatial_context), and the respective message is 
communicated to the user via a textual description, which is 
then converted to haptograms as described next. 
 
 
 
 
 
 
Figure 4. Sample instantiation of an activity involving two people discussing in the kitchen. 
133
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE 2. 
EXAMPLE QUERIES FOR WHICH THE ONTOLOGY CAN PRODUCE RESULTS IN NATURAL LANGUAGE TEXT 
# 
Query 
Answer 
Q1 
Where is my <phone> now? 
Your <phone> is located on your <left>, <close to> you. 
Q2 
In which room am I now located? 
You are located in the <kitchen>. 
Q3 
When and where was my <mug> detected for 
the last time? 
Your <mug> was found in the <living room>, <10 sec-
onds> ago. 
Q4 
Which <entity> is observed on my <left> 
side? 
A <table> is on your <left> side.  
Q5 
Which are the objects I am <close to>? 
A <laptop> and a <chair> have been found <very close> to 
you. 
Q6 
How many persons have been detected <close 
to> me? 
There are <3> persons <close to> you. 
Q7 
Are there any <known> persons detected? 
<John> is detected <far from> you. 
 
This flexible ontology-based representation allows the 
system to convey various types of information to the user. In 
order to deliver semantic content through haptic media, mes-
sages generated by the KB should be structured as a natural 
language phrase rather than a simple alert or indication. This 
information is then communicated to the user by aligning 
each word or sentence with its haptogram representation, 
allowing for a semantically richer messaging mechanism.  
In the final HIPI prototype, the user will also be able to 
submit queries through a special panel. For this reason, we 
have implemented a set of SPIN rules [35] that allow the 
KBS to respond to queries submitted by the person wearing 
the HIPI as natural language phrases. Table 2 includes a rep-
resentative subset of such queries. 
 
IV. 
INTRODUCING HAPTOGRAMS 
A. 
From user needs to haptogram design considerations 
Based on an extensive user survey to advance the match 
between human needs and system support in the specific 
case of deafblindness, we opted for the transmission of per-
ceptible messages to a grid of actuators mounted onto a 
smart textile surface on the body. The set of haptic signals 
had to: (i) be easy to distinguish and perceive; (ii) represent a 
simple language whose vocabulary is either similar to estab-
lished vocabularies familiar to the user, or intuitive and sim-
ple enough to be learned; (iii) enable sentence building; and 
(iv) comply with ontology constraints for message genera-
tion. Compared with SHC (e.g., in [3] and [4]), where simple 
and typically environment-related messages are conveyed to 
the back of the user and other appropriate parts of the body 
by a human signer, our idea was to similarly convey simple 
messages, but digitally, by means of actuators. Towards this, 
a number of complex issues needed to be considered.  
A human communicator can simply adapt communica-
tion to the actual situational circumstances. For example, in a 
confined physical space a signer may switch to using the 
upper arm instead of the back. Similarly, any distortion in 
reception (as indicated by the body language of the receiver) 
may be addressed by repetition of the message or further 
clarification. In an automated system these decisions and 
interventions need to be preconceived and replaced by other 
means. Also, in SHC, the signs can easily be adapted and 
renegotiated between sender and receiver to facilitate percep-
tion based on the situational experiences of the receiver. 
Therefore, next to being idiosyncratic, SHC signs can vary 
from person to person, and even established tactile sign lan-
guages are culturally bound, existing in variants from coun-
try to country.  
The level of complexity goes beyond this, as by its nature 
deafblindness involves a very broad spectrum of abilities and 
impairments, e.g., from those who may have just a handful 
of concepts in their vocabulary to those who possess a full 
and rich language ability and can communicate in a sophisti-
cated manner using tactile sign language or even spoken 
language. This means that the set of concepts and haptogram 
patterns has to be customizable in due course. Furthermore, 
SHC involves fine-tuned, compound three-dimensional 
hand-on-body movements, while the actuator-based haptic 
patterns need to be rather simple and conveyable in a two-
dimensional format, at least for the time being. To handle 
these research constraints, we fell back on user-centric co-
design where potential users’ preferences and feedback have 
been captured. This involved an extensive user study (i.e., 80 
detailed interviews conducted in 5 different countries), and 
ideation workshops (involving participants with deafblind-
ness and communication experts), leading to psychophysics 
experiments.  
The current co-design was constrained by different tech-
nological and disciplinary bottlenecks. One of the goals was 
to build a communication module which could manage at 
least 100 concepts by haptograms, in the range of the small-
est SHC vocabulary known to us [38]. The matrix arrange-
ment of vibrotactile actuators to generate 100 different pat-
terns was considered a viable solution to this end. However, 
a 3 x 3 actuator grid was too small to allow for this variety, 
so we opted for a 4 x 4 one as our starting point. On the oth-
er hand, static patterns on one’s back, not a screen with the 
best resolution, are more difficult to distinguish than dy-
namic ones, whereas the latter also extend the number of 
possible haptograms. Because our ontology at the time of 
writing this article already included hundreds of concepts to 
match the capacity of visual detection, 4 x 4 dynamic pat-
terns were a compromise between detection capacity vs. the 
need to keep things simple for psychophysical experimenta-
tion: opting for a larger grid, apart from its wiring problems 
from a hardware and smart textiles perspective, would have 
unnecessarily overcomplicated testing. Also, we had to keep 
134
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

in mind that haptogram evaluation by default is an iterative 
process, to result in designs to be scrapped and replaced 
time and again. 
B. 
Technical aspects.  
Haptograms as a term were proposed by [36]. In their ap-
proach, “Haptogram” is a system designed to provide a 
point-cloud tactile display via acoustic radiation pressure: a 
tiled 2-D array of ultrasound transducers is used to produce a 
focal point that is animated to produce arbitrary 2-D and 3-D 
tactile shapes. The switching speed is very high, so that hu-
mans feel the distributed points simultaneously. The Hapto-
gram system comprises a software and a hardware compo-
nent; the software component enables users to author and/or 
select a tactile object, create a point-cloud representation, 
and generate a sequence of focal points to drive the hard-
ware.  
Our idea of a haptogram is different and corresponds to 
logograms. That is, concepts are communicated by touch-
based drawings, and not character sequences to invoke them. 
Haptograms are, in short, synthetically generated haptic pat-
terns with a meaning, to be communicated as stimuli to the 
human body. They could be discretized, i.e., represented by a 
number of matrix cells, but could also be continuous, illus-
trated for example as Bezier curves [37]. The meaning of 
such patterns could be both at word and sentence level. Fur-
ther, we distinguish between stable vs. changing patterns and 
call them static vs. dynamic haptograms in a communication 
context, where both can be pulsating for easier recognition. 
The purpose of haptograms in our framework is to imple-
ment an ontology-constrained messaging language for situa-
tion awareness assessments, and as raw material for every-
day conversation. Both visual and audial information can be 
translated to haptic patterns.  
Such haptograms can be mapped to one or more parts of 
the body, in single or multiple modes. Unlike studies that 
focus on using hands (e.g., by developing communicative 
gloves), our system does not use the hands in order to keep 
them free for other purposes, such as holding a cane, giving a 
handshake, examining surfaces or objects, etc. The semantic 
content is transferred to the body to trigger actuators of vi-
bration, pressure, tapping etc., as well as combinations there-
of.  
Actuators to display haptograms are advantageously ar-
ranged in a matrix form of rows and columns. It is not neces-
sary that they are of equal number; rather the general ap-
proach is an m x n arrangement with m rows and n columns.  
A two-dimensional (2D) arrangement of haptic actuators has 
several benefits over, say, a one-dimensional (1D) linear 
array arrangement. A single point vibrator - what could be 
called a zero-dimensional (0D) arrangement - can only con-
vey alarm-like semantic content, such as “Something is 
happening”. (For example, this is the case for cellphones 
which, while vibrating, can indicate that there is an incom-
ing call or time is out). This way is not best suited to con-
veying richer content like what is happening, or if several 
things are happening in parallel, including their direction, 
etc.  Compared to 1D, a 2D arrangement enables richer di-
rectional information: it is possible to identify from where a 
haptic stimulus is coming. In a 2D arrangement one can use 
the columns, say, for a certain general class of ontological 
entities (e.g., related to eating, moving, friends, relation-
ships, etc.) and letting each row to correspond to different 
concepts therein these realms. This could not be done in 0D 
and 1D. 
The world is inherently three-dimensional but humans 
have a developed understanding and capability for extract-
ing information out of two-dimensional projections. This 
holds for exploration both by sight and touch. In the latter 
case, a 2D vibrator arrangement is much closer to a 2D top-
ological tactile description of a surface than a 1D array. In 
fact, most often interpreters using SHC describe a room or 
scene in front of the user by sketching it as a 2D projection, 
indicating borders, exits and windows, significant objects 
and persons. A 2D arrangement of actuators lends itself very 
well to this case. Synonymous is to say that 2D enables lo-
calization better.  2D arrangements also open up for dynam-
ic patterns as actuators corresponding to dots in a haptogram 
could be made to vibrate in a certain order. This is not only 
beneficial for perception, compared to the static case, but 
also yields more possible combinations of patterns over m x 
n vibrators. In the limit of increasing m and n within a given 
area and even letting m and n approaching infinity, the reso-
lution increases and continuous patterns are made possible. 
Of course, human capability to resolve two spatially or tem-
porally separated haptic stimuli is limited, and has to be 
taken into account. Finally, a 2D arrangement is also a step 
towards haptic pictures, i.e., leaving the paradigm of using 
symbols. Haptic pictures are such that they mimic a tactile 
object by levels of contrast of its parts. 
Our vocabulary was designed for in-principle receiver 
testing over a 4 x 4 actuator grid for a proof of concept. We 
were interested in finding out if the ontology and such a hap-
tic conceptual vocabulary can be aligned, and how pattern 
sequences reminiscent of sentences can implement the 
transmission of more complex semantic content. To enable 
hypothesis testing, we departed from the following: 
 
• 
Screen on the back, vibrational actuators only: 
•  
104 dynamic signs, matching the magnitude of 
[38]; 
•  
Homonymy disambiguation by different dynam-
ics for the same patterns vs. synonymy represen-
tation by two different approaches. 
• 
Vocabulary signs for: 
•  
Start, stop, alert, sentence type markers, ques-
tion words, agree/disagree; 
•  
Known/unknown person, personal pronouns (4); 
•  
Place adverbs (15), time adverbs (4); 
•  
Nouns (48), verbs (13), adjectives (3). 
Concrete examples below go back to the ontology-
compliant sentence sample in Table 2. 
C. 
Examples 
As haptograms in principle can be static or dynamic, and 
can represent both word meaning and sentence meaning, Fig. 
5 illustrates the basic idea of the former which was derived 
from the ASCII code table, where in its matrix cells, instead 
135
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

of characters, concepts are encoded by bit strings in the rows 
vs. columns. 
In Fig. 6, we illustrate two sample dynamic haptograms. 
In the upper part, numbers indicate the firing sequence of the 
actuators for concepts (a) and (b), meaning “stand” and 
“door”. In the lower part the completed shapes of the dynam-
ic haptograms are indicated. 
Moving over to sentence meaning, in Fig. 7 we show 
how three simple statements, “You are in the kitchen” (Ex-
ample 1), “The bottle is close to the left on the table” (Exam-
ple 2), and “There is an unknown person in the hallway” 
(Example 3), can be constructed by concatenating dynamic 
haptograms. The statement begins with a single-blink sign, 
indicating the start of a new message, and finishes with a 
double-blink, pulsating one, indicating end of transmission. 
Any statement can be accompanied by a separate alert sign 
to add weight to the communicated content. 
Apart from this example, our test included declarative 
sentences, questions and exclamations to enable a future 
dialogue between two users with deafblindness, or a user 
with deafblindness and her/his assistant, family member, 
educator, etc. Further, the vocabulary is both aligned with the 
ontology, and is including concepts and parts of speech not 
covered by the current version, i.e., hints at expansion oppor-
tunities. Likewise, e.g., logical operators, numbers, signs for 
operations etc. can be added following the same line of 
thought. 
Experiment constraints included that over a 4 x 4 grid, no 
digital numbers could be specified, and screen resolution, 
i.e., actuator grid granularity was too low for simulating gen-
uine SHC signs.  
 
 
Figure 5. The idea of static haptograms over a 4 x 4 actuator grid. 
 
 
Figure 6. Unfolding sequences of two patterns over a 4 x 4 actuator grid, 
yielding different dynamic haptograms: (a) “stand”; (b) “door”. 
V. 
HAPTOGRAM REALIZATION IN SMART TEXTILES  
Haptograms are to be transmitted to different locations 
on the body and need ample space as they are inherently 
extended. Very naturally this leads to the use of textiles. Tex-
tiles have been there in all human activities throughout the 
ages, regardless of sex, social status, culture, and occupation. 
In the form of garments, they cover large parts of the body, 
such as one’s back or upper arms, those areas where hapto-
grams are to be transferred. Important is also that textiles are 
pliable and adapted to the complicated geometry of the hu-
man body. They could be made to tightly fit one’s shape, 
which is very important for any haptic communication as 
typically, good mechanical contact is central.  
Textile is then a platform. Its enrichment with sensors 
and actuators, including haptic devices, is often referred to as 
smart textiles.  A basic thought behind the application of 
haptograms has been to allow the transmission of perceptible 
messages to the users via actuators mounted into and onto 
textiles. We opted for discrete haptic signs inherent in the 
matrix approach above.   
We define location as the geometrical position on the 
body, i.e., an anatomical measure, whereas placement is the 
positioning of some actuator (or sensor) on the textile. A 
given actuator with a given placement is positioned as close 
as possible to a target location when taking on the garment, 
and while wearing it. Further on, we define cell as a physical 
construction on, or in the textile, such as a pocket having a 
certain placement from which actuation could take place. 
Typically, we will have many more cells than vibrators in a 
garment. Cells that have an actuator that is actuating are ac-
tive or firing. Such an active cell corresponds to a dot in a 
pattern, a set of active dots. Thus, a pattern of actuators 
could either simultaneously become active, which is the stat-
ic version of haptograms, or becoming active in a sequence, 
i.e., the dynamic alternative.  
Last but not least, from a smart textile perspective, a set 
of cells close to each other constitute a panel to host actua-
tors in any particular arrangement.  There could be a panel 
for the back, another for the upper arms, for the waist, and so 
on.  
We constructed a textile testing device for the back (Fig. 
8). The human back has a complicated geometry with both 
convex and concave regions. Added to this is that users are 
of different sizes. In order to fulfil the simultaneous need of 
fixed location of actuators, as well as good mechanical at-
tachment for maximal haptic signal transmittance, we opted 
for a construction with both vertical and horizontal stripes 
that could be tightened individually at the front by buckles, 
giving a good fit to the body for all sizes.  This structure 
takes inspiration from a weave with interlaced parts. On the 
inside, towards the body, detachable pockets are placed, 
each of them able to host an actuator. These pockets have 
openings as well so that cables for powering could be taken 
out and contacted. In total, a very versatile system was cre-
ated for the realization of haptograms. 
In Fig. 9 the placements of 16 vibrotactile elements in a 
4 × 4 matrix arrangement is illustrated. The stripes are ar-
136
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

ranged so that the actuators can be changed from the out-
side, making it simple to study pattern arrangements without 
taking off and on the garment.  
 
 
 
 
 
Figure 7. Examples 1-3 are system responses to predefined user queries 
over a 4 x 4 actuator grid, with patterns plus their unfolding sequences in 
red. 
 
Figure 8. Back side of the textile testing device. Black squares belong to 
stripes in the vertical direction, white in the horizontal direction. Together a 
chessboard like appearance is formed. This results in an inherent matrix for 
specifying sign component positions. 
137
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

VI. 
CONCLUSION AND FUTURE WORK  
A constraint on this report has been the COVID-19 pan-
demic which, because of the extreme vulnerability of our 
user group, prevented the evaluation phase of communica-
tion by haptograms and smart textiles. This will be remedied 
in a next article in due course. Thereby our design consider-
ations had to remain on a theoretical and methodological 
level. 
By means of a proof-of-concept prototype which is work 
in progress, we adopted a multidisciplinary approach to de-
velop a smart-textile-based communication system for use 
by people with deafblindness. Focusing on the communica-
tion module here, a new ontology connects computer vision 
with the user to label detected semantic content about ob-
jects, persons and situations for navigation and situational 
awareness. Such labelled content is then translated to a hap-
togram vocabulary with static vs. dynamic patterns, which 
are mapped to the body.  A haptogram denotes a tactile 
symbol composed over a touchscreen, its dynamic nature 
referring to the act of writing or drawing. A vest made of 
smart textile, in the current variant equipped with a 4 x 4 
grid of vibrotactile actuators, was used to transmit hapto-
grams on the user’s back. Thereby system messages of dif-
ferent complexity -- both alerts and short sentences -- can be 
received by the user, who then has the option to respond by 
pre-coded questions and messages.  
The core target groups are users with deafblindness with 
a diverse range of communication skills, capabilities and 
needs. The design of our solution is therefore intended to 
cater for this diversity through personalization, both in terms 
of the complexity and number of haptograms, and in terms 
of fit and comfort. The current actuator arrangement made 
the transmission of 104 dynamic signs possible, including 
homonyms. This is in the range of the simplest available 
SHC resource. At the same time, over a 4 x 4 grid, no digital 
numbers or an alphabet could be specified, and screen reso-
lution was too low to emulate genuine SHC signs.  
By means of grids with more actuators, displays with 
higher resolution can be implemented and tested, paving the 
way for an extended haptogram vocabulary which can cover 
more detailed ontology content and, possibly, bridge lan-
guage barriers among this demographic. Further, we plan to 
add a mobile sender unit to the prototype to enable de facto 
two-way communication, and aim to test multi-display mes-
sage transfer by replacing the smart vest by a smart garment. 
Psychophysical experiments to evaluate the prototype are in 
place and in progress. 
ACKNOWLEDGMENTS 
The authors are grateful to Sarah Woodin (University of 
Leeds) and Astrid Kappers (Eindhoven University of Tech-
nology) for helpful suggestions. This paper benefitted from 
generous advice by four unknown reviewers. This work has 
been partially funded by the European Union’s Horizon 2020 
research and innovation programme under grant agreement 
No 780814 SUITCEYES. 
 
Figure 9. In-principle arrangement of 4 × 4 actuators on the back of the 
smart vest. Actuators are to be placed beneath the red markers, towards the 
skin. The non-flat geometry of the human body that any haptic device has 
to handle is clearly visible. 
REFERENCES 
[1] S. Darányi, N. Olson, M. Riga, E. Kontopoulos, and I. 
Kompatsiaris, 
“Static 
and 
dynamic 
haptograms 
to 
communicate semantic content: towards enabling face-to-face 
communication for people with deafblindness,” paper 
presented at the “SyMpATHY: SeMAntic Technologies for 
Healthcare and Accessibility Applications” Workshop at 
SEMAPRO-19. Porto, 24 September 2019. 
[2] A. Israr and I. Poupyrev, “Tactile Brush: Drawing on Skin 
with a Tactile Grid Display,” Proc. SIGCHI Conference on 
Human Factors in Computing Systems, ACM, pp. 2019-2028, 
2011. 
[3] R. M. Lahtinen, Haptices and Haptemes: A Case Study of 
Developmental Process in Social-Haptic Communication of 
Acquired Deafblind People. PhD dissertation, University of 
Helsinki, 2008. 
[4] R. M. Lahtinen, R. Palmer, and M. Lahtinen, Environmental 
Description: For Visually and Dual Sensory Impaired People. 
A1 Management UK, 2010. 
[5] J. Chen, R. Turcott, P. Castillo, W. Setiawan, F. Lau, et al., 
“Learning to Feel Words: A Comparison of Learning 
Approaches to Acquire Haptic Words,” Proc. 15th ACM 
Symposium on Applied Perception, ACM, pp. 11-18, 2018. 
[6] C.M. Reed, H.Z. Tan, Z.D. Perez, E.C. Wilson, F.M. 
Severgnini, et al., “A phonemic-based tactile display for 
speech 
communication,” 
IEEE 
Transactions 
on 
Haptics, 12(1), pp. 2-17, 2018. 
[7] M. Walker and K. B. Reed, “Tactile Morse code using 
locational stimulus identification,” IEEE Transactions on 
Haptics, 11(1), pp. 151-155, 2017. 
[8] L.A. Jones and K. Ray, “Localization and Pattern Recognition 
with Tactile Displays”, in Proc. IEEE Symposium on Haptic 
138
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Interfaces for Virtual Environments and Teleoperator 
Systems, 13-14 March 2008, Reno, Nevada, USA, pp. 33-39. 
[9] L.A. Jones, J. Kunkel, and E. Piateski, “Vibrotactile pattern 
recognition on the arm and back,” Perception, vol. 38, pp. 52-
68, 2009. 
[10] T. L. McDaniel, Somatic ABC’s: A Theoretical Framework 
for Designing, Developing and Evaluating the Building 
Blocks 
of 
Touch-based 
Information 
Delivery. 
PhD 
dissertation, Arizona State University, 2012. 
[11] S. Kursun-Bahadir, F. Kalaoglu, and V. Koncar, “Analysis of 
vibrotactile perception via e-textile structure using fuzzy 
logic.” Fibres & Textiles in Eastern Europe, pp. 91-97, 2012. 
[12] P. Lemmens, F. Crompvoets, and J. van den Eerenbeemd, 
“Design of a tactile jacket with 64 actuators,” Guidelines for 
Haptic Lo-Fi Prototyping, pp. 21-22, 2004. 
[13] R. W. Lindeman, R. Y. Yanagida, and J. L. Sibert, “Towards 
Full-body Haptic Feedback: The Design and Deployment of a 
Spatialized Vibrotactile Feedback System,” in Proc. of the 
ACM Symposium on Virtual Reality Software and 
Technology (VRST ’04). Association for Computing 
Machinery, New York, NY, USA, pp. 146–149, 2004, 
doi:10.1145/1077534.1077562. 
[14] F. Gemperle, N. Ota, and D. Siewiorek, "Design of a 
Wearable 
Tactile 
Display," 
Proc. 
Fifth 
International 
Symposium on Wearable Computers, Zurich, Switzerland, pp. 
5-12, 2001, doi:10.1109/ISWC.2001.962082. 
[15] U. Yang, Y. Jang, and G.J. Kim, “Designing a Vibro-Tactile 
Wear for Close Range Interaction for VR-based Motion 
Training,” in Proc. International Conference on Artificial 
Reality and Telexistence, pp. 4-9, 2002. 
[16] L. Gasparri and D. Marconi, "Word Meaning", The Stanford 
Encyclopedia of Philosophy (Fall 2019 Edition), Edward N. 
Zalta (ed.). 
Available 
from: 
https://plato.stanford.edu/archives/fall2019/entries/word-
meaning/ 2020.06.03. 
[17] A. Wierzbicka, Semantics. Primes and Universals. New York, 
Oxford University Press, 1996. 
[18] A. Wierzbicka, Understanding Cultures Through Their Key 
Words. English, Russian, Polish, German, and Japanese. New 
York, Oxford University Press, 1997. 
[19] J. A. Fodor, The Language of Thought. Cambridge, Mass, 
Harvard University Press, 1975. 
[20] N.S. Trubetzkoy, Grundzüge der Phonologie (Travaux du 
Cercle Linguistique de Prague, 7). Prague, 1939. 
[21] Z. Harris, “Distributional structure,” in Papers in Structural 
and Transformational Linguistics, pp. 775-794, New York, 
Humanities Press, 1970. 
[22] J.R. Firth, “A synopsis of linguistic theory 1930-1955,” 
Studies in Linguistic Analysis, pp. 1-32, 1957. [Reprinted in 
F.R. Palmer, Ed. Selected Papers of J.R. Firth 1952-1959. 
London, Longman, 1968]. 
[23] L. 
Wittgenstein, 
Philosophical 
Investigations. 
Oxford, 
Blackwell, 1967. 
[24] R. Carnap, The Logical Syntax of Language. New York, 
Humanities Press, 1937. 
[25] S. Zucchi,  Formal Semantics of Sign Languages. Language 
and Linguistics Compass 6/11, pp. 719-734, 2012. 
[26] S. Ojala, Towards an Integrative Information Society: Studies 
on Individuality in Speech and Sign. TUCS Dissertations No 
135, Turku, Turku Centre for Computer Science, 2011. 
[27] N. Caporusso, Issues, Challenges and Practices in Advancing 
Pervasive Human-Computer Interaction for People with 
Combined Hearing and Vision Impairments. PhD dissertation. 
IMT Institute for Advanced Studies, Lucca, 2012. 
[28] T. Edwards, Language Emergence in the Seattle Deafblind 
Community. PhD dissertation, Berkeley University of 
California, 2014. 
[29]  G. Meditskos and I. Kompatsiaris, “iKnow: Ontology-driven 
situational awareness for the recognition of activities of daily 
living,” Pervasive and Mobile Computing, vol. 40, pp. 17-41,  
2017. 
[30] G. Meditskos, E. Kontopoulos, and I. Kompatsiaris, 
“Knowledge-driven Activity Recognition and Segmentation 
Using Context Connections,” in Proc. International Semantic 
Web Conference, Springer, Cham, pp. 260-275, October 
2014. 
[31] K. Janowicz, A. Haller, S. J. Cox, D. Le Phuoc, and M. 
Lefrançois, “SOSA: A lightweight ontology for sensors, 
observations, samples, and actuators,” Journal of Web 
Semantics, Vol. 56, pp. 1-10, 2019. 
[32] D. Brickley and L. Miller, FOAF Vocabulary Specification 
0.99. Namespace Document. [Online]. Available from: 
http://xmlns. com/foaf/spec/ 2019.08.13. 
[33] M. Lefranois, Planned ETSI SAREF Extensions based on the 
W3C&OGC 
SOSA/SSN-compatible 
SEAS 
Ontology 
Patterns. Workshop on Semantic Interoperability and 
Standardization in the IoT, Amsterdam, Netherlands SIS-IoT, 
2017. 
[34] R. Falco, A. Gangemi, S. Peroni, D. Shotton, and F. Vitali, 
“Modelling OWL Ontologies with Graffoo,” Proc. European 
Semantic Web Conference, Springer, Cham, pp. 320-325, 
2014. 
[35] H. Knoblauch, Spin-SPARQL Syntax. Member Submission, 
W3C, 2011. 
[36] G. Korres and M. Eid, “Haptogram: Ultrasonic point-cloud 
tactile stimulation,” IEEE Access, Vol.4, pp. 7758-7769, 
2016. 
[37] D. Salomon, The Computer Graphics Manual. 1st ed., 
London, Springer, 2011, doi:10.1007/978-0-85729-886-7. 
[38] Danish Association of the Deafblind, 103 Haptic Signals – a 
reference book, 2012, ISBN 978-87-989299-4-9. Available 
from: 
http://wasli.org/wp-content/uploads/2013/07/103-
Haptic-Signals-English.pdf/ 2020.06.03.  
 
139
International Journal on Advances in Intelligent Systems, vol 13 no 1 & 2, year 2020, http://www.iariajournals.org/intelligent_systems/
2020, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

