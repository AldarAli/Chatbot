 
 
Uncertainty Quantification for Modeling and Simulation with Calibration 
 
Ma Zhibo 
Institute of Applied Physics and Computational 
Mathematics 
Beijing, China 
E-mail: mazhibo@iapcm.ac.cn 
Yu Ming 
Key Laboratory for Computational Physics, Institute of 
Applied Physics and Computational Mathematics 
Beijing, China 
E-mail: yu_ming@iapcm.ac.cn 
 
 
Abstract—Calibration improves the consistency between 
simulation results and test data of a system, but it doesn't 
mean that the epistemic uncertainty of Modeling and 
Simulation (M&S) for subsystem is reduced, so propagation 
analysis with many uncertain inputs often leads to an 
overvaluation of uncertainty. As new system-level test is 
unavailable, it is unpractical to quantify M&S uncertainty with 
comparison between simulation results and test data. Taking 
advantage of the fact that calibration reduces the epistemic 
uncertainty of system-level simulation, we propose a method 
for Uncertainty Quantification (UQ), in which the uncertainty 
from comparison with existing system-level test data and the 
propagated uncertainty induced by additional cognitive defect 
for new system are used rationally. An example with virtual 
tests is displayed in which the method is demonstrated and 
validated. 
Keywords-uncertainty quantification; modeling & simulation; 
calibration; verification & validation; reliability certification. 
I. 
 INTRODUCTION 
M&S needs to experience verification, validation and 
accreditation (VV&A) procedure to assess its credibility for 
intended use [1][2]. However, it is still difficult to detect and 
eliminate all drawbacks even if the verification and 
validation are adequately implemented. Additionally, owing 
to inevitable discretization errors, simulation results of 
complicated physical processes often have systematic errors. 
Calibration is then used to rectify errors and improve 
consistency between simulation results and test data. It is a 
long-standing case to predict the performance of a new 
engineering system with calibrated codes. When system-
level test could not be fulfilled for a new system, it would be 
a great challenge for engineering design or reliability 
certification to quantify the uncertainty of prediction offered 
by M&S [3]. 
In many cases, a new system is only a modified version 
of its prototype. Some system-level test data for the 
prototype generally exist and could be used for calibration 
and uncertainty quantification [4]. The differences between a 
new system and its prototype are mainly caused by redesign 
or by state shift arising from long period stockpile, which 
may bring on recertification or assessment in engineering. In 
the case that system-level test is forbidden, numerical 
simulations for the modified design parameters or the 
additional engineering factors, and uncertainty quantification 
of the simulation results are consequently the main 
approaches to supply information for the recertification and 
assessment. 
According to the concept of Verification and Validation 
(V&V), the parameter space of an engineering system and its 
environment may be divided into application domain and 
validation domain which corresponding to the new system 
and its prototype respectively. A complex system may be 
divided into an arbitrary number of progressively simpler 
hierarchy tiers [1]. Without system-level test of the new 
system, the uncertainty information of M&S in application 
domain may have two sources, one is obtained by 
extrapolation from the uncertainty in validation domain 
which is quantified by comparison between the simulation 
results and the existent system-level test data [4], the other is 
obtained by propagation of the M&S uncertainty from lower 
level tiers to system level tier [5]. 
The uncertainty quantification with single information 
source has been widely studied, such as the UQ method 
based on comparison and propagation. In a probability frame, 
Oberkampf and Roy offered a quantification method of 
M&S errors according to comparison between simulation 
results and the statistics of test data such as sample average 
and standard deviation [1]. Helton gave a discussion about 
sensitivity analysis and Monte Carlo sampling used for 
uncertainty propagation [6]. Liu et al. used non-intrusive 
polynomial chaos to quantify the propagation of parameter 
uncertainties in Jones-Wilkins-Lee equation-of-state (JWL-
EOS) for explosive in a detonation system [7]. With the 
assumption that new system-level test can not be 
implemented, Ma et al. put forward a method to extrapolate 
the uncertainties from validation domain to application 
domain [4][8]. Up to now, it is still a choke point for UQ of 
M&S that how to fuse two kinds of information that comes 
from comparison and propagation, respectively. 
Techniques of information fusion have become more and 
more important for reliability analysis as the data lack is just 
about a ubiquitous problem. Information from comparison 
and propagation are obtained from different cognitive 
approaches. It is necessary in engineering and rational in 
science to fuse them. 
The uncertainties of M&S are mainly epistemic and are 
suitably represented or fused with interval theory. The UQ 
method should obey two basic principles, the unknown true 
value should be covered by the estimated uncertainty interval 
and the estimation of the uncertainty should be minimized 
7
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-419-0
ADVCOMP 2015 : The Ninth International Conference on Advanced Engineering Computing and Applications in Sciences

 
 
based on the available information [4]. If the estimation is 
only based on the extrapolated uncertainty originated by 
comparison, the additional cognitive defect of M&S for a 
new system may be neglected, and the true-value-covered 
principle may be violated on account of underestimation of 
the uncertainty. As a result, the risk to accept an unreliable 
system may be augmented. If the estimation is based on 
direct summation of the uncertainties from comparison and 
propagation, the estimated uncertainty may be irrationally 
magnified which may lead to violating the uncertainty-
minimized principle and consequently increase the risk to 
reject a reliable system.   
The paper is organized as follows. In Section 2,  
properties of M&S experienced calibration are analyzed. 
Section 3 offers a quantification method of total uncertainty 
of M&S based on information fusion, in which the basic 
component is an extrapolated uncertainty from comparison 
on system level, and an incremental component is the 
propagated uncertainty related to new system. Section 4 
gives a comparison-based UQ methods needed in Section 3. 
An example to show these methods is displayed in Section 5 
with a shock problem. Finally, we give a conclusion in 
Section 6. 
II. 
PROPERTIES OF CALIBRATED M&S 
There are two approaches to improve consistency 
between simulation results and test data. One is to enhance 
the cognitive ability by which the epistemic uncertainties in 
M&S are reduced. This is also an ideal approach for M&S 
development. The other is based on existent cognitive ability, 
to make artificially errors produced in M&S compensated 
with each other. Calibration of M&S depends mostly on the 
mechanism of error compensations. However, when M&S is 
used as prediction, the calibration only works well as the 
modification of the new system is not very large compared to 
the systems on which the calibration is made. 
In this paper, the mathematical model is divided into an 
entity model and a physics model. The former represents the 
specific engineering system depicted by design parameters, 
such as material type, shape, size, mass, and initial or 
boundary conditions when the system works. The latter 
represents the abstract laws of hylic world, such as equations 
of state and constitution, turbulence model, detonation model, 
the universal conservation equations of mass, momentum, 
and energy etc. The uncertainties of physics models are 
usually greater than that of entity models, as dynamic 
measuring and converse reckon are generally involved to 
determine the parameters and forms of physics models. 
Calibration is achieved by comparison with test data, in 
which the forms and parameters of models, methods and 
parameters of computation, knobs, and computer code are 
adjusted and then fixed. Knobs here are referred to the ad 
hoc parameters added to a model to simply obtain agreement 
with test data but lack definite physics significances or lack 
actual evaluating information [2]. Via sufficient verification 
and validation, knobs could be reduced but it is difficult to 
eliminate absolutely due to the existence of discretization 
errors and the deficiency in modeling and simulation for 
complex systems [9]. 
Generally, calibration is executed based on a range of 
entity models, to which we call calibration domain in the 
model space. Validation activities executed after calibration 
also have their validation domains, in which the M&S 
uncertainties may be quantified according to test data. After 
calibration is finished, the computer code and the parameters 
that need adjustment should be fixed for intended use. The 
fixation is usually relative and periodical, as the evaluation 
on parameters in physics model probably depend on methods 
and parameters of numerical computation under the 
expectation of good agreement with test data. The version of 
code and the parameters of physics models may vary with 
the development of M&S. 
Comparison and propagation are two basic approaches to 
gain information of uncertainties and, from the point of 
methodology on cognition, they are pertaining to induction 
and deduction, respectively. As the former is based on 
practice and observation to apperceive the realities,  
information obtained from comparison is generally with an 
inherent credibility than that from propagation and it may 
dominate in information fusions when conflict occurs 
between them. 
The characters in numerical simulations with calibration 
are summarized as follows: 
 
Uncertainties on system level can be effectively 
reduced by calibration. However, as the entity model 
departing from calibration domain, the error 
compensation may be gradually fading away and the 
simulation results for a new system may have lager 
deviations from the true values; 
 
Uncertainties obtained by comparison in validation 
domain could be extrapolated into application 
domain, while the extrapolated uncertainties do not 
include the uncertainties that introduced by 
additional cognitive defect of the M&S for a new 
system in application domain; 
 
Calibration can not reduce certainly the M&S 
uncertainties under system level, so the traditional 
propagation gives usually an overestimation of the 
M&S uncertainties on system level; 
 
The epistemic M&S uncertainties that come from 
comparison in validation domain should have a 
dominate weight than that come from propagation 
when information fusion is implemented; 
 
Without system-level test of new system, there are 
two independent information sources of M&S 
uncertainties for system level. One is that from the 
comparison in validation domain and the other is the 
additional uncertainty propagated from under system 
levels which induced by extra cognitive defects that 
M&S encounters. They can be fused based on 
interval theory and their additive property.  
III. 
UQ OF M&S WITH CALIBRATION 
The most important problem is to fuse information from 
comparison and propagation and to keep the UQ method 
observe the true-value-covered and uncertainty-minimized 
principles [4]. 
8
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-419-0
ADVCOMP 2015 : The Ninth International Conference on Advanced Engineering Computing and Applications in Sciences

 
 
It is known that both aleatory and epistemic  uncertainties 
can be quantified by test, but only the epistemic uncertainties 
can be reduced by test data. For nondeterministic M&S, the 
aleatory uncertainties that come from comparison and 
propagation respectively could be depicted by probability 
and be fused by Bayesian theory with the weight relative to 
their information quantities [10][11]. As the deterministic 
M&S only produces epistemic uncertainties, uncertainties 
from propagation should only be distributed little or zero 
weight in fusions when uncertainties from comparison exist. 
In the case that system-level test of a new system is 
unavailable and no direct information from comparison 
could be used, we consider the following schemes for 
uncertainty quantification:  
 
Only 
use 
the 
extrapolated 
uncertainty 
from 
comparison in validation domain; 
 
Only use the uncertainty from propagation; 
 
Use both of above information.  
The first scheme may leave out the additional 
uncertainties induced by the extra cognitive defects for new 
systems. The second scheme does not make use of the 
existing comparison to reduce the epistemic uncertainty and 
the uncertainty induced by propagation method and 
numerical computations are difficult to be quantified into a 
total M&S-uncertainty on system level. The third scheme has 
the most reasonable idea, but needs a proper design to avoid 
the disadvantages appearing in the former two schemes. 
Based on the third scheme, we disassemble the 
uncertainty from propagation (
n
propagatio
Uapplication
) to two parts. One 
refers to the propagated uncertainty for the systems in 
validation domain (
propagation
Uvalidation
). The other refers to an 
additional propagated uncertainty induced by an extra 
cognitive defects of new systems in application domain 
(
propagation
U 
). They have an approximate relationship 
propagation
propagation
validation
n
propagatio
application
U
U
U



 .                (1) 
According to the three sources of M&S uncertainty, we 
have 










3
1
3
1
i
propagation
i
propagation
validation
i
i
n
propagatio
n
applicatio
i
n
propagatio
application
U
U
U
U
,     (2) 
where 
1U propagation
,
2U propagation
,
3U propagation
 represent the 
propagated uncertainties on system level that born from 
entity 
modeling, 
physics 
modeling 
and 
numerical 
computation, respectively. 
    In validation domain, as the information from 
comparison has an overwhelming weight than that from 
propagation, we neglect the contribution of 
propagation
Uvalidation
 for 
information fusion. In application domain, the additional 
propagated uncertainty 
propagation
U 
 has no corresponding 
uncertainty from comparison, so 
propagation
U 
 must be counted 
wholly in the total M&S uncertainty.  
Thus, the M&S uncertainty for a new system is 
composed of two components and they have an additive 
property, which is represented as 
       
propagation
ion
extrapolat
n
applicatio
S
M
application
U
U
U



&
                 (3) 
So, the steps of UQ for M&S may be arranged as follows: 
 
To calibrate the M&S with available test data and 
finish the fixation of concerned parameters and the 
computer code;  
 
To quantify M&S uncertainty based on comparison 
with test data in validation domain; 
 
To extrapolate uncertainties in validation domain to 
obtain M&S uncertainty (
ion
extrapolat
Uapplication
) for the new 
system 
based 
on 
the 
relationship 
between 
uncertainties and parameters of the entity model; 
 
To quantify the additional propagated M&S 
uncertainty (
propagation
U 
) for the new system; 
 
To obtain the total M&S uncertainties of the new 
system by (3).  
IV. 
UQ BASED ON COMPARISON 
 Test data may have aleatory uncertainties owing to the 
randomness in manufacture of physical models and in test 
measure. As these uncertainties are essentially not induced 
by deterministic M&S, it is necessary for a comparison-
based UQ to build properly statistics to get rid of the impacts 
of the aleatory uncertainties on the quantification of 
epistemic uncertainties. 
    There are two cases when comparison is carried out:  
 
One simulation to one test (one-to-one); 
 
One simulation to many tests (one-to-many). 
 In the case of one-to-one, each physical model to 
undergo test must be measured and the results are used for 
M&S. The simulation results and the test data have one-to-
one relationship. The difference between them after test error 
is recouped reflect directly the error of M&S for this physical 
model. If the number of test or simulation is n , we have  
M&S uncertainty as: 
test
test
i
s
m
i
m s
U
y
MAX y
U
 


&
&
, 
n
i
2,1 

,     (4) 
where 
s
iym
&  and 
iytest
 are results of M&S and test data of 
physical model i , respectively. 
Utest
 is uncertainty of the 
test data. As n  is small   is advised to be evaluated as 1
  
to evade the risk of underestimation for 
Um s
& . But as n  is 
great enough,   could be evaluated as 0  or 1
  to evade the 
risk of overestimation. 
The case of one-to-many corresponds to the repeated 
tests, in which the mathematical modeling is based just on 
one set of parameters that evaluated generally from the 
average values of design for the physical models, and only 
one set of M&S result is output. Although all physical 
models are manufactured according to a same design, their 
test results may be stochastic owing to the random of 
manufacture and test measurement.  
In order to screen the interference of aleatory uncertainty 
induced by these random factors, we suggest to dig out the 
M&S uncertainty from the difference between the M&S 
result and the average of the test data,  



n
i
test
i
test
y
n
y
1
1
,                            (5) 
9
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-419-0
ADVCOMP 2015 : The Ninth International Conference on Advanced Engineering Computing and Applications in Sciences

 
 


2
1
1
2
1
1











n
i
test
itest
y
y
n
s
,                (6) 
         


n
s
t
y
y
U
test
s
m
m s







/ ,2
1
&
&
.             (7) 
In the following example, we assign the confidence 
 =0.95, and 



t 1 / ,2
 is the quantile of 
 2
/
1
 
 for t -
distribution with the freedom of 
  n 1
. 
When n  is great enough and 
Utest
 is exactly estimated, 
the result from (4) and that from (5)-(7) should be accordant 
with each other. 
V. 
AN EXAMPLE 
A. Description of System 
The system is simply composed of a 1-dimensional shock 
problem as shown in Figure 1, in which the left part is a 
high-pressure gas and the right part is a high-density metal 
with initial parameters of pressure, density, inner energy and 
velocity 
1
1
1
1
,
,
,
e v
p 
 and 
2
2
2
2
,
,
,
e v
p

, respectively, 
where 
1v , 
2v , 
2e , 
2p  are zero. A left-marching expanding 
wave and a right-marching shock wave occur from the 
material interface. The metal is pushed by the high pressure 
of gas to move rightward. The response of interest for this 
system is the interface displacement D  toward right when 
time is at 10 s
 . 
B. Design of Tests 
 Tests are needed in the procedures of calibration and 
validation to prove the UQ method is valid or not. As the 
exact solution of system response 
*
D  exists, we use virtual 
tests to replace the real tests. It is needed to predefine a set of 
entity and physics models that express the real aging of the 
products with different stockpile time. In this paper, we call 
them as true aging models, which are only used to give 
virtual test data and are not necessary to be displayed in the 
context, as if in real tests the true parameters of the aging 
models are unknown. 
The way to do the virtual test (called briefly test in the 
following text) is as follows. At first, we evaluate design 
parameters of the physical models according to the true 
aging models. Then change the design parameters to be 
random by adding virtual random variables (to simulate the  
High Pressure Gas:
p1,1,e1,v1
Material
Interface
at 10.0s
High Density Metal:
p2,2,e2,v2
Material
Interface
at 0.0s
D
 
Figure 1.  Entity model of the system 
random process of manufacture). The exact solution of each 
physical model is obtained based on the sampling value of 
the randomized model parameters. Finally, we get results 
that regarded as real test data from the exact solution plus a 
sampling value of another random variable that added to the 
exact solution (to simulate the random process of real test). 
The parameters that need to add a virtual random variable 
are the design values 
*
1
 , 
*
2
 , 
*
1e , 
*
1
, 
*
 2
 and the exact 
solution of system response 
*
D . Their virtual random 
variables 
1~
, 
2~
, 
1~e , 
1~ , 
2~
 and D~  are supposed to 
follow normal distributions with zero-means and deviations 
of 4.5kg/m3, 36.0kg/m3, 0.2MJ/kg, 0.01, 0.2 and 0.1mm,  
respectively. The true parameters (unknowns in real tests) of 
physical 
models 
are 
formed 
as 
1
*
1
1
~



test 
, 
2
*
2
2
~



test 
, 
1
*
1
1
~e
e
etest


, 
1
*
1
1
~



test 
, 
2
*
2
2
~



test 
. And the true test data are formed as 
D
D
Dtest
*  ~

. 
C. Calibration 
In this system, two types of parameters are calibrated, 
namely numerical parameters and physics parameters. The 
numerical methods do not need to be calibrated as the 
physical process is not complicated. Like the sequence of 
V&V, numerical parameters should be calibrated before 
physics parameters. Calibration on numerical parameters is 
just based on the test data of fresh products considering the 
adequacy of test data and the least disturbance of aging 
models. And calibration on physics parameters is based on 
the test data of aged products. 
The numerical parameters to be calibrated are artificial 
viscosity coefficients corresponding to the selected steps of 
space and time. The physics parameters to be calibrated are 
from aging models for 
1e  and 
2
  of aged materials. All the 
calibrated numerical and physics parameters form a fixed 
association in M&S for intended use. 
Calibration could be executed through following steps: 
 
Based on the demand analysis of M&S, determine 
numerical 
methods, 
numerical 
and 
physics 
parameters or knobs need to be calibrated, and the 
approach to get the reference solution for M&S 
(Here the reference solutions are test data); 
 
Choose fresh products to be tested and give their 
design 
values 
of 
physics 
parameters 
as 
3
*
1
/
2500 0.
kg m
 
,
MJ kg
e
/
0.6
*
1 
,
0.3
*
1 
,   
3
*
2
/
20000 0.
kg m
 
, 
0.5
*
 2 
. Obtain 
test
1
, 
test
e1
, 
test
1
,
test
2
,  
test
 2
and the corresponding test 
data 
Dtest
 for five physical models by plus the 
sampling values of their virtual random variables 
and the design values or exact solutions; 
 
Obtain 
the 
optimally 
calibrated 
numerical 
parameters 
through 
comparison 
between 
one 
numerical 
result 
Dm s
&
 and 
five 
test 
data 
Dtest
=3.856, 3.852, 3.841, 4.010, 3.757 mm, such as 
10
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-419-0
ADVCOMP 2015 : The Ninth International Conference on Advanced Engineering Computing and Applications in Sciences

 
 
the artificial viscosity coefficients 
 
1.5 and 
 
0.06 corresponding to the initial grid width 
1.0
x 
mm and time step 
s
t
  .0 0016
. The 
artificial viscosity model used is 
        












0
0
0
kk
kk
c
v
l
v
l
q









，
，
, 
       where q  is the viscous pressure, l  is the grid size, c  
is the sound speed and 
v
 is the divergence of 
velocity. The numerical result corresponding to these 
optimal values is 
Dm s
&  =3.891mm; 
 
Obtain the optimally calibrated physics parameters 
based on the aging model and the comparison 
between numerical results and test data about 
stockpile time of 10 years, 30 years and 50 years.           
The aging models describe the changing of physics 
parameters are 
 
 

2
1
1
1
1
0.1
0
b t
a t
e
e t



 ,              (8) 
 
 

2
2
2
2
2
0.1
0
b t
a t
t


 

 ,             (9) 
where the time t  is in “year” and the calibrated         
parameters are 




















10 .
0.3
,
10
5.1
10 ,
5.1
10 ,
5.0
5
2
4
2
5
1
4
1
b
a
b
a
                              (10) 
 
Finish calibration by fixing the calibrated numerical 
methods and parameters. 
D.  Validation 
In model space, the validation domain is defined as the 
stockpile time from 0 years to 50 years, in which the 
validation tests are for the stockpile time of 0 year, 10 years, 
20 years, 30 years, 40 years, 50 years. For each stockpile 
time there are five repeated tests but only one numerical 
result. 
By (7), in which  =0.95, and 

.0 025,4
/ ,2
1-
t
t
 

=2.7764, 
uncertainty in validation domain are quantified as 
U t
=0.142, 0.128, 0.283, 0.152, 0.202, 0.257 mm for 
stockpile time t =0, 10, 20, 30, 40, 50 years. 
E. Uncertainty Quantification in Application Domain 
In model space, the application domain is defined as the 
stockpile time great than 50 years. There is no system-level 
test in this domain. 
In order to quantify the first item in the right hand of (3), 
we have to determine the function showing uncertainty 
varies with the stockpile time. Here, a second order 
polynomial is used   
 
2
2
1
0
a t
a t
a
U t



.                (11) 
Based on uncertainties in validation domain, we have 





















































.0 257
.0 202
.0 152
.0 283
.0 128
142
.0
2500
50
1
1600
40
1
900
30
1
400
20
1
100
10
1
0
0
1
2
1
0
a
a
a
.         (12) 
The minimum-norm solution of this over-determined 
equation is 

 

-5
2
1
0
-1.38 10
0.0026,
0.14,
,
,


a
a
a
 
With this solution, we get the extrapolated M&S uncertainty 
ion 
extrapolat
U80 year
0.261mm for 80 years stockpile by (11). It is 
just the value of 
ion
extrapolat
Uapplication
 in (3). 
Uncertainties of physics parameters for different 
stockpile time are listed in Table 1, in which the aleatory 
uncertainties are induced from manufacture and the 
epistemic uncertainties are induced from the cognitive defect 
of the statistical population average of physics parameters. 
The additional epistemic uncertainty of 80 years stockpile 
comparing to 0~50 years stockpile is 0.2 MJ/kg for U
e1
 and 
0.2 for U
2
. 
The sensitivities of system response D  to each physics 
parameter are in Table 2.  
For system of 80 years stockpile, the uncertainty 
propagated from the additional uncertainties of physics 
parameters is 
 









.
120
.0
5.0
7.0
.0 103
6.0
8.0
499
.0
/
/
2
1
2
1
mm
U
D
U
e
D
U
e
n
propagatio









 


 



 
The uncertainty of 80 years stockpile quantified by (3) is: 

.
.0 381
80
&
80
mm
U
U
U
propagation
ion
extrapolat
year
S
M
year




   (13) 
If all the epistemic uncertainties are propagated, we get 
the uncertainty that comes from propagation as 



mm
U
x
D
i
x
i
i
496
.0
5
1


 


 . 
Moreover, if the aleatory uncertainties are also 
propagated, the uncertainty from propagation will reach 
0.928mm. However, it couldn’t be regarded as total M&S 
uncertainty. From here, we see that the method depicted by 
(3)-(7) can reduce epistemic uncertainties through calibration 
and filter the aleatory uncertainties by properly defined 
statistics. 
TABLE I.  
UNCERTAINTIES OF PHYSICS PARAMETERS  
Uncertainty type 
Aleatory 
Epistemic 
Stockpile (Year) 
0~50 
80 
0~50 
80 
1U
（kg/m3） 
13.50 
5.00 
2U
（kg/m3） 
108.00 
40.00 
e1U
（MJ/kg） 
0.60 
0.60 
0.80 
1U
（1） 
0.03 
0.01 
2U
（1） 
0.60 
0.50 
0.70 
11
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-419-0
ADVCOMP 2015 : The Ninth International Conference on Advanced Engineering Computing and Applications in Sciences

 
 
TABLE II.  
SENSITIVITY OF D TO PHYSICS PARAMETERS 
/ 1
D 
 






/m3
kg
mm
 
/ 2
D 
 






/m3
kg
mm
 
/ e1
D 
 






kg
MJ
mm
/
 
/ 1
D 
 
（mm） 
/  2

D
 
（mm） 
1.085×10-3 
-1.356×10-4 
0.499 
1.400 
-0.103 
F. Validity of the UQ Method 
In practice, the system-level test is not available in 
application domain. We have specially arranged five new 
tests of 80 years stockpile here aims to prove the UQ method 
is valid. This procedure starts with uncertainty assessment by 
comparison the numerical results and test data as specified in 
(7). Then compare the assessment result with the prediction 
result in (13). If the latter is greater than the former just in a 
little amount, it may offer a positive evidence of validity for 
the UQ method in the sight of obeying the true-value-
covered and uncertainty-minimized principles. 
Implementation of the tests for 80 years stockpile is 
similar to the tests as narrated above, i.e., the values of 
physics parameters in physical models are obtained by the 
true aging models and sampling, the test data are obtained by 
exact solutions and sampling, the physics parameters for 
M&S are from (10) and their deviations from the true aging 
models agree with the uncertainty in Table 2. 
The five test data 
Dtest
=3.455, 3.338, 3.216, 3.395, 
3.174 mm and the numerical result is 
Dm s
&  =3.499mm, in 
which the parameters for M&S are 
1
=2500kg/m3, 
1e
=5.4MJ/kg, 
 1
=3.0, 
2
=20000kg/m3, 
 2
=6.02. The 
assessed M&S uncertainty 
S
M
year
U
&
80
 =0.331mm. 
The result in (13) shows that the uncertainty (0.381mm) 
obtained by prediction with (3) is slightly greater than the 
uncertainty (0.331mm) obtained by assessment, from which 
the success of the UQ methods is exhibited. 
VI. 
CONCLUSION 
When system-level test is unavailable, the prediction by 
M&S and its uncertainty are the most important information 
for reliability certification or assessment, and propagation is 
the most imaginable UQ method. As the system becoming 
complicated and its hierarchy having more multiple tiers, the 
numerical errors and the great number of uncertain input 
factors will make it impractical to quantify the M&S 
uncertainty just by propagation. Based on the reality that the 
prototype of a new system generally has some test data and 
the awareness that the epistemic uncertainty of M&S could 
be reduced by calibration with existed test data, an UQ 
method is put forward to synthesize the uncertainties in 
validation 
domain 
and 
the 
propagated 
additional 
uncertainties. With an example the method is shown to 
observe the true-value-covered and uncertainty-minimized 
principles of UQ for M&S that is used as prediction.  
 
ACKNOWLEDGMENT 
This research is supported by the National Nature 
Science Foundation (Grant No. 11371066, 11272064) and 
CAEP (Grant No. 2012B0102010, 2013A0101004) of China. 
 
REFERENCES 
[1] W. L. Oberkampf and C. J. Roy, "Verification and validation 
in science computing," Cambridge University Press, 2010. 
[2] C. J. Roy and W. L. Oberkampf, "A comprehensive 
framework for verification, validation, and uncertainty 
quantification in scientific computing," Computer Methods in 
Applied Mechanics and Engineering, vol. 200, no. 25-28, 
2011, pp. 2131-2144. 
[3] Z. B. Ma, Y. J. Ying and J. S. Zhu, "QMU certifying method 
and its implementation," Chinese Journal of Nuclear Science 
and Engineering, vol. 29, no. 1, 2009, pp. 1-9. 
[4] Z. B. Ma, H. J. Li, J. W. Yin and W. B. Huang, "Uncertainty 
Quantification of Numerical Simulation for Reliability 
Analysis," Chinese Journal of Computational Physics，2014, 
31(4): pp. 424-430. 
[5] W. Chen, L. Baghdasaryan, T. Buranathiti and J. Cao, "Model 
validation via uncertainty propagation," AIAA Journal, vol. 
42, no. 7, 2004, pp. 1406-1415. 
[6] J. C. Helton, "Conceptual and Computational Basis for the 
Quantification of Margins and Uncertaity," Sandia Report, 
SAND2009-3055. 
[7] Q. Liu, R. L. Wang, Z. Lin and W. Z. Wen, "Uncertainty 
quantification for JWL EOS parameters in explosive 
numerical simulation," Chinese Journal of Explosion and 
Shock Waves, vol. 33, no. 6, 2013, pp. 647-654. 
[8] Z. B. Ma, M. Zheng and J. W. Yin, "Quantification of 
uncertainties in detonation simulations," Chinese Journal of 
Computational Physics, vol. 28, no. 1, 2011, pp. 66-74. 
[9] D. Higdon and M. kennedy, "Combining field observations 
and simulations for calibration and prediction," SIAM Journal 
of Scientific Computing, vol. 26, 2004, pp. 448-466. 
[10] J. C. Helton, "Uncertainty and sensitivity analysis in the 
presence of stochastic and subjective uncertainty," Journal of 
Statistical Computation and Simulation, vol. 57, 1997, pp. 3-
76. 
[11] I. Babuska, F. Nobile and R. Tempone, "A systematic 
approach to model validation based on bayesian updates and 
prediction related rejection criteria," Computer Methods in 
Applied Mechanics and Engineering, vol. 197, no. 29-32, 
2008, pp. 2517-2539. 
 
12
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-419-0
ADVCOMP 2015 : The Ninth International Conference on Advanced Engineering Computing and Applications in Sciences

