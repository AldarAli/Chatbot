Self-monitoring Reinforcement Metalearning
for Energy Conservation in
Data-ferried Sensor Networks
Ben Pearre and Timothy X. Brown
University of Colorado, Boulder, CO, USA
{benjamin.pearre,timxb}@colorado.edu
Abstract—Given multiple widespread stationary data sources
such as ground-based sensors, an unmanned aircraft can ﬂy over
the sensors and retrieve their data via a wireless link. When
sensors have limited energy resources, they can reduce the energy
used in data transmission if the ferry aircraft is allowed to extend
its ﬂight time. Complex vehicle and communication dynamics and
imperfect knowledge of the environment confound planning since
accurate system models are difﬁcult to acquire and maintain, so
we present a reinforcement learning approach that allows the
ferry aircraft to optimise data collection trajectories and sensor
energy use in situ, obviating the need for system identiﬁcation. We
address a key problem of reinforcement learning—the high cost
of acquiring sufﬁcient experience—by introducing a metalearner
that transfers knowledge between tasks, thereby reducing the
number of ﬂights required and the frequency of signiﬁcantly
suboptimal ﬂights. The metalearner monitors the quality of its
own output in order to ensure that its recommendations are
used only when they are likely to be beneﬁcial. We ﬁnd that
allowing the ferry aircraft to double its range can reduce sensor
radio transmission energy by 60% or better, depending on the
accuracy of the aircraft’s information about sensor locations.
Keywords-Sensor networks; data ferries; energy optimisation;
reinforcement learning; metalearning
I. INTRODUCTION
We consider the problem of collecting data from widespread
energy-limited stationary data sources such as ground-based
sensors. Our approach uses a ﬁxed-wing unmanned aircraft
(UA) to ﬂy over the sensors and gather the data via a wireless
link [1]. We assume that the UA has a known range limit
and can be recharged/refuelled at a base station, and that the
sensors may continuously generate data over long periods,
so that the UA needs to ferry the data to a collection site
over repeated ﬂights. The goal is to trade energy used by the
UA against energy saved by the sensor nodes. The system is
difﬁcult to model, so the challenge is to develop a model-free
approach that can quickly learn to minimise the sensors’ radio
transmission energy subject to the UA’s range constraints.
The problem may be subdivided into the following pieces:
Aircraft trajectory optimisation seeks to discover a ﬂight path
over the sensor nodes (a so-called tour) that minimises some
mission cost. We decompose this piece as follows:
● Tour Design decides in what order to visit sensor nodes
of known location, or establishes a search pattern when
the locations are unknown.
● Trajectory Optimisation ﬁnds a sequence of waypoints
the UA should follow in order to visit the sensor nodes.
● Vehicle Control translates the waypoints into control
surface and engine commands.
Radio energy optimisation consists of the following:
● Radio Design chooses radio hardware and protocols to
support high-efﬁciency communication.
● Power Management varies the transmission power of
nodes’ radios during interaction with the ferry aircraft.
This paper focuses on the Aircraft Trajectory Optimisation
and Radio Power Management layers. We assume that the
tour is given and that the nodes’ locations are known only
approximately as when, for example, the sensors have been
deployed from an aircraft. Vehicle control to track a set of
waypoints requires an autopilot, whose behaviour is a complex
function of the waypoints, weather, aircraft dynamics, and the
control models within the autopilot. Similarly, communication
system performance is a complex function of the radio pro-
tocols, antenna patterns, noise, and interference. We assume
autopilot and communication systems are black boxes whose
speciﬁc functionality is unknown to the upper layers. Only
aggregate performance of the ferry system is reported to the
learner.
In [2], we examined model-free minimisation of UA tra-
jectory length. Here we extend the technique: since network
lifetime or maintenance costs may depend on the energy
reserves of the sensor nodes, we seek to minimise their
transmission energy cost per bit.
Data ferries can be highly effective for reducing radio
energy requirements. Jun et al. [3] compare ferry-assisted
networks with hopping networks in simulation and ﬁnds that
a ferry can reduce node energy consumption by up to 95%
(further gains would have been possible with a broader conﬁg-
uration space). Tekdas et al. [4] reach a similar conclusion on
a real toy network in which wheeled robots represent ferries.
Anastasi et al. [5] consider the total energy requirement per
message including overhead associated with turning a node’s
radio on in order to search for a ﬁxed-trajectory ferry. Ma
and Yang [6] optimise the lifetime of nodes by choosing
between multi-hop node-to-node routing and changing the
ferry’s route and speed. Optimal solutions under the trade-
off between energy use and latency have been examined for
296
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-207-3
SENSORCOMM 2012 : The Sixth International Conference on Sensor Technologies and Applications

a ﬁxed ferry trajectory [7], and adaptively with a known
radio model and simple ﬂight dynamics [8]. In [9], a node
learns whether to transmit to the ferry or wait depending on
the anticipated trajectory; range affects transmission power.
Anastasi et al. [10] review techniques for energy minimisation
for both ferried and general sensor networks.
Past work on ferried networks has made various assump-
tions about ferry and communication system dynamics, which
we broadly categorise as follows. In Visit models, the ferry
exchanges all data upon visiting a node [11]–[14]. Communi-
cation radius models exchange all data instantaneously below
a certain range [4], [6], [15]–[18]. A learning variant to this
model is found in [19]: planning assumes a communication
radius, but the ferry also exchanges data opportunistically,
possibly allowing improvements to the trajectory. Variable rate
models base rate on communication range [20], [21]. Stachura
et al. [22] assume probabilistic packet loss based on distance,
achieving the same effect. Mobile nodes are treated in [23],
which uses a model of a UA equipped with a beamforming
antenna to plan trajectories that maximise the signal-to-noise
ratio (SNR) to each node.
Models are necessarily approximate, but inaccuracies can
lead to poor performance in the ﬁeld. The difﬁculty of
generating and updating sufﬁciently accurate models under
possibly changing conditions motivates our question: can a
sensor network system simultaneously learn to optimise both
the aircraft’s ﬂight path and the sensors’ radio policies, in
a reasonable time, directly on a radio ﬁeld? The goal is
to provide a UA with approximate information about the
geometry of a sensor network, and to have it improve its per-
formance rapidly and autonomously. We focus on minimising
the number of ﬂight tours required to get a good solution,
since computational costs are comparatively minor.
Our contributions are as follows:
● We demonstrate the feasibility of a reinforcement learning
approach for rapid discovery of energy-saving network
policies that trade UA ﬂight time for sensor energy. The
policies are learned without a system model and despite
potentially inaccurate sensor node locations, unknown
radio antenna patterns, and ignorance of the internals of
the autopilot.
● We introduce a reinforcement metalearner that learns to
speed up and stabilise the performance of the learner, and
transfers acquired knowledge about the policy optimisa-
tion process to unseen problems.
● A poorly trained metalearner may degrade the learner’s
performance, and should not be allowed to inﬂuence
learning. We introduce a method of monitoring the per-
formance of the metalearner without allowing it to affect
network behaviour before it has been trained sufﬁciently.
● We show our two independent optimisers—waypoint lo-
cation and transmission power policy—can operate simul-
taneously on the same sampling ﬂights.
Our learning framework quickly produces trajectories that
exploit the limits of ferry endurance. For example, when the
allowable ﬂight length is twice that of a handcoded reference
trajectory and accurate sensor location information is available,
the system learns to reduce sensor communication energy
by roughly 60% after a few dozen ﬂights, and when the
sensor location information is approximate, the learners do
even better. More important than this speciﬁc result is the
development of a general technique that allows data-ferry
networks to efﬁciently optimise their performance in situ.
Section II describes our radio model. Section III describes
how our simulated autopilot control policies interpret way-
points. Section IV reviews the learning algorithm we use and
describes how we apply it to our sensor energy optimisation
problem. In Section V, we develop our metalearner. Section VI
presents our results. Section VII concludes.
II. RADIO ENVIRONMENT
Our goal is to evaluate the use of model-free optimisation
in a complex, unknown radio environment. We introduce a
radio model that incorporates several complicating factors that
are rarely considered: variable-rate transmission, point noise
sources, and directional antennas. This model extends that
introduced in our previous work [2] only by giving the nodes
dipole antennas.
The signal to noise ratio at node a from node b is given by
SNRab =
P(a,b)
N + ∑i P(a,ni)
(1)
P(a,b) is the power received by node a from node b, N
is background noise from electronics and environment, and
ni are noise sources. The power between a and b is usually
computed as
P(a,b) =
P0,adϵ
0
∣Xa − Xb∣ϵ
(2)
for reference transmit power P0,a, reference distance d0 = 1,
distance between transmitter and receiver ∣Xa−Xb∣, and propa-
gation decay ϵ. However, antenna shape and radio interactions
with nearby objects make most antennas directional, so the ori-
entations of the antennas affect power. We model the aircraft’s
antenna as a short dipole with gain 1.5 = 1.76 dBi oriented
on the dorsal-ventral axis of the aircraft, yielding a toroidal
antenna pattern. We model the nodes’ ﬁelds similarly with
random ﬁxed orientations, so we adjust the power computation
in (2) to:
P ′(a,b) = sin2(ξab)sin2(ξba)P(a,b)
(3)
where ξxy is the angle between antenna x’s pole and the di-
rection to y. This depends on the vector between the UA’s and
node’s positions (∈ R3), the aircraft’s orientation (∈ R3), and
the transmitter’s orientation, although the latter is assumed not
to change. Here we consider only constant-altitude trajectories.
In order to evaluate (3), we require the UA’s position and
orientation. A full dynamical simulation of the aircraft is
unnecessarily complex for our purposes, so we assume that
course and heading φ are the same (yaw = 0), pitch = 0, and
roll ψ =
π
2 tanh2 9φ, which varies between 0 for a straight
course and ±54○ for our maximum turning rate of 9φ ≃ 0.347
rad/s (i.e. the UA ﬂies a complete circle in 20s).
297
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-207-3
SENSORCOMM 2012 : The Sixth International Conference on Sensor Technologies and Applications

We use the Shannon-Hartley law [24] to compute the data
transmission rate between transmitter a and receiver b:
Rab = β log2(1 + SNRab)
(4)
This assumes that data rate varies continuously. The hardware
may use discrete rates that are chosen according to current
SNR conditions, but [21] indicates that the difference in
trajectories and performance outcomes between continuously
variable and the discrete rates of 802.11g may be negligible
for this type of problem.
This model ignores many characteristics of a real radio
environment such as occlusion, reﬂection, higher-order di-
rectionality, and multipath propagation. Moreover, we do not
simulate obvious protocol modiﬁcations that would allow other
sensor nodes to cease transmission and thereby reduce interfer-
ence with the active node. However, in part due to the latter
omission, the model produces ﬁelds that have irregularities
similar to those that occur in real radio environments, and thus
it meets our aim of having a complex simulation environment
within which we can test whether the aircraft can learn in situ.
III. AUTOPILOTS
The aircraft is directed to follow some trajectory through
an autopilot control policy.
A. Reference autopilot control policy
The Reference autopilot is borrowed from [21] and does
not learn. It assumes that the aircraft has an estimate of the
locations of the sensor nodes (although these can be difﬁcult
to discover [25]). While communicating only with the target
node the aircraft ﬂies at constant speed v towards the tangent
of a circle of minimum turning radius about the node’s nominal
location, circles it at the maximum turning rate ω until D bytes
are received, and then proceeds to the next node. The result
is the Reference trajectory.
B. Learning autopilot control policy
The learning autopilot places a GPS waypoint at the as-
sumed location of each sensor node (we will assume that node
identities and approximate locations are known during tour
initialisation, although the assignment could instead occur on
the ﬂy as nodes are discovered). The aircraft collects data from
any node opportunistically while ﬂying towards the tangent of
a minimum-turning-radius circle about the waypoint, and then
if necessary circles the waypoint exchanging data only with
the assigned node until it has collected sufﬁcient data. The
true node location and the waypoint location may differ; ﬁnal
waypoint positions are learned as described next.
IV. LEARNING
Policy gradient reinforcement learning (PGRL) [26] consists
of a family of techniques for model-free optimisation of
control policies. Key elements are a policy gradient estimator,
a reward function, and a policy representation. We introduce
these by describing and applying the well-known “episodic
REINFORCE” estimator to our waypoint-placement problem,
and then introduce the more sophisticated radio transmission
power policy.
A. Policy Gradient Reinforcement Learning (PGRL)
A policy π(s,u;θ) = Pr(u∣s;θ) deﬁnes the probability of
choosing action u given state s under the policy parametrised
by θ ∈ Rn. The expected reward averaged over all states and
actions under a policy π(s,u;θ) is denoted J(π(s,u;θ)) or
abbreviated as J(θ). PGRL techniques are ways of estimating
the gradient of the expected reward: ̂gθ = ̂
∇θJ(θ).
Our task can be broken down into distinct “trials”, each
consisting of a complete execution of π(θ) over some bounded
time (e.g., the aircraft ﬂying a complete tour τ) and receiving
reward r at the end. During a trial, the policy deﬁnes a
probability distribution over the action chosen at any point.
Assume that the controller makes some ﬁnite number H of
decisions uk at times tk,k ∈ 1...H during a trial; discretising
time in this manner makes it possible to compute the prob-
ability of a trajectory under a policy as the product of the
probabilities of each (independent) decision at each time tk.
So Pr(τ∣θ) = ∏H
k=1 Pr(uk∣sk;θ).
To optimise θ, we estimate the gradient using stochastic
optimisation’s “likelihood-ratio trick” [27] or reinforcement
learning’s “episodic REINFORCE” (eR) [26], [28] with non-
discounted reward. Each element ∇θi of the gradient is esti-
mated as:
̂
gθi = ⟨(
H
∑
k=1
∇θi log Pr(uk∣sk;θ) − µ∑∇)(
H
∑
k=1
γtkrk − bi)⟩
(5)
where µ∑∇ is the mean over trials of the ∑∇θi terms, bi is
a “reward baseline” for element θi, computed as the inter-
trial weighted mean of rewards, using the per-trial weight
(∑H
k=0 ∇θi log Pr(uk∣sk;θ))
2 [26], and ⟨⋅⟩ is the average over
some number N of trajectories. The term µ∑∇ is the mean
over trials of the ∑∇θi terms; this term does not appear in
[26] but is included here in order to reduce the variance of
the “characteristic eligibility”; we found it to further improve
the gradient estimation process. We postpone discussion of the
temporal discount factor γ to §V-C; for now consider γ = 1.
Once a policy gradient estimate ̂gθ = ̂
∇θJ for episode e is
obtained, we take a step of some length α in that direction,
θe+1 = θe + α ̂gθ
∣ ̂gθ∣
(6)
thus creating a new policy. The gradient estimation and update
may be repeated until a design requirement is met, until
convergence to a local maximum, or forever to track a time-
varying environment. The theoretical guarantee of convergence
to a locally optimal policy is only available if α decreases
over time. That guarantee is useful, but does not directly
apply to learners operating in a changing environment and
is unnecessary for our tasks.
B. Reward
The reward function (or its additive inverse, the cost func-
tion) expresses the desiderata of solutions as a scalar quantity.
298
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-207-3
SENSORCOMM 2012 : The Sixth International Conference on Sensor Technologies and Applications

We seek the policy that leads to the lowest total transmission
power subject to a maximum desired tour length dmax, which
indicates that we are nearing the endurance limit of the aircraft.
We use the following:
r = −⎛
⎝max(0,d − dmax)ϱ +
∑
j∈nodes
ϕj
H
∑
k=1
Pjk∆t⎞
⎠
(7)
d is the current trajectory path length, dmax is the soft maxi-
mum range of the aircraft, ϱ controls the severity of the soft
maximum distance penalty, Pjk is the transmission power of
node j at timestep k of length ∆t, and ϕj is a weighting for
the value of energy for node j. Note that d is not penalised
until the aircraft exceeds dmax.
The aircraft’s autopilot may be programmed to return to
base by some hard length constraint dhardmax > dmax, and a cost
for underrunning data-collection objectives could be included
in (7). This produces similar results in our simulations but
complicates our explanation.
C. Waypoint placement
We consider a sequence of nodes that need to be visited in
some order {a,b1,...,bn,c} that was determined by a higher-
level planner [13], [17]. The aircraft must ﬂy a trajectory that
starts at a and ends at c and allows exchange of Dj bytes
of data with each of the n sensor nodes b1 to bn. Thus we
seek the path a → c that minimises (7). That sufﬁcient data
are collected from each node is guaranteed by the autopilot
policies (§III).
Trajectory policies for the autopilot are implemented
as sequences of constant-altitude waypoints. So for m
waypoints, the waypoint policy’s parameter vector θ
=
[x1 y1 x2 y2 ...xm ym]T . In order to be used by (5) the
controller adds noise such that Pr(τ∣θ) can be computed. In
a real system, actuator noise or autopilot error E can be used
for this purpose if ∇θ log Pr(u + E ∣ θ) can be computed, but
in our simulations we simply add Gaussian noise directly to
the waypoint locations at the beginning of each tour:
u
∼
N(θ,Σ)
(8)
∇θ log Pr(u∣s;θ)
=
1
2 (Σ−1 + Σ−1 T )(u − θ)
(9)
D. Radio transmission power
The data-ferrying approach allows sensors to communicate
with distal base stations without the need for high-powered
radios, but the energy that nodes spend in communicating with
the ferry is still non-negligible [3], [4]. Recall the data rate
from Section II:
Rab = β log2(1 + SNRab)
(10)
The derivative of
rate
power
∇P
β
P log2 (1 + P
N ) =
β
N P log(2) (1 + P
N ) − β log(1 + P
N )
P 2 log(2)
(11)
is negative whenever
P
P + N < log (P + N
N
)
(12)
which is true except at P = 0. So while reducing power
results in a lower energy cost per bit, lower transmission rates
require longer trajectories. Given an externally deﬁned trade-
off between ferry trajectory length and node energy savings,
when should a sensor transmit, and at what power?
The difﬁculty of predicting the SNR between transmitter
and aircraft again suggests reinforcement learning. We assume
that at each timestep a sensor can transmit with power P ∈
[0,Pmax], that it occasionally sends short probe packets at
P = Pmax, and that the aircraft’s radio can use this to measure
the current maximum SNR and provide instructions to the
node. These packets are too brief to transmit sensor data or
use much power, so we do not model them explicitly. Here,
too, the learning approach will silently optimise around such
quirks of real hardware.
The power policy is a learned function that controls the
power a node uses to transmit given a reported SNR, given
in dB. Our desired behaviour is to transmit at a target
power Ptarget ≤ Pmax whenever the probed SNR exceeds
some threshold RT . So, the policy has action u = P, state
s = SNRprobed, and is parametrised by θ = [Ptarget,RT ].
Reinforcement learning requires exploration noise, so at each
timestep the policy π draws the actual transmission power P
from a Gaussian (truncated on [0,Pmax]) whose mean is taken
from a sigmoid of height Ptarget:
π(s,u;θ)
=
Pr(u∣s;θ)
(13)
∼
N (
Ptarget
1 + eφ(RT −s) , σ), truncated on [0,Pmax]
When s = RT , the mean transmit power is 50% of Ptarget,
going to 100% as s increases above RT and vice versa,
thus implementing the desired behaviour with exploration.
φ controls the sigmoid’s width, and σ controls Gaussian
exploration. For example, when Ptarget = Pmax and RT and σ
are small, the policy mimics the full-power Reference policy.
The policy’s derivatives are:
∇θ log π(s,u;θ) =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
u−
Ptarget
1+eφ (RT −s)
σ2 (1+eφ (RT −s))
−
Ptarget φ eφ (RT −s) (u−
Ptarget
1+eφ (RT −s) )
σ2 (1+eφ (RT −s))
2
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦(14)
Unlike the waypoint-placement policy, this one is closed-loop:
sensing packets detect SNR, which informs the choice of
action u (transmission power) at each timestep. Thus we use
the full loop-closing capabilities of the episodic REINFORCE
algorithm of §IV-A. This policy and the waypoint-placement
one run in parallel, using the same ﬂights to estimate their
reward gradients.
As a concession to practicality, the power policy incorpo-
rates a failsafe mechanism: when the aircraft’s soft maximum
299
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-207-3
SENSORCOMM 2012 : The Sixth International Conference on Sensor Technologies and Applications

0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
−4500
−4000
−3500
−3000
−2500
−2000
−1500
−1000
P
Energy reward surface, single node
RT
reward
Fig. 1.
Energy reward landscape for an example single node, with ﬁxed
waypoint position. As transmission power P and threshold SNR RT change,
energy savings may lead to greater reward up to a certain point. But the high
cost of exceeding the aircraft’s range constraint creates a steep “cliff” in the
reward landscape.
range has been exceeded, radio power is set to 100%, ensuring
that the UA does not become stuck in nearly inﬁnite loops.
V. METALEARNING
Waypoint location optimisation is fairly straightforward [2].
However, our energy reward function (7) is highly nonlinear
with respect to the power policy parameters in the vicinity of
the optimal solution. Figure 1 shows a portion of the reward
landscape for trajectories looping a typical node. When RT is
small and P is near 1, the gradient is not difﬁcult to estimate—
exploration noise will generally average over the ridges and
valleys. However, in the vicinity of the optimal solution (the
crest of the hill), there is a steep cliff: if RT becomes too
high or P becomes too low and the aircraft must remain
near the sensor for a long time in order to collect all the
data, which activates the aggressive length-overrun term of
the reward function.
Conventional PGRL repeatedly estimates the reward gradi-
ent near the current policy and takes a hillclimbing step. Near
the optimum, hillclimbing updates can result in the learner
taking a step off the cliff, or “cliff-jumping”. Furthermore, the
cliff contains local regions in which a problematic reverse-
sloped ledge structure is apparent—it is possible for a local
gradient estimate to suggest a step further off the cliff. Con-
ﬁdence regions can mitigate this problem, but they generally
fail to re-use information acquired during past steps (but see
[29] for a counterexample). The problematic structure in the
reward landscape motivated the development of a technique
to encode knowledge of the process of optimising on reward
landscapes like ours.
Consider the intuition: when a trial leads to an overly long
trajectory, it is generally helpful to increase power or allow
transmission at lower SNR. Conversely, for a trajectory that
does not use the aircraft’s full range node energy can be
reduced by using lower power. The PGRL gradient estimation
ﬁnds policy updates that, on average, tend to obey these
heuristics, but the microstructure and the abrupt cliff near the
global optimum frequently lead to poor updates. Our goal in
developing a metalearner is to give the UA the ability to use
experience with past problems to improve learning speed and
robustness on new problems and automatically capture such
heuristics. We investigate the following questions:
● Can a metapolicy that encodes knowledge about optimis-
ing policies in this domain be learned through experience?
● Can such a metapolicy transfer knowledge between prob-
lems?
● Can we monitor the quality of the metapolicy’s recom-
mendations in order to prevent a poor metapolicy from
adversely affecting the optimisation process?
● Can the metapolicy be used to speed or stabilise the
learning of energy-saving policies for sensor networks?
A. Metapolicy
Our energy “metapolicy” examines each trajectory and
produces a guess as to the best update, ∆θ, to the base power
policy’s parameters θ = [Ptarget,RT ]. For each element θi of
θ, we use a simple neural network, a so-called single-layer
perceptron (see [30]) with one input—the fraction of allowed
aircraft range used—and two outputs—suggested changes to
the base policy’s two parameters:
sµ
=
d
dmax
(15)
uµ
=
∆θi
(16)
πµ(sµ,uµ;Θi)
=
Pr(uµ∣sµ;Θ)
(17)
=
N (tanh(Θ1,isµ + Θ2,i),
σµ)
∇Θi log π(sµ,uµ;Θ)
=
[ sµZ
Z
]
(18)
where Z
=
(19)
σ−2
µ (uµ − tanh(sµΘ1,i + Θ2,i))(1 − tanh(sµΘ1,i + Θ2,i)2)
If the perceptual space is enriched with other inputs or requires
a richer representation, other models can be used. Note,
however, that more complex models with more parameters
increases the number of runs necessary for learning a good
metapolicy.
B. Metareward
The metalearner’s objective is to learn a metapolicy that
takes as input a policy operating on a trajectory, and outputs
an “action” consisting of an improvement of the base policy’s
parameters. So as our metareward rµ we choose the reward
improvement between trials:
rµ = ri − ri−1
(20)
where ri is the base reward received on trajectory i.
300
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-207-3
SENSORCOMM 2012 : The Sixth International Conference on Sensor Technologies and Applications

C. Time-discounted credit
The metalearner receives µ-reward (20) after every µ-action,
and each µ-action also—to a lesser extent—affects future µ-
states and thus potential µ-rewards, so it would be appropriate
to use a time-discounted eligibility (γ < 1 in (5)). But further
improvements are to be gained by using a more sophisticated
gradient estimator, which we introduce here:
1) G(PO)MDP: (Here we drop the µ-preﬁx, as this section
describes a well-known general technique.) In reinforcement
learning, when an action u is taken at time tu and a reward r
is received at future time tr, the action is assigned credit for
the reward based on an estimate of how important the action
was in producing the reward. In eR (§IV-A), greater weight
may be given to rewards received early in the episode than on
those received later, modulated by the term γtk, 0 < γ ≤ 1 in
(5). G(PO)MDP [31] uses the separation in time between tu
and tr to assign credit in proportion to γtr−tu, tu < tr. We
use G(PO)MDP as described in [26]. The gradient estimator
is related to (5):
̂
gθi = ⟨
H
∑
p=0
(
p
∑
k=0
∇θi log Pr(uk∣sk;θ))(γtkrk − bi)⟩
(21)
We use the optimal baseline bi shown in [26].
2) Sliding trajectory windows: As presented above, (21)
learns from rewards received early in the trajectory but not
later, since γt drives the value of later rewards to 0. Therefore
we break a trajectory into sequences of ⟨sµ,aµ,rµ⟩, with
one sequence starting at each timestep, and present those as
separate trajectories to (21). Sequence length n is chosen such
that γn ≥ 0.05 > γn+1: the terms beyond this increase compu-
tational burden without signiﬁcantly improving accuracy.
D. PGRL+µ: Combining gradient and metapolicy updates
Changes to the base policy’s θ can come from the base
PGRL estimator (§IV-D) after every epoch, or from the
metapolicy (17) via uµ after every trial. When the base PGRL
estimator produces an estimate, we use it to adjust θ. But
we can also pretend that it came from πµ, and use it as uµ
for the computation of ∇θ log π(sµ,uµ;Θ). Thus both the
PGRL and the πµ updates and metarewards can be used to
form the µ-trajectory for (21).1 Although tuning can improve
performance, for expository simplicity we set the magnitudes
of all updates to be the same.
E. Autodetect metalearner: Monitoring metapolicy quality
Early in training, the metalearner can give poor advice,
leading to high-cost policies. If ample non-mission training
time is allocated, then such runs do not pose a problem. In our
single-node example, roughly 50 runs of 100 trials each were
required before the metapolicy reliably improved upon PGRL.
Our hope is that any knowledge encoded by the metalearner
can be transferred between tasks and that therefore metapolicy
1While the base gradient update provides a legitimate value for uµ and
hence ∇θ log π(sµ, uµ; Θ), it means that the forward roll-out is off-policy.
This is theoretically interesting, but in practice, for our application, not
problematic.
training time for any actual scenario may be low, but here we
show how the metalearner can be trained, tuned, and tested
without signiﬁcantly interfering with the performance of a live
network.
After each epoch, PGRL adjusts θ by some amount ∆∇θ,
and the following trial yields a µ-reward (20). If instead it
had been the metalearner that had recommended the same
update ∆µθ = ∆∇θ, the same reward (on average) would
have been observed. Moreover, when the two recommend
“opposite” changes to θ, it is more often the case than not
that the reward earned by the metalearner would at least have
had the opposite sign to that actually seen. Thus it is possible
to make an educated guess as to whether the metapolicy’s
output would have led to an improvement in the base policy,
without actually making the suggested change.
We deﬁne the update disagreement δ as the unsigned angle
between ∆∇θ and ∆µθ. We estimate metapolicy quality by
tracking the average disagreement between base policy and
metapolicy updates. For each policy update, we score the
comparison between the metapolicy to PGRL as follows:
∇ good:
δ ≤ π
2
1
rµ > τ
δ ≥ 3π
2
-1
∇ bad:
δ ≤ π
2
-1
rµ < −τ
δ ≥ 3π
2
1
Our estimate of metapolicy quality is the mean of the trial-
by-trial scores during a run, which is roughly equivalent to
the negative of the slope of a linear regression of metareward
vs. disagreement, but offers better numerical properties: the
result is bounded, which eliminates the effect of outliers
due to unusual exploration noise; it discards the ambiguous
cases in which the PGRL and δ differ by around
π
2 ; and
it produces a valid result when the metapolicy and policy
[dis]agree perfectly. τ was roughly hand-tuned to yield a
positive metareward quality at, on average, the same time
as the standard PGRL+µ metalearner started to outperform
PGRL, yielding τ = 1000. We perform exponential smoothing
(base 0.9) across runs on the result in order to use more
available information.
VI. RESULTS
We generate random data-ferrying problems each of which
consists of a random position and orientation for each sensor.
At each timestep the aircraft ﬂies some distance towards the
next waypoint, measures the current SNRs via probe packets,
and requests some data from a node at the power indicated by
the power policy. A trial is a single complete ﬂight over the
radio ﬁeld. An epoch is a small number of trials, after which
we estimate the policy gradients and update the policies. A run
is an attempt to optimise radio power and waypoint position
policies for a given problem, and here consists of 100 trials.
For each problem we generate a new random radio ﬁeld and
re-initialise the policies, but not the metapolicies. Although
it is possible to learn as soon as we have enough trials to
produce a gradient estimate, for simplicity we instead hold
301
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-207-3
SENSORCOMM 2012 : The Sixth International Conference on Sensor Technologies and Applications

Auto µ: trial 75, dist 167%, energy 23%
0
20
40
60
80
100
−1.4
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
trial
log2(Σ energy/ref)
Energy use
 
 
runs 1−20
runs 80−100
P=0.5
PGRL
PGRL+µ
Auto µ
Fig. 2.
Learning to minimise energy use for the single-node case. Left:
sample trajectory plotted in space, superimposed over reference rate contours
that show what the aircraft would see in ﬂat level ﬂight (not what it actually
sees as it steers and banks). The aircraft starts at ⋆; the waypoint is at 
. Circle
size is proportional to data rate. Right: energy use for the three algorithms
for the current experiment, averaged over the last few runs, which measures
the performance gains possible from a well-trained metalearner.
the metapolicy’s parameters Θ constant during each run. An
experiment is a set of 100 runs during which the metalearners
have the opportunity to adapt. For each experiment we re-
initialise the metapolicy parameters. To generate the graphs,
we average over 50 experiments.
We will compare the metalearners to two non-learning
approaches and to PGRL:
Reference is the non-learning autopilot deﬁned in §III-A.
Half-power learns waypoint placement as described in
§IV-C, but instead of learning the power policy, sets P = Pmax
2 .
This is guaranteed to increase trajectory length by a factor ≤ 2.
PGRL uses the Learning autopilot and the conventional
PGRL approach described in §IV-D, without the metalearner.
Parameters: The aircraft ﬂies at speed v = 1 at altitude
z = 3. The maximum turning rate of ω = 20○/s yields a turning
radius r =
v
ω ≈ 2.9. Radios use Pmax = 150 and bandwidth
β = 1, and the background noise N = 2. Each sensor’s data
requirement req = 20. These parameters do not qualitatively
affect the results, and can be replaced by appropriate values
for any given hardware. For waypoint placement the learning
rate α = 0.5 and the exploration parameter σ = 1. The
power policy uses α = 0.3, σ = 0.2, φ = 1,ϕj = 1∀j.
Policy gradient estimates for waypoint placement and energy
are computed and applied every 4 trials (one epoch) for
reasons described in [2]. Metapolicy gradient estimates are
computed and applied after each run as described in §V-C.
The metalearner’s temporal reward discount γ = 0.25.
A. Learning from base gradient, Metalearning
Figure 2 shows the energy use of our learners on a single
node over the course of 100 trials. The behaviour of the
metalearners’ early learning is illustrated by performance plots
over runs 1–20. We contrast this with the performance of well-
trained networks over runs 80–100. All performance graphs
show log2 of the ratio of performance compared to Reference.
0
20
40
60
80
100
0
0.2
0.4
0.6
0.8
1
trial
log2(distance/ref)
Trajectory length
 
 
runs 1−20
runs 80−100
P=0.5
PGRL
PGRL+µ
Auto µ
0
20
40
60
80
100
−1.4
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
trial
log2(cost/ref)
Cost
Fig. 3.
Further learning details for the 1-node case. Left: trajectory length.
Right: aggregate cost vs. trials relative to Reference (averaged over the last
few runs of all experiments).
In our scenarios, Half-power (or “P=0.5”) consistently
reduces sensor energy requirements to about 2−0.65 ≃ 65%
of Reference in exchange for a 10% length increase, although
this varies with data requirement—for high requirements (e.g.,
req > 100) Half-power reduces energy use by only 20% with
a 60% increase in trajectory length. Despite the ﬁxed power
policy, energy use decreases slightly over time as waypoint
positions are optimised.
Figure 3 shows how the learners’ behaviours change during
each run as the waypoint and energy policies are reﬁned. After
20 or so trials, PGRL performs very well (Figure 3, Cost), but
as it nears the optimal solution it falls into a cycle of discover-
ing and rediscovering the cliff when random exploration steps
take trajectory length over dmax, resulting in frequent high-
cost trajectories. It still outperforms Reference and untrained
(runs 1–20) PGRL+µ, but Half-power is superior. In contrast,
the trained (runs 80–100) metalearners outperform PGRL both
early in each problem (the ﬁrst few trials) during which the
metapolicies rapidly push the policies towards lower energy
use, and later, where they almost completely eliminate cliff-
jumping.
The higher-level time-varying behaviour shown by the met-
alearners can be seen in the difference between runs 1–20 and
80–100, and more explicitly in Figure 4. As the metalearners
observe the learners solving new problems, they reﬁne their
µ−policies, yielding performance that improves from run to
run. Figures 2 and 3 show snapshots of per-trial performance
averaged over runs 1–20 and 80–100, and the improvement
of average per-trial performance over runs can be seen in
Figure 4.
PGRL+µ usually outperforms the non-meta approaches by
a signiﬁcant margin after about 30 runs, allowing discovery of
policies that use only 40% of the sensor energy of Reference
and 65% that of Half-power, while seldom exceeding the
aircraft’s soft range limit. Perhaps surprisingly, even with such
a simple representation we found that handcoding a metapolicy
that outperforms the learned one was not easy. However,
Figure 4 shows that early in metapolicy training, especially
for the ﬁrst 20 runs, PGRL+µ performs poorly, producing
302
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-207-3
SENSORCOMM 2012 : The Sixth International Conference on Sensor Technologies and Applications

0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
dist ratio
∆θ
PGRL+µ: πµ, run 8
 
 
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
dist ratio
∆θ
PGRL+µ: πµ, run 100
0
20
40
60
80
100
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
run
log2(cost/ref)
Cost vs. µ−experience, trials 80−100
 
 
1−10
1−100
80−100
−1
−0.8
−0.6
−0.4
−0.2
0
runs
mean cost, log2(alg/ref)
Cost (trials 1−20)
 
 
1−10
1−100
80−100
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
runs
mean cost, log2(alg/ref)
Cost (trials 80−100)
0
20
40
60
80
100
−0.05
0
0.05
0.1
0.15
run
πµ quality estimate
πµ quality estimate
P
R50
P=0.5
PGRL
PGRL+µ
Auto µ
P=0.5
PGRL
PGRL+µ
Auto µ
Fig. 4. Metalearning on 1 node. Top left: Trained (trials 80–100) performance
of the algorithms over runs, during which the metalearners improve with
experience. Right: The estimate of the quality of the metalearner’s output
through time, both for PGRL+µ, which does not use the information, and for
Autodetect, which does. Middle: representations of the metapolicies’ average
actions uµ = ∆θ vs. the observation of the previous trajectory, “dist ratio”
sµ =
d
dmax , shown early in µ−training (run 8) and late (run 100). Bottom:
Average performance of the algorithms relative to Reference at key points
during learning and metatraining. Shown are the values measured across the
runs indicated on the horizontal axis for left: just the ﬁrst 20 trials and right:
(same legend) the last 21 trials in each run. Error bars show problem-to-
problem standard deviation 1σ of solution quality compared to Reference.
trajectories that are on average no better than Reference and
are frequently far worse.
This problem is alleviated by Autodetect: in early runs,
before its metapolicy has been well-trained, the quality mea-
sure often prevents the metapolicy from being used. As can
be seen from the πµ quality graph in Figure 4, the quality
measure is on average above 0 (the threshold for use of the
metapolicy’s output), but in individual experiments it drops
below 0 at the appropriate times and thus disables the use
of the metapolicy. The Cost vs. µ−experience and bar graphs
show the consequence: Autodetect’s performance tracks that
of PGRL early in training, and as the metalearner’s experience
with different problems in the domain grows, it surpasses
PGRL and performs as well as PGRL+µ.
0
20
40
60
80
100
−1
−0.5
0
0.5
1
1.5
2
2.5
run
log2(cost/ref)
Cost with µ−experience, trials 50−100
 
 
20
40
60
80
100
−1
−0.5
0
0.5
1
1.5
2
2.5
trial
log2(cost/ref)
Cost
 
 
PGRL+µ
PGRL
Fig. 5.
Compensating for poorly chosen learning rates: PGRL vs. Autodetect
with αw = 0.97trial and energy policy learning rate αe = 1 — a value only
twice one that performs well. (10-experiment average)
The bar graphs in Figure 4 break down performance over
the two learning timescales in order to show how quickly
learning progresses without (runs 1–10) and with (runs 80–
100) a well-trained metalearner. Performance over trials 1–
20 shows initial learning speed, while performance over trials
80–100 shows what may be expected as the network matures.
This further emphasises Autodetect’s ability to nearly match
the performance of the better of PGRL or PGRL+µ.
Another interesting feature of the Autodetect learner, visible
in Figure 4’s πµ quality graph, is that the measured metapolicy
quality progresses differently from that of PGRL+µ: the
quality estimate stays higher initially, but begins to track that
of PGRL+µ around run 40. The two metapolicies’ parameters
evolve differently due to the differences in training: while
PGRL+µ always sees meta-actions from both the metapolicy
and PGRL, Autodetect sees only the latter until it has proven
itself, thus obtaining fewer training examples drawn from a
different distribution. Raising Autodetect’s threshold makes it
less likely that the metapolicy gives bad advice to the learner,
but reduces µ−learning speed. The effect that this has on our
metapolicy quality estimate through time is intriguing, but we
leave investigation as future work.
Much of the training time shown in Figure 4 may be
required only once in a “lifetime” due to the transferability of
the trained metapolicies. Once their metapolicies are trained,
the metalearners facilitate the discovery of a good policy
extremely rapidly—after only a few trials. They aggressively
push policy changes that they have found in the past result in
higher performance: quickly reducing energy use until nearing
the UA’s range limit and then backing off without requiring
further exploration of the cliff’s high-cost trajectories.
B. Sensitivity to learning rates
The base PGRL learner can achieve stable results if the
learning step sizes such as α in (6) are chosen carefully. If
learning rates are small, learning is slow, but a larger values
exacerbate the learner’s tendency to cliff-jump. Therefore it is
typical in reinforcement learning to have α decrease over the
course of a run—for example α = α0xe,x < 1 for the update
after epoch e. Ideally α → 0 as the policy nears the optimal.
303
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-207-3
SENSORCOMM 2012 : The Sixth International Conference on Sensor Technologies and Applications

For our task, this both provides a convergence guarantee for
the hillclimber and reduces cliff-jumping, but it requires hand-
tuning of x and α0. Furthermore it eliminates the system’s
ability to adapt to a slowly changing environment; e.g., foliage
growth or physically shifted sensors.
A poor choice of αe interferes with the learners’ ability
to remain near the optimum. Here we have set αe = 1. We
set the waypoint-placement learning rate αw = 0.97trial in
order to allow both policies to converge more quickly, which
is unnecessary in our other tests but helpful here. Figure 5
shows results: while αe = 0.3 (as shown in §VI-A) produces
reasonable results, αe = 1 results in the exploration of many
high-cost trajectories. PGRL performs extremely poorly, but
PGRL+µ learns to anticipate the large jumps that result from
the poor choice of αe, resulting in costs worse than with a
carefully chosen αe but still signiﬁcantly better than PGRL.
The ability of the metalearner to stabilise learners with less
hand-tuning may be a great asset in real-world systems.
C. Knowledge transfer with metapolicy initialisation
While the learned energy and waypoint policies are highly
problem-speciﬁc, the metapolicy is more broadly applicable.
This is shown in §VI-A by the gains on new problems after
training on previous ones. But can a metapolicy trained for
our single-node scenario be used to accelerate policy learning
for problems drawn from a broader domain? The question of
metapolicy transferability motivates us to modify our problem
space as follows:
● For each problem, we place 5 sensor nodes randomly on
a 40 × 40 ﬁeld. This yields a great variety of radio ﬁeld
shapes since the nodes interfere with each other.
● For each problem, the data requirement for each node is
drawn randomly from [10...30].
● We have corrupted the aircraft’s information about the
nodes’ locations with Gaussian noise with σ = 3.
Figure 6 shows training results: for “Transfer” the metapol-
icy is set to the ﬁnal Autodetect value learned during the
generation of Figures 2–4:
Θ = [
1.70
−1.16
−1.61
1.34 ]
(22)
Figure 6 shows the results in the extended domain: the
earlier metapolicy achieves better than a 60% reduction in
sensor radio transmission energy over Reference, and again
offering speed and stability improvements over non-meta
PGRL. The ability of single-node metapolicies to generalise to
a broader problem conﬁguration space is encouraging, but it is
not perfect. For example, if we allow node data requirements to
be drawn randomly in [1...10] the metapolicy is not cautious
enough in its recommendations for policy updates due to the
short contact times. Over multiple runs the metapolicy does
continue to adapt based on policy learning from the new
problems, but due to space constraints we do not study multi-
node metapolicy learning here.
We show progress to 200 trials for this scenario. Figure 6
shows that for Transfer this is overkill, but PGRL has not yet
converged. Performance gains in this scenario, even with a
repurposed metapolicy, are slightly greater than in our ﬁrst ex-
ample, with Transfer achieving a 62% savings and even Half-
power effecting a 45% savings. This difference is primarily
due to the sensor position misinformation: Reference does not
understand the error and so its performance degrades sharply
as the information is compromised, whereas the learners—
even Half-power—modify their trajectories to work around the
misinformation.
VII. CONCLUSION AND FUTURE WORK
We have demonstrated the feasibility of a reinforcement
learning approach for autonomous discovery of energy-saving
behaviours for sensor networks. With a soft trajectory length
limit of twice a handcoded reference and a single sensor, the
UA tended to ﬂy 75% further than under Reference while
sensors reduced communication energy by 60%. When the
UA’s knowledge of sensor positions is degraded by even a
modest amount, the advantage of the learning approaches
increases rapidly.
The UA does not model the environment, but learns directly
from experience, saving the costs of locating the sensors and
building system models, and eliminating the effects of mod-
elling errors. We use a trajectory encoding that is well-suited
to the task of interfacing between learning algorithms and
proprietary autopilots. Additionally, we concurrently optimise
both power and waypoint-placement policies.
Our reinforcement metalearner uses experience with the
process of learning data-ferrying policies in order to accelerate
and stabilise a conventional PGRL system, and transfers
acquired knowledge about the policy-optimisation process to
a range of unseen problems. This furnishes a new mechanism
for approaching the global optimum of an unseen data-ferrying
scenario extremely quickly while sampling few high-cost
trajectories.
Much work remains to be done. For example, the meta-
learner reduces the necessity of hand-tuning the system by
compensating for poor choice of parameters. A poorly chosen
exploration rate frequently produces trajectories that greatly
exceed the aircraft’s range limit. Preliminary results show that
as the metalearner gains experience with the base optimiser
it learns to compensate, modulating the problematic policy
updates and keeping trajectory costs lower, but a full investi-
gation into the range and limits of this capability, both in the
data-ferrying domain and for general model-free optimisation,
remains to be done. Also, we have examined only learning
in the vicinity of a few nodes. While preliminary results on
the transfer of this metaknowledge to multi-node problems
is promising, efﬁciently scaling the metalearning mechanism
to sensor networks of many nodes is important future work.
Finally, a more detailed characterisation of when and to
what extent the metaknowledge is transferable will be key to
understanding the broader applicability of the technique.
REFERENCES
[1] A. Jenkins, D. Henkel, and T. Brown, “Sensor data collection through
gateways in a highly mobile mesh network,” in Proc. IEEE Wireless
304
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-207-3
SENSORCOMM 2012 : The Sixth International Conference on Sensor Technologies and Applications

Transfer: trial 195, req [ 25 24 13 24 25 ]
dist 177%, energy [ 43 25 48 11 23 ]%
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
trial
log2(distance/ref)
Trajectory length
 
 
0
50
100
150
200
−1.5
−1
−0.5
0
trial
log2(Σ energy/ref)
Energy use
 
 
P=0.5
PGRL
Transfer
0
50
100
150
200
−1.5
−1
−0.5
0
0.5
1
trial
log2(cost/ref)
Cost
Fig. 6.
Transferring the metapolicy to a larger ﬁeld with varying data requirements and incorrect node position information. See Figures 2–4 for descriptions
of the graphs. Left: the nodes are at ▲; their assumed positions, to which the waypoints were initialised, are at △; and “energy” is relative to Reference.
Communications and Networking Conference (WCNC).
Hong Kong:
IEEE, 2007, pp. 2784–2789.
[2] B. Pearre and T. X. Brown, “Fast, scalable, model-free trajectory
optimization for wireless data ferries,” in IEEE International Conference
on Computer Communications and Networks (ICCCN), 2011, pp. 370–
377.
[3] H. Jun, W. Zhao, M. H. Ammar, E. W. Zegura, and C. Lee, “Trading
latency for energy in densely deployed wireless ad hoc networks using
message ferrying,” Ad Hoc Netw., vol. 5, pp. 444–461, May 2007.
[4] O. Tekdas, J. Lim, A. Terzis, and V. Isler, “Using mobile robots to
harvest data from sensor ﬁelds,” IEEE Wireless Communications special
Issue on Wireless Communications in Networked Robotics, vol. 16, pp.
22–28, 2008.
[5] G. Anastasi, M. Conti, and M. Di Francesco, “Reliable and energy-
efﬁcient data collection in sparse sensor networks with mobile elements,”
Perform. Eval., vol. 66, pp. 791–810, December 2009.
[6] M. Ma and Y. Yang, “Sencar: An energy-efﬁcient data gathering mech-
anism for large-scale multihop sensor networks,” IEEE Transactions on
Parallel and Distributed Systems, vol. 18, pp. 1476–1488, 2007.
[7] R. Sugihara and R. K. Gupta, “Optimizing energy-latency trade-off in
sensor networks with controlled mobility,” in IEEE INFOCOM Mini-
conference, 2009, pp. 2566–2570.
[8] D. Ciullo, G. Celik, and E. Modiano, “Minimizing transmission energy
in sensor networks via trajectory control,” in IEEE Symposium on
Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks
(WiOpt), 2010, pp. 132–141.
[9] L. B¨ol¨oni and D. Turgut, “Should i send now or send later? a decision-
theoretic approach to transmission scheduling in sensor networks with
mobile sinks,” Wireless Communications and Mobile Computing, vol. 8,
no. 3, pp. 385–403, 2008.
[10] G. Anastasi, M. Conti, M. Di Francesco, and A. Passarella, “Energy
conservation in wireless sensor networks: a survey,” Ad Hoc Netw.,
vol. 7, pp. 537–568, May 2009.
[11] Y. Gu, D. Bozdag, R. W. Brewer, and E. Ekici, “Data harvesting with
mobile elements in wireless sensor networks,” Computer Networks 50,
vol. 17, pp. 3449–3465, 2006.
[12] A. A. Somasundara, A. Ramamoorthy, and M. B. Srivastava, “Mobile
element scheduling with dynamic deadlines,” IEEE Transactions on
Mobile Computing, vol. 6, no. 4, pp. 395–410, 2007.
[13] D. Henkel and T. X. Brown, “Towards autonomous data ferry route
design through reinforcement learning,” in Proceedings of the 2008
International Symposium on a World of Wireless, Mobile and Multimedia
Networks (WOWMOM).
Washington, DC, USA: IEEE, 2008, pp. 1–6.
[14] T. He, K. won Lee, and A. Swami, “Flying in the dark: Controlling
autonomous data ferries with partial observations,” in Proceedings of the
eleventh ACM international symposium on Mobile ad hoc networking
and computing (MobiHoc).
New York, NY, USA: ACM, 2010, pp.
141–150.
[15] W. Zhao and M. H. Ammar, “Message ferrying: Proactive routing in
highly-partitioned wireless ad hoc networks,” in Proceedings of the
The Ninth IEEE Workshop on Future Trends of Distributed Computing
Systems, ser. FTDCS ’03.
Washington, DC, USA: IEEE, 2003, pp.
308–314.
[16] M. Dunbabin, P. Corke, I. Vasilescu, and D. Rus, “Data muling over
underwater wireless sensor networks using an autonomous underwater
vehicle,” in Proc. of IEEE International Conference on Robotics and
Automation (ICRA), 2006, pp. 2091–2098.
[17] M. M. Bin Tariq, M. Ammar, and E. Zegura, “Message ferry route
design for sparse ad hoc networks with mobile nodes,” in MobiHoc:
Proceedings of the 7th ACM international symposium on Mobile ad hoc
networking and computing.
New York, NY, USA: ACM, 2006, pp.
37–48.
[18] R. Sugihara and R. K. Gupta, “Improving the data delivery latency
in sensor networks with controlled mobility,” in Proc. 4th IEEE inter-
national conference on Distributed Computing in Sensor Systems, ser.
DCOSS.
Berlin, Heidelberg: Springer-Verlag, 2008, pp. 386–399.
[19] ——, “Path planning of data mules in sensor networks,” in ACM Trans.
Sen. Netw., vol. 8, no. 1. New York, USA: ACM, Aug. 2011, pp. 1–27.
[20] D. Henkel and T. X. Brown, “On controlled node mobility in delay-
tolerant networks of unmanned aerial vehicles,” in International Sym-
posium on Advance Radio Technolgoies, 2008, pp. 7–16.
[21] A. Carfang, E. W. Frew, and T. X. Brown, “Improved delay-tolerant
communication by considering radio propagation in planning data ferry
navigation,” in Proc. AIAA Guidance, Navigation, and Control. Toronto,
Canada: AIAA, August 2010, pp. 5322–5335.
[22] M. Stachura, A. Carfang, and E. W. Frew, “Active sensing by unmanned
aircraft systems in realistic communication environments,” in IFAC
Workshop on Networked Robotics, 2009, pp. 62–67.
[23] F. Jiang and A. L. Swindlehurst, “Optimization of uav heading for the
ground-to-air uplink,” IEEE Journal on Selected Areas in Communica-
tions, vol. 30, no. 5, pp. 993–1005, 2012.
[24] C. E. Shannon, “Communication in the presence of noise,” in Proc.
Institute of Radio Engineers, vol. 37, no. 1, 1949, pp. 10–21.
[25] N. Wagle and E. W. Frew, “A particle ﬁlter approach to wiﬁ target
localization,” in AIAA Guidance, Navigation, and Control Conference.
Toronto, Canada: AIAA, August 2010, pp. 2287–2298.
[26] J. Peters and S. Schaal, “Reinforcement learning of motor skills with
policy gradients,” Neural Networks, vol. 21, no. 4, pp. 682–697, 2008.
[27] P. Glynn, “Likelihood ratio gradient estimation: An overview,” in Pro-
ceedings of the 1987 Winter Simulation Conference, 1987, pp. 366–375.
[28] R. J. Williams, “Simple statistical gradient-following algorithms for
connectionist reinforcement learning,” Machine Learning, vol. 8, pp.
229–256, 1992.
[29] J. Z. Kolter, Z. Jackowski, and R. Tedrake, “Design, analysis and
learning control of a fully actuated micro wind turbine,” in Proceedings
of the 2012 American Control Conference (ACC), 2012.
[30] J. A. Hertz, A. S. Krogh, and R. G. Palmer, Introduction to the Theory
of Neural Computation.
Perseus Books, 1991.
[31] J. Baxter and P. L. Bartlett, “Inﬁnite-horizon policy-gradient estimation,”
Journal of Artiﬁcial Intelligence Research, vol. 15, pp. 319–350, 2001.
305
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-207-3
SENSORCOMM 2012 : The Sixth International Conference on Sensor Technologies and Applications

