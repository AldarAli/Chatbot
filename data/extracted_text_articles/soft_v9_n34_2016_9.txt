260
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Unsupervised curves clustering by minimizing
entropy: implementation and application to air trafﬁc
Florence Nicol
Universit´e F´ed´erale de Toulouse
Ecole Nationale de l’Aviation Civile
F-31055 Toulouse FRANCE
Email: florence.nicol@enac.fr
St´ephane Puechmorel
Universit´e F´ed´erale de Toulouse
Ecole Nationale de l’Aviation Civile
F-31055 Toulouse FRANCE
Email: stephane.puechmorel@enac.fr
Abstract—In many applications such as Air Trafﬁc Management
(ATM), clustering trajectories in groups of similar curves is
of crucial importance. When data considered are functional in
nature, like curves, dedicated algorithms exist, mostly based
on truncated expansion on Hilbert basis. When additional con-
straints are put on the curves, as like in applications related
to air trafﬁc where operational considerations are to be taken
into account, usual procedures are no longer applicable. A
new approach based on entropy minimization and Lie group
modeling is presented here and its implementation is discussed
in detail, especially the computation of the curve system density
and the entropy minimization by a gradient descent algorithm.
This algorithm yields an efﬁcient unsupervised algorithm suitable
for automated trafﬁc analysis. It outputs cluster centroids with
low curvature, making it a valuable tool in airspace design
applications or route planning.
Keywords–curve clustering; probability distribution estimation;
functional statistics; minimum entropy; Lie group modeling; air
trafﬁc management.
I.
INTRODUCTION
Clustering aircraft trajectories is an important problem in
Air Trafﬁc Management (ATM). It is a central question in
the design of procedures at take-off and landing, the so called
sid-star (Standard Instrument Departure and Standard Terminal
Arrival Routes). In such a case, one wants to minimize
the noise and pollutants exposure of nearby residents while
ensuring runway efﬁciency in terms of the number of aircraft
managed per time unit.
The same question arises with cruising aircraft, this time
the mean ﬂight path in each cluster being used to opti-
mally design the airspace elements (sectors and airways).
This information is also crucial in the context of future air
trafﬁc management systems where reference trajectories will
be negotiated in advance so as to reduce congestion. A special
instance of this problem is the automatic generation of safe and
efﬁcient trajectories, but in such a way that the resulting ﬂight
paths are still manageable by human operators. Clustering is
a key component for such tools: major trafﬁc ﬂows must be
organized in such a way that the overall pattern is not too
far from the current organization, with aircraft ﬂying along
airways. The classiﬁcation algorithm has thus not only to
cluster similar trajectories but at the same time makes them
as close as possible to operational trajectories. In particular,
straightness of the ﬂight segments must be enforced, along with
a global structure close to a graph with nodes corresponding
to merging/splitting points and edges the airways. Moreover,
the clustering procedure has to deal with trajectories that are
very similar in shape but are oriented in opposite directions.
These ﬂight paths should be sufﬁciently separate in order to
prevent hazardous encounters. Using the approach developed
in [1], a Lie group modeling is proposed to take into account
the direction and the position of the aircraft trajectories. The
main computational complexity of such a clustering algorithm
focuses on the computation of the curve system density. This
computational cost can be reduced by choosing appropriate
kernel functions.
This paper is organized as follows. First, previous related
works is presented. Next, in Section III, the notion of spatial
curve density and the related entropy are introduced for dealing
with curve systems. Then, the modeling of trajectories with a
Lie group approach and the statistical estimation of Lie group
densities are presented. In Section IV, the unsupervised entropy
clustering approach developed in [2] is extended to the new
setting of Lie group modeling. In Section V, a discussion on
fast implementation and algorithms presented in [1] is detailed.
Finally, results on synthetic examples and real trajectory data
are given and a conclusion is drawn.
II.
PREVIOUS RELATED WORK
Several well established algorithms may be used for per-
forming clustering on a set of trajectories, although only a few
of them were eventually applied to air trafﬁc analysis. The
spectral approach relies on trajectories modeling as vectors
of samples in a high dimensional space, and uses random
projections as means of reducing the dimensionality. The huge
computational cost of the required singular values decompo-
sition is thus alleviated, allowing use on real recorded trafﬁc
over several months. It was applied in a study conducted by the
Mitre corporation on behalf of the Federal Aviation Authority
(FAA) [3]. The most important limitation of this approach is
that the shape of the trajectories is not taken into account
when applying the clustering procedure unless a resampling
procedure based on arclength is applied: changing the time
parametrization of the ﬂight paths will induce a change in
the classiﬁcation. Furthermore, there is no means to put a
constraint on the mean trajectory produced in each cluster:
curvature may be quite arbitrary even if samples individually
comply with ﬂight dynamics.

261
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Another approach is taken in [4], with an explicit use of an
underlying graph structure. It is well adapted to road trafﬁc as
vehicles are bound to follow predetermined segments. A spatial
segment density is computed then used to gather trajectories
sharing common parts. For air trafﬁc applications, it may be of
interest for investigating present situations, using the airways
and beacons as a structure graph, but will misclassify aircraft
following direct routes which is quite a common situation,
and is unable to work on an unknown airspace organization.
This point is very important in applications since trajectory
datamining tools are mainly used in airspace redesign. A
similar approach is taken in [5] with a different measure of
similarity. It has to be noted that many graph-based algorithms
are derived from the original work presented in [6], and
exhibit the aforementioned drawbacks for air trafﬁc analysis
applications.
An interesting vector ﬁeldbased algorithm is presented in
[7]. A salient feature is the ability to distinguish between close
trajectories with opposite orientations. Nevertheless, putting
constraints on the geometry of the mean path in a cluster
is quite awkward, making the method unsuitable for our
application.
Due to the functional nature of trajectories, that are basi-
cally mappings deﬁned on a time interval, it seems more appro-
priate to resort to techniques based on times series as surveyed
in [8], [9], or functional data statistics, with standard references
[10], [11]. In both approaches, a distance between pairs of
trajectories or, in a weaker form, a measure of similarity must
be available. The algorithms of the ﬁrst category are based on
sequences, possibly in conjunction with dynamic time warping
[12], while in functional data analysis, samples are assumed
to come from an unknown underlying function belonging to a
given Hilbert space. However, it has to be noticed that apart
from this last assumption, both approaches yield similar end
algorithms, since functional data revert for implementation
to usual ﬁnite dimensional vectors of expansion coefﬁcients
on a suitable truncated basis. For the same reason, model-
based clustering may be used in the context of functional data
even if no notion of probability density exists in the original
inﬁnite dimensional Hilbert space as mentioned in [13]. A nice
example of a model-based approach working on functional
data is funHDDC [14].
III.
DEALING WITH CURVE SYSTEMS: A PARADIGM
CHANGE
When working with aircraft trajectories, some speciﬁc
characteristics must be taken into account. First of all, ﬂight
paths consist mainly of straight segments connected by arcs
of circles, with transitions that may be assumed smooth up to
at least the second derivative. This last property comes from
the fact that pilot’s actions result in changes on aerodynamic
forces and torques and a straightforward application of the
equations of motion. When dealing with sampled trajectories,
this induces a huge level of redundancy within the data, the
relevant information being concentrated around the transitions.
Second, ﬂight paths must be modeled as functions from a
time interval [a, b] to R3 which is not the usual setting for
functional data statistics: most of the work is dedicated to
real valued mappings and not vector ones. A simple approach
will be to assume independence between coordinates, so that
the problem falls within the standard case. However, even
with this simplifying hypothesis, vertical dimension must be
treated in a special way as both the separation norms and
the aircraft maneuverability are different from those in the
horizontal plane.
Finally, being able to cope with the initial requirement of
compliance with the current airspace structure in airways is
not addressed by general algorithms. In the present work, a
new kind of functional unsupervised classiﬁer is introduced,
that has in common with graph-based algorithms an esti-
mation of trafﬁc density but works in a continuous setting.
For operational applications, a major beneﬁt is the automatic
building of a route-like structure that may be used to infer new
airspace designs. Furthermore, smoothness of the mean cluster
trajectory, especially low curvature, is guaranteed by design.
Such a feature is unique among existing clustering procedures.
Finally, our Lie group approach makes easy the separation
between neighboring ﬂows oriented in opposite directions.
Once again, it is mandatory in air trafﬁc analysis where such
a situation is common.
A. The entropy of a system of curves
Considering trajectories as mappings γ : [t0, t1] → R3
induces a notion of spatial density as presented in [15]. As-
suming that after a suitable registration process all ﬂight paths
γi, i = 1, . . . , N, are deﬁned on the same time interval [0, 1] to
Ω a domain of R3, one can compute an entropy associated with
the system of curves using the approach presented in [16]. Let
a system of curves γ1, . . . , γN be given, its entropy is deﬁned
to be:
E(γ1, . . . , γN) = −
Z
Ω
˜d(x) log

˜d(x)

dx,
where the spatial density ˜d is computed according to:
˜d: x 7→
PN
i=1
R 1
0 K (∥x − γi(t)∥) ∥γ′
i(t)∥dt
PN
i=1 li
.
(1)
In the last expression, li is the length of the curve γi and K
is a kernel function similar to those used in nonparametric
estimation. A standard choice is the Epanechnikov kernel:
K : x 7→ C

262
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
•
The individual lengths will be minimized: it is a direct
consequence of the fact that the density has a term in
γ′ within the integral that will favor short trajectories.
Using a standard gradient descent algorithm on the entropy
produces an optimally concentrated curve system, suitable for
use as a basis for a route network. In Section V, this algorithm
is applied on a curve system produced by an automated
trajectory planner.
The displacement ﬁeld for trajectory j is oriented at each
point along the normal vector to the trajectory, with norm given
by:
Z
Ω
γj(t) − x
∥γj(t) − x∥

N
K′ (∥γj(t) − x∥) log

˜d(x)

dx∥γ′
j(t)∥
(2)
−
Z
Ω
K (∥γj(t) − x∥) log

˜d(x)

dx

γ′′
j (t)
∥γ′
j(t)∥

N
(3)
+
Z
Ω
˜d(x) log( ˜d(x))dx

γ′′
j (t)
∥γ′
j(t)∥

N
,
(4)
where the notation v|N stands for the projection of the vector
v onto the normal vector to the trajectory. An overall scaling
constant of:
1
PN
i=1 li
,
where li is the length of trajectory i, has to be put in front of the
expression to get the true gradient of the entropy. In practice,
it is not needed since algorithms will adjust the size of the step
taken in the gradient direction. Another formulation using the
scaled arclength in the entropy can be found in [2]. While
being equivalent to the one presented above, since it relies on
a reparametrization, only the term related to the kernel gradient
remains in the ﬁnal expression. As a consequence, there is no
need to project moves onto the normal to the curves. However,
it introduces a constraint that must be taken into account in
numerical implementations. So far, the principle retained is
to resample the curves after the update so as to ensure that
the deﬁning property (constant velocity) of the arclength is
preserved.
B. A Lie group modeling
While satisfactory in terms of trafﬁc ﬂows, the previous
approach suffers from a severe ﬂaw when one considers ﬂight
paths that are very similar in shape but are oriented in opposite
directions. Since the density is insensitive to direction reversal,
ﬂight paths will tend to aggregate while the correct behavior
will be to ensure a sufﬁcient separation in order to prevent
hazardous encounters. Taking aircraft headings into account in
the clustering process is then mandatory when such situations
have to be considered.
This issue can be addressed by adding a penalty term
to neighboring trajectories with different headings but the
important theoretical property of entropy minimization will be
lost in the process. A more satisfactory approach will be to take
heading information directly into account and to introduce a
notion of density based on position and velocity.
Since the aircraft dynamics is governed by a second order
equation of motion of the form:

γ′(t)
γ′′(t)

= F

t;
γ(t)
γ′(t)

,
it is natural to take as state vector:

γ(t)
γ′(t)

.
The initial state is chosen here to be:

0d
e1

,
with e1 the ﬁrst basis vector, and 0d the origin in Rd. It is
equivalent to model the state as a linear transformation:
0d ⊗ e1 7→ T(t) ⊗ A(t)(0d ⊗ e1) = γ(t) ⊗ γ′(t),
where T(t) is the translation mapping 0d to γ(t) and A(t) is
the composite of a scaling and a rotation mapping e1 to γ′(t).
Considering the vector (γ(t), 1) instead of γ(t) allows a matrix
representation of the translation T(t):

γ(t)
1

=

Id
γ(t)
0
1
 
0d
1

.
From now, all points will be implicitly considered as having
an extra last coordinate with value 1, so that translations are
expressed using matrices. The origin 0d will thus stand for the
vector (0, . . . , 0, 1) in Rd+1. Gathering things yields:

γ(t)
γ′(t)

=

T(t)
0
0
A(t)
 
0d
e1

.
(5)
The previous expression makes it possible to represent a
trajectory as a mapping from a time interval to the matrix Lie
group G = Rd×Σ×SO(d), where Σ is the group of multiples
of the identity, SO(d) the group of rotations and Rd the group
of translations. Please note that all the products are direct. The
A(t) term in the expression (5) can be written as an element
of Σ ⊗ SO(d). Starting with the deﬁning property A(t)e1 =
γ′(t), one can write A(t) = ∥γ′(t)∥U(t) with U(t) a rotation
mapping e1 ∈ Sd−1 to the unit vector γ′(t)/∥γ′(t)∥ ∈ Sd−1.
For arbitrary dimension d, U(t) is not uniquely deﬁned, as it
can be written as a rotation in the plane P = span(e1, γ′(t))
and a rotation in its orthogonal complement P⊥. A common
choice is to let U(t) be the identity in P⊥ which corresponds in
fact to a move along a geodesic (great circle) in Sd−1. This will
be assumed implicitly in the sequel, so that the representation
A(t) = Λ(t)U(t) with Λ(t) = ∥γ′(t)∥Id becomes unique.
The Lie algebra g of G is easily seen to be Rd × R ×
Asym(d) with Asym(d) is the space of skew-symmetric d × d
matrices. An element from g is a triple (u, λ, A) with an
associated matrix form:
M(u, λ, A) =


0
u
0
0
0
0
λId + A

 .
(6)
The exponential mapping from g to G can be obtained in a
straightforward manner using the usual matrix exponential:
exp((u, λ, A)) = exp(M(u, λ, A)).

263
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
The matrix representation of g may be used to derive a
metric:
⟨(u, λ, A), (v, µ, B)⟩g = Tr

264
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Given the random vectors Xi, i = 1, . . . , n, in Sd−1, the
estimator of the spherical distribution is given by:
bf(x) = 1
n
n
X
i=1
KV MF (x; Xi, κ)
(12)
= cd(κ)
n
n
X
i=1
eκXT
i x, κ > 0, x ∈ Sd−1.
(13)
The quantity x−Xi which appears in the linear kernel density
estimator is replaced by XT
i x which is the cosine of the
angles between x and Xi, so that more important weights
are given on observations close to x on the sphere. The
concentration parameter κ is a smoothing parameter that plays
the role of the inverse of the bandwidth parameter as deﬁned
in the linear kernel density estimation. Large values of κ
imply greater concentration around the mean direction and lead
to undersmoothed estimators whereas small values provide
oversmoothed circular densities [20]. Indeed, if κ
=
0,
the vMF kernel function reduces to the uniform circular
distribution on the hypersphere. Note that the vMF kernel
function is convenient when the data is rotationally symmetric.
The vMF kernel function is a convenient choice for our
problem because this p.d.f. is invariant under the action on
the sphere of the rotation component of the Lie group G.
Moreover, this distribution has properties analogous to those
of multivariate Gaussian distribution and is the limiting case
of a limit central theorem for directional statistics. Other
multidimensional distributions might be envisaged, such as the
bivariate von Mises, the Bingham or the Kent distributions
[18]. However, the bivariate von Mises distribution being a
product kernel of two univariate von Mises kernels, it is more
appropriate for modeling density distributions on the torus and
not on the sphere. The Bingham distribution is bimodal and
satisﬁes the antipodal symmetry property K(x) = K(−x).
This kernel function is used for estimating the density of
axial data and is not appropriate for our clustering approach.
Finally, the Kent distribution is a generalization of the vMF
distribution, which is used when we want to take into account
the spread of data. However, the rotation-invariance property
of the vMF distribution is lost.
As for the scaling component of G, the usual kernel
functions such as the Gaussian and the Epanechnikov kernel
functions are not suitable for estimating the radial distribution
of a random vector in Rd. When distributions are deﬁned over
a positive support (here in the case of non-negative data), these
kernel functions cause a bias in the boundary regions because
they give weights outside the support. An asymmetrical kernel
function on R+ such as the log-normal kernel function is
a more convenient choice. Moreover, this p.d.f. is invariant
by change of scale. Let R1, . . . , Rn be univariate random
variables from a p.d.f. which has bounded support on [0; +∞[.
The radial density estimator may be deﬁned by means of a sum
of log-normal kernel functions as follows:
bg(r) = 1
n
n
X
i=1
KLN(r; ln Ri, h), r ≥ 0, h > 0,
(14)
where
KLN(x; µ, σ) =
1
√
2πσx e− (ln x−µ)2
2σ2
(15)
is the log-normal kernel function and h is the bandwidth
parameter. The resulting estimate is the sum of bumps de-
ﬁned by log-normal kernels with medians Ri and variances
(eh2 −1)eh2R2
i . Note that the log-normal (asymmetric) kernel
density estimation is similar to the kernel density estimation
based on a log-transformation of the data with the Gaussian
kernel function. Although the scale-change component of G is
the multiplicative group R+, we can use the standard Gaussian
kernel estimator and the metric on R.
IV.
UNSUPERVISED ENTROPY CLUSTERING
The ﬁrst thing to be considered is the extension of the
entropy deﬁnition to curve systems with values in G. Starting
with expression from (1), the most important point is the
choice of the kernel involved in the computation. As the
group G is a direct product, choosing K = Kt.Ks.Ko with
Kt, Ks, Ko functions on respectively the translation, scaling
and rotation part will yield a G-invariant kernel provided the
Kt, Ks, Ko are invariant on their respective components.
Since the translation part of G is modeled after Rd, the
Epanechnikov kernel is a suitable choice. As for the scaling
and rotation, the choice made follows the conclusion of Section
III-C: a log-normal kernel and a von-Mises one will be used
respectively. Finally, the term ∥γ′(t)∥ in the original expression
of the density, that is required to ensure invariance under re-
parametrization of the curve, has to be changed according
to the metric in G and is replaced by ⟨⟨γ′(t), γ′(t)⟩⟩1/2
γ(t). The
density at x ∈ G is thus:
dG(x)) =
PN
i=1
R 1
0 K (x, γi(t)) ⟨⟨γ′
i(t), γ′
i(t)⟩⟩1/2
γi(t)dt
PN
i=1 li
(16)
where li is the length of the curve in G, that is:
li =
Z 1
0
⟨⟨γ′
i(t), γ′
i(t)⟩⟩1/2
γi(t)dt.
(17)
The expression of the kernel evaluation K (x, γi(t)) is split
into three terms. In order to ease the writing, a point x in G will
be split into xr, xs, xo components where the exponent r, s, t
stands respectively for translation, scaling and rotation. Given
the fact that K is a product of component-wise independent
kernels it comes:
K (x, γi(t)) = Kt

265
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
with normalizing constant as given in (11). In the general case,
it is also possible, writing the rotation as a sequence of moves
on spheres Sd−1, Sd−2, . . . and the distribution as a product of
von-Mises on each of them, to have a vector of parameters κ:
it is the approach taken in [21] and it may be applied verbatim
here if needed.
The entropy of the system of curves is obtained from the
density in G:
E(dG) = −
Z
G
dG(x) log dG(x)dµG(x)
(21)
with dµG the left Haar measure. Using again the fact that G is a
direct product group, dµ is easily seen to be a product measure,
with dxt, the usual Lebesgue measure on the translation part,
dxs/xs on the scaling part and dxo on Sd−1 for the rotation
part. It turns out that the 1/xs term in the expression of dxs/xs
is already taken into account in the kernel deﬁnition, due to the
fact that it is expressed in logarithmic coordinates. The same
is true for the von-Mises kernel, so that in the sequel only the
(product) Lebesgue measure will appear in the integrals.
Finding the system of curves with minimum entropy re-
quires a displacement ﬁeld computation as detailed in [16].
For each curve γi, such a ﬁeld is a mapping ηi : [0, 1] → TG
where at each t ∈ [0, 1], ηi(t) ∈ TGγi(t). Compare to the
original situation where only spatial density was considered,
the computation must now be conducted in the tangent space
to G. Even for small problems, the effort needed becomes
prohibitive. In Section V, we will present in detail an efﬁcient
implementation of this algorithm. First, note that the structure
of the kernel involved in the density can help in cutting the
overall computations needed. Since it is a product, and the
translation part is compactly supported, being an Epanechnikov
kernel, one can restrict the evaluation to points belonging to its
support. Density computation will thus be made only in tubes
around the trajectories. Second, for the target application that
is to cluster the ﬂight paths into a route network and is of pure
spatial nature, there is no point in updating the rotation and
scaling part when performing the moves: only the translation
part must change, the other two being computed from the
trajectory. The initial optimization problem in G may thus
be greatly simpliﬁed. Finally, binning techniques
[22] will
be used to reduce the computational cost of the translation,
rotation and scale components in the kernel K.
Let ϵ be an admissible variation of curve γi, that is a
smooth mapping from [0, 1] to TG with ϵ(t) ∈ Tγi(t)G and
ϵ(0) = ϵ(1) = 0. We assume furthermore that ϵ has only a
translation component. The derivative of the entropy E(dG)
with respect to the curve γi is obtained from the ﬁrst order
term when γi is replaced by γi + ϵ. First of all, it has to be
noted that dG is a density and thus has unit integral regardless
of the curve system. When computing the derivative of E(dG),
the term
−
Z
G
dG(x)∂γidG(x)
dG(x) dµG(x) = −
Z
G
∂γidG(x)dµG(x)
will thus vanish. It remains:
−
Z
G
∂γidG(x) log dG(x)dµG(x).
The density dG is a sum on the curves, and only the i-th term
has to be considered. Starting with the expression from (16),
one term in the derivative will come from the denominator. It
computes the same way as in [16] to yield:
γt′′
i (t)
⟨⟨γ′
i(t), γ′
i(t)⟩⟩G

N
E(dG)
(22)
Please note that the second derivative of γi is considered only
on its translation component, but the ﬁrst derivative makes use
of the complete expression. As before, the notation |N stands
for the projection onto the normal component to the curve.
The second term comes from the variation of the numerator.
Using the fact that the kernel is a product KtKsKo and that
all individual terms have a unit integral on their respective
components, the expression becomes very similar to the case
of spatial density only and is:
−
Z
G
K (x, γi(t)) log dG(x)dµG(x)

γt′′
i (t)
⟨⟨γ′
i(t), γ′
i(t)⟩⟩1/2
G

N
(23)
+
Z
Rd e(t)Kt′ 
266
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
a discrete grid [23] or Monte-Carlo methods when the di-
mensionality of the problem induces an intractable computa-
tional cost. In the present case, where the integration has to
be conveyed in four dimensions, grid based approaches can
still be applied. Extension to trajectories with values in R3
will increase the dimension to 6 which mandates the use of
stochastic approximations. In any case, it must be noted that a
high accuracy in the result is not needed, so that randomized
algorithms may be used without impairing the convergence of
the subsequent gradient iteration.
The computation of the density itself is more constraining
as its original deﬁnition involves for each point where its value
is needed a summation of integrals over all trajectories which
may quickly become prohibitive. In a previous work [16]
where the density was two-dimensional, a grid based approach
was selected, which allows a very simple discrete convolution
formulation. Here, due to the higher dimensionality, a crude
extension of the method seems to yield an unacceptable
increase of both the computational cost and memory footprint.
However, it turns out that the problem is less complex than
expected as a result of the product form of the kernel. Starting
with the expression (16), the critical point in the evaluation of
the density at a given point x = (x1, x2, θ, s) is the sum:
N
X
i=1
Z 1
0
K (x, γi(t)) ⟨⟨γ′
i(t), γ′
i(t)⟩⟩1/2
γi(t)dt.
(29)
Using any classical quadrature formula, the integral may be
reduced to a ﬁnite sum, yielding a double sum:
N
X
i=1
Mi
X
j=1
wijK (x, γi(tij)) ⟨⟨γ′
i(tij), γ′
i(tij)⟩⟩1/2
γi(tij)
(30)
where Mi is the number of sample points tij chosen on
trajectory i and the wij are the quadrature weights. The
expression (30) is fully general, but a simpler choice is made
in practice: the sampling points tij are selected to be evenly
spaced and the weights all equal to 1. It is nothing but the
rectangle quadrature formula, whose accuracy is sufﬁcient for
the application in mind. Switching to a higher order formula
is straightforward. In (30), the evaluation of the kernel has the
highest cost since the norm ⟨⟨γ′
i(tij), γ′
i(tij)⟩⟩1/2
γi(tij) does not
depend on x and can be computed once for all. To compute the
density at a single point, the total number of kernel evaluations
is PN
i=1 Mi, with typical values of N = 100, Mi = 20 for the
analysis of a control sector to N = 10000, Mi = 100 in the
case of a country sized airspace. While acceptable in the ﬁrst
case, a direct application of the formula is not efﬁcient enough
in the second.
Recalling that the kernel K is a product of three elementary
kernels K = KtKoKs, it is clear that K will vanish outside
of the support of any of the three. As mentioned before, Kt is
selected to be an Epanechnikov kernel which is compactly
supported, so that K itself will vanish when the distance
between the translation components of x and γi(tij) is large
enough. The sum (30) will thus have almost all terms vanishing
if the bandwidth of Kt is adequately selected. Finally, using
the t superscript to denote the translation part of the points:
Kt(xt, γt
i(tij)) = 2
π ep

267
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
q) the number of bins in the rotation (resp. scale) component.
The domain of variation for the translation component is a
rectangle [a1, b1] × [a2, b2], the interval [0, 2π] for the rotation
and [s1, s2] for the scale. An elementary cell in the grid where
a given point (x1, x2, θ, s) lies can be determined using the
following procedure:
•
The block coordinates (i, j) is found from the couple
(x1, x2) as:
i = (m1 − 1)x1 − a1
b1 − a1
, j = (m2 − 1)x2 − a2
b2 − a2
.
•
Within the block Mi,j, the respective rotation and
scale indices (k, l) are obtained pretty much by the
same way:
k = (p − 1) θ
2π , l = (q − 1) s − s1
s2 − s1
.
Please note that all indices are zero-based, so that M0,0 is the
ﬁrst block in the matrix M. Actual elements in M are referred
to using quadruples (i, j, p, q), with the ﬁrst two components
designing the block and the remaining two locating the element
in the block.
As mentioned above, a beneﬁt of the binning procedure
is the ability to pre-compute the kernel values, since the
difference between any two grid points is known. As an
example, for the translation component, the value for the kernel
Kt can be stored as a m1m2 × m1m2 matrix Kt with entries
Kt
(i,j),(k,l) = Kt

268
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
B. Moving density computation to GPUs
There is an increasing interest in the numerical analysis
community for GPU computation. These massively parallel
processors, ﬁrst intended to perform tasks related to 3D scenes
display, have proved themselves very efﬁcient in problems
where it is possible to formulate the solution has a set of
asynchronous and independent tasks. Due to the high number
of processing units available, GPUs excel in many algorithms
coming from the ﬁeld of linear algebra, simulation, PDE
solving. In the clustering application described here, GPU
computing can leverage the efﬁciency of density computation
that leads naturally to parallel processing. Some care must be
taken however as simultaneous accesses to common memory
locations may impair the overall performance.
First of all, one can note that computation within a M
block, that is updating the rotation and scale part of the density
requires only the knowledge of the samples with translation
coordinates falling within the corresponding grid cell and is
independent of the computation made on another block. This
gives access to the ﬁrst level of parallelism. On most GPU
architectures, the computation may be organized in thread
blocks. It is the case within the CUDA programming model
of NVIDIA, and a thread block size of 16 × 16 was selected.
The size of the kernel grids in rotation and scale components
were chosen accordingly. To maximize the performance, the
corresponding block in the matrix M is ﬁrst copied to local
memory (designed as ”shared memory” in CUDA), then all
computation are performed on it within the given thread
block. At the end of the updating phase, the local memory is
transferred back to the global one. The storage needed for the
local block is 256 times the size of a ﬂoat, which yields a total
of 1Ko, well below the 48Ko limit of the CUDA architecture.
At the beginning of the density computation, a global
memory block representing the whole of matrix M is allocated
on the device and set to 0. One thread block (256 threads) is
dedicated to a single block in M, for a total of m1×m2×256
threads. Depending on the hardware and the choice made on
m1, m2, this value can exceed the maximum number of threads
allowed on a particular GPU. In such a case, the update is
performed on submatrices of the original matrix M. With the
typical values given previously, the maximal number of threads
of the GTX980 used for the development is not reached.
Using the GPU to affect the sample points γi(tij) to the
right block in M will not improve the performance. A better
choice is to use the CPU to perform the task, then to send the
processed array of samples to the GPU device.
A second level of parallelism will be to consider updates
of submatrices of blocks in M instead of single blocks. The
expected gain is small, except when more that one GPU are
present in the system. The implementation details are not given
here, trying to improve the overall algorithm being still a work
in progress.
C. Implementing the gradient descent algorithm
Once the density grid M has been computed, the im-
plementation of the gradient move is quite straightforward
and requires only the ability to estimate the ﬁrst and second
derivative on each trajectory. A very classical ﬁnite differences
scheme gives a sufﬁcient accuracy to obtain convergence on
most situations. It takes the form of the product of a matrix
Di with theNi ×4 matrix of samples (γi(ti1), . . . , γi(tiNi)) to
yield the matrix of derivative estimates (γ′
i(ti1), . . . , γ′
i(tiNi)).
Please note that the coordinates are put in columns, while
the samples are in row. Iterating the product with Di will
give rise to the second derivative. Generally speaking, Di is
obtained from the Lagrange interpolation polynomial and can
be constructed using the algorithm 2 (in the sequel d is the
degree of the interpolating polynomial).
Algorithm 2 Computation of the derivation matrix Di
1: Di ← 0
2: for k = 0 . . . Ni do
3:
offset = GETOFFSET(k)
4:
for j = 0 . . . N1 do
5:
Di[k, j + offset] =LAGRANGE(j,k,offset)
6:
end for
7: end for
8: function GETOFFSET(i)
9:
if i < d/2 then
10:
o ← 0
11:
else if i > Ni − d/2 − 1 then
12:
o ← Ni − d
13:
else
14:
o ← i − d/2
15:
end if
16:
return o
17: end function
18: function LAGRANGE(k,j,offset)
19:
w ← 1.0
20:
for a = 0 . . . d do
21:
if a ̸= k then
22:
w ← w ∗ (ti,k+offset − ti,a+offset)
23:
end if
24:
end for
25:
s ← 0.0
26:
for a = 0 . . . d do
27:
if a ̸= k then
28:
p ← 1.0
29:
for b = 0 . . . d do
30:
if b ̸= k ∧ b ̸= a then
31:
p ← p ∗ (ti,j − ti,b+offset)
32:
end if
33:
end for
34:
s ← s + p
35:
end if
36:
end for
37:
return s/w
38: end function
As mentioned before, integrals are computed using a
quadrature formula, that was chosen to the simplest one, with
all weights equal.
D. Results
In Figures 1 and 2, the problem of automatic conﬂict
solving is addressed. From an initial ﬂight plan, we have
generated a conﬂicting set of trajectories that are converging
to a single unsafe point. The median of the initial ﬂows
correspond to the ﬂight plan showed in Figure 1. Next, an
automated planner has proposed a solution by generating a
set of safe trajectories that are relatively complex and may

269
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
fail to be manageable by air trafﬁc controllers. In Figure 2,
the minimization entropy criterion has deformed the proposed
ﬂight paths and produced straighten trajectories with route-
like behavior. The median of the initial and the ﬁnal ﬂows are
represented in Figures 1 and 2.
Figure 1. Initial ﬂight plan.
Figure 2. Entropy minimal curve system from the initial ﬂight plan.
However, because the spatial density is not sensitive to the
directional information, the entropy based procedure will tend
to aggregate ﬂight paths that should be sufﬁciently separated
such that trajectories with opposite directions. The Lie group
modeling will take into account the direction and the position
of the curves and the algorithm works as expected, avoiding
going too close to trajectories with opposite directions as
indicated on Figure
3. Note that using the Lie approach
properly separates the two left ﬂight paths that have similar
shape but opposite directions.
In a more realistic setting, arrivals and departures at
Toulouse Blagnac airport were analyzed. The dataset used was
a a record of approximately 1700 trajectories between Paris
Orly and Toulouse Blagnac. The projection used focuses on
Blagnac and exaggerates the lateral deviation to enlighten the
ﬂuxes separation. The algorithm performs well as indicated
on Figure 4. Four clusters are identiﬁed, with mean lines
represented through a spline smoothing between landmarks.
It is quite remarkable that all density-based algorithms were
unable to separate the two clusters located at the right side
of the picture, while the present one clearly show a standard
Figure 3. Clustering using the Lie approach.
approach procedure and a short one.
Figure 4. Mean cluster trajectories at Toulouse airport (nautical miles). Red
arrivals and grey departures.
An important issue still to be addressed with the extended
algorithm is the increase in computation time that reaches 20
times compared to the approach using only spatial density en-
tropy. In the current implementation, the time needed to cluster
the trafﬁc presented in Figure 3 is in the order of 0.01s on a
XEON 3Ghz machine and with a pure java implementation.
For the case of Figure 4, 5 minutes are needed on the same
machine for dealing with the set of 1784 trajectories.
Finally, a similar procedure was used to obtain the so-called
bundled trafﬁc. Here the purpose of the algorithm is not to
cluster ﬂight paths, but instead simplify the picture so that an
operator is able to extract quickly the interesting features. The
heading information was not used in the experiments, since
the main goal was to extract the major ﬂows followed by the
aircraft so as to dimension the airspace sectoring. One picture
represents one day of recorded trafﬁc over France, with all low
altitude ﬂights removed: they correspond to general aviation
and are not to be considered in the design of the airspace.
Roughly 8000 trajectories are processed for one day. Since
there is no heading information, the execution time is greatly
reduced. Furthermore, the software was implemented here in
C++, resulting in a total processing time of 5 seconds on the
same machine as above.

270
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 5. Recorded trajectories for one day trafﬁc over France.
Figure 6. Bundled trajectories.
VI.
CONCLUSION AND FUTURE WORK
The entropy associated with a system of curves has proved
itself efﬁcient in unsupervised clustering application where
shape constraints must be taken into account. For using it in
aircraft route design, heading and velocity information must be
added to the state vector, inducing an extra level of complexity.
In our algorithm, we cannot enforce the regulatory separation
norms, just construct clusters with low interactions. Please note
that we can consider the current algorithm as a preprocessing
phase. In a second step, we could imagine running an algorithm
based on, for instance, optimal control in order to keep in line
with the minimum separation norms. The present work relies
on a Lie group modeling as an unifying approach to state
representation. It has successfully extended the notion of curve
system entropy to this setting, allowing the heading/velocity to
be added in a intrinsic way. The method seems promising, as
indicated by the results obtained on simple synthetic situations,
but extra work needs to be dedicated to algorithmic efﬁciency
in order to deal with the operational trafﬁc datasets, in the
order of tens of thousand of trajectories.
Moreover, the choice of the kernel bandwidth parameters
should be explored in the next step of this work. Indeed, as
it is noted in [2], kernel bandwidth values will inﬂuence the
effect of the minimization entropy procedure on the curve
straightening: straightening is preeminent for low values, while
gathering dominates at high bandwidths. An automatic proce-
dure in the choice of bandwidth parameter is then desirable and
an adaptive bandwidth procedure may be of some interest.
Generally speaking, introducing a Lie group approach to
data description paves the way to new algorithms dedicated
to data with a high level of internal structuring. Studies are
initiated to address several issues in high dimensional data
analysis using this framework.
REFERENCES
[1]
F. Nicol and S. Puechmorel, “Unsupervised aircraft trajectories cluster-
ing: a minimum entropy approach,” in ALLDATA 2016, The Second
International Conference on Big Data, Small Data, Linked Data and
Open Data.
Lisbon, Portugal: IARIA, 2016.
[2]
S. Puechmorel and F. Nicol, “Entropy minimizing curves with
application to ﬂight path design and clustering,” Entropy, vol. 18,
no. 9, 2016, p. 337. [Online]. Available: http://www.mdpi.com/1099-
4300/18/9/337
[3]
M. Enriquez, “Identifying temporally persistent ﬂows in the terminal
airspace via spectral clustering,” in ATM Seminar 10, FAA-Eurocontrol,
Ed., 06 2013.
[4]
M. El Mahrsi and F. Rossi, “Graph-based approaches to clustering
network-constrained trajectory data,” in New Frontiers in Mining Com-
plex Patterns, ser. Lecture Notes in Computer Science, A. Appice,
M. Ceci, C. Loglisci, G. Manco, E. Masciari, and Z. Ras, Eds. Springer
Berlin Heidelberg, 2013, vol. 7765, pp. 124–137.
[5]
J. Kim and H. S. Mahmassani, “Spatial and temporal characterization
of travel patterns in a trafﬁc network using vehicle trajectories,”
Transportation Research Procedia, vol. 9, 2015, pp. 164 – 184, papers
selected for Poster Sessions at The 21st International Symposium on
Transportation and Trafﬁc Theory Kobe, Japan, 5-7 August, 2015.
[6]
M. Ester, H. P. Kriegel, J. Sander, and X. Xu, “A density-based
algorithm for discovering clusters in large spatial databases with noise.”
AAAI Press, 1996, pp. 226–231.
[7]
N. Ferreira, J. T. Klosowski, C. E. Scheidegger, and C. T. Silva, “Vector
ﬁeld k-means: Clustering trajectories by ﬁtting multiple vector ﬁelds,”
in Computer Graphics Forum, vol. 32, no. 3pt2.
Blackwell Publishing
Ltd, 2013, pp. 201–210.
[8]
T. W. Liao, “Clustering of time series data - a survey,” Pattern
Recognition, vol. 38, 2005, pp. 1857–1874.
[9]
S. Rani and G. Sikka, “Recent techniques of clustering of time series
data: A survey,” International Journal of Computer Applications, vol. 52,
no. 15, August 2012, pp. 1–9, full text available.
[10]
F. Ferraty and P. Vieu, Nonparametric Functional Data Analysis: Theory
and Practice, ser. Springer Series in Statistics.
Springer, 2006.
[11]
J. Ramsay and B. Silverman, Functional Data Analysis, ser. Springer
Series in Statistics.
Springer New York, 2006.
[12]
W. Meesrikamolkul, V. Niennattrakul, and C. Ratanamahatana, “Shape-
based clustering for time series data,” in Advances in Knowledge
Discovery and Data Mining, ser. Lecture Notes in Computer Science,
P.-N. Tan, S. Chawla, C. Ho, and J. Bailey, Eds.
Springer Berlin
Heidelberg, 2012, vol. 7301, pp. 530–541.
[13]
A. Delaigle and P. Hall, “Deﬁning probability density for a distribution
of random functions,” The Annals of Statistics, vol. 38, no. 2, 2010,
pp. 1171–1193.
[14]
C. Bouveyron and J. Jacques, “Model-based clustering of time series
in group-speciﬁc functional subspaces,” Advances in Data Analysis and
Classiﬁcation, vol. 5, no. 4, 2011, pp. 281–300.

271
International Journal on Advances in Software, vol 9 no 3 & 4, year 2016, http://www.iariajournals.org/software/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[15]
S. Puechmorel, “Geometry of curves with application to aircraft trajec-
tory analysis.” Annales de la facult´e des sciences de Toulouse, vol. 24,
no. 3, 07 2015, pp. 483–504.
[16]
S. Puechmorel and F. Nicol, “Entropy minimizing curves with applica-
tion to automated ﬂight path design,” in GSI 2015, Second International
Conference, Palaiseau, France, October 28-30, 2015.
Springer, 2015.
[17]
D. Scott, Multivariate Density Estimation: Theory, Practice, and Visu-
alization, ser. A Wiley-interscience publication.
Wiley, 1992.
[18]
K. Mardia and P. Jupp, Directional Statistics, ser. Wiley Series in
Probability and Statistics.
Wiley, 2009.
[19]
K. V. Mardia, “Statistics of directional data,” Journal of the Royal
Statistical Society. Series B (Methodological), vol. 37, no. 3, 1975, pp.
349–393.
[20]
E. Garc´ıa-Portugu´es, R. M. Crujeiras, and W. Gonz´alez-Manteiga,
“Kernel density estimation for directional–linear data,” Journal of
Multivariate Analysis, vol. 121, 2013, pp. 152–175.
[21]
P. E. Jupp and K. V. Mardia, “Maximum likelihood estimators for the
matrix von mises-ﬁsher and bingham distributions,” Ann. Statist., vol. 7,
no. 3, 05 1979, pp. 599–606.
[22]
M. P. Wand, “Fast computation of multivariate kernel estimators,”
Journal of Computational and Graphical Statistics, vol. 3, no. 4, 1994,
pp. 433–445.
[23]
G.
Dahlquist
and
˚A.
Bj¨orck,
Numerical
Methods
in
Scientiﬁc
Computing:
Volume
1,
ser.
SIAM
e-books.
Society
for
Industrial
and
Applied
Mathematics,
2008.
[Online].
Available:
https://books.google.fr/books?id=qy83gXoRps8C

