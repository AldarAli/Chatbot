Decision-Theoretic Planning for Cloud Computing
Rafael Mendes, Rafael Weingartner, Guilherme Geronimo, Gabriel Br¨ascher
Alexandre Flores, Carlos Westphall, Carla Westphall
Department of Informatics and Statistics
Federal University of Santa Catarina
Florian´opolis, Brazil 88040-970
Email: {rafaeldesouzamendes,rafaelweingartner,guilherme.geronimo,gabrascher,alexandre.augusto.ﬂores}@gmail.com,
{westphal,carlamw}@inf.ufsc.br
Abstract—This paper presents a mathematical model of decision
planning for autonomic Cloud Computing based on the decision-
theoretic planning model. It uses Markov decision process on
the cloud manager to evaluate decisions and manage the Cloud
environment. Also, it contributes to the state-of-art of Cloud
Computing approaching the planning phase of the autonomic
process with a mathematical model, considering two important
factors, (1) the uncertainty of action’s results and (2) the utility of
the actions. Both factors are needed when dealing with complex
systems as a Cloud.
Keywords-cloud computing; decision-theoretic planning; auto-
nomic computing; self-management
I.
INTRODUCTION
The decision-theoretic planning (DTP) problems were ex-
tensively researched during the last decades. The main problem
with the decision-theoretic (or probabilistic) approach for the
planning phase in autonomic computing is the need to provide
extensive information about the transitions between system
states. However, with the arise of Cloud Computing (CC),
sensor networks and other technologies that enabled the moni-
toring and collection of large volumes of data, the information
became abundant and the recomendation of utility to solve the
contradictions between rules on large rule-based polices [1],
[2] must be taken seriously. On big data environments, the
DTP problems no longer exist, enabling its application for the
planning phase on the autonomic loop.
This work presents a model that plans actions for CC
management systems using a decision-theoretic approach. It
contributes to the state-of-art in CC research by:
•
(i) Adapting the decision-theoretic models, which was
based on Markov Decision Process (MDP), to use
in the planning phase of the autonomic management
loop;
•
(ii) Introducing decision-theoretic and MDP for plan-
ning in CC;
•
(iii) Applying mathematical models on a concrete
decision making scenario for self-conﬁguration of
CloudStack [3].
This paper is organized as follows. Section II presents an
overview of the concepts that are required to understand the
proposed model. Section III presents the related works. In
section IV is presented a conceptual proposal to guide the
decision mechanism to CC.The Sections V and VI discusses
and presents a mathematical model for the MDP approach,
presenting a study case scenario of a Cloud implementation
with CloudStack and Xen Cloud Platform. Finally, Section
VII concludes the paper and presents future works that will
improve the presented model.
II.
LITERATURE REVIEW
A. Cloud Computing
After some years, the deﬁnition of CC that has grown in
acceptance was created by NIST[4]:
Cloud computing is a model for enabling ubiqui-
tous, convenient, on-demand network access to a
shared pool of conﬁgurable computing resources
(e.g., networks, servers, storage, applications, and
services) that can be rapidly provisioned and released
with minimal management effort or service provider
interaction. This cloud model is composed of ﬁve
essential characteristics, three service models, and
four deployment models.
Another important contribution to CC also can be found in
[5, Section 3]; “Cloud Computing: the need for monitoring”,
where are stated some useful concepts to understand the
fundamental elements of a Cloud.
As stated in [6], to deal with a complex system like a
Cloud, it is necessary to be able to accurately capture its states.
Beyond the well-known CC characteristics, like on-demand
self-service, broad network access, resource pooling, rapid
elasticity, measured service, etc. [4], [7], it is important to
highlight the stakeholder heterogeneity characteristic. This
characteristic is poorly deﬁned and appears in some works
like stakeholder, actors or roles.
In [7] the stakeholders are deﬁned as roles:
•
Cloud Consumer;
•
Cloud Provider;
•
Cloud Auditor;
191
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

•
Cloud Broker ;
•
Cloud Carrier.
Litoiu at al. [8] presents four type of stakeholders: infras-
tructure providers, platform providers, application providers
and end users, although, it does not describe these stakeholders
roles. In the same paper [8], there is a change on the terms used
to present the stakeholders; the term actors is used instead of
stakeholders. It places the actors in function of service models:
Infrastructure as a Service (IaaS), Platform as a Service (PaaS)
and Software as a Service (SaaS); introducing a whole different
set of actors as layer owners, that are: IaaS owner, PaaS owner,
SaaS owner and end users.
Leimeister et al. [9] deﬁnes ﬁve actors in the CC value net-
work: Customer, Service providers, Infrastructure providers,
Aggregate services providers (aggregators), Platform provider
and Consulting.
In [10], it can be observed a different deﬁnition of roles
on Cloud environments. The work deﬁnes these roles as
stakeholders and present the following concepts: Consumers,
Providers, Enablers and Regulators.
Letaifa et al. [11] present a deﬁnition of actors and roles
in cloud computing systems as: Vendors, Developers and End
users.
In Tan et al. [12], although the work focus is adoption (or
not) of SaaS, it classiﬁes stakeholders in three categories: SaaS
infrastructure provider, SaaS provider and SaaS consumer.
There is no concise deﬁnition of CC stakeholders and inter-
ests. Furthermore, those distinct deﬁnitions may indicate that
each Cloud implementation requires a speciﬁc stakeholder’s
modeling.
B. Autonomic Computing
The Autonomic Computing (AC) concept was based on
the human nervous system, which regulates critical functions
such as heart rate and body temperature, in the absence of a
conscious brain [13]. AC systems have many common points
with Expert Systems (ES) but are less generic, applied to
management and control of wide computational systems, while
the ES are applied in a more generic way. The AC differs from
ES principally when it addresses the ”action taking”, that was
unusual in ESs, as stated in [14].
AC systems are based on MAPE-K control cycle, that
consists in Monitor, Analyse, Plan, Execute and Knowledge
elements, Fig. 1 shows the MAPE-K life cycle.
An autonomic system, as shown in [13], to be able to
perform self-management, must present four main abilities:
•
self-conﬁguration - the ability of conﬁgure itself ac-
cording to high-level policies;
•
self-optimization - the capacity of optimize its use of
resource;
•
self-protection - autonomic systems must protect itself
from malicious or incorrect user behavior;
•
self-healing - the ability of detect, diagnoses and ﬁx
problems.
Figure 1.
Structure of and autonomic agent.
In [2], the abilities were extended, adding four attributes
of autonomic systems:
•
self-awareness - the system must be aware of its
internal state;
•
self-situation - it should detect its current external
operating conditions;
•
self-monitoring - it has to detect changing circum-
stances;
•
self-adjustment - it has to adapt accordingly to external
or internal changes.
As stated in [15]:
The overall goal of Autonomic Computing is the
creation of self-managing systems; these are proac-
tive, robust, adaptable and easy to use. Such ob-
jectives are achieved though self-protecting, self-
conﬁguring, self-healing and self-optimizing activ-
ities ...To achieve these objectives a system must
be both self-aware and environment-aware, meaning
that it must have some concept of the current state
of both itself and its operating environment. It must
then self-monitor to recognize any change in that
state that may require modiﬁcation (self-adjusting)
to meet its overall self-managing goal. In more
detail, this means a system having knowledge of
its available resources, its components, their desired
performance characteristics, their current status, and
the status of inter-connections with other systems.
C. Markov Decision Process
Broadly speaking, it can be said that the planning tech-
niques developed in the Artiﬁcial Intelligence domain are
192
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

concerned to obtain a course of actions which conducts the
agent to a goal state or to an improvement in its condition.
In deterministic planning approaches, each action leads to a
single state. On the other side, the DTP is a non-deterministic
way of modeling the decision taken problem where each action
(or exogenous event) can lead the system state to more than
one possible states with a certain probability.
To deal with probabilistic non-determinism, many math-
ematical tools must be used. A common framework used
as underlying model to DTP is the MDP [16] that exposes
the probabilistic relation between the system’s states. Another
framework is the decision theory [17] which combines the
probability theory with utility theory.
In order to model the planning problem for a stochastic
dynamic system, it is necessary to present a basic problem
formulation using a MDP. This paper will model the problem
according to [16], that presents the follow key elements:
•
a set of decision epochs;
•
a set of system state;
•
a set of actions;
•
a set of transition probabilities (state X action);
•
a set of rewards or costs for transitions.
The
problem
can
be
mathematically
expressed
as
{T, S, As, pt(s′
t+1|s, a), rt(st, a)}, where S is the set of states
that the system can assume; As is the set of actions that can be
taken over the system at state s; pt(s′
t+1|st, a) is the transition
function that maps in time t a state s to a state s′
t+1, in time
t + 1, give an action a; rt(st, a) is function which gives the
reward for the execution of an action a on state st.
The Fig. 2 shows a graphical representation of a MDP,
where the green circles are the states, the red circles represents
the actions, the arrows are the transitions between states, the
numbers over the arrow are the probabilities to achieve a state,
and the numbers indicated by the yellow arrows are the reward
value of the transition.
Figure 2.
A graphical representation of MDP [18].
III.
RELATED WORKS
Werner et al. [19] propose an integrated solution for cloud
environment based on organization model of autonomous
agent. It introduced concepts of rules and beliefs that in this
paper can be compared with decision rules and transition
functions, respectively. Despite the afﬁnity in terms of goals
and some architectural elements, [19] used neither MDP nor
decision-theoretic approach and did not deepen the model to
a level that would allow some kind of mathematical or logical
inference.
The work in [8] presents a model that includes stake-
holders, goals, sensors and actuators. It also provides an
optimization and control module in the presented architecture
which works in a multi-layer approach. However, it did not
deepen to a level of inference and did not situate itself on AC
context.
In [20], the author describe a MDP focused on the self-
conﬁguration phase. It situates the CC management on AC
context, applying learning enforcement to deduce the action
to be taken. The main differences between [20] and this paper
is that the ﬁrst one maintains the focus just in management of
virtualized resource, as well as, not guiding the goals to meet
stakeholder’s interests.
A multi-level (physical and logical) resource manager is
proposed in [1] to self-optimize on AC context. However, the
paper uses utility functions regardless the uncertainty inherent
to CC. It does not consider the action probabilities in this
model.
An introduction of the term Autonomic Cloud Computing
was made on [21], however, the work transits just as a survey
and architecture proposal, without any inference model.
Sharma et al. [22] consider both utility and probability
in their decision model. However, the work just provides a
speciﬁc model that are based in a static set of actions to be
optimized.
IV.
STAKEHOLDERS, INTERESTS AND CLOUD
COMPUTING
This work introduces the idea of interests. Interests have
been implicitly referenced in many works that address the CC,
like [23]. It is relevant to explain stakeholder’s concerns in a
way that they lead the decisions on a cloud autonomic system.
Sharma et al. [22] presents two approaches on decisions for
dynamic provisioning: cloud provider centric and customer-
centric. The paper differentiates them as follows:
Cloud provider centric approaches attempt to maxi-
mize revenue while meeting an applications SLA in
the face of ﬂuctuating workloads, while a customer-
centric approach attempts to minimize the cost of
renting servers while meeting the applications SLA.
This deﬁnition states important aspects of decision on CC
management:
•
different types of stakeholders have different interests
over the Cloud;
193
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

•
any decision method inherently carries the ability to
beneﬁt the interests of some stakeholders and harm
another;
•
the stakeholder’s interests are not always reconcilable
given certain constraints of resources and service
demand;
Therefore, it is possible to postulate that an autonomic
system that intends to manage a Cloud must guide its decisions
in order to maximize the total satisfaction of the stakeholders.
The satisfaction function can have many forms. Here will
be considered just a function σ : S → R, where S is the set of
possible states of the Cloud, and R is the set of real numbers.
A. Interests on the Cloud
Any measurable variable may constitute the state of Cloud.
The set of variables that deﬁne a state s ∈ S can be something
really big. However, looking from the view of the Cloud’s
stakeholders, the indicators contained in the SLAs, SLOs and
in some operative constraints can help to reduce the sample
space. Therefore, the indicators which will compose a Cloud
state must be at most the intersection of variables speciﬁed in
all SLAs, SLOs and operative constraints. These indicators are
the measurable interests.
Once the measurable interest that will compose the cloud
state set S were selected, it is important to reﬁne the indicators,
reducing the amount of information, avoiding redundancy, and
adjusting the measures. This way, the states changes would be
compatible with the speed of the decision making system.
V.
A DECISION-THEORETIC MODEL TO CC
To introduce a decision-theoretic model in CC, it is neces-
sary to ﬁnd the elements of MDP on the Cloud.
A. MDP for CC
The ﬁrst element to be modeled is the decision epochs. The
idea of applying MDP to decision making in CC induces to
think in discrete time decisions (epochs). For continuous time
decisions, control theory have been more adherent.
Decisions on a Cloud environment may be taken in preset
time slots or triggered by events coming from the monitoring
system. The time slot must be adjusted to be large enough
that a selected action can be computed, executed and evaluated
before a next decision can be taken. The slot also must be short
enough that some important event is not missed. In any case,
the decision epochs will be treated as a time instant t.
The second element is the Cloud state, presented in
subsection
IV-A.
It
can
be
deﬁned
as
a
tuple
s
=
(x1, x2, · · · , xn), where x1, x2, · · · , xn are values of random
variables X1, X2, · · · , Xn that compose the measurable inter-
ests set. The set of all possible tuples will be expressed as the
set S of the Cloud’s states.
The third element is a set of possible actions to be taken in
each state s ∈ S, that can be represented as the set As ⊆ A,
where A is the set of all possible actions to be taken over the
Cloud in any state.
The fourth item to be modeled is the probability distribu-
tion pt(s′
t+1|st, a), that express the probability of the system
to assume the state s′
t+1 ∈ S at time t + 1 giving that was
executed the action a ∈ As at state st in time t.
At last, the ﬁfth element is a real-valued reward function
rt : S × A → R (in another form: rt(st, a)) that gives
the reward received by the decision maker at time t for the
execution of the action a in state st.
B. Histories, Decision Rules and Policies
Considering that at time 1 an action a1 executed over a
state s1 will generate a state s2 at time 2, it is plausible
to say that after a time t there will exist a history ht =
(s1, a1, s2, a2, · · · , st). Being the Ht the set of all possible
histories, that will be characterized by the Cartesian product
{S × A × S × A × · · · × A × S} = {S × A}t−1 × S, it is
possible to say that the history ht constitute an observation of
system states and actions.
A decision rule for CC can not be memory less. Likewise,
must be history dependent. Also, in a real Cloud management,
the rule should be non stationary, in other words, itself will
change over time. Therefore, the decision function to the epoch
t ( dt : Ht → Ast) when receiving a decision history ht returns
an action a. In another form: dt(ht) ∈ Ast.
The speciﬁc decision rules implemented will be described
in the Section VI.
As depicted in Section II, a policy provides to decision
maker, a prescription for an action selection under any possible
future system state. Thus, on a Cloud where the decision rules
are time and history dependent, there will a policy which will
look like: π = (d1, d2, d3, · · · , dt).
Clearly, when the decision horizon is inﬁnite (as in CC), it
is impractical to compute and evaluate all decision possibilities
for all future states that the Cloud may assume in time.
Although it has a ﬁnite horizon for planning, the size of S
and A may result in a really hard work, since the number of
policies is {S × A}N−1 × S, where N is the time horizon to
compute.
VI.
MATHEMATICAL MODELING
A. Study Case: the Cloud
1) Cloud environment: The Cloud used to ground the
experiments was a CloudStack [3], using Xen Cloud Platform
[24] and Xen hypervisor [25], [26] at the bare layer.
The Cloud Lab can be summarized as follows:
•
Top level, cloud management – applying Cloudstack
over the infrastructure to manage not just the hardware
and software pieced together, but also to provide an
easy and user friendly way to create, destroy and
update resources on the ﬂy;
•
Underlying structure, hypervisor level – the core struc-
ture that is responsible for running the VMs. It is based
on Ubuntu server 12.10 64bits, on which was installed
and conﬁgured the Xen hypervisor 64bits hypervisor
version 4.1 and Xen Cloud Plataform (XCP) version
194
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

TABLE I.
CLOUD SERVERS CONFIGURATIONS
Name
CPU
Memory
Role
Cluster
Server 1
Pentium D @ 3.4Ghz
2 GB
Storage server
-
Server 2
Core 2 E6600 @ 2.4Ghz
7GB
Processing server
PV
Server 3
Core 2 quad, Q8200 @2.3Ghz
4GB
Processing server
PV
Server 4
Xeon E5405 @ 2Ghz
8GB
Processing server
HVM
Server 5
Phenom 9650 @ 2.3Ghz
4GB
Processing server
HVM
Server 2
Phenom II 965 @ 3.4Ghz
4GB
Processing server
HVM
1.6. Since the platform has a heterogeneous server
environment, this level was then subdivided into two
clusters, a cluster that contains the physical hosts with
hardware virtual machine capabilities (HVM) and a
pure virtual cluster (PV) that contains all the servers
that does not have support for HVM machines;
•
Bottom level structure, storage level – at the lower
level of the structure there is the storage, on which
was built upon a Raid controller. The Raid controller
exports 3 volumes to the storage server, a total of
3.18 Terabits using either RAID1 or RAID5, granting
at least reliability against hard drive failures. The
volumes exported by the RAID controller are mounted
on a passive storage server as XFS ﬁle system and then
exported the XFS partitions as network ﬁle system
(NFS).
Table I shows the hardware conﬁgurations of each server
in the Cloud, and the role that it plays.
2) Cloud Stakeholders: There are three types of stakehold-
ers interested in the Cloud environment:
•
the cloud decision committee – that is interested in
the maximization of resource usage and the equipment
aggregation of research departments by accession to
Cloud, in the scope of the Federal University of Santa
Catarina, Brazil;
•
the cloud managers – which have interest in satisfying
the concerns of cloud decision committee with avail-
ability assurance and energy economy;
•
the researchers – having interests that their VMs stay
available and maximize their capacity of memory,
CPU and storage.
3) Cloud problems: From the stakeholders interests it is
possible derive some decision problems:
1)
Structure Aggregation: It is necessary to decide if a
PM can integrate the cloud or not;
2)
Maximize the resource usage: It is necessary to add
and remove resource (CPU, memory) to idle and
overloaded VMs;
3)
Ensure availability: It is necessary not to lose user
requests, that can be characterized as network pack-
ages.
4)
Energy Economy: It is necessary to minimize the
energy consumption;
5)
Obtain availability and maximize the capacities of
VMs: the decision needs are covered on items 3 and
2.
These decision problems have consequences when placed
side-by-side. Although it is possible to provide more resource
to VMs via hypervisor, the operation system will not recognize
the additional resource, to add or remove them when it is
necessary stop and restart the VMs. This way, to provide
availability, it is critical to execute these operations in a
moment with low probability of request loss.
To achieve energy economy, it is important to shutdown
PMs, but, it is necessary to execute VM migrations at a
moment when the request loss is not likely to happen. The
aggregation of new equipments can be made at any time if
there exists a good link quality between the new host and the
core of the cluster where it will be added.
B. The model
To get a mathematical modeling that covers the study case,
it is necessary to model four basic structures, which will be
presented below.
1) Cloud State: In order to cover stakeholders interests,
there was selected a set of basic metrics to compose the Cloud
states as follows:
•
the PM state – on or off – SP M = {on, on};
•
the PM energy consumption – level of consumption –
SE = {low, average, high};
•
VM state – up or down – SV M = {up, up};
•
CPU
–
level
of
CPU
utilization
–
Scpu
=
{idle, underused, welldimensioned, overloaded};
•
memory – level of memory utilization – Smem =
{idle, underused, welldimensioned, overloaded};
•
network link utilization– level of link utilization –
Slu = {idle, underused, wellused, overloaded};
•
network link quality – level of link quality – Slq =
{poor, regular, good};
Given the sets PM of all PMs in the Cloud, V M of all
created VMs, it is possible to deﬁne the following sets to
compose the Cloud state:
•
ST P M = PM × SP M, of each PM state;
•
SEP M = PM × SE, of each PM energy consump-
tion;
•
SCP M = PM × Scpu, of PM CPU utilization;
•
SM P M = PM × Smem, of PM memory utilization;
•
LU P M = PM × Slu, of PM network connection
utilization;
•
LQP M = PM × Slq, of PM network link quality;
•
ST V M = V M × SP M, of each VM state;
•
SCV M = V M × Scpu, of VM CPU usage;
•
SM V M = V M × Smem, of VM memory utilization;
•
LU V M = V M × Slu, of VM network connection
utilization;
•
N P M, the number of PMs in the Cloud.
195
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

This way, there exists a set that can cover all possible Cloud
state is the set S = ST P M × ST E × SCP M × SM P M ×
LU P M × LQP M × ST V M × SCV M × SM V M × LU V M ×
N P M, where a state can be represented at time t as st ∈ S.
The set of states in this example cannot be stationary, once
VMs may be created or destroyed and PMs can be added or
removed.
2) Actions: It is the set of base actions that can be executed
on the environment that is being used to run the experiments:
•
a1 – turn on PM;
•
a2 – turn off PM;
•
a3 – turn on VM;
•
a4 – turn off VM;
•
a5 – migrate VM;
•
a6 – scale up VM;
•
a7 – scale down VM;
•
a8 – admit PM;
•
a9 – refuse PM;
•
a0 – no action;
It will be considered as different action, the actions ex-
ecuted on distinct resource (e.g., a1(pm1)). This way, the
set of action will be larger than the nine basic actions listed
above. However, the actions cannot be executed in any state
of a resource. For instance, a PM in up state cannot be
turned off. To get the set of all possible actions which can
be performed in state st, there will be deﬁned a function
αt : S → 2A that receives the state of the Cloud and returns a
set of actions, where 2A is the power set of A set. In another
form αt(st) = Ast. As the Cloud state, α is not a stationary
functions and can change in time.
3) Transition
Function:
Considering
a
state
st
⊃
{ST P M1 = on} over which the action aP M1
2
is executed, it
will be induced to a state s′
t+1 ⊃ {ST P M1 = off ∪LQP M1 =
bad} with a probability x, if there is demand to PM1, and a
state s′′
t+1 ⊃ {ST P M1 = off ∪ LQP M1 = {regular, good}}
with probability 1 − x otherwise, depending on whether the
machine is on or off, and how much requests exits to its VMs.
The state and action relations must be captured by the
transition function. It must provide the probability that an
action a, executed on state st, results in a state s′
t+1, for each
time t. In other words, pt(s′
t+1|a, st).
On a real-world CC management application, the transition
functions will be the product of a series of machine learning
and forecasting methods. In this paper, we will only assume
that there is a non-stationary probability function.
4) Reward Function: Giving the satisfaction evaluation
function described in Section IV, here presented as v = σ(st),
it is possible to establish a reward function that evaluates
the decision maker reward to lead the system from a state
st to a state st+1. It can be expressed as: rt(st, a, st+1) =
σ(st+1) − σ(st).
When a decision maker recognizes the Cloud in state st
and evaluates the impact of action a over that state, it does
not have the resultant state st+1. This way, it is necessary to
introduce the uncertainty, as seen on (1).
rt(st, a) =
X
st+1∈S
r(st, a, st+1).pt(st+1|s, a)
(1)
It is assumed that each stakeholder k ∈ K has a set of
interests Ik ⊆ I, each ik ∈ Ik having a weight wik. Therefore,
if there exists a function σIk : S → R that evaluates the Cloud
state according to the interest i of stakeholder k, it is possible
to propose a weighted interest function σk as presented in (2).
σk =
X
ik∈Ik
wik · σIk(st)
(2)
The σ function presented above can be obtained by the
sum of all weighted interests, as presented by (3).
σ =
X
k∈k
σk(st)
(3)
The expected reward is computed as presented in (1).
C. Decision rules
It is simple to elaborate a decision rule for one MDP epoch.
All is needed is the selection of the action that has the best
expected reward (like in (1)). This way, the decision rule to
t = 1 must return the set of the best actions as shown in (4).
A∗
t = arg max
a∈A
{
X
st+1∈S
r(st, a, st+1).pt(st+1|st, a)}
(4)
Nevertheless, to compute the expected reward for a policy
πN
t , where t is the actual time and N is the horizon of the
policy, it is necessary to sum the product of all rewards that
achieve a ﬁnal state and the probability to achieve each state,
like in (5).
r(πN
t ) =
X
hN
t ∈HN
t
(σ(sN) − σ(st)) · p(sN)
(5)
The p(sN) can be computed from the sum of all proba-
bilities of histories p(hN
t ) that start on st and ﬁnishes on sN.
Considering XN(hN
t ) = sN as a random variable that returns
the state N of a history, it is possible observe in (6) how to
compute a state probability.
p(sx) =
X
hN
t ,XN=sx
p(hN
t )
(6)
The probability of a history can be calculated by (7).
p(hN
t ) = p(st+1|st, at) ×
N−1
Y
i=t+1
p(si+1|si, ai)
(7)
196
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

VII.
CONCLUSION AND FUTURE WORKS
This article has presented a decision-theoretic modeling to
decision making for CC management in an AC context, using
the MDP as a mathematical framework.
This paper contributed to the state-of-the-art in CC research
in the sense that it tackles the phase planning of an autonomic
cycle with a mathematical model which takes into consider-
ation the uncertainty of action resulting in complex systems
such as a CC management systems.
For future work, the following steps will be considered:
•
A big data model to feed the transition function
created with the monitoring data bases;
•
Extend the CloudStack to implement the model on its
resource manager and perform experiments to observe
the performance of the model in taking decisions;
•
Analyze a meta-management model to optimize the
autonomic planner;
•
Research methods of action discovery and learning.
REFERENCES
[1]
W. Walsh, and G. Tesauro, and J. Kephart, and R. Das, “Utility functions
in autonomic systems”, in: Autonomic Computing, 2004. Proceedings.
International Conference on, May 2004, pp. 70–77.
[2]
S. Dobson, and R. Sterritt, and P. Nixon, and M. Hinchey, “Fulﬁlling the
vision of autonomic computing”, Computer, vol. 43, no. 1, Jan. 2010,
pp. 35–41.
[3]
Apache Foundation. Apache CloudStack, 2013 retrieved in September
2013 from http://cloudstack.apache.org/
[4]
P. Mell and T. Grance, The NIST Deﬁnition of Cloud Computing,
Tech. rep., National Institute of Standards and Technology, Information
Technology Laboratory, Jul. 2009.
[5]
G. Aceto, and A. Botta, and W. de Donato, and A. Pescap`e, “Cloud
monitoring: A survey”, Computer Networks, vol. 57, issue 9, Jun. 2013,
pp. 2093–2115.
[6]
A. Viratanapanu, and A. Hamid, and Y. Kawahara, and T. Asami, “On
demand ﬁne grain resource monitoring system for server consolidation”,
Kaleidoscope: Beyond the Internet? - Innovations for Future Networks
and Services, 2010 ITU-T, Dec. 2010, pp. 1–8.
[7]
R. B. Bohn, and J. Messina, and F. Liu, and J. Tong, and J. Mao, “Nist
cloud computing reference architecture”, in: Services (SERVICES), 2011
IEEE World Congress on, July 2011, pp. 594–596.
[8]
M. Litoiu, and M. Woodside, and J. Wong, and J. Ng, and G. Iszlai, “A
business driven cloud optimization architecture”, in: In Proceedings of
the 2010 ACM Symposium on Applied Computing (SAC ’10), 2010, pp.
380–385.
[9]
S. Leimeister, and M. Bhm, and C. Riedl, and H. Krcmar, “The business
perspective of cloud computing: Actors, roles and value networks”, in: In
Proceedings of the 7th international conference on Economics of grids,
clouds, systems, and services (GECON’10), 2010, pp. 129–140.
[10]
S. Marston, and Z. Li, and S. Bandyopadhyay, and J. Zhang, and
A. Ghalsasi, “Cloud computing - The business perspective”. Decis.
Support Syst. vol. 51, no. 1, Apr. 2011, pp. 176–189.
[11]
A. B. Letaifa, and A. Haji, and M. Jebalia, and S. Tabbane, “State of the
Art and Research Challenges of new services architecture technologies:
Virtualization, SOA and Cloud Computing”, International Journal of Grid
& Distributed Computing, vol. 3, issue 4, Dec. 2010, pp. 69–88.
[12]
C. Tan, and K. Liu, and L. Sun, “A design of evaluation method for saas
in cloud computing”, Journal of Industrial Engineering and Management,
vol. 6, no. 1, Feb. 2013, pp. 50–72.
[13]
J. Kephart and D. Chess, “The vision of autonomic computing”,
Computer, vol. 36, no. 1, Jan. 2003, pp. 41–50.
[14]
S. Gutirrez and J. Branch, “A comparison between expert systems and
autonomic computing plus mobile agent approaches for fault manage-
ment”, DYNA, vol. 78, no. 168, Aug. 2011, pp 173–180.
[15]
R. Sterritt and D. Bustard, “Autonomic computing - a means of
achieving dependability?”, in: Engineering of Computer-Based Systems,
2003. Proceedings. 10th IEEE International Conference and Workshop
on the, Apr. 2003, pp. 247–251.
[16]
M. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic
Programming, Wiley Series in Probability and Statistics, Wiley, 2009.
[17]
B. Lindgren, Elements of decision theory, Macmillan, 1971.
[18]
Wikipedia, Markov Decision Process, 2013 retrieved in September 2013
from http://en.wikipedia.org/wiki/Markov decision process
[19]
J. Werner, and G. Geronimo, and C. B. Westphall, and F. L. Koch,
and R. Freitas, C. M. Westphall, “Environment, services and network
management for green clouds”, CLEI Electron. J. vol. 15, no. 2, Aug.
2012, pp 2–2.
[20]
J. Rao, Autonomic Management of Virtualized Resources in Cloud
Computing, Ph.D. thesis, Wayne State University, Jan. 2011.
[21]
R. Buyya, and R. Calheiros, and X. Li, “Autonomic cloud computing:
Open challenges and architectural elements”, in: Emerging Applications
of Information Technology (EAIT), 2012 Third International Conference
on, Sep. 2012, pp. 3–10.
[22]
U. Sharma, Elastic resource management in cloud computing platforms,
Ph.D. thesis, University of Massachusetts, May 2013.
[23]
D.
Durkee, “Why Cloud Computing Will Never Be Free”, Queue
Journal vol. 8 no. 4, Apr. 2010, pp. 20-29.
[24]
The Linux Foundation, Xen Cloud Platform, June 2013 retrieved in
September 2013 from http://www.xenproject.org/downloads/xen-cloud-
platform-archives.html
[25]
The Linux Foundation, Xen Hypervisor, 2013 retrieved in September
2013 from http://www.xenproject.org/users/virtualization.html
[26]
P. Barham et al., “Xen and the Art of Virtualization”, in: Proceedings of
the nineteenth ACM symposium on Operating systems principles, Oct.
2003, pp. 164–177.
197
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-318-6
ICN 2014 : The Thirteenth International Conference on Networks

