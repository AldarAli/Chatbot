Smart Shopping Cart Learning Agents 
 
Dilyana Budakova 
Department Of Computer Systems and 
Technologies 
Technical University of Sofia, Branch 
Plovdiv, Plovdiv, Bulgaria 
email: dilyana_budakova@yahoo.com  
Lyudmil Dakovski   
Department of Communication & 
Computer Technologies 
European Polytechnic University 
Pernik, Bulgaria 
email: lyudmil.dakovski@epu.bg  
Veselka Petrova-Dimitrova  
Department of Computer Systems and 
Technologies  
Technical University of Sofia, Branch 
Plovdiv, Plovdiv, Bulgaria  
email: vesi_s_petrova@yahoo.com  
 
Abstract—The paper describes the design, implementation and 
user evaluation of utility-based and goal-based intelligent 
learning agents for smart shopping cart. In keeping user’s 
shopping habits or user’s shopping list, they guide visitors 
through the shops and the goods in the shopping center or 
according to new promotions in the shops, respectively. It is 
envisaged that concrete implementation of the shopping agents 
will be running on each shopping cart in the shopping centers 
or on holographic displays. The k-d decision tree, the best 
identification tree, and reinforcement-learning algorithm are 
used for agents learning. The task environment is partially 
observable, cooperative, deterministic, and a multi - agent 
environment, with some stochastic and uncertainty elements. It 
incorporates text-to-speech and speech recognizing technology, 
Bluetooth low energy technology, holographic technology, 
picture exchange communication system. Machine learning 
techniques are used for agents modeling. This kind of 
intelligent system enables people with different communication 
capabilities to navigate in large buildings and in particular to 
shop in the large shopping centers and maximize user comfort. 
Some initial user opinions of the shopping cart agents are 
presented. Different embodiments of the shopping agents are 
discussed like holographic agent embodiment, embodied 
virtual agent or social robot embodiment. Some approaches of 
realization of a smart robotic shopping cart that can follow the 
user are discussed too. The performance of the Q learning 
algorithm with an introduced environment measures model (a 
model of the environment criteria) is proposed and explored. 
Study of the learning parameter is presented. Smart Shopping 
Cart Learning Agents modeling and development task allows 
for applying and improving the learning algorithms. 
Keywords-smart shopping cart learning agent; machine 
learning; reinforcement learning; Q learning; decision tree; 
identification tree; ambient intelligence; holographic technology; 
beacon-based technology; assistive technologies. 
I. 
 INTRODUCTION 
In big and unfamiliar indoor spaces, such as shopping 
centers, airports, stadiums, hotels, office buildings, people 
may have difficulties with finding the desired destination. 
Many categories of people – the elderly, the children, the 
people with visual or hearing impairment, with difficulties in 
communication 
etc. 
– 
need 
specialized 
ways 
of 
communication [1][2][3][4]. This paper presents modeling, 
implementation and user evaluation of three intelligent 
learning agents for smart shopping cart. They guide visitors 
through the shops and the goods in the shopping center 
according to users’ shopping habits, user’s shopping list or 
according to new promotions in the shops, respectively [1]. It 
is envisaged that concrete implementation of the shopping 
agents will be running on each shopping cart in the shopping 
centers or on holographic displays [1]. The paper is inspired 
by [1] presented at the Cognitive 2019 conference. 
The task environment incorporates text-to-speech and 
speech recognizing technology, Bluetooth low energy 
technology, holographic technology, information kiosks, 
picture exchange communication system.  
The rest of the paper is structured as it follows: in Section 
II the technologies that the task environment incorporates are 
briefly discussed; in Section III the task environment 
specification, including performance measure, properties, 
environment actuators and sensors description is presented; 
the agent programs realization of the goal-based learning 
agent, personal utility-based learning agent and utility-based 
learning agent by means of a decision k-d tree, identification 
tree and reinforcement-learning are explained in Section IV; 
the degree of development of the proposed cognitive 
architecture components is explained in Section V; an 
empirical survey about the interest of end customers to the 
used technologies;  a survey about the way the customers 
perceive the three developed agents; a survey about users’ 
opinion and possibilities for shopping agents embodiments 
and user following smart shopping cart realizations; a survey 
of the performance of the Q learning algorithm with an 
introduced environment measures model (a model of the 
environment criteria) and  study of the learning parameter ϒ 
are considered in Section VI; A section for future work is 
Section VII; in the VIIIth Section a number of conclusions 
are drawn. 
II. 
BACKGROUND TECHNOLOGY USED FOR TASK 
ENVIRONMENT 
Beacons are used to mark the location of objects and 
navigate people in indoor spaces [5][6][7][8][9]. They work 
on the principle of lighthouses by emitting signals at short 
intervals based on Bluetooth Low Energy (BLE) technology. 
The distance to the Beacon can be defined depending on the 
signal strength [10]. In addition to emitting advertising or 
other types of announcements, it is also possible to locate 
beacons [5][7][9].  
109
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Holograms are made of light and sound, appear in the 
around space and reply to gestures, voice and gaze 
commands [11]. A hologram can be placed and integrated in 
the real world or can tag along with user as an active part of 
user’s world helping for navigation in indoor spaces. 
Another possible solution to the problem of orientating 
people in indoor spaces is the use of embodied 
conversational information kiosks [12][13]. These systems 
use the information they have both about their own location 
and about the layout of the building and give instructions to 
the users how to find the desired place in the building.   
The information kiosks are a collection of different 
technologies such as video processing from face detection, 
speaker-independent speech recognition, array microphone 
for noise cancellation, a database system, and a dynamic 
question answering system [13][14]. 
The Picture Exchange Communication System (PECS) 
[15][16] allows people with little or no communication 
abilities to communicate using pictures. People using PECS 
are taught to approach another person and give them a 
picture of a desired item in exchange for that item [17][18].  
Screen readers [19][20][21] and text-to-speech (TTS) 
systems [22] enable blind and vision impaired people to use 
computers and provide the key to education and 
employment.   
According to [23], the first step in designing an agent 
must always be to specify the task environment as fully as 
possible. That includes performance measure, environment 
actuators and sensors description. That’s why we will 
consider smart shopping problems, task environment 
specifying and shopping learning agents modeling in the 
next section.  
III. 
SPECIFYING THE TASK ENVIRONMENT 
It is envisaged that shopping agents will be implemented 
on the shopping cart. The consumers will run their cart 
following the directions given by the agents. In the future, 
the shopping agents can be implemented on a robotic 
shopping cart like an autonomous Kuka robot that can be 
controlled by gestures [24][25][26][27][28] to follow the 
user. Then, the environment will become very complex and 
similar to the environment of the automated driver. 
The modulus of the system prototype is given in Figure 
6. The task environment consists of four main blocks: input, 
output, shopping, and navigation.  
The technologies, used in the input block, are face 
detection and speech recognition. The equipment comprises 
a camera, a microphone, a keyboard, a mouse, and a touch 
screen. The general object detection algorithm consisting of 
a cascade of classifiers proposed by Viola and Jones [29] is 
used to detect faces. For video processing, C# and Intel 
OpenCV library [30] is used. 
The output block uses speech synthesis and virtual 
character visualization for giving information to the user.  
 The shopping block includes: drag and drop pictures for 
creating the shopping list (Figure 4); pictures-to-speech 
convertor;  
The navigation block includes: Beacons/iBeacons or/and 
Holograms for smart buildings, smart shopping mall 
navigation. Using of Google Beacon Platform or/and 
Microsoft HoloLens respectively is needed. 
Agent programs include goal-based learning agent, 
utility-based learning agent and personal utility-based 
learning agent realization by means of reinforcement-
learning, decision k-d tree and identification tree building. 
A. Performance Measure  
The performance measure, to which the shopping agents 
are aspired include getting to the correct shop in the 
shopping mall; getting to the new promotion in the shopping 
mall; minimizing the path when going through the shops 
from the shopping list; maximizing passenger comfort; 
maximizing purchases; and enabling people with different 
communication possibilities to navigate in big buildings and 
in particular to shop in the big shopping centers. 
B. Environment 
Any shopping agent deals with a variety of shops in the 
shopping malls; the newest promotion could be in each and 
any of the shops in a mall; the agents can recommend 
visiting the shops in a mall in various sequences. An option 
is to visit all desired shops following the shortest possible 
way. Another option is to go around the shops in accordance 
with the arrangement of the items on the shopping list or in 
accordance with shopping habits of the user. A third option 
is to go to the shops in accordance with the availability of 
sales or new promotions. The location of the shops in an 
exemplary Mall is given in Figure 1. The model of the 
environment in Figure 3 is presented in the form of a graph. 
The nodes are the shops and the edges are the connecting 
corridors.   
Figure 1.  Exemplar location of eight shops in a shopping center.  
C. Actuators 
The shopping agents are visualized on the display screen. 
Only the head of an agent is modeled by means of the 
program 
Crazy 
Talk. 
Face 
animation 
includes 
synchronization of the lip movement with the pronounced 
text and expressed emotions. The agents’ faces normally 
express friendliness and calmness and when a new 
promotion or sale is announced they express excitement and 
joy. The emotions of elevation are realized through changing 
the strength and the height of the speech and by visualizing a 
model of the emotion “joy” on the face.  
 
 
 
 
 
 
 
 
 
 
 
 
110
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

The shopping agents can be realized as holographic 
agents. In this case they will be visualized on holographic 
displays. Then not only the heads of the agents will be 
modeled, but also their bodies, their gestures, clothes, as well 
as the way of keeping the appropriate distance to the 
consumer;   
D. Interaction and Sensors 
For interaction both with the intelligent agents and the 
consumers are used: Keyboard entry; Microphone; Touch 
Screen; Camera – OpenCV, Face Detecting; Natural 
Language Understanding; Speech recognizing; drag-and-
drop 
pictures, 
pictures 
to 
speech 
convertor; 
Beacons/iBeacons or/and Holograms for smart shopping 
mall navigation. 
E. Properties of a Task Environment 
The behavior of the three intelligent agents is mutually 
complementary. They aim at facilitating the user access to 
the desired commodities and increasing the number of 
purchases, made by him/her, as well as at offering 
information about promotions and sales, in which he/she is 
interested.   
The agent does not know when a new promotion or a 
new customer will appear. Therefore he/she periodically 
checks on the site of the mall if there are files, containing 
information about new promotions or sales and reads them if 
available. Then, he/she transmits this information to the 
customers, planning to visit the corresponding shops. 
Whenever a new customer appears, the agent receives his/her 
shopping list and defines the sequence for visiting the shops 
in the mall. That’s why task environment is partially 
observable, cooperative and a multi-agent environment. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  Shop k-d decision tree. 
 
 
 
 
 
 
 
Figure 3.  Presentation of the location of the shops in an exemplary mall 
by an orientated graph.  
The shopping world is also deterministic with some 
stochastic elements and contains elements of uncertainty of 
the environment. The task environment is episodic and it can 
be realized either as static or as semi-dynamic environment. 
The environment can be regarded as static as the location of 
the shops in the mall is known. The agent receives the whole 
shopping list and suggests a certain path around the shops. 
Whenever there is information about a new promotion or 
sale appearing during the shopping, the agent can 
dynamically recommend a change in that shopping sequence.  
The environment can be regarded as both known and 
sequential because every next shop to visit is determined by 
the current location of the user and by the items he/she has 
pointed at as important to buy. 
IV. 
SMART SHOPPING LEARNING AGENTS MODELING 
Three software agents have been realized. The first one is 
a utility-based learning agent, the second is a goal-based 
learning agent, while the third is personal utility-based 
learning agent.  
A. Utility-Based Learning Agent 
One of the agents can be regarded as a Utility-based 
agent. That is because it feels happy when discovering that 
there is a promotion or a sale in a shop, in which the 
customer is interested to go.  
The utility-based agent uses a decision k-d tree to quickly 
find where, (in which shop) the customer is located 
according to his/her coordinates. The theory of building and 
implementing a decision k-d tree is given in [31]. The 
customer is depicted in Figure 1 by means of an emoticon, 
which can be moved using the mouse and placed everywhere 
on the shown map of the shops in the shopping center. 
Another way of finding the location of the customer is by 
using estimate beacons sensors or holograms.  
The Utility-based agent checks if there are new files 
about promotions or sales published on the site of the 
shopping center. In case there are such files, it withdraws 
them and informs the customer about those of them, which 
are related to the shops the customer intends to visit.  
The information about promotions and sales is given to 
the customer also in the case when it can be seen from the 
shopping list that the customer has planned to visit a 
particular shop where there is a promotion or a sale.  
The 
customer 
receives 
notifications 
about 
promotions/sales when he/she goes past a beacon as well.  
 
    
  
Figure 4.  Making a shopping list by dragging and dropping pictures. 
3 
1 
4 
5 
6 
7 
0 
2 
Y 
 
0.
 
2.
 
4.
 
6.
 
 
 
 
 
1.
 
 3.
 
5.
 
7.
 
 
 
 
 
 
 
 
           X 
 
 
 
 
 
 
 
 
 
 
 
111
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

1) Decision k-d Tree Realization 
In order to build the decision tree, the location of the 
eight shops in the exemplary shopping mall, given in Figure 
1, is considered. As it is described in [31][32] all shops are 
divided first by width alone into two sets, each with an equal 
number of shops. Next each of the two sets is divided by 
heights alone. Finally, each of those four sets is divided by 
width alone, producing eight sets of just one block each. The 
shop sets are divided horizontally and vertically until only 
one block remains in each set as it is shown in Figure 2. The 
overall result is called a k-d tree, where the term k-d is used 
to emphasize that the distances are measured in  
k-dimensions.  
Finding the nearest block is really just a matter of 
following a path through a decision tree that reflects the way 
the objects are divided up into sets. As the decision tree in 
Figure 2 shows, only three one–axis comparisons are 
required to guess the shop, in which the user is positioned. 
In general [31], the decision tree with branching factor 
k=2 and depth d=3 will give 23= 8 leaves (shops in our task). 
Accordingly, if there are n shops (or goods, or users) to be 
identified, d will have to be large enough to ensure that 2d≥n. 
Then, the number of comparisons required, which 
corresponds to the depth of the tree, will be of the order of 
log2n.     
B. Goal-Based Learning Agent 
According to [33][34][35], Reinforcement learning is a 
method of learning, by which what to do is taught, i.e., how 
to match a situation to an action, so that a numerical reward 
received as a signal, is maximized. The teacher does not 
point at the actions to be undertaken. Instead, the trainee has 
to find out those, leading to the greatest reward and try to 
realize them.  In the most interesting and challenging cases, 
not only the immediate reward could be taken into account 
when choosing an action, but also the further situations and 
the future rewards.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 5.  Message about a promotion of a new Compact Disk. 
All reinforcement learning agents have explicit goals, can 
sense aspects in their environment and choose actions, which 
influence it. The agent is realized by a program, matching the 
way the agent perceives reality and the actions it undertakes.  
A reinforcement learning algorithms is used for the 
second Goal-based learning agent. The agent receives the 
shopping list from the customer (this is what the agent 
perceives) and informs the customer about the sequence of 
shops he/she can visit in order to buy all the goods needed 
(these are the actions the agent undertakes). The shortest 
possible route is suggested, in accordance with the particular 
shopping list.  
Since the goal is to visit all the shops from the shopping 
list, the particular shopping list can be regarded as a plan or a 
sequence of goals to achieve in order to fulfill the task 
completely.  
It is also possible for the agent to get the exact location of 
a customer and a particular shop to get to. The shortest 
possible path to the desired shop is suggested in this case as 
well.  
In order to realize the agent’s learning process the 
following is to be developed: Environment model; Rewards 
model; Agent’s memory model; Agent’s behavior function; 
Value of the training parameter.  
The environment model is a graph (Figure 3) of the 
different environment conditions. The nodes in the graph 
(Figure 1) are the shops in the exemplary shopping mall. The 
edges point at the shops, between which there is a transition. 
Then, this graph is presented by an adjacency matrix. The 
number of rows and columns in this matrix is equal to the 
number of shops in the mall. Zero is put in the matrix in a 
place where there is a connection between the number of a 
shop, set by a number of a row, and the number of a shop, 
given by a number of a column. Values of -1 are placed in 
the other positions of the adjacency matrix.  
The rewards model is needed to set a goal for the agent.  
Reaching every shop from the customer’s shopping list is 
such a goal. Since the agent is a goal-based one, it behavior 
can be changed by just setting a new goal, changing the 
rewards model [33].  A reward is only given when the agent 
gets to a particular shop.  
The agent’s memory is modeled by presenting it with the 
help of an M-matrix (Memory of the agent). The rows in the 
M-matrix represent the current location of the customer, 
while the columns are the shops, where he/she can go. It is 
assumed at the beginning that the agent does not have any 
knowledge and therefore all elements in the M-matrix are 
zeros.  
The rule for calculating the current location of the 
customer at the moment of choosing the next shop to visit is 
as it follows:  
M (current location of the customer, chosen shop to visit 
next) = R(current location of the customer, next shop) + ϒ. 
Max[М(next shop, all possible shops where the customer 
could go from the next shop)]. 
The following is taken into account in the above formula: 
The immediate reward, obtained when the customer decides 
from the current location to go to a next shop: R(current 
position, chosen shop to go next);  The biggest possible 
future reward. This is the biggest reward, chosen from 
among the rewards, which would have been obtained when 
the customer goes out of the next shop and enters any 
possible shop: Max[М(next shop, all possible shops where it 
is possible to go from the next shop)]. The value of the 
learning parameter ϒ defines the extent, to which the agent 
112
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

will take into account the value of the future reward. The 
value of the learning parameter ϒ is within 0 to 1 (0 ≤ϒ< 1). 
If ϒ is closer to zero, then the agent will prefer to consider 
only the immediate reward. Experiments have shown that in 
this case it is impossible to teach the agent to achieve the 
goal. If ϒ is closer to one, then the agent will consider the 
future reward to a greater extent. This is the better option for 
successful training of the agent. The value of the learning 
parameter was experimentally chosen to be ϒ=0.8. At this 
value, the obtained weights for all possible actions are 
clearly identifiable and the process of training is reliable. A 
random initial position is chosen for the customer in the 
algorithm for training the agent. The following steps are 
realized until the target shop is reached:  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6.  Specifying the task environment and smart shopping learning agents modeling. 
Environment 
NAVIGATION 
Beacons/iBeacons 
or/and Holograms 
for smart buildings, 
smart shopping mall 
navigation (using 
Google Beacon 
Platform or/and 
Microsoft HoloLens 
respectively) 
and Headsets)  
Android Studio/ 
Visual Studio.NET 
 
 
 
 
Camera 
OpenCV 
Face Detecting 
 
Microphone 
 
INPUT 
 
LCD display or 
Touch Screen 
 
Natural Language 
Understanding C# 
VisualStudio.NET 
SHOPPING 
Creating a shopping 
list by using key 
combinations, drag-
and-drop pictures, 
pictures to speech 
convertor. 
C# VisualStudio.NET 
Virtual agent 
visualization 
 
TTS Engine 
Speech synthesis 
 
OUTPUT 
 
Visualization of 
messages C# 
 
Agent programs 
Utility-based learning agent 
Agent purpose: search for promotions and sales; 
defining customer’s location; informing the 
customer about promotions and sales.  
K-D decision tree for finding of the user’s 
location.  
Checking on the site of the mall for new attached 
files, 
containing 
information 
about 
new 
promotions or sale in a shop, downloading and 
reading such files if available. Informing the 
customer about a promotion or a sale in case 
he/she is in the corresponding shop or intends to 
visit it. Visual Studio.NET 
Goal-based learning agent 
Agent purpose: directing the customer on his/her 
way to reaching all desired shops from the 
shopping list. 
Reinforcement-learning algorithm for finding out 
the optimal path to all items on the shopping list.  
 
Model of the environment  
 
Model of the rewards  
*Algorithm for changing the reward model 
according  to the new goal set in front of the 
agent. 
*Defining the sequence of visiting the desired by 
the customer shop in the shopping center .  
Visual Studio.NET 
Personal utility-based learning agent 
Agent purpose: to have a knowledge about the 
shopping habits of each customer according to 
the best identification tree of that customer. The 
shopping rules for each customer are derived 
from that tree. Then the agent can foresee 
relatively precisely what categories of goods 
he/she will purchase for a particular visit to the 
mall. 
Therefore, 
the 
agent 
can 
suggest 
immediately a shopping route and check if there 
are promotions on the goods from the categories, 
in which the customer could be interested at this 
particular moment. Visual Studio.NET 
 
 
 
 
 
 
 
 
113
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7.  А survey about the interest of end customers in beacon-based services, holographic technology, PECS, TTS and Speech Synthesis technologies. 
 
Figure 8.  А survey about rediness of the user to use beacon-based services and holographic technology. 
One of all possible shops is chosen, where it is possible 
to go from the current position. The shop, to which the 
customer would go next is considered. For this next position 
now all the shops, to which it is possible to go further are 
considered. The value of the highest reward is taken. The 
next position is then set as a current one.  
C. Personal shopping Utility-Based Learning agent 
The purpose of the personal shopping utility-based 
learning agent is to have a knowledge about the shopping 
habits of each customer. The personal shopping utility-based 
learning agent recognizes the customer at the moment he/she  
registers himself/herself by means of his/her shopping card.  
All the purchases are grouped in categories in accordance 
with the shops in the shopping mall. A characteristic table 
with positive and negative examples is created for each 
category of goods. When a product from a considered 
category is purchased during a given visit of a customer to 
the mall, this is a positive example for this category of 
goods. 
For 
each 
realized 
purchase 
the 
following 
characteristics are saved in the characteristic table: season 
(spring, summer, autumn, winter); month (1-12); number of 
the week in the month (1-4); day (workday, weekend, public 
holiday, birthday); purchased good from another category 
(the categories, related to the other available shops in the 
mall are considered); if there were promotions of goods from 
the considered category at the moment of the purchase. 
These characteristics are used for tests, allowing to make 
a classification of the examples of purchases for each 
category of goods. The most important characteristic for 
each customer is found out. This is the characteristic, for 
which the biggest number of examples fall into a subset of 
either positive or negative examples. 
A subset, in which the examples are either positive or 
negative is called homogeneous. The examples, fallen into a 
non-homogeneous subset should be classified once again by 
114
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

using another characteristic, i.e., another criterion for 
classification. The procedure is repeated until all examples 
are classified, i.e., until all examples fall into homogeneous 
subsets. Thus the smallest identification tree is built. This is 
the tree, which classifies the examples in the best way. The 
shopping rules for each customer are derived from that tree. 
Each test (characteristic) is regarded as a prerequisite, while 
the result from the test, i.e., the fact if a given category of 
goods is purchased or not, is regarded as a conclusion. Here 
is an example of an obtained shopping rule for a customer: If 
the season is summer and it is a weekend, then the customer 
buys a product from the category of desserts.  
When the agent has at disposal the shopping rules of a 
customer, it can foresee relatively precisely what categories 
of goods he/she will purchase for a particular visit to the 
mall. Therefore, the agent can suggest immediately a 
shopping route and check if there are promotions on the 
goods from the categories, in which the customer could be 
interested at this particular moment. 
V. 
DEGREE OF DEVELOPMENT OF THE PROPOSED 
COGNITIVE ARCHITECTURE COMPONENTS 
The goal-based learning agent, the utility-based learning 
agent and personal utility-based learning agent are fully 
developed. The head of each agent is modeled and 
visualized. We have used Crazy Talk 6 for emotion 
modeling. The decision k-d tree, identification tree algorithm 
and reinforcement-learning algorithm are completed and 
used for agents function realization. The program for 
creating a shopping list by using key combinations and drag 
and drop pictures is ready. The picture to speech converter 
program can pronounce all the existing pictures and the 
created shopping list. The agents can recognize and react to a 
few speech commands. They start communication with the 
users when detecting a face in front of themselves. A number 
of experiments are conducted with some Estimote beacons 
and a notification program [36]. The complete beacon based 
navigation system and the corresponding software are not 
ready yet, however. The holograms and holographic 
computer have not been used for now. We have just  
obtained the holographic computer and we have got start to 
use it now. The holographic agent’s visualization are 
planned.  Experiments in a real shopping center and with real 
users shopping data are planned as well. User following 
smart shopping cart realizations is forthcoming too. 
VI. 
EMPIRICAL SURVEY 
The survey was conducted at the university. The total 
number of 115 students were offered the questionnaire. All 
of the participants were between the ages of 18 to 23 years 
old. 
A. А survey about the interest of end customers in beacon-
based services, holographic technology, PECS, TTS and 
Speech Synthesis technologies 
To investigate people’s mindset towards the use of 
beacons, the use of holograms, drag-drop pictures, pictures 
to speech, TTS and Speech Synthesis an empirical study was 
conducted. The survey’s purpose was to explore the interest 
of end customers in beacon-based services, holographic 
technology, PECS, TTS and Speech Synthesis technologies 
and the willingness to use them. As a base we use [9] but 
append some questions about new technologies. 
With this end in view, we designed a questionnaire with 
the following tree sections. The participants were asked 
(Figure 7), whether they (1) own a smart-phone, (2) know 
Bluetooth, (3) know Bluetooth Low Energy, (4) know 
Holographic computer, (5) know Holographic technologies, 
(6) know beacons, (7) have used beacons before, (8) have 
used holograms before, (9) are  familiar with PECS, (10) are 
familiar with TTS and speech recognizing technologies. 
This helps to understand, whether consumers are aware 
of beacons. Then, they were given a short introduction of the 
beacon technology, holographic technology, PECS, TTS and 
speech recognizing technologies, as a preparation for the 
remaining questions. Participants were asked to assess the 
usefulness of typical applications, which were based on 
already existing scenarios by using beacon-based or 
holographic realizations (Figure 8): General situations, such 
as navigation on airports, coupons in stores, information on 
exhibits in museums, etc.; Specific retail store types, e.g., 
supermarkets, outdoor stores, furniture shops; Applications 
in a super-market, e.g., personal welcome message, 
navigation to products on the shopping list, information on 
products, special offers, and electronic payment at the 
checkout; Applications in a stadium, e.g., offers for seat 
category upgrade, navigation to wardrobe/restrooms, special 
offers for drinks and snacks.  
Beacon-based technology and holographic technology 
are little known and the services based on them are not used 
widely yet, but the respondents declared willingness and 
readiness to use them. 
Five blind people aged 45-65 also took part in the survey. 
These respondents were not familiar with the described 
technologies and had not used them before. However, they 
do know and use in their daily routine Internet, Skype, 
smartphones, e-mail, all TTS programs and desktop reading 
programs. They showed great enthusiasm and willingness to 
get acquainted with beacon-based services and holographic 
services for navigation in buildings. 
B. А survey about the way the customers perceive the two 
developed agents and whether they consider their 
purpose useful  
The capabilities of the three agents were demonstrated in 
front of the students. The idea of Smart Shopping and Smart 
Shopping Cart Agents was presented. Then, the students 
were asked to  evaluate usefulness of the three agents, to 
compare  their  functionality and  to consider the services, of 
which  agent prefer; to say their opinion about shopping with 
Shopping Cart Smart Agents. Some of the questions were: 
Would you use a shopping cart with Intelligent Virtual 
Agents installed on it; would you use a shopping cart with 
Personal Shopping Utility Based Learning Agent installed on 
it; is it useful for you to be informed by Smart Shopping Cart 
Agents (SSCA) about the latest promotion in the shops you 
visit; is it useful for you if SSCA explain to you how to get 
115
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

to a given shop in the mall; do you think that shopping is 
more comfortable when communicating with SSCA; do you 
think the sales will go up as a result of the communication 
between the customers and SSCA during shopping.  
It can be seen from (Figure 9) that, the customers would 
use SSCA and they think the agents will be useful and their 
presence would make the shopping practice more 
comfortable. The personal shopping utility agent and utility-
based learning agent that search for promotions and sales are 
the preferred agents. 
Figure 9.  A survey about the way the customers perceive the three 
developed agents and whether they consider their purpose useful 
(values 1-10). 
C. A survey about user’s opinion and possibilities for 
shopping agents embodiments and user following smart 
shopping cart realizations 
During the third survey the students were asked to guess, 
which of six possible shopping carts they would use. The 
options are: 1) a standard shopping cart with a tablet attached 
to it, on which the virtual shopping agents are visualized; 2) 
a robotized shopping cart, capable of following the user and 
controllable by means of gestures; 3) a robotized shopping 
cart with a robotized hand/arm, capable of following the user 
and taking/giving objects, and controllable by gestures; 4) a 
standard shopping cart, pushed by a human-like robot; 5) a 
standard shopping cart, equipped with a holographic screen, 
on which a hologram of a human-like shopping robot is 
visualized; 6) a standard shopping cart, equipped with a 
holographic screen, on which a 3D scene with a hologram of 
a human – shopping assistant is visualized. 
The results from the survey are given in Figure10. The 
students have the strongest preference to the following two 
options: 1) a robotized shopping cart, capable of following 
the user and controllable by means of gestures; and 2) a 
standard shopping cart, equipped with a holographic screen, 
on which a hologram of a human-like shopping robot is 
visualized; 
Having in mind the obtained results, the possibilities for 
realization of these two shopping cart types will be 
discussed. 
1) A robotized shopping cart, capable of following the 
user and controllable by means of gestures 
It is characteristic of this task that the velocities are very 
low; all the other shopping carts and users are regarded as 
dynamic obstacles to be avoided. Commodity shelves are 
static obstacles, which should be avoided, too. The shopping 
cart needs to recognize the user to be followed. It has to 
recognize the user’s gestures and have a reaction to them. 
The distance between the shopping cart and the user should 
be kept the same as well. 
Consequently, it can be summarized, that the autonomous 
user following is inherently a complex cognitive process 
associated with object recognition, static or dynamic 
obstacles avoidance in the environment, in which humans 
and robots work together; position determining; predicting 
the direction of movement; decision making, context 
understanding; working with uncertain and incomplete 
knowledge; vehicle communication and targeted actions and 
movements.  
User following achievement approaches are Motion 
planning approaches [37]; Robust motion control approaches 
[38][39][40]; Game theory approach and Connectionist 
approach [41][42][43]. 
2) A standard shopping cart, equipped with a 
holographic screen, on which a hologram of a human-like 
shopping robot is visualized  
It turns out that the users prefer the holographic shopping 
agents to look like a human, but not to be modeled as people 
at 100%. The users know that they are not people actually 
and reject them, perceiving this overly realistic model as a 
deception. That is why we are working to find a suitable 
model for the appearance of a humanoid robot that can be 
perceived by people well. 
Modeling of the whole body of the humanoid robot is 
envisaged, as well as choosing a 3D scene that fits in the 
context of the mall. 
The holographic robot must adhere to the social norms of 
communicating with consumers. Not to enter their personal 
space. Not to intrude. To keep distance when communicating 
with the user. The robot must behave as a mall officer.  
Communication and negotiation skills are needed [44], 
related to directing viewer to the speaker; awaiting for the 
speaker to finish; taking the turn; answering a question; 
asking a question; waiting for the answer; analysis of the 
answer and continuing the conversation; disengagement 
from the user. Skills for grasping holographic goods and 
presenting them in front of the user will also be needed. Such 
skills a holographic agent can learn by using deep neural 
networks (DNNs) and recurrent neural networks (RNNs) 
[41][42][43]. 
 
116
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
Figure 10.  A survey about user’s opinion for shopping agents embodiments 
and user following smart shopping cart realizations. (values 1-10). 
The holographic agent has to understand where the user’s 
attention is directed, to understand the user’s reaction toward 
its actions or words. It means that scenarios have to be 
developed for the human-holographic agent and both 
reinforcement learning and Learning from Demonstration 
(LfD) can be used for the purpose [39][40]. 
In multi-agent scenarios, Learning from Demonstration 
paradigm already allows the robot to observe the partner's 
reaction that matches its movement. On the one hand, LfD 
speeds up reinforcement learning algorithms and can be used 
to re-demonstrate policies as well as to build new policies. 
LfD develops with the introduction and improvement of 
ways to obtain teacher feedback. In the latter algorithms the 
feedback may even be in the form of corrective advice. 
Therefore these algorithms could be successfully used to 
develop algorithms for acquiring social behavior by the 
holographic agents. Training can also be done by encoding 
the action-reaction patterns in a Hidden Markov Model 
(HMM) using [38]. 
D. A survey of the performance of the Q learning 
algorithm with an introduced environment measures 
model (a model of the environment criteria), presented 
as Matrix K  
Q learning that is simplification of reinforcement 
learning is an extremely flexible method. It allows to easily 
find an optimal path from each position in the modeled 
environment to the goal state.  
It is known that Imitation Learning is a way to optimize 
the Reinforcement learning and Q learning in particular. The 
task considered here is related to smart shopping realization 
but it does not allow a teacher to show how to reach the goal 
in order to achieve better results.  
This is due to the availability of lots of ways for 
achieving a particular goal. Besides, the goal is different 
every time. The shops and stands in the Shopping Center that 
consumers want to reach are different. Some consumers look 
for promotional goods; others need artwork. Some customers 
use shopping as a therapy and want to reach the most 
frequently visited shops and to go through the busiest lobbies 
and hallways; others want to avoid the crowded zones. So in 
this task it is important not only to reach the goal. It is of the 
same level of importance how it is reached and what criteria 
a certain path meets.  
In order to make the Q learning agent find the optimal 
sequence of lobbies or hallways, meeting a specific criterion, 
the use of environment measures model represented as 
matrix K is introduced.  
For the purposes of the experiment the Shopping Center 
is represented by a graph with 17 nodes and 36 edges 
between them as shown in Figure 11. Every shop in the 
considered Shopping Center is represented as a graph. Every 
lobby or a hallway, connecting the shops, is represented by 
an edge. The busiest and most wanted to go through lobbies 
or hallways are marked in orange color and have a measure 
of 1 in the K matrix (Figure12). The secondary, distant, non-
desirable pathways are marked in blue and have a measure of 
2 in the K matrix (Figure12). The environment measures 
model presented by matrix K is similar to the environment 
reward model, presented by matrix R (Figure12). The values 
of a given criterion, to which corresponds each edge in the 
graph, are kept in the K matrix. The minus one (-1) in the 
matrix R and in the matrix K says that has no edge on this 
place in the graph. Now the learning algorithm is changed. 
The agent has to go only through those edges in the graph, 
which have a specific measure value. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 11.  A Shopping Center with 17 shops and lobbies or hallways 
between them, presented by an undirected graph. 
The experiment is conducted in the following way: a goal 
is set in front of the agent to reach node 15 in the graph; a 
reward of 100 for going through the edge, connecting nodes 
11 and 15 is announced as well in reward matrix R. Other 
117
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

edges have zero reward (Figure 12). The black dot line 
denotes the optimal path found from node 0 to node 15.  
First stage. No criterion is set, which the desired 
sequence of edges should meet in order to reach the goal.  
 
//R - reward matrix 
public static int [,] R = new int[,] { 
{ -1,  0,  0,  0,  0,  0, -1, -1,  -1, -1, -1, -1, -1, -1, -1, -1, -1 }, 
{  0, -1,  -1, 0,  0, -1, -1, -1,  -1, -1, -1, -1, -1, -1, -1, -1, -1 }, 
{  0, -1, -1, -1,  0,  0, -1, -1,  -1, -1, -1, -1, -1, -1, -1, -1, -1 }, 
{  0,  0, -1, -1,  0, -1,  0,  0,  -1, -1, 0, -1, -1, -1, -1, -1, -1 }, 
{  0,  0,  0,  0, -1,  0, -1,  0,   0, -1, -1, -1, -1, -1, -1, -1, -1 }, 
{  0, -1,  0, -1,  0, -1, -1, -1,   0,  0, -1, -1, -1, -1,  0, -1, -1 }, 
{ -1, -1, -1,  0, -1, -1, -1, -1,  -1, -1,  0,  0, -1, -1, -1, -1, -1 }, 
{ -1, -1, -1,  0,  0, -1, -1, -1,   0, -1, -1,  0, -1, -1, -1, -1, -1 }, 
{ -1, -1, -1, -1,  0,  0, -1,  0,  -1,  0, -1, -1,  0,  0, -1, -1, -1 }, 
{ -1, -1, -1, -1, -1,  0, -1, -1,   0, -1, -1, -1, -1,  0,  0, -1, -1 }, 
{ -1, -1, -1,  0, -1, -1,  0, -1,  -1, -1, -1,  0, -1, -1, -1, 0, -1 }, 
{ -1, -1, -1, -1, -1, -1,  0,  0,  -1, -1,  0, -1,  0, -1, -1,  100, -1 }, 
{ -1, -1, -1, -1, -1, -1, -1, -1,   0, -1, -1,  0, -1,  0, -1,  0,  0 }, 
{ -1, -1, -1, -1, -1, -1, -1, -1,   0,  0, -1, -1,  0, -1,  0, -1,  0 }, 
{ -1, -1, -1, -1, -1,  0, -1, -1,  -1,  0, -1, -1, -1,  0, -1, -1,  0 }, 
{ -1, -1, -1, -1, -1, -1, -1, -1,  -1, -1,  0,  0,  0, -1, -1, -1,  0 }, 
{ -1, -1, -1, -1, -1, -1, -1, -1,  -1, -1, -1, -1,  0,  0,  0,  0, -1 }}; 
 
//K- measure matrix 
public static int[,] K = new int[,] { 
{ -1,  2,  2,  2,  1,  2, -1, -1,  -1, -1, -1, -1, -1, -1, -1, -1, -1 }, 
{  2, -1,  -1, 2,  1, -1, -1, -1,  -1, -1, -1, -1, -1, -1, -1, -1, -1 }, 
{  2, -1, -1, -1,  1,  2, -1, -1,  -1, -1, -1, -1, -1, -1, -1, -1, -1 }, 
{  2,  2, -1, -1,  1, -1,  2,  2,  -1, -1,  2, -1, -1, -1, -1, -1, -1 }, 
{  1,  1,  1,  1, -1,  2, -1,  1,   1, -1, -1, -1, -1, -1, -1, -1, -1 }, 
{  2, -1,  2, -1,  2, -1, -1, -1,   1,  2, -1, -1, -1, -1,  2, -1, -1 }, 
{ -1, -1, -1,  2, -1, -1, -1, -1,  -1, -1,  2,  1, -1, -1, -1, -1, -1 }, 
{ -1, -1, -1,  2,  1, -1, -1, -1,   1, -1, -1,  1, -1, -1, -1, -1, -1 }, 
{ -1, -1, -1, -1,  1,  1, -1,  1,  -1,  2, -1, -1,  1,  1, -1, -1, -1 }, 
{ -1, -1, -1, -1, -1,  2, -1, -1,   2, -1, -1, -1, -1,  1,  1, -1, -1 }, 
{ -1, -1, -1,  2, -1, -1,  2, -1,  -1, -1, -1,  1, -1, -1, -1,  2, -1 }, 
{ -1, -1, -1, -1, -1, -1,  1,  1,  -1, -1,  1, -1,  1, -1, -1,  2, -1 }, 
{ -1, -1, -1, -1, -1, -1, -1, -1,   1, -1, -1,  1, -1,  1, -1,  1,  2 }, 
{ -1, -1, -1, -1, -1, -1, -1, -1,   1,  1, -1, -1,  1, -1,  2, -1,  1 }, 
{ -1, -1, -1, -1, -1,  2, -1, -1,  -1,  1, -1, -1, -1,  2, -1, -1,  2 }, 
{ -1, -1, -1, -1, -1, -1, -1, -1,  -1, -1,  2,  2,  1, -1, -1, -1,  2 }, 
{ -1, -1, -1, -1, -1, -1, -1, -1,  -1, -1, -1, -1,  2,  1,  2,  2, -1 }}; 
 
Figure 12.  Reward matrix R and Measure matrix K. 
The optimal path found from node 0 to the goal is given 
in Figure 13.  It can be seen that the path goes through edges 
with a different value of the criterion, set in the K matrix.  
Second stage. The agent receives a requirement to reach 
the goal by going only through edges with a measure value 
of 1. The optimal path found from node 0 to the goal is given 
in Figure 14. As it can be seen, the path goes only through 
edges with a measure value of 1 for the criterion, set in the K 
matrix.   
Third stage. The agent has to reach the goal by going 
only through edges, having a measure value of 2. The 
optimal path for this case, starting from node 0 and going to 
the goal, is shown in Figure 15. As it shows, the path goes 
only through the edges with a measure value of 2 for the 
criterion, set in the K matrix.  
In the example under consideration, there are primary 
and secondary paths that connect all locations in the example 
Shopping Center. There might be a situation, in which a 
primary or a secondary path to a given place is missing. Then 
the algorithm can be modified by allowing the agent to go 
through a certain number of edges, which do not correspond 
to the value of the criterion “measure” in the K matrix. 
An advantage of the proposed modification of the Q 
learning algorithm is that it allows the agent to give 
explanation of the reasons why a given path to the goal has 
been chosen. In addition, the proposed modification allows 
to introduce various criteria for a path choice. If the criteria 
from Maslow’s theory of personality motivation are used 
[45], a model of a system of values could be developed using 
different scenarios.  
 
 
 
Figure 13.  Optimal path from node 0 to node 15. Requirement for measure 
value not set.  
 
 
 
Figure 14.  Optimal path from node 0 to node 15. Requirement for measure 
value: to be equal to 1. 
 
 
 
 
Figure 15.  Optimal path from node 0 to node 15. Requirement for measure 
value: to be equal to 2. 
118
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

E. Study of the learning parameter ϒ 
It is known that the value of the learning parameter ϒ is 
within 0 to 1 (0 ≤ϒ< 1). The aim is to study if any of these 
values is more appropriate to be preferred during a Q 
learning agent training.  
The experiment is conducted in the following way: the 
graph in Figure 11 is considered; a goal is given to the agent 
and the training gets started. Twelve experiments are carried 
out actually with each value of the learning parameter ϒ  - 
from 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 to 1, 
respectively.   
First stage: The number of episodes required to train the 
agent to reach the goal using the optimal path from each 
node in the graph is considered. The exploration by the agent 
of each node-to-node path until it reaches the goal node is 
called an episode. 
The training is considered completed when any further 
change of the assessment of each edge in the graph does not 
lead to a change in the optimal paths found. The results from 
this stage are shown in Figure 16. It is obvious that values 
from 0.3 to 0.9 of the learning parameter ϒ are appropriate 
for realizing the process of training. When these values are 0, 
0.1, 0.2 and 1, no paths to reach the goal are found. 
 
 
Figure 16.  Number of episodes needed to find the optimal paths at values 
of the learning parameter from 0 to 1 during a Reinforcement learning 
agent training.  
Second stage. The number of episodes required to train 
the agent to reach the maximum value in the assessment of 
each edge in the graph is considered. During the training 
process the assessment of each edge in the graph increases 
until it reaches its maximum value. When the maximum 
value is reached, the further training no longer changes the 
assessments of the edges in the graph. The results from this 
stage are presented in Figure 17.  
Comparing the results of the two stages of the 
experiment, it can be seen that finding the optimal paths to 
the goal requires much fewer episodes than reaching the 
maximum value of each edge in the graph. Besides, the 
values of the learning parameter of 0.8 and 0.9 offer the 
greatest possibilities both for training and for finding optimal 
paths. As it can be seen in Figure 17, these learning 
parameter values require an average of 800 and 1600 
episodes, respectively, until the maximum value in the 
assessment of each edge in the graph is reached.  
VII. FUTURE WORK 
Many tasks remain to be solved. The work on the 
development of the Reinforcement learning algorithm will 
continue in the first place; opportunities for modeling the 
training agent's value system will be looked for; efforts will 
be put to modeling a system for generating explanations by 
the trained agent. Using the holographic computer, it is now 
possible to model and visualize a virtual advertising agent. It 
is assumed that the communication with such an agent will 
be engaging and helpful to consumers. As mentioned in this 
article, there is a lot of interest in modeling a robotic 
shopping cart to follow the consumer. Efforts will therefore 
be made to this end. For example, it is important to combine 
and share intelligent behaviors such as: wander behavior; 
path following; collision avoidance; obstacle and wall 
avoidance; patrol between a set of points; flee behavior.  
 
 
Figure 17.  Number of episodes needed to reach the maximum value of the 
assessment for each edge in the graph under consideration.  
VIII. CONCLUSION 
The paper describes the design and implementation of an 
intelligent Smart Shopping Cart Learning Agents prototype 
and their environment. The system differs from other 
intelligent systems by the combination of machine learning 
techniques, beacon-based navigation and/or hologram-based 
navigation in the mall, the integration of Picture Exchange 
Communication System in it and by its language 
understanding and speech synthesis capabilities, drag-and-
drop techniques and keyboard button combinations enabled 
access.  
The task environment is partially observable, cooperative 
and a multi-agent environment. The shopping world is 
deterministic with some stochastic and uncertainty elements. 
The task environment is episodic and can be realized either 
as static or as semi-dynamic. 
The utility-based agent uses a decision k-d tree to quickly 
find where, in which shop the customer is located according 
to his/her coordinates. It getting to the new promotion in the 
shopping mall according to user‘s shopping list and inform 
them.  
119
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Reinforcement learning algorithm is used for the other 
Goal-based learning agent. The agent gets the shopping list 
from the customer and informs the customer about the 
sequence, in which he/she can visit the shops to buy all 
needed goods. 
The personal utility-based agent makes the best 
identification tree according to the shopping data of each 
user. That way the agent knows their shopping habits and 
can suggest a shopping route and check if there are 
promotions on the goods from the categories, in which the 
customer could be interested.  
The performance measure, to which the shopping agents 
are aspired includes getting to the correct shop in the 
shopping mall; getting to the new promotion in the shopping 
mall according to user‘s shopping list or according to user’s 
shopping habits; minimizing the path when going through 
the shops from the shopping list; maximizing customer 
comfort; maximizing purchases; and enabling people with 
different communication capabilities to navigate in big 
buildings and in particular to shop in big shopping centers. 
The empirical survey conducted with a limited number of 
users showed their positive mindset for using such Smart 
Shopping Cart Learning Agents in indoor spaces. The utility-
based learning agent that search and informs for promotions 
and sales is the preferred one.  
The performance of the Q learning algorithm with an 
introduced environment measures model (a model of the 
environment criteria) is proposed and explored. Study of the 
learning parameter ϒ is presented. 
In the future work, it is intended to realize user following 
smart shopping cart and holographic visualization of the 
shopping agents. 
ACKNOWLEDGMENT 
The authors gratefully acknowledge the financial support 
provided within the Ministry of Education and Science of 
Bulgaria Research Fund Project FNI DN17/11. 
REFERENCES 
[1] Dilyana Budakova, Lyudmil Dakovski, Veselka Petrova-
Dimitrova, “Smart Shopping Cart Learning Agents Modeling 
and Evaluation,” The Eleventh International Conference on 
Advanced 
Cognitive 
Technologies 
and 
Applications 
(COGNITIVE 2019) IARIA, 05-09 May 2019, Venice, Italy, 
pp. 12-19, ISSN: 2308-4197, ISBN: 978-1-61208-705-4. 
[2] Y. J. Chang, S. M. Peng, T. Y. Wang, S. F. Chen, Y. R. Chen, 
and H. C. Chen, “Autonomous indoor wayfinding for 
individuals 
with 
cognitive 
impairments,” 
Journal 
of 
NeuroEngineering and Rehabilitation, 7(1), pp. 1– 13, 2010. 
[3] V. Kulyukin, C. Gharpure, J. Nicholson, and G. Osborne, 
“Robot-assisted wayfinding for the visually impaired in 
structured indoor environments,” Autonomous Robots, 
Springer, 21(1), pp. 29–41, 2006. 
[4] L. Niua and Y. Song, “A schema for extraction of indoor 
pedestrian navigation grid network from floor plans,” In The 
International Archives of the Photogrammetry, Remote 
Sensing and Spatial Information Sciences, volume XLI-B4, 
Prague, Czech Republic, pp. 325-330, 2016. 
[5] My Dream Companion project – YGA. [Online]. Available 
from: https://www.yga.org.tr/en/visually-impaired-techologies 
[retrieved: 12, 2019]. 
[6] Y. Zhuang, J. Yang, Y. Li, L. Qi, and N. El-Sheimy, 
“Smartphone-based indoor localization with Bluetooth low 
energy beacons,” Sensors, April 2016. 16(5), pp. 596, doi: 
10.3390/s16050596. 
[7] D. Ahmetovic, et al. “Navcog: A navigational cognitive 
assistant for the blind,” In International Conference on 
Human Computer Interaction with Mobile Devices and 
Services, ACM, 2016, pp. 90-99. 
[8] A. Ch. Seyed, N. Vinod, and S. Kaushik, “IBeaconMap: 
Automated Indoor Space Representation for Beacon-Based 
Wayfinding,” 
Human-Computer 
Interaction 
arXiv: 
1802.05735v1 [cs.HC], USA, 2018. 
[9] A. Thamm, J. Anke, S. Haugk, and D. Radic, “Towards the 
Omni-Channel: 
Beacon-based 
Services 
in 
Retail,” 
International Conference on Business Information Systems 
(BIS 2016), Springer, Leipzig, Germany, July 2016, pp. 181-
192. July 2016. DOI: 10.1007/978-3-319-39426-8_15. 
[10] Google 
Beacon 
Platform. 
[Online]. 
Available 
from: 
https://www.youtube.com/watch?v=0QeY9FueMow,  
[retrieved: 12, 2019]. 
[11] Microsoft 
HoloLens 
holographic 
computer. 
[Online]. 
Available 
from: 
https://developer.microsoft.com/en-
us/windows/mixed-reality/,  [retrieved: 12, 2019]. 
[12] J. Cassell, et al. “MACK: Media lab Autonomous 
Conversational Kiosk,” Imagina’02, Monte Carlo, 2002 vol. 
2, 
pp. 
12–15. 
[Online]. 
Available 
from: 
https://www.media.mit.edu/gnl/pubs/imagina02.pdf, 
[retrieved: 12, 2019]. 
[13] L. McCauley and S. D’Mello, “MIKI: A speech enabled 
intelligent kiosk,” IVA 2006, LNAI 4133, Springer, 2006, 
pp.132-144.  
[14] Tomorrow‘s digital signage, today with 3d holographic 
kiosks, Intel IoT Digital kiosks, Solution brief, [Online], 
Available 
from: 
https://d1io3yog0oux5.cloudfront.net/ 
_35cb5a593466915af09244e7df3c153e/provision/db/255/661/
pdf/Intel+Provision+3D+Brief+2016.pdf, 
[retrieved 
12, 
2019]. 
[15] A. Bondy and L. Frost, PECS - Picture Exchange 
Communication 
System. 
[Online]. 
Available 
from: 
https://pecsusa.com/apps/,  [retrieved: 12, 2019]. 
[16] C. Lord, M. Rutter, P. C. Dilavore, and S. Risi, “ADOS - 
Autism 
Diagnostic 
Observation 
Schedule,” 
Western 
Psychological Services, 2002. 
[17] National Autism Resources Inc. [Online]. Available from:  
https://www.nationalautismresources.com/the-picture-
exchange-communication-system-pecs/, [retrieved: 12, 2019]. 
[18] What is PECS. [Online]. Available from: https://pecs-
unitedkingdom.com/pecs/, [retrieved: 12, 2019]. 
[19] JAWS. 
[Online]. 
Available 
from: 
http://www.freedomscientific.com/Products/Blindness/JAWS 
[retrieved: 12, 2019].  
[20] NVDA. [Online]. Available from: https://www.nvaccess.org/, 
[retrieved: 12, 2019]. 
[21] Vocalizer Android Dariya Bulgarian voice. [Online]. 
Available from: https://play.google.com/store/apps/details?id 
=es.codefactory.vocalizertts&hl=bg, [retrieved: 12, 2019]. 
[22] Innoetics TTS Reader Female Bulgarian Voice IRINA. 
[Online]. 
Available 
from: 
https://www.innoetics.com/, 
[retrieved: 12, 2019]. 
[23] S. Russell and P. Norvig, “Artificial Intelligence A Modern 
Approach”, Prentice Hall, Third Edition, 2010, ISBN-13 978-
0-13-604259-4, ISBN-10 0-13-604259-7. 
[24] Kuka 
robot. 
[Online]. 
Available 
form: 
https://cyberbotics.com/doc/guide/robots, 
[retrieved: 
12, 
2019]. 
120
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[25] N. Ç. Kılıboz and U. Güdükbay, “A hand gesture recognition 
technique for human–computer interaction,” Journal of Visual 
Communication and Image Representation, Elsevier, Volume 
28, pp. 97-104, April 2015. 
[26] N. G. Shakev, S. A. Ahmed, A. V. Topalov, V. L. Popov, and 
K. B. Shiev, “Autonomous Flight Control and Precise 
Gestural Positioning of a Small Quadrotor,” Learning 
Systems: From Theory to Practice, Springer, pp. 179-197, 
2018. | 
[27] E. Coupeté, et al. “New Challenges for Human-Robot 
Collaboration in an Industrial Context: Acceptability and 
Natural 
Collaboration,” 
Fifth 
workshop 
"Towards 
a 
Framework for Joint Action", IEEE RO-MAN, 2016, pp. 1-4.  
[28] S. R. Fletcher and P. Webb, “Industrial robot ethics: facing 
the challenges of human-robot collaboration in future 
manufacturing systems,” A world with robots: international 
conference on robot ethics (ICRE 2015) Springer, 2015, pр. 
159-169. 
[29] P. Viola and M. Jones, “Rapid object detection using a 
boosted cascade of simple features,” Computer Vision and 
Pattern Recognition (CVPR 2001) IEEE, Vol.1, 2001, pp. I-
511-I-518. doi: 10.1109/CVPR.2001.990517. 
[30] The Intel Open Source Computer Vision Library, vol. 2006. 
[Online]. 
Available 
from: 
 
https://software.intel.com,  
[retrieved: 12, 2019]. 
[31] P. H. Winston, “Artificial Intelligence,” Addison-Wesley 
Publishing Company, ISBN-13: 978-0201533774 ISBN-10 
[32] Y. Y. Song and Y. Lu, “Decision tree methods: applications 
for classification and prediction,” Shanghai Archives of 
Psychiatry, Vol. 27, No.2, pp. 130-135, 2015. doi: 
10.11919/j.issn.1002-0829.215044, PMID: 26120265. 
[33] R. S. Sutton and A. G. Barto, Reinforcement Learning: An 
Introduction, The MIT Press, Cambridge, London, England, 
2014. 
[Online]. 
Available 
from:  
http://incompleteideas.net/book/ebook/the-book.html, 
[retrieved: 12, 2019]. 
[34] A. Gosavi, “Reinforcement Learning: A Tutorial Survey and 
Recent Advances,” INFORMS Journal on Computing, Vol. 
21 No.2, pp. 178-192, 2008. 
[35] R. R. Torrado, P. Bontrager, J. Togelius, J. Liu, D. Perez-
Liebana, “Deep Reinforcement Learning for General Video 
Game AI,” IEEE Conference on Computatonal Intelligence 
and Games, CIG. 2018-August, 10.1109/CIG.2018.8490422 
[36] Estimote 
Beacons. 
[Online]. 
Available 
from: 
https://estimote.com/, [retrieved: 12, 2019]. 
[37] A. Best, S. Narang, D. Barber, and D. Manocha, “AutonoVi: 
Autonomous Vehicle Planning with Dynamic Maneuvers and 
Traffic Constraints”, IEEE/RSJ International Conference on 
Intelligent Robots and Systems (IROS 2017) IEEE, 2017, 
DOI: 10.1109/IROS.2017.8206087. 
[38] D. Lee, C. Ott, and Y. Nakamura, “Mimetic communication 
model with compliant physical contact in human-humanoid 
interaction”, International Journal of Robotics Research, 
SAGE, Volume 29 issue13, pp. 1684–1704, November 2010. 
[39] B. Argall, “Learning Mobile Robot Motion Control from 
Demonstration and Corrective Feedback”, Robotics Institute 
Carnegie Mellon University Pittsburgh, PA 15213, March 
2009. 
[40] H. B. Amor, D. Vogt, M. Ewerton, E. Berger, B. Jung, J. 
Peters, “Learning Responsive Robot Behavior by Imitation,” 
IEEE/RSJ International Conference on Intelligent Robots and 
Systems (IROS 2013) IEEE, Tokyo, Japan, November 3-7, 
2013, pp. 3257-3264.  
[41] K. Takahashi, K. Kim, T. Ogata, S. Sugano, “Tool-body 
assimilation model considering grasping motion through deep 
learning,” Robotics and Autonomous Systems, Elsevier, 
Volume 91, pp. 115–127, 2017. 
[42] G. I. Parisi, J. Tani, C. Weber, S. Wermter, “Emergence of 
multimodal action representations from neural network self-
organization,” Cognitive Systems Research, Elsevier, Volume 
43, pp. 208-221, June 2017. 
[43] K. Noda, H. Arie, Y. Suga, and T. Ogata, “Multimodal 
integration learning of robot behavior using deep neural 
networks,” Robotics and Autonomous Systems, Elsevier, 
Volume 62, pp. 721–736, 2014. 
[44] S. Monahan, et al., “Autonomous Agent that Provides 
Automated Feedback Improves Negotiation Skills,” Chapter 
in Artificial Intelligence in Education, Springer International 
Publishing, volume 10948, 2018. 
[45] A. Maslow, Motivation and Personality, Harper & Row  
Publishers, 
1970. 
[Online]. 
Available 
from: 
https://www.academia.edu/29491165/Motivation_and_Person
ality_BY_Abraham_Maslow, [retrieved: 12, 2019]. 
 
121
International Journal on Advances in Internet Technology, vol 12 no 3 & 4, year 2019, http://www.iariajournals.org/internet_technology/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

