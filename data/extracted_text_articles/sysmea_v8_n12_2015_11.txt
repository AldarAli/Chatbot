Stochastic Models for Quantum Device Conﬁguration and Self-Adaptation
Sandra K¨onig∗ and Stefan Rass†
∗Digital Safety & Security Department, Austrian Institute of Technology, Klagenfurt, Austria
Sandra.Koenig@ait.ac.at
†Department of Applied Informatics, System Security Group,
Universit¨at Klagenfurt, Universit¨atsstrasse 65-67, 9020 Klagenfurt, Austria
stefan.rass@aau.at
Abstract—Quantum carriers of information are naturally fragile
and as such subject to inﬂuence by various environmental factors.
Cryptographic techniques that exploit the physical properties of
light particles to securely transmit information strongly hinge
on a proper calibration and parameterizations to correctly
distinguish natural distortions from artiﬁcial ones, the latter of
which would indicate the presence of an attacker. Consequently,
it is necessary and useful to know how environmental working
conditions inﬂuence a quantum device so as to optimize its
operational performance (say, the qubit transmission or error
rates, etc.). This work extends a previous copula-based modeling
approach to build a stochastic model of how different device
parameters depend on one another and how they inﬂuence the
device performance. We give a full detailed practical description
of how a model can be ﬁt to the data, how the goodness of ﬁt can
be tested, and how the quantities of interest for a self-calibration
can be obtained from the resulting stochastic models.
Keywords–stochastic modeling; copula; estimation; goodness of
ﬁt; quantum network; quantum devices; statistics
I.
INTRODUCTION
Quantum key distribution (QKD) is a technique that exploits
light (particles) as carrier of information. The natural fragility
of such a carrier naturally ties even passive eavesdropping
attempts to an unavoidable increase of errors that is detectable
for the user(s) of the quantum channel. To reliably indicate the
presence of an adversary by classifying some errors as being
artiﬁcial and distinguishing these from natural error rates,
several environmental factors have to be taken into account
to compute the expected channel characteristics (error rate,
noise, etc.) when the transmission is unimpeded. To this end,
[1] proposed the use of copula models to capture the inﬂuence
of environmental factors on the performance characteristic of
a QKD device, most importantly, the quantum bit error rate,
which indicates the presence or absence of an intruder.
Physically, the fact that any access to the channel induces
errors is implied by the impossibility of creating a perfect copy
of a single photon. This fundamental result of quantum physics
was obtained by [2].
Recent experimental ﬁndings on the quantum key distribu-
tion network demonstrated as the result of the EU project
SECOQC (summarized in [3]) raised the question of how
much environmental inﬂuences affect the “natural” quantum
bit (qubit) error rate (QBER) observed on a quantum line that
is not under eavesdropping attacks. A measurement sample
reported in [4] was used to gain ﬁrst insights in the problem,
but the deeper mechanisms of dependency between QBER
and the device’s working conditions have not been modeled
comprehensively up to now.
The desire of having a model that explains how the QBER
depends on environmental parameters like temperature, humid-
ity, radiation, etc. is motivated by the problem of ﬁnding a good
calibration of QKD devices, so that the channel performance
is maximized. Unfortunately, with the QBER being known
to depend on non-cryptographic parameters, it is difﬁcult to
give reasonable threshold ﬁgures that distinguish the natural
error level from that induced by a passive eavesdropping.
We spare the technical details on how a QBER threshold is
determined for a given QKD protocol here (that procedure is
speciﬁc for each known QKD protocol and implementation),
and focus our attention on a statistical approach to obtain a
model of interplay between the qubit error rate and various
environmental parameters. More precisely, our work addresses
the following problem: given the current working conditions
of a QKD device, what would the natural qubit error rate
be, whose transgression would indicate the presence of an
eavesdropper? The basic intention behind this research is
aiding practical implementations of QKD-enhanced networks,
where our models provide a statistically grounded help to react
on changing environmental conditions.
For that purpose, we utilize a general tool of probability
theory, a copula function, which is an interdependency model
as contrasted to the parameter model (probability distribution
of a single environment parameter). In that regard, we outline
in Section II the basics of copula theory to the extent required
here. This is to quickly get to the point where we can give
effective methods to infer an expected qubit error rate upon
known external inﬂuence parameters.
The remainder of this work is structured as follows: after
theoretical groundwork in Section II, we move on by showing
how to use empirical data (measurements) drawn from a given
device to construct an interdependency model that explains
how the QBER and other variables mutually depend on each
other. Section IV then describes how to single out the QBER
from this overall dependency structure towards computing
the expected error rate from the remaining variables. The
concluding Section V summarizes the procedure and provides
ﬁnal remarks.
124
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Related Work
Surprisingly, there seem to be only a few publications
paying attention to statistical dependencies of cryptographic
parameters and the working conditions of a real device, such
as [4], [5]. While most experimental implementations of QKD,
such as [3], [6]–[9] give quite a number of details on device
parameters, optimizations of these are mostly out of focus. An
interesting direction of research is towards becoming “device-
independent” [10], [11], which to some extent may relieve
issues of hacking detection facilities, yet leaves the problem
of optimal device conﬁguration nevertheless open. The idea of
self-adaptation is not new and has already seen applications
in the quantum world [12]–[14] including the concept of
copulas, applications of the latter to the end of self-adaption
remain a seemingly new ﬁeld of research. Copulas have been
successfully applied to various problems of explaining and
exploiting dependencies among various risk factors (related to
general system security [15], [16]), and the goal of this work
is taking ﬁrst steps in a study of their applicability in the yet
unexplored area of self-conﬁguring quantum devices.
II.
PRELIMINARIES AND NOTATION
We denote random variables by uppercase Latin letters
(X, Z, . . .), and let matrices be uppercase Greek or bold-
printed Latin letters (Σ, D, . . .). The symbol X ∼ F(x)
denotes the fact that the random variable X has the distribution
function F. For each such distribution, we let the correspond-
ing lower-case letter denote its density function, i.e., f in the
example case.
For self-containment of our presentation, we give a short
overview of the most essential facts about copulas that we are
going to use, as for a more detailed introduction we refer to
[17].
Deﬁnition II.1. A copula is a (n-dimensional) distribution
function C : [0, 1]n → [0, 1] with uniform marginal distri-
butions.
Especially, a copula satisﬁes the following properties:
Lemma II.1.
1)
For
every
u1, . . . , un
∈
[0, 1],
C(u1, . . . , un) = 0 if at least one of the arguments is
zero and
2)
C(u1, . . . , un) = ui if uj = 1 for all j ̸= i.
A family of copulas that leads to handy models in higher
dimensions is known as the family of Archimedean copulas,
of which many extensions exist.
Deﬁnition II.2. An Archimedean copula is determined by the
so called generator function φ(x) via
C(u1, . . . , un) = φ−1(φ(u1) + . . . + φ(un)).
(1)
The generator function φ : [0, 1] → [0, ∞] has to satisfy
φ(1) = 0 and φ(∞) = 0, furthermore, φ has to be n-
monotone, i.e., to be differentiable up to order n − 2 with
(−1)n−2φ(n−2)(t) being nondecreasing and convex and
(−1)iφ(i)(t) ≥ 0 for 0 ≤ i ≤ n − 2
for all t ∈ [0, ∞).
As one of the cornerstones in copula theory, Skl˚ars theo-
rem connects these functions to the relationship between n
univariate distribution functions and their joint (multivariate)
distribution:
Proposition II.2. Let the random variables X1, . . . , Xn have
distribution functions F1, . . . , Fn respectively and let H be
their joint distribution function. Then there exists a copula C
such that
H(x1, . . . , xn) = C(F1(x1), . . . , Fn(xn))
(2)
for all xi, . . . , xn ∈ R. If all the Fis are continuous, then the
copula C is unique.
The usefulness of this result lies in the fact that the joint
distribution function of X1, . . . , Xn can be decomposed into n
univariate functions F1, . . . , Fn that describe the behaviour of
the individual variables and another component (namely the
function C) that describes the dependence structure, which
allows to model them independently.
Conversely, it is also possible to extract the dependence
structure from the marginal distributions Fi and the joint
distribution H via
C(u1, . . . , un) = H(F −1
1
(u1), . . . , F −1
n (un))
(3)
where F −1
i
(u) denotes the pseudo-inverse of Fi(x), which is
given by F −1
i
(u) = sup{x|Fi(x) ≤ u}. A special case of this
connection between Copula and random variables leads to an
alternative characterization of independence, which is usually
written as H(x1, . . . , xn) = F1(x1) · . . . · Fn(xn).
Example II.3. If the (unique) copula from (3) turns out to
be the product copula C(u1, . . . , un) = u1 · . . . · un, then the
random variables X1, . . . , Xn are independent.
III.
A COPULA MODEL OF THE QKD NETWORK
A. Summary of the Data
A summary description of the measurement data obtained
from an implemented QKD network in Vienna [3] can be
found in [5]. The following quantities were measured and
are used here (abbreviation in brackets): qubit error rate in
percentage terms (QBER), air temperature (TEMP), relative
humidity (HUM), sunshine duration in seconds (DUR), global
radiation in watt/m2(RAD).
Since we are here focusing on the relationship between
QBER and environmental quantities, we only use data that
were measured on the same device to avoid getting biased
results. The quantiles of our sample of size n = 276 are
displayed in Table I.
Throughout the rest of the paper, let D denote the data
matrix that comprises the entirety of samples as a table with
headings corresponding to the row labels in Table I. Thus, the
matrix D is of shape (n×5) for our n = 276 samples, and has
entries (X1, . . . , X5) modeling the measurements of (QBER,
TEMP, HUM, DUR, RAD) as random variables.
125
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE I. Quantiles of measured quantities
min
q0.25
median
q0.75
max
QBER
0.98
1.33
1.47
1.63
2.12
TEMP
117.00
134.75
148.00
163.00
184.00
HUM
71.00
80.00
84.00
91.00
93.00
DUR
0.00
0.00
0.00
0.00
600.00
RAD
0.00
0.00
0.00
146.00
539.00
B. Building up a Model
Mainly interested in the dependence structure, we do not
make explicit assumptions about the distributions of the
quantities each, but rather use U(0, 1)-distributed pseudo-
observations U1, . . . , Un transformed from the empirical dis-
tributions of the quantities. A basic ﬁrst choice is to consider
a multidimensional copula C that models the joint distribution
H of all the quantities via H(x1, . . . , xn) = C(U1, . . . , Un).
Fitting a copula is usually done by maximizing the log-
likelihood function
ℓ(x1, . . . , xn) = log [c (u1, . . . , un)] ,
with c denoting the density of the copula C. In a general set-
ting, this can easily become infeasible in our ﬁve-dimensional
case, so we ﬁrst choose a parametric family Cθ of copulas and
then seek the parameter θ that maximizes the one-dimensional
function
ℓ(θ) = log [cθ (u1, . . . , un)] .
As for the parametric family, we ﬁrst choose the Gumbel
copula, which is generated by φ(t) = (− ln(t))θ, yielding
C(u1, . . . , un) = exp
n
−[(− ln(u1))θ + . . . + (− ln(un))θ]1/θo
.
A p-value of zero clearly shows that this model is not
describing the data properly.
The above model is simple to construct and to use but it
also has its weaknesses: ﬁrstly it describes the behaviour of
ﬁve random variables with just one number and secondly its
components are all exchangeable. Taking a closer look at the
pairwise correlations of the considered quantities (Figure 1),
we see that this exchangeability is not fulﬁlled in our case.
To take care of possibly different correlations among the
occurring variables, we consider a more ﬂexible model called
nested copulas (sometimes also called hierarchical copulas),
which is often used in ﬁnance, see for example [15]. The basic
idea of a nested copula model is to use several copulas at
different levels to describe the relation between the variables.
For clarity of such a hierarchically constructed probability
distribution we use a graphical tree-notation like shown in
Figure 2 to “depict” the (otherwise complicated) distribution
function. To formally specify the latter, we introduce some
notational conventions: at each level ℓ ∈ 1, . . . , L (counting
bottom-up in the hierarchy tree) we have nℓ copulas, where
Cℓ,j, j ∈ 1, . . . , nℓ, is the j−th copula at level ℓ. Further,
every copula Cℓ,j has dimension dℓ,j that gives the number of
arguments ui that directly or indirectly enter this copula.
QBER
120
140
160
180
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
GG
GGGG
GG
GGG
G
G
G
G
GGGGGGG
G
G
G
G
GGG
G
G
GG
G
GGGG
G
GG
GG
G
GG
G
G G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
GGG
G
G
G
G
G
G
G
GGGG
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
GGG
GGGGGGGG
G
G
G
GGG
G
GG
G
G
GGGG
G
GG
GG
G
GGG
G
G
GG
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
GG
G
G
G
GG
G
G
GG
G
G
G
G
GGGGG
GG
GGG
G
GG
GGGG GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GGG
GGG
G
GG
G
G
G
G
G
G
G
GG
G
GGG
G
G
GG
G
GG
G
G
G
G
G
G
G
G
GG
G
GGG
GG
G
G
G
G
G
GG
G
G
G
GGG
G
G
G
G
G
G
G
G
GG
GG
GGG
GGGGGGG
GG
G
G
GGG
G
G
GG
G
G
G G
GG
G
G
G
G
G
GG
GGG
GGG
G
G
G
G
G
GG GG
GGG
G
GG
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
0
200
400
600
G
G
GG
G
G
G
GG
GG
G
G
G
GG
G
G
GG
G
G
G
G
GGGGG
GG
GGG
G
GG
GGGGGG
GGG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GGG
G
GG
G
G
G
G
G
G
G
GG
G
GG
G
G
GG
G
GG
G
GG
G
G
G
G
GG
G
GGGG
G
G
G
G
GG
G
G
GGG
G
G
G
G
G
G
G
G
GG
GG
GG
G
GGGGGGGGG
G
G
GG
G
GG
G
G
GG
GG
G
G
GG
G
GGGGG
G
G
G
G
G
G
G
G
G
GGGG
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
1.0
1.4
1.8
G
G
GG
G
G
G
GG
GG
G
G
G
GG
G
G
GG
G
G
G
G
GGGGG
GG
GGG
G
GG
GGGGGG
GGG
GG
G
G
G
G
GGGG
GG
G
G
GG
G G
G G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
GG
G
G
GG
G
GG
G
GG
G
G
G
G
GG
G
GGGG
G
G
G
G
GG
G
G
GGG
G
G
G
G
G
G
G
G
GG
GGG
GG G
GG
GGG
GGG
G
G
G
G
G
GGG
G
GG
G
G
GG
GG
G
G
G
G
G
G
G
GG
G G
G
G
GG G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
120
140
160
180
G
G
G
GG
G
G
G
GG
G
G
GG
G
G
GGG
GGG G
G G
G G
G
GGG
GG GG GGG
G
GG GGG
G
G
GGG G
GGG
GGG
G
G
GG
G
GGGGG
GG
GG
G
G
G
G G
G GG GG
GG
GG
G G G
G
G
GG G GGG
G G GG
GG
G
GG
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
GG G
G
G
G
GGG
G
G
G
G
GG
G
GG
GGGG
G
G
GG
G
G
G
GG
G
G
GG
G
G
G
G G
G
GG GG GG
GGG
G
GG G
GGG
GG
G
GG G
G
GG
G
G
G
G
G
G
GG
G
G
G
GG
GGG
G
G
G
G
G GGGG G G
G
GG
G
G
GG
GGG GG
GG
G
GG
G
G
G
G
G
G
GG
G
G G G
G
GG G
GG
G G
TEMP
GGGGGGGGGGGGGGGGGGGGGGG
GGG
GGGG
GG
GGGGGGGGG
GGG
GG
GGG
GG
G
GG
GGG
GG
GG
GGG GG
G
G
GGGGG
GGGG
GG GGGGG GGGGGGGGGG
GGGGGGGGGG
GG
GG
GGGGGGGGGG
GG
GGGGGGG
GGGGGGGGG GGGGGGGGGGG
GGG
GG
G
G
GG
G
GG
G
G
GGG G
GGG
G
G GG
G
GGGGGGGGGG GGGGGGGG
G
GG G
GGGGGGGGGGGGGGGG
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
GGGGGGGGGGGGGGGGGGGGG
G
G
GGGGGGG
GG
G
GG
G
G
G
G GGGGGGGG
G
G
G
GGGGGGGGGGGG
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
G
GGGG
GGG GG
GG
GGG
GG
GG
GGGGG
GGGG
GGGG
G
G
G G
G
G
G
GGGGGGGGGG
G
GGG
GGGGGGGGGGGGGGGG
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
GGGGGGGGGGGGGGGG
GGG
GGGG G
G G
G
G
GGGGG
G
G
GGG
G GG
G
G
GG
GG
GG
G
GGG G
G GG
G
G G
G
G
G
GGGGGGGGG
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG GGGGG GG
G
GGG
GG
G
G
G
GG
G
GGG
GGGG
GG
GG
GG
G
G
G
G
GGG
G
G
G G
G
G
GG G
G
G
G
G
G
G
G
G
GGGGGGG
G
GGG
GGGGGGGGGGGGGGGG
G
G
G
G
G
G
G
GG
G
G
GG
G
G
GG
GG G
G G
G G
G
GGG
G GG GGG
G
GG GG
G
G
GG G
GG
G
G
G
GG
G
G
GG
G
GG
GG
G
G
G GG G
G G
G
GG
G G G
G
G
G G
GG
G G GG
G
G
GG
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG G
G
G
G
G
GG
G
G
G
G
GG
G
GG
GGGGG
G
G
G G
G
G
GG
G
G
G
G
G
G
G G
G
GG G G
GG
G
GG G
GGG
GG
GGG G
G
GGGG
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G GGG G G
G
GG
G
G
GG
GGG G
GG
G
GG
G
G
G
G
G
G
GG
G
G G G
G
GG G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGGGGGG
G
G
GG
G
G
GGGGGGGGGG
GGGGGGGG
GG G
GGG
GG
GGG
GGGGG GG
G
GGG
GG
GG
G
G
G
G
G
G
GG
G
GG
G
GG
G
G
G
G
G
G
G
G
G
GG
GGG
GGGG
GG
G
G
G
GGGG
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GGGG
GGGGGG
GG
GGGGGG
GGG
GGG
G
GGGG
G
GGG
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
HUM
GGGGGGGG
GGG
GGGG
G
GGGGGGGG
G
G
GG
G
G
G
G GGGGGG
G
G
G
GG
GGG
GGGGG
GGGG
GGGGGGGGGGGG
GGG
G
GGGG GG
GGGG
GG
G
G
GGGGGG
GGG
G
G
G G
G
G
G G
GGGGG
G
GG
GGGGGGG
75
80
85
90
GGGGGGGG
GGGGGGG
GG
GGG
GGGG G
G G
G
G
GGGGG
G
GGG
G
G GG
GG
GG
GG
GG
G
GGG G
G
G
G
G
G G
G
G
G
GGG
GGGGG
GGGG
GGGGGGGGGGGG
GGG GGGGG GG
G
GGG
GGG
G
G
GG
G
GGGGG
GG
GG
G
G
GG
G
G
G
GG
G
G
G
G G
G
G
GG G
G
G
G
G
G
G
G
G
GGGGG
G
GG
GGGGGGG
0
200
400
600
G
G
G
G
G
G
G
GG
G
G
GG
G
G
GG
GG G
G G
G G
G
GGG
G GG GGG
G
GG GG
G
GGG G
GG
G
G
G
G
G
GGG
G
G
G
G
G
G G G
G G
G
G
G
G
G G
G
G
G
G
G
G
G G G
G
G
GG
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
GG
G
G
G
G
GG
G
GG
GGG
G
G
G G
G
G
G
G
G
G
G
G
G
G G
G
GG G G
GG
G
G
G
G
GGG
GG
GG
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
GGG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
GG G
GG
GG
G
G
G
G
G
G
G
G
G G G
G
GG G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGGGGGG
G
GGG
G
G
GGGGGGGGGGGGGGGGG
G
G G
GGG
GG
GGG
G
GGG
G
G
G
GGG
GG
GG
G
G
G
G
G
G
GG
G
GG
G
GG
G
G G
G
G
G
G
G
G
GG
GGGGGGG
GGG
G
G
GGG
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GGGG
G
G
GGGGG
G
G
GG
GGGGG GG
G
GGG
G
GGGG
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GGGGGG
G
G G
GG
GGG
G
G
G
GG
GG
GGG
G
G
GG
G
G
G
G
GG
GGG
G
G
GG
GG GGG
GGGG
G
GG
GGGG
GG
GGGG
GGGG GGG
G
GGG
G
GG
G GGGG
G
G
GGG
GG
G
G G
GG
G
G
G
G
G
G
G
G GGGGG
G GG GGGGGGG
DUR
GGGGGGG
GGG
GGGG G
G
G G
G
GGG
G
GGGG
G
GG
G
G
G
G
GG
GG
G
GGG G
G
G
G
G
G
G
G
G
G
GG GGGGG
G
GG
GGG
GGG
G
G
G
G
G
GGG
GGGG
G
G
GG
G
G
G
GGGG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
1.0
1.4
1.8
G
G
G
G
G
G
G
GG
G
G
GG
G
G
GG
GG G
G G
G G
G
GGG
G GG GGG
G
GG GG
G
GGG G
GG
GG
G
G
G
G
GGGGGG
GG
GG
G
G
G
G
G
G GGG GG
G
G
GG
G
G
G G
G
G
GGG
GG
GG G
G
G
G
GG
G
GG
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
GG
G
G
G
G
GG
G
GG
GGG
G
G
G G
G
G
G
G
G
G
G
G
G
G G
G
G G
GG
GGG
G
G
G G
GGG
GG
GGG
G
G G
G
GGG
G
G
G G
G
GG
G
G
G
GGG
GG
G
G
G
G
G
G G
G
G
G G G
G
GG
G
G
GG
GG G
GG
GG
G
G
G
G
G
G
G
G
G G G
G
GG G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGGGGGG
G
GGG
G
G
GGGGGGGGGGGGGGGGGGG
G
G
G
G
G G
GGG
GG
G
GGG
G
GGG G
G
G
GGG
GGG
GG
GG
G
G
G
G
GG
G
GG
G
GG
G
GG
G
G G
G
G
G
G
G
G
GG
GGGGGGG
GGG
G
G
GGG
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
GG
GGGGGGGGG
G
G
GGG
GGGGG
GG GG
GGGG
G
GGG
GG
G
GGG
G
GG
G
G
G
GG
G
GGG
G
G
G
G
G
G
G
G
GGG
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
75
80
85
90
GGGGGG
G
G G
GGGGGG
GGGG
GGG
GG
G
G
G
G
G
GGG
GG
G
G
GG
G
G
GGG
G
GGGG
GGGG
GG
GGG
GGG
GGG GGGG
GGGG
G
GG
GGGG
GG
GGGG
GGGG GGG
GGGGG
GGGGGGGG
GG
G
G
GG
G
GGGG
G GG
G
GGG G
GGGG
G
G GG
G
GGG
G
G
GGGGGGGG GGGGGG
G GG GGGGGGG
GGGGGGGGGGGGGG
G
G
G
G
GGGGG
G
GGG
G
GG
G
G
G
G GGGGGG
GG
GGG
G
G
G
GGGGGG
GGGGG
G
GGGGGGGGG
G
G GG
GGG
GGGG
GG
GGGGGG
GGGGG
GGG
G
G
G G
G
G
G
GGGGG
0
200
400
0
200
400
RAD
Figure 1. Pairwise correlations among variables
θ = 31
θ = 21
θ = 11
u3
u2
u1
u4
θ = 31
θ = 21
θ = 11
u1
u2
u3
u4
Figure 2. Fully nested vs. partially nested copula
For the sake of illustration only, two example cases of
nesting are shown in Figure 2 for the four-dimensional case:
the fully nested copula, which adds one dimension at each
step (left side) and a partially nested copula where the number
of copula decreases at each level (right side). Our task in the
following is ﬁnding out the particular structure of nesting of
the random variables, based on the empirical data available (on
which, e.g., Figure 1 is based on).
Formally, a fully nested copula is deﬁned by
C(u1, . . . , un) =
φ−1
n−1[φn−1(. . . [φ2(φ−1
1 [φ1(u1) + φ1(u2)] + φ2(u3)]
+ . . . + φn−2(un−1)) + φn−1(un))],
(4)
where the occurring generator functions φ1, . . . , φn−1 may
come from different families of Archimedean copulas.
All in all, the dependence structure is determined by n − 1
parameters (instead of just one as in the model above) and
there are n(n−1)
2
different bivariate margins.
126
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

θ = 1.6
θ = 5.79
θ = 2.58
QBER
HUM
TEMP
DUR
θ = 3.54
RAD
Figure 3. Dependence structure for HAC model
The partially nested copula may be deﬁned similarly, for
reasons of clarity and comprehensibility we here give the
expression for n = 4, corresponding to the case shown in
the right side of Figure 2:
C(u1, u2, u3, u4) = φ−1
21 [φ21(φ−1
11 [φ11(u1) + φ11(u2)]
+ φ21(φ−1
12 [φ12(u3) + φ12(u4)])],
(5)
where the generator φij is from the jth copula on the ith level,
usually denoted by Cij.
Finding a suitable nested copula model may quickly become
laborious since one might have to check all possible subsets of
variables and compare the goodness of ﬁt of the corresponding
estimated copula. Handling this problem in R, one may use
the package HAC, introduced in [18]. In our case, we ﬁnd
that a suitable model consists of four two-dimensional Gumbel
copulas, which are deﬁned as follows:
Deﬁnition III.1. A Gumbel copula is an Archimedean copula
that is generated by
φ(t) = (− ln(t))θ
for θ ≥ 1. In the two-dimensional case, the copula is explicitly
given by
C(u, v) = exp
"
−

(− ln(u))θ + (− ln(v))θ
 1
θ #
(6)
for u, v ∈ [0, 1].
The dependence structure between the considered quantities
is shown in Figure 3.
It is known that in a nested copula model with a Gumbel
generator the parameters have to decrease with the level (see
[15] for fully nested copulas and [19] for the general case).
Since in our case the parameters on the upper levels are rather
close, we consider a modiﬁcation of this model by allowing to
aggregate Copulas whose parameters do not differ too much.
A justiﬁcation for this approach is the close relation between
the parameter θ of the generator and Kendall’s tau τ, which is
connected to copulas via
τ = 4
Z
[0,1]2 C(u, v)dC(u, v) − 1.
(7)
For Archimedean copulas with generator function φ(t), it was
shown in [17] that (7) simpliﬁes to
τ = 1 + 4
Z 1
0
φ(t)
φ′(t)dt,
(8)
which for the Gumbel copula leads to
τ = 1 − 4
Z 1
0
(− log(t))θ · t
θ(− log(t))θ−1 dt
= 1 − 1
θ.
Hence, if the parameters of two subsequent copulas are close,
so is their dependence when characterized through Kendall’s
τ and it might be beneﬁcial to model the affected variables
with only one copula.
These calculations can conveniently be done with the help of
Rs function estimate.copula from the HAC package. This
function estimates both the structure of the hierarchical copula
as well as all corresponding parameters for several different
Archimedean copula families. The ﬁtting is most commonly
done by Maximum Likelihood or quasi Maximum Likelihood.
A simple improvement of this estimation is given in appendix
A. Once a suitable model has been found the HAC package also
allows to compute the density or the cumulative distribution
function for a sample from the corresponding hierarchical
copula, which will be used to test the goodness of ﬁt as
described below.
C. Goodness of ﬁt test for Hierarchical Archimedian Copulas
In order to get an impression on how suitable each of the
above models is, we adapted the bootstrapping goodness of ﬁt
test [20] that was used in the case of a one-parametric copula
to the estimation of nested copulas.
We leave the details of the testing algorithm to the literature
[20], and conﬁne ourselves to a brief description here and an
implementation outline in appendix A, to make things at least
plausible: in general, we would consider a model Fﬁt as a
“good ﬁt”, if its Cramer-van Mises statistic being the integrated
squared difference between Fﬁt and the true distribution is
“small”. The exact numeric magnitude (limit) for a value to
be “small” in that sense is unclear, however, and must be
ﬁxed ﬁrst. This is done by bootstrapping: to get an idea of
when a deviation is “small” (good ﬁt) or “large” (bad ﬁt),
we draw artiﬁcial data samples from the estimated model
Fﬁt, and re-ﬁt another model Fre-ﬁt to the so-obtained data.
Since the new model is based on data coming from Fﬁt, its
deviation from Fﬁt, i.e., its Cramer-van Mises statistics, must
be “small” in the sense we need (no matter of its particular
numerical magnitude). Given this bootstrapping threshold for
127
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

small deviations, we can then move on testing the real data
against the ﬁtted model Fﬁt, computing another Cramer-van
Mises statistic. This ﬁnal value is then compared to be larger
or smaller than the previously obtained bootstrapping threshold
(limit for small deviations) to obtain an empirical p-value of
the test.
In our ﬁrst 200 trial tests, each of which with a sample size
of N = 1000 and a conﬁdence level of 0.95, we never got a
positive p-value if the tolerance was set to zero. When copulas
are allowed to be aggregated, a p-value of 0.014 was found
once, which still leads to rejection of the null hypothesis that
the data at hand are drawn from a distribution given through
this copula. This indicates that some preconditioning of the
data matrix might be necessary to get a good ﬁt. One solution
for such a preprocessing is described in the next section.
D. Preconditioning Towards Better Fits
As indicated by our quantum network data, it may oc-
casionally be the case that none of the tried copula-models
models the data satisfactorily. More precisely, existing software
packages for copula ﬁtting (such as HAC in R) assume positive
correlations between all variables of interest. Unfortunately,
our experimental QKD prototype supplied data exhibiting
negative correlations amongst some of the observed variables.
In order to ﬁx this, we can apply a linear transformation M
to the data matrix D in order to make all pairwise correlations
in the transformed data matrix M · D strictly positive. To this
end, consider the Cholesky-decomposition of the covariance
matrix Σ of the data D, given as Σ = UT · U = UT · I · U.
By the linearity properties of covariance, it is easy to check that
the covariance matrix of D·U−1 is the identity matrix, having
zero-correlations among all pairwise distinct variables. It is
then a simple matter of multiplication with another invertible
matrix (with low condition number to avoid numerical round-
off-errors in the inverse transform) with all strictly positive
entries to artiﬁcially introduce positive correlations, as required
in the copula ﬁtting process. Given such a matrix A, the ﬁnal
linear transformation takes the form
D′ := D · (U−1 · A),
(9)
thus our pre-conditioning transformation matrix is M := U−1·
A, where U comes out of the Cholesky decomposition of the
original covariance matrix Σ, and A can be chosen freely,
subject to only positive entries and a good condition number
(for numerically stable invertibility).
In our experiments, we used a bootstrap ﬁtting with toler-
ance ε = 0.4. We constructed A as a 5 × 5-matrix having
Gamma-distributed entries (with shape-parameter 5 and scale-
parameter 1/2). In 5 out of 200 trials, the p-value after pre-
conditioning with M = U−1A was larger than 0.05. The best
ﬁt giving p = 0.613 was obtained under the transformation
coefﬁcients (rounded to three decimals after the comma)
A =





0.122
4.444
0.378
1.634
4.384
0.650
0.870
1.321
0.941
2.293
0.606
3.326
0.763
2.172
2.102
2.534
0.415
2.055
1.969
1.659
2.668
2.031
3.590
2.241
1.015




 ,
whose condition number is ∥A∥2 · ∥A−1∥2 ≈ 24.4945, and
determinant given as det(A) ≈ 29, thus indicating good
numerical stability for the inverse transformation.
In a second run of 200 experiments, we lowered the toler-
ance ε = 0, and did the preconditioning as before. This time,
we got 20 out of 200 trials with a positive p-value, although
only in three cases, our ﬁt was accepted at p > 0.05. The best
ﬁt was obtained at p = 0.536, showing that the preconditioning
works equally well with more complex hierarchical structures
due to lower tolerance levels.
This transformation is applied before the copula ﬁt, and
must be carried through the derivation of predictive densities
when obtaining a ﬁt. More speciﬁcally, with the preconditioned
random vector being Y = M · X to which we could ﬁt a
density function (copula model) fY , then the original data X
is distributed with density function
fX(x) = fY (M · x) · |det(M)| ,
(10)
where the determinant is a constant, and not even the inversion
of the transformation matrix M is actually required.
The preconditioning does come at the drawback of loosing
the copula-representation of the joint density, which simpliﬁes
the subsequent construction of conditional (predictive) densi-
ties. Without this representation, i.e., when one is forced to
work with a model of the form (10), computing conditional
and predictive densities works via the deﬁnition, i.e.,
f(x1|x2, . . . , xn)
= f(x1,...,xn)
f(x2,...,xn)
=
f(x1,...,xn)
R
R f(x1,...,xn)dx1 ,
(11)
where f(x1, . . . , xn) is the joint density obtained through (10)
and the marginal density can be computed by (numerical)
integration (e.g., Monte-Carlo algorithms; cf. [21]), which can
be complex. To ease matters, we thus assume the model of the
joint variables to take the form (2) as in proposition (II.2).
As an open issue, moreover, it remains interesting to ﬁnd
better ways than simple try-and-error to ﬁnd a preconditioning
matrix A that gives better ﬁts than the plain data would do.
Moreover, we believe that this trick may be of independent
interest and use in other applications of copula theory, not
limited to statistical descriptions of quantum key distribution
devices.
IV.
PREDICTION OF QBER RATES
Based on a model that describes the relationship between
QBER and the environmental quantities, we look for a pre-
diction of the QBER when all the other quantities are known.
Having an idea of what values are to be expected, one might
128
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

suspect an adversary to be present if these values are clearly
exceeded. An essential ingredient to ﬁnd a prediction is the
conditional density, as it shows which values are likely in
a given situation, that is, we seek the density of QBER
conditional on all the other environmental parameters, i.e., the
function
f(QBER|TEMP, HUM, DUR, RAD).
Section IV-A describes the general technique to compute the
sought density, taking QBER as the n-th variable xn in the
upcoming descriptions. We stress that, however, the method
is equivalently applicable to predict any other variable than
QBER, too.
A. Computing Conditional Densities via Copulas
In the case where all the marginals and the copula are con-
tinuous, it holds for the transformed variables ui = F −1
i
(xi)
by the independence of copula and margins that
f(x1, . . . , xn) = f1(x1) · . . . · fn(xn) · cn(u1, . . . , un),
where cn(u1, . . . , un) denotes the density of the n-dimensional
copula Cn(u1, . . . , un) and fi denotes the density of the
marginal distribution Fi.
Example IV.1. In the case of independent random variables,
the above formula yields cn(u1, . . . , un) = 1, which is the
derivative of the independence copula Cn(u1, . . . , un) =
u1 · · · un from Example II.3.
With this decomposition, the conditional density is obtained
as
f(xn|x1, . . . , xn−1) = fn(xn)
cn(u1, . . . , un)
cn−1(u1, . . . , un−1)
(12)
for ui
=
Fi(xi). Using (12) to compute the condi-
tional density requires the lower-dimensional copula density
cn−1(u1, . . . , un−1), excluding the variable un (corresponding
to the variable xn of interest). So, computing the conditional
density (12) from our full n-dimensional copula model pro-
ceeds as follows: let the variable xi range within [xi, xi], then
the (n − 1)-dimensional marginal density is
f(x1, . . . , xn−1) =
Z xn
xn
f(x1, . . . , xn)dxn
=
Z xn
xn
n
Y
j=1
fj(xj)cn(F1(x1), . . . , Fn(xn))dxn
= [∆(xn) − ∆(xn)] ·
n−1
Y
j=1
fj(xj)
with
∆(x) :=
∂n−1
∂x1 · · · ∂xn−1
Cn(F1(x1), . . . , Fn−1(xn−1), Fn(x))
From this, the sought conditional distribution is immediately
found as
f(xn|x1, . . . , xn−1) = fn(xn)cn(F1(x1), . . . , Fn(xn))
∆(xn) − ∆(xn)
(13)
Note that the density fn of the variable of interest can be
estimated both parametrically or non-parametrically (e.g., via
kernel estimators), while in practice the distribution functions
are estimated empirically to avoid additional assumptions.
In a general setting, we ﬁrst compute the copula density (if
the copula at hand is differentiable), the tedious technicalities
of which may conveniently be handled by a computer algebra
system like MATHEMATICA or MAPLE. Again, this procedure
simpliﬁes within a smaller family of copulas.
For a n-dimensional Archimedean copula, the density turns
out to be
c(u1, . . . , un) = (φ−1)(n)(φ(u1) + . . . + φ(un))
n
Y
i=1
φ′(ui)
where (φ−1)(n)(t) denotes the n-th derivative of the inverse
function φ−1(t). This can be computed for Gumbel, Frank
and Ali-Mikhael-Haq copulas, as for example done in [22],
but becomes infeasible for the Gaussian copula considered at
the beginning.
In the case of a nested copula, there is no simple closed
expression available. One has to compute the derivative of the
top level copula that describes the behaviour of all variables
together, which invokes the chain rule. While this may get
complex in the general case, it is still practicable in our case.
In models that involve more levels of sub-copulas than
the one considered here, one might use the derivative of
CL,1(CL−1,1, . . . , CL−1,nL−1) that evaluates to
∂dCL,1
∂u1 · · · ∂ud
=
d−nL−1
X
i=0
X
k1,...,knL−1

∂d−iCL,1
∂Ck1
L−1,1 · · · ∂C
knL−1
L−1,nL−1
×
nL−1
Y
r=1
X
v1,...,vkr
∂|v1|CL−1,r
∂v1
· · · ∂|vkr |CL−1,r
∂vkr

where the outer sum is taken over all integers k1, . . . , knL−1
that sum up to d − i and satisfy kj ≤ dL−1,j while the inner
sum is over partitions v1, . . . , vkr of those ui showing up in
the r-th copula at level L − 1. For more details about this
speciﬁc case, see [19].
B. Self-Adaptation to Environmental Conditions
For a general description, we relabel the variables and
let Xn be the device or performance parameter that we
wish to predict based on the known environmental conditions
x1, . . . , xn−1. Section IV-C illustrates this for Xn = QBER
and (X1, X2, X3, X4) = (DUR, RAD, TEMP, HUM).
A prediction of Xn, e.g., the QBER rate given the current
environmental conditions, is then given by the conditional
expectation or, alternatively, by any value xn that maximizes
129
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

expression (13) for f(xn|x1, . . . , xn−1) for the given values
x1, . . . , xn−1. This maximization can be done using standard
numerical techniques, whose details are outside our scope here.
Since the indication of an adversary’s presence hinges
on known performance characteristics, most importantly the
QBER rate, it is easy to adapt the respective thresholds to the
expected values under the current environmental conditions.
Adapting to different conditions then amounts to doing the
optimization again under the new conﬁguration.
C. A Worked Example
The density c(u1, . . . , u5) of the top level copula CL,1 can
be calculated by applying the chin rule. To avoid errors in
potentially messy calculations like the following, a computer
algebra system may come in handy.
The copula C describing our network was found to be
exp















−



(− ln u1)θ2 + (− ln u2)θ2 θ1
θ2 +



(− ln u3)θ4 + (− ln u4)θ4 θ3
θ4 +
(− ln u5)θ3


θ1
θ3


1/θ1














(14)
Generally, it holds
∂5C3,1
∂u1 · · · ∂u5
=
∂5C3,1
∂2C2,1∂3C2,2
· ∂2C2,1
∂u1∂u2
·
∂3C2,2
∂2C1,1∂u5
· ∂2C1,1
∂u3∂u4
,
where the two most inner derivatives compute as
∂2C
∂u1∂u2
=
1
u1 · u2
(log(u1) · log(u2))θ−1
· exp
"
−

(− log(u1))θ + (− log(u2))θ
 1
θ #
·

(− log(u1))θ + (− log(u2))θ
 1
θ −2
·
 
1.0
1.2
1.4
1.6
1.8
2.0
0
200
400
600
800
1000
1200
QBER in a different environment
QBER
conditional density
Figure 5. Density of QBER in a slightly different environment
1.0
1.2
1.4
1.6
1.8
2.0
0
2
4
6
8
QBER in given environment (extended model)
QBER
conditional density
Figure 6. Density of QBER in a given environment based on the extended
model
the conditional density still exhibits a single maximum and
thus allows again to determine unlikely values.
In appendix A we explain how this estimation procedure can
be improved. Figure 6 shows the conditional density based on
this modiﬁed model. The density exhibits a similar behavior,
i.e. there is a narrow peak corresponding to the most plausible
values of the QBER in the given environment.
A more detailed documentation of our experiments is found
in appendix A, where we give a step-by-step description of
the calculations, augmented by R-code to help the reader in
applying our method in other scenarios.
V.
CONCLUSION
Now, we come back to the initial problem that motivated
this entire study. Recall that in a QKD setting, an unnaturally
high qubit error rate indicates the presence of an adversary.
Conversely, we need an idea about the “natural” rate of qubit
errors. Given the conditional density (12) and according to
the previous remarks, we can thus obtain a threshold for
the qubit error rate that is tailored to the implementation,
environment and device, and which can be adapted to changing
environmental conditions. The steps are the following, and
graphically summarized in Figure 7:
1)
We run the device in a setting where there is no
eavesdropper on the line to draw a series of measure-
ments under clean conditions. In particular, we elicit all
environmental variables of interest, especially the qubit
error rate.
2)
We ﬁt a copula model to the so-obtained data D, pos-
sibly doing a pre-conditioning (as described in Section
III-D) for a statistically and numerically good ﬁt. The
ﬁtting can be done using standard statistical software
like R, using copula-speciﬁc libraries like HAC [18].
The derivation of the conditional distribution is easy by
virtue of computer algebra systems like MATHEMAT-
ICA.
3)
Having the copula-model, we obtain the conditional
distribution (13) of the QBER under all environmental
inﬂuences. Its maximization gives the currently valid
threshold under the present environmental conditions.
Speaking differently, this process tells us which values
of the QBER are not likely enough to occur for a given
value of the keyrate.
The respective details of each step have been described in
previous sections, giving examples along the way to illustrate
the particular tasks. Nevertheless, the above process remains
of generic nature and calls for appropriate instantiation (e.g.,
different environmental inﬂuences such as noisy source and
detectors or turbulence structure of the air could be consid-
ered).
Once the probability density of the QBER conditional on
current working conditions is obtained, it is a simple matter
to equip a QKD device with sensory to keep the expected
natural QBER rate continuously updated. We stress that this
updating is unaffected by the presence of an attacker, unless
the intruder manages to steer the environmental conditions in
a way s/he likes. Assuming the absence of such an ability,
the copula dependency model and its implied predictive dis-
tributions are an effective mean to let the devices re-calibrate
themselves under the changing working conditions. Next steps
in this research direction comprise practical experiments under
variable lab conditions to test the quality of QBER adaption in
terms of a performance gain over statically conﬁgured devices.
As an important side-effect, this would also reveal possibilities
131
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

data
fit copula 
model 
(sect. III.B)
samples from a 
random vector 
(X1, …, Xn)
goodness-of-
fit-test
do preconditioning
(sect. III.D)
good fit
bad 
fit
was a preconditioning 
required?
inverse trans-
formation (eq. (9))
yes
compute 
(numerically) via 
eq. (10)
compute directly 
(sect. IV.A, eq. 
(12))
model (predictive 
density, eq. (10))
no
current environmental 
conditions
compute 
expected channel 
characteristics
numerical 
optimization
re-calibrate the 
device
collect new data
(re-)initiate process 
upon initial or 
changed working 
condition
Figure 7. Building up and using the stochastic models for device calibration
to attack a QKD line by changing environmental factors. Such
an attack has seemingly not been considered in the literature
so far.
REFERENCES
[1]
S. K¨onig and S. Rass, “Self-adaption of quantum key distribution
devices to changing working conditions,” in Proc. of the Interna-
tional Conference on Quanum-, Nano- and Microtechnology (ICQNM).
IARIA XPS Press, 2014, pp. 1–7.
[2]
W. K. Wootters and W. H. Zurek, “A single quantum cannot be cloned,”
Nature, vol. 299, no. 802, 1982, pp. 802–803.
[3]
Peev et al., “The SECOQC quantum key distribution network in
Vienna,” New Journal of Physics, vol. 11, no. 7, 2009, p. 075001.
[4]
K. Lessiak, C. Kollmitzer, S. Schauer, J. Pilz, and S. Rass, “Statistical
analysis of QKD networks in real-life environments,” in Proceedings
of the Third International Conference on Quantum, Nano and Micro
Technologies.
IEEE Computer Society, February 2009, pp. 109–114.
[5]
K. Lessiak, “Application of generalized linear (mixed) models and
nonparametric regression models for the analysis of QKD networks,”
Master’s thesis, Universit¨at Klagenfurt, 2010.
[6]
T. Schmitt-Manderbach, “Long distance free-space quantum key distri-
bution,” Ph.D. dissertation, Ludwig–Maximilians–University Munich,
Faculty of Physics, 2007.
[7]
H. Xu, L. Ma, A. Mink, B. Hershman, and X. Tang, “1310-nm quantum
key distribution system with up-conversion pump wavelength at 1550
nm,” Optics Express, vol. 15, Jun. 2007, pp. 7247–7260.
[8]
M. Li et al., “Measurement-device-independent quantum key distribu-
tion with modiﬁed coherent state,” Opt. Lett., vol. 39, no. 4, Feb 2014,
pp. 880–883.
[9]
P.
Jouguet,
S.
Kunz-Jacques,
A.
Leverrier,
P.
Grangier,
and
E.
Diamanti,
“Experimental
demonstration
of
long-
distance
continuous-variable
quantum
key
distribution,”
Nature
Photonics,
no.
5,
2013,
pp.
378–381.
[Online].
Available:
http://www.nature.com/nphoton/journal/v7/n5/full/nphoton.2013.63.html
[retrieved: September, 2014]
[10]
A. Ac´ın, N. Brunner, N. Gisin, S. Massar, S. Pironio, and V. Scarani,
“Device-independent security of quantum cryptography against collec-
tive attacks,” Physical Review Letters, vol. PRL 98, 230501, no. 1–4,
2007.
[11]
Y. Liu et al., “Experimental measurement-device-independent quantum
key distribution,” Phys. Rev. Lett., vol. 111, no. 13, 2013, p. 130502.
[Online]. Available: http://www.biomedsearch.com/nih/Experimental-
Measurement-Device-Independent-Quantum/24116758.html [retrieved:
September 2014]
[12]
C. Ruican, M. Udrescu, L. Prodan, and M. Vladutiu, “Adaptive vs.
self-adaptive parameters for evolving quantum circuits,” in Evolvable
Systems: From Biology to Hardware, ser. Lecture Notes in Computer
Science, G. Tempesti, A. Tyrrell, and J. Miller, Eds.
Springer Berlin
Heidelberg, 2010, vol. 6274, pp. 348–359.
[13]
C.-J. Lin, C.-H. Chen, and C.-Y. Lee, “A self-adaptive quantum radial
basis function network for classiﬁcation applications,” in Proc. of
International Joint Conference on Neural Networks, Vol. 4.
IEEE,
July 2004, pp. 3263–3268.
[14]
A. M. Al-Adilee and O. N´an´asiov´a, “Copula and s-map on a quantum
logic.” Inf. Sci., vol. 179, no. 24, 2009, pp. 4199–4207.
[15]
P. Embrechts, F. Lindskog, and A. McNeil, Modelling Dependence with
Copulas and Applications to Risk Management, Handbook of Heavy
Tailed Distributions in Finance, Elsevier, 2001.
[16]
D. Kelly, “Using copulas to model dependence in simulation risk assess-
ment,” in Proc. of 2007 ASME International Mechanical Engineering
Congress and Exposition. American Society of Mechanical Engineers,
2007, pp. 81–89.
[17]
R. Nelsen, An Introuction to Copulas.
Springer, 2006.
[18]
O.
Okhrin
and
A.
Ristig,
“Hierarchical
archimedean
copulae:
The
HAC
package,”
Journal
of
Statistical
Software,
vol.
58,
no. 4, 2014, pp. 1–20. [Online]. Available: http://sfb649.wiwi.hu-
berlin.de/papers/pdf/SFB649DP2012-036.pdf
[retrieved:
September,
2014]
[19]
C. Savu and M. Trede, “Hierarchies of Archimedean copulas,” Quanti-
tative Finance, vol. 10, no. 3, February 2010, pp. 295–304.
[20]
C. Genest and B. R´emillard, “Validity of the parametric bootstrap for
goodness-of-ﬁt testing in semiparametric models,” Annales de l’institut
Henri Poincar´e (B) Probabilit´es et Statistiques, vol. 44, no. 6, 2008, pp.
1096–1127. [Online]. Available: http://eudml.org/doc/78005 [retrieved:
September, 2014]
[21]
C. P. Robert, The Bayesian choice.
New York: Springer, 2001.
[22]
C. Savu and M. Trede, “Goodness-of-ﬁt tests parametric families of
Archimedean copulas,” Quantitative Finance, vol. 8, no. 2, March 2008,
pp. 109–116.
[23]
J.-D. Fermanian, D. Radulovic, and M. Wegkamp, “Weak convergence
of empirical copula processes,” Bernoulli, vol. 10, no. 5, 10 2004, pp.
847–860. [Online]. Available: http://dx.doi.org/10.3150/bj/1099579158
APPENDIX
To ease reproducing our computations in practical appli-
cations, we attach our R-implementation of the procedures
sketched in the previous paragraphs here. Inline, we comment
132
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

on the code where necessary to extend the description in the
body of the paper.
The libraries that we used were copula, HAC and MASS.
The original data has been loaded into a data frame X.
The following code decorrelates the data and leaves a data
frame Y whose covariance structure is the identity matrix:
U <- chol(cov(X)) # Cholesky decomposition
Uinv <- solve(U)
# inversion of U
X <- as.matrix(X) # coerce X into a matrix
Y <- X%*%Uinv
# do the decorrelation
This data frame is then (positively) recorrelated by the matrix
A as described in Section III-D.
A <- matrix(c(...)) # matrix values
Z <- Y%*%A
# re-correlation
In the paper this whole process is described by equation (9).
Given the positively recorrelated data, the ﬁtting method
from the HAC package applies, giving us a copula model and
the θ-values (cf. Figure 3). We used full maximum likelihood
estimation (ML) here.
UZ <- pobs(Z)
# pseudo observations
estim.full <- estimate.copula(
UZ,
method = 2, # = full ML
hac = estim,
margins = NULL,
epsilon = 0.4)
theta.full <- get.params(estim.full)
At this stage, we ought to check the goodness of ﬁt for
the copula model. Here, we enter the bootstrapping stage as
sketched in Section III-C. An empirical d-dimensional copula
based on n data records in a matrix V ∈ Rn×d is deﬁned by
CV(u) =
1
n
Pn
i=1 I(V i
1 ≤ u1, . . . , V i
d ≤ ud), where I is an
indicator function. (The estimate ˆC(u) = CV(u) is known to
converge uniformly to the underlying true copula, at least in
the case of independent marginal distributions [23].)
empCop <-function(V, u){
1/n * length(which(V[,1] <= u[1] &
V[,2] <= u[2] &
V[,3] <= u[3] &
V[,4] <= u[4] &
V[,5] <= u[5]))
}
Next comes the bootstrapping procedure, which takes N iter-
ations (N = 1000 in our experiments). A single test for the
goodness of ﬁt can be implemented as follows:
estimatedCopula <- estimate.copula(UZ,
method = 1, margins = NULL,
epsilon = 0.4)
This estimate can be improved in the following way:
# quasi ML estimation as before (method = 1)
qMLCopulaEst <- estimate.copula(UZ,
method = 1, margins = NULL,
epsilon = 0.4)
# update (method = 1 -> full ML)
estimatedCopula <- estimate.copula(UZ,
method = 2,
hac = qMLCopulaEst,
margins = NULL, epsilon=0.4)
Notice however that this increases the runtime signiﬁcantly.
For the bootstrap (as prescribed by [20]), we need to cast the
observations into uniformly distributed values by applying the
empirical copula function based on the pseudo-observations
UZ from above. This gives the data matrix C1. The estimated
copula should, by construction, resemble this data quite well,
and thus perform equally good as the empirical copula function
in casting the observations into uniformly distributed values.
Hence, we should almost obtain the same results by applying
the ﬁtted copula (distribution function pHAC) to UZ, giving the
observation data C2. The difference between the two tells the
numeric magnitude of a “small deviation” between the data
and the model (cf. Section III-C).
C1 <- apply(UZ, 1,
function(x)(empCop(UZ,x)))
C2 <- pHAC(UZ,
estimatedCopula,
margins=NULL)
Sn <- sum((C1 - C2)ˆ2)
# bootstrap value
The actual bootstrap is done by drawing random values from
the copula model (function rHAC), turning it into pseudo-
observations and estimating the copula in the same way as
before, but based on the random observations now. Over N
repetitions (we took N = 1000), the k-th such ﬁt is “accepted”,
if its deviation Snk is less than Sn, as computed above, i.e.,
the p-value of the test is deﬁned as [20] p = 1
N
Pn
k=1 I(Snk >
Sn), with Sn being Sn from above. To save space in the listing
below, the ellipsis (...) in the parameter list is to be replaced
by the same parameters in the identical calls as in previous
listings.
pValueEst <- 0
for(k in 1:N) {
Xk <- rHAC(n, estimatedCopula)
Uk<-pobs(Xk)
bootstrapQML <- estimate.copula(Uk,
method = 1, ...)
bootstrapEst <- estimate.copula(Uk,
method = 2,
hac = bootstrapQML, ...)
C1 <- apply(Uk, 1,
function(x)(empCop(Uk,x)))
C2 <- pHAC(Uk, bootstrapEst, ...)
Snk <- sum((C1 - C2)ˆ2)
if (Snk > Sn) {
133
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

pValueEst <- pValueEst + 1
}
}
pValueEst <- pValueEst / N
Our experiments revealed that a single trial usually yields
not a good ﬁt, so the above iteration can be repeated until
a sufﬁciently large p-value is obtained (in our setting, we took
200 rounds to come up with a few good ﬁts).
Given that the ﬁt has a p-value > 0.05, we accept it and step
towards estimating the predictive density; equation (13): First,
we need the unconditional density of QBER, which in our case
is the ﬁrst variable in the (still re-correlated) data frame Z. We
ﬁtted a gamma-distribution by maximum likelihood:
f <- fitdistr(Z[,1], "gamma")
fn <- function(x) {
dgamma(x, shape = f$estimate[1],
rate = f$estimate[2])
}
The conditional density is then directly computed from formula
(13), by ﬁrst transforming the input data into uniformly dis-
tributed values (by applying the empirical marginal distribution
functions obtained from a call to ecdf) and implementing the
expression for ∆ as a function delta (omitted here for space
reasons):
# get the empirical distribution functions
F1<-ecdf(Z[,1]) # QBER
F2<-ecdf(Z[,3]); # HUM
F3<-ecdf(Z[,2]); # TEMP
F4<-ecdf(Z[,5]); # RAD
F5<-ecdf(Z[,4]); # DUR
# range of QBER
qbermin<-min(X[,1])
qbermax<-max(X[,1])
# conditional density function
conddens<-function(DUR,RAD,TEMP,HUM,QBER){
# transform data into uniformly distr.
u1<-F1(QBER); u2<-F2(HUM);
u3<-F3(TEMP); u4<-F4(RAD);
u5<-F5(DUR)
# conditional density formula (13)
fn(QBER) * cn(u1,u2,u3,u4,u5) /
(delta(F1(qbermaxz),u2,u3,u4,u5) -
delta(F1(qberminz),u2,u3,u4,u5))
}
The conddens function is now ready to be used for con-
ﬁguring the device, for example, by determining its maxi-
mum w.r.t. QBER (maximum likelihood estimation), given
the current environmental conditions DUR, RAD, TEMP and
HUM. We stress, however, that care has to be taken since all
this construction works on the transformed data Z rather than
the actual (physical) measurements X. In order to properly
apply the function, we therefore must transform the current
environmental data in much the same way as the data has
been transformed to ﬁnd a suitable model. That is, we apply
the transformation matrix M to the physical input data and use
the results as the arguments in the conddens function: calling
xdat the real environmental conditions (values as given in
Section IV-C), then the transformed zdat is the input to
conddens as described above.
Zdat<-matrix(rep(0,l*5),nrow=l)
# relabel the variables to fit notation
# of the derivatives cn and delta
colnames(Zdat) <- c("QBER", "HUM",
"TEMP", "RAD", "DUR")
# transform QBER-values in given environment
for (i in 1:l){
xdat<-c(x[i],148,90,0,0)
zdat<-t(xdat)%*%Uinv%*%A
zdat<-t(zdat)
DUR<-zdat[4]; RAD<-zdat[5];
HUM<-zdat[3]; TEMP<-zdat[2]; QBER<-zdat[1]
Zdat[i,]<-c(QBER,HUM,TEMP,RAD,DUR)
}
# determine range of transformed data
# (input to function delta)
minz<-min(Zdat[,1])
maxz<-max(Zdat[,1])
The density is then visualized by plotting QBER-values x
against the corresponding output of the conddens function
y for each of those values.
# range of QBER
qbermin<-min(X[,1]) # 0.98
qbermax<-max(X[,1]) # 2.12
x<-seq(qbermin,qbermax,0.01)
l<-length(x)
# corresponding values of density
y<-rep(0,l)
for (i in 1:l){
y[i]<-conddens(Zdat[i,1],Zdat[i,2],
Zdat[i,3],Zdat[i,4],Zdat[i,5])
}
plot(x, y,
type=’l’,
main="QBER in a given environment",
xlab=’QBER’,ylab=’conditional density’)
134
International Journal on Advances in Systems and Measurements, vol 8 no 1 & 2, year 2015, http://www.iariajournals.org/systems_and_measurements/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

