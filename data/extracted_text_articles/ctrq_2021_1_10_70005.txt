Reliability Assessment of Erasure-Coded Storage
Systems with Latent Errors
Ilias Iliadis
IBM Research Europe
8803 R¨uschlikon, Zurich, Switzerland
email: ili@zurich.ibm.com
Abstract—Large-scale storage systems employ erasure-coding
redundancy schemes to protect against device failures. The
adverse effect of latent sector errors on the Mean Time to
Data Loss (MTTDL) and the Expected Annual Fraction of Data
Loss (EAFDL) reliability metrics is evaluated. A theoretical
model capturing the effect of latent errors and device failures
is developed, and closed-form expressions for the metrics of
interest are derived. The MTTDL and EAFDL of erasure-coded
systems are obtained analytically for (i) the entire range of bit
error rates, (ii) the symmetric, clustered, and declustered data
placement schemes, and (iii) arbitrary device failure and rebuild
time distributions under network rebuild bandwidth constraints.
For realistic values of sector error rates, the results obtained
demonstrate that MTTDL degrades, whereas EAFDL remains
practically unaffected. It is also shown that the declustered data
placement scheme offers superior reliability.
Keywords–Storage; Unrecoverable or latent sector errors; Reli-
ability analysis; MTTDL; EAFDL; RAID; MDS codes; stochastic
modeling.
I.
INTRODUCTION
Today’s large-scale data storage systems and most cloud
offerings recover data lost due to device and component
failures by deploying efﬁcient erasure coding schemes that
provide high data reliability. The replication schemes and the
Redundant Arrays of Inexpensive Disks (RAID) schemes, such
as RAID-5 and RAID-6, which have been deployed extensively
in the past thirty years [1-4] are special cases of erasure codes.
Modern storage systems though use advanced, more powerful
erasure coding schemes. The effectiveness of these schemes
has been evaluated based on the Mean Time to Data Loss
(MTTDL) [1-10] and, more recently, the Expected Annual
Fraction of Data Loss (EAFDL) reliability metrics [11-14].
The latter metric was introduced because Amazon S3 [15],
Facebook [16], LinkedIn [17] and Yahoo! [18] consider the
amount of lost data measured in time.
The reliability level achieved depends not only on the
particular choice of the erasure coding scheme, but also on
the way data is placed on storage devices. The reliability
assessment presented in [3] demonstrated that, for a replication
factor of three, a declustered data placement scheme achieves
a superior reliability than other placement schemes. This is
the scheme that was originally used by Google [19], Facebook
[20], and Microsoft Azure [21], but, to improve data reliability
and storage efﬁciency further, today they use erasure coding
schemes that offer higher efﬁciency [22-24].
The reliability of storage systems is further degraded by the
occurrence of unrecoverable sector errors, that is, errors that
cannot be corrected by the standard sector-associated error-
correcting code (ECC) nor by the re-read mechanism of hard-
disk drives (HDDs). These sector errors are latent because their
existence is only discovered when there is an attempt to access
them. Once an unrecoverable or latent sector error is detected,
it can usually be corrected by the erasure coding capability.
However, if this is not feasible, it is permanently lost, leading
to an unrecoverable failure. Consequently, unrecoverable errors
do not necessarily lead to unrecoverable failures. Permanent
losses of data due to latent errors are quite pronounced in
higher-capacity HDDs and storage nodes [25-28]. Analytical
reliability expressions for MTTDL that take into account the
effect of latent errors have been obtained predominately using
Markovian models, which assume that component failure and
rebuild times are independent and exponentially distributed
[7][10][26][27]. The effect of latent errors on MTTDL of
erasure-coded storage systems for the realistic case of non-
exponential failure and rebuild time distributions was assessed
in [28][29] for a limited range of error rates.
In this article, we consider the entire range of sector error
rates and assess the effect of latent errors not only on MTTDL,
but also on the amount of lost data for the realistic case of non-
exponential failure and rebuild time distributions. It is known
that the actual latent-error rates degrade MTTDL by orders of
magnitude [7][10][26][28]. Does this also apply to the case of
the EAFDL metric given that, when a data loss occurs, the
amount of sectors lost due to latent errors is much smaller
than the amount of data lost due to a device failure? What is
the range of error rates that cause EAFDL to deteriorate? This
article addresses these critical questions.
Analytical results for the MTTDL and EAFDL metrics in
the context of general erasure-coded storage systems, but in
the absence of latent errors, were obtained in [11-13]. The
ﬁrst analytical assessment of EAFDL in the presence of latent
errors was presented in [30] for the case of RAID-5 systems by
presenting a comprehensive theoretical stochastic model that
captures all the details of the rebuild process. This model was
subsequently extended to a signiﬁcantly more complex one
for the case of RAID-6 systems [14]. Clearly, extending this
model further to assess EAFDL in the presence of latent errors
for arbitrary erasure coding schemes seems to be a daunting
task because of its state explosion. To assess the reliability of
erasure-coded systems, we adopt the non-Markovian method-
ology developed in prior work [11-13] to evaluate MTTDL and
EAFDL of storage systems and extending it to assess the effect
of latent errors. The validity of this methodology for accurately
assessing the reliability of storage systems has been conﬁrmed
by simulations in several contexts [3][8][11][31]. It has been
1
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-841-9
CTRQ 2021 : The Fourteenth International Conference on Communication Theory, Reliability, and Quality of Service

demonstrated that theoretical predictions of the reliability of
systems comprising highly reliable storage devices are in good
agreement with simulation results. Consequently, the emphasis
of the present work is on theoretically evaluating the reliability
of storage systems with latent errors.
The reliability results obtained by the model developed here
are shown to be in agreement with previous speciﬁc theoretical
and simulation results presented in the literature. We verify
its validity by comparing the results obtained for the cases
of RAID-5 and RAID-6 systems and showing that they match
with those derived by the detailed models in [14]. Furthermore,
we demonstrate that the model developed yields theoretical
reliability results that match well with the simulation results
obtained in [32], which studies the effect of erasure codes
deployed in a realistic distributed storage conﬁguration. This
establishes a conﬁdence for the model presented, the results
obtained, and the conclusions drawn. The model developed is
a practical one that takes into account the characteristics of
latent errors observed in real systems. It is realistic because
it considers general device failure distributions including real-
world ones, such as Weibull and gamma. It can also be used
to assess system reliability when scrubbing is employed by
applying the methodology described in [7]. This is the ﬁrst
work to study the effect of latent errors on EAFDL for general
erasure-coded storage systems.
Note that the storage model considered in this work
is relevant and realistic because it properly captures the
characteristics of erasure coding and of the rebuild process
associated with the declustered placement scheme currently
used by Google [22], Windows Azure [24], Facebook [33],
and DELL/EMC [34]. Consequently, the theoretical results
derived here are important because they can be used to assess
the reliability of the above schemes and also determine the
parameter values that ensure a desired level of reliability. It
can also be used to assess system reliability when scrubbing
is employed by applying the methodology described in [7].
A simulation analysis of reliability aspects of erasure-coded
data centers was presented in [35]. Various conﬁgurations were
considered and it was shown that erasure codes and redundancy
placement affect system reliability. In [32] it was recognized
that it is hard to get statistically meaningful experimental
reliability results using prototypes because this would require
a large number of machines to run for years. This underscores
that usefulness of the analytical reliability results derived in
this article.
The key contributions of this article are the following.
We consider the reliability of erasure-coded storage systems
with latent errors and derive closed-form expressions for the
MTTDL and EAFDL reliability metrics for the entire range of
sector error rates, and for the symmetric, clustered, and declus-
tered data placement schemes. We subsequently demonstrate
that, in the range of typical sector-error rates, unrecoverable
failures are frequent, which degrades MTTDL. However, the
relative increase of the amount of data loss is negligible, which
leaves EAFDL practically unaffected in this range.
The remainder of the article is organized as follows.
Section II describes the storage system model and the cor-
responding parameters considered. Section III presents the
general framework and methodology for deriving the MTTDL
and EAFDL metrics analytically for the case of erasure-
coded systems and in the presence of latent errors. Closed-
TABLE I.
NOTATION OF SYSTEM PARAMETERS
Parameter
Deﬁnition
n
number of storage devices
c
amount of data stored on each device
l
number of user-data symbols per codeword (l ≥ 1)
m
total number of symbols per codeword (m > l)
(m, l)
MDS-code structure
s
symbol size
k
spread factor of the data placement scheme, or
group size (number of devices in a group) (m ≤ k ≤ n)
b
average reserved rebuild bandwidth per device
Bmax
upper limitation of the average network rebuild bandwidth
X
time required to read (or write) an amount c of data at an average
rate b from (or to) a device
FX(.)
cumulative distribution function of X
Fλ(.)
cumulative distribution function of device lifetimes
Pbit
probability of an unrecoverable bit error
seff
storage efﬁciency of redundancy scheme (seff = l/m)
U
amount of user data stored in the system (U = seff n c)
˜r
MDS-code distance: minimum number of codeword symbols lost
that lead to an irrecoverable data loss
(˜r = m − l + 1 and 2 ≤ ˜r ≤ m)
C
number of symbols stored in a device (C = c/s)
µ−1
mean time to read (or write) an amount c of data at an average rate
b from (or to) a device (µ−1 = E(X) = c/b)
λ−1
mean time to failure of a storage device
(λ−1 =
R ∞
0 [1 − Fλ(t)]dt)
Ps
probability of an unrecoverable sector (symbol) error
PDL
probability of data loss during rebuild
PUF
probability of data loss due to unrecoverable failures during rebuild
PDF
probability of data loss due to a disk failure during rebuild
Q
amount of lost user data during rebuild
H
amount of lost user data, given that data loss has occurred, during
rebuild
S
number of lost symbols during rebuild
form expressions for relevant reliability metrics are derived
for the symmetric, clustered, and declustered data placement
schemes. Section IV presents numerical results demonstrating
the adverse effect of unrecoverable or latent errors and the
effectiveness of these schemes for improving system reliability.
Finally, we conclude in Section V.
II.
STORAGE SYSTEM MODEL
To assess the reliability of erasure-coded storage systems,
we adopt the model used in [13] and extend it to cover the
case of latent errors. The storage system comprises n storage
devices (nodes or disks), where each device stores an amount
c of data such that the total storage capacity of the system
is n c. This does not account for the spare space used by the
rebuild process.
A. Redundancy
User data is divided into blocks (or symbols) of a ﬁxed size
s (e.g., sector size of 512 bytes) and complemented with parity
symbols to form codewords. We consider (m, l) maximum
distance separable (MDS) erasure codes, which map l user-
data symbols to a set of m (> l) symbols, called a codeword,
having the property that any subset containing l of the m
symbols can be used to reconstruct (recover) the codeword.
The corresponding storage efﬁciency seff and amount U of
user data stored in the system is
seff = l/m
and
U = seff n c = l n c/m .
(1)
Also, the number C of symbols stored in a device is
C = c/s .
(2)
Our notation is summarized in Table I. The derived param-
eters are listed in the lower part of the table. To minimize the
2
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-841-9
CTRQ 2021 : The Fourteenth International Conference on Communication Theory, Reliability, and Quality of Service

Figure 1. Clustered and declustered placement of codewords of length m = 3
on n = 6 devices. X1, X2, X3 represent a codeword (X = A, B, C, . . . , L).
risk of permanent data loss, the m symbols of each codeword
are spread and stored on m distinct devices. This way, the
system can tolerate any ˜r − 1 device failures, but ˜r device
failures may lead to data loss, with
˜r = m − l + 1 ,
1 ≤ l < m
and
2 ≤ ˜r ≤ m .
(3)
Examples of MDS erasure codes are the replication, RAID-5,
RAID-6, and Reed–Solomon schemes.
B. Symmetric Codeword Placement
In a symmetric placement scheme, the system effectively
comprises n/k disjoint groups of k devices, and each codeword
is placed entirely in one of these groups. Within each group,
all

using the rebuild bandwidth available on each surviving device.
Let us denote by bu (≤ b) the average rate at which the
amount of data corresponding to the number Cu of symbols to
be rebuilt at exposure level u is written to selected device(s).
This rate depends on Bmax, the upper limitation of the average
network rebuild bandwidth [13]. Also, let 1/µ and FX(.)
denote the mean and the cumulative distribution function of
the time X required to read (or write) an amount c of data
from (or to) a device, respectively. Therefore,
µ−1 ≜ E(X) = c/b .
(5)
4) Failure and Rebuild Time Distributions: The lifetimes
of the n devices are assumed to be independent and identically
distributed, with a cumulative distribution function Fλ(.) and a
mean of 1/λ. The results in this article hold for highly reliable
storage devices, which satisfy the condition [13][31]
µ
Z ∞
0
Fλ(t)[1 − FX(t)]dt ≪ 1,
with λ
µ ≪ 1 .
(6)
5) Amount of Data to Rebuild and Rebuild Times at Each
Exposure Level: We denote by ˜nu the number of devices
at exposure level u whose failure causes an exposure level
transition to level u + 1, and Vu the fraction of the Cu most-
exposed codewords that have a symbol stored on any given
such device. Note that ˜nu depends on the codeword placement
scheme. Let Ru denote the rebuild time of the most-exposed
codewords at exposure level u. Then, [12, Eq. (34)]
R1 = (b/b1) X .
(7)
Let αu be the fraction of the rebuild time Ru still left when
another device fails, causing the exposure level transition u →
u+1. In [37, Lemma 2], it was shown that, for highly reliable
devices satisfying condition (6), αu is approximately uniformly
distributed in (0, 1). We proceed by considering that the rebuild
time Ru+1 is determined completely by Ru and αu in the same
manner as in [12][13][36]. A moment’s reﬂection leads to the
following relation:
Cu+1 ≈ Vu αu Cu , for u = 1, . . . , ˜r − 1 .
(8)
Repeatedly applying (8) and using (4) and the convention that
for any sequence δi, Q0
i=1 δi ≜ 1, yields
Cu ≈ C
u−1
Y
i=1
Vi αi , for u = 1, . . . , ˜r .
(9)
6) Unrecoverable Errors: The reliability of storage systems
is affected by the occurrence of unrecoverable or latent errors.
Let Pbit denote the unrecoverable bit-error probability. Accord-
ing to the speciﬁcations, Pbit is equal to 10−15 for SCSI drives
and 10−14 for SATA drives [7]. Assuming that bit errors occur
independently over successive bits, the unrecoverable sector
(symbol) error probability Ps is
Ps = 1 − (1 − Pbit)s ,
(10)
with s expressed in bits. Assuming a sector size of 512 bytes,
the equivalent unrecoverable sector error probability is Ps ≈
Pbit × 4096, which is 4.096 × 10−12 in the case of SCSI and
4.096×10−11 in the case of SATA drives. In practice, however,
and also owing to the accumulation of latent errors over
time, these probability values are higher. Indeed, empirical
ﬁeld results suggest that the actual values can be orders of
magnitude higher, reaching Ps ≈ 5 × 10−9 [38].
III.
DERIVATION OF MTTDL AND EAFDL
The reliability metrics are derived using the methodol-
ogy presented in [11][12][13] and extending it to assess
the effect of latent errors. This methodology uses the direct
path approximation [10], does not involve Markovian analysis
[3][8][11][31][36], and holds for general failure time distri-
butions, which can be exponential or non-exponential, such as
the Weibull and gamma distributions that satisfy condition (6).
At any point in time, the system can be thought to be in one
of two modes: normal or rebuild mode. A ﬁrst device failure
causes a transition from normal to rebuild mode. A rebuild
process attempts to restore the lost data, which eventually leads
the system either to a data loss (DL) with probability PDL or
back to the original normal mode by restoring initial redun-
dancy, with probability 1 − PDL. Any symbols encountered
with unrecoverable or latent errors are usually corrected by
the erasure coding capability. However, it may not be possible
to recover multiple unrecoverable errors in a codeword, which
therefore leads to data loss.
A. Reliability Analysis
At any exposure level u (u = 1, . . . , ˜r − 1), data loss
may occur during rebuild owing to one or more unrecoverable
failures, which is denoted by the transition u → UF. Moreover,
at exposure level ˜r−1, data loss occurs owing to a subsequent
device failure, which leads to the transition to exposure level
˜r. Consequently, the direct paths that lead to data loss are the
following:
−−→
UFu : the direct path of successive transitions 1 → 2 →
· · · → u → UF, for u = 1, . . . , ˜r − 1, and
−−→
DF : the direct path of successive transitions 1 → 2 →
· · · → ˜r − 1 → ˜r,
with corresponding probabilities PUFu and PDF, respectively.
It holds that
PUFu = Pu Pu→UF , for u = 1, . . . , ˜r − 1 ,
(11)
where Pu is the probability of entering exposure level u, which
is derived in Appendix A as follows:
Pu ≈ (λ c)u−1
1
(u − 1)!
E(Xu−1)
[E(X)]u−1
u−1
Y
i=1
˜ni
bi
V u−1−i
i
, (12)
and Pu→UF is the probability of encountering an unrecoverable
failure during the rebuild process at this exposure level.
In [10], it was shown that PDL is accurately approximated
by the probability of all direct paths to data loss. Therefore,
PDL ≈ PDF +
˜r−1
X
u=1
PUFu .
(13)
4
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-841-9
CTRQ 2021 : The Fourteenth International Conference on Communication Theory, Reliability, and Quality of Service

Proposition 1: For u = 1, . . . , ˜r − 1, it holds that
PUFu ≈ − (λ c)u−1 E(Xu−1)
[E(X)]u−1
 u−1
Y
i=1
˜ni
bi
V u−1−i
i
!
· log(ˆqu)−(u−1)
 
ˆqu −
u−1
X
i=0
log(ˆqu)i
i!
!
,
(14)
where
ˆqu ≜ q
C Qu−1
j=1 Vj
u
,
(15)
qu = 1 −
m−u
X
j=˜r−u
m − u
j

P j
s (1 − Ps)m−u−j ,
(16)
PDF ≈ (λ c)˜r−1
1
(˜r − 1)!
E(X ˜r−1)
[E(X)]˜r−1
˜r−1
Y
i=1
˜ni
bi
V ˜r−1−i
i
. (17)
Proof: Equation (14) is obtained in Appendix A. Equation
(17) is obtained from the fact that PDF = P˜r and, subsequently,
from (12) by setting u = ˜r.
We proceed to derive the amount of data loss during
rebuild. Let Q, H, and S be the amount of lost user data,
the conditional amount of lost user data, given that data loss
has occurred, and the number of lost symbols, respectively.
Let also QDF and QUFu denote the amount of lost user data
associated with the direct paths −−→
DF and −−→
UFu, respectively.
Similarly, we consider the variables HDF, HUFu, SDF, and
SUFu. Then, the amount Q of lost user data is obtained by
Q ≈





HDF ,
if −−→
DF
HUFu ,
if −−→
UFu , for u = 1, . . . , ˜r − 1
0 ,
otherwise .
(18)
Thus,
E(Q) ≈ PDF E(HDF) +
˜r−1
X
u=1
PUFu E(HUFu)
(19)
= E(QDF) +
˜r−1
X
u=1
E(QUFu) ,
(20)
where
E(QDF) = PDF E(HDF) ,
(21)
and
E(QUFu) = PUFu E(HUFu) , u = 1, . . . , ˜r − 1 . (22)
Note that the expected amount E(Q) of lost user data is
equal to the product of the storage efﬁciency and the expected
amount of lost data, where the latter is equal to the product
of the expected number of lost symbols E(S) and the symbol
size s. Consequently, it follows from (1) that
E(Q) = l
m E(S) s
(2)
=
l
m
E(S)
C
c .
(23)
Similarly, E(QDF) = l
m E(SDF) s
(2)
=
l
m
E(SDF)
C
c , (24)
and
E(QUFu) = l
m E(SUFu) s
(2)
=
l
m
E(SUFu)
C
c .
(25)
Proposition 2: For u = 1, . . . , ˜r − 1, it holds that
E(QUFu) ≈ c l ˜r
m (λ c)u−1 1
u!
E(Xu−1)
[E(X)]u−1
 u−1
Y
i=1
˜ni
bi
V u−i
i
!
·
m − u
˜r − u

P ˜r−u
s
,
for Ps ≪
1
m − ˜r , (26)
E(QDF) ≈ c l
m (λ c)˜r−1
1
(˜r − 1) !
E(X ˜r−1)
[E(X)]˜r−1
˜r−1
Y
i=1
˜ni
bi
V ˜r−i
i
.
(27)
Proof: Equation (26) is obtained in Appendix B. Equation
(27) is obtained from (26) by setting u = ˜r, which is the same
result as Eq. (25) of [13].
The MTTDL and EAFDL metrics are determined by [11,
Eqs. (4), (5), and (9)]:
MTTDL ≈ E(T)
PDL
=
1
n λ PDL
,
(28)
EAFDL ≈
E(Q)
E(T) · U = n λ E(Q)
U
(1)
= m λ E(Q)
l c
,
(29)
with E(T) = 1/(n λ) and 1/λ expressed in years.
B. Symmetric and Declustered Placement
We consider the case m < k ≤ n. The special case k = m
corresponding to the clustered placement has to be considered
separately for the reasons discussed in Section II-C2. At each
exposure level u, for u = 1, · · · , ˜r − 1, it holds that [12][13]
˜nsym
u
= k − u ,
(30)
bsym
u
= min((k − u) b, Bmax)
l + 1
,
(31)
V sym
u
= m − u
k − u .
(32)
The corresponding parameters ˜ndeclus
u
, bdeclus
u
, and V declus
u
for the
declustered placement are derived from (30), (31), and (32) by
setting k = n.
C. Clustered Placement
It holds that [12][13]
˜nclus
u
= m − u , bclus
u
= min( b , Bmax/l ) , V clus
u
= 1 . (33)
Remark 1: It follows from (33) that a system is not
bandwidth-constrained when Bmax
≥
l b. Then, bclus
u
=
min(b, Bmax/l) = b. In the case of RAID-5 and RAID-6, it
holds that m − l = 1 and m − l = 2 or, equivalently, ˜r = 2
and ˜r = 3, respectively, such that (13), (14), and (17) yield
P RAID-5
DL
≈ (m − 1) λ c
b
+ 1 − (1 − Ps)(m−1) C ,
(34)
which is the same result as Eq. (85) of [14], and
P RAID-6
DL
≈ 1 − qC
1 +

1 + 1 − qC
2
log(qC
2 )

(m − 1) λ c
b
+ (m − 1)(m − 2)
2
λ c
b
2 E(X2)
[E(X)]2 ,
(35)
where q1, q2 are determined by (16). This result is in agreement
with Eq. (243) of [14]. Also, (20), (26), and (27) yield
E(QRAID-5) ≈
l
m (m − 1)
 λ c
b + 2 Ps

c ,
(36)
which is the same result as Eq. (105) of [14], and
E(QRAID-6) ≈ l
m
(m − 1)(m − 2)
2
·
"λ c
b
2 E(X2)
[E(X)]2 + 3 λ c
b Ps + 3 P 2
s
#
c . (37)
This result is in agreement with Eq. (264) of [14].
5
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-841-9
CTRQ 2021 : The Fourteenth International Conference on Communication Theory, Reliability, and Quality of Service

TABLE II.
TYPICAL VALUES OF DIFFERENT PARAMETERS
Parameter
Deﬁnition
Values
n
number of storage devices
64
c
amount of data stored on each device
12 TB
l
user-data symbols per codeword
13, 14, 15
m
symbols per codeword
16
s
symbol (sector) size
512 B
b
rebuild bandwidth per device
50 MB/s
λ−1
mean time to failure of a storage device
300,000 h
to
1,000,000 h
U
amount of user data stored in the system
624 to 720 TB
µ−1
time to read an amount c of data at a rate b
from a storage device
66.7 h
IV.
NUMERICAL RESULTS
Here, we assess the reliability of the clustered and declus-
tered schemes for a system comprised of n = 64 devices
(disks), where each device stores an amount c = 12 TB, and
m = 16, l = 13, 14, and 15, and symbol size s is equal to a
sector size of 512 bytes.
Typical parameter values are listed in Table II. The annu-
alized failure rate (AFR) is in the range of 0.9% to 3%, which
corresponds to a mean time to failure in the range of 300, 000
h to 1, 000, 000 h. The parameter λ−1 is chosen to be equal to
300, 000 h. It is assumed that the reserved rebuild bandwidth
b is equal to 50 MB/s, which yields a rebuild time of a device
µ−1 = c/b = 66.7 h, and that the network rebuild bandwidth
is sufﬁciently large (Bmax ≥ n b = 3.2 GB/s). We assume
that the rebuild time distribution is deterministic, such that
E(Xk) = [E(X)]k. The obtained results are accurate because
(6) is satisﬁed, given that λ/µ = 2.2 × 10−4 ≪ 1.
First, we assess the reliability for the declustered placement
scheme (k = n = 64). The probability of data loss PDL is
determined by (13) as a function of Ps and shown in Figure
4. The probabilities PUFu and PDF are also shown, as obtained
from (14) and (17), respectively. We observe that PDL increases
monotonically with Ps and exhibits a number of ˜r plateaus. In
the interval [4.096 × 10−12, 5 × 10−9] of practical importance
for Ps, which is indicated between the two vertical dashed
lines, the probability of data loss PDL and, by virtue of (28), the
MTTDL are degraded by orders of magnitude. The normalized
λ MTTDL measure is obtained from (28) and shown in Figure
5. Increasing the number of parities (reducing l) improves
reliability by orders of magnitude.
The normalized expected amount E(Q)/c of lost user
data relative to the amount of data stored in a device is
obtained from (20) and shown in Figure 6. The normalized
expected amounts E(QUFu)/c and E(QDF)/c are also shown
as determined by (26) and (27), respectively. The normalized
EAFDL/λ measure is obtained from (29) and shown in Figure
7. We observe that E(Q) and EAFDL increase monotonically,
but they are practically unaffected in the interval of interest
because they degrade only when Ps is much larger than the
typical sector error probabilities. For the EAFDL metric too,
increasing the number of parities (reducing l) results in a
reliability improvement by orders of magnitude.
The reliability metrics corresponding to the clustered place-
ment scheme (k = m = 16) are plotted in Figures 8, 9, 10, and
11. We observe that the reliability achieved by the clustered
data placement scheme does not reach the level achieved by
the declustered one.
The performance of certain erasure coding schemes was
assessed in [32] by obtaining the probability of data loss
PDL using a detailed distributed storage simulator. The PDL
values corresponding to Ps = 4.096 × 10−12 (Pbit = 10−15)
for two of the conﬁgurations considered are indicated by the
squares in Figure 12. This ﬁgure also shows the probabilities
of data loss PDL that correspond to these two conﬁgurations
and obtained from (13) as a function of Ps. We observe that the
theoretical results are in agreement with the simulation results,
which conﬁrms the validity of the model and the analytical
expressions derived.
V.
CONCLUSIONS
The effect of latent sector errors on the reliability
of erasure-coded data storage systems was investigated. A
methodology was developed for deriving the Mean Time to
Data Loss (MTTDL) and the Expected Annual Fraction of
Data Loss (EAFDL) reliability metrics analytically. Closed-
form expressions capturing the effect of unrecoverable la-
tent errors were obtained for the symmetric, clustered and
declustered data placement schemes. We demonstrated that
the declustered placement scheme offers superior reliability
in terms of both metrics. We established that, for realistic
unrecoverable sector error rates, MTTDL is adversely affected
by the presence of latent errors, whereas EAFDL is not.
The analytical reliability expressions derived here can identify
storage-efﬁcient data placement conﬁgurations that yield high
reliability.
Applying these results to assess the effect of network
rebuild bandwidth constraints is a subject of further inves-
tigation. The reliability evaluation of erasure-coded systems
when device failures, as well as unrecoverable latent errors
are correlated is also part of future work.
APPENDIX A
We consider the direct path −−→
UFu = 1 → 2 → · · · →
u → UF and proceed to evaluate PUFu(R1, ⃗αu−1), the prob-
ability of entering exposure level u through vector ⃗αu−1 ≜
(α1, . . . , αu−1) and given a rebuild time R1, and then en-
countering an unrecoverable failure during the rebuild process
at this exposure level. It follows from (11) that
PUFu(R1, ⃗αu−1) = Pu(R1, ⃗αu−2) · Pu→UF(R1, ⃗αu−1) . (38)
It follows from Eq.(111) of [12] by setting ˜r = u that
Pu(R1, ⃗αu−2) ≈ (λb1R1)u−1
u−1
Y
i=1
˜ni
bi
(Vi αi)u−1−i .
(39)
Unconditioning (39) on
⃗αu−2
and using the fact that
E
Qu−1
i=1 αu−1−i
i

= Qu−1
i=1 (u − i)−1 = [(u − 1)!]−1 yields
Pu(R1) ≈ (λb1R1)u−1
1
(u − 1)!
u−1
Y
i=1
˜ni
bi
V u−1−i
i
.
(40)
Unconditioning (40) on R1 and using (5) and (7) yields (12).
We now proceed to calculate Pu→UF(R1, ⃗αu−1). Upon
entering exposure level u, the rebuild process attempts to
restore the Cu most-exposed codewords, each of which has
m − u remaining symbols. Let us consider such a codeword,
and let Lu be the number of symbols permanently lost and Iu
be the number of symbols in the codeword with unrecoverable
errors. Owing to the independence of symbol errors, Iu follows
6
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-841-9
CTRQ 2021 : The Fourteenth International Conference on Communication Theory, Reliability, and Quality of Service

10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
 PDL
(a) l = 13
(˜r = 4)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
 PDL
(b) l = 14
(˜r = 3)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
 PDL
(c) l = 15
(˜r = 2)
Figure 4.
Probability of data loss PDL vs. Ps for l = 13, 14, 15; m = 16, n = k = 64 (declustered scheme), λ/µ = 0.0002, c = 12 TB, and s = 512 B.
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
100
105
1010
 MTTDL
(a) l = 13
(˜r = 4)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
100
105
1010
 MTTDL
(b) l = 14
(˜r = 3)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
100
105
1010
 MTTDL
(c) l = 15
(˜r = 2)
Figure 5.
Normalized MTTDL vs. Ps for l = 13, 14, and 15; m = 16, n = k = 64 (declustered scheme), λ/µ = 0.0002, c = 12 TB, and s = 512 B.
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-10
100
E(Q) / c
(a) l = 13
(˜r = 4)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-10
100
E(Q) / c
(b) l = 14
(˜r = 3)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-10
100
E(Q) / c
(c) l = 15
(˜r = 2)
Figure 6.
Normalized amount of data loss E(Q) for l = 13, 14, 15; m = 16, n = k = 64 (declustered scheme), λ/µ = 0.0002, c = 12 TB, and s = 512 B.
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
102
EAFDL / 
(a) l = 13
(˜r = 4)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
102
EAFDL / 
(b) l = 14
(˜r = 3)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
102
EAFDL / 
(c) l = 15
(˜r = 2)
Figure 7.
Normalized EAFDL vs. Ps for l = 13, 14, and 15; m = 16, n = k = 64 (declustered scheme), λ/µ = 0.0002, c = 12 TB, and s = 512 B.
7
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-841-9
CTRQ 2021 : The Fourteenth International Conference on Communication Theory, Reliability, and Quality of Service

10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
 PDL
(a) l = 13
(˜r = 4)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
 PDL
(b) l = 14
(˜r = 3), RAID-6
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
 PDL
(c) l = 15
(˜r = 2), RAID-5
Figure 8.
Probability of data loss PDL vs. Ps for l = 13, 14, 15; n = 64, k = m = 16 (clustered scheme), λ/µ = 0.0002, c = 12 TB, and s = 512 B.
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
100
105
1010
 MTTDL
(a) l = 13
(˜r = 4)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
100
105
1010
 MTTDL
(b) l = 14
(˜r = 3), RAID-6
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
100
105
1010
 MTTDL
(c) l = 15
(˜r = 2), RAID-5
Figure 9.
Normalized MTTDL vs. Ps for l = 13, 14, and 15; n = 64, k = m = 16 (clustered scheme), λ/µ = 0.0002, c = 12 TB, and s = 512 B.
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-10
100
E(Q) / c
(a) l = 13
(˜r = 4)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-10
100
E(Q) / c
(b) l = 14
(˜r = 3), RAID-6
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-10
100
E(Q) / c
(c) l = 15
(˜r = 2), RAID-5
Figure 10.
Normalized amount of data loss E(Q) for l = 13, 14, 15; n = 64, k = m = 16 (clustered scheme), λ/µ = 0.0002, c = 12 TB, and s = 512 B.
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
102
EAFDL / 
(a) l = 13
(˜r = 4)
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
102
EAFDL / 
(b) l = 14
(˜r = 3), RAID-6
10-18 10-16 10-14 10-12 10-10 10-8 10-6 10-4 10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-14
10-12
10-10
10-8
10-6
10-4
10-2
100
102
EAFDL / 
(c) l = 15
(˜r = 2), RAID-5
Figure 11.
Normalized EAFDL vs. Ps for l = 13, 14, and 15; n = 64, k = m = 16 (clustered scheme), λ/µ = 0.0002, c = 12 TB, and s = 512 B.
8
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-841-9
CTRQ 2021 : The Fourteenth International Conference on Communication Theory, Reliability, and Quality of Service

10-16 10-14 10-12 10-10 10-8
10-6
10-4
10-2
100
Unrecoverable Symbol Error Probability ( Ps )
10-15
10-10
10-5
100
 PDL
Replication
RS(14,10)
Figure 12.
PDL vs. Ps for n = k = 210, λ−1 = 3 years, µ−1 = 34 hours,
λ/µ = 0.0013, c = 15 TB, and s = 512 B. Reliability schemes: 3-way
replication and MDS(14,10).
a binomial distribution with parameter Ps, the probability that
a symbol has a unrecoverable error. Thus, for i = 0, . . . , m−u,
P(Iu = j) =
m − u
j

P j
s (1 − Ps)m−u−j ,
(41)
≈
m − u
j

P j
s ,
for Ps ≪
1
m − u − j , (42)
such that
E(Iu) =
m−u
X
j=1
j P(Iu = j) = (m − u) Ps .
(43)
Clearly, the symbols lost due to the device failures can be
corrected by the erasure coding capability only if at least l of
the remaining m − u symbols can be read. Thus, Lu = 0 if
and only if Iu ≤ m−u−l or, by virtue of (3), Iu ≤ ˜r−1−u.
Thus, the probability qu that a codeword can be restored is
qu = P(Lu = 0) = 1 − P(Iu > ˜r − u) ,
(44)
which, using (41), yields (16).
Note that, if a codeword is corrupted, then at least one of
its l user-data symbols is lost. Owing to the independence of
symbol errors, codewords are independently corrupted. Con-
sequently, the conditional probability PUF|Cu of encountering
an unrecoverable failure during the rebuild process of the Cu
codewords is
PUF|Cu = 1 − qCu
u
,
for u = 1, . . . , ˜r .
(45)
Substituting (9) into (45) and using (15) yields
Pu→UF(R1, ⃗αu−1) ≈ 1 − q
C Qu−1
j=1 Vj αj
u
= 1 − ˆq
Qu−1
u j=1 αj
.
(46)
Substituting (46) into (38) yields
PUFu(R1, ⃗αu−1) ≈ Pu(R1, ⃗αu−2)

1 − ˆq
Qu−1
j=1 αj
u

.
(47)
Unconditioning (47) on ⃗αu−1 and using (39) yields
PUFu(R1) ≈ Pu(R1) − (λb1R1)u−1
 u−1
Y
i=1
˜ni
bi V u−1−i
i
!
· E⃗αu−1
" u−1
Y
i=1
αu−1−i
i
!
ˆq
Qu−1
j=1 αj
u
#
. (48)
For αi ∼ U(0, 1) and for all q ∈ R, it can be shown that
E
" u−1
Y
i=1
αu−1−i
i
!
q
Qu−1
i=1 αi
#
=
1
(u − 1)! + log(q)−(u−1)
 
q −
u−1
X
i=0
log(q)i
i!
!
.
(49)
From (40) and (49), (48) yields
PUFu(R1) ≈ − (λb1R1)u−1
 u−1
Y
i=1
˜ni
bi V u−1−i
i
!
· log(ˆqu)−(u−1)
 
ˆqu −
u−1
X
i=0
log(ˆqu)i
i!
!
. (50)
Unconditioning (50) on R1, and using (5) and (7), yields (14).
APPENDIX B
At exposure level u, when Iu ≥ m − u − l + 1 = ˜r − u,
the number Lu of lost symbols is Iu + u. Consequently, the
expected number E(Lu) of lost symbols is
E(Lu) =
m−u
X
i=˜r−u
(i + u) P(Iu = i) ,
(51)
where P(Iu = i) is given by (41). Considering approximation
(42), it follows that
E(Lu) ≈ ˜r
 
m − u
˜r − u
!
P ˜r−u
s
,
for Ps ≪
1
m − ˜r .
(52)
The expected number E(SU|Cu) of symbols lost due to
unrecoverable failures during the rebuild of the Cu codewords
at exposure level u is equal to Cu E(Lu), which yields
E(SU|Cu)
(52)
≈ Cu ˜r
m − u
˜r − u

P ˜r−u
s
, Ps ≪
1
m − ˜r .
(53)
Substituting (9) into (53) yields
E(SU|⃗αu−1) ≈ C


u−1
Y
j=1
Vj αj

 ˜r
m − u
˜r − u

P ˜r−u
s
.
(54)
Subsequently, the expected number E(SUFu|R1, ⃗αu−1) of
symbols lost due to unrecoverable failures encountered during
rebuild in conjunction with entering exposure level u through
vector ⃗αu−1, and given a rebuild time R1, is
E(SUFu|R1, ⃗αu−1) = Pu(R1, ⃗αu−1) E(SU|⃗αu−1) .
(55)
Substituting (39) and (54) into (55) yields
E(SUFu|R1, ⃗αu−1) ≈ (λb1R1)u−1
"u−1
Y
i=1
˜ni
bi (Vi αi)u−i
#
· C ˜r
 
m − u
˜r − u
!
P ˜r−u
s
,
Ps ≪
1
m − ˜r .
(56)
Unconditioning (56) on ⃗αu−1 and R1, and using (5), (7), and
given that E(Qu−1
i=1 αu−i
i
)=Qu−1
i=1 (u−i+1)−1 =1/u!, yields
E(SUFu|R1) ≈ (λb1R1)u−1
 u−1
Y
i=1
˜ni
bi V u−i
i
!
1
u! C ˜r
·
 
m − u
˜r − u
!
P ˜r−u
s
,
Ps ≪
1
m − ˜r .
(57)
9
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-841-9
CTRQ 2021 : The Fourteenth International Conference on Communication Theory, Reliability, and Quality of Service

Unconditioning (57) on R1, and using (5) and (7), yields
E(SUFu) ≈ (λ c)u−1 E(Xu−1)
[E(X)]u−1
 u−1
Y
i=1
˜ni
bi V u−i
i
!
1
u! C ˜r
·
 
m − u
˜r − u
!
P ˜r−u
s
,
Ps ≪
1
m − ˜r .
(58)
Substituting (58) into (25) yields (26).
REFERENCES
[1]
D. A. Patterson, G. Gibson, and R. H. Katz, “A case for redundant
arrays of inexpensive disks (RAID),” in Proc. ACM Int’l Conference
on Management of Data (SIGMOD), Jun. 1988, pp. 109–116.
[2]
P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson,
“RAID: High-Performance, reliable secondary storage,” ACM Comput.
Surv., vol. 26, no. 2, Jun. 1994, pp. 145–185.
[3]
V. Venkatesan, I. Iliadis, C. Fragouli, and R. Urbanke, “Reliability of
clustered vs. declustered replica placement in data storage systems,”
in Proc. 19th Annual IEEE/ACM Int’l Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Jul. 2011, pp. 307–317.
[4]
I. Iliadis, D. Sotnikov, P. Ta-Shma, and V. Venkatesan, “Reliability of
geo-replicated cloud storage systems,” in Proc. 2014 IEEE 20th Paciﬁc
Rim Int’l Symposium on Dependable Computing (PRDC), Nov. 2014,
pp. 169–179.
[5]
M. Malhotra and K. S. Trivedi, “Reliability analysis of redundant arrays
of inexpensive disks,” J. Parallel Distrib. Comput., vol. 17, 1993, pp.
146–151.
[6]
A. Thomasian and M. Blaum, “Higher reliability redundant disk arrays:
Organization, operation, and coding,” ACM Trans. Storage, vol. 5, no. 3,
2009, pp. 1–59.
[7]
I. Iliadis, R. Haas, X.-Y. Hu, and E. Eleftheriou, “Disk scrubbing versus
intradisk redundancy for RAID storage systems,” ACM Trans. Storage,
vol. 7, no. 2, 2011, pp. 1–42.
[8]
V. Venkatesan, I. Iliadis, and R. Haas, “Reliability of data storage
systems under network rebuild bandwidth constraints,” in Proc. 20th
Annual IEEE Int’l Symposium on Modeling, Analysis, and Simula-
tion of Computer and Telecommunication Systems (MASCOTS), Aug.
2012, pp. 189–197.
[9]
J.-F. Pˆaris, T. J. E. Schwarz, A. Amer, and D. D. E. Long, “Highly re-
liable two-dimensional RAID arrays for archival storage,” in Proc. 31st
IEEE Int’l Performance Computing and Communications Conference
(IPCCC), Dec. 2012, pp. 324–331.
[10]
I. Iliadis and V. Venkatesan, “Most probable paths to data loss: An
efﬁcient method for reliability evaluation of data storage systems,” Int’l
J. Adv. Syst. Measur., vol. 8, no. 3&4, Dec. 2015, pp. 178–200.
[11]
——, “Expected annual fraction of data loss as a metric for data storage
reliability,” in Proc. 22nd Annual IEEE Int’l Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Sep. 2014, pp. 375–384.
[12]
——, “Reliability evaluation of erasure coded systems,” Int’l J. Adv.
Telecommun., vol. 10, no. 3&4, Dec. 2017, pp. 118–144.
[13]
I. Iliadis, “Reliability evaluation of erasure coded systems under rebuild
bandwidth constraints,” Int’l J. Adv. Networks and Services, vol. 11,
no. 3&4, Dec. 2018, pp. 113–142.
[14]
——, “Data loss in RAID-5 and RAID-6 storage systems with latent
errors,” Int’l J. Adv. Software, vol. 12, no. 3&4, Dec. 2019, pp. 259–
287.
[15]
Amazon Web Services, ”Amazon Simple Storage Service (Amazon
S3),” 2021. [Online]. Available: http://aws.amazon.com/s3/ [retrieved:
March, 2021]
[16]
D. Borthakur et al., “Apache Hadoop goes realtime at Facebook,” in
Proc. ACM Int’l Conference on Management of Data (SIGMOD), Jun.
2011, pp. 1071–1080.
[17]
R. J. Chansler, “Data availability and durability with the Hadoop
Distributed File System,” ;login: The USENIX Association Newsletter,
vol. 37, no. 1, Feb. 2013, pp. 16–22.
[18]
K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop
Distributed File System,” in Proc. 26th IEEE Symposium on Mass
Storage Systems and Technologies (MSST), May 2010, pp. 1–10.
[19]
S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google ﬁle system,” in
Proc. 19th ACM Symposium on Operating Systems Principles (SOSP),
Oct. 2003, pp. 29–43.
[20]
D.
Borthakur.
HDFS
and
Erasure
Codes
(HDFS-RAID),
Aug.
2009. [Online]. Available: https://hadoopblog.blogspot.com/2009/08
[retrieved: March, 2021]
[21]
B. Calder et al., “Windows Azure Storage: a highly available cloud stor-
age service with strong consistency,” in Proc. 23rd ACM Symposium
on Operating Systems Principles (SOSP), Oct. 2011, pp. 143–157.
[22]
D. Ford et al., “Availability in globally distributed storage systems,”
in Proc. 9th USENIX Symposium on Operating Systems Design and
Implementation (OSDI), Oct. 2010, pp. 61–74.
[23]
S. Muralidhar et al., “f4: Facebook’s Warm BLOB Storage System,”
in Proc. 11th USENIX Symposium on Operating Systems Design and
Implementation (OSDI), Oct. 2014, pp. 383–397.
[24]
C. Huang et al., “Erasure coding in Windows Azure Storage,” in Proc.
USENIX Annual Technical Conference (ATC), Jun. 2012, pp. 15–26.
[25]
E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large
disk drive population,” in Proc. 5th USENIX Conference on File and
Storage Technologies (FAST), Feb. 2007, pp. 17–28.
[26]
A. Dholakia, E. Eleftheriou, X.-Y. Hu, I. Iliadis, J. Menon, and K. Rao,
“A new intra-disk redundancy scheme for high-reliability RAID storage
systems in the presence of unrecoverable errors,” ACM Trans. Storage,
vol. 4, no. 1, 2008, pp. 1–42.
[27]
I. Iliadis, “Reliability modeling of RAID storage systems with latent
errors,” in Proc. 17th Annual IEEE/ACM Int’l Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Sep. 2009, pp. 111–122.
[28]
V. Venkatesan and I. Iliadis, “Effect of latent errors on the reliability
of data storage systems,” in Proc. 21th Annual IEEE Int’l Symposium
on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), Aug. 2013, pp. 293–297.
[29]
I. Iliadis and V. Venkatesan, “Rebuttal to ‘Beyond MTTDL: A closed-
form RAID-6 reliability equation’,” ACM Trans. Storage, vol. 11, no. 2,
Mar. 2015, pp. 1–10.
[30]
I. Iliadis, “Data loss in RAID-5 storage systems with latent errors,” in
Proc. 12th Int’l Conference on Communication Theory, Reliability, and
Quality of Service (CTRQ), Mar. 2019, pp. 1–9.
[31]
V. Venkatesan and I. Iliadis, “A general reliability model for data storage
systems,” in Proc. 9th Int’l Conference on Quantitative Evaluation of
Systems (QEST), Sep. 2012, pp. 209–219.
[32]
M. Silberstein, L. Ganesh, Y. Wang, L. Alvisi, and M. Dahlin,
“Lazy means smart: Reducing repair bandwidth costs in erasure-coded
distributed storage,” in Proc. 7th ACM Int’l Systems and Storage
Conference (SYSTOR), Jun. 2014, pp. 15:1–15:7.
[33]
K. V. Rashmi et al., “A ”Hitchhiker’s” guide to fast and efﬁcient
data reconstruction in erasure-coded data centers,” in Proc. 2014 ACM
conference on SIGCOMM, Aug. 2014, pp. 331–342.
[34]
DELL/EMC Whitepaper, ”PowerVault ME4 Series ADAPT Software,”
Feb. 2019. [Online]. Available: https://www.dellemc.com/ [retrieved:
March, 2021]
[35]
M. Zhang, S. Han, and P. P. C. Lee, “SimEDC: A simulator for the
reliability analysis of erasure-coded data centers,” IEEE Trans. Parallel
Distrib. Syst., vol. 30, no. 12, 2019, pp. 2836–2848.
[36]
V. Venkatesan and I. Iliadis, “Effect of codeword placement on the
reliability of erasure coded data storage systems,” in Proc. 10th Int’l
Conference on Quantitative Evaluation of Systems (QEST), Sep. 2013,
pp. 241–257.
[37]
——, “Effect of codeword placement on the reliability of erasure coded
data storage systems,” IBM Research Report, RZ 3827, Aug. 2012.
[38]
I. Iliadis and X.-Y. Hu, “Reliability assurance of RAID storage systems
for a wide range of latent sector errors,” in Proc. 2008 IEEE Int’l
Conference on Networking, Architecture, and Storage (NAS), Jun.
2008, pp. 10–19.
10
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-841-9
CTRQ 2021 : The Fourteenth International Conference on Communication Theory, Reliability, and Quality of Service

