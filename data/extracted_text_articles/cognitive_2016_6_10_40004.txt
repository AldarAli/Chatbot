Applying Pairing Support Vector Regression Algorithm to GPS GDOP 
Approximation 
Pei-Yi Haoa                                 Chao-Yi Wub 
Department of Information Management 
National Kaohsiung University of Applied Sciences,  Kaohsiung, Taiwan 
Email:  haupy@cc.kuas.edu.twa    wububbles@gmail.comb 
Abstract—Global Positioning System (GPS) has extensively 
been employed in various applications, including the use of 
GPS to analyze the cognitive diseases and find better treatment. 
Geometric Dilution of Precision (GDOP) is an indicator 
showing how well the constellation of GPS satellites is 
geometrically organized. GPS positioning with a smaller 
GDOP value usually yields better accuracy. However, the 
calculation of GDOP is a time- and power-consuming task that 
requires to solving measurement equations with complicated 
matrix transformation and inversion. When selecting the one 
with the lowest GDOP for positioning from many GPS 
constellations, methods that can fast and accurately calculate 
GPS GDOP are imperative. Previous works have shown that 
numerical regression on GPS GDOP can yield satisfactory 
results and eliminate many calculation steps. This paper 
employs a new pairing support vector regression algorithm 
(pair-SVR) to the approximation of GPS GDOP. The pair-SVR 
determines indirectly the regression function through a pair of 
nonparallel insensitive upper- and lower-bound functions, each 
of which is solved by support vector machine (SVM)- type 
quadratic programming problems (QPP) with smaller-sized. 
This strategy makes the pair-SVR not only have the faster 
learning speed than the classical SVR, but also be suitable for 
many cases, especially when the noise is heteroscedastic. 
Besides, pair-SVR improves the sparsity than that of twin 
support vector regression (TSVR) by employing the concept of 
insensitive zone. This makes the prediction time complexity of 
pair-SVR is obviously smaller than TSVR. The experimental 
results show that pair-v-SVR gains better performance for the 
approximation of GPS GDOP than previous support vector 
regression machine. 
Keywords- GDOP; GPS; kernel-based method; support 
vector machine; support vector regression. 
I. 
 INTRODUCTION 
Cognitive impairment manifests in changed out-of-home 
mobility. Until recently, the assessment of outdoor mobility 
relied on the reports of family care-givers and institutional 
staff and used observational approaches, activity monitoring 
or behavioural checklists. Shoval et al. [1] apply GPS to 
analyze the mobility of the people who have Alzheimer's 
disease and related cognitive diseases.  Shoval et al. [2] 
apply GPS to measure the out-of-home mobility of older 
adults with differing cognitive functioning. The GPS is a 
satellite based navigation system that helps users to 
determine their locations on Earth. A GPS receiver compares 
the time difference between the signal transmitted by a 
satellite and the time it was received and calculates the 
distance between the satellite and GPS receiver. GPS 
receivers analyze such signals from at least 3 satellites and 
use triangulation to determine the user’s location. GPS, 
which consists of at least 24 active satellites provides 24-
hour, all-weather, worldwide coverage with position, 
velocity and timing information. Nowadays, GPS has 
become a popular tool for positioning and navigation. 
However, the accuracy of GPS positing may unavoidably 
degrade by two causes [3]: the errors in each observable 
signal, and the geometry formed by the observables 
employed for positioning or navigation. Reasons resulting in 
the former factor include ionospheric delay, tropospheric 
delay, satellite clock and receiver clock offsets, receiver 
noise and multi-path problems. The later one is usually 
referred to as the GDOP, which describes the effect of 
geometry on the relationship between measurement error and 
position determination error. 
GDOP is an indicator showing how well the constellation 
of GPS satellites is organized geometrically. Because some 
receiver device may be restricted to processing a limited 
number of visible satellites, hence, it needs to select the 
satellite subset that offers the best or most acceptable 
solution. Since GDOP provides a simple interpretation of 
how much positioning precision can be diluted by a unit of 
measurement error, positioning or navigation can obtain a 
better quality by choosing the combination of satellites in a 
satellite constellation with GDOP as small as possible. 
Existing methods for calculating GDOP include matrix 
inversion, closed-form algorithms, maximum volume of 
tetrahedrons, etc. The most accurate method for determining 
GDOP is to use matrix inversion to all combinations and 
select the minimum one. However, this approach is a time- 
and power- consuming task because it usually requires 
considerable computational power and a large amount of 
operations 
for 
exhaustively 
examining 
all 
possible 
combinations of satellites. It would be a computational 
burden for real time application and mobile device. Closed 
form methods simplify the computational procedure under 
specific circumstances, but there still has roundoff errors due 
to the floating-point operations. Instead of directly 
calculating 
the GDOP equations and avoiding the 
complicated solving of matrix inversion, Simon and El-
Sherief [4][5] rephrase the calculation of GDOP as 
regression/ approximation problems and apply neural 
networks (NN) to solve such problems. However, solving 
regression problems using NN usually suffer from the slow 
training speed and difficulty in determining the network 
architecture. Besides, the overfitting problem degrades the 
generalization ability of NN applications when the numbers 
of features and training samples are large. Wu first employs 
102
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

the support vector regression machine for the approximation 
of GPS GDOP [6]. 
The SVMs have been very successful in many fields. 
Peng proposed a TSVR for data regression [9]. In TSVR, a 
pair of smaller sized QPPs is solved rather than the large 
single QPP in the SVR. This strategy makes the training 
speed of TSVR is faster than classical SVR machine. 
However, the major disadvantage of TSVR is its prediction 
speed is significantly slow due to the loss of sparsity. In 
TSVR, the number of basis function used for estimating the 
final regression function is equal to the number of training 
samples. Therefore, predicting using TSVR is a time-
consuming task for large-scale data set. In many real 
applications, the prediction speed is more important than 
training speed. Hence, it is necessary to improve the sparsity 
of TSVR, since a sparse regression model means a low 
economy for storage requirement and a high efficiency for 
the real time prediction. 
In the spirit of TSVR, this paper proposes a novel pair-
SVR, which seeks a pair of nonparallel bound function of 
regression model by solving two related SVM-type problems, 
each of which is smaller than conventional SVM. The major 
benefit of the proposed pair-SVR is the efficiency for both 
learning and prediction. We improve the sparsity of TSVR 
by adopting an insensitive zone that is determined by a pair 
of nonparallel upper bound and lower bound function. Only 
samples outside the insensitive zone are captured as SVs, and 
only those SVs construct the final regression model. In 
general, the number of SV is very few. This makes the 
prediction time cost of pair-SVR is obviously smaller than 
TSVR. Besides, the strategy of solving two QPPs with 
smaller-sized instead of a single large QPP makes the 
training time complexity of pair-SVR approximately 4 times 
smaller than that of a classical SVR. 
The rest of this paper is organized as follows. Section II 
gives a brief overview of GDOP and TSVR. Section III 
describes a modification of TSVR, called pair-SVR, and 
applies pair-SVR for GDOP approximation. Experiments are 
presented in Section IV, and some concluding remarks are 
given in Section V. 
II. 
BACKGROUND 
A. Geometric Dilution of Precision  
In GPS applications, the GDOP is often used to select a 
subset of satellites from all visible ones. In order to 
determine the position of a receiver, pseudoranges from n 
(4) satellites must be used at the same time. By linearizing 
the pseudorange equation with Taylor’s series expansion at 
the approximate (or nominal) receiver position, the 
relationship between pseudorange difference (
 i
) and 
positioning difference (
 ix
) can be summarized as follows 
[2]: 



























































n
b
u
u
u
n
n
n
n
v
v
v
t
c
z
y
x
e
e
e
e
e
e
e
e
e









2
1
3
2
1
23
22
21
13
12
11
2
1
1
1
1
1
      (1) 
Equation (1) can have a general form represented as 
z = Hx + v                                (2) 
where the geometry matrix H is n × 4, n ≥ 4, since it is 
necessary to use at least 4 satellites to determine a position 
in a 3-dimension space. Such an overdetermined system like 
(2) has no exact solution. However, the n columns of H are 
linearly independent since they are signals received from 
individual satellites independently. The linear least squares 
solution can be obtained by solving the normal equations 
H v
H Hx
H z
t
t
t


                       (3) 
H has full rank and 
H H
M
t

is invertible, then we can 
have 
H z
H H
x
t
t
) 1
(

 
                          (4) 
GDOP becomes a linear least-squares solution of a 
linearized pseudorange equation by taking the difference 
between the estimated and the true positions. Therefore 
H v
H H
x
t
t
) 1
(
~


                          (5) 
The quality of this solution is evaluated by cov(·), which 
denotes the covariance of a measurement. 


t t
t
t
t
H
H H
v
H
H H
x
1
1
)
cov( ) (
)
(
)
cov(


 
         (6) 
If all components of v are pairwise uncorrelated and have 
variance σ2, cov(v) can be normalized to an identity matrix 
cov(v)=σ2I, and a simplified expression of (6) can be 
obtained as 
1
2
)
(
)
cov(


H H
x
t


                    (7) 
This quantifies the magnification of pseudorange errors onto 
the user position errors. Let 
H H
M
t

  be the measurement 
matrix. The GDOP factor is defined as 


)
det(
)
(
)
(
1
H
H
H H
H H
t
t
t
trace adj
trace
GDOP



      (8) 
B.  Twin Support Vector Regression  
The TSVR finds a pair of nonparallel functions around 
the data points [9].  In general, it considers the following 
pair of functions for the nonlinear case: 
1
1
1
)
(
( )
b
f
T


w K A,x
x
 and 
2
2
2
)
(
( )
b
f
T


w K A,x
x
 
each one determines the   -insensitive down- or up-bound 
function, respectively. The functions f1(x) and f2(x) are 
obtained by solving the following pair of QPPs: 
0
,
)
(
to
subject
)
(
2
1
minimize
1
1
1
1
2
1
1
1
, 1
1









ξ
ξ
e
e
Kw
Y
e ξ
e
Kw
e
Y
w


b
N
C
b
T
b
            (9) 
0
,
)
(
to
subject
)
(
2
1
minimize
2
2
2
2
2
2
2
2
, 1
1









ξ
ξ
e
e
Kw
Y
e ξ
e
Kw
e
Y
w


b
N
C
b
T
b
         (10) 
103
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

where 1, 2 0 are the insensitive parameters. C1, C2 0 are 
the regularization parameters. e are vector of ones of N 
dimensions. Y is the target vector Y=(y1,..,yN)T.  is the 
slack vector  =(1,.., N)T. K(A,x) is the column vector 
T
N
k
k
, ))
, ),..., (
(
(
1
x
A
A x
 where Ai are the ith training sample 
(row vector). K is the N by N kernel matrix such that 
Kij=k(Ai,Aj). By considering the Karush-Kuhn-Tucker 
(KKT) conditions for the Lagrangian functions of (9) and 
10), we obtain the dual QPPs, which are  
e
α
f α
H α
f H H H
H α
H H H
α
N
C
T
T
T
T
T
T
T
1
1
1
0
s.t.
)
(
)
(
2
1
max







       (11) 
and 
e
β
h β
H β
h H H H
H β
H H H
β
N
C
T
T
T
T
T
T
T
2
1
1
0
s.t.
)
(
)
(
2
1
max







       (12) 
where 
]
[
H  K e
, 
e
Y
f
 1

, and 
e
Y
h
2

. After optimizing 
(11) and (12), we obtain the augmented vectors for f1(x) and 
f2(x), which are 
)
(
)
(
1
1
1
α
f
H
H H
w







T
T
b
   
)
(
)
(
1
2
2
β
h
H
H H
w







T
T
b
   (13) 
Then, the estimated regressor is constructed by as follows: 
)
2 (
1
)
(
)
2 (
1
( ))
( )
2 (
1
)
(
2
1
2
1
2
1
b
b
f
f
f
T
T






K A,x
w
w
x
x
x
     (14) 
III. 
GDOP APPROXIMATION USING PAIRING SUPPORT 
VECTOR REGRESSION ALGORITHM 
A. Pairing Support Vector Regression Algorithm  
Motivated by TSVM, the goal of the proposed pair-
SVR algorithm is to estimate a pair of nonparallel function 
f1(x) and f2(x) by solving two SVM type QPPs with smaller 
size, each of which determines the upper bound and lower 
bound of the insensitive zone, such that the insensitive zone 
includes all training samples with smallest size. According 
to the concept of kernel-based learning, a non-linear 
function is obtained via a linear learning machine in a 
kernel-introduced feature space while the capacity of the 
learning machine is controlled by a parameter that is 
independent to the dimensionality of the space. The basic 
concept is that a nonlinear regression function is estimated 
via simply mapping the training data vector 
ix  by : 
n
R → 
F into a high-dimensional feature space F. Therefore, the 
proposed pair-SVR aims at estimating the following two 
functions: 
1
1
1
( )
( )
b
f



x
w
x
, where 
w  F
, 
x  Rn
, 
b  R
, 
2
2
2
( )
( )
b
f



x
w
x
, where 
w  F
, 
x  Rn
, 
b R
 
For the estimation of 
1
1
1
( )
( )
b
f



x
w
x
, the upper 
bound function of the insensitive zone, we force the upper 
bound function f1(x) to move downward via minimizing 
2
w1
 and b1 in the objective function, and requires all 
training data 
)
,
(
xi yi
 to be below the upper bound function 
in the constraint, simultaneously. Hence, the problem of 
estimating the w1 and b1 is equivalent to solve the following 
QPP: 




N
i
i
w b
C
b
i
1
1
1
1
2
1
, ,
2
1
minimize
1
1 1


w
                        (15) 
subject to  
 
i
i
i
y
b
1
1
1
)
(




x
w
    
and  
1  0
 i
  for i=1,…,N.  
We can find the solution of this QPP in dual variables by 
finding the saddle point of the Lagrangian: 

 














N
i
i
i
N
i
i
i
i
i
N
i
i
y
b
C
b
L
1
1
1
1
1
1
1
1
1
1
1
1
2
1
)
(
2
1
 



x
w
w
   (16) 
where 
i1
  and 
i1
  are the nonnegative Lagrange multipliers. 
Differentiating L with respect to w1, b1 and 
i1
  and setting 
the result to zero, we obtain:  
0
  1 
L w





N
i
i
i
1
1
1
(x )
w

,                    (17) 
0
  1 
L b

1
1
1 


N
i
 i
,                                 (18) 
0
 1 

i
L 

1
1
1
1
and
C
C
i
i
i






,        (19) 
Substituting Eqs. (17)-(19) into L, we obtain the following 
dual problem 









N
i
i
i
N
i
N
j
j
i
j
i
y
1
1
1
1
1
1
)
(
)
(
2
1
max

 
x
x
      (20) 
subject to     
,1
1
1 


N
i
 i
 and 

.
,0
1
1
C
 i 
 
Parameter b1 can be calculated from the KKT conditions:  


0
)
(
1
1
1
1





i
i
i
i
y
b


x
w
,             (21) 
 

0
1
1
1


i
i
C


                                       (22) 
For some 

1 
1
,0 C
 i 
, we have 
1  0
 i
 and moreover the 
second factor in (21) equals to zero. Hence, b1 can be 
calculated as follows: 
)
(
1
1
i
iy
b
x
w



 for some

1 
1
,0 C
 i 
. 
Finally, the upper bound function of the regression model is  
1
1
1
1
)
( ,
( )
b
k
f
N
i
i
i



x x
x

.                      (23) 
104
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

For the estimation of 
2
2
2
( )
( )
b
f



x
w
x
, the 
lower bound function of the insensitive zone, intuitively, we 
should force f2(x) to move upward via maximizing 
2
w2
 
and b2 in the objective function, and requires$ all training 
data 
)
,
(
xi yi
 to be above the lower bound function in the 
constraint, simultaneously. However, maximizing 
2
w2
 
violates the principle of sparsity regression model. Hence, 
we apply the following trick to estimate the lower bound 
function. First, we multiplies the desired target yi by -1 and 
estimates a mirroring function of f2(x). We force the 
mirroring 
function 
2
2
2
( )
( )
b
f



x
w
x
 to 
move 
downward, and require all instances 
)
,
(
i
i
x y
 to be below 
the mirroring function, simultaneously. Finally, the lower 
bound function is 
( )
)
(
2
2
x
x
f
f
 
. The problem for seeking 
2
2
2
( )
( )
b
f



x
w
x
 is equivalent to solve the following 
optimization problem: 




N
i
i
w b
C
b
i
1
2
1
2
2
2
, ,
2
1
minimize
2
2 2


w
                          (24) 
subject to  
i
i
i
y
b
2
2
2
)
(

 


x
w
    
and  
2  0
 i
  for i=1,…,N.  
 
Similar to the above Lagrange multipliers substituting 
procedure, we obtain its dual problem as  









N
i
i
i
N
i
N
j
j
i
j
i
y
1
2
1
1
2
2
)
(
)
(
2
1
max

 
x
x
    (26) 
subject to      
,1
1
2 


N
i
 i
 and 

.
,0
1
2
C
 i 
 
After 
solving (26), 
we obtain the 
weight 
vector 




N
i
i
i
1
2
2
(x )
w

. While parameter b2 can be calculated 
from the KKT conditions:  


0
)
(
2
2
2
2





i
i
i
i
y
b


x
w
,             (27) 
 

0
2
2
2


i
i
C


                                         (28) 
For some 

2 
2
,0 C
 i 
, we have 
2  0
 i
 and moreover the 
second factor in (27) equals to zero. Hence, b2 can be 
calculated as follows: 
)
(
2
2
i
iy
b
x
w


 
 for some

2 
2
,0 C
 i 
. 
Finally, the lower bound function of the regression model is  
2
1
2
2
2
)
( ,
( )
( )
b
k
f
f
N
i
i
i

 
 


x x
x
x

.          (29) 
After seeking the upper bound and lower bound function, 
the final regression function is obtained as follows 


)
2 (
1
)
) ( ,
(
2
1
( )
( )
2
1
)
(
2
1
1
2
1
2
1
b
b
k
f
f
f
N
i
i
i
i








x
x
x
x
x


      (30) 
The training samples with corresponding 1i, 2i>0 are 
called support vectors since only those data vectors 
construct the final regression function. Noted, according to 
the KKT conditions (17), (18), (22), and (23), only points 
outside the insensitive zone (or lying on the upper or lower 
bound function of the insensitive zone) are captured as 
support vectors. Usually, the number of support vector is 
very few. Hence, pair-SVR significantly enhances the 
sparsity than that of TSVR. 
B. GDOP approximation by pair-SVR 
In a complex system, if the input–output values were 
most concerned, rather than how their relationships are 
organized, numeric regression for fitting input–output data 
provides a efficient solution. Previous studies have reported 
that numerical regression on GPS GDOP can yield 
satisfactory results and reduce many calculation steps [4-6]. 
This paper apply the proposed pair-SVR algorithm as a 
means for the approximation of GPS GDOP. 
In (8), GDOP is mainly evaluated by 

HtH 1
. 
Nevertheless, it is not a good practice to invert the normal 
equations matrix. If the matrix is well-conditioned and 
positive definite, that is, it has full rank, the normal 
equations in (8) can be solved directly by using the 
Cholesky decomposition [10], 
R R
H H
t
t

, where R is an 
upper triangular matrix. It gives 













10
9
8
7
6
5
4
3
2
1
h
l
symmetrica
h
h
h
h
h
h
h
h
h
R
                         (31) 
where 

ij
t
kh
 H H
, 1 ≤ i ≤ j ≤ 4, k = 1,…, 10. A regression 
problem is formed as a functional mapping R10→R1 with 10 
inputs from h1, h2, . . . , h10 and one output for GDOP. Based 
on these settings, pair-SVR can be trained with a set D of 
input-output pairs di ∈ D, 

i 
i
i
GDOP
h
h h
d
) ,
,
,
(
10
2
1


 and 
expected to produce a functional approximation for GDOP. 
In each di, 
i
h
h h
)
,
,
(
10
2
1

 is computed from 
Ht H
 and 
GDOPi is determined by (8).  
IV. 
EXPERIMENTS 
In experiment part, we first adopt a heteroscedastic 
dataset to verify the effectiveness of the proposed pairing 
support vector regression algorithm. The Gaussian kernel  
)
exp(
)
,
(
2
j
i
j
i
k
x
x
x x




 
is used here. The RBF kernel is widely used due to its 
simplicity and the ability to deal with nonlinear problems. 
105
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

The optimal value of model-parameters was tuned using a 
grid search mechanism. For simplicity, we set C1=C2. The 
training data sets are generated by 
,
.0 05)
( 1.0
3.0
2.0
)
sin(2
2.0
2
2
k
k
k
k
k
e
x
x
x
y






      (32) 
2,1 ,....,51,
1),
.0 02(



k
k
xk
                            
where 
ke  represents a real number randomly generated in 
the interval [−1; 1]. This dataset has heteroscedastic noise 
structure, i.e., the noise is strongly corrected with the input 
value x. This example was also used in [11]. Figure 1 shows 
the regression function estimated by classical SVR (  -
SVR), TSVR, and the proposed pair-SVR. The support 
vectors are marked with circles in  -SVR and pair-SVR. In 
 -SVR and pair-SVR, the number of basis function for 
estimating the regression function is equal to number of 
support vector. However, in TSVR, the number of basis 
function is equal to the number of training samples. Hence, 
the sparsity of TSVR is worst. The  -SVR is based on the 
assumption that the noise level is uniform throughout the 
domain. The assumption of a uniform noise model, however, 
is not always satisfied. In many regression tasks, the spread 
of noise might depend on location. Due to the assumption 
that the  -insensitive zone has a tube (or slab) structure, the 
test error (risk) in  -SVR is sensitive toward the changes in 
  on this heteroscedastic data. As shown in Figure 1 (a) and 
(b), parameter   determines the trade-off between sparsity 
and accuracy. As seen from Figure 1 (c), the nonparallel 
bound functions of TSVR captures the characteristics of the 
data set well and yield satisfactory regression function. 
However, the major disadvantage of TSVR is it loses the 
sparsity. The prediction time cost of TSVR is worst among 
the three approaches. Figure 1 (d) shows that the proposed 
pair-SVR derives the satisfying solution to estimating the 
distribution of noise and captures well the characteristics of 
the data set. More importantly, our approach preserves the 
benefit of TSVR, i.e., fast speed in learning, and meanwhile 
has the benefit of sparsity of classic  -SVR, that is, small 
prediction time complexity.  
We then employ the proposed pair-SVR algorithm to the 
approximation of GPS GDOP problem. In order to fairly 
compare the performance of other support vector regression 
algorithm, the same data set from [12] is adopted. In the 
dataset, more than 2500 data pairs are obtained. For forming 
the geometry matrix 
H H
M
t

, signals from 4 different 
satellites are randomly selected, from which GDOP is 
computed according to (8). The input h1, h2, . . . , h10 are 
extracted by using the Cholesky decomposition on 
R R
H H
t
t

. Finally, inputs are collected accordingly from 
(31). To avoid the biased results, we employ the ten-fold 
cross-validation 
for 
the 
estimation 
of 
regression 
performance. All results are presented in average. The 
effectiveness of the GDOP regression model is evaluated by 
the mean square errors (MSEs):  






n
i
i
i
GDOP
f x
n
MSE
1
2
( )
1

 
The model parameters of classical SVR, TSVR and the 
proposed pair-SVR are tuned by grid-search strategy and 
ten-fold cross-validation procedure. Table I reports a 
comparison of the regression performance of classical SVR, 
TSVR, and the proposed pair-SVR in terms of MSEs, 
training time, and sparsity (the number of basis function (BF) 
used in the final regression model) on the GPS GDOP 
approximation problem. Noted, the number of basis 
function used in the regression model estimated by TSVR 
equals the number of training samples, while the number of 
basis functions in classical SVR and the proposed pair-SVR 
equals the number of support vectors (SVs). In general, the 
number of SVs is much fewer than training samples. 
Because the number of basis function is the main factor that 
effects the prediction time, the prediction speed of TSVR is 
the slowest due to the sparsity of TSVR is the worst. As 
shown in Table I, the training speed of classical SVR is the 
slowest because the learning of SVR needs to solve a large 
dense QPP. The computational complexity for solving the 
SVM-type QPP is O(N3), where N is the number of training 
samples. On other hand, the TSVR and the proposed pair-
SVR employ the strategy that solves two smaller-sized QPP 
rather than a single larger QPP. This strategy makes the 
learning speed of TSVR and the pair-SVR is approximately 
four times faster than classical SVR, as shown in Table I. 
Another disadvantage of TSVR is it considers only the 
training error instead of the generalization performance in 
the primal problem. In other words, TSVR performs the 
empirical risk minimization principle. However, it is well 
known that the superior generalization capability of SVM is 
achieved by performing of the structure risk minimization 
principle. Because both classical SVR and the proposed 
pair-SVR perform the structure risk minimization principle 
by introducing a regularization term that captures the 
characteristics of model complexity, they yield better MSEs 
than TSVR, as shown in Table I. Experimental results have 
demonstrated the effectiveness of the proposed method. 
TABLE I.  
PERFORMANCE COMPARISONS. 
Model 
SVR 
TSVR 
Pair-SVR 
MSEs 
0.808 
0.811 
0.808 
Training time 
273.1 
0.947 
2.562 
Num of BFs 
1032.1 
2250 
956.4 
 
In summary, the proposed approach not only has the 
superiority in faster training speed, but also owns better 
generalization ability and faster prediction speed. 
V. 
CONCLUSION 
One of the more common behavioral manifestations of 
dementia-related disorders is severe problems with out-of-
home mobility. Various efforts have been attempted to 
attain a better understanding of mobility behavior, but most 
106
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

studies are based on institutionalized patients and the 
assessment usually relies on reports of caregivers and 
institutional staff, using observational approaches, activity 
monitoring, or behavioral checklists. Previous studies have 
reported that GPS is an advanced research tool able to 
understand out-of-home behavior better than was possible 
with previous methods. In this paper, the new pair-SVR 
algorithm is proposed to evaluate nonlinear regression 
models for the approximation of GPS GDOP, which can 
improve the use of GPS and advanced tracking technologies 
for the analysis of mobility in cognitive diseases. Motivated 
by TSVR, the pair-SVR estimates indirectly the regression 
model through a pair of nonparallel insensitive upper bound 
and lower bound functions solved by two smaller sized 
SVM- type problems, which makes the pair-SVR not only 
yield the faster learning speed than the classical SVR, but 
also be suitable for many cases, especially when the noise is 
heteroscedastic, that is, the noise is strongly corrected with 
the input. Besides, we improved the sparsity of TSVR by 
introducing an insensitive zone constructed by a pair of 
nonparallel upper bound and lower bound function. Only 
points outside the zone are captured as SVs, and only those 
SVs 
construct 
the 
final 
regression 
function. 
The 
experimental results validate that the pair-SVR not only has 
small training cost, but also owns good generalization 
ability and sparsity. 
REFERENCES 
[1] N. Shoval, et. al. The use of advanced tracking technologies 
for the analysis of mobility in Alzheimer's disease and related 
cognitive diseases, BMC Geriatrics,2008,8:7. 
[2] N. Shoval, et. al. Use of the Global Positioning System to 
Measure the Out-of-Home Mobility of Older Adults with 
Differing Cognitive Functioning, ARTICLE in AGEING 
AND SOCIETY 31(05): · JULY 2011,  pp. 849 - 869 
[3] V. Ashkenazi, “Coordinate systems: How to get your position 
very precise and completely wrong,” J. Navig., vol. 39, no. 2, 
May 1986, pp. 269–278. 
[4] D. J. Jwo and K. P. Chin, “Applying back-propagation neural 
networks to GDOP approximation,” J. Navig., vol. 55, no. 1, 
Jan. 2002, pp. 97–108. 
[5] D. Simon and H. El-Sherief, “Navigation satellite selection 
using neural networks,” Neurocomputing,7(3), 1995, pp. 247–
258. 
[6] C.-H. Wu, W.-H. Su, and Y.-W. Ho, “A Study on GPS GDOP 
Approximation Using Support-Vector Machines,” IEEE 
Transactions on Instrumentation & Measurement, 60(1), 2011, 
pp 137 – 145. 
[7] V. Vapnik,  The Nature of Statistical Learning Theory. 
Springer, New York, 2000. 
[8] Jayadeva, R. Khemchandani, and S. Chandra, “Twin support 
vector machines for pattern classiﬁcation,” IEEE Tran. on 
Pattern Analysis and Machine Intellegence 29 (5) (2007) pp. 
905–910.  
[9] X. Peng, “TSVR: an efﬁcient twin support vector machine for 
regression,” Neural Networks 23 (3) (2010) pp. 365–372. 
[10] A. H. Roger and R. J. Charles, Matrix Analysis. Cambridge, 
U.K.: Cambridge Univ. Press, 1982. 
[11] P.-Y. Hao, “New Support Vector Algorithms with Parametric 
Insensitive/Margin Model,” Neural Networks, vol. 23, no. 1, 
January 2010, pp. 60-73.   
[12] C.-H. Wu and W-H. Su, “A comparative study on regression 
models of gps gdop using soft-computing techniques,” in 
Proceedings of the 2009 IEEE International Conference on 
Fuzzy Systems (Fuzz-IEEE 2009), Korea, Aug. 2009, pp. 
1513-1516. 
 
(a)                                                                                        (b) 
 
(c)                                                                                        (d) 
Figure 1. Regression models obtained by (a) -SVR (=0.1), (b) -SVR (=0.01), (c) TSVR, and (d) pair- SVR 
 
107
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-462-6
COGNITIVE 2016 : The Eighth International Conference on Advanced Cognitive Technologies and Applications

