Laser-based Cooperative Estimation of Pose and Size of Moving Objects using  
Multiple Mobile Robots 
Yuto Tamura, Ryohei Murabayashi 
Graduate School of Science and Engineering 
Doshisha Unversity 
Kyotanabe, Kyoto 610-0394 Japan 
Masafumi Hashimoto, Kazuhiko Takahashi 
Faculty of Science and Engineering  
Doshisha Unversity 
Kyotanabe, Kyoto 610-0394 Japan 
e-mail: {mhashimo, katakaha}@mail.doshisha.ac.jp
 
 
Abstract—This paper presents laser-based tracking (estimation 
of pose and size) of moving objects using multiple mobile 
robots as sensor nodes. Each sensor node is equipped with a 
single-layer laser scanner and detects moving objects, such as 
people, cars, and bicycles, in its own laser-scanned images by 
applying an occupancy-grid-based method. Each sensor node 
then estimates the objects’ poses (positions and velocities) and 
sizes using Bayesian filtering and sends these estimates to a 
central server. The central server combines the estimates to 
improve the tracking accuracy and then feeds the information 
back to the sensor nodes. In this cooperative-tracking method, 
the sensor nodes share their tracking information, allowing 
tracking of invisible or partially visible objects. The 
hierarchical architecture of cooperative tracking also makes 
the system scalable and robust. Experimental results using two 
sensor nodes confirm the performance of our tracking method. 
Keywords—moving-object tracking; cooperative tracking; 
pose and size estimation; laser scanner; mobile robot; sensor 
node 
I.  INTRODUCTION 
Tracking of multiple moving objects is an important 
issue in the safe navigation of mobile robots and vehicles. 
The use of laser scanners, radars, or stereo cameras in 
mobile robotics and vehicle automation has attracted 
considerable interest [1]–[7]. The term “tracking” means 
estimating the pose (position and velocity) and size of 
moving object throughout this paper. 
Recently, numerous studies have been conducted on 
multirobot coordination and cooperation [8][9]. When 
multiple robots are located near each another, they can share 
their sensing data through communication network. The 
multirobot team can then be considered a multisensor 
system. Even if moving objects locate outside the sensing 
area of the robot are occluded, they can be found using 
tracking data from other robots in the team. Hence, multi 
robot system can improve the accuracy and reliability with 
which moving objects are tracked [10]–[16].  
Such cooperative tracking or cooperative object 
localization can also be applied to vehicle automation, 
including intelligent transportation systems (ITS) and 
systems for personal mobility devices, as shown in Fig. 1. 
Cooperative tracking enables the detection of moving objects 
in the blind spot of each vehicle and can be used to detect 
sudden changes in a crowded urban environment such as 
people appearing on roads or vehicles making unsafe lane 
changes. It can therefore prevent traffic accidents. 
Our previous works presented a cooperative people-
tracking method in which multiple mobile robots or vehicles 
were used as mobile sensor nodes and equipped with laser 
scanners [17][18]. The covariance intersection method [19] 
was applied to operate the tracking system effectively in a 
decentralized manner without any central server. In 
cooperative people tracking, each person could be assumed 
to be a mass point because of the small size, and mass-point 
tracking (only the pose estimation) was then performed. 
In the real world, several types of moving objects, such 
as people, cars, bicycles, and motorcycles, exist. Therefore, 
we should design a cooperative-tracking system for moving 
objects. In vehicle (car, motorcycle, and bicycle) tracking, 
we have to consider moving objects as rigid bodies and 
estimate both the poses and sizes to avoid the collisions in a 
crowded environment. 
 
 
 
Figure 1.  Example of cooperative tracking in urban environments. 
13
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

Tracking of a rigid body is known as extended-object 
tracking, and many related studies have been conducted 
[20]–[24]. However, to the best of our knowledge, 
cooperative tracking using multiple mobile sensor nodes 
covers only mass-point tracking under the assumption that 
the tracked object is small. It estimates only the object’s 
pose but does not estimate its size. 
Therefore, we presented a laser-based cooperative-
tracking method for rigid bodies that estimates both poses 
and sizes of people and vehicles using multiple mobile 
sensor nodes [25]. In a crowded environment, a vehicle is 
occluded or rendered partially visible by each sensor node. 
To correctly estimate the size of the vehicle, the laser 
measurements captured by sensor nodes in the team have to 
be merged. Our previous cooperative-tracking method for 
rigid bodies applied a centralized architecture. Each sensor 
node detected laser measurements related to the moving 
objects in its sensing area and transmitted the measurement 
information to a central server, which then estimated the 
poses and sizes of the objects. Such a centralized 
architecture imposes a computational burden upon the 
central server. Moreover, the architecture has a weakness 
for fault of communication system between sensor nodes 
and central server.  
To address this problem, in this paper, we present a 
hierarchical method of cooperative tracking by which the 
poses and sizes of moving objects are locally estimated by 
the sensor nodes. Moreover, these estimates are then merged 
by a central server. The rest of the paper is organized as 
follows. Section II gives an overview of our experimental 
system. In Sections III and IV, cooperative tracking is 
discussed. In Section V, we describe an experiment in 
moving-object tracking using two mobile sensor nodes in an 
outdoor environment. We present our conclusions in Section 
VI. 
II. EXPERIMENTAL SYSTEM AND COOPERATIVE TRACKING 
OVERVIEW 
Fig. 2 shows the mobile-sensor node system used in our 
experiments. Each of the two sensor nodes has two 
independently driven wheels. A wheel encoder is attached to 
each drive wheel to measure its velocity. A yaw-rate gyro is 
attached to the chassis of each robot to sense the turning 
velocity. These internal sensors calculate the robot’s pose 
using dead reckoning. 
Each sensor node is equipped with a forward-looking 
laser scanner (SICK LMS100) to capture laser-scanned 
images that are represented by a sequence of distance 
samples in a horizontal plane of 270°. The angular resolution 
of the laser scanner is 0.5°, and each scan image comprises 
541 distance samples. Each sensor node is also equipped 
with RTK–GPS (Novatel ProPak-V3 GPS). The sampling 
frequency of the sensors is 10 Hz. 
 
We use broadcast communication over a wireless local 
area network to exchange information between the central 
server and the sensor nodes. The computer used in the sensor 
nodes and the central server is an Iiyama 15X7100-i7-VGB 
with a 2.8 GHz Intel core i7-4810MQ processor, and the 
 
Figure 2.  Overview of the mobile sensor node. 
 
 
Merging of tracking 
information 
 
Figure 3.  System overview of cooperative tracking. 
 
 
operating system used is Microsoft Windows 7 Professional. 
 
Fig. 3 shows the sequence of moving-object tracking. 
Each sensor node independently finds the moving objects in 
its own laser-scanned images using an occupancy-grid 
method [26]. The sensor node then tracks the moving 
objects (estimates their poses and sizes), and the information 
is uploaded to the central server. The information includes 
the time stamp, the number of the objects tracked, and their 
pose and size. The central server merges the information. It 
estimates the poses and sizes of the moving objects using a 
Bayesian filter. The estimated information is then fed back 
to the sensor nodes. 
To map the laser-scanned images onto the world 
coordinate frame (on which the grid map is represented), 
each sensor node accurately identifies its own position based 
on dead reckoning and GPS information using an extended 
Kalman filter [18]. 
III. TRACKING BY SENSOR NODE 
In this section, we describe the process of estimating the 
poses and sizes of moving objects using Bayesian filter in 
conjunction with data association. 
14
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

A. Pose and Size Estimation 
We represent the shape of the moving object by a 
rectangle of width W and length L. We detail the size-
estimation method in Fig. 4, where red circles indicate laser 
measurements of the moving object (hereafter, moving-
object measurements), green lines are the feature lines 
extracted from those measurements, the green dashed 
rectangle is the estimated rectangle, and the green star is 
the centroid of that rectangle. As shown in Fig. 4, an xvyv-
coordinate frame is defined, on which the yv-axis aligns 
with the heading (orange arrow) of the tracked object. From 
the clustered moving-object measurements, we extract the 
width Wmeas and length Lmeas.  
When a moving object is perfectly visible, its size can be 
estimated from these measurements. In contrast, when the 
object is partially occluded by other objects, its size cannot 
be accurately estimated. Therefore, the size of the partially 
occluded object is estimated by the following equation [20]: 
 
)
(
)
(
1
1
1
1
k
meas
k
k
k
meas
k
k
L
G L
L
L
W
G W
W
W
                  (1) 
 
where W and L are estimates of width and length, 
respectively, and k and k
1 are time steps. G is the filter 
gain, given by 
k
p
G
)
1(
1
 [20], and p is a parameter. 
As the value of p increases, the reliabilities of the current 
measurements of Wmeas and Lmeas increase. We assume that a 
vehicle passes at 60 km/h in front of the sensor node. After 
the vehicle enters the surveillance area of the sensor node, 
we aim to estimate 99% of the size (p = 0.99) within 10 
scans (1 s) of the laser scanner. We can then determine G as 
follows: 
 
.0 369
.0 99)
1(
1
.0 99)
1(
1
10
k
G
    
10
for
10
for
k
k
        (2) 
 
For a perfectly visible object, we set G = 1 in (1) and 
 
 
 
 
Figure 4.  Size estimation of a vehicle. 
estimate its size. 
The estimated size of the tracked object is used to 
classify the object as a person or a vehicle. If the estimated 
size in length or width is larger than 0.8 m, the object is 
assumed to be a vehicle. However, if the size is smaller than 
0.8 m, it is assumed to be a person. 
We then define the centroid position (green star in Fig. 
4) of the rectangle estimated by (1). From the centroid 
position, the pose of the tracked object, position and 
velocity 
( , , , )
x y x y
on the world coordinate frame, is 
estimated using the Kalman filter under the assumption that 
the object is moving at an almost constant velocity.  
To extract Wmeas and Lmeas from the moving-object 
measurements, we have to obtain the heading of the tracked 
object. As shown in Fig. 4, we extract two feature lines 
(green 
lines 
in 
Fig. 
4) 
from 
the 
moving-object 
measurements using the split-and-merge method [27] and 
RANSAC [28] and determine the heading of the object from 
the orientations of the feature lines. When the two feature 
lines cannot be extracted, we determine the heading from 
the estimated velocity 
( , )
x y
 of the object. 
B. Data Association 
To track objects in multi-object and multi-measurement 
environments, we apply data association (i.e., one-to-one 
matching 
of 
tracked 
objects 
and 
moving-object 
measurements). As shown in Fig. 5, a validation gate 
(validation region) is set around the predicted position 
(black circle) of each tracked object. The validation gate is 
rectangular, with a length and width 0.5 m greater than 
those of the object estimated at the previous time step 
(green dashed rectangle).  
We refer to a representative point of grouped moving-
object measurements (red and blue circles) as the 
representative 
measurement 
(light 
blue 
triangles). 
Representative measurements inside the validation gate are 
assumed to originate from the tracked object and are used to 
update the pose of the tracked object with the Kalman filter. 
Measurements outside the validation gate are identified as 
false and discarded.  
Figs. 6 and 7, respectively, show an exemplary laser 
image and data association for a case in which two people 
move close to a car. In these figures, red circles indicate 
moving-object measurements, light blue triangles indicate 
representative measurements, black circles indicate tracked  
 
 
 
 
Figure 5.  Laser images and data association. 
15
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

 
Figure 6.  Laser images of a case in which two people are moving close to 
a car. 
 
 
           
 
 
Figure 7.  Data association for the laser images in Fig. 6. 
 
 
objects, and VG stands for validation gate. The right table in 
Fig. 7 shows the correspondence between tracked objects 
and representative measurements. 
As 
shown 
in 
Fig. 
7, 
multiple 
representative 
measurements are often obtained inside a validation gate in 
the real world, and multiple tracked objects also compete for 
representative measurements. To achieve a reliable data 
association, we introduce the following rules: 
a) Person: Because a person is small, he/she usually 
result in one representative measurement. Thus, if a tracked 
object is assumed to be a person, one-to-one matching of the 
tracked person and a representative measurement is 
performed. 
b) Vehicle (car, motorcycle, or bicycle): Because a 
vehicle is large, as shown in Fig. 7, it often produces 
multiple representative measurements. Thus, if a tracked 
object is assumed to be a vehicle, one-to-many matching of 
the tracked vehicle and representative measurements is 
performed. 
As shown in Fig. 6, on urban streets, people often move 
close to vehicles, whereas vehicles move far away from 
each other. Thus, when representative measurements of 
people exist in the validation gate of a tracked vehicle, they 
might be matched to the tracked vehicle. To avoid this, we 
begin data association for people. 
We illustrate our data-association method from Fig. 7, in 
which the validation gates of a person and a car overlap. If 
tracked objects T2 and T3 are determined to be people, the 
representative measurement M3 is matched with T2 and the 
representative measurement M4 nearest to T3 is matched 
with T3, both through one-to-one matching. Subsequently, 
if the tracked object T1 is determined to be a vehicle, the 
two representative measurements M1 and M2 in the 
validation gate are matched with T1 through one-to-many 
matching. If the validation gates of several people overlap, 
one-to-one matching is performed using the global nearest 
neighbor method [18][29]. 
A representative measurement that is not matched with 
any tracked objects is assumed either to originate from a 
new moving object or to be a false alarm. Therefore, we 
tentatively initiate tracking of the measurement with the 
Kalman filter. If the measurement remains visible, it is 
assumed to originate from a new object and tracking is 
continued. If the measurement disappears quickly, it is 
treated as a false alarm, and tentative tracking is terminated. 
Moving objects appear in and disappear from the sensing 
area of the laser scanner. They also occlude each other and 
are occluded by other objects in the environment. To 
maintain reliable tracking under such conditions, we 
implement a rule-based tracking-handling system [18]. 
IV. MERGING OF TRACKING DATA BY A CENTRAL SERVER 
The information concerning objects tracked by the sensor 
nodes is combined using data association. We present an 
example of our data-association procedure in Figs. 8 and 9, 
in which two sensor nodes are tracking a car. In Fig. 8, red 
and blue rectangles indicate the sizes of the tracked objects 
#A (TA) and #B (TB), as estimated by sensor nodes #1 and 
#2, respectively. Orange arrows indicate the headings of the 
objects.  
If TA and TB originate from the same object, their 
position, velocity, and heading estimates will have similar 
values. If the tracked object is a vehicle, the size estimated 
by sensor nodes will be large. If it is a person, the estimated 
size will be small. Therefore, we set a validation gate with a 
constant radius of 3 m around the TA position (red star in Fig. 
8) and introduce the following rules to match TB with TA: 
a) Same or different object: When the estimated position 
of TB (blue star) is located within the validation gate, and the 
 
 
 
 
 
Figure 8.  Data association of tracking information related to objects TA 
and TB.  
16
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

 
 
Figure 9.  Integration of tracking information. 
 
 
differences in the velocity and heading estimates of TA and 
TB are less than 0.8 m/s and 15°, respectively, the objects 
TA and TB are determined to originate from the same object. 
Otherwise, the objects TA and TB are determined to be 
different objects. 
b) Vehicle or person: When the width and/or length 
estimates of the matched objects TA and TB are larger than 
0.8 m, their objects are determined to originate from the 
same vehicle. When their width and length estimates are less 
than 0.8 m, the objects TA and TB are determined to 
originate from the same person.  
When more than two tracked objects (e.g., TB and TC) 
are present in the validation gate of TA, the similar data 
association rules are applied.  
After the two tracked objects TA and TB have been 
matched, their tracking information is combined. As shown 
in Fig. 9, we select the tracked object TB, which has a larger 
rectangle (blue rectangle) than TA (red rectangle), and 
define an xvyv-coordinate frame on which the yv-axis aligns 
with the heading of TB. A rectangle (the green dashed 
rectangle) is then generated that encloses the two rectangles 
of TA and TB using positional information on their vertices. 
We then estimate the size of the integrated object using (1) 
based on the width and length of the new rectangle. 
From the centroid position (green star) of the new 
rectangle, the position and velocity of the integrated object 
are estimated using the Kalman filter under the assumption 
that the object is moving at an almost constant velocity.  
V. EXPERIMENTAL RESULTS 
We evaluated our cooperative-tracking method by 
conducting an experiment in a parking environment, as 
shown in Fig. 10. Two mobile sensor nodes tracked a car 
(vehicle #1), a motorcycle (vehicle #2), and two pedestrians 
(persons #1 and #2). Fig. 11 shows the movement paths of 
the sensor nodes (black dashed lines), vehicles #1 and #2 
(blue and green lines), and persons #1 and #2 (red and black 
lines). The moving speeds of the sensor nodes, car, 
motorcycle, and people were approximately 1.5, 15, 20, and 
6 km/h, respectively.  
Fig. 12 (a) shows the position and size results estimated 
by cooperative tracking. We plot estimated rectangles every 
1 s (10 scans). For comparison, individual tracking by each 
sensor node was also conducted. The tracking results for 
sensor nodes #1 and #2 are shown in Figs. 12 (b) and (c), 
respectively.  
The estimated size of car (vehicle #1) using cooperative 
and individual tracking is shown in Figs. 13 (a), (b), and (c). 
In these figures, red and blue lines indicate the estimated 
length and width, respectively. Two dashed lines indicate  
 
 
 
 
Sensor node #1 
Vehicle #2 
Person #2 
 
(a) Photo by camera #A 
 
 
Sensor node #2 
Vehicle #1 
Person #1 
 
(b) Photo by camera #B 
 
Figure 10.  Photo of the experimental environment. 
 
 
 
Sensor node #2 
Sensor node #1 
Person #2 
Person #1 
Vehicle #1 
Vehicle #2 
Camera #A 
Camera #B 
 
 
Figure 11.  Movement paths of sensor nodes and moving objects 
 
W 
17
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

   
    
 
 
  (a) Cooperative tracking                              (b) Individual tracking by sensor node #1                  (c) Individual tracking by sensor node #2 
 
Figure 12.  Tracks and sizes of moving objects estimated by cooperative- and individual-tracking methods. 
 
 
            
            
 
 
  (a) Cooperative tracking                                    (b) Individual tracking by sensor node #1                      (c) Individual tracking by sensor node #2 
 
Figure 13.  Size of car (vehicle #1) estimated by cooperative- and individual-tracking methods. 
 
 
TABLE I.  PROCESSING TIME OF PROPOSED HIERARCHICAL COOPERATIVE 
TRACKING 
 
 
Maximum [ms] 
Minimum [ms] 
Mean [ms] 
Central server 
2.3 
0.1 
0.8 
Sensor node #1 
47.9 
36.8 
41.7 
Sensor node #2 
49.8 
39.3 
43.0 
 
 
TABLE II.  PROCESSING TIME OF PREVIOUS CENTRALIZED COOPERATIVE 
TRACKING 
 
 
Maximum [ms] 
Minimum [ms] 
Mean [ms] 
Central server 
23.8 
2.2 
7.9 
Sensor node #1 
41.5 
36.1 
38.2 
Sensor node #2 
45.7 
36.5 
  38.8 
 
 
the true length and width of the car. 
In individual tracking, each sensor node partially tracks 
moving objects because the objects leave from the sensing 
area of the sensor nodes and are blocked by parked cars. In 
contrast, cooperative tracking always tracks the moving 
objects, because the two sensor nodes share the tracking 
data. It is clear from Figs. 12 and 13 that cooperative 
tracking offers better tracking accuracy than individual 
tracking.  
We examined the processing times of the sensor nodes 
and the central server in the experiment. Tables I and II 
show the results of our proposed hierarchical tracking 
scheme and the previous centralized cooperative-tracking 
scheme, respectively.  
In our previous method [25], the central server estimated 
the poses and sizes of moving objects based on the moving-
objects measurements sent from the sensor nodes. 
Conversely, in our proposed method, the sensor nodes 
locally estimate the poses and sizes of moving objects, and 
the central server merges these estimates. Therefore, the 
hierarchical cooperative-tracking scheme reduces the 
computational burden on the central server. 
VI. CONCLUSIONS 
This paper presented a laser-based cooperative-tracking 
for moving objects using multiple mobile robots as sensor 
nodes. The moving objects were assumed to be rectangular 
rigid bodies, and the poses (positions and velocities) and 
sizes were locally estimated by the sensor nodes. These 
estimates were then merged by a central server. The 
effectiveness of such a hierarchical cooperative-tracking 
18
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

method was demonstrated by an experiment in which a car, 
a motorcycle, and two pedestrians were tracked using two 
sensor nodes.  
In this study, single-layer laser scanners on mobile 
sensor modes were used to sense the surrounding 
environments. Multilayer laser scanners provide richer 
information than single-layer laser scanners and thus 
improve recognition of the surrounding environment. 
Research is currently being conducted on the design of 
cooperative-tracking system using multiple sensor nodes 
equipped with multilayer laser scanners.  
ACKNOWLEDGMENT  
This study was partially supported by the Scientific 
Grants #26420213, the Japan Society for the Promotion of 
Science (JSPS), and the MEXT-Supported Program for the 
Strategic Research Foundation at Private Universities, 
2014–2018, Ministry of Education, Culture, Sports, Science 
and Technology, Japan. 
REFERENCES 
[1] K. O. Arra and O. M. Mozos, Special issue on: People 
Detection and Tracking, Int. J. of Social Robotics, vol.2, no.1, 
2010. 
[2] C. Mertz, et al., “Moving Object Detection with Laser 
Scanners,” J. of Field Robotics, vol.30, pp. 17–43, 2013. 
[3] T. Ogawa, H. Sakai, Y. Suzuki, K. Takagi, and K. Morikawa, 
“Pedestrian Detection and Tracking using In-vehicle Lidar for 
Automotive Application,” Proc. of IEEE Intelligent Vehicles 
Symp. (IV2011), pp. 734–739, 2011. 
[4] A. Mukhtar, L. Xia, and T.B. Tang, “Vehicle Detection 
Techniques for Collision Avoidance Systems: A Review,” 
IEEE Trans. on Intelligent Transportation Systems, vol. 16, pp. 
2318–2338, 2015. 
[5] H. Cho, Y. W. Seo, B.V.K. V. Kumar, and R. R. Rajkumar, “A 
Multi-sensor Fusion System for Moving Object Detection and 
Tracking in Urban Driving Environments,” Proc. of  Int. Conf. 
on IEEE Robotics and Automation (ICRA2014), pp. 1836–
1843, 2014.  
[6] D. Z. Wang, I. Posner, and P. Newman, “Model-free Detection 
and Tracking of Dynamic Objects with 2D Lidar,” Int. J. of 
Robotics Research, vol.34, pp. 1039–1063, 2015 
[7] D. Z. Wang, I. Posner, P. Newman, “What could move? 
Finding cars, pedestrians and bicyclists in 3D laser data,” Proc. 
of IEEE Int. Conf. on Robotics and Automation (ICRA2012), 
pp. 4038–4044, 2012. 
[8] Z. Yan, N. Jouandeau, and A. A. Cherif, “A Survey and 
Analysis of Multi-Robot Coordination,” Int. J. of Advanced 
Robotic Systems, vol. 10, pp. 1–18, 2013. 
[9] S. Nadarajah and K. Sundaraj, “A Survey on Team Strategies 
in Robot Soccer: Team Strategies and Role Description,” 
Artificial Intelligence Review, vol. 40, pp. 271–304, 2013. 
[10] Z.Wang and D. Gu, “Cooperative Target Tracking Control of 
Multiple Robots,” IEEE Trans. on Industrial Electronics, vol. 
59, pp. 3232–3240, 2012. 
[11] K. Zhou and S. I. Roumeliotis, “Multirobot Active Target 
Tracking with Combinations of Relative Observations,” IEEE 
Trans. on Robotics, vol. 27, pp. 678–695, 2011. 
[12] A. Ahmad and P. Lima, “Multi-robot Cooperative Spherical-
Object Tracking in 3D Space based on Particle Filters,” 
Robotics and Autonomous Systems, vol. 61, pp. 1084–1093, 
2013. 
[13] P. U. Limaa, et al., “Formation Control Driven by Cooperative 
Object Tracking,” Robotics and Autonomous Systems, vol. 63, 
Part 1, pp. 68–79, 2015. 
[14] C. Robin and S. Lacroix, “Multi-robot Target Detection and 
Tracking: Taxonomy and Survey,” Autonomous Robots, vol. 
40, pp. 729–760, 2016. 
[15] C. T. Chou, J. Y. Li, M. F. Chang, and L. C. Fu, “Multi-Robot 
Cooperation Based Human Tracking System Using Laser 
Range Finder,” Proc. of IEEE Int. Conf. on Robotics and 
Automation (ICRA2011), pp. 532–537, 2011. 
[16] N. A. Tsokas and K. J. Kyriakopoulos, “Multi-robot Multiple 
Hypothesis Tracking for Pedestrian Tracking,” Autonomous 
Robot, vol. 32, pp. 63–79, 2012. 
[17] K. Kakinuma, M. Hashimoto, and K. Takahashi, “Outdoor 
Pedestrian Tracking by Multiple Mobile Robots based on 
SLAM and GPS Fusion,” Proc. of IEEE/SICE Int. Symp. on 
System Integration (SII2012), pp. 422–427, 2012. 
[18] M. Ozaki, K. Kakinuma, M. Hashimoto, and K. Takahashi, 
“Laser-based Pedestrian Tracking in Outdoor Environments by 
Multiple Mobile Robots,” Sensors, vol. 12, pp. 14489–14507, 
2012. 
[19] S.J. Julier and J.K. Uhlmann, “A Non-divergent Estimation 
Algorithm in the Presence of Unknown Correlations,” Proc. of 
the IEEE American Control Conf., pp. 2369–2373, 1997. 
[20] F. Fayad and V. Cherfaoui, “Tracking Objects using a Laser 
Scanner in Driving Situation based on Modeling Target 
Shape,” Proc. of the 2007 IEEE Int. Vehicles Symp. (IV2007), 
pp. 44–49, 2007. 
[21] T. Miyata, Y. Ohama, and Y. Ninomiya, “Ego-Motion 
Estimation and Moving Object Tracking using Multi-layer 
LIDAR,” Proc. of IEEE Intelligent Vehicles Symp. (IV2009), 
pp. 151–156, 2009 
[22] K. Granstrom, C. Lundquist, F. Gustafsson, and U. Orguner, 
“Radom Set Methods, Estimation of Multiple Extended 
Objects,” IEEE Robotics & Automation Magazine, pp. 73–82, 
June 2014 
[23] L. Mihaylova, et al., “Overview of Bayesian Sequential Monte 
Carlo Methods for Group and Extended Object Tracking,” 
Digital Signal Processing, vol. 25, pp.1–16, 2014. 
[24] J. Lan and X. R. Li, “Tracking of Extended Object or Target 
Group using Random Matrix Part I: New Model and 
Approach,” Proc. of 15th Int. Conf. on Information Fusion 
(FUSION2012), pp.2177–2184, 2012. 
[25] M. Hashimoto, R. Izumi, Y. Tamura, and K. Takahashi, 
“Laser-based Tracking of People and Vehicles by Multiple 
Mobile Robots,” Proc. of the 11th Int. Conf. on Informatics in 
Control, Automation and Robotics (ICIT2014), pp. 522–527, 
2014. 
[26] M. Hashimoto, S. Ogata, F. Oba, and T. Murayama, “A Laser- 
based Multi-Target Tracking for Mobile Robot,” Intelligent 
Autonomous Systems 9, pp. 135–144, 2006. 
[27] V. Nguyen, A. Martinelli, N. Tomatis, and R. Siegwart, “A 
Comparison of Line Extraction Algorithms using 2D Laser 
Rangefinder for Indoor Mobile Robotics,” Proc. of 2005 
IEEE/RSJ Int. Conf. on Intelligent Robots and Systems 
(IROS2009), pp. 1929–1934, 2009. 
[28] M. Fischler and R. Bolles, “Random Sample Consensus: A 
Paradigm for Model Fitting Applications to Image Analysis 
and Automated Cartography,” Proc. of Image Understanding 
workshop, pp. 71–88, 1980.  
[29] P. Konstantinova, A. Udvarev, and T. Semerdjiev, “A Study 
of a Target Tracking Algorithm Using Global Nearest 
Neighbor Approach,” Proc. of Int. Conf. on Systems and 
Technologies, 2003. 
 
 
19
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

