Reliability of Erasure Coded Systems under Rebuild
Bandwidth Constraints
Ilias Iliadis
IBM Research – Zurich
8803 R¨uschlikon, Switzerland
Email: ili@zurich.ibm.com
Abstract—Modern storage systems employ erasure coding redun-
dancy and recovering schemes to ensure high data reliability
at high storage efﬁciency. The widely used replication scheme
belongs to this broad class of erasure coding schemes. The
effectiveness of these schemes has been evaluated based on the
Mean Time to Data Loss (MTTDL) and the Expected Annual
Fraction of Data Loss (EAFDL) metrics. To improve the reliability
of data storage systems, certain data placement and rebuild
schemes reduce the rebuild times by recovering data in parallel
from the storage devices. It is often assumed though that there
is sufﬁcient network bandwidth to transfer the data required
by the rebuild process at full speed. In large-scale data storage
systems, however, the network bandwidth is constrained. This
article obtains the MTTDL and EAFDL of erasure coded systems
analytically for the symmetric, clustered, and declustered data
placement schemes under network rebuild bandwidth constraints.
The resulting reliability degradation is assessed and the results
obtained establish that the declustered placement scheme offers
superior reliability in terms of both metrics. Efﬁcient codeword
conﬁgurations that achieve high reliability in the presence of
network rebuild bandwidth constraints are identiﬁed.
Keywords–Storage;
Reliability;
Data
placement;
MTTDL;
EAFDL; RAID; MDS codes; Information Dispersal Algorithm; Pri-
oritized rebuild; Repair bandwidth; Network bandwidth constraint.
I.
INTRODUCTION
In today’s large-scale data storage systems, data redun-
dancy is introduced to ensure that data lost owing to device and
component failures can be recovered. Appropriate redundancy
schemes are deployed to prevent permanent loss of data and,
consequently, enhance the reliability of storage systems. The
effectiveness of these schemes has been evaluated based on the
Mean Time to Data Loss (MTTDL) [1-20] and, more recently,
the Fraction of Data Loss Per Year (FDLPY) [21] and the
equivalent Expected Annual Fraction of Data Loss (EAFDL)
reliability metrics [22-24]. Analytical reliability expressions
for the MTTDL were obtained predominately using Markovian
models, which assume that component failure and rebuild
times are independent and exponentially distributed. In practice
though, these distributions are not exponential. To cope with
this issue, system reliability was assessed in [16][18][23][24]
using an alternative methodology that does not involve any
Markovian analysis and considers the practical case of non-
exponential failure and rebuild time distributions. Moreover,
the misconception reported in [25] that MTTDL derivations
based on Markovian models provide unrealistic results was
dispelled in [26] by invoking improved MTTDL derivations
that yield satisfactory results, and also by drawing on prior
work that analytically obtains MTTDL without involving any
Markovian analysis.
Earlier works have predominately considered the MTTDL
metric, whereas recent works have also considered the EAFDL
metric [22][23][24]. The introduction of the latter metric was
motivated by the fact that Amazon S3 considers the durability
of data over a given year [27], and, similarly, Facebook [28],
LinkedIn [29] and Yahoo! [30] consider the amount of data
lost in given periods.
To protect data from being lost and improve the reliability
of data storage systems, replication-based storage systems
spread replicas corresponding to data stored on each storage
device across several other storage devices. To improve the
low storage efﬁciency associated with the replication schemes,
erasure coding schemes that provide a high data reliability as
well as a high storage efﬁciency are deployed. Special cases
of such codes are the Redundant Arrays of Inexpensive Disks
(RAID) schemes, such as RAID-5 and RAID-6, that have been
extensively deployed in the past thirty years [1][2].
State-of-the-art data storage systems [31-34] employ more
general erasure codes that affect the reliability, performance,
and the storage and reconstruction overhead of the system.
In this article, we focus on the reliability assessment of
erasure coded systems in terms of the MTTDL and EAFDL
metrics. These metrics were analytically derived in [23] for the
symmetric, clustered, and declustered data placement schemes
under the assumption that there is sufﬁcient network bandwidth
to transfer the data required by the rebuild process at full speed.
For instance, in the case of a declustered placement, redundant
data associated with the data stored on a given device is placed
across all remaining devices in the system. In this way, the
rebuild process can be parallelized, which in turn results in
short rebuild times. The restoration time can be minimized
provided there is sufﬁcient network rebuild bandwidth avail-
able. In large-scale data storage systems though, the network
bandwidth is constrained.
The effect of network rebuild bandwidth constraints on
the reliability of replication-based storage systems was stud-
ied in [8][15]. It was found that spreading replicas over
a higher number of devices than what the network rebuild
bandwidth can support at full speed during a parallel rebuild
process, led to system reliability being signiﬁcantly reduced.
The reliability of erasure coded systems in the absence of
bandwidth constraints was assessed in [23]. The MTTDL and
EAFDL metrics were obtained analytically for the symmetric,
clustered, and declustered data placement schemes based on
a general framework and methodology. In this article, we
recognize that this methodology also holds in the case of
network rebuild bandwidth constraints and apply it to derive
enhanced closed-form reliability expressions for the MTTDL
1
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

and EAFDL metrics for these placement schemes in the
presence of such rebuild bandwidth constraints. Subsequently,
we provide insight into the effect of the placement schemes and
the impact of the available network rebuild bandwidth on sys-
tem reliability. The validity of this methodology for accurately
assessing the reliability of storage systems was conﬁrmed by
means of simulation in several contexts [14-16, 18, 22]. It was
demonstrated that the theoretical predictions for the reliability
of systems comprised of highly reliable storage devices match
well with the simulation results obtained. Consequently, the
emphasis of the present work is on the theoretical assessment
of the effect of network rebuild bandwidth constraints on the
reliability of erasure coded systems. Also, this work extends
the reliability results obtained in [15] for the special case of
replication-based storage systems to the more general case of
erasure coded systems.
The remainder of the article is organized as follows. Sec-
tion II describes the storage system model and the correspond-
ing parameters considered. Section III presents the adaptation
of a general framework and methodology for deriving the
MTTDL and EAFDL metrics analytically for the case of
erasure coded systems under network rebuild bandwidth con-
straints. Closed-form expressions for the symmetric, clustered,
and declustered placement schemes are derived. Section IV
presents numerical results demonstrating the effectiveness of
the erasure coding redundancy schemes for improving the
system reliability. It also assesses the sensitivity to the network
rebuild bandwidth constraints under various codeword conﬁg-
urations. Section V provides a discussion on the applicability
of the results obtained. Finally, we conclude in Section VI.
II.
STORAGE SYSTEM MODEL
Modern data storage systems use erasure coded schemes
to protect data from device failures. When devices fail, the
redundancy of the data affected is reduced and eventually lost.
To avoid irrecoverable data loss, the system performs rebuild
operations that use the data stored in the surviving devices
to reconstruct the temporarily lost data, thus maintaining the
initial data redundancy. We proceed by brieﬂy reviewing the
basic concepts of erasure coding and data recovery procedures
of such storage systems. To assess their reliability, we consider
the model used in [23], and adopt and extend the notation.
More precisely, the storage system considered comprises n
storage devices (nodes or disks), with each device storing an
amount c of data, such that the total storage capacity of the
system is n c.
A. Redundancy
User data is divided into blocks (or symbols) of a ﬁxed size
(e.g., sector size of 512 bytes) and complemented with parity
symbols to form codewords. We consider (m, l) maximum
distance separable (MDS) erasure codes, which are a mapping
from l user data symbols to a set of m (> l) symbols, called
a codeword, having the property that any subset containing
l of the m symbols of the codeword can be used to decode
(reconstruct, recover) the codeword. The corresponding storage
efﬁciency, seff, is given by
seff = l
m .
(1)
TABLE I.
NOTATION OF SYSTEM PARAMETERS
Parameter
Deﬁnition
n
number of storage devices
c
amount of data stored on each device
l
number of user-data symbols per codeword (l ≥ 1)
m
total number of symbols per codeword (m > l)
(m, l)
MDS-code structure
k
spread factor of the data placement scheme, or
group size (number of devices in a group)
b
reserved rebuild bandwidth per device
Bmax
maximum network rebuild bandwidth
Fλ(.)
cumulative distribution function of device lifetimes
seff
storage efﬁciency of redundancy scheme (seff = l/m)
U
amount of user data stored in the system (U = seff n c)
˜r
minimum number of codeword symbols lost that lead to an irrecov-
erable data loss (˜r = m − l + 1 and 2 ≤ ˜r ≤ m)
Nb
maximum number of devices from which rebuild can occur at full
speed in parallel (Nb = Bmax/b)
Beff
effective network rebuild bandwidth
1/µ
time to read (or write) an amount c of data at a rate b from (or to)
a device (1/µ = c/b)
1/λ
mean time to failure of a storage device
(1/λ =
R ∞
0 [1 − Fλ(t)]dt)
Consequently, the amount of user data, U, stored in the system
is given by
U = seff n c = l n c
m
.
(2)
The notation used is summarized in Table I. The parameters are
divided according to whether they are independent or derived,
and are listed in the upper and the lower part of the table,
respectively.
The m symbols of each codeword are stored on m distinct
devices, such that the system can tolerate any ˜r − 1 device
failures, but ˜r device failures may lead to data loss, with
˜r = m − l + 1 .
(3)
From the preceding, it follows that
1 ≤ l < m
and
2 ≤ ˜r ≤ m .
(4)
Examples of MDS erasure codes are the following:
Replication: A replication-based system with a replication
factor r can tolerate any loss of up to r − 1 copies of some
data, such that l = 1, m = r and ˜r = r. Also, its storage
efﬁciency is equal to s(replication)
eff
= 1/r.
RAID-5: A RAID-5 array comprised of N devices uses an
(N, N − 1) MDS code, such that l = N − 1, m = N and
˜r = 2. It can therefore tolerate the loss of up to one device,
and its storage efﬁciency is equal to s(RAID-5)
eff
= (N − 1)/N.
RAID-6: A RAID-6 array comprised of N devices uses an
(N, N − 2) MDS code, such that l = N − 2, m = N and
˜r = 3. It can therefore tolerate a loss of up to two devices,
and its storage efﬁciency is equal to s(RAID-6)
eff
= (N − 2)/N.
Reed–Solomon: It is based on (m, l) MDS erasure codes.
B. Symmetric Codeword Placement
According to a symmetric codeword placement, each code-
word is stored on m distinct devices with one symbol per
device. In a large storage system, the number of devices, n, is
usually much larger than the codeword length, m. Therefore,
there are many ways in which a codeword of m symbols can
be stored across a subset of the n devices. For each device
in the system, the redundancy spread factor k denotes the
number of devices over which the codewords stored on that
device are spread [18]. The system effectively comprises n/k
2
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

Figure 1. Clustered and declustered placement of codewords of length m = 3
on n = 6 devices. X1, X2, X3 represent a codeword (X = A, B, C, . . . , L).
disjoint groups of k devices. Each group contains an amount
U/k of user data, with the corresponding codewords placed
on the corresponding k devices in a distributed manner. Each
codeword is placed entirely in one of the n/k groups. Within
each group, all

Figure 3.
Rebuild under clustered placement.
where Nb speciﬁes the effective maximum number of devices
from which rebuild can occur in parallel at full speed, and is
given by
Nb ≜ Bmax
b
.
(7)
Note that Nb may not be an integer; it only represents the
effective maximum number of devices from which distributed
rebuild can occur at full speed. Substituting b = cµ into (6),
we get
Beff = min(˜k, Nb) cµ .
(8)
A similar reconstruction process is used for other symmet-
ric placement schemes within each group of k devices, except
for the clustered placement. When clustered placement is used,
the codeword symbols are spread across all k = m devices in
each group (cluster). Therefore, reconstructing the lost symbols
on the surviving devices of a group will result in more than
one symbol of the same codeword on the same device. To
avoid this, the lost symbols are reconstructed directly in spare
devices as shown in Figure 3. In these reconstruction processes,
decoding and re-encoding of data are assumed to be done
on the ﬂy and so the time taken for reconstruction is equal
to the time taken to read and write the required data to the
devices. Note also that alternative erasure coding schemes have
been proposed to reduce the amount of data transferred over
the storage network during reconstruction (see [35][36] and
references therein).
D. Exposure Levels and Amount of Data to Rebuild
At time t, Dj(t) denotes the number of codewords that
have lost j symbols, with 0 ≤ j ≤ ˜r. The system is at exposure
level u (0 ≤ u ≤ ˜r), where
u =
max
Dj(t)>0 j.
(9)
The system is at exposure level u if there are codewords with
m − u symbols left, but there are no codewords with fewer
than m − u symbols left in the system, that is, Du(t) > 0,
and Dj(t) = 0, for all j > u. These codewords are referred
to as the most-exposed codewords. At t = 0, Dj(0) = 0, for
all j > 0, and D0(0) is the total number of codewords stored
in the system. Device failures and rebuild processes cause the
values of D1(t), · · · , D˜r(t) to change over time, and when a
data loss occurs, D˜r(t) > 0. Device failures cause transitions
to higher exposure levels, whereas rebuilds cause transitions to
lower ones. Let tu denote the time of the ﬁrst transition from
exposure level u − 1 to exposure level u, and t+
u the instant
immediately after tu. Then, the number, Cu, of most exposed
codewords when entering exposure level u, u = 1, . . . , ˜r, is
given by Cu = Du(t+
u ).
Analytic expressions for the reliability metrics of interest
were derived in [23], using the direct path approximation,
which considers only transitions from lower to higher exposure
levels [14][16][18]. This implies that each exposure level is
entered only once.
E. Failure and Rebuild Time Distributions
We adopt the model and notation considered in [24]. The
lifetimes of the n devices are assumed to be independent and
identically distributed, with a cumulative distribution function
Fλ(.) and a mean of 1/λ. Real-world distributions, such as
Weibull and gamma, as well as exponential distributions that
belong to the large class deﬁned in [16] are considered. The
storage devices are characterized to be highly reliable in that
the ratio of the mean time 1/µ to read all contents of a device
(which typically is on the order of tens of hours), to the mean
time to failure of a device 1/λ (which is typically at least on
the order of thousands of hours) is small, that is,
λ
µ = λ c
b
≪ 1 .
(10)
We consider storage devices whose the cumulative distri-
bution function Fλ satisﬁes the condition
µ
Z 1/µ
0
Fλ(t) dt ≪ 1,
with λ
µ ≪ 1 ,
(11)
such that the MTTDL and EAFDL reliability metrics of erasure
coded storage systems tend to be insensitive to the device
failure distribution, that is, they depend only on its mean 1/λ,
but not on its density Fλ(.)[23].
III.
DERIVATION OF MTTDL AND EAFDL
The MTTDL metric assesses the expected amount of time
until some data can no longer be recovered and therefore is
irrecoverably lost, whereas the EAFDL metric assesses the
fraction of stored data that is expected to be lost by the system
annually. The MTTDL(Bmax) and EAFDL(Bmax) metrics are
derived as a function of Bmax based on the framework and
methodology presented in [23]. More speciﬁcally, this method-
ology uses the direct path approximation and does not involve
any Markovian analysis. It holds for general failure time
distributions, which can be exponential or non-exponential,
such as the Weibull and gamma distributions that satisfy
condition (11). Note that this framework is general in that it
also applies in the case where the network rebuild bandwidth
is constrained. The only parameters that are affected by the
network rebuild bandwidth constraint are the rebuild rates and,
accordingly, those parameters that depend on them, such as
the rebuild times. Analytic expressions for the two metrics
of interest were derived in [23, Equations (44) and (45)] as
follows:
MTTDL(Bmax) ≈
1
n λ
(˜r − 1)!
(λ c)˜r−1
˜r−1
Y
u=1
bu(Bmax)
˜nu
1
V ˜r−1−u
u
,
(12)
4
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

and
EAFDL(Bmax) ≈ m λ (λ c)˜r−1 1
˜r !
˜r−1
Y
u=1
˜nu
bu(Bmax) V ˜r−u
u
,
(13)
where ˜nu represents the number of devices at exposure level
u whose failure before the rebuild of the most-exposed code-
words causes an exposure level transition to level u + 1. Also,
Vu represents the fraction of the most-exposed codewords at
exposure level u that have symbols stored on a newly failed
device that causes the exposure level transition u → u+1. Note
that this fraction depends only on the codeword placement
scheme. As mentioned in the preceding, bu, the rate at which
the amount of data that needs to be rebuilt at exposure level u is
written to selected device(s), depends on Bmax, the maximum
network rebuild bandwidth.
Remark 1: From [23, Equation (43)], it follows that the
expected amount E(H) of data lost, given that a data loss has
occurred, does not depend on bu and therefore is not affected
by the maximum network rebuild bandwidth. Consequently,
this reliability metric is not considered in this article.
Remark 2: The analytic expressions for the MTTDL and
EAFDL reliability metrics were derived in [23] in the absence
of network rebuild bandwidth constraints. Consequently, they
correspond to the case of Bmax = ∞, with the two metrics
being denoted by MTTDL(∞) and EAFDL(∞), respectively.
From (12) and (13), it follows that
MTTDL(Bmax)
MTTDL(∞)
=
EAFDL(∞)
EAFDL(Bmax) = θ ,
(14)
where θ represents the reliability reduction factor that assesses
the reliability degradation due to a network rebuild bandwidth
constraint, and is given by
θ ≜
˜r−1
Y
u=1
bu(Bmax)
bu(∞)
.
(15)
Remark 3: From (15), and given that bu(Bmax) decreases
as Bmax decreases, it follows that θ decreases as ˜r increases
and Bmax decreases.
A. Symmetric Placement
We consider the case where the redundancy spread factor
k is in the interval m < k ≤ n. As discussed in [23, Section
III-B], at each exposure level u, the prioritized rebuild process
recovers one of the u symbols that each of the most-exposed
codewords has lost by reading m − ˜r + 1 of the remaining
symbols from the ˜nu surviving devices in the affected group.
According to [23, Equation (46)], it holds that
˜nsym
u
= k − u .
(16)
Furthermore, in the absence of a network rebuild bandwidth
constraint, the total write bandwidth, which is also the rebuild
rate bu, is given by [23, Equation (47)]
bsym
u (∞) =
˜nsym
u
m − ˜r + 2 b
(16)
=
(k − u) b
m − ˜r + 2 ,
u = 1, . . . , ˜r−1 .
(17)
In the presence though of a network rebuild bandwidth con-
straint, Bmax, and according to (6), with ˜k = ˜nu = ˜nsym
u , the
rebuild rate bu is given as a function of Bmax by
bsym
u (Bmax) =
Beff(˜nu)
m − ˜r + 2 = min(˜nu b, Bmax)
m − ˜r + 2
= min(˜nu, Nb) b
m − ˜r + 2
(16)
=
min(k − u, Nb) b
m − ˜r + 2
, for u = 1, . . . , ˜r − 1 .
(18)
Substituting (17) and (18) into (15) yields
θ sym =
˜r−1
Y
u=1
min(k − u, Nb)
k − u
.
(19)
Note that when Nb ≥ k − 1, the system reliability is not
affected because all rebuilds are performed at full speed, and
therefore the θ factor is equal to one. However, when Nb <
k − 1, it may not be possible for some of the rebuilds to be
performed at full speed, and therefore the factor θ will be less
than one, which affects the system reliability. Consequently,
the reliability reduction factor, θ, depends on the bandwidth
constraint factor, φ, given by
φ ≜ min
Nb
k , 1

(7)
= min
Bmax
k b , 1

, with 0 ≤ φ ≤ 1 .
(20)
From (19) and (20), and recognizing that min(k − u, Nb) =
min(min(k − u, k), Nb)
=
min(k − u, min(k, Nb))
=
min(k min(1, Nb/k), k −u) = min(k φ, k −u), it follows that
θ sym =
˜r−1
Y
u=1
min

φ
1 − u
k
, 1

.
(21)
Using (3) and (21), and the fact that MTTDL(∞) and
EAFDL(∞) are given by [23, Equations (49) and (50)],
respectively, (14) yields
MTTDLsym
k (Bmax) ≈
1
n λ

b
(l + 1) λ c
m−l
(m − l)!
m−l
Y
u=1
 k − u
m − u
m−l−u
m−l
Y
u=1
min

φ
1 − u
k
, 1

,
(22)
and
EAFDLsym
k (Bmax) ≈ λ
(l + 1) λ c
b
m−l
m
(m − l + 1)!
m−l
Y
u=1
m − u
k − u
m−l+1−u , m−l
Y
u=1
min

φ
1 − u
k
, 1

,
(23)
where Bmax is expressed via φ given by (20).
Note that for a replication-based system, for which m = r
and l = 1, and by virtue of (19) and (21), (22) is in agreement
with Equation (24) of [15], with c/b = 1/µ.
Remark 4: From (22) and (23), it follows that MTTDLsym
k
depends on n, but EAFDLsym
k
does not.
Remark 5: From (22) and (23), and for any value of φ, it
can be proved that for m−l ≥ 2, MTTDLsym
k
is increasing in k.
It can also be proved that for any m−l ≥ 1, EAFDLsym
k
is not
increasing in k. Consequently, within the class of symmetric
placement schemes considered, that is, for l+1 < m < k ≤ n,
the MTTDLsym
k
is maximized and the EAFDLsym
k
is minimized
by the declustered placement scheme, that is, when k = n.
5
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

B. Clustered Placement
In the clustered placement scheme, the n devices are
divided into disjoint sets of m devices, referred to as clusters.
According to the clustered placement, each codeword is stored
across the devices of a particular cluster. At each exposure
level u, the rebuild process recovers one of the u symbols that
each of the Cu most-exposed codewords has lost by reading
m − ˜r + 1 of the remaining symbols. Note that the remaining
symbols are stored on the m − u surviving devices in the
affected group. According to [23, Equation (53)], it holds that
˜nclus
u
= m − u .
(24)
In the case of clustered placement, the rebuild process
recovers the lost symbols by reading l symbols from l of the
˜nu surviving devices of the affected cluster. In the absence of
a network rebuild bandwidth constraint, the symbols are read
at a rate of b from each of the l devices, such that the effective
network rebuild bandwidth is equal to Beff = l b. Subsequently,
the lost symbols are computed on-the-ﬂy and written to a spare
device at a rate of Beff/l = b. Consequently, it holds that
bclus
u (∞) = b ,
u = 1, . . . , ˜r − 1 .
(25)
In the presence though of a network rebuild bandwidth con-
straint, Bmax, the effective network rebuild bandwidth is equal
to Beff = min(l b, Bmax), which implies that the lost symbols
are written to a spare device at a rate of Beff/l. Thus, the
rebuild rate bu is given as a function of Bmax by
bclus
u (Bmax) = Beff(Bmax)
l
= min(l b, Bmax)
l
= min(l, Nb) b
l
,
for u = 1, . . . , ˜r − 1 . (26)
Substituting (25) and (26) into (15) yields
θ clus =
min(l, Nb)
l
˜r−1
.
(27)
As l < m, it holds that min(l, Nb) = min(min(l, m), Nb) =
min(min(Nb, m), l)=min(m min(Nb/m, 1), l)=min(mφ, l),
where, analogously to (20), and with k = m,
φ ≜ min
Nb
m , 1

(7)
= min
Bmax
m b , 1

, where 0 ≤ φ ≤ 1 .
(28)
Consequently, (27) yields
θ clus = min
m
l φ , 1
˜r−1
.
(29)
Remark 6: From (29), it follows that for m φ/l ≥ 1 or,
equivalently, for φ ≥ seff = l/m, θ clus is equal to one, which
implies that the bandwidth constraint does not affect the system
reliability.
Using (3) and (29), and the fact that MTTDL(∞) and
EAFDL(∞) are given by [23, Equations (56) and (57)],
respectively, (14) yields
MTTDLclus(Bmax) ≈
1
n λ
min(m φ, l) b
l λ c
m−l
1

0
0.2
0.4
0.6
0.8
1
10
−40
10
−30
10
−20
10
−10
10
0
Bandwidth Constraint Factor (φ)
Reliability Reduction Factor (θ)
 
 
˜r = 2
˜r = 4
˜r = 8
˜r = 16
˜r = 32
(a) k = 40
0
0.2
0.4
0.6
0.8
1
10
−40
10
−30
10
−20
10
−10
10
0
Bandwidth Constraint Factor (φ)
Reliability Reduction Factor (θ)
 
 
˜r = 2
˜r = 4
˜r = 8
˜r = 16
˜r = 32
˜r = 64
(b) k = 100
0
0.2
0.4
0.6
0.8
1
10
−40
10
−30
10
−20
10
−10
10
0
Bandwidth Constraint Factor (φ)
Reliability Reduction Factor (θ)
 
 
˜r = 2
˜r = 4
˜r = 8
˜r = 16
˜r = 32
˜r = 64
˜r = 128
(c) k = 200
Figure 4.
Reliability reduction factor vs. bandwidth constraint factor for various values of ˜r; symmetric placement.
0
0.2
0.4
0.6
0.8
1
10
−40
10
−30
10
−20
10
−10
10
0
Bandwidth Constraint Factor (φ)
Reliability Reduction Factor (θ)
 
 
˜r = 2
˜r = 4
˜r = 8
˜r = 16
˜r = 32
(a) m = 40
0
0.2
0.4
0.6
0.8
1
10
−40
10
−30
10
−20
10
−10
10
0
Bandwidth Constraint Factor (φ)
Reliability Reduction Factor (θ)
 
 
˜r = 2
˜r = 4
˜r = 8
˜r = 16
˜r = 32
˜r = 64
(b) m = 100
0
0.2
0.4
0.6
0.8
1
10
−40
10
−30
10
−20
10
−10
10
0
Bandwidth Constraint Factor (φ)
Reliability Reduction Factor (θ)
 
 
˜r = 2
˜r = 4
˜r = 8
˜r = 16
˜r = 32
˜r = 64
˜r = 128
(c) m = 200
Figure 5.
Reliability reduction factor vs. bandwidth constraint factor for various values of ˜r; clustered placement.
In particular, we consider a system containing 120 devices
under a declustered placement scheme (k = n = 120), which
according to Remark 5 is the optimal one within the class
of symmetric schemes. The amount of user data stored, U, is
determined by the storage efﬁciency, seff, via (2). As discussed
in Section II-E, the analytical reliability results obtained are
accurate when the storage devices are highly reliable, that
is, when the ratio λ/µ of the mean rebuild time 1/µ to
the mean time to failure of a device 1/λ is very small.
We proceed by considering systems for which it holds that
λ/µ = λ c/b = 0.001.
The combined effect of the network rebuild bandwidth con-
straint and the system efﬁciency on the normalized λ MTTDL
measure is obtained by (32) and shown in Figure 6 as a func-
tion of the codeword length. In particular, when the codeword
length is equal to the system size (m = k = n), the placement
becomes clustered and the normalized λ MTTDL measure is
obtained by (30). Three cases for the network rebuild band-
width constraint were considered: φ = 1 corresponds to the
case where there is no network rebuild bandwidth constraint
given that Nb ≥ k = 120 or, equivalently, Bmax ≥ k b = 120 b;
φ = 0.1 and φ = 0.01 correspond to the cases where
Nb = 0.1 k = 12 and Nb = 0.01k = 1.2 or, equivalently,
Bmax = 0.1 k b=12 b and Bmax = 0.01 k b=1.2 b, respectively.
The values for the storage efﬁciency are chosen to be fractions
of the form z/(z +1), z = 1, . . . , 7, such that the ﬁrst point of
each of the corresponding curves is associated with the single-
parity (z, z +1)-erasure code, and the second point of each of
the corresponding curves is associated with the double-parity
(2z, 2z + 2)-erasure code.
For all values of φ considered, we observe that the MTTDL
increases as the storage efﬁciency seff decreases. This is
because, for a given m, decreasing seff implies decreasing l,
which in turn implies increasing the parity symbols m − l
and consequently improving the MTTDL. Furthermore, for a
given storage efﬁciency, seff, the MTTDL decreases by orders
of magnitude as the maximum network rebuild bandwidth
decreases. We now proceed to identify the optimal codeword
length, m∗, that maximizes the MTTDL for a given bandwidth
constraint and storage efﬁciency. The optimal codeword length
is dictated by two opposing effects on reliability. On the one
hand, larger values of m imply that codewords can tolerate
more device failures, but on the other hand, they result in a
higher exposure degree to failure as each of the codewords
is spread across a larger number of devices. In Figure 6,
the optimal values, m∗, are indicated by the circles, and the
corresponding codeword lengths are indicated by the vertical
dotted lines. By comparing Figures 6(a), (b), and (c), we
deduce that as φ decreases, so do the optimal codeword
lengths. For example, in the case of seff = 3/4 and φ = 1,
the maximum MTTDL value of 4×1078 is obtained when
m = m∗ = 92. However, in the case of φ = 0.1, the maximum
MTTDL value of 6×1057 is obtained for m∗ = 84. The reason
for the reduction of the optimal codeword length is due to
the fact that for a given value of seff and as m increases,
so does ˜r, which, according to Remark 3, results in a smaller
reliability reduction factor. Thus, the reliability reduction factor
corresponding to m = 92 is smaller than the one corresponding
to m = 84, which in turn causes the MTTDL for m = 92 to
no longer be optimal as it becomes smaller than the one for
m = 84. Note that for m = 84 and seff = 3/4, from (1) and
(3), it follows that l = 63 and ˜r−1 = 21. From (21), and given
7
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

0
20
40
60
80
100
120
10
0
10
50
10
100
10
150
10
200
10
250
m
λ MTTDL
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(a) φ = 1
0
20
40
60
80
100
120
10
0
10
50
10
100
10
150
10
200
10
250
m
λ MTTDL
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(b) φ = 0.1
0
20
40
60
80
100
120
10
0
10
50
10
100
10
150
10
200
10
250
m
λ MTTDL
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(c) φ = 0.01
0
20
40
60
80
100
120
10
0
10
50
10
100
10
150
10
200
10
250
m
λ MTTDL
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(d) φ = 0.001
Figure 6.
Normalized MTTDL vs. codeword length for seff = 1/2, 2/3, 3/4, 4/5, 5/6, 6/7, and 7/8; n = k = 120, λ/µ = 0.001.
0
20
40
60
80
100
120
10
−250
10
−200
10
−150
10
−100
10
−50
10
0
m
EAFDL / λ
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(a) φ = 1
0
20
40
60
80
100
120
10
−250
10
−200
10
−150
10
−100
10
−50
10
0
m
EAFDL / λ
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(b) φ = 0.1
0
20
40
60
80
100
120
10
−250
10
−200
10
−150
10
−100
10
−50
10
0
m
EAFDL / λ
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(c) φ = 0.01
0
20
40
60
80
100
120
10
−250
10
−200
10
−150
10
−100
10
−50
10
0
m
EAFDL / λ
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(d) φ = 0.001
Figure 7.
Normalized EAFDL vs. codeword length for seff = 1/2, 2/3, 3/4, 4/5, 5/6, 6/7, and 7/8; n = k = 120, λ/µ = 0.001.
that u ≤ ˜r − 1 = 21 ≪ k = 120, such that φ/(1 − u/k) ≈ φ,
it now follows that θ ≈ φ˜r−1 = 0.121 = 10−21, which implies
that the reliability is reduced by 21 orders of magnitude. In
the cases of φ = 0.01 and φ = 0.001, the maximum MTTDL
values of 6×1037 and 8×1019 are obtained for m∗ = 76 and
m∗ = 68, respectively.
The combined effect of the network rebuild bandwidth
constraint and the system efﬁciency on the normalized
EAFDLdeclus/λ measure is obtained by (31) and (33), and
shown in Figure 7 as a function of the codeword length. We
observe that the EAFDL increases as the storage efﬁciency
seff decreases. Furthermore, for a given storage efﬁciency, seff,
the EAFDL increases by orders of magnitude as the maximum
network rebuild bandwidth decreases. Similarly to the case of
MTTDL, by comparing Figures 7(a), (b), and (c), we observe
that as φ decreases, so do the optimal codeword lengths. For
example, in the case of seff = 3/4 and φ = 1, the minimum
EAFDL value of 4×10−84 is obtained when m = m∗ = 88.
However, in the case of φ = 0.1, the minimum EAFDL value
of 9×10−64 is obtained for m∗ = 80, which implies that the
reliability is reduced by 20 orders of magnitude. In the cases
of φ = 0.01 and φ = 0.001, the minimum EAFDL values
of 2×10−44 and 6×10−27 are obtained for m∗ = 72 and
m∗ = 64, respectively. By comparing Figures 6 and 7, we
deduce that in general the optimal codeword lengths m∗
MTTDL
(for MTTDL) and m∗
EAFDL (for EAFDL) are similar.
Reducing Bmax or, equivalently, φ, affects the optimal
codeword length as follows.
Proposition 1: For any storage efﬁciency seff, and for both
reliability metrics, the optimal codeword length m∗ decreases
as φ decreases.
Proof: Consider two bandwidth constraint factors φ1 and
φ2 with φ1 > φ2. Let m∗
1 and m∗
2 be the corresponding optimal
codeword lengths for the MTTDL metric. We shall now show
that m∗
1 ≥ m∗
2.
As m∗
1 is the optimal codeword length for φ1, it holds
that MTTDL(φ1, m)
≤
MTTDL(φ1, m∗
1) for all m
≥
m∗
1. Also, from (1) and (3), it holds that ˜r
=
(1 −
seff) m + 1, which implies that as m increases, so does
˜r. From (15), it follows that θ(2)/θ(1)
=
Q˜r−1
u=1
bu(φ2)
bu(φ1),
which, owing to the fact that bu(φ2)
≤
bu(φ1)
∀u,
decreases
as
˜r
or,
equivalently,
m
increases.
Conse-
quently, θ(2)
m /θ(1)
m
≤ θ(2)
m∗
1/θ(1)
m∗
1 for all m ≥ m∗
1. Also,
from (14), it follows that MTTDL(φ2,m)/MTTDL(φ1,m)
=
θ(2)
m /θ(1)
m
for
all
values
of
m.
From
the
preced-
ing,
it
follows
that
MTTDL(φ2,m)/MTTDL(φ1,m)
=
θ(2)
m /θ(1)
m ≤ θ(2)
m∗
1/θ(1)
m∗
1 = MTTDL(φ2,m∗
1)/MTTDL(φ1,m∗
1)≤
MTTDL(φ2,m∗
1)/MTTDL(φ1,m) for all m ≥ m∗
1. Thus,
MTTDL(φ2,m) ≤ MTTDL(φ2,m∗
1) for all m ≥ m∗
1, which in
turn implies that m∗
2 ≤ m∗
1. The proof for EAFDL is similar
to that for MTTDL and is therefore omitted.
From (22) and (23), it follows that the optimal codeword
length depends on k and φ, but not on the storage system size,
n. To investigate the behavior of the optimal codeword length,
m∗, as the group size, k, increases, we proceed by considering
the normalized optimal codeword length r∗, namely, the ratio
of m∗ to k:
r∗ ≜ m∗
k
.
(34)
The r∗ values for the MTTDL and EAFDL metrics are shown
in Figures 8 and 9, respectively, for various storage efﬁciencies.
According to Proposition 1, for any storage efﬁciency seff and
for any given group size k, the optimal codeword lengths and,
consequently, the r∗ values decrease as φ decreases. Also,
when the bandwidth constraint factor φ is small, the r∗ values
ﬁrst decrease and then gradually increase as k increases. The
initial decrease is due to the fact that the optimal codeword
length m∗ remains ﬁxed and equal to z + 1, which is the
minimum possible codeword length for the storage efﬁciency
fractions z/(z + 1), z = 1, . . . , 7. For example, in the case
of seff = 7/8 and φ = 0.001, m∗ = 8 for k < 115 in the
case of MTTDL, or for k < 90 in the case of EAFDL, as
8
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

0
20
40
60
80
100
120
0
0.2
0.4
0.6
0.8
1
Group Size (k)
r*
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(a) φ = 1
0
20
40
60
80
100
120
0
0.2
0.4
0.6
0.8
1
Group Size (k)
r*
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(b) φ = 0.1
0
20
40
60
80
100
120
0
0.2
0.4
0.6
0.8
1
Group Size (k)
r*
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(c) φ = 0.01
0
20
40
60
80
100
120
0
0.2
0.4
0.6
0.8
1
Group Size (k)
r*
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(d) φ = 0.001
Figure 8.
r∗ for MTTDL vs. group size for seff = 1/2, 2/3, 3/4, 4/5, 5/6, 6/7, and 7/8; λ/µ = 0.001.
0
20
40
60
80
100
120
0
0.2
0.4
0.6
0.8
1
Group Size (k)
r*
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(a) φ = 1
0
20
40
60
80
100
120
0
0.2
0.4
0.6
0.8
1
Group Size (k)
r*
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(b) φ = 0.1
0
20
40
60
80
100
120
0
0.2
0.4
0.6
0.8
1
Group Size (k)
r*
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(c) φ = 0.01
0
20
40
60
80
100
120
0
0.2
0.4
0.6
0.8
1
Group Size (k)
r*
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(d) φ = 0.001
Figure 9.
r∗ for EAFDL vs. group size for seff = 1/2, 2/3, 3/4, 4/5, 5/6, 6/7, and 7/8; λ/µ = 0.001.
shown in Figures 8(d) and 9(d), respectively. However, it can
be proved that as k increases further, the r∗ values for MTTDL
and EAFDL approach a common value that depends only on
the storage efﬁciency, seff, but not on the bandwidth constraint
factor, φ, and are in the interval [e−1/2 = 0.606, 0.648].
V.
DISCUSSION
Although erasure coding schemes provide a high data
reliability at a high storage efﬁciency, the rebuild process
involves I/O operations and network transfers that increase
the consumption of device and network bandwidth. In par-
ticular, large MDS codes pose a challenge on the usage of
network resources given that a lost symbol is recovered via an
(m, l) erasure code through the transfer of a large number
of l symbols from l surviving devices over the network.
Consequently, recovering large amounts of data results in
additional trafﬁc over increased time periods, which has an
impact on the latency of the foreground workload and therefore
affects system performance. This issue, also known as the
repair bandwidth problem, has prompted the development of
alternative erasure coding schemes that aim at reducing the
amount of data transferred over the storage network during
reconstruction (see [35][36] and references therein). They can,
however, result in higher amounts of data being read from the
surviving devices and therefore in longer rebuild times. The
effect of these methods on system reliability is beyond the
scope of this paper and is a subject of further investigation.
The analytical ﬁndings of this work are relevant for the
case of large data centers employing erasure coding where the
excessive rebuild trafﬁc competes with the huge amount of
trafﬁc generated by the frequent access of a large number of
storage devices. To ensure a desired performance level, the
network bandwidth devoted to the repair trafﬁc needs to be
contained. For small values of φ and k, a small codeword
length should be selected, as discussed in Section IV. For large
values of k, the codeword length should still be kept relatively
small for performance reasons. This is in agreement with
the practical values given in [36] for the various parameters
considered. In particular, to keep the storage overhead low,
the storage efﬁciency should be chosen in the range of 0.66 to
0.75.
VI.
CONCLUSIONS
Data storage systems use erasure coding schemes to recover
lost data and enhance system reliability. Network rebuild band-
width constraints, however, may degrade reliability. A general
methodology was applied for deriving the Mean Time to Data
Loss (MTTDL) and the Expected Annual Fraction of Data
Loss (EAFDL) reliability metrics analytically. Closed-form
expressions capturing the effect of a network rebuild band-
width constraint were obtained for the symmetric, clustered
and declustered data placement schemes. We established that
the reliability of storage systems is adversely affected by the
network rebuild bandwidth constraints. The declustered place-
ment scheme was found to offer superior reliability in terms
of both metrics. An investigation of the reliability achieved
by this scheme under various codeword conﬁgurations was
subsequently conducted. The results obtained demonstrated
that both metrics are optimized by similar codeword lengths.
For large storage systems that use a declustered placement
scheme, the optimized codeword lengths are about 60% of
the storage system size, independently of the network rebuild
bandwidth constraints. The analytical reliability expressions
derived can be used to identify redundancy and recovery
schemes, as well as data placement conﬁgurations that can
achieve high reliability. The results obtained can also be
used to adapt the data placement schemes when the available
network rebuild bandwidth or the number of devices in the
system changes so that the system maintains a high level of
reliability.
Extending the methodology developed to derive the reli-
ability of erasure coded systems under bandwidth constraints
9
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

for arbitrary rebuild time distributions and in the presence of
unrecoverable latent errors is a subject of further investigation.
Also, owing to the parallelism of the rebuild process, the model
considered yields very small rebuild times for large system
sizes. Taking into account the fact that the rebuild times cannot
be smaller than the actual failure detection times requires a
more sophisticated modeling effort, which is also part of future
work.
REFERENCES
[1]
D. A. Patterson, G. Gibson, and R. H. Katz, “A case for redundant arrays
of inexpensive disks (RAID),” in Proceedings of the ACM SIGMOD
International Conference on Management of Data, Jun. 1988, pp. 109–
116.
[2]
P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson,
“RAID: High-performance, reliable secondary storage,” ACM Comput.
Surv., vol. 26, no. 2, Jun. 1994, pp. 145–185.
[3]
M. Malhotra and K. S. Trivedi, “Reliability analysis of redundant arrays
of inexpensive disks,” J. Parallel Distrib. Comput., vol. 17, Jan. 1993,
pp. 146–151.
[4]
W. A. Burkhard and J. Menon, “Disk array storage system reliability,”
in Proceedings of the 23rd International Symposium on Fault-Tolerant
Computing, Jun. 1993, pp. 432–441.
[5]
K. S. Trivedi, Probabilistic and Statistics with Reliability, Queueing and
Computer Science Applications, 2nd ed.
New York: Wiley, 2002.
[6]
Q. Xin, E. L. Miller, T. J. E. Schwarz, D. D. E. Long, S. A. Brandt, and
W. Litwin, “Reliability mechanisms for very large storage systems,” in
Proceedings of the 20th IEEE/11th NASA Goddard Conference on Mass
Storage Systems and Technologies (MSST), Apr. 2003, pp. 146–156.
[7]
T. J. E. Schwarz, Q. Xin, E. L. Miller, D. D. E. Long, A. Hospodor,
and S. Ng, “Disk scrubbing in large archival storage systems,” in
Proceedings of the 12th Annual IEEE/ACM International Symposium
on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), Oct. 2004, pp. 409–418.
[8]
Q. Lian, W. Chen, and Z. Zhang, “On the impact of replica placement to
the reliability of distributed brick storage systems,” in Proc. 25th IEEE
International Conference on Distributed Computing Systems (ICDCS),
Jun. 2005, pp. 187–196.
[9]
S. Ramabhadran and J. Pasquale, “Analysis of long-running replicated
systems,” in Proc. 25th IEEE International Conference on Computer
Communications (INFOCOM), Apr. 2006, pp. 1–9.
[10]
B. Eckart, X. Chen, X. He, and S. L. Scott, “Failure prediction models
for proactive fault tolerance within storage systems,” in Proceedings
of the 16th Annual IEEE International Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Sep. 2008, pp. 1–8.
[11]
A. Thomasian and M. Blaum, “Higher reliability redundant disk arrays:
Organization, operation, and coding,” ACM Trans. Storage, vol. 5, no. 3,
Nov. 2009, pp. 1–59.
[12]
K. Rao, J. L. Hafner, and R. A. Golding, “Reliability for networked
storage nodes,” IEEE Trans. Dependable Secure Comput., vol. 8, no. 3,
May 2011, pp. 404–418.
[13]
I. Iliadis, R. Haas, X.-Y. Hu, and E. Eleftheriou, “Disk scrubbing versus
intradisk redundancy for RAID storage systems,” ACM Trans. Storage,
vol. 7, no. 2, Jul. 2011, pp. 1–42.
[14]
V. Venkatesan, I. Iliadis, C. Fragouli, and R. Urbanke, “Reliability of
clustered vs. declustered replica placement in data storage systems,” in
Proceedings of the 19th Annual IEEE/ACM International Symposium
on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), Jul. 2011, pp. 307–317.
[15]
V. Venkatesan, I. Iliadis, and R. Haas, “Reliability of data storage
systems under network rebuild bandwidth constraints,” in Proceedings
of the 20th Annual IEEE International Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Aug. 2012, pp. 189–197.
[16]
V. Venkatesan and I. Iliadis, “A general reliability model for data
storage systems,” in Proceedings of the 9th International Conference
on Quantitative Evaluation of Systems (QEST), Sep. 2012, pp. 209–
219.
[17]
J.-F. Pˆaris, T. J. E. Schwarz, A. Amer, and D. D. E. Long, “Highly
reliable two-dimensional RAID arrays for archival storage,” in Pro-
ceedings of the 31st IEEE International Performance Computing and
Communications Conference (IPCCC), Dec. 2012, pp. 324–331.
[18]
V. Venkatesan and I. Iliadis, “Effect of codeword placement on the
reliability of erasure coded data storage systems,” in Proceedings of the
10th International Conference on Quantitative Evaluation of Systems
(QEST), Sep. 2013, pp. 241–257.
[19]
I. Iliadis and V. Venkatesan, “An efﬁcient method for reliability evalu-
ation of data storage systems,” in Proceedings of the 8th International
Conference on Communication Theory, Reliability, and Quality of
Service (CTRQ), Apr. 2015, pp. 6–12.
[20]
——, “Most probable paths to data loss: An efﬁcient method for
reliability evaluation of data storage systems,” Int’l J. Adv. Syst.
Measur., vol. 8, no. 3&4, Dec. 2015, pp. 178–200.
[21]
S. Caron, F. Giroire, D. Mazauric, J. Monteiro, and S. P´erennes, “P2P
storage systems: Study of different placement policies,” Peer-to-Peer
Networking and Applications, Mar. 2013, pp. 1–17.
[22]
I. Iliadis and V. Venkatesan, “Expected annual fraction of data loss as a
metric for data storage reliability,” in Proceedings of the 22nd Annual
IEEE International Symposium on Modeling, Analysis, and Simulation
of Computer and Telecommunication Systems (MASCOTS), Sep. 2014,
pp. 375–384.
[23]
——, “Reliability assessment of erasure coded systems,” in Proceed-
ings of the 10th International Conference on Communication Theory,
Reliability, and Quality of Service (CTRQ), Apr. 2017, pp. 41–50.
[24]
——, “Reliability evaluation of erasure coded systems,” Int’l J. Adv.
Telecommun., vol. 10, no. 3&4, Dec. 2017, pp. 118–144.
[25]
J. G. Elerath and J. Schindler, “Beyond MTTDL: A closed-form RAID
6 reliability equation,” ACM Trans. Storage, vol. 10, no. 2, Mar. 2014,
pp. 1–21.
[26]
I. Iliadis and V. Venkatesan, “Rebuttal to ‘Beyond MTTDL: A closed-
form RAID-6 reliability equation’,” ACM Trans. Storage, vol. 11, no. 2,
Mar. 2015, pp. 1–10.
[27]
“Amazon
Simple
Storage
Service.”
[Online].
Available:
http://aws.amazon.com/s3/ [retrieved: November 2017]
[28]
D. Borthakur et al., “Apache Hadoop goes realtime at Facebook,”
in Proceedings of the ACM SIGMOD International Conference on
Management of Data, Jun. 2011, pp. 1071–1080.
[29]
R. J. Chansler, “Data availability and durability with the Hadoop
Distributed File System,” ;login: The USENIX Association Newsletter,
vol. 37, no. 1, 2013, pp. 16–22.
[30]
K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop
Distributed File System,” in Proceedings of the 26th IEEE Symposium
on Mass Storage Systems and Technologies (MSST), May 2010, pp.
1–10.
[31]
D. Ford et al., “Availability in globally distributed storage systems,”
in Proceedings of the 9th USENIX Symposium on Operating Systems
Design and Implementation (OSDI), Oct. 2010, pp. 61–74.
[32]
C. Huang et al., “Erasure coding in Windows Azure Storage,” in
Proceedings of the USENIX Annual Technical Conference (ATC), Jun.
2012, pp. 15–26.
[33]
S. Muralidhar et al., “f4: Facebook’s Warm BLOB Storage System,”
in Proceedings of the 11th USENIX Symposium on Operating Systems
Design and Implementation (OSDI), Oct. 2014, pp. 383–397.
[34]
“IBM Cloud Object Storage.” [Online]. Available: www.ibm.com/
cloud-computing/products/storage/object-storage/how-it-works/
[re-
trieved: November 2017]
[35]
A. G. Dimakis, K. Ramchandran, Y. Wu, and C. Suh, “A survey on
network coding for distributed storage,” Proc. IEEE, vol. 99, no. 3,
Mar. 2011, pp. 476–489.
[36]
M. Zhang, S. Han, and P. P. C. Lee, “A simulation analysis of
reliability in erasure-coded data centers,” in Proceedings of the 36th
IEEE Symposium on Reliable Distributed Systems (SRDS), Sep. 2017,
pp. 144–153.
10
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-629-3
CTRQ 2018 : The Eleventh International Conference on Communication Theory, Reliability, and Quality of Service

