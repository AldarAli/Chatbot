Activity Recognition Using Wearable Sensors for
Healthcare
Annapurna Soumya Evani∗, Bharadwaj Sreenivasan†, Joshi Shruti Sudesh‡, Monika Prakash§,
Jyotsna Bapat¶
International Institute Of Information Technology, Bangalore, India
{annapurna.soumya∗,bharadwaj.s†,Shruti.Joshi‡, Monika.P§}@iiitb.org
jbapat@iiitb.ac.in¶
Abstract—Wireless
Sensor
Networks
(WSNs)
formed
by
wireless sensors can be designed to interact continuously with
the real world. WSNs help in communicating accurate real
time information and have proliferated from the areas of
industrial processing and environmental monitoring to science
and healthcare. Today’s healthcare systems are facing two
serious challenges: rapid growth in adult population and severe
nursing shortage. As per United Nations (UN) statistics, the
number of people in the world aged 60 years and above are
expected to increase from 605 million to 2 billion by 2050.
Nursing shortage has been identiﬁed as a global crisis since 2002
and India has a nurse density per thousand population of 0.80.
These statistics suggest a urgent need for automated monitoring
systems to aid the healthcare industry. In this paper, we describe
the implementation of a patient activity monitoring system using
wearable ﬂex sensors. Patient activities are classiﬁed as standing,
sitting, or walking using a lightweight, low latency algorithm.
Our approach features a unique sensing technique and facilitates
a cost eﬀective and energy eﬃcient solution for healthcare.
Keywords—Healthcare; activity recognition; ﬂex sensor; wireless
sensor networks.
I.
Introduction
A. Motivation
The increase in the number of people availing healthcare
services has stressed the need for a larger nursing pool.
According to global statistics, the demand for registered nurses
is expected to grow from 2 million to 3.2 million between 2008
and 2018, a 60% increase [1]. In the Indian healthcare sector,
there is a shortage of 350,000 nurses, with a nurse to patient
ratio of 1:1205 [2]. As per World Health Organization (WHO),
India is ranked 52 out of 57 countries facing Human Resources
for Health (HRH) crisis [3]. With this global shortage, patient
care is expected to be compromised. There have been reports
of nurses being stressed and overworked resulting in signiﬁcant
attrition. In such a grave scenario, it is necessary to think
of alternatives for taking care of the elderly population. One
approach is to reduce the need for around-the-clock care
givers and automate the monitoring processes in the healthcare
industry.
B. Background
Applications of WSNs (Wireless Sensor Networks) in
healthcare has been an area of interest in recent years. These
applications require collecting large data sets from multiple
sensors and manipulating the context to ﬁnd the patterns per-
taining to the medical aspect being monitored. The parameters
such as security, privacy, user friendliness, scalability, context-
awareness are critical in designing healthcare monitoring appli-
cation [4]. There have been signiﬁcant contributions in the area
of vital sign monitoring in hospitals by providing assistance to
patients with declining sensory and motor capabilities, at-home
assistance to patients suﬀering from Alzheimer’s disease, and
depression for the elderly population and many more [5]. The
sensors in smart health systems are capable of sensing heart
activity, temperature, brain activity, glucose levels, behavioral
patterns, etc. [6]. Researchers in computer, networking, and
medical ﬁelds are working together to create intelligent health-
care monitoring systems. In the future, personal area network
technologies such as radio frequency identiﬁcation (RFID),
Bluetooth, ZigBee, and wireless sensor networks are expected
to work together with infrastructure based networks to provide
context-aware applications [4].
In this paper, we propose an approach in-line with one of
the current on-going research interests and design principles,
i.e., activity detection for patients. Activities such as sitting,
standing, and walking are recognized by the system using
wearable sensor. This activity data can be further analyzed to
know whether the patient is healthy and is following his/her
routine day-to-day activities without constant vigilance by a
real person. Also, it is possible to detect inactivity, which
can be treated as an abnormal behavior and an alarm can be
triggered alerting concerned people.
C. State of the Art
Existing activity recognition systems can be broadly clas-
siﬁed into video sensor-based activity recognition and physical
sensor based activity recognition [7]. Video sensor-based activ-
ity recognition is a traditional method, which was extensively
used [8][9][10][11]. It involves continuous capture of human
activities through cameras; however, the video method raises
privacy issues and requires the individual being monitored to
either remain within the vicinity of the camera or to have a
video recorder attached onto the body [12]. Moreover, the ex-
traction of features from the captured images requires complex
computations. Due to these limitations, wearable sensor based
activity recognition, which requires less data processing (phys-
ical sensor based activity recognition) is recommended. Inertial
sensors - accelerometer and gyroscope are the most widely
used body worn sensors for activity recognition [12][13][14].
Although these sensors are accurate in monitoring activities,
the proposed solutions using them are complex and expensive.
To overcome these challenges, we propose a novel method
of human activity recognition using a low weight wearable ﬂex
173
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-296-7
SENSORCOMM 2013 : The Seventh International Conference on Sensor Technologies and Applications

sensor. This low cost high resistance sensor enables the product
to be an aﬀordable solution in detecting patient activities in
hospitals.
This paper is organized as follows: Section II discusses
System Architecture comprising System Design and Operation.
Section III describes Implementation Details explaining the
Algorithm of the system. Section IV explains the Experimental
Setup and Results. Sections V and VI discusses the conclusion
and future scope, respectively.
II.
System Architecture
A. Overview
The posture recognition system consists of three main
components: the Flex sensor, the Controller, and the Wireless
Transceiver. The ﬂex sensor is placed on the kneecap of the
patient. Based on readings from the ﬂex sensor, the controller
can make intelligent decisions about the posture of the patient.
This information is transferred to a central base station, which
collates the results from diﬀerent controllers. This allows
a centralized database to be created for all patients using
the posture recognition system. The doctor or nurse has the
freedom of monitoring several patients with the help of a
centralized architecture. The patients can conduct their day-
to-day activities without being deprived of their privacy.
B. System Design
The Project uses an IRIS - XM210 mote with an
MDA100CB sensor board. The Sensor board hosts the ﬂex
sensor as shown in Figure 1.The ﬂex Sensor is a resistive
device used to detect the ﬂexing of the entity hosting it. The
Fig. 1. Sensor with the sensor mote
speciﬁcations of the sensor are shown in Table I.
TABLE I. Flex Sensor Speciﬁcations
Manufacturer
Spectra Symbol
Length (inches)
4.5
Flat Resistance
10KΩ
Resistance Tolerance
± 30%
Bend Resistance Range
60KΩ - 110KΩ
Placement
Knee
Fig. 2. System Architecture
C. System Operation
The block diagram of the system is shown in Figure 2.
The operation of the system can be broadly classiﬁed as a
three phase phenomenon - Sensor data collection, In-network
processing and Activity display.
Sensor data collection
The ﬂex sensor’s resistance increases linearly with the bend
of the sensor. In order to measure the voltage across the sensor,
a voltage divider circuit is constructed as shown in Figure 3.
Fig. 3. Flex sensor circuit
Vout =
R ∗ Vin
R + R flex
(1)
The output Vout is given by equation (1). The 10 bit
Analog to Digital Converter (ADC) present in the IRIS mote
is programmed to sample the raw sensor data (Vout) at regular
intervals. The digitized sample values are considered for fur-
ther processing in the sensor node.
In-network processing
This phase is the most important phase in sensor networks.
To minimize the energy expenditure, radio transmissions must
be reduced by local processing of the data. The sampled values
are processed using a threshold technique to determine the
activity of the patient. The number of transmissions is reduced
174
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-296-7
SENSORCOMM 2013 : The Seventh International Conference on Sensor Technologies and Applications

by detecting only the change in the posture of the patient,
which is then communicated. In order to know whether a node
is alive, the posture of the person is sent to the base station
with a longer periodic interval, regardless of the change in
state of the person.
Activity Display
The base station is a ZigBee-compliant device, which
operates at 2.4 GHz range. The base station sends the received
packets to the central coordinator over a serial link. The central
coordinator hosts the application, which interprets information
from the received packets and displays the current activity of
the patient in a GUI. Each packet is tagged with a 2-byte
Mote ID, which is unique for a given sensor mote. By using
the Mote ID, the base station diﬀerentiates data from diﬀerent
motes and generates activity history of every patient.
III.
Implementation Details
The posture classiﬁcation algorithm primarily classiﬁes the
states into a static or a dynamic state. Sitting and standing falls
into the category of a static state, whereas walking and activity
transitions are part of the dynamic state. The algorithm is
maintained asymptotically less complex, as energy consumed
for processing is lower compared to transmitting.
Another important parameter considered during the imple-
mentation of the algorithm is response time. The ﬁnite size
of the memory used in storing the samples is a constraint
in achieving the required accuracy in real time applications;
however, the larger the memory, greater the delay incurred in
processing. To maintain this trade-oﬀ between accuracy and
delay, optimal memory size is chosen.
The ﬂowchart of the system is shown in Figure 4. The
Fig. 4. Flow Diagram at Sensor Mote
postures can be subdivided into two categories: static and
dynamic. To classify postures as either static or dynamic, we
have created a metric, the Threshold Crossing Rate (TCR).
The TCR is computed based on the rate of oscillation of
voltage signal Vflex, where Vflex is voltage measured across
the series resistor R as shown in Figure 3. The TCR is further
processed to determine between static and dynamic states. It
should be mentioned that the walking speed of the patient does
aﬀect calculation of TCR. After the determination, we further
process the TCR signal to ﬁnd out whether it is one among
the following states: sitting, standing, or walking.
Static postures such as sitting and standing are prone to
noise in the Vflex. This noise is due to the sensitivity of
Vflex to the movements of the patient. In order to remove
such jitter, we compute the stability of Vflex for a window’s
worth of data. After ﬁnding whether the signal is stable, we
use a simple thresholding technique to determine between
the sitting and standing states. Transitions between activities
can cause Vflex to look similar to a walking scenario. This
introduces false states into the decision system and thus can
reduce the accuracy. To remove such transitions, a fourth state
is introduced, which is called the “Null" state. All unknown
oscillations and transitions in Vflex are categorized into this
“Null" state. To accomplish this, we apply a threshold on the
TCR signal, which is also computed empirically.
IV.
Experimental Setup and Results
A. Threshold Determination
As per the algorithm, a threshold is used for classifying
the patient postures. The threshold is determined for all pos-
tures by sampling the sensor values at regular intervals. The
experiment was conducted on 30 people, and the readings were
found to be stable throughout the experimental phase.
The pattern observed for the activities, - sitting, standing,
and walking is shown in Figure 5. The voltage signal shows
the ﬂex sensor readings with respect to time. The system state
signal is encoded as 0, 0.5, and 1, where the values refer to
sitting, standing, and walking, respectively.
The threshold remained relatively constant due to the casing
supporting the ﬂex sensor. It helped in measuring precise
readings from the sensor. It saved extra overhead required for
computation of adaptive threshold, which would have been
mandatory due to ﬂuctuations in readings. The deployment of
the sensor mote for the experiment is as shown in Figures 6
and 7.
Figure 8 and Figure 9 show the display at the base station
used for monitoring the patients.
B. Range, Delay, and Accuracy Measurements
The experiments for range, delay, and accuracy calculations
were conducted in a hospital-like environment. The test envi-
ronment had rooms located on both sides of a long passage.
The base station was located at the extreme end of the passage.
The system yielded good results for both line of sight and
multipath conditions. The activity detection of patients in the
rooms covering the opposite end of the base station was also
successful. The indoor line of sight range was approximately
500 meters. The delay observed over this range was between
2 - 4 seconds.
Position of the sensor also inﬂuence the accuracy of the
system. The ﬂex sensor should be placed on the middle of
175
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-296-7
SENSORCOMM 2013 : The Seventh International Conference on Sensor Technologies and Applications

Fig. 5. Threshold Detection Technique
Fig. 6. Sensor view in sitting posture
the person’s knee cap so that maximum bend is faced by it.
For measuring the success rate, 30 people were asked to per-
form activities like sitting, standing and walking in a random
manner. The percentage of time when the actual posture of the
person conformed with decision from the Activity Recognition
system was used to compute the success rate. We achieved a
success rate of 81% based on the test trails.
V.
Conclusion
The project successfully demonstrated the recognition of
human activities - sitting, standing, and walking with an
Fig. 7. Sensor View In Standing Posture
Fig. 8. GUI at the Central Base station
Fig. 9. GUI Depicting Postures
accuracy of 81% through a resizable ﬂex sensor band worn on
the knee. The developed system is cost eﬀective as only one
sensor is required, and also making a ﬂex sensor is feasible
through inexpensive materials. Adding to the advantage, the
ﬂex sensor is a resistance device that does not require an ex-
ternal power source like active inertial sensors [13]. Also, low
energy consumption is maintained by performing in-network
processing resulting in long life of the product. Finally, the
robust casing and light weight of the product makes it so
comfortable that patients would get a feel of just wearing a
knee band. Thus, this presents a low cost and a low power
solution for remotely monitoring patients in hospitals.
176
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-296-7
SENSORCOMM 2013 : The Seventh International Conference on Sensor Technologies and Applications

VI.
Future Scope
We have proposed a novel activity recognition system using
the ﬂex sensor. To make the system cost eﬀective, we propose
that the ﬂex sensor be constructed using simple inexpensive
materials. The system has the capability of detecting postures
such as walking, sitting, and standing. It can be extended to
detect the fall and the location of the patient using localization
techniques. We also propose that constant human monitoring
at the base station can be eliminated by alerting the care giver
over SMS.
Acknowledgment
This paper was supported by CEEMS Laboratory at the
International Institute of Information Technology, Bangalore
funded by the Government of Karnataka. We are grateful to
Joseph Jeﬀrey for his immense support throughout the phase
of design and conceptualization of the system.
References
[1]
C. A. Holland, N. Ferrell, and D. James. (2009) The nursing
shortage: Exploring the situation and solutions. [retrieved: May,
2013].
[Online].
Available:
http://www.minoritynurse.com/article/
nursing-shortage-exploring-situation-and-solutions
[2]
M. Doshi, “Northbridge capital report on hospital sector,” November
2010,
[retrieved:
June,
2013].
[Online].
Available:
http://www.
northbridgeasia.com/research_reports.aspx
[3]
WHO,
“World
health
organisation:
World
health
statistics
2011,”
2011,
[retrieved:
May,
2013].
[Online].
Available:
http://uhc-india.org/uploads/ThammaRaoD_
HumanResourcesforUniversalHealthCoverage.pdf
[4]
H. Alemdar and C. Ersoy, “Wireless sensor networks for healthcare:
A survey,” Computer Networks, vol. 54, no. 15, pp. 2688 – 2710,
2010. [Online]. Available: http://www.sciencedirect.com/science/article/
pii/S1389128610001398
[5]
P. Neves, M. Stachyra, and J. Rodrigues, “Application of Wireless
Sensor Networks to Healthcare Promotion,” Journal of Communications
Software and Systems (JCOMSS), vol. 4, no. 3, 2008.
[6]
K. Grgi´c, D. Žagar, and V. Križanovi´c, “Medical applications of
wireless sensor networks - current status and future directions,”
Medicinski Glasnik, vol. 9, 1840-0132. [Online]. Available: http:
//hdl.handle.net/10400.11/548
[7]
A. Jehad Sarkar, D. Guan, T. Ma, W. Yuan, and Y.-K. Lee, “Review of
Sensor-based Activity Recognition Systems,” vol. 28, no. 5, 2011, pp.
418–433.
[8]
X. Xu, J. Tang, X. Zhang, X. Liu, H. Zhang, and Y. Qiu, “Exploring
techniques for vision based human activity recognition: Methods,
systems, and evaluation,” Sensors, vol. 13, no. 2, pp. 1635–1650,
2013. [Online]. Available: http://www.mdpi.com/1424-8220/13/2/1635
[9]
P. Turaga, R. Chellappa, V. S. Subrahmanian, and O. Udrea, “Machine
recognition of human activities: A survey,” Circuits and Systems for
Video Technology, IEEE Transactions on, vol. 18, no. 11, pp. 1473–
1488, Nov.
[10]
R. Bodor, B. Jackson, N. Papanikolopoulos, and H. Tracking, “Vision-
based human tracking and activity recognition,” in Proc. of the 11th
Mediterranean Conf. on Control and Automation.
Kostrzewa Joseph,
2003, pp. 18–20.
[11]
C. N. Joseph, S. Kokulakumaran, K. Srijeyanthan, A. Thusyanthan,
C. Gunasekara, and C. Gamage, “A framework for whole-body gesture
recognition from video feeds,” in Industrial and Information Systems
(ICIIS), 2010 International Conference on, 29 2010-Aug. 1, pp. 430–
435.
[12]
O. Lara and M. Labrador, “A survey on human activity recognition using
wearable sensors,” Communications Surveys Tutorials, IEEE, vol. PP,
no. 99, pp. 1–18, 2012, [retrieved: June, 2013].
[13]
H. Junker, O. Amft, P. Lukowicz, and G. Tr
“Gesture spotting
with body-worn inertial sensors to detect user activities,” Pattern
Recognition, vol. 41, no. 6, pp. 2010 – 2024, 2008, [retrieved:
June, 2013]. [Online]. Available: http://www.sciencedirect.com/science/
article/pii/S0031320307005110
[14]
B.
Florentino-Liaño,
N.
O’Mahony,
and
A.
Artés-Rodríguez,
“Hierarchical dynamic model for human daily activity recognition.”
in BIOSIGNALS, S. V. Huﬀel, C. M. B. A. Correia, A. L. N.
Fred, and H. Gamboa, Eds.
SciTePress, 2012, pp. 61–68, [retrieved:
June, 2013]. [Online]. Available: http://dblp.uni-trier.de/db/conf/biostec/
biosignals2012.html#Florentino-LianoOA12
177
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-296-7
SENSORCOMM 2013 : The Seventh International Conference on Sensor Technologies and Applications

