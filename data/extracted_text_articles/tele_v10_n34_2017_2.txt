A Practical Overview of Recursive Least-Squares Algorithms for Echo Cancellation
Camelia Elisei-Iliescu, Constantin Paleologu,
Cristian Stanciu, Cristian Anghel, Silviu Ciochin˘a
University Politehnica of Bucharest, Romania
Email: {pale,cristian,canghel,silviu}@comm.pub.ro
Jacob Benesty
INRS-EMT
University of Quebec, Montreal, Canada
Email: benesty@emt.inrs.ca
Abstract—Due to its fast convergence rate, the recursive least-
squares (RLS) algorithm is very popular in many applications of
adaptive ﬁltering. However, the computational complexity of this
algorithm represents a major limitation in some applications that
involve long ﬁlters, like echo cancellation. Moreover, the speciﬁc
features of this application require good tracking capabilities
and double-talk robustness for the adaptive algorithm, which
further imply an optimization process on its parameters. In the
case of most RLS-based algorithms, the performance can be
controlled in terms of two main parameters, i.e., the forgetting
factor and the regularization term. In this paper, we outline
the inﬂuence of these parameters on the overall performance
of the RLS algorithm and present several solutions to control
their behavior, taking into account the speciﬁc requirements of
echo cancellation application. The resulting variable forgetting
factor RLS (VFF-RLS) and variable-regularized RLS (VR-RLS)
algorithms could represent appealing solutions for real-world
scenarios, as indicated by simulations performed in the context
of both network and acoustic echo cancellation.
Keywords–Adaptive ﬁlters; Echo cancellation; Recursive least-
squares (RLS) algorithm; Variable forgetting factor RLS (VFF-
RLS); Variable regularized RLS (VR-RLS).
I.
INTRODUCTION
The recursive least-squares (RLS) algorithm [1], [2], [3] is
one of the most popular adaptive ﬁlters. As compared to the
normalized least-mean-square (NLMS) algorithm [2], [3], the
RLS offers a superior convergence rate especially for highly
correlated input signals. Of course, there is a price to pay
for this advantage, which is an increase in the computational
complexity. For this reason, it is not very often involved in
echo cancellation [4], [5], where long ﬁlters are required.
In both network and acoustic echo cancellation contexts
[4], [5], the basic principle is to build a model of the echo
path impulse response that needs to be identiﬁed with an
adaptive ﬁlter, which provides at its output a replica of the echo
(that is further subtracted from the reference signal). The main
difference between these two applications is the way in which
the echo arises. In the network (or electrical) echo problem,
there is an unbalanced coupling between the 2-wire and 4-
wire circuits which results in echo, while the acoustic echo
is due to the acoustic coupling between the microphone and
the loudspeaker (e.g., as in speakerphones). However, in both
cases, the adaptive ﬁlter has to model an unknown system,
i.e., the echo path. The system model for echo cancellation is
summarized in Section II.
Even if the formulation of the echo cancellation problem
is straightforward, its speciﬁc features represent a challenge
for any adaptive algorithm. There are several issues associated
with this application, and they are as follows. First, the echo
paths can have excessive lengths in time, e.g., up to hundreds
of milliseconds. Consequently, long length adaptive ﬁlters
are required (hundreds or even thousands of coefﬁcients),
inﬂuencing the convergence rate of the algorithm. Besides, the
echo paths are time-variant systems, requiring good tracking
capabilities for the echo canceller. Second, the echo signal
is combined with the near-end signal; ideally, the adaptive
ﬁlter should separate this mixture and provide an estimate
of the echo at its output as well as an estimate of the near-
end from the error signal. This is not an easy task since the
near-end signal can contain both the background noise and the
near-end speech; the background noise can be non-stationary
and strong while the near-end speech acts like a large level
disturbance. Last but not least, the input of the adaptive ﬁlter
(i.e., the far-end signal) is mainly speech, which is a non-
stationary and highly correlated signal that can inﬂuence the
overall performance of adaptive algorithms.
Different types of adaptive ﬁlters have been involved in the
context of echo cancellation. The RLS-based algorithms would
represent a very appealing choice (especially in terms of the
convergence rate), if the computational complexity issue could
be overcome. In this paper, we provide a practical overview
on several RLS-based algorithms that could be used for echo
cancellation, focusing on their key parameters.
It is well known that the performance of the RLS algorithm
is mainly controlled by two important parameters, i.e., the
forgetting factor and the regularization term. Similar to the
attributes of the step-size from the NLMS-based algorithms,
the performance of RLS-type algorithms in terms of conver-
gence rate, tracking, misadjustment, and stability depends on
the forgetting factor [2], [3]. The classical RLS algorithm
uses a constant forgetting factor (between 0 and 1) and needs
to compromise between the previous performance criteria.
When the forgetting factor is very close to one, the algorithm
achieves low misadjustment and good stability, but its tracking
capabilities are reduced [6]. A small value of the forgetting
factor improves the tracking but increases the misadjustment,
and could affect the stability of the algorithm [7]. Motivated
by these aspects, a number of variable forgetting factor RLS
(VFF-RLS) algorithms have been developed, e.g., [8]–[11]
(and references therein).
It should be mentioned that in the context of system
identiﬁcation (like in echo cancellation), where the output of
the unknown system is corrupted by another signal (which is
usually an additive noise), the goal of the adaptive ﬁlter is
not to make the error signal goes to zero, because this will
96
International Journal on Advances in Telecommunications, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/telecommunications/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

introduce noise in the adaptive ﬁlter. The objective instead is
to recover the “corrupting signal” from the error signal of the
adaptive ﬁlter after this one converges to the true solution. This
was the approach behind the VFF-RLS algorithm proposed in
[10], which is analyzed in Section III.
As compared to the forgetting factor, the regularization
parameter has been less addressed in the literature. Apparently,
it is required in matrix inversion when this matrix is ill con-
ditioned, especially in the initialization stage of the algorithm.
However, its role is of great importance in practice, since reg-
ularization is a must in all ill-posed problems (like in adaptive
ﬁltering), especially in the presence of additive noise [12]–[15].
Consequently, in Section IV, we focus on the regularized RLS
algorithm [3]. Following the development from [13], a method
to select an optimal regularization parameter is presented, so
that the algorithm could behave well in all noisy conditions.
Since the value of this parameter is related to the echo-to-noise
ratio (ENR), a simple and practical way to estimate the ENR in
practice is also presented, which leads to a variable regularized
RLS (VR-RLS) algorithm. Also, a low-complexity version of
the proposed VR-RLS algorithm is developed, based on the
dichotomous coordinate descent (DCD) method [16], [17].
The simulation results (presented in Section V) are per-
formed in the context of both network and acoustic echo
cancellation. The results support the theoretical ﬁndings and
indicate the good performance of these algorithms. Finally, the
conclusions are provided in Section VI.
II.
SYSTEM MODEL FOR ECHO CANCELLATION
In the context of echo cancellation (Figure 1), the micro-
phone or desired signal at the discrete-time index n is
d(n) = xT (n)h + v(n) = y(n) + v(n),
(1)
where
x(n) = [ x(n)
x(n − 1)
· · ·
x(n − L + 1) ]T
(2)
is a vector containing the L most recent time samples of
the zero-mean input (loudspeaker) signal x(n), superscript T
denotes transpose of a vector or a matrix,
h = [ h0
h1
· · ·
hL−1 ]T
(3)
is the impulse response (of length L) of the system (from the
loudspeaker to the microphone) that we need to identify, and
v(n) the zero-mean near-end signal. In case of single-talk (i.e.,
the near-end speech is absent), v(n) can usually be considered
a zero-mean stationary white Gaussian noise signal with the
variance σ2
v = E
[
v2(n)
]
, where E[·] denotes mathematical
expectation. The signal y(n) is called the echo in the context
of echo cancellation that we want to cancel with an adaptive
ﬁlter [4], [5].
Then, our objective is to estimate or identify h with an
adaptive ﬁlter:
bh(n) =
[ bh0(n)
bh1(n)
· · ·
bhL−1(n)
]T ,
(4)
in such a way that for a reasonable value of n, we have for
the (normalized) misalignment:
h − bh(n)

2
2
∥h∥2
2
≤ ι,
(5)
x(n)
far-end 
h
 ( )
n
h
 ( )
y(n)
y n
+
–
near-end 
+
+
e(n)
d(n)
v(n)
Figure 1. General conﬁguration for echo cancellation.
where ι is a predetermined small positive number and ∥·∥2 is
the ℓ2 norm. In this context, the a priori error signal is deﬁned
as
e(n) = d(n) − xT (n)bh(n − 1) = d(n) − by(n),
(6)
where the vector bh(n − 1) contains the adaptive ﬁlter coefﬁ-
cients at time n − 1 and by(n) is the output of the adaptive
ﬁlter.
III.
VARIABLE FORGETTING FACTOR RLS ALGORITHM
The classical RLS algorithm can be immediately deduced
from the normal equations, which are
bRx(n)bh(n) = brdx(n),
(7)
where
bRx(n)
=
n
∑
i=1
λn−ix(i)xT (i)
=
λ bRx(n − 1) + x(n)xT (n),
(8)
brxd(n)
=
n
∑
i=1
λn−ix(i)d(i)
=
λbrxd(n − 1) + x(n)d(n),
(9)
and the parameter λ is the forgetting factor. According to (1),
the normal equations become
n
∑
i=1
λn−ix(i)xT (i)bh(n)
=
n
∑
i=1
λn−ix(i)y(i) +
n
∑
i=1
λn−ix(i)v(i).
(10)
For a value of λ very close to 1 and for a large value of n, it
may be assumed that
1
n
n
∑
i=1
λn−ix(i)v(i) ≈ E [x(n)v(n)] = 0.
(11)
Consequently, taking (10) into account,
n
∑
i=1
λn−ix(i)xT (i)bh(n) ≈
n
∑
i=1
λn−ix(i)y(i)
=
n
∑
i=1
λn−ix(i)xT (i)h,
(12)
97
International Journal on Advances in Telecommunications, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/telecommunications/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

thus bh(n) ≈ h and e(n) ≈ v(n). Now, for a small value of
the forgetting factor, so that λk ≪ 1 for k ≥ n0, it can be
assumed that
n
∑
i=1
λn−i(•) ≈
n
∑
i=n−n0+1
λn−i(•).
According to the orthogonality theorem [2], [3], the normal
equations become
n
∑
i=n−n0+1
λn−ix(i)e(i) = 0L×1,
where 0L×1 denotes a vector with all its L elements equal
to zero. This is a homogeneous set of L equations with n0
unknown parameters, e(i). When n0 < L, this set of equations
has the unique solution e(i) = 0, for i = n − n0 + 1, . . . , n,
leading to by(n) = y(n) + v(n). Consequently, there is a
“leakage” of v(n) into the output of the adaptive ﬁlter. In this
situation, the signal v(n) is cancelled; even if the error signal
is e(n) = 0, this does not lead to a correct solution from the
system identiﬁcation point of view. A small value of λ or a
high value of L intensiﬁes this phenomenon.
Summarizing, for a low value of λ the output of the
adaptive system is by(n) ≈ y(n) + v(n), while λ ≈ 1
leads to by(n) ≈ y(n). Apparently, for a system identiﬁcation
application, a value of λ very close to 1 is desired; but in this
case, even if the initial convergence rate of the algorithm is
satisfactory, the tracking capabilities suffer a lot. In order to
provide fast tracking, a lower value of λ is desired. On the
other hand, taking into account the previous aspects, a low
value of λ is not good in the steady-state. Consequently, a
VFF-RLS algorithm (which could provide both fast tracking
and low misadjustment) can be a more appropriate solution,
in order to deal with these aspects.
Let us start the development by writing the relations that
deﬁne the classical RLS algorithm:
k(n) =
P(n − 1)x(n)
λ + xT (n)P(n − 1)x(n),
(13)
bh(n) = bh(n − 1) + k(n)e(n),
(14)
P(n) = 1
λ
[
P(n − 1) − k(n)xT (n)P(n − 1)
]
,
(15)
where k(n) is the Kalman gain vector, P(n) is the inverse of
the input correlation matrix, and e(n) is the a priori error signal
deﬁned in (6). The a posteriori error signal can be deﬁned using
the adaptive ﬁlter coefﬁcients at time n, i.e.,
ε(n) = d(n) − xT (n)bh(n)
(16)
Using (6) and (14) in (16), it results
ε(n) = e(n)
[
1 − xT (n)k(n)
]
.
(17)
According to the problem statement, it is desirable to recover
the system noise from the error signal. Consequently, it can
be imposed the condition:
E
[
ε2(n)
]
= σ2
v.
(18)
Using (18) in (17) and taking (13) into account, it ﬁnally results
E
{[
1 −
θ(n)
λ(n) + θ(n)
]2}
=
σ2
v
σ2e(n),
(19)
where θ(n) = xT (n)P(n − 1)x(n). In (19), we assumed that
the input and error signals are uncorrelated, which is true when
the adaptive ﬁlter has started to converge to the true solution.
We also assumed that the forgetting factor is deterministic
and time dependent. By solving the quadratic equation (19), it
results a variable forgetting factor
λ(n) =
σθ(n)σv
σe(n) − σv
,
(20)
where E
[
θ2(n)
]
= σ2
θ(n). In practice, the variance of the
error signal is estimated based on
bσ2
e(n) = αbσ2
e(n − 1) + (1 − α)e2(n),
(21)
where α = 1 − 1/(KL), with K ≥ 1. Also, the variance of
θ(n) is evaluated in a similar manner, i.e.,
bσ2
θ(n) = αbσ2
θ(n − 1) + (1 − α)θ2(n).
(22)
The estimate of the noise power, bσ2
v [which should be used
in (20) from practical reasons], can be evaluated in different
ways, e.g., [10], [19], [20].
Theoretically, σe(n) ≥ σv in (20). Compared to the least-
mean-square algorithms [where there is the gradient noise, so
that σe(n) > σv], the RLS algorithm with λ(n) ≈ 1 leads
to σe(n) ≈ σv. In practice (since power estimates are used),
several situations have to be prevented in (20). Apparently,
when bσe(n) ≤ bσv, it could be set λ(n) = λmax, where λmax is
very close or equal to 1. But this could be a limitation, because
in the steady-state of the algorithm bσe(n) varies around bσv. A
more reasonable solution is to impose that λ(n) = λmax when
bσe(n) ≤ ρbσv,
(23)
with 1 < ρ ≤ 2. Otherwise, the forgetting factor of the
proposed VFF-RLS algorithm is evaluated as
λ(n) = min
[
bσθ(n)bσv
ζ + |bσe(n) − bσv|, λmax
]
,
(24)
where the small positive constant ζ prevents a division by zero.
Before the algorithm converges or when there is an abrupt
change of the system, bσe(n) is large as compared to bσv; thus,
the parameter λ(n) from (24) takes low values, providing fast
convergence and good tracking. When the algorithm converges
to the steady-state solution, bσe(n) ≈ bσv [so that the condition
(23) is fulﬁlled] and λ(n) is equal to λmax, providing low mis-
adjustment. The resulted VFF-RLS algorithm is summarized
in Table I. It can be noticed that the mechanism that controls
the forgetting factor is very simple and not expensive in terms
of multiplications and additions.
IV.
VARIABLE REGULARIZED RLS ALGORITHM
In this section, a different version of the RLS algorithm
is presented, which allows us to outline the importance of the
regularization parameter. Let us consider the regularized least-
squares criterion:
J(n) =
n
∑
i=0
λn−i [
d(i) − bhT (n)x(i)
]2
+ δ
bh(n)

2 ,
(25)
where λ is the same exponential forgetting factor and δ is
the regularization parameter. From (25), the update of the
regularized RLS algorithm [3] results in
bh(n) = bh(n − 1) +
[
bRx(n) + δIL
]−1
x(n)e(n),
(26)
98
International Journal on Advances in Telecommunications, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/telecommunications/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE I. VFF-RLS algorithm.
Initialization:
P(0) = γIL (γ > 0)
bh(0) = 0L×1
bσ2
e(0) = bσ2
θ(0) = 0
Parameters:
α = 1 −
1
KL (with K > 1) weighting factor
λmax, upper bound of the forgetting factor (very close or equal to 1)
ζ > 0, very small number to avoid division by zero
bσ2
v, system noise power (estimated)
For time index n = 1, 2, ...:
e(n) = d(n) − xT (n)bh(n − 1)
θ(n) = xT (n)P(n − 1)x(n)
bσ2
e(n) = αbσ2
e(n − 1) + (1 − α)e2(n)
bσ2
θ(n) = αbσ2
θ(n − 1) + (1 − α)θ2(n)
λ(n) =





λmax, if bσe(n) ≤ ρbσv (where 1 < ρ ≤ 2)
min
[
bσθ(n)bσv
ζ+|bσe(n)−bσv| , λmax
]
, otherwise
k(n) = P(n − 1)x(n)
λ(n) + θ(n)
bh(n) = bh(n − 1) + k(n)e(n)
P(n) =
1
λ(n)
[
P(n − 1) − k(n)xT (n)P(n − 1)
]
where the matrix bRx(n) from (8) is an estimate of the
correlation matrix of x(n) at time n, IL is the identity matrix
of size L × L, and e(n) is the a priori error signal deﬁned
in (6). We will assume that the matrix bRx(n) has full rank,
although it can be very ill conditioned. As a result, if there
is no noise, regularization is not really required; however, the
more the noise, the larger should be the value of δ.
Summarizing, the regularized RLS algorithm is deﬁned by
the relations (6), (8), and (26). In the following, we present
one reasonable way to ﬁnd the regularization parameter δ. It
can be noticed that the update equation of the regularized RLS
can be rewritten as [13]
bh(n) = Q(n)bh(n − 1) + eh(n),
(27)
where
Q(n) = IL −
[
bRx(n) + δIL
]−1
x(n)xT (n)
(28)
and
eh(n) =
[
bRx(n) + δIL
]−1
x(n)d(n)
(29)
is the correctiveness component of the algorithm, which de-
pends on the new observation d(n). In this context, we can
notice that Q(n) does not depend on the noise signal and
Q(n)bh(n − 1) in (27) can be seen as a good initialization of
the adaptive ﬁlter. In fact, (29) is the solution of the noisy
linear system of L equations:
[
bRx(n) + δIL
]
eh(n) = x(n)d(n).
(30)
Let us deﬁne
ee(n) = d(n) − ehT (n)x(n),
(31)
the error signal between the desired signal and the estimated
signal obtained from the ﬁlter optimized in (29). Consequently,
we could ﬁnd δ in such a way that the expected value of ee2(n)
is equal to the variance of the noise, i.e.,
E
[
ee2(n)
]
= σ2
v.
(32)
This is reasonable if we want to attenuate the effects of the
noise in the estimator eh(n).
For the sake of simplicity, let us assume that x(n) is
stationary and white. Apparently, this assumption is quite
restrictive, even if it was widely used in many developments in
the context of adaptive ﬁltering [2], [3]. However, the resulting
VR-RLS algorithm will still use the full matrix bRx(n) and,
consequently, it will inherit the good performance feature of
the RLS family in case of correlated inputs. In this case and
for n large enough (also considering that the forgetting factor
λ is on the order of 1 − 1/L), we have
[
bRx(n) + δIL
]
≈
[ σ2
x
1 − λ + δ
]
IL
≈
[
Lσ2
x + δ
]
IL
(33)
and xT (n)x(n) ≈ Lσ2
x, where σ2
x = E
[
x2(n)
]
is the variance
of the input signal. Next, from (1), we can deﬁne the echo-to-
noise ratio (ENR) as
ENR = σ2
y
σ2v
,
(34)
where σ2
y = E
[
y2(n)
]
is the variance of y(n). Developing
(32) and based on the previous approximations, we obtain the
quadratic equation:
δ2 − 2 Lσ2
x
ENRδ −
(
Lσ2
x
)2
ENR
= 0,
(35)
with the obvious solution:
δ = L
(
1 + √1 + ENR
)
ENR
σ2
x
= βσ2
x,
(36)
where
β = L
(
1 + √1 + ENR
)
ENR
(37)
is the normalized regularization parameter of the RLS algo-
rithm.
As we can notice from (36), the regularization parameter
δ depends on three elements, i.e., the length of the adaptive
ﬁlter, the variance of the input signal, and the ENR. In most
applications, the ﬁrst two elements (L and σ2
x) are known,
while the ENR can be estimated. Using a proper evaluation of
the ENR, the algorithm should own good robustness features
against the additive noise.
Let us assume that the adaptive ﬁlter has converged to a
certain degree, so that we can use the approximation
y(n) ≈ by(n).
(38)
99
International Journal on Advances in Telecommunications, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/telecommunications/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Hence,
σ2
y ≈ σ2
by,
(39)
where σ2
by = E
[
by2(n)
]
. Since the output of the unknown
system and the noise can be considered uncorrelated, (1) can
be expressed in terms of power estimates as
σ2
d = σ2
y + σ2
v,
(40)
where σ2
d = E
[
d2(n)
]
. Using (39) in (40), we obtain
σ2
v ≈ σ2
d − σ2
by.
(41)
The power estimates can be evaluated in a recursive manner
[similar to (21) and (22)] as
bσ2
d(n) = αbσ2
d(n − 1) + (1 − α)d2(n),
(42)
bσ2
by(n) = αbσ2
by(n − 1) + (1 − α)by2(n).
(43)
Therefore, based on (39) and (41), an estimation of the ENR
is obtained as
d
ENR(n) =
bσ2
by(n)
|bσ2
d(n) − bσ2
by(n)|,
(44)
so that the variable regularization parameter results in
δ(n) =
L
[
1 +
√
1 + d
ENR(n)
]
d
ENR(n)
σ2
x
= β(n)σ2
x,
(45)
where
β(n) =
L
[
1 +
√
1 + d
ENR(n)
]
d
ENR(n)
(46)
is the variable normalized regularization parameter. Conse-
quently, based on (45), we obtain a variable-regularized RLS
(VR-RLS) algorithm, with the update:
bh(n) = bh(n − 1) +
[
bRx(n) + δ(n)IL
]−1
x(n)e(n),
(47)
where bRx(n) is recursively evaluated according to (8) and
δ(n) is computed based on (42)–(45).
At this point, some practical issues should be outlined.
The absolute values in (44) prevent any minor deviations
(due to the use of power estimates) from the true values,
which can make the denominator negative. The VR-RLS is
a non-parametric algorithm, since all the parameters in (44)
are available. Also, good robustness against the additive noise
variations is expected. The main drawback is due to the
approximation in (39). This assumption will be biased in the
initial convergence phase or when there is a change of the
unknown system. Concerning the initial convergence, we can
use a constant regularization parameter δ in the ﬁrst steps of
the algorithm (e.g., in the ﬁrst L iterations).
However, the VR-RLS algorithm faces two main challenges
in terms of computational complexity. The ﬁrst one is the
update of the matrix bRx(n) from (8), while the second issue
is related to the evaluation of the last term from the right-hand
side of (47), which contains both the matrix inversion and the
product with the input vector.
The complexity of (8) can be greatly reduced taking into
account that the vector x(n) has the time shift property [see
(2)] and the matrix bRx(n) is symmetric. Thus, only the ﬁrst
column of this matrix has to be computed, i.e.,
bR(1)
x (n) = λ bR(1)
x (n − 1) + x(n)x(n),
(48)
since the lower-right (L − 1) × (L − 1) block of bRx(n) can
be obtained by copying the (L − 1) × (L − 1) upper-left block
of the matrix bRx(n − 1).
The evaluation of the last term from the right-hand side
of (47) is more challenging. In fact, the basic problem can be
interpreted in terms of solving the normal equations [3]:
R(n)bh(n) = brxd(n),
(49)
where
R(n) = bRx(n) + δ(n)IL
(50)
and brxd(n) is deﬁned in (9). As an alternative to the classical
approaches [2], [3], the normal equations (49) can be recur-
sively solved using the dichotomous coordinate descent (DCD)
method [16]. The basic idea is to express the problem in terms
of auxiliary normal equations with respect to increments of the
ﬁlter weights [17]. In our case, we need to solve
R(n)△bh(n) = p(n),
(51)
where △bh(n) is the increment of the ﬁlter weights and
p(n) = λr(n − 1) + x(n)e(n),
(52)
with r(n) representing the so-called residual vector associated
to the solution [17]. Consequently, following the previous de-
velopment and the steps presented in [17], the low-complexity
version of the proposed VR-RLS algorithm, namely VR-RLS-
DCD, is summarized in Table II, where step 6 involves the
DCD iterations.
The DCD algorithm [16] is based on coordinate descent
iterations with a power of two variable step-size, q. It does not
need multiplications or divisions (these operations are simply
replaced by bit-shifts), but only additions, so that it is well
suited for hardware implementation. In our case, the auxiliary
normal equations from step 6 are solved by using the DCD
with a leading element [17]. An insightful analysis of this
algorithm can be found in [17]. Also, detailed implementation
aspects are discussed in [18].
Here, we brieﬂy outline some of the important parameters
of the DCD algorithm (using the notation from [17]). First,
the parameters H and Mb represent the maximum amplitude
expected for the values of △bh(n), respectively the number
of bits used for their representation. If the value of H is
chosen accordingly, the values of the step-size q correspond
to the powers of 2 and are associated with the bits comprising
the binary representation of each computed value in the
solution vector. In this case, any multiplication with q can be
replaced by a bit-shift. Second, the parameter Nu represents the
maximum number of allowed (or “successful”) iterations per-
formed for △bh(n) [17]; in practice Nu ≪ L. The arithmetic
complexity of the DCD algorithm is proportional to LNu but
using only additions. Consequently, the complexity associated
to the matrix inversion is greatly reduced as compared to
the classical method [which requires O(L3) operations] and
100
International Journal on Advances in Telecommunications, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/telecommunications/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE II. VR-RLS-DCD algorithm.
Initialization: bh(0) = 0, r(0) = 0, b
Rx(0) = 0L
For n = 1, 2, . . .
Step 1:
b
Rx(n) = λ b
Rx(n − 1) + x(n)xT (n)
[using (48)]
Step 2:
Compute δ(n) based on (42)–(45)
Step 3:
R(n) = b
Rx(n) + δ(n)IL
Step 4:
e(n) = d(n) − bhT (n − 1)x(n)
Step 5:
p(n) = λr(n − 1) + x(n)e(n)
Step 6:
R(n)△bh(n) = p(n) ⇒ △bh(n), r(n)
(to be solved with DCD iterations [17])
Step 7:
bh(n) = bh(n − 1) + △bh(n)
0
20
40
60
80
100
120
−0.04
−0.02
0
0.02
0.04
0.06
Samples
Amplitude
(a)
0
100
200
300
400
500
−0.01
−0.005
0
0.005
0.01
0.015
Samples
Amplitude
(b)
Figure 2. Impulse responses used in simulations.
even to the regular RLS algorithm [2], [3] [which is based
on the matrix inversion lemma and needs O(L2) operations].
Therefore, the DCD-based algorithms are very appealing for
real-world applications.
Nevertheless, the RLS-DCD algorithm proposed in [17]
uses a constant regularization for bRx(0), but the inﬂuence
of this parameter is negligible due to the forgetting factor in
the update of the matrix bRx(n). On the other hand, using
a proper estimation of the regularization parameter within the
algorithm (i.e., steps 2 and 3 in Table II), the robustness against
additive noise can be improved. Thus, the proposed VR-RLS-
DCD algorithm owns this robustness feature, but also the low-
complexity advantage inherited from the DCD method.
V.
SIMULATION RESULTS
First, let us consider a network echo cancellation scenario,
in the framework of G168 Recommendation [21]. The echo
path is depicted in Figure 2(a); it is the fourth impulse
response (of length L = 128) from the above recommendation.
The sampling rate is 8 kHz. All adaptive ﬁlters used in the
experiments have the same length as the echo path. The far-
end signal (i.e., the input signal) is a speech signal. The output
0
2
4
6
8
10
−35
−30
−25
−20
−15
−10
−5
0
5
Time (seconds)
Misalignment (dB)
 
 
RLS, λ = 1 − 1/L
RLS, λ = 1
VFF−RLS
Figure 3. Misalignment of the RLS algorithm (using different constant values
of the forgetting factor) and the VFF-RLS algorithm. The input signal is
speech, L = 128, and ENR = 20 dB. Echo path changes at time 5 seconds.
of the echo path is corrupted by an independent white Gaussian
noise with 20 dB ENR. An echo path change scenario is some
experiments (in order to evaluate the tracking capabilities of
the algorithms), by shifting the impulse response to the right
by 8 samples in the middle of simulation. The performance
measure is the normalized misalignment (in dB) evaluated as
Mis(n) = 20log10
h(n) − bh(n)

2
∥h(n)∥2
.
(53)
In the ﬁrst experiment, the performance of the VFF-RLS
algorithm (presented in Section III) is evaluated, as compared
to the classical RLS algorithm deﬁned in (13)–(15), which uses
different constant values of the forgetting factor. A single-talk
case is considered and the echo path changes in the middle
of simulation. It can be noticed in Figure 3 that the VFF-RLS
algorithm achieves the same initial misalignment as the RLS
with its maximum forgetting factor, but it tracks as fast as
the RLS with the smaller forgetting factor. As expected, the
classical RLS algorithm using constant forgetting factors has
to compromise between these performance criteria, i.e., the
larger the value of λ, the better the misalignment level but
worse the tracking capability.
Next, the performance of the VR-RLS algorithm (from
Section IV) is investigated, as compared to the regularized
RLS algorithm deﬁned by the update (26), using different
constant values of the regularization parameter. Based on
(37), we can determine the values of the optimal normalized
regularization parameter of the RLS algorithm for different
cases; for example, let us consider two values of the ENR, i.e.,
20 dB (the true one) and 0 dB. Using appropriate notation, we
obtain β20 = 14.14 and β0 = 309.01, respectively. Next, we
compare the regularized RLS algorithm using these constant
regularization parameters with the VR-RLS algorithm. The
constant forgetting factor is set to λ = 1 − 1/(3L) for all
the algorithms.
101
International Journal on Advances in Telecommunications, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/telecommunications/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

0
2
4
6
8
10
−30
−25
−20
−15
−10
−5
0
5
Time (seconds)
Misalignment (dB)
 
 
RLS, β20
RLS, β0
VR−RLS
Figure 4. Misalignment of the regularized RLS algorithm (using different
constant values of the regularization parameter) and the VR-RLS algorithm.
The input signal is speech, L = 128, and ENR = 20 dB. Echo path changes
at time 5 seconds.
0
2
4
6
8
10
−30
−20
−10
0
10
20
30
Time (seconds)
Misalignment (dB)
 
 
RLS, β20
RLS, β0
VR−RLS
Figure 5. Misalignment of the regularized RLS algorithm (using different
constant values of the regularization parameter) and the VR-RLS algorithm.
The input signal is speech, L = 128, and ENR = 20 dB. Near-end speech
appears between time 2.5 and 5 seconds (double-talk scenario).
In Figure 4, a single-talk scenario is considered and an echo
path change is introduced in the middle of the simulation. It
can be noticed that the VR-RLS algorithm behaves similarly to
the RLS algorithm using the constant parameter β20, which is
associated to the value of the true ENR. Also, it can be noticed
that a larger value of the normalized regularization parameter
(β0) improves the misalignment but affects the convergence
rate and tracking.
In Figure 5, a double-talk scenario [4], [5] is considered.
The near-end speech appears between time 2.5 and 5 seconds,
so that the signal v(n) is now non-stationary, since it contains
both noise and speech. It is clear that the VR-RLS algorithm
0
10
20
30
40
50
60
−30
−25
−20
−15
−10
−5
0
5
Time (seconds)
Misalignment (dB)
 
 
RLS, β20
RLS−DCD
VR−RLS
VR−RLS−DCD
Figure 6. Misalignment of the regularized RLS (using β20), RLS-DCD,
VR-RLS, and VR-RLS-DCD algorithms. The input signal is speech,
L = 512, and ENR = 20 dB. Echo path changes at time 30 seconds.
is more robust in this case as compared to the regularized RLS
using constant values of β. It should be outlined that we do
not use any double-talk detector (DTD) [4], [5] with the VR-
RLS algorithm. Therefore, the VR-RLS algorithm owns good
robustness features against double-talk, which is an important
gain in practice.
The second set of simulations is performed in the context
of acoustic echo cancellation [4], [5]. The unknown system,
i.e., the echo path, is a measured acoustic impulse response
depicted in Figure 2(b). It has 512 coefﬁcients and the same
length is used for the adaptive ﬁlter (L = 512). The output
of the echo path is corrupted by a white Gaussian noise with
different ENRs, i.e., 20 dB, 10 dB, and 0 dB. Based on (37),
we can determine the values of the optimal normalized regular-
ization parameter in these cases. Using appropriate notation,
we obtain β20 = 56.57, β10 = 221.01, and β0 = 1236.07,
respectively. In simulations, we compare the regularized RLS
algorithm using these constant regularization parameters with
the proposed VR-RLS and VR-RLS-DCD algorithms. Also,
the RLS-DCD algorithm [17] is included for comparison, using
Nu = 8, Mb = 16, and H = 1 (the same parameters are used
in the VR-RLS-DCD algorithm). The forgetting factor is set
to λ = 1 − 1/(16L) for all the algorithms.
In the ﬁrst set of experiments, the value of the ENR is
set to 20 dB. In Figure 6, an echo path change scenario is
simulated in the middle of the experiment, by shifting the
impulse response to the right by 25 samples. First, it can
be noticed that the VR-RLS and VR-RLS-DCD algorithms
behave very similarly and are close to the regularized RLS
algorithm using the constant (optimal) parameter β20, which
is associated to the value of the ENR. As expected, there is
an inherent delay in the initial convergence rate and tracking
reaction of the variable-regularized algorithms (as compared to
the RLS-DCD algorithm), due to the approximation in (39).
In Figure 7, a double-talk scenario is considered; the near-end
speech appears between time 27 and 30 seconds. It is clear that
the VR-RLS and VR-RLS-DCD algorithms are more robust in
102
International Journal on Advances in Telecommunications, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/telecommunications/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

0
10
20
30
40
50
60
−30
−25
−20
−15
−10
−5
0
5
10
15
20
25
Time (seconds)
Misalignment (dB)
 
 
RLS, β20
RLS−DCD
VR−RLS
VR−RLS−DCD
Figure 7. Misalignment of the regularized RLS (using β20), RLS-DCD,
VR-RLS, and VR-RLS-DCD algorithms. The input signal is speech,
L = 512, and ENR = 20 dB. Near-end speech appears between time 27 and
30 seconds (double-talk scenario).
0
10
20
30
40
50
60
−20
−15
−10
−5
0
5
Time (seconds)
Misalignment (dB)
 
 
RLS, β10
RLS−DCD
VR−RLS
VR−RLS−DCD
Figure 8. Misalignment of the regularized RLS (using β10), RLS-DCD,
VR-RLS, and VR-RLS-DCD algorithms. The input signal is speech,
L = 512, and ENR = 10 dB. Echo path changes at time 30 seconds.
this case, since the estimated ENR from (44) also includes the
contribution of the near-end signal.
In the second set of experiments, we select a lower ENR
value, i.e., 10 dB. In this case, the importance of regularization
becomes more apparent. As we can see from Figure 8, the
VR-RLS and VR-RLS-DCD algorithms behave similarly to
the regularized RLS using the constant (optimal) parameter
β10, and outperform the RLS-DCD algorithm (in terms of
misalignment). Also, as we can notice in Figure 9, the variable-
regularized algorithms are much more robust to double-talk, as
compared to their counterparts.
Finally, in the last set of experiments, we consider ENR =
0 dB. As expected, according to the results in Figure 10, the
0
10
20
30
40
50
60
−20
−15
−10
−5
0
5
10
15
20
25
Time (seconds)
Misalignment (dB)
 
 
RLS, β10
RLS−DCD
VR−RLS
VR−RLS−DCD
Figure 9. Misalignment of the regularized RLS (using β10), RLS-DCD,
VR-RLS, and VR-RLS-DCD algorithms. The input signal is speech,
L = 512, and ENR = 10 dB. Near-end speech appears between time 27 and
30 seconds (double-talk scenario).
0
10
20
30
40
50
60
−10
−5
0
5
Time (seconds)
Misalignment (dB)
 
 
RLS, β0
RLS−DCD
VR−RLS
VR−RLS−DCD
Figure 10. Misalignment of the regularized RLS (using β0), RLS-DCD,
VR-RLS, and VR-RLS-DCD algorithms. The input signal is speech,
L = 512, and ENR = 0 dB. Echo path changes at time 30 seconds.
VR-RLS and VR-RLS-DCD algorithms behave now similarly
to the regularized RLS using the constant (optimal) parameter
β0, and are much better as compared to the RLS-DCD algo-
rithm. Besides, according to Figure 11, the variable-regularized
algorithms outperform by far their counterparts in terms of
double-talk robustness.
VI.
CONCLUSIONS
The RLS algorithms are very appealing due to their fast
convergence rate. In this paper, we have focused on the main
parameters that control the performance of these algorithms,
i.e., the forgetting factor and the regularization term. In order
to achieve a better compromise between the performance
103
International Journal on Advances in Telecommunications, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/telecommunications/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

0
10
20
30
40
50
60
−10
−5
0
5
10
15
20
25
Time (seconds)
Misalignment (dB)
 
 
RLS, β0
RLS−DCD
VR−RLS
VR−RLS−DCD
Figure 11. Misalignment of the regularized RLS (using β0), RLS-DCD,
VR-RLS, and VR-RLS-DCD algorithms. The input signal is speech,
L = 512, and ENR = 0 dB. Near-end speech appears between time 27 and
30 seconds (double-talk scenario).
criteria (i.e., convergence and tracking versus misadjustment
and robustness), these parameters could be controlled. In
this context, the solutions presented in Sections III and IV
led to the VFF-RLS and VR-RLS algorithms, respectively.
Also, in Section IV, a low-complexity version of the VR-RLS
algorithm was derived, based on the DCD method, namely the
VR-RLS-DCD.
The ﬁrst set of experiments was performed in the context of
network echo cancellation. According to the simulation results,
the VFF-RLS and VR-RLS algorithms perform very well as
compared to their classical counterparts (which use constant
values of the key parameters).
The second set of simulations was performed in an acoustic
echo cancellation scenario. The results indicate that the VR-
RLS and VR-RLS-DCD algorithms own good robustness fea-
tures against the near-end signal. In other words, the robustness
of the algorithm against ENR variations (e.g., like double-talk)
can be controlled in terms of the regularization parameter.
Moreover, due to its low-complexity feature, the VR-RLS-
DCD algorithm could be a reliable candidates for real-world
echo cancellation applications.
ACKNOWLEDGMENT
This work was supported by European Space Agency under
Grant no. 4000121222/17/NL/CBi and University Politehnica
of Bucharest under Grant no. 18/05.10.2017.
REFERENCES
[1]
C. Elisei-Iliescu and C. Paleologu, “Recursive least-squares algorithms
for echo cancellation – An overview and open issues,” in Proc. ICN,
2017, pp. 87–91.
[2]
S. Haykin, Adaptive Filter Theory. Fourth Edition, Upper Saddle River,
NJ: Prentice-Hall, 2002.
[3]
A. H. Sayed, Adaptive Filters. New York, NY: Wiley, 2008.
[4]
J. Benesty, T. Gaensler, D. R. Morgan, M. M. Sondhi, and S. L.
Gay, Advances in Network and Acoustic Echo Cancellation. Berlin,
Germany: Springer-Verlag, 2001.
[5]
C. Paleologu, J. Benesty, and S. Ciochin˘a, Sparse Adaptive Filters for
Echo Cancellation. Morgan & Claypool Publishers, 2010.
[6]
S. Ciochin˘a, C. Paleologu, J. Benesty, and A. A. Enescu, “On the
inﬂuence of the forgetting factor of the RLS adaptive ﬁlter in system
identiﬁcation,” in Proc. IEEE ISSCS, 2009, pp. 205–208.
[7]
S. Ciochin˘a, C. Paleologu, and A. A. Enescu, “On the behaviour of
RLS adaptive algorithm in ﬁxed-point implementation,” in Proc. IEEE
ISSCS, 2003, pp. 57–60.
[8]
S. Song, J. S. Lim, S. J. Baek, and K. M. Sung, “Gauss Newton variable
forgetting factor recursive least squares for time varying parameter
tracking,” Electronics Lett., vol. 36, pp. 988–990, May 2000.
[9]
S.-H. Leung and C. F. So, “Gradient-based variable forgetting factor
RLS algorithm in time-varying environments,” IEEE Trans. Signal
Processing, vol. 53, pp. 3141–3150, Aug. 2005.
[10]
C. Paleologu, J. Benesty, and S. Ciochin˘a, “A robust variable forgetting
factor recursive least-squares algorithm for system identiﬁcation,” IEEE
Signal Processing Lett., vol. 15, pp. 597–600, 2008.
[11]
Y. J. Chu and S. C. Chan, “A new local polynomial modeling-based
variable forgetting factor RLS algorithm and its acoustic applications,”
IEEE/ACM Trans. Audio, Speech, Language Processing, vol. 23, pp.
2059–2069, Nov. 2015.
[12]
P. C. Hansen, Rank-Deﬁcient and Discrete Ill-Posed Problems: Numer-
ical Aspects of Linear Inversion. Philadelphia, PA: SIAM, 1998.
[13]
J. Benesty, C. Paleologu, and S. Ciochin˘a, “Regularization of the RLS
algorithm,” IEICE Trans. Fundamentals, vol. E94-A, pp. 1628–1629,
Aug. 2011.
[14]
Y. V. Zakharov and V. H. Nascimento, “Sparse sliding-window RLS
adaptive ﬁlter with dynamic regularization,” in Proc. EUSIPCO, 2016,
pp. 145–149.
[15]
C. Elisei-Iliescu, C. Stanciu, C. Paleologu, J. Benesty, C. Anghel, and
S. Ciochin˘a, “Robust variable regularized RLS algorithms,” in Proc.
IEEE HSCMA, 2017, pp. 171–175.
[16]
Y. V. Zakharov and T. C. Tozer, “Multiplication-free iterative algorithm
for LS problem,” IEE Electronics Lett., vol. 40, pp. 567–569, Apr. 2004.
[17]
Y. V. Zakharov, G. P. White, and J. Liu, “Low-complexity RLS algo-
rithms using dichotomous coordinate descent iterations,” IEEE Trans.
Signal Processing, vol. 56, pp. 3150–3161, July 2008.
[18]
J. Liu, Y. V. Zakharov, and B. Weaver, “Architecture and FPGA design
of dichotomous coordinate descent algorithms,” IEEE Trans. Circuits
and Systems I: Regular Papers, vol. 56, pp. 2425–2438, Nov. 2009.
[19]
C. Paleologu, S. Ciochin˘a, and J. Benesty, “Variable step-size NLMS
algorithm for under-modeling acoustic echo cancellation,” IEEE Signal
Processing Lett., vol. 15, pp. 5–8, 2008.
[20]
M. A. Iqbal and S. L. Grant, “Novel variable step size NLMS algorithms
for echo cancellation,” in Proc. IEEE ICASSP, 2008, pp. 241–244.
[21]
Digital Network Echo Cancellers, ITU-T Rec. G.168, 2002.
104
International Journal on Advances in Telecommunications, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/telecommunications/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

