Analytical Modeling of Partially Shared Caches in Embedded CMPs 
 
Wei Zang and Ann Gordon-Ross* 
 Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, 32611, USA  
weizang@ufl.edu & ann@ece.ufl.edu  
*Also with the NSF Center for High-Performance Reconfigurable Computing (CHREC) at the University of Florida 
 
Abstract—In modern ubiquitous devices, optimizing shared last-
level caches (LLCs) in embedded chip multi-processor systems 
(CMPs) is critical due to the increased contention for limited 
cache space from multiple cores. We propose cache partitioning 
with partial sharing (CaPPS) to reduce LLC contention and 
improve utilization. CaPPS can reduce the average LLC miss 
rate by 25% and 17% as compared to baseline configurations 
and private partitioning, respectively. To facilitate fast design 
space exploration, we develop an analytical model to quickly 
estimate the miss rates of all CaPPS configurations with an 
average error of only 0.73% and with an average speedup of 
3,966X as compared to a cycle-accurate simulator. 
Keywords—cache partitioning; analytical modeling 
 
I. 
INTRODUCTION  
Many chip multi-processor systems (CMPs) leverage 
shared last-level caches (LLCs) (e.g., second-/third-level), 
such as the ARM Cortex-A, Intel Xeon, and Sun T2 
[1][10][11]. To improve cache utilization, LLCs should be 
large enough to accommodate all sharing cores’ data, but 
long access latencies and high power consumption typically 
precludes large LLCs from embedded systems with strict 
area/energy/power 
constraints. 
Since 
battery-operated 
devices (e.g., cell phones, tablets, laptops, etc.) have limited 
energy reserves and satisfying the applications’ quality-of-
services (QoSs) is typically required, optimizing small 
LLCs’ performance is significantly more challenging due to 
contention for limited cache space.  
Shared LLCs afford high cache utilization and no 
coherence overhead, however, high contention and unfair 
cache utilization degrades performance. A core’s LLC 
occupancy (utilized space) is flexible and dictated by the 
core’s application’s demands. Cores with high LLC 
requirements occupy a large LLC area and cause high, 
potentially unfair, contention. For example, streaming 
multimedia applications occupy the LLC with a large 
amount of single-accessed data and unfairly evict the other 
cores’ data, thus increasing LLC miss rates. For example, 
this unfair cache utilization is common in mobile systems 
when a local music/movie player and other web-service 
applications are co-executed.       
 To eliminate shared LLC contention, cache partitioning 
[5][15][18] partitions the cache, allocates quotas (a subset of 
partitions) to the cores, and optionally configures the 
partitions/quotas (e.g., size and/or associativity [15][18]) to 
the allocated core’s requirements. Each core’s cache 
occupancy is constrained to the core’s quota to ensure fair 
utilization. Set partitioning partitions and allocates quotas at 
the cache set granularity and is typically implemented using 
operating system (OS)-based page coloring [12]. However, 
due to this OS modification requirement, hardware-based 
way partitioning is more widely used. Way partitioning 
partitions and allocates quotas at the cache way granularity 
[15][18]. However, way partitioning for shared LLCs 
typically uses private partitioning, which restricts quotas for 
exclusive use by the allocated core only and can lead to poor 
cache utilization if a core does not occupy the core’s entire 
allocated quota.   
In this paper, we propose to improve way partitioning’s 
cache utilization using cache partitioning with partial sharing 
(CaPPS). CaPPS improves cache utilization via sharing 
configuration, which enables a core’s quota to be configured 
as private, partially shared with a subset of cores, or fully 
shared with all other cores. Whereas sharing configuration 
increases the design space and thus increases optimization 
potential, this large design space significantly increases 
design space exploration time. To facilitate design space 
exploration, we develop an offline analytical model to 
quickly estimate cache miss rates for all partitioning and 
sharing configurations, which enables determining LLC 
configurations for any optimization that evaluates cache miss 
rates (e.g., performance, energy, energy delay product, 
power, etc.). The analytical model probabilistically predicts 
the miss rates when multiple applications are co-executing 
using the isolated cache access distribution for each 
application (i.e., the application is run in isolation with no 
co-executing applications). Although several previous works 
[3][4][6] have developed analytical models to predict shared 
LLC contention offline, these works’ caches where 
completely shared by all cores and did not consider partial 
sharing, which vastly increases the design space and thus 
optimization potential. Due to CaPPS’s extensive design 
space, experiments reveal that CaPPS can reduce the average 
LLC miss rates by as much as 25% and 17% as compared to 
baseline configurations and private partitioning, respectively. 
The analytical model estimates cache miss rates with an 
average error of only 0.73% and is 3,966X faster on average 
than a cycle-accurate simulator.        
II. 
RELATED WORK 
Since CaPPS uses way partitioning and we developed an 
analytical model to predict the shared ways’ cache 
contention, we compare our work with prior work in these 
areas.  
For way partitioning, Qureshi and Patt [15] developed 
utility-based cache partitioning (UCP) that used an online 
monitor to track the cache misses for all possible numbers of 
13
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

ways assigned to each core. Greedy and refined heuristics 
determined the cores’ quotas. Varadarajan et al. [18] 
partitioned the cache into small direct-mapped cache units, 
which were privately assigned to the cores and the cache 
partitions had configurable size, block size, and associativity. 
Kim et al. [13] developed static and dynamic cache 
partitioning 
for 
fairness 
optimization. 
Static 
cache 
partitioning used the cache access’s stack distance profile to 
determine 
the 
cores’ 
requirements. 
Dynamic 
cache 
partitioning increased/decreased the cores’ quotas in 
accordance with the miss rate changes between evaluation 
intervals. Private LLCs also benefit from way partitioning. In 
CloudCache [14], the private caches were partitioned, but a 
core could share nearby cores’ (limited access latencies) 
private caches. MorphCache [17] partitioned the level two 
and level three caches and allowed subsets of cores’ private 
caches to be merged and fully shared by the subset. Although 
some of these prior works in private LLC partitioning 
[14][17] enabled a core to share other cores’ quotas, CaPPS 
is more flexible than these works by enabling a portion/all of 
a core’s quota to be shared with any subset of cores.   
Prior works on analytical modeling to determine cache 
miss rates targeted only fully shared caches. Chandra et al. 
[3] proposed a model using access traces for isolated threads 
to predict inter-thread contention for a shared cache. Reuse 
distance profiles were analyzed to predict the extra cache 
misses for each thread due to cache sharing, but the model 
did not consider the interaction between cycles per 
instruction (CPI) variations and cache contention. Eklov et 
al. [6] proposed a simpler model that calculated the CPI 
considering the cache misses caused by contention by 
predicting the reuse distance distribution of an application 
when co-executed with other applications based on the 
isolated reuse distance distribution of each application. Chen 
and Aamodt [4] proposed a Markov model to estimate the 
cache miss rates for multi-threaded applications with inter-
thread communication.  
Analytically predicting the cache miss rate for CaPPS is 
more challenging than prior works, since in CaPPS, only the 
interleaved LLC accesses of other cores that pollute the 
partially shared ways affect the core’s miss rate. Determining 
the effects of these interleaved accesses on the miss rate 
introduces extensive complexity.  
III. CACHE PARTITIONING WITH PARTIAL SHARING 
To accommodate the LLC requirements for multiple 
applications co-executing on different cores, CaPPS 
partitions the shared LLC at the way granularity and 
leverages sharing configuration to allocate the partitions to 
each core’s quota. To facilitate fast design space exploration, 
an analytical model estimates the cache miss rates for the 
CaPPS configurations using the applications’ isolated LLC 
access traces. We assume that the cores execute different 
applications in independent address spaces, thus there is no 
shared instruction/data address or coherence management, 
which is a common case in mobile systems running disparate 
applications and is similar to assumptions made in prior 
works [3][6].  
A. Architecture and Sharing Configurations 
CaPPS’s sharing configurations enable a core’s quota to 
be configured as private, partially shared with a subset of 
cores, or fully shared with all other cores. Fig. 1 (a)-(c) 
illustrates sample configurations, respectively, for a 4-core 
CMP (C1 to C4
CaPPS uses the least recently used (LRU) replacement 
policy, but we note that the analytical model can be extended 
to approximate estimations for other replacement policies, 
such as pseudo-LRU. To reduce the sharing configurability 
with no effect on cache performance and to minimize 
contention, cores share an arbitrary number of ways starting 
with the LRU way, then second LRU way, and so on since 
these ways are least likely to be accessed. For example, in 
) and an 8-way LLC: (a) each core’s quota 
has a configurable number of private ways; (b) the cores’ 
quotas are partially shared with subsets of cores; and (c) all 
of the four cores fully share all of the ways.  
Fig. 1 (b) two of C1’s ways are shared with C2., therefore, 
C1’s two most recently used (MRU) blocks are cached in 
C1’s two private ways, and the two LRU blocks are cached 
in the two ways shared with C2 and these two LRU blocks 
are the only replacement candidates for C2
[7]
’s accesses. 
Maintaining this LRU ordering and determining replacement 
candidate can be easily implemented using a linked list or 
systolic array implementation 
 for conventional LRU 
caches with the integration of column caching [5] to achieve 
low hardware overhead and without increasing the cache 
access time. Since the hardware implementation is 
straightforward and is not the focus of this paper, we omit 
the implementation details for brevity.        
B. Analytical Modeling Overview 
For applications with fully/partially shared ways, the 
analytical model probabilistically determines the miss rates 
using the isolated cache access distributions for the co-
executing applications. These distributions are recorded 
during isolated access trace processing. The isolated LLC 
access traces can be generated with a simulator/profiler by 
running each application in isolation on a single core with all 
other cores idle. For applications with only private ways, 
there is no cache contention and the miss rate can be directly 
determined from the isolated LLC access trace distribution.   
Fig. 2 illustrates the contention in the shared ways using 
sample time-ordered isolated (C1, C2) and interleaved/co-
executed (C1&C2) access traces to an arbitrary cache set 
from cores C1 and C2. C1’s and C2’s accesses are denoted as 
Xi and Yi, respectively, where i differentiates accesses to 
unique cache blocks. The first access to X3 and the second 
access to X1 occurred at times 𝑡1 and 𝑡2, respectively. C1
8-way LLC
8-way LLC
8-way LLC
Shared by all of 
the four cores
(a)
(b)
(c)
Private 
for C1
Private 
for C2
Private 
for C3
Private for C4
Private 
for C1
Private for C2
Shared by 
C1 & C2
Private for C3
Shared by 
C3 & C4
’s 
 
Figure 1. Three sample sharing configurations: (a) the cores’ quotas are 
private; (b) some ways are partially shared with a subset of cores; and (c) the 
entire LLC is fully shared with all cores. 
14
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

second access to X1 will be a cache hit if C1’s number of 
private ways is greater than or equal to five because four 
unique blocks are accessed between the two accesses to X1. 
Alternatively, if C1’s number of private ways is smaller than 
five and C1 shares ways with C2, X1’s hit/miss is dictated by 
the interleaved accesses from C2. For example, if C1 has six 
allocated ways and two of the LRU ways are shared with C2, 
X3 evicts X1 from C1’s private way into a shared way. 
Therefore, C2’s accesses between 𝑡1 and 𝑡2 dictates whether 
X1 has been evicted from the cache or not. If C2’s accesses 
between 𝑡1 and 𝑡2 evict two or more blocks into the shared 
ways, X1
In order to determine the contention effects to C
’s second access will be a cache miss.   
1’s miss 
rate, C1 and C2
’s number of accesses 𝑛1 (Section III
.D.a) 
and 𝑛2 (Section III.D.b), respectively, during the time period 
(𝑡1, 𝑡2) must be estimated. Since the number of blocks 𝑅 
from 𝑛2 evicted into the shared ways dictates whether C1’s 
blocks (e.g., X1
 in Fig. 4
) are still in the shared ways, we 
calculate the probability 𝑝(𝑛2, 𝑅) that 𝑅 number of blocks 
are evicted into the shared ways (Section III.D.c) to estimate 
C1
’s miss rate (Section III
.D.d).          
C. Isolated Access Trace Processing 
To accumulate the isolated cache access distribution, we 
record the reuse distance and stack distance for each access 
in the isolated LLC access trace, which can be obtained 
using a stack-based trace-driven simulator [9]. For an 
accessed address T that maps to a cache set, the reuse 
distance 𝑟 is the number of accesses to that set between this 
access to T and the previous access to any address in the 
same block as T. The stack distance 𝑑 is the number of 
unique block addresses, or conflicts, in this set of accesses. 
For example, in Fig. 2, C1’s second access to X1
In each cache set, we accumulate the number of accesses 
𝑁𝑑 for each stack distance 𝑑 (𝑑 ∈ [0, 𝐴]), where 𝐴 is the LLC 
associativity. We accumulate the number of accesses with 
𝑑 > 𝐴 in 𝑁𝐴 together with the number of accesses with 𝑑 = 𝐴, 
since all accesses with 𝑑 ≥ 𝐴 are cache misses in any 
configuration. Given this information, for any access, the 
probabilistic information for the access’ stack distance is 
𝑝(𝑑 < 𝑑𝑖 ) = (∑
𝑑=𝑑𝑖−1 𝑁𝑑
𝑑=0
) (∑ 𝑁𝑑)
ൗ
 
and 
𝑝(𝑑 ≥ 𝑑𝑖 ) = 1 −
𝑝(𝑑 < 𝑑𝑖 ), (∀𝑑𝑖 ∈ [1, A]). For all of the accesses for each 𝑑, 
we accumulate a histogram of different 𝑟 and calculate the 
average 𝑟̅ over all 𝑟.     
 has 𝑟 = 7 
and 𝑑 = 4. 
The analytical model uses the base (best case) CPU 
cycles 𝐶𝑦𝑐𝑙𝑒𝑠𝑏𝑎𝑠𝑒 to calculate the CPU cycles required to 
complete the application when co-executed with other 
applications. 𝐶𝑦𝑐𝑙𝑒𝑠𝑏𝑎𝑠𝑒 assumes that all LLC accesses are 
hits. An application’s total number of CPU cycles 𝐶𝑦𝑐𝑙𝑒𝑠𝑒𝑥𝑒 
are recorded in the isolated execution to calculate 𝐶𝑦𝑐𝑙𝑒𝑠𝑏𝑎𝑠𝑒 
using 𝐶𝑦𝑐𝑙𝑒𝑠𝑏𝑎𝑠𝑒 = 𝐶𝑦𝑐𝑙𝑒𝑠𝑒𝑥𝑒 − 𝑚𝑒𝑥𝑒 · 𝐿𝐿𝐶𝑙𝑎𝑡𝑒𝑛𝑐𝑦, where 𝑚𝑒𝑥𝑒 
is the number of LLC misses in the application’s isolated 
execution and 𝐿𝐿𝐶𝑙𝑎𝑡𝑒𝑛𝑐𝑦 is the delay cycles incurred by an 
LLC miss.          
Since the access distributions across the cache sets are 
different, the distributions are individually accumulated and 
recorded for each set to estimate the number of misses in 
each set’s accesses. Since the analysis is the same for all 
cache sets, we present the analytical model for one arbitrary 
cache set. 
D. Analysis of the Shared Ways’ Contention 
First, we describe the analytical model to analyze the 
shared ways’ contention for a sample CMP with two cores 
C1 and C2 and then generalize the analytical model to any 
number of cores. A sharing configuration allocates 𝐾C1 
number of ways to core C1, where 𝐾𝑃,C1 ways are private and 
the remaining 𝐾𝑆 (𝐾𝑆 = 𝐾C1 − 𝐾𝑃,C1) ways are shared with 
core C2. 𝐾C2 and 𝐾𝑃,C2 similarly denote these values for C2. 
For C1, all accesses with a stack distance 𝑑 ≤ 𝐾𝑃,C1 − 1 result 
in cache hits and all accesses with 𝑑 ≥ 𝐾C1 are cache misses. 
The cache hit/miss determination of the accesses where 
𝐾𝑃,C1 ≤ 𝑑 ≤ 𝐾C1 − 1 depends on the interleaved accesses from 
C2, and the following subsections elaborate on the estimation 
method for these accesses. If C1 only has private ways, then 
𝐾𝑃,C1 = 𝐾C1, and these estimations are not required since the 
number of misses for C1
a. Calculation of 𝑛1  
 can be directly calculated using 
∑
𝑁𝑑,C1
𝑑=𝐾C1−1
𝑑=0
. 
For an arbitrary stack distance 𝐷 in [𝐾𝑃,C1, 𝐾C1 − 1], the 
associated 𝑟̅ was determined during isolated access trace 
processing. This subsection presents the calculation of 𝑛1 for 
C1
Fig. 3
’s accesses with stack distance 𝐷 based on 𝑟. 
 depicts C1’s isolated access trace to an arbitrary 
cache set, where the second access to X1 has a stack distance 
𝐷 and reuse distance 𝑟̅. X3’s access evicts X1 from C1’s 
private ways, therefore, the numbers of conflicts before and 
after X3 are (𝐾𝑃,C1 − 1) and (𝐷 − (𝐾𝑃,C1 − 1)), respectively. 
Confi denotes the first access of the i-th conflict with X1. We 
denote the number of accesses before X3 
time
Access trace in one cache set
      X1           X2             X3            X3         X2         X4                X5          X1
C1
         Y1           Y2          Y3                                       Y4   Y5        Y1
C2
X1     Y1  X2      Y2    X3  Y3      X3         X2         X4   Y4   Y5 X5   Y1   X1
C1&C2
t1
t2
as 𝑛0, which can be 
any 
integer 
in 
[𝐾𝑃,C1 − 1, 𝑟̅ − (𝐷 − 𝐾𝑃,C1) − 2]. 
After 
 
 
Figure 2. Two cores’ isolated (C1, C2) and interleaved (C1&C2
X1           …...           X3       ...   Conf(Kp,C1-1)      …...  Conf3     ...    Conf2    ...   Conf1      X1
time
d≥1
d≥2
d≥3
d≥Kp,C1-1
d<1
d<2
d<Kp,C1-1
...
...
n0 
n1 
C1
t1
t2
Access trace in one cache set
n accesses, D conflicts  
) access 
traces for an arbitrary cache set. 
 
Figure 3. C1’s isolated access trace to an arbitrary cache set for calculating 𝒏𝟏. 
15
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

determining the probability 𝑝(𝑛0, (𝐾𝑃,C1 − 1)) for each 𝑛0 
(where 𝐾𝑃,C1 − 1 indicates the number of conflicts in the 𝑛0 
accesses), we can calculate 𝑛0’s expected value 𝑛ത0 for the 
evaluated configuration’s associated 𝐾𝑃,C1 using 𝑛ത0 =
∑(𝑛0 ⋅ 𝑝(𝑛0, (𝐾𝑃,C1 − 1))), and 𝑛1’s expected value is: 
𝑛ത1 = 𝑟̅ − 𝑛ത0 − 1. 
For a particular 𝑛0 ∈ [𝐾𝑃,C1 − 1, 𝑟̅ − (𝐷 − 𝐾𝑃,C1) − 2], the 
probability is:  
𝑝 ቀ𝑛0, ൫𝐾𝑃,C1 − 1൯ቁ = 𝑝(𝐸𝐴, 𝐸𝐵|𝐸𝐶) = 𝑝𝑏𝑒𝑓𝑜𝑟𝑒(𝐸𝐴) ⋅ 𝑝𝑎𝑓𝑡𝑒𝑟(𝐸𝐵)
𝑝𝑡𝑜𝑡𝑎𝑙(𝐸𝐶)
     (1) 
where 𝐸𝐴 is the event that the 𝑛0 accesses have exactly 
(𝐾𝑃,C1 − 1) conflicts and 𝐸𝐵 is the event that the 𝑛1 accesses 
have exactly (𝐷 − (𝐾𝑃,C1 − 1)) conflicts. 𝑝𝑏𝑒𝑓𝑜𝑟𝑒(𝐸𝐴) and 
𝑝𝑎𝑓𝑡𝑒𝑟(𝐸𝐵) are the occurrence probabilities of 𝐸𝐴 and 𝐸𝐵, 
respectively. 𝐸𝐶 is the event that the 𝑟 accesses have exactly 
𝐷 conflicts and 𝑝𝑡𝑜𝑡𝑎𝑙(𝐸𝐶)  is the probability of 𝐸𝐶’s 
occurrence, which is the summation of (𝑝𝑎𝑓𝑡𝑒𝑟(𝐸𝐵) ⋅
𝑝𝑏𝑒𝑓𝑜𝑟𝑒(𝐸𝐴)) for all 𝑛0. To calculate 𝑝𝑏𝑒𝑓𝑜𝑟𝑒(𝐸𝐴) and 
𝑝𝑎𝑓𝑡𝑒𝑟(𝐸𝐵), we examine the sufficient conditions that 𝐸𝐴 and 
𝐸𝐵 occur. In Fig. 3, the first access following X1 must be 
different from X1 (for 𝐷 > 0), which is Conf1 satisfying 
𝑑 ≥ 1, since Conf1 has at least one conflict: X1. The second 
conflict Conf2 satisfies 𝑑 ≥ 2, since Conf2 has at least two 
conflicts: Conf1 and X1. The accesses between Conf1 and 
Conf2 satisfy 𝑑 < 1 since these accesses can only be Conf1. 
Conf3 satisfies 𝑑 ≥ 3 since Conf3 has at least three conflicts: 
Conf2, Conf1, and X1. The accesses between Conf2 and 
Conf3 satisfy 𝑑 < 2, since these conflicts can only be Conf2 
or Conf1, etc. Similarly, ConfKp,C1-1 satisfies 𝑑 ≥ (𝐾𝑃,C1 − 1) 
and the accesses between X3 and ConfKp,C1-1
where 𝑆𝑎 is a set including all 𝑎⃗ satisfying ∑ 𝑎𝑖 = 𝑛0 −
(𝐾𝑃,C1 − 1). Similarly, defining 𝑏ሬ⃗ = (𝑏0, 𝑏1, … , 𝑏𝐷−𝐾𝑝,C1) where 
𝑏𝑖 ∈ [0, 𝑛1 − (𝐷 − 𝐾𝑃,C1 + 1)], 𝑝𝑎𝑓𝑡𝑒𝑟(𝐸𝐵) is: 
𝑝𝑎𝑓𝑡𝑒𝑟(𝐸𝐵) = ቐ
ෑ
𝑝൫𝑑 ≥ 𝑖 + 𝐾𝑃,C1൯
𝑖=𝐷−𝐾𝑃,C1
𝑖=0
ቑ
⋅ ቐ ෍ ቌ
ෑ
𝑝൫𝑑 < 𝑖 + 𝐾𝑃,C1൯
𝑏𝑖
𝑖=𝐷−𝐾𝑃,C1
𝑖=0
ቍ
∀𝑏ሬ⃗∈𝑆𝑏
ቑ    (3) 
 satisfy 𝑑 <
(𝐾𝑃,C1 − 1). Thus, defining 𝑎⃗ = (𝑎1, 𝑎2, … , 𝑎𝐾𝑝,C1−1) where 
𝑎𝑖 ∈ [0, 𝑛0 − (𝐾𝑃,C1 − 1)], 𝑝𝑏𝑒𝑓𝑜𝑟𝑒(𝐸𝐴) is: 
𝑝𝑏𝑒𝑓𝑜𝑟𝑒(𝐸𝐴) = ቐ
ෑ
𝑝(𝑑 ≥ 𝑖)
𝑖=𝐾𝑃,C1−1
𝑖=1
ቑ ⋅ ቐ ෍ ቌ
ෑ
𝑝(𝑑 < 𝑖)𝑎𝑖
𝑖=𝐾𝑃,C1−1
𝑖=1
ቍ
∀𝑎ሬ⃗∈𝑆𝑎
ቑ  (2) 
where 𝑆𝑏 is a set including all 𝑏ሬ⃗ satisfying ∑ 𝑏𝑖 = 𝑛1 − (𝐷 −
𝐾𝑃,C1 + 1). 
b. Calculation of 𝑛2 
To determine the contention effect from C2, the expected 
number of accesses 𝑛ത2 from C2 is estimated based on the 
ratio of the number of cache set accesses from C1 and C2 
where ∑ 𝑁𝑑,C1and ∑ 𝑁𝑑,𝐂2 are the total number of LLC 
accesses from C
per 
cycle: 
𝑛ത1
𝑛ത2
=
∑ 𝑁𝑑,𝐶1 𝐶𝑦𝑐𝑙𝑒𝑠
෣ 𝐶1
⁄
∑ 𝑁𝑑,𝐶2 𝐶𝑦𝑐𝑙𝑒𝑠
⁄ ෣ 𝐶2
                                      (4) 
1 and C2, respectively. 𝐶𝑦𝑐𝑙𝑒𝑠
෣ C1is the 
number of CPU cycles required to execute the application on 
C1 when C2
where 𝑑𝑒𝑙𝑎𝑦𝑏𝑢𝑠_𝑐𝑜𝑛𝑡𝑒𝑛𝑡𝑖𝑜𝑛 is the delay imposed by the shared 
bus contention from the higher level caches (closer to the 
CPU) of each core to the shared LLC. 𝑑𝑒𝑙𝑎𝑦𝑏𝑢𝑠_𝑐𝑜𝑛𝑡𝑒𝑛𝑡𝑖𝑜𝑛 is  
derived by calculating the bus contention probability that 
another core is sending a read/write request to the LLC and 
the LLC is returning that core’s requested block 
simultaneously with the evaluated core’s bus request. The 
bus contention probability is dictated by each core’s bus 
request probability, which is equal to the total number of bus 
requests generated from the core’s higher level cache misses 
divided by 𝐶𝑦𝑐𝑙𝑒𝑠C𝚤
෣
. 
 is co-executing another application, and 
𝐶𝑦𝑐𝑙𝑒𝑠
෣ C2 is similarly defined. 𝐶𝑦𝑐𝑙𝑒𝑠C𝚤
෣
 can be calculated 
using 𝐶𝑦𝑐𝑙𝑒𝑠𝑏𝑎𝑠𝑒 and the number of LLC misses 𝑚ෝ estimated 
with the contention: 
𝐶𝑦𝑐𝑙𝑒𝑠C𝚤
෣
= 𝐶𝑦𝑐𝑙𝑒𝑠𝑏𝑎𝑠𝑒 + 𝑚ෝ · 𝐿𝐿𝐶𝑙𝑎𝑡𝑒𝑛𝑐𝑦 +  𝑑𝑒𝑙𝑎𝑦𝑏𝑢𝑠_𝑐𝑜𝑛𝑡𝑒𝑛𝑡𝑖𝑜𝑛     (5) 
c. Calculation of  𝑝(𝑛2, 𝑅) 
𝑝(𝑛2, 𝑅) is the probability that 𝑅 number of blocks are 
evicted from C2
(4)
’s private ways in the 𝑛2 accesses. Directly 
using the expected 𝑛2 to calculate 𝑝(𝑛ത2 , 𝑅) will introduce a 
large bias (approximate 10% error) in the estimated LLC 
miss rate, since different values of 𝑛2 result in different 
hit/miss determinations and using one expected value 𝑛ത2 will 
estimate all 𝑛2 as hits/misses. Thus, we model 𝑛2 using a 
Poisson distribution 𝑝(𝑛2) = 𝑃𝑜𝑖𝑠𝑠𝑜𝑛(𝑛2, 𝜆), where 𝜆 is 𝑛ത2 if 
the LLC is accessed randomly. However, since the LLC’s 
accesses are generally not random and not uniformly 
distributed in time (which makes 
 valid), we use an 
empirical variable 𝑒 to adjust 𝜆 to 𝜆 = 𝑛ത2/𝑒. Our experiments 
indicated that 𝑒 = 5 was appropriate for our training 
benchmark suite, which contains a wide variety of typical 
CMP applications, and is thus generally applicable. Since the 
range of 𝑛2 is infinite in the Poisson distribution, and 𝑛2 with 
very small 𝑝(𝑛2) has minimal effect on the miss rate 
estimation, we only consider the 𝑛2 with 𝑝(𝑛2) > 0.01 and 
calculate the associated 𝑝(𝑛2, 𝑅).   
To calculate 𝑝(𝑛2, 𝑅) for an arbitrary 𝑛2, 𝑅 is determined 
by evaluating the 𝑛2 accesses in chronological order with an 
initial value of 𝑅 = 0. If there is one access with 𝑑 > 𝐾𝑃,C2 +
𝑐𝑢𝑟𝑟𝑒𝑛𝑡 𝑅, fetching this address into C2
with the initial case 𝑝(𝑛2 = 0, 𝑅 = 0) = 1. 
’s private ways will 
evict one block into the shared ways and thus 𝑅 is 
incremented by 1. Therefore, we can calculate 𝑝(𝑛2, 𝑅) 
inductively:  
𝑝(𝑛2, 𝑅) =
⎩
⎪
⎨
⎪
⎧𝑝(𝑛2 − 1, 𝑅 − 1) ⋅ 𝑝 ቀ𝑑 ≥ 𝐾𝑃,C2 + (𝑅 − 1)ቁ ,       𝑅 = 𝑛2 
𝑝(𝑛2 − 1, 𝑅) ⋅ 𝑝൫𝑑 < 𝐾𝑃,C2 + 𝑅൯                                            
    +𝑝(𝑛2 − 1, 𝑅 − 1) ⋅ 𝑝 ቀ𝑑 ≥ 𝐾𝑃,C2 + (𝑅 − 1)ቁ , 𝑅 < 𝑛2
𝑝(𝑛2 − 1, 𝑅) ⋅ 𝑝൫𝑑 < 𝐾𝑃,C2 + 𝑅൯,                             𝑅 = 0 
  
d. Calculation of the LLC Miss Rates 
Considering the impact of 𝑅 to the accesses with stack 
distance 𝑑 ∈ [𝐾𝑃,C1, 𝐾C1 − 1], the number of cache hits for C1 
is: 
16
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

ℎC1
෢ =
෍
𝑁𝑑,C1
𝑑=𝐾𝑝,C1−1
𝑑=0
+
෍
൮𝑁𝑑,C1
𝑑=𝐾C1−1
𝑑=𝐾𝑝,C1
⋅
෍
൮ቌ
෍
𝑝(𝑛2,𝑅)
𝑅=𝐾C1−𝑑−1
𝑅=0
ቍ ⋅ 𝑝(𝑛2)൲
∀𝑛2: 𝑝(𝑛2)>0.01
൲    (7) 
After accumulating ℎC1
෢  for all cache sets, the number of LLC 
misses 𝑚C1
ෞ  and the LLC miss rates can be determined. 
Finally, we generalize the analytical model to estimate 
the LLC miss rate for any core Ci when j additional cores 
(denoted as Cj) share cache ways with Ci by calculating the 
expected number of accesses 𝑛തC𝑗 from the additional cores 
during the time (𝑡1, 𝑡2) and then estimating 𝑝(𝑛C𝑗, 𝑅C𝑗) 
similarly as estimating 𝑛ത2 and 𝑝(𝑛2, 𝑅) for C2
(6)
. The 
generalized expression of 
 is: 
ℎC𝚤
෢ =
෍
𝑁𝑑,C𝑖
𝑑=𝐾𝑝,C𝑖−1
𝑑=0
+
෍
൫𝑁𝑑,C𝑖 ⋅ 𝑝ℎ൯
𝑑=𝐾C𝑖−1
𝑑=𝐾𝑝,C𝑖
                 (8) 
where: 
𝑝ℎ = ෍ ቌෑ ቀ𝑝(𝑛C𝑗) ⋅ 𝑝(𝑛C𝑗, 𝑅C𝑗)ቁ
C𝑗∈Cሬሬ⃗
ቍ
∀Cሬሬ⃗∈𝑆C 
                      (9)                                                      
where Cሬ⃗ = (𝑛C1, 𝑛C2, … , 𝑛𝑗) with 𝑝(𝑛C𝑗) > 0.01 and 𝑆C is a set 
including all Cሬ⃗ satisfying ∑ 𝑅C𝑗 ≤ 𝐾C𝑖 − 𝑑 − 1. 
 According to (5), a circular dependency exists where 
𝐶𝑦𝑐𝑙𝑒𝑠
෣  is used to estimate 𝑚ෝ and 𝑚ෝ is used to calculate 
𝐶𝑦𝑐𝑙𝑒𝑠
෣ . The solution cannot be represented using a closed 
form, thus we iteratively solve for 𝑚ෝ. The initial value of 𝑚ෝ 
is acquired assuming there is no contention (i.e., all 𝐾C𝑖 
number of ways are privately used by Ci
 
(5)
), and 𝑚ෝ is used in 
 to calculate the initial value of 𝐶𝑦𝑐𝑙𝑒𝑠
෣ . 𝐶𝑦𝑐𝑙𝑒𝑠
෣  is provided 
back into the analytical model to update 𝑚ෝ and the new 𝑚ෝ is 
used to update 𝐶𝑦𝑐𝑙𝑒𝑠
෣ . This iterative process continues until a 
stable 𝑚ෝ (with a precision of 0.001%) is achieved. 
Experimental results indicated that only four iterations were 
required for the results to converge.  
The analytical model’s runtime complexity depends on 
the evaluated sharing configuration and the isolated cache 
access distribution for each application. Due to the large 
number of complex and interdependent variables and 
unknowns, the complexity of the model is intractable, thus in 
our experiments, we evaluate the analytical model’s 
measured execution time.     
IV. 
EXPERIMENT RESULTS  
We verified the advantages of CaPPS as compared to two 
baseline configurations and private partitioning. We also 
verified the accuracy of our estimated LLC miss rates 
obtained via the analytical model and evaluated the 
analytical model’s ability to determine the optimal 
(minimum LLC miss rate) configuration in the CaPPS design 
space. Additionally, we illustrate the analytical model’s 
efficiency by comparing the time required to calculate the 
LLC miss rates as compared to using a cycle-accurate 
simulator that generates the exact cache miss rates for all 
configurations.  
A. Experiment Setup 
We used twelve benchmarks from the SPEC CPU2006 
suite [16], which were compiled to Alpha_OSF binaries and 
executed using “ref” input data sets. Due to incorrect 
execution, we could not evaluate the complete suite. Even 
though our work is targeted towards embedded systems, we 
did not use embedded system benchmark suites since these 
suites contain only small kernels, which do not sufficiently 
access the LLC, and do not represent our targeted embedded 
CMP domain. Since complete execution of the large SPEC 
benchmarks prohibits exhaustive examination of the entire 
CaPPS design space, and since most embedded benchmarks 
have stable behavior during execution, for each SPEC 
benchmark, we performed phase classification using 
SimPoint [8] to select 500 million consecutive instructions 
with similar behavior as the simulation interval to mimic an 
embedded application with high LLC occupancy. 
We generated the exact cache miss rates for comparison 
purposes using gem5 [2] and modeled four in-order cores 
with the TimingSimple CPU model, which stalls the CPU 
when fetching from the caches and memory. Each core had 
private level-one (L1) instruction and data caches. The 
unified level-two (L2) cache and all lower level memory 
hierarchy components were shared among all cores. We 
modified the L2 cache replacement operation in gem5 to 
model CaPPS. TABLE I shows the parameters used for each 
system component. Since four cores shared the eight-way 
LLC (i.e., L2 cache), CaPPS’s design space had 3,347 
configurations. 
Before CaPPS simulation, we executed each benchmark 
in isolation during the benchmark’s simulation interval and 
recorded the isolated LLC access traces and the CPU cycles 
𝐶𝑦𝑐𝑙𝑒𝑠𝑒𝑥𝑒. For CaPPS simulation, we arbitrarily selected four 
benchmarks to be co-executed, which formed a benchmark 
set, and we evaluated sixteen benchmark sets. Since the four 
benchmarks’ simulation intervals were at different execution 
points, we forced the four cores to simultaneously begin 
executing at each benchmark’s associated simulation 
interval’s starting instruction using a full-system checkpoint. 
The full-system checkpoint was created by aggregating the 
isolated-benchmark checkpoints, which were generated by 
fast-forwarding the benchmark to the starting instruction of 
the benchmark’s associated simulation interval when the 
benchmark was executed in isolation.   
TABLE I. CMP SYSTEM PARAMETERS 
CPU 
2 GHz clock, single thread  
L1 instruction 
cache 
Private, total size of 8 KB, block size of 64 B, 2-way 
associativity, LRU replacement, access latency of 2 
CPU cycles  
L1 data cache 
Private, total size of 8 KB, block size of 64 B, 2-way 
associativity, LRU replacement, access latency of 2 
CPU cycles 
L2 unified 
cache 
Shared, total size of 1 MB, block size of 64 B, 8-way 
associativity, LRU replacement, access latency of 20 
CPU cycles, non-inclusive 
Memory  
Total size of 3 GB, access latency of 200 CPU cycles  
L1 caches to 
L2 cache bus 
Shared, width of 64 B, 1 GHz clock, first come first 
serve (FCFS) scheduling 
Memory bus 
Width of 64 B, 1 GHz clock 
 
17
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

For each simulation, the system execution was 
terminated when any core reached 500 million instructions. 
Due to varying CPU stall cycles across the benchmarks, at 
the termination point, not all cores had completed executing 
the simulation interval. However, this termination approach 
guaranteed that the cache miss rates reflected a fully-loaded 
system (i.e., full LLC contention since all cores were running 
during the entire system execution). Since we focused on the 
cache miss rates rather than the absolute number of cache 
misses, the incomplete benchmarks’ execution had no impact 
on the evaluation. Similarly, due to statistical predictions, the 
applications 
are 
not 
required 
to 
begin 
execution 
simultaneously to garner accurate results.   
Although our experiments used only four cores and the 
LLC was a shared 8-way L2 cache, the analytical model 
itself does not include any limitations on the number of 
cores, the hierarchical level of the LLC, or the cache 
parameters (e.g., total size, block size, and associativity for 
our experiments).  
B. CaPPS Evaluation 
To validate the advantages of CaPPS, we compared 
CaPPS’s ability to reduce the LLC miss rate as compared to 
two baseline configurations and private partitioning, since 
shared LLC partitioning in previous works [13][15][18] only 
provided private partitioning.  
Fig. 4 depicts the average LLC miss rate reductions for 
CaPPS’s optimal configurations (the configurations with 
minimum average LLC miss rate in CaPPS’s design space) 
as compared to two baseline configurations: 1) even-private-
partitioning: the LLC is evenly partitioned using private 
partitioning (first bar); and 2) fully-shared: the LLC is fully 
shared by all cores (second bar). Across all benchmark sets, 
the average and maximum average LLC miss rate reductions 
were 25.58% and 50.15%, respectively, as compared to 
even-private-partitioning, 
and 
19.39% 
and 
41.10%, 
respectively, as compared to fully-shared. 
The third bar in Fig. 4 depicts the average LLC miss rate 
reductions for CaPPS’s optimal configuration as compared to 
private partitioning’s optimal configuration, which is the 
configuration with minimum LLC miss rate in the private 
partitioning’s design space consisting of 35 configurations—
approximately 1% of CaPPS’s design space. Across all 
benchmark sets, the average and maximum reductions in 
CaPPS’s average LLC miss rates as compared to private 
partitioning were 16.92% and 43.02%, respectively. 
C. Analytical Model’s Accuracy Evaluation 
For each benchmark set, we compared the average LLC 
miss rate for the four cores determined by the analytical 
model with the exact miss rate determined by gem5 for each 
configuration in CaPPS’s design space. We calculated the 
average and standard deviation of the miss rate errors across 
the 3,347 configurations. Fig. 5 depicts the results for each 
benchmark set. The black markers indicate the average miss 
rate errors and the gray-shaded upper and lower ranges are 
the corresponding standard deviations. Averaged over all 
sixteen benchmark sets, the average miss rate error and 
standard deviation are -0.73% and 1.30%, respectively.   
Since the analytical model’s cache miss rates are 
inaccurate, we compared the absolute difference between the 
LLC miss rates of the analytical model’s minimum LLC 
miss rate configuration and the actual minimum LLC miss 
rate configuration as determined via exhaustive search. 
Comparing with an exhaustive search is appropriate for 
evaluating the analytical model’s efficacy, which is only 
affected by the estimated miss rate errors in determining the 
optimal configuration. The results indicate that fourteen out 
of sixteen benchmark sets’ differences were less than 1% and 
the maximum and average differences over all benchmark 
sets was negligible, 1.3% and 0.36%, respectively.   
D. Analytical Model’s Time Evaluation 
To evaluate the execution time efficiency of the 
analytical model, we compared the time required to estimate 
the LLC miss rates (including the time for isolated trace 
access generation) for all configurations in the CaPPS design 
space as compared to using gem5. We implemented the 
analytical model in C++ compiled with O3 optimizations. 
We tabulated the user time reported from the Linux time 
command for the simulations running on a Red Hat Linux 
Server v5.2 with a 2.66 GHz processor and 4 gigabytes of 
RAM. Fig. 6 depicts the speedup of the analytical model for 
each benchmark set as compared to gem5. Over all 
benchmark sets, the average speedup is 3,966X, with 
maximum and minimum speedups of 13,554X and 1,277X, 
respectively. For one benchmark set, the time for simulating 
all 3,347 configurations using gem5 was approximately three 
months, and comparatively, the analytical model took only 
 
Figure 4. Average LLC miss rate reductions for CaPPS’s optimal 
configurations compared to even-private-partitioning, fully-shared, and 
private partitioning. 
 
 
Figure 5. The average and standard deviation of the average LLC miss rate 
error determined by the analytical model. 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
Average LLC miss rate reduction 
Compared to even-private-partitioning 
Compared to fully-shared 
Compared with private partitioning 
-4% 
-3% 
-2% 
-1% 
0% 
1% 
2% 
Average and staandard deviation of 
estimated  average L2 miss rate error 
18
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

two to three hours. 
V. 
CONCLUSIONS AND FUTURE WORK  
In this paper, we presented cache partitioning with partial 
sharing (CaPPS)—a novel cache partitioning and sharing 
architecture that improves shared last-level cache (LLC) 
performance with low hardware overhead for chip multi-
processor systems (CMPs). Since CaPPS affords an 
extensive design space for increased optimization potential, 
CaPPS can reduce the average LLC miss rate by as much as 
25% and 17% as compared to baseline configurations and 
private partitioning, respectively. To quickly estimate the 
miss rates of CaPPS’s sharing configurations, we developed 
an offline, analytical model that achieved an average miss 
rate estimation error of only 0.73%. As compared to 
exhaustive exploration (since no heuristics exist) of the 
CaPPS design space to determine the lowest energy cache 
configuration, the analytical model affords an average 
speedup of 3,966X. Finally, CaPPS and the analytical model 
are applicable to CMPs with any number of cores and place 
no limitations on the cache parameters.  
Future work includes extending the analytical model to 
optimize for any design goal, such as performance or energy 
delay product, leveraging the offline analytical results to 
guide online scheduling for performance optimizations in 
real-time embedded systems, including accesses to shared 
address space, incorporating cache prefetching in our 
analytical model, and extending CaPPS to proximity-aware 
cache partitioning for caches with non-uniform accesses. 
ACKNOWLEDGEMENTS  
This work was supported by the National Science 
Foundation (CNS-0953447). Any opinions, findings, and 
conclusions or recommendations expressed in this material 
are those of the author(s) and do not necessarily reflect the 
views of the National Science Foundation. 
 
REFERENCES 
[1] 
ARM 
Cortex-A 
Series, 
http://www.arm.com/products/processors/cortex-a/index.php. 
[2] 
N. Binkert, et. al. The gem5 Simulator, http://gem5.org [retrieved: 
Feb., 2013].  
[3] 
D. Chandra, F. Guo, S. Kim, and Y.  Solihin, “Predicting Inter-Thread 
Cache Contention on a Chip Multi-Processor Architecture”, In 
Proceedings of HPCA, Feb. 2005, pp. 340-351. 
[4] 
X. E. Chen and T. M. Aamodt, “A first-order fine-grained 
multithreaded throughput model”, In Proceedings of HPCA, Feb. 
2009, pp. 329-340. 
[5] 
D. Chiou, D. Chiouy, L. Rudolph, S. Devadas, and B. S. Ang, 
“Dynamic Cache Partitioning via Columnization”, Computation 
Structures Group Memo 430. M.I.T. 2000. 
[6] 
D. Eklov, D. Black-schaffer, and E. Hagersten, “Fast Modeling of 
Shared Cache in Multicore Systems”, In Proceedings of HiPEAC, Jan. 
2011, pp. 147-157. 
[7] 
J. P. Grossman, “A Systolic Array for Implementing LRU 
Replacement,” Project Aries Technical Memo ARIES-TM-18, AI Lab, 
M.I.T., Cambridge, MA, 2002. 
[8] 
G. Hamerly, E. Perelman, J. Lau, and B. Calder, “SimPoint 3.0: Faster 
and More Flexible Program Analysis”, Journal of Instruction-level 
Parallelism, 2005, pp. 1-28. 
[9] 
M. D. Hill and A. J. Smith, “Evaluating Associativity in CPU Caches,” 
IEEE Trans. on Computers, Vol. 38, No. 12, 1989, pp. 1612-1630.  
[10] Intel Core Duo Processor, http://ark.intel.com/products/family/22731. 
[11] K. Johnson and M. Rathbone, “Sun’s Niagara Processor”, NYU 
Multicore Programming, 2010. 
[12] R. E. Kessler and M. D. Hill, “Page Placement Algorithms for Large 
Real-indexed Caches”, ACM Trans. on Computer Systems, Vol. 10, 
No. 4, 1992, pp. 338-359. 
[13] S. Kim, D. Chandra, and Y. Solihin, “Fair Cache Sharing and 
Partitioning in a Chip Multiprocessor Architecture”, In Proceedings of 
PACT, Sep.-Oct. 2004, pp. 111-122. 
[14] H. Lee, S. Cho, and B. Childers, “CloudCache: Expanding and 
Shrinking Private Caches”, In Proceedings of  HPCA, Feb. 2011, pp. 
219-230. 
[15] M. Qureshi and Y. Patt, “Utility-Based Cache Partitioning: A Low-
Overhead, High-Performance, Runtime Mechanism to Partition Shared 
Caches”, In Proceedings of MICRO, Nov. 2006, pp. 423-432. 
[16] SPEC CPU2006. http://www.spec.org/cpu2006 [retrieved: Sep., 2011]. 
[17] S. Srikantaiah, E., T. Zhang, M. Kandemir, M. Irwin, and Y. Xie, 
“MorphCache: a Reconfigurable Adaptive Multi-level Cache 
Hierarchy for CMPs”, In Proceedings of HPCA, Feb. 2011, pp. 231-
242. 
[18] K. Varadarajan, et al., “Molecular Caches: A Caching Structure for 
Dynamic Creation of Application-specific Heterogeneous Cache 
Regions”, In Proceedings of MICRO, Nov. 2006, pp. 433-442. 
 
 
Figure 6. The analytical model’s simulation time speedup compared to 
gem5. 
0 
2000 
4000 
6000 
8000 
10000 
12000 
14000 
Speedup 
19
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

