Analytical Modeling of Partially Shared Caches in Embedded CMPs 
 
Wei Zang and Ann Gordon-Ross* 
 Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, 32611, USA  
weizang@ufl.edu & ann@ece.ufl.edu  
*Also with the NSF Center for High-Performance Reconfigurable Computing (CHREC) at the University of Florida 
 
Abstractâ€”In modern ubiquitous devices, optimizing shared last-
level caches (LLCs) in embedded chip multi-processor systems 
(CMPs) is critical due to the increased contention for limited 
cache space from multiple cores. We propose cache partitioning 
with partial sharing (CaPPS) to reduce LLC contention and 
improve utilization. CaPPS can reduce the average LLC miss 
rate by 25% and 17% as compared to baseline configurations 
and private partitioning, respectively. To facilitate fast design 
space exploration, we develop an analytical model to quickly 
estimate the miss rates of all CaPPS configurations with an 
average error of only 0.73% and with an average speedup of 
3,966X as compared to a cycle-accurate simulator. 
Keywordsâ€”cache partitioning; analytical modeling 
 
I. 
INTRODUCTION  
Many chip multi-processor systems (CMPs) leverage 
shared last-level caches (LLCs) (e.g., second-/third-level), 
such as the ARM Cortex-A, Intel Xeon, and Sun T2 
[1][10][11]. To improve cache utilization, LLCs should be 
large enough to accommodate all sharing coresâ€™ data, but 
long access latencies and high power consumption typically 
precludes large LLCs from embedded systems with strict 
area/energy/power 
constraints. 
Since 
battery-operated 
devices (e.g., cell phones, tablets, laptops, etc.) have limited 
energy reserves and satisfying the applicationsâ€™ quality-of-
services (QoSs) is typically required, optimizing small 
LLCsâ€™ performance is significantly more challenging due to 
contention for limited cache space.  
Shared LLCs afford high cache utilization and no 
coherence overhead, however, high contention and unfair 
cache utilization degrades performance. A coreâ€™s LLC 
occupancy (utilized space) is flexible and dictated by the 
coreâ€™s applicationâ€™s demands. Cores with high LLC 
requirements occupy a large LLC area and cause high, 
potentially unfair, contention. For example, streaming 
multimedia applications occupy the LLC with a large 
amount of single-accessed data and unfairly evict the other 
coresâ€™ data, thus increasing LLC miss rates. For example, 
this unfair cache utilization is common in mobile systems 
when a local music/movie player and other web-service 
applications are co-executed.       
 To eliminate shared LLC contention, cache partitioning 
[5][15][18] partitions the cache, allocates quotas (a subset of 
partitions) to the cores, and optionally configures the 
partitions/quotas (e.g., size and/or associativity [15][18]) to 
the allocated coreâ€™s requirements. Each coreâ€™s cache 
occupancy is constrained to the coreâ€™s quota to ensure fair 
utilization. Set partitioning partitions and allocates quotas at 
the cache set granularity and is typically implemented using 
operating system (OS)-based page coloring [12]. However, 
due to this OS modification requirement, hardware-based 
way partitioning is more widely used. Way partitioning 
partitions and allocates quotas at the cache way granularity 
[15][18]. However, way partitioning for shared LLCs 
typically uses private partitioning, which restricts quotas for 
exclusive use by the allocated core only and can lead to poor 
cache utilization if a core does not occupy the coreâ€™s entire 
allocated quota.   
In this paper, we propose to improve way partitioningâ€™s 
cache utilization using cache partitioning with partial sharing 
(CaPPS). CaPPS improves cache utilization via sharing 
configuration, which enables a coreâ€™s quota to be configured 
as private, partially shared with a subset of cores, or fully 
shared with all other cores. Whereas sharing configuration 
increases the design space and thus increases optimization 
potential, this large design space significantly increases 
design space exploration time. To facilitate design space 
exploration, we develop an offline analytical model to 
quickly estimate cache miss rates for all partitioning and 
sharing configurations, which enables determining LLC 
configurations for any optimization that evaluates cache miss 
rates (e.g., performance, energy, energy delay product, 
power, etc.). The analytical model probabilistically predicts 
the miss rates when multiple applications are co-executing 
using the isolated cache access distribution for each 
application (i.e., the application is run in isolation with no 
co-executing applications). Although several previous works 
[3][4][6] have developed analytical models to predict shared 
LLC contention offline, these worksâ€™ caches where 
completely shared by all cores and did not consider partial 
sharing, which vastly increases the design space and thus 
optimization potential. Due to CaPPSâ€™s extensive design 
space, experiments reveal that CaPPS can reduce the average 
LLC miss rates by as much as 25% and 17% as compared to 
baseline configurations and private partitioning, respectively. 
The analytical model estimates cache miss rates with an 
average error of only 0.73% and is 3,966X faster on average 
than a cycle-accurate simulator.        
II. 
RELATED WORK 
Since CaPPS uses way partitioning and we developed an 
analytical model to predict the shared waysâ€™ cache 
contention, we compare our work with prior work in these 
areas.  
For way partitioning, Qureshi and Patt [15] developed 
utility-based cache partitioning (UCP) that used an online 
monitor to track the cache misses for all possible numbers of 
13
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

ways assigned to each core. Greedy and refined heuristics 
determined the coresâ€™ quotas. Varadarajan et al. [18] 
partitioned the cache into small direct-mapped cache units, 
which were privately assigned to the cores and the cache 
partitions had configurable size, block size, and associativity. 
Kim et al. [13] developed static and dynamic cache 
partitioning 
for 
fairness 
optimization. 
Static 
cache 
partitioning used the cache accessâ€™s stack distance profile to 
determine 
the 
coresâ€™ 
requirements. 
Dynamic 
cache 
partitioning increased/decreased the coresâ€™ quotas in 
accordance with the miss rate changes between evaluation 
intervals. Private LLCs also benefit from way partitioning. In 
CloudCache [14], the private caches were partitioned, but a 
core could share nearby coresâ€™ (limited access latencies) 
private caches. MorphCache [17] partitioned the level two 
and level three caches and allowed subsets of coresâ€™ private 
caches to be merged and fully shared by the subset. Although 
some of these prior works in private LLC partitioning 
[14][17] enabled a core to share other coresâ€™ quotas, CaPPS 
is more flexible than these works by enabling a portion/all of 
a coreâ€™s quota to be shared with any subset of cores.   
Prior works on analytical modeling to determine cache 
miss rates targeted only fully shared caches. Chandra et al. 
[3] proposed a model using access traces for isolated threads 
to predict inter-thread contention for a shared cache. Reuse 
distance profiles were analyzed to predict the extra cache 
misses for each thread due to cache sharing, but the model 
did not consider the interaction between cycles per 
instruction (CPI) variations and cache contention. Eklov et 
al. [6] proposed a simpler model that calculated the CPI 
considering the cache misses caused by contention by 
predicting the reuse distance distribution of an application 
when co-executed with other applications based on the 
isolated reuse distance distribution of each application. Chen 
and Aamodt [4] proposed a Markov model to estimate the 
cache miss rates for multi-threaded applications with inter-
thread communication.  
Analytically predicting the cache miss rate for CaPPS is 
more challenging than prior works, since in CaPPS, only the 
interleaved LLC accesses of other cores that pollute the 
partially shared ways affect the coreâ€™s miss rate. Determining 
the effects of these interleaved accesses on the miss rate 
introduces extensive complexity.  
III. CACHE PARTITIONING WITH PARTIAL SHARING 
To accommodate the LLC requirements for multiple 
applications co-executing on different cores, CaPPS 
partitions the shared LLC at the way granularity and 
leverages sharing configuration to allocate the partitions to 
each coreâ€™s quota. To facilitate fast design space exploration, 
an analytical model estimates the cache miss rates for the 
CaPPS configurations using the applicationsâ€™ isolated LLC 
access traces. We assume that the cores execute different 
applications in independent address spaces, thus there is no 
shared instruction/data address or coherence management, 
which is a common case in mobile systems running disparate 
applications and is similar to assumptions made in prior 
works [3][6].  
A. Architecture and Sharing Configurations 
CaPPSâ€™s sharing configurations enable a coreâ€™s quota to 
be configured as private, partially shared with a subset of 
cores, or fully shared with all other cores. Fig. 1 (a)-(c) 
illustrates sample configurations, respectively, for a 4-core 
CMP (C1 to C4
CaPPS uses the least recently used (LRU) replacement 
policy, but we note that the analytical model can be extended 
to approximate estimations for other replacement policies, 
such as pseudo-LRU. To reduce the sharing configurability 
with no effect on cache performance and to minimize 
contention, cores share an arbitrary number of ways starting 
with the LRU way, then second LRU way, and so on since 
these ways are least likely to be accessed. For example, in 
) and an 8-way LLC: (a) each coreâ€™s quota 
has a configurable number of private ways; (b) the coresâ€™ 
quotas are partially shared with subsets of cores; and (c) all 
of the four cores fully share all of the ways.  
Fig. 1 (b) two of C1â€™s ways are shared with C2., therefore, 
C1â€™s two most recently used (MRU) blocks are cached in 
C1â€™s two private ways, and the two LRU blocks are cached 
in the two ways shared with C2 and these two LRU blocks 
are the only replacement candidates for C2
[7]
â€™s accesses. 
Maintaining this LRU ordering and determining replacement 
candidate can be easily implemented using a linked list or 
systolic array implementation 
 for conventional LRU 
caches with the integration of column caching [5] to achieve 
low hardware overhead and without increasing the cache 
access time. Since the hardware implementation is 
straightforward and is not the focus of this paper, we omit 
the implementation details for brevity.        
B. Analytical Modeling Overview 
For applications with fully/partially shared ways, the 
analytical model probabilistically determines the miss rates 
using the isolated cache access distributions for the co-
executing applications. These distributions are recorded 
during isolated access trace processing. The isolated LLC 
access traces can be generated with a simulator/profiler by 
running each application in isolation on a single core with all 
other cores idle. For applications with only private ways, 
there is no cache contention and the miss rate can be directly 
determined from the isolated LLC access trace distribution.   
Fig. 2 illustrates the contention in the shared ways using 
sample time-ordered isolated (C1, C2) and interleaved/co-
executed (C1&C2) access traces to an arbitrary cache set 
from cores C1 and C2. C1â€™s and C2â€™s accesses are denoted as 
Xi and Yi, respectively, where i differentiates accesses to 
unique cache blocks. The first access to X3 and the second 
access to X1 occurred at times ğ‘¡1 and ğ‘¡2, respectively. C1
8-way LLC
8-way LLC
8-way LLC
Shared by all of 
the four cores
(a)
(b)
(c)
Private 
for C1
Private 
for C2
Private 
for C3
Private for C4
Private 
for C1
Private for C2
Shared by 
C1 & C2
Private for C3
Shared by 
C3 & C4
â€™s 
 
Figure 1. Three sample sharing configurations: (a) the coresâ€™ quotas are 
private; (b) some ways are partially shared with a subset of cores; and (c) the 
entire LLC is fully shared with all cores. 
14
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

second access to X1 will be a cache hit if C1â€™s number of 
private ways is greater than or equal to five because four 
unique blocks are accessed between the two accesses to X1. 
Alternatively, if C1â€™s number of private ways is smaller than 
five and C1 shares ways with C2, X1â€™s hit/miss is dictated by 
the interleaved accesses from C2. For example, if C1 has six 
allocated ways and two of the LRU ways are shared with C2, 
X3 evicts X1 from C1â€™s private way into a shared way. 
Therefore, C2â€™s accesses between ğ‘¡1 and ğ‘¡2 dictates whether 
X1 has been evicted from the cache or not. If C2â€™s accesses 
between ğ‘¡1 and ğ‘¡2 evict two or more blocks into the shared 
ways, X1
In order to determine the contention effects to C
â€™s second access will be a cache miss.   
1â€™s miss 
rate, C1 and C2
â€™s number of accesses ğ‘›1 (Section III
.D.a) 
and ğ‘›2 (Section III.D.b), respectively, during the time period 
(ğ‘¡1, ğ‘¡2) must be estimated. Since the number of blocks ğ‘… 
from ğ‘›2 evicted into the shared ways dictates whether C1â€™s 
blocks (e.g., X1
 in Fig. 4
) are still in the shared ways, we 
calculate the probability ğ‘(ğ‘›2, ğ‘…) that ğ‘… number of blocks 
are evicted into the shared ways (Section III.D.c) to estimate 
C1
â€™s miss rate (Section III
.D.d).          
C. Isolated Access Trace Processing 
To accumulate the isolated cache access distribution, we 
record the reuse distance and stack distance for each access 
in the isolated LLC access trace, which can be obtained 
using a stack-based trace-driven simulator [9]. For an 
accessed address T that maps to a cache set, the reuse 
distance ğ‘Ÿ is the number of accesses to that set between this 
access to T and the previous access to any address in the 
same block as T. The stack distance ğ‘‘ is the number of 
unique block addresses, or conflicts, in this set of accesses. 
For example, in Fig. 2, C1â€™s second access to X1
In each cache set, we accumulate the number of accesses 
ğ‘ğ‘‘ for each stack distance ğ‘‘ (ğ‘‘ âˆˆ [0, ğ´]), where ğ´ is the LLC 
associativity. We accumulate the number of accesses with 
ğ‘‘ > ğ´ in ğ‘ğ´ together with the number of accesses with ğ‘‘ = ğ´, 
since all accesses with ğ‘‘ â‰¥ ğ´ are cache misses in any 
configuration. Given this information, for any access, the 
probabilistic information for the accessâ€™ stack distance is 
ğ‘(ğ‘‘ < ğ‘‘ğ‘– ) = (âˆ‘
ğ‘‘=ğ‘‘ğ‘–âˆ’1 ğ‘ğ‘‘
ğ‘‘=0
) (âˆ‘ ğ‘ğ‘‘)
àµ—
 
and 
ğ‘(ğ‘‘ â‰¥ ğ‘‘ğ‘– ) = 1 âˆ’
ğ‘(ğ‘‘ < ğ‘‘ğ‘– ), (âˆ€ğ‘‘ğ‘– âˆˆ [1, A]). For all of the accesses for each ğ‘‘, 
we accumulate a histogram of different ğ‘Ÿ and calculate the 
average ğ‘ŸÌ… over all ğ‘Ÿ.     
 has ğ‘Ÿ = 7 
and ğ‘‘ = 4. 
The analytical model uses the base (best case) CPU 
cycles ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ ğ‘ğ‘ğ‘ ğ‘’ to calculate the CPU cycles required to 
complete the application when co-executed with other 
applications. ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ ğ‘ğ‘ğ‘ ğ‘’ assumes that all LLC accesses are 
hits. An applicationâ€™s total number of CPU cycles ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ ğ‘’ğ‘¥ğ‘’ 
are recorded in the isolated execution to calculate ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ ğ‘ğ‘ğ‘ ğ‘’ 
using ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ ğ‘ğ‘ğ‘ ğ‘’ = ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ ğ‘’ğ‘¥ğ‘’ âˆ’ ğ‘šğ‘’ğ‘¥ğ‘’ Â· ğ¿ğ¿ğ¶ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦, where ğ‘šğ‘’ğ‘¥ğ‘’ 
is the number of LLC misses in the applicationâ€™s isolated 
execution and ğ¿ğ¿ğ¶ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦ is the delay cycles incurred by an 
LLC miss.          
Since the access distributions across the cache sets are 
different, the distributions are individually accumulated and 
recorded for each set to estimate the number of misses in 
each setâ€™s accesses. Since the analysis is the same for all 
cache sets, we present the analytical model for one arbitrary 
cache set. 
D. Analysis of the Shared Waysâ€™ Contention 
First, we describe the analytical model to analyze the 
shared waysâ€™ contention for a sample CMP with two cores 
C1 and C2 and then generalize the analytical model to any 
number of cores. A sharing configuration allocates ğ¾C1 
number of ways to core C1, where ğ¾ğ‘ƒ,C1 ways are private and 
the remaining ğ¾ğ‘† (ğ¾ğ‘† = ğ¾C1 âˆ’ ğ¾ğ‘ƒ,C1) ways are shared with 
core C2. ğ¾C2 and ğ¾ğ‘ƒ,C2 similarly denote these values for C2. 
For C1, all accesses with a stack distance ğ‘‘ â‰¤ ğ¾ğ‘ƒ,C1 âˆ’ 1 result 
in cache hits and all accesses with ğ‘‘ â‰¥ ğ¾C1 are cache misses. 
The cache hit/miss determination of the accesses where 
ğ¾ğ‘ƒ,C1 â‰¤ ğ‘‘ â‰¤ ğ¾C1 âˆ’ 1 depends on the interleaved accesses from 
C2, and the following subsections elaborate on the estimation 
method for these accesses. If C1 only has private ways, then 
ğ¾ğ‘ƒ,C1 = ğ¾C1, and these estimations are not required since the 
number of misses for C1
a. Calculation of ğ‘›1  
 can be directly calculated using 
âˆ‘
ğ‘ğ‘‘,C1
ğ‘‘=ğ¾C1âˆ’1
ğ‘‘=0
. 
For an arbitrary stack distance ğ· in [ğ¾ğ‘ƒ,C1, ğ¾C1 âˆ’ 1], the 
associated ğ‘ŸÌ… was determined during isolated access trace 
processing. This subsection presents the calculation of ğ‘›1 for 
C1
Fig. 3
â€™s accesses with stack distance ğ· based on ğ‘Ÿ. 
 depicts C1â€™s isolated access trace to an arbitrary 
cache set, where the second access to X1 has a stack distance 
ğ· and reuse distance ğ‘ŸÌ…. X3â€™s access evicts X1 from C1â€™s 
private ways, therefore, the numbers of conflicts before and 
after X3 are (ğ¾ğ‘ƒ,C1 âˆ’ 1) and (ğ· âˆ’ (ğ¾ğ‘ƒ,C1 âˆ’ 1)), respectively. 
Confi denotes the first access of the i-th conflict with X1. We 
denote the number of accesses before X3 
time
Access trace in one cache set
      X1           X2             X3            X3         X2         X4                X5          X1
C1
         Y1           Y2          Y3                                       Y4   Y5        Y1
C2
X1     Y1  X2      Y2    X3  Y3      X3         X2         X4   Y4   Y5 X5   Y1   X1
C1&C2
t1
t2
as ğ‘›0, which can be 
any 
integer 
in 
[ğ¾ğ‘ƒ,C1 âˆ’ 1, ğ‘ŸÌ… âˆ’ (ğ· âˆ’ ğ¾ğ‘ƒ,C1) âˆ’ 2]. 
After 
 
 
Figure 2. Two coresâ€™ isolated (C1, C2) and interleaved (C1&C2
X1           â€¦...           X3       ...   Conf(Kp,C1-1)      â€¦...  Conf3     ...    Conf2    ...   Conf1      X1
time
dâ‰¥1
dâ‰¥2
dâ‰¥3
dâ‰¥Kp,C1-1
d<1
d<2
d<Kp,C1-1
...
...
n0 
n1 
C1
t1
t2
Access trace in one cache set
n accesses, D conflicts  
) access 
traces for an arbitrary cache set. 
 
Figure 3. C1â€™s isolated access trace to an arbitrary cache set for calculating ğ’ğŸ. 
15
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

determining the probability ğ‘(ğ‘›0, (ğ¾ğ‘ƒ,C1 âˆ’ 1)) for each ğ‘›0 
(where ğ¾ğ‘ƒ,C1 âˆ’ 1 indicates the number of conflicts in the ğ‘›0 
accesses), we can calculate ğ‘›0â€™s expected value ğ‘›à´¤0 for the 
evaluated configurationâ€™s associated ğ¾ğ‘ƒ,C1 using ğ‘›à´¤0 =
âˆ‘(ğ‘›0 â‹… ğ‘(ğ‘›0, (ğ¾ğ‘ƒ,C1 âˆ’ 1))), and ğ‘›1â€™s expected value is: 
ğ‘›à´¤1 = ğ‘ŸÌ… âˆ’ ğ‘›à´¤0 âˆ’ 1. 
For a particular ğ‘›0 âˆˆ [ğ¾ğ‘ƒ,C1 âˆ’ 1, ğ‘ŸÌ… âˆ’ (ğ· âˆ’ ğ¾ğ‘ƒ,C1) âˆ’ 2], the 
probability is:  
ğ‘ á‰€ğ‘›0, àµ«ğ¾ğ‘ƒ,C1 âˆ’ 1àµ¯á‰ = ğ‘(ğ¸ğ´, ğ¸ğµ|ğ¸ğ¶) = ğ‘ğ‘ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘’(ğ¸ğ´) â‹… ğ‘ğ‘ğ‘“ğ‘¡ğ‘’ğ‘Ÿ(ğ¸ğµ)
ğ‘ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™(ğ¸ğ¶)
     (1) 
where ğ¸ğ´ is the event that the ğ‘›0 accesses have exactly 
(ğ¾ğ‘ƒ,C1 âˆ’ 1) conflicts and ğ¸ğµ is the event that the ğ‘›1 accesses 
have exactly (ğ· âˆ’ (ğ¾ğ‘ƒ,C1 âˆ’ 1)) conflicts. ğ‘ğ‘ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘’(ğ¸ğ´) and 
ğ‘ğ‘ğ‘“ğ‘¡ğ‘’ğ‘Ÿ(ğ¸ğµ) are the occurrence probabilities of ğ¸ğ´ and ğ¸ğµ, 
respectively. ğ¸ğ¶ is the event that the ğ‘Ÿ accesses have exactly 
ğ· conflicts and ğ‘ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™(ğ¸ğ¶)  is the probability of ğ¸ğ¶â€™s 
occurrence, which is the summation of (ğ‘ğ‘ğ‘“ğ‘¡ğ‘’ğ‘Ÿ(ğ¸ğµ) â‹…
ğ‘ğ‘ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘’(ğ¸ğ´)) for all ğ‘›0. To calculate ğ‘ğ‘ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘’(ğ¸ğ´) and 
ğ‘ğ‘ğ‘“ğ‘¡ğ‘’ğ‘Ÿ(ğ¸ğµ), we examine the sufficient conditions that ğ¸ğ´ and 
ğ¸ğµ occur. In Fig. 3, the first access following X1 must be 
different from X1 (for ğ· > 0), which is Conf1 satisfying 
ğ‘‘ â‰¥ 1, since Conf1 has at least one conflict: X1. The second 
conflict Conf2 satisfies ğ‘‘ â‰¥ 2, since Conf2 has at least two 
conflicts: Conf1 and X1. The accesses between Conf1 and 
Conf2 satisfy ğ‘‘ < 1 since these accesses can only be Conf1. 
Conf3 satisfies ğ‘‘ â‰¥ 3 since Conf3 has at least three conflicts: 
Conf2, Conf1, and X1. The accesses between Conf2 and 
Conf3 satisfy ğ‘‘ < 2, since these conflicts can only be Conf2 
or Conf1, etc. Similarly, ConfKp,C1-1 satisfies ğ‘‘ â‰¥ (ğ¾ğ‘ƒ,C1 âˆ’ 1) 
and the accesses between X3 and ConfKp,C1-1
where ğ‘†ğ‘ is a set including all ğ‘âƒ— satisfying âˆ‘ ğ‘ğ‘– = ğ‘›0 âˆ’
(ğ¾ğ‘ƒ,C1 âˆ’ 1). Similarly, defining ğ‘áˆ¬âƒ— = (ğ‘0, ğ‘1, â€¦ , ğ‘ğ·âˆ’ğ¾ğ‘,C1) where 
ğ‘ğ‘– âˆˆ [0, ğ‘›1 âˆ’ (ğ· âˆ’ ğ¾ğ‘ƒ,C1 + 1)], ğ‘ğ‘ğ‘“ğ‘¡ğ‘’ğ‘Ÿ(ğ¸ğµ) is: 
ğ‘ğ‘ğ‘“ğ‘¡ğ‘’ğ‘Ÿ(ğ¸ğµ) = á‰
à·‘
ğ‘àµ«ğ‘‘ â‰¥ ğ‘– + ğ¾ğ‘ƒ,C1àµ¯
ğ‘–=ğ·âˆ’ğ¾ğ‘ƒ,C1
ğ‘–=0
á‰‘
â‹… á‰ à· á‰Œ
à·‘
ğ‘àµ«ğ‘‘ < ğ‘– + ğ¾ğ‘ƒ,C1àµ¯
ğ‘ğ‘–
ğ‘–=ğ·âˆ’ğ¾ğ‘ƒ,C1
ğ‘–=0
á‰
âˆ€ğ‘áˆ¬âƒ—âˆˆğ‘†ğ‘
á‰‘    (3) 
 satisfy ğ‘‘ <
(ğ¾ğ‘ƒ,C1 âˆ’ 1). Thus, defining ğ‘âƒ— = (ğ‘1, ğ‘2, â€¦ , ğ‘ğ¾ğ‘,C1âˆ’1) where 
ğ‘ğ‘– âˆˆ [0, ğ‘›0 âˆ’ (ğ¾ğ‘ƒ,C1 âˆ’ 1)], ğ‘ğ‘ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘’(ğ¸ğ´) is: 
ğ‘ğ‘ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘’(ğ¸ğ´) = á‰
à·‘
ğ‘(ğ‘‘ â‰¥ ğ‘–)
ğ‘–=ğ¾ğ‘ƒ,C1âˆ’1
ğ‘–=1
á‰‘ â‹… á‰ à· á‰Œ
à·‘
ğ‘(ğ‘‘ < ğ‘–)ğ‘ğ‘–
ğ‘–=ğ¾ğ‘ƒ,C1âˆ’1
ğ‘–=1
á‰
âˆ€ğ‘áˆ¬âƒ—âˆˆğ‘†ğ‘
á‰‘  (2) 
where ğ‘†ğ‘ is a set including all ğ‘áˆ¬âƒ— satisfying âˆ‘ ğ‘ğ‘– = ğ‘›1 âˆ’ (ğ· âˆ’
ğ¾ğ‘ƒ,C1 + 1). 
b. Calculation of ğ‘›2 
To determine the contention effect from C2, the expected 
number of accesses ğ‘›à´¤2 from C2 is estimated based on the 
ratio of the number of cache set accesses from C1 and C2 
where âˆ‘ ğ‘ğ‘‘,C1and âˆ‘ ğ‘ğ‘‘,ğ‚2 are the total number of LLC 
accesses from C
per 
cycle: 
ğ‘›à´¤1
ğ‘›à´¤2
=
âˆ‘ ğ‘ğ‘‘,ğ¶1 ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ 
à·£ ğ¶1
â„
âˆ‘ ğ‘ğ‘‘,ğ¶2 ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ 
â„ à·£ ğ¶2
                                      (4) 
1 and C2, respectively. ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ 
à·£ C1is the 
number of CPU cycles required to execute the application on 
C1 when C2
where ğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦ğ‘ğ‘¢ğ‘ _ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› is the delay imposed by the shared 
bus contention from the higher level caches (closer to the 
CPU) of each core to the shared LLC. ğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦ğ‘ğ‘¢ğ‘ _ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› is  
derived by calculating the bus contention probability that 
another core is sending a read/write request to the LLC and 
the LLC is returning that coreâ€™s requested block 
simultaneously with the evaluated coreâ€™s bus request. The 
bus contention probability is dictated by each coreâ€™s bus 
request probability, which is equal to the total number of bus 
requests generated from the coreâ€™s higher level cache misses 
divided by ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ Cğš¤
à·£
. 
 is co-executing another application, and 
ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ 
à·£ C2 is similarly defined. ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ Cğš¤
à·£
 can be calculated 
using ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ ğ‘ğ‘ğ‘ ğ‘’ and the number of LLC misses ğ‘šà· estimated 
with the contention: 
ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ Cğš¤
à·£
= ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ ğ‘ğ‘ğ‘ ğ‘’ + ğ‘šà· Â· ğ¿ğ¿ğ¶ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦ +  ğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦ğ‘ğ‘¢ğ‘ _ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›     (5) 
c. Calculation of  ğ‘(ğ‘›2, ğ‘…) 
ğ‘(ğ‘›2, ğ‘…) is the probability that ğ‘… number of blocks are 
evicted from C2
(4)
â€™s private ways in the ğ‘›2 accesses. Directly 
using the expected ğ‘›2 to calculate ğ‘(ğ‘›à´¤2 , ğ‘…) will introduce a 
large bias (approximate 10% error) in the estimated LLC 
miss rate, since different values of ğ‘›2 result in different 
hit/miss determinations and using one expected value ğ‘›à´¤2 will 
estimate all ğ‘›2 as hits/misses. Thus, we model ğ‘›2 using a 
Poisson distribution ğ‘(ğ‘›2) = ğ‘ƒğ‘œğ‘–ğ‘ ğ‘ ğ‘œğ‘›(ğ‘›2, ğœ†), where ğœ† is ğ‘›à´¤2 if 
the LLC is accessed randomly. However, since the LLCâ€™s 
accesses are generally not random and not uniformly 
distributed in time (which makes 
 valid), we use an 
empirical variable ğ‘’ to adjust ğœ† to ğœ† = ğ‘›à´¤2/ğ‘’. Our experiments 
indicated that ğ‘’ = 5 was appropriate for our training 
benchmark suite, which contains a wide variety of typical 
CMP applications, and is thus generally applicable. Since the 
range of ğ‘›2 is infinite in the Poisson distribution, and ğ‘›2 with 
very small ğ‘(ğ‘›2) has minimal effect on the miss rate 
estimation, we only consider the ğ‘›2 with ğ‘(ğ‘›2) > 0.01 and 
calculate the associated ğ‘(ğ‘›2, ğ‘…).   
To calculate ğ‘(ğ‘›2, ğ‘…) for an arbitrary ğ‘›2, ğ‘… is determined 
by evaluating the ğ‘›2 accesses in chronological order with an 
initial value of ğ‘… = 0. If there is one access with ğ‘‘ > ğ¾ğ‘ƒ,C2 +
ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ ğ‘…, fetching this address into C2
with the initial case ğ‘(ğ‘›2 = 0, ğ‘… = 0) = 1. 
â€™s private ways will 
evict one block into the shared ways and thus ğ‘… is 
incremented by 1. Therefore, we can calculate ğ‘(ğ‘›2, ğ‘…) 
inductively:  
ğ‘(ğ‘›2, ğ‘…) =
â©
âª
â¨
âª
â§ğ‘(ğ‘›2 âˆ’ 1, ğ‘… âˆ’ 1) â‹… ğ‘ á‰€ğ‘‘ â‰¥ ğ¾ğ‘ƒ,C2 + (ğ‘… âˆ’ 1)á‰ ,       ğ‘… = ğ‘›2 
ğ‘(ğ‘›2 âˆ’ 1, ğ‘…) â‹… ğ‘àµ«ğ‘‘ < ğ¾ğ‘ƒ,C2 + ğ‘…àµ¯                                            
    +ğ‘(ğ‘›2 âˆ’ 1, ğ‘… âˆ’ 1) â‹… ğ‘ á‰€ğ‘‘ â‰¥ ğ¾ğ‘ƒ,C2 + (ğ‘… âˆ’ 1)á‰ , ğ‘… < ğ‘›2
ğ‘(ğ‘›2 âˆ’ 1, ğ‘…) â‹… ğ‘àµ«ğ‘‘ < ğ¾ğ‘ƒ,C2 + ğ‘…àµ¯,                             ğ‘… = 0 
  
d. Calculation of the LLC Miss Rates 
Considering the impact of ğ‘… to the accesses with stack 
distance ğ‘‘ âˆˆ [ğ¾ğ‘ƒ,C1, ğ¾C1 âˆ’ 1], the number of cache hits for C1 
is: 
16
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

â„C1
à·¢ =
à·
ğ‘ğ‘‘,C1
ğ‘‘=ğ¾ğ‘,C1âˆ’1
ğ‘‘=0
+
à·
àµ®ğ‘ğ‘‘,C1
ğ‘‘=ğ¾C1âˆ’1
ğ‘‘=ğ¾ğ‘,C1
â‹…
à·
àµ®á‰Œ
à·
ğ‘(ğ‘›2,ğ‘…)
ğ‘…=ğ¾C1âˆ’ğ‘‘âˆ’1
ğ‘…=0
á‰ â‹… ğ‘(ğ‘›2)àµ²
âˆ€ğ‘›2: ğ‘(ğ‘›2)>0.01
àµ²    (7) 
After accumulating â„C1
à·¢  for all cache sets, the number of LLC 
misses ğ‘šC1
à·  and the LLC miss rates can be determined. 
Finally, we generalize the analytical model to estimate 
the LLC miss rate for any core Ci when j additional cores 
(denoted as Cj) share cache ways with Ci by calculating the 
expected number of accesses ğ‘›à´¤Cğ‘— from the additional cores 
during the time (ğ‘¡1, ğ‘¡2) and then estimating ğ‘(ğ‘›Cğ‘—, ğ‘…Cğ‘—) 
similarly as estimating ğ‘›à´¤2 and ğ‘(ğ‘›2, ğ‘…) for C2
(6)
. The 
generalized expression of 
 is: 
â„Cğš¤
à·¢ =
à·
ğ‘ğ‘‘,Cğ‘–
ğ‘‘=ğ¾ğ‘,Cğ‘–âˆ’1
ğ‘‘=0
+
à·
àµ«ğ‘ğ‘‘,Cğ‘– â‹… ğ‘â„àµ¯
ğ‘‘=ğ¾Cğ‘–âˆ’1
ğ‘‘=ğ¾ğ‘,Cğ‘–
                 (8) 
where: 
ğ‘â„ = à· á‰Œà·‘ á‰€ğ‘(ğ‘›Cğ‘—) â‹… ğ‘(ğ‘›Cğ‘—, ğ‘…Cğ‘—)á‰
Cğ‘—âˆˆCáˆ¬áˆ¬âƒ—
á‰
âˆ€Cáˆ¬áˆ¬âƒ—âˆˆğ‘†C 
                      (9)                                                      
where Cáˆ¬âƒ— = (ğ‘›C1, ğ‘›C2, â€¦ , ğ‘›ğ‘—) with ğ‘(ğ‘›Cğ‘—) > 0.01 and ğ‘†C is a set 
including all Cáˆ¬âƒ— satisfying âˆ‘ ğ‘…Cğ‘— â‰¤ ğ¾Cğ‘– âˆ’ ğ‘‘ âˆ’ 1. 
 According to (5), a circular dependency exists where 
ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ 
à·£  is used to estimate ğ‘šà· and ğ‘šà· is used to calculate 
ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ 
à·£ . The solution cannot be represented using a closed 
form, thus we iteratively solve for ğ‘šà·. The initial value of ğ‘šà· 
is acquired assuming there is no contention (i.e., all ğ¾Cğ‘– 
number of ways are privately used by Ci
 
(5)
), and ğ‘šà· is used in 
 to calculate the initial value of ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ 
à·£ . ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ 
à·£  is provided 
back into the analytical model to update ğ‘šà· and the new ğ‘šà· is 
used to update ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ 
à·£ . This iterative process continues until a 
stable ğ‘šà· (with a precision of 0.001%) is achieved. 
Experimental results indicated that only four iterations were 
required for the results to converge.  
The analytical modelâ€™s runtime complexity depends on 
the evaluated sharing configuration and the isolated cache 
access distribution for each application. Due to the large 
number of complex and interdependent variables and 
unknowns, the complexity of the model is intractable, thus in 
our experiments, we evaluate the analytical modelâ€™s 
measured execution time.     
IV. 
EXPERIMENT RESULTS  
We verified the advantages of CaPPS as compared to two 
baseline configurations and private partitioning. We also 
verified the accuracy of our estimated LLC miss rates 
obtained via the analytical model and evaluated the 
analytical modelâ€™s ability to determine the optimal 
(minimum LLC miss rate) configuration in the CaPPS design 
space. Additionally, we illustrate the analytical modelâ€™s 
efficiency by comparing the time required to calculate the 
LLC miss rates as compared to using a cycle-accurate 
simulator that generates the exact cache miss rates for all 
configurations.  
A. Experiment Setup 
We used twelve benchmarks from the SPEC CPU2006 
suite [16], which were compiled to Alpha_OSF binaries and 
executed using â€œrefâ€ input data sets. Due to incorrect 
execution, we could not evaluate the complete suite. Even 
though our work is targeted towards embedded systems, we 
did not use embedded system benchmark suites since these 
suites contain only small kernels, which do not sufficiently 
access the LLC, and do not represent our targeted embedded 
CMP domain. Since complete execution of the large SPEC 
benchmarks prohibits exhaustive examination of the entire 
CaPPS design space, and since most embedded benchmarks 
have stable behavior during execution, for each SPEC 
benchmark, we performed phase classification using 
SimPoint [8] to select 500 million consecutive instructions 
with similar behavior as the simulation interval to mimic an 
embedded application with high LLC occupancy. 
We generated the exact cache miss rates for comparison 
purposes using gem5 [2] and modeled four in-order cores 
with the TimingSimple CPU model, which stalls the CPU 
when fetching from the caches and memory. Each core had 
private level-one (L1) instruction and data caches. The 
unified level-two (L2) cache and all lower level memory 
hierarchy components were shared among all cores. We 
modified the L2 cache replacement operation in gem5 to 
model CaPPS. TABLE I shows the parameters used for each 
system component. Since four cores shared the eight-way 
LLC (i.e., L2 cache), CaPPSâ€™s design space had 3,347 
configurations. 
Before CaPPS simulation, we executed each benchmark 
in isolation during the benchmarkâ€™s simulation interval and 
recorded the isolated LLC access traces and the CPU cycles 
ğ¶ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ ğ‘’ğ‘¥ğ‘’. For CaPPS simulation, we arbitrarily selected four 
benchmarks to be co-executed, which formed a benchmark 
set, and we evaluated sixteen benchmark sets. Since the four 
benchmarksâ€™ simulation intervals were at different execution 
points, we forced the four cores to simultaneously begin 
executing at each benchmarkâ€™s associated simulation 
intervalâ€™s starting instruction using a full-system checkpoint. 
The full-system checkpoint was created by aggregating the 
isolated-benchmark checkpoints, which were generated by 
fast-forwarding the benchmark to the starting instruction of 
the benchmarkâ€™s associated simulation interval when the 
benchmark was executed in isolation.   
TABLE I. CMP SYSTEM PARAMETERS 
CPU 
2 GHz clock, single thread  
L1 instruction 
cache 
Private, total size of 8 KB, block size of 64 B, 2-way 
associativity, LRU replacement, access latency of 2 
CPU cycles  
L1 data cache 
Private, total size of 8 KB, block size of 64 B, 2-way 
associativity, LRU replacement, access latency of 2 
CPU cycles 
L2 unified 
cache 
Shared, total size of 1 MB, block size of 64 B, 8-way 
associativity, LRU replacement, access latency of 20 
CPU cycles, non-inclusive 
Memory  
Total size of 3 GB, access latency of 200 CPU cycles  
L1 caches to 
L2 cache bus 
Shared, width of 64 B, 1 GHz clock, first come first 
serve (FCFS) scheduling 
Memory bus 
Width of 64 B, 1 GHz clock 
 
17
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

For each simulation, the system execution was 
terminated when any core reached 500 million instructions. 
Due to varying CPU stall cycles across the benchmarks, at 
the termination point, not all cores had completed executing 
the simulation interval. However, this termination approach 
guaranteed that the cache miss rates reflected a fully-loaded 
system (i.e., full LLC contention since all cores were running 
during the entire system execution). Since we focused on the 
cache miss rates rather than the absolute number of cache 
misses, the incomplete benchmarksâ€™ execution had no impact 
on the evaluation. Similarly, due to statistical predictions, the 
applications 
are 
not 
required 
to 
begin 
execution 
simultaneously to garner accurate results.   
Although our experiments used only four cores and the 
LLC was a shared 8-way L2 cache, the analytical model 
itself does not include any limitations on the number of 
cores, the hierarchical level of the LLC, or the cache 
parameters (e.g., total size, block size, and associativity for 
our experiments).  
B. CaPPS Evaluation 
To validate the advantages of CaPPS, we compared 
CaPPSâ€™s ability to reduce the LLC miss rate as compared to 
two baseline configurations and private partitioning, since 
shared LLC partitioning in previous works [13][15][18] only 
provided private partitioning.  
Fig. 4 depicts the average LLC miss rate reductions for 
CaPPSâ€™s optimal configurations (the configurations with 
minimum average LLC miss rate in CaPPSâ€™s design space) 
as compared to two baseline configurations: 1) even-private-
partitioning: the LLC is evenly partitioned using private 
partitioning (first bar); and 2) fully-shared: the LLC is fully 
shared by all cores (second bar). Across all benchmark sets, 
the average and maximum average LLC miss rate reductions 
were 25.58% and 50.15%, respectively, as compared to 
even-private-partitioning, 
and 
19.39% 
and 
41.10%, 
respectively, as compared to fully-shared. 
The third bar in Fig. 4 depicts the average LLC miss rate 
reductions for CaPPSâ€™s optimal configuration as compared to 
private partitioningâ€™s optimal configuration, which is the 
configuration with minimum LLC miss rate in the private 
partitioningâ€™s design space consisting of 35 configurationsâ€”
approximately 1% of CaPPSâ€™s design space. Across all 
benchmark sets, the average and maximum reductions in 
CaPPSâ€™s average LLC miss rates as compared to private 
partitioning were 16.92% and 43.02%, respectively. 
C. Analytical Modelâ€™s Accuracy Evaluation 
For each benchmark set, we compared the average LLC 
miss rate for the four cores determined by the analytical 
model with the exact miss rate determined by gem5 for each 
configuration in CaPPSâ€™s design space. We calculated the 
average and standard deviation of the miss rate errors across 
the 3,347 configurations. Fig. 5 depicts the results for each 
benchmark set. The black markers indicate the average miss 
rate errors and the gray-shaded upper and lower ranges are 
the corresponding standard deviations. Averaged over all 
sixteen benchmark sets, the average miss rate error and 
standard deviation are -0.73% and 1.30%, respectively.   
Since the analytical modelâ€™s cache miss rates are 
inaccurate, we compared the absolute difference between the 
LLC miss rates of the analytical modelâ€™s minimum LLC 
miss rate configuration and the actual minimum LLC miss 
rate configuration as determined via exhaustive search. 
Comparing with an exhaustive search is appropriate for 
evaluating the analytical modelâ€™s efficacy, which is only 
affected by the estimated miss rate errors in determining the 
optimal configuration. The results indicate that fourteen out 
of sixteen benchmark setsâ€™ differences were less than 1% and 
the maximum and average differences over all benchmark 
sets was negligible, 1.3% and 0.36%, respectively.   
D. Analytical Modelâ€™s Time Evaluation 
To evaluate the execution time efficiency of the 
analytical model, we compared the time required to estimate 
the LLC miss rates (including the time for isolated trace 
access generation) for all configurations in the CaPPS design 
space as compared to using gem5. We implemented the 
analytical model in C++ compiled with O3 optimizations. 
We tabulated the user time reported from the Linux time 
command for the simulations running on a Red Hat Linux 
Server v5.2 with a 2.66 GHz processor and 4 gigabytes of 
RAM. Fig. 6 depicts the speedup of the analytical model for 
each benchmark set as compared to gem5. Over all 
benchmark sets, the average speedup is 3,966X, with 
maximum and minimum speedups of 13,554X and 1,277X, 
respectively. For one benchmark set, the time for simulating 
all 3,347 configurations using gem5 was approximately three 
months, and comparatively, the analytical model took only 
 
Figure 4. Average LLC miss rate reductions for CaPPSâ€™s optimal 
configurations compared to even-private-partitioning, fully-shared, and 
private partitioning. 
 
 
Figure 5. The average and standard deviation of the average LLC miss rate 
error determined by the analytical model. 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
Average LLC miss rate reduction 
Compared to even-private-partitioning 
Compared to fully-shared 
Compared with private partitioning 
-4% 
-3% 
-2% 
-1% 
0% 
1% 
2% 
Average and staandard deviation of 
estimated  average L2 miss rate error 
18
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

two to three hours. 
V. 
CONCLUSIONS AND FUTURE WORK  
In this paper, we presented cache partitioning with partial 
sharing (CaPPS)â€”a novel cache partitioning and sharing 
architecture that improves shared last-level cache (LLC) 
performance with low hardware overhead for chip multi-
processor systems (CMPs). Since CaPPS affords an 
extensive design space for increased optimization potential, 
CaPPS can reduce the average LLC miss rate by as much as 
25% and 17% as compared to baseline configurations and 
private partitioning, respectively. To quickly estimate the 
miss rates of CaPPSâ€™s sharing configurations, we developed 
an offline, analytical model that achieved an average miss 
rate estimation error of only 0.73%. As compared to 
exhaustive exploration (since no heuristics exist) of the 
CaPPS design space to determine the lowest energy cache 
configuration, the analytical model affords an average 
speedup of 3,966X. Finally, CaPPS and the analytical model 
are applicable to CMPs with any number of cores and place 
no limitations on the cache parameters.  
Future work includes extending the analytical model to 
optimize for any design goal, such as performance or energy 
delay product, leveraging the offline analytical results to 
guide online scheduling for performance optimizations in 
real-time embedded systems, including accesses to shared 
address space, incorporating cache prefetching in our 
analytical model, and extending CaPPS to proximity-aware 
cache partitioning for caches with non-uniform accesses. 
ACKNOWLEDGEMENTS  
This work was supported by the National Science 
Foundation (CNS-0953447). Any opinions, findings, and 
conclusions or recommendations expressed in this material 
are those of the author(s) and do not necessarily reflect the 
views of the National Science Foundation. 
 
REFERENCES 
[1] 
ARM 
Cortex-A 
Series, 
http://www.arm.com/products/processors/cortex-a/index.php. 
[2] 
N. Binkert, et. al. The gem5 Simulator, http://gem5.org [retrieved: 
Feb., 2013].  
[3] 
D. Chandra, F. Guo, S. Kim, and Y.  Solihin, â€œPredicting Inter-Thread 
Cache Contention on a Chip Multi-Processor Architectureâ€, In 
Proceedings of HPCA, Feb. 2005, pp. 340-351. 
[4] 
X. E. Chen and T. M. Aamodt, â€œA first-order fine-grained 
multithreaded throughput modelâ€, In Proceedings of HPCA, Feb. 
2009, pp. 329-340. 
[5] 
D. Chiou, D. Chiouy, L. Rudolph, S. Devadas, and B. S. Ang, 
â€œDynamic Cache Partitioning via Columnizationâ€, Computation 
Structures Group Memo 430. M.I.T. 2000. 
[6] 
D. Eklov, D. Black-schaffer, and E. Hagersten, â€œFast Modeling of 
Shared Cache in Multicore Systemsâ€, In Proceedings of HiPEAC, Jan. 
2011, pp. 147-157. 
[7] 
J. P. Grossman, â€œA Systolic Array for Implementing LRU 
Replacement,â€ Project Aries Technical Memo ARIES-TM-18, AI Lab, 
M.I.T., Cambridge, MA, 2002. 
[8] 
G. Hamerly, E. Perelman, J. Lau, and B. Calder, â€œSimPoint 3.0: Faster 
and More Flexible Program Analysisâ€, Journal of Instruction-level 
Parallelism, 2005, pp. 1-28. 
[9] 
M. D. Hill and A. J. Smith, â€œEvaluating Associativity in CPU Caches,â€ 
IEEE Trans. on Computers, Vol. 38, No. 12, 1989, pp. 1612-1630.  
[10] Intel Core Duo Processor, http://ark.intel.com/products/family/22731. 
[11] K. Johnson and M. Rathbone, â€œSunâ€™s Niagara Processorâ€, NYU 
Multicore Programming, 2010. 
[12] R. E. Kessler and M. D. Hill, â€œPage Placement Algorithms for Large 
Real-indexed Cachesâ€, ACM Trans. on Computer Systems, Vol. 10, 
No. 4, 1992, pp. 338-359. 
[13] S. Kim, D. Chandra, and Y. Solihin, â€œFair Cache Sharing and 
Partitioning in a Chip Multiprocessor Architectureâ€, In Proceedings of 
PACT, Sep.-Oct. 2004, pp. 111-122. 
[14] H. Lee, S. Cho, and B. Childers, â€œCloudCache: Expanding and 
Shrinking Private Cachesâ€, In Proceedings of  HPCA, Feb. 2011, pp. 
219-230. 
[15] M. Qureshi and Y. Patt, â€œUtility-Based Cache Partitioning: A Low-
Overhead, High-Performance, Runtime Mechanism to Partition Shared 
Cachesâ€, In Proceedings of MICRO, Nov. 2006, pp. 423-432. 
[16] SPEC CPU2006. http://www.spec.org/cpu2006 [retrieved: Sep., 2011]. 
[17] S. Srikantaiah, E., T. Zhang, M. Kandemir, M. Irwin, and Y. Xie, 
â€œMorphCache: a Reconfigurable Adaptive Multi-level Cache 
Hierarchy for CMPsâ€, In Proceedings of HPCA, Feb. 2011, pp. 231-
242. 
[18] K. Varadarajan, et al., â€œMolecular Caches: A Caching Structure for 
Dynamic Creation of Application-specific Heterogeneous Cache 
Regionsâ€, In Proceedings of MICRO, Nov. 2006, pp. 433-442. 
 
 
Figure 6. The analytical modelâ€™s simulation time speedup compared to 
gem5. 
0 
2000 
4000 
6000 
8000 
10000 
12000 
14000 
Speedup 
19
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

