Energy Efﬁciency of Server Virtualization
Jukka Kommeri
Helsinki Institute of Physics,
Technology program, CERN,
CH-1211 Geneva 23, Switzerland
jukka.kommeri@cern.ch
Tapio Niemi
Helsinki Institute of Physics,
Technology program, CERN,
CH-1211Geneva 23, Switzerland
tapio.niemi@cern.ch
Olli Helin
Helsinki Institute of Physics,
Technology program, CERN,
CH-1211 Geneva 23, Switzerland
olli.helin@cern.ch
Abstract—The need for computing power keeps on growing.
The rising energy expenses of data centers have made server
consolidation and virtualization important research areas.
Virtualization and its performance have received a lot of
attention and several studies can be found on performance.
So far researches have not studied the overall performance
and energy efﬁciency of server consolidation. In this paper we
study the effect of server consolidation on energy efﬁciency with
an emphasis on quality of service. We have studied this with
several synthetic benchmarks and with realistic server load by
performing a large set of measurements. We found out that
energy-efﬁciency depends on the load of the virtualized service
and the number of virtualized servers. Idle virtual servers do
not increase the consumption much, but on heavy load it makes
more sense to share hardware resources among fewer virtual
machines.
Keywords-virtualization; energy-efﬁciency; server consolida-
tion; xen; kvm; invenio; cmssw
I. INTRODUCTION
The need for computing power in data centers is heavily
growing due to cloud computing. Large data centers host
cloud applications on thousands of servers [1], [2]. Tradi-
tionally, servers have been purchased to host one service
each (e.g., a web-server, a DNS server). According to many
studies the average utilization rate of a server is around 15%
of maximum but depends much on the service and it can be
even as low as 5% [3], [4].
This level of utilization is very low compared to any ﬁeld
in industry. A common explanation for the low utilization is
that data centers are build to manage peak loads. However,
this is not a new data center speciﬁc issue, since high peak
loads are common in many other ﬁelds. Even with this low
level of utilization the computers are usually operational
and consuming around 60% of their peak power [5]. Low
utilization level is inefﬁcient through the increased impact on
infrastructure, maintenance and hardware costs. For exam-
ple, low utilization reduces the efﬁciency of power supplies
[6] causing over 10% losses in power distribution. Thus,
computers should run in near full power when they do value
adding work, because then they operate most efﬁciently
when comparing consumed energy per executed task [3].
A solution for increasing utilization is server consolidation
by using virtualization technologies. This enables us to
combine several services into one physical server. In this
way, these technologies make it possible to take better
advantage of hardware resources.
In this study, we focus on energy efﬁciency of different
virtualization technologies. Our aim is to help the system
administrator to decide how services should be consolidated
to minimize energy consumption without violating quality
of service agreements.
We studied energy consumption of virtualized servers with
two open source virtualization solutions; KVM and Xen.
They were tested both under load and idle. Several synthetic
tests were used to measure the overhead of virtualization on
different server components. Also a couple of realistic test
cases were used; an Invenio database service and CMSSW,
The CMS Software Framework, physics analysis software.
The results were compared with the results of the same tests
on hardware without virtualization. We also studied how
overhead of virtualization develops by sharing resources of
physical machines equally among different number of virtual
machines and running the same test set on each virtual
machine set.
II. RELATED WORK
Virtualization and its performance is a well-studied area
but these studies mainly focus on performance, isolation,
and scheduling. Even though energy efﬁciency is one of
the main reasons for server consolidation and virtualization,
it has not received much attention. Many of these studies
evaluate overhead differences between different virtualiza-
tion solutions.
Regola et al. studied the use of virtualization in high
performance computing (HPC) [7]. They believed that virtu-
alization and the ability to run heterogeneous environments
on the same hardware would make HPC more accessible
to bigger scientiﬁc community. They concluded that the I/O
performance of full virtualization or para-virtualization is
not yet good enough for low latency and high throughput
applications such as MPI applications.
Nussbaum et al. [8] made another study on the suitabil-
ity of virtualization on HPC. They evaluated both KVM
and Xen in a cluster of 32 servers with HPC Challenge
benchmarks. These studies did not ﬁnd a clear winner.
90
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-189-2
ENERGY 2012 : The Second International Conference on Smart Grids, Green Communications and IT Energy-aware Technologies

They concluded the performance of full virtulization is
far behind that of paravirtulization. Also running workload
among different number of virtual machines did not seem
to have an effect. Verma et al. [9] also studied the effect
of having the same workload on different number of virtual
machines. They found out that virtualization and division
load between several virtual machines does not impose
signiﬁcant overhead.
Padala et al. [10] carried out a performance study of vir-
tualization. They studied the effect of server load and virtual
machine count on multi tier application performance. They
found OS virtualization to perform much better than par-
avirtualization. The overhead of paravirtualization explained
by L2 cache misses, which in the case of paravirtualization
increased more rapidly when load increased.
Another study from Deshane et al. [11] compare scal-
ability and isolation of a paravirtualized Xen and a full
virtualized KVM server. Results said that Xen performs
signiﬁcantly better than KVM both in isolation and CPU
performance. Surprisingly, a non-paravirtual system outper-
forms Xen in I/O performance test.
As we can see from previous work, the energy-efﬁciency
has not received much attention. Our study focuses mainly
on the energy-efﬁciency of virtualization.
III. TESTS AND TEST ENVIRONMENT
Our tests aimed at measuring the energy consumption
and overhead of virtualization with a diverse test set. We
used both synthetic and real applications in our tests and
measured how performance is affected by virtualization. We
compared the results of the measurements, that were done
on virtual machines, with the results of the same tests on
physical hardware. First we measured the idle consumption
of virtualized machines using different number of virtual
machines. Then we compared different virtualization tech-
nologies and operating systems.
A. Test Hardware
The tests were conducted in our dedicated test envi-
ronment. The test computer was a Dell PowerEdge R410
server with two Intel Xeon E5520 processors and 16 GB of
memory. Hyper-threading was disabled for the processors
and clock speed was the default 2.26 GHz for all cores.
The Turbo Boost option of Intel was enabled. The system
had a single 250 GB hard disk drive. As a client computer
we used a server with dual Xeon processors running at
2.80 GHz. Network was routed through D-Link DGS-1224T
gigabit routers. Power usage data was collected with a Watts
up? PRO meter via a USB cable. Power usage values were
recorded every second. For physics analysis tests (CMSSW)
and some idle tests, we used a dual processor Opteron 2427
server with 32GB of memory and 1 TB hard disk.
B. Used Virtualization Technologies
The operating system used in all machines, virtual or
real, was a standard installation of 64-bit Ubuntu Server
10.04.3 LTS. The same virtual machine image was used
with both KVM and Xen guests. The image was stored in
a raw format, i.e., a plain binary image of the disc image.
We used the Linux 3.0.0 kernel. It was chosen as it had
the full Xen hypervisor support. With this kernel we were
able to compare Xen with KVM without a possible effect
of different kernels on performance.
For CMSSW tests, a virtual machine with Scientiﬁc Linux
5 was installed with CMSSW version 4.2.4. For these tests
real data ﬁles produced by the CMS experiment was used.
These data ﬁles were stored on a Dell PowerEdge T710
server and shared to the virtual machines with a network
ﬁle system, NFSv4.
C. Test Applications
Our synthetic test collection consisted of Linpack [12],
BurnInSSE 1, Bonnie++ [13] and Iperf [14]. Processor per-
formance was measured with an optimized 64-bit Linpack
test. This benchmark was run in sets of thirty consecu-
tive runs and power usage was measured for whole sets.
In addition, processor power consumption measurements
were conducted with ten minute burn-in runs with 64-bit
BurnInSSE collection using one, two and four threads. Disk
input and output performance was measured using Bonnie++
1.96. The number of ﬁles for a small ﬁle creation test was
400. For a large ﬁle test the ﬁle size was 4 GB. For Bonnie++
tests, the amount of host operating system memory was
limited to 2.5 GB with a kernel parameter and the amount
of guest operating system memory was limited to 2 GB.
For hardware tests, a kernel limit of 2 GB was used. The
tests were carried out ten times. Network performance was
measured using Iperf 2.0.5. Three kinds of tests were run:
one where the test computer acted as a server, another where
it was the client and a third where the computer did a
loopback test with itself. Testing was done using four threads
and a ten minute timespan. All three types of tests were
carried out ﬁve times.
As real world applications, we used two different systems.
The ﬁrst one was based on the Invenio document repository
[15]. We used an existing Invenio installation, which had
been modiﬁed for the CERN library database. The Invenio
document repository software suite was v0.99.2. The doc-
ument repository was run on an Apache 2.2.3 web server
and a MySQL 5.0.77 database management system. All this
software were run on Scientiﬁc Linux CERN 5.6 inside a
chroot environment. Another server was used to send HTTP
requests to our test server. The requests were based on
an anonymous version of a real-life log data from similar
document repository in use at CERN. The requests were sent
1http://www.roylongbottom.org.uk
91
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-189-2
ENERGY 2012 : The Second International Conference on Smart Grids, Green Communications and IT Energy-aware Technologies

using the Httperf web server performance test application
[16].
The second real application was a physics data analysis
that used the CMSSW framework [17]. This analysis task
is a very typical one in high-energy physics. We used real
data created at CERN. The data was stored in a ROOT
image[18] ﬁles, which our case were of size 4GB. Normally,
a data analysis with this data takes days to perform, thus
we limited the number of events of one analysis task to
300. With this limitation the analysis takes 10 minutes on
the Opteron hardware. The data was located on network ﬁle
system, NFS, and reading it caused very little network trafﬁc,
2kB per task.
IV. RESULTS
A. Idle consumption
First we studied idle energy consumption with different
virtualization solutions and with different number of virtual
machines. We also tested the effect of having different
operating systems in virtual machines.
Figure 1 shows the power consumption of two differ-
ent virtualization solutions. We had three virtual machines
running idle in both cases. The ﬁgure shows how energy
consumption of two different virtualization solutions behave
when the servers are idle. It shows how overhead of virtu-
alization depends on the virtualization solution and kernel
version. The difference between KVM and hardware is less
than 3%, which is already a big improvement compared to
three separate physical machines running idle. This test was
run with the Dell R410 server.
0
5
10
Time (min)
70
75
80
85
90
95
100
105
110
115
120
125
130
135
Power consumption (W)
Xen, Linux 3.0.0
Xen, Linux 2.6.32.46
KVM
Hardware
Figure 1.
Typical idle power consumption
The second idle measurement was run on the dual proces-
sor Opteron server. Test measured the energy comsumption
of the physical hardware for 20 minutes. Figure 2 shows how
the operating system affects the idle consumption. The same
test was repeated with different number of virtual machines
on the same physical hardware. The energy consumption
cumulates with the virtual machine count when we have
Scientiﬁc Linux 5 (SLC5) but with Ubuntu it remains almost
the same as hardware.
0
1
2
3
4
5
6
7
8
37.00
38.00
39.00
40.00
41.00
42.00
43.00
SLC5
Ubuntu
Number of VMs
Energy (Wh)
Figure 2.
Energy consumption of idle virtual machines
B. Synthetic tests
We used synthetic tests to stress different server compo-
nents; CPU, I/O and network. With these tests we studied
in which situations virtualization causes the most overhead.
0
50
100
150
200
250
300
350
400
450
% of hardware result
29.6
34.2
31.4
139.0
Hardware
Xen
KVM (Writeback)
KVM (Writethrough)
Figure 3.
The energy consumption of Bonnie++ (Wh)
As seen in Figure 3, when running a set of synthetic
disk operations Xen uses slightly more energy compared to
hardware. With KVM the situation is different. When using
the default writethrough cache, KVM uses around 350%
more energy than hardware. About 90% of the test time is
spent doing the small ﬁle test part of Bonnie++. Switching
to writeback cache, results of KVM are actually slightly
better than hardware results. Writeback cache writes only
to a cache and stores data to the disk only just before the
cache is replaced. This is a cache mode that is not safe for
production use and is available mainly for testing purposes.
0
20
40
60
80
100
120
% of hardware result
1 thread (W)
4 threads (W)
Hardware
KVM
Xen
143.4
143.6
151.9
204.8
204.3
184.2
Figure 4.
Power consumption under high CPU load with BurnInSSE
92
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-189-2
ENERGY 2012 : The Second International Conference on Smart Grids, Green Communications and IT Energy-aware Technologies

0
20
40
60
80
100
120
140
% of hardware result
26.2
26.2
26.4
29.3
1882.1
1875.8
1879.4
1684.0
Energy
consumption (Wh)
Speed (MFLOPS)
Hardware
KVM
Xen, Linux 2.6.32.46
Xen, Linux 3.0.0
Figure 5.
Energy consumption under high CPU load with Linpack
In Figure 4, power consumption is tested under full CPU
load between 1 and 4 threads running BurnInSSE64. With 1
thread, KVM and hardware use the same amount of power,
but Xen uses around 10% more. With 4 threads the situation
is the other way around: Xen uses less power than KVM
and hardware. The explanation to this can be seen in Figure
5. Even though Xen uses more power in the single-threaded
LINPACK test, it is slower: the CPU is not running at its full
turbo boosted speed, but Xen has a systematic overhead in
power consumption compared to the others. With 4 threads,
Xen’s CPUs are not running at full speed so the power usage
is not so great as with hardware or KVM, and the effect
of overhead in power consumption is overshadowed by the
power usage of 4 computing threads.
0
20
40
60
80
100
120
% of hardware result
140.0
150.1
147.2
139.2
148.9
151.7
192.0
193.8
173.9
53.3
43.8
29.0
Mean power
consumption
as client (W)
Mean power
consumption
as server (W)
Mean power
consumption,
loopback (W)
Bandwidth,
loopback (Gb/s)
Hardware
KVM
Xen
Figure 6.
Power consumption under Iperf network trafﬁc test
Iperf test results in Figure 6 show a similar trend: KVM
uses slightly more power than hardware while Xen conse-
quently uses slightly more power than KVM. Interestingly,
when a Xen virtual machine was running as server it used
slightly more power than when running as client. With
KVM and hardware it was the other way around. In the
loopback mode, one can ﬁnd similar results with Xen as
in the LINPACK test in Figure 5: for some reason, Xen’s
performance is capped and consequently bandwidth in the
loopback mode is much worse than with KVM or hardware,
and on the other hand mean power consumption is lower.
C. Realistic load
Realistic tests were designed such that we would get better
understanding of energy usage in two different real world
situations: web services and physics analysis.
The ﬁrst realistic test was a CERN document server
repository case. In this test, we sent HTTP requests, which
were based on CERN library log data, to a virtualized web
server. We measured both performance and power consump-
tion. We ran the same test with and without virtualization.
We compared two virtualization solutions and hardware to
measure the overhead of virtualization. In all Invenio tests,
the Invenio installation was in a chroot environment with a
complete SLC5 installation. To assure that chroot between
the operating system and the Invenio web application did
not have any negative effects on test results, a comparative
test was performed between the base system and another
chroot environment using a copy of the base system as the
new root.
0
20
40
60
80
100
120
140
160
% of hardware result
88.0
99.7
134.6
Mean power consumption
using 1 VM (W)
Hardware
KVM
Xen
0
20
40
60
80
100
120
140
160
112.7
125.6
142.0
Mean power consumption
using 3 VMs (W)
Figure 7.
Power consumption of different virtualization solutions with
different number of virtual machines in the repository test
In Figure 7, we have the results for Httperf rates 5 and
15. On the left side we have one virtual machine with rate 5
workload and on the right side 3 virtual machines with load
5. It shows the power consumption levels when we have
more virtual machines and load.
0
25
50
75
100
125
150
175
200
225
250
275
300
% of hardware result
112.7
125.6
142.0
32.7
41.1
93.1
34.9
72.3
91.0
Mean power
consumption (W)
Mean response
time (ms)
Mean transfer
time (ms)
Hardware
KVM
Xen
Figure 8. Power consumption and Httperf results of different virtualization
solutions
Figure 8 shows the results of comparison of hardware,
Xen and KVM with different number of virtual machines.
We tested overhead of virtualization by increasing the
number of virtual machines with similar workload. These
93
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-189-2
ENERGY 2012 : The Second International Conference on Smart Grids, Green Communications and IT Energy-aware Technologies

0
20
40
60
80
100
120
140
160
180
200
220
240
260
% of hardware result
Mean power
consumption (W)
Mean response
time (ms)
Mean transfer
time (ms)
KVM, 1 VM
KVM, 2 VMs mean
KVM, 3 VMs mean
99.7
112.3
125.6
19.1
35.0
41.1
27.9
44.1
72.3
Figure 9.
The effect of workload on virtual machine performance with
KVM using different amounts of virtual machines
results are illustrated in Figure 9
0
25
50
75
100
125
150
175
200
225
250
% of hardware result
Mean power
consumption (W)
Mean response
time (ms)
Mean transfer
time (ms)
KVM, 2 VCPUs
KVM, 4 VCPUs
KVM, 6 VCPUs
99.7
112.0
127.8
19.1
27.5
39.8
27.9
34.2
39.2
Figure 10.
The effect of workload on virtual machine performance with
KVM using different virtual machine resources
In Figure 10, we have the results of a test where, instead
of growing the number of virtual machines, we increased
the load and resources of one virtual machine. Table IV-C
shows the rates and resources given to virtual machines in
these tests. The MaxClients setting refers to the maximum
clients setting in Apache web server conﬁguration.
Table I
SETTINGS FOR CHANGING LOAD AND RESOURCES OF A SINGLE
VIRTUAL MACHINE
VCPUs
Memory (GB)
MaxClients
Request rate
2
5
8
5
4
8
15
10
6
15
24
15
Figures 11 and 12 illustrate the effect of virtualization on
quality of service. They show a cumulative distribution of
response times the Httperf test application reported for the
HTTP requests.
In Figure 11, we have the results of running workload of
15 queries per second on 3 virtual machines and on hard-
ware for comparison. Corresponding performance results are
shown in Figure 8. These ﬁgures show how the response
times increase when the same workload is divided into 3
virtual machines.
0
10
20
50
100
Response time (ms)
90
95
99
100
% of responses
Hardware
KVM
Xen
Figure 11.
The impact of virtualization solution on quality of service
0
10
20
50
100
Response time (ms)
90
95
99
100
% of responses
KVM, 1 VM
KVM, 2 VMs
KVM, 3 VMs
Figure 12.
The impact of different loads on quality of service
To better show the virtualization overhead effect on
different workload and number of virtual machines we
compared KVM with rates 5, 10, and 15. The results of
this experiment can be seen in Figure 12. Corresponding
performance measurements are shown in Figure 9.
HW
1 VM
3 Vms
5 Vms
15 Vms
3.00
3.50
4.00
4.50
5.00
5.50
6.00
6.50
7.00
Number of virtual machines
Energy (Wh) / Job 
HW
1 VM
3 Vms
5 Vms
15 Vms
20.00
25.00
30.00
35.00
40.00
45.00
50.00
55.00
60.00
65.00
Number of virtual machines
Throughput ( jobs / hour )
Figure 13.
Running 15 jobs in different number of virtual machines
As our second realistic load, we had a physics analysis
application. Here we consider one run of the test application
as a job. In Figure 13, we have the results of running 15
jobs in 5 different virtual machine sets. The ﬁgure illustrates
how the energy efﬁciency degrades as the number of virtual
machines increases. 15 virtual machines running one job is
6.8 times less energy efﬁcient than running 15 jobs on one
virtual machine.
Figure 14 shows the effect of workload on energy-
efﬁciency. We tested different workloads on 5 identical
virtual machines sharing the the same physical server.
94
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-189-2
ENERGY 2012 : The Second International Conference on Smart Grids, Green Communications and IT Energy-aware Technologies

10
15
20
4.2
4.3
4.4
4.5
4.6
4.7
4.8
4.9
5
5.1
Number of jobs
Energy (Wh) / job 
10
15
20
36
38
40
42
44
46
48
50
Number of jobs
Throughput ( jobs / hour )
Figure 14.
Running different workload on 5 virtual machines
V. CONCLUSIONS
The overhead of virtualization is a well-known fact re-
ported in many publications. Although the technologies have
been improving a lot during the past ﬁve years, the perfor-
mance of a virtualised system is still far from the hardware
level. However, this does not mean that virtualization could
not be useful in improving energy-efﬁciency in large data
centers but it means that one should know how to apply this
technology to achieve savings in energy consumption.
We studied the energy-efﬁciency virtualization technolo-
gies and how different load affects it. Our research indicates
that idle power consumption of virtualized server is close to
zero. However, this depends a lot on the operating system
running on the virtual machine, but it is always a small
number compared to idle energy consumption of a physical
server. Our study also indicates that virtualization overhead
has great impact on energy-efﬁciency. This means that it
would make more sense to share the physical resources
among few virtual machines with heavy load instead of
larger set of light-loaded ones.
ACKNOWLEDGMENT
Many thanks to Jochen Ott from CMS@CERN exper-
iment for providing help in installing the CMSSW and
providing with a typical analysis job. Also we would like to
thank Salvatore Mele, Tibor Simko, Jean-Yves LeMeur of
CERN library and Invenio developers, for providing realistic
data and a test case for our analysis.
REFERENCES
[1] B. Sch¨appi, F. Bellosa, B. Przywara, T. Bogner, S. Weeren,
and A. Anglade, “Energy efﬁcient servers in europe,” Austrian
Energy Agency, Tech. Rep. October, 2007.
[2] E. STAR, “Report to congress on server and data center
energy efﬁciency,” U.S. Environmental Protection Agency
ENERGY STAR Program, Tech. Rep., 2007.
[3] L. A. Barroso and U. H¨olzle, “The case for energy-
proportional computing,” Computer, vol. 40, pp. 33–37, 2007.
[4] W. Vogels, “Beyond server consolidation,” Queue, vol. 6, pp.
20–26, January 2008.
[5] D. Meisner, B. T. Gold, and T. F. Wenisch, “Powernap:
eliminating server idle power,” in Proceeding of the 14th
international conference on Architectural support for pro-
gramming languages and operating systems, ser. ASPLOS
’09.
Washington, DC, USA: ACM, 2009, pp. 205–216.
[6] U. H¨olzle and B. Weihl, “High-efﬁciency power supplies for
home computers and servers,” Google, Tech. Rep., 2006.
[7] N. Regola and J.-C. Ducom, “Recommendations for virtu-
alization technologies in high performance computing,” in
Cloud Computing Technology and Science (CloudCom), 2010
IEEE Second International Conference on, 30 2010-dec. 3
2010, pp. 409–416.
[8] L. Nussbaum, F. Anhalt, O. Mornard, and J.-P. Gelas, “Linux-
based virtualization for hpc clusters,” Network, pp. 221–234,
2009.
[9] A. Verma, P. Ahuja, and A. Neogi, “Power-aware dynamic
placement of hpc applications,” in Proceedings of the 22nd
annual international conference on Supercomputing, ser. ICS
’08.
New York, NY, USA: ACM, 2008, pp. 175–184.
[10] P. Padala, X. Zhu, Z. Wang, S. Singhal, and G. Shin, K.,
“Performance evaluation of virtualization technologies for
server consolidation,” Work, no. HPL-2007-59, p. 15, 2007.
[11] T. Deshane, Z. Shepherd, J. Matthews, M. Ben-Yehuda,
A. Shah, and B. Rao, “Quantitative comparison of xen and
kvm,” in Xen summit.
USENIX association, June 2008.
[12] J. Dongarra, P. Luszczek, and A. Petitet, “The linpack
benchmark: past, present and future,” Concurrency and
Computation Practice and Experience, vol. 15, no. 9, pp.
803–820, 2003. [Online]. Available: http://doi.wiley.com/10.
1002/cpe.728
[13] B. Martin, “Using bonnie++ for ﬁlesystem performance
benchmarking,” Linuxcom, vol. Online edi, 2008.
[14] M. Egli and D. Gugelmann, “Iperf - network stress tool,”
Source, pp. 1–2, 2007.
[15] J. Caffaro and S. Kaplun, “Invenio: A modern digital library
for grey literature,” in Twelfth International Conference on
Grey Literature, Prague, Czech Republic, Dec 2010.
[16] D. Mosberger and T. Jin, “httperf - a tool for measuring
web server performance,” SIGMETRICS Perform. Eval. Rev.,
vol. 26, pp. 31–37, Dec 1998.
[17] F. Fabozzi, C. Jones, B. Hegner, and L. Lista, “Physics
analysis tools for the cms experiment at lhc,” Nuclear Science,
IEEE Transactions on, vol. 55, pp. 3539–3543, 2008.
[18] I. Antcheva and et al., “Root a c++ framework for petabyte
data storage, statistical analysis and visualization,” Computer
Physics Communications, vol. 180, no. 12, pp. 2499 – 2512,
2009.
95
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-189-2
ENERGY 2012 : The Second International Conference on Smart Grids, Green Communications and IT Energy-aware Technologies

