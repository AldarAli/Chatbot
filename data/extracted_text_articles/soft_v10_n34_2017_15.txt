Accurate and Robust Skin Feature Extraction Scheme for Aging Estimation 
 
Hyungjoon Kim, Jisoo Park, Eenjun Hwang 
School of Electrical Engineering 
Korea University 
Anam-Dong, Seongbuk-Gu, Seoul, Republic Korea 
E-mail: {hyungjun89, jisoo_park, ehwang04}@korea.ac.kr 
Woogeol Kim 
VC Smart Validation Team 
LG Electronics 
Pyeongtaek, Republic Korea 
E-mail: woogeol.kim@lge.com
 
 
Abstract— In this paper, we revise our method for skin feature 
extraction based on cell segmentation to improve its accuracy, 
efficiency and robustness that are very critical in the realistic 
skin condition analysis. In order to achieve such goals, we 
enhance the contrast of wrinkles on the skin by using the 
contrast limited adaptive histogram equalization (CLAHE) and 
highlight the depth of wrinkles by using the extended-minima 
transform. By performing watershed transform, we can 
segment the skin image into labelled skin cells and calculate 
various skin features from the labelled cells. We focus on two 
types of skin features that play a key role in assessing the degree 
of skin aging; cell features and wrinkle features. To evaluate the 
performance and robustness of our revised method, we collected 
skin images using three different types of microscopy cameras 
and extracted their cell and wrinkle features using the revised 
method. Through various experiments, we show that our revised 
method achieves 10% increase in the skin feature extraction 
accuracy and 50% decrease in the skin feature extraction time 
compared to our previous method. 
Keywords- Skin analysis; Feature extraction; Wrinkle feature; 
Contrast stretching; Microscopy image. 
 
I. 
INTRODUCTION 
In this paper, we reinforce the experimental part of our 
work [1] for skin feature extraction scheme. Various factors, 
such as exposure to sunlight or pollution, smoking and 
excessive drinking are known to accelerate the normal skin 
aging process and eventually lead to premature skin aging.  
Since skin is the outermost part of the human body, people 
have shown great interest in the beauty and health of their skin. 
This in return leads to diverse studies related to skin analysis, 
skin treatment, and skin aging. Usually, the degree of skin 
aging has been evaluated by dermatologists based on their 
personal experience or knowledge. This is because there is no 
standard method for quantitative and objective evaluation. If 
such method was available, then users would get consistent 
and quantitative information about their skin condition, and 
hence perform suitable treatment for their skin more 
effectively and conveniently. 
In our previous work, we proposed a scheme for skin 
texture aging trend analysis based on diverse skin texture 
features such as texture length, width, depth, cell count and 
cell area. To extract such features, we cropped microscopy 
skin image, carried out histogram equalization, removed noise 
and then binarized the image using the Otsu threshold. After 
that, we segmented the skin texture into cells by using the 
watershed algorithm and calculated their features [2][3]. 
However, there are several problems in our previous work. 
Two serious problems among them are feature extraction time 
and feature extraction accuracy. The former is due to the fact 
that its feature extraction is pixel-based and hence all pixels in 
each cell should be considered. The latter is due to the fact that 
preprocessing steps in the previous work were optimized for 
a specific microscope camera. Hence, the feature extraction 
result might be not good when using other types of cameras. 
In this paper, we modify some of the preprocessing steps 
and segmentation method in the previous work to improve the 
accuracy, efficiency and robustness of skin feature extraction. 
Figure 1 shows the overall steps to achieve that, which can be 
divided into preprocessing, cell segmentation and feature 
extraction. In the preprocessing, the original image is cropped 
to reduce the effect of vignetting. Then, contrast stretching is 
applied in order to enhance the intensity of wrinkles against 
skin. Denoising filters are applied to the image to reduce noise 
in the image. In the cell segmentation, extended-minima 
transform and watershed algorithm are used for cell-based 
segmentation. Each cell cluster is labeled, and the labeled 
information is utilized for calculating skin features. We 
extract five features from the skin image to analyze the skin 
condition.  
The remainder of this paper is organized as follows. In 
Section II, we introduce several related works for skin analysis, 
wrinkle detection and cell segmentation. Detailed techniques 
for skin segmentation are described in Section III and skin 
feature extraction method is described in Section IV. We 
explain our experiment and conclude this paper in Section V. 
 
II. 
 RELATED WORKS 
So far, medical analysis and diagnosis based on biometric 
images have been performed in the various domains. Skin 
analysis is one of the most popular and interesting tasks, since 
skin is the outermost part of the human body. Various methods 
for extracting diverse features from skin images and 
evaluating its condition quantitatively have been proposed.  
As an effort to detect skin wrinkles, H. Tanaka et al. 
applied a cross-binarization method to digital skin image to 
get its binary image, and then, the short straight line matching 
method to detect wrinkles from the binary image and measure 
their length [4]. More specifically, for each base line in the  
335
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

cross-binarized image, if more than 70% of its pixels are 
marked black, then the line is considered a wrinkle. After that, 
they continue from the end of current base line to create a new  
base line. This repeats until the end of the wrinkle or the end 
of the image is reached. J. Ute et al. measured the topography  
of skin surface using an optical 3D device and showed that 
there is a significant dependency between skin surface 
topography and the age [5]. On the other hand, G. O. Cula et 
al. developed the automatic facial wrinkles detection 
algorithm based on estimating the orientation and frequency 
of the elongated spatial feature, captured via digital image 
filtering [6]. Recently Yow. Ai Ping et al. proposed the 
ASHIMA system framework and showed how to process HD-
OCT (High-Definition Optical Coherence Tomography) skin 
images automatically to measure the epidermal thickness and 
skin surface topography [7]. H. Razalli et al. estimated 
human’s age range based on facial wrinkle analysis [8].  They 
mentioned when fewer wrinkles are detected or extracted, it 
will consequently affect the process to estimate the correct age. 
To solve that, they proposed a new method to extract facial 
wrinkles in a face image using Hessian based filter (HBF) for 
age estimation. They tested their proposed method for FG-
NET database and compared its performance with other 
methods. 
One of the most popular algorithms for segmenting a skin 
microscope image into cells is watershed segmentation. 
Although it has been a long time since this algorithm was 
introduced, it is still widely used in various image processing 
applications. A. Das and D. Ghoshal segmented human skin 
region using watershed segmentation [9]. They converted 
RGB image into YCbCr color image, and then modified 
marker based watershed algorithm has been applied on Cr 
component for segmentation of human skin region. They 
compared their method with other algorithms. The methods 
were tested on FRI CVL Face Database. Z. JunXiong et al 
proposed a method for extracting features of jujube fruit 
wrinkle using the watershed segmentation [10]. They first 
converted original RGB image into gray scale image, and then, 
applied morphological reconstruction to remove noise. After 
that, they performed the H-minima extended transformation 
to label the foreground of jujube fruit images, and then 
segmented the labeled foreground regions using a distance 
transform-based watershed algorithm. 
 
III. 
SKIN SEGMENTATION METHOD 
A. Preprocessing 
Direct image processing on microscope image or captured 
image might cause several problems if the image is in RGB 
(Red-Green-Blue) form. Usually, dealing with RGB image 
shows less accuracy than dealing with gray image. Other 
typical factors to decrease the accuracy are vignetting effect 
and noises. To avoid these problems, original images need to 
be converted into binary images through preprocessing. In this 
work, preprocessing consists of three steps: (i) the original 
image is cropped to reduce the effect of vignetting. (ii) 
Contrast stretching [11] is applied to make brightness 
difference between skin and wrinkle bigger. (iii) Adaptive 
histogram equalization is applied to the image. Figure 2 shows 
overall steps of preprocessing and their result on an example 
skin image.  
 
1) Cropping 
Due to the limitations of the camera and the interference 
of the light source, captured images may have a noise known 
as vignetting effect. Vignetting is a phenomenon where the 
 
Figure 1.  Overall scheme of skin feature extraction 
 
336
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
outer edges of the images become dark due to the reduction of 
light at the periphery of camera lens, and as a result, the 
captured images have different color histogram distributions. 
An example of vignetting effect is shown in Figure 3, where 
we can see the dark area around the corner.  In order to avoid 
such phenomenon, we cropped 300 by 300 pixels from the 
center of the image, which has the concentrated luminous 
source of the image. Figure 2 (b) shows the result of image 
cropping for an input image shown in Figure 2 (a), where the 
rectangle indicates the cropping size. 
 
 
Figure 3.  Example of vignetting effect 
 
2) Contrast stretching 
Accurate detection of skin wrinkles is very crucial in the 
skin analysis and the accuracy can be improved by clearly 
separating skin and wrinkle pixels in the image. However, 
original images often lack sufficient contrast depending on 
the conditions under which the image was photographed such 
as light source and shooting area. Insufficient contrast could 
make certain areas in the image have similar contrast even 
though they must be distinguished. This problem can be 
moderated by contrast stretching. Contrast stretching expands 
the dynamic range of the intensity levels so that it spans the 
color distance between skin and wrinkle. The equation of  
contrast stretching used in this paper is shown in Eq. (1). 
 
𝑉𝑜𝑢𝑡 = {
0.6 × 𝑉𝑖𝑛 + 0.4,
𝑉𝑖𝑛 >  𝛼
0.25 × 𝑉𝑖𝑛,                  𝑉𝑖𝑛 <  𝛽
𝑉𝑖𝑛 = 𝑉𝑖𝑛,                    𝑂𝑡ℎ𝑒𝑟𝑠
 
(1) 
 
Here, 𝑉 is a value in HSV domain and 𝛼 and  𝛽 are two 
constants calculated by Eq. (2). Other constants are obtained 
experimentally. 
 
𝛼 = √max(𝑉) × min(𝑉) /𝑉𝑎𝑣𝑒𝑟𝑎𝑔𝑒 
𝛽 = max (𝑉) × √max(𝑉) × min(𝑉) 
(2) 
 
Figure 2 (c) shows the effect of contrast stretching for the 
cropped image. In the figure, we can see that the intensity of 
the skin pixels is reduced and the color distinction between 
skin and winkle becomes more prominent. 
 
3) Contrast limited adaptive histogram equalization  
Skin wrinkles can be detected using the watershed 
algorithm [12]. However, we observed that the algorithm 
couldn’t detect all the wrinkles due to the lack of contrast. 
Hence, before we apply the watershed algorithm to the skin 
image, we need to enhance the intensity of winkles by using 
the contrast limited adaptive histogram equalization (CLAHE) 
method to the image [13]. Histogram equalization is a gray 
scale transformation used for contrast enhancement. It aims to 
get an image with uniformly distributed intensity levels over 
the whole intensity scale. In some case, the result of histogram 
equalization might be worse compared to the original image 
since the histogram of the resulting image becomes 
approximately flat. For instance, when high peaks in the 
histogram are caused by an uninteresting area, histogram 
equalization results in enhanced visibility of unwanted image 
area. This means that the local contrast requirement is not 
satisfied, and as a result, minor contrast differences are 
 
Figure 2.  The example of overall preprocessing steps 
 
337
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

entirely ignored when the number of pixels falling in a 
particular gray range is relatively small. 
An adaptive method to avoid this drawback is block-based 
processing of histogram equalization [14]. In this method, an 
image is divided into sub-images or blocks, and histogram 
equalization is performed on each sub-images or blocks. Then, 
blocking artifacts among neighboring blocks are minimized 
by filtering or bilinear interpolation. 
The CLAHE method uses a clip limit to overcome the 
noise problem. That is, the amplification is limited by clipping 
the histogram at a predefined value before computing the 
Cumulative Distribution Function (CDF). The value at which 
the histogram is clipped, the so-called clip limit, depends on 
the normalization of the histogram and thereby on the size of 
the neighborhood region. The redistribution will push some 
bins over the clip limit again, resulting in an effective clip 
limit that is larger than the prescribed limit. 
In our work, we need to remove hairs from the gray image. 
Hairs can be mistaken for wrinkles and hence they are the 
most critical and common noise in the wrinkle detection. Skin 
hairs are easily removed by a simple threshold filter. Figure 2 
(d) shows the result of the CLAHE method. 
 
4) Binarization 
In order to measure the width of wrinkles, we need to 
detect the contour of wrinkles and their skeleton. The contour 
and skeleton can be obtained by performing canny edge 
detection and morphological thinning [15][16]. They are 
effectively working on the binary image. Hence, we apply 
Otsu threshold to the resulting image of CLAHE method to 
obtain its binary image [17]. Figure 2 (g) shows the result of 
binarization. 
 
5) Denoise 
Even though CLAHE images and binarized images are 
ready for feature extraction, there are still likely to have some 
noises due to the hardware limitation or environmental factors 
when taking pictures. To reduce the effect of such noises, we 
perform a denoise process. We apply different denoise process 
to the CLACHE and binarized images, In the case of CLAHE 
image, top-hat, bottom-hat filter is applied to remove noise. 
Figure 2 (e) shows the result after applying top, bottom-hat 
filter. After filtering, Extended-minima transform and 
watershed segmentation are performed for cell segmentation. 
In the case of binarized images, morphological closing is 
performed to remove noise. Figure 2 (h) shows the result of 
morphological closing.  
 
6) Extended-minima transform 
Even though watershed transform is widely used for image 
segmentation, it often suffers from the over-segmentation 
problem since regional minima or ultimate eroded points are 
employed for segmenting cells directly. One of the key factors 
that determine the accuracy of skin image segmentation by 
wrinkle cells is how much the minima points are extended. In 
this paper, we revise the extended-minima transform, which 
is the regional minima of the H-minima transform. Regional 
minima are connected components of pixels with a constant 
intensity value, and whose external boundary pixels have 
higher value.  
In other words, the result of h-minimum operator is linked 
to the depth of the minima. In a skin image, wrinkle cells 
consist of some minima and maxima. Minima correspond to 
parts of low depth points and maxima correspond to high 
depth points. Therefore, using the extended-minima transform, 
we can increase the depth between wrinkle cell clusters. It can 
help the watershed transform to cluster the wrinkle cells.  First, 
we extract minima from an image and extend the depth of the 
points. Figure 2 (h) shows the result after imposing the 
extended minima to the original gray scale image. 
 
B. Segmentation processing 
In this section, we describe how to segment a skin image 
into wrinkle cells using the extended-minima transform [18] 
and watershed transform. Watershed transform is one of the 
most widely used image segmentation techniques in image 
processing and we use it for segmenting a skin image into 
wrinkle cells. Especially, we perform the extended-minima 
transform before the watershed transform in order to increase 
the accuracy of finding wrinkle cells. 
 
 
(a) Overlapping objects 
(b) Distances  
(c) Separated objects 
Figure 4.  Segmentation using watershed transform 
 
1) Watershed segmentation 
Image segmentation is a computer analysis of image 
objects to decide which pixel of the image belongs to which 
object. Basically, this is the process of separating objects from 
background, as well as from each other. Watershed transform 
is a powerful and well-known tool for performing image 
segmentation. Figure 4 shows how to segment two 
overlapping circles using the watershed transform. 
 
 
 
 
(a) Labeled image 
(b) Filtered image with valid cells 
Figure 5.  Labelling process 
338
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 To segment them, an image distance to the background is 
computed. The maxima of the distance (i.e., the minima of the 
opposite of the distance) are chosen as markers, and the 
flooding of basins from such markers separates the two circles 
along a watershed line. We adapt these steps to our skin image, 
so that pixels of each wrinkle cell are clustered. 
 
2) Cell labelling 
Wrinkle cell labelling can be easily done by applying the 
watershed transform to the skin image. From the result of 
watershed transform, we can get a collection of wrinkle cells. 
Each cell contains the positions of the pixels in the cell which 
belong to same cluster. Figure 5 (a) shows an example of 
wrinkle cells labeling, where each cell is labeled using a 
different color. 
 
3) Pixel thresholding 
Sometimes, the segmentation result contains unexpected 
cells with very small size, which are usually noise or moles. 
Since they are not the regular wrinkle cells, they should be 
removed. We can calculate area of every labelled cell easily 
by counting the number of pixels in the cell. The threshold 
size by which we determine whether to remove the cell or not 
can be calculated by Eq. (3). 
 
𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 =
∑
𝐴𝑟𝑒𝑎𝑖
𝑖
𝑆𝑐𝑎𝑙𝑒𝑅𝑎𝑡𝑖𝑜 × 𝑛 
(3) 
 
Here, 𝐴𝑟𝑒𝑎𝑖 is the number of pixels in the 𝑖th cell and 𝑛 is 
the total number of cells. Also, 𝑆𝑐𝑎𝑙𝑒𝑅𝑎𝑡𝑖𝑜 is defined by  
Eq. (4). 
 
𝑆𝑐𝑎𝑙𝑒𝑅𝑎𝑡𝑖𝑜 = max(𝐴𝑟𝑒𝑎) + min(𝐴𝑟𝑒𝑎)
𝑚𝑒𝑎𝑛(𝐴𝑟𝑒𝑎)
 
(4) 
 
Figure 5 (b) shows all the noisy cells detected by this 
method. We remove them by merging with their neighboring 
cell or enclosing cell. 
 
IV. 
SKIN FEATURE EXTRACTION 
A. Defining skin features 
 
TABLE I.  ASSUMPTIONS BASED ON COMMON KNOWLEDGE OF SKIN 
1. Total wrinkle length decreases with age. 
2. Wrinkle width increases with age. 
3. Wrinkle depth increases with age. 
4. Wrinkle cell area increases with age. 
5. The number of cells decreases with age. 
6. Diameter ratio of inscribed circle and circumscribed circle of 
a cell decreases with age. 
7. Total length of lines connecting cross points of a cell 
increases with age. 
 
We have developed algorithms for extracting various cell 
and wrinkle features from microscopy skin images. Our 
feature extraction method is based on the labeled image 
described so far. Before we describe our feature extraction 
scheme in detail, we first show several assumptions we made 
based on common knowledge of skin [2] in Table I. 
In the table, the 6th assumption needs some explanation. 
Usually, each wrinkle cell has its own shape and the cell shape 
is getting distorted with aging. So, it might be helpful to 
evaluate how much a skin cell is distorted for skin aging 
estimation. In this paper, we represent the degree of distortion 
by the ratio of inscribed circle and circumscribed circle. Hence, 
if the cell’s shape is very regular like regular polygon, the ratio 
approaches to 1. However, this method requires much 
processing time since it considers all the pixels in a cell. To 
alleviate this problem, we replace existing method with 
regionprops. This method considers how irregular a cell shape 
is based on the angle instead of the ratio of radius length. In 
other words, we consider the slope of principal horizontal axis 
as the distortion degree of a cell. 
Finally, we define five skin features for skin aging 
estimation. They are cell count, average cell area, average cell 
gradient, total wrinkle length, and average wrinkle width. Cell 
count indicates how many cells are in the skin image. Average 
cell area indicates the average area of cells in the skin image. 
Cell gradient is what we described above,   
The wrinkle itself is a very important clue for estimating 
the degree of skin aging. We use two wrinkle features in this 
work; the total wrinkle length and the average wrinkle width. 
We will describe detailed steps for extracting these features in 
the following. 
 
B. Calculating skin features 
In this section, we describe how to calculate five skin 
features. At first, the number of cells can be easily obtained 
by counting the number of labeled cells while excluding cells 
with invalid size as we described earlier. 
 
 
Figure 6.  Example of calculating angle 
 
The average cell area is also simply calculated by dividing 
the total number of pixels in the labeled cells by the number 
of cells which we obtained in the first place. In order to 
calculate the cell gradient, we used the regionprops function 
[19] which calculates a set of features for each labeled region.  
339
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

One of the major features in the result of regionprops is 
the scalar angle value for each labeled region. It can be 
obtained by calculating the angle between the x-axis and the 
major axis of the ellipse that has the same second-moment as 
the region. Figure 6 illustrates how to calculate the angle. In 
the Figure, white line describes x-axis and red arrow shows a 
major axis. The angle between these two lines is calculated by 
regionprops method. 
Total wrinkle length can be calculated using the line 
sieving method. This method first counts the pixels on the 
horizontal and vertical texture lines. It then counts the pixels 
along the diagonal line, and estimates the actual wrinkle 
length considering its slope. In the case of single pixel islands 
on the image, we simply count these islands and add the 
number to the total length. 
To calculate average wrinkle width is a little complicated 
compared with other features. Figure 7 shows overall steps for 
to obtaining wrinkle width. We first apply morphological 
thinning on binary image. Thinning is a morphological 
operation that removes foreground in overall binary image. 
Figure 7 (b) shows the result of morphological thinning on the 
original image in Figure 7 (a). It consists of a set of 1x1 pixel 
points called skeleton. These pixels have specific direction, 
thus we can calculate each point’s direction. In order to 
calculate the direction, we used Principal Component 
Analysis (PCA) algorithm [20]. PCA algorithm is a method of 
calculating Eigen value and Eigen vector by using all data’s 
covariance and average.  
 
 
Figure 8.  Calculating wrinkle width 
 
 Using this algorithm, we can get all of points on the 
skeleton’s direction. Then, we can get a perpendicular line for 
each point. After calculating perpendicular lines, we can get 
the wrinkle contour by using canny edge detection. Figure 7 
(c) shows the wrinkle contour detected by using canny edge 
detection. Finally, we can merge the skeleton and contour 
together as shown in Figure 7 (d). When aforementioned 
processes are done, we can calculate its wrinkle width.  
Figure 8 shows how to calculate average wrinkle thickness. 
The left image in the figure is merged image of wrinkle 
skeleton and contour as shown in Figure 7 (d). The right image 
magnifies a specific point of the image. In the figure, each 
white ‘’ is skeleton point, and the line passing the skeleton 
point is a normal line. The red circles indicate the intersection 
of line and wrinkle contour. The length between these two 
intersection points is the wrinkle thickness at that point. 
 
 
 
(a) Camera A 
(b) Camera B 
 
(c) Camera C 
Figure 9.  Microscopy cameras compatible with smartphone 
 
 
 
 
(a) A binary image 
(b) Its skeleton image 
(c) Contour image 
(d) Skeleton + contour 
Figure 7.  Extracting wrinkle skeleton and contour 
 
340
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

V. 
EXPERIMENTS 
In order to evaluate the performance of our revised scheme, 
we performed several experiments based on the Matlab 2016a. 
To collect skin images, we used three different microscopy 
cameras shown in Figure 9, which are easily connected with 
smartphone. The reason we used different cameras is that 
images show quite different characteristics depending on the 
camera. Hence, to see the robustness of our method, we 
decided to test diverse cameras. 
The scales of the cameras A, B, and C are 50X to 500X, 
25X to 400X and 60X, respectively. In our previous work, we  
used the camera C only. We got approximately 300 face skin 
images and 60 hand skin images. 
In our previous work, we evaluated the accuracy of our 
revised method by matching binary image pixel and cell 
contour [1]. In that case, we consider binary image as ground 
truth. However, sometimes binary image is not appropriate for 
use as ground truth. A typical case is when the original image 
contains various unexpected characteristics such as noise, 
mole, and light reflection. To solve this problem, we have 
made the ground truth manually. In the evaluation, we focused 
on two tasks: wrinkle line detection and cell detection. 
Numerical data for some of the experimental results are shown 
in Table II. More detailed analysis of the results is described 
in the next Section. 
 
TABLE II.  NUMERICAL DATA FOR EXPERIMENTAL RESULT 
 
Sub 1 
Sub 2 
Sub 3 
Sub 4 
Sub 5 
Sub 6 
… 
AVERAGE 
Face Wrinkle-CAMERA A-Revised method 
96.512 
97.11 
97.362 
94.912 
95.334 
96.112 
… 
96.520  
Face Wrinkle-CAMERA A-Previous method 
88.905  
87.013  
90.288  
89.991  
86.032  
84.950  
… 
86.276  
Face Wrinkle-CAMERA B-Revised method 
95.748  
96.869  
97.390  
97.938  
96.381  
96.920  
… 
96.837  
Face Wrinkle-CAMERA B-Previous method 
84.897  
85.647  
85.327  
88.739  
85.773  
87.137  
… 
86.389  
Face Wrinkle-CAMERA C-Revised method 
95.720  
96.801  
95.147  
94.510  
94.490  
96.400  
 
95.912  
Face Wrinkle-CAMERA C-Previous method 
89.604  
89.585  
91.200  
90.073  
89.979  
91.369  
… 
89.781  
Hand Wrinkle-CAMERA A-Revised method 
96.456  
97.957  
95.095  
97.016  
97.476  
96.094  
… 
97.260  
Hand Wrinkle-CAMERA A-Previous method 
87.967  
89.220  
89.837  
91.700  
88.448  
89.302  
… 
89.115  
Hand Wrinkle-CAMERA B-Revised method 
97.021  
95.170  
96.664  
95.308  
97.943  
96.215  
… 
97.103  
Hand Wrinkle-CAMERA B-Previous method 
88.935  
88.290  
89.139  
88.235  
89.325  
89.574  
… 
88.835  
Hand Wrinkle-CAMERA C-Revised method 
97.015  
97.109  
95.165  
96.463  
98.658  
96.231  
… 
97.303  
Hand Wrinkle-CAMERA C-Previous method 
91.888  
91.051  
92.364  
88.966  
90.729  
89.719  
… 
90.927  
Face Wrinkle-CAMERA A-Revised method 
98.520  
97.437  
97.991  
95.554  
96.143  
96.340  
… 
96.612  
Face Wrinkle-CAMERA A-Previous method 
86.353  
84.977  
85.957  
83.203  
84.901  
84.431  
… 
85.255  
Face Wrinkle-CAMERA B-Revised method 
94.966  
97.051  
96.825  
96.576  
95.209  
97.670  
… 
96.543  
Face Wrinkle-CAMERA B-Previous method 
83.696  
83.047  
83.369  
83.084  
81.596  
83.060  
… 
83.346  
Face Wrinkle-CAMERA C-Revised method 
96.304  
96.156  
95.979  
96.601  
95.132  
98.099  
… 
97.135  
Face Wrinkle-CAMERA C-Previous method 
87.754  
87.214  
87.199  
86.436  
88.253  
87.101  
… 
87.234  
Hand Wrinkle-CAMERA A-Revised method 
96.193  
98.259  
96.683  
96.319  
96.614  
96.060  
… 
96.908  
Hand Wrinkle-CAMERA A-Previous method 
88.245  
91.891  
91.020  
89.673  
88.108  
88.500  
… 
89.365  
Hand Wrinkle-CAMERA B-Revised method 
97.057  
98.025  
95.902  
94.904  
95.785  
97.568  
… 
97.234  
Hand Wrinkle-CAMERA B-Previous method 
89.596  
89.657  
89.075  
89.389  
88.729  
90.067  
… 
89.205  
Hand Wrinkle-CAMERA C-Revised method 
96.427  
98.244  
96.940  
97.454  
98.342  
98.828  
… 
97.789  
Hand Wrinkle-CAMERA C-Previous method 
90.128  
92.507  
91.906  
91.178  
91.051  
91.028  
… 
91.246  
 
 
 
 
 
(a) Original cropped image 
(b) Ground truth 
(c) Removed skin area 
(d) Proposed method 
(e) Overlapping 
Figure 10.  Evaluation of cell detection accuracy 
 
341
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
(a) Accuracy of face wrinkle line detection 
(b) Accuracy of hand wrinkle line detection 
 
 
(c) Accuracy of face cell detection 
(d) Accuracy of hand cell detection 
Figure 11.  Accuracy comparison 
 
A. Wrinkle line detection accuracy 
To evaluate the accuracy of wrinkle detection, we made 
ground truth manually using all the images captured using 
three cameras. That is, we identified and marked cell 
boundaries (wrinkle) with eyes.  
Figure 10 shows an example. Figure 10 (a) shows an original 
image of 300x300 pixel. Figure 10 (b) shows all the manually 
identified wrinkle lines on the skin image, and Figure 10 (c) 
shows only wrinkle lines. On the other hand, Figure 10 (d) 
shows all the wrinkle lines detected by our proposed method. 
The detection accuracy can be confirmed visually by 
overlapping Figure 10 (c) and Figure10 (d) as shown in 
Figure 10 (e) and quantitatively measured by using Eq. (5). 
 
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = 
𝑁𝑊𝑃 ∩ 𝑊𝑃𝐺𝑇
𝑁𝑊𝑃
 × 100 
(5) 
 
Here, 𝑁𝑊𝑃 is the number of wrinkle pixels and 𝑊𝑃𝐺𝑇 is 
the number of pixels on the wrinkle lines in the ground truth.  
Basically, the equation counts those pixels that exist on 
both wrinkle lines, and then they are divided by the total 
number of pixels on the cell contour lines. 
Figure 11 compares the accuracy of our previous method 
and revised method on the face and hand skin images. From 
the figure, we can see that our revised method achieved higher 
accuracy compared to the previous method.  
Moreover, our revised method is more robust in hardware 
characteristics. For instance, Figure 12 shows the result of 
wrinkle detection on the facial and hand skin using three 
different cameras. As shown in the figure, our previous 
method showed different but poor performance depending on 
camera. However, our revised method showed good and 
consistent performance regardless of camera.  
 
B. Cell detection 
To evaluate the accuracy of cell detection, we used the 
same ground truth that we mentioned in the last section as 
follows.  If the cell detected by our revised method has a 
similar location with some cell in the ground truth and the 
ratio of the cell pixels over the ground truth cell is more than  
80%, we declare the cell correctly detected. The accuracy 
of cell detection is compared in Figure 11 (c) and (d). The 
accuracy is a little lower compared to the wrinkle detection 
accuracy. This is because in the case of cell detection, all 
pixels need to be matched 
342
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
 
 
(a) Camera A and previous method 
(b)  Camera B and previous method 
(c)  Camera C and previous method 
 
 
 
(d)  Camera A and revised method 
(e)  Camera B and revised method 
(f)  Camera C and revised method 
Figure 12.  Robustness comparison  
 
C. Execution time 
Next, we compare the execution time of our previous and 
revised methods spent for cell detection. Here, total execution 
time consists of preprocessing time, cell segmentation time 
and feature extraction time. Figure 13 compares the execution 
time of previous method and revised method spent for 
analyzing one skin image. In the figure, both methods have 
very similar segmentation time since they use same 
segmentation method. But, we can see considerable 
reductions in the preprocessing time and feature extraction 
time. Especially, feature extraction time has been reduced to 
more than 1/3 by removing pixel-based computation. This 
reduction is very critical since feature extraction time occupies 
considerable part of total execution time. As a result, our 
revised method has reduced the execution time by half 
compared with the previous method.  
VI. 
CONCLUSION 
In this paper, we revised our method for skin feature 
extraction to improve its accuracy, efficiency and robustness. 
To improve the accuracy of skin cell detection, we enhanced 
the contrast of wrinkles on the skin by using the contrast 
limited adaptive histogram equalization (CLAHE) and 
highlighted the depth of wrinkles by using the extended-
minima transform. In the experiments, we collected skin 
images using diverse cameras and measured the accuracy and 
execution time. The result shows that the performance has 
improved twice in terms of execution time and the accuracy 
also has improved by 10%. In addition, the performance was 
not affected much by the camera type, which shows the 
robustness of our method. 
 
 
 
Figure 13.  Comparison of excution time 
343
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

ACKNOWLEDGEMENT 
This work was supported by Institute for Information & 
communications Technology Promotion (IITP) grant funded 
by the Korea government (MSIP) (No. R0190-16-2012, High 
Performance Big Data Analytics Platform Performance 
Acceleration Technologies Development). 
 
REFERENCES 
[1] W. Kim, H. Kim, and E. Hwang, “Improving Feature 
Extraction Accuracy for Skin Analysis,” The Ninth 
International Conferences on Advances in Multimedia, pp. 26-
31, April 2017. 
[2] Y. H. Choi, D. Kim, E. Hwang, and B. Kim, "Skin texture 
aging trend analysis using dermoscopy images," Skin Research 
and Technology, vol. 20, no. 4, pp. 486-497, 2014.  
[3] Y.-H. Choi, Y.-S. Tak, S. Rho, and E. Hwang, "Skin feature 
extraction and processing model for statistical skin age 
estimation," Multimedia tools and applications, vol. 64, no. 2, 
pp. 227-247, 2013. 
[4] H. Tanaka et al., "Quantitative evaluation of elderly skin based 
on digital image analysis," Skin research and technology, vol. 
14, no. 2, pp. 192-200, 2008. 
[5] U. Jacobi et al., "In vivo determination of skin surface 
topography using an optical 3D device," Skin Research and 
Technology, vol. 10, no. 4, pp. 207-214, 2004. 
[6] G. O. Cula, P. R. Bargo, A. Nkengne, and N. Kollias, 
"Assessing 
facial 
wrinkles: 
automatic 
detection 
and 
quantification," Skin Research and Technology, vol. 19, no. 1, 
pp. e243-e251, 2013. 
[7] A. P. Yow et al., "Automated in vivo 3D high-definition optical 
coherence tomography skin analysis system," in Engineering 
in Medicine and Biology Society (EMBC), 2016 IEEE 38th 
Annual International Conference of the, pp. 3895-3898, 2016. 
[8] Razalli, et al., "Age Range Estimation Based on Facial Wrinkle 
Analysis Using Hessian Based Filter," Advanced Computer 
and Communication Engineering Technology, Springer 
International Publishing, pp. 759-769, 2016. 
[9] A. Das and D. Ghoshal, "Human Skin Region Segmentation 
Based 
on 
Chrominance 
Component 
Using 
Modified 
Watershed Algorithm," Procedia Computer Science, 89, pp. 
856-863, 2016. 
[10]  R. C. Gonzalez and R. E. Woods, “Digital Image Processing,” 
Addison-Wesley, Third edition, 2008. 
[11] Z. Junxiong, M. Qingqin, L. Wei, and X. Tingting, "Feature 
extraction of jujube fruit wrinkle based on the watershed 
segmentation," International Journal of Agricultural and 
Biological Engineering, vol. 10, no.4, pp.165-172, 2017 
[12] L. J. Belaid and W. Mourou, "Image segmentation: a watershed 
transformation algorithm," Image Analysis & Stereology, vol. 
28, no. 2, pp. 93-102, 2011. 
[13] Pizer, et al., "Contrast-limited adaptive histogram equalization: 
speed and effectiveness," in Visualization in Biomedical 
Computing, Proceedings of the First Conference on, pp. 337-
345, 1990. 
[14] Y. C. Hum, K. W. Lai, and M. I. Mohamad Salim, 
"Multiobjectives bihistogram equalization for image contrast 
enhancement," Complexity, vol. 20, no. 2, pp. 22-36, 2014. 
[15] J. Canny, “A computational approach to edge detection,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 
Vol. 8, No. 6, pp. 679-698, 1986. 
[16] L. Lam, S. W. Lee, and C. Y. Suen, “Thinning Methodologies-
A Comprehensive Survey,” IEEE Transactions on Pattern 
Analysis and Machine Intelligence, Vol 14, Issue 9, pp. 879, 
1992. 
[17] N. Otsu, "A threshold selection method from gray-level 
histograms," Automatica, vol. 11, no. 285-296, pp. 23-27, 1975. 
[18] P. Soille, Morphological image analysis: principles and 
applications. Springer Science & Business Media, 2013. 
[19] A. Othmani, et al., "Region-based segmentation on depth 
images from a 3D reference surface for tree species 
recognition," Image Processing (ICIP), 2013 20th IEEE 
International Conference on. IEEE, pp. 3399-3402, 2013. 
[20] S. Wold, K. Esbensen, and P. Geladi, "Principal component 
analysis," Chemometrics and intelligent laboratory systems, 
vol. 2, no. 1-3, pp. 37-52, 1987.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
344
International Journal on Advances in Software, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/software/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

