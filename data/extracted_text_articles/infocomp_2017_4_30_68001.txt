A Parallel Nonzero CP Decomposition Algorithm
for Higher Order Sparse Data Analysis
Oguz Kaya
Department of Computer Science
´Ecole Normale Superi´eure de Lyon and INRIA, 46 All´ee d’Italie 69007, Lyon, France
e-mail: oguz.kaya@ens-lyon.fr
Abstract—In the age of big data, tensor decomposition methods
are increasingly being used due to their ability to naturally
express and analyze high dimensional big data. Obtaining these
decompositions gets very expensive both in terms of computa-
tional and memory requirements, particularly as the tensor’s
dimensionality increases. To efﬁciently handle such high dimen-
sional tensors, we propose a parallel algorithm for computing a
CP decomposition in which factor matrices are assumed to not
contain any zero elements. This additional constraint enables a
very efﬁcient computation of the costliest step of the algorithms
for computing CANDECOMP/PARAFAC (CP) decomposition for
high dimensional tensors, and the performance gains increase
with the dimensionality of tensor. For an N-dimensional tensor
having k nonzero entries, our method provides O(log k) and
O(log N log k) faster preprocessing times, and performs O(N)
and O(log N) less work in computing a CP decomposition over
two efﬁcient state-of-the-art libraries SPLATT and HYPERTEN-
SOR, respectively. With these algorithmic contributions and a
highly tuned parallel implementation, we achieve up to 16.7x
speedup in sequential, and up to 10.5x speedup in parallel
executions over these libraries on a 28-core workstation. In doing
so, we incur with up to 24x less preprocessing time, and use up to
O(log N) less memory for storing intermediate computations.
We show using a real-world tensor that the accuracy of our
method is comparable to the standard CP decomposition.
Keywords–parallel sparse tensor factorization; CP decomposi-
tion; higher order data analysis.
I.
INTRODUCTION
In parallel with the overwhelming increase in the size of big
data problems, the variety of data features also grows, which in
turn raises the data dimensionality. Such high dimensional big
data can be naturally modeled by tensors, or multi-dimensional
arrays, and effectively analyzed using tensor decomposition
methods. For this reason, tensors have been increasingly used
in many application domains in the recent past, including
the analysis of Web graphs [1], knowledge bases [2], recom-
mender systems [3], signal processing [4], computer vision [5],
health care [6], and many others [7]. In these applications,
tensor decomposition algorithms are employed to perform a
profound analysis of the high dimensional data to extract
hidden information, or predict some missing data elements of
interest. To this end, there have been considerable efforts in
providing numerical algorithms for tensor decompositions [7],
and in developing efﬁcient computational methods and high
performance software to render these algorithms amenable to
use in real-world applications [8]–[13].
One of the most popular tensor decomposition methods is
called the CANDECOMP/PARAFAC decomposition (CP, or
CPD). The most common algorithm for computing the CP
decomposition is an iterative method using alternating least
squares (ALS), and is therefore called CP-ALS [14], [15]. At
the core of the CP-ALS algorithm, each iteration involves a
special operation called the matricized tensor-times Khatri-Rao
product (MTTKRP). For a sparse N-dimensional tensor, this
operation carries out the element-wise multiplication of N −1
matrix row vectors and accumulates their scaled sum, which is
repeated for each nonzero element of the tensor. As the tensor
gets higher dimensional, the cost of this operation dramatically
increases, and efﬁciently carrying out this operation becomes
crucial to be able to process such tensors. For this reason,
there has been signiﬁcant recent efforts in the literature towards
efﬁciently computing this operation in particular, and CP-
ALS in general, in different computational settings such as
MATLAB [8], [16], MapReduce [17], shared memory [11],
and distributed memory parallel environments [9], [13], [18],
[19]. In this paper, we are interested in an efﬁcient parallel
computation of CP-ALS for high dimensional big sparse
tensors in a shared memory environment, which is particularly
motivated by emerging big data applications [6].
We summarize our contributions in this work as following:
•
We introduce a method for efﬁciently computing
MTTKRP for high dimensional sparse tensors. This
scheme asymptotically reduces the computational cost
of MTTKRP as well as the cost of a common pre-
computation step performed in the state-of-the-art.
•
We provide an efﬁcient parallelization of this compu-
tational scheme that runs up to 10.5 times faster than
the state-of-the-art on a 28-core workstation.
The rest of the paper is organized as follows. In Section II,
we provide our tensor notation and some tensor operations,
then describe the CP-ALS algorithm and the MTTKRP oper-
ation. Next, in Section III, we provide an overview of existing
methods for computing CP-ALS together with a summary of
their computational and memory costs. Then, in Section IV
we describe our method for computing MTTKRP and CP-
ALS, which imposes a nonzero constraint on factor matrices to
reduce the computational costs. We compare the complexity of
our approach with the state of the art, and discuss a carefully
tuned parallelization of this approach for a shared memory
NUMA architecture. Finally, Section V provides a comparison
of sequential and parallel executions of our method with two
state-of-the-art implementations.
II.
BACKGROUND
A. Notation
We mostly follow the tensor notation used in [7], [19]. We
denote the set {1, . . . , k} of integers by Nk for k ∈ Z+. We
denote vectors using bold lowercase Roman letters, as in x.
Similarly for matrices, we use bold uppercase Roman letters,
e.g., X. For tensors, we use bold calligraphic fonts, e.g., X. We
deﬁne the order of a tensor as the number of its dimensions or
modes, and denote it by N. We use italic lowercase letters with
corresponding indices to represent vector, matrix, and tensor
40
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

elements, e.g., xi for a vector x, xij for a matrix X, and xijk
for a 3-dimensional tensor X. For column vectors of a matrix,
we use the corresponding lowercase letters with a subscript
corresponding to the column index, e.g., xi to denote the ith
column of X. A slice of a tensor in the nth mode is a set
of tensor elements obtained by ﬁxing the index only along
the nth mode. For matrix rows and columns as well as tensor
slices, we use the MATLAB notation, e.g., X(i, :) and X(:, j)
are the ith row and the jth column of X, whereas X(:, :, k)
represents the kth slice of X in the third dimension.
A tensor X ∈ RI1×···×IN can be matricized; a matrix
X can be associated with X by identifying a subset of its
modes to correspond to the rows of X, and the rest of the
modes to correspond to the columns of X. This is done by
mapping the corresponding elements of X to those of X.
We will be exclusively dealing with the matricizations of
tensors along a single mode, meaning that a single mode is
mapped to the rows of the resulting matrix, and the rest of
the modes correspond to its columns. We use X(d) to denote
matricization along the mode d, e.g., for X ∈ RI1×···×IN ,
the matrix X(1) ∈ RId×I1···Id−1Id+1···IN denotes the mode-1
matricization of X. Speciﬁcally, in this matricization the tensor
element xi1,...,iN corresponds to the element of X(1) with
row and column indices

i1, i2 + PN
j=3
h
(ij − 1) Qj−1
k=2 Ik
i
.
Matricizations in other modes are deﬁned similarly.
The Hadamard product of two vectors u, v ∈ RI is a
vector w = u ∗ v, w ∈ RI, where wi = ui · vi for i ∈ NI.
Similarly, Hadamard division of the two vectors is a vector
w = u ⊘ v with elements wi = ui/vi, vi ̸= 0. Hadamard
product and division of matrices of same size are deﬁned
similarly. The Kronecker product of vectors u ∈ RI and
v ∈ RJ results in the vector w = u ⊗ v where w ∈ RIJ
is deﬁned as
w = u ⊗ v =


u1v
u2v
...
uIv

 .
For matrices U ∈ RI×K and V ∈ RJ×K, their Khatri-Rao
product corresponds to their column-wise Kronecker product
W = U ⊙ V = [u1 ⊗ v1, . . . , uK ⊗ vK] ,
(1)
where W ∈ RIJ×K. We use the shorthand notation ◦i̸=nU(i)
to denote an associative operation U(1)◦· · ·◦U(n−1)◦U(n+1)◦
· · · ◦ U(N) over a set {U(1), . . . , U(N)} of matrices (and
similarly for vectors).
B. CP decomposition
The rank-R CP-decomposition of a tensor X expresses
X as the sum of R rank-1 tensors. For instance, for X ∈
RI×J×K, with CP decomposition we obtain X ≈ PR
r=1 ar ◦
br ◦ cr where ar ∈ RI, br ∈ RJ, and cr ∈ RK. This de-
composition gives an element-wise approximation (or equality)
xi,j,k ≈ PR
r=1 airbjrckr. The minimum R value rendering this
approximation an equality for all tensor elements is called as
the rank (or CP-rank) of the tensor X. Here, the matrices
A = [a1, . . . , aR], B = [b1, . . . , bR], and C = [c1, . . . , cR]
are called the factor matrices, or factors. For an N-mode tensor
X ∈ RI1×···×IN , we use U(1), . . . , U(N) to refer to the factor
matrices respectively having I1, . . . , IN rows and R columns.
Input: X: An N-mode tensor, X ∈ RI1,...,IN
R: The rank of CP decomposition
U(1), . . . , U(N): Initial factor matrices
Output: [[λ; U(1), . . . , U(N)]]: The rank-R CP decomposition of X
1: for n = 1, . . . , N do
▶ Initialization
2:
W(n) ← U(n)T U(n)
3: repeat
4:
for n = 1, . . . , N do
5:
M(n) ← X(n)(⊙(i̸=n)U(i))
▶ MTTKRP
6:
H(n) ← ∗(i̸=n)W(n)
7:
U(n) ← M(n)H(n)†
8:
λ ← COLUMN-NORMALIZE(U(n))
9:
W(n) ← U(n)T U(n)
10: until convergence or the maximum number of iterations
11: return [[λ; U(1), . . . , U(N)]]
Figure 1. CP-ALS: ALS algorithm for computing CPD.
Input: X: N-dimensional sparse tensor
U(1) . . . U(N): Factor matrices
n: The dimension of matricization for MTTKRP
Output: M(n): MTTKRP result matrix of size In × R
1: M(n) ← 0
2: for xi1,...,in,...,iN ∈ X do
3:
M(n)(in, :) += xi1,...,in,...,iN
h
∗(j̸=n)

U(j)(ij, :)
i
Figure 2. Performing MTTKRP for a sparse tensor X in a mode n.
The standard algorithm for computing the CP decomposi-
tion is CP-ALS, which establishes a good trade-off between
convergence rate (number of iterations) and iteration cost [7]. It
is an iterative algorithm, shown in Figure 1, that progressively
updates the factors U(n) in an alternating fashion starting from
an initial guess, which can be randomly set or obtained from
the truncated SVD of the matricizations of X [7]. CP-ALS
iterates until it can no longer improve the solution, or it reaches
the allowed maximum number of iterations. Each iteration of
CP-ALS consists of N subiterations, and the nth subiteration
updates U(n) using X as well as other factor matrices.
Computing the matrix M(n)
∈ RIn×R at Line 5 of
Figure 1 is the sole part involving the tensor X, and is
the most expensive computational step of the CP-ALS al-
gorithm, both for sparse and dense tensors. The operation
M(n) ← X (n)(⊙(i̸=n)U(i)) is called the matricized tensor-
times Khatri-Rao product (MTTKRP). The Khatri-Rao product
of the involved U(n)s deﬁnes a matrix of size (Q
i̸=n Ii) × R
according to Equation (1), which can get very costly in terms
of computational and memory requirements when Ii or N is
large—which is indeed the case for many real-world sparse
tensors. To alleviate this, various methods are proposed in the
literature that enable performing MTTKRP without forming
the Khatri-Rao product, which is made possible by exploiting
the sparsity of the tensor. In Figure 2, we provide a such
algorithm for computing MTTKRP in mode n using a sparse
tensor X. This computation amounts to performing Hadamard
product of N − 1 vectors of size R and accumulating these
products for each nonzero element of the tensor. The overall
cost of the algorithm is O(nnz(X)NR) as each nonzero
induces an Hadamard product of N − 1 row vectors followed
by an addition of a row vector of size R.
III.
RELATED WORK
Computing the CP decomposition of a big sparse tensor
can get very costly; for this reason, computing it efﬁciently
41
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

by exploiting the sparsity of the tensor has attracted signif-
icant recent interest in the scientiﬁc community in various
computational settings. In [20], Bader and Kolda demonstrate
efﬁciently carrying out many sparse tensor operations, includ-
ing MTTKRP, in MATLAB. Their approach for MTTKRP
translates into Figure 2. In [21], Chi and Kolda present an al-
ternative Alternating Poisson Regression (CP-APR) algorithm
for computing the CP decomposition of large scale sparse
datasets. GIGATENSOR [17] provides a distributed memory
parallelization of CP-ALS using the Map-Reduce framework.
DFACTO [18] is a distributed memory parallel implemen-
tation (C++, MPI) that formulates MTTKRP as a series of
sparse matrix-vector multiplication. SPLATT [11]–[13] is an
efﬁcient parallelization of MTTKRP and CP-ALS both in
shared [11], [12] and distributed memory environments [13]
using OpenMP and MPI, and is implemented in C. DFACTO
and SPLATT aim to reduce the cost of Figure 2 with the
help of the following observation. For a 3-dimensional tensor
X having nonzeros xi,j,k1 and xi,j,k2, one can ﬁrst compute
xi,j,k1U(3)(k1, :) + xi,j,k2U(3)(k2, :), then multiply this result
with U(2)(j, :) to obtain the ﬁnal contribution to M(1)(i, :). In
other words, multiplying all tensor nonzeros with a matrix in
one dimension before proceeding to the other can potentially
reduce the number of Hadamard multiplications performed,
and this reduction is possible owing to overlapping nonzero
indices. However, the worst-case complexity of this approach
stays the same, i.e., O(nnz(X)NR). HYPERTENSOR [10] is
an efﬁcient sparse tensor factorization library implemented in
C++ using OpenMP and MPI for parallelism. It employs a data
structure called dimension tree to reduce the MTTKRP cost by
storing and reusing some intermediate results for MTTKRP in
the course of CP-ALS iterations. MTTKRP is expressed as
a series of R tensor-times-vector multiply operations, whose
amortized cost translates to O(nnz(X) log NR) for each tensor
dimension, which provides signiﬁcant performance gains as the
tensor’s dimensionality increases.
IV.
PARALLEL CP DECOMPOSITION USING NONZERO
FACTORS
Here, we ﬁrst introduce our approach for efﬁciently per-
forming MTTKRP with the assumption that factor matrices
do not involve zeros or very small entries. When a such
entry U(n)(i, j) with |U(n)(i, j)| < ϵ is encountered in the
course of CP-ALS, we slightly “perturb” the factor matrix
by replacing it with sign(U(n)(i, j))ϵ where sign(x) equals
to -1 if x is negative, and 1 otherwise. In practice such a
perturbation is expected to have a negligible impact on the
quality of solution for a sufﬁciently small ϵ. Next, we introduce
a shared memory parallelization of this scheme, and argue
how to establish load balance among processes. Finally, we
discuss optimization strategies for better parallel performance
on a NUMA architecture.
A. Computing CP decomposition with nonzero factors
The cost of the algorithm in Figure 1 is dominated by the
MTTKRP step at Line 5 that involves the multiplication of the
elements of the sparse tensor X with the rows of N −1 factor
matrices at each subiteration. This amounts to performing
N − 1 vector Hadamard products and a vector addition for
each nonzero element of the tensor, as pointed out at Line 3
of Figure 2. Here, we present a new technique for efﬁciently
performing this costly step with the nonzero factor matrix
assumption. In this case, for each nonzero xi1,...,iN ∈ X,
instead of performing the Hadamard product
of N − 1 row
vectors, one can precompute a vector zi1,...,iN
∈ RR as
zi1,...,iN = ∗(n∈NN)U(n)(in, :), then perform the MTTKRP
update due to this nonzero as M(n)(in, :) += zi1,...,iN ⊘
U(n)(in, :). A similar idea is also employed in the CP-APR
algorithm for handling sparse tensors [21]. Here, the cost
per nonzero reduces to a single Hadamard division, which
can always be performed since U(n)(in, j) ̸= 0. Once new
U(n) is computed using M(n) at Line 7, zi1,...,iN needs to
be updated accordingly with the new U(n)(in, :). This can
be done by dividing it with the old value of U(n)(in, :),
then multiplying by its new value, which amounts to a single
Hadamard multiplication and division. This way, instead of
N − 1 vector Hadamard products, we perform a Hadamard
multiplication and two Hadamard divisions for each tensor
element, which effectively reduces the cost of MTTKRP to
O(nnz(X)R). In contrast, SPLATT and DFACTO require up to
N − 1 vector Hadamard products per tensor nonzero, yielding
the worst-case complexity O(nnz(X)NR), and HYPERTEN-
SOR takes O(nnz(X) log NR) time. Therefore, our approach
provides signiﬁcant computational gains over all these methods
particularly as X gets higher dimensional. In doing so, we use
only U(n) for executing CP-ALS in dimension n, whereas
SPLATT and DFACTO access all N − 1 factor matrices except
U(n); hence, our method also yields a better memory footprint.
In our method, we need to store the matrix Z which
takes O(nnz(X)R) space. In contrast, HYPERTENSOR uses
O(log N) buffers each taking up to O(nnz(X)R) space.
SPLATT uses only O(PR) memory for intermediate results
for an execution using P threads, yet it incurs the highest
computational cost.
B. Parallelization
Performing MTTKRP in a mode n amounts to performing
a divide-add operation for each vector zi1,...,iN to eventually
form the matrix row M(n)(in, :). Similarly, after the new
U(n)(in, :) is computed, one needs to update the vector
zi1,...,iN with a Hadamard multiplication and a division. There-
fore, all nonzero elements of X whose nth index equals to in
contributes a summand to M(n)(in, :), and the corresponding
vectors in Z needs to be updated subsequently using the
old and new values of U(n)(in, :). To perform this, for each
dimension n ∈ {1, . . . , N} and for each matrix row in ∈ In
we compute a reduction list of tensor nonzeros whose nth
index is in, which we denote as rl(n)(in). This way, each
row M(n)(in, :) can be computed in parallel by performing
|rl(n)(in)| vector operations. Similarly, once U(n) is com-
puted, one can process each row in in parallel to update the
corresponding vectors zi1,...,iN for each contributing nonzero
xi1,...,iN .
The parallel algorithm for computing the CP decomposition
is shown in Figure 3. In the main subiteration loop, we
ﬁrst compute the MTTKRP result M(n) in parallel using
P processes at Lines 4–9. This step assumes a partition
I(n)
1
, . . . , I(n)
P
of row indices 1, . . . , In. Each process p com-
putes the set I(n)
p
of rows of the matrix M(n) independently
thanks to the precomputed reduction lists. Immediately after
the process uses the entry zi1,...,iN in MTTKRP, it divides
it by the old value of U(n)(i, :). Once M(n) is formed,
at Line 10 we compute H(n) by performing the Hadamard
42
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

Input: X: An N-mode tensor, X ∈ RI1,...,IN
R: The rank of CP decomposition
U(1), . . . , U(N): Initial factor matrices with nonzero entries
Output: [[λ; U(1), . . . , U(N)]]: The rank-R CP decomposition of X
1: Initialize Z and W(n) for all n ∈ NN.
2: repeat
3:
for n = 1, . . . , N do
4:
parallel for p = 1 . . . P do
▶ Compute M(n)(I(n)
p
, :)
5:
for in ∈ I(n)
p
do
6:
M(n)(in, :) ← 0
7:
for zi1,...,iN ∈ rl(n)(in) do
8:
M(n)(in, :) += zi1,...,iN /U(n)(in, :)
9:
zi1,...,iN = zi1,...,iN ⊘ U(n)(in, :)
10:
H(n) ← ∗(i̸=n)W(i)
▶ Matrix Hadamard product
11:
U(n) ← M(n)H(n)†
▶ Row-parallel GEMM
12:
λ ← NONZERO-COLUMN-NORMALIZE(U(n))
13:
W(n) ← U(i)T U(i)
▶ Row-parallel SYRK
14:
parallel for p = 1 . . . P do
▶ Update Z
15:
for in ∈ I(n)
p
do
16:
for zi1,...,iN ∈ rl(n)(in) do
17:
zi1,...,iN = zi1,...,iN ∗ U(n)(in, :)
18: until convergence or reaching maximum number of iterations
19: return [[λ; U(1), . . . , U(N)]]
Figure 3. Parallel CP-ALS with nonzero factors.
product of R × R matrices W(k) for k ̸= n, whose cost
is negligible as R is a small constant in practice. Next, at
Line 11 we update the factor U(n) in a parallel dense matrix
multiplication step, in which each process p performs the
multiplication M(n)(I(n)
p
, :)H(n)†. Once U(n) is computed,
we swap its small entries with ϵ or −ϵ and normalize its
columns in a combined step at Line 12 in which each process
p works on the sub-matrix U(n)(I(n)
p
, :). Using the updated
U(n), we ﬁrst compute the new W(n) in another parallel
dense matrix multiplication step at line Line 13, where the
process p similarly performs U(n)(I(n)
p
, :)T U(n)(I(n)
p
, :), then
multiply the entries of Z with the corresponding rows of
U(n) using the same parallelization scheme as in Line 4. The
initialization of matrices Z as well as W(n) at Line 1 are done
in parallel similar to the manner of updating these matrices in
the iteration loop. At the end of each iteration, one has to check
the convergence as well. This computation takes insigniﬁcant
amount of time [19], hence we skip the details.
Reduction lists for a dimension n can be computed by
making two passes over the tensor nonzero indices in nth
dimension to form a very efﬁcient compressed data structure
consisting of rl(n), which correspond to reduction list pointers,
and rli(n), which correspond to the elements in the reduction
list, in O(nnz(X)) time. This yields O(Nnnz(X)) cost for all
dimensions, and we can process each dimension in parallel.
In contrast, existing methods in the literature require sorting
the tensor indices which takes O(Nnnz(X) log(nnz(X))) time
for SPLATT [12], and O(N log Nnnz(X) log(nnz(X))) for
HYPERTENSOR [19]. We show this data structure for a small
tensor X ∈ R5×5×5×5 in Figure 4, and skip the computational
details.
C. Load balancing
For a P-way parallel execution of Figure 3, one needs to
partition the row indices 1, . . . , In into P sets I(n)
1
, . . . , I(n)
P
for each dimension n. There are two types of computational
costs imposed on each process p by a such partition. First,
1 2 3 4
2 5 1 2
4 3 4 5
3 2 5 1
5 3 4 2
3 5 4 1
1 2 5 4
2 4 3 2
3 2 2 1
4 1 4 3
1
2
3
4
5
6
7
8
9
10
Z
1
7
2
8
4
6
9
3
10
5
1
2
3
4
5
M
(1)
U
(1)|
rl
(1)
rli
(1)
Figure 4. Performing MTTKRP for a 4-dimensional tensor X ∈ R5×5×5×5
in the ﬁrst mode. Red (dark) and yellow (light) colors represent memory
regions that are ﬁrst touched by two different threads.
the process p performs O(P
i∈I(n)
p
|rl(n)(i)|) vector Hadamard
operations at Lines 8, 9 and 17. Second, it performs the
multiplication of matrices of size |I(n)
p
| × R and R × R at
Line 11, and of two matrices of size |I(n)
p
|×R at Line 13. To
balance the ﬁrst cost pertaining to sparse tensor computations,
one has to make sure that the associated cost P
i∈I(n)
p
|rl(n)(i)|
is partitioned equitably to processes. Regarding the second cost
for dense matrix operations, each process should have equal
number of rows, i.e., |I(n)
p
| should be balanced. Though these
rows can be partitioned arbitrarily, in practice we desire to
assign a contiguous set of rows to each thread to preserve
the data locality. This not only helps improve the memory
footprint of the MTTKRP step, but also increases the efﬁciency
of BLAS routines used for dense matrix computations.
We deﬁne partitioning problem in this case as follows. For
each row in, we have an associated pair (|rl(n)(in, :)|, 1) of
costs that corresponds to sparse tensor and dense matrix com-
putations, respectively. We aim to partition this “chain” of rows
into P contiguous parts so that both cost metrics are balanced
across processes. The single-cost version of this problem cor-
responds to the chains-on-chains partitioning (CCP) problem
in the literature for which many fast optimal algorithms and
effective heuristics exist [22]. We employ CCP algorithms by
using only the ﬁrst cost metric |rl(n)(i, :)| for partitioning,
as we observe that in practice, balancing this metric also
establishes good row-balance.
D. Optimizations for NUMA scalability
Performing MTTKRP for sparse tensors is an extremely
memory bound operation as the tensor is very sparse in
general, and the data accesses due to tensor nonzeros lack
locality. Therefore, optimizing the memory footprint of the
implementation plays a crucial role in obtaining high per-
formance. Particularly on a NUMA architecture, one has
to carefully allocate memory pages in NUMA nodes to be
able to utilize the available memory bandwidth at maximum,
and distribute the memory pages equitably across NUMA
nodes. In most systems, this can be ensured by properly
using memory ﬁrst-touch policies after allocation, which in
turn yields adequate memory page-to-socket bindings. In our
implementation, after the allocation each thread performs a
ﬁrst-touch on the matrix rows as well as the rows of rl(1) and
rli(1) that it owns. For the matrix Z, each thread initializes a
block of nnz(X)/P vectors. This way, we not only maximize
the NUMA bandwidth utilization, but also aim to reduce the
inter-NUMA node memory requests as much as possible. This
allocation scheme together with a balanced partitioning is
43
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

TABLE I. PER-ITERATION CP-ALS RUNTIME RESULTS (IN
SECONDS) FOR SEQUENTIAL, SINGLE-SOCKET (14 THREADS) AND
DUAL-SOCKET PARALLEL EXECUTIONS OF ALL METHODS.
Method | Data
ten-4D
ten-8D
ten-16D
ten-32D
P = 1
splatt
175.1
791.2
3766.9
19994.8
hypertensor
98.3
306.3
929.3
2873.8
cp-eps
130.3
284.9
586.9
1199.4
P = 14
splatt
15.1
68.4
280.3
1292.2
hypertensor
11.8
33.3
88.5
354.5
cp-eps
13.1
27.5
56.12
111.31
P = 28
splatt
11.5
47.2
190.7
683.8
hypertensor
13.0
36.6
69.1
215.0
cp-eps
8.3
17.5
36.2
65.3
shown in Figure 4 for two threads.
V.
EXPERIMENTS
We compared our algorithm with two state-of-the-art im-
plementations, SPLATT and HYPERTENSOR, and ran them on
a workstation having 768GBs of memory and two Intel(R)
Xeon(R) E5-2695 CPUs, each having 14 cores running at
2.30GHz as well as L1, L2, and L3 caches of sizes 32K, 256K,
and 35M, respectively. We performed two types of experiments
to measure the parallel scalability and the accuracy of our
method. The ﬁrst experiment compares the runtime of three
methods using synthetically generated high dimensional sparse
tensors. The second experiment uses a real-world tensor to
compare the quality of approximation of the standard CP-
ALS computation with our method. In all experiments, we
use ϵ = 10−6 as the threshold parameter.
A. Scalability
We compare the runtime of three methods using 4, 8,
16, and 32-dimensional randomly (uniform) generated tensors
of size 10M at each dimension, and having 100M nonzero
elements. We employ synthetic data instead of real-world
tensors for two reasons. First, with random data we are
able to control the dimensionality of the tensor while ﬁxing
other tensor parameters, e.g., dimension sizes, the number
of nonzeros, and the distribution of nonzero indices; thereby,
observe the performance of the algorithms with the increasing
tensor dimensionality in a controlled manner. Second, there is
a lack of available big high dimensional sparse tensors in the
literature in parallel to the lack of efﬁcient computational tools
to handle such tensors. We run all three implementations using
1, 14 (single socket), and 28 cores/threads (two sockets) for 20
CP-ALS iterations using R = 16, and report the average time
spent per iteration in Table I with labels splatt, hypertensor,
and cp-eps corresponding to SPLATT, HYPERTENSOR, and
our algorithm provided in Figure 3, respectively.
In Table I, we observe that using ten-4D, hypertensor runs
the fastest using 1 and 14 cores, yet cp-eps surpasses hyper-
tensor using 28-cores owing to better NUMA optimizations
described in Section IV-D. In all other instances, cp-eps stays
the fastest among all three methods, and the performance gains
increase steadily as the tensor’s dimensionality grows. Using
4-dimensional to 32-dimensional tensors, we observe that the
TABLE II. INITIAL SETUP TIME (IN SECONDS) FOR PARALLEL
SPARSE CP-ALS.
Method | Data
ten-4D
ten-8D
ten-16D
ten-32D
splatt
60
84
232
617
hypertensor
70
167
418
983
cp-eps
16
18
23
41
1
14
28
Number of cores
0.0
5.0
10.0
15.0
20.0
Speedup over splatt
1.00
1.00
1.00
7.00
3.61
3.20
16.70
11.60
10.50
splatt
hypertensor
cp-eps
Figure 5. Speedup over splatt on ten32D.
speedup of cp-eps over splatt consistently increases from
1.39x to 10.47x using 28 threads, and from 1.34x to 16.67x
using a single thread, which conforms with the algorithm
complexities provided in Section IV-A. Similarly, the speedup
of cp-eps over hypertensor varies from 1.56x to 3.29x using
28 threads, and from 0.75x to 2.40x using a single thread.
Figure 5 demonstrates the speedup results of cp-eps and
hypertensor over splatt for ten-32D.
In Table II, we compare the time spent on setting up data
structures for MTTKRP using 28 threads on all datasets. We
observe that cp-eps performs the preprocessing step up to 15x
faster than splatt, and up to 24x faster than hypertensor.
This signiﬁcant improvement is possible owing to the smaller
asymptotic complexity described in Section IV-B, as well as
the parallelization using cp-eps.
B. Accuracy
In computing Figure 3, we impose the constraint on factor
matrices that they do not contain very small elements, which
perturbs the decomposition slightly and can potentially affect
the quality of approximation. To assess this, we compare the
accuracy of this method with that of the original CP-ALS algo-
rithm. We employ a 3-dimensional real-world tensor obtained
from the Never Ending Language Learning (NELL) dataset
of the “Read the Web” project [2], which consists of tuples
of the form (entity × relation × entity) such as (’Chopin’,
’plays musical instrument’, ’piano’). Nonzeros of this tensor
correspond to such “facts” discovered by NELL from the web,
while the values of nonzeros are set to the “belief” scores for
these facts. The tensor is of size 1.6M×297×338K and has 2M
nonzeros. We run hypertensor and cp-eps 100 times with the
rank of approximation R ∈ {25, 50, 75, 100}, and compute
the geometric mean of approximation quality in each case.
In Figure 6 we detail all these results. We observe that both
methods produce equally good approximations to the original
44
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

25
50
75
100
The rank of approximation
0.0
5.0
10.0
15.0
20.0
25.0
30.0
35.0
40.0
The quality of approximation (percentage)
13.35
22.38
29.35
35.45
13.35
22.38
29.34
35.45
hypertensor
cp-eps
Figure 6. Accuracy comparison of hypertensor and cp-eps on NELL-2. We
show the geometric mean of approximation values of 100 CP-ALS
executions with random initial factor matrices using both methods.
tensor up to a small margin of error for R = 75 mostly due
to randomization in factor matrix initialization. This shows
that the nonzero constraint on factor matrices indeed has a
negligible effect on the accuracy.
VI.
CONCLUSION
In this work, we propose an efﬁcient parallel algorithm for
computing a CP decomposition in which the factor matrices
are assumed to not contain any zero elements. This constraint
enables a computational scheme that provides O(log k) and
O(log N log k) faster preprocessing times for a tensor with
k nonzero entries, and performs O(N) and O(log N) less
MTTKRP work over two efﬁcient state-of-the-art implemen-
tations splatt and hypertensor with a negligible effect on
the accuracy. We achieve up to 16.7x speedup in sequential
and 10.5x speedup in parallel executions over these methods,
with up to 24x less data preprocessing time, using up to
O(log N) less memory for storing intermediate computations.
With these advancements, our approach renders the analysis
of higher order big sparse datasets amenable both in terms
of computational and memory requirements for real world
applications.
ACKNOWLEDGMENT
Part
of
this
work
was
supported
by
GENCI-
[TGCC/CINES/IDRIS]
(Grant
2016-17
-
i2016067501),
and was performed using compute resources at ENS de Lyon.
REFERENCES
[1]
T. G. Kolda and B. Bader, “The TOPHITS model for higher-order web
link analysis,” in Proceedings of Link Analysis, Counterterrorism and
Security 2006, 2006, pp. 26–29.
[2]
A. Carlson et al., “Toward an architecture for never-ending language
learning,” in AAAI, vol. 5, 2010, p. 3.
[3]
S. Rendle and T. S. Lars, “Pairwise interaction tensor factorization for
personalized tag recommendation,” in Proceedings of the Third ACM
International Conference on Web Search and Data Mining, ser. WSDM
’10.
New York, NY, USA: ACM, 2010, pp. 81–90.
[4]
L. D. Lathauwer and B. D. Moor, “From matrix to tensor: Multilinear
algebra and signal processing,” in Institute of Mathematics and Its
Applications Conference Series, vol. 67, 1998, pp. 1–16.
[5]
M. A. O. Vasilescu and D. Terzopoulos, “Multilinear analysis of image
ensembles: TensorFaces,” in Computer Vision—ECCV 2002. Springer,
2002, pp. 447–460.
[6]
I. Perros, R. Chen, R. Vuduc, and J. Sun, “Sparse hierarchical Tucker
factorization and its application to healthcare,” in Data Mining (ICDM),
2015 IEEE International Conference on, Nov 2015, pp. 943–948.
[7]
T. G. Kolda and B. Bader, “Tensor decompositions and applications,”
SIAM Review, vol. 51, no. 3, 2009, pp. 455–500.
[8]
B. W. Bader et al., “Matlab tensor toolbox version 2.6,” Available
online, Retrieved: March 2017.
[9]
O. Kaya and B. Uc¸ar, “Scalable sparse tensor decompositions in
distributed memory systems,” in Proceedings of the International Con-
ference for High Performance Computing, Networking, Storage and
Analysis, ser. SC ’15.
New York, NY, USA: ACM, 2015, pp. 77:1–
77:11.
[10]
——, “High performance parallel algorithms for the Tucker decompo-
sition of sparse tensors,” in 45th International Conference on Parallel
Processing (ICPP ’16), Aug 2016, pp. 103–112.
[11]
S. Smith, N. Ravindran, N. D. Sidiropoulos, and G. Karypis, “SPLATT:
Efﬁcient and parallel sparse tensor-matrix multiplication,” in 29th IEEE
International Parallel & Distributed Processing Symposium.
Hyder-
abad, India: IEEE Computer Society, May 2015, pp. 61–70.
[12]
S. Smith and G. Karypis, “Tensor-matrix products with a compressed
sparse tensor,” in Proceedings of the 5th Workshop on Irregular Appli-
cations: Architectures and Algorithms.
ACM, 2015, p. 7.
[13]
——, “A medium-grained algorithm for sparse tensor factorization,” in
2016 IEEE International Parallel and Distributed Processing Sympo-
sium, IPDPS 2016, Chicago, IL, USA, May 23-27, 2016, 2016, pp.
902–911.
[14]
D. J. Carroll and J. Chang, “Analysis of individual differences in
multidimensional scaling via an N-way generalization of “Eckart-
Young” decomposition,” Psychometrika, vol. 35, no. 3, 1970, pp. 283–
319.
[15]
R. A. Harshman, “Foundations of the PARAFAC procedure: Models and
conditions for an “explanatory” multi-modal factor analysis,” UCLA
Working Papers in Phonetics, vol. 16, 1970, pp. 1–84.
[16]
C. A. Andersson and R. Bro, “The N-way toolbox for MATLAB,”
Chemometrics and Intelligent Laboratory Systems, vol. 52, no. 1, 2000,
pp. 1–4.
[17]
U. Kang, E. Papalexakis, A. Harpale, and C. Faloutsos, “GigaTensor:
Scaling tensor analysis up by 100 times - Algorithms and discoveries,”
in Proceedings of the 18th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ser. KDD ’12.
New York,
NY, USA: ACM, 2012, pp. 316–324.
[18]
J. H. Choi and S. V. N. Vishwanathan, “DFacTo: Distributed factor-
ization of tensors,” in 27th Advances in Neural Information Processing
Systems, Montreal, Quebec, Canada, 2014, pp. 1296–1304.
[19]
O. Kaya and B. Uc¸ar, “Parallel CP decomposition of sparse tensors
using dimension trees,” Inria - Research Centre Grenoble – Rhˆone-
Alpes, Research Report RR-8976, Nov. 2016.
[20]
B. W. Bader and T. G. Kolda, “Efﬁcient MATLAB computations with
sparse and factored tensors,” SIAM Journal on Scientiﬁc Computing,
vol. 30, no. 1, December 2007, pp. 205–231.
[21]
E. C. Chi and T. G. Kolda, “On tensors, sparsity, and nonnegative
factorizations,” SIAM Journal on Matrix Analysis and Applications,
vol. 33, no. 4, 2012, pp. 1272–1299.
[22]
A. Pinar and C. Aykanat, “Fast optimal load balancing algorithms for
1D partitioning,” Journal of Parallel and Distributed Computing, vol. 64,
no. 8, 2004, pp. 974 – 996.
45
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

