Fast Singular Value Decomposition for Large-scale Growing Data
Jengnan Tzeng
Department of Mathematical Sciences, National Chengchi University
Taipei, Taiwan
jengnan@math.nccu.edu.tw
Abstract—Singular value decomposition (SVD) is a funda-
mental technique in linear algebra, and it is widely applied
in many modern information technologies, for example, high
dimensional data visualization, dimension reduction, data min-
ing, latent semantic analysis, etc. However, when the matrix size
of the data is huge and continuously growing, the matrix can
not be loaded all at once into the computer memory and O(n3)
computational cost of SVD becomes infeasible. To resolve this
problem, we will adapt a fast multidimensional scaling method
to obtain a fast SVD method, given that the signiﬁcant rank of
a huge matrix is small. This proposed fast SVD method can be
easily implemented via parallel computing. We also propose a
fast update method to be applied when the huge data is updated
continuously. We will demonstrate that the approximated SVD
result is sufﬁciently accurate, and most importantly it can be
derived very efﬁciently. Using this fast update method, many
modern techniques based on SVD which were infeasible will
become viable.
Keywords-Singular value decomposition; multidimensional
scaling; parallel computing; huge matrix.
I. INTRODUCTION
Singular value decomposition (SVD) and Principle com-
ponent analysis (PCA) are two fundamental techniques in
linear algebra and statistics. There are many modern appli-
cations based on these two tools, such as linear discriminate
analysis [1], multidimensional scaling analysis [2], feature
extraction, high dimensional data visualization, etc. In recent
years, digital information has been proliferating and many
analytic methods based on PCA and SVD are facing the
challenge of their signiﬁcant computational cost. Thus, it is
crucial to develop a fast method of PCA and SVD.
In 2008, Tzeng et al. [4] developed a fast multidimen-
sional scaling (MDS) method which turned the classical
O(n3) MDS method to be linear. MDS is a method to
represent the high dimensional data into the low dimensional
conﬁguration. Because of the phenomenon of the curse
of dimensionality, MDS is widely used in data mining,
clustering, and many recommendation systems for web
services. When the data conﬁguration is Euclidean, MDS is
similar to principle component analysis (PCA), in that both
can remove inherent noise with its compact representation
of data. The O(n3) computational complexity makes it
infeasible to apply to huge data, for example, when the
sample size is more than one million.
In the following section, we will show how to adapt clas-
sical MDS to be a fast Split-and-combine MDS (SCMDS).
And using this SCMDS, we can modify the PCA and SVD
method to become fast methods.
II. METHODOLOGY
In 2008, we adapted the classical MDS so as to reduce
the original O(n3) complexity to O(n) [4], in which we
have proved that when the data dimension is signiﬁcantly
smaller than the number of data entries, there is a fast linear
method for classical MDS. The following section begins
with a review of SCMDS. Then we will demonstrate how
to adapt SCMDS method to become the fast PCA, and with
further modiﬁcation, the fast PCA can become the fast SVD.
A. From MDS to SCMDS
The main idea of fast MDS is using statistical resampling
to split data into overlapping subsets. We perform the
classical MDS on each subset and get the compact Euclidean
conﬁguration. Then we use the overlapping information
to combine each conﬁguration of subsets to recover the
conﬁguration of the whole data. Hence, we named this fast
MDS method by Split-and-combine MDS (SCMDS).
Assume X1 and X2 are matrices in which the columns are
the two coordinates of the overlapping points obtained by
applying MDS to two grouped data sets. Then there exists an
afﬁne mapping that maps X1 to X2. Let ¯X1 and ¯X2 be the
means of columns of X1 and X2, respectively. In order to
obtain the afﬁne mapping, we apply QR factorization to both
X1 − ¯X1 ˙1T and X2 − ¯X2 ˙1T . Then we have X1 − ¯X1 ˙1T =
Q1R1 and X2 − ¯X2 ˙1T = Q2R2. It is clear that the mean
of the center of column vectors of X1 − ¯X1 ˙1T has been
shifted to zero. Because X1 and X2 come from the same
data set, the difference between X1 − ¯X1 ˙1T and X2 − ¯X2 ˙1T
is a rotation. Therefor, the triangular matrices R1 and R2
should be identical when there is no noise in X1 and X2.
Due to randomness of the sign of columns of Qi in QR
factorization, the sign of columns of Qi might need to be
adjusted according to the corresponding diagonal elements
of Ri, so that the signs of tridiagonal elements of R1 and
R2 are the same.
After necessary modiﬁcation to the sign of columns of
Qi, we conclude that
QT
1 (X1 − ¯X1 ˙1T ) = QT
2 (X2 − ¯X2 ˙1T ).
193
ICDS 2011 : The Fifth International Conference on Digital Society
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-116-8

Furthermore, we have
X1 = Q1QT
2 X2 − Q1QT
2 ( ¯X2 ˙1T ) + ¯X1 ˙1T .
Here, the unitary operator is U = Q1QT
2 and the shifting is
b = −Q1QT
2 ¯X2 + ¯X1. Since the key processing of ﬁnding
this afﬁne mapping is QR decomposition, the computational
cost is O(k3), where k is the number of columns of X1 and
X2. Therefore, the cost O(k3) complexity is limited by the
number of samples in each overlapping region. The proof
of the computational cost of SCMDS is given as follows:
Assume that there are N points in a data set, we divide
these N samples into K overlapping subgroups, where NG
is the number of points in each subgroup and NI the number
of points in each intersection region. Then we have the
relationship
KNG − (K − 1)NI = N
or
K = (N − NI)
(NG − NI).
For each subgroup, we apply classical MDS to compute the
conﬁguration of each group data, which cost O(N 3
G). In each
overlapping region, we apply QR factorization to compute
the afﬁne transformation, which cost O(N 3
I ). Assume that
the true data dimension is p, and the lower bound of NI
is p + 1. For the convenience, we take NG = αp for some
constant α > 2. Then the total computational cost is about
N − p
(α − 1)pO(α3p3) + N − αp
(α − 1)pO(p3) ≈ O(p2N).
The ﬁrst term of above equation is the complexity of MDS
in K groups, and the second term is the complexity of QR
in the K − 1 overlapping regions.
When p << N, the computational cost O(p2N) is much
smaller than O(
√
NN), which is the computation time of
the fast MDS method proposed by Morrison et al., 2003
[3]. The key idea of our fast MDS method is to split data
into subgroups, then combine the conﬁgurations to recover
the whole one. Since all the order three complexities are
restricted in the small number of data entries, we can therefor
speed up MDS. The concept of split-and-combine is also
similar to the concept of parallel computing. Thus, SCMDS
method can be easily implemented via a parallel algorithm.
B. From SCMDS to SCPCA
Because MDS is similar to principle component analysis
(PCA) when the data conﬁguration is Euclidean, we can
adapt SCMDS method to obtain the fast PCA in the same
constrain p << N.
Assume that X is a p-by-N matrix, where there are N
samples with p dimension. D = XT X indicates product
matrix of X, and ˙1 is an N-by-1 vector whos elements are
all 1’s. We deﬁne a symmetric matrix B by
B
=

X − 1
N X ˙1˙1T
T 
X − 1
N X ˙1˙1T

=
D − 1
N D˙1˙1T − 1
N
˙1˙1T D + 1
N 2 ˙1˙1T D˙1˙1T
=
D − ¯
Dr − ¯
Dc + ¯
Dg,
where
¯
Dr
=
1
N D˙1˙1T is the row mean matrix of D,
¯
Dc =
1
N ˙1˙1T D is the column mean matrix of D and
¯
Dg =
1
N2 ˙1˙1T D˙1˙1T is the ground mean matrix of D. The
operator from D to D − ¯
Dr − ¯
Dc + ¯
Dg is called double
centering. If we deﬁne a matrix H by
H = I − 1
N
˙1˙1T ,
B can be simpliﬁed to B = HDH. Since matrix B is
symmetric, the SVD decomposes B into B = ZΣZT . Then
we have
√
B = ZΣ
1
2 P T = (X − 1
N X ˙1˙1T )T ,
for some unitary matrix P. In practice, we set P = I to
obtain the MDS result
√
B. Therefore, the row vector of
√
B is the coordinates of X with the mean of data been
shifted to the original point and rotated by some unitary
matrix P.
If D is a distance matrix with each element di,j =
p
(xi − xj)T (xi − xj), the double center of D2 is equiv-
alent to −2B, provided that PN
i=1 xi = 0. Hence, the MDS
method performs double centering on D2, multiplies by − 1
2,
and then performs SVD, which gives the conﬁgurations of
the data set.
The constrain PN
i=1 xi = 0 in MDS is the same as the
constrain in computing PCA. The score matrix P of PCA
is an unitary matrix, and it is derived by
(X − 1
N X ˙1˙1T )(X − 1
N X ˙1˙1T )T = PΣP T .
If we have the unitary matrix P, we can use (X −
1
N X ˙1˙1T )T P to obtain
√
B. The result of MDS (
√
B) simply
uses the ﬁrst r orthogonal columns of this unitary matrix to
represent the data. We deﬁne an orthogonal matrix Zr by the
ﬁrst r columns of Z and the sub-diagonal matrix Σr by the
up-left block of Σ, then √Br = ZrΣ
1
2r is the r-dimensional
conﬁguration of data set. If we have r-dimensional MDS
conﬁguration, we can obtain the ﬁrst r columns of the score
matrix of PCA, denoted by Pr. That is
Pr = (X − 1
N X ˙1˙1T )Z(Σ
1
2r )−1.
Thus, the key scheme of PCA is embedded in the MDS
method. When the number of samples is large and the data
set is high dimensional, the complexity is costly. However,
if there are many samples which are linearly dependent,
the actual rank of the data matrix is much smaller than
194
ICDS 2011 : The Fifth International Conference on Digital Society
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-116-8

the matrix size. In this case, SCMDS has advantage in
computing speed. And the approach of obtaining Pr by
SCMDS is called SCPCA.
C. From SCPCA to SCSVD
The concept of SVD and PCA are very similar. Since
the PCA starts from decomposing the covariance matrix of
data set, it can be considered as adjusting the center of
mass of a row vector to zero. On the other hand, SVD
operates directly on the product matrix without shifting.
If the mean of the matrix rows is zero, the eigenvectors
derived by SVD are equal to the eigenvectors derived by
the PCA. We are looking for a method which will give a
fast method to produce the SVD result without recomputing
the eigenvectors of the whole data set, when the PCA result
is given. The following is the mathematical analysis for this
process.
Let X be a column matrix of data set. ˜X = X − ¯X · ˙1T ,
where ¯X is the mean of columns of X. Hence, the row mean
of ˜X is zero. Assume that we have the PCA result of X,
that is, ˜X ˜XT = PΣ2P T . Then we have ˜X = PΣU T for
some orthogonal matrix U. Assume that the rank of ˜X is r
and r is much smaller than the matrix size, we observe that
rank(X) = r or rank(X) = r + 1, depending on whether
¯X is spanned by P. If ¯X is spanned by P, then
X = ˜X + ¯X · ˙1T = PΣU T + P · c · ˙1T = P(ΣU T + c · ˙1T ),
where c is the coefﬁcient vector of ¯X when represented by
P, i.e., ¯X = P · c.
If the singular value decomposition of ΣU T + c · ˙1T is
W ˆΣV T , we have
X = P(W ˆΣV T ) = (PW)ˆΣV T = Z ˆΣV T .
Because the matrix W is unitary, Z = PW is automatically
an orthogonal matrix as well. Then we have the SVD of X.
Checking the matrix size of ΣU T +c· ˙1T , we can see that
to compute the SVD of ΣU T + c · ˙1T is not a big task. This
is because ΣU T + c · ˙1T is a r-by-n matrix, and under our
assumption, r is much smaller than n, so we can apply the
economic SVD to obtain the decomposition of ΣU T +c· ˙1T .
On the other hand, if ¯X is not spanned by P, the analysis
becomes
X = ˜X+ ¯X·˙1T = [P|pr+1]
 Σ
0
0
0
 
U T
0

+ c · ˙1T

,
where pr+1 is a unit vector deﬁned by
pr+1 =
(I − PP T ) ¯X
∥(I − PP T ) ¯X∥.
Using the same concept of diagonalization in the case when
¯X is spanned by P, we ﬁnd the SVD of
 Σ
0
0
0
 
U T
0

+ c · ˙1T

= W ˆΣV T .
Then X
=
[P|pr+1]W ˆΣV T
=
Z ˆΣV T , where Z
=
[P|pr+1]W is another orthogonal matrix and hence the SVD
of X is completed.
From the above analysis, we can have a fast PCA method
by computing SCMDS ﬁrst, then adapt the MDS result to
obtain PCA. We named this approach SCPCA. Similarly, the
fast SVD method which computes SCMDS ﬁrst, then adapts
MDS result to obtain PCA, and ﬁnally adapts PCA result to
SVD, is called the SCSVD. These two new methods work
when the rank of X is much smaller than the number of
samples and the number of variables. To obtain the exact
solution, the parameter NI must be greater than the rank
of X. In SCPCA or SCMDS method, if Ni ≤ r, we only
get the approximated solution of PCA and SVD. Under the
necessary criterion, we can reduce the computational com-
plexity from min{O(p2n), O(pn2)} to min{O(rp), O(rn)}.
If the signiﬁcant rank is not small, for example,r ≈ √pn,
the computational complexity becomes almost the same as
the original PCA and SVD. Our method has no advantage
in the latter case.
III. SVD FOR CONTINUOUSLY GROWING DATA
In this section, we look for the solution when the data is
updated constantly and we need to compute SVD continu-
ously. Instead of scanning all the data again, we try to use
the previous SVD result together with the new updated data
to compute the next SVD.
Let A be an m-by-n matrix, where m is the number of
variables and n is the number of samples. And we assume
that both m and n are huge. When new data comes in,
we collect these new data to form a column matrix which
is denoted by B. Assume that we have the singular value
decomposition of A, that is
A = ZΣV T ,
where Z ∈ Mm(ℜ), V ∈ Mn(ℜ) are orthogonal and Σ
is a diagonal. Since the data gets updated, the data matrix
becomes
A1 = [A|B] .
To compute the singular value decomposition of A1, we need
to compute the eigenvalue and eigenvector of A1AT
1 .
We can represent the column matrix B by B = ZC,
where C is the coefﬁcient matrix of B with columns of Z
as the basis. Since Z is orthogonal, the coefﬁcient matrix C
can be computed easily by C = ZT B. Then we have
A1AT
1
=
[A|ZC] [A|ZC]T
=
AAT + ZC(ZC)T
=
Z(Σ2 + CCT )ZT
=
ZU ˆΣ2U T ZT
=
Z1 ˆΣ2ZT
1 .
(1)
Note that the matrix Σ2 + CCT is positive symmetric.
Using the spectrum theorem, we can decompose this matrix
195
ICDS 2011 : The Fifth International Conference on Digital Society
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-116-8

into U ˆΣ2U T . Because the matrix U is unitary, Z1 is Z
rotated by U.
When the matrix size of A is huge, the computational
cost of SVD is high. If the data is constantly growing, it
is difﬁcult to compute the singular value decomposition of
A1 in real time. Therefore, we look for an approximated
solution with fast method.
Let Z = [z1, z2, · · · , zm]. If the new updated data B has
only the components in {z1, z2, · · · , zr}, where r << m,
then only r-dimensional space will be perturbed by this new
data. This is proved as follows.
Theorem Let A = ZΣV T . Assume that A1 = [A|B],
where B has no component in i-th column of Z for i > r.
Then the singular value decomposition of A1 has the same
spectrum σi and singular vector zi, vi for i > r.
Proof: Let
A =
r
p − r
(
Z1
Z2
)
r
p − r
 Σ1
0
0
Σ2


V T
1
V T
2

,
Where Z1 and V1 are the ﬁrst r columns of Z and V .
Because B has no component in Z2, B = Z1C for some
C. Then A1AT
1 can be written as
A1AT
1
=
[A|B][A|B]T
=
AAT + UU T
=

We also use ﬁxed parameter NI = 51 and Ng = 2NI
in each simulation test. We can see that the computational
cost of SVD follows the order 3 increase, compared with
linear increase of SCSVD. The error between economical
SVD and economical SVD, and that between SVD and
SCSVD are shown in Figure 2. Because the results between
economical SVD and SVD are very similar, we use solid line
to represent the value of economical SVD and circle plot to
represent SCSVD. The values in both Figure 1 and Figure
2 are the mean of the results from 100 repeated simulated
matrices. The errors between SVD and economic SVD, and
that between SVD and SCSVD are all under the 10−4 level.
Thus, when the estimated rank of SCSVD is greater than
the true rank, the accuracy of SCSVD is pretty much the
same as SVD in the case of small rank matrix.
The purpose of the second simulation experiment is to
observe the approximation performance of applying SCPCA
to big full rank matrix. We generate random matrix with
ﬁxed number of columns and rows, say 1000. The square
matrix is created by the form, Ap×r ·Br×n +αEp×n, where
r is the essential rank, E is the perturbation and α is a small
coefﬁcient for adjusting the inﬂuence to the previous matrix.
Such matrix can be considered as a big sized matrix with
small rank added by a full rank perturbation matrix. We will
show that our method works well for this type of matrices.
Figure 3 shows the error vs. estimated rank, where the
error is computed by the difference between the original
matrix and the composition of three matrices from SCSVD.
All the elements of matrices A, B and E are randomly
generated from the normal distribution N(0, 1), where α =
0.01 and the essential rank r = 50. We can see that
when the estimated rank increases, the composition error
decreases. Especially when the estimated rank is greater than
the essential rank r, the composition error decays rapidly.
Thus, it is important to make sure that the estimated rank
is greater than the essential rank. In other words, when the
estimated rank of SCSVD is smaller than the essential rank,
our SCSVD result can be used as the approximated solution
of SVD.
In the last experimental result, we will show that we can
set the estimated rank r = 3, starting from the SCSVD result
and using the previous updating method to continuously
update the new SVD. We will show that the performance
of the ﬁrst three components decays very slowly. Thus,
many SVD-based modern techniques, for example, Fisher
linear discrimination, Latent semantic analysis [5], eigen-
taste recommendation system [6], dimensional reduction,
etc, become feasible even when dealing with huge data set.
We produce a series square random matrices A with size
n-by-n for n between 1000 and 3000. Then we decompose
A by SVD to obtain A = ZΣV T . We reset the diagonal
terms of Σ to be exponential decay, so that the data can
simulate the meaningful data in the real world. The maximal
spectrum is set to be 104. Then we compose A by the new
diagonal matrix V . We use SCSVD with estimated rank 3,
and the parameter NI = n
10, Ng = 2NI.
We make 16 updates to the data, and each time we add
10% samples of original data. The new data is simulated
from the normal distribution N(0, 1). We use our updating
method to compute the ﬁrst three new columns of Z and
compare it with the true SVD result. Let a(t), b(t) be the
maximal and minimal element of the absolute values of
ˆZ(t)T
3
Z(t)
3 , respectively, where ˆZ(t)
3
is the t-th updated Z
by our updating method taking only the ﬁrst three columns,
and Z(t) is the t-th updated Z by normal SVD. If a(t) and
b(t) are close to 1, the updated Z derived by our updating
method is very close to the true Z. In Figure 4, we can see
that both a(t) and bt) are close to 1, and they decay very
slowly as the matrix size increases. In Figure 4, every point
is the average value of 32 repeating simulations.
V. CONCLUSION AND FUTURE WORK
We proposed fast PCA and SVD methods derived from
the technique of SCMDS method. The new PCA and SVD
have the same accuracy as the traditional PCA and SVD
ones when the rank of a matrix is much smaller than its
matrix size. The results of applying SCPCA and SCSVD to
a full rank matrix are also quite reliable when the essential
rank of the matrix is much smaller than its matrix size.
In most information technology applications, the essential
rank of a matrix is usually much smaller than its matrix
size. In such cases, utilizing SCPCA or SCSVD in huge
data applications will render good approximated results.
Since the concept of split-and-combine is very similar to
that of parallel computing, this SC-series methods (Split-
and-combine series) can be easily implemented via parallel
computing. Using our updating method for the growing data,
we show that the approximated solution is very close to the
actual solution, even when the estimated rank is as small as
r = 3.
For the future work, we will focus on the cases when
the data contains missing values. Our intuitive speculation
is that the processing of splitting data should be somehow
related to the locations where the missing values occur. We
believe that it would be an interesting topic worth further
exploration.
197
ICDS 2011 : The Fifth International Conference on Digital Society
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-116-8

Figure 1.
Comparison of the elapsed time between economical SVD (the
solid line) and SCSVD (the dashed line).
Figure 2.
Comparison of the composition errors between economical SVD
(the solid line) and SCSVD (the circle plot).
REFERENCES
[1] D. J. Hand, ”Discrimination and classiﬁcation”, Wiley Series
in Probability and Mathematical Statistics, Chichester: Wiley,
1981
[2] M. Cox, T. Cox ”Multidimensional scaling”, Handbook of data
visualization, Springer, 2008
[3] A. Morrison, G. Ross and M. Chalmers, ”Fast multidi-
mensional scaling through sampling, springs and interpola-
tion”,Information Visualization, Vol. 2 , Issue 1, pp. 68 - 77,
2003
[4] J. Tzeng, H. Lu and W. Li, ”Multidimensional scaling for large
genomic data sets”, BMC Bioinformatics, 9:179, 2008
Figure 3. The effect of estimated rank to the composition error. The matrix
size is 1000-by-1000 and its essential rank is 50 (α = 0.01). When the
estimated rank is greater than 50, there is almost no composition error
Figure 4.
The orthogonality between approximated SVD and true SVD.
The solid line is a(t) and the dashed line is b(t)
[5] G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Landauer,
R.A. Harshman, L. A. Streeter and K. E. Lochbaum, ”Infor-
mation retrieval using a singular value decomposition model
of latent semantic structure”, Annual ACM conference on
Research and Development in Information Retrieval, pp. 465-
480, 1988
[6] K. Goldberg, T. Roeder, D. Gupta and C. Perkins, ”Eigentaste:
A constant time collaborative ﬁltering method”, Information
Retrieval, Springer, 2001
198
ICDS 2011 : The Fifth International Conference on Digital Society
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-116-8

