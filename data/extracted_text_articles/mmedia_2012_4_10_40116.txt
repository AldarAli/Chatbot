Video Retrieval by Managing Uncertainty in Concept Detection using
Dempster–Shafer Theory
Kimiaki Shirahama
Graduate School of Economics, Kobe University
2-1, Rokkodai, Nada, Kobe 657-8501, Japan
shirahama@econ.kobe-u.ac.jp
Kenji Kumabuchi and Kuniaki Uehara
Graduate School of System Informatics, Kobe University
1-1, Rokkodai, Nada, Kobe 657-8501, Japan
kumabuchi@ai.cs.kobe-u.ac.jp, uehara@kobe-u.ac.jp
Abstract—This paper focuses on concept-based video retrieval
which examines whether a shot is relevant or irrelevant to
a query based on detection results of concepts, like Person,
Building and Car. One key problem is uncertainty in concept
detection. Even for state-of-the-art methods, it is difﬁcult to
accurately detect concepts present in a shot. Relying on such
uncertain concept detection results degrades retrieval perfor-
mance. To overcome this problem, Dempster–Shafer Theory
(DST) is used to represent the probability that a concept is
possibly present in a shot. By incorporating DST into maximum
likelihood estimation, our method estimates the probabilistic
distribution of concepts’ presences which characterize shots
relevant to the query. A preliminary experiment on TRECVID
2009 video data supports the effectiveness of our method.
Keywords-Video retrieval; Concept Detection; Uncertainty;
Dempster-Shafer Theory; Evidential EM Algorithm
I. INTRODUCTION
Video retrieval can be treated as a machine learning
process, one that constructs a classiﬁer for discriminating
between shots that are relevant or irrelevant to a query. A
large number of example shots are required to construct
a classiﬁer that can accurately retrieve relevant shots, ir-
respective of object appearance, environment and camera
technique. However, it is impractical to prepare enough
example shots to suit all possible queries. This insufﬁciency
of example shots is a key factor in the challenging problem
of the semantic gap between low-level features computed
automatically and high-level semantics perceived by human.
To bridge the semantic gap, one promising approach is
concept-based video retrieval which retrieves shots, where
concepts (e.g., Person, Building and Car) related to a query
are detected. This approach utilizes concept detectors that
detect the presence of a concept in a shot. These are
constructed using a large number of training shots that are
annotated to indicate the presence or absence of a concept.
Hence, the concept can be detected robustly, irrespective of
its size, position and direction on the screen. A large number
of researchers reported that using such concept detection
results as ‘intermediate’ features signiﬁcantly improves re-
trieval performance [1], [2], [3].
Figure 1 outlines concept-based video retrieval. First of
all, a shot is associated with concept detection scores, each
of which represents the probability of a concept’s presence
(Figure 1 (d)). Given a query represented using text and
example shots (i.e., ‘multimodal query’ in Figure 1 (a)),
concepts related to the query are selected (Figure 1 (b)). A
classiﬁer is then constructed to discriminate between relevant
and irrelevant shots to the query, using detection scores of
selected concepts (Figure 1 (c)).
Text: Tall buildings are shown
Example shots:
Building, Outdoor,
Tower, etc.
Video
archive
c) Classifier
construction
Retrieved
shots
d) Shot
representation
{ 0.8,     0.7,     0.1,  .... } 
Cityspace Person
Building
b) Concept
selection
a) Multimodal query
Figure 1.
An overview of concept-based video retrieval.
We will now summarize the tasks necessary for concept-
based video retrieval. The ﬁrst is how to deﬁne a vocabulary
of concepts. The most popular vocabulary is the Large-Scale
Concept Ontology for Multimedia (LSCOM) [4]. LSCOM
deﬁnes a standardized set of 1, 000 concepts in the broadcast
news video domain. These are selected based on their
‘utility’ for classifying content in videos, their ‘coverage’
for responding to a variety of queries, their ‘feasibility’ for
automatic detection, and the ‘availability’ (or ‘observability’)
for large-sized training data.
The second task is how to select concepts related to a
query. Several concept selection methods have been pro-
posed so far. For example, concepts can be selected based on
their lexical similarity to query terms and on detection scores
in example shots [1], [7]. We have also developed a concept
selection method using various concept relationships (e.g.,
generalization/specialization, sibling, part-of etc.) deﬁned in
a knowledge base [8].
The last task that this paper addresses is how to construct
a classiﬁer that discriminates between relevant and irrelevant
shots to a query. In this task, detection scores for multiple
concepts are fused into a single relevance score, which rep-
resents the relevance of a shot to the query. However, even
for most effective methods, it is difﬁcult to accurately detect
71
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-195-3
MMEDIA 2012 : The Fourth International Conferences on Advances in Multimedia

any kind of concept. For example, TRECVID is an annual
competition where concept detectors developed all over the
world are benchmarked using large-scale video data [5].
At TRECVID 2010, the top-ranked methods achieved high
performances for concepts such as Mountain and Vehicle
(with average precisions greater than 0.2). On the other hand,
the detection of concepts like Bus and Sitting down was dif-
ﬁcult (with average precisions less than 0.05). Thus, relying
on such ‘uncertain’ concept detection results signiﬁcantly
degrades retrieval performance.
We introduce a method which constructs a classiﬁer based
on uncertain concept detection scores using Dempster–
Shafer Theory (DST) [9]. DST is a generalization of
Bayesian theory, where a probability is not assigned to
a variable, but instead to a subset of variables. Such a
probability is called a belief mass. We consider two variables
P and A which represent the presence and absence of a
concept in a shot, respectively. In addition, we consider
{P, A} which represents the uncertainty of whether the
concept is present or not. Based on these variables, we deﬁne
three belief masses m({P}), m({A}) and m({P, A}). Here,
m({P}) and m({A}) denote the probability that the concept
is deﬁnitely present in a shot, and the probability that it is
deﬁnitely absent, respectively, while m({P, A}) denotes the
probability that the concept is possibly present in the shot.
By incorporating belief masses into maximum likelihood
estimation, we can construct a classiﬁer that can account
for the uncertainty in concept detection.
II. RELATED WORK
We will review existing methods for constructing classi-
ﬁers based on concept detection scores. These classiﬁers can
be roughly grouped into four categories: linear combination,
discriminative, similarity-based and probabilistic. Linear
combination classiﬁers compute the relevance score of a shot
by weighting detection scores for multiple concepts. Popular
weighting methods use the lexical similarity between query
terms and a concept, their correlation (co-occurrence), and
the detection scores of the concept in example shots [1], [7].
Discriminative classiﬁers consider a shot as a multi-
dimensional vector, where each dimension represents the
detection score of a concept. Based on this, a discriminative
classiﬁer, typically an SVM, is constructed using example
shots [1], [3]. The relevance score of a shot is obtained as
the classiﬁer’s output.
Similarity-based classiﬁers compute the relevance score of
a shot as its similarity to example shots in terms of concept
detection scores. Li et al. used the cosine similarity and a
modiﬁed entropy as similarity measures [10].
Finally, probabilistic classiﬁers estimate a probabilistic
distribution of concepts using concept detection scores in
example shots, and use it to compute the relevance score
of a shot. Rasiwasia et al. computed the relevance score
as the similarity between the multinomial distribution of
concepts estimated from example shots and the multinomial
distribution estimated from the shot [11].
Our method constructs a classiﬁer that is an extension
of probabilistic classiﬁers. Speciﬁcally, ordinal probability
(or Bayesian) theory cannot represent the uncertainty of a
concept’s presence in a shot. The only way to represent
the uncertainty is to assign 0.5 to probabilities of the
concept’s presence and absence. Compared to this, DST can
represent the uncertainty using m({P, A}). Therefore, the
representation of concept detection scores in our method is
much more powerful than that of existing methods. To the
best of our knowledge, such a representation has not been
used in any previous methods.
III. CONCEPT-BASED VIDEO RETRIEVAL BASED ON
EVIDENTIAL EM ALGORITHM
In this section, we present a classiﬁer construction method
that accounts for the uncertainty in concept detection. First,
we describe a method that computes the plausibility of a
concept’s presence (or absence) by combining belief masses
for the concept. The plausibility represents the upper bound
of probability that the concept is present (or absent) in a
shot [9]. Thus, the plausibility of the concept’s presence is
useful for recovering false negative detections, while the
plausibility of its absence is useful for alleviating false
positive detections. We then present a probabilistic model
based on plausibilities and Evidential EM (E2M) algorithm,
which estimates parameters of the model based on maximum
likelihood estimation [9].
Plausibility computation based on DST: Let sj
i be the
detection score of the i-th shot (1 ≤ i ≤ N) for the
j-th concept (1 ≤ j ≤ M). Based on sj
i, we consider
three belief masses mj
i({P}), mj
i({P, A}) and mj
i({A}),
where the superscript j and the subscript i represent the
j-th concept and the i-th shot, respectively. DST offers
various combinations of belief masses based on set-theoretic
operations. The following combination is used to compute
the plausibilities plj1
i
of the j-th concept’s presence, and
plj0
i
of its absence:
plj1
i
=
∑
B∩{P }̸=φ
mj
i(B) = mj
i({P}) + mj
i({P, A}),
plj0
i
=
∑
B∩{A}̸=φ
mj
i(B) = mj
i({A}) + mj
i({P, A}),(1)
where the presence and absence of the j-th concept are
represented by ‘1’ and ‘0’, respectively. B indicates any
subset of variables overlapping {P} or {A}. For plj1
i , {P}
and {P, A} are referred by B as shown in the right-hand
side. Thus, by deﬁning plj1
i
as the sum of mj
i({P}) and
mj
i({P, A}), almost all shots where the j-th concept is
present have relatively large plj1
i . The same is true of plj0
i .
To compute plj1
i
and plj0
i , we extract three types of
intervals using sj
i. The ﬁrst type characterizes mj
i({P})
72
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-195-3
MMEDIA 2012 : The Fourth International Conferences on Advances in Multimedia

where the number of shots annotated with the j-th concept’s
presence is much larger than the number of shots annotated
with its absence. In the second type of interval characterizing
mj
i({A}), the number of the latter type of shots is much
larger than the number of the former type of shots. The last
type of interval for mj
i({P, A}) does not have a distribution
that is biased towards shots annotated with the j-th concept’s
presence or absence. However, directly estimating mj
i({P}),
mj
i({A}) and mj
i({P, A}) is difﬁcult, since we have no pri-
ori knowledge about their probabilistic distributions. Thus,
following the spirit of DST in equation (1), we compute plj0
i
and plj1
i
based on the lower and upper bounds of sj
i, which
are deﬁned as the minimum and maximum of the interval for
mj
i({P, A}), respectively. Thereby, almost all shots where
the j-th concept is presence have sj
i larger than the lower
bound, while almost all shots where it is absent have sj
i
smaller than the upper bound.
To implement the above idea, we construct a linear
SVM using shots annotated with the j-th concept’s presence
and absence. In Figure 2, the former and latter cases are
represented as ×s and +s, respectively, where each shot
is represented using sj
i (i.e., a real number). As shown in
Figure 2, we use the left and right support vectors as the
lower and upper bounds. It can be considered that if sj
i
is larger than the lower bound, the probability of the j-th
concept’s presence in the i-th shot is at least greater than 0;
that is, plj1
i
> 0. It is also reasonable to assume that a larger
plj1
i
is computed for a larger sj
i. Hence, plj1
i
is computed
using Line 1 in Figure 2, where plj1
i
is 0 at the lower bound
and 1 at sj
i = 1. Similarly, Line 0 is used to compute plj0
i ,
where plj0
i
is 0 at the upper bound and 1/ρ at sj
i = 0.
 0
 0.4
 0.6
 0.8
 1
Decision
boundary
1
Line 1
Line 0
pl i
j1
or
pl i
j0
s i
j
 0.2
Shot where the concept is present
Shot where the concept is absent
Support vector
     Upper bound
Support vector
     Lower bound
1/ρ
Figure 2. Illustration of the plausibility computation using support vectors.
The following two points are important for the compu-
tation of plj1
i
and plj0
i . The ﬁrst is that, in order for the
interval between the lower and upper bounds to include shots
annotated with the j-th concept’s presence as well as shots
annotated with its absence, we tune the SVM parameters,
C+ and C−, which penalize mis-classiﬁcation of the former
and latter types of shots, respectively [12]. The second point
is that since the number of shots where the j-th concept is
present is much smaller than the number of shots where it
is absent, putting the same priority on plj1
i
and plj0
i
leads
to a classiﬁer that favors the latter type of shot. Thus, plj0
i
is decreased using ρ.
E2M algorithm: Assume xi = (x1
i , · · · , xM
i ) as the vector
representation of the i-th shot where the j-th dimension
represents the ‘complete’ presence (or absence) of the j-
th concept with no uncertainty, i.e., xj
i ∈ {P, A}. Clearly,
obtaining xi is impossible because we only have uncertain
concept detection scores. The best we can do is to estimate
the probability and plausibility of xj
i = P or xj
i = A. The
plausibility is modeled as either plj1
i
or plj0
i , based on DST.
We use the likelihood L(θ; pli) for a given pli, which is the
set of plausibilities computed for xi [9]:
L(θ; pli) =
∑
xi∈Ω
p(xi; θ)pli(xi),
(2)
where Ω is the domain in which xi is deﬁned as an M-
dimensional vector, and p(xi; θ) is the probability that xi is
observed (or generalized) based on the probabilistic distribu-
tion with the parameter θ. Equation (2) shows that p(xi; θ)
represents the imprecision resulting from the population
of concept detection scores, while pli(xi) represents the
uncertainty related to the error in concept detectors. By
assuming that each shot is independently and identically
distributed, equation (2) can be extended to N shots:
L(θ; pl) =
N
∏
i=1
Li(θ; pli) =
N
∏
i=1
∑
xi∈Ω
p(xi; θ)pli(xi),
(3)
E2M algorithm proposed in [9] computes θ that max-
imizes L(θ; pl) given plausibilities for N example shots
(pl1, · · · , plN) based on the Expectation Maximization (EM)
algorithm. For reasons of space, we will only describe how
E2M algorithm is applied to concept-based video retrieval;
please refer to [9] for a complete description. We denote
xj1
i
= 1 if the j-th concept is present in xi and otherwise
xj1
i
= 0. Similarly, xj0
i
= 1 if it is absent in xi, and
otherwise xj0
i
= 0. Assuming that xj
i is independent from
each other, p(xi; θ) is written as follows:
p(xi; θ) =
M
∏
j=1
1
∏
h=0
(αjh)xjh
i ,
(4)
where θ = {αjh} is the set of parameters representing the
probability of the j-th concept’s presence (αj1) or absence
(αj0). By applying Equation (4) to Equation (3), we get:
L(θ; pl) =
N
∏
i=1
M
∏
j=1
1
∑
h=0
pljh
i αjh,
(5)
where the derivation of ∑ pljh
i αjh is based on the fact that
∑ p(xi; θ)pli(xi) in Equation (3) can be considered to be
the expectation of pli [9]. E2M algorithm extracts θ that
maximizes the likelihood in Equation (5) as follows:
E-Step: Using θq which is the estimate of θ at the q-th
iteration, compute Q(θ, θq), which is the expectation of the
log-likelihood of x = {x1, · · · , xN}:
Q(θ, θq) =
N
∑
i=1
M
∑
j=1
1
∑
h=0
γjh(q)
i
log αjh(q),
(6)
73
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-195-3
MMEDIA 2012 : The Fourth International Conferences on Advances in Multimedia

where
γjh(q)
i
= αjh(q)pljh
i /
1
∑
h′=0
αjh′(q)pljh′
i
.
(7)
M-Step: Update θ so as to maximize Q(θ, θq):
αjh(q+1) =
N
∑
i=1
γjh(q)
i
.
(8)
Finally, given plt of a test shot xt, we compute its
relevance score as the following likelihood L(θ; plt):
L(θ; plt) =
M
∏
j=1
1
∑
h=0
pljh
t αjh.
(9)
This likelihood represents the agreement between plausi-
bilities plt of xt and the probabilistic distribution, with θ
estimated by E2M algorithm. The set of 1, 000 test shots
with the largest L(θ; plt) is returned as a retrieval result.
IV. PRELIMINARY EXPERIMENTAL RESULTS
To test our method, we used TRECVID 2009 video
data [5]. This data consists of 219 development and 619
test videos, comprising 36, 106 shots and 97, 150 shots,
respectively. We used concept detection scores provided by
the City University of Hong Kong, where detection scores
for 374 LSCOM concepts are assigned to every shot [6].
Our method was tested for the query “A view of one or more
tall buildings and the top story visible”. Ten example shots
were selected from the development videos. Based on the
text description and the example shots, 20 concepts related
to the query (e.g., Building, Tower, Sky, etc.) were selected
using the method in [8].
We conducted a preliminary experiment to examine the
effectiveness of using plausibilities, instead of directly using
concept detection scores. The following three methods were
compared: (1) DST: A probabilistic classiﬁer is constructed
using plausibilities modeled based on DST, (2) Sum: The
relevance score of a shot is computed as the sum of concept
detection scores (i.e., linear combination with no weights),
(3) Prod: The relevance score is computed as the product
of concept detection scores [2]. Fig. 3 shows a comparison
of retrieval performances between DST, Sum and Prod in
terms of their average precision. DST is superior to the
other two. We are now testing DST for different queries, and
implementing a probabilistic classiﬁer construction method
that directly uses concept detection scores.
Figure 3.
Performance comparison between DST, Sum and Prod.
V. CONCLUSION AND FUTURE WORK
In this paper, we introduced a probabilistic classiﬁer
construction method which can account for the uncertainty in
concept detection by modeling plausibilities of a concept’s
presence and absence based on DST. We plan to explore
the following points to improve the retrieval performance of
our method. First, in addition to example shots representing
shots that are relevant to a query, we plan to use counter-
example shots representing irrelevant shots and incorporate
them into E2M algorithm. Thereby, irrelevant shots that are
retrieved based only on example shots may be excluded
from the retrieval result. Second, even for the same query,
relevant shots show different combinations of concepts due
to varied camera techniques. Thus, we aim to incorporate a
mixture model into E2M algorithm. Third, we will explore a
method which reﬁnes plausibilities of a concept’s presence
and absence by considering other concepts based on the
knowledge base.
REFERENCES
[1] Natsev A., Haubold A., Te˘si´c, Xie L. and Yan R., “Semantic Concept-
based Query Expansion and Re-ranking for Multimedia Retrieval,”
in Proc. of ACM MM 2007, 2007, pp. 991–1000.
[2] Snoek C. et al., “The mediamill TRECVID 2009 semantic video
search engine,” in Proc. of TRECVID 2009, 2009, pp. 226–238.
[3] Ngo C. et al., “VIREO/DVM at TRECVID 2009: High-level feature
extraction, automatic video search and content-based copy detection,”
in Proc. of TRECVID 2009, 2009, pp. 415–432.
[4] Naphade M., Smith J., Te˘si´c J., Chang S., Hsu W., Kennedy L.,
Hauptmann A. and Curtis J., “Large-scale concept ontology for
multimedia,” IEEE Multimedia, vol. 13, no. 3, pp. 86–91, 2006.
[5] Smeaton A., Over P. and Kraaij W., “Evaluation campaigns and
TRECVid,” in Proc. of MIR 2006, 2006, pp. 321–330.
[6] Jiang Y., Ngo C. and Yang J., “Towards optimal bag-of-features for
object categorization and semantic video retrieval,” in Proc. of CIVR
2007, 2007, pp. 494–501.
[7] Wei X., Jiang Y. and Ngo C., “Concept-driven multi-modality fusion
for video search,” IEEE Transactions on Circuits and Systems for
Video Technology, vol. 21, no. 1, pp. 62–73, 2011.
[8] Shirahama K. and Uehara K., “Constructing and utilizing video
ontology for accurate and fast retrieval,” International Journal of
Multimedia Data Engineering and Management, vol. 2, no. 4, pp.
59–75, 2011.
[9] Denœux T., “Maximum likelihood estimation from uncertain data
in the belief function framework,” IEEE Transactions on Knowledge
and Data Engineering, (PrePrint).
[10] Li X., Wang D., Li J. and Zhang B., “Video search in concept
subspace: A text-like paradigm,” in Proc. of CIVR 2007, 2007, pp.
603–610.
[11] Rasiwasia N., Moreno P. and Vasconcelos N., “Bridging the gap:
Query by semantic example,” IEEE Transactions on Multimedia,
vol. 9, no. 5, pp. 923–938, 2007.
[12] Hsu C., Chang C. and Lin C., A Practical Guide to Support Vector
Classiﬁcation, http://www.csie.ntu.edu.tw/∼cjlin/papers/guide/guide.
pdf retrieved February 2012.
74
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-195-3
MMEDIA 2012 : The Fourth International Conferences on Advances in Multimedia

