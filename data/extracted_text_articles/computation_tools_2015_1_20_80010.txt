A Specialized Recursive Language
for Capturing Time-Space Complexity Classes
Emanuele Covino and Giovanni Pani
Dipartimento di Informatica
Università di Bari, Italy
Email: emanuele.covino@uniba.it, giovanni.pani@uniba.it
Abstract—We provide a resource-free characterization of reg-
ister machines that computes their output within polynomial
time O(nk), by deﬁning our version of predicative recursion
and a related recursive programming language. Then, by
means of some restriction on composition of programs, we
deﬁne a programming language that characterize the register
machines with a polynomial bound imposed over time and
space complexity, simultaneously. A simple syntactical analysis
allows us to evaluate the complexity of a program written in
these languages.
Keywords–Specialized
computation
languages,
time-space
classes, implicit computational complexity, predicative recursion.
I.
INTRODUCTION
The deﬁnition of a complexity class is usually made by
imposing an explicit bound on time and/or space resources
used by a Turing Machine (or another equivalent model)
during its computation. On the other hand, different ap-
proaches use logic and formal methods to provide languages
for complexity-bounded computation; they aim at studying
computational complexity without referring to external mea-
suring conditions or a particular machine model, but only by
considering language restrictions or logical/computational
principles implying complexity properties. In particular, this
is achieved by characterizing complexity classes by means
of recursive operators with explicit syntactical restrictions
on the role of variables.
The ﬁrst characterization of this type was given by
Cobham [4], in which the polynomial-time computable
functions are exactly those functions generated by bounded
recursion on notation; Leivan [8] and Bellantoni and Cook
[1] gave other characterizations of PTIMEF. Several other
complexity classes have been characterized by means of
unlimited operators: see, for instance, Leivant and Marion
[9] and Oitavem [10] for PSPACEF and the class of the
elementary functions; Clote [3] for the deﬁnition of a
time/space hierarchy between PTIMEF and PSPACEF; Leivant
[6], [7] and [8] for a theoretical insight. All these approaches
have been dubbed Implicit Computational Complexity: they
share the idea that no explicitly bounded schemes are needed
to characterize a great number of classes of functions and
that, in order to do this, it sufﬁces to distinguish between safe
and unsafe variables (or, following Simmons [11], between
dormant and normal ones) in the recursion schemes. This
distinction yields many forms of predicative recursion, in
which the being-deﬁned function cannot be used as counter
into the deﬁning one. The two main objectives of this
area are to ﬁnd natural implicit characterizations of various
complexity classes of functions, thereby illuminating their
nature and importance, and to design methods suitable for
static veriﬁcation of program complexity. This approach
represent a bridge between the complexity theory and the
programming language theory; a mere syntactical inspection
allows us to evaluate the complexity of a given program writ-
ten in one of the previous mentioned specialized languages.
Our version of the safe recursion scheme on a binary
word algebra is such that f(x, y, za) = h(f(x, y, z), y, za);
throughout this paper we will call x, y and z the auxiliary
variable, the parameter, and the principal variable of a
program deﬁned by recursion, respectively. We don’t allow
the renaming of variable z as x, and this implies that the step
program h cannot assign the previous value of the being-
deﬁned program f to the principal variable z: in other words,
we always know in advance the number of recursive calls
of the step program in a recursive deﬁnition. We obtain that
z is a dormant variable, according to Simmons’ approach,
or a safe one, following Bellantoni and Cook.
In Section II, starting from a natural deﬁnition of con-
structors and destructors over an algebra of lists, we give
our deﬁnition of recursion-free programs and of the safe
recursion scheme. In section III, we recall the deﬁnition
of computation by register machines as provided by [8]. In
section IV, we deﬁne the hierarchy of classes of programs
Tk, with k ∈ N, where programs in T1 can be computed by
register machines within linear time, and Tk+1 are programs
obtained by one application of safe recursion to elements in
Tk; we prove that they are computable within time O(nk).
We then restrict Tk to the hierarchy Sk, whose elements
are the programs computable by a register machine in
linear space. By means of a restricted form of composition
between programs, we deﬁne, in Section V, a polytime-
space hierarchy T Sqp, such that each program in T Sqp
can be computed by a register machine within time O(np)
and space O(nq), simultaneously. We have that S
k<ω Tk
captures PTIME. Even though this is a well-known result
(see [8]), we use it to prove the second one (see also [5]
and [9] for other approaches to the characterization of joint
time-space classes). Both results are a preliminary step for an
implicit classiﬁcation of the hierarchy of time-space classes
between PTIME and PSPACE, as deﬁned in [3]. In Section VI
we summarize the results and give some hints about future
work.
8
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

II.
BASIC INSTRUCTIONS AND DEFINITION SCHEMES
In this section we introduce the basic operators of our
programming language (for a different approach see [2]).
The language is deﬁned over a binary word algebra, with
the only restriction that words are packed into lists, with the
symbol © acting as a separator between words. In this way,
we are able to handle a sequence of words as a single object.
A. Recursion-free programs and classes T0 and S0
B is the binary alphabet {0, 1}. a, b, a1, . . . denotes
elements
of
B,
and
U, V, . . . , Y
denotes
words
over
B.
p, q, . . . , s, . . .
denotes
lists
in
the
form
Y1©Y2© . . . ©Yn−1©Yn. ϵ is the empty word. The i-
th component (s)i of s = Y10Y20 . . . 0Yn−10Yn is Yi. |s|
is the length of the list s, that is the number of letters
occurring in s. We write x, y, z for the variables used in a
program, and we write u for one among x, y, z. Programs
will be denoted with f, g, h, and they will have the form
f(x, y, z), where some among the variables may be absent.
Deﬁnition 2.1: The basic instructions are:
1)
the identity I(u), that returns the value s assigned
to u;
2)
the constructors Ca
i (s), that adds the digit a at the
right of the last digit of (s)i, with a = 0, 1 and
i ≥ 1;
3)
the destructors Di(s), that erases the rightmost digit
of (s)i, with a = 0, 1 and i ≥ 1.
Constructors Ca
i (s) and destructors Di(s) leave s unchanged
if it has less than i components.
Example 2.1: Given the word s = 01©11©©00, we
have that |s| = 9 and (s)2 = 11. We also have C1
1(01©11) =
011©11, D2(0©0©) = 0©©, D2(0©©) = 0©©.
Deﬁnition 2.2: Given the programs g and h, f is deﬁned
by simple schemes if it is obtained by:
1)
renaming of x as y in g, that is, f is the result of
the substitution of the value of y to all occurrences
of x into g. Notation: f =RNMx/y(g);
2)
renaming of z as y in g, that is, f is the result of
the substitution of the value of y to all occurrences
of z into g. Notation: f =RNMz/y(g);
3)
selection in g and h, when for all s, t, r we have
f(s, t, r) =
( g(s, t, r)
if the rightmost digit
of (s)i is b
h(s, t, r)
otherwise,
with i ≥ 1 and b = 0, 1. Notation: f =SELb
i(g, h).
Example 2.2: if f is deﬁned by RNMx/y(g) we have
that f(t, r) = g(t, t, r). Similarly, f deﬁned by RNMz/y(g)
implies that f(s, t) = g(s, t, t). Let s be the word 00©1010,
and f =SEL0
2(g, h); we have that f(s, t, r) = g(s, t, r), since
the rightmost digit of (s)2 is 0.
Deﬁnition 2.3: f is obtained by safe composition of h
and g in the variable u if it is obtained by substitution of h
to u in g; if u = z, then x must be absent in h. Notation:
f =SCMPu(h, g).
Deﬁnition 2.4: A modiﬁer is obtained by the safe com-
position of a sequence of constructors and a sequence of
destructors.
Deﬁnition 2.5: T0 is the class of programs deﬁned by
closure of modiﬁers under SEL and SCMP.
Deﬁnition 2.6: Given f ∈ T0, the rate of growth rog(f)
is such that
1)
if f is a modiﬁer, rog(f) is the difference between
the number of constructors and the number of
destructors occurring in its deﬁnition;
2)
if
f=SELb
i(g, h),
then
rog(f)
is
max(rog(g), rog(h));
3)
if
f=SCMPu(h, g),
then
rog(f)
is
max(rog(g), rog(h)).
Deﬁnition 2.7: S0 is the class of programs in T0 with
non-positive rate of growth, that is S0 = {f ∈ T0|rog(f) ≤
0}.
Note that all elements in T0 and in S0 modify their inputs
according to the result of some test performed over a ﬁxed
number of digits. Moreover, elements in S0 cannot return
values longer than their input.
B. Safe recursion and classes T1 and S1
Deﬁnition 2.8: Given
the
programs
g(x, y)
and
h(x, y, z), f(x, y, z) is deﬁned by safe recursion in the
basis g and in the step h if for all s, t, r we have

f(s, t, a)
=
g(s, t)
f(s, t, ra)
=
h(f(s, t, r), t, ra).
Notation: f =SREC(g, h).
In particular, f(x, z) is deﬁned by iteration of h(x) if for
all s, r we have

f(s, a)
=
s
f(s, ra)
=
h(f(s, r)).
Notation: f =ITER(h). We write h|r|(s) for ITER(h)(s, r)
(i.e., the |r|-th iteration of h on s).
Deﬁnition 2.9: T1 (respectively, S1) is the class deﬁned
by closure under simple schemes and SCMP of programs in
T0 (resp., S0) and programs obtained by one application of
ITER to T0 (resp., S0).
Notation: T1=(T0, ITER(T0); SCMP, SIMPLE)
(resp., S1=(S0, ITER(S0); SCMP, SIMPLE)).
As we have already stated in the Introduction, we call
x, y and z the auxiliary variable, the parameter, and the
principal variable of a program obtained by means of the
previous recursion scheme. The renaming of z as x is not
allowed (see deﬁnitions 2.2 and 2.3), implying that the step
program of a recursive deﬁnition cannot assign the recursive
9
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

call to the principal variable. This is the key of the time-
complexity bound intrinsic into our programs, together with
the limitations imposed to the renaming of variables.
Deﬁnition 2.10:
1)
Given f ∈ T1, the number of
components of f is max{i|Di or Ca
i or SELb
i occurs
in f}. Notation: #(f).
2)
Given a program f, its length is the number of
constructors, destructors and deﬁning schemes oc-
curring in its deﬁnition. Notation: lh(f).
III.
COMPUTATION BY REGISTER MACHINES
In this section, we recall the deﬁnition of register machine
(see [8]), and we give the deﬁnition of computation within
a given time (or space) bound.
Deﬁnition 3.1: Given a free algebra A generated from
constructors c1, . . . , cn (with arity(ci) = ri), a register
machine over A is a computational device M having the
following components:
1)
a ﬁnite set of states S = {s0, . . . , sn};
2)
a ﬁnite set of registers Φ = {π0, . . . , πm};
3)
a collection of commands, where a command may
be:
a branching siπjsi1 . . . sik, such that when M is in
the state si, switches to state si1, . . . , sik according
to whether the main constructor (i.e., the leftmost)
of the term stored in register πj is c1, . . . , ck;
a constructor siπj1 . . . πjri ciπlsr, such that when
M is in the state si, store in πl the result of the
application of the constructor ci to the values stored
in πj1 . . . πjri , and switches to sr;
a p-destructor siπjπlsr (p ≤ max(ri)i=1...k),
such that when M is in the state si, store in πl
the p-th subterm of the term in πj, if it exists;
otherwise, store the term in πj. Then it switched
to sr.
A conﬁguration of M is a pair (s, F), where s ∈ S and
F
: Φ → A. M induces a transition relation ⊢M on
conﬁgurations, where κ ⊢M κ′ holds if there is a command
of M whose execution converts the conﬁguration κ in κ′. A
computation of M on input ⃗X = X1, . . . , Xp with output
⃗Y = Y1, . . . , Yq is a sequence of conﬁgurations, starting
with (s0, F0), and ending with (s1, F1) such that:
1)
F0(πj′(i)) = Xi, for 1 ≤ i ≤ p and j′ a
permutation of the p registers;
2)
F1(πj′′(i)) = Yi, for 1 ≤ i ≤ q and j′′ a
permutation of the q registers;
3)
each conﬁguration is related to its successor by
⊢M;
4)
the last conﬁguration has no successor by ⊢M.
Deﬁnition 3.2: A register machine M computes the pro-
gram f if, for all s, t, r, we have that f(s, t, r) = q implies
that M computes (q)1, . . . , (q)#(f) on input (s)1, . . . ,
(s)#(f), (t)1, . . . , (t)#(f), (r)1, . . . , (r)#(f).
Deﬁnition 3.3:
1)
For each input ⃗X (with | ⃗X| =
n), M computes its output within time O(p(n)) if
its computation runs through O(p(n)) conﬁgura-
tions; M computes its output in space O(q(n)) if,
during the whole computation, the global length of
the contents of its registers is O(q(n)).
2)
For each input ⃗X (with | ⃗X| = n), M needs time
O(p(n)) and space O(q(n)) if the two bounds oc-
cur simultaneously, during the same computation.
Note that the number of registers needed by M to compute
a given f has to be ﬁxed a priori (otherwise, we should have
to deﬁne a family of register machines for each program to
be computed, with each element of the family associated to
an input of a given length). According to deﬁnition 2.10 and
3.2, M uses a number of registers which linearly depends
on the highest component’s index that f can manipulate or
access with one of its constructors, destructors or selections;
and which depends on the number of times a variable is used
by f, that is, on the total number of different copies of the
registers that M needs during the computation. Both these
numbers are constant values.
Unlike the usual operators cons, head and tail over Lisp-
like lists, our constructors and destructors can have direct
access to any component of a list, according to deﬁnition 2.1.
Hence, their computation by means of a register machine
requires constant time, but it requires an amount of time
which is linear in the length of the input, when performed
by a Turing machine.
Codes. We write si©Fj(π0)© . . . ©Fj(πk) for the word
that encodes a conﬁguration (si, Fj) of M, where each
component is a binary word over {0, 1}.
Lemma 3.1: f belongs to T1 if and only if f is com-
putable by a register machine within time O(n).
Proof: To prove the ﬁrst implication we show (by
induction on the structure of f) that each f ∈ T1 can be
computed by a register machine Mf in time cn, where c is
a constant which depends on the construction of f, and n
is the length of the input.
Base. f ∈ T0. This means that f is obtained by closure of
a number of modiﬁers under selection and simple schemes;
each modiﬁer g can be computed within time bounded by
lh(g), the overall number of basic instructions and deﬁnition
schemes of g, i.e. by a machine running over a constant
number of conﬁgurations; the result follows, since the safe
composition and the selection can be simulated by our model
of computation.
Step. Case 1. f =ITER(g), with g ∈ T0. We have that
f(s, r) = g|r|(s). A register machine Mf can be deﬁned as
follows: (s)i is stored in the register πi (i = 1 . . . #(f)) and
(r)j is stored in the register πj (j = #(f) + 1 . . . 2#(f));
Mf runs Mg (within time lh(g)) for |r| times. Each time
Mg is called, Mf deletes one digit from one of the registers
π#(f)+1 . . . π2#(f), starting from the ﬁrst; the computation
stops, returning the ﬁnal result, when they are all empty.
Thus, Mf computes f(s, r) within time |r|lh(g).
Case 2. Let f be deﬁned by simple schemes or SCMP. The
result follows by direct simulation of the schemes.
10
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

In order to prove the second implication, we show that
the behaviour of a k-register machine M which operates
in time cn can be simulated by a program in T1. Let
nxtM be a program in T0, such that nxtM operates on
input s = si©Fj(π0)© . . . ©Fj(πk) and it has the form
if state[i](s) then Ei, where state[i](s) is a test which
is true if the state of M is si, and Ei is a modiﬁer which
updates the code of the state and the code of one among
the registers, according to the deﬁnition of M. By means of
c − 1 SCMP’s we deﬁne nxtc
M in T0, which applies c times
nxtM to the word that encodes a conﬁguration of M. We
deﬁne in T1

linsimM(x, a) =
x
linsimM(x, za) =
nxtc
M(linsimM(x, z))
linsimM(s, t) iterates nxtM(s) for c|t| times, returning the
code of the conﬁguration which contains the ﬁnal result of
M.
IV.
THE TIME HIERARCHY
In this section, we recall the deﬁnition of classes of
programs T1 and S1; we deﬁne our hierarchy of classes
of programs, and we state the relation with the classes
of register machines, which compute their output within a
polynomially-bounded amount of time.
Deﬁnition 4.1: ITER(T0) denotes the class of programs
obtained by one application of iteration to programs in T0.
T1 is the class of programs obtained by closure under safe
composition and simple schemes of programs in T0 and
programs in ITER(T0).
Tk+1 is the class of programs obtained by closure under
safe composition and simple schemes of programs in Tk
and programs in SREC(Tk).
Notation: T1=(T0, ITER(T0); SCMP, SIMPLE).
Tk+1=(Tk, SREC(Tk); SCMP, SIMPLE).
Deﬁnition 4.2: ITER(S0) denotes the class of programs
obtained by one application of iteration to programs in S0.
S1 is the class of programs obtained by closure under safe
composition and simple schemes of programs in S0 and
programs in ITER(S0).
Sk+1 is the class of programs obtained by closure under sim-
ple schemes of programs in Sk and programs in SREC(Sk).
Notation: S1=(S0, ITER(S0); SCMP, SIMPLE).
Sk+1=(Sk, SREC(Sk); SIMPLE).
Hence, hierarchy Sk, with k ∈ N, is a version of Tk in
which each program returns a result whose length is exactly
bounded by the length of the input; this doesn’t happen if
we allow the closure of Sk under SCMP. We will use this
result to evaluate the space complexity of our programs.
Lemma 4.1: Each f(s, t, r) in Tk (k ≥ 1) can be com-
puted by a register machine within time |s|+lh(f)(|t|+|r|)k.
Proof: Base. f ∈ T1. The relevant case is when f is
in the form ITER(h), with h a modiﬁer in T0. In lemma
3.1 (case 1 of the step) we have proved that f(s, r) can be
computed within time |r|lh(h); hence, we have the thesis.
Step. f
∈
Tp+1. The most signiﬁcant case is when
f =SREC(g, h). The inductive hypothesis gives two register
machines Mg and Mh which compute g and h within
the required time. Let r be the word a1 . . . a|r|; recalling
that f(s, t, ra) = h(f(s, t, r), t, ra), we deﬁne a register
machine Mf such that it calls Mg on input s, t, and calls
Mh for |r| times on input stored into the appropriate set of
registers (in particular, the result of the previous recursive
step has to be stored always in the same register). By
inductive hypothesis, Mg needs time |s|+lh(g)(|t|)p in order
to compute g; for the ﬁrst computation of the step program
h, Mh needs time |g(s, t)| + lh(h)(|t| + |a|r|−1a|r||)p.
After |r| calls of Mh, the ﬁnal conﬁguration is obtained
within overall time |s| + max(lh(g), lh(h))(|t| + |r|)p+1 ≤
|s| + lh(f)(|t| + |r|)p+1.
Lemma 4.2: The behaviour of a register machine which
computes its output within time O(nk) can be simulated by
an f in Tk.
Proof: Let M be a register machine respecting the
hypothesis. As we have already seen, there exists nxtM ∈ T0
such that, for input the code of a conﬁguration of M, it
returns the code of the conﬁguration induced by the relation
⊢M. Given a ﬁxed i, we write the program σi by means of i
safe recursions nested over nxtM, such that it iterates nxtM
on input s for ni times, with n the length of the input:
σ0 :=ITER(nxtM) and
σn+1 :=IDTz/y(γn+1), where γn+1 :=SREC(σn, σn).
We have that
σ0(s, t) = nxt|t|
M(s), σn+1(s, t) = γn+1(s, t, t), and
( γn+1(s, t, a)
= σn(s, t)
γn+1(s, t, ra)
= σn(γn+1(s, t, r), t)
= γn(γn+1(s, t, r), t, t)
In particular we have
σ1(s, t) =
γ1(s, t, t) =
σ0(σ0(. . . σ0(s, t) . . .))
|
{z
}
|t| times
= nxt|t|2
M
σ2(s, t) =
γ2(s, t, t) =
σ1(σ1(. . . σ1(s, t) . . .))
|
{z
}
|t| times
= nxt|t|3
M
By induction we see that σk−1 iterates nxtM on input s
for |t|k times, and that it belongs to Tk. The result follows
deﬁning f(t) = σk−1(t, t), with t the code of an initial
conﬁguration of M.
Theorem 4.1: f belongs to Tk if and only if f is com-
putable by a register machine within time O(nk).
Proof: By lemma 4.1 and lemma 4.2.
We recall that register machines are polytime reducible to
Turing machines; this implies that S
k<ω Tk captures PTIME
(see [8]).
11
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

V.
THE TIME-SPACE HIERARCHY
In this section, we deﬁne a time-space hierarchy of
recursive programs (see [4]), and we state the equivalence
with the classes of register machines which compute their
output with a simultaneous bound on time and space.
Deﬁnition 5.1: Given the programs g and h, f is ob-
tained by weak composition of h in g if f(x, y, z) =
g(h(x, y, z), y, z). Notation: f =WCMP(h, g).
In the weak form of composition the program h can be sub-
stituted only in the variable x, while in the safe composition
the substitution is possible in all variables.
Deﬁnition 5.2: For all p, q ≥ 1, T Sqp is the class of
programs obtained by weak composition of h in g, with
h ∈ Tq, g ∈ Sp and q ≤ p.
Lemma 5.1: For all f in Sp, we have |f(s, t, r)| ≤
max(|s|, |t|, |r|).
Proof: By induction on p. Base. The relevant case is
when f ∈ S1 and f is deﬁned by iteration of g in S0 (that is,
rog(g) ≤ 0). By induction on r, we have that |f(s, a)| = |s|,
and |f(s, ra)| = |g(f(s, r))| ≤ |f(s, r)| ≤ max(|s|, |r|).
Step. Given f ∈ Sp+1, deﬁned by SREC in g and h in Sp,
we have
|f(s, t, a)|
= |g(s, t)|
by deﬁnition of f
≤ | max(|s|, |t|)|
by inductive hypothesis.
and
|f(s, t, ra)|
= |h(f(s, t, r), t, ra)|
≤ | max(|f(s, t, r)|, |t|, |ra|)|
≤ | max(max(|s|, |t|, |r|), |t|, |ra|)|
≤ | max(|s|, |t|, |ra|)|.
by deﬁnition of f, inductive hypothesis on h and induction
on r.
Lemma 5.2: Each f in T Sqp (with p, q ≥ 1) can be
computed by a register machine within time O(np) and
space O(nq).
Proof: Let f be in T Sqp. By deﬁnition 5.2, f is deﬁned
by weak composition of h ∈ Tq into g ∈ Sp, that is,
f(s, t, r) = g(h(s, t, r), t, r). The theorem 4.1 states that
there exists a register machine Mh which computes h within
time nq, and there exists another register machine Mg which
computes g within time np. Since g belongs to Sp, lemma
5.1 holds for g; hence, the space needed by Mg is at most
n.
Deﬁne now a machine Mf that, by input s, t, r, performs
the following steps:
(1) it calls Mh on input s, t, r;
(2) it calls Mg on input h(s, t, r), t, r, stored in the appro-
priate registers.
According to lemma 4.1, Mh needs time equal to |s| +
lh(h)(|t| + |r|)q to compute h, and Mg needs |h(s, t, r)| +
lh(g)(|t| + |r|)p to compute g.
This happens because lemma 4.1 shows, in general, that the
time used by a register machine to compute a program is
bounded by a polynomial in the length of its inputs, but,
more precisely, it shows that the time complexity is linear
in |s|. Moreover, since in our language there is no kind
of identiﬁcation of x as z, Mf never moves the content
of a register associated to h(s, t, r) into another register
and, in particular, into a register whose value plays the
role of recursive counter. Thus, the overall time-bound is
|s|+lh(h)(|t|+|r|)q+lh(g)(|t|+|r|)p which can be reduced
to np, being q ≤ p.
Mh requires space nq to compute the value of h on input
s, t, r; as we noted above, the space needed by Mg for the
computation of g is linear in the length of the input, and
thus the overall space needed by Mf is still nq.
Lemma 5.3: A register machine which computes its out-
put within time O(np) and space O(nq) can be simulated
by an f ∈ T Sqp.
Proof: Let M be a register machine, whose computation
is time-bounded by np and, simultaneously, it is space-
bounded by nq. M can be simulated by the composition
of two machines, Mh (time-bounded by nq), and Mg (time-
bounded by np and, simultaneously, space-bounded by n):
the former delimits (within nq steps) the space that the latter
will successively use in order to simulate M.
By theorem 4.1 there exists h ∈ Tq which simulates the
behaviour of Mh, and there exists g ∈ Tp which simulates
the behaviour of Mg; this is done by means of nxtg, which
belongs to S0, since it never adds a digit to the description
of Mg without erasing another one.
According to the proof of lemma 4.2, we are able to deﬁne
σn−1 ∈ Sn, such that σn−1(s, t) = nxt|t|n
g
. The result
follows deﬁning sim(s) = σp−1(h(s), s) ∈ T Sqp.
Theorem 5.1: f belongs to T Sqp if and only if f is
computable by a register machine within time O(np) and
space O(nq).
Proof: By lemma 5.2 and lemma 5.3.
VI.
CONCLUSIONS
We have provided a resource-free characterization of reg-
ister machines that computes their output within polynomial
time, and we have extended it to register machines that
computes their output with a polynomial bound imposed on
time and space, simultaneously; this is made by a version of
predicative recursion and a related recursive programming
language. A program written in this languages can be
analyzed, and the complexity of the program is evaluated
by means of this simple analysis. This result represents
a preliminary step for a resource-free classiﬁcation of the
hierarchy of time-space classes between PTIME and PSPACE,
as deﬁned in [3].
12
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

REFERENCES
[1]
S. Bellantoni and S. Cook, "A new recursion-theoretic characterization
of the poly-time functions," Computational Complexity, vol. 2, 1992,
pp. 97-110.
[2]
S. Caporaso, G. Pani, E. Covino, "A Predicative Approach to the Clas-
siﬁcation Problem," Journal of Functional Programming, vol. 11(1),
Jan. 2001, pp. 95-116.
[3]
P. Clote, "A time-space hierarchy between polynomial time and
polynomial space," Math. Sys. The., vol. 25, 1992, pp. 77-92.
[4]
A. Cobham, "The intrinsic computational difﬁculty of functions," in
Proceedings of the International Conference on Logic, Methodology,
and Philosophy of Science. North-Holland, Amsterdam, 1962, pp. 24-
30, Y. Bar-Hillel Ed.
[5]
M. Hofmann, "Linear types and non-size-increasing polynomial time
computation," Information and Computation, vol. 183(1), May 2003,
pp. 57-85.
[6]
D. Leivant, "A foundational delineation of computational feasibility,"
in Proceedings of the Sixth Annual IEEE symposium on Logic in
Computer Science (LICS’91). IEEE Computer Society Press, 1991, pp.
2-11.
[7]
D. Leivant, "Stratiﬁed functional programs and computational com-
plexity," in Proceedings of the 20th Annual ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages (POPL’93).
ACM, New York, 1993, pp. 325-333.
[8]
D. Leivant, Ramiﬁed recurrence and computational complexity I: word
recurrence and polytime. Birkauser, 1994, pp. 320-343, in Feasible
Mathematics II, P. Clote and J. Remmel Eds.
[9]
D. Leivant and J.-Y. Marion, "Ramiﬁed recurrence and computational
complexity II: substitution and polyspace," Computer Science Logic,
LNCS vol. 933, 1995, pp. 486-500, J. Tiuryn and L. Pocholsky Eds.
[10]
I. Oitavem, "New recursive characterization of the elementary func-
tions and the functions computable in polynomial space," Revista
Matematica de la Univaersidad Complutense de Madrid, vol 10.1, 1997,
pp. 109-125.
[11]
H. Simmons, "The realm of primitive recursion," Arch.Math. Logic,
vol. 27, 1988, pp. 177-188.
13
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-394-0
COMPUTATION TOOLS 2015 : The Sixth International Conference on Computational Logics, Algebras, Programming, Tools, and Benchmarking

