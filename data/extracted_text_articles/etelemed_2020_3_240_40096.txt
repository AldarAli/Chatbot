Spatial Awareness for the Deafblind in Natural Language Presentation using SPIN 
Rules: A Use Case in the SUITCEYES Platform 
 
Vasileios Kassiano, Thanos G. Stavropoulos,  
Spiros Nikolopoulos and Ioannis Kompatsiaris 
Information Technologies Institute (ITI) 
Centre for Research & Technology Hellas (CERTH) 
Thessaloniki, Greece 
e-mail: {vaskass, athstavr, nikolopo, ikom}@iti.gr 
Marina Riga 
 
Department of Mechanical Engineering, Aristotle 
University of Thessaloniki, Thessaloniki, Greece 
e-mail: mriga@isag.meng.auth.gr
 
 
Abstract— This paper presents a rule-based approach towards 
spatial awareness for the deafblind through natural language 
constructs. The approach entails two components, a novel 
ontology for the interoperable representation of data pertaining 
the domain (objects, space, etc.) and a rule set to derive the 
natural language constructs for spatial awareness and answer 
related user queries. The rule set is expressed in SPARQL 
Inferencing Notation (SPIN), which enables simplicity and 
flexibility in rule definition as opposed to other frameworks. 
Both are applied in a use case scenario of the SUITCEYES 
platform for the deafblind, extending it with the ability to 
answer spatial awareness queries. More specifically, the 
ontology component uses rules to provide the users of the 
platform answers to queries regarding their environment. We 
present those rules and show how they inform the user of their 
surroundings, using natural language. Furthermore, we provide 
a differential population solution to avoid overloading the 
ontology with unnecessary data. 
Keywords- 
ontology; 
rules; 
natural 
language; 
SPIN; 
deafblindness. 
I. 
 INTRODUCTION 
Communication 
with 
and 
between 
users 
with 
deafblindness is constrained by the medical nature of this 
disability, ranging from congenital to acquired deafblindness, 
including worsening sight or worsening hearing or both over 
time, plus, ultimately, symptoms of ageing as well. 
This paper presets an approach towards spatial awareness 
for the deafblind using an ontology and a rule set to provide 
the user with information about their environment expressed 
in natural language and to dynamically update the spatial 
information by only keeping the most recent and relevant 
information provided to the ontology. Also, we apply a use-
case of our approach in the SUITCEYES platform [1], by 
testing it with incoming data from the SUITCEYES system. 
The purpose of our work is to provide a way to represent 
spatial context and enable spatial awareness using semantic 
web technologies and rules to form natural language 
constructs that can support the deafblind. 
The rest of the paper is structured as follows: In Section II 
we present the most related work with our approach and 
ontologies regarding natural language. In Section III, we 
present the ontology that was developed, which extends 
already existing ontologies. In Section IV, we describe our 
method for providing spatial context in natural language and 
in Section V we describe our method for dynamically 
updating the ontology with differential population. Finally, in 
Section VI we offer a proof of concept, a use-case performed 
in the SUITCEYES platform. 
II. 
RELATED WORK 
More and more Internet of Things (IoT) applications are 
used for healthcare purposes due to their high interoperability 
and expressiveness [2][3][4]. These types of applications 
acquire knowledge from multiple sources and from 
continuous and heterogeneous data flows [5][6]. Semantic 
technologies provide comprehensive tools and methods for 
representing knowledge and producing new ones. IoT 
environments are increasingly found in home healthcare 
technologies in actions that create better living conditions for 
the elderly, through the use of IoT technologies, such as 
Active and Healthy Ageing (AHA) and Home Ambient 
Assisted Living (AAL). 
Furthermore, in the case of deafblind people, the simple 
and accurate representation of their surrounding environment 
is one of the most basic needs for their quality of life. This can 
be achieved by providing the nature of their surroundings in 
natural language. Some of the most relevant works for 
language processing with ontologies are [7][8]. 
KnowSense [3] is an activity monitoring system for 
elderly with dementia, deployed in controlled and diffuse 
environments. Semantic Web technologies, such as OWL 2, 
are widely used in KnowSense to display sensor and specific 
application observations, as well as to implement solutions for 
identifying activities and identifying problems in everyday 
life activities with the aim of clinical evaluation in various 
stages of dementia. Description Logic Reasoning (DL 
reasoning) for activity detection and SPARQL questions are 
used to extract clinical problems. 
ACTIVAGE [4] is a large-scale pilot project, with the 
purpose of developing Smart Living solutions that strengthens 
active and healthy aging. The ACTIVAGE IoT Ecosystem 
Suite (AIOTES) project, contains a set of techniques, tools, 
and methodologies (rule-based reasoning, interoperable 
ontologies, etc.) that increases semantic interoperability at 
different levels between heterogeneous IoT platforms. The 
approach uses multiple mechanisms of reasoning that can 
160
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-763-4
eTELEMED 2020 : The Twelfth International Conference on eHealth, Telemedicine, and Social Medicine

improve the understanding of patients' heterogeneous data and 
help generate new knowledge by providing services to end 
users. 
Dem@Care [4] is a system based on heterogenous sensors 
that provides support for independent living for elder people 
with dementia or similar health problems. This approach 
incorporates a heterogeneous set of detection methods and 
technologies, including video, audio, in addition to normal, 
environmental, 
and 
other 
measurements. 
Semantic 
technologies (e.g., rule-based reasoning) are used to process 
and analyze sensor data according to user requirements. This 
leads to feedback and decision support, which is 
communicated to end users through appropriately designed 
user interfaces. The support includes various clinical 
scenarios, both short (trials in hospital settings) and long term 
(daily living at work), for independent living. 
In [9], a system for healthcare in Smart Home 
environments 
is 
developed, 
which 
considers 
social 
relationship-based contexts to provide a fully personalized 
healthcare service. 
An ontology-based sensor selection for real-world 
wearable activity recognition is presented in [10], in which the 
use of ontologies is proposed to thoroughly describe the 
wearable sensors available for the activity recognition 
process. This enables the semantic selection of sensors to 
support a continuity of recognition. 
In [7], an extended version of a linguistic ontology is 
presented that works particularly with space. Language 
regarding space, spatial relationships and actions in space is 
covered and an ontological structure that relates such 
expressions with ontology classes is developed. Finally, 
examples of the ontology’s results based on natural language 
examples are presented. 
In [8], a project in which ontologies are part of the 
reasoning process used for information management and for 
the presentation of information is presented. Both accessing 
and presenting information are mediated via natural language 
and the ontologies are coupled with the lexicon used in the 
natural language component. This work, as well as [7], is 
related with the natural language aspect of our approach. 
An approach to transform natural language sentences into 
SPARQL is proposed in [10], with the use of background 
knowledge from ontologies and lexicons. The results of this 
approach show that the diagnosis process and the data search 
for a broad range of users is improved. 
In [11], an evaluation is made on how efficient the 
SPARQL query language is and the SPARQL Inferencing 
Notation (SPIN) when utilized to identify data quality 
problems in Semantic Web data automatically, and within the 
Semantic Web technology stack. 
In the case of deafblind patients, the simple and accurate 
representation of their environment is one of the most 
important needs. In [7][8][9] even though forms of natural 
language processing through ontologies are proposed, they do 
not involve healthcare or wearable sensors. For the healthcare 
related work [2][3][4] and [9][12], the natural language 
presentation of information component is absent. In the 
SUITCEYES project, we combine techniques involving both 
semantic technologies for healthcare and representation of the 
environment of the deafblind patients using natural language. 
The SUITCEYES ontology [13] includes concepts for spaces, 
e.g., rooms, halls, stairways etc., and entities found in them, 
e.g., objects and persons. In this work, we reuse and extend 
this ontology with classes and properties, and use rules 
expressed in SPARQL and SPIN notation, to achieve spatial 
awareness with natural language constructs for the deafblind. 
III. 
THE PROPOSED FRAMEWORK 
A. The Proposed Ontology 
1) Ontology Components 
The SUITCEYES ontology [13] is extended for the 
purposes of this work can be found online [14]. One of its aims 
is to integrate heterogeneous, multimodal input from different 
sensors in a formal and semantically enriched basis, and thus 
to combine user’s context-related information so as to provide 
enhanced situational awareness that can potentially augment 
users’ navigation and communication capabilities. The 
ontology 
was 
developed 
to 
augment 
its 
semantic 
interoperability with other ontologies that exist in the domains 
of interest and to enrich its semantic representation 
capabilities for covering the additional concepts and 
functional requirements that may emerge through any system 
addressed to deafblind people.  
In ontology engineering, it is common practice to reuse 
existing third-party models and vocabularies during the 
development of a custom ontology. This approach was also 
followed here, including the adoption of third-party 
vocabularies in order to rely on previously used and validated 
ontologies.  
The semantic representation of objects and activities from 
the Dem@Care ontology [4] was adopted, which contains a 
set of descriptions of every-day activities and common objects 
used in an every-day context that are highly relevant to our 
goals. Moreover, the ontology is using SOSA/SSN [15] 
ontologies for representing sensors and the respective 
observations. The Friend-Of-A-Friend (FOAF) specification 
[16] is used for representing persons and social associations. 
Finally, the Smart Energy Aware Systems (SEAS) Building 
Ontology [17] was integrated, which is a schema for 
describing the core topological concepts of a building, such as 
buildings, building spaces and rooms. Figure 1, shows the 
basic classes imported from existing ontologies for 
interoperability. 
2) Ontology Concepts 
Some of the basic classes of the ontology represent 
objects, spaces and people that can be detected as raw data 
form sensors, such as cameras, or processed data, through a 
visual analysis component. For example, some of the objects 
that can be found in the ontology are computer, laptop, alarm 
clock, mug, table, chair and other everyday objects and some 
of the spaces include bedroom, bathroom, living room and 
other spaces that can usually be found in a home environment. 
We extend these concepts by adding further objects, spaces 
and rooms that could be useful to any deafblind user. In Figure 
2, the main object entities of the combined ontologies are 
presented, while in Figure 3, the main spaces, such as rooms, 
are presented. 
161
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-763-4
eTELEMED 2020 : The Twelfth International Conference on eHealth, Telemedicine, and Social Medicine

3) Spatial Relations 
Topological relations of geometric objects have been 
widely described in literature and are generally utilized for 
navigation-, location- and context-based services. More 
specifically, the Egenhofer relations [18] or the DE-9IM 
topological model [19] can be used to specify how an object 
is located in space in relation to some reference object. For 
any two spatial objects, which can be points, lines and/or 
polygonal areas (represented by the definition of a bounding 
box), there are 9 relations derived from the model, which are: 
equals, disjoint, intersects, touches, contains, convers, 
covered by, and within. Distinct specializations of topological 
relations also exist, such as the so called alignment relations 
(horizontally aligned or vertically aligned) and orientation 
relations (left of, right of, top of and bottom of); these are 
considered in literature as mereology and parthood relations, 
as described in detail in [20] and visualized in Figure 4, from 
different perspectives (object-centered vs observer-centered). 
 
Figure 4. Object centered (left) and observer-centered (right) frames of 
reference 
We focus on specific spatial relations that have to do with 
the orientation (left/right), existence (in a room) and the 
distance (far/close/immediate). Thus, in the ontology, an 
entity that occupies space (e.g., persons, objects) is considered 
as a SpatialEntity and the occupied space (e.g., a room or a 
location) belongs to the SemanticSpace representation. These 
two aspects formulate the respective entity’s Spatial Context, 
which provides information regarding the entity’s relationship 
to the semantic space it is located in. Examples include: in, on, 
left, right, far, close, etc. The aforementioned concepts are 
depicted in Figure 7. 
B. Spatial Information Presented in Natural Language 
In Table 1, we present a list of indicative queries that are 
used in the ontology to provide the user with a natural 
 
Figure 1. Classes imported from the Dem@Care and SEAS ontology 
for interoperability 
 
 
Figure 2. Object Class of the ontology 
 
 
Figure 3. Space Class of the ontology 
 
162
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-763-4
eTELEMED 2020 : The Twelfth International Conference on eHealth, Telemedicine, and Social Medicine

language output regarding spatial information of their 
surroundings. In these queries, variables are used to cover a 
broad number of objects and spatial contexts. This kind of 
inference is achieved by using a set of ontological rules, 
written in SPARQL/SPIN notation, that run on top of the 
ontology, whenever a specific query is triggered by the user. 
Within the context of this scenario, a list of indicative queries 
have been created that can dynamically change on specific 
aspects, i.e., to cover different entities of interest, different 
spatial relations (with respect to the distance of position 
left/right), etc. 
 
TABLE 1. LIST OF RULES 
 
# 
Query 
Nat. Lang. Output 
1 
Where is my <object >? 
Your <object > is on your 
<right/left spatial context> side, 
<close/far spatial context> 
to/from you. 
2 
How many <objects> are on 
my <right/left spatial context> 
side? 
<# counted> objects, an 
<object1> and an <object 2> are 
on your  
<right/left spatial context> side. 
3 
Which <objects> are <spatial 
context> to/from me? 
An <object1>, <object2> and 
object3> are located <spatial 
context> to/from you. 
 
The variables in the above queries are given a specific 
value, depending on what the user wants to ask. For example, 
the first query can be transformed in natural language to: 
“Where is my laptop” and its output can be: “Your laptop is 
on your right side, close to you”. The implementation of this 
query is presented as an ontological rule written in 
SPARQL/SPIN syntax in Figure 5, using synthetic data that 
we created that include various objects and spatial contexts. 
Most of our ontological rules use SPARQL CONSTRUCT 
and DELETE/INSERT commands, in order to create new 
triples in the ontology and thus enrich the knowledge stored 
in the schema. 
In Figure 6, the implementation of the #3 query is 
presented, which in natural language translates to “Which 
objects are close to me?”, which using our synthetic data 
produces the output: “A laptop, a TV and a chair are located 
close to you”. For this query, we have skipped the construct 
rule, which is the same as the #1 query. 
 
 
Figure 7. Extended Semantic Spaces and Spatial Contexts of the 
SUITCEYES ontology 
CONSTRUCT{ 
 
?detection sot:producesOutput ?output. 
 
?output sot:refersToDetection ?detection. 
 
?output a sot:Output. 
 
?output sot:hasTextualDescription ?description 
} 
WHERE{ 
BIND(sospin:Function_SelectLatestObjectDetection() AS ?detection). 
?detection a sot:Detection. 
?detection sot:detectsObject ?object. 
?object a ?objectType. 
FILTER(?objectType = sot:Laptop). 
?object rdfs:label ?objectName. 
?object sot:hasSpatialContext ?spatialContext. 
?spatialContext a ?spatialContextType. 
?spatialContextType rdfs:subClassOf sot:spatialContext. 
FILTER((?spatialContextType = sot:RightSpatialContext) || 
(?spatialContextType = sot:LeftSpatialContext)). 
BIND(sospin:Function_leftRightContext(?spatialContext) AS 
?leftright_annotation). 
FILTER((?spatialContextType = sot:CloseSpatialContext) || 
(?spatialContextType = sot:FarSpatialContext). 
BIND(sospin:Function_CloseFarContext(?spatialContext) AS 
?closefar_annotation). 
BIND (BNODE() AS ?output). 
BIND(CONCAT(?objectName, "is on your", ?leftright_annotation, " 
side", ?closefar_annotation, " to you") AS ?description). 
} 
Figure 5. SPARQL/SPIN rule for query #1 
 
WHERE{ 
 
BIND(sospin:Function_SelectLatestObjectDetection() AS 
?detection). 
 
?detection a sot:Detection. 
 
?detection sot:detectsObject ?object. 
 
?detection sot:detectsObject ?object2. 
 
?detection sot:detectsObject ?object3. 
 
?object a ?objectType. 
 
?object2 a ?objectType. 
 
?object3 a ?objectType. 
 
?object sot:hasSpatialContext ?spatialContext1. 
 
?object2 sot:hasSpatialContext ?spatialContext2. 
 
?object3 sot:hasSpatialContext ?spatialContext3. 
 
?spatialContext a ?spatialContextType. 
 
?spatialContext2 a ?spatialContextType2. 
 
?spatialContext3 a ?spatialContextType3. 
 
?spatialContextType rdfs:subClassOf sot:SpatialContext. 
 
?spatialContextType2 rdfs:subClassOf sot:SpatialContext. 
 
?spatialContextType3 rdfs:subClassOf sot:SpatialContext. 
 
FILTER((?SpatialContextType = sot:closeSpatialContext) 
 
&& (?SpatialContextType2 = sot:closeSpatialContext) && 
 
(?SpatialContextType3 = sot:closeSpatialContext)). 
 
BIND(sospin:Function_CloseFarContext(?spatialContext) 
 
 
AS ?closefar_annotation). 
 
BIND (BNODE() AS ?output). 
 
BIND(CONCAT(“A “, ?objectName, “ , a “,  
 
 
?objectName2, “ and a “, ?objectName3, “, are 
 
 
“. ?closefar_annotation, “ to you.” AS  
 
 
?description). 
} 
Figure 6. SPARQL/SPIN rule for query #3 
 
 
 
163
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-763-4
eTELEMED 2020 : The Twelfth International Conference on eHealth, Telemedicine, and Social Medicine

C. Dynamic Update of the ontology with Differential 
Population Procedure 
In our proposed method, we use an efficient approach to 
store the incoming data to the ontology. We call this approach 
“differential population” of the ontology, which means that 
instead of populating the ontology with every Detection type 
data coming from every message in the message bus, a 
method is applied that checks if the incoming detection data 
is already included in the ontology. If it is, then we change 
only the timestamp of the existing Detection instance, 
otherwise we add the new Detection to the ontology. This 
method is implemented in java code and the pseudocode is 
presented in Figure 8. 
 
Figure 8. Pseudocode for the Differential Population procedure 
By using this technique, we limit the volume of data that 
are inserted in the ontology by only applying a simple check 
each time a message arrives from the message bus. This 
increases efficiency, considering that in a home environment 
the same objects could be detected in the same place multiple 
times (e.g., a TV almost never changes place in a home) and 
that many sensor systems continuously send data via their 
sensors. 
IV. 
PROOF OF CONCEPT – THE SUITCEYES USE CASE 
The SUITCEYES system tries to enrich the spatial 
awareness of the deafblind by implementing a solution 
involving a vest with a processing unit that receives raw 
information from sensors and actuators (mainly cameras and 
haptograms), and advanced information through a visual 
analysis component and a semantic component. An important 
aspect of the platform is the integration of information coming 
from the environment (via sensors) and from the system’s 
analysis components (camera feed and visual analysis). The 
most important sensors of the system are static cameras placed 
in rooms and cameras integrated a vest that the deafblind user 
wears (dynamic). In this sense, the ontology is primarily 
focused on semantically representing aspects relevant to the 
users’ context, in order to provide them with enhanced 
situational awareness and augment their navigation and 
communication capabilities. More importantly, the ontology 
also serves as the connection between environmental cues and 
content communicated to the user via haptograms. The 
deafblind user receives the output of the ontology via haptic 
sensors. i.e., a vest that vibrates in specific patterns on the 
user’s back and a special tablet that has haptic capabilities so 
the user can form patterns to ask questions regarding their 
environment. These mechanics and translation patterns from 
text to haptics are outside of the scope of this paper, which 
focuses on the information models and rules to form the 
natural language constructs to be translated. To test the 
correctness of those constructs formed using the rules, we 
have used synthetic data received by the SUITCEYES system. 
We present the output of the SPARQL/SPIN rule 
presented in Figure 5 by using the Protégé [21] software. In 
Figure 9, we present the object and description returned by the 
query. The description will be used as the output to the user. 
 
Figure 9. Query output of “Where is my laptop” 
In Figure 10 and Figure 11, we present the objects 
returned from the query “Which objects are close to me” with 
their spatial context, and the output that the user will get as 
textualDescription. 
 
Figure 10. Objects and their Spatial Context from query #3 
 
Figure 11. Query output of “Which objects are close to me” 
By receiving the expected results regarding our synthetic 
data, we validated the correctness of our queries. 
We also used the same synthetic data to test the 
differential population procedure. In the example below, we 
assume that a detection of a laptop in the living room exists 
in our ontology and that a same new detection arrives from 
the SUITCEYES system with a more recent timestamp. In 
Figure 12, we present what the outcome would be without 
using our differential population method, i.e., the storing of 2 
similar detections. 
 
Figure 12. Detections saved without the differential population procedure 
In Figure 13, the outcome of the same incoming detection 
is presented, but with using the differential population 
procedure. Here, only the first detection is stored, with its 
timestamp field changed to match that of the second, 
incoming detection. 
 
Figure 13. Detections saved with the differential population procedure 
This absence of almost identical detections could save a 
lot of space in real applications that continuously receive and 
store data. 
V. 
CONCLUSION AND FUTURE WORK 
In this paper, our method for providing spatial awareness 
to people with deafblindness, using natural language and 
semantic reasoning with SPIN/SPARQL notation was 
presented. Also, the differential population technique used for 
updating the data in the ontology was proposed. Finally, we 
tested our methods using synthetic data coming from the 
SUITCEYES platform. 
With today’s technology, people with deafblindness can 
be provided with advanced tools that enhance their spatial 
164
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-763-4
eTELEMED 2020 : The Twelfth International Conference on eHealth, Telemedicine, and Social Medicine

awareness. For our future work, we aim for the integration of 
our methods with the SUITCEYES project in two phases. The 
first phase includes the use of real data provided by sensors, 
such as cameras and the visual analysis component, and the 
second phase includes deafblind users’ interaction with the 
system. 
ACKNOWLEDGMENT 
This work was supported by the European Union's 
Horizon 2020 Research and Innovation Program through the 
project SUITCEYES under Grant 780814. 
REFERENCES 
[1] 
“SUITCEYES Project.” https://suitceyes.eu (accessed Sep. 
13, 2020). 
[2] 
G. Fico et al., “Co-creating with consumers and 
stakeholders to understand the benefit of internet of things 
in smart living environments for ageing well: The approach 
adopted in the madrid deployment site of the activage large 
scale pilot,” 2017, doi: 10.1007/978-981-10-5122-7_272. 
[3] 
G. Meditskos, T. G. Stavropoulos, S. Andreadis, and I. 
Kompatsiaris, 
“KnowSense: A semantically-enabled 
pervasive 
framework 
to 
assist 
clinical 
autonomy 
assessment,” 2015. 
[4] 
T. G. Stavropoulos, G. Meditskos, S. Andreadis, and I. 
Kompatsiaris, 
“Dem@Care: 
Ambient 
sensing 
and 
intelligent decision support for the care of dementia,” 2015. 
[5] 
A. Pliatsios, C. Goumopoulos, and K. Kotis, “A Review on 
IoT 
Frameworks 
Supporting 
Multi-Level 
Interoperability{\textendash}The 
Semantic 
Social 
Network of Things Framework,” Int. J. Adv. Internet 
Technol., vol. 13, pp. 46–64, 2020, [Online]. Available: 
https://www.iariajournals.org/internet_technology/inttech
_v13_n12_2020_paged.pdf. 
[6] 
A. I. Maarala, X. Su, and J. Riekki, “Semantic Reasoning 
for Context-Aware Internet of Things Applications,” IEEE 
Internet Things J., 2017, doi: 10.1109/JIOT.2016.2587060. 
[7] 
J. A. Bateman, J. Hois, R. Ross, and T. Tenbrink, “A 
linguistic ontology of space for natural language 
processing,” 
Artif. 
Intell., 
2010, 
doi: 
10.1016/j.artint.2010.05.008. 
[8] 
D. Estival, C. Nowak, and A. Zschorn, “Towards ontology-
based 
natural 
language 
processing,” 
2004, 
doi: 
10.3115/1621066.1621075. 
[9] 
H. Lee and J. Kwon, “Ontology model-based situation and 
socially-aware health care service in a smart home 
environment,” 
Int. 
J. 
Smart 
Home, 
2013, 
doi: 
10.14257/ijsh.2013.7.5.24. 
[10] 
M. Sander, U. Waltinger, M. Roshchin, and T. Runkler, 
“Ontology-based translation of natural language queries to 
SPARQL,” 2014. 
[11] 
C. Fürber and M. Hepp, “Using SPARQL and SPIN for 
data quality management on the Semantic Web,” 2010, doi: 
10.1007/978-3-642-12814-1_4. 
[12] 
C. Villalonga, H. Pomares, I. Rojas, and O. Banos, 
“MIMU-Wear: Ontology-based sensor selection for real-
world wearable activity recognition,” Neurocomputing, 
2017, doi: 10.1016/j.neucom.2016.09.125. 
[13] 
S. Darányi, N. Olson, M. Riga, E. Kontopoulos, and I. 
Kompatsiaris, “Static and Dynamic Haptograms to 
Communicate Semantic Content : Towards Enabling Face-
to-Face Communication for People with Deafblindness,” 
SEMAPRO 2019, The Thirteenth International Conference 
on Advances in Semantic Processing, Porto, September 22-
26, 2019. 2019. 
[14] 
“Multimedia Knowledge and Social Media Analytics 
Laboratory.” https://mklab.iti.gr/ (accessed Sep. 21, 2020). 
[15] 
K. Janowicz, A. Haller, S. J. D. Cox, D. Le Phuoc, and M. 
Lefrançois, “SOSA: A lightweight ontology for sensors, 
observations, samples, and actuators,” J. Web Semant., 
2019, doi: 10.1016/j.websem.2018.06.003. 
[16] 
D. 
Brickley 
and 
L. Miller, 
“FOAF 
Vocabulary 
Specification 0.99,” Namespace Document. 2014. 
[17] 
M. Lefrançois, “Planned ETSI SAREF extensions based on 
the W3C&OGC SOSA/SSN-compatible SEAS ontology 
patterns,” 2017. 
[18] 
M. J. Egenhofer, “A formal definition of binary topological 
relationships,” 1989, doi: 10.1007/3-540-51295-0_148. 
[19] 
E. Clementini, J. Sharma, and M. J. Egenhofer, “Modelling 
topological 
spatial 
relations: 
Strategies 
for 
query 
processing,” Comput. Graph., 1994, doi: 10.1016/0097-
8493(94)90007-8. 
[20] 
A. C. Varzi, “Spatial Reasoning and Ontology: Parts, 
Wholes, and Locations,” in Handbook of Spatial Logics, 
2007. 
[21] 
“Protege” https://protege/stanford.edu (accessed Sep. 01, 
2020). 
 
165
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-763-4
eTELEMED 2020 : The Twelfth International Conference on eHealth, Telemedicine, and Social Medicine

