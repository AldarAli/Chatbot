GFSM: a Feature Selection Method for Improving
Time Series Forecasting
Youssef Hmamouche∗, Piotr Przymus†, Alain Casali‡ and Lotﬁ Lakhal§
LIF - CNRS UMR 7279,
Aix Marseille Universit´e, Marseille, France
Emails: ∗youssef.hmamouche@lif.univ-mrs.fr, †piotr.przymus@lif.univ-mrs.fr,
‡alain.casali@lif.univ-mrs.fr, § lotﬁ.lakhal@lif.univ-mrs.fr
Abstract—Handling time series forecasting with many predictors
is a popular topic in the era of ”Big data”, where wast amounts
of observed variables are stored and used in analytic processes.
Classical prediction models face some limitations when applied
to large-scale data. Using all the existing predictors increases the
computational time and does not necessarily improve the forecast
accuracy. The challenge is to extract the most relevant predictors
contributing to the forecast of each target time series. We propose
a causal-feature selection algorithm speciﬁc to multiple time series
forecasting based on a clustering approach. Experiments are
conducted on US and Australia macroeconomic datasets using
different prediction models. We compare our method to some
widely used dimension reduction and feature selection methods
including principal component analysis PCA, Kernel PCA and
factor analysis. The proposed algorithm improves the forecast
accuracy compared to the evaluated methods on the tested
datasets.
Keywords–Time Series Forecasting; Feature Selection; Multi-
variate prediction models; Artiﬁcial Neural Networks.
I.
INTRODUCTION
Time series analysis and data mining incorporates a set
of tools, methods, and models for describing the evolution
of data over time. Such tools play important role in business
analysis and business intelligence systems where they generate
new, valuable information by combining trends, forecasts,
correlations, causalities etc. This additional information can
be then used to improve the decision-making process and
contribute to more intelligent and efﬁcient decisions.
This article is an extended version of research from [1].
Compared to the previous version, we have discussed possible
extensions to the algorithm and signiﬁcantly expanded the
experimental section.
In summary, we try to improve the forecasting of a
time series using multivariate models, by selecting only the
most relevant varaiables. This leads us to the problem of
hidden common factors that may cause multiple variables.
To overcome this problem, we propose a feature selection
algorithm based on the Granger causality graph. Each time
series is represented as a node in the graph and the causality
is expressed as edges weights. We follow the classical notion
of Granger predictive causality presented in [2], in order to
compute the dependencies between each two variables.
One of the ﬁrst successful univariate time series forecasting
models was the Auto-Regressive model [3]. Various versions
of this model are still in use today. They are based on the same
principle. That is, they take into account historic observations
in order to predict the future of variable. Univariate models
are limited only to one source of information, and thus,
they cannot utilize potentially-exploitable time series. To do
better, researchers began to introduce multivariate time series
analysis and forecasting models capable of exploiting multiple
time series [4]. Many of those concepts are still present in
forecasting tools nowadays.
Multivariate analysis increases the complexity of models
compared to univariate ones, as multivariate models describe
the forecasted time series based on (i) its historical observa-
tions and (ii) the historical observations of other series in the
dataset. On the plus side, utilizing relevant information from
other variables [5] may improve the resulting forecast.
Building a model using all existing variables is usually not
an viable option as it may add to much noise to ﬁt an accurate
model. For example, in [6], [7], based on two macroeconomic
datasets, it was found that using more than 30% − 60% of the
existing predictors does not improve forecast quality and in
fact may worsen the results. This rises question how to select
relevant variables, that was already investigated in several
works [5]–[8].
We can distinguish two popular approaches. One is to
enforce the model to discard irrelevant information either
by shrinking the coefﬁcients or eliminating them. Shrinkage
models, Lasso, Lars or regressors based on neural networks
are examples of this approach [6], [7].
The second approach is to use a two step model, where (i)
a separate procedure is used to extracts relevant information
from multiple variables, and then the selected variables are
used to build a multivariate forecasting model. The extraction
may be done using feature selection, dimension reduction or
by using the notion of causality [1], [5], [8]. In the second step,
any multivariate forecasting model can be used (including the
ones from previous paragraph).
Our proposed approach design is motivated by industrial
needs, where precise forecasts are requested in the presence of
huge amount of observed variables. The primary goal was to
provide a forecast horizon for a set of variables, which allows
to do an educated guess when to buy or sell products. The
secondary goal was to detect the frauds in public markets. We
mainly work on the prices of raw materials and/or ﬁnished
products available on public markets.
This paper is organized as follows. First, we discuss the
related works (Section II) and prediction models (Section III).
Next, we talk about causality measures (Section IV). Then, we
discuss our approach in Section V and evaluate its performance
in Sections VI and VII. Finally, in Section IX, we summarize
our contributions and possible future research.
255
International Journal on Advances in Systems and Measurements, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/systems_and_measurements/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

II.
RELATED WORK
Using all the existing variables in a multivariate model has
some drawbacks. First for Auto-Regressive based models, if
the number of regressors is proportional to the sample size, the
ordinary least squares (OLS) forecasts may not be efﬁcient [5].
Secondly, the most accurate forecasts are generally obtained
using smaller number of predictors [7], [6]. Thus, in this
section we discuss works that deal with the problem of large
number of predictors.
Feature selection refers to the act of extracting a subset
of the most relevant variables (features) of size k from a set
of variables of size n >> k. Dimension reduction methods
generate a new features with lower dimension from the original
features by transforming them. Both of them can be used to
optimize the inputs of prediction models. If additionally we
are interested with descriptive analysis, feature selection tech-
niques give more information as they select existing variables.
The Principal Component Analysis (PCA) is one of the
most common dimension reduction methods used. Based on
a set of variables, this method takes advantage of the inter-
correlation between them [9]. The idea is to generate the
principal linearly uncorrelated variables that describe as much
as possible the original variables. The Kernel PCA method is
a non-linear version of PCA, that extends it by considering
the non-linear relationships between variables using kernel
techniques [10]. Factor analysis (FACT) is another technique,
similar to PCA in the sense that it generates uncorrelated
factors of the original variables, additionally it ﬁts a model
of error terms associated with factors [9]. Both PCA and
FACT are used to construct the dynamic factor model [11],
[12], [5], [13]. It is a prediction model that is designed for
high-dimensional time series, or time series where the number
of observations exceeds the number of variables. The idea is
to ﬁnd a small number of hidden factors (dynamic factors),
that drive all of the observed variables. Thus, each variable
can be constructed as a combination of those factors. The
observed variables forecasts are constructed based on forecasts
of dynamic factors.
Another approach for dealing with many predictors is based
on the idea of shrinking the coefﬁcients of irrelevant variables
towards (or exactly to) 0. This can be achieved by ﬁtting the
regression model with constraints on coefﬁcients. There are
numerous well known shrinkage/regularization methods, for
instance the Lasso [14] and Ridge [15] methods. While they
are associated with the problem of multiple regressions, they
can be easily adapted to address the problem of forecasting
[16], [17], [6], [18], [7].
Artiﬁcial neural networks are also a common choice for
solving this problem. In [19], the authors propose an automatic
approach for stock market forecasting and trend analysis. A
pre-processing step was applied using the PCA in order to
transform the data into a set of uncorrelated variables and to
reduce the dimension of the input variables. Then, an artiﬁcial
neural network was used for forecasting the stock outputs, and
ﬁnally a neuro-fuzzy system is used to analyse the forecasts
trends. Similarly, in [8], a two step data mining process was
proposed for forecasting daily stock market, using PCA as a
ﬁrst step to reduce the dimension of predictor variables, then,
a feed-forward neural network is trained for prediction. In
[20], when forecasting time series that represent series of brain
images, with a number of variables larger than the number of
observations, the authors propose a feature selection method
based on PCA, Recursive Feature Elimination and Support
Vector Machines. In [21], a two-step forecasting approach
was presented to forecast two years of Australian electricity
load time series. First, correlation, Mutual Information and
instance-based feature selection methods are applied in order
to extract the relevant informative lag variables. And second,
a multivariate artiﬁcial neural network and statistical models
are applied to make forecasts.
III.
PREDICTION MODELS
In this section, we present a description of prediction mod-
els used in this research. Two types of models are discussed,
statistical and artiﬁcial neural network models.
A. Statistical models
Many prediction models are based on the same principle
as the Auto-Regressive model AR(p) [22]. The idea is to
expresses an univariate time series as a linear function of its
p precedent values:
yt = α0 + α1yt−1 + · · · + αpyt−p + ϵt,
where p is the order of the model, α0 ...αp are the parameters
of the model, and ϵt is a white noise error term.
The Moving Average model of order q (MA(q)) has the
same expression but for the error terms:
yt = α0 + α1ϵt−1 + · · · + αpϵt−q + ϵt
(1)
The ARMA(p, q) model [22] expresses errors terms and
past values of the time series in the same model. It can be
expressed as follows:
yt = α0 +
p
X
i=1
αiyt−i +
q
X
i=1
βiϵt−i + ϵt.
For non-stationary time series, the ARIMA(p, d, q) model [22]
is more preferable; it applies the ARMA(p, q) model after a
differencing step, in order to obtain stationary time series,
where d is the order of differencing (computing d times the
differences between consecutive observations).
In [23], the Vector Auto-Regressive (VAR) model was
introduced as an extension of the AR model. Consider a k-
dimensional time series Yt, the VAR(p) system expresses each
univariate variable of the multivariate time series Yt as a linear
function of the p previous values of itself and the p previous
values of the other variables:
Yt
= α0 + Pp
i=1 AiYt−i + ϵt,
where ϵt is a white noise with a mean of zero, and A1, . . . , Ap
are (k × k) matrix parameters of the model.
In [24], the Vector Error Correction (VECM) was intro-
duced. This model transforms the VAR model by taking into
account non-stationarity of the time series and by including
cointegration equations. To simplify matters, let us consider
two multivariate time series (Yt) integrated of order one, which
means all the variables of Yt are I(0) orI(1), stationary or
256
International Journal on Advances in Systems and Measurements, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/systems_and_measurements/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

integrated of order 1. The VECM Model can be written as
follows:
∆Yt
=
ΠYt−1 +
p−1
X
i=1
Γi∆Yt−i + ut,
where Π is the matrix representing the co-integration equa-
tions, which can be generated by VAR model, and Γi are the
matrix parameters of the VECM model. If rk(Π) = 0, then
there are no cointegration equations, so that the VECM model
is reduced to the VAR form applied after differencing time
series.
B. Artiﬁcial Neural Network Model
ANNS are increasingly popular for forecasting high-
dimensional time series. The relation between target and
predictors is encoded in a network of neurons characterized
generally by a non-linear function. The ANNS are usually able
to model time series more dynamically compared to classical
models.
The principle of using ANNS in time series forecasting is
simple. It consists of transforming the data into a supervised
learning problem, where the inputs represent the lagged values
of the predictors and the target variables. Then, the network
is initialized, commonly with randomly generated parameters.
Finally, the network is trained based on its inner objective
function (that depends on the network structure, activation
function, and other learning parameters). Formally, given a k-
dimensional time series (y1, . . . , yk) and a network function
fnn, the resulting time series can be expressed as:
yt = fnn(yt−1, . . . , yt−p, . . . , yk(t−1), . . . , yk(t−p)) + ϵt.
During the training step, the parameters of the network are
updated through the back-propagation step, using an opti-
mization algorithm, such as Gradient Descent or Stochastic
Gradient Descent, which aim to ﬁnd a local minimum of the
error function ϵt using some criteria. Popular criteria choices
are the mean squared error (mse) or the mean absolute error
(mae). Note that other metaheuristics can be used to try to
ﬁnd the global optimum of the error function, such as Genetic
Algorithms and Particle Swarm Optimization [25], [26].
In [27], the vector autoregressive neural network (VAR-
NN) model is investigated, as an extension of the classical
VAR process, based on a multi-layer perceptron. In [28], a
comparison between the VAR model and a multi-layered feed-
forward neural network has been presented for forecasting
macroeconomic variables. It was shown that the neural network
has a superior forecasts results than the VAR model in this
particular case. In [29], the VAR-NN model shows good
performance compared with the VARMA model in the task
of predicting non linear functions.
The main drawback of multi-layered perceptron neural
network is that the neurons are not able to remember past
information. Because each neuron provides an output based
directly on the activation function and the input values
ht = f(Wxt + b).
Yet, in time series (and sequences in general), maintaining in-
formation inside the network may improve the performance of
the model. Recurrent Neural Networks (RNNs) were designed
to handle this issue. Their structures allow hidden layers to be
self-connected, in a way that output may depend on the current
input and its previous state. Mathematically, the function of
RNNs neurons can be expressed as follows:
ht = f(Wxt + Uht−1 + b).
Unfortunately, the maintained memory in the network is
short, because the current state of network depends only on
the previous one. The Long Short Term Memory network
introduced in [30] was consequently designed to address this
problem. The authors propose a speciﬁc structure, by adding
extra options transforming traditional neurons into blocks, able
to model the mechanics necessary for the network to forget and
remember informations. Thus, it can learns how to use long-
term information passed through the network in the working
memory. The mathematical formulation of a hidden LSTM
block can be written as follow:
ft = σ(Wfxt + Ufht−1 + bf),
it = σ(Wixt + Uiht−1 + bi),
ct = ft ⊙ ct−1 + it ⊙ tanh(Wcxt + Ucht−1 + c0),
ot = σ(Woxt + Uoht−1 + bo),
ht = ot ⊙ tang(ct),
where ⊙ represents the element-wise multiplication xt is the
input vector, (W, U, b) are the parameters of the model, σ and
tanh are the sigmoid and tangent activation functions, ft is
the forget gate layer responsible for updating the weight of
remembering the previous information, it is the input gate
layer responsible for updating new information, ct is the cell
state at time t, ot is the output gate layer, and ht is to output
of the cell.
IV.
CAUSALITY MEASURES
Studying the relationships between time series is an im-
portant task for multivariate time series analysis, which can be
exploited for forecasting. The common measures like Corre-
lation and Mutual Information, are symmetric, so they do not
provide enough information about the dependencies between
variables, i.e., which variables inﬂuence the other. The goal
of causality is to detect the impact of one time series on an
another one in terms of prediction.
In this section, we discuss two different causality measures,
the Granger causality [2], and the Transfer entropy [31]. Let
us consider two univariate time series xt, yt. The Granger
deﬁnition of causality acknowledges the fact that xt causes yt
if it contains information helpful to predict yt. In other words,
xt causes yt if a prediction model that uses both xt and yt
performs better than the one that is based merely on yt.
We detail here the standard Granger causality test [2],
which uses the VAR model with a trend term. The test
compares two models. The ﬁrst one only takes into account
the precedent values of yt, and the second uses both xt and
yt in order to predict yt. If there is a signiﬁcant difference
between the two models, then it can be ascertained that the
added variable (xt) causes yt:
Model1 :
yt
=
α0 + αt +
p
X
i=1
αiyt−i + ϵt.
Model2 :
yt
=
α0 + αt +
p
X
i=1
αiyt−i +
p
X
i=1
βixt−i + ϵt.
257
International Journal on Advances in Systems and Measurements, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/systems_and_measurements/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

The next step of the test is to compare the residual sum
of squares (RSS) of these models using the Fisher test. The
statistic of the test is expressed as follow:
F = (RSS1 − RSS2)/p)
(RSS2/n − 2p − 1),
where RSS1 and RSS2 are the residual sum of squares related
to Model1 and Model2 respectively, n is the size of the
predicted vector. Two hypotheses are tested,
•
H0: ∀i ∈ {1, . . . , p}, βi = 0,
•
H1: ∃i ∈ {1, . . . , p}, βi ̸= 0.
Under the null hypothesis H0 (the hypothesis that x does not
cause y), F follows the Fisher distribution with (p, n−2p−1)
degrees of freedom. Therefore, the Granger causality test is
carried out at a level α in order to examine the null hypothesis.
The p-value of the test is the probability to observe the given
result under the assumption that H0 is true. In our case, we
consider the causality as one minus p-value in order to express
values of causalities in the range [0, 1].
Transfer Entropy is another causality measure based on in-
formation theory. Before discussing this measure, it is worth to
recall the notion of mutual information between two processes
(or two univariate time series) I and J. It measures the mutual
dependencies (symmetric) between the two processes. Con-
sider two processes I and J, with probability distribution p(i)
and p(j), joint probability p(i, j), and conditional probability
p(i|j). The mutual information between the two processes I
and J can be expressed using the Kullback entropy as follows:
MIJ =
X
i∈I,j∈J
p(i, j)log( p(i|j)
p(i)p(j)).
The main drawback of this measure is that is symmetric and
does not model the transfer of information from one process to
another. The transfer entropy was proposed as an extension of
mutual information in [31]. And the idea is to add a time shift
parameter, that allows modeling the non symmetric transfer of
information between processes. Even though Granger causality
and Transfer entropy may seem to be based on different
concepts, an interesting ﬁnding was presented in [32], showing
that they are equivalent for variables with normal distributions.
The mathematical formulation of transfer entropy from J to
the I can be expressed as follows:
TJ→I =
X
i∈I,j∈J
p(in+1, ik
n, jl
n)log(p(in+1 | ik
n, jl
n)
p(in+1 | ikn)
.
V.
OUR PROPOSAL
Our approach focuses on the selection of the top predic-
tor variables by describing their hidden dependencies using
bivariate causality. Let us consider Y = {y1, y2, . . . , yn} a
multivariate time series and a target variable y. The goal is
to select a subset of Y , for which we have the more accurate
forecasts of y.
We assume that causality is more important than correlation
measures when forecasting time series. Because, in contrast to
correlation, causality models the non-symmetric dependencies
between variables. In other words, if two variables are cor-
related, it does not identify which variable has an impact on
the other. By considering this hypothesis, one solution is to
choose a set of variables having strong causality regarding
to the target y. This is equivalent to a univariate feature
selection technique that ranks variables based on causality
to the target. This approach was already investigated in [33]
and [34] using the Granger causality test. The main limitation
of those approaches is that they potentially ignore hidden
relationship between predictor variables. That is, the same
hidden source of information may be exploited despite the fact
of using with multiple selected variables.
Let us underline that from a theoretical point of view,
there are Pk
i=1

Input: Set of predictors time series Y = {y1, y2, . . . , yn},
y the target variable, MINCAUS Min-Causality threshold,
k the selection size
Output: GFSM-CL the set of the selected variables associ-
ated to y
for i = 1 to n do
if causality(yi → y) ≤ MINCAUS then
Y = Y \ {yi}
end if
if Y.size() ⩽ k then
return GFSM-CL = Y
end if
end for
/* Build the matrix of causalities.
*/
Let MC be the adjacency matrix of causalities
for each yi, yj in Y such that i ̸= j do
MC[i, j] = MC[j, i] = 1 − max(causality(yi →
yj), causality(yj → yi))
end for
/* The clustering step.
*/
Clusters = clustering(MC, k)
for each Cluster cl in Clusters do
GFSM-CL = GFSM-CL ∪ arg max
clj∈cl
(causality(clj →
y)
end for
return GFSM-CL
Figure 2. The GFSM Algorithm.
B. Scalability of the proposed algorithm
Scalability of the algorithm is crucial when handling high-
dimensional time series, thus, in this part we discuss it.
Granger causality is computed by comparing forecasting ac-
curacy between univariate model and bivariate model (usually
followed by signiﬁcance test). This requires that two forecast-
ing models have to be constructed to compute the Granger
causality. An univariate AR model on target variable and a
bivariate VAR model with a target variable and one additional
predictor.
The algorithm presented uses the Granger causality graph.
It requires computation of a set of tuples G = (P × P) \ ∆,
where P is the set of predictors and ∆ = (p1, p2) ∈ P × P :
p1 = p2. Therefore, we have to compute univariate models for
all elements in P and bivariate models for tuples in G. This is
trivially scalable as all models can be computed independently.
Finally, the results should be grouped by target variable and
simple statistical tests of accuracy computed for univariate
model on variables p ∈ P, and a bivariate model (p1, p2) ∈ G
(simple task in map reduce approach). As a result, we compute
the adjacency matrix.
For ﬁnancial time series it is reasonable to assume that the
resulting matrix will have reasonable size and will feet single
computation node. In rare cases, when the matrix is very large,
scalable clustering algorithms could be used (like k-medoids
and other methods investigated in [35] and [36]).
C. Example
Consider Y = {y1, . . . , y8} a set of predictors and a target
variable y9. Let us apply the GFSM algorithm in order to select
4 predictor variables from Y that will contribute to forecast y.
1) The matrix of causalities: First, the algorithm computes
the Granger causalities between variables in pairs. In this ex-
ample, we take the matrix of causalities of a dataset containing
nine variables:
MC =


1.00 0.935 0.999 0.999 0.832 0.998 0.998 0.933 0.998
0.28
1.00 0.877 0.87 0.224 0.785 0.801 0.999 0.868
0.033 0.656 1.00 0.106 0.479 0.944 0.775 0.082 0.905
0.028 0.647 0.239 1.00 0.483 0.944 0.776 0.096 0.905
0.7
0.457 0.977 0.978 1.00 0.343 0.031 0.398 0.901
0.808 0.417 0.818 0.817 0.906 1.00 0.997 0.431 0.722
0.274 0.742 0.992 0.992 0.942 0.959 1.00 0.906 0.788
0.327 0.999 0.998 0.998 0.427 0.895 0.996 1.00 0.900
0.304 0.071 0.581 0.584 0.205 0.448 0.999 0.754 1.00


2) Clustering and selecting the variables: The algorithm
partitions the variables based on the symmetrical matrix (as
mentioned in the algorithm 2). The idea behind symmetrizing
the matrix of causalities is to be able to perform the clustering
task. By using the Partitionning Arround Medoids (PAM) [37],
let us also underline that this method partitions elements from
a symmetric dissimilarity matrix and minimizes dissimilarities
within clusters. In our case, the algorithm maximizes causali-
ties within clusters. That is why we use 1 minus the causality
matrix as an input of the PAM method.
Then, the algorithm chooses from each cluster the element
that has maximal causality on the target. The obtained clus-
tering vector associated to {y1, . . . , y8} is (1, 2, 1, 1, 3, 1, 4, 2).
And based on the causalities to the target (last column of the
adjacency matrix), the selected variables are {y1, y5, y7, y8}.
3) Evaluating the clusters: The quality of the causalities
founded depends on, ﬁrst the type of the data, and second,
on the evaluation of the clustering task. In our case, we eval-
uate the quality of the clusters using the following objective
function:
minimize G(x)
= Pn
i
Pn
j (1 − max(cij, cji)) × zij,
where
1)
zij =
 1
if yi, yj belong to the same cluster
0
otherwise.
2)
cij = causality(yi → yj).
This evaluation can be used in general as measure of causal
relationships in multivariate time series. In the example, the
value of G is 0.000168.
VI.
EXPERIMENTAL EVALUATION
In this section, we describe the experiments design, the
datasets used, the forecasting methodology, and the methods
and models implemented.
A. Datasets
The experiments are performed on macroeconomic datasets
of US and Australia.
•
The US macroeconomic dataset [6]: quarterly numeric
time series, containing 143 features and 200 observa-
tions, spanning the period 1960 − 2008.
•
The Australian macroeconomic dataset [7]: quarterly
numeric time series spanning the period 1984 − 2015,
comprising 117 variables and 119 observations.
These datasets were used in the context of forecasting with
many predictors in [6], [7]. Different models were evaluated
259
International Journal on Advances in Systems and Measurements, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/systems_and_measurements/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

using those datasets, the naive-benchmark and the AR (4) as
baseline models, the dynamic factor model, the VAR model
and other shrinkage methods. Both datasets contain three main
series, that we focus on in the experiments:
•
US dataset: GDP: Real gross domestic product, CPI
(Cpi all items (sa) fred), Fedfuns (Interest rate: federal
funds (effective)).
•
Australia dataset: GDP: Real gross domestic product,
CPI (Consumer Price Index), IBR (overnight interbank
rate).
B. Methods: feature selection and dimension reduction tech-
niques
We use the Scikit-learn machine learning module [38] for
the implementation of the following methods:
•
Principal component analysis (PCA) [39].
•
Kernel Principal component analysis (Kernel PCA)
[10].
•
Factor Analysis (FACT) [9].
And we implemented the following two methods:
•
A univariate feature selection method using Granger
causality (UFSM), similar to the one proposed in [33].
•
Our proposal, by generating two versions using two
clustering methods in the Algorithm 2. The ﬁrst one
is based on the Parttionning Arround Medoids (PAM)
[37] (pGFSM), and the second (hGFSM) is based on
the hieearchical clustering [40].
C. Methods: Prediction models
The used prediction models can be classiﬁed into three
types; baseline model, statistical models, and artiﬁcial neural
networks (ANNS):
•
Baseline model: we use the naive-benchmark (4)
model, that predict the next value of a variable based
on the mean of the last 4 values,
•
Statistical models: we use the AR(4) model and the
ARIMA (4, d, q) models (with automatic determination
of the parameters d and q, see Section III for details.)
to analyse forecasts without the use of predictors. And
the Vector Error Correction Model (see Section III for
mathematical formulation). We use the implementa-
tion from the forecast R package [41].
•
ANNS models: we use the following strategy to build
ANNS models. We transform the data into a supervised
learning problem, based on the lag parameter, the
target variable, and the selected predictors. Then, we
adapt two existing ANNs models to our problem. We
use the multilayer perceptron and the Long Short Term
Memory networks from the deep learning python
library; keras [42].
For the model based the MLP structure (VARMLP), we
use one hidden layer using a simple rule of thump to
determine the number of hidden neurons, 2/3×(n+1),
where n is the number of inputs. And for the LSTM
based model, we use the same number of units in
hidden layer as the number of input variables. In both
cases, the models inputs depend on the lag parameter
p and the number of predictors k, n = k×p. Then, the
Dimesion reduction 
and feature extraction
Forecast 
analysis
Time series 
datasets
Multivariate models: VECM, 
VARMLP, LSTM
Univariate models: Naive-
benchmark,  AR,ARIMA
Figure 3. The Used Forecasting Process.
networks are trained using back-propagation through
time, and the error functions are minimized using the
stochastic gradient decent algorithm.
D. Forecasting procedure
We implemented an automatic step-wise forecasting pro-
cess, as seen in Figure 3. First, It reduces the number of
predictors using feature selection and dimension reduction
techniques. Second, it applies the forecasting models. And ﬁ-
nally, it evaluates the quality of the obtained forecasts. We note
that the reduction step is not required for univariate models,
because they consider just the target variable to make forecasts.
The pre-processing step consists in transforming time series
to stationary via differencing, and this step is not required
for all models such VECM and ARIMA as they take it into
account automatically. For the proposed algorithm, we use the
Granger causality test as causality measure. The lag parameter
for Granger causality test is automatically determined using
the Akaikes Information Criterion (AIC) [43] with a maximum
value of 4 equivalent to 4 quarters.
We adopt the same forecasting procedures utilized to fore-
cast US and Australia datasets in [6] and [7]. We consider a lag
parameter equivalent to 4 quarters for prediction models. The
number of predictions for testing the models is 100 predictions
for US dataset and 75 for Australia dataset. Finally, the
prediction step is performed using a rolling window procedure
(we move one step each time, update models based on last
values, and compute the next forecasts), and we focus on the
ﬁrst horizon forecast.
E. Forecast accuracy measures
Two measures of forecast accuracy are used. The classical
root mean square error (RMSE), and the mean absolute scaled
error (MASE). The MASE is based on the errors of the forecasts
and the mean absolute error of the naive method on the in-
sample:
RMSE =
sPh
t=1(yt+n − ˆyt+n)2
h
,
MASE =
1
h
Pn+h
t=n+1 | yt − ˆyt |
1
n−1
Pn
t=2 | yt − yt−1 |,
where h is the number of predictions, (ˆyn+1, . . . , ˆyn+h) are
the forecasts, (yn+1, . . . , yn+h) are the real values.
VII.
RESULTS
We show in Tables I and II the forecast accuracy relative
to naive-benchmark model using the RMSE and the MASE
measures. These experiments are made on an single computer
with processor 2,2 GHz Intel Core i7 and 16Gb of RAM. The
evaluations are performed by executing the models (in rows) on
260
International Journal on Advances in Systems and Measurements, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/systems_and_measurements/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

all the features generated by all the methods (in columns). Due
to the computational time limitations, and the large number of
the obtained models with the associated subset of predictors,
we ﬁx the maximum number of the selected features at 20,
and we show only the optimal number k that provides the
best performance for each method, with an exception for
the method named UFSM, which does no require an input
parameter specifying the number of features.
VIII.
DISCUSSION
In this section, we discuss the obtained results, and we try
to explain some ﬁndings. We also compare the global results
with papers [6] and [7].
The main observation is that the two instances of the
proposed algorithm, i.e., based on the partitioning and the hier-
archical clustering techniques, outperform the other methods
for most of the target time series. This is especially visible
when they are used with artiﬁcial neural networks models.
In general, the results reveal that the forecasts of all
univariate models used are improved by multivariate models.
However, univariate models are competitive for some variables.
This is mainly due to the lack of relevant dependencies be-
tween some variables. For example, with the GDP variable of
Australian datasets, we slightly improved the naive-benchmark
model, only by using the pGFSM and hGFSM methods.
When forecasting the CPI variable of Australian dataset,
authors of [7] showed that the used multivariate models do
not improve the forecast accuracy of the AR(4) and the
naive-benchmark univariate models. Nevertheless, we show
that when we use the GFSM method with the LSMT model,
we improve the accuracy of forecast for this variable. As a
consequence, we argue that some of the obtained forecasts
in [7] and [6] can be improved using the proposed feature
selecting algorithm and neural network models.
In addition, we note that the best results are obtained
generally with a number of variables less than 15. This
conﬁrms relatively the results in [7], where the best results of
multivariate models are obtained using a number of predictor
variables less than 20 − 40.
As a side note, we do not discuss the statistical signiﬁcance
of the forecast accuracy. It is worth to mention that some
authors, such as [44], have argued that statistical signiﬁcance
testing of forecast accuracy should be avoided, as test results
may be misleading and that practice may actually harm the
progress of forecasting ﬁeld. We also notice that for some
target variables, in which, the UFSM method is applied to
select the predictor variables, the VECM model could not be
ﬁtted, and we do not have predictions for those variables. This
is mainly due to the important number of features generated
by this method, and this may cause some problems with
matrix operations (obtaining singular matrix) when ﬁtting the
parameters of the VECM model. To avoid this problem, one
should reduce more the number of predictor variables or reduce
the lag parameter. Another alternative consists in utilizing
artiﬁcial neural networks or adopt shrinkage approaches to
solve linear models.
Finally, we note that for the PCA and Kernel PCA dimen-
sion reduction methods, it is possible to have both automatic
number of features k or a speciﬁc number given in the input.
Currently, our proposal requires from the user the number of
features to be selected. It is equal to the number of clusters
generated by the clustering method used inside the algorithm.
Consequently, this algorithm can be extended to provide an
automatic number of features by using some methods of
automatic selection of the optimal number of clusters [45],
[46].
IX.
CONCLUSIONS
In the literature, a little attention has been paid to the
role of causality in feature selection for multiple time series
forecasting. While the impact of direct dependencies between
variables is not negligible in many types of real time series,
and the causality may help to detect the most relevant predictor
variables. In this paper, we investigated its role and we
proposed a feature selection algorithm speciﬁc to time series
forecasting with the idea of avoiding duplicated dependencies
between the predictor variables using a clustering approach.
The causality measure adopted for the experiments is the
Granger causality, but the proposed algorithm is applicable for
other similarity measures, for instance, the transfer entropy.
We have presented a benchmark experiments, by evaluating
some two-step approaches, that are based on feature selection
and dimension reduction techniques as a ﬁrst step before
applying prediction models. Experiments are conducted on
real macroeconomic datasets of US an Australia [6], [7]. And
we compared the proposed algorithm to some well known
exiting methods, using several prediction models. The results
show that the proposed algorithm is very competitive for both
datasets in terms of RMSE and MASE as forecast accuracy
measures, and works well with the VARMLP and the LSTM
artiﬁcial neural network models.
In future work, we aim to adopt a more deeper analysis on
the graph of casualties than the clustering approach, in order
to tackle dependencies between time series. As in the current
work, we test our approach on macroeconomic datasets, we
also aim to apply it on other type of data, as well as study the
applicability of the feature selection methods on the types of
the models (i.e., prediction, regression and others).
261
International Journal on Advances in Systems and Measurements, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/systems_and_measurements/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

TABLE I. The Forecast Accuracy Results Using The RMSE Measure, Relative To The Naive-Benchmark Model For AUSTRALIA And US Datasets.
Datasets
Series
RMSE
Models
PCA
Kernel PCA
FACT
UFSM
pGFSM
hGFSM
US DATASET
CPI
AR
1.04
1.04
1.04
1.04
1.04
1.04
ARIMA
1.05
1.05
1.05
1.05
1.05
1.05
VECM
0.97
0.97
0.95
1.27
0.81
0.76
VARMLP
0.54
0.96
1.00
0.97
0.87
0.86
LSTM
1.07
1.07
0.56
1.06
0.96
0.90
GDP
AR
0.92
0.92
0.92
0.92
0.92
0.92
ARIMA
0.93
0.93
0.93
0.93
0.93
0.93
VECM
0.97
0.96
0.97
0.88
0.88
VARMLP
0.88
0.89
0.94
1.25
0.85
0.78
LSTM
0.90
0.90
0.86
0.87
0.76
0.78
Fedfuns
AR
0.96
0.96
0.96
0.96
0.96
0.96
ARIMA
0.89
0.89
0.89
0.89
0.89
0.89
VECM
1.06
1.06
1.21
0.98
0.94
VARMLP
0.86
0.86
1.12
2.07
0.84
0.80
LSTM
0.75
0.71
0.78
1.16
0.81
0.78
AUSTRALIA DATASET
RGDP
AR
1.06
1.06
1.06
1.06
1.06
1.06
ARIMA
1.07
1.07
1.07
1.07
1.07
1.07
VECM
1.24
1.99
1.34
3.28
1.32
1.29
VARMLP
1.02
1.08
1.15
1.67
1.02
1.02
LSTM
1.04
1.05
1.01
1.40
1.00
0.96
IBR
AR
0.92
0.92
0.92
0.92
0.92
0.92
ARIMA
0.92
0.92
0.92
0.92
0.92
0.92
VECM
1.27
1.85
1.11
2.30
1.01
1.01
VARMLP
1.23
1.64
1.03
1.64
0.86
0.88
LSTM
1.14
0.86
1.02
1.71
0.80
0.75
CPI
AR
0.87
0.87
0.87
0.87
0.87
0.87
ARIMA
1.01
1.01
1.01
1.01
1.01
1.01
VECM
1.06
1.83
1.03
1.04
1.02
1.02
VARMLP
0.98
1.03
1.00
1.01
0.78
0.77
LSTM
1.02
1.03
1.00
1.01
0.82
0.88
TABLE II. The Forecast Accuracy Results Using The MASE Measure, Relative To The Naive-Benchmark Model For AUSTRALIA And US Datasets.
Dataset
Series
MASE
Models
PCA
Kernel PCA
FACT
UFSM
pGFSM
hGFSM
US DATASETS
CPI
AR
0.87
0.87
0.87
0.87
0.87
0.87
ARIMA
0.90
0.90
0.90
0.90
0.90
0.90
VECM
0.87
0.87
0.91
1.52
0.91
0.87
VARMLP
0.85
0.83
0.88
0.96
0.84
0.85
LSTM
0.94
0.95
0.89
0.97
0.84
0.89
GDP
AR
0.92
0.92
0.92
0.92
0.92
0.92
ARIMA
0.90
0.90
0.90
0.90
0.90
0.90
VECM
0.95
0.95
1.00
0.00
0.91
0.91
VARMLP
0.90
0.90
0.99
1.26
0.82
0.82
LSTM
0.92
0.93
0.85
0.89
0.77
0.77
Fedfuns
AR
0.95
0.95
0.95
0.95
0.95
0.95
ARIMA
0.86
0.86
0.86
0.86
0.86
0.86
VECM
1.17
1.17
1.32
1.04
0.95
VARMLP
0.93
0.93
1.16
2.06
0.93
0.85
LSTM
0.80
0.75
0.80
1.21
0.85
0.82
AUSTRALIA DATASET
CPI
AR
0.89
0.89
0.89
0.89
0.89
0.89
ARIMA
1.02
1.02
1.02
1.02
1.02
1.02
VECM
1.21
1.74
1.11
1.11
1.10
1.10
VARMLP
1.01
1.05
1.13
1.04
0.83
0.88
LSTM
1.04
1.06
1.03
1.03
0.89
0.95
IBR
AR
0.89
0.89
0.89
0.89
0.89
0.89
ARIMA
0.85
0.85
0.85
0.85
0.85
0.85
VECM
1.33
1.52
1.20
2.18
0.98
0.98
VARMLP
1.28
1.46
1.20
2.03
0.88
0.91
LSTM
1.24
1.00
1.11
0.85
0.85
0.84
RGDP
AR
1.05
1.05
1.05
1.05
1.05
1.05
ARIMA
1.08
1.08
1.08
1.08
1.08
1.08
VECM
1.37
1.96
1.46
2.98
1.33
1.33
VARMLP
1.07
1.13
1.15
1.08
1.02
1.07
LSTM
1.10
1.11
1.06
1.43
0.99
1.00
262
International Journal on Advances in Systems and Measurements, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/systems_and_measurements/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

PCA
KPCA
FACT
UFSM
pGFSM
hGFSM
Methods
0
5
10
15
20
25
Optimal number of features
VECM
VARMLP
LSTM
(a) RGDP
PCA
KPCA
FACT
UFSM
pGFSM
hGFSM
Methods
0
5
10
15
20
25
Optimal number of features
VECM
VARMLP
LSTM
(b) Australia CPI
PCA
KPCA
FACT
UFSM
pGFSM
hGFSM
Methods
0
5
10
15
20
25
30
Optimal number of features
VECM
VARMLP
LSTM
(c) IBR
PCA
KPCA
FACT
UFSM
pGFSM
hGFSM
Methods
0
10
20
30
40
50
Optimal number of features
VECM
VARMLP
LSTM
(d) GDP
PCA
KPCA
FACT
UFSM
pGFSM
hGFSM
Methods
0
2
4
6
8
10
12
14
16
Optimal number of features
VECM
VARMLP
LSTM
(e) US CPI
PCA
KPCA
FACT
UFSM
pGFSM
hGFSM
Methods
0
5
10
15
20
25
Optimal number of features
VECM
VARMLP
LSTM
(f) Fedfuns
Figure 4. The Best Number Of Features According To RMSE Of The Methods Used.
REFERENCES
[1]
Y. Hmamouche, A. Casali, and L. Lakhal, “A causality-based feature
selection approach for multivariate time series forecasting,” in DBKDA,
2017, pp. 97–102.
[2]
C. W. J. Granger, “Testing for causality,” Journal of Economic Dynam-
ics and Control, vol. 2, Jan. 1980, pp. 329–352.
[3]
G. Walker, “On Periodicity in Series of Related Terms,” Proceedings
of the Royal Society of London. Series A, Containing Papers of a
Mathematical and Physical Character, vol. 131, no. 818, 1931, pp. 518–
532.
[4]
P. Whittle, “The Analysis of Multiple Stationary Time Series,” Journal
of the Royal Statistical Society. Series B (Methodological), vol. 15,
no. 1, 1953, pp. 125–139.
[5]
J. H. Stock and M. W. Watson, “Chapter 10 Forecasting with Many Pre-
dictors,” in Handbook of Economic Forecasting, C. W. J. G. G. Elliott
and A. Timmermann, Eds.
Elsevier, 2006, vol. 1, pp. 515–554.
[6]
——, “Generalized Shrinkage Methods for Forecasting Using Many
Predictors,” Journal of Business & Economic Statistics, vol. 30, no. 4,
Oct. 2012, pp. 481–493.
[7]
B. Jiang, G. Athanasopoulos, R. J. Hyndman, A. Panagiotelis, and
F. Vahid, “Macroeconomic forecasting for Australia using a large
number of predictors,” Monash University, Department of Econometrics
and Business Statistics, Monash Econometrics and Business Statistics
Working Paper 2/17, 2017.
[8]
X. Zhong and D. Enke, “Forecasting daily stock market return using
dimensionality reduction,” Expert Systems with Applications, vol. 67,
2017, pp. 126–139.
[9]
I. T. Jolliffe, “Principal Component Analysis and Factor Analysis,”
in Principal Component Analysis, ser. Springer Series in Statistics.
Springer, New York, NY, 1986, pp. 115–128.
[10]
B. Sch¨olkopf, A. Smola, and K.-R. M¨uller, “Nonlinear Component
Analysis as a Kernel Eigenvalue Problem,” Neural Computation, vol. 10,
no. 5, Jul. 1998, pp. 1299–1319.
[11]
J. Geweke, “The dynamic factor analysis of economic time series,”
Latent Variables in Socio-Economic Models, 1977.
263
International Journal on Advances in Systems and Measurements, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/systems_and_measurements/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[12]
J. H. Stock and M. W. Watson, “Forecasting Using Principal Compo-
nents From a Large Number of Predictors,” Journal of the American
Statistical Association, vol. 97, no. 460, Dec. 2002, pp. 1167–1179.
[13]
J. H. Stock and M. Watson, “Dynamic Factor Models,” in Oxford
Handbook on Economic Forecasting.
Oxford University Press, 2011.
[14]
R. Tibshirani, “Regression Shrinkage and Selection Via the Lasso,”
Journal of the Royal Statistical Society, Series B, vol. 58, 1994, pp.
267–288.
[15]
A. E. Hoerl and R. W. Kennard, “Ridge Regression: Biased Estimation
for Nonorthogonal Problems,” Technometrics, vol. 12, no. 1, 1970, pp.
55–67.
[16]
J. H. Wright, “Forecasting US inﬂation by Bayesian model averaging,”
Journal of Forecasting, vol. 28, no. 2, Mar. 2009, pp. 131–144.
[17]
A. Carriero, G. Kapetanios, and M. Marcellino, “Forecasting large
datasets with Bayesian reduced rank multivariate models,” Journal of
Applied Econometrics, vol. 26, no. 5, Aug. 2011, pp. 735–761.
[18]
D. Korobilis, “Hierarchical shrinkage priors for dynamic regressions
with many predictors,” International Journal of Forecasting, vol. 29,
no. 1, Jan. 2013, pp. 43–59.
[19]
A. Abraham, B. Nath, and P. K. Mahanti, “Hybrid Intelligent Systems
for Stock Market Analysis,” in Computational Science - ICCS 2001.
Springer, Berlin, Heidelberg, May 2001, pp. 337–345.
[20]
H. Yoon and C. Shahabi, “Feature subset selection on multivariate time
series with extremely large spatial features,” in Data Mining Workshops,
2006. ICDM Workshops 2006. Sixth IEEE International Conference on.
IEEE, 2006, pp. 337–342.
[21]
I. Koprinska, M. Rana, and V. G. Agelidis, “Correlation and instance
based feature selection for electricity load forecasting,” Knowledge-
Based Systems, vol. 82, Jul. 2015, pp. 29–40.
[22]
G. Box, “Box and Jenkins: Time Series Analysis, Forecasting and
Control,” in A Very British Affair, ser. Palgrave Advanced Texts in
Econometrics.
Palgrave Macmillan UK, 2013, pp. 161–215.
[23]
M. H. Quenouille, “The analysis of multiple time-series,” 1957.
[24]
S. Johansen, “Estimation and Hypothesis Testing of Cointegration
Vectors in Gaussian Vector Autoregressive Models,” Econometrica,
vol. 59, no. 6, 1991, pp. 1551–1580.
[25]
J. N. D. Gupta and R. S. Sexton, “Comparing backpropagation with a
genetic algorithm for neural network training,” Omega, vol. 27, no. 6,
Dec. 1999, pp. 679–684.
[26]
K. Khan and A. Sahai, “A Comparison of BA, GA, PSO, BP and LM
for Training Feed forward Neural Networks in e-Learning Context,”
International Journal of Intelligent Systems and Applications, vol. 4,
no. 7, pp. 23–29.
[27]
D. U. Wutsqa, “The Var-NN Model for Multivariate Time Series
Forecasting,” MatStat, vol. 8, no. 1, Jan. 2008, pp. 35–43.
[28]
A. D. Aydin and S. C. Cavdar, “Comparison of Prediction Performances
of Artiﬁcial Neural Network (ANN) and Vector Autoregressive (VAR)
Models by Using the Macroeconomic Variables of Gold Prices, Borsa
Istanbul (BIST) 100 Index and US Dollar-Turkish Lira (USD/TRY)
Exchange Rates,” Procedia Economics and Finance, vol. 30, Jan. 2015,
pp. 3–14.
[29]
D. U. Wutsqa, S. G. Subanar, and Z. Sujuti, “Forecasting performance
of VAR-NN and VARMA models,” in Proceedings of the 2nd IMT-GT
Regional Conference on Mathematics, 2006.
[30]
S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural
Comput., vol. 9, no. 8, Nov. 1997, pp. 1735–1780.
[31]
T. Schreiber, “Measuring Information Transfer,” Physical Review Let-
ters, vol. 85, no. 2, Jul. 2000, pp. 461–464.
[32]
L. Barnett, A. B. Barrett, and A. K. Seth, “Granger causality and transfer
entropy are equivalent for Gaussian variables,” Physical Review Letters,
vol. 103, no. 23, Dec. 2009.
[33]
Y. Sun, J. Li, J. Liu, C. Chow, B. Sun, and R. Wang, “Using causal
discovery for feature selection in multivariate numerical time series,”
Machine Learning, vol. 101, no. 1-3, Jul. 2014, pp. 377–395.
[34]
X. Zhang, Y. Hu, K. Xie, S. Wang, E. W. T. Ngai, and M. Liu,
“A causal feature selection algorithm for stock prediction modeling,”
Neurocomputing, vol. 142, Oct. 2014, pp. 48–59.
[35]
M. C. K. Babu, P. Nagendra, “IJETT - Survey on Clustering on the
Cloud by UsingMap Reduce in Large Data Applications,” International
Journal of Engineering Trends and Technology.
[36]
Y. Wu, Y. Zhu, T. Huang, X. Li, X. Liu, and M. Liu, “Distributed
Discord Discovery: Spark Based Anomaly Detection in Time Series,”
in 2015 IEEE 17th International Conference on High Performance Com-
puting and Communications, 2015 IEEE 7th International Symposium
on Cyberspace Safety and Security, and 2015 IEEE 12th International
Conference on Embedded Software and Systems, Aug. 2015, pp. 154–
159.
[37]
A. P. Reynolds, G. Richards, B. de la Iglesia, and V. J. Rayward-Smith,
“Clustering Rules: A Comparison of Partitioning and Hierarchical Clus-
tering Algorithms,” Journal of Mathematical Modelling and Algorithms,
vol. 5, no. 4, Dec. 2006, pp. 475–504.
[38]
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander-
plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and ´E. Duch-
esnay, “Scikit-learn: Machine Learning in Python,” Journal of Machine
Learning Research, vol. 12, no. Oct, 2011, pp. 2825–2830.
[39]
M. E. Tipping and C. Bishop, “Probabilistic Principal Component
Analysis,” Journal of the Royal Statistical Society, Series B, vol. 21/3,
Jan. 1999.
[40]
F. Murtagh and P. Legendre, “Ward’s Hierarchical Agglomerative
Clustering Method: Which Algorithms Implement Ward’s Criterion?”
Journal of Classiﬁcation, vol. 31, no. 3, Oct. 2014, pp. 274–295.
[41]
R. Hyndman, M. O’Hara-Wild, C. Bergmeir, S. Razbash, and E. Wang,
“Forecast: Forecasting Functions for Time Series and Linear Models,”
Feb. 2017.
[42]
F. Chollet and others, “Keras: Deep learning library for theano and
tensorﬂow,” URL: https://keras. io/k, 2015.
[43]
H. Akaike, “A new look at the statistical model identiﬁcation,” IEEE
Transactions on Automatic Control, vol. 19, no. 6, Dec. 1974, pp. 716–
723.
[44]
J. S. Armstrong, “Signiﬁcance tests harm progress in forecasting,”
International Journal of Forecasting, vol. 23, no. 2, Apr. 2007, pp. 321–
327.
[45]
L. Kaufman and P. J. Rousseeuw, Finding Groups in Data: An Intro-
duction to Cluster Analysis.
John Wiley & Sons, Sep. 2009.
[46]
R. Tibshirani, G. Walther, and T. Hastie, “Estimating the number of
clusters in a data set via the gap statistic,” Journal of the Royal Statistical
Society: Series B (Statistical Methodology), vol. 63, no. 2, Jan. 2001,
pp. 411–423.
264
International Journal on Advances in Systems and Measurements, vol 10 no 3 & 4, year 2017, http://www.iariajournals.org/systems_and_measurements/
2017, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

