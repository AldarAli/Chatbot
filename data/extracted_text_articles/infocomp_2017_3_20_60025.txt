Mathematical Aspects of Application of Neural Networks to Processes with Delays 
 
Zlatinka Kovacheva 
Department of Mathematics and Applied Sciences 
Middle East College 
Muscat, Oman 
e-mail: zlatinka@mec.edu.om 
Val√©ry Covachev 
Institute of Mathematics and Informatics 
Bulgarian Academy of Sciences 
Sofia, Bulgaria 
e-mail: vcovachev@hotmail.com
 
 
Abstract‚ÄîNeural networks are used to solve different kinds of 
problems from a wide range of disciplines. A brief overview of 
the history and performance of neural networks is given. Some 
neural network models are presented. Additionally, we 
summarize our  results concerning the existence and global 
exponential stability of an equilibrium point or periodic 
solution of these models.  
Keywords-neuron; artificial neural network; processes with 
delay. 
I. 
 INTRODUCTION  
Artificial Neural Networks (ANN) are computational 
paradigms, which implement simplified models of their 
biological counterparts, biological neural networks. 
Although the initial intent of ANN was to explore and 
reproduce human information processing tasks, such as 
speech, vision, and knowledge processing, ANN also 
demonstrated their superior capability for classification and 
function approximation problems. This has great potential 
for solving complex problems, such as systems control, data 
compression, optimization problems, pattern recognition, 
and system identification. 
Neural networks have wide applicability to real world 
business problems. In fact, they have already been 
successfully applied in many industries. Since neural 
networks are best at identifying patterns or trends in data, 
they are well suited for prediction or forecasting needs 
including: sales forecasting, industrial process control, 
customer research, data validation, risk management, target 
marketing and so on. 
ANN are also used in the following specific paradigms: 
recognition of speakers in communications; diagnosis of 
hepatitis; recovery of telecommunications from faulty 
software; interpretation of multi-meaning Chinese words; 
undersea mine detection; texture analysis; three-dimensional 
object recognition; hand-written word recognition; and facial 
recognition [1], [2], [3]. 
In this paper, we are focusing on the application of neural 
networks to processes with delay. The rest of this paper is 
organized as follows: In Section II, we give some 
information on the history and action of artificial neurons. In 
Section III, we consider some neural network models, 
namely, continuous-time neural networks of Hopfield- and 
Cohen-Grossberg-type and their discrete-time counterparts. 
We conclude the paper in Section IV.  
 
 
II. 
ARTIFICIAL NEURON 
An artificial neuron is a device with many inputs and one 
output. The neuron has two modes of operation; the training 
mode and the using mode. In the training mode, the neuron 
can be trained to fire (or not), for particular input patterns. In 
the using mode, when a taught input pattern is detected at the 
input, its associated output becomes the current output. If the 
input pattern does not belong in the taught list of input 
patterns, the firing rule is used to determine whether to fire 
or not. 
The first artificial neuron was produced in 1943 by the 
neurophysiologist Warren McCulloch and the logician 
Walter Pitts [4]. But the technology available at that time did 
not allow them to do too much. Neural networks process 
information in a similar way the human brain does. The 
network is composed of a large number of highly 
interconnected processing elements (neurons) working in 
parallel to solve a specific problem. Neural  networks learn 
by example. 
In the human brain, a typical neuron collects signals from 
others through a host of fine structures called dendrites. The 
neuron sends out spikes of electrical activity through a long, 
thin stand known as an axon, which splits into thousands of 
branches. At the end of each branch, a structure called a 
synapse converts the activity from the axon into electrical 
effects that inhibit or excite activity from the axon into 
electrical effects that inhibit or excite activity in the 
connected  neurons. 
A more sophisticated neuron is the McCulloch and Pitts 
model (MCP) [4]. The difference from the previous model is 
that the inputs are ‚Äúweighted‚Äù, the effect that each input has 
at decision making is dependent on the weight of the 
particular input. The weight of an input is a number which, 
when multiplied by the input, gives the weighted input. 
These weighted inputs are then added together and, if they 
exceed a pre-set threshold value, the neuron fires. In any 
other case, the neuron does not fire. In mathematical terms, 
the neuron fires if and only if 
 
 

=1
> 	, 
where ,  = 1, 
, are weights, ,  = 1, 
, inputs, and 
	 a threshold. The addition of input weights and of the 
24
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

threshold makes this neuron a very flexible and powerful 
one. The MCP neuron has the ability to adapt to a particular 
situation by changing its weights and/or threshold. Various 
algorithms exist that cause the neuron to ‚Äúadapt‚Äù; the most 
used ones are the Delta rule and the back error propagation 
[2],[3]. The former is used in feed-forward networks and the 
latter in feedback networks. 
An important application of neural networks is pattern 
recognition. Pattern recognition can be implemented by 
using a feed-forward neural network that has been trained 
accordingly. During training, the network is trained to 
associate outputs with input patterns. When the network is 
used, it identifies the input pattern and tries to output the 
associated output pattern. The power of neural networks 
comes to life when a pattern that has no output associated 
with it, is given as an input. In this case, the network gives 
the output that corresponds to a taught input pattern that is 
least different from the given pattern. 
 
 
III. 
NEURAL NETWORK MODELS 
A. Hopfield-Type Neural Networks 
Hopfield-type (additive) networks have been studied 
intensively during the last two decades and have been 
applied to optimization problems. The original model [5] 
used two-state threshold ‚Äúneurons‚Äù that followed a 
stochastic algorithm: each model neuron  had two states, 
characterized by the values 
 or 
 (which may often be 
taken as 0 and   1, respectively). The input of each neuron 
came from two sources, external inputs  and inputs from 
other neurons. The total input to neuron  is then  
Input to  =  =  	

+ , 
where 	 can be viewed as a description of the synaptic 
interconnection strength from neuron  to neuron .  The 
motion of the state of a system of   neurons in the state 
space describes the computation that the set of neurons is 
performing. A model, therefore, must describe how the state 
evolves in time, and the original model describes this in 
terms of a stochastic evolution. Each neuron samples its 
input at random times. It changes the value of its output or 
leaves it fixed according to a threshold rule with thresholds 
: 
 ‚Üí 

if
 	

+  < ,
 ‚Üí 

if
 	

+  > .
 
A simple Hopfield-type neural network is the following 
one: 
$
%&'()
%(
= ‚àí &'()
+
+  ,- .&'()/
0
1
+ ,      = 1, 
, 
where   denotes the number of units (neurons) in the 
network, &'() denotes the state of the -th unit at time (, the 
positive constants $ and + are the neuron amplifier input 
capacitance and resistance, respectively,   is the constant 
input from outside the network, - .&'()/  denotes the 
output of the -th unit on the -th unit at time (, , is the 
weight (strength) of the synaptic connection between the -
th unit and the -th unit.  
In the formulation of the above system, it is implicitly 
assumed that the neurons process input, produce output and 
communicate with each other instantaneously. But this is 
usually not true and there can be significant time delays 
both in neural processing and axonal transmission. Such 
delays can be concentrated (discrete), or continuously 
distributed over  a certain duration of time, finite or infinite.  
Most widely studied and used neural networks can be 
classified as either continuous or discrete. Recently, there 
has been a somewhat new category of neural networks 
which are neither purely continuous-time nor purely 
discrete-time. This third category of neural networks, called 
impulsive neural networks, displays a combination of 
characteristics of both the continuous and discrete systems. 
To the best of our knowledge, impulsive neural networks 
first appeared in 1999 [6], yet we would mention that after 
the publication of our paper [7] in 2004 hundreds or maybe 
thousands of papers devoted to impulsive neural networks 
appear each year.  
In order to solve problems in the fields of optimization, 
neural control and signal processing, neural networks have 
to be designed such that there  is only one equilibrium point 
and this equilibrium point is globally asymptotically stable 
so as to avoid the risk of having spurious equilibria and 
local minima. In the case of global stability, there is no need 
to be specific about the initial conditions for the neural 
circuits since all trajectories starting from anywhere settle 
down at the same unique equilibrium. If the equilibrium is 
exponentially asymptotically stable, the convergence is fast 
for real-time computations. 
     In our paper [7], we considered several Hopfield-type 
systems incorporating the aforementioned features. All of 
them can be put together as the following: 
 
                 %&'()
%(
= ‚àí,&'() +  2- .&'()/
0
1
               '1) 
+  34 .&5( ‚àí œÑ7/
0
1
 
+  %‚Ñé 9: ;'<)&'( ‚àí <) %<
=

>
0
1
+ ,    ( > 0,    ( ‚â† (A, 
 
                               ‚àÜ&'(A) = ‚àíCA&'(A)                               '2) 
+ :
œàA'<)&'<) %<
FG
FGHI
+ Œ≥A,      = 1, 
,     K ‚àà ‚Ñï, 
with initial values prescribed by piecewise-continuous 
functions &'<) = œï'<)  which are bounded for < ‚àà
25
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

'‚àí‚àû, 0P. The coefficient , > 0 is the rate with which the -
th unit self-regulates or resets its potential when isolated 
from other units and inputs; -'‚àô) , 4'‚àô) , ‚Ñé'‚àô)  denote 
activation functions; the parameters 2 , 3 , %  represent 
the weights (or strengths) of the synaptic connections 
between the  -th unit and the  -th unit; the constant  
represents an input signal introduced from outside the 
network to the -th unit; œÑ are nonnegative numbers whose 
presence indicates the delayed transmission of signals at 
time ( ‚àí œÑ  from the -th unit to the -th unit; the delay 
kernels ;'<) incorporate the fading past effects (or fading 
memories) of the  -th unit on the  -th unit; ‚àÜ&'(A)  =
&'(A + 0) ‚àí &'(A ‚àí 0)
 denote 
impulsive 
state 
displacements at fixed instants of time (A 'K ‚àà ‚Ñï) involving 
integral terms whose kernels œàA'<): S(AT, (AP ‚Üí ‚Ñù  are 
measurable functions, essentially bounded on the respective 
interval. Here, it is assumed that the sequence of times 
V(AWA1
=
 satisfies 
0 = ( < ( < (X < ‚ãØ < (A ‚Üí ‚àû
 as 
K ‚Üí ‚àû; CA and  Œ≥A are some constants. 
     Using the Contraction Mapping Principle (Banach‚Äôs 
Fixed Point Theorem), we found sufficient conditions for 
the existence of a unique equilibrium point of the above 
system. Further, using a suitable Lyapunov functional we 
found sufficient conditions for the global exponential 
stability of the equilibrium (that is, each solution of the 
system tends exponentially to the equilibrium point).  
   Recently, we considered a class of Hopfield neural 
networks with integral impulsive conditions and finite 
distributed delays, formulated in the form of an œâ-periodic 
system of impulsive delay differential equations 
 
%&'()
%(
= ‚àí,'()&'() 
+  2- 9: ;'<)&'( ‚àí <) %<
[

>
0
1
+ '(),     ( ‚â† (A, 
 
‚àÜ&'(A) = ‚àí\A&'(A) 
+  CAŒ¶ 9:
3A'<)&'<) %<
FG
FGHI
>
0
1
+ Œ≥A, 
   = 1, 
,     K ‚àà ‚Ñ§. 
 
   Using the Contraction Mapping Principle, we found 
sufficient conditions for the existence of a unique œâ -
periodic solution. Moreover, if an œâ -periodic solution 
exists, using an appropriate Lyapunov functional we found 
sufficient conditions for its global exponential stability. We 
noted that the above-mentioned œâ-periodic solution can be 
found approximately by the method of successive 
approximations. 
 
B. Cohen-Grossberg Neural Networks 
We have also studied continuous-time impulsive neural 
networks more general than the Hopfield-type neural 
networks, such as the Cohen-Grossberg neural networks. 
Thus, in [8] we considered the impulsive Cohen-Grossberg 
neural  network with S-type delays  
 
%&'()
%(
= ,5&'()7 _‚àí25&'()7 +  3- .&'()/
0
1
 
+  % : 4 .&'( + Œ∏)/  %Œ∑'Œ∏)

Tb
+ 
0
1
c , ( > 0, ( ‚â† (A, 
 
‚àÜ&'(A) = ‚àíCA&'(A) + : &'(A + Œ∏) %Œ∂A'Œ∏)

Tb
+ Œ≥A, 
                         
 = 1, 
,     K ‚àà ‚Ñï, 
 
with initial values prescribed by piecewise-continuous 
functions &'<) = Œ¶'<) with discontinuities of the first kind 
for < ‚àà S‚àíe, 0P . Here ,'&) denotes an amplification 
function; 2'&)  denotes an appropriate function which 
supports the stabilizing (or negative) feedback term 
,'&)2'&) of the unit ; the past effect of the -th unit on 
the -th unit is given by a Lebesgue-Stieltjes integral; the 
impulsive state displacements at fixed moments of time (A, 
K ‚àà ‚Ñï, also involve Lebesgue-Stieltjes integrals. This type of 
delays in the presence of impulses is more general than the 
usual types of delays studied in the literature. In fact, 
concentrated delays correspond to the points of discontinuity 
of  the bounded variation functions. 
For the above system, sufficient conditions are found for 
the existence of a unique equilibrium point and its global 
exponential stability. Examples of impulsive systems 
satisfying the sufficient conditions obtained are given, 
namely, the differential system with S-type delays  
 
&f'() = '2 + sin &'())S‚àí2&'() + 0.1 arctan &'() 
+0.15 arctan &X'() + 0.1 : &'( + Œ∏) %lm

T
 
                                          +0.15 : &X'( + Œ∏) %lm

T
n, 
 
&fX'() = '3 + sin &X'())S‚àí3&X'() + 0.15 arctan &'() 
‚àí0.2 arctan &X'() + 0.1 : &'( + Œ∏) %lm

T
 
                                             ‚àí0.2 : &X'( + Œ∏) %lm

T
n, 
 
provided with one of the following three sets of impulse 
conditions: 
26
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

‚àÜ&'(A) = ‚àí 1
2 &'(A) + 1
4 : &'(A + Œ∏) %lm

T
,
‚àÜ&X'(A) = ‚àí 1
4 &X'(A) + 1
4 : &X'(A + Œ∏) %lm

T
;
 
 
‚àÜ&'(A) = ‚àí100&'(A) + : &'(A + Œ∏) %lm

T
,
‚àÜ&X'(A) = ‚àí50&X'(A) + : &X'(A + Œ∏) %lm

T
,
(A = 10K,     K ‚àà ‚Ñï;
 
 
‚àÜ&'(A) = ‚àí'K + 1)&'(A) + K : &'(A + Œ∏) %lm

T
,
‚àÜ&X'(A) = ‚àí'KX + 1)&X'(A) + KX : &X'(A + Œ∏) %lm

T
,
(A = KX,     K ‚àà ‚Ñï.
 
 
In all three cases, the equilibrium point '0,0)r is globally 
exponentially stable with Lyapunov exponent respectively: 
1 in the first case, 0.039 in the second case, and any 
Œª ‚àà '0,0.5) in the third case.    
 
C. Discrete-Time Neural Networks 
For different Hopfield-type and Cohen-Grossberg neural 
networks, we have found their discrete-time counterparts and 
found sufficient conditions for existence and global 
exponential stability of equilibria and periodic solutions. 
Here, we recall just the results of our paper [9], where we 
obtain a discrete-time counterpart of system (1), (2). Let 
‚Ñé > 0 denote a uniform discretization step size and S(/‚ÑéP 
denote the greatest integer in (/‚Ñé . For convenience, we 
denote S(/‚ÑéP, u ‚àà V0W ‚à™ ‚Ñï, and, by an abuse of notation, 
write &'u)  instead of &'u‚Ñé) . Further on, we denote 
Œ∫ = xœÑ/‚Ñéy, ,  = 1, 
. Finally, we replace the integral 
terms z
;'<)&'( ‚àí <) %<
=

, ,  = 1, ,
  by sums of the 
form ‚àë
|'})&'u ‚àí }),
=
~1
 where } = S</‚ÑéP,  |'}) 
stands for |'}‚Ñé) and &'u ‚àí }) for &5'u ‚àí })‚Ñé7. 
Now, on the interval Su‚Ñé, 'u + 1)‚Ñé) (u ‚àà V0W ‚à™ ‚Ñï) we 
approximate system (1)  by 
 
                  %&'<)
%<
= ‚àí,&'<) +  2- .&'u)/
0
T
             '3) 
+  34 .&5u ‚àí Œ∫7/
0
T
 
+  %‚Ñé  |'})&'u ‚àí })
=
~1
¬Ä
0
T
+ . 
 
We rewrite equation (3) in the form  
%
%< '&'<)l¬Å¬Ç¬É) = l¬Å¬Ç¬É  2- .&'u)/
0
T
 
+  34 .&5u ‚àí Œ∫7/
0
T
 
+  %‚Ñé  |'})&'u ‚àí })
=
~1
¬Ä
0
T
+ ¬Ä ,  = 1, 
, 
 
and integrate it over the interval Su‚Ñé, 'u + 1)‚ÑéP to obtain 
 
&'u + 1) = lT¬Å¬Ç¬Ñ&'u) + 1 ‚àí lT¬Å¬Ç¬Ñ
,
 2- .&'u)/
0
T
 
                               +  34 .&5u ‚àí Œ∫7/
0
T
                         '4) 
+  %‚Ñé  |'})&'u ‚àí })
=
~1
¬Ä
0
T
+ ¬Ä ,  = 1, 
, 
u ‚àà V0W ‚à™ ‚Ñï,    = 1, 
. 
 
This system is the discrete-time analogue of the system 
without  impulses  (1). It is provided with initial values of the 
form &'‚àí‚Ñì) = œÜ'‚àí‚Ñì) '‚Ñì ‚àà V0W ‚à™ ‚Ñï), where the sequences 
VœÜ'‚àí‚Ñì)W‚Ñì1
= are bounded for all  = 1, 
. The method used 
here is called semi-discretization [1]. It is easy to see that 
systems  (1) and (4) have the same equilibria if any.  
Further on, denote uA = S(A/‚ÑéP  we approximate the 
impulsive conditions (2) by  
 
            &'uA
¬á) ‚àí &'uA
T) =

CA‚Ñì&'‚Ñì)
¬àG
‚Ñì1¬àGHI¬á
+ Œ≥A,        '5)  
 = 1, 
,   K ‚àà ‚Ñï 
 
where, for convenience, u = ‚àí1 and CA‚Ñì  are suitably 
chosen constants. 
Finally, we find sufficient conditions for the global 
exponential stability of the unique equilibrium point of the 
system  (4), (5). 
 
 
 
IV. 
CONCLUSION 
In the present paper we gave a short overview of the 
history, performance and applications of neurons and neural 
networks. We presented several neural network models and 
our results concerning the existence and global exponential 
stability of an equilibrium point or periodic solution of these 
models.  
 
 
 
27
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

REFERENCES 
 
[1] S. Mohamad and K. Gopalsamy, ‚ÄúDynamics of a class of 
discrete-time neural networks and their continuous-time 
counterparts,‚Äù Math. Comput. Simulat.,  vol. 53, pp. 1-39, 
2000. ISSN: 03784754 
[2] A. I. Galushkin, Neural Networks Theory. Berlin-Heidelberg: 
Springer-Verlag, 2007. ISBN: 9783540481249 
[3] J. Heaton, Introduction to the Math of Neural Networks. 
Heaton Research. ISBN: 9781604390339 
[4] W. McCulloch and W. Pitts, ‚ÄúA logical calculus of the ideas 
immanent in nervous activity,‚Äù Bull. Math. Biophys., vol. 9, 
pp. 127-147,  1943. ISSN: 0007-4985 
[5] J. J. Hopfield, ‚ÄúNeurons with graded response have collective 
computational properties like those of two-state neurons,‚Äù 
Proc. Natl. Acad. Sci. USA, vol. 81, pp. 3088-3092, 1984. 
ISSN: 1091-6490 
[6] Z.-H. Guan and G. Chen, ‚ÄúOn delayed impulsive Hopfield 
neural networks,‚Äù Neural Netw., vol. 12, pp. 273-280, 1999. 
ISSN: 0893-6080 
[7] H. Ak√ßa, R. Alassar, V. Covachev, Z. Covacheva, and E. Al-
Zahrani, ‚ÄúContinuous-time additive Hopfield-type neural 
networks with impulses,‚Äù J. Math. Anal. Appl., vol. 290, pp. 
436-451, 2004. ISSN: 0022-247X, 1096-0813 
[8] H. Ak√ßa and V. Covachev, ‚ÄúImpulsive Cohen-Grossberg 
neural networks with S-type distributed delays,‚Äù Tatra Mt. 
Math. Publ., vol. 48, pp. 1-13, 2011. ISSN: 1210-3195 
[9] H. Ak√ßa, R. Alassar, V. Covachev, and Z. Covacheva, 
‚ÄúDiscrete counterparts of continuous-time additive Hopfield-
type neural networks with impulses,‚Äù Dyn. Syst.  Appl., vol. 
13, pp. 77-92, 2004. ISSN: 1056-2176 
 
 
 
 
 
 
 
28
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-567-8
INFOCOMP 2017 : The Seventh International Conference on Advanced Communications and Computation

