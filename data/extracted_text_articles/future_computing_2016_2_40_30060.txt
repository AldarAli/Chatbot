Use of RBM for Identifying Linkage Structures of Genetic Algorithms
Hisashi Handa
School of Science and Engineering
Kindai University
Kowakae 3-4-1, Higashi-Osaka, Osaka 577–8502, Japan
Email: handa@info.kindai.ac.jp
Abstract—Linkage Identiﬁcation is a quite important in Genetic
Algorithm studies. If we found linkage structures, we can improve
coding methods, genetic operations. The Restricted Boltzmann
Machine (RBM) is adopted to capture linkage structures in this
study. The learning data of the RBM consist of data generated by
referring to the nonlinearity check of the Linkage Identiﬁcation
by non-Linearity Check. The activation values of the neurons in
the hidden layer in the RBM and corresponding learning data
are visualized. We can easily ﬁnd out linkage structures by using
the resultant images.
Keywords–Genetic
Algorithms;
Linkage
Identiﬁcation;
Re-
stricted Boltzmann Machines.
I.
INTRODUCTION
Linkage Identiﬁcation plays crucial roles in Evolutionary
Computation. The linkage means that a certain tuple of genes
appears with a higher rate more than the expected value of a tu-
ple of independent genes [1]. Such genes with higher linkages
are supposed to be functionally dependent. Based upon the
Building Block Hypothesis [2], several building blocks which
have higher linkages are juxtapositionally searched. They are
combined with each other by using crossovers implicitly.
Therefore, the GA community has been studying this area,
since a wrong problem encoding method with strong linkage
causes of performance deteriorations [3]. We can elaborate
genetic operators if we found the linkage structure in advance.
Although Estimation of Distribution Algorithms (EDA) are a
systematic approach to solving problems taking into account
for problem structures, some EDA, such as the Distribution
Estimation Using Markov network (DEUM) still need linkage
structures in order to design Markov Networks [4][5].
In this paper, we use RBM to capture the linkage structures.
That is, we do not pursue to develop a new mechanism to
detect linkage structures. We can detect linkage structure by
seeing the visualized image as a consequence of the learning
of the RBM.
The organization of the remainder of the paper is as
follows: Section II brieﬂy summarizes related works. Section
III introduces Linkage Identiﬁcation by Nonlinearity Check
(LINC). We employ the non-linearity check in the LINC to
detect the non-linearity in the proposed method. Section IV
explains the RBM. Section V describes the proposed method.
Preliminary experiments on several benchmark problems are
examined in Section VI. Section VII concludes this paper.
II.
RELATED WORKS
One aspect of linkage identiﬁcation problems is that they
can be regarded as a permutation problem [3]. In genetic
algorithms we usually use bit string representation. Therefore,
in order to address linkage identiﬁcation problems, we can
solve for an ordering problem of individual representation.
The effectiveness of the inversion operator is investigated by
Bagley [6], where the inversion operator reverses the order of
a partial sub-string in an individual. The idealized reordering
operator introduced by Goldberg and Bridges [7] can address
the linkage learning problems. Ying-ping showed the perfor-
mance of the genetic algorithms with the idealized reordering
operator can be improved by using tournament selection [8].
Conventional linkage identiﬁcation studies tackle to devise
the mechanism to detect linkages: LINC by Munetomo et al.
uses non-linearity check among two loci to ﬁnd out the linkage
[9]. His group has been extended to cope with more realistic
problems, such as Linkage Identiﬁcation with Epistasis Mea-
sure considering monotonicity (LIEM), and Linkage Identiﬁ-
cation by non Monotonicity Detection (LIMD) [10][11].
Our main contribution of this paper is to tackle to address
of the linkage identiﬁcation problems by using representational
learning algorithms, i.e., the RBM.
III.
LINKAGE IDENTIFICATION BY NON-LINEARITY
CHECK
LINC by Munetomo et al. focused on the non-linearity
among two loci. This paper adopts this nonlinearlity check
method in order to generate learning data of the RBM.
Suppose that s indicates an individual whose string length
is L. In order to investigate the linkage structures, s is
randomly sampled at ﬁrst. Base upon the individual s, non-
linearity is checked as followings: Now, ∆fi(s) stands for the
ﬁtness difference if the ith locus is changed:
∆fi(s)
=
f(..¯si....) − f(..si....)
(1)
∆fj(s)
=
f(.....¯sj.) − f(.....sj.)
(2)
∆fij(s)
=
f(..¯si..¯sj.) − f(..si..sj.),
(3)
where ∆fij(s) means the ﬁtness difference if the ith and jth
loci are simultaneously changed.
Equation (4) is used if the non-linearity exists among the
ith and jth loci:
|∆fij(s) − (∆fi(s) + ∆fj(s))| > e,
(4)
where e is a small positive number.
41
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-461-9
FUTURE COMPUTING 2016 : The Eighth International Conference on Future Computational Technologies and Applications

weight
visible
layer
hidden
layer
{
Nv
neurons
{
Nh
neurons
Figure 1. Depiction of Restricted Boltzmann Machine
IV.
RESTRICTED BOLTZMANN MACHINES
RBM is a two-layered network as shown in Figure 1: The
visible layer v and the hidden layer h. Each neuron in the
visible layer v is fully connected with all the neuron in the
hidden layer h. Note that there are no connections among
neurons in the visible layer and among neurons in the hidden
layer.
Energy function E(v, h) can be deﬁned as follows:
E(v, h) = −
Nh
∑
i
Nv
∑
j
wijhivj −
Nv
∑
j
bjvj −
Nh
∑
i
cihi,
(5)
where Nh and Nv denotes the number of neurons in the hidden
layer h and in the visible layer v, respectively. hi indicates ith
neuron in the hidden layer. vj is jth neuron in the visible layer.
wij, bj, and ci are learning parameters for the RBM: weight
values, bias values for the visible layer, and bias values for
hidden layer. Parameters, such as wij, bj, and ci, are learned
by using Constrictive Divergence method.
The joint probability P(v, h) is represented by using this
energy function E(v, h) in equation (6):
P(v, h) = 1
Z exp(−E(v, h)),
(6)
where Z is a normalized factor.
For input v, the activation values of the neurons in the
hidden layer is calculated as follows:
p(hi = 1|v) = σ(ci +
Nv
∑
j=1
wijvj),
(7)
where σ means Sigmoid function.
V.
PROPOSED METHOD
A. Overview
This paper uses the RBM to identify the linkages in
problems. First, the non-linearity check in (4) is used to
detect loci which may have linkages. Such loci information is
transformed into learning data. These learning data are learned
by using the RBM. After the learning of the RBM, the learned
RBM examines the learning data again in order to investigate
the activated values for the learning data. The averaged vector
of the learning data, such that a certain neuron in the hidden
layer is activated, is calculated. An image is generated by using
a set of the averaged vectors for all the neurons in the hidden
layer. From the image, we can ﬁnd out if the linkage exists.
1: repeat
2:
An individual s is randomly generated.
3:
for each pair (i, j) ∈ A do
4:
if rand() < p then
5:
The pair (i, j) is examined the non-linearity check
in (4).
6:
if non-linearity is found for the pair (i, j) then
7:
A learning data based on the pair (i, j) is ap-
pended.
8:
end if
9:
end if
10:
end for
11: until The number of learning data reaches to the prede-
ﬁned number
Figure 2. Procedure of generating learning data
0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0
0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0
0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0
Figure 3. Learning data generated by the procedure in Figure 2
B. Generation of Learning Data
We assume that in order to solve a given problem to be
optimized, i.e., ﬁtness function, a binary encoding method with
L bits is used. That is, individuals s are represented by a L
bit string. Figure 2 summarizes the procedure of generating
learning data, where A denotes a set of all the combinations
of two numbers picked from 1 to L without repetition. The
“rand()” at line 4 in the procedure is supposed to be random
function which returns a random number in [0, 1] over the
uniform distribution. A parameter p at line 4 is set to be 0.05
in this study. The number of learning data at line 11 in the
procedure is set to be 100,000.
The generation of learning data at line 7 in Figure 2 is
described as follows: First, an L bit-string is generated, where
all the data in the string is set to be 0. Second, ﬂip the ith
and jth data to 1. Figure 3 shows an example of learning data
generated by the procedure in Figure 2. Each row denotes each
learning data so that this problem seems to be the one of 27-
bits length. At the ﬁrst row, the second and the third loci is
set to be 1. Hence, this learning data implies these loci may
have a linkage at these loci. This example only shows 10 data
while the number of learning data in this study is set to be
100,000.
These learning data are applied to the RBM. The number of
neurons in the visible layer Nv is the same as the string length
L. The number of neurons in the hidden layer Nh is set to be
around 90 % of Nv. These numbers are varied for problem
instances; the exact number of Nv and Nh is explained in
Section VI.
42
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-461-9
FUTURE COMPUTING 2016 : The Eighth International Conference on Future Computational Technologies and Applications

C. Generation of Averaged Vectors
As depicted in Figure 4, after the learning of the RBM,
the learned RBM examines the learning data again in order to
investigate the activated values of the neurons in the hidden
layer. The input pattern, i.e., each of learning data and the
corresponding activated values, is stored.
Suppose that ui denotes a set of learning data, such that
the ith neuron in the hidden layer is activated. In this paper,
the hidden neuron is activated if the activated value is greater
than 0.5. The average vector x∗
i is averaged over the entire
learning data in the set ui. Figure 4 shows an example, where
the averaged vector is calculated when the second neuron in
the hidden layer is activated. The averaged vector x∗
i is a real-
valued vector with the L dimensions. The range of the elements
of the averaged vector x∗
i is [0, 1], since each learning data is
a binary vector.
D. Visualization of the Averaged Vector
A binary vector bi = (b1
i , b2
i , . . . , bL
i ) is generated from
each of the averaged vector x∗
i = (x∗,1
i
, x∗,2
i
, . . . , x∗,L
i
), where
i = (1, 2, . . . , Nh): Each bj
i is set to be 1 if x∗,j
i
> 0.5.
Otherwise, bj
i is set to be 0.
These binary vectors bi are only used for sorting the
averaged vectors x∗
i . That is, these binary vectors are regarded
as binary numbers so that these binary numbers are used as
a key for the sort. Based upon the key, the x∗
i is sorted. The
sorted x∗
i is used to generate an image, such that the ping or
red color at a certain pixel indicates there may be linkage at
corresponding variables.
Figure 5 shows an example of generated image. Each
averaged vector is delineated horizontally. Each element of
the averaged vector x∗
i is assigned 10x10 pixels. Hence, the
image has the width L × 10, and the height Nh × 10.
As in Figure 5, white color denotes that the corresponding
element of the averaged value is 0, which means it has
no linkage. Meanwhile, red color means the corresponding
element has linkage with the other colored element in the same
row. Pink color indicates marginal: strong pink denotes that it
tends to have linkage while light pink denotes that it may have
linkage.
VI.
EXPERIMENTS
A. Fitness Functions
•
n-trap function has n-bit linkages. Suppose that The
n-trap function is composed of m sub-functions ft,
where m = L/n:
ft(s, k) =
{
n
if nk = n
n − 1 − nk
Otherwise,
(8)
where nk indicates the number of one’s in the kth
sub-function.
Fitness function ftrap can be deﬁned as follows:
ftrap(s) =
m
∑
k
ft(s, k)
(9)
•
6-trap bi-polar function is similar to the n-trap
function where n = 6. The sub-function fb of the
TABLE I. FITNESS FUNCTIONS AND THE NUMBER OF NEURONS
IN THE VISIBLE LAYER Nv AND THE HIDDEN LAYER Nh
ﬁtness function
L(= Nv)
Nh
5-trap
50
40
six-bipolar
48
40
hierarchical trap
27
24
hierarchical trap
81
72
6-trap bi-polar function, however, has two peaks.
fb(s, k) =
{
3
if abs(3 − nk) = 0
2 − abs(3 − nk)
Otherwise,
(10)
where nk is the same meaning as in the n-trap
function.
Fitness function f6trapBP can be deﬁned as follows:
f6trapBP(s) =
L/6
∑
k
fs(s, k)
(11)
•
hierarchical trap function can be recursively deﬁned:
there are a large number of 3-trap-like function, which
are connected as in a tree. For explanation, we assume
that L = 27. We have nine 3-trap-like function f 1
h at
the bottom level as follows:
fh(s, k) =
{
3
if nk = 3
3 − nk
Otherwise,
(12)
If nk = 3 or nk = 0, such function outputs 1 or 0 to
the upper level, respectively. Otherwise, the function
output nil.
In the second level, i.e., the upper level of the bottom
level. We have three 3-trap-like function. The outputs
from the bottom level are used for ﬁtness calculations.
Therefore, each 3-trap-like function at the second
level is connected to three 3-trap-like functions at the
bottom level. The 3-trap-like functions output nil if
there is/are (an) output(s) nil from at least one 3-trap-
like function at the lower level. Otherwise, 3-trap-like
function fh in (12) is used.
At the top level, there is a 3-trap function. As in the
3-trap-like function at the second level, this function
at the top level uses the output from the functions at
the lower level, i.e., the second level. This function at
the top level also output nil if at least one 3-trap-like
function(s) output(s) nil.
The ﬁtness of the hierarchical trap functions is cal-
culated by summing all the outputs of the 3-trap-like
function at the top level and the one of 3-trap-like
functions at the other level. Note that nil is regarded
as 0 for this calculation.
B. Experimental Settings
As described in Table I, we examined several ﬁtness
functions explained the previous subsection. Except for the
hierarchical trap function, all the functions are decomposable
functions. Table I also summarizes the number of neurons in
the visible layer Nv and the hidden layer Nh of the RBM.
43
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-461-9
FUTURE COMPUTING 2016 : The Eighth International Conference on Future Computational Technologies and Applications

1 0  0  1
0 1  0  1
1 1  0  0
Learning 
Data
No.1:
No.2:
No.3:
0.9  0.1  0.9
0.1  0.7  0.9
0.9  0.8  0.2
Activated
Values
Figure 4. Learning data generated by the procedure in Figure 2
{
Nh 
rows
{
Nv columns
Averaged
Vector of
4th neuron
in Hidden
Layer
This activated vector claims
6th to 10th variables have a linkage
Figure 5. Example of visualization
Figure 6. Experimental Results for 5-trap function
C. Experimental Results of Decomposable Functions
Figures 6 and 7 show the experimental results for the 5-trap
function and the 6-trap bipolar function, respectively. There are
ten and eight linkage clusters in these functions. Even though
the non-linearity check examined in this paper concerns with
only two bits, the proposed method can capture such linkage
structures well.
The ﬁrst averaged vector in Figure 7 depicted lighter pink
for all the variables. Such lighter pink patterns are often
appeared.
D. Experimental Results of Hierarchical Trap Function
Figures 8 and 9 show the experimental results of the non-
decomposable function, i.e., the Hierarchical Trap function.
Figure 7. Experimental results for 6-trap bipolar function
Figure 8. Experimental results of 27-bit Hierarchical Trap Function
We examined 27-bit Hierarchical Trap Function — three level
problem —, which is explained in subsection VI-A. Moreover,
81-bit Hierarchical Trap Function — four level problem — is
also examined. These ﬁgures show that the proposed method
can ﬁnd out linkage structures for the bottom level and the
second level. For the second level, we can see that the lighter
pink areas are for each tuple of corresponding nine variables.
However, the third level and the fourth level (for 81-bit
problem) cannot extract linkage structure at all. The reason for
this is that it is unable to capture such dependence structure by
only two bits which are examined in the non-linearity check in
this paper. Therefore, we think that non-linearity check with
the higher order is needed.
44
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-461-9
FUTURE COMPUTING 2016 : The Eighth International Conference on Future Computational Technologies and Applications

Figure 9. Experimental results of 81-bit Hierarchical Trap Function
Figure 10. Experimental results of 81-bit Hierarchical Trap function by the Deep Boltzmann Machine
E. Comparison with “Deep” Structures
Recently, the RBM is used in Deep Learning, which is a
part of Deep Belief Net., or Deep Boltzmann Machine. We
examine the effect of deep structure in neural networks for
linkage detection. In Figure 10, the experimental results of
the DBM are shown. We employed 81-bit Hierarchical Trap
function for this comparison. The network structure of the
DBM, i.e., the number of neurons in each layer, is 81, 72,
63, and 54. For comparison, we constitute the RBM such that
the number of neurons in the visible layer and the hidden
layer are 81 and 54, respectively. The experimental results
45
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-461-9
FUTURE COMPUTING 2016 : The Eighth International Conference on Future Computational Technologies and Applications

Figure 11. Experimental results of 81-bit Hierarchical Trap function by a single RBM with 54 neurons in the hidden layer
of the RBM are shown in Figure 11. It seems that there are
no signiﬁcant differences between these ﬁgures. Even if deep
structures are employed, we cannot detect linkage structures
in the third and fourth levels. We might need to examine more
various network structures for the DBM. However, we think
that the input for the RBM and the DBM are modiﬁed in
order to capture linkage structure, i.e., non-linearity check so
that such higher dependency are disregarded at the learning
data generation.
VII.
CONCLUSION
In this study, we employed the RBM to detect linkage
structures in ﬁtness function which are used in Genetic Algo-
rithms. In order to constitute the learning data for the RBM,
non-linearity check used in LINC is used. We also introduced
a visualization method by using sorted averaged vectors. We
can easily ﬁnd out by using the resultant images if the linkage
structures exist.
Future works are summarized as follows: We extend non-
linearity check mechanism to cope with a higher order of
linkage detection. We think that more bits should be simulta-
neously checked. On the other hand, we would like to use the
deep learning in this study. For this purpose, the representation
of learning data must be considered.
ACKNOWLEDGMENT
This work was partially supported by the Grant-in-Aid
for Scientiﬁc Research (C) and the Grant-in-Aid for Young
Scientists (B) of MEXT, Japan (26330291, 23700267)
REFERENCES
[1]
D. R. Newman, “The Use of Linkage Learning in Genetic Algorithms,”
http://www.ecs.soton.ac.uk/
drn05r/ug/irp/,
Last
Update:
September
2006.
[2]
D. E. Goldberg, “Genetic Algorithms in Search, Optimization, and
Machine Learning,” Addison-Wesley, 1989.
[3]
C. Ying-ping, Y. Tian-li, S. Kumara, and D. E. Goleberg, “A Survey of
Linkage Learning Techniques in Genetic and Evolutionary Algorithms,”
Technical Report IlliGAL Report, 2007.
[4]
P. Larranaga and J. A. Lozano (Eds.), “Estimation of distribution al-
gorithms: A new tool for evolutionary computation,” Kluwer Academic
Publishers, 2002.
[5]
S. K. Shakya, J. A. W. McCall, and D. F. Brown, “Updating the proba-
bility vector using MRF technique for a Univariate EDA,” Proceedings
of the Second Starting AI Researchers ’ Symposium, volume 109 of
Frontiers in artiﬁcial Intelligence and Applications, 2004, pp. 15-25.
[6]
J. D. Bagley, “The behavior of adaptive systems which employ genetic
and correlation algorithms,” Ph.D. dissertation, University of Michigan,
Ann Arbor, MI, 1967.
[7]
D. E. Goldberg and C. L. Bridges, “An analysis of a reordering operator
on a GA-hard problem,” Biological Cybernetics, vol. 62, 1990, pp. 397-
405.
[8]
C. Ying-ping and D. E. Goleberg, “An analysis of a reordering operator
with tournament selection on a GA-hard problem,” Proceedings of Ge-
netic and Evolutionary Computation Conference 2003 (GECCO-2003),
2003, pp. 825-836.
[9]
M. Munetomo and D. E. Goldberg, “Designing a genetic algorithm
using the linkage identiﬁcation by nonlinearity check.” Technical Report
IlliGAL Report No.98014, University of Illinois at Urbana-Champaign,
1998.
[10]
M. Munetomo and D. E. Goldberg, “Linkage identiﬁcation by non-
monotonicity detection for overlapping functions,” Evolutionary Com-
putation, Vol. 7, No. 4, 1999, pp. 377–398.
[11]
M. Munetomo, “Linkage identiﬁcation with epistasis measure con-
sidering monotonicity conditions,” Proceedings of the 4th Asia-Paciﬁc
Conference on Simulated Evolution and Learning, 2002, pp. 550-554.
[12]
A. Fischer and C. Igel, “An Introduction to Restricted Boltzmann
Machines,” Proc. CIARP 2012, LNCS 7441, 2012, pp. 14-36.
[13]
R. Salakhutdinov and G. E. Hinton, “Deep Boltzmann machines,” Proc.
JMLR Workshop and Conference Proceedings: AISTATS 2009, vol. 5,
2009, pp. 448-455.
46
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-461-9
FUTURE COMPUTING 2016 : The Eighth International Conference on Future Computational Technologies and Applications

