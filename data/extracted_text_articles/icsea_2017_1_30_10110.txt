Automatic Documentation of the Development of Numerical Models for Scientiﬁc
Applications using Speciﬁc Revision Control
Martin Zinner∗, Karsten Rink†, René Jäkel∗, Kim Feldhoff∗, Richard Grunzke∗,
Thomas Fischer†, Rui Song‡, Marc Walther†§, Thomas Jejkal¶, Olaf Kolditz†∥, Wolfgang E. Nagel∗
∗ Center for Information Services and High Performance Computing (ZIH)
Technische Universität Dresden
Dresden, Germany
E-mail: martin.zinner1@mailbox.tu-dresden.de, {rene.jaekel, kim.feldhoff}@tu-dresden.de,
{richard.grunzke, wolfgang.nagel}@tu-dresden.de
† Department of Environmental Informatics
Helmholtz Centre for Environmental Research (UFZ)
Leipzig, Germany
E-mail: {karsten.rink, thomas.fischer, marc.walther, olaf.kolditz}@ufz.de
‡ Technical Information Systems
Technische Universität Dresden
Dresden, Germany
E-mail: rui.song@tu-dresden.de
§ Professorship of Contaminant Hydrology
Technische Universität Dresden
Dresden, Germany
E-mail: marc.walther@tu-dresden.de
¶ Institute for Data Processing and Electronics
Karlsruhe Institute of Technology (KIT)
Karlsruhe, Germany
E-mail: thomas.jejkal@kit.edu
∥ Professorship Applied Environmental System Analysis
Technische Universität Dresden
Dresden, Germany
E-mail: olaf.kolditz@ufz.de
Abstract—As software becomes increasingly complex, automatic
documentation of the development is becoming ever more impor-
tant. In this paper, we present a novel, general strategy to build a
revision control system of the development of numerical models
for scientiﬁc applications. We set up a formal methodology of the
strategy and show the consistency, correctness, and usefulness of
the presented strategy to automatically generate a documentation
for the evolution of the model.
Keywords–Software development; Automatic generation of doc-
umentation; Revision control; Backup and Restore; Metadata;
Improvement of research environment; Support of research process.
I.
INTRODUCTION
In scientiﬁc applications, dedicated software packages are
used to create numerical models for the simulation of physical
phenomena, in particular environmental phenomena, such as
ﬂooding, groundwater recharge or reactive transport using
innovative numerical methods. Such simulations are crucial
for solving major challenges in coming years, including the
prediction of possible effects of climate change [1] [2], the
development of water management schemes for (semi) arid re-
gions [3] [4] or the reduction of groundwater contamination [5]
[6].
The modeling process is usually a complete workﬂow,
starting with data acquisition and integration to set up a model,
simulate (multiple) processes, and as well as the analysis, and
visualization of calculated results. Unfortunately, the modeling
process itself in general is not transparent and traceable and
often poorly documented. A typical model – conceived as
a set of parameter ﬁles – is developed over many weeks or
months and usually a large number of revisions are necessary
for updating and reﬁning the model, such that the simulation
represents the natural process as realistically and plausibly as
possible.
In this paper, we address this challenge, speciﬁcally,
we present a revision control system, which in addition to
the backup/restore functionality tracks the changes in each
18
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-590-6
ICSEA 2017 : The Twelfth International Conference on Software Engineering Advances

modeling step, thus generating an internal documentation of
the evolution of the model.
The structure of the paper is as follows: In Section II, we de-
ﬁne the environment and set up the terminology; in Section III,
we give a short overview over the state of art and detail some
differences of our approach, both in concept and realization;
in Section IV, we demonstrate our novel strategy, which is
used for the revision control system to generate the implicit
documentation of the evolution of the model; in Section V,
we augment the classical pseudo code presentation of the
algorithms to a formal, mathematical description of our selective
backup strategy and show the consistency and correctness of
the backup and restore functionalities. We present the software
implementing the formal description and its application to a
use case of the UFZ in Section VI. Finally, we conclude our
work and give an outlook for future research and development
in Section VII.
II.
MAIN CHALLENGES AND OBJECTIVES
The basic concept for the simulation is the model. It
is developed over several modeling steps, named revisions,
compare Figure 1. After each revision, a simulation is performed
(see Figure 2). The ﬁrst setup of the model is often used to get
an overview over existing data and to detect potential problems.
Further revisions try to solve these problems by adding data,
mesh reﬁnements, new parametrizations, etc.
The framework for revision control for scientiﬁc appli-
cations is being implemented at the Helmholz-Centre for
Environmental Research (UFZ) [7] using Karlsruhe Institute
of Technology Data Manager (KIT DM) [8] as a software
framework for building up repositories for research data.
The Metadata Management for Applied Sciences (MASi) [9]
research data management service is currently being prepared
for production at the Center for Information Services and
High Performance Computing (ZIH) at Technische Universität
Dresden. It utilizes the advanced KIT DM framework to
enable a service that enables the metadata-driven management
of research data from arbitrary communities. This includes
automating as many processes as possible including metadata
generation and data pre-processing.
The current solution, utilized at UFZ, is fully ﬁle based and
it is usually stored locally on the laptop of each scientist. The
Figure 1. Model development over revisions
number of parameter ﬁles is up to several hundreds, with each
ﬁle up to several megabytes. The changes from one modeling
step to the next can be a) minor, e.g., one parameter value
Figure 2. Revision and simulation as part of the model development process
changed in a single input ﬁle; b) major, e.g., both geometry
and the grid are modiﬁed.
The main deﬁciencies of the current solution at UFZ are:
1)
Overview is lost (especially after handling the model
for a long time),
2)
difﬁcult to trace which parameter has been changed
when and why,
3)
no implicit or explicit documentation of the changes,
4)
each user stores the data on his laptop at his own
discretion,
5)
data is lost if hard disk crashes and there is no backup,
6)
joint working on the same model is laborious.
The beneﬁts of the new framework will include those of a
classical revision control system (like Git [10], or Apache
Subversion [11]), especially:
1)
Uniform, central, and consistent storage of the individual
modeling steps a) each scientist will be able to view the
simulation data he is entitled to b) backup functionality
if the data is lost,
2)
possibility to track and analyze / evaluate the changes,
3)
data is still available if the PhD student leaves the
company,
4)
shared access of the latest development of the model.
We deﬁne by a revision the state of the components already
persisted and accessible by a unique identiﬁer. Thus, the content
of the components of a revision cannot be altered any more. The
current set of the components, which can be actively changed
is called the working set.
The main objectives we focus on, to achieve our scope, are:
1)
Central persistent storage of the model to include all the
modeling steps and the management of the revisions.
2)
Design and development of a metadata repository
regarding a) revision control and b) the changes of the
parameter ﬁles between subsequent revisions. Addition-
ally, information regarding parameter values, simulation
software, etc. can be persisted.
19
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-590-6
ICSEA 2017 : The Twelfth International Conference on Software Engineering Advances

3)
An efﬁcient and disk space saving strategy, such that a
speciﬁc parameter ﬁle is stored only if its content has
been modiﬁed.
4)
Generation of an internal documentation of the model
development, such that it can be easily understood and
reconstructed.
It is out of scope of this research to persistently store the
results of the simulation. If necessary, it can be generated
again or a direct storage strategy could be used. Storing the
results of the simulation together with the parameter ﬁles would
leverage our sophisticated storage strategy, since the size of
the parameter ﬁles is in the range of megabytes, where the size
of the simulation ﬁles is in the range of gigabytes. The storage
of the simulation results is only meaningful if it takes too long
to newly generate the results.
Figure 3. Tree structure of the development of the model
The model development is stored in a tree structure, such
that each node (revision) has a unique link to its predecessor,
see Figure 3. The tree structure is necessary to be able to
identify modeling steps where the results of the simulation are
not promising, and thus this revision is not pursued further
(termed abandoned). In this case, the development of the model
is continued from a previous revision (termed active), thus
performing a rollback on the evolution of the model and creating
a new branch in the version control tree structure.
Usually, metadata is deﬁned as data about data. Metadata
ﬁles can be generated automatically or they can be set up
manually. The ﬂow conﬁguration ﬁle is the main metadata ﬁle
and it is generated automatically during the evolution process
of the model and contains the basic information regarding the
revision control system, which is necessary to generate the
internal documentation of the evolution of the model, i.e.,
a)
model name;
b)
predecessors and the current revision;
c)
cryptographic hash value and status of the parameter
ﬁles;
d)
parameter change information in condensed form, etc.
The main metadata ﬁle sustains the possibility to automatically
capture, track, analyze, and evaluate the changes in each
modeling step.
Additionally, users can deﬁne their own metadata ﬁles,
which can be created for the whole case study or for a speciﬁc
revision and could contain additional information regarding
a) name of the project; b) model area; c) modeled process;
d) software used, including the version; e) contact person; f)
source reference regarding the applied methods and data used;
g) utilization rights, etc.
Besides documentation, metadata also allows for easy
identiﬁcation of the uploaded data. Search for speciﬁc values
(e.g., model name, author, etc.) over all metadata elements can
be performed for example by using ElasticSearch [12].
III.
RELATED WORK
The concept of revision control systems (RCS) is not new
(see Tichy [13]). The task of the RCS as deﬁned by Tichy is
version control, i.e., keeping software systems consisting of
many versions and conﬁgurations well organized. The concept
of a revision is similar to our approach, an ancestral tree is
used for storing revisions. The major difference is that – as
set up by Tichy – each object (like a ﬁle) has his own revision
tree, whereas we follow an overarching concept, such that ﬁles
may remain unchanged between revisions. Furthermore, the
evolvement of the revision is linear, but it can use side branches,
for example one for the productive version and one for the
development [13].
Löh et al. [14] present a formal model to reason about
version control, in particular modeling repositories as a multiset
of patches. Patches abstract over the data on which they operate,
making the framework equally suited for version control from
highly-structured XML to blobs of bits. The mathematical
deﬁnition of patches and repositories enable Löh et al. to
reason about complicated issues, such as conﬂicts and conﬂicts
resolution. The main application ﬁeld that Löh et al. targets
is the distributed (software) development with its challenges
regarding the complex operations on the repositories, such
as merging branches or resolving conﬂicts. They introduce a
precise, mathematical description of the version control system
to accurately predict when conﬂicts may arise and how they
may be resolved.
Our mathematical model is not based on the work of Löh et al.,
it has been developed from scratch to enable the characterization
of the selective backup strategy.
The possibility to use metadata, such as the patch’s author,
time of creation, or some form of documentation is shortly
discussed in [14]. Details are left to the designers of a speciﬁc
revision control system. Also the concept of reverting changes,
i.e., the ability to return to a previous version by undoing a
modiﬁcation that later turns out to be undesired, is discussed
from a theoretical point of view.
As stated in [15] there are some basic goals of a versioning
system, such that:
1)
People are able to work simultaneously, not serially.
2)
When people are working at the same time, their
changes do not conﬂict with each other.
These two goals do not apply in our case. Formally, users can
work simultaneously, making changes independently, but for a
simulation they need all the parameter ﬁles. The classical use
case, such that a programmer changes the internal speciﬁcation
of a module without changing the external interface is not
applicable in our case, each change in a parameter ﬁle leads to
different simulation results. Unfortunately, the usual versioning
systems do not support our advanced requirements regarding
20
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-590-6
ICSEA 2017 : The Twelfth International Conference on Software Engineering Advances

usage of metadata and enhanced automatic documentation
generation of the evolution of the model.
The automatic generation of documentation has also been
the scope of intense academic and industrial research. It has
been recognized that the importance of good documentation
is critical for user acceptance [16]. Jesus describes in [17] a
paradigm for automatic documentation generation based on
a set of rules that, applied to the models obtained as result
of the analysis and design phases, gives an hypertext network
describing those models. On the contrary, our approach has the
advantage that the algorithm that is used for the selective backup
strategy also delivers the data for the automatic documentation.
PLANDoc [18] documents the activity – of planning engineers
– by taking as input a trace of the engineer’s interaction
with a network planning tool. Similarly, in [19] Alida, an
approach for fully automatic documentation of data analysis
procedures, is presented. During analysis, all operations on data
are registered. Subsequently, these data are made explicit in
XML graph representation, yielding a suitable base for visual
and analytic inspection. The high level approach in Alida –
using the information generated during the production process
to automatically create the documentation – is similar to ours.
IV.
SELECTIVE BACKUP STRATEGY
The aim of our revision control system is to provide an
enhanced backup strategy, termed selective backup strategy,
such that only the components of the working set that have
been modiﬁed are considered for backup, see Figure 4. This is
an enhancement of the usual incremental backup strategies –
such that a particular modiﬁcation of a ﬁle is stored only once
– in order to provide the framework to generate the metadata
regarding the modiﬁcations and accordingly to generate the
implicit documentation of the model.
Figure 4. Selective backup strategy. Uploaded ﬁles at ﬁrst and second revision.
A correspondent selective restore strategy is used, i.e., the
latest versions of all components are downloaded, such that at
the end the recent version of the model is assembled out of
the historical backups.
A. Backup
Only part of the current working set is uploaded into the
data repository, and the uploaded information cannot be altered
or removed later. According to our selective backup strategy a)
for the ﬁrst revision: all components are uploaded (see Figure 4
left side); b) for the subsequent revisions: only the components
that have been modiﬁed are uploaded (see Figure 4 right side).
B. Restore
It will be possible to download all relevant information
regarding a speciﬁc revision (including parameter ﬁles and
metadata ﬁles). This requires identifying and downloading
the full set of components necessary to run a simulation.
The required information is stored in the main metadata ﬁle
during the backup process. Hence, the main metadata ﬁle
stores information regarding all ﬁles that have been uploaded
including the unique identiﬁcation of the uploaded object and
the cryptographic hash values of the respective ﬁles.
1) Full Restore: The full restore should be applied if ﬁles
have been lost, or the development of the model is intended
to be pursued by other users, etc. The full restore retrieves
the whole set of parameter ﬁles, such that a simulation can be
done on the restored system.
2) Revision Restore: This functionality restores the ﬁles
corresponding to a (previous) revision and permits to continue
the simulation corresponding from that revision. This method
enables the tree structure of the revision history. The corre-
sponding information is retrieved from/written to the main
metadata ﬁle.
C. Flow Conﬁguration
We present now some implementation details. The relevant
information for the functioning of the selective backup and the
corresponding restore strategy is stored / updated automatically
– using XML – in the ﬂow conﬁguration ﬁle.
This ﬁle stores general information as: a) short name of
the model; b) the number of the last revision; c) the object id
under which the ﬁles belonging to that revision were uploaded;
d) additional information in order to identify the project,
the revision, etc. A simple example is given in Figure 5.
Additionally, general information regarding the revision history
1 <GeneralConfiguration>
2
<ModelShortName>cube_1e0_neumann</ModelShortName>
3
<LastRevisionNr>25</LastRevisionNr>
4
<LastDigitalObjectID>3ccafed6-cf0d-486d-bbe3-
edff4159f6c5</LastDigitalObjectID>
5
<LastNotes>cube_1e0_neumann / Incr. / It.Nr.: 25 /
Standard Incremental Upload</LastNotes>
6 </GeneralConfiguration>
Figure 5. Excerpt 1 of example conﬁguration ﬁle
such as: a) the revision number; b) the object id of the uploaded
object; c) the predecessor (parent revision); the total number
of ﬁles versus the number of ﬁles, which have been changed
and in consequence uploaded, etc., as given in Figure 6. File
and revision speciﬁc information are also tracked, such that
for each ﬁle the revision where the ﬁle has been changed and
the corresponding cryptographic values are tracked. This way,
it is ensured that a speciﬁc state of a ﬁle is stored only once
and that the revision under which this ﬁle has been stored can
unambiguously be identiﬁed, see Figure 7 for an example.
21
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-590-6
ICSEA 2017 : The Twelfth International Conference on Software Engineering Advances

1 <Revision>
2 <RevisionNr>7</RevisionNr>
3 <DigitalObjectID>8ce20b42-c15f-427c-ac1a-d575295f5412</
DigitalObjectID>
4 <ParentRevisionNr>3</ParentRevisionNr>
5 <TotalNrFiles>20</TotalNrFiles>
6 <NrFilesUploaded>1</NrFilesUploaded>
7 <Notes>cube_1e0_neumann / Incr. / It.Nr.: 2 / For Testing
Upload and Download</Notes>
8 </Revision>
Figure 6. Excerpt 2 of example conﬁguration ﬁle
1 <FileCharacteristics>
2
<FileName>cube_1x1x1.gml</FileName>
3
<FileLastRevision>43</FileLastRevision>
4
<FileHistory>
5
<FileRevision>
6
<StorageRevisionNr>1</StorageRevisionNr>
7
<StorageDigitalObjectID>8aac5970-a366-49ce-a052
-6c8f82ced85c</Storage\-Digital\-ObjectID>
8
<FileSize>1622</FileSize>
9
<FileCreationTime>2016-08-18T08:25:33Z</
FileCreationTime>
10
<FileLastAccessTime>2016-08-23T15:59:55Z</
FileLastAccessTime>
11
<FileLastModifiedTime>2016-08-18T08:25:33Z</
FileLastModifiedTime>
12
<FileCryptoIDMD5>66
a9800e8b02d85001cdd13930b85ea3</
FileCryptoIDMD5>
13
<FileCryptoIDSHA1>5
c942ba146b41e38262f23ed350e214c757d8803</
FileCrypto\-IDSHA1>
14
</FileRevision>
Figure 7. Excerpt 3 of example conﬁguration ﬁle
D. Accurate versioning
Based on the architecture of they system, the selective
backup strategy corresponds to a centralized revision control
system, i.e., there is a central revision number – in our case
the modeling step –, such that the version of each ﬁle is tied
to this central revision number. In contrast, revision control
systems like Git [10] are decentralized, i.e., generally, users
maintain the versioning of their part, without affecting the
overall release number. In our case, this centralized approach
is of crucial importance, since small changes in one parameter
ﬁle can substantially affect the outcome of the simulation. The
selective backup strategy enables a paradigm change in the
theory and practice of (centralized) revision control systems, it
enables an accurate tracking of the changes during each revision
on ﬁle level including the identiﬁcation of the effective version
of each ﬁle. This means especially that in contrast to Git and
SVN [11], during revision change, each ﬁle is compared to
previous versions and either assigned a new version number or –
if possible – reassigned a previous one. Such a distinction is not
absolutely necessary, for example during software development
(main application ﬁeld for Git and SVN), but it is of crucial
importance for pursuing the exact model development.
We illustrate now the selective backup strategy by means
of the example as delineated in Table I. Let {F1,F2,F3,...,Fn}
be ﬁles comprising a numerical model and {R1,R2,R3,...,Rm}
the revisions to adjust the model for a successful simulation of
a process. According to the architecture we use, the number of
revisions is greater than the number of the ﬁles involved, i.e.,
n < m. We number the versions of a speciﬁc ﬁle continuously
R1
R2
R3
R4
R5
R6
. . .
Rm
F1
V
R1
1
V
R2
2
V
R3
3
V
R3
3
V
R3
3
V
R6
4
. . .
V
Rmk1
m1
F2
V
R1
1
V
R2
2
V
R3
3
V
R4
4
V
R2
2
V
R6
5
. . .
V
Rmk2
m2
F3
V
R1
1
V
R1
1
V
R3
2
V
R3
3
V
R1
1
V
R6
4
. . .
V
Rmk3
m3
...
...
...
...
...
...
...
...
Fn
V
R1
1
V
R1
1
V
R1
1
V
R1
1
V
R5
2
V
R5
2
. . .
V
Rmkn
mn
Table I. Example for the selective backup strategy.
in the order they were generated, starting with 1. We notate
by an upper index the revision during which the version was
generated, i.e., for the ﬁle F1 the version V R6
4
represents the
fourth version generated during revision R6. Not all ﬁles are
necessarily updated with a revision. For instance, ﬁle F1 is
unchanged during revisions R4, R5, thus the model keeps using
the ﬁle version V R3
3 . In contrast, ﬁle F2 is modiﬁed in each
revision. However, during revision R5, the ﬁle is reverted to a
previous state such that its version is equal to V R2
2
and, instead
of storing a duplicate, the previous copy of the ﬁle is used.
Using revision systems like Git or SVN, a new version of
the ﬁle F2 would be created. Instead, our algorithm, using the
selective backup strategy, veriﬁes if a version with new content
has been created or the respective content has already been
used before.
The use of the selective backup strategy is not restricted to
the model development for scientiﬁc application, but can be
applied everywhere where a centralized revision control system
is used. Its intended target are applications, which need to track
the effective version of the ﬁles, potentially related to revision
numbers.
V.
THE FORMAL MODEL
We introduce a mathematical model in order to use the
advantages of the rigor of a formal approach over the inaccuracy
and the incompleteness of natural languages. We augment
the classical pseudo code presentation of the algorithms to a
formal, mathematical description and show the consistency and
correctness of the backup and restore functionalities.
A. Notations
We use a calligraphic font to denote the index sets. We
denote by C := {Ci | i ∈ C and Ci is a component} the ﬁnite
set of the components, i.e., the disjunct union of the parameter
ﬁles and the metadata ﬁles. Let S be an arbitrary set. We
notate by P(S) the power set of S, i.e., the set of all subsets
of S, including the empty set and S itself. By card(S) we
notate the cardinality of S. Let n ∈ N and let f : X → X be
a function. Finally, we denote by f n : X → X the function
obtained by composing f with itself n times, i.e., f 0 := idX
and f n+1 := f n ◦ f.
B. Introducing Components and Revisions
Some components – at least one, but not necessary all
– are modiﬁed, then a simulation is performed. We call this
state of the components a revision. Each revision is backed
up to a persistent storage. We have in a natural way a total
22
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-590-6
ICSEA 2017 : The Twelfth International Conference on Software Engineering Advances

ordering < on the set of the revisions considering the order
they were generated. We denote by R the ordered set of the
revisions, i.e., R := {Ri | i ∈ R and Ri is a revision}. Let m :=
card(R) be the number of revisions. In order to keep the
notations straightforward we set R := {1,2,3,...,m}, such that
∀i ∈ R \ {m} : Ri < Ri+1. We denote by C(i)
k
the component
Ck having the state at revision Ri, therefore, we denote by
Ri := {C(i)
k | k ∈ C } the set of the components having the state
corresponding to revision Ri.
We denote by R the matrix of the evolution of the model,
hence R := {C(i)
k | k ∈ C and i ∈ R}. Therefore, R contains
the history of the content changes of the components during
the evolution process of the model.
Let HASH be the set of all the hash values. We deﬁne the
content of a component Ck ∈ C corresponding to a revision Ri
formally as the function:
Deﬁnition V.1 (Content of components) We set
CONT: R → HASH,
C(i)
k 7→ CONT(C(i)
k ) := hash value of C(i)
k .
Remark V.1 Let k ∈ C , let i, j ∈ R, such that i ̸= j. In order
to track the change process during the evolution process of the
model, we are interested only in comparing the content of the
same component at different revisions (i.e., the content of C(i)
k
versus the content of C(j)
k ).
■
Deﬁnition V.2 (Origin of a revision) Let R,Q ∈ R. We say
that Q is the origin of R, notated by Q = ORIGIN(R), if and
only if the revision R has been obtained by direct modiﬁcation
of the content of the components having the state at revision
Q. For formal reasons we deﬁne ORIGIN(R1) := R1.
Remark V.2 Let R ∈ R arbitrarily chosen. Then there exists
a unique Q ∈ R, such that Q = ORIGIN(R). This is a direct
consequence of the deﬁnition above (see Deﬁnition V.2).
■
Let m = card(R), such that m ≥ 1. We deﬁne the predecessor
and the successor of a revision formally as:
Deﬁnition V.3 (Predecessor of a revision) We set
PRED: R → R,
R 7→ PRED(R) := ORIGIN(R).
Deﬁnition V.4 (Successor of a revision) We set
SUCC: R → P(R),
R 7→ SUCC(R) := {Q ∈ R | R = ORIGIN(Q)}.
Remark V.3 ∀R ∈ R ⇒ PRED(R) is unequivocally deter-
mined (see Remark V.2), in contrast, there exists to a revision
R ∈ R a subset J of R, such that SUCC(R) = {R j | j ∈ J}. ■
Unfortunately, the structure of the evolution of the model is
not linear. If for any reason the evolution of the model is in
impasse, then the development of the model is not continued
from the latest revision, but a previous revision is taken as a
starting point. The revision, which led to the impasse is not
pursued any more (i.e., it is abandoned). On the other side,
a revision is active if it is part of the successful completion
of the model. Formally, we deﬁne the status of a revision as
follows:
Deﬁnition V.5 (Status of a revision) Let m := card(R) the
number of revisions. We set
STATUS: R → {active,abandoned},
R 7→ STATUS(R) :=



active
if ∃n ≥ 0 :
R = PREDn(Rm),
abandoned
otherwise.
Informally, the predecessor of a component C(i)
k
is the com-
ponent C(j)
k , such that Rj was the latest revision where the
component Ck has been changed. Formally, we model the
successor and predecessor of a component Ck during the
revision process as a function.
Let i ∈ R and k ∈ C arbitrarily chosen. Set A(i,k) :=
max{l ∈ R : l < i and CONT(C(i)
k ) ̸= CONT(C(l)
k )} then
Deﬁnition V.6 (Predecessor of a component) We set
PRED: R → R,
C(i)
k 7→ PRED(C(i)
k ) :=









C(i)
k
if (i = 1),
C(A(i,k))
k
if (i > 1)
and A(i,k) exists,
C(1)
k
otherwise.
Deﬁnition V.7 (Successor of a component) We set
SUCC: R → P(R),
C(i)
k 7→ SUCC(C(i)
k ) := {C(j)
k
∈ R | C(i)
k = PRED(C(j)
k )}.
Remark V.4 The predecessor of C(i)
k
is uniquely determined.
This follows directly from the deﬁnition above. In contrast, the
successor of C(i)
k
is not necessary unique, but there exists a
unique Rj ∈ R, such that C(j)
k
∈ SUCC(R(i)
k ) and STATUS(Rj)
is active. Similar considerations also hold for revisions.
■
Proposition V.1 (Existence and uniqueness) Let
R ∈ R,
such that STATUS(R) = active. If SUCC(R) ̸= ∅ then
there exists a unique Q ∈ R, such that Q ∈ SUCC(R) and
STATUS(Q) = active.
Hint The existence and the uniqueness follows directly from
the deﬁnition of the status of a revision (see Deﬁnition V.5)
and the uniqueness of the predecessor (see Remark V.3).
■
We are now able to formulate our strategy to generate the
successive revisions.
Lemma V.1 (Linearity) Let m = card(R). Then there exists a
unique subset R
′ of R with R
′ ={R1,Ri1,Ri2,Ri3,...,Ril,Rm},
such that Ri1 ∈ SUCC(R1) and ∀ik : i1 ≤ ik < il ⇒ Ri(k+1) ∈
SUCC(Rik) and Rm ∈ SUCC(Ril) and ∀R ∈ R
′ : STATUS(R) =
active and ∀R ∈ R \R′ : STATUS(R) = abandoned.
Hint It is a direct consequence of the uniqueness of active
successors (see Proposition V.1).
■
Corollary V.1 The sequence of the active revisions is linear.
■
23
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-590-6
ICSEA 2017 : The Twelfth International Conference on Software Engineering Advances

Let i ∈ R arbitrarily chosen, a component can have at the
revision Ri two statuses modiﬁed and preserved, the value
modiﬁed means that the component has been modiﬁed during
the revision Ri, in contrast preserved means that the component
remained unchanged at revision Ri. More formally, we deﬁne
the function:
Deﬁnition V.8 (Status of a component) We set
STATUS : R → {modiﬁed, preserved},
C(i)
k 7→ STATUS(C(i)
k ) :=



modiﬁed
if (i = 1),
modiﬁed
if condition 2 holds,
preserved
otherwise.
with condition 2: ∀ j ∈ R with 1 ≤ j < i : CONT(C(i)
k ) ̸=
CONT(C(j)
k ).
Remark V.5 From a formal point of view, all components
corresponding to the ﬁrst revision are considered modiﬁed. For
the subsequent revisions only the components whose content
has been altered are considered modiﬁed.
■
C. Backing up a revision
Based on the values of the STATUS function, we deﬁne
the upload strategy. We are interested to upload only those
components, which have been modiﬁed since the latest revision
and the current state has not been uploaded previously.
Deﬁnition V.9 (Upload of a component) We set
UPLOAD: R → {yes, no},
C(i)
k 7→ UPLOAD(C(i)
k ) :=
yes if condition 1 holds,
no otherwise.
with condition 1: STATUS(C(i)
k ) = modiﬁed.
Remark V.6 This means especially that C(i)
k
will be uploaded
if and only if it has been modiﬁed and its content is different
from the content of all its predecessors.
■
We will deﬁne now a function in order to model the backup
process of an entire revision. As we will see, only those
components will be backed up at a speciﬁc revision Ri, which
have been modiﬁed at this revision.
Deﬁnition V.10 (Backup of a revision) We set
BACKUP: R → R,
Ri 7→ BACKUP(Ri)
:= {C(i)
k | k ∈ C , such that UPLOAD(C(i)
k ) = yes}.
Remark V.7 This means especially that the components that
have not been changed at revision Ri are not included in the
backup of the revision Ri, this is the quintessence of the selective
backup strategy.
■
In order to be able to model the download and restore
process, we need to do some additional analysis. Those opposite
functions cannot be deﬁned straightforwardly as the reverse
function of BACKUP and UPLOAD, since after the restore is
fulﬁlled, all the relevant components must be available, not
only those persisted at the corresponding revision.
In order to have all the relevant information for the
restore process, we build during the evolution of the model
a matrix (INF(i)
k )k∈C,i∈R, such that this matrix contains the
information relevant for the download and restore operations.
This information contains the content of the components, such
that comparisons can be done and relate it to the previous
backups.
Hence, the matrix (INF(i)
k )k∈C,i∈R contains at least the
information regarding the revision at which the component was
physically stored, such that it can be retrieved from there and
additional information regarding the content (hash value) of
the components.
Formally, we deﬁne (INF(i)
k )k∈C,i∈R as a function:
Deﬁnition V.11 (Component upload meta inf) We set
INF: R ×C → R ×HASH,
(i,k) 7→ INF(i,k) := ( j,v)
if

C(i)
k ∈ BACKUP(Rj) and CONT(C(i)
k ) = v

.
Remark V.8 This means especially that a component C(i)
k
having the hash value = v has been backed up at revision Rj
and j ∈ R is the lowest index number, such that CONT(C(i)
k ) =
CONT(C( j)
k ).
■
D. Restoring a revision
We deﬁne now the opposite function to UPLOAD as follows:
Deﬁnition V.12 (Download of a component) We set
DOWNLOAD: R → R,
C(i)
k 7→ DOWNLOAD(C(i)
k ) := C(j)
k
if (UPLOAD(C(j)
k ) = yes
and CONT(C(j)
k ) = CONT(C(i)
k )).
Remark V.9 DOWNLOAD(C(i)
k ) = C(j)
k
means especially that
the component Ck was uploaded at the revision R j.
■
We deﬁne the restore function having the opposite functionality
to the backup function. The main difference to the usual
restore strategy is that restoring the components backed up
at revision Ri is not enough, since usually only a subset of the
components are backed up at a speciﬁc revision. To circumvent
this impediment, the revisions at which those components have
been physically uploaded are identiﬁed and are restored from
those locations. When the restore operation is completed, then,
the complete set of components necessary for a simulation is
available.
Formally as a function:
Deﬁnition V.13 (Restore of a revision) We set
RESTORE: R → P(R),
Ri 7→ RESTORE(Ri) :=
{C(j)
k
| k ∈ C , such that DOWNLOAD(C(i)
k ) = C(j)
k }.
Remark V.10 This means especially that the latest version of
the components are restored.
■
24
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-590-6
ICSEA 2017 : The Twelfth International Conference on Software Engineering Advances

We are now able to formulate the Lemma regarding the
uniqueness of the upload, i.e., a new state of a component
Ck during the revision process is backed up only once.
Lemma V.2 (Uniqueness of the upload) Let k ∈ C and v ∈
HASH be arbitrarily chosen. If ∃j ∈ R : CONT(C(j)
k ) = v then
there exists a unique i ∈ R, such that UPLOAD(C(i)
k ) = yes
and CONT(C(i)
k ) = v.
Hint Set i := min{l ∈ R | CONT(C(l)
k ) = v}. Then according to
the deﬁnition of the status of a component (see Deﬁnition V.8)
STATUS(C(l)
k ) = modiﬁed holds true. The result follows from the
deﬁnition of the upload of the components (see Deﬁnition V.9).
■
We can formulate now the main Lemma, which states that we
have no spurious downloads.
Lemma V.3 (Accuracy and completeness) Let k ∈ C be ar-
bitrarily chosen. We have:
a)∀i, j ∈ R : DOWNLOAD(C(i)
k ) = C(j)
k
⇒

When the primitive Upload is called for a project ﬁle for
the ﬁrst time, the corresponding dynamic ﬂow conﬁguration
ﬁle is initialized. For an example of a ﬂow conﬁguration
ﬁle, see the excerpts in Figures 5–7. The revision number
is set to one and the cryptographic MD5 and SHA-1 hashes
of each ﬁle are calculated and stored in the dynamic ﬂow
conﬁguration ﬁle. While SHA-1 is practically collision free and
it is also used by Git for integrity purposes [20], alternatively,
SHA-512 could be used for enhanced security [21] [22]. For
subsequent uses of the Upload-primitive, the cryptographic
values of the parameter and metadata ﬁles are compared to
the respective values stored – for previous revisions – in the
ﬂow conﬁguration ﬁle. If the cryptographic values of ﬁle F is
different of all the previous cryptographical values of ﬁle F,
then the content of F is considered modiﬁed and it is backed
up within the current revision. Otherwise, F is not part of the
current revision. A corresponding entry is made in the dynamic
ﬂow conﬁguration ﬁle regarding the revision under which the
ﬁle – having the given content – has been backed up. Hence, the
ﬁle cube_1x1x1.gml has the cryptographic values stored
at revision 1 (see Figure 7). If the ﬁle cube_1x1x1.gml
has, for example, not been altered at revision 2 then no similar
entry is performed in the dynamic ﬂow conﬁguration ﬁle.
When starting the primitive Download – i.e., downloading
the ﬁles corresponding to the latest revision – then the
relevant information regarding the physical storage place of
the latest version of each ﬁle – i.e., <StorageDigitalObjectID>
– is retrieved from the dynamic ﬂow conﬁguration ﬁle, by
considering the latest entry for each ﬁle (see Figure 7). Hence,
all the related ﬁles can be accessed on the repository and
retrieved accordingly.
The use case at UFZ is not designed for concurrent use,
where users are confronted with conﬂicts and their resolution,
as it is the case for the software development environment. In
contrast, at UFZ simultaneous work is strictly related to the
model development process. The model is developed in steps,
small changes in a few parameter ﬁles can have tremendous
impact on the model. Hence, members of a team can download
the latest revision, can work simultaneously improving the next
step, can compare the changes of the parameter ﬁles and the
simulation results, but eventually, the members of the team have
to agree on the best outcome for the next step, thus uploading it
as the next revision. Alternatively, they can agree on abandoning
the current revision by continuing the model development from
an older revision. The team members have to download the
revision, they agreed upon, and continue developing the model
from there.
In addition to the dynamic ﬂow conﬁguration ﬁle – upon
whose content they do not have direct inﬂuence – users can
deﬁne and set up their own metadata ﬁles. These metadata
ﬁles can contain additional – high-level or aggregated –
information regarding the model development and can be
used for additional documentation or for identifying model
or revision characteristics. The metadata ﬁles are also very
important to enable the differentiated security policy at UFZ,
such that users can access the metadata ﬁles – for example by
using ElasticSearch [12] – if they have the appropriate rights
on the ﬁle system. In contrast, users can access simulation
data (parameter ﬁles) according to their rights on the repository
system KIT DM. Thus, metadata for projects is accessible for
all members within the UFZ, while sensible data can only be
accessed by a small number of researchers related to the project.
All ﬁles of the examples can be found at [23].
VII.
CONCLUSION AND FUTURE WORK
Irrespective of the fact that creating documentation is
a very challenging task and that writing documentation is
considered by most of the developers as an extra-effort rather
than a commendation, it is rather impossible providing precise,
exact, accurate, uniform and consistent documentation for
developers and users requirements. Therefore, formalized
automated documentation methods are necessary to develop.
The main advantage of the automatic generation of the
documentation of the modeling process is the accuracy of
the documentation, since there is no discrepancy between the
actual generation of the model (developer’s perspective) and
the corresponding documentation (user’s perspective). This way,
we have circumvented the dilemma of writing exact manual
documentation and have contributed to the paradigm change
towards design and implementation of automatic documentation
assuring accuracy and exactness.
Since our formal model is independent of the use case at
UFZ, our approach has a generic character and it can be applied
to all domains where numerical models are developed.
Currently, there is little comfort for the user who employs
the framework. In the current implementation, the executable
is started in the command line, also the ﬂow conﬁguration ﬁle,
which contains the documentation for tracking the evolution
of the model is in XML-format. Accordingly, additional
research is necessary to deﬁne and implement corresponding
GUI to assure the expected readability of the document by
extracting and visualizing the appropriate information. In order
to build meaningful user interfaces, an intense dialog between
developers and users is essential [24].
Additionally, further research is necessary to generate a high
level form documentation of the changes in the parameter ﬁles.
For example, when running stochastic simulations (e.g. Monte
Carlo approach [25]) if parameters are changed in many places,
then appropriate mechanism should be set up assuring the
consistency of the changes and the appropriate documentation.
We think that by studying the automatically generated
documentation regarding the development of the modeling
workﬂows – especially those steps, which did not lead to a
successful completion of the simulation – there is an increased
possibility of knowledge extraction (by using machine learning
strategies or similar techniques), such that the generation of
the modeling workﬂows can be dramatically improved and the
number of modeling steps can be considerably reduced.
ACKNOWLEDGMENT
This work was supported in parts by the German Federal
Ministry of Education and Research (BMBF, 01IS14014A-
D) by funding the competence center for Big Data “ScaDS
Dresden/Leipzig”. This work was also supported in parts by
the German Research Foundation (DFG) via the MASi project
(NA 711/9-1, STO 397/4-1). We are also thankful to Dr. Nico
Hoffmann (Technische Universität Dresden) for his valuable
advices and comments during the developing and writing
process.
26
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-590-6
ICSEA 2017 : The Twelfth International Conference on Software Engineering Advances

REFERENCES
[1]
Intergovernmental Panel on Climate Change, Climate Change 2014 –
Impacts, Adaptation and Vulnerability: Regional Aspects.
Cambridge
University Press, 2014.
[2]
C. J. Vörösmarty et al., “Global threats to human water security and
river biodiversity,” Nature, vol. 467, 2010, pp. 555–561.
[3]
J. Grundmann, N. Schütze, G. H. Schmitz, and S. Al-Shaqsi, “Towards
an integrated arid zone water management using simulation-based
optimisation,” Environ Earth Sci, vol. 65, no. 5, 2012, pp. 1381–1394.
[4]
H. Hötzl, P. Möller, and E. Rosenthal, The Water of the Jordan Valley.
Springer, 2009.
[5]
M. Walther, J.-O. Delfs, J. Grundmann, O. Kolditz, and R. Liedl,
“Saltwater intrusion modeling: Veriﬁcation and application to an
agricultural coastal arid region in Oman,” Journal of Computational
and Applied Mathematics, vol. 236, no. 18, 2012, pp. 4798–4809,
fEMTEC 2011: 3rd International Conference on Computational Methods
in Engineering and Science, May 9–13, 2011. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0377042712000659
[6]
C. Liu, Q. Wang, C. Zou, Y. Hayashi, and T. Yasunari, “Recent trends
in nitrogen ﬂows with urbanization in the shanghai megacity and the
effects on the water environment,” Environmental Science and Pollution
Research, vol. 22, no. 5, Mar 2015, pp. 3431–3440. [Online]. Available:
https://doi.org/10.1007/s11356-014-3825-4
[7]
Helmholtz Centre for Environmental Research – UFZ, “Homepage of
Helmholtz Centre for Environmental Research,” https://www.ufz.de/
index.php?en=34216, retrieved: July 2017.
[8]
Karlsruhe Institute of Technology – KIT, “KIT Data Manager,” http:
//datamanager.kit.edu/index.php/kit-data-manager, retrieved: July 2017.
[9]
R. Grunzke et al., “Towards a metadata-driven multi-community research
data management service,” PeerJ PrePrints, vol. 5, 2017, p. e2831.
[Online]. Available: https://doi.org/10.7287/peerj.preprints.2831v1
[10]
S. Chacon and B. Straub, Git and Other Systems.
Berkeley, CA:
Apress, 2014, pp. 307–356. [Online]. Available: http://dx.doi.org/10.
1007/978-1-4842-0076-6_9
[11]
C. M. Pilato, B. Collins-Sussman, and B. W. Fitzpatrick, Version
control with subversion - the standard in open source version
control.
O’Reilly, 2008, retrieved: July 2017. [Online]. Available:
http://www.oreilly.de/catalog/9780596510336/index.html
[12]
C. Gormley and Z. Tong, Elasticsearch: The Deﬁnitive Guide, 1st ed.
O’Reilly Media, Inc., 2015.
[13]
W. F. Tichy, “Rcs – a system for version control,” Software: Practice
and Experience, vol. 15, no. 7, 1985, pp. 637–654. [Online]. Available:
http://dx.doi.org/10.1002/spe.4380150703
[14]
A. Löh, W. Swierstra, and D. Leijen, “A Principled Approach to Version
Control,” http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.
8649, retrieved: July 2017.
[15]
E. Sink, Version Control by Example, 1st ed.
PYOW Sports Marketing,
2011.
[16]
C. L. Paris, Automatic documentation generation: Including examples.
Berlin, Heidelberg: Springer Berlin Heidelberg, 1995, pp. 12–25.
[Online]. Available: http://dx.doi.org/10.1007/BFb0034794
[17]
R. Swan and J. Allan, “Automatic generation of overview timelines,” in
Proceedings of the 23rd Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval, ser. SIGIR ’00.
New York, NY, USA: ACM, 2000, pp. 49–56. [Online]. Available:
http://doi.acm.org/10.1145/345508.345546
[18]
K. McKeown, K. Kukich, and J. Shaw, “Practical issues in automatic
documentation generation,” in Proceedings of the Fourth Conference on
Applied Natural Language Processing, ser. ANLC ’94.
Stroudsburg,
PA, USA: Association for Computational Linguistics, 1994, pp. 7–14.
[Online]. Available: http://dx.doi.org/10.3115/974358.974361
[19]
B. Möller, O. Greß, and S. Posch, “Knowing what happened - automatic
documentation of image analysis processes,” in Computer Vision
Systems - 8th International Conference, ICVS 2011, Sophia Antipolis,
France, September 20-22, 2011. Proceedings, 2011, pp. 1–10. [Online].
Available: https://doi.org/10.1007/978-3-642-23968-7_1
[20]
M. Stevens, E. Bursztein, P. Karpman, A. Albertini, and Y. Markov,
“The ﬁrst collision for full sha-1,” Cryptology ePrint Archive, Report
2017/190, 2017, http://eprint.iacr.org/2017/190.
[21]
C. Dobraunig, M. Eichlseder, and F. Mendel, Analysis of SHA-
512/224 and SHA-512/256.
Berlin, Heidelberg: Springer Berlin
Heidelberg, 2015, pp. 612–630. [Online]. Available: https://doi.org/10.
1007/978-3-662-48800-3_25
[22]
M. Szydlo and Y. L. Yin, Collision-Resistant Usage of MD5
and
SHA-1
Via
Message
Preprocessing.
Berlin,
Heidelberg:
Springer Berlin Heidelberg, 2006, pp. 99–114. [Online]. Available:
https://doi.org/10.1007/11605805_7
[23]
D. Y. Naumov et al., “ufz/ogs-data: Initial zenodo release,” Aug. 2017.
[Online]. Available: https://doi.org/10.5281/zenodo.840660
[24]
C. Helbig, L. Bilke, H.-S. Bauer, M. Böttinger, and O. Kolditz,
“Meva - an interactive visualization application for validation of
multifaceted meteorological data with multiple 3d devices,” PLOS
ONE, vol. 10, no. 4, 04 2015, pp. 1–24. [Online]. Available:
https://doi.org/10.1371/journal.pone.0123811
[25]
E. Jang et al., “Identifying the inﬂuential aquifer heterogeneity factor
on nitrate reduction processes by numerical simulation,” Advances in
Water Resources, vol. 99, no. Complete, 2017, pp. 38–52.
27
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-590-6
ICSEA 2017 : The Twelfth International Conference on Software Engineering Advances

