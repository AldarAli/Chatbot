Interactive Gesture Chair
Muhammad Muhaiminul Islam, Shamsul Arefin, Hasan Mahmud, Md. Kamrul Hasan
Systems and Software Lab (SSL), Department of Computer Science and Engineering
Islamic University of Technology (IUT)
Dhaka, Bangladesh
e-mail:{foysol16, sajib312, hasan, hasank}@iut-dhaka.edu
Abstract— Nowadays, computer operators and office workers
have
a
sedentary
lifestyle.
Technology
is
continuously
improving, but our actions become more mechanic. Office
employees have to stay in front of the PC almost all the day
long and this
sedentary behavior
is
not
good
for their
health. As a consequence, people of all ages suffer through
health problems which are related to ergonomic factors.
Doctors always suggest to take breaks and move during work
hours to decrease the probability of a chronic disease. We are
proposing a system to interact with the PC using a chair. By
equipping a chair with motion sensing, the movement gesture
of a user can be detected which can be used as input device for
the PC. We have applied a machine learning algorithm to
calculate the threshold for detecting three types of gestures (tilt
backward, rotate right, rotate left) to control Windows Photo
Viewer. The user evaluation shows that more than 80% of the
users found it interesting and they achieved around 92%
accuracy while controlling the application.
Keywords—sedentariness, gesture chair, Ergonomic factors,
HCI, MPU-6050
I.
INTRODUCTION
During everyday office work, we generally control our
computers with keyboard and mouse sitting in front of them.
When we work in our office in front of computer, we spend
most of our work time in a sedentary way. We remain seated
when we are on our way to the office by car, during
meetings, during lunch, etc. This sedentary behavior is
considered as an important ergonomic factor which may lead
to a variety of chronic diseases for people of all ages [23].
Due to prolonged seating, people may suffer from back pain,
neck pain, etc. Therefore, many proposals have emerged to
keep people moving during their work day. However, for
most of the office workers, it is difficult to achieve a
considerable reduction of the time spent seated within the
office environment. To promote physical activity even in
such sedentary situations, this work explores the possibilities
of using an interactive office chair to smoothly integrate
physical
activity
into
the
daily
working
routine.
By
equipping a flexible chair with a motion sensor, the
movements of a person sitting on the chair can be tracked
and transformed into input events that trigger various actions
on a computer. Besides, interacting with computers for a
long period of time is tiresome also, so there is a need of an
alternate way to do the tasks other than regular mouse and
keyboard operations. This way, the “Interactive Gesture
Chair” becomes
an input device that is ubiquitously
embedded into the working environment and provides an
office worker with the possibility to use the movements of
his body for rotating, tilting or bouncing a chair to intuitively
control operations on desk computers. . Considering health
issues, in our proposed architecture, the user has to move his
body which is at least better than staying a long period of
time in the same position. Recently, Massachusetts Institute
of Technology (MIT) have started working on chair gesture
[11][14][15]. Utilizing these chair gestures into a frequently
used
application
is
challenging.
The
success
of
this
‘interactive gesture chair’ depends on proper integration of
the gestures with a frequently used desktop application. We
can see that ‘Windows Photo Viewer’ is an application
which is frequently used to view photos by the people of all
ages and professions such as teachers, students, researchers,
office workers etc. We have integrated our ‘Interactive
Gesture Chair’ with a customized Windows Photo Viewer to
watch pictures considering the ergonomic issues, which will
reduce the probability of developing chronic disease.
We have collected data from sensors attached with the
chair and labelled them with gesture names (tilt backward,
rotate right, rotate left). We applied decision tree algorithm
to automatically calculate threshold values to determine
gestures. Afterwards, these gestures are
mapped
with
windows commands to control the photo viewer application.
The rest of the paper is structured as follows. In Section
II, we present the related work. In Section III, we describe
our system design. Section IV discusses the implementation
and Section V presents the evaluation of our proposal. We
conclude in Section VI.
II.
RELATED WORKS
Nowadays people are trying to design some natural ways
to interact with computers instead of mouse and keyboard.
To follow that, people are equipping sensors with frequently
used things to establish communication with a PC. A chair is
one of the frequently used pieces of furniture to be equipped
with sensors to use as input device of computer. In the
previous works of Media Interaction Lab [14][21] they have
used Gyroscope and Accelerometer to detect movement of
the chair and controlled multimedia player by some defined
gestures. Another chair based gesture detection [15] uses
Lumia smartphone for getting sensors data which performs
both
music
player
control
and
Web
browsing.
The
Unadorned Desk [22] is an example for this kind of
interaction.
It uses physical space around a desktop
computer for mouse input. Internet Chair [6] was used for
performing tilting, rotating gestures for browsing and
17
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

navigation through Web pages. ChairIO [1] introduced chair
based gaming control like joystick. The ChairMouse [3]
translated natural chair rotation into cursor movement for
effective navigation through large displays.
In all existing works, thresholding approach is applied to
detect gestures. Thresholding does not work properly
because the threshold was empirically determined which has
some problems in case of gesture variation produced by
various users of different weight and height. Thresholding
does not give a universal threshold value that will work for
every user. We have collected data of the people of different
regions of the world such as Africa, Middle East and Asia
and then we applied a decision tree algorithm. The decision
tree would give better result than a threshold.
III.
SYSTEM DESIGN
To detect the movements of the chair we need to equip
the chair with accelerometer and gyroscope. For that we
need a Magnetic Pickup Unit (MPU), MPU-6050 sensor.
MPU-6050 is an Inertia Measurement Unit (IMU) sensor.
Among many IMU sensors, we found that MPU 6050 to be
the most reliable and accurate IMU sensor. Apart from being
significantly cheaper than the other sensors, the MPU 6050
performs much better too. The MPU 6050 is a 6 DOF
(Degrees of Freedom) or a six axis IMU sensor, which
means that it gives six values as output: three values from the
accelerometer and three from the gyroscope. The MPU 6050
is a sensor based on MEMS (Micro Electro Mechanical
Systems) technology. Both the accelerometer and
the
gyroscope are embedded inside a single chip. This chip uses
I2C (Inter Integrated Circuit) protocol for communication
[23].
The MPU 6050 communicates with the Arduino through
the I2C protocol. Arduino [24] is an open source framework,
a mega board to read inputs from sensor such as MPU 6050.
The MPU 6050 is connected to Arduino as shown in Figure
1.
Figure 1. MPU 6050 interfaced with Arduino Mega
IV. IMPLEMENTATION
Our proposed system is shown in Figure 4. It is a
portable system because we can move the chair anywhere
while the sensors are accoutered at the back side of the chair.
We have collected the data from the accelerometer and
gyroscope for the people of different weights and ages for
rotating right, rotating left and tilting back. Values have been
passed through a Bluetooth device to PC from Arduino to
analyze. We have used a machine learning algorithm which
is a decision tree to find out the threshold value for 3
gestures (tilt backward, rotate right, rotate left). To apply a
decision tree on our collected data we had to label our data
by left, right, back and steady (no gesture). Before labeling
our data we have discarded the first 200 data points from
every
gesture
instance
to
remove
noise.
The
main
functionality of a machine learning algorithm is to classify
the input data into a class. If an input data cannot be
classified into any class then that input data is classified into
no class. As ‘no class’ we have used ‘steady’ to define that
the current input data cannot be classified into rotate left,
rotate right or tilt back. After labeling the data, we made a
file that includes only labeled data. We used that file as an
input file for Weka [25] to run the decision tree on that
labeled data. After running the decision tree on the labeled
data, it generates a threshold value for every gesture as
shown in Figure 3. We used that threshold values to define
gestures. Using these defined three gestures, we have
controlled Windows Photo Viewer. Rotate Right gesture is
used to view next photo, rotate left gesture is used to view
previous photo and Tilt Back Gesture is used to turn
ON/OFF the slide show, as shown in Figure 2. The tabular
representation of gesture mapping is shown in Table 1.
Figure 2. Definition of gestures
Figure 3. Result of decision tree
18
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

Figure 4. Back View of Chair
TABLE 1: TABULAR REPRESENTATION OF GESTURE MAPPING
Rotate left
To view previous photo
Rotate right
To view next photo
Tilt backward
To ON/OFF slide show
V.
EVALUATION
The primary purpose of our evaluation was to evaluate
the current system by end users to facilitate future change.
We chose 10 users as participants to evaluate our system to
different extent. They were selected based on their weight
and height. Their weight was 46 to 146kg and height was 60
inch to 80 inch. Our experiment also shows that threshold
depends on height and weight. That is why height and
weight
are
important
factors
in
gesture
recognition.
Participants were randomly assigned to perform some
gestures from the set of defined gestures. We have defined
so far rotate left, rotate right and tilt back gestures. For now
we experimented with Windows Photo Viewer of Microsoft
Windows 7. Some photos were selected and kept into a
folder for this evaluation purpose. One of these photos was
opened by Windows Photo Viewer. Then Windows Photo
Viewer was controlled by the gestures, such as ON/OFF
slide show, view next photo, view previous photo. Users
performed various gestures in random order. Each of them
performed different gestures for a specific duration of 300
seconds. During this time, the number of gestures detected
for each user was slightly different. For example, one
performed 70 gestures but another performed 56 gestures in
300 seconds. Also, we counted correct and incorrect
detection of gestures. The error rate was around 5 per 60
gestures. There was another challenging issue of detecting
tilt back gestures, people having more weight felt easy to
perform the tilt back gesture. Users had to fill out a small
questionnaire with various aspects of the experiment. We
found almost 70-80% had found it interesting, 10-15%
found it somehow cumbersome. Questionaries’ also include
some open ended questions about improving our system.
Some said, it would be better if the program would have
good graphical user interface. Some pointed out that,
unintentional movement sometimes triggers meaningless
gesture events. Participants seemed to be concerned about
accidentally triggering actions on the computer through
naturally occurring movements (e.g., fidgeting, stretching).
Indeed, since users are constantly moving while seated on
chair, a major challenge for chair-based interaction is how
to effectively distinguish chair gestures from natural body
movement that may occur unconsciously during regular
work. An easy approach to avoid such unintentional input
is to let the user decide when gesture
input started
by
providing
mechanisms
to toggle
gesture
start or stop
dynamically when they need. Some more manual mode-
switching (e.g., pressing button on UI or maybe some voice
controls) will be part of our future research. Moreover, some
participants
had
bitter
comments
regarding
the
chair
gestures as they became annoyed or tired when performing
over a longer period of time. Since moving the whole body
to perform
gestures
with an active chair involves more
muscles than standard mouse or keyboard use, a
certain
level
of
fatigue
may
occur
with
frequently giving
gesture input. However, we still can consider
potential
positive sides (i.e., breaking up the monotony, relaxing)
over negative effects (i.e., fatigue, distraction) of the
proposed gestural chair interaction.
VI.
CONCLUSION AND FUTURE WORK
We designed a system considering the chair gestures as
optional input modality so that people can use these gestures
occasionally when they prefer to interact with computers. To
do that a chair is accoutered with accelerometer and
gyroscope sensors. These sensors data provides us the
opportunity for real-time interaction with various types of
computer applications.
We applied decision tree to find a
universal threshold on the sensor data to define gestures and
we found a universal threshold for every gesture. For that we
faced some challenges. One of the challenges was to detect
tilt back gestures, people having more weight felt easy to
perform the tilt back gesture but the people having less
weight faced some difficulties to perform tilt back gesture.
We have overcome this challenge by using decision tree
algorithm. Another challenge was to distinguish chair
gestures from natural body movement that may occur
unconsciously during regular work. Therefore, in the future,
we will attach an indicator which will tell the system when to
apply gestures. Another challenge was to remove noise from
sensor data. In the future, we will approach some other
machine learning algorithms for improving detections of
such gestures.
19
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

REFERENCES
[1]
S. Beckhaus, K. Blom and M. Haringer , “ChairIO –The Chair-Based
Interface. In Concepts and Technologies for Pervasive Games.” 2007,
231–264.
[2]
M. Van Beurden, W. Ijsselsteijn and Y. De Kort. , “User Experience
of
Gesture
Based
Interfaces:
A
Comparison
with
Traditional
Interaction Methods on Pragmatic and Hedonic Qualities.” GW 2011.
[3]
A. Endert , P. Fiaux , H. Chung, M. Stewart, C. Andrews , C. North ,
“ChairMouse:
Leveraging
Natural
Chair
Rotation
for
Cursor
Movement on Large, High-Resolution Displays”. ACM alt.CHI,
2011.
[4]
T. Springer , “The Future of Ergonomic Office Seating.” Knoll
WorkplaceResearch,2010.
[https://www.knoll.com/media/477/936/wp_future_ergonomic_seatin
g.pdf]
[5]
M. Karam and M. C. schraefel. . , “A taxonomy of gestures in human
computer
interactions.”
Technical
report
ECSTR-IAM05-009,
Electronics and Computer Science, University of Southampton, 2005.
[6]
M. Cohen, “The Internet Chair.” International Journal of Human
Computer Interaction 15, 2 (2003).
[7]
I. Daian,  A. M. Van Ruiten, A. Visser and S. Zubić , “Sensitive 
Chair: A Force Sensing Chair with Multimodal Real Time Feedback
via Agent.” In Proc. ECCE ’07, ACM Press (2007).
[8]
N. Owen, A. Bauman, and W. Brown , “Too much sitting: a novel
and important predictor of chronic disease risk?” In British Journal of
Sports Medicine 43, 2 (2009), 81–83.
[9]
J. Forlizzi, C. DiSalvo, J. Zimmerman, B. Mutlu and A. Hurst, “The
SenseChair: The lounge chair as an intelligent assistive device for
elders.” In Proc. DUX ’05, AIGA (2005), 31.
[10] R. J. K. Jacob, “Eye Movement-Based Human-Computer Interaction
Techniques: Toward Non-Command Interfaces.” In Advances in
Human-Computer Interaction (1993), 151–190.
[11] K. Probst, J. Leitner, F. Perteneder, M. Haller, A. Schrempf, and J.
Glöckl, “Active Office: Towards an Activity-Promoting Office
Workplace Design.” In Proc. CHI EA ’12, ACM Press (2012), 2165–
2170
[12] D. Saffer, “Designing Gestural Interfaces.” O’Reilly Media, 2008.
[13] H. Pohland R. Murray-Smith , “Focused and Casual Interactions:
Allowing Users to Vary Their Level of Engagement.” CHI 2013,
2223–2232.
[14] K. Probst, D. Lindlbauer, P. Greindl, M. Trapp, M. Haller , B.
Schwartz, and A. Schrempf, “Rotating, Tilting, Bouncing: Using an
Interactive Chair to Promote Activity in Office Environments.” CHI
EA 2013, 79–84
[15] K. Probst, D. Lindlbauer, M. Haller, B. Schwartz, and A. Schrempf,
“A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair
Gestures for Focused and Peripheral Interaction.” CHI 2014.
[16] T. Baudeland M. Beaudouin-Lafon, “Charade: Remote Control of
Objects Using Free-hand Gestures.” InCommunications of the ACM
36, 7 (1993).
[17] T. Vanhala, V. Surakka, and J. Anttonen, “Measuring Bodily
Responses to Virtual Faces with a Pressure Sensitive Chair.” In Proc.
NordiCHI ’08, ACM Press (2008), 555–559.
[18] S. Nanayakkara, L. Wyse, and E. A. Taylor, “Effectiveness of the
haptic chair in speech training.” In Proc. ASSETS ’12, ACM Press
(2012).
[19] B. MacKay, D. Dearman, K. Inkpen, and C. Watters, “Walk’n Scroll:
A
Comparison
of
Software-based
Navigation
Techniques
for
Different Levels of Mobility.” In Proc. MobileHCI ’05, ACM Press
(2005), 183–190.
[20] K. Probst, D. Lindlbauer, M. Haller, B. Schwartz, A. Schrempf, 2014.
, “Exploring the Potential of Peripheral Interaction through Smart
Furniture”, in Peripheral Interaction: Shaping the Research and
Design Space, Workshop at CHI 2014.
[21] D. Hausen, S. Boring, and S. Greenberg, “The Unadorned Desk:
Exploiting the Physical Space around a Display as an Input Canvas.”
INTERACT 2013, 140–158
[22] W. F. Booth, Ph.D., K. C. Roberts, Ph.D. and J. M. Laye, Ph.D. ,
“Lack of exercise is a major cause of chronic diseases.” Compr
Physiol. 2012 April ; 2(2): 1143–1211.
[23] Inter Integrated Circuit (I2C) protocol: http://i2c.info/
[24] Arduino introduction: https://www.arduino.cc/en/Guide/Introduction
[25] Weka 3: Data Mining Software in Java:
http://www.cs.waikato.ac.nz/~ml/weka/index.html
20
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

