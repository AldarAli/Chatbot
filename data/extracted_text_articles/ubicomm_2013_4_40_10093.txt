Smart Chair: What Can Simple Pressure Sensors under the Chairs’ Legs Tell Us
about User Activity?
Jingyuan Cheng, Bo Zhou, Mathias Sundholm, Paul Lukowicz
Group Embedded Intelligence
German Research Center for Artiﬁcial Intelligence
Kaiserslautern, Germany
{jingyuan.cheng, bo.zhou, mathias.sundholm, paul.lukowicz }@ dfki.de
Abstract—In this paper, we investigate how much information
about user activity can be extracted from simple pressure sensors
mounted under the legs of a chair. We show that it is possible to
detect not only different postures (0.826 accuracy for 5 subjects
and 7 classes), but also subtle hand and head related actions
like typing and nodding (0.880 accuracy for 5 subjects and 5
classes). Combining features related to postures and such simple
actions, we can detect high-level activities such as working on a
PC, watching a movie or eating in a continuous, real-life data
stream. In a dataset of 105.6 hours recorded from 4 subjects, we
achieved 0.783 accuracy for 7 classes.
Keywords—ubiquitous computing; smart chair; resistive pres-
sure sensor; activity recognition
I. INTRODUCTION
In general, people spend a lot of time sitting: when working
in the ofﬁce, in a meeting, in the theater, when having a meal,
when playing games or watching TV. In [1], it has been shown
that on average, people with sedentary jobs sit 9.95 hours at
working days and 8.07 hours when not working. From the
above, instrumented chairs may seem like an obvious approach
to activity recognition. On the other hand, there is the question
that how much information can be extracted from chair-
mounted sensors. Per deﬁnition, when sitting, people tend
to engage in activities that involve little motion of the body
trunk and are mostly determined by hand actions and cognitive
processes. At the same time, the body trunk is the main
physical interface between the chair and the user, and thus
the main potential source of information. As a consequence,
the vast majority of work on chair based activity monitoring
has focused on posture analysis and the measurement of
physiological parameters that can be extracted from the trunk
by electrodes integrated in the seat (see related work section).
Such work has so far relied on sensors integrated in the seat
itself or the backrest (often using smart textiles), which implies
signiﬁcant effort in terms of unobtrusive instrumentation in
every day environments.
A. Paper Contribution
Building on such work, the core ideas that this paper
explores, are two fold:
1) Many shoulder, arm, hand and head related actions leave
a subtle but characteristic signature in terms of weight
distribution changes on the chair. Such signatures can
be combined with the correlation between posture and
activity for the recognition of complex activities, which
are not directly related to trunk motions.
2) The weight distribution and posture changes can be
detected using simple, cheap pressure sensors mounted
under the chair, without the need to instrument the
seating surface or the backrest. This means that existing
chairs can be easily retroﬁtted, allowing for the instru-
mentation of entire ofﬁces, meeting rooms, or theaters.
From the above ideas, the paper makes the following concrete
contributions:
• We describe the measurement system, which is based on
our own resistive foam sensor, which is simple and cheap;
while at the same time providing the required resolution
and measurement range.
• We validate the hypothesis that posture changes can
be detected using pressure sensors integrated under the
chairs’ legs (as opposed to pressure sensors matrices on
the seat itself as investigated by related work). This is
done in experiments with 5 subjects and 7 postures (e.g.
sit straight, lean left / right / forward / backward, raise
one hand, cross one leg over the other knee), giving a
recognition rate of 0.826 on average.
• We validate the assumption that actions related to arm,
hand and head motion produce detectable signatures in
the pressure signals. This is done in experiments with
5 subjects and 5 actions (e.g. typing keyboard, clicking
mouse, nodding, clapping hands and sitting still), giving
a recognition rate of 0.880 on average.
• We demonstrate that such information can be used to dis-
criminate high-level activities such as playing a computer
game, working on the computer and eating a snack. From
4 test subjects, 105.6 hours of data recording, we are able
to divide high-level activities into 7 classes and achieve
an overall accuracy of 0.783.
B. Related Work
As early as 1997, Tan, etc. studied the possibility of using
a chair as a control interface [2], the idea led to a series
of papers with on-chair sensor matrix, used for real-time
posture tracking or game interface [3]. By using a near-optimal
sensor placement strategy, Multu, etc. managed to reduce the
sensor number to 19 while still having 78% accuracy from
10 postures
[4]. Beside posture, pressure matrix can also
recognize who is sitting on a car seat [5]. There are already
81
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

Fig. 1.
System Design; a) raw materials for the pressure sensor; b) a sensor
prototype; c) a chair equipped with sensors
a wide range of pressure sensors/matrices commercially avail-
able with different shapes, sizes, pressure ranges [6].
Another approach has been to use the contact area between
the torso and the chair to measure various physiological
parameters. Examples are the measurement of vital signs in
aircraft seats [7], car seats [8] and regular chairs [9].
C. Paper Structure
In this paper, we ﬁrst introduced the system setup in Section
II. Then the evaluation in Section III is done by recognizing
postures, minor activities and daily activities from experiment
results. At last, current work is concluded, and future plans
proposed.
II. SYSTEM DESIGN
Our prototype is composed of a normal ofﬁce chair with 4
pressure pads, one under each leg. The hardware is completely
hidden under the chair (as shown in Fig. 1). With a 5500mAh
battery, the system runs continuously for 36 hours.
A. Physical background
The above system provides two types of information: (1) the
total weight (vertical force component) applied to the chair;
(2) the distribution of the weight/force applied to the four legs.
Obviously, different body postures lead to different weight
distributions (sitting straight or leaning aside). Thus, the low
frequency ”baseline” of the signal is essentially determined by
posture and posture changes. On top of this baseline, there are
higher frequency ﬂuctuations related to the fact that body parts
do not move in isolation. Instead, the human body is a coupled
mechanical system where even a subtle movement of one part
inﬂuences many others. For example, nodding is not only the
head moving up and down, but a complex action concerning
the whole vertebral column, the buttocks and sometimes the
feet. Even subtle limb motions can propagate all the way to
the chair’s legs. As a consequence, many activities, that one
may not obviously associate with weight distribution changes
(e.g. typing), have a weak but characteristic signature in the
high frequency signals from pressure sensors in the chair’s
legs. In addition, different high-level activities are associated
with certain posture types (e.g. leaning forward when typing).
B. System Design
The core requirements resulting from the above considera-
tions are high dynamic range and high precision. In our sensor,
the baseline weight of a person sitting in a chair produces a
signal ∼ 2V while the smallest labeled signal for ”typing”, is
in the range of single mV . At the same time, since we aim to
eventually be able to easily retroﬁt whole meeting rooms or
theaters, a simple, low cost system is needed.
Our solution is based on polyethylene foam that changes its
resistance as a function of mechanical pressure. The resistance
of a single 3mm layer varies from several hundred kΩ to
several MΩ. Preliminary tests indicated that the force from
an occupied chair is large enough to saturate a single layer.
As a consequent state, to improve the stiffness of the whole
sensor pad, we have compressed 4 layers of 1 × 1cm2 PE
foam into the thickness of one layer and embedded it into the
center of a single layer 4×4cm2 pad using normal thread. Two
electrodes made of metal textile are then secured by normal
thread and isolating tape, which turns the whole pad into a
resistive pressure sensor.
Each sensor is connected in series to a resistor; a low
noise 2.5V DC voltage is applied across the two, converting
resistance change to voltage change, which is then fed into a
4 channel 24-bit ADC sampling at 25Hz. The overall noise
level of the analog circuit is ∼ 0.22mV (RMS).
To enable the user to freely move the chair and to isolate
the noise from mains power, we use a battery to power the
system and a 2.4GHz Zigbee module to transfer data.
III. EVALUATION STRATEGY AND RESULTS
We evaluate the system in two steps. First we demonstrate
the ability of our system to recognize a set of predeﬁned
postures and simple actions in controlled lab experiments. We
then proceed to investigate the usefulness of our concept for
the discrimination of more complex, high-level activities in
real life data streams.
A. Predeﬁned Postures and Minor Activities
The subject is seated on the chair in front of a desk with a
computer and asked to perform the activity displayed on the
screen, in total 12 postures and actions × 20 times each. The
postures are sitting straight, leaning forward / backward / left
/ right, sitting with one leg cross the other knee, and sitting
with one hand raised in the air. The actions are, nodding,
clapping hands, typing on the keyboard, moving and clicking
the mouse. We also include the class ”vacant chair’ in the
experiment. Each activity lasts for 15 seconds, followed by a
small pause, where the subject returns to the default posture
(sitting straight). The activities’ order is randomized in each
round except ”vacant chair”, which is always at the round’s
end so that the test subject seat him/herself differently in the
next round.
Overall 5 healthy subjects (1 female, 4 males, aged 23-34
years) participated in the data recording.
The ﬁrst and last 2 seconds of each activity are left
out, where the subject reacts to the instructions or returns
82
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

Fig. 2.
Confusion matrix of predeﬁned postures: a) each test subject b)
average of all subjects c) when merged into a single dataset
to the default posture. The following features are used for
classiﬁcation:
1) Cross-channel features (8 total):
• Mean and RMS of 4 sensors combined.
• Center of weight in two directions: calculated as the
differences between sensors on the left and right side of
the chair, and between the sensors on the front and back
side of the chair, both divided by the sum of all 4 sensors.
• Activity level features: calculated as mean, median of the
summed 1st derivative’s absolute values in time domain
of 4 sensors.
• Median and mean of absolute values sum of 4 sensors
after removing DC using a high-pass ﬁlter (fc = 0.5Hz).
2) Single channel features (7× 4 total):
• The mean magnitude, the central frequency, and the mag-
nitude of the central frequency in 5 uniform frequency
bands between 0 and 12.5Hz.
We ﬁrst evaluate 7 sitting postures. The classiﬁcation is per-
formed using stratiﬁed 10-fold cross-validation with an LDA
classiﬁer [10]. The average accuracy with subject dependent
training is 0.826 (e.g. if the chair knows who is sitting on
it, balanced F-score in Fig. 2 b). If data from all subjects is
merged into one (e.g. the chair doesn’t know the user), then
the accuracy drops to 0.629 (Fig. 2 c)).
We then evaluated the 4 simple actions of the hands and the
head: typing on the keyboard, clicking the mouse, clapping
hands and nodding. The ﬁrst two are tiny activities with hand
and little arm movement; in the latter two, the vertebral column
also moves a little. A 5th class is added as sitting still, where
we randomly pick 20 out of 100 postures without movement
(viz. sitting straight and leaning aside). The average accuracy
is 0.880 for the user independent and 0.748 for the across all
users case (details in Fig. 3).
B. High-level Daily Activities
Next we consider 7 high-level activities that are routinely
performed while sitting: working on PC, eating, playing video
Fig. 3.
Confusion matrix of predeﬁned activities: a) each test subject b)
average of all subjects c) when merged into a single dataset
games, watching movie, talking with others, browsing the
Internet and vacant seat. We record data from 4 healthy
subjects (1 female, 3 males, aged 24–34 years). Thus, the
subjects are asked to sit on the equipped chair and perform
their normal work routine (which mostly takes place in front of
a computer in the student room) for at least 8 hours × 3 days.
They are also asked to play games and watch movies (which
we assume is not part of a daily work routine). For privacy
and practicability issues, the experiment is not video recorded
and not precisely labeled. Instead, the subjects keep a log
of their major activities throughout the day in an experience
sampling like approach. Overall, the data encompasses 105.6
hours, out of which 79.2 hours the chair was occupied (details
in Table. I).
One key feature for distinguishing the above high-level
activities is the ”activity level”, deﬁned as mean and median of
the 1st derivative’ absolute value in time domain. It increases
starting ”watch movies”, through ”play games”,”browse the
Internet”,”work on PC”,”eat”,”talk with others”. Even though
the exact way of performing the speciﬁc high-level activities
differs across subjects, their activity levels are still comparable.
Another important feature is the weight center, which is
related to the attention on the PC screen for certain activities.
It is further in the back when watching movie and browsing
website where overview of the whole screen is important.
When talking or eating, no screen is needed, so occupants
tend to sit comfortably in the middle; while when it comes to
computer work and game, where close attention needs to be
paid to the screen and frequent keyboard/mouse operation is
required, the weight center is pushed forward.
In a 5 min window jumping in steps of 30 sec over the
daily data stream, the same set of features were calculated
as in Section III-A. When the windows covers multiple high
level activities,a majority decision is made. Classiﬁcation is
performed using 10-fold cross-validation method with an LDA
classiﬁer. The confusion matrix is given in Fig. 4.
The accuracy is 0.783 for the user dependent case and 0.645
83
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

TABLE I
STATUS AND DURATION FOR EACH SUBJECT (HOURS)
Status
Center of weight
Typical activities
S1(F)
S2(M)
S3(M)
S4(M)
Sum hours
watch movie
back/middle
seldom any movement
3.7
2.4
2.8
1.6
10.5
play game
front/middle
frequent mouse movement, seldom
body movement
4.4
1.6
4.8
1.6
12.4
browsing the In-
ternet
/
some mouse and body movement
0.4
1.9
2.4
0.5
5.2
computer work
much in the front
frequent typing on keyboard, some
mouse and body movement
12.5
9.9
8.9
10.5
42
eat
front/middle
some arm/body movement
0.5
0.1
1.7
0.4
2.7
talk with others
middle/back/aside
bursting arm/head/body movement
0.8
2.7
1.5
1.4
6.4
nobody on chair
/
much less pressure, no movement
10.2
6.7
1.5
8.5
26.4
Sum hours
32.6
25.3
23.7
24.0
105.6
Fig. 4.
Confusion matrix of high-level daily activities: a) each test subject
b) average of all subjects c) when merged into a single dataset
accross all users. This is far from perfect, but also far above
random, making it possible for anonymous implementations.
Note that errors are not only due to misclassiﬁcation, but
also related to labeling inaccuracies and the complexity of
the activities. For example, working on computer might well
include browsing the Internet for a short while (to seek for
information). Some subject ate snacks when watching movie,
or browsed the Internet while having main meal.
IV. CONCLUSION
While the classiﬁcation results shown in the previous section
are far from perfect; they are also far above random. Overall,
we believe that the fact that such results can be achieved using
signals from just four simple pressure sensors mounted under
the chairs legs is surprising and relevant for a wide range of ap-
plications. In particular the ability to easily retroﬁt large rooms
(e.g. a conference room, theater) opens up the interesting
research opportunities in the area of social interaction. Another
possibility is to equip other furniture (table, bed, couch and
etc.) for more complex activity recognition at home or in
public spaces; yet the physical sensors need to be improved
to support larger force. We will also investigate the possi-
bilities of putting the sensors under furniture with different
support, such as castors. In such an approach, visual and audio
information is not required, therefore implementation could
be anonymous, protecting the occupants’ privacy. Finally, we
will investigate the combination of furniture integrated sensors
with information from users’ smart-phones as a way of tying
activities to speciﬁc persons.
Note that although our validation is done in the ofﬁce and
based on ofﬁce activities; the aim of the paper is not to
develop a practical system for the tracking of ofﬁce activities
and/or related applications. Instead, the main contribution is
to show that high-level activities not necessarily related to
body torso trunk motions can be detected from simple, easily
retroﬁtted sensors integrated under the chairs’ legs and the
ofﬁce activities are merely an easily obtainable dataset.
REFERENCES
[1] C. G. Ryan, P. M. Dall, M. H. Granat, and P. M. Grant, “Sitting patterns
at work: objective measurement of adherence to current recommenda-
tions,” Ergonomics, vol. 54, no. 6, 2011, pp. 531–538.
[2] H. Z. Tan, A. Pentland, and I. Lu, “The chair as a novel haptic
user interface,” in Proceedings of the Workshop on Perceptual User
Interfaces, Banff, Alberta, Canada, 1997, pp. 56–57.
[3] H. Z. Tan, L. A. Slivovsky, and A. Pentland, “A sensing chair using
pressure distribution sensors,” Mechatronics, IEEE/ASME Transactions
on, vol. 6, no. 3, 2001, pp. 261–268.
[4] B. Mutlu, A. Krause, J. Forlizzi, C. Guestrin, and J. Hodgins,
“Robust, low-cost, non-intrusive sensing and recognition of seated
postures,” in Proceedings of the 20th annual ACM symposium
on User interface software and technology, ser. UIST ’07.
New
York, NY, USA: ACM, 2007, pp. 149–158. [Online]. Available:
http://doi.acm.org/10.1145/1294211.1294237
[5] X. Xie, B. Zheng, and W. Xue, “Object identiﬁcation on car seat based
on rough sets,” in Communication Software and Networks (ICCSN),
2011 IEEE 3rd International Conference on, 2011, pp. 157–159.
[6] I.
Tekscan,
“Tekscan
pressure
sensors,”
http://www.tekscan.com/
pressure-sensors, accessed: 2013-05-07.
[7] J. Schumm et al., “Unobtrusive physiological monitoring in an airplane
seat,” Personal and Ubiquitous Computing, vol. 14, no. 6, 2010, pp.
541–550.
[8] M. Walter, B. Eilebrecht, T. Wartzek, and S. Leonhardt, “The smart car
seat: personalized monitoring of vital signs in automotive applications,”
Personal and Ubiquitous Computing, vol. 15, no. 7, 2011, pp. 707–715.
[9] Y. G. Lim, K. K. Kim, and S. Park, “Ecg measurement on a chair without
conductive contact,” Biomedical Engineering, IEEE Transactions on,
vol. 53, no. 5, 2006, pp. 956–959.
[10] W. J. Krzanowski, “Principles of multivariate analysis: A user’s perspec-
tive,” 1988.
84
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-289-9
UBICOMM 2013 : The Seventh International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

