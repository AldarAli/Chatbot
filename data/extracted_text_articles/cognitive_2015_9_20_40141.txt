Simulation of the Emergence of Language Groups
Using the Iterated Learning Model on Social Networks
Makoto Nakamura
Japan Legal Information Institute,
Graduate School of Law,
Nagoya University
Email: mnakamur@nagoya-u.jp
Ryuichi Matoba
Department of Electronics
and Computer Engineering,
National Institute of Technology,
Toyama College,
Email: rmatoba@nc-toyama.ac.jp
Satoshi Tojo
School of Information Science,
JAIST,
Email: tojo@jaist.ac.jp
Abstract—In evolutionary linguistics, the Iterated Learning Model
(ILM) is often used for simulating the ﬁrst language acquisition.
Our purpose in this paper is to develop an agent-based model for
language contact based on ILM. We put a learning agent on each
node in the social network. Our experimental result showed that
the language exposure rather deteriorates the emergence of local
common languages, and grammars become non-compositional,
which is different from our expectation. However, we have shown
that an excessive string-clipping as well as a language exposure
may constrain the appearance of local language community,
independent of the shape of networks.
Keywords–Simulation, Language Acquisition, Iterated Learning
Model, Social Network.
I.
INTRODUCTION
Thus far, simulation studies have played an important role
in the ﬁeld of the evolution of language [1]. Especially, a very
important function of simulation is to prove if a prediction
actually and consistently derives from a theory [2]. So far, there
have been a variety of methodologies proposed on simulating
the evolution of languages, each of which belongs to a different
level of abstraction. Simulation studies for population dynam-
ics alone include an agent-based model of language acquisition
by Briscoe [3], which was developed toward a formal model
of language acquisition device. On the other hand, Nowak [4]
proposed a mathematical theory of the evolutionary dynamics
of language called the language dynamics equation. The lan-
guage dynamics equation is highly abstract, while agent-based
model is considered to be a concrete, or less abstract.
Our goal is to provide a framework that represents the
diachronic change in language by the contact among language
communities. It would be useful not only for simulating typical
language changes but also for novel phenomena taking place
in the cyber world. In recent decades, the evolution of the
Internet makes users possible to participate in discussions with
anonymous people concerning their favorite topics beyond the
physical distance. They do not only exchange small bits of
information, but rather seem to establish a durable channel to
communicate among people sharing common tastes on a chat
or bulletin board system. They often employ spoken language
instead of formal one after sharing common interests, and thus
the expression tends to be spontaneous and haphazard. This
phenomenon is often seen in language contact, but the time
and size of the language change on the internet are extremely
fast and large, respectively [5]. Using the framework, it would
be possible to deal with this rapid language change as a
phenomenon of language evolution.
There have been simulation models dealing with language
change. We employ them as possible. Thus far, Nakamura et
al. [7] proposed a mathematical framework for the emergence
of creoles [6] based on the language dynamics equation. To-
ward more concrete analysis, they introduced a spatial structure
to the mathematical framework [8] [9], in which learning
agents contact with neighbors according to the learning al-
gorithm. Furthermore, the spatial structure was expanded into
complex networks [10]. Their studies are based on a hypothesis
about the emergence of creoles, that is, language contact
is likely to stimulate creolization. However, their learning
mechanisms are too simple to observe language changes from
a linguistic aspect, as languages are deﬁned as similarity
measures in a numeric matrix.
We propose an agent based model to deal with grammatical
changes in the language community. Therefore, our purpose
in this paper is to show a relationship between communi-
cation among learning agents and grammatical changes. We
employ Simon Kirby’s Iterated Learning Model (ILM) [11],
which shows a process of grammatical evolution through
generations. Kirby’s ILM has often been used in simulation
models concerning language evolution [12]. One important
reason for this is that ILM is robust against input sentences
in terms of a syntactic learning. As long as learning from
a single parent, its infant agent receives sentences derived
from a consistent grammar, it is possible to acquire a concise
grammar. Currently, the learning situation in ILM is extended
to a multiple families connecting with a network. We can
observe the language change, not only in diachronic situation,
i.e., in parent-child relation, but also in synchronic situation.
Thus far, we have shown a pilot version [13], where we
found a problem reported by Smith and Hurford [20], that is, in
the case learning agents potentially have more than one teacher
agent, the length of syntax rules tends to increase rapidly over
generations due to the addition of symbols of meaningless
terminal symbols. This problem causes an unnatural learning,
which results in a fatal combinatorial explosion. We solved
this problem and try again with a larger number of agents.
175
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

Figure 1. The iterated learning model
This paper is organized as follows. In Section 2, we
introduce Kirby’s ILM. In Section 3, we propose an agent-
based model for language contact. In Section 4, we examine
our proposed method, and conclude in Section 5.
II.
ITERATED LEARNING MODEL FOR SOCIAL
NETWORKS
In this section, we mention how to deal with ILM on
the social networks. Firstly, we brieﬂy explain Kirby’s ILM.
After that, we introduce the modiﬁcation for social networks
by Matoba et al. [14] in order to avoid the combinatorial
explosion, which enables us the expansion of ILM.
A. Brieﬁng Kirby’s Iterated Learning Model
Kirby [11] introduced the notions of compositionality and
recursion as fundamental features of grammar, and showed
that they made a human possible to acquire compositional lan-
guage. Figure 1 illustrates ILM. In each generation, an infant
can acquire grammar in his/her mind given sample sentences
from his/her mother. When the infant has grown up, he/she be-
comes the next parents to speak to a newborn baby with his/her
grammar. As a result, infants can develop more compositional
grammar through the generations. Note that the model focuses
on the grammar change in multiple generations, not on that in
one generation. Although the poverty of stimulus explains the
necessity of the universal grammar [15], Kirby [11] modeled
it as learning through bottlenecks, which are rather necessary
for the learning. Also, he adopted the idea of two different
domains of language [16]–[19], namely, I-language and E-
language; I-language is the internal language corresponding
to speaker’s intention or meaning, while E-language is the
external language, that is, utterances. In his model, a parent is a
speaker agent and his/her infant is a listener agent. The speaker
agent gives the listener agent a pair of a string of symbols as
an utterance (E-language), and a predicate-argument structure
(PAS) as its meaning (I-language). A number of utterances
would form compositional grammar rules in listener’s mind,
through learning process. This process is iterated generation
by generation, and converges to a compact, limited number of
grammar rules.
According to Kirby’s ILM, the parent agent gives the infant
agent a pair of a string of symbols as an utterance, and PAS
as its meaning. The agent’s linguistic knowledge is a set of a
pair of a meaning and a string of symbols, as follows.
S/love(john, mary) → hjsbs,
(1)
where the meaning, that is the speaker’s intention, is
represented by a PAS love(john, mary) and the string of
symbols is the utterance “hjsbs”; the symbol ‘S’ stands for
Verb: admire, detest, hate, like, love
Noun: john, mary, pete, heather, gavin
e.g.) love(mary, john)
(Identical arguments are prohibited.)
Figure 2. Words used in the experiment.
the category Sentence. The following rules can also generate
the same utterance.
S/love(x, mary)
→
h N/x sbs
N/john
→
j
(2)
where the variable x can be substituted for an arbitrary element
of category N.
The infant agent has the ability to generalize his/her
knowledge with learning. This generalizing process consists of
the following three operations [11]; chunk, merge, and replace.
Chunk
This operation takes pairs of rules and looks for
the most-speciﬁc generalization.
{
S/love(john, pete)
→
ivnre
S/love(mary, pete)
→
ivnho
⇒
{ S/love(x, pete)
→
ivn N/x
N/john
→
re
N/mary
→
ho
(3)
Merge
If two rules have the same meanings and strings,
replace their nonterminal symbols with one com-
mon symbol.
Replace If a rule can be embedded in another rule, replace
the terminal substrings with a compositional rule.
In Kirby’s experiment [11], ﬁve predicates and ﬁve object
words shown in Figure 2 are employed. Also, two identical
arguments in a predicate like love(john, john) are prohibited.
Thus, there are 100 distinct meanings (5 predicates × 5
possible ﬁrst arguments × 4 possible second arguments) in
a meaning space.
The key issue in ILM is to make the situation of poverty
of stimulus. As long as an infant agent is given all sentences
in the meaning space during learning, he/she does not need to
make a compositional grammar; he/she would just memorize
all the meaning-sentence pairs. Therefore, agents are given
a part of sentences in the whole meaning space. The total
number of utterances the infant agent receives during learning
is parameterized. Since the number of utterances is limited,
the infant agent cannot learn the whole meaning space, the
size of which is 100; thus, to obtain the whole meaning space,
the infant agent has to generalize his/her own knowledge by
self-learning, i.e., chunk, merge, and replace. The parent agent
receives a meaning selected from the meaning space, and utters
it using her own grammar rules. When the parent agent cannot
utter because of lack of her grammar rules, she invents a new
rule. This process is called invention. Even if the invention does
not work to complement the parent agent’s grammar rules to
utter, she utters a randomly composed sentence.
176
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

Figure 3. Image of clipping process
B. Process for String Clipping
When an infant agent has a number of teacher agents
consisting of his/her parent and neighbors, as the teacher
agents have their own compositional grammar rules, they
are inconsistent with each other. Although the infant agent
tries to ﬁnd a common chunk among utterances, it would be
a short string. Since there is little probability of making a
chunk from short strings, only long ones are likely to survive
toward next generations. As a result, learning agents tend
to have compositional rules with extremely long strings over
generations [20].
Matoba et al. [14] proposed a clipping process in their
model, which solves the above problem. This process is called
backclipping. After learning process of the infant agent, he/she
curtails symbols in his/her grammar rules from the tail of
string, unless it contains ambiguity. As a result, when the infant
agent becomes the new parent agent in the next generation, the
grammar set does not contain extremely long rules any more.
Figure 3 illustrates the clipping process in our model. The
infant agent tries to utter strings of like(john, mary) as shortly
as possible. Firstly, he/she chooses a grammar rule from his/her
grammar set for generating utterance of like(john, mary), and
deletes symbols one by one, i.e., “cba”, “ed”, “abef”. In case
of “cba”, this string does not exist in the grammar rules of the
infant agent, then the infant agent executes backclipping, and
the string becomes “cba” to “cb”. The string “cb” does not
exist in the grammar rules of the infant agent, so the infant
agent executes backclipping, and the string becomes “cb” to
“c”. Since “c” exists in the grammar rules of infant agent, the
infant agent does not abridge it anymore, and adopts “cb” as
the clipped word of “cba”. The same process is also applied to
the other words. As a result, the sentence becomes shortened
from “cbaabefed” to “cbabeed”.
Actually, such phenomenon occurs in the real world, as
cutting the beginning and/or the end of a word off. The deletion
of a part of a word constructs a new and shorter word;
e.g.) Hamburger → burger,
Inﬂuenza → ﬂu,
Examination → exam
A position of clipping is dependent on a phonological
reason [21]. Since ILM does not deal with phonological
information, we need to ﬁnd an alternative way to shorten
strings. In English, we often omit a few syllables of each
word [22];
e.g.) advertisement, doctor, laboratory, professor,
demonstration, captain, etc.
TABLE I. NETWORK CHARACTERISTICS
Network type
Average
Average
(N = 100)
Degree
shortest path
Complete graph
99.00
1.00
Star
1.98
1.98
(a)
Scale-free
3.96
3.28
(b)
Small-world
4.00
3.41
(c)
2D lattice
4.00
12.88
Ring
2.00
25.25
(a) Scale-free (ScFre)
(b) Small-world (SmWld)
(c) 2D-lattice (Lat2D)
Figure 4. Examples of the networks (N = 100)
III.
AGENT-BASED MODEL FOR LANGUAGE CONTACT
In this section, we explain how language groups emerge in
the agent-based model. Agents can get contact with neighbors
on the network (Section III-A), who speak to the infant in a
certain ratio of language exposure (Section III-B). The com-
munication may affect agents’ languages, which are classiﬁed
into groups by the language similarity (Section III-C).
A. Social Networks for Language Communities
Social networks play an important role of language change,
regardless of whether they are connected by an actual or vir-
tual relationship. Some simulation studies deal with complex
networks [10] [23] [24]. There are several types of networks,
each of which characterizes many real-world communities.
Table I shows network characteristics, in which each value
is calculated based on 100 nodes [24]. The average degree
denotes the average number of edges connected to a node. The
average shortest path length stands for the average smallest
number of edges, via which any two nodes in the network can
be connected to each other. In this paper, we examine scale-
free, small-world and 2D-lattice networks.
Figure 4 shows examples of networks, in which the preced-
ing two networks are regarded as complex networks and the
latter is for comparison. Each agent is assigned on an node in
the networks.
177
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

Figure 5. Language input from neighbors depending on the exposure ratio α
B. Exposure Ratio α
Nakamura et al. [7] has introduced an exposure ratio α,
which determines how often language learners are exposed to
a variety of language speakers other than their parents. They
modiﬁed the learning algorithm of Nowak et al. [4], taking the
exposure ratio into account in order to model the emergence
of creole community. They have shown that a certain range
of α is necessary for a creole to emerge. This parameter was
further employed for the following network studies [8]–[10].
In some communities, a child learns language not only
from his/her parents but also from other adults, whose language
may be different from the parental one. In such a situation, the
child is exposed to other languages, and thus may learn the
most communicative language. In order to assess how often the
child is exposed to other languages, let us divide the language
input into two categories: one is from his/her parents, and the
other is from other language speakers. The ratio of the latter to
the total amount of language input is called an exposure ratio
α. This α is subdivided into smaller ratios corresponding to
those other languages, where each ratio is in proportion to the
population of the language speakers.
An example distribution of languages is shown in Figure 5.
Let Gi be the language of Agent i. Suppose a child has parents
who speak Gp, he/she receives input sentences from Gp in the
proportion of 1 − α, and from non-parental languages Gi(i ̸=
p) in the proportion of αxi, where xi denotes a population
ratio of Gi speakers in the neighbors.
C. Distance between Languages and Language Groups
In this section, we discuss how to deal with languages in
the framework of ILM on a social network. An infant receives
a meaning-signal pair from his parent and neighbors according
to the exposure ratio α. The number of utterances an infant
receives is ﬁxed, and he/she receives them in proportional
to the language distribution for neighbors like the pie chart
shown in Figure 5. The population consists of non-overlapping
generations, that is, infants at each generation are born at the
same time, become parents at the same time, and die at the
same time. The network is ﬁxed through generations.
In order to compare between languages, we deﬁne the
distance in languages by the edit distance, known as the
Levenshtein distance [25]; we count the number of inser-
tion/elimination operations to change one word into the other.
For example, the distance between “abc” and “bcd” becomes
2 (erase ‘a’ and insert ‘d’). Once the learning process has been
Figure 6. Calculation of the distance between languages
Figure 7. Clustering languages (N = 25)
ﬁnished, each agent has his/her own grammar rules. In other
words, each agent can enumerate all the sentences he/she can
utter as E-language derived from I-language. Figure 6 depicts
an image of enumeration. Note that all the compositional
grammar rules are expanded into a set of holistic rules, which
do not include any variable, i.e., a rule consists of a sequence
of terminal symbols. Since the Levenshtein distance between
corresponding strings can be calculated, the average distance
normalized at the range from 0 to 1 comes into the distance
between languages.
Since agents independently invent languages, their acquired
languages are different from each other. In order to classify
agents into groups by the language similarity, we introduce a
clustering method, recognizing a cluster as a language group.
The relationship among languages is represented by a dendro-
gram shown in Figure 7. The vertical axis denotes the height
of the tree, which generally depicts the mergers or divisions
which have been made at successive level. We employed the
complete linkage method throughout the experiments. The
number of languages, therefore, depends on the cutting point of
the tree. In this case, the community is regarded as consisting
of three-language groups at the height of θ = 0.7.
IV.
EXPERIMENTAL RESULTS
Our purpose of these experiments is to examine how the
conﬁguration of networks affects the language learning by
infant agents. We expect language groups to emerge depending
on the types of networks and other conditions. Therefore, we
examine three types of networks; Scale-free, Small-world, and
2D-lattice networks. Scale-free and small-world networks are
drawn with BA [26] and WS [27] models, respectively. The
number of nodes is ﬁxed to N = 100. In BA networks, the
178
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

number of edges to add in each step is 2, and the power is set
to 0.2. The generation of multiple edges is allowed. In WS,
the number of neighborhood is 2, and the rewriting probability
is set to 1.
We measure (a) the number of language groups, (b) the
number of grammar rules and (c) expressivity of the gram-
mar. (a) is calculated by setting the threshold to distinguish
languages in the dendrogram to θ = 0.7. (b) denotes the
average number of grammar rules created in an agent at
Generation 100. (c) is deﬁned as the ratio of the number
of utterable meanings derived from the grammar rules to the
whole meaning space. Each infant agent receives 50 sentences,
while the meaning space is 100 (5 predicates × 5 possible ﬁrst
arguments × 4 possible second arguments). Therefore, agents
need to acquire a compositional grammar for high expressivity.
Since the exposure ratio α activates communication with
neighbors, we also expect local dialects to emerge through
communication. We parameterize 0 ≤ α ≤ 1, where the larger
the value α is, the more frequently the neighbors speak to the
infant agent. The situation α = 1 is an extreme case that the
infant’s parent does not speak to him/her at all, but neighbors
do.
Figure 8 shows experimental results. All the data are an
average of 50 trials. Since scale-free and small-world networks
are randomly drawn for each trial, no phenomenon peculiar to
a speciﬁc network appears in the results.
We classiﬁed languages into groups at 100th-Generation.
Cutting the dendrogram at θ = 0.7, we count the number of
language groups in the community. Figure 8a shows the change
in the number of language groups for every α. The labels
“ScFre-lg,” “SmWld-lg” and “Lat2D-lg” denote the numbers
of language groups in the scale-free, small-world, and 2D-
lattice networks, respectively.
Analyzing the results in Figure 8a, we can imply that
each agent speaks a language different from others, as long
as the number of language groups is almost the same as
the population of the community. The language exposure is
expected to make neighbors share a common language, but
the result became different from our expectation. The reason
is considered that the clipping process in ILM takes a chance
for chunking from non-compositional sentences uttered by
different agents. Finally, the most important thing is that there
is no big difference between three networks. The exposure
ratio α seems too effective to show minor difference between
them. Another reason comes from the number of agents,
which is insufﬁcient to show difference between the networks.
Despite employment of the clipping process, it was difﬁcult
to increase the number of agents more than 100 in a practical
computational time.
We also investigate acquired grammars. The number of
grammar rules and its expressivity are shown in Figures 8b
and 8c, respectively. Figure 8b shows that grammars become
non-compositional according to α. The sufﬁx ‘-cmp’ denotes
the number of compositional and holistic rules and the sufﬁx
‘-lex’ is the number of lexical rules. The decrease of lexical
rules implies holistic rules occupy agents’ knowledge, while
an ideal compositional grammar consists of one compositional
rule and ten lexical rules. Figure 8c is inevitably reﬂected by
the compositionality. The language exposure negatively affects
common languages. There is little difference between networks
70
80
90
100
α
Number of Language Groups
0.0
0.2
0.4
0.6
0.8
1.0
ScFre−lg
SmWld−lg
Lat2D−lg
(a) Language groups (θ = 0.7)
15
20
25
30
35
40
α
Number of Rules
0.0
0.2
0.4
0.6
0.8
1.0
ScFre−cmp
ScFre−lex
SmWld−cmp
SmWld−lex
Lat2D−cmp
Lat2D−lex
(b) Grammar rules
50
60
70
80
90
α
Expressivity
0.0
0.2
0.4
0.6
0.8
1.0
ScFre−exp
SmWld−exp
Lat2D−exp
(c) Expressivity (%)
Figure 8. Experimental results
in grammatical analysis as well as the result of language
groups.
The series of experimental results differs from our expecta-
tion and from the former studies [7]–[10]. However, we have
shown that an excessive string-clipping as well as a larger
value of exposure ratio may constrain the appearance of local
language community, independent of the shape of networks.
179
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

V.
CONCLUSION
In this paper, we proposed an agent-based model for lan-
guage contact. We employed Kirby’s iterated learning model
and complex networks. Languages are measured with the Lev-
enshtein distance of utterances, which enables us to show the
language divergence by the clustering. The language exposure
is expected to make neighbors communicate with each other.
Totally, we succeeded to implement a linguistic community
with learning agents connected with a social network. The
network model makes it possible to observe not only di-
achronic but also synchronic changes in grammar. We achieved
implementation of a large-scale, agent-based model where 100
processes of ILM run in parallel, which contributes to the
simulation study on language evolution.
Although we had been faced with a serious problem in
terms of constructing a network model with ILM, the new
method for a string clipping solved the combinatorial explo-
sion. We need to investigate the algorithm of a string clipping
and grasp why it works wrong for language contact.
In the near future, we plan to run more different types of
simulations toward the framework of for the diachronic change
in languages by language contact.
ACKNOWLEDGMENT
This work was partly supported by Grant-in-Aid for Young
Scientists (B) (KAKENHI) No. 23700310, and Grant-in-Aid
for Scientiﬁc Research (C) (KAKENHI) No.25330434 from
MEXT Japan.
REFERENCES
[1]
C. Lyon, C. Nehaniv, and A. Cangelosi, Eds., Emergence of Commu-
nication and Language.
Springer, 2007.
[2]
A. Cangelosi and D. Parisi, Eds., Simulating the Evolution of Language.
London: Springer, 2002.
[3]
E. J. Briscoe, “Grammatical acquisition and linguistic selection,” in
Linguistic Evolution through Language Acquisition: Formal and Com-
putational Models, T. Briscoe, Ed.
Cambridge University Press, 2002,
ch. 9.
[4]
M. A. Nowak, N. L. Komarova, and P. Niyogi, “Evolution of universal
grammar,” Science, vol. 291, 2001, pp. 114–118.
[5]
D. Crystal, Internet Linguistics: A Student Guide, 1st ed.
New York,
NY, 10001: Routledge, 2011.
[6]
J. Arends, P. Muysken, and N. Smith, Eds., Pidgins and Creoles.
Amsterdam: John Benjamins Publishing Co., 1994.
[7]
——, “Exposure dependent creolization in language dynamics equa-
tion,” in New Frontiers in Artiﬁcial Intelligence, ser. Lecture Notes in
Artiﬁcial Intelligence, A. Sakurai, K. Hasida, and K. Nitta, Eds., vol.
3609.
Springer, 2006, pp. 295–304.
[8]
——, “Self-organization of creole community in spatial language
dynamics,” in Proc. of 2nd IEEE International Conference on Self-
Adaptive and Self-Organizing Systems (SASO2008), Venice, 2008, pp.
459–460.
[9]
——, “Prediction of creole emergence in spatial language dynamics,”
in LATA 2009 (Proc. of 3rd International Conference on Language and
Automata Theory and Applications), ser. Lecture Notes in Artiﬁcial
Intelligence, A.H.Dediu, A.M.Ionescu, and C.Martin-Vide, Eds., vol.
5457.
Tarragona: Springer, 2009, pp. 614–625.
[10]
——, “Self-organization of creole community in a scale-free network,”
in Proc. of 3nd IEEE International Conference on Self-Adaptive and
Self-Organizing Systems (SASO2009), San Francisco, 2009, pp. 293–
294.
[11]
S. Kirby, “Learning, bottlenecks and the evolution of recursive syntax,”
in Linguistic Evolution through Language Acquisition: Formal and
Computational Models, T. Briscoe, Ed.
Cambridge University Press,
2002.
[12]
M. Delz, B. Layer, S. Schulz, and J. Wahle, “Overgeneralization
of Verbs - the Change of the German Verb System,” in Proc. of
EVOLANG9, 2012, pp. 96–103.
[13]
——, “Multilayered formalisms for language contact,” in Proc. of WS
on Constructive Approaches to Language Evolution, Kyoto, 2012, pp.
145–147.
[14]
R. Matoba, H. Sudo, M. Nakamura, S. Hagiwara, and S. Tojo, “Process
Acceleration in the Iterated Learning Model with String Clipping,”
International Journal of Computer and Communication Engineering,
vol. 4, no. 2, 2014.
[15]
N. Chomsky, Rules and Representations.
Oxford: Basil Blackwell,
1980.
[16]
D. Bickerton, Language and Species.
University of Chicago Press,
1990.
[17]
N. Chomsky, Knowledge of Language:Its Nature, Origin, and Use.
New York: Praeger, 1986.
[18]
J. R. Hurford, Language and Number: the Emergence of a Cognitive
System.
Oxford: Basil Blackwell, 1987.
[19]
S. Kirby, Function, Selection, and Innateness: The Emergence of
Language Universals.
Oxford University Press, 1999.
[20]
K. Smith and J. R. Hurford, “Language Evolution in Populations:
Extending the Iterated Learning Model,” in Proc. of ECAL03, 2003,
pp. 507–516.
[21]
D. Jamet, “A Morphological Approach of Clipping in English. Can
the Study of Clipping Be Formalized?” Lexis, no. 1, 2009, pp. 15–31.
[22]
A. Veisbergs, “Clipping in English and Latvian,” Poznan Studies in
Contemporary Linguistics, no. 35, 1999, pp. 153–163.
[23]
X. Castell´o et al., “Modelling language competition: bilingualism and
complex social networks,” in Proc. of EVOLANG7, 2008, pp. 59–66.
[24]
T. Gong, L. Shuai, M. Tamariz, and G. J¨ager, “Studying Language
Change Using Price Equation and P´olya-urn Dynamics.” PLoS One,
vol. 7, no. 3:e33171, 2012.
[25]
D. Jurafsky and J. H. Martin, Speech and Language Processing:
An Introduction to Natural Language Processing, Computational
Linguistics, and Speech Recognition.
Upper Saddle River, NJ, USA:
Prentice Hall PTR, 2000.
[26]
A.-L. Barabasi and R. Albert, “Emergence of scaling in random
networks,” Science, vol. 286, no. 5439, 1999, pp. 509–512.
[27]
D. J. Watts and S. H. Strogatz, “Collective dynamics of ’small-world’
networks,” Nature, vol. 393, 1998, pp. 440–442.
180
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

