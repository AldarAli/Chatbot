192
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 Energy Saving Techniques Comparison for Green Computing in Cloud Server 
        
       Bilal Ahmad  
      Sally McClean 
     Darryl Charles 
        Gerard Parr 
School of Computing, 
School of Computing,  
School of Computing,  
School of Computing, 
      Ulster University  
      Ulster University  
      Ulster University       University of East Anglia 
        Coleraine, UK                    Coleraine, UK                    Coleraine, UK  
      Norwich, UK 
                ahmad-b@ulster.ac.uk 
si.mcclean@ulster.ac.uk dk.charles@ulster.ac.uk 
     g.parr@uea.ac.uk 
 
 
Abstract - The IT industry has been revolutionized in the past few 
decades. Cloud Computing companies (Google, Yahoo, Gaikai, 
ONLIVE, Amazon and eBay) use large data centers which are 
comprised of virtual computers that are placed globally and 
require a lot of power cost to maintain. Demand for energy 
consumption is increasing day by day in IT firms. Therefore, 
Cloud Computing companies face challenges towards the 
maintenance of power costs. Energy consumption is dependent 
upon several factors, e.g., service level agreement, virtual 
machine selection techniques, optimization policies, workload 
types etc. We address a solution for the energy saving problem 
by enabling dynamic voltage and frequency scaling technique for 
gaming data centers. The dynamic voltage and frequency scaling 
technique is compared against non-power aware and static 
threshold detection techniques. This helps service providers to 
meet the quality of service and quality of experience constraints 
by meeting service level agreements. The CloudSim platform is 
used for implementation of the scenario in which game traces are 
used as a workload for testing the technique. Selection of better 
techniques can help gaming servers to save energy cost and 
maintain a better quality of service for users placed globally. The 
novelty of the work provides an opportunity to investigate which 
technique behaves better, i.e., dynamic, static or non-power 
aware. The results demonstrate that less energy is consumed by 
implementing a dynamic voltage and frequency approach in 
comparison with static threshold consolidation or non-power 
aware technique. 
Keywords- Energy Saving Technique; Dynamic Frequency 
Scaling; Static Threshold and Non-Power Aware Technique; 
Service Level Agreement; Quality of Service. 
I. 
INTRODUCTION 
Cloud Computing is one of the latest technologies that is 
growing across the globe rapidly. It is forming pillars for 
upcoming advancements in computing covering all aspects of 
parallel and distributed computing. All computing services are 
available as pay as you go services over the internet. With the 
era of globalization, computing is also being transformed into 
a model where service is provided based on user requirements 
instead of hosting them permanently [1]. This provides the 
industry with the liberty to reach the users doorstep for the 
provision of services [2]. Cloud Computing provides users 
with multiple advantages, e.g., services, resources, and 
developer tools. It facilitates researchers to develop, tests and 
implement their ideas. It also provisions them to use the latest 
services on different devices (tablets, phones, home appliances 
etc.). Cloud Computing has unmatchable advantages to its 
predecessors because of technological advancements, e.g., 
virtualization, storage, processing, memory, performance, low 
cost, ease of excess, mobility, high expansibility, reliability, 
and fast bandwidth etc. These advancements and innovations 
in the ﬁeld of cloud technology provisions the industries to 
have unlimited computational power while maintaining good 
quality of service (QoS). Cloud industries must maintain 
several service level agreements (SLAs) to meet high quality 
of service requirements from the user and service provider 
perspective. The service provider is also responsible for the 
availability of the resources whenever and wherever they are 
required by the user. This also presents challenge how energy 
consumption can be reduced while having minimum service 
level agreement violations (SLAVs). The IT industry can save 
energy and power cost using service-oriented architecture 
alongside cloud computing. Whether from the domain of 
parallel or distributed computing there are three major service 
models, namely Software as Service (SaaS), Platform as 
Service (PaaS), and Infrastructure as Service (IaaS) [3]. The 
corresponding large amount of data management and 
streaming leads to an increase in energy consumption.  All kind 
of services (gaming, internet of things, Big Data etc.) that are 
hosted over the cloud environment are maintained using large 
data centers that are placed globally. When observed closely, 
it can be seen that these servers are not running at their full 
performance, i.e., 100% utilization while remaining idle at 
other times. Therefore, an ample amount of energy is wasted 
to keep these servers running 24/7. This causes a major rise in 
cost and threat to the environment as large amount of carbon 
dioxide (CO2) is produced by these data servers [4]. 
Consequently, data centers are becoming unmaintainable. 
Therefore, a lot of work is being carried out, researchers are 
investigating different kinds of algorithms and techniques. 
There are different procedures in which this workload can be 
handled ranging from dynamic to static threshold and non-
power aware technique. 
Dynamic voltage and frequency scaling (DVFS) is a 
technique that works by dynamically controlling the data in the 
hosts. It reduces the use of underutilized resources by 
dynamically controlling the frequency parameter and uses 
different strategies to reduce energy consumption by shifting 
load to the underutilized servers dynamically. Therefore, for 
the implementation of DVFS one needs to understand different 
factors like frequency and static power consumption. 
Similarly, in static threshold technique upper and lower limits 
are set for the workload, and virtual machine allocation and 
relocation is done based on the defined threshold. In this virtual 

193
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
machine are selected depending on factors, e.g., minimum 
migration time, maximum correlation and minimum 
utilization. The amount of power that is being used in the data 
center can be managed by exploiting the trade-offs between 
service quality and service level agreement. Therefore, if 
virtualization is used in these big gaming server’s energy 
consumptions can be reduced, and better quality of service 
could be provided. The hosts that are under or overloaded can 
be relocated, and energy could be saved in this aspect.  Services 
provided by Cloud provisioners varies with time and have 
different workloads that require dynamic or static allocation of 
resources especially for Big Data Applications and Multiplayer 
Games. The migration of virtual machine can help in saving of 
energy, but it can also degrade the quality of service on the 
other hand. Tradeoff is required to be managed between user 
experience and quality of service. Therefore, such techniques 
are required to be implemented in gaming with awareness of 
dynamic and static workloads. This can help in the reduction 
of energy consumption while maintaining a quality of service 
and quality of experience [5]. 
There are several simulation tools utilised for this research 
purpose, each having their specific deﬁned use. All these tools 
have one thing common, namely they all use a stack-based 
design as cloud computing is a combination of internet, grid, 
and distributed computing. The stack-based design provides 
users with the ability to add their own designed code in the 
model. This helps in implementation of optimization 
techniques and management of resources for the improvement 
of quality of services. Despite all efforts and advancements in 
the field of cloud computing many of the users are still not able 
to take full advantage of the technology for the following 
reasons [6].  
1. 
Limited capability of devices (processing, speed, 
graphics) 
2. 
Network 
limitations 
(bandwidth, 
geographical 
location)   
3. 
Latency rates to central servers (slow internet 
connections) 
Therefore, better experimentation and development of an 
algorithm can help in saving energy cost and can increase 
proﬁts. For testing of new algorithms in IT industry researcher 
needs to have a secure platform. The selected platform should 
be fail-safe and must avoid risk to customers data privacy and 
data impairment [7]. Most cloud computing platforms are 
software based as it is very difﬁcult and expensive to set a 
cloud server for test and trials purposes for each researcher. 
For example, it is practically difficult for a researcher to use a 
data server consisting of 200 physical machines because of 
maintenance costs, (e.g., energy, space, expense, power, and 
cooling requirements) [8]. There is also no speciﬁc platform 
due to the following reasons: the relocation of the virtual 
machine, conﬁdentiality and data integrity, a need for energy 
management, and cost modelling [9]. The main purpose of 
carrying this research is, therefore, to find how resource 
optimization can be performed in the gaming data centres. In 
our work, we consider the following aspects of service quality: 
energy consumption and service level agreements, by using 
online gaming data in our experiments. In this paper, DVFS, 
Non-Power Aware and Static Threshold virtual machine 
consolidation technique will be tested and implemented for 
the improvement of energy consumption and SLAs. Better 
results are expected to be achieved using dynamic voltage and 
frequency technique as compared to a static threshold or non-
power aware technique; this hypothesis will be verified using 
real-time gaming workload.  
The rest of this paper is organized as follows, Section II 
describes the related work; Section III presents the basics 
about platform and techniques; Section IV addresses the 
simulation environment; Section V discusses performance 
analysis and provides a discussion of our approach while, 
conclusions and future work close the article. 
 
 
 
Figure 1: Overview of System Architecture 
II. 
RELATED WORK 
The concept of dynamic voltage and frequency scaling has 
been used by Ahmad et al. Tests were performed using 
gaming data. The results show that dynamic voltage and 
frequency scaling technique saves more energy as compared 
to non-power aware technique [1]. Work has been carried in 
the ﬁeld of cloud computing particularly relating to the cluster 
servers and virtualized servers. Here, the authors use a single 
system by implementing and comparing three different energy 
saving concepts, i.e., the supply voltage of underloaded 
servers is reduced, idle servers are left in sleep mode and 
thirdly, the two techniques are combined for analysis. The 
author proposes that DNS and changing voltages together 
provide better results for energy saving. However, the paper 
lacks cost comparison for quality of service matrices [10]. A 
solution is provided to save cost and to earn more proﬁt on a 
large data scale by managing the scheduling of heterogeneous 
machines with multiple users. This work is limited to just one 

194
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
quality of service metric, i.e., cost from the service provider 
perspective [11]. Another algorithm was designed to optimize 
energy by using the concept of multi objective workﬂow and 
dynamic voltage scaling. However, the user was given the 
ability to choose between the cost or energy criterion [12]. In 
the ﬁeld of computing, distributed computing provides the 
user with fault tolerance, organization, and support for 
resources. Typically, resources are allocated to the users based 
on load balancing technique. In this, all resources are allocated 
to the broker that is wholly responsible for the provisioning of 
resources when required [13].  
The author addresses the issue related to quality of service 
and service level agreement by using the energy constraint as 
a 
core 
parameter. 
Virtualization 
concept 
has 
been 
implemented in graphics card and central processing unit. The 
test helps in determining how latency factor can be improved 
by exploiting the game frames. The results predict that quality 
of service could be enhanced by exploiting a trade-off 
between different factors, e.g., data buffering, scalability, 
redundancy and game latency [14]. The virtualization concept 
has been used by the author for maintenance of quality of 
service. The idle virtual machines are migrated from servers 
for maintenance of load balancing. The technique suggests 
that energy can be saved in small online cloud servers. 
However, a live migration technique was used and can cause 
bottleneck in large and busy network [15]. In [16] the authors 
propose quality of service algorithms using scheduling 
policies. However, the work was related to virtualization 
mechanism only for large scale global data centers. Further 
work was carried out relating the energy saving mechanism to 
different kinds of workﬂow on the Green Cloud Platform 
using bi-objective scheduling to meet the quality of service 
matrices for energy consumption [17].  
On the other hand, some work  about energy saving has 
been carried using Big Data with single purpose applications 
[7]. The concept of virtualization has been implemented by 
the author using local regression robust migration algorithm. 
Work suggests that latency and service quality can be 
achieved in Big Data servers by using this virtualization 
technique. However, a tradeoff is required between quality of 
service and quality of experience [18].   
By looking at the related work it can be concluded that 
main research area involves single servers and unique tasks. 
However, these days’ cloud computing platforms like Gaikai, 
OnLive, and Amazon EC2 have servers that are using 
multipurpose applications that are dispersed geographically. 
However, there is a research gap in the ﬁeld of gaming 
especially for multiplayer games with users placed far apart 
from each other.  
III. 
BASICS ABOUT PLATFORM AND TECHNIQUES 
CloudSim is one of the platforms which provides QoS 
parameters such as: energy, cost model, latency, virtual 
machine characteristics, federation policy, and analyzing the 
network communication model. Based on this platform, 
several popular models have also been designed, namely 
iFogSim, Cloud Analyst, Network CloudSim and iCaroCloud. 
Therefore, it provides enough leverage for researchers to use 
it to perform tests and develop new models as required. 
CloudSim has a layered architecture which provides user with 
the ability to design and implement applications. It supports 
core functions, such as handling of events, creation of cloud 
servers, hosts, brokers, and virtual machines [19]. The 
CloudSim simulation layer supports creation of hosts under 
virtual machines, application execution and application 
monitoring. A researcher who wants to implement an 
application relating energy, hosts, VM and data centers will 
be doing at this level. This layer supports the SaaS platform 
and provides users with deﬁned quality of service levels with 
complex load reporting and application performance reports 
[20]. The topmost layer in the CloudSim architecture is where 
a user writes a code and it allows the user to deﬁne several 
virtual machines, hosts, data centers, brokers, tasks etc.     
 Therefore, it allows researchers to extend this layer and 
perform different tasks such as: generation of workload for 
monitoring designed experiments, designing of different 
cloud scenarios for robust testing and implementation of 
conventional applications in the cloud environment [19]. IaaS 
services can be simulated by extending different entities 
present in cloud environments such as data centers. Such data 
centers consist of many hosts which are assigned to more than 
one virtual machines depending upon the rules deﬁned by the 
service provider [21]. The data center can also manage more 
than one host (physical components representing the 
computing server) which further manages virtual machines. 
Host provisioning supports single and multiple core nodes. 
Similarly, virtual machine allocation creates virtual machine 
scenarios on hosts for storage and memory related tasks [6]. 
After modelling and designing of the application, it is 
allocated to a running virtual machine through a speciﬁc 
deﬁned procedure. The virtual machines required to host 
multiple applications are provided on a First Come First Serve 
basis depending upon different hardware factors (storage, 
memory, cores etc.). Therefore, simulation test scenarios 
relating to CPU cores are dependent upon factors such as time 
usage, space sharing policy or allocating virtual machines as 
and when required [22].     
 It can analyse the system and its components properties, 
e.g., the number of virtual machines, data centers, resource 
provisioning policies and hosts [23]. It has the capability to 
support single and multi-cloud environments. The platform 
has a wide implementation in computing industry for testing 
of energy management systems and resource allocation 
scenarios (HP Labs in USA). It provides support for 
simulation of virtualized data centers in the cloud environment 
(memory, storage, bandwidth, and virtual machines). Cloud 
Sim has number of compelling features that provide support 
and speed up the development process of the applications [24]. 
These features are discussed below: 
a) Fast Processing: It has very fast performance in 
implementation of different scenarios in cloud environment 
designed by the researchers for simulation and testing purpose 
[23].  
b) Flexible Approach: It provides a flexible approach 
for implementation of new ideas which are easily applicable 
on heterogeneous cloud environments, (e.g., Amazon, 
Microsoft Azure, Google) [24].  

195
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
c) Support for Modelling and Simulation: It provides 
support for modelling and simulation of data centres, virtual 
machines, and hosts in cloud environment  [25].  
d) Self-Contained Platform: It can model cloud servers, 
service brokers, resource provisioning and allocation policy 
[23].  
e) Network Support: Supports the implementation of 
network connections among different elements of the 
developed application in the simulation environment [24]. 
f) 
Federation Policy: Supports federation policy that is 
an application from one environment can be tested in another 
environment. This feature is very important for researchers as 
they can run and test code on multi-platforms.  
g) Availability of Virtualisation Engine: Provides the 
user with the ability to create, manage and test multiple virtual 
machines in the cloud environment [26].  
h) Ease of Allocation: It supports both time shared and 
space shared allocation of virtualised services in designed 
application [27].  
i) 
Energy and Cost Model: It provides support for the 
cost and energy development models depending upon the 
resources being used in the designed test environment [28].  
j) 
Service 
Area: 
It 
supports 
IaaS 
platform 
(infrastructure as service), i.e., a user pays only for the 
services he is using. It also provides users with the virtualised 
resources using the internet as a backbone communication 
medium.  
k) Data Type, and Availability: The platform is open 
source and data types are user defined [27].  
l) 
Programming Language and GUI: Java is used as a 
basic program development language and lacks the graphical 
user interface. Therefore, outputs are available in the form of 
java executed outputs [29]. 
 
 
 
Figure 2: Layered CloudSim Architecture Overview 
A. DVFS Technique 
CloudSim can calculate the power consumption of data 
centers using the DVFS technique. It uses the current metric 
for cloud host input and returns calculated power as an output. 
Energy consumptions model has been designed and the total 
power consumed can be  calculated during the designed 
experiment. CloudSim also provides developers with the 
capability for experimentation of dynamic scenarios, i.e., a 
different number of data centers or hosts can be created and 
deleted for testing unpredictable events in which users can 
join and leave the cloud application [30]. The DVFS scheme 
is limited to CPU optimization and adjusts the CPU power 
according to the workload that is being run on it. However, 
other components of the system, i.e., memory, storage, RAM, 
bandwidth and network interfaces, keep running on the same 
original frequency, and no scaling is applied to them. The use 
of Dynamic Power Management (DPM) can turn down the 
power consumption for all the components of the system. The 
CPU has number of states for frequency and voltage which 
suggests that it provide better power performance as compared 
to basic approach [31]. Thus, the powering up of a system will 
require a large amount of energy using DPM as compared to 
DVFS technique [19]. 
 
      𝑓𝑜𝑝(𝑖, 𝑘) ∈ {𝑓1(𝑖), 𝑓2(𝑖), … . . 𝑓𝑘(𝑖), … . , 𝑓𝑀𝐴𝑋(𝑖)} 
       (1) 
 
     𝑀𝐼𝑃𝑆𝑎𝑣𝑖𝑙𝑎𝑏𝑙𝑒(𝑖, 𝑘) =
𝑓𝑜𝑝(𝑖,𝑘)
𝑓𝑀𝐴𝑋(𝑖) 𝑀𝐼𝑃𝑆𝑀𝐴𝑋(𝑖)  
       (2) 
 
    𝑢𝑎𝑣𝑖𝑙𝑎𝑏𝑙𝑒.𝑐𝑝𝑢(𝑖, 𝑘) = 𝑀𝐼𝑃𝑆𝑎𝑣𝑖𝑙𝑎𝑏𝑙𝑒(𝑖, 𝑘)
𝑀𝐼𝑃𝑆𝑀𝐴𝑋(𝑖)
                             (3) 
    
CPU performance of the physical machine ‘i’ can be 
characterized as 𝑀𝐼𝑃𝑆𝑀𝐴𝑋(𝑖)  at its maximum frequency 
𝑓𝑀𝐴𝑋(𝑖). Real time available cycles, i.e.,  𝑀𝐼𝑃𝑆𝑎𝑣𝑖𝑙𝑎𝑏𝑙𝑒(𝑖, 𝑘) 
are dependent upon the𝑓𝑜𝑝(𝑖, 𝑘) which represent the current 
operational frequency of the CPU. ′𝑘′ represents the operating 
DVFS mode whereas, 𝑢𝑎𝑣𝑖𝑙𝑎𝑏𝑙𝑒.𝑐𝑝𝑢(𝑖, 𝑘)  is the maximum cpu 
utilization without performance degradation. All the CPU 
frequencies are available in the form of set ranging from 0 to 
𝑓𝑀𝐴𝑋(𝑖) [32].  
 
B. Static Threshold VM Consolidation Technique 
In this type of technique upper and lower threshold limits 
are defined for the CPU. The host under or overloading state 
is determined, and virtualization is performed. When this 
procedure is called it works by determining the current CPU 
utilization and differentiates it with the defined threshold 
level. On the basis of this differentiation, hosts are selected for 
relocation. The algorithm calculates the mean of ‘n’ latest 
CPU utilization and compares it with the defined threshold 
value. As a result, host over or underloaded state is 
determined. The resource provisioning is achieved by virtual 
machine relocation. The VMs that are allocated to the hosts 
initially are under or over utilized. Therefore, relocation helps 
in resource provisioning and helps in reduction of bottlenecks. 

196
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
The host node that is present on the systems is not turned off 
or sent in sleep mode. It remains active and helps in reduction 
of downtime and provides a better quality of service and can 
help in energy reduction. The virtual machines that are present 
on the system can be selected for relocation using three 
different approaches defined below; 
a) Minimum Migration Time Policy: The selection of 
the virtual present on the host is performed on the basis of its 
migration time. The virtual machine that requires minimum 
migration time is selected. The time is calculated on the basis 
of RAM and bandwidth using the following equation. 
 
      (
𝑅𝐴𝑀𝑢(𝑣)
𝑁𝐸𝑇𝑗 ) ≤ (
𝑅𝐴𝑀𝑢(𝑎)
𝑁𝐸𝑇𝑗 ),    𝑣 ∈ 𝑉𝑗 | ∀𝑎 ∈ 𝑉𝑗 
     (4) 
 
Vj represents total number of virtual machines that are 
associated with host ‘j’. Whereas, RAMu(a) is the RAM used 
by virtual machine (a) and NETj shows total available 
bandwidth of host ‘j’. 
b) 
Minimum Utilization: The virtual machines that are 
required to be relocated in under or over utilized hosts are 
selected on their utilization criteria. The virtual machine that 
are having minimum utilization are selected for migration 
from one host to another when required.    
 
 
Figure 3: Proposed System Architecture  
       c)  Maximum Correlation Policy: In this type, virtual 
machine is selected on the basis of maximum correlation. 
Virtual machine having higher value of resource utilization 
has higher probability of host overloading. Multiple 
correlation coefficient (MCC) is used for estimation of CPU 
utilization and intra virtual machine correlation. MCC 
coefficient has a squared correlation for dependent variable of 
real and predicted values [33].  
IV. 
SIMULATION 
 For the implementation and evaluation of the proposed 
experiments, CloudSim simulation platform is used to provide 
users with the ability to perform the desired tests. The 
experiments are carried out by using traces from a game as 
workload for the dynamic voltage frequency, non-power aware 
and static consolidation technique. The designed simulation 
consists of heterogeneous data centers consisting of 800 
physical hosts and 1000 virtual machines which are 
dynamically allocated by the broker. Half of the hosts are HP 
ProLiant ML110G4 (Xeon3040) and the other half are HP 
ProLiant ML110G5 (Xeon3075) servers. The system’s 
frequency characteristics are deﬁned based on how many 
instructions can be executed in one second (MIPs). Therefore, 
HP 
ProLiant 
ML110G4 
(Xeon3040) 
and 
ML110G5 
(Xeon3075) have MIPs rating of 1860 MHz and 2660 MHz, 
both being dual-core servers. The deﬁned system speciﬁcations 
are suited to the hardware requirements for the experimental 
workloads and are shown in Table I. 
TABLE I.  DETAILS OF THE SYSTEM PARAMETERS 
 
System (HP ProLiant) 
MIPs 
Rating 
Cores  
RAM 
Hard 
Disk 
ML110G4 (Xeon3040) 
1860 
MHz  
Dual 
32 GB 
1 GB 
ML110G5 (Xeon3075) 
2660 
MHz 
Dual 
32 GB 
1 GB 
 
In DVFS and NPA, no dynamic allocation of virtual 
machines is performed, and host power adjustment is done 
based on their CPU utilization. Whereas, when tested with 
static threshold concept the virtual machine selection and 
consolidation is performed on the basis of MTT, MU and MC 
policy. A ﬁxed MIPs value is provided having a value of 1000 
MIP per second for a virtual machine. The simulated model has 
a bandwidth rate of 1 Gbits per second and RAM 32 GB for 
each system. A ﬁxed deﬁned gaming workload is provided in 
this experiment that consists of traces from a popular 
multiplayer online game, namely World of Warcraft having a 
dataset size of 3.5 GB.  
The data set consists of traces from real data of the popular 
massively multiplayer online game, World of Warcraft 
(runtime of 1107 days, 91065 avatars, 667032 sessions, users 
located globally in 3 continents with different time zones) 
collected to analyze the quality of service parameters and 
consisting of game time, race attributes, current position, 
profession info, game position information, game level etc. 
[31]. It provides execution time of each host and energy is 
calculated based on power consumed by individual host. It uses 

197
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
time shared policy and rating of the processing elements is 
calculated by having millions of instructions per second. 
 
 
Figure 4: Flow Chart for the VM Consolidation 
The total MIPs, i.e., total execution time is the sum of all 
the MIPs from each processing element (PE). Here, it is 
assumed that all the processing elements have same rating in 
the used machine. The service level agreements are also 
required as it is necessary to maintain the quality of service 
matrices [34]. The detailed parameters are summarized in 
Table II. 
TABLE II.  DETAILED DESCRIPTION OF SYSTEM PARAMETERS 
 
Host MIPs 
Host RAM 
Host PE(s) 
1860 
32768 MBs 
02 
2660 
32768 MBs 
02 
 
  The reasoning behind the service level agreement 
violations (SLAV) time per active hosts is based on the 
observation that if there is an application that is managing the 
virtual machine migrations and it is busy with a host that has 
100% utilization, it will not be able to address other hosts 
waiting for service provisioning. Therefore, virtual machines 
are deprived of the desired performance level causing SLA 
violations [35]. The mathematical definitions and formula are 
as follow, 
 
                      𝑆𝐿𝐴𝑉(𝐻) =
1
𝐻(𝑛) ∑
𝑆𝐿𝐴𝐻(𝑡)𝑖
𝐴𝐻(𝑡)𝑖
𝑛
𝑖=1
     
 (5) 
 
  SLAV(H) is the violation of per unit time for active hosts, 
H(n) is number of hosts, SLAH(t)i represents the time duration 
that leads to service level agreement violations by reaching 
CPU utilization of 100% and AH(t)i is the total number of 
hosts(i) in the active state [32]. 
 
              𝑃(𝑣𝑚) =
1
𝑉𝑀(𝑛) ∑
𝑃𝑑(𝑘)
𝐶𝑝𝑢(𝑘)
𝑛
𝑖=1
                                (6) 
 
  P(vm) is the effect on the performance because of virtual 
machines migration, VM(n) represents the total number of 
virtual machines, Pd(k) represents the level of degradation in 
the service of a particular virtual machine when it is migrated, 
Cpu(k) represents the total utilization of CPU of a particular 
virtual machine. Therefore, whenever a cloud server is 
considered for service level agreement violations it always 
depends on the above two factors independently described in 
Equation (5) and Equation (6).  
The SLA level is the product of two matrices, i.e., how 
many SLAV there are per unit time of active hosts and how 
much of the performance degradation is because of virtual 
machine migration, Equation (7). Therefore, SLA is because of 
two factors: one is virtual machine migration and the other is 
when a host is overloaded resulting in SLAV as follows [35], 
 
                SLA = SLAV (H) × P (vm)  
                  (7) 
   
The overall performance of cloud servers can be analyzed 
by using the following equation,  
 
           Perf (DC) = Energy × SLAV  
                  (8)  
 The CPU time is calculated from the following formula, 
     
𝐶𝑃𝑈(𝑡) =
𝐶(𝐿𝑒)
𝑃𝑒 × (1.0 − 𝐶(𝐿𝑜))                               (9) 
 
 CPU(t) = CPU Time, PE = MIPs of one Processing 
Element, C(Le) = length of cloudlet, and C(Lo) = load of 
cloudlet. Here, MIPs represent how many instructions can be 
executed in one second, PE(x) the number of MIPs of one 
processing element, PE(y) represents MIPs of N number of 
hosts, 
 
      Total MIPs = PE(x) + PE(y)N(host)                   (10) 

198
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
       Cost per million instructions related to a resource can be 
calculated using Equation (11). In this, Cost(s) = cost per 
second and PE(MIPs) = calculating MIPs of one processing 
element.  
𝑀𝐼 =
𝐶𝑜𝑠𝑡(𝑠)
𝑃𝐸(𝑀𝐼𝑃𝑠)                                                  (11)      
        The required execution time can be calculated using the 
following equation. Whereas, Sys(t) is current time in 
millisecond, Exe(t) is system execution time and 1000 is the 
defined MIPs rating. 
 
𝑇𝑖𝑚𝑒 = 𝑆𝑦𝑠(𝑡) − 𝐸𝑥𝑒(𝑡)
1000
                                       (12)  
       
 Thus, energy consumed by each host, performance 
measure, CPU utilization, total execution time, and SLA 
violations count can be calculated by using the above equations 
[35]. Experimentation results are shown in Section V. 
V. PERFORMANCE ANALYSIS AND DISCUSSION 
This test calculates the energy performance across the data 
center in the given simulation environment. All the tests are 
carried out in the simulation environment, i.e., the CloudSim 
package which is conﬁgured using Eclipse Luna and Java IDE. 
DVFS, NPA and STVM techniques have been applied to 
analyze the gaming workload of the World of Warcraft 
multiplayer online game. The workload consists of data traces 
from servers which are collected over time of 1107 days. The 
above consolidation techniques are implemented for load 
management. Under or overloaded virtual machines are 
selected for relocation based on minimum migration time, 
maximum correlation and minimum utilization. A typical 
game workload has been provided for testing the behavior of 
the proposed techniques. The DVFS, NPA and STVM 
simulation models with the same speciﬁcations are used for 
power and service level agreement analyzation of same gaming 
workload. The main difference between the NPA and DVFS 
models lies in how resources are allocated to the hosts. All the 
parameters (RAM, bandwidth, storage, I/O file size etc.) are 
deﬁned however, for DVFS, resources are allocated based on 
dynamic voltages and frequency ﬂuctuations of the central 
processing unit for the active hosts.  
In the NPA model hosts consume the maximum amount of 
power, thus increasing the cost of services and causing loss of 
proﬁt for service providers. Figure 5 shows power 
consumption in the cloud environment with a ﬁxed number of 
hosts and MIPs using DVFS and NPA. For DVFS, the data 
show a linear trend for CPU power consumption as compared 
to NPA technique. The results are by way of a reality check 
and verify the theoretical concept that in DVFS, the CPU 
adjusts frequency according to the workload to minimize the 
power consumption and thus provides a linear trend. The hosts 
using DVFS technique for the same gaming data consume less 
energy as compared to the NPA technique. In NPA technique 
hosts are loaded to maximum values and consume more energy 
resulting in greater values of CO2 emissions. 
  
 
 
Figure 5: Consumption in a Data Center 
   
  Figure 6 shows different execution time by virtual 
machines using all three different techniques with the same 
workload and experimentation setup. It could be seen from the  
results that selection of the virtual in under or overloaded host 
takes minimum mean time. Whereas, migration of virtual 
machine from one host to another requires more time. 
Therefore, downtime in the network can be reduced if 
appropriate virtual machine relocation technique is selected. 
The difference in the amount of energy consumption, service 
level agreement and quality of service degradation can be seen 
through the results which are estimated based on CPU 
utilisation, static threshold and non-power aware technique.  
 
Figure 6: VM Execution Time for Each Host 
 
 
0
50
100
150
200
250
300
350
400
450
Energy (kW/Sec)
Hosts
Energy Comparion
DVFS
NPA
0
0.005
0.01
0.015
0.02
0.025
0.03
VM Selection
Mean Time
VM Relocation
Mean Time
Host Selection
Mean Time
Execution Time  (sec) 
DVFS
NPA
MMT
MC
MU

199
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
 
Figure 7: Number of VM Migration 
Comparison of three different approaches is carried out based 
on a service level agreement. Results show that minimum 
service level agreement degradation (SLAV) is achieved by 
using DVFS technique. Therefore, by using DVFS technique 
overall SLA violation can be reduced. The reduction in SLA 
performance degradation suggests that quality of service and 
quality of experience can be enhanced by using DVFS 
technique (Figure 8).  
 
 
 
Figure 8: Service Level Agreement Violation (SLAV) 
In STVM, virtual machines that have minimum utilisation 
have higher rate of selection as compared to maximum 
correlation or minimum migration time. Therefore, this shows 
that more energy is saved in threshold techniques when virtual 
machines are selected on the base of utilisation in underutilized 
hosts, as shown in Figure 9.  
 
 
Figure 9: Detailed Analysis of Proposed System. 
 
The results also prove that quality of service is directly 
proportional to service level agreements, i.e., if QoS is not 
observed for a certain amount of time then we have SLA 
violation. Thus, by using DVFS, performance can be 
improved, and energy consumption can be minimized resulting 
in a lot of cost saving for Big Data from commercial point of 
view (Figure 9). Whereas, STVM behaves better when 
workload is not of big size and is not changing dynamically. 
        
 
        Figure 10:  Analysis of Energy Consumption the Proposed System 
 
From the results, it can be seen that if the DVFS technique is 
used, the best results for energy utilisation are achieved and 
14% of energy could be saved in comparison to the NPA 
technique using the same gaming workload (Figure 10). 
Whereas, the static threshold gives minimum energy 
consumption when used with maximum correlation policy. 
0
5000
10000
15000
20000
25000
30000
35000
0
0
26634
2430
30188
Number of Migrated VMs 
Service Level Agreement
DVFS
NPA
MMT
MC
MU
VM Policies
0.00%
0.02%
0.04%
0.06%
0.08%
0.10%
0.12%
DVFS
NPA
MMT
MC
MU
0.04%
0.10%
0.07%
0.10%
0.06%
0.05%
0.11%
0.07%
0.11%
0.07%
Service Level Agreement
SLA Performance
Degradtion
Overall SLA
Violation
0
100
200
300
400
500
600
700
800
900
Energy
Concumption
Host Shutdown
Mean Time
Before Host
Shutdown
St Dev Time
Before Host
Shutdown
Detailed Analysis of Proposed System
DVFS
NPA
MMT
MC
MU
28%
42%
9%
9%
12%
P o w e r  C o n s u m p t i o n  C o m p a r i s o n  
DVFS
NPA
MMT
MC
MU

200
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
The reason static threshold performs minimum energy 
utilization is that the upper and lower threshold limits are 
defined in the system. Whereas, this approach will not be 
suitable with the dynamic workload environment. DVFS 
provides better trade-off for exploitation of SLAs per host for 
maintenance of quality of service and quality of experience. 
During the whole experiment DVFS uses fewer resources in 
the host when analyzed. Less energy consumption mean time 
and number of host shutdown are performed during the 
experimentation. These results show that overall the best 
quality of service can be achieved by implementing DVFS in 
gaming servers placed globally. 
VI. 
CONCLUSION AND FUTURE WORK 
The simulation tests that have been designed using 
CloudSim platform and are based on three different 
consumption approaches, i.e., dynamic voltage and frequency 
scaling, non-power aware and static threshold virtual machine 
consolidation technique. The same workload (game data) and 
data center speciﬁcations are set for testing which technique 
performs better for power saving and meet service level 
agreements. The workload provided demonstrates that 
dynamic voltage frequency scaling saves more energy as 
compared to general non-power aware or static virtual 
machine consolidation approach for dynamic workloads. It 
has less SLA violations which is important for maintaining 
QoS and QoE. Static virtual machine consolidation technique 
has a better ratio of service level agreement violation when 
used with maximum correlation virtual machine policy for 
workloads allocated statically. CloudSim provides the ability 
to test the same workload scenario on two different 
approaches, i.e., static and dynamic. When compared to 
dynamic voltage and frequency technique, static virtual 
machine consolidation provides better results for small 
workloads under static allocation. In real-world for large 
cloud gaming servers, it is difficult to maintain upper and 
lower workload limits. Therefore, the effectiveness of this 
approach becomes impractical in dynamic environments.  By 
using this simulation environment, a researcher can 
experiment and determine the amount of resources required, 
(e.g., the number of cloudlets, bandwidth, RAM, cost etc.) for 
maintaining the quality of service. Therefore, from the 
simulation results, it can be verified that cloud gaming data 
centers with the proposed DVFS technique can yield less 
energy consumption while fulﬁlling service level agreements 
for maintaining a good quality of service leading to better 
quality of experience (QoE) for users placed globally. 
     In the future, this work will be enhanced, and better 
ways and techniques to save energy will be explored for Big 
Data, Internet of Things and Gaming data centers. Other 
extensions include that an analysis between number of users 
and submitted jobs can be carried out. This can help in energy 
improvement and optimization by carrying out failure-
analysis in cloud environment. Along with this, an effort will 
be carried out to merge this workload in current CloudSim 
framework and make it public for research societies around 
the world. 
ACKNOWLEDGMENTS 
The authors would like to acknowledge partial 
support from the BT-Ireland Innovation Centre (BTIIC) 
and Ulster University. 
REFERENCES 
1. 
Ahmad, B., et al., Analysis of energy saving 
technique in CloudSim using gaming workload, in 
Proceedings of the Ninth International Conference 
on Cloud Computing, GRIDS, and Virtualization, 
IARIA. 2018. 
2. 
Sidana, S., et al. NBST algorithm: A load balancing 
algorithm in cloud computing. in 2016 International 
Conference on Computing, Communication and 
Automation (ICCCA). 2016. 
3. 
Rawat, P.S., et al. Power consumption analysis 
across heterogeneous data center using CloudSim. in 
2016 3rd International Conference on Computing for 
Sustainable Global Development (INDIACom). 
2016. 
4. 
Luo, H., et al. The dynamic migration model for 
cloud 
service 
resource 
balancing 
energy 
consumption and QoS. in The 27th Chinese Control 
and Decision Conference (2015 CCDC). 2015. 
5. 
Arroba, P., et al. DVFS-Aware Consolidation for 
Energy-Efficient Clouds. in 2015 International 
Conference 
on 
Parallel 
Architecture 
and 
Compilation (PACT). 2015. 
6. 
Varasteh, A. and M. Goudarzi, Server Consolidation 
Techniques in Virtualized Data Centers: A Survey. 
IEEE Systems Journal, 2017. 11(2): p. 772-783. 
7. 
Tian, W., et al., Open-source simulators for Cloud 
computing: Comparative study and challenging 
issues. Simulation Modelling Practice and Theory, 
2015. 58, Part 2: p. 239-254. 
8. 
Prazeres, C. and M. Serrano. SOFT-IoT: Self-
Organizing FOG of Things. in 2016 30th 
International Conference on Advanced Information 
Networking and Applications Workshops (WAINA). 
2016. 
9. 
Kliazovich, D., P. Bouvry, and S.U. Khan. DENS: 
Data 
Center 
Energy-Efficient 
Network-Aware 
Scheduling. 
in 
Green 
Computing 
and 
Communications (GreenCom), 2010 IEEE/ACM Int'l 
Conference on & Int'l Conference on Cyber, 
Physical and Social Computing (CPSCom). 2010. 
10. 
Burge, J., P. Ranganathan, and J.L. Wiener. Cost-
aware scheduling for heterogeneous enterprise 
machines (CASH&#x2019;EM). in 2007 IEEE 
International Conference on Cluster Computing. 
2007. 
11. 
Cao, F., M.M. Zhu, and C.Q. Wu. Energy-Efficient 
Resource Management for Scientific Workflows in 
Clouds. in 2014 IEEE World Congress on Services. 
2014. 

201
International Journal on Advances in Intelligent Systems, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/intelligent_systems/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
12. 
Buyya, R., C.S. Yeo, and S. Venugopal. Market-
Oriented Cloud Computing: Vision, Hype, and 
Reality for Delivering IT Services as Computing 
Utilities. 
in 
2008 
10th 
IEEE 
International 
Conference on High Performance Computing and 
Communications. 2008. 
13. 
Beloglazov, A. and R. Buyya, Optimal online 
deterministic algorithms and adaptive heuristics for 
energy 
and 
performance 
efficient 
dynamic 
consolidation of virtual machines in Cloud data 
centers. Concurr. Comput. : Pract. Exper., 2012. 
24(13): p. 1397-1420. 
14. 
Zhao, Z., K. Hwang, and J. Villeta, GamePipe: A 
virtualized cloud platform design and performance 
evaluation. 2012. 1-8. 
15. 
Shea, R., et al., Cloud gaming: architecture and 
performance. IEEE Network, 2013. 27(4): p. 16-21. 
16. 
Varasteh, A. and M. Goudarzi, Server Consolidation 
Techniques in Virtualized Data Centers: A Survey. 
IEEE Systems Journal, 2015. PP(99): p. 1-12. 
17. 
Yannuzzi, M., et al., A New Era for Cities with Fog 
Computing. IEEE Internet Computing, 2017. 21(2): 
p. 54-67. 
18. 
Oikonomou, E., D. Panagiotou, and A. Rouskas, 
Energy-aware Management of Virtual Machines in 
Cloud Data Centers, in Proceedings of the 16th 
International 
Conference 
on 
Engineering 
Applications of Neural Networks (INNS). 2015, 
ACM: Rhodes, Island, Greece. p. 1-6. 
19. 
Calheiros, R.N., et al., CloudSim: a toolkit for 
modeling and simulation of cloud computing 
environments 
and 
evaluation 
of 
resource 
provisioning algorithms. Softw. Pract. Exper., 2011. 
41(1): p. 23-50. 
20. 
Tso, F.P., et al. The Glasgow Raspberry Pi Cloud: A 
Scale Model for Cloud Computing Infrastructures. in 
2013 IEEE 33rd International Conference on 
Distributed Computing Systems Workshops. 2013. 
21. 
Keller, G., et al. DCSim: A data centre simulation 
tool. in 2013 IFIP/IEEE International Symposium on 
Integrated Network Management (IM 2013). 2013. 
22. 
Horvath, T., et al., Dynamic Voltage Scaling in 
Multitier Web Servers with End-to-End Delay 
Control. IEEE Transactions on Computers, 2007. 
56(4): p. 444-458. 
23. 
Atiewi, S. and S. Yussof. Comparison between 
Cloud Sim and Green Cloud in Measuring Energy 
Consumption in a Cloud Environment. in 2014 3rd 
International Conference on Advanced Computer 
Science Applications and Technologies. 2014. 
24. 
Garg, S.K. and R. Buyya. NetworkCloudSim: 
Modelling 
Parallel 
Applications 
in 
Cloud 
Simulations. in 2011 Fourth IEEE International 
Conference on Utility and Cloud Computing. 2011. 
25. 
Wickremasinghe, B., R.N. Calheiros, and R. Buyya. 
CloudAnalyst: A CloudSim-Based Visual Modeller 
for Analysing Cloud Computing Environments and 
Applications. in 2010 24th IEEE International 
Conference on Advanced Information Networking 
and Applications. 2010. 
26. 
Song, J., et al., FCM: Towards fine-grained GPU 
power management for closed source mobile games, 
in International Great Lakes Symposium on VLSI 
(GLSVLSI). 2016. p. PP: 353-356. 
27. 
Adhikary, T., et al., Energy-Efficient Scheduling 
Algorithms for Data Center Resources in Cloud 
Computing, in 2013 IEEE 10th International 
Conference on High Performance Computing and 
Communications & 2013 IEEE International 
Conference 
on 
Embedded 
and 
Ubiquitous 
Computing (HPCC_EUC). 2013. p. PP: 1715-1720. 
28. 
Wadhwa, B. and A. Verma, Energy saving 
approaches for Green Cloud Computing: A review, 
in 2014 Recent Advances in Engineering and 
Computational Sciences (RAECS). 2014. p. PP:1-6. 
29. 
Shuja, J., et al., Survey of Techniques and 
Architectures for Designing Energy-Efficient Data 
Centers. IEEE Systems Journal, 2016. 10(2): p. 507-
519. 
30. 
Long, S. and Y. Zhao. A Toolkit for Modeling and 
Simulating Cloud Data Storage: An Extension to 
CloudSim. in 2012 International Conference on 
Control 
Engineering 
and 
Communication 
Technology. 2012. 
31. 
Lei, Y.-T.L.a.K.-T.C.a.Y.-M.C.a.C.-L. World of 
Warcraft Avatar History Dataset. Proceedings of 
ACM Multimedia Systems 2011 2011  Feb]; 
Available 
from: 
http://mmnet.iis.sinica.edu.tw/dl/wowah/. 
32. 
Arroba, P., et al., Dynamic Voltage and Frequency 
Scaling-aware dynamic consolidation of virtual 
machines for energy efficient cloud data centers. 
Concurrency 
and 
Computation: 
Practice 
and 
Experience, 2017. 29(10): p. e4067-n/a. 
33. 
Theja Perla, R. and S.K.K. Babu, Evolutionary 
Computing Based on QoS Oriented Energy Efficient 
VM Consolidation Scheme for Large Scale Cloud 
Data Centers, in Cybernetics and Information 
Technologies. 2016. p. 97. 
34. 
Ahmed, A. and A.S. Sabyasachi. Cloud computing 
simulators: A detailed survey and future direction. in 
2014 IEEE International Advance Computing 
Conference (IACC). 2014. 
35. 
Wang, J.V., et al. A Stable Matching-Based Virtual 
Machine Allocation Mechanism for Cloud Data 
Centers. in 2016 IEEE World Congress on Services 
(SERVICES). 2016. 
 

