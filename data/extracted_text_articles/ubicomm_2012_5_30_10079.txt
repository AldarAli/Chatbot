Techniques for Interacting With Small Devices  
 
Javier Oliver 
Department of Computer Engineering 
University of Deusto 
Bilbao, Spain 
javier.oliver@deusto.es 
Begoña García 
DeustoTech 
University of Deusto 
Bilbao, Spain 
mbgarciazapi@deusto.es
 
 
Abstract— Digital mobile devices keep reducing their size as 
time goes by. The limiting factor is no longer battery size or 
electronics miniaturization, but the dimensions of the input 
and output hardware devices that mediate the communication 
between the user and the machine. In this paper, the 
difficulties of interacting with small screen devices are 
underlined and some of the most promising techniques to 
address this issue are explained. These techniques include the 
use of magnetic field detectors, mobile phone cameras to track 
movement, tangible interfaces and voice controlled virtual 
joysticks. 
Keywords-interaction techniques; small device interaction  
I. 
 INTRODUCTION 
Recent evolution of digital mobile devices has produced 
smaller and smaller products, to the point that some of them 
can be hidden inside the clothes or even implanted under the 
skin. The limiting factor of this miniaturization is no longer 
electronic design or even battery size. The main issue that 
conditions this constant reduction in the dimension of these 
mobile devices is the size of the input and output hardware 
that is required to implement the user interface [1]. The size 
of a screen should be big enough for a user with normal 
visual acuity to read the text, and buttons in a keyboard 
should be big enough for a normal sized finger to press them. 
Although the size of the mobile devices keeps shrinking, 
the fingers of the users, or the size of the text they can read 
keep constant. If the use of smaller and smaller digital 
products wants to be fostered, existing technologies should 
be used creatively, and new technologies should be 
developed to implement a new generation of user interfaces 
that can overcome the difficulties of interacting with small 
mobile devices. Our main purpose in the rest of this paper is 
to describe some of the most promising innovative 
interaction techniques, giving a small sample of what 
interaction might look like in the near future.  
In the following section we define what we understand 
by small mobile devices. In Section III we describe some of 
the techniques that can be used to interact with these small 
devices, and we present our conclusions in Section IV. 
II. 
WHAT ARE SMALL MOBILE DEVICES? 
Cellular phones are by far the most common mobile 
device available nowadays. It has been estimated that there 
were about 6.000 million of these devices in the world at the 
beginning of 2012 [2]. Most cellular phones fall under the 
category of small mobile devices because their design is not 
optimized for interaction and this produces a number of 
difficulties when the interaction tasks are carried out, namely 
low readability of small screens and hard to push small 
buttons. 
But, even smaller mobile devices have been marketed, 
making the problem of fluid and effortless interaction even 
harder. For example, many devices designed for open air 
activities, such as GPS systems or training computers have 
screens of about 2.5’’ in diagonal. Siftables are small blocks 
that include wireless communication capabilities, sensing 
and a small screen of about two inches in diagonal. This 
design offers a whole new set of tangible user interface 
techniques to interact with digital information, making use of 
our high dexterity in manipulating digital objects [3]. Some 
of the smallest devices that have been designed are the 
Telebeads: electronic wearable jewelry objects that can be 
used as mnemonic aids and communication appliances to 
help in the managing of social network data. These systems 
have screens as small as a fraction of an inch [4]. 
III. 
ASSORTED INTERACTION TECHNIQUES FOR SMALL 
DEVICES 
According to Fitt’s Law [5], target size and interaction 
time are inversely proportional. But in the small touch 
screens of portable devices, real state is very limited, so how 
big should targets be for a comfortable and fast interaction? 
Interaction guidelines offer different views on the subject 
[5]. In the iPhone Human Interface Guidelines [5] it is 
suggested that the minimum target size should be 44x44 
pixels. Windows Phone UI Design and Interaction Guide 
says that the minimum should be 26x26 pixels, and Nokia’s 
Developers Guidelines propose 28x28 pixels. However, the 
average index finger is between 45 and 57 pixels wide, more 
than any of the above recommendations. 
In the case of very small touch screens, the so called fat 
finger problem is exacerbated [6]. The finger occludes most 
of the screen real state and interaction is greatly hampered. 
In these situations, it has been proposed to move the touch 
sensitive hardware to the back of the device, so that the 
screen is visible and the position of the finger is shown by a 
small cursor [7]. Traditional touch screen pointing 
techniques try to alleviate screen occlusion by using offset 
129
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

cursors or a method called Shift where the user is shown a 
representation of the area occluded by the finger in a free 
region of the small screen. The exact position of the finger is 
shown by means of a cursor, and this helps performing the 
positioning task. When the Shift technique was compared 
with the interaction on the back of the device, it was shown 
that Shift did not work for screen sizes below one inch 
diagonally, 
whereas 
back-of-device 
interaction 
was 
successful almost independently of screen size. 
Another approach to the interaction with a small screen is 
based on a magnetic field detector. Located behind the 
screen, this device is capable of very accurate positioning 
and leaves the screen completely visible [8]. In this study, a 
1.5 inch screen was used, with a resolution of 280 x 220 
pixels. The magnetometer used was capable of providing an 
angular accuracy of about two degrees at a cost of five US 
dollars. Users wore a small magnet in the index finger, which 
provided a useful range of about 10 cm, for a total active 
area of about 300 cm2. This setup increased the operational 
area offered by the original 1.5 inch screen by a factor of 
more than 50. 
TinyMotion is a software that uses the camera of a mobile 
phone as an input device. TinyMotion analyzes in real time a 
series of images taken by the mobile phone camera and 
extracts information about the movement of the phone. In 
order to evaluate the applicability of this approach, the 
TinyMotion team developed a number of applications and 
video games, all of which were controlled by moving the 
mobile phone is various ways [9]. In an application called 
Mobile Gesture, the user presses the OK button before 
writing a character, and then presses the #  button to indicate 
the end of the writing process. Writing in this case means 
moving the mobile phone with the camera on to recognize 
the strokes of the character.  
The recognizer code can detect western characters, 
punctuation symbols and more than 8,000 Chinese and 
Japanese characters. It takes about 20 ms to recognize a 
western character and about 40 ms to recognize one of the 
Chinese or Japanese characters (the hardware used is a 
Motorola v710 mobile phone, an unmodified model bought 
in 2005). TinyMotion has undergone several evaluations. To 
begin with, an informal usability test was carried out with 13 
users. The results were very encouraging because the system 
was found to be very responsive. Many different 
backgrounds for the camera were used, and most of them 
worked very well. Even pointing the mobile phone camera to 
the blue sky gave good results.  
The only failures to detect the movement were those with 
very extreme lighting conditions or very rapidly changing 
backgrounds such as a dark room, the surface of a computer 
screen switched off or pointing the camera through the 
window of a moving car. Some users even found it more 
convenient not to move the mobile phone and move the other 
hand in front of the camera instead. One of the most 
innovative interaction techniques for small screen is the 
combination of sensing technology and tangible user 
interfaces used in the Siftables project [3]. Tangible user 
interfaces are based on the notion of providing physical 
handles for digital objects, thus being able to access digital 
information by manipulating common objects. In some 
instances of tangible user interfaces, the system projects 
graphics onto the handles and in others, these handles are 
simply used to control more conventional graphical user 
interfaces. Some of the advantages of tangible user interfaces 
include [3]: 
 
• 
Less significant cognitive requirements than an 
equivalent graphical user interface. 
• 
Faster interaction 
• 
Two handed input of data is supported, although 
multi touch screens offer this capability for more 
conventional interfaces. 
 
The other technology that that Siftables project uses is the 
Sensor Network User Interfaces (SNUI). These are sets of 
elements capable of communication and sensing, that can 
have an organized behavior and be manipulated so that they 
conform a tangible user interface to access digital data. 
Siftables are small square tiles of about 36 mm per side and 
10 mm thick. Each of them has a small color screen, an 
accelerometer, a set of infrared transceivers, a battery and an 
RF radio. With this hardware, the Siftables can sense their 
own motion, and also contacts with another objects. They 
can detect movements like elevation, tilting or vibration. 
They can also detect other tiles other tiles situated close by. 
The communication capabilities of the system allows 
tiles to share information with other tiles or with a central 
computer located in the vicinity. The capabilities of the 
Siftable system are allowing the development of new 
interaction techniques in the domain of SNUI’s, analogous to 
the more familiar metaphors of graphical user interfaces: 
 
• 
Shaking or piling several of the elements at the same 
time could be interpreted as classifying them as 
belonging to the same group. 
• 
Putting several tiles together could form a bigger 
screen to show large documents. 
• 
Shaking vertically could mean yes and shaking 
horizontally could mean no. 
 
A prototype photo sorting application has been developed 
by the Siftable team to illustrate the possibilities of SNUIs 
[3]. 
The hands can be avoided altogether in the interaction 
with very small screens, thus eliminating the fat finger 
problem. The Vocal Joystick is a system that allows the 
control of a pointing device by means of the voice. The 
technique can be used for onscreen selection, arbitrary point 
navigation and path following as required in drawing 
applications and videogames [10]. Vocal Joystick can 
recognize verbal and non verbal vocalizations, and other 
sound characteristics, such as loudness and pitch, and 
transform them into movements of the cursor. The process is 
continuous, and mouse movements are generated without 
delay. The Vocal Joystick can be implemented in an average 
personal computer and only requires a microphone and a 
sound card. The sound produced by the user is continuously 
130
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

monitored, and the movement of the pointer is immediately 
generated. Vowel quality depends on the articulation 
configuration of the mouth, and depending on the ability of 
the user two methods can be used: four direction or eight 
direction modes. In addition to the above, the system can 
also recognize a number of short sounds that can be used as 
trigger actions to perform functions such as a mouse click. 
There is a standard set of sounds, but adaptation to particular 
users is also possible, improving the performance of the 
interaction. As users become more experienced with Vocal 
Joystick they can reach interaction speeds comparable to 
hand operated joysticks. 
A technology that completely eliminates the need of a 
screen is the use of passive magnetic tags. Magnetic tracking 
does not need a direct line of sight, but in the case of motion 
capture devices, they require active sensors, and this involves 
complex equipment and a wire connection between the 
detectors and the processing unit. On the other hand, passive 
magnetic tags can be powered by radiofrequency energy sent 
by the base station, and if attached to common everyday 
objects, these can be tracked, their orientation can be 
detected, and they can even respond to other actions such as 
pressure, finger position, etc. With this passive magnetic tag 
technology, and a few plastic objects, an interesting digital 
musical instrument with a tangible user interface has been 
developed. 
A total of sixteen small plastic objects have been used to 
control a music producing application. Some of the objects 
have three orthogonal magnetic tags to monitor orientation in 
addition to distance to a reference point. 
Each tagged object produces a different output when it is 
close to the receptor. An attached computer generates the 
corresponding  MIDI messages that are sent to a number of 
music synthesizers. In addition to the sound output, the 
computer also generates background graphics. Although tags 
working in neighboring frequencies may show small 
interferences, all of the tagged objects can be used together.  
The general public has had a very positive reaction to this 
system, and because of the simple interface, its use is very 
intuitive. The output of the system is somehow limited, so 
improvements are being made to turn this enjoyable demo 
into a full fledged musical instrument. 
Another approach that can be taken is that of implanted 
interfaces. Just as pacemakers or hearing aids can be 
surgically placed underneath the skin, small input and output 
devices can also be permanently implanted under the skin of 
the users. In a recent work, it has been proposed that user 
interfaces could be implanted to allow users to perform 
simple interactions with their own bodies. In this study, a 
simulated implant was made to obtain an initial qualitative 
feedback on the use of implanted interfaces. A device with 
three inputs (button, tap sensor and pressure sensor) and 
three outputs (LED, vibration motor and piezo buzzer) was 
placed on the left arm of four volunteers and covered with 
silicon artificial skin. The volunteers had to perform some 
simple everyday tasks, such as taking a bus or asking for 
directions to go to the post office. As a secondary task, they 
had to pay attention to the output of the device and answer 
with the appropriate input control. In general, the participants 
considered that the device was easy to use, and all felt that 
the vibration motor was the easiest output channel to 
perceive. All users were able to see the blinking LED when 
looking at it, even in direct sunlight. Although the authors 
conclude that it is feasible to operate small and simple user 
interfaces implanted under the skin, they put forward some 
challenges associated with the use of these interfaces. 
Regarding input, it has to cross the skin, so the use of sound 
or light is somehow limited. Also, accidental operation of the 
controls has to be considered and avoided. Output is 
normally visual, auditory or tactile, and the bandwidth is 
small. For example, in the case of visual stimuli, typically a 
small LED flashing through the skin is used. Tactile 
feedback could be specially appropriate because it would not 
be perceived by anyone except the users. 
IV. 
CONCLUSION AND FUTURE WORK 
The continuous reduction in size of digital mobile 
devices is presenting new challenges to interaction designers. 
When input and output hardware is reduced beyond a certain 
point, traditional interaction techniques have to be applied in 
creative ways or new interaction techniques have to be 
developed [11-15] to meet communication needs between 
the user and the system. In this paper, several innovative 
techniques have been described, including the Shift method, 
magnetic field detectors, the use of mobile phone cameras to 
track movement, tangible interfaces and voice controlled 
pointer management systems. Future work should widen the 
spectrum of the techniques reviewed here, and provide some 
kind of categorization. 
Educators and practitioners should be familiar with these 
new trends in interaction with small devices to prepare future 
professionals for the interface design scenarios that they will 
meet in their careers. 
 
REFERENCES 
[1] 
Ni T. and Baudisch P. Dissapearing mobile devices. 22nd Annual 
Symposium on User Interface Software and Technology , October 4-
7, Victoria, British Columbia, Canada, 2009, pp. 101-110.  
[2] 
Whitney L. 2011 ends with almost 6 billion mobile phone 
subscriptions. CNET News, January 4, 2012. Available at: 
http://news.cnet.com/8301-1023_3-57352095-93/2011-ends-with-
almost -6 -billion-mobile-phone-subscriptions/, 2012 [retrieved: 
september, 2012]. 
[3] 
Merrill D., Kalanithi J., and Maes P. Siftables: Towards Sensor 
Network User Interfaces. In Proceedings TEI'07, 2007. 
[4] 
Labrune J.B. and Mackay W. Telebeads: Social Network Mnemonics 
for Teenagers. In Proc IDC '06, 2006. 
[5] 
Anthony T. Finger-Friendly Design: Ideal Mobile Touchscreen 
Target Sizes. Smashing Magazine. Available at: http://uxdesign. 
smashingmagazine.com/2012/02/21/finger-friendly-design-ideal-
mobile-touchscreen-target-sizes/, 2012 [retrieved: september, 2012]. 
[6] 
Siek K.A., Rogers Y., and Connelly K.H. Fat Finger Worries: How 
Older and Younger Users Physically Interact with PDAs. In Proc. 
INTERACT’05, 2005. 
[7] 
Baudisch P. and Chu G. Back-of-Device interaction Allows Creating 
Very Small Touch Devices. In Proc CHI 2009.  
[8] 
Harrison C. and Hudson S.E. Abracadabra: Wireless, High-Precision, 
and Unpowered Finger Input for Very Small Mobile Devices. 22nd 
131
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

Annual Symposium on User Interface Software and Technology , 
October 4-7, Victoria, British Columbia, Canada, 2009, pp. 121-124. 
[9] 
Wang J., Zhai S., and Canny J. Camera phone based motion sensing: 
interaction techniques, applications and performance study. In 
Proceedings of the 19th annual ACM symposium on User interface 
software and technology, 2006. 
[10] Harada S., Landay J.A., Malkin J., Li X., and Bilmes J.A. The Vocal 
Joystick: Evaluation of Voice-based Cursor Control Techniques. In 
Proc ASSETS, 2006, pp. 197-204. 
[11] Holtz C., Grossman T., Fitzmaurice G., and Agur A. Implanted User 
Interfaces. CHI´12, Austin, Texas, 2012, pp. 503-512. 
[12] Butler A., Izadi S., and Hodges S. SideSight: multi-"touch" 
interaction around small devices. Proceedings of the ACM 
Symposium on User Interface Software and Technology (UIST '08), 
2008. 
[13] Cho S.J., Murray-Smith R., and Kim Y.B. Multi-context photo 
browsing on mobile devices based on tilt dynamics. In Proceedings of 
the 9th international conference on Human computer interaction with 
mobile devices and services, 2007, pp. 190-197. 
[14] Paradiso J.A., Hsiao K.Y., and Benbasat A. Tangible music interfaces 
using passive magnetic tags. In Proceedings of the 2001 Conference 
on New interfaces for Musical Expression, 2001. 
[15] Yatani K. and Truong K.N. SemFeel: A User Interface with Semantic 
Tactile Feedback for Mobile Touch-screen Devices. 22nd Annual 
Symposium on User Interface Software and Technology , October 4-
7, Victoria, British Columbia, Canada, 2009, pp. 111-120. 
 
 
 
 
 
 
132
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-236-3
UBICOMM 2012 : The Sixth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

