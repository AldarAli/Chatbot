Human Friendly Autonomous Robot using Dempster-Shafer Sensor Fusion and 
Velocity Potential Field Control 
 
                    Dan-Sorin Necsulescu, Yu Hu,                                                 Jurek Sasiadek 
Department of Mechanical Engineering,                       Department of Mechanical and Aerospace Engineering, 
University of Ottawa,                                                                   Carleton University, 
Ottawa, Canada                                                                          Ottawa, Canada, 
         e-mail: necsu@uottawa.ca,  yhu061@uottawa.ca                                e-mail: jurek_sasiadek@carleton.ca 
 
Abstract— In this paper, a human friendly autonomous robot, 
was presented. Navigation of this robot applies sensor fusion 
technique based on Dempster-Shafer method and velocity 
potential field. The controller of the autonomous mobile robot 
is designed to lead the robot toward the goal while avoiding 
collisions in complex environments and maintain human safety 
as a first priority. The approach is based on Dempster-Shafer 
sensor fusion of signals from sonar and passive infrared 
sensors to allow the robot to identify human presence. A 
velocity potential field robot controller is formulated for the 
case of avoiding collisions while giving higher priority to 
collision avoidance with humans as opposed of objects. 
Simulations and experiments illustrate the performance of the 
approach in extreme situations. 
Keywords- human friendly robots; Dempster-Shafer evidence 
theory; sensor fusion; human safety.  
I. 
 INTRODUCTION  
Recently, with rise of the cost of labor, robots play 
more important roles than before. To become more widely 
applied in the real world, it is necessary that the mobile 
robots have much more artificial intelligence to work in 
more complex environments, which might include humans 
or vehicles with unknown and unpredictable motions, such 
as factory or home environments. To achieve this goal, the 
robot has to be able to identify and distinguish the obstacles, 
and the human and avoid collisions with both of them. 
Furthermore, the robot should always put human safety as 
its first consideration.  
In this paper, a sensor fusion approach based on 
Dempster-Shafer evidence theory is combined with velocity 
potential field approach used for robot control. The 
justification of this choice is the ability of Dempster-Shafer 
evidence theory to include supporting evidence, refuting 
evidence and an uncertainty interval, that permit a suitable 
use of expert knowledge regarding autonomous robots 
navigation issues. The detection area of the robot is 
subdivided into several zones. The sensor fusion approach is 
employed to fuse both uncertain observations of sonar 
sensors and passive infrared sensors and to estimate the 
probability of being occupied by a human in each zone [1] 
[2]. Then, a human friendly mobile robot navigation 
approach is used for robot motion control. The controller is 
based on the velocity potential field method (VPF), which is 
used to lead the robot moving to the goal avoiding the 
obstacles [3]. For the novel controller, in this case an 
improvement of the VPF allows the robot to avoid the 
human a first priority and only afterwards the obstacles. 
This novel VPF approach was not applied before for robot 
collision avoidance.  Dempster-Shafer evidence theory 
permits to fuse outputs from various sensors and then 
provide the VPF controller with the required distances 
between the robot, the obstacles and the goal. The paper 
focuses on the novel results presented in the thesis [14]. 
The paper presents in Section 2 the Demster Shafer 
evidence theory used for sensor fusion. Section 3 focuses on 
the model used for the calculation of the avoidance distance 
to obstacles. Section 4 presents velocity potential field 
approach for robot navigation in the presence of humans. 
Simulation results are the topic of section 5, while 
experimental results are presented in Section 6, followed by 
conclusions in Section 7. 
 
II. 
SENSOR FUSION METHOD BASED ON DEMPSTER-
SHAFER EVIDENCE THEORY  
In this paper, two types of sensors are used to help the 
robot sense the environment. The first one is the Passive 
Infrared Sensor (PIR sensor), which senses the heat emitted 
by humans. PIR sensor is, however, not sufficiently 
accurate. Performing hundreds of experiments, in at about 
10 % of the experiments the human sensor did not work 
well. Besides, PIR sensor might identify warm air generated 
by a heat source as coming from a human. The second type 
of sensor is the ultrasonic sensor. It emits high frequency 
sound waves to the objects and then receives them to 
determine how far they are. Since human is the first priority 
of the robot, information which is collected by different 
kinds of sensors is combined to identify human with a 
higher probability [4]-[9].  
Dempster-Shafer evidential theory (D-S theory) was 
chosen to support the probability calculation given its ability 
when the sensors contributing information cannot associate 
a 100 % probability to their output decisions. The algorithm 
captures and combines whatever certainty exists in the 
92
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-405-3
ICAS 2015 : The Eleventh International Conference on Autonomic and Autonomous Systems

object-discrimination capability of the sensors. Knowledge 
from multiple sensors about events (called propositions) is 
combined using D-S theory to find the intersection or 
conjunction of the propositions and their associated 
probabilities [4], using 
 
             







0
0
;
)
) (
(
1
)
) (
(
)
(
j
i
k
k
j
i
B
A
j
i
C C
B
A
j
i
k
A m B
m
m A m B
C
m


                (1) 
         
where Ai and Bj are the focal elements of mA and mB, 
respectively. mA and mB are the Basic Probabilities 
Assignments (BPA) which are combined while Ck are the 
focal elements of the combined BPA [12]. 
The frame of discernments is in this case a set of cells in 
the occupancy grids as shown in Figure 1. The eight grids are 
able to cover the human sensors detection area. 
 
 
Figure 1. Human Sensors Detection Area 
 
Both ultrasonic sensors and PIR sensors keep watching   
and each sensor contributes by assigning its BPA over its 
own frame of discernments [10] [11].  
The D-S theory is used to combine two focal elements 
BPAs. 
 
Sensor fusion is an iterative process with a time step chosen 
of 0.05s, described as follows: 
1. Read the ultrasonic sensor outputs looking for the 
presence of a human. (assign the BPAs to each grid) 
2. Read the human sensors outputs.(assign the BPAs to 
each grid) 
3. Combine the final results of both kinds of sensors using 
D-S theory after one time step.  
4. Calculate the probability (the combined BPAs) that 
quantifies how probable the grid is to be occupied by a 
human. 
5. Compare the probability value with the setting value 
and make the decision whether there is a human in the 
grid and then generate a corresponding response. 
6. Initialize the BPA of the occupied grid to zero to restart 
the all progress. 
7. Repeat from step one again until the mission is 
completed. 
 
III. 
CALCULATION OF THE AVOIDING DISTANCE 
 
The robot, the human zone and obstacle zone, shown in 
Figure 2, were built around the robot to avoid collision even 
in the worst case when the robot has the least amount of 
time to avoid a head-on collision. We denote the human 
zone from Figure 2 as A0 the obstacle zone as A1 and the 
work area excluding A0 and A1 as A. The shapes of the zones 
were designed based on the positions of the sensors. 
 
  
 
Figure 2. The Human Zone and the Obstacle Zone  
 
The human zone was designed for human avoidance. In 
the worst human avoidance case, the human and the robot 
were moving toward each other.  
Once the robot detected the human in its human zone, it 
turns into a different direction, from the original direction 
toward the goal, to avoid the collision.  Given the human 
size and velocity of the human and the robot   
 
                                 
m
rh
4.0

                                 (2) 
 
the calculation is based on 
           

 

  

at
h
mry
a
mr
mry
a dt r
t
t
V


)
(
1
             (3)      
            
h
mry
mry a
a
mr
r
t
a
t
t
V


 


  
2
1
2
) 1
(
          (4) 
 
 
where  is the maximum velocity of the robot, ta is the time 
that robot needs to accelerate from zero velocity to 
V mr
 in 
Y-direction, 
 mry
 is the distance that the robot is moving in 
Y-direction,  is the radius of the robot, amry is the robot 
93
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-405-3
ICAS 2015 : The Eleventh International Conference on Autonomic and Autonomous Systems

acceleration magnitude in Y-direction, 
 1t
 is the total time 
of the avoidance and rh is the radius of human active area. 
Finally, the radius of the human zone r0 should satisfy the 
constraint 
                             
1
0
mrx
hx
r




                           (5) 
 
where 
 hx
 and 
 mrx1
is the human and robot moving 
distance in X-direction, respectively. Based on the above 
model, calculation for the worst case, gives the radius of the 
human zone  of 30cm.  
 
IV. 
VELOCITY POTENTIAL FIELD METHOD WITH THE 
CONSIDERATION OF HUMAN PRESENCE 
In this paper, the velocity potential field method [3], 
obtained from velocity potentials defined in hydrodynamics, 
is modified for planning a path which avoids collisions with 
the human and the obstacles, such that, during the 
avoidance, the robot will avoid the human before it starts to 
avoid the obstacles. The robot is guided by its velocity 
commands which are given by its navigation controller 
(Figure 3). 
 
 
 
Figure 3. The Attractive, Rotation and Repulsive Velocity Commands 
 
The distance between robot and goal 
dgoal
 is 
 
            
2
2
)
(
)
(
x
xG
y
yG
d goal




              (6) 
              
where 
 refers to the goal and
 to the center and 
robot. 
The goal radius is  
 
                                 
m
g rad
 0.4
 
 
VPF approach, presented in detail in [3], results in velocity 
commands for the robot controller.  
The attractive velocity function is defined as [3]   
 
                
)
(1
2
rad
goal
g
d
mr
a
V
V






                    (7)  
where 
V mr
 is the maximum velocity of robot. 
The repulsive velocity function is defined as [3]  
 
              
rad
obs
obs
d
obs
rep
d
V





1
0.5
                      (8) 
       
where
dobs
 is the distance between the obstacle and the 
robot which is obtained by the sensors. 
Here, we also created a repulsive velocity function for the 
human, which has a larger gain than the repulsive velocity 
function for obstacle 
 
              
rad
obs
obs
d
obs
rephuman
d
V





1
0.7
               (9)   
             
 Both 
dobs
1
 and 
rad
obs
obs
 d

 in the equation cause a sustained 
increase of the repulsive velocity if the robot keeps 
approaching the obstacles or human, which means that the 
closer the obstacle or human is, the larger is the repulsive 
velocity.   
The rotation velocity function is defined as [4]  
 
                  
rad
obs
obs
d
obs
rot
d
V





1
0.5
                (10)    
     
where dobs  is the distance between obstacle and robot which 
is obtained by the sensors. 
 
The resultant velocity command is given by 
 
 
      






















1
1
0
0
,
,
,
,
,
A
AP
P
V
V
V
A
A P
AP
P
V
V
V
A
AP
P
V
V
V
A
P
V
V
obs
r
rot
rep
a
h
h
r
roth
reph
a
h
r
roth
reph
a
r
a
sum
      (11) 
 
where Va,, Vrep, and Vrot, are the attractive, repulsive and 
rotation velocity commands, respectively. The Vreph, and Vroth 
are velocity commands regarding a human. A is the whole 
map area while A0 and A1 represent the human zone and 
obstacle zone. 
 
94
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-405-3
ICAS 2015 : The Eleventh International Conference on Autonomic and Autonomous Systems

V. 
SIMULATION RESULTS FOR HUMAN FRIENDLY ROBOT 
 
In order to test if the robot is able to avoid collisions as 
expected, simulations were carried out using MATLABTM.  
Figure 4 presents the results of the simulation with the 
following symbols: 
 
The blue rectangle with the red circle: the human  
The black rectangle: the obstacle 
The blue polygon with light blue circle: the robot 
 
In these simulations, the robot did not know it will face 
a dangerous situation. The aim of creating such an extreme 
situation is to test if the robot controller has the ability to 
avoid the human and the obstacle while it still has time and 
space.  
 
 
 
 
(a) 
 
 
(b) 
 
(c) 
 
(d) 
 
 
(e) 
 
(f) 
  
95
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-405-3
ICAS 2015 : The Eleventh International Conference on Autonomic and Autonomous Systems

 
(g) 
 
(h)                
 
Figure 4.  Higher Human Priority (Two Humans and One Obstacle) 
 
As shown in Figure 4, the robot turned around and ran 
away from the two human and, afterwards avoided the 
obstacle moving in a direction where there is no danger. 
This illustrated that the proposed VPF based controller 
permits to avoid fixed obstacles and avoid humans and 
arrive to the designated goal. 
 
VI. 
EXPERIMENTAL RESULTS 
 
Experiments were carried out for the same scenario as in 
simulations, in which the robot suddenly faces the moving 
human and obstacles at the same time and, during the 
avoidance, the robot should detour human and obstacles 
with different priorities, such that the human always has a 
higher priority in avoiding collision than obstacles.  
For this scenario, two humans kept approaching the 
robot located in front and left side and the wall was in its 
right side. The purpose to design this situation is to test if 
the robot is able to avoid humans first (even has a risk to 
collide with an obstacle) and only afterwards to start 
avoiding the obstacles. As we can see from the Figure 5, the 
robot avoided the two humans first even while moving in 
the direction of the wall. Then, the robot avoided the wall 
and ran away from the danger of collision. It can be noted 
that the simulation results from Figure 4 (a)-(f) correspond 
to the snapshots of experimental results from Figure 5 (a)-
(f). Experimental results confirmed that the proposed 
combination of Dempster-Shafer sensor fusion with velocity 
potential field based controller successfully avoided fixed 
obstacles and humans in moving towards the designated 
goal. 
 
 
(a) 
 
(b) 
 
(c) 
 
96
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-405-3
ICAS 2015 : The Eleventh International Conference on Autonomic and Autonomous Systems

(d) 
 
(e)     
  
(f)    
Figure 5 Two Human and One Obstacle Avoidance  
 
VII. CONCLUSIONS 
 
The paper presents a novel approach to robot-human 
collision avoidance using Dempster-Shafer method based on 
evidence theory. This methodology allows to integrate 
signals from multiple sensors. This supplies more reliable 
distance estimation to a velocity potential field controller.  
The results presented in this paper show that the new 
human friendly mobile robot navigation controller based on 
Dampter-Shafer sensor fusion is able to lead the robot, 
avoid collisions with the obstacles and humans while always 
maintaining human safety as its first priority.  
In the extreme case, when the robot does not have 
enough room to avoid the collision with the human and an 
obstacle, it chooses to protect the human even if in the 
process it might collide with an obstacle. 
      In the work presented in this paper, the robot has only 
one infrared sensor to sense the human. Although the sonar 
sensor helps the infrared sensor it cannot distinguish human 
on its own. This method is good enough for human 
detection, but it cannot ensure complete security for the 
human. In the future, another sensor based on infrared 
camera will have to be added to the robot so that human 
movements can be further recorded by the robot through 
processing pictures captured by the infrared camera and thus 
the human safety can be further improved. 
 
REFERENCES 
[1] G. Benet, F. Blanes, J. E. Simó, and P. Pérez, “Using infrared 
sensors for distance measurement in mobile robots,” Rob. 
Auton. Syst., vol. 40, no. 4, Sep. 2002, pp. 255–266. 
[2] G. Tuna, V. C. Gungor, and K. Gulez, “An autonomous 
wireless sensor network deployment system using mobile 
robots for human existence detection in case of disasters,” Ad 
Hoc Networks, vol. 13, Feb. 2014, pp. 54–68. 
[3] E. Pruner, “Control of Self-Organizing and Geometric 
Formations”, A thesis presented for the degree of Master of 
Applied Science in engineering, University of Ottawa, 2013, 
pp. 66-73. 
[4] L. A. Klein, “Sensor and data fusion: a tool for information 
assessment and decision making”, Spie.org,  2004, pp. 149-
180. 
[5] R. R. Murphy, “Dempster–Shafer Theory for Sensor Fusion 
in Autonomous Mobile Robots”, IEEE Transactions on 
Robotics and Automation, vol.14, No.2, Apr 1998, pp. 197-
206. 
[6] L. Zeng, “Design and control of human friendly robots”, A 
thesis presented for the degree of Ph.D of Applied Science in 
engineering, McMaster University, 2010, pp. 79-92. 
[7] H. M. Choset, “Principles of robot motion: theory, algorithms, 
and implementations.” MIT press, 2005, pp. 301-322. 
[8] G. M. Bone and L. Zeng,“Collision avoidance for 
nonholonomic mobile robots among unpredictable dynamic 
obstacles including humans,” 2010 IEEE Int. Conf. Autom. 
Sci. Eng, Aug. 2010, pp. 940–947. 
[9] Y. Lu, L. Zeng and G. M. Bone, “Multisensor System for 
Safer Human-Robot Interaction,” 2005 IEEE International 
Conference, April, 2005, pp. 1767–1772. 
[10] G. Feng, X. Guo and G. Wang, “Infrared motion sensing 
system for human-following robots,” Sensors Actuators A 
Phys., vol. 185, Oct. 2012, pp. 1–7. 
[11] M. Kam, X. Zhu and P. Kalata, “Sensor Fusion for Mobile 
Robot Navigation,” Proceedings of the IEEE 1997, vol. 85, 
no. 1, 1997, pp108-119.  
[12] T. Ali and P. Dutta, “Methods  to Obtain Basic Probability 
Assignment in Evidence Theory”, Int. J. of Computer 
Applications, Jan. 2012, pp. 46-51. 
[13] Y. Guo, A. Song, J. Bao, H. Tang and J. Cui, “A Combination 
of Terrain Prediction and Correction for Search and Rescue 
Navigation”, Int. J.of Advanced Robotic Systems, No. 3, 2009,  
pp. 207-214. 
[14] Y. Hu, “Human Friendly Robot”, MASc Thesis, University of 
Ottawa, 2014, pp. 2-14. 
 
 
 
 
97
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-405-3
ICAS 2015 : The Eleventh International Conference on Autonomic and Autonomous Systems

