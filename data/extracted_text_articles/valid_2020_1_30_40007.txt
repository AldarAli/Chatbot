Toward an Exact Simulation Interval for
Multiprocessor Real-Time Systems Validation
Joumana Lagha, Jean-Luc B´echennec, S´ebastien Faucou and Olivier-H Roux
Universit´e de Nantes, ´Ecole Centrale de Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France
Email: ﬁrstname.lastname@ls2n.fr
Abstract—In order to study the schedulability of complex real-
time systems, simulation can be used. Of course, to achieve formal
validation of schedulability, simulations must be run long enough
such that the schedule repeats. An upper bound on the length
of the simulation that is valid for a very wide class of systems
running on top of identical multiprocessor platforms is given
in a previous work. It is known that this bound is pessimistic.
In this paper, we derive a characterization of the exact bound
for the same class of systems and describe an algorithm for
its computation. We use it to quantify the pessimism of the
upper bound on a set of synthesized systems. We also give some
directions to explore the complexity vs. tightness trade-off for
this problem.
Keywords–Real time scheduling; Multiprocessor; Simulation.
I.
INTRODUCTION
The correctness of real-time software systems does not
depend only on the value of results, but also on the date they
are produced. A real-time software is usually composed of a set
of recurring tasks that spawns jobs. Each job must be executed
within a given deadline. Scheduling algorithms are used to
allocate execution time to jobs. Schedulability analysis is used
to validate that the resulting schedule meets all deadlines.
For well-deﬁned classes of systems, efﬁcient schedulability
tests exist [1]. For complex systems, that are not in one of
these classes, it is sometimes possible to rely on simulation.
More precisely, this is possible if the context does not yield
scheduling anomalies, ie. when response times variations are
monotonic with regards to other system parameters. In this
paper, we will assume work under this hypothesis and refer
the reader to Section 7 of [2] for a discussion on this point.
To achieve formal validation of the system, the simulation
must be run on an interval long enough such that the schedule
repeats. If the scheduler is deterministic and memoryless, then,
if in this interval all jobs meet their deadline, it can be safely
concluded that the system is schedulable. The length of this
interval can be discovered during simulation by comparing
each new state to those encountered so far. The main drawback
of this approach is that it requires to memorize all states,
so it quickly becomes intractable. An alternative consists of
computing an upper bound B on the length of the simulation
interval and then simulate the system on [0, B). In 2016,
Goossens et al. [2] proposed an upper bound that is valid
for a wide class of systems: periodic asynchronous tasks with
arbitrary deadlines and structural constraints (such as prece-
dence, mutual exclusion and self-suspension) scheduled on top
of an identical multiprocessor platform by any deterministic
and memoryless algorithm.
It is known that this bound is pessimistic, especially
because it does not take into account the processing power
of the platform. Before looking for possible improvements,
it is interesting to evaluate how pessimistic it is. To answer
this question, we derive a characterization of the exact bound
for the same class of systems and describe an algorithm for
its computation. The algorithm relies on an enumeration of
the state space and has factorial time complexity. On a set of
synthetic systems, we ﬁnd out that the pessimistic bound is
at least twice too long when the number of tasks is greater
than three times the number of processors. Based on the exact
formulation of the bound, we also suggest directions to explore
tightness vs. complexity trade-off for this problem.
The paper is organized as follows: in Section II, we review
related works. In Section III, we deﬁne notations and expose
the state-of-the-art. In Section IV, we give a characterization of
the exact bound and derive an algorithm for its computation. In
Section V, we compare the state-of-the-art and the exact bound
on a set of synthetic benchmarks to quantify its pessimism.
In Section VI, we present possible directions to explore the
tightness vs. complexity trade-off before concluding the paper.
II.
RELATED WORKS
The ﬁrst result on simulation intervals is obtained by Leung
and Merrill [3] in 1980, with Omax + 2H (where Omax is
the maximum activation offset and H is the hyperperiod) as
an upper bound for independent asynchronous task systems
with constrained deadlines scheduled with a ﬁxed-task priority
algorithm. The same bound was later deemed valid for systems
with arbitrary deadlines by Goossens and Devillers [4]. For
multiprocessor platforms, Cucu and Goossens [5] derive in
2007 a result for independent asynchronous task systems (a
task system is asynchronous if at least two tasks have their
ﬁrst activation on different dates) with arbitrary deadlines
scheduled by a global ﬁxed-task priority algorithm. They also
prove that any feasible schedule generated by a deterministic
and memoryless scheduler is ultimately periodic. In 2012, Baru
et al. [6] proposed an upper bound on the simulation interval
for asynchronous task systems with constrained deadlines sub-
ject to simple precedence constraints running on an identical
multiprocessor platform and scheduled by any deterministic
and memoryless algorithm. The same interval is used and
tuned for ﬁxed-job priority schedulers and independent tasks
in N´elis et al. [7]. The most recent and general result is the one
proposed by Goossens et al. [2] in 2016, that applies to a very
large class of systems: asynchronous task systems with arbi-
trary deadlines, subject to structural constraints (precedence,
mutual exclusion, self suspension), scheduled on an identical
7
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-830-3
VALID 2020 : The Twelfth International Conference on Advances in System Testing and Validation Lifecycle

multiprocessor platform by any deterministic and memoryless
scheduler. This bound has a low complexity, is safe but not
always tight. For many systems, it is very pessimistic and too
big to be used for practical purpose. Thus, in this paper, we aim
at deriving an exact bound. To do so, we relax the constraint on
the complexity of the computation. Dues to its complexity, our
bound can be computed for a restricted class of systems. For
these systems, it provides an exact simulation interval. While
deriving an exact bound, we also highlight how to explore
the complexity vs. precision trade-off, paving the way to the
development of low complexity yet precise bounds.
III.
UPPER BOUND ON THE SIMULATION INTERVAL [2]
A. Model, notations, and deﬁnitions
N is the set of integer numbers. Let v be a vector of NN.
∀i ∈ [1, N], v[i] is the ith element of the vector v. We note that
0 the null vector: ∀i ∈ [1, N]. 0[i] = 0. The usual operators
+, −, ×, < and = are used on vectors of NN and are the point-
wise extensions of their counterparts in N. {x | P(x)} is set
set of all x such that predicate P(x) is true. [x | P(x)] is the
list of all x such that predicate P(x) is true.
Let Θ = {τ1, τ2, . . . , τN} be a set of N asynchronous
periodic tasks, where each task τi is the 4-tuple of non negative
integers ⟨Oi, Ci, Ti, Di⟩, where Oi is the release time of the
ﬁrst job of τi, Ci is the execution time of τi, Ti is the period of
τi, and Di its deadline. We assume that periods and deadlines
are unrelated (i.e., Di can be smaller than, equal to, or greater
than Ti). H = lcmτi∈Θ{Ti} is the hyperperiod of Θ.
At runtime, each task τi spawns an inﬁnite sequence of
jobs τi,1, τi,2, . . .. Job τi,j enters the system at date ai,j =
Oi+(j−1)Ti. It must be executed before date di,j = ai,j+Di.
Let S(t) be the state of the system at date t. It is deﬁned
by S(t) = (Crem1(t), . . . , Cremn(t), Ω1(t), . . . Ωn(t)), where
Cremi is the remaining work to process for the jobs of task τi
activated prior to t, and Ωi(t) is a decrementing clock counting
the time until the next release of a job of τi.
Θ is executed on a platform composed of m identical pro-
cessors. Jobs are scheduled by a deterministic and memoryless
scheduler (see below). A given job is executed sequentially
(no inner parallelism) but can migrate from one processor to
another during its execution. It is assumed that there is no
penalty to migrate from one processor to another. Moreover, it
is assumed that a job cannot start its execution while all jobs
of the same task activated before are not ﬁnished.
Deﬁnition 1 (Feasible schedule): A feasible schedule for
Θ is an inﬁnite schedule such that every job τi,j is fully
executed in its time window [ai,j, di,j].
Deﬁnition 2 (Deterministic and memoryless scheduler):
A scheduler such that the scheduling decision at time t is
unique and depends only on the current state of the system.
Deﬁnition 3 (Valid simulation interval): Interval [0,B) is a
valid simulation interval for Θ scheduled with a determin-
istic and memoryless scheduler if and only if ∃(t1, t2) ∈
[0, B]2.
t1 ̸= t2 ∧ S(t1) = S(t2).
The model has two features that make its schedulability
analysis complex: arbitrary deadlines and asynchronous acti-
vation. Both are sources of backlog between hyperperiods.
Deﬁnition 4 (Backlog): The backlog βi(t) of a task τi at
date t is deﬁned as the remaining work to be processed for
jobs of τi activated strictly before t.
In the following, we assume that all hypotheses formulated
in this section hold.
B. Ruling out asynchronous activations
To rule out the complexity arising from asynchronous task
activation, Goossens et al. observe that a simple transformation
can be applied to an asynchronous task set Θ to obtain a
synchronous task set Θ′ such that the length of the simulation
interval of Θ′ (considering any deterministic and memoryless
scheduler) is not smaller than that of Θ. For each task τi =
⟨Oi, Ti, Di⟩, the transformation yields τ ′
i = ⟨0, Ti, Oi + Di⟩.
The idea is that all feasible schedules of Θ are also feasible
schedules of Θ′. Thus, if a simulation is run for a duration long
enough to validate any feasible schedule of Θ′, it is also long
enough to validate any feasible schedule of Θ. A detailed proof
is given in [2].
Given this result, we can now reason as if we only had
to handle synchronous task sets. Thus, we can now give a
trivial upper bound on the backlog of a task at the end of a
hyperperiod: ∀q > 0. βi(qH) ≤ (Oi + Di) − Ti. From now
on, we note βmax
i
= max{0, (Oi + Di) − Ti} the maximum
backlog for task τi at any date t = qH in any feasible schedule,
and we note βmax = maxτi∈Θ βmax
i
.
C. Extension to structural constraints
The approach used to rule out asynchronous activation
can be used to extend the result to systems with structural
constraints. Structural constraints are deﬁned as “a relation
between jobs or subjobs, forbidding some execution orders,
preemptions, or insuring a minimal delay between the end
of a job (or sub-job) and the start of another one” [2].
Let Θ a system with structural constraints. Let Θ′ denote
the same system where all structural constraints have been
removed. Obviously, all feasible schedules of Θ are also
feasible schedules of Θ′. Thus, a valid simulation interval for
Θ′ is also a valid simulation interval for Θ.
D. Deriving the bound
In any non trivial synchronous system such that at least two
tasks have different periods, the search for the upper bound of
a valid simulation interval can be reduced to solutions of the
form B = qH (with q a positive integer) by deﬁnition of H
(the hyperperiod of Θ) since local clocks are equal in S(0)
and S(qH).
By deﬁnition, for any non negative integer q, Cremi(qH) =
βi(qH), so in any feasible schedule, Cremi(qH) ≤ βmax
i
.
Then, we can bound the number of different states of the
system in any feasible schedule at the end of a hyperperiod:
|{S(qH) | q ∈ N}| ≤ Q
i∈[1,N] (βmax
i
+ 1). Using the
assumption that the scheduler is deterministic and memoryless,
it is sufﬁcient to run the simulation long enough to cover a
number of hyperperiods equal to the number of different states
at the end of a hyperperiod. If the schedule is not feasible then
a deadline miss will be discovered. If the schedule is feasible,
8
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-830-3
VALID 2020 : The Twelfth International Conference on Advances in System Testing and Validation Lifecycle

either the same state will have been encountered twice, or all
states will have been explored. This yields the bound:
B0 = H ×
Y
i∈[1,N]
(βmax
i
+ 1)
(1)
E. Non tightness of B0
As claimed by the authors in [2], the bound is safe but
not tight. This is illustrated in Figure 1. Let us consider a
system with two tasks τ1 and τ2 such that 0 < βmax
1
< βmax
2
,
running on a monoprocessor platform. The size of the state
β1
β2
βmax
1
βmax
2
β1 + β2 = max{βmax
1
, βmax
2
}
Figure 1. Illustration of the non-tightness of the bound: states in the red
dotted area do not belong to any feasible schedule.
space considered by the bound B computed above is the
number of points with integer coordinates in the rectangle
of width βmax
2
and height βmax
1
. Now, let us consider the
points in the red dotted area. They correspond to a pending
work at the end of a hyperperiod, which is strictly greater than
max{βmax
1
, βmax
2
} = βmax
2
. Starting from such a state at any
t = qH, in any schedule, at least one job activated before t will
ﬁnish after t + βmax
2
thus missing its deadline. We conclude
that this state can not belong to any feasible schedule.
IV.
EXACT BOUND ON THE SIMULATION INTERVAL
A. Characterization
We have seen with Figure 1 that B0 fails to take into
account diagonal constraints arising from the fact that the plat-
form limits the execution parallelism and thus the maximum
amount of cumulative backlog at the end of a hyperperiod.
We can generalize this argument to derive a characterization
of the bound as a set of linear constraints. Let Λ ⊆ Γ be
a subset of the task set. On a monoprocessor platform, the
cumulative backlog at the end of a hyperperiod generated by
tasks in Λ is bounded by max[βmax
i
| τi ∈ Λ] where max
returns the maximum value of a list. On a 2-processor platform,
execution parallelism allows us to achieve a higher bound:
max2[βmax
i
| τi ∈ Λ] where max2 returns the sum of the 2
greatest values of a list. Indeed, even if two jobs can be exe-
cuted in parallelism, in a feasible schedule, they cannot overrun
their deadlines. Thus, when the time is past the penultimate
deadline, only one job among the jobs activated before the end
of the hyperperiod, has not reached its deadline, so in a feasible
schedule only this job could be running. Further generalizing
this argument, on a m-processor platform, every Λ ⊆ Γ yields
the constraints P
τi∈Λ βi ≤ maxm[βmax
i
| τi ∈ Λ] where
maxm returns the sum of the m greatest values of a list. This
allows us to characterize the number of possible states at the
end of a hyperperiod (since we know that all local clocks
are null at such instants, the state is truncated to its Cremi(t)
components).
S =
n
x |x ∈ NN ∧
∀Λ ⊆ Θ.
X
τi∈Λ
x[i] ≤ maxm[βmax
i
| τi ∈ Λ]
o
(2)
From this, we can derive the exact value of the bound on
the simulation interval:
B1 = H × |S|
(3)
Note that using 2 to compute B involves computing the
power set of Θ (to enumerate all possible values of Λ), which
has 2|Θ| elements, and then enumerating the number of integer-
coordinate points over a linear polyhedron deﬁned by 2|Θ|
constraints. It must also be noticed that B0 corresponds to
the enumeration of the points with integer coordinates of the
smallest hyperrectangle that contains S and is exact when the
deﬁnition of S involves no diagonal constraints, i.e., when the
number of tasks is not greater than the number of processors.
B. Computation of B1
To count the number of states in S, we rely on a ﬁxed
point computation. We start from state 0 and date qH. We
expand the set of states time unit per time unit. Each time
unit, we add states that have a cumulative backlog that ﬁts in
this extra time unit while taking into account platforms and
tasks constraints. We stop once we have reached a ﬁxed point
over the set of states. We ﬁrst describe the algorithm, then
prove its termination, soundness, completeness, and apply it
to a simple example.
1) One time unit mappings: Let us consider Act : N →
{0, 1}N such that ∀t ∈ [0, βmax). ∀i ∈ [1, N]. Act(t)[i] =
0 iff t ≤ βmax
i
, and Act(t)[i] = 1 otherwise. That is to
say Act(t)[i] = 1 iff a job of τi activated before t has not
necessarily reached its deadline at date t.
Let Incr = {v | v ∈ {0, 1}N ∧ PN
i=1 v[i] ≤ m}. An
element of Incr is a mapping of tasks to processors (remember
that jobs of the same task must execute sequentially). As an
example, for N = 3 tasks and m = 2 processors we have:
Incr =
0
0
0

,
0
0
1

,
0
1
0

,
1
0
0

,
1
1
0

,
0
1
1

,
1
0
1

Let incr be a function from date to parts of Incr such
that incr(t) = {v|v ∈ Incr ∧ v × Act(t) = v}. incr(t)
describes the mappings of tasks to processors for [t, t + 1) in
any feasible schedule, discarding those which execute a job
of a task that has already missed its deadline. For example,
9
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-830-3
VALID 2020 : The Twelfth International Conference on Advances in System Testing and Validation Lifecycle

assuming N = 3 tasks, m = 2 processors, and Act(1) =
0
0
1

,
then we have:
incr(1) =
0
0
0

,
0
0
1

Lemma 1: A one time unit mapping of tasks onto pro-
cessor inc for interval [qH + t, qH + t + 1) for any non
negative integer q is part of a feasible schedule if and only
if inc ∈ incr(t).
Proof: Follows from the deﬁnition of incr.
Deﬁnition 5 (Possible mapping): A possible mapping is a
one time unit mapping of tasks onto processor inc ∈ incr(t).
2) Fixed point algorithm: Let S be a set of states. We
deﬁne the successor of S by elapsing one time unit from date
qH + t to qH + t + 1 as follows:
next(S, t) = {s + inc | s ∈ S ∧ inc ∈ incr(t)}
(4)
Given this deﬁnition of next, the set of possible states of
the system at the end of a hyperperiod in any feasible schedule
is the smallest ﬁxed point of:

S0 = {0}
Sn+1 = Sn ∪ next(Sn, n)
(5)
The resulting bound B1 can be computed with algorithm
in Figure 2 below.
Figure 2. Fixed point algorithm for the computation of the exact bound B1.
3) Termination: Recall that βmax
= maxτi∈Θ{βi} is
the greatest possible backlog of any task at the end of a
hyperperiod. From the deﬁnition of function incr, we have
incr(βmax) = {0} and then the smallest ﬁxed point is met at
worst in βmax steps. During the computation of B1 each state
of S has to be stored. The number of states is upper bounded
by the B0. During the computation of B1, each new state has
to be compared to the set of states already explored. Hence
our algorithm has also a factorial complexity in the state space
size. A more detailed analysis, including a complexity analysis
of the problem is out of the scope of this paper.
4) Soundness and Completeness :
Theorem 1 (Completeness and Soundness): s ∈ S if and
only if s is reachable by a feasible schedule from 0 .
Proof: Soundness. Ab absurdo. Assume that there exists
a state s ∈ S which is not reachable by a feasible schedule
from 0. Then, there exists t ∈ [0, βmax), a state st ∈ St that is
reachable through possible mappings from 0 and a state st+1 ∈
St+1 that is not reachable through possible mappings from
0 such that st+1 ∈ next({st}, t). Then, there exists inc ∈
incr(t) such that st+1 = st + inc whereas inc is not possible
at date t contradicting Lemma 1.
Completeness. Ab absurdo. Assume that there exists a state
s which is reachable through possible mappings from 0 and
such that s ̸∈ S. Then, there exists t ∈ [0, βmax) and a state
st+1 that is reachable by a possible mapping from st ∈ St
such that st+1 ̸∈ next({st}, t). Then, there exists inc such
that sk+1 = sk + inc and inc ̸∈ incr(t) whereas inc is
possible at date t contradicting Lemma 1.
5) Example: Consider a system with N = 3 tasks running
on a platform with m = 2 processors. The charactetistics of
the tasks are such that
β1
β2
β3
=
1
1
3

. The possible mappings of
tasks to processors in the ﬁrst time unit after a hyperperiod is
given by:
incr(0) = Incr =
0
0
0

,
0
0
1

,
0
1
0

,
1
0
0

,
1
1
0

,
0
1
1

,
1
0
1

and possible mappings in the following time units are given
by:
incr(1) = incr(2) =
0
0
0

,
0
0
1

and incr(3) =
0
0
0

Now, let us compute the smallest ﬁxed point.
S0 =
0
0
0

, S1 =
0
0
0

,
0
0
1

,
0
1
0

,
1
0
0

,
1
1
0

,
0
1
1

,
1
0
1

,
S2 =
0
0
0

,
0
0
1

,
0
1
0

,
1
0
0

,
1
1
0

,
0
1
1

,
1
0
1

,
0
0
2

,
1
1
1

,
0
1
2

,
1
0
2

and then
S = S3 =
0
0
0

,
0
0
1

,
0
1
0

,
1
0
0

,
1
1
0

,
0
1
1

,
1
0
1

,
0
0
2

,
1
1
1

,
0
1
2

,
1
0
2

,
0
0
3

,
1
1
2

,
0
1
3

,
1
0
3

We obtain B1 = H×|S| = 15H. With the same system, we
have B0 = H×(2×2×4) = 16H. The state that was discarded
in B1 is
1
1
3

because it requires 5 time units of computation
but a valid schedule cannot have more than max2{1, 1, 3} = 4
time units of pending work.
V.
EXPERIMENTATION
A. Setup
The computation of B1 has factorial time complexity so it
does not scale to big systems. Its main interest is to provide
a reference to assess the tightness of approximate bounds.
In particular, it is worth asking when B0 is a reasonable
10
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-830-3
VALID 2020 : The Twelfth International Conference on Advances in System Testing and Validation Lifecycle

TABLE I. DETAILS AND PARAMETERS FOR THE 5 SERIES OF EXPERIMENTS
Series
N
m
βmax
Watchdog expiry time
Timeout (%)
1
1 ≤ N ≤ 12
1 ≤ m ≤ 8
20
15 min for N ≤ 9 and 20 min for N > 9
18.47
2
1 ≤ N ≤ 12, 11 excluded
1 ≤ m ≤ 8
10
15 min for N ≤ 10 and 45 min when N is 12
8.92
3
1 ≤ N ≤ 12
1 ≤ m ≤ 8
5
15 min
7
4
k ≤ N ≤ 9 with k = 2 × m
1 ≤ m ≤ 4
(10 − N) × 8
10 min
0.75
5
16
4
2 ≤ βmax ≤ 6
10 min
29
approximation, and if it is worth searching for less pessimistic
approximations for certain systems. Thus, in this section,
we evaluate the pessimism of bound B0 with respect to
B1. We also provide some results concerning the resource
consumptions of the computation of B1 to characterize the
range of systems that it can solve.
We implemented algorithm 2 in C, using red-black trees
as the data structure for state sets. Computations have been
run on a Debian GNU/Linux 8.8 system (kernel 3.16.0-4)
with Intel(R) Xeon(R) CPU E5-2620 @ 2.00GHz and 128GB
RAM. The evaluation set is based on ﬁve series of experiments.
In each case, we take 20 samples per point (a point is
deﬁned by a number of tasks, a number of processors, and
a value for βmax), and the algorithm is applied to each
point. For each sample, the maximum backlog of each task
is randomly generated between 1 and βmax using a uniform
distribution. The code is instrumented to report execution time
and maximum memory consumption of the computation of B1.
Lastly, a watchdog is used to stop the computation of B1 after
a pre-deﬁned amount of time. Parameter values for each series
are shown in Table I.
We provide a set of graphs that have been chosen to be
as representative as possible of the data set. For each point,
we represent the arithmetic average as well as minimum and
maximum values among all 20 samples. In each ﬁgure the y-
axis represents the ratio B1/B0 as a percentage. The quantity
associated with the x-axis varies so it is speciﬁed in each ﬁgure.
B. Pessimism of B0
Figure 3 shows the result when the number of tasks
increases for a given number of processors. Figure 4 shows
the result when the number of processors increases for a given
number of tasks. As expected, both ﬁgures show that when
the number of diagonal constraints increases, B0 becomes
more pessimistic. From 2, diagonal constraints appear for sets
Λ ⊆ Θ such that |Λ| > m, i.e., when the platform does
not offer enough parallelism. Thus, the number of diagonal
constraints increases with N
m. Figure 5 plots B1
B0 against N
m. It
shows that, on this data series, B0 quickly becomes a loose
approximation of B1: when N
m becomes greater than 3, B1
B0 falls
to 50 %, and below for higher values of N
m. The complexity
of the computation of B1 does not allow us to extend the plot
further but it is expected that, as the number of linear constraint
increases, B1
B0 asymptotically tends to zero.
Figure 7 plots B1
B0 against the standard deviation computed
over the list [βmax
i
| τi ∈ Θ]. Although it is not as clear as
the impact of N
m, it shows that when the standard deviation
is small, B0 tends to be more pessimistic. Figure 7 also
shows that similar values of B1
B0 can be reached for different
values of βmax with similar dispersion of values of βmax
i
. An
2
3
4
5
6
7
8
9
0
20
40
60
80
100
Number of tasks
Relative number of states (%)
m=1
m=2
m=4
Figure 3. N = 1 to 9 tasks, m = 1 to 4 processors, βmax = 20.
1
2
3
4
5
6
7
8
0
20
40
60
80
100
Number of processors
Relative number of states (%)
N=6
N=8
N=10
N=12
Figure 4. N ∈ {6, 8, 10, 12} tasks, m = 1 to 8 processors, βmax = 10.
intuitive interpretation can be formulated from the example
of ﬁgure 1: if βmax
1
= βmax
2
then the diagonal constraints
β1 + β2 ≤ max{betamax
1
, βmax
2
} removes half of the points
of B0 and this is the worst case. So, the closer the values of
βmax
i
in numerous mismatch, the more states it removes. And
of course, a small standard deviation denotes a system with a
small dispersion of βmax
i
values.
C. Scalability of algorithm 2
The computation of B1 requires a factorial number of
comparisons with regards to the size of the state space of the
system. Thus, it is sensible to every parameter that has an
impact on the state space: number of tasks N, of processors
m, and the maximum backlogs of tasks βmax
i
.
Table II groups results for N = 16 and m = 4. In this
case, the average execution time increases from 6.25 s to 385 s
11
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-830-3
VALID 2020 : The Twelfth International Conference on Advances in System Testing and Validation Lifecycle

2
3
4
0
20
40
60
80
100
N
m
Relative number of states (%)
Figure 5. N = 1 to 12 tasks, m = 1 to 4 processors, βmax = 20.
2
3
4
5
6
5
10
15
βmax
Relative number of states (%)
Figure 6. N = 16 tasks, m = 4 processors, βmax = 2 to 6.
1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5 7
65
70
75
80
85
90
95
100
standard deviation of [βmax
i
| τi ∈ Θ]
Relative number of states (%)
βmax = 5
βmax = 10
βmax = 20
Figure 7. N = 8 tasks, m = 4 processors, βmax ∈ {5, 10, 20}.
TABLE II. EXECUTION TIME (IN SECONDS), WHEN N = 16 AND
m = 4
βmax
Average
Maximum
Minimum
Standard deviation
3
6.25
22
microseconds
6.146244039
4
124.68
364
2
143.8439944
5
385
599
94
169.7838733
TABLE III. EXECUTION TIME (IN SECONDS), WHEN m = 4 AND
βmax = 20
N
Average
Maximum
Minimum
Standard deviation
6
5.95
33
microseconds
9.827324956
7
124.2
511
1
136.6607786
8
254.54
701
18
302.4903777
just by increasing βmax from 3 to 5. Table III groups results
for m = 4 and βmax = 20. In this case, the average execution
time increases from 5.95 s to 254.54 s just by increasing N
from 6 to 8. Lastly, Table IV groups results for N = 8 and
βmax = 20. In this case, the average time increases from 1.05 s
to 130.4 s just by increasing the m from 1 to 3. From these
three tables, the inﬂuence of the individual βmax
i
values on
the overall execution time can also be seen: in Table II for
example, with N = 16, m = 4 and βmax = 5, the execution
time varies from 94 s to 599 s.
Similar results are observed for memory occupation. In-
deed, the whole state space has to be stored. Table V shows for
instance the maximum memory occupation for varying values
of N and m when βmax = 20. As expected, the time and
space complexity of algorithm 2 makes it impossible to deal
with systems that have too large a state space. Nevertheless,
many industrial systems use small multicore platforms. For in-
stance, the 32 bit Microcontroller TriCore family developed by
Inﬁneon for the embedded automotive market offers platforms
with 1 to 6 cores. Moreover, not all tasks in these systems have
a non null backlog at hyperperiod boundaries, so B1 could be
of practical use for these systems. Additional experiments on
industrial benchmarks are required to provide an answer to this
question and it is out of the scope of this paper.
VI.
CONCLUSION
The problem addressed in this paper is to compute an exact
bound on the simulation interval for systems of asynchronous
periodic tasks with arbitrary deadlines subject to structural
TABLE IV. EXECUTION TIME (IN SECONDS), WHEN N = 8 AND
βmax = 20
m
Average
Maximum
Minimum
Standard deviation
1
1.05
6
microseconds
1.637552731
2
98
343
4
95.8200067
3
130.4
899
1
361.2071865
TABLE V. RESIDENT SET SIZE USED (IN MB), WHEN βmax = 20
N
m
Average
Maximum
Minimum
Std dev.
5
1
4.056
5.492
3.980
0.3380934782
6
1
13.935
47.980
3.988
13.92257703
7
1
108.555
640.840
3.812
166.1081405
8
1
1398.797
8286.392
66.756
2350.883606
9
1
15832.102
103894.004
640.600
28790.4245
5
2
8.250
19.372
3.980
5.43988676
6
2
35.805
168.132
3.988
39.78900124
7
2
238.146
940.680
10.796
229.4609528
8
2
3027.923
10032.492
147.412
3030.038648
9
2
27974.981
95923.060
1027.404
15778.69099
12
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-830-3
VALID 2020 : The Twelfth International Conference on Advances in System Testing and Validation Lifecycle

constraints scheduled by any deterministic and memoryless
algorithm on a uniform multiprocessor platform. A very simple
yet pessimistic solution for this problem is already known
in the state-of-the-art. We formulate a characterization of the
bound that involves the cardinal of the set of points with
integer coordinates in a polyhedron deﬁned by an exponential
number of linear constraints. We propose and prove a ﬁxed
point algorithm to compute this set, which has factorial time
complexity.
We rely on an implementation of this algorithm to estimate
the pessimism of the bound known from the state-of-the-
art through a set of experiments on synthetic systems. In
the results of these experiments we observe two points: (i)
the bound from the state-of-the-art quickly becomes a loose
estimation of the exact bound when the number of tasks
becomes greater than the number of processors of the platform;
(ii) the time complexity of our algorithm is too high to deal
with anything but small systems. From these two points, we
conclude that there is an interest in looking at approximate
bounds that lie in the middle between the state-of-the-art
and the exact bound. Our formulation of the problem as a
linear system already gives us a direction. The state-of-the-art
provides a simple but pessimistic solution by discarding all
diagonal constraints, while the exact bound does the opposite.
So, as a direct follow-up to the work described here, we will
now explore the idea to take into account a subset of the
diagonal constraints to ﬁnd a good trade-off between precision
and time complexity.
REFERENCES
[1]
R. I. Davis, A. Zabos, and A. Burns, “Efﬁcient exact schedulability tests
for ﬁxed priority real-time systems,” IEEE Transactions on Computers,
vol. 57, no. 9, 2008, pp. 1261–1276.
[2]
J. Goossens, E. Grolleau, and L. Cucu-Grosjean, “Periodicity of real-
time schedules for dependent periodic tasks on identical multiprocessor
platforms,” Real-Time Syst, vol. 52, no. 6, 2016, pp. 808–832.
[3]
J. Leung and J. Merrill, “A note on preemptive scheduling of periodic,
real-time tasks,” Information Processing Letters, vol. 11, no. 3, 1980, p.
115–118.
[4]
J. Goossens and R. Devillers, “Feasibility intervals for the deadline
driven scheduler with arbitrary deadlines,” in Proceedings Sixth Interna-
tional Conference on Real-Time Computing Systems and Applications.
RTCSA’99 (Cat. No.PR00306), 1999, pp. 54–61.
[5]
L. Cucu and J. Goossens, “Feasibility intervals for multiprocessor ﬁxed-
priority scheduling of arbitrary deadline periodic systems,” in 2007
Design, Automation Test in Europe Conference Exhibition, 2007, pp.
1–6.
[6]
J. Baro, F. Boniol, M. Cordovilla, E. Noulard, and C. Pagetti, “Off-
line (optimal) multiprocessor scheduling of dependent periodic tasks,” in
Proceedings of the 27th annual ACM symposium on applied computing
(SAC), 2012, pp. 1815–1820.
[7]
V. N´elis, P. Yomsi, and J. Goossens, “Feasibility intervals for homoge-
neous multicores, asynchronous periodic tasks, and fjp schedulers,” in
Proceedings of the 21st international conference on real-time networks
and systems, 2013, pp. 277–286.
13
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-830-3
VALID 2020 : The Twelfth International Conference on Advances in System Testing and Validation Lifecycle

