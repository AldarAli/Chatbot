Task-based Guidance 
of Multiple UAV Using Cognitive Automation 
 
Johann Uhrmann & Axel Schulte 
Institute of Flight Systems 
Universität der Bundeswehr München 
Munich, GERMANY 
{johann.uhrmann|axel.schulte}@unibw.de 
 
 
Abstract—This article discusses different dimensions of 
automation in the integration of multiple, detached, unmanned 
sensor platforms into a military helicopter scenario. Artificial 
cognitive units implement parts of human-like knowledge-rich 
task execution aboard a highly automated vehicle. Artificial 
cognition, being the method used, allows task execution beyond 
pre-scripted 
and 
predefined 
instruction 
sets, 
utilizing 
reasoning about the current situation to support goal-driven 
behaviour during task execution instead. The tasks assigned by 
the human operator are formulated at an abstraction level that 
might as well be used to task human subordinates within a 
mission. Like human subordinates, the UAV uses its cognitive 
capabilities to adapt task execution to the currently known 
situation including knowledge about the task assignments of 
teammates.  
Keywords – Task-based guidance; goal-driven behaviour; 
artificial 
cognititive 
units; artificial 
cognition; 
level 
of 
automation. 
I. 
 INTRODUCTION 
The utilization of UAVs as detached sensor platforms of 
a manned helicopter in a military scenario requires a change 
in the UAV guidance paradigm that enables a single human 
operator to control the UAVs while being the commander of 
a manned aircraft.  
 
 
Figure 1.  Helicopter simulator of the Institute of Flight Systems 
If those UAVs were manned assets, the commander 
would just assign tasks referring to the mission elements and 
the current situation and leave the details of task execution as 
well as the application of “common sense”-knowledge 
generating 
local 
tactical 
behaviours 
to 
the 
human 
subordinate.  
Some current research approaches concerning UAV 
guidance allow the definition of scripts or plays [1] to define 
action sequences for one or multiple UAVs. Moreover, some 
of these systems also react to changes in the situation like a 
new threat along a flight route [2]. However, the resulting 
behaviours of suchlike systems are solely defined at design-
time. The goals of the behaviour of the UAVs are not 
expressed in the system but are implicitly encoded in the 
implementation of the guidance system. This article 
describes the system architecture, knowledge and goals 
driving task-based, cooperative and cognitive 
UAV 
automation. The resulting type of supervisory control shall 
avoid at least some of the issues of conventional automation 
by taking a step towards human-centred automation [3]. This 
is integrated in the helicopter research flight simulator of the 
Institute of Flight Systems at the Universität der Bundeswehr 
München and evaluated in experiments with experienced 
German Army aviators. In these experiments, the pilots had 
to perform several, dynamic troop transport missions 
including an unscheduled combat recovery task with the 
support of the manned helicopter and three UAVs. 
The following sections describe the task-based guidance 
approach, its design, the resulting adaptable levels of 
automation and first experimental results. 
II. 
RELATED WORK 
Most current research projects in the area of UAV 
guidance and mission management focus on solving 
problems in the field of trajectory generation [4] and 
management and the achievement of what is mostly referred 
to as “full autonomy” by the application of control 
algorithms [5]. 
This research concentrates on optimizing within a given 
constraint set. However, such constraint sets and parameters 
are either static or the definition is left to the human operator 
or the experimenter. If the handling and monitoring of the 
control algorithms of multiple UAV is allocated to the 
commander of a manned helicopter, then the result is error-
47
COGNITIVE 2011 : The Third International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-155-7

prone behaviour and high workload for the operator [6]. 
Therefore, we present a system that integrates flight control, 
payload control and radio links into one entity. This entity 
uses its knowledge about the situation, the mission, the 
vehicle and its capabilities to provide an interface to the 
human operator that allows UAV guidance on a situation 
adaptive task level rather than sub-system handling.  
Previous articles focus on the requirements engineering 
[6] and global system design and test environment including 
the integration of assistant systems [7] [8] [9]. The main 
contribution of this article consists of a discussion of the 
resulting levels of automation gained by task based guidance, 
a description of the knowledge base to realize task based 
guidance as well as the first experimental evaluation of the 
system. 
III. 
TASK-BASED GUIDANCE 
Task based guidance aims at integrating multiple 
unmanned vehicles into a manned helicopter mission in a 
similar manner as integrating additional manned helicopters 
into the scenario. Therefore, the guidance of unmanned 
vehicles should be on an abstraction level that allows the 
allocation of a series of tasks to each UAV. Suchlike tasks 
are issued by the human operator and request the 
achievement of goals, e.g., the request of reconnaissance 
information about a landing site. The interpretation of the 
tasks and the use of on-board systems to fulfil these tasks are 
left to the UAV. The series of tasks is on a similar 
abstraction level as tasks assigned to a pilot during mission 
briefing in a conventional, manned helicopter mission.  
Moreover, just like a human pilot, UAVs should also use 
opportunities of supporting the mission, e.g., by getting 
sensor information of nearby objects, without a direct 
command from the operator. 
This implies UAV guidance and mission management on 
a level where one or more UAVs are controlled by tasks that 
use mission terms instead of waypoints and the request of 
results rather than in-detail configuration of flight control 
functions and sensor payload. The latter should be generated 
aboard the UAV by its on-board automation. 
The tasks currently implemented in the experimental 
setup are 
 
a departure task that respects basic air traffic 
regulations of the airfield and makes the UAV depart 
via a given, named departure location. 
 
a transit task that causes a flight to a specific, named 
location. While being in transit, the UAV configures 
the camera into forward looking mode. Known 
threats will be automatically avoided, if possible. 
 
a recce route task that causes the UAV to fly a route 
to a named destination. The sensor payload will be 
configured to provide reconnaissance information 
about the flight path. If the UAV possesses 
knowledge about another UAV also tasked with a 
recce of the same route, it will modify its flight path 
to maximize sensor coverage. 
 
a recce area task that causes the UAV to gather 
recce information about a named area. The camera 
will be used to provide ortho-photos of the area. 
 
an object surveillance task. While working on this 
task, the UAV will use the payload control to deliver 
a continuous video stream of a named location. 
 
a cross corridor task makes the UAV fly through a 
transition corridor between friendly and hostile 
territory. To avoid friendly fire and ease cooperation 
with the own ground based air defence; this crossing 
is modelled as separate task. Moreover, it is the only 
task allowed to cross the border between friendly 
and hostile territory. 
 
a landing task causes the UAV to take an approach 
route to an airfield and to land at that airfield. 
 
The capability of understanding these tasks at mission 
level, i.e., understanding the current situation, planning 
towards task execution, using the flight control system, 
communication equipment and mission payload of the UAV 
requires an automation that incorporates certain sub-
functions as found in cognitive behaviour of a human [7], 
thereby creating cognitive behaviour of the automation. The 
following chapters discuss the resulting levels of automation 
and describe the architecture and information processing of a 
so-called artificial cognitive unit (ACU). 
 
IV. 
LEVELS OF AUTOMATION 
Currently, UAV systems operate on a wide range of 
different guidance modes. That modes cover the whole range 
from direct manual control [10], flight control based  [6], 
scripted behaviours [1] up to above-mentioned task-based 
guidance [7]. These guidance modes form a stack of 
abstraction layers as depicted in Figure 2.  
 
 
Figure 2.  Levels of abstraction in UAV guidance  
Sheridan and Verplank [11] describe a different view of 
levels of automation. These levels are mostly independent 
from the chosen abstraction layer but focus on task allocation 
and authority sharing between the human and the 
automation. They range from barely manual control to 
automation that does neither allow intervention from the 
human operator nor provide information about the action 
taken. In the design of current UAV guidance systems, 
various levels are present, e.g., in waypoint based guidance 
Task based guidance (single UAV)
FMS
Ground 
mapping
Object 
surveillance
Autopilot
R/C pilot
manual
Payload control
Flight control
mission management
automation
48
COGNITIVE 2011 : The Third International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-155-7

systems, the definition of waypoints is the sole responsibility 
of the human operator. No automation support is provided in 
that task. However, automatic flight termination systems do 
not allow the human to veto on the decision of the 
automation but merely report the flight termination after its 
execution, i.e., level 7 according to Sheridan and Verplank 
[11]. 
The task based guidance described further in this article 
introduces an additional dimension in the levels of 
automation. While the abstraction layer is fixed in the current 
setup, i.e., only the task based layer is available to the human 
operator, the operator can choose to provide different tasks to 
the UAV. The UAV will check the tasks for consistency and 
may insert additional tasks to restore a consistent task 
agenda. The consistency check and completion of the task 
agenda is based on a planning scheme, which behaves 
deterministic with respect to the current tactical situation and 
the task elements known so far. Thus, the operator may 
choose to specify only task elements relevant to him or her 
and leave the specification of other tasks to the UAV. This 
particular type of adaptable automation allows the 
specification of strict task agendas, i.e., the human operator 
defines every task of the UAV. However, also loose task 
agendas may be defined, i.e., the human operator only 
defines the most important tasks and leaves the details to the 
UAV. 
Moreover, this kind of automation also can reduce the 
chance of human errors, because unintentionally omitted 
tasks are also completed by the automation. 
 
V. 
SYSTEM ARCHITECTURE 
In order to implement suchlike machine behaviours, this 
chapter will provide an overview of the design principles and 
information processing architecture enabling task based 
guidance capabilities. 
A. Design of knowledge-based Artificial Cognitive Units 
Based on models of cognitive capabilities of human 
pilots, artificial cognitive units (ACUs) were designed. As 
depicted in Figure 3, these units become the sole interface 
between the human operator and the vehicle [12] in the work 
system [13]. This additional automation allows the desired 
shift in the guidance paradigm from the subsystem level, i.e., 
separate flight guidance and payload management, to 
commanding intelligent participants in the mission context. 
 
Figure 3.  Work system “UAV guidance” 
To understand and execute tasks with respect to the 
current situation, the ACU requires relevant parts of the 
knowledge and capabilities of human pilots. That knowledge 
can be grouped into system management, understanding and 
evaluating mission objectives in the context of the current 
scenario as well as knowledge to interact with the human 
operator [14]. This knowledge is derived by formalization of 
domain specific procedures defined in documents like the 
NATO doctrine for helicopter use in land operations [15]. 
Furthermore, interviews with experienced helicopter pilots 
revealed relevant knowledge. The interviews and the 
additional evaluation of recordings of training missions used 
the Cognitive Process Method [16]. For every phase, the 
human’s objective is evaluated. Moreover, all possible and 
hypothetic action alternatives to pursue the objective are 
determined. Furthermore, all environmental knowledge is 
gathered, which is used to select a particular action 
alternative or which influences the execution of a chosen 
action. At last, the procedures to execute the actions are 
evaluated. 
B. Human-Machine Interface 
To support the guidance of multiple UAV from a manned 
helicopter, the human-machine interface (HMI) has to 
integrate into the manned helicopter. Moreover, using an 
audio interface, i.e., speech recognition, to guide the UAV 
was rejected by a majority of the interviewed pilots due to 
the already high radio traffic that has to be handled by the 
helicopter crew. 
Therefore, a graphical interface was chosen to interact 
with the UAV. This interface is integrated into two identical 
multifunctional displays available to the commander of the 
manned helicopter. Figure 4 depicts the implemented 
multifunctional display format. 
 
 
Figure 4.  UAV tasking interface 
On the lower left of the multifunctional keyboard, the 
operator can switch between UAV control and the displays 
of the manned helicopter (A/C / UAV). Above, the current 
UAV can be selected. On the top left, the operator can select 
three different modes: CAM, TASKS, and ID. The right 
multifunctional soft keyboard shows the context sensitive 
options for the current mode chosen on the left. 
Human 
Operator
Task 
Based
Guidance
(ACU)
Payload 
Management 
System
Flight 
Management 
System
Payload
Avionics
Operating Force
Operation Supporting Means
environment
work
objective
work
result
49
COGNITIVE 2011 : The Third International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-155-7

CAM provides a live video stream from the camera of 
the currently selected UAV. The TASKS can be used to 
monitor the current tactical situation and to manipulate the 
task elements of the currently selected UAV.  
In TASKS, the current tactical situation is displayed as 
well as the task elements of the UAV. The currently active 
task is highlighted in yellow. A task can be inserted into the 
task agenda of the UAV by choosing the task type as shown 
on the right in Figure 4, optionally choosing the predecessor 
of the task on the map and selecting the target position of the 
task. A task can be selected for immediate execution. This 
functionality can be used to start the execution of the first 
task as well as for skipping tasks. Additionally, tasks can be 
deleted and moved, i.e., the target area description of the task 
is altered. If tasks are added, deleted or modified, the UAV 
will maintain a consistent task agenda by inserting missing 
tasks depending on the current tactical situation. As long as 
this planning is in progress, it is indicated on the bottom of 
the display as shown for UAV number 2 in Figure 4. To 
prevent immediate re-insertion of deleted task elements, the 
consistency checks are delayed after the operator deletes a 
task element. This allows further modifications of the task 
agenda by the human operator without being interrupted by 
the UAV. 
The ID display mode is used to review photos taken by 
the UAV and to classify the objects on the images into pre-
defined types (car, military vehicle, ground based air 
defence) and hostility, i.e., neutral, friend or foe. Those 
classifications are also reflected in the tactical situation 
shown in the task mode as well as the electronic map 
displays available to the pilot flying. Furthermore, those 
classifications will be transmitted to the UAVs in order to 
support reaction to the changed tactical environment, e.g., to 
plan flight routes around hostile air defence. 
The combination of those display functionalities shall 
allow the human operator to guide the UAVs to support a 
military air assault mission that involves operation over 
hostile areas and support of infantry troops. Moreover, by 
tasking the UAVs using mission terms, e.g., by selection of 
“area reconnaissance of the primary landing site”, the control 
of three UAVs shall be feasible and enhance mission safety 
by providing valuable information about mission relevant 
areas and routes without risking exposure of own troops to 
threats like ground based air defence and other opposing 
forces. 
C. Information Processing 
The implementation of artificial cognitive units is based 
on the Cognitive System Architecture (COSA) framework 
[16]. This framework is based upon Soar [17] and adds 
support for object-oriented programming as well as 
stereotypes for structuring the knowledge into environment 
models, desires, action alternatives and instruction models. 
This (a-priori) knowledge constitutes the application 
specific part of the Cognitive Process, which is described in 
detail by Putzer and Onken [16] as well as Onken and 
Schulte [13]. Information and knowledge processing as well 
as interfacing with the environment is depicted in Figure 5. 
The following describes the information processing steps 
using examples of the knowledge of the UAV’s on-board 
ACU. 
 
 
Figure 5.  Knowledge processing in the Cognitive Process [16] 
 
Input data are retrieved from the environment by input 
interfaces. There are three types of input interfaces: (1) 
reading sensor information from the sensors of the UAV, (2) 
reading information from the communication link of the 
UAV and (3) providing results from on-board automation, 
e.g., information about flight routes generated by an external 
route planner. 
The environment models of the a-priori knowledge of the 
ACU drive the interpretation of input data into instances of 
semantic concepts. Those concepts form an understanding of 
the current tactical environment including knowledge about 
existence and positions of threats, areas, bases, landing sites, 
routes, waypoints etc. Environment models instantiate on the 
arrival of matching input data and matching situational 
knowledge. All instances of environment models, i.e., 
beliefs, form the picture of the current situation of the UAV. 
Notable examples for environment models are instructions, 
tasks and roles. Instructions represent requests sent from the 
operator to the UAV and instantiate upon the arrival of the 
corresponding input data. Instructions can request to insert, 
delete or immediately execute a task in the task agenda. If 
the instruction is to insert a new task into the agenda of the 
UAV, the corresponding task environment model becomes 
active and creates an instance. This instance will refer to the 
environmental models describing the target area of the task, 
the task type, and the state of the task, e.g., “scheduled for 
execution”. For every task, there is an instance of a 
precondition and a postcondition. The precondition builds a 
representation of all required prerequisites of a task, e.g., 
being airborne and being near a specific location to start the 
task. The postcondition builds a representation of particular 
effects of the task, e.g., a transit flight has the postcondition 
of being airborne at the target location. A role is an 
environment model describing the specific part of the current 
UAV in a task shared among multiple UAV, e.g., if multiple 
UAV have the common task to retrieve recce information 
about a flight route, a role can tell the UAV to fly in a certain 
distance left from the route to maximize sensor coverage. 
Environ-
ment
Planning
Plan
Scheduling
Instructions
Environment
Models
Instruction Models
Desires
Goals
Action
Alternatives
Input
Interface
Output
Interface
Input Data
situational
knowledge
a-priori-
knowledge
Goal
Determi-
nation
Belief
observable behaviour of CP
= ACU behaviour
Interpretation
50
COGNITIVE 2011 : The Third International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-155-7

Desires describe world states the UAV should maintain. 
If a desire detects the violation of the state in the belief, then 
it activates by instantiating into an active goal. One example 
of a desire is “know what to do next”, i.e., to have a belief 
that designates the current task at hand. Whenever the UAV 
does not have the situational knowledge about its current 
task, this desire activates and makes the UAV act towards 
the determination of the next task. Another desire is to 
comply with the current task, i.e., to actually take the steps 
necessary to fulfil the task. The desire to “have a unique 
role” in common tasks enables multiple UAVs to work on 
the same task by sharing parts of it, i.e., roles in the task. For 
example, if three UAVs share the task of retrieving visual 
sensor information about a flight route, then the systems 
infer from this situation that the roles of flying the centre line 
of the track and to fly left and right of the track exist. Each 
UAV selects a role either random or – if selection knowledge 
is present – it selects a role that fits best for the UAV. To 
avoid duplicate roles for a single task, every UAV that has a 
non-unique role re-selects a role. This technique has shown 
to be sufficient for real-time conflict resolution of at least 
three UAV. 
“Having a consistent agenda” is one of the central desires 
of the ACU. This desire activates if there is a mismatch 
between the aforementioned postconditions of a task and the 
preconditions of its successor. Moreover, it activates, if 
tactical restrictions are violated, e.g., if a task other than 
“cross corridor” crosses the border to the opposing terrain or 
if an area is not entered or left at the designated entry/exit 
points. The activation of this desire is reported to the human 
operator as “UAV planning” as depicted for UAV2 in Figure 
4. Moreover, the desire of having a consistent agenda also 
provides knowledge about the severity of violations. This 
guides the resolution of inconsistencies from the most 
important violation to the least important violation. “Using 
opportunities” as they arise makes the UAV exploit chances 
of gathering additional reconnaissance information, if this 
does not lead to a neglect of the current task. For instance, if 
there is a sensor footprint of an unidentified force, then the 
ACU can decide to use the sensors of the UAV to generate 
additional information about that location to support the 
identification of that force. The ACU combines its 
knowledge about the type of sensor information, i.e., 
“unidentified sensor-hotspot”, the availability of its sensors, 
the availability of sensor information from its own sensors 
and from other UAV and its relative position to the 
unidentified force. This combination of knowledge enables 
the UAV to safely detect and use the chance of getting more 
information about the location. Moreover, the UAV also 
behave cooperative as the decision to generate additional 
sensor information is suppressed if another UAV has 
generated that sensor information from a similar angle to the 
unidentified force.  
Action alternatives provide ways to support active goals. 
They instantiate if a corresponding goal is active, but only if 
the current situational knowledge allows the selection of the 
action alternative. If more than one action alternative can be 
proposed, then the action alternatives model selection 
knowledge to prefer one alternative over the other. To 
maintain a consistent agenda, action alternatives can propose 
to add additional tasks like recce route, transit or cross 
corridor. If the crossing of a corridor can be inserted, it is 
preferred over the other action alternatives to provide a 
separation between “tasks over own territory” and “tasks 
over opposing territory” for subsequent action alternatives. 
Depending on the tactical situation, further tasks are inserted 
until a consistent agenda is achieved, i.e., all active goals of 
having a consistent agenda are fulfilled. As mentioned in 
Section IV, the human operator can exploit this behaviour of 
the ACU by skipping tasks on purpose and thereby shifting 
the completion and specification of missing tasks to the 
ACU. 
Every task of the ACU has its corresponding action 
alternative that supports the execution of the task in case of 
an active goal to comply with the current task. 
After one or more action alternatives are chosen, the 
instruction models become active and support the action 
alternatives by generating instructions on the output interface 
of the ACU. Those instructions are read by the output 
interface and cause the transmission of radio messages, 
configuration changes at the flight control system or the 
payload system or activate on-board automation, e.g., a route 
planner. 
In combination, all those processing steps depicted in 
Figure 5 generate purely goal-driven behaviour that allows 
reasoning over the tactical situation and the task elements 
entered by the human operator to provide situation-
dependent actions, which are consistent with tactical 
concepts of operations. 
VI. 
EXPERIMENTAL SETUP AND FIRST RESULTS 
Experiments were conducted with experienced German 
Army helicopter pilots in order to evaluate the task based 
guidance approach. The simulator cockpit shown in Figure 1 
has been used to perform military transport helicopter 
missions. The objective of the missions was to pick up troops 
from a known location and to carry them to a possibly 
threatened destination. According to the briefing, three 
UAVs should be used to provide reconnaissance information 
about the flight routes and landing sites in order to minimize 
exposure of the manned helicopter to threats. In addition to 
the tasks to perform in previous baseline experiments 
without task based guidance [6], in this experiment an 
unscheduled combat recovery task was issued to the crew as 
soon as the main mission objective had been accomplished. 
Prior to the measurements, every test person had been 
given one and a half day of system training. The test persons 
acted as pilot flying and pilot non-flying. This configuration 
was chosen to evaluate the effects of the UAV guidance to 
crew cooperation and crew resource management. 
The following data were recorded during the experiment:  
 
Interaction of the operator with the system 
 
Commands sent to the UAV via data link 
  
In every simulation run, the simulation had been halted 
twice, i.e., in the ingress and during a demanding situation 
while the helicopter is near the hostile target area, to get 
measures of the operator’s workload using NASA TLX [18]. 
51
COGNITIVE 2011 : The Third International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-155-7

During the simulation halt, all displays and the virtual pilot 
view were blanked and the intercom between pilot flying and 
pilot non-flying was disabled. To get an indication of the test 
persons’ situation awareness, the test persons were 
simultaneously questioned about the current tactical 
situations, system settings, e.g., radio configuration, and the 
upcoming tasks of the UAV and the manned helicopter. This 
measure is an adaption of the SAGAT technique [19]. After 
every mission, a debriefing follows which includes questions 
about the system acceptance, system handling, interface 
handling as well as feedback about the degree of realism of 
the simulation environment. 
 
 
Figure 6.  Subjective Pilot Ratings for HMI / Consistency Management 
 
Figure 6 shows some of the subjective ratings of the test 
persons. The chosen type of human-machine interface and 
the automatic insertion of task elements to maintain a 
consistent agenda are generally accepted by all test persons. 
The test persons stated that handling the UAVs consumed an 
average of 62% of the time while 34% remained for acting as 
commander of the manned helicopter. Evaluation of the 
simulation data shows that test persons used less than 50% of 
the available time for UAV guidance. 
TLX measures of the pilot non-flying range from 23% of 
subjective workload during the ingress over friendly territory 
up to a value of 60% during time-critical re-planning of 
multiple UAV in the target area. 
While in hostile areas, the manned helicopter operated 
within the terrain mapped by ortho-photos 94.5% of the time.  
VII. 
 CONCLUSION 
The experiments showed that artificial cognitive units 
make the guidance of multiple UAV in a military helicopter 
mission feasible with moderate workload. It was shown that 
artificial cognition aboard the UAVs effectively support the 
human operator in his/her task of increasing mission safety 
by providing recce information about flight routes and 
operation areas. Moreover, depending on the desired level of 
control over the UAVs, the test persons stated that they 
instructed the UAVs on a detailed or rough level, i.e., 
specification of every task element or skipping tasks on 
purpose and leaving the detailed planning to the UAV and its 
cognitive capabilities. 
 
REFERENCES 
[1] C. Miller, H. Funk, P. Wu, R. Goldman, J. Meisner, and M. 
Chapman, The Playbook™ Approach to Adaptive Automation, Ft. 
Belvoir: Defense Technical Information Center, 2005. 
[2] M. Valenti, Implementation of a manned vehicle-UAV mission 
system, 2004. 
[3] D. D. Woods and N. B. Sarter, “Learning from automation surprises 
and "going sour" accidents: Progress on human-centered automation : 
final report,” Va: Ohio State University, Institute for Ergonomics, 
Cognitive Systems Engineering Laboratory; National Aeronautics and 
Space Administration; National Technical Information Service, 
distributor, 1998. 
[4] I. Kaminer, O. Yakimenko, A. Pascoal, and R. Ghabcheloo, “Path 
Generation, Path Following and Coordinated Control for TimeCritical 
Missions of Multiple UAVs,” in American Control Conference,  
2006, pp. 4906–4913. 
[5] S. Wegener, UAV autonomous operations for airborne science 
missions, 2004. 
[6] J. Uhrmann, R. Strenzke, A. Rauschert, and A. Schulte, “Manned-
unmanned teaming: Artificial cognition applied to multiple UAV 
guidance,” in NATO SCI-202 Symposium on Intelligent Uninhabited 
Vehicle Guidance Systems, 2009. 
[7] J. Uhrmann, R. Strenzke, and A. Schulte, “Task-based Guidance of 
Multiple Detached Unmanned Sensor Platforms in Military 
Helicopter Opertations,” in COGIS 2010, 2010. 
[8] R. Strenzke and A. Schulte, “The MMP: A Mixed-Initiative Mission 
Planning System for the Multi-Aircraft Domain,” in Scheduling and 
Planning Applications woRKshop (SPARK) at ICAPS, 2011. 
[9] D. Donath, A. Rauschert, and A. Schulte, “Cognitive assistant system 
concept for multi-UAV guidance using human operator behaviour 
models,” in Conference on Humans Operating Unmanned Systems 
(HUMOUS'10), 2010. 
[10] H. Eisenbeiss, “A mini unmanned aerial vehicle (UAV): system 
overview and image acquisition,” International Archives of 
Photogrammetry. Remote Sensing and Spatial Information Sciences, 
vol. 36, no. 5/W1, 2004. 
[11] T. B. Sheridan and W. L. Verplank, “Human and Computer Control 
of Undersea Teleoperators,” Ft. Belvoir: Defense Technical 
Information Center, 1978. 
[12] J. Uhrmann, R. Strenzke, and A. Schulte, “Human Supervisory 
Control of Multiple UAVs by use of Task Based Guidance,” in 
Conference 
on 
Humans 
Operating 
Unmanned 
Systems 
(HUMOUS'10), 2010. 
[13] R. Onken and A. Schulte, “System-ergonomic design of cognitive 
automation: Dual-mode cognitive design of vehicle guidance and 
control work systems,” Heidelberg: Springer-Verlag, 2010. 
[14] G. Jarasch, S. Meier, P. Kingsbury, M. Minas, and A. Schulte, 
“Design Methodology for an Artificial Cognitive System Applied to 
Human-Centred Semi-Autonomous UAV Guidance,” in Conference 
on Humans Operating Unmanned Systems (HUMOUS'10), 2010. 
[15] NATO, Use of helicopters in land operations - doctrine, 49(E). 
[16] H. Putzer and R. Onken, “COSA - A generic cognitive system 
architecture based on a cognitive model of human behavior,” 
Cognition, Technology & Work, vol. 5, no. 2, pp. 140–151, 2003. 
[17] J. Laird, A. Newell, and P. S. Rosenbloom, “Soar: An architecture for 
general intelligence,” Stanford, CA: Dept. of Computer Science, 
Stanford University, 1986. 
[18] S. G. Hart and L. E. Staveland, “Development of NASA-TLX (Task 
Load Index): Results of empirical and theoretical research,” Human 
mental workload, vol. 1, pp. 139‐183, 1988. 
[19] M. R. Endsley, “Situation awareness global assessment technique 
(SAGAT),” in Aerospace and Electronics Conference 1988, pp. 789 – 
795, 1988. 
 
hindering
surprising
wearing
Automatic Task Insertion
supportive
comprehensible
relieving
useless
necessary
time-consuming
time-saving
unpleasant
pleasant
unusable
highly usable
general interaction
target ident.
area recce
time crit. retasking
Human Machine Interface
52
COGNITIVE 2011 : The Third International Conference on Advanced Cognitive Technologies and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-155-7

