Quality and Performance Optimization of Sensor
Data Stream Processing
Anja Klein
SAP Research Center Dresden, SAP AG
Chemnitzer Str. 48
01187 Dresden, Germany
anja.klein@sap.com
Wolfgang Lehner
Database Technology Group, TU Dresden
Helmholtzstrae 10
01069 Dresden, Germany
wolfgang.lehner@tu-dresden.de
Abstract—Intelligent sensor devices together with data stream
management systems allow the automatic recording and process-
ing of huge data volumes to guide any kind of process control
or business decision. However, a crucial problem is posed by
data quality deﬁciencies due to imprecise sensors, environmental
inﬂuences, transfer failures, etc. If not handled carefully, they
misguide decisions and lead to inappropriate reactions. In this
paper, we present the quality-driven optimization of stream
processing that improves the resulting quality of data and service.
After an introduction to data quality management in data
streams, we deﬁne the targeted optimization problem comprising
the optimization objectives and parameters that conﬁgure the
required stream processing operators. Based on the generic op-
timization framework, we discuss and evaluate the optimization
execution in batch and continuous mode. Further, we shed light
on the crucial deﬁnition of the stream partition length used for
the optimization that signiﬁcantly inﬂuences the optimization
performance. Finally, we provide a detailed validation of the
proposed optimization strategies as well as the scalability of the
overall approach not only at artiﬁcial data streams, but also using
the real-world example of contact lens production monitoring.
Keywords - Data Quality; Data Stream Processing; Query
Optimization; Heuristic Optimization
I. INTRODUCTION
Data stream management systems have been developed to
process continuous data ﬂows of high data rate and volume.
For example, turnover values or sales volume may be streamed
from distributed afﬁliations to the central controlling depart-
ment to derive management strategies. Further, data streams
are recorded in sensors networks to control manufacturing
processes or maintenance activities. In this paper we illustrate
the quality control in contact lens manufacturing, where lens
thickness and axial difference are measured to derive a quality
indicator for the production line.
In most applications data stream systems encounter re-
stricted resources, such as limited memory capacity, data
transfer capability and computational power. To meet these
constraints, data stream volume has to be reduced by pro-
cessing the streamed information. Data reduction always goes
along with a loss of information. Data processing results
such as aggregations can only be approximated, so that an-
swers are incorrect or incomplete with respect to the true
outcome. Moreover, most data stream sources suffer from
limited data quality in multiple dimensions from the beginning.
The correctness is decreased for example by restricted sensor
precisions, by typos in text elements, or by RFID readers
failing to scan an item properly. The completeness of a data
stream is reduced whenever a true world event is missed due
to sensor or system malfunction. Information and decisions
derived from such falsiﬁed sensor data are likely to be faulty,
too. Therefore, data quality problems have to be handled
carefully.
Data quality information, that describe data quality de-
ﬁciencies due to data sources and/or data processing, can
be recorded and transfered in the data stream, e.g., by the
quality propagation model (QPM) presented in [1]. It provides
a comprehensive framework for data quality management in
streaming environments. Only then, data quality information
are provided to the user to enable the comprehensive evalu-
ation of imprecise and/or incomplete data stream processing
results. Faulty information can be detected to prevent from
incorrect decisions. Furthermore, quality information may
identify processed data stream results as too insecure to derive
any suitable decision. In that case, the data quality has to be
improved to re-enable the conﬁdent decision-making.
This paper details and extends the quality-driven opti-
mization of sensor data stream processing to improve the
resulting data quality, while complying to the given system
constraints, that we ﬁrst published in [2]. Based on data
quality information provided in the stream, the data stream
operators are conﬁgured to maximize the quality outcome
to meet user-deﬁned quality requirements. The online tun-
ing is performed continuously in parallel to the traditional
data stream processing. The optimal operator conﬁguration is
adapted to varying data stream characteristics and changing
user-deﬁned requirements on resulting data quality.
For the ﬁeld of data quality improvement and optimization
our contributions are as follows.
• We present the deﬁnition of streaming data quality and
discuss the conﬂicts between the embraced data quality
dimensions. Further, we identify candidates for the data
quality improvement out of a comprehensive set of data
stream operators. We discuss derived parameters and their
impact on the data quality-driven improvement of the
249
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

stream processing in Section II.
• In Section III, we propose the optimization framework
that adapts the stream processing for data quality im-
provement. We discuss the optimization execution in
batch and continuous mode that supports the determi-
nation of the applied stream partition length. Further,
we present the data quality improvement task as multi-
objective optimization problem and range it in the tradi-
tional operations research classiﬁcation.
• We discuss the speciﬁc components of the heuristic
optimization algorithm Quality-driven evolution strategy
(QES) to solve the data quality optimization problem in
Section IV.
• We present a comprehensive evaluation of the presented
algorithm in Section V. We analyze the practicability
of the proposed optimization framework, validate the
inﬂuences of different optimization strategies, compare
the performance of QES to further optimization heuristics
and evaluate the capability at the example of the contact
lens production control.
We complete this paper with a discussion of related work
in the ﬁeld of data stream quality, quality improvement and
optimization in Section VI and concluding remarks in Sec-
tion VII.
II. PROBLEM ANALYSIS
In this section, we ﬁrst deﬁne the data quality in data
streams to derive objectives for the data-quality driven op-
timization, which are then discussed in detail. Afterwards, the
candidates for data quality improvement are described. First,
the sampling rate conﬁguration is illustrated, followed by the
interpolation conﬁguration. Then, the conﬁguration impact of
group size for aggregation, frequency analysis and ﬁltering as
well as of data quality window size is depicted.
A. Data Quality in Data Streams
A data stream comprises a continuous stream of m tuples
τ, consisting of n attribute values Ai(1 ≤ i ≤ n) and the
represented time interval [tb, te]. To allow for the efﬁcient
management of data quality in data streams, we adopt the data
quality window approach introduced in [1]. DQ information
is not forwarded for each single data item, but aggregated
over ωi data items independent for each stream attribute Ai.
The stream is partitioned into κi consecutive, non-overlapping
jumping data quality windows w(k) (1 ≤ k ≤ κi), each of
which is identiﬁed by its starting point twb, its end point
twe, the window size ωi and the corresponding attribute
Ai as illustrated in Figure 1. Beyond the data stream items
x(j)(twb ≤ j ≤ twe), the window contains |Q| data quality
information qw, each obtained by averaging the tuple-wise DQ
information over the window.
Furthermore, the executed data processing steps have to be
tracked in the quality propagation model (QPM) to not lose
data quality information. When data streams are aggregated or
joined, their data quality information have to be summarized,
too. Only then, the data quality path through the processing can
be monitored and data quality deﬁciencies introduced during
data stream processing are captured.
0.8 …
0.9
…
Completeness
1
1
AmountOfData
0,00081967 …
0.00053587
…
Confidence
…
…
…
0.004
0.412
210
0.0039
…
Accuracy
0.403 0.409 0.398 0.392 0.415 0.394 0.410 0.387 0.403
…
CenterThickness
219
218
217
216
215
214
213
212
211
…
Timestamp
tb= 210
tb= 215
te= 214
te= 219
= 5
= 5
Fig. 1.
Data stream sample
The probably best-known/most referenced, comprehensive
and balanced deﬁnition of data quality was presented 1996 by
Wang et al. [3]. They distinguish four data quality categories of
15 data quality dimensions incorporating data quality aspects
of raw data, data sets as well as user requirements. We
deﬁne data quality in data streams based on their ﬁndings on
intrinsic (accuracy, believability, objectivity, reputation) and
contextual data quality dimensions (value-added, relevancy,
timeliness, completeness, amount of data). Due to high data
stream volume and rate, a manual evaluation of data quality is
not possible, so that subjective dimensions of representation
and accessibility cannot be measured to describe data streams.
While the accuracy describes the systematic error of data
stream values resulting from deﬁciencies in data sources,
the believability in context of data streams can be equated
with the conﬁdence describing random errors produced by
unforeseeable inﬂuences (e.g., environmental affects in sensor
networks). Objectivity and reputation of automatic data stream
sources can be assumed as guaranteed and do not have
to be monitored. As value-added and relevancy are aspects
subjective to the respective user, the contextual dimensions
reduce to timeliness or up-to-dateness, completeness deﬁning
the rate of originally measured stream items compared to
interpolated ones and the amount of data d of raw data items
represented by an aggregation result.
Deﬁnition:
The data quality Q of a data stream D is deﬁned
by the set of ﬁve data quality dimensions: accuracy a, conﬁ-
dence ϵ, completeness c, amount of data d and timeliness u.
As stated in the introduction, the data quality-driven opti-
mization conﬁgures the data stream processing to maximize
the above deﬁned stream data quality while guaranteeing the
compliance to the restricted system resources. As metric for
the resource load, we use the data stream volume V . The
less volume is needed, the better the constraints are met.
Hence, the minimization of the data volume is admitted to
the optimization goals. Finally, from the user’s point of view a
high data volume also has positive effects. The more details are
given in the data stream, the better decisions can be derived.
To measure this data granularity, we select the timeframe T
represented by one data stream tuple. While raw data depict
250
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

one point in time, the result of a data stream aggregation
represents a larger time interval. The wider the timeframe,
the lower is the granularity. To support the detailed evaluation
of streaming data the granularity has to be maximized.
Operator
Parameter
a
ϵ
c
d
u
V
T
Projection
—
Selection
—
Join
—
Aggregation
GroupSize l
+
-
-
Sampling
Rate rsa
-
-
+
+
Frequ. an.
GroupSize l
+
-
-
Filter
GroupSize l
Algebra
—
Threshold
—
WindowSize ω
+
-
(-)
Accuracy a
Completeness c
Stream volume V
Conﬁdence ϵ
Amount of data d
Granularity T
Timeliness u
TABLE I
OPTIMIZATION CANDIDATES FOR QUALITY IMPROVEMENT
Table I summarizes the optimization objectives composed of
data quality dimensions, data stream volume, and granularity.
To identify candidates for the data quality improvement, we
analyze the operator repository of the QPM consisting of tra-
ditional data stream operators like join and selection, operators
of the signal analysis, which are often applied to sensor data
streams, and operators of the numerical algebra like addition or
division as well as the threshold control. Besides the operator
conﬁgurations, we present the size of the jumping data quality
windows ω as interesting parameter. Table I shows the impact
of a parameter increase: the quality values are either increased
(+) or decreased (-).
B. Objectives
This section deﬁnes the ﬁtness functions for each objec-
tive of the data quality-driven optimization. As the accuracy
describes defects or imprecisions of data stream sources, it
cannot be improved by any operator conﬁguration. This data
quality can be removed from the list of optimization objectives.
The objectives determined above span different value do-
mains. For example, the window completeness constitutes
values in the range 0 ≤ cw ≤ 1, while the absolute statistical
error in the dimension conﬁdence is unlimited 0 ≤ ϵw ≤ ∞.
To allow the quantitative comparison of different objectives,
we normalize the objective functions to the range [0, 1].
1) Conﬁdence: The conﬁdence illustrates the statistical
error ϵ due to random environmental interferences (e.g., vibra-
tions, shocks) deﬁning the interval [v−ϵ; v+ϵ] around the data
stream value v containing the true value with the conﬁdence
probability p. ϵ is deﬁned by the (1−p/2)-quantile α and the
data varianceσ2 of each data quality window w. For example,
for p = 99% the initial conﬁdence of a data quality window
including the lens thickness measurements
{0.396mm, 0.428mm, 0.412mm, 0.379mm, 0.403mm}
(1)
is set to ϵw = α · σ = 2.58 · 0, 000286mm = 0, 0007224mm.
The average statistical error over all data stream attributes
has to be minimized to maximize the data quality conﬁdence.
The objective function is normalized by division with the
maximal statistical conﬁdence error ϵmax in the stream. The
objective fϵ is deﬁned as follows.
fϵ
:
min
1
n · ϵmax
n
X
i=1
1
κi
κi
X
k=1
ϵw(k)
(2)
2) Completeness: The completeness addresses the problem
of missing values due to stream source failures or malfunc-
tions. Multiple estimation or interpolation strategies exist to
deal with missing values in ETL processes and data cleansing
[4]. We apply the linear interpolation as compromise between
the quality of value estimation and computational capacity.
The data quality dimension completeness c is accordingly
stated as the ratio of originally measured, not interpolated
values compared to the size of the analyzed data quality
window.
For example, the sensor for axial difference misses the lens
at timestamp t =′ 237′. To nevertheless derive the quality
indicator, the missing value is computed as follows.
ax(′237′) = 1
2 · (ax(′236′) + ax(′238′))
(3)
To note the sensor failure, the completeness of the data quality
window [′230′,′ 239′] is set to cw = 0.9.
To conform with the objective above, the objective of max-
imal completeness is transformed to the minimizing problem
fc, which minimizes the ratio of interpolated data items. Here,
no normalization is required as the domain [0, 1] is already
provided by the completeness deﬁnition.
fc
:
min
1
n
n
X
i=1
1
κi
κi
X
k=1
(1 − cw(k))
(4)
3) Amount of Data: The amount of data determines the set
of raw data x used to derive a data stream tuple y = f(x). The
higher the amount of data, the more reliable is the processed
information. To eliminate outliers to derive statistically stable
information of the production line quality, the quality indicator
is averaged over a certain set of contact lenses. Thereby, the
aggregation group size l = 20 leading to d = 20 produces
more reliable results than l = 2(d = 2).
To transform the objective of maximal amount of data to
a minimization problem, we calculate the difference to the
highest possible amount of data d = m, that comprises the
complete data stream. The maximum m serves at the same
time as normalization weight.
251
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

fd
:
min
1
n · m
n
X
i=1
1
κi
κi
X
k=1
(m − dw(k))
(5)
4) Timeliness: The timeliness is deﬁned as difference be-
tween tuple timestamp t(j)(1 ≤ j ≤ m) and current system
time clock. For example, the timeliness of ax(′13 : 26 : 54′)
at ′13 : 28 : 10′ is u = 76s. To maximize the data quality
dimension timeliness, the average tuple age normalized by the
maximum age umax has to be minimized.
fu :
min
1
m · umax
m
X
j=1
u(j)
(6)
=
1
clock − tmin
·

clock − 1
m
m
X
j=1
t(j)

 (7)
5) Data Stream Volume: The data stream volume V deﬁnes
the number of transfered data values in n stream attributes over
m data tuples. Besides, the transfered data quality information
have to be incorporated. The additional volume is computed
based on the number of transfered data quality dimensions Qi
per attribute Ai and the average data quality window size ¯ωi.
V
=
m · (n + 1) +
n
X
i=1
m
¯ωi
· |Qi|
(8)
The average volume of a data stream tuple of the contact
lens stream described with |Qi| = 4 dimensions and an
average data quality window size of ¯ωi = 20 results in
V = (4+1)+1/20·4 = 5.2. To normalize the stream volume
to the data range [0, 1], we refer to the maximal stream length
mmax = rmax/r · m determined by the current stream rate r
and the maximal manageable rate rmax(e.g., rmax = 1/ms).
The maximal data volume further depends on the maximal
data quality window size ω = 1, such that
Vmax
=
mmax · (n + 1) + mmax ·
n
X
i=1
|Qi|.
(9)
To minimize the costs for data stream transfer and processing,
the normalized data stream volume has to be minimized.
fV
:
min
V
Vmax
(10)
6) Granularity: The data stream granularity T is measured
as the average timeframe [te−tb] of all data stream tuples. For
example, the timeframe of the averaged quality indicator with
l = 20 constitutes in T = 20s. For raw data items describing
one point in time, the granularity equals 0, as te = tb. To
maximize the granularity, the average timeframe normalized
by its maximum Tmax has to be minimized.
fT
:
min
1
m · Tmax
m
X
j=1
te(j) − tb(j)
(11)
thc
ax
Sampling
Num. Op.
Selection
Aggregation
Join
Data Source
the1
the2
the3
the4
. 2
sum
avg
drift
thd = 1/4 (the1+the2
+ the3+ the4)
th = 1/3 (thd+2thm)
scrap =
th + sum(ax)
rthe1
rsa,ax
lax
rax
Fig. 2.
Processing tree
C. Conﬁguration Parameters
The analysis of typical data stream operators identiﬁed
sampling, aggregation and frequency analysis as conﬁguration
candidates for the data quality-driven optimization. This sec-
tion ﬁrst deﬁnes the dimension of the optimization problem
domain. Afterwards, the impact of the conﬁguration parame-
ters of the identiﬁed candidates are discussed.
Figure 2 shows the processing graph of the contact lens
application. The goal is to monitor the overall production
quality, i.e., the fraction of contact lenses that have to be
removed from the production line as scrap, to predict the next
maintenance date. Therefore, four thickness measurements the
at the lens edge are summarized and added to the weighted
center thickness thc. All sensor streams are sampled up
rsa > 1 or down rsa < 1 to reduce the overall data volume
and adjust the data stream rates in case of missing data tuples.
Then, the axial difference stream ax is aggregated to sum up
potential errors. The measurements are combined to determine
the scrap indicator scrap. If scrap < 0.5 the contact lens does
not fulﬁll the required quality levels and has to be removed
from the production. To produce stable monitoring results,
individual scrap indicators are averaged in sliding windows.
Finally, the drift of the scrap fraction, i.e the drift of the
lens production quality, can be used to guide the maintenance
planning of the production line.
To optimize the resulting data quality, all sampling rates
and group sizes can be modiﬁed. For example, the contact
lens scenario comprises |sam| = 8 sampling operators and
|agg| = 3 aggregations. If applied in the data stream pro-
cessing, also the group size of performed frequency anal-
yses fre can be optimized. The size of the data quality
windows constitutes the last optimization parameter, that can
be determined independently for each data source s ∈ S.
The contact lens quality requires |S| = 6 sensors, adding 6
252
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

parameters to the optimization problem. The overall problem
optimizes dim = |sam|+|agg|+|fre|+|S| parameters, e.g.,
dim = 8 + 3 + 0 + 6 = 17, deﬁning the dimension of the
problem domain.
1) Sampling Rate: The down-sampling reduces the data
stream volume by randomly skipping a given set of data items
deﬁned by the sampling rate rsa < 1. The information loss
provoked by down-sampling represents a statistical error and
has to be captured in the dimension conﬁdence. Reconsider the
data quality window presented above in Equation 1 with the
true average avg = 0.4mm sampled with rsa = 0.5. The sam-
ple average ranges from avg = 0.386mm to avg = 0.414mm
corresponding to a of e+ = 0, 0143mm.
Deﬁnition:
The statistical error due to down-sampling can
be estimated based on the conﬁdence interval deﬁned by
Haas in [5], such that
ϵ+ = α · σ(w)
√ω · rsa
·
√
1 − rsa,
(12)
where σ2(w) states the variance of data stream values in
the respective DQ window and α describes the conﬁdence
probability p as the (1 − p/2)-quantile.
Low sampling rates skip large data stream parts resulting
in a high statistical error due to higher information loss. The
conﬁdence error rises with decreased sampling rate. On the
contrary, low down-sampling rates reduce the data stream vol-
ume. The objectives of minimized conﬁdence ϵ and minimized
stream volume V conﬂict with each other. As timeliness and
data stream volume are aligned due to higher processing times
for large tuple numbers, the objectives of minimal timeliness
u and minimal conﬁdence ϵ conﬂict with each other in the
same manner.
The up-sampling (rsa > 1) inserts data items into the data
stream. For example, a linear data interpolation is executed
with the rate rsa = 2, which doubles the data stream length.
Hence, the up-sampling increases the fraction of interpolated
data and has to be tracked by updating the DQ dimension
completeness.
Deﬁnition:
During up-sampling, the window completeness
cw is divided by the sampling rate rsa > 1, such that the
completeness is stated as c′
w = cw/rsa.
The higher the up-sampling rate, the more data items are
generated. The lower is the resulting completeness and the
higher the data stream volume. Hence, high up-sampling rates
have a negative impact both on completeness c and data stream
volume V .
As described above, the relation of stream rates has to
remain constant for each processing tree level. For example,
the up-sampling in data stream thc increases the down-
sampling rate for the1−e4. The up-sampling has an indirect
positive impact on the statistical error in the data quality di-
mension conﬁdence. The objective of maximized completeness
c achieved by low sampling rates rsa contradicts with the
minimization of the conﬁdence ϵ resulting from high rsa.
2) Group Size: First and foremost, the group size l consti-
tutes a parameter in the deﬁnition of data processing queries.
For example during the calculation of the turnover, it is
essential whether the sales volumes of one day (l = 1d) or
one month (l = 30d) are summed up. However, during the
processing of high-volume data streams situations can arise,
where the group size must not be deﬁned strictly.
The group size of aggregation and frequency analysis1
share the same impact on the optimization objectives, as they
summarize stream tuples to a smaller data volume. The larger
their group size is deﬁned, the more the data stream volume
is reduced.
The more tuples are aggregated or serve as basis for fre-
quency analysis, the larger is the amount of data of outcoming
results. As the aggregation reduces the data volume, maximal
amount of data goes along with minimal data stream volume.
However, the summarization of information has a negative
impact on the data stream granularity, as one processing result
represents the timeframe spanned by all incoming data tuples
in the respective group. The larger the group size, the larger is
the resulting timeframe. During the conﬁguration of the group
size l positive effects on data stream volume V and amount
of data d oppose negative effects on the granularity T.
The objective conﬂict can be resolved by conﬁguring the
data stream rate with the help of sampling and/or interpolation.
During group sizes increase, the same stream rate increase
leads to consistent amount of data. However, the consistency
of optimal stream volume V , amount of data d and granularity
T interferes with the objectives of minimized conﬁdence ϵ and
maximized completeness c as declared above.
3) Window Size: The deﬁnition of the size of data quality
windows constitutes a compromise between ﬁne granularity
and high volume of transfered data quality information.
The wider the data quality window, the lower is the overhead
for the data quality transfer and, thus, the lower is the overall
stream volume. On the contrary, the larger the data quality
window is deﬁned, the more tuple-wise DQ information have
to be aggregated in one window-wise quality value balancing
out meaningful data quality peaks.
Therefore, we add the window size ω itself as optimization
parameter. The objective function fω deﬁning the averaged
window size ¯ωi over all data quality windows and stream
attributes normalized with the maximal possible window size
ω = m has to be minimized.
fω
:
min
1
n · m
n
X
i=1
¯ωi
(13)
As more data quality information have to be transfered and
processed, the objective of low window sizes ω contradicts
with minimal data stream volume V and maximal timeliness
u.
1The frequency analysis computes amplitude and phase of all stream
inherent frequency bands.
253
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

ϵ
c
d
u
V
T
ω
ϵ
-
rsa
rsa
rsa, l
rsa, l
c
rsa
-
rsa
rsa
rsa, l
rsa, l
d
rsa
-
rsa, l
u
rsa
rsa
-
l
ω
V
rsa, l
rsa, l
-
l
ω
T
rsa, l
rsa, l
rsa, l
l
l
-
ω
ω
ω
-
TABLE II
OBJECTIVE CONFLICTS
Table II summarizes the conﬂicts between the optimization
objectives. The cells indicate the conﬁguration parameter
leading to the respective conﬂict.
III. QUALITY-DRIVEN OPTIMIZATION
In this section, we present the framework architecture for
DQ-driven optimization consisting of satisﬁability checks and
optimization component. To solve the deﬁned problem, we
propose the quality-driven evolution strategy.
A. Optimization Framework
The data quality-driven optimization is executed contin-
uously to tune the data stream processing during system
runtime. As soon as an optimal parameter set is found and
deployed, it has to be checked against the currently processed
data stream. The online tuning allows the seamless adaptation
to varying stream rates, measurement values and data quality
requirements.
First, the system evaluates by means of static information
like maximal sensor stream rate or sensor precision, if the user-
deﬁned quality requirements can be accomplished or conﬂicts
exclude a realization of all sub-objectives. In the latter case,
the conﬂict is reported to the user. To check the satisﬁability
of DQ requirements, no access to streaming data is needed.
Heuristic optimization algorithms approximate the optimal
problem solution by iteratively improving the achieved ﬁtness.
Different solution individuals have to be applied, evaluated and
compared.
As the optimization must not interfere with the ongoing data
stream processing, it is separated on an independent system
component as illustrated in Figure 3. To execute the optimiza-
tion in parallel with the traditional data stream processing,
the processing path with all its operators is copied in the
optimization component. In each iteration of the optimization
algorithm the evaluated solution individual determines the
speciﬁc path conﬁguration of sampling and interpolation rates
as well as group and window sizes.
Each solution is evaluated by directing a representative data
stream partition through the conﬁgured processing path. We
propose two approaches for the partition selection in Section
III-C. As soon as the partition is completely processed, the
average data quality result for each dimension, the average
granularity and the used data stream volume are computed
Data Stream Processing
Data
Sources
Data
Sink
Data Quality
Initialization
rsa,opt, lopt
Ȧopt
Copy of Data Stream
Processing
Optimization Algorithm
fitness =
f(İ,c,d,u,V,T,Z)
next solution
rsa , l, Ȧ
DQ-Driven Optimization
Satisfiability Check
User-defined Quality
Requirements
Fig. 3.
Optimization process
and returned to the optimization algorithm to calculate the
ﬁtness of the tested conﬁguration. If the user-deﬁned quality
requirements are not met, the ﬁtness guides the determination
of the next solution individual to iteratively improve the
achieved ﬁtness.
As soon as the requirements are accomplished, the optimiza-
tion problem is solved. The new parameter setting is applied
to the original processing path. The sampling operators are up-
dated with the optimized sampling rates rsa,opt. The frequency
analyses and aggregations are updated with the determined
group sizes lopt. Finally, the data quality initialization at the
sensor nodes is re-conﬁgured with the new window sizes ωopt.
After deploying the found parameter set, the next optimization
run is performed to adapt the processing to dynamic streams
characteristics.
The logical distinction between optimization and processing
enables also the physical separation, for example on a distinct
server node. Thus, the optimization task has no negative
impact on the performance of the traditional data stream
processing.
B. Satisﬁability Check
Four quality checks have to be performed to prevent from
optimizing against unsatisﬁable user-requirements as shown in
Algorithm 1.
First, the desired conﬁdence error is evaluated. The best
possible conﬁdence is achieved, when no down-sampling is
performed. Only initial statistical errors of the sensors reduce
the conﬁdence. Thus, the requested conﬁdence objective ϵreq
must not be smaller than the square root of all initial conﬁ-
dence errors ϵSi (line 1).
Second, only completeness deﬁciencies due to up-sampling
can be reduced by DQ-driven optimization. The completeness
objective creq must not exceed the average initial completeness
cSi provided by the sensors (line 2).
The conﬂicting objectives of minimal data stream volume
V , minimal window size ω and maximal amount of data d
are compared based on the resulting stream length m. As
the amount of data cannot exceed the stream length, it must
254
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

hold that dreq ≤ m. By applying Equation 8, we derive the
satisﬁability check shown in line 3.
Finally, the conﬂict between amount of data d and gran-
ularity T is controlled based on the maximal stream rate
rmax introduced in Section II-B in line 4. For example, the
required amount of data of dreq = 100 and the maximal rate
of rmax = 1/ms leads to the minimal achievable granularity
of Tmin = 100ms.
Algorithm 1: Satisﬁability Check
Input: ϵreq, creq, Vreq user-deﬁned requirements
Output: sat=FALSE satisﬁability
if ϵreq ≥
qP|S|
i=1 ϵ2
Si
1
∧creq ≤
1
|S|
P|S|
i=1 cSi
2
∧dreq ≤
Vreq
n+1+Pn
i=1
1
ωreq |Qi|
3
∧Treq ≥ dreq
rmax
4
5 then
sat = TRUE;
6
C. Batch vs. Continuous Optimization
The stream partition for optimization constitute a data
stream window of ζ data tuples. It may either be selected
in batch-mode at the beginning of each optimization run and
used in each iteration without changes. At the other hand, the
window can be updated for each iteration with current tuples
to reﬂect the dynamic progression of the data stream and allow
the continuous optimization.
]
j
j +100
Optimization run i
run i +1
]
Iteration k   k+1
k+2
Data stream
run i +1
a)
b)
Fig. 4.
Batch & continuous optimization
The batch-mode selects a constant stream partition of the
last ζ tuples for one complete optimization run as shown
in Figure 4a. The parameter ζ depicts the trade-off between
representative partition length and duration of one optimization
run. Besides, the length is restricted by limited memory
capacity in most streaming environments. It only represents
a small, static data stream window.
In contrast, the continuous optimization approach follows
the dynamic stream behavior by selecting a new stream parti-
tion for each iteration of the applied optimization algorithm.
As shown in Figure 4b, a larger stream fraction is used.
However, the continuous mode exchanges the base data for
optimization in each iteration. A good solution derived from
the ﬁtness in iteration k on partition k may perform poor if
applied to base data k + 1 in iteration k + 1. The ﬁtness will
not increase monotonically, but may diverge. To guarantee the
algorithm termination, additional criteria like allowed duration
or number of ﬁtness evaluations have to be deﬁned.
The static optimization in batch-mode guarantees the con-
verging of the ﬁtness function and, thus, the accomplishment
of user-deﬁned quality requirements. However, the computed
optimal solution only holds for the processed short stream
partition, so that a permanent control with subsequent opti-
mization runs is necessary. The continuous approach allows
the inclusion of dynamic data stream alterations during the
optimization process. Thereby, divergences of the ﬁtness func-
tion may arise that must be encountered by supplementary
terminations rules.
D. Optimization Classiﬁcation
The operations research combines mathematics and formal
science to ﬁnd and deﬁne methods and algorithms to arrive at
optimal or near optimal solutions to complex problems. Op-
timization problems are classiﬁed according to their domain,
their objective function and respective problem constraints. In
this section, we will range the optimization problem of data
quality-driven stream processing in this classiﬁcation scheme.
The problem analysis revealed seven objectives, which
partly conﬂict with each other and deﬁne a multi-objective
optimization problem. There exist various strategies to solve
these problems. On the one hand, the Pareto optimization
allows the ﬁnding of a set of optimal compromises, that
dominate all other possible solutions. That is, the improvement
of one sub-objective is only possible with a decline of one or
more of the other sub-objectives. In Section IV we deﬁne the
multi-objective quality-driven evolution strategy (MO-QES),
which optimizes the Pareto front in the problem search domain
by solving the objective fmulti.
Deﬁnition:
The multi-objective ﬁtness function fmulti
:
Rdim 7→ R7 deﬁnes the ﬁtness of a solution individual as
the Pareto-dominance of the achieved sub-objective fi(i ∈
{ϵ, c, d, u, V, T, δ}).
On the other hand, sub-objectives can be summarized in
one optimization function fsingle, which will be minimized
or maximized by the single-objective quality-driven evolution
strategy (SO-QES). Cost weights deﬁned for each objective
determine the order and importance of optimization. However,
the user-deﬁned weighing may restrict the search space result-
ing in minor optimization solutions. Multiple optimization runs
are required to determine optimal cost weights.
Deﬁnition:
The single-objective ﬁtness function fsingle :
Rdim
7→ R calculates the overall ﬁtness of a solution
individual as weighted sum of the obtained sub-objectives,
such as
fsingle
=
X
i∈{ϵ,c,d,u,V,T,δ}
ci · fi
(14)
The data quality dimensions completeness, amount of data
and timeliness are deﬁned using linear computation functions,
so that fc, fd and fu pose linear optimization problems.
255
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

The same holds for the data quality window size fω and
the granularity fT . Due to the square root deﬁnition of the
statistical error computation, the objective function fϵ gives a
non-linear optimization problem. A second non-linear problem
is posed by fV , which minimizes the stream volume based on
the window size ω.
While the parameters l and ω as well as ﬁtness of the sub-
objectives fd, fu, fV , fT and fω allow only discrete values,
the domains of the parameters rsa and the objective functions
fϵ and fc propose continuous optimization problems.
Moreover, the search space is restricted by side conditions.
To guarantee join partners, the data stream rates in one tree
level have to stay in constant relation to each other. For
example, the duplication of rthe1 (see Figure 2) requires i.a.
the duplication of rax. Either, the rate of the sampling operator
rsa,ax has to be doubled, or the group size lax has to be
halved. Thus, the dependence of sampling rates and group
sizes deﬁnes side constraints for each processing tree level,
such that
∀ level l
∀ ri, rj ∈ l : ri = rj.
(15)
As in multi-objective optimization problems the most com-
plex sub-objective deﬁnes the complexity of the overall prob-
lem, the quality-driven optimization is deﬁned as follows.
Deﬁnition:
The quality-driven process optimization consti-
tutes a multi-objective, non-linear, continuous optimization
problem with side conditions.
There exists no deterministic algorithm to solve optimiza-
tion problems of this complexity in reasonable time. However,
the stream processing optimization shall be executed on-the-
ﬂy without interrupting the data ﬂow. It is essential to provide
good solutions in an acceptable timeframe. Further, the optimal
solution is not required in most cases. Rather, the user or data
consuming application deﬁnes data quality levels, which have
to be met. Heuristic algorithms offer an appropriate answer
to such optimization problems. They provide fast results by
approximating the optimal solution.
Heuristic optimization algorithms range from simple ap-
proaches like the Monte-Carlo-Search over more sophisticated
strategies such as Hill Climbing and Simulated Annealing to
complex Evolutionary Algorithms. Evolutionary Algorithms
comprise genetic algorithms working on binary data and the
evolution strategy supporting real parameters and objectives.
As only the evolution strategy can solve real-valued multi-
objective as well as single-objective problems, we apply this
heuristic to solve the DQ-driven optimization.
IV. EVOLUTION STRATEGY
The evolution strategy constitutes a stochastic, population-
based search heuristic inspired by the principles of natural
evolution. It can be applied to arbitrary optimization problems
and requires nothing but the objective function(s) of the
optimization problem to guide its search. A population of
possible solutions is iteratively recombined and mutated to
select the best population individual as approximated global
optima.
To improve the data quality via conﬁguration of the data
processing, we have to adapt the generic algorithm structure
of the evolution strategy to the deﬁned quality-driven optimiza-
tion problem. In this section, we present the speciﬁcation of
the quality-driven evolution strategy (QES) including speciﬁc
functions for recombination, mutation and selection.
Algorithm 2: QES
Input: domain of possible inputs,
DQ user-deﬁned requirements
Output: P population of optimal solutions
1 t = 0;
2 initialize(P(t), domain);
3 while DQ.notAchieved() do
Pc(t) = recombine(P(t));
4
mutate(Pc(t));
5
P(t + 1) = selectNextGeneration(Pc(t), P(t));
6
t = t + 1;
7
end
8
Algorithm 2 shows the overall structure of QES. The ﬁrst
population P(0) is randomly initialized in line 2 to cover
the complete search domain. The ﬁrst step of the repeated
iteration process recombines the solution individuals of the
current population P(t) in line 4 to build new solution
candidates Pc(t). They are randomly mutated to allow new
solutions to enter the population in line 5. Finally, they are
evaluated with the help of the objective function fsingle or
fmulti, respectively, to select the best individuals of Pc(t) to
build the next generation P(t + 1) in line 6. The quality-
driven evolution strategy terminates, when all user-deﬁned data
quality requirements are met (line 3). Other termination criteria
are the allowed number of performed solution evaluations or
the planned execution time.
The recombination is designed to hand down and combine
positive traits of different solutions individuals. First, parent
solutions are chosen randomly from the current generation.
Then, the dim parameter conﬁgurations of parent pairs are
combined. The children’s parameters are determined by aver-
aging each of the parents’ parameters. As group and window
sizes only allow integer values, they are rounded up.
Algorithm 3 illustrates the mutation of the recombined
solution candidates. Due to different value domains, speciﬁc
mutation steps sizes are applied to each parameter type:
sampling and interpolation rates ∆rsa/in in line 4, group
size ∆l (line 6) and window size ∆ω (line 8). To ease
the application of the QES and to allow for the automatic
adaptation, the speciﬁc step sizes are considered as additional
optimization parameters. The parameter vector is extended by
three variables: dim′ = dim + 3. The step sizes themselves
are mutated by as shown in line 10.
The mutation is executed individually for each solution of
256
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Algorithm 3: mutate()
Input: P current population
Output: Pm mutated population
1 forall a ∈ P do
index = random(1, dim + 3);
2
oldV alue = a.getParameterAt(index);
3
if type(index) = sampling || interpolation then
4
newV alue = oldV alue ± ∆rsa/in;
5
else if type(index) = groupSize then
6
newV alue = oldV alue ± ∆l;
7
else if type(index) = windowSize then
8
newV alue = oldV alue ± ∆ω;
9
else
10
newV alue = oldV alue ± 0.1 · oldV alue ;
11
a.setParameterAt(index, newV alue);
12
end
13
the current population. The parameter to mutate is selected
randomly from the conﬁguration set (line 1 & 2) and mutated
according to the respective step size (lines 3-10). The new
solution individual b is created by exchanging the mutated
parameter in the current solution a (line 11).
The last algorithm step, the selection, evaluates the new
solution candidates to form the next population generation.
The quality-driven evolution strategy follows the (µ + λ)-
approach, that produces a monotonically nondecreasing ﬁtness
curve. One population consists of µ elements, which are used
to produce λ candidates. The ﬁtness of all parent and child
solutions is calculated with the help of the objective function.
The µ ﬁttest solution individuals build the new generation as
starting point for the next algorithm iteration.
The single-objective quality-driven evolution strategy (SO-
QES) uses the cost-weighted objective function fsingle. The
optimization problem deﬁned in fmulti is solved by the multi-
objective evolution strategy (MO-QES). The comparison of
optimization results allows conclusions on the impact of differ-
ent cost settings. Furthermore, the following section evaluates
the performance of the two optimization approaches.
V. EVALUATION
In this evaluation, we examine data quality-driven optimiza-
tion of the data stream processing at real-world data streams
to empirically answer the following questions.
1) Which impact has the cost weighing on the optimal
conﬁguration of the data stream processing?
2) What
are
the
beneﬁts
of
batch
and
continuous
optimization?
3) How
do
single-
and
multi-objective
optimization
compete with each other?
4) Do the presented algorithms scale for the complex data
stream processing with high sensor numbers?
We have implemented the quality-driven optimization de-
scribed in this paper using the data stream management
system PIPES [6] and the Java-based optimization framework
JavaEva [7].
A. Experimental Setting
We ran our experiments on a dual-core 2x2GHz Centrino
Duo processor with 2GB of main memory, running Microsoft
Windows XP Professional 2002. All Java-based systems were
executed using JRE Version 6.
We use an artiﬁcial dataset to analyze the effects of the
optimization modes and test the performance of the designed
algorithms. Therefore, we generated data streams subject to
the standard normal distribution (µ = 0, σ = 1) with randomly
varying stream rates in the range of 1/ms ≤ r ≤ 100/ms.
We simulated queries over 2 to 128 of such generated streams
joined in pairs of two. Each data stream is sampled in each
query tree level to ﬁnd one-to-one-join partners; aggregations
are spread randomly.
Further, we applied the real-world dataset of contact lens
manufacturing available at [8] to analyze the impact of cost
weights and to examine the practicability of the presented
algorithm. It consists of measurements of the thickness of
lens center and edge and the axial difference. As described in
Section II-C, the production quality is monitored to predict the
optimal maintenance planning for re-calibrating the production
line.
For both datasets, we assumed a systematic error of op-
timistic 1%, while the statistical measurement error was de-
rived from the measurements’ variance using the conﬁdence
probability p = 99%. To simulate sensor failures in the
artiﬁcial dataset, we randomly skipped 2% of the data tuples.
To initialize the lens data completeness, we identiﬁed missing
measurements by comparing the recorded timestamps to the
planned sensor rates.
B. Impact of Weights
This section answers the ﬁrst of the above questions at
the sample objectives of maximal completeness and minimal
conﬁdence (compare Table I). Figure 5 shows the Pareto
front of the multi-objective optimization. Minimal statistical
conﬁdence errors produced by high sampling rates are only
achieved at the expense of high values of incompleteness and
vice versa. The optimal compromises represented by the Pareto
front can be re-produced by the single-objective optimization
using sophisticated weighing.
Points A,B and C in Figure 5 show exemplary single-
objective results. The higher the weight was determined, the
better the proposed optimal conﬁguration suits the respective
sub-objective. If the cost for incomplete data tuples exceeds
the conﬁdence weight, low sampling and interpolation rates are
proposed (point C). High costs for statistical errors in the DQ
dimensions conﬁdence (point A) lead to high sampling rates
resulting in less data loss. Similar cost weights result in a well-
balanced compromise between completeness and conﬁdence
as given in point B.
Figure 6 illustrates the Pareto front and cost impact of the
conﬂicting objectives maximal amount of data and maximal
257
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

0
0,2
0,4
0,6
0,8
1
0
0,2
0,4
0,6
0,8
1
uncompleteness
confidence
A(cc=0, ce=1, rsa=3.3)
B(cc=0.06, ce=0.26, rsa=0.7)
C(cc=1, ce=0, rsa=0.1)
Fig. 5.
Conﬁdence vs. completeness
0
0,2
0,4
0,6
0,8
1
0
0,2
0,4
0,6
0,8
1
amount of data
granularity
A(cd=0, cT=1, l=1)
B(cd=0.2, cT=0.11, l=3)
C(cd=1, cT=0, l=15)
Fig. 6.
Amount of data vs. granularity
0,0
0,2
0,4
0,6
0,8
1,0
0
1
2
3
4
termination probability
inter-partition-variance
Fig. 7.
Termination probability
0,0
0,2
0,4
0,6
0,8
1,0
0
1
2
3
4
normalized fitness
inter-partition-variance
batch
continuous
Fig. 8.
Normalized ﬁtness
granularity. High amount of data leads long timeframes rep-
resented by each tuples, i.e., low granularity. The higher the
cost weight for the amount of data exceeds the cost for ﬁne
granularity, the higher is the average group size in the proposed
optimization solution and vice versa (compare weights of
points A, B and C in Figure 6).
Finally, we evaluated the conﬂicting objectives of minimal
data stream volume and minimal data quality window size.
Similar to the above evaluations, higher weighing of the stream
volume results in low window sizes and vice versa.
C. Comparison of Optimization Modes
This section answers the second question. First, we check
the termination probability of the continuous optimization,
which selects a new data stream partition as basis for each
optimization iteration. Therefore, we applied the artiﬁcial data
set described above to the single-objective optimization. We
kept the intra-partition-variance (σ = 1), but introduced an
inter-partition-variance ˇσ by modifying the mean µ for each
partition.
Figure 7 shows the termination probability of the single- and
multi-objective optimization for an increasing inter-partition-
variance. For ˇσ < 1, the continuous optimization is likely to
terminate successfully. The termination probability decreases
for 1 ≤ ˇσ ≤ 3 and converges to 0 for ˇσ > 3. The termination
probability is independent from the applied partition length.
To compare the quality of the optimization results provided
by batch and continuous mode, we apply the computed
optimization results, i.e., the operator conﬁgurations, to the
ongoing stream and compare the achieved overall quality.
Figure 8 shows the normalized ﬁtness for an increasing inter-
partition-variance. The batch mode allows good results for
low variances ˇσ ≤ 0.5. The continuous mode adapts better
to changing situations and thus provides appropriate ﬁtness
values also for a higher inter-partition-variance. However, the
increasing termination probability limits the application of the
continuous mode to the bound ˇσ ≤ 2. Here, a small DQ
improvement is only possible by using batch mode again.
258
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

0,01
0,1
1
10
1
10
100
iteration duration
number of sensors
MC
SO
MO
[s]
Fig. 9.
Iteration duration vs. number of sensors
0,01
0,1
1
10
1
10
100
iteration duration
number of aggregations
MC
SO
MO
[s]
Fig. 10.
Iteration duration vs. number of aggregations
0,01
0,1
1
10
10
100
1000
10000
iteration duration
partition length
MO
SO
MC
[s]
[tuple]
Fig. 11.
Iteration duration vs. partition length
0
1
10
100
1
10
100
optimization time
DQ improvement
MO
SO-R
MC
SO-O
[%]
[s]
Fig. 12.
Duration of DQ improvement
D. Performance Tests
In this section, we show the scalability of the presented
algorithms by means of the artiﬁcial data set and complex
query structure deﬁned in Section V-A. As no inter-partition-
variance was introduced, the evaluation tests were performed
in batch-mode. We ﬁrst analyze the performance of the
proposed algorithms with regard to increasing numbers of
data sources (sensors), applied aggregations and frequency
analyses, respectively. As the number of sampling operators
is deﬁned by the executed joins and, thus, by the number of
sensors, an individual scalability test for that operator class
is not required. Then, we evaluate the impact of the length
of the data stream partition used for optimization. Finally, we
compare the optimization time required by the single- and
multi-objective optimization strategy to improve the overall
quality.
The Monte-Carlo-Search (MC) performs a random search
over the problem domain and serves as reference value deﬁn-
ing the lower performance bound [9]. The single-objective
optimization is executed with randomly chosen weights (SO-
R) as well as optimal weights (SO-O), which determine a
well-balanced objective compromise. As the iteration duration
of the single-objective optimization does not depend on the
weights, these performance test results of SO-R and SO-O
have been summarized to SO. Finally, the multi-objective
optimization (MO) approximates the Pareto front of all optimal
compromises.
Figure 9 illustrates the impact of the sensor number on the
time required for one iteration of the respective optimization
algorithm executed with a partition length of 1000 tuples.
The more complex the algorithm, the longer takes one iter-
ation. The performance difference between single- and multi-
objective optimization is caused by the complex Pareto front
computation. For all tested algorithms, the iteration duration
increases linearly with the sensor number.
Figure 10 shows the scalability for increasing numbers of
aggregations and frequency analyses. Again, the time required
for one iteration rises linearly.
Figure 11 shows the iteration duration (in seconds) for 16
259
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

-0,010
-0,005
0,000
0,005
0,010
0,015
0,0
0,2
0,4
0,6
0,8
1,0
0
20
40
60
80
100
drift
scrap indicator
time
quality indicator
drift
Fig. 13.
Lens quality control before optimization
-0,010
-0,005
0,000
0,005
0,010
0,015
0,0
0,2
0,4
0,6
0,8
1,0
0
20
40
60
80
100
drift
scrap indicator
time
quality indicator
drift
Fig. 14.
Lens quality control with MO
sensors and 10 aggregations for increasing partition lengths (in
data tuples). The processing time rises nearly linearly for small
to medium partition lengths of 100 to 1000 stream tuples. Only
for very large stream partitions, the iteration duration exhibits
an exponential character.
Figure 12 compares the overall time performance of single-
and multi-objective optimization with respect to the achieved
quality improvement for 16 sensor data sources and 10
randomly inserted aggregations. The quality improvement is
expressed as percentage value (qbefore − qafter)/qbefore. The
Monte-Carlo-Search performs worth followed by the randomly
initiated single-objective optimization (SO-R), requiring 5.2
and 2.9 seconds, respectively, for a DQ improvement of 10%.
The single-objective optimization executed with well-balanced
weights (SO-O) performs best (1.6s for 10%). However, the
deﬁnition of these weights requires multiple optimization runs
and has to be adapted as soon as stream characteristics or user
requirements change. The multi-objective optimization (MO)
is a little slower (1.9s) due to the complex computation of the
Pareto front. However, the result comprises the complete set of
all optimal compromises and no pre-processing to determine
optimal weights is necessary.
The evaluation showed, that the designed quality-driven
optimization provides good scalability with regard the applied
stream partition length as well as increasing complexity of the
data stream processing. Data quality and quality of service
could be improved within few seconds. Further, we deduce
that the single-objective optimization in batch mode is the best
choice for constant user requirements and steady data streams.
If streaming data values present high ﬂuctuations or user re-
quirements are often adjusted, the multi-objective optimization
constitutes the better option. Here, the inter-partition-variance
has to be analyzed to estimate the termination probability.
While the continuous mode provides better results for medium
variances, the batch mode has to be applied for highly varying
data streams to prevent from divergent ﬁtness functions.
E. Lens Production Optimization
To approve the suitability of the designed algorithms, we
refer to the application scenario of scrap monitoring for pre-
dictive maintenance in the contact lens production introduced
in Section II-C. Due to numerical errors and missing sensor
measurements, the predicted maintenance date deviates from
the optimal point in time. The maintenance is scheduled either
too early (the calculated quality drift exceeds the true value),
or too late (the drift of the computed quality indicator is too
low).
To improve the reliability of the determined maintenance
planing, we aim to optimize the underlying data stream
processing. The ﬁrst optimization parameters are the sampling
rates on the data streams of the lens thicknesses and axial dif-
ference. The group sizes of the axial difference summation and
sliding average aggregation of the quality identiﬁer determine
the compromise of detailed information and outlier balancing.
The last optimization criteria is given by the group size of
the drift calculation that provides statistically stable short- or
long-term variations.
We start the optimization process with arbitrary settings of
the optimization parameters. Figure 13 illustrates the scrap
indicator of the contact lenses and the derived drift of the
production quality. To guarantee correct maintenance activities
so that no lens under the scrap threshold of 0.5 remain
in the production line, the drift is monitored against the
drift threshold of -0,005. As the calculated drift suffers from
measurement errors and uncertainties due to sensor failures,
the lowest possible bound of the drift must be taken into ac-
count. Therefore, the absolute measurement error (the sum of
systematic and statistical error) is illustrated as error bars of the
drift function. The arrow indicates the resulting maintenance
time at t = 19. However, due to the high measurement error
before the optimization, this maintenance date is too early
considering the actual scrap indicator, that falls below 0.5 at
t = 64.
Figure 14 shows the same situation after 10 iteration of the
260
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

multi-objective optimization of the data stream processing has
been executed. The absolute measurement errors have been
decreased signiﬁcantly, such that the ”‘lost”’ production time
could be reduced by starting the maintenance only at t = 39.
0,0
0,2
0,4
0,6
0,8
1,0
init
MC
SO-R
SO-O
MO
num. error / overall data quality
average numerical error
overall data quality
Fig. 15.
Improvement of overall DQ & num. error
Figure 15 shows the overall data quality and the numerical
error after 100 iterations for each of the presented optimization
heuristics, averaged over 10 optimization runs. Already the
simple random Monte-Carlo-Search halves the numerical error
deﬁning the uncertain range of the quality drift. The ran-
domly initiated single-objective optimization (SO-R) further
decreases the error by 30%. The best results are obtained by
SO-O and MO. SO-O achieves an overall error reduction of
73%. MO provides a set of optimal compromise solutions: as
all sub-objectives shall be improved, Figure 15 illustrates the
error reduction (74%) for the well-balanced compromise of
weights: cc = 0.06, cϵ = 0.26, cd = 0.2, cT = 0.11, cV =
0.2, cω = 0.11 (compare point B in Figure 5 and 6).
After the data quality-driven optimization, the numerical er-
rors are decreased leading to a narrower uncertainty range. The
determined maintenance planning approximates the optimal
point in time with higher conﬁdence. Premature maintenance,
that unnecessarily interrupts the contact lens production, as
well as too late activities, that risk loss of sales due to low
production quality, are both prevented.
VI. RELATED WORK
In this section, we will discuss related work in the ﬁelds of
data quality management and optimization methods, especially
focusing on multi-objective optimization. This paper gave
an extended view over the quality-driven optimization of
sensor data stream processing, that we ﬁrst published in [2].
Besides detailed deﬁnitions of the objective functions as well
as optimization parameters, we added the discussion of the
batch and continuous optimization strategy as well as the
crucial deﬁnition of the stream partition length used for the
optimization. Further, the evaluation was deepened to show
the inﬂuences of different optimization techniques as well as
the scalability of the overall approach not only at artiﬁcial data
streams, but also using the real-world example of contact lens
production monitoring.
Traditional approaches of query optimization in database
and data stream systems aim at minimal processing time and
maximal data throughput. The quality of service is improved
to provide fast processing results to the user. In this paper,
we address the opposite problem: we improve the quality of
data. We maximize the data quality of processing results under
consideration of restricted system resources, so that the quality
of service remains in acceptable ranges.
Multiple publications underline beneﬁts of the data quality
management in data warehouses and databases [10][11]. To
deﬁne the term data quality, different sets of data quality
dimensions are discussed i.a. in [12] and [3]. While there
are different approaches to structure data quality metadata in
databases [13][14], [1] presents the ﬁrst data quality model
suitable for data streaming environments using so called
jumping data quality windows.
Data quality improvement in the context of data warehouse
and information systems is achieved by data cleaning [15][16].
For example, the Total Data Quality Management (TDQM)
provides tools to analyze the data quality in information sys-
tems and suggests data cleansing techniques for DQ improve-
ment [17]. However, prior work in this domain suffers from
the major drawback, that either an active participation of users
or domain experts in data quality improvement is necessary
or the presented approaches refer to a (set of) reference data
source(s) providing better or optimal data quality. It is obvious
that in case of sensor data, the manual subsequent data quality
correction for each measurement item is not feasible, and a
high-quality reference for comparison is not present.
Instead, the data and data quality processing has to be
conﬁgured to reduce the error ampliﬁcation. Based on the
classiﬁcation of the quality-driven optimization problem, we
identiﬁed the heuristic evolution strategy as appropriate tool
to approximate the optimal problem solution in an acceptable
timeframe. For the evaluation of different implementations of
the evolution strategy we rely on the comprehensive study
and experimental analysis provided in prior work and focus
on the SPEA as a well-studied algorithm with high rankings
in multiple test cases [18][19].
The application to multi-objective optimization problems is
described in [20]. Empirical studies showed the practicabil-
ity and superiority of the multi-objective evolution strategy
(MOES) [21][22]. [23] discusses the beneﬁts of parallel execu-
tion of evolutionary algorithms, which would further improve
the performance of the quality-driven evolution strategy.
VII. CONCLUSION
In this paper, we presented the quality-driven optimization
of sensor stream processing. On the one hand, the data quality
of processing results, expressed by the DQ dimensions accu-
racy, conﬁdence, completeness, amount of data and timeliness,
were improved. On the other hand, the quality of service was
261
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

increased by minimizing the data stream volume to comply
with resource constraints in data streaming environments.
To identify candidates for the data quality improvement, we
analyzed the operator repository of the quality propagation
model presented in [1] and extracted sampling rate and group
size as conﬁguration parameters. Furthermore, the size of
jumping data quality windows was detected as promising
parameter for the data quality optimization. Based on these
insights, we deﬁned the multi-objective, non-linear, continuous
optimization problem with side conditions.
To solve the problem of quality-driven optimization, we
presented the generic optimization framework that can be
instantiated with any optimization algorithm. Optimization
time and quality were improved by satisﬁability checks and
two optimization modes for changing stream characteristics.
Evolutionary algorithms represent the most promising op-
timization strategies for the deﬁned complex optimization
problem. Thus, we developed the quality-driven evolution
strategy QES as sample instantiation of the generic framework.
Finally, we evaluated the proposed optimization strategy
with the help of artiﬁcial data streams as well as real-world
data from the contact lens production control. We showed,
that QES solves the optimization problem in a reasonable
timeframe and provides good scalability for complex data
stream processing queries. The maintenance prediction for
contact lens production could be determined more precisely
by improving the data quality of the calculated maintenance
date.
VIII. ACKNOWLEDGMENT
This work has been partially supported by the European
Commission under the contract FP7-ICT-224282 (GINSENG).
REFERENCES
[1] A. Klein, “Incorporating quality aspects in sensor data streams,” in
Proceedings of the 1st ACM Ph.D. Workshop in CIKM (PIKM), 2007,
pp. 77–84.
[2] A. Klein and W. Lehner, “How to optimize the quality of sensor data
streams,” in ICCGI ’09: Proceedings of the 2009 Fourth International
Multi-Conference on Computing in the Global Information Technology.
Washington, DC, USA: IEEE Computer Society, 2009, pp. 13–19.
[3] R. Y. Wang and D. M. Strong, “Beyond accuracy: What data quality
means to data consumers,” Journal of Management Information Systems,
vol. 12, no. 4, pp. 5–33, 1996.
[4] M.-L. Lee, T. W. Ling, H. Lu, and Y. T. Ko, “Cleansing data for mining
and warehousing,” in DEXA, 1999, pp. 751–760.
[5] P. J. Haas, “Large-sample and deterministic conﬁdence intervals for
online aggregation,” in SSDM, 1997, pp. 51–63.
[6] J. Kraemer and B. Seeger, “Pipes - a public infrastructure for processing
and exploring streams,” in SIGMOD, G. Weikum, A. C. Koenig, and
S. Deloch, Eds., 2004, pp. 925–926.
[7] A. Zell, “Javaeva: A java based framework for evolutionary algo-
rithms,” 2009, http://www.ra.cs.uni-tuebingen.de/software/EvA2, last ac-
cess: 14.06.2010.
[8] R. L. Edgeman and S. B. Athey, “Digidot plots for process surveillance,”
Quality Progress, vol. 23, no. 5, pp. 66–68, 1990.
[9] N. R. Patel, R. L. Smith, and Z. B. Zabinsky, “Pure adaptive search
in monte carlo optimization,” Mathematical Programing, vol. 43, no. 3,
pp. 317–328, 1989.
[10] C. Batini and M. Scannapieco, Data Quality: Concepts, Methodologies
and Techniques.
Springer-Verlag, 2006.
[11] J. M. Juran, Juran’s quality control handbook, F. M. Gryna, Ed.
McGraw-Hill, 1988.
[12] F. Naumann and C. Rolker, “Assessment methods for information quality
criteria,” in ICIQ, 2000, pp. 148–162.
[13] V. C. Storey and R. Y. Wang, “An analysis of quality requirements in
database design,” in ICIQ, 1998, pp. 64–87.
[14] D. M. Strong, Y. W. Lee, and R. Y. Wang, “Data quality in context,”
Communications of the ACM, vol. 40, no. 5, pp. 103–110, 1997.
[15] M. A. Hern´andez and S. J. Stolfo, “Real-world data is dirty: Data
cleansing and the merge/purge problem,” Data Mining Knowledge
Discovery, vol. 2, no. 1, pp. 9–37, 1998.
[16] R. C. Morey, “Estimating and improving the quality of information in a
mis,” Communications of the ACM, vol. 25, no. 5, pp. 337–342, 1982.
[17] J. M. Pearson, C. S. McCahon, and R. T. Hightower, “Total quality
management: are information systems managers ready?” Information
Management, vol. 29, no. 5, pp. 251–263, 1995.
[18] Z. Michalewicz, Genetic Algorithms Plus Data Structures Equals Evo-
lution Programs.
Springer-Verlag, 1994.
[19] D. E. Goldberg, Genetic Algorithms in Search, Optimization, and
Machine Learning.
Addison-Wesley Professional, 1989.
[20] K. C. Tan, E. F. Khor, and T. H. Lee, Multiobjective Evolutionary
Algorithms and Applications (Advanced Information and Knowledge
Processing).
Springer-Verlag, 2005.
[21] E. Zitzler, K. Deb, and L. Thiele, “Comparison of multiobjective
evolutionary algorithms: Empirical results,” Evolutionary Computing,
vol. 8, no. 2, pp. 173–195, 2000.
[22] T. Hanne, “Global multiobjective optimization using evolutionary algo-
rithms,” Journal of Heuristics, vol. 6, no. 3, pp. 347–360, 2000.
[23] D. A. V. Veldhuizen, J. B. Zydallis, and G. B. Lamont, “Issues
in parallelizing multiobjective evolutionary algorithms for real world
applications,” in ACM Symposium on Applied Computing, 2002, pp.
595–602.
262
International Journal on Advances in Networks and Services, vol 3 no 1 & 2, year 2010, http://www.iariajournals.org/networks_and_services/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

