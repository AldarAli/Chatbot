Virtualizing Reconﬁgurable Hardware to Provide Scalability in Cloud Architectures
Oliver Knodel, Paul R. Genssler and Rainer G. Spallek
Department of Computer Science
Technische Universität Dresden
Dresden, Germany
Email: {ﬁrstname.lastname}@tu-dresden.de
Abstract—Field Programmable Gate Arrays (FPGAs) provide
a promising opportunity to improve performance, security and
energy efﬁciency of computing architectures, which are essential
in modern data centers. Especially the background acceleration
of complex and computationally intensive tasks is an important
ﬁeld of application. The ﬂexible use of reconﬁgurable devices
within a cloud context requires abstraction from the actual
hardware through virtualization to offer these resources to service
providers. In this paper, we enhance our related Reconﬁgurable
Common Computing Frame (RC2F) approach, which is inspired
by system virtual machines, for the profound virtualization of
reconﬁgurable hardware in cloud services. Using partial recon-
ﬁguration, our hardware and software framework virtualizes
physical FPGAs to provide multiple independent user designs
on a single device. Essential components are the management of
the virtual user-deﬁned accelerators (vFPGAs), as well as their
migration between physical FPGAs to achieve higher system-wide
utilization levels. We create homogenous partitions on top of an
inhomogeneous FPGA fabric to offer an abstraction from physical
location, size and access to the real hardware. We demonstrate
the possibilities and the resource trade-off of our approach in a
basic scenario. Moreover, we present future perspectives for the
use of FPGAs in cloud-based environments.
Keywords–Cloud Computing; Virtualization; Reconﬁgurable
Hardware; Partial Reconﬁguration.
I.
MOTIVATION
Cloud computing is based on the idea of computing as a
utility. The user gains access to a shared pool of computing
resources or services that can rapidly be allocated and released
“with minimal management effort or service provider inter-
action“ [1]. An essential advantage, compared to traditional
models in which the user has access to a ﬁxed number of
computing resources, is the elasticity within a cloud. Even
in peak load situations, a sufﬁcient amount of resources are
available [2].
With the theoretically unlimited number of resources, their
enormous energy consumption arises as a major problem
for data centers housing clouds. One possibility to enhance
computation performance by simultaneously lowering energy
consumption is the use of heterogeneous systems, ofﬂoading
computationally intensive applications to special hardware co-
processors or dedicated accelerators. Especially reconﬁgurable
hardware, such as Field Programmable Gate Arrays (FPGAs)
provide an opportunity to improve computing performance [3],
security [4] and energy efﬁciency [5].
A profound and ﬂexible integration of FPGAs into scalable
data center infrastructures which satisfy the cloud character-
istics is a task of growing importance in the ﬁeld of energy-
efﬁcient cloud computing. In order to achieve such an integra-
tion, the virtualization of FPGA resources is necessary. The
provision of virtual FPGAs (vFPGAs) makes reconﬁgurable
resources available to customers of the data center provider.
Therefore, service providers will be called users throughout
this paper. The users can accelerate speciﬁc services, reduce
energy consumption and thereby service costs.
The virtualization of reconﬁgurable hardware devices is a
recurring challenge. Decades ago, the virtualization of FPGA
devices started due to the limitation of logical resources [6].
Nowadays, FPGAs have grown in size and full utilization
of the devices cannot always be achieved in practice. One
possibility to increase utilization is our virtualization approach
which allows for ﬂexible design sizes and multiple hardware
designs on the same physical FPGA. One challenge of this
approach are the unsteady load situations of elastic clouds,
which process short- and long-running acceleration services.
In this paper, we introduce our virtualization concept for
FPGAs, which is inspired by traditional virtual machines
(VMs). One physical FPGA can consist of multiple vFPGAs
belonging to different services with different runtimes. Each
vFPGA can be conﬁgured using partial reconﬁguration [7] and
the internal conﬁguration access port (ICAP). The vFPGAs
are, therefore, ﬂexible in their physical size and location.
Moreover, they are fully homogenous among each other and
thereby become a wholesome virtualized cloud component,
which supports even an efﬁcient migration of a whole vFPGA
context. Especially the vertical scaleability of vFPGAs from
small designs up to full physical FPGAs is gaining impor-
tance by providing efﬁcient utilization of the reconﬁgurable
resources in modern cloud architectures.
The paper is structured as follows. Section II introduces
similar concepts and related research in the ﬁeld of virtu-
alization of reconﬁgurable hardware, cloud architectures and
bitstream relocation. In Section III, we give an overview on our
virtualization concept. Our prototype, which implements our
concept with homogenous and in their size ﬂexible vFPGAs,
is presented in Section IV followed by device utilization,
vFPGA size and performance results in Section V. Section VI
concludes and gives an outlook.
II.
RELATED WORK
The provisoning of reconﬁgurable hardware in data centers
and cloud environments has gained more and more importance
in the last years as shown by the overview from Kachris et al.
[8]. Initially used mainly on the network infrastructure level,
FPGAs are now also employed on the application level of
33
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-585-2
CENICS 2017 : The Tenth International Conference on Advances in Circuits, Electronics and Micro-electronics

data centers. Typical use cases in this ﬁeld are background
accelerations of speciﬁc functions with static hardware designs.
The FPGAs’ special feature to reconﬁgure hardware at runtime
is still used rather rarely. Examples are the anonymization of
user requests [9] and increasing security [4] by outsourcing
critical parts to attack-safe hardware implementations. In most
cases, the FPGAs are not directly useable or conﬁgurable by
the user, because the devices are due to a missing provisioning
or virtualization hidden deeply in the data center.
A comparable contribution with stronger focus on the trans-
fer of applications into an FPGA grid for high performance
computing is shown in [10]. The application focus on a single
cloud service model with background acceleration of services
using FPGAs. An approach which places multiple user designs
on a single FPGA is introduced by Fahmy et al. [11], using
tightly attached FPGAs to ofﬂoad computationally intensive
tasks. The FPGAs are partially reconﬁgurable and can hold up
to four individual user designs. The approach was extended
by Asiatici et al. in [12] with additional memory virtualization.
A cloud integration model with network-attached FPGAs and
multiple user designs on one FPGA is introduced by Weeras-
inghe et al. [13].
The term virtualization itself is used for a wide range
of concepts. An example for abstractions on the hardware
description level is VirtualRC [14], which uses a uniform hard-
ware / software interface to realize communication on different
FPGA platforms. BORPH [15] provides a similar approach,
employing a homogeneous UNIX interface for hardware and
software. The FPGA paravirtualization pvFPGA [16], which
integrates FPGA device drivers into a paravirtualized Xen
virtual machine, presents a more sophisticated concept. A
framework for the integration of reconﬁgurable hardware
into cloud architecture is developed by Chen et al. [17] and
Byma et al. [18].
Approaches more closely related to the context-save-and-
restore mechanism required by our migration concept can
be found in the ﬁeld of bitstream readback, manipulation
and hardware preemption. In ReconOS [19], hardware task
preemption is used to capture and restore the states of all ﬂip-
ﬂops and block RAMs on a Virtex-6 to allow multitasking
with hardware threads. In combination with homogenous bit-
streams for different physical vFPGA positions, methods like
relocation of designs as shown in [20], provide an opportunity
for an efﬁcient context migration of virtualized FPGAs.
III.
FPGA VIRTUALIZATION APPROACH
As the cloud itself is based on virtualization, the integration
of FPGAs requires a profound virtualization of the reconﬁg-
urable devices in order to provide the vFPGAs as good as other
resources in the cloud. Furthermore, it is necessary to abstract
from the underlying physical hardware.
A. Requirements for Virtual FPGAs in a Cloud Environemnt
As discussed in Section II, the term virtualization is used
for a wide range of concepts. The application areas of FPGAs
in clouds require a direct use of the FPGA resources to be
efﬁcient. Thus, an abstraction from the physical FPGA infras-
tructure is only possible in size and location. Our approach
is related to traditional system virtualization with VMs that
corresponds to a Type-1 bare-metal virtualization with use of
a hypervisor [21]. This kind of virtualization is designed for
Partial Reconfigurable Regions 
vFPGA 0
Frontend
0
Frontend
1
Frontend
3
FPGA Hypervisor 
Static
vFPGA 1
Frontend
2
Backend
Interface
Hardware
Interface
Physical FPGA (Resources, ICAP, PCIe Endpoint, …)
Management/
Configuration
Static
DomU
DomU
Dom0
vFPGA 4
Frontend
4
Frontend
5
DomU
vFPGA 5
DomU
Figure 1. Paravirtualization concept used in RC2F to provide virtual FPGAs
(vFPGAs) using partial reconﬁguration. vFPGAs can be combined to group
larger regions and thereby provide more resources.
the efﬁcient utilization of the physical hardware with multiple
users. Therefore, it is necessary to adapt the required FPGA
resources closely to the requirements of the users’ hardware
design capsuled by vFPGAs. By this, an efﬁcient utilization
of the physical hardware with multiple concurrent vFPGAs on
the same hardware can be achieved.
Furthermore, the vFPGA has to appear as a fully us-
able physical FPGA with separated interfaces and its own
infrastructure management like clocking and resetting. For an
efﬁcient cloud architecture which requires elasticity [1], it is
necessary to migrate vFPGAs with their complete context (reg-
isters and BlockRAM), which requires to enclose a complete
state management of the vFPGA as described in [22]. An
extraction of internal DSP registers is not supported in recent
Xilinx FPGAs and must be considered in the design.
B. FPGA Virtualization Approach
We decided to virtualize the FPGA similar to a paravirtual-
ized system VM executed by a hypervisor to provide access to
the interfaces. Figure 1 shows an FPGA virtualization inspired
by the paravirtualization introduced before. The virtualization
is limited to the interfaces and the designs inside the re-
conﬁgurable regions, which constitute the actual vFPGAs as
unprivileged Domain (DomU). Each vFPGA design is gener-
ated using the traditional design ﬂow with predeﬁned regions
for dynamic partial reconﬁguration [7] and static interfaces.
The vFPGAs can have different sizes (Figure 1) and operate
completely independent from each other. The infrastructure
encapsulating the vFPGAs has to be located in the static region
corresponding to a privileged domain (Dom0) or hypervisor.
The interface providing access to the vFPGAs is a so-called
frontend interface, which is connected inside the hypervisor to
the backend interface in the static FPGA region. There, all
frontends are mapped to the static PCIe-Endpoint and the on-
board memory controller inside the Dom0, which also manages
the states of the vFPGAs.
IV.
FPGA PROTOTYPE RC2F
Our prototype RC2F introduced in [23] provides multiple
concurrent vFPGAs allocated by different users on a single
physical FPGA. The main part of the FPGA frame(work)
consists of a hypervisor managing conﬁguration and user cores,
as well as monitoring of status information. The controller’s
memory space is accessible from the host through an API.
Input- and output-FIFOs are providing high throughput for
streaming applications. The vFPGAs appear to the user as
individual devices inside the System VM on the host.
34
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-585-2
CENICS 2017 : The Tenth International Conference on Advances in Circuits, Electronics and Micro-electronics

JTAG
Flash
BBRAM
Channel
Interface
PCIe Core
 Dom0 Hypervisor     
    Virtualization Layer 
      
Reconfig Area 1:      vFPGA 0
Reconfig Area N:      vFPGA N
PCI-Express Endpoint
Hypervisor 
Control Unit
Config Space
System Status
Reconfiguration
vFPGA
Control Unit
Clocking
State Transitions
IPv4-Address
Accelerator Design — DomU
Accelerator Design — DomU
…
||||||||||
Ethernet
 Eth Core
PCI Virtualization
VLAN/IP
||||||||||
64
vControl 
Config Space
Virtual States
||||||||||
||||||||||
vControl
Config Space
Virtual States
32
host
stream
net
packet
32
64
64
64
host
stream
net
packet
32
32
…
…
 Memory Core
Memory Controller
Memory Virtualization (Pagetable)
256
256
256
ICAP 
Config
Encryption
Unit
AES, RSA, SHA
DDR3 RAM
FPGA Board
Network
Host
Backend
Interface
Frontend
Interface
Hardware
Interface
Channel Virtualization
…
…
Figure 2. Virtualization frame RC2F with hypervisor, I/O components and
partial reconﬁgurable areas housing the vFPGAs. The vFPGAs have access
to the host using PCIe (FIFO interface and conﬁg space), to the Cloud
network using Ethernet and the virtualized DDR3 memory.
A. System Architecture
The physical FPGAs are located inside a host system
and are accessible via PCIe. On both hardware components
(host and FPGA), there are hypervisors managing access,
assignment and conﬁguration of the (v)FPGAs. Based on
our concept, we transform the FPGAs into vFPGAs with an
additional state management and a static frontend interface as
shown in Figure 1. Our architecture, designed to provide the
vFPGAs, is shown in Figure 2. The hypervisors manage the on-
chip communication between backend and frontend interfaces
for PCIe (Our prototype uses a PCIe-Core from Xillybus for
DMA access [24]), Ethernet and a DDR3 RAM. The RAM is
virtualized using page tables, managed by the host hypervisor,
which also manages the vFPGA states we introduced in [22].
The number of frontends and their locations are deﬁned by
the physical FPGA architecture as shown in Figure 6. The
Hypervisor Control Unit manages the ICAP controller and the
vControl units, which maintain and monitor the vFPGAs.
To exchange large amounts of data between the host (VM)
and the vFPGAs a FIFO interface is used. To exchange state
and control information the vFPGAs can be controlled by the
user via a memory interface as shown in Figure 3. The memory
is mainly intended for simple transfers and conﬁguration tasks
like resets, state management (pause, run, readback, migrate)
and the selection of a vFPGA system clock. In addition to these
static ﬁelds, there is also a user-describable memory region
which can be used as virtual I/O. The communication using
Ethernet is also provided but out of the scope of this paper.
B. Conﬁguration of the FPGA Hypervisor
The tasks of the FPGA hypervisor are the management
of its local vFPGAs and their encapsulation, the state man-
agement, as well as the reconﬁguration using the ICAP. The
interaction between host and FPGA hypervisor is based on
the conﬁguration memory shown in Figure 4, which includes
4 Virtualisierung der FPGAs für den Einsatz in einer dynamischen Cloud-Architektur
0
7
8
16 15
24 23
31
vFPGA Design Name
(ASCII)
00h
01h
User Resets
Clock Select
Design Status
02h
vFPGA State
(current and upcoming)
Test Loopbacks
Reserved
Static
￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿
03h
User Describable
04h
hhhhhhhhhhhhhhhhhhhhhhhhhh
hhhhhhhhhhhhhhhhhhhhhhhhh
h
Reconﬁgurable
￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿
1Fh
Abbildung 4.8: Konﬁgurationsspeicher für die vFPGAs mit festen und vom Nutzer deﬁnierbaren Bereich.
Reconfig Area 1:      vFPGA v0
Accelerator Design — DomU
||||||||||
||||||||||
128
vControl
User Config
Virtual State
host
stream
net
packet
32
32
256
Channel Virtualization 
Memory Virtualization 
Frontend
Interface
Abbildung 4.9: Architektur eines vFPGAs des RC2F mit Konﬁgurationsspecher (vControl) und Kommunikationskanä-
len. Die Datenleitungen für den Speicher und die beiden FIFOs sind je doppelt als Ein- und Ausgabe
vorhanden.
Kennung zu ermöglichen. Der Nutzer kann den Zustand seines vFPGAs einsehen und ändern, wobei der
Hypervisor eine höhere Priorität hat. Der Nutzer kann ebenfalls auf Basis des Systemtaktes eine eige-
nen Takt für seinen vFPGA auswählen und diesen auch zurücksetzten. Die Möglichkeiten entsprechen
im Wesentlichen denen eines einfachen vollwertigen physischen FPGAs. Der hintere Teil des Konﬁgura-
tionsspeicher liegt in der rekonﬁgurierbaren Region und kann völlig frei vom Nutzer beschrieben werden.
Der Beriech kann als einfache I/O-Ports des vFPGAs angesehen werden, wodurch der Nutzer in seinen
Gestaltungsmöglichkeiten Freiheiten wie auf einem physischen FPGA hat.
4.4.2 Zustände der vFPGAs und deren Verwaltung
Um die vFPGAs wie Virtuelle Maschinen nutzen zu können sind entsprechende Zustandsübergänge und
deren Verwaltung erforderlich. Die Steuerung erfolgt dabei vom Host-Hypervisor aus über den zuvor er-
läuterten Konﬁgurationsspeicher des FPGA-Hypervisors. Ein direkter Zugriff der Nutzer selbst auf die
Zustände ist ebenso auch vom Konﬁgurationsspeicher der vFPGAs möglich. Abbildung 4.10 gibt einen
Überblick über die möglichen Zustände und deren Übergänge. Um den kompletten Lebenszyklus abzu-
decken ist dabei neben den Zuständen auf dem FPGA noch weitere Zustände auf dem Host-Systems
innerhalb des Hypervisors erforderlich um analog zu Betriebssystemen die vFPGA-Designs als Instan-
zen anzusehen, welche direkt Nutzern zugeordnet sind und auch Nutzerspeziﬁsche Daten enthalten
104
Figure 3. Register and memory interface for the management of vFPGAs
accessible by the user VM (rc2f_cs).
4.4 Entwurf der Virtualisierung für FPGAs im Cloud-Einsatz – RC2F
0
7
8
15
16
23
24
31
Design Name
(ASCII)
00h
01h
Version
(ASCII)
02h
System Status
Resets
vFPGAs
(Number)
Reconﬁg Status
03h
Reserved
04h
IPv4-Address (Hypervisor)
05h
Channel Conﬁguration
06h
Encryption Conﬁguration
Hypervisor
￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿
07h
ID
Reserved
vFPGA State
(current and upcoming)
Channel Parameter
Memory Parameter
08h
IPv4-Address
09h
AES-Key
(128-Bit)
0Ah
0Bh
0Ch
0Dh
vFPGA Location
(range)
0Eh
vFPGA 0
￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿
0Fh
User Describable
0Ch
hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
h
vFPGA 1-N
￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿￿
FFh
Abbildung 4.7: Prototypischer Aufbaue des Konﬁgurationsspeicher für den FPGA-Hypervisor zur Administration des
physischen FPGAs einschließlich der Bereiche für die Verwaltung von bis zu 30 vFGPAs.
ein separater Beriech reserviert. Eine Zuordnung von vFPGA und realen Nutzer ist dabei lediglich dem
Hypervisor auf dem Host bekannt.
4.4.1.2 vFPGA Steuereinheit
Die vFPGAs, welche vom Nutzer wie in Abbildung 4.9 abgebildet wahrgenommen werden, stellen ein
eigenes (wenn auch virtuelles) System dar und verfügen als solches eigene Kommunikationskanäle und
einen eigenen Konﬁgurationsspeicher wie auch der Hypervisor. Die beiden zum Datenaustauch vorge-
sehenen Kanäle sind dabei der Streaming- und der Paketbasierte Kanal, welche in Abschnitt 4.3.3 als
notwendig erachtet worden sind. Der vFPGA Konﬁgurationsspeicher, welcher in Abbildung 6.1.2.2 ab-
gebildet ist wird in den Speicher der Nutzer-VM eingeblendet und kann mit Hilfe der in Abschnitt 5.1.3
erläuterten API konﬁguriert werden, beziehungsweise auch von außen, ohne VM, über die Netzwerkver-
bindung. Mit Hilfe des Moduls zur
Der Konﬁgurationsspeicher ist dabei in zwei Teile eingeteilt. Der statische Bereich der vom Nutzer in
seinem Design nicht verändert werden kann, sondern nur im Inhalt um eine Systemweite eindeutige
103
Figure 4. Register and memory interface for the management of the FPGA
hypervisor accessible by the host hypervisor (rc2f_gcs).
conﬁguration of the FPGA hypervisor (system status, reconﬁg-
uration data and status) and the administration of the vFPGAs.
Other important vFPGA-related entries are an AES-key for
encryption of the vFPGA-bitsteam and the allocated vFPGA
region(s) for additional validation during reconﬁguration. The
information inside the FPGA hypervisor are only accessible
and modiﬁable through the host hypervisor.
C. The Role of the Host-Hypervisor
Our virtualization concept on the host-system includes
passing through the vFPGAs’ FIFO channels and the conﬁg-
uration memories from the host-hypervisor to the user VMs
(DomU) and the FPGA hypervisor memory to the management
VM (Dom0). The overall system architecture on hypervisor
level of host and FPGA is shown in Figure 5. The frontend
FIFOs and the FPGA memories are mapped to device ﬁles
inside the host hypervisor. There, our system forwards the user
devices to the assigned VM using inter-domain communication
based on vChan from Zhang et al. [25] in our Xen virtualized
environment, similar to pvFPGA [16].
The management VM thereby accesses the FPGA hypervi-
sor’s conﬁguration memory and the ICAP on the FPGA via a
dedicated FIFO interface for the conﬁguration stream (read and
write). Thus, only the hypervisors can conﬁgure the vFPGA
regions on the physical FPGA whereby a sufﬁcient level of
security can be guaranteed.
35
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-585-2
CENICS 2017 : The Tenth International Conference on Advances in Circuits, Electronics and Micro-electronics

FPGA
Host
Host Hypervisor
FPGA Hypervisor Hardware 
Interface
Backend
Interface
 DomU — vFPGA 0
Frontend 
Interface
||||||||
Hardware 
Driver
Dom0 — Management 
||||||||
Dom0 — Management
FPGA Configuration,
user assignment and
access control
…
Frontend 
Interface
Device 
files
DomU — VM 0 
Inter-
Domain 
Channel
rc2f_gcs
rc2f_system_read
rc2f_system_write
rc2f_config_read
rc2f_config_write
/dev
rc2f_read
rc2f_write
rc2f_cs
/dev
Device 
files
device  assignment
Backend 
Interface
||||||||
Mem
…
…
Figure 5. System architecture on the hypervisor level of the host system.
FIFOs (rc2f_write, rc2f_read) and conﬁguration memories (rc2f_cs) are
displayed in the different host memories.
FPGA-Hypervisor
(PCIe + DDR)
FPGA-Hypervisor
 (Ethernet + ICAP)
Frontend 4 Frontend 3 Frontend 2 Frontend 1 Frontend 0
Frontend 5
FPGA Infrastructure
PCIe
PCIe
Partition Pins
Figure 6. Layout of a Xilinx Virtex-7 XC7VX485T with six vFPGA regions
conﬁgurable using dynamic partial reconﬁguration. The regions and their
number are determined by the height of the conﬁguration frames, which
consist of one complete column inside a clock region. Regions are
homogenous to allow migration of vFPGAs.
D. Mapping vFPGAs onto physical FPGAs
In our example we use six frontends on a Xilinx Virtex-7.
Depending on the resources required, the utilization of up to
six different-sized vFPGAs is possible with the same static
without reprogramming. If one of the vFPGAs covers more
than one region, only one frontend connection is used as
shown in Figure 1. Among the vFPGAs, the partition pins (PP)
between the static and the reconﬁgurable regions are placed
with identical column offset as shown in Figure 6. The regions
forming the vFPGAs are not free from static routes as for
example the region vFPGA 5 shows.
To reduce migration times, all components which hold
the context of the current vFPGA design as registers, FIFOs
or BlockRAM, are placed at the same positions inside each
vFPGA. Therefore, it is necessary that all of these positions
exist in each region. Hardmacros like PCIe-Endpoints or
parts of the FPGA infrastructure interrupt the homogenous
structures. Thus, we establish homogenous vFPGAs, which
are identical among each other by excluding these areas in all
vFPGAs as shown in Figure 6. The advantage of this approach
is that only one mask ﬁle is necessary to extract the content
of the different vFPGAs. Furthermore, it allows the provision
of almost identical vFPGAs.
E. Extended Design ﬂow
For our virtualization we extend the Xilinx Vivado design
ﬂow to generate vFPGA bitstreams from user-netlists for
every possible vFPGA position. First, directly after synthesis
the required region size (single, double, etc.) is chosen (see
Table I for appropriate vFPGAs). Afterwards, the design is
placed at a ﬁrst vFPGA region. Before the routing step, the
vFPGA region is expanded over the full width of the vFPGA
for unlimited routing of the design inside the uninterrupted
region. The placements of the same design for all the other
vFPGA positions are created by setting the LOC (Location)
and BEL (Basic Element Location) information accordingly
to the initial placed design. Only the routing is carried out for
the additional vFPGA designs to allow static routes inside the
different vFPGAs, resulting in designs with identical register
and BlockRAM positions for each vFPGA locations on the
physical FPGA. After generation of the ﬁrst bitstream, a
mask for extracting the context bits is generated to allow an
efﬁcient migration in signiﬁcantly less time compared to our
ﬁrst approach in [22]. This allows ﬂexible placement of the
vFPGA designs at various positions in a cloud system, as
well as the migration between vFPGAs on the same or to
other physical FPGAs. The bitstreams required for all possible
vFPGA positions belonging to a single user design are stored
as virtual reconﬁgurable accelerator images (vRAI).
F. Description of vFPGAs
The execution of a vRAI requires allocation of a vFPGA
which fulﬁlls all requirements. Therefore, it is necessary to
describe the vFPGAs in a particular conﬁguration ﬁle. Figure 7
gives an overview of such an conﬁguration, which is evaluated
by the resource management system to allocate the necessary
resources. After allocation the host hypervisor chooses from
the vRAI the appropriate bitstream and conﬁgures the device.
service = ’ba’
#Background Acceleration Service
name = ’vfpga-kmeans’
#vFPGA/User Design Name
vm = [’vm1-pvm’]
#VM-Instance Name
vfpga = 1
#Number of vFPGA
size = [3]
#vFPGA Size
memory = [2000]
#DDR-Memory Size in MByte
vif = [’ip=10.0.0.43’]
#vFPGA-IP
boot = [’running’]
#Initial vFPGA-State
design = [’kmeans.vrai’]#Initial Design
Figure 7. Conﬁguration ﬁle for the allocation of a single vFPGA with
network access and external memory of 2 GByte.
V.
IMPLEMENTATION RESULTS AND SCENARIO
The resources required for the implementation described in
the previous section are shown in the following with a real-
world scenario based on our motivation from Section I.
A. Implementation
The resource consumption of our prototype introduced in
Figure 2 is shown in Table I. Furthermore, the table introduces
the size of homogenous vFPGA regions as outlined in Figure 6.
#»ρ =
 
SliceLUT s
SliceRegister
BlockRAM
DSP
...
!
(1)
is used in the following to describe the resources. The aggre-
gated homogenous vFPGAs #     »
ρagg can be calculated using
#      »
ρagg = #           »
ρsingle · nagg − (nagg − 1) · #     »
ρppr
(2)
where #           »
ρsingle are the resources of a single vFPGA region, nagg
is the number of aggregated vFPGAs and #     »
ρppr represents the
36
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-585-2
CENICS 2017 : The Tenth International Conference on Advances in Circuits, Electronics and Micro-electronics

partition pin region (PPR) necessary to exclude the unused
frontend interfaces from the grouped vFPGAs. The open
frontends are therefore treated as stubs and are securely sealed
using a partial vFPGA bitstream. The cost of the provision of
identical vFPGAs are in the case of our Virtex-7 XC7VX485T
FPGA only 6.44% of slices registers/LUTs and 8.33% of the
BlockRAM tiles compared to a compete, but inhomogeneous
region. In our ﬂoor planning shown in Figure 6 there are
no further DSPs affected. All regions except the largest one
(Hexa), which has only one possible position, are homogenous.
The throughput between vFPGAs and host (PCIe Gen2 8x
on a Xilinx VC707) with different numbers of concurrently
active vFPGAs is shown in Figure 8. The throughput of a single
design is limited by a user clock of 100 MHz and a 64-bit data
interface. Starting from three vFPGAs, a limitation due to the
concurrent users occurs. The throughput shown in Figure 8 is
the minimal guaranteed throughput for each vFPGA.
The size of the vRAI packages and the number of possible
locations on the physical device are shown in Table II. With
69.2 MByte, a quad vFPGA with bitstreams for three possible
positions and a mask ﬁle for context migration is the largest
vRAI package. Compared to our ﬁrst approach, the information
necessary for context migration is reduced by several orders
of magnitude by using homogenous vFPGAs.
1,350
1,800
Single Read
Mean
std
250475026.7850524025633029.0542382832599033.9839261689112029.504137232831300
30.47143158443910031.252840325748 2.80200902948665
799364049.1712187617276050.548218683686600
55.1973188215500045.6152561008666057.6952277099541053.488359505554 5.02290886560428
6265530098.3273192803942 105.7129795679990090.7576040976515082.34858527025100112.17174552161000103.376123191362 13.06041935488240
Single Write
Mean
std
9830440671.825548648674 674.8449959587550673.4199356396930668.913417181503 678.5291360575810675.94010536796106.9245854588828
173390723.5786156041460737.5745301125040 738.6479176505350730.8853387879290732.7832661396960732.3017044601410 6.6467872641011
9097020766.7515597130850765.6901346983110766.0689849035990761.912678997624 764.8338465505460764.79604981928201.4087603127583
009000782.4752085553700783.0205033520190782.6661357738370782.1714713212690783.0613329482720781.9877658915180 1.3982248593214
022270790.8749636035930790.6521735368640792.5137229316790793.0785954720410793.1026496847230792.03951063659600.8059270532933
8390200796.8842601218870796.1475610959860798.2477994700770796.959284185946 797.2549340211990796.62078768519600.7700823304090
9224690799.9750964645540799.0790606028520799.2541378028600799.5227856944880799.3932374865330799.44770801204700.2200507351250
954330800.5477262503640800.8875833834980801.183666195989 800.7929674553260800.3112705350860800.81902590931500.2702825201301
615840801.6807184117210 801.7176901504480801.6891754078440801.7299711712760 801.6109532478750801.7169182330910 0.1312041000567
0057400802.3483782846300802.2485092775120802.3303031677810802.2182265865060802.3017635654040802.29178378219500.0448712141281
3447530802.4892260454040802.4675128083170802.5535421444230802.5377065678640802.421475706819 802.49916906861800.0426239141019
0057400802.3483782846300802.2485092775120802.3303031677810802.2182265865060802.3017635654040802.29178378219500.0448712141281
024
Maximal
Peak
1
2
0.5 707.192945693709 1263.86616440347 1374.5677435174
1 785.790063965695 1401.13371173734 1508.009680955
2 868.172173010644 1542.96210737859 1644.336734054
4 948.161765174605 1690.43784013079 1861.2062010342
8 1053.78260293348 1862.67365538673 2080.062042170
16
1170.6573987694 2096.57927858817 2286.028835814
32 1281.17945812402 2299.57975292207 2437.8438805684
64 1457.63153525243 2672.97293981627 2719.934752820
128 1574.91457649194 2740.2645045281 2738.296698700
256 1604.74149723555
2776.574146196 2734.700742900
512 1605.09497745867 2765.81708990194 2734.621799894
1024 1604.74149723555
2776.574146196 2734.700742900
Throughput in MByte/s
0
750
1,500
2,250
3,000
Data size in MByte
0.5
1
2
4
8
16
32
64
128
256
512 1,024
One
Two
Three
Four
Five
Six vFPGAs
Aggregated
Figure 8. Throughput between host and FPGA with different numbers of
concurrent vFPGAs. The diagram shows for each number of vFPGAs the
average throughput of one representative vFPGA. The aggregated throughput
is thereby the average throughput of all vFPGA compositions on the device.
B. Scenario
In the following, we show a scenario based on a typical
real-world application for our virtualization approach. The goal
is it to migrate vFPGA designs to achieve a high utilization
as shown in Figure 9(c). In a system with jobs arriving and
being ﬁnished at different points in time, situations as shown
in Figure 9(a) can occur. The fragmentation of the physical
FPGA restricts only one small vFPGA and one aggregated
double sized vFPGA. By migrating the design from user 3 from
vFPGA 5 to vFPGA 0 as shown in Figure 9(b), an area for a
group of three vFPGAs (triple) becomes available and makes
higher utilization of the physical device possible.
VI.
CONCLUSION AND OUTLOOK
This paper presented a comprehensive virtualization con-
cept for reconﬁgurable hardware and its integration into a
cloud environment. Our deﬁnition of the term virtualization
vFPGA 0: Empty
vFPGA 4: Empty
vFPGA 2: User 2 - Design BSMC10
vFPGA 3: Empty
vFPGA 1: User 4 - Design Crypto
vFPGA 5: User 3 - Design BSMC10
FPGA-Hypervisor
(PCIe + DDR)
FPGA-Hypervisor
 (Ethernet + ICAP)
Frontend 4 Frontend 3 Frontend 2 Frontend 1 Frontend 0
Frontend 5
(a) Fragmentation of the physical FPGA caused by
dynamic de- and allocation.
vFPGA 0: User 3 - Design BSMC10
vFPGA 4:  Empty 
vFPGA 2: User 2 - Design BSMC10
vFPGA 3: Empty
vFPGA 1: User 4 - Design Crypto
vFPGA 5:  Empty
FPGA-Hypervisor
(PCIe + DDR)
FPGA-Hypervisor
 (Ethernet + ICAP)
Frontend 4 Frontend 3 Frontend 2 Frontend 1 Frontend 0
Frontend 5
(b) Defragmentation providing aggregated vFPGA
regions for larger designs.
vFPGA 0: User 3 - Design BSMC10
vFPGA 3: User 1 -  Design kMeans 
vFPGA 2: User 2 - Design BSMC10
vFPGA 1: User 4 - Design Crypto
FPGA-Hypervisor
(PCIe + DDR)
FPGA-Hypervisor
 (Ethernet + ICAP)
Frontend 4 Frontend 3 Frontend 2 Frontend 1 Frontend 0
Frontend 5
(c) Utilization of the free region with a design using
three aggregated vFPGAs (Triple).
Figure 9. Szenario with different users and designs on a Xilinx Virtex-7
XC7VX485T with six (vertically) scaleable vFPGAs.
is inspired by traditional VMs whose functionalities are trans-
ferred to reconﬁgurable hardware. We develop a paravirtual-
ized infrastructure on a physical FPGA device with multiple
vFPGAs. The concept is integrated into a framework, which
allows for interaction with the vFPGAs similar to traditional
VMs. We create homogenous regions for the vFPGAs on the
physical FPGA to optimize the process of vFPGA migration
between different physical FPGAs. Implementation details
are described, the necessary resources and the virtualization
overhead are presented.
37
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-585-2
CENICS 2017 : The Tenth International Conference on Advances in Circuits, Electronics and Micro-electronics

TABLE I. NUMBER OF AVAILABLE RESOURCES INSIDE THE STATIC AND THE AGGREGATED VFPGA REGIONS AND UTILIZATION OF STATIC CONTAINING
INFRASTRUCTURE AND HYPERVISOR. THE PARTITION PIN REGION (PPR) IS NECESSARY TO EXCLUDE AND ISOLATE UNUSED PARTITION PINS (PP).
Ressource
Static
Utilization of static region
PPR
Into aggregated vFPGA regions
region
HFa
Pb
Ec
Md
Total
Single
Dual
Triple
Quad
Quint
Hexae
Slice LUTs
94,824
26%
3%
2%
11%
42%
1,200
30,800
60,400
90,000
120,800
151,600
188,400
Slice Register
189,648
11%
2%
1%
4%
18%
2,400
61,600
120,800
180,000
241,600
303,200
376,800
Block RAM Tile
369
23%
2%
2%
3%
30%
0
100
200
300
400
500
600
DSPs
726
–
–
–
–
–
20
340
660
980
1,320
1,660
1,940
aHF: Hypervisor and Frontends
bP: PCIe-Endpoint
cE: Ethernet
dM: DDR3 Memory
eLargest region without considering homogeneity
TABLE II. SIZE OF A SINGLE BITSTREAM FOR A VFPGA REGION, NUMBER
OF POSSIBLE POSITIONS INSIDE THE FPGA AND SIZE OF THE VRAIS.
Single
Dual
Triple
Quad
Quint
Hexa
Bitstream (MByte)
4.8
9.0
13.0
17.3
21.3
25.3
Locations
6
5
4
3
2
1
vRAI (MByte)
33.6
54.0
65.0
69.2
63.9
50.6
One signiﬁcant result of this paper is that the provision
of homogenous FPGA resources is possible with state-of-the-
art FPGAs. We think that such approaches are necessary for
establishing FPGAs in modern data centers housing clouds.
Certainly, when cloud providers like Amazon expand their
cloud architectures with high-end FPGAs, such as Xilinx
Virtex-7 UltraScale devices [26] it is necessary to utilize the
hardware efﬁciently with multiple designs in a scaleable frame
inside one physical FPGA. Such kind of ﬂexible approach
allows for adaption the individual resources to the users’
requirements.
REFERENCES
[1]
P. Mell and T. Grance, “The NIST deﬁnition of cloud com-
puting, Revised”, Computer Security Division, Information
Technology Laboratory, NIST Gaithersburg, 2011.
[2]
M. Armbrust, A. Fox, R. Grifﬁth, et al., “A view of cloud
computing”, Communications of the ACM, vol. 53, pp. 50–58,
2010.
[3]
T. El-Ghazawi, E. El-Araby, M. Huang, et al., “The promise
of high-performance reconﬁgurable computing”, IEEE Com-
puter, vol. 41, no. 2, pp. 69–76, 2008.
[4]
J.-A. Mondol, “Cloud security solutions using FPGA”, in
PacRim, Paciﬁc Rim Conf. on, IEEE, 2011, pp. 747–752.
[5]
A. Putnam, A. M. Caulﬁeld, E. S. Chung, et al., “A reconﬁg-
urable fabric for accelerating large-scale datacenter services”,
in Computer Architecture (ISCA), 41st Int’l Symp. on, 2014.
[6]
W. Fornaciari and V. Piuri, “Virtual FPGAs: Some steps
behind the physical barriers”, in Parallel and Distributed
Processing, Springer, 1998, pp. 7–12.
[7]
Xilinx Inc., Vivado Design Suite User Guide – Partial Recon-
ﬁguration, UG909 (v2017.1), April 5, 2017.
[8]
C. Kachris and D. Soudris, “A survey on reconﬁgurable
accelerators for cloud computing”, in Field Programmable
Logic and Applications (FPL), 26th Int’l Conf. on, 2016.
[9]
K. Eguro and R. Venkatesan, “FPGAs for trusted cloud
computing”, in Field Programmable Logic and Applications
(FPL), 22nd Int’l Conf. on, IEEE, 2012, pp. 63–70.
[10]
J. Dondo Gazzano, F. Sanchez Molina, F. Rincon, and J. C.
López, “Integrating reconﬁgurable hardware-based grid for
high performance computing”, The Scientiﬁc World Journal,
2015.
[11]
S. A. Fahmy, K. Vipin, and S. Shreejith, “Virtualized FPGA
accelerators for efﬁcient cloud computing”, in Cloud Comput-
ing Technology (CloudCom), Int’l Conf. on, IEEE, 2015.
[12]
M. Asiatici, N. George, K. Vipin, S. A. Fahmy, and P. Ienne,
“Designing a virtual runtime for FPGA accelerators in the
cloud”, in Field Programmable Logic and Applications, Int’l
Conf. on, 2016.
[13]
J. Weerasinghe, F. Abel, C. Hagleitner, and A. Herkersdorf,
“Enabling FPGAs in Hyperscale Data Centers”, in Cloud and
Big Data Computing (CBDCom), Int’l Conf. on, IEEE, 2015.
[14]
R. Kirchgessner, G. Stitt, A. George, and H. Lam, “VirtualRC:
a virtual FPGA platform for applications and tools portability”,
in FPGAs, Proc. of the ACM/SIGDA Int’l Symp. on, 2012.
[15]
H. K.-H. So and R. Brodersen, “A uniﬁed hardware/soft-
ware runtime environment for FPGA-based reconﬁgurable
computers using BORPH”, ACM Transactions on Embedded
Computing Systems (TECS), vol. 7, no. 2, p. 14, 2008.
[16]
W. Wang, M. Bolic, and J. Parri, “pvFPGA: Accessing an
FPGA-based hardware accelerator in a paravirtualized envi-
ronment.”, Hardware/Software Codesign and System Synthesis
(CODES+ISSS), 2013 Int’l Conf. on, pp. 1–9, 2013.
[17]
F. Chen, Y. Shan, Y. Zhang, et al., “Enabling FPGAs in the
cloud”, in Computing Frontiers, Proc. of the 11th ACM Conf.
on, ACM, 2014, p. 3.
[18]
S. Byma, J. G. Steffan, H. Bannazadeh, A. L. Garcia, and
P. Chow, “FPGAs in the Cloud: Booting Virtualized Hardware
Accelerators with OpenStack”, in Field-Programmable Cus-
tom Computing Machines (FCCM), 22nd Annual Int’l Symp.
on, IEEE, 2014, pp. 109–116.
[19]
M. Happe, A. Traber, and A. Keller, “Preemptive Hardware
Multitasking in ReconOS”, in Applied Reconﬁgurable Com-
puting, Springer, 2015, pp. 79–90.
[20]
J. Rettkowski, K. Friesen, and D. Göhringer, “RePaBit: Au-
tomated generation of relocatable partial bitstreams for Xilinx
Zynq FPGAs”, in ReConFigurable Computing and FPGAs
(ReConFig), 2016 International Conference on, IEEE, 2016,
pp. 1–8.
[21]
J. E. Smith and R. Nair, “The architecture of virtual ma-
chines”, Computer, vol. 38, no. 5, pp. 32–38, 2005.
[22]
O. Knodel, P. Genßler, and R. Spallek, “Migration of long-
running tasks between reconﬁgurable resources using virtu-
alization”, in ACM SIGARCH Computer Architecture News
Volume 44, HEART 2016, ACM, 2016.
[23]
O. Knodel and R. G. Spallek, “Computing framework for
dynamic integration of reconﬁgurable resources in a cloud”, in
2015 Euromicro Conference on Digital System Design, DSD
2015, IEEE, 2015, pp. 337–344.
[24]
Xillybus Ltd., Haifa, Israel, An FPGA IP core for easy DMA
over PCIe, Website, Online: http://xillybus.com, 2017.
[25]
X. Zhang, S. McIntosh, P. Rohatgi, and J. L. Grifﬁn,
“Xensocket: A high-throughput interdomain transport for vir-
tual machines”, in Middleware 2007, Springer, 2007, pp. 184–
203.
[26]
Amazon Inc., Amazon EC2 F1 Instances – Run Custom
FPGAs in the AWS Cloud, Website, Online: https : / / aws .
amazon.com/ec2/instance-types/f1/, 2017.
38
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-585-2
CENICS 2017 : The Tenth International Conference on Advances in Circuits, Electronics and Micro-electronics

