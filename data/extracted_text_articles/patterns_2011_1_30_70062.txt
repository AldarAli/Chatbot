A Novel Live 3D Objects Reconstruction Scheme
Hui Zheng, Jie Yuan, Sidan Du 
School of Electronic Science and Engineering, Nanjing University 
Nanjing, China 
Email: hzheng.nju@gmail.com, yuanjie@nju.edu.cn, coff128@nju.edu.cn 
 
Abstract— In this paper, we describe a novel method to 
create the complete 3D model of the object on uncalibrated 
images. First, we match the points both detected by multi-
scale Harris corner detection algorithm and line detection 
technique.  Second, we perform a projected reconstruction 
based on factorization using Singular Value Decomposition 
(SVD). After that, we are able to upgrade from projective to 
Euclidean structure and then eliminate the ambiguity in 
Euclidean reconstruction. Finally, we use 3D registration 
algorithm based on common points to build the whole 3D 
model of the object. Sufficient experiments proved the 
validity and efficiency of the method. 
 
Keywords-3D reconstruction; ambiguity in Euclidean 
Reconstruction; 3D Registration; line detection 
I. 
INTRODUCTION 
Reconstruction of the object in 3D from images taken 
by uncalibrated cameras has long been a topic of research 
in computer vision. Factorization has been a common and 
reliable method for 3D reconstruction and motion recovery. 
Sturm and Triggs [1] proposed a projective reconstruction 
algorithm based on factorization, they recovered projective 
depths by estimating a set of fundamental matrixes. Ponce 
[2] upgraded the projective reconstruction to Euclidean 
reconstruction on the assumption that the cameras are 
zero-skew. Mei Han and Takeo Kanade recovered the 
shape and motion from image sequence which taken by 
uncalibrated camera using factorization method in [3]. R 
Szeliski mentioned the ambiguity problem in the process 
of simultaneously recovering structure and motion with 
uncalibrated cameras in [4], this ambiguity would cause 
errors in 3D registration. 
Displaying after meshing the 3D models is one of the 
important applications for 3D reconstruction. Direct 
feature points detection algorithm [9] [10] would have 
edge and corner information lost during 3D reconstruction, 
which would cause the unsatisfactory display. To solve 
this problem, the method describe in [5] using the feature 
points which are detected both by Harris and Sift [9] for 
reconstruction, but it would not reduce the edge 
information loss. 
In this paper, the line detection algorithm and the 
feature points detection and matching algorithm are 
proposed in Section II. In Section III, we introduce the 
projective reconstruction and Euclidean reconstruction in 
Subsection A and Subsection B, and then propose an 
algorithm 
to 
eliminate 
ambiguity 
in 
Euclidean 
reconstruction after identifying its cause in Subsection C. 
In order to create the complete 3D model, we introduce the 
detail of 3D registration technique in Section IV. 
Experimental processes and results are introduced in 
Section V; we use four cameras for synchronized taking 
݊ሺ݊ ൐ 2ሻ  pictures for the moving object, and then 
reconstruct each of the four parts of the object from the 
images taken by each camera, and finally create the 
complete 3D model. In Section VI, we give a conclusion 
of this paper. There are three contributions in this paper: 
 Propose a 3D reconstruction method using line 
detection technique that ensures the correctness and 
completeness of corners and edges on the 3D model. 
 Describe the cause of ambiguity in Euclidean 
reconstruction and propose a method to eliminate 
ambiguity. 
 Increase the quantity of common points in 3D using 
guided matching algorithm that enhance accuracy of 
3D registration. 
II. 
FEATURE POINTS DETECTION AND MATCHING 
In this paper, we use the multi-scale Harris corner 
detection algorithm [8] to detect feature points on images. 
This algorithm is the combination of Harris operator and 
scale space theory. We first detect the Harris feature points 
in different scales, and then using LOG operator to select 
the appropriate scale and obtain the location of the feature 
points. 
The feature points detected by traditional detection 
algorithm [5] [9] [10] cannot cover the edges and corners 
of the object, which will cause the loss of edge and corner 
information when reconstructing the 3D model. So we 
detect lines on images using line detection algorithm [6] 
based on Hough transform, and then match the points on 
the lines for reconstruction. 
In this paper, we use the wide baseline matching 
algorithm [11] to match the points which are detected on 
the images taken by adjacent cameras, we call it “match 
between cameras”; we use the algorithm based on guided 
matching to match the points which are detected on the 
images taken by one camera, we call it “match in camera”. 
There are two steps in “matching between cameras”. 
First, we execute initial matching. Calculate the correlation 
coefficient of feature points of the two images respectively. 
When the correlation coefficient is larger than a given 
threshold value, both the feature points are considered as 
the candidate of matching points. Search support strength 
from the neighborhood to accumulate matching strength. 
We merely consider the maximum support of each 
neighborhood as the initial matching points. Next, we use 
RANSAC method to calculate the fundamental matrix F 
and homography matrix H, and remove the mismatches at 
the same time. 
There are also two steps in “matching in camera”. We 
execute initial matching at first. Next, we using guided 
13
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

matching algorithm [12] to find the points on other images 
that correspond to the feature points obtained from 
matching points between cameras and detected lines. 
Guided matching is a technique which can redirected 
match the designated feature points by using epipolar 
geometry and homography constraint. 
The outline of feature points detection and matching is 
as follows: 
Suppose ܫ௝
௜ሺ݅ ൌ 1 … ݊ሻ is the image taken by ݆thሺ1 ൑
j ൑ 4ሻ camera at time i. 
a) 
ܲ௝
௜ ൌdetect feature points on ܫ௝
௜ 
b) 
match between cameras: 
 ሼܱܲଵଶ
௡  ܱܲଶଵ
ଵ ሽ ൌ matchሼܲଵ
௡ ܲଶ
ଵሽ; 
 ሼܱܲଶଷ
௡  ܱܲଷଶ
ଵ ሽ ൌ matchሼܲଶ
௡ ܲଷ
ଵሽ; 
  
  ሼܱܲଷସ
௡  ܱܲସଷ
ଵ ሽ ൌ matchሼܲଷ
௡ ܲସ
ଵሽ; 
 ሼܱܲସଵ
௡  ܱܲଵସ
ଵ ሽ ൌ matchሼܲସ
௡ ܲଵ
ଵሽ; 
cሻ 
line detection: 
  ݉ ൌ ቔ݊
2ቕ ; 
  ܲܮଵ
௠ ൌ ݈݅݊݁ ݀݁ݐ݁ܿݐሼܫଵ
௠ ሽ; 
  ܲܮଶ
௠ ൌ line detectሼܫଶ
௠ ሽ; 
  ܲܮଷ
௠ ൌ line detectሼܫଷ
௠ ሽ; 
  ܲܮସ
௠ ൌ line detectሼܫସ
௠ ሽ; 
        ܲܮ are the points spaced selected on detected lines 
dሻ 
match in camera: 
Camera1: 
{ܲܫଵ
ଵ ܲܫଵ
ଶ … ܲܫଵ
௡ሽ ൌ initial match ሼܲଵ
ଵ ܲଵ
ଶ … ܲଵ
௡ሽ; 
ሼܲܫܮଵ
ଵ ܲܫܮଵ
ଶ … ܲܫܮଵ
௡ሽ ൌ  
guided match ܲܮଵ
௠ on ܫଵ
௠ to ܫଵ
௜ሺ݅ ് ݉ሻ; 
ሼܲܫܱଵଶ
ଵ  ܲܫܱଵଶ
ଶ … ܲܫܱଵଶ
௡ ሽ ൌ  
guided match ܱܲଵଶ
௡  on ܫଵ
௡ to ܫଵ
௜ሺ݅ ് ݊ሻ; 
ሼPIOଵସ
ଵ  PIOଵସ
ଶ … PIOଵସ
୬ ሽ ൌ 
guided match ܱܲଵସ
ଵ  on ܫଵ
ଵ to ܫଵ
௜ሺ݅ ് 2ሻ; 
ܲܯଵ
௜ ൌ ܲܫଵ
௜ ∪ ܲܫܮଵ
௜ ∪ ܲܫܱଵଶ
௜ ሺ݅ ൌ 1 … ݊ሻ; 
    ሼܲܯଵ
ଵ ܲܯଵ
ଶ … ܲܯଵ
௡ሽ are final matching points of 
Camera 1. Camera 2, camera 3 and camera 4 do 
the similar way with Camera 1. 
III. 
3D RECONSTRUCTION 
In order to create the complete 3D model, we have to 
reconstruct each of the four parts of the object. In Section 
III, we will describe the reconstruction algorithm. 
A. Projective Reconstruction 
Suppose there are m object points ܺ௝ሺ݆ ൌ 1 … ݉ሻ and 
n perspective matrixes ܲ௜ሺ݅ ൌ 1 … ݊ሻ, ݔ௜௝ is the projection 
of ܺ௝ that projected by ܲ௜. 


11 11
12 12
1
1
1
21 21
22 22
2
2
2
1
2
1
1
2
2
n
n
n
n
n
m
m
m
m
mn mn
m
x
x
x
P
x
x
x
P
W
X X
X
PX
x
x
x
P











































(1) 
In (1), ܹ is scaled measurement matrix and ߣ௜௝ is a 
non-zero factor called projective depth. The goal of 
projective reconstruction is to estimate the projective depth 
of ݔ௜௝. We use the algorithm based on SVD described in [3] 
[7] to estimate ߣ௜௝: 
a) 
Set ߣ௜௝ ൌ  1 . Compute the current scaled 
measurement matrix ܹ, by (1) 
b) 
Perform SVD decomposition on ܹ : ܷܦܸ ൌ
 SVDሺܹሻ.  
c) 
Set ܲᇱ ൌ ܷସ, ܺ’ ൌܦସ ∗ ܸସ
்  where ܷସ,ܦସ, ܸସ
்  are 
the first four lines of ܷ,ܦ,ܸ. 
d) 
Set ܹ′ ൌ ܲᇱ ∗ ܺᇱ, update ߣ௜௝ and ܹ: 
e) ߣ௜௝ ൌߣ௜௝∗
ௐᇱ೔ೕ
೅ ∗ௐ೔ೕ
ௐ೔ೕ
೅∗ௐ೔ೕ ;    ܹ௜ ൌߣ௜௝∗ݔ௜௝ 
f) 
Go to step 3 until Dሺ5,5ሻ is small enough. 
B. Euclidean Reconstruction 
The factorization of (1) recovers the motion ܲ  and 
shape ܺ up to a 4 ൈ 4 linear projective transformation ܪ: 
ܹ ൌ ܲᇱ ൈ ܺᇱ ൌ ሺܲܪሻ ൈ ሺܪିଵܺሻ 
(2) 
The goal of Euclidean reconstruction is to calculate the 
matrix ܪ that upgrades the ܲ and ܺ in projective space to 
ܲᇱ and ܺᇱ in Euclidean space. The ܲ௜
ᇱ in Euclidean space 
can be written as: 
ܲ௜
ᇱ ൌߙ௜ܭ∗ ሾܴ௜|ܶ௜ሿ 
   
(3) 
where ߙ௜ is a none-zero real number; ܴ௜ is a orthogonal 
matrix, which shows the rotation of the camera; ܶ௜ is a 
vector, which shows the position of the camera. Because 
of the orthogonality of ܴ௜, we rewrite ܪ as ܪ ൌ ሾܣ|ܤሿ, 
where ܣ is 4 ൈ 3 and ܤ is 4 ൈ 1. We have: 
ܲ௜ܣܣ்ܲ௜
் ൌߙ௜
ଶܭܴ௜ܴ௜
்ܭ் ൌߙ௜
ଶܭܭ் (4) 
In this paper we only discuss the case that camera 
pixels are square, and we shift principal point to the 
original, so the intrinsic parameters matrix ܭ can be 
denoted by: 
ܭ ൌ ൥
ܽ௫
0
0
0
ܽ௬
0
0
0
1
൩ 
         
    (5) 
where ܽ௫ ൌ ܽ௬. Substituting (5) into (4)： 
ܯ௜ ൌ ܲ௜ܣܣ்ܲ௜
் ൌ ܲ௜ܳܲ௜
் ൌ ቎
ߙ௜
ଶܽ௫
ଶ
0
0
0ߙ௜
ଶܽ௬
ଶ
0
0
0ߙ௜
ଶ
቏   (6) 
where  
ܳ ൌܣܣ்.   (7) 
14
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

From (6) we can obtain the equation: 
ܯ௜ሺ0,0ሻ ൌܯ௜ሺ1,1ሻ 
ܯ௜ሺ1,2ሻ ൌܯ௜ሺ1,3ሻ ൌܯ௜ሺ2,3ሻ ൌ 0 
    (8) 
We can set up 4 equations from each frame, and given 
ߙଵ ൌ 1, we have 4݊ ൅ 1 equations to solve ܳ which have 
10 unknown elements. Then we get the matrix ܣ from ܳ 
by SVD decomposition. 
To solve B we denote ܲᇱby ܲᇱ ൌ ሾܨ|ܶሿ ൌ ܲሾܣ|ܤሿ and 
substitute ܲ ൌ ሺܲ௫ ܲ௬ ܲ௭ሻ், ܶ ൌ ሺܶ௫ ܶ௬ ܶ௭ሻ் into it : 
ܶ௫ ൌ ܲ௫ ∗ܤ; ܶ௬ ൌ ܲ௬ ∗ܤ; ܶ௭ ൌ ܲ௭ ∗ܤ          (9) 
Put the origin of the world coordinate system at the 
center of gravity of the scaled object points to enforce: 
்ೣ೔
்೥೔ ൌ
∑
ఒ೔ೕ∗
೘
ೕసభ
௫೔ೕ
∑
ఒ೔ೕ
೘
ೕసభ
       
்೤೔
்೥೔ ൌ
∑
ఒ೔ೕ∗
೘
ೕసభ
௬೔ೕ
∑
ఒ೔ೕ
೘
ೕసభ
 
    (10) 
From (9) (10) we set up 2n liner equations to solve B. 
After ܣ and ܤ have been computed, we can obtain 
motion matrix ܲ′ and shape matrix ܺ′ in Euclidean space. 
C. Eliminate Ambiguity in Euclidean Reconstruction 
There exists an ambiguity in Euclidean Reconstruction 
we described in Section III Subsection B. The 4 ൈ 3 
matrix ܣ that we obtained from ܳ  using SVD 
decomposition is non-unique. Suppose A is a solution of 
(7), we have: 
ܳ௜௝ ൌܣ௜ଵ ∗ܣ௝ଵ ൅ܣ௜ଶ ∗ܣ௝ଶ ൅ܣ௜ଷ ∗ܣ௝ଷ        (11) 
From (11) we know A still a solution if we reverse any 
columns of it: 
ܳ௜௝ ൌ ሺ∓ܣ௜ଵሻ ∗ ሺ∓ܣ௝ଵሻ ൅ 
ሺേܣ௜ଶሻ ∗ ሺേܣ௝ଶሻ ൅ ሺേܣ௜ଷሻ ∗ ሺേܣ௝ଷሻ 
(12) 
So, there are at most 8 solutions for equation ܳ ൌܣܣ். 
Substitute ܣ into equation ܲ′ ൌ ሾܨ|ܶሿ ൌ ܲ ൈ ሾܣ|ܤሿ: 
ܨ௜௝ ൌ ܲ௜ଵ ∗ܣଵ௜ ൅ ܲ௜ଶ ∗ܣଶ௜ ൅ ܲ௜ଷ ∗ܣଷ௜ 
       (13) 
Moreover, ܲ′ and ܺ′ satisfy: 
ܲ′ ൈ ܺ′ ൌ ܹ  
 
(14) 
From (13) and (14) we know that we will obtain 
different motion matrix ܲ′  and shape matrix ܺ′ 
corresponding to different solutions of (7). Suppose ܣଵ 
and ܣଶ are different solutions of (7), and the ݆thሺ݆ ൏ 4ሻ 
column of ܣଵ has opposite signs to the jth column of ܣଶ, 
then each members of jth column of ܲଵ
′ and  ܲଶ
′  computed 
from ܣଵ and ܣଶ is opposite numbers to each other. Also 
according to (14) we know that the jth row of ܺଵ
′  and ܺଶ
′  
has opposite signs corresponding to Pଵ
′ and  Pଶ
′. When we 
perform Euclidean reconstruction using the method 
mentioned in Section III Subsection B, we will obtain one 
solution randomly from all solutions which satisfy (11) 
and (14). 
Suppose ܲ௔
′ and ܺ௔
′  are the solution sets which satisfy 
(7) (13) and (14).We can divide ܺ௔
′  into two groups ܺ௟
′ and 
ܺ௥
′ . Every solution in one group can transform to every 
other one in this group through rotation and translation, 
but the solutions in different groups cannot transform to 
each other like that. We can image this situation as every 
solution ܺ௟
௜′ ሺܺ௟
௜′ ∈ ܺ௟
′ሻ  is in the left-handed coordinate 
system, and ܺ௥
௜′ ሺܺ௥
௜′ ∈ ܺ௥
′ ሻ  is in the right-handed 
coordinate system. 
We must make sure all shapes that are reconstructed 
from the images which taken by different cameras are in 
the same type of coordinate system when we generate the 
complete 3D model of the object using 3D registration 
algorithm. Figure 1 shows the wrong result that doing 3D 
registration with two shapes that in different groups. 
Suppose ܫ௝
௜ is the image taken by ݆th camera at time ݅, 
ܲ௝
௜′ is the motion matrix of ݆th camera at time ݅, ܺ௝
′ is the 
shape matrix which reconstruct from ܫ௝
௜. If we have the 
images ൛ܫ௝
௧భ …ܫ௝
௧భൟ and ሼܫ௝
௧మ …ܫ௝
௧మሽሺ݆ ൌ 1, 2, 3, 4ሻ which are 
taken by each camera at different moment ݐଵ and ݐଶ, the 
rotational direction of the object from ݐଵ to ݐଶ computed 
from each motion matrix ܲ௝
௜′ሺ݅ ൌݐଵ,ݐଶ; ݆ ൌ 1 … 4ሻ must 
be the same(when static cameras capture videos of moving 
object, the motions that obtained from motion matrixes of 
cameras have opposite directions with the motion of 
object). For example, if the object rotate clockwise from ݐଵ 
to ݐଶ, the rotational directions that we computed from ܲ௝
௧భ′ 
and ܲ௝
௧మ′ must be counter-clockwise. But if we have the 
wrong situation like Figure 1, each rotational direction 
obtained from each motion matrixes must be the opposite, 
so we can eliminate ambiguity according to this property. 
From (6) we compute intrinsic parameters matrix ܭ as: 
ܽ௫ ൌ ܽ௬ ൌ ට
ெ೔ሺ଴,଴ሻାெ೔ሺଵ,ଵሻ
ଶ௔೔
 
 
(15) 
We 
eliminate 
the 
ambiguity 
of 
Euclidean 
reconstruction after normalize ܲ௝
௜′: 
a) 
Normalize ܲ௝
௜′ሺ݅ ൌݐ1,ݐ2ሻ: 
 if ቀܲ௝
௜′ሺ1,1ሻ ൏ 0ቁ
 
ܲ௝
௜′ሺ: ,1ሻ ൌ െܲ௝
௜′ሺ: ,1ሻ    ܺ௝
′ሺ1, : ሻ ൌ െܺ௝
′ሺ1, : ሻ 
 if ቀܲ௝
௜′ሺ2,2ሻ ൏ 0ቁ
 
ܲ௝
௜′ሺ: ,2ሻ ൌ െܲ௝
௜′ሺ: ,2ሻ    ܺ௝
′ሺ2, : ሻ ൌ െܺ௝
′ሺ2, : ሻ 
b) 
Compute ܭ௝ and ሼܴ௝
௧ଵ, ܴ௝
௧ଶሽ from (15) and (3). 
c) 
Compute rotational direction ܴܦ௝ from ሼܴ௝
௧ଵ, ܴ௝
௧ଶሽ. 
d) 
Compare ܴܦ௝ሺ1 ൏ ݆ ൑ 4ሻ with ܴܦଵ: 
e) 
if ൫ܴܦ௝ has the opposite direction to ܴܦଵ൯        
 ܲ௝
௜′ሺ: ,3ሻ ൌ െܲ௝
௜′ሺ: ,3ሻ    ܺ௝
′ሺ3, : ሻ ൌ െܺ௝
′ሺ3, : ሻ  
15
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

ሼ
o
ሼ
c
e
b
w
r
w
w
W
(
e
Figure 1.  Wro
Suppose ሼ
ሼܱܲସଵ
௡  ܱܲଵସ
ଵ ሽ 
obtained in 
ሼܺଶଷ ܺଷଶሽ  ሼܺ
corresponding
We take ܺ
example to d
between ܺଵଶ a
ܺ
where ݏ is n
rotation matrix
ܴ ൌ  ܷ
We compu
ܷܦܸ ൌݏݒ
where ܺଵଶ஺ ܺଶ
ܺଵଶ஺ ൌ
We obtain
where C′ in (2
ܥ′ ൌܦሺ1
We obtain t by
ݐ ൌ
ଵ
௡ ∑
Finally we
(23). 
V. 
E
In this Se
experiments an
ong solution becau
shapes in d
IV. 
3D R
ܱܲଵଶ
௡  ܱܲଶଵ
ଵ ሽ ሼ
are matchings
Section II. A
ܺଷସ ܺସଷሽ  ሼܺସଵ
g to them.  
ܺଵଶ from Came
escribe 3D re
and ܺଶଵ is: 
ܺଵଶ ൌݏ ൈ ܴ ൈ
non-zero num
x, ݐ is translati
∗ ݀݅ܽ݃ ሺ1，1
ute U, D, V in (
ݒ݀൫∑
ሺܺଵଶ௜
௡
௜ୀଵ
ଶଵ஺ are the cen
ൌ
ଵ
௡ ∑
ܺଵଶ௜
௡
௜ୀଵ
n s from 
ݏ ൌ ∑
||௑
೙
೔సభ
0) compute as
1,1ሻ ൅ܦሺ2,2ሻ
y substituting 
∑
ܺଵଶ௜
௡
௜ୀଵ
െ ܵ
e accomplish 
ܾܺଶ
′ ൌݏ
EXPERIMENTAL
ection, we wi
nd analyze the
use perform 3D re
different groups. 
REGISTRATION
ሼܱܲଶଷ
௡  ܱܲଷଶ
ଵ ሽ 
s between ca
And we can
ଵ ܺଵସሽ , which
era 1 and ܺଶଵ f
egistration. Th
ൈ ܺଶଵ ൅ݐ  
mber called sc
ion vector. We
1，1，݀݁ݐ ሺܷ
17) from 
௜ െ ܺଵଶ஺ሻ൫ܺଶଵ
nter of gravity 
; ܺଶଵ஺ ൌ
ଵ
௡ ∑௡
௜
஼′
௑మభ೔ି௑మభಲ||మ 
s (21). 
൅ ݀݁ݐሺܷܸ′ሻ ∗
s, R into (22). 
∗ ܴ ∗
ଵ
௡ ∗ ∑௡
௜ୀଵ
3D registrat
ൈ ܴ ൈ ܺଶ
′ ൅ݐ 
L ANALYSIS AN
ill show the 
e results.  
egistration with tw
N 
ሼܱܲଷସ
௡  ܱܲସଷ
ଵ ሽ
ameras which 
n find  ሼܺଵଶ ܺ
h are 3D po
from Camera 2
he transformat
 
(
cale factor, ܴ
e compute ܴ a
ܷܸ′ሻሻ ∗ ܸ′       (
௜ െ ܺଶଵ஺൯′൯   (
of ܺଵଶ  ܺଶଵ:
ܺଶଵ௜
௡
௜ୀଵ
         (
     
       (
∗ܦሺ3,3ሻ.       (
ଵ ܺଶଵ௜
          (
ion according
 
      (
ND RESULTS
processes of 
 
wo 
and 
we 
ܺଶଵሽ 
oints 
2 as 
tion 
(16) 
ܴ  is 
as: 
(17) 
(18) 
(19) 
(20) 
(21) 
(22) 
g to 
(23) 
our 
s
th
F
c
th
r
m
a
3
m
o
th
o
u
to
c
c
m
m
a
c
F
c
te
o
P
We use fou
imultaneously
hree photos o
Figure 2 show
cameras. We 
hen display th
Reprojectiv
epresents the 
main reprojecti
are reconstruct
3D registration
method is sma
other parts to p
he mean repro
other part. 
In order to 
using guided m
o obtain more 
compares the 
common point
matching techn
matching techn
a normalized 
component (x
From Table 2
common point
echnique, and
our method is m
TABLE 1. MEA
TABLE 2. MEAN
 
Part 1 with 
Part 2 
Part 1 with 
Part 3 
Part 2 with 
Part 3 and 4
 
Mean error 
(pixel)
Max error 
(pixel)
Figure 2.  O
ur static uncali
y of the movin
on the object 
ws two of th
implement ou
e model with t
ve error is a
reconstruction
ive error of mi
ted from imag
n. We can see 
all and accepta
part 1’s coordin
ojective error 
enhance the a
matching techn
common poin
mean registr
ts obtained in
nique with th
nique. We tran
one before 
x,  y or z) of 3
2, we can se
ts than the me
d the mean re
much smaller 
AN REPROJECTIVE
N REGISTRATION E
POINTS IN R
Obtain com
points with g
matchin
Quantity
103 
0
122 
0
197 
0
Part1 
0.154 
1.118 
Original images 
ibrated camera
ng object. Ea
with small-s
he images tak
ur method in 
texture in Ope
an important 
n accuracy. Ta
iddle image of
ges taken by e
the reconstruc
able. Because 
nate system in
in part 1 is a 
accuracy of 3D
nique in step ‘m
nts for 3D regi
ration error a
n 3D registrat
hose obtained 
nsform the 3D
registration, 
3D points fal
ee our metho
thod without g
egistration err
than the other 
 ERROR AFTER 3D
ERROR AND QUAN
REGISTRATION  
mmon 
guided 
ng 
Ob
p
gu
Mean 
error 
Qua
0.0129 
1
0.0109 
2
0.0198 
2
Part 2 
P
0.362 
0
1.220 
1
as to take pho
ach camera tak
scale moveme
ken by differ
MATLAB, a
enGL. 
criterion wh
able 1 shows t
f each part wh
ach camera af
ction error of o
we translate t
n our experime
little lower th
D registration, 
match in came
istration. Tabl
and quantity 
tion with guid
without guid
D coordinate in
in which ea
lls into ሾെ1 ,
od obtain mo
guided matchi
ror computed 
one.  
D REGISTRATION
NTITY OF COMMON
btain common 
points without 
uided matching
antity 
Mea
erro
11 
0.025
23 
0.020
24 
0.055
Part 3 
Part
0.406 
0.57
1.966 
2.3
otos 
kes 
ent. 
ent 
and 
ich 
the 
ich 
fter 
our 
the 
ent, 
han 
we 
era’ 
e 2 
of 
ded 
ded 
nto 
ach 
1ሿ. 
ore 
ing 
by 
N 
 
n 
r 
59
07
51
t 4
72 
34 
16
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

c
o
r
c
f
d
l
l
m
t
i
c
b
c
c
l
i
u
m
c
s
Figure 3 
cameras’. The
other and th
registration. F
camera”. The
figure: the po
detection algo
lines and corn
loss of the 3
matchings in 
the quantity o
increase the ac
Figure 5 sh
color. Figure 6
by performing
can see the ed
complete.  
Figure 7 sh
line detection 
information lo
unsatisfactory
Figure 8 
method. Appa
corner inform
satisfactory di
 
Figure 5.  Image
Figure 3.  Mat
Figure 4.  M
shows the 
e points in ther
here are enou
Figure 4 show
ere are three 
oints detected
orithm are the 
ners prevent th
3D model; the
“matching be
of common po
ccuracy of 3D 
hows the dete
6 shows the m
g 3D triangula
dges and corne
hows the resu
technique. Th
oss of the mo
y. 
shows the 3D
arently our m
mation loss 
isplay. 
e with detected lin
tch between group
Match in groups 
result of ‘m
re are matche
ugh matching
ws one result 
parts matchin
d by multi-sca
basic matchin
he edge and co
e points corre
etween camer
oints in 3D, w
registration. 
ected lines on 
mesh of the 3D
ation algorithm
ers of the box
ult of 3D recon
here is a lot of
odel and the 
D model reco
method preven
effectively 
nes 
Figur
ps 
matching betw
d accurately e
g points for 
of “matching
ng points in 
ale Harris cor
ngs; the points
orner informat
esponding to 
ras” step incre
which lead to
image with w
D points genera
m. From that 
x are accurate 
nstruction with
f edge and cor
display effec
onstructed by 
nts the edge 
and shows
re 6.  3D mesh
ween 
each 
3D 
g in 
this 
rner 
s on 
tion 
the 
ease 
the 
white 
ated 
we 
and 
hout 
rner 
ct is 
our 
and 
s a 
r
m
e
a
a
s
u
a
T
m
F
a
C
[1
[2
[3
[4
[5
Figure 7. 
Figur
Several con
esults. Firstly
method insure
edges and corn
algorithm we 
ambiguity in E
ince we incre
using guided 
accuracy result
The display ef
method is satis
 
This paper 
Funds for the C
and National N
China, Number
 
1] Peter Stru
algorithm f
In Europe
Cambridge
2] Jean Ponce
Reconstruc
Proc. Work
Large-Scale
3] Mei Han a
Uncalibrate
of Compute
4] Richard S
Application
5] Keju Peng,
“3D recon
points,”  P
(ROBIO’09
 Reconstruct wit
 
re 8.  Reconstruc
VI. 
CO
nclusions can b
y, line detect
es the accura
ners on recon
proposed in
Euclidean recon
ease the quant
matching te
ts of 3D recon
ffect of the m
sfactorily. 
VII. ACKNO
is supported 
Central Unive
Natural Science
r:  BK2010386
REFER
um and Bill T
for multi-image 
ean conf. Com
, U.K., 1996,  p
e, “On Computin
ctions under the
kshop. 3D Stru
e Environments
and Takeo Kan
ed Cameras,” Pr
er Vision (WAC
Szeliski. “Com
ns,” Chapter 7, 2
, Xin Chen, Do
nstruction based
Proc. IEEE Co
9), pp. 960-964,
 
thout line detectio
ction result of our 
ONCLUSION 
be drawn from
tion algorithm
acy and comp
nstructed mode
n Section III
nstruction effe
tity of comm
echnique, we 
nstruction and
model reconst
OWLEDGEMEN
by the Fundam
ersities, Numb
e Funds of Jian
6. 
RENCES 
Triggs, “A fa
projective struc
mputer Vision 
pp. 709-720. 
ng Metric Upgr
e Rectangular P
ucture from M
s (SMILE’00), p
nade, “Creating
roc. IEEE Work
CV2000), pp.17
mputer Vision: 
2010, pp. 343-3
ongxiang Zhou,
d on SIFT an
onf. Robotics 
, December 200
on technique 
method 
m the experim
m using in o
pleteness of t
el. Secondly, t
 eliminates t
ectively. Third
on points in 
 obtained hi
d 3D registratio
tructed from o
NT 
mental Resear
ber: 11070210
ngsu Province
actorization ba
cture and motio
— ECCV '
rades of Project
Pixel Assumptio
Multiple Images 
pp. 52-67, 2002
g 3D Models w
kshop. Applicat
78-185, 2000. 
Algorithms a
374 
, and Yunhui L
nd Harris feat
and Biomimet
09  
ment 
our 
the 
the 
the 
dly, 
3D 
igh 
on. 
our 
rch 
51, 
e of 
ased 
on,” 
'96, 
tive 
on,” 
of 
2. 
with 
tion 
and 
Liu, 
ture 
tics  
17
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

[6] Thuy Tuong Nguyen, Xuan Dai Pham, and Jae Wook Jeon, 
“An improvement of the Standard Hough Transform to 
detect line segments,” Proc. IEEE Conf. Industrial 
Technology (ICIT’08), Chengdu, China, pp. 1-6, 2008. 
[7] Toshio Ueshiba and Fumiaki Tomita, “A Factorization 
Method for Projective and Euclidean Reconstruction from 
Multiple Perspective Views via Iterative Depth Estimation,”  
Proc. European Conf. Computer Vision, pp. 296-310, 1998. 
[8] Krystian, Mikolajczyk and Cordelia Schmid, “Scale & 
Affine Invariant Interest Point Detectors,” International 
Journal of Computer Vision, 2004, pp. 63-68. 
[9] David G. Lowe, “Distinctive Image Features from Scale-
Invariant Keypoints,” International Journal of Computer 
Vision. 2004, pp. 91-110. 
[10] Zhiyong Ye, Yijian Pei, and Jihong Shi, “An Adaptive 
Algorithm for Harris Corner Detection,” Proc. IEEE Conf. 
Computational Intelligence and Software Engineering 
(CiSE’09), pp. 1-4, December 2009. 
[11] Jiangjian Xiao and Mubarak Shah, “Two-frame wide 
baseline matching,” Computer Vision, 2003. Proceedings. 
Ninth IEEE International Conference on. 2003, pp. 603-609. 
[12] Benjamin Ochoa and Serge Belongie, “Covariance 
propagation for guided matching,” Proc. Workshop.  
Statistical Methods in Multi-Image and Video Processing 
(SMVP’06). On CD-ROM, 2006. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
18
PATTERNS 2011 : The Third International Conferences on Pervasive Patterns and Applications
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-158-8

