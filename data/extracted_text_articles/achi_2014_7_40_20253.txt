TouchPair : Dynamic Analog-Digital Object Pairing for Tangible Interaction using 
3D Point Cloud Data 
 
Unseok Lee and Jiro Tanaka 
Department of Computer Science 
University of Tsukuba 
Tsukuba, Ibaraki, Japan 
{leeunseok, jiro}@iplab.cs.tsukuba.ac.jp 
 
 
Abstract— Sensor-based pairing technology between digital 
objects for interactions are used widely (e.g., smart phone to 
Bluetooth headset). In addition, research about interactions 
between daily normal analog objects (e.g., a doll, Lego block) 
and digital objects has progressed and is also popular. 
However, such research can only involve interactions with pre-
setup objects. The paired objects cannot be changed 
dynamically. In this paper, we propose a new analog-digital 
object pairing method by intuitive touch interactions using 
three-dimensional point cloud data. Several touch pairing 
methods are described in detail and paired objects are changed 
dynamically using the proposed method. In addition, a simple 
tangible interaction between two objects is described after 
pairing. Finally, we demonstrate the high recognition rate of 
the proposed method using experiments and describe our 
system’s contribution.  
Keywords-dynamic pairing; point cloud; tangible interaction; 
3d gesture;human computer interaction 
I. 
 INTRODUCTION 
In everyday life, the touching action is natural and 
common. We touch objects to use them (e.g., a doll or toy to 
play, open a bottle cap for drinking). Touch interactions with 
digital devices have also become natural in recent years, 
because smart devices with touch screens and touch pads are 
now used widely. 
 
 
Figure 1. Analog-Digital objects in everyday life 
 Simultaneously, in the field of human–computer 
interaction (HCI), research on interactions between physical 
objects and digital devices has progressed rapidly. A 
physical object is set as an input unit and the digital device is 
controlled by it. Such interactions are used widely and have 
become a ‘natural’ method. However, to use a physical 
object as an input unit, much effort and time is initially 
needed to set up sensors. Moreover, it takes time and effort 
to apply sensors again when changing the physical object as 
the input unit. In addition, the digital object is limited to a 
particular physical object. Thus, there is no ‘natural’ 
interaction between various objects. Regarding the input unit, 
research on methods for making a tangible object for which 
touch sensing is possible has progressed. For example, in the 
bowl project [2] , a simple media player in a bowl sits on a 
living room table and a range of physical objects can be 
placed within it. When an object is placed in the bowl, 
related media are played on the TV. The project used radio-
frequency identification (RFID) sensors for tangible 
interactions. However, the system could not provide dynamic 
pairing between objects. The interactions and possible 
objects were also limited. 
The “HandSense” [3] prototype used capacitive sensors 
for detecting when it was touched or held against a body part. 
It could determine whether a device was held in the left or 
right hand by measuring the capacitance on each side. 
Raphael [4] presented a method for prototyping grasp-
sensitive surfaces using optical fibers. However, all of these 
examples require attaching sensors to the devices. This is 
unnatural in the real world. Also, the paired object cannot be 
changed dynamically. 
In this paper, we propose a new method for dynamic 
pairing with tangible interactions between analog objects and 
digital objects in practical circumstances. This dynamic 
pairing is achieved through touch interactions. For example, 
one hand grasps a doll, an analog object. Then, the other 
hand grasps a smart phone, a digital object. The doll and 
smart phone are paired and prepared for tangible interactions. 
The system makes it possible to pair the doll with touch in 
three dimensions. The smart phone then shows feedback 
from interactions with the doll. We can change the paired 
objects dynamically. Figure 1 shows examples of pairing 
analog and digital objects in everyday life. Our pairing 
technique is based on three-dimensional(3D) point cloud 
data using two Kinect units. They capture and calibrate 3D 
point cloud data. Our system determines touch pairing and 
tangible interactions of the paired analog object, based on 
these calibrated data. In this way, the system can readily 
166
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

recognize what objects are touched and trace what objects 
are paired. In addition, we can recognize the touched 
position and movements of the objects. We present the 
results of tests of recognition rate for pairing using the 
proposed method. 
The rest of this paper is organized as follows. Section 2 
introduces related work on depth-based touch sensing and 
tangible interactions. We describe in detail the principles of 
the pairing method, the system specifications, and tangible 
interactions in section 3. We present details on the high 
recognition rate of our system in section 4. Finally, we 
describe our contribution and future work in section 5. 
II. 
RELATED WORK 
A. Depth-based Touch Sensing Technologies 
In recent years, depth-based cameras and related 
technology have developed rapidly. Research on obtaining 
3D data on objects using depth information has also 
progressed. The framework for 3D sensing using depth 
cameras has been improved markedly [5]. 
Florian et al. implemented tangible interactions using a 
depth camera and a 3D sensing framework [1]. They 
implemented touch detection and object interaction, 
supporting multi-touch and tangible interactions with 
arbitrary objects. They used images from a depth camera to 
determine whether a user’s finger touched the object. 
However, they were unable to support 3D touching and 
dynamic pairing between objects for tangible interactions. 
Andrew et al. used depth-sensing cameras to detect touch 
on a tabletop [7], using the camera to compare the current 
input depth image against a model of the touch surface. The 
interactive surface need not be instrumented in advance for 
the interaction and this approach allows touch sensing on 
non-flat surfaces. However, they only supported simple 
touch recognition and could not address touch in any 
direction with 3D objects. 
B. Tangible Interactions with Analog Objects 
“Digital Desktop” by Wellner et al. [8] was used in an 
early attempt to merge the physical and digital worlds. They 
implemented a digital working space on a physical desktop 
where physical paper served as an electronic document. The 
interaction with papers was by means of bare fingers. “Icon 
Sticker” [9], based on this idea, is similar. Icon Sticker is a 
paper representation of digital content. It consists of 
transferring icons from the computer screen to paper, so they 
can be handled in the real world and used to access digital 
content directly. An icon is first converted into a 
corresponding barcode, which is printed on a sticker. Then 
the sticker can be attached to a physical object. To access the 
icon, the user scans the barcode on the sticker with a barcode 
scanner. “Web Sticker” [10] uses barcodes to represent 
online information. It is similar to Icon Sticker, but instead of 
icons it manages Web bookmarks. They use a handheld 
device with a barcode-reading function to capture the input 
and display related information. 
There were also attempts to improve tagging of physical 
objects for a more natural tangible interaction. Nishi et al. 
[11] registered real objects on a user’s desktop based on a 
user indicating a region on the desk by making a snapshot 
gesture with four fingers. A color histogram was used to 
model the object and a pointing gesture was used to trigger 
the recognition. “Enhance Table” also uses a color histogram 
to model objects. However, the size is predefined and the 
system is limited to mobile phone recognition. 
Although many previous tangible interaction studies have 
used physical objects for interactions, most of them are 
token-based approaches and provide only limited use of real 
objects. They do not support 3D object tracking or pairing 
for tangible interactions. Thus, to overcome this, we propose 
a robust 3D object-tracking method that detects touch in 
three dimensions. The system supports dynamic pairing 
between analog and digital objects, and makes analog objects 
accessible to touch anywhere. 
III. 
TOUCHPAIR 
A. Hardware and Software 
Our system consists of two Microsoft Kinect sensors for 
Xbox 360 with stands. Two computers (Intel i7 2.4 Hz, 8 Gb 
RAM, and GeForce GTX 660M graphics card) are used to 
handle the 3D data. A desk and the pairing object for 
interaction are installed. The analog and digital objects are 
placed anywhere. The Kinect sensors are set at 80 cm from 
the desk. The computers are connected to each Kinect sensor, 
and the digital objects have wireless internet or Bluetooth 
connections with the computer, installed in the bottom of the 
desk (Fig. 2). 
 
 
Figure 2. TouchPair System Configuration 
Our system uses OpenFrameworks OpenNI [6] for the 
Kinect sensors and a point cloud method that provides 
example add-ons of frameworks. The system obtains 3D 
point cloud data and maps the RGB data to the point cloud. 
The movement of the points is based on pairing recognition. 
The proposed system was implemented on a Microsoft 
Windows 7 platform. The pairing recognition module was 
implemented in Visual Studio 2010 and OpenNI 1.5.4. 
167
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

B. TouchPair Architecture 
The entire system consists of three major modules(Fig. 3). 
The input data are obtained by the two Kinect sensors, on the 
left and right sides. The data are sent for processing. The 
process is detailed below. 
 
 
Figure 3. Diagram of TouchPair System Architecture 
1) 3D calibration and reconstruction module: In this 
module, we calibrate depth data for each object, obtained 
from the two Kinect sensors. The system makes a 3D 
reconstruction using a point cloud library with calibrated 
data. The module commands store calibrated and 
reconstructed data in a database, which is then used by the 
touch-recognition module. After storage, the module sends 
messages to the touch-recognition and pairing module about 
object ID and object location using the 3D point cloud data. 
2) Touch-recognition and pairing module: Touch is 
recognized in terms of the depth and position of the object 
and hands using 3D tracking. Using the previous depth 
information from the 3D reconstruction based on the point 
cloud, the system determines whether the hand touched the 
object, and if so, the position of the object. The system 
recognizes the time of touching between the user’s 3D hand 
point cloud data and the object 3D point cloud data, then 
determines whether they are paired. A paired analog 
object’s 3D point cloud data are stored and sent to the 
tracking module with information on the object type. We 
defined a limited objects database. 
3) Module for tracking paired objects and interactions: 
After pairing, the system tracks the analog object based on 
saved 3D point cloud data. The paired digital object can be 
tracked; however, the paired objects do not commonly move. 
The paired analog object is tangible, based on 3D analog 
object data, from the 3D calibration and reconstruction 
module. We can make an interaction with the digital object 
in this module; the interaction is shown by visual feedback. 
C. TouchPair Method using 3D Point Cloud Data 
Our proposed method uses 3D point cloud processing of 
Kinect depth data. A point cloud itself is a set of data points 
in a coordinate system. We measured a large number of 
points on the surface of an object using OpenFramework [6]. 
 
 
Figure 4. 3D Point Cloud Data 
The system obtains RGB data from two Kinect sensors 
(Fig. 4) and assigns them to the depth area. However, all 
directions of the object cannot be reconstructed. Thus, we 
find the most appropriate location for the Kinects and 
position them so that they cover most of the experimental 
space. 
1) Touch gesutre and recognition: Our touch pair 
system recognizes the touch actions of users’ hands based 
on depth. The system calculates the depth of each point 
between a user’s hand and the object by filtering closer data. 
The flow of recognition is as follows: 
a) Calculation of all point depths: Calculate all points 
of the analog and digital objects and the user’s hand on the 
table. 
b) Determination of finger position: The system 
calculates the minimum and maximum depth of the finger 
by defined thresholds because we hold our fingers in 
specific ways when we touch something. 
c) Determination of hand position: The system 
calculates the minimum and maximum depths of all fingers 
and the palm; from the front view, the system uses depth 
information from both sensors simultaneously. 
d) Determination of grasping: Using depth data on 
users’ hands and on objects collected from both sensors 
simultaneously, we found certain threshold values for 
recognizing the act of grasping. 
2) Analog-digital object pairing: Our system performs 
time calculations between touched analog-digital objects 
versus touched object-object pairing. Implementation of 
object-object touching is shown in Figure 5c,d. When the 
system recognizes that the user wants to pair, the color is 
changed. The red color refers to the digital object and the 
analog object is blue. The recognized hand is shown in 
yellow using 3D point cloud data. The main steps are as 
follows. 
168
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

 
Figure 5. (a) Pairing gesture with hand touching to digital object and 
grasping analog object from right camera (b) Pairing gesture from left 
camera (c) Pairing by object-object touching, toy and smart pad are pairing 
(d) Detailed object-object pairing image 
a) Time calculation: For pairing, the user maintains a 
touching posture for a few seconds after touching is 
recognized between the objects. The color is changed after 
the pairing. 
b) Tracking a paired set: To track paired objects, the 
system calculates 3D point cloud data continuously, which 
are provided by the real-time reconstruction module (Fig. 3). 
The color of the tracked object is shown. 
c) Changing a paired set: To change a paired object 
set, a pairing gesture is made for some time period. The user 
touches what he/she wants to pair. After a few seconds, 
performing the pairing gesture (Fig. 5) will change the pair 
set as indicated by the color feedback. 
D. Tangible Interactions 
After pairing objects, the system can be used in various 
tangible interactions. We designed a flight joystick using 
analog objects. Such a joystick can be used when playing a 
flight or gun game. Our system made a pairing between a 
notebook PC and a lotion bottle with a cap as a game 
controller. The game can be controlled by pushing the lotion 
bottle’s cap (see Fig. 6b) and moving it. The movements can 
be recognized based on the cloud data for the object and the 
hand. We also designed a map controller with paired objects 
(see Figs. 5d and 6c). Our system tracks the two toys and the 
touched position; the map is moved to provide feedback. 
Finally, we implemented a Windows Media Player 
controller with a paired green cube (see Fig. 6d). The system 
recognizes rotation of the cube’s cap, then changes the 
volume as feedback. The application changes songs when the 
lotion bottle cap is pressed. 
Figure 6. (a) Flight joystic controller (b) Paired analog object controller as 
flight joystic (c) Control the map by paired two toys (d) Control media 
player by paired green cube by rotation. 
IV. 
EVALUATION 
We evaluated the touch pair system focusing on 
recognition accuracy. We evaluated touch recognition while 
changing the analog and digital objects alternately. We also 
evaluated 3D object recognition and tracking to demonstrate 
the system’s usability and robustness. 
A. Recognition Accuracy Experiment 
Four analog objects and three digital objects were used. 
We evaluated finger touching, hand touching, grasping, and 
object-object touching for each object. The experiments were 
performed on computers (Intel Core i7 CPU, 2.5 GHz, and 
8.0 Gb RAM) using two Microsoft Kinect sensors for Xbox. 
We performed the experiments with 10 volunteers. We 
explained each touch pairing method sufficiently. Then a 
volunteer performed four touching gestures to each object 
100 times. We defined 3 s as the period for completing 
pairing via touching. When the system recognized a pairing, 
it showed color feedback. Touch recognition success alone 
was not counted. The participants were allowed to touch 
analog objects only with the finger and digital objects only 
with the hand. Table 1 shows the average pairing recognition 
success for the 10 volunteers. 
Our proposed system showed >90% pairing recognition 
with the objects provided. We found that the average finger-
based pairing recognition rate was higher than hand-based 
pairing. Finger pairing was recognized best when one or two 
fingers were used. Hand pairing requires checking whether 
the palm is touching. Thus, hand-based pairing recognition 
was less accurate than finger-based pairing. In addition, the 
success rate for touching a smart phone was lower than that 
for touching the other objects. This may have been because 
of the size of the object. Most adult hands are bigger than 
169
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

most smart phones. Thus, it becomes difficult for the system 
to find the positions of the finger and palm. 
TABLE I.  
FINGER AND HAND TOUCH PAIRING RECOGNITION RESULT 
  Digital 
 
Analog 
Note PC 
Smart Pad 
Smart Phone 
finger 
hand 
finger 
hand 
finger 
hand 
Toy 
98.3 
95.4 
99.1 
95.3 
95.3 
93.2 
Black 
Doll 
95.7 
93.3 
96.2 
94 
92.2 
90.1 
Green 
Cube 
98.4 
96.7 
99.3 
95.7 
96 
94 
Pet 
Bottle 
96.7 
95.3 
97.1 
95.4 
93 
91 
percentage of recognition rate(%) 
Table 2 shows the results for other pairing methods, such 
as grasping and object-object pairing recognition. The 
experiments were performed in the same way as described in 
Table 1. 
TABLE II.  
GRASPING AND OBJECT-OBJECT TOUCH PAIRING 
RECOGNITION RESULT 
Digital 
 
Analog 
Note PC 
Smart Pad 
Smart Phone 
grasp 
object 
grasp 
object 
grasp 
object 
Toy 
91.4 
94.3 
89.1 
98.1 
87.3 
94.2 
Black 
Doll 
85.4 
95.4 
85.1 
97 
85.2 
91.1 
Green 
Cube 
91.2 
96.2 
90.1 
96.7 
87 
96 
Pet 
Bottle 
90.1 
97.1 
90.2 
97.4 
88 
94.3 
percentage of recognition rate(%) 
Grasping-based pairing recognition accuracy was >85% 
with the objects provided. This method uses point data from 
many directions. Generally, the front, side, and back surfaces 
of an object are touched when holding something with the 
hand. Thus, grasping has to be determined by analyzing the 
data from many directions. Thus, on the whole, the 
recognition rate was low. We also found that the recognition 
rate differed by object size and hardness. A plastic toy, green 
cube, and bottle are relatively hard. However, a doll is very 
soft. When the user touches or grasps a softer object, the 
system has difficulty determining touch depth. Thus, 
recognition accuracy was lowest for the doll among all 
objects provided. However, generally, the recognition rate 
was high. 
B. Real-time 3D Object Tracking Accuracy 
We evaluated object tracking after pairing. We moved 
analog objects during a 10 min experimental period (e.g., 
left-right, front-back).  
Figure 7 shows the recognition accuracy for these tests. 
We obtained recognition rates of >80% for all objects. 
Because we used two Kinect sensors for real-time 3D object 
reconstruction and data comparisons, we obtained low error 
rates. 
 
 
Figure 7. Analog Objects Recognition Accuracy 
Figure 8 shows the recognition accuracy for three digital 
objects over 10 min. Smart phone recognition was <80% in 
around 4 min and 8 min after tracking. This is because the 
user was holding the smart phone with his/her hand. In 
particular, when we moved the smart phone with a front-
back motion, the recognition rate decreased.  
 
Figure 8. Digital Objects Recognition Accuracy 
When the smart phone was near the user’s body, the 
errors likely occurred because the smart phone “disappears” 
from the camera. However, we can overcome this problem 
by incorporating 3D data learning (obtained experimentally) 
into our system. 
V. 
CONCLUSIONS AND FUTURE WORK 
Our new dynamic analog-digital pairing method uses 3D 
point cloud data to assess tangible interactions dynamically. 
We used dynamic pairing based on finger and hand touching, 
grasping, and object-object touching. The accuracies were 
>90% for finger and hand touching and >85% for grasping 
and object-object touching. Almost the same results were 
obtained when we changed the locations of pairs 
dynamically. We obtained good real-time 3D object tracking 
results as well, despite using objects of different size, shape, 
and hardness. 
The contributions of the present study can be 
summarized as follows. First, we have provided a new 
pairing method. Using this method to dynamically pair 
analog and digital objects, various tangible interactions can 
be achieved. Second, there is no additional device or sensor, 
so varied object recognition and pairing are possible. Third, 
170
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

we obtained good recognition rates and tracking results using 
the proposed method. 
In future work, we expect to pair users and digital objects 
using this system. After pairing a user and a digital device, a 
digital object can be controlled by the user’s gestures and 
motions. In addition, we expect to develop various tangible 
interaction applications based on our proposed system. For 
example, an augmented graphical support interface with a 
paired object using a projector or head-mounted display 
could be developed. 
REFERENCES 
[1] F. Klompmaker, K. Nebe, and A. Fast, “dSensingNI: a 
framework for advanced tangible interaction using a depth 
camera,” TEI '12 Proceedings of the Sixth International 
Conference 
on 
Tangible, 
Embedded 
and 
Embodied 
Interaction, pp, 217-224. 
[2] E. S. Martinusseon, J. Knutsen, and T. Arnall, “Bowl: token-
based media for children,” DUX '07 Proceedings of the 2007 
conference on Designing for User eXperiences, Nov. 2007, 
Article No. 17. 
[3] R. Wimmer and S. Boring, “HandSense: discriminating 
different ways of grasping and holding a tangible user 
interace,” in TEI '09 Proceedings of the 3rd International 
Conference on Tangible and Embedded Interaction, pp. 359–
362. 
[4] R. Wimmer, “FlyEye: Grasp-Sensitive Surfaces Using Optical 
Fiber,” TEI '10 Proceedings of the fourth international 
conference on Tangible, embedded, and embodied interaction, 
pp.245–248. 
[5] OpenNI 3D sensing framework : Last visited on Nov, 2013. 
 http://www.openni.org. 
[6] openFrameworks : Last visited on Aug, 2013. 
 http://www.openframeworks.cc. 
[7] A. D. Wilson, “Using a depth camera as a touch sensor,” ITS 
'10 ACM International Conference on Interactive Tabletops 
and Surfaces, pp 69-72. 
[8] P. 
Wellner, 
“The 
digitaldesk 
calculator: 
Tangible 
manipulation on a desk top display,” in 4th annual ACM 
symposium on User interface software and technology UIST 
1991, pp.27-33. 
[9] I. Siio and Y. Mima, “Iconstickers: converting computer icons 
into real paper icon,” in 8th International Conference on 
Human-Computer 
Interaction: 
Ergonomics 
and 
User 
Interfaces, pp. 271-275. 
[10] P. Ljungstrand, J. Redstrom, and L. E. Holmquist, 
“Webstickers: using physical tokens to access, manage and 
share bookmarks to the web,” in Designingaugmented reality 
environments DARE 2000, pp.23-31. 
[11] T. Nishi, Y. Sato, and H. Koike, “Interactive object 
registration and recognition for augmented desk interface,” in 
IFIP conference on human-computer interface Interact 2001, 
pp.240-246.
 
171
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

