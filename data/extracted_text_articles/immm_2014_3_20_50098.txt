Improving Digital Forensics Through Data Mining 
 
Chrysoula Tsochataridou, Avi Arampatzis, Vasilios Katos 
Department of Electrical and Computer Engineering 
Democritus University of Thrace 
Xanthi, Greece 
{chrytsoc, avi, vkatos}@ee.duth.gr 
 
 
Abstract—In this paper, we reflect upon the challenges a 
forensic analyst faces when dealing with a complex 
investigation and develop an approach for handling and 
analyzing large amounts of data. As traditional digital forensic 
analysis tools fail to identify hidden relationships in complex 
modus operandi of perpetrators, in this paper, we employ data 
mining techniques in the digital forensics domain. We consider 
as a vehicle the Enron scandal, which is recognized to be the 
biggest audit failure in the U.S. corporate history. In 
particular, we focus on the textual analysis of the electronic 
messages 
sent 
by 
Enron 
employees, 
using 
clustering 
techniques. Our goal is to produce a methodology that could be 
applied by other researchers, who work on projects that 
involve email analysis. Preliminary findings show that it is 
possible to use clustering techniques in order to effectively 
identify malicious collaborative activities. 
Keywords-Digital Forensics; Email Analysis;Text Mining; 
Clustering; Weka; Simple K-means. 
I. 
 INTRODUCTION 
The incorporation of computer technology in modern life 
has increased the productivity and the efficiency in several 
aspects of it. However, computer technology is not only used 
as a helpful tool that enhances traditional methodologies. In 
unethical hands, it can be used as a crime committing tool as 
well. Particularly, technically skilled criminals exploit its 
computing power and its accessibility to information, in 
order to perform, hide or aid unlawful or unethical activities. 
Nowadays, the number of information security incidents is 
increasing globally. Considering the fact that a big 
percentage of the total information produced is digital, arises 
the need of retrieving electronic evidence in a manner that 
does not affect its value and integrity [1]. 
Most of the collected digital evidence is often in the form 
of textual data, such as e-mails, chat logs, blogs, webpages 
and text documents. Due to the unstructured nature of such 
textual data, investigators, during the stage of analysis 
usually employ searching tools and techniques to identify 
and extract useful information from a text. Textual 
information represents one of the core data sources that may 
contain significant information. The amount of textual data, 
even on a single personal digital device, is usually very large, 
in the order of thousands of texts or short messages. The 
analyst, in this context, encounters objective difficulties in 
data content analysis and in finding important investigational 
patterns. 
In this paper, we propose an approach for handling and 
analyzing large amounts of textual data using a tool for data 
analysis and predictive modeling called Weka [2], which 
provides a collection of machine learning algorithms that 
perform data mining techniques. Text mining has been 
proved to be able to profitably support intelligence and 
security activities in identifying, tracking, extracting, 
classifying and discovering patterns useful for building 
investigative scenarios [3]. The data we experiment with are 
the emails of the Enron corpus. The objective is to develop a 
method for future investigators, so that they can effectively 
identify and gain information from a large volume of 
unstructured textual data. This was accomplished first by 
parsing the data which were organized in folders, then by 
storing them into a MySQL database [4] to better manage 
them and finally by performing data mining techniques to the 
textual data in order to draw some conclusions about the 
content of the emails. The proposed method is especially 
useful in the early stage of an investigation when the 
researchers may have a little clue of how to begin with. As 
such, the main contribution of this paper is the demonstration 
of use of several standard data mining techniques in the 
domain of digital forensics as the current state of the art in 
digital forensics is limited in content based searching, rather 
than identifying contextual relations. 
The remainder of this paper is organized as follows. 
Section II includes the related work and research that was 
conducted during the previous years by other scientists who 
dealt with email analysis. Section III introduces the reader to 
the Enron case. This section describes the Enron corpus of 
emails, the script that was developed to process the email 
objects as well as the MySQL database which was designed 
to store the data retrieved. Section IV describes the data 
mining techniques and tools that were used to process the 
textual data and enabled us to further analyze them in 
Section V. Finally, Section VI draws our conclusions. 
II. 
RELATED WORK 
In 2004, Bryan Klimt and Yiming Yang conducted email 
classification for the Enron dataset [5]. Their goal was to 
explore how to classify messages as organized by a human. 
In order to accomplish it, Support Vector Machine (SVM) 
[6] classifier was used after they had cleaned the data from 
duplicate messages. Moreover, in 2005, Jitesh Shetty and 
Jafar Adibi created a MySQL database for the Enron dataset 
and statistically analyzed it [7]. In addition to this, they 
45
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-364-3
IMMM 2014 : The Fourth International Conference on Advances in Information Mining and Management

 
Figure 1.  The Database Schema 
derived a social network from the dataset and presented a 
graph of it.  
Concerning the text mining part, which is the extraction 
of knowledge from text documents, there were several tools 
proposed. Some of those were the Email Mining Tool (EMT) 
and the Malicious E-mail Tracking (MET). Those tools were 
developed at the Columbia University and employed data 
mining techniques to perform behavior analysis as well as 
social network analysis [8]. Furthermore, in the field of text 
mining, R. Al-Zaidy B. C. Fung, A. M. Youssef and F. 
Fortin proposed a data mining algorithm to discover and 
visualize criminal networks from a collection of text 
documents [9]. This paper also used Enron email corpus as a 
case study of real-life cybercrime. 
III. 
THE ENRON CASE 
The fraud investigated in this paper is the Enron scandal. 
The scandal was revealed in October 2001 and eventually led 
to the bankruptcy of the energy company Enron Corporation, 
based in Houston, Texas [10]. The fall of Enron has been 
characterized as the greatest failure in the history of 
American capitalism and its collapse had a major impact on 
financial markets, since Enron dealt with many financial 
institutions and organizations. The company’s collapse 
caused investors to lose a large amount of money and 
employees to lose their jobs, their medical insurances as well 
as their retirement funds. Additionally, it caused the 
dissolution of Arthur Andersen LLP, which was the audit 
company that performed both the internal and external 
accounting for Enron Corporation [11]. The federal 
investigations lasted 5 years and revealed the complex and 
illegal accounting practices that were conducted and 
encouraged by Enron’s former executives. The investigations 
also came up with 31 terabytes of digital data including data 
from 130 computers, thousands of e-mails, and more than 10 
million pages of documents, culling evidence that helped 
deliver convictions of the company’s top executives, among 
others [12]. The collection of those emails is used to form 
our forensic methodology. 
A.  The Enron dataset 
A few years after the scandal, a part of the digital 
collection was published by William Cohen, a professor at 
Carnegie Mellon University. The collection originally 
consisted of 1,500,000 emails [13]. However, some of them 
were withdrawn because of the complaints that former 
employees made, since they believed that their personal life 
was violated. The Enron dataset used in our project consists 
of 519,000 electronic messages both personal and formal, 
excluding the files that were attached in those mails [14]. 
The emails of this collection follow the RFC 5322 format 
and were used as a first material in order to produce a 
methodology that could be applied by other researchers, who 
work on projects that involve email analysis [14]. To achieve 
this goal, a certain procedure was followed. 
B. Directory traversing and processing of email objects 
The first step was to download the previously described 
dataset which included electronic messages organized in 
3,500 files. Each and every one of the 151 employees that 
participated in this collection was represented by a unique 
folder. The employee’s folder included other subfolders such 
as “inbox”, “sent_items”, “deleted_items” etc. Finally, inside 
 those subfolders were the electronic messages. It is obvious 
that various levels of folders had to be traversed in order to 
reach the electronic message. This became feasible via the 
development of a Python script. The script conducted in the 
first place directory traversing and data parsing (meaning 
syntactic analysis) soon after the electronic message was 
reached.  
As mentioned earlier, the messages were formulated 
according to the RFC 5322 format. For the purpose of data 
46
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-364-3
IMMM 2014 : The Fourth International Conference on Advances in Information Mining and Management

parsing, the message was separated into header and body. 
The header of the message included information such as the 
date, the electronic addresses of the sender and the 
recipients, the subject and the id of the message. The body 
part represented the text of the message. After the split of the 
message into those two parts, the syntactic analysis of the 
header was performed, line by line, so that the necessary data 
could be extracted. The size of the data was large causing 
difficulties in storing and processing them using simple text 
files. Thus, the creation of a database that would solve those 
problems seemed reasonable. 
C. Description of the MySQL Database 
The data retrieved from the syntactic analysis of the 
message were inserted via a custom Python script into the 
corresponding fields of the database tables. The MySQL 
database named “enron” consisted of 4 tables. The table 
“MESSAGE” included fields with records related to the 
electronic message (Date, Subject, body, etc.), the table 
“RECIPIENTINFO” included fields with records that 
referred to the recipients of the message, the table 
“OTHER_RECEIVERS” included fields with records 
regarding the recipients of the BCC and CC type emails. 
Finally, a table named “DELETED_MAILS” was also 
created to store the data of the messages that employees used 
to delete. The database schema is shown in Fig. 1. 
IV. 
D ATA MINING TECHNIQUES WITH WEKA 
Having stored the necessary data in the MySQL database, 
the next step is to perform data mining techniques on them, 
in order to extract some useful information. For this purpose 
Weka is used, which is a collection of machine learning 
algorithms for data mining tasks [15]. Weka contains tools 
for data pre-processing, classification, regression, clustering, 
association rules and visualization [16]. This paper focuses 
on textual analysis of the electronic messages using 
clustering techniques. Therefore, a series of filters and 
algorithms were applied to the subject and the body of the 
messages. Since Weka’s software is written in Java, Weka 
provides access to SQL databases using Java Database 
Connectivity (jdbc connector) and can process the result 
returned by a database query. In this case, msubject and 
mbody were loaded from the database into Weka via the 
execution of a query. An example of a query executed 
follows below. 
 
select msubject,mbody from MESSAGE 
where from_email=”ken.lay@enron.com”; 
 
Via the execution of the query above, the fields msubject 
and mbody of table “MESSAGE” are loaded as attributes 
into Weka. The messages processed and then clustered were 
the ones that were sent by key executives of the company. 
The top executives examined were Jeffrey Skilling, Kenneth 
Lay, Andrew Fastow, Tim Belden, Mark Koenig and 
Vincent Kaminski. The reason of using the “where 
from_email=user@enron.com ” statement is to specify each 
time whose messages are going to be loaded. 
 
A. The StringToWordVector filter 
After the data is loaded, the filter StringToWordVector is 
applied to the attributes msubject and mbody. Those 
attributes contain instances which represent the subject and 
the body of the messages sent by the x executive. The 
StringtoWordVector filter pre-processes the data so that the 
clustering algorithm Simple K-means can later be applied. 
This filter converts string attributes like msubject, mbody 
into a set of attributes that each of them represents a single 
word. In other words, StringToWordVector creates a list of 
words which are actually attributes. These words existed 
inside the subject and the body of the messages. Before the 
application of the filter some settings have been set in the 
pop-up window of StringToWordVector. The settings used 
are the ones below: 
• 
TF-IDF Transform, (term frequency–inverse 
document frequency), which is a numerical statistic 
used as a weighting factor that reflects how 
important a word is to a message of the collection. 
The tf-idf value increases proportionally to the 
number of times a word appears in a message, but is 
offset by the frequency of the word in the corpus 
[17]. The weighting factor gets bigger when the 
word appears frequently in a small amount of 
messages and smaller when the word appears 
frequently in the majority of messages [17]. For 
example, the word “energy”, which is going to 
appear many times in most of the emails- as Enron 
was originally an energy company- is of little 
importance despite its common use and is going to 
get a low weighting factor. Some other words 
instead, appear many times in a few messages. From 
the investigational point of view, the smaller the 
community that discusses a specific matter is, the 
more suspicious this pattern of behavior becomes. 
Therefore, these words get a high weighting factor 
because they might be indicative of malicious 
activities. It should be noted that TF-IDF is a 
standard effective transformation in Information 
Retrieval, however, recent empirical evidence shows 
that it may not be the best choice for some 
classification methods, e.g. SVM [18]. Nevertheless, 
there is no study, in our knowledge, suggesting that 
the same holds for clustering with K-means as 
employed in this work; consequently, we hold to 
traditional methods and use TF-IDF. 
• 
minTermFreq, which defines a minimum frequency 
of a word occurrence. If set on 3 for example, the 
words that come up after the filter’s application are 
those which appear at least 3 times in the collection. 
• 
Stemmer, which removes the endings of the words. 
In general, stemming is the process of reducing 
inflected or derived words to their stem, base or root 
form [16]. 
• 
Stopwords: This parameter if set to “True”, 
activates the default stopwordlist of Weka. A 
stopwordlist is a list of words such as because, is, 
then, often that are commonly used and do not 
47
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-364-3
IMMM 2014 : The Fourth International Conference on Advances in Information Mining and Management

provide important information about the content of a 
message. 
• 
WordsToKeep: This parameter predetermines the 
number of the words that are going to appear after 
we apply the StringToWordVector filter. 
Table I shows the settings in the pop-up window of 
StringToWordVector, when the filter was applied to the 
string attributes msubject and mbody of Lay’s sent messages. 
It should be noted that the settings - minTermFrequency and 
words_to_keep in particular - may differ between suspects, 
depending on the number of messages sent by each of them. 
TABLE I.  
STRING TO WORD VECTOR SETTINGS FOR LAY’S 
MESSAGES 
StringToWordVector 
settings 
Lay 
IDF Transform 
TRUE 
TF Transform 
TRUE 
minTermFrequency 
3 
words to keep 
1500 
stoplist Weka 3-7-1 
TRUE 
Stemmer 
SnowballStemmer 
Delimeters 
0123456789!#$%^&*(
)-=_+\/;"',.><][ 
B. Simple K-means algorithm 
After the application of the filter, the string attributes 
msubject, mbody are converted into a list of words 
(dictionary), which are obviously the most frequent words 
that exist in the messages that sent the x executive (Kenneth 
Lay in the example above). The next step is to apply the 
Simple K-means algorithm [19] in order to perform cluster 
analysis on the messages. Simple K-means is very popular 
and one of the simplest algorithms which uses unsupervised 
learning to solve clustering problems. Although it has some 
drawbacks (strong sensitivity to outliers and noise; works 
best with hyper-spherical cluster shapes; number of clusters 
and initial seed value need to be specified beforehand; 
converges to local optima), other clustering algorithms with 
better features tend to be much more expensive [16]. Our 
focus in this study is to investigate and demonstrate how 
clustering can be useful for forensic analysis of data. The 
dataset we employ (519,000 electronic messages) can be 
considered large for many clustering algorithms, but Simple 
k-means is easy to be implemented efficiently and applied 
even to large datasets. 
Cluster analysis is the task of grouping a set of messages 
in such way that messages from the same group (called 
cluster) are more similar to each other than to those in the 
other clusters. Simple K-means separates the emails into K 
groups (clusters). The K variable is an integer number and 
expresses the number of groups into which the messages are 
divided. The objective of this algorithm is the minimization 
of the mean squared Euclidean distance of the messages 
from the centers of their clusters [17]. Apart from separating 
the messages into K clusters, Simple K-means gives a 
weighting factor in the words that resulted after we applied 
the StringToWordVector filter. This weighting factor 
indicates the correlation of the word to the content of the 
messages that belong to a cluster. Moreover, the weighting 
factor depends on the word’s frequency in the messages in 
general. The weighting factor is different for the same word 
from cluster to cluster. This is because one word can be more 
representative of the content of the messages in cluster0 than 
in cluster1. The biggest the factor is, the more representative 
of the cluster the word is. This function of Simple K-means 
helps in better understanding the content of clusters of 
messages. There is also a pop-up window for Simple K-
means with a series of settings. The parameters which have 
been set before the application of the algorithm are: 
• 
numClusters, which determines the number of the 
clusters into which the messages will be separated 
[17]. 
• 
maxIterations, which expresses the maximum 
number 
of 
iterations 
and 
predetermines 
the 
implementation time of the algorithm Simple K- 
means [17]. 
• 
Seed: With this setting multiple iterations of the 
algorithm with different random initial centers can 
be done [17]. 
The first 3 rows of Table II show the values that were 
given to the settings of Simple K-means pop-up window for 
the cluster analysis of Lay’s sent messages, before the 
algorithm was executed. The other rows indicate Simple K- 
means behavior after the execution of the algorithm to the 
data. 
Similar 
to 
StringToWordVector, 
the 
settings 
number_of_clusters and seed may differ between the 
suspects examined. The clusters for each suspect in the 
performed experiments were not more than six. This choice 
was guided by the practical demand of obtaining a limited 
number of informative groups. 
TABLE II.  
SIMPLE K-MEANS SETTINGS FOR THE CLUSTER 
ANALYSIS OF LAY’S SENT MESSAGES 
SimpleK-means settings 
Lay 
number of clusters 
5 
max iterations 
30 
Seed 
8 
iterations needed 
2 
sum of squared errors 
1815.91 
Attributes 
598 
 
The final values given in the parameters above were 
chosen based on the fact that their combination minimized 
the sum of squared errors. The distribution of messages into 
clusters is being described in the next section. 
V. 
RESULT ANALYSIS 
In this section, the results that came up after the 
application of the Simple K-means algorithm will be 
analyzed. As it was previously stated, the data processed 
were the subject (msubject) and the body (mbody) of the 
48
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-364-3
IMMM 2014 : The Fourth International Conference on Advances in Information Mining and Management

messages that were sent by Enron’s key “players”. The 
cluster analysis which was conducted resulted in the 
configuration of five clusters of messages. Table III shows 
the distribution of Lay’s sent messages per cluster.  
TABLE III.  
D ISTRIBUTION OF L AY ’ S SENT MESSAGES INTO 
5 CLUSTERS 
Clustered Instances 
Cluster 0 
7566 
94% 
Cluster 1 
255 
3% 
Cluster 2 
16 
0% 
Cluster 3 
70 
1% 
Cluster 4 
127 
2% 
 
It is being obvious, that the majority of messages (7566 
messages) that Lay sent belongs to cluster0. The second 
cluster (cluster1) includes 255 messages, the third (cluster2) 
16 messages, the fourth (cluster3) 70 messages and finally 
the fifth one (cluster4) includes 127 messages. With the help 
of MS Excel, Weka’s results are processed. Specifically, 
every time an executive’s messages are being clustered, a 
table is produced. 
Each column of this table represents a cluster of 
messages and includes the most important words of the 
cluster. The words for each cluster are being written in a 
descending order, according to the weighting factor that 
Simple K-means gave in each word for every cluster. The 
bigger the factor is, the most important the word for the 
cluster becomes. Consequently, the most representative 
words of a cluster’s content possess the first positions of 
each column. 
TABLE IV.  
THE FIRST MOST FREQUENT AND IMPORTANT WORDS IN K ENNETH LAY’S CLUSTERS OF MESSAGES 
Cluster 0 
Cluster 1 
Cluster 2 
Cluster 3 
Cluster 4 
Analyst 
calendars 
AFL 
Tuesday 
Electric 
Billy 
Noon 
AFLCIO 
Open 
Embedded 
ChairManagement 
St 
CounselAFLCIO 
Hold 
Energy 
ChairmanSubject 
Joannie 
DC 
October 
Gas 
CommitteeAssociate 
Williamson 
DSilvers 
Forward 
HNG 
DepartmentHuman 
Note 
Damon 
Meeting 
Natural 
LayDepartment 
questions 
Dsilvers 
Monday 
Pipeline 
Leading 
Call 
General 
Directors 
Transco 
LeadsProgram 
Place 
November 
Managing 
allies 
Lemmons 
Executive 
Press 
Bringing 
arrival 
Office 
announced 
Release 
Quarterly 
arrived 
Program 
Basis 
ReleaseKen 
Earlier 
chairman 
ProgramDate 
quarter 
Silvers 
Executive 
challenges 
ProgramJohn 
Directors 
SilversAssociate 
Announced 
changing 
RepsProgram 
Managing 
Street 
Basis 
commitment 
Resources 
bringing 
aflcio 
Quarter 
dedication 
Sherriff 
quarterly 
apologies 
PMTo 
developing 
SupervisorsEmbedded 
Monday 
inadvertantly 
Lay 
directors 
Worlds 
meeting 
omitted 
MessageFrom 
endeavors 
WorldwideFrom 
October 
release 
Work 
energy 
Asset 
forward 
doc 
Kenneth 
environment 
Broadest 
Earlier 
press 
Office 
Era 
businesses 
purpose 
Washington 
KennethSubject 
facing 
Campus 
Committee 
attached 
Rosalee 
Felt 
Cc 
 
fax 
Dont 
gratitude 
49
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-364-3
IMMM 2014 : The Fourth International Conference on Advances in Information Mining and Management

Table IV shows the most frequent words that exist in 
every cluster of Lay’s messages. Our aim is to understand 
the content of the messages that belong to each cluster and 
then make some assumptions about the concerns of the 
executives (in this case Lay’s) through time. It is worth 
mentioning that the table below is just a sample. The original 
table was very large to fit in this paper. For this reason, we 
chose to show the table with the first 30 most important 
words for each cluster of messages. 
The same process was also applied in the cases of the 
other executives. The collected emails were processed and 
clustered separately, thus obtaining four different scenarios, 
one for each employee. The underlying hypothesis was that 
email contents can also be characterized by the role the 
mailbox owner played within the company. Tables V to VIII 
that follow, report on the results obtained by these 
experiments. Each table shows the terms that characterize 
each and every one of the clusters. For each cluster, the most 
descriptive words between the 30 most frequent words of the 
cluster are listed. 
TABLE V.  
LAY’S RESULTS 
cluster 
Most frequent and important words 
0 
LEADS Program, Program, graduates, 
worldwide, guide, supervisors, direction, 
importance, members, philosophy 
1 
Joannie Williamson, executive, director, 
meeting, earlier, October, purpose, 
Committee 
2 
AFL, AFLCIO, counsel, release, November, 
Press, Ken, Silvers, apologies 
3 
Open, meeting, directors, office, Kenneth, 
Rosalee 
4 
HNG, Natural, Gas, challenges, changing, 
commitment, dedication, era, history, ideas, 
gratitude 
TABLE VI.  
SKILLING’S RESULTS 
cluster 
Most frequent and important words 
0 
Axis, analysts, create, Department, find 
manager, dataProgram, search, Toolbox, 
profile, qualifications, applications, website, 
career 
1 
slipped, busy, running, meeting, Baxter, 
Whalley 
2 
Stanford, Survey, favorite, firm, feedback, 
employees, McKinsey, Globe, marketplace 
3 
Harvard, Presented, GarvinProfessor, 
ProfessorDavid, Plan, seminar, drinks, 
partnership, HBS 
4 
Amanda, Andrew, attend, plz, Colwell, 
Donahue, Fastow, Scott, Shapiro, Kaminski, 
Whalley, Wednesday 
5 
Skilling, Joannie, dinner, discuss, meeting, 
questions, company 
 
The analysis of the tables was the part of the process that 
involved a significant amount of uncertainty. Based on the 
words of each cluster of messages and on previous research 
that was conducted on the Enron case, an attempt was made 
on  drawing  some  conclusions  regarding  the content of the 
TABLE VII.  
FASTOW’S RESULTS 
cluster 
Most frequent and important words 
0 
Brasoil, Brazil, Rio, transaction, 
accountants, agreement 
1 
AIG, Highstar, coming, fund, discuss, 
Louise, date 
2 
Lay, Cliff, Andy, Jeff, Louise, critical, LJM, 
didn’t, interest, job, loss, lost, market, 
members 
TABLE VIII.  BELDEN’S RESULTS 
cluster 
Most frequent and important words 
0 
Article, Californiacentric, Pools, 
PowerExchange, Professor, Wilson, 
economist, versuscentralized, powerpools, 
pros, cons, fit, advised, compares, people, 
market, California 
1 
Communicating, DianaScholtes, risk, 
cashflows, headaches, problem, California, 
term 
2 
FRR, frequency, pool, construction, 
hydrological,machines, spin, Ingersoll 
3 
Communication, Excellence,GlobalMarkets, 
Improve, Introduce, Influence, Promote, 
Simplify, application, opportunity, diagnosis 
4 
information, prices, market, electricity, 
observations, analysis, demand, power, 
purchases, FERC, year, month, Ray Alvarez 
 
messages that were sent by some of the top executives of the 
organization. Even by a superficial inspection of the words in 
the first column of Table IV, represented as cluster0, we 
assume that Lay’s messages had to do with a program called 
“LEADS”. There is a great chance that Enron had employed 
graduates from that program. Lay’s messages tend to inspire 
the superior executives so that they help those new 
employees to adapt easily in Enron’s environment. Those 
assumptions are based on the presence of the words LEADS 
Program, Program, graduates, worldwide, guide, supervisors, 
direction, importance, members, philosophy (some words 
cannot be seen in Table IV, but exist in the original full table 
and are worth mentioning). After some research from 
external public sources, it was discovered that the UC 
LEADS Program is one of the most prestigious fellowships 
awarded by the University of California system [20]. This 
program supports up to nine UCLA upper-division 
undergraduate students in the fields of science, technology, 
engineering, and mathematics with educational experiences 
that prepare them to claim positions of leadership in 
academia, industry, government, and public services 
following the completion of a doctoral degree. This confirms 
our initial hypothesis for the content of Lay’s message, since 
we know the target group of employees that Enron use to 
have fulfills the above criteria. From the investigational point 
of view, some interesting aspects emerges in Lay’s results as 
there is an interesting cluster (cluster 1) in which the context 
is referring to a meeting that was scheduled to take place 
earlier than it was originally planned. The words “purpose” 
and “Committee” might be an indication of why the meeting 
had to be rescheduled, something that causes suspicion as we 
50
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-364-3
IMMM 2014 : The Fourth International Conference on Advances in Information Mining and Management

know that there was a committee that was monitoring and 
investigating Enron’s financial statements and balance sheet. 
Skilling’s emails do not underline any particular trend. It 
could be interesting to analyze more in depth cluster 4 where 
several names of top executives are being noticed. However, 
Fastow’s results seem extremely interesting. Specifically, 
cluster 0 includes words that express business activities and 
transactions. After some electronic research, it was found 
that in September 1999, Enron sold LJM I a 13% stake in a 
company that was building a power plant in Cuiaba, Brazil. 
This sale, for approximately $11.3 million, altered Enron's 
accounting treatment of a related gas supply contract and 
enabled Enron to realize $34 million of mark-to-market 
income in the third quarter of 1999, and another $31 million 
of mark-to-market income in the fourth quarter of 1999 [21]. 
The offshore company LJM was Fastow’s creation and this 
transaction seemed to be very critical. Cluster 2 also 
underlines a compact group of emails in which important 
financial aspects are discussed. It’s worth noticing that the 
name “Louise” exists both in clusters 1 and 2, which 
indicates that Louise and Fastow had a close professional 
relationship. 
Belden’s emails have no particular terms. The only 
aspect that can be clearly understood is that his position is 
tightly linked to California business and electricity market. 
This definitely addresses the need to examine more in depth 
those emails, as they may lead to the acquisition of valuable 
information regarding the California crisis and Enron’s role 
in it [22]-[23]. 
Summarizing, we made an effort to approach the content 
of the messages of the other clusters in a similar manner. 
Initially, we examined the words of each cluster of messages 
in order to formulate an assumption related to the content of 
the emails. Then, we cross-validated our hypothesis via 
further literature research confirming some keywords to 
events relating to suspicious activities as shown above.  
VI. 
CONCLUSIONS 
The textual analysis of each cluster of messages was a 
process that provided useful information to the researcher. 
However, there are risks in the analysis of the results since 
this process is cumbersome and difficult because of the 
unstructured nature of the text documents. It is important that 
every time cross-validation of the digital evidence with other 
resources is conducted to reveal the truth of a fact. 
Concerning the clustering techniques used, the Simple K-
means algorithm was efficient and performed cluster analysis 
in a satisfying level without significant differences in the 
resulting clusters when the parameters in the algorithm’s 
pop-up window were changed for the same person. The 
content of the messages that belonged to each cluster was 
obvious enough to make our assumptions and gain a deeper 
idea of each executive’s concerns. Moreover, neither null 
clusters nor large squared errors were noticed after the 
execution of the algorithm. Finally, with respect to the matter 
of large datasets, it was noticed that the process was time-
consuming and inaccurate. 
 
REFERENCES 
[1] U.S. Department of Justice, “Electronic Crime Scene 
Investigation: A guide for First Responders”, [Online]. 
Available:  http://www.ncjrs.gov/pdffiles1/nij/219941.pdf. 
[Accessed 26 05 2014] 
[2] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann 
and I. H. Witten, "The WEKA Data Mining Software: An 
Update”, SIGKDD Explorations, vol. 11, Issue 1, 2009. 
[3] W. Fan, L. Wallace, S. Rich and Z. Zhang, “Tapping the  
power of text mining”. Comm. of the ACM. 49, pp. 76–82 , 
2006. 
[4] MySQL Database [Online] Available: http://www.mysql.com/ 
[5] B. Klimt and Y. Yang, "The Enron Corpus: A new dataset for 
email 
classification 
research", 
[Online]. 
Available:  
http://www.bklimt.com/papers/2004_klimt_ecml.pdf. 
[Accessed 26 05 2014]. 
[6] C. Cortes and V. Vapnik, "Support-vector networks”, 
Machine Learning, vol. 20, no. 3, pp.273-297, 1995. 
[7] J. Shetty and J. Adibi, "The enron email dataset, database  
schema and brief statistical report", [Online]. Available: 
http://foreverdata.com/1009/Enron_Dataset_Report.pdf. 
[Accessed 26 05 2014]. 
[8] S. J. Stolfo, S. Hershkop, K. Wang, and O. Nimeskern, 
"EMT/MET: systems for modeling and detecting errant e-
mails", Proceedings of DARPA Information Survivability 
Conference and Exposition, 2003.  
[9] R. Al-Zaidy, B. C. Fung, A. M. Youssef, and F. Fortin, 
"Mining 
criminal 
networks 
from 
unstructured 
text 
documents", [Online]. Available: http://dmas.lab.mcgill.ca/ 
fung/pub/AFYF12diin.pdf. [Accessed 26 05 2014]. 
[10] Wikipedia, the free encyclopedia, "Enron scandal", [Online]. 
Available: 
http://en.wikipedia.org/wiki/Enronscandal. 
[Accessed 26 05 2014]. 
[11] W. W. Bratton, "Enron and the dark side of shareholder 
value", 
[Online].Available: 
 
http://papers.ssrn.com/sol3/ 
papers.cfm? abstract_id=301475.  [Accessed 26 05 2014]. 
[12] The FBI, federal bureau of investigation, "Digital Forensics: 
It's a bull market", [Online]. Available:  http://www.fbi.gov/ 
news/stories/2007/may/ rcfl050707. [Accessed 26 05 2014]. 
[13] W. Cohen, MLD, CMU, "Enron Email Dataset",[Online]. 
Available: https://www.cs.cmu.edu/~enron/. [Accessed 26 05 
2014]. 
[14] E. P. Resnick, "Internet Message Format", [Online]. 
Available: http://tools.ietf.org/html/rfc5322.  [Accessed 26 05 
2014]. 
[15] Machine Learning Group at the University of Waikato, "Data 
Mining:Practical Machine Learning Tools and Techniques", 
[Online]. 
Available:http://www.cs.waikato.ac.nz/~ml/weka/book.html.    
[Accessed 26 05 2014]. 
[16] I. H. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and S. J. 
Cunningham, "Weka: Practical machine learning tools and 
techniques with Java implementation,", Department of 
Computer Science, University of Waikato, New Zealand, 
[Online]. 
Available: 
http://www.cs.waikato.ac.nz/~ml/ 
publications/1999/99IHW-EF-LT-MH-GH-SJC-Tools-
Java.pdf. [Accessed 26 05 2014]. 
[17] C. D. Manning, P. Raghavan, and H. Schutze, Introduction to 
Information Retrieval, Cambridge University Press, 2008. 
[18] G. Forman, "BNS feature scaling: an improved representation 
over tf-idf for svm text classification”. In proceedings of the 
17th ACM conference on Information and knowledge 
management (CIKM ’08), ACM, New York, NY, USA, pp. 
263-270, 2008. 
51
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-364-3
IMMM 2014 : The Fourth International Conference on Advances in Information Mining and Management

[19] J. A. Hartigan, M. A. Wong, "Algorithm AS 136: A K-Means 
Clustering Algorithm”, Journal of the Royal Statistical 
Society, Series, vol. 28, no. 1, pp. 100–108, 1979. 
[20] Univeristy of California, "UC LEADS", [Online]. Available: 
http://www.ugresearchsci.ucla.edu/ucleads.htm.  
[Accessed 26 05 2014]. 
[21] FindLaw, 
"Brazil 
is 
rising", 
[Online]. 
Available:  
http://news.findlaw.com/wsj/docs/enron/sicreport/chapter6.ht
ml.[Accessed 26 05 2014].  
[22] Wikipedia, the free encyclopedia, “California electricity 
crisis,[Online]. 
Available: 
http://en.wikipedia.org/wiki 
/California_ electricity_crisis. [Accessed 26 05 2014]. 
[23] J. Leopold, “Enron linked to California blackouts”, 
Available:http://www.marketwatch.com/story/enron-caused-
california-blackouts-traders-say. [Accessed 26 05 2014] 
 
 
52
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-364-3
IMMM 2014 : The Fourth International Conference on Advances in Information Mining and Management

