RefRec: Indoor Positioning Using a Camera Recording
Floor Reï¬‚ections of Lights
Shota Shimada
Graduate School of
Information Science and Technology
Hokkaido University
Sapporo, Japan
Email: shimadas@eis.hokudai.ac.jp
Hiromichi Hashizume
National Institute of Informatics
Tokyo, Japan
Email: has@nii.ac.jp
Masanori Sugimoto
Graduate School of
Information Science and Technology
Hokkaido University
Sapporo, Japan
Email: sugi@ist.hokudai.ac.jp
Abstractâ€”In recent years, there has been a growing interest in
indoor positioning techniques using the ubiquitous infrastructure.
This paper describes an indoor positioning method using light-
emitting diode light reï¬‚ecting from the ï¬‚oor and a smartphone
camera recording it. Almost all traditional methods must detect
the light source directly. However, there are some constraints to
their usage since light sources cannot always be detected directly.
Our proposal aims to solve this problem by estimating the position
of a camera that does not face the light directly but records
light reï¬‚ected from the ï¬‚oor. The camera need not detect the
ceiling lights directly from an image, unlike existing methods.
Experimental results show that the proposal requires less than
1/100 the number of pixels for localization as do existing methods,
and the 3-D position and attitude can be estimated within 0.27 m
and 5.78 degrees at the 90th percentile in a 4.0 m square room.
Keywordsâ€“Visible light positioning; Received signal strength;
Angle of arrival.
I.
INTRODUCTION
With the development of ubiquitous computing, there is
great interest in Indoor Positioning and Indoor Navigation
(IPIN) technology for mobile devices. In 2017, United States
(US)$2,642 million in revenue from IPIN techniques was real-
ized and this revenue is predicted to reach US$43,511 million
by 2025 [1]. That is because IPIN has valuable applications in
medical care, manufacturing, advertising, and sales, amongst
others. In particular, the retail industry led the IPIN market in
2017 and predicted that demand such as improved customer
searches, effective route planning, and optimized customer
targeting would continue to increase. Customers usually do
not prefer to obtain additional devices for IPIN, whereas many
people have mobile phone devices.
From this background, many methods of the indoor posi-
tioning for mobile devices have been proposed. These utilize
radio waves, infrared, sound, computer vision, visible lights,
and others [2]. Among them, Visible Light Positioning (VLP),
using a transmitter and receiver constructed by Light-Emitting
Diode (LED) and camera or Photodiode (PD), has shown some
promise for indoor positioning [3]. The receiver recognizes the
light of the transmitters and then calculates its relative position
to the transmitters. Compared with other methods such as those
based on radio waves, VLP has three major advantages. First,
VLP is efï¬cient because LEDs can be used not only as lighting
for humans but also as transmitters for positioning. Second,
visible light does not penetrate objects, which reduces the
multipath problem. Third, it is easier to install the VLP system
because many buildings have the infrastructure for lighting.
Usually, LEDs should be modulated in high-frequency
ranges to avoid humans sensing ï¬‚ickering, and to enable
detection by the receiver. However, PDs used in Commercial
Off-The-Shelf (COTS) mobile devices are not sensitive enough
and their response rates are too slow for VLP. Therefore, we
focus on a camera-based method using only a mobile device
as a receiver.
Most of the existing camera-based methods use a large
image for positioning because they need to detect multiple light
sources installed at different places on a ceiling, directly from
the image. It is computationally demanding, and some existing
methods cannot calculate in real time using smartphones
[4][5]. Also, it is too difï¬cult to capture several LEDs into the
image if the ceiling is low (it means a Loss of Signal: LOS).
The user will therefore be forced to detect the LEDs by moving
the camera. Some prior work also requires changing the
camera to use a Complementary Metal Oxide Semiconductor
(CMOS) image sensor that implements a rolling shutter. This
feature enables multiple samplings in one image, but customers
usually do not like the distortion of the image caused by the
rolling shutter. However, there are developments to overcome
this problem and this effect may disappear in the future [6].
In order to address these problems, we propose the VLP
method RefRec, which does not ï¬nd a light source directly but
records reï¬‚ected light on the ï¬‚oor. In our proposal, a camera
captures light from the ceiling that is reï¬‚ected by the ï¬‚oor
and estimates the distance from the LED. No matter where
the camera captures the ï¬‚oor reï¬‚ection, the ceiling light will
be reï¬‚ected everywhere on the ï¬‚oor. The challenge is that
reï¬‚ections of multiple lights will be overlapped. To separate
them, the frame rate of the camera and each frequency of
LED light is determined by DC-biased Optical-Orthogonal
Frequency Division Multiplexing (DCO-OFDM) [10]. Our
experimental in a 4.0 m square room shows that our method
requires 32 Ã— 32 Ã— 9 pixels for self-positioning within 0.27 m
and 5.78â—¦ at the 90th percentile. A comparison with existing
methods is shown in Table I. The details are described in
Section II.
Our contributions are summarized as follows:
57
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-811-2
UBICOMM 2020 : The Fourteenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

TABLE I. COMPARISON WITH EXISTING VLP METHODS USING LED.
Epsilon [7]
Luxapose [4]
PIXEL [8]
Rajagopal [5]
Nakazawa [9]
Our proposal
Accuracy of position
âˆ¼0.4m
âˆ¼0.1m
âˆ¼0.3m
N/A
âˆ¼0.1m
âˆ¼0.27m
Accuracy of rotation
N/A
âˆ¼ 3â—¦
N/A
N/A
N/A
âˆ¼ 5.78â—¦
Method
Model
AoA
Polarized
PRR
Model
RSS
Coverage
5.0 mÃ—8.0 m
1.0 mÃ—1.0 m
2.4 mÃ—1.8 m
3.9 mÃ—8.0 m
1.0 mÃ—2.4 m
4.0 mÃ—4.0 m
Facing
Ceiling
Ceiling
Ceiling
Floor
Ceiling and ï¬‚oor
Floor
Distortion
No
Yes
No
Yes
Yes
No
Extra
PD
No
Filter
No
No
No
Number of LEDs
5
5
8
4
2
4
Resolution
N/A
7712Ã—5360
120Ã—160
1280Ã—720
3280Ã—2460
32Ã—32Ã—9
1472Ã—1104
â€¢
The VLP method does not require large images and
does not have rolling shutter distortion.
â€¢
The six-degrees-of-freedom mobile attitude estimation
algorithm uses only modulated LEDs and a camera.
â€¢
Performance evaluation of the proposal was carried
out by real-time positioning experiments.
We describe the challenges of prior works in Section II, our
new method for six-degrees-of-freedom localization in Section
III, the details of our prototype in Section IV, the experiments
to demonstrate the advantages of our method in Section V, and
the limitations of our proposal in Section VI. Our conclusion
and future work are summarized in Section VII.
II.
RELATED WORK
To explain the VLP method using PDs or cameras, some
existing methods are selected, and their challenges are dis-
cussed.
A. Visible light positioning using PDs
Among the methods using PDs, Time of Arrival (ToA)
[11], Time Difference of Arrival (TDoA) [12], Angle of Arrival
(AoA) [13], and Received Signal Strength (RSS)-based [14]
ones have been proposed. Our proposed method utilizes these
previous RSS-based studies. Epsilon was the ï¬rst indoor VLP
designed in the academic community [7]. It detects the binary
shift keying of the LED using the prototype device with the PD
and derives the position by triangulation. Accuracies of 0.4 m,
0.7 m, and 0.8 m at the 90th percentile were achieved in three
different ofï¬ce spaces. NALoc uses the same type of device as
the ambient light sensor that is implemented in a smartphone.
The results gave 90th-percentile errors of less than 0.35 m for
the 2-D position, but the device was evaluated separately from
the smartphone and not in a built-in setting [15].
B. Visible light positioning using camera
Camera-based methods allow geometrical separation of
the light sources, allowing for more accurate positioning
[16][17]. Luxapose can compute the position and posture of the
smartphone by capturing ceiling lights directly with a camera
[4]. The error is less than 10 cm and less than 3â—¦. It uses
7712Ã—5360 pixels in a WindowsPhone 8 smartphone camera
as the receiver. The calculation requires a cloud server for
high-quality image processing. PIXEL is a polarization-based
localization method [8]. Only 120 Ã— 160 pixels are required.
It can be measured in several seconds with an accuracy of
0.4 m. However, a polarizing ï¬lter must be attached to the
camera, so there is a risk of impairing the original image.
Rajagopalâ€™s approach uses light reï¬‚ected by the ï¬‚oor [5]. It
is similar to our idea, but they focus on the rolling shutter
distortion to receive an Identiï¬er (ID) from the reï¬‚ected light.
Carriers up to 8 kHz can be received with a channel separation
of 200 Hz. Tag information is transmitted by assigning ON and
OFF bits to different frequencies. The data rate is 10 bps, and
up to 29 light sources can be uniquely separated. Positioning
accuracy was not discussed in the paper because the research
aimed for the semantic positioning from differences in Packet
Reception Rates (PRR). Further, because MATLAB was used
for calculation, processing was not in real time. Nakazawaâ€™s
method uses a dual-facing camera and calculates its own
position by the relationship between ceiling light and reï¬‚ected
light on the ï¬‚oor. Large coverage and high accuracy are
achieved using only two LEDs. This method also requires
a large image so average processing time is 1.2 sec [9]. A
comparison of conventional methods is summarized in Table I.
Distortion means dose these methods require the rolling shutter
distortion or not. Extra indicates an additional device for VLP.
III.
SYSTEM DESCRIPTION
An overview of our system is shown in Figure 1. Our
method is based on the following key approaches. RSS from
the LED will decrease with the distance from the LED to
the ï¬‚oor. More than two LEDs are mounted on the ceiling,
which is parallel to the Xâ€“Y plane. The kth LEDâ€™s known 3-D
coordinate is (xLk, yLk, zh). Each LED broadcasts a sinusoid
wave with its own unique frequency. Modulated light from the
LEDs is reï¬‚ected by the ï¬‚oor, which is parallel to the Xâ€“Y
plane where Z = 0. The camera of the mobile device captures
any part of the ï¬‚oor PFi, then estimates its own position and
attitude by means of three steps.
First, each distance di,k =
p
(xLk âˆ’ xFi)2 + (yLk âˆ’ yFi)2
is estimated. Second, the positions of several points PFi on
the ï¬‚oor captured by the camera are estimated using di,k.
Finally, the camera position (x, y, z) and attitude (Î¸x, Î¸y, Î¸z)
are estimated by optical AoA, using each PFi. The deï¬nition
of attitude (Î¸x, Î¸y, Î¸z) for a smartphone is shown in Figure
2a. The head of the smartphone points in the Yc direction and
the camera faces the ï¬‚oor at its initial status (Î¸x, Î¸y, Î¸z) =
(0, 0, 0). Also, we deï¬ne 2-D coordinates in the image as
shown in Figure 2b. The following sections provide details
of the three steps.
A. Distance between the light source and the photographed
ï¬‚oor
The ï¬rst step is to ï¬nd the distance di,k from the inter-
section Pk of the perpendicular passing through the kth light
source and the ï¬‚oor to the point PFi captured by the camera.
The camera detects the intensity of the received signal from the
58
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-811-2
UBICOMM 2020 : The Fourteenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

Phone
Recorded
Image
LED 1 (ğ‘¥!!, ğ‘¦!!, ğ‘§")
LED 2 (ğ‘¥!", ğ‘¦!", ğ‘§")
ğ‘ƒ!!(ğ‘¥!!, ğ‘¦!", 0)
ğ‘ƒ!"(ğ‘¥!", ğ‘¦!#, 0)
ğ‘ƒ!#(ğ‘¥!#, ğ‘¦!$, 0)
ğ‘ƒ!$(ğ‘¥!$, ğ‘¦!%, 0)
LED 0 (ğ‘¥!#, ğ‘¦!#, ğ‘§")
(ğ‘¥, ğ‘¦, ğ‘§)
ğ‘
ğ‘‹
ğ‘Œ
Figure 1. System overview.
(ğ‘¢, ğ‘£)
ğ‘!
ğ‘‹!
ğ‘Œ!
ğ‘‰
ğ‘ˆ
ğ‘‚
ğœƒ": Yaw
ğœƒ#: Roll
ğœƒ$: Pitch
(a) Deï¬nition of attitude for a smartphone.
(ğ‘¢, ğ‘£)
+ğ‘§: Up
+ğ‘¥: Right
+ğ‘¦: Forward
ğ‘‰
ğ‘ˆ
ğ‘‚
ğœƒ!: Yaw
ğœƒ": Roll
ğœƒ#: Pitch
(b) Deï¬nition of 2-D
coordinates in the image.
Figure 2. Deï¬nitions of attitude and 2-D coordinates for mobile positioning.
photographed image and substitutes it into the diffusion model
of the LED reï¬‚ected light. To reduce the inï¬‚uence of other
light sources, the LED blinks a sinusoidal wave orthogonal
in frequency to those of other light sources. We deï¬ne bk(t),
which includes a signal from the kth LED at time t, as follows:
bk(t) = sin(2Ï€t(Akfs + mk)) + Î±,
(1)
where fs is the basic signal frequency and Ak is a natural
number so that the signal gets to a high enough frequency
not to cause ï¬‚ickering; Î± is the direct current component,
which makes bk(t) always a positive value; and mk is a natural
number that uniquely identiï¬es the frequency for each LED.
Assuming that the camera frame rate is fc = fs, the
shutter cycle is Tc = 1/fc, the exposure time ratio is Î·,
and the exposure time is Î·Tc. By taking N images with
the camera and separating the received light in the frequency
domain, it is possible to extract the signal intensities of the
unique frequencies. Therefore, the number of detectable LEDs
is N/2 âˆ’ 1, and mk < N/2 should be satisï¬ed as per the
sampling theorem. By capturing bk(t) from the kth LED with
a camera, the resulting brightness Ii,n of the PFi on the nth
image is an integral:
Ii,n =
X
k
2Ï€X(di,k)
Tc
Z Î·Tc
0
bk(t + Î´Tc + nTc)dt.
(2)
where Î´ is the delay of the shutter timing with respect to
the signal, and X(di,k) is the attenuation function that is
determined by the distance and transfer efï¬ciency from the
kth LED as a transmitter to the receiver. Hence, our purpose
is to calculate di,k from the inverse function of X(di,k) using
Ii,n.
Now, Bi,l is obtained by the Fourier transform of the video
stream Ii = (Ii,0, Ii,1, ..., Ii,Nâˆ’1):
Bi,l = 1
N
Nâˆ’1
X
n=0
Ii,ne
âˆ’j2Ï€nl
N
.
(3)
By assuming Î²(mk,k) obtained by Fourier transform of
bk(t), Bi,l is shown as follows [18]:
Bi,mk = Î·X(di,k)ejÏ€fk(2Î´+Î·)sinc (Ï€fkÎ·) Î²(mk,k)
(4)
where fk = Akfs + mk. Thus, the unique frequency mk can
be extracted. Note that the amplitude spectrum |Î²(mk,k)| is
affected by Î· and sinc. Attenuation of magnitude affects the
accuracy of positioning, so Î· must be set uniquely. Note that
X(di,k) is the product of attenuation of the LEDâ€™s signal and
transfer efï¬ciency. We assume the attenuation of the LEDâ€™s
signal is inversely proportional to the square of the distance
and attenuates by the cosine of the radiation angle Î¸. However,
it is difï¬cult to model the attenuations theoretically because
reï¬‚ecting properties are very complicated in the real environ-
ment [19]. Our previous work showed attenuation on the ï¬‚oor
can be approximated by a hyperbolic secant distribution [20].
Therefore,
X(di,k) =
Ck
e
Ï€
2 Ïƒdi,k + eâˆ’ Ï€
2 Ïƒdi,k ,
(5)
where Ïƒ is the radiation characteristic of the LED, and Ck
is the transmission efï¬ciency determined by the receiver sen-
sitivity. Hence, the amplitude spectrum |Bi,mk| is shown as
follows:
|Bi,mk| = Î·X(di,k)sinc (Ï€fkÎ·) |Î²(mk,k)|.
(6)
Thus, di,k can be calculated as
di,k = 1
Ïƒ coshâˆ’1(
Câ€²
k
|Bi,mk|)
(7)
where Câ€²
k = Î·Cksinc (Ï€fkÎ·) |Î²(mk,k)|.
B. Position on the ï¬‚oor recorded by a camera
The second step is to obtain PFi-captured positions on
the ï¬‚oor, using three or more di,k estimated by the previous
step. Now, we assume that three perpendiculars from the
light sources pass through the ï¬‚oor at the points (xL0, yL0),
(xL1, yL1), and (xL2, yL2) (see Figure 1). The estimated
distances of these intersections to the point (xFi, yFi) are di,0,
di,1, and di,2, respectively:
ï£±
ï£²
ï£³
p
(xFi âˆ’ xL0)2 + (yFi âˆ’ yL0)2 = di,0
p
(xFi âˆ’ xL1)2 + (yFi âˆ’ yL1)2 = di,1
p
(xFi âˆ’ xL2)2 + (yFi âˆ’ yL2)2 = di,2
(8)
By solving this, (xFi, yFi) can be calculated. When there are
more than three LEDs installed in the building, it can be solved
as an optimization problem:
min
X
k
(
p
(xFi âˆ’ xLk)2 + (yFi âˆ’ yLk)2 âˆ’ di,k)2.
(9)
59
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-811-2
UBICOMM 2020 : The Fourteenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

Function 
generator
Power
supply
Camera:
iPhone 7
Height:
1.15 m
LED:
BXRE-50C4001-B-74 
Height:
2.6 m
LED:
BXRE-50C4001-B-74 
LED:
BXRE-50C4001-B-74 
LED:
BXRE-50C4001-B-74 
Figure 3. Positioning of the LEDs in the room and the camera settings.
C. Position and attitude of the camera
The last step is to estimate the position and attitude of the
camera. These are calculated using the optical AoA method
with the traditional camera matrix
sp = M[R t]P,
(10)
where s means the scale coefï¬cient, M denotes the intrinsic
properties, and [R t] represents the extrinsic properties [21].
p is the 2-D image coordinates, and P deï¬nes the 3-D world
coordinates as follows:
p =
" uI0
...
uIi
...
vI0
...
vIi
...
1
...
1
...
#
(11)
P =
ï£®
ï£¯ï£°
xF0
...
xFi
...
yF0
...
yFi
...
0
...
0
...
1
...
1
...
ï£¹
ï£ºï£»
(12)
where the 3-D world coordinates (xFi, yFi) are captured by
a camera as 2-D image coordinates (uI0, vI0) on the image.
The camera position and rotation matrix [R t] is obtained by
minimization of ||A[R| t]P âˆ’ sp||2.
IV.
IMPLEMENTATION DETAILS
The prototype of RefRec using LEDs and a smartphone
was implemented in our laboratory as shown in Figure 3. A
ï¬‚oor made from a patternless nonglow mat was chosen because
our prior work has shown that ï¬‚oor material might affect the
result adversely [20].
A. LED transmitter
BXRE-50C4001-B-74-type LEDs from Bridgelux were
used as the LED transmitter. Our proposal assumes that the
light source is not an area or a line source, e.g., a ï¬‚at panel or a
bar light. Extending the experiments to including these sources
remains to be done in future work. Four LEDs above the room
were set as shown in Figure 3. The height of the ceiling is 2.6




	









 	 (
)
 	
  (	)
Figure 4. Absolute mean error of estimated distance for each ISO and
shutter speed setting.
m, coverage is 4.0 m square, LEDs 0, 1, 2, and 3 were set at
(1.0, 0.5, 2.6), (1.0, 3.0, 2.6), (3.2, 0.5, 2.6), and (3.2, 3.0, 2.6)
m. This setting was the same as the original built-in formation
of ï¬‚uorescent lights in this room. No other objects were placed
in the room to evaluate the performance properly in an ideal
environment. The signal parameters were set at fs = 50,
Î± = 1, Ak = 2 (at any k), and mk = (1, 6, 13, 20). Hence, the
LEDs were modulated at 101, 106, 113, and 120 Hz. These
frequencies are higher than the 100 Hz modulation frequency
for ï¬‚uorescent lights in east Japan to ensure people do not
experience any ï¬‚ickering. The transmitter represented any
signal by the Pulse Density Modulation (PDM). A 5 V pulse
signal from a function generator (NF Corporation WF-1948)
was ampliï¬ed to 34 V using a Metal-Oxide-Semiconductor
Field-Effect Transistor (MOS-FET) K703 and a power supply.
The frequency of the pulse signal was about 8 MHz. This
was a prototype and the transmitter can be made cheaper and
smaller by using a circuit similar to a dimmable off-the-shelf
LED in our future experiments.
B. Camera receiver
An iPhone 7 was used as a receiver. Smartphones in recent
years have generally higher-performance chipsets and cameras
than the iPhone 7. Therefore, we believe that the experiments
in this paper can be reproduced on other smartphones that
users have. The frame rate fc = N = 50 was set to avoid
effects from other ï¬‚uorescent lights. In east Japan, ï¬‚uorescent
lights are modulated by AC 100 Hz, so 50 fps is the orthogonal
frequency, which will treat other ï¬‚uorescent lights just as DC
sources (the same as sunshine). The F value is ï¬xed on f/1.8 in
the case of the iPhone 7. The shutter speed and ISO sensitivity
should be arbitrarily set so that the pixels are not saturated.
The focus was ï¬xed on the ï¬‚oor. Distortion of the image was
calibrated using Zhangâ€™s method [22].
To reveal the relationship between the camera parameter
and positioning accuracy, for each ISO and shutter speed
setting, each distance was estimated by the following equation:
d0 =
p
(xL0 âˆ’ xF )2 + (yL0 âˆ’ yF )2.
(13)
Note that (xF , yF ) is the ï¬‚oor 2-D coordinates PF captured
by the principal point on the image. The iPhone 7 was ï¬xed
horizontally to a tripod 1.15 m above the ï¬‚oor and moved from
d0 = 0 m to d0 = 3.5 m in 0.5 m increments. Each distance
was estimated 100 times and absolute mean errors are shown in
Figure 4. When the ISO value was set too low, the distance d0
could not be estimated correctly. When the ISO value was set
60
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-811-2
UBICOMM 2020 : The Fourteenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

over 300, estimates at the centimeter level were achieved. We
also found that shutter speeds should be shortened. However,
shutter speeds that are too short make estimation difï¬cult
because the images become too dark. Hence, the settings of the
camera are suggested that the ISO value is set at over 300, and
the shutter speed is set at less than 1/300 sec. We subsequently
set the shutter speed at 1/500 sec. and the ISO value at 500.
V.
EVALUATION
In order to clarify the advantages and limitations of our
proposal, the performance of RefRec was evaluated in our
experiment.
A. Estimation of captured ï¬‚oor PF positions
First, one PF estimated by equation (9). The iPhone 7
does not support small resolution read-outs, so the resolution
is set at 960Ã—540 pixels, and used only 32Ã—32 pixels around
the principal point. Our previous work revealed this resolution
to be smaller than that used in conventional methods and it
enables real-time performance, while sufï¬cient accuracy could
be achieved for many applications [20]. The arrangement of
the PF s are shown in Figure 5a as ï¬lled circles. The triangles
indicate the LED positions on the ceiling. Since the parameters
of equation (7) are affected by the individual differences of
smartphone cameras and LEDs, calibration is performed before
evaluation. Speciï¬cally, a camera was set with a tripod below
LED 0 to receive a signal, and then updated the constant Câ€²
k
in equation (7) with d0 = 0. The position of each PF was
estimated 100 times. All measurements were performed in
real time on the smartphone. The mean positions of the PF s
estimated by the camera are shown as non-ï¬lled circles in
Figure 5a and the errors from the true PF positions are shown
as arrows. The error became larger outside the rectangular area
with the coordinates of the four LEDs as vertices. Therefore, in
order to measure a wider area, it is necessary to arrange more
LEDs to increase the rectangular area. Since the LED beacons
were set as square, the error should ideally be point symmetric
with (x, y) = (2.1, 1.75) as the origin coordinates. However,
Figure 5a did not show symmetry because each LED has a
slightly different feature. The cumulative distribution function
of the estimated absolute errors are shown in Figure 5b. An
estimation error of less than 0.42 m at the 90th percentile was
achieved.
B. Estimation of mobile device attitude using captured ï¬‚oor
PF positions
Next, nine PFi (0 â‰¤ i < 9) were estimated by one image
stream. A resolution of 1920Ã—1080 pixels was set. The princi-
pal point was (uc, vc) = (524.86, 959.07). As Figure 6a shows,
the 2-D image coordinates in p are (uIj, vIk) = (300j +200âˆ’
uc, 300k + 200 âˆ’ vc) (0 â‰¤ i < 3, 0 â‰¤ j < 3). A camera was
set in the room with parameters (x, y, z) = (1.0, 0.5, 1.15) and
(Î¸x, Î¸y, Î¸z) = (0, 0, Ï€/4). Each PF position was estimated 100
times in real time and the positions are shown in Figure 6b. The
accuracy of the estimated azimuth Î¸z for every combination
of PFi was evaluated. The total number of PFis is nine, so the
total number of combinations of choosing more than one point
from these is 502. This evaluation was processed off-line. The
estimated absolute median error at each total number of PFis
used to calculate Î¸z is shown in Figure 7a. When the number of
PFis used is two, the estimated errors are very different from
LED 2
LED 3
LED 1
LED 0










 ()
 ()
 	


 	

(a) LED arrangement, PF measurement points, and
estimated positions.











 			 (
)

(b) CDF of absolute errors of estimated PF positions.
Figure 5. LED arrangement and estimated errors of PF positions.
which the PFis are selected. The best 90th percentile absolute
error was 6.15â—¦, with the worst 90th percentile absolute error
at 27.09â—¦. When the baseline length was long (e.g., (uI0,
vI0) and (uI2, vI2)), the accuracy improved. On the contrary,
the results for the cases with short baseline lengths (e.g.,
(uI1, vI2) and (uI2, vI2)) were inaccurate. As the number of
PFis increased, the difference between combinations (standard
deviation) decreased. When the number of PFis used was
eight, the estimated errors were similar, with the best 90th
percentile absolute error of 3.25â—¦, and the worst 90th percentile
absolute error of 5.77â—¦. In the next section, we discuss the use
of all the PFi-captured positions.
The six-degrees-of-freedom mobile device attitude were
estimated using nine obtained PFis. The results are shown in
Figure 7. The estimated 3-D coordinate positions of the smart-
phone (x, y, z) are shown in Figure 7b. The 90th percentile
absolute errors of the x, y, and z coordinates are 0.2073 m,
0.1713 m, and 0.002464 m. Thus, an absolute error of less
than 0.27 m in 3-D localization was achieved. x and y are
more sensitive than z. The estimated mobile attitude (Î¸x, Î¸y)
is shown in Figure 7c. The 90th-percentile attitude errors were
less than 2.47â—¦ and 4.93â—¦ for the pitch and roll angles. The
cumulative distribution function for the azimuth Z in Figure
7d shows a 90th percentile absolute error is less than 3.45â—¦.
The pitch and roll of the smartphone can also be obtained
precisely using the Inertial Measurement Unit (IMU) [23].
However, it is difï¬cult to estimate the azimuth using IMU
because the magnetic ï¬eld is unstable in the room. Our results
show RefRec has potential for more mobile applications.
C. Angle precision of different attitude of the smartphone
To investigate the accuracy limitations of the smartphoneâ€™s
angle estimation, orientation errors from PFi were evaluated
for each attitude of the smartphone. The pitch, roll, and yaw
of the smartphone were changed by 10 degrees each, and its
61
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-811-2
UBICOMM 2020 : The Fourteenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

(a) Positions of p and the image captured
by the smartphone.








	


 ()
 ()
(b) Estimated positions of nine PFis
picked up to calculate.
Figure 6. Arrangement of estimated PFis and how many PFis affect
positioning error.
Ã—
Ã—
Ã—
Ã—
Ã—
Ã—
Ã—









	
	

 	
  
 
  (	)
(a) Azimuth Î¸z at each total number of
points PFi used.












 			 (
)

ğ‘¥
ğ‘¦
ğ‘§
(b) 3-D coordinates of mobile position x,
y, and z.












 			 (Â°)

ğœƒ!
ğœƒ"
(c) Pitch Î¸x and roll Î¸y.












 			 (
	)

-
-
-








   	
 (Â°)












 			 (
	)

(d) Azimuth (yaw) Î¸z.
Figure 7. CDF of estimated absolute errors about six degrees-of-freedom
using nine PFis.
attitude was estimated 100 times. Considering the use case of
holding a smartphone, the roll, pitch, and yaw were limited to
âˆ’30â—¦ to 30â—¦, 0â—¦ to 30â—¦, and 0â—¦ to 90â—¦. Each 90th percentile
absolute rotation error at each posture of the smartphone is
shown in Figure 8. Means of 90th percentile absolute rotation
error were 5.69â—¦, 5.78â—¦, and 3.96â—¦ for the roll, pitch, and yaw.
VI.
DISCUSSION
In this section, we discuss the limitations of our proposal
and potential remaining future work.
A. Comparison of performance
A comparison of the performance of positioning is pre-
sented in Table I. Please note that each experimental en-
vironment differs in terms of LED installation spacing and
measurement area size. Performance comparisons in the same
condition are difï¬cult to make because these differences have a
signiï¬cant impact on accuracy. For example, the best-accuracy
Luxapose in Table I is difï¬cult to localize in our experimental
environment where the LED spacing is too wide to record
multiple LEDs directly using smartphone camera.
-
-
-








   	
 (Â°)












   	
 (Â°)















   	
 (Â°)














   	
 (
	




	




-
-
-








   	
 (Â°)
 	
  (Â°)
Figure 8. 90th percentile absolute rotation error at each posture of the
smartphone.
B. Materials of transmitter and receiver
Changing the materials of the ï¬‚oor and the LEDs as
transmitters may cause errors. In particular, LEDs with a
Lambertian emission radiation characteristic are better. Our
previous research showed that the difference of reï¬‚ections
resulting from different ï¬‚oor materials causes errors [20].
Future work must include investigating ï¬‚oors that consist of
glowing materials or contain some patterns.
C. Attitude of smartphone
The attitude of the smartphone, especially its pitch and
roll (tilt) attitude, may cause errors. We suggest that the
smartphone should be held horizontally. We are of the opinion
that this is easier to achieve than the existing method, which
requires moving the mobile to seek the LEDs directly.
D. Power consumption
The camera consumes power using our positioning method.
Approximately 300 mW will be consumed using an off-
the-shelf camera [24]. If the user wants to navigate, it is
necessary to continue activating the camera, so it consumes
a considerable amount of power. However, because only a few
pixels are required by our method, the power requirement can
be reduced if the image sensor can activate only the necessary
elements.
E. Solution in the real environment
In our experiments, nothing was placed in the measurement
space in order to minimize signal changes. However, in a
real environment there are many obstacles, such as objects
or humans. The shadow of so-called ambient occlusion might
be harmful to positioning. In the future, we will propose how
to select an area that does not include shadows. We will also
investigate inï¬‚uences and conduct experiments in larger spaces
using, for example, 16 LEDs in a 10 m square room.
VII.
CONCLUSION AND FUTURE WORK
Visible light positioning for smartphones is regarded as
a promising technique that is expanding the market in many
industries. This paper describes an approach to avoid existing
limitations, such as LOS, by using a camera recording light
reï¬‚ected by the ï¬‚oor. Our prototype showed 90th percentile 3-
D localization and attitude estimation errors that were within
62
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-811-2
UBICOMM 2020 : The Fourteenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

0.27 m and 5.78â—¦. Our proposal has larger coverage and a
smaller image requirement than conventional techniques. We
also mention that some conditions may affect the positioning
accuracy, such as the ï¬‚oor materials, radiation characteristics
of the LED, and tilt of the smartphone, amongst others.
A combination of our proposal and conventional techniques
should reduce the limitations and improve accuracy. Future
work will explore cases where, for example, more people
hold their smartphones and move around, objects are placed
on the ï¬‚oor to cause occlusion, more LEDs are used, and
experimenting is done in a larger area.
ACKNOWLEDGMENT
This research was supported by Global Station for Big Data
and CyberSecurity, a project of Global Institution for Collab-
orative Research and Education at Hokkaido University, JSPS
KAKENHI Grant Number 19H04222, and Tateisi Science and
Technology Foundation.
REFERENCES
[1]
P. Lanjudkar, â€œIndoor Positioning and Indoor Navigation (IPIN)
Market,â€
https://www.alliedmarketresearch.com/indoor-positioning-
and-indoor-navigation-ipin-market, 2018, [retrieved: Sep,2020].
[2]
P. Davidson and R. PichÂ´e, â€œA survey of selected indoor positioning
methods for smartphones,â€ IEEE Communications Surveys & Tutorials,
vol. 19, no. 2, 2016, pp. 1347â€“1370.
[3]
S. D. Lausnay, L. D. Strycker, J. P. Goemaere, B. Nauwelaers, and
N. Stevens, â€œA survey on multiple access visible light positioning,â€ in
Proc. 2016 IEEE Int. Conf. on Emerging Technologies and Innovative
Business Practices for the Transformation of Societies, Port Louis,
Mauritius, Aug 2016, pp. 38â€“42.
[4]
Y.-S. Kuo, P. Pannuto, K.-J. Hsiao, and P. Dutta, â€œLuxapose: Indoor
positioning with mobile phones and visible light,â€ in Proc. 20th annual
Int. Conf. Mobile Computing and Networking, Maui, Hawaii, 2014, pp.
447â€“458.
[5]
N. Rajagopal, P. Lazik, and A. Rowe, â€œVisual light landmarks for mobile
devices,â€ in Proc. 13th Int. Symp. Information Processing in Sensor
Networks, Berlin, Germany, 2014, pp. 249â€“260.
[6]
Sony Semiconductor Solutions Corporation, â€œSony Develops the Indus-
tryâ€™s First*1 3-Layer Stacked CMOS Image Sensor with DRAM for
Smartphones,â€
https://www.sony.net/SonyInfo/News/Press/201702/17-
013E/, [retrieved: Sep,2020].
[7]
L. Li, P. Hu, C. Peng, G. Shen, and F. Zhao, â€œEpsilon: A visible light
based positioning system.â€ in Proc. 11th USENIX Symp. Networked
Systems Design and Implementation, vol. 14, Seattle, WA, United
States, 2014, pp. 331â€“343.
[8]
Z. Yang, Z. Wang, J. Zhang, C. Huang, and Q. Zhang, â€œWearables
can afford: Light-weight indoor positioning with visible light,â€ in Proc.
13th Annual Int. Conf. Mobile Systems, Applications, and Services,
New York, NY, United States, 2015, pp. 317â€“330.
[9]
Y. Nakazawa et al., â€œPrecise indoor localization method using dual-
facing cameras on a smart device via visible light communication,â€
IEICE Trans. Fundamentals of Electronics, Communications and Com-
puter Sciences, vol. E100.A, no. 11, 2017, pp. 2295â€“2303.
[10]
J. Armstrong and A. Lowery, â€œPower efï¬cient optical ofdm,â€ Electronics
letters, vol. 42, no. 6, 2006, pp. 370â€“372.
[11]
T. Q. Wang, Y. A. Sekercioglu, A. Neild, and J. Armstrong, â€œPo-
sition accuracy of time-of-arrival based ranging using visible light
with application in indoor localization systems,â€ Journal of Lightwave
Technology, vol. 31, no. 20, 2013, pp. 3302â€“3308.
[12]
S.-Y. Jung, S. Hann, and C.-S. Park, â€œTdoa-based optical wireless
indoor localization using led ceiling lamps,â€ IEEE Trans. Consumer
Electronics, vol. 57, no. 4, 2011.
[13]
S.-H. Yang, H.-S. Kim, Y.-H. Son, and S.-K. Han, â€œThree-dimensional
visible light indoor localization using aoa and rss with multiple optical
receivers,â€ Journal of Lightwave Technology, vol. 32, no. 14, 2014, pp.
2480â€“2485.
[14]
H. Steendam, T. Q. Wang, and J. Armstrong, â€œTheoretical lower
bound for indoor visible light positioning using received signal strength
measurements and an aperture-based receiver,â€ Journal of Lightwave
Technology, vol. 35, no. 2, Jan 2017, pp. 309â€“319.
[15]
L. Yang, Z. Wang, W. Wang, and Q. Zhang, â€œNaloc: Nonlinear ambient-
light-sensor-based localization system,â€ Proc. ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies, vol. 2, no. 4, 2018,
pp. 1â€“22.
[16]
M. S. Rahman, M. M. Haque, and K.-D. Kim, â€œIndoor positioning
by led visible light communication and image sensors,â€ International
Journal of Electrical and Computer Engineering, vol. 1, no. 2, 2011, p.
161.
[17]
M. Yoshino, S. Haruyama, and M. Nakagawa, â€œHigh-accuracy position-
ing system using visible led lights and image sensor,â€ in Proc. Radio and
Wireless Symposium, Orlando, FL, United States, 2008, pp. 439â€“442.
[18]
S. Shimada, T. Akiyama, H. Hashizume, and M. Sugimoto, â€œOfdm
visible light communication using off-the-shelf video camera,â€ in Proc.
15th ACM Conf. Embedded Network Sensor Systems, Delft, Nether-
lands, 2017, p. 57.
[19]
H. Zhang and F. Yang, â€œPush the limit of light-to-camera communica-
tion,â€ IEEE Access, vol. 8, 2020, pp. 55 969â€“55 979.
[20]
S. Shimada, H. Hashizume, and M. Sugimoto, â€œIndoor positioning using
reï¬‚ected light and a video camera,â€ in Proc. 9th Int. Conf. Indoor
Positioning and Indoor Navigation, Nantes, France, 2018, pp. 1â€“8.
[21]
G. Bradski and A. Kaehler, Learning OpenCV: Computer vision with
the OpenCV library.
â€ Oâ€™Reilly Media, Inc.â€, 2008.
[22]
Z. Zhang, â€œA ï¬‚exible new technique for camera calibration,â€ IEEE
Trans. pattern analysis and machine intelligence, vol. 22, no. 11, 2000,
pp. 1330â€“1334.
[23]
P. Zhou, M. Li, and G. Shen, â€œUse it free: Instantly knowing your
phone attitude,â€ in Proc. 20th annual Int. Conf. Mobile Computing and
Networking, Maui, Hawaii, 2014, pp. 605â€“616.
[24]
R. LiKamWa, B. Priyantha, M. Philipose, L. Zhong, and P. Bahl,
â€œEnergy characterization and optimization of image sensing toward
continuous mobile vision,â€ in Proc. 11th annual Int. Conf. Mobile
Systems, Applications, and Services, Taipei, Taiwan, 2013, pp. 69â€“82.
63
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-811-2
UBICOMM 2020 : The Fourteenth International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies

