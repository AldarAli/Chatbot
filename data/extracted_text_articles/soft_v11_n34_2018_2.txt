214
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Revision Control and Automatic Documentation
for the Development Numerical Models for Scientiﬁc Applications
Martin Zinner∗, Karsten Rink†, René Jäkel∗, Kim Feldhoff∗, Richard Grunzke∗,
Thomas Fischer†, Rui Song‡, Marc Walther†§, Thomas Jejkal¶, Olaf Kolditz†∥, Wolfgang E. Nagel∗
∗ Center for Information Services and High Performance Computing (ZIH)
Technische Universität Dresden
Dresden, Germany
E-mail: {martin.zinner1, rene.jaekel, kim.feldhoff}@tu-dresden.de,
{richard.grunzke, wolfgang.nagel}@tu-dresden.de
† Department of Environmental Informatics
Helmholtz Centre for Environmental Research (UFZ)
Leipzig, Germany
E-mail: {karsten.rink, thomas.fischer, marc.walther, olaf.kolditz}@ufz.de
‡ Technical Information Systems
Technische Universität Dresden
Dresden, Germany
E-mail: rui.song@tu-dresden.de
§ Professorship of Contaminant Hydrology
Technische Universität Dresden
Dresden, Germany
E-mail: marc.walther@tu-dresden.de
¶ Institute for Data Processing and Electronics
Karlsruhe Institute of Technology (KIT)
Karlsruhe, Germany
E-mail: thomas.jejkal@kit.edu
∥ Professorship for Applied Environmental System Analysis
Technische Universität Dresden
Dresden, Germany
E-mail: olaf.kolditz@ufz.de
Abstract—As software becomes increasingly complex, automatic
documentation of the development is becoming ever more im-
portant. In this paper, we present a novel, general strategy to
build a revision control system for the development of numerical
models for scientiﬁc applications. We set up a formal methodology
of the strategy and show the consistency, correctness, and
usefulness of the presented strategy to automatically generate a
documentation for the evolution of the model. As a use case, the
proposed system is employed for managing the development of
hydrogeological models for simulating environmental phenomena
within a research environment.
Keywords–Software development; Automatic generation of doc-
umentation; Revision control; Backup and Restore; Metadata;
Improvement of research environment; Support of research process.
I.
INTRODUCTION
In scientiﬁc applications, dedicated software packages are
used to create numerical models for the simulation of physical
phenomena [1]. In the scope of this paper we will focus on
environmental phenomena, such as the simulation of ﬂooding,
groundwater recharge or reactive transport using innovative
numerical methods. Such simulations are crucial for solving
major challenges in coming years, including the prediction of
possible effects of climate change [2] [3], the development of
water management schemes for (semi) arid regions [4] [5] or
the reduction of groundwater contamination [6] [7].
The modeling process is usually a complete workﬂow,
consisting of a number of recurring steps. To better understand
the need for documentation and storing multiple versions of
the same model, we would like to roughly outline the process:
1)
Data acquisition: Relevant data sets required for setting
up a model and parameterizing a process simulation are
collected. For hydrogeological processes, this includes
the digital elevation model (DEM), stratigraphic infor-
mation from boreholes or other sources, production
rates from wells, precipitation rates from climate
stations, in- and outﬂow rates for the region of interest,
etc.

215
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 1. Example for the development of a numerical model over multiple iterations. All ﬁgures depict a numerical groundwater model for a region in the
Middle East [8]. Figure (a) and (e) show the initial model from January 2011. Figure (b) and (f) depict that state of the model in August 2011 when more
information on the surface had been added. Figure (c) and (g) show the complex state at December 2012 when also additional subsurface information had been
integrated. Finally, Figure (d) and (h) show the ﬁnal state of the model in April 2013 after a number of simpliﬁcations. Figure (a)–(d) show the top of region of
interest, while Figure (e)–(h) show an isometric view such that stratigraphic information is visible.
2)
Data integration: Information is usually collected from
different sources and data sets have been acquired using
various measurement devices (e.g., remote sensing data
from satellites, sensor data, manually created logs)
When aggregating the information, artifacts in data and
inconsistencies between data sets need to be removed.
This includes fairly simple tasks, such as projecting all
data sets into the same geographic coordinate system,
but also challenging work, such as dealing with missing
or conﬂicting data [9].
3)
Model Generation: Information from the input data sets
is used to create a numerical model. For instance, DEM
and borehole information are used to create a ﬁnite ele-
ment mesh of the subsurface region of interest, source-
and sink terms have to be integrated into the mesh,
precipitation and outﬂow information are used to create
boundary conditions, and mesh elements need to be
parameterized based geohydrological parameters [10].
4)
Process Simulation: Time stepping scheme, non-linear
solver type, pre-conditioner and linear solver for the
numerical schemes need to be selected and parameter-
ized. For HPC-Applications, a domain decomposition
needs to be performed for the the subsurface mesh. At
this point, one or multiple runs of the actual simulation
are done [11]
5)
Validation / Visualization: Simulation results are visual-
ized and checked for plausibility. Results are compared
to actual numbers, for example from observation
wells, outﬂow measurements or using other means of
validation [12].
In reality, the above workﬂow will not be executed as linear
as suggested above. Often, it is not obvious exactly, which
data sets are necessary to run a meaningful simulation. Precise
data sets are often hard to come by and need to be requested
from state ofﬁces or bought from commercial services. The
best practice is to set up one or more initial models using data
that is available at the time to get a ﬁrst prototype and see
potential problems during simulation or when comparing results
to actual validation data. In addition, researchers usually want
to create a model that is as complex as necessary but as simple
as possible. Complex models tend to be more precise but have
more degrees of freedom: boundary conditions and (coupled)
processes are hard to parameterize and numerically challenging,
the run time is usually (much) longer and problems occurring
due to the structure of the model are harder to track down.
In contrast, simple models might not be able to represent the
region of interest or the simulated process adequately and the
correctness or precision of simulation results may be insufﬁcient.
Unfortunately, the modeling process itself in general is not
transparent and traceable and often poorly documented. A
typical model – consisting of a set of parameter ﬁles – is
developed over many weeks or months (see Figure 1). Usually a
large number of revisions are necessary to update and reﬁne the
model, such that the simulation represents the natural process as
realistically and plausibly as possible. Examples for reasons to
adjust the ﬁnite element mesh representing a subsurface model
domain include changes to element size used to either allow
for an adequate representation of the processes of interest (e.g.,
groundwater ﬂow, heat conduction, dispersion of chemical
compounds), integration of additional datasets (e.g., river /
stream networks, wells, distribution of soil types) or availability
of more precise measurements for data already integrated,
re-meshing due to numerically difﬁcult conﬁgurations, or

216
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 2. Model development over several revisions
information reduction when the model has become too complex.
Other parts of the model, such as boundary conditions or the
numerical conﬁguration might also change for different reasons.
Especially with multiple researchers working on one model, it
is not hard to imagine that it can become difﬁcult to keep track
of changes and the reasons why certain aspects of a model
have been adjusted; especially when models are developed or
maintained over a period of multiple years. It can also become
difﬁcult to restore a certain previous state of a model after a
series of changes by one or more scientists over a given period
of time.
In this paper, we address these particular challenges.
Speciﬁcally, we present a revision control system, which in
addition to the backup/restore functionality tracks the changes in
each modeling step, thus generating an internal documentation
of the evolution of the model.
A. Technical Challenges and Objectives
The basic concept for the simulation of a certain phe-
nomenon in a given region is a model. As shown in the previous
section, this model is developed over several iterative steps.
In the scope of this paper, we will refer to these steps as
“revisions”, compare Figure 2. As mentioned, the ﬁrst setup of
the model is often used to get an overview over existing data
and to get familiar both with the region of interest as well as
the basic behavior of phenomena within that region. At this
stage, potential problems, missing data or speciﬁc numerical
requirements might already become apparent. After creating the
ﬁrst revision (as well as after each subsequent modeling step) a
simulation is run, using the model. Depending on the result of
the process simulation, further revisions will try to solve these
issues by addressing the shortcomings of the simulation result.
Typical follow-up steps include adding reﬁned or previously
missing data, adjusting or reﬁning the ﬁnite element mesh,
changing process parameterizations or numerical schemes, etc.
(see Figure 3).
The framework for revision control for environmental model
development is being implemented at the Helmholtz-Centre
for Environmental Research (UFZ) [13] using the Karlsruhe
Institute of Technology Data Manager (KIT DM) [14] as a
software framework for creating and maintaining repositories
for research data.
The
Metadata
Management
for
Applied
Sciences
(MASi) [15] research data management service is currently
being prepared for production at the Center for Information
Services
and
High
Performance
Computing
(ZIH)
at
Technische Universität Dresden. It utilizes the advanced
KIT DM framework to provide a service that enables the
metadata-driven management of data from arbitrary research
communities. This includes automating as many processes as
possible including metadata generation and data pre-processing.
The current solution, utilized at UFZ, is completely ﬁle
based and it is usually stored locally on the laptop of each
scientist.
Depending on the complexity of the model and the phe-
nomena that need to be simulated, the number of parameter
ﬁles varies between three and several hundred, with each ﬁle
up to several megabytes. The minimum conﬁguration for the
OpenGeoSys simulation software [11] requires a ﬁnite element
mesh, geometric information to specify spatial conditions, as
well as a project ﬁle containing all process-based information
and numerical parameterizations. However, for complex case
studies, additional ﬁles may become part of the model, for
instance to represent boundary conditions. Examples include
weather radar data (typically one ﬁle per timestep), data
from observation wells (typically one time series per well),
geometries of changing conditions in the model domain such
as advancing/receding coastlines during ﬂoods/droughts (one
ﬁle per timestep). The changes from one revision of the model
to the next can be very small, e.g., when one parameter value
changed in an input ﬁle. However, the changes can also become
major, for example when a geometric constraint is updated,
which in turn requires re-meshing the model domain and
adjusting associated boundary conditions and possibly even the
precise type of process that is used because the domain that

217
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 3. File-based view of revisions and simulation as part of the model development process. Here, a revisions is represented by a collection of input ﬁles.
Running a simulation will give a set of output ﬁles. If simulation results are not satisfactory given existing validation measurements or the researcher’s experience,
or if the simulation does not converge at all, changes to the model will be made based on the result. Examples include reﬁning the mesh (e.g., if the simulation
started to oscillate), adding data (such as a river geometry to use as an additional boundary condition), update cell properties (e.g., adjust permeability of
stratigraphic layers so groundwater ﬂow will behave differently) or adjust the parameterization of the numerical process (e.g., choose smaller time steps if the
simulation did not converge).
Figure 4. Example for changes of a ﬁnite element mesh: The left-hand side
depicts the homogenous mesh created for the ﬁrst iteration of the modeling
process, the right hand side the ﬁnal adaptive mesh, which is reﬁned towards
rivers and wells [8].
had previously been considered saturated is now unsaturated.
Figure 4 shows an example of changes made to a ﬁnite element
mesh during the development of a model.
The main deﬁciencies for researchers working with the
current ﬁle-based solution are:
1)
No overview of the development of the model (especially
after handling the model for a long time)
2)
Difﬁculty to trace parameter changes and the reasons
for those changes
3)
No implicit or explicit documentation of the changes
4)
Each user stores the data on his laptop at his own
discretion
5)
Data is lost if hard disk crashes and there is no backup
6)
Joint working on the same model is cumbersome
The beneﬁts of the new framework will include the advantages
of a classical revision control system (like Git [16], or Apache
Subversion [17]), in particular:
1)
Uniform, central, and consistent storage of the individual
modeling steps a) each scientist will be able to view the
simulation data he is entitled to b) backup functionality
if the data is lost,
2)
possibility to track and analyze / evaluate the changes,
3)
data is still available if the PhD student leaves the
company,
4)
shared access of the latest development of the model.
A revision is deﬁned as the state of the components already
persisted and accessible by a unique identiﬁer. Thus, the content
of the components of a revision cannot be altered any more. The
current set of the components, which can be actively changed
is called the working set.
The main objectives we focus on, to achieve our scope, are:
1)
Central persistent storage of the model to include all the
modeling steps and the management of the revisions.

218
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 5. Tree structure of the development of the model
2)
Design and development of a metadata repository
regarding a) revision control and b) the changes of the
parameter ﬁles between subsequent revisions. Addition-
ally, information regarding parameter values, simulation
software, etc. can be persisted.
3)
An efﬁcient and disk space saving strategy, such that a
speciﬁc parameter ﬁle is stored only if its content has
been modiﬁed.
4)
Generation of an internal documentation of the model
development, such that it can be easily understood and
reconstructed.
It is out of scope of this research to persistently store the
results of the simulation. If necessary, it can be generated
again or a direct storage strategy could be used. Storing the
results of the simulation together with the parameter ﬁles would
leverage our sophisticated storage strategy, since the size of
the parameter ﬁles is in the range of megabytes, where the
size of the simulation ﬁles is in the range of gigabytes. The
storage of the simulation results is only meaningful if it takes
too long to newly generate the results. The model development
is stored in a tree structure, such that each node (revision)
has a unique link to its predecessor, see Figure 5. The tree
structure is necessary to be able to identify modeling steps
where the results of the simulation are not promising, and thus
this revision is not pursued further (termed abandoned). In
this case, the development of the model is continued from a
previous revision (termed active), thus performing a rollback
on the evolution of the model and creating a new branch in
the version control tree structure.
Usually, metadata is deﬁned as data about data. Metadata
ﬁles can be generated automatically or they can be set up
manually. The ﬂow conﬁguration ﬁle is the main metadata ﬁle
and it is generated automatically during the evolution process
of the model and contains the basic information regarding the
revision control system, which is necessary to generate the
internal documentation of the evolution of the model, i.e.,
a)
model name;
b)
predecessors and the current revision;
c)
cryptographic hash value and status of the parameter
ﬁles;
d)
parameter change information in condensed form, etc.
The main metadata ﬁle sustains the possibility to automatically
capture, track, analyze, and evaluate the changes in each
modeling step.
Additionally, users can deﬁne their own metadata ﬁles,
which can be created for the whole case study or for a speciﬁc
revision and could contain additional information regarding
a) name of the project; b) model area; c) modeled process;
d) software used, including the version; e) contact person; f)
source reference regarding the applied methods and data used;
g) utilization rights, etc.
Besides documentation, metadata also allows for easy
identiﬁcation of the uploaded data. Search for speciﬁc values
(e.g., model name, author, etc.) over all metadata elements can
be performed for example by using ElasticSearch [18].
B. Outline
The structure of the paper is as follows: In Section II, we
give a short overview over the state of art and detail some
differences of our approach, both in concept and realization;
in Section III, we demonstrate our novel strategy, which is
used for the revision control system to generate the implicit
documentation of the evolution of the model; in Section IV,
we augment the classical pseudo code presentation of the
algorithms to a formal, mathematical description of our selective
backup strategy and show the consistency and correctness of
the backup and restore functionalities. We present the software
implementing the formal description and its application to a
use case of the UFZ in Section V. Finally, we conclude our
work and give an outlook for future research and development
in Section VI.
II.
RELATED WORK
The concept of revision control systems (RCS) is not new
(see Tichy [19]). The task of the RCS as deﬁned by Tichy is
version control, i.e., keeping software systems consisting of
many versions and conﬁgurations well organized. The concept
of a revision is similar to our approach, an ancestral tree is
used for storing revisions. The major difference is that – as
set up by Tichy – each object (like a ﬁle) has his own revision
tree, whereas we follow an overarching concept, such that ﬁles
may remain unchanged between revisions. Furthermore, the
evolvement of the revision is linear, but it can use side branches,
for example one for the productive version and one for the
development [19].
Löh et al. [20] present a formal model to reason about
version control, in particular modeling repositories as a multiset
of patches. Patches abstract over the data on which they operate,
making the framework equally suited for version control from
highly-structured XML to blobs of bits. The mathematical
deﬁnition of patches and repositories enable Löh et al. to
reason about complicated issues, such as conﬂicts and conﬂicts
resolution. The main application ﬁeld that Löh et al. targets
is the distributed (software) development with its challenges
regarding the complex operations on the repositories, such
as merging branches or resolving conﬂicts. They introduce a
precise, mathematical description of the version control system
to accurately predict when conﬂicts may arise and how they
may be resolved.
Our mathematical model is not based on the work of
Löh et al., it has been developed from scratch to enable the
characterization of the selective backup strategy.

219
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
The possibility to use metadata, such as the patch’s author,
time of creation, or some form of documentation is shortly
discussed in [20]. Details are left to the designers of a speciﬁc
revision control system. Also the concept of reverting changes,
i.e., the ability to return to a previous version by undoing a
modiﬁcation that later turns out to be undesired, is discussed
from a theoretical point of view.
As stated in [21] there are some basic goals of a versioning
system, such that:
1)
People are able to work simultaneously, not serially.
2)
When people are working at the same time, their
changes do not conﬂict with each other.
These two goals do not apply in our case. Formally, users can
work simultaneously, making changes independently, but for a
simulation they need all the parameter ﬁles. The classical use
case, such that a programmer changes the internal speciﬁcation
of a module without changing the external interface is not
applicable in our case, each change in a parameter ﬁle leads to
different simulation results. Unfortunately, the usual versioning
systems do not support our advanced requirements regarding
usage of metadata and enhanced automatic documentation
generation of the evolution of the model.
The automatic generation of documentation has also been
the scope of intense academic and industrial research. It has
been recognized that the importance of good documentation
is critical for user acceptance [22]. Jesus describes in [23] a
paradigm for automatic documentation generation based on
a set of rules that, applied to the models obtained as result
of the analysis and design phases, gives an hypertext network
describing those models. On the contrary, our approach has the
advantage that the algorithm that is used for the selective backup
strategy also delivers the data for the automatic documentation.
PLANDoc [24] documents the activity – of planning engineers
– by taking as input a trace of the engineer’s interaction
with a network planning tool. Similarly, in [25] Alida, an
approach for fully automatic documentation of data analysis
procedures, is presented. During analysis, all operations on data
are registered. Subsequently, these data are made explicit in
XML graph representation, yielding a suitable base for visual
and analytic inspection. The high level approach in Alida –
using the information generated during the production process
to automatically create the documentation – is similar to ours.
As a ﬁnal note, we looked very carefully at the existing
revision control systems, both at the academic and the com-
mercial ones, and found no adequate revision control system,
worth to adapt to fulﬁll our needs towards a system, which
automatically supports and tracks the evolution of the model
during its development process.
III.
SELECTIVE BACKUP STRATEGY
The aim of our revision control system is to provide an
enhanced backup strategy, termed selective backup strategy,
such that only the components of the working set that have
been modiﬁed are considered for backup, see Figure 6. This
is an enhancement of the usual incremental backup strategies,
storing a particular modiﬁcation of a ﬁle only once, in order to
provide the framework to generate the metadata regarding
the modiﬁcations and accordingly to generate the implicit
documentation of the model.
A correspondent selective restore strategy is used, i.e., the
latest versions of all components are downloaded, such that at
the end the recent version of the model is assembled out of
the historical backups.
A. Backup
Only part of the current working set is uploaded into the
data repository, and the uploaded information cannot be altered
or removed later. According to our selective backup strategy a)
for the ﬁrst revision: all components are uploaded (see Figure 6
left side); b) for the subsequent revisions: only the components
that have been modiﬁed are uploaded (see Figure 6 right side).
B. Restore
It will be possible to download all relevant information
regarding a speciﬁc revision (including parameter ﬁles and
metadata ﬁles). This requires identifying and downloading
the full set of components necessary to run a simulation.
The required information is stored in the main metadata ﬁle
during the backup process. Hence, the main metadata ﬁle
stores information regarding all ﬁles that have been uploaded
including the unique identiﬁcation of the uploaded object and
the cryptographic hash values of the respective ﬁles.
1) Full Restore: The full restore should be applied if ﬁles
have been lost, or the development of the model is intended
to be pursued by other users, etc. The full restore retrieves
the whole set of parameter ﬁles, such that a simulation can be
done on the restored system.
2) Revision Restore: This functionality restores the ﬁles
corresponding to a (previous) revision and permits to continue
the simulation corresponding from that revision. This method
enables the tree structure of the revision history. The corre-
sponding information is retrieved from/written to the main
metadata ﬁle.
C. Flow Conﬁguration
We present now some implementation details. The relevant
information for the functioning of the selective backup and the
corresponding restore strategy is stored / updated automatically
– using XML – in the ﬂow conﬁguration ﬁle.
This ﬁle stores general information as: a) short name of
the model; b) the number of the last revision; c) the object id
under which the ﬁles belonging to that revision were uploaded;
d) additional information in order to identify the project,
the revision, etc. A simple example is given in Figure 7.
Additionally, general information regarding the revision history
such as: a) the revision number; b) the object id of the uploaded
object; c) the predecessor (parent revision); the total number
of ﬁles versus the number of ﬁles, which have been changed
and in consequence uploaded, etc., as given in Figure 8. File
and revision speciﬁc information are also tracked, such that
for each ﬁle the revision where the ﬁle has been changed and
the corresponding cryptographic values are tracked. This way,
it is ensured that a speciﬁc state of a ﬁle is stored only once
and that the revision under which this ﬁle has been stored can
unambiguously be identiﬁed, see Figure 9 for an example.

220
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 6. Selective backup strategy. Uploaded ﬁles at ﬁrst (left) and second revision (right).
1 <GeneralConfiguration>
2
<ModelShortName>cube_1e0_neumann</ModelShortName>
3
<LastRevisionNr>25</LastRevisionNr>
4
<LastDigitalObjectID>3ccafed6-cf0d-486d-bbe3-
edff4159f6c5</LastDigitalObjectID>
5
<LastNotes>cube_1e0_neumann / Incr. / It.Nr.: 25 /
Standard Incremental Upload</LastNotes>
6 </GeneralConfiguration>
Figure 7. Excerpt 1 of example conﬁguration ﬁle
1 <Revision>
2 <RevisionNr>7</RevisionNr>
3 <DigitalObjectID>8ce20b42-c15f-427c-ac1a-d575295f5412</
DigitalObjectID>
4 <ParentRevisionNr>3</ParentRevisionNr>
5 <TotalNrFiles>20</TotalNrFiles>
6 <NrFilesUploaded>1</NrFilesUploaded>
7 <Notes>cube_1e0_neumann / Incr. / It.Nr.: 2 / For Testing
Upload and Download</Notes>
8 </Revision>
Figure 8. Excerpt 2 of example conﬁguration ﬁle
D. Accurate versioning
Based on the architecture of the system, the selective backup
strategy corresponds to a centralized revision control system,
i.e., there is a central revision number – in our case the modeling
step –, such that the version of each ﬁle is tied to this central
revision number. In contrast, revision control systems like
Git [16] are decentralized, i.e., generally, users maintain the
versioning of their part, without affecting the overall release
number. In our case, this centralized approach is of crucial
importance, since small changes in one parameter ﬁle can
substantially affect the outcome of the simulation. The selective
backup strategy enables a paradigm change in the theory and
practice of (centralized) revision control systems, it enables
1 <FileCharacteristics>
2
<FileName>cube_1x1x1.gml</FileName>
3
<FileLastRevision>43</FileLastRevision>
4
<FileHistory>
5
<FileRevision>
6
<StorageRevisionNr>1</StorageRevisionNr>
7
<StorageDigitalObjectID>8aac5970-a366-49ce-a052
-6c8f82ced85c</Storage\-Digital\-ObjectID>
8
<FileSize>1622</FileSize>
9
<FileCreationTime>2016-08-18T08:25:33Z</
FileCreationTime>
10
<FileLastAccessTime>2016-08-23T15:59:55Z</
FileLastAccessTime>
11
<FileLastModifiedTime>2016-08-18T08:25:33Z</
FileLastModifiedTime>
12
<FileCryptoIDMD5>66
a9800e8b02d85001cdd13930b85ea3</
FileCryptoIDMD5>
13
<FileCryptoIDSHA1>5
c942ba146b41e38262f23ed350e214c757d8803</
FileCrypto\-IDSHA1>
14
</FileRevision>
Figure 9. Excerpt 3 of example conﬁguration ﬁle
an accurate tracking of the changes during each revision on
ﬁle level including the identiﬁcation of the effective version
of each ﬁle. This means especially that in contrast to Git and
SVN [17], during revision change, each ﬁle is compared to
previous versions and either assigned a new version number or –
if possible – reassigned a previous one. Such a distinction is not
absolutely necessary, for example during software development
(main application ﬁeld for Git and SVN), but it is of crucial
importance for pursuing the exact model development.
We illustrate now the selective backup strategy by means
of the example as delineated in Table I. Let {F1,F2,F3,...,Fn}
be ﬁles comprising a numerical model and {R1,R2,R3,...,Rm}
the revisions to adjust the model for a successful simulation of
a process. According to the architecture we use, the number of
revisions is greater than the number of the ﬁles involved, i.e.,

221
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
n < m. We number the versions of a speciﬁc ﬁle continuously
in the order they were generated, starting with 1. We notate
by an upper index the revision during which the version was
generated, i.e., for the ﬁle F1 the version V R6
4
represents the
fourth version generated during revision R6. Not all ﬁles are
necessarily updated with a revision. For instance, ﬁle F1 is
unchanged during revisions R4, R5, thus the model keeps using
the ﬁle version V R3
3 . In contrast, ﬁle F2 is modiﬁed in each
revision. However, during revision R5, the ﬁle is reverted to a
previous state such that its version is equal to V R2
2
and, instead
of storing a duplicate, the previous copy of the ﬁle is used.
Using revision systems like Git or SVN, a new version of
the ﬁle F2 would be created. Instead, our algorithm, using the
selective backup strategy, veriﬁes if a version with new content
has been created or the respective content has already been
used before.
The use of the selective backup strategy is not restricted to
the model development for scientiﬁc application, but can be
applied everywhere where a centralized revision control system
is used. Its intended target are applications, which need to track
the effective version of the ﬁles, potentially related to revision
numbers.
IV.
THE FORMAL MODEL
We introduce a mathematical model in order to use the
advantages of the rigor of a formal approach over the inaccuracy
and the incompleteness of natural languages. We augment
the classical pseudo code presentation of the algorithms to a
formal, mathematical description and show the consistency and
correctness of the backup and restore functionalities.
A. Notations
We use a calligraphic font to denote the index sets. We
denote by C := {Ci | i ∈ C and Ci is a component} the ﬁnite
set of the components, i.e., the disjunct union of the parameter
ﬁles and the metadata ﬁles. Let S be an arbitrary set. We
notate by P(S) the power set of S, i.e., the set of all subsets
of S, including the empty set and S itself. By card(S) we
notate the cardinality of S. Let n ∈ N and let f : X → X be
a function. Finally, we denote by f n : X → X the function
obtained by composing f with itself n times, i.e., f 0 := idX
and f n+1 := f n ◦ f.
B. Introducing Components and Revisions
Some components – at least one, but not necessary all
– are modiﬁed, then a simulation is performed. We call this
state of the components a revision. Each revision is backed
up to a persistent storage. We have in a natural way a total
ordering < on the set of the revisions considering the order
they were generated. We denote by R the ordered set of the
revisions, i.e., R := {Ri | i ∈ R and Ri is a revision}. Let m :=
card(R) be the number of revisions. In order to keep the
notations straightforward we set R := {1,2,3,...,m}, such that
∀i ∈ R \ {m} : Ri < Ri+1. We denote by C(i)
k
the component
Ck having the state at revision Ri, therefore, we denote by
Ri := {C(i)
k | k ∈ C} the set of the components having the state
corresponding to revision Ri.
We denote by R the matrix of the evolution of the model,
hence R := {C(i)
k | k ∈ C and i ∈ R }. Therefore, R contains the
history of the content changes of the components during the
evolution process of the model.
Let HASH be the set of all the hash values. We deﬁne the
content of a component Ck ∈ C corresponding to a revision Ri
formally as the function:
Deﬁnition IV.1 (Content of components) We set
CONT: R → HASH,
C(i)
k 7→ CONT(C(i)
k ) := hash value of C(i)
k .
Remark IV.1 Let k ∈ C, let i, j ∈ R , such that i ̸= j. In order
to track the change process during the evolution process of the
model, we are interested only in comparing the content of the
same component at different revisions (i.e., the content of C(i)
k
versus the content of C(j)
k ).
■
Deﬁnition IV.2 (Origin of a revision) Let R,Q ∈ R. We say
that Q is the origin of R, notated by Q = ORIGIN(R), if and
only if the revision R has been obtained by direct modiﬁcation
of the content of the components having the state at revision
Q. For formal reasons we deﬁne ORIGIN(R1) := R1.
Remark IV.2 Let R ∈ R arbitrarily chosen. Then there exists
a unique Q ∈ R, such that Q = ORIGIN(R). This is a direct
consequence of the deﬁnition above (see Deﬁnition IV.2).
■
Let m = card(R), such that m ≥ 1. We deﬁne the predecessor
and the successor of a revision formally as:
Deﬁnition IV.3 (Predecessor of a revision) We set
PRED: R → R,
R 7→ PRED(R) := ORIGIN(R).
Deﬁnition IV.4 (Successor of a revision) We set
SUCC: R → P(R),
R 7→ SUCC(R) := {Q ∈ R | R = ORIGIN(Q)}.
Remark IV.3 ∀R ∈ R ⇒ PRED(R) is unequivocally deter-
mined (see Remark IV.2), in contrast, there exists to a revision
R ∈ R a subset J of R , such that SUCC(R) = {Rj | j ∈ J}. ■
Unfortunately, the structure of the evolution of the model is
not linear. If for any reason the evolution of the model is in
impasse, then the development of the model is not continued
from the latest revision, but a previous revision is taken as a
starting point. The revision, which led to the impasse is not
pursued any more (i.e., it is abandoned). On the other side,
a revision is active if it is part of the successful completion
of the model. Formally, we deﬁne the status of a revision as
follows:
Deﬁnition IV.5 (Status of a revision) Let m := card(R) the
number of revisions. We set
STATUS: R → {active,abandoned},
R 7→ STATUS(R) :=



active
if ∃n ≥ 0 :
R = PREDn(Rm),
abandoned
otherwise.

222
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Table I. Example for the selective backup strategy.
R1
R2
R3
R4
R5
R6
...
Rm
F1
V R1
1
V R2
2
V R3
3
V R3
3
V R3
3
V R6
4
...
V
Rmk1
m1
F2
V R1
1
V R2
2
V R3
3
V R4
4
V R2
2
V R6
5
...
V
Rmk2
m2
F3
V R1
1
V R1
1
V R3
2
V R3
3
V R1
1
V R6
4
...
V
Rmk3
m3
...
...
...
...
...
...
...
...
Fn
V R1
1
V R1
1
V R1
1
V R1
1
V R5
2
V R5
2
...
V
Rmkn
mn
Informally, the predecessor of a component C(i)
k
is the com-
ponent C(j)
k , such that R j was the latest revision where the
component Ck has been changed. Formally, we model the
successor and predecessor of a component Ck during the
revision process as a function.
Let i ∈ R and k ∈ C arbitrarily chosen. Set A(i,k) :=
max{l ∈ R : l < i and CONT(C(i)
k ) ̸= CONT(C(l)
k )} then
Deﬁnition IV.6 (Predecessor of a component) We set
PRED: R → R,
C(i)
k 7→ PRED(C(i)
k ) :=









C(i)
k
if (i = 1),
C(A(i,k))
k
if (i > 1)
and A(i,k) exists,
C(1)
k
otherwise.
Deﬁnition IV.7 (Successor of a component) We set
SUCC: R → P(R),
C(i)
k 7→ SUCC(C(i)
k ) := {C(j)
k
∈ R | C(i)
k = PRED(C(j)
k )}.
Remark IV.4 The predecessor of C(i)
k
is uniquely determined.
This follows directly from the deﬁnition above. In contrast, the
successor of C(i)
k
is not necessary unique, but there exists a
unique R j ∈ R, such that C(j)
k
∈ SUCC(R(i)
k ) and STATUS(Rj)
is active. Similar considerations also hold for revisions.
■
Proposition IV.1 (Existence and uniqueness) Let
R ∈ R,
such that STATUS(R) = active. If SUCC(R) ̸= ∅ then there
exists a unique Q ∈ R, such that Q ∈ SUCC(R) and
STATUS(Q) = active.
Hint The existence and the uniqueness follows directly from
the deﬁnition of the status of a revision (see Deﬁnition IV.5)
and the uniqueness of the predecessor (see Remark IV.3).
■
We are now able to formulate our strategy to generate the
successive revisions.
Lemma IV.1 (Linearity) Let m = card(R). Then there exists
a unique subset R
′ of R with R
′ ={R1,Ri1,Ri2,Ri3,...,Ril,Rm},
such that Ri1 ∈ SUCC(R1) and ∀ik : i1 ≤ ik < il ⇒ Ri(k+1) ∈
SUCC(Rik) and Rm ∈ SUCC(Ril) and ∀R ∈ R
′ : STATUS(R) =
active and ∀R ∈ R \R′ : STATUS(R) = abandoned.
Hint It is a direct consequence of the uniqueness of active
successors (see Proposition IV.1).
■
Corollary IV.1 The sequence of the active revisions is linear.
■
Let i ∈ R arbitrarily chosen, a component can have at the
revision Ri two statuses modiﬁed and preserved, the value
modiﬁed means that the component has been modiﬁed during
the revision Ri, in contrast preserved means that the component
remained unchanged at revision Ri. More formally, we deﬁne
the function:
Deﬁnition IV.8 (Status of a component) We set
STATUS : R → {modiﬁed, preserved},
C(i)
k 7→ STATUS(C(i)
k ) :=



modiﬁed
if (i = 1),
modiﬁed
if condition 2 holds,
preserved
otherwise.
with condition 2: ∀ j ∈ R with 1 ≤ j < i : CONT(C(i)
k ) ̸=
CONT(C( j)
k ).
Remark IV.5 From a formal point of view, all components
corresponding to the ﬁrst revision are considered modiﬁed. For
the subsequent revisions only the components whose content
has been altered are considered modiﬁed.
■
C. Backing up a revision
Based on the values of the STATUS function, we deﬁne
the upload strategy. We are interested to upload only those
components, which have been modiﬁed since the latest revision
and the current state has not been uploaded previously.
Deﬁnition IV.9 (Upload of a component) We set
UPLOAD: R → {yes, no},
C(i)
k 7→ UPLOAD(C(i)
k ) :=
yes if condition 1 holds,
no otherwise.
with condition 1: STATUS(C(i)
k ) = modiﬁed.
Remark IV.6 This means especially that C(i)
k
will be uploaded
if and only if it has been modiﬁed and its content is different
from the content of all its predecessors.
■
We will deﬁne now a function in order to model the backup
process of an entire revision. As we will see, only those
components will be backed up at a speciﬁc revision Ri, which
have been modiﬁed at this revision.

223
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Deﬁnition IV.10 (Backup of a revision) We set
BACKUP: R → R,
Ri 7→ BACKUP(Ri)
:= {C(i)
k | k ∈ C, such that UPLOAD(C(i)
k ) = yes}.
Remark IV.7 This means especially that the components that
have not been changed at revision Ri are not included in the
backup of the revision Ri, this is the quintessence of the selective
backup strategy.
■
In order to be able to model the download and restore
process, we need to do some additional analysis. Those opposite
functions cannot be deﬁned straightforwardly as the reverse
function of BACKUP and UPLOAD, since after the restore is
fulﬁlled, all the relevant components must be available, not
only those persisted at the corresponding revision.
In order to have all the relevant information for the
restore process, we build during the evolution of the model
a matrix (INF(i)
k )k∈C,i∈R , such that this matrix contains the
information relevant for the download and restore operations.
This information contains the content of the components, such
that comparisons can be done and relate it to the previous
backups.
Hence, the matrix (INF(i)
k )k∈C,i∈R contains at least the
information regarding the revision at which the component
was physically stored, such that it can be retrieved from there
and additional information regarding the content (hash value)
of the components.
Formally, we deﬁne (INF(i)
k )k∈C,i∈R as a function:
Deﬁnition IV.11 (Component upload meta inf) We set
INF: R ×C → R ×HASH,
(i,k) 7→ INF(i,k) := ( j,v)
if

C(i)
k ∈ BACKUP(R j) and CONT(C(i)
k ) = v

.
Remark IV.8 This means especially that a component C(i)
k
having the hash value = v has been backed up at revision Rj
and j ∈ R is the lowest index number, such that CONT(C(i)
k ) =
CONT(C(j)
k ).
■
D. Restoring a revision
We deﬁne now the opposite function to UPLOAD as follows:
Deﬁnition IV.12 (Download of a component) We set
DOWNLOAD: R → R,
C(i)
k 7→ DOWNLOAD(C(i)
k ) := C(j)
k
if (UPLOAD(C(j)
k ) = yes
and CONT(C(j)
k ) = CONT(C(i)
k )).
Remark IV.9 DOWNLOAD(C(i)
k ) =C(j)
k
means especially that
the component Ck was uploaded at the revision Rj.
■
We deﬁne the restore function having the opposite functionality
to the backup function. The main difference to the usual
restore strategy is that restoring the components backed up
at revision Ri is not enough, since usually only a subset of the
components are backed up at a speciﬁc revision. To circumvent
this impediment, the revisions at which those components have
been physically uploaded are identiﬁed and are restored from
those locations. When the restore operation is completed, then,
the complete set of components necessary for a simulation is
available.
Formally as a function:
Deﬁnition IV.13 (Restore of a revision) We set
RESTORE: R → P(R),
Ri 7→ RESTORE(Ri) :=
{C(j)
k
| k ∈ C, such that DOWNLOAD(C(i)
k ) = C(j)
k }.
Remark IV.10 This means especially that the latest version
of the components are restored.
■
We are now able to formulate the Lemma regarding the
uniqueness of the upload, i.e., a new state of a component
Ck during the revision process is backed up only once.
Lemma IV.2 (Uniqueness of the upload) Let k ∈ C and v ∈
HASH be arbitrarily chosen. If ∃ j ∈ R : CONT(C(j)
k ) = v then
there exists a unique i ∈ R , such that UPLOAD(C(i)
k ) = yes and
CONT(C(i)
k ) = v.
Hint Set i := min{l ∈ R | CONT(C(l)
k ) = v}. Then according to
the deﬁnition of the status of a component (see Deﬁnition IV.8)
STATUS(C(l)
k ) = modiﬁed holds true. The result follows from the
deﬁnition of the upload of the components (see Deﬁnition IV.9).
■
We can formulate now the main Lemma, which states that we
have no spurious downloads.
Lemma IV.3 (Accuracy and completeness) Let k ∈ C be ar-
bitrarily chosen. We have:
a)∀i, j ∈ R : DOWNLOAD(C(i)
k ) = C(j)
k
⇒

224
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
1 <OpenGeoSysProject>
2
<mesh>cube_1x1x1_hex_1e0.vtu</mesh>
3
<geometry>cube_1x1x1.gml</geometry>
4
5
<processes>
6
<process>
7
<name>GW23</name>
8
<type>GROUNDWATER_FLOW</type>
9
<process_variable>pressure</process_variable>
10
<hydraulic_conductivity>K</hydraulic_conductivity
>
11
<linear_solver>
12
<lis>-i cg -p jacobi -tol 1e-16 -maxiter 10000
</lis>
13
<eigen>
14
<solver_type>CG</solver_type>
15
<precon_type>jacobi</precon_type>
16
<max_iteration_step>10000</
max_iteration_step>
17
<error_tolerance>1e-16</error_tolerance>
Figure 10. Excerpt of example project ﬁle cube_1e0_neumann.prj
V.
IMPLEMENTATION AND APPLICATION TO SPECIFIC USE
CASE
We developed and tested AGEDRE (Automatic GEneration
of Documentation using REvision control) as a prototype at
UFZ and validated our theoretical concepts. We used a client /
server environment at the ZIH of the Technische Universität
Dresden, implementing our client in Java from scratch. On the
server side, we used KIT DM [14] as the repository.
AGEDRE is a command line utility, offering the basic
functionality required for a revision control system and a
sophisticated error handling to deal with the complexity of the
selective backup strategy on the client side and of KIT DM on
the server side. In order to persist the data, AGEDRE offers two
primitives, FullUpload as a primitive for a complete upload
of all data related to a project and Upload as a selective
backup strategy to store only modiﬁed ﬁles. Accordingly,
the opposite primitives to retrieve the data are Download,
DownloadRevision and DownloadFile. The primitive
Download is only formally the counterpart of Upload, it
retrieves the latest version of each ﬁle, which has been uploaded,
i.e., the ﬁles of the latest revision. As mentioned, the user
needs the latest version of all parameter and metadata ﬁles
in order to be able to perform the simulation. In contrast,
the primitive DownloadRevision is used to continue the
modeling process from an older revision, it restores the ﬁles
into the working directory, thus overwriting the latest revision.
The latest revision is backed up to the ﬁle system, in order to
avoid loss of data in case of inadvertent use of this primitive.
The primitive DownloadFile has been introduced in order
to restore the latest backed up version of a ﬁle, if it has been
accidentally deleted or has been corrupted.
The project ﬁle (see Figure 10 for an example) is
the leading ﬁle regarding the conﬁguration of a sim-
ulation.
It
contains
the
names
of
the
additional
pa-
rameter ﬁles (namely cube_1x1x1_hex_1e0.vtu and
cube_1x1x1.gml) and the conﬁguration parameters for the
simulation. Hence, each project ﬁle corresponds to a model
and accordingly, the development of the model comprises
modiﬁcations of the project ﬁle and the corresponding parameter
ﬁles.
When the primitive Upload is called for a project ﬁle for
1
<points>
2
<point id="0" x="0" y="0" z="0"/>
3
<point id="1" x="0" y="0" z="1"/>
4
<point id="2" x="0" y="1" z="1"/>
5
<point id="3" x="0" y="1" z="0"/>
6
</points>
7
8
<surfaces>
9
<surface id="0" name="left">
10
<element p1="0" p2="1" p3="2"/>
11
<element p1="0" p2="3" p3="2"/>
12
</surface>
13
<surface id="1" name="right">
14
<element p1="4" p2="6" p3="5"/>
15
<element p1="4" p2="6" p3="7"/>
16
</surface>
Figure 11. Excerpt of example geometry ﬁle cube_1x1x1.gml
the ﬁrst time, the corresponding dynamic ﬂow conﬁguration
ﬁle is initialized. For an example of a ﬂow conﬁguration
ﬁle, see the excerpts in Figures 7–9. The revision number
is set to one and the cryptographic MD5 and SHA-1 hashes
of each ﬁle are calculated and stored in the dynamic ﬂow
conﬁguration ﬁle. While SHA-1 is practically collision free and
it is also used by Git for integrity purposes [26], alternatively,
SHA-512 could be used for enhanced security [27] [28]. For
subsequent uses of the Upload-primitive, the cryptographic
values of the parameter and metadata ﬁles are compared to
the respective values stored – for previous revisions – in the
ﬂow conﬁguration ﬁle. If the cryptographic values of ﬁle F is
different of all the previous cryptographical values of ﬁle F,
then the content of F is considered modiﬁed and it is backed
up within the current revision. Otherwise, F is not part of the
current revision. A corresponding entry is made in the dynamic
ﬂow conﬁguration ﬁle regarding the revision under which the
ﬁle – having the given content – has been backed up. Hence, the
ﬁle cube_1x1x1.gml has the cryptographic values stored
at revision 1 (see Figure 9). If the ﬁle cube_1x1x1.gml
has, for example, not been altered at revision 2 then no similar
entry is performed in the dynamic ﬂow conﬁguration ﬁle.
When starting the primitive Download – i.e., downloading
the ﬁles corresponding to the latest revision – then the
relevant information regarding the physical storage place of
the latest version of each ﬁle – i.e., <StorageDigitalObjectID>
– is retrieved from the dynamic ﬂow conﬁguration ﬁle, by
considering the latest entry for each ﬁle (see Figure 9). Hence,
all the related ﬁles can be accessed on the repository and
retrieved accordingly.
The use case at UFZ is not designed for concurrent use. In
a software development environment, users of version control
systems are confronted with merge conﬂicts and their resolution.
In contrast, simultaneous work is in the scope of our application
strictly related to the model development process. The model
is developed iteratively, small changes in a few parameter ﬁles
can have tremendous impact on the model. Hence, members
of a team can download the latest revision and can work
simultaneously improving the next step. They can compare
the changes of parameter ﬁles and the simulation results.
Conﬂicts can theoretically occur but are much more unlikely
due to a smaller number of users editing ﬁles and each user
usually having a speciﬁc task to solve. Also, a (semi-)automatic
handling of conﬂicts is near impossible in this scenario since

225
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
ﬁxing conﬂicts requires a contextual understanding of what a
speciﬁc change means in the context of a given model. Members
of the team have to agree on the best outcome for the next
step before uploading it as the next revision. Alternatively, they
can agree on abandoning the current revision by continuing the
model development from an older revision. All team members
have to download the revision they agreed upon, and continue
development from that point.
In addition to the dynamic ﬂow conﬁguration ﬁle – upon
whose content they do not have direct inﬂuence – users can
deﬁne and set up their own metadata ﬁles. These metadata
ﬁles can contain additional – high-level or aggregated –
information regarding the model development and can be
used for additional documentation or for identifying model
or revision characteristics. The metadata ﬁles are also very
important to enable the differentiated security policy at UFZ,
such that users can access the metadata ﬁles – for example by
using ElasticSearch [18] – if they have the appropriate rights
on the ﬁle system. In contrast, users can access simulation
data (parameter ﬁles) according to their rights on the repository
system KIT DM. Thus, metadata for projects is accessible for
all members within the UFZ, while sensible data can only be
accessed by a small number of researchers related to the project.
All ﬁles of the examples can be found at [29].
VI.
CONCLUSION AND FUTURE WORK
Irrespective of the fact that creating documentation is
a very challenging task and that writing documentation is
considered by most of the developers as an extra-effort rather
than a commendation, it is rather impossible providing precise,
exact, accurate, uniform and consistent documentation for
developers and users requirements. Therefore, formalized
automated documentation methods are necessary to develop.
The main advantage of the automatic generation of the
documentation of the modeling process is the accuracy of
the documentation, since there is no discrepancy between the
actual generation of the model (developer’s perspective) and
the corresponding documentation (user’s perspective). This way,
we have circumvented the dilemma of writing exact manual
documentation and have contributed to the paradigm change
towards design and implementation of automatic documentation
assuring accuracy and exactness.
Since our formal model is independent of the use case at
UFZ, our approach to automatically generate a documentation
for the evolution of the model during model development has
a generic character and it can be applied to all domains where
numerical models are developed. Moreover, the formal model
is generalizable beyond the use case presented in this paper.
We have based our solution at UFZ exclusively on common
techniques, which are not dependent on speciﬁc ﬁle formats
or speciﬁc applications. Examples include the utilization of
the XML format for the documentation, hash code based
ﬁle comparison techniques as well as methods which are
independent of the syntax or semantic of the underlying data.
With one exception, i.e., the speciﬁcation of the list of ﬁle types
which should be monitored, we use only assumptions speciﬁc
to the open source project OpenGeoSys [11], to applications
in the earth modeling domain or to numeric simulations. The
system could version and document images, texts, or any other
data in the same way.
We have opted for a revision control system using the
KIT DM framework based on MASi among others, to have a
completely decoupled access to the information surrounding
the model development, i.e., metadata easily accessible from
various locations and the model development itself, accessible
only to the members of the developing team. Alternatively, using
modern methods for concurrency control applied to modern
database systems is a viable and future-oriented approach. This
way, the difﬁculties on joint and concurrent development could
be diminished.
To summarize, we have described in detail to what extent
our versioning system facilitates simpliﬁes and makes fully
transparent the activity of application scientist. We have pre-
sented the main challenges in Section I-A, then the challenges
are substantiated and concrete solutions are provided to them.
Moreover, in Section IV we have set up a formal model in which
the given solution is validated. This gives us hints regarding
the generalization spectrum beyond the use case at UFZ.
Currently, there are no convenience features for users
employing the framework. In the current implementation
prototype, the executable is started in the command line, also
the ﬂow conﬁguration ﬁle, which contains the documentation
for tracking the evolution of the model is in XML format.
Accordingly, additional user tests are necessary to deﬁne
and implement a corresponding GUI to assure the expected
readability of the document by extracting and visualizing the
appropriate information. In order to build meaningful user
interfaces, an intense dialog between developers and users is
essential [30].
Additionally, further research is necessary to generate a
high level form documentation of the changes in the parameter
ﬁles. For example, when running stochastic simulations (e.g.,
using the Monte Carlo approach [31]) and parameters are
simultaneously changed in many places, then an appropriate
mechanism should be set up to assure the consistency of the
changes and the creation of correct documentation.
We believe that by studying the automatically generated
documentation regarding the development of the modeling
workﬂows – especially those steps, which did not lead to a
successful completion of the simulation – there is an increased
possibility of knowledge extraction (by using machine learning
strategies or similar techniques), such that the generation of
the modeling workﬂows can be dramatically improved and the
number of modeling steps can be considerably reduced.
ACKNOWLEDGMENT
This work was supported in parts by the German Federal
Ministry of Education and Research (BMBF, 01IS14014A-
D) by funding the competence center for Big Data “ScaDS
Dresden/Leipzig”. This work was also supported in parts by
the German Research Foundation (DFG) via the MASi project
(NA 711/9-1, STO 397/4-1). We are also thankful to Dr. Nico
Hoffmann (Technische Universität Dresden) for his valuable
advices and comments during the developing and writing
process and Dr. Agnes Sachse (né Gräbe) for her data on
her hydrogeological case study.

226
International Journal on Advances in Software, vol 11 no 3 & 4, year 2018, http://www.iariajournals.org/software/
2018, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
REFERENCES
[1]
M. Zinner et al., “Automatic documentation of the development of
numerical models for scientiﬁc applications using speciﬁc revision
control,” in ICSEA 2017, The Twelfth International Conference on
Software Engineering Advances, L. Lavazza, R. Oberhauser, R. Koci,
and S. Clyde, Eds., Oct. 2017, pp. 18–27, IARIA Conference. [Online].
Available: http://www.thinkmind.org/index.php?view=article&articleid=
icsea_2017_1_30_10110
[2]
Intergovernmental Panel on Climate Change, Climate Change 2014 –
Impacts, Adaptation and Vulnerability: Regional Aspects.
Cambridge
University Press, 2014.
[3]
C. J. Vörösmarty et al., “Global threats to human water security and
river biodiversity,” Nature, vol. 467, 2010, pp. 555–561.
[4]
J. Grundmann, N. Schütze, G. H. Schmitz, and S. Al-Shaqsi, “Towards
an integrated arid zone water management using simulation-based
optimisation,” Environ Earth Sci, vol. 65, no. 5, 2012, pp. 1381–1394.
[5]
H. Hötzl, P. Möller, and E. Rosenthal, The Water of the Jordan Valley.
Springer, 2009.
[6]
M. Walther, J.-O. Delfs, J. Grundmann, O. Kolditz, and R. Liedl,
“Saltwater intrusion modeling: Veriﬁcation and application to an
agricultural coastal arid region in Oman,” Journal of Computational
and Applied Mathematics, vol. 236, no. 18, 2012, pp. 4798–4809,
fEMTEC 2011: 3rd International Conference on Computational Methods
in Engineering and Science, May 9–13, 2011. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0377042712000659
[7]
C. Liu, Q. Wang, C. Zou, Y. Hayashi, and T. Yasunari, “Recent trends
in nitrogen ﬂows with urbanization in the shanghai megacity and the
effects on the water environment,” Environmental Science and Pollution
Research, vol. 22, no. 5, Mar 2015, pp. 3431–3440. [Online]. Available:
https://doi.org/10.1007/s11356-014-3825-4
[8]
A. Gräbe et al., “Numerical analysis of the groundwater regime in the
western Dead Sea Escarpment, Israel + West Bank,” Environ Earth Sci,
vol. 69, no. 2, 2013, pp. 571–585.
[9]
K. Rink, L. Bilke, and O. Kolditz, “Visualisation Strategies for
Environmental Modelling Data,” Env Earth Sci, vol. 72, no. 10, 2014,
pp. 3857–3868.
[10]
T. Fischer, D. Naumov, S. Sattler, O. Kolditz, and M. Walther,
“GO2OGS 1.0: a versatile workﬂow to integrate complex geological
information with fault data into numerical simulation models,”
Geoscientiﬁc Model Development, vol. 8, 2015, pp. 3681–3694.
[Online]. Available: http://www.geosci-model-dev.net/8/3681/2015/
[11]
O. Kolditz et al., “OpenGeoSys: An open source initiative for numerical
simulation of thermo-hydro-mechanical/chemical (THM/C) processes in
porous media,” Environ Earth Sci, vol. 67, no. 2, 2012, pp. 589–599.
[12]
K. Rink et al., “Virtual geographic environments for water pollution
control,” Int J Dig Earth, vol. 11, no. 4, 2018, pp. 397–407.
[13]
Helmholtz Centre for Environmental Research – UFZ, “Homepage of
Helmholtz Centre for Environmental Research,” https://www.ufz.de/
index.php?en=34216, retrieved: November 2018.
[14]
Karlsruhe Institute of Technology – KIT, “KIT Data Manager,” http:
//datamanager.kit.edu/index.php/kit-data-manager, retrieved: November
2018.
[15]
R. Grunzke et al., “The MASi repository service - comprehensive,
metadata-driven and multi-community research data management,”
Future Generation Computer Systems, 2018. [Online]. Available:
https://doi.org/10.1016/j.future.2017.12.023
[16]
S. Chacon and B. Straub, Git and Other Systems.
Berkeley, CA:
Apress, 2014, pp. 307–356. [Online]. Available: http://dx.doi.org/10.
1007/978-1-4842-0076-6_9
[17]
C. M. Pilato, B. Collins-Sussman, and B. W. Fitzpatrick, Version
control with subversion - the standard in open source version
control.
O’Reilly, 2008, retrieved: November 2018. [Online]. Available:
http://www.oreilly.de/catalog/9780596510336/index.html
[18]
C. Gormley and Z. Tong, Elasticsearch: The Deﬁnitive Guide, 1st ed.
O’Reilly Media, Inc., 2015.
[19]
W. F. Tichy, “Rcs – a system for version control,” Software: Practice
and Experience, vol. 15, no. 7, 1985, pp. 637–654. [Online]. Available:
http://dx.doi.org/10.1002/spe.4380150703
[20]
A. Löh, W. Swierstra, and D. Leijen, “A Principled Approach to Version
Control,” http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.
8649, retrieved: November 2018.
[21]
E. Sink, Version Control by Example, 1st ed.
PYOW Sports Marketing,
2011.
[22]
C. L. Paris, Automatic documentation generation: Including examples.
Berlin, Heidelberg: Springer Berlin Heidelberg, 1995, pp. 12–25.
[Online]. Available: http://dx.doi.org/10.1007/BFb0034794
[23]
R. Swan and J. Allan, “Automatic generation of overview timelines,” in
Proceedings of the 23rd Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval, ser. SIGIR ’00.
New York, NY, USA: ACM, 2000, pp. 49–56. [Online]. Available:
http://doi.acm.org/10.1145/345508.345546
[24]
K. McKeown, K. Kukich, and J. Shaw, “Practical issues in automatic
documentation generation,” in Proceedings of the Fourth Conference on
Applied Natural Language Processing, ser. ANLC ’94.
Stroudsburg,
PA, USA: Association for Computational Linguistics, 1994, pp. 7–14.
[Online]. Available: http://dx.doi.org/10.3115/974358.974361
[25]
B. Möller, O. Greß, and S. Posch, “Knowing what happened - automatic
documentation of image analysis processes,” in Computer Vision
Systems - 8th International Conference, ICVS 2011, Sophia Antipolis,
France, September 20-22, 2011. Proceedings, 2011, pp. 1–10. [Online].
Available: https://doi.org/10.1007/978-3-642-23968-7_1
[26]
M. Stevens, E. Bursztein, P. Karpman, A. Albertini, and Y. Markov,
“The ﬁrst collision for full sha-1,” Cryptology ePrint Archive, Report
2017/190, 2017, http://eprint.iacr.org/2017/190.
[27]
C. Dobraunig, M. Eichlseder, and F. Mendel, Analysis of SHA-
512/224 and SHA-512/256.
Berlin, Heidelberg: Springer Berlin
Heidelberg, 2015, pp. 612–630. [Online]. Available: https://doi.org/10.
1007/978-3-662-48800-3_25
[28]
M. Szydlo and Y. L. Yin, Collision-Resistant Usage of MD5
and
SHA-1
Via
Message
Preprocessing.
Berlin,
Heidelberg:
Springer Berlin Heidelberg, 2006, pp. 99–114. [Online]. Available:
https://doi.org/10.1007/11605805_7
[29]
D. Y. Naumov et al., “ufz/ogs-data: Initial zenodo release,” Aug. 2017.
[Online]. Available: https://doi.org/10.5281/zenodo.840660
[30]
C. Helbig, L. Bilke, H.-S. Bauer, M. Böttinger, and O. Kolditz,
“Meva - an interactive visualization application for validation of
multifaceted meteorological data with multiple 3d devices,” PLOS
ONE, vol. 10, no. 4, 04 2015, pp. 1–24. [Online]. Available:
https://doi.org/10.1371/journal.pone.0123811
[31]
E. Jang et al., “Identifying the inﬂuential aquifer heterogeneity factor
on nitrate reduction processes by numerical simulation,” Advances in
Water Resources, vol. 99, Jan. 2017, pp. 38–52.

