Effect of Lazy Rebuild on Reliability of
Erasure-Coded Storage Systems
Ilias Iliadis
IBM Research Europe – Zurich
8803 R¨uschlikon, Switzerland
email: ili@zurich.ibm.com
Abstract—Erasure-coding redundancy schemes are employed to
ensure improved reliability of storage systems against device
failures. The effect of the lazy rebuild scheme on the Mean Time
to Data Loss (MTTDL) and the Expected Annual Fraction of
Data Loss (EAFDL) reliability metrics is evaluated. A theoretical
model that considers the effect of latent errors and device failures
is developed. Analytical reliability expressions for the symmetric,
clustered, and declustered data placement schemes are derived. It
is demonstrated that the employment of lazy rebuild results in a
reliability degradation of orders of magnitude. Independently of
whether lazy rebuild is used, for realistic values of sector error
rates, the results obtained demonstrate that MTTDL degrades,
whereas EAFDL remains practically unaffected. It is also shown
that the declustered data placement scheme offers superior
reliability.
Keywords–Storage; Deferred recovery or repair; Unrecoverable
or latent sector errors; Reliability analysis; MTTDL; EAFDL;
RAID; MDS codes; stochastic modeling.
I.
INTRODUCTION
Efficient erasure coding schemes that provide high data
reliability are employed in today’s large-scale data storage
systems to recover data lost due to device and component
failures. Special cases of erasure codes are the replication
schemes and the Redundant Arrays of Inexpensive Disks
(RAID) schemes, such as RAID-5 and RAID-6, which have
been deployed extensively in the past thirty years [1-4]. Mod-
ern storage systems though use advanced, powerful erasure
coding schemes that offer high storage efficiency and improve
data reliability [5-8]. The reliability of storage systems is also
improved by employing a declustered data placement scheme,
but is adversely affected by latent or unrecoverable sector
errors that are discovered when there is an attempt to access
these sectors [9]. Permanent losses of data due to latent errors
are quite pronounced in higher-capacity HDDs and storage
nodes [10-12].
Despite the reduction in storage overhead and the improve-
ment of reliability achieved, erasure coding is hindered from
becoming more pervasive in large-scale distributed storage
systems by the repair problem. This issue arises from the
increased network traffic needed to repair data lost due to
device failures and generated by the downloads and disk
IOPS performed during the data recovery process [6][7][13].
To cope with the repair problem and reduce the amount of
data transmitted during rebuilds, a lazy rebuild scheme was
proposed in [14]. A careful scheduling of rebuild operations
substantially reduces recovery bandwidth, while keeping the
impact on read performance and data durability low. Lazy
recovery reduces repair bandwidth at the expense of increasing
the amount of degraded stripes, which in turn affects system
reliability. The lazy recovery scheme bears some similarity to
the practice of delaying recovery of failed nodes by a fixed
amount of time (typically 15 minutes) to avoid unnecessary
repairs of short transient failures [5]. The main difference,
however, is that a lazy repair is initiated based on the state of
the system and does not depend on the time that has elapsed
after a node failure. This results in transferring less data than
the delayed recovery scheme.
The key contributions of this article are the following. We
consider the reliability of erasure-coded storage systems when
a lazy rebuild scheme is employed and derive closed-form
expressions for the MTTDL and EAFDL reliability metrics
for the symmetric, clustered, and declustered data placement
schemes. We adopt the non-Markovian methodology devel-
oped in prior work [15-17] to evaluate MTTDL and EAFDL
of storage systems. The validity of this methodology for
accurately assessing the reliability of storage systems has been
confirmed by simulations in several contexts [3][15][18][19].
It has been demonstrated that theoretical predictions of the re-
liability of systems comprising highly reliable storage devices
are in good agreement with simulation results. Consequently,
the emphasis of the present work is on theoretically assessing
the effect of lazy rebuilds on the reliability of storage systems.
We extend the reliability model presented in [9] to take into
account lazy rebuilds. The model developed is relevant and
realistic because it properly captures the characteristics of
erasure coding and of the rebuild process associated with the
declustered placement scheme currently used by Google [5],
Microsoft Azure [7], Facebook [13], and DELL/EMC [20].
The theoretical reliability results obtained here can be used to
determine the parameter values that ensure a desired level of
reliability. They can also be used to assess system reliability
when scrubbing is employed by applying the methodology
described in [21]. We subsequently use these results to demon-
strate the effect of latent errors and system parameters on
system reliability.
The remainder of the article is organized as follows.
Section II describes the storage system model and the cor-
responding parameters considered. Section III presents the
general framework and methodology for deriving the MTTDL
and EAFDL metrics analytically for the case of erasure-
coded systems that employ a lazy rebuild scheme. Closed-
form expressions for relevant reliability metrics are derived
for the symmetric, clustered, and declustered data placement
schemes. Section IV presents numerical results demonstrating
the effectiveness of erasure coding schemes for improving
system reliability as well as the adverse effect of lazy rebuilds.
1
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-944-7
CTRQ 2022 : The Fifteenth International Conference on Communication Theory, Reliability, and Quality of Service

TABLE I.
NOTATION OF SYSTEM PARAMETERS
Parameter
Definition
n
number of storage devices
c
amount of data stored on each device
l
number of user-data symbols per codeword (l ≥ 1)
m
total number of symbols per codeword (m > l)
(m, l)
MDS-code structure
d
lazy rebuild threshold (0 ≤ d < m − l)
s
symbol size
k
spread factor of the data placement scheme, or
group size (number of devices in a group) (m ≤ k ≤ n)
b
average reserved rebuild bandwidth per device
Bmax
upper limitation of the average network rebuild bandwidth
X
time required to read (or write) an amount c of data at an average
rate b from (or to) a device
FX(.)
cumulative distribution function of X
Fλ(.)
cumulative distribution function of device lifetimes
Pbit
probability of an unrecoverable bit error
seff
storage efficiency of redundancy scheme (seff = l/m)
U
amount of user data stored in the system (U = seff n c)
˜r
MDS-code distance: minimum number of codeword symbols lost
that lead to permanent data loss
(˜r = m − l + 1 and 2 ≤ ˜r ≤ m)
C
number of symbols stored in a device (C = c/s)
µ−1
mean time to read (or write) an amount c of data at an average rate
b from (or to) a device (µ−1 = E(X) = c/b)
λ−1
mean time to failure of a storage device
(λ−1 =
R ∞
0 [1 − Fλ(t)]dt)
Ps
probability of an unrecoverable sector (symbol) error
PDL
probability of data loss during rebuild
PUF
probability of data loss due to unrecoverable failures during rebuild
PDF
probability of data loss due to a disk failure during rebuild
Q
amount of lost user data during rebuild
H
amount of lost user data, given that data loss has occurred, during
rebuild
S
number of lost symbols during rebuild
Finally, we conclude in Section V.
II.
STORAGE SYSTEM MODEL
Here we briefly review the operational characteristics of
erasure-coded storage systems. To assess their reliability, we
adopt the model used in [9] and extend it to cover the case of
lazy rebuilds. The storage system comprises n storage devices
(nodes or disks), where each device stores an amount c of
data such that the total storage capacity of the system is n c.
This does not account for the spare space used by the rebuild
process.
User data is divided into blocks of fixed size s and com-
plemented with parity symbols to form codewords. Maximum
Distance Separable (MDS) erasure codes (m, l) that map l
user-data symbols to codewords of m symbols are employed.
They have the property that any subset containing l of the
m codeword symbols can be used to reconstruct (recover)
a codeword. The corresponding storage efficiency seff and
amount U of user data stored in the system is
seff = l/m
and
U = seff n c = l n c/m .
(1)
Also, the number C of symbols stored in a device is
C = c/s .
(2)
Our notation is summarized in Table I. The derived param-
eters are listed in the lower part of the table. To minimize the
risk of permanent data loss, the m symbols of each codeword
are spread and stored on m distinct devices. This way, the
system can tolerate any ˜r − 1 device failures, but ˜r device
failures may lead to data loss, with
˜r = m − l + 1 ,
1 ≤ l < m
and
2 ≤ ˜r ≤ m .
(3)
Examples of MDS erasure codes are the replication, RAID-5,
RAID-6, and Reed–Solomon schemes.
Data is stored according to symmetric placement schemes,
including the clustered and declustered placement schemes,
as shown in Figure 1 of [9][17]. The system comprises n/k
disjoint groups of k devices. Within each group, all

has lost. To improve reliability, the vulnerability window is
reduced by recovering only one symbol as opposed to the
scheme considered in [14] that recovers multiple symbols. In
a distributed rebuild process, the codewords are reconstructed
by reading symbols from an appropriate set of surviving
devices and storing the recovered symbols in the reserved spare
space of these devices. During this process, it is desirable to
reconstruct the lost codeword symbols on devices in which
another symbol of the same codeword is not already present.
In the case of clustered placement, the codeword symbols
are spread across all k (= m) devices in each group (cluster).
Therefore, reconstructing the lost symbols on the surviving
devices of a group would result in more than one symbol of
the same codeword on the same device. To avoid this, the
lost symbols are reconstructed directly in spare devices as
described and shown in Figure 3 of [17].
3) Rebuild Process: A certain portion of the device band-
width is reserved for read/write data recovery during the
rebuild process, and the remaining bandwidth is used to serve
user requests. Let b denote the actual average reserved rebuild
bandwidth per device. Lost symbols are rebuilt in parallel using
the rebuild bandwidth b available on each surviving device. The
amount of data corresponding to the number Cu of symbols
to be rebuilt at exposure level u is written at an average rate
bu (≤ b) to selected device(s). For the time X required to read
(or write) an amount c of data from (or to) a device it holds
that
µ−1 ≜ E(X) = c/b .
(6)
4) Failure and Rebuild Time Distributions: The lifetimes
of the n devices are assumed to be independent and identically
distributed, with a cumulative distribution function Fλ(.) and a
mean of 1/λ. The results in this article hold for highly reliable
storage devices, which satisfy the condition [17][19]
µ
Z ∞
0
Fλ(t)[1 − FX(t)]dt ≪ 1,
with λ
µ ≪ 1 .
(7)
5) Amount of Data to Rebuild and Rebuild Times at Each
Exposure Level: We denote by ˜nu the number of devices
at exposure level u whose failure causes an exposure level
transition to level u + 1, and Vu the fraction of the Cu most-
exposed codewords that have a symbol stored on any given
such device. Note that ˜nu depends on the codeword placement
scheme. Let Ru denote the rebuild time of the most-exposed
codewords at exposure level u and αu be the fraction of the
rebuild time Ru still left when another device fails, causing
the exposure level transition u → u + 1. For u ≤ d, no
rebuild is performed and therefore αu = 1. For u > d, αu
is approximately uniformly distributed in (0, 1) [23, Lemma
2]. Therefore,
αu ≊
 1 ,
for u = 1, . . . , d
U(0, 1) ,
for u = d + 1, . . . , ˜r − 1 .
(8)
We proceed by considering that the rebuild time Ru+1 is
determined completely by Ru and αu in the same manner
as in [16][17][22]. For the rebuild schemes considered, the
fraction of the Cu most-exposed codewords that were not yet
considered by the rebuild process upon the next device failure
is roughly equal to the fraction αu of the rebuild time Ru still
left. Therefore, upon the next device failure, an approximate
number αu Cu of the Cu codewords were not yet considered by
the rebuild process. Clearly, the fraction Vu of these codewords
that have symbols stored on the newly failed device depends
only on the codeword placement scheme. Consequently, the
number Cu+1 of the most-exposed codewords upon entering
exposure level u + 1 is
Cu+1 ≈ Vu αu Cu , for u = 1, . . . , ˜r − 1 .
(9)
Repeatedly applying (9) and using (5) and the convention that
for any sequence δi, Q0
i=1 δi ≜ 1, yields
Cu ≈ C
u−1
Y
i=1
Vi αi , for u = 1, . . . , ˜r .
(10)
6) Unrecoverable Errors: The reliability of storage systems
is affected by the occurrence of unrecoverable or latent errors.
Let Pbit denote . According to the specifications, the unrecover-
able bit-error probability Pbit is equal to 10−15 for SCSI drives
and 10−14 for SATA drives [21]. Assuming that bit errors occur
independently over successive bits, the unrecoverable sector
(symbol) error probability Ps is
Ps = 1 − (1 − Pbit)s ,
(11)
with s expressed in bits. Assuming a sector size of 512 bytes,
the equivalent unrecoverable sector error probability is Ps ≈
Pbit × 4096, which is 4.096 × 10−12 in the case of SCSI and
4.096×10−11 in the case of SATA drives. In practice, however,
and also owing to the accumulation of latent errors over
time, these probability values are higher. Indeed, empirical
field results suggest that the actual values can be orders of
magnitude higher, reaching Ps ≈ 5 × 10−9 [24].
III.
DERIVATION OF MTTDL AND EAFDL
The reliability metrics are derived using the direct-path-
approximation methodology presented in [9][15][16][17] and
extending it to assess the effect of lazy rebuilds.
At any point in time, the system is in one of two modes:
non-rebuild or rebuild mode. Note that part of the non-rebuild
mode is the normal mode of operation where all devices are
operational and all data in the system has the original amount
of redundancy. In the context of lazy rebuild, when the first
device fails, the system does not enter the rebuild mode.
Subsequently, we refer to the device failure that causes the
transition from non-rebuild to rebuild mode as an initial device
failure, which should not be confused with the first device
failure. Consequently, an initial device failure triggers a rebuild
process that attempts to restore the lost data, which eventually
leads the system either to a Data Loss (DL) with probability
PDL or back to the original normal mode by restoring initial
redundancy, with probability 1 − PDL.
Let T be a typical interval of a non-rebuild period, that is,
the time interval from the time the system is brought to its
original state until a subsequent initial device failure occurs
that causes the system to enter exposure level d + 1. It then
holds that T = Pd
u=0 Tu, where T0 denotes the time interval
from the time the system is brought to its original state until
the first device failure and Tu denotes the time that the system
spends at exposure level u. For a system comprising n devices
with a mean time to failure of a device equal to 1/λ, it holds
that E(T0) = 1/(n λ). Given that the number of devices
3
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-944-7
CTRQ 2022 : The Fifteenth International Conference on Communication Theory, Reliability, and Quality of Service

at exposure level u whose failure causes an exposure level
transition to level u+1 is ˜nu, it holds that E(Tu) = 1/(˜nu λ).
From the above, it follows that
E(T) =
d
X
u=0
E(Tu) =
 d
X
u=0
1
˜nu
! 
λ ,
where ˜n0 ≜ n ,
(12)
where ˜nu is determined by (35) or (38).
The MTTDL metric is then obtained by [15, Eq. (5)]:
MTTDL ≈ E(T)
PDL
.
(13)
The EAFDL is obtained as the ratio of the expected amount
E(Q) of lost user data, normalized to the amount U of user
data, to the expected duration of T [15, Eq. (9)]:
EAFDL ≈
E(Q)
E(T) · U
(1)
=
m E(Q)
n l c E(T) .
(14)
where E(T) is determined by (12) and expressed in years.
The expected amount E(H) of lost user data, given that
data loss has occurred, is determined by [15, Eq. (8)]:
E(H) = E(Q)
PDL
.
(15)
A. Reliability Analysis
The reliability evaluation of the lazy rebuild scheme is
based on the reliability analysis presented in [9]. The MTTDL
and EAFDL reliability metrics were determined by first deriv-
ing the probability of data loss PDL and the expected amount
E(Q) of lost user data. Central to these derivations are the
variables αu that represent the fractions of the rebuild times
Ru still left when device failures cause exposure level tran-
sitions. These variables were assumed to be independent and
approximately uniformly distributed in (0, 1). However, in the
case of lazy rebuild, these variables are distributed according to
(8). We now proceed to derive the various measures of interest.
At any exposure level u (u = d + 1, . . . , ˜r − 1), data loss
may occur during rebuild owing to one or more unrecoverable
failures, which is denoted by the transition u → UF. Moreover,
at exposure level ˜r−1, data loss occurs owing to a subsequent
device failure, which leads to the transition to exposure level
˜r. Consequently, the direct paths that lead to data loss are the
following:
−−→
UFu : the direct path of successive transitions 1 → 2 →
· · · → u → UF, for u = d + 1, . . . , ˜r − 1, and
−−→
DF : the direct path of successive transitions 1 → 2 →
· · · → ˜r − 1 → ˜r,
with corresponding probabilities PUFu and PDF, respectively.
1) Data Loss: It holds that
PUFu = Pu Pu→UF , for u = d + 1, . . . , ˜r − 1 ,
(16)
where Pu is the probability of entering exposure level u, which
is derived in Appendix A as follows:
Pu ≈
(λ c Qd
j=1 Vj)u−d−1
(u − d − 1)!
E(Xu−d−1)
[E(X)]u−d−1
u−1
Y
i=d+1
˜ni
bi
V u−1−i
i
,
(17)
and Pu→UF is the probability of encountering an unrecoverable
failure during the rebuild process at this exposure level.
In [25], it was shown that PDL is accurately approximated
by the probability of all direct paths to data loss. Therefore,
PDL ≈ PDF +
˜r−1
X
u=d+1
PUFu .
(18)
Approximate expressions for the probabilities of data loss
PUFu and PDF are subsequently obtained by the following
proposition.
Proposition 1: For u = d + 1, . . . , ˜r − 1, it holds that
PUFu
≈ −

λ c
d
Y
j=1
Vj


u−d−1
E(Xu−d−1)
[E(X)]u−d−1
 u−1
Y
i=d+1
˜ni
bi
V u−1−i
i
!
· log(ˆqu)−(u−d−1)
 
ˆqu −
u−d−1
X
i=0
log(ˆqu)i
i!
!
,
(19)
where
ˆqu ≜ q
C Qu−1
j=1 Vj
u
,
(20)
qu = 1 −
m−u
X
j=˜r−u
m − u
j

P j
s (1 − Ps)m−u−j ,
(21)
PDF ≈
(λ c Qd
j=1 Vj)˜r−d−1
(˜r − d − 1)!
E(X ˜r−d−1)
[E(X)]˜r−d−1
˜r−1
Y
i=d+1
˜ni
bi
V ˜r−1−i
i
.
(22)
Proof: Equation (19) is obtained in Appendix A. Equation
(22) is obtained from the fact that PDF = P˜r and, subsequently,
from (17) by setting u = ˜r.
The MTTDL metric is obtained by substituting (18) into
(13) as follows:
MTTDL ≈
E(T)
PDF + P˜r−1
u=d+1 PUFu
,
(23)
where E(T), PUFu and PDF are determined by (12), (19), and
(22), respectively.
2) Amount of Data Loss: We proceed to derive the amount
of data loss during rebuild. Let Q, H, and S be the amount of
lost user data, the conditional amount of lost user data, given
that data loss has occurred, and the number of lost symbols,
respectively. Let also QDF and QUFu denote the amount of
lost user data associated with the direct paths −−→
DF and −−→
UFu,
respectively. Similarly, we consider the variables HDF, HUFu,
SDF, and SUFu. Then, the amount Q of lost user data is
obtained by
Q ≈





HDF ,
if −−→
DF
HUFu ,
if −−→
UFu , for u = d + 1, . . . , ˜r − 1
0 ,
otherwise .
(24)
4
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-944-7
CTRQ 2022 : The Fifteenth International Conference on Communication Theory, Reliability, and Quality of Service

Thus,
E(Q) ≈ PDF E(HDF) +
˜r−1
X
u=d+1
PUFu E(HUFu)
(25)
= E(QDF) +
˜r−1
X
u=d+1
E(QUFu) ,
(26)
where
E(QDF) = PDF E(HDF) ,
(27)
and
E(QUFu) = PUFu E(HUFu) , u = d + 1, . . . , ˜r − 1 .
(28)
Note that the expected amount E(Q) of lost user data is
equal to the product of the storage efficiency and the expected
amount of lost data, where the latter is equal to the product
of the expected number of lost symbols E(S) and the symbol
size s. Consequently, it follows from (1) that
E(Q) = l
m E(S) s
(2)
=
l
m
E(S)
C
c .
(29)
Similarly, E(QDF) = l
m E(SDF) s
(2)
=
l
m
E(SDF)
C
c , (30)
and
E(QUFu) = l
m E(SUFu) s
(2)
=
l
m
E(SUFu)
C
c .
(31)
Proposition 2: For u = d + 1, . . . , ˜r − 1, it holds that
E(QUFu) ≈ c l ˜r
m

λ c Qd
j=1 Vj
u−d−1
(u − d)!
E(Xu−d−1)
[E(X)]u−d−1
 
d
Y
j=1
Vj
!
·
 u−1
Y
i=d+1
˜ni
bi V u−i
i
!  
m − u
˜r − u
!
P ˜r−u
s
,
Ps ≪
1
m − ˜r , (32)
E(QDF) ≈ c l ˜r
m
 
λ c
d
Y
j=1
Vj
!˜r−d−1
1
(˜r − d)!
E(X ˜r−d−1)
[E(X)]˜r−d−1
·
 
d
Y
j=1
Vj
!  ˜r−1
Y
i=d+1
˜ni
bi V ˜r−i
i
!
.
(33)
Proof: Equation (32) is obtained in Appendix B. Equation
(33) is obtained from (32) by setting u = ˜r.
The EAFDL metric is obtained by substituting (26) into
(14) as follows:
EAFDL ≈ m [E(QDF) + P˜r−1
u=d+1 E(QUFu)]
n l c E(T)
,
(34)
where E(QUFu) and E(QDF) are determined by (32) and (33),
respectively, and E(T) by (12) and expressed in years.
The conditional amount E(H) of lost user data, given
that data loss has occurred, is obtained from (15), PDL is
determined by (18), (19), and (22), and E(Q) is determined
by (26), (32), and (33).
B. Symmetric and Declustered Placement
We consider the case m < k ≤ n. The special case k = m
corresponding to the clustered placement has to be considered
separately for the reasons discussed in Section II-A2. At each
exposure level u, for u = 1, · · · , ˜r − 1, it holds that [16][17]
˜nsym
u
= k − u ,
(35)
bsym
u
= min((k − u) b, Bmax)
l + 1
,
(36)
V sym
u
= m − u
k − u .
(37)
The corresponding parameters ˜ndeclus
u
, bdeclus
u
, and V declus
u
for the declustered placement are derived from (35), (36), and
(37) by setting k = n.
C. Clustered Placement
At each exposure level u, for u = 1, · · · , ˜r − 1, it holds
that [16][17]
˜nclus
u
= m − u , bclus
u
= min( b , Bmax/l ) , V clus
u
= 1 . (38)
Remark 1: From (19), (22), (32), and (33), and consider-
ing expressions (35) through (38), it follows that PUFu and
E(QUFu) are mainly determined by the term (λ c/b)u−d−1,
and PDF and E(QDF) by the term (λ c/b)˜r−d−1. According
to (7), λ c/b ≪ 1, such that, for fixed values of ˜r and u,
increasing d causes these parameters to increase. Therefore,
by virtue of (23) and (34), increasing d causes MTTDL to
decrease and EAFDL to increase. Consequently, for fixed
values of m and l, deferring rebuilds degrades reliability.
D. Equivalent Systems
We call equivalent systems those that employ a given
codeword length m and have the same number m − l − d
of exposure levels at which the rebuild process is active. In
this case, it holds that l + d = z, and from (3) and (4), it
follows that
0 ≤ d < z < m
and
d + 1 ≤ u ≤ m − z + d + 1 .
(39)
Next, we compare the MTTDL and EAFDL of equivalent
systems. For Ps = 0, substituting (12), (19), and (22) into (23)
yields
MTTDL(d + 1)
MTTDL(d)
≈ E(T|d + 1)
E(T|d)
·
1
m−z+d
Y
u=d+1
Vu
·
m−z+d
Y
i=d+1
˜ni(d)
bi(d)
m−z+d+1
Y
i=d+2
˜ni(d + 1)
bi(d + 1)
.
(40)
From (12), it follows that E(T|d+1) > E(T|d). Also, Vu
represent fractions, which implies that Vu ≤ 1. Consequently,
the product of the first two terms of (40) is greater than 1.
For a symmetric placement scheme that is not bandwidth
constrained, it follows from (36) that ˜nu(d)/bu(d) = (l +
1)/b = (z − d + 1)/b. Substituting this into (40) yields
MTTDL(d + 1)
MTTDL(d)
≈ E(T|d + 1)
E(T|d)
·
m−z+d
Y
u=d+1
V −1
u
·
z − d + 1
z − d
m−z
> 1 .
(41)
For a clustered placement scheme that is not bandwidth
constrained, it follows from (38) that ˜nu(d)/bu(d) = (m −
u)/b. Substituting this into (40), and using (38), yields
MTTDL(d + 1)
MTTDL(d)
≈ E(T|d + 1)
E(T|d)
· m − d − 1
z − d − 1 > 1 .
(42)
Similarly, from (32) it follows that
E(QUFu+1 |d+1)
ld+1 E(T |d+1)
E(QUFu |d)
ld E(T |d)
≈
E(T|d)
E(T|d + 1) · ˜r(d + 1)
˜r(d)
·
u
Y
i=d+1
Vi · z − d − 1
m − u
· A ,
(43)
where
A =
(
(
z−d
z−d+1)u−d−1 ,
for symmetric placement
m−u
m−d−1 ,
for clustered placement .
(44)
5
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-944-7
CTRQ 2022 : The Fifteenth International Conference on Communication Theory, Reliability, and Quality of Service

(a) k = 64 (declustered data placement scheme)
(b) k = 16 (clustered data placement scheme)
Figure 1.
Normalized MTTDL vs. Ps for various MDS(m, l, d) codes ; n = 64, λ/µ = 0.0002, c = 12 TB, and s = 512 B.
TABLE II.
TYPICAL VALUES OF DIFFERENT PARAMETERS
Parameter
Definition
Values
n
number of storage devices
64
c
amount of data stored on each device
12 TB
l
user-data symbols per codeword
13, 14, 15
m
symbols per codeword
16
s
symbol (sector) size
512 B
b
rebuild bandwidth per device
50 MB/s
λ−1
mean time to failure of a storage device
300,000 h
to
1,000,000 h
U
amount of user data stored in the system
624 to 720 TB
µ−1
time to read an amount c of data at a rate b
from a storage device
66.7 h
It can be shown that
E(T |d)
E(T |d+1) · ˜r(d+1)
˜r(d)
< 1. Consequently,
from (34), (39), and (43), and recognizing that A < 1 and
E(QDF) = E(QUF˜r), it follows that
EAFDL(d + 1)
EAFDL(d)
< 1 .
(45)
Remark 2: Within the class of equivalent systems, accord-
ing to (42) and (45), deferring rebuilds improves reliability,
despite the fact that rebuilds are performed at the same number
of exposure levels. This is because increasing d amounts to
decreasing l, and therefore at a reduced number of symbols
read at each exposure level. This in turn results in reduced
vulnerability window and therefore improved reliability.
IV.
NUMERICAL RESULTS
Here, we assess the reliability of the clustered and declus-
tered schemes for a system comprised of n = 64 devices
(disks), where each device stores an amount c = 12 TB, and
m = 16, l = 13, 14, and 15, and the symbol size s is equal
to a sector size of 512 bytes.
Typical parameter values are listed in Table II. The Annu-
alized Failure Rate (AFR) is in the range of 0.9% to 3%, which
corresponds to a mean time to failure in the range of 300, 000
h to 1, 000, 000 h. The parameter λ−1 is chosen to be equal to
300, 000 h. It is assumed that the reserved rebuild bandwidth
b is equal to 50 MB/s, which yields a rebuild time of a device
µ−1 = c/b = 66.7 h, and that the network rebuild bandwidth
is sufficiently large (Bmax ≥ n b = 3.2 GB/s). We assume
that the rebuild time distribution is deterministic, such that
E(Xk) = [E(X)]k. The obtained results are accurate because
(7) is satisfied, given that λ/µ = 2.2 × 10−4 ≪ 1.
First, we assess the reliability for the declustered placement
scheme (k = n = 64) for various MDS-coded configurations
with m = 16 and varying values of l and d. These config-
urations are denoted by MDS(m,l,d) and the corresponding
results are shown in Figures 1, 2, and 3 by solid lines for
d = 0 (no lazy rebuild employed), dashed lines for d = 1
and dotted lines for d = 2. Six configurations are considered:
MDS(16,13,0), MDS(16,13,1), MDS(16,13,2), MDS(16,14,0),
MDS(16,14,1), and MDS(16,15,0), for each of the declus-
tered and clustered data placement schemes. In particular,
for the clustered placement scheme, the MDS(16,15,0) and
MDS(16,14,0) configurations correspond to the RAID-5 and
RAID-6 systems.
The normalized λ MTTDL measure is obtained from (13)
as a function of Ps and shown in Figure 1(a) for the declustered
data placement scheme. We observe that MTTDL decreases
monotonically with Ps and exhibits m − l − d plateaus. In
the interval [4.096 × 10−12, 5 × 10−9] of practical importance
for Ps, which is indicated between the two vertical dashed
lines, MTTDL is degraded by orders of magnitude. Increasing
the number of parities (reducing l) improves reliability by
orders of magnitude. By contrast, and according to Remark 1,
employing lazy rebuild degrades reliability by orders of magni-
tude. Moreover, for equivalent systems, such as MDS(16,15,0),
MDS(16,14,1) and MDS(16,13,2), and according to Remark 2,
MTTDL increases as d increases.
The normalized λ MTTDL measure for the clustered data
placement scheme is shown in Figure 1(b). We observe that the
declustered placement scheme achieves a significantly higher
MTTDL than the clustered one.
The normalized EAFDL/λ measure is obtained from (14)
and shown in Figure 2. We observe that EAFDL increases
monotonically, but it is practically unaffected in the interval
of interest because it degrades only when Ps is much larger
than the typical sector error probabilities. For the EAFDL
6
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-944-7
CTRQ 2022 : The Fifteenth International Conference on Communication Theory, Reliability, and Quality of Service

(a) k = 64 (declustered data placement scheme)
(b) k = 16 (clustered data placement scheme)
Figure 2.
Normalized EAFDL vs. Ps for various MDS(m, l, d) codes ; n = 64, λ/µ = 0.0002, c = 12 TB, and s = 512 B.
(a) k = 64 (declustered data placement scheme)
(b) k = 16 (clustered data placement scheme)
Figure 3.
Normalized E(H) vs. Ps for various MDS(m, l, d) codes ; n = 64, λ/µ = 0.0002, c = 12 TB, and s = 512 B.
metric too, increasing the number of parities (reducing l)
results in a reliability improvement by orders of magnitude.
By contrast, employing lazy rebuild degrades reliability by
orders of magnitude. Moreover, for equivalent systems, such as
MDS(16,15,0), MDS(16,14,1) and MDS(16,13,2), and accord-
ing to Remark 2, EAFDL decreases as d increases, but for the
clustered placement scheme it is not significantly affected. We
observe that for both MTTDL and EAFDL reliability metrics,
the reliability level achieved by the declustered data placement
scheme is higher than that of the clustered one.
The normalized expected amount E(H)/c of lost user data,
given that a data loss has occurred, relative to the amount of
data stored in a device is obtained from (15) and shown in
Figure 3. In contrast to the PDL, EAFDL, and E(Q) metrics
that increase monotonically with Ps, we observe that E(H)
does not do so. The reason for that is the following. For
Ps ≫ 10−14, data loss is more likely to be due to sector
errors than to device failures. Given that sector errors result in
a negligible amount of data loss compared with the substantial
data losses caused by device failures, when Ps increases
over the value of 10−14, the conditional amount of lost data
decreases. Clearly, this is reversed for high values of Ps, and
the conditional amount of lost data increases.
Also, in the interval [4.096 × 10−12, 5 × 10−9] of practical
importance for Ps, and by contrast to MTTDL and EAFDL,
employing lazy rebuild does not affect E(H) significantly.
Moreover, for equivalent systems, such as MDS(16,15,0),
MDS(16,14,1) and MDS(16,13,2), and for higher values of
d, E(H) is lower for the declustered data placement scheme,
but it is not significantly affected for the clustered one.
V.
CONCLUSIONS
The effect of the lazy rebuild scheme on the reliability
of erasure-coded data storage systems was investigated. A
methodology was developed for deriving the Mean Time to
Data Loss (MTTDL) and the Expected Annual Fraction of
7
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-944-7
CTRQ 2022 : The Fifteenth International Conference on Communication Theory, Reliability, and Quality of Service

Data Loss (EAFDL) reliability metrics analytically. Closed-
form expressions capturing the effect of unrecoverable latent
errors were obtained for the symmetric, clustered and declus-
tered data placement schemes. We demonstrated that system
reliability is significantly degraded by the employment of the
lazy rebuild scheme. We also demonstrated that the declustered
placement scheme offers superior reliability in terms of both
metrics. We established that, for realistic unrecoverable sector
error rates, MTTDL is adversely affected by the presence of
latent errors, whereas EAFDL is not. The analytical reliability
expressions derived here can identify lazy rebuild schemes that
reduce the volumes of repair traffic and at the same time ensure
a desired level of reliability.
Applying these results to assess the effect of network
rebuild bandwidth constraints is a subject of further inves-
tigation. The reliability evaluation of erasure-coded systems
when device failures, as well as unrecoverable latent errors
are correlated is also part of future work.
APPENDIX A
Proof of Proposition 1.
We consider the direct path −−→
UFu = 1 → 2 → · · · →
u → UF and proceed to evaluate PUFu(Rd+1, ⃗αu−1), the
probability of entering exposure level u through vector ⃗αu−1 ≜
(α1, . . . , αu−1) and given a rebuild time Rd+1, and then en-
countering an unrecoverable failure during the rebuild process
at this exposure level. It follows from (16) that
PUFu(Rd+1, ⃗αu−1) = Pu(Rd+1, ⃗αu−2)·Pu→UF(Rd+1, ⃗αu−1) .
(46)
To evaluate the above product, we first establish the fol-
lowing lemma.
LEMMA 1: For u = d + 1, . . . , ˜r − 1, it holds that
Pu(Rd+1, ⃗αu−2) ≈ (λbd+1Rd+1)u−d−1
u−1
Y
i=d+1
˜ni
bi
(Vi αi)u−1−i
(47)
with the convention that for any integer j and for any sequence
δi , Q0
i=j δi ≜ 1.
Proof: As the rebuild times are proportional to the amount
of data to be rebuilt and are inversely proportional to the
rebuild rates, it holds that
Rd+1
X
= Cd+1
C
b
bd+1
,
(48)
Using (8) and (10), (48) yields
Rd+1 ≈


d
Y
j=1
Vj


b
bd+1
X ,
(49)
Also,
Ru+1
Ru
= Cu+1
Cu
bu
bu+1
,
for u = d + 1, . . . , ˜r − 2 .
(50)
Combining (9) and (50) yields
Ru+1 ≈ Vu αu
bu
bu+1
Ru ,
for u = d + 1, . . . , ˜r − 2 . (51)
Repeatedly applying (51) yields
Ru ≈ bd+1
bu
Rd+1
u−1
Y
j=d+1
Vj αj ,
u = d + 1, . . . , ˜r − 1 .
(52)
We denote by ˜nu the number of devices at exposure level
u whose failure causes an exposure level transition to level
u + 1. Subsequently, the transition probability Pu→u+1 from
exposure level u to u + 1 depends on the duration of the
corresponding rebuild time Ru and the aggregate failure rate
of these ˜nu highly reliable devices, and is given by [19]
Pu→u+1 ≈ ˜nu λ Ru ,
for u = d + 1, . . . , ˜r − 1 .
(53)
Substituting (52) into (53) yields
Pu→u+1(Rd+1, ⃗αu−1) ≈ ˜nu λ bd+1
bu
Rd+1
u−1
Y
j=d+1
Vj αj .
(54)
The probability Pu of entering exposure level u can be
approximated by the probability of the direct path d + 1 →
d+2 → · · · → u of successive transitions from exposure level
d + 1 to u, that is,
Pu ≈
u−1
Y
i=d+1
Pi→i+1, ,
for u = d + 2, . . . , ˜r .
(55)
Substituting (54) into (55), and using the fact that Pd+1 = 1,
yields (47).
Given that the elements of ⃗αu−2 are independent random
variables approximately distributed according to (8), such that
E(αk
i ) ≈ 1/(k + 1) for i ≥ d + 1, we have
E
 u−1
Y
i=d+1
αu−1−i
i
!
=
u−1
Y
i=d+1
E(αu−1−i
i
)
≈
u−1
Y
i=d+1
1
u − i =
1
(u − d − 1)! .
(56)
Unconditioning (47) on ⃗αu−2 and using (56) yields
Pu(Rd+1) ≈ (λbd+1Rd+1)u−d−1
(u − d − 1)!
u−1
Y
i=d+1
˜ni
bi
V u−1−i
i
. (57)
Unconditioning (57) on Rd+1 and using (6) and (49) yields
(17).
We now proceed to calculate Pu→UF(Rd+1, ⃗αu−1). Upon
entering exposure level u, the rebuild process attempts to
restore the Cu most-exposed codewords, each of which has
m−u remaining symbols. The probability qu that a codeword
can be restored is determined by (21), which is Equation
(16) of [9]. Note that, if a codeword is corrupted, then at
least one of its l user-data symbols is lost. Owing to the
independence of symbol errors, codewords are independently
corrupted. Consequently, the conditional probability PUF|Cu
of encountering an unrecoverable failure during the rebuild
process of the Cu codewords is
PUF|Cu = 1 − qCu
u
,
for u = d + 1, . . . , ˜r .
(58)
Substituting (10) into (58) and using (20) yields
Pu→UF(Rd+1, ⃗αu−1) ≈ 1 − q
C Qu−1
j=1 Vj αj
u
= 1 − ˆq
Qu−1
u j=1 αj
. (59)
8
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-944-7
CTRQ 2022 : The Fifteenth International Conference on Communication Theory, Reliability, and Quality of Service

Substituting (59) into (46) yields
PUFu(Rd+1, ⃗αu−1) ≈ Pu(Rd+1, ⃗αu−2)

1 − ˆq
Qu−1
j=1 αj
u

.
(60)
Unconditioning (60) on ⃗αu−1, and using (8) and (47), yields
PUFu(Rd+1) ≈ Pu(Rd+1) − (λbd+1Rd+1)u−d−1
·
 u−1
Y
i=d+1
˜ni
bi V u−1−i
i
!
E⃗αu−1
" u−1
Y
i=d+1
αu−1−i
i
!
ˆq
Qu−1
j=d+1 αj
u
#
.
(61)
LEMMA 2: For αi distributed according to (8) it holds that
E
" u−1
Y
i=d+1
αu−1−i
i
!
q
Qu−1
i=d+1 αi
#
=
1
(u − d − 1)! + log(q)−(u−d−1)
 
q −
u−d−1
X
i=0
log(q)i
i!
!
. (62)
Proof: It holds that
q
Qu−1
i=d+1 αi = e log(q) Qu−1
i=d+1 αi =
∞
X
j=0
log(q)j (Qu−1
i=d+1 αi)j
j!
,
(63)
which implies that
 u−1
Y
i=d+1
αu−1−i
i
!
q
Qu−1
i=d+1 αi
=
 u−1
Y
i=d+1
αu−1−i
i
! 

∞
X
j=0
log(q)j (Qu−1
i=d+1 αi)j
j!


=
∞
X
j=0
log(q)j Qu−1
i=d+1 αu−1−i+j
i
j!
.
(64)
Consequently,
E
" u−1
Y
i=d+1
αu−1−i
i
!
q
Qu−1
i=d+1 αi
#
−
1
(u − d − 1)!
=
∞
X
j=0
log(q)j Qu−1
i=d+1 E(αu−1−i+j
i
)
j!
−
1
(u − d − 1)!
≈
∞
X
j=0
log(q)j Qu−1
i=d+1 E(αu−1−i+j
i
)
j!
−
1
(u − d − 1)!
≈
∞
X
j=0
log(q)j Qu−1
i=d+1
1
u−i+j
j!
−
1
(u − d − 1)!
=
∞
X
j=0
log(q)j
(u − d − 1 + j)! −
1
(u − d − 1)!
=
∞
X
j=1
log(q)j
(u − d − 1 + j)!
= log(q)−(u−d−1)
∞
X
i=u−d
log(q)i
i!
= log(q)−(u−d−1)
 ∞
X
i=0
log(q)i
i!
−
u−d−1
X
i=0
log(q)i
i!
!
= log(q)−(u−d−1)
 
elog(q) −
u−d−1
X
i=0
log(q)i
i!
!
.
(65)
From (57) and (62), (61) yields
PUFu(Rd+1) ≈ − (λbd+1Rd+1)u−d−1
 u−1
Y
i=d+1
˜ni
bi V u−1−i
i
!
· log(ˆqu)−(u−d−1)
 
ˆqu −
u−d−1
X
i=0
log(ˆqu)i
i!
!
. (66)
Unconditioning (66) on Rd+1, and using (6) and (49), yields
(19).
□
APPENDIX B
Proof of Proposition 2.
The expected number E(SU|Cu) of symbols lost due to
unrecoverable failures during the rebuild of the Cu codewords
at exposure level u is determined by Equation (53) of [9]:
E(SU|Cu) ≈ Cu ˜r
m − u
˜r − u

P ˜r−u
s
, Ps ≪
1
m − ˜r .
(67)
Substituting (10) into (67) yields
E(SU|⃗αu−1) ≈ C


u−1
Y
j=1
Vj αj

 ˜r
m − u
˜r − u

P ˜r−u
s
.
(68)
Subsequently, the expected number E(SUFu|Rd+1, ⃗αu−1)
of symbols lost due to unrecoverable failures encountered
during rebuild in conjunction with entering exposure level u
through vector ⃗αu−1, and given a rebuild time Rd+1, is
E(SUFu|Rd+1, ⃗αu−1) = Pu(Rd+1, ⃗αu−1) E(SU|⃗αu−1) .
(69)
Substituting (47) and (68) into (69) yields
E(SUFu|Rd+1, ⃗αu−1) ≈ (λbd+1Rd+1)u−d−1
" u−1
Y
i=d+1
˜ni
bi (Vi αi)u−i
#
· C
 
d
Y
j=1
Vj
!
˜r
 
m − u
˜r − u
!
P ˜r−u
s
,
Ps ≪
1
m − ˜r . (70)
Unconditioning (70) on ⃗αu−1, and given that (56) implies that
E(Qu−1
i=d+1 αu−i
i
)=1/(u − d)!, yields
E(SUFu|Rd+1) ≈ (λbd+1Rd+1)u−d−1
 u−1
Y
i=d+1
˜ni
bi V u−i
i
!
1
(u − d)!
· C
 
d
Y
j=1
Vj
!
˜r
 
m − u
˜r − u
!
P ˜r−u
s
,
Ps ≪
1
m − ˜r . (71)
Unconditioning (71) on Rd+1, and using (6) and (49), yields
E(SUFu) ≈
 
λ c
d
Y
j=1
Vj
!u−d−1
E(Xu−d−1)
[E(X)]u−d−1
 u−1
Y
i=d+1
˜ni
bi V u−i
i
!
·
1
(u − d)! C
 
d
Y
j=1
Vj
!
˜r
 
m − u
˜r − u
!
P ˜r−u
s
,
Ps ≪
1
m − ˜r .
(72)
Substituting (72) into (31) yields (32).
□
9
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-944-7
CTRQ 2022 : The Fifteenth International Conference on Communication Theory, Reliability, and Quality of Service

REFERENCES
[1]
D. A. Patterson, G. Gibson, and R. H. Katz, “A case for redundant
arrays of inexpensive disks (RAID),” in Proc. ACM Int’l Conference
on Management of Data (SIGMOD), Jun. 1988, pp. 109–116.
[2]
P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson,
“RAID: High-Performance, reliable secondary storage,” ACM Comput.
Surv., vol. 26, no. 2, Jun. 1994, pp. 145–185.
[3]
V. Venkatesan, I. Iliadis, C. Fragouli, and R. Urbanke, “Reliability of
clustered vs. declustered replica placement in data storage systems,”
in Proc. 19th Annual IEEE/ACM Int’l Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Jul. 2011, pp. 307–317.
[4]
I. Iliadis, D. Sotnikov, P. Ta-Shma, and V. Venkatesan, “Reliability of
geo-replicated cloud storage systems,” in Proc. 2014 IEEE 20th Pacific
Rim Int’l Symposium on Dependable Computing (PRDC), Nov. 2014,
pp. 169–179.
[5]
D. Ford et al., “Availability in globally distributed storage systems,”
in Proc. 9th USENIX Symposium on Operating Systems Design and
Implementation (OSDI), Oct. 2010, pp. 61–74.
[6]
A. G. Dimakis, K. Ramchandran, Y. Wu, and C. Suh, “A survey on
network coding for distributed storage,” Proc. IEEE, vol. 99, no. 3,
Mar. 2011, pp. 476–489.
[7]
C. Huang et al., “Erasure coding in Windows Azure Storage,” in Proc.
USENIX Annual Technical Conference (ATC), Jun. 2012, pp. 15–26.
[8]
S. Muralidhar et al., “f4: Facebook’s Warm BLOB Storage System,”
in Proc. 11th USENIX Symposium on Operating Systems Design and
Implementation (OSDI), Oct. 2014, pp. 383–397.
[9]
I. Iliadis, “Reliability assessment of erasure-coded storage systems with
latent errors,” in Proc. 14th Int’l Conference on Communication Theory,
Reliability, and Quality of Service (CTRQ), Apr. 2021, pp. 15–24.
[10]
A. Dholakia, E. Eleftheriou, X.-Y. Hu, I. Iliadis, J. Menon, and K. Rao,
“A new intra-disk redundancy scheme for high-reliability RAID storage
systems in the presence of unrecoverable errors,” ACM Trans. Storage,
vol. 4, no. 1, 2008, pp. 1–42.
[11]
I. Iliadis, “Reliability modeling of RAID storage systems with latent
errors,” in Proc. 17th Annual IEEE/ACM Int’l Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Sep. 2009, pp. 111–122.
[12]
V. Venkatesan and I. Iliadis, “Effect of latent errors on the reliability
of data storage systems,” in Proc. 21th Annual IEEE Int’l Symposium
on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), Aug. 2013, pp. 293–297.
[13]
K. V. Rashmi et al., “A ”Hitchhiker’s” guide to fast and efficient
data reconstruction in erasure-coded data centers,” in Proc. 2014 ACM
conference on SIGCOMM, Aug. 2014, pp. 331–342.
[14]
M. Silberstein, L. Ganesh, Y. Wang, L. Alvisi, and M. Dahlin,
“Lazy means smart: Reducing repair bandwidth costs in erasure-coded
distributed storage,” in Proc. 7th ACM Int’l Systems and Storage
Conference (SYSTOR), Jun. 2014, pp. 15:1–15:7.
[15]
I. Iliadis and V. Venkatesan, “Expected annual fraction of data loss as
a metric for data storage reliability,” in Proc. 22nd Annual IEEE Int’l
Symposium on Modeling, Analysis, and Simulation of Computer and
Telecommunication Systems (MASCOTS), Sep. 2014, pp. 375–384.
[16]
——, “Reliability evaluation of erasure coded systems,” Int’l J. Adv.
Telecommun., vol. 10, no. 3&4, Dec. 2017, pp. 118–144.
[17]
I. Iliadis, “Reliability evaluation of erasure coded systems under rebuild
bandwidth constraints,” Int’l J. Adv. Networks and Services, vol. 11,
no. 3&4, Dec. 2018, pp. 113–142.
[18]
V. Venkatesan, I. Iliadis, and R. Haas, “Reliability of data storage
systems under network rebuild bandwidth constraints,” in Proc. 20th
Annual IEEE Int’l Symposium on Modeling, Analysis, and Simula-
tion of Computer and Telecommunication Systems (MASCOTS), Aug.
2012, pp. 189–197.
[19]
V. Venkatesan and I. Iliadis, “A general reliability model for data storage
systems,” in Proc. 9th Int’l Conference on Quantitative Evaluation of
Systems (QEST), Sep. 2012, pp. 209–219.
[20]
DELL/EMC Whitepaper, ”PowerVault ME4 Series ADAPT Software,”
Feb. 2019. [Online]. Available: https://www.dellemc.com/ [retrieved:
March, 2022]
[21]
I. Iliadis, R. Haas, X.-Y. Hu, and E. Eleftheriou, “Disk scrubbing versus
intradisk redundancy for RAID storage systems,” ACM Trans. Storage,
vol. 7, no. 2, 2011, pp. 1–42.
[22]
V. Venkatesan and I. Iliadis, “Effect of codeword placement on the
reliability of erasure coded data storage systems,” in Proc. 10th Int’l
Conference on Quantitative Evaluation of Systems (QEST), Sep. 2013,
pp. 241–257.
[23]
——, “Effect of codeword placement on the reliability of erasure coded
data storage systems,” IBM Research Report, RZ 3827, Aug. 2012.
[24]
I. Iliadis and X.-Y. Hu, “Reliability assurance of RAID storage systems
for a wide range of latent sector errors,” in Proc. 2008 IEEE Int’l
Conference on Networking, Architecture, and Storage (NAS), Jun.
2008, pp. 10–19.
[25]
I. Iliadis and V. Venkatesan, “Most probable paths to data loss: An
efficient method for reliability evaluation of data storage systems,” Int’l
J. Adv. Syst. Measur., vol. 8, no. 3&4, Dec. 2015, pp. 178–200.
10
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-944-7
CTRQ 2022 : The Fifteenth International Conference on Communication Theory, Reliability, and Quality of Service

