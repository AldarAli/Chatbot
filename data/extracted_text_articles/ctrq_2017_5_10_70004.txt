Reliability Assessment of Erasure Coded Systems
Ilias Iliadis and Vinodh Venkatesan
IBM Research – Zurich
8803 R¨uschlikon, Switzerland
Email: {ili,ven}@zurich.ibm.com
Abstract—Replication is widely used to enhance the reliability
of storage systems and protect data from device failures. The
effectiveness of the replication scheme has been evaluated based
on the Mean Time to Data Loss (MTTDL) and the Expected
Annual Fraction of Data Loss (EAFDL) metrics. To provide high
data reliability at high storage efﬁciency, modern systems employ
advanced erasure coding redundancy and recovering schemes.
This article presents a general methodology for obtaining the
EAFDL and MTTDL of erasure coded systems analytically for
the symmetric, clustered and declustered data placement schemes.
Our analysis establishes that the declustered placement scheme
offers superior reliability in terms of both metrics. The analytical
results obtained enable the derivation of the optimal codeword
lengths that maximize the MTTDL and minimize the EAFDL.
Keywords–Reliability metric; MTTDL; EAFDL; RAID; MDS
codes; Information Dispersal Algorithm; Prioritized rebuild.
I.
INTRODUCTION
The reliability of storage systems is affected by data losses
due to device and component failures, including disk and node
failures. Permanent loss of data is prevented by deploying
redundancy schemes that enable data recovery. However, addi-
tional device failures that may occur during rebuild operations
could lead to permanent data losses. Over the years, several
redundancy and recovery schemes have been developed to
enhance the reliability of storage systems. These schemes
offer different levels of reliability, with varying corresponding
overheads due to the additional operations that need to be
performed, and different levels of storage efﬁciencies that
depend on the additional amount of redundant (parity) data
that needs to be stored in the system.
The effectiveness of the redundancy schemes has been
evaluated predominately based on the Mean Time to Data
Loss (MTTDL) metric. Closed-form reliability expressions are
typically obtained using Markov models with the underlying
assumption that the times to component failures and the rebuild
times are independent and exponentially distributed [1-13].
Recent work has shown that these results also hold in the
practical case of non-exponential failure time distributions.
This was achieved based on a methodology for obtaining
MTTDL that does not involve any Markov analysis [14]. The
MTTDL metric has been used extensively to assess tradeoffs,
to compare schemes and to estimate the effect of various
parameters on system reliability [15-18].
To cope with data losses encountered in the case of
distributed and cloud storage systems, data is replicated and
recovery mechanisms are used. For instance, Amazon S3 is
designed to provide 99.999999999% (eleven nines) durability
of data over a given year [19]. Similarly, Facebook [20],
LinkedIn [21] and Yahoo! [22] consider the amount of data
lost in given periods. To address this issue, a recent work
has introduced the Expected Annual Fraction of Data Loss
(EAFDL) metric [23]. It has also presented a methodology
for deriving this metric analytically in the case of replication-
based storage systems, where user data is replicated r times
and the copies are stored in different devices. As an alternative
to replication, storage systems use advanced erasure codes
that provide a high data reliability as well as a high storage
efﬁciency. The use of such erasure codes can be traced back
to as early as the 1980s when they were applied in systems
with redundant arrays of inexpensive disks (RAID) [1][2]. The
RAID-5, RAID-6 and replication-based systems are special
cases of erasure coded systems. State-of-the-art data storage
systems [24][25] employ more general erasure codes, where
the choice of the codes used greatly affects the performance,
reliability, and storage and reconstruction overhead of the
system. In this article, we focus on the reliability assessment
of erasure coded systems and how the choice of codes affects
the reliability in terms of the MTTDL and EAFDL metrics.
The MTTDL of erasure coded systems has been obtained
analytically in [26]. To reduce the amount of data lost, it is
imperative to assess not only the frequency of data loss events,
which is obtained through the MTTDL metric, but also the
amount of data lost, which is expressed by the EAFDL metric
[23]. The EAFDL and MTTDL metrics provide a useful proﬁle
of the size and frequency of data losses. Towards that goal, we
present a general framework and methodology for deriving
the EAFDL analytically, along with the MTTDL, for the
case of erasure coded storage systems. The model developed
captures the effect of the various system parameters as well
as the effect of various codeword placement schemes, such as
clustered, declustered, and symmetric data placement schemes.
The results obtained show that the declustered placement
scheme offers superior reliability in terms of both metrics. We
also investigate the effect of the codeword length and identify
the optimal values that offer the best reliability.
The remainder of the paper is organized as follows. Section
II describes the storage system model and the correspond-
ing parameters considered. Section III presents the general
framework and methodology for deriving the MTTDL and
EAFDL metrics analytically for the case of erasure coded
systems. Closed-form expressions for the symmetric, clus-
tered, and declustered placement schemes are derived. Section
IV compares these schemes and establishes that the declus-
tered placement scheme offers superior reliability. Section V
presents a thorough comparison of the reliability achieved
by the declustered placement scheme under various codeword
conﬁgurations. Finally, we conclude in Section VI.
II.
STORAGE SYSTEM MODEL
The storage system considered comprises n storage devices
(nodes or disks), with each device storing an amount c of data,
41
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-550-0
CTRQ 2017 : The Tenth International Conference on Communication Theory, Reliability, and Quality of Service

TABLE I.
NOTATION OF SYSTEM PARAMETERS
Parameter
Deﬁnition
n
number of storage devices
c
amount of data stored on each device
l
number of user-data symbols per codeword (l ≥ 1)
m
total number of symbols per codeword (m > l)
(l, m)
MDS-code structure
s
symbol size
k
spread factor of the data placement scheme
b
reserved rebuild bandwidth per device
1/λ
mean time to failure of a storage device
seff
storage efﬁciency of redundancy scheme (seff = l/m)
U
amount of user data stored in the system (U = seff n c)
˜r
minimum number of codeword symbols lost that lead to an irrecov-
erable data loss (˜r = m − l + 1 and 2 ≤ ˜r ≤ m)
1/µ
time to read (or write) an amount c of data at a rate b from (or to)
a device (1/µ = c/b)
such that the total storage capacity of the system is n c. Modern
data storage systems use various forms of data redundancy
to protect data from device failures. When devices fail, the
redundancy of the data affected is reduced and eventually lost.
To avoid irrecoverable data loss, the system performs rebuild
operations that use the data stored in the surviving devices
to reconstruct the temporarily lost data, thus maintaining the
initial data redundancy.
A. Redundancy
According to the erasure coded schemes considered, the
user data is divided into blocks (or symbols) of a ﬁxed size
(e.g., sector size of 512 bytes) and complemented with parity
symbols to form codewords. In this article, we consider (l, m)
maximum distance separable (MDS) erasure codes, which are a
mapping from l user data symbols to a set of m (> l) symbols,
called a codeword, in such a way that any subset containing
l of the m symbols of the codeword can be used to decode
(reconstruct, recover) the codeword. The corresponding storage
efﬁciency, seff, is given by
seff = l
m ,
(1)
such that the amount of user data, U, stored in the system is
given by
U = seff n c = l n c
m
.
(2)
The notation used is summarized in Table I. The parameters are
divided according to whether they are independent or derived,
and are listed in the upper and the lower part of the table,
respectively.
The m symbols of each codeword are stored on m distinct
devices, such that the system can tolerate any ˜r − 1 device
failures, but a number of ˜r device failures may lead to data
loss, with
˜r = m − l + 1 .
(3)
From the preceding, it follows that
1 ≤ l < m
and
2 ≤ ˜r ≤ m .
(4)
Examples of MDS erasure codes are the following:
Replication: A replication-based system with a replication
factor r can tolerate any loss of up to r − 1 copies of some
data, such that l = 1, m = r and ˜r = r. Also, its storage
efﬁciency is equal to s(replication)
eff
= 1/r.
RAID-5: A RAID-5 array comprised of N devices uses an
(N − 1, N)-MDS code, such that l = N − 1, m = N and
˜r = 2. It can therefore tolerate the loss of up to one device,
and its storage efﬁciency is equal to s(RAID-5)
eff
= (N − 1)/N.
RAID-6: A RAID-6 array comprised of N devices uses an
(N − 2, N)-MDS code, such that l = N − 2, m = N and
˜r = 3. It can therefore tolerate a loss of up to two devices,
and its storage efﬁciency is equal to s(RAID-6)
eff
= (N − 2)/N.
Reed–Solomon: It is based on (l, m)-MDS erasure codes.
B. Symmetric Codeword Placement
We consider a placement where each codeword is stored
on m distinct devices with one symbol per device. In a large
storage system, the number of devices, n, is typically much
larger than the codeword length, m. Therefore, there exist
many ways in which a codeword of m symbols can be stored
across a subset of the n devices. For each device in the
system, let its redundancy spread factor k denote the number
of devices over which the codewords stored on that device
are spread [26]. In a symmetric placement scheme, the m − 1
symbols of each codeword corresponding to the data on each
device are equally spread across k − 1 other devices, such
that these devices alltogether form a group of k devices. It
also holds that the m − 2 codeword symbols corresponding to
the codewords shared by any two devices within this group
are equally spread across k − 2 other devices, and so on.
Consequently, all the symbols of each codeword in the system
are contained within such a group, which implies that the
system is effectively comprised of n/k disjoint groups of k
devices. Each group contains an amount U/k of user data, with
the corresponding codewords placed on the corresponding k
devices in a distributed manner. We proceed by considering
the clustered and declustered placement schemes, which are
special cases of symmetric placement schemes for which k is
equal to m and n, respectively.
1) Clustered Placement: In this placement scheme, the n
devices are divided into disjoint sets of m devices, referred
to as clusters. According to the clustered placement, each
codeword is stored across the devices of a particular cluster. In
such a placement scheme, it can be seen that no cluster stores
the redundancies that correspond to data stored on another
cluster. The entire storage system can essentially be modeled
as consisting of n/m independent clusters. In each cluster,
data loss occurs when ˜r devices fail successively before rebuild
operations complete successfully.
2) Declustered Placement: In this placement scheme, all

1) Exposure Levels and Amount of Data to Rebuild: At
time t, let Dj(t) be the number of codewords that have lost
j symbols, with 0 ≤ j ≤ ˜r. The system is at exposure level e
(0 ≤ e ≤ ˜r), where
e =
max
Dj(t)>0 j.
(5)
In other words, the system is at exposure level e if there are
codewords with m−e symbols left, but there are no codewords
with fewer than m − e symbols left in the system, that is,
De(t) > 0, and Dj(t) = 0 for all j > e. These codewords are
referred to as the most-exposed codewords. Let the number of
most-exposed codewords when entering exposure level e be
denoted by Ce, e = 1, . . . , ˜r. At t = 0, Dj(0) = 0 for all
j > 0 and D0(0) is the total number of codewords stored in
the system. Device failures and rebuild processes cause the
values of D1(t), · · · , D˜r(t) to change over time, and when a
data loss occurs, D˜r(t) > 0. Device failures cause transitions
to higher exposure levels, whereas rebuilds cause transitions
to lower ones.
In this article, we will derive the reliability metrics of inter-
est using the direct path approximation, which considers only
transitions from lower to higher exposure levels [14][26][27].
This implies that each exposure level is entered only once.
2) Prioritized or Intelligent Rebuild: At each exposure
level e, the prioritized or intelligent rebuild process attempts
to bring the system back to exposure level e−1 by recovering
one of the e symbols that each of the most-exposed codewords
has lost, that is, by recovering a total number of Ce symbols.
Let Ae denote the amount of data corresponding to the Ce
symbols and let s denote the symbol size. Then, it holds that
Ae = Ce s .
(6)
The notation used is summarized in Table II. For an exposure
level e (< ˜r), Ae represents the amount of data that needs to
be rebuilt at that exposure level. In particular, upon the ﬁrst-
device failure, it holds that
A1 = c .
(7)
D. Rebuild Process
During the rebuild process, a certain proportion of the
device bandwidth is reserved for data recovery, with b denoting
the actual reserved rebuild bandwidth per device. The rebuild
bandwidth is usually only a fraction of the total bandwidth
available at each device; the remainder is used to serve user
requests. Let us denote by be (≤ b) the rate at which the
amount Ae of data that needs to be rebuilt at exposure level e
is written to selected device(s). In particular, let us denote by
1/µ the time required to read (or write) an amount c of data
from (or to) a device, given by
1
µ = c
b .
(8)
E. Failure and Rebuild Time Distributions
In this work, we assume that the lifetimes of the n devices
are independent and identically distributed, with a cumulative
distribution function Fλ(.) and a mean of 1/λ. We further
consider storage devices with failure time distributions that
belong to the large class deﬁned in [14], which includes
real-world distributions, such as Weibull and gamma, as well
TABLE II.
NOTATION OF SYSTEM PARAMETERS AT EXPOSURE LEVELS
Parameter
Deﬁnition
e
exposure level
Ce
number of most-exposed codewords when entering exposure level
e
Re
rebuild time at exposure level e
Pe→e+1
transition probability from exposure level e to e + 1
˜ne
number of devices at exposure level e whose failure causes an
exposure level transition to level e + 1
αe
fraction of the rebuild time Re still left when another device fails
causing the exposure level transition e → e + 1
Ve
fraction of the most-exposed codewords that have symbols stored
on another of the ˜ne devices
Ae
amount of data corresponding to the Ce symbols (Ae = Ce s)
be
rate at which recovered data is written at exposure level e
as exponential distributions. The storage devices are highly
reliable when the ratio of the ﬁxed time 1/µ to read all
contents of a device (which typically is on the order of tens of
hours) to the mean time to failure of a device 1/λ (which is
typically at least on the order of thousands of hours) is small,
that is, when
λ
µ = λ c
b
≪ 1 .
(9)
According to [14][26], when the cumulative distribution
function Fλ satisﬁes the condition
µ
Z 1/µ
0
Fλ(t) dt ≪ 1,
with λ
µ ≪ 1 ,
(10)
the MTTDL reliability metric of replication-based or erasure
coded storage systems tends to be insensitive to the device
failure distribution, that is, the MTTDL depends only on its
mean 1/λ, but not on its density Fλ(.). In [23], it was shown
that this also holds for the EAFDL metric in the case of
replication-based storage systems, and in this article, we will
show that this is also the case for erasure coded systems.
III.
DERIVATION OF MTTDL AND EAFDL
We brieﬂy review the general methodology for deriving the
MTTDL and EAFDL metrics presented in [23]. This method-
ology does not involve any Markov analysis and holds for
general failure time distributions, which can be exponential or
non-exponential, such as the Weibull and gamma distributions.
At any point of time, the system can be thought to be in
one of two modes: normal mode and rebuild mode. During
normal mode, all data in the system has the original amount
of redundancy and there is no active rebuild process. During
rebuild mode, some data in the system has less than the original
amount of redundancy and there is an active rebuild process
that is trying to restore the lost redundancy. A transition from
normal mode to rebuild mode occurs when a device fails;
we refer to the device failure that causes this transition as a
ﬁrst-device failure. Following a ﬁrst-device failure, a complex
sequence of rebuild operations and subsequent device failures
may occur, which eventually leads the system either to an
irrecoverable data loss (DL) with probability PDL or back to the
original normal mode by restoring initial redundancy, which
occurs with probability 1 − PDL. The MTTDL is then given
by [23]
MTTDL ≈
1
n λ PDL
.
(11)
43
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-550-0
CTRQ 2017 : The Tenth International Conference on Communication Theory, Reliability, and Quality of Service

Let H denote the corresponding amount of data lost condi-
tioned on the fact that a data loss has occurred. The metric
of interest, that is, the Expected Annual Fraction of Data Loss
(EAFDL), is subsequently obtained as the ratio of the expected
amount of data lost to the expected time to data loss normalized
to the amount of user data:
EAFDL =
E(H)
MTTDL · U ,
(12)
with the MTTDL expressed in years. Let us also denote by
Q the unconditional amount of data lost upon a ﬁrst-device
failure. Note that Q is unconditional on the event of a data
loss occurring in that it is equal either to H if the system
suffers a data loss prior to returning to normal operation or to
zero otherwise, that is,
Q =
H ,
if DL
0 ,
if no DL .
(13)
Therefore, the expected amount of data lost, E(Q), upon a
ﬁrst-device failure is given by
E(Q) = PDL E(H) .
(14)
From (11), (12) and (14), we obtain the EAFDL as follows:
EAFDL ≈ n λ E(Q)
U
,
(15)
with 1/λ expressed in years.
A. Reliability Analysis
From (11) and (15), it follows that the derivation of the
MTTDL and EAFDL metrics requires the evaluation of the
PDL and E(Q), respectively. These quantities are derived
by considering the direct path approximation [12][13][27],
which, under conditions (9) and (10), accurately assesses the
reliability metrics of interest [14][23].
Next, we present the general outline of the methodology
in more detail.
1) Direct Path to Data Loss: Consider the direct path
of successive transitions from exposure level 1 to ˜r. In
[12][13][27], it was shown that PDL can be approximated by
the probability of the direct path to data loss, PDL,direct, that is,
PDL ≈ PDL,direct =
r−1
Y
e=1
Pe→e+1,
(16)
where Pe→e+1 denotes the transition probability from exposure
level e to e + 1. The above approximation holds when storage
devices are highly reliable, that is, it holds for arbitrary device
failure and rebuild time distributions that satisfy conditions (9)
and (10). In this case, the relative error tends to zero as λ/µ
tends to zero [14].
As the direct path to data loss dominates the effect of all
other possible paths to data loss considered together, it follows
that the amount of data lost H can be approximated by that
corresponding to the direct path:
H ≈ Hdirect .
(17)
Also, from (13) and (17) it follows that
Q ≈
Hdirect ,
if DL follows the direct path
0 ,
otherwise .
(18)
Consequently, to derive the amount of data lost, it sufﬁces to
proceed by considering the H and Q metrics corresponding to
the direct path to data loss.
Note that the amount of data lost, H, is the amount
of user data stored in the most-exposed codewords when
entering exposure level ˜r, which can no longer be recovered
and therefore is irrecoverably lost. As the number of these
codewords is equal to C˜r and each of these codewords contains
l symbols of user data, it holds that
H = C˜r l s
(6)
= l A˜r .
(19)
2) Amount of Data to Rebuild and Rebuild Times at Each
Exposure Level: We now proceed to derive the conditional
values of the random variables of interest given that the system
goes through this direct path to data loss. Let Re denote the
rebuild times of the most-exposed codewords at each exposure
level in this path, and let αe be the fraction of the rebuild time
Re still left when another device fails causing the exposure
level transition e → e + 1. In [28, Lemma 2], it was shown
that, for highly reliable devices satisfying conditions (9) and
(10), αe is approximately uniformly distributed between zero
and one, that is,
αe ∼ U(0, 1),
e = 1, . . . , ˜r − 1 .
(20)
Let ⃗α denote the vector (α1, . . . , α˜r−1), ⃗αe the vector
(α1, . . . , αe), ⃗Ce the vector (C1, · · · , Ce) and ⃗Ae the vector
(A1, · · · , Ae). Clearly, for the rebuild schemes considered, the
fraction αe of the rebuild time Re still left also represents the
fraction of the most-exposed codewords not yet recovered upon
the next device failure. Therefore, the number of most-exposed
codewords that are not yet recovered is equal to αe Ce. Clearly,
the fraction Ve of these codewords that have symbols stored on
the newly failed device depends on the codeword placement
scheme. Consequently, the number of most-exposed codewords
when entering exposure level e + 1 is given by
Ce+1 = Ve αe Ce, ,
e = 1, . . . , ˜r − 1 ,
(21)
and by virtue of (6), the corresponding amount of data that is
not yet rebuilt is given by
Ae+1 = Ve αe Ae, ,
e = 1, . . . , ˜r − 1 ,
(22)
with Ve depending only on the placement scheme.
Repeatedly applying (22) and using (7) yields
Ae = c
e−1
Y
j=1
Vj αj .
(23)
Remark 1: From (23), it follows that the expected amount
of data to be rebuilt at each exposure level does not depend
on the duration of the rebuild times.
At exposure level 1, according to (7), the amount A1 of data
to be recovered is equal to c. Given that this data is recovered
at a rate of b1 and that the time required to write an amount c
of data at a rate of b is equal to 1/µ, it follows that the rebuild
time R1 is given by
R1 = b
b1
· 1
µ .
(24)
44
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-550-0
CTRQ 2017 : The Tenth International Conference on Communication Theory, Reliability, and Quality of Service

As the rebuild times are proportional to the amount of data
to be rebuilt and are inversely proportional to the rebuild rates,
it holds that
Re+1
Re
= Ae+1
Ae
·
be
be+1
,
e ≥ 1 .
(25)
Using (22), (25) yields
Re+1 = Ve αe
be
be+1
Re ,
e = 1, . . . , ˜r − 2 ,
(26)
or
Re = Ge−1 αe−1 Re−1 ,
e = 2, . . . , ˜r − 1 ,
(27)
where
Ge ≜
be
be+1
Ve ,
e = 1, . . . , ˜r − 2 .
(28)
Repeatedly applying (27) and using (28) yields
Re = b1
be
R1
e−1
Y
j=1
Vj αj ,
e = 1, . . . , ˜r − 1 .
(29)
Let ˜ne be the number of devices at exposure level e whose
failure before the rebuild of the most-exposed codewords
causes an exposure level transition to level e+1. Subsequently,
the transition probability Pe→e+1 from exposure level e to
e+1 depends on the duration of the corresponding rebuild time
Re and the aggregate failure rate of these ˜ne highly reliable
devices, and is given by [14]
Pe→e+1 ≈ ˜ne λ Re ,
for e = 1, . . . , ˜r − 1 .
(30)
Substituting (29) into (30) yields
Pe→e+1(⃗αe−1) ≈ ˜ne λ b1
be
R1
e−1
Y
j=1
Vj αj .
(31)
3) Estimation of PDL: Consider the direct path 1 → 2 →
· · · → ˜r of successive transitions from exposure level 1 to ˜r.
For ease of reading, we denote the successive transitions from
exposure level e to ˜r by e → ˜r. We ﬁrst evaluate PDL, the
probability of data loss. From (16), and using the fact that αe
does not depend on R1, α1, · · · , αe−1, it follows that
PDL ≈ P1→˜r = P1→2P2→˜r = P1→2Eα1[P2→˜r(α1)]
= P1→2Eα1[P2→3(α1)P3→˜r(α1)]
= P1→2Eα1[P2→3(α1)Eα2|α1[P3→˜r(α1, α2)]]
= · · ·
= P1→2Eα1[P2→3(⃗α1)Eα2[P3→4(⃗α2) · · ·
· · · Eα˜r−2[P˜r−1→˜r(⃗α˜r−2)] · · · ]
= E⃗α˜r−2[P1→2P2→3(⃗α1) · · · P˜r−1→˜r(⃗α˜r−2)]
= E⃗α˜r−2
"˜r−1
Y
e=1
Pe→e+1(⃗αe−1)
#
= E⃗α˜r−2[PDL(⃗α˜r−2)] ,
(32)
where
PDL(⃗α˜r−2) ≜
˜r−1
Y
e=1
Pe→e+1(⃗αe−1) ,
(33)
with
P1→2(⃗α0) ≜ P1→2 .
(34)
Substituting (31) into (33), and using (30) and (34), yields
PDL(⃗α˜r−2) ≈ (λ b1 R1)˜r−1
˜r−1
Y
e=1
˜ne
be
(Ve αe)˜r−1−e .
(35)
Unconditioning (35) on ⃗α˜r−2, and given that the elements
of ⃗α˜r−2 are independent random variables approximately
distributed according to (20) such that E(αk
e) ≈ 1/(k + 1),
(32) yields
PDL ≈ (λ b1 R1)˜r−1
1
(˜r − 1)!
˜r−1
Y
e=1
˜ne
be
V ˜r−1−e
e
.
(36)
Using (8) and (24), (36) yields
PDL ≈ (λ c)˜r−1
1
(˜r − 1)!
˜r−1
Y
e=1
˜ne
be
V ˜r−1−e
e
.
(37)
4) Estimation of E(Q): We now proceed to evaluate E(Q),
the expected amount of data lost. Considering the direct path
1 → 2 → · · · → ˜r of successive transitions from exposure
level 1 to ˜r, it follows from (18) and the fact that αe does not
depend on R1, α1, · · · , αe−1, that
E(Q) ≈ P1→2E(Q|1 → 2)
= P1→2Eα1|R1[E(Q|α1)]
= P1→2Eα1[P2→3(α1)E(Q|α1, 2 → 3)]
= P1→2Eα1[P2→3(α1)Eα2|α1[E(Q|α1, α2)]]
= · · ·
= P1→2Eα1[P2→3(⃗α1)Eα2[P3→4(⃗α2) · · ·
· · · P˜r−1→˜r(⃗α˜r−2)Eα˜r−1(Q|⃗α˜r−1)] · · · ]
= E⃗α˜r−1[P1→2P2→3(⃗α1) · · · P˜r−1→˜r(⃗α˜r−2)
E(Q|⃗α˜r−1)]
(17)(18)
=
E⃗α˜r−1
" ˜r−1
Y
e=1
Pe→e+1(⃗αe−1)
!
E(H|⃗α˜r−1)
#
(33)
= E⃗α˜r−1[PDL(⃗α˜r−2) E(H|⃗α˜r−1)]
(19)
= E⃗α˜r−1[PDL(⃗α˜r−2) E(l A˜r|⃗α˜r−1)]
= E⃗α˜r−1[PDL(⃗α˜r−2) l E(A˜r|⃗α˜r−1)]
= E⃗α˜r−1[G(⃗α˜r−1)] ,
(38)
where
G(⃗α˜r−1) ≜ l PDL(⃗α˜r−2) E(A˜r|⃗α˜r−1) .
(39)
Using (23) and (35), (39) yields
G(⃗α˜r−1) ≈ l c (λ b1 R1)˜r−1
˜r−1
Y
e=1
˜ne
be
(Ve αe)˜r−e .
(40)
Unconditioning (40) on ⃗α˜r−1, and given that the elements
of ⃗α˜r−1 are independent random variables approximately
distributed according to (20) such that E(αk
e) ≈ 1/(k + 1),
(38) yields
E(Q) ≈ l c (λ b1 R1)˜r−1 1
˜r !
˜r−1
Y
e=1
˜ne
be
V ˜r−e
e
.
(41)
Using (8) and (24), (41) yields
E(Q) ≈ l c (λ c)˜r−1 1
˜r !
˜r−1
Y
e=1
˜ne
be
V ˜r−e
e
.
(42)
45
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-550-0
CTRQ 2017 : The Tenth International Conference on Communication Theory, Reliability, and Quality of Service

5) Evaluation of E(H): The expected amount E(H) of
data lost conditioned on the fact that a data loss has occurred is
obtained from (14) as the ratio of E(Q) to PDL. Consequently,
using (37) and (42), it follows that
E(H) = E(Q)
PDL
≈
 
l
˜r
˜r−1
Y
e=1
Ve
!
c .
(43)
Remark 2: From (43), it follows that the expected amount
of data lost conditioned on the fact that a data loss has occurred
does not depend on the duration of the rebuild times.
6) Evaluation of MTTDL and EAFDL: Substituting (37)
into (11) yields
MTTDL ≈
1
n λ
(˜r − 1)!
(λ c)˜r−1
˜r−1
Y
e=1
be
˜ne
1
V ˜r−1−e
e
.
(44)
Substituting (2) and (42) into (15) yields
EAFDL ≈ m λ (λ c)˜r−1 1
˜r !
˜r−1
Y
e=1
˜ne
be
V ˜r−e
e
.
(45)
B. Symmetric Scheme
Here, we consider the case where the redundancy spread
factor k is in the interval m < k ≤ n. As discussed in
Section II-C2, at each exposure level e, the prioritized rebuild
process recovers one of the e symbols that each of the Ce
most-exposed codewords has lost by reading m − ˜r + 1 of
the remaining symbols. Thus, there are Ce symbols to be
recovered in total, which corresponds to an amount Ae of data.
For the symmetric placement discussed in Section II-B, these
symbols are recovered by reading (m − ˜r + 1) Ce symbols,
which corresponds to an amount (m− ˜r +1) Ae of data, from
the k − e surviving devices in the affected group. Note that
these are precisely the devices at exposure level e whose failure
before the rebuild of the most-exposed codewords causes an
exposure level transition to level e + 1. Consequently, it holds
that
˜nsym
e
= k − e .
(46)
Furthermore, the recovered symbols are written to the spare
space of these devices in such a way that no symbol is written
to a device in which another symbol corresponding to the same
codeword is already present. Owing to the symmetry of the
symmetric placement, the same amount of data is being read
from each of the ˜ne devices. Similarly, the same amount of
data is being written to each of the ˜ne devices. Consequently,
the total read/write rebuild bandwidth b of each device is split
between the reads and the writes, with the read rate being equal
to (m − ˜r + 1) b/(m − ˜r + 2) and the write rate being equal
to b/(m − ˜r + 2). Therefore, the total write bandwidth, which
is also the rebuild rate be, is given by
bsym
e
=
˜nsym
e
m − ˜r + 2 b ,
e = 1, . . . , ˜r − 1 .
(47)
Once all lost codeword symbols have been recovered, they are
transferred to a new replacement device.
When the system enters exposure level e, the number of
most-exposed codewords that need to be recovered is equal to
Ce, e = 1, . . . , ˜r. Upon the next device failure, the expected
number of most-exposed codewords that are not yet recovered
is equal to αe Ce. Owing to the nature of the symmetric
codeword placement, the newly failed device stores codeword
symbols corresponding to only a fraction
V sym
e
= m − e
k − e ,
e = 1, . . . , ˜r − 1 .
(48)
of these most-exposed, not yet recovered codewords.
Substituting (46), (47) and (48) into (44), (45) and (43),
and using (3) yields
MTTDLsym
k
≈
1
n λ

b
(l + 1) λ c
m−l
(m − l)!
m−l
Y
e=1
 k − e
m − e
m−l−e
,
(49)
EAFDLsym
k
≈ λ
(l + 1) λ c
b
m−l
m
(m − l + 1)!
m−l
Y
e=1
m − e
k − e
m−l+1−e
,
(50)
and
E(H)sym
k
≈
 
l
m − l + 1
m−l
Y
e=1
m − e
k − e
!
c
(51)
= l (m − 1)! (k − m + l − 1)!
(m − l + 1) (k − 1)! (l − 1)! c .
(52)
Note that for a replication-based system, for which m = r
and l = 1, (49) and (50) are in agreement with (42.b) and
(43.b) of [23], respectively.
Remark 3: From (49), (50), and (51), it follows that
MTTDLsym
k
depends on n, but EAFDLsym
k
and E(H)sym
k
do
not.
Remark 4: From (49), (50), and (51), it follows that, for
m − l = 1, MTTDLsym
k
does not depend on k, whereas for
m−l > 1, MTTDLsym
k
is increasing in k. Also, for m−l ≥ 1,
EAFDLsym
k
and E(H)sym
k
are decreasing in k. Consequently,
within the class of symmetric placement schemes considered,
that is, for m < k ≤ n, the MTTDLsym
k
is maximized and the
EAFDLsym
k
and the E(H)sym
k
are minimized when k = n.
C. Clustered Placement
As discussed in Section II-B1, in the clustered placement
scheme, the n devices are divided into disjoint sets of m
devices, referred to as clusters. According to the clustered
placement, each codeword is stored across the devices of a
particular cluster. At each exposure level e, the rebuild process
recovers one of the e symbols that each of the Ce most-exposed
codewords has lost by reading m − ˜r + 1 of the remaining
symbols. Note that the remaining symbols are stored on the
m − e surviving devices in the affected group. As these are
precisely the devices at exposure level e whose failure before
the rebuild of the most-exposed codewords causes an exposure
level transition to level e + 1, it holds that
˜nclus
e
= m − e .
(53)
46
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-550-0
CTRQ 2017 : The Tenth International Conference on Communication Theory, Reliability, and Quality of Service

The rebuild process in clustered placement recovers the lost
symbols by reading m − ˜r + 1 symbols from m − ˜r + 1 of the
˜ne surviving devices of the affected cluster. The lost symbols
are computed on-the-ﬂy and written to a spare device using
the rebuild bandwidth at a rate of b. Consequently, it holds
that
bclus
e
= b ,
e = 1, . . . , ˜r − 1 .
(54)
When the system enters exposure level e, the number of
most-exposed codewords that need to be recovered is equal to
Ce, e = 1, . . . , ˜r. Upon the next device failure, the expected
number of most-exposed codewords that have not yet been
recovered is equal to αe Ce. Clearly, all these codewords have
symbols stored on the newly failed device, which implies that
V clus
e
= 1 ,
e = 1, . . . , ˜r − 1 .
(55)
Substituting (53), (54) and (55) into (44), (45) and (43),
and using (3) yields
MTTDLclus ≈
1
n λ
 b
λ c
m−l
1

B. Minimizing EAFDL
From (57) and (60), it follows that
EAFDLdeclus
EAFDLclus ≈ (l+1)m−l (l − 1)!
(m − 1)!
m−l
Y
e=1
m − e
n − e
m−l+1−e
.
(69)
Remark 7: From (69), it follows that the placement that
minimizes EAFDL does not depend on λ, b and c.
Depending on the values of m and l, we consider the
following two cases:
1) m − l = 1: For m − l = 1, (69) yields
EAFDLdeclus
EAFDLclus ≈
m
n − 1
= 1 for n = m + 1
< 1 for n ≥ m + 2 .
(70)
2) m − l ≥ 2: For m − l ≥ 2, (69) can be written as
follows:
EAFDLdeclus
EAFDLclus ≈ l + 1
m − 1
l + 1
m − 2 · · · l + 1
l
l
n − m + l
m−l−1
Y
e=1
m − e
n − e
m−l+1−e
.
(71)
Using (63), (71) yields
EAFDLdeclus
EAFDLclus
<
l + 1
n − m + l ≤
l + 1
(m + 1) − m + l = 1 .
(72)
Remark 8: From the preceding, it follows that the EAFDL
is minimized by the declustered placement scheme. In partic-
ular, when m − l = 1 and n = m + 1, the clustered and
declustered placement schemes yield the same EAFDL.
C. Minimizing E(H)
From (58) and (61), and using (63), it follows that
E(H)declus
E(H)clus ≈
m−l
Y
e=1
m − e
n − e
< 1 .
(73)
Remark 9: From (73), it follows that the declustered place-
ment minimizes E(H) for any λ, b, c.
D. Synopsis
We summarize our ﬁndings regarding the reliability offered
by the data placement schemes as follows. Independently of
the device reliability characteristics and mean expressed by
1/λ, the reserved rebuild bandwidth b and the device capacity
c, the declustered placement scheme minimizes the expected
amount of data lost when loss occurs. Also, for m − l =
1, the clustered placement scheme maximizes the MTTDL,
but the declustered placement scheme minimizes the EAFDL.
However, for m − l ≥ 2, and for practical values of n and m,
the declustered placement scheme maximizes the MTTDL and
at the same time minimizes the EAFDL.
V.
RELIABILITY COMPARISON
Here, we assess the relative reliability of the declustered
placement, which according to Remarks 6, 8 and 9 is the
optimal one, under various codeword lengths m. We perform a
fair comparison by considering systems with the same amount
of user data, U, stored under the same storage efﬁciency, seff.
From (2), it follows that the number of devices n is ﬁxed.
Also, from (1) it follows that
m − l = (1 − seff) m = h m ,
(74)
where h is given by
h ≜ 1 − seff
(75)
and is ﬁxed.
Using (74) to substitute l in (59) and (60) yields
MTTDLdeclus ≈
1
n λ

b
[(1 − h)m + 1] λ c
hm
(hm)!
hm
Y
e=1
 n − e
m − e
hm−e
,
(76)
and
EAFDLdeclus ≈ λ
[(1 − h)m + 1] λ c
b
hm
m
(hm + 1)!
hm
Y
e=1
m − e
n − e
hm+1−e
.
(77)
As
discussed
in
Section
III-A,
the
direct-path-
approximation method yields accurate results when the
storage devices are highly reliable, that is, when the ratio λ/µ
of the mean rebuild time 1/µ to the mean time to failure of a
device 1/λ is very small. We proceed by considering systems
for which it holds that λ/µ = λ c/b = 0.001. The combined
effect of the number of devices and the system efﬁciency on
the λ MTTDLdeclus measure is obtained by (76) and shown in
Figure 1 as a function of the codeword length. The values for
the storage efﬁciency are chosen to be fractions of the form
z/(z + 1), z = 1, . . . , 7, such that the ﬁrst point of each of
the corresponding curves is associated with the single-parity
(z, z + 1)-erasure code, and the second point of each of the
corresponding curves is associated with the double-parity
(2z, 2z + 2)-erasure code. We observe that the MTTDL
increases as the storage efﬁciency seff decreases. This is
because, for a given m, decreasing seff implies decreasing l,
which in turn implies increasing the parity symbols m − l and
consequently improving MTTDL.
Let us now consider the single-parity codewords, which
correspond to the ﬁrst points of the curves. As seff increases,
so do m and l, which results in a decreasing MTTDL for these
codewords. This is due to the fact that as m increases, there
are l data symbols, that is, more data symbols associated with
each parity. This is in accordance with the results presented
in Figure 2 of [26]. We observe that the same applies for
the double-parity codewords, which correspond to the second
points of the curves.
The combined effect of the number of devices and the
system efﬁciency on the EAFDLdeclus/λ measure is obtained
48
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-550-0
CTRQ 2017 : The Tenth International Conference on Communication Theory, Reliability, and Quality of Service

0
5
10
15
20
10
0
10
5
10
10
10
15
10
20
10
25
m
λ MTTDL
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(a) n = 20
0
10
20
30
40
10
0
10
10
10
20
10
30
10
40
10
50
m
λ MTTDL
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(b) n = 40
0
10
20
30
40
50
60
10
0
10
20
10
40
10
60
10
80
m
λ MTTDL
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(c) n = 60
Figure 1.
MTTDLdeclus vs. codeword length for seff = 1/2, 2/3, 3/4, 4/5, 5/6, 7/8; λ/µ = 0.001.
0
5
10
15
20
10
−25
10
−20
10
−15
10
−10
10
−5
m
EAFDL / λ
 
 
seff= 1/2
seff= 2/3
seff= 3/4
seff= 4/5
seff= 5/6
seff= 6/7
seff= 7/8
(a) n = 20
0
10
20
30
40
10
−50
10
−40
10
−30
10
−20
10
−10
m
EAFDL / λ
 
 
seff= 1/2
seff= 2/3
seff= 3/4
seff= 4/5
seff= 5/6
seff= 6/7
seff= 7/8
(b) n = 40
0
10
20
30
40
50
60
10
−80
10
−60
10
−40
10
−20
m
EAFDL / λ
 
 
seff= 1/2
seff= 2/3
seff= 3/4
seff= 4/5
seff= 5/6
seff= 6/7
seff= 7/8
(c) n = 60
Figure 2.
EAFDLdeclus vs. codeword length for seff = 1/2, 2/3, 3/4, 4/5, 5/6, 7/8; λ/µ = 0.001.
20
40
60
80
100
120
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of Nodes (n)
r*
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(a) MTTDL
20
40
60
80
100
120
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of Nodes (n)
r*
 
 
seﬀ = 1/2
seﬀ = 2/3
seﬀ = 3/4
seﬀ = 4/5
seﬀ = 5/6
seﬀ = 6/7
seﬀ = 7/8
(b) EAFDL
Figure 3.
r∗ vs. number of devices for seff = 1/2, 2/3, 3/4, 4/5, 5/6, 7/8; λ/µ = 0.001.
by (77) and shown in Figure 2 as a function of the codeword
length. We observe that the EAFDL increases as the storage
efﬁciency seff increases. Also, as seff increases, the EAFDL
for the single-parity codewords, which correspond to the ﬁrst
points of the curves, also increases. We observe that the same
applies for the double-parity codewords, which correspond to
the second points of the curves.
We now proceed to identify the optimal codeword length,
m∗, that maximizes the MTTDL or minimizes the EAFDL
for a given storage efﬁciency. The optimal codeword length
is dictated by two opposing effects on reliability. On the one
hand, large values of m imply that codewords can tolerate
more device failures, but on the other hand result in a higher
exposure degree to failure as each of the codewords is spread
across a large number of devices. In Figures 1 and 2, the
optimal values, m∗, are indicated by the circles, and the
corresponding codeword lengths are indicated by the vertical
dotted lines. We observe that for small values of n, it holds that
m∗ = n, whereas for large values of n it holds that m∗ < n.
By comparing Figures 1 and 2, we deduce that in general the
optimal codeword lengths for MTTDL and EAFDL are similar,
but not identical. Let us deﬁne by r∗ the ratio of m∗ to n, that
is
r∗ ≜ m∗
n
.
(78)
49
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-550-0
CTRQ 2017 : The Tenth International Conference on Communication Theory, Reliability, and Quality of Service

The r∗ values for various values of the system efﬁciency and
for the two metrics of interest are shown in Figure 3. From
Figures 3(a) and (b) we deduce that the optimal codeword
lengths for MTTDL and EAFDL are similar, and for some
values of n even identical. It can be proved that as n grows
to inﬁnity, the r∗ values for MTTDL and EAFDL approach
a common value that depends on seff and is roughly equal to
0.6.
VI.
CONCLUSIONS
We considered the Mean Time to Data Loss (MTTDL)
and the Expected Annual Fraction of Data Loss (EAFDL)
reliability metrics of storage systems using advanced erasure
codes. A methodology was presented for deriving the two
metrics analytically. Closed-form expressions capturing the
effect of various system parameters were obtained for the
symmetric, clustered and declustered data placement schemes.
We established that the declustered placement scheme offers
superior reliability in terms of both metrics. Subsequently,
a thorough comparison of the reliability achieved by the
declustered placement scheme under various codeword con-
ﬁgurations was conducted. The results obtained show that the
optimal codeword lengths for MTTDL and EAFDL are similar
and, as the system size grows, approach a common value that
depends only on the storage efﬁciency.
Extending the methodology developed to derive the reli-
ability of erasure coded systems under arbitrary rebuild time
distributions and in the presence of unrecoverable latent errors
is a subject of further investigation.
REFERENCES
[1]
D. A. Patterson, G. Gibson, and R. H. Katz, “A case for redundant arrays
of inexpensive disks (RAID),” in Proceedings of the ACM SIGMOD
International Conference on Management of Data, Jun. 1988, pp. 109–
116.
[2]
P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson,
“RAID: High-performance, reliable secondary storage,” ACM Comput.
Surv., vol. 26, no. 2, Jun. 1994, pp. 145–185.
[3]
M. Malhotra and K. S. Trivedi, “Reliability analysis of redundant arrays
of inexpensive disks,” J. Parallel Distrib. Comput., vol. 17, Jan. 1993,
pp. 146–151.
[4]
W. A. Burkhard and J. Menon, “Disk array storage system reliability,”
in Proceedings of the 23rd International Symposium on Fault-Tolerant
Computing, Jun. 1993, pp. 432–441.
[5]
K. S. Trivedi, Probabilistic and Statistics with Reliability, Queueing and
Computer Science Applications, 2nd ed.
New York: Wiley, 2002.
[6]
Q. Xin, E. L. Miller, T. J. E. Schwarz, D. D. E. Long, S. A. Brandt, and
W. Litwin, “Reliability mechanisms for very large storage systems,” in
Proceedings of the 20th IEEE/11th NASA Goddard Conference on Mass
Storage Systems and Technologies (MSST), Apr. 2003, pp. 146–156.
[7]
T. J. E. Schwarz, Q. Xin, E. L. Miller, D. D. E. Long, A. Hospodor,
and S. Ng, “Disk scrubbing in large archival storage systems,” in
Proceedings of the 12th Annual IEEE/ACM International Symposium
on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), Oct. 2004, pp. 409–418.
[8]
S. Ramabhadran and J. Pasquale, “Analysis of long-running replicated
systems,” in Proc. 25th IEEE International Conference on Computer
Communications (INFOCOM), Apr. 2006, pp. 1–9.
[9]
B. Eckart, X. Chen, X. He, and S. L. Scott, “Failure prediction models
for proactive fault tolerance within storage systems,” in Proceedings
of the 16th Annual IEEE International Symposium on Modeling,
Analysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), Sep. 2008, pp. 1–8.
[10]
K. Rao, J. L. Hafner, and R. A. Golding, “Reliability for networked
storage nodes,” IEEE Trans. Dependable Secure Comput., vol. 8, no. 3,
May 2011, pp. 404–418.
[11]
J.-F. Pˆaris, T. J. E. Schwarz, A. Amer, and D. D. E. Long, “Highly
reliable two-dimensional RAID arrays for archival storage,” in Pro-
ceedings of the 31st IEEE International Performance Computing and
Communications Conference (IPCCC), Dec. 2012, pp. 324–331.
[12]
I. Iliadis and V. Venkatesan, “An efﬁcient method for reliability evalu-
ation of data storage systems,” in Proceedings of the 8th International
Conference on Communication Theory, Reliability, and Quality of
Service (CTRQ), Apr. 2015, pp. 6–12.
[13]
——, “Most probable paths to data loss: An efﬁcient method for
reliability evaluation of data storage systems,” International Journal on
Advances in Systems and Measurements, vol. 8, no. 3&4, Dec. 2015,
pp. 178–200.
[14]
V. Venkatesan and I. Iliadis, “A general reliability model for data
storage systems,” in Proceedings of the 9th International Conference
on Quantitative Evaluation of Systems (QEST), Sep. 2012, pp. 209–
219.
[15]
A. Dholakia, E. Eleftheriou, X.-Y. Hu, I. Iliadis, J. Menon, and K. Rao,
“A new intra-disk redundancy scheme for high-reliability RAID storage
systems in the presence of unrecoverable errors,” ACM Trans. Storage,
vol. 4, no. 1, May 2008, pp. 1–42.
[16]
A. Thomasian and M. Blaum, “Higher reliability redundant disk arrays:
Organization, operation, and coding,” ACM Trans. Storage, vol. 5, no. 3,
Nov. 2009, pp. 1–59.
[17]
K. M. Greenan, J. S. Plank, and J. J. Wylie, “Mean time to mean-
ingless: MTTDL, Markov models, and storage system reliability,” in
Proceedings of the USENIX Workshop on Hot Topics in Storage and
File Systems (HotStorage), Jun. 2010, pp. 1–5.
[18]
I. Iliadis, R. Haas, X.-Y. Hu, and E. Eleftheriou, “Disk scrubbing versus
intradisk redundancy for RAID storage systems,” ACM Trans. Storage,
vol. 7, no. 2, Jul. 2011, pp. 1–42.
[19]
“Amazon
Simple
Storage
Service.”
[Online].
Available:
http://aws.amazon.com/s3/ [retrieved: March 2017]
[20]
D. Borthakur et al., “Apache Hadoop goes realtime at Facebook,”
in Proceedings of the ACM SIGMOD International Conference on
Management of Data, Jun. 2011, pp. 1071–1080.
[21]
R. J. Chansler, “Data availability and durability with the Hadoop
Distributed File System,” ;login: The USENIX Association Newsletter,
vol. 37, no. 1, 2013, pp. 16–22.
[22]
K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop
Distributed File System,” in Proceedings of the 26th IEEE Symposium
on Mass Storage Systems and Technologies (MSST), May 2010, pp.
1–10.
[23]
I. Iliadis and V. Venkatesan, “Expected annual fraction of data loss as a
metric for data storage reliability,” in Proceedings of the 22nd Annual
IEEE International Symposium on Modeling, Analysis, and Simulation
of Computer and Telecommunication Systems (MASCOTS), Sep. 2014,
pp. 375–384.
[24]
C. Huang et al., “Erasure coding in Windows Azure Storage,” in
Proceedings of the USENIX Annual Technical Conference (ATC), Jun.
2012, pp. 15–26.
[25]
“IBM Cloud Object Storage.” [Online]. Available: www.ibm.com/
cloud-computing/products/storage/object-storage/how-it-works/
[re-
trieved: March 2017]
[26]
V. Venkatesan and I. Iliadis, “Effect of codeword placement on the
reliability of erasure coded data storage systems,” in Proceedings of the
10th International Conference on Quantitative Evaluation of Systems
(QEST), Sep. 2013, pp. 241–257.
[27]
V. Venkatesan, I. Iliadis, C. Fragouli, and R. Urbanke, “Reliability of
clustered vs. declustered replica placement in data storage systems,” in
Proceedings of the 19th Annual IEEE/ACM International Symposium
on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), Jul. 2011, pp. 307–317.
[28]
V. Venkatesan and I. Iliadis, “Effect of codeword placement on the
reliability of erasure coded data storage systems,” IBM Research Report,
RZ 3827, Aug. 2012.
50
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-550-0
CTRQ 2017 : The Tenth International Conference on Communication Theory, Reliability, and Quality of Service

